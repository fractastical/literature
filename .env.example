# Literature Search and Management System - Environment Configuration
# Copy this file to .env and customize as needed

# ============================================================================
# SEARCH SETTINGS
# ============================================================================

# Default number of results per source per search
LITERATURE_DEFAULT_LIMIT=25

# Maximum total results to return from any source
LITERATURE_MAX_RESULTS=100

# Comma-separated list of sources to use
# Available: arxiv, semanticscholar, biorxiv, pubmed, europepmc, crossref, openalex, dblp
LITERATURE_SOURCES=arxiv,semanticscholar,biorxiv,pubmed,europepmc,crossref,openalex,dblp

# User agent string for API requests
LITERATURE_USER_AGENT=Research-Template-Bot/1.0 (mailto:admin@example.com)

# Search delays (seconds between requests)
LITERATURE_ARXIV_DELAY=3.0
LITERATURE_SEMANTICSCHOLAR_DELAY=1.5

# Retry settings for API requests
LITERATURE_RETRY_ATTEMPTS=3
LITERATURE_RETRY_DELAY=5.0

# Request timeout (seconds)
LITERATURE_TIMEOUT=30.0

# ============================================================================
# PDF DOWNLOAD SETTINGS
# ============================================================================

# Directory for downloaded PDFs
LITERATURE_DOWNLOAD_DIR=data/pdfs

# PDF download timeout (seconds, larger files need more time)
LITERATURE_PDF_DOWNLOAD_TIMEOUT=60.0

# PDF download retry settings
LITERATURE_DOWNLOAD_RETRY_ATTEMPTS=2
LITERATURE_DOWNLOAD_RETRY_DELAY=2.0

# Maximum parallel download workers
LITERATURE_MAX_PARALLEL_DOWNLOADS=4

# PDF download attempt limits (to prevent excessive retries)
LITERATURE_MAX_URL_ATTEMPTS_PER_PDF=8
LITERATURE_MAX_FALLBACK_STRATEGIES=3

# Use browser-like User-Agent for downloads (helps avoid 403 errors)
LITERATURE_USE_BROWSER_USER_AGENT=true

# ============================================================================
# FILE PATHS
# ============================================================================

# BibTeX bibliography file
LITERATURE_BIBTEX_FILE=data/references.bib

# JSON library index file
LITERATURE_LIBRARY_INDEX=data/library.json

# ============================================================================
# UNPAYWALL (Open Access PDF Fallback)
# ============================================================================

# Enable Unpaywall API for open access PDF resolution
LITERATURE_USE_UNPAYWALL=true

# Email for Unpaywall API (required if LITERATURE_USE_UNPAYWALL=true)
UNPAYWALL_EMAIL=your@email.com

# ============================================================================
# LLM SETTINGS (Ollama)
# ============================================================================

# Ollama server URL
OLLAMA_HOST=http://localhost:11434

# Default Ollama model (128K context, fast)
OLLAMA_MODEL=gemma3:4b

# Generation defaults
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# Context window size (128K default for gemma3:4b)
LLM_CONTEXT_WINDOW=131072
LLM_NUM_CTX=131072  # Alternative name for context_window (Ollama parameter)

# Request timeout (seconds)
LLM_TIMEOUT=60

# Seed for reproducibility (optional)
LLM_SEED=

# Maximum tokens for long responses
LLM_LONG_MAX_TOKENS=16384

# System prompt (optional, uses default if not set)
# LLM_SYSTEM_PROMPT=You are an expert research assistant.

# ============================================================================
# SUMMARIZATION SETTINGS
# ============================================================================

# Maximum parallel summarization workers
MAX_PARALLEL_SUMMARIES=1

# Maximum PDF characters to process (model-aware if not set)
# Auto-detected: 50K for <7B, 100K for 7-13B, 200K for >13B
LLM_MAX_INPUT_LENGTH=

# Two-stage summarization for large texts
LITERATURE_TWO_STAGE_ENABLED=true
LITERATURE_TWO_STAGE_THRESHOLD=200000
LITERATURE_CHUNK_SIZE=15000
# Chunk overlap (model-aware: 200 for <7B, 500 for >=7B)
LITERATURE_CHUNK_OVERLAP=

# Summarization timeout (seconds)
# Default: 600s. Adaptive timeout automatically adds 1s per 1000 chars of prompt (max 1200s)
# For large prompts (>200K chars), the timeout will scale automatically
LLM_SUMMARIZATION_TIMEOUT=600

# ============================================================================
# API KEYS (Optional)
# ============================================================================

# Semantic Scholar API key (optional, for higher rate limits)
SEMANTICSCHOLAR_API_KEY=

# ============================================================================
# LOGGING
# ============================================================================

# Log level: 0=DEBUG, 1=INFO, 2=WARN, 3=ERROR
LOG_LEVEL=1

# Disable emoji output in logs
NO_EMOJI=

