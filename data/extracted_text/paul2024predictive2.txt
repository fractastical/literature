ON PREDICTIVE PLANNING AND COUNTERFACTUAL LEARNING
IN ACTIVE INFERENCE
A PREPRINT
Aswin Paul1, 2, 3, Takuya Isomura4 and Adeel Razi1, 5, 6
1 Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University, Clayton 3800, Australia
2 IITB-Monash Research Academy, Mumbai, India
3 Department of Electrical Engineering, IIT Bombay, Mumbai, India
4 Brain Intelligence Theory Unit, RIKEN Center for Brain Science, Wako, Saitama, Japan
5 Wellcome Trust Centre for Human Neuroimaging, University College London, WC1N 3AR London, United Kingdom
6 CIFAR Azrieli Global Scholars Program, CIFAR, Toronto, Canada
March 20, 2024
ABSTRACT
Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent
behaviour is increasingly important. Active inference, regarded as a general theory of behaviour,
offers a principled approach to probing the basis of sophistication in planning and decision-making.
In this paper, we examine two decision-making schemes in active inference based on â€™planningâ€™
and â€™learning from experienceâ€™. Furthermore, we also introduce a mixed model that navigates the
data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate
balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that
requires adaptability from the agent. Additionally, our model provides the opportunity to analyze
the evolution of various parameters, offering valuable insights and contributing to an explainable
framework for intelligent decision-making.
Keywords Active inference Â· Decision making Â· Data-complexity trade-off Â· Hybrid models
1 Introduction
Defining and thereby separating the intelligent â€œagentâ€ from its embodied â€œenvironmentâ€, which then provides feedback
to the agent, is crucial to model intelligent behaviour. Popular approaches, like reinforcement learning (RL), heavily
employ such models containing agent-environment loops, which boils down the problem to agent(s) trying to maximise
reward in the given uncertain environment Sutton and Barto [2018].
Active inference has emerged in neuroscience as a biologically plausible framework Friston [2010], which adopts
a different approach to modelling intelligent behaviour compared to other contemporary methods like RL. In the
active inference framework, an agent accumulates and maximises the model evidence during its lifetime to perceive,
learn, and make decisions Da Costa et al. [2020], Sajid et al. [2021], Millidge et al. [2020]. However, maximising the
model evidence becomes challenging when the agent encounters a highly â€™entropicâ€™ observation (i.e. an unexpected
observation) concerning the agentâ€™s generative (world) model Da Costa et al. [2020], Sajid et al. [2021], Millidge et al.
[2020]. This seemingly intractable objective of maximising model evidence (or minimising the entropy of encountered
observations) is achievable by minimising an upper bound on the entropy of observations, called variational free energy
Da Costa et al. [2020], Sajid et al. [2021]. Given this general foundation, active inference Friston et al. [2017] offers
excellent flexibility in defining the generative model structure for a given problem and has attracted much attention in
various domainsKuchling et al. [2020], Deane et al. [2020].
In this work, we develop an efficient decision-making scheme based on active inference by combining â€™planningâ€™ and
â€™learning from experienceâ€™. After a general introduction to generative world models in the next section, we take a
arXiv:2403.12417v1  [cs.AI]  19 Mar 2024
On predictive planning and counterfactual learning in AIF A PREPRINT
closer look at the decision-making aspect of active inference. Then, we summarise two dominant approaches in active
inference literature: the first based on planning (Section 2.3.1), and the second based on counterfactual learning (cf.
Section 2.3.2). We compare the computational complexity and data efficiency (cf. Section 3.2) of these two existing
active inference schemes and propose a mixed or hybrid model that balances these two complementary schemes (Section
3.3). Our proposed hybrid model not only performs well in an environment that demands adaptability (in Section 3.5)
but also provides insights regarding the explainability of decision-making using model parameters (in Section 4.1).
2 Methods
2.1 Agent-environment loop in active inference
Generative models are central to establishing the agent-environment loop in an active inference model. The agent is
assumed to hold a scaled-down model of the external world that enables the agent to predict the external dynamics
and future observations. The agent can then use its available actions to pursue future outcomes, ensuring survival. We
stick to a partially observed Markov decision process (POMDP) based generative model Kaelbling et al. [1998] in this
paper. POMDPs are a general case of Markov decision processes (MDPs), which are controllable Markov chains apt for
modelling stochastic systems in a discrete state space. In the following section, we provide the specific details of a
POMDP-based generative model.
2.2 POMDP-based generative models
In active inference, agents learn the generative model about external states and optimise their decisions by minimising
variational free energy. The POMDP is a universal framework to model discrete state-space environments, where
the likelihood and state transition are expressed as tractable categorical distributions. Thus, we adopted the POMDP
as our agentâ€™s generative model. The POMDP-based generative model is formally defined as a tuple of finite sets
(S, O, T, U,B, A, D, E) such that:
â—¦ st âˆˆ S : states and s1 is a given initial state.
â—¦ ot âˆˆ O : where ot = st in the fully observable setting, and ot = f(st) in a partially observable setting.
â—¦ T âˆˆ N+, is a finite time horizon available per episode.
â—¦ ut âˆˆ U : actions, for e.g., U={Left, Right, Up, Down}.
â—¦ B : encodes one-step transition dynamics, such that P(st|stâˆ’1, utâˆ’1, B) is the probability that action utâˆ’1
taken at state stâˆ’1 at time t âˆ’ 1 results in st at time t.
â—¦ A : encodes the likelihood distribution, P(ot|st, A) for the partially observable setting.
â—¦ D : prior about the state (s) at the starting time point used for the Bayesian inference of state (s) at time t = 1.
â—¦ E : prior about action-selection used to take action in the simulations at time t = 1.
In the POMDP, hidden states (s) generate observation (o) through the likelihood mapping (A) in the form of a categorical
distribution, P(ot|st, A) = Cat(A). The states s are determined by the transition matrix (B) given the agentâ€™s action
(u), P(st|stâˆ’1, utâˆ’1, B) = Cat(B(stâˆ’1 âŠ— utâˆ’1)). Thus, the generative model in question is given as:
P(o1:t, s1:t, u1:t) = P(A)P(B)P(D)P(E)
tY
Ï„=1
P(oÏ„ |sÏ„ , A)
tY
Ï„=2
P(sÏ„ |sÏ„âˆ’1, uÏ„âˆ’1, B). (1)
Under the mean-field approximation, an approximate posterior distribution (concerning hidden-states s) is given as:
Q(st+1)| {z }
Posterior
= Ïƒ
ï£«
ï£­logP(st+1)| {z }
Prior
+ log(ot+1 Â· Ast+1)| {z }
Likelihood
ï£¶
ï£¸, (2)
where the posterior beliefs about states and parameters are expressed as categorical distribution, Q(st) = Cat(st) and
Dirichlet distribution, Q(A) = Dir(a), respectively. Hence, under this POMDP setup, variational free energy is given
as:
2
On predictive planning and counterfactual learning in AIF A PREPRINT
F =
X
s1:t
Q(s1:t)[ logQ(s1:t) âˆ’ logP(o1:t|s1:t) âˆ’ logP(s1:t) ]
+
X
u1:t
Q(u1:t)[ logQ(u1:t) âˆ’ logP(u1:t|s1:t) ] +DKL [Q(Î¸)||P(Î¸)]. (3)
Variations of F give appropriate posterior expectations about states and parameters. Some optional parameters,
depending on the specific decision-making scheme used, are:
â—¦ C : prior preferences over outcomes, P(o|C). Here, C is the preference for the predefined goal state. This
parameter is generally used in the planning-based active inference models Sajid et al. [2021], Paul et al. [2021].
â—¦ Î“(t): A time-specific risk parameter that the agent maintains to update the state-action mapping CL in the CL
scheme as in Isomura et al. [2022].
â—¦ Î²(s, t): A state-dependent bias parameter used in the mixed model proposed in this paper.
These are used to parameterise the distribution of actions u, and actions are optimised through variational free energy
minimisation. Further details are explained in the subsequent sections.
2.3 Decision-making schemes in active inference
Decision-making under active inference is formulated as minimising the (expected) variational free energy of future
time steps Kaplan and Friston [2018], Friston et al. [2009], Friston [2012]. This enables an agent to deploy a planning-
based decision-making scheme, where an agent predicts possible outcomes and makes decisions to attain states and
observations that minimise expected free energy (EFE). Classically, active inference optimises policies â€“ i.e., sequences
of actions in time â€“ instead of a state-action mapping in methods like Q-Learning Sutton and Barto [2018] in RL to
choose the policy that minimises EFE Sajid et al. [2021]. However, such formulations limit agents to solve environments
only with low-dimensional state-space Sajid et al. [2021], Paul et al. [2021].
Several improvements to the framework followed, including the recent sophisticated inference scheme Friston et al.
[2021] that uses a recursive form of free energy to ease the computational complexity of policy search. The sophisticated
inference method uses a forward tree search in time to evaluate EFE; however, it restricts the planning depth of agents
Friston et al. [2021] due to computational complexity. More innovative algorithms like dynamic programming can be
used to linearise the planning Paul et al. [2023], Da Costa et al. [2020]. The proposed linearised planning method was
called Dynamic programming in expected free energy (DPEFE) in Paul et al. [2023]. This DPEFE algorithm performs
at par with benchmark reinforcement learning methods like Dyna-Q in environments similar to grid world tasks Paul
et al. [2021] (See Section 2.3.1 for technical details of this method). A generalisation of the DPEFE algorithm was
recently proposed as â€˜inductive-inferenceâ€™ to model â€™intentional behaviourâ€™ in agents Friston et al. [2023].
Another recent work deviates from this classical approach of predictive planning and employs â€œlearning from experienceâ€
to determine optimal decisions Isomura et al. [2022]. This scheme is mathematically equivalent to a particular class of
neural networks accompanied by some neuromodulations of synaptic plasticity Isomura and Friston [2020], Isomura
et al. [2022]. It uses counterfactual learning (the CL method in this paper) to accumulate a measure of â€˜riskâ€™ over
time-based on environmental feedback. Subsequent work that validates this scheme experimentally using in-vitro neural
networks has also appeared recently Isomura et al. [2023].
The following summarises the critical algorithmic details of both schemes: DPEFE in Sec.2.3.1 and CL scheme in
Sec.2.3.2. Both schemes are proposed based on conventional POMDPs.
2.3.1 DPEFE scheme and action precision
The DPEFE scheme in this paper is based on the work in Paul et al. [2021]. This scheme was generalised to a POMDP
setting in the paper Paul et al. [2023]. The model parameters used are as given in Sec.2.2. The action-perception
loop in the DPEFE scheme comprises perception (i.e., identifying states that cause observations), planning, action
selection, and learning model parameters. In this paper, all environments are fully observable since our focus is on
decision-making rather than perception, hence O = S.
The action selection in the DPEFE scheme is implemented as follows: After evaluating the expected free energy (EFE,
G) of future observations using dynamic programming (cf. Paul et al. [2023]), the agent evaluates the probability
distribution for selecting an action u as:
3
On predictive planning and counterfactual learning in AIF A PREPRINT
PDPEFE(u|s) = Ïƒ (âˆ’Î± G(u|s)) . (4)
Here, Ïƒ is the classical softmax function, rendering actions with smaller EFE being selected with larger probabilities.
The action precision parameter (Î±) may be tuned to increase/decrease the agentâ€™s action selection confidence. For a
detailed description of the evaluation of the EFE (G) and the DPEFE algorithm, refer to Paul et al. [2023] (Section 5).
2.3.2 CL method and risk parameter
Instead of attempting to minimise the EFE directly, in the counterfactual learning (CL) method, the agent learns a
state-action mapping CL. This state-action mapping is learned through an update equation mediated by a â€™riskâ€™ term Î“t
as defined in Isomura et al. [2022]:
CL â† CL + t âŸ¨ (1 âˆ’ 2 Î“t)âŸ¨ut âŠ— stâˆ’1âŸ© âŸ©. (5)
Here, âŸ¨Â·âŸ© refers to the average over time, and âŠ— is the Kronecker-product operator. Given the state-action mapping CL,
the agent samples actions from the distribution,
P(u|s)CL = Ïƒ (ln CL Â· stâˆ’1) . (6)
In the simulations, Î“t with the following functional form is used: When the agent is at the start position â€” or when
the agentâ€™s action causes a â€œhigh riskâ€ â€” the value of 0.9 is substituted, i.e., Î“t â† 0.9. Otherwise, Î“t decreases
continuously following the equation
Î“t â† Î“t âˆ’ 1
Tgoal âˆ’ t. (7)
Here, Tgoal is when the agent receives a positive environmental reward. So, the sooner the agent comes to the desirable
state, the quicker the Î“t (i.e., risk) converges to zero 1.
All the update rules defined in the paper can be derived from the postulate that the agent tries to minimise the (variational)
free energy (Eq. 3) w.r.t the generative model Paul et al. [2023], Isomura et al. [2022]. In the rest of the paper, we
investigate the performance of the two schemes â€” i.e. the DPEFE and the CL method â€” and consider a scheme
combining them. The following section explores how these two schemes perform in a given environment.
3 Results
We now test the performance of two decision-making schemes (DPEFE and CL) in benchmark environments such as
the Cart Pole - v1 (Fig. 1) from OpenAIGym.
3.1 Cart Pole - v1 (OpenAI Gym task)
In a Cart Pole - v1 environment, an agent is rewarded for balancing the pole upright (within an acceptable range) by
moving the cart sideways (Fig.1 (A)). An episode terminates when the pole or cart crosses the acceptable range (Â±12
degrees for the pole and Â±2.4 units frame size for the cart, Fig.1 (B)). This problem is inherently spontaneous, without
the need for planning from the controller, where the agent must react to the current situation of the cart and the pole.
We then test the active inference in a mutating setup, where the environment mutates to a more challenging version with
half the acceptable range for both the pole and cart position (Â±6 degrees for the pole and Â±1.2 units frame size for the
cart). The performance of the active inference agents with different planning is summarised in Fig.2 (A).
As expected, the CL method agent outperforms other active inference schemes (As the problem demands spontaneous
control, favouring a state-action mapping over planning). The agents quickly learn the necessary state-action mapping
and balance the pole more effectively than other planning-based schemes. We observe this also after the mutation in
the environment at episode number 100. The improved performance of the CL method agent after mutation warrants
additional investigation; however, it can be attributed to the increased feedback frequency due to the increased failure
rate after mutation.
In Fig.2 (B), we see the evolution of the risk term (Î“). The risk Î“ settles to a value less than 0.5 as the agent learns more
about the environment. It is interesting to note the increase in Î“ when faced with a mutation in the environment in Fig.2
1For the exact form of the generative model and free energy, refer to Isomura and Friston [2020].
4
On predictive planning and counterfactual learning in AIF A PREPRINT
Modality Range Episode
termination
Cart Position -4.8 to 4.8 Â±2.4
Cart Velocity -inf to inf NA
Pole Angle -24Â°to 24Â° Â±12Â°
Pole Angular 
Velocity
-inf to inf NA
A B
Figure 1: A: A snapshot from the Cart Pole - v1 environment (from OpenAI Gym), B: Environment summary: The
objective is to balance the pole (brown) upright as long as possible without meeting the episode termination criteria, i.e.
without the pole and cart crossing pole-angle and cart-position thresholds respectively.
A B
Figure 2: A: Performance of active inference agents with different decision-making schemes in the mutating Cart Pole -
v1 ( with a mutation at episode 100). After Episode 100, the environment mutates to a harder version, which the agents
must adapt to. B: Evolution of the risk parameter (Î“t) of the CL method agent when embodied in the Mutating Cart
Pole problem. We can observe the spike at episode 100, consistent with mutation, and the reduced risk resulting in
improved performance in the second half of the trial.
5
On predictive planning and counterfactual learning in AIF A PREPRINT
A B
Figure 3: A: A snapshot of the 900-state grid world (maze) environment. B: The optimal solution for the maze is
shown in A. This is a complex maze, as when actions are taken randomly, it takes around9000 steps to navigate the
grid against the optimal route with 47 steps.
(B) as expected. The risk-reducing behaviour correlates with the increase in performance after episode number 100,
highlighting the explainability of the active inference framework. Next, we test the agents in a fundamentally different
environment â€“ a maze task â€“ which warrants the need for planning for the future.
3.2 Complex maze task and data-complexity trade-off
To compare the performance of the two agents in a strategic task, we simulate the performance in a standard grid world
task as shown in Fig.3 (A). The optimal solution to this grid problem is demonstrated in Fig.3 (B). This is a complex
grid world, which is non-trivial compared to grid world tasks used in the past literature to solve Sajid et al. [2021], as
it will take around nine thousand steps for an agent to reach the goal state if actions are taken randomly against the
optimal route with length 47.
The performance is evaluated regarding how soon the agent can finish an episode (i.e., the length of an episode (lower
the better) for reaching the goal state). The simulation results showing the performance of DPEFE and CL agents
are plotted in Fig.4 (A). These results show that the predictive planning-based DPEFE agent can learn quickly (i.e.,
within ten episodes) to navigate this grid. In the simulations, the action precision used by the DPEFE agent is Î± = 1
substituted in (4). The agent tends to navigate in even lower time steps for a higher action precision (Ïƒ), always sticking
to optimal actions. Additionally, we observe that the CL method agent takes longer to learn the optimal path. This
result (Fig.4 (A)) shows that the CL agent needs more experience in the environment (i.e. more data) to solve it.
In Fig.4 (B), we compare major active inference algorithmsâ€™ computational complexity associated with planning
for decision-making. The DPEFE algorithm is computationally efficient compared to other popular active inference
schemes Sajid et al. [2021], Friston et al. [2021]. Please note that this figure also emphasises how the CL method has
no computational complexity associated with planning. So, it is clear that the CL method agent is computationally
cheaper than the DPEFE agent as there is no planning component. The computational complexity of the DPEFE
agent is associated with the planning depth (time horizon of planning, T), as seen in Fig.4 (B). This demonstrates a
data-complexity trade-off between both these schemes.
This realisation motivates us towards a mixed model, where we propose to develop an agent that can balance the two
schemes according to the resources available to the agent. This makes much sense from the neuro-biological perspective,
as biological agents continually try to balance resources to learn and plan for the future versus the experience they
already have. This idea also relates to the classic exploration-exploitation dilemma in reinforcement learning Triche
et al. [2022].
6
On predictive planning and counterfactual learning in AIF A PREPRINT
A B
Figure 4: A: Performance comparison of DPEFE and CL agents in the 900-state grid scheme with 300 episodes. The
DPEFE agent learns to navigate the grid faster (With a lower episode length) than the CL method agent. B: Comparison
of computational complexity between state-of-the-art active inference algorithms Sajid et al. [2021], Friston et al.
[2021], the DPEFE method Paul et al. [2021] and CL method Isomura et al. [2022]. Please note that the y-axis is in the
log scale. The computational complexity was calculated for the algorithms to implement planning in a standard grid
like in Fig.3.
3.3 Integrating the two decision-making approaches
To enable the agent to balance its ability to predict future outcomes and use prior experience, we introduce a state-
dependent bias parameter that evolves with experience (Î²(s, t) âˆˆ [0, 1]) to the model. This addition is motivated by
the hypothesis that an agent maintains a sense of bias, quantifying its confidence in the experience of deciding (in the
past) in that particular state.
When exposed to a new environment, an agent starts with an equal bias for DEEFE (predictive planning) and CL
schemes, represented by a prior bias parameter Î²prior = 0.5.
Over the episodes, the agent will have the probability distributions for decision-making from both models. These
distributions enable decision-making given the present state (s). In a fully observable environment (MDP), s is known
to the agent (i.e. O = S, or A = I, the identity mapping). In the partially observable case (POMDP), the agent infers the
(hidden) state (s) from observation (o) by minimising variational free energy Da Costa et al. [2020], Sajid et al. [2021].
Given the state estimation, P(u|s)DPEFE and P(u|s)CL are the distributions used for sampling decision-making corre-
sponding to the DPEFE scheme and CL method respectively (See Section 2.3.1 and Section 2.3.2 for details).
Given these distributions, the agent can now evaluate how â€™usefulâ€™ they are using their Shannon entropy (H(X)). This
measure is beneficial as it represents how â€™sureâ€™ that particular distribution is regarding a decision in that state(s).
Namely, if the agent has confidence in a specific action, the action distribution tends to be a one-hot vector favouring
the confident action; hence, the entropy of the distribution tends to zero, in contrast to the uniform distribution (not
favouring any action) with maximum entropy. Thus, comparing this quantity enables the selection of the most confident
strategy from the pool of different schemes.
Based on this observation, over time, the agent can use this entropy measure to update the value of Î²(s, t) as follows:
Î²(st) â† Î²(st) + Î± (H(PCL(u|st)) âˆ’ H(PDPEFE(u|st))) . (8)
Here, Î± is a normalisation parameter stabilising the updated value, and we make sure that Î² âˆˆ [0, 1] by re-calibrating
Î² <0 as Î² = 0 and Î² >1 as Î² = 1. From a Bayesian inference perspective, one may view the updated belief Î²
in Eq.(8) as a posterior belief representing how likely the DPEFE model is selected, similar to the Bayesian model
selection schemes.
7
On predictive planning and counterfactual learning in AIF A PREPRINT
s1 o1
Bu
s2 o2
Bu
s3 o3
sT oT
u1
u2
sT-1 oT-1
Bu
uT-1
ğ”¸A
A
A
A
A
ğ”¸
ğ”¸
ğ”¸
=
=
=
=
s1
s2
s3
sT-1
ğ”¹u
ğ”¹u
ğ”» ğ”¼
u1
Î“(t)
â„‚ğ•ƒ
â„‚ğ•ƒ
â„‚ ğ”¾
u2
ğ«
uT-1
PDPEFE
PMM
â„‚ğ•ƒ
Generative process
Generative model
Perception CL method DPEFE 
Planning Decision makingReal world
Figure 5: Flow diagram of the agent-environment loop in the proposed mixed model combining planning and coun-
terfactual learning. There is a key distinction between the generative process and the generative model in the active
inference framework. In a POMDP, we assume that the observations are generated by the generative process (â€˜Real
worldâ€™) by â€˜hidden-statesâ€™ (st) through a state-observation mapping (A), both being inaccessible to the agent. In the
generative model, the agent uses ot to maintain an optimal belief about the hidden state st (â€˜Perceptionâ€™). Subsequently,
the agent uses the planning method (DPEFE) and Counterfactual (CL) method to combine the action distributions using
the model-bias parameter Î² for decision-making. The decision at time t influences the hidden state of the â€˜Real worldâ€™
at the next step, completing the loop. The generative process can be thought of as the environment the agent tries to
survive in, whereas the generative model is completely part of the agent and can be interpreted as the â€˜imaginaryâ€™ world
the agent assumes it survives.
Using this measure of bias Î²(st), the agent can now evaluate a new distribution for decision-making,PMM , where MM
stands for the mixed model as:
P(u|st)MM = P(u|st)1âˆ’Î²(st)
CL Â· P(u|st)Î²(st)
DPEFE. (9)
The flow diagram describing the proposed mixed modelâ€™s POMDP-based â€œagent-environmentâ€ loop is given in Fig.5. 2.
3.4 Deriving update equations for the mixed model from variational free energy
Eqs.8 and 9 can be derived from variational free energy minimisation under a POMDP generative model. The variational
free energy for the mixed model is defined as:
2For a detailed description of various parameters in the hybrid model, refer to Section 2.2, Section 2.3.1, and Section 2.3.2.
8
On predictive planning and counterfactual learning in AIF A PREPRINT
F =
tX
Ï„=1
sÏ„ Â· {ln sÏ„ âˆ’ ln A Â· oÏ„ âˆ’ ln B sÏ„âˆ’1}
+
tX
Ï„=1
uÏ„ Â· {ln uÏ„ + Î²Î± Â· GÏ„ âˆ’ (1 âˆ’ Î²)(1 âˆ’ 2Î“t) lnCL sÏ„âˆ’1}
+DKL [Q(Î²)||P(Î²)] + DKL [Q(Î¸)||P(Î¸)] (10)
When Î“t = 0 and Î²prior = 0.5, the derivative of F with respect to Î² = E[Î²] gives the posterior expectation as follows:
Î² = sig
 
âˆ’
tX
Ï„=1
uÏ„ Â· Î± Â· GÏ„ âˆ’
tX
Ï„=1
uÏ„ Â· ln CLsÏ„âˆ’1
!
. (11)
Interestingly, this posterior expectation can be rewritten using the entropies of DPEFE and CL. The aboveF becomes
variational free energy (Eq. 3) for DPEFE or CL when Î² = 1 or 0, respectively.
Thus, minimising F with respect to uÏ„ yields:
uÏ„ = Ïƒ(âˆ’Î± Â· GÏ„ ), (12)
for DPEFE and,
uÏ„ = Ïƒ(ln CLsÏ„âˆ’1), (13)
for CL (note that Î“t = 0 is usually supposed in CL when generating actions).
Thus, from the definition of the Shannon entropy, we obtain
HDPEFE = âˆ’
tX
Ï„=1
uÏ„ Â· ln uÏ„ =
tX
Ï„=1
uÏ„ Â· Î± Â· GÏ„ , (14)
and,
HCL = âˆ’
tX
Ï„=1
uÏ„ Â· ln uÏ„ = âˆ’
tX
Ï„=1
uÏ„ Â· ln CLsÏ„âˆ’1. (15)
Hence, Î² can be rewritten as:
Î² = sig(âˆ’HDPEPE + HCL). (16)
When |HCL âˆ’ HDPEPE| << 1, Eq.8 approximates Eq.16. Minimisation of F further yields Eq.9 as it is an expression
using the probability distribution and equivalent to the posterior expectation:
uÏ„ = Ïƒ(âˆ’Î²Î± Â· GÏ„ + (1 âˆ’ Î²) lnCLsÏ„âˆ’1). (17)
Therefore, the update rules for the mixed model (Eqs.8 and 9) can be formally derived from variational free energy
minimisation.
3.5 Performance of mixed-model in a mutating maze environment
We now examine the proposed mixed scheme with agents of different planning power (i.e. different planning depths, N
3) in a similar environment. The computational complexity of the DPEFE scheme is linearly dependent on the planning
time horizon (planning depth), i.e. T, and holds for the mixed-model agent as well (see Fig.4). Thus, an agent with
planning depth N = 50 takes up twice the computational resources while planning compared to an agent with N = 25.
We use a mutating grid environment to test the performance of the mixed model-based agent. This mutating grid scheme
is illustrated in Fig.6. The agent starts in a more accessible grid version with an optimal path of four steps (Fig.6, (A)).
After 300 episodes, the environment mutates to the complex version of the grid shown in the previous section (See
Fig.6 (B)). This setup also enables us to study how adaptable the agent is to new environmental changes.
The performance is summarised in Fig.7. We observed that all three mixed model agents (with varying levels of
planning ability) learned to navigate the more accessible grid within the first ten episodes (Fig.7: A). However, when
3We refer to the planning horizon of the mixed-model as N, and the DPEFE method as T to avoid confusion.
9
On predictive planning and counterfactual learning in AIF A PREPRINT
A B
Figure 6: The mutating grid scheme used for studying agentâ€™s adaptability. The agent learns to navigate the easy grid
(A) in the first half (300 episodes) and faces environment mutation and should learn to solve the hard grid (B).
A B
Figure 7: A: Performance of mixed model agents with different planning depths in the mutating grid scheme, B:
Performance of mixed model agents with different planning depths in the hard maze simulated separately.
10
On predictive planning and counterfactual learning in AIF A PREPRINT
A B
Figure 8: A: Evolution of the Risk parameter (Î“) of the mixed-model agent when embodied in the mutating grid scheme,
B: Evolution of the model mixing parameter (Î²) of the mixed-model agent when embodied in the mutating grid scheme.
the environment mutated to the rigid grid in episode number 300, the agents learned similar to the performance we
observe when navigating that grid alone, Fig.7: B, (i.e., complex grid with 900 states).
We also observed that the agent with higher planning ability learned to navigate the grid faster and more confidently
than the other two. This result demonstrates that the proposed mixed model enables agents to balance the two
decision-making approaches in the active inference framework.
It is considered that the brain of biological organisms also employs mechanisms to switch multiple strategies. Our model
is potentially helpful for unveiling efficient decision-making mechanisms in the brain and their neuronal substrates and
developing computationally efficient bio-mimetic agents.
4 Discussion
4.1 Explainability of the active inference models
An additional advantage of the mixed model proposed (and the POMDP-based generative models) is that we can probe
the model parameters to understand the basis of intelligent behaviour demonstrated by agents through the lens of active
inference. Models that rely on artificial neural networks (ANNs) to scale up the models Fountas et al. [2020] have
limited explainability regarding how agents make decisions, especially when faced with uncertainty.
In Fig.8: (A), we can probe to see the evolution of the risk (Î“t) in the model (associated with the CL method scheme
as defined in Isomura et al. [2022]). We can observe that the modelâ€™s risk quickly tends to zero when the easy grid is
presented and solved; however, it shoots up when faced with the environment mutation.
Similarly, the evolution of the bias parameter (that balances the DPEFE and CL method in the mixed model) is shown
in Fig.8: (B). Here, we also observe how the agent consistently maintains a higher bias to the DPEFE model when it
has a higher planning ability (i.e. the agent with a planning depth of N = 50 compared to bias in agents with N = 25,
and N = 5).
We should note that the value of the bias parameter never increases more than 0.5, even when the DPEFE agent is
planning at T = 50. In the simulations, we start with a bias Î² = 0.5 and update Î² according to (8). This shows how the
agent eventually learns to rely on the mixed modelâ€™s CL scheme (i.e., experience). Still, the DPEFE component (i.e.
planning) accelerates learning and performance to aid decision-making. Such insights into the explainability of the
agentâ€™s behaviour via model parameters help study the basis of natural/synthetic intelligence.
11
On predictive planning and counterfactual learning in AIF A PREPRINT
4.2 Conclusions
This paper compared and contrasted two decision-making schemes in the active inference framework. Observing the
pros and cons of both approaches, we examined them on tasks that demand spontaneous (Cart Pole - v1) and strategic
(maze) decision-making, thereby testing a hybrid approach. The insights observed in this work will help improve
algorithms used for control, given the excitement around using active inference schemes Da Costa et al. [2022].
We leave the detailed analysis of behavioural dependence on parameters and model expansion in more demanding
environments to future work. Systematic comparison with ANNs (Artificial Neural Networks) aided models like in the
results of Fountas et al. [2020] is also a promising direction to pursue.
5 Software note
The grid environment and agents (DPEFE, CL and Mixed-model schemes) were custom-written in Python. All scripts
are available at the following link: https://github.com/aswinpaul/aimmppcl_2023.
6 Acknowledgments
AP acknowledges research sponsorship from IITB-Monash Research Academy, Mumbai and the Department of
Biotechnology, Government of India. TI is funded by the Japan Society for the Promotion of Science (JSPS) KAKENHI
(Refs: JP23H04973 & JP23H03465) and the Japan Science and Technology Agency (JST) CREST (Ref: JPMJCR22P1).
AR is funded by the Australian Research Council (Ref: DP200100757) and the Australian National Health and
Medical Research Council Investigator Grant (Ref: 1194910). AR is affiliated with The Wellcome Centre for Human
Neuroimaging, supported by core funding from Wellcome [203147/Z/16/Z]. AR is also a CIFAR Azrieli Global Scholar
in the Brain, Mind & Consciousness Program.
References
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition,
2018. URL http://incompleteideas.net/book/the-book-2nd.html.
Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127â€“138, 2010.
ISSN 1471-0048. doi:10.1038/nrn2787. URL https://doi.org/10.1038/nrn2787.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active in-
ference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology , 99:102447, 2020. ISSN
0022-2496. doi:10.1016/j.jmp.2020.102447. URL https://www.sciencedirect.com/science/article/
pii/S0022249620300857.
Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystified and compared. Neural
Computation, 33(3):674â€“712, January 2021. ISSN 0899-7667. doi:10.1162/neco_a_01357. URL https://doi.
org/10.1162/neco_a_01357.
Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Whence the expected free energy?, 2020.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference. Network
Neuroscience, 1(4):381â€“414, 2017. doi:10.1162/NETN_a_00018. URL https://doi.org/10.1162/NETN_a_
00018.
Franz Kuchling, Karl Friston, Georgi Georgiev, and Michael Levin. Morphogenesis as bayesian inference: A variational
approach to pattern formation and control in complex biological systems. Physics of Life Reviews , 33:88â€“108,
2020. ISSN 1571-0645. doi:https://doi.org/10.1016/j.plrev.2019.06.001. URL https://www.sciencedirect.
com/science/article/pii/S1571064519300909.
George Deane, Mark Miller, and Sam Wilkinson. Losing ourselves: Active inference, depersonalization, and meditation.
Frontiers in Psychology , 11, 2020. ISSN 1664-1078. doi:10.3389/fpsyg.2020.539726. URL https://www.
frontiersin.org/articles/10.3389/fpsyg.2020.539726.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(1):99â€“134, 1998. ISSN 0004-3702. doi:https://doi.org/10.1016/S0004-
3702(98)00023-X. URL https://www.sciencedirect.com/science/article/pii/S000437029800023X.
Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, and Adeel Razi. Active inference for stochastic control. In Machine
Learning and Principles and Practice of Knowledge Discovery in Databases, pages 669â€“680, Cham, 2021. Springer
International Publishing. ISBN 978-3-030-93736-2. doi:https://doi.org/10.1007/978-3-030-93736-2_47.
12
On predictive planning and counterfactual learning in AIF A PREPRINT
Takuya Isomura, Hideaki Shimazaki, and Karl J. Friston. Canonical neural networks perform active inference.
Communications Biology, 5(1):55, 2022. ISSN 2399-3642. doi:10.1038/s42003-021-02994-2. URL https:
//doi.org/10.1038/s42003-021-02994-2 .
Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological Cybernetics, 112
(4):323â€“343, 2018. ISSN 1432-0770. doi:10.1007/s00422-018-0753-2. URL https://doi.org/10.1007/
s00422-018-0753-2 .
Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement learning or active inference? PLOS ONE, 4(7):
1â€“13, 07 2009. doi:10.1371/journal.pone.0006421. URL https://doi.org/10.1371/journal.pone.0006421.
Karl Friston. A free energy principle for biological systems. Entropy (Basel, Switzerland), 14:2100â€“2121, 11 2012.
doi:10.3390/e14112100.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated inference. Neural
Computation, 33(3):713â€“763, February 2021. ISSN 0899-7667. doi:10.1162/neco_a_01351. URL https://doi.
org/10.1162/neco_a_01351.
Aswin Paul, Noor Sajid, Lancelot Da Costa, and Adeel Razi. On efficient computation in active inference. arXiv
preprint arXiv:2307.00504, 2023.
Karl J. Friston, Tommaso Salvatori, Takuya Isomura, Alexander Tschantz, Alex Kiefer, Tim Verbelen, Mag-
nus T. Koudahl, Aswin Paul, Thomas Parr, Adeel Razi, Brett J. Kagan, Christopher L. Buckley, and Maxwell
James D. Ramstead. Active inference and intentional behaviour. ArXiv, abs/2312.07547, 2023. URL https:
//api.semanticscholar.org/CorpusID:266191299.
Takuya Isomura and Karl Friston. Reverse-Engineering Neural Networks to Characterize Their Cost Functions.
Neural Computation, 32(11):2085â€“2121, 11 2020. ISSN 0899-7667. doi:10.1162/neco_a_01315. URL https:
//doi.org/10.1162/neco_a_01315.
Takuya Isomura, Kiyoshi Kotani, Yasuhiko Jimbo, and Karl J. Friston. Experimental validation of the free-energy princi-
ple with in vitro neural networks. Nature Communications, 14(1):4547, 2023. ISSN 2041-1723. doi:10.1038/s41467-
023-40141-z. URL https://doi.org/10.1038/s41467-023-40141-z .
Anthony Triche, Anthony S. Maida, and Ashok Kumar. Exploration in neo-hebbian reinforcement learning:
Computational approaches to the explorationâ€“exploitation balance with bio-inspired neural networks. Neu-
ral Networks , 151:16â€“33, 2022. ISSN 0893-6080. doi:https://doi.org/10.1016/j.neunet.2022.03.021. URL
https://www.sciencedirect.com/science/article/pii/S0893608022000995.
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using Monte-Carlo
methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.
Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How active inference could help
revolutionise robotics. Entropy, 24(3), 2022. ISSN 1099-4300. doi:10.3390/e24030361. URL https://www.mdpi.
com/1099-4300/24/3/361.
13