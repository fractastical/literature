HYPOTHESIS AND THEORY
p
ublished: 14 June 2016
doi: 10.3389/fncom.2016.00056
Frontiers in Computational Neuroscience | www.frontiersin .org 1 June 2016 | Volume 10 | Article 56
Edited by:
F
lorentin Wörgötter,
University Goettingen, Germany
Reviewed by:
Goren Gordon,
Weizmann Institute of Science, Israel
Li Su,
University of Cambridge, UK
*Correspondence:
M. Berk Mirza
muammer.mirza.15@ucl.ac.uk
Received: 15 March 2016
Accepted: 25 May 2016
Published: 14 June 2016
Citation:
Mirza MB, Adams RA, Mathys CD and
Friston KJ (2016) Scene Construction,
Visual Foraging, and Active Inference.
Front. Comput. Neurosci. 10:56.
doi: 10.3389/fncom.2016.00056
Scene Construction, Visual Foraging,
and Active Inference
M. Berk Mirza1*, Rick A. Adams2, 3, Christoph D. Mathys1, 4, 5and Karl J. Friston1
1 Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, London, UK, 2 Division of
Psychiatry, University College London, London, UK, 3 Institute of Cognitive Neuroscience, University College London,
London, UK, 4 Translational Neuromodeling Unit, Institute for Biomedical Engineering, University of Zurich and Swiss Federal
Institute of Technology in Zurich (ETHZ), Zurich, Switzerland, 5 Max Planck UCL Centre for Computational Psychiatry and
Ageing Research, London, UK
This paper describes an active inference scheme for visual searches and the perceptual
synthesis entailed by scene construction. Active inference assumes that perception
and action minimize variational free energy, where actions are selected to minimize
the free energy expected in the future. This assumption generalizes risk-sensitive
control and expected utility theory to include epistemic value; namely, the value (or
salience) of information inherent in resolving uncertainty about the causes of ambiguous
cues or outcomes. Here, we apply active inference to saccadic searches of a visual
scene. We consider the (difﬁcult) problem of categorizing a scene, based on the
spatial relationship among visual objects where, crucially, visual cues are sampled
myopically through a sequence of saccadic eye movements. This means that evidence
for competing hypotheses about the scene has to be accumulated sequentially, calling
upon both prediction (planning) and postdiction (memory). Our aim is to highlight some
simple but fundamental aspects of the requisite functional anatomy; namely, the link
between approximate Bayesian inference under mean ﬁeld assumptions and functional
segregation in the visual cortex. This link rests upon the (neurobiologically plausible)
process theory that accompanies the normative formulation of active inference for
Markov decision processes. In future work, we hope to use this scheme to model
empirical saccadic searches and identify the prior beliefs that underwrite intersubject
variability in the way people forage for information in visual scenes (e.g., in schizophrenia).
Keywords: active inference, visual search, Bayesian inference, scene construction, free energy, information gain,
epistemic value, salience
INTRODUCTION
We have a remarkable capacity to sample our visual world in an eﬃcient fashion, resolving
uncertainty about the causes of our sensations so that we can act accordingly. This capacity calls
on the ability to optimize not just beliefs about the world that is “out there” but also the way in
which we sample information ( Howard, 1966; Shen et al., 2011; Wurtz et al., 2011; Andreopoul
 os
and Tsotsos, 2013; Pezzulo et al., 2013). This is particularly evident in active vision, where discre te
and restricted (foveal) visual data is solicited every few 100 ms, through saccadic eye movements
(Grossberg et al., 1997; Srihasam et al., 2009). In this paper, we consider the principles that underlie
t
his visual foraging—and how it is underwritten by resolving uncertainty about the visual scene that
is being explored. We approach this problem from the point of view of active inference; namely,
Mirza et al. Visual Foraging and Active Inference
the assumption that action and perception serve to minimize
surprise or uncertainty under prior beliefs about how sensati ons
are caused.
Active or embodied inference is a corollary of the free
energy principle that tries to explain everything in terms of th e
minimization of variational free energy. Variational free energy
is a proxy for surprise or Bayesian model evidence. This means
that minimizing free energy corresponds to avoiding surprise s
or maximizing model evidence. The embodied or situated
aspect of active inference acknowledges the fact that we are t he
authors of the sensory evidence we garner. This means that th e
consequences of sampling or action must themselves be inferr ed.
In turn, this implies that we have (prior) beliefs about our
behavior. Active inference assumes that the only self-cons istent
prior belief is that our actions will minimize free energy; in
other words, we (believe we) will behave to avoid surprises or
resolve uncertainty through active sampling of the world. Thi s
paper illustrates how this works with simulations of saccadic
eye movements and scene construction using a discrete (Mark ov
decision process) formulation of active inference (
Friston et al.,
2011, 2015).
We consider the problem of categorizing a scene based
upon the sequential sampling of local visual cues to construct
a picture or hypothesis about how visual input is generated.
This is essentially the problem of scene construction (
Hassabis
and Maguire, 2007; Zeidman et al., 2015 ), where each scene
corresponds to a hypothesis or explanation for sequences of
visual cues. The main point that emerges from this perspective
is that the scene exists only in the eye of the beholder: it is
represented in a distributed fashion through recurrent mess age
passing or belief propagation among functionally segregated
representations of where (we are currently sampling) and what
(is sampled). This application of active inference emphasizes
the epistemic value of free energy minimizing behavior—as
opposed to the pragmatic (utilitarian) value of searching for
preferred outcomes. However, having said this, the theory (r esp.
simulations) uses exactly the same mathematics (resp. softw are
routines) that we have previously used to illustrate foraging
behavior in the context of reward seeking (
Friston et al., 2015 ).
Our aim is to introduce a model of epistemic foraging that can
be applied to empirical saccadic eye movements and, ultimately ,
be used to phenotype individual subjects in terms of their prior
beliefs: namely, the prior precision of beliefs about competin g
epistemic policies and the precision of prior preferences (c.f.,
“incentive epistemic” and motivational salience). This may be
particularly interesting when looking at schizophrenia and o ther
clinical phenotypes that show characteristic abnormalities during
visual (saccadic) exploration. For example, schizophrenia ha s
been associated with “aberrant salience, ” in which subject s
attend to—and hence saccade to—inconsequential features o f
the environment (
Kapur, 2003; Beedie et al., 2011 ). It is
unclear, however, whether “aberrant” salience is epistemic or
motivational, or both; put simply, do subjects with schizophre nia
fail to gather information, and/or fulﬁll their goals?
This paper comprises three sections. In the ﬁrst, we brieﬂy
rehearse active inference and the underlying formalism. The
second section describes the paradigm that it subsequently
modeled using the formalism of the ﬁrst section. In brief, this
requires agents to categorize a scene based upon discrete (vi sual)
cues that can be sampled from one of four peripheral locations
(starting from central ﬁxation). Crucially, the scene categ ory
is determined purely by the spatial relationships among the
cues—as opposed to their absolute position. By equipping the
agent’s generative model with preferences for correct (as oppo sed
to incorrect) feedback, we also model the overt reporting of
categorical decisions; thereby emulating a speeded response task.
In the ﬁnal section, we characterize sequences of trials und er
diﬀerent levels of prior precision and preferences (for avoiding
incorrect feedback). The results are characterized in term s
of simulated electrophysiological responses, saccadic inter vals,
and the usual behavioral measures of speed and accuracy. We
conclude with a brief discussion of how this model might be use d
in an empirical (computational psychiatry) setting.
ACTIVE INFERENCE AND EPISTEMIC
VALUE
Active inference is based upon the premise that every living thi ng
minimizes variational free energy. This single premise leads to
some surprisingly simple update rules for action, perception,
policy selection, and the encoding of salience or precision. In
principle, the active inference scheme described below can be
applied to any paradigm or choice behavior. Indeed, earlier
versions have already been used to model waiting games (
Friston
et al., 2013 ), two-step maze tasks ( Friston et al., 2015 ), the urn
task and evidence accumulation ( FitzGerald et al., 2015 ), trust
games from behavioral economics ( Moutoussis et al., 2014 ),
addictive behavior ( Schwartenbeck et al., 2015 ) and engineering
benchmarks such as the mountain car problem. It has also been
used in the setting of computational fMRI (
Schwartenbeck et al.,
2014).
Active Inference and Generative Models
Active inference rests upon a generative model of observed
outcomes. This model is used to infer the most likely causes
of outcomes in terms of expectations about states of the
world. These states are called hidden states because they can
only be inferred indirectly through, possibly limited, senso ry
observations. Crucially, observations depend upon action, w hich
requires the generative model to entertain expectations und er
diﬀerent policies or action sequences. Because the model
generates the consequences of sequential action, it has expl icit
representations of the past and future; in other words, it is
equipped with a working memory and expectations about future
(counterfactual) states of the world under competing policie s.
These expectations are optimized by minimizing variational f ree
energy, which renders them (approximately) the most likely
(posterior) expectations about states of the world, given the
current observations.
Expectations or beliefs about the most likely policy are based
upon the prior belief that policies are more likely if they
pursue a trajectory or path that has the least free energy (or
greatest model evidence). As we will see below, this expected
Frontiers in Computational Neuroscience | www.frontiersi n.org 2 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
free energy can be expressed in terms of epistemic and extrinsi c
value, where epistemic value scores the information gain or
reduction in uncertainty about states of the world—and extri nsic
value depends upon prior beliefs about future outcomes. These
prior preferences play the role of utility in economics and
reinforcement learning.
Having evaluated the relative probability of diﬀerent policie s,
expectations under each policy can then be averaged in
proportion to their (posterior) probability. In statistics, th is is
known as Bayesian model averaging . The results of this averaging
specify the next most likely outcome, which determines the
next action. Once an action has been selected, it generates
a new outcome and the (perception and action) cycle starts
again. The resulting behavior is a principled interrogation a nd
sampling of sensory cues that has both epistemic and pragmatic
aspects. Generally, behavior in an ambiguous and uncertain
context is dominated by epistemic drives until there is no fur ther
uncertainty to resolve—and extrinsic value predominates. A t this
point, explorative behavior gives way to exploitative behavio r. In
this paper, we are primarily interested in the epistemic behavio r,
and only use extrinsic value to encourage the agent to report i ts
decision, when it is suﬃciently conﬁdent.
In more detail: expectations about hidden states (and the
precision of beliefs about competing policies) are updated to
minimize variational free energy under a generative model. T he
generative model considered here is fairly generic (see Figure 1).
Outcomes at any particular time depend upon hidden states,
while hidden states evolve in a way that depends upon action.
Formally, this model is speciﬁed by two sets of matrices (stric tly
speaking these are multidimensional arrays). The ﬁrst, A m,
maps from hidden states to the m-th outcome and can embody
ambiguity in the outcomes generated by any particular state. The
m-th sort of outcome here can be considered the m-th modality;
for example, exteroceptive or proprioceptive observations. The
second set of matrices B n(a), prescribe the transitions among
the n-th hidden states, under an action, a. The n-th sort of
hidden state can correspond to diﬀerent factors or attributes of
the world; for example, the location of an object and its identi ty.
The remaining parameters encode prior beliefs about the initi al
states D n, the precision of beliefs about policies γ = 1/β , where
a policy returns an action at a particular time a = π (t) and prior
preferences Cm that deﬁne the expected free energy (see below).
The form of the generative model in Figure 1 means that
outcomes are generated in the following way: ﬁrst, a policy is
selected using a softmax function of expected free energy for each
policy, where the inverse temperature or precision is selected
from a prior (exponential) density. Sequences of hidden states
are then generated using the probability transitions speciﬁe d by
a selected policy. These hidden states then generate outcome s
in several modalities. Figure 2 (left panel) provides a graphical
summary of the dependencies implied by the generative model
in Figure 1. Perception or inference corresponds to inverting or
FIGURE 1 | Formal speciﬁcation of the generative model and (app roximate) posterior. (A) These equations specify the form of the (Markovian) generat ive
model used in this paper. A generative model is essentially a speciﬁcation of the joint probability of outcomes or consequ ences and their (latent or hidden) causes.
Usually, this model is expressed in terms of a likelihood (the probability of consequences given causes) and priors o ver the causes. When a prior depends upon a
random variable it is called an empirical prior. Here, the generative model speciﬁes the mapping between hid den states and observable outcomes in terms of the
likelihood. The priors in this instance pertain to transiti ons among hidden states that depend upon action, where actio ns are determined probabilistically in terms of
policies (sequences of actions). The key aspect of this gene rative model is that, a priori, policies are more probable if they minimize the (path integ ral of) expected free
energy G. Bayesian model inversion refers to the inverse mapping fro m consequences to causes; i.e., estimating the hidden state s and other variables that cause
outcomes. (B) In variational Bayesian inversion, one has to specify the for m of an approximate posterior distribution, which is provid ed on the right panel. This particular
form uses a mean ﬁeld approximation, in which posterior belie fs are approximated by the product of marginals or factors. H ere, a mean ﬁeld approximation is applied
both to posterior beliefs at different points in time and dif ferent sorts of hidden states. See the main text and Table 1 for more detailed explanation of the variables.
Frontiers in Computational Neuroscience | www.frontiersi n.org 3 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 2 | Graphical model corresponding to the generative mod el. (A) The left panel shows the conditional dependencies implied b y the generative model of
previous ﬁgure. Here, the variables in white circles constit ute (hyper) priors, while the blue circles denote random var iables. This format shows how outcomes are
generated from hidden states that evolve according to proba bilistic transitions, which depend on policies. The probab ility of a particular policy being selected depends
upon expected free energy and a precision or inverse tempera ture. (B) The right panels show an example of different hidden states a nd outcomes modalities. This
particular example will be used later to model perceptual ca tegorization in terms of three scenarios or scenes ( ﬂee, feed, or wait). The two outcome modalities
effectively report what is seen and where it is seen. See the m ain text for a more detailed explanation.
ﬁtting this generative model, given a sequence of outcomes. This
corresponds to optimizing the expected hidden states, policies
and precision with respect to variational free energy. These
(posterior) estimates constitute posterior beliefs, usuall y denoted
by the probability distribution Q(x), where x = ˜s, π, γ are the
hidden or unknown variables.
Variational Free Energy and Inference
In variational Bayesian inference, model inversion entail s
minimizing variational free energy with respect to the suﬃci ent
statistics (i.e., parameters) of the posterior beliefs (see Figure 1,
right panel and Table 1 for a glossary of expressions):
Q(x) = arg minQ(x) F
≈ P(x|˜o)
F = EQ[ln Q(x) − ln P(˜o|x) − ln P(x)]
= EQ[ln Q(x) − ln P(x|˜o) − ln P(˜o)]
= D[Q(x)||P(x|˜o)]
 
relative entropy
− ln P(˜o)  
log evidence
= D[Q(x)||P(x)]  
complexity
− EQ[ln P(˜o|x)]  
accuracy
(1)
where ˜o = (o1, . . . , ot) denotes observations up until the current
time.
Remarks
Because the relative entropy (or Kullback-Leibler divergen ce)
cannot be less than zero, the penultimate equality means that free
energy is minimized when the approximate posterior becomes
the true posterior. At this point, the free energy becomes the
negative log evidence for the generative model (
Beal, 2003). This
means minimizing free energy is equivalent to maximizing mo del
evidence, which is equivalent to minimizing the complexity
of accurate explanations for observed outcomes (last equalit y
above).
Minimizing free energy ensures expectations encode posterior
beliefs, given observed outcomes. However, beliefs about po licies
rest on future outcomes. This means that policies should, a priori,
minimize the free energy of beliefs about the future. This ca n be
formalized by making the log probability of a policy proportional
to the free energy expected in the future (
Friston et al., 2015 ):
G(π ) =
∑
τ G(π, τ )
G(π, τ ) = E ˜Q[ln Q(sτ |π ) − ln Q(sτ |oτ , π ) − ln P(oτ )]
Frontiers in Computational Neuroscience | www.frontiersi n.org 4 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
TABLE 1 | Glossary of expressions.
Expression Description
oτ = (o1
τ , . . . , oM
τ ) ∈ { 0, 1}, oτ ∈ [0, 1],
⌢
oτ = ln oτ Outcomes in M modalities, their posterior expectations and logarithms
˜o = (o1, . . . , ot) Sequences of outcomes up until the current time
sτ = (s1
τ , . . . , sN
τ ) ∈ { 0, 1}, sτ ∈ [0, 1],
⌢
s τ = ln sτ Hidden states in N factors, their posterior expectations and logarithms
˜s = (s1, . . . , sT ) Sequences of hidden states up until the end of the current tri al
π = (π1, . . . , π K), π ∈ [0, 1], ⌢π = ln π Policies specifying action sequences, their posterior exp ectations and logarithms
a = π (t) = (a1, . . . , aN) Action or control variables for each factor of hidden states
γ , γ = 1
β The precision (inverse temperature) of beliefs about polic ies and its posterior expectation
β Prior expectation of temperature (inverse precision) of be liefs about policies
Am Likelihood array mapping from hidden states to the m-th modality
Bn(a), Bn,πτ = Bn(a = π (τ )) Transition probability for the n-th hidden state under each action
Cm
τ Logarithm of the prior probability of the m-th outcome; i.e., preferences or utility
Dn Prior expectation of the n-th hidden state at the beginning of each trial
F:Fπ = F(π ) = ∑
τ F(π, τ ) = ∑
n,τ F(π, τ, n) Variational free energy for each policy
G:Gπ = G(π ) = ∑
τ G(π, τ ) = ∑
m,τ G(π, τ, m) Expected free energy for each policy
Hm Entropy of the m-th outcome
A· s• = ∑
i,j,k... Ai,j,k...s1
i s2
j s3
k . . . Dot product (or sum of products), returning a scalar
A· s/n A dot product over all but the n-th vector
= E ˜Q[ln Q(sτ |π ) − ln Q(sτ |oτ , π )]
  
(negative) mutual information
− E ˜Q[ln P(oτ )]
  
expected log evidence
= E ˜Q[ln Q(oτ |π ) − ln Q(oτ |sτ , π )]
  
(negative) epistemic value
− E ˜Q[ln P(oτ )]
  
extrinsic value
= D[Q(oτ |π )||P(oτ )]  
expected cost
+ E ˜Q[H[P(oτ |sτ )]]
  
expected ambiguity
where ˜Q = Q(oτ , sτ |π ) = P(oτ |sτ )Q(sτ |π ) ≈ P(oτ , sτ |˜o, π ) and
Q(oτ |sτ , π ) = P(oτ |sτ ) for τ > t.
Remarks
The expected relative entropy now becomes mutual information
or epistemic value, while the expected log-evidence becomes
extrinsic value—if we associate the prior preferences with va lue
or utility. The ﬁnal equality expression shows how expected
free energy can be evaluated relatively easily: it is just th e
divergence between the predicted and preferred outcomes, plus
the ambiguity (i.e., entropy) expected under predicted states .
There are several helpful interpretations of expected free
energy that appeal to (and contextualize) established constr ucts.
For example, maximizing epistemic value is equivalent to
maximizing (expected) Bayesian surprise (
Itti and Baldi,
2009), where Bayesian surprise is the Kullback-Leibler (KL)
divergence between posterior and prior beliefs. This can also
be interpreted in terms of the principle of maximum mutual
information or minimum redundancy (
Barlow, 1961; Linsker,
1990; Olshausen and Field, 1996; Laughlin, 2001 ). This is
because epistemic value is the mutual information between
hidden states and observations. In other words, it reports
the reduction in uncertainty about hidden states aﬀorded by
observations. Because the information gain cannot be less t han
zero, it disappears when the (predictive) posterior ceases to
be informed by new observations. Heuristically, this means
epistemic behavior will search out observations that resolve
uncertainty about the state of the world (e.g., foraging to r esolve
uncertainty about the hidden location of prey or ﬁxating on
informative part of a face). However, when there is no posterior
uncertainty—and the agent is conﬁdent about the state of the
world—there can be no further information gain and epistemic
value will be the same for all policies, enabling extrinsic val ue
to dominate. This resolution of uncertainty is closely relat ed
to satisfying artiﬁcial curiosity (
Schmidhuber, 1991; Still and
Precup, 2012 ) and speaks to the value of information ( Howard,
1966).
The expected complexity or cost is exactly the same quantity
minimized in risk sensitive or KL control ( Klyubin et al.,
2005; van den Broek et al., 2010 ), and underpins related (free
energy) formulations of bounded rationality based on comple xity
costs ( Braun et al., 2011; Ortega and Braun, 2013 ). In other
words, minimizing expected complexity or cost renders behavio r
risk sensitive, while maximizing expected accuracy induces
ambiguity-sensitive behavior. This completes our descripti on of
free energy. We now turn to belief updating that is based on
minimizing free energy under the generative model describe d
above.
Belief Updating and a Neuronal (Process)
Theory
In practice, expectations about hidden variables can be update d
using a standard gradient descent on variational free energ y.
Figure 3 provides an example of these updates. It is easy to see
that the updates minimize variational free energy because th ey
converge when the free energy gradients are zero: i.e., ∇F = 0.
Although the updates look complicated, they are remarkably
plausible in terms of neurobiological schemes—as discussed
Frontiers in Computational Neuroscience | www.frontiersi n.org 5 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 3 | Schematic overview of the belief updates describin g active inference: (A) The left panel lists the belief updates mediating, percepti on, policy
selection, precision, and action selection; (B) while the right panel assigns the quantities that are update d (sufﬁcient statistics or expectations) to various brain ar eas.
The implicit attribution should not be taken too seriously b ut serves to illustrate the functional anatomy implied by th e form of the belief updates. Here, we have
assigned observed outcomes to visual representations in th e occipital cortex; with exteroceptive ( what) modalities entering a ventral stream and proprioceptive ( where)
modalities originating a dorsal stream. Hidden states enco ding context have been associated with the hippocampal form ation, while the remaining states encoding
sampling location and spatial invariance have been assigne d to the parietal cortex. The evaluation of policies, in term s of their (expected) free energy, has been placed
in the ventral prefrontal cortex. Expectations about polic ies per se and the precision of these beliefs have been associated with striatal and ventral tegmental areas to
indicate a putative role for dopamine in encoding precision . Finally, beliefs about policies are used to create Bayesia n model averages of future outcomes (in the frontal
eye ﬁelds)—that are fulﬁlled by action, via the deep layers of t he superior colliculus. The arrows denote message passing a mong the sufﬁcient statistics of each factor
or marginal. Please see the text and Table 1 for an explanation of the equations and variables. In this pap er, the hat notation denotes a natural logarithm; i.e.,
⌢
o = lno.
elsewhere ( Friston et al., 2014 ). For example, expectations about
hidden states are a softmax function (c.f., neuronal activa tion
function) of two terms. The ﬁrst is a decay term, because the l og
of a probability is always zero or less
⌢
s
n,π
τ = ln sn,π
τ ≤ 0. The
second is the free energy gradient, which is just a linear mix ture
of (spiking) activity from other representations (expectatio ns).
Similarly, the precision updates are a softmax function of fre e
energy and its expected value in the future, weighted by precis ion
or inverse temperature. The expected precision is driven by the
diﬀerence in expected free energy with and without observatio ns;
much like dopamine is driven by the diﬀerence in expected and
observed rewards (
Schultz et al., 1997 ). See Friston et al. (2015)
for further discussion.
The key thing about these updates is that they provide a
process theory that implements the normative theory oﬀered by
active inference. In other words, they constitute speciﬁc proc esses
that make predictions about neuronal dynamics and responses.
Although the focus of this paper is on behavior and large-scale
functional anatomy, we will illustrate the simulated neuro nal
responses associated with active inference in later section s.
Functional Segregation and the Mean Field
Approximation
An important aspect of the belief updating in Figure 3 is that it is
formulated for a particular form of posterior density. This form
rests upon something called a mean ﬁeld approximation , which is
Frontiers in Computational Neuroscience | www.frontiersi n.org 6 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
a ubiquitous device in Bayesian statistics (and statistica l physics)
(Jaakkola and Jordan, 1998 ). Figure 1 (right panel) expresses the
posterior as a product of independent marginal distributions o ver
diﬀerent sorts of hidden states (i.e., factors) at diﬀerent ti me
points. This mean ﬁeld assumption is quite important: it means
hidden states are represented in a compact and parsimonious
fashion. In other words, instead of encoding expectations of a
full joint distribution over several factors (e.g., where a n object
is and what an object is), we just need to represent both attrib utes
in terms of their marginal distributions. Similarly, inste ad of
representing the entire trajectory of hidden states over tim e, we
can approximate the trajectory by encoding expectations at eac h
time point separately. This leads to an enormous simpliﬁcation
of the numerics and belief updating. However, there is a price t o
pay: because the posterior beliefs are conditionally independe nt,
dependencies among the factors are ignored. Generally, this leads
to overconﬁdence, when inferring hidden states—we will see a n
example of this below.
From a neurobiological perspective, the mean ﬁeld
approximation corresponds to the principle of functional
segregation, in which representations are anatomically segregated
in the cortical mantle (
Zeki and Shipp, 1988 ). A nice example
of this is the segregation of ventral and dorsal visual proces sing
streams that deal with “ what” and “ where” attributes of a visual
scene, respectively ( Ungerleider and Mishkin, 1982 ). In the
absence of a mean ﬁeld approximation, there would be neuronal
representations of every possible object in every location. I t
is this aspect of approximate (variational) Bayesian inferenc e
we emphasize in this paper, by sketching the implications for
large-scale functional anatomy. The segregation or factor ization
into “ what” and “ where” attributes is particularly prescient for
the oculomotor control of saccadic eye movements. This is
because—as we will see next—action is only speciﬁed by the
states or attributes of the world that it can change. Clearly,
saccadic eye movements only change where one is looking but
not what is sampled. This means that only one factor or posterio r
marginal is suﬃcient to prescribe action.
For illustrative purposes, Figure 3 shows how the variables
in our scheme could be encoded in the brain. The encoding of
object identity is assigned to inferotemporal cortex (
Seeck et al.,
1995). The representation of location is associated with (dorsal )
extrastriate cortex ( Haxby et al., 1994 ). Beliefs about sampling
locations and spatial invariances are attributed to parietal cortex,
which anticipates the retinal location of stimuli in the futu re and
updates the locations of stimuli sampled in the past ( Duhamel
et al., 1992 ). Inference about scene identity (based on the spatial
relationships among objects) is attributed to the hippocampus
(
Rudy, 2009 ). Beliefs about policies are assigned to the striatum
(Frank, 2011 ), which receives inputs from prefrontal cortex,
ventral tegmental area and hippocampus to coordinate planning
(in prefrontal cortex,
Tanji and Hoshi, 2001 ) and execution
(VTA/SN), given a particular context. In active inference, a ction
selection depends upon the precision of beliefs about policies
(future behavior), encoded by dopaminergic projections from
VTA/SN to the striatum (
Schwartenbeck et al., 2014 ). Frontal
eye ﬁelds are involved in saccade planning ( Srihasam et al.,
2009) and the superior colliculus mediates eye movement control
(Grossberg et al., 1997 )—by fulﬁlling expectations about action
that are conveyed from frontal eye ﬁelds.
Action and Behavior
The equations in Figure 3 conclude with a speciﬁcation of
action, where action is selected to minimize the diﬀerence
(KL divergence) between the outcome predicted under each
action—based on beliefs about the current state—and the
outcome predicted for the next state. This speciﬁcation of
action is considered reﬂexive by analogy to motor reﬂexes
that minimize the discrepancy between proprioceptive signals
(primary aﬀerents) and descending motor commands or
predictions. If we regard competing policies as models of
behavior, the expected outcome is formally equivalent to Bayesian
model average of outcomes, under posterior beliefs about policies.
Summary
By assuming a generic (Markovian) form for the generative
model, it is fairly simple to derive Bayesian updates that clarif y
the interrelationships between perception, policy selection,
precision and action. In brief, the agent ﬁrst infers the hidd en
states under each model or policy that it entertains. It then
evaluates the evidence for each policy based upon prior beliefs or
preferences about future states. Having optimized the conﬁde nce
in beliefs about policies, their expectations are used to form a
Bayesian model average of the next outcome, which is realize d
through action. The anatomy of the implicit message passing is
not inconsistent with functional anatomy in the brain: see
Friston
et al. (2014) and Figure 3. Figure 3 shows the functional anatomy
implied by the belief updating and mean ﬁeld approximation in
Figure 1. Here, we have assumed two input modalities ( what and
where) and four sets of hidden states; one encoding the content
of the visual scene and the remaining three encoding the loca tion
at which it was sampled (and various spatial transformations).
The anatomical designation in Figure 3 should not be taken
too seriously—the purpose of this illustration is to highlight the
recurrent message passing among the expectations that constit ute
beliefs about segregated or factorized states of the world. Here, we
emphasize the segregation between what and where streams—
and how the dorsal where stream supplies predicted outcomes
(to frontal eye ﬁelds) that action can realize (via the superio r
colliculus). The precision of beliefs about policies has been
assigned to dopaminergic projections to the striatum. We will
use this particular architecture in the next section to illus trate
the behavioral (and electrophysiological) responses that eme rge
under this scheme.
Although the generative model—speciﬁed by the ( A,B,C,D)
matrices—changes from application to application, the belief
updates in Figure 3 are generic and can be implemented using
standard software routines (here, spm_MDP_VB_X.m). These
routines are available as Matlab code in the SPM academic
software: http://www.ﬁl.ion.ucl.ac.uk/spm/. In fact, the fo llowing
simulations can be reproduced (and modiﬁed) by downloading
the DEM Toolbox and invoking DEM_demo_MDP_search.m.
This annotated code can also be edited and executed via a
graphical user interface; by typing >> DEM and selecting the
Visual foraging demo. This demo can be compared with the
Frontiers in Computational Neuroscience | www.frontiersi n.org 7 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
equivalent variational ﬁltering scheme (for continuous st ate-
space models) in the Visual search demo, described in Friston
et al. (2012) .
ACTIVE INFERENCE AND VISUAL
FORAGING
This section uses active inference for Markov decision proce sses
to illustrate epistemic foraging in the setting of visual sea rches.
Here, the agent has to categorize a scene on the basis of the
relative position of various visual objects—that are initia lly
unknown. Crucially, the agent can only sample one object or
location at a time and therefore has to accumulate evidence f or
competing hypotheses about the underlying scene. When the
agent is suﬃciently conﬁdent about its perceptual categoriza tion,
it makes a saccade to a choice location—to obtain feedback
(“right” or “ wrong”). A priori , the agent prefers to be right
and does not expect to be wrong. We ﬁrst illustrate a single
trial in terms of behavior and underlying electrophysiologi cal
responses. The next section then considers sequences of tria ls
and how average behavior (accuracy, number of saccades
and saccadic intervals) depends upon prior preferences and
precision.
This demonstration uses a mean ﬁeld approximation to the
posterior over diﬀerent hidden states ( context, sampling location,
and spatial transformations ). In addition, we consider two
outcome modalities (exteroceptive or “ what” and proprioceptive
or “ where”). In this example, the agent has to categorize a
scene that comprises cues at four peripheral locations, starti ng
from a central ﬁxation point. This involves a form of scene
construction, in which the relationship between various cu es
determines the category of scene. The scene always contains
a bird and seed, or a bird and a cat. If the bird is next to
the seed or the cat, then the scene is categorized as “ feed”
or “ ﬂee, ” respectively. Conversely, if the seed is diagonally
opposite the bird, the category is “ wait.” The particular
positions of the cues are irrelevant; the important attribute s
are their spatial relationship. This means hidden states have
to include spatial mappings that induce invariances to spatial
transformations. These are reﬂections around the horizont al and
vertical axes.
The right panel of Figure 2 shows the hidden states in more
detail: there are two outcome modalities ( what and where),
encoding one of six cues ( distractor, seed, bird, cat, and right
or wrong feedback) and one of eight sampled locations (central
ﬁxation, four quadrants, and three choice locations that provide
feedback about the respective decision). The hidden states h ave
four factors; corresponding to context ( feed, ﬂee, and wait),
the currently sampled location (the eight locations above) a nd
two further factors modeling invariance (i.e., with and wit hout
reﬂections about the vertical and horizontal axes). The thr ee
scenes under each context (ﬂee, feed and wait) in the top righ t
panel of Figure 2 are referred to as base scenes. The context or
category deﬁnes the objects (distractor, seed, bird, and ca t) and
their relative locations. The hidden states mediating (ver tical and
horizontal) transformations deﬁne the absolute locations and are
implemented with respect to the base scenes. For example, in
the case of a ﬂee scene, the bird and cat may exchange location s
under a vertical transformation. Since the absolute and rel ative
positions of the objects (and the objects themselves) are hid den
causes of the scene’s appearance, they are not aﬀected by the
agent’s actions.
Heuristically, the model in Figure 2 generates outcomes in the
following way. First, one of the three canonical scenes or con texts
is selected. This scene can be ﬂipped vertically or horizontal ly (or
both) depending upon the spatial transformation states. Final ly,
the sampled location speciﬁes the exteroceptive visual cue and t he
proprioceptive outcome signaling the location. This model can be
speciﬁed in a straightforward way by specifying the two outcom es
for every combination of hidden states in A1 ∈ R6×(3×8×2×2)
and A2 ∈ R8×(3×8×2×2). The arrays in these two matrices just
contain a one for each outcome when the combination of hidden
states generates the appropriate outcome, and zeros elsewhere .
These two matrices encode the observation likelihoods in the two
outcome modalities what and where. Here, A1 deﬁnes the identity
(what) of objects that are likely to be sampled (i.e., observed),
under all possible combinations of hidden states, while A2 deﬁnes
the likely locations ( where) of the objects. The transition matrices
are similarly simple: because the only state that changes is t he
sample location, the transition matrices are identity matric es,
apart from the (action dependent) matrix encoding transition s
among sampled locations:
B2
ij(k) = R(8×8)×8 =
{ 1, i = k
0, i ̸=k
where k ∈ { 1, 2, ..., 8}. Prior beliefs about the initial states Dn
(context and projections) were uniform distributions; apart f rom
the sampled location, which always started at the central ﬁxat ion
D2 = [1, 0, . . . , 0]. Here, n indicates the dimension of the hidden
states with n ∈ {1, 2, 3, 4}. There are four dimensions of hidden
states, namely context, sampling location, and the two spatia l
transformations.
Although we have chosen to illustrate a particular paradigm,
the computational architecture of the scheme is fairly gener ic.
Furthermore, after the generative model has been speciﬁed,
all its parameters are speciﬁed through minimization of free
energy. This means there are only two parameters that can be
adjusted; namely, prior preferences about outcomes, C and prio r
precision, γ . In our case, the agent has no prior preference
(i.e., ﬂat priors) about locations but believes that it will c orrectly
categorize a scene after it has accumulated suﬃcient eviden ce.
Prior preferences over the outcome modalities were therefor e
used to encourage the agent to choose policies that elicited co rrect
feedback C1 = [0, . . . , 0, c, −2c]:c = 2, with no preference for
sampled locations C2 = [0, . . . , 0]. Here, c is the utility of
making a correct categorization, −2c is the utility of being wrong.
These preferences mean that the agent expects to obtain correc t
feedback exp (c) times more than visual cues—and believes it
will solicit incorrect feedback very rarely. The prior precision
of beliefs about behaviors (policies or future actions) γ plays
the role of an inverse temperature parameter. As the precision
increases, the sampling of the next action tends to be more
Frontiers in Computational Neuroscience | www.frontiersi n.org 8 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
deterministic; favoring the policy with the lowest expected f ree
energy. Conversely as the precision of beliefs decreases the
distribution of beliefs over the policies becomes more unifo rm;
i.e., the agent becomes more uncertain about the policy it is
pursuing.
With these preferences, the agent should act to maximize
epistemic value or resolve uncertainty about the unknown
context (the scene and its spatial transformations), until t he
uncertainty about the scene is reduced to a minimum. At this
point, it should maximize extrinsic value by sampling the choic e
location it believes will provide feedback that endorses its beliefs.
This speaks to the trade-oﬀ between exploration and exploitati on.
Essentially, actions associated with exploration of the sce ne
(one of four quadrants) have no extrinsic value—they are
purely epistemic. In contrast, actions associated with the ch oice
locations (locations that are used to report the scene’s cate gory)
have extrinsic value, because the agent has prior preferences
about the consequences of these actions. The contributions of
epistemic and extrinsic value to policy (and subsequent actio n)
selection are determined by their contributions to expected free
energy (see Equation 2). In other words, there is only one
imperative (to minimize free energy); however, free energy c an
always be expressed as a mixture of epistemic and extrinsic
value. The relative contribution is determined by the precis ion
of prior preferences, in relation to the epistemic part. The
exploration and exploitation dilemma is resolved such that wh en
the extrinsic value of the policies associated with a choice
is greater than the epistemic value, the agent terminates the
exploration and exploits one of the choice locations (i.e., de clares
its decision). This reﬂects a general behavioral pattern dur ing
active inference; namely, uncertainty is resolved via mini mizing
a free energy that is initially dominated by epistemic value— until
extrinsic value or prior preferences dominate and exploitatio n
supervenes. Notice that pragmatic behavior (choice behavior ) is
driven by preferences in one modality (exteroceptive outcome s),
while action is driven by predictions in another (proprioceptiv e
sampling location). Despite this, action brings about preferr ed
outcomes. This rests upon the recurrent belief updating that l inks
the “what” and “ where” streams in Figure 3. In this graphic, we
have assumed that proprioceptive information has been passed
from the trigeminal nucleus, via the superior colliculus to v isual
cortex (
Donaldson, 2000).
Simulating Saccadic Searches
Figure 4 shows the results of updating the equations in Figure 3,
using 16 belief updates between each of ﬁve saccades. Beliefs
about hidden states are updated using a gradient descent on
variational free energy. This gradient descent usually con verges
to a minimum within about 16 iterations. We therefore ﬁxed
the number of iterations to 16 for simplicity. This imposes
a temporal scheduling on belief updates and ensures that the
majority (here, more than 80%) of epochs attain convergence
(this convergence can be seen in later ﬁgures, in terms of
simulated electrophysiological responses). The belief updat es
are shown in terms of posterior beliefs about hidden states
(upper left panels), posterior beliefs about action (upper center
panel) and the ensuing behavior (upper right panel). Here,
the agent constructed policies on-the-ﬂy by adding all possib le
actions (saccadic movement to the eight possible locations) to
previous actions. This means that the agent only looks one mov e
ahead—and yet manages to make a correct categorization in
the minimum number of saccadic eye movements: in this trial,
the agent ﬁrst looks to the lower right quadrant and ﬁnds a
distractor (omitted in the ﬁgures for clarity). It then sample s the
upper quadrants to resolve uncertainty about the context, bef ore
soliciting feedback by choosing the (correct) choice locati on.
The progressive resolution of uncertainty over the three ini tial
saccades is shown in more detail in the lower panels.
Here, posterior beliefs about the state of the world (the natu re
of the canonical scene and spatial transformations) are illus trated
graphically by weighting the predicted visual cue—under each
state—in proportion to the posterior beliefs about that state.
Each successive image reports the posterior beliefs after the ﬁ rst
three saccades to the peripheral locations, while the insert in the
center is the visual outcome after each saccade. Initially, all four
peripheral cue locations could contain any of the visual object s;
however, after the ﬁrst saccade to the lower right quadrant, the
agent believes that the objects (bird and seed or cat) are in
the upper quadrants. It then conﬁrms this belief and resolves
uncertainty about vertical reﬂection by sampling the upper rig ht
quadrant to disclose a bird. Finally, to make a deﬁnitive dec ision
about the underlying scene, it has to sample the juxtaposed
location to resolve its uncertainty about whether this cont ains
seed or cat. Having observed cat, it can then make the correct
choice and fulﬁll its prior beliefs or preferences.
This particular example is interesting because it illustrate s
the overconﬁdence associated with a mean ﬁeld approximation.
Note that after the ﬁrst saccade the agent assumes that the
scene must be either a feed or ﬂee category, with no horizontal
reﬂection. This is reﬂected in the fact that the lower quadra nts
are perceived as empty. If the agent was performing exact
Bayesian inference it would allow for the possibility of a wait
scenario, with the bird and seed on the diagonal locations. I n
this instance, it would know that there must have been either
a vertical or horizontal reﬂection (but not both). However,
this knowledge (belief) cannot be entertained under the mea n
ﬁeld approximation, because inferring a vertical or horizonta l
reﬂection depends on whether or not the scene is a wait category.
It is these conditional dependencies that are precluded by the
mean ﬁeld approximation; in other words, posterior beliefs about
one dimension of hidden states (e.g., reﬂection) cannot depe nd
upon posterior beliefs about another (e.g., scene category).
The agent therefore ﬁnds the most plausible explanation for
the current sensory evidence, in the absence of conditional
dependencies; namely, there has been no vertical reﬂection a nd
the scene is not “ wait.” If the brain does indeed use mean ﬁeld
approximations—as suggested by the overwhelming evidence for
functional segregation—one might anticipate similar percept ual
synthesis and saccadic eye movements in human subjects.
In principle, one could compare predictions of empirical eye
movements under active inference schemes with and without
mean ﬁeld approximations—and test the hypothesis that the
brain uses marginal representations of the sort assumed here (see
Discussion).
Frontiers in Computational Neuroscience | www.frontiersi n.org 9 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 4 | Simulated visual search: (A) This panel shows the expectations about hidden states and th e expectations of actions are shown in (B) (upper middle),
producing the search trajectory in (C)—after completion of the last saccadic movement. Expectati ons are shown in image format with black representing 100%
probability. For the hidden states each of the four factors o r marginals are shown separately, with the true states indic ated by cyan dots. Here, there are ﬁve saccades
and the agent represents hidden states generating six outco mes (the initial state and ﬁve subsequent outcomes). The resu lts are shown after completion of the last
saccadic, which means that, retrospectively, the agent bel ieves it started in a ﬂee context, with no horizontal or vertical reﬂection. The seque nce of sampling locations
indicates that the agent ﬁrst interrogated the lower right qu adrant and then emitted saccades to the upper locations to co rrectly infer the scene—and make the
correct choice (indicated by the red label). The lower panel (D) illustrates the beliefs about context during the ﬁrst four sa ccades. Initially, the agent is very uncertain
about the constituents of each peripheral location; howeve r, this uncertainty is progressively resolved through epis temic foraging, based upon the cues that are
elicited by saccades (shown in the central location). The bl ue dots indicate the sampling location after each saccade.
Electrophysiological Correlates of
Variational Belief Updating
Figure 5 shows the belief updating during the above visual search
to emulate electrophysiological responses measured in empiri cal
studies. The upper left panel shows simulated neuronal activit y
(ﬁring rates) for units encoding the ﬁrst (scene category) h idden
state using an image (or raster) format. Units here correspond to
the expectations (posterior probabilities) about hidden stat es of
the world. There are six hidden states for each of the three sc enes
(ﬂee, feed, or wait) at six diﬀerent times. Crucially, there are two
sorts of time shown in these responses. Each block of the raste r
encodes the activity over 16 time bins (belief updates) betwe en
one saccade and the next, with one hidden state in each row. Ea ch
row of blocks reports expectations about one of the three hidde n
states at diﬀerent times in the future (or past)—here, beliefs about
the context following each of the six saccades. Each column o f
blocks shows the expectations (about the past and future) at a
particular point during the trial. This eﬀectively shows the bel iefs
about the hidden states in the past and the future. For example,
the second row of blocks summarizes belief updates about the
second epoch over subsequent saccades; i.e., expectations ab out
the context in the second saccade are updated in the following
saccades (blocks to the right), while the ﬁrst column of bloc ks
encodes beliefs about future states prior to emission of the ﬁ rst
saccade; i.e., expectations about the context in the second t ime
step is projected into the past (one block above) and into the
future (one block below). This means beliefs about the curre nt
state occupy blocks along the leading diagonal (highlighted in
red), while expectations about states in the past and future ar e
above and below the diagonal, respectively. For example, the
color density in the ﬁrst row denotes the posterior probability of
the context being “ﬂee” during the ﬁrst saccade: this expecta tion
about context prior to the ﬁrst saccade only becomes deﬁnitiv e
at around 0.9 s (during the fourth saccade). Conversely, row
12 denotes the posterior probability of ‘wait’ during the four th
saccade: note that this converges to zero before the fourth s accade
has occurred.
This format illustrates the encoding of states over time,
emphasizing the implicit representation of the past and future.
To interpret these responses in relation to empirical results, o ne
can assume that outcomes are sampled every 250 ms (
Srihasam
et al., 2009 ). Note the changes in activity after each new outcome
is observed. For example, the two units encoding the ﬁrst two
hidden states start oﬀ with uniform expectations over the thr ee
scenes that switches after the second and fourth saccade to
eventually encode the expectation that the ﬁrst ( ﬂee) scene is
being sampled. Crucially, by the end of the visual search, thes e
expectations pertain to the past; namely, the context at the star t
Frontiers in Computational Neuroscience | www.frontiersi n.org 10 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 5 | Simulated electrophysiological responses: this ﬁgure reports
the belief updating behind the behavior shown in the previou s ﬁgure. (A) The
upper left panel shows the activity (ﬁring rate) of units enco ding the context or
scene in image (raster) format, over the six intervals betwe en saccades. These
responses are organized such that the upper rows encode the p robability of
alternative states in the ﬁrst epoch, with subsequent epochs in lower rows. (B)
The upper right panel plots the same information to illustra te the evidence
accumulation and the resulting disambiguation of context. (C) The simulated
local ﬁeld potentials for these units (i.e., the rate of chang e of neuronal ﬁring)
are shown in the middle left panel. (D) The middle right panel shows average
local ﬁeld potential over all units before (dotted line) and a fter (solid line)
bandpass ﬁltering at 4 Hz, superimposed upon its time frequen cy
decomposition. (E) The lower panel illustrates simulated dopamine responses
in terms of a mixture of precision and its rate of change.
of the trial. In other words, these memories are based upon
postdiction. Although not illustrated here, this can be very useful
when updating beliefs between trials (when the context does n ot
change).
The upper right panel plots the same information
(expectations about the hidden states) to highlight saltato ry
evidence accumulation, in which expectations diverge as the
search progresses. This belief updating is formally identical
to evidence accumulation described by drift diﬀusion or race -
to-bound models (
de Lafuente et al., 2015; Kira et al., 2015 ).
Furthermore, the separation of timescales implicit in variat ional
updating reproduces the stepping dynamics seen in parietal
responses during decision-making (
Latimer et al., 2015 ). The
middle left panel shows the associated local ﬁeld potentials,
which are simply the rate of change of neuronal ﬁring shown
on the upper right panel. The middle right panel of Figure 5
shows the simulated local ﬁeld potential averaged over all un its
before (dotted line) and after (solid line) bandpass ﬁlterin g at
4 Hz. These responses are superimposed on its time frequency
decomposition. The key observation here is that depolarizatio n
in the theta range coincides with induced responses—a theme
that we pursue elsewhere in terms of theta-gamma coupling in
the brain (
Canolty et al., 2006; Lisman and Redish, 2009; Friston
et al., 2014 ).
The lower panel illustrates simulated dopamine responses in
terms of a mixture of expected precision γ (or equivalently
inverse temperature 1 /β) and its rate of change. Here, we
see a phasic suppression when the null or distractor cue is
sampled after the ﬁrst saccade, followed by a phasic burst
when the bird is seen—and a degree of uncertainty about
policies is resolved. A second burst occurs on the third sacca de,
when the agent resolves uncertainty about the underlying sc ene
(and the decision it will report). Collectively, these simula ted
electrophysiological responses are not dissimilar to the sor ts of
responses recorded in empirical studies; however, in this paper ,
we are primarily interested in modeling (epistemic) behavior.
Figure 5 shows some of the expectations that are updated
using the scheme presented in the left panel of Figure 3. These
simulated electrophysiological responses can be associated with
activity in the various brain regions in Figure 2; i.e., expectations
about hidden states encoding context s1,π
τ with the hippocampus
and the expected precision of beliefs γ with the VTA/SN. In the
ﬁnal section, we consider multiple trials and how performance
depends upon prior preferences and precision.
THE EFFECTS OF PRIOR BELIEFS ON
PERFORMANCE
Figure 6 summarizes the (simulated) behavioral and
physiological responses over 32 successive trials in which
the context (scene and spatial transformations) was selected
at random. Each trial comprises six saccades following an
initial ﬁxation. The ﬁrst panel shows the initial states on ea ch
trial as colored circles for each of the four marginal hidden
states: context, sampling location (always central ﬁxation ﬁ rst),
horizontal, and vertical ﬂips and subsequent policy selectio n (in
image format) over the eight actions (i.e., locations) consi dered at
each saccade. Here, the actions one to ﬁve correspond to visit ing
the central ﬁxation point and quadrants with cues (locations t wo
to ﬁve), whereas, actions six to eight select the locations r eporting
the choice (ﬂee, feed, and wait). Choice locations are just th ere
to enable the agent to report its beliefs about the scene categ ory.
The second panel reports the agent’s decision about the categor y
of the scene and whether this categorization is correct (enc oded
by colored circles) and performance in terms of expected utili ty
and reaction time. Expected utility (black bars) is the utili ty of
the observed outcomes averaged over time. The utility of an
outcome is deﬁned by the prior preference. Note, that because
preferences are log probabilities they are always negative—a nd
the best outcome is zero. The performance measure diﬀers
across trials because the number of saccades the agent employ s
Frontiers in Computational Neuroscience | www.frontiersi n.org 11 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 6 | Simulated responses over 32 trials: this ﬁgure reports the behavioral and (simulated) physiolog ical responses during 32 successive trials. The scenes
in these 32 trials were speciﬁed via randomly selected hidden states of the world. (A) The ﬁrst panel shows the hidden states of the scene (as colored circles) and the
selected action (i.e., the sampled location) on the last sac cade. The y-axis on this panel shows two quantities. The sele cted action is shown using black bars. The
agent can saccade to locations one to eight, where the locati ons six to eight correspond to the choice locations the agent uses to report the scene category. The true
hidden states are shown with colored circles. These specify the objects in the scene and their locations (in terms of the c ontext and spatial transformations). The
second row of cyan dots indicates that the agent always start s exploring a scene from the central ﬁxation point. Individual rows in the y-axis indicate the sampled
locations according to the following: Fix, Fixation; U. Lef t, Upper left; L. Left, Lower Left; U. Right, Upper Right; L. R ight, Lower Right; and Ch. Flee, Choose Flee; Ch.
Feed, Choose Feed; and Ch. Wait, Choose Wait. (B) The second panel reports the ﬁnal outcomes (encoded by colore d circles) and performance measures in terms of
preferred outcomes (utility of observed outcomes), summed over time (black bars) and standardized reaction times (cya n dots). The ﬁnal outcomes are shown for the
sample location (upper row of dots) and outcome (lower row of dots): yellow means the agent made a right choice. (C) The third panel shows a succession of
simulated event related potentials following each outcome . These are taken to be the rate of change of neuronal activity , encoding the expected probability of hidden
states encoding context (i.e., simulated hippocampal acti vity).
before categorizing a scene diﬀers from trial to trial. The re action
times or saccadic intervals (cyan dots) here are based upon th e
actual processing time in the simulations and are shown after
normalization to a mean of zero and standard deviation of one .
Our deﬁnition of reaction time as the actual processing time
(using Matlab tic-toc facility) in the simulations is based upon
the assumption that belief updates in the brain—via neuronal
message passing—follow a similar scheduling to the exchange of
suﬃcient statistics described in Figure 3.
These simulations show that, with the exception of the third
trial, the agent makes veridical decisions on every occasio n.
Interestingly, the third (incorrect) trial is associated w ith the
greatest reaction time. Reaction time here varies because th e
minimization of free energy converges at a certain tolerance
(here, the variational updates terminate when the decrease i n free
energy falls below 1/128). The lower panel shows the simulate d
electrophysiological responses using the same format as in th e
previous ﬁgure. Here, we see bursts of high-frequency activi ty
every 100 ms or so; in other words, a nesting of gamma activity
in the alpha range.
The associated behavior, over the ﬁrst nine trials is depicte d
in Figure 7. Again, with the exception of the third trial, we see
optimal search behavior, with a correct choice after the mini mum
number of saccades. For example, on the ﬁrst trial, the ﬁrst
saccade samples a bird, which just requires a second saccade to
the adjacent location in order to completely disambiguate th e
context. A detailed analysis of the belief updating for the fai led
trial suggested that this was an unlucky failure of the mean
ﬁeld approximation; particularly the factorization over time—
and a partial failure of convergence due to the use of a ﬁxed
number (i.e., 16) of iterations. These sorts of failures hig hlight the
distinction between exact Bayesian inference and approximat e
Bayesian inference that may underlie bounded rationality i n real
agents. With these simulated responses is at hand, we can now
assess the eﬀects of changing prior preference and priors over th e
precision of beliefs about action or policies.
Frontiers in Computational Neuroscience | www.frontiersi n.org 12 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 7 | Sequences of saccades: this ﬁgure illustrates the behavior for
the ﬁrst nine trials shown in the previous ﬁgure using the same f ormat as
Figure 4 (upper right panel). The numbers on the top left in each cell s how the
trial number. With the exception of the third trial, the agen t is able to recognize
or categorize the scene after a small number of epistemicall y efﬁcient
saccades.
Clearly, there are many model parameters (and
hyperparameters) we could consider, in terms of their eﬀects on
simulated behavior. We focused on the precision of preference s
and policies because these correspond intuitively to the diﬀer ent
aspects of salience that may be aberrant in schizophrenia.
Motivational salience can be associated with the preferences
that incentivise choice behavior. Conversely, the precisio n of
beliefs about policies speaks to the visual salience associat ed with
information gain and epistemic value. Heuristically, one mi ght
expect diﬀerent patterns of behavior depending upon whether
subjects have imprecise preferences (i.e., are not conﬁdent
about what to do), as opposed to imprecise beliefs about the
consequences of their actions (i.e., not conﬁdent about how to
do it). In what follows, we address this heuristic using simu lated
behavior.
The Effect of Priors
Finally, Figure 8 reports the performance during presentations
of 300 trials, where hidden states of the world were selected
randomly—and we allowed the agent to make up to 8 saccades.
We measured the performance over these trials in terms
of percent accuracy (a correct choice in the absence of an
incorrect choice), decision time or number of saccades unti l
any (correct or incorrect) choice and reaction time or sacca dic
interval (measured in seconds). Here, we repeated the 300 tri al
paradigm over all combinations of eight levels of prior prefere nce
and precision. To manipulate the precision of preferences, we
increased the parameter c—specifying the prior preferences for
diﬀerent outcomes—from 0 to 4 (i.e., no preferences to very
precise preferences).
The left panel in Figure 8 (Accuracy) shows that accurate
categorization requires both precise preferences and a high
precision. Interestingly, precise prior preferences degrade
accuracy when the prior precision is very low. With greater
prior preference, the agent does not want to make mistakes.
However, a low prior precision precludes a resolution of
uncertainty about the scene. The combination of these two pri ors
discourages the agent from making a choice, resulting in an
incorrect categorization. The trials where agent doesn’t a ttempt
to categorize the scene are considered an incorrect categori zation.
When prior preferences are less precise, the agent is less afraid
of making an incorrect choice, resulting in an improvement
in performance but it is still below the chance level. Similarl y,
greater prior precision does not improve accuracy when prior
preference is low. In short, the agent only respond accurately
when prior preference and precision are high, as seen on the
upper right portion of the image.
The center panel (Decision Time) shows decision time in
terms of number of saccades before choosing a choice location .
When prior preferences are high and prior precision is very low
(ﬁrst column), it takes seven or eight saccades for the agent to
make a decision. Comparing this ﬁgure with the accuracy resul ts,
it can be seen that accuracy is low even though the agent is
making more saccades; i.e., taking its time. When prior precisi on
is high but prior preference is very low, the agent rushes to mak e
a decision—but in the absence of precise prior preferences it
makes mistakes (see left panel). In short, the agent successf ully
categorizes a scene when it deploys three to four saccades (uppe r
right quadrants), under precise preferences and high precision.
The right panel (Reaction Time) shows the reaction time in
terms of actual processing time of the simulations. Although ,
quantitatively, reaction times only vary between about 800 a nd
900 ms, there seems to be a systematic eﬀect of prior precision,
with an increase in reaction time at very low levels.
Crucially, results demonstrate a distinct dependency of
accuracy and decision time on prior preference and prior
precision. This speaks to the possibility of distinct behaviora l
phenotypes that are characterized by diﬀerent combinations
of prior preference and precision. For example, agents who
do not expect themselves to make mistakes may choose
more assiduously, inducing a classical speed accuracy trade- oﬀ.
Conversely, subjects with more precise beliefs about their ch oices
may behave in a more purposeful and deliberate fashion, taking
less time to obtain preferred outcomes. We pursue this theme in
the discussion.
DISCUSSION
In summary, we have presented an active inference formulatio n
of epistemic foraging that provides a framework for
Frontiers in Computational Neuroscience | www.frontiersi n.org 13 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
FIGURE 8 | Performance and priors: this ﬁgure illustrates the average performance over 300 tria ls. (A) The insert (lower panel) shows the prior parameters that
were varied; namely, prior preference and precision. These parameters are varied over eight levels. (B) For each combination, the accuracy, decision and reaction t ime
were evaluated using simulations (upper row). The accuracy is expressed as the percentage of correct trials (deﬁned as a c orrect choice in the absence of a
proceeding or subsequent incorrect choice). Decision time is deﬁned in terms of the number of saccades until a (correct or incorrect) decision. Reaction time or the
interval between saccades is measured in seconds and corres ponds to the actual computation time during the simulations .
understanding the functional anatomy of visual search enta iled
by sequences of saccadic eye movements. This formulation
provides an elementary solution to the problem of scene
construction in the context of active sensing and sequentia l
policy optimization, while incidentally furnishing a model o f
spatial invariance in vision.
Although the problem considered above is relatively simple,
it would confound most existing approaches. For example,
reinforcement learning and optimal control theories are not
applicable because the problem is quintessentially epistemic
(belief-based) in nature. This means that the optimal action
depends on beliefs or uncertainty about hidden states. This
context sensitivity precludes any state-action policy and
implicitly any scheme based on the Bellman optimality principle
(
Bellman, 1952 ). This is because the optimal action from any
state depends upon beliefs about that state and all others.
Although, in principle, a belief-state (partially observed) Ma rkov
decision process could be entertained (
Bonet and Geﬀner,
2014), the combinatorics of formulating beliefs states over
3 × 8 × 2 × 2 = 96 hidden states are daunting. Furthermore,
given the problem calls for sequential policy optimization—
and that ﬁve moves are necessary to guarantee a correct
categorization—one would have to evaluate 8 5 = 32768
policies.
The active inference solution oﬀered here is based upon
minimizing the path-integral of (expected) free energy under
a mean ﬁeld approximation. The exciting thing about this
approach is that, computationally, it operates (nearly) in real -
time. For example, the reaction times in Figure 8 are based
on the actual computation time using a standard desktop
personal computer. This computational eﬃciency may be useful
for neurorobotic applications. Having said this, the primary
motivation for developing this scheme was to characterize
empirical (human) visual searches given observed performanc e,
eye movement, and electrophysiological responses.
The example in this paper has some limitations: for example,
all potential spatial combinations of objects can be obtained using
just two transformations (e.g., the cat can never be below th e
bird), and scenes in larger grid worlds may not be describabl e in
terms of simple transformations from a small number of contex ts.
Clearly, the brain does not use the mean ﬁeld approximation
used to illustrate the scheme—but questions about diﬀerent
forms of meaningful approximations can, in principle, be
answered empirically using Bayesian model comparison of such
approximations when explaining behavioral or neuroimaging
data.
This toy example shows how a scene comprising 2 × 2
quadrants can be explored using the resolution of uncertainty .
Frontiers in Computational Neuroscience | www.frontiersi n.org 14 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
A scene of this small size could be explored systematically, i f
ineﬃciently, (e.g., in a clockwise manner) or by just visiti ng all
locations randomly. However, more complex scenes—which we
hope to use in future work—could not be categorized eﬃciently
in such a fashion. In future work, we intend to expand the
scene in terms of its size and contents, while retaining the
same (active inference) formulation of exploration and ensu ing
categorization. Our hope is to characterize diﬀerent behavio ral
phenotypes, deﬁned in terms of the free parameters of this model;
namely, the prior preferences and precision. This paradigm will
be used to test the aberrant salience hypothesis of schizophre nia.
For that purpose, the experimental design will include task
irrelevant distractors (as opposed to the null cues used above ),
probabilistic relationships between the contents of the scen e and
its category—and a greater number of cue locations. In princi ple,
this will allow us to explain the diﬀerence between normal and
schizotypal visual searches in terms of prior preferences, prio r
precision or a mixture of the two.
Although the accuracy, number of saccades and saccadic
intervals ( Figure 8) provide a degree of validation for active
inference in this setting, it is unlikely that these response s will
provide an eﬃcient estimate of subject-speciﬁc priors, such as
prior preferences and precision. However, it is relatively eas y
to ﬁt the individual saccadic eye movements by evaluating th e
probability of each saccade in relation to posterior beliefs about
action, using the history of action and outcomes in the model
above. This means, in principle, it should be possible to estimat e
things like prior preference and precision eﬃciently, given the
sequence of eye movements from any subject. In subsequent
work, we will use the active inference scheme described in
this paper to explain empirical eye movements in terms of
subject-speciﬁc priors. This enables one to simulate or model
electrophysiological responses or identify the regional cor relates
of belief updating, using functional magnetic resonance
imaging. This speaks to the ultimate aim of this work, which
is to provide a computational phenotyping of individuals, in
the hope of characterizing the (formal or computational)
psychopathology of conditions like addiction and
schizophrenia.
AUTHOR CONTRIBUTIONS
MM, CM, RA conceived the idea. KF, MM, RA, CM contributed
to the writing. KF, MM conducted the simulations.
FUNDING
MM, RA, and KF are members of PACE (Perception and
Action in Complex Environments), an Innovative Training
Network funded by the European Union’s Horizon 2020 research
and innovation programme under the Marie-Sklodowska-Curie
Grant Agreement No 642961. CM is funded by the Max
Planck Society. KF is funded by the Wellcome trust (Ref:
088130/Z/09/Z).
REFERENCES
Andreopoulos, A., and Tsotsos, J. (2013). A computational learning theory of active
object recognition under uncertainty. Int. J. Comput. Vis. 101, 95–142. doi:
10.1007/s11263-012-0551-6
Barlow, H. (1961). “Possible principles underlying the transformations o f sensory
messages, ” inSensory Communication, ed W. Rosenblith (Cambridge, MA: MIT
Press), 217–234.
Beal, M. J. (2003). Variational Algorithms for Approximate Bayesian Inference.
Ph.D. thesis, University College London.
Beedie, S. A., Benson, P. J., and St Clair, D. M. (2011). Atypica l scanpaths
in schizophrenia: evidence of a trait- or state-dependent phenome non?
J. Psychiatry Neurosci. 36, 150–164. doi: 10.1503/jpn.090169
Bellman, R. (1952). On the theory of dynamic programming. Proc. Natl. Acad. Sci.
U.S.A 38, 716–719.
Bonet, B., and Geﬀner, H. (2014). Belief tracking for planning wit h sensing: width,
complexity and approximations. J. Artif. Intell. Res. 50, 923–970. doi: 10.1613/
jair.4475
Braun, D. A., Ortega, P. A., Theodorou, E., and Schaal, S. (2011). “Path integral
control and bounded rationality, ” in 2011 IEEE Symposium on Adaptive
Dynamic Programming and Reinforcement Learning (ADPRL) (Paris: IEEE).
Canolty, R. T., Edwards, E., Dalal, S. S., Soltani, M., Nagarajan, S . S., Kirsch, H. E.,
et al. (2006). High gamma power is phase-locked to theta oscillations i n human
neocortex. Science 313, 1626–1628. doi: 10.1126/science.1128115
de Lafuente, V., Jazayeri, M., and Shadlen, M. N. (2015). Represe ntation of
accumulating evidence for a decision in two parietal areas. J. Neurosci. 35,
4306–4318. doi: 10.1523/JNEUROSCI.2451-14.2015
Donaldson, I. M. (2000). The functions of the proprioceptors of the eye muscles.
Philos. Trans. R. Soc. B Biol. Sci. 355, 1685–1754. doi: 10.1098/rstb.2000.0732
Duhamel, J. R., Colby, C. L., and Goldberg, M. E. (1992). The updati ng of the
representation of visual space in parietal cortex by intended eye mov ements
Science 255, 90–92. doi: 10.1126/science.1553535
FitzGerald, T. H., Schwartenbeck, P., Moutoussis, M., Dolan, R. J ., and Friston,
K. (2015). Active inference, evidence accumulation, and the urn task. Neural
Comput. 27, 306–328. doi: 10.1162/NECO_a_00699
Frank, M. J. (2011). Computational models of motivated action selec tion
in corticostriatal circuits. Curr. Opin. Neurobiol. 21, 381–386 doi:
10.1016/j.conb.2011.02.013
Friston, K., Adams, R. A., Perrinet, L., and Breakspear, M. (2012). Perceptions
as hypotheses: saccades as experiments. Front. Psychol. 3:151. doi:
10.3389/fpsyg.2012.0015
Friston, K., Mattout, J., and Kilner, J. (2011). Action unders tanding and active
inference. Biol. Cybern. 104, 137–160. doi: 10.1007/s00422-011-0424-z
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T ., and Pezzulo, G.
(2015). Active inference and epistemic value. Cogn Neurosci. 6, 187–214. doi:
10.1080/17588928.2015.1020053
Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M. , Behrens, T., and
Dolan, R. J. (2014). The anatomy of choice: dopamine and decision -making.
Philos. Trans. R. Soc. Lond. B Biol. Sci. 369:20130481. doi: 10.1098/rstb.2013.
0481
Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M. , Behrens, T., and
Raymond Dolan, R. J. J. (2013). The anatomy of choice: active inf erence and
agency. Front. Hum. Neurosci. 7:598. doi: 10.3389/fnhum.2013.00598
Grossberg, S., Roberts, K., Aguilar, M., and Bullock, D. (1997). A neural model
of multimodal adaptive saccadic eye movement control by superior collic ulus.
J. Neurosci. 17, 9706–9725.
Hassabis, D., and Maguire, E. A. (2007). Deconstructing episod ic memory with
construction. Trends Cogn. Sci. 11, 299–306. doi: 10.1016/j.tics.2007.05.001
Haxby, J. V., Horwitz, B., Ungerleider, L. G., Maisog, J. M., Pietri ni, P., and
Grady, C. L. (1994). The functional organization of human extra striate cortex:
a PET-rCBF study of selective attention to faces and locations. J. Neurosci. 14,
6336–6353
Howard, R. (1966). Information value theory. IEEE Trans. Syst. Sci. Cybern. 2,
22–26. doi: 10.1109/TSSC.1966.300074
Frontiers in Computational Neuroscience | www.frontiersi n.org 15 June 2016 | Volume 10 | Article 56
Mirza et al. Visual Foraging and Active Inference
Itti, L., and Baldi, P. (2009). Bayesian surprise attracts human at tention. Vision Res.
49, 1295–1306. doi: 10.1016/j.visres.2008.09.007
Jaakkola, T., and Jordan, M. (1998). “Improving the mean ﬁeld approximat ion via
the use of mixture distributions, ” in Learning in Graphical Models , Vol. 89, ed
M. Jordan, (Netherlands: Springer), 163–173.
Kapur, S. (2003). Psychosis as a state of aberrant salience: a frame work
linking biology, phenomenology, and pharmacology in schizophrenia. Am. J.
Psychiatry 160, 13–23. doi: 10.1176/appi.ajp.160.1.13
Kira, S., Yang, T., and Shadlen, M. N. (2015). A neural implementatio n
of Wald’s sequential probability ratio test. Neuron 85, 861–873. doi:
10.1016/j.neuron.2015.01.007
Klyubin, A. S., Polani, D., and Nehaniv, C. L. (2005). “Empowerment : a universal
agent-centric measure of control, ” in Proc. CEC 2005, Vol. 1 (Edinburgh: IEEE),
128–135.
Latimer, K. W., Yates, J. L., Meister, M. L., Huk, A. C., and Pillow , J. W.
(2015). NEURONAL MODELING. Single-trial spike trains in parietal c ortex
reveal discrete steps during decision-making. Science 349, 184–187. doi:
10.1126/science.aaa4056
Laughlin, S. B. (2001). Eﬃciency and complexity in neural coding. Novartis Found.
Symp. 239, 177–187. doi: 10.1002/0470846674.ch14
Linsker, R. (1990). Perceptual neural organization: some approac hes based on
network models and information theory. Annu. Rev. Neurosci. 13, 257–281.
Lisman, J., and Redish, A. D. (2009). Prediction, sequences an d the
hippocampus. Philos. Trans. R. Soc. Lond. B Biol. Sci . 364, 1193–1201.
doi: 10.1098/rstb.2008.0316
Moutoussis, M., Trujillo-Barreto, N. J., El-Deredy, W., Dolan, R. J. , and Friston,
K. J. (2014). A formal model of interpersonal inference. Front. Hum. Neurosci.
8:160. doi: 10.3389/fnhum.2014.00160
Olshausen, B. A., and Field, D. J. (1996). Emergence of simple-cell re ceptive ﬁeld
properties by learning a sparse code for natural images. Nature 381, 607–609.
Ortega, P. A., and Braun, D. A. (2013). Thermodynamics as a theory of decision-
making with information-processing costs. Proc. R. Soc. A 469, 2153. doi:
10.1098/rspa.2012.0683
Pezzulo, G., Rigoli, F., and Chersi, F. (2013). The mixed instrume ntal
controller: using value of information to combine habitual choice
and mental simulation. Front. Psychol. 4:92. doi: 10.3389/fpsyg.2013.
00092
Rudy, J. W. (2009). Context representations, context function s, and the
parahippocampal-hippocampal system. Learn. Mem. 16, 573–585. doi:
10.1101/lm.1494409
Schmidhuber, J. (1991). “Curious model-building control systems, ” in Proc.
International Joint Conference on Neural Networks , Vol. 2, (Singapore: IEEE),
1458–1463.
Schultz, W., Dayan, P., and Montague, P. R. (1997). A neural
substrate of prediction and reward. Science 275, 1593–1599. doi:
10.1126/science.275.5306.1593
Schwartenbeck, P., FitzGerald, T. H., Mathys, C., Dolan, R., and F riston, K. (2014).
The dopaminergic midbrain encodes the expected certainty about de sired
outcomes. Cereb. Cortex 25, 3434–3445. doi: 10.1093/cercor/bhu159
Schwartenbeck, P., FitzGerald, T. H., Mathys, C., Dolan, R., Wurs t, F.,
Kronbichler, M., et al. (2015). Optimal inference with suboptimal mod els:
addiction and active Bayesian inference. Med. Hypotheses 84, 109–117. doi:
10.1016/j.mehy.2014.12.007
Seeck, M., Schomer, D., Mainwaring, N., Ives, J., Dubuisson, D ., Blume, H., et al.
(1995). Selectively distributed processing of visual object reco gnition in the
temporal and frontal lobes of the human brain. Ann. Neurol. 37, 538–545. doi:
10.1002/ana.410370417
Shen, K., Valero, J., Day, G. S., and Paré, M. (2011). Investigat ing the role of the
superior colliculus in active vision with the visual search paradig m. Eur. J.
Neurosci. 33, 2003–2016. doi: 10.1111/j.1460-9568.2011.07722.x
Srihasam, K., Bullock, D., and Grossberg, S. (2009). Target select ion by the
frontal cortex during coordinated saccadic and smooth pursuit eye movements.
J. Cogn. Neurosci. 21, 1611–1627. doi: 10.1162/jocn.2009.21139
Still, S., and Precup, D. (2012). An information-theoretic approach to
curiosity-driven reinforcement learning. Theory Biosci. 131, 139–148. doi:
10.1007/s12064-011-0142-z
Tanji, J., and Hoshi, E. (2001). Behavioral planning in the prefront al cortex Curr.
Opin. Neurobiol. 11, 164–170 doi: 10.1016/S0959-4388(00)00192-6
Ungerleider, L. G., and Mishkin, M. (1982). “Two cortical visual systems, ” in
Analysis of Visual Behavior , eds D. Ingle, M. A. Goodale, and R. J. W. Mansﬁeld
(Cambridge, MA: MIT Press), 549–586.
van den Broek, J. L., Wiegerinck, W. A. J. J., and Kappen, H. J. (2010 ). Risk-sensitive
path integral control. UAI 6, 1–8.
Wurtz, R. H., McAlonan, K., Cavanaugh, J., and Berman, R. A. (2011) .
Thalamic pathways for active vision. Trends Cogn. Sci . 5, 177–184. doi:
10.1016/j.tics.2011.02.004
Zeidman, P., Lutti, A., and Maguire, E. A. (2015). Investigati ng the functions
of subregions within anterior hippocampus. Cortex 73, 240–256. doi:
10.1016/j.cortex.2015.09.002
Zeki, S., and Shipp, S. (1988). The functional logic of cortical c onnections. Nature
335, 311–317.
Conﬂict of Interest Statement: The authors declare that the research was
conducted in the absence of any commercial or ﬁnancial relations hips that could
be construed as a potential conﬂict of interest.
Copyright © 2016 Mirza, Adams, Mathys and Friston. This is an open- access article
distributed under the terms of the Creative Commons Attribu tion License (CC BY).
The use, distribution or reproduction in other forums is perm itted, provided the
original author(s) or licensor are credited and that the ori ginal publication in this
journal is cited, in accordance with accepted academic prac tice. No use, distribution
or reproduction is permitted which does not comply with thes e terms.
Frontiers in Computational Neuroscience | www.frontiersi n.org 16 June 2016 | Volume 10 | Article 56