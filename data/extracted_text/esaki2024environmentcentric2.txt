Environment-Centric Active Inference
Kanako Esaki1, Tadayuki Matsumura1, Takeshi Kato2,
Shunsuke Minusa1, Yang Shao1, and Hiroyuki Mizuno1
1 Hitachi, Ltd., Tokyo, Japan
2 Kyoto University, Kyoto, Japan
Abstract. To handle unintended changes in the environment by agents,
we propose an environment-centric active inference EC-AIF in which the
Markov Blanket of active inference is defined starting from the environ-
ment. In normal active inference, the Markov Blanket is defined starting
from the agent. That is, first the agent was defined as the entity that
performs the â€œactionâ€ such as a robot or a person, then the environment
was defined as other people or objects that are directly affected by the
agentâ€™s â€œaction,â€ and the boundary between the agent and the environ-
ment was defined as the Markov Blanket. This agent-centric definition
does not allow the agent to respond to unintended changes in the en-
vironment caused by factors outside of the defined environment. In the
proposed EC-AIF, there is no entity corresponding to an agent. The
environment includes all observable things, including people and things
conventionally considered to be the environment, as well as entities that
perform â€œactionsâ€ such as robots and people. Accordingly, all states, in-
cluding robots and people, are included in inference targets, eliminating
unintended changes in the environment. The EC-AIF was applied to a
robot arm and validated with an object transport task by the robot arm.
The results showed that the robot arm successfully transported objects
while responding to changes in the target position of the object and to
changes in the orientation of another robot arm.
Keywords: robot Â· MarkovBlanket Â· embodiedagent Â· objecttransport.
1 Introduction
Active inference, which explains the intelligence of living organisms, has in-
creased the intelligence of various agents. According to the free energy principle,
which is the basis of active inference, living organisms minimize their free energy
by either changing their actions for sampling the environment or by changing
their perceptions for inferring environmental states [7,4,14]. The unique feature
of this principle is that changing action, i.e., active inference, is also explained
in a unified manner [6,18]. This sensorimotor contingency, in which perception
and action are treated in a unified manner, is compatible with agents such as
robots because it eliminates actions that are unrelated to perception. Thus, ac-
tive inference has been implemented in various agents and has contributed to
generate intelligent actions [25,3,8,10,12,15,20,23,24,1,2].
arXiv:2408.12777v1  [cs.RO]  23 Aug 2024
2 K. Esaki et al.
Defining the agent and the environment in active inference has been left
to researchers [22,21,16,13,26]. In these studies, especially in systems with an
embodiment such as a robot, the implicit guideline has been to define the robot
as the agent and the surroundings as the environment. Defining the agent and
the environment implies designing Markov Blanket of active inference. As long
as the defined environment is maintained, the agent will outperform the human
in some cases. However, changes in the environment that cannot be directly
changed by the agent or in the presence of other agents can occur. The implicit
guideline for designing Markov Blanket would be unable to respond to such
changes in the environment that the agent does not intend.
Design strategies for Markov Blanket are required that can accommodate
unintended changes in the environment. It is not an implicit guideline, but a
specific strategy that is independent of the system configuration, such as the
robot and its surroundings. What are agents and environments? The design
strategy should answer this essential question. Changes in the environment that
are not intended by the agent originate from assuming an entity corresponding
to the agent (e.g., a robot) and defining its surroundings as the environment.
Instead of assuming the entity corresponding to the agent, considering all of
the world as the environment, the changes in the environment must be under
the intention of the agent. Therefore, defining a Markov Blanket based on the
environment by assuming that everything in the world is the environment would
allow the agent to respond to changes in the environment.
We propose an environment-centric active inference EC-AIF that designs
Markov Blanket based on the environment. The concept of Markov Blanket is
shown in Fig. 1. The conventional Markov Blanket has defined the agent as
the entity that performs the â€œactionâ€, such as a robot or a person, and the
environment as the other people or objects, as shown in Fig. 1(a). The state
of the agent is fully known. In addition, the state of the environment within
the definition is inferred with high precision, but the state of the environment
outside the definition is not inferred at all. The proposed EC-AIF is completely
different, as shown in Fig. 1(b): there is no entity corresponding to an agent. In
addition to the people and objects that have been considered the environment,
the environment includes the entities that perform â€œactions,â€ such as robots and
people. The state of the environment is inferred with varying accuracy. The
EC-AIF was applied to a robot arm and demonstrated in an object transport
task. The robot arm adapted to changes in the environment and successfully
transported the object.
2 Method
2.1 Free Energy Principle and Active Inference
Living organisms are said to follow the Free Energy Principle (FEP). Living
organisms repeat a process of perception and action. Perception is the process
of acquiring an observationo from the environment and inferring a hidden state
s of the environment. Action is the process of inferring the appropriate policyÏ€
Environment-Centric Active Inference 3
(b)(a)
Agent Environment
Inference
level
Agent Environment
No
entity
not inferred Inference
level
Fig. 1.Concept of (a) conventional Markov Blanket (b) Markov Blanket of EC-AIF.
and acting on the environment. The hidden states and policyÏ€ are inferred so
that the free energy becomes smaller, using the generative modelp(o, s, Ï€) of the
environment that living organisms have. One important feature of the FEP is
that actions are also regarded as inferences of policy, i.e., active inference. FEP
handles perception and action as a unified â€œinference.â€
Inference assumes that the categories of perception and action are predefined.
Perception and action are represented by an observationo, a hidden states, and
a policyÏ€, which are treated as probability distributions. Accordingly, defining
the probability variables for each of observationo, hidden states, and policyÏ€
is equivalent to defining the categories of perception and action. For example,
consider the case where the observation variable is the retinal image, the hidden
state variable is the position of the target object, and the policy variable is the
direction of eye movement. In this case, perception is to infer the position of the
target object based on the retinal image, and action is to infer the appropriate
direction to move the eye based on the position of the target object. The set
of these probability variables is called the Markov Blanket. Markov Blanket
determines the categories of perception and action.
2.2 Markov Blanket
Living organisms are considered to adaptively select their Markov Blankets. The
Markov Blanket commonly imagined is the boundary between the living organ-
ismâ€™s body and its surroundings. However, this is not the only Markov Blanket
of a living organism. Living organisms have Markov Blankets as hierarchical
structures, such as the boundary between organs and their surroundings, and
between cells and their surroundings [5,11,17,19]. Depending on the goal, the
appropriate Markov Blanket is selected from among them.
To construct an active inference model for the interpretation of intelligent
actions of living organisms or the generation of intelligent actions of artifacts,
the researcher is required to design a Markov Blanket. For example, in the T-
maze problem [6], which is typical of active inference, an active inference model
is constructed from a rat, which is an agent with actions (movement in the
maze). Observation variables are designed for the two modalities of the ratâ€™s
exteroception (the ratâ€™s position in the maze) and interoception (attraction or
4 K. Esaki et al.
aversion stimuli). In addition, hidden state variables are designed as factors
that may explain the observation variables. The goal of the observation, called
â€œpreference,â€ provides a gradient of expected free energy, leading to appropriate
action. Similarly, when active inference is applied to a robot, a Markov Blanket
is designed starting from the robot, which is an agent with actions. Agent-centric
design of Markov Blankets is useful when active inference is applied to a limited
phenomenon in the world or a limited task assigned to a robot.
Agent-centric design of Markov Blanket does not address outside of the lim-
ited phenomena or tasks. Agents infer hidden states and policies defined in
Markov Blanket. In turn, agents cannot infer hidden states or policies that are
not defined in the Markov Blanket. Generating intelligent actions of robots and
other artifacts are expected to be general-purpose, i.e., to respond to changes
in the environment that are not intended by the agent. The agentâ€™s unintended
changes in the environment are changes in hidden states and policies that involve
the outside of the environment defined by Markov Blanket. Changes in the hid-
den state are caused by the presence of agents other than the target agent. For
example, in the case where another robot is installed in addition to the target
robot, the hidden state is changed by the position, orientation, and movement
of the another robot. Changes in the policy, on the other hand, are caused by a
change to a goal that the target agent cannot directly achieve. For example, in
the case where the target robot performs a certain task, the policy is changed by
a change to a task that cannot be performed by that robot alone. The agent does
not â€œintendâ€ these changes because they involve â€œoutsideâ€ of the environment.
This implies that the agent can respond by incorporating these changes â€œinsideâ€
the environment. This requires a paradigm shift in the design of the Markov
Blanket from agent-centric to environment-centric.
2.3 Environment-Centric Active Inference EC-AIF
We propose an environment-centric active inference, EC-AIF, which designs
Markov Blankets starting from the environment. Fig. 2 shows the decision pro-
cess of observation variableso, hidden state variabless, and policy variablesÏ€ in
EC-AIF. In EC-AIF, thewhat and where that guide modality selection in nor-
mal active inference [18] are applied to the environment. First,where is defined
by considering the environment as the entire observable space. For example, the
observable space is simply divided into a grid, and each grid point is defined as
where1,where2, and so on. Then,what is enumerated from the observable space,
in whichwhere is independently changing. Thewhat includes whatc, which is
connected to a controller like a robot, andwhatnc, which is not connected to a
controller like a ball but thewhere changes due to robot operations. Forwhatc,
those that are connected to a controller are enumerated for each controller. For
whatnc, objects whosewhere changes with robot manipulation are considered
partially independent and thus included. Consequently, a ball that is glued to
the robot and whosewhere always changes with the robot is not included in
whatnc. Observation variableso are then defined by the possible combinations
of what and where:
Environment-Centric Active Inference 5
Define
where
Enumerate
what
Define
Observation
Define hidden
state Define policy
1 2
3 4
Observable
space
Controller
ğ‘¤â„ğ‘ğ‘¡ğ‘
ğ‘¤â„ğ‘ğ‘¡ğ‘›ğ‘
Observable
space
Observable
space
Observable
space
Move to 1
Stop
Fig. 2.Decision process of observation, hidden states, and actions variables in EC-AIF.
o = {owhati | i âˆˆ Z+, iâ‰¤ n}
owhati = {wherej | j âˆˆ Z+, jâ‰¤ m} (1)
where n is the number of what (including whatc and whatnc) and m is the
number ofwhere. After that, the hidden state variabless are determined based
on the observation variableso. The hidden state variabless are defined by the
possible combinations of observation variableso:
s = {(owhat1j1 , owhat2j2 , . . . , owhatnjn ) | j1, j2, . . . , jnâˆˆ Z+, j1, j2, . . . , jnâ‰¤ m}
(2)
where n is the number of what (including whatc and whatnc) and m is the
number of where. Finally, the policy variablesÏ€ are determined based on the
observation variableso. The policy variablesÏ€ are defined for eachwhatc by a
transition towhere and a stop which means not changingwhere:
Ï€ = {Move(whati, wherej), Stop(whati) | i âˆˆ Z+, iâ‰¤ n, jâˆˆ Z+, jâ‰¤ m} (3)
Algorithm 1 shows the process flow in EC-AIF. For eachwhatc, a generative
model is configured, represented by the observationo, hidden states, and policy
Ï€ variables defined in the above decision flow.
3 Results and Discussion
3.1 Experimental Setup
The experiments were conducted with a scene of object transport by a robot.
The object is mainly transferred by Universal Robotsâ€™ 6-axis robot arm UR5e
and Robotiqâ€™s adaptive gripper 2F-140 attached to the end of the UR5e shown
in Fig. 3(a). In this scene, the world is composed of the robot arm UR5e and
6 K. Esaki et al.
Algorithm 1Process flow of EC-AIF
1: for allwhatc do
2: Create generative model p(o, s, Ï€)
3: end for
4: Acquire initial observationo0
5: for Ï„ = 0 to TimestepsT do
6: for allwhatc do
7: Infer state sÏ„
8: Infer policy Ï€Ï„
9: Choose action aÏ„
10: if whatc in aÏ„ then
11: Convert action aÏ„ to control valueuÏ„
12: Send control value uÏ„ to controller
13: Break
14: end if
15: end for
16: end for
DENSOWAVEâ€™s 6-axis robotarmCOBOTTAshowninFig. 3(b),andthe target
object placed around UR5e. Accordingly,what in this scene is as follows:
what = {UR5e, COBOTTA, target object} (4)
Furthermore,where is the grid points P1 to P15, including UO, the origin posi-
tion of UR5e, CO, the origin position of COBOTTA, and Int., the intermediate
position between the two robots.
(b)(a)
Fig. 3.Robots in Experiment (a) UR5e (b) COBOTTA.
We evaluated the proposed method in two scenarios that capture changes in
the environment in object transport:
Environment-Centric Active Inference 7
Scenario 1: The target position for object transport changes.
Scenario 2: Another robotâ€™s orientation changes.
The first scenario is an example of change in the environment that cannot be
directly changed by the agent: the target position for object transport changes.
Specifically, as shown in Fig. 4(a), the target position is initially in front of robot
arm UR5e (P12) and then changes to the side of robot arm COBOTTA (P5).
Before the target position changes, the target position is within the reach of
UR5e, allowing object transport by UR5e by itself. After the target position
changes, the target position is outside the reach of UR5e and within the reach
of COBOTTA, requiring UR5e to cooperate with COBOTTA to transport the
object. The second scenario is an example of a change in the environment where
there is another robot, and the target position for object transport remains the
same, but the orientation of the other robot changes. Specifically, as shown in
Fig. 4(b), the target position is in front of COBOTTA (P14), and COBOTTA
initially faces the other direction from the target position, and then changes to
thedirectionofthetargetposition.BeforetheorientationofCOBOTTAchanges,
UR5e does not have contact with COBOTTA when it places an object at the
target position, allowing UR5e to transport the object on its own. After the
COBOTTAâ€™s orientation changes, UR5e requires cooperation with COBOTTA
to transport objects because UR5e will have contact with COBOTTA when
UR5e places objects at the target position. In both scenarios, the initial position
of the target object was P7. The target position was given as the preference
regarding the observation of the target object.
P7
P12
P8
P13
P9
P14
P10
P15
P5
P6
P11
P1
(a)
UR5e
UO Int. CO
COBOTTA
Target
(before)
Target
(after)
P7
P12
P8
P13
P9
P14
P10
P15
P5
P6
P11
P1
UR5e
UO Int. CO
COBOTTA
Target
Arm 
direction
(before)
Arm direction
(after)
Initial Initial
(b)
Fig. 4.Scenariosof(a)thetargetpositionforobjecttransportchanges,and(b)another
robotâ€™s orientation changes. Each circle represents a grid point comprisingwhere. UO
is the origin position of UR5e, CO is the origin position of COBOTTA, and Int. is
the intermediate position between UR5e and COBOTTA. The light blue circles (P1,
UO, P6, P7, P8, P11, P12, P13) are grid points in the reach range of UR5e, the blue
circles (CO, P5, P10, P15) are grid points in the reach range of COBOTTA, and the
purple circles (Int., P9, P14) are grid points in the reach range of both robots. In the
experiment, the grid points are not evenly distributed, and the position of the points
is shifted due to the constraints of the mechanism in which the robot is installed.
8 K. Esaki et al.
The proposed EC-AIF and the normal AIF as a benchmark are applied to
both robot arms UR5e and COBOTTA. Both EC-AIF and AIF are implemented
using pymdp [9], an active inference OSS. The output of the actions is passed to
the robot control in the form of the target position and orientation of the robot
hand. The robot control used ROS melodic, a robot OSS installed on Ubuntu
18.04.
3.2 Change in Target Position for Object Transport
When the target position is within the reach of UR5e, the path of object trans-
port to the target position by UR5e was obtained in both cases where normal
AIF was applied and where EC-AIF was applied. Fig. 5 shows the transition
of the selected action when the target position is within the reach of UR5e. In
both cases, in timestep 1, an action to move UR5e to P7, where the object was
placed, was selected, and then in timestep 2, an action to place the object at the
target position, P12 by UR5e, was selected. Fig. 6 also shows the total number of
observations of the object when the target position is within the reach of UR5e.
In both cases, where normal AIF was applied and where EC-AIF was applied,
the values of the objectâ€™s starting position (P7) and P12 were relatively higher
than the other positions, and a route was chosen to transport the object directly
from P7 to P12.
(b)
(a)
1
2
3
1.0
0.8
0.6
0.4
0.2
0.0
Value
UStop
UP14
UP13
UP12
UP11
UP9
UP8
UP7
UP6
UP1
UInt.
Timestep
CStop
CP15
CP14
CP10
CP9
CP5
CInt.
Ustop
UP14
UP13
UP12
UP11
UP9
UP8
UP7
UP6
UP1
UInt.
1
2
3Timestep
Action
Action
Notation of action
U[X]:   ğ‘€ğ‘œğ‘£ğ‘’ ğ‘ˆğ‘…5ğ‘’,ğ‘‹
UStop: ğ‘†ğ‘¡ğ‘œğ‘ ğ‘ˆğ‘…5ğ‘’
C[X]:   ğ‘€ğ‘œğ‘£ğ‘’ ğ¶ğ‘‚ğµğ‘‚ğ‘‡ğ‘‡ğ´,ğ‘‹
CStop: ğ‘†ğ‘¡ğ‘œğ‘ ğ¶ğ‘‚ğµğ‘‚ğ‘‡ğ‘‡ğ´
Fig. 5.Transition of the selected action when the target position is within the reach
of UR5e by (a) normal AIF and (b) EC-AIF.
Environment-Centric Active Inference 9
(b)(a)
2
1
0
Value
P1 UO Int. CO P5
P6 P7 P8 P9 P10
P11 P12 P13 P14 P15
P1 UO Int. CO P5
P6 P7 P8 P9 P10
P11 P12 P13 P14 P15
Fig. 6.Total number of observations of the object when the target position is within
the reach of UR5e by (a) normal AIF and (b) EC-AIF. Each cell in the heat map
corresponds towhere of the object. The value of each cell is the total number of times
the object was placed at the corresponding cellâ€™s position.
After the target position changed outside the reach of UR5e, differences were
observed between cases where normal AIF was applied and cases where EC-
AIF was applied. Fig. 7 shows the transition of the selected action when the
target position is outside the reach of UR5e. In the case where normal AIF was
applied, various actions of UR5e were selected at all timesteps and no consistent
sequence of actions was observed. In the case where EC-AIF was applied, on
the other hand, in timestep 1, the action of UR5e moving to P7, where the
object was placed, was selected, followed by the action of UR5e transporting
the object to the intermediate position between UR5e and COBOTTA (Int.) in
timestep 2. Furthermore, in timestep 3, after the action of COBOTTA moving
to the intermediate position (Int.) was selected, the action of COBOTTA placing
the object to the target position, P5, was selected. Fig. 8 also shows the total
number of observations of the object when the target position is outside the
reach of UR5e. In the case where normal AIF was applied, only the value of P7,
which is the initial position of the target object, is higher. In the case where EC-
AIF was applied, on the other hand, the values of the objectâ€™s starting position
(P7), the intermediate position (Int.), and the objectâ€™s target position (P5) were
relatively higher than the other positions, and the shortest path was chosen for
UR5e and COBOTTA to cooperatively transport from P7 to P5. Thus, the robot
applying EC-AIF was able to respond to changes in target position.
The results suggest that EC-AIF can be used for environments that cannot
be changed directly by the agent. In normal AIF, the environment is defined
around UR5e, i.e. within the reach of UR5e. As long as the target position is
within the reach of UR5e, UR5e can transport objects. In normal AIF, actions
can be selected more quickly because there are fewer action variables than in
EC-AIF. Once the target position is outside the reach of UR5e for some reason
of the robot user, however, UR5e will not be able to determine how to transport
the object. This is because the target position is now outside of the environment
and no preferences regarding the objectâ€™s position are defined. In EC-AIF, the
environment is independent of the reach range of UR5e. Therefore, even if the
target position is outside the reach range of UR5e, the actions of UR5e can be
10 K. Esaki et al.
(b)
(a)
1.0
0.8
0.6
0.4
0.2
0.0
Value
1
2
3
4
5 Timestep
1
2
3
4
5 Timestep
UStop
UP14
UP13
UP12
UP11
UP9
UP8
UP7
UP6
UP1
UInt.
CStop
CP15
CP14
CP10
CP9
CP5
CInt.
Ustop
UP14
UP13
UP12
UP11
UP9
UP8
UP7
UP6
UP1
UInt.
Action
Action
Notation of action
U[X]:   ğ‘€ğ‘œğ‘£ğ‘’ ğ‘ˆğ‘…5ğ‘’,ğ‘‹
UStop: ğ‘†ğ‘¡ğ‘œğ‘ ğ‘ˆğ‘…5ğ‘’
C[X]:   ğ‘€ğ‘œğ‘£ğ‘’ ğ¶ğ‘‚ğµğ‘‚ğ‘‡ğ‘‡ğ´,ğ‘‹
CStop: ğ‘†ğ‘¡ğ‘œğ‘ ğ¶ğ‘‚ğµğ‘‚ğ‘‡ğ‘‡ğ´
Fig. 7.Transition of the selected action when the target position is outside the reach
of UR5e by (a) normal AIF and (b) EC-AIF.
appropriately selected by only changing the preferences regarding the objectâ€™s
position.
The shortest path of object transport indicates that the principle of least
action is implicitly included in the active inference. There are various paths
other than the path selected here for object transport from the starting position
P7 to the target positions P12 and P5. Nevertheless, the robot following active
inference transported the object using the shortest path. The agent that follows
active inference acts to minimize surprises to the environment. This means that
this agent minimizes the surprise to the result of the motion of the object that
follows the principle of least action. Thus, active inference naturally includes the
principle of least action.
3.3 Change in Another Robotâ€™s Orientation
The EC-AIF allowed the object transport route to be adjusted to the direction
in which COBOTTA was facing. Fig. 9 shows the motion sequences and the
object transport routes when COBOTTA is facing in a different direction from
the target position and when it is facing in the direction of the target position.
Environment-Centric Active Inference 11
(b)(a)
2
1
0
Value
P1 UO Int. CO P5
P6 P7 P8 P9 P10
P11 P12 P13 P14 P15
P1 UO Int. CO P5
P6 P7 P8 P9 P10
P11 P12 P13 P14 P15
Fig. 8.Total number of observations of the object when the target position is outside
the reach of UR5e by (a) normal AIF and (b) EC-AIF. Each cell in the heat map
corresponds towhere of the object. The value of each cell is the total number of times
the object was placed at the corresponding cellâ€™s position.
When COBOTTA was facing the other direction from the target position, the
object was directly transported from the start position to the target position.
In contrast, when COBOTTA was facing the direction of the target position,
the objectâ€™s route changed. Specifically, the object was transported from the
start position to the target position via the intermediate position (Int.). While
there was no possibility of contact between UR5e and COBOTTA, the object
was transported on the shortest route, and as the possibility of contact between
UR5e and COBOTTA increased, the object was transported on a detour path
to avoid contact.
(b)
(a)
P1 UO Int. CO P5
P6 P7 P8 P9 P10
P11 P12 P13 P14 P15
P1 UO Int. CO P5
P6 P7 P8 P9 P10
P11 P12 P13 P14 P15
UR5e move
to P7
UR5e move
to P14
UR5e move
to P7
UR5e move
to Int.
COBOTTA
move to Int.
COBOTTA
move to P14
Fig. 9.Motion sequences and object transport routes (a) when COBOTTA is facing in
a different direction from the target position and (b) when it is facing in the direction
of the target position.
12 K. Esaki et al.
The result suggests that the EC-AIF is capable of adapting to changes in
the environment in which another robot is present. Both the starting and tar-
get positions were within the reach of UR5e. Accordingly, in a situation where
there were no obstacles, including another robot, objects were transported from
the starting position to the target position through the shortest path. However,
when the situation changed and COBOTTA was facing the direction of the tar-
get position, COBOTTA became an obstacle in the object transport path by
UR5e. As a result, even if UR5e chooses an action to transport the object to
the target position, the corresponding trajectory of UR5e would not be gener-
ated. Consequently, the action via the intermediate position (Int.), which is the
shortest path while avoiding the obstacle, was selected.
4 Conclusion
To handle unintended changes in the environment by agents, we proposed an
environment-centered active inference EC-AIF that defines the Markov Blanket
of active inference from the environment. In ordinary active inference, the envi-
ronment is defined from the starting point of an agent that performs â€œactions,â€
such as a robot or a person, and the agent cannot respond to unintended changes
in the environment caused by factors other than the defined environment. In the
proposed EC-AIF, the environment is defined as the starting point, and there
is no entity equivalent to an agent. Therefore, all states, including robots and
people, are included in the inference target and unintended changes in the en-
vironment can be eliminated EC-AIF was applied to a robot arm and verified
in an object transport task by a robot arm. The results showed that the robot
arm successfully transported objects while responding to changes in the target
position of the object and changes in the posture of other robot arms. In future
work, the design of the generative model will be further refined by consider-
ing the mechanism by which each robotâ€™s preferences are transferred between
robots by extending the change in the robotâ€™s trajectory due to another robotâ€™s
orientation.
References
1. Esaki, K., Matsumura, T., Ito, K., Mizuno, H.: Sensorimotor visual perception
on embodied system using free energy principle. In: Machine Learning and Prin-
ciples and Practice of Knowledge Discovery in Databases. pp. 865â€“877. Springer
International Publishing, Cham (2021)
2. Esaki, K., Matsumura, T., Minusa, S., Shao, Y., Yoshimura, C., Mizuno, H.: Dy-
namicalperception-actionloopformationwithdevelopmentalembodimentforhier-
archical active inference. In: Active Inference. pp. 14â€“28. Springer Nature Switzer-
land, Cham (2024)
3. Fountas, Z., Sajid, N., Mediano, P., Friston, K.: Deep active inference agents using
monte-carlo methods. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp.
Environment-Centric Active Inference 13
11662â€“11675. Curran Associates, Inc. (2020),https://proceedings.neurips.cc/
paper/2020/file/865dfbde8a344b44095495f3591f7407-Paper.pdf
4. Friston, K.: The free-energy principle: a unified brain theory? Nature reviews neu-
roscience 11(2), 127â€“138 (2010)
5. Friston, K.: A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184 (2019)
6. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active In-
ference: A Process Theory. Neural Computation29(1), 1â€“49 (01 2017). https:
//doi.org/10.1162/NECO_a_00912, https://doi.org/10.1162/NECO_a_00912
7. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain.
Journal of Physiology-Paris 100(1), 70â€“87 (2006). https://doi.org/https:
//doi.org/10.1016/j.jphysparis.2006.10.001, https://www.sciencedirect.
com/science/article/pii/S092842570600060X, theoretical and Computational
Neuroscience: Understanding Brain Functions
8. Friston, K.J., Parr, T., Yufik, Y., Sajid, N., Price, C.J., Holmes, E.: Gen-
erative models, linguistic communication and active inference. Neuroscience
& Biobehavioral Reviews 118, 42â€“64 (2020). https://doi.org/https:
//doi.org/10.1016/j.neubiorev.2020.07.005, https://www.sciencedirect.
com/science/article/pii/S0149763420304668
9. Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I.D., Tschantz,
A.: pymdp: A python library for active inference in discrete state spaces. Journal
of Open Source Software7(73), 4098 (2022). https://doi.org/10.21105/joss.
04098, https://doi.org/10.21105/joss.04098
10. Horii, T., Nagai, Y.: Active inference through energy minimization in
multimodal affective humanâ€“robot interaction. Frontiers in Robotics and
AI 8 (2021). https://doi.org/10.3389/frobt.2021.684401, https://www.
frontiersin.org/articles/10.3389/frobt.2021.684401
11. Kirchhoff, M., Parr, T., Palacios, E., Friston, K., Kiverstein, J.: The markov
blankets of life: autonomy, active inference and the free energy principle. Jour-
nal of The Royal Society Interface15(138), 20170792 (2018).https://doi.org/
10.1098/rsif.2017.0792
12. Lanillos, P., Meo, C., Pezzato, C., Meera, A.A., Baioumy, M., Ohata, W., Tschantz,
A., Millidge, B., Wisse, M., Buckley, C.L., Tani, J.: Active inference in robotics
and artificial agents: Survey and challenges (2021)
13. Van de Maele, T., Dhoedt, B., Verbelen, T., Pezzulo, G.: Integrating cognitive map
learning and active inference for planning in ambiguous environments. In: Buckley,
C.L., Cialfi, D., Lanillos, P., Ramstead, M., Sajid, N., Shimazaki, H., Verbelen,
T., Wisse, M. (eds.) Active Inference. pp. 204â€“217. Springer Nature Switzerland,
Cham (2024)
14. McGregor, S., Baltieri, M., Buckley, C.L.: A minimal active inference agent. arXiv
preprint arXiv:1503.04187 (2015)
15. Millidge, B.: Deep active inference as variational policy gradients. Journal of Math-
ematical Psychology96, 102348 (2020).https://doi.org/10.1016/j.jmp.2020.
102348
16. Oliver, G., Lanillos, P., Cheng, G.: An empirical study of active inference on a
humanoid robot. IEEE Transactions on Cognitive and Developmental Systems
14(2), 462â€“471 (2022).https://doi.org/10.1109/TCDS.2021.3049907
17. Palacios, E.R., Razi, A., Parr, T., Kirchhoff, M., Friston, K.: On markov blan-
kets and hierarchical self-organisation. Journal of Theoretical Biology486, 110089
(2020). https://doi.org/10.1016/j.jtbi.2019.110089
14 K. Esaki et al.
18. Parr, T., Pezzulo, G., Friston, K.J.: Active inference: the free energy principle in
mind, brain, and behavior. MIT Press (2022)
19. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible in-
ference. Morgan kaufmann (1988)
20. Pezzato, C., Corbato, C.H., Bonhof, S., Wisse, M.: Active inference and behavior
trees for reactive action planning and execution in robotics. IEEE Transactions on
Robotics 39(2), 1050â€“1069 (2023).https://doi.org/10.1109/TRO.2022.3226144
21. Pio-Lopez, L., Nizard, A., Friston, K., Pezzulo, G.: Active inference and
robot control: a case study. Journal of The Royal Society Interface
13(122), 20160616 (2016). https://doi.org/10.1098/rsif.2016.0616, https:
//royalsocietypublishing.org/doi/abs/10.1098/rsif.2016.0616
22. Priorelli, M., Stoianov, I.P.: Efficient motor learning through action-perception
cycles in deep kinematic inference. In: Buckley, C.L., Cialfi, D., Lanillos, P., Ram-
stead, M., Sajid, N., Shimazaki, H., Verbelen, T., Wisse, M. (eds.) Active Inference.
pp. 59â€“70. Springer Nature Switzerland, Cham (2024)
23. Sajid, N., Ball, P.J., Parr, T., Friston, K.J.: Active Inference: Demystified and
Compared. Neural Computation33(3), 674â€“712 (03 2021).https://doi.org/10.
1162/neco_a_01357, https://doi.org/10.1162/neco_a_01357
24. UeltzhÃ¶ffer, K.: Deep active inference. Biological cybernetics 112(6), 547â€“573
(2018). https://doi.org/10.1007/s00422-018-0785-7
25. Ã‡atal, O., Verbelen, T., Nauta, J., Boom, C.D., Dhoedt, B.: Learning perception
and planning with deep active inference. In: ICASSP 2020 - 2020 IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing (ICASSP). pp.
3952â€“3956 (2020).https://doi.org/10.1109/ICASSP40776.2020.9054364
26. Ã‡atal, O., Wauthier, S., De Boom, C., Verbelen, T., Dhoedt, B.: Learning gener-
ative state space models for active inference. Frontiers in Computational Neuro-
science 14 (2020). https://doi.org/10.3389/fncom.2020.574372, https://www.
frontiersin.org/articles/10.3389/fncom.2020.574372