1
Adaptive Active Inference Agents for Heterogeneous and Lifelong
Federated Learning
Anastasiya Danilenka , Alireza Furutanpey , Victor Casamayor Pujol , Boris Sedlak , Anna Lackinger ,
Maria Ganzha , Marcin Paprzycki , Schahram Dustdar , Fellow, IEEE
This work has been submitted to the IEEE for possible publication. Copyright may be transferred
without notice, after which this version may no longer be accessible.
Abstract—Handling heterogeneity and unpredictability are two
core problems in pervasive computing. The challenge is to seam-
lessly integrate devices with varying computational resources in
a dynamic environment to form a cohesive system that can fulfill
the needs of all participants. Existing work on adaptive systems
typically focuses on optimizing individual variables or low-level
Service Level Objectives (SLOs), such as constraining the usage
of specific resources. While low-level control mechanisms permit
fine-grained control over a system, they introduce considerable
complexity, particularly in dynamic environments. To this end, we
propose drawing from Active Inference (AIF), a neuroscientific
framework for designing adaptive agents. Specifically, we intro-
duce a conceptual agent for heterogeneous pervasive systems that
permits setting global systems constraints as high-level SLOs.
Instead of manually setting low-level SLOs, the system finds
an equilibrium that can adapt to environmental changes. We
demonstrate the viability of our AIF agents with an extensive
experiment design, using heterogeneous and lifelong federated
learning as an application scenario. We conduct our experiments
on a physical testbed of devices with different resource types and
vendor specifications. The results provide convincing evidence
that an AIF agent can adapt a system to environmental changes.
In particular, the AIF agent can balance competing SLOs
in resource heterogeneous environments to ensure up to 98%
fulfillment rate.
Index Terms—Adaptive Computing, Service Level Objectives,
Active Inference, Federated Learning, Edge Computing.
I. INTRODUCTION
The Distributed Computing Continuum is an emerging
paradigm for systems that can seamlessly integrate multiple
layers of computing infrastructure [1]. Computing contin-
uum systems promise to enable infrastructure-critical perva-
sive applications with stringent requirements, such as Mobile
Augmented Reality (MAR) for cognitive applications [2]
and remote sensing for disaster management [3]. There are
three recurrent characteristics among pervasive applications
deployed on a continuum. First, is their reliance on AI-based
methods for tasks that classical control structures cannot solve
efficiently or with sufficient precision [4]. For example, MAR
*Corresponding author: Anastasiya Danilenka
Anastasiya Danilenka and Maria Ganzha are with Faculty of Mathematics
and Information Science, Warsaw University of Technology (email: anas-
tasiya.danilenka.dokt@pw.edu.pl, maria.ganzha@pw.edu.pl)
Alireza Furutanpey, Boris Sedlak, Anna Lackinger, and Schahram
Dustdar are with Distributed Systems Group, TU Wien (email:
a.furutanpey@dsg.tuwien.ac.at, boris.sedlak@dsg.tuwien.ac.at,
a.lackinger@dsg.tuwien.ac.at, dustdar@dsg.tuwien.ac.at)
Victor Casamayor Pujol is with Distributed Intelligence and
Systems-Engineering Lab, Universitat Pompeu Fabra (email:
victor.casamayor@upf.edu)
Marcin Paprzycki is with Systems Research Institute, Polish Academy of
Sciences (email: paprzyck@ibspan.waw.pl)
applications must process streams of high-dimensional data
that a service could ideally process at the source to fulfill
a sub-10 millisecond latency Service Level Objective (SLO).
The caveat is that resources in proximity are constrained.
Typical solutions involve task partitioning and lightweight
data reduction methods that minimize the penalty for of-
floading to remote resources [5]. The second characteristic is
heterogeneity, i.e., resource-asymmetry, vendor specifications,
and usage patterns. Although pervasive applications follow
an overall common objective, a system must consider the
individual properties and objectives of participants. Third,
is the continuously drifting problem domain intrinsic to the
dynamic environments of pervasive applications, such that
the source distribution drifts over time and data volume is
non-static. Conclusively, a necessary precondition is a system
that can adapt to non-identically and independently distributed
(non-IID) data. Moreover, the system must facilitate collab-
oration between heterogeneous devices to fulfill their SLOs
by distributing workload fairly and considering the individual
properties of participants.
The focus of this paper is on lifelong heterogeneous feder-
ated learning (FL) as we find it best encapsulates the primary
challenges of pervasive applications that share the described
characteristics. In general, FL participants collaborate for a
common objective, i.e., to maximize the prediction perfor-
mance. Yet, each participant has a private local validation
set to determine whether their criteria are locally met. Time
constraints that ensure smooth operations and resource asym-
metry further instigate friction when attempting to satisfy local
objectives. Hence, despite a common objective, to fulfill the
SLOs of each participant individually, a delicate balance is
necessary. Lastly, the dynamic environment gradually drifts
the distribution and varies the data volume.
This work aims to demonstrate the viability of Active In-
ference (AIF) in designing adaptive agents that can gracefully
handle the challenging requirements of pervasive applications.
While AIF is a neuroscientific framework, recent work has
shown promising results by conceiving methods from the
underlying ideas for workload scheduling in distributed sys-
tems [6], [7]. In particular, we find that the objectives of Active
Inference and pervasive computing intrinsically intertwine.
Context awareness is crucial for pervasive applications as these
systems operate in dynamic environments and must adapt to
changes in their surroundings. Precisely context awareness is a
defining characteristic of AIF agents. However, the current ap-
plication of AIF for systems is more conceptual and only par-
tially implements the core components of the AIF framework.
Other work on systems that adapt to changing requirements
arXiv:2410.09099v2  [cs.LG]  8 Mar 2025
2
typically focuses on optimizing individual variables, such as
learning rate or setting low-level SLOs as constraints on
specific resources [8], [9]. Despite providing more fine-grained
control, it is unreasonable to expect application developers to
understand the implications of each low-level constraint to the
overall system, particularly in dynamic environments where
resources are scarce and availability is less predictable. In
contrast, our AIF agent permits setting high-level SLO targets
to find an equilibrium, defined as the system configuration that
fulfills all its SLOs, without attempting to enforce constraints
from possibly conflicting low-level SLOs.
We design experiments that accurately reflect the relevant
real-world conditions by implementing a physical testbed con-
sisting of heterogeneous devices with varying resource types
and computational capabilities. Additionally, we leverage a
controlled process for data generation to evaluate the adapt-
ability to a dynamic environment precisely. We extensively
evaluate our agents with a strong emphasis on reproducibil-
ity. The results underpin the claim that an AIF agent can
successfully balance competing SLOs among clients despite
considerable resource asymmetry and adapt to the dynamic
environment. Still, we transparently discuss current limitations
by accentuating the parts of our result that best show our
agent’s weaknesses. The intention is to foster research interest
in AIF from a systems perspective, as we sincerely believe
that it poses an exceptionally promising research direction for
pervasive applications and the compute continuum. Naturally,
we open-source our repository as an addition to the community
to reproduce, scrutinize, and extend our approach 1.
We summarize our contributions as:
• An adaptive mechanism for heterogeneous lifelong FL
based on AIF which allows handling non-IID data distri-
butions and heterogeneous device characteristics inherent
in pervasive computing environments.
• A conceptual AIF agent that balances multiple SLOs
during model training. When SLOs have competing
targets, agents can autonomously infer optimal training
configurations without manual intervention.
• Empirical evaluation of AIF agents for pervasive FL tasks
under real-world-inspired conditions, incorporating data
and resource heterogeneity through a reproducible exper-
imental setup with a physical testbed of heterogeneous
devices.
II. B ACKGROUND AND RELATED WORK
A. Lifelong Heterogeneous Federated Learning
In Federated Learning, participants train a global model to
maximize prediction performance without disclosing private
data. Participants optimize and validate the model parameters
with their local dataset in a training round before aggregating
their weights globally.
1) Lifelong Federated Learning: FL is lifelong when train-
ing continuously adapts to concept drifts and other changes
occurring in continuous data streams [10]. Introducing con-
cept drifts is an intrinsic property of the dynamic deploy-
ment environment of pervasive applications. The presence of
1https://github.com/adanilenka/adaptive aif agents for fl
concept drift can lead to both prolonged time until model
convergence and reduced model performance, stressing the
importance of treating concept drifts in FL and the need for
targeted solutions for different types of concept drifts [11]. Yet,
there is limited research in lifelong learning for FL [12]. The
current approaches to adapt to concept drifts rely on custom
drift detectors to understand when the drift occurs [13], [14],
which results in the need to tune said detectors to the use case
and potential drift scenarios, leaving the challenge on how
to distinguish drifts from anomalous data. Moreover, although
concept drifts are one of the most challenging issues to face for
lifelong FL, they are not the only one, as discussed further in
this section. This highlights the need for more self-adapting
mechanisms, e.g., by introducing meta-learning concepts to
FL [15], that can maintain the performance of the FL systems.
2) Heterogeneous Federated Learning: We refer to Fed-
erated Learning as heterogeneous when data is non-IID and
hardware specifications vary among participants. Moreover,
hardware heterogeneity typically implies that resources are
constrained, as less powerful devices, often located closer to
the data source, must also be accommodated in the learning
process.
Optimizing FL workflows, especially in the presence of het-
erogeneities, is important for minimizing the time-to-accuracy
of model training [16].
In that context, Kundroo et al. [9] proposed FedHPO, a
federated optimization algorithm that accelerates each client’s
training by modifying its hyperparameters, such as learning
rate or epochs. However, FedHPO introduces additional algo-
rithm parameters to set and tune, e.g., patience or thresholds
to guide the optimization process, which limits its flexibility
in dynamic environment usage. To optimize for local training
time in dynamic and heterogeneous devices conditions, an
asynchronous FL approach FedTS was proposed by Li et
al. [17], empowering the FL server to detect and optimize
for slower clients. Still, the scheme focuses on ensuring time
constraints for heterogeneous devices and does not cover
lifelong scenarios with dynamic data distributions.
Several studies also explored multi-objective optimization
(MOO) in FL to balance competing objectives. One approach
is to optimize neural network models instead of client-specific
hyperparameter optimization [18], [19]. Additionally, a signif-
icant number of existing research focuses on optimizing client
selection or clustering instead of adjusting the parameters
of individual clients to reach multi-dimensional goals [20]–
[22]. Although these approaches offer practical solutions for
balanced MOO, they do not take into account the individual
parameters of heterogeneous clients. Moreover, a vast area of
research lies in adopting Bayesian Optimization (BO) [23]
to the needs of FL. Along with grid search, BO is used
for hyperparameter tuning in FL [24], still, the downside of
both methods includes their inherent incentive to find the
best possible hyperparameters set (either in one-shot fashion
or during multiple communication rounds) which makes it
more difficult to adapt them to changing conditions of lifelong
learning. Another drawback of the classical BO is its focus
on one singular objective to optimize, e.g., model accuracy.
This problem is starting to be addressed by Multi-objective
3
Bayesian Optimization (MOBO) [20]. Yet, the presented
MOBO approach also does not consider lifelong learning
scenarios.
A number of automated optimization tools are available to
use for hyperparameter optimization tasks and were adopted
by the industry, such as HyperOpt [25] and Optuna [26],
with such platforms as RayTune [27] allowing for distributed
parameter tuning. Lately, FL research started to adopt hyper-
parameter optimization tools [28]–[32]. However, currently,
the application of hyperparameter tuning still does not cover
lifelong learning scenarios.
Therefore, existing work on optimization in FL does not
consider changing environments and lifelong FL scenarios
or lacks individual clients’ hyperparameter tuning in general,
which is crucial for pervasive applications.
3) Service Level Objectives for Federated Learning: SLOs
are definable constraints on a system that operators may use
as contracts with application developers [33], [34]. Low-level
SLOs quantify directly observable measures, such as CPU or
memory usage. High-level SLOs abstract low-level SLOs to
reduce the difficulty of diagnosing and configuring complex
and wide-spanning systems, i.e., compute continuums with
measures such as throughput or monetary costs.
For our purposes, high-level SLOs provide an intuitive
interface to set targets for an AIF agent and quantitative
measures to determine its adaptability to a dynamic envi-
ronment. In particular, maintaining prediction performance
and minimizing round duration are two primary objectives
for lifelong heterogeneous federated learning. An SLO on
prediction performance ensures consistent solution quality.
In contrast, an SLO on timeliness is crucial as resources
are constrained, and a considerably slower client can delay
global weight updates. Time and prediction performance SLOs
abstract more detailed system parameters that have an impact
on them, focusing on end-user experience and overall system
performance.
There exist multiple approaches that aim to combine SLOs
with dynamic processing requirements: Zhang et al. [35]
presented Octopus – the framework that finds optimal ser-
vices configurations in multi-tenant edge computing scenarios.
Octopus predicts SLO fulfillment of two variables based on a
deep neural network. Shubha et al. [8] presented AdaInf, which
detects SLO violations of a GPU scheduling task whenever
variable drifts occur. Through AdaInf, it is possible to find
SLO-fulfilling resource allocations between model training
and inference. Although these approaches are SLO-aware,
oriented at continuous processes, and may utilize agents for the
decision-making process, they are primarily used for inference
services and do not consider the scenario of optimizing the
training of ML models, which require more flexible and high-
level SLOs definitions, appropriate for the considered FL/ML
scenario.
B. Active Inference
Active inference is a neuroscientific framework based on
the free energy principle (FEP) [36]. AIF agents adjust their
model according to new observations and enact environmental
changes to suit their preferences. The objective is to minimize
the difference between the agent’s internal representation
and real-world models, i.e., to adapt to its environment. In
principle, the underlying framework of AIF generally applies
to adaptive systems [37]. Therefore, it is reasonable to assume
that AIF is a promising direction for computing continuums
that must adapt to a dynamic environment [6].
AIF agents continuously evaluate the expected free energy
(EFE) for different policies and assess their impact on un-
derlying models. EFE constitutes the planning ability of AIF
agents, as it allows for evaluating policies of custom length
into the future, utilizing the current understanding of the world
model (generative world model) as the source for simulations.
In a system’s context, an agent who understands the world
model will minimize EFE by selecting policies likely to fulfill
SLOs. We describe EFE with two distinct components [38]:
EFE = −
Pragmatic Value
z }| {
EQ(o|π)[ln P(o|C)] −EQ(o,s|π)DKL [Q(s|o) ∥Q(s)]| {z }
Information Gain
(1)
The information gain (IG) estimates how much the model
can improve by choosing a particular policy and aims to
resolve the uncertainty currently present in the generative
world model. Thus, IG takes into consideration the approx-
imate posterior Q over hidden states (variables in the world
that the agent cannot observe) given observations and Q
of hidden states only – our prior beliefs about the model
before any observation. Here, the agent aims at maximizing
the divergence, thus, looking for the most informative future
steps. Conversely, the pragmatic value (PV) estimates how
close a possible outcome (observation O) will be to the
agent’s preferred observations (C), focusing on meeting the
expectation defined for the agent. Together, IG and PV balance
the exploration/exploitation trade-off.
The cases in which AIF was used to dynamically support
computing systems are mainly focused on robotics; Oliver et
al. [39] give a comprehensive overview of how AIF allows
(robotic) systems to act under uncertainty. Nevertheless, the
application of AIF extends to continuous stream processing
systems, such as provided by Sedlak et al. [6], [7], which
uses a wide set of processing metrics as sensory observations.
Actions taken by the processing system were elastic adapta-
tions, e.g., scaling resources or quality, allowing to empirically
find system configurations that fulfill SLOs.
Proved useful for ensuring the adaptability of robotic and
stream processing systems, AIF could address the existing
research gap in optimizing dynamic pervasive FL systems.
To sum up, while AdaInf and Octopus are SLO-aware in-
ference services, they are designed for dynamic and resource-
efficient model serving , particularly in scenarios like multi-
model and edge inference. In contrast, traditional FL hy-
perparameter tuning methods, such as BO and grid search,
aim to achieve one best FL model but do not inherently
address the adaptability and continuity required for lifelong
learning. Active Inference, with its inherent adaptability, in-
nate exploration-exploitation trade-off handling, and potential
explainability through causality, offers a promising theoretical
4
solution for the presented gap. To the best of our knowledge,
this paper is the first attempt to apply AIF to FL (and, by ex-
tension, lifelong FL), addressing the challenges of adaptability
and heterogeneity in this domain.
III. P ROBLEM STATEMENT
We consider a lifelong heterogeneous FL system consisting
of an orchestrator with N participants. In Figure 1 we can
observe a central model and orchestrator in the Cloud that
communicates with different IoT/Edge devices. These produce
a stream of data that is used by an ML model; at the same time,
these devices embed an AIF agent that adjusts the training
of their ML models to achieve optimal (SLO fulfillment)
performance.
Fig. 1. Heterogeneous FL with data streams and AIF agents
The objective of the FL system described in Figure 1 is to
maximize the overall SLO fulfillment across all timestamps.
The challenge is to ensure the highest possible SLO fulfillment
(reaching equilibrium), given heterogeneous participants and
data within a dynamic environment. Client hardware is het-
erogeneous in vendor specification, available resource types,
and overall computational capacity. For example, some devices
may have onboard accelerators, such as GPUs, and others may
only work with energy-efficient CPUs. The data source is non-
IID with temporal correlations, i.e., the training must adapt to
non-stationary data. SLOs are set on a global system level
and all clients aim to fulfill the same SLOs. SLOs aim to
ensure smooth operations, i.e., timely training and consistently
adequate model performance, and clients check after each
training round whether the SLOs are fulfilled locally.
We introduce a mechanism to control and manage SLO ful-
fillment by defining FL training configurations, which specify
training parameters that directly affect the system’s ability to
fulfill SLOs. Configurations function as levers that an agent
can change to fulfill the high-level SLOs.
IV. P ROPOSED METHOD
This section presents the design of the AIF agents that
optimize a heterogeneous and lifelong federated system to
fulfill SLOs. Figure 2 illustrates how system entities interact.
(1) The orchestrator sends the current global model to the
client, initiating a new FL round; (2) the client requests a
Orchestrator
Aggregates model
updates
Client AIF Agent
Sends global model Requests configuration
Returns selected configuration
Sends new observationReturns updated model
Device
Updates 
BN
Next round
Performs 
model training
Awaits for
all clientsresponses
Evaluates all 
configurations based on EFE
Fig. 2. Sequence diagram for one FL round of the proposed method
configuration from the AIF Agent; (3) the AIF agent evaluates
configurations based on EFE and returns the best one; (4)
the client trains the model using the selected configuration;
(5) after training, the client sends a new observation to the
AIF Agent and returns the updated model to the Orchestrator,
which aggregates all updates; (6) the AIF agent updates its
world model (Bayesian Network) for future configuration
optimization; (7) the next FL round begins. Algorithm 1
summarizes the overall procedure for client-side training with
the rest of the section elaborating on notable details about the
process for an agent to find and choose optimal FL training
configurations in a dynamic environment.
A. Learning a Simple World Model
The generative model is at the core of an AIF agent, i.e., as
an agent interacts with its environment, it updates its internal
world representation in a perception-action cycle to improve
its understanding and align its behavior to reach set goals. We
choose Bayesian Networks (BNs) as they provide interpretable
graphical representations of learned causal structures and offer
a principled framework for probabilistic reasoning. This work
considers discrete BNs with uniform priors. Each agent’s
initial Bayesian network structure is unknown, as there are
no assumptions on prior knowledge of the environmental
dynamic. The agents require only starting knowledge of the
BN variables and their respective cardinalities, thus, specifying
the considered features in the environment and their respective
precision. We define the BN B of an agent as:
B = (G, P)
where G = (V, E) is a directed acyclic graph (DAG), and P
is the set of conditional probability distributions:
P = {P(Xi|Pa(Xi))}n
i=1
Pa(Xi) represents the parents of Xi in G, for which the joint
distribution of the variables is factorized as:
P(X1, X2, . . . , Xn) =
nY
i=1
P(Xi|Pa(Xi))
5
Algorithm 1: On Client Training Procedure
Procedure TRAIN (global model, is lifelong )
train set ← FETCH NEXT TRAIN SET ()
config, expected ig ← INFER BEST CONFIG ()
With config:
updated model, metrics ←
TRAIN MODEL (global model, train set,
config)
slos fulfilled ← CHECK SLO FULFILLMENT ()
If is lifelong :
new obs ← slos fulfilled ∪ config ∪
metrics
UPDATE BN(new obs, expected ig)
return updated model, metrics
Procedure INFER BEST CONFIG ()
configs ← {}
foreach c in possible configs do
EFE c, igc ← CALCULATE EFE (c)
configs ← configs ∪ (EFE c, igc)
return possible configsarg min(configs.EFE)
Procedure UPDATE BN(new obs, expected ig)
obs surprise ← CALCULATE SURPRISE (BN ,
new obs)
if obs surprise > expectedig then
BN ← DO STRUCTURE LEARNING ()
BN ← DO PARAMETER ESTIMATION (BN )
else
BN ← DO PARAMETER UPDATE (BN ,
new obs)
The BN vertices are divided into three categories:
1) Configuration vertices : Represent the (hy-
per)parameters of the system that are available for
the agent to set.
2) SLO vertices : Binary vertices that encode SLO being
fulfilled or not and allow for finding dependencies
between SLOs (and their fulfillment) and other vertices
of the BN.
3) System vertices: Additional vertices provide a more
comprehensive overview of the system dynamic, such
as resource usage, and their connection to SLOs.
Figure 3 illustrates the process of learning the structure of
the BN as the FL rounds progress. We use Hill Climb
Search [40] and Bayesian estimation to perform structural
learning and parameter estimation. We use variable estimation
to perform exact inference, such that an AIF agent utilizes
precise computation to leverage the uncertainty of BNs. As the
FL rounds progress, the BN causal structures are progressively
discovered. Moreover, the agent experiences further refine the
conditional probability distributions.
To allow the agent to adapt to significant discrepancies
between expected and observed outcomes, we distinguish
between two update types. If the observed IG is higher than
expected, the agent discards structure information from the
Time
SLO
Perform.
SLO
Learning
Rate
Batch
Size
CPU
Usage
Federated Rounds
0 t + n
Time
SLO
Perform.
SLO
Learning
Rate
Batch
Size
CPU
Usage
Time
SLO
Perform.
SLO
Learning
Rate
Batch
Size
CPU
Usaget
Fig. 3. BN structure update throughout FL training rounds (blue vertices
represent SLOs, green – configuration variables)
previous iterations and initializes structure re-learning of the
BN. Structure re-learning prioritizes the edges that include
SLOs as the dependent node, to ensure that the relations that
have an immediate impact on agent decisions are considered
first. Conversely, if the observed IG is within expectations
(equal or below expected IG), the agent only initializes a
parameter update on the BN, merely accommodating new data
that followed the expectations of the current generative model.
To ensure the BN does not learn from early-stage ML
model performance data, that do not accurately describe the
relationship between performance SLO and configuration, and
instead focuses on the lifelong part of the training only,
observations from early FL rounds are omitted until the
model performance stabilizes. When the global FL model
performance gets sufficiently close to the target performance
SLO, a lifelong learning flag signals the AIF agent to start
learning, before that, the configuration is taken at random, as
no pre-defined strategies are defined for this warm-up stage.
B. EFE and SLO-aware Configuration Selection
Due to SLO and configuration vertices present in the BN,
the task is to choose the configuration with the highest chance
of fulfilling SLOs by correctly discovering the connections
between the vertices and resolving uncertainty about the world.
The agent calculates the EFE for each configuration available
to the system to determine configuration optimality using
the formula in Equation (1). Specifically, it calculates the
pragmatic value as:
Pragmatic Value =
X
SLOs
P(SLOs | configuration)
× preference vector
(2)
and the Information Gain as:
Information Gain = I(A, q) (3)
The preference vector encodes the agent’s goal, i.e., the
desired outcome, and is expressed as a logarithm of the
normalized preferences. The list of outcomes consists of all
combinations of possible SLO values. Since the agent is to
fulfill all set SLOs, the state where all SLOs are fulfilled
will have a higher preference. For example, for one binary
outcome SLO, one can set the preference vector to [0.001,
0.999], specifying that the second outcome is preferred.
Information gain quantifies the expected Bayesian surprise
that measures how much observing new data would update
the agent’s belief about hidden states. First, for each possible
6
configuration, the agent assesses which observation is most
likely. An observation is a configuration and an associated out-
come. The agent uses a particular configuration as evidence to
predict the possible observation with a maximum a posteriori
(MAP) query to simulate possible future. Then, following the
implementation in [41], the calculation (Equation 3) uses the
likelihood of SLOs fulfillment (matrix A) and the predictive
density over hidden states q derived from the BN to predict
IG of a specific configuration.
In summary, the information gain and pragmatic value
balance a trade-off between exploring and taking the actions
that most likely result in SLOs fulfillment. Once the agent has
selected a configuration, the local training round starts. On
completion, the agent collects lower-level metrics and checks
SLO fulfillment. Lastly, it associates the outcomes with the
configuration and adds it to the history dataset as a new
observation for further updates.
V. E VALUATION
A. Experiment Design
The experiment design examines the AIF agent’s behavior
and adaptability to heterogeneity and lifelong FL.
1) Test Bed: We implement a physical testbed with con-
strained devices to replicate a heterogeneous resource environ-
ment. Additionally, we use a virtual machine with server-grade
hardware for experiments in more controlled environments.
Table I summarizes the hardware specifications.
TABLE I
TESTBED HARDWARE SPECIFICATIONS
Device CPU Accelerator
Virtual Machine 8x Xeon @ 3.7 GHz Tes. 2560 CC
Orin NX 8x Cortex @ 2 GHz Amp. 1024 CC 32 TC
Xavier NX 4x Cortex @ 2 GHz V ol. 384 CC 48 TC
Raspberry Pi 4 4x Cortex @ 1.8 GHz N/A
Raspberry Pi 5 6x Cortex @ 2.0 GHz N/A
2) Implementation Details: We implement the prediction
model as a simple Artificial Neural Network (ANN) with
PyTorch consisting of two fully connected layers using ReLU
activation for non-linearity.
We extend the Flower [42] framework to support FL. We
implement the agent BNs with pgmpy [43] and information
gain with pymdp [41]. We implement a controllable data
generation process using River [44]. A more detailed technical
description is out of scope and we refer interested readers to
the accompanying repository.
3) Application Scenario: We emulate a dynamic environ-
ment by controlling the data generation process to introduce
concept and volume drifts. Each client device represents a
different participant. The challenge is, for the system to adapt
to the drifts or to the varying computational resources of
participants. The experiments consider the fulfillment of two
binary high-level SLOs:
1) Time: fulfilled if a local training round does not exceed
a set limit (e.g., 2 seconds).
2) Prediction Performance : fulfilled if the primary vali-
dation metric (accuracy) exceeds a set value.
We choose time and performance as SLOs as balancing them
is non-trivial. For example, focusing exclusively on fulfilling
prediction performance may require spending an excessive
amount of time and vice versa. The configurable hyperparam-
eters are Batch Size BS ∈ {8, 32, 64, 256, 512} and Learning
Rate LR ∈ {0.0005, 0.001, 0.005, 0.01}, as there is a clear
connection to them and the system’s training objective and
considered drift types, e.g., learning rate tuning was proposed
to battle concept drift [12]. The values chosen were selected
to span across reasonable ranges that can make a difference
in terms of SLOs while being distinct from each other. Each
client initializes an independent data stream locally.
The data generator G is also parametrized by a drift
parameter, drift, which controls the presence and speed of
data drifts, where drift = 0indicates no data drift.
In each federated round, clients train and validate their
prediction model using the data samples available in an online
learning fashion. At round t, each client n possesses two
datasets: Validation t = {(xb, yb)}Bt
b=1 ∼ Gn and Train t =
Validationt−1, where x is a feature vector, y label assigned to
the data sample and Bt the size of the data set drawn from
the data generator at round t.
This way, clients acquire a new batch of data for validation
while the previous batch is re-used for training. Previous round
training samples are discarded.
4) Baselines: We choose two baselines to present AIF
agents. The first set focuses on the presentation of the be-
haviour of AIF agents, aiming at identifying both adaptable
behaviour pattern and assessing agents’ performance. The
second baseline compares AIF agents to the state-of-the-art
Optuna framework to assess the optimality of hyperparameter
choices made by the agents.
We define two baselines to illustrate AIF agents behaviour
under data distribution drifts:
1) Random: Represents a complete lack of adaptability
and intelligent choice of hyperparameters. This baseline
randomly chooses a new configuration for each federated
training round.
2) Fixed optimal : Represents the case where parameter
tuning was performed and the optimal configuration
is set once at the beginning of the training and is
never changed. To select this configuration, each of the
possible configurations was tested and the one with the
highest mean SLOs fulfillment at the end of the observed
period (around round 50) was taken as the optimal.
To align Optuna with the AIF agent, its hyperparameter
search process was modified. First, the hyperparameter study
was set to cover all FL rounds, with the first hyperparameters
set trial starting after the global model performance reached
sufficient accuracy (same as for AIF agent as described in
Section IV) and performing one trial per FL round until the
end of the training. To select the configuration for the current
FL round, first, an Optuna trial is performed and added to the
study (stored in the local Optuna database). Then, the best
configuration is inferred, and the FL training round starts. To
maintain the integrity of Optuna trials in the lifelong learning
process, each trial is conducted using a copy of the global
model from the previous round, evaluated on the training
7
data from that same round. By isolating trials from ongoing
learning dynamics, we prevent information leakage in both
directions – from the current global model to the trials and vice
versa – preserving the validity of the optimization process.
To make Optuna SLO-aware, the study was designed in
MOO fashion, where SLOs variables served as objectives.
Here, training time and 1 – validation set accuracy served
as two objectives for optimization (minimization). By default,
for MOO Optuna returns not a single best configuration, but
a Pareto front – a set of feasible configurations, leaving the
final choice up to the user. In the experiments, we first attempt
to filter out only those configurations in the Pareto front that
fulfill both SLOs, then both time and performance metrics are
normalized and weighted equally into one feature, where the
best configuration is returned for the FL round training.
Each experiment describes the results regarding SLO ful-
fillment, as the evaluation metric expected by the users, and
EFE dynamics that explain the learning and adaptation of AIF
agents.
Cumulative SLO fulfillment at round t is calculated as:
SLO Fulfillmentt =
Pt
i=1 SLO fulfilledi
t (4)
Each experiment was repeated ten times with different
random seeds that controlled the random processes, such as
the data generator and ANN weights initialization. For the
evaluation, the first considered timestep included in SLO
fulfillment tracking for a particular client run was the one
where the “lifelong” flag (model performance stabilizes and
the agent starts learning) becomes true. The reported results
are averaged across all participating clients and experiments,
if not stated otherwise. The number of local epochs was set to
3 for all experiments with no client subsampling, i.e., all FL
clients participated in every round. The SGD optimizer was
used by default. It is also worth reminding that agents learn
their BNs from scratch in every experiment.
B. Agent Demonstration
Consider a real-life situation where clients experience
changes in the amount of collected data, e.g., seasonal de-
mands in shops specializing in certain types of products or
bursts of the number of service requests. For this experiment,
such quantity drift is modeled by increasing the number of
samples drawn from the data generator every 50 epochs,
starting with 5,000 samples, then increasing to 10,000 and
15,000. The SLOs were set to 2 seconds for time and 97%
for model performance SLO. The number of federated clients
was set to 10. The dynamic of both SLO fulfillments is shown
in Figure 4.
For the fixed “optimal” baseline, the cumulative fulfillment
was high for both SLOs until the first quantity drift occurred
at the 50th round. After this round, time SLO stopped being
consistently fulfilled, which led to a steady decline in time
SLO fulfillment. However, looking at the AIF approach, it is
evident that despite the time SLO becoming more challenging
to fulfill, it still manages to recover after the quantity drift, with
the mean time SLO fulfillment per round being consistently
50 1000
0.5
1
50 1000
0.5
1
aif fixed random
Federated Round Federated Round
Cumulative Fulfillment
Performance SLO
Approach:
Time SLO
Fig. 4. Mean cumulative SLOs fulfillment with two quantity drifts (red lines
mark the drift start). Semitransparent lines show mean SLO fulfillment at a
single federated round.
larger than 50%. Still, after the second quantity drift happened,
the time SLO became even more challenging, which led
to a more prominent decline in time SLO fulfillment. To
understand the choices made by the AIF agents, we examine
mean EFE over all configurations at each epoch (Figure 5).
50 100
8, 0.0005
8, 0.001
8, 0.005
8, 0.01
32, 0.0005
32, 0.001
32, 0.005
32, 0.01
64, 0.0005
64, 0.001
64, 0.005
64, 0.01
256, 0.0005
256, 0.001
256, 0.005
256, 0.01
512, 0.0005
512, 0.001
512, 0.005
512, 0.01
1.1 
1.2 
1.3 
1.4 
EFE 
Federated Round
Configuration (Batch Size, Learning Rate)
Fig. 5. Mean EFE with two quantity drifts with each line representing one
possible training configuration. Lower values represent configurations that the
AIF agents favor.
It is visible how the configurations preferred by the AIF
agents change after the observed environmental changes. In
the beginning, it is shown how agents slowly come to prefer
configurations (256, 0.005) and (256, 0.001) (compared to
the fixed baseline being (256, 0.01)). However, after the first
quantity drift, this preference shifted to a larger batch size
of 512. This is an expected behavior as the amount of data
doubled, but the time constraint remained the same. Still, after
the amount of data increased again, there was no more space to
increase the batch size. Therefore, the increase of EFE across
all configurations can be observed, signaling that.
As shown in Equation (1), the information gain term ac-
counts for explorative behavior and is compared to the actual
observed information gain during each federated round to
estimate how “surprised” the agent is. Figure 6 a shows the
mean information gain across clients.
Here, the process of agents adapting to the environment can
be seen, with the observed IG decreasing as the BN becomes
more confident. However, two prominent spikes occur right
after the quantity drifts, illustrating the “surprised” agents
8
50 100
0
0.05
0.1
0.15
50 100
0
0.2
0.4
Observed IG Expected IG
Federated Round Federated Round
Mean IG
IG
a) b)
Fig. 6. a) Mean observed IG over all clients and repetitions, b) Observed and
expected information gain for one client at one run
detecting the environmental change.
To better illustrate the dynamics of looking for the best
configuration after the second quantity drift, the entire history
of expected and observed IG of one client can be observed
(Figure 6 b). This client initially settled down for a config-
uration of (512, 0,01), which worked for the agent for 68
epochs due to the observed IG being less or equal to the
agent’s expectations. However, at round 68, the agent was
surprised because the time SLO was not fulfilled despite using
the “time-proven” configuration. Despite being surprised, the
agent only retrained the structure of the BN (indicated by the
abrupt change in the expected IG). It happened several times
more, but the agent preferred exploiting its knowledge. After
round 100, the agent again started to be surprised, leading
to a change in strategy. The agent went exploring, as visible
by the increased expected information gain. These spikes are
also associated with the agent choosing previously unexplored
(or poorly explored) configurations. For instance, round 104
corresponds to the agent choosing configuration (64, 0.005).
This example illustrates how AIF agents treat changes in the
observed environment and can independently balance between
exploration and exploitation.
As the setup for the experiment consisted of ten clients and
ten separate experiments, it is possible to represent configura-
tions preferred by the agents at three critical points (before the
first quantity drift, before the second quantity drift, and at the
end of the experiment) into distributions over configurations.
These distributions are given in Figure 7.
8, 0.00058, 0.00532, 0.000532, 0.00132, 0.0164, 0.000564, 0.00164, 0.005256, 0.0005256, 0.001256, 0.005256, 0.01512, 0.0005512, 0.001512, 0.005512, 0.010
5
10
15
20 Before quantity drifts
After the 1st quantity drift
After the 2nd quantity drift
Configuration
% of Clients
Fig. 7. Chosen configurations distributions before and after quantity drifts
Despite the expected behavior, a minority of agents still
settle for clearly non-optimal configurations. Another obser-
vation is the shift in the preferred configurations after the
first quantity drift towards a bigger batch size. To quantify the
observed changes in the distributions, Fisher’s exact test was
performed on the distributions with batch sizes 256 and 512
(too low values were filtered out to focus the test on the sen-
sible configurations as the rest represent wrong configurations
and are irrelevant for the test). The p-value of this test was
reported to be 0.0293, showing the statistical significance in
the observed changes between the distribution of the preferred
configurations before and after the first quantity drift.
Observed EFE after the second quantity drift shows that the
agents’ behavior is dictated by the combination of available
configurations and set SLOs. To better estimate the effect
SLOs have on the preferred configurations, a set of experi-
ments was conducted that modified the SLOs considered in
the experiments. Table II shows the SLOs chosen for each
experiment compared to the SLOs considered in the previous
experiment.
TABLE II
COMPARISON OF TIME AND PERFORMANCE SLO S FOR DIFFERENT
EXPERIMENTS
Experiment Time SLO (s) Performance SLO (%)
Fulfillable SLOs 2 97
Unfulfillable SLOs 0.1 99.5
Easily Fullfillable SLOs 3600 50
Time Relaxed 3600 97
Performance Relaxed 2 50
Figure 8 shows the mean EFE over five clients used for
experiments and ten repetitions.
8, 0.0005
8, 0.005
32, 0.0005
32, 0.005
64, 0.0005
64, 0.005
256, 0.0005
256, 0.005
512, 0.0005
512, 0.005
50 100
8, 0.0005
8, 0.005
32, 0.0005
32, 0.005
64, 0.0005
64, 0.005
256, 0.0005
256, 0.005
512, 0.0005
512, 0.005
50 100
0.8
1
1.2
1.4
1.6
EFE:
Federated Round Federated Round
Configuration (BS, LR)Configuration (BS, LR)
(a) Both SLOs relaxed (b) Both SLOs unfulfillable
(c) Performance SLO relaxed (d) Time SLO relaxed
Fig. 8. Mean EFE under different SLOs setups and two quantity drifts
The EFE shows that having not challenging or unrealistic
SLOs (Figure 8 a and b, respectively) leads to EFE being
consistent across all available configurations, with EFE being
high when the target SLOs are unfulfillable and EFE being
equally low when the targets are too easy to fulfill. The
situation is different when one of the two SLOs is relaxed
while the other is somehow challenging to fulfill. Thus, when
performance is relaxed, the agents are incentivized to optimize
the behavior towards time SLO, leading to agents settling for
the largest available batch size (512) regardless of the learning
rate. The situation is different regarding time-relaxed SLO –
when the performance is targeted, agents tend to explore more
configurations as the task is still not that challenging for them,
leading to diverse behaviors. Still, the resulting EFE heatmap
9
expresses some preference bias compared to the experiment
with two SLOs relaxed. This “uncertain” behavior could be
attributed to quantity drifts not impacting the performance
goals.
The evaluation shows the potential of AIF agents in de-
tecting the changes in the environment and the ability to
initiate system re-configuration with no human supervision.
However, the SLO fulfillment is not perfect. Two main ex-
planations were identified up until now: (1) “unsupervised”
BN structure learning using Hill Climb Search may struggle
to discover meaningful causal relationships when limited data
is available [45], (2) in the absence of observations with both
SLOs fulfilled, an agent may either stuck in forever exploring
state or stick to the strategy that guarantees at least one SLO
fulfillment and focus on exploiting sub-optimal behavior.
After dissecting the behaviour of the agents in a simpler sce-
nario, the next section focuses on more practical experimental
setups and presents evaluation of AIF agents’ behaviour in the
presence of concept drifts and resources heterogeneity.
VI. R ESULTS
A. Hyperparameter Tuning Comparison
To demonstrate both the validity and advantages of the
proposed AIF agents, we compare them against Optuna, a
state-of-the-art framework for hyperparameter optimization in
ML. Optuna serves as a strong baseline due to its efficiency
in finding optimal configurations. However, unlike Optuna,
which focuses on static optimization, AIF agents offer con-
tinuous adaptability, making them better suited for dynamic
and heterogeneous FL environments.
1) Concept Drift: The first experiment included a concept
drift, which was present from the beginning of the training,
and both approaches needed to find the best configuration to
meet the time SLO of 3s and performance SLO of 85%. The
SLOS fulfillment is shown in Figure 9.
20 40 60 800
0.5
1
20 40 60 800
0.5
1
aif optuna
Federated Round Federated Round
Cumulative Fulfillment
Performance SLO
Approach:
Time SLO
Fig. 9. Mean cumulative SLOs fulfillment for the scenario with a persistent
concept drift
It is seen that both Optuna and AIF have similar time SLO
fulfillment, with performance SLO being slightly better for
AIF after training round 60.
To assess what configurations were favored by both ap-
proaches, EFE (Figure 10) and normalized objectives can be
examined for AIF and Optuna, respectively.
For AIF, the best configuration based on EFE is (64,
0.005). For Optuna, the result is based on separate time and
20 40 60 80
8, 0.0005
8, 0.001
8, 0.005
8, 0.01
32, 0.0005
32, 0.001
32, 0.005
32, 0.01
64, 0.0005
64, 0.001
64, 0.005
64, 0.01
256, 0.0005
256, 0.001
256, 0.005
256, 0.01
512, 0.0005
512, 0.001
512, 0.005
512, 0.01
1.2 
1.4 
EFE 
Federated Round
Configuration (Batch Size, Learning Rate)
Fig. 10. Mean EFE for the scenario with a persistent concept drift
performance assessments and their combination (Figure 11).
For the time SLO, the best configurations are those of larger
batch sizes, as expected. For performance, Optuna focused
on a set of configurations from batch size 8 through 64 and
learning rates of 0.001 through 0.01. The combination of both
SLOs lands in the range of batch sizes (32, 64, 256) and
learning rates of (0.005, 0.01), with (64, 0.005) being the most
preferred configuration.
8 32 64 256 512 
0.0005 
0.001 
0.005 
0.01 
8 32 64 256 512 
 8 32 64 256 512 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
Total Score
Batch Size Batch Size Batch Size
Learning Rate
(a) Time (b) Performance (c) Total
Fig. 11. Optuna preferred configurations at the end (last 10 rounds) of training
Comparing the two approaches, it is seen that both AIF
agents and Optuna converge to similar configurations in this
scenario.
2) Concept and Quantity Drifts after Round 50: Next
evaluation scenario introduced two drifts at the same time,
simulating a change in both quantity and distribution of data
received by the FL clients.
The scenario was set as follows. First, 10,000 samples were
used per FL training round with no concept drift; after round
50 – a slight drift appeared and 20,000 samples started to be
drawn from the data generator. The resulting SLOs fulfillment
is presented in Figure 12 for time SLO – 4 seconds and
performance SLO – 90%.
It is seen that adding concept drift to data leads to an im-
mediate decline in performance SLO as previously established
configurations and the trained model do not account for that.
However, AIF agents are able to return to a relatively high
performance SLO fulfillment in a faster and more reliable way
than Optuna.
10
50 1000
0.5
1
50 1000
0.5
1
aif optuna
Federated Round Federated Round
Cumulative Fulfillment
Performance SLO
Approach:
Time SLO
Fig. 12. Mean cumulative SLOs fulfillment for the scenario with concept and
quantity drifts after round 50
Based on EFE shown in Figure 13, it is seen that after the
drifts appeared after round 50, a set of the previously favored
configurations (256, 0.005) and (64, 0.005) gradually lost their
importance and agents focused mostly on (256, 0.001).
50 100
8, 0.0005
8, 0.001
8, 0.005
8, 0.01
32, 0.0005
32, 0.001
32, 0.005
32, 0.01
64, 0.0005
64, 0.001
64, 0.005
64, 0.01
256, 0.0005
256, 0.001
256, 0.005
256, 0.01
512, 0.0005
512, 0.001
512, 0.005
512, 0.01
0.9 
1 
1.1 
1.2 
1.3 
EFE 
Federated Round
Configuration (Batch Size, Learning Rate)
Fig. 13. Mean EFE for the scenario with concept and quantity drifts after
round 50
These experiments illustrate that AIF agents align with
commonly used hyperparameter optimization methods while
providing adaptability where necessary. Compared to the state-
of-the-art Optuna framework adapted to the lifelong heteroge-
neous FL scenario, AIF agents are able to fully recover from
the concept drift after 43 rounds, while Optuna fails to return
to its best performance even after 100 rounds after the drift
introduction.
3) Device Heterogeneity: The next experiment focused on
inspecting the ability of AIF agents to adapt to the resources
available to the agents located at the edge devices and aligning
them with the SLOs set.
For the experiments, three edge devices (Raspberry Pi 5,
Nvidia Orin NX, and Nvidia Xavier NX) were used as separate
federated clients and were tasked to participate in the FL for
75 epochs, while a Raspberry Pi 4 device served as the FL
orchestrator. No data drifts were introduced in this experiment.
Due to the differences in resources, it was expected that agents
would prefer different configurations under the same target
SLOs. In addition to different available resources, the size
of the neural networks was also changed, so to change the
utilization of available resources on the device.
The first experiment featured a smaller neural network (64
and 32 units in 2 layers). The comparison of SLO fulfillment
across various devices is shown in Figure 14, and mean EFE
is shown in Figure 15.
20 40 600
0.5
1
20 40 600
0.5
1
Orin NX RPI 5 Xavier NX
Federated Round Federated Round
Cumulative Fulfillment
Performance SLO
Device:
Time SLO
Fig. 14. Mean SLOs fulfillment for different devices. Time SLO: 2 seconds,
performance SLO: 97%
20 40 60
8, 0.0005
8, 0.005
32, 0.0005
32, 0.005
64, 0.0005
64, 0.005
256, 0.0005
256, 0.005
512, 0.0005
512, 0.005
20 40 60
 20 40 60
1
1.1
1.2
1.3
1.4
1.5
EFE
Federated RoundFederated RoundFederated Round
Configuration (Batch Size, Learning Rate)
(a) Orin NX (b) Xavier NX (c) RPI 5
Fig. 15. Mean EFE for different devices
It is shown that all devices manage to fulfill the set SLOs.
It is also worth noting that Raspberry Pi 5 managed to fulfill
performance SLO slightly faster than devices that used GPU
but occasionally struggled to maintain flawless time SLO
fulfillment. When looking at mean EFE, it is clear that in order
to better utilize its cores (as there is no GPU), Raspberry Pi
can successfully employ a vast range of configurations, while
devices with GPU choose bigger batch sizes to better utilize
their parallelization capabilities.
The next experiment was conducted with the same set of
devices but with a wider neural network (5120 and 512 units
compared to 64 and 32 used in the previous experiment). The
time SLO was adjusted to 15 seconds. The resulting SLOs
fulfillment is shown in Figure 16 and mean EFE is shown
in Figure 17. The performance SLO fulfillment at the final
FL round was 96.9% for Orin NX and 98.7% for RPI 5 and
Xavier NX, while the time SLO was 99% for Nvidia devices
and 87.8% for RPI 5.
Here the change in the model architecture impacted the
optimal configuration choice for Raspberry Pi 5. As seen from
the mean EFE, the batch size had to be increased to 256 to
fit into the time and performance SLOs, while Nvidia devices
utilized a more comprehensive range of configurations.
Results presented in this section show how AIF agents
display adaptive behaviour in the presence of changing data
11
20 40 600
0.5
1
20 40 600
0.5
1
Orin NX RPI 5 Xavier NX
Federated Round Federated Round
Cumulative Fulfillment
Performance SLO
Device:
Time SLO
Fig. 16. Mean SLOs fulfillment for different devices with a wider network.
Time SLO: 15 seconds, performance SLO: 97%
20 40 60
8, 0.0005
8, 0.005
32, 0.0005
32, 0.005
64, 0.0005
64, 0.005
256, 0.0005
256, 0.005
512, 0.0005
512, 0.005
20 40 60
 20 40 60
1
1.2
1.4
1.6
EFE
Federated RoundFederated RoundFederated Round
Configuration (Batch Size, Learning Rate)
(a) Orin NX (b) Xavier NX (c) RPI 5
Fig. 17. Mean EFE for different devices and wider network
as well as adaptation to the local resources to ensure the
fulfillment of set SLOs.
VII. C ONCLUSION
This work presented AIF agents that are able to adaptively
change their behavior in response to dynamic environments.
We evaluated the proposed AIF agents in lifelong heteroge-
neous FL, utilizing a set of both dynamic data and diverse
devices. We showed that AIF agents are able to fulfill compet-
ing SLOs and unfolded the behaviors of agents. We compared
AIF agents to the hyperparameter-tuning framework Optuna
adjusted to lifelong learning and showcased how the adaptive
nature of AIF agents allows for faster performance recovery
in the presence of data drift.
Future work can further expand the usage of the active
inference framework to orchestrate distributed learning sys-
tems, for instance, by fulfilling system-level SLOs, such as
fairness of participation or global model performance. Another
line of research can target the scalability of the proposed
framework, as the usage of BNs can potentially introduce
computational bottleneck as the number of considered vertices
and their cardinalities grow. Enhancements of the current
method can improve the ability of the agents to find causal
dependencies in limited data, making them more robust, tar-
geting the limitations of the discrete BN, introducing temporal
dependencies to capture the environmental dynamics more
precisely, and providing more nuanced SLOs specifications to
enable tracking SLOs in a range.
ACKNOWLEDGMENTS
We thank Alexander Knoll for providing us with the hard-
ware infrastructure and Pantelis Frangoudis for his valuable
suggestions and feedback.
The work of Anastasiya Danilenka was conducted during
the research visit funded by the Warsaw University of Tech-
nology within the Excellence Initiative: Research University
(IDUB) programme. The work of Maria Ganzha was co-
funded by the Centre for Priority Research Area Artificial
Intelligence and Robotics of Warsaw University of Technology
within the Excellence Initiative: Research University (IDUB)
programme. Further, this work was supported in part by
EU Horizon under Grants 101135576 (INTEND), 101079214
(AIoTwin), and 101070186 (TEADAL). Ayuda CNS2023-
144359 financiada por MICIU/AEI/10.13039/501100011033 y
por la Uni ´on Europea NextGenerationEU/PRTR.
REFERENCES
[1] S. Dustdar, V . Casamayor Pujol, and P. K. Donta, “On Distributed
Computing Continuum Systems,” IEEE Transactions on Knowledge and
Data Engineering, vol. 35, no. 4, pp. 4092–4105, Apr. 2023.
[2] T. Rausch, W. Hummer, C. Stippel, S. Vasiljevic, C. Elvezio, S. Dustdar,
and K. Kr ¨osl, “Towards a platform for smart city-scale cognitive
assistance applications,” in 2021 IEEE Conference on Virtual Reality
and 3D User Interfaces Abstracts and Workshops (VRW) , 2021, pp.
330–335.
[3] M. Aboualola, K. Abualsaud, T. Khattab, N. Zorba, and H. S. Hassanein,
“Edge technologies for disaster management: A survey of social media
and artificial intelligence integration,” IEEE Access, vol. 11, pp. 73 782–
73 802, 2023.
[4] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y . Zomaya,
“Edge intelligence: The confluence of edge computing and artificial
intelligence,” IEEE Internet of Things Journal , vol. 7, no. 8, pp. 7457–
7469, 2020.
[5] A. Furtuanpey, P. Raith, and S. Dustdar, “Frankensplit: Efficient neural
feature compression with shallow variational bottleneck injection for
mobile edge computing,” IEEE Transactions on Mobile Computing , pp.
1–17, 2024.
[6] B. Sedlak, V . C. Pujol, P. K. Donta, and S. Dustdar, “Equilibrium in the
Computing Continuum through Active Inference,” Future Generation
Computer System, 2024.
[7] B. Sedlak, V . C. Pujol, A. Morichetta, P. K. Donta, and S. Dustdar,
“Adaptive Stream Processing on Edge Devices through Active Infer-
ence,” 2024, arXiv:2409.17937 [cs].
[8] S. S. Shubha and H. Shen, “AdaInf: Data Drift Adaptive Scheduling
for Accurate and SLO-guaranteed Multiple-Model Inference Serving at
Edge Servers,” in Proceedings of the ACM SIGCOMM 2023 Conference,
ser. ACM SIGCOMM ’23. New York, NY , USA: Association for
Computing Machinery, Sep. 2023, pp. 473–485.
[9] M. Kundroo and T. Kim, “Federated learning with hyper-parameter opti-
mization,” Journal of King Saud University - Computer and Information
Sciences, vol. 35, no. 9, p. 101740, 2023.
[10] A. L. Su ´arez-Cetrulo, D. Quintana, and A. Cervantes, “A survey on
machine learning for recurring concept drifting data streams,” Expert
Systems with Applications , vol. 213, p. 118934, 2023.
[11] G. Yang, X. Chen, T. Zhang, S. Wang, and Y . Yang, “An impact study
of concept drift in federated learning,” in 2023 IEEE International
Conference on Data Mining (ICDM) , 2023, pp. 1457–1462.
[12] E. Jothimurugesan, K. Hsieh, J. Wang, G. Joshi, and P. B. Gibbons,
“Federated learning under distributed concept drift,” in Proceedings
of The 26th International Conference on Artificial Intelligence and
Statistics, ser. Proceedings of Machine Learning Research, F. Ruiz, J. Dy,
and J.-W. van de Meent, Eds., vol. 206. PMLR, 25–27 Apr 2023, pp.
5834–5853.
[13] ——, “Federated learning under distributed concept drift,” in Interna-
tional Conference on Artificial Intelligence and Statistics. PMLR, 2023,
pp. 5834–5853.
[14] L. Rahimli, F. M. Awaysheh, S. Al Zubi, and S. Alawadi, “Federated
learning drift detection: An empirical study on the impact of concept and
data drift,” in 2024 2nd International Conference on Federated Learning
Technologies and Applications (FLTA), 2024, pp. 241–250.
12
[15] Z. Lu, H. Pan, Y . Dai, X. Si, and Y . Zhang, “Federated learning with
non-iid data: A survey,”IEEE Internet of Things Journal, vol. 11, no. 11,
pp. 19 188–19 209, 2024.
[16] Z. Jiang, W. Wang, B. Li, and Q. Yang, “Towards efficient synchronous
federated training: A survey on system optimization strategies,” IEEE
Transactions on Big Data , vol. 9, no. 2, pp. 437–454, 2022.
[17] Q. Li, Z. Gao, Y . Sun, Y . Wang, R. Wang, and H. Zhu, “An efficient
asynchronous federated learning protocol for edge devices,” IEEE In-
ternet of Things Journal , vol. 11, no. 17, pp. 28 798–28 808, 2024.
[18] H. Zhu and Y . Jin, “Multi-objective evolutionary federated learning,”
IEEE transactions on neural networks and learning systems , vol. 31,
no. 4, pp. 1310–1322, 2019.
[19] G. Paragliola, “Evaluation of the trade-off between performance and
communication costs in federated learning scenario,” Future Generation
Computer Systems, vol. 136, 06 2022.
[20] M. Badar, S. Sikdar, W. Nejdl, and M. Fisichella, “Fairtrade: Achieving
pareto-optimal trade-offs between balanced accuracy and fairness in
federated learning,” Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 38, no. 10, pp. 10 962–10 970, Mar. 2024.
[21] A. Lackinger, P. A. Frangoudis, I. ˇCili´c, A. Furutanpey, I. Murturi,
I. P. ˇZarko, and S. Dustdar, “Inference load-aware orchestration for
hierarchical federated learning,” in 2024 IEEE 49th Conference on Local
Computer Networks (LCN) , 2024, pp. 1–9.
[22] S. Yan, P. Zhang, S. Huang, H. Sun, Y . Zhang, and A. Tolba, “Node
selection algorithm for federated learning based on deep reinforcement
learning for edge computing in iot,” Electronics, vol. 12, p. 2478, 05
2023.
[23] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian
optimization of machine learning algorithms,” in Advances in
Neural Information Processing Systems , F. Pereira, C. Burges,
L. Bottou, and K. Weinberger, Eds., vol. 25. Curran Associates,
Inc., 2012. [Online]. Available: https://proceedings.neurips.cc/paper
files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf
[24] Z. Dai, B. K. H. Low, and P. Jaillet, “Federated bayesian optimization
via thompson sampling,” in Advances in Neural Information Processing
Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 9687–
9699. [Online]. Available: https://proceedings.neurips.cc/paper files/
paper/2020/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf
[25] J. Bergstra, D. Yamins, and D. Cox, “Making a science of model search:
Hyperparameter optimization in hundreds of dimensions for vision
architectures,” in Proceedings of the 30th International Conference on
Machine Learning , ser. Proceedings of Machine Learning Research,
S. Dasgupta and D. McAllester, Eds., vol. 28, no. 1. Atlanta, Georgia,
USA: PMLR, 17–19 Jun 2013, pp. 115–123. [Online]. Available:
https://proceedings.mlr.press/v28/bergstra13.html
[26] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A next-
generation hyperparameter optimization framework,” in Proceedings
of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining , ser. KDD ’19. New York, NY , USA:
Association for Computing Machinery, 2019, p. 2623–2631. [Online].
Available: https://doi.org/10.1145/3292500.3330701
[27] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and
I. Stoica, “Tune: A research platform for distributed model selection and
training,” 2018. [Online]. Available: https://arxiv.org/abs/1807.05118
[28] M. Chadha, P. Khera, J. Gu, O. Abboud, and M. Gerndt, “Training
heterogeneous client models using knowledge distillation in serverless
federated learning,” in Proceedings of the 39th ACM/SIGAPP
Symposium on Applied Computing , ser. SAC ’24. New York, NY ,
USA: Association for Computing Machinery, 2024, p. 997–1006.
[Online]. Available: https://doi.org/10.1145/3605098.3636015
[29] M. Krouka, A. Koskela, and T. Kulkarni, “Communication-efficient
differentially private federated learning using second-order information,”
in Privacy Regulation and Protection in Machine Learning , 2024.
[Online]. Available: https://openreview.net/forum?id=J0IwrkICp8
[30] J.-M. Park, W.-J. Jang, T.-H. Oh, and S.-H. Lee, “Overcoming client
data deficiency in federated learning by exploiting unlabeled data on
the server,” IEEE Access, vol. 12, pp. 130 007–130 021, 2024.
[31] X. Li, Z. Wu, W. Zhang, Y . Zhu, R.-H. Li, and G. Wang, “Fedgta:
Topology-aware averaging for federated graph learning,” Proc. VLDB
Endow., vol. 17, no. 1, p. 41–50, Sep. 2023. [Online]. Available:
https://doi.org/10.14778/3617838.3617842
[32] J. Parra-Ullauri, X. Zhang, A. Bravalheri, R. Nejabati, and
D. Simeonidou, “Federated hyperparameter optimisation with flower
and optuna,” in Proceedings of the 38th ACM/SIGAPP Symposium on
Applied Computing, ser. SAC ’23. New York, NY , USA: Association
for Computing Machinery, 2023, p. 1209–1216. [Online]. Available:
https://doi.org/10.1145/3555776.3577847
[33] S. Nastic, A. Morichetta, T. Pusztai, S. Dustdar, X. Ding, D. Vij, and
Y . Xiong, “Sloc: Service level objectives for next generation cloud
computing,” IEEE Internet Computing , vol. 24, no. 3, pp. 39–50, 2020.
[34] V . Casamayor Pujol, B. Sedlak, Y . Xu, P. K. Donta, and S. Dustdar,
“Invited paper: Deepslos for the computing continuum,” in Proceedings
of the 2024 Workshop on Advanced Tools, Programming Languages, and
PLatforms for Implementing and Evaluating Algorithms for Distributed
Systems, ser. ApPLIED’24. New York, NY , USA: Association for
Computing Machinery, 2024, p. 1–10.
[35] Z. Zhang, Y . Zhao, and J. Liu, “Octopus: SLO-Aware Progressive
Inference Serving via Deep Reinforcement Learning in Multi-tenant
Edge Cluster,” in Service-Oriented Computing, Cham, 2023.
[36] K. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
brain,” Journal of physiology-Paris, vol. 100, no. 1-3, pp. 70–87, 2006.
[37] K. J. Friston, M. J. Ramstead, A. B. Kiefer, A. Tschantz, C. L.
Buckley, M. Albarracin, R. J. Pitliya, C. Heins, B. Klein, B. Millidge,
D. A. Sakthivadivel, T. S. C. Smithe, M. Koudahl, S. E. Tremblay,
C. Petersen, K. Fung, J. G. Fox, S. Swanson, D. Mapes, and G. Ren ´e,
“Designing ecosystems of intelligence from first principles,” Collective
Intelligence, vol. 3, no. 1, p. 26339137231222481, 2024. [Online].
Available: https://doi.org/10.1177/26339137231222481
[38] T. Parr, G. Pezzulo, and K. J. Friston, Active inference: the free energy
principle in mind, brain, and behavior . MIT Press, 2022.
[39] G. Oliver, P. Lanillos, and G. Cheng, “An Empirical Study of Active
Inference on a Humanoid Robot,” IEEE Transactions on Cognitive and
Developmental Systems, vol. 14, no. 2, pp. 462–471, Jun. 2022.
[40] D. Koller and N. Friedman, Probabilistic Graphical Models: Principles
and Techniques - Adaptive Computation and Machine Learning . The
MIT Press, 2009.
[41] C. Heins, B. Millidge, D. Demekas, B. Klein, K. Friston, I. D. Couzin,
and A. Tschantz, “pymdp: A python library for active inference in
discrete state spaces,” Journal of Open Source Software , vol. 7, no. 73,
p. 4098, 2022.
[42] D. J. Beutel, T. Topal, A. Mathur, X. Qiu, J. Fernandez-Marques,
Y . Gao, L. Sani, H. L. Kwing, T. Parcollet, P. P. d. Gusm ˜ao, and N. D.
Lane, “Flower: A friendly federated learning research framework,” arXiv
preprint arXiv:2007.14390, 2020.
[43] Ankur Ankan and Abinash Panda, “pgmpy: Probabilistic Graphical
Models using Python,” in Proceedings of the 14th Python in Science
Conference, Kathryn Huff and James Bergstra, Eds., 2015, pp. 6 – 11.
[44] J. Montiel, M. Halford, S. M. Mastelini, G. Bolmier, R. Sourty,
R. Vaysse, A. Zouitine, H. M. Gomes, J. Read, T. Abdessalem, and
A. Bifet, “River: machine learning for streaming data in python,” J.
Mach. Learn. Res. , vol. 22, no. 1, jan 2021.
[45] N. K. Kitson, A. C. Constantinou, Z. Guo, Y . Liu, and K. Chobtham, “A
survey of Bayesian Network structure learning,” Artificial Intelligence
Review, vol. 56, no. 8, pp. 8721–8814, Aug. 2023.