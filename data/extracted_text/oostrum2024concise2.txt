arXiv:2406.07726v4  [cs.LG]  17 May 2025
A Concise Mathematical Description of Active
Inference in Discrete Time
Jesse van Oostrum, Carlotta Langer, Nihat Ay
Abstract
In this paper we present a concise mathematical descriptionof active
inference in discrete time. The main part of the paper serves as a ba-
sic introduction to the topic, including a detailed exampleof the action
selection mechanism. The appendix discusses the more subtle mathemat-
ical details, targeting readers who have already studied the active infer-
ence literature but struggle to make sense of the mathematical details
and derivations. Throughout, we emphasize precise and standard mathe-
matical notation, ensuring consistency with existing texts and linking all
equations to widely used references on active inference. Additionally, we
provide Python code that implements the action selection and learning
mechanisms described in this paper and is compatible withpymdp envi-
ronments.
Introduction
Active inference is a theory that describes the action selection and learning
mechanisms of an agent in an environment. We aim to present a concis e mathe-
matical description of the theory so that readers interested in th e mathematical
details can quickly ﬁnd what they are looking for. We have paid special atten-
tion to choosing notation that is more in line with standard mathematic al texts
and is also descriptive, in the sense that dependencies are made exp licit. Hence,
the focus of this paper lies on the mathematical details and derivatio ns rather
than verbal motivations and justiﬁcations.
The paper consists of a main text and an appendix. The main text pro vides
a clear introduction to active inference in discrete time, accessible f or people
new to the topic. It is divided into two parts: inference, which assum es a
given generative model, and learning, which explains how the agent ac quires this
model. The main text concludes with a worked example of action select ion. The
appendix delves into ﬁner details and derivations, catering to reade rs familiar
with active inference who seek clarity on the mathematical aspects .
To complement our theoretical exposition, we provide a Python imple menta-
1
tion1 of the action selection and learning mechanisms described in this pape r,
which is compatible with pymdp environments. This code is more minimalistic,
which makes it easier to understand than other implementations suc h as SPM
and pymdp.
1 Set-up and notation
In this paper we consider an active inference agent acting in a discre te-time
setting with a ﬁnite-time horizon. This means that we consider a sequ ence of T
time steps and at every time step τ the agent receives an observation oτ , and
performs an action aτ . We use τ for arbitrary time steps and the letter t to
denote the current time step. We use the subscript τ :τ ′ to denote a sequence
of variables, e.g. oτ :τ ′ = ( oτ , . . . , o τ ′ ). A sequence of (future) actions is called a
policy2 and is denoted by πt = at:T , with π = π1. We write a1:t−1 for actions
that were performed in the past and πt for future actions that still need to be
selected.
The agent models the dynamics of the environment using an internal generative
model. This model uses a variable sτ , called an internal state, to represent the
state of the environment 3 at time step τ. The model is given by the following
probability distribution:
p(o1:T , s 1:T |a1:T −1, θ ).
This probability distribution factorizes according to the graph in Figu re 1. In
s1 s2 s3
o1 o2 o3
a1 a2
sT
oT
aT −1
Figure 1: Graphical representation of the generative model
the ﬁrst part of this paper we assume that this generative model is given and
need not be learned, and we therefore suppress the dependence on the parameter
θ. In the second part we discuss how the model is learned.
Suppose the agent is at time step t. It will have received observations o1:t and
performed actions a1:t−1. We use qt(sτ :τ ′ ) to denote the (approximate) posterior
1https://github.com/jessevoostrum/active-inference
2Note that in a reinforcement learning context the term “poli cy” has a diﬀerent meaning.
3Note that the number of possible internal states is usually m uch smaller than the actual
number of states the environment can be in.
2
distribution of the generative model, p(sτ :τ ′ |o1:t, a 1:t−1), and also refer to this
as the belief of the agent about the variable sτ :τ ′ .
2 Inference
2.1 Action selection according to active inference
Let an agent be at time step t, having received observations o1:t and performed
actions a1:t−1. According to active inference an agent selects its next action by
sampling a policy πt from the following distribution (equation (10) in [4]):
σ(−G(πt|o1:t, a 1:t−1)) (1)
and selecting the action at corresponding to that policy. 4 The function σ denotes
the softmax function deﬁned in (48) and G is the expected free energy function
given by
G(πt|o1:t, a 1:t−1) = −
(
Eqt(ot+1:T |π t)
[
DKL
(
qt(st+1:T |ot+1:T , π t) ∥ qt(st+1:T |πt)
) ]
+ Eqt (ot+1:T |π t)
[
ln pC (ot+1:T )
] )
.
(2)
Note that according to (1) the agent is more likely to sample policies πt that
have a low expected free energy G(πt, o 1:t, a 1:t−1). In Section 2.2 we discuss
equivalent formulations and diﬀerent interpretations of the expec ted free en-
ergy. The distributions qt(ot+1:T |πt), qt(st+1:T |ot+1:T , π t), qt(st+1:T |πt), needed
for the calculation of G, are (approximate) posterior distributions of the gen-
erative model of the agent after having observed o1:t and performed a1:t−1. In
Section 2.3 we describe how the agent infers these posterior distrib utions. The
distribution pC is a preference distribution over observations that we assume is
given to the agent. This distribution is distinct from the generative m odel p.
Remark 1. Note that in certain descriptions of active inference in discrete time
also a variational free energy term F appears in the distribution in equation (1)
(e.g. equation (B.9) in [8]). This term is only relevant in speciﬁc cases th at
we will discuss in Remark 2 in Appendix A. Furthermore a habit term E is
sometimes included that is also considered in Appendix A, but discarde d here
for simplicity.
4Note that at every time step a new policy is sampled, only the a ction at for the corre-
sponding time step t is executed, and the rest of the actions in πt are discarded.
3
2.2 Expected free energy
Recall that equation (2) gives the following expression for the expe cted free
energy function:
G(πt|o1:t, a 1:t−1) = −
(
Eqt(ot+1:T |π t)
[
DKL
(
qt(st+1:T |ot+1:T , π t) ∥ qt(st+1:T |πt)
) ]
+ Eqt (ot+1:T |π t)
[
ln pC (ot+1:T )
] )
.
The ﬁrst term between the brackets on the RHS is called epistemic value or in-
formation gain. It measures the average change in belief about the future state s
st+1:T due to receiving future observations ot+1:T . The second term, known
as utility, quantiﬁes the similarity between the expected future observatio n dis-
tribution and the preferred observation distribution. As previous ly mentioned,
the agent is more likely to sample policies with low expected free energy , which
correspond to high information gain and utility.
An equivalent formulation of expected free energy is given by
G(πt|o1:t, a 1:t−1) = Eqt(st+1:T |π t)
[
H
[
p(ot+1:T |st+1:T )
] ]
+DKL
(
qt(ot+1:T |πt) ∥ pC (ot+1:T )
)
.
(3)
The ﬁrst term on the RHS is referred to as ambiguity. It measures the average
uncertainty an agent has about its future observations given kno wledge of its
future states. The second term is called expected complexityor risk. It represents
the divergence between expected and preferred future observ ations. The agent
favors policies with low ambiguity and risk.
In Appendix C we show that both expressions of the expected free energy are
equal.
In practice, often the following mean ﬁeld approximations are made:
qt(st+1:T ) =
T∏
τ =t+1
qt(sτ ),
pC (ot+1:T ) =
T∏
τ =t+1
pC (oτ ).
Equation (2) and (3) can then be written as follows:
G(πt|o1:t, a 1:t−1) =
T∑
τ =t+1
Gτ (πt, o 1:t, a 1:t−1), (4)
4
with
Gτ (πt|o1:t, a 1:t−1) = −
(
Eqt(oτ |π t)
[
DKL
(
qt(sτ |oτ , π t) ∥ qt(sτ |πt)
)]
+ Eqt(oτ |π t)
[
ln pC (oτ )
] )
(5)
= Eqt(sτ |π t)
[
H
[
p(oτ |sτ )
] ]
+ DKL
(
qt(oτ |πt) ∥ pC (oτ )
)
.
It is outside the scope of this paper to further derive or motivate t he expected
free energy. We refer the reader to Appendix B.2.5 in [8] and [3, 10, 7 ] for more
details.
2.3 State inference
In this section we describe the simplest form of state inference, wh ich is obtained
by applying Bayes’ rule. State inference methods as described in e.g . [8, 6, 9] can
be thought of as computationally eﬃcient approximations of what is d escribed
here. See Appendix A for more details on these methods.
Above we deﬁned qt to be the (approximate) posterior of the generative model
given o1:t, a1:t−1. In this section we make the conditioning variables explicit
and write q( · |o1:t, a 1:t−1) instead.
Current and future state inference
We start by studying the generative model that is assumed to be giv en to the
agent. The generative model can be decomposed as follows: (see F igure 1)
p(o1:T , s 1:T |a1:T −1) = p(s1:T |a1:T −1)p(o1:T |s1:T )
p(s1:T |a1:T −1) = p(s1)
T∏
τ =2
p(sτ |sτ −1, a τ −1)
p(o1:T |s1:T ) =
T∏
τ =1
p(oτ |sτ ).
At every time step the agent updates its belief about the current s tate it is in.
Before having performed any observations, its belief about the cu rrent state is
equal to the prior belief p(s1). After receiving o1 it will update its belief using
Bayes’ rule:
q(s1|o1) ∝ p(o1|s1)p(s1).
5
Subsequently, it will perform an action a1 (selected as described in Section 2.1)
and receive a next observation o2. The belief about s2 is given by
q(s2|o1:2, a 1) ∝ p(o2|s2, o 1, a 1)p(s2|o1, a 1)
= p(o2|s2)
∑
s1
p(s2|s1, o 1, a 1)p(s1|a1, o 1)
= p(o2|s2)
∑
s1
p(s2|s1, a 1)q(s1|o1).
For a general t the belief about st is updated as follows:
q(st|o1:t, a 1:t−1) ∝ p(ot|st)
∑
st−1
p(st|st−1, a t−1)q(st−1|o1:t−1, a 1:t−2). (6)
For future time point τ > t , the belief about the state sτ is given by
q(sτ |o1:t, a 1:τ −1) =
∑
sτ −1
p(sτ |sτ −1, a τ −1)q(sτ −1|o1:t, a 1:τ −2). (7)
The belief about a future state given a future observation is calcula ted as follows:
q(sτ |oτ , o 1:t, a 1:τ −1) ∝ p(oτ |sτ )q(sτ |o1:t, a 1:τ −1). (8)
Future observation inference
In order to compute G in (2), we need a posterior distribution q(oτ |o1:t, a 1:τ −1)
over future observations oτ . This can be computed as follows:
q(oτ |o1:t, a 1:τ −1) =
∑
sτ
p(oτ |sτ )q(sτ |o1:t, a 1:τ −1). (9)
Past, current, and future state inference
In order to perform inference over states in the past, present a nd future (which
is needed for the learning of the generative model and for the comp utation of
the variational free energy over states), the agent can use the following formula:
q(s1:T |o1:t, a 1:t−1, π t) ∝ p(o1:t|s1:T , a 1:t−1, π t)p(s1:T |a1:t−1, π t)
= p(o1:t|s1:t)p(s1:T |a1:t−1, π t)
=
t∏
τ =1
p(oτ |sτ ) p(s1)
T∏
τ =2
p(sτ |sτ −1, a τ −1), (10)
where we use aτ −1 in the last term for elements of both a1:t−1 and πt.
6
3 Learning
In the above section, we have assumed that the agent has access to a generative
model p(o1:T , s 1:T |a1:T −1, θ ). In this section we discuss how the parameter θ of
this model is learned. The generative model consists of three (con ditional) cat-
egorical distributions that are parametrized by θ = ( θD, θ A, θ B) in the following
way:
p(s(j)
1 |θD) = θD
j , (11)
p(o(i)
τ |s(j)
τ , θ A) = θA
ij , (12)
p(s(j)
τ |s(k)
τ −1, a (l)
τ −1, θ B) = θB
jkl , (13)
where we have enumerated the elements of the observation, actio n and state
space with the bracketed superscript (·). In order to learn the parameters of the
generative model, we adopt a Bayesian belief updating scheme with a D irichlet
prior. (See Appendix B for details on this.) More speciﬁcally, the prior over θ
is parametrized by α = ( α D, α A, α B ) and is given by
p(θ|α ) = p(θD|α D)
∏
j
p(θA
•j |α A)
∏
k,l
p(θB
•kl|α B ) (14)
p(θD|α D) ∝
∏
j
(
θD
j
) α D
j −1
, (15)
p(θA
•j |α A) ∝
∏
i
(
θA
ij
) α A
ij −1
, (16)
p(θB
•kl|α B ) ∝
∏
j
(
θB
jkl
) α B
jkl −1
, (17)
where θ•j denotes the vector ( θ1j , . . . , θ nj ).
Now after performing actions a1:T −1 and receiving observations o1:T we want
to update our belief about θ according to Bayes’ rule. The true posteriors over
these parameters are not Dirichlet distributions. (See Appendix B f or details.)
The active inference literature suggests to set the approximate p osterior distri-
bution to be a Dirichlet distribution and update the hyperparameter α in the
following way (equation (B.12) in [8] and equation (21), (A.6) and (A.7) in [4]):
α D
j
′
= α D
j + qT
(
s(j)
1
)
, (18)
α A
ij
′
= α A
ij +
T∑
τ =1
/BD
o(i) (oτ )qT
(
s(j)
τ
)
, (19)
α B
jkl
′
= α B
jkl +
T∑
τ =2
qT
(
s(j)
τ
)
qT
(
s(k)
τ −1
)
/BD
a(l) (aτ −1). (20)
7
The distributions qT (sτ ) , τ ∈ { 1, . . . , T } are approximate posteriors obtained 5
using the current version of the generative model (before θ has been updated).
In Appendix B we elaborate on the origin of this learning rule.
Note the similarity with the standard update rule for Dirichlet priors g iven
in (31). In the standard update rule the element α i∗ of the hyperparameter
corresponding to the observation x(i∗) is incremented by 1, which makes this
observation more likely in the updated distribution. In the updates ( 18)–(20)
the hyperparameters are incremented by the amount of posterio r belief in that
state or state transition, e.g. α D
j is incremented by q
(
s(j)
1 |o1:T , a 1:T −1
)
.
In order to go from a Dirichlet distribution p(θ|α ) to an actual value of the pa-
rameter that can be used for the generative model, the mean of th e distribution
can be used, which is given by
ˆθi = Ep(θ|α )[θi]
= α i
∑
j α j
. (21)
For example, after the learning step, the new distribution over s1 is given by
p(s(j)
1 |ˆθD) = α D
j
′
∑
j α D
k
′ .
This concludes the discussion of learning in the context of the active inference
framework.
4 Example: T-maze example
In this section we discuss the action selection mechanism of an active inference
agent in the T-maze environment depicted in Figure 2. (See also Sect ion 7.3 of
[8] and [5].) This illustrates the theory of action selection and state inf erence
that is presented in Section 2.
5See Section 2.3.
8
? /cheese ? /cheese
cue
Figure 2: T-maze environment
Description of the (internal) generative model of the agent
State, observation and action spaces
The internal state space of the agent 6 S has two dimensions, 7, Location and
Reward condition, and can be described as follows:
S = SL × S R,
SL = {center, right arm, left arm, cue location},
SR = {reward on right, reward on left}.
A typical element of the state space is written as s = ( sL, s R).
The observation space O has three dimensions 8,9 , Location, Reward, and Cue,
and can be described as follows:
O = OL × OR × OC ,
OL = {center, right arm, left arm, cue location},
OR = {no reward, reward, loss},
OC = {cue right, cue left}.
A typical element of the observation space is written as o = ( oL, o R, o C ).
6In this example the state of the environment (generative pro cess) is the same as the state
space of the internal world model of the agent (generative mo del). Note that this is in general
not the case. In a more realistic setting the state space of th e environment will be much more
complex than the internal state space.
7The dimensions of the state space are sometimes referred to a s state factors.
8The dimensions of the observation space are sometimes refer red to as observation modal-
ities.
9Note that we follow here the description from [5]. In [8] the c ue observation is absorbed
into the location observation.
9
The space of actions A is described by
A = {move to center, move to right arm,
move to left arm, move to cue location}.
Note that these actions are always available, independent of the cu rrent location
of the agent. A typical element of the action space is written as a.
In the following we use [dir] as a placeholder for left and right, and [loc]
as a placeholder for the four locations center, right arm, left arm and cue
location.
Observation kernel p(o|s)
We now specify the observation kernel p(o|s) of the generative model of the
agent. First note that the observation dimensions are independen t, that is
p(o|s) = p(oL|s)p(oR|s).
The beliefs about the location observation given the state are mode lled as fol-
lows:
p(oL|s) =
{
1 if oL = sL
0 otherwise ,
which implies that the location can be unambiguously inferred from the obser-
vation.
The beliefs about the reward observation given the state are mode lled as follows:
p(oR = no reward|sL ∈ { center, cue location}, s R) = 1 ,
p(oR = no reward|sL /∈ { center, cue location}, s R) = 0 ,
p(oR = reward|sL = [dir] arm, s R = reward on [dir]) = 0 . 98,
p(oR = loss|sL = right arm, s R = reward on left) = 0 . 98,
p(oR = loss|sL = left arm, s R = reward on right) = 0 . 98.
This implies that the agent observes no reward when it is in location center
or cue location, it observes reward when it is in the same arm as speciﬁed by
the reward condition with high probability, and it observes loss when it is in
the opposite arm of the reward condition with high probability.
The beliefs about the cue observation given the state are modelled a s follows:
p(oC = cue [dir]|sL = cue location, s R = reward on [dir]) = 1 ,
p(oC = cue [dir]|sL ∈ S L \ {cue location}, s R) = 0 . 5.
This implies that the cue observation is completely informative about t he reward
condition when the agent is at the cue location, and otherwise indepe ndent of
the actual reward condition.
10
Transition dynamics kernel p(sτ +1|sτ , a τ )
We continue by describing the transition dynamics kernel p(sτ +1|sτ , a τ ).
p(sL
τ +1 = [loc]|sL
τ ∈ { center, cue location}, s R
τ , a τ = go to [loc]) = 1 ,
p(sL
τ +1 = [dir] arm|sL
τ = [dir] arm, s R
τ , a τ ) = 1 .
This implies that if the agent is in center or cue location it will be in the
location speciﬁed by the action in the next time step. If it is in one of th e arms
however, it will stay there, independent of the choice of action aτ .
p(sR
τ +1 = reward on [dir]|sR
τ = reward on [dir], s L
τ , a τ ) = 1 .
This implies that the reward condition stays constant throughout t he trajectory.
Preference distribution pC and prior over states pD
The unnormalized preference distribution pC can be chosen to favor observations
with reward and discourage observations with loss as follows:
pC (([loc], no reward, cue [dir])) = 2 ,
pC (([loc], reward, cue [dir])) = 3 ,
pC (([loc], loss, cue [dir])) = 1 .
Finally we let the prior belief over states pD be uniform.
Action selection procedure
We will now simulate the trajectory of an agent acting according to a ctive
inference. We set the time horizon to T = 3, which implies that the policies will
have length 2.
Time step 1
The agent starts by receiving an observation o1 = ( center, no reward, cue
right). It now updates its beliefs about the current state such that
qt(sL
1 = center, s R
1 = reward on [dir]) = 0 . 5.
Subsequently it computes its beliefs about future states and obse rvations given
a policy using equation (7), (8), (9). For example for π∗
1 = ( move to cue
location, move to left arm) we have
q1(sL
2 = cue location, s R
2 = reward on [dir] | π1 = π∗
1 ) = 0 . 5, (22)
q1(sL
3 = left arm, s R
3 = reward on [dir] | π1 = π∗
1 ) = 0 . 5,
11
and
q1(oL
2 = cue location, o R
2 = no reward, o C
2 = cue [dir] | π1 = π∗
1 ) = 0 . 5,
q1(oL
3 = left arm, o R
3 = reward, o C
3 = cue [dir] | π1 = π∗
1 ) = 0 . 25,
q1(oL
3 = left arm, o R
3 = loss, o C
3 = cue [dir] | π1 = π∗
1 ) = 0 . 25,
and for example for o∗
2 = ( cue location, no reward, cue left) we have
q1(sL
2 = cue location, s R
2 = reward on left | o2 = o∗
2, π 1 = π∗
1 ) = 1 . (23)
Note here the reduction of uncertainty about s2 due to the observation o∗
2, repre-
sented by the epistemic value deﬁned in Section 2.2 given by the KL dive rgence
between the distributions (23) and (22).
The agent now computes G and plugs this into equation (1) and gets the fol-
lowing posterior distribution over policies:
a2 →
a1 ↓
center right arm left arm cue location
center 0.022 0.041 0.041 0.046
right arm 0.041 0.075 0.075 0.083
left arm 0.041 0.075 0.075 0.083
cue location 0.046 0.083 0.083 0.091
and in this scenario it samples a policy with as ﬁrst action move to cue location
with highest probability.
Time step 2
After having performed action a∗
1 = move to cue location, the next obser-
vation it receives is o∗
2 = ( cue location, no reward, cue right). Its belief
about the current state is now given by
q2(sL
2 = cue location, s R
2 = reward on right) = 1 .
For instance, when π∗
2 = ( move to left arm), the beliefs about future states
are
q2(sL
3 = left arm, s R
3 = reward on right | π2 = π∗
2 ) = 1 ,
and the observation beliefs
q2(oL
3 = left arm, o R
3 = reward, o C
3 = cue [dir] | π2 = π∗
2 ) = 0 . 01,
q2(oL
3 = left arm, o R
3 = loss, o C
3 = cue [dir] | π2 = π∗
2 ) = 0 . 49.
Since all uncertainty has already been taken away by the last obser vation, con-
ditioning on o∗
3 = ( left arm, reward, cue left) will make no diﬀerence to the
belief about the state s3, i.e.
q2(sL
3 = left arm, s R
3 = reward on right | o3 = o∗
3, π 2 = π∗
2 ) =
q2(sL
3 = left arm, s R
3 = reward on right | π2 = π∗
2 ) = 1 ,
12
which will cause the epistemic value term in G to be zero.
The agent now calculates G again and obtains the following distribution over
policies:
a2 center right arm left arm cue location
0.20 0.52 0.08 0.20 ,
and will then sample the policy ( move to right arm) with highest probability.
Acknowledgements
The authors would like to thank Thomas Parr, Conor Heins, Ryan Smit h, Beren
Millidge, Pablo Lanillos, Sean Tull, Stephen Mann, Pradeep Kumar Baner jee,
Frank R¨ oder and Lance Da Costa for helpful discussions and comm ents and
acknowledge the support of the Deutsche Forschungsgemeinsch aft Priority Pro-
gramme “The Active Self” (SPP 2134).
References
[1] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and
machine learning. Vol. 4. 4. Springer, 2006.
[2] David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. “Variational infer-
ence: A review for statisticians”. In: Journal of the American statistical
Association 112.518 (2017), pp. 859–877.
[3] Lancelot Da Costa et al. “Active inference as a model of agency” . In:
arXiv preprint arXiv:2401.12917 (2024).
[4] Lancelot Da Costa et al. “Active inference on discrete state-sp aces: A
synthesis”. In: Journal of Mathematical Psychology99 (2020), p. 102447.
issn: 0022-2496. doi: https://doi.org/10.1016/j.jmp.2020.102447.
[5] Conor Heins. Active Inference Demo: T-Maze Environment. https://pymdp-rtd.readthedocs.io/en/la 
[6] Conor Heins et al. “pymdp: A Python library for active inference in dis-
crete state spaces”. In: The Journal of Open Source Software7.73 (2022),
p. 4098.
[7] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. “ Whence
the expected free energy?” In: Neural Computation 33.2 (2021), pp. 447–
482.
[8] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the
free energy principle in mind, brain, and behavior. MIT Press, 2022.
[9] Ryan Smith, Karl J Friston, and Christopher J Whyte. “A step-b y-step
tutorial on active inference and its application to empirical data”. I n:
Journal of mathematical psychology107 (2022), p. 102632.
[10] Ran Wei. “Value of Information and Reward Speciﬁcation in Active In-
ference and POMDPs”. In: arXiv preprint arXiv:2408.06542 (2024).
13
A Variational free energy minimization
Preliminaries
In this section we use x for a general observed variable and z for a general latent
variable.
Let p(x, z ) = p(x|z)p(z) be a generative model. In order to perform inference
over the latent variables after making an observation x, one has to compute
the posterior p(z|x). This posterior is often hard to compute directly. One can
instead approximate this posterior by ﬁnding the distribution qx(z) in a family
of distributions Q that minimizes the following function:
F (˜q|x) =
∑
z
˜q(z) (ln ˜q(z) − ln p(x, z )) ,
called the variational free energy. Note that we distinguish notatio nally ˜q, which
is a generic element of Q and a variable in F , and qx, which is the minimizer of
F for a ﬁxed x, i.e.
qx = arg min
˜q∈Q
F (˜q|x).
Note that when Q is large enough, for example when z is discrete and Q is the
set of all probability distributions over z, then the minimizer of the free energy
is equal to the exact posterior distribution and we have
qx(z) = p(z|x).
We can replace ln p(x, z ) in F by a general function f, i.e.
Ff (˜q|x) =
∑
z
˜q(z) (ln ˜q(z) − f(x, z )) .
The minimizer can be found by substituting g(x, z ) = ef(x,z ) as follows:
Ff (˜q, x ) =
∑
z
˜q(z) (ln ˜q(z) − ln g(x, z ))
=
(∑
z
˜q(z)
(
ln ˜q(z) − ln g(x, z )
∑
z′ g(x, z ′)
) )
+ ln
∑
z′
g(x, z ′).
If Q is again large enough, the minimizer is given by:
qx(z) = g(x, z )∑
z g(x, z )
= ef(x,z )
∑
z ef(x,z )
= σ(f(x, z )), (24)
14
where σ is the softmax function deﬁned in (48) and in the case that f(x, z ) =
ln p(x, z ) we have
qx(z) = σ(ln p(x, z ))
= p(z|x).
The variational free energy can be written as follows:
F (˜q|x) = DKL(˜q(z) ∥ p(z|x)) − ln p(x)
≥ − ln p(x),
which shows that the negative of the variational free energy is a low er bound on
the evidence (ELBO). By minimizing F w.r.t. ˜q we get the following approximate
equality (equation (B.2) in [8]):
F (qx|x) ≈ − ln p(x). (25)
Variational free energy minimization in active inference
Active inference adopts the perspective that perception, action selection and
learning can be interpreted as minimizing one single variational free en ergy
function F . In its complete form it can be written as follows:
F (˜q|o1:t, a 1:t−1) = E˜q(s1:T ,θ,π t) [ln ˜q(s1:T , θ, π t) − ln p(s1:T , o 1:t, θ, π t|a1:t−1)] .
The distributions in the family Q are assumed to factorize as follows:
q(s1:T , θ, π ) = q(θD)q(θA)q(θB )q(π)
T∏
τ =1
q(sτ ),
which is sometimes referred to as the mean-ﬁeld approximation. Now F can be
written as follows:
F (˜q|o1:t, a 1:t−1) = E˜q(s1:T ,θ,π t)
[
ln ˜q(πt) + ln ˜q(θ) +
T∑
τ =1
ln ˜q(sτ ) − ln p(πt)
− ln p(θ) − ln p(s1|θD) −
t∑
τ =1
ln p(oτ |sτ , θ A) −
T∑
τ =2
ln p(sτ |sτ −1, a τ −1, θ B )
]
,
(26)
where we use aτ −1 in the last term for elements of both a1:t−1 and πt.
Perception
For studying perception, equation (26) can be rewritten as follows :
F (˜q|o1:t, a 1:t−1) = E˜q(π t)
[
Fπ t (˜q|o1:t, a 1:t−1)
]
+ C\s
15
where C\s is independent of ˜ q(s1:T ) and
Fπ t (˜q|o1:t, a 1:t−1) =
E˜q(s1:T )
[
ln ˜q(s1:T ) − E˜q(θ)[ln p(s1:T , o 1:t|θ, a 1:t−1, π t)]
]
. (27)
Perception according to active inference is minimizing Fπ t w.r.t. ˜q(s1:T ). The
minimizer is written qt(s1:T |πt) = q(s1:T |πt, o 1:t, a 1:t−1).
We can use the factorizing properties to rewrite (27) as follows:
Fπ t (˜q|o1:t, a 1:t−1) =
T∑
τ =1
E˜q(sτ )[ln ˜q(sτ )] − E˜q(s1,θ D)[ln p(s1|θD)]
−
t∑
τ =1
E˜q(sτ ,θ A)p(oτ |sτ , θ A) −
T∑
τ =2
E˜q(sτ ,s τ −1,θ B)p(sτ |sτ −1, a τ , θ B ),
which is equivalent to equation (6) in [4]. 10 One can also get rid of the expec-
tations over θ by replacing them by an estimator ˆθ. Then we get
Fπ t (˜q|o1:t, a 1:t−1) =
T∑
τ =1
E˜q(sτ )[ln ˜q(sτ )] − E˜q(s1)[ln p(s1|ˆθD)]
−
t∑
τ =1
E˜q(sτ )p(oτ |sτ , ˆθA) −
T∑
τ =2
E˜q(sτ ,s τ −1)p(sτ |sτ −1, a τ , ˆθB ),
which is equivalent to (B.4) in [8].
Learning
Updating the θ parameter (learning) happens at the end of an episode ( t = T ).
The agent has observed o1:T and performed a1:T −1. The variational free energy
from equation (26) can be written as follows:
F (˜q|o1:T , a 1:T −1) =
E˜q(θ)
[
ln ˜q(θ) − EqT (s1:T )[ln p(o1:T , s 1:T , θ |a1:T −1)]
]
+ C\θ , (28)
where C\θ is independent of ˜q(θ) and we have ﬁxed qT (s1:T ) to be the approxi-
mate posterior over states, inferred using the current (not-up dated) belief p(θ).
If the current belief p(θ) is a Dirichlet distribution with hyperparameter α , then
the minimizer qT (θ) of F will also be a Dirichlet distribution with hyperparam-
eter α ′ as given in (18)–(20). In Appendix B we derive this, and relate it to a
well known variational inference algorithm called coordinate ascent variational
inference (CAVI).
10Note that in [4] only the parameter θA is treated as a variable and θD and θB are
considered ﬁxed.
16
Action selection
Finally we can also view action selection as the minimization of the variatio nal
free energy function (26). We can rewrite this function as follows:
F (˜q|o1:t) = E˜q(π )
[
ln ˜q(π) − ln p(π) + E˜q(s1:T ) [ln ˜q(s1:T ) − ln p(o1:t, s 1:T |π)]
]
+ C\π
= E˜q(π ) [ln ˜q(π) − ln p(π) + Fπ (˜q(s1:T )|o1:t)] + C\π ,
where Fπ is deﬁned in (27), C\π is independent of ˜ q(π), and we replaced the
expectation over ˜q(θ) by an estimator ˆθ and suppress the dependence of the
generative model p on ˆθ in the notation. (This is equivalent to the second line
in equation (B.7) in [8].) What is important to note here, is that the agen t is
trying to infer an action sequence (policy) π of both future and past actions.
We have therefore dropped the dependence on a1:t−1 in both F and Fπ , and
instead π is a sequence of action starting at τ = 1 instead of τ = t. In Section
2, we always ﬁxed the past actions to the actions that were actually performed,
which is no longer the case here.
We minimize F w.r.t. ˜q(π) and ˜q(s1:T ) and using (24) we get for the minimizers
respectively
qt(π) = σ (− ln p(π) + Fπ (qt(s1:T )|o1:t)) , (29)
and qt(s1:T ) is the minimizer of Fπ . Now we can use
p(π) = σ(ln E(π) − G(πt|o1:t, a 1:t−1)),
which corresponds to the last line equation (B.7) in [8]. 11 The term E(π) is a
habit term, signifying what policies the agent is usually exercising. Plug ging
this back into (29) gives
qt(π) = σ (− ln E(π) + G(πt|o1:t, a 1:t−1) + Fπ (qt|o1:t)) , (30)
which corresponds to equation (B.9) in [8].
Remark 2. We now try to interpret this derivation conceptually. Note that due
to (25) we have the following approximate equality:
F (˜q|o1:t) = E˜q(π ) [ln ˜q(π) − ln p(π) + Fπ (˜q(s1:T ), o 1:t)] + C\π
≈ E˜q(π ) [ln ˜q(π) − ln p(π) − ln p(o1:t|π)] + C\π
= E˜q(π ) [ln ˜q(π) − ln p(o1:t, π )] + C\π ,
which implies that the minimizer qt(π) is approximately equal to the posterior
p(π|o1:t). In other words, this says that we select the policy that is most pr obable
given the past observations. That is, the agent forgets which pas t actions it has
11It can be argued that calling this a prior is incorrect, since it actually depends on the
observations o1:t.
17
performed, and tries to infer these based on the past observatio ns. Then it
tries to ﬁnd the most likely sequence of future actions to go with this sequence
of past actions. Selecting future actions in this way however only ma kes sense
when certain past action sequences make certain future action se quences more
likely. For example, let our action space consist of two actions {left, right}
and policies consist of sequences of actions of length two. Now supp ose that
the prior over policies dictates that the agent almost certainly perf orms the
policies ( left, left) or ( right, right). This implies that having inferred the
ﬁrst action gives the agent more information about the most likely ne xt action.
However, if both next actions are equally likely given a ﬁrst action, ac cording to
the prior, then the likelihood term p(o1:t|π) does not have any information about
the next action. Note that in the calculation of G the past actions are ﬁxed to
the actions that have been performed. Therefore G will not make certain future
action sequences more likely based on possible past action sequence s. Therefore,
the term Fπ in (30) only becomes relevant when the habit term E makes certain
future action sequences more likely based on past action sequence s.
B Learning Preliminaries
Bayesian belief updating
The learning process of an active inference agent is formulated as B ayesian
belief updating over the parameters. In general, Bayesian belief up dating can
be described as follows. Let θ be the parameter of a model pθ we want to learn
and x the output of this model. We start with a prior belief p(θ|α ) which is
parametrized by the hyperparameter α . Now our posterior belief about θ is given
by the distribution p(θ|x, α ) which is obtained by Bayes’ rule. In some special
cases12, the posterior distribution belongs to the same parametrized family as
the prior, such that p(θ|x, α ) = p(θ|α ′). Then the learning can be summarized
by the update from α to α ′.
Categorical model without latent variables
Now we let the model be a categorical distribution over elements {x(1), . . . , x (n)}
parametrized by θ = ( θ1, . . . , θ n). That is,
p(x(i)|θ) = θi, ∀i ∈ { 1, . . . , n }.
The prior over the parameter θ is given by the Dirichlet distribution parametrized
by the hyperparameter α = ( α 1, . . . , α n). That is,
p(θ|α ) ∝
∏
i
θα i−1
i .
12For details, see the theory of conjugate priors.
18
After observing x∗, the posterior is given by
p(θ|x∗, α ) ∝ p(x∗|θ)p(θ|α )
=
∏
i
θ
α i−1+/BD
x(i) (x∗)
i .
Note that this is again a Dirichlet distribution with hyperparameter α ′ such
that
α ′
i = α i +
/BD
x(i) (x∗), ∀i ∈ { 1, . . . , n }. (31)
That is, the hyperparameter corresponding to the observation x∗ is increased
by one. This will make this observation more likely in the updated distrib ution.
Categorical model with latent variables
Exact posterior
Now we let the model be a joint distribution over the product space o f obser-
vations x and latent states z, given by {x(1), . . . , x (n)} × { z(1), . . . , z (m)}. The
model is parametrized by θ = ( θD, θ A) as follows:
p(z(j)|θD) = θD
j , (32)
p(x(i)|z(j), θ A) = θA
ij. (33)
The prior over the θ is deﬁned as follows:
p(θ|α ) = p(θD|α D)
∏
j
p(θA
•j |α A) (34)
p(θD|α D) ∝
∏
j
(
θD
j
) α D
j −1
, (35)
p(θA
•j |α A) ∝
∏
i
(
θA
ij
) α A
ij −1
, (36)
where θ•j denotes the vector ( θ1j , . . . , θ nj ). After observing x∗ = x(i∗ ), the
exact posterior is given by
p(θ|x∗, α ) ∝ p(x∗|θ)p(θ|α )
=
∑
j
p(x∗|z(j), θ A)p(z(j)|θD)p(θA|α A)p(θD|α D)
∝
∑
j
θA
i∗j θD
j

∏
j′
∏
i′
(
θA
i′j′
) α A
i′j′ −1



∏
j′′
(
θD
j′′
) α D
j′′ −1


=
∑
j

∏
j′
∏
i′
(
θA
i′j′
) α A
i′j′ −1+
/BD
i∗ (i′)/BD
j (j′)



∏
j′′
(
θD
j′′
) α D
j′′ −1+
/BD
j (j′′ )

.
19
Note that this is no longer a Dirichlet distribution. Below we discuss how
the Dirichlet distribution shows up in an algorithm for approximating th e true
posterior.
Mean-ﬁeld approximation
We can also approximate the posterior over θ by minimizing the following vari-
ational free energy function:
F (˜q|x∗) = E˜q(z,θ ) [ln ˜q(z, θ ) − ln p(x∗, z, θ |α )] , (37)
where we assume the approximate posterior over both θ and z factorizes as
follows:
q(z, θ ) = q(z)q(θ).
The coordinate ascent variational inference (CAVI) algorithm [2, 1 ] updates
the distributions q(z) and q(θ) iteratively, each time holding one distribution
ﬁxed while updating the other. More speciﬁcally, we can initialize q(θ) with our
prior belief p(θ|α ) and minimize F w.r.t. ˜q(z). The variational free energy now
becomes
F (˜q|x∗, q (θ)) = E˜q(z)
[
ln ˜q(z) − Eq(θ)[ln p(z|x∗, θ )]
]
+ C\z , (38)
where C\z is independent of ˜ q(z). The minimizer q(z) is proportional to
q(z) ∝ exp
(
Eq(θ)[ln p(z|x∗, θ )]
)
. (39)
(See equation (24).) We then ﬁx this q(z) and optimize F w.r.t. ˜q(θ) and get
F (˜q|x∗, q (z)) = E˜q(θ)
[
ln ˜q(θ) − Eq(z)[ln p(x∗, z, θ |α )]
]
+ C\θ , (40)
where C\θ is independent of ˜ q(θ). The minimizer q(θ) is proportional to
q(θ) ∝ exp
(
Eq(z)[ln p(x∗, z, θ |α )]
)
. (41)
These two steps are performed iteratively until the beliefs about θ and z have
converged.
Note that when p(x, z, θ |α ) is a categorical model with Dirichlet priors, as de-
ﬁned in (32)–(36), we can rewrite equation (41) as follows:
q(θ) ∝ exp
(
Eq(z)[ln p(x∗, z, θ |α )]
)
= exp
(
Eq(z)
[
ln p(x∗|z, θ A) + ln p(z|θD) + ln p(θ|α )
])
= exp

∑
j
q(z(j))
[
ln p(x∗|z(j), θ A) + ln p(z(j)|θD)
]

p(θ|α )
=
∏
j
(
θA
i∗j
) q(z(j) ) (
θD
j
) q(z(j) ) (
θD
j
) α D
j −1 ∏
i
(
θA
ij
) α A
ij −1
=
∏
j
(
θD
j
) α D
j +q(z(j) )−1 ∏
i
(
θA
ij
) α A
ij +
/BD
i∗ (i)q(z(j) )−1
20
Note that this is again a Dirichlet distribution with updated parameter α ′ =
(α D′
, α A′
) given by
α D
j
′
= α D
j + q(z(j)), (42)
α A
ij
′
= α A
ij +
/BD
i∗ (i)q(z(j)). (43)
Update (42) makes latent states with high q(z) more likely in the updated dis-
tribution. Update (43) makes sure that latent states are more like ly to generate
the observation x(i∗ ), especially those with high q(z). Note the similarity with
the update rule (31) for categorical models without latent variable s.
POMDP model
Now we let p be the generative model from an active inference agent as describ ed
in (11)–(17). Similar to (37) the variational free energy now becom es
F (˜q|o1:T , a 1:T −1) =
E˜q(s1:T ,θ ) [ln ˜q(s1:T , θ ) − ln p(o1:T , s 1:T , θ |a1:T −1, α )] . (44)
We use the mean-ﬁeld approximation q(s1:T , θ ) = q(s1:T )q(θ). Equivalent to
(38)–(39) we can start by minimizing F w.r.t. ˜q(s1:T ) and get a minimizer
qT (s1:T ) using the current belief about θ. Then, similar to (40)–(41) we can use
this minimizer to write F as follows:
F (˜q|o1:T , a 1:T −1, q T (s1:T )) =
E˜q(θ)
[
ln ˜q(θ) − EqT (s1:T )[ln p(o1:T , s 1:T , θ |a1:T −1, α )]
]
+ C\θ , (45)
21
and work out the minimizer. This gives
q(θ) ∝ exp
(
EqT (s1:T )[ln p(o1:T , s 1:T , θ |a1:T −1, α )]
)
= exp
(
EqT (s1:T )
[ T∑
τ =1
ln p(oτ |sτ , θ A) + ln p(s1|θD)
+
T∑
τ =2
ln p(sτ |sτ −1, a τ −1, θ B)
])
p(θ|α )
= exp
(∑
s1:T
qT (s1:T )
[ T∑
τ =1
ln p(oτ |sτ , θ A) + ln p(s1|θD)
+
T∑
τ =2
ln p(sτ |sτ −1, a τ −1, θ B )
])
p(θ|α )
=
∏
j
(
θD
j
) qT (s(j)
1 ) ∏
i
(
θA
ij
) ∑ T
τ =1
/BD
o(i) (oτ )qT (s(j)
τ )
∏
k
(
θB
jkl
) ∑ T
τ =2 qT (s(j)
τ )qT (s(k)
τ −1)
/BD
a(l) (aτ −1)
p(θ|α )
=
∏
j
(
θD
j
) α D
j +qT (s(j)
1 )−1 ∏
i
(
θA
ij
) α A
ij +∑ T
τ =1
/BD
o(i) (oτ )qT (s(j)
τ )−1
∏
k
(
θB
jkl
) α B
jkl +∑ T
τ =2 qT (s(j)
τ )qT (s(k)
τ −1)
/BD
a(l) (aτ −1)−1
.
This gives the update rules for α given in (18)–(20).
Remark 3. Note that these update rules are actually just the ﬁrst iteration o f
the CAVI algorithm described above. It will therefore in general no t minimize
the variational free energy in equation (44). Instead it minimizes th e quantity
given in equation (28) and (45) where qT (s1:T ) is ﬁxed. Note however that if
one would assume qT (s1:T ) to be given, the following variational free energy
would be the natural choice to minimize:
E˜q(θ)
[
ln ˜q(θ) − ln
(
EqT (s1:T )[p(o1:T , θ |s1:T , a 1:T −1, α )]
)]
,
since this has as minimizer the exact posterior distribution.
22
C Further details
Equivalent formulations of expected free energy
Recall that the expected free energy in equation (2) is given by
G(πt|o1:t, a 1:t−1) = −
(
Eqt(ot+1:T |π t)
[
DKL
(
qt(st+1:T |ot+1:T , π t) ∥ qt(st+1:T |πt)
)]
+ Eqt (ot+1:T |π t)
[
ln pC (ot+1:T )
] )
.
We can derive the equivalent formulation from equation (3) as follows . We ﬁrst
expand the KL divergence term to get
G(πt|o1:t, a 1:t−1) = Eqt(st+1:T ,o t+1:T |π t)
[
ln qt(st+1:T |πt) − ln qt(st+1:T |ot+1:T , π t)
− ln pC (ot+1:T )
]
.
(46)
Using Bayes’ rule we can rewrite
− ln qt(st+1:T |ot+1:T , π t) = − ln qt(ot+1:T |st+1:T , π t) − ln qt(st+1:T |πt)
+ ln qt(ot+1:T |πt).
Plugging this into (46) and using that qt(ot+1:T |st+1:T , π t) = p(ot+1:T |st+1:T )
we get
G(πt|o1:t, a 1:t−1) = Eqt (st+1:T ,o t+1:T |π t)
[
ln p(ot+1:T |st+1:T ) + ln qt(ot+1:T |πt)
− ln pC (ot+1:T )
]
= Eqt(st+1:T |π t)
[
H
[
p(ot+1:T |st+1:T )
] ]
+ DKL
(
qt(ot+1:T |πt) ∥ pC (ot+1:T )
)
,
where the last line is equal to equation (3).
Independence between state factors and observation modali ties
In order to make the computation of state inference more eﬃcient , the agent can
use independencies between diﬀerent state factors and observa tion modalities
(diﬀerent dimensions of state and observation space). More spec iﬁcally, we
assume that given a state, the diﬀerent observation modalities are independent,
which translates to:
p(oτ |sτ ) =
∏
m
p(om
τ |sτ ).
23
We use superscript m and f to denote a speciﬁc observation modalities and
state factors respectively. Furthermore, we assume a certain s tate factor to be
independent of all other state factors in the same and previous tim e step, given
the same state factor in the previous time step and the last action, i.e.:
p(sτ |sτ −1, a τ −1) =
∏
f
p(sf
τ |sf
τ −1, a τ −1).
For a ﬁxed state factor f equation (6) and (7) now become
qt(sf
t ) ∝ p(ot|sf
t )
∑
sf
t−1
p(sf
t |sf
t−1, a t−1)qt−1(sf
t−1) (47)
qt(sf
τ |a1:τ −1) =
∑
sf
τ −1
p(sf
τ |sf
τ −1, a τ −1)qt(sf
τ −1|a1:τ −2),
and equation (10) becomes
qt(s1:T |πt) ∝
t∏
τ =1
∏
m
p(om
τ |sτ ) p(s1)
T∏
τ =2
∏
f
p(sf
τ |sf
τ −1, a τ −1).
Fixed point iteration
Equation (47) involves the distribution p(ot|sf
t ). We can however not access
this directly. To ﬁnd an approximate solution, we can use ﬁxed point it eration
as follows:
q(i+1)
t (sf
t ) ∝
∑
s\f
t
q(i)
t (s\f
t )p(ot|st)
∑
sf
t−1
p(sf
t |sf
t−1, a t−1)qt−1(sf
t−1),
where \f denotes the set of all state factors apart from f.
Softmax function
Deﬁnition 1. Let S = {x(1), . . . , x (n)} be a ﬁnite set and µ : S → R a function.
The softmax function σ is given by
σ(µ(x(i))) = eµ (x(i))
∑
j eµ (x(j)) . (48)
Note that σ(µ(x(i))) > 1 and ∑
S σ(µ(x(i))) = 1. Therefore the softmax function
can be used to turn µ into a probability distribution.
24