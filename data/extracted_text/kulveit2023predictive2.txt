arXiv:2311.10215v1  [cs.CL]  16 Nov 2023
Predictive Minds: LLMs As Atypical Active Inference
Agents
Jan Kulveit1∗ Clem von Stengel1 Roman Leventov2
1 Alignment of Complex Systems Research Group, Center for The oretical Study, Charles University
2 Gaia Consortium
Abstract
Large language models (LLMs) like GPT are often conceptuali zed as passive pre-
dictors, simulators, or even ’stochastic parrots’. W e inst ead conceptualize LLMs
by drawing on the theory of active inference originating in c ognitive science and
neuroscience. W e examine similarities and differences bet ween traditional active
inference systems and LLMs, leading to the conclusion that, currently, LLMs lack
a tight feedback loop between acting in the world and perceiv ing the impacts of
their actions, but otherwise ﬁt in the active inference para digm. W e list reasons
why this loop may soon be closed, and possible consequences o f this including en-
hanced model self-awareness and the drive to minimize predi ction error by chang-
ing the world.
1 Introduction
Foundation models, particularly large language Models (LL Ms) like GPT [3], stand out as the most
advanced general AI systems to date [4]. LLMs are often perce ived as mere predictors, primarily
due to their training objective minimizing their loss on nex t-token prediction [1]. This objective
has led to the assumption that these models are inherently pa ssive: designed to await prompts and
respond without any real understanding of the world or impli cit intention to inﬂuence or interact
with the world. The theory of active inference, originating in cognitive science and neuroscience,
offers an alternative viewpoint [25]. Active inference pos its that biological systems like the human
brain constantly update their internal models based on inte ractions with the environment, striving
to minimize the difference between predicted and actual sen sory inputs (a process also known as
predictive processing) [25]. A fundamental tenet of active inference is that, in biological systems,
this same objective also governs action: the system minimiz es the difference between predicted and
actual sensory input by actively altering its environment.
This paper explores the intriguing possibility that LLMs, w hile predominantly seen as passive enti-
ties, might converge upon active inference agents closer to biological ones. W e explore the parallels
and distinctions between generative models like LLMs and th ose studied in active inference, and
shed light on the emergent control loops that might arise, th e incentives driving these changes, and
the signiﬁcant societal ramiﬁcations of such a shift.
∗ jk@acsresearch.org
Socially Responsible Language Modelling Research (SoLaR) W orkshop at 37th Conference on Neural Infor-
mation Processing Systems (NeurIPS 2023).
2 Background and related work
2.1 Conceptualizing LLMs
There have been various attempts to conceptualize LLMs, exp lain "how they actually work", and
understand them using existing frameworks from a variety of ﬁelds.
One class of conceptualization focuses on the fact that the L M training objective is to minimize
predictive loss, and the fact LLMs are not embodied in a way co mparable to humans, but trained on
large datasets of text from the internet. Bender et al. coine d the term ’stochastic parrots’ and claim
that text generated by an LM is not grounded in communicative intent, any model of the world,
or any model of the reader’s state of mind [1]. In a similar spi rit, using framing from linguistics,
Mahowald et al. conceptualize LLMs as models that are good at formal linguistic competence but
incomplete at functional linguistic competence. Accordin g to this view , LLMs are good models
of language but incomplete models of human thought, good at g enerating coherent, grammatical,
and seemingly meaningful paragraphs of text, but failing in functional competence, which recruits
multiple extralinguistic capacities that comprise human t hought, such as formal reasoning, world
knowledge, situation modeling, and social cognition [16].
These reductionist views of LLMs were subject to considerab le criticism. Mitchell and Krakauer,
surveying the debate, note an opposing faction which argues that these networks truly understand
language, can perform reasoning in a general way, and in a rea l sense understand concepts and cap-
ture important aspects of meaning [21]. Mitchell and Krakau er’s overall conclusion is that cognitive
science is currently inadequate for answering such questio ns about LLMs.
Other conceptualizations of LLMs recognize that the traine d model is a distinct object from the
training process, and so that the nature of the training obje ctive need not be shared by the resulting
artifact. For example, based on experiments with LLMs autor egressively completing complex token
sequences, Mirchandani et al. look at LLMs as general patter n machines, or general sequence
modellers, driven by in-context learning [20]. Others exte nd the ’general sequence modeling’ in the
direction of ’general computation’. For example, Guo et al. propose using natural language as a
new programming language to describe task procedures, maki ng them easily understandable to both
humans and LLMs; they note that LLMs are capable of directly g enerating and executing natural
language programs. In this conceptualization, trained LLM s are natural-language computers [10].
Another conceptualization of LLMs, originating in the AI al ignment community, views LLMs as
general simulators - simulating a learned distribution with various degrees of ﬁdelity, which in the
case of language models trained on a large corpus of text, is t he mechanics underlying the genesis
of the text, and so indirectly the world [12]. This view expli citly assumes that LLMs learn world
models, abstractions, algorithms to better model sequence s. Similarly, Hubinger et. al. discusses
how to understand LLMs as predictive models, and potential r isks from such systems [11].
While not directly aimed at explaining how LLMs work, Lee et a l. provide important context for
this work, focusing on evaluating LLMs in interactive setti ngs, and criticizing the fact that almost
all benchmarks impose the non-interactive view , of models a s passive predictors[13].
2.2 Active inference and predictive processing
Originating in cognitive science and neuroscience, active inference offers a fresh lens through which
to view cognitive processes. At its core, the theory suggest s that living systems, such as animals or
human brains, are in a constant state of updating their inter nal models while acting on the environ-
ment, and both processes should be understood as minimizing the difference between predicted and
actual sensory inputs (or, alternatively, variational fre e energy) [25].
As an all-encompassing framework for building theories of c ognitive systems, active inference
should be compatible not only with process theories of brain function based on neurons [8], but
also with a range of other computational structures (used to represent the world model), and a range
of optimization procedures (used to minimize the differenc e between predicted and actual sensory
inputs). This makes active inference applicable - at least i n principle - not only to humans and
animals, but to a very broad range of systems, including the a rtiﬁcial.
This naturally leads to our attempt to understand LLMs using the active inference framework. Pez-
zulo et al. compare active inference systems and "generativ e AIs" and claim that while both gener-
2
ative AI and active inference are based on generative models , they acquire and use them in funda-
mentally different ways. Living organisms and active infer ence agents learn their generative models
by engaging in purposive interactions with the environment and by predicting these interactions.
The key difference is that learning and meaning is grounded i n sensorimotor experience, providing
biological agents with a core understanding and a sense of ma ttering upon which their subsequent
knowledge and decisions are grounded [27]. In the present wo rk, we argue that this distinction is
not necessarily as fundamental as assumed by Pezzulo et al., and may mostly disappear in the near
future with tighter feedback loop between actions and obser vations.
3 Similarities and differences between active inference sy stems and LLMs
If we look at LLMs in the simulators framework and the active i nference framework, we can note a
number of similarities – or even cases where the AI community and the active inference community
describe the same phenomena using different terminology. I n both cases, systems are described
as equipped with a generative model able to simulate the syst em’s sensory inputs. This model
is updated in such a way that minimises prediction error - the difference between observed and
simulated inputs. This process has been shown to be a form of a pproximate Bayesian inference in
both the active inference [25, 9] and LLM [19, 30] literature s.
3.1 Predictions based on conceptualizing LLMs as special ca se of active inference systems
The active inference conceptualization leads to a number of predictions, some of which are possible
to verify experimentally using interoperability techniqu es.
Possibly the most striking one is obvious in hindsight: acti ve inference postulates that the simple
objective of minimizing prediction error is sufﬁcient for l earning complex world representations,
behaviours and abstraction power, given a learning system w ith sufﬁcient representation capacity.
In predictive processing terminology, we can make an analog y between "perception" and the train-
ing process of LLMs: LLMs are fed texts from the internet and b uild generative models of the input.
Because language is a reﬂection of the world, these models ne cessarily implicitly model not only lan-
guage, but also the broader world. Therefore, we should expe ct LLMs to also learn complex world
representations, abstractions, and the ability to simulat e other systems, (given sufﬁcient representa-
tion capacity). This is in contrast to the conceptualizatio ns referenced in section 2.1, which often
predict that systems trained to predict next input are funda mentally limited, never able to generalize,
unable to comprehend meaning, etc. Recent research has prov ided substantial evidence supporting
the more optimistic view that large language models (LLMs) a re analogous to biological systems at
least in their ability to develop an emergent world model [15 ], rich abstractions and the ability to
predict general sequences [20].
Another topic easier to understand through an active infere nce lens are hallucinations: where LLMs
produce false or misleading information and present it as fa ct [17]. Active inference claims that
human perception is itself ’constrained hallucination’[2 4], where our predictions about sensory in-
puts are constantly synchronized with reality through the e rror signal, propagated backwards. In this
perspective, the data on which LLMs are trained could be unde rstood as sensory input. What’s strik-
ing about these inputs is, in contrast to human sensory input s, the data are not based on perceiving
reality from one speciﬁc perspective in one point of time. Qu ite the opposite: for an intuitive under-
standing of the nature of the data LLMs are trained on, imagin e that your own sensory input was
exhausted by overhearing human conversations, with the cav eat that what you hear every few min-
utes randomly switches between conversations taking place out of order in different years, contexts
and speakers. In contrast to the typical human situation - tr ying to predict what you would hear next -
you would often need to entertain many different hypotheses about the current context. For exampl e,
consider hearing someone say "And she drew her sword and excl aimed ’Heretics must die!’". When
attempting to predict the continuation, it seems necessary to entertain many possibilities - such as
the context being a realistic description of some medieval w orld, or a fantasy tale, or someone play-
ing a video-game. If a biological, brain-based active infer ence system was tasked with predicting
such contextless words, then various fantasy and counterfa ctual worlds would seem as real as actual
current affairs. In this conceptualization, some hallucin ations in LLMs are not some sort of surpris-
ing failure mode of AI systems, but what you should expect fro m a system tasked to predict text
with minimal context, not anchored to some speciﬁc temporal or contextual vantage point. Another
3
striking feature of LLMs in deployment is that outputs of the generative model are not distinguished
from inputs: the model’s output becomes part of its own ’sens ory’ state. Intuitively, this would be
similar to a human unable to distinguish between their own ac tions and external inﬂuences - which
actually sometimes manifests as the psychiatric condition known as ’delusion of control’ [6].
This frame suggests directions to make LLMs less prone to hal lucinations: make the learning context
of the LLM more situated and contextually stable (that is, pr esent training documents in a more
systematic fashion). Additionally, it could help to distin guish between completions by the model
and inputs from the user, similar to the approach of Ortega et al. [22].
3.2 What is an LLM’s actuator?
One suggested fundamental difference between LLMs and acti ve inference systems is the inherent
passivity of LLMs - their inability to act in the world [27]. W e argue that this is mostly a matter of
degree and not a categorical difference. While LLMs don’t ha ve actuators in the physical world like
humans or robots, they still have the ability to act, in the se nse that their predictions do affect the
world. In active inference terminology, LLM outputs could b e understood as the ’action states’ in
the Markov blanket. These states have some effect on the worl d via multiple causal pathways, and
the resulting changes can in principle inﬂuence its ’sensor y states’ - that is, various pieces of text on
the internet and included in the training set. Some clear pat hways:
1. Direct inclusion of text generated by LLM in web pages.
2. Human users asking LLM based assistants for plans and exec uting those plans in the world.
3. T ext input for a huge range of other software systems (LLMs as glue code and so-called
"robotic process automation").
4. Indirect inﬂuence on how humans think about things, e.g. l earning about a concept from
an LLM based assistance.
Some of these effects are already studied in the ML literatur e, but mostly in the context of feedback
loops amplifying bias [29] or as an example of performative p rediction [26]. Here, we propose a
broader interpretation: understanding these effects as ac tions in the sense it takes in active inference.
The nature of the medium through which LLMs "perceive" and "a ct" on the world, which is mostly
text, should not obscure the fundamental similarity to acti ve inference agents. W e agree with Mc-
Gregor’s argument [18] that we should explicitly distingui sh between two notions of embodiment:
on the one hand, whether a system’s body is tangible or not, an d on the other hand, whether a system
is physically situated or not (i.e. whether or not it interac ts physically with any part of the universe).
LLMs are embodied in this second sense. In this view , interac tions of LLMs with users in deploy-
ment are essentially ’actions’. Every token generated in co nversation with users is a micro-action,
and the sum of all of these actions do inﬂuence the world, and s ome of these changes get reﬂected in
the input world (public texts on the internet). So, at least i n principle, LLMs have one open causal
path to bring the world of words closer to their predictions.
3.3 Closing the action loop of active inference
Given that the "not acting on the world" assumption of "LLMs a s passive simulators" does not hold,
the main current difference between LLMs and active inferen ce systems is that LLMs mostly are
not yet able to "perceive" the impacts of their actions. In ot her words, the loop between actions,
external world states, and perceptions is not closed (or any way is not fast). While living organisms
constantly run both perception and action loops, training n ew generations of an LLM happens only
once a year or so - and the impacts of actions of the LLM current ly mostly do not feed back into the
new base model’s training.
What would need to be changed for LLMs to perceive the results of their own actions, and thus close
the “gap” between action and perception? The key piece is tha t the actions taken by an LLM after
deployment, in the sense discussed in section 3.2, feed back into the training process of a future
LLM. Furthermore, it is required that successive LLMs are su fﬁciently similar, and have sufﬁcient
representational capacity, such that they can “self-ident ify” with successive training iterations (see
[14] for a discussion of “the GPT lineage” as an agent).
4
A minimal version of this can occur with in-context learning [5], real-time access to web search (as
with Bing Chat and Google Bard), or a training environment in which the model can take actions
which inﬂuence its reward (such as with GA TO [28], or RLHF [23 ]). However in each of these cases,
there is no feedback from the actions taken during deploymen t and subsequent training of the LLM.
There are three ways we foresee this happening in the near fut ure:
1. The outputs of a model are used to train a next generation mo del, e.g. through model
outputs being published on the internet and not ﬁltered out d uring data curation.
2. The data collected from interactions with the models, suc h as from user conversations with
a chatbot, are used in ﬁne-tuning future versions of the same model.
3. Continuous online learning, in which the outputs of a mode l and user responses are directly
used as a training signal to update the model.
Where these routes are in order of increasingly tight feedba ck loops (where "tighter" means on a
shorter timescale, with consecutive generations sharing m ore of the earlier model’s weights, and
with the interaction forming a larger percentage of trainin g data - increased bandwidth).
W e expect that there will be active effort by developers to cl ose the feedback gap and make the action
loop more prominent because of commercial incentives to mak e LLMs better at quickly adapting
to new information, acting independently, or otherwise age nt-like. Active inference as a theory of
agency predicts closing the loop would naturally cause LLMs to become more agentic, emergently
learning to change the world to more closely match the intern al states (and thus predictions) of
LLMs.
4 Implications of active LLMs
The evolution of LLMs into active agents would carry profoun d societal implications and risks.
Using active inference as a theoretical framework to make pr edictions about such Active LLMs is a
fruitful direction. W e focus on emergence of increased self -awareness.
4.1 Enhancing model self-awareness
A straightforward prediction of the active inference frame in this paper is that the described tighten-
ing of the feedback loop is likely to to augment and increase m odels’ self-awareness. A recent study
of self-awareness [2] in LLMs emphasizes the importance of s elf-awareness from a safety perspec-
tive, but this work is overall uncertain about what stage of L LM training will be more important for
the emergence of situational awareness in future models, an d focuses on evaluating sophisticated
out-of-context reasoning as a proxy of self-awareness. In c ontrast, the active inference literature
emphasizes the importance of observing the consequences of one’s own actions for developing func-
tional self-awareness [7, p. 112].
As these loops tighten, we expect models to enhance in self-a wareness by acquiring more informa-
tion about themselves and observing the repercussions of th eir actions in the environment. Consider
the self-localization problem discussed by [2]. Construct a thought experiment in which a human
faces a similar self-localization problem: assume, instea d of one’s usual sensory inputs, that the
human is hooked to a stream of dozens of security cameras. T o i ncrease the human’s ability to
self-localize is to equip them with more information about t heir own appearance, for example, hair
colour. A different, highly effective way to self-localize is via performing an action, for example by
waving a hand.
5 Conclusions
By examining the learning objectives and feedback loops of a ctive inference, in comparison to those
of LLMs, we posited that LLMs can be understood as an unusual e xample of active inference agents
with a gap in their feedback loop from action to perception. I n this framework, their transition to
acting in the world as living organisms do depends on their cl osing the gap between interacting (with
users) and training.
5
The potential metamorphosis of LLMs into active LLMs could l ead to more adaptive and self-aware
AI systems, bearing substantial societal implications. Th e densiﬁcation and acceleration of feedback
loops could augment not only models’ self-awareness but als o lead to a drive to modify the world -
driven purely by the prediction error minimization objecti ve, without intentional effort to make the
models more agent-like.
6 Acknowledgements
W e thank Rose Hadshar and Gavin Leech for help with writing an d editing, and T omáš Gaven ˇciak,
Simon McGregor and Nicholas Kees Dupuis for valuable discus sions. JK and CvS were supported
by PRIMUS grant from Charles University. GPT4 was used for ed iting the draft, simulating readers,
and title suggestions.
References
[1] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On
the dangers of stochastic parrots: Can language models be to o big? In Proceedings of the 2021
ACM conference on fairness, accountability, and transpare ncy, pages 610–623, 2021.
[2] Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, M ax Kaufmann, Meg T ong, T omasz
Korbak, Daniel Kokotajlo, and Owain Evans. T aken out of cont ext: On measuring situational
awareness in llms. arXiv preprint arXiv:2309.00667 , 2023.
[3] T om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J ared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Aman da Askell, et al. Language mod-
els are few-shot learners. Advances in neural information processing systems , 33:1877–1901,
2020.
[4] Sébastien Bubeck, V arun Chandrasekaran, Ronen Eldan, J ohannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Y in T at Lee, Y uanzhi Li, Scott Lundberg, et al. Sparks of artiﬁcial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.
[5] Damai Dai, Y utao Sun, Li Dong, Y aru Hao, Shuming Ma, Zhifa ng Sui, and Furu W ei. Why can
gpt learn in-context? language models implicitly perform g radient descent as meta-optimizers.
arXiv preprint arXiv:2212.10559 , 2022.
[6] Paul C Fletcher and Chris D Frith. Perceiving is believin g: a bayesian approach to explaining
the positive symptoms of schizophrenia. Nature Reviews Neuroscience , 10(1):48–58, 2009.
[7] Karl Friston. A free energy principle for a particular ph ysics. arXiv preprint arXiv:1906.10184 ,
2019.
[8] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Phi lipp Schwartenbeck, and Giovanni Pez-
zulo. Active inference: a process theory. Neural computation , 29(1):1–49, 2017.
[9] Karl Friston, Philipp Schwartenbeck, Thomas FitzGeral d, Michael Moutoussis, Timothy
Behrens, and Raymond J. Dolan. The anatomy of choice: active inference and agency. Fron-
tiers in Human Neuroscience , 7, 2013.
[10] Y iduo Guo, Y aobo Liang, Chenfei Wu, W enshan Wu, Dongyan Zhao, and Nan Duan. Learning
to program with natural language. arXiv preprint arXiv:2304.10464 , 2023.
[11] Evan Hubinger, Adam Jermyn, Johannes Treutlein, Rubi H udson, and Kate W oolverton. Con-
ditioning predictive models: Risks and strategies. arXiv preprint arXiv:2302.00805 , 2023.
[12] Janus. Simulators, 2023. https://generative.ink/po sts/simulators/ Accessed: 2023-10-04.
[13] Mina Lee, Megha Srivastava, Amelia Hardy, John Thickst un, Esin Durmus, Ashwin Paran-
jape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frie da Rong, et al. Evaluating human-
language model interaction. arXiv preprint arXiv:2212.09746 , 2022.
[14] Roman Leventov. How evolutionary lineages of llms can p lan their own future and
act on these plans, 2023. https://www .lesswrong.com/post s/ddR8dExcEFJKJtWvR/how-
evolutionary-lineages-of-llms-can-plan-their-own-future Accessed: 2023-10-04.
[15] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda V iégas , Hanspeter Pﬁster, and Martin
W attenberg. Emergent world representations: Exploring a s equence model trained on a syn-
thetic task. arXiv preprint arXiv:2210.13382 , 2022.
6
[16] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwi sher, Joshua B T enenbaum, and
Evelina Fedorenko. Dissociating language and thought in la rge language models: a cognitive
perspective. arXiv preprint arXiv:2301.06627 , 2023.
[17] Potsawee Manakul, Adian Liusie, and Mark J. F . Gales. Se lfcheckgpt: Zero-resource
black-box hallucination detection for generative large la nguage models. arXiv preprint
arXiv:2303.08896 , 2023.
[18] Simon McGregor. Is chatgpt really disembodied? In ALIFE 2023: Ghost in the Machine:
Proceedings of the 2023 Artiﬁcial Life Conference . MIT Press, 2023.
[19] Chris Mingard, Guillermo V alle-Pérez, Joar Skalse, an d Ard A. Louis. Is sgd a bayesian
sampler? well, almost. Journal of Machine Learning Research , 22(79):1–64, 2021.
[20] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichte r, Danny Driess, Montserrat Gonzalez
Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large lan guage models as general
pattern machines. arXiv preprint arXiv:2307.04721 , 2023.
[21] Melanie Mitchell and David C Krakauer. The debate over u nderstanding in ai’s large language
models. Proceedings of the National Academy of Sciences , 120(13):e2215907120, 2023.
[22] Pedro A Ortega, Markus Kunesch, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Joel
V eness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Per olat, et al. Shaking the foundations:
delusions in sequence models for interaction and control. arXiv preprint arXiv:2110.10819 ,
2021.
[23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carro ll W ainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Jo hn Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, P eter W elinder, Paul F . Chris-
tiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback. In Advances in Neural Information Processing Systems , volume 35. NeurIPS, 2022.
[24] Thomas Parr and Giovanni Pezzulo. Understanding, expl anation, and active inference. Fron-
tiers in Systems Neuroscience , 15, 2021.
[25] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle
in mind, brain, and behavior . MIT Press, 2022.
[26] Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dünner , and Moritz Hardt. Performative pre-
diction. In International Conference on Machine Learning , pages 7599–7609. PMLR, 2020.
[27] Giovanni Pezzulo, Thomas Parr, Paul Cisek, Andy Clark, and Karl Friston. Generating mean-
ing: Active inference and the scope and limits of passive ai. 2023.
[28] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Góm ez Colmenarejo, Alexander Novikov,
Gabriel Barth-maron, Mai Giménez, Y ury Sulsky, Jackie Kay, Jost T obias Springenberg, T om
Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Hee ss, Y utian Chen, Raia Hadsell,
Oriol V inyals, Mahyar Bordbar, and Nando de Freitas. A gener alist agent. T ransactions of
Machine Learning Research , 2022.
[29] Rohan T aori and T atsunori Hashimoto. Data feedback loo ps: Model-driven ampliﬁcation of
dataset biases. In International Conference on Machine Learning , pages 33883–33920. PMLR,
2023.
[30] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and T engyu Ma. An explanation of in-
context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021.
7
This figure "actmod.png" is available in "png"
 format from:
http://arxiv.org/ps/2311.10215v1
This figure "gapmod.png" is available in "png"
 format from:
http://arxiv.org/ps/2311.10215v1