Applications of the Free Energy Principle to
Machine Learning and Neuroscience
Beren Millidge
NI VE
U R
S
E I
H T
T Y
O H
F G
R
E
D I N B U
Doctor of Philosophy
Institute for Adaptive and Neural Computation
School of Informatics
University of Edinburgh
2021
1202
nuJ
03
]IA.sc[
1v04100.7012:viXra
Abstract
In this thesis, we explore and apply methods inspired by the free energy principle to
two important areasin machinelearningand neuroscience. Thefree energyprinciple
is a general mathematical theory of the necessary information-theoretic behaviours
of systems which maintain a separation from their environment. A core postulate of
the theory is that complex systems can be seen as performing variational Bayesian
inferenceandminimizinganinformation-theoreticquantitycalledthevariationalfree
energy. The freeenergy principleoriginatedin, andhas beenextremely influentialin
theoreticalneuroscience,havingspawnedanumberofneurophysiologicallyrealistic
processtheories,andmaintainingcloselinkswithBayesianBrainviewpoints.
Thethesisissplitintothreemainpartswhereweapplymethodsandinsightsfromthe
freeenergyprincipletounderstandquestionsfirstinperception,thenaction,andfinally
learning. Specifically, in the first section, we focus onthe theory of predictive coding,
a neurobiologically plausible process theory derived from the free energy principle
under certain assumptions, which argues that the primary function of the brain is to
minimizepredictionerrors. Wefocusonscalinguppredictivecodingarchitecturesand
simulate large-scale predictive coding networks for perception on machine learning
benchmarks;weinvestigatepredictivecoding’srelationshiptootherclassicalfiltering
algorithms,andwedemonstratethatmanybiologicallyimplausibleaspectsofcurrent
modelsofpredictivecodingcanberelaxedwithoutundulyharmingtheperformance
ofpredictivecodingmodelswhichallowsforapotentiallymoreliteraltranslationof
predictivecodingtheoryintocorticalmicrocircuits.
Inthesecondpartofthethesis,wefocusontheapplicationofmethodsderivingfromthe
freeenergyprincipletoaction. Westudytheextensionofmethodsof‘activeinference’,
aneurobiologicallygroundedaccountofactionthroughvariationalmessagepassing,
to utilize deep artificial neural networks, allowing these methods to ‘scale up’ to be
competitivewithstateoftheartdeepreinforcementlearningmethods. Additionally,we
i
showthattheseactiveinferenceinspiredmethodscanbringconceptualclarityandnovel
perspectivesto deepreinforcement learning. We showhow active inferencereveals the
importanceof deepgenerative models andmodel-based planningfor adaptive action,
as wellas information-seekingexploration whicharises undera unifiedmathematical
frameworkfromactiveinference. Finally,weprovideaunifiedmathematicallyprinci-
pledframeworkforunderstandingandderivingmanyinformation-seekingexploration
objectives through the lens of a dichotomy between ‘evidence’ and ‘divergence’ ob-
jectives. We show that this distinction is crucial for understanding and relating the
many exploratory objectives in both the reinforcementlearning, active inference, and
cognitivesciencecommunitiesandthatthisprovidesageneralmathematicalframework
forspecifyingtheobjectivesunderlyingintelligent,adaptivebehaviour.
Finally,wefocusonapplicationsofthefreeenergyprincipletoquestionsoflearning
where we attempt to understand how credit assignment can take place in the brain.
First,wedemonstratethat,undercertainconditions,thepredictivecodingalgorithmcan
closelyapproximatethebackpropagationoferroralgorithmalongarbitrarycomputation
graphs,whichunderliesthetrainingofessentiallyallcontemporarymachinelearning
architectures,thusindicating apotentialpathto the directimplementationofmachine
learningalgorithmsinneural circuitry. Finally,we explore otheralgorithmsforbiologi-
callyplausiblecreditassignmentinthebrain,andpresentActivationRelaxation,anovel
algorithm which can approximatebackprop usingonly locallearning rules whichare
substantiallysimplerthanthosenecessaryforpredictivecoding. Weadditionallyshow
thatthesomerelaxationsthatapplytopredictivecoding,alsoworkfortheactivation
relaxationalgorithm,thusproducinganextremelyelegantandeffectivealgorithmfor
localapproximationstobackpropinthebrain.
In sum, we believe we have demonstrated the theoretical utility of the free energy
principle,bydemonstratinghowmethodsinspiredbyitcaninterfaceproductivelywith
otherfields,specificallyneuroscienceandmachinelearning,todevelopandimprove
ii
existing methods, as well as inspire novel advances, in all three areas of perception,
action,andlearning. Moreover,throughoutthisthesis,wedemonstrateimplicitly,the
theoretical benefit brought about by the FEPs unified treatment of these seemingly
disparateprocesses,undertherubricoffreeenergyminimization.
iii
Acknowledgements
Iwouldlike tothankmysupervisor,Richard Shillcock, forallhis helpandadviceover
theyears;forhisgivingmefreedomtoworkonthetopicsinthisthesiseventhoughthey
did notalign withhis plannedresearch trajectory, and forhis perseverance inhandling
myendlessdrafts. Secondly,ahugethankstoChristopherLBuckley,AlecTschantz,
andAnil Sethforhosting mefor ayearat theSacklerCentre andtheEvolutionary and
AdaptiveSystemsGroup(EASY)attheUniversityofSussex. Youhavealltaughtme
somuchaboutthecraftofresearch,andwithoutyourmentorshipandguidance,Iwould
notbehalftheresearcherIamtoday. IwouldalsoliketothankConorHeinsformany
stimulatingconversationsaroundthefreeenergyprincipleandrelatedtopics. Finally,
andaboveall, Iwouldlike tothankmywife, MycahBanks,for herunendingloveand
supportthroughoutthisentireprocess;heraidwithproofreadingandfigurepreparation
for many papers, and her sacrifice inputting up with ahusband who always hassome
moreresearchtodo. Withoutyou,noneofthiswouldbepossible. Thankyou,Mycah.
iv
Declaration
Ideclarethatthisthesiswascomposedbymyself,thattheworkcontainedhereinismy
ownexceptwhereexplicitlystatedotherwiseinthetext,andthatthisworkhasnotbeen
submittedforanyotherdegreeorprofessionalqualificationexceptasspecified.
(BerenMillidge)
v
Table of Contents
1 Introduction 1
1.1 ThesisOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 StatementofContributions . . . . . . . . . . . . . . . . . . . . . . . 7
1.2.1 IncludedinThesis . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.2 NotIncludedintheThesis . . . . . . . . . . . . . . . . . . . 13
2 TheFreeEnergyPrinciple 16
2.1 HistoryandLogicalStructure . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.3 MarkovBlankets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.4 VariationalInference . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.5 IntrinsicandExtrinsicinformationgeometries . . . . . . . . . . . . . 37
2.6 Self-OrganizationandVariationalInference . . . . . . . . . . . . . . 40
2.7 TheExpectedFreeEnergyandActiveInference . . . . . . . . . . . . 47
2.8 PhilosophicalStatusoftheFEP . . . . . . . . . . . . . . . . . . . . . 50
2.9 DiscussionofAssumptionsrequiredfortheFEP . . . . . . . . . . . . 57
2.9.1 AssumptionsontheFormoftheLangevinDynamics . . . . . 60
2.9.2 TheMarkovBlanketCondition . . . . . . . . . . . . . . . . 61
2.9.3 AssumptionsofthefreeenergyLemma . . . . . . . . . . . . 63
2.10 ActiveInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
2.10.1 DiscreteState-spacemodelsandPerception . . . . . . . . . . 71
vi
2.10.2 ActionSelectionandtheExpectedFreeEnergy . . . . . . . . 72
2.11 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3 PredictiveCoding 76
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
3.2 PredictiveCoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.3 Hierarchicalpredictivecoding . . . . . . . . . . . . . . . . . . . . . 83
3.3.1 DynamicalPredictivecoding . . . . . . . . . . . . . . . . . . 90
3.4 PredictiveCodingandKalmanFiltering . . . . . . . . . . . . . . . . 101
3.4.1 TheKalmanFilter . . . . . . . . . . . . . . . . . . . . . . . 105
3.4.2 PredictiveCodingasKalmanFiltering . . . . . . . . . . . . . 107
3.4.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.5 RelaxedPredictiveCoding . . . . . . . . . . . . . . . . . . . . . . . 122
3.5.1 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
3.5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4 ScalingActiveInference 144
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.1.1 ReinforcementLearning . . . . . . . . . . . . . . . . . . . . 145
4.1.2 DeepReinforcementLearning . . . . . . . . . . . . . . . . . 154
4.1.3 Model-freevsModel-based . . . . . . . . . . . . . . . . . . 156
4.1.4 ExplorationandExploitation . . . . . . . . . . . . . . . . . . 159
4.1.5 ControlasInference . . . . . . . . . . . . . . . . . . . . . . 163
4.2 DeepActiveInference . . . . . . . . . . . . . . . . . . . . . . . . . 168
4.2.1 Model-Free: ActiveInferenceasVariationalPolicyGradients 171
4.2.2 Model-based: ReinforcementLearningthroughActiveInference189
vii
4.2.3 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . 199
4.2.4 IterativeandAmortisedInference . . . . . . . . . . . . . . . 201
4.2.5 ControlasHybridInference . . . . . . . . . . . . . . . . . . 206
4.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5 TheMathematicalOriginsofExploration 218
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.2 OriginsoftheExpectedFreeEnergy . . . . . . . . . . . . . . . . . . 221
5.2.1 ControlasInferenceandActiveInference . . . . . . . . . . . 228
5.3 EvidenceandDivergenceObjectives . . . . . . . . . . . . . . . . . . 234
5.3.1 ControlasInference . . . . . . . . . . . . . . . . . . . . . . 241
5.3.2 KLControl . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
5.3.3 ActiveInference . . . . . . . . . . . . . . . . . . . . . . . . 243
5.3.4 ActionandPerceptionasDivergenceMinimization . . . . . . 245
5.4 Towards a General Theory of Mean-Field Variational Objectives for
Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
5.4.1 EncodingValue . . . . . . . . . . . . . . . . . . . . . . . . . 250
5.4.2 GeneralGraphicalModels . . . . . . . . . . . . . . . . . . . 261
5.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
6 CreditAssignmentintheBrain 268
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
6.1.1 BackpropagationintheBrain . . . . . . . . . . . . . . . . . 270
6.2 PredictiveCodingApproximatesBackpropAlongArbitraryComputa-
tionGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
6.2.1 MethodsandResults . . . . . . . . . . . . . . . . . . . . . . 286
6.2.2 RNNandLSTM . . . . . . . . . . . . . . . . . . . . . . . . 292
6.3 InterimDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
6.4 ActivationRelaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 305
viii
6.4.1 MethodandResults . . . . . . . . . . . . . . . . . . . . . . 309
6.4.2 LooseningConstraints . . . . . . . . . . . . . . . . . . . . . 311
6.4.3 InterimDiscussion . . . . . . . . . . . . . . . . . . . . . . . 318
6.5 Three-FactorLearningRulesandaDirectImplementation . . . . . . 322
6.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
7 Discussion 333
7.1 Question1: ScalingActiveInference . . . . . . . . . . . . . . . . . . 337
7.2 Question2: TheMathematicalOriginsofExploration . . . . . . . . . 345
7.3 Question3: CreditAssignmentintheBrain . . . . . . . . . . . . . . 352
7.4 ClosingThoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
A DerivationofKalmanFilteringEquationsfromBayes’Rule 367
B AppendixB:EquationsoftheLSTMcell 370
C AppendixC:PredictiveCodingUndertheLaplaceApproximation 372
References 375
ix
List of Figures
2.1 ThelogicalflowoftheargumentoftheFEPfromtheinitialformulation
tothecrucialapproximateBayesianinferencelemma. Webeginwitha
settingofrandomLangevinstochasticdynamicalsystems,whichpos-
sessanon-equilibrium-steadystate. ByapplyingtheAodecomposition,
we can understand the dynamics in terms of a gradient descent upon
thesurprisal. UpontheadditionofaMarkovBlanketpartition,wecan
expresssubsets intermsoftheirownmarginalflows viathemarginal
flow lemma. If we then identify the internal states as parametrizing
avariationaldistributionovertheexternalstates,we caninterpretthe
marginalflowonthesurprisalasaflowonthevariationalfreeenergy,
undertheLaplaceapproximation. . . . . . . . . . . . . . . . . . . . 20
x
2.2 TheintuitionbehindtheMarkovBlanketpartition. Thebrain(orbacil-
lus) consists of internal states µ which are separated from the outside
world(externalstatesηbytheblanketstatesb,whichcanthemselves
bepartitionedintosensorystatess,representingthesensoryepithelia,
andwhich aredirectlyinfluenced byexternalstates, andactivestatesa
representingtheorganismseffectorsandwhicharedirectlyinfluenced
by internal states, and act on external states. We see that perception
concernstheminimizationoffree energyoftheinternalstates,while
action concerns the minimization of the expected free energy of the
activestates. FigureoriginallyappearedinFriston(2019a) . . . . . . 32
3.1 MNISTdigits inthetraining setrecreatedbythe network. Toprowthe
actualdigits,bottomrow,thepredictivereconstructions. . . . . . . . 86
3.2 UnseenMNISTdigitsinthetestsetrecreatedbythenetwork. Toprow
theactualdigits,bottomrow,thepredictivereconstructions. . . . . . . 87
3.3 Images of hallucinated digits "dreamt" by the network. These were
generated by sampling the latent space around the representations of
someexemplardigitsinthelatentspace,andthenlettingthepredictive
codingnetworkgenerateitspredictionfromthechosenlatentstate. . . 87
3.4 APCAclusteringplotofthevaluesoftestMNISTdigitsinthelatent
space. Eventhoughthe20dimensionallatentspacehasbeenreduced
down to two, clusters are still visible. For instance, all the 1s are
clustered in the top left corner. We thus see that predictive coding
appears to be a powerful and fully unsupervised learning algorithm,
capable of separating out distinct digits in the latent space, despite
not being trained with any label information at all – and purely on
reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
xi
3.5 TestsetCIFARdigitsreconstructedbythenetwork. Thefirstofthetwo
linesistheimageandthesecondisthereconstruction. Thenetworkis
extremelygoodatreconstructingCIFARimages. . . . . . . . . . . . 89
3.6 The CIFAR model interpolating between a horse and a cat. Read the
imageslefttorighttoptobottom-liketext. Theinterpolationisdone
bysteppinginthelatentspacefromtherepresentationofthefirstimage
inthedirectionoftheseconduntilitisreached. . . . . . . . . . . . . 90
3.7 Predictionerrorsandpredictionforsimpletoydynamicalmodels. The
task of the dynamical predictive coding model is to learn to predict
a sinewave using only the first two dynamical orders – so including
position, velocity, and acceleration. The model starts from randomly
initialized parameters. We see that the model very quickly learns to
matchthe incomingsinewaveobservations withonlyminimal errorat
thebeginning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
3.8 Dynamical models tested on more challenging sawtooth and square
waveinputs. Themodelwasrandomlyinitializedandonlymodelledthe
firsttwodynamicalorders(soposition,velocity,acceleration). Apart
fromabriefinitialperiodofuncertainty,themodelrapidlylearnedto
predictthesemorechallengingwaveshapes. . . . . . . . . . . . . . . 99
3.9 The training graphsof thefull constructmodel. It cansuccessfully pre-
dictthefirstthreetemporalderivativesofasinewave,andalsominimise
prediction error up to multiple hierarchical layers. The full construct
modelwasrandomlyinitialized,andlearntbothparametersandinferred
statespurely online– thusachievinga‘double deconvolution’(Friston,
Trujillo-Barreto,&Daunizeau,2008). . . . . . . . . . . . . . . . . . 100
xii
3.10 The true dynamics, control input, and observations generated by a
random C matrix. These are the source of truth that the predictive
coding Kalman filter tries to approximate. The observations differ
substantiallyfromthetruedynamicsduetotherandomCmatrix,which
makesthe inference problemfaced bythe predictive coding filtermuch
more challenging,since itmust de-scramblethe observations toinfer
thetruedynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
3.11 Tracking performance of our gradient filter compared to the true val-
uesandtheanalyticalKalmanFilter.Weshowthetrackingover2000
timesteps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.12 Here, we zoom in on 100 timestep period to demonstrate tracking
performanceinminiatureandtheeffectoffewgradientupdates. Inthis
case,weplottedtheestimatesafteronly5steps. Eventwostepsoften
suffice(withalargelearningrate)toprovideveryaccurateestimates . 115
3.14 FilteringperformanceforadaptivelylearningboththeAandBmatrices
inconcert(secondrow). Thefilteringbehaviouroftheoftherandomly
initialized filters without adaptive learning is also shown. Importantly,
withlearningtheestimatedposition,velocity,andaccelerationstrack
theirtruevaluespreciselywhilethe estimateswithoutlearningandjust
randomlyinitializedAorAandBmatricesrapidlydivergefromthetruth.118
3.15 Very poor tracking behaviour with a learnt C matrix. This is despite
thefactthattheBayesianlossfunctionrapidlydecreasestoaminimum.
This shows that the filter can find a prediction-error minimizing "so-
lution"whichalmostarbitrarilydepartsfromrealityiftheC-matrixis
randomized. PanelDshowsthelosscomputedbythenetworkwhich
rapidly declines, even while the predictions rapidly diverge from the
truth(Panelsa,b,c) . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
xiii
3.16 The weight transportproblem and our solution. On the leftis the stan-
dard predictive coding network architecture. Our diagram represents
thepredictionerrorsεofonelayerreceivingpredictionsandtransmitted
prediction errors to the value neurons of the layer above. Prediction
errors are transmitted upwards using the same weight matrix θT as
thepredictionsaretransmitteddownwards. Ontheright,oursolution
eschews this biological implausibility by proposing a separate set of
backwards weight θ˜ (in red), which are learned separately using an
additionalHebbianlearningrule. . . . . . . . . . . . . . . . . . . . . 129
3.17 Testaccuracyofpredictivecodingnetworkswithbothlearntbackwards
weights, and the ideal weight transposes with both relu and tanh ac-
tivation functions on the MNIST and FashionMNIST datasets. Both
networksobtainalmostidenticallearningcurves,thussuggestingthat
learntbackwardsweightsallowforasolutiontotheweight-transport
problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.18 Testaccuracy ofpredictivecoding networkswithand withoutthenon-
linear derivative term, using relu and tanh activation functions on the
MNIST and FashionMNIST datasets. We find that on the MNIST
datasetperformanceissimilar,whileontheFashionMNISTdatasetand
thetanhactivationfunction,thelackofthenonlinearderivativeappears
toslightlyhurtperformance. . . . . . . . . . . . . . . . . . . . . . . 132
3.19 The error-connectivity problem and our solution. On the left, the
biologically implausible one-to-one connectivity between value and
error nodes required by the standard predictive coding theory. On
the right, our solution to replace these one to one connections by a
fullyconnectedconnectivitymatrixψ. BylearningψwithaHebbian
learning rule we are able to achieve comparable performance to the
one-to-oneconnectionswithafullydispersedconnectivitymatrix. . . 133
xiv
3.20 Testaccuracyofpredictivecodingnetworkswithandwithoutlearnable
error connections for both relu and tanh activation functions on the
MNISTandFashionMNISTdatasets. Weseethat,interestingly,using
learnterrorweightsdecreasedperformanceonlywiththetanhbutnot
therelunonlinearity,andthenonlyslightlyintheFashionMNISTcase. 135
3.21 Schematic representations of the architecture across two layers of a.)
the standard predictive coding architecture and b.) The fully relaxed
architecture. Importantly,thisarchitecturehasfullconnectivitybetween
allnodesandalsonon-symmetricforwardandbackwardsconnectivity
inallcases. Ineffect,thisarchitectureonlymaintainsabipartitegraph
betweenerrorandvalueneurons,butnootherclearstructure . . . . . 136
3.22 Test accuracy standard and fully relaxed predictive coding networks
(the combined algorithm), for both relu and tanh activation functions
on the MNIST and FashionMNIST datasets. We see that, interest-
ingly,performance is degraded in all cases and that the relu networks
areespeciallyaffected–withcatastrophicdeclinesinperformanceto
become almost random. The reasons for this are currently unknown
andwillbeinvestigatedinfuturework. . . . . . . . . . . . . . . . . . 137
4.1 Graphicalmodelforcontrolasinference,withoptimalityvariablesΩ.
Otherthantheoptimalityvariables,thegraphicalmodeltakestheform
ofaMarkovDecisionProcesswithactionsaandstatess. Thestateof
aspecifictimestepdependsontheactionandstateofthelasttime-step.
By writing out an explicit graphical model like this, we can apply a
whole field’s worth of inference algorithms on graphical models like
thistosolvecontrolproblems. . . . . . . . . . . . . . . . . . . . . . 163
xv
4.2 ComparisonofthemeanrewardobtainedbytheActiveInferenceagent
comparedtotworeinforcementlearningbaselinealgorithms–Actor-
Critic and Q learning on the CartPole environment. We demonstrate
the learning curves over 2000 episodes, averaged over 5 different seeds.
500 is the maximum possible reward. We see that while the vanilla
actor critic agent initially learns faster, over a long time horizon, the
activeinferenceagentoutperformsit–andbothperformbetterthanthe
vanillaQlearningagent. . . . . . . . . . . . . . . . . . . . . . . . . 181
4.3 ComparisonofActiveInference with standardreinforcementlearning
algorithms on the Acrobot environment. Here we see the learning
curves plotted over five seeds over 20000 episodes. The maximum
possible reward in this environment was 0, so no agents are optimal.
Weseeagainthatactiveinferenceoutperformstheothertwomethods
consistently. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
4.4 ComparisonofActiveInferencewithreinforcementlearningalgorithms
on the Lunar-Lander environment. Learning curves presented over
15000 episodes, averaged over 5 seeds. Here the vanilla policy gra-
dient algorithm strongly outperforms the others, for unclear reasons,
although active inference is still comparable with the other standard
reinforcementlearningalgorithms. Ascoreof200isoptimal. . . . . . 183
4.5 WecomparethefullActiveInferenceagent(entropyregularization+
transitionmodel) withan ActiveInference agentwithout thetransition
model, and without both the entropy term and the transition model).
Weseethatwhileremovingthetransitionmodelappearstohavelittle
effect, removing the entropy regularisation term substantially impairs
performance. Thismaybeduetotheentropytermaidinginstavingoff
policycollapse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
xvi
4.6 ComparisonoftherewardsobtainedbythefullyablatedActiveInfer-
enceagentwithstandardreinforcement-learningbaselinesofQ-learning
and Actor-Critic on the CartPole environment. Learning curves are
averaged over 5 seeds. We see that despite being fully ablated, the
activeinferenceagentcontinoustoperformcomparablywithstandard
reinforcementlearningagents. . . . . . . . . . . . . . . . . . . . . . 185
4.7 (A) Mountain Car: Average return after each episode on the sparse-
rewardMountainCartask. Ouralgorithmachievesoptimalperformance
inasingletrial. (B)CupCatch: Averagereturnaftereachepisodeon
thesparse-rewardCupCatchtask. Here,resultsamongstalgorithmsare
similar,withallagentsreachingasymptoticperformanceinaround20
episodes. (C&D)HalfCheetah: Averagereturnaftereachepisodeon
thewell-shapedHalfCheetahenvironment,fortherunningandflipping
tasks, respectively. Wecompare ourresults tothe average performance
ofSACafter100episodeslearning, demonstratingouralgorithmcan
perform successfully in environments which do not require directed
exploration. Each line is the mean of 5 seeds and filled regions show
+/-standarddeviation. . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4.8 (A & B) Mountain Car state space coverage: We plot the points in
state-spacevisitedbytwoagents-onethatminimizesthefreeenergyof
theexpectedfuture(FEEF)andonethatmaximisesreward. Theplots
arefrom20episodesandshowthattheFEEFagentsearchesalmostthe
entirety of state space, while the reward agent is confined to a region
thatcanbereachedwithrandomactions. (C)AntMazeCoverage: We
plotthepercentageofthemazecoveredafter35episodes,comparing
the FEEF agent to an agent acting randomly. These results are the
averageof4seeds. . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
xvii
4.9 OverviewofclassicRLandcontrolalgorithmsinourscheme. Standard
model-freeRLcorrespondstoamortisedpolicies,planningalgorithms
areiterativeplanning,andcontroltheoryinfersiterativepolicies. The
amortisedplansquadrantisempty,perhapssuggestingroomfornovel
algorithms. The position of the algorithms within the quadrant is not
meaningful. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
4.10 (a - c): Amortised predictions of q (a|s;θ) are shown in red, where •
φ
denote the expected states, shaded areas denote the predicted actions
varianceateachstep,andtheexpectedtrajectoryrecoveredbyiterative
inferenceisshowninblue. At theonsetoflearning(a),theamortised
predictions are highly uncertain, and thus have little influence on the
final approximate posterior. As the amortised model f (·) learns (b),
φ
thecertaintyoftheamortisedpredictionsincrease,suchthatthefinal
posteriorremainsclosertotheinitialamortisedguess. Atconvergence,
(c), the iterative phase of inference has negligible influence on the
final distribution, suggesting convergence to a model-free algorithm.
(d) Here, we compare our algorithm to its constituent components –
thesoft-actorcritic(SAC)andanMPCalgorithmbasedonthecross-
entropymethod(CEM).Theseresultsdemonstratethatthehybridmodel
significantlyoutperformsbothofthesemethods. . . . . . . . . . . . . 212
xviii
5.1 Numerical illustration of optimizing a multimodal desired distribu-
tion with an Evidence objective (Panel A) vs a Divergence Objective
(panel B). The desire distribution consistedof the sum of two univari-
ate Gaussian distributions, with means of 1 and 4 and variances of
1 and 0.4 respectively. We then optimized an expected future distri-
bution, which also consisted of two Gaussians with free means and
variancesusingbothanEvidenceandaDivergenceobjective. Ascan
be seen, optimizing the Evidence Objective results in the agent fit-
ting the predicted future density entirely to an extremely sharp peak
around the mode of the desired distribution. Conversely, optimizing
a divergence objective leads to a precise match of the predicted and
desireddistributions(panelBshowsthe twodistributions almostpre-
cisely on top of one another). As a technical note, to be able to see
both the evidence and deisre distributions on the same scale, for the
evidenceobjectivethepredicteddistributionisnormalizedbutthede-
sired distribution is not. Code for these simulations can be found at:
https://github.com/BerenMillidge/origins_information_seeking_exploration.237
6.1 Top: Backpropagation on a chain. Backprop proceeds backwards
sequentially and explicitly computes the gradient at each step on the
chain. Bottom: Predictivecodingonachain. Predictions,andprediction
errorsareupdatedinparallelusingonlylocalinformation. Importantly,
whiletheoriginalcomputationgraph(blacklines)mustbeaDAG,the
augmentedpredictivecodinggraphiscyclic,duetothebackwards(red)
predictionerrorconnections. . . . . . . . . . . . . . . . . . . . . . . 283
√
6.2 Top: Thecomputationgraphofthenonlineartestfunctionv =tan( θv )+
L 0
sin(v2). Bottom: graphsofthelogmeandivergencefromthetruegra-
0
dient and the divergence for different learning rates. Convergence to
theexactgradientsisexponentialandrobusttohighlearningrates. . . 289
xix
6.3 Meandivergencebetweenthetruenumericalandpredictivecodingbackprops
overthe course oftraining. In general, the divergenceappeared tofollowa
largely random walk pattern, and was generally neglible. Importantly, the
divergencedidnotgrowovertimethroughouttraining,implyingthaterrors
fromslightlyincorrectgradientsdidnotappeartocompound. . . . . . . . 293
6.4 TrainingandtestaccuraciesoftheCNNnetworkontheSVHNandCIFAR
datasetsusingthecross-entropyloss. Ascanbeseenperformanceremainsvery
close to backprop, thus demonstrating that our predictive coding algorithm
canbeusedwithdifferentlossfunctions,notjustmean-squared-error. . . . 294
6.5 Test accuracyplotsfor thePredictiveCodingand BackpropRNNand
LSTMon theirrespectivetasks,averagedover5 seeds. Performance is
againindistinguishablefrombackprop. . . . . . . . . . . . . . . . . . 297
6.6 Training losses for the predictive coding and backprop RNN. As ex-
pected,theyareeffectivelyidentical. . . . . . . . . . . . . . . . . . . 298
6.7 Computation graph and backprop learning rules for a single LSTM
cell. InputstotheLSTMcellarethecurrentinputx andtheprevious
t
embedding h . These are then passed through three gates – an input,
t
forget,andoutputgate,beforetheoutputofthewholeLSTMcellcan
becomputed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
6.8 TheLSTMcellcomputationgraphaugmentedwitherrorunits,evincing
the connectivity scheme of the predictive coding algorithm. The key
moveis toassociateeachintermediatenodeinthecomputationgraph
withitsownpredictionerrorunit . . . . . . . . . . . . . . . . . . . . 300
6.9 TraininglossesforthepredictivecodingandbackpropLSTMsaveraged
over5seeds. Theperformanceofthetwotrainingmethodsiseffectively
equivalent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
xx
6.10 Divergencebetweenpredictivecodingandthecorrectbackpropgradi-
ents as a function of sequence length. Crucially, this divergence only
increases linearly in the sequence length, allowing for very accurate
gradientcomputationevenwithlongsequences. . . . . . . . . . . . . 302
6.11 Number of iterations to reach convergence threshold as a function
of sequence length. Importantly, the number of iterations required
to converge appears to grow sublinearly with sequence length, again
implying that convergence is not computationally unattainable even
withverylongsequences. . . . . . . . . . . . . . . . . . . . . . . . . 303
6.12 TraininglossesforthepredictivecodingandbackpropLSTMsaveraged
over5seeds. Theperformanceofthetwotrainingmethodsiseffectively
equivalent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
6.13 Train and test accuracy and gradient angle (cosine similarity) for AR
vs backprop for MNIST and Fashion-MNIST datasets. Importantly
the training and testaccuracies are virtually identical betweenthe AR-
trainedandbackprop-trainednetworks. Additionally,thegradientangle
betweentheARupdateandbackpropisalwayssubstantiallylessthan
the90degreesrequiredtoallowforlearning. . . . . . . . . . . . . . 310
6.14 Trainandtestaccuracyandgradientangle(cosinesimilarity)forARvs
backprop for MNIST and Fashion-MNIST datasets. Importantly, we
seethatevenwiththeadditionalrelaxations,theARtrainedalgorithm
performscomparablytobackprop . . . . . . . . . . . . . . . . . . . 313
xxi
6.15 AnglebetweentheARandbackpropupdatesinthelearnablebackwards
weights, no nonlinear derivatives, and the combined conditions. At
all times this angle remains under 90 degrees and is steady for all
cases apart from the no nonlinear derivatives cases, where it appears
to increase over time. Interestingly, this does not appear to hinder
learning performance noticeably, and may simply reflect the angle
gettingincreasinglyworseasthenetworkconverges. . . . . . . . . . 315
6.16 Assessing whether the frozen feedforward pass assumption can be
relaxed. We show the resulting performance (test accuracy) against
baselineofrelaxingthisassumptionontheMNISTdataset. Allresults
averaged over 10 seeds. These results show clearly that the frozen
feedforwardpassassumptioncanberelaxedforthenonlinearderivative
and in the weight update nonlinear derivative, but not both nonlinear
derivativessimultaneously,anddefinitelynotusingthexT terminthe
weightupdate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
6.18 Performance (test accuracy), averaged over 10 seeds, on CIFAR10
demonstratingthescalabilityofthelearnablebackwardsweightsand
dropping the nonlinear derivatives in a CNN architecture, compared
to baseline AR without simplifications. Performance is equivalent
throughout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
6.20 Potentialschematicforadirectimplementationofbackpropinthebrain.
Allthatisnecessaryforthistobeplausibleisthree-factorlearningrules.325
xxii
Chapter 1
Introduction
TheFreeEnergyPrinciple(FEP)(Friston,2019a;Friston&Ao,2012a;Friston,Kilner,
&Harrison,2006;Parr, DaCosta,&Friston, 2020)isanemergingtheoryintheoretical
neurosciencewhichaimstotackleanextremelydeepandfundamentalquestion–can
onecharacterisenecessarybehaviourofanysystemthatmaintainsastatisticalseparation
from itsenvironment(Bruineberg, Dolega, Dewhurst, & Baltieri,2020; Friston, 2019a;
Parr, Da Costa, & Friston, 2020)? Specifically, it argues that any such system can
be seen as performing an elemental kind of Bayesian inference where the dynamics
of the internal states of such a system can be interpreted as minimizing a variational
free energy functional (Beal, 2003) 1, and thus performing approximate (variational)
Bayesian inference(Friston, 2019a). The FEP isthus effectively aformalization and
generalizationoftheAshbyangoodregulatorprinciple(Conant&RossAshby,1970),
whereanintrinsicpropertyofthesekindsofsystemsisthattheyinsomesensecometo
embodyaBayesianmodeloftheirsurroundings,andtheperformainferenceusingthis
model(Baltieri,Buckley,&Bruineberg,2020).
The free energy principle therefore provides a close link between the notions of self-
organisationand dissipativestructures inthermodynamics (Prigogine&Lefever,1973;
1Hencethenamethe‘freeenergyprinciple’
1
Chapter1. Introduction 2
Seifert,2008), withcybernetic notionsoffeedback, regulation, andcontrol(Johnson
& Moradi, 2005; Kalman et al., 1960; Wiener, 2019), to more ‘cognitive’ ideas of
inference and learning (Dayan & Daw, 2008; Rao & Ballard, 1999; Schmidhuber,
1991). Specifically,weseethat,insomesense,allofthesenotionscanbeconstruedas
necessaryproperties and consequencesof systemsthat self-organizeto, and maintain
themselvesat,anon-equilibriumsteadystate. Whilehavingdevelopedovertimeintoa
verygeneraltheory ofself-organisingsystems,thefree energyprinciplehasemerged
fromtheoreticalneuroscienceasawaytounderstandthepropertiesofbiologicaland
cognitive systems, especially the brain (Friston, 2003; Friston et al., 2006). As such,
themost developedprocess theories,which areexplicitlyinspired bythe freeenergy
principle – predictive coding (Friston, 2005; Mumford, 1992; Rao & Ballard, 1999),
and active inference (Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo, 2017a;
Friston,Rigoli,etal.,2015a; Friston, Samothrakis,&Montague,2012),eachpurport
to be theories of inference, action, and learning in the brain. A core tenet of the free
energyprincipleisthatperception,action,andlearningcanallbeunifiedunderasingle
inferenceobjectivewhichminimizesasingleobjective–thevariationalfreeenergy. As
weshallsee,thesedifferent‘processtheories’arisesimplyfromthechoiceofaspecific
parametrization of the generative model and the variational density, where certain
choices have been found to be useful and also give potentially biologically plausible
inferenceandlearningrules. Inthisthesis,wefocusonthehighlevelapplicationsof
thefreeenergyprincipletoneuroscienceandtomachinelearning,andmakemultiple
contributions to the literature through the application of the free energy principle to
all of perception, action, and learning. Specifically, in this thesis, we are primarily
concernedwithscalingupmethodswhichhaveemergedinthetheoreticalneuroscience
literature, to the extremely challenging and complex tasks which can be solved with
modernmachine learningmethods. The valueof scalingup suchmethodsis twofold.
Firstly,FEPinspiredprocesstheoriesoftenpossesssignificantbiologicalplausibility,
in that they provide a potential account of what the brain is doing – thus, if they can
Chapter1. Introduction 3
scale them up to handle the sort of tasks that the brain must solve, then we can be
more confident that these theories could, in theory, be actually implemented in the
brain. Secondly,thefreeenergyprincipleanditsprocesstheoriescontainmanyinsights
whichcanpotentiallybeusedtoimproveandextendcurrentstateoftheartmethodsin
machinelearning. Inthisthesis,weaimtopresentbothkindsofcontributions–firstly
bydemonstratingthatFEPinspiredmodelsandprocesstheoriescanscale,andsecondly,
by showcasing how ideas from the FEP can be used to advance the field of machine
learningorneuroscienceonitsownterms.
1.1 Thesis Overview
This thesis is organized into three main parts. We begin with a detailed introduction
and mathematicalwalkthrough of theFree Energy Principle (FEP) as presentedin its
mostrecentincarnation(Friston,2019b;Parr,DaCosta,&Friston,2020)(Chapter2)
andthen, inthe firstsectionof originalwork (Chapter3), weconsiderapplications of
the free energy principle to perception – and make contributions to the FEP process
theory of predictive coding. In the second section (Chapters 4 and 5), we consider
applications of the free energy principle to action, and work with the process theory
ofactiveinference. Finally,inthethirdsection(Chapter6),weconsiderapplications
of the free energy principle to learning, and especially focus on what FEP-inspired
modelsandprocesstheoriescantellusaboutthenatureofcreditassignmentinthebrain.
BelowisachapterbychapterbreakdownoftheworkinthethesisandwhatIseearethe
main contributions toboth machine learningand neurosciencein each. Throughout the
thesis,sinceeachchaptercoversafairlydistincttopic,Ihavetriedtomakeeachchapter
modularand mostlyindependent of the others. Eachchapter containsan introduction
and mini literature review on the field it discusses, as well as presenting my original
work.
InChapter2,Iwillgiveadetailedoverviewofthefreeenergyprinciple,startingfrom
Chapter1. Introduction 4
first principles, and include adiscussion of the mathematical assumptions andprovide
someofmyopinionsonthephilosophicalnatureofthetheoryanditspotentialutility. I
willthengiveabriefwalk-throughofdiscretestatespaceactiveinferenceaspresented
in(DaCosta,Parr,etal.,2020;Friston,FitzGerald,Rigoli,Schwartenbeck,&Pezzulo,
2017b;Friston,Rigoli,etal.,2015a)whichwillbethefocusofthe‘scalingup’workin
Chapter4.
InChapter 3,wedealprincipally withmodelsofperception andpredictivecoding. We
begin bygiving abriefoverview andmathematicalwalkthrough ofpredictivecoding
theory as it is presented in (Buckley, Kim, McGregor, & Seth, 2017; Friston, 2005,
2008a). We then cover in depth two contributions to the theory of predictive coding.
First, we present work where we scale up and empirically test the performance of
large scale predictive coding networks on machine learning datasets, which had not
been tested before in theliterature. We alsoclarify the relationship between predictive
coding and other known algorithms such as Kalman filtering. Secondly, we discuss
relaxing various relatively un-biologically plausible aspects of the predictive coding
equations,suchastheneedforsymmetricforwardandbackwardsweights,thenecessity
of using nonlinear derivatives in the update rules, and the one-to-one error to value
neuron connectivity required by the standard algorithms. All of these conditions put
serious constraints on the biological plausibility of the algorithm, and here we show
thattosomeextenttheycaneachberelaxedwithoutharmingperformance.
Then, in Chapters 4 and 5, I present my work on the applications of the free energy
principletoquestionsofactionselectionandcontrol. Chapter4focusespredominantly
onscalingup activeinference methods toachieve resultscomparabletothoseachieved
inthedeepreinforcementlearningliterature,whileChapter5takesamoreabstractand
mathematicalapproach and investigatesindepthsthe mathematicaloriginof objective
functionalswhichcombinebothexploitatoryandexploratorybehaviour–anapproach
whichhasthepotentialtofinessetheexploration-exploitationdilemna(Friston,Rigoli,
Chapter1. Introduction 5
etal.,2015a).
Specifically, in Chapter 4, I will first review the rudiments of reinforcement learning
(RL), and its current incarnation in deep reinforcement learning, including the two
paradigmsof model-freeand model-basedapproaches. Iwill thenpresent twoof my
contributionsofmergingactiveinferenceanddeepreinforcementlearningunderthenew
paradigmofdeepactiveinference. Iwillfirstdiscussdeepactiveinferenceinthemodel-
freeparadigm,andshowhowinthiscasetheactiveinferenceequationscannaturallybe
understoodas specifying anactor-criticarchitecturewitha bootstrappedvaluefunction,
except one where the value-function becomes the expected-free energy functional,
which provides an intrinsic source of exploratory drive to the algorithm which can
improveperformance. Ithendiscussthesimilaritiesanddifferencestostandarddeep
reinforcement learning algorithms and empirically compare the performance of the
algorithms on a number of challenging continuous control tasks from OpenAI gym
(Brockmanetal.,2016).
Then I will present a second piece of work which applies active inference instead to
themodel-basedreinforcementlearningparadigm. Wewillshowthatinthiscase,we
canusethepowerfulgenerative ‘worldmodels’(Ha&Schmidhuber,2018) ofactive
inferencetoworkastransitionmodelsofthelearntdynamics,andthentheuseofaction
selectioninplanning. Theuseoftheexpectedfreeenergyfunctionalagainfurnishes
anintrinsic exploratoryforactiveinferenceagents,which weagainshowiscrucialto
effective, goal-directed exploration and that it empirically improves performance on a
suiteofcontinuouscontroltasks. Wethenconceptualizereinforcementlearningthrough
the lens of inference, and understand the distinction between model-free and model-
based reinforcement learning through the lens of iterative and amortised inference.
Wethendemonstratehowthesetwotypesofinferencecanbecombined,leadingtoa
novelhybridinferencealgorithmwhichweshowattainsboththesampleefficiencyof
model-basedreinforcement learningwith thehigher asymptoticperformance andfast
Chapter1. Introduction 6
computationtimeofmodel-freeRL.
Then, in Chapter 5, we move into a more abstract, mathematical domain. Here we
grapplewithdeepquestionsunderlyingtheobjectivefunctionsofreinforcementlearning.
Specifically,wewishtounderstandthemathematicaloriginandnatureoftheexpected
freeenergytermwhichgrantsdeepactiveinferenceagentstheirsuperiorexploratory
capacities. Having tackled this, we turn to the deeper question of the mathematical
originofinformation-seekingexploratorytermswithintheinferenceobjectiveoptimized
in reinforcement learning methods, and thus the mathematical origin of exploratory
drives. We present a new dichotomy between evidence and divergence objectives,
and demonstrate how only divergence objectives, which intuitively can be seen as
minimizingthedivergencebetweenthepredictedanddesiredfutures,ratherthansimply
maximizingthelikelihood ofthedesiredfuture,arerequiredtoobtainsuchterms. We
thenrelatethisfundamentaldichotomytoanumberofobjectivesprominentinboththe
cognitivescience,neuroscience,andmachinelearningliteratures. Finally,wefurther
seektoexplorethegeneralpossiblespaceofvariationalobjectivefunctionalsforcontrol,
andprovideawide-rangingcategorisationofthepotentialofsuchfunctionalswithin
ourframework.
Finally, in Chapter 6, we turn to the application of insights and ideas from the free
energy principle to learning. Specifically, we focus on the vexing question of how
to achieve credit assignment in the brain. This is necessary since, we assume, that
most of the statistical ‘parameters’ in the brain – such as synaptic weights – start out
initialized fairlyrandomly during development and thusneed to betrained or learned
through interactions with the environment 2. Understanding how this learning can
take place is a fundamental question within neuroscience. One approach, which has
recently been immensely successful in machine learning with large artificial neural
networks,istheideaoflearningthroughgradientdescentusingthebackpropagation
2Itispossiblethatsomepathways,especiallylow-levelsubcorticalpathwaysmaybe,tosomeextent,
hardwiredbyevolution. However, itisgenerallyconsideredinfeasiblefortheimmensenumberand
complexityoftheneocorticalcircuitrytobehardwiredinthisway
Chapter1. Introduction 7
of error algorithm. Since backpropagation of error is such a successful algorithm in
artificialneuralnetwork–which,althoughsimplifiedareneverthelessgenerallyquite
a close substrate to biological neural networks – it is very likely that it would also
workto successfullytrainbiological neuralnetworks,if itcouldbe implementedina
biologically plausible manner in suchnetworks. The question, then, becomes whether
andhowbackpropcanbeimplementedinbiologicallyplausibleneuralnetworks. While
thisisanextremelybroadquestionwhichcannotlikelybeansweredinasinglethesis,
wepresenttwonovelcontributionstothisquestionhere.
Specifically,inChapter6,wefirstprovideabriefreviewofthecreditassignmentprob-
leminthebrain,aswellasthebackpropagationalgorithm(andautomaticdifferentiation
ingeneral),forcontext,andthenpresentourtwocontributionstothisfield. First,we
demonstratehowundercertainconditions,predictivecodingitselfcanbeutilizedasabi-
ologicallyplausiblemethod ofcreditassignment inthebrain, canapplyto any arbitrary
computation graph, and can be used to train modern machine learning architectures
suchasCNNsandLSTMswithperformancecomparabletobackprop. Secondly,we
introduceanovel,simpleralgorithmforcreditassignmentinthebrain,whichwecall
ActivationRelaxation andthendiscuss theirsimilaritiesand differences. We end witha
discussionofthecurrentstateofcreditassignmentalgorithmsandbackpropagationin
thebrain,andtheimportanceofthisfieldofresearch.
Finally,inChapter7,weprovideadiscussionandoverviewoftheworkinourthesis.
Wewillbrieflysurveywhathasbeenachieved,andwherethelimitationsanddirections
forfutureworklie,aswellastheimplicationsoftheworkofthisthesis.
1.2 Statement of Contributions
ThisstatementprovidesadetailedoverviewoftheworkundertakeninthisPhDwhich
has resulted in research papers, both the papers included in this thesis and also those
notincluded. Iprovideabriefsummaryofthekeyresultsandnarrativeofeachpaper,
Chapter1. Introduction 8
aswellasadetailedbreakdownmycontributions. *denotesequalcontribution.
1.2.1 Included in Thesis
1.2.1.1 Chapter3
• Predictive Coding – a Theoretical and Experimental review (2021). Beren
Millidge,AlexanderTschantz,AnilSeth,ChristopherLBuckley. InPreparation
This paper provides a full review of recent advances in predictive coding, as
well as the mathematical basis of the theory. It covers all of the mathematical,
implementational, and neuronal aspects of predictive coding theory. As a first
authorpaper,Iconceptualisedtheidea,collatedthenecessarymaterialsforthe
review, and wrote the paper. Alexander Tschantz, Christopher L Buckley, and
AnilSeth,contributededitsandothereditorialsuggestions.
• NeuralKalmanFiltering(2021). BerenMillidge,AlexanderTscahtnz,AnilSeth,
ChristopherLBuckley. Arxiv
This paper reviews the close connection between Kalman filtering and linear
predictivecoding,demonstratesthatpredictivecodingcancloselyapproximate
theperformanceofKalmanfilteringonfilteringtasks,andproposesalow-level
neural implementation of predictive coding which could be implemented in
the brain. As a first author paper, I conceptualised the idea, worked out the
mathematical derivations, implemented the model and experiments, and wrote
afirstdraftofthepaper. AlexanderTschantz,ChristopherLBuckley,andAnil
Sethcontributededitorialandnarrativesuggestions.
• Relaxed Predictive Coding (2020). Beren Millidge, Alexander Tschantz, Anil
Seth,ChristopherLBuckley. Arxiv
Thispaper showshow severalbiologically implausibleaspectsof thepredictive
codingalgorithm–backwardsweightsymmetry,nonlinearderivatives,andone-to-
Chapter1. Introduction 9
oneerrorunitconnectivity–canberelaxedwithoutundulyharmingperformance
onchallenging objectrecognitiontasks. As afirstauthor paper, Iconceptualised
theidea,implementedthecodeandexperiments,andwroteupafirstdraftofthe
paper. Alexander Tschantz, Anil Seth, and Christopher L Buckley contributed
editorialsuggestions.
• ImplementingPredictiveProcessing andActiveInference: PreliminarySteps and
Results(2019). BerenMillidge,RichardShillcock.
This paper provides reference implementations of multi-layer predictive coding
networkstrainedforobjectrecognitionwithinamachinelearningparadigm. As
a first author paper, I conceptualised the idea, wrote the code and experiments,
andwroteuptheinitialdraftofthepaper. RichardShillcockcontributededits.
1.2.1.2 Chapter4
• DeepActiveInferenceasVariationalPolicyGradients(2019). BerenMillidge.
PublishedintheJournalofMathematicalPsychology.
Thispapermergesactiveinferenceandmodel-freedeepreinforcementlearning
to create a deep active inference agent, very similar to actor-critic methods in
deepreinforcementlearning. Theperformanceofdeepactiveinferenceanddeep
reinforcementlearningiscomparedonasuiteofOpenAIGymcontinuouscontrol
tasks. Asasoleauthorpaper,Iconceptualisedtheidea,executeditmathematically
andincode,designedandimplementedtheexperimentsandwroteupthepaper.
• ReinforcementLearningthroughActiveInference(2020). AlexanderTschantz*,
BerenMillidge*,AnilSeth,ChristopherLBuckley. PublishedinICLRwork-
shoponBridgingAIandCognitiveScience.
Chapter1. Introduction 10
Thispaperappliesactiveinferencetomodel-basedreinforcementlearningmeth-
odsforcontinuouscontrol. Weuseanensembletransitionmodelparametrised
by deep neural networks for model-based planning. The expected free energy
andfreeenergy-of-the-expectedfutureobjectivefunctionalsprovideadditional
exploratorybonuses which allow considerablygreater and faster performance of
the method compared to standard deep reinforcement learning baselines. I was
jointfirstauthoronthispaper. Whiletheinitialideawasprimarilyconceptualized
byAlexander TschantzandChristopher LBuckley, Icontributedequally tothe
design andimplementation of thealgorithm, the experiments, andthe writing of
thepaper.
• Reinforcement Learning as Iterative and Amortised inference (2020). Beren
Millidge*,AlexanderTschantz*,AnilSeth,ChristopherLBuckley. Arxiv.
Thisshortworkshoppaperderivesthekeymathematicalresultofunderstanding
model-free and model-based reinforcement learning in terms of iterative and
amortisedinference. Itthenpartitionsknownreinforcementlearningalgorithms
intoaquadrantbasedontwoorthogonalaxes–firstlywhetheritusesiterativeor
amortisedreinforcementlearning,and secondly whetherweoptimizeoverplans
or over policies. As a joint first author, I contributed equally (with Alexander
Tschantz) in the idea, formulation of the dichotomy, and the mathematical deriva-
tions. Ialsowastheprimaryauthorofthetextofthepaper. AlexanderTschantz,
ChristopherLBuckley,andAnilSethalsocontributededitstothepaperdraft.
• ControlasHybridInference(2020). AlexanderTschantz,BerenMillidge,Anil
Seth,ChristopherLBuckley. PublishedinICMLworkshopontheTheoretical
FoundationsofReinforcementLearning.
This paper combines both iterative (model-based) and amortised (model-free)
Chapter1. Introduction 11
reinforcement learning methods to obtain a hybrid method which combines both
thesample-efficiency ofmodel-basedRL,with theasymptoticperformance and
fast computation of model-freemethods. As secondauthor, Icontributed equally
to the idea, the mathematical derivation, and architectural formulation of the
hybridagent. AlexanderTschantztooktheleadwithimplementingtheagentin
code,designingandrunning,theexperiments,andwritinguptheinitialdraftof
thepaper. IthencontributedpapereditsalongwithAnilSethandChristopherL
Buckley.
1.2.1.3 Chapter5
• WhencetheExpectedFreeEnergy(2020). BerenMillidge,AlexanderTschantz,
AnilSeth,ChristopherLBuckley. PublishedinNeuralComputation.
This paper investigates the mathematical origin of the expected free energy
functional in active inference, demonstrates its relationship to other algorithm,
andproposesanovel,moreprincipledobjective,thefreeenergyoftheexpected
future(FEEF).Asafirstauthor,Iprimarilyconceptualisedtheideaandworked
outthemathematicalresults. Ialsowroteuptheinitialdraftandwasinstrumental
in handling later edits. Christopher L Buckley also contributed significantly to
someofthemathematicalresults. AlexanderTschantz,ChristopherLBuckley,
andAnilSeth,alsocontributedthrougheditstothemaintextofthepaper.
• Onthe relationship ofActiveInferenceandControlas Inference(2020). Beren
Millidge,Alexander Tschantz,AnilSeth,Christopher LBuckley. Publishedin
theIEEEInternationalWorkshoponActiveInference.
This paper derives the relationship between active inference methods, and the
control as inferenceparadigm which is popular withinthe reinforcement learning
community. Asfirstauthor,Iconceptualisedtheidea,derivedtheprimarymathe-
maticalresults,andwrotetheinitialdraftofthepaper. AlexanderTschantz,Anil
Chapter1. Introduction 12
Seth,andChristopherLBuckleycontributedpaperedits.
• UnderstandingtheOriginofInformation-SeekingExplorationinProbabilistic
Objectives for Control (2021).Beren Millidge, Alexander Tschantz, Anil Seth,
ChristopherLBuckley. SubmittedtoArxiv.
Thispaperintroducesthedichotomybetweenevidenceanddivergenceobjectives,
demonstrateshowdivergenceobjectivesarenecessaryfortheemergenceofinfor-
mationmaximizingexploration, andunifiesmanydisparateobjectivesproposed
in themachine learning andcognitive sciencecommunities under thisformalism.
As a first author paper, I conceptualised and derived the mathematical results,
andwrote upafirstdraft ofthepaper. AlexanderTschantz,Christopher LBuck-
ley,andAnilSethcontributedpapereditsandnarrativesuggestions. Alexander
Tschantzcontributedheavilytothecognitivescienceandpsychophysicssections.
1.2.1.4 Chapter6
• Predictive Coding Approximates Backprop along Arbitrary Computation Graphs
(2020). BerenMillidge,AlexanderTschantz,AnilSeth,ChristopherLBuckley.
Arxiv
Thispaperdemonstratesthatpredictivecodingcanapproximatethebackpropaga-
tionoferroralgorithmalongarbitrarycomputationgraphs. Predictivecodingis
usedtotrainstateoftheartmachinelearningarchitectures,andobtainsidentical
performancetobackpropevenfordeepandcomplexarchitectures. Asafirstau-
thorpaper,Iconceptualisedtheidea,implementedthemodelsandexperimentsin
code,andwroteuptheinitialdraftofthepaper. AlexanderTschantz,Christopher
LBuckley,andAnilSethcontributedpapereditsandnarrativesuggestions.
• ActivationRelaxation: ALocal, DynamicalApproximationtoBackpropagation
intheBrain(2020). BerenMillidge,AlexanderTschantz,AnilSeth,Christopher
LBuckley. Arxiv
Chapter1. Introduction 13
This paper introduces a novel algorithm for approximating backpropagation in a
local,biologicallyplausibleway,whichwecalltheactivationrelaxationalgorithm.
Crucially,this approachissignificantlysimplerthan predictivecoding,in that it
does not require a special population of error neurons. Additionally, we show
thatseveralofthe remainingbiologically implausibleaspectsof thealgorithm –
specificallythe symmetricbackwards weights–can alsoberelaxed,leading to
anextremelysimpleandbiologicallyplausiblealgorithmforcreditassignment.
Asafirstauthorpaper,Iinventedthealgorithmandperformendthemathematical
derivations. Iimplementedthemodelincode,andrantheexperiments. Iwrote
uptheinitialdraftofthepaper. AlexanderTschantz,ChristopherLBuckley,and
AnilSethcontributededitorialsuggestions.
• Investigating the Scalability and Biological Plausibility of the Activation Re-
laxation Algorithm (2020). Beren Millidge, Alexander Tschantz, Anil Seth,
ChristopherLBuckley. PublishedatNeurIPS2020Workshop: BeyondBackprop.
This paper extends and empirically tests the Activation Relaxation algorithm
on more challenging tasks including large-scale CNN models. Moreover, it
investigatesthedegreetowhichthelooseningtheassumptionsoftheactivation
relaxationalgorithmhinderperformance. Asafirstauthorpaper,Iconceptualized
the core ideas to test, implemented the experiment and analyzed the results. I
wroteuptheinitialdraftofthepaper. AlexanderTschantz,ChristopherLBuckley,
andAnilSethcontributededitorialsuggestions.
1.2.2 Not Included in the Thesis
• Combiningactiveinferenceandhierarchicalpredictivecoding–atutorialreview
andcasestudy(2019). BerenMillidge,RichardShillcock.
This paper uses hierarchical predictive coding networks as a dynamics model
for a simple active inference approach which is then applied to discrete action
Chapter1. Introduction 14
reinforcement learning tasks such as the cart-pole. As a first author paper, I
conceptualised the idea, implemented the code and experiments, and wrote up
theinitialdraft. RichardShillcockcontributedpapereditsandsuggestions.
• A predictive processing account of visual saliency using cross-predicting autoen-
coders(2018). BerenMillidge,RichardShillcock. Psyarxiv.
This paper demonstrates how applying a cross-modal prediction objective in
predictive coding, allows for the development of error representations which
provideagoodempiricalmatchtoestimatesofvisualsaliencyinnaturalimage
scenes. As a first author paper, I contributed equally in conceptualising the
idea with my supervisor, Richard Shillcock. I implemented the models and
experiments,andwroteuptheinitialdraftofthepaper. RichardShillcockthen
contributedwitheditorialsuggestions.
• ExploringinfantvocalimitationinTadaridabrasiliensismexicana(2019). Richard
Shillcock, Beren Millidge, Andrea Ravignani. Published in Neurobiology of
SpeechandLanguage.
Thispaperintroducesamulti-agentmodelofvocalimitationinbatinfants,which
providesagradientsoundscapewhichcanguidemotherstopupsinacrowdedbat
colony. Asasecondauthorpaper,Icontributedsubstantiallytothedevelopment
of the model. I implemented the model in code and ran the experiments. I
contributedsubstantiallytothewritingofthepaper.
• Curious inferences: reply to Sun and Firestone on the Dark Room Problem
(2020). AnilSeth,BerenMillidge,ChristopherLBuckley,AlexanderTschantz.
PublishedinTrendsinCognitiveScience.
ThisshortresponsearguesagainsttheDarkRoomProbleminpredictivecoding
bysuggestingthattheintrinsicexploratorydrivesoftheexpectedfreeenergyand
other objectives such as thefree energy of theexpected future suffice todrive the
Chapter1. Introduction 15
agentawayfromdark-roomenvironments. Iwasinvolvedintheconceptualisation
andwritingofthepiece,althoughthemainimpetusbehindthisresponselaywith
AnilSeth.
• TheAcquisitionofCulturallyPatternedAttentionStylesunderActiveInference
(2020). Axel Constant, Alexander Tschantz, Beren Millidge, Filipo Criado-
Boado,AndyClark. Arxiv.
This paper presents an active inference model of culturally patterned saccade
behaviour trained on archaeological vase patterns. It demonstrates that more
complexpatterns resultinmoreverticallyorientedsaccadebehaviour,thus cor-
roboratingexperimentalstudies. Ijointlydesignedandimplementedtheactive
inferencemodelwithAlexanderTschantzandrantheexperiments. Ialsowrote
the first draft of the methods section of the paper. Andy Clark, Felipe Criado-
Boado,andAxelConstantconceptualisedtheideaandexperiments.
Chapter 2
The Free Energy Principle
Thefreeenergyprincipleisagrandtheory,arisingoutoftheoreticalneuroscience,with
deep ambitions to provide a unified understanding of the nature of self organisation
under the rubric of Bayesian inference (Friston, 2010, 2019c; Friston & Ao, 2012a;
Friston et al., 2006). Perhaps the central postulate of this theory is the ‘Free Energy
Lemma’ which states that one can interpret any self organizing system, of any type
and on any scale, as performing a kind of elemental Bayesian inference upon the
external environment that surrounds it (Friston, 2013, 2019a; Friston & Ao, 2012b).
More generally than this, it claims to provide a recipe, in terms of a set of statistical
independencies–whichwe callthe‘MarkovBlanket’,following(Pearl,2011)–which
definepreciselyandmathematicallywhatitmeanstobeasystematall(Friston,2019a).
Understandingself-organizationthroughthelensofinferenceprovidesanexceptionally
powerful perspective for understanding the nature of self-organizing systems, as it
allows one to immediately grasp the nature of the dynamics which undergird self-
organization,aswellasapplytheextremelylargeandpowerfulliteratureonBayesian
inference methods and algorithms to the dynamics of self-organizing systems (Parr,
Da Costa, & Friston, 2020; Parr, Sajid, & Friston, 2020; Yedidia, 2011). Moreover,
by framing everything in statistical terms – in terms of conditional independencies,
16
Chapter2. TheFreeEnergyPrinciple 17
generativemodels,andapproximatevariationaldistributions–thefreeenergyprinciple
providesanovelandpowerfulvocabularytotalkaboutsuchsystems,aswellastoask
questionssuchas‘whatkindofgenerativemodeldoesthissystemembody?’ (Baltieri
etal.,2020;Maturana&Varela,2012)whichwouldbeimpossibletoaskandanswer
without it. Ultimately, this new statistical and inferential perspective upon dynamics
mayleadtoimportantadvancesornovelinsights.
Thisperspectivealsohasexceptionallycloserelationshipswithearlycyberneticviews
ofcontrolandregulation(Conant&RossAshby,1970;Kalman,1960;Wiener,2019),
andphilosophicallytheFEPcanbeseenasamathematicalgeneralizationofAshby’s
notion that every good regulator of a system must become, in effect, a model of the
system(Conant& RossAshby,1970). TheFEPnuances thisnotionslightlyby instead
stating thatevery systemthat regulatesitself againstthe external environment, must in
some senseembody agenerative modelof the environment, andalso thatthe flowof
the internalstates of the system necessarily perform approximatevariational inference
upon anapproximate posteriordistribution over theexternal statesof the environment,
suchthat,broadly,theytrackthefluctuationsintheexternalenvironment.
The free energy principle originated in theoretical neuroscience, as an attempt to
understandthemathematicalpropertiesthataself-organisingliving,bioticsystem,must
possess in order to sustain itself against thermodynamic equilibrium. It was first and
especially applied to understanding the function of the brain (Friston, 2012; Friston,
Daunizeau,Kilner,&Kiebel,2010;Fristonetal.,2006),andhasbeendevelopedinto
two main process theories – predictive coding (Friston, 2003, 2005, 2008a; Rao &
Ballard,1999)andactiveinference(DaCosta,Parr,etal.,2020;Friston,Daunizeau,&
Kiebel,2009;Friston,FitzGerald,etal.,2017b;Friston,Rigoli,etal.,2015a;Friston,
Rosch,Parr,Price,&Bowman,2018a;Fristonetal.,2012)whichhavebeeninvestigated
in a wide variety of paradigms, where it has been used to investigate a wide variety
of phenomena from (Friston, Levin, Sengupta, & Pezzulo, 2015; Friston, Rigoli, et
Chapter2. TheFreeEnergyPrinciple 18
al., 2015a; Friston et al., 2014), information foraging and saccades (Parr, 2019; Parr
&Friston, 2017b, 2018a)exploratory behaviour(Friston, DaCosta, Hafner, Hesp,&
Parr, 2020; Friston, Lin, et al., 2017; Friston, Rigoli, et al., 2015a; Schwartenbeck,
FitzGerald, Dolan, & Friston, 2013), concept learning (Schwartenbeck et al., 2019),
andavarietyofneuropsychiatricdisorders(Adams,Perrinet,&Friston,2012;Cullen,
Davey,Friston,&Moran,2018;Lawson,Rees,&Friston,2014;Mirza,Adams,Parr,&
Friston,2019). TheseprocesstheoriestranslatetheabstractformulationoftheFEPinto
concreteandpracticalalgorithmsbyspecifyingcertaingenerativemodels,variational
distributions, and inference procedures, and have been shown to be extremely useful
bothinprovidingpowerfulandbiologicallyplausibletheoriesoflearningandinference
inthebrain,andalsoindevelopinghighlyeffectiveinferencealgorithmswhichhave
advancedthestate oftheartinmachinelearning(Millidge,2020;Millidge, Tschantz,
Seth,&Buckley,2020b;Parr,Markovic,Kiebel,&Friston,2019;Tschantz,Millidge,
Seth,&Buckley,2020b).
In this chapter, we will provide a relatively self-contained step through of the key
mathematical results of the most recent incarnation of the free energy principle as
presented in (Friston, 2019a; Parr, Da Costa, & Friston, 2020), as well as the details
ofthediscrete-state-spaceactiveinferenceprocesstheory(DaCosta,Parr,etal.,2020;
Friston, Rigoli, et al., 2015a). While none of the material in this chapter is original,
it is necessary (especially the material on active inference), to understand what is
to come in later chapters. Since this thesis covers a fairly wide range of topics, each
individualthesischapteralsocomeswithitsownliteraturereviewcoveringthenecessary
backgroundfortheoriginalmaterialinthatchapter.
Itisimportanttonotethatthematerialinthefirstsectionofthischapter(walkthroughof
theFreeEnergyprincipleasdescribedinFriston(2019a))isnotoriginaltomeanddraws
heavilyfromanunpublishedmonograph(Friston,2019a),albeitamonographwhichhas
widelybeenviewedasthecanonicalreferencepointforthetheory. Additionally,many
Chapter2. TheFreeEnergyPrinciple 19
coreelementsofthetheorypresentedherehavebeenpublishedelsewhere(Friston,2013;
Friston, Da Costa, & Parr, 2020; Friston, Wiese, & Hobson, 2020; Parr, Da Costa, &
Friston,2020). Notably,afairamountofthematerialcoveredhereisalsocontroversial
within the community and the validity of many required assumptions still remains
to be assessed. Where relevant, in this section, we provide additional disclaimers
highlightingcoreassumptionsandpotentiallyproblematicelementsofthemathematical
exposition – and additionally in the discussion section we include an itemized list of
allassumptionsaswellascriticaldiscussionsoneach. Whilesomeofthismaterialis
somewhat extraneous to the original work covered in later chapters, we believe that
thispresentationofthefreeenergyprinciplegivesthereader valuablecontextintothe
broader paradigm of the FEP which has inspired much of the original work in this
thesis. Additionally,bycondensingthelogicalflowoftheFEP,andprovidingadetailed
criticaldiscussionof thelogicandassumptionsrequired, weaimto provideabroader
servicetothecommunitybyhelpingtomakeclearthecurrentstateaswellascurrent
controversiesanddebatesatthecutting-edgeofthefreeenergycommunity.
Finally,itisimportanttonotethattheprocesstheoriesderivedfromtheFEP–which
we will primarily focus on in the rest of the thesis – do not strictly require the full
mathematical structure of the FEP to hold for their validity. As scientific theories
abouttherealworld,theyriseorfallonempiricalconsiderationsindependentlyofthe
overallmathematicalconstructoftheFEP,andthuswhilethematerialinthischapter
is useful contextually, it is not necessary to understand the work in the chapters that
follow. Throughout we have aimed to make sure that each chapter, by and by large,
is ‘modular’, so that they can be read and understood in isolation. As such, we have
striventoensurethateachchaptercontainssufficientbackgroundinformationwithinit
toletitbeunderstoodandevaluatedindependentlyoftheothers.
Chapter2. TheFreeEnergyPrinciple 20
Langevin Dynamics Non-Equilibrium Steady State
p*(x)
x·=f(x,t)+ω
Ao Decomposition
x·=(Γ−Q)∇xlnp*(x)
Marginal Flow Lemma Markov Blanket Condition
f μ(π)=𝔼p(π˜|π) [f μ(x)]=(Γ−Q)∇μlnp*(π)
p(x)=p(η|b)p(μ|b)p(b)
Identical true and variational posterior Particular Free Energy Parametrisation by argmax
q(η;η)=p(η|b) μ(b)=argmaxp(μ|b)
f μ(π)=(Γ−Q)∇μℱparticular(μ,s,a) η(b)=argmaxp(η|b)
η(b)=σ(μ(b))
Free Energy Lemma
Laplace Approximation
(Approximate Bayesian Inference)
q(η;μ)=𝒩(σ(μ),Σ(η))
μ·=(Γ−Q)∇μℱ(μ,s,a)
Figure 2.1: The logical flow of the argument of the FEP from the initial formulation to
thecrucialapproximateBayesianinferencelemma. Webeginwithasettingofrandom
Langevinstochasticdynamicalsystems,whichpossessanon-equilibrium-steadystate.
By applying the Ao decomposition, we can understand the dynamics in terms of a
gradientdescentuponthesurprisal. UpontheadditionofaMarkovBlanketpartition,we
canexpresssubsetsintermsoftheirownmarginalflowsviathemarginalflowlemma. If
we then identify the internal states as parametrizing a variational distribution over the
external states, we can interpret the marginal flow on the surprisal as a flow on the
variationalfreeenergy,undertheLaplaceapproximation.
Chapter2. TheFreeEnergyPrinciple 21
2.1 History and Logical Structure
Historically, the free energy principle has evolved over the course of about fifteen
years. Itsintellectualdevelopmentcanbestbeseenintwophases. Inthefirstphase,an
intuitiveandheuristictreatmentemergedwithFristonetal.(2006)whichstatedthatthe
imperativeto minimizevariational freeenergyemergedfrom anecessaryimperative
of minimizing the system’s entropy, or log model evidence, which is upper bounded
by variational free energy. This imperative emerges due to the self-sustaining nature
of biological systems such as brains, in that they maintain a set distribution against
theinexorablyincreasingentropicnatureofthermodynamicreality(Friston,2009). In
ordertodoso,systemsmustconstantlyseektoreduceandmaintaintheirentropyacross
theirstatespace. SincetheVFEiscomputationallytractablewhiletheentropyitselfis
not,itwaspostulatedthatneuralsystemsmaintainthemselvesbyimplicitlyminimizing
thisproxyratherthantheactualentropyitself(Friston,2010).
Later, in the second phase (Friston, 2013), this heuristic argument and intuition was
related more formally to concepts in stochastic thermodynamics (Friston & Ao, 2012a,
2012b). Specifically, the framework developed mathematically into a description
of stochastic dynamics (as stochastic differential equations) separated into ‘external,
internal, and blanket’ states by a statistical construct called a Markov Blanket. This
blanketmakesprecisethestatisticalindependenceconditionsrequiredtomakesenseof
talkingabouta‘system’asdistinctfromits‘environment’. Moreover,byseparatingthe
‘blanket’ into ‘sensory’ and ‘active’ states, one can obtain a statistical description of
the coreelements ofa perception-action-loop, acentral conceptin cybernetics, control
theory, and reinforcement learning. Secondly, the theory developed a precise notion
ofwhatitmeanstomaintainastable‘phenotype’whichisinterpretedmathematically
as a non-equilibrium steady-state density (NESS) over the state-space. This steady
state is non-equilibrium due to the presence of ‘solenoidal flows’ which are flows
orthogonal to the gradient of the NESS density. Mathematically, such flows do not
Chapter2. TheFreeEnergyPrinciple 22
increaseordecreasetheentropyofthesteady-state-density,butdo,however,incontrast
to an equilibrium steady state (ESS), provide a clear arrow of time. Given this, it is
claimed, that under certain conditions, one can draw a relationship between the flow
dynamicsandtheprocessofvariationalBayesianinferencethroughtheminimization
of the variational free energy (VFE)– which measures the discrepancy between an
approximateposteriorandgenerativemodel–andthatthedynamicsthatresultfrom
thisspecifickindofflowunderaMarkovblanketattheNESSdensitycanbeseenas
approximatingagradientdescentupontheVFE,thuslicensingtheinterpretationofthe
systemasperformingabasickindofBayesianinferenceor,‘self-evidencing’T(Clark,
2015;Hohwy,Roepstorff,&Friston,2008)
While the intuitions and basic logical structure of the theory has remained roughly
constantsinceFriston(2013);FristonandAo(2012a),themathematicalformulationand
some of the arguments have been refined in the most recent Friston (2019a) monograph
andrelatedpapers(Friston,DaCosta,&Parr,2020;Parr,DaCosta,&Friston,2020).
These papers have drawn close connections between the formulation of free energy
principle,andmanyaspectsofphysicsincludingtheprincipleofleastactioninclassical
mechanics, and notions of information length and the arrow of time in stochastic
thermodynamics. Additionally, the Particular Physics monograph (Friston, 2019a)
contains a novel information-geometric gloss on the nature of the Bayesian inference
occurring in the system. Specifically, it argues that the internal states of the system
canbeseenaspointsonanstatisticalmanifold thatparametrizedistributionsoverthe
external states, and that thus the internal states can be described using a ‘dual-aspect
information geometry.’ According to this perspective, internal states evolve in both
the ‘intrinsic’ state space of the system’s physical dynamics, while simultaneously
parameterising a manifold of statistical beliefs about external states - the so-called
‘extrinsic’informationgeometry.
While the mathematical depths of the FEP often appears formidably complex to the
Chapter2. TheFreeEnergyPrinciple 23
uninitiated,theactuallogicalstructureofthetheoryisrelativelystraightforward. First,
we want to define what it means to be ‘a system’ that keeps itself apart from the
outside‘environment’overaperiodoftime. TheFEPanswersthisquestionprecisely
itsownway. Wedefine a‘system’as adynamicalsystem whichhasa non-equilibrium
steadystate (NESS)which itmaintainsover anappreciablelength oftime, andthat the
dynamicsarestructuredinsuchawaythattheyobeythe‘MarkovBlanketCondition’.
Specifically, having a NESScan beintuitivelythought ofas defining dynamicswhich
producesomethinglikeasystem–i.e. arecognizablepatternofstateswhichpersists
relatively unchanged for some period of time. For instance, we can think of the
biological systems in such a manner. Biological organisms maintain relatively steady
states, against constant entropic dissipation, for relatively long (by thermodynamic
standards) periods of time. Of course, from a purely thermodynamical perspective,
in resisting entropy themselves, biological organisms are not countering the law of
thermodynamics. To achieve their steady state requires a constant influx of energy
– hence it is a non-equilibrium steady state (NESS). From this perspective, we can
understand biologicalorganisation to be the processof creating ‘dissipative structures’
(Kondepudi & Prigogine, 2014; Prigogine & Lefever, 1973) which only manage to
maintain themselves at steady state and reduce their own entropy at the expense of
consuming energy and increasing the entropy production rate of their environment
(Prigogine, 2017). Illustrative physical examples of similar NESS states are Benard
convectioncells,andtheBelousov-Zhabotinskyreaction(Zwanzig,2001). Inpractical
terms,wecanconsidertheNESSdensitytobethe‘phenotype’ofthesystem. Fromthe
perspectiveoftheFEP,wearenotusuallyconcernedwithwhetherasetofdynamics
possesses a NESS density, or how convergence to the NESS density works, instead
wetakeitasanaxiomthatwepossessasystemwithaNESSdensity,andareinstead
concernedwiththedynamicalbehaviourofthesystemat theNESSdensity. Whilethis
is clearly a special case, nevertheless dynamical systems at NESSalready exhibit rich
behaviours to effectively maintain themselves there, and it is these properties which
Chapter2. TheFreeEnergyPrinciple 24
necessarily any system which maintains itself at NESS, which are the fundamental
objectofstudyoftheFEP.
Secondly,nowthatwehaveasystemwhichhasaNESSdensity,andthusexhibitssome
stabilitythroughtime,wealsorequireastatisticalwaytoseparatethe‘system’fromthe
‘environment’. TheFEP handlesthisby stipulatingthat anysystem itconsiders must
fulfil aset ofcriteria whichwe callthe Markov Blanketconditions. These conditions,
derivingfrom theideaofMarkovblanketsin Bayesian networks(Pearl,2011,2014),
set forth a set of conditional independence requirements that allow a system to be
statisticallyseparatedfromitsenvironment1. Specifically,werequirethatthedynamics
of the system can be partitioned into three sets of states – ‘internal’ states which
belongtothesystemofstudy,‘external’stateswhichcorrespondtotheenvironment,
and ‘blanket states’ which correspond to the boundary between the system and its
environment. Specifically,werequiretheinternalstatestobeconditionallyindependent
of the external states given the blanket states, and vice versa. Thus all ‘influence’ of
theenvironmentmusttravelthroughtheblanket,andcannotdirectlyinteractwiththe
internalstatesofthesystemwhichare‘shielded’behindtheblanket2
Now that we have a system with a NESS density which obeys the Markov Blanket
conditions, so that we can partition it into external, internal, and blanket states, we
then wish to understand the dynamics of the system at the NESS density, so we can
understandthenecessarybehavioursofthesystemtoallowtheNESStobemaintained.
The derivation of the FEP then uses the Helmholtz (Ao) decomposition (Yuan & Ao,
2012;Yuan,Ma,Yuan,&Ao,2011;Yuan,Tang,&Ao,2017)torepresentthedynamics
asagradientflowonthelogoftheNESSdensity(whichiscalledthesurprisal)with
1WheneverwesayMarkovBlanket,followingstandarduseintheliterature,wemeantheminimal
Markovblanket–i.e. theMarkovBlanketwhichrequiresthefewestnumberofblanketstatestoachieve
therequiredconditionalindependencies.
2Interestingly, mathematically, the MB condition and all of the FEP is completely symmetrical
between‘internal’and‘external’states. Thusfromtheperspectiveofthesystem,the‘externalstates’are
itsenvironment,butfromtheperspectiveoftheenvironment,the‘externalstates’arethesystem. This
meansthattheenvironmentmodelsandperformsinferenceaboutthesystemjustasthesystemmodels
andperformsinferenceontheenvironment. Wecanthusthinkoftheenvironment-systeminteractionas
adualityofinference,whereeachtriestomodelandinfertheotherinaloop.
Chapter2. TheFreeEnergyPrinciple 25
both dissipative (in the direction of the gradient) and solenoidal (orthogonal to the
gradient) components. Now that we can express the flows of the system in terms of
gradientsofthelogNESSdensity,wetheninvoketheMarginalFlowLemmatowrite
outthedynamicsofeachcomponentofthepartitioneddynamics (i.e. external,internal,
andblanketstates)solely intermsofa gradientflow onitsownmarginalNESSdensity.
Thismeansthatwecanexpress, forinstance,thedynamicsoftheinternalstatessolely
in termsof gradient flows onthe marginal NESS densityover the internal andblanket
states.
Given this marginal partition, we can analyze and understand each of the flows in
each partition of the system independently. Specifically, to understand the Ashbyan
notion that ‘every good regulator of a system is a model of the system’ , we wish to
understandtherelationshipbetweentheflowsoftheinternalandexternalstates,which
are statistically separated from the blanket. Despite this separation, it is possible to
define a mapping between the most likely internal state, given a specific configura-
tion of the blanket states, and the distribution over the most likely external state of
the system. We can use this mapping to interpret internal states as parametrizing a
variationalorapproximatedistributionover theexternalstates. Thisinterpretationsets
up the ‘dual-aspect’ information geometry of the internal states, since internal state
changessimultaneouslyrepresentchangesinparametersofthedistributionoverinternal
states(whichcanpotentiallybenon-parametric),andchangestotheparametersofthe
variational distribution over external states. This latter interpretation means that the
internalstatesparameteriseastatisticalmanifoldequippedwithaFisherinformation
metric (if the variational distribution is in the exponential family), and in general be-
comes amenable to the techniques of information geometry (Amari, 1995; Ollivier,
Arnold,Auger,&Hansen,2017)Finally,giventhatwecaninterprettheinternalstates
as paramterising a distribution over external states, we can reconsider the gradient
flowuponthelogNESSdensitywithanewlight. Specifically,wecanunderstandthe
marginalNESSdensitytorepresenttheimplicitgenerativemodelofthesystem,andthe
Chapter2. TheFreeEnergyPrinciple 26
gradientflowdynamicsasadescentuponthefreeenergy,withaperfectBayes-optimal
posterior. Alternatively,ifweinvokeanapproximateposteriordistributionoverexternal
stateswhichisparametrizedbytheinternalstates,wecanrepresentthegradientflowof
the internal states as performing an approximate minimization of the variational free
energy(VFE),andthustheinternalstatesofthesystemcanbeinterpretedasperforming
approximatevariationalBayes. Thisisthekeyresult oftheFEP.It states,simply, that
the necessarydynamics ofany system thatmaintains itselfat a non-equilibrium steady
state,andpossessesaMarkovBlanket,canbeinterpretedasmodelling,andperforming
approximate variational inference upon the external states beyond its own Markov
Blanket. ItthusgeneralizesandmakespreciseAshby’snotionthateverygoodregulator
mustinsomesensebeamodelofthesystem(Conant&RossAshby,1970). Herewe
seethatinordertomaintainanon-equilibriumsteadystate,tocounteractthedissipative
forces inherent inthermodynamics, it is necessary to perform somekind of inference
abouttheenvironmentbeyondthesystemitself.
2.2 Formulation
Here we begin the precise mathematical description of the FEP. We aim to provide a
consistentnotation,andmoredetailedderivationsofkeyresultsthanareoftenpresented.
The presentation in this chapter mostly follows the order of presentation in Friston
(2019a), although many circumstantial topics are omitted to focus on the main flow
oftheargument. Webegin withthebasicmathematicalsettingandformulationofthe
theory. Weassume that the dynamics wewish to describe can beexpressed in terms of
aLangevinstochasticdifferentialequation(Jaswinski,1970),
dx
= f(x)+ω (2.1)
dt
wherex=[x ...x ]isavectorofstatesofsomedimensionality,andf(x)isanarbitrary
0 N
nonlinear but differentiable function of the states. Expressing the dynamics in terms
of a Langevin stochastic differential equation is a very flexible parametrization of
Chapter2. TheFreeEnergyPrinciple 27
the dynamics, and is the standard form studied in the field of stochastic differential
equations,thusallowingtheimmediateuseofresultsfromthatfield. Specifically,here
weassumealreadythatthisprocessisnothistorydependent. Thedynamicsonlydepend
ontheinstantaneousvaluesofthestates. Inpractice,historydependentsystemscanbe
representedinthisfashion,albeitsomewhatunintuitivelybyaddingsufficientstatistics
ofthehistorytothestateitself. ωisassumedtobewhite(zeroautocorrelation)Gaussian
noisewithzeromeansuchthatω=N(x;0,2Γ)whereΓisthehalfthevarianceofthe
noise. Zero autocorrelation means that the covariance between the noise at any two
timeinstants,eveninfintesimallyclosetogether,is0–E[ω ωT ]=0. Weassumethat
t t+δ
thisnoiseisaddedadditivelytothedynamics.
Thisstochasticdifferentialequationcanalsoberepresentednotintermsofdynamically
changingstates, butin termsofadynamically changingprobabilitydistribution over
states. ThistransformationisachievedthroughtheFokker-Planckequation,bywhich
wecanderivethatthechangeinthedistributionoverstatescanbewrittenas,
dp(x,t)
=−∇ f(x,t)p(x,t)+∇ Γ∇ p(x,t) (2.2)
x x x
dt
Where p(x,t) is the instantaneous distribution over the states at a given timet. Here
∇ f(x,t) is the gradient function and simply denotes the vector of partial deriva-
x
tives of the function f with respect to each element of the vector x. ∇ f(x) =
x
[ ∂f(x,t) , ∂f(x,t) ,..., ∂f(x,t) ]. ∇2f(x)representsthematrixofsecondpartialderivativesof
∂x
0
∂xN ∂xN x
thefunction.
Next, wepresuppose thatthe dynamicsexpressed inEquation 2.1 tend towards anon-
equilibrium steady state lim p(x,t) → p∗(x) where we represent the steady state
t→∞
distribution as p∗(x). Note that this distribution no longer depends on time, since it
is by definition at a steady state. We use p∗ to make clear that this distribution is at
steadystate. Bydefinitionasteadystatedistributiondoesnotchangewithtime,sothat
dp∗(x)
=0.
dt
Chapter2. TheFreeEnergyPrinciple 28
Thedistinctionbetweenanequilibriumsteadystateandanon-equilibriumsteadystate
(NESS)distributionissubtleandimportant. Anequilibriumsteadystate,mathemati-
cally,isonewherethepropertyofdetailedbalanceholds. Thismeansthatanytransition
between states at equilibrium is justas likely togo in the ‘forwards’ direction as it is
togoin the‘backwards’direction. In effect, thedynamicsarecompletely symmetric
to time, and thus there is no notion of an arrow of time in such systems. Conversely,
a non-equilibrium steady state is one where detailed balance does not hold, so there
is a directionality to the dynamics, and thus an arrow of time, even though the actual
distribution over states remains constant. From a thermodynamic perspective, the
equilibrium-steady-state isthe inexorable endpointof the second law of thermodynam-
ics, since it is the maximum entropy state. Conversely, a NESS is not a maximum
entropysolution,sincethedirectionalityofthedynamicsmeansthatthereisadegreeof
predictabilityinthesystemwhichcould intheorybeexploitedtoproducework. Non-
equilibrium steady states canarise inthermodynamic systemsbutrequire anexternal
sourceof drivingenergy asaconstantinput tothesystem, whichisthen dissipatedto
the external surroundings and gives the NESS a positive entropy production rate. To
takeanintuitiveexample,wecanthinkaboutthethermodynamicequilibriumofacup
ofcoffeewithcreamadded. Theequilibriumsteadystate(ESS)iswhenthecoffeeand
creamhavecompletelydiffusedintooneanother,sothatthecreammaintainsaconstant
proportionthroughouttheentirecoffeecup. Thiswillbetheinevitableresult(bythe
secondlawofthermodynamics)ofaddinganinitiallylowentropyhighlyconcentrated
cream scoop into the coffee. On the other hand, we can think of the non-equilibrium
steadystate(NESS)astobewhenthecreamandcoffeeareequallydiffusedthroughout,
butsomebody3 isconstantlystirring thecoffeeina specificdirection. Here,we areat
steady state because the concentrations of cream and coffee don’t change over time,
butneverthelessthereisadirectionalitytothedynamicsinthedirectionofthestirring.
3Ofcoursetheanalogyfailsheresincethisrepresentsasystemwithexternaldriving(theperson
stirring)whereasthetrueNESShasnoexternaldrivingandassuchisjust‘intrinsicallybeingstirred’
withnostirrer.
Chapter2. TheFreeEnergyPrinciple 29
Thisdirectionalityisonlymaintainedduetoaconstantinputofenergy4 tothesystem
(thestirring)5. Theflowcausedbythestirringisreferredtoasthe‘solenoidalflow’and
mathematicallyisnecessarilyorthogonaltothegradientofthesteadystatedistribution.
This is necessary so that the solenoidal flow does not ascend or descent the gradient
of the density, and thus change the steady state distribution which, as a steady state,
bydefinitioncannotchange6. Biologicalselforganizingsystemsareoftenconsidered
tobe‘dissipativestructures’,ornon-equilibriumsteadystatesfromtheperspectiveof
thermodynamics (Kondepudi & Prigogine, 2014; Prigogine & Lefever, 1973), since
they maintain a relatively steady state over time which requires a constant influx of
energytomaintain.
Given that we presuppose a system with a NESS density, we wish to understand the
dynamics at the NESS density – specifically, how does the solenoidal flow help prevent
thesystemfromrelaxingintoanequilibrium-steady-state(ESS)?Tounderstandthis,
we utilize the Helmholtz decomposition (Friston & Ao, 2012b; Yuan & Ao, 2012;
Yuanetal.,2017)torewritethedynamicsattheNESSintoaformofadissipativeand
solenoidalascentuponthegradientofthelogNESSdensity,
dx
=(Γ(x)−Q(x))∇ lnp∗(x) (2.3)
x
dt
WhereΓ(x)isadissipativecomponentoftheflowwhichtriestoascendthelogdensity.
ItistheamplitudeoftherandomfluctuationsintheoriginalSDEformulation(R.Jordan,
4It’simportanttonotethathereweareusingphysicalintuitionandconceptslike‘energy’inapurely
metaphoricalsense. AllresultshereapplytoarbitrarySDEswhichdonotnecessarilyfollowthesame
constraintsasphysicalsystems–i.e. respectconservationofenergy
5Interestingly, physicalexperiencewiththisanalogywouldsuggestthatthesolenoidaldynamics
leadingtoNESSwouldleadtofasterconvergencetotheNESSdensitycomparedtothestrictlydissipative
dynamics leading to ESS – effectively, stirring helps the cream diffuse faster. This insight has been
applied to the design of highly efficient Markov-Chain-Monte-Carlo samplers in machine learning
(M.J.Betancourt,2013;Metropolis,Rosenbluth,Rosenbluth,Teller,&Teller,1953;Nealetal.,2011)
6Importantly,inthiscoffee-creamexample,wearenotclaimingthatifthestirringisremovedthen
it will settle into a different steady state distribution, merely that the steady state with the stirring is
non-equilibriumsteadystate(NESS)whilethesteady-statewithoutstirringisanequilibriumsteady
state(ESS)duetothelackofsolenoidalflow. AddingsolenoidalflowtoanESSalwayscangeneratea
NESSwhiletheconverseisnottrue. ThereareNESSswhichcanexistsolelyinvirtueoftheirsolenoidal
dynamicswithoutacorrespondingESS.Anexampleofthiswouldbeaspinningtop,whichremains
spinningsolelyduetoitssolenoidalmotion.
Chapter2. TheFreeEnergyPrinciple 30
Kinderlehrer,& Otto,1998; Yuan, Ma, Yuan,& Ao,2010; Yuanet al.,2011), which in
effectareconstantlytryingto‘smoothout’theNESSdensityandincreaseitsentropy.
Conversely, the Q(x) represents the solenoidal portion of the flow which, although
orthogonaltothegradientofthe logpotential,successfullycounteractsthedissipative
effects of the Γ(x) terms to maintain the dynamics at a steady state. While Γ(x) and
Q(x)canintheorybestate-dependent,fromhereonoutwetypicallyassumethatthey
arenot–Γ(x)=Γ;Q(x)=Q,andadditionallyassumethatΓisadiagonalmatrix7,so
thereisnocross-correlationbetweenstatesinthenoiseaddedtothesystem.
ItisstraightforwardtoverifythattheHelmholtzdecompositionofthedynamicssatisfies
dp∗(X)
the steady state condition = 0 by plugging this form into the Fokker-Planck
dt
equation(Equation2.2),
dp∗(x)
=−∇ (cid:2) (Γ−Q)∇ lnp∗(x) (cid:3) p∗(x)+Γ∇2p∗(x)
dt x x x
=−∇ (cid:2) (Γ−Q) ∇ x p∗(x) (cid:3) p∗(x)+Γ∇2p∗(x)
x p∗(x) x
=−∇ (cid:2) (Γ−Q)∇ p∗(x) (cid:3) +Γ∇2p∗(x)
x x x
=−Γ∇2p∗(x)+∇ Q∇ p∗(x)+Γ∇2p∗(x)
x x x x
=∇ Q∇ p∗(x)=0 (2.4)
x x
Wherethelastlinefollowsbecause,bydefinition,thegradientofthesolenoidalflow
with respect to the gradient of the log density is 0, since the solenoidal flow must be
orthogonal to the gradient of the density, which is represented by the solenoidal Q
matrixbeingantisymmetricQ=−QT.
2.3 Markov Blankets
Fromthesepreliminaries,wehaveasetofdynamicsofstatesx,whichpossessaNESS
density,andwecanexpressthedynamicsattheNESSdensityintermsofdissipativeΓ
7Technically,weonlyneedtoassumeablock-diagonalmatrix,butwealsotypicallyalsoassumethat
thenoiseineachstatedimensionisindependent
Chapter2. TheFreeEnergyPrinciple 31
andasolenoidalQflowsonthegradientofthelogdensity. Now,webegintoexplore
thestatisticalstructureofthesedynamicsintermsofaMarkovBlanket. Specifically,
we next require that we can partition the states x of the dynamics into three separate
units. External states η, internal states µ, and blanket states b such that x =[η,µ,b].
Intuitively,theexternalstates represent the‘environment’;theinternal statesrepresent
the‘system’ wewishto describe,and theblanketstates representthe statistical barrier
between the system and its environment. For instance, we might wish to describe
the dynamical evolution of a simple biological system such as a bacterium in such a
manner. Here,theinternalstateswoulddescribetheinternalcellularenvironmentofthe
bacterium – thecytoplasm, thenucleus, the ribosomes etc. The external states would
be the environment outside the bacterium, while the blanket states would represent
the cell membrane, sensory epithelia, and potentially active instruments such as the
flagella which interact physically with the external environment. The key intuition
behind the FEP is that although all influence between external and internal states is
mediated by the blanket states, simply maintaining the non-equilibrium steady state
againstenvironmentalperturbationsrequiresthattheinternalstatesinsomesensemodel
andperform(variational)Bayesianinferenceontheexternalstates. TheMarkovBlanket
conditionisstraightforward. Itsimplystatesthat theinternalandexternal statesmust
beindependentgiventheblanketstates,
p∗(x)= p∗(η,µ,b)= p∗(η|b)p∗(µ|b)p∗(b) (2.5)
Whilein probabilistictermsthisfactorisationisstraightforward, ithasmore complex
consequencesforthedynamicalflowofthesystem. Firstly,weadditionallydecompose
theblanketstatesintosensorysandactiveastatessuchthatb=[s,a]andthus,ultimately
x=[η,µ,s,a]. Sensorystates areblanketstates thatare causal children of theexternal
states – i.e. the states that the environment acts on directly. Active states are those
blanket states that are not causal children of the external states. Essentially, external
statesinfluencesensorystates,whichinfluenceinternalstates,whichinfluenceactive
Chapter2. TheFreeEnergyPrinciple 32
Figure 2.2: The intuition behind the Markov Blanket partition. The brain (or bacillus)
consistsofinternalstatesµwhichareseparatedfromtheoutsideworld(externalstates
η by the blanket states b, which can themselves be partitioned into sensory states s,
representingthesensoryepithelia,andwhicharedirectlyinfluencedbyexternalstates,
andactivestatesarepresentingtheorganismseffectorsandwhicharedirectlyinfluenced
by internal states, and act on external states. We see that perception concerns the
minimizationoffreeenergyoftheinternalstates,whileactionconcernstheminimization
of the expected free energy of the active states. Figure originally appeared in Friston
(2019a)
Chapter2. TheFreeEnergyPrinciple 33
states, which influence external states. The circular causality implicit in this loop is
what allows theMarkov Blanketcondition torepresent theperception-action loop. For
notationalpurposes,wealsodefineautonomousstatesα=[a,µ]whichconsistofactive
and internal states, and particular states π=[µ,s,a] which consist of sensory, active,
andinternalstates.
The next step is to understand what the conditional independence requirements put
forth in Equation 2.5 imply for the dynamics of the flow. Specifically, we obtain the
marginalflowlemma(seeFriston(2019d)forafullderivation),whichstatesthatthe
marginal flow of a partition, averaged under its complement, can be expressed as an
Ao-decomposed flow on the gradients of the log of the marginal distribution. For
instance, the flow of the internal states µ, averaged under the complement π˜ of the
particularstatesπcanbeexpressedas,
dµ(x)
f (π)=:E [ ]=(Γ −Q ∇ lnp∗(π)+Q ∇ lnp∗(π) (2.6)
µ p(π˜|π)
dt
µµ µµ) µ µµ˜ µ˜
Importantly, we see that the marginal flow lemma allows us to express the flow of a
subsetofstates,averagedundertheircomplements,intermsofindependentHelmholtz
decompositions on their marginal NESS densities, if we ignore solenoidal coupling
terms (such as Q ). This allows us to investigate in detail the information-theoretic
µµ˜
interactions of one set of states with another, and allows us to gain intuition and
understandingofthecoreinformation-theoreticpropertiesoftheperception-actionloop.
Forinstance, usingthemarginal flowlemmawe canexpresstheflowofautonomous
(activeandinternal)α=(a,i)as,
f (x)=(Γ −Q )∇ lnp∗(π) (2.7)
α αα αα α
whereweseethatautonomousstatesfollowagradientdescentonthemarginalNESS
densityoftheinternal,sensory,and activestates,andattempttosuppresstheir surprisal
or,onaverage,theirentropy. Wecanuseaseriesofmathematical‘inflationarydevices’
Chapter2. TheFreeEnergyPrinciple 34
toexpressthissurprisalintermsofitsinteractionwiththeexternalstatesbeyondthe
blanket.
−lnp∗(π)=E (cid:2) −lnp∗(π) (cid:3)
p∗(η|π)
=E (cid:2) lnp∗(η|π)−lnp∗(η,π) (cid:3)
p∗(η|π)
=E (cid:2) lnp∗(η|π)−lnp∗(π|η)−lnp∗(η) (cid:3)
p∗(η|π)
=E (cid:2) −lnp∗(π|η) (cid:3) +D (cid:2) p∗(η|π)||p(η) (cid:3) (2.8)
p∗(η|π) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Inaccuracy Complexity
Thus we can see that the flow of autonomous states acts to minimize the inaccuracy
(maximizeaccuracy)andminimizethecomplexityoftheexternalstateswithrespect
totheparticularstatesofthesysteminquestion. Parsedintomoreintuitiveterms,we
canthusseethattheflowof‘system’states(π)aimtomaximizethelikelihood ofthe
internalstatesgiventheexternalstates–i.e. performmaximumlikelihoodinferenceon
themselves(c.f. ‘selfevidencing’(Hohwy,2016))–whilesimultaneouslyminimizing
thecomplexity –orthe divergence betweentheexternal states giventheinternal states,
andthe‘prior’distribution overtheexternalstates. Inshort,by re-expressingtheflow
ininformation-theoreticterms,wecanobtainadecompositionoftheentropyterminto
intuitive and interpretable sub-components which can help us reason about the kinds of
behavioursthesesystemsmustexhibit.
2.4 Variational Inference
Variationalinferenceisatechniqueandmethodforapproximatingintractableintegrals
inBayesianstatistics(Feynman,1998;Fox&Roberts,2012;Ghahramani&Beal,2001;
M.Jordan,Ghahramani,Jaakkola,&Saul,1998;M.I.Jordan,Ghahramani,Jaakkola,
& Saul, 1999; Neal & Hinton, 1998). Typically, a direct application of Bayes-rule to
computeposteriorsincomplicatedsystemsfailsduetotheintractabilityofthelogmodel
evidence,whichappearsinthedenominatorofBayes’rule. Whilethereexistnumerical
or sampling-based methods to precisely compute this integral, they typically scale
Chapter2. TheFreeEnergyPrinciple 35
poorlywiththedimensionoftheproblem–aphenomenonwhichisknownasthecurse
of dimensionality (Goodfellow, Bengio, & Courville, 2016). Variational techniques
originated frommethods in statisticalphysics in the1970s and 1980s (Feynman, 1998),
andwerethentakenupinmainstreamstatisticsandmachinelearninginthe1990s(Beal,
2003;Ghahramani&Beal,2001;M.Jordanetal.,1998)wheretheyhavebecomean
influential,oftendominantapproachforapproximatingposteriorsandfittingcomplex
high-dimensionalBayesianmodelstodata(Beal,2003;Blei, Kucukelbir,&McAuliffe,
2017; Dayan, Hinton, Neal, & Zemel, 1995; Feynman, 1998; Ghahramani, Beal, et al.,
2000;M.I.Jordanetal.,1999;Kingma&Welling,2013).
Thecoreideaofvariationalinferenceistoapproximateanintractableinferenceproblem
withatractableoptimizationproblem. Thus,insteadofdirectlycomputing aposterior
distribution p(H|D)where H issome setof hypothesesand D isthe data, weinstead
postulatean approximate orvariationaldistributionq(H|D;θ)whichis often, although
not always, parametrized with some fixed number of parameters θ. We then seek to
optimize the parameters θ to minimize the divergence between the approximate and
trueposterior,
θ∗ =argminD [q(H|D;θ)||p(H|D)] (2.9)
KL
θ
Unfortunately, this optimization problem is itself intractable since it contains the in-
tractable posterior as an element. Instead, we minimize a tractable bound on this
quantitycalledthevariationalfreeenergy(VFE)F(D,θ),
F(D,θ)=D [q(H|D;θ)||p(H,D)]
KL
=D [q(H|D;θ)||p(H|D)]−lnp(D)
KL
≥D [q(H|D;θ)||p(H|D)] (2.10)
KL
Since the VFE is based on a divergence between the variational distribution and the
generativemodel p(D,H),itistractableasweassumeweknowthegenerativemodel
thatgaverisetothedata. ByminimizingtheVFE,therefore,wereducethedivergence
Chapter2. TheFreeEnergyPrinciple 36
between the true and approximate posteriors, and thus improve our estimate of the
posterior.
Secondly, the variational free energy is simultaneously a bound upon the log model
evidencelnp(D),aquantityofgreatimportantformodel-selection(Friston,Parr,&Zei-
dman,2018;Geweke,2007),andwhichisusuallyintractabletocomputeduetotheim-
(cid:82)
plicitintegrationoverallpossiblehypotheses(orparameters) p(D)= dHp(D|H)p(H).
lnp(D)=D [q(H|D;θ)||p(H,D)]−F(D,θ)
KL
≥−F(D,θ) (2.11)
The second line follows due to the non-negativity of the KL divergence. The VFE is
the foundation of the free energy principle as, we shall show, we can interpret self-
organizingsystemswhich maintain themselvesatanon-equilibrium-steadystate to be
implicitlyminimizingtheVFE,andthusperformingvariationalBayesianinference.
ItisimportanttonoteherethatwhilethevariationalfreeenergyF isnottechnicallya
KL divergence, since the two distributions it involves do not share the same support
(one being a posterior and the other a joint), for notational convenience in this thesis
we slightly abuse the KL notation to represent free energies of one form or another.
Formally,wewilluseF[q(x),p(x,y)]=E [lnp(y|x)]+D [q(x)||p(x)]=lnp(y)+
q(x) KL
D [q(x)||p(x|y)]:=D [q(x)||[(x,y)].
KL KL
We can gain some intuitionfor the effectsof minimizing theVFE by decomposing into
variousconstituentterms. Herewe showcase twodifferentdecompositions which each
givelighttocertainfacetsoftheobjectivefunction,
F(D,θ)=E )[lnp(H,D)]−H[q(H|D;θ)] (2.12)
q(H|D;θ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Energy Entropy
=−E [lnp(D|H)]+D [q(H|D;θ)||p(H)] (2.13)
q(H|D;θ) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Accuracy Complexity
Hereweseethatwecandecomposethevariationalfreeenergyintotwoseparatedecom-
positions,eachconsistingoftwoterms. ThefirstdecompositionsplitstheVFEintoan
Chapter2. TheFreeEnergyPrinciple 37
‘energy’term,whicheffectivelyscoresthelikelihoodofthegenerativemodelaveraged
under the variational distribution, whilst the entropy term encourages the variational
distributiontobecomemaximallyentropic. Essentially,thisdecompositioncanbeinter-
preted as requiring that the variational distribution maximize the joint probability of
thegenerativemodel(energy),whilesimultaneouslyremainingasuncertainaspossible
(entropy)8. Theseconddecomposition–intoan‘accuracy’anda‘complexity’term–
speaksmoretotheroleoftheVFEininference. Heretheaccuracytermcanbeinter-
pretedasdrivingthevariationaldensitytoproduceamaximumlikelihoodfitofthedata,
bymaximizingtheirlikelihoodunderthevariationaldensity. Thecomplexitytermcan
beseenasaregularizer,whichtriestokeepthevariationaldistributionclosetotheprior
distribution, and thus restrains variational inference from pure maximum-likelihood
fitting.
2.5 Intrinsic and Extrinsic information geometries
Now,wewishtounderstandtherelationshipbetweentheinternalstatesandtheexternal
states, which are separated by the blanket states. Importantly, the existence of the
blanket means that we can define a mapping between the most likely internal state,
given a specific blanket state, and a distribution over external states 9. We define the
mostlikelyinternalandexternalstatesgivenablanketstateas,
ηηη(b)=argmaxp(η|b)
η
µµµ(b)=argmaxp(i|b) (2.14)
µ
Next, we assume that there is a smooth and differentiable function σ which maps
8Interestingly,thisenergy,entropydecompositionispreciselywhythisinformation-theoreticquantity
isnamedthevariationalfreeenergy. Thethermodynamicfreeenergy,acentralquantityinstatistical
physics,hasanidenticaldecompositionintotheenergyandtheentropy.
9Thisfunctionisdefinedifweassumeinjectivitybetweenthemostlikelyinternalandblanketstates
(Parr,DaCosta,&Friston,2020).
Chapter2. TheFreeEnergyPrinciple 38
betweenthemostlikelyinternalandexternalstatesgivenablanketstate,
ηηη(b)=σ(µµµ(b)) (2.15)
Importantly, we interpret the output of this function – the most likely external states
given the blanket states – as parametrizing the mean over a full distribution over the
external states, as a function of the internal states q(η;ηηη(b)) = q(η;σ(µµµ(b))). This
allowsustointerprettheflowofinternalstatesasparametrisingdistributionsoverthe
externalstates.
Crucially,wecansaythatifanygivensetofinternalstatesparametrizesadistribution
over external states, then the space of internal states effectively represent a space of
distributions over external states, parametrized by internal states. This space of dis-
tributions may be, and usually is, curved and non-euclidean in nature. The field of
information geometry has emerged to allow us to describe and mathematically charac-
terisesuchspacescorrectly(Amari,1995;Caticha,2015). Akeyresultininformation
geometryisthatthespaceofparametersoffamiliesofexponentialdistributionsisanon-
euclideanspacewiththeFisherInformationasitsmetric. Ametricissimplyanotionof
(cid:113)
distanceforagivenspace. Forinstance,inEuclideanspace,themetricis ∑ Nx2 where
µ µ
N isthedimensionalityofthespace. Wecanrepresentgeneralcoordinatetransformers
onspaceswithanymetricthroughtheuseofametrictensorGGG. Essentially,wemeasure
differencesbetweendistributionsintermsoftheKLdivergence,andthusifwewantto
seehowaninfinitesimalchangeintheparametersofadistributionresultsinchangesto
thedistributionitself,wecanmeasureaninfinitesimalchangeintheirKLdivergenceas
afunctionoftheinfinitesimalchangeintheparameters. i.e.
∂p(x;θ)
= lim D [p(x;θ)||p(x;θ+δθ)] (2.16)
KL
∂θ δθ→0
Inthe caseof thespaceof parametersofexponential distributions,the metrictensoris
theFisherinformation,whicharisesasfromthe Taylorexpansion oftheinfinitesimal
KLdivergencebetweenthetwodistributions. Wedefineθ(cid:48) =θ+δθ. Specifically,since
Chapter2. TheFreeEnergyPrinciple 39
thereisonlyaninfintesimalchange,wecanTaylor-expandaroundθ(cid:48) =θtoobtain,
∂D [p(x;θ)||p(x;θ(cid:48))]
D [p(x;θ)||p(x;θ(cid:48))]≈D [p(x;θ)||p(x;θ)]+ KL | (θ−θ(cid:48))
KL KL θ=θ(cid:48)
∂θ
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
=0
=0
∂2D [p(x;θ)||p(x;θ(cid:48))]
+ KL | (θ−θ(cid:48))2 (2.17)
∂θ2
θ=θ(cid:48)
‘Wherethefirsttwotermsvanish,soweneedonlyhandlethesecondterm,
∂2D [p(x;θ)||p(x;θ(cid:48))]
D [p(x;θ)||p(x;θ(cid:48))]≈ KL | (θ−θ(cid:48))2
KL ∂θ2 θ=θ(cid:48)
(cid:90) ∂lnp(x;θ)∂lnp(x;θ)
= p(x;θ)
∂θ ∂θ
=F (2.18)
where µ is the Fisher information. Since the internal states can be interpreted as
parametrizingdistributionsoverexternalstates,asparameters,theylieonaninformation-
geometricmanifoldwithaFisherinformationmetric. Thisistheextrinsicinformation
geometry. Simultaneously,theinternalstatesalsoparametrize(implicitly)asecond(em-
pirical)distributionovertheinternalstates. Thisparametrizationgivesrisetoasecond
informationgeometry–theintrinsicgeometry,sinceitrepresentstherelationshipthe
internalstateshavetothedistributionoverthemselves. Specifically,supposeµµµdefine
thesufficientstatisticsofavariationaldensityoverinternalstatesq(µ;µµµ),andηηη=σ(µµµ)
definethesufficientstatisticsofthevariationaldensityoverexternalstatesq(η;ηηη),then
wecanseethattheinternalstatesinfactparametrizetwodensitiesandthuspartakein
twosimultaneousinformationgeometries. First,thereisametricdefinedoverthespace
ofinternaldensities,
∂2D [q(µ;µµµ)||q(µ;µµµ+δµµµ)]
KL
µ(µµµ)= | (2.19)
∂µµµ2 µµµ+δµµµ=µµµ
whichiscalledtheintrinsicinformationgeometry. Andsecondly,ametricdefinedover
thespaceofexternaldensities,parametrizedbyinternalstates,
∂2D [q(η;ηηη)||q(η;ηηη+δηηη)]
KL
µ(ηηη)= | (2.20)
∂ηηη2 ηηη+δηηη=ηηη
Chapter2. TheFreeEnergyPrinciple 40
which is called the extrinsic information geometry. These well-defined intrinsic and
extrinsic information geometries, allow us to interpret the motion of the internal as
also movement on the intrinsic and extrinsic statistical manifolds. Crucially, enabling
usto makemathematicallyprecisethe linkbetweentwo conceptuallydistinctideas –
dynamicalmotioninspace,andvariationalinference(i.e. Bayesianbeliefupdating)on
parametersofdistributions. Usingthisunderlyinginformation-geometricframework,
inthenextsectionweshallgoontoseehowwecaninterpretthedynamicsofanon-
equilibriumsystemat NESSasperformingapproximatevariationalBayesian inference
onitsexternalenvironment.
2.6 Self-Organization and Variational Inference
Herewepresentthekeyresultsofthefreeenergyprincipleviathefreeenergylemma.
This says, firstly, that the dynamics of the autonomous states can be interpreted as
minimizingafreeenergyfunctionalovertheexternalstates,andthus can beconstrued
asperformingakindofelementalBayesian(variational)inference. Specifically,wewill
firstconsiderthegeneralcaseintermsofthe‘particular’freeenergy,whichstipulatively
assumesthatthesystemobtainsthecorrectposteriorateverytime-point,renderingthe
traditional variational bound superfluous, and thus demonstrating that in a way self-
organizingsystemsmaintainingthemselvesatNESScanbeconstruedasperforming
exact Bayesian inference on the generative model they embody through their NESS
density. We thus reach the key statement of the FEP – that the dynamics of self-
organizingsystemsthatmaintain themselves atNESScanbe interpreted asperforming
exact Bayesian inference on the external states beyond their blanket or, alternatively,
theycanbeinterpretedasapproximatingapproximate(variational)Bayesianinference.
We thenintroducethe generalcaseof thevariationalfreeenergy,which isingeneral a
bounduponthelogofthe NESSdensity,andweshowinthespecialcaseofassuming
thatthevariationaldistributionoverexternalstateswhichisparametrizedbytheinternal
Chapter2. TheFreeEnergyPrinciple 41
states can be approximated by the Laplace approximation, that we can interpret the
flow of autonomous states as directly performing a descent upon the variational free
energy and thus directly performing variational Bayesian inference. Since we, as
the modeller, can specify the variational distribution in any desired way, then this
meansthat thisinterpretation istenable foran extremely widerange ofsystems. The
Laplaceapproximation approximatesthevariationaldistributionas aGaussian where
the variance is a function of the curvature at the mean. Intuitively, this assumption
is that the Gaussian is tightly peaked around the mean value. This approximation is
theoretically well-justified, due to the underlying Gaussianity of the stochastic noise
in the system, and the likely concentration of the probability mass around the mean.
Moreover,theGaussiandistributionarisesregularlyinnaturewheneveraveragesover
largenumbers ofindependent eventsare taken(c.f. the CentralLimit Theorem(CLT)),
andcan thusbeconsidereda naturalmodellingchoice fordistributionof themodeof
theexternalstatesgiventheblanket,whichlikelyis composedofcontributionsfroma
largenumberofspecificexternalstates.
Torecall,wecanwritetheflowofautonomousstatesα=(i,a)intermsofagradient
descent on the log NESS density of the particular states lnp(π) with both dissipative
andsolenoidalcomponentsviatheHelmholtzdecomposition.
f (x)=−(Γ−Q)∇ lnp∗(s,α) (2.21)
α α
Then we can define the particular free energy as the variational free energy, where
the variational distribution over external states, is stipulatively defined to be equal to
the ‘true’ posterior distribution over external states given the particular states q(η|π)=
p∗(η|π) 10. With this assumption, we can define the particular free energy using the
10Weimplicitlyassumeherethatthevariationaldistributioncanbestipulatedtobeofthesamefamily
ofthetrueposterior,sothattheycanmatchoneanother
Chapter2. TheFreeEnergyPrinciple 42
standardformforthevariationalfreeenergy
F =D [q(η|π)||p∗(η,π)]
particular KL
=−E [lnp∗(η|π)]+D [(qη|π)||p∗(π)]
q(η|π KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Accuracy Complexity
=lnp∗(π)+D [q(η|π)||p∗(η|π)]
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Evidence Bound
=lnp∗(π) (2.22)
where the last line follows because the bound is always 0 since we have defined the
variational and true posteriors to be the same. Importantly, we see that the particular
freeenergyisthenequaltothelogoftheNESSdensityoverthesensory,internal,and
activestates. Assuch,wecanrewritethedynamicsoftheautonomousstatesdirectlyin
termsoftheparticularfreeenergy,
f (x)=−(Q−Γ)∇ F (s,α) (2.23)
α α particular
While this may seem like just a mathematical sleight of hand, it demonstrates how
systems which maintain the statistical structure of a Markov Blanket at equilibrium
caninfactbeinterpretedasperformingvariationalBayesianinferencewithacorrect
posteriordistribution. If,conversely,werelaxthisassumptionsomewhat,sothat,asis
typicalforvariationalinferencewhentheclassofdistributionsrepresentedunderthe
variationaldensitydoesnotincludethetrueposterior,thenweretainanapproximate
relationship. Thatis,whenq(η|π;θ)≈ p(η|π),weobtain,
F =D [q(η|π)||p∗(η,π)]
KL
=lnp∗(π)+D [q(η|π)||p∗(η|π)]
KL
≈lnp∗(π)
=⇒ f (x)≈−(Q−Γ)∇ F(s,α)
α α
Sowe canseethat inthis case,wecan interpretthe dynamics ofthe autonomousstates
as approximating approximate Bayesian inference. This is perhaps the most general
Chapter2. TheFreeEnergyPrinciple 43
statement of the FEP – that the dynamics of a system which maintains the statistical
structure ofa Markov Blanketat NESS againstexternal dissipative perturbations,can
be interpretedas performing approximatevariational Bayesian inferenceto optimize a
distributionovertheexternalstatesoftheenvironment,parametrizedbyitsowninternal
states. Thedistinctionbetweenvariationalandparticularfreeenergy,withtheparticular
free energy always using the stipulatively correct posterior, while being somewhat a
mathematicaltrick,isalsoausefulphilosophicaldistinctiontodraw. Ineffect,wecan
thinkofthesystemasalwaysperformingcorrectBayesianinference,simplybecause
the inference is over the system itself, where the generative model of the system is
simply its NESS density. Conversely, we can see the approximation arising from the
approximate variational distribution as being related to the imperfection of our own
understandingofthesystemasanexogenousmodeller. Thesystemisperfectlyhappy
usingitsBayes-optimalposterioratalltimes. Avariationaldistributiondistinctfrom
this posterior must be, in some sense, the creature and creation of the modeller, not
of the system, and as such the approximations to the dynamics that arise from this
approximation is due to the approximations implicit in modelling rather than in the
dynamicsofthesystemper-se. Itisalsoimportanttonotethatwhilewehaveusedan
approximationsign,inrealitythevariationalfreeenergyisanupperbound uponthelog
modelevidenceortheparticularfreeenergy– i.e. F ≥F andtheapproximate
particular
dynamics can be interpreted as driving the system towards the minimization of this
bound,andthusincreasingtheaccuracyoftheapproximationinamanneranalogousto
thesimilarprocessinherentinvariationalinference.
Whileinthegeneralcaseabove,therelationshipbetweenthedynamicsofthesystem
andvariationalinferenceisonlyapproximate,ifweareonlyinterestedinthemodeof
theexternalstates–i.e. themostlikelyexternalstateconfiguration–insteadofthefull
distribution, then the approximation becomes exact and we can directly see that the
dynamicsofthe systemdoperformvariationalinferenceupon themodeofthe external
states. Herewecanseethat,inasense,themaximum-a-posteriori(MAP)modesfor
Chapter2. TheFreeEnergyPrinciple 44
theinternalstatespreciselytracktheMAPmodesfortheexternalstatesandthus,under
theLaplaceapproximation,canbeseenasdirectlyperformingaminimizationofthe
variationalfreeenergy.
Firstly, recall from previously that we had defined the smooth mapping between the
modes of the external and internal states given the blanket state, ηηη(b)=σ(µµµ(b)). By
applying the chain rule to this function, it is straightforward to derive the flow of the
externalmodewithrespecttotheinternalmode,
∂σ(µµµ(b))
f (b)= f (b) (2.24)
ηηη µµµ
∂µµµ(b)
Then, assuming that the mapping is invertible (requiring that the internal states and
externalstates have thesame dimensionality),orrather inthe generalcasethat ithasa
Moore-Penrose pseudoinverse, we can express the dynamics of the internal mode in
termsofthedynamicsoftheexternalmode,
∂σ(µµµ(b)) −1
f (b)= f (b) (2.25)
µµµ ηηη
∂µµµ(b)
Similarly, we can derive the expression of the NESS density over the external mode
in terms of the mode of the internal states, which provides a precise mapping, called
thesynchronizationmanifold,betweenthetwodensities,eventhough theyareinfact
separatedbytheMarkovBlanket,
∂lnp(ηηη(b)|b) ∂lnp(ηηη(b)|b)∂σ(µµµ(b))
= (2.26)
∂µ ∂ηηη(b) ∂µ
Combining Equation 2.24 and Equation 2.26 and using the fact that the flow of the
external mode, bythe marginal flowlemma is, f (b)=(Γ −Q )∇ lnp(ηηη(b)|b), we
ηηη η η η
canexpresstheflowoftheinternalmodeintermsofthemarginalNESSdensityover
the external states, thus understanding how the internal states probabilistically track
Chapter2. TheFreeEnergyPrinciple 45
changesintheirenvironment,
∂σ(µµµ(b)) −1dηηη(b)
f (b)=
µµµ
∂µµµ(b) dt
∂σ(µµµ(b)) −1
= (Γ −Q )∇ lnp(ηηη(b)|b)
η η η
∂µµµ(b)
∂σ(µµµ(b)) −1 ∂σ(µµµ(b)) −1 ∂σ(µµµ(b))
= (Γ −Q ) ∇ lnp(ηηη(b)|b)
η η η
∂µµµ(b) ∂µµµ(b) ∂µµµ(b)
=(Γ −Q )∇ lnp(σ(µµµ(b))) (2.27)
σ σ µ
∂σ(µµµ(b)) −1 ∂σ(µµµ(b)) −1
where(Γ −Q )= (Γ −Q ) . Crucially,thisexpressionallowsus
σ σ ∂µµµ(b) η η ∂µµµ(b)
towritetheflowoftheinternalmodeasagradientdescentontheNESSdensityofthe
external mode as a function of the internal mode, given the blanket, with respect to
theinternalstates. Fascinatingly,thisrelationshiptakesthesamegeneralformofthe
HelmholtzdecompositionwithseparatedissipativeΓ andsolenoidalQ components
σ σ
which are simply the original dissipative and solenoidal components with respect to
theinternalstatesmodulatedbytheinverseofthemappingfunctionσ. Ineffect,this
implementsacoordinatetransformbetweenthecoordinatesoftheflowoftheexternal
statestothecoordinates oftheflowofthe mode oftheexternalstates,as afunctionof
internalstates.
NowwedemonstratehowwecaninterpretthisgradientdescentontheNESSdensity
of the mode over external states in terms of a direct descent on the variational free
energy,andthusasdirectlyandexactlyperformingvariationalinference. First,wemust
define ourvariational distribution q(ηηη|b;µµµ)which isa distributionover themodes of
external states, given the blanketstates, parametrized by the mode ofthe internal states.
Sincewe areonlyinterested nowin distributionsover themodeofthe externalstates,
a reasonable assumption is that it is approximately Gaussian distributed due to the
centrallimittheorem. ThismeansthataLaplaceapproximation,whichisaGaussian
approximation where the covariance is simply a function of the mean, derived via a
secondorderTaylor-expansionofthedensityatthemode,isagoodapproximationto
Chapter2. TheFreeEnergyPrinciple 46
usehere. Wethusdefinethevariationaldensityas,
q(ηηη|b;µµµ)=N(ηηη;µµµ,Σ(µµµ))
∂2σ(µµµ) −1
whereΣ(µµµ)= (2.28)
∂σ2
Importantly,ifwesubstitutethisdefinitionofqintothevariationalfreeenergyanddrop
constantsunrelatedtothevariationalparametersµµµ,weobtain,
1 ∂2σ(µµµ) −1
F =lnp(µµµ,b)+ tr(Σ(µµµ)) +ln|Σ(µµµ)|
2 ∂σ2
∂F ∂lnp(µµµ,b)
=⇒ =
∂µ ∂µ
The second line follows since this is the only term where i is directly utilized. Then,
from this definition, we can see that the variational free energy is actually precisely
thegradienttermweseeintheexpressionfortheflowoftheinternalstatemode,thus
allowingustorewriteitas,
f (b)=(Γ −Q )∇ F (2.29)
µµµ σ σ µ
After this thicket of mathematics, we thus see a crucial result for the FEP. That, with
aLaplace-encodedvariationaldensity,wecanseethatthemodeoftheinternalstates
precisely tracks the mode of the external states, and the dynamics that allows it to
do so are precisely those of a gradient descent on the variational free energy, thus
enablinganexactinterpretationoftheflowoftheinternalstatesasperformingBayesian
inferenceontheexternalstates. ThisproofdemonstratesthefundamentallyAshbyan
natureofself-organizationatnon-equilibriumsteadystate,wheresystems,inorderto
maintaintheirsteadystate,andthusexistenceasdistinctsystems,arenecessarilyforced
toengageinsomedegreeofmodellingortrackingexternalstatesoftheenvironment,
inordertocountertheirdissipativeperturbations. Interestingly,thisexactrelationship
to variational inferenceonly emerges whenconsidering the modes of thesystem, not
the full distribution over environmental and internal states as was done previously,
where we only obtained an approximation to variational inference. Perhaps this is
Chapter2. TheFreeEnergyPrinciple 47
because, in some sense, the system need not perform inference on full distributions,
but only on modes. This perhaps makes more intuitive sense within the cybernetic
Ashbyanparadigm where,in general,the systemis seenassignificantly smallerthan
the environment, and thus simplycannot be expected to encode a fully accuratemodel
of the entire environment which, in the extreme case, includes the entire rest of the
universe. Instead, the system simply models and tracks coarse-grained environmental
variablessuchasthemode.
2.7 The Expected Free Energy and Active Inference
Sofar,wehaveonlyconsideredtherelationshipbetweeninternalandexternalstates,
and observed that the flow of the internal state can be considered to be performing a
variationalgradientdescentontheparametersofthevariationaldensityoverexternal
states. Theinternalstatedynamicsexactlyfollowavariationalgradientdescentifwe
assumethattheinternalstatesparametrizeaLaplacianapproximateposterior,orthey
approximately follow a variational gradient descent if we assume a broader class of
variational posteriors. From this, we can interpret the flow of the internal states as
performingsomekindof‘perceptual’inferenceaboutthecausesoffluctuationsinthe
blanketstates–namely,theexternalstates. Butwhatabouttheactivestates? Howdo
theyfitintothispicture?
First,werecallfromtheapproximateBayesianinferencelemmathatwecanexpressthe
flowoftheautonomousstates(activeandinternal)intermsofanapproximategradient
descentonthevariationalfreeenergy(Equation2.29). Bythemarginalflowlemma,if
we ignore solenoidal coupling betweeninternal and active states, we can partition this
descentintoseparate(marginal)descentsontheinternalandtheactivestates,allowing
ustowritetheflowoftheactivestatesas
f (x)≈(Γ −Q )∇ F(s,α) (2.30)
a aa aa a
Chapter2. TheFreeEnergyPrinciple 48
where Γ and Q are the block matrices corresponding solely to the interactions
aa aa
betweenactivestatesinthelargerΓandQmatrices. Crucially,ifwerecallthedefinition
ofthevariationalfreeenergy,
F(π)=D [q(η|π;µµµ)||p∗(η,π)]
KL
=−E [−lnp∗(π|η)]+D [q(η|π;µµµ)||p∗(η)] (2.31)
q(π|µµµ) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Inaccuracy Complexity
Crucially,theonlyterminthisdecompositionthatdependsontheactivestatesaisthe
firstinaccuracyterm. Thus, we can straightforwardly write down the flow of the active
statesas,
f (x)≈(Γ −Q )∇ E [−lnp∗(π|η)] (2.32)
a aa aa a q(η|µµµ)
Where we can intuitively see that the flow of the active states effectively minimize
inaccuracy (or maximize accuracy). In effect, we can interpret the flow of the active
statesattheNESSdensitytotrytoensurethatthevariational‘beliefs’encodedbythe
blanketand internalstates ofthe systemare asaccurate as possible. Sinceactive states
canonlyinfluenceexternalstatesandnot internalstates,thewaythisisachievedisby
actingupontheexternalstatestobringthemintoalignmentwiththebeliefsrepresented
bytheinternalstates–henceactiveinference.
While this provides a good characterisation of the flow of the system at equilibrium,
weareoftenalsointerestedinthepropertiesofdynamicalsystemsastheself-organize
towardsequilibrium. Specifically,wewishtocharacterisethenatureoftheactivestates
during thisprocess of self-organization, sothat we canunderstand the necessarykinds
ofactivebehaviouranyself-organizingsystemmustevince. Tobegintounderstandthe
natureofthisself-organizationwefirstdefineanotherinformationtheoreticquantity,the
ExpectedFreeEnergy(EFE)whichservesasanupper-boundonsurprisalthroughout
the entire process of self-organization, with equality only at the equilibrium itself.
Sincewehavethisupper-bound,wecaninterpretself-organizingsystemsawayfrom
equilibrium, by following their surprisal dynamics as approximating expected free
Chapter2. TheFreeEnergyPrinciple 49
energy minimization, using logic directly analogous to the approximate Bayesian
inferencelemma. Conversely,turningthislogicaroundletsusconstruct self-organizing
systemsby definingsome desiredNESS density, andthen prescribingdynamics which
simplyminimizetheEFE.
Tohandlesystemsawayfromequilibrium,wedefinesomenewterminology. Wedefine
p(η ,µ ,s ,a |η ,µ ,s ,a )tobetheprobabilitydensityoverthevariablesofthesystem
t t t t 0 0 0 0
atsometimet,whichdependsonsomesetofinitialconditionse ,µ ,s ,a . Tosimplify,
0 0 0 0
weaverageovertheexternalinitialconditionandonlyrepresenttheparticularinitial
conditionπ =(µ ,s ,a ). NextwedefinetheexpectedfreeenergyG(π)similarlyto
0 0 0 0
thevariationalfreeenergy,butwiththecurrent-timepredictivedensitytakingtheplace
oftheapproximatevariationalposterior,andtheNESSdensitytakingtheplaceofthe
generativemodel.
G(π)=E [lnp(η |π ,π )−lnp∗(η,π)]
p(ηt,πt)|πt) t t 0
=E [−lnp∗(π|η)]+D [p(η |π ,π )||p∗(η)] (2.33)
p(ηt,πt)|πt) KL t t 0
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Ambiguity Risk
We see that the EFE mandates the minimization of both ambiguity (i.e. avoiding
situationswhichareheavilyuncertain)andrisk(avoidinglargedivergencesbetween
thecurrentstatedensityandtheequilibriumstate. Itisstraightforwardtoseethatthe
EFEisanupperboundontheexpectedpredictivesurprisalatanytime-point,byusing
thefactthattheKL-divergenceisalwaysgreaterthanorequalto0,
D [p(η ,π |π )||p∗(η,π)]≥0
KL t t 0
=⇒ G(π )+E lnp(π |π )]≥0
t p(ηt,πt)|πt) t 0
=⇒ G(π )≥−E [lnp(π |π )]
t p(ηt,πt)|πt) t 0
Similarly,itisstraightforwardtoseethatatequilibrium,theEFEsimplybecomesthe
surprisal.
D [p(η ,π |π )||p∗(η,π)]=G(π )+E [lnp(π |π )]=0
KL t t 0 t p(ηt,πt)|πt) t 0
=⇒ G(π )=−E [lnp(π |π )] (2.34)
t p(ηt,πt)|πt) t 0
Chapter2. TheFreeEnergyPrinciple 50
Sincethisisthecase,wecanunderstandtheEFEaseffectivelyquantifyingthediscrep-
ancybetweenthecurrentpredictivedensityandtheequilibrium. Becauseofthis,we
canseethattheEFEisnecessarilyaLyapunovfunctionofself-organizingdynamics,
and it makes sense to interpret self-organizing dynamics under a Markov blanket as
minimizing the EFE. Conversely, if one wants to define a set of dynamics that self-
organizetosomegivenattractor p∗(η,π)thenonesimplyneedstodefinedynamicsthat
minimizetheEFEtoachieveconvergencetotheequilibrium(inthecasewherethere
arenolocalminima).
Takingthisconverseapproachallowsustomovefromsimplyprovidinganinterpretative
characterisationofgivendynamicsintermsofinference,andmoveinsteadtoconstruct-
ingor definingsystems, or agents,which canachieve specificgoals. This approachis
takenintheliteratureonactiveinferenceprocesstheories(DaCosta,Parr,etal.,2020;
Friston, FitzGerald, et al., 2017a; Friston, Rigoli, et al., 2015a; Friston et al., 2012)
whereinsteadofsimplydescribingagivenstochasticdifferentialequation,weinstead
considertheNESSdensitytobethepreferencesordesiresoftheagentoftenrepresented
as a Boltzmann distribution over environmental rewards p∗(η,π)=exp(−r(η)) and
the active states (the agent’s actions) being computed through a minimization of the
EFE,withthisminimizationeithertakingplacedirectlyasagradientdescentincontin-
uoustimeandspace(Fristonetal.,2009)orelseasanexplicitmodel-basedplanning
algorithmasinthediscrete-timeanddiscrete-spaceformulation(Friston,FitzGerald,
etal.,2017b;Millidge,2020;Millidge,Tschantz,Seth,&Buckley,2020b;Tschantz,
Millidge,etal.,2020b).
2.8 Philosophical Status of the FEP
Itisworthsteppingbackfromthemathematicalmorass,atthispoint,totrytodefineat
a high level what kind of theory, philosophically speaking the FEP is, and what kind
ofclaimsitmakes. Therehavebeennumerousdebatesintheliteratureaboutwhether
Chapter2. TheFreeEnergyPrinciple 51
the FEP is ‘falsifiable’, or whether it is ‘correct’, and whether or not it makes any
specific, empiricalclaims (Andrews,2020; D. Williams, 2020). However oftendebates
on this matter are obscured or confused by the challenging and deep mathematical
background required for a full understanding of the specifics of the FEP. It is clear
from the mathematics that the FEP offers only an ‘interpretation’ of already extant
dynamics. Inshort,FEPpresupposestheexistenceofthekindsofdynamicsitwishesto
makesenseof–dynamicalsystemswhichorganizethemselvesintoanon-equilibrium
steady state, and which maintain the requisite statistical independency structure of
the Markov Blanket condition. Once these conditions are satisfied, the FEP gives an
interpretation of the dynamical evolution of such a system as performing a kind of
variationalBayesianinferencewherebytheinternalstatesofthesystem(definedbythe
MarkovBlanketpartition)canbeseenasinferringorrepresentingexternalstateswhich
areotherwisestatisticallyisolated behindtheMarkovBlanket. Crucially, theFEP,inits
mostgeneral formulationdoesnot makeany specific predictionsabout theflowof the
system. Itoffersaninterpretationonly. WhilesystemsthatimplementtheFEPcanbe
derived,andseveralprocesstheorieshavebeenexplicitlyderivedfromwithintheFEP
framework (Friston,2005;Friston, Rigoli,et al.,2015a), allsuchtheories necessitate
makingspecific andultimately arbitrarymodelling choices, suchas ofthe generative
modelandvariationaldensity. Suchchoicessitbelowthelevelofabstractionthatthe
mathematicaltheoryoftheFEPexistsat. TheFEPoffersamathematicalinterpretation
onlyofcertaindynamicalstructures.
The FEP is often compared and analogised to the principle of least action in physics
(Lanczos,2012)whichallowsone todescribemanyphysicalprocesses(althoughnot
all)asminimizingthepathintegralofafunctionalcalledthe‘action’overatrajectory
of motion (Sussman & Wisdom, 2015). This argument is often used to claim, in
myopinion correctly,that theFEPis amathematical‘principle’ orinterpretationand
thereforecannotbefalsifiedorempiricallytested. Inmyopinion,however,theprinciple
ofleastactionis,initsphilosophicalstatus,notdirectlyanalogoustotheFEP.While
Chapter2. TheFreeEnergyPrinciple 52
the relationship between the path integral of the action and the dynamics prescribed
bytheEuler-Lagrangeequationsissimplyamathematicaltruth,theprincipleofleast
action itself, as applied to physics contains a fundamentally empirical and falsifiable
claim–thatphysicalsystemsintherealworldcanbewelldescribedthroughitsown
mathematicalapparatus–thatisofdynamicsderivedfromminimizinganaction. This
claim is in principle falsifiable. Not all dynamical systems can be derived from least
actionprinciples. Ifphysicalsystemspredominantlycamefromtheclassthatcannotbe
soderived,theprincipleofleastactioninphysicswouldbeeffectivelyfalsified,andthe
mathematicalapparatusunderlyingitwouldhavebecomenothingmorethananarcane
mathematical curiosity. So far as we know, there is no a-priori reason why much of
physicscanbesowellunderstoodthroughactionprinciples,andindeedthereareareas
of physics – such as statistical mechanics and thermodynamics, and dissipative non-
conservativesystemsingeneral–whichcannot (sofar)bedescribedstraightforwardly
intheseterms.
It appears a closer physics analogy to the FEP might be one direction of Noether’s
theorem. Noether’s theorem proves a direct correspondences between symmetries
or invariances in a given system, and conservation laws. For instance, in physical
systems,time-translationsymmetry implies theconservationofenergy,androtational
symmetry(oftheunderlyingeuclideanspace,notanygivenobjectwithinit)impliesthe
conservationofangularmomentum. TheFEP,similarly,canbethoughttoshowacorre-
spondence between the dynamicsof a certain kind of system (NESS density, Markov
Blanket conditions) and thedynamics ofvariational Bayesianinference. Interestingly,
whilethe‘forward’directionfromtheNESSdensityandMarkovBlanketconditions
is treated in the FEP, any reverse conditions – i.e. whether the presence of Bayesian
inferencedynamicsimpliesanykindofstatisticalstructureuponthedynamicsofthe
systemremainsunclear,andthisislikelyafruitfuldirectionforfurthertheoreticalwork.
Noether’stheorem, unliketheprincipleofleast action, matchesmorecloselythanthe
principleofleastactionsinceitonlyspecifiescorrespondencesbetweencertainkindsof
Chapter2. TheFreeEnergyPrinciple 53
mathematicalobjects(symmetriesandconservationlaws)justastheFEPonlyspecifies
acorrespondencebetweendynamicalflowsatNESSofasystemwithaMarkovBlanket,
andthegradientflowsonthevariationalfreeenergy.
WhileitsstatusasamathematicalprincipleandinterpretationonlycanshieldtheFEP
fromthe possibility ofanempirical‘falsification’, thisdoesnot meanthatthetheory is
not subject to some kind of implicit intellectual review. Much of the core motivation
behindtheFEPhasbeentotrytoderiveuniversalpropertiesofthekindofbiological
self-organizing systems which give rise to structured behaviour including relatively
‘highlevel’processessuchastheperception-action-loop,explicitperceptionandinfer-
enceaboutthecausesoftheexternalworldand,ultimately,prospectiveinferenceand
planning. For instance, much of the FEP literature has been focused on and applied
to understanding brain function (Friston, 2008a; Friston, FitzGerald, et al., 2017b;
Friston,Rigoli,etal.,2015a). ThisambitionrenderstheFEPopentoquestionsaboutits
‘applicability’, if not its falsifiability. The FEP imposes relatively stringent conditions
thatdynamicalsystemsmustsatisfyforthelogicalstepsintheFEPtohold. Inthenext
section,wepresentadetaileditemizedlistandcriticaldiscussionofalltheassumptions
required. Someespeciallykeyassumptions,whichsubstantiallyrestrictthepotential
classofsystemstheFEPcanapplytoare:
• ThatthesysteminquestioncanbeadequatelyrepresentedasaLangevinequation
(i.e. thesystemisMarkovanddoesnotdependonhistory)withadditivewhite
Gaussiannoise.
• thatthedynamicalsystemasawholehaveawell-definedNESSdensity(including
overtheexternalstates).
• that the system obey the Markov Blanket conditions, which are, in general,
relativelyrestrictiveaboutthekindsofflowsthatarepossible,andappeartohave
becomemore restrictivein Friston, DaCosta, andParr (2020),whichprecludes
any solenoidal coupling between active and sensory states (indeed the didactic
Chapter2. TheFreeEnergyPrinciple 54
treatmentsofthefreeenergylemmatypicallyrequireablock-diagonalQmatrix,
meaning no solenoidal coupling between subsets of states). If this assumption
isrelaxed,thenthereareadditionalsolenoidalcouplingtermsintheflowofthe
internalstates,soatbestonecansaythatgradientascentuponthesurprisalisa
componentoftheflow.
• Thattherebeaninjectivemappingbetweenthemost-likelyinternalstategiventhe
blanketandthemodeofthedistributionofexternalstatesgiventheblanketstates,
whichisadditionallysmoothanddifferentiable(thisisrequiredforthedual-aspect
informationgeometry,andthustheidentificationwithBayesianinference).
These conditions are quite strict about the class of systems that the FEP can apply
to, and it is unclear if ‘real systems’ of the kind that FEP desires to explain – such
as biological self-organization, and especially brains, can fulfil them. If it turns out
that such systems flagrantly violate the conditions for the FEP, then the FEP cannot
besaidtoapplytothemandthuscannotbeofuseinunderstandingthem,evenasan
interpretatory device. In this case, the FEP would fail the applicability criterion, and
would cease to be particularly useful for its original goals of neuroscience, even if it
remainsnottechnicallyfalsifiedanddoes,infact,applytosomeobscuremathematical
class of dynamical systems. Importantly, many of the assumptions of the FEP, when
interpreted strictly, do not appear to hold in general for complex biological systems
suchasbrains. Forinstance,totakeextremebutillustrativeexamples,itisclearthatno
biologicalsystemiseverinatruenon-equilibriumsteadystate,sinceeventuallyallsuch
organismswillageanddie,andindeedeventuallytheentireuniversewilllikelydecay
toathermodynamicequilibriumstate. Additionally,theMarkovBlanketassumption
is directly violated by things such as x-rays (and indeed gravity) which can directly
interactwith‘internalstates’ofthebrain,suchasneurons,withoutfirstpassingthrough
theMarkovBlanketofthephysicalboundariesofthebrainandthesensoryepithelium.
As such, for a real physical system, we must take the assumptions of the FEP to be
Chapter2. TheFreeEnergyPrinciple 55
onlyapproximations,whichholdlocally,orapproximately,butnotforalltimeandwith
complete perfection. It remains to be seen, and empirically investigated if possible,
theextenttowhichthemathematicalinterpretationsandlogicalstatementsoftheFEP
remainrobusttosuchslightrelaxationsofitscoreassumptions.
WhiletheFEPprovidesamathematicalinterpretationofcertainkindsofdynamicsin
terms of inference, it also, largely, remains to be seen whether such an interpretation
is useful for spurring new ideas, questions, and developments within the fields the
FEP hopes to influence – such as neuroscience, cognitive science, and dynamical
systemstheory. ReturningtoouranaologiesoftheleastactionprincipleandNoether’s
theorem,whilebothofthesemathematicalresultsonlyprovideinterpretationsofknown
dynamics, byoperating ata highlevel ofabstraction theyprovidepowerful capabilities
forgeneralization. Forinstancetheprincipleofleastactionallowsfordynamicstobe
derived, via the Euler-Lagrange equations, directly from the high level specification
of the action. For instance, the potentially new or counterfactual laws of physics
can be derived simply by postulating a given Lagrangian or Hamiltonian and then
workingthroughthemathematicalmachineryofthe principle ofleastactiontoderive
the ensuing dynamics. Additionally, by investigating invariances in the action, one
canoftenunderstand thekindsofinvariancesanddegrees offreedomthatexist inthe
actuallyrealizeddynamics. Similarly,Noether’stheoremallowsonetoplaywithsetting
upcertainconservedquantitiesorsymmetriesa-priori,andthenworkoutpreciselythe
consequencesthattheseentailforthedynamics.
It is currently unclear to what extent the FEP offers such powerful advantages of
abstractionandgeneralization. ThisislargelyduetotheFEPbeingimmatureasafield
comparedtothecornerstonesofclassicalphysics,andthemajorityoftheresearcheffort
sofarhasgoneintomakingthetheorypreciseratherthanderivingconsequencesand
generalizationsfromit,buttherearesomepromisinginitialsignswhichhavejustbegun
to emerge in the literature of the power the FEP perspective offers. From a practical
Chapter2. TheFreeEnergyPrinciple 56
perspective, the FEP appears to offer a number of novel techniques. Firstly, given a
desiredNESSdensity,thefreeenergylemmaprovidesastraightforwardwayofderiving
dynamicswhichwillnecessarilyreachthatdensity,duetothefactthatthevariational
freeenergybecomesaLyapunovfunctionofthesystemasawhole. Thisapproachhas
strongpotentiallinkstoMarkov-Chain-Monte-Carlomethodsinmachinelearningand
statistics, which aim to approximate an intractable posterior distribution by the time
evolutionofaMarkovprocess(M.Betancourt,2017;Brooks,Gelman,Jones,&Meng,
2011;Chen,Fox,&Guestrin,2014;Metropolisetal.,1953;Nealetal.,2011). TheFEP
providesanewperspectiveonsuchsystemsasfundamentallyperformingvariational
Bayesian inference, and may in future be used to develop improved algorithms in
this domain, akin to the developments of Hamiltonian (M. J. Betancourt, 2013) and
Riemannian MCMC (Girolami & Calderhead, 2011) methods. For instance, there is
muchpotentialintheideaofsolenoidalflowspeedingupconvergencetothedesired
equilibrium density (Ma, Chen, & Fox, 2015). Conversely, the FEP, through the
Helmholtz decomposition, may additionally provide tools for inferring the eventual
NESSdensitygivenaspecificsetofdynamics(Friston,2019a;Maetal.,2015). This
wouldallow,again,forananalyticalorempiricalcharacterisationoftheultimatefate
of a system, and allow for characterising different kinds of systems purely by their
dynamicsfarfromequilibrium.
Asecondstrandofpotentiallydirectlyusefulresearchwhichhasbeguntoarisefrom
theFEPisempiricalandstatisticalmethodologiesfordefining,computing,andapproxi-
matingMarkovBlankets. Thisimpliestheabilitytoinferthestatisticalindependency
structureofthedynamicspurelyeitherfromanalyticalknowledgeofthedynamicsor,
alternatively,frompurelyobservedtrajectories. Therearealreadytwoapproachesto
achievethisintheliterature. Onewhichutilizesgraphtheoryintheformofthegraph
LaplaciantoinfernodesoftheMarkovblanketbasedontheparents,andchildrenof
parentsofthelargesteigenstatesoftheJacobian(Friston,2013;Friston,Fagerholm,et
al.,2020;Palacios,Razi,Parr,Kirchhoff,&Friston,2017). Asecondapproachdirectly
Chapter2. TheFreeEnergyPrinciple 57
usestheHessianofthedynamicstoattempttoreadofftheconditionalindependency
requirementsitimplies(Friston, Fagerholm,etal.,2020). Theseapproachesmay have
substantial merit and utility in understanding the effective statistical independency
structure of complex dynamical processes, especially questions regarding functional
independence in the brain. This strand of research heavily relates to the question of
abstractionindynamicalsystems–namely,whethercomplexsystemscanorcannotbe
straightforwardlypartitionedintoindependentsubsystemswhichcanthenbeabstracted
over. Forinstance,theidealwouldbetheabilityto,givenacomplexhigh-dimensional
dynamical system, parse this system into individual ‘entities’ (separated by Markov
blankets)whichinteractwitheachotheraccordingtoanothersetof(hopefullysimpler)
dynamical rules. This would allow for an automatic procedure to transform a high
dimensionalcomplexsystemintoasimpler,low-dimensionalapproximatesystemmore
amenableforanalysisand,ultimatelyunderstanding(Friston,2013;Fristonetal.,2007;
Parr,Sajid,&Friston,2020).
2.9 Discussion of Assumptions required for the FEP
Here we provide a general overview and short discussion of every assumption required
at each stage of the FEP. Ultimately, the overall picture that emerges is that the FEP
requiresmanyassumptionstowork,anditisunlikelythatallofthemcanbefulfilled
by the kinds of complex self-organizing systems that the FEP ultimately ‘wants’ to
be about – such as biological self organization and, ultimately, brains. However, this
is not necessarily overly problematic for the FEP as many of its assumptions may be
approximately, orlocallytrue oversmallenough timeperiods. Thisisnotnecessarily
a bad thing – almost all of the sciences ultimately use simplified models to try to
understandtheirultimateobjectsofstudyinamoretractableway. TheFEPissimply
continuingthattradition,butifwedothis,weneedtomakeexplicitthekeydistinction
betweenthemodelandtherealityor,morememorably,themapandtheterritory.
Chapter2. TheFreeEnergyPrinciple 58
The first set of key assumptions that the FEP makes comes through the definition of
thekindsofstochasticdynamical systemsthatitworkswith. Specifically,wemakethe
followingassumptionsabouttheformofthedynamicswedealwith,
• The system as a whole can be modelled as a Langevin SDE of the form dx =
dt
f(x)+ω
• ThenoiseωisGaussianwith0meanandacovariancematrix2Γ.
• Thenoiseisadditivetothedynamics
• Γdoesnotchangewithtime
• Γhasnostatedependence(noheteroscedasticnoise)
• Γisadiagonalmatrix(eachstatedimensionhasindependentnoise)
• Thedynamics f(x)donotthemselveschangewithtime.
Wealsomustmakethefollowingassumptionsaboutthesystemasawhole,
• The system is ergodic, which means that state and time averages coincide or,
alternatively, that there must be some probability of ultimately reaching every
partofthesystemfromeveryotherpart.
• Thesystempossessesawellcharacterizednonequilibrium-steady-statedensity
(NESS),whichdoesnotchangeovertime
• Once the system reaches this NESS density it cannot escape it – there is no
metastabilityormultiplecompetingattractors.
These assumptions setup the basic formalism we wish to consider. From here, we
then apply the Ao decomposition to rewrite the dynamics in the form of a gradient
descentonthelogofthepotentialfunctionwithdissipativeandsolenoidalcomponents
f(x)=(Q−Γ)∇ lnp∗(x). Tobeabletoimplementthisdecompositionrequires,
x
• Thedynamicsfunction f besmoothanddifferentiable
Chapter2. TheFreeEnergyPrinciple 59
Now,weapplytheMarkovBlanketconditionsattheNESSdensity,
• Thestatespacexcanbepartitionedintoasetoffourstates–internali,externale,
activeaandsensoryswhich,attheNESSdensityfulfillthefollowingconditional
independencerelationships: p∗(x)= p∗(η|s,a)p∗(µ|s,a)p∗(s,a).
• WethusrequireallpartitionstobeatNESS,includingtheexternalstates. This
meansthattheenvironmentalsohastobeatsteadystate,notjustthesystem.
• We often assume no solenoidal coupling between internal and sensory states
(internal statesdo notdirectly act onsensory states– only theexternal statesdo),
nor between active and external states (active states drive the external state but
arenotdrivenbyit). MathematicallythiscorrespondstoQ =0,Q =0.
s,µ, η,a
• WemayevenrequirethatQbeblockdiagonal–thusallowingfornosolenoidal
couplingbetweensubsetsoftheMarkovBlanketatall.
GiventheMarkovBlanketconditionshold,wecanthenbegintomovetowardsthefree
energylemma. Tobeginwith,wemustfirstassume,
• Thereisauniqueargmaxηηη,µµµexistsforbothinternalandexternalstatesforevery
blanketstateb.
• Thatthereexistsafunctionσwhichmapsfromµµµtoηηη
• Thatσisinvertible
• Thatσisdifferentiable
• Fortheparticularfreeenergy,weassumethatthevariationalposteriorq(η;µµµ)is
equaltothetrueposterior,andthusthatthetrueposteriorcanberepresentedbya
vectorofsufficientstatistics(µµµ).
These assumptions on σ are quite restrictive. A more detailed discussion of what
theseassumptionsrequire canbefoundin thenextsectionof thischapter,whereevery
restrictionislistedanddiscussedinsomedepth.
Chapter2. TheFreeEnergyPrinciple 60
Finally,toreachthefreeenergylemma,wemustmakethefollowingassumptions,
• The flow of the sufficient statistic of external states ηηη follows the same (Ao-
decomposition)dynamicsastheexternalstatesthemselves
• The variational distribution q(η;µµµ) is a Laplace distribution (Gaussian) with a
fixedcovarianceΣasafunctionofµµµ.
Thisfirst assumptionhascome underheavy controversy andisdiscussed in moredetail
later. These additionalassumptionspertain totheLaplace approximation, butthe final
assumptionhereappearstogobeyondwhatistypicallyrequiredbyvariationalLaplace
where,sincetheconditionaldistributionisafunctionoftheblanket,onewouldexpect
theconditionalcovariancetobeonetoo.
2.9.1 Assumptions on the Form of the Langevin Dynamics
The FEP formulation makes reasonably strong assumptions about the nature of the
dynamicsthatitmodels–restrictingthemtotheformofstochasticdynamicswhichcan
bewritten asa Langevin equationwithadditiveGaussiannoise. While theassumptions
onthedynamicsfunctionarenotthatstrong,onlyrequiringdifferentiabilityandtime-
independence,therestrictionsonthenoiseinthesystemarequitesevere.
Firstly,itisimportanttonotethatusingadditivewhitenoise,whileacommonmodelling
assumptionduetoitsmathematicalsimplicity,neverthelessimposessomerestrictions
on the kind of systems that can be modelled – especially as complex self organizing
systemstypicallyevincesomekindofcoloredsmoothnoise,aswellasoftenpower-law
noise distributionswhich are associatedwith self-organized criticality(Ovchinnikov,
2016).
However, thefurtherassumptionsontheΓcovariancematrix –thatitisdiagonal,state-
independent,andtime-independent–arealsostrongadditionalrestrictions. Specifically,
thismeansthatthenoisetoeverydimensioninthesystemiscompletelyindependentof
Chapter2. TheFreeEnergyPrinciple 61
anyotherdimension,andthatthenoiseisconstantateverypointthroughoutthestate
spaceandthroughouttime.
2.9.1.1 ErgodicityandtheAoDecomposition
The Ao decomposition requires both that the dynamics possess a consistent non-
equilibrium steady state density (which forms the potential function) and also that
the dynamicsare ergodic. Additionally, this ergodicityassumption is implicitlyused
intheBayesianmechanics,whichallowsexpectationsofthesurprisaltobetakenand
interpretedasentropies,andthustoultimatelyderiveaninterpretationofthedynamics
intermsofaccuracyandcomplexity. Ingeneral,formanybiologicalandself-organizing
systems,ergodicitydoesnotholdandsuchsystemstypicallyexhibitsubstantialamounts
of path dependence and irreversibility. This means that on a strict reading, for most
systemstheFEPdesirestomodel,theergodicityassumptiondoesnothold. However,
itmaystillbepossibletodescribeergodicityasholdinglocallyinthesmallregionof
thestatespacearoundtheNESSdensityandthismaybesufficientforanapproximate
versionoftheFEPtohold,althoughtheresistanceoftheFEPtoslightperturbationsof
itsassumptionsremainsunclear.
2.9.2 The Markov Blanket Condition
2.9.2.1 Is Information Retained Behind the Blanket? – The Time Synchronous
MarkovBlanketCondition
Apotentiallysubstantialproblem,whichhasbeenraisedbyMartinBiehlandNathaniel
Virgo,fortheFEPisthattheMarkovBlanketconditionswouldappeartoverystrongly
implythattheinternalstatescannotstoreanymoreinformationabouttheexternalstates
thantheblanketstates. Thisfactcanbederivedfromastraightforwardapplicationofthe
dataprocessinginequality. Translatedintotheterminologyofbiologicalsystemslike
brains,thiswouldmeanthatthestateofthebraincouldcontainnomoreinformation
Chapter2. TheFreeEnergyPrinciple 62
abouttheenvironmentthanthestateofthesensoryepitheliaandactuatorsatthecurrent
time. Ineffect,thiswouldruleoutsystemsobeyingtheFEPfromexhibitinganysortof
longtermmemoryorlearning–clearlyaveryundesirableside-effect.
In discussions within the community, there have been many attempts to finesse this
apparent difficulty with appeals to notion of nested temporal scales and the local
applicabilityoftheFEP.TheintuitiveargumentisthatiftheMarkovBlanketconditions
rule out information storage on the macroscale where they apply locally, they may
nevertheless allow for the slow accumulation of information over a longer timescale.
Effectively, if we can imagine that there are two kinds of variables – ‘fast’ variables
whichcanchangeovertheagiventimescaleand‘slow’variableswhichdonot. Then,
ifwecanconsidertheslowvariablesfixedoversometimescale,thenwecanconsider
the fast variables to reach a NESS density over that timescale, however over longer
timescales,the valuesof thefastvariablescaninfluence theslow variablesleading to
themchangingovertime,andthusinducingadifferentNESSdensityoveramongthe
fastvariables. Thechangeintheslowvariablescanbeconsideredtobelearning,and
couldallow fortheaccumulationof information overtime. Thisprocessoftimescale
separation is directly analogous to the classical distinction between inference (fast)
andlearning(slow)inmachinelearningand controltheory,andcanalsobeexpressed
physicallyintermsofanadiabaticreduction(Friston,2019a)whichexplicitlyseparates
out the dynamics of the system into fast and slow eigenmodes. This construction,
however, does require a notion of ‘approximate’ NESS for a timescale which is long
enoughforthe‘fast’variablesbutalsoshortenoughforthe‘slow’variablestoappear
fixed.
2.9.2.2 TheRealConstraintsonSolenoidalCoupling?
While the Markov blanket conditions only explicitly disallow solenoidal coupling
directly between the internal and external states – Q = 0, the free energy lemma
µ,η
appearstorequireasignificantlygreaterreductionofsolenoidalcoupling. Specifically,
Chapter2. TheFreeEnergyPrinciple 63
the free energy lemma requires that, for a straightforward identification of the surprisal
with the free energy, that the form of the dynamics foreach marginal subset of statesin
thepartitiontakethesameformasthedynamicsofthefullsetofstatesx. Specifically,
thismeansthatallsolenoidalcouplingbetweenthesubsetsmustbesuppressed,sinceif
theywerenotthen,bythemarginalflowlemma,therewouldbeadditionalsolenoidal
coupling terms in Equation 2.29, which would complicate the relation to free energy
minimizationwithadditionalsolenoidalterms. Assuch, forthefreeenergylemma, as
currentlypresented,weappeartohavetheextremelystrongconditionofthediagonality
ofQ,whereeachsubsetintheMarkovBlanketisonlyallowedsolenoidalinteractions
withitself.
Itisimportanttonotethatthisrestrictionissignificantlystrongerthanthoserequiredjust
bytheMarkovBlanketcondition,andindeedisstrongereventhantheflowconstraints
proposed in Friston, Da Costa, and Parr (2020). While this does not entirely rule out
any interactions between different subsets of the Markov blanket, it does mean that
allinteractionshavetobemediatedthroughthegradientterm,sinceboththeΓandQ
matricesareassumedtobediagonal. However,itmaybethattheadditionalsolenoidal
termsinthefreeenergylemmaasaresultofnon-diagonalQarenotthatdeleteriousto
the theory since as these are purely solenoidal terms, they are orthogonal to the flow
anddonotaffecttheultimateminimaofthesystem.
2.9.3 Assumptions of the free energy Lemma
2.9.3.1 Theσfunction
The existenceand general properties ofthe σ function have also recently elicitedmuch
discussionanddebatewithinthecommunity. Specifically,itisnotatallclearthatthis
functionexistsinthegeneralcase,forarbitrarydynamicsfunctions f andconditional
NESS distributions p∗(η|b) and p∗(µ|b). In later papers it is assumed to exist under
the condition of injectivity between ηηη and µµµ. In effect, this means that there must be
Chapter2. TheFreeEnergyPrinciple 64
a unique mapping between ηηη and µµµ for all blanket states – i.e. that for every blanket
state, if the argmax of the internal states is µµµ, then the argmax of the external states
mustbeηηη. Additionally,theremustbeacorresponding(andseparate)externalargmax
for every internal argmax. There may, however, be some external argmaxes with no
correspondinginternalargmaxes(althoughtheconverseconditiondoesnothold). This
requires that the dimensionality of the external states be greater than or equal to the
dimensionalityoftheinternalstates–whichshouldgenerallyholdformostreasonable
systems where we can safely assume that the environment is larger than the system
itself. Thisinjectivityconditionalso guaranteesinvertibilityinthecasethat the internal
and external state spaces are of the same dimension. It is also possible to use the
Moore-Penrosepseudoinverseforthecasewheretheexternalstatespaceislarger,at
thecostofthefreeenergylemmabecomingapproximateinsteadofexact.
The differentiability of theσfunction isa morestringent condition. Inmany casesthis
isunlikelytobemet,sincetheargmaxfunctionswhichtheσfunctionmapsbetweenare
generallynondifferentiable. Itremainsuncleartowhatextentdifferentiableσfunctions
canexistinsystemsofinterest.
2.9.3.2 TheflowoftheSufficientStatisticsη
An additional important assumption necessary for the free energy lemma, is that the
flowofthesufficientstatisticsoftheexternalmodefollowthesameflowastheexternal
statesgenerally. Thisassumptionturnsouttobecrucialtothefreeenergylemmawhich
relies heavily in the fact that the flow of the sufficient statistic ηηη can be written as a
gradient descenton thelog surprisal –which can thenbe expressed interms ofa free
energyundertheLaplaceapproximation.
Thisassumptionisalsoproblematicandhasbeenthesourceofmuchdiscussionwithin
the community. The extent to which this assumption is justified remains unclear.
Specifically,itappearstoruleouttheuseofarbitraryfunctionsξ(tobediscussedinthe
Chapter2. TheFreeEnergyPrinciple 65
next section) to parametrize the external sufficient statistic (although not the internal
sufficient statistic). The assumption effectively holds to the extent to which one can
describe the sufficient statistic as equal to some external state ηηη(b)≈η, which may
occur often for the argmax but not necessarily always. It remains to be seen whether
the argmax is in fact the optimal such function – which is dependent on the blanket,
but which can identify a consistent η to identify with and thus partake in the same
dynamics.
2.9.3.3 PotentialandOptimalξFunctions
WhilethedidactictreatmentoftheFEPin(Friston,2019a;Parr,DaCosta,&Friston,
2020),itisassumedthattheσfunctionrelatestheargmaxofηandofµ,thisisasimple
assumption and is not particularly required by the theory. It only requires that there
be some function not that it necessarily be an argmax. This means that we could, in
theoryuseanarbitraryfunctionµµµ(b)=ξ(b)insteadoftheargmax. Indeed,wemight
desire to make this function contain as much information as possible about the true
conditional distribution of the internal states given the external states, so that when
theσfunctionmapsthistothesufficientstatisticoftheexternaldensityitcanbeseen
asperforminginferencewiththemostinformationpossiblebetweentheexternaland
internalstates. Anadditionalbenefitofdefininganarbitraryfunctionforξinsteadof
usingξ(b)=argmaxp(µ|b)isthatwecanmakeξdifferentiable,whichalleviatesmuch
ofthedifficultyofmakingσdifferentiableaswell.
While this approach brings many benefits, it also has the drawback of the necessity
to choose a suitable function ξ which introduces another degree of freedom into the
modelling process. One possible condition is that we could chose the optimal ξ to
be theone that contains the mostinformation about theinternal stateor, alternatively
minimizestheKLbetweentheapproximateconditionaldistributionovertheinternal
states parametrized vy ξ and the true conditional over the blanket states. That is, we
Chapter2. TheFreeEnergyPrinciple 66
coulddefine,
ξ∗ =argminD [q(µ;ξ(b))||p(µ|b)]
KL
ξ
Thiswouldreducethenumberofdegreesoffreedomofξandprovideavalidmodelling
target,althoughtheactualcomputabilityofthisminimizationprocessispotentiallya
problem, as is whether this objective is actually optimal. Nevertheless, the use of an
arbitraryξfunctionforthesufficientstatisticsoftheinternalstatesmayyetresolveor
ameliorate some of the difficulties with the free energy lemma, and is an interesting
inroadtobeginunderstandingvariousrelaxationsorextensionstothecurrentincarnation
ofthefreeenergyprinciple.
2.10 Active Inference
In the previous section, we have covered the very general and abstract form of the
FEP, here we elucidate the central process theory that has emerged from the FEP
literature in theoretical neuroscience – Active Inference (Friston et al., 2009, 2012).
Active inference is a normative theory of perception, decision-making, and learning
which ties these three core cognitive processes together under the general paradigm
of variational inference via the minimization of the variational free energy (Friston,
FitzGerald,etal.,2017b;Friston,Rigoli,etal.,2015a). Specifically,itviewsallofthese
processes as emerging out of a central imperative of the system to minimize its free
energy over time, and thusperform inference. Perceptioncan be quiteclearly stated as
aninferenceproblemofinferringthehiddenstatesandcausesoftheworldfromsensory
observations. Learningtoocanbeinterpretedasinferenceovertheparametersofthe
generativemodel,whichtakesplaceonaslowertimescalethanperceptualinference.
Finally,actionselection,decision-making,and planningcanbedescribedasinference
on policies over trajectories into the future. While there are numerous methods to
performthisinference,activeinferencechoosestominimizeanexpectedfreeenergy
Chapter2. TheFreeEnergyPrinciple 67
functionalwhich encodesgoalsin termsofa prioroverfuturestates (DaCosta, Parr,et
al.,2020).
Active inference has been applied widely and productively in theoretical neuroscience,
andactiveinferencemodelshavebeenproposedforplanningandnavigation(R.Kaplan
& Friston, 2018), saccadic eye movements (Parr & Friston, 2017b, 2018a, 2018b)
and visual foraging (Heins et al., 2020; Parr, 2019), and general planning problems
inreinforcementlearningandmachinelearning(Millidge,2020;Tschantz,Millidge,
Seth,&Buckley,2020a;Tschantz,Millidge,etal.,2020b;Ueltzhöffer,2018). Addi-
tionally,bysimulatingvariouslesionsorincorrectupdaterulesintheactiveinference
scheme, one may obtaininteresting behavioural anomalies orshortfalls which one can
thenanalogizetoknownpsychiatricdisorders–anapproachknownascomputational
psychiatry (Cullen et al., 2018; Parr, 2019). By drawing correspondences between
disordersofbehaviourintractableartificialsystemsandthepsychiatricsymptomsof
disorders in humans or other animals, one may be able to shed new light upon the
actualmechanisticunderpinningsofsuchdisorders,whichmayleadtonovelhypothe-
ses, experimental protocols and, ultimately, treatments. Computational psychiatric
approaches using active inference have pioneered statistical, mechanistic models of
impulsivity(Mirzaetal.,2019),visualneglect(Parr&Friston,2018c),autism(Lawson
etal.,2014),schizophrenia(Adamsetal.,2012)substanceusedisorderandaddiction
(Schwartenbecketal.,2015),andrumination(Hespetal.,2020). Finally,theepistemic
imperativesthatarisefromtheminimizationoftheexpectedfreeenergyfunctionalhave
givenrisetoanumberofsimulationstudiesapplyingtheapproachtoexplorationtasks
(Friston, Lin, et al., 2017; Friston, Rigoli, et al., 2015a; Schwartenbeck et al., 2013),
visualforagingandotherinformation-seekingsaccadebehaviour(Heinsetal.,2020;
Parr&Friston,2017a),andexplorationincomplexsparse-rewardenvironmentsfrom
reinforcementlearning(Tschantz,Millidge,etal.,2020b),whichwillbesignificantly
expandeduponinlaterchaptersofthisthesis.
Chapter2. TheFreeEnergyPrinciple 68
Broadly, there are two main classes of active inference models in the literature –
continuous-time, continuous-statemodels, whichare anextension ofpredictive coding
modelsofbrainfunction(Baltieri&Buckley,2017,2019;Fristonetal.,2009;Friston,
Daunizeau,etal.,2010;Millidge,2019a;Pio-Lopez,Nizard,Friston,&Pezzulo,2016),
anddiscrete-timediscrete-state-spaceactiveinferencemodelswhichhavebeenheavily
developedintheliteratureinthepastdecade,andperhapsnowformthemaintheoryof
activeinferenceappliedtothebrain(DaCosta,Parr,etal.,2020;Friston,FitzGerald,et
al.,2017b). Allofthesemodels,however,ultimatelyarederivedfromthesamemathe-
maticalapparatus. Assuch, an advantageof active inferencefor modellingbehaviour
is that due to its developed and shared mathematical apparatus, different models are
specifiedsimplythroughthegenerativemodelandvariationaldistribution,andcanbe
directlycomparedthroughBayesianmodelcomparisontechniques. Thiscanbeusedto
fitactiveinferencemodelstoempiricalbehaviouraldatainastraightforwardfashion.
Continuous time, continuous state based models are explained in depth in Chapter 3,
whereIdiscussmyworkwiththesemodelsinthecontextofpredictivecoding. Here,
wepresentanintroductiontodiscrete-state-spaceactiveinferencewhichshallformthe
basisofmyworkinChapters4and5ofthethesis.
Here we introduce a more standard notation which shall be used for the rest of the
thesis. Weconsiderouragent tobesituatedinaPartiallyObservedMarkovDecision
Process (POMDP) (Kaelbling, Littman, & Moore, 1996; Sutton, 1990). The agent is
given observations o, and must infer the hidden states of the world x that gave rise
to the observations. The agent may additionally possess models with parameters θ,
whichcanalsobeoptimized. Finally,actionselectionconsistsofinferringactionsa,or
policies π=[a ,a ,...a ] which are simply sequences of actions in order to achieve
0 1 N
some desired goal. Active inference is based around the fundamental imperative of
minimizingthevariationalfreeenergy(VFE).WemayrecallfromEquation2.12that
the VFE consists of the KL divergence between a variational distribution q(x|o)and a
generativemodel p(o,x).
Chapter2. TheFreeEnergyPrinciple 69
F(o)=D [q(x|o)||p(o,x)] (2.35)
KL
Ifweadditionallywanttoinfertheparametersθ,wecanextendthegenerativemodel
andvariationaldensitytoincludeadistributionovertheparameters–thisprovidesa
fullyBayesiantreatmentofparametersincontrasttomanymachinelearningschemes
whichtreatthemeffectivelyaspointdistributions,
F(o)=D [q(x,θ|o)||p(o,x,θ)] (2.36)
KL
In order to implement a specific active inference scheme, the key thing to specify is
thenatureofthegenerativemodel,andthenatureofthevariationaldistribution. These
twodistributionssufficetocompletelyspecifythemodel,andwiththesedistributions
set, the processes of learning, inference, and action selection can be handled by the
standardmathematicalapparatusofthetheory. Specifically,givenaspecificvariational
distributionanda generative model,we canimplementperception asa minimization of
the VFE with respect to the variational distribution with respect to the hidden states,
and we can implement learning as the minimization of the VFE with respect to the
parameters
Perception: argminF(o)
q(x|o)
Learning: argminF(o) (2.37)
q(θ|x,o)
There are then two separate ways to implement action. The most straightforward
approach, which is utilized in the continuous time version of active inference is to
similarlyimplementactionasagradientdescentontheVFEwithrespecttoaction,
Action: argminF(o(a)) (2.38)
a
Wherewehavemadetheimplicitdependenceoftheobservations,andhencetheVFE
onactionexplicit,whichmakessuchaminimizationnon-trivial. Asecondapproach,
Chapter2. TheFreeEnergyPrinciple 70
which is typically used in the discrete-state-space paradigm is to assume a specific
functionalformforthevariationalposterioroverpolicies–thatofasoftmaxdistribution
overtheExpectedFreeEnergy(EFE)G(o,x)(Friston,Rigoli,etal.,2015a),
Action(discrete-state-space): Q(π)=σ(−G(o,x)) (2.39)
whereσisasoftmaxfunction. Effectively,whatthisstatesisthattheoptimalpolicyisa
softmax distributions over the path-integrals of the EFE into the future. Effectively, the
optimaldistributionoverpoliciesissimplyonethatselectspoliciesinproportionwith
theexponentiatedEFEresultingfromexecutingthatpolicyinthefuture. Thismeans
thatthepolicywiththegreatestEFEismostlikely,whilepolicieswithlesserEFEare
exponentially lesslikely tobe selected basedon thedifference between their EFE and
thatofthebestpolicy.
Discretestate-spaceactiveinferencethereforeoptimizestwocomplementaryobjective
functions. Optimizing the variational free energy, which is used for perception and
learning,ensuresthattheagentlearnsanaccurateworldmodel,andisabletoaccurately
infer the hidden states of the world from current observations. The second objective,
theexpectedfreeenergy,isusedtoscorepotentialplansoractionpolicies,toallowthe
agenttomakedecisionswhichareadaptiverelativetoitsgoals. Tosuccessfullypredict
and infer with trajectories in the future requires a highly developed and accurate world-
model,abletomakeaccuratemulti-steppredictionsoftheconsequencesofaction. Such
aworldmodelisprovidedbytheminimizationoftheVFEintheinferenceandlearning
steps. This separation of inference and action selection into two separate objectives
– the VFE and the EFE – introduces a measure of complexity into the theory which
may or may not be unavoidable. In Chapter 5, we focus especially on this question
andinvestigatethenatureoftheEFE,andwhetherallfacetsofinference,learning,and
actionselectioncanbesubsumedunderasingleunifiedobjective.
Chapter2. TheFreeEnergyPrinciple 71
2.10.1 Discrete State-space models and Perception
Thecorecomponentofthediscrete-state-spacemodelisthediscretegenerativemodels
and variational densities it is based upon. Specifically, we split the generative model
intoalikelihoodandpriordistribution p(o,x)= p(o|x)p(x),andthenrepresent eachof
thesedistributionsasacategoricaldistribution,
p(o,x)= p(o|x)p(x)
p(o|x)=Cat(o;oˆ)=AAA
p(x)=Cat(x;xˆ)=BBB (2.40)
Acategoricaldistributionisonethatsimplydirectlyassignssomeprobabilityvalueto
everypossiblediscretecontingency. Theparametersoˆ andxˆofthesedistributionsare
simplytheseprobabilityvalues,whichcanberepresentedstraightforwardlyinterms
ofmatrices. AAA∈RO×RX issimplyanormalizedmatrixofprobabilitiesrepresenting
thelikelihoodcontingencies–thatis,foreveryhiddenstatex,whatistheprobability
of each potential outcome. Similarly, the transition matrix BBB∈RX ×RX is a matrix
of probabilities representing the probability oftransitioning fromany onehidden state
to any other hidden state. Similarly, we define our variational distribution q(x|o =
o ;xˆ )∈RX tobeacategoricaldistributionoftheprobabilityofeachdiscretehidden
µ q
stateasafunctionoftheobservedobservationo . Withourvariationalandgenerative
µ
modelset,wecanexplicitlywriteoutandevaluatetheVFE,
F =D [q(x|o)||p(o,x)]
KL
=Eq(x|o;xˆ )[lnq(x|o;xˆ )]−E [lnp(o|x;oˆ)]−E [lnp(x;xˆ)]
q q q(x|o;xˆq) q(x|o;xˆq)
=xˆ lnxˆ −xˆ lnAAA−xˆ lnBBB (2.41)
q q q q
Where we have simply explicitly written out the variational free energy in terms of
the parameters of the categorical distributions. The expectation operator E[] can be
computedasasimpledotproductinsteadofanintegralduetothediscretestatespace.
Importantly,becausebothourvariationaldensityandgenerativemodelsarecategorical
Chapter2. TheFreeEnergyPrinciple 72
distributions,wecanderiveananalyticalexpressionfortheminimumofF withrespect
tothevariationalparametersxˆ ,allowingforanexactBayes-optimalsingle-stepupdate
q
forperception,
∂F ∂
= [xˆ lnxˆ −xˆ lnAAA−xˆ lnBBB]
q q q q
∂xˆ ∂xˆ
q q
=lnxˆ +111−lnAAA−lnBBB
q
∂F
=0 =⇒ xˆ∗ =σ(−lnAAA−lnBBB) (2.42)
∂xˆ q
q
Learningcanbeapproachedsimilarly,byplacingsuitablehyperpriors(typicallydirichlet
(Schwartenbeck et al., 2019)) upon the parameters of the AAA and BBB matrices and then
minimizing the VFE with respect to these parameters. For more information on how
learning is implemented see Da Costa, Parr, et al. (2020); Friston, FitzGerald, et al.
(2017b).
2.10.2 Action Selection and the Expected Free Energy
Action selection is then handled via the variational posterior being equal to the soft-
maxed path integral of the EFE through time (Equation 2.39). Typically, in small
discrete state spaces, this path integral can be computed exactly, by simply comput-
ing the EFE for every single policy and every single possible trajectory through the
state-space. Unfortunately,thisapproachscalesexponentiallyinthetimehorizon,and
is thus not suitable for long, open-ended tasks, although it remains a highly effective
methodforsimulatingshorttasks,suchassingletrialsinapsychophysical,orsimple
decision-making, paradigm (Friston, Da Costa, Hafner, et al., 2020; Friston, Rigoli,
et al., 2015b; Schwartenbeck et al., 2015). Various methods have been proposed to
handlethisexponentialcomplexity. Onecommonlyproposedmethodistosimplyprune
potentialtrajectorieswhichbecometoounlikely(i.e. havetoolowanEFE)tobeworth
considering further. Typically, such methods, however, do not reduce the algorithm
toasmaller(polynomial)complexityclass,butinsteadsimplyreducetheexponential
Chapter2. TheFreeEnergyPrinciple 73
coefficientwhichallowsthemethodtoscaletoslightlylargertasksbutdoesnotremove
the fundamental exponential complexity of the algorithm. Other approaches involve
approximating the path integral with either bootstrapping value-function methods (Mil-
lidge,2020),whichtakeadvantageoftherecursivetemporaldecompositionoftheEFE,
or alternatively Monte-Carlotechniques whichapproximate theEFE through arandom
sampling oftrajectories, whichcorresponds to classical model-predictive control algo-
rithms(Kappen,Gómez,&Opper,2012). AnadditionalconsiderationintheEFEisthe
need to specify a desired or goal state for the action selection mechanism to achieve.
This can be considered to be a probabilistic description of rewards in reinforcement
learningandpsychology, orofutility ineconomics. Mathematically, thisspecification
is achieved by defining a biased generative model p˜(o,x) which contains a desired
distribution p˜ whichencodestherewardsorutilityaseffectivelypriorsintheinference
procedure.
Sinceactionselectionisdependententirelyonthepath-integraloftheEFE,theprop-
ertiesoftheEFEfunctionalessentiallydeterminesthekindofbehaviourthatfinding
trajectoriesthatminimizetheEFEwillinduce. Here,weshowcasetwodecompositions
oftheEFE,anddiscussitsintrinsicexploratorydrive.
G(o,x)=E [lnq(x)−lnp˜(o,x)]
q(o,x)
=E [lnp(o|x)]+D [q(x)||p˜(x)]
q(o,x) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Ambiguity Risk
=E [lnp˜(o)]−E D [q(x|o)||q(x)]+E D [q(x|o)||p(x|o)]
q(o,x) q(o) KL q(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue InformationGain PosteriorDivergence
(2.43)
The first decompositioninto risk and ambiguity obtains when thegoal distributionis
specifiedintermsofthehiddenstatesoftheworld p˜(x). Inthiscase,EFEminimization
can be thought of as directly trying to match the expected states of the world to the
desiredstates,andthusachievingone’sgoals,whilesimultaneouslytryingtominimize
Chapter2. TheFreeEnergyPrinciple 74
theambiguityoftheobservationsonereceives. Theseconddecompositionintoextrinsic
and intrinsic value (information gain) occurs when the goal distribution is specified
in terms of the observations p˜(o). The extrinsic value term can be thought of as the
expectedrewardorexpectedutility,sinceitistheaverageamountofrewardexpected
underthe predictedobservation distribution. Mostinterestingly inthis decomposition,
however, is the information gain term which encourages the agent to maximize the
divergence between the variational prior and the variational posterior. Effectively
thistermencouragestheagenttoseekoutinformationintheenvironmentwhichwill
maximally update its beliefs about the world. In effect, this term encouragesa specific
kind of information-seeking exploration, where agents that minimize the EFE are
effectivelydriventoseekoutandintegrateresolvableuncertaintyabouttheworldinto
theirworldmodel.
ThisintriguingpropertyofactiveinferenceagentswhichminimizetheEFEhasbeen
extensivelyinvestigatedintheliterature–fromsimpletaskssuchastheT-mazewhich
requiredecidingwhethertogaininformationbyseekingoutacueornot(Friston,Rigoli,
etal.,2015a),toplanningvisualsaccadesinawaywhichmaximizestheinformation
aboutthescenegained(Heinsetal.,2020;Parr&Friston,2017b)andtousingdirected
exploration to develop highly sample efficient and powerful general reinforcement
learningalgorithmsfor sparse-rewardenvironments(Millidge, 2019b). Moreover,the
factthattheEFEnaturallygivesrisetoaninformation-seekingexplorationtermoffers
apromisingandfascinatingavenueforresolvingtheexploration-exploitationtradeoff
and deriving optimalexploration strategies directly fromvariational Bayesianinference
algorithms.
2.11 Discussion
In this chapter, we have already covered a substantial amount of ground. We have
reviewedthecoretenetsofthefreeenergyprinciple,providedamathematicallydetailed
Chapter2. TheFreeEnergyPrinciple 75
walk-through of the core results, and discussed their philosophical implications and
meaning. We have additionally provided a short review of a core process theory –
discrete state space active inference – which we will fundamentally build upon in
various ways in the rest of this thesis. Chapter 3 will focus on applying the free
energy principle to perception – and will focus on the implementation and extension
of the process theory of predictive coding. Chapter 4 will focus on merging active
inferenceaspresentedherewithmoderndeepreinforcementlearningmethodstoallow
active inference approaches, which are currently bottlenecked due to their discrete
explicittabularrepresentationsandtheexponentialcomplexityoftheactionselection
algorithm, to be extended to challenging machine learning problems. Chapter 5 will
focusespecially ontheExpected FreeEnergy termandwill seektounraveltheorigin
of its information seeking properties and in the process will reveal deep connections
betweenactiveinferenceandothervariationalBayesianapproachestoactionsuchas
control asinference, aswell as revealing asubstantially richerlandscape ofpotential
variationalfunctionalsforcontrolthanhasbeenpreviouslyrealized.
Chapter 3
Predictive Coding
3.1 Introduction
Inthischapter,weconsidertheapplicationofthefreeenergyprincipletoperception.
Here, we focus entirely on visual perception and the process theory of predictive
coding(Bastosetal.,2012;Buckleyetal.,2017;Friston,2003,2005;Spratling,2017).
Thischapterisorganizedintofourrelativelyindependentsectionswhicheachpresent
a separate piece of work, which extends or contributes to the theory or practice of
predictivecoding. Thegeneralthemeofourworkaimstoscaleuppredictivecodingto
reachthelevelsofperformanceachievedbymachinelearning,aswellastounderstand
the potential biological plausibility of the theory. Both of these are important for
understandingthepotentialpredictivecodinghasasageneraltheoryofcorticalfunction
since,ifitisactuallyimplementedinthebrain,itmustmeetbothbarsofextremelyhigh
scalability (since the brain effortlessly handles perception and inference with extremely
detailedandcomplexinputs,aswellasconstructingextremelypowerfulandgeneral
representations),aswellasthebiologicalplausibilitynecessarytoallowthedynamics
prescribedbypredictivecodingtobeimplementedbyneuralcircuitry.
Webeginwithamathematicalintroductiontopredictivecodinganditsdynamics. This
76
Chapter3. PredictiveCoding 77
isfollowedwith thepresentationof ourwork where weexperimentwith implementing
largerscalepredictivecodingnetworksthanpreviouslyintheliterature,andvalidate
their performance and capabilities on benchmark machine learning datasets – thus
demonstrating that predictive coding as a theory can be scaled up to the standards of
moderndeeplearning. Wealsoexperimentwithdynamicalpredictivecodingnetworks
usinggeneralizedcoordinates(asintroduced(Fristonetal.,2008))aswellascombining
bothhierarchicalanddynamicalpredictivecodingnetworks,althoughtheseimplemen-
tationsareonlytestedonrelativelysmalltoytasksandscalingtheseuptolargerand
muchmorechallengingdynamicaltasks,suchasvideoprediction,remainsanimportant
avenueforfuturework.
Inthesecondsection,wefocusonunderstandinghowpredictivecodingcanbewrittenas
afilteringalgorithm–andthuscanbeappliedproductivelytofundamentallydynamical
insteadof staticstimuli. We demonstratepreciselyhow predictivecoding isrelatedto
Kalmanfiltering–aubiquitousandextremelysuccessfulBayesianfilteringalgorithm
(Kalman&Bucy,1961;Kalmanetal.,1960)–andalsoshowhowpredictivecodingcan
extendthisalgorithmtoallowfortheonlinelearningoftheparametersofthegenerative
model(aswell asinferenceofthestates)– acapabilitywhichisnotusually achieved
withKalmanfilteringalone. Wevalidatetheperformanceofthisalgorithmonsimple
filteringtasks.
Fourthly,weinvestigatethebiologicalplausibilityofpredictivecoding,showhowthe
standardmodelpossessesthreekeyimplausibilities–weighttransport,nonlinearderiva-
tives, and one-to-one errorunit connectivity, andshow how each can beovercomewith
biologicallyplausible additionsto thealgorithm withoutcausing muchof adegradation
intheclassificationperformanceofthealgorithm.
Chapter3. PredictiveCoding 78
3.2 Predictive Coding
Predictive coding is an influential theory in computational and cognitive neuroscience,
whichproposesa potential unifyingtheoryofcorticalfunction(Clark, 2013a;Friston,
2003, 2005, 2010; Rao & Ballard, 1999; Seth, 2014) – namely that the core function
ofthebrainissimplytominimizepredictionerror,wherethepredictionerrorsdenote
mismatches betweenpredicted inputand the inputactually received. Thisminimization
canbeachievedinmultipleways: throughimmediateinference aboutthehiddenstates
of the world, which can explain perception (Beal, 2003), through updating a global
world-modeltomakebetterpredictions,whichcouldexplainlearning(Friston,2003;
Neal&Hinton,1998),andfinallythroughactiontosamplesensorydatafromtheworld
that conforms to the predictions (Friston et al., 2009), which potentially provides an
accountofadaptivebehaviourandcontrol. Predictionerrorminimizationcanalsobe
influencedby modulatingthe precision(orinverse variance)of sensorysignals, which
may shed light on the neural implementation of attention mechanisms (Feldman &
Friston, 2010; Kanai, Komura, Shipp, & Friston, 2015). Predictive coding boasts an
extremelydeveloped andprincipled mathematical framework, which formulatesit as a
variationalinferencealgorithm(Bleietal.,2017;Ghahramanietal.,2000;M.Jordanet
al., 1998), alongside many empirically tested computational models with close links
to machine learning (Beal, 2003; Dayan et al., 1995; Hinton & Zemel, 1994), which
addresshowpredictivecodingcanbeusedtosolvechallengingperceptualinferenceand
learningtaskssimilar tothosefacedby thebrain. Moreover,predictivecodingalso has
been translatedinto neurobiologicallyplausible microcircuit processtheories (Bastos
et al., 2012; Shipp, 2016; Shipp, Adams, & Friston, 2013) which are increasingly
supportedbyneurobiologicalevidence(Walsh,McGovern,Clark,&O’Connell,2020).
Predictivecodingasatheoryisalsosupportedbyalargeamountofempiricalevidence
andoffersa single mechanismthataccountsfordiverseperceptualandneurobiological
phenomenasuchasend-stopping(Rao&Ballard,1999),bistableperception(Hohwy
etal.,2008;Weilnhammer,Stuke,Hesselmann,Sterzer,&Schmack,2017),repetition
Chapter3. PredictiveCoding 79
suppression (Auksztulewicz & Friston, 2016), illusory motions (Lotter, Kreiman, &
Cox, 2016; Watanabe, Kitaoka, Sakamoto, Yasugi, & Tanaka, 2018), and attentional
modulationofneural activity(Feldman&Friston,2010; Kanaietal.,2015). Assuch,
and perhaps uniquely among neuroscientific theories, predictive coding encompasses
allthreelayersofMarr’shierarchybyprovidingawell-characterisedandempirically
supported view of ‘what the brain is doing’ at the computational, algorithmic, and
implementationallevel(Marr,1982).
Thecoreintuitionbehindpredictivecodingisthatthebrainiscomposedofahierarchyof
layers,whicheachmakepredictionsabouttheactivityofthelayersbelow(Clark,2015;
Friston,2008a). Thesedescendingdownwardpredictionsateach levelarecompared
with the activity and inputs of each layer to form prediction errors – which is the
informationineachlayerwhichcouldnotbesuccessfullypredicted. Theseprediction
errorsarethenfedupwardstoserveasinputstohigherlevels,whichcancanthenbe
utilizedtoreduce theirownpredictionerror. The ideaisthat,overtime,thehierarchy
oflayersinstantiatesa rangeofpredictionsatmultiplescales, fromthefinedetailsin
localvariationsofsensorydataatlowlevels,toglobalinvariantpropertiesofthecauses
of sensory data (e.g., objects, scenes) at higher or deeper levels.1. Predictive coding
theoryclaimsthatthegoalofthebrainasawhole,insomesense,istominimizethese
prediction errors, and in the process of doing so performs both perceptual inference
and learning. Both of these processes can be operationalized via the minimization
of prediction error, first through the optimization of neuronal firing rates on a fast
timescale,and thenthe optimizationof synapticweights onaslowtimescale (Friston,
2008a). Predictivecodingproposesthatusingasimpleunsupervisedlossfunction,such
assimplyattemptingtopredictincomingsensorydata,issufficienttodevelopcomplex,
general,andhierarchicallyrichrepresentationsof theworldinthebrain,anargument
1Thispatterniswidelyseeninthebrain(Grill-Spector&Malach,2004;Hubel&Wiesel,1962)and
alsoindeep(convolutional)neuralnetworks(Olah,Mordvintsev,&Schubert,2017),butitisunclear
whetherthispatternalsoholdsfordeeppredictivecodingnetworks,primarilyduetotherelativelyfew
instancesofdeepconvolutionalpredictivecodingnetworksintheliteraturesofar.
Chapter3. PredictiveCoding 80
whichhasfoundrecentsupportintheimpressivesuccessesofmodernmachinelearning
modelstrainedonunsupervisedpredictiveorautoregressiveobjectives(Brownetal.,
2020; J. Kaplan et al., 2020; Radford et al., 2019). Moreover, the fact that, in these
machine learning models, errors are computed at every layer means that each layer
only has to focus on minimizing local errors rather than a global loss. This property
potentiallyenables predictivecoding tolearnin abiologically plausiblewayusing only
local and Hebbian learning rules (Friston, 2003; Millidge, Tschantz, & Buckley, 2020a;
Whittington&Bogacz,2017).
While originating from many varied intellectual currents, including the speculations
of Helmholtz (Helmholtz, 1866), ideas in information theory (Shannon, 1948) and
Barlow’s minimumredundancy principle(Barlow etal., 1961),aswell asideas from
cybernetics(Seth, 2014; Wiener, 2019)and earlywork on machinelearning (Hinton&
Zemel,1994;M.Jordanetal.,1998),modernpredictivecodingcanbebestdescribed
as a variational inference algorithm (Beal, 2003) on the hidden causes of sensory
sensations,underGaussianand Laplaceassumptions. Variationalinferenceisamethod
of approximate Bayesian inference, arising in statistical physics (Feynman, 1998),
whichturnsanintractableinferenceproblemintoapotentiallytractableoptimization
problem. Inbrief,wepostulateavariationaldensityq,underthecontrolofthemodeller,
andtrytominimizethedivergencebetweenthisvariationaldensityandthetrueposterior.
Since this divergence is not tractable either(since it contains the trueposterior), instead
weoptimizeatractableboundonthisdivergenceknownasthevariationalfreeenergy
F whichmeasurestheexpecteddifferencebetweenthelogsofthevariationalposterior
and the generative model. To make this concrete, suppose we have observations (or
data)o;wewishtoinferhidden,orlatent,statesoftheworldx,withagenerativemodel
p(o,x) and a variational density q(x|o;φ) with parameters φ. Then, we can write the
Chapter3. PredictiveCoding 81
variationalfreeenergyF as,
p(o,x)
D [q(x|o;φ)||p(x|o)]=D [q(x|o;φ)|| ]
KL KL
p(o)
=D [q(x|o;φ)||p(o,x)]+E [lnp(o)]
KL q(x|o;φ)
=D [q(x|o;φ)||p(o,x)]+lnp(o)
KL
≤D [q(x|o;φ)||p(o,x)]=F (3.1)
KL
To derive a specific variational inference algorithm – such as predictive coding – we
mustexplicitlyspecifytheforms ofthevariationalposteriorandthegenerativemodel.
Inthe caseof predictivecoding, weassume aGaussianform forthe generativemodel
p(o,x;θ)= p(o|x;θ)p(x;θ)=N(o; f(x;θ ),Σ )N(x;g(µ¯;θ ),Σ ) where we first par-
1 2 2 1
titionthegenerativemodelintolikelihood p(o|x;θ)andprior p(x;θ)terms. Themean
of the likelihood Gaussian distribution is assumed to be some function f of the hidden
states x, which can be parametrized with parameters θ, while the mean of the prior
Gaussian distribution is set to some arbitrary function g of the prior mean µ¯. We
also assume that the variational posterior is a dirac-delta (or point mass) distribution
q(x|o;φ)=δ(x−µ)withacenterφ=µ2.
Given these definitions of the variational posterior and the generative model, we can
write down the concrete form of the variational free energy to be optimized. We first
decomposethevariationalfreeenergyintoan‘Energy’andan‘Entropy’term
F =D [q(x|o;φ)||p(o,x;θ)]
KL
=E [lnq(x|o;φ)]−E [lnp(o,x;θ)] (3.2)
q(x|o;φ) q(x|o;φ)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Entropy Energy
where,sincetheentropyofthedirac-deltadistributionis0(itisapointmassdistribu-
2Inpreviousworks,predictivecodinghastypicallybeenderivedbyassumingaGaussianvariational
posteriorundertheLaplaceapproximation. Thisapproximationeffectivelyallowsyoutoignorethe
varianceoftheGaussianandconcentrateonlyonthemean. Thisprocedureiseffectivelyidenticalto
thedirac-deltadefinitionmadehere,andresultsinthesameupdatescheme. However,thederivation
usingtheLaplaceapproximationismuchmoreinvolvedso,forsimplicity,hereweusetheDiracdelta
definition. SeeAppendixC,orBuckleyetal.(2017)foradetailedwalkthroughoftheLaplacederivation
Chapter3. PredictiveCoding 82
tion),wecanignoretheentropytermandfocussolelyonwritingouttheenergy.
E [lnp(o,x;θ)]=E [ln (cid:0) N(o; f(θ x),Σ )N(x;g(θ µ¯),Σ ) (cid:1) ]
q(x|o;φ) δ(x−µ) 1 1 2 2
(cid:124) (cid:123)(cid:122) (cid:125)
Energy
=lnN(o; f(θ µ),Σ )+lnN(µ;g(θ µ¯),Σ )
1 2 2 1
(o− f(θ µ)2 (µ−g(θ µ¯))2
1 2
= −ln2πΣ + −ln2πΣ
2 1
Σ Σ
2 1
=Σ−1ε2+Σ−1ε2−ln4πΣ Σ (3.3)
2 o 1 x 1 2
where we define the prediction errors ε =o− f(θ µ) and ε =µ−g(θ µ¯). We thus
o 1 x 2
see that the energy term, and thus the variational free energy, is simply the sum of
two squared prediction error terms, weighted by their inverse variances, plus some
additionallogvarianceterms.
Finally, to derive the predictive coding update rules, we must make one additional
assumption–thatthevariationalfreeenergyisoptimizedusingthemethodofgradient
descentsuchthat,
dµ ∂F
=− (3.4)
dt ∂µ
Given this, we can derive dynamics for all variables of interest (µ,θ ,θ ) by taking
1 2
derivativesofthevariationalfreeenergyF. Theupdaterulesareasfollows
dµ ∂F ∂f
=− =Σ−1ε θT −Σ−1ε
dt ∂µ 2 o ∂µ 1 x
dθ ∂F ∂f
1 = =−Σ−1ε µT
dt ∂θ 2 o ∂θ
1 1
dθ ∂F ∂g
2 = =−Σ−1ε µ¯T (3.5)
dt ∂θ 1 x ∂θ
2 2
Furthermorewhileitispossibletorunthedynamicsfortheµandtheθsimultaneously,it
isoftenbettertotreatpredictivecodingasanEMalgorithm(Dempster,Laird,&Rubin,
1977)andalternatetheupdates. Empirically,itistypicallybesttoruntheoptimization
oftheµs, with fixedθuntilclose toconvergence, andthen runthe dynamicson theθ
withfixedµforashortwhile(Friston,2005). Thisimplicitlyenforcesaseparationof
Chapter3. PredictiveCoding 83
timescalesuponthemodelwheretheµareseenasdynamicalvariableswhichchange
quicklywhile the θareslowly-changing parameters. Forinstance,theµsare typically
interpretedasrapidlychangingneuralfiringrates,whiletheθsaretheslowlychanging
synapticweightvalues(Friston,2005;Rao&Ballard,1999).
Finally,wecanseehowthisderivationofpredictivecodingmapsontoputativepsycho-
logicalprocessesofperception andlearning. Theupdatesof theµcanbeinterpreted as
aprocessofperception,sincetheµismeanttocorrespondtothetruelatentstateofthe
environmentgeneratingtheoobservations. Bycontrast,thedynamicsoftheθcanbe
thought ofas correspondingto learning, since these θ effectivelydefine the mapping
betweenthelatentstateµandtheobservationso.
3.3 Hierarchical predictive coding
Thusfar,wehaveonlyderivedapredictivecodingschemewithasingleleveloflatent
variables µ . However, the expressivity of such a scheme is limited. The success of
1
deepneuralnetworks inmachinelearninghave demonstratedthathavinghierarchical
setsoflatentvariablesiskeytoallowingmethodstolearnabstractionsandtohandle
intrinsicallyhierarchicaldependenciesofthesorthumansintuitivelyperceive(Hinton,
Srivastava, & Swersky, 2012; Krizhevsky, Sutskever, & Hinton, 2012). Predictive
codingcanbestraightforwardlyextendedtohandlehierarchicaldynamicsofarbitrary
depth. Thisisdonethroughpostulatingmultiplelayersoflatentvariablesx1...x and
L
thendefiningthegenerativemodelasfollows,
L−1
p(x ...x )= p(x )∏ p(x |x ) (3.6)
0 L L l l+1
l=0
where p(x |x )=N(x ; f (θ x ,Σ )andthefinallayer p(x )=N(x |x¯ ,Σ )has
l l+1 l l l+1 l+1 l L L L L
an arbitrary prior x¯ and the latent variable at the bottom of the hierarchy is set to
L
the observation actually received x =o. Similarly, we define a separate variational
0
posteriorforeachlayerq(x
1:L
|o)=∏ L
l=1
δ(x
l
−µ
l
),thenthevariationalfreeenergycan
Chapter3. PredictiveCoding 84
bewrittenasasumofthepredictionerrorsateachlayer,
L
F = ∑Σ−1ε2+ln2πΣ (3.7)
l l l
l=1
whereε =µ − f (θ µ ). Giventhatthefreeenergydividesnicelyinto thesumof
l l l l+1 l+1
layer-wisepredictionerrors,itcomesasnosurprisethatthedynamicsoftheµandthe
θaresimilarlyseparableacrosslayers.
dµ ∂F ∂f
l =− =Σ−1 ε l−1 θT −Σ−1ε (3.8)
dt ∂µ l−1 l−1 ∂µ l l l
l l
dθ ∂F ∂f
l =− =Σ−1ε l−1 µ (3.9)
dt ∂θ l l−1 ∂θ l
l l
We see that the dynamics for the variational means µ depend only on the prediction
errors at their layer and the prediction errors on the level below. Intuitively, we can
thinkoftheµsastryingtofindacompromisebetweencausingerrorbydeviatingfrom
thepredictionfromthelayerabove,andadjustingtheirownpredictiontoresolveerror
atthelayerbelow. Inaneurally-implementedhierarchicalpredictivecodingnetwork,
prediction errors would be the only information transmitted ‘upwards’ from sensory
datatowardslatentrepresentations,whilepredictionswouldbetransmitted‘downwards’.
Cruciallyforconceptualreadingsofpredictivecoding,thismeansthatsensorydatais
not transmitteddirectlyupthroughthehierarchy,asisassumedinmuchofperceptual
neuroscience. The dynamics for the µs are also fairly biologically plausible as they
are effectively just the sum of the precision-weighted prediction errors from the µs
own layer and the layer below, the prediction errors from below being transmitted
backupwardsthroughthe synapticweightsθT andweightedwiththegradientof the
activationfunction f .
l
Importantly, the dynamics forthe synaptic weights is entirely local, needing only the
predictionerrorfromthelayerbelowandthecurrentµatthegivenlayer. Thedynamics
Chapter3. PredictiveCoding 85
thusbecomesaHebbianrulebetweenthepresynapticε andpostsynapticµ ,weighted
l−1 l
bythegradientoftheactivationfunction.
Baseduponourpreviouswork(Millidge,2019c),wepresentempiricalevaluationsand
demonstrationsoftheexpressivepowerofhierarchicalpredictivecodingnetworkson
standardmachinelearningbenchmarkswithlearnablegenerativemodels. Whilesome
previousworkhasimplementedhierarchicalpredictivecodingmodelsandtestedthem
on‘blinddeconvolution’ofsimulatedERPdata(Friston,2005,2008a),wepresenta
keydemonstrationof predictivecodingnetworkswithin amachinelearningparadigm,
andwithacompletelylearntgenerativemodel.
First, we tested the potential of predictive coding networks as autoencoders (Hinton
& Zemel, 1994) from machine learning. Here, the goal of the network is simply to
reconstructitsinputdata. Intheory,thiscanbedonetriviallybelearningtheidentity
mapping,sotomakeitdifficultwecreateaninformationbottleneck(Tishby,Pereira,&
Bialek,2000)inthehiddenlayers,suchthattheinputiscompressedtoamuchsmaller
latent code, which must then be decompressed to successfully reconstruct the image.
Autoencoders of this type are widely used in machine learning and a probabilistic
variant – variational autoencoders (Kingma & Welling, 2013) are still state of the art
at many image generation tasks (Child, 2020). Here we demonstrate that predictive
codingnetworkscanalso function aspowerfulautoencoders. Wefirsttestthe potential
ofpredictivecodingontheMNISTdataset–astandardmachinelearningbenchmark
datasetof60,00028x28grayscalehandwrittendigits. Weutilizedathreelayerpredictive
codingnetwork,with aninputand outputdimensionalityof784 (thesizeofa flattened
vectoroftheMNISTdigit),andalatentdimensionalityof20,meaningthatthenetwork
hadtolearntocompressa784dimensionalmanifoldintoa20dimensionallatentspace.
We trainedthe predictive codingnetwork accordingto Equations 3.5with a batchsize
of 64 and a learning rate of 0.01. A sigmoid nonlinearity was used on the input and
latentlayers. Wetrainedthenetworksfor100epochs. Eachepochconsistedofupdating
Chapter3. PredictiveCoding 86
theµsusingEquation3.8for100steps,andthenupdatingtheweightsθusingEquation
3.9once. ThemodelwasabletorecreateMNISTdigitssuccessfully,asshowninthe
examplereconstructionsbelow:
Figure3.1: MNISTdigitsinthetrainingsetrecreatedbythenetwork. Toprowtheactual
digits,bottomrow,thepredictivereconstructions.
Additionally,themodelwas alsoabletorecognizeandreconstruct previouslyunseen
MNIST digits, albeit with slightly lower fidelity. Nevertheless it is impressive how
rapidlyandwellthenetworkisabletogeneralizetocompletelyunseendigits.
Chapter3. PredictiveCoding 87
Figure3.2: UnseenMNISTdigitsinthetestsetrecreatedbythenetwork. Toprowthe
actualdigits,bottomrow,thepredictivereconstructions.
Sincethemodelisagenerativemodel,itisalsoabletogeneralizeoutsidethetraining
settogenerate,or‘dream’,completelyunseendigitsbysamplingfromthelatentspace.
Examplesareshownbelow:
Figure3.3: Imagesofhallucinateddigits"dreamt"bythenetwork. Theseweregenerated
bysamplingthelatentspacearoundtherepresentationsofsomeexemplardigitsinthe
latentspace, and thenletting thepredictivecoding networkgenerate itsprediction from
thechosenlatentstate.
Chapter3. PredictiveCoding 88
Figure3.4: APCAclusteringplotofthevaluesoftestMNISTdigitsinthelatentspace.
Eventhoughthe20dimensionallatentspacehasbeenreduceddowntotwo,clusters
are still visible. For instance, all the 1s are clustered in the top left corner. We thus
see that predictive coding appears to be a powerful and fully unsupervised learning
algorithm, capableof separating outdistinct digits inthe latent space, despitenot being
trainedwithanylabelinformationatall–andpurelyonreconstruction.
Finally, tovisualizethe latentspace,PCA (principalcomponentsanalysis) was applied
to the learned representations in the 20 dimensional latent space to shrink it down to
a two dimensional space. It is apparent upon inspection of Figure 3.4 that the latent
space, even when shrunk down to two dimensions, does a good job of clustering the
MNIST,puttingallthe1sinthetopleftcorner,orallthezerosinthemiddleright. This
strongly indicates that the predictive coding model is able to learn the categories of
digitsdespitebeingtrainedinanentirelyunsupervisedwaywithoutanyknowledgeof
thetrueidentitiesofthedigits.
Weadditionallytestedthenetwork’scapabilitytoreconstructCIFARimages,whichare
32x32colourimagesofnaturalscnes. OurnetworkwasthesameintheMNISTcase
exceptthatweusedalatentdimensionof50.
The hierarchical predictive coding CIFAR models are able to learn to reconstruct
CIFARimageswithimpressivefidelitygiventhattheywerecompressedfroma1024
Chapter3. PredictiveCoding 89
dimensionalimageintoa50dimensionallatentstate.
Figure3.5: TestsetCIFARdigitsreconstructedbythenetwork. Thefirstofthetwolines
is the image and the second is the reconstruction. The network is extremely good at
reconstructingCIFARimages.
Importantly, due to having learnt a latent space, we are able to interpolate between
images, and thus investigate the properties of the learnt latent space. To interpolate,
webeginwithtwoimageso ando andtakethedifferencevectorinthelatentspace
1 2
ε= f(o )− f(o ) where f is the encoder function. Then, we step in the latent space
2 1
Chapter3. PredictiveCoding 90
by a constant α and decode such that oˆ = g(f(o )+αε) where g is the decoding
2
function. Westeppedinincrementsofα=0.1. Belowwecanseethenetworksmoothly
interpolatingbetweenan imageofahorse andacat. Wesee thatthepredictivecoding
networkappearstonaturallylearn asmoothlatentspace,allowingforgeneralizationto
unseenimagesandsmoothinterpolationbetweenclassesinthelatentspace.
Figure3.6: TheCIFARmodelinterpolatingbetweenahorseandacat. Readtheimages
left to right top to bottom - like text. The interpolation is done by stepping in the latent
spacefromtherepresentationofthefirstimageinthedirectionoftheseconduntilitis
reached.
3.3.1 Dynamical Predictive coding
Whilesofar,wehaveonlyconsideredmodellingjustasinglestaticstimuluso. However,
thedatathebrainreceivescomesintemporalsequenceso¯=[o ,o ...]. Tomodelsuch
1 2
temporalsequences,itisoftenusefultosplitthelatentvariablesinto states,whichcan
Chapter3. PredictiveCoding 91
vary with time, and parameters which cannot. In the case of sequences, instead of
minimizing the variational free energy, we must instead minimize the free action F ¯ ,
whichissimplythepathintegralofthevariationalfreeenergythroughtime:
µ∗ =argminF ¯
µ
(cid:90)
F ¯ = dtF
t
F =D [q(x |o ;φ)||p(o ,x |x )] (3.10)
t KL t t t t t−1
Whiletherearenumerousmethodstohandlesequencedata,oneinfluentialandelegant
approach(Friston,2008a;Friston,Stephan,Li,&Daunizeau,2010;Fristonetal.,2008)
is to represent temporal data in terms of generalized coordinates of motion. These
coordinates represent not just the immediate observation state, but all the temporal
derivatives of the observation. For instance, suppose that the brain represents beliefs
about the position of an object. Under a generalized coordinate model, it would also
represent beliefs about the velocity (first time derivative), acceleration (second time
derivative), jerk (third time derivative) and so on. All these time derivative beliefs
are concatenated to form a generalized state. The key insight into this dynamical
formulationis,thatwhenwritteninsuchaway,manyofthemathematicaldifficultiesin
handlingsequencesdisappear,leaving relativelystraightforwardandsimplevariational
filteringalgorithmswhichnativelyhandlesmoothlychangingsequences. Forinstance,
we maintain a coherent concept of a stationary state, since we can define it as one in
whichnoneofthetimederivativesarechanging. Thisallowsthevariationalinference
proceduretotrackamovingtargetbyrepresentingitasasteadystateinamovingframe
ofreference.
Becausethegeneralisedcoordinatesarenotationallyawkward,wewillbeveryexplicit
in the following. We denote the time derivatives of the generalized coordinate using
a (cid:48), so µ(cid:48) is the belief about the velocity of the µ, just as µ is the belief about the
‘position’ about the µ. A key point of confusion is that there is also a ‘real’ velocity
Chapter3. PredictiveCoding 92
of µ, which we denote µ˙, which represents how the belief in µ actually changes over
time. Importantly,thisisnotnecessarilythesameasthebeliefinthevelocity: µ˙ (cid:54)=µ(cid:48),
except at the equilibrium state, which can be understood as the path of least action.
Intuitively,thismakessenseasatequilibrium(mimimumofthefreeaction, andthus
perfectinference),ourbeliefaboutthevelocityofmuµ(cid:48) andthe‘real’velocityperfectly
match. Awayfromequilibrium,ourinferenceisnotperfectsotheydonotnecessarily
match. We denote the generalized coordinate representation of a state µ˜ as simply a
vector of each of the beliefs about the time derivatives µ˜ =[µ,µ(cid:48),µ(cid:48)(cid:48),µ(cid:48)(cid:48)(cid:48)...]. We also
define the operator D which maps each element of the generalised coordinate to its
timederivativei.e. Dµ=µ(cid:48),Dµ˜ =[µ(cid:48),µ(cid:48)(cid:48),µ(cid:48)(cid:48)(cid:48),µ(cid:48)(cid:48)(cid:48)(cid:48)...]. Withthisnotation,wecandefine
a dynamical generative model using generalized coordinates. Crucially, we assume
that the noise ω in the generative model is not white noise, but is coloured, so it has
non-zeroautocorrelationandcanbedifferentiated. Effectively,colourednoiseallows
one to model relatively slowly (not infinitely fast) exogenous forces on the system.
Formoreinformationoncolourednoisevswhitenoisesee(Fristonetal.,2008;Yuan
& Ao, 2012). With this assumption we can obtain a generative model in generalized
coordinatesofmotionbysimplydifferentiatingtheoriginalmodel.
o= f(x)+ω x=g(x¯)+ω
o x
o(cid:48) = f(cid:48)(x)x(cid:48)+ω(cid:48) x(cid:48) =g(cid:48)(x¯)x(cid:48)+ω(cid:48)
o x
o(cid:48)(cid:48) = f(cid:48)(x)x(cid:48)(cid:48)+ω(cid:48)(cid:48) x(cid:48)(cid:48) =g(cid:48)(x¯)x(cid:48)(cid:48)+ω(cid:48)(cid:48)
o x
... ... (3.11)
Where we have applied a local linearisation assumption (Friston et al., 2008) which
drops the cross terms in the derivatives. We can write these generative models more
compactlyingeneralizedcoordinates.
o˜= f˜(x˜)+ω˜ x˜=g˜(x˜¯)+ω˜ (3.12)
o x
which,writtenprobabilisticallyis p(o˜,x˜)= p(o˜|x˜)p(x˜). Ithasbeenshown(Fristonet
al., 2008) that the optimal (equilibrium) solution to this free action is the following
Chapter3. PredictiveCoding 93
stochasticdifferentialequation,
∂E [lnp(o˜,x˜)]
µ˙˜ =Dµ˜+ q(x˜|o˜;µ˜) +ω˜ (3.13)
∂µ˜
Where ω˜ is the generalized noise at all orders of motion. Intuitively, this is because
when
∂E
q(x|o;µ)
[lnp(o˜,x˜)]
= 0 then µ˙˜ = Dµ˜, or that the ‘real’ change in the variable is
∂µ
precisely equal to the expected change. This equilibrium is a dynamical equilibrium
which moves over time, but precisely in line with the beliefs µ(cid:48). This allows the
systemtotrackadynamicallymovingoptimalsolutionprecisely,andthegeneralized
coordinateslet uscapturethis motionwhile retaining thestatic analyticalapproachof
anequilibrium solution, whichwouldotherwise necessarilypreclude motion. Thereare
multipleoptionstoturnthisresultintoavariationalinferencealgorithm. Note,theabove
equationmakesnoassumptionsabouttheformofvariationaldensityorthegenerative
model, and thus allows multimodal or nonparametric distributions to be represented.
For instance, the above equation (Equation 3.13) could be integrated numerically by
a number of particles in parallel, thus leading to a generalization of particle filtering
(Friston, 2008b). Alternatively, a fixed Gaussian form for the variational density can
be assumed, using the Laplace approximation. In this case, we obtain a very similar
algorithm topredictive coding asbefore, but usinggeneralized coordinatesof motion.
Inthelattercase,wecanwriteoutthefreeenergyas,
F =lnp(o˜|x˜)p(x˜)
t
∝Σ˜−1ε˜2+Σ˜−1ε˜2 (3.14)
o o x x
Where ε˜ = o˜− f˜(x˜) and ε˜ = o˜−g˜(x˜¯). Moreover, the generalized precisions Σ˜−1
o x
not only encode the covariance between individual elements of the data or latent
space at each order, but also the correlations between generalized orders themselves.
Since we are using a unimodal (Gaussian) approximation, instead of integrating the
stochasticdifferentialequationsofmultipleparticles,weinsteadonlyneedtointegrate
thedeterministicdifferentialequationofthemodeofthefreeenergy,
µ˙˜ =Dµ˜−Σ˜−1ε˜ −Σ˜−1ε˜ (3.15)
o o x x
Chapter3. PredictiveCoding 94
which cashes out in a scheme very similar to standard predictive coding (compare to
Equation3.8),butingeneralizedcoordinatesofmotion. TheonlydifferenceistheDµ˜
termwhichlinkstheordersofmotiontogether. Thistermcanbeintuitivelyunderstood
as providing the ‘prior motion’ while the prediction errors provide ‘the force’ terms.
Tomakethisclearer,let’stakeaconcretephysicalanalogywhereµisthepositionof
some object and µ(cid:48) is the expected velocity. Moreover, the object is subject to forces
Σ˜−1ε˜ +Σ˜−1ε˜ which instantaneously affect its position. Now, the total change in
o o x x
position µ˙˜ can be thought of as first taking the change in position due to the intrinsic
velocity ofthe objectDµand adding thaton to the extrinsic changes due to thevarious
exogenousforces.
Things get more complex when we consider a model which has both dynamical and
hierarchical components where there are interactions between them. This we call a
full-construct model following the lead of this tutorial (Buckley, Kim, McGregor, &
Seth,2017). Inafullconstructmodelthereisadynamicalhierarchyoflevelswhereit
isassumedthateachdynamicalorderisonlyabletoaffectthelevelbelow:
o= f(µ;θ)
µ= f(cid:48)(µ(cid:48);θ(cid:48))
µ(cid:48) = f(cid:48)(µ(cid:48)(cid:48);θ(cid:48)(cid:48))
... (3.16)
Similarly,thereissimultaneouslyahierarchyoflevels,whereeachlevelisassumedto
Chapter3. PredictiveCoding 95
bepredictedbythelevelaboveit:
o=g(µ ;θ )
0 0
µ =g(cid:48)(µ ;θ )
0 1 1
µ =g(cid:48)(µ ;θ )
1 2 2
µ =g(cid:48)(µ ;θ )
2 3 3
... (3.17)
Therefore,eachnodeinthelatticeofhierarchicalanddynamichierarchiesisinfluenced
by two separate predictions - the dynamical prediction going from higher dynamical
orderstolower,andthehierarchicalpredictionpropagatingfromhigherlevelsofthe
hierarchytolowerones. Thus,asinglestateofacause-unitµn,whereiisthelevelof
i
thehierarchy,andnisthedynamicalorder,isdefinedtobe:
µn = f(µn+1;θn+1)+g(µn ;θn ) (3.18)
i i i i+1 i+1
Thismeansthatthevariationalfreeenergymustsumoverbothdynamicalandhierar-
chicalpredictionerrors,suchthat:
F
=∑∑Σn−1(εn)2
(3.19)
i i
i n
Andthatadditionallythe updates fortherepresentation-unitsandtheweightsmusttake
thisintoaccount. Therevisedupdaterulesarepresentedbelow:
dµn df dg
i =Σ−1 εn+1 θn+1+Σ−1εn+Σ−1εn θn +Σ−1εn (3.20)
dt n+1 i dµn i n i i−1 i−1dµn i−1 i i
i i
Andfortheweightstheupdaterulethusbecomes:
dθn
i =Σnεn(
df
µn+1T +
dg
µn T) (3.21)
dt i i dθn i dθn i+1
i i
These rules appear somewhat more complicated than the corresponding rules in the
staticcase. Neverthelesstheyonlyincuralinear(intheorderofgeneralizedcoordinates
considered)additionalcomputationalcost.
Chapter3. PredictiveCoding 96
Preliminary dynamical and full construct models were implemented and tested on
simple stimuli. The first task the dynamical models were tested on was predicting a
sine wave. This is the perfect toy-task since sine waves have analytic derivatives to
any infinite degree. We used a dynamical model which represented three orders of
generalized motion. The model was trained to predict a sine wave autoregressively
and its’ first two temporal derivatives. The model rapidly learned to predict the sine
wave, ascan beseen fromthe traininggraphs below. However, therewas aconsistent
phase-errorin the predictions it made,which couldhave beencaused bythe rapidrate
ofchangeofthesine-waveobservations.
Chapter3. PredictiveCoding 97
(a)Incomingsensedata-i.e. a (b)Firstderivativeoftheincom-
sinewave ingsensedata
(c) Predicted incoming sense (d)Predictiontemporalderiva-
data tiveoftheincomingsensedata
(e) Prediction error at the first (f) Prediction error at the sec-
dynamicallevel onddynamicallevel
(h)Activationoftherepresenta-
(g)Acitvationofrepresentation
tionunitsattheseconddynam-
unitsatthefirstdynamicallevel
icallevel
Figure3.7: Predictionerrorsandpredictionforsimpletoydynamicalmodels. Thetaskof
thedynamicalpredictivecodingmodelistolearntopredictasinewaveusingonlythe
first twodynamical orders – soincluding position, velocity, and acceleration. The model
startsfromrandomlyinitializedparameters. Weseethatthemodelveryquicklylearns
tomatchtheincomingsinewaveobservationswithonlyminimalerroratthebeginning.
Chapter3. PredictiveCoding 98
Thedynamicalmodeldoesnotonlyworkwithsinewaves. Themodelwasalsotested
on more jerky waveforms sawtooth waves. In this case a two layer linear dynamical
model was used which learned to predict the sawtooth wave and its first temporal
derivative. The model predicts the wave very successfully, including the temporal
derivative,althoughthereitisalittlelesssuccessful. Onceagainthereisapersistent
patternedpredictionerror,likelycausedbythelagtimebetweenthemodelspredictions
andthetheobservationsitreceives.
Chapter3. PredictiveCoding 99
(a)Incomdingsensedata-the (b)Firstderivativeoftheincom- (c) Predicted incoming sense
sawtoothwave ingsensedata data
(d)Predictiontemporalderiva- (e) Prediction error at the first (f) Prediction error at the sec-
tiveoftheincomingsensedata dynamicallevel onddynamicallevel
(h)Activationoftherepresenta-
(g)Activationofrepresentation
tionunitsattheseconddynam-
unitsatthefirstdynamicallevel
icallevel
Figure 3.8: Dynamical models tested onmore challenging sawtoothand square wave
inputs. The model was randomly initialized and only modelled the first two dynamical
orders (soposition, velocity, acceleration). Apart from abrief initialperiod of uncertainty,
themodelrapidlylearnedtopredictthesemorechallengingwaveshapes.
Wecanalsotrain‘full-construct’modelsondynamicalstimuli. Hereweusedamodel
withtwohierarchicallayersandthreedynamicallayersandwastrainedautoregressively
topredictasine-wave. Trainingwas‘online’withalearningrateof0.01. Thegenerative
model parameters θ were updated initialized randomly and updated each epoch. For
every‘tick’ofthesinewave,thevariationalparametersµwereupdatedfor100steps.
Chapter3. PredictiveCoding 100
Traininggraphsareshownbelow:
(a) Incomding sense data - (b) First derivative of the (c)Predictedincomingsense
Sinewave sense-data data
(e)Themodels’predictionof (f) the models’ prediction of
(d)Themodels’predictionof
the first derivative of the in- the second derivative of the
theincomignsensedata
comingsense-data incomingsense-data
(g)Thepredictionerroratthe (h)Thepredictionerroratthe (i)Thepredictionerroratthe
firsthierarcicallayer secondhierarcicallayer firstdynamicallayer
Figure 3.9: The training graphs of the full construct model. It can successfully predict
the first three temporal derivatives of a sine wave, and also minimise prediction error
up to multiple hierarchical layers. The full construct model was randomly initialized,
and learnt both parameters and inferred states purely online – thus achieving a ‘double
deconvolution’(Fristonetal.,2008).
The top two rows of graphs show the incoming sense data and the first two temporal
Chapter3. PredictiveCoding 101
derivatives ofthe sense-data. Thenext tworowsshow theprediction errorsovertime
forvariouslevelsofthehierarchy. Thefullconstructmodelappearsabitlessstableand
successfulthanthesimpledynamicalmodel,likelybecauseitis much morecomplex
andhasmanymoremovingparts. Neverthelessitmanagestolearnthesinewaveshapes
relatively faithfully and does also manage to rapidly reduce the prediction error over
time. Moreover these sorts of tasks do not really play well to the strength of the full-
constructmodelsincetheinputdata(thesinewave)containsnosuitablehierarchical
structureforthehigherlevelstomodel. Itseemslikelythatasthesemodelsarescaled
uptomorechallengingtasks,thegreaterexpressivityandpowerofthefull-construct
models will become more apparent. So far, we have only experimented with ‘full-
construct’ models on simple toy tasks such as sine waves. An interesting avenue for
futurewouldworkbeexperimentingastowhetherfullconstructmodelscouldbescaled
up to handle challenging machine learning tasks dealing with sequential data such
as video prediction. While predictive coding networks have been proposed for this
task (Lotter et al., 2016), none to our knowledge have explicitly utilized generalized
coordinatesinanycapacity. Thereis,however,aliteratureusingthepredictivecoding
schemes with generalized coordinates in nonlinear (chaotic) systems with separation
oftemporalscales. Thisenablestherecognitionandpredictionofthingslikebirdsong
andspeech–andindeedtheirlearningofparticularsongs. Crucially,theseapplications
restupongeneralisedcoordinatesofmotion,usuallyupto4thordermotion(Friston&
Frith,2015;Friston&Kiebel,2009;Isomura,Parr,&Friston,2019)
3.4 Predictive Coding and Kalman Filtering
Akeyintuitionbehindtheutilityofpredictivecodingisthatitnaturallyhandlesfiltering
tasks. Filteringtasksrequireconstantupdatesofamovingstateestimategivensequences
of new data. Effectively, filtering is the task of learning and inferring movements in
thehiddenstateoftheworldfromchanginginputsequences–asopposedtotheusual
Chapter3. PredictiveCoding 102
machinelearningtaskofinferringhiddenstates(suchaslabels)fromsinglestaticinputs.
Importantlythecoretaskfacedbymuch ofthebrainisfundamentallyoneoffiltering,
sincetheinputsthebrainreceivesareactuallytemporallyextendedsequences,rather
thanstaticflashes. Forinstance, invisionthetaskofthebrainisnottocategorizestatic
imagesbutrathertoinferthestateof,andultimatelyinteractwith,asmoothlychanging
externalworldsituatedincontinuous time. Moreover,itisknownthatthebrain takes
substantialadvantageoftheadditionalinformationgivenbyintegratingsequencesover
time such as optical flow (Gibson, 2002) and active motionto explore different angles
onagivenscene(Henderson,2017).
If, as we generally assume throughout the thesis, that the brain is fundamentally a
(Bayesian)inferencemachine,thenthecoretaskofthebrainmustbeBayesianFiltering
insteadofstaticBayesianinference(Särkkä,2013). Bayesianfilteringismathematically
somewhatmoreinvolved,duetotheneedtoperforminferenceoversequencesinstead
ofsingledata-points,butthereareawidevarietyofalgorithmsintheliteraturewhich
performBayesianfiltering,oftenhighlyeffectively(Kutschireiter,2018;Kutschireiter,
Surace, & Pfister, 2020). Mathematically, we can formalize the filtering problem as
follows (Jaswinski, 1970; Stengel, 1994). We have an estimated state xˆ , and some
t
modelofhowtheworldevolves(thedynamicsmodel): xˆ = f(xˆ ). Wealsoreceive
t+1 t
observations o, and you have some model of how the observations depend on the
estimated state (the observation model): o = g(xˆ ). The task, then, is to compute
t
p(xˆ |xˆ ,o ). Inthegeneralnonlinearcase,thiscalculationisanalyticallyintractable
t+1 t 1...t
and extremely expensive to compute exactly. Some form of approximate solution is
required. Twoformsofapproximationaregenerallyused. Thefirstistoapproximate
themodel-suchasbyassuminglinearityofthedynamicsandobservationmodels.
Thesecondmethodistoapproximatetheposterior–usuallywithasetofsamples(or
particles). This approach is taken by the class of particle filtering algorithms which
track the changing posterior by propagating the particles through the dynamics and
Chapter3. PredictiveCoding 103
thenresamplingbaseduponupdatedmeasurementinformation(Arulampalam,Maskell,
Gordon, & Clapp, 2002; N. J. Gordon, Salmond, & Smith, 1993). This approach
can handle general nonlinear filtering cases, but suffers strongly from the curse of
dimensionality. If the state-space is high-dimensional the number of particles required
foragoodapproximationgrowsrapidly(Doucet,Godsill,&Andrieu,2000).Moreover,
there has been some fascinating work on implementing particle filtering methods in
neuralcircuitry(Kutschireiter,Surace,Sprekeler,&Pfister,2015),aswellasspeculation
aboutwhetherperhapsthebrainmayutilizeparticleorsamplingmethodsforinference
insteadofvariationalones(Sanborn&Chater,2016).
Nevertheless, here wefocus primarilyon approximatevariationalapproaches toinfer-
ence. Specifically, we first demonstrate that predictive coding is naturally a filtering
algorithm–perhapsmorenaturally thanoneappliedtostaticdatasets. Theonlychange
to the algorithm is simply what is predicted. If predictive coding is set up so as to
predict thenext input (Clark,2013b; Mumford, 1992), whichis highly plausible inthe
brain, then it can perform variational Bayesian filtering. In this section, we explore
this predictive coding filtering algorithm and show, crucially, that in the linear case
it becomes a variant of Kalman filtering – a fundamental and ubiquitous algorithm
in classical control (Kalman, 1960; Kalman et al., 1960). Moreover, in the nonlinear
case,predictivecodingbecomesavariantofextendedKalmanfiltering(Ollivier,2019).
The Kalman Filter solves the general filtering problem by making two simplifying
assumptions. Thefirstisthatboththedynamicsmodelandtheobservationmodelare
linear. ThesecondassumptionisthatnoiseenteringthesystemiswhiteandGaussian.
This makes both the prior and likelihoods Gaussian. Since the Gaussian distribution
is a conjugate prior to itself, this induces a Gaussian posterior, which can then serve
asthepriorin the nexttimestep. Sinceboth priorandposteriorareGaussian,filtering
can continue recursively for any number of time-steps without the posterior growing
in complexity and becoming intractable. The Kalman filter is the Bayes-optimal so-
lution provided that the assumptions of linear models and white Gaussian noise are
Chapter3. PredictiveCoding 104
met(Kalman, 1960). TheKalman Filter, dueto itssimplicityand utilityiswidely used
in engineering, time-series analysis, aeronautics, and economics (Grewal & Andrews,
2010;Harvey,1990;Leondes,1970;Schneider,1988).
Sincepredictivecodingpossessesseveralneurophysiologicallyrealisticprocesstheories
(Bastosetal.,2012),thiscorrespondenceprovidesanavenueforabiologicallyplausible
implementation ofKalman filteringin the brain. There issubstantial evidencethat the
brainiscapableofBayes-optimalintegrationofnoisymeasurements,andisapparently
in possession of robust forward models both in perception (Simoncelli, 2009; Zago,
McIntyre, Senot, & Lacquaniti, 2008) and motor control (Gold & Shadlen, 2003;
Munuera,Morel,Duhamel,&Deneve,2009;Todorov,2004). deXivry,Coppe,Blohm,
andLefevre(2013)haveevenshownthataKalmanfiltersuccessfullyfitspsychomotor
dataonvisuallyguidedsaccadesandsmoothpursuitmovement,althoughtheyremain
agnostic on how it may be implemented in the brain. We demonstrate, however, a clear
mathematicallinkoftherelationshipbetweenKalmanfilteringandpredictivecoding,
allowingusfirsttouseresultsfromKalmanfilteringtounderstandtheperformanceof
predictivecodingalgorithms,andsecondenablingustoutilizetheprocesstheoriesof
predictivecodingtounderstandhowthebrainmayperformcrucialfilteringtasks.
First,werevealthepreciserelationshipbetweenKalmanfilteringandpredictivecoding
–namelythatbothoptimizethesameBayesianobjective,whichisconvexinthelinear
case. However, the Kalman filter solves the optimization problem analytically, thus
givingrisetoitsalgebraiccomplexitiesandespeciallythehighlyneurobiologicallyim-
plausibleKalmangainmatrix. Predictivecoding,ontheotherhand,solvestheobjective
throughaprocessofgradientdescentonthesufficientstatisticsofthevariationaldis-
tribution,therebyobtainingbiologicallyplausibleHebbianupdaterules. Additionally,
the fullyBayesian perspective grantedby predictive codingalso allows usto perform
learning of the generative model – i.e. learning the coefficients of the dynamics and
likelihood matrices – which allows us to handle cases where the model of theworld is
Chapter3. PredictiveCoding 105
unknown,incontrasttotraditionalKalmanfilteringwhichassumesaccurate(andlinear)
dynamicsandobservationmodelsoftheworld. Whilethecloserelationshipbetween
Kalmanfilteringand(linear)predictivecodinghasbeenhintedatbefore(Friston,2005,
2008a),thereitisclaimedthatpredictivecodingis‘equivalent’toKalmanfiltering–
which is not thecase except insofar as the twoalgorithms optimize the same objective.
BaltieriandBuckley(2020)providesidebysidecomparisonsoftheupdaterulesfor
predictivecodingandKalmanfiltering,butdonotgobeyondthissuperficialanalysisto
uncoverthepreciserelationshipbetweenthem.
Secondly, wedirectlycomparetheperformanceoftheKalmanfilterandourpredictive
coding algorithm on a simplified location tracking task – which the Kalman filter
excelsat. Weshowthatdespitethepredictivecodingalgorithmperformingagradient
descentinsteadofananalyticalsolution,itperformscomparablywiththeKalmanfilter
and, due to the convexity of the underlying optimization problem, requires very few
iterationstoconverge. Thisrapidconvergenceisimportant,sincethebrainisheavily
time-constrained in its inferences – choices often must be made fast. Secondly, we
demonstratethatthelearningrulesforthelikelihoodanddynamicsmatricesallowusto
performonlinetrackingevenwhenthemodeliscompletelyunknown. Weonlyshow
thatthis isthecase forthedynamics, however, aslearningdoes notperformwell with
anunknownobservationmodel. Wehypothesizethatthisisduetotheill-posednessof
theresultingoptimizationproblem.
3.4.1 The Kalman Filter
TheKalmanFilterisdefineduponthefollowinglinearstate-space3
x =Ax +Bu +ω
t+1 t t
o =Cx +z (3.22)
t+1 t+1 t
3Forsimplicity,themodelispresentedindiscretetime. ThecontinuoustimeanalogueoftheKalman
filteristheKalman-Bucyfilter(Kalman&Bucy,1961). Generalizationofthisschemetocontinuous
timeisanavenueforfuturework.
Chapter3. PredictiveCoding 106
Where x represents the hidden or internal state at time t. u is the control - or known
t t
inputstothesystem-attimet. MatricesA,B,andC parametrizethelineardynamicsor
observationmodels,andωandzarebothzero-meanwhitenoiseGaussianprocesses
withcovarianceΣ andΣ ,respectively. Sincetheposterior p(x |o ,x )isGaussian,
ω z t+1 1...t t
itcanberepresentedbyitstwosufficientstatistics–themeanµandcovariancematrix
Σ .
x
Kalmanfilteringproceedsbyfirst‘projecting’forwardthecurrentestimatesaccording
tothedynamicsmodel. Thenthese estimatesare‘corrected’bynewsensorydata. The
Kalmanfilteringequationsareasfollows:
Projection
µˆ =Aµ +Bu
t+1 t t
Σˆ (t+1)=AΣ (t)AT +Σ (3.23)
x x ω
Correction
µ =µˆ +K(o −Cµˆ )
t+1 t+1 t+1 t+1
Σ (t+1)=(I−K)Σˆ (t+1)
x x
K =Σˆ (t+1)CT[CΣˆ (t+1)CT +Σ ]−1 (3.24)
x x z
Whereµ andΣ (t)arethemeanandvarianceoftheestimateofthestatexattimet,and
t x
K istheKalmangainmatrix. Althoughtheseupdaterulesprovideananalyticallyexact
solutiontothefilteringproblem,thecomplicatedlinearalgebraexpressions,especially
that for the Kalman gain matrix K, make it hard to see how such equations could be
implementeddirectlyinthebrain.
Importantly,theseKalmanfilteringequationscanbederiveddirectlyfromBayes’rule.
ThemeanoftheposteriordistributionisalsotheMAP(maximum-a-posteriori)point,
sinceaGaussiandistributionisunimodal. Thus,toestimatethenewmean,wesimply
Chapter3. PredictiveCoding 107
havetoestimate,
argmax p(xˆ |o ,xˆ )∝argmaxp(o |xˆ )p(xˆ |xˆ )
t+1 t+1 t t+1 t+1 t+1 t
xˆt+1 xˆt+1
=argmaxN(o ;Cxˆ ,Σ )N(xˆ ;Axˆ +Bu ,Σ )
t+1 t+1 z t+1 t t ω
xˆt+1
1
=argmax exp(−(y−Cµ )TΣ (y−Cµ )
t+1 Z t+1
Z
µ
t+1
+(µ −Aµ −Bu )TΣˆ (µ −Aµ −Bu )
t+1 t t x t+1 t t
=argmin−(y−Cµ )TΣ (y−Cµ )+(µ −Aµ −Bu )TΣˆ (µ −Aµ −Bu )
t+1 Z t+1 t+1 t t x t+1 t t
µ
t+1
(3.25)
In the second line, the algebraic form of the Gaussian density is substituted and we
haveswitchedthemaximizationvariabletoµ duetothefactthatthemaximumofa
t+1
Gaussianisalsoitsmean. Wealsominimizethelogprobabilityinsteadofmaximizing
theprobability,whichgetsridoftheexponentialandthe normalizingconstant(which
can be computed analytically since the posterior is Gaussian) 4. From this objective,
one can simply solve analytically for the optimal µ and Σ. For a full derivation see
AppendixA.
3.4.2 Predictive Coding as Kalman Filtering
Here wedemonstrate therelationship betweenpredictive codingand Kalmanfiltering.
First,weneedtoexplicitlywriteoutandadaptthemathematicalapparatusofpredictive
coding to filtering problems. To do so, we need to perform variational inference
over full trajectories o ,x of observations and hidden states. If we then assume
1:T 1:T
trajectories are Markov, and are thus licensed to apply a Markov factorization of the
generative model p(o
1:T
,x
1:T
)= p(o
1
|x
1
)p(x
1
)∏
t
T
=2
p(o
t
|x
t
)p(x
t
|x
t−1
) 5 and a mean-
fieldtemporalfactorizationofthevariationaldensity,sothatitisindependentacross
timesteps q(x
1:T
;θ)=∏
t
T
=1
q(x
t
;θ), then the variational free energy of the trajectory
4Thelogtransformationisvalidundermaximization/minimizationsincethelogfunctionismonotonic.
5Where,tomakethisexpressionnotafunctionofx ,weimplicitlyaverageoverourestimateof
t−1
x fromtheprevioustimestep: p(x|x )=E [p(x|x )]
t−1 t t−1 q(xt−1) t t−1
Chapter3. PredictiveCoding 108
factorizesintoindependentlyoptimizablefree-energiesofaparticulartimestep,
T
F(o )= ∑F (o )
1:T t t
t=1
F (o )=D [q(x ;θ)||p(o ,x |x )] (3.26)
t t KL t t t t−1
This temporal factorization of the free energy means that the minimization at each
timestepisindependentofthe others,andsoweonlyneed considerasingleminimiza-
tionofasingletimesteptounderstandthesolution,sincealltime-stepswillbeidentical
in terms of the solution method. Applying the linear Gaussian assumptions of the
Kalmanfilter,wecanspecifyourgenerativemodelintermsofGaussiandistributions,
p(o ,x |x )= p(o |x )p(x |x )
t t t−1 t t t t−1
=N(o ;Cx ,Σ )N(x |Ax ,Σ ) (3.27)
t t z t t−1 x
SinceweknowtheposteriorisGaussian,itmakessensetoalsouseaGaussiandistribu-
tionforthevariationalapproximatedistribution. Importantly,forpredictivecodingwe
makeanadditionalassumption–theLaplaceApproximation–whichcharacterisesthe
varianceofthisGaussianasananalyticfunctionofthemean,thusdefining,
q(x ;θ)=N(x ;µ ,σ(µ)) (3.28)
t t t
whereθ=[µ ,σ(µ )]aretheparametersofthevariationaldistribution–inthiscasea
t t
meanandvariancesincewehaveassumedaGaussianvariationaldistribution. Withthe
variationaldistributionandgenerativemodelpreciselyspecified,itisnowpossibleto
explicitlyevaluatethevariationalfreeenergyforaspecifictime-step,
F (o )=F (o )=D [q(x ;θ)||p(o ,x |x )]
t t t t KL t t t t−1
=−E [lnp(o ,x |x )]−H[q(x ;θ)] (3.29)
q(xt;θ) t t t−1 t
Where the second term is the entropy of the variational distribution. Since we are
only interested in minimizing with respect to the mean µ and the expression for the
t
Chapter3. PredictiveCoding 109
entropyofa Gaussiandoesnot dependonthe mean,wecan ignorethisentropy termin
subsequentsteps. Thekeyquantityisthe‘energy’termE [lnp(o ,x |x ]. Since
q(xt;θ t t t−1
theLaplaceapproximationensures that mostoftheprobabilitydistributionis nearthe
modeµ ofthevariationaldistribution,wecanwellapproximatetheexpectationusinga
t
Taylorexpansiontosecondorderaroundthemode,
∂p(o ,x |x )
E [lnp(o ,x |x )]≈lnp(o ,µ |µ )+E[ t t t−1 | [x −µ ]
q(xt;θ t t t−1 t t t−1
∂x
xt=µt t t
t
∂2p(o ,x |x )
+E[ t t t−1 | [x −µ ]2
∂x2
xt=µt t t
t
∂p(o ,x |x )
=lnp(o ,µ |µ )+ t t t−1 | [E[x ]−µ ]
t t t−1
∂x
xt=µt t t
t (cid:124) (cid:123)(cid:122) (cid:125)
=0
∂2p(o ,x |x )
+ t t t−1 | E[(x −µ )2] (3.30)
∂x
t
2
xt=µt
(cid:124)
t
(cid:123)(cid:122)
t
(cid:125)
=σ
SincethefirsttermvanishesasE[x ]−µ =µ −µ =0andwecanneglectthesecond
t t t t
term since it only depends on σ and not µ, then the only term that matters for the
minimizationisthefirsttermlnp(o ,µ |µ ). Thismeansthatwecanwritetheoverall
t t t−1
optimizationproblemsolvedbypredictivecodingas,
argminF (o )=argminlnp(o ,µ |µ ) (3.31)
t t t t t−1
µt µt
whichisthesameastheMAPoptimizationproblempresentedinEquation3.25. This
means that ultimately the variational inference problem solved by predictive coding
andtheMAPestimationproblemsolvedbytheKalmanfilterarethesamealthoughthe
interpretationofµ differsslightly–frombeingaparameterofaGaussianvariational
t
distribution versus simplya variable in thegenerativemodel – theactual update rules
involvingµ arethesameinbothcases. Nowweknowthat(linear)predictivecoding
t
andKalmanfilteringsharethesameobjective,wecanpreciselystatetheirdifferences.
WhileKalmanfilteringanalyticallysolvesthisobjectivedirectly, inpredictivecoding,
weinsteadsetthedynamicsoftheparameterstobeagradientdescentonthevariational
freeenergy,whichreducestotheMAPobjectivesolvedbytheKalmanFilter.
Chapter3. PredictiveCoding 110
Forinstance,wecanderivethedynamicswithrespecttothevariationalparametersµ
t+1
which,inneuralprocesstheories,aretypicallyoperationalizedasthe‘activation’units
as,
dL
=2CTΣ y−(CTΣ C+CTΣTC)µ +(Σ +ΣT)µ −2Σ Aµ −2Σ Bu
dµ z z z t+1 x x t+1 x t x t
t+1
=2CTΣ Cµ −2CTΣ Cµ +2Σ µ −2Σ Aµ −2Σ Bu
z t+1 z t+1 x t+1 x t x t
=−CTΣ [y−Cµ ]+Σ [µ −Aµ −Bµ ]
z t+1 x t+1 t t
=−CTΣ ε +Σ ε (3.32)
z z x x
Where ε = y−Cµ and ε = µ −Aµ −Bu .Thus, we can see that the gradient
z t+1 x t+1 t t
perfectlyrecapitulatesthestandardpredictivecodingschemewithprecisionweighted
predictionerrors. Similarly,bytakinggradientswithrespecttotheA,B,andC matrices
of the generative model, we obtain familiar looking update rules which consist of
Hebbianupdaterulesbetweenthepredictionerrorsandthepresynapticactivations
dL d
= [−2µT Σ Aµ +µTATΣ Aµ +µTATΣ Bu +uTBTΣ Aµ ]
dA dA t+1 x t t x t t x t t x t
=−2ΣTµ µT +ΣTAµ µTΣ Aµ µT +Σ Bu µT +ΣTBu µT
x t+1 t x t t x t t x t t x t t
=−Σ [µ −Aµ −Bu ]µT
x t+1 t t t
=−Σ ε µT (3.33)
x x t
AndsimilarlyfortheBmatrix.
dL dL
= [2uTBTΣ Aµ +uTBTΣ Bu −2µT Σ Bu ]
dB dB t x t t x t t+1 x t
=(Σ +ΣT)Bu uT +2Σ Aµ uT −2Σ µ uT
x x t t x t t x t+1 t
=−Σ [µ −Aµ −Bu ]uT
x t+1 t t t
=−Σ ε uT (3.34)
x x t
Chapter3. PredictiveCoding 111
AndtheC observationmatrix.
dL dL
= [−2µT CTRy+µT CTRCµ ]
dC dC t+1 t+1 t+1
=−2RyµT +2RCµ µT
t+1 t+1 t+1
=−R[y−cµ ]µT
t+1 t+1
=−Rε µT (3.35)
y t+1
Chapter3. PredictiveCoding 112
3.4.3 Results
100
80
60
40
20
0
0 200 400 600 800 1000
Timestep
eulaV
Time course of true dynamical variables
1.0
0.8
0.6
0.4
0.2
Position
Velocity
Acceleration 0.0
0 200 400 600 800 1000
Timestep
(a)TrueDynamics
eulaV
Time Course of Control Input
Control Input
(b)ControlInput
0
−50
−100
−150
−200
−250
0 200 400 600 800 1000
Timestep
eulaV
Time course of obser ations (random C matrix)
Obser ed Position
Obser ed Velocity
Obser ed Acceleration
(c)Observations
Figure3.10: Thetruedynamics,controlinput,andobservationsgeneratedbyarandom
Cmatrix. ThesearethesourceoftruththatthepredictivecodingKalmanfiltertriesto
approximate. The observations differ substantially from the true dynamics due to the
random C matrix, which makes the inference problem faced by the predictive coding
filtermuchmorechallenging,sinceitmustde-scrambletheobservationstoinferthetrue
dynamics.
Chapter3. PredictiveCoding 113
We now compare the analytical Kalman filter with our predictive coding algorithm
onasimplefilteringapplication–thatoftrackingthemotionofanacceleratingbody
given only noise sensor measurements. The body is accelerated with an initial high
acceleration that rapidly decays according to an exponential schedule. The filtering
algorithmmustinfertheposition, velocity,andtrueaccelerationofthe bodyfromonly
akinematicdynamics model andnoisysensormeasurements. The body isadditionally
perturbedbywhiteGaussiannoiseinalloftheposition,velocityanddisplacement. The
controlscheduleandthetrueposition,velocityanddisplacementofthebodyareshown
inFigure3.10below.
The analytical Kalman filter was set up as follows. It was provided with the true
kinematicdynamicsmatrix(A)andthetruecontrolmatrix(B),
 
1 dt 1dt2
 2 
 
A=,0 1 dt 
 
 
0 0 1
(cid:20) (cid:21)
B= 0 0 1 (3.36)
TheobservationmatrixCmatrixwasinitializedrandomlywithcoefficientsdrawnfrom
anormaldistributionwith0meanandavarianceof1. Thiseffectivelyrandommapping
of sensory states meant that the filter could not simply obtain the correct estimate
directly but had to disentangle the measurements first. The Q and R matrices of the
analyticalKalmanfilterweresettoconstantdiagonalmatrices,wheretheconstantwas
thestandardvarianceofthenoiseaddedtothesystem.
Chapter3. PredictiveCoding 114
40
30
20
10
0
0 200 400 600 800 1000
Timestep
eulaV
detciderP
Estimated Position
True Value
Kalman Filter
Gradient Method 80
60
40
20
0
0 200 400 600 800 1000
Timestep
(a)Position
eulaV
detciderP
Estimated Velocity
True Value
Kalman Filter
Gradient Method
(b)Velocity
100
80
60
40
20
0
0 200 400 600 800 1000
Timestep
eulaV
detciderP
Estimated Acceleration
True Value
Kalman Filter
Gradient Method
(c)Acceleration
Figure3.11: Trackingperformanceofourgradientfiltercomparedtothetruevaluesand
theanalyticalKalmanFilter.Weshowthetrackingover2000timesteps.
The performance of the analytical Kalman filter which computed updates using equa-
tions 1-3 is compared with that of our neural Kalman filter using gradient descent
dynamics.6 InthiscomparisontheA,B,andCmatricesarefixedtotheircorrectvalues
and only the estimated mean is inferred according to Equation 3.32. Comparisons
are provided for a number of different gradient steps. As can be seen in Figure 3.12,
6The code used for these simulations is freely available and online at https :
//github.com/Bmillidgework/NeuralKalmanFiltering
Chapter3. PredictiveCoding 115
19
18
17
16
15
14
13
0 10 20 30 40 50 60 70 80 90 100
Timestep
eulaV
detciderP
Estimated Position
True Value
Kalman Filter 60
5 Gradient Steps
2 Gradient Steps
58
56
54
52
50
0 10 20 30 40 50 60 70 80 90 100
Timestep
(a)Position
eulaV
detciderP
Estimated Velocity
True Value
Kalman Filter
5 Gradient Steps
2 Gradient Steps
(b)Velocity
101.25
101.00
100.75
100.50
100.25
100.00
99.75
99.50
0 10 20 30 40 50 60 70 80 90 100
Timestep
eulaV
detciderP
Estimated Acceleration
True Value
Kalman Filter
5 Gradient Steps
2 Gradient Steps
(c)Acceleration
Figure3.12: Here,wezoominon100timestepperiodtodemonstratetrackingperfor-
manceinminiatureandtheeffectoffewgradientupdates. Inthiscase,weplottedthe
estimatesafteronly5steps. Eventwostepsoftensuffice(withalargelearningrate)to
provideveryaccurateestimates
Chapter3. PredictiveCoding 116
onlya smallnumber(5) ofgradient descent stepsare requiredtoobtain performance
very closely matching the analytical result. This is likely due to the convexity of the
underlying optimization problem, and meansthat usinggradient descent for"online"
inference is not prohibitively slow. The simulation also shows the estimate for too
few (2) gradient steps for which results are similar, but the estimate may be slightly
smoother.
Next,wedemonstratetheadaptivecapabilitiesofouralgorithm. InFigure3.14,weshow
theperformanceofour algorithminpredictingtheposition, velocity,and acceleration
of the body when provided with a faulty A matrix. Using Equation 3.33, our model
learns the A matrix online via gradient descent. To ensure numerical stability, a very
small learning rate of 10−5 must be used. The entries of the A matrix given to the
algorithmwereinitializedasrandomGaussiannoisewithameanof0andastandard
deviation of 1. The performance of the algorithm without learning the A matrix is
alsoshown,and estimation performanceis completely degradedwithout theadaptive
learning. Thelearningprocessconvergesremarkablyquickly. Itisinteresting,moreover,
tocomparethematrixcoefficientslearnedthroughtheHebbianplasticitytotheknown
truecoefficients. Oftentheydonotmatchthetruevalues,andyetthenetworkisable
to approximate Kalman filtering almost exactly. Precisely how this works is an area
forfutureexploration. Ifasystemsimilartothisisimplementedinthebrain,thenthis
could imply thatthe dynamics model inherentin the synaptic weightmatrix should not
necessarilybeinterpretable.
Wealsoshow(secondrowofFigure3.14)that,perhapssurprisingly,boththeAandB
matrixcanbelearnedsimultaneously. Inthesimulationspresentedbelow, eitheronly
the A matrix, or both the A and the B matrix were initialized with random Gaussian
coefficients, and thenetwork learnedto obtain accurateestimates of thehidden state in
thesecases. 7
7TheresultsofonlylearningtheBmatrixwereextremelysimilarforthatoftheAmatrix.Forconcise-
ness,theresultswerenotincluded. InterestedreadersareencouragedtolookattheNKF B atrix.ipynb
A m
fileintheonlinecodewheretheseexperimentswererun.
Chapter3. PredictiveCoding 117
100
80
60
40
20
0
−20
0 300 600 900 1200 1500
Timestep
eulaV
detciderP
Estimated position
True Value 140
Kalman Filter
Gradient Method 120
No A learning
100
80
60
40
20
0
−20
0 300 600 900 1200 1500
Timestep
(a)PositionforlearntAmatrix
eulaV
detciderP
Estimated velocity
True Value
Kalman Filter
Gradient Method
No A learning
(b)VelocityforlearntAmatrix
100
80
60
40
20
0
0 300 600 900 1200 1500
Timestep
eulaV
detciderP
Estimated acceleration
True Value
Kalman Filter
Gradient Method
No A learning
(c)Filteringperformanceforadaptivelylearning
justtheAmatrix.
WealsotriedadaptivelylearningtheC matrixusingEquation3.35,butallattemptsto
do sofailed. Although the exact reason isunclear, wehypothesise that an incorrectC
matrixcorruptstheobservationswhichprovidestheonly"sourceoftruth"tothesystem.
Ifthedynamicsarecompletelyunknownbutobservationsareknown,thenthetruestate
ofthesystemmustbeatleastapproximatelynearthatimpliedbytheobservations,and
the dynamicscan beinferred from that. On theother hand, ifthe dynamics areknown,
buttheobservationmappingisunknown,thentheactualstateofthesystemcouldbe
Chapter3. PredictiveCoding 118
40
20
0
−20
−40
−60
−80
−100
0 200 400 600 800 1000
Timestep
e
laV
detciderP
Estimated Position
350
Tr e Val e
Kalman Filter
Gradient Method 300
No Learning
250
200
150
100
50
0
0 200 400 600 800 1000
Timestep
(a)Position: learntAandBmatrices
eulaV
detciderP
Estimated Velocity
True Value
Kalman Filter
Gradient Method
No Learning
(b)Velocity: learntAandBmatrices
100
50
0
−50
−100
−150
−200
0 200 400 600 800 1000
Timestep
eulaV
detciderP
Estimated Acceleration
True Value
Kalman Filter
Gradient Method
No Learning
(c)Acceleration: learntAandBmatrices
Figure3.14: Filteringperformance foradaptivelylearningboththeAandB matrices in
concert (second row). The filtering behaviour of the of the randomly initialized filters
withoutadaptivelearningisalsoshown. Importantly,withlearningtheestimatedposition,
velocity, andaccelerationstrack their true valuespreciselywhilethe estimateswithout
learning and just randomly initialized A or A and B matrices rapidly diverge from the
truth.
Chapter3. PredictiveCoding 119
onany ofalarge number ofpossibledynamicaltrajectories, buttheexact specificsof
which are underspecified. Thus the network learns a C matrix which corresponds to
some dynamical trajectory, which succeeds in minimizing the loss function, but which
iscompletelydissimilartotheactualtrajectorythesystemundergoes. Thiscanbeseen
byplottingthelossobtainedaccordingtoEquation3.25inFigure3.15,whichrapidly
decreases,althoughtheestimatedivergesfromthetruevalues.
3.4.4 Discussion
HerewehaveelucidatedthepreciserelationshipbetweenKalmanfiltering–anoptimal
linear Bayesian filtering algorithm – and predictive coding – a neurophysiologically
realistic theory of cortical function. Specifically, that they both optimize the same
objectivefunction–aBayesianMAPfilteringobjective–whiletheKalmanfiltering
solves the resulting optimization problem analytically, predictive coding approaches
derive their dynamics from a gradient descent on the same objective. "This result
provides a new perspectiveon predictive coding; especially if wemake the simplifying
assumptionthattheprecisionsarenotoptimisedwithrespecttovariationalfreeenergy
– or, in the examples above, we assume the conditional covariance is zero. This
reduces variational inference to a MAP optimisation problem. This follows due to
the Laplace approximation, which effectively means the variational precision can be
derived analytically from the expectation (from the curvature of the log likelihood at
theexpectation)–see(Friston&Stephan,2007)fordetails. Alternatively,wecanjust
ignoretheconditionaluncertaintyasintheMAPoptimisationperspective.
Ourworkalsodemonstrateshowstraightforwardlypredictivecodingcanbeappliedto
solveBayesianfilteringproblemsratherthansimplystaticBayesianinferenceproblems.
Sincethebrainisenmeshedincontinuoussensoryexchangewithaconstantlymoving
world, filtering is a much more realistic challenge to solve than pure inference on a
static dataset. It thus seems likely that the neural circuitry dedicated to perception
Chapter3. PredictiveCoding 120
40
30
20
10
0
0 200 400 600 800 1000
Timestep
eulaV
detciderP
Estimated Position
True Value
Kalman Filter 80
Gradient Method
60
40
20
0
−20
−40
−60
0 200 400 600 800 1000
Timestep
(a)Position: learntCmatrix
eulaV
detciderP
Estimated Velocity
True Value
Kalman Filter
Gradient Method
(b)Velocity: learntCmatrix
100
80
60
40
20
0
0 200 400 600 800 1000
Timestep
eulaV
detciderP
Estimated Acceleration
40
30
20
10
True Value
Kalman Filter
Gradient Method 0
0 200 400 600 800 1000
Timestep
(c)Acceleration: learntCmatrix
ssoL
Bayesian loss function
(d)Lossfunctionovertimesteps
Figure3.15: VerypoortrackingbehaviourwithalearntCmatrix. Thisisdespitethefact
that the Bayesian loss function rapidly decreases to a minimum. This shows that the
filter can find a prediction-error minimizing "solution" which almost arbitrarily departs
from reality if the C-matrix is randomized. Panel D shows the loss computed by the
networkwhichrapidlydeclines,evenwhilethepredictionsrapidlydivergefromthetruth
(Panelsa,b,c)
Chapter3. PredictiveCoding 121
is specialized for solving precisely these sorts of filtering problems. Moreover, due
topredictivecoding’sbiologicallyrealisticproperties,ourresultsprovideapowerful
biologicallyplausibleapproachforhowthebrainmightsolvesuchfilteringproblems.
Nevertheless,thereremainseveraldeficienciesofouralgorithm(andpredictivecoding
more generally) in terms of biological plausibility which it is important to state. Our
modelassumesfullconnectivityforthe‘diffuse’connectivityrequiredtoimplement
matrix multiplications. Additionally in other cases it requires one-to-one excitatory
connectivity,bothconstraintswhicharenotfullyupheldinneuralcircuitry. Additionally,
inonecase (thatofthe"Cmatrix"between thepopulationsofneuronsrepresentingthe
estimate andthe sensoryprediction errors), wehave assumeda complete symmetryof
backwardandforwardweights, suchthattheconnectionswhichembodythe Cmatrix
downwards also implement the CT matrix when traversing upwards. This is also a
constraint not satisfied within the brain. Additionally, our model can represent negative
numbers in states or prediction errors, which rate-coded neurons cannot. Several of
these implausibilities will be directly addressed in the context of (static) predictive
codinglaterinthischapter.
Webelieve,however,thatdespitesomelackofbiologicalplausibility,ourmodelisuseful
in that it shows how a standard engineering algorithm can be derived in a way more
amenabletoneuralcomputation,andprovidesasketchathowitcouldbeimplemented
inthebrain. Moreover,wehopetodrawattentiontoBayesianfilteringalgorithmsand
how they can be implemented neurally, instead of just Bayesian inference on static
posteriors.
Finally,whileouralgorithmandexperimentshaveonlyconsideredthelinearcase,it
can be straightforwardly extended to the nonlinear case, where it results in standard
nonlinearpredictivecodingasdiscussedpreviously. Explicitlyandempiricallycompar-
ingtheperformanceofouralgorithmagainstnonlinearextensionstotheKalmanfilter
suchasextendedorunscented(Wan&VanDerMerwe,2000)Kalmanfilteringarean
Chapter3. PredictiveCoding 122
importantandexcitingavenueforfuturework.
3.5 Relaxed Predictive Coding
In the literature predictive coding has been proposed as a general theory of cortical
function (Friston, 2003, 2005, 2008a; Kanai et al., 2015; Spratling, 2008). There is
additionallyasmallliteratureofprocesstheorieswhichtrytotranslatethemathematical
formalism into purported neural circuitry (Bastos et al., 2012; Kanai et al., 2015;
Keller & Mrsic-Flogel, 2018), and some of the predictions of predictive coding have
beenextensivelycomparedandevaluatedagainstneurophysiologicaldata(Aitchison
& Lengyel, 2017; Clark, 2015; Friston, 2008a; Huang & Rao, 2011; Walsh et al.,
2020). Despite thegeneral acceptanceof predictivecoding asa biologicallyplausible
algorithm which could in theory be implemented in the brain, there nevertheless are
severalhighlyimplausibleaspectsofthecorealgorithmthathavebeenlargelyglossed
overintheformulationsoftheprocesstheories,whichfocusedprimarilyonmacro-scale
connectivityconstraintsinsteadoftheprecisemathematicalformofthelearningand
update rulesin thealgorithm (Bastoset al., 2012). Here,we introduce threepotentially
severe biological implausibilities which emerge directly from the form of the predictive
codingalgorithmanddemonstrateempiricallyhow,withsomeingenuityandadaptation
ofthealgorithm,theseimplausibleassumptionscanbe‘relaxed’withoutmajordamage
tothe empiricalperformanceof predictivecoding networksonobject recognitiontasks.
Thisworkisbasedon(Millidge,Tschantz,Seth,&Buckley,2020d)
Recall, thatthe coreof thepredictivecoding formalism isthree keyrelationships. First,
theconceptof predictionerrorasthe differencebetweentheactivityofthe neuronsina
layerandthetop-downpredictionsfromhigherlayers. Second,theupdateruleforthe
activitiesofalayer,whichminimizesboththepredictionerrorsatitsownlayer,aswell
as the layer below. And thirdly, the learning rule for the weights, as a local Hebbian
Chapter3. PredictiveCoding 123
functionofthepredictionerrorsattheirownlayer(Friston,2005).
ε =µ − f(θ µ )
l l l+1 l+1
dµ
l =−ε +θ Tεl−1f(cid:48)(θ µ )
l l l l
dt
dθ
l =εl−1f(cid:48)(θ µ )µ T (3.37)
l l l
dt
where f(cid:48)(θ µ )representsthepartialderivativeofthepost-activationswithrespectto
l l
eithertheµortheθdependingupontheupdaterule. Equation3.37statesthatprediction
errors are computed as a simple subtraction of the value neurons at a layer and the
prediction from the layer above. The vector µ represents the activity of the value
l
neuronsataspecificlevell. Thevectorε isavectoroftheactivityoftheerrorneurons
l
atalevell. Predictionsaremappeddownfromthehigherlayersthroughasetofweights,
denotedW whichisanM×N matrixwhereMis the numberofneuronsatlevell and
Nisthenumberofneuronsatlevell+1. f(x)isanonlinearactivationfunctionapplied
tothe outputsof aneuron and f(cid:48)(x)= ∂f(x) is thepointwisederivative ofthe activation
∂x
function8.
Equation3.8specifiestheupdaterulefortheµ ataspecificlayer. Theupdateisequal
l
tothesumofthepredictionerrorsprojectedupfromthelayerbelow, multiplied bythe
topdownpredictionsandprojectedbackthroughtheweightmatrixandsubtractedfrom
the prediction errors atthe current layer. Thisis a biologically plausible learning rule
as it is a simple sum of multiplication of locally available information. Note: the the
update includesprediction errorterms fromboth the currentlayer and the layer below.
Thisequationiswhyitisnecessarytotransmitpredictionerrorsupwards.
Equation 3.9 is the update rule for the weights θ. This obeys Hebbian plasticity since
itissimplyamultiplicationofthetwoquantitiesavailableateachendofthesynaptic
connection–thepredictionerrorofthelayerbelowandthevalueneuronsatthecurrent
8Herewehavespecializedthepredictivecodingupdaterulessomewhatfromanyarbitraryfunction f
toanelementwisenonlinearityfollowedbyamultiplicationwithaweightmatrixθ. Thisisbecausein
thissectionweconsidertheapplicationofpredictivecodingtotrainartificialneuralnetworkswiththis
specifictypeofstructure
Chapter3. PredictiveCoding 124
layer. The only slight difficulty is the derivative of the nonlinear activation function
oftheprediction. Whilethisinformationislocallyavailableinprinciple,itrequiresa
somewhatmorecomplex neuralarchitectureand itisnot certainthatthederivativesof
activationfunctionscanbecomputedstraightforwardlybyneurons. Luckily,weshow
belowthatthistermisnotneededforsuccessfuloperationofthelearningrule.
Importantly, we additionally recall that predictive coding can be considered to be a
variational inference algorithm, as Equations 3.8 and 3.9 can be directly derived as a
gradientdescentuponthevariationalfreeenergyF =∑ L ε2 which(underGaussian
i=0 i
assumptions)takestheform ofasimplesumof squared predictionerrorsateachlayer.
WeadditionallyignoretheprecisionparametersΣ inthisanalysissinceingeneraltheir
l
biologicalplausibility hasnot been stronglyanalyzed inthe literature (althoughthere
are some speculative suggestions linking them to either lateral connectivity (Friston,
2005)orelsesubcorticalactivityinthepulvinar(Kanaietal.,2015)).
Specifically,wefocusonthreeimportantimplausibilities. Thefirstistheproblemof
weight symmetry, or the required equality of forward and backward weights. This
problem is often called the weight transport problem in the literature (Crick, 1989;
Lillicrap, Cownden, Tweed, & Akerman, 2016; Lillicrap, Santoro, Marris, Akerman,
&Hinton,2020). Thelearningrulesinthesenetworksrequireinformationtobesent
‘backwards’throughthenetwork. Sincesynapticconnectionsaregenerallyassumedto
beuni-directional,inpracticethismeansthatthesebackwardmessagesneedtobesent
throughasecondsetofbackwardsconnectionswiththeexactsamesynapticweights
as the forward connections. Clearly, expecting the brain to have an identical copy
of forward and backward weights is infeasible. Mathematically, this problem arises
fromtheθT terminthe dynamicsequationfortheµs(Equation3.8), sincethisweight
l
transposeuses theforwardweight matrixθbutinstead mapsthe bottom-up prediction
errortothelevelabove,thusrequiringinformationtobesent‘backwards’or‘upwards’
through ‘downwards’ connections. In the brain, this would require information to
Chapter3. PredictiveCoding 125
propagate backwards from the soma of the post-synaptic neuron, back through the
axon and to the soma of the pre-synaptic cell – a possibility which is considered to
beextremelyimplausible(Lillicrap,Cownden,Tweed,&Akerman,2014). Here,we
addressthisprobleminpredictivecodingnetworksbyusingaseparatesetofrandomly
initialized backwards weights trained with a separate Hebbian learning rule, which
also only requires local information. This removes the necessity of beginning with
symmetrical or identical weights and proposes a biologically plausible method of
learninggoodbackwardweightsfromscratchinanunsupervisedfashion. Inthebrain
thiswould beimplementedas areciprocal set of‘backwards’ connectionsgoingfrom
thelower-layerstohigherlayers,whicharedefinitelypresentinthebrain(Grill-Spector
&Malach,2004).
Theweighttransportproblemisalsopresentinneuralimplementationsofthebackprop-
agation of error algorithm from machine learning, and there exists a small literature
addressingit within thiscontext. A keypaper (Lillicrapet al.,2014, 2016) showsthat
simply usingrandom backwardsweights issufficient forsome degreeof learning. This
methodiscalledfeedbackalignment(FA)sinceduringtrainingthefeedforwardweights
learn to align themselves with the random feedback weights so as to transmit useful
gradient information. A variant of this – direct feedback alignment (DFA) (Nøkland,
2016) has been shownthat directforward-backwardconnectivity isnot necessaryfor
successful learning performance. Instead, all layers can receive backwards feedback
directly from the output layer. It has also been shown (Liao, Leibo, & Poggio, 2016)
that performance with random weights is substantially improved if the forward and
backwardconnectionssharemerelythesamesign,whichislessofaconstraintthanthe
exactvalue. Onefurtherpossibilityistolearnthebackwardsweightswithanindepen-
dentlearningrule. ThishasbeenproposedindependentlyinAmit(2019)andAkrout,
Wilson,Humphreys,Lillicrap,andTweed(2019)whoinitializethebackwardsweights
randomly,buttrainthemwithsomelearningrule. Ourworkherediffersprimarilyin
that we show that this learning rule works for predictive coding networks while they
Chapter3. PredictiveCoding 126
only apply it to deep neural networks learnt with backprop. Moreover, our Hebbian
learningrulecanbestraightforwardlyderivedinamathematicallyprincipledmanneras
partoftheoverarchingvariationalframeworkofpredictivecoding.
The second problem is one of backward nonlinear derivatives. In predictive coding
networks (along with backprop), the update and learning rules require the pointwise
derivativesoftheactivationfunctiontobecomputedateachneuron. Mathematically,
thisisthe f(cid:48)(θ µ )term. Forindividualbiologicalneurons,while anonlinearforward
l l
activation function is generally assumed, the ability to compute the derivative of the
activationfunctionisnotknowntobestraightforward. Whileinsomecasesthisissue
can be ameliorated by a judicious choice of activation function – for instance the
pointwise derivative of a rectified linear unit is simply 0 or 1, and is a simple step
function of the firing rate – the problem persists in the general case. Here, we show
that,somewhatsurprisingly,itispossibletosimplyignorethesepointwisederivatives
withrelativelylittleimpactonlearningperformance,despitetheupdaterulesnowbeing
mathematicallyincorrect. Thismayfreethebrainoftheburdenofhavingtocompute
thesequantities.
Athirdissue,specifictoframeworksthatexplicitlyrepresentpredictionerrors,isthe
requirementofone-to-oneconnectionsbetweenactivationunitsandtheircorresponding
errorunits. Whilenotimpossible,thisprecise,one-to-oneconnectivitypatternislikely
difficultfor the brainto createand maintainthroughout developmentand learning. One
possibility,asexploredbySacramento,Costa,Bengio,andSenn(2018)isthatprediction
errorsandpredictionsmaybehousedinseparatedendriticcompartmentsonasingle
neuron,thuspotentiallyobviatingthisissue(althoughtheirschemereliedonanother
set of one-to-one connections between pyramidal cells and inhibitory interneurons).
However, the plausibility of this idea in terms of actual dendritic morphology and
neurophysiologyisunclear. Instead,herewepresentanetwork-levelsolutionandshow
that learning can continue unaffected despite random connectivity patterns between
Chapter3. PredictiveCoding 127
valueanderrorunitsaslongastheseconnectionweightscanalsobelearned. Wepropose
afurtherHebbianandbiologicallyplausiblelearningruletoupdatetheseweightswhich
also only requires local information. Finally, we experiment with combining our
solutionstoall oftheseproblemstogether toproduceafully relaxed predictivecoding
architecture. Importantly,thisarchitecturepossessesasimplebipartitebutotherwise
fullyconnectedconnectivitypatternwithseparatelearnableweightmatricescovering
everyconnection,allofwhichareupdatedwithlocalHebbianlearningrules. Weshow
thatdespitethesimplicityoftheresultingrelaxedscheme,thatitcanstillbetrainedto
high classificationaccuracy comparable with standard predictivecoding networks and
ANNsusingbackpropagation.
3.5.1 Methods
To test the performance of the predictive coding network under various relaxations,
we utilizethe canonical MNISTand FashionMNIST(Xiao, Rasul, & Vollgraf, 2017a)
benchmark datasets. Since this is a supervised classification task, we follow the ap-
proachofWhittingtonandBogacz(2017)andMillidge,Tschantz,andBuckley(2020a),
whoutilizeda‘reverse’predictivecodingarchitecturewheretheinputswerepresented
to the top layer of the network and the labels were predicted at the bottom. This for-
mulationallowsforthestraightforwardrepresentationofsupervisedlearningproblems
inpredictivecoding. Ineffect,thenetworktriestogeneratethelabelfromtheimage.
We utilized a 4-layer predictive coding network consisting of 784, 300, 100, and 10
neuronsineachlayerrespectively. Wetestedbothrectified-linear(relu)andhyperbolic
tangent (tanh) activation functions, which are the most common activation functions
usedinmachinelearning. During training theµswereupdatedfor 100iterationsusing
Equation3.8withboththeinputandlabelsheldfixed. Aftertheiterationsoftheµs,the
remainingpredictionerrorsinthenetworkwereusedtoupdatetheweightsaccordingto
Equation3.9. Attesttime,adigitimagewaspresentedtothenetwork,andthetop-down
predictionsofthenetworkwerepropagateddownwardstoproduceapredictionatthe
Chapter3. PredictiveCoding 128
lowestlayer,whichwascomparedtothetruelabeltoobtainthetestaccuracy.
ThenetworkwastrainedandtestedontheMNISTandFashionMNISTdatasets. MNIST
isadatasetof60,00028×28grayscaleimagesofhandwrittendigitsfrom0to9. The
goalistopredictthedigitfromtheimage. TheFashionMNISTdatasetcontainsimages
ofdifferenttypesofclothing,whichmustbesortedintotenclasses. FashionMNISTis
amorechallengingclassificationdataset,whilealsoservingasadrop-inreplacement
forMNISTsinceitsdataisinexactlythesameformat. Theinputimageswereflattened
into a 784x1 vector before being fed into the network. Labels were represented as
one-hot vectors and were smoothed using a value of 0.1 for the incorrect labels. The
datasetwassplitintoatrainingsetof50000andatestsetof10000images. Allweight
matriceswereinitializedasdrawsfromamultivariateGaussianwithameanof0and
avariance of0.05. Itislikely, giventhelargeliteratureonhow tobestinitializedeep
neuralnetworks,thatthereexistmuchbetterinitializationschemesforpredictivecoding
networksaswell,howeverwedidnotinvestigatethishere. Allresultspresentedwere
averaged over 5 random seeds. We plot error bars around the means as the standard
deviationoftheseeds.
3.5.2 Results
3.5.2.1 WeightTransport
Mathematically, the weight transport problem is caused by the θT term in Equation
3.9. Inneuralcircuitrythisweighttransposecorrespondstotransmittingthemessage
backwards through the same connections or, alternatively, an identical copy of the
backward weights. We wish to replace this copy of the forward weights with an
alternative, unrelated set of weights θ˜. Unlike FA or DFA methods, which simply
userandombackwardsweights,weproposetolearnthebackwardsweightsthrougha
simplesynapticplasticityrule.
dθ
˜l
=µl(f(cid:48)(θlµl)εl−1)T
dt
Chapter3. PredictiveCoding 129
𝜃 , 𝜃 𝜃
𝜇 𝜀 𝜇 𝜇 𝜀 𝜇
𝜇 𝜀 𝜇 𝜇 𝜀 𝜇
θ˜
Figure 3.16: The weighttransport problem andour solution. On the leftis the standard
predictivecodingnetworkarchitecture. Ourdiagramrepresentsthepredictionerrorsεof
one layer receiving predictionsand transmitted predictionerrors to thevalue neurons of
thelayerabove. Predictionerrorsaretransmittedupwardsusingthesameweightmatrix
θT
as the predictions are transmitted downwards. On the right, our solution eschews
thisbiologicalimplausibilitybyproposingaseparatesetofbackwardsweightθ˜
(inred),
whicharelearnedseparatelyusinganadditionalHebbianlearningrule.
ThisruleisHebbiansinceitisjustthemultiplicationoftheactivitiesoftheunitsateach
endofthe connection. Thebackwards pointwisederivative posesaslightproblemin
thatitisfirstmultipliedwiththeerrorsofthelevelbelow. However,asweshowbelow,
pointwisenonlinearderivativesarenotactuallyneededforgoodlearningperformance,
sotheproblemissurmounted. Thisruleissimplythetransposedversionoftheoriginal
weightupdaterule(Equation3.9). Andthus,iftheforwardandbackwardsweightsare
initializedtothesamevalue,barringnumericalerror,theywillstaythesamethroughout
training. Importantly, we demonstrate here that this rule allows rapid and effective
learningoftheweightseveniftheforwardandbackwardsmatricesareinitializedina
completelyindependent(andrandom)fashion. Thismeansthatinthebraintheforwards
andbackwardsconnectionsoriginatecompletely independently, andthat,moreover, if
weaccepttheforwardweightupdateruleasplausible,weshouldacceptthebackwards
weightupdateaswell,thusleadingtonogreaterdemandsofbiologicalplausibilityfor
thisbackwardsweightupdate.
Thisprocedureallowsustobeginwitharandomlyinitializedsetofbackwardsweights,
Chapter3. PredictiveCoding 130
andthen applyingthelearning ruletothese weightsallows ustovery quicklyrecover
performanceequaltotheidenticalbackwardsweights. AsshowninFigure3.17,perfor-
mancebothwithandwithoutthelearntbackwardsweightsisalmostidenticalforboth
thereluandtanhnonlinearitiesandtheMNISTandFashionMNISTdatasets,thussug-
gestingthatthisapproachofsimplylearninganindependentsetofbackwardsweights
isahighlyeffectiveandrobustmethodfortacklingtheweighttransportproblem.
(a)MNISTdataset;tanhactivation (b)MNISTdataset;relu
(c)Fashiondataset;tanh (d)Fashiondataset;relu
Figure 3.17: Test accuracy of predictive coding networks with both learnt backwards
weights, and the ideal weight transposes with both relu and tanh activation functions
on the MNIST and FashionMNIST datasets. Both networks obtain almost identical
learning curves, thus suggesting that learnt backwards weights allow for a solution to
theweight-transportproblem.
Chapter3. PredictiveCoding 131
3.5.2.2 Backwardsnonlinearderivatives
The second remaining biological implausibility is that of the backwards nonlinear
derivatives. Note that in Equations 3.37, an f(cid:48) term regularly appears denoting the
pointwise derivative of the nonlinear activation function. Since these are pointwise
derivatives,whenthemathematicsistranslatedtoneuralcircuitry,thesederivativesneed
tobecomputedateachindividualneuron. Itisnotclearwhetherneuronsarecapable
of easily computing with the derivative of their own activation function. We apply
a straightforward remedy to address this. We simply experiment with removing the
pointwisederivativesfromtheupdaterules. ForinstanceEquation3.8wouldbecome
just dµl =−εl+θlT εl−1. Perhapssurprisinglywefoundthatthismodification,although
dt
notmathematicallycorrect,didlittletoimpairperformanceofthemodelatclassification
tasks, except perhaps for the hyperbolic tangent nonlinearity in the FashionMNIST
case.
Ineffect, byremoving thepointwise nonlinearderivatives, wehave madethegradient
updateslinear inthe parameters. Since thereal updatesare nonlinear,our updaterules
aresimplytheprojectionofthenonlinearupdaterulesontoalinearsubspace. However,
usingasimilarargumenttothatinfeedbackalignment,wehypothesizethatitislikely
thatthelinearprojectionofthenonlinearupdatesarequitecloseinangletothenonlinear
updates, sothe directionof the linear gradient, averaged overmany batchesand update
steps, is sufficiently close to the true gradient as to allow for learning in this model.
Analternativeoptionistonotethatthederivativesofthenonlinearitydependclosely
upon the value of the function. In the case of the relu nonlinearity, the derivative is
only different from 1 when the activity is 0 (the neuron does not fire). When there is
nofiring,therecanstillbeupdatestothedynamics–whichdependontheprediction
errors and not on the actual firing rate – and so there is still error when dropping the
derivative term. However, if most activations are greater than 0, the error should be
minimal, which is what we appear to observe. Similarly, in the hyperbolic tangent
Chapter3. PredictiveCoding 132
(a)MNISTdataset;tanhactivation (b)MNISTdataset;reluactivation
(c)Fashiondataset;tanhactivation (d)Fashiondataset;reluactivation
Figure3.18: Testaccuracyofpredictivecodingnetworkswithandwithoutthenonlinear
derivative term, using relu and tanh activation functions on the MNIST and Fashion-
MNIST datasets. We find that on the MNIST dataset performance is similar, while on
the FashionMNIST dataset and the tanh activation function, the lack of the nonlinear
derivativeappearstoslightlyhurtperformance.
nonlinearity, the region of activation between −1 and 1 is broadly linear, and thus
we should expect the dropping of the nonlinear derivative term in this region to have
relativelylittleeffect. Therobustnessandrelativelylittleimpactontrainingtherefore
suggestthattheactivationsofthepredictivecodingnetworklargelyremainwithinthis
stable regime over the course of training – an intriguing and important finding given
thatwemadenoefforts(suchasregularization)tospecificallyencouragethisoutcome.
If brains operated in a similar regime, it may mean that explicit computation of the
activityderivativesisunnecessary,whichwouldmakecreditassignmentsubstantially
easier(ifapproximate).
Chapter3. PredictiveCoding 133
𝜇 𝜇 𝜇 𝜇
𝜓
𝜀 𝜀 𝜀 𝜀
Figure3.19: Theerror-connectivityproblemandoursolution. Ontheleft,thebiologically
implausible one-to-one connectivity between value and error nodes required by the
standard predictive coding theory. On the right, our solution to replace these one to
one connections by a fully connected connectivity matrix ψ. By learning ψ with a
Hebbianlearningruleweareabletoachievecomparableperformancetotheone-to-one
connectionswithafullydispersedconnectivitymatrix.
3.5.2.3 Errorconnections
Thethirdandfinalbiologicalimplausibilitythatweaddressinthissectionisthatofthe
one-to-oneconnectionsbetweenvalueandtheerrorunitsatagivenlayer. Thiscanbe
seendirectlyforthepredictionerrorsinEquation3.37,butbrokendownintoindividual
components(orneurons).
εl =µl− f(∑θl+1µl+1) (3.38)
i i i,j j
j
Weseethattheactivityoftheerrorunitvector(i.e. eacherrorneuronε)isdrivenbya
i
one-to-one connection from its matching value neuron µ. By contrast, the top-down
i
predictions have a diffuse connectivity pattern, where every value neuron µ in the
j
layeraboveaffectseacherrorneuronε throughthesynapticweightθ . Aone-to-one
i i,j
connectivitystructureisahighlypreciseandsensitivepatternanditisdifficulttosee
howitcouldfirstdevelopandthenbemaintainedinthebrainthroughoutthecourseof
an organismslife. Additionally, while preciseconnectivity can exist intheory, there is
littleevidenceneurophysiologically(Bastosetal.,2012;Walshetal.,2020)forthekind
Chapter3. PredictiveCoding 134
ofregular and repeatableone-to-oneconnectivitypatterns thatpredictivecodingwould
requireinthebrain. Moreover,ifpredictivecodingwereimplementedthroughoutthe
cortex, this one-to-one connectivity should be highly visible to neuroscientists. To
relaxthisone-to-oneconnectivityconstraint,wepostulateadiffuseconnectivitypattern
between them, mediated by a set of connection weights ψ. The new equation for the
predictionerrorsbecomes:
εl =∑ψl µl − f(∑θl+1µl+1)
i i,k k i,j j
k j
=⇒ εl =ψlµl− f(θl+1µl+1) (3.39)
Whileusingrandomlyinitializedweightsψcompletelydestroyslearningperformance,
it is possible to learn these weights in an online unsupervised fashion using another
Hebbian learning rule. The learning rule for the error weights ψ can be derived as a
gradientdescentonthevariationalfreeenergyfunction,wherebynowtheprediction
errorsincludetheerrorweights,
dψl ∂F ∂ε
l
=− =−ε
l
dt ∂ψ ∂ψ
=−ε µT (3.40)
l l
This rule is completely linear and Hebbian since it is simply a multiplication of the
activationsatthetwoendpointsoftheconnection. WeshowinFigure3.20thatusing
thisruleallowsequivalentlearningperformancetotheone-to-onecaseastherequired
weightvaluesarerapidlylearnt.
We see that, overall, training performance can be maintained even with learnt error
connections, a perhaps surprising result given how key the prediction errors are in
drivinglearning. Interestingly,weseeastrongeffectofactivationfunctiononperfor-
mance. Performanceisindistinguishablefrombaselinewithareluactivationfunction
butasymptotesataslightlylowervaluethanthebaselinewithtanh. Investigatingthe
reasonforthebetterperformanceoftherelunonlinearitywouldbeaninterestingtask
forfuturework.
Chapter3. PredictiveCoding 135
(a)MNISTdataset;tanhactivation (b)MNISTdataset;reluactivation
(c)Fashiondataset;tanhactivation (d)Fashiondataset;reluactivation
Figure3.20: Testaccuracyofpredictivecodingnetworkswithandwithoutlearnableerror
connectionsforbothreluandtanhactivationfunctionsontheMNISTandFashionMNIST
datasets. We see that, interestingly, using learnterror weights decreasedperformance
onlywiththetanhbutnottherelunonlinearity,andthenonlyslightlyintheFashionMNIST
case.
3.5.2.4 CombiningRelaxations
Itisalsopossibletocombinealloftheaboverelaxationstogetherinparallel,tocreatea
networkarchitecturewhichisavoidsmanyofthemajorbiologicalplausibilitypitfallsof
predictivecoding. Aschematicrepresentationofthiscombinedarchitecturecompared
tothestandardpredictivecodingarchitectureisshownbelow(Figure3.21).
Chapter3. PredictiveCoding 136
(a)StandardPC (b)RelaxedPC
Figure3.21: Schematicrepresentationsofthearchitectureacrosstwolayersofa.) the
standardpredictivecodingarchitectureandb.) Thefullyrelaxedarchitecture. Importantly,
thisarchitecturehasfullconnectivitybetweenallnodesandalsonon-symmetricforward
and backwards connectivity in all cases. In effect, this architecture only maintains a
bipartitegraphbetweenerrorandvalueneurons,butnootherclearstructure
The relaxed architecture no longer has any one-to-one connectivity patterns, which
havebeenreplacedwithfullconnectivitymatricesparametrisedbytheerrorconnection
weights ψ. Moreover, the forwards and backwards weights are separated into two
separateandindependentweightmatricesθandθ˜ whilethestandardpredictivecoding
model uses the true weight transpose θT, which requires copying the weights. In
essence,thefullyrelaxedarchitecturesimplyconsistsoftwobipartitepopulationsof
neurons, whichonlysynapse ontothe otherpopulation. Beyondthisthere isno special
connectivity structure required. Nevertheless, we show that even this very simple
architecture, with only Hebbian learning rules, can still be trained to perform well at
supervisedclassification.
Wetestedtheclassificationabilityofthefullyrelaxedarchitecturewithbothhyperbolic
tangent andrectified linearactivation functions, andon the MNISTand FashionMNIST
datasets, andthe resultsare shown inFigure 3.22. Overallwe found thatanother strong
effectofactivationfunctionwherethistimetrainingwasunstableanddivergedwhen
usingrectifiedlinear unitsbutnotwhen usingtanhneurons. Wehypothesizethat this
couldbeduetotherectifiedlinearunitsnothavingasaturationpointunliketanhand
Chapter3. PredictiveCoding 137
(a)MNISTdataset;tanhactivation (b)MNISTdataset;reluactivation
(c)Fashiondataset;tanhactivation (d)Fashiondataset;reluactivation
Figure3.22: Testaccuracystandardandfullyrelaxedpredictivecodingnetworks(the
combined algorithm), for both relu and tanh activation functions on the MNIST and
FashionMNIST datasets. We see that, interestingly,performance is degraded in all
casesand thattherelunetworks areespeciallyaffected– withcatastrophicdeclinesin
performancetobecomealmostrandom. Thereasonsforthisarecurrentlyunknownand
willbeinvestigatedinfuturework.
thusbeingmorepronetoexplodinggradients. Wefound,ontheotherhand,thatwhile
performance of the fully relaxed network asymptotically tended to be slightly worse
thanthestandard,itwasstillveryhighonboththeMNISTandfashionMNISTdatasets,
thusshowingthat even highlyrelaxedandextremelylocal networks withanextremely
generic, essentially fully connected, connectivity pattern can be trained to very high
accuraciesusingthispredictivecodingalgorithmwhichonlyrequiresHebbianupdates.
Chapter3. PredictiveCoding 138
3.5.3 Discussion
We have shown that it is possible to surmount three key biological implausibilities
of the predictive coding learning rules, and thus strengthen the case that predictive
coding may be implemented in cortical circuitry. In the weight transport and error
connections case, the solution has been to propose a separate set of weights which
are themselves learnable by a Hebbian rule. In the backwards nonlinearities case, it
suffices simply to ignore the biologically implausible terms in the rule. Moreover,
we have shown that performance, at least under the hyperbolic tangent nonlinearity,
is still stable and roughly comparable with the baseline when all the relaxations are
combinedtogether,thusresultinginanextremelylocal,straightforward,andbiologically
plausible architecture. Overall, we believe these results show that predictive coding
offers a surprisingly robust model of learning and inference in the brain and that it
can survive often severe perturbations to its basic equations. Through this work we
have substantially diminished theconstraints a neurophysiologicallyrealistic process
theory of predictive coding must satisfy. In so doing, it is possible that predictive
coding may now fit a greater part of neurophysiological data, while opening the way
topotentiallyconstructingnovelmicrocircuitdesignswhichimplementrelaxedforms
ofpredictivecoding. Theremay alsobe gainsfrom applyingthese heuristics toother
biologically plausible approximations to backprop. We show in the final chapter on
biologicallyplausiblecreditassignmentinthebrain,thatmanyofthesetechniquesalso
workforotheralgorithms,thushintingatpotentiallygeneralpropertiesofperturbational
robustnessofthese neurallyinspiredlearningalgorithmswhichmaybe ofsignificant
theoreticalinterest.
Anadditionaltheoreticalnoteisthe poweroftheassumptionofvariationaloptimality.
LikeaLagrangianinphysics,thevariationalfreeenergyF enablespotentiallycomplex
‘laws of motion’ to be derived through a simple mathematical apparatus and which
canbeextendedtoleadtootherwise-difficultinsights. Forinstance,thelearningrules
Chapter3. PredictiveCoding 139
fortheerror-weightsψcanbederivedstraightforwardlyinthevariationalframework
as simple gradient descents on F given an augmented generative model containing
theψterm. Regardlessofone’stheoreticalorontologicalcommitmentstovariational
inference in the brain, the mathematical reformulation of neural activity as encoding
solutions toa variational inference problem allows considerablemodelling flexibility
andmathematicalinsightthroughwhichresultscanbeeasilyderivedwhichwouldbe
muchhardertoachievethroughothermeans.
Havingremovedthesymmetricbackwardsweightsandtheone-to-oneerrorconnectivity
scheme, we are leftwith an essentially bipartitegraph. There are connectionsbetween
the value and error units of the same level, and the error units of one level and the
value units of the level above, but crucially there are no direct connections between
the value or error units of one layer and those of the layer above. Stepping out of
the predictive coding framework, we have effectively shown that a simple bipartite
connectivitystructureandHebbianlearningrulessufficestolearncomplexinput-output
mappings,andmaymathematicallyapproximatebackpropagation. Thisisasurprising
result given that previously Hebbian learning has not generally been thought to be
sufficienttolearncomplexrepresentationsinthebrain(Baldi&Sadowski,2016). This
shows that perhaps it is possible for the brain to go further with clever connectivity
patternsandHebbianlearningthanpreviouslythought.
Itisalsoimportanttonoteherethatwhilewehaveresolvedseveralbiologicalimplausi-
bilitiesofpredictivecoding,therearestillseveralotherdifficultiesthatmustbefaced
beforeadirect implementation ofpredictivecoding ispossiblegiven whatwecurrently
knowaboutneuralcircuitry. Akeychallengeisthesimpleproblemofnegativepredic-
tionerrorsandactivations–inthemathematicalformalism,predictionerrorsandvalues
arerealnumberswhichcanbebothpositiveandnegative,andtoanydegreeofaccuracy.
Inthebrain, however,weassumethatthese numbers arerepresentedbyaveragefiring
rates, which cannot go negative, and additionally have a degree of accuracy constrained
Chapter3. PredictiveCoding 140
bytheintrinsicnoiselevelsofthebrainandtheintegrationwindowsoverwhichpost-
synaptic neurons can listen. While it seems likely that a lack of numerical accuracy
is not that significant for neural networks in general, given recent results in machine
learningdemonstrating thatonly 16bit floatsare necessaryat most(Gupta, Agrawal,
Gopalakrishnan,&Narayanan,2015),theissueofnegativenumbersissubstantialsince
negativepredictionerrorsareabsolutelynecessaryforthefunctioningofthealgorithm.
One possibility is that the brain could maintain a high default firing rate, and treat
deviations belowthis defaultasnegative. However, themaintenanceofa sufficiently
high default firing rate would be energy inefficient, and there is much evidence that
neuronsprimarilymaintainlowtonicfiringrates(Walshetal.,2020). Anotheroption
could be to utilize separate populations of ‘positive’ and ‘negative’ neurons, perhaps
excitatory and inhibitory neurons, however this would require a precise connectivity
scheme to integrate these two contributions together, which has largely not yet been
workedoutinthecontextofpredictivecoding.
An additional limitation of our work is that we have only tested the performance of
the relaxations on relatively small networks and using the relatively simple MNIST
and Fashion-MNIST datasets which are simple enough that even fairly non-scalable
methods can work on them. A prime example of this is the study by Bartunov et al.
(2018)whoshowedthatmanyofthemethodsintheliteratureforalternativebiologically
plausiblemethodsforcreditassignment,althoughperformingwellonMNIST,generally
performed poorly on more challenging datasets such as CIFAR10, CIFAR100, and
ImageNet. Akeytaskmustbetoinvestigatethescalingpropertiesoftheserelaxations
to morechallenging tasksand datasets,as wellas differentnetwork architecturessuch
asconvolutionalneural networks. Some preliminary, butsupportiveresults comefrom
Millidge,Tschantz,Seth,andBuckley(2020a)(discussedinchapter6)whereweshow
thatthelearnablebackwardsweightsanddroppingthenonlinearderivativesdoinfact
scaletolargerscaleCNNnetworks.
Chapter3. PredictiveCoding 141
3.6 Conclusion
Inthischapter,wehavestudiedtheapplicationofthefreeenergyprincipletoperception
– specifically by investigating and proposing substantial extensions to the predictive
codingprocesstheory(Friston,2003,2005,2008a)aswellastestingtheperformance
oflarge-scaleimplementationsofthetheory. Overall,webelievethatourworkinthis
chapter has make substantial improvements to the theory and practice of predictive
coding.
Specifically, for the first time, we have implemented and tested predictive coding
modelsonalargerscalethanpreviously–andhavecomparedthemagainstmachine
learningapproachesonstandardmachinelearningdatasetssuchasMNIST.Wehave
demonstratedthatpredictivecodingnetworksareabletosuccessfullyreconstructdigits
successfully,interpolatebetweenthem,andseparateoutdifferentdigitrepresentations
inthe learntlatentspace despitebeing trained withan entirelyunsupervisedobjective.
Moreover,wehavedemonstratedthatpredictivecodingcanachievethisinhierarchical
anddynamicalsetupswithrandomizedinitialweightswhicharethenlearned,incontrast
topriorwork(Friston,2005,2008a;Fristonetal.,2008)whichprimarilyfocusonthe
inference capabilities of predictive codingand provide a-priorithe correct generative
model. Wealsoimplementedanddemonstratedthatdynamical,andbothhierarchical
anddynamicalpredictivecodingmodelscanfunctionwellonsimpletoytasksandcan
veryquicklylearnvariouschallengingwave-forms.
Secondly,wehaveinvestigatedtheuseofpredictivecodingalgorithmsforfilteringtasks
(as opposed to static inference). In filtering, the aim is to infer an entire trajectory of
statesgivenatrajectoryofobservations,insteadofsimplyinferringasinglehiddenstate
givenasingleobservation. Wemakeprecise,forthefirsttime,thepreciserelationship
between Kalman filtering – a ubiquitous algorithm in classical control and filtering
theory – and predictive coding which is that the predictive coding dynamics can be
derivedasagradientdescentontheGaussianmaximum-a-posterioriobjective,which
Chapter3. PredictiveCoding 142
theKalmanfilterequationscanbederivedasananalyticalsolutionto. Wethendemon-
stratethesuccessfulfilteringcapabilitiesofourpredictivecodingfilteringalgorithm,
and demonstrate how the broader variational approach allows us to successfully also
learntheparametersofthegenerativemodelonlineforfilteringtasks–thusperforming
doubledeconvolutionwhereweinferbothstatesandparameterssimultaneously.
Finally,wehaveinvestigatedandimprovethebiologicalplausibilityofthepredictive
coding process theory which, after all, is often explicitly proposed as a neuroscientific
theory of cortical function (Bastos et al., 2012; Friston, 2003). Here we focus on
andpresentsolutionstothreeoutstandingissuesofbiologicalimplausibilitywiththe
standardpredictivecodingdynamics. First,weaddresstheweighttransportproblem–
theneedtotransmitactivitiesbackwards–byproposingasetofindependentbackwards
weightswhichareinitialized randomly, andthencanalsobelearntwith anindependent
and biologicallyplausible Hebbian update rule. Secondly, we address theproblem of
nonlinear derivatives by showing that in manycases these derivatives can be dropped
from the update rules with relatively small performance penalties, and thirdly, we
addresstheissueofneedingpreciseone-to-oneerrortovalueneuronconnectivityby
proposinginsteadfullydistributedconnectivitybetweenerrorand valueneurons,but
withanadditionallearnableweightmatrixwhichcanbeadditionallyoptimizedwith
another Hebbian rule. We show that these relaxations can substantially improve the
biologicalplausibilityofthepredictivecodingalgorithmwhileonlycausingrelatively
smalldegradationsofperformanceonmachinelearningbenchmarkclassificationtasks.
Overall, therefore, we believe that in this chapter we have made significant contri-
butions to the theory and practice of predictive coding. On the theoretical level, we
havedemonstrated itsrelationshiptoKalman filtering, andwehaveaddressedseveral
outstandingchallengesofbiologicalimplausibilitythatthestandardtheoryfaces. Onan
implementationalandpracticallevel,wehaveempiricallyinvestigatedfor thefirsttime
theperformanceofpredictivecodingnetworkswithinthemachinelearningparadigm
Chapter3. PredictiveCoding 143
onmachinelearningbenchmarktasks,andespeciallyincaseswherethetruegenerative
modelis notprovidedto thenetwork a-priori. Additionally,through ourwork, wehave
substantiallyscaleduppredictivecodingapproachestohandlesignificantlylargerand
morechallengingtasksthanpreviously,andhavemadetheseimplementationsavailable
tothecommunitythroughanumberofopen-sourcesoftwareprojects9
In the next two chapters, we advance from the problem of perception, to consider
the problem of action selection through the lens of the free energy principle, and
its concomitant process theory active inference. In some ways this problem is more
challengingthanpureperception,sinceitrequiresthemodellingofentiretrajectoriesof
observations,states,andactions,inordertomakethebestlongtermdecisionswhich
will, over time, outperform locally greedy options. In the next Chapter (Chapter 4),
we focus primarily on scaling up existing active inference models using deep neural
networkstomatchtheperformanceofstateoftheartreinforcementlearningalgorithms.
Inthechapterafterthat(Chapter5),weaimtoprovideadeepmathematicalinvestigation
and, ultimately, insight into the nature of objective functionals which combine both
reward-seekingandinformation-seekingimperatives.
9See: https://github.com/BerenMillidge/PredictiveCodingBackprop,
https://github.com/BerenMillidge/RelaxedPredictiveCoding,https://github.com/BerenMillidge/NeuralKalmanFiltering
Chapter 4
Scaling Active Inference
4.1 Introduction
In this chapter, we consider the application of the free energy principle to action
selection,orcontrol,problems. Whileinthepreviouschapteronperception,wefocused
ontheprocesstheoryofpredictivecoding,herewefocusontheprocesstheoryofactive
inference, and are especially inspired by thediscrete state-space active inference theory
introducedinChapter2. Here,weaimtosolveakeylimitationofthosemethods–their
scalability. We propose to do so by parametrizing the key densities of the generative
model and recognition distribution by deep neural networks, and then utilizing the
tools ofdeep reinforcementlearning to allowactive inference agentsto scale tolevels
comparablyachievedbycontemporarymachinelearning.
This chapter comprises multiple sections. At the beginning, we give a detailed in-
troduction to reinforcement learning (Sutton & Barto, 2018), and especially deep
reinforcementlearning,aswellasthecontrolofinferenceframework(Levine,2018;
K.C.Rawlik,2013)fromreinforcementlearningwhichalsoframesthecontrolproblem
as one of inference. Then we present two studies where we demonstrate that active
inferenceapproachescanscaleuptobecomparablewithcontemporarydeepreinforce-
144
Chapter4. ScalingActiveInference 145
mentmethods. Weachievethisscalingbyfirstparametrizingthekeydistributionsin
the active inference model bydeep neural networks trainedthrough gradient descent,
andsecondlybyapproximatingthe(exponentialtime)computationofthepathintegral
ofthe expected freeenergyeither withanamortized neural networkprediction, orelse
through monte-carlotrajectory sampling usinga continuous actionplanning algorithm.
Indoingso,wecreatealgorithmsthatarecomparable in scalability andperformance
tocurrentmethodsindeepreinforcementlearning. Additionally,wedemonstratethat
oftentimesthesealgorithmscanoutperformtheirreinforcementlearningcounterparts
duetotheuniquepropertiesandinsightsactiveinferencebringstothetable.
In the second section of this chapter, we focus a little more abstractly in trying to
understand the difference between model-free and model-based reinforcement learn-
ing approaches in terms of inference, and determine that this difference is primarily
due to the difference between what we call iterative variational inference – where
the parameters of the variational distribution are directly optimized – and amortized
inference – where instead the parameters of a function which outputs the parameters
of the variational distribution are optimized. Given this distinction, we first use it to
presentataxonomyofawiderangeofcurrentreinforcementlearningalgorithmsusing
asimpletwo-dimensionalquadrant,andsecondly,wederivenovelalgorithmswhich
emergebycombiningbothiterativeandamortizedinferencetogether–anapproachwe
callhybrid inference–whichresultsinpowerfulalgorithmswhichcombinethebenefits
ofeachapproach,whileamelioratingtheirrespectiveweaknesses.
4.1.1 Reinforcement Learning
Thereinforcementlearning,orcontrol,problemisoneofthemostfundamentalprob-
lems in artificial intelligence and in engineering adaptive systems (Dayan & Hinton,
1997;Kaelblingetal.,1996;Sutton,1990;Sutton,Barto,etal.,1998),andconcernsthe
computationofoptimalaction(Todorov,2008;Wolpert,1997). Theproblemissimple.
Chapter4. ScalingActiveInference 146
Weassumethatthereissomekind ofagentinsomekindofenvironment,andthatthe
agentcantakeactionswhichaffecttheenvironment(Suttonetal.,1998). Supposethat
theagenthassomekindofnotionofgoalsordesiresthatitwantstoachieve–whether
theseareencodedasadesiredistribution,asanobjectivefunction,orasrewardsgiven
bytheenvironment. Thecontrolproblemistocomputetheoptimalactionscheduleto
fulfilltheagent’sgoals. Asmightbeexpected,thisquestionhashugeapplicationsand
implicationsforanextremelywiderangeoffields,frommachinelearningandartificial
intelligence (understanding how to make artificial agents act to achieve their goals)
(V.Mnihetal.,2013;Schrittwieseretal.,2019;Schulman,Levine,Abbeel,Jordan,&
Moritz,2015;Silveretal.,2016;Suttonetal.,1998)tocognitivescienceandeconomics
(understandinghowhumansimplementactionstrategiestoachievetheirgoals)(Daw,
O’doherty, Dayan, Seymour, & Dolan, 2006; Dayan & Daw, 2008; Todorov, 2008;
Wolpert,1997)tobiology(howdoallsortsofbiologicalsystemsactadaptively)(Dayan,
2009;Krebs,Kacelnik,&Taylor,1978;Mehlhornetal.,2015;Pyke,1984)tocontrol
theory(howtodesignandprogramsystemswhichcanadaptivelyregulateandcontrol
their environments) (Johnson & Moradi, 2005; Kalman et al., 1960; Kappen, 2005;
Kirk,2004;Kwakernaak&Sivan,1972;Sethi&Thompson,2000).
While this provides an intuitive specification of the control problem, to make real
progresswemustmakeitprecisemathematically. First,weassumethattheenvironment
has states which we denote x and that the agent can emit actions a. Secondly, we
assume that there exists some reward function which emits rewards r dependent on
environmentalstatesr= f(x). Theonlythingtheagenthascontroloverareitsactions
awhichcan affect theenvironmentto giveit morerewards. We assumethattheagent
optimizesover trajectoriesofstatesandactionsgoingintothefuture,whichwedenote
as x˜, r˜, and a˜. For the moment, to retain full generality, we remain indifferent to
(cid:82)
whethertheagentconsiderstimecontinuousordiscrete,sothatx˜=∑ x(t)= dtx(t).
t
Finally, we assume that the environmental dynamics and the rewards granted can
bothbestochasticandcanthusbemathematicallyformalizedintermsofprobability
Chapter4. ScalingActiveInference 147
distributions p(x˜|a˜)and p(r˜|x˜). Akeyadvantageofthisprobabilisticformalismisthat
it allows us to represent (and remain agnostic between) intrinsic stochasticity in the
environment,and the agent’s uncertaintyabout theenvironment. If theenvironmentor
rewardsareinfactdeterministicandknown,wecansimplysetthedistributionstobe
diracdeltastorecover adeterministicframework. Underthisformalism, theobjective
ofthecontrolproblemistomaximize1,
(cid:90)
L =argmax dx˜p(r˜|x˜,a˜)p(x˜|a˜)p(a˜)
control
a
=argmaxE [lnp(r˜|x˜,a˜)] (4.1)
p(x˜|a˜)p(a˜)
a
Where we can safely take the log of the reward function since log is a monotonic
function and does not impact the optimum of the optimization process, but tends to
makethingsnicernumerically. Essentially,whatthisstatesisthatthecontrolobjectiveis
simplytomaximizetheprobabilityoramountofrewardexpectedunderthetrajectoryof
environment statesgiven theagent’s trajectory ofactions. From this,wecansee thatto
firstgetahandleonthecontrolproblem,weneedtounderstandfirstlytheenvironmental
dynamics p(x˜|a˜) and the reward or utility function p(r˜|x˜,a˜). We typically represent
thesedynamicsasstochasticdifferential(ordifference)equationsdependingonwhether
timeisdiscreteorcontinuous,asfollows,
dx
= f(x ,a ,ω) (4.2)
1:t 1:t
dt
forcontinuoustime,whereωissomekindofnoiseor,
x = f(x ,a ,ω) (4.3)
t+1 1:T 1:T
fordiscretetime. Onesimplifyingassumptionsweoftenmakeisthatthedynamicsare
Markovian,meaning thatthestate attimet+1canbe computedsolelyin termsofthe
1Here, followingtheconventioninreinforcementlearningandeconomics, weareoptimisticand
wetalkaboutreward(orequivalentlyutility)maximization. Controltheory,ontheotherhand,takes
amoredepressiveinterpretationandworksintermsofminimizingcosts. Mathematically,thesetwo
formulationsarecompletelyequivalent.
Chapter4. ScalingActiveInference 148
stateattimet andtheactionattimet,thusthatthedynamicsbecome,
x = f(x ,a ,ω) (4.4)
t+1 t t
Thisapproachsimplifiestheanalysisconsiderably. Anadditionalassumption,which
is often made, is that the rewards depend only on the state and actions at the current
time p(r˜|x˜,a˜)=Π p(r(t)|s(t),a(t)). Underthese assumptions the environment of the
t
controlproblem canbeconsideredto beaMarkovDecisionProcess(MDP). Itisalso
sometimesthecasethatweassumewedonotknowthetruestateoftheenvironment,
but are only given access to partial observations o˜ which may not be Markov, even
thoughthehiddenstatesareMarkov. Theobservationsarerelatedtothestatesthrough
a likelihoodmapping p(o˜|x˜). Thistype of environment is called a Partially-Observed
MarkovDecisionProcess(POMDP)(Kaelblingetal.,1996)andissubstantiallyharder
to solve optimally than an MDP due to the need to correctly infer the hidden states x˜
fromtheobservationso˜. Nevertheless,thePOMDPmodelhasagreatdealofgenerality
since, as the state is hidden, it can be whatever is necessary to preserve Markovian
dynamics, thus enabling any non-Markovian environment to be written in terms of a
MarkovianPOMDP.
Early approaches to the control problem tried to use methods in variational calculus
to directly find analytical solutions to the control problem. Such approaches yielded
successinsomesimplebutimportant,cases,suchasMarkovlinearGaussiandynamics
andquadraticcosts. Theseconditionscorrespondtodynamicswhichcanbespecified
as(assumingdiscretetime),
x =Ax +Ba +ω
t+1 t t
r =xTQx +aTRa (4.5)
t t t t t
WhereA,B,Q,andR,areknownmatricesandω∼N(0,I)iswhiteGaussianWiener
noise(Wiener,2019). Inthiscase,ananalyticalsolutionexistsinbothcontinuousand
discretetimewhichgivesrisetothelinearquadraticregulator(Kalman,1960; Kalman
Chapter4. ScalingActiveInference 149
etal.,1960;Kirk,2004),acenterpieceofmoderncontroltheorywhichisremarkably
effective for controlling even complex systems, and for which control solutions can
becomputedveryrelativelycheaplyandinrealtime. Whilethelineardynamicsand
quadratic costs conditions are quite restrictive (especially the linear dynamics), the
linearquadraticregulatorapproachcanbeextendedsomewhattononlineardynamics
bysimplyusingalocallinearityapproximationateverytimestepandapplyingmodel-
predictive control. This iterative LQR (W. Li & Todorov, 2004) algorithm, is quite
robustandcanachievesignificantfeatsofnonlinearcontrol,includinguseincontrolling
industrial robotics (Feng, Whitman, Xinjilefu, & Atkeson, 2014). Other variational
approaches have also been applied and can be quite effective in many cases. For
instance,Pontryagin’smaximimumprinciple(Kirk,2004;Kopp,1962)or‘bang-bang’
control canbe appliedproductively tofind optimal policiesin manysettings. Recently,
there has been advances using path integral methods and the Feynman-Kac lemma to
findcontrolsolutionsforcertainclassesofnonlineardynamics(Kappen,2005;Kappen
etal.,2012;G.Williams,Wagener,etal.,2017).
Another approach to the control problem, which can work for arbitrary dynamics, is
tosimply optimize thecontrolfunction bygradientdescentwith respecttothe actions.
Suchan approachgoesbythe nameofpolicy gradients,since givena policyfunction
a = f (s) parametrised by parameters φ, we can simply compute gradients of the
φ
controlproblemloss
∂L
control andoptimizetheparametersbystochasticgradientdescent.
∂φ
Thechiefdifficultyistopropagategradientsthroughthepotentiallynondifferentiable
and unknown expectation under the environmental dynamics E . Luckily, this is
p(x˜|a˜)
achievablethroughthepolicygradienttheorem(Suttonetal.,1998;R.J.Williams&
Chapter4. ScalingActiveInference 150
Zipser,1989b)
∂L ∂ (cid:90)
control
= dx˜p(x˜,a˜)lnp(r˜|x˜,a˜)
∂φ ∂φ
(cid:90) ∂
= dx˜ p(x˜,a˜)lnp(r˜|x˜,a˜)
∂φ
(cid:90) ∂lnp(x˜,a˜)
= dx˜p(x˜,a˜) lnp(r˜|x˜,a˜)
∂φ
∂lnp(x˜,a˜)
=E [lnp(r˜|x˜,a˜) ] (4.6)
p(x˜,a˜)
∂φ
Whichallowsanestimateofthegradienttobecomputedsimplythroughaveragesof
environmental dynamics. Importantly, this approach does not require knowledge of
thetrue environmental dynamicsat all(unlike classical controltheory), since weonly
requiresamplesfromtheenvironmentwhichcanbeobtainedsimplythroughinteracting
withit. Intuitively,wecanthinkofthistheoremassayingthatthegradientofthecontrol
objectiveissimplytheaveragegradientofthepolicy,weightedbytherewardsreceived.
While this method works, the downside is that since the expectation is effectively
computedthroughMonte-Carlosampling(andgenerally relativelyfewsamples at that),
thegradientestimates generallyhaveaveryhigh variance, whichmakeslearning trou-
blesomeandslow. Anumberofbaselineapproacheshavebeeninventedtotrytodeal
withthisproblem(Sutton&Barto,2018)andtomakeitmoretractable. Nevertheless,
policy gradient approaches can be scaled up and applied successfully in challenging
tasks,especiallycontinuouscontroltasks,byparametrizingthepolicy f (x)withadeep
φ
neuralnetworkanddirectlyapplyingthepolicygradienttheoremwithsomeadditional
tricks(Schulmanetal.,2015;Schulman,Wolski,Dhariwal,Radford,&Klimov,2017).
AnotherapproachtothecontrolprobleminMarkovconditions(butarbitrarydynamics
aslongastheyareMarkov)istousearecursivesolutionmethodpioneeredbyRichard
Bellman (Bellman, 1952). He noticed that optimal solutions to the control problem
satisfy an interesting recursive relationship – that the optimal path to the goal at a
timestep t, must include the optimal path to the goal at a later timestep t+1. This
property allows you to build up a backwards recursion where you start at the goal at
Chapter4. ScalingActiveInference 151
theendandthenworkbackwards,constructingtheoptimalpathinapiecewisefashion
fromthepreviousoptimalpath. Thekeymathematicalquantity,here,isthecost-to-go,
which intuitively is the cost of the optimal trajectory from the current position to the
goal. Inmodernreinforcementlearning parlance, thiscost-to-goiscalledtheoptimal
valuefunctionofastate,andisconverselytheexpectedrewardwhichwouldbeattained
fromagivenstateassumingtheoptimalpolicyisfollowed. Writtenoutmathematically,
thisapproachgivesrisetotherecursiveBellmanequation,
V∗(x )=r(x )+E )[V∗(x )] (4.7)
t t p(x
t+1
|at t+1
which simply states that the optimal value function of a current state, is the reward
of the current state plus the maximum average value function of the next state. In
effect, if iterated backwards from the end (where the optimal value function is simply
r(x ) and assumed known) this recursion allows you to build up the optimal path by
T
workingbackwards. Ifallenvironmentalstatesandactionsareknownandfinite,then
this algorithm can be run explicitly to compute the optimal solution in polynomial
time (asopposed to theexponential time approachof just tryingall possible pathsand
pickingthebest). Thisisthusadynamicprogrammingalgorithmwhichisequivalentto
otherstandarddynamicprogrammingalgorithmsincomputersciencesuchasDijkstra’s
algorithmfortheshortestpaths.
Importantly, the Bellman recursion holds not just for the optimal policy and value
function, but indeed for any policy and value function, this allows solution methods
using this recursion to apply even when the state and action space is too large to
representexplicitly. Inthiscase,wecanwritetheBellmanrecursionas,
V(x )=r(x )+E )[V(x )] (4.8)
t t p(x
t+1
|at,xts t+1
and, without working backwards from the end, we can simply estimate the value
functionsV (x)ofagivenpolicyπbymovingaroundinourenvironment,computing
π
Chapter4. ScalingActiveInference 152
rewardsandstoringthestateandthenextstate,andapplyingEquation4.8. Ifwedothis
sufficientlyforagivenpolicy,wecanthenformanestimateoftheglobalvaluefunction
V (x) for all x. With this value function, we can then improve the policy, by simply
π
defininganewpolicythattakesπ=max(r(x ,a )+E [V(x )]. Somewhat
t t p(x
t+1
|xt,at) t+1
surprisingly, it has been proven that if the estimated value function is accurate, then
the new policy defined in such a manner is necessarily the same or better (in terms
of average reward obtained from the MDP) than the previous policy, and that if we
iterate the process of sampling new states to estimate the value function, and then
improvingthepolicy,thenwewillconvergeupontheoptimalpolicyπ∗ (Sutton&Barto,
2018). Thisapproach,calledpolicyiteration,isthecornerstoneofclassicreinforcement
learning algorithms such as temporal difference learning (Sutton, 1988), and SARSA
(Singh&Sutton,1996;Sutton,1996).
AcloselyrelatedapproachiscalledQ-learning(Watkins&Dayan,1992),whichinstead
of using the value function, instead maintains an estimate of the state-action value
functionQ(x,a),whichiscalledtheQ-functionforhistoricalreasons. TheQfunction
satisfiesasimilarrecursiverelationshiptothevaluefunction,
Q(x ,a )=r(x ,a )+argmaxE )[Q(x ,a )]
t t t t p(x
t+1
,a
t+1
|at,xt t+1 t+1
at
Given an estimate of the Q function, the policy improvement step is simple. πt+1 =
argmaxQ(s,a). TheQ-learningalgorithmcombinesbothpolicyevaluation(estimating
a
thevalueorQfunction)andpolicyimprovementintoonecontinuousalgorithm,which
canbesimplydefinedas,
a(x )=max Q(x ,a )
t at t t
Q(x ,a )=r(x ,a )+max E [Q(x ,a )] (4.9)
t t t t a t+1 p(x t+1 |xt,at) t+1 t+1
The Q-learning algorithm is extremely popular and effective and has been central to
many major successes of reinforcement learning, from playing backgammon (Tesauro,
1994) to Atari (V. Mnih et al., 2013; Schrittwieser et al., 2019). The Q function and
Chapter4. ScalingActiveInference 153
(cid:82)
valuefunctioncan berelatedstraightforwardly byV(x)= daQ(x,a)–i.e. thevalue
function is simply the Q function averaged over all actions. Another approach is to
write everything instead in terms of an advantage function A(x,a)=Q(x,a)−V(x)
which simply subtracts the action-independent value function baseline from the Q-
value, effectively normalizing it, since the only important thing from the perspective
ofQlearningistherelativevaluesofeachaction. Thisapproachreducesthegradient
variancewhentryingtoestimatetheQfunctionandoftenmakestheresultingalgorithms
morestable(Hesseletal.,2018).
In classical reinforcement learning we typically represent the Q and value function
explicitlyindiscrete-stateanddiscrete-actionenvironments. Forinstance,withdiscrete
states, the value function V(x) would simply be a vector of length S where S is the
numberofdistinctstates. TheQfunctionQ(x,a)issimplyanS×AmatrixwhereAis
theactiondimension.
Anotherusefulrepresentationisthesuccessorrepresentation(Dayan&Hinton,1997).
This approach rewrites the value function in terms of the instantaneous reward r(x)
andasuccessormatrixM(x,x(cid:48))whichisaS×Smatrixwhichrepresentstheaverage
transition probabilities for a given policy from state x to state x(cid:48). This matrix M can
bethought ofasthe stationary transitiondistribution oftheMarkovchainfor agiven
policy. Thevaluefunctioncanbedecomposedinto,
V(x)=M(x,x(cid:48))r(x) (4.10)
The successor representation, crucially, by separating out the policy-dependent compo-
nent (M) from the reward r allows for computation of different value functions for a
givenpolicyrapidlyunderdifferentrewardfunctions. Thusthisrepresentationallows
for very flexiblechanges of behaviour givena changein reward. Ofcourse the optimal
policy also changes under a change of reward function, and this change cannot be
straightforwardlydeterminedsolelybythesuccessorrepresentation.
Chapter4. ScalingActiveInference 154
4.1.2 Deep Reinforcement Learning
WhileclassicalreinforcementlearningapproachestypicallyrepresentthevalueorQ
functions explicitly as vectors and matrices, such methods only work for relatively
smallanddiscretestateandactionspaces,andcannoteasilyscaletotheextremelylarge
stateand actionspaces requiredfor playingcomplexgames aswell asfor complex con-
tinuouslyvaluedactionspacessuchasinrobotics,wheresimplydiscretizingthespace
withasufficientlyfinegridwillsimplyresultintoomanystatestohandle. Therefore,
tomaintainscalability,insteadofexplicitlyrepresentingthestateandvaluefunctions,it
becomesnecessarytoapproximatethemwithpowerfulfunctionapproximators. While
other approachesusing linear features (Baird,1995; G. J.Gordon, 1995), or nonlinear
basisfunctionkernels(Doya,2000)arepossible,recentsystemshaveoverwhelmingly
useddeepneuralnetworkstoapproximatethevalueorQfunctionsdirectly. Tomake
this explicit, instead of representing the value function V[x] as a vector, we instead
representitasafunctionV (x)whichtakesastatex andmapsittoascalarvalue. This
ψ
function is implemented by a deep neural network with parameters ψ. We can then
learntheseparametersusingstochasticgradientdescentonalossfunctionwhichisa
modifiedversionoftheBellmanrecursion,
L (x)=(V (x)−r(x)−max E [V (x(cid:48))]))2 (4.11)
Valuenet ψ a p(x(cid:48)|x,a) ψ
which is the squared residual between the value predicted for a state x by the value
network, and the value predicted by the Bellman recurrence relation. By using this
approach,therefore,weutilizetheintrinsicgeneralizationcapabilitiesofdeepneural
networks to allow us to estimate value or Q functions for an otherwise intractably
largespace. ThisapproachcanbestraightforwardlyextendedtoQ-learningandother
Bellmanbasedmethods. Similarly,applyingpolicygradientswithdeepneuralnetworks
isevensimpler–wesimplyparametrisethepolicyq (a|s)withadeepneuralnetwork
ψ
with parameters ψand trainit directlyusing stochasticgradient descenton thepolicy
gradientobjective(Schulmanetal.,2015,2017).
Chapter4. ScalingActiveInference 155
To get this approach working well in practice, however, requires quite a number of
tricks. Forinstance,theneuralnetworkscannotbetrainedsimplythroughcontinuous
interaction with the environment, as this gives rise to correlated data which leads to
overfittingandcatastrophicforgettingwithinthevaluenetwork(V.Mnihetal.,2013).
Thus,instead,itisnecessarytomakethedatafedintothenetworkasi.i.d aspossible
byusing amemoryreplaybuffer, whichstoresallexperiencetheagent hasencountered
over its lifetime and then replays it at random to be optimized according to Equation
4.8 (V. Mnih et al., 2013). Similarly, note that in the value network update equation
(Equation4.11),theparametersofthevaluenetworkappearstwice–oncecomputing
thevalueofx andagaincomputingthevaluesofx(cid:48). Ithasbeenfoundempirically,that
thisleadstoinstabilitiesintheoptimizationprocesswhichoftendestroylearning,since
theoptimizationprocessiseffectivelychasingasetofmovingtargets. Toresolvethis
problem,thevalueestimatesofx(cid:48) areoftencomputedusinga‘frozen’valuenetwork
whichisnotoptimizeddirectly,butisacopyofthevaluenetworkfromsomenumberof
iterationspast. Thefrozennetworkisthenupdatedtomatchthecurrentvaluenetwork
everygiven numberof iterations(V.Mnih et al., 2015). Another issueis thatthe value
estimatesareoftenskewedextremelypositiveduetothemaxoperatorintheobjective
interactinginaccuratevaluefunctionestimates. Thiscanbeamelioratedempiricallyby
simplytrainingtwo(ormany)valuenetworksinparallel,andthenchoosingthesmallest
valueestimatesoutofallofthem,atechniqueknownasduelingvaluenetworks(Wang
etal.,2016). Forathoroughreviewoftricksandtipsfortrainingdeepreinforcement
learningagents,wesuggestFujimoto,vanHoof,andMeger(2018);Hesseletal.(2018).
Nevertheless, once all of these instabilities have been addressed, the result is an ex-
tremelypowerfulandgenerallearningtechniquewhichhasbeendemonstratedempiri-
callytoscaleuptosolveverychallengingtaskssuchasAtarigames(A.Mnih&Gregor,
2014;V.Mnihetal.,2015)andGo(Silveretal.,2016,2017),StarcraftII(Vinyalsetal.,
2019),andultimatelyverychallengingtasksinrobotics(Chua,Calandra,McAllister,
&Levine,2018;Nagabandi,Konoglie,Levine,&Kumar,2019;Watter,Springenberg,
Chapter4. ScalingActiveInference 156
Boedecker,&Riedmiller,2015;G.Williams,Aldrich,&Theodorou,2017).
4.1.3 Model-free vs Model-based
It is important to note that all the methods we have discussed so far require samples
from the true environmental dynamics to approximate the expectation E from
p(x˜,a˜)
the ultimate loss function (Equation 4.1. While these samples are easy to acquire in
the case where interacting with the environment is cheap and easy, such as when the
environmentisasimulationsuchasagameoranOpenAIgymenvironment(Brockman
et al., 2016), in many real world tasks this is not the case. For instance, in robotics,
interactingwiththerealenvironmentisoftenslow(therealrobothastoactuallymoveor
dothingsinphysicalspace)andcostly(thismovementrequirespowerandalsoinduces
wear and tear on the robot. In extreme situations, bad policies may actually result in
the robot damaging itself). In this case, it is often better if we can somehow eschew
interacting with the real environment in favour of a model of the real environment.
Havinga‘worldmodel’(Ha&Schmidhuber,2018)oftheenvironmentallowstheagent
toplanandtestdifferentpotentialcoursesofactionwithouthavingtosustaincostlyand
slowinteractionswiththerealworld. Theutilityofmodelsdoesnotjustextendtothe
environmentaldynamics. Itisoftenthecasethattheactualrewardfunctionoftheagent
is unknown. This is not generally true in reinforcement learning and control theory,
whichtypically have wellspecified rewards, butis oftentrue inthecase ofbiological
organismsencounteringnovelcontingencies,whereitisnotnecessarilyknowna-priori
ifasituationisgoodorbad. Thus,wecanalsolearnamodeloftherewardfunctionas
well.
Mathematically, we can formalize this property of having a model of the transition
dynamics or reward functions by postulating additional probability densities which
the agent possesses q (x˜|a˜) and q (r˜|x˜,a˜) which represent the agent’s model of the
φ θ
true environmental dynamics p(x˜|a˜) and true reward function p(r˜|x˜,a˜). Then, using
Chapter4. ScalingActiveInference 157
importancesampling,wecanintroducethesemodelsintothepreviouslossfunction,
L =argmaxE [lnp(r˜|x˜,a˜)]
control p(x˜|a˜)p(a˜)
a
q(x˜|a˜) q(r˜|x˜,a˜)
=argmaxE [lnp(r˜|x˜,a˜) ]
p(x˜|a˜)p(a˜)
q(x˜|a˜) q(r˜|x˜,a˜)
a,φ,θ
p(x˜|a˜) p(r˜|x˜,a˜)
=argmaxE [lnq(r˜|x˜,a˜) ]
q(x˜|a˜)p(a˜)
q(x˜|a˜) q(r˜|x˜,a˜)
a,φ,θ
p(x˜|a˜) p(r˜|x˜,a˜)
=argmaxE ln [lnq(r˜|x˜,a˜) ]
q(x˜|a˜)p(a˜)
q(x˜|a˜) q(r˜|x˜,a˜)
a,φ,θ
=argmaxE [lnq(r˜|x˜,a˜)]−KL[q(x˜|a˜)||p(x˜|a˜)]−E (cid:2)
q(r˜|x˜,a˜)
(cid:3)
q(x˜|a˜)p(a˜) q(x˜|a˜)p(a˜) p(r˜|x˜,a˜) RewardModelIdentification
a,φ,θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
RewardMaximization SystemIdentification (cid:124) (cid:123)(cid:122) (cid:125)
(4.12)
Here we see that the objective can be partitioned into three terms. The first, reward
maximization, is equivalent to the original control objective except it utilizes the
modelenvironmentaldynamicsandrewardfunctioninsteadofthetrueenvironmental
dynamics and reward function. The second term, ‘system identification’, encodes
the KL divergence between the true and modelled environmental dynamics, and is
minimized with respect to the parameters of the model of the dynamics. Optimizing
thistermencouragestheagenttolearnanaccuratedynamicsmodeloftheworld. The
thirdterm,‘rewardmodelidentification’encouragestherewardmodeltomatchthetrue
rewardfunctionandoptimizingthistermwithrespecttotheparametersofthereward
model minimizes the difference between the model reward distribution and the true
rewarddistribution.
Interestingly, mathematically, the minimization of all of the three parameters is over
allthreeterms. Thisleadsto,forinstance,theparametersoftherewardanddynamics
modeltobeoptimizedovertherewardmaximizationterm,whicheffectivelyencourages
thedynamics andrewardmodels totryto learnpositivelybiased dynamicsandreward
models which are encouraged to give out larger rewards. Conversely, the divergence
Chapter4. ScalingActiveInference 158
terms between true and modelled environmental dynamics and reward function are also
being optimized with respect to action, so the optimisation process also encourages
action to make the true dynamics and true reward function correspond more closely
to the modelled ones. These interactions between the optimizations of the different
termsoftenhavestrangeanddeleteriouseffectsonagentbehaviour,especiallylearning
the dynamics to maximize the reward, which can give agents a highly dysfunctional
‘optimismbias’(Levine,2018). Assuch,inpractice,thisoptimizationisoftensplitinto
threeseparateandindependentoptimizationsforeachsetofparametersrespectively,
argmaxE [lnq(r˜|x˜,a˜) (4.13)
q(x˜|a˜)p(a˜)
a
argminKL[q (x˜|a˜)||p(x˜|a˜)]
φ
φ
q(r˜|x˜,a˜;θ)
argminE (4.14)
q(x˜|a˜)p(a˜)
p(r˜|x˜,a˜)
θ
These three optimization processes correspond to maximizing the reward (standard
reinforcementlearning),learningtheenvironmentaldynamicsmodel,andlearningthe
reward function, respectively. In control theory, the process of learning a dynamics
modeliscalledsystemidentification.
Given good reward and dynamics models, it is then possible to utilize standard model-
free reinforcement learning approaches to estimate Q and value functions, or estimate
policygradientsdirectlythroughsimulatedsamplesofthedynamicsmodel. Withthese
itisthenpossibletolearnpoliciesintheusualwaywithouteverhavingtointeractwith
the environment (except to learn the models in the first place). This approach, first
introducedin the Dynaarchitecture (Sutton,1991) hasbeen studiedin theliterature for
a long time, andis generally more sample efficientthan pure model-free reinforcement
learning which learns directly from environmental transitions, as learning dynamics
modelsoftheworldistypicallyfasterthanlearningrewardmodels,sincetheprediction
errorsinthestateoftheworldisaricherinformationalsignalthantherewardprediction
error since the reward is usually scalar while the world-state is typically very high
Chapter4. ScalingActiveInference 159
dimensional.
Another approach, once you have a model is to skip the model-free methods and
move directly to planning using the model. In the simplest case, this can be done by
samplingdifferent actiontrajectories, simulatingtheir consequencesand rewards using
thedynamicsandrewardmodels,andthensimplychoosingtheactiontrajectorywith
thebestestimated rewards. Moreadvancedmethodsincludethecross-entropy method,
whichtriestofitaprobabilistic(Gaussian)actiondistributiontomaximizerewards,and
thepath-integralmethod,which fitsaBoltzmanndistributionofactiontrajectories over
rewards(Kappenetal.,2012;K.C.Rawlik,2013;J.TheodorouEvangelosndBuchli&
Schaal,2010b;D.Williams,2018;G.Williams,Aldrich,&Theodorou,2017). Thiskind
ofmodel-basedplanningisoftencombinedwithModel-Predictive-Control,whichis
simplywhereyoure-planfullyateverytime-step,andhasbeenusedtoreachstateofthe
artperformanceonawidevarietyofreinforcementlearning(Nagabandietal.,2019)and
roboticstasks (G.Williams,Drews, Goldfain,Rehg,& Theodorou,2016; G.Williams,
Wagener,etal.,2017),allwhilerequiringsubstantiallyfewerenvironmentalinteractions
thanmodel-freereinforcementlearningapproaches.
4.1.4 Exploration and Exploitation
An interesting question that arises in the control problem, wherever there are any
unknowns,eitheroftheoptimalvaluefunctionandpolicyortheenvironmentaldynam-
ics and reward function, is the exploration-exploitation tradeoff (Berger-Tal, Nathan,
Meron,&Saltz,2014;Cohen,McClure,&Yu,2007;Dayan&Daw,2008;Kaelbling,
Littman,&Cassandra,1998;Mobbs,Trimmer,Blumstein,&Dayan,2018;Suttonet
al.,1998). Thistrade-offarisesbecauseinordertoobtainamoreaccurateestimate,it
is usually necessary to explore new regions of the state-space away from the locally
optimallocation. However,byignoringthelocallyoptimumcourseofaction,youincur
an opportunity cost equal to the difference between the (usually worse) reward from
Chapter4. ScalingActiveInference 160
theexploringcomparedtothelocaloptimum. Conversely,byonlyeverexploitingthe
localoptimum andneverexploring, ifthere areactuallybetter optimaoutthere, which
you have simply not found, you incur a constant opportunity cost of the distance to
the trueoptimum every singletime you exploityour local optimum. Thus,to find the
trulyoptimalbehaviours,itisnecessaryto explorewidely,andnotjustbesuckedinto
whatevertheclosestlocaloptimayoufind. However,explorationhasanintrinsiccost
associated with it, since assuming you have a halfway decent local optimum, almost
everythingyouexplorewillbeworsethanthat,andthusexplorationmustbekepttoa
minimumtomaximizereturn,eveninthelongrun.
Theexploration-exploitationtradeoffisalsocentralto thebehaviourandperformance
ofreinforcementlearningagents,especiallydeepreinforcementlearningagentswhich
cannot explicitly represent every contingency in the state-space. Empirically, it has
been found that except in extremely simple tasks, or where the reward function is
a smooth gradient to the global optimum, some forms of exploration are necessary
to achieve good performance with deep reinforcement learning techniques. A large
numberofheuristic exploration techniques havebeen developedin theliteraturewhich
work well for many tasks. Perhaps the simplest of these is the ε-greedy approach
(Sutton et al., 1998), which simply takes the greedy action (1−ε) amount of the
time, and takes a random action ε percent of the time, where ε is a small number
– typically about 0.02−0.05. This method essentially bakes in a certain degree of
randomexplorationintothemethodsothatitwill(eventually)exploreallcontingencies
purelyduetoitsrandomactions. Inmanycontroltasksthismethodgeneratessufficient
explorationtoenablegoodperformance. Moresophisticatedvariantsannealthevalue
of ε over time; working on the idea that at the beginning when little is known you
should explore more, and then later when you already have a pretty good policy you
should explore less. Another approach in algorithms like Q learning is that instead
of simply taking the maximum value, take actions with a probability equal to their
βexp(−Q(a|x))
softmaxed Q-values – q(a|x)= where β is a parameter which controls
∑a exp(−Q(a|x))
Chapter4. ScalingActiveInference 161
thespreadorentropyofthisdistribution. Whenβislarge,thenthedistributionishighly
peakedandtendstowardsthemax. Whenβissmall,thenthedistributiontendstowards
a uniform over all action possibilities. This method is called Boltzmann exploration
(Cesa-Bianchi,Gentile,Lugosi,&Neu,2017),sincetheactiondistributionisequivalent
tothe Boltzmann distributionofstatisticalmechanics with theβparameterfunctioning
asaninversetemperature.
Another approach, which we will explore in detail in the next section, is to add an
entropy maximization term to the objective (Levine, 2018). Thus, instead of simply
maximizingthereward,thegoalistomaximizetherewardwhilekeepingtheentropy
ofthepolicyasgreataspossible,andthusmakingactionasrandomaspossible2 While
stillarandomkindofexploration,thisperformsbetterthanεgreedyapproachessinceit
explicitlytrades offrandomnessand rewardmaximizationin theobjective, ratherthan
asanεparameterthatneedstobetunedbyhand.
While all these exploration methods work well in practice in many benchmark en-
vironments for deep reinforcement learning, they all fundamentally utilize random
exploration – i.e. actions are selected randomly in order to drive exploration. It is
importanttonote, however,thatthisstrategyis necessarilyveryinefficient,sinceeven
withpurelyrandomactions,arandomwalkwillexploreslowly. Indeed,thesemethods
tendtoperform ratherpoorlyinmorechallenging largeenvironmentswherealluseful
contingencies cannot realistically be explored with a random walk in a reasonable
amount of time,and tend to perform especiallypoorly insparse reward environments,
whererewardsarehardtoobtainandoftenrequireaprettygoodpolicytoevengetany
rewardatall(Tschantz,Millidge,etal.,2020b). Agoodexampleofsuchanenviron-
mentismanygames,whereyouareonlyrewardeda1ifyouwinthegame. However,to
2Thisideaissimilarto, butdistinctfrom, ideassuchasupper-confidence-boundsamplingwhich
assignoptimismbonusestostatesinproportiontotheinversedegreetowhichtheyhavebeensampled.
Thekeydifferenceisthatmaximumentropyapproachesprovidebonusestoactionsbasedontheoverall
entropyoftheactiondistributionwhileUCBalgorithmsprovidebonusestostatesbasedontheirepistemic
uncertainty.
Chapter4. ScalingActiveInference 162
evenwinanygamesatallrequiressomeskillatplaying3. Toaddresstheshortcomings
of random exploration, a significant amount of work has been done on directed, or
information-seeking, exploration. Here, exploratoryactions arenot completelyrandom
butinsteaddirectedatsomeexploratorygoal–usuallytoaccumulateinformationorto
resolveuncertaintyabouttheworld. Thismakessenseasinastationaryenvironment
thereislittlepointinrepeatedlyexploringbadoptionswhichareknowntobebad. The
keyistoexplorewherethereisremainingresolvableuncertainty.
A number of different objectives, or ‘intrinsic measures’ (Oudeyer & Kaplan, 2009)
havebeenproposedtoachievethis. Theseincludepredictionerrorminimization(Pathak,
Agrawal, Efros, & Darrell, 2017), ensemble divergence (Chua et al., 2018), explicit
informationgain(Shyam, Jas´kowski, &Gomez, 2019; Sun,Gomez, &Schmidhuber,
2011; Tschantz, Millidge, et al., 2020b), and empowerment (Klyubin, Polani, & Ne-
haniv, 2005). Typically such approaches postulate a separate ‘exploration objective’
andthen eitheroperatein two phaseswherebyfirst themodeloptimizesthe exploration
objective,andthenitswitchestooptimizingthegreedyobjective(Shyametal.,2019),
or alternatively, the exploration and greedy objectives are added together to form a
unified objective function which is minimized throughout (Tschantz, Millidge, et al.,
2020b). Suchapproachesthereforeencouragetheagenttoseekabalancebetweenits
exploratoryandreward-maximizationimperatives. Thishasthetheoreticaladvantageof
encouragingtheagenttoonlyexploreregionsofthestate-spacewhichcombinebotha
highexpectedrewardand muchresolvableuncertainty,asopposedtosimplyresolving
uncertainty forits ownsake. In the literature, most ofthese exploratory objectivesare
simplypostulatedandarguedforonintuitivegroundsandthenempiricallycompared.
3Thisisoftenaddressedinpracticebyusingrewardshaping,wheretypicallyeithertheagentdesigner
ortheenvironmentdesignerwillcreatea‘proxy’rewardfunctionfortherealonewhichislesssparse.
Forinstance,inagamelikechess,insteadofsimplyrewardingwinningorlosingthegame,theagent
mightgetrewardsfortakingpieces,orgainingpositionaladvantage. Whilethisapproachworksvery
wellinpractice,itrequireshumaninterventionforeverytasktheagenttriestoaccomplish,andisoften
trickytodesignaproxyrewardwhichsuccessfullyleadstothecorrectultimatebehaviour. Ultimately,we
wantagentstobeabletointeractwiththeworldfullyautonomously,whichmeansthattheyshouldnot
needspecialhuman-designedrewardfunctionstohandleanynewsituation,andsowedonotconsider
rewardshapingfurther
Chapter4. ScalingActiveInference 163
Figure 4.1: Graphical model for controlas inference, with optimality variablesΩ. Other
thantheoptimalityvariables,thegraphicalmodeltakestheformofaMarkovDecision
Process with actions a and states s. The state of a specific timestep depends on the
actionand stateof thelast time-step. Bywriting outan explicitgraphicalmodel likethis,
wecanapplyawholefield’sworthofinferencealgorithmsongraphicalmodelslikethis
tosolvecontrolproblems.
However, with the exception of the entropy maximizationdiscussed above, which we
shallseearisesfromexplicitlyconsideringcontrolasavariationalinferenceproblem,
themathematicallyprincipledoriginsoftheseadditionalexploratoryterms,especially
information-gainandempowermenttermsremainsmysterious. Chapter5isdedicated
toderivingthemathematicalbasisforsuchobjectives.
4.1.5 Control as Inference
Sincewehavebeensointerestedinunderstandingbrainfunctionthroughthelensof
Bayesian(variational)inferenceinthisthesis,anaturalquestionarisesastowhether
the control problem as discussed previously can be cast in such an inference frame-
work. It turns out that this is indeed the case, and is quite straightforward to achieve.
Startingfrom(Attias,2003)andthendevelopedby(Kappenetal.,2012;Levine,2018;
K.C.Rawlik,2013;J.TheodorouEvangelosndBuchli&Schaal,2010a;Todorov,2008;
Toussaint&Storkey,2006),asmalllineoftheliteraturehasworkedoninvestigatingand
developingthecloseconnectionsbetweenthecontrolproblemandBayesianinference.
While the control objective (Equation 4.1 is a probabilistic objective, it is not yet an
Chapter4. ScalingActiveInference 164
inference objective. There is nothing there to be inferred. The key step is to define
dummy variables Ω which are binary random variables which simply whether a
1:T
given trajectory timestep is optimal or not. Ω = 1 if the timestep is optimal and
t
Ω = 0 if it is not optimal. Given this dummy variable, the task of inferring the
t
optimalpolicycanbewrittensimplyasfindingthedistribution p(a˜,x˜|Ω˜ =1). Tobegin
inferringthisdistribution,itisfirstnecessarytomakeonemoreassumptionaboutthe
dummy variables Ω, in order to operationalize the notion of optimality. We define
p(Ω =1|a ,x )∝exp(−r(x ,a ))suchthattheprobabilityofoptimalityisproportional
t t t t t
to the exponentiated reward. Intuitively, this can be seen as a mathematical trick
allowingthe‘log-likelihoodofoptimality’tobeequaltothereward,thusallowingusto
castrewardmaximizationasaprocessofmaximumlikelihoodestimation.
OnewaywecanfindthecrucialdistributionissimplytodirectlycomputeitviaBayes
rule. First,wewriteoutBayesruleexplicitly,
p(Ω˜,x ,a )
p(a |x ,Ω˜)= t t
t t p(Ω˜,x )
t
p(Ω˜|x ,a )p(a |x )p(x )
t t t t t
=
p(Ω˜|x )p(x )
t t
p(Ω˜|x ,a )p(a |x )
t t t t
=
p(Ω˜|x )
t
p(Ω˜|x ,a )
t t
≈ (4.15)
p(Ω˜|x )
t
Where,inthefinalline,weassumethattheactionprior p(a |x )isuniform. Now,look-
t t
ingatthetwotermswehaveleft,wecanintuitivelythinkofthenumerator p(Ω˜|x ,a )as
t t
representingtheprobabilityofoptimalityofallfuturestates,giventhecurrentstateand
action. However,wehaveanothertermforthis–thecost-to-go,ortheQ-function. In
effect,weobtaintheBellmanrecursiverelationshipdirectlyfromBayesrule. Secondly,
thedenominator p(Ω˜|x )= (cid:82) da p(Ω˜|x ,a )clearlycorrespondstothevaluefunction.
t t t t
Chapter4. ScalingActiveInference 165
Wecanfromthisdirectlyderiverecursiverelationshipsamongtheseterms,
(cid:90)
p(Ω |x ,a )= dx da p(Ω |x ,a )p(x ,a |x ,a )p(Ω |x ,a )
t:T t t t+1 t+1 t+1:T t+1 t+1 t+1 t+1 t t t t t
(4.16)
Wecanthussee,thatduetoourdefinitionofoptimalitythat p(Ω |x ,a )=exp(−r(a ,x ),
t t t t t
that to obtain the correspondence to the value and Q function requires the log of the
optimalityprobability. Wethushave,
Q =lnp(Ω |x ,a )
CAI t:T t t
V =lnp(Ω |x ) (4.17)
CAI t:T t
Where, by taking the log of the integral and exponential, we effectively have the log-
softmaxfunctioninsteadofthemaxintheBellmanequations. Thiscorrespondstoa
‘soft’maximuminsteadofthehardmaximumusedinthetraditionalBellmanrecursion.
However, other than that, our new definitions of the value and Q function satisfy the
standard Bellman recursive relationship, and as such can be used to derive all the
traditionalreinforcementlearningalgorithmssuchasQ-learning,temporaldifference
learning,andSARSA(Sutton,1996).
Another approach is to use variational inference and attempt to approximate the true
posteriordistributiongivenoptimality p(a˜,x˜|Ω˜)withavariationaldistributionq(x˜,a˜).
Tomakethisapproximationaccurate,wethuswishtominimizethedivergencebetween
thetwodistribution.
L =D [q(x˜,a˜)||p(x˜,a˜|Ω˜)]
CAI KL
p(x˜,a˜,Ω˜)
=D [q(x˜,a˜)|| ]
KL Ω˜
=D [q(x˜,a˜)||p(x˜,a˜,Ω˜)]+lnΩ˜ (4.18)
KL
(cid:124) (cid:123)(cid:122) (cid:125)
ELBO
WhereweonlyneedtooptimizetheEvidenceLowerBound(ELBO)termsincelnΩ˜ is
constantwithrespecttothevariationaldensityq(x˜,a˜). Crucially,wecanthensplitup
Chapter4. ScalingActiveInference 166
theELBOtermasfollows,
L =D [q(x˜,a˜)||p(x˜,a˜,Ω˜)]
CAI KL
=D [q(a˜|x˜)q(x˜)||p(Ω˜|x˜,a˜)p(a˜|x˜)p(x˜)]
KL
=E [lnp(Ω˜|x˜,a˜)]+D [q(a˜|x˜)||p(a˜|x˜)]+D [q(x˜)||p(x˜)] (4.19)
q(x˜,a˜) KL KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
RewardMaximization ActionDivergence DynamicsDivergence
Ifwethenassumethatthevariationaldynamicsq(x˜)areequaltothetrueenvironmental
dynamics p(x˜)(theagentcannotchangethedynamicsexceptthroughaction)thenthe
actiondivergencetermdisappears. Additionally,ifweusethefact,definedearlier, that
p(Ω˜|x˜,a˜)=∏
t
exp(−r(a
t
,x
t
)), thenwe obtainan objective whichlooks substantially
moresimilartotraditionalreinforcementlearningobjectivesexceptforanadditional
action divergence term between the variational action distribution (the policy) and a
prioractiondistribution.
T
L =E [∏r(x ,a )]+D [q(a˜|x˜)||p(a˜|x˜)] (4.20)
CAI q(a˜|a˜)p(x˜) t t KL
t (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) ActionDivergence
RewardMaximization
If we further assume a uniform action prior, then the control as inference objective
reducesto,
T
L =E [∏r(x ,a )]−H[q(a˜|x˜)]
CAI q(a˜|a˜)p(x˜) t t
t
T
=E [∏r(x ,a )−lnq(a |x )] (4.21)
q(a˜|a˜)p(x˜) t t t t
t
Whichissimplyrewardmaximizationwhilealsosimultaneouslymaximizingtheen-
tropyofthepolicyq(a˜|x˜). Thisobjectivehasbeenutilizedinanumberofrecentworks
(Abdolmalekietal.,2018;Haarnoja,Tang,Abbeel,&Levine,2017;Haarnoja,Zhou,
Abbeel, & Levine, 2018;K. Rawlik, Toussaint, & Vijayakumar, 2013), and formsthe
basis of the soft-actor critic architecture (Haarnoja, Zhou, Abbeel, & Levine, 2018),
whichsimplyoptimizesarelativelystandardactor-criticarchitectureonthisobjective.
Ithasbeenfoundtoreachstate-of-the-artperformanceformodel-freereinforcement
Chapter4. ScalingActiveInference 167
learning on a wide range of challenging continuous control tasks (Haarnoja, Zhou,
Hartikainen,etal.,2018;Hesseletal.,2018). Moreover,thesimplicityandrobustness
ofthisalgorithmallowittoserveasaninfluentialbenchmarkforthefield. ThisCAI
objectivecanbe straightforwardlyoptimizedby takinggradientsofL againstwhat-
CAI
everparametersthereare. Forinstance,optimizingthecontrol-as-inferenceobjective
is identical to standard policy gradients except that each reward the agent receives
has lnq(a |x ) subtracted from it. This differentiates control as inference from previ-
t t
ousworks(O’Reilly,Wyatte,&Rohrlich,2017),whichheuristicallyusedanentropy
regulariser to prevent policy collapse, but computed the entropy directly outside of
the expression for the reward. The control-as-inference objective is both simpler to
implementandmorerobustthanthismethod.
Importantly, the control as inference objective, as it is a variational bound, can be
derived directly from, and as a lower bound on the marginal likelihood of optimality
lnp(Ω˜). Thederivationisstraightforwardandgoesasfollows,
(cid:90) q(a˜|x˜)
lnp(Ω˜)=ln p(Ω˜,x˜,a˜)
q(a˜,x˜)
p(Ω˜,x˜,a˜)
≤E [ ]
q(a˜,x˜)
q(a˜,x˜)
p(Ω˜|x˜,a˜)p(a˜|x˜)
≤E [ ]
q(a˜|x˜)p(x˜)
q(a˜|x˜)
T
≤E [∏r(x ,a )]−D [q(x˜|a˜)||p(a˜|x˜)]
q(a˜|x˜)p(x˜) t t KL
t
≤L (4.22)
CAI
Under theassumption thatthe variational and generative dynamicsare thesame. Over-
all, the control as inference approach demonstrates that it is possible, even relatively
straightforward, to derive a range of reinforcement learning algorithms from a vari-
ational approach on an MDP graphical model augmented with additional optimality
variables. Doing so results in the standard reward maximization objective plus a reg-
Chapter4. ScalingActiveInference 168
ularisation term which tries to keep the learned policy q(a˜|x˜) as close as possible to
someactionprior p(a˜|x˜). Iftheactionpriorissettobeuniform,thenthisregularising
KLdivergencesimplyreducestothemaximizationoftheentropyoftheactionpolicy.
This action entropy maximization term functions as a powerful regulariser and implicit
exploratory drive, which aims to keep the policy as random as possible while still
maintainingperformance. Thistermisespeciallypowerfulandimportantforpreventing
policycollapse,awell-knownphenomenoninpolicygradientandactor-criticmethods
(Fujimotoetal.,2018),inwhichtheprobabilisticpolicyofanagenttypicallycollapses
tosomedeterministicpolicywhichmaynotevenbeverygood. Onceitisinthisstate,
it is very difficult for the agent to continue to explore to find better policies, since it
hasminimalprobabilityoftakingotheractions. Thetheoreticalbenefitsofthisaction
entropytermhavebeendemonstratedempiricallyintheliterature,where‘soft’control
as inferenceapproaches havegenerally shownto outperform classicalreinforcement
approachesaswellasbeingmorestableandeasytotrain. However,itisimportantto
notethatalthoughtheactionentropytermiseffectiveatmaintainingexploration,itonly
encouragesrandom exploration, orrandomwalkbehaviourinaction-space. To solve
sparserewardtasksinareasonableamountoftime,itispossiblethatamoreintelligent,
directed explorationstrategyis needed,which focusesexplicitlyonminimizing resolv-
ableuncertaintyabout theworld. Suchinformation-seekingexplorationobjectivesare
a major focus and benefit of active inference, and understanding their mathematical
natureandoriginsisthemajortaskofChapter5ofthisthesis.
4.2 Deep Active Inference
Activeinferenceandreinforcementlearningbothpurporttosolvethesamefundamental
problem – that of adaptive action selection to maximize some notion of rewards or
desires,given uncertaintyabouttheworldand abouttheoptimalpolicy totake. While
activeinferencearisesfrom theparadigmofvariationalBayesianinference withpos-
Chapter4. ScalingActiveInference 169
teriorpolicydistributionsandcomplexgenerativeworldmodels(Friston,FitzGerald,
et al., 2017b; Friston, Rigoli, et al., 2015b), reinforcement learning arises primarily
fromtheBellmanequationandtherecursivepropertiesofoptimality(Kaelblingetal.,
1996;Sutton&Barto,2018),andutilizesconstructssuchasvaluefunctionsandpolicy
networkstolearnadaptivebehaviourevenonchallengingandcomplexcontroltasks.
Despitetheverydifferentoriginsofthetwofields,sincetheyarefundamentallytryingto
solvethesameproblem,itseemslikelythatthereismucheachfieldcanlearnfromthe
other, sincetheyboth illuminatedifferencefacets ofthesame reality. Specifically,the
discrete-state-spaceactiveinferencemodelsintroducedpreviously,inChapter2,suffer
from many limitations of scale. Specifically, they represent the core distributions as
discretecategoricalvariables,whichrequirearelativelysmall,discreteandknownstate-
spacetofunction. Moreover,activeinferencemodelstypicallyassumeknowledgeofthe
truegenerativeprocess(i.e. thelikelihoodandpriormatrices(AandB)),whichoften
cannotsimplybeassumedinmorerealisticcontroltasks. 4. Anadditional, andserious
obstacletothescalabilityofclassicalactiveinferencemethodsisthecomputationof
the policy prior, which is often taken to be the softmax of the expected free energy
over all policies. This is typically computed explicitly and exactly in the literature
(Da Costa, Parr, et al., 2020; Friston, Rigoli, et al., 2015b), and requires an explicit
enumeration of every single policy and its associatedtrajectory for which the expected
freeenergycanthenbecomputed. Incomputationalcomplexityterms,thisresultsin
exponentialcomplexityinboththetimehorizonandthesizeofthediscretestate-space,
whichclearlyposesasignificantcomputationalscalingissueevenforrelativelysmall
state-spaces and time-horizons. It is largely this obstacle which has prevented the
application of truly large scale active inference models and limited most studies to
4ThereissomeworkempiricallyinvestigatinglearningtheAmatrixusingdirichlethyperpriorsover
itsvalues(Schwartenbecketal.,2019),andtherulesforlearningtheBmatrixarealsostraightforward.
However, most of the active inference literature eschews these methods in favour of hand-designed
likelihoodandtransitionmatrices(Friston,Rigoli,etal.,2015b;Friston,Rosch,Parr,Price,&Bowman,
2018b;Fristonetal.,2012;Parr&Friston,2017b),andlargescalestudiesoftheeffectivenessofthese
learningalgorithmshasnotbeenascertainedatscale.
Chapter4. ScalingActiveInference 170
toy tasks. There have been several methods in the literature proposed to somewhat
amelioratethecomputationalexpenseoftheexpectedfreeenergy,notablybypruning
away policies which have an a-priori likelihood less than some threshold (Friston,
DaCosta,Hafner,etal.,2020). However,althoughsuchapproachesenablescalingto
slightlylargertasks,theydonotattackthefundamentallyexponentialcomplexityofthe
algorithm,rathertheysimplyreducetheexponentialcoefficient.
Indeed, all the scaling limitations of active inference are almost identical to those
of tabular reinforcement learning with explicitly represented state and action value
functions. In reinforcement learning, this scaling barrier was removed through the
useofdeepneuralnetworksasflexiblefunctionapproximators,tolearn,viagradient
descent,toapproximatetherequiredconstructsbytrainingonadatasetofenvironmental
interactions(Sutton&Barto,2018). Weproposeasimilarapproachmayproveequally
usefulforscalingupactiveinferencemodels. Inthenexttwosectionswepresenttwo
studies which attempt to scale up active inference by using deep neural networks to
flexibly approximate key densities in the active inference equation, as well as utilize
methodsfromdeepreinforcementlearningtoapproximatetheevaluationoftheexpected
freeenergyoverpolicies,whichisfundamentaltoactionselectioninactiveinference.
Inthefirststudy,whichisbasedonthepaper(Millidge,2020),weareheavilyinspired
by model-free deep reinforcement learning algorithms. We represent the transition
dynamics, observation likelihood, and variational action distribution as neural net-
works trained to jointly minimize the variational free energy. Similarly, we utilize a
bootstrapppedvaluenetworktoapproximatetheexpected-freeenergyvaluefunction.
We show that this model-free deep active inference approach can scale to perform
equivalently, if not sometimes superiorly to contemporary deep reinforcement learning
approaches.
In the second study, which is based on the paper (Tschantz, Millidge, et al., 2020b),
whichwasajointcollaborationwithAlexanderTschantzattheUniversityofSussex,
Chapter4. ScalingActiveInference 171
we utilize a scheme inspired by model-based active inference for action selection.
Specifically,weuseamodel-basediterativeplannertoestimatethevariationalaction
distribution,andestimatetheexpectedfreeenergyvaluefunctionbasedonsimulated
rollouts within the planner. We focus more heavily on the exploratory nature of the
behaviourfurnishedbytheexpectedfreeenergy(andfreeenergyoftheexpectedfuture–
tobediscussedinchapter4)objectivesanddemonstratethatoptimizingtheseobjectives
leadstoempiricallybetterperformance,especiallyonsparse-rewardtaskswhichrequire
substantialamountsofexploration.
4.2.1 Model-Free: Active Inference as Variational Policy Gradients
4.2.1.1 Derivation
The fundamental idea of active inference is to reformulate the control problem as a
variational inference one, and then use variational methods to solve it. Specifically,
we wish to recast the problem of control into one of inferring the optimal state and
actiondistribution. Theformalsetupweusetodescribethisisadiscrete-timePartially
ObservedMarkovDecisionProcess(POMDP)model. Inthismodel,theagentreceives
observationso ∈O,whicharegeneratedbysomehiddenenvironmentalstatex ∈X
1:T 1:T
whichsatisfiestheMarkovproperty. Theobservationsthemselvesdonotnecessarily
havetobeMarkov. Theagentcanthen emit actionsa ∈Awhichcanalterthe latent
1:T
stateoftheenvironmentandthusgeneratenewobservations. Weassumethattheagent
maintainsadesiredistribution p˜(o
1:T
)=∏
t:T
exp(−r(o
t
))overobservationssuchthat
itmostdesirestobeexperiencinghighrewards. Observations,states,andactionsare
optimizedoverfulldiscrete-timetrajectoriesfromtimest =0toagiventimehorizon
T. Given these assumptions, we can write down a factorization of the environmental
POMDPasfollows,
T
p (o ,x |a )= p (x )p (o |x )∏p (o |x )p(x |x ,a ) (4.23)
env 1:T 1:T 1:T env 1 env 1 1 env t t t t−1 t−1
t=2
We then assume that the agent knows the basic POMDP structure and factorisation
Chapter4. ScalingActiveInference 172
propertiesoftheagent(althoughnotnecessarilyanydetailsabouttheprecisedistribu-
tionsinvolved), andmaintainsan additional generativedistributionover actions which
specify its ideal action generating process. We can thus write the agent’s generative
modelas,
T
p (o ,x ,a )= p (o ,x )p (a )∏p (o |x )p (x |x ,a )p (a |x )
agent 1:T 1:T 1:T agent 1 1 agent 1 agent t t agent t t−1 t−1 agent t t
t=2
(4.24)
Fromnowon,sincethetrueenvironmentalgenerativeprocessisneverknown,wedo
notrefertoit,onlythegenerativemodeloftheagent. Thus,fornotationalconvenience,
wedenote p simplyas p. Theinferenceproblemwewishtosolve,istoinferthe
agent
optimalactionandstatedistributiongivenobservations. Thatis,thekeyideainactive
inferenceistoinferthedistribution,
p(x ,a |o ) (4.25)
1:T 1:T 1:T
Notethatunlikeincontrolasinferenceapproacheswhichencoderewarddirectlyinto
the inference processby performing inference on a graphical modelaugmented with
additional optimalitynodes, here we encode rewards or goals intothe model through
the action prior, as we shall see later. In most situations, a direct computation of
this posterior distribution is intractable, so we resort to a variational approximation.
We define the variational distribution q(a ,x |o ) which is under the control of
1:T 1:T 1:T
theagent,andthentrytominimizethedivergencebetweenthetrueandapproximate
posteriors,
L =argminD [q(a ,x |o )||p(x ,a |o )] (4.26)
KL 1:T 1:T 1:T 1:T 1:T 1:T
a
1:T
Thisdivergenceisstillintractablesinceitcontainstheintractableposterior,howeverwe
canderiveacomputableboundonthisdivergenceknownasthevariationalfreeenergy,
Chapter4. ScalingActiveInference 173
whichwecanthenoptimize,
F(o )=D [q(a ,x |o )||p(x ,a ,o )]
1:T KL 1:T 1:T 1:T 1:T 1:T 1:T
=KL[q(a ,x |o )||p(x ,a |o )]+lnp(o )
1:T 1:T 1:T 1:T 1:T 1:T 1:T
≥KL[q(a ,x |o )||p(x ,a |o )] (4.27)
1:T 1:T 1:T 1:T 1:T 1:T
Now, if we study the expression for the variational free energy F in some detail, we
canseethatitcanbesplitupintothreeinterpretableterms,
F(o )=D [q(a ,x |o )||p(x ,a ,o )]
1:T KL 1:T 1:T 1:T 1:T 1:T 1:T
=−E [lnp(o |x )]+E D [q(x |o )||p(x )]
q(a ,x |o ) 1:T 1:T q(a |x ) KL 1:T 1:T 1:T
1:T 1:T 1:T 1:T 1:T
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ReconstructionError StateDivergence
+D [q(a |x )||p(a |x )] (4.28)
KL 1:T 1:T 1:T 1:T
(cid:124) (cid:123)(cid:122) (cid:125)
ActionDivergence
IfweapplytheMarkovassumptioninthegenerativemodel,andassumethatthevaria-
tionalposteriorfactorisesacrosstimesuchthatq(a
1:T
,x
1:T
|o
1:T
)=∏
t
T
=1
q(a
t
|x
t
)q(x
t
|o
t
),
then we find that the previous derivation (Equation 4.28) in terms of full trajectories
simplifiesconsiderablyintoasumofindividualtimesteps. Thus,wecanwrite,
T T
F(o )= ∑F (o )= ∑D [q(a ,x |o )||p(x ,a ,o )]
1:T t t KL t t t t t t
t=0 t=0
T
= ∑−E [lnp(o |x )]+E D [q(x |o )||p(x |x ,a )]
q(at,xt|ot t t q(at|xt) KL t t t t−1 t−1
t=0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ReconstructionError StateDivergence
+D [q(a |x )||p(a |x )] (4.29)
KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125)
ActionDivergence
Examining theseterms, we cansee several familiar objectives fromthe machine learn-
ing literature. For instance, the first reconstruction error term is simply just the log-
likelihoodofobservationsexpectedunderthetrajectorybeliefdistribution. Thisterm
iscommonlyoptimizedinallsortsofmachinelearningtasks,ofespecialinteresthere
is its use as part of the objective of the variational autoencoder (Kingma & Welling,
2013)Ifthelikelihoodterm p(o |x )canbethoughtofasthedecoderofavariational
t t
Chapter4. ScalingActiveInference 174
autoencoder,thenconverselytheq(x |o )termcanbethoughtofastheencoder. Sim-
t t
ilarly, the state divergence term is often used as a regulariser or a method of training
a transition model in model-based reinforcement learning. Indeed, we can see the
transitiondynamicsterm p(x |x ,a )asencodingadirectmodelofthetransition
t t−1 t−1
dynamics. Thus, it is straightforward to parameterise these distributions using deep
neuralnetworks. Theq(x |o )and p(o |x )distributionsareparametrisedbytheencoder
t t t t
anddecoderofavariationalautoencoderrespectively,whichcanbetrainedthrougha
reconstructionlossontheenvironmentalobservations. The p(x |x ,a )distribution
t t−1 t−1
canbeencodedasadeepneuralnetworktrainedonthetransitiondynamicsoftheenvi-
ronment. Withthis,weturntothetwotermsintheactiondivergence. Thefirst,q(a |x ),
t t
canbethoughtofasaparametrizedpolicynetwork,ofthekindusedinpolicygradients
or actorcritic methodsin reinforcementlearning. Specifically, it canbe thought ofas a
simplemappingbetweenastateandthecorrectactiontooutputfromthisstate. Sofar
theinferenceprocedurewehavewrittenhasnonotionofrewardsorgoals. Toachieve
adaptive reward-sensitiveaction inference, thismust beaddedsomewhere. Following
common practice in the active inference literature, we encode goals or rewards into
the the action prior by assuming that it is equal to the softmax of the expected free
energiesoffuturetrajectories p(a |x )=σ(γG(x ,a ,o ))whereG istheexpected
t t t:T t:T t:T
freeenergyfunctionalfromactiveinference,γisaprecisionparameterwhichcontrols
exp(−x)
theentropyor‘temperature’ofthesoftmax,andσ(x)= denotesthesoftmax
(cid:82)dxexp(−x)
function. Intuitively,wecanconsiderthisagenttryingtooptimizethesumovertimeof
itsexpectedfreeenergy(andthustheextrinsicandintrinsicvaluecomponentsofthe
EFE),andthenselectingactionswith aprobabilityproportionaltotherelativevalue of
eachchoice. SuchanactionprioreffectivelyimplementsaBoltzmannactiondistribu-
tion,whichhasbeenempiricallystudiedinhumanandanimalchoicebehaviours(Daw
etal.,2006). Theactiondivergencetermthemsimplytriestominimizethedivergence
betweenthevariationalactionpolicyq(a |x )parametrisedbyadeepneuralnetwork,
t t
andthe‘ideal’actiondistribution p(a |x ).
t t
Chapter4. ScalingActiveInference 175
Thekeydifficulty,then,isthecomputationofthesoftmaxedexpectedfreeenergy,as
thisisapathintegraloftheexpectedfree energy ofatrajectoryintothefuture. Unlike
intabularactiveinferenceapproaches,wecannotsimplyenumerateallpossiblefuture
trajectoriesandevaluatethem. Instead,wemakeuseofatrickfromdeepreinforcement
learning, called bootstrapping, which takes advantage of the recursive nature of the
Bellman equation. Here, we first note that the expected free energy, since it can be
simplywrittenasapathintegralthroughtime,obeysasimilarrecursiverelationship,
T
G(o ,x )=∑G (o ,x )
t:T t:T t t t
t
=⇒ G(o ,x )=G (o ,x )+E [G(o ,x )] (4.30)
t:T t:T t t t p(o
t+1
,x
t+1
|xt,at) t+1:T t+1:T
From here, we can expand theexpectedfree energy term using its standard definition
(Friston,Rigoli,etal.,2015b)toobtain,
G (o ,x )=E [lnq(x )−lnp˜(o ,x )]
t t t q(ot,xt) t t t
≈E [lnq(x )−lnq(x |o )−lnp˜(o )]
q(ot,xt) t t t t
≈−E [lnp˜(o )]−E D [q(x |o )||q(x )]
q(ot,xt) t q(ot) KL t t t
≈− E [r(o )] −E D [q(x |o )||q(x )] (4.31)
q(ot,xt) t q(ot) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
RewardMaximization InformationGain
Here, we see that the expected free energy can be (approximately) decomposed into
a reward maximization term and also an information gain term to be maximized.
Thisinformationgainterm,herebetween posteriorandpriorexpectationsoverstates,
canbeseenasanexploration-inducing‘intrinsicreward’inherenttoactiveinference
agents,whichfurnishesthemwithgreaterdirectedexplorationcapabilitiesthanbaseline
reinforcementlearningagentswhichlackthisadditionaltermandonlyfocusongreedy
rewardmaximization.
Puttingthisalltogether,werealizethatwecanexpressthepathintegraloftheexpected
Chapter4. ScalingActiveInference 176
freeenergyrecursivelyas,
G(o x )=E [r(o )]−E D [q(x |o )||q(x )]+E [G(o ,x )]
t:T t:T q(ot,xt) t q(ot) KL t t t q(x
t+1
,o
t+1
|xt,at)q(at|xt) t+1:T t+1:T
(4.32)
Toefficientlyapproximatethis,wecanthenutilizethebootstrappingmethodusedin
deep Q learning. Here we explicitly train a neural network to predict the expected
freeenergyvaluefunctionG(o ,x ). Whileitmayseemlikethisrequiresexplicit
t:T t:T
computation of the path integral, to produce correct targets for the network, in fact it
doesnotduetotherecursivenatureoftheexpectedfreeenergyequation. Let’sdenote
the expected free energy value function predicted by the network as G (o ,x ) which
φ t t
takesasinputsonlythecurrentstateandobservationandusesittopredictthefullpath
integral. Thevaluenetworkhasparametersφ. WecanthenapproximatethetrueEFE
recursiverelationship,
Gˆ(o x )=E [r(o )]−E D [q(x |o )||q(x )]+E [G (o ,x )]
t:T t:T q(ot,xt) t q(ot) KL t t t q(x
t+1
,o
t+1
|xt,at)q(at|xt) φ t t
(4.33)
wherewehavereplacedtherecursivecomputationofthefutureexpectedfreeenergies
withthepredictionfromthevaluenetwork,whichthusdefinesthevalue‘estimate’Gˆ.
Then, totrain thevaluenetwork, we simplyminimize thesquared differencebetween
theestimatedEFEandthepredictedEFE,
L (o ,x )=||Gˆ(o ,x )−G (o ,x )||2 (4.34)
value t t t t φ t t
Thismethodis referredtoasbootstrapping becausetherealexpectedfree energypath
integral is never computed. Instead the targets are also constructed from the value
networkwhicharethenusedtotrainthevaluenetwork. Whilethiscircularrelationship
maymakeitunintuitivethatthismethodcanwork,wecanunderstandwhyitdoeswork
empiricallythroughthefactthatateachstepoftheoptimization,somelocalinformation
abouttheEFEisfedintothenetworkthroughthe(true)computationofG (o ,x ). Thus
t t t
over time, this local information builds up and allows the network to converge to the
Chapter4. ScalingActiveInference 177
correctvaluefunction. Importantly,inQ learning,thereare alsoseveral tricksthathave
been foundto benecessary toensure astable convergence. Principally, usingthe same
valuenetworktoupdateboththetargetsandthepredictionssimultaneouslydoesleadto
instabilitiesandpossibledivergence. Toamelioratethis,weusea‘frozen’copyofthe
value network to compute the targets G (o ,x ) and hold it fixed while the true value
φ t t
networkisupdatedforN steps. AftertheN stepswereplacethefrozenvaluenetwork
withanotherfrozencopyofthenewlytrainedvaluenetwork. Inthisway, thetargetsdo
notchangerapidlyoverthecourseofoptimizationbutneverthelesswillslowlyconverge
towardsthecorrecttargets. Empirically,asinQ-learning,wefoundtheuseofatarget
network a necessity for stable learning of the value function. Given a trained value
networkwhichcanoutputapredictionfortheexpectedfreeenergyvaluefunction,we
can then compute the action prior p(a |x ) for any given state and observation. We
t t
have thus made concrete every single one of the distributions in the variational free
energyfunctionalF,sothatitcanbeexplicitlyevaluated. Onceitcanbeevaluated,its
gradients can be computed through automatic differentiation techniques, and we can
trainalltheparametersoftheagentjointlythroughstochasticgradientdescent.
4.2.1.2 Model
Atthispoint,toavoidlosingsightofthewiderpicture,itisworthwhiletotakeastep
backandlookatthemodelasawhole. Weproposetorepresentandtrainthevariational
posteriorq(x |o )andtheobservationlikelihood p(o |x )asa variationalautoencoder.
t t t t
We additionally represent the dynamics distribution p(x |x ,a ) as a deep neural
t t−1 t−1
network transition model. We represent the variational action posterior q(a |x ) as a
t t
deepneural network‘policy network’and canestimate theaction prior p(a |x )using
t t
thesoftmaxed predictionsofanexpected freeenergyvaluenetworkG (o ,x ). All of
φ t t
these terms are combinedaccording to Equation 4.29 to compute the totalvariational
freeenergywhichformstheunifiedlossfunctionofthemodel. Then,alltheparameters
of each network, except for the value network, are optimized according to a gradient
Chapter4. ScalingActiveInference 178
descent on this total loss. The value network, by contrast, is trained using its own
separatebootstrappingloss.
Thefulldeepactiveinferencealgorithmispresentedbelow,
Algorithm1:DeepActiveInference
InitializeObservationNetworksQ (s|o),p (o|s)withparametersθ.
θ θ
InitializeStateTransitionNetwork p (s|x ,a )withparametersφ
φ t−1 t−1
InitializepolicynetworkQ (a|s)withparametersξ
ξ
InitializebootstrappedEFE-networkG (s,a)withparametersψ
ψ
Receivepriorstates
1
Takeprioractiona
1
Receiveinitialobservationo
1
Receiveinitialrewardr
1
whilet <T do
xˆ ←Q (s|o)(o )x ← p (s|x ,a )(sˆ)a ∼Q (a|s)Receiveobservation
t θ t (cid:100)t+1 φ t−1 t−1 t ξ
o Receiverewardr
t+1 t+1
(cid:92)
G(s,a)←r +E [logx −logxˆ ]+E [G (x ,a )]
t+1 Q(x (cid:100)t+1 t+1 Q(x ,a ) ψ t+2 t+2
t+1 t+1 t+1
F ←E [logp(o|s)]+KL[xˆ ||x ]+
Q(s) t+1 (cid:100)t+1
E [ (cid:82) daQ (a|s)σ(−γG (s,a)(x ))+H(Q (a|s))]θ←θ+αdF
Q(s) ξ ψ t+1 ξ dθ
(cid:92)
φ←φ+αdF L←||G(s,a)−G (s,a)||2 ψ←ψ+αdL
dφ ψ dψ
end
Whileallthe derivations are straightforwardlypresentedfora fullPOMDPmodel, in
our experiments we only utilized simple MDP examples with observable state. This
was because the main difficulty and contribution of this work is the action selection
mechanism, and that the extension to POMDPs is straightforward by just training a
separate variational autoencoder to estimate the latent state given the observations.
Becauseofthisthetwotermsq(x |o )and p(o |x )aresuperfluousandnotcomputed.
t t t t
This leaves the transition model p(x |x ,a ), the policy network q(a |x ) and the
t t−1 t−1 t t
Chapter4. ScalingActiveInference 179
valuenetworkG (x ,o ). Thetransitionmodelisnecessarytocomputetheexploratory
φ t t
informationgaintermintheexpectedfreeenergy.
Werepresentedthepolicynetwork,transitionmodel,andvaluenetworkeachastwo-
layer fully connected neural networks with relu activation functions and a hidden
size of 200 neurons. All networks, except the value network, were optimized by
minimizingjointlythevariationalfreeenergy,throughstochasticgradientdescentwith
theADAMoptimizerandalearningrateof1e-4. Thevaluenetworkwastrainedonthe
bootstrappingobjective(Equation4.34)withthesamelearningrateandoptimizer. A
memorybufferwasusedtostoreall theagent’sexperiencewhichwasreplayedtothe
agentfortrainingatrandominminibatchesofsize64. Nopreprocessingwasdoneon
theinputdataforanyoftheexperiments. Themodelswereimplementedandautomatic
differentiation was performed in the Flux.jl machine learning framework 5. We used
a target network to stabilize the learning of the value network. The target network
was copied from the value network every 50 episodes. Each episode consisted of a
full iteration through the replay buffer. Additionally, as is common in reinforcement
learningtasks,weutilizedatemporaldiscountonthereward. Ourtemporaldiscount
factorwas0.99forallmodels.
4.2.1.3 Results
Wecomparedtheperformanceoftheactiveinferenceagenttotwostrongmodel-free
reinforcement learning baselines. Deep-Q learning (V. Mnih et al., 2013, 2015), and
anactor-critic(V.Mnihetal.,2016)architecture. DeepQlearningparametrisestheQ
function usinga deepneural networkwith a similar bootstrapping objective tothe EFE
valuefunctionthat wecomputedearlier, exceptonlyusingreward. For theQ-learning
agent we utilized the same hyperparameters and value network size that we used for
the active inference agent, to enable afair comparison. Moreover,we implemented the
5The code to reproduce all experiments can be found at:
https://github.com/BerenMillidge/DeepActiveInference.
Chapter4. ScalingActiveInference 180
Q-learningagentwithBoltzmannexploration(Cesa-Bianchietal.,2017),whichisvery
similartothesoftmaxfunctionusedtocomputetheactionprior.
Wealsocomparedthedeepactiveinferenceagenttoanactor-criticarchitecture. Unlike
aQ-learningagentwhichcomputesactionsdirectlybymaxingovertheQ-function,the
actor-criticarchitecturemaintainsaseparate‘actor’orpolicynetworkwhichistrained
ona policygradient objectivebasedon valuefunctionestimates learntby the‘critic’–
a value function estimator trained through bootstrapping. The policy network and value
network ofthe actor-critic architectureswere trained usingthe samehyperparameters
andarchitectureastheactiveinferenceagent,toenableafaircomparisonofthemethods.
We compared theperformanceoftheactiveinferenceagentonthreecontinuouscontrol
environments from the OpenAI Gym (Brockman et al., 2016). These were CartPole,
Acrobot, andLunarLander. Thecartpole environmentis simpleand requiresthe agent
to balance a pole atop a cart by pushing the cart either to the left or to the right. The
state-spaceofthecartpoleenvironmentisafourdimensionalvector,comprisingthecart
positionangleandvelocity,aswellastheangleandvelocityofthepole. Arewardof
+1isgivenforeachtimesteptheepisodedoesnotendupto200steps. Theepisodewill
endearlyifthecartismorethan2.4unitsfromthecentre(thecarthasleftthescreen),
orelsethepoleangleismorethan15degreesfromvertical(thepolehasfallendown).
The acrobot environment requires the agent to learn to swing up and balance a triple
jointedpendulum. Ithasastate-spaceof6dimensionswhichrepresenttheanglesand
velocitiesofthejoints. Theactionspaceisathreedimensionalvectorcorresponding
to the force the agent wishes to exert on each joint. The reward schedule is -1 for
every timestep the pendulum is above the horizontal, and 0 if it is above horizontal.
Theacrobotisachallengingtaskforexploration,sincepurelyrandomactionsarevery
unlikelytoleadtoanyreward. Thelunarlanderenvironmentrequirestheagenttolearn
tolandasimulatedspacecraftonasurfacewithinatargetregioninaNewtonianphysics
environment. It hasa 8-dimensional state-space anda four-dimensional action space,
Chapter4. ScalingActiveInference 181
withactionscorrespondingtofireleftengine,firerightengine,fireupwardsengine,and
extenddocking legs. The agentreceives arewardof 100forlanding onthelaunchpad
(locatedat(0,0)),anda-0.3rewardforeverytimesteptherocket’senginesarefiring. It
doesnotreceiveapenaltyforsimplydoingnothing.
We comparethe performanceof the active inference, Q-learning,and actor-criticagent,
in terms of pure reward obtained in each environment 6. We ran for 15000 episodes
over20randomseedsforeachagentandplottedmeanrewardsobtainedbelow,
Figure 4.2: Comparison of the mean reward obtained by the Active Inference agent
comparedtotworeinforcementlearningbaselinealgorithms–Actor-CriticandQlearning
ontheCartPoleenvironment. Wedemonstratethelearningcurvesover2000episodes,
averagedover5differentseeds. 500isthemaximumpossiblereward. Weseethatwhile
the vanilla actor critic agent initially learns faster, over a long time horizon, the active
inference agent outperforms it – and both perform better than the vanilla Q learning
agent.
6Notethiscomparisonbasedpurelyonrewardactuallypenalizestheactiveinferenceagenttosome
degree,sinceitdoesnotsimplyoptimizethereward,butalsobysatisfyingtheepistemicdrivesfurnished
bytheEFEobjectivefunction
Chapter4. ScalingActiveInference 182
Figure4.3: ComparisonofActiveInferencewithstandardreinforcementlearningalgo-
rithmsontheAcrobotenvironment. Hereweseethelearningcurvesplottedoverfive
seedsover20000episodes. Themaximumpossiblerewardinthisenvironmentwas0,
sonoagentsareoptimal. Weseeagainthatactiveinferenceoutperformstheothertwo
methodsconsistently.
Chapter4. ScalingActiveInference 183
Figure4.4: ComparisonofActiveInferencewithreinforcementlearningalgorithmsonthe
Lunar-Landerenvironment. Learningcurvespresentedover15000episodes,averaged
over5seeds. Herethevanillapolicygradientalgorithmstronglyoutperformstheothers,
forunclearreasons,althoughactiveinferenceisstillcomparablewiththeotherstandard
reinforcementlearningalgorithms. Ascoreof200isoptimal.
Sincetheactiveinferenceagentpossessesseveraldistinctfeaturesbeyondthestandard
actor critic architectures, we performed an ablation study to understand whence its
boostinperformancearose.
Chapter4. ScalingActiveInference 184
Figure4.5: WecomparethefullActiveInferenceagent(entropyregularization+transition
model) with an Active Inference agent without the transition model, and without both
the entropy term and the transition model). We see that while removing the transition
modelappearstohavelittleeffect,removingtheentropyregularisationtermsubstantially
impairs performance. This may be due to the entropy term aiding in staving off policy
collapse.
Chapter4. ScalingActiveInference 185
Figure 4.6: Comparison of the rewards obtained by the fully ablated Active Inference
agentwithstandard reinforcement-learning baselinesofQ-learningandActor-Criticon
the CartPole environment. Learning curves are averaged over 5 seeds. We see that
despitebeingfullyablated,theactiveinferenceagentcontinoustoperformcomparably
withstandardreinforcementlearningagents.
We see that the key factor in the superior performance of the active inference agent
is the additional action entropy term that is additionally optimized. This provides
additional empirical confirmation to the success of control-as-inference approaches
in the reinforcement literature which similarly utilize such an entropy regularisation
term. Perhaps surprisingly, we found little effect ofthe epistemic terms in theEFE on
totalperformance. Wehypothesisethatthiswasfortworeasons. Firstly,thetasksthat
were tested possessed a relatively dense reward structure, sufficient to be learned by
standard reinforcement learning agents utilizing only random exploration strategies,
andthusthatmoreadvancedandpowerfulexplorationstrategiesarelikelyunnecessary
forsuchtasks. Secondly,theepistemicactiontermwastheinformationgainbetween
the prior and posterior states, which is effectively a measure of the predictive success
of the transition model. Importantly, we found that the transition model very rapidly
convergedduringtraining,muchfasterthanthepolicyorvaluenetwork,andthusthat
Chapter4. ScalingActiveInference 186
throughoutmostofthetrainingperiod,theexplorationtermwasthusnegligible.
4.2.1.4 InterimDiscussion
Inthissection,wehavedemonstratedhowactiveinferenceapproachescanbestraightfor-
wardlyscaledupbyparametrizingthelikelihood,inference,andtransitiondistributions
with deep neural networks, and then additionally approximating the path integral of
the expected free energy with an amortized value network. This is possible because
the expected free energy satisfies a similar Bellman-like recursion to the reward in
reinforcementlearning,becauseitisfactorizableacrosstimeintoasumofindependent
time-steps. Importantly, wehavedemonstratedthat bytakingthisapproach, ouractive
inference agent can handle complex machine learning benchmark tasks just as well
asseveralcoredeepreinforcementlearningapproaches,thusrenderingitsignificantly
morescalablethanpreviouseffortsintheliteraturewhichhavegenerallybeenrestricted
tosmall,discrete,andstraightforwardlyenumerablestatespaces.
Moreover,wehaveshownthatouralgorithmiscompetitive,andinsomecasessuperior,
to standard baselinereinforcement learning agents ona suite of reinforcement learning
benchmarktasksfromOpenAIGym(Brockmanetal.,2016). Whileouractiveinference
agentperformedworsethandirectpolicygradientsontheLunar-Landertask,webelieve
this is due to the inaccuracy of the expected-free energy-value-function estimation
network,sincethepolicygradientmethoduseddirectandunbiasedmonte-carlosamples
of the reward rather than a bootstrapping estimator. Since the performance of Active
Inference,atleastinthecurrentincarnation,issensitivetothesuccessfultrainingofthe
EFE-network, we believe that improvements here could substantially aid performance.
Moreover,itisalsopossibletoforego orcurtailtheuseofthebootstrappingestimator
and use the generative model to directly estimate future states and the expected-free
energythereof,attheexpense ofgreatercomputationalcost. Wetakethisapproachof
using the transition model to generate sample rollouts and using these to compute a
Monte-CarloestimateoftheEFEpathintegralinthenextsection,wheretheseestimates
Chapter4. ScalingActiveInference 187
areusedtoinformmodel-predictiveplanning.
An additional advantage of our approach is that due to having the transition model,
it is possible to predict future trajectories and rewards N steps into the future instead
of just the next time-step. These trajectories can then be sampled from and used to
reducethevarianceofthebootstrappingestimator,whichshouldworkaslongasthe
transition model is accurate. This N could perhaps even be adaptively updated given
the current accuracy of the transition model and the variance of the gradient updates.
Thisisawayofcontrollingthebias-variancetrade-offintheestimator,sincethefuture
samples should reduce bias while increasing thevariance of the estimate, and alsothe
computationalcostforeachupdate.
Anotherimportantparameterinactiveinferenceistheprecision(Feldman&Friston,
2010; Kanai, Komura, Shipp, & Friston, 2015), which in the discrete-state-space
paradigm corresponds to the inverse temperature parameter in the softmax and so
controls the stochasticity of action selection 7. In all simulations reported above we
used a fixed precision of 1. However, in the discrete state-space case, the precision
is often explicitly optimized against the variational free energy, and the same can be
done in our deep active inference algorithm. In fact, the derivatives of the precision
parameter canbe computed automaticallyusing automatic differentiation. Determining
the impact of precision optimization on the performance of these algorithms is a
potentiallyworthwhileavenueforfuturework.
While we did not find that using the epistemic reward helped improve performance
onourbenchmarks,thiscould beduetothesimplicityofthetasksweweretryingto
solve, forwhich randomexploration is sufficient. Inthe nextsection, we demonstrate
thattheepistemicaffordancesengenderedbytheuseoftheEFEvaluefunctionprove
instrumentalinattaininghighperformanceinsparse-rewardtasks.
7In the continuous predictive coding paradigm the precision modulates the ‘importance’ of the
predictionerrors.
Chapter4. ScalingActiveInference 188
Theentropyregularizationtermwhichemergesdirectlyfromthemathematicalformu-
lation of active inference proved to be extremely important, and was often the factor
causing the superior performance of our active inference agent to the reinforcement
learning baselines. This entropy term is interesting, since it parallels similar develop-
ments in reinforcement learning, which have also found that adding an entropy term to
thestandardsumofdiscountedreturnsobjectiveimprovesperformance,policystability
andgeneralizability(Haarnoja,2018;Haarnoja,Tang,Abbeel,&Levine,2017). This
is of even more interest given that these algorithms can be derived from a similar
variationalframeworkwhichalsocastscontrolasinference(Levine,2018). Later(in
Chapter5),wediscussinsignificantdetailhowsuchparadigmsrelatetoactiveinfer-
ence. Additionally,manyofthedifferencesbetweenactiveinferenceandthestandard
policy gradients algorithm – such as the expectation over the action, and the entropy
regularization term – have been independently proposed to improve policy gradient
and actor critic methods (Fujimoto et al., 2018). The fact that these improvements
fallnaturallyoutoftheactiveinferenceframeworkcouldsuggestthatthereisdeeper
significance to the probabilistic inference formulation espoused by active inference.
Theotherkeydifferencebetweenpolicygradientsandactiveinferenceistheoptimiza-
tionofthepolicyprobabilitiesversusthelogpolicyprobabilities,andmultiplyingby
the log of the probabilities of the estimated values, rather than the estimated values
directly. It is currently unclear precisely how important these differences are to the
performanceofthealgorithm,andtheireffectonthenumericalstabilityorconditioning
of the respective algorithms, and this is also an important avenue for future research.
However, the comparable performance of active inference to actor-critic and policy
gradient approaches in our results suggest that the effect of these differences may be
minor.
Chapter4. ScalingActiveInference 189
4.2.2 Model-based: Reinforcement Learning through Active Infer-
ence
Whilethe lastsectionfocusedon scalingactiveinferenceusingmodel-free reinforce-
ment learning methods, here we focus on scaling active inference in a way inspired
bymodel-basedreinforcementlearningmethods. Model-basedreinforcementlearning
is perhaps a better fit for the central ideas in the classical active inference. Tabular
active inference, after all, is a model-based algorithm which explicitly evaluates and
optimizesfuture plansusingexplicit models ofthe environmentaldynamics. Moreover,
astabular active inferenceexplicitlyreplans ateverytime-step, itcan beconsideredto
beamodel-predictivecontrolalgorithm. Infact,theexplicitenumerationandevaluation
ofeverypossiblepolicycanperhapsbestbethoughtofasatrulyexhaustiveplanning
algorithm.
Similartoourpreviousapproachdescribedearlier,weproposetodevelopdeepactive
inference methods which utilize deep neural networks to parametrize key distributions
from the active inference framework. Specifically, we maintain deep neural network
representationsoftheobservationlikelihooddistribution p(o |x )aswellasthetransi-
t t
tionmodelwhichparametrisesthedynamicsmodeloftheenvironment p(x |x ,a ).
t t−1 t−1
The major difference is how the action policy q(a |x ) is handled. In the previous
t t
model-freeapproach,thisdistributionwasrepresentedasanindependentpolicyneural
networkwhichwastrainedagainsttheactionpriorwhichrepresentedthesoftmaxof
the expected free energy value function in an actor-critic like fashion. Here, we treat
theactionposteriorastheoutputofamodel-basedplanningalgorithm.
Specifically, and quite elegantly, we can show that under certain conditions of the
generativemodelofthefuture,thatwecanderivetheoptimalplanasasoftmaxover
the expected free energy in the future, (which was merely assumed to be the action
prior in the model-free case). Moreover, we then show that this path integral can be
approximatedbymonte-carlosamplingintheformofamodel-basedplanningalgorithm
Chapter4. ScalingActiveInference 190
whichsamplesandevaluatesgivenpotentialfuturetrajectoriesusingthetransitionand
rewardmodelspossessedbytheagent.
(cid:16) (cid:17)
F ˜ =D q(o ,x ,θ,π)(cid:107)p˜(o ,x ,θ)
π KL t t t t
=E [logq(o ,x ,θ|π)+logq(π)−logp˜(o ,x ,θ,π)]
q(ot,xt,θ,π) t t t t
(cid:104) (cid:105)
=E E [logq(π)−[logp˜(o ,x ,θ)−logq(o ,x ,θ|π)]
q(π) q(ot,xt,θ|π) t t t t
(cid:104) (cid:105)
=E logq(π)−E [logp˜(o ,x ,θ)−logq(o ,x ,θ|π)]
q(π) q(ot,xt,θ|π) t t t t
(cid:104) (cid:105)
=E logq(π)− (cid:2) −E [logq(o ,x ,θ|π)−logp˜(o ,x ,θ)] (cid:3)
q(π) q(ot,xt,θ|π) t t t t
(cid:104) (cid:2) (cid:3)(cid:105)
=E
q(π)
logq(π)−loge − −E q(ot,xt,θ|π) [logq(ot,xt,θ|π)−logp˜(ot,xt,θ)]
(cid:104) (cid:0) (cid:1)(cid:105)
=E logq(π)−loge −D KL q(ot,xt,θ|π)(cid:107)p˜(ot,xt,θ)
q(π)
(cid:16) (cid:0) (cid:1)(cid:17)
=D q(π)(cid:107)e
−D
KL
q(ot,xt,θ|π)(cid:107)p˜(ot,xt,θ)
KL
(cid:16) (cid:17)
=D
q(π)(cid:107)e−F˜
π (4.35)
KL
Itisimportanttonotethatherewedonotusetheexpectedfreeenergyasourobjective,
unlikeinstandardactiveinference. Instead,weusetherecentlyintroducedobjective:
thefreeenergyoftheexpectedfuture(FEEF).Thisobjectivemaintainstheexploratory
information gain terms of the traditional expected free energy while possessing a clear
mathematical origin with strong intuitive grounding. Specifically, the FEEF can be
definedas,
FEEF=F ˜ =D [q(o ,x ,θ|π)||p˜(o ,x ,θ)
π KL t t t t
≈E (x |π)D [q(o |x )||p˜(o )]−E D [q(x |o ,θ)||q(x )]
q t KL t t t q(ot;θ) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue StateInformationGain
− D [q(θ|x )||q(θ)] (4.36)
KL t
(cid:124) (cid:123)(cid:122) (cid:125)
ParameterInformationGain
Where wecan see thatthe FEEF canbe splitinto approximately threeterms – thelike-
lihooddivergencetermwhichmeasureshowmuchtheexpectedobservationsdiverge
Chapter4. ScalingActiveInference 191
fromthedesiredobservations,andwhicheffectivelyencodesrewardorutilityseeking
behaviour,andaninformationgaintermtobemaximizedwhichinducesexploratory,
uncertainty reducing behaviour. Since the FEEF objective includes both latent states
x and parameters θ, we actually obtain two separate information gain terms, one for
thestates andone fortheparameters. Thecore findingand argumentofthis partof the
chapter, and an example of what the theory of active inference can bring to contem-
porarydeepreinforcementlearning,isthattheexploratoryinformation-seekingterms
furnishedbyactiveinferenceobjectivessuchastheexpectedfreeenergy,orfreeenergy
of the expected future, by inducing purposeful and goal-directed exploratory behaviour,
theycanoutperformtraditionalrandomexplorationonanumberofchallengingrein-
forcementlearningtasks,andtheiradvantagesbecomeespeciallyapparentinthecaseof
sparserewardswhererandomexplorationisoftensimplyinsufficienttofindanygood
solutionsin areasonable time. Moreover, recentwork inthe literature,which utilizes
exploratory objectives, but in afirst exploratory, and then exploitatoryphase, we argue
thatitisnecessarytocombinethetwoobjectivestobejointlyoptimized. Inthiswaythe
agentisfurnishedwith adesireforgoaldirectedexploration. Itisnotrewardedsimply
forreducinganyuncertainty,butonlyuncertaintythatalsoexistsinrewardingregions
inthestate-space. Inthisway,agentsexplorepreciselyonlyasmuchasneeded,thus
providingasteptowardsapracticalsolutiontotheexploration-exploitationtradeoff.
4.2.2.1 Model
As in previous work, we extended active inference by using deep neural networks to
parametrize key densities. Our model utilized a neural network transition model to
modelthedistribution p(x |x ). SincetheFEEFobjectiverequirestheevaluation
t t−1,a
t−1
)
of an information gain term over the parameters (denoted θ) of the transition model,
wemaintainedanapproximatedistributionovertheparametersθusinganensembleof
transition models with independently initialized parameters, and trained on different
batchesfrom thereplaybuffer. Thisensembleapproach hasbeenfound tobe widely
Chapter4. ScalingActiveInference 192
usefulinmodel-basedreinforcementlearningandtoofferasuperiorrepresentationof
thetrue posteriorovertheparameters thancompetingmethods suchas Bayesian neural
networks. Importantly, utilizing an explicit ensemble of transition models allows the
estimatedposterior overthe parametersto bemultimodal, asopposed totheunimodal
Gaussianassumption implicitinthe Bayesianneuralnetworksapproach (Gal,McAllis-
ter,& Rasmussen,2016;Tran, Dusenberry,van derWilk,& Hafner,2018). Moreover,
anensembleof modelshasbeenfound empirically tohelpavoidoverfitting inlow-data
regimes,whicharealsowhentheadvantagesofmodel-basedreinforcementlearning
are most apparent. Each element of the transition model ensemble was implemented
asanindependentneuralnetworkwithtwohiddenlayerswith400neuronseach. The
networks used the swish activation function. The transition networks predicted the
difference in the next state (Shyam et al., 2019) instead of the next state, as this has
been found to help capture environmental dynamics more accurately in practice. Since
weareevaluatingfuturesimulatedrollouts,wecannotsimplyrelyonenvironmentally
provided ‘true rewards’. While many methods in the literature assume the existence
ofaknownrewardfunctionwhichcanbequeriedevenforcounterfactualorsimulated
trajectories(Chuaetal.,2018;Hafneretal.,2018),wedonot,assucharewardoracleis
unrealisticinmanyifnotmostsituations. Instead,welearntarewardmodelbasedupon
previousinteractionswiththerealenvironment,andthenusedtherewardmodeltoscore
proposedtrajectories. Therewardmodelwasparametrisedbyatwolayermulti-layer
perceptron network with 400 units in the hidden layer and a relu activation function.
Therewardmodelwastrainedonamean-squareerrorlossbetweenactuallyobserved
rewards foragiven state, andthereward predictedbythereward model. Importantly,
theFEEF objective definesthe extrinsicrewardto betheKL divergencebetweenthe
observationlikelihoodandthedesiredobservationdistribution. Sinceourmodelwas
situatedpurelyinanMDPsettingwithfullyobservedstate,theonlyobservationwasthe
reward,andthustherewardmodeldoubledasthelikelihoodmodel. Wesetthedesired
reward observations to be a Gaussian distribution with a variance of 1 centred at the
Chapter4. ScalingActiveInference 193
maximumpossiblerewardfortheenvironment. Sinceweinterpretthepredictionsofthe
rewardmodel asrepresenting themean ofa Gaussiandistribution, wecananalytically
calculate theKL divergence term. This allowed us to straightforwardlycompute and
optimizetherewardmaximizationpartoftheFEEFobjective. Similarly,toevaluatethe
informationgaintermsoftheFEEFobjective,wecanrewriteitinamoretractableway,
E D (cid:0) q(θ|x )(cid:107)q(θ) (cid:1)
q(xt|θ) KL t
=E (cid:2) logq(θ|x )−logq(θ) (cid:3)
q(xt|θ)q(θ|rs) t
=E (cid:2) logq(x |θ)+logq(θ)−logq(x )−logq(θ) (cid:3)
q(xt,θ) t t
=E (cid:2) logq(x |θ)−logq(x ) (cid:3)
q(xt,θ) t t
=E (cid:2) logq(x |θ) (cid:3) −E (cid:2) logE q(x |θ) (cid:3)
q(θ)q(xt|θ) t q(θ)q(xt|θ) q(θ) t
=−E H (cid:2) q(x |θ) (cid:3) +H (cid:2)E q(x |θ) (cid:3) (4.37)
q(θ) t q(θ) t
Ineffect,theparameterinformationgaintermdecomposesintoanentropyoftheaverage
state minus the average of the entropy. The average of the entropy can be computed
semi-analyticallysince eachtransitionmodel ensemblemodelsa Gaussiandistribution,
whichhasaknownandanalyticallycalculableentropy. Then,theaveragecansimplybe
performedbydirectlyaveragingtheentropiesofeachmemberoftheensembletogether.
Theentropyoftheaveragetermismoredifficult,sincetheaverageofmanydifferent
GaussiandistributionsisnotnecessarilyGaussian. Assuchweapproximateitwitha
nearestneighbourentropyapproximation(Mirchev,Kayalibay,Soelch,vanderSmagt,
&Bayer,2018)whichwefoundworkedwellinpractice.
Totrainthemodel,weoptimizedtherewardandtransitionmodelsondatatakenfroma
replay buffer. They were trained with stochastic gradient descent on their respective
lossfunctionsusinganegativeloglikelihoodloss. Wecold-startedthetrainingofthe
agentattheendofeveryepisode,aswefoundthatthisledtomoreconsistentbehaviour
and performance. We initialized each episode with a dataset D taken from an agent
Chapter4. ScalingActiveInference 194
witharandompolicy,toensuresomedegreeoftransitionandrewardmodelaccuracy
beforebeginningwiththemodel-basedplanning.
To computeactionsweusedamodel-based planner(CEM)(Rubinstein,1997)within
the model-predictive control paradigm. The CEM planning algorithm generates and
evaluatestherewardofalargenumberofactiontrajectories,thentakesthemeanand
variance of some elite set of actions (usually the top 10) of the actions and restarts
theevaluationusingactionssampledfrom aGaussiandistribution withthismeanand
variance. Foreverytimestep,a 30stepplanninghorizonwasusedresultingina 30-step
actionplan,ofwhichthefirstactionwasexecuted. Inaccordancewithmodel-predictive
control,wereplanateachstep. FortheCEMalgorithm,weused700candidateaction
sequencestobeevaluatedineachiteration,for7iterations. Totrainthetransitionand
rewardmodelsweusedtheADAMoptimizerwithalearningrateof1e-4andtrained
Chapter4. ScalingActiveInference 195
for100epochs.
Algorithm2:Inferenceofq(π)
Input: PlanninghorizonH |OptimisationiterationsI |Numberofcandidate
policiesJ |Currentstates |Likelihood p(o |s )|Transitiondistribution
t τ τ
p(s |s ,θ,π)|ParameterdistributionP(θ)|Globalprior p˜(o )
τ τ−1 τ
Initializefactorizedbeliefoveractionsequencesq(π)←N(0,I).
foroptimisationiterationi=1...I do
SampleJ candidatepoliciesfromq(π)
forcandidatepolicy j=1...J do
π(j) ∼q(π)
−F ˜ j =0
π
forτ=t...t+H do
q(s |s ,θ,π(j))=E (cid:2) p(s |s ,θ,π(j)) (cid:3)
τ τ−1 q(s
τ−1
|θ,π(j)) τ τ−1
q(o |s ,θ,π(j))=E (cid:2) p(o |s ) (cid:3)
τ τ q(s
τ
|θ,π(j)) τ τ
−F ˜ j ←−F ˜ j +E (cid:2) D (cid:0) q(o |s ,θ,π(j))(cid:107)p˜(o ) (cid:1)(cid:3) +
π π q(s
τ
,θ|π(j)) KL τ τ τ
H[q(s |s ,θ,π(j))]−E (cid:2) H[q(s |s ,π(j),θ)] (cid:3)
τ τ−1 q(θ) τ τ−1
end
end
q(π)←refit(−F ˜ j )
π
end
returnq(π)
4.2.2.2 Results
Wetestedtheperformanceofouralgorithmagainststrongmodel-freeandmodel-based
baselinesonanumberofchallengingcontroltasks8. Weutilizedfirstthemountain-car
taskfromOpenAIGym,whichrequirestheagenttosteeracarona1Dlinetoagoal.
Thistaskisdifficultbecausetheagentmustfirstmovethecarawayfromthegoalupa
8Different tasks are utilized in this section compared to the previous one because here we are
dealingwithcontinuousactionswhileintheprevioussectionwedealtonlywithlearningdiscrete-action
controllers.
Chapter4. ScalingActiveInference 196
hill,tobuild upmomentumtobeabletogetoverthelargerhill. Thisposes adifficult
exploration problem which purely random exploration agents struggle to solve. By
contrast,ouragentcansolvethistaskinstantaneously,withinonlyasingleepisode,due
toitsgoaldirectedexploration.
1.0
0.5
0.0
0 20
Episodes
snruter
egarevA
A Mountain Car B Cup Catch C Cheetah Flip D Cheetah Run
400
1000 500
250 200
500
0 0
0
250
200
0 20 40 0 50 100 0 50 100
Episodes Episodes Episodes
FEEF Reward Variance SAC
Figure4.7: (A)MountainCar: Averagereturnaftereachepisodeonthesparse-reward
MountainCartask. Ouralgorithmachievesoptimalperformanceinasingletrial. (B)Cup
Catch: Averagereturnaftereachepisodeonthesparse-rewardCupCatchtask. Here,
resultsamongstalgorithmsaresimilar,withallagentsreachingasymptoticperformance
inaround20episodes. (C&D)HalfCheetah: Averagereturnaftereachepisodeonthe
well-shapedHalfCheetahenvironment,fortherunningandflippingtasks,respectively.
WecompareourresultstotheaverageperformanceofSACafter100episodeslearning,
demonstrating our algorithm can perform successfully in environments which do not
requiredirectedexploration. Eachlineisthemeanof5seeds andfilledregionsshow+/-
standarddeviation.
Sincethemountaincarenvironmentpossessesonlyatwodimensionalstatespace,we
canexplicitlyplotandcomparethedegreeofthespacecoveredbytheactiveinference
agentvsthegreedyrewardmaximizingreinforcementlearningagent.
Wethusseethattheexploratorydrivesinherentintheactiveinferenceagentpropelit
toexploreasignificantlylargerfractionofthestate-spacethantherewardmaximizing
agent,anditisthisexplorationwhichallowsittostumbleuponthegoalandthusrapidly
learntosolvethetask. Bycontrast,therandomexplorationreward-maximizingagentis
unabletoescapethelocalminimumatitsstartlocationbyapurelyrandomwalk,since
Chapter4. ScalingActiveInference 197
0.06
0.04
0.02
0.00
0.02
0.04
0.06
1.0 0.5 0.0 0.5
Position
yticoleV
State Coverage (FEEF)
0.06
0.04
0.02
0.00
0.02
0.04
0.06
1.0 0.5 0.0 0.5
Position
yticoleV
State Coverage (Reward)
100
80
60
40
20
0
FEEF Random
)tnecrep(
egarevoc
ezaM
A B C Ant Maze Coverage
Figure 4.8: (A & B) Mountain Car state space coverage: We plot the points in state-
spacevisited bytwo agents-one thatminimizes thefree energyofthe expectedfuture
(FEEF)andonethat maximisesreward. Theplotsarefrom20 episodes andshowthat
theFEEFagentsearchesalmosttheentiretyofstatespace,whiletherewardagentis
confinedto aregion thatcanbe reachedwith random actions. (C) Ant Maze Coverage:
We plot the percentage of the maze covered after 35 episodes, comparing the FEEF
agenttoanagentactingrandomly. Theseresultsaretheaverageof4seeds.
todosorequirestoomanycorrectmovesinasequencethancanbegeneratedwithina
reasonabletime. Wealsotestedouragentonsparserewardversionsoftwochallenging
controltasks. Inthefirsttask,CupCatch,theagentmustactuateacuptocatchaball
thrownatit. Theagentreceivesarewardof1ifitcatchestheball,andarewardof0if
itdoesnot. Similarly,intheHalf-Cheetahenvironment,theagenttakescontrolofthe
limbsofarunningplanarcheetahinasemi-realisticphysicssimulation. Itsgoalisto
maximizethevelocityatwhichthecheetahmovesforward,ideallybyrunning. Wealso
experimented with a no-reward environment to test the pure exploration capabilities
of our agent. For this we utilized the ant-maze environment in which the agent must
actuateanant-likerobotinordertoexploreasmuchofamazeaspossible. Theagent
receivesnoextrinsicrewardsatanypointinthistask.
Overall, we see thatthe activeinference andrewardmaximization agentperforms simi-
larlyinthecup-catchenvironment. Wehypothesisethatthisisbecause,evenwiththe
sparsereward,therewardiseasyenoughtoobtainevenwithpurelyrandombehaviour.
Similarly,onthehalf-cheetahbenchmark,ourmodelperformssignificantlybetterthan
Chapter4. ScalingActiveInference 198
themodel-freeSACagent,due tothesuperiorsample efficiencyofmodel-basedover
model-free methods, however it also does not show much significant improvement
compared to reward maximizing model-based baselines. However in the ant-maze
environment, our agent evinces significantly more exploration capacity andexplores a
substantiallylargerproportionofthe total statespacethananyotheragent, onceagain
demonstratingitssuperiorexploratorycapabilities.
4.2.2.3 InterimDiscussion
Sofar,inthischapter,wehaveappliedanactiveinferenceperspectivetoreinforcement
learningandrecastthetraditionalRLobjectiveintoamoreactive-inference-inspiredone;
reformulatingtherewardmaximizationobjectiveasthatasminimizingthedivergence
between predicted and desired probabilistic futures. From this starting point, we can
deriveanovelalgorithmthatexhibitshighperformanceaswellasrobustness,flexibility,
andsampleefficiencyinanumberofenvironmentsthatareknowntobeexceptionally
challenging for traditional reinforcement learning methods, while also performing
comparablyinenvironmentswherestandardRLmethodsdowell.
We believe that through these two studies, we have convincingly demonstrated that
activeinferenceapproachescanbesuccessfullyscaledtolevelsequaltocontemporary
reinforcementlearningmethodsin bothmodel-freeandmodel-basedparadigms. More-
over, we hope to have shown some ways in which the integration of active inference
and reinforcement learning can provide novel and useful perspectives to inform and
inspireworkinthedeepreinforcementlearningcommunity. Wedemonstratethatthe
exploration-inducing properties of active inference objective functionals such as the
ExpectedFreeEnergyandtheFreeEnergyoftheExpectedFuturearehighlybeneficial
especiallyinmorechallengingtaskswithsparseornorewards,whilealsoperforming
comparably to pure reward maximization approaches on dense reward tasks that can
be solved with purely random exploration. Moreover, combining both exploratory
and reward maximizing terms in a single objective function and jointly optimizing
Chapter4. ScalingActiveInference 199
them is crucial to derive algorithms which can simply learn to solve a task without
separate exploratory and exploitatory phases as in much of the literature (Shyam et
al.,2019),althoughtheideaofinducingexplorationbyoptimizinganepistemicterm
(Oudeyer & Kaplan,2009; Pathak et al., 2017;Schmidhuber, 2007) has been applied
previously in reinforcement learning, and that can be competitive directly with both
purely exploratory and purely exploitatory tasks in the regions where each of these
methodsexcel.
An additional idea, inspired by active inference, which could inform reinforcement
learning perspective is the idea of representing preferences, instead of scalar reward
values,asadistributionoverobservations. Webelievethatthismodellingchoicecould
enable greater flexibility in learning non-scalar, non-monotonic reward functions, as
well as providing a natural, Bayesian framework for handling the case of unknown,
uncertain, or nonstationary reward distributions. We believe that in many naturalistic
settings,especially forbiologicalorganisms, rewards arenotsimply givena-priori by
some known oracle, but are task-dependent, contingent, often highly uncertain, and
nonstationary. ActiveinferenceprovidesastraightforwardBayesianaccounttohandle
precisely such conditions. Although in this work, and in much of the literature, we
instead take extremely simplifying assumptions such as p˜(o)=exp(−r(o)) to make
active inference as close as possible to reinforcement learning, future work should
insteadheadintheoppositedirectionandtrytodeliberatelyexploretheregionswhere
active inference approaches offer greater flexibility than the traditional reinforcement
learningparadigm,andthusdemonstratetheadvantagesofactiveinferenceapproaches
there.
4.2.3 Related Work
Before our work in deep active inference there was a small amount of prior work
whichisimportanttoreview. TheseminalpaperwhichbeganthisfieldisDeepActive
Chapter4. ScalingActiveInference 200
Inference (Ueltzhöffer, 2018),which initiated theidea ofusing deep neuralnetworks
to approximate key densities within the active inference paradigm. This paper uses
small neuralnetworks to parametrisethe transition dynamicsand likelihood in active
inference, and uses genetic algorithms to directly optimize a policy module of the
expected free energy in a black-box fashion. This is because, under their problem
setup, the expected free energy depends on the environmental dynamics and is thus
nondifferentiable,assumingtheenvironmentitselfisunknown. Theyproducedasimple
agentwhichcanlearntosolvethemountaincarproblemfromOpenAIgymaftermany
iterations. Thispaperwasmyinspirationtodivedeeperintotryingtounderstandthe
commonalitiesanddifferencesbetweendeepactiveinferenceanddeepreinforcement
learning.
Another piece of work, arisingcontemporaneously with my own initial work (Millidge,
2019a,2020),wasthatbyÇatal,Nauta,Verbelen,Simoens,andDhoedt(2019). They
alsoparametrisedthelikelihoodandtransitiondynamicsusingdeepneuralnetworks,
and additionally explicitly utilized an expected free energy value function. However,
instead of directly solving the sparse-reward challenge implicit in the mountain-car
environmenttheytestedtheiragentin,they instead constructeda hand-crafted ‘state-
prior’ generated from expert-rollouts which already directly solved the task, thus
providinganeffectivelydenserewardsignalforthissparserewardproblem.
SomeotherrelatedworkisthatofCullenetal.(2018)whoalsoappliedactiveinference
tomorecomplexnon-toyenvironments. Theytrainedanactiveinferenceagenttoplay
a subset of DOOM – the ‘take-cover’ environment in OpenAI Gym. However, they
still fundamentally utilised the discrete-state-space active inference formulation by
discretisingthecontinuousDOOMenvironmentinto8discretestatesusingtheHarris
Cornerdetectionalgorithm,and thenapplyingdiscrete-stateactiveinferenceonto the
discretestates.
Justaftermyinitialworkcamesimilarworkby(Tschantz,Baltieri,Seth,&Buckley,
Chapter4. ScalingActiveInference 201
2020), who instead applied active inference in a model-based fashion. Their model
parametrisesthetransitiondynamicsusingadeepneuralnetwork,andthenusesmodel
basedplanning(usingtheCEMalgorithm)tooptimizetheexpectedfreeenergyover
time. Wejointlyextendedtheirmodelin(Tschantz,Millidge,etal.,2020b)toinvestigate
explicitly the exploratory effects of the EFE or FEEF objective, and whether such
methodscanbefurtherscaledthroughlearningthetransitionandrewardmodel.
Afterourwork,severalrecentapproacheshavescaledactiveinferencefurther. Çatal,
Verbelen, Nauta, De Boom, and Dhoedt (2020), situate deep active inference within
apurely POMDP setting,usinga VAEencoderand decodertoparametrizethe likeli-
hood and state-posterior mappings, and then explicitly compute an action search tree
using their transition model to approximate the path integral over the expected free
energy through time. Similarly, Fountas, Sajid, Mediano, and Friston (2020), also
explicitlycomputeamodel-basedEFEsearchtreeintheirtasks,whilesimultaneously
approximating the output of the action planner with a model-free ‘habitual’ policy
network.
4.2.4 Iterative and Amortised Inference
Now that we firmly understand the notion of implementing control as an inference
procedure,itisworthrecappingafundamentaldistinctionbetweentwodifferenttypes
ofinference,whichareimportantandimplicitintheliterature,butrarelywellexplained.
The crucial distinction is between what we call iterative and amortised inference.
IterativeinferenceisthekindthatarisesdirectlyfromanaiveapplicationofBayesRule,
andwasthestandardinferenceapproachuseduntiltheriseofdeeplearningveryrecently
(Beal,2003;M.Jordanetal.,1998;Wainwright&Jordan,2008). Almostall‘classical’
variational or Bayesian inference methods are iterative. Amortised inference only
becameprominentwiththeadventofthevariationalautoencoder(Kingma&Welling,
2013),buthassincebecomethedominantapproach,especiallywithinmachinelearning.
Chapter4. ScalingActiveInference 202
Thekeydistinctionisthatiterativeinferencedirectlyoptimizestheparametersofthe
variationaldistribution. Forinstance,supposeweassumeourvariationaldistributionis
Gaussian,thaniterative inferencetriestooptimizethemeansandvarianceφ={µ,σ}
ofthisGaussiantofitsomedatapoint. Thisiswhatisimpliedbythestandardreading
oftheELBOorvariationalfreeenergyequation(Beal,2003;Hinton&Zemel,1994).
Iterative=argmaxE [lnq(x;φ)−lnp(o,x)] (4.38)
q(x;φ)
φ
Amortised inference, by contrast, does not directly optimize the parameters of the
variational distribution. Rather, it learns the parameters of a function that maps data
to parameters of the variational distribution. Effectively, the variational parameters
themselves are never optimized directly, they are simply spit out of the amortised
function f, which is then learned. φ = f (D). Rather, it is the parameters of this
ψ
amortisation function f that are learned. Importantly, these functions are optimized
ψ
notjustagainstasingledata-pointbutacrossthewholedatasetDR. Oncelearned,the
amortisationfunction f canbequicklyusedtocomputeestimatedvariationalparameters
φˆ for any data-point, thus amortising the cost of inference across the whole dataset.
By contrast,iterative inference must startfrom scratchfrom eachindividual data-point
given and optimize the variational parameters afresh. While it is often written the
same way, to make the notation very explicit, we write the amortisation objective to be
optimizedas,
(cid:104) (cid:105)
Amortised =argmaxE E [lnq(x;φˆ = f (D))−lnp(o,x)] (4.39)
ψ
p(D) q(x;φˆ=f
ψ
(D)) ψ
The reasonthat amortised inference has risento such popularityand ubiquity lately is
due to the fact that the amortisation function f is straightforward to implement as a
ψ
deepneuralnetwork,whereψaretheneuralnetworkweightswhichcanbetrainedbya
gradientdescentontheELBOorvariationalfreeenergy. Forinstance,inavariational
autoencoder, f iseffectivelyimplementedbytheencoder,whichmapsthedatadirectly
ψ
Chapter4. ScalingActiveInference 203
to the variational parameters (the mean and variance of the Gaussian). The iterative
approach,bycontrast,wouldforegotheencoderandrungradientdescentdirectlyon
the mean and variance themselves for each data-points. We thus see why amortised
methodsarepreferred. Theamortisedmethodcaninferthemeanandvariancequickly,
inonefeedforwardpassofthenetwork,whilegradientdescenttrainingissplitoveran
entiredataset. Bycontrast,theiterativeapproachwouldrequireagradientdescentfor
everyinferencethatthenetworkwishestomake. Importantly,however,thevariational
parametersfound bytheamortisedmethods areingeneralworseestimatesthanthose
found by iterative inference. This is simply because the iterative inference method
optimizestheparametersafreshwitheachdata-point,whileamortisedinferencemust
try to estimate them given a general function which must work for every datapoint.
Thustheamortisationfunction f mustgeneralizeinawaythatiterativemethodsdonot
ψ
haveto,and thusanygeneralizationerrorwillcausetheamortisedmethodtoperform
worse. Thisdifferenceinperformanceiscalledtheamortisationgap.
Interestingly, it has recently been shown (Marino, Yue, & Mandt, 2018), that you
can gain performance improvements by combining iterative and amortised inference
together. Forinstance, inavariational autoencoder, ifyoufirstperform theamortised
mappingtoobtaininitialestimatesofthevariationalparametersφbutthenrunseveral
iterative descent steps directly on your initial estimates of the parameters, this can
improvethe inferenceaccuracy andreducethe amortisationgapcompared tothepure
amortisation approach with only a relatively small computational penalty for each
inference.
Given that we know we can understand control problems in terms of inference, it is
alsointerestingtoconsiderwhetherthetypeofinferenceappliedincontrolasinference
canbebestunderstoodasiterativeoramortisedinference. Indeed,wearguethatthis
distinction between iterative and amortised inference maps rather cleanly (although
not perfectly) to the distinction between model-based and model-free reinforcement
Chapter4. ScalingActiveInference 204
learning. Wheremodel-freeRLcan bethoughtof asamortisedinference andmodel-
based as iterative inference. The reasoning here is straightforward but requires some
subtlyaboutwhatexactlyisbeinginferred.
The key quantity to be inferred in control as inference approaches is the variational
distributionoveractionsq(a|x). Model-freeapproacheswhichtrytomaintainaconstant
estimate of the value function, Q function or advantage function using the Bellman
equationcanbeunderstoodasamortisedinference. Thisismostexplicitinthecaseof
actor-critic orpolicy gradientmethods whichexplicitlymaintain anamortised policy
q (a|s), which is implemented as a deep neural network with weights ψ where the
ψ
weights are not optimized separately for each data-point, but rather across all data-
points. Approaches based purely on value function learning, such as Q learning, can
also be expressed in such a manner, because here the optimal policy depends in a
straightforwardwayupon theamortisedvaluefunction. ForstandarddeterministicQ
learningwehavethatq (a|s)=δ(a−max Q (s,a)),orthattheactiondistributionis
ψ a ψ
a dirac-delta over the maximum value of the Q function, which is itselfamortised and
implementedasadeepneuralnetwork. Insoftmethods,thedelta-maxisreplacedbya
softmaxoverallactionvalues,sothatactionsareselectedwithprobabilityproportional
totheirrelativeexponentiatedmagnitudes.
Model-based methods, by contrast, appear to correspond to iterative inference ap-
proaches. The key to understanding this is that it is the planner which matters and is
effectively doing theinference, not anything todo with themodel – i.e. the transition
model–inmodel-basedmethods,whichisoftenamortised. Wecantreatthevarietiesof
planning algorithmssuch asCEM and pathintegral controlas optimizing theactions or
actionsequencesdirectlyoverthecourseofmultipleiterations,andthuscorrespondsto
iterative inference. IndeedOkada andTaniguchi (2020)have shown how thesestandard
planningalgorithmscanbederivedasvariationalinferencealgorithmsthemselvesunder
certainconditions.
Chapter4. ScalingActiveInference 205
To support these identifications intuitively model-free methods share the same advan-
tagesanddisadvantagesasamortisedinference–thattheyaretrainedacrossawhole
dataset butfast tocompute for anyindividual instance, andless sampleefficient, since
theamortisationfunctioncanonlybelearntacrossawiderangeofexperiencetoenable
goodgeneralization. Model-basedmethodsaretheoppositeandsharethepropertiesof
iterativeinferenceapproaches. Theyareverysampleefficientandperformwellwith
very small amounts of data (since planning occurs for each datapoint independently,
the only need for data is in the amortised training of the transition model). However,
theyaremuchmorecomputationallyexpensiveperdatapoint,sincetheymustunder-
take an iterative planning process for each state, instead of directly mapping a state
to an action, as an amortised policy does. Interestingly, however, for model-free vs
model-basedapproaches,theamortisationgapisoftentheotherwayaround. Currently,
model-freeamortisedpoliciesgenerallyachievea higher asymptoticaccuracythando
model-based planners (Haarnoja, Zhou, Abbeel, & Levine, 2018; Hafner, Lillicrap,
Ba,&Norouzi,2019;Shyametal.,2019). Thisisfortworeasons. Firstly,thereisan
additionaldistinctionwhichmustbemadebetweeninferringasingleaction,asisdone
bymodelfreepolicies,andinferringawholesequenceofactions(anactionplan)which
iswhatistypicallydonebymodel-basedplanners(althoughoftenthiswholesequence
isdiscardedandrecomputedeverytime,anapproachwhichiscalledmodelpredictive
control). Inferring afull planis almostalwaysharder thaninferring asingle action to
take immediately, and this may be the cause of some of the reverse amortisation gap.
Anadditionaland potentiallymoreseriousissue isthatcurrentplanning methodsare
generallyquitecrudeandcannotrepresentexpressivedistributionsoveractionplans.
For instance the cross-entropy method can only represent single unimodal Gaussian
plans,andsimilarlypathintegralcontrol,theotherstateoftheartmethod(E.Theodorou
& Todorov, 2012; J. Theodorou Evangelosnd Buchli & Schaal, 2010b; D. Williams,
2018;G.Williams,Aldrich,&Theodorou,2017;G.Williams,Wagener,etal.,2017)
suffersfromsimilarconstraints. Whilethere hasbeensomerecentworkinimproving
Chapter4. ScalingActiveInference 206
the expressivity of planning methods, such as multimodal CEM (Okada, Kosaka, &
Taniguchi,2020),muchworkremainstobedoneheretobeabletomatchtheexpressive
capabilitiesofdeepneuralnetworkpolicies.
Finally, it is important to note that the above distinction has revealed an additional
orthogonaldimensionofwhetherit issingleactionsthatareinferredorwhole action
plans. Wethus seethat we canplot reinforcement learningand control methodsinto a
quadrantwith twoorthogonaldimensions– whetheriterativeor amortised inferenceis
used,andwhetherfullactionplansorjustsingleactionsareinferred. Wethusseethat
the standard distinctions of ‘model-free’ vs ‘model-based’ themselves map onto the
diagonalofthequadrant. Model-freereinforcementlearningisamortisedinferenceof
singleactions,whilethestandardmodel-basedmethodscorrespondtoiterativeinference
of full action plans. Importantly, there are several methods on the off diagonal, such
as iLQR (W. Li & Todorov, 2004) which infers single actions in an iterative fashion.
Understandingandplottingreinforcementlearningmethodsinsuchawayrevealsthe
fullspaceofmethodsandwhichareasarepotentiallyunderexplored. Forinstance,we
immediatelyseethatthereareveryfew,ifany,methodswhichutilizeamortisedplans,
eventhoughlearningamortisedplanscouldwellbestraightforwardandmayevenbe
beneficial. Thiswouldthenbeafertileareaforfuturework.
4.2.5 Control as Hybrid Inference
4.2.5.1 Introduction
Building on the observation that iterative and amortised inference can be combined
toconstructaniterative-amortisedinferenceschemewhichcombinestheadvantages
andamelioratestherespectivedisadvantagesofbothiterativeandamortisedinference
– enabling rapid and flexible inference with a high asymptotic performance, and an
adaptiveschemewhichcanleverageadditionalcomputingpoweronlywhereitismost
needed. In this section, we apply iterative-amortised combination to reinforcement
Chapter4. ScalingActiveInference 207
Policies Planning
Policy Gradients
(Williams 1992)
Amortised Q-learning
(Watkins 1989)
Soft Actor-Critic
(Haarnoja 2018)
iLQR CMA-ES MPPI
(Li and Todorov 2004) (Hansen 2005) (Williams 2017)
Iterative
CEM SMPC
PID (Rubinstein 1997) (Piche 2019)
(Baltieri 2019) PAETS PI2
(Okada 2019) (Theodorou 2010)
Figure 4.9: Overview of classic RL and control algorithms in our scheme. Standard
model-free RL corresponds to amortised policies, planning algorithms are iterative
planning, and control theory infers iterative policies. The amortised plans quadrant is
empty, perhaps suggesting room for novel algorithms. The position of the algorithms
withinthequadrantisnotmeaningful.
learningwhichcanbeseenascombiningmodel-basedandmodel-freeRL,usingthe
identificationpreviouslydeveloped.
Specifically,variantsofmodel-basedplanningalgorithmscanbederivedasvariational
inference algorithms using mirror descent to optimize the variational free energy in
an iterative fashion (Okada & Taniguchi, 2020). Additionally, as discussed previously,
model-freereinforcementlearningmethodssuchasQ-learning,policygradients,and
actor-criticcan becastasoptimizinganother variationalfreeenergy bound,butrather
this time in an amortised fashion. Given this, there are multiple potential ways to
combine model-based and model-free reinforcement learning approaches. Perhaps
one of thesimplest approaches, which we applyin this study, is to use themodel-free
policyasaninitializationofthemodel-basedplanner. Model-basedplanningalgorithms
suchasCEMorMPPI,requireaninitialactiondistribution p
1
(a
1:T
)=∏
t
T
=0
p
1
(a
t
)to
beginwith,whichtheytheyproceedtooptimize. Usuallythisinitialactiondistribution
is set to some simple known distribution such as a zero-centred normal distribution
p (a )=N(a;0,σ )withavarianceparameterσ whichbecomesahyperparameterof
1 t a a
Chapter4. ScalingActiveInference 208
theplanningalgorithm.
Instead,weproposetoinitializetheplannerwiththeresultsoftheamortisedmodel-free
policy network p (a ) = q (x ). To do this for a potential action trajectory requires
1 t φ t
knowledge of future states to feed into the model-free policy network. However,
conveniently, the model-based planner also has access to a transition model which is
usedtogeneratethesesimulatedstatetrajectories,giventheactionsoutputbythepolicy
network.
4.2.5.2 ModelandHyperparameterDetails
Tomakeourmodelconcrete,weneedtoinstantiatemanydistributionssuchasthetran-
sitionmodels p (x |x ,a ),theparameterisedpolicyq (a |x )andthevariational
θ t+1 t t φ t:T t:T
iterativeplanningalgorithmwhichinstantiatesq(a |x ;ψ).
t:T t
Thetransitionmodelwasinstantiatedasanensembleofthreelayermulti-layerpercep-
tronnetworkswithahiddensizedimensionofsize250,whichwastrainedtooutputa
Gaussiandistribution(meanandvariance)overthechangeinenvironmentstate. That
is,ratherthanexplicitlymodel p(x |x ,a ),weinsteadmodelled p(x −x |x ,(x −
t+1 t t t+1 t t t
x ),a ), which we could then use to reconstruct the next predicted environmental
t−1 t
stateassˆ =x +(x −x ). Trainingthetransitionmodeltopredictstatedifferences
t+1 t t+1 t
instead of the states directly is a common trick used in model-based reinforcement
learning which has been found to significantly improve modelling performance by
incorporating explicit information about the derivatives of the states, which is hard
to derive solely from the states themselves. To obtain a measure of uncertainty over
the transition model parameters θ, which can be utilized to drive information-gain
maximizing exploration, we maintained an ensemble of 5 transition models which
were eachtrained on independentlysampled batches oftransition data. Each ensemble
possessedindependentrandomlyanduniformlyinitializedweights.
For the amortised action policy, we utilized the soft-actor-critic architecture (SAC)
Chapter4. ScalingActiveInference 209
(Haarnoja,Zhou,Abbeel,&Levine,2018),withapolicy-networkwhichconsistedofa
three-layerMLPmodelwithahiddendimensionof256. Wedidnotuseanadaptiveα
parameterfortheSACagentbutsetitto0.2throughout.
Fortheiterativeplanner,weusedthestandardandpowerfulCEMalgorithm(DeBoer,
Kroese,Mannor,&Rubinstein,2005),withatime-horizonof7,anumberofiterationsof
10,andatrajectorysamplesizeof500. Foreachgeneratedtrajectory,toencouragemore
exploration,weaddedadditionalactionnoisesampledindependentlyfrom∼N(0,0.3).
We maintained a memory buffer of all environmental interactions seen by the agent,
andused various samplesto trainthe transitionmodel andamortised policy network.
Wetrainedthemodelover10epochswhichiteratedoverthefullmemorybuffer,witha
batchsizeof50. Thefullcontrolashybridinferencealgorithmisdefinedasfollows,
Chapter4. ScalingActiveInference 210
Algorithm3:InferringactionsviaCHI
Input: PlanninghorizonH |OptimisationiterationsI |NumberofsamplesK |
Currentstates |Transitiondistribution p (s |s ,a )|Amortisationfunction
t λ t+1 t t
f (·)
φ
AmortisedInference:
p
φ
(τ)=δ(s
t
)∏
t
T
(cid:48)=t
p
λ
(s
t(cid:48)+1
|s
t(cid:48)
,a
t(cid:48)
)q
φ
(a
t(cid:48)
|s
t(cid:48)
;θ)
Extractθ(1) ={µ ,σ2 }from p (τ)
t:T t:T φ
Initialiseq(a;θ)withparametersθ(1)
IterativeInference:
foroptimisationiterationi=1...I do
SampleK actionsequences{(a) ∼q(a;θ)}K
k k=1
InitialiseparticleweightsW(i) :={w (i) }K
k k=1
foractionsequencek=1...K do
(cid:0) (cid:1) (cid:0) (cid:1)
w (i+1) ← W (a) k ·q(i) (a) k ;θ
(cid:104) (cid:105)
k (cid:0) (cid:1) (cid:0) (cid:1)
∑K
j=1
W (a)j ·q(i) (a)j;θ
θ(i+1) ←refit (cid:0) W(i+1 (cid:1)
end
end
Extractµ fromq(a;θ)
t:T
returnµ
t
4.2.5.3 RelatedWork
There has been a small amount of prior work aiming at combining model-free and
model-based (Che et al., 2018; S. Li, 2020). For instance, a strand of research has
focusedonusingalearnedtransitionmodeltogenerateadditionalsimulateddatawhich
can then be used to train a model-free policy ‘offline’. This approach was pioneered
with the Dyna architecture (Sutton, 1991), but has also been extended and applied in
moremoderndeep reinforcement learningsettings(Gu,Lillicrap, Sutskever,&Levine,
Chapter4. ScalingActiveInference 211
2016). Conversely, in (Farshidian, Neunert, & Buchli, 2014) and (Nagabandi, Kahn,
Fearing, & Levine, 2018) a model-based planner was used to initialize a model-free
policy–theoppositedirectiontoourmodel. Ourapproachdoessharesimilaritieswith
theapproachusedinAlphaGo(Silveretal.,2017)whichusedlearnedamortizedpolicy
networks to generate proposals for the monte-carlo-tree-search (MCTS) used to select
movesinthatapproach. However,theirapproachwasjustifiedonheuristicgroundsand
they did not consider how their approach corresponds to a mathematically principled
combinationofiterativeandamortisedvariationalinference. Indeed,itisnotyetclear
iftheMCTSalgorithmcanbecastasperformingsomekindofvariationalinferenceor
not.
Whileweare thefirsttoconsiderthe combination ofamortisedanditerativeinference
in reinforcement learning, and tomake the connection to model-based andmodel-free
methods, there is a line of work combining the two approaches to inference in the
contextofunsupervisedgenerativemodelling,typically usingvariational autoencoders.
(H.Kim,Kim,Jeong,Levine,&Song,2018),employamortisedinferenceinaVAEto
initializethesetofvariationalparameterswhicharethenoptimizeddirectlyagainstthe
ELBO.AsimilarapproachwastakenbyMarino(Marinoetal.,2018),whoshowedthat
byrepeatedlyencodingthegradientsandoptimizingthevariationalparametersagainst
theELBO,whichwasfoundempiricallytoimproveperformanceandhelpnarrowthe
amortisationgap.
4.2.5.4 Results
Wetestedourhybridagentfirstonadidactictoycontinuouscontroltask. Thegoalof
thistaskwastosimplyexplorehowtheiterativeandamortisedcontrolschemesinteract.
Theenvironmentwasasimple2-Dplanarenvironment,wheretheagentbeganinthe
bottom-leftcorner,andwherethegoalwastoarriveinthetop-rightcorner. Thereward
signalwasasmoothgradientfieldleadingtothetop-rightwhichwasimplementedas
r(x,y)=1−(||(x,y)−(g ,g )||2)whereg andg representthexandycoordinatesof
x y x y
Chapter4. ScalingActiveInference 212
Figure 4.10: (a - c): Amortised predictions of q (a|s;θ) are shown in red, where •
φ
denotetheexpectedstates,shadedareasdenotethepredictedactionsvarianceateach
step, and the expected trajectory recovered by iterative inference is shown in blue. At
the onset oflearning (a), the amortised predictionsare highly uncertain, and thus have
littleinfluenceonthefinalapproximateposterior. Astheamortisedmodel f (·)learns
φ
(b), the certainty of the amortised predictions increase, such that the final posterior
remains closer tothe initial amortised guess. At convergence, (c), the iterative phase of
inferencehasnegligibleinfluenceonthefinaldistribution,suggestingconvergencetoa
model-freealgorithm. (d)Here,wecompareouralgorithmtoitsconstituentcomponents
–thesoft-actor critic(SAC)andan MPCalgorithmbasedon thecross-entropymethod
(CEM). Theseresults demonstrate thatthe hybridmodel significantly outperformsboth
ofthesemethods.
thegoalstate. Inthecentreoftheenvironmenttherewasanimpassablewallexceptfor
asmallopeningthroughwhichtheagentcouldpass. Theagentcouldcontrolitsxandy
velocity–a=(x˙,y˙)withamaximumvelocityof0.05andaminimumvelocityof-0.05.
Thegraphshowstheevolutionoftheagent’siterativeandamortisedpoliciesasitlearns
tocompletethetask. Ascanbeseen,theiterativepolicystartsouthighlyuncertain,with
ahighvariance. Astheamortisedpolicyisslowlylearnt,thevarianceoftheiterative
policy shrinks, and the resulting policy closely matches the amortised policies. This
immediatelysuggestsanadaptivemethodofsavingcomputation–whenthevarianceof
theiterativepolicyissmall,ortheiterativepolicyisveryclosetotheamortisedpolicy,
rely solely on the computationally cheap amortised policy only. Conversely, when
Chapter4. ScalingActiveInference 213
theamortisedpolicyortheiterativepolicyishighlyuncertain(asatthebeginningof
training),thenthecomputationallyexpensivemodel-predictive-controloftheiterative
policy should be utilized. In this way, the agent can attain the impressive sample
efficiencyandrapidperformanceofmodel-basedplanningatthebeginningoftraining,
whenthe amortised policyispoor,butthenoncethe amortised policybecomesgood,
theagentcansimplyrelyonthatandthusachievethehighasymptoticperformanceand
relativecomputationalcheapnessofmodel-freeRL.
Wealsocomparedthehybridagentonachallengingcontinuouscontroltask–HalfChee-
tah Run S17,A6. This environment requires the agent to take control of a bipedal
simulatedcheetah ina planarenvironmentwith semi-realisticphysics. The agent’sgoal
isto move thecheetah’slimbsinsuch awayasto maximizetheoverallvelocity ofthe
cheetah,whilesimultaneouslyminimizingthetotalactionapplied. Therewardfunction
for the task was r = v−0.1a2 where v denotes the overall velocity of the cheetah.
The hybrid agent was evaluated against strong model-free (SAC) and model-based
(CEM)baselines. AscanbeseenfromFigure4.10thehybridagentsignificantlyoutper-
formsbothbaselinesandsimultaneouslyachievesthesampleefficiencyofmodel-based
methodswithsuperiorperformancetothemodel-freeSACagent.
4.2.5.5 InterimDiscussion
Empirically,wefindthatthehybridagentperformswell,andthattheinteractionofthe
iterative and amortised inference components allow for a natural adaptive scheme to
switch between and apportion computation in a way that maximizes the computational
resourcesavailabletotheagent. Moreover,theuseofanamortisedpolicytoinitializethe
iterativeplannercutssignificantlydownonthecomputationalexpenseoftheplannerand
tends tostabilize performance. Additionally,the use ofthe iterative planner atthe start
meansthattheagentrapidlydiscovershighlyrewardingtrajectorieswhicharethenused
totrainthe SACagent,andthus provides apowerfulsourceof implicit explorationfor
themodel-freeSACagent. Interestingly,however,thishighlyrewardingdatagenerated
Chapter4. ScalingActiveInference 214
by the iterative planner comes with a cost – it is heavily biased towards positive
trajectories,andthustheSACagent,asitisnotexposeddirectlytonegativetrajectories
intherealworld,simplydoesnotlearnthemandthuslearnsahighlyoptimisticvalue
function,whichperformspoorlywheninteractingwiththerealenvironment. Wecall
thisthedata-biasissueandleftuncheckeditinhibitedtheperformanceandlearningof
thealgorithm.
To ameliorate the data-bias issue, we instead train the SAC agent from the simulated
rolloutsoftheiterativeplanner. Theserollouts,especiallyintheearlystagesofiteration,
contain many examples of (predicted) negative trajectories, which thus helps render
the dataset fed to the model-free SAC agent less positively biased. These rollouts do
havetheir owndifficulties –namely thatthey are fundamentally from asimulation and
thusmaybeapoorrepresentationoftheactualdynamicsoftheworld(especiallywhen
thetransitionmodelispoor). Secondly,therolloutsarestillbiasedtosomedegreeby
theoperationoftheiterativeplannerevenintheearlystagesofiteration. Additionally,
thisbecomesmoreacuteasthemodel-freepolicybecomesbetter,asitlearnstoavoid
negative contingencies and its actionpredictions are thenused to initialize theplanner,
thus creating a compounding positive bias to the data that is fed into the SAC agent.
Nevertheless,wefindempiricallythatthissolutionsufficestotrainahighperformance
model-freepolicynetwork.
On a moretheoretical level, it is important tonote that we chose arelatively straight-
forward scheme of combination – using the amortised policy to simply initialize the
model-basedplanner. A moreinvolved,butslightly moreprincipledmethod maybe to
settheactionprioroftheiterativeplanner(whichiscurrentlyassumeduniform,asis
standardinthecontrolasinferenceframework)totheamortisedpolicy p(a|s)=q (a|s).
φ
Usingthismethod,insteadofadirectinitialization,theamortisedpolicywouldserveas
aregularizerontheiterativemodel-basedplanner,ensuringthattheresultingiterative
policy is penalized for its divergence from the model-free amortised policy. Such
Chapter4. ScalingActiveInference 215
regularisationmethodshavebeenfoundtobebeneficialforthestabilityoflearningina
numberofreinforcementlearningalgorithmsandespeciallyinpolicygradientmethods,
wheremethodssuchasPPO(Schulmanetal.,2017)reachstateoftheartperformance.
4.3 Conclusion
Inthischapter,wehaveinvestigatedtheapplicationofmethodsderivedfromthefree
energy principle – specifically active inference – to the general problem of optimal
actionselectionandcontrol. Wehavefocusedespeciallyonacorelimitationofcurrent
active inference methods: their scalability. We have demonstrated how many of the
keydistributionswhichariseoutofthefreeenergyobjectivecanbeparametrizedusing
deepneuralnetworks,toderiveschemeswhichcanlookverysimilartocontemporary
deep reinforcement methods – both model-free and model-based methods. We show
that these methods – which we call deep active inference approaches – can perform
comparablyandoftenbettertotheirdeepreinforcementlearningcounterparts.
Specifically, in the first study presented in this chapter, we showcase how active in-
ference can be interpreted through the lens of model-free reinforcement learning. In
this case, we use a learnt action policy q(a|s) and parametrize the action prior using
an amortized expected-free energy value network, to approximate the required path
integral over the expected free energy. The resulting algorithm looks very similar to
actor-criticmethodsinmodel-freereinforcementlearning,butusingtheexpectedfree
energy instead of the reward. Additionally, we find that this approach also utilizes
additionalentropyregularisationtermswhichcanbeshowntosubstantiallyimprovethe
stabilityandtheperformanceoftheresultingalgorithm–thusdemonstratinghowin-
sightsandthemathematicalformalismofactiveinferencecanalsoleadtoimprovements
inreinforcementlearningalgorithms.
In the second study, we instead approximate the path integral of the expected free
energy,withmonte-carlosamplingoftrajectoriesandultimatelyuseamodel-predictive
Chapter4. ScalingActiveInference 216
control planning algorithm to compute optimal trajectories instead of an amortised
policy network. This simple change is sufficient to move us into the realm of model-
based reinforcement learning. Here, we show that active inference can again attain
theperformanceofcomparablemodel-baseddeepreinforcementlearningalgorithms,
and can be applied to solve challenging continuous control tasks. Additionally, here
thesuperiorexploratorycapabilitiesoftheexpected-freeenergyfunctionalcomeinto
play, and we see that they allow the construction of powerfully exploratory goal-
directed, information-seeking behaviours, which can solve very challenging sparse
reward tasks, such as the mountain car, with ease. This demonstrates another way in
which insights from active inference can aid the development of deep reinforcement
learningalgorithms.
Wethenturntoamoreabstractconsiderationofthedifferencebetweenmodel-basedand
model-freereinforcementlearningintermsofinference–aninsightwhichisheavily
enabled by the active inference formulation of action selection as fundamentally an
inferenceproblem. Wedemonstratethatwecanseethedistinctionbetweenmodel-free
and model-based as simply that of iterative vs amortized inference, where iterative
variationalinferencedirectlyoptimizestheparametersofthevariationaldistribution,
while amortized inference instead optimizes the parameters of a mapping function
whichmapsobservationsdirectlytovariationalparameters. Wethenshowhowthere
is a separate dichotomy between whether policies or plans are inferred, and that this
provides us with a simple two dimensional quadrant scheme upon which to place all
major reinforcement learning algorithms. It also demonstrates that there are several
areaswhichareunderexploredintheliterature–especiallythedirectcomputationof
amortizedplans.
Finally,weusethisinsightintothenatureofmodel-basedandmodel-freereinforcement
learningintermsofiterativeandamortizedinferencetoaskhowthesetwoapproaches
can be combined. We show that this can yield powerful algorithms which can com-
Chapter4. ScalingActiveInference 217
bineboththesample-efficiencyandrapidlearningofmodel-basedplanningwiththe
asymptotic performance and computational cheapness of model-free reinforcement
learning. Importantly,investigationsintothisfieldofcombinedorhybridreinforcement
learningalgorithmsareonlyjustbeginning,andtherearemanydesignchoicesleftto
beextensivelyinvestigatedinfuturework.
Overall,crucially,wehaveshownthatthefreeenergyprincipleandactiveinferencecan
besuccessfullyappliedandscaleduptohandlelargeandchallengingcontroltasksand
tocreatealgorithms whichperformcomparablywithstate oftheartmethodsin deep
reinforcementlearning.
In the next chapter, we extend the intuitions provided here about the importance of
combiningexplorationandexploitationandturntoamoreabstractandmathematical
analysis of what kind of mathematical procedure gives rise to the combination of
exploratory and epistemic action that characterise such objective functionals as the
ExpectedFreeEnergyandtheFreeEnergyoftheExpectedFuture.
Chapter 5
The Mathematical Origins of
Exploration
5.1 Introduction
Inthepreviouschapter,wehaveseentheimportanceandbenefitsofinformation-seeking
asopposedtorandomexplorationforreinforcementlearningtasks. Information-seeking
exploration,whichexplicitlyaimstoreduceuncertaintyabouteithertheenvironment
ortheagent’smodeloftheenvironment,providesapowerfulexplorationstrategythat
allowstherapidandefficientexplorationofanenvironment,asopposedtotherandom
walkstrategyemployedbyrandomexploration. Moreover,whencombinedinasingle
lossfunctionwitharewardmaximizingterm,thiscombinationresultsingoal-oriented
exploration where the agent is only driven to explore contingencies which are also
likely to lead to high reward. We have seen that this goal-directed, or goal-oriented,
explorationmechanismhasperformedwellinmodel-basedreinforcementlearningtasks
includingsparse-rewardenvironmentswhicharechallengingforstandardreinforcement
learning agents. Moreover, this kind of exploration is almost certainly necessary for
biological organisms in more ecologically valid tasks, where rewards are often very
218
Chapter5. TheMathematicalOriginsofExploration 219
sparse and environments are typically very large compared to those in mainstream
reinforcementlearningbenchmarks.
Inthischapter,wetakeamoreabstractperspective,andstudyindepththequestionofthe
mathematicaloriginandmeaningofsuchgoal-directedexplorationobjectiveswhich
unite both reward seeking and information maximizing terms in a single objective.
Specifically, we seek to understand whence they arise, and what the mathematical
formulationwhichcangiverisetothemis. Whileforpracticalpurposesandengineering
applications it is often sufficient to glue different terms together in an ad-hoc way to
constructanobjectivewhichgivesrisetosomedesiredbehaviour,wewishtoprobethe
deepertheoryunderlyingsuchfunctionalswhichhassofarremainedmostlymysterious.
It is the hope that by mathematically understanding the origin and nature of such
objectives,aswellastheirproperties,wecanilluminateaswatheofcurrentmethodsin
reinforcementlearning,cognitivesciences,decisiontheory,andbehaviouraleconomics,
aswellasdeeplyunderstandinghowtheyinterrelatetooneanother. Moreover,itseems
likely, given the generally productive dialectic between theory and practice in all of
these fields, that by contributing to the underlying theory of such objectives, we can
ultimately contribute tothe design ofmore powerful objectives and methodsthan are
currentlyusedintheliterature.
Tobegin,wewishtodeeplyexaminetheoriginandnatureoftheExpectedFreeEnergy
(EFE) functional. The EFE is central to the theory of active inference, where it is
proposedthatall agents underthefreeenergyprinciple, whichmustseektominimize
thelongtermpathintegraloftheirsurprisemustchoosepoliciesthatminimizetheEFE.
The EFE has been widely used in almost all models in discrete-time active inference
(DaCosta,Parr,etal.,2020;Friston, FitzGerald,etal.,2017a,2017b;Friston,Rigoli,
etal.,2015b;Friston,Rosch,etal.,2018a)withtheexceptionofthelaterdevelopment
of the generalized free energy (Friston, Rigoli, et al., 2015a; Parr & Friston, 2017a,
2017b).Despitethisubiquitoususewithintheactiveinferencecommunity,theprecise
Chapter5. TheMathematicalOriginsofExploration 220
mathematical origin and nature of the EFE functional have remained unclear. In the
literature,theEFEisoftenjustifiedthroughareductio-ad-absurdumargument(Friston,
Rigoli, et al., 2015a) which runs as follows – since (we assume under the FEP) all
agents minimize free energy, then they must think they will minimize free energy in
thefuture. Sincethefutureisuncertain,insteadofthestandardvariationalfreeenergy
(VFE),theymustinsteadminimizetheirexpected freeenergy(EFE),elsetheyarenota
free-energyminimizingagent(disprovenconclusion)1. Centraltothislogicistheclaim
thattheEFEisthe‘natural’extensionoftheVFEtoaccountforuncertainfutures.
Inthefirstsectionofthischapter,weinvestigatethisclaimindetail. Specifically,we
argue that the EFE is not necessarily the only way to extend the VFE into the future,
andthat thereare infactother,more straightforward extensions,such asan objective
we call the free energy of the future (FEF). We then perform a direct side-by-side
comparison of the EFE and FEF functionals and comment on their similarities and
differences,anddiscusstheirrespectiveboundingbehaviourontheexpectedfreeenergy.
Wethendiscusshowactiveinferenceapproachesarerelatedtothecontrolasinfrerence
framework,anddecideupontwokeydifferences–theobjectivefunctionalutilizedfor
action selection, where active inference uses the EFE, and control as inference uses
theFEF,andsecondlytheencodingofvalueorgoalsintotheinferenceprocess,where
activeinferencedirectlyencodesvaluesintothegenerativemodelthroughthe useofa
biaseddesiredistribution p˜(o),controlasinferenceinsteadusesindependentoptimality
variables p(Ω|o)2. Finally,wethenintroduceasecondobjectivefunctional,whichwe
callthefreeenergyofthe Expected Future(FEEF)whichcombinesbothanintuitively
groundedstartingpointwiththesameexplorationseekingtermasispresentintheEFE,
andwhichwasinvestigatedpreviouslyinChapter4. Wediscussthenatureofdifferent
possibleobjectivefunctionalsforcontrol.
1Technicallythisismoreofaninductionargumentthanareductio-ad-absurdum,butwestillreferto
itassuchduetoitsdescriptionasareductiointheliterature(Friston,Rigoli,etal.,2015a)
2Wealsodenoteanydistributioninvolvingadesiredistributionwitha p˜and,forinstance,referto
p˜(o,x)asabiasedgenerativemodel)
Chapter5. TheMathematicalOriginsofExploration 221
In the second halfof this chapter, we retreatfrom the specifics of the EFE, active infer-
ence,andcontrolasinference,toinsteaddefineageneralframeworkforunderstanding
theoriginofinformationseekingexplorationtermsincontrolfunctionals. Wearguethat
thekeydistinction,isthatbetweenevidenceobjectives,whichmaximizethelikelihood
ofachievingadesiredistribution,withdivergencefunctionalswhichtrytominimize
the divergence between a predicted and desire functional. Specifically, divergence
objectives give rise to information gain terms while evidence objectives do not. We
tracethiscapacitytothefactthatdivergenceobjectivesimplicitlymaximizetheentropy
oftheagent’sfuture,inacloseconnectiontoempowermentobjectives,whileevidence
objectivesdonot. Finally,weputallthistogetherintoacoherentframeworkwhichcan
be usedto understandthe fulllandscape ofvariationalobjective functionalsfor control
tasks.
Thematerialinthischapterisheavilybasedonthreefirst-authorpapers. Whencethe
expected free energy (Millidge, Tschantz, & Buckley, 2020b) (published in neural
computation), On the relationship between active inference and control as inference
(published at the IEEE international workshop on active inference) (Millidge, Tschantz,
Seth, & Buckley, 2020b), and (Millidge, Tschantz, Seth, & Buckley, 2021) Under-
standing the Origin of Information-Seeking Exploration in Probabilistic Objectives for
Control,Arxiv(tobesubmittedtoRoyalSocietyInterface).
5.2 Origins of the Expected Free Energy
Here we investigate the origins of the expected free energy (EFE) term within active
inference. It is often claimed that the reason active inference agents minimize this
term is that free energy minimizing agents mustminimize the variational free energy
(VFE) into the futurewhich, sincethe future is uncertain, constitutes theexpected free
energy. Tomakethisclaimprecise,weneedtounderstandexactlythevariationalfree
energy‘intothefuture’shouldconsistof. Wearguethatitmustsatisfytwoconditions,
Chapter5. TheMathematicalOriginsofExploration 222
which are both satisfied by the variational free energy, and which are crucial for that
objectivefunctionstooperate. First,wearguethat,liketheVFE,theVFEextendedinto
the future should be a divergence between a variational approximate posterior and a
generative modeloffuture states. Secondly, wearguethat, againliketheVFE, anyfree
energyofthefutureshouldadditionallybeaboundonthelogmodelevidenceoffuture
observations. Theseconditionsarebothimportantpreciselybecausetheydefinewhy
thevariationalfreeenergyisuseful. Minimizingthedivergencebetweentheposterior
andthegenerativemodelisusefulsinceitimplicitlymakesthevariationalposteriora
goodapproximation. Conversely, boundingthelogmodel evidenceisusefulsincethe
log-model evidence provides averygeneral measureof how‘good’ a specificmodel
is, which can be used for Bayesian model-comparison or even just to understand the
amount of inherent information in the data. Moreover, the log model evidence has
especialimportformethodsundertheaegisofthefreeenergyprinciplesincethelog
model evidence is simply the surprisal −lnp(o) which is the basic quantity which is
minimizedthroughoutthetheory.
First,weneedtodefinepreciselythemathematicalsetupoftheproblem. Weassume
thatouragentexistsinaPOMDPenvironmentwithstatesx,observationso,policies
(sequencesofactions)π=[a ,a ...a ]. Theagentmaintains avariationaldistribution
1 2 T
over the states and actions and a generative model over the states, observations, and
policies. Specifically, although we are technically interestedin thefunctionals over a
full trajectory o , in practice the functional decomposes into a sum of independent
1:T
functionals for each timestep. Thus, for understanding the behaviour of agents opti-
mizingthefunctional,itsufficestoconsideronlyasingletimestepofthefunctionalat
o .
t
Wearguethattheexpectedfreeenergydoesnotfulfiltheseconditions,butratheranother
objectivefunctionaldoes,whichwecallthefreeenergyofthefuture(FEF).Wedefine
Chapter5. TheMathematicalOriginsofExploration 223
theFEFtobe,
FEF (π)=E [lnq(x |o )−lnp˜(o ,x )]
t q(ot,xt|π) t t t t
=E D [q(x |o )||p˜(o ,x )] (5.1)
q(ot) KL t t t t
whichissimplytheKLdivergencebetweentheapproximateposteriorgenerativemodel
overfuturestates,averagedundertheexpectedfutureobservationsq(o ). Thistrivially
t
satisifesthefirstcondition,sinceitisaKLdivergencebetweenthevariationalposterior,
andthegenerativemodel,asistheVFE.Next,weshowthatthisfunctionalisabound
ontheexpectedlogmodelevidenceinthefuture.
(cid:90)
−E (cid:2) lnp˜(o ) (cid:3) =−E (cid:2) ln dx p˜(o ,x ) (cid:3) (5.2)
q(ot|π) t q(ot|π) t t t
(cid:90) q(x |o )
=−E (cid:2) ln dx p˜(o ,x ) t t (cid:3)
q(ot|π) t t t
q(x |o )
t t
≤−E
(cid:90)
dx q(x |o ) (cid:2) ln
p˜(o
t
,x
t
)
(cid:3)
q(ot|π) t t t
q(x |o )
t t
≤−E (cid:2) ln
p˜(o
t
,x
t
)
(cid:3)
q(ot,xt|π)
q(x |o )
t t
q(x |o )
≤E (cid:2) ln t t (cid:3)
q(ot,xt|π)
p˜(o ,x )
t t
≤E D [q(x|o )||p˜(o ,x |π)]=FEF(π) (5.3)
q(ot|π) KL t t t
Crucially,wecanseethatthisisanupperbound onthelogmodelevidence,andthus
minimizing the FEF will tend to decrease the gap between the FEF and the expected
log-modelevidence. ThisfunctionalthusexhibitsidenticalbehaviourtotheVFE.To
gain a better understanding of the key differences between the EFE and the FEF, we
canexhibitthemsidebyside.
FEF=E [lnq(x |o )−lnp˜(o ,x )]
q(ot,xt|π) t t t t
EFE=E [lnq(x |π)−lnp˜(o ,x )] (5.4)
q(ot,xt|π) t t t
Chapter5. TheMathematicalOriginsofExploration 224
Whilethetwoformulationsmaylookverysimilar,thekeydistinctionisthattheFEF
optimizesthedivergencebetweenthevariationalposterior q(x |o )andthegenerative
t t
model while the EFE minimizes the variational prior q(x ). While this difference
t
mayseemsmall, weseethatit hasasignificantimpactwhen itcomestothe resulting
interpretabletermsfromthedecompositionofthetwofunctionals,
FEF=−E (cid:2) lnp˜(o |x ) (cid:3) +E D [q(x |o )||q(x |π)] (5.5)
q(ot,xt|π) t t q(ot|π) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue EpistemicValue
EFE=−E (cid:2) lnp˜(o ) (cid:3) −E D [q(x |o )||q(x |π)] (5.6)
q(ot,xt|π) t q(ot|π) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue EpistemicValue
Specifically,weseethatwhileboththeFEFandtheEFEcanbesplitinto‘extrinsic’and
‘intrinsic’valueterms,theintrinsicvaluetermintheFEFispositivewhileintheEFEit
isnegative. SpecificallythismeansthattheFEFtriestominimizeexplorationandkeep
theposteriorandpriorasclosetogetheraspossible. Thisminimizinginformationgain
termisanalogoustothecomplexitytermintheVFEwhichfunctionsasaregulariser
whichattemptstokeeptheposteriorasclosetotheprioraspossible,whilestillfitting
thedata. Here,weseethatthegoaloftheFEFisto,ineffect,maximizereward,while
tryingtolearnaslittleabouttheenvironmentaspossible. Whilethismayseemtobean
unfortunateobjective,insomesmallcasesitmaybebeneficial,especiallywheninthe
caseofofflinereinforcementlearning,wherethereisnocontinualinteractionwithan
environment,onlytryingtolearnanoptimalpolicyfromagivendatasetofinteractions
((Levine, 2018). In such cases, failures of generalization and extrapolation can often
result in poor results whenever the learned policy is moved even slightly off the data
manifold,andthiskindofconservativeregularisationofthelearningprocesscanprove
highlybeneficial(Levine,Kumar,Tucker,&Fu,2020). Bycontrast,theEFEmaximizes
theinformationgainterm,sinceitisnegative,andtriestodrivetheposteriorandprior
asfarapartaspossible. Thisresultsininformation-seekingexploration.
However, while the EFE has an intuitively better exploratory grounding, it is not a
Chapter5. TheMathematicalOriginsofExploration 225
boundonthelogmodelevidence,astheFEFis. Wecanshowthisstraightforwardlyby
notingthatthe‘extrinsicvalue’termoftheEFEsimplyisthelogmodelevidence,
EFE=E [lnq(x |π)−lnp˜(o ,x )]
q(ot,xt|π) t t t
≈E [lnq(x |π)−lnq(x |o )−lnp˜(o )]
q(ot,xt|π) t t t t
≈ −E [lnp˜(o )] −E D [q(x |o )(cid:107)q(x |π)]| (5.7)
q(ot|π) t q(ot|π) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
NegativeExpectedLogModelEvidence InformationGain
andthatthus bythenon-negativityof KLdivergences,theEFEis alowerbound onthe
log model evidence. This bound is in the wrong direction, since to make it tight, the
EFEshouldbemaximized insteadofminimized.
Importantly, in the definition of the EFE there is an approximation step where we
have approximated p(x |o ) with the approximate posterior q(x |o ). If we make this
t t t t
approximationexplicit,wecanwritetheEFEas,
EFE=E [lnq(x |π)−lnp˜(o ,x )]
q(ot,xt|π) t t t
≈E [lnq(x |π)−lnp(x |o )−lnp˜(o )]
q(ot,xt|π) t t t t
≈E [lnq(x |π)−lnp(x |o )−lnp˜(o )+lnq(x |o )−lnq(x |o )]
q(ot,xt|π) t t t t t t t t
≈ −E [lnp˜(o )] +E D [q(x |o )(cid:107)p(x |o )]|
q(ot|π) t q(ot|π) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
NegativeExpectedLogModelEvidence PosteriorApproximationError
(cid:124) (cid:123)(cid:122) (cid:125)
FEF
−E D [q(x |o )(cid:107)q(x |π)]| (5.8)
q(ot|π) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125)
InformationGain
Where we see that the EFE can be both an upper and lower bound on the log model
evidencedependingonwhethertheinformationgaintermortheposteriordivergence
term is larger. We can thus see that the likely time-course of the EFE is to cycle
around the bound over the course of inference until, potentially, it reaches it. This is
because, at the start of training, when inference is poor, the posterior divergence is
likelygreaterthantheinformationgain,sotheEFEfunctionsasanupperboundand
Chapter5. TheMathematicalOriginsofExploration 226
minimizingitgetsusclosertothetruelogmodelevidence. Thiseffectlikelyquickly
fadesawayastheinformationgaintermbecomesbigger,andheretheEFEminimizing
agent will preferentially explore its environment in an information-seeking fashion,
drivingtheEFEawayfromthereallogmodelevidencefortheenvironment. Finally,
if there are no residual sources either of posterior divergence (so that the true and
approximateposteriorsareinthesameclass),orinformationgain(sothattheagenthas
aperfectmodeloftheenvironment,andtheenvironmenthasnointrinsicstochasticity
which gives rise to aleatoric uncertainty), then both the posterior divergence and the
informationgaintermswillbe0,andtheEFEwillfinallyconvergetoexactlythelog
model evidence. While this behaviour of the EFE functional may lead to adaptive
behaviour,itisnotparticularlymathematicallyprincipledasanextensiontotheVFE,
and thus it is not necessarily clear why the EFE should be considered to be a better
extensionoftheVFEthantheFEF.
This derivation also reveals an interesting connection between the EFE and the FEF.
Specifically,itisrevealedthattheEFEissimplytheFEFminusanadditionalinforma-
tiongainterm,thuseffectivelycomprisingthefreeenergyintothefuture(FEF),with
an additional exploratory information gain term. This derivation can also be derived
straightforwardlyfromadirectcomparisonofthetwofunctionals,
q(x |o ) q(x |o )
FEF (π)−IG =E ln( t t )−E ln( t t )
t t q(ot,xt|π)
p˜(o ,x )
q(ot,xt|π)
q(x |π)
t t t
q(x |o )q(x |π)
=E ln( t t t )
q(ot,xt|π)
p˜(o ,x )q(x |o )
t t t t
q(x |π)
=E ln( t )
q(ot,xt|π)
p˜(o ,x )
t t
=EFE(π) (5.9)
t
WecanthusunderstandtheoriginoftheinformationgaintermintheEFE–itissimply
theFEFintothefutureminustheinformationgainexplorationterm. Thismeansthat,
ineffect,theexploratorypropertiesoftheEFEaresimplypresentbyconstruction. Is
Chapter5. TheMathematicalOriginsofExploration 227
it possible, then, to derive mathematically or intuitively principled objectives which
maintaintheinformationseekingpropertiesoftheEFE?
While this question is definitively answered later in this chapter, here we present a
hint of the solution. We propose a novel objective, which we call the free energy
of the expected future (FEEF), which can be characterised simply as the divergence
between the expected beliefs about future observations and states q(o ,x ) and the
t t
desireddistribution p˜(o ,x ). TheFEEFobjectivecanbewrittenas,
t t
π∗ =argminD [q(o ,x |π)||p˜(o ,x )] (5.10)
KL t:T t:T t:T t:T
π
Ineffect,thisobjectivecanbeunderstoodascompellinganagenttobringapredicted
(variational)worldandadesired(generative)distributionintoalignment. Thisobjective
hasastrongintuitivebasisforunderstandingadaptiveaction,sincewearesimplytrying
tominimizethedifferencebetweenourveridicalbeliefsaboutthefutureandourdesires.
Sincethedesiredistributionisassumedfixed,theonlywaytomaximizetheiralignment
istotakeactiontoforcethepredictedbeliefdistributiontowardsthedesireddistribution.
If the belief distribution is accurate, then this will result in trajectories that really do
taketheagenttowardsitsdesireddistribution. Crucially,wecanthendecomposethis
objectiveintoanextrinsicandintrinsicinformationseekingterm,justliketheEFE.
FEEF(π) =E ln (cid:2)
q(o
t
,x
t
|π)
(cid:3)
t q(ot,xt|π)
p˜(o ,x )
t t
≈E D (cid:2) q(o |x )(cid:107)p˜(o ) (cid:3) −E D (cid:2) q(x |o )(cid:107)q(x |π) (cid:3) (5.11)
q(xt|π) KL t t t q(ot|π) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue IntrinsicValue
Here,we seethatthe epistemic informationseekingtermis identicaltothat of theEFE,
andthuswewouldexpectFEEFandEFEminimizingagentstoshowsimilarexploratory
behaviour. Thekeydifferencebetweentheseobjectiveslies intheextrinsicvalueterm.
While the EFE simply aims to maximize the likelihood of the desire distribution
Chapter5. TheMathematicalOriginsofExploration 228
underthevariationalbelief distribution,theFEEFexplicitlytriestominimizetheKL
divergencebetweenthem,andthustrytomatchthetwodistributions.
Anotherwayoflookingatthesamethingistoconsiderthisstraightforwardrelationship
betweentheEFEandtheFEEF,
FEEF(π) =D (cid:2) q(o ,x )(cid:107)p˜(o ,x ) (cid:3)
t KL t t t t
=E (cid:2) lnq(o |x )]+E (cid:2) lnp˜(o |x )]−E D (cid:2) q(x |o )(cid:107)q(x |π) (cid:3)
q(ot,xt) t t q(ot,xt) t t q(ot|π) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ObservationLikelihood EFE
(5.12)
Wecanthussee,thattheFEEFissimplytheEFEplusanobservationlikelihoodentropy
term. Thistermistobemaximizedandthuseffectivelyprovidesanadditionalsource
ofrandomexplorationtotheFEEFagentratherthantheEFE.Ineffect,theFEEFagent
optimizestheEFEwhiletryingtokeepitsobservationmappingasrandomaspossible.
AnotheradvantageoftheFEEF,isthatitisequivalenttotheVFEatthepresenttime.
This is because the observation entropy term is constant since it cannot be affected
by future observations, and thus the expression as a whole reduces to the VFE. This
meansthattheFEEFcanbeusedasaunifiedobjectiveforbothperceptionandaction,
while the EFE can only be used for control. Due to this, a FEEF agent can have all
distributions trained jointly on the FEEF objective while for an active inference agent,
typically,ifthetransitionandlikelihoodmatricesarelearnt,theyareoptimizedagainst
theVFE,whileonlyactionselectiontakesplaceusingtheEFE.Thisaddsanadditional
degreeofsimplicityandelegancetoFEEF-minimzingagentswhiletheyretainthesame
exploratorybehavioursasactiveinferenceagents.
5.2.1 Control as Inference and Active Inference
This relationship between the FEF and the EFE sheds light upon the relationship
between active inference and control as inference approaches to control. While the
formulations at an abstract level are very similar – both attempt to solve the control
Chapter5. TheMathematicalOriginsofExploration 229
problem by deriving inference algorithms which operate on graphical models, and
usually utilize the machinery of variational inference to do so – at a lower detailed
levelthe theories appear quite different and are presented with substantially differing
motivationsandnotation. Usingournewfoundunderstandingofvariationalobjective
functionalsofthefuture,suchastheEFEandtheFEF,herewepindownwhatexactly
therelationshipbetweencontrolasinferenceandactiveinferenceis.
First,wenotethattherearemanystraightforwardnotationaldifferencesbetweenthe
theories which can be overcome. One obvious difference is that active inference is
primarilyconcernedwiththeinferringofpolicies(oractionsequences)whilecontrol
asinferenceconcernsitselfwithsimplyinferringpolicies,orsingleactionsforagiven
timestep. It is important to note, however, that it is possible to reformulate active
inferencesothatitinferspolicies,and,conversely,toreformulatecontrolasinference
sothatitinfersfullactionplans. Aseconddistinctionisthatactiveinferenceistypically
formulatedforPOMDPswhilecontrolasinferenceonlyforMDPs. Itisstraightforward,
however, to extend control as inference to the POMDP setup, which results in the
followingobjective,
(cid:16) (cid:17)
L(φ) =D q (x ,a )(cid:107)p(x ,a ,o ,Ω )
CAI KL φ t t t t t t
(cid:16) (cid:17)
=−E (cid:2) lnp(Ωx ,a ) (cid:3) +D q(x )(cid:107)p(x |x ,a )
q
φ
(xt,at) | t t KL t t t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue Statedivergence
(cid:16) (cid:17)
+E (cid:2) D q (a |x )(cid:107)p(a |x ) (cid:3) −E (cid:2) lnp(o |x ) (cid:3) (5.13)
q(xt) KL φ t t t t q
φ
(xt,at) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ActionDivergence ObservationAmbiguity
Herewehaveusednotationstandardincontrolasinferencederivations,namelyq (a |x )
φ t t
is an amortized state-action policy and Ω is the ‘optimality variable’. Importantly,
t
this novel extension of control-as-inference to a POMDP setting leads directly to a
straightforwardimplementationintermsofdeepreinforcementlearning,similartothe
approaches in Chapter 4. Specifically, this objective can either be expressed directly
as a sum over trajectories, and thus optimized by planning algorithms using model-
baseddeepreinforcementlearningor,alternatively,itcanbeexpressedrecursivelyand
Chapter5. TheMathematicalOriginsofExploration 230
computedusingavalueorQfunctionapproachwhichlendsitselfnaturallytomodel-
free deep reinforcement learning approaches. The key extension would be learning
a probabilistic encoder-decoder model, most likely a VAE, to infer the distributions
q(x |o )and p(o |x )andthentooptimizetheentropyoftheVAEdecoderinthecontrol
t t t t
objective, in accordance with this objective function. Empirically investigating the
performanceofthismethod,andtheimpactoftheobservationambiguityterm,hasnot,
tomyknowledge,beenexploredintheliterature,andwouldbeaninterestingavenue
forfurtherwork.
Secondly,wecansimilarlyreformulatethecontrolasinferenceapproachtoinferplans
instead of policies. This is done by extending the generative model to cover whole
trajectoriesinsteadofsingleobservations,states,oractions. Wetheninferaconstant
random variable π the policy for the whole trajectory. Written out explicitly, from
this generative model you can derive a variant of the active inference optimal plan
derivationtodiscoverthattheoptimalplanunderthecontrolasinferenceissimplythe
softmaxpathintegraloverthevariationalfreeenergy(VFE),augmentedwithoptimality
variables,andextendedintothefuture.
(cid:16) (cid:17)
L =D q(x ,π)(cid:107)p(x ,π,o ,Ω )
CAI KL t:T t:T t:T t:T
(cid:16) T T (cid:17)
=D q(π)∏q(x |π)(cid:107)p(π)∏p(Ω |x ,π)p(o |x )p(x |x ,π)
KL t t t t t t t−1
t t
(cid:16) T (cid:17)
(cid:2) (cid:3)
=D q(π)∑D q(x |π)(cid:107)p(Ω |x ,π)p(o |x )p(x |x ,π) (cid:107)p(π)
KL KL t t t t t t t−1
t
(cid:16) T (cid:17) (cid:16) T (cid:17)
=D q(π)(cid:107)p(π)exp(−∑L (π)) =⇒ q∗(π)=σ p(π)−∑L (π)
KL t t
t t
(5.14)
WecandecomposethisVFEfunctionalintothefutureas,
Chapter5. TheMathematicalOriginsofExploration 231
L (π) =E (cid:2) lnq(x |π)−lnp(x ,π,o ,Ω )]
t CAI q(xt|π) t t t t
(cid:16) (cid:17)
=−E (cid:2) lnp(Ω |x ,π) (cid:3) +D q(x |π)(cid:107)p(x |x ,π) −E (cid:2) lnp(o |x ) (cid:3)
q(xt|π) t t KL t t t−1 q(xt|π) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue Statedivergence ObservationAmbiguity
(5.15)
WhichwecanseeisequivalenttothestandardcontrolasinferencePOMDPobjective,
except that it is missing the action divergence terms. The action divergence terms
are missing simply because full policies are inferred instead of individual actions.
Conversely,wecanrederiveactiveinferencetoinferindividualactionsratherthanfull
policies. Todoso,wesimplyneedtoaddindividualactionsintothegenerativemodel
andvariationaldensityandthencrankthroughthederivation,
(cid:104) (cid:105)
−F (φ)=E lnq (a ,x )−lnp˜(x ,o ,a )
t q(ot,xt,at) φ t t t t t
(cid:104) (cid:105)
=−E (cid:2) lnp˜(o |a ) (cid:3) −E D (cid:0) q(x |o ,a )(cid:107)q(x |a ) (cid:1)
q(ot|at) t t q(ot,at|xt) KL t t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue IntrinsicValue
(cid:104) (cid:105)
+E D (cid:0) q (a |x )(cid:107)p(a |x ) (cid:1) (5.16)
q(xt) KL φ t t t t
(cid:124) (cid:123)(cid:122) (cid:125)
ActionDivergence
Hereweseethattheexpressiontobeoptimizedwithrespecttothepolicyparametersφ
issimply thestandardactiveinferenceobjectivewithanadditional action-divergence
term. If weassumethe actionprior p(a |x )isuniform, thenwe regainthe well known
t t
control as inference policy entropy term. Now that we have extended the theories to
allowforadirectside-by-sidecomparison,wecanseethatthetwomajordifferences
lieintheinformationgaintermforactiveinferenceasopposedtothecomplexity‘state-
divergence’ term for control as inference, and secondly that the control as inference
approachcontainsanadditional‘likelihoodentropy’terminitsobjectivewhichactive
inferencelacks. Weknownowthattheinformationgainterminactiveinferencearises
directlyfromthedefinitionoftheEFEfunctional,whichisnotanintrinsicpartofactive
Chapter5. TheMathematicalOriginsofExploration 232
inferenceandmaynotbetheoreticallyjustified. Indeed,ifwereplacetheEFEinthe
active inference derivation with the FEF, we can obtain an objective identical to the
controlasinferenceapproachexceptthatithasnolikelihoodentropyterm.
−F ˆ (φ)=E (cid:2) lnq (x ,a )−lnp˜(o ,x ,a ) (cid:3)
t q
φ
(xt,ot,at) φ t t t t t
(cid:16) (cid:17) (cid:16) (cid:17)
=−E (cid:2) lnp˜(o |x ) (cid:3) +D q(x )(cid:107)p(x |x ,a ) +D q (a |x )(cid:107)p(a |x )
q
φ
(xt,at) t t KL t t t−1 t−1 KL φ t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue Statedivergence ActionDivergence
(5.17)
This means that, effectively, we can consider control as inference as active inference
withaFEFobjectiveor,conversely,thatactiveinferenceissimplycontrolasinference
withanonstandard EFEobjective. The questionthenremains,why doesthecontrolas
inferenceobjectivepossessanadditionallikelihoodentropytermwhichactiveinference
does not, since it is not due to a difference in objective function. We demonstrate
that the difference actually arises due to the way goals or likelihoods are encoded
in the inference procedure. Specifically, active inference encodes goals or rewards
directly into the generative model, so that active inference agents function with a
biased generative model which blends reward-driven and veridical perception. By
contrast,controlasinference,keepsveridicalinferenceandrewardcomputationentirely
separate,andinsteadincludesrewardsintotheinferenceprocedurethroughtheuseof
exogenousoptimalityvariables. Thetwomethodsthussolvesubtlydifferentinference
problems due to these distinctions in how they encode rewards. Put simply, active
inferenceencodesrewardsandgoalsthroughpriors;controlasinferenceencodesthem
through conditioning. Phrased intuitively, we can think of control as inference as
solvingtheinferenceproblem: AssumingthatIhaveactedoptimallyinthefuture,what
actionsdoIinferIwillhavetaken,whileactiveinferencesolvestheinferenceproblem:
Infer my most likely actions, given that I strongly believe in the future I will observe
highly desirable states. In sum, control as inference maintains a strict distinction
Chapter5. TheMathematicalOriginsofExploration 233
between a veridical perceptual generative model, which generates objectively likely
futureoutcomesforagivenactionsequence,whileactiveinferencemaintainsabiased
generativemodelwhichpreferentiallypredictsdesiredoutcomes. Controlasinference
conditions this accurate generative model on observing high reward contingencies, and
thus maintains an adaptive action plan. Active inference, on the other hand, simply
maximizesthelikelihoodof thisbiasedgenerativemodel, thusalsoinferringadaptive
actions.
Whilethisdistinctionmayseemarcane,itactuallyspeakstodeepphilosophicaldiffer-
ences in perspectives between the two theories. Control as inference arises from the
cognitivist and logical views of artificial intelligence, which maintains that in some
sense intelligence is pure thought, which can then be unbiasedly applied to inferring
action. Control as inference maintains the modularity thesis, which is widespread
throughout artificial intelligence and engineering fields which is the strict separation
ofperceptionandcontrol. Perceptionaimstobuildupanaccurateworldmodel,while
controlusesthisworldmodeltocomputethebestactions. Activeinference,bycontrast,
hasmuchmoreincommonwithenactivist,embodied,andcyberneticalviewsofcontrol,
which see the agent and environment inextricably enmeshed in a fundamental feedback
process–theperceptionactionloop. Here,itisunnecessarytomaintaina strict separa-
tionbetweenperceptionandcontrol. Indeed,veridicalperceptionisunnecessarysince
theultimateaimofperception,inthisview,isnottheunbiasedmodellingoftheworld,
butrathertosubserveadaptiveaction.
The mathematical effect of this distinction is that control as inference maintains two
separatelikelihoods-averidicalperceptuallikelihood p(o |x ),andareward-encoding
t t
optimality likelihood p(Ω|x ,a ), while active inference only maintains a biased ob-
t t
servational likelihood p˜(o ). Due to this, control as inference approaches possess an
t
additionallikelihoodentropytermwhichactiveinferenceoneslack.
Chapter5. TheMathematicalOriginsofExploration 234
5.3 Evidence and Divergence Objectives
Given that we now see that the control as inference framework, even when extended
to full action policies and partially observable environments, does not manifest an
information-seekingexplorationtermwhiletheexpectedfreeenergydoesand,more-
over, that conversely the EFE does not function as a bound on a known quantity like
the expected model evidence which is used in variational inference, we are left with
thequestionoftryingtounderstandwhetherandhowinformation-seekingobjectives
can be derived in a mathematically principled manner. Here, we argue that objective
functionalsforcontrolexistingintheliteraturecanbesensiblysplitintotwoseparate
classes –evidence objectives, which typically arise from a direct variational inference
approach,andwhichdonotmanifestinformationgainterms,anddivergenceobjectives
whichariseoutofdirectlyminimizingaKLdivergencebetweentwomodels,andwhich
dogiverisetoinformationseekingterms. Wethenshowhowwellknownobjectivesin
avarietyofliteraturescanbeunderstoodinourscheme.
First, we need to formalize our mathematical setup. We assume that we have an
environmentwhich,fromtheagent’sperspective,isanunknownblackbox. Theagent
inputsactionsa totheenvironmentanditoutputsobservationso inreturn. The
1:T 1:T
exact process that producesthese observationsin theenvironment isforever unknown.
The agentcanassumethatthe observations inthe environment areproduced bysome
form of POMDP model with hidden latent states x , but this is merely the agent’s
1:T
model of the environment. The agent can also parametrize its model with learnable
parametersθ, whichitassumesare eitherconstantthroughoutits entirelifetime,or else
change on a much slower timescale than the latent states x . Next, to formalize a
1:T
control, asopposed tojustan inferenceproblem, weneed somenotion ofthe reward or
goal that the agent wishes to achieve. We formalize this by specifying that the agent
possessesanadditionaldesiredistribution p˜(o )overtheobservationsitreceives. This
1:T
distributionencodesthe‘idealworld’oftheagentinthesensethattheseareprecisely
Chapter5. TheMathematicalOriginsofExploration 235
the observations it aims toachieve. To mapthis to a standardreinforcement learning
setting, with rewards, we can perform the now familiar trick of defining, p˜(o ) =
1:T
exp(−r(o )) where r(o ) is the total reward achieved across a given observation
1:T 1:T
trajectory. Importantly,thisequivalence tostandardreinforcementmethodologyisonly
aspecialcase,andthatthisformulationintermsofadesiredistributionismoreflexible
in its specification of the rewards. Specifically, here we assume no specific form of
the desire distribution, nor any factorization of desires across timesteps, for instance,
sothedesiredobservationsinthefuturecandependonthedesiredobservationsatthe
presentinarbitrarilycomplexways. Itisalsostraightforwardtoextendtheframework
toadesireddistributionsoveractionsaswell, by defining p˜(o ,a ). Thisisuseful
1:T 1:T
for instance if we want to penalize the costs (i.e. energetic for biological organisms
or robotic systems) of action. For instance, to define a quadratic cost in the action
magnitude, we can define p˜(a
1:T
)=∏
t
TN(a;0,σ), or a Gaussian distribution in the
action centered around 0. The variance of the Gaussian (σ) effectively defines the
weightingcoefficientwhichscalesthesizeofthepenalty. Here,tokeepnotationsimple,
wedonotdiscussactionpenaltiesandonlyworkwithadesiredobservationdistribution
p˜(o ),buttheextensiontoactionsisentirelystraightforward.
1:T
The general objective of the control problem is to compute an action trajectory a
1:T
which results in the realization of your goals, which are defined through the desire
distribution p˜(o ). Wealsoassume,thatiftheagentactuallyexecutesagivenaction
1:T
trajectory,itwillreceivearealtrajectoryofobservationsfromtheenvironmentaccording
to some distribution p(o |a ). This distribution can be approximated either by
1:T 1:T
samplingrealtrajectoriesfromtheenvironment,asisimplicitlythecaseinmodel-free
reinforcement learning methods, or else the agent can explicitly model this distribution
as p(o |a ;θ), with parameters θ, which may be implemented as a deep neural
1:T 1:T
network,forexample. Wecallthisdistributionthepredicteddistribution,sinceinsome
senseitiswhattheagentpredictswilloccurifitexecutesagivenactiontrajectory.
Chapter5. TheMathematicalOriginsofExploration 236
With the preliminaries settled, we can formally define the evidence and divergence
objectives. Evidence objectives tryto maximize thelikelihood of thedesire distribution
averagedunderthepredicteddistribution. Intuitively,wewishtofindtheactiontrajec-
torythat maximizesthe expectedlikelihood ofthe desiredistribution. Mathematically,
wecanrepresentthisas,
L =argmaxE [lnp˜(o )] (5.18)
evidence p(o |a ) 1:T
1:T 1:T
a
1:T
Where here, as is usual, we optimize the log of the desire distribution instead of the
desire distribution itself. Since the log is a monotonic function, this does not affect
theactualoptimumoftheproblemandsinceweoftenfactorizethedesiredistribution
intoproducts,thelogwillturnthatintoasum,whichisusuallymuchbetterbehaved
numerically.
Conversely, for a divergence objective, instead of maximizing the likelihood of the
desiresunderthepredicteddistribution,weinsteadwithtodirectlyminimizethediver-
gence of the desire and predicted distribution. In effect, we want to make the desire
distributionandthepredicteddistributionmatch. Mathematically,wecanwritethisas,
L =argminD [p(o )|a )||p˜(o )] (5.19)
divergence KL 1:T 1:T 1:T
a
1:T
Wherehere weuse theKL divergenceD [Q||P]=E ln Q . Intuitively, wecan think
KL Q P
ofthedistinctionbetweenevidenceanddivergenceobjectivesbeingthattheevidence
objective seekstomatch thepredicted distribution tothe mode ofthe desiredistribution
– i.e. it focuses all effort onto the most desired observations, while neglecting the
lesser desired observations. This is why evidence objectives typically arise from
direct maximizing principles such as utility maximization, or variational inference.
Meanwhile,divergenceobjectivesseektopreciselymatchthepredicteddistributionand
thedesireddistribution,sothatifsomeobservationisnotthemostpreferredone,but
hassomemoderatelevelofpreference,thedivergenceobjectivewouldseektohavethat
Chapter5. TheMathematicalOriginsofExploration 237
observation manifest some amount of time in proportion to its relative preferredness.
Thismeansthatingeneral,ifthedesiredistributionisspreadout,thenagentswillseek
to realize a spread-out distribution of predicted observations. In contrast, under an
evidence objective, even if the desire distribution is broad, the agent will continue to
placeallefforttokeepthepredicteddistributionapeakaroundthemodeofthedesire
distribution.
(a)OptimizingwithanEvidenceObjective (b)OptimizingwithaDivergenceObjective
Figure 5.1: Numerical illustration of optimizing a multimodal desired distribution with
anEvidenceobjective(PanelA)vsaDivergenceObjective(panelB).Thedesiredistri-
bution consisted of the sum of two univariate Gaussian distributions, with means of 1
and 4 and variances of 1 and 0.4 respectively. We then optimized an expected future
distribution, which also consisted of two Gaussians with free means and variances
using both an Evidence and a Divergence objective. As can be seen, optimizing the
Evidence Objective results in the agent fitting the predicted future density entirely to
an extremely sharp peak around the mode of the desired distribution. Conversely,
optimizing a divergence objective leads to a precise match of the predicted and de-
sireddistributions(panelBshowsthetwodistributionsalmostpreciselyontopofone
another). As a technical note, to be able to see both the evidence and deisre dis-
tributions on the same scale, for the evidence objective the predicted distribution is
normalizedbutthedesireddistributionisnot. Codeforthesesimulationscanbefound
at: https://github.com/BerenMillidge/origins_information_seeking_exploration.
Chapter5. TheMathematicalOriginsofExploration 238
Another way we can interpret the difference between the objectives is in terms of the
effectof theobjectiveupon thepredicted distribution p(o |a ). Specifically,we can
1:T 1:T
thinkofthedivergenceobjectiveasabalancebetweentryingtomaximizethelikelihood
ofthedesireddistributionunderthepredicteddistribution,andtryingtomaximizethe
entropyofthepredicteddistribution. Wecanthinkofthisasthedivergenceobjective
as effectively saying ‘try to maximize your desires or utility while also keeping the
futureasbroadaspossible’–i.e. keepingyouroptionsopen. Thiscanbedemonstrated
mathematicallythroughthesimpledefinitionoftheKLdivergence,
L =argminD [p(o )|a )||p˜(o )]
divergence KL 1:T 1:T 1:T
a
1:T
=argmin−H[p(o |a )]−E [lnp˜(o )]
1:T 1:T p(o |a ) 1:T
1:T 1:T
a 1:T (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
PredictedEntropy EvidenceObjective
Here we see that we can express the divergence objective simply as the evidence
objective plusthe maximization ofthe entropyofthe predicteddistribution. Ineffect,
thedivergenceobjectivesimplyincludesanentropyregularisertothestandardevidence
objective. Conversely,wecan alsoexpresstherelationshipbetweenthe objectivesin
the other way. We can think of the evidence objective as simply trying to match the
predicted and desire distribution while simultaneously minimizing the entropy of the
predicteddistribution. Thisisstraightforwardtoshowmathematically,
L =argmaxE [lnp˜(o )]
Evidence p(o |a ) 1:T
1:T 1:T
a
1:T
p(o |a )
=argmaxE [lnp˜(o ) 1:T 1:T ]
p(o |a ) 1:T
1:T 1:T p(o |a )
a 1:T 1:T 1:T
=argmax −D [p(o |a )||p(o ˜ )]− H[p(o |a )] (5.20)
KL 1:T 1:T 1:T 1:T 1:T
a 1:T (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Divergence ExpectedFutureEntropy
Thisformulation givesastraightforward intuitionfor the ‘mode-seeking’behaviour of
theevidenceobjective. Theevidenceobjectiveseeks tomatchthepredicted anddesire
distribution, while also being penalized for the breadth of the predicted distribution.
Thebestwaytoresolvethistensionisbyformingahighlypeakedpredicteddistribution
Chapter5. TheMathematicalOriginsofExploration 239
aroundthemodeofthedesiredistributionsothatitcancoverasmuchprobabilitymass
aspossible.
Interestingly, differences between the two formulations generally only emerge when
thedesiredistributionisbroadandcomplex. Here,thedivergenceobjectivewilltend
toforcethepredicteddistributionanddesiredistributiontomatchintheircomplexity,
while the evidence distribution will seek out and focus around its mode. Another
intuitive wayof thinkingabout thedistinction is thatthe divergenceobjective implicitly
maximizes some sort of future empowerment, by implicitly trying to keep all future
optionsopen,byseekingtomakefutureobservationsasentropicaspossible. Evidence
objectives, by contrast, seek the opposite. They aim for ‘precise futures’ where the
amount of future variability is as low as possible. It is straightforward to show that,
unlike evidence objectives, divergence objectivescan be immediately decomposed into
an‘extrinsic’valuedivergenceterm,andaninformation-seekingexploratoryterm,
p(o |a )
D [p(o |a )||p˜(o )]=E [ln 1:T 1:T ]
KL 1:T 1:T 1:T p(o |a )
1:T 1:T p˜(o )
1:T
p(o |a )
=E [ln 1:T 1:T ]
p(o |a )
1:T 1:T p˜(o )
1:T
p(o ,x |a )
=E [ln 1:T 1:T 1:T ]
p(o |a )
1:T 1:T p˜(o )p(x |o )
1:T 1:T 1:T
=E D [p(o |x )||p˜(o )]
p(x ) KL 1:T 1:T 1:T
1:T
(cid:124) (cid:123)(cid:122) (cid:125)
DesireDivergence
−E D [p(x |o )||p(x )] (5.21)
p(o |a ) KL 1:T 1:T 1:T
1:T 1:T
(cid:124) (cid:123)(cid:122) (cid:125)
InformationGain
Importantly,thepropertythatdivergenceobjectivesgiverisetodirectedinformation-
seekingexploratoryterms,arisesdirectlyfromthepreviouslydiscussedintuitionthat
they attempt to maximize the entropy of future observations. Such information gain
termsarisewheneverthepredicteddistributionisextendedto modeladditionallatent
Chapter5. TheMathematicalOriginsofExploration 240
variablesorparameters. Theproofofthisisstraightforward,
H[p(o |a )]=E [lnp(o |a )]
1:T 1:T p(o ,x |a ) 1:T 1:T
1:T 1:T 1:T
p(o ,x |a )
=E [ln 1:T 1:T 1:T ]
p(o ,x |a )
1:T 1:T 1:T p(x |o )
1:T 1:T
=−E H[p(o |x )]−E D [p(x |o )||p(x )]
p(x ) 1:T 1:T p(o |a ) KL 1:T 1:T 1:T
1:T 1:T 1:T
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LikelihoodEntropy ExpectedInformationGain
(5.22)
Putverbally,thisrelationshipshowsthattomaximizetheentropyofadistribution,if
the distribution can be understood in terms of a latent set of variables, requires both
maximizing the conditional entropy of the variable given the latent variables, while
simultaneously maximizing the mutual information of the latent variables between
the observed and latent variables. Intuitively, within our context, this means that to
maximize the entropy of future observations, it is necessary to successfully model the
relationship between these future observations and their latent states, which entails
maximizingthemutualinformationbetweenthelatentsandtheobservations. Itisthe
maximizationofthismutualinformationwhichundergirdstheexploratoryinformation-
seeking behaviour whichis manifested bydivergenceobjectives. Conversely, the fact
that evidence objectives seek to minimize the entropy of the predicted distribution
meansthattheyimplicitlyseektominimizetheamountofmutualinformationbetween
observationandlatentvariables. Wecanthinkofthisasdivergenceobjectivesaimto
reachagivensetofgoalswhilealsolearningasmuchaspossibleabouttheenvironment,
inordertopreciselymatchthetwodistributions. Evidenceobjectives,ontheotherhand,
seektoreachtheirgoalswhilelearningas little aspossibleabouttheenvironment, and
keepingtheenvironmentasregularandpredictableaspossible.
Nowthatwehaveproposedandgivenconsiderableintuitionforourdichotomybetween
evidence and divergence objectives, we look to see where these objectives appear in
theliterature,andhowtheycanexplaindifferencesinexploratorybehaviourbetween
differingparadigms.
Chapter5. TheMathematicalOriginsofExploration 241
5.3.1 Control as Inference
It is straightforward to show that the control as inference, and variational inference
objectives are bounds upon the evidence objective. Put simply, we have that control
as inference aims to solve the inference problem of inferring an action distribution
given a desired set of observations. Specifically, we seek to obtain the distribution
p(a |o˜ ) where we use o˜ to denote a set of hypothetical ‘optimal’ actions which
1:T 1:T
havebeenconditionedupon. Tofindthisposterior, weuseavariationalapproximation
by defining the variational density q(a ) and optimizing the following variational
1:T
lowerbound,
D [q(a ||p(a |o˜ )]=D [q(a ||p˜(o ,a )]−lnp(o )
KL 1:T 1:T 1:T KL 1:T 1:T 1:T 1:T
≥D [q(a ||p˜(o ,a )]
KL 1:T 1:T 1:T
(cid:124) (cid:123)(cid:122) (cid:125)
CAIObjective
Which serves as the control as inference objective. It is then straightforward to show
thatthisobjectiveservesasaboundonanevidenceobjective,
(cid:90)
argmaxlnp˜(o )=argmaxln dxp˜(o x )
1:T 1:T 1:T
a a
1:T 1:T
(cid:90) p˜(o x )q(a )
1:T 1:T 1:T
=argmaxln dx
q(a )
a 1:T 1:T
p˜(o ,a )
≥argmaxE [ln 1:T 1:T ]
q(a )
1:T q(a )
a 1:T 1:T
≥argmin−D [q(a )||p˜(o ,a )]
KL 1:T 1:T 1:T
a )
1:T
=≥L (5.23)
CAI
And thus we can see that the control as inference framework optimizes a variational
bound on the evidence objective. This straightforwardly explains why control as
inference approaches do not give rise to directed, information-seeking exploration,
but rather instead only induce random action entropy maximizing exploration terms.
Thisrandom exploration, whilehighly efficientinmanycontemporarydense-reward
reinforcementlearningbenchmarktasks,wherearandompolicyoftensufficestocover
Chapter5. TheMathematicalOriginsofExploration 242
enough ofthe statespace, it isincreasingly ineffective inextremely high dimensional,
andsparserewardenvironments.
5.3.2 KL Control
Another control method in the literature that has been applied, and studied fairly
extensivelyisKLcontrol. Althoughnotaswidelyusedinreinforcementlearning,itis
oftenimplicitoptimizedincontroltasks,andhasdeeprelationshipswiththebeginning
of the control as inference approach, as well as more esoteric path integral methods.
Moreover, it has recently seen renewed application in deep reinforcement learning
approachessuchasstate-marginalmatchingof(L.Leeetal.,2019). KLcontrol,asthe
nameimplies,choosescontroltominimizetheKLdivergencebetweenthecurrentstate
andasetofdesiredstates,leadingtothefollowingobjectivefunction,
L =argminD [p(x )||p˜(x )] (5.24)
KL KL 1:T 1:T
a
1:T
Here we have used x instead of o to denote that KL control has typically only been
appliedtofully-observedMarkovianMDPenvironmentsasopposedtofullPOMDPdy-
namics. Assuch,whiletheKLcontrolobjectiveisclearlyjustthedivergenceobjective,
itssuperiorexploratorycapabilitieshavenotbeensignificantlyexploredintheliterature
duetotheonlyapplicationscurrentlybeinginfullyobservableenvironmentswhilethe
information-seekingexploratorytermsrequiretheextensiontohiddenvariablemodels.
Another interesting point is that the objective in continuous time active inference in
predictive coding models can also be as a KL divergence between a desired ‘set-point’
andacurrentlyobservedpoint,andisthusaninstanceofKLcontrol. However,these
modelsalsodo nothandlelatentvariablemodels,andthus alsodonotmanifestthefull
exploratorycapabilitiesofdivergenceobjectives.
Chapter5. TheMathematicalOriginsofExploration 243
5.3.3 Active Inference
Given that we know from previously, that the expected free energy contains an informa-
tion gain term, which gives rise to the information-seeking exploratory behaviour of
activeinferenceagents,itisworthinvestigatingtherelationshipoftheEFEtoevidence
and divergence functionals. Recall, from the previous section that the EFE formed
neitheranuppernoralowerbounduponthelogmodelevidence,butinsteadformedan
upperboundwhentheposteriordivergencewasgreaterthantheinformationgainterm,
anda lower boundotherwise,with thegoalof eventuallyconvergingdirectlytothe log
modelevidenceinthecasethatbothofthesetermsbecomezero. Notingthatthelog
modelevidencesimply,asthenamesuggests,istheevidence,objective,wecanrewrite
thisintermsofournewunderstandingas,
E [lnp˜(o)]=E [lnq(x)−lnp˜(o,x)]+E D [q(x|o)||q(x)]−E D [q(x|o)||p(x|o)]
q(o,x) q(o,x) q(o) KL q(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EvidenceObjective EFE InformationGain PosteriorDivergence
=⇒ E [lnp˜(o)]≥E [lnq(x)−lnp˜(o,x)]
q(o,x) q(o,x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EvidenceObjective EFE
If E D [q(x|o)||q(x)]≥E D [q(x|o)||p(x|o)] (5.25)
q(o) KL q(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
InformationGain PosteriorDivergence
sothattheEFEdoesnotstandinastraightforwardrelationshipasboundinanyspecific
directionontheevidenceobjective. Nevertheless,sincetheEFEcontainsaninformation
gain term to be maximized, as do divergence objectives, we might expect to obtain a
straightforward relationship between the EFE and the divergence objective. We can
writeouttherelationshipbetweenthedivergenceobjectiveandtheEFEasfollows,
Chapter5. TheMathematicalOriginsofExploration 244
(cid:82)
dxp(o,x)
D [p(o)||p˜(o)]=E [ln ]
KL p(o)
p˜(o)
(cid:124) (cid:123)(cid:122) (cid:125)
DivergenceObjective
(cid:82)
dxp(o,x)q(x|o)q(o,x)
=E [ln ]
p(o)
p˜(o)q(x|o)q(o,x)
(cid:82)
dxp(o,x)q(o,x)
≤E [ln ]
p(o)
p˜(o)q(x|o)q(o,x)
(cid:82)
dxp(o,x)q(o|x)q(x)
≤E [ln ]
p(o)
p˜(o)q(x|o)q(x|o)q(o)
≤E [lnq(x)−lnq(x|o)−lnp˜(o)]−E D [q(x|o)||p(o,x)]
p(o)q(x|o) p(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EFE VFE
+E D [q(x|o)||q(x)] (5.26)
q(x|o)p(o) KL
(cid:124) (cid:123)(cid:122) (cid:125)
InformationGain
Here we see that the EFE can be expressed in terms of the divergence objective, an
information gain term, and, interestingly, the variational free energy. In effect, the
divergence objective consists of the EFE, the VFE, and an information gain term.
Specifically, the EFE becomes an upper bound on the divergence objective when the
information gain term is greater than the variational free energy. This is similar to
previously where we saw that the EFE became a bound on the evidence when the
information gain is less than the posterior divergence. It is thus clear that the EFE
objective does not serve as a valid and consistent bound on either of the divergence
ortheevidenceobjectives. Similarly,wecanexpresstheEFEdirectlyintermsofthe
divergenceobjectiveasfollows,
D [p(o)||p˜(o)]=E D [p(o)q(o,x)||p˜(o)q(o,x)]
KL q(x|o)p(o) KL
(cid:124) (cid:123)(cid:122) (cid:125)
DivergenceObjective
=E D [p(o)q(o|x)q(x)||p˜(o)q(x|o)q(o)]
q(x|o)p(o) KL
=E [lnq(x)−lnp˜(o)−lnq(x|o)]+E D [q(x|o)||q(x)]− H[p(o)]
q(x|o)p(o) p(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EFE InformationGain MarginalEntropy
(5.27)
Chapter5. TheMathematicalOriginsofExploration 245
Where we see that the divergence objectives simply is the EFE plus an information
gain term, minus the marginal or predicted entropy term. As such, even within this
framework,themathematicaloriginandthebehaviouroftheEFEremainsunclearsince
the EFE does not form consistent bounds on either objective, but instead oscillates
aboveandbelowboth.
5.3.4 Action and Perception as Divergence Minimization
Arecentframework,inspiredbyactiveinferenceandadvancesindeepreinforcement
learning, which aims to unify perception and action under a single framework is
ActionandPerception asDivergenceMinimization(Hafneret al.,2020)(APDM).This
frameworkproposesthatbothactionandperceptioncanbemodelledasanagenttrying
tomininimizeadivergencefunctionalbetweentwodistributionsan‘actual’distribution
A(x,o),andatargetdistributionT(x,o).
L =D [A(x,o)||T(x,o)]
APDM KL
=E D [A(o|x)||T(o)]−E [lnT(x|o)−lnA(x)] (5.28)
A(x) KL A(x,o)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
RealizingLatentPreferences InformationBound
−E [lnT(x|o)−lnA(x)]=−E [lnT(x|o)−lnA(x)+lnA(x|o)−lnA(x|o)]
A(x,o) A(x,o)
(cid:124) (cid:123)(cid:122) (cid:125)
InformationBound
=−E D [A(x|o)||A(x)]+E D [A(x|o)||T(x|o)]
A(o) KL A(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
InformationGain PosteriorDivergence
(5.29)
By expressing this bound explicitly, we can see how it is an upper bound on the
informationgain,sincethe posterior divergenceisalways positive. Thetightnessofthe
boundthendependsonhowcloselytheactualandtargetdistributionsmatch. Ingeneral,
we can use this approach to write out a full expression for the divergence objective
Chapter5. TheMathematicalOriginsofExploration 246
betweentwojointdistributionsoverbothobservationsandlatentvariables.
L =argminD [p(o,x)||p˜(o,x)]
joint KL
a
=E D [p(o|x)||p˜(o)]−E [lnp˜(x|o)−lnp(x)]
p(x) KL p(o,x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LikelihodDivergence InformationBound
=E D [p(o|x)||p˜(o)]−E D [p(x|o)||p(x)]+E D [p(x|o)||p˜(x|o)]
p(x) KL p(o) KL p(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LikelihodDivergence InformationGain PosteriorDivergence
(5.30)
Ineffect,weseethatminimizingthedivergencebetweentwojointdistributionsrequires
theminimizationsofboththelikelihooddivergenceand theposteriordivergence,while
alsorequiringthemaximizationoftheinformationbetweenposteriorandpriorofthe
firstterminthejointKL.
It is also straightforward to relate this joint divergence to the divergence objective,
whichisthedivergencebetweenmarginalsinsteadofjoints.
D [p(o,x)||p˜(o,x)]=D [p(o)p(x|o)||p˜(x|o)p˜(x|o)]
KL KL
(cid:124) (cid:123)(cid:122) (cid:125)
JointDivergence
=D [p(o)||p˜(o)]+E D [p(x|o)||p˜(x|o)]
KL p(o) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
DivergenceObjective PosteriorDivergence
≥D [p(o)||p˜(o)] (5.31)
KL
(cid:124) (cid:123)(cid:122) (cid:125)
DivergenceObjective
Since the posterior divergence is always positive (as a KL divergence), we observe that
the joint divergence is simply an upper bound on the divergence objective. Since the
divergence is minimized, this bound is in the correct direction, and thus minimizing
the joint divergence is a reasonable proxy for minimizing the marginal divergence
objective. By minimizing the joint, it implicitly encourages agents to minimize both
the marginal divergence as well as the divergence between the predicted and desired
posteriordistributions.
Whilethegeneric APDM divergence,asjustadivergence ofjoints,isstraightforwardly
anupperboundonthedivergenceobjective,weshowthatunderthecommondefinitions
Chapter5. TheMathematicalOriginsofExploration 247
oftheactualandtargetdistributions,theAPDMdivergencecanalsobeunderstoodasa
lowerboundontheevidenceobjective,thusprovidingabridgebetweenthetwoobjec-
tives. Although the actual and target distributions can be defined differently depending
on the objective you desire to reproduce, one canonical form of the actual and target
distributions,whichcanreproducecontrolasinferenceaswellasvariationalperceptual
inferenceisasfollows. Wedefinetheactualdistributiontobethecombinationofthe
‘real’ data distribution p(o) and also a variational belief distribution q(x|o) such that
A(o,x)=q(x|o)p(o). Similarly, we define the target distribution to be the product of
the agent’s veridical generative model p(o,x) and the exogenous desire distribution
p˜(o) such that T(o,x) = p(o,x)p˜(o). This target distribution is valid as long as the
ultimateobjectiveisoptimizedviagradientsofthedivergence,whichdoesnotrequire
thatthe targetdistributionbe normalized. Underthis definitionofthe actualand target
distributions,theAPDMobjectivebecomes,
L =D [q(x|o)p(o)||p(o,x)p˜(o)] (5.32)
APDM KL
In the case of known observations in the past, we assume that the data distribution
becomespointsaround theactuallyobservedobservations p(o)=δ(o=oˆ)whilethe
desiredistribution becomesuniform –as thereis littleuse forcontrol inhaving desires
about the unalterable past. Under these assumptions, the APDM objective simply
becomestheELBOorthenegativefreeenergy, thusreplicatingperceptualinference.
However, on inputs in the future, the data distribution becomes a function of action
(since actions can change future observations) and the desire distribution becomes
relevant, thusallowing theminimizationoftheAPDM functionaltounderwritecontrol.
To gain a better intuition for the interplay of perception and control in the APDM
Chapter5. TheMathematicalOriginsofExploration 248
functional,weshowcasethefollowingdecomposition,
L =D [q(x|o)p(o)||p(o,x)p˜(o)]
APDM KL
=E D [q(x|o)||p(o,x)]+D [p(o)||p˜(o)] (5.33)
p(o) KL KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ELBO DivergenceObjective
whichdemonstratesthattheAPDMobjectiveeffectivelyunifiesactionandperception
by summing together a perceptual objective (VFE) with the divergence objective for
control. This confirms the previous finding that the APDM objective forms an upper
bound on the divergence objective since the ELBO, as a KL divergence, is bounded
belowby0. WealsoobservethatthisformoftheAPMDobjectiveisalsoapproximately
a lower bound on the expected evidence objective, thus providing a link between the
twoobjectives.
(cid:90)
E [lnp˜(o)]=E [ln dxp˜(o,x)]
p(o|a) p(o|a)
(cid:90) p˜(o,x)q(x|o)p(o,x)
=E [ln dx ]
p(o|a)
q(x|o)p(o,x)
p˜(o,x)p(o,x)
≥E [ln ]
p(o)q(x|o)
q(x|o)p(o,x)
p˜(o)p˜(x|o)p(o,x)
≥E [ln ]
p(o)q(x|o)
q(x|o)p(o)p(x|o)
≥−E [D [q(x|o)p(o)||p(o,x)p˜(o)]+E [lnp˜(x|o)−lnp(x|o)]
p(o)q(x|o) KL p(o)q(x|o)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
APDMObjective PosteriorDivergenceBound
≈≥E [D [q(x|o)p(o)||p(o,x)p˜(o)] (5.34)
p(o)q(x|o) KL
(cid:124) (cid:123)(cid:122) (cid:125)
APDMObjective
Which is approximately equal to the APDM objective under the condition that the
posteriordivergence boundbetweendesireposterior p˜(x|o)andtrueposterior p(x|o)is
small.
Chapter5. TheMathematicalOriginsofExploration 249
5.4 Towards a General Theory of Mean-Field Variational
Objectives for Control
Nowthatweunderstandthedivisionofobjectivesforcontrolintotwoclassesofevi-
denceanddivergencefunctionals,wecanstarttotrytounderstandthefullpossibilities
ofthespace ofpotentialobjectives. Here,inthis finalsectionofChapter5, wetryto
presentpreciselysuchataxonomy. Wefocusspecificallyon‘mean-field’variational
objectivefunctionals,meaningthatwecansplittheobjectiveintoanumberofindepen-
dentobjectivesforeachtime-stepofatrajectorywhichcanbeminimizedindependently.
Crucially, sucha mean-field assumption is alsomade in thetraditional reinforcement
learningparadigm,whereitisanecessarypreconditionfortheBellmanequation,and
alsoisstandardinthecontrolasinferenceframeworkaswell.
Whilethedivisionintodivergenceandevidenceobjectivesisclearlyimportant,itcannot
be the full story. Recall that one of the key differences between control as inference
andactiveinferencediscussedearlier,wasnotjusttheobjectiveoftheEFEvstheFEF,
butalso thewayvalue wasencodedinto theinferenceprocedure. Controlasinference
encodedvaluevia anadditional setof ‘optimalityvariables’which wereaugmentations
to the graphical model, and did not interact with any previously existing variables.
Active inference,by contrast,encoded value directlythrougha biasedgenerative model
of the observations 3. We call the method used by control as inference, which does
not affect any currently existing variable an exogenous encoding of value, while the
methodusedbyactiveinference,sinceitencodesvaluedirectlyintothemodelitself,we
callanendogenousencoding. ExogenousandEndogenousencodingsofvalueprovide
an orthogonal dimension of objective variablility on top of the evidence-divergence
dichotomy, sinceclearly onecan have anevidence,or adivergenceobjectivewith both
an endogenous or an exogenous value encoding. Finally, the actual specifics of the
generative model used clearly affects the variational objective irrespective of whether
3Activeinferencecanalsobeformulatedwithbiasedstates,see(DaCosta,Parr,etal.,2020)
Chapter5. TheMathematicalOriginsofExploration 250
it is an evidence or divergence objective, or uses an endogenous or exogenous value
encoding. For instance, whether weconsider latent states, or various different types of
modelparametersinthegenerativemodelleadstoadifferentobjectivefunctional.
We thus see that we can break down the landscape of potential mean field objective
functionalsforcontrolintothreeorthogonaldimensions.
• Whetheranevidenceordivergencefunctionalisused.
• Whethervalueisencodedexogenously,orendogenously.
• Thegenerativemodelunderlyingtheobjectivefunctional.
Under different values for each of these dimensions, the objective functional that is
specifiedchangesinastraightforwardandprincipledmanner. Thus,ourschemeallows
thedirectderivationofanygivenfunctional,andanunderstandingofitsdecompositions,
andhencethebehaviouritinduces,foranychoiceofthesevariables. Whilewehave
covered the effect of choosing an evidence or a divergence functional previously, we
havenot yetbeen explicitabout theeffectof theother twodimensions. Inthissection,
weexploretheeffectsoftheseadditionaldimensionsofdesignchoiceinmoredetail.
5.4.1 Encoding Value
However,theneedtoencodegoalsordesiresintotheinferenceprocedureimmediately
introducesdesignchoicesofhowexactlythisistobedone. Wearguethatthesedesign
choicescanfirstbesplitalongtwoorthogonalaxes–firstly,whethergoalsareencoded
exogenously asanadditional inputto theinferenceprocess, orendogenously through
fundamentally biasing one or more aspects of the inference model. The second axis
of variation is whether goals and desires enter the inference procedure through the
generativemodelorthevariationaldistribution. Makingdifferentdesignchoiceshere
producesavarietyofdifferentvariationalalgorithmsforcontrol.
If goals are encoded through the generative model, then whether the goals are encoded
Chapter5. TheMathematicalOriginsofExploration 251
exogenously or endogenously is the primary distinction between the formalisms of
control-as-inferenceandactiveinference. Ontheother hand,encodingsgoalsthrough
thevariationalmodelinsteadleadstonovelalgorithmswhichgenerallyhavenotbeen
much explored in the literature. Encoding goals exogenously through the variational
distribution leads to a variational bound similar to control-as-inference but with an
extrinsicvaluetermwithareversed-KL-divergencewhichthusexhibitsmode-seeking
ratherthanmean-seekingbehaviour,whichisrelatedtopseudolikelihoodmethods(Pe-
ters& Schaal,2007) invariationalreinforcementlearning. Encoding endogenous goals
through the variational distribution leads to a novel class of reverse-active-inference
algorithms which minimize a variational divergence between a biased approximate
posteriorandaveridicalgenerativemodel.
5.4.1.1 ExogenousValue: Maximum-EntropyRL
Toencodegoalsexogenouslyintothegenerativemodel,wemustaugmentthePOMDP
graphical model with additional optimality variables Ω . The idea here is that the
t:T
optimalityvariablesarebinarybernoullivariableswhichmarkwhetheratrajectoryis
optimal from the current state where the probability of optimality is often set to the
exponentiatedreward p(Ω =1)∝exp(r )sothatlnp(Ω =1)=r . Adaptiveactions
t t t t
are then inferred by first assuming that the agent has acted optimally into the future,
andtheninferringtheactionsthatwouldbeconsistentwiththatbelief–i.e. wewishto
infer the posterior p(a |x ,Ω =1). This posterior can then be approximated by
t:T t:T t:T
minimizingtheaugmentedvariationalbound:
(cid:16) (cid:17) (cid:16) (cid:17)
F =D q(x ,a |o )(cid:107)p(o ,x ,a ,Ω ) ≥D q(x ,a |o )(cid:107)p(o ,x ,a |Ω )
Ω KL t t t t t t t KL t t t t t t t:T
(5.35)
Bysplittingapartthisboundintoitsconstituentparts,wecaninvestigatetheexpected
Chapter5. TheMathematicalOriginsofExploration 252
behaviourofagentswhichactsoastominimizethebound.
(cid:16) (cid:17)
F =D q(x ,a |o )(cid:107)p(o ,x ,a ,Ω )
Ω KL t t t t t t t
(cid:16) (cid:17)
=D q(a |x )q(x |o )(cid:107)p(Ω |x ,a )p(o |x )p(a |x )p(x |x ,a )
KL t t t t t t t t t t t t t−1 t−1
(cid:16) (cid:17)
=−E (cid:2) lnp(Ω |x ,a ) (cid:3) −E (cid:2) lnp(o |x ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3)
q(xt,at|ot) t t t q(xt,at|ot) t t q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ObservationAmbiguity ActionDivergence
(cid:16) (cid:17)
+D q(x |o )(cid:107)p(x |x ,a ) (5.36)
KL t t t t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125)
StateDivergence
Theboundthussplitsintofourseparateandidentifiableterms–extrinsicvalue,obser-
vationambiguity,actiondivergence,andstatedivergence. Thefirstextrinsicvalueterm
correspondstotheexpectedexternalrewardsgivenbytheenvironment. Thisisdueto
thedefinitionofoptimalitythatlnp(Ω |x ,a )=:r(x ,a )sothattheextrinsicvalueis
t t t t t
simply the expected reward of a given state-action pair. By minimizing the negative
expected reward, we wish to maximize the expected reward on a given time-step. This
is identical to the standard reinforcement learning objective of reward maximization
sothatifF onlycontainedtheextrinsicvalueterm,itwouldbeexactlyequivalentto
Ω
reinforcementlearningexceptthattheexpectationistakenwithrespecttotheagent’s
beliefsoverstatesandactionsratherthanthetrueenvironmentaltransitiondynamics.
The observation ambiguity term E (cid:2) lnp(o |x ) (cid:3) grants a bonus to agents for
q(xt,at|ot) t t
reaching areas of state-space with a high expected likelihood, that is areas where
the state-observation mapping is highly precise. In effect, if the agent must learn a
likelihood mapping, this discourages exploration by granting bonuses for staying in
regions already well characterised. This term only arises in the POMDP setting due
to the addition of a likelihood term in the generative model. The third term is the
actiondivergence which istobeminimized,andpenalizestheagentforthedivergence
betweenits variationalpolicyq(a |x )andits priorpolicy p(a |x ). Iftheprior policy
t t t t
is assumed to be uniform such that p(a |x )=: 1 where |A| is the cardinality of the
t t |A|
action space in discrete action-spaces, and p(a |x )=:Unif(a ,a ) in continuous
t t min max
Chapter5. TheMathematicalOriginsofExploration 253
actionspaceswithaminimumandmaximumactionvalue,thenthisactiondivergence
termreducestothenegativeexpectedactionentropy−E (cid:2) H[q(a |x )] (cid:3) . Thefinal
q(xt|ot) t t
termisthestatedivergence,sothattheagenttriestominimizethedivergencebetween
itsvariationalposterioroverthestateandthepriorstateexpectedunderthegenerative
model. If the transition model is learnt, this leads it to prioritising transitions with
knowndynamics,againcausingtheagenttoprimarilyconfineitselftoregionsofthe
state-spaceithasalreadymodelledwell. IntheMDPsetting,withoutobservations,this
termvanishessincethevariationalposteriorq(x |o )becomesthevariationalpriorq(x )
t t t
whichisoftenassumedequaltothegenerativeprior p(x |x ,a ). Thusinthecase
t t−1 t−1
ofanMDPwithauniformactionpriorweobtainthemaximum-entropyRLobjective:
F =−E (cid:2) lnp(Ω |x ,a ) (cid:3) −E (cid:2) H[q(a |x )] (cid:3) (5.37)
maxent q(xt,at) t t t q(xt) t t
Whichinducesagentsbothtomaximizeexpectedrewardswhilealsomaximizingthe
policy entropy. Intuitively this means that the agent should try to maximize rewards
whileactingasrandomlyaspossible(maximizingentropy). Thispolicyentropyterm
thus provides a crude bonus for random exploration and often helps prevents the
commonly-observedphenomenonof‘policycollapse’(Fujimoto etal.,2018)whereby
reinforcementlearningagentswilloftenrapidlylearntoputallprobabilitymassontoa
singleaction, thuspreventingother actionsfrombeingtaken, hinderingexplorationand
long-termperformance. Interestingly,whenextendedtothePOMDPcasewithlearnt
transition and likelihood models, this formalism gives rise to additional observation-
ambiguityandstate-divergencetermswhichservetofurtherdisincentiviseexploration
by penalising moving too far from the prior predicted state and giving bonuses for
highlypredictablelikelihoods. Intuitivelywecanthinkoftheseextratermsastryingto
doawaywiththeadditionaluncertaintyinducedbythePOMDPsetting,sotheagent
confinesitselftotheregionwhichisasclosetoanMDPaspossible.
Chapter5. TheMathematicalOriginsofExploration 254
5.4.1.2 EndogenousValue: ActiveInference
While maximum entropy reinforcement methods can be derived by encoding goals
exogenouslytothegenerativemodelthroughtheuseofadditional‘optimalityvariables’,
it is also possible to encode goals endogenously by directly biasing some aspect of
the generative model towards preferred outcomes. Intuitively, we can think of the
difference as follows. With exogenous optimality variables, we possess a veridical
generative model outputting the likely and unbiased trajectories for a given series of
actions. Wethen‘shift’thesetrajectoriestoconvergeonthegoalbyconditioningonthe
optimalityvariables,andtheninfertheactionsconsistentwiththeshiftedtrajectories.
By contrast, active inference endogenously encodes goals by biasing the model so
thatinsteadofaveridicalgenerativemodelwhichgeneratestrajectoriesthatarethen
shifted, instead we have a biased generative model which directly outputs a biased
trajectoryofobservationso˜ convergingonthegoal,whichcanthenbeusedtoinfer
t:T
theactionsconsistentwiththisbiasedtrajectory. Inthismanner,insteadofproposing
additionaloptimalityvariables,wedirectlypositabiasedgenerativemodel p˜(o ,x ,a )
t t t
andoptimisethebiasedvariationalbound:
(cid:16) (cid:17)
F =D q(x ,a |o )(cid:107)p˜(o ,x ,a )
likelihood−AIF KL t t t t t t
(cid:16) (cid:17)
=D q(a |x )q(x |o )(cid:107)p˜(o |x )p(x |x ,a )p(a |x )
KL t t t t t t t t−1 t−1 t t
(cid:16) (cid:17)
=−E (cid:2) lnp˜(o |x ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3)
q(xt|ot) t t q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ActionDivergence
(cid:16) (cid:17)
+D q(x |o )(cid:107)p(x |x ,a ) (5.38)
KL t t t t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125)
StateDivergence
This variational bound decomposes into three terms as can be seen above. The first,
extrinsic value, is such because it is the biased probability of observations expected
under the variational distribution. If we assume that the biased generative model is
influenced by the external rewards in the environment, to allow for consistency with
reinforcementlearningandthemaximum-entropyRLframeworksuchthatlnp˜(o |x )∝
t t
Chapter5. TheMathematicalOriginsofExploration 255
exp(r(x ) then this first term reduces to minimizing the expected sum of negative
t
rewards,ormaximisingexpectedrewards. Theothertwoaction-divergenceandstate-
divergence terms are equivalent to the terms in the maximum entropy bound above,
exceptthattheactiveinferenceboundislackingthe‘observationambiguity’term. This
isbecauseexogenouslyencodinggoalsaddsanadditionalsetofoptimalityvariables
tothevariationalbound,thusmaintainingadistinctionbetweenveridicalobservations
andbiasedoptimalityvariables,whileendogenouslyencodinggoalsneedsto‘hijack’at
leastonedegreeoffreedomintheboundinordertoencodethegoalsdirectly. Herethe
observationambiguitytermhaseffectivelybeenhijackedbybeingbiasedwithreward
toinsteadencodetheextrinsicvalue.
Interestingly,thechoiceofencodinggoalsendogenouslyalsogivesanadditionalsetof
choicesofhowtobiasthegenerativemodel. Thekeychoiceisbetweenhavingabiased
likelihood, as done above, or a biased marginal and posterior. This decomposition is
shownbelow:
(cid:16) (cid:17)
F =D q(x ,a |o )(cid:107)p˜(o ,x ,a )
marginal−AIF KL t t t t t t
(cid:16) (cid:17)
=D q(a |x )q(x |o )(cid:107)p˜(o )p˜(x |o )p(a |x )
KL t t t t t t t t t
(cid:16) (cid:17) (cid:16) (cid:17)
=−E (cid:2) lnp˜(o ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3) +D q(x |o )(cid:107)p˜(x |o )
q(xt|ot) t q(xt|ot) KL t t t t KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ActionDivergence BiasedStateDivergence
(5.39)
Here,weassumethatlnp˜(o )∝exp(r(o ))suchthattheextrinsicvaluetermisagain
t t
directlyequaltotherewards. Aninterestingdifferenceisthatthebiasedstate-divergence
isnowbetweenthevariationalposteriorandthebiasedstateposterior,whichrepresents
the posterior over states that would be observed given the biased observations. This
gives this second term also the flavour of an extrinsic value as the goal is not only to
maximizerewards but matchthe veridicial variational distribution tothe biasedstate
distributioninducedbythedesiredobservations
Alternatively, it is also possible to consider encoding the bias into the states p˜(x )
t
Chapter5. TheMathematicalOriginsofExploration 256
insteadoftheobservations. Inthiscase,weobtainthefollowingobjectivefunctional:
(cid:16) (cid:17)
F =D q(x ,a |o )(cid:107)p˜(o ,x ,a )
state−AIF KL t t t t t t
(cid:16) (cid:17)
=D q(a |x )q(x |o )(cid:107)p(o |x )p˜(x )p(a |x )
KL t t t t t t t t t
(cid:16) (cid:17) (cid:16) (cid:17)
=D q(x |o )(cid:107)p˜(x ) −E (cid:2) lnp(o |x ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3)
KL t t t q(xt,at|ot) t t q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ObservationAmbiguity ActionDivergence
(5.40)
In this case, we have regained the observation-ambiguity term and instead the state-
divergencetermhasbeen‘hijacked’bytherewardstobecometheextrinsicvalueterm,
whichhasbecomethedivergencebetweenpredictedanddesiredstates.
Giventhatthereexistthesetwodesignchoicesofwhethertoencodegoalsexogenously
orendogenouslyandwhichleadtosubtlydifferentobjectives,anaturalquestiontoask
iswhataretherelativeadvantagesanddisadvantagesofthetwomethods? Whattrade-
offsexistandwheremighteachbeuseful? Ingeneral,theprimarypracticaldifference
betweenthemethodsisthatbyendogenouslyencodingrewards,theagentlosesadegree
offreedom, whichmanifestsitself astheloss ofanobservation ambiguitytermif the
goalsareencodedthroughbiasedobservations,orthelossofthestate-divergenceterm
if the goals areencoded asa desiredstate distribution. Theseadditional termstend to
discourageexplorationbycausingtheagenttoremaininareaswithknownmappings,
sothatbydisablingthemendogenousgoalstendtoencourageexploration. Ontheother
hand,bykeepingtowell-knownregionswherethePOMDPbehaveslikeanMDP,the
exogenousgoalsmethodenforcesagreaterconservativebiastowardssafety,whichmay
beespeciallyusefulinsettingswhereexplorationiscostly,satisficingpoliciesareeasy
to find, orin policies learntin an offline orimitation-learning setting whereventuring
outsideofthetrainingdistributioncanhavedeleteriouseffectsonthepolicy.
There are also important philosophical and representational differences between the
two. Onarepresentationalnote,althoughinthederivationsaboveboththeoptimality
variableandthebiasedgenerativemodelhavebeendefinedintermsofexponentiated
Chapter5. TheMathematicalOriginsofExploration 257
reward, this is not necessarily the case. Since in the endogenous encoding case, the
goalsareencodeddirectlyaspriordistributionsintothegenerativemodel,itispossible
to model complex and potentially nonstationary goals distributions in this manner.
However, due to the optimality variables being binary, and conditioned upon, this may
constrain their representational power compared to the endogenous method, although
inpracticethisdifferencemaybenegligibleasalthoughthevariablesthemselvesare
binary,theprobabilityofoptimalitybeingone,canbedefinedasanarbitraryfunctionof
thestates, actionsorobservations. Thusin practice,theremay belittlerepresentational
difference between the two methods, except that the endogenous case has a slightly
simplerintuitivejustificationasdirectlyspecifying adesireddistributionoverstatesor
observations.
The difference between exogenous and endogenous encodings of value also has sig-
nificant philosophical import. Exogenous encodings, by adding desires on top of an
unbiasedgenerativemodelmaintainacleandistinctionbetweenveridicalperception
andactionselection,andgoals. Thismaintainsthecoremodularitythesisofmuchwork
in artificial intelligence that perception, and action selection should be kept separate
suchthatfirstaveridicalworld-modelisconstructedwhichtriestoaccuratelymodel
the world, then given a set of arbitrary goals, a general-purpose planner or policy
can be utilised or learnt to enable the agent to achieve these goals. This approach
corresponds to the classical perceive-(value)-plan-act cycle in cognitive science and
maintainsseparatemodulesforagoal-agnosticperceptualsystem,agoal-agnosticplan-
neroraction-selectionmechanism,andthenasetofgoalswhicharenotintrinsictothe
agentbut whichareconstructedor handed-downfromonhigh. Endogenousencoding
methods, by contrast, tend to blur the boundaries between these sytems, since goals
areencodedandadaptiveactionsareselectedthroughaprocessofbiased perception
andinferencewherebyanagentdoesnotfirstinferatruetrajectory,comparesittoits
goals,andthentriestomatchthetoo,insteaditsimplyseesabiasedtrajectoryleading
to its goals and then acts consonantly with what it sees. This view has close links to
Chapter5. TheMathematicalOriginsofExploration 258
embodied and enactivist views in philosophy and cognitive science which stress that
ratherthandistinctmodularsystemsofperception,valuation,andactionthereisinstead
a single combined system or sensorimotor loop which directly acts on sensorimotor
contingenciesinanadaptivefashion(Baltieri&Buckley,2018).
5.4.1.3 EncodingValueintotheVariationalDistribution
Previously, we have encoded values either exogenously or endogenously into the
generativemodeloftheagent. Inthecaseofendogenousencodings,thismeansthatthe
agent makes biased predictions, rather than forming biased inferences. However, it is
also possible to consider and investigate what happens if instead values were encoded
intothevariationaldistributionsothattheagent’sinferenceprocedureratherthanmodel
isbiased. Wefirstconsiderthecaseofexogenouslyencodedgoals. Thisrequiresthat
thevariational distributionisaugmented withbinaryoptimality variables, justlike the
generative model was previously giving q(a ,x ,Ω |o ). From this we can write the
t t t t
relevantfreeenergyfunctional:
(cid:16) (cid:17)
F =D q(a ,x ,Ω |o )(cid:107)p(o ,x ,a )
qΩ KL t t t t t t t
(cid:16) (cid:17)
=D q(Ω |x ,a )q(a |x )q(x |o )(cid:107)p(o |x )p(a |x )p(x |x ,a )
KL t t t t t t t t t t t t t−1 t−1
(cid:16) (cid:17)
=E (cid:2) lnq(Ω |x ,a ) (cid:3) −E (cid:2) lnp(o |x ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3)
q(xt,at|ot) t t t q(xt,at|ot) t t q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ObservationAmbiguity ActionDivergence
(cid:16) (cid:17)
+D q(x |o )(cid:107)p(x |x ,a ) (5.41)
KL t t t t−1 t−1
(cid:124) (cid:123)(cid:122) (cid:125)
StateDivergence
Weseethattheresultingfunctional,undertheassumptionthatlnq(Ω|x ,a )∝exp(r(x ,a ),
t t t t
isexactlyequivalenttothefunctional obtained fortheexogenouslyencodedgenerative
model,uptoasigndifferenceintheextrinsictermwhichcanbefinessedwithoutloss
ofgeneralitybyinvertingthesignoftherewardfunction. Wethusseethat,ifvaluesare
exogenouslyencoded,itdoesnotmatterwhichdistributiontheyareprimarilyencoded
through. Intuitively this is because since the veridical distributions are maintained
Chapter5. TheMathematicalOriginsofExploration 259
through endogenous coding, they are unaffected by the encoding of value into them,
thuswhichoneto choosehasnoeffectoveralluptoa trivialsign differencewhichcan
beeasilyfinessedthroughnegativelyencodingreward.
When rewards are endogenously encoded, however, the resulting functionals are not
equivalent. Wefirstshowthis withabiased-statefunctional whichshouldbecompared
to Equation 5.41, where we directly bias the state-inference part of the variational
functionalsoastopreferentiallyinferbeingindesired states fromagivenobservation
q˜(a ,x |o )=:q(a |x )q˜(x |o ). Throughthisdecompositionweobtainthefunctional:
t t t t t t t
(cid:16) (cid:17)
F =D q˜(x ,a |o )(cid:107)p(o ,x ,a )
state−q−AIF KL t t t t t t
(cid:16) (cid:17)
=D q(a |x )q˜(x |o )(cid:107)p(o |x )p(x |x ,a )p(a |x )
KL t t t t t t t t−1 t−1 t t
(cid:16) (cid:17)
=D q˜(x |o )(cid:107)p(x |x ,a ) −E (cid:2) lnp(o |x ) (cid:3)
KL t t t t−1 t−1 q(xt,at|ot) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ObservationAmbiguity
(cid:16) (cid:17)
+E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3) (5.42)
q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125)
ActionDivergence
whichis verysimilarto thecorrespondingstate extrinsicvaluefunctional inEquation
5.40, except that in the extrinsic value term the divergence is between the biased
variationalposteriorandaveridicalgenerativeprior,ratherthanaveridicalvariational
posterior and a biased generative prior. By having the biases occur on the left side
of the KL, we are essentially minimizing the reverse-KL compared to when value is
encoded into the generative model. This gives the resulting agents a mode-seeking
ratherthanamean-seekingbehaviour,sinceagentsoptimizingunderthereverseKLwill
suffera largepenaltyif thedesiresare inregionswith avery lowveridicalprobability.
Moreover, there is a further subtle difference in the POMDP case here since we are
contrastingthebiaseddistributionwithapriorratherthanaposterior. However,inthe
Chapter5. TheMathematicalOriginsofExploration 260
MDPcasethisdifferencevanishes,andweobtainthesimplifiedfunctional:
(cid:16) (cid:17)
F =D q˜(a ,x )(cid:107)p(x ,a )
qMDP KL t t t t
(cid:16) (cid:17)
=D q˜(a |x )q˜(x )(cid:107)p(a |x )p(x |x ,a )
KL t t t t t t t−1 t−1
(cid:16) (cid:17) (cid:16) (cid:17)
=D q˜(x )(cid:107)p(x |x ,a ) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3) (5.43)
KL t t t−1 t−1 q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ActionDivergence
Which is lacking the observation ambiguity term due to being an MDP, and also
the extrinsic value has become the divergence between desired variational states and
predictedgenerativestates. Thisfunctionalisexactlyequivalenttothealternateformula
with values encoded endogenously into the generative model except that it uses the
reverse KL divergence. Moreover these functionals are deeply related to KL control
with an additional action divergence term, and thus when value is instead encoded
intothevariationaldistributionwehavereverse-KLcontrolwhichuses thereverse KL
divergenceandis veryclosely related topseudo-likelihood methodsinreinforcement
learning(Abdolmalekietal.,2018;Peters&Schaal,2007).
Overallthenwehaveexploredtwoorthogonalaxesofvariationfortheproblemofhow
toencodeanotionofvalue,reward,ordesiresintoanotherwisevalue-agnosticvaria-
tional inferenceprocedure inorder to beable to infer adaptiveactions. We have shown
thatthefirstquestionofwhethervalueistobeencodedexogenouslyorendogenously
makes subtle but significant differences in the resulting functionals. Specifically, by
requiringtheutilisationandbiasingofoneofthevariablesinthemodeltoencodevalue,
endogenousencodingtendstoloseonedegreeoffreedoninitsfunctionalcomparedto
exogenousencoding. InthePOMDPsettingthisistypicallytheobservation-ambiguity
term if goals are encoded into observations, or the state-divergence term if goals are
encodedintostates. Beyondthis,weseethatthesedifferentmeansofencodingvalue
also has significant philosophical importance as to the nature of perception, action
andvalue. Exogenouslyencodinggoalssupportsamodulardescriptionofthesethree
functionsasindependentsystemswhichareeachagnosticwithrespecttotheoutputsof
Chapter5. TheMathematicalOriginsofExploration 261
theothers,whileendogenouslyencodinggoalsmergesthemalltogetherintoamixed
systemwhereactionandperceptionareintrinsicallybiasedbygoalstowardsadaptive
outcomes. Moreover, this dichotomy of exogenous or endogenous encoding is the
primary difference between variational control frameworks arising from reinforcement
learning and from active inference, and this fact speaks to the difference in underly-
ingcognitivephilosophybetweenthesetheorieswherereinforcementlearningdraws
heavily fromcognitivist andrepresentational traditionsinartificial intelligencewhich
prize principled, independent, and modular systems, while active inference comes
from a heavily embodied and enactive viewpoint influenced by dynamical systems
theorywherebysystemsareseenprimarilyintermsoftheirsituatednesswithinaaction-
perceptionsensorimotorloop,andthereisnotnecessarilyanycleandistinctionbetween
phasesorsubsystemsofthisloop.
Finally,wehaveseenthatthesecondaxisofvariationiswhetherthegoalsareencoded
intothegenerativemodelorthevariationaldistribution. Theeffectsofthisdifference
are subtler than for the exogenous vs endogenous encoding dichotomy. Variational and
generative encodingsareequivalentuptoa signdifferenceintheexogenous case since
theencodingdoesnotaffecttheveridicalityofthedistribution,whileintheendogenous
casethiscausessubtledifferencessuchthatthegenerativeandvariationalencodings
aretypicallyequivalentuptotheextrinsicvaluetermswhichbecomesthereverse-KL
forthevariationalencodingrelativetothegenerative. Thisleadstoacloseconnection
with pseudolikelihood methods in reinforcement learning (Abdolmaleki et al., 2018)
andinfactprovidesageneralisationofthesemethodstothePOMDPsetting.
5.4.2 General Graphical Models
Inallpreviouswork,wehaveprimarilyfocusedonhowthealgorithmsandfunctionals
differ when placed into a POMDP setting with visible observations and unobserved
(Markovian) hidden states, and actions which affect the hidden states. However this
Chapter5. TheMathematicalOriginsofExploration 262
is fundamentally a modelling choice. For instance, if state information is perfectly
observed, as is often assumed in RL, then the problem reduces to a simple MDP
formulation. TheMDPformulationwithexogenousrewardswasaddressedinEquation
5.41, however when encoding rewards endogenously, this must be encoded into the
generativeorvariationaldistributions,sothatwiththeFEEFobjectivefunctionalthe
extrinsictermbecomessimplyastatedivergence.
(cid:16) (cid:17)
FEEF =D q(x ,a )(cid:107)p˜(x ,a )
MDP KL t t t t
(cid:16) (cid:17) (cid:16) (cid:17)
=D q(x )(cid:107)p˜(x ) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3) (5.44)
KL t t q(xt) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ActionDivergence
Here no information gain is possible since states are directly observed, thus the only
exploration is through random entropy maximizing terms. Importantly, if the goals
wereinsteadencodedintothevariationaldistribution,theonlyeffectthiswouldhave
wouldbetofliptheextrinsicvalueKLintoareverse-KLandthusinducemode-seeking
rather than mean-seeking distribution matching behaviour. In the MDP setting, then,
weobtainamaximum-entropyKL-controlobjective.
While it is possible to restrict the generative models only to MDPs, we can also
considerextendingthemtoalsoexplicitlymodelthepriorandposteriordistributions
of parameters underlying the variational and generative distributions. For instance,
we have implicitly been utilising a variational posterior q(x |o ), a transition model
t t
p(x |x ,a ), a likelihood model p(o |x ), an action policy q(a |x ) and an action
t t−1 t−1 t t t t
prior p(a |x ). Allofthesedistributionsmayhaveparameters,orbeparameterisedbya
t t
flexiblefunctionapproximatorsuchasaneuralnetwork,whichitselfhasparameters.
Theseparameterscanbebeincludedinthevariationalinferenceprocedurebyadding
a generative model and variational distribution over the parameters. For instance,
supposing we have some set of parameters θ which parametrise the transition model
suchthatthetransitionmodelbecomes p(x |x ,a ;θ),thenwecanposeaninference
t t−1 t−1
problemandwritedownavariationalfreeenergyfunctionalwhichalsoincludesamodel
Chapter5. TheMathematicalOriginsofExploration 263
overparameters:
(cid:16) (cid:17)
F =D q(x ,a ,θ |o )(cid:107)p˜(o ,x ,a ,θ )
θ KL t t t t t t t t
(cid:16) (cid:17)
=D q(a |x )q(θ |x )q(x |o )(cid:107)p˜(o |x )p(a |x )p(x |x ,a ,θ )p(θ )
KL t t t t t t t t t t t t−1 t−1 t t
(cid:16) (cid:17)
=−E (cid:2) lnp˜(o |x ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3)
q(xt|ot) t t q(xt|ot) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ActionDivergence
(cid:16) (cid:17) (cid:16) (cid:17)
+E (cid:2) D q(x |o )(cid:107)p(x |x ,a ,θ) (cid:3) +D q(θ |x )(cid:107)p(θ ) (5.45)
q(θt) KL t t t t−1 t−1 KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
StateDivergence ParameterDivergence
We thus see that minimizing the variational free energy directly, requires minimizing
the divergence between posterior and prior beliefsover the parameters, thus implicitly
penalising updates which cause large parameter updates, and thus disincentivising
exploration. In general, by adding additional variables to the variational free energy,
we simply obtain additional divergence terms to be minimized as above, essentially
penalisingdeviationsbetweentheposteriorandpriorbeliefsforthatvariable. However,
and analogously with the states, when we utilize a FEEF objective functional, we
can obtain a parameter information gain term as well as a countervailing parameter
approximationerrortermasshownbelow:
(cid:16) (cid:17)
FEEF =D q(o ,x ,a ,θ )(cid:107)p˜(o ,x ,a ,θ )
θ KL t t t t t t t t
(cid:16)
=D q(o |x )q(a |x )q(x |θ)q(θ )q(x |o )q(θ |x )
KL t t t t t t t t t t
(cid:17)
(cid:107)p˜(o |x )p(a |x )p(x |x ,a ,θ )p(θ )q(x |o )q(θ |x )
t t t t t t−1 t−1 t t t t t t
(cid:16) (cid:17) (cid:16) (cid:17)
=E (cid:2) D q(o |x )(cid:107)p˜(o |x ) (cid:3) +E (cid:2) D q(a |x )(cid:107)p(a |x ) (cid:3)
q(at,xt) KL t t t t q(ot,xt) KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExtrinsicValue ActionDivergence
(cid:16) (cid:17)
−E (cid:2) D q(x |o )(cid:107)q(x ) (cid:3)
q(ot) KL t t t
(cid:124) (cid:123)(cid:122) (cid:125)
ExpectedInformationGain
(cid:16) (cid:17) (cid:16) (cid:17)
+E (cid:2) D q(x |o )(cid:107)p(x |x ,a ,θ ) (cid:3) −D q(θ |x )(cid:107)q(θ )
q(ot)q(θt) KL t t t t−1 t−1 t KL t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ExpectedPosteriorDivergence ParameterInformationGain
(cid:16) (cid:17)
+D q(θ |x )(cid:107)p(θ ) (5.46)
KL t t t
(cid:124) (cid:123)(cid:122) (cid:125)
ParameterDivergence
Chapter5. TheMathematicalOriginsofExploration 264
In the general case adding additional variables to the FEEF objective will create an
information gain term in that variableas well as the counteractingposterior divergence
term. Analogously,addingadditionalvariablestotheclassofdivergencefunctionalswill
also createinformation gain termsin those variables withoutthe posterior divergence.
The reason for this behaviour can ultimately be derived directly from the variational
marginalentropybyconsideringaugmentingitwithparametersθ:
VME =E (cid:2) lnq(o ) (cid:3) =E (cid:2) lnq(o ) (cid:3)
q(ot) t q(ot,xt,θt) t
=E (cid:2) ln
q(o
t
,x
t
,θ
t
)
(cid:3)
q(ot,xt,θt)
q(x ,θ |o )
t t t
=E (cid:2) ln
q(o
t
|x
t
)q(x
t
|θ
t
)q(θ
t
)
(cid:3)
q(ot,xt,θt)
q(x |o )q(θ |x )
t t t t
(cid:16) (cid:17) (cid:16) (cid:17)
=− H (cid:2) q(o |x ) (cid:3) −E (cid:2) D q(x |o )(cid:107)q(x |θ ) (cid:3) −D q(θ |x )(cid:107)q(θ )
t t q(ot)q(θt) KL t t t t KL t t t
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LikelihoodEntropy
ExpectedStateInformationGain ParameterInformationGain
(5.47)
Thus we have seen that by implicitly maximizing the marginal entropy, we in fact
are implicitly optimising the information gain for any latent variables in the model.
Similarly, as discussed previously, the primary difference between exogenous and
endogenousencoding ofvalue isthatendogenous encodingslack adegree offreedom
that exogenous encodings can make use of. In general therefore, as we augment the
graphicalmodelsthesefunctionalsarederivedfromwithadditionalvariables,wesee
that the exogenous encoding is roughly equivalent to the endogenous encoding with
anadditionaldegreeoffreedom. Thisthus derivestwo‘scaling laws’forourfamilies
of functionals as additional sets of variables are added to the functional – that the
exogenous encoding will approximate the endogenous encoding with an additional
degree of freedom, and that with VFE derived functionals we will obtain divergence
terms to be minimized between the posterior and prior for the variable, with a FEEF
functionalwewillobtainaninformationgaintermandaposteriorapproximationerror
term,whilewithadivergencebasedfunctionalwewillobtainaninformationgainterm
onlyinthenewvariable.
Chapter5. TheMathematicalOriginsofExploration 265
5.5 Discussion
In this chapter, we have answered and discussed two key questions. Firstly, we now
understandthemathematicaloriginofinformation-seekingexplorationtermsinvaria-
tionalobjectivesforcontrol. Whilethisappearsarcane,thisisactuallyaquestionwith
deep implications both philosophically, as well as for applications. On a philosophi-
calandmathematicalnote,wehaveuncoveredtheprincipledmathematicaloriginof
information-maximizingexploration. Weseethatitarisesfromminimizingdivergences
between a predicted and a desired distribution, and specifically from the predicted
entropy maximization half of the divergence objective. In effect, we see that it is by
tryingtomaximizetheentropyoffutureobservations,thatinducesinformationseeking
explorationinlatentspace,wheneveranylatentstatesorparametersareaddedtothe
generative model. This makes intuitive sense – if the future is broad and entropic, it
contains much information, and to maximize that breadth entails learning about the
world in order to ensure a precise match everywhere between predicted distribution
and a complex desire distribution. Conversely, we have seen that standard evidence
objectivesusedinvariationalinferenceorcontrolresultineffectivelytryingtomatchthe
predictedanddesireddistributionwhiletryingtominimizetheentropyoffuturestates
or, alternatively, to make the future maximially predictable as well as inconformity to
theagent’sdesires. Assuch,theagent’sgoalistoeffectivelyminimizetheamountof
informationitreceivesabouttheworld,learningonlyasmuchasissufficientforcontrol,
and thusnot givingrise toany kindof information-seekingexploratory behaviour. We
canthusunderstandpreciselywhystandardobjectivessuchascontrolasinferenceare
insufficient to obtain information seeking behavioural objectives. Similarly, our ap-
proach allows usto understandand rationalizea number ofapproaches inthe literature
(Klyubinetal.,2005;Oudeyer&Kaplan,2009;Sunetal.,2011)whichaddadditional
exploratory terms to their reward-maximizing agents, with the heuristic justification
that they should increase exploration. Indeed, many such approaches are implicitly
minimizing a divergence objective without realizing it. Our advances here make this
Chapter5. TheMathematicalOriginsofExploration 266
practiceexplicitandallowsonetounderstandtheprecisemathematicalnatureofthe
objectivebeingoptimized. Moreover,thisdistinctionalsoshedslightonthepossible
objective functions used by biological creatures such as humans in psychological or
behaviouraleconomicstasks,forinstance,onecanunderstandtheotherwise-puzzling
phenomenonofprobabilitymatching,astheinevitableoutcomeofoptimizingadiver-
gencefunctional,andthiselegantlyexplainsbothwhyitispresentandadditionally,why
itisbeneficial–sinceoptimizingthisobjectiveinmorecomplexenvironmentsnaturally
leadstoexploratoryinformation-seekingbehaviourwhichoftenwilloutperformpure
rewardmaximizationevenonitsownterms.
Thisapproachisalsousefulforapplications,sincewecanbeginbyderivingavariety
ofmethodsusingdivergenceobjectivesandunderstandingtheirexploratorybehaviour.
While little work has yet been done on explicitly divergence minimizing agents, we
believe that this will be an important area in the future, as successful protocols and
algorithmsforexplorationinreinforcementlearningwillbecomeincreasinglyimportant
asthesparsity,dimensionality,anddifficultyofRLbenchmarktasksincreases.
Secondly, we understand the relationship between control as inference and active
inference. Weknow thatthe keydifference issimply adifference in objective function
between the two (the EFE vs the FEF), and secondly a distinction between them is
their encodings of value – that active inference uses an endogenous, while control
as inference uses an exogenous encoding. Moreover, understanding the distinctions
betweenthetwotheories,aswellasthegeneralandbroaddistinctionbetweenevidence
anddivergenceobjectives, thenallows usto raiseour eyesand perceivea muchlarger
vista of the full landscape of potential objective functionals for control. In the latter
halfof thischapter,wehave seenthatthese functionals canvaryalong twoorthogonal
dimensions – whether an evidence or divergence functional is used, as well as the
natureoftheir encodingofvalue(endogenous orexogenous)aswell aswhethervalue
isencodedintothe variationalorgenerativedistribution. Moreover, wehavederiveda
Chapter5. TheMathematicalOriginsofExploration 267
goodunderstandingofthe impactofdifferentdefinitionsofthe generative model upon
theobjectivefunctionalsthatresult. Thisallowsusabroadanduniqueunderstandingof
thepossiblespaceofobjectivefunctionals,aswellasthedesignchoiceswhichinfluence
which one to choose. Future work in this area should investigate the actual impacts
of different choices on agent behaviour, both in simple toy environments where the
effectscanbeeasilyunderstood,aswellasinmorecomplexanddifficultbenchmark
taskswhereusingdifferentobjectivesmaywellgiverisetoalgorithmswhicharemore
effective than current agents. As an example, while only using random exploration,
control as inference inspired approaches such as the soft-actor-critic have given rise
tostateoftheartperformance. Weseeno reasonwhymoreexploratoryinformation-
seekingagents,poweredbydivergenceobjectives,shouldnotleadtosimilargainsin
performance, especially on challenging sparse-reward tasks. While we present some
preliminary resultsto this effectin Chapter 4, muchmore work remainsto be doneto
pindownwhatgains,ifany,arepossiblebythisapproach.
Finally, our approach directly gives us the objective functionals utilizing different
generative models. For instance, if you want to extend your reinforcement learning
algorithms to POMDPs, or POMDPs with hierarchical levels of latent states, or to
explicitlymodeldistributionsoverdifferentmodelparameters,ortoexplicitlymodela
rewarddistributionorrewardmodel,thenourframeworkprovidesarecipeforprecisely
andimmediatelyderivingthenecessaryobjectivetooptimize.
In the next chapter, we move on to consider applications of the free energy principle
to learning, where we focus on deriving novel algorithms which can perform credit
assignmentinneuralnetworksinabiologicallyplausiblefashion.
Chapter 6
Credit Assignment in the Brain
6.1 Introduction
In this chapter we shift gears again and now consider applications of the free energy
principletotheproblemsoflearninginthebrain. Specifically,hereweaimtounder-
stand the nature of credit assignment in the brain, and focus on how and whether the
backpropagationoferroralgorithmwhichunderpinsalltherecentsuccessesofmachine
learningin trainingdeep artificialneural networks,could potentiallybe implementedin
thebrain. Inthischapter,wepresentthefruitofourworkinvestigatingthisextremely
important question for the case of rate-coded integrate and fire neurons engaged in a
static task (such as object recognition), where there is only a feedforward pass to be
concerned with and all backpropagation is through space, and not time. While this
settingisconsiderablysimplifiedfromtheonethebrainfacesinreality,itisalsomuch
moretractableandwell-understoodandsolvingtheprobleminthisdomainmayprovide
vitalcluesintothefullsolution.
This chapter is split into four relatively independent sections. In the first section, we
provideageneralintroductionandminiliteraturereviewonbackpropagationandprevi-
ousattemptstoderivebiologicallyplausiblealgorithmstoimplementbackpropagation
268
Chapter6. CreditAssignmentintheBrain 269
inthebrain. Inthenexttwosections,wethenpresentourworkderivingnewalgorithms
forbiologicallyplausibleapproximationstothebackpropagationoferroralgorithm.
In the first section, we show that predictive coding – the free energy process theory
fromchapter3–can,if set-upcorrectly,exactlyapproximatethebackpropagationof
error algorithm along arbitrary computation graphs. This result is fascinating since
predictive coding has a long history and well-developed literature on its properties,
performance, and especially its biological plausibility, as well as possessing several
well-developed theoretical neural implementations (Bastos et al., 2012; Kanai et al.,
2015; Keller & Mrsic-Flogel, 2018). We empirically validate this approximation to
backprop and showcase that predictive coding can perform equally to backprop at
trainingcomplexmachinelearningarchitecturessuchasCNNsandLSTMs.
Secondly, we develop a novel algorithm – Activation Relaxation (AR) – which also
canasymptoticallyconvergetotherequiredbackpropagationerrorgradientsusingonly
localconnectivity–and whichdoesnot requiretwo separate populationof valueand
error neurons. We empirically show that this algorithm can train complex machine
learning architectures with performance equal to backprop and, additionally, demon-
strate that the same relaxations shown in Chapter 3 for predictive coding – such as
using learnable backwards weights to overcome the weight transport problem, and
dropping the nonlinear derivatives also work for the AR algorithm, thus importantly
both substantiallyimproving the overall biological plausibility of the AR algorithmas
wellasdemonstratingthegeneralizabilityoftheresultsinChapter3tootheralgorithms
1.
Finally, in the third section, we have included a more speculative discussion on a
potentialfurtheralgorithmforsolvingbackpropinrate-codedneuronsdirectly,instead
ofinaniterativefashion. Wediscussthepossiblelimitationsofthisalgorithmaswell
as the required neural circuitry and, for the first time, begin to precisely understand
1Forthisthesis,theARalgorithmisnotdirectlyrelatedtotheFEP,althoughitwasinitiallyinspired
byresearchintopredictivecodingwhichisaprocesstheoryoftheFEP
Chapter6. CreditAssignmentintheBrain 270
whatthekeyproblemsarefortherate-codedsenseandalsowhatarealsolutionwould
looklike.
6.1.1 Backpropagation in the Brain
Duetotheimmensesuccessofmachinelearningapproachesbaseduponconnection-
ist deep neural networks trained upon the backpropagation of error algorithm, our
paradigmsofhowthebrainfunctionsisalsoshifting. Specifically,theparadigmthatthe
brain,oratleasttheneocortex,isfundamentallyablank-slatelearningmachinewhich
usesgeneralpurposelearningalgorithmstohandleinputs,akintoadeepneuralnetwork
is becoming increasingly influential in neuroscience, partially displacing older views
that the brain consists of a series of separate ‘modules’ (Fodor, 1983; Pinker, 2003),
eachofwhichperformsasinglespecializedfunctionusingwhatareeffectivelyspecific
and pre-set algorithms, hard-coded over the course of evolutionary history. While
functionalspecialisationisanextremelynotablecharacteristicofthebrain,itis more
widelybelievedthatthisspecialisation,especiallyinthecortex,isduetodifferencesin
inputandsmallinductivebiasesshapingthenatureandoutputofaverygenerallearning
algorithm which is implemented throughout the cortex, rather than each functional
module possessing its own independent and isolated suite of algorithms. This view
is supported by evidence of a remarkable uniformity of cortical cytoarchitecture and
neuroanatomy,beliedbytheheterogeneityobservedinsubcorticalareas(Bear,Connors,
&Paradiso,2020).
While,foralongtime,itwasempiricallyunclearwhetherthefundamentalsofintelli-
gence–suchasrobustandgeneralizableperception,naturallanguagecapabilities,and
adaptiveactionplanning–couldemergesolelyfromlearningalgorithmswithrelatively
fewinductivebiases,appliedtovastamountsofdata,thepastdecadeanditsimmense
advancesinmachinelearningaresuggestivethatthismaybepossibleafterall. Modern
machine learning, effectively, represents the culminationand empirical verification of
Chapter6. CreditAssignmentintheBrain 271
earlierconnectionisttheory(Rumelhart,Hinton,&Williams,1986).
This view places the central focus on learning. Since the backpropagation of error
algorithmhasprovensoimmenselysuccessfulinmachinelearning,totrainanextremely
widevarietyofarchitecturestoperformanimpressivearrayoftasks(Goodfellowetal.,
2014;Krizhevskyetal.,2012;Radfordetal.,2019;Schmidhuber,1999;Schrittwieser
etal., 2019),and giventhat thebrain itself facesan almostidenticalcredit assignment
probleminhavingtoadjustsynapticstrengthstoallowforlearningtooccur,itisavery
interestingandimportantquestiontoaskwhetherthelearningalgorithmimplementedin
thebrain couldsimply bebackprop. Ifthis questionwere conclusively answeredin the
affirmative, it would represent an enormousconceptual breakthrough in neuroscience
sinceitwouldprovide,forthefirsttime,ageneralandpowerfulorganizingprinciple
forthebrain(oratleastthecortex)asawhole,itwouldallowtheimportationdirectly
intoneuroscienceof alargequantityofresults frommachinelearning. Such ananswer
would additionally have deep philosophical implications. It would imply that there
is very little effective difference between current machine learning methods and the
kinds of learning, inference, andplanning algorithms that areimplemented in the brain
to giverise to undeniablyintelligent andapparently consciousbehaviour, andas such
would imply that the current paradigm in machine learning suffices, with more scale
andpotentiallymoreexpressivearchitectures,tocreatefullygeneralintelligencesakin
tohumansorbeyond(Bostrom,2017).
Ontheotherhand,ifitwereshownthatthebrainwereconclusivelynotdoingbackprop,
then this would also be an advance, although a lesser one, in neuroscience. Such a
conclusionwouldnecessarilyshedlightupontheactualalgorithmsutilizedbythebrain
forcreditassignmentandlearning,whichwouldprovidebothageneralprinciplefor
understandingthefunctionandoperationofthebrainovertime,aswellasundoubtedly
provide key insights for machine learning in developing, improving, and scaling up
currentmethods.
Chapter6. CreditAssignmentintheBrain 272
Thetheoryandalgorithmforbackpropagationoferror(Backprop)emergesinthe1970s
(Linnainmaa,1970),andbythe1980swaswidelyusedfortrainingconnectionistneural
networks (Griewank et al., 1989; Rumelhart et al., 1986; Rumelhart & Zipser, 1985)
Alreadyinthe1980s,researchershadproposedthatbackpropcouldbeimplemented
inthebrain,andtriedtofindcommonalitiesbetweenthencontemporaryneuroscience
andprogressinconnectionistmodellingusingneuralnetworks(Rumelhartetal.,1986).
However, a number of articles argued convincingly that a direct implementation of
backpropagation is biologically implausible (Crick, 1989), which dampened down
potential interest in this connection considerably until the question was re-raised by
the successes of machine learning in the 2010s. Before investigating the ways in
which backpropagation appears biologically implausible, and how these issues might
beaddressed,wefirstgiveadetailedintroductiontothebackpropalgorithm.
Firstwemustdecideonsometerminology. Neuralnetworksaretypicallytrainedtomin-
imize someloss function L. The credit assignmentproblem concerns thecomputation
ofthederivativesofthelosswithrespecttoeveryparameterofthenetwork ∂L. Back-
∂W
propagationoferrorisanalgorithmthatsolvesthecreditassignmentproblemexactly
usingthetechniqueofAutomaticDifferentiation(AD)(Baydin,Pearlmutter,Radul,&
Siskind,2017;Griewanketal.,1989;Paszkeetal.,2017;VanMerriënboer,Breuleux,
Bergeron, & Lamblin, 2018). 2 . Given these derivatives, the network can be trained
withthestochasticgradientdescentalgorithmW =W +η∂L. However,giventhe
t+1 t ∂W
gradients computed by backprop, other gradient algorithms are possible including a
varietyofmodified descentproceduressuch asNesterovMomentum (Nesterov, 1983),
RMS-prop(Hintonetal.,2012)andAdam(Kingma&Ba,2014)secondordermethods
such as natural gradients (Amari, 1995) and Gauss-Newton optimization, as well as
stochastic sampling methods such as stochastic langevin dynamics (Welling & Teh,
2011),andHamiltonianMCMC(Nealetal.,2011).
2There are other methods for computing derivatives, such as finite differences, but they are less
accurateandmorecomputationallycostlythanADandarenotgenerallyusedinmachinelearning
Chapter6. CreditAssignmentintheBrain 273
The fundamental mathematics underlying backprop is the chain rule of calculus. Es-
sentially, if we have a function – such as a neural network – which consists of the
composition of many differentiable functions, then we can express the derivative of
a complex composite function as a product of the derivatives of all the component
functionswithrespecttooneanother. Supposewehavetheforwardfunctionandloss
function,
yˆ= fL(WLfL−1(Wl−1fL−2(... f0(W0x))))
L =g(yˆ,t) (6.1)
where yˆ is the prediction outputted by the neural network, L is the number of layers,
[fL... f0] is the activation functions for each layer and [WL...W0] is the weights or
parametersforeachlayer. L istheoveralllossfunction,t isthedesiredtargetoutputs
andg istheloss function. Withthis forward function,wecan computethederivative of
thelosswithrespecttoanyparameterset–forinstanceW0 usingthechainrule,
∂L ∂L ∂yˆ (cid:2) l=1 ∂yl (cid:3) ∂y1
= ∏ (6.2)
∂W0 ∂yˆ ∂yL ∂yl−1 ∂W0
l=L
which allows us to express the derivative of a product of all the derivatives of the
intermediatecomponent functions. In general,this ispossiblefor anyfunction aslong
as every component function is differentiable. We can represent any function as a
computationgraph whichisagraphwhereeachintermediatestepinthecomputationis
avertexandeach componentfunctionisan edge. Whilethisexample,and mostsimple
neural network, is simply a chain graph, other more complex graphs are possible. If
there are multiple paths through the graph, the derivatives of the paths are summed
together. As long as every component function is differentiable, every one of the
derivatives in Equation 6.2 can be explicitly computed and evaluated, thus allowing
thefullderivative ∂L tobecomputedexplicitly. Importantly,thecomputationalcost
∂W0
ofsuch anevaluationis generallylinearin thenumberof componentfunctions –and
thusis ofapproximatelythe samecomplexity assimplyevaluatingthefunction inthe
first place. AD approaches like this, then, allow for the evaluation of derivatives of
Chapter6. CreditAssignmentintheBrain 274
any differentiable computation for a low andconstant additional computational cost–
allowingfortheirwidespreadusewithinmachinelearning.
TherearetwoapproachesinADtocomputingthechainofderivativesasinEquation6.2,
whicharecalled‘forward-mode’and‘reverse-mode’AD.Thedifferencebetweenthese
methodsiseffectivelywhethertheproductofderivativesiscomputedfromrighttoleft
(forward-mode)orlefttoright(reversemode). Forward-modeeffectivelyaccumulates
∂y1
thegradientsstartingwiththeinitialjacobian andthenmovingleftwardsdownthe
∂W0
chain,inthesamedirectionastheoriginalfunctionevaluation. Thisallowsderivative
evaluationtotake placeinparallelwith originalfunctionevaluationthroughthe useof
‘dualnumbers’(Griewanketal.,1989)whichextendeverynumberwithan‘derivative
part’ analogous to how complex numbers extend real numbers with a complex part.
Sincedual-numberscanbeevaluatedinparallelwiththeoriginalfunctionevaluation,
the computational cost of forward-mode AD is of the order of the input dimension
andithasaconstantmemorycost,sincenointermediateproductsneedtobestoredin
memory.
Reverse-modeAD,conversely,evaluatesthechainofderivativesfromlefttoright. That
is, it starts with and accumulates onto the vector ∂L, which is also called the adjoint
∂yˆ
or pullback 3. It then iterates recursively ‘backwards’ through the chain using the
followingequation,
∂L ∂L
∂yl+1
j
=∑ (6.3)
∂yl ∂yl+1 ∂yl
j j
where the sum simply states that if there are multiple potential paths in the graph,
they should be summed together. Since it starts at the ‘end’ and works backwards,
reverse-modeADrequiresthefunctiontobeevaluatedfirstafterwhichthederivatives
can begin to be computed in a backwards sweep, thus leading reverse-mode AD to
use characteristic forward (function evaluation) and backwards (derivative evaluation)
sweeps. Since all intermediate activities must be stored in memory, reverse-mode
3Incontinuoustime,Equation6.3becomestheadjointODE
Chapter6. CreditAssignmentintheBrain 275
AD has a memory cost linear in the number of component functions, as well as a
computationalcostwhichscaleswiththedimensionoftheoutput. Sinceneuralnetworks
typically have a scalar output loss and very high dimensional inputs (such as image
pixels), reverse-mode AD is typically computationally cheaper and is the method used
inpracticefortrainingdeepnetworks. ReversemodeADalsohastheadvantagethat
itbackpropagatesthegradientsbacktowheretheweightsaredirectly,whileforward-
modeADdoescomputetheweights,buttheyareallbunchedupattheendofthegraph
by the loss, and therefore would need, in a physical system such as the brain, to be
transmittedbacktowheretheweightswereoriginallyaswell.
Here,wefocusprimarilyontheimplementationofreverse-modeADinthebraindueto
itsgenerallysuperiorcomputationalcapabilities(andavoidanceofthisweightlocality
problem for forward mode AD). It is possible, however, that the brain may use some
combinationofforwardandreversemodeinpractice. Oneespeciallyappealingmethod
istousereverse-modeADtohandlehierarchicalnetworks–i.e. nonlocalityinspace,
whileusingforwardmodeADtohandlerecurrentcreditassignmentthroughtime. Here
forwardmodeADhastheclearadvantagethatthegradientsmoveforwardintimeatthe
samerateastheweightsthemselves,sothatthereisnolocalityproblemhere. Infact,
thelocalityproblemnowafflictsreverse-modeADwhich,inthiscircumstance,needsto
backpropagategradientsbackwardsthroughtime,whichisproblematic. Herewedonot
addressthetemporalcreditassignmentproblemandfocusentirelyonbackpropagation
throughspace,whereweassumereverse-modeADisthebestapproach.
Although reverse-mode AD is a well characterised algorithm, it is not at all clear
whetheritcanbeimplementatedinthebrain. Specifically,backpropagationhasthree
principal problems which make its apparent biological plausibility dubious – firstly,
backpropagationappearstorequirenon-localinformationtransfer,sincethegradientof
thesynapticweightsdependsonactivityfromtherestofthenetworkwhichultimately
leadstothefinallossoutcome. Secondly,ifwelookatthespecificcaseofarate-coded
Chapter6. CreditAssignmentintheBrain 276
standardintegrateandfiremodel, whereby theoutputisafunctionoftheinput andthe
synapticweights,
yl+1 = f(Wlyl) (6.4)
thenthederivativeofthelosswithrespecttothepre-activationsbecomes,
∂L
=
∂L ∂f(Wlyl) WlT
(6.5)
∂y ∂yl+1 ∂yl
l
whichrequiresboththederivativeoftheactivationfunction,whichmayormaynotbe
easytocomputelocally,andalsothetransposeofthefeedforwardweightsWlT
. This
transposeisproblematicsinceeffectivelyitrequiresthebackwardspassactivationsto
be sent ‘backwards’ through the forwards weights – a process which is biologically
implausible. This problemis calledthe‘weight transportproblem’. Finally, ifwe look
attheupdaterulefortheweightsthemselves,
∂L ∂L ∂f(Wlyl)
= yL (6.6)
∂Wl ∂yl+1 ∂Wl
whichwhileitdoesdependonthepre-synapticactivationsyl alsodependsontheadjoint
vector ∂L which is generally non-local. Interestingly, the update rule specifically
∂yl+1
doesnot dependatallonthepost-synapticactivity,incontrasttothewidelyaccepted
view of Hebbian plasticity being implemented in the brain, although it does depend
∂f(Wlyl)
on the derivative of the post-synaptic activity with respect to the weights ,
∂Wl
whichmayor maynotbedifficult tocompute. Theissuefirstof computingtheadjoint
vector ∂L inabiologicallyplausiblemanner,andthentransmittingittotherequired
∂yl+1
synapses, since it is non-local, thus form the core issue standing in the way of any
biologicallyplausibleimplementationofreverse-modeAD.Afinalissuerelatestothe
need,inreverse-modeAD,forseparateforwardandbackwardsphases,whilethebrain
presumablyneedstooperatecontinuouslyintime. Ithasbeensuggestedthatthebrain’s
rhythmicoscillations(Buzsaki,2006)mayallowitto‘multiplex’forwardandbackward
passestogether,althoughthisintuitionhasnotyet,tomyknowledge,beenmadeprecise
intheliterature.
Chapter6. CreditAssignmentintheBrain 277
Duetoageneralunderstandingthatbackpropagationisbiologicallyimplausible,much
researchhasfocusedonotherpotentiallymorebiologicallyplausiblemethodsbywhich
the brain might learn. A large amount of attention has focused on Hebbian update
rules(Gerstner&Kistler,2002),whichonlyutilizethepostandpresynapticactivities,
basedontheinitial intuitions ofDonaldHebb(Hebb,1949). A numberofvariantsof
Hebbian learning have been proposed, and the full class of potential algorithms has
been exhaustively analysed in (Baldi & Sadowski, 2016). However, a key issue with
Hebbianlearningisthat,sinceitcanonlyuselocalinformationintheformofpreand
postsynapticactivities,itcannotincorporateinformationaboutthedistantlossfunction,
and thus allow for the precise goal-directed learning that backprop is capable of. In
effect,Hebbianlearningcanonlycaptureandstrengthenthelocalcorrelationsoffiring
rates across a network and thus, while it can often be used to solve tasks in shallow
networkswithjustasingleorafewhiddenlayers,itfailstoscalesuccessfullytodeep
layers(Lillicrap&Santoro,2019).
Asecondapproachistouseglobalneuromodulatorsasapartofa‘three-factor’learning
rulewhich includescontributions fromboththe pre-synapticand post-synapticactivity
as well as this ‘third-factor’ (Gershman, 2018) which is often conceptualised to be
dopamine, in light of the fact that dopaminergic connections from the mid-brain are
well-knowntoinnervatelargepartsofthecortex(Dawetal.,2006). Thesedopaminergic
neurons could be signalling some kind of global reward signal, inducing all neurons
to increase their weight when a positive reward is encountered and decrease it when
a negative reward occurs (Lillicrap et al., 2020; Roelfsema & Ooyen, 2005; Seung,
2003),inaprocedurewhichiseffectivelyequivalenttopolicygradientmethodsfrom
reinforcementlearning(R.J.Williams&Zipser,1989a). Whilesuchapproachescan,
asymptotically, learncomplex functions indeep neural networks, their keylimitation is
thatthe gradientestimatesthey computehave extremely highvariance, sincetheglobal
neuromodulator cannot distinguish whether a particular weight helped give rise to a
rewardor not, and thus cannot provide precise feedback likebackprop. Instead it takes
Chapter6. CreditAssignmentintheBrain 278
a substantial amount of trials for the random noise provided by the contributions of
all the other neurons in the brain to be averaged out to get at the contribution of just
asinglesynapticweight,whichleadstoslowandunstablelearningincomplextasks
withdeepnetworks(Lillicrap&Santoro,2019). Additionally,thisapproachimplicitly
assumesthattheentirebrainisoptimizedend-to-endforreward,howeverthisseems
potentiallyunlikelygiventhatlargepartsofthecorticesdealwithaspectslikesensory
stimuli whichare distant fromreward andmost likely useauxiliary losses suchas their
own immediate prediction errors. Nevertheless, if it turns out that backpropagation
is not, in fact, used in the brain, then global neuromodulatory rules like this may be
the second best option. Importantly,even if itis thecase thatthe braindoes backprop,
it likely also uses this global neuromodulatory approach in a modulatory function to
bias learning towards high reward contingencies and perhaps also to adaptively tune
learningratesthroughoutthecortexsothathighlyvalencedexperiences(eitherpositive
ornegative)haveastrongeffectonplasticitythroughoutthebrain.
Finally,thereisalsoasmallbutgrowingliteratureattemptingtounderstandhowand
whetherbackpropagationcanbedirectlyimplementedinthebrain–namelywhetherit
ispossibletodesignneuralcircuitstoworkaroundthekeylimitationsproposedearlier.
Animportantlineofworktacklestheweighttransportproblem. Lillicrapetal.(2016)
demonstratethatinrealityprecisecopyingoftheforwardandbackwardsweightsisnot
necessary,andinfactrandomfixedbackwardweightssufficeduetothephenomenon
of ‘feedback alignment’ whereby the random feedback weights effectively force the
forward weights to align with the backward ones to be able to learn. This approach
can be improved by ensuring that the sign of the elements of the feedback weight
matrixmatchesthesignoftheforwardweightmatrix(potentiallyamorebiologically
plausibleconstraintthanexactcopyingofvalues)(Liaoetal.,2016), orelselearning
the backwards weights using an additional plasticity rule (Akrout et al., 2019; Amit,
2019; Millidge, Tschantz, Seth, & Buckley, 2020d). While the feedback alignment
techniquedoesnottypicallyscaletodeeparchitectures,asthefeedbackpathbecomes
Chapter6. CreditAssignmentintheBrain 279
increasingly corrupted (Bartunov et al., 2018), an approach called direct feedback
alignment (DFA) (Nøkland, 2016), whereby all layers are directly connected to the
output layer through random backwards weights, has been shown to be able to scale
to large deep architectures (Launay, Poli, & Krzakala, 2019), although not the usual
convolutionalneuralnetworksusedinvision. Whileanimpressiveresult,DFAitself
violates known neural connectivity constraints which feature reciprocal connectivity
betweenregionsandnoteverylayerreceivingdirectbackwardsconnectionsfromthe
‘output’.
Secondly, another line of work has focused on the algorithm of target-propagation
(Bengio&Fischer,2015;D.-H.Lee,Zhang,Fischer,&Bengio,2015)whichissimilar
to backprop except that instead of providing gradients back to the weights at each layer
itprovidestargetswhichcanthenbeoptimizedlocally. Thesetargetsareproducedby
mapping backwardsthe output of thenetwork ‘nudged’ towards the truetarget through
an inverse mapping at each layer. The intuition, is that we want to find the targets
which,ifthelowerlayershadmatchedtheiractivations,wouldhaveproducedafinal
outputclosertothetarget. Thepastyearhasseensubstantialadvancesinthetheoretical
analysis of target-prop, where it is now recognised to not approximate backprop, but
instead be performing a hybrid form of Gauss-Newton optimization (Bengio, 2020;
Meulemans,Carzaniga,Suykens,Sacramento,&Grewe,2020). Whilepromising,large-
scalestudiesonthestabilityandscalabilityoftarget-proplearninghavenotyetbeen
done,althoughinitialresultsarepromising(Bartunovetal.,2018). Additionally,target-
propdoesnotprovidesolutionstotheweighttransportproblems(nowcomplicatedby
theadditionalnecessityoflearningthebackwards inverseweights),and thenecessity
of storing and comparing the forward and backward information across phases. The
intuition of using the final loss function to compute layer-wise targets has also been
applied, with variations, in other works (Kaiser, Mostafa, & Neftci, 2020; Ororbia &
Mali,2019;OrorbiaII,Haffner,Reitter,&Giles,2017).
Chapter6. CreditAssignmentintheBrain 280
Another approachis touse only local information atthe synapses, but use abackwards
phasewhichinsteadofbeingpurelysequentialistreatedasadynamicalsystemswhich
undergoesmultipleiterations. Usingthisapproach,whileittakeslongerthanasequen-
tial backwards pass, also allows using only local information, since the information
about the losscan be ‘leaked’ slowlybackwards using the dynamicsover time instead
ofhavingtobeexplicitlytransmittedbackwards. Thisallowsthebraintooperateusing
the same dynamical rules at all times, in general, instead of sequential forwards and
backwards transmission of information. Onekey example of such aframework is the
algorithmofEquilibriumPropagation(Bengio,Mesnard,Fischer,Zhang,&Wu,2017;
Scellier&Bengio,2017;Scellier,Goyal,Binas,Mesnard,&Bengio,2018a,2018b),
which uses two dynamical phases – a free phase where the dynamics of the system
are allowed to evolve without any influence of the targets, and a clamped phase in
whichtheoutputunitsareheldatavaluenudgedtowardsthetargets,whichdestabilizes
thefree-phaseequilibriumandinsteadsendsthenetworktowardsadifferentclamped
equilibriumstate. Itturnsoutthatthedifferencebetweenthesetwostatescorresponds
closelytothe gradients whichwouldotherwisehavebeenbackpropagated throughthe
networkand canthus beused toadjust thesynaptic weights(Scellier &Bengio, 2017).
Equilibriumpropagationhasbeen extensivelytestedon small datasetslikeMNISTand
CIFAR, although itisnotyetknownhowwellthemethod scales. Additional problems
withEPareitsnecessaryuseoftwodistinctbackwardsphasesand,crucially,thestorage
ofinformation(theequilibriuminthefree-phase,throughouttheentiretyoftheclamped
phasebeforetheirsubtractiontoobtainthegradients. Anotheriterativealgorithmwhich
has beenshown toapproximate backpropis predictivecoding (Whittington &Bogacz,
2017).
Inthisthesischapter,wemaketwocontributionstothetheoryofiterativealgorithmsfor
approximatingbackprop. Firstly, weextendwork byWhittington andBogacz (2017),
showingthatpredictivecodingcanapproximatebackpropbymakingthisclaimprecise
and exact, and extending it to arbitrary computation graphs (Millidge, Tschantz, &
Chapter6. CreditAssignmentintheBrain 281
Buckley,2020a). Specifically,weshowthatpredictivecodingprovidesafullygeneral
iterative approach to approximating reverse-mode automatic differentiation through
anidentificationoftheequilibriumpredictionerrorwiththeadjointterm. Thisallows
us to define predictive coding networks which can train any contemporary machine
learningarchitecturewithaccuracyequivalenttobackpropinalocalandbiologically
plausible (ish) manner. We demonstrate this capability on CNNs and LSTMs, thus
substantially extending the rangeand scale ofarchitectures to whichpredictive coding
hasbeenapplied.
Secondly, we propose a novel iterative algorithm – Activation Relaxation (Millidge,
Tschantz, Buckley, & Seth, 2020)– which converges precisely to the exact backprop
gradients, while alsoconsiderably simplifying thepredictive codingupdate rules and
obviatingtheneedforseparatepopulationsoferrorand‘value’neuronswhichpredictive
codingpossesses. Additionally, wedemonstratethatcertain remainingimplausibilities
in the algorithm such as the weight transport problem and the nonlinear derivatives
problem can be ‘relaxed’ while retaining learning performance almost equivalent to
backprop,evenonchallengingandlarge-scalecomputervisiontasks.
A final issuewhich undermines manyof the proposed learning rulesfor both sequential
methodsliketarget-propanditerativeoneslikeEPorpredictivecoding,isthenecessity
ofthree-factorlearningruleswithprecisevectorfeedback,likebackprop. Itisstillfairly
unclear whether such rules can actually be implemented in the brain, although there
has been some work showing that prediction error, or gradient like quantities could
in theory be transmitted backwards through the network using segregated dendrites
(Sacramento et al., 2018) which may help maintain separate error representations
independently of the firing rates of the rest of the somatic neuron. While the actual
biological plausibility of this approach is unclear, in the last section of this chapter,
wewillspeculatethatifitisplausible,andthebraincanmaintainandupdateseparate
error and value representations on single neurons, or indeed on separate populations
Chapter6. CreditAssignmentintheBrain 282
butwithprecisethree-factorlearningrules,thenthatisallthatisnecessaryforadirect
biologically plausible implementation of backprop, especially given recent research
showingthattheweighttransportproblemcanbelargelyovercomethroughlearningthe
backwardsweights. Wepresentatheoreticalalgorithm,extremelysimilartotarget-prop,
and with three-factor learning rules, which precisely corresponds to backprop in the
brain,whichismathematicallyequivalenttobackprop. Thus,ifthree-factorlearning
rules are possible in the brain, either through segregated dendrites, or else precise
interneuron connectivty, then exact backpropagation can be performed as simply as
target-proporanyotheralgorithm.
6.2 Predictive Coding Approximates Backprop Along
Arbitrary Computation Graphs
Here we demonstrate that predictive coding can approximate backpropagation on
arbitrary computation graphs. As we recall from chapter 5, predictive coding arises
froma variationalinferencealgorithm onthe activations oneach layer ofthe hierarchy,
whereby the predictive coding update rules can be derived as a gradient descent on
thevariationalfreeenergyF. Toshowcasehowthismethodologyextendstoarbitrary
graphs,wemustdefinethevariationalinferenceproblemtobesolvedonanarbitrary
computationgraph. First,wemustmakethenotionofacomputationgraphexplicit.
AcomputationgraphG ={E,V}isadirectedacyclicgraph(DAG)whichcanrepre-
sent the computational flow of essentially any program or computable function as a
composition of elementary functions. Each edge e ∈E of the graph corresponds to
i
an intermediate step – the application of an elementary function – while each vertex
v ∈Visanintermediatevariablecomputedbyapplyingthefunctionsoftheedgesto
i
thevaluesoftheiroriginatingvertices. v denotesthevectorofactivationswithinalayer
i
andwedenotethesetofallverticesas{v}. Effectively,computationflows‘forward’
i
Chapter6. CreditAssignmentintheBrain 283
∂L
=
∂L ∂v
1
∂L
=
∂L ∂v
2
∂L
=
∂L ∂v
3
∂v
0
∂v
1
∂v
0
∂v
1
∂v
2
∂v
1
∂v
2
∂v
3
∂v
2
v 0 v z1 v 2 v 3 T
∂v̂ ∂v̂ ∂v̂
v· =−ϵ +ϵ 1 v· =−ϵ +ϵ 2 v· =−ϵ +ϵ 3
0 0 1 v 1 1 2 v 2 2 3 v
0 1 2
v v v
0 1 2
v 1 ̂ v 2 ̂ v 3 ̂
ϵ ϵ ϵ ϵ T
0 1 2 3
Figure6.1: Top: Backpropagationonachain. Backpropproceedsbackwardssequen-
tiallyandexplicitlycomputesthegradientateachsteponthechain. Bottom: Predictive
codingonachain. Predictions,andpredictionerrorsareupdatedinparallelusingonly
local information. Importantly, while the original computation graph (black lines) must
beaDAG,theaugmentedpredictivecodinggraphiscyclic,duetothebackwards(red)
predictionerrorconnections.
fromparentnodestoalltheirchildrenthroughtheedgefunctionsuntiltheleafnodes
give the final output of the program as a whole (see Figure 6.1 and 6.2 (top) for an
example). GivenatargetT andalossfunctionL=g(T,v ),thegraph’soutputcanbe
out
evaluatedand, andifevery edgefunctionisdifferentiable, automaticdifferentiationcan
beperformedonthecomputationgraph.
Wecanextendpredictivecodingtoarbitrarycomputationgraphsinasupervisedsetting
bydefiningtheinferenceproblemtobesolvedasthatofinferringthevertexvaluev
i
ofeachnodeinthegraphgivenfixedstartnodesv (thedata),andendnodesv (the
0 N
targets). Wedefineagenerativemodelwhichparametrisesthevalueofeachvertexgiven
thefeedforwardpredictionofitsparents, p({v
i
})= p(v
0
...v
N
)=∏ N
i
p(v
i
|P(v
i
))4,and
4Thisincludestheprior p(v ),whichsimplyhasnoparents.
0
Chapter6. CreditAssignmentintheBrain 284
afactorised,variationalposteriorQ({v
i
}|v
0
,v
N
)=Q(v
1
...v
N−1
|v
0
,v
N
)=∏ N
i
Q(v
i
|P(v
i
),C(v
i
)),
whereP(v)denotesthesetofparentsandC(v)denotesthesetofchildrenofagiven
i i
node v. From this, we can define a suitable objective functional, the variational free
i
energyF (VFE),whichactsasanupperboundonthedivergencebetweenthetrueand
variationalposteriors.
F =KL[(Q(v ...v |v ,v )(cid:107)p(v ...v )]≥KL[(Q(v ...v )|v ,v )(cid:107)p(v ...v |v ,v )]
1 N−1 0 N 0 N 1 N−1 0 N 1 N−1 0 N
N
≈ ∑εTε
i i
i=0
(6.7)
UnderGaussian assumptionsfor thegenerative model p({v
i
})=∏ N
i
N(v
i
;vˆ
i
,Σ
i
), and
thevariationalposteriorQ({v
i
})=∏ N
i
N(v
i
),wherethe‘predictions’vˆ
i
= f(P(v
i
);θ
i
)
are defined as the feedforward value of the vertex produced by running the graph
forward, and all the precisions, or inverse variances, Σ−1 are fixed at the identity, we
i
can write F as simply a sum of prediction errors (see chapter 5), with the prediction
errors defined as ε =v −vˆ. Since F is an upper bound on the divergence between
i i i
true and approximate posteriors, by minimizing F, we reduce this divergence, thus
improving the quality of the variational posterior and approximating exact Bayesian
inference. PredictivecodingminimizesF byemployingtheCauchymethodofsteepest
descenttosetthedynamicsofthevertexvariablesv asagradientdescentdirectlyonF
i
dv ∂F ∂vˆ
i = =ε i − ∑ ε j j (6.8)
dt ∂v ∂v
i i
j∈C(vi)
ThedynamicsoftheparametersoftheedgefunctionsW suchthatvˆ = f(P(v);W),can
i i
alsobederivedasagradientdescentonF. Inaneuralnetworkmodel,theparameters
correspond to the synaptic weights of each layer of the neural network. Importantly
thesedynamicsrequireonlyinformation(thecurrentvertexvalue,predictionerror,and
predictionerrorsofchildvertices)locallyavailableatthevertex.
dW ∂F ∂vˆ
i i
= =ε (6.9)
i
dt ∂W ∂W
i i
Chapter6. CreditAssignmentintheBrain 285
To run generalized predictive coding on a given computation graph G ={E,V}, we
augmentthe graph with error units ε∈E to obtain an augumentedcomputationgraph
G˜ = {E,V,E}. The predictive coding algorithm then operates in two phases – a
feedforward sweep and a backwards iteration phase. In the feedforward sweep, the
augmentedcomputationgraphisrun forward toobtainthesetofpredictions{vˆ},and
i
predictionerrors{ε}={v −vˆ}foreveryvertex. Toachieveexactequivalencewith
i i i
thebackpropgradientscomputedontheoriginalcomputationgraph,weinitializev =vˆ
i i
in the initial feedforward sweep so that the output error computed by the predictive
codingnetworkandtheoriginalgraphareidentical–anassumptionwecallthefixed
predictionassumption.
Inthebackwardsiterationphase,thevertexactivities{v}andpredictionerrors{ε}are
i i
updatedwithEquation6.8forallverticesinparalleluntilthevertexvaluesconvergetoa
minimumofF. AfterconvergencetheparametersareupdatedaccordingtoEquation6.9.
Notewealsoassume,followingWhittingtonandBogacz(2017), thatthepredictionsat
each layerare fixed at thevalues assigned duringthe feedforward pass throughoutthe
optimisationofthevs. Thisisthefixed-predictionassumption. Ineffect,byremoving
thecouplingbetweenthevertexactivitiesoftheparentsandthepredictionatthechild,
this assumption separates the global optimisation problem into a local one for each
vertex. Weimplementthesedynamicswith asimpleforwardEulerintegration scheme
sothattheupdaterulefortheverticesbecamevt+1 ←vt−ηdF whereηisthestep-size
i i dvt
i
parameter. Importantly, if the edge function linearly combines the activities and the
parametersfollowedbyanelementwisenonlinearity,thenboththeupdateruleforthe
vertices(Equation6.8)andtheparameters(Equation6.9)becomeHebbian. Specifically,
the update rules for the vertices and weights become d
d
v
t
i =ε
i
−∑
j
ε
j
f(cid:48)(θ
j
vˆ
j
)θT
j
and
Chapter6. CreditAssignmentintheBrain 286
dθi =ε f(cid:48)(θvˆ)vˆT,respectively.
dt i i i i
Algorithm4:GeneralizedPredictiveCoding
Dataset: D ={X,L},AugmentedComputationGraphG˜ ={E,V,E},inference
learningrateη ,weightlearningrateη
v θ
for(x,L)∈D do
vˆ ←x
0
forvˆ ∈Vdo
i
vˆ ← f({P(vˆ);θ)
i i
end
ε ←L−vˆ
L L
whilenotconverged do
for(v,ε)∈G˜ do
i i
ε ←v −vˆ
i i i
vt+1 ←vt+η dF
i i vdvt
i
end
end
forθ ∈Edo
i
θt+1 ←θt+η dF
i i θdθt
i
end
end
6.2.1 Methods and Results
Todemonstratethatthispredictivecodingschemeapproximatesthebackpropagation
of error algorithm at convergence is fairly straightforward. The key step is to show
that the recursionrelationship of the adjoint termfor the equilibrium of theprediction
errors in predictive coding is identical to that in reverse-mode AD, even for arbitrary
computation graphs. To make this clear more intuitively, we first demonstrate that,
at the equilibrium of the dynamics, the prediction errors ε∗ converge to the correct
i
backpropagatedgradients ∂L,andconsequentlytheparameterupdates(Equation6.9)
∂vi
Chapter6. CreditAssignmentintheBrain 287
becomepreciselythoseofabackproptrainednetwork.
First,underthefixedpredictionassumption,wecandirectlysolvefortheequilibrium
ofthedynamicsbysettingthetimederivativeto0,
∂vˆ
ε∗ = ∑ ε∗ i (6.10)
i j ∂v
j
j∈C(vi)
Ifwe comparethisto therecursive relationshipinherent toreverse-mode AD(Equation
6.3),wecanseethatthepredictionerrorssatisfythesamerecursiverelationship. Since
thisrelationshipisrecursive, allthatisneededforthe prediction errorsthroughoutthe
graphtoconverge tothebackpropagatedderivativesis forthepredictionerrorsatthe
finallayertobeequaltotheoutputgradient: ε∗ = ∂L. Toseethisexplicitly,consider
L ∂vˆL
a mean-squared-error loss function 5. at the output layer L= 1(T −vˆ )2 with T as a
2 L
vectorof targets, anddefiningε =T −vˆ . We thenconsider theequilibrium valueof
L L
the prediction error unit at a penultimate vertex ε . By Equation 6.10, we can see
L−1
thatatequilibrium,
∂vˆ ∂vˆ
ε∗ =ε∗ L =(T −vˆ∗) L (6.11)
L−1 L ∂v L ∂v
L−1 L−1
since,(T −vˆ )= ∂L,wecanthenwrite,
L ∂vˆL
∂L ∂vˆ ∂L
ε∗ = L = (6.12)
L−1 ∂vˆ ∂v ∂v
L L−1 L−1
Thusthepredictionerrorsofthepenultimatenodesconvergetothecorrectbackprop-
agated gradient. Furthermore, recursing through the graph from children to parents
allowsthecorrectgradientstobecomputed6. Thus,byinduction,wehaveshownthat
thefixedpointsofthepredictionerrorsoftheglobaloptimizationcorrespondexactly
5Whilethemean-squared-errorlossfunctionfitsmostnicelywiththeGaussiangenerativemodel,
otherlossfunctionscanbeusedinpractice. Ifthelossfunctioncanberepresentedasalogprobability
distribution, then the generative model can be amended to simply set the output distribution to that
distribution. Ifnot, thenthereisnofullyconsistentgenerativemodel(althoughallnodesexceptthe
outputremainGaussian),butthealgorithmwillstillworkinpractice. SeeFigure6.4forresultsforCNNs
trainedwithacrossentropyloss.
6Somesubtletyisneededheresincev mayhavemanychildrenwhicheachcontributetotheloss.
L−1
However, these different paths sum together at the node v , thus propagating the correct gradient
L−1
backwards.
Chapter6. CreditAssignmentintheBrain 288
tothebackpropagatedgradients. Intuitively,ifweimaginethecomputation-graphas
a chain and the error as ‘tension’ in the chain, backprop loads all the tension at the
end (the output) and then systematically propagates it backwards. Predictive coding,
however, spreads the tension throughout the entire chain until it reaches an equilibrium
wheretheamountoftensionateachlinkispreciselythebackpropagatedgradient.
Byasimilarargument,itisapparentthatthedynamicsoftheparametersθ asagradient
i
descentonF alsoexactlymatchthebackpropagatedparametergradients.
dW ∂F ∂ε∗
i = =ε∗ i
dt ∂W i ∂W
i i
∂L ∂vˆ ∂L
i
= = (6.13)
∂vˆ ∂W ∂W
i i i
Whichfollowsfromthefactthatε∗ = dL andthat dε∗ i = dvˆi.
i dvˆi dθ dθi
To demonstrate empirically that this approach works, we present a numerical test in
the simple scalar case, where we use predictive coding to derive the gradients of an
√
arbitrary,highlynonlineartestfunctionv =tan( θv )+sin(v2)whereθisanarbitrary
L 0 0
parameter. For our tests, we set v to 5 and θ to 2. The computation graph for this
0
function is presented in Figure 6.2. Although simple, this is a good test of predictive
coding because the function is highly nonlinear, and its computation graph does not
follow a simple layer structure but includes some branching. An arbitrary target of
T =3wassetattheoutputandthegradientofthelossL=(v −T)2 withrespecttothe
L
inputv was computedby predictive coding. We show (Figure6.2) that the predictive
0
coding optimisation rapidly converges to the exact numerical gradients computed by
automatic differentiation, and that moreover this optimization is very robust and can
handleevenexceptionallyhighlearningrates(upto0.5)withoutdivergence.
Secondly, we wish to show empirically that this approach can be used to train deep
neural network architectures, of the kind used in machine learning, up to high levels
of performance equivalent to those trained with backprop. First, we demonstrate a
predictivecodingCNNmodel. Convolutionalneuralnetworkshavebeenacornerstone
ofmachinelearningsincethepioneeringdemonstrationsoftheirpoweronImageNetby
Chapter6. CreditAssignmentintheBrain 289
θ ϵ 1 d d μ θ 1 v 1̂ ϵ 2 d d μ v 1̂ 2 v 2̂ ϵ 3 d d μ v 2̂ 3 v 3̂
ϵ 1 d d μ v 0 1 * μ 1 (⋅ = ,
ϵ
⋅ θ
1
) *v 0 μ 2= ⋅
ϵ
(
2
v 1̂) μ 3= t ta a n n ( (
ϵ
v ⋅ 2̂
3
) )
ϵL d
d
μ
v 3
L
̂
ϵ
L
v +(⋅,⋅) T
0 μ 4=v
0
2
μ 5=sin(v 4̂)
μL=v 3̂ +v 5̂
ϵ dμ 4 (⋅)2 sin(⋅)
4dv 0 ϵ 4 dμ ϵ 5 ϵL d d μ v 5 L ̂
v 4̂ ϵ 5dv 4̂ 5 v 5̂
√
Figure 6.2: Top: The computation graph of the nonlineartest function v =tan( θv )+
L 0
sin(v2). Bottom: graphs of the log mean divergence from the true gradient and the
0
divergencefordifferentlearningrates. Convergencetotheexactgradientsisexponential
androbusttohighlearningrates.
Chapter6. CreditAssignmentintheBrain 290
(Krizhevskyetal.,2012),andarestillwidelyusedasstateoftheartmodelsforimage
processing.
ThekeyconceptinaCNNisthatofanimageconvolution,whereasmallweightmatrix
isslid(orconvolved)acrossanimagetoproduceanoutputimage. Eachpatchofthe
output image only depends on a relatively small patch of the input image. Moreover,
the weights of the filter stay the same during the convolution, so each pixel of the
output imageis generated usingthe same weights. The weight sharingimplicit in the
convolutionoperationenforcestranslationalinvariance,sincedifferentimagepatches
areallprocessedwiththesameweights.
Theforwardequationsofaconvolutionallayerforaspecificoutputpixel
k=i+fl=j+f
v = ∑ ∑ θ x (6.14)
i,j k,l i+k,j+l
k=i−fl=j−f
Where v is the (i, j)th element of the output, x is the element of the input image
i,j i,j
andθ isanweightelementofafeaturemap. Toset-upapredictivecodingCNN,we
k,l
augment each intermediate x and v with error units ε of the same dimension as the
i i i
outputoftheconvolutionallayer.
Predictionsvˆareprojectedforwardusingtheforwardequations. Predictionerrorsalso
needtobe transmittedbackwardsforthearchitectureto work. To achievethis we must
havethatpredictionerrorsaretransmittedupwardsbya‘backwardsconvolution’. We
thusdefinethebackwardspredictionerrorsεˆ asfollows:
j
i+f j+f
εˆ = ∑ ∑ θ ε˜ (6.15)
i,j j,i i,j
k=i−fl=j−f
Where ε˜ is an error map zero-padded to ensure the correct convolutional output size.
Inferencein thepredictivecodingnetworkthen proceedsbyupdating theintermediate
valuesofeachlayerasfollows:
dv
l
=ε −εˆ (6.16)
l l+1
dt
Chapter6. CreditAssignmentintheBrain 291
TheCNNweightscanbeupdatedusingthesimpleHebbianruleofthemultiplication
ofthepreandpostsynapticpotentials.
dθ
l =∑ε v T (6.17)
dt
li,j l−1i,j
i,j
In our experiments we used a relatively simple CNN architecture consisting of one
convolutionallayerofkernelsize5,andafilterbankof6filters. Thiswasfollowedby
amax-poolinglayerwitha(2,2)kernelandafurtherconvolutionallayerwitha(5,5)
kernel and filter bank of 16 filters. This was then followed by three fully connected
layersof200,150,and10(or100forCIFAR100)outputunits. Eachconvolutionaland
fully connectedlayer usedthe relu activation function, except theoutput layer which
waslinear. Althoughthisarchitectureisfarsmallerthanstateoftheartforconvolutional
networks, our primary purpose here is to demonstrate the equivalence of predictive
coding and backprop. Further work could investigate scaling up predictive coding to
morestate-of-the-artarchitectures.
Ourdatasetsconsistedof32x32RGBimages. Wenormalisedthevaluesofallpixelsof
eachimagetoliebetween0and1,butotherwiseperformednootherimagepreprocess-
ing. Wedidnotusedataaugmentationofanykind. Wesettheweightlearningratefor
thepredictivecodingandbackpropnetworks0.0001. Aminibatchsizeof64wasused.
These parameters were chosen without any detailed hyperparameter search and so are
likelysuboptimal. Themagnitudeofthegradientupdateswasclampedtoliebetween
-50and50inallofourmodels. Thiswasdonetopreventdivergences,asoccasionally
occurredintheLSTMnetworks,likelyduetoexplodinggradients.
First, we compare the test and training accuracy, as well as training loss plots for
convolutional CNNs trained on CIFAR10. Figure 6.4 shows convincingly that the
accuracy andindeedthe trainingdynamics ofpredictivecodingand backpropare the
same, to allintents and purposes, thusdemonstrating that predictive coding approaches
can approximate backprop to a very high accuracy, using only local learning rules.
To investigate this further, we explicitly plotted the divergence between the gradient
Chapter6. CreditAssignmentintheBrain 292
estimatesproducedbypredictivecodingandtheanalyticallycorrectgradientsproduced
bybackprop.
Importantly, wefound that thedivergence betweenthe true andpredictivecoding gradi-
ents was extremely small, and remained approximately constant throughout training
suggestingthatpredictivecodingnetworksdonotsufferfromaccumulatingerrorsin
theirgradientapproximationprocess. Importantly,toachievethislevelofconvergence
required100backwardsiterationsusinganinferencelearningrateof0.1. Thismeans
that the predictive coding has an approximately 100x computational overhead com-
pared tobackprop –largely rendering ituncompetitive for directcompetition inserial
computers. Nevertheless, this choice of 100 iterations is on the high end of what is
necessary,sinceweareprimarilyconcernedwithshowingtheasymptoticequivalence,
andinrealitythenumberofiterationsrequiredmaybesubstantiallylower. Addition-
ally, predictive coding is a fully parallel algorithm unlike backprop, which must be
implementedsequentiallyandisabetterfitforthehighlyparallelneuralcircuitry.
It is also important to note that while predictive coding ‘naturally’ uses the mean-
squared error loss – so that the output error is a standard prediction error, other loss
functions as possible, such as the widely used cross-entropy loss. Predictive coding
can be straightforwardly extended to cover other loss functions by simply replacing
the final prediction error ε with the gradient of the loss function with respect to the
L
outputs. Herewedemonstratethatpredictivecodingwithamulti-classcrossentropy
lossalso performsequivalently tothe network trainedwith backpropon theCIFAR and
SVHNdatasets.
6.2.2 RNN and LSTM
6.2.2.1 RNN
We additionally tested a predictive coding RNN and LSTM. To train these recurrent
networkswithpredictivecoding,wesimplyusedtheapproachofusingpredictivecoding
Chapter6. CreditAssignmentintheBrain 293
(a)ConvLayer1 (b)ConvLayer2
(c)FCLayer1 (d)FCLayer2
Figure 6.3: Mean divergence between the true numerical and predictive coding backprops
overthecourseoftraining. Ingeneral,thedivergenceappearedtofollowalargelyrandomwalk
pattern,andwasgenerallyneglible. Importantly,thedivergencedidnotgrowovertimethroughout
training,implyingthaterrorsfromslightlyincorrectgradientsdidnotappeartocompound.
Chapter6. CreditAssignmentintheBrain 294
(a)SVHNtestaccuracy
SVHNtrainingaccuracy
(b)CIFARtrainingaccuracy (c)CIFARtestaccuracy
Figure6.4: TrainingandtestaccuraciesoftheCNNnetworkontheSVHNandCIFARdatasets
usingthecross-entropyloss. Ascanbeseenperformanceremainsveryclosetobackprop,thus
demonstratingthatourpredictivecodingalgorithmcanbeusedwithdifferentlossfunctions,not
justmean-squared-error.
Chapter6. CreditAssignmentintheBrain 295
to approximate backpropagation through time (BPTT) and applied predictive coding to
theunrolledcomputationgraph. Withlongsequencelengths,thisleadtoextremelydeep
graphsforpredictivecodingtotrain. Crucially,wedemonstratethatpredictivecoding’s
ability to train such graphs is not impaired by their depth, meaning that predictive
coding asa trainingalgorithm hasan exceptionalscalability forextremelydeep models
of the kind increasingly used in contemporary machine learning (He, Zhang, Ren, &
Sun,2016;Radfordetal.,2019)
The computation graph on RNNs is relatively straightforward. We consider only a
singlelayerRNNherealthoughthearchitecturecanbestraightforwardlyextendedto
hierarchicallystackedRNNs. AnRNNissimilartoafeedforwardnetworkexceptthat
itpossessesanadditionalhiddenstatehwhichismaintainedandupdatedovertimeasa
function of both the current input x and the previous hidden state. The output of the
networkyisafunctionofh. ByconsideringtheRNNatasingletimestepweobtainthe
followingequations.
h = f(θ h +θ x ) (6.18)
t h t−1 x t
y =g(θ h ) (6.19)
t y t
Wherefandgareelementwisenonlinearactivationfunctions. Andθ ,θ ,θ areweight
h x y
matricesforeachspecificinput. TopredictasequencetheRNNsimplyrollsforward
theaboveequationstogeneratenewpredictionsandhiddenstatesateachtimestep.
Itisimportanttonotethatthisisanadditionalaspectofbiologicalimplausibilitythat
wedonotaddressinhere. BPTTrequiresupdatestoproceedbackwardsthroughtime
fromtheendofthesequencetothebeginning. Ignoringanybiologicalimplausibility
withtherulesthemselves,thisupdatingsequenceisclearlynotbiologicallyplausibleas
naivelyitrequiresmaintainingtheentiresequenceofpredictionsandpredictionerrors
perfectlyin memoryuntil theend ofthe sequence,and waitinguntil thesequence ends
beforemakinganyupdates. Thereisasmallliteratureontryingtoproducebiologically
plausible, or forward-looking approximations to BPTT which does not require updates
Chapter6. CreditAssignmentintheBrain 296
to be propagated back through time (Lillicrap & Santoro, 2019; Ollivier, Tallec, &
Charpiat,2015;Steil,2004;Tallec&Ollivier,2017;R.J.Williams&Zipser,1989b).
While this is a fascinating area, we do not address it here. We are solely concerned
with the fact that predictive coding approximates backpropagation on feedforward
computationgraphsforwhichtheunrolledRNNgraphisasufficientsubstrate.
TolearnapredictivecodingRNN,wefirstaugmenteachofthevariablesh andy of
t t
theoriginalgraphwithadditionalerrorunitsε andε . Predictionsyˆ ,hˆ aregenerated
ht yt t t
according to the feedforward rules (16). A sequence of true labels {T ...T } is then
1 T
presented to the network, and then inference proceeds by recursively applying the
followingrulesbackwardsthroughtimeuntilconvergence.
ε =L−yˆ
yt t
ε =h −hˆ
ht t t
dh
t =ε −ε θT −ε θT (6.20)
dt ht yt y h t+1 h
Uponconvergencetheweightsareupdatedaccordingtothefollowingrules.
dθ T ∂g(θ h )
y = ∑ε y t hT
dt yt ∂θ t
t=0 y
dθ T ∂f(θ h +θ x )
x = ∑ε h t−1 x t xT
dt ht ∂θ t
t=0 x
dθ T ∂f(θ h +θ x )
h = ∑ε h t−1 x t hT (6.21)
dt ht ∂θ t+1
t=0 h
Sincethe RNNfeedforward updatesare parameter-linear,these rulesare Hebbian,only
requiring the multiplication of pre and post-synaptic potentials. This means that the
predictivecodingupdatesproposedherearebiologicallyplausibleandcouldintheory
be implemented in the brain. The only biological implausibility remains the BPTT
learningscheme.
Our RNN was trained on a simple character-level name-origin dataset which can be
found here: https://download.pytorch.org/tutorial/data.zip. The RNN was presented
Chapter6. CreditAssignmentintheBrain 297
withsequencesofcharactersrepresentingnamesandhadtopredictthenationalorigin
of the name – French, Spanish, Russian, etc. The characters were presented to the
network as one-hot-encoded vectors without any embedding. The output categories
were also presented as a one-hot vector. The RNN has a hidden size of 256 units. A
tanhnonlinearity was usedbetween hidden states and the outputlayer was linear. The
networkwastrainedonrandomlyselectedname-categorypairsfromthedataset.
WefirstpresentthetrainingandtestaccuracyforthebackpropRNNs,averagedover
fiveseeds. Ingeneral,performancebetweenthebackprop-trainedandpredictivecoding
networkswasindistinguishableonthistask.
Figure6.5: Testaccuracy plotsforthe Predictive Codingand Backprop RNNand LSTM
ontheirrespectivetasks,averagedover5seeds. Performanceisagainindistinguishable
frombackprop.
Additionally,ThetraininglossforthepredictivecodingandbackpropRNNs,averaged
over5seedsispresentedbelow(Figure6.6).
6.2.2.2 LSTM
Unlike the other two models, the LSTM possesses a complex and branching internal
computation graph, and is thus a good opportunity to make explicit the predictive
coding ‘recipe’ for approximating backprop on arbitrary computation graphs. The
computationgraph fora single LSTM cell is shown(with backpropupdates) inFigure
6.8. PredictionfortheLSTMoccursbysimplyrollingforwardacopyoftheLSTMcell
Chapter6. CreditAssignmentintheBrain 298
Figure 6.6: Training losses for the predictive coding and backprop RNN. As expected,
theyareeffectivelyidentical.
foreachtimestep. TheLSTMcellreceivesitshiddenstateh andcellstatec fromthe
t t
previoustimestep. Duringtrainingwecomputederivativesontheunrolledcomputation
graphandreceivebackwardsderivatives(orpredictionerrors)fromtheLSTMcellat
timet+1. For afulland detailedsetof equationsspecifyingthe complexLSTM cell,
seeAppendixB.
The recipe to convert this computation graph into a predictive coding algorithm is
straightforward. We first rewire the connectivity so that the predictions are set to the
forwardfunctionsoftheirparents. Wethencomputetheerrorsbetweentheverticesand
thepredictions.
During inference, theinputs h ,x and the output y are fixed. The vertices andthen the
t t t
predictionerrorsareupdated. Thisrecipeisstraightforwardandcaneasilybeextended
toothermorecomplexmachinelearningarchitectures. Thefullaugmentedcomputation
graph,includingthevertexupdaterules,ispresentedinFigure6.7.
Forthe LSTMwe alsoobserveda closecorrespondencebetween theperformance(in
termsoftrainingandtestaccuracy)betweenthepredictivecodingandbackpropagation
networks,thusdemonstratingthatpredictivecodingcanconvergetotheexactbackprop
Chapter6. CreditAssignmentintheBrain 299
dL dv 3 dL dv dL dc t+1
dv 3 dc t v 3=c t v 2 dv 7 dv 7 3 v 7=v 3+v 6 dc t+1 dv 7 c t+1
c
t × +
dL dv dL dv dL dv
6 7 9
dv dv dv dv dv dv
dL dv 6 4 × v 6= 7 v 5 v 6 4 v 9=tanh(v 7) tanh 9 7
v 2=σ(θ f v 1) σ dv 3 v 4 d = v 3 2 σ(θ inp v 1) σ v 5=tanh(θ c v 1) tanh d d v L 6 v d d 8 v v = 6 5 σ(θ 0 v1)σ v 10=v 8 v 9 × d d v L 10 d d v v 1 9 0 h t+1
dL dv
θ f θ inp θ c θ o θ ydv 10 dv 1 8 0 d d L yd d v y d d h t L +1 d d h v t 1 + 0 1
h t v 1= ⊕ h t⊕x t d d v L 2 d d v v 2 1 d d v L 4 d d v v 4 1 d d v L 5 d d v v 5 1 d d v L 8 d d v v 8 1 10
dL dv 1 dL dv 1 σ y=σ(θ y v 10)
dv 1 dh t dv 1 dx t dL
x dy
t
y t−T
Figure 6.7: Computation graph and backprop learning rules for a single LSTM cell.
InputstotheLSTMcellarethecurrentinputx andthepreviousembeddingh . These
t t
are then passed through three gates – an input, forget, and output gate, before the
outputofthewholeLSTMcellcanbecomputed
gradients even on exceptionally deep and complex computation graphs such as the
LSTM
Importantly, we observed rapid convergence to the exact backprop gradients even in
the case of very deep computation graphs (as is an unrolled LSTM with a sequence
length of 100). Although convergence was slower than was the case for CNNs or
lessersequencelengths,itwasstillstraightforwardtoachieveconvergencetotheexact
numericalgradientswithsufficientiterations.
Belowweplotthemeandivergencebetweenthepredictivecodingandtruenumerical
gradients as a function of sequence length (and hence depth of graph) for a fixed
computationalbudget of200 iterationswith aninferencelearning rateof 0.05. Ascan
beseen,thedivergenceincreasesroughlylinearlywithsequencelength. Importantly,
evenwithlongsequences,thedivergenceisnotespeciallylarge,andcanbedecreased
further by increasing the computational budget. As the increase is linear, we believe
Chapter6. CreditAssignmentintheBrain 300
v dμ v
3 ϵ 7 7
ϵ 7dv 3 ϵ ϵ dμ 9
c t v 2= μ ϵ σ 3 3 ( = θ f v c 1 t v ) 2 × σ 3 ϵ ϵ 3 v d d 4 μ v 2 3 μ 4=σ σ (θ inp v ϵ 1) v μ 4 7 μ 6 = = v 3 v ϵ + 5 6 v 4 d v d t 6 a μ v 4 6 + × nh ϵ 7 6 ϵ ϵ 7 v ϵ d d 6 v 6 μ v d d 5 6 7 μ v 5 6 ϵ 8 v σ 9 8 d μ v μ 9 7 8 ϵ = = 10 t d σ a d μ n ( v θ 1 h 8 0 0 ( v v μ 1 7 1 ) ) 0= ta × v n 8 v h ϵ 9 ϵ 9 ϵ 10 v d d 9 μ v 1 9 0 ϵ ct+1 c t+1
2 4 5 10
θ f θ inp θ c θ o θ y v 10 h t+1
h t v 1= ⊕ h t⊕x t ϵ 2 d d μ v 1 2 ϵ 4 d d μ v 1 4 ϵ 5 d d μ v 1 5 ϵ 8 d d μ v 1 8 ϵyd d ϵ v μ 1 y 0 ϵ ht+1
σ y T
μ y=σ(θ y v 10)
x
t
Figure6.8: TheLSTMcellcomputationgraphaugmentedwitherrorunits,evincingthe
connectivity scheme of the predictive coding algorithm. The key move is to associate
eachintermediatenodeinthecomputationgraphwithitsownpredictionerrorunit
thatpredictivecodingapproachesshouldbescalableevenforbackpropagatingthrough
verydeepandcomplexgraphs.
Wealsoplotthenumberofiterationsrequiredtoreachagivenconvergencethreshold
(here taken to be 0.005) as afunction of sequence length (Figure 6.11). We see that the
numberofiterationsrequiredincreasessublinearlywiththesequencelength,andlikely
asymptotes at about 300 iterations. Although this is a lot of iterations, the sublinear
convergence nevertheless shows that the method can scale to even extremely deep
graphs.
Our architecture consisted of a single LSTM layer (more complex architectures would
consistof multiple stackedLSTMlayers). TheLSTM wastrainedona next-character
character-levelpredictiontask. The datasetwasthe fullworks ofShakespeare, down-
loadablefromTensorflow. Thetextwasshuffledandsplitintosequencesof50charac-
ters,whichwerefedtotheLSTMonecharacteratatime. TheLSTMwastrainedthen
topredictthenextcharacter, soastoultimatelybeabletogeneratetext. Thecharacters
Chapter6. CreditAssignmentintheBrain 301
Figure6.9: TraininglossesforthepredictivecodingandbackpropLSTMsaveragedover
5seeds. Theperformanceofthetwotrainingmethodsiseffectivelyequivalent.
werepresentedasone-hot-encodedvectors. TheLSTMhadahiddensizeandacell-size
of 1056 units. A minibatch size of 64 was used and a weight learning rate of 0.0001
was used for both predictive coding and backprop networks. To achieve sufficient
numericalconvergencetothecorrectgradient,weused200variationaliterationswith
an inference learning rate of 0.1. This rendered the predictive LSTM approximately
200x as costly as the backprop LSTM to run. A graph of the LSTM training loss for
both predictive coding and backprop LSTMs, averaged over 5 random seeds, can be
foundbelow(Figure6.12).
6.3 Interim Discussion
Herewehaveshownthatpredictivecodingcanbeapplieddirectlytoarbitrarycompu-
tationgraphsandcanrapidlyandeffectivelyconvergetotheexactgradientsrequired
for the backpropagation of error algorithm. We have demonstrated this on deep and
state of the art machine learning architectures, thus achieving significantly greater
scale than previous works using predictive coding (Millidge, 2019c; Orchard & Sun,
2019; Whittington & Bogacz, 2017). Moreover, the predictive coding learning rule
Chapter6. CreditAssignmentintheBrain 302
Figure6.10: Divergencebetweenpredictivecodingandthecorrectbackpropgradients
as a function of sequence length. Crucially, this divergence only increases linearly in
the sequence length, allowing for very accurate gradient computation even with long
sequences.
usesonlylocallearningdynamicsandHebbianweightupdatesinthecaseoftheusual
feedforwardneuralnetworks(althoughtheydiffersomewhatfortheLSTM).Weights
areupdatedusingonlylocalpredictionerrors.
This approach also is innovative in that it phrases backprop in terms of variational
inferenceonthevaluesofthenodesinthecomputationgraph. Whileitmayseemjust
likeamathematicalconvenience,itactuallyhasdeepimplications. Itdrawsanotherlink
between the processes of optimization and variational inference, in a rather different
manner fromthat which haslargelybeen explored before. Insteadof conceptualising
inference as optimization, as is typically done in variational inference, we instead
conceptualizeacorecomponentofoptimization–creditassignment–purelyintermsof
inference. Whilethisdualityhasbeenconsideredbeforefortwolayernetworks(Amari,
1995),ourapproachissubstantiallymorepowerfulandgeneral,byshowcasingthatit
Chapter6. CreditAssignmentintheBrain 303
Figure 6.11: Number of iterations to reach convergence threshold as a function of
sequence length. Importantly, thenumber of iterations requiredto converge appears to
growsublinearlywithsequencelength,againimplyingthatconvergenceisnotcomputa-
tionallyunattainableevenwithverylongsequences.
holdsforarbitrarycomputationgraphs. Additionally,ourapproachprovidesanavenue
forinterestinggeneralizationsofbackpropthroughtheuseofprecisionparametersin
predictivecoding. Notethatinouranalysis,wehaveimplicitlyassumedthatallofthe
precisionparametersaresettotheidentityΣ=I,astheydonotfeatureinthelearning
andupdaterules,astheydoinChapter3. Ifwereintroduceprecisioninthiscontext,we
seethatithastheroleofmodulatinggradientmagnitudes–effectivelyimplementingan
adaptivelearningrate. Whatthismeans,intuitively,isthatwecanthinkaboutprecision
weightinginthiscaseasenablinganuncertaintyawarebackprop,whichspecifically
weights gradients by how uncertain they are – or by their variance. Effectively, this
method,withlearnableprecisions,woulddown-weighthighlyvariableanduncertain
gradients while upweighting those known to be certain. When applied to the input
thiswouldmimicfeaturesofattention,bydownweightingnoisyorotherwiseuncertain
Chapter6. CreditAssignmentintheBrain 304
Figure 6.12: Training losses for the predictive coding and backprop LSTMs averaged
over5seeds. Theperformanceofthetwotrainingmethodsiseffectivelyequivalent.
inputs andhaving themplay littlerole inlearning. Whilesuch an adaptive modulatory
role for precision may bring learning benefits, this must be explored further, as the
authorhopestodoinfuturework.
Finally, it is worth discussing several drawbacks of the method. The key one is its
computational cost. The networks presented here were trained with 100 dynamical
iterations to converge to the prediction error equilibrium before each weight update,
givingpredictivecodinganapproximately100xcomputationalcostcomparedtoback-
prop. This is obviously highly significant and renders these approaches unusable for
largescalenetworksonserialVon-Neumanncomputers. Whilethebrainutilizeshighly
parallel circuitry, and may therefore be more suited to such an iterative algorithm,
therearestillissueswithrequiringadynamicaliterationforconvergence. Specifically,
such iterations still require time and discrete phases so that the system cannot likely
simplyoperate incontinuous time. Moreover, if toomanyiterations arerequired, since
the brain must respond to a continually changing world instead of just single images
presentedinisolation,itmaybecomeoverwhelmedbyeventsandfailcomputationally,
if the input changes faster than it can dynamically converge to a solution. While not
Chapter6. CreditAssignmentintheBrain 305
necessarily as severe as needing dynamical approaches for inference, which would
entirelyhamstringanyresponse,herethedynamicsareonlyrequiredforlearningand
weightupdates,thismayneverthelessprovetobeasubstantialdrawbackofthemethod.
Our method, like most others, also requires two distinct phases which must either be
somehowcoordinatedexplicitly,ormultiplexedinthebrain.
Additionally, the fixed-prediction assumption embedded in the model requires main-
taining the stored memory of the feedforward pass values somewhere in the network
throughoutthebackwardsdynamicalphasewhichispotentiallyproblematicinneural
circuitry. Finally,althoughthepredictivecodinglearningrulesarelocal,inthesense
that they only require information from the same layer, they still require information
fromthe predictionerror unitstobe transmittedto theactivityunits, wherewe assume
thesynapticweightsarelocated(althoughinasegregateddendritemodeltheycould
justbeindifferentdendrites(Sacramentoetal.,2018)).
6.4 Activation Relaxation
Hereweintroduceaseconditerativealgorithmwhich approximates theexactbackprop-
agationgradientsasymptoticallyattheequilibriumofadyanmicalsystem. Wecallthis
algorithmActivationRelaxation (AR)because ofthe natureof theupdate rules,which
iterativelyupdate theactivationsofneuronsin theiterativephase ratherthanprediction
errors. Crucially,theupdaterulesproposedbyARareexceedinglysimpleandelegant,
anddonotrequireadditionalpopulationsof‘errorneurons’asinpredictivecoding,or
multiplebackwardsphasesasinEquilibirium-prop. ARarisesquitestraightforwardly
by trying to take a first principles approach to the iterative backprop approximation
schemes.
Toestablishnotation,weconsiderthesimplecaseofafully-connecteddeepmulti-layer
perceptron(MLP)composedofL layersofrate-codedneuronstrainedinasupervised
setting. The firing rates of these neurons are represented as a single scalar value xl,
i
Chapter6. CreditAssignmentintheBrain 306
referred to as the neurons activation, and a vector of all activations at given layer is
denoted asxl. The activationsof thehierarchically superordinatelayer area functionof
thehierarchicallysubordinatelayersactivationsxl+1 = f(Wlxl),whereWl ∈Θisthe
setofsynapticweights,andtheproductofactivationandweightsistransformedthrough
a nonlinear activationfunction f. The finaloutputxL of thenetwork iscompared with
the desired targets T, according to some loss function L(xL,T). In this work, we
takethislossfunctiontobethemean-squared-error(MSE)L(xL,T)= 2 1 ∑ i (x i L−T i )2,
althoughthealgorithmappliestoanyotherlossfunctionwithoutlossofgenerality(see
AppendixB).Wedenotethegradient ofthelosswithrespect totheoutputlayeras dL.
dxL
In the caseof the MSE loss, thegradient of the output layeris just the predictionerror
εL =(xL−T).
Firstly,weknowthatthekeyquantitywewishtoapproximateistheadjointterm ∂L
∂x
l
for a given layer l. If we know this adjoint, and have it present somewhere in the
local environment, thenthe gradient withrespect to theweights can becomputed using
onlylocallyavailableinformation. Inpredictivecoding,wecomputethisadjointterm
using the recursive relationship of the prediction errors. Here, we take a different
approach. Instead,weaskwhatisthesimplestpossibledynamicalsystemwhichcan
convergetotheexactadjointtermattheequilibrium. Aftersomethought,weemergeat
astraightforwardleakyintegratormodel.
dxl ∂L
=−xl+ (6.22)
dt ∂xl
which,atequilibrium,convergesto
dxl ∂L
=0 =⇒
x∗l
= (6.23)
dt ∂xl
This update rule includes the very adjoint term we are trying to compute, however, so
thesedynamicsarenotimmediatelycomputable. Tomakethemso,wefirstsplitupthe
adjoint. Bythechainrule,wecanwriteEquation6.22as,
dxl ∂L ∂xl+1(cid:12)
=−xl+ (cid:12) (6.24)
dt ∂xl+1 ∂xl (cid:12) xl=x¯l
Chapter6. CreditAssignmentintheBrain 307
wherex¯l isthevalueofxl computedintheforwardpass. Next,wenotethatifweusethe
activationsoftheneuronsateachlayer insteadofpredictionerrors toaccumulatethe
adjoint,we canexpress thisin termsofthe equilibriumactivation ofthesuperordinate
layer,
dxl ∂xl+1(cid:12)
=−xl+x∗l+1 (cid:12) (6.25)
dt ∂xl (cid:12) xl=x¯l
However,toachievethesedynamicsexactlyinamultilayerednetworkwouldrequire
thesequentialconvergenceoflayers,aseachlayermustconvergetoequilibriumbefore
thedynamics ofthe layerbelowcan operate. Thissequentialconvergencewould make
thealgorithmnobetterthanthesequentialbackwardssweepofbackprop. However,if
weapproximatetheequilibriumactivationsofthelayerwiththecurrentactivation,this
allowsustorunalllayersinparallel,yielding,
dxl ∂xl+1(cid:12)
=−xl+x∗l+1 (cid:12)
dt ∂xl (cid:12) xl=x¯l
∂xl+1(cid:12)
≈−xl+xl+1 (cid:12) (6.26)
∂xl (cid:12) xl=x¯l
≈−xl+xl+1f(cid:48)(Wl,x¯l)WlT
(6.27)
where f(cid:48) =
∂f(cid:48)(Wl,x¯l)
representsthepartialderivativeofthe postsynaptic activationwith
∂x¯l
respect to the presynaptic activation. Despite this approximation, we argue that the
system nevertheless converges to the same optimum as Equation 6.23. Specifically,
becauseweevaluate ∂xl+1 atthefeedforwardpassvaluex¯l,thistermremainsconstant
∂xl
throughouttherelaxationphase7. Keepingthistermfixedeffectivelydecoupleseach
layerfromanybottom-upinfluence. Ifthetop-down inputisalsoconstant,becauseit
hasalreadyconvergedsothatxl+1 ≈xl+1∗ ,thenthedynamicsbecomelinear,andthe
system is globally stable due to possessing a Jacobian which is everywhere negative-
definite. The top-layer is provided with the stipulatively correct gradient, so it must
7Theneedtokeepthistermfixedthroughouttherelaxationphasedoespresentapotentialissueof
biological plausibility. In theory it could be maintained by short-term synaptic traces, and for some
activationfunctionssuchasrectifiedlinearunitsitistrivial. Moreover,laterweshowthatthistermcan
bedroppedfromtheequationswithoutapparentill-effect
Chapter6. CreditAssignmentintheBrain 308
converge. Recursingbackwardsthrougheachlayer, weseethatoncethetop-levelhas
converged,sotoomustthepenultimatelayer,andsothroughtoalllayers.
Crucially, Equation 6.26, which is core to the AR algorithm is extremely simple and
biologicallyplausible. Itonlyrequiresthattheactivationsofagivenlayeraresensitive
tothedifferencebetweentheir ownactivityandthatofthelayerabovemappedthrough
thebackwardsweights,andmodulatedbythenonlinearderivativeofthepostsynaptic
potential. This update rule thus functions as a kind of prediction error, but one that
emergesbetweenlayers, ratherthanbeing representedbyspecificprediction errorunits
atagivenlayer.
Computationally, the AR algorithm proceeds as follows. First, a standard forward
pass computes thenetworkoutput, which is compared with thetarget tocalculate the
top-layererrorderivativeε andthusupdatetheactivationofthepenultimatelayer. 8.
L
Then,thenetworkentersintoarelaxationphasewhereEquation6.26isiteratedglobally
for all layers until convergence for each layer. Upon convergence, the activations of
eachlayerarepreciselyequalthebackpropagatedderivatives,andareusedtoupdate
theweights(viaEquation(6.28).
∂L ∂L ∂xl+1
=
∂Wl ∂xl+1 ∂Wl
=
∂L
f(cid:48)(Wlxl)xLT
(6.28)
∂xl+1
8Thistop-layererrorissimplyapredictionerrorfortheMSEloss,butmaybemorecomplicatedand
lessbiologically-plausibleforarbitrarylossfunctions
Chapter6. CreditAssignmentintheBrain 309
Algorithm5:ActivationRelaxation
DatasetD ={X,T},parametersΘ={W0...WL},inferencelearningrateη ,
x
weightlearningrateη .
θ
for(x0,t ∈D)do
for(xl,Wl)foreachlayer do
xl+1 = f(Wl,xl)
end
whilenotconverged do
εL =T −xL
dxL =−xL+εL∂εL
∂xL
forxl,Wl,xl+1 foreachlayer do
dxl =−xl+xl+1∂xl+1
∂xl
xlt+1 ←xlt +η dxl
x
end
end
forWl ∈{W0...WL}do
Wlt+1 ←Wlt +η xl ∂xl
θ ∂Wl
end
end
6.4.1 Method and Results
We first demonstrate that our algorithm can train a deep neural network with equal
performance to backprop. For training, we utilised the MNIST and Fashion-MNIST
(Xiao, Rasul, & Vollgraf, 2017b) datasets. The MNIST dataset consists of 60000
trainingand10000test28x28imagesofhandwrittendigits,whiletheFashion-MNIST
dataset consists of 60000 training and 10000 test 28x28 images of clothing items.
The Fashion-MNIST dataset is designed to be identical in shape and size to MNIST
whilebeingharderto solve. Weuseda4-layer fully-connected multi-layerperceptron
(MLP) with rectified-linear activation functions and a linear output layer. The layers
Chapter6. CreditAssignmentintheBrain 310
(a)MNISTtrainaccuracy (b)MNISTtestaccuracy (c)MNISTgradientangle
AR/
(d)Fashiontrainaccuracy (e)Fashiontestaccuracy (f)Fashiongradientangle
Figure 6.13: Train and test accuracy and gradient angle (cosine similarity) for AR
vs backprop for MNIST and Fashion-MNIST datasets. Importantly the training and
test accuracies are virtually identical between the AR-trained and backprop-trained
networks. Additionally, the gradient angle between the AR update and backprop is
alwayssubstantiallylessthanthe90degreesrequiredtoallowforlearning.
consisted of 300, 300, 100, and 10 neurons respectively. In the dynamical relaxation
phase, we integrate Equation 6.26 with a simple first-order Euler integration scheme.
xlt+1 =xlt
−η
dxl
where η was a learning rate which was set to 0.1. The relaxation
x dt x
phase lasted for 100 iterations, which we found sufficient to closely approximate the
numerical backprop gradients. After the relaxation phase was complete, the weights
wereupdatedusingthestandardstochasticgradientdescentoptimizer,withalearning
rate of 0.0005. The weights were initialized as draws from a Gaussian distribution
with a mean of 0 and a variance of 0.05. Hyperparameter values were chosen based
on initial intuition and were not found using a grid-search. The AR algorithm was
appliedtoeachminibatchof64digitssequentially. Thenetworkwastrainedwiththe
mean-squared-errorloss.
Chapter6. CreditAssignmentintheBrain 311
InFigure6.13weshowthatthetrainingandtestperformanceofthenetworktrained
withactivation-relaxationisnearlyidenticaltothatofthenetworktrainedwithback-
propagation, thus demonstrating that our algorithm can correctly perform credit as-
signmentindeepneuralnetworkswithonlylocallearningrules. Wealsoempirically
investigatetheanglebetweentheAR-computedgradientupdatesandthetruebackprop-
agatedupdates. ThegradientangleA wascomputedusingthecosinesimilaritymetric
A(∇ )=cos−1
∇T
θ
∇∗
θ , where ∇ was the AR-computed gradients and ∇∗ were the
θ ||∇ ||||∇∗ θ θ
θ θ
backpropgradients. To handlethefact thatwehadgradient matriceswhilethecosine
similaritymetriconlyappliestovectors,following(Lillicrapetal.,2016),wesimply
flattened the gradient matrices into vectors before performing the computation. We
seethatthe updatescomputedbyAR areveryclose inangletothe backpropupdates
(under 10 degrees), although the angle increases slightly over the course of training.
TheconvergenceintrainingandtestaccuraciesbetweentheARandbackpropshows
that this slight difference in gradient angle is not enough to impede effective credit
assignmentandlearninginAR.InAppendixC,wetakeasteptowardsdemonstrating
thescalabilityofthisalgorithm,byshowingpreliminaryresultsthatindicatethatAR,
includingwiththebiologicallyplausiblesimplificationsintroducedbelow, canscaleto
deeperCNNarchitecturesandmorechallengingclassificationtasks.
6.4.2 Loosening Constraints
While the AR algorithm as above precisely approximates adjoint term ∂L central
∂xl
to backprop, using only local learning rules, it still retains a number of biological
implausibilities. Thecoreimplausibilityistheweighttransportproblem,whichisstill
presentduetotheweighttransposepresentinEquation6.26. Followingourprevious
workonrelaxedpredictivecoding(Chapter3),wedemonstratehowthesameremedies
canbedirectlyappliedtotheARalgorithmwithoutjeopardisinglearningperformance.
Toaddresstheweighttransportproblem,wetakeinspirationsfromtheapproachesof
Chapter6. CreditAssignmentintheBrain 312
feedback alignment (Lillicrapet al., 2016), and (Millidge, Tschantz, Seth, & Buckley,
2020d). Wepostulateanindependentseparatesetofbackwardsweightsψl,sothatthe
updaterulefortheactivationsbecomes,
dxl
=xl−xl+1f(cid:48)(Wlxl)ψl (6.29)
dt
Then, following our work on relaxed preditive coding in Chapter 5, we learn these
backwardsweightswiththefollowingHebbianupdate,
dψl
=xl+1T f(cid:48)(Wlxl)xl (6.30)
dt
Thebackwardsweightswereinitializedasdrawsfroma0mean,0.05varianceGaussian.
InFigure6.14weshowthatstrongperformanceisobtainedwiththelearntbackwards
weights. We found that using random feedback weights without learning (i.e. feedback
alignment), typically converged to a lower accuracy and had a tendency to diverge,
which maybe dueto a simple Gaussian weightinitialization usedhere. Nevertheless,
whenthebackwardsweightsarelearnt,wefindthatthealgorithmisstableandcanobtain
performancecomparablewithusingtheexactweighttransposes(Figure6.14). Thisisa
very strong and encouraging result. First that this learning rule enables performance
with exact weight transposes is impressive, since it implies that the Hebbian update
ruleon the backwardsweights worksand ishighly effective,evenearly onin training.
Secondly, the generalizability of this remedy for weight transport, from predictive
codingnetworks,deepneuralnetworks(Akroutetal.,2019;Amit,2019)withbackprop,
andnowARsuggeststhatthebackwardsweightsmaybeabletobeindependentand
robustlylearnedfromscratchinthebrain,thuslargelyresolvingtheweighttransport
problemaltogether.
WeadditionallyplottheanglebetweentheARwithlearnablebackwardsweightsand
thetrue BPgradients (Figure6.15). The anglestarts outvery large(about70 degrees)
sincethebackwardsweightsarerandomlyinitializedbutthenrapidlydecreasestoabout
30degreesasthebackwardsweightsarelearnt,whichseemsempiricallytobesufficient
toenablestronglearningperformance.
Chapter6. CreditAssignmentintheBrain 313
(a) MNIST backwards (b) MNIST nonlinear deriva- (c)MNISTcombined
weights tives
(d) Fashion backwards (e)Fashionnonlinearderiva- (f)Fashioncombined
weights tives
Figure 6.14: Train and test accuracy and gradient angle (cosine similarity) for AR vs
backpropforMNISTandFashion-MNISTdatasets. Importantly,weseethatevenwith
theadditionalrelaxations,theARtrainedalgorithmperformscomparablytobackprop
Chapter6. CreditAssignmentintheBrain 314
Anadditionalpotentialbiologicalimplausibilitytoaddressisthenonlinearderivative
problem, whichconsists ofthe
f(cid:48)(Wl,x¯l)WlT
term. The biologicalplausibility ofthis
termdependsheavilyupontheactivationfunctionusedinthenetwork. Forinstance,in
a relunetwork, thisterm is trivial, being0 ifthe postsynapticoutput isgreater than0,
and 1 if it is. However, other commonly usedactivation functions like tanh, sigmoid,
and especiallysoftmax are morecomplexand may be challenging to compute locally
inthebrain. Here,weexperiment with simplydroppingthenonlinearderivativeterm
fromtheupdaterule,whichresultsinthefollowingdynamics,
dxl
=xl−xl+1WlT
(6.31)
dt
Although the gradients do no longer match backprop, we show in Figure 6.14 that
learningperformanceagainstthestandardmodelisrelativelyunaffected,showingthat
the influence of the nonlinear derivative is small. We hypothesise that by removing
the nonlinear derivative, we are effectively projecting the backprop update onto the
closestlinearsubspace,whichisstillsufficientlycloseinangletothetruegradientthat
itcansupportlearning. Alternatively,itcouldbethatintheregimeofstandardactivity
values prevailing throughout the network during training, the nonlinear derivatives
generally are close to 1, and thus have little effect on the update rules in any case. If
thisisthecase,thengiventhatwemadenoparticularefforttoconstraintheactivities
ofthenetwork, itsupposesthatthis property, ifitexists, maybehighlybeneficialfor
simplifyingthecomputationsinthebrain.
Byexplicitlyplottingtheangle(Figure6.15),weseethatitalwaysremainsunderabout
30 degrees, sufficient for learning, although the angle appears to rise over the course
of training, potentially due to the gradients becoming smaller and more noisy as the
networkgetsclosertoconvergence.
Moreover,wecancombinethesetwochangesofthealgorithmsuchthatthereisbothno
nonlinearderivative andalsolearnablebackwardsweights. Perhapssurprisingly, when
Chapter6. CreditAssignmentintheBrain 315
(a)Backwardsweightangle (b)Nonlinearderivativesan- (c)Combinedangles
gle
Figure6.15: Angle betweenthe ARand backpropupdates inthe learnablebackwards
weights,nononlinearderivatives,andthecombinedconditions. Atalltimesthisangle
remains under 90 degrees and is steady for all cases apart from the no nonlinear
derivatives cases, where it appears to increase over time. Interestingly, this does not
appear to hinder learning performance noticeably, and may simply reflect the angle
gettingincreasinglyworseasthenetworkconverges.
wedothisweretainequivalentperformancetothefullARalgorithm(seeFigure6.14),
andthereforeavalidapproximationtobackpropinanextremelysimpleandbiologically
plausibleform. Theactivationupdateequationforthefullysimplifiedalgorithmis:
dxl
=xl−xl+1ψ (6.32)
dt
whichrequiresonlylocallyavailableinformationandismathematicallyverysimple. In
effect, each layer is only updated using its own activations and the activations of the
layerabovemappedbackwardsthroughthefeedbackconnections,whicharethemselves
learned through a local and Hebbian learning rule. This rule maintains high training
performance and a close angle between its updates and the true backprop updates
(Figure6.15),andis,atleastintheory,relativelystraightforwardtoimplementinneural
orneuromorphiccircuitry.
Finally, we note that the AR update rules require the nonlinear derivative f(cid:48)(Wlxl)
to be evaluated with the activity xl evaluated at its feedforward pass value xl = x¯l.
Additionally, intheweightupdate,thevalueoftheactivityxl needstobeevaluatedat
Chapter6. CreditAssignmentintheBrain 316
itsfeedforwardpassvalue
∂L = ∂L ∂xl+1(cid:12) (cid:12) =xl+1∗ x¯T ∂f(Wlx¯l) T (6.33)
∂Wl ∂xl+1 ∂Wl (cid:12) xl+1=x¯l+1 ∂x¯
We call this the frozen feedforward pass assumption, and it is very closely related to
the fixed-prediction assumption in predictive coding or, similarly the requirement in
equilibrium-propagation to store all the intermediate equilibrium values of the free
phase. Hereweinvestigatetowhatextentthisassumptioncanalsoberelaxed.
Weevaluatewhetherthenonlinearderivativetermcanbeunfrozensothatitusesthe
currentvalueoftheactivityina.) thefunctionderivative f(cid:48) inEquation6.26,b.) inthe
weightupdateequation(Equation6.28),andc.) weinvestigatewhethertheactivation
valueitselfcanbereplacedintheweightupdateequation.
InFigure6.16,weseethatthefrozenfeedforwardpassassumptioncanberelaxedinthe
caseofthenonlinearderivativesforboththeARupdateandtheweightupdateequation.
However, relaxing it in the case of the weight update equation destroys performance.
Thismeansthatultimately,adirectimplementationofARinbiologicalcircuitrywould
requireneuronstostorethefeedforwardpassvalueoftheirownactivations.
Alltheexperimentssofarhavebeendoneontherelativelysimpleandstraightforward
MNIST dataset with small MLP models. However, it is also important to verify the
scalability of this method. Here we demonstrate that AR can be used to train large
CNNsonchallenging imagerecognitiondatasets(SVHN,CIFAR10,andCIFAR100)
and,moreover,thatthepreviouslooseningsofthebiologicallyimplausibleconstraints
on the algorithm still do not appear to degrade performance unduly. This extension
to CNNs is especially important because other biologically plausible schemes such
asfeedbackalignment(Lillicrapetal.,2014,2016),anddirectedfeedbackalignment
(Nøkland,2016),havebeenshowntostrugglewiththeCNN116architectures(Launay
etal.,2019). Wetestedthesimplifications(droppingnonlinearityorlearningbackwards
weights)onjusttheconvolutionallayersofthenetwork,justthefully-connectedlayers
of the network, or both together. We found that ultimately performance was largely
Chapter6. CreditAssignmentintheBrain 317
(a) MNIST nonlinear derivative relax-(b)MNISTnonlinearderivativeweight
ationupdate update
(c)MNISTbothnonlinearderivative (d)MNISTcurrentxweightupdate
Figure6.16: Assessingwhetherthefrozenfeedforwardpassassumptioncanberelaxed.
We show the resulting performance (test accuracy) against baseline of relaxing this
assumptionontheMNISTdataset. Allresultsaveragedover10seeds. Theseresults
showclearlythatthefrozenfeedforwardpassassumptioncanberelaxedforthenonlinear
derivativeandintheweightupdatenonlinearderivative,butnotbothnonlinearderivatives
simultaneously,anddefinitelynotusingthexT
termintheweightupdate
maintainedevenwhenbothconvolutionalandfullyconnectedlayersinthenetworkused
learnablebackwardsweightsorhadtheirnonlinearderivativesdroppedfromboththe
updateand weightequations. Theseresults speakto thescalability andgeneralisability
oftheserelaxations,andthegeneralrobustnessoftheARalgorithm. Weimplemented
thelearnablebackwardsweightsoftheCNNbyapplyingEquation6.28totheflattened
formoftheCNNfilterkernelweights.
OurCNNconsistedofaconvolutionallayerfollowedbyamax-poolinglayer,followed
byanadditionalconvolutionallayers,thentwofullyconnectedlayers. Theconvolutional
layershad32and64filtersrespectively,whiletheFClayershad64,120,and10neurons
Chapter6. CreditAssignmentintheBrain 318
respectively. For CIFAR100 there are 100 output classes so the final layer had 100
neurons. Thelabelswereone-hot-encodedandfedtothenetwork. Allinputimageswere
normalizedsothattheirpixelvalueslayintherange[0,1]butnootherpreprocessing
wasundertaken. Weused hyperbolic tangent activationsfunctionsatevery layerexcept
thefinallayerwhichwaslinear. Thenetworkwastrainedonamean-square-errorloss
function.
(a) Conv backwards (b) FC backwards (c) Both backwards
weights weights weights
(d) Conv no nonlinear (e) FC no nonlinear (f) Both no nonlinear
derivative derivative derivative
WeseethatthesimplificationsalsoscaletotheCNNfortheSVHNdataset,although,
interestingly, performance is degraded on this dataset when both convolutional and
FC nonlinearities are dropped. However, since this does not occur in the other, more
challenging,CIFARdatasets,wetakethisresulttobeananomaly.
6.4.3 Interim Discussion
InsumtheARalgorithmusesonlysimplelearningrulestoasymptoticallyapproximate
the adjoint terms of backprop over the course of multiple dynamical iterations. We
have demonstrated that the AR algorithm can apply to arbitrary computation graphs,
as can predictive coding, and can be used to train deep CNN models on challenging
Chapter6. CreditAssignmentintheBrain 319
(a)Convbackwardsweights (b)FCbackwardsweights (c)Bothbackwardsweights
(d)Convnononlinearderiva- (e) FC no nonlinear deriva- (f) Both no nonlinear deriva-
tive tive tive
Figure6.18: Performance(testaccuracy),averagedover10seeds,onCIFAR10demon-
stratingthescalabilityofthelearnablebackwardsweightsanddroppingthenonlinear
derivatives in a CNN architecture, compared to baseline AR without simplifications.
Performanceisequivalentthroughout.
object recognition tasks with a performance equivalent to backprop. Moreover, AR
eschewsmuchofthecomplexityofcompetingschemessuchaspredictivecoding,by
not requiring two separate populations of value and error neurons, and equilibrium
propagationbynotneedingtwoseparatebackwardsphases–afreephaseandaclamped
phase. Additionally, we have demonstrated that some of the remaining biological
implausibilities of AR, such as the weight transport and backwards nonlinearities
problems, can be successfully ameliorated through the right extensions to the algorithm
suchaslearnablebackwardsweightswithminimaleffectonoverallperformance. Other
constraints,suchasthenecessitytousethefeedforwardpassactivitiesinthedynamics
instead of the current activities cannot be relaxed without catastrophically damaging
overallperformance.
Chapter6. CreditAssignmentintheBrain 320
(a) Conv backwards (b) FC backwards (c) Both backwards
weights weights weights
(d) Conv no nonlinear (e) FC no nonlinear (f) Both no nonlinear
derivative derivative derivative
Like predictive coding, a limitation of this method is its intrinsically iterative nature.
Thisiterationschememeansthatitisatleastseveraltimesmorecostlythanstandard
backpropagationoferror–forinstance,intheseexperimentsweused100iterationsto
reachexactconvergencetothebackpropagatedgradients,althoughthisisnotstrictly
necessary for good performance. While some of this computational cost may be
amelioratedbytheintrinsicparallelismofneuralcircuitry,thereisneverthelessatiming
issueifconvergenceisrequiredtobecompletebeforethenextsensorydatumarrives,
and issues of gradient interference if it is not. A speculative solution to this could be
synchronizationmediatedby thebrain’s alpha/betaorgammaband frequenciesinthe
cortex,whereone dynamical phaseiteratingtoconvergence wouldcorrespondtoone
wavelengthof the band. Such an identification ishighly speculativehowever,and the
computationalfunctionofsuchoscillationsinthebrainarestilllargelymysteriousand
highlycontroversial(Buzsaki,2006).
One additionaland importantdrawback of theAR algorithmis that itrequires keeping
a memory of the feedforward pass activations throughout the backwards dynamical
phase. Thisisbecausetheactivationsswaptheirpurposefromrepresentingfeedforward
Chapter6. CreditAssignmentintheBrain 321
pass values in the forward phase, to representing gradients in the backwards phase.
Takenliterally,thisrequiresthatthelearningrulesbecomenon-localintime,although
in practice the feedforward pass activations are just stored. While this is the most
substantial drawback to the biological plausibility of the algorithm, it is important
to note that this drawback is also shared with every other iterative algorithm in the
literature. Predictivecodingsimilarlyrequiresthefixed-predictionassumption,which
iseffectivelythesame,andequilibrium-propagationrequiresthatalltheactivationsat
the equilibrium of the free phase are stored while the clamped phase converges. We
suggestthatthisnecessityforstorageiniterativealgorithmstoapproximatebackpropis
universalanditarisesforasimplereason. Namely,thatthebackpropagatedgradients
themselvesfundamentallyonlydependuponthefeedforwardpassvalues,sinceweare
backpropagatingonlyonthefeedforwardpassandnotthroughthedynamicsthemselves.
Since the gradients ultimately depend only on the feedforward pass, any dynamical
scheme to approximate them must ‘remember’ what these values are, somehow. In
theory, it may be possible to design algorithms such that this memory is implicit and
thusnotexplicitlynecessary,butnosuchalgorithmshave,tomyknowledge,beenfound
sofar.
This issue of memory is also closely related to the core computational properties of
reverse-modeAD.Specifically,thatitrequiresstorageinmemoryofallintermediate
activationsintheforwardpass. Thismeansthatthememoryissuealsocloselyappliesto
sequentialbackwardalgorithmssuchastarget-propagationand, indeed,backpropitself.
Thefundamentalfactisthatbackwardspasscanonlytakeplaceafter theforwardpass
iscomplete,andthatitrequiresknowledgeofforwardpassactivities. Thisfactcannot
beignoredorcleverlywishedawaybyanyalgorithmbutmustsimplybeaddressed. If
thebrainisperformingreverse-modeAD,thenitsimplymusthavesomewaytostore,
implicitlyorexplicitly,thefeedforwardpassvalues. Thequestionthenbecomeshow
can this be donein the brain? Some possibilities are multiplexing using the brain’s own
intrinsicrhythms(Buzsaki,2006),orsomekindofspecialparallelclassorneuronsto
Chapter6. CreditAssignmentintheBrain 322
maintainactivityexplicitly(O’Reilly,Braver,Cohen,etal.,1999),orelsestorageatthe
synaptic level through mechanisms like eligibility traces (Bellec et al., 2020). While
wehereremainambiguousonthemeansbywhichsuchstorageisachieved,wehave
reachedapointofconceptualclarityinknowingthattheremustbestorage. Theonly
questionishow.
6.5 Three-Factor Learning Rules and a Direct Implemen-
tation
Afterhavinggonethroughseveraliterativealgorithmsforapproximatingthebackprop-
agationoferroralgorithm,itisworthtakingastepbacktounderstandwhathasbeen
shown and what is truly necessary. Here, we argue that in fact, despite strong claims
thatbackpropisbiologicallyimplausible,in fact anactualimplementationofbackprop
in the brain could be surprisingly simple and biologically plausible. Moreover, that
manyofthealgorithms,includingtheiterativealgorithmspreviously,maysimplybe
complicating matters. The material in this section is speculative and early stage, and
willbeinvestigatedfurtherinfuturework.
First,webeginbystating,somewhatboldly,thatingeneraltheweighttransportproblem
is solved. That is, there is now fairly strong evidence in the literature (Akrout et al.,
2019;Amit,2019;Lillicrapetal.,2016;Millidge,Tschantz,Seth,&Buckley,2020d)
firstly that a precise equality of forward and backwards weights is not necessary for
goodlearningperformance,andsecondly,thatcompetitiveperformancewithbackprop
canbemaintained,evenfordeepnetworks,throughlearnablebackwardsweightswhich
updatewithafairlystraightforwardHebbianlearningrule. Ifweassumethattheweight
transportproblemissolved,thenthereisonlythelocalityissuesremainingwithadirect
implementationofbackprop.
Firstly, we note that if we are implementing reverse-mode AD, and we find a way to
Chapter6. CreditAssignmentintheBrain 323
compute the adjoint ∂L locally at a layer then we actual weight update ∂L = ∂L ∂xl
∂xl ∂Wl ∂xl ∂Wl
requiresonlylocalinformation. Thismeansthatalmosttheentirechallengeofbackprop
iscomputingtheadjointtermlocally.
Secondly, we notice that the standard forward pass of an artificial neural network
xl = f(Wlxl−1) does not actually correspond to how it would work in the brain since
here the activation function (which is typically assumed to be applied through the
thresholdformakinganactionpotentialinthecellsoma)isappliedafter theweights,
while inthe brain thesynaptic weightsare on thedendrites of thepost-synaptic neuron,
and thus occur after the action potential. This means that instead we should use the
morebiologicallyplausibleforwardpassas,
xl =Wlf(xl−1) (6.34)
Wherethe orderof theweights andthe activation functionare switched. Specifically,
thismeansthattheactivationfunctiononlyappliestothepresynapticactivityandnot
the weights. This approach considerably simplifies the requisite gradients, so we get
thefollowingexpressionsfortheadjointandtheweightupdate
∂L = ∂L Wl+1T f(cid:48)(xl)
∂xl ∂xl+1
∂xl ∂L
= f(xl) (6.35)
∂Wl ∂xl
Specifically,theadjointrecursionissimplytheadjointofthelayerabove,multiplied
through the backwards weights and the derivative of the activation function of the
pre-synaptic activity. The weight update is substantially simpler since, because the
forward pass is linear in the weights, it is simply a multiplication of the adjoint with
the presynaptic activity which is essentially Hebbianexcept with the adjointreplacing
the post-synaptic term. Interestingly, this applies that in this case of backprop, the
post-synaptictermshouldhavenoeffectonthesynapticweights,whichisaverystrong
andcounterintuitiveempiricalpredictionofbackprop.
Chapter6. CreditAssignmentintheBrain 324
Whileitmayseemthattheswitchintheorderoftheweightsandtheactivationfunction
mighthavesomeseriousimpactontheexpressivepowerofthenetwork,wearguethat
itlikelydoesnot. Infact,thetwoformulationsareequivalentindeepneuralnetworks
exceptforthe beginningandending layers. To seethis,we cansimplyexplicitlywrite
outtheexpressionforthefunctioncomputedbya4layerneuralnetwork,
y= f(W4f(W3f(W2f(W1x))))≈W4f(W3f(W2f(W1f(x)))) (6.36)
For the vast majority of layers, these expressions are the same up to a re-bracketing.
The only difference is the last layer has no activation function using the more neural
forwardpass(butofteninneuralnetworksweusealinearlastlayeranyway),andthat
the input is first passed to an activation function activation function where this is not
common in standard ANNs – but this function could always be set to the identity to
maketheequivalenceexact. Ingeneral,theeffectofthesedifferenceswillbeminorfor
deepnetworks.
Nowwenotethecrucialpoint,thattheonlybiologicalimplausibilitiesintheselearning
rules is the necessity to have the adjoint value present at the synapse for the weight
updaterule,sinceitisjustthemultiplicationoftheadjointandthepresynapticactivation.
The recursive computation of the adjoint itself (Equation 6.3 is relatively plausible,
since the only difficulty is the nonlinear derivative term f(cid:48)(x) which now is only the
derivative oftheactivation functionappliedtothe pre-synapticinput. Importantly,for
spikingneurons,thisderivativeistrivialasisessentiallyconsistsofaspikewhenthe
neuronfiresandnotwhenitdoesn’t. Assuch,thisruleeffectivelysaysthattheadjoint
should only be updated when there is presynaptic firing. Putting this all together, we
can imagineimplementing thisin thebrain in afairly directforward-backward scheme
asinFigure6.20
Specifically,ifweassumethattheweighttransportproblemcanbesolvedwithinde-
pendently learnable backwards weights, then the recursion for the adjoint becomes
Chapter6. CreditAssignmentintheBrain 325
dL
ϵ L=
dŷ
ϵ
d d W t 4 = d d W L 4 =ϵ y L ̂= f(x W 3 ) 4 f(x 3 W ) 4 L Ψ 4 d d Ψ t 4 =f(x 3 )ϵ L T
x f′( x 3 ) f(x 3 ) dL
x 3 =W 3 f(x 2 ) 3 I I 3 ≈ dx =ϵ LΨ 4 f′( x 3 )
3 3
d d W t 3 = d d W L 3 =I 3 f(x 2 ) W 3 Ψ 3 d d Ψ t 3 =f(x 2 )I 3 T
x 2 =W 2 f(x 1 ) x 2 f(x 2 ) f′( x 2 ) I 2 I 3 ≈ d d x L =I 3 Ψ 3 f′( x 2 )
2
d d W t 2 = d d W L 2 =I 2 f(x 1 ) x
1
W 2 f′( x 1 ) f(x 1 ) Ψ 2 I
1
d I
3
d Ψ t ≈ 2 =
d
d
x
L f
1
(x = 1 )I I 2
2
T Ψ
2
f′( x
1
)
Figure6.20: Potentialschematicforadirectimplementationofbackpropinthebrain. All
thatisnecessaryforthistobeplausibleisthree-factorlearningrules.
simply,
∂L ∂L
= ψT f(cid:48)(xl)
∂xl ∂xl+1
∂L
≈ ψT (6.37)
∂xl+1
where thesecond lineis the recursionif wesimply ignorethe nonlinear derivative term,
aswehavealsofounddoesnothinderlearninginpractice(Millidge,Tschantz,Buckley,
& Seth, 2020; Millidge, Tschantz, Seth, & Buckley, 2020a, 2020d; Ororbia & Mali,
2019). Thisrecursionisextremelysimpleandisjusttheadjointmappedthroughthe
backwardsweightstothe layerbelow. Thus, wecanimaginekeeping separate forward
andbackwardpasseswherebytheadjointisrepresentedbyinterneuronsIl. Here,the
updaterulessimplybecome,
Il =Il+1ψT
∂L
=Ilf(xl) (6.38)
∂Wl
Thesimplicityoftheseupdaterulesimpliesthattheonlypotentialbiologicalimplausi-
bilityis inthe weightupdate 6.28wherewe have transmittedthe value oftheadjoint
Chapter6. CreditAssignmentintheBrain 326
‘interneurons’tothesynapsesofthepost-synapticneuron,andusedthemtoupdatethose
weights. Throughacarefulanalysis,wehaverevealedthisquestiontobetheultimate
cruxofwhetherbackpropinthebrainisplausibleornot. Importantly,thisabilitytouse
theadjointsaspartofa‘local’learningruleiscrucialtoeverypurported‘biologically-
plausible’methodintheliterature–frompredictivecoding,totarget-propagation,to
equilibrium-propandAR.
Whether or not the adjoint can be transmitted to the synaptic weights in the brain is
currentlyacontroversialandunresolvedquestion,andtomyknowledge,hasnotbeen
studied directly. While it may seem fairly obscure, this analysis suggests that this
question is absolutely crucial to our understanding of whether the brain can directly
implement backprop or not. An important possibility is that of segregated dendrites
(Sacramentoetal.,2018). Havingseparatedendriticcompartmentswould,presumably,
straightforwardly enable the broadcast of the adjoint values to the synaptic weights,
sincetheywouldbelocatedinthedendritictreeofthesameneuron. Thekeyquestion
would then become whether information transmitted to the dendrites could remain
segregated. Thatis,couldbothEquation6.34andEquation6.38beimplementedusing
thesameneuron. Ifthisisthecasethenitwouldsuggestanincrediblysimplebiological
implementationforbackpropagation,needingonlyasingletypeofneuronwhichwould
bothsendforwardconnectionsandreciprocallyreceivebackwardsconnectionsfrom
neuronsin thelayerabove. Such asimplearchitecture wouldlendgreat supporttothe
idea that the brain can indeedimplement backprop, andperhaps that backpropagation
issostraightforwardthatitmayevenfunctionasacomputationalprimitiveinneural
circuitry.
6.6 Discussion
Overall,inthischapter,wehaveshownthatpredictivecoding,asanapproximationto
thebackpropagationoferroralgorithm,canbeextendedtoarbitrarycomputationgraphs
Chapter6. CreditAssignmentintheBrain 327
and we have applied predictive coding to large-scale machine learning architectures
suchasCNNsandLSTMsanddemonstratedthattheyperformcomparablytobackprop
trainednetworks. Moreover,wehavepositedanoveliterativealgorithm–Activation
Relaxation– thatalso convergesto theexact backpropgradients, doesnot require two
separatepopulationsofpredictionsandpredictionerrorunits,andusesextremelysimple
andelegantlearningandupdaterules. WehaveshownthatARalsoscalestolarge-scale
CNNneuralnetworkmodelsandiscompetitivewithbackproptrainednetworksatscale.
Finally,takingexperiencefromourpreviousworkinthisfield,wehavere-analyzesdthe
problemofbackpropagationinthebrainfromfirstprinciplesanddiscovered,somewhat
surprisingly, that if we assume that the weight transport problem is solved, the only
majorissueofbiologicalimplausibilityiswhetherrecursivelycomputedadjointscan
be‘transferred’ontosynapsestobeabletoformpartofthesynapticweightupdates. If
they can, and it seems likely that this is possible through a mechanism of segregated
dendritesor,alternatively,backpropagatingactionpotentials(Stuart,Spruston,Sakmann,
&Häusser,1997),thenwecanbefairlycertainthatpropagationinthebrainisatleast
theoretically achievable. This is a remarkable turn-around from the consensus only
fiveyearsagothatitwascompletelybiologicallyimplausible,andspeakstotherapid
developmentandadvancesinthisfield.
Additionally,whilethisthesischapterhaspresentedasubstantialextensiontoanexisting
algorithm(predictivecoding),andanentirelynovelalgorithmforcreditassignmentin
thebrain(activationrelaxation),wealsowishtohighlighttheconceptualcontributions
we have made while thinking about these issues deeply. In my opinion, these are
perhaps the most important sections of thiswork. Namely, firstly, the issue of memory
and time in any implementation or approximation to reverse-mode AD. Specifically,
thatanybiologicallyplausiblealgorithm,whethersequentialoriterative,duetothevery
natureofreverse-modeADmust storethevaluesofthefeedforwardpassthroughout
the backwards sweep or phase, either implicitly or explicitly. While the rationale for
thisseems obviousinretrospect, itwasnot clearbeforehand, andisstill notatall clear
Chapter6. CreditAssignmentintheBrain 328
in the literature. Indeed, the dependence of almost all of these biologically plausible
algorithmsonthememoryofforwardpassvaluesisgenerallyobfuscatedorpresented
asaminorhindrance,wheninfactitisanabsolutelyirrevocablefactofthecomptuation
these algorithms are trying to render biologically plausible. Secondly, and perhaps
most importantly, we have reached the crux of the issue of whether backpropagation
inthebrainisplausible–namelywhetheradjoints,whichmustremainseparatefrom
the post-synapticactivation – canmodulate synapticweight updates. If theycan, then
very biologically plausible and elegant schemes exist for a direct implementation of
backpropagation in the brain (see Equations 6.38, 6.28). If it is not possible, then it
seemslikely,giventhattheadjoint equationisabsolutelyfundamentaltoreverse-mode
AD,thatbackpropagationinthebrainisnotplausibleor,atleast,isexplicitlyachievable
except via some roundabout method. Moreover, if the transport of the adjoint onto
the synaptic weight terminals is possible, then it must be supported by some kind of
dedicatedneurophysiologicalmechanism,whichcanandmustbestudiedindetailifwe
aretounderstandtheexplicitdetailsofthiskeyaspectofcreditassignmentinthebrain.
Nevertheless, I now believe that the key mathematical and conceptual issues in the
questionofwhetherthebraincandobackpropagation(inthisrate-codedstaticmodel)
havebeenlargelyworkedoutanddependnowsolelyondetailsofneurophysiology.
Thecrucialcaveattothisresponseisthatwehaveonlyworkedoutcreditassignmentin
an incredibly simplified model of what occurs in the brain – namely with rate-coded
integrate and fire neurons – on static inputs. Both of these assumptions, however,
arefalseinthebrain. Firstly,neuronsarespikingnetworkswhichmaycommunicate
using precise spike timings to convey information. Understanding how to perform
credit assignment in such spiking networks is still a young and open field, although
there has been much recent progress (Kaiser et al., 2020; Neftci, Mostafa, & Zenke,
2019; Schiess, Urbanczik, & Senn, 2016; Zenke & Ganguli, 2018). Moreover, and
crucially, the key problem the brain faces is not just backpropagation through space
(i.e. layers),butbackpropagationthroughtime. Thebrainmustbeabletoassigncredit
Chapter6. CreditAssignmentintheBrain 329
correctly to temporally distantevents fromthe synaptic weight values that, ultimately
causedthem. While amathematical formulationofreverse-mode ADcanbe directly
formulatedbysimplyperformingbackproponacomputationgraph‘unrolledthrough
time’, inpractice thismeans thatin thebackwards phasethattime mustrun backwards
or,alternativelythatthenetworkmuststorenotonlythefeedforwardpass,butitsentire
history, which isdefinitely biologically implausible. The key questionis thus how to
implement backpropagation through time in a biologically plausible manner. There
hasbeenmuchrecentprogressinthisfield,alsocombinedwithspikingnetworkssuch
as (Bellec et al., 2020; Zenke & Ganguli, 2018). However, the innovative approach
engenderedbyEligibilityPropagation(E-prop)onlyappliestosinglelayerrecurrent
networks,leavingopenthequestionofhowtomarrybackpropagationthroughspace
andbackpropagationthroughtime.
An additionalinteresting considerationis that throughout, andgenerally in thelitera-
ture,onlyreverse-modeADisconsideredtobeacontenderforthecreditassignment
algorithm implemented in the brain. This is due to the historical use and generally
better computational properties of reverse-mode for artificial neural networks (Bay-
din et al., 2017; Griewank et al., 1989) and has thus become the dominant paradigm
(Goodfellow et al., 2016; Rumelhart & Zipser, 1985; Silver et al., 2017). However,
thisisnotnecessarilythecaseinhighly parallel architectures likethebrain,forwhich
additional forward computation cost may be effectively negligible due to the degree
of parallelization. Forward-mode AD can be implemented through dual numbers –
which directly pair activations with their derivatives, and which it is interesting to
think about how this could relate to neural activity and synapses. Moreover, a key
computationaladvantageofforward-modeADisthatitimposesnomemorycost,since
it is entirely online and requires no storage of intermediate activations, thus entirely
obviating the memory issues inherent in implementations of reverse-mode AD. The
disadvantage, however, with forward-mode AD in the brain is that it dislocates the
derivative computation from the physical location of the synapses. The computations
Chapter6. CreditAssignmentintheBrain 330
andderivatives‘moveforwards’upthroughhigherlayersandlevelsofprocessingwhile
thesynapsesremainfirmlyput. itisnecessary,then,wheneverthecomputationsand
derivativesreachtheendoftheprocesstotransmitthefullycomputedderivativesback
to where they originated. Precisely working out this process has, to my knowledge,
not yet been done, but it may result in a practicable algorithm. One important case
where forward-modeAD makes sense is in backpropagation through timesince, as the
computationmovesforwardintime,sodothesynapsesthemselves. Thus,thecorrect
derivativesarealwayslocallyavailablepreciselywhentheyareneeded. Forward-mode
AD through time is known as real-time-recurrent learning (RTRL) (R. J. Williams &
Zipser, 1989a), and is potentially a good algorithm for the brain to solve recurrence,
although it is extremely computationally expensive, rendering it uncompetitive with
reverse-mode BPTT for training large neural networks. Moreover, by taking various
sparseapproximations toRTRL, itis possible toreduce thecomputational costat the
costofsomewhatreducedlearningperformance. Algorithmssuchaseligibility-prop
essentiallytrytomakeRTRLupdatesbiologicallyplausible,withsomesuccess.
6.7 Conclusion
In thischapter, we have proposedtwo novel biologically plausible algorithms for credit
assignment in the brain. Firstly, we demonstrate that predictive coding, under the
fixed prediction assumption, and set-up in a ‘reverse mode’ naturally computed the
gradients requiredfor thebackpropagation of erroralgorithm, as itsdynamics satisfy
the same recursive structure of the adjoint equation and thus, the fixed points of the
predictionerrors,uponconvergence,equalthebackpropagatederrorgradientswhich
can then be used to perform backprop. We have extensively empirically validated
this correspondence and used it to train large-scale and complex machine learning
architectures such as CNNs and LSTMs with performance equal to those trained by
backprop.
Chapter6. CreditAssignmentintheBrain 331
Secondly, wehave utilizedtheinsightsgainedbyour workwithpredictivecodingto
deriveanew,andmuchsimpleralgorithmwhichwecallActivationRelaxation(AR).
Here,insteadofusingseparatepredictionerrorneurons,wesimplyupdatetheactivation
ofthevalueneuronsthemselvestobecomeequaltothebackpropagatederrorsduringthe
backwards iteration phase. While this eschews the fixed feedforward pass assumption
required forpredictive coding, it introducesa similar requirementof storing theinitial
feedforward pass value throughout the backwardsiteration phase, so that they can then
beusedduringtheweightupdates. Wealsoempiricallyvalidatethiscorrespondence
anddemonstratethatARcanbeusedtotrainmachinelearningarchitectureswiththe
sameperformanceasbackpropagation. Importantly,wealsoinvestigatethepotentialfor
applyingthesamebiologicallyplausiblerelaxationstotheARalgorithmasweapplied
to predictive coding in Chapter 3, and show that the relaxations perform just as well
in this new setting – speaking to robustness and generalizability of these relaxations.
Overall,webelievetheARalgorithmissimpler,moreelegant,andmorebiologically
plausible than competing iterative backprop schemes such as predictive coding and
equilibrium-propagation. However,itsuffersfromthelimitationsinherentinalliterative
approaches–thenecessitytosomehowstorethefeedforwardpassvaluesthroughout
the backwards pass. A clear understanding of this limitation, then opens the way
for future work to try to remedy it or propose a different method entirely for solving
backpropagationinthebrain.
Finally, we have included some current (and unpublished) speculations on the po-
tential solution to backpropagation in the brain for simple feedforward networks of
rate-codedintegrateandfireneurons,andhaveconstructedarelativelydirectmethod
ofimplementingbackpropagationwithonlyafewmovingcomponents. Importantly,
thisconstructionreliesheavilyfirstonitsnonstandarddefinitionoftheforwardpass,
using xl+1 =W f(xl) – or the weights after the activation function, rather than inside
of it – which is non-standard for artificial neural networks, but is actually more bio-
logically plausible, and secondly on the solution to the weight transport problem to
Chapter6. CreditAssignmentintheBrain 332
allowforlearnablebackwardweights. Withtheseissuescircumvented,webelievethat
biologicallyplausiblebackpropagationforrate-codedintegrateandfireneuronsactually
turns outto berelatively straightforward. Thekey next movefor future work, now that
this base of understanding is established, is to start to attack the substantially harder
problemofbiologicallyplausibleimplementationsofbackpropagationthroughtime,as
wellaswithspikingneuronmodels.
Overall,inthischapter,webelievethatwehavemadeseveralclearcontributionstowards
understanding the biological plausibility of backpropagation in the brain – firstly by
providingandempiricallyvalidatingtwonewiterativealgorithms(predictivecoding
andactivationrelaxation)andsecondlybycomingtoamuchclearerunderstandingof
whatexactlytheremainingstumblingblockstoabiologicalimplementationare.
Chapter 7
Discussion
The computer scientist and mathematician Richard Hamming in his insightful essay
You and Your Research describes how he would pose the following question to his
colleaguesatBellLabs–‘Whatisthemostimportantquestioninyourfield,andwhy
aren’tyou workingonit?’. Asmight beexpected, thismadehimunpopularwithmany
ofhiscolleagues. However,itspeaksanimportanttruth–theabsoluteandoverriding
importanceofposingandworkingontherightquestions. Animportantquestionthatis
impossible is futile. A tractable but unimportant question is useless. Throughout my
PhD,IhaveendeavouredtoorientbyHamming’smaxim; tofindthemostimportant
yetsolvablequestionwithinmyfieldandtotrytoanswerit. Thisthesis,then,canbe
seenasaconcatenationofthreequestionsofprogressively(inmyopinion)increasing
scopeandimportance.
Thefirstquestionislocaltotheactiveinferencecommunity,butisveryimportantwithin
it. Namely,canactiveinferencebecombinedwithcontemporarydeepreinforcement
learning methods, and thus be scaled to the kind of tasks that can be handled by
contemporary deep reinforcement learning? Conversely, does the theory of active
inferenceitselfcontainanyinsightswhichcanbeusefulformachinelearningtheorists
and practitioners? I believe that through my work (Millidge, 2019a, 2020; Millidge,
333
Chapter7. Discussion 334
Tschantz,Seth,&Buckley,2020b;Tschantz,Millidge,etal.,2020a,2020b)andothers
(Çatal et al., 2020; Fountas et al., 2020; Tschantz, Baltieri, et al., 2020; Ueltzhöffer,
2018), both sides of this question have been definitively answered in the affirmative.
Activeinferencecanbestraightforwardlyscaledupusingartificialneuralnetworksand
thetechniquesofdeepreinforcementlearningwhile,conversely,activeinferencehas
manyinterestingpropertiesandideaswhichcouldbeofusetothedeepreinforcement
learningcommunity. Inthisthesis,wehaveexploredseveraloftheseideasandprimarily
focused on how deep active inference and deep reinforcement learning can be merged.
Nowthatthishas beenanswered,future workshouldfocuson theconverse– how deep
activeinferencediffersfromdeepreinforcementlearningandtheextenttowhichitcan
informandleadtonovelandperformantalgorithmsindeepreinforcementlearning.
Thesecondquestionismorebroadlytargetedtothereinforcementlearningandcognitive
science communities, and concerns the mathematical origins of exploration. This
question is central to a number of related disciplines such as reinforcement learning
(Sutton&Barto,2018),decisiontheory(Dawetal.,2006),controltheory(Kalmanet
al., 1960), andbehavioural economics (Tversky &Kahneman, 1974), whichall share
the same fundamental object of study – adaptive decision-making under uncertainty.
Where there is uncertainty so that the true dynamics of the environment and/or the
value of each possible contingency are not known, then the optimal policy cannot
straightforwardlybecomputed,andagentsarenecessarilyfacedwiththeexploration-
exploitation trade-off. This trade-off arises because new information can generally
onlybeobtainedbytryingnewcoursesofactionorventuringintonewregionsofthe
state-space. However, to explore new regions necessarily has an opportunity cost of
not doing what you thought to be the current best option, which could instead have
beenexploited. Giventhattosucceedatcomplextasks,itisalmostalwaysnecessary
to explore, it is important to figure out how to explore in the most efficient manner.
Specifically,wewishtodesignalgorithmsthatcanacquiretheinformationnecessary
for success as rapidly as possible while incurring the minimum opportunity cost. In
Chapter7. Discussion 335
theliterature, ithas beendiscovered thata verygood heuristicfordoing thisis simply
to optimize a combination of the greedy reward maximization objective exploit with
anadditionalinformationgainexplorationtermexplore(Schmidhuber,2007;Shyam
etal.,2019;Tschantz,Millidge,etal.,2020b). Specifically,therewardmaximization
partoftheobjectiveensuresthatagentsdonotspendlargeamountsoftimeexploring
informativebutbarrenregions,whiletheinformationgaintermshelptheagentnotto
getstuckinlocallygreedy,butgloballypooroptima. Whilethisobjectiveworkswell
inpractice,itsmathematicaloriginandnatureremainsobscure. Intheliterature,this
approachis oftendescribed intuitivelyas simplyadding anadditionalexploratory term
to the loss function. While random entropy-maximizing exploration can be derived
straightforwardly fromvariationalinference approaches toaction (Levine, 2018), the
mathematicalorigin andcommitmentsof specificallyoptimizinginformation-seeking
explorationtermshasremainedmysterious. Thisisthesecondquestionwesetoutto
answer in this thesis – mathematically, from what sort of fundamental objectives do
information-gain exploration terms arise, and how can we characterise the possible
spaceofsuchobjectives?
InChapter5,weanswerthisquestion. Weshowthatinformationgainmaximizingex-
plorationarisesfromminimizingthedivergencebetweentwodistributions–apredicted
or expected distribution over likelystates, given actions, anda desired distribution over
states,whichencodesthe goalsoftheagent. Thisdifferscruciallyfrom evidenceobjec-
tives,whicharetypicallyusedincontrolasinferenceschemes(Levine,2018;K.Rawlik
et al., 2013), which only seek to maximize the likelihood of the desired states, rather
thanexplicitlymatchthetwodistributions. Thisfindinghasimportantimplicationsfor
awiderangeoffields. Specifically,wearguethatanykindofinformation-maximizing
exploratory behaviour can be seen as implicitly aiming for a matching of two distri-
butionsratherthanalikelihoodmaximization. This,forinstance,canexplainseveral
phenomenon,suchastheprobabilitymatchingbehaviourthatisregularlyobservedin
humanparticipantsincognitivescienceandbehaviouraleconomicstasks(Dawetal.,
Chapter7. Discussion 336
2006; Shanks, Tunney, & McCarthy, 2002; Vulkan, 2000; West & Stanovich, 2003),
which are puzzling under the presumption of evidence maximization (Gaissmaier &
Schooler,2008;Tversky&Kahneman,1974). Moreover,byunderstandingtheorigin
ofinformation-seekingbehaviourasemergingdirectlyfromdivergenceobjectives,it
provides us with a greater and deeper understanding of what agents which optimize
these information-seeking terms are actually doing, while the ensuing mathematical
understanding may allow us to manipulate these terms more confidently into more
easilycomputableortractableversionswhichcouldaidimplementationsdirectly.
The third question is one with the greatest scope and importance. This question is
how can credit assignment be implemented in the brain? And, specifically, whether
and how (if it does) the brain can implement the backpropagation of error algorithm.
The solution to such a question would be of great importance to neuroscience, since
itwouldprovideaunifyingviewandmechanistic,algorithmicexplanationofatleast
partofcorticalfunction. Moreover,itwouldexplainatadetailedleveloneofthecore
functionalities of the brain, and the one that underpins almost all adaptive behaviour.
Credit assignment is crucial to any kind of long-range learning of the kind that must
be occurring in the brain. It is crucial for everything from learning the best way to
formandinterpretsensoryrepresentations,toactionselectionoperationsto,potentially
long term memory and complex cognitive processing. While the brain undoubtedly
performsa substantialamountof top-downcontextual feedbackprocessing aswellas
variouskindsofhomeostaticplasticity,whichbothremainpoorlyunderstood,wealso
know from the stunning success of machine learning in the last decade that simple
feedforward passes on large neural networks trained with the backpropagation of
error algorithm can accomplish tasks such as visual object recognition (Child, 2020;
Krizhevskyetal.,2012),generatingrealisticimagesfromtextinputs((Radfordetal.,
2021),human-passablenaturallanguagegeneration(Radfordetal.,2019),andplaying
at a superhuman level games such as Go (Silver et al., 2017), Atari (V. Mnih et al.,
2013,2015;Schrittwieseretal.,2019),andStarcraft(Vinyalsetal.,2019),whichten
Chapter7. Discussion 337
yearsagowerethoughttobeextremelychallenging,ifnotimpossibleforcomputersto
accomplish. Creditassignment,then,mustbeoneofthecoreoperationsinthebrain,
andif wecanunderstandthis thenitispossible wemayobtaina grasponhow known
computational algorithms are implemented in the brain which allows us to grapple
tractablywithitsimmensecomplexity.
WhileI,andthefieldasawhole,havetakenstepstowardsaddressingandansweringthis
question,wearestillalongwayfromaviablesolutionforthebrain. Nevertheless,Ifeel
that,ingeneral,withtheprofusionofalgorithmsaddressingissuessuchastheweight
transport problem (Akrout et al., 2019; Lillicrap et al., 2016; Nøkland, 2016), and
wellasaddressingissuesoflocality(Ororbia&Mali,2019;Scellier&Bengio,2016;
Whittington&Bogacz,2017),thefieldisclosetoagoodsolutionforthecaseofrate-
codedneuronsonatemporallystaticgraph. However,asolutionundertheseconstraints
is fundamentally only an abstraction of a much messier reality, where neurons in the
brainarenotrate-codedbutspiking,andmustalsoachievecreditassignmentnotjust
across space (layers) but across time (Lillicrap et al., 2020). While there are some
approaches which grapple with these additional, and harder problems (Bellec et al.,
2020;Zenke&Ganguli,2018),therearenotmanyandwearefarfromaviableglobal
solution to this problem. I suspect it is into these new domains that future research
shouldprimarilybedirected,andwhereimportantadvanceswillbemade.
7.1 Question 1: Scaling Active Inference
Itturnsouttheactiveinferenceapproachescanbequitestraightforwardlymergedwith
thoseusedindeepreinforcementlearningandcanthusstraightforwardlybe‘scaledup’
toachieveperformancecomparablewiththestateoftheart. Moreover,differentchoices
leaddirectlytodifferentschoolsofmodel-freeormodel-basedreinforcementlearning.
Specifically, active inference fundamentally operates on several core probabilistic
distributionsandobjectives. Thekeydistributionsarethelikelihooddistribution p(o|x),
Chapter7. Discussion 338
andthe transitiondistribution p(x |x ,a ). While standarddiscrete-state-space active
t t−1 t
inference approaches parametrize these explicitly with categorical distributions, and
optimizationofthevariationalfreeenergyexactlyusinganalyticalsolutionsresulting
in a fixed-point iteration algorithm, deep reinforcement learning algorithms instead
amortize these distributions with artificial neural networks, and instead optimize the
variational free energy with respect to the amortised parameters (the weights of the
artificialneuralnetworks). InpureinferencetermsthiscanbeseenasanE-Malgorithm,
witha trivialE-step (amortised inferenceas aforwardpass throughthe networks),and
then an iterative M-step which corresponds to a gradient step of stochastic gradient
descentontheweightsoftheneuralnetworks.
Thenexttranslationistoidentifythevaluefunctionindeepreinforcementlearningwith
thepathintegraloftheexpectedfreeenergyovertimeinactiveinference. Thereforethe
(cid:82)
derivationoftheoptimalpolicyunderactiveinferenceq(π)=σ( dtG(π) )canbeseen
t
asadesignchoiceinactiveinferencetoperformwhatiseffectivelyThompsonsampling
overthesoftmaxedvaluefunctioninreinforcementlearning,achoicewhichisoften,
but not necessarily made in comparable reinforcement learning algorithms (Osband
& Van Roy, 2015). Finally, the sole remaining question remains how to compute
or approximate the path integral of the expected free energy, or the value function.
Whilethe discretestate-space activeinference literature typicallyonly deals with short
timehorizons andsmallstate-spaceswhere thisintegralcan beexhaustivelycomputed
(DaCosta,Parr,etal.,2020),orelsebysimplyenumeratingandpruningunlikelypaths
(Friston, Da Costa, Hafner, et al., 2020), the statespaces and time horizons in deep
reinforcementlearningproblemsaretypically largeenough that thisapproachbecomes
infeasible.
Thesoleremainingquestionremainshowtoapproximatethispathintegral,andherewe
canaugmentactiveinferencewithmethodswellusedinthedeepreinforcementlearning
community. The approach taken by model-free reinforcement learning is to utilize the
Chapter7. Discussion 339
iterativeandrecursivenatureoftheBellmanequationtomaintainandupdateatalltimes
a bootstrapped estimate of the value function (Kaelbling et al., 1996; V. Mnih et al.,
2013). Thisapproachwaspioneeredthroughthetemporal-difference(Sutton,1988),and
Q-learningalgorithms(Watkins&Dayan,1992),andgivesrisetothemodel-freefamily
ofreinforcementlearningalgorithms. Translatingthisintothetermsofactiveinference
is quite straightforward. Since the expected free energy objective can be factorised
into separate independent contributions for each timestep, the path integral satisfies a
similar recursive Bellman-like equation. This equivalence has been recently used to
provethesimilaritiesbetweenactiveinferenceandreinforcementlearning(DaCosta,
Sajid, Parr, Friston, & Smith, 2020). This approach allows us to straightforwardly
define Q-learning and actor-critic like active inference approaches, as was pioneered
in my paper (Millidge, 2019b). One minor distinction is that the computation of the
expectedfreeenergycontainsaninformationgaintermwhichnecessitatesamodelof
thestatesordynamicsoftheworld,whichwouldnotbenecessarywhenusingstandard
reinforcementlearningapproaches,butthisinformationgainyieldssuperiorexploratory
capabilitiesandultimatelyperformance.
While,inMillidge(2020)weexplicitlyincludedactionwithinthegenerativemodelin
the agent, sothat the actionprior p(π)becomes thepath integral of theexpectedfree
energy, and the variational policy posterior q(π) becomes the independently trained
policy, thus recapitulating and providing a variational inference gloss on standard
actor-criticalgorithms,thisisfundamentallyadesignchoice. Ifweinsteadignorethe
policy prior p(π) and define the variational policy posterior q(π) directly in terms of
thevaluefunction,weobtainanalgorithmverysimilartosoft-Q-learningfromdeep
reinforcementlearning(Haarnoja,Zhou,Abbeel,&Levine,2018),exceptitoptimizes
avaluefunctionaloftheexpectedfreeenergyinsteadofthereward.
Conversely,wecantaketheapproachusedindeepreinforcementlearningtoapproxi-
mate the value function at every timestep through samples of model rollouts. This is
Chapter7. Discussion 340
straightforward because the value function is just fundamentally the expected value
oftherewardacrossallpossible trajectoriesunderagivenpolicy. Usingmodel-based
rolloutstoapproximatethisissimplytakingamonte-carloapproximationofanexpecta-
tionwheretherealenvironmentaldynamicsareapproximatedbythemodel’stransition
dynamics. Byusingimportancesamplingonthisobjective,wecanunsurprisinglysee
that the goodness of this approximation depends crucially on the match between the
trueandmodelleddynamics.
Ifweareequippedwithamodelofthetransitiondynamicsoftheworld p(x |x ,a ),
t t−1 t−1
wecanapproximatethepathintegraloftheexpectedfreeenergyovertimeinasimilar
way. Bysimulatingrolloutsthroughthetransitionmodelunderagivenpolicy, andthen
averagingtogetherthepathintegraloftheexpectedfreeenergyacrossrollouts,weform
a monte carlo estimate of the expected free energy value function. This can then be
usedtodirectlycomputetheposteriordistributionoverpolicies,orelsecanbefedinto
aniterativeplanningalgorithmsuchaspathintegralcontrolorCEMwhichcanthenbe
usedtoobtainanactionplan. Usingmodel-predictivecontrol(replanningateverystep),
then allows for the creation of flexible plans for any given situation. Due to utilizing
atransitionmodelandsimulatedrolloutstoestimatethelocalvaluefunction,instead
ofbootstrappingfrompreviousexperience,thismodel-basedapproachissubstantially
moresample efficientthanthe model-freealternative. In theTschantz,Millidge, etal.
(2020b)paper,wetookthisapproachanddemonstratedperformancecomparabletoor
superiortostandardmodel-basedbenchmarks. Additionally,asbefore,theexploratory
propertiesoftheexpectedfreeenergyfunctionalleadtoimprovedperformance.
Given that we thus know that active inference can relatively straightforwardly be
mapped to existing algorithms in deep reinforcement learning, we now turn to the
other face of the question – whether deep reinforcement learning can learn anything
fromactiveinference. Weagainargueintheaffirmative. Namelythatactiveinference,
throughtheexpectedfreeenergyfunctional,providessuperiorexploratorycapabilities
Chapter7. Discussion 341
of active inference agent which, in challenging sparse-reward tasks are necessary to
obtainintelligentbehaviourwhererandomexplorationissimplynotsufficient. While
in reinforcement learning this is a small literature on training agents with additional
exploratory loss functions (Chua et al., 2018; Klyubin et al., 2005; Nagabandi et al.,
2019;Pathaket al.,2017;Shyametal.,2019;Still&Precup,2012), activeinference
provides a mathematically principled and unified way of looking at this, rather than
simplypostulatingad-hocadditionallossfunctions(Oudeyer&Kaplan,2009). More-
over,activeinferencealsoprovidesatheoryofreinforcementlearningdeeplygrounded
in variational inference, and can thus, for instance, be straightforwardly extended to
POMDP models in a way that is nontrivial for standard reinforcement learning algo-
rithms. Finally, by proposing a unified objective of the expected free energy, active
inference allows, in principle, all hyperparameters of the algorithm to be optimized
directlybygradientdescentsagainstthisobjective,thustheoreticallyobviatingtheneed
forexpensivehyperparamtersweepsandtuning.
Althoughthecloseconnectionbetweendeepactiveinferenceanddeepreinforcement
learning is now understood, and we know it is possible to scale up active inference
tothelevelofdeepreinforcementlearning,therestillremainsmuchworkto bedone
actuallyrealizingthisconnectionandconstructingdeepactiveinferenceagents,using
theinsightsof activeinference,which cancompetehead-to-headwiththe stateofthe
artindeepreinforcementlearningand,potentially,exceedit. Ibelievethatespecially
asthefieldmovestowardsfacingmorechallengingenvironmentswithsparserewards,
theexploratorydrivesimplicitly embeddedwithinactiveinferenceagents willbecome
increasinglyimportantandimpactful,sincemanycurrentenvironmentspossessstraight-
forward dense rewards which provide a continuous reward gradient from any initial
condition to a successful final policy. In such environments purely random exploration
sufficesforlearningeffectivepolicies. However,such environmentsarenotingeneral
representative of the kind of environments that face biological organisms in the real
world,andthustomodeltheirbehaviouradditionalexploratoryinstinctsappeartobe
Chapter7. Discussion 342
required.
Furthermore, on a general note, while current work has been focused on trying to
maximize the commonalities between deep active inference and deep reinforcement
learning, as thegoal has been toestablish the connectionand derive proof ofprinciple
scaledupactiveinferencemodels,laterworkshouldgotheotherway,andtrytoretain
the scalability of deep reinforcement learning while maximizing on what is unique
aboutactiveinference.
Oneintriguing possibility inthisdirection istoexperimentwith morecomplexdistribu-
tions of rewards. While active inference can be formulating in a reward maximizing
way,therealobjectactiveinferencehandlesisthebiasedgenerativemodel p˜(o,x),or
thedesiredistribution p˜(o). Whilethiscanbe definedtobeequaltorewardmaximiza-
tion bysimply definingthe desire distributionto be aBoltzmann distribution overthe
realized rewards p˜(o)=exp(−r(o)) (Friston et al., 2012), this is not the only way to
do it. Indeed, more complex and potentially multimodal reward distributions could
be defined and optimized directly in the algorithm. In theory this could lead to more
flexiblebehaviouror,alternatively,beingabletomodelmorecomplex,context-sensitive
orcontingentrewardsnaturallywithintheframework. Therehasbeenverylittlework
doneinthisdirection,anditremainsanexcitingavenueforfuturework.
Anotherinterestingdirectionistoexplicitlymodelthegenerativeprocessesproducing
actionwithinaninferenceframework. Forinstance,search algorithms suchasMonte-
Carlo-Tree-Searchhavebeenvitalinthesuccessinkeyreinforcementlearningtasks
suchasplayingGoandChess(Silveretal.,2017),andcantheoreticallybewrittenin
a probabilistic generative model. Doing this would then allow them to be combined
productivelywithallthestandardtoolsofBayesianandvariationalinferenceandcould
potentially lead to substantially more flexible algorithms for action selection which
would provide extremely powerful inductive biases over standard MLP policy modules
whichwouldallowfastandveryeffectivelearning. Inasimilarvein,continuous-action
Chapter7. Discussion 343
planning algorithms, which are currently rather primitive (such as CEM (Rubinstein,
1997)andpathintegralcontrol(Kappen,2007))andcanonlyexplicitlymodelunimodal
policies,aregenerallyquiteineffective. OkadaandTaniguchi(2020)hasshownhow
many of these algorithms can be directly modelled as part of a generative model and
directlyusedinvariationalinference,andtheyusethisresulttoderivemoreeffective
algorithms such as multimodal CEM (Okada et al., 2020). Further extending this
work may lead to the derivation of highly effective and efficient planning algorithms
for continuouscontrol, whichare currentlysorely lackingand which wouldlead toa
substantialimprovementintheabilitiesofmodel-basedcontinuouscontrol.
Anotherinterestingavenue,whichhasbeguntobeexploredintheliterature(Friston,
DaCosta,Hafner,etal.,2020),whichmayleadtomorenuancedandeffectiveforms
ofexploration,istoexplicitlymodel,inmodelrollouts,thechangeinitsownbeliefs
theagentexpectstoencounter. Ifthisisexplicitlymodelled,thentheagentcandesign
explorationstrategiesspecificallytotesthypothesesandexploredifferentstrategies. In
short,thiskindofmetaself-knowledgeofthelikelychangesofone’sownbeliefsare
vitalforthekindofscientificandexperimentalthinkingthatoftencharacteriseshumans’
phenomenologicalexperienceoftheplanningprocess. Suchagentswouldbeableto
intelligentlyconsiderandcomputethevalueofinformationbothnowandtheexpected
value of information in the future under their expected future beliefs. Basic models
ofthishavebeenexploredusingthediscrete-state-spaceparadigm(Friston,DaCosta,
Hafner, et al., 2020; Hesp et al., 2020), however figuring out how to implement this
within the deep reinforcement learning paradigm in a computationally tractable and
efficient way, as well as to test its performance on tasks which require such nuanced
explorationstrategiesremainsaseriouschallengeandaworthyresearchproject.
Finallytheperspectiveofactiveinference–thatactionandcontrolaremerelyinference
problems over a graphical model which includes action variables – naturally lends
itselftoanunderstandingofdifferentkindsofinference–specificallyamortisedand
Chapter7. Discussion 344
iterative inference (Y. Kim, Wiseman, Miller, Sontag, & Rush, 2018; Marino et al.,
2018; Millidge, Tschantz, Seth, & Buckley, 2020c). Understandinghow these different
typesofactioncanbecombinedandmergedtogether,toinheritthestrengthsofboth
andamelioratetheweaknessesofeachother,isultimatelygoingtobeveryimportant
in designing algorithms which can initially learn rapidly from data and then slowly
converge to a high asymptotic performance. There is also strong, but circumstantial
evidence, that a system like this, which combines iterative and amortised inference,
takes place in the brain. For instance, whenever you start learning a new skill, it
takesalotofthoughtandexplicitmentalplanning,butyoucanlearnquicklywithout
needing an extremely large number of interactions with the environment. However,
as you continue to practice, slowly your skills become habitual. They do not need
mental effort andcan occur almost automatically, allowing you to focus on other things
while they are occurring. This distinction between conscious, effortful action and
unconscious,effortlesshabitispreciselythedistinctionbetweeniterativeandamortised
inference. The benefits of having such a hybrid system are obvious. It is quick to
learn new skills since it can explicitly plan with them, while once a skill has been
practiced many times it can be offloaded onto a computationally cheap habit system.
This eliminates the necessity, in current model-based reinforcement learning systems,
toundertakeexpensivemodel-predictivecontrolandplanningoneverysingletimestep,
even when the system has practiced a given contingency many many times, and also
wherecomputingthebestactionisactuallyextremelystraightforward.
Whilewehaveundertakensomepreliminaryworkinthisdirection,asisreviewedin
thisthesis,reallythecombinationofthetwotodesignsystemswithexplicitplanning
and habit systems is just beginning and there are very many questions which remain
unanswered. Forinstance,whatisthebestwaytotrainthehabitualsystem–shouldit
betrainedtomimicthedecisionsoftheexplicitplanner,orbecompletelyindependently
trained on the reward, or both (i.e. by using the output of the planner as a regulariser
of somekind)? Should thehabit systemand explicitplanner optimizeseparate reward
Chapter7. Discussion 345
functions (for instance, should the planner be more exploratory and the habit system
justoptimizerewards?)? Shouldthehabitsystembeusedbytheplannerinanymanner
– forinstance habitvaluefunctions couldbe used toendstop themodel rollouts ofthe
planner to provide better local value function estimates? Should the habit policy be
used to initialize the planner? How should these systems interact to produce actual
outputactions–shouldtheoutputbethesumofbothsystems? orshouldtherebesome
gatingmechanismwhichselectsoneortheothertoproducetheoutput? Ifthereissuch
agatingmechanism,howdoesitworkandhowshoulditcompute? Howdoweknow
when to turn off the planner and just use the habit system, if ever? The answer to all
of these questions is a fascinating combination of engineering practice and machine
learningtheory,andtheendresultofgettingthesequestionsrightwillbeflexible,robust,
adaptableandsample-efficientsystemswhichalsohaveahighasymptoticperformance
with low latency and computationalcost when thehabit is established. Such systems
could also be used to model similar computations that occur in biological brains and
may shed light into the design choicesavailable for such system and, ultimately, their
neuralimplementations.
7.2 Question 2: The Mathematical Origins of Exploration
InChapter5,wedelveddeeplyintothemathematicaloriginsofexploratorybehaviour.
We saw that to obtain information-seeking exploration as a core part of the objective
functional,inadditiontorewardmaximizationcruciallyentailsminimizingadivergence
objectiveinsteadofanevidenceobjective. Wethenrelatedthisnewdichotomybetween
divergenceandevidenceobjectivestoawiderangeofcurrentlyusedobjectiveswithin
thereinforcementlearningandtheoreticalneurosciencecommunities. Theimportance
ofthisresult,really,liesnotintherelationshiptoexistingmethods,butwhatittellsus
aboutthedeepfoundationofexploration. Putsimply,weseethatextrinsicexploratory
drivesemergefromtryingtomatchratherthanmaximize. Matchingtriestomaintain
Chapter7. Discussion 346
the complexity of the inputs, so that given a complex desire distribution, agents are
driventostabilizeasimilarlycomplexfuture. Conversely,maximizingimplicitlytries
to simplify the inputs, ideally a maximizing agent would collapse all future inputs
to a dirac delta around the future reward. It is only the extent to which there is
uncertainty in the world, or in the reward function, or a lack of controllability in the
world which prevents this full maximization. This difference is what gives rise to
intrinsicexploratorybehaviourinthedivergenceminimizationcase,andtoeffectively
anti-exploratory,information-minimizingbehaviourintheevidence,reward,orutility
maximizingcase. Weadditionallyseethatrewardmaximizingagentsdonothave,and
cannot have, any intrinsic exploratory drives, since the very nature of their objective
compels them to minimize information gain. Information and learning, to them, is a
costwhichmustbeborne,andnotarewardtobepursuedforitsownsake.
Thisadditionalinformationgaintermisveryimportant,becauseitdrivesagentswhich
optimize it to explore and seek out new contingencies, and to find and update upon
resolvable uncertainty in their world. This means that agents which have an expressive
desiredistribution,willtendtolearnfasterandbetterworldmodels,aswellaspursue
more exploratory policies, which in the long run lead to higher performance than
purely reward-maximizingagents, even whenjudged onrewards alone. Thishas been
investigatedbyourselvesinChapter4,aswellasundermanyotherapproachesinthe
literature. Whatismostinterestinghereisthatourunderstandingoftheseobjectivesas
divergence objectives provides a precise mathematical characterisation of what these
objectivesareimplicitlydoing.
It is important to note that it is possible to derive some form of information gain ex-
ploration directly from reward maximization – in the form of explicitly computing
andcalculatingwiththevalueofinformation(Osband,VanRoy,Russo,&Wen,2019;
Schmidhuber,2007;Still&Precup,2012;Tishby&Polani,2011),wherewecanoper-
ationalizethevalueofinformationpurelyintermsofrewardastheadditionalamount
Chapter7. Discussion 347
ofrewardexpectedgivenbetterpoliciesastheresultofobtainingandintegratingthe
informationintoyourworldandpolicymodels. Whilethisvalueofinformationcompu-
tationistheoreticallyoptimalgivenarewardmaximizationobjective,inpracticeitis
intractabletocomputeexactly,andtherehasbeenlittleinvestigationintheliteratureas
to direct approximations of this term. However, there are some heuristic approaches
whichseektoapproximateit,althoughitisnotclearhowwell. Forinstance,Osbandet
al.(2019)arguethatusing‘optimisticvaluefunctions’whichautomaticallyup-weight
unknown contingenciescanlead toa goodapproximation ofthe valueof information,
since in practice, the agent will automatically explore until it has diminished its opti-
misticbiastotheextentthatthebiasforallcontingenciesislowerthanthecurrentbest
option. Thisapproach,underthenamesoftheupper-confidenceboundiswidelyusedin
themulti-armedbanditliterature(Garivier&Moulines,2011)andadditionallyhasbeen
usedtogreateffectinMonte-CarloTreeSearchalgorithms(Kocsis&Szepesvári,2006),
andhasbeeninvestigated tosomedegreewithindeepreinforcementlearning (Silver
et al., 2017). Another approach, known as Thompson sampling (Russo & Van Roy,
2016),explicitlycomputesorapproximatestheposteriordistributionoveractionsgiven
the current history of observations and rewards, and then achieves some degree of
explorationbysamplingfromthisposterior–theideabeingthatin uncertain regions,
theposteriordistributionshouldbefairlyuniform,andthusprovideseffectivelyrandom
exploration,whilewhentheposteriorissharpthenitislikelythatthetrueoptimumhas
beenfoundandthusexplorationisunnecessaryandcostly.
An important challenge is that inference based approaches such as control as inference
do not naturally compute any analogue of the value of information. This is because,
ultimately,theseapproachestakeamean-fieldfactorisationacrosstimeandsplittheir
objective upacrosstime-steps. Thismeansthat informationcanonly flow throughtime
through the transition model, and thus the agent cannot model any kind of learning in
thefuture,whereinformationitmayormaynotdiscoverinthefutureleadsittochange
its model, leading it to perform better (or worse) in the future. Due to this limitation,
Chapter7. Discussion 348
whichisultimatelyappliedforreasonsofcomputationaltractability,approacheslike
controlas inferencedo notcomputeany kindofvalueof informationacross time-steps.
This limitation also applies to the information-seeking methods we discuss which arise
fromdivergencefunctionals. Ifthesefunctionalsarealsomean-fieldfactorized,then
agentsonlyseektomaximizetheinformation-gaininthecurrenttime-step. Extending
thesemethodsbyrelaxingthetemporalmean-fieldassumptionswilllikelyyieldmore
effective and nuanced forms of exploration, which can induce consistent exploratory
behaviour across multiple timesteps and thus handle more complex contingencies.
However, designing effective and mathematically tractable algorithms which can do
thislargelyremainsanavenueforfuturework.
However,thisinformation-maximizingexplorationwithdivergencefunctionalsisnot
doneexplicitlytogainanykindoffuturereward. Instead,agentsoptimizingdivergence
objectivestreatoptimizinginformationgainasanintrinsicgood. Thisiswhatallowsan
informationgain objective toariseeventhoughthe objective satisfiesthesamemean
field assumptions as the control as inference objective. However, this means that to
someextentdivergenceminimizingagentswillcontinuetoexplorebeyondthepoint
whichisstrictlynecessaryforrewardmaximization. Thismeansthat,ineffect,from
theperspectiveofapurelyrewardmaximizingagent,divergenceminimizationisjustan
additionalexploratoryheuristicbywhichtoapproximatethevalueofinformationterms
withinamean-fieldformulation. However,ingeneral,ithasbeenempiricallyfoundthat
theinformationseekingexplorationinamean-fieldfashion,whilenotstrictlythevalue
ofinformation,givesagoodapproximationingeneralandwillleadtogoodperformance
especially in high dimensional, sparse environments. However, because it explicitly
trades off a reward maximizing and an information-seeking objective, it will tend to
over-explorerelativetopurereward-maximizingvalueofinformationcomputation,by
exploringregionswithmuchresolvableuncertaintybutrelativelylittlereward,andwill
continue exploring even when it is likely (but not certain) that the optimal solution
hasbeenfound. However,aslongasalluncertaintyintheenvironmentisresolvable,
Chapter7. Discussion 349
then the divergence objective will eventually converge to the reward maximization
objectivesincetheinformation gaintermwilleventuallybecome negligibleoncethe
agentpossessesaverygoodandaccurateworldmodel.
Aninterestingdirectionforfutureworkwilllieinrelaxingthemeanfieldassumptions
whichcurrentlyunderpinallofthesefunctionals. Whileitislikelythatafullrelaxation
willbeintractable,therearemanyintermediaterelaxationswhichhavebeenproposed
inthegeneralvariationalinferenceliteraturewhichcouldprovehighlyfruitfulwithin
thecontroltask. Forinstance,theBethefreeenergyandrelatedobjectives(Pearl,2014;
Schwöbel, Kiebel, & Markovic´, 2018; Yedidia, Freeman, & Weiss, 2001), allow for
temporalpairwisecorrelationstobeexplicitlyconsidered. Moreover,thereisahighly
generalfamilyof ‘regiongraph’approximations(Yedidia,2011;Yedidia, Freeman,&
Weiss, 2005)whichhavebeendeveloped within thevariationalinferenceliterature,and
whichallowformorecomplexinteractionstobemodelledwithinarelativelytractable
computationalframeworkandwhichhavebeenfoundtoimproveinferenceperformance.
If there is a way to compute successively better Bayesian approximations to things
likethevalueofinformation,ortorelaxthetemporalmean-fieldapproximationsmade
in contemporary divergence and evidence objectives for control, it will likely come
from a thoroughmathematical and experimental investigation ofthese more advanced
and accurate approximation techniques. Understanding how the information gain
functionals fromdivergence objectivesfunction andchange as thetemporal mean-field
approximationisrelaxedisespeciallyinteresting,sincepreliminaryinvestigations,even
withinthemeanfieldparadigm,showthatwhenexpresslywritingouttheobjectivein
termsofentiretrajectories,termssimilartoempowerment(Klyubinetal.,2005)and
filteringinformationgain(backwardsintime)result.
Related to the relaxation of the temporal mean-field approximation, there also needs
to be much work allowing agents to explicitly model changes to their own beliefs in
thefuture. Thisisnecessarytotrulycomputerealisticinformation-seekingobjectives
Chapter7. Discussion 350
whenutilizingmulti-stepplanningalgorithms,sincecurrentlyallinformationgainis
computedwithrespecttotheagent’scurrentbeliefs,whichwillnotnecessarilyholdin
the futureif and when itactually eventuallyreaches this information. Such a process
wouldenableanagenttounderstandhowvariouskindsofinformationwouldchangeits
ownbeliefsandpolicies,andthusbeabletoplanformultiplesequential‘realizations’.
Humanplanningiscertainlycapableofsuchintrospectivecapabilities,wherewecan,
forinstance,decidetoseekoutandinvestigatecertainphenomenoninordertobeable
to understand and better seek out information about another task, and so on. Some
fascinatingrecentworkhasbeguntoexplorethesesortsofmetacognitiveabilitieswithin
theactiveinferenceframework(Friston,DaCosta,Hafner,etal.,2020),butonlywithin
a discrete state space and nonscalable task. The true issue with such approaches will
betheirinherentcomputationaldifficulty,sinceonanaiveapproachtheywillrequire
theagent tosimulate itsown beliefupdates, requiringit tostoreand introspectupon a
copyofitsownmodelsandinferenceprocedures. However,sincesuchmetacognitive
abilitiesarelikelycrucialtoeffectivelongrangeplanning,animportantstrandoffuture
workwillbedesigningapproximationsandobjectiveswhichcanaccomplishthisina
computationallytractablemanner.
Additionally, the divergencevs evidence framework presentsa new class ofobjectives
whichare,intheory,distinctfromtheusualparadigmofrewardorutilitymaximization.
Here, instead we simply seek to minimize divergence to a complex reward or desire
distribution. Intheory,theideaofhavingacomplex,multimodalandpotentiallynon-
scalar desire distributioninstead of a simply scalar reward function to maximize is that
intheoryitallowsformorecomplexnotionsofgoalsordesirestobeimplementedand
optimized by agents. Specifically, it allows for vector-valued, and multimodal goals
which arenot wellhandled withinthe standardreward-maximizationframework,but
whichcanbestraightforwardlyhandledwithinourformulationofadesiredistribution
by both evidence and divergence objectives. While current work has mostly focused
ondemonstratingtheequivalenceofreward-maximization,andthedesiredistribution
Chapter7. Discussion 351
undercertainconditions(thedesiredistributionbeingaBoltzmanndistributionofthe
reward), and thus showing that the probabilistic case is a strict generalization of the
scalardeterministiccase,therealinterestoftheprobabilisticrepresentationisprecisely
howitcandifferfromsimplereward-maximization. Muchworkremainstobedoneto
understandthepossibilitiesformoreflexibleandexpressivegoalorvaluerepresentation
which is unlocked by this more general formalism, and how it can be leveraged to
designartificialsystemswhichperformmorecapablyinpractice.
Finally,theideaofdivergenceminimizationalsohasextremelycloselinkswithMarkov-
Chain-Monte-Carloinferenceprocedures,whereithasrecentlybeenrealizedthatmuch
ofthisparadigmcanbeexpressedwithinasimpleframeworkof stochasticdynamical
systemstheory,wherebyallthevarioussamplerscanbeinterpretedasimplementinga
certainstochasticdifferentialequationwhichexplicitlyperformsagradientdescentona
divergenceobjective(Maetal.,2015),withdifferentalgorithmsintheliteraturesimply
specifyingdifferentnoisetermsandsolenoidalflows(Yuanetal.,2017). Beyondthis,
theideaofdivergenceminimizationcanalsointriguinglybelinkedtorecentadvancesin
stochasticnon-equilibrium thermodynamics(Seifert,2012), whichhas developed ways
to translate classic thermodynamic notions of entropy and entropy production from
propertiesoflargeensemblestopropertiesofindividualstatisticaltrajectories(Esposito
&VandenBroeck,2010). Acrucialresultinthisnewformalismofstochasticdynamics
isthatanysystemwithpositiveentropyproductioncanbeconstruedasminimizinga
divergence between its current state and its ultimate steady state density (Esposito &
Van den Broeck, 2010) – and can thus be interpreted as performing a form of direct
divergence minimization – thus potentially implying that this objective may in some
sense be a more natural one for systems and agents to perform than pure evidence
maximization,andsecondlythatthelawsofthermodynamicsthemselvesmayimplicitly
requireinformation-maximizing behaviourfrom disspativenon-equilibriumsystems.
While these links currently remain speculative, and further investigation will likely
discover greaternuance andrequire some qualificationof theseclaims, in myopinion
Chapter7. Discussion 352
thereissignificantpotentialheretolinkdiscoveriesinstochasticthermodynamicsto
helpusbuildafully general pictureofthenecessarynatureofexploratorybehaviours
insystemsevincingtheclassicaction-perceptionloop.
7.3 Question 3: Credit Assignment in the Brain
By showing that predictive coding can approximate backpropagation along arbitrary
computationgraphs,insteadofjustMLPs,wehavespecificallyturnedpredictivecoding
from a direct model of brain function or perception, into a learning algorithm which
canbeappliedtoarbitraryarchitectures. Thisdualperspective,wherepredictivecoding
is both a generic learning algorithm, as well as specifically a model of learning and
perceptualinferenceinthebrainismostinterestingand,asfarasIamaware,isunique
to predictive coding. Specifically, casting predictive coding as a learning algorithm
makes clear several interesting correspondences between inference procedures and
learning. Specifically,thatwecanderivealearningalgorithmonacomputationalgraph
bytryingtoinferthevaluesofthenodesinthegraph. Predictivecodingadditionally
providesforastraightforwardextensiontobackpropintheformofprecisions,which
allow forthe learnableup ordown weightingof certaingradient signalsdepending on
the intrinsic noise of their generating process. Such a system, while not particularly
usefulinthestandardmachinelearningparadigmofindependent,identicallydistributed
datasets, may prove extremely important for learning with more ecologically valid
sensory streams which contain various amounts of noise and distractor information
whichshouldnotbelearntfrom. Thisperspectiveoflearningandcreditassignmentas
akindofinferencealsoimmediatelylendsitselftofurtherapplicationsandextensions
beyond just precision. For instance, the effect of different generative models other
than Gaussian remain to be determined, as well as potentially different optimization
procedures and variational functionals to be optimized. In general, this perspective
allowsthehighlydevelopedmachineryofinferenceingraphicalmodels(Beal,2003;
Chapter7. Discussion 353
Ghahramani & Beal, 2001; Pearl, 2014; Yedidia, 2011) to be deployed to improve
creditassignment andoptimizationalgorithms. Thisarea,I believe,is anexcitingand
potentiallyhighlyimpactfuloneforfutureworksincetheimpactofimprovingeither
credit assignment or optimization processes, which are at the heart of all of modern
machinelearning,willnecessarilybesubstantial.
Furthermore,bydemonstratingthatmanyofthebiologicalimplausibilitiesinthepre-
dictivecodingschemecanberelaxed(Millidge,Tschantz,Seth,&Buckley,2020d),we,
for the first time, demonstrate a biologically plausible local approximation to backprop
whichdoesnotsufferfromeitherissuesoflocalityorissuesofweighttransport. Fur-
thermore,wedevelopanovelandmuchsimplifiedalgorithm–ActivationRelaxation–
whichpossesses relativelystraightforward, local,and elegantupdaterules whencom-
paredwith predictivecoding whichsucceeds inapproximating thebackpropagationof
erroralgorithmtoarbitrary accuracygivenenoughiterationsteps. Moreover,wehave
shownthatthesamerelaxationswhichworkwithpredictivecoding,alsoworkwiththe
activation relaxation algorithm, thus demonstrating the robustness andefficacy both of
theARalgorithmandofthemethodsofrelaxationutilized. Crucially,ifwelookatthe
final,relaxedARupdaterule(Equation6.32,weseethatthechangeinactivitiesfora
layeronlyrequiresthecurrentactivationvaluesofthelayerabove,mappedthroughthe
backwardsweightswhicharelearntindependentlyoftheforwardweights,besubtracted
fromthecurrentactivationofthelayer. Thisissufficient,overanumberofiterations,
to allow the activations of each layer of the network to converge to the gradients of
backprop. Then,oncethisisachieved,theweightscanbeupdated.
While these algorithms have considerable advantages – they exactly approximate back-
propgivenenoughiterations,theyrequireonlybiologicallyplausiblelocalupdaterules,
andtheyaresimpleandpotentiallystraightforwardtoimplementinneuralcircuitry,they
alsohavesubstantialdisadvantages. Ibelievethesedisadvantagesareworthdiscussing
insomedepthsincetheyprovideaprecisespecificationoftheareasforimprovement
Chapter7. Discussion 354
incurrentalgorithms. Thefundamentaldisadvantageofiterativeschemeslikethisis
their iterative nature. Specifically, they require separatephases of operation – aforward
phasewhichisequivalenttoafeedforwardpassthroughthenetwork,andabackwards
iterativephaseofmultipledynamicaliterations. Asignificantissueisthatitisunclear
whether such phases can realistically exist within the brain. While there is evidence
for different oscillatory frequency bands in the brain (Buzsaki, 2006), and even in
superficialvsdeepcorticallayers(Bastos,Lundqvist,Waite,Kopell,&Miller,2020;
Bastosetal., 2015),itisunclearwhetherthese rhythmsdo,orcan,coordinate separate
feedforwardanditerativephases. Suchascheme,ifimplementedinthebrain,would
formakindofclock,onlyallowingfeedforwardinformationtobeprocessedinshort
burstsinbetweentheiterativephases. Whilenotimpossible,thisseemsatoddswithour
currentunderstandingofthebrainwherefeedforwardandfeedbackinputsarecombined
together in real time. Another very straightforward problem is simply the number of
iterationstheseschemesrequire. Thebraincannotwaitfortensorhundredsofiterations
untilconvergence,evenundergenerousassumptionsaboutrhythmicactivityimplement-
ing the phases, while these schemes often require a substantial amount of iterations
toconvergeto nearlyexactlythe backpropgradients. Whilethis could beameliorated
somewhatwithhighlearningrates,andsettlingforlessthanexactconvergencetothe
backpropgradients, ithasnotyet beenextensively investigatedwhetherthe algorithms
are even stable under such conditions. Understanding and optimizing the number of
iterationsandvariousparameterslikethelearningrateisstillanopenareaofresearch.
Unlike backprop with deep neural networks, where all hyperparameters have been
effectivelyextensivelytunedintheliteratureoveradecadeofexperimentation,good
hyperparametersettingsforthesealternativealgorithmshavebarelybeenexploredat
all,andtheirempiricallimitsofperformanceatscalehavelargelyyettobedetermined.
Afinalseriousdifficultywithsuchapproachesisthattomatchbackproptheyrequire
some level of nonlocality in time, where information from the feedforward pass is
storedandthenutilizedthroughoutorattheendofthebackwardspass. Thisstorageof
Chapter7. Discussion 355
information is fundamentally necessary because the backprop gradients depend only
on the state of the network in the feedforward pass. If the state changes due to the
iterative algorithm, then the old information from the forward pass must be stored
somehow to maintain convergence to backprop on the forward pass. This manifests
itselfinpredictivecodingasthefixed-predictionassumption,whichexplicitlyassumes
thatthe values ofthe forwardpass arestored. In theAR algorithm, it manifests inthe
necessitytostoretheoriginalvalueoftheactivityneuronstousetoupdatetheweights
afterthebackwardspassiscomplete. Duetothisstorage,theactualupdateequations
becomenonlocalintime. Thisshortcomingcouldpotentiallybeaddressedintwoways.
Firstly,it mightbepossibletostorethe information fromtheforwardpass,for instance
inlocalrecurrentunitswhichareinsulatedfromtheactivitychangesinthedynamical
iterations, or secondly, if the number of iterations is short enough, such information
could be persisted simply through multi-step recurrent connectivity. Nevertheless, even
ifthiscouldbedone,thecircuitrytoalignandensurethecorrecttimeofarrivalofall
thenecessarysignalscouldbequitecomplex.
In this field, it is important to reflect deeply upon the role and utility of simplified
models of neural dynamics. Almost all work in this area operates with very simple
modelsofrate-codedintegrateandfireneurons,typicallyofteninatemporallystatic
‘instantaneous’ computationgraph. Biological plausibility within this model, whilea
somewhat vagueconcept, isoftendefinedby conditionssuchasonlylocalconnectivity,
or that connectivity must be additionally Hebbian, and that neural connectivity cannot
be too precise, and that information cannot be directly transmitted backwards along
axons. However,thisdefinitionnecessarilyignoressomeimportantaspectsofreality.
Obviously the brain uses spiking neurons and must also achieve credit assignment
through time, but additionally there are even questions about the simplification of
neuralarchitecture. Forinstance,typicallysuchmodelsimplicitlyassumeamultilayer
perceptron(MLP)stylearchitectureasjustastackoffullyconnectedlayers,however
each region ofthe cortex has anintricate 6-layerstructure, and it isnot clear whether
Chapter7. Discussion 356
eachsuch layerinthe cortexshouldbe consideredasa singlelayer oftheMLP,oronly
the cortical region. If the former, then the different properties and neurophysiology
of each cortical layer is not modelled. If the latter, then it is not at all clear to what
extenttheentirecorticalregioncanbemodelledasasimplefullyconnectedlayer. An
additional interesting neurophysiological fact which is rarely explicitly modelled is the
predominance of cortical columns in the visual cortical regions. These columns do
not at all correspond to fully connected layers and it is not yet fully clear what their
computationalrole is. A straightforwardhypothesisis thattheyare thebrain’s wayof
implementingalocalreceptivefieldoperation,likeconvolutioninconvolutionalneural
networks,butwithouttheadvantageofsharedweightsacrossspacewhichiscoretothe
generalizationcapabilitiesoftheCNN(Hawkins&Blakeslee,2007).
Thequestionremains,however,howapplicablearesuchsimplifiedrate-codedmodels
tothefullcomplexityofcreditassignmentinspikingnetworksthroughtime. Thehope,
ideally,isthatbyandbylargeimportantcomputationalprimitivesandalgorithmswhich
havebeendevelopedforbiologicallyplausiblecreditassignmentinrate-codedneural
networks remain functional, or only require minor adaptation, to work in a spiking
context. Thisisastrongpossibility,butitcouldalsobecompletelyfalse,andthatthe
brain implements an algorithm whichrelies heavily on the unique properties ofspiking
networks for its credit assignment capabilities. Ultimately, before wefully understand
the mechanisms of credit assignment in the brain, it will be hard to fully assess the
degree to which work on rate-coded models will generalize. There has been some
preliminarysuccessesthough. Forinstance,thesurrogategradienttechniqueshowsthat
withonlyminortweaking(definingasurrogategradienttoavoidthenondifferentiable
thresholdspikingfunction),backpropthroughtime(BPTT)canbestraightforwardly
used to trainspiking neural networks for complex tasks (Neftci et al., 2019; Zenke &
Ganguli,2018). However,thesemethodscurrentlyutilizethebiologicallyimplausible
BPTTalgorithmanditisuncleartowhatextentbiologicallyplausiblealternativesto
BPTT can be straightforwardly applied in such a manner. To answer such a research
Chapter7. Discussion 357
question wouldbe an importantand timely researchagenda which wouldsubstantially
advanceourunderstandingofthegeneralizabilityofsuchmodels.
Fundamentally,thecorechallengeisthatoftime. Mostmodels(withsomeexceptions
(Bellec et al., 2020; Schiess et al., 2016)) focus only on backpropagation and credit
assignmentthroughspace(i.e. layersofaneuralnetworkarchitecture)andnotthrough
time. However, time is an inextricable component of the computation in the brain.
Fundamentally, perception in the brain is not about handling static i.i.d datasets, but
ratherfilteringonconstantlychangingsensorystreamsofinformation. Moreover,credit
assignmentthroughtimeisinsomesenseasubstantiallyharderproblem,inthatthekey
information necessary at each step disappears, while when backproapgating through
spacetheinformationisalwayspresentsomewhereinthegraph. InthecaseofBPTT,all
theintermediateactivationsateachtimesteparestoredandthenreplayedinbackwards
sequence once the sequence has ended. However, such an acausal solution is clearly
notsuitableforcomputationinthebrain. TherearealternativestoBPTTwhichdonot
requireexplicitlyrepeatingcomputations backwardsthroughtime,butallowfor online
integrationofgradients. ThekeysuchalgorithmistheRTRLalgorithmwhichmaintains
aJacobianofvaluesateachtimestepanditerativelyupdatesthemateverytimestep. In
algorithmicterms,RTRLcorrespondstoforward-modelautomaticdifferentiationwhile
BPTT corresponds to reverse-mode. Unfortunately, however, RTRL is substantially
moreexpensivetoimplementondigitalcomputers,scalingwithO(n4)whereNisthe
numberofparameters, rather thanO(n2T)forBPTT,where Tisthetimehorizon. One
keyadvantageofRTRLhoweveristhatunlikeBPTTitisnotboundbysequencelength.
WhileBPTT mustchoose somepoint tostopand thenbackpropagate allthe gradients
and then truncate gradients from after time T, RTRL can operate on sequences with
indefinite length without issues, although credit is slowly diluted away through time.
However,RTRLisalsolikelynotneurallyplausibleanditisnotclearhowtoimplement
RTRL in systems with multiple recurrent layers interacting without a blowup in the
numberoflearningrulesrequired. Mypersonalhunchisthatextendingcurrentmodels
Chapter7. Discussion 358
of local, biologically plausible credit assignment to spiking neural networks will not
be especially challenging, although they might not be the method that the brain uses.
However, I believethat credit assignment through time isa fundamentally different and
more challenging problem than credit assignment through space, and that ultimately, to
solve theproblemof creditassignmentin thebrain wemustgrapple headon withthe
problemoflocalcreditassignmentthroughtime.
There are already some methods existing in the literature which accomplish this, specif-
ically eligibility-prop (Bellec et al., 2020) proposes eligibility traces to compute the
RTRL algorithm in a local fashion. However the method currently only works for a
single recurrent layer while the brain is deep both in time and in space, and that this
depthrequiresnewalgorithms,orthecombinationofexistingalgorithmsforbackprop
inspacewiththoseforbackpropintime. Itisalsouncleartowhatextentthebraincom-
putes the full credit assignment mandated by RTRL or else some sparse approximation
therein,perhapsaidedbythenaturalsparsitypropertiesofspikingneuralnetworks.
An additional consideration when thinking about credit assignment in the brain is
the question of feedforward vs feedback processing (Kriegeskorte, 2015). While the
BPTTisonlydesignedforbackpropagatingthroughimmediatelyrecurrentfeedforward
neuralnetworks, thebrainactuallycontainsa multitudeoflong-rangerecurrentloops
mediated by top-down feedback connectivity (Felleman & Van Essen, 1991; Grill-
Spector&Malach,2004). Understandingthepropertiesoftheseandwhethertheyare
used for credit assignment, or whether the brain computes credit backwards through
thesetop-downconnections isalso crucialfor understandingthe fullpicture ofcredit
assignment in the brain. An additional, and almost entirely unanswered question is
theroleoflongtermmemoryincreditassignment. Currentmethods,includingBPTT,
typically use some temporal cut-off to stop considering gradient information beyond
some time-horizon, and a key flaw in naive recurrent architectures is that information is
slowlylostovertime(Hochreiter&Schmidhuber,1997;Ollivieretal., 2015). Thekey
Chapter7. Discussion 359
innovation inLSTMunits isthat theycontain anexplicitforget/remember gatewhich
allowsformemoriestobestoredpotentiallyindefinitely(Hochreiter&Schmidhuber,
1997). There are additionally various modifications for recurrent RNNs which help
ameliorate this problemas well (Ollivier et al., 2015). Since (we assume) the brain is
equippedwith afairlystandard recurrentarchitecture, thearchitecturalor learning-rule
modificationsthatenableit toavoidthis problemandtosuccessfullystoreand utilize
evenlongtermcredit assignmentremainstobeexploredandisaveryimportantand
fundamentalquestioninunderstandingcreditassignmentandlearninginthebrain. One
hypothesis is that synapses in the brain may store a temporal hierarchy of eligibility
traces which retain information over progressively longer timescales, thus allowing
themtoact oninformationwhichhasbeen accumulatedeven intherelativelydistant
past. However,figuringouttheactualspecificoperationofsuchasystemremainsan
openresearchchallenge.
A further interestingquestion, raisedby therecent successesof instantaneousfeedfor-
wardarchitecturessuchastransformersinprocessingsequentialdatasuchasnatural
languagetextoverrecurrentarchitecturessuchasLSTMsistowhatextentrecurrence
is actually a useful computational primitive for handling sequence data as opposed
to one the brain may be forced into due to its inherent computational constraints and
needforonlineprocessinginsteadofbatchprocessingattheendofasequence,asin
transformers. Akey advantageofattention, asimplementedintransformers (Vaswani
etal.,2017),isthatitenablesarbitrarytime-to-timemodulation,ratherthaninrecurrent
architectures where the immediate past inputs are combined with the present ones to
predictthefuture. Inthisway,transformerscanhandledatainafundamentallyacausal
manner,withaccompanyingcomputationaladvantages. Itremainstobeseenwhether
attention like mechanismscan be implemented ina recurrent way, whether recurrent
architectures can be successfully scaled to the level that transformers have done, or
whethertheyremainonaninferiorscalingcurve(J.Kaplanetal.,2020),andtheprecise
mechanism bywhich similar sequence computationsare implemented in thebrain. The
Chapter7. Discussion 360
key lesson of attention is that recurrence is not the only way to handle intrinsically
sequential inputs. It remains to be seen whether other non-recurrent mechanisms are
implementedinthebrain.
Importantly,althoughthislineofworkhasfocusedondeterminingwhether the back-
propagationoferroralgorithmcanbeimplementedinthebrain,whichisinspiredby
theimpressivesuccessofmodernmachinelearning,whichisbasedonthisalgorithm,it
isnot entirely certainthatthe brain achieves creditassignmentthroughbackpropaga-
tionatall. Thereareseveralalternativemethodswhichareworthdiscussinginsome
detail. Forinstance,itisalsopossiblethatthebrainmaybeimplementingsomemore
advancedkindoflearningalgorithmthanstochasticgradientdescent. Meulemansetal.
(2020)showedthattargetpropagationimplementsanapproximateversionofasecond
ordergradientdescent scheme–knownasGauss-Newtonoptimization. Similarlythere
areotheroptimizationmethods,suchasconjugategradients,orcoordinateascent,or
fixed-point iteration, as well as a variety of probabilistic message passing schemes
whichdonotrequireexplicitgradientstobecomputed(Parretal.,2019;Yedidia,2011).
Moreover, it is possible that the brain, while needing to compute gradients, does not
compute by thebackpropagation of error algorithm. For instance, itis possible to com-
putegradientsbyfinitedifferences,andalthoughthesefinitedifferencesperformstrictly
worsethanautomaticdifferentiationtechniquescomputationally,theyareverysimpleto
implementinpractice. Forinstance,thebraincouldeasilycontaincircuitswhichcould
compute timederivatives of a constantlyvarying temporal stream. Then, once we have
thetime derivatives oftwo variables, itis possibletodirectly computetheir gradientby
dx = dx/ dy . Such a circuit would only need local temporal finite differences as well
dy dt dt
asadivisivefeedbackconnectiontocombinethetwotimederivatives,wellwithinthe
possibilitiesaffordedbyknownneurophysiologicalconstraints. Afurtheroptionwould
bethatthebrainsimplymaynotcomputewithderivativesatall,allthewhileoptimizing
someobjectivefunction. Therearemany‘blackbox’optimizationmethodswhichdo
notrequiregradients oftheobjective. Forinstance, geneticalgorithms(Salimans, Ho,
Chapter7. Discussion 361
Chen, Sidor, & Sutskever, 2017), or other brute-force-esque algorithms may instead
beimplemented,andindeedithasbeenshownthatinsomecasesgeneticalgorithms,
whensufficientlyscaledareabletocompetewithbackproponsomeoptimizationtasks
(Salimans et al., 2017; Such et al., 2017). Another possibility, is that the brain could
learn solely by global rewards broadcast to all neurons, where learning effectively
takes place via the policy gradient theorem (Roelfsema & Ooyen, 2005). There is
some circumstantial neurophysiological evidence in favour of this – specifically the
well known role of dopamine as a spur to synaptic plasticity (Dayan, 2009; Dayan
& Daw, 2008), as well as the fact that many cortical pyramidal cells receive global
dopaminergicinputsfromsubcorticalregions. Therehavealsobeenavarietyofmodels
(Pozzi,Bohté,&Roelfsema,2018;Roelfsema&Ooyen,2005)proposedofthiskind
ofglobalreward-drivenlearning. However,thistypeoflearningsuffersfromasevere
intrinsic flaw that the gradients it estimates for each parameter have extremely high
variance. This is because each neuron is only provided with a global reward signal,
which is ultimately caused by the interaction of an extremely large number of other
neurons. Thus, averaging out all the noise introduced by all the other neurons in the
brain requires a very largestatistical sample, thuseffectivelyleading toextremely high
variance gradients and slow learning, which scales increasingly poorly with network
size. Thisispreciselywhybackpropissuchausefulalgorithm,sinceitprovidesprecise
feedback to each neuron about the loss, thus meaning that the only source of noise
is minibatch noise rather than intrinsic noise due to the activities of other neurons.
Thismeansthatbackpropcomputedgradientshavemuchlowervarianceandcanlead
to much faster learning. Nevertheless, it remains inarguable that pyramidal cells are
generallyinnervatedbydopaminergicinputsandthatdopamine,whichisreleasedwhen
there is a reward prediction error in the basal ganglia (Dayan, 2009; Schultz, 1998;
Schultz, Tremblay, & Hollerman, 1998) can strongly modulate learning. This may
implythatthereareeffectivelytwolearningsystemsontopofoneanotherinthebrain.
Thefirst,potentiallyolder,istheglobalslowdopaminergicsystem,whilethesecond,
Chapter7. Discussion 362
entirelycorticallybasedusesprecisevector-feedbackwithsomebackpropagationlike
algorithm. Theglobalrewardsignals,thencanpotentiallymodulatevariousaspectsof
learningsothatcontingencieswithhighrewardpredictionerrorareespeciallysalient
and may induce larger changes than those without (Daw et al., 2006; Roelfsema &
Ooyen,2005).
Finally,itisalwayspossiblethatwehavebeenmisledbythecontemporarysuccessesof
machinelearning,andthatthecoreformulationwhereweperceivethebrainimplicitly
orexplicitlyoptimizingsomeobjectiveissimplywrong,andthatthe brain cannotbe
describedinsuchawayatall. Whilethisisaverygeneralformulationofperception
andlearning,withsupportfrombothmachinelearningandstatisticsandcontroltheory
inengineering,itneverthelesscouldnotbeaproductiveframeworkinwhichtothink
aboutthefunctionofthebrain. Suchascenariowouldarise,forinstance,ifthebrain
was merely a grab-bag of various heuristics and reflexes, implicitly tuned over the
courseofevolution,withoutanyseriouspotentialforlearning. Whilethismaybetrue
ofthe brainsofsomesimple animals,itis clearlynotfor humansandothercreatures
with complex, learntcognition. Nevertheless,if this is somehowthe case, the question
willthenbecomewhatisaproductivemathematicalframeworkinwhichtothinkabout
what the brain is doing, if not in the language of probabilistic models and objective
functions. It may be that the answer to this question, if it must be posed, is more
interestingand productive than discoveringthat the brainwas simplydoing backprop
allalong. Nevertheless,itisextremelyunclearatpresentwhichofthesepossibilities
is true. My opinion, given the mathematical elegance and generality, as well as the
empirical successes of theobjective function viewpoint, is that it isa valuable and most
likely correct framework for understanding the operation of the brain. However, it is
always worth noting that this is purely a speculative hunch, and there remains little
non-circumstantialevidenceonewayoranother.
Chapter7. Discussion 363
7.4 Closing Thoughts
Thisthesisistitled‘ApplicationsoftheFreeEnergyPrincipleforMachineLearningand
Neuroscience’,andthroughoutthethesiswehaveendeavouredtodemonstratehowto
adaptandextendmethodsfromthefreeenergyprincipleanditsprimaryprocesstheories
– active inference and predictive coding – to make advances in deep reinforcement
learning, and in neuroscientific theories of perception and credit assignment in the
brain. Interestingly, the pattern of progress in this thesis is that, in parallel, between
the neuroscience and the machine learning, is that first we simply try to adapt and
extendthemethodsofthefreeenergyprincipletotesttheircapabilitiesagainstother
existingmethodsfromthecoreliteraturesofthesesubjects,andthenstrivetoshowhow
processtheorieslikeactiveinferenceandpredictivecodingcanextendandadvanceupon
the current state of the art. Then, this process inevitably reveals new and interesting
questions by itself, which are somewhat separate from the free energy principle and
itsprocesstheories. Thesequestionsarefirstlythemathematicaloriginofexploration
(which emerges from considering the nature of the expected free energy objective in
active inference), and secondly credit assignment in the brain, which emerges from
considering how predictive coding relates to backpropgation of error. In the second
set of chapters (5 and 6) we have then tried to address these further questions, using
methods inspired by the free energy principle, but not necessarily directly deriving
from it. We believe that in many ways this reflects the theoretical fertility and utility
of the FEP as an abstract principle, that it allows the postulation and exploration of
deeperquestionsthanwouldbepossiblewithoutit. Indeed,thistheoreticalfertilitymay
be one of the primary means of judging the utility of the FEP since, as we discussed
in Chapter 2, the FEP is technically non-falsifiable and can be considered more of a
mathematicalprinciple,orperspective,ratherthanatheory. Ifthisisthecase,thenthe
workinthisthesisprovidessupporttothecontentionthattheFEPprovidesausefuland
fruitfulperspectivetounderstandingavarietyofquestionsbothmachinelearningand
neuroscience.
Chapter7. Discussion 364
Thephenomenologicalprocessofintellectualunderstandingisaninterestingone. At
first you appear to spend ages groping around in the dark, hitting various unknown
objectsinyourway,butslowlygatheringapictureoftheobstaclesinyourpath. Then,
atlonglast,youeventuallystumbleyourwaytoalight-switchandthewholevistais
revealed. What were once unknown lurking obstacles are transformed into precisely
specified,andtractable,objects,whoserelationtooneanothercanbeseenatfirstglance.
In the light, everything is at once clearer, but also smaller. Questions and dilemmas
which seems huge and irreducibly complex are revealed to be straightforward, even
trivial,inthelightofunderstanding. Somuchsothatitisofteneasytoforget,looking
back,howthesequestionsappearedwhentheanswerwasunknown. Fromapersonal
perspective, and one I have hoped to share in this thesis, I believe I have managed
to obtain and present a clear understanding of two topics which were not clear in the
literaturebeforemyPhD– whetheractive inference canbesuccessfullyscaledupto
competewithmodernreinforcementlearningmethods(andtherelationshipsbetween
thetwotheories),andthemathematicaloriginsoftheexplorationtermintheexpected
freeenergy,anditsrelationshiptomorestandardfunctionalssuchasthevariationalfree
energyforperception. Ihaveadditionallycontributedtothetheoryandimplementation
ofpredictivecodingmodels,aswellasalgorithmsforbiologicallyplausiblebackpropin
thebrain. Ifeel, however,that whileIhaveuncoveredcertain key aspectsandfacets of
theproblemofcreditassignmentinthebrain,whichwerepreviouslyshrouded,Ihave
notyetreachedthepointwhereallmysteryfallsawayandthelightpoursin. Similarly,
Ihopethattheworkinthisthesishashelpedyou,thereader,understandsomethingsat
leastalittlemoreclearly.
To conclude, we offer some ideas of for the future development of the ideas worked
out in this thesis. We believe it is clear, we have shown, that active inference can be
productively related and merged with the large and extremely powerful set of deep
reinforcementlearningagentstoenablegeneralandflexiblelearningbasedalgorithms
which can succeed in challenging tasks, even those requiring a considerably degree
Chapter7. Discussion 365
ofexploration. Furtherprogressinthisfield,fromtheperspectiveofactive inference,
shouldmovebeyondmerelyscalingup,andinsteadfocusontheuniqueinsightsand
ideas that active inference brings to the table. Examples of this include its distinct
and exploratory objective functionals such as the Expected Free Energy, or the free
energy of the Expected Future. Another potentially interesting avenue lies in active
inference’s more flexible considerationof reward as aspecific prior distribution, rather
than a scalar value. This could enable more flexible behaviour through better reward
speicfication, and include the ability to learn flexible reward distributions on the fly,
effectivelyperformingrewardshapinginasemi-autonomousmanner. Afinalavenuefor
explorationistherefinementofthespecificgenerativemodelsusedincontrolagents. A
keytenetofactiveinferenceistheideaofderivingpowerfulbehavioursfrominference
on detailed and flexible generative models of the dynamics of the world. While in
thisthesis,wehavefocusedprimarilyonsimplyapproximatingandparametrizingthe
distributionsinthegenerativemodelwithdeepneuralnetworks,amorerefinedapproach
mightbetoinvestigatewhethertheworldcanbefactorizedinatractablemanner,and
design generative models to explicitly exploit these factorizations to allow for more
tractable and efficient inference – effectively giving the agent just the right inductive
biasesabouttheworld tosubserveitscontrolobjectives. Secondly,for thequestionof
creditassignmentinthebrain,Ibelievethatthekeyadvanceswillcomeinunderstanding
thetemporal componentofcredit assignment, andthis requiresdeeplyunderstanding
thefundamentalcomputationsofthebrainasperformingdynamicalalgorithmssuch
as filtering and smoothing rather than merely static Bayesian inference. This line of
research would focus attention both on how the brain can achieve backpropagation
throughtimeaswellasthroughspace. Therearealsoextremelyinterestinglinkshere
with predictive coding, where dynamical approaches using generalized coordinates
remainunderexplored,anditmaybethatbyfurtherdevelopingtheexplicitlyBayesian
approachestofilteringprovidedbythedynamicalformulationsofpredictivecoding,we
canbettercharacteriseandunderstandthetemporalnatureofthebrain’scomputations.
Chapter7. Discussion 366
The work in this thesis relating predictive coding and Kalman filtering is only the
beginning,thereneedstobemuchworkdonescalingandpreciselycharacterisingthe
performanceofdynamicalpredictivecodingalgorithms,understandingwhethersuch
recurrent predictive codingnetworks canbe utilized toperform someform of temporal
creditassignment,asstaticpredictivecodingnetworkscan.
Appendix A
Derivation of Kalman Filtering
Equations from Bayes’ Rule
InthisappendixwederivetheKalmanfilteringequationsdirectlyfromBayesrule. The
firststepistoderivetheprojectedcovariance,
E[xˆ xˆT ]=E[(Ax+Bu+ω)(Ax+Bu+ω)T] (A.1)
t+1 t+1
=E[AxxTAT]+E[AxuTBT]+E[AxωT]+E[BuxTAT]+E[BuωT]+E[ωTAT]
x
(A.2)
+E[ωuTBT]+E[ωωT] (A.3)
=AE[xxT]AT +E[ωωT] (A.4)
=AΣ (t)AT +Σ (A.5)
x ω
Step 11 uses the fact that matrices A,B are constant so come out of the expectation
operator, and that it is assumed that covariances between the state, the noise, and the
control–E[xuT],E[xωT],E[uωT]–are0. Step13usesthefactthatE[xxT]=Σ (t)and
x
thatE[ωωT]=Σ .
ω
Nextweoptimizethefollowinglossfunction,derivedfromBayes’ruleabove(equation
367
AppendixA. DerivationofKalmanFilteringEquationsfromBayes’Rule 368
5).
L=−(y−Cµ )TΣ (y−Cµ )+(µ −Aµ −Bu )TΣˆ (µ −Aµ −Bu )
t+1 Z t+1 t+1 t t x t+1 t t
(A.6)
ToobtaintheKalmanestimateforµ wesimplytakederivativesoftheloss,setitto
t+1
zeroandsolveanalytically.
dL
0= [µT [CTRC+Σ ]µ −µT [CTRy−Σ Aµ −Σ Bu ] (A.7)
dµ t+1 x t+1 t+1 x t x t
t+1
−[yTR−µTATΣ −uTBTΣ ]µ (A.8)
t x t x t+1
=2[CTRC+Σ ]µ −2[CTRy+Σ (Aµ +Bu )] (A.9)
x t+1 x t t
µ =[CTRC+Σ−1[CTRy+Σ (Aµ +Bu ] (A.10)
t+1 x x t t
=[Σ−1−Σ−1CT[CΣ CT +R]−1CΣ−1[CTRy+Σ (Aµ +Bu )] (A.11)
x x x x x t t
=[Σ−1−KCΣ−1][CTRy+Σ (Aµ +Bu )] (A.12)
x x x t t
=Aµ +Bu +Σ−1CTRy−KCΣ−1CTRy−KC(Aµ +Bu ) (A.13)
t t x x t t
=µˆ −KCµˆ +[Σ−1CTR−KCΣ−1CTR]y (A.14)
t+1 t+1 x x
=µˆ −KCµˆ +KK−1[Σ−1CTR−KCΣ−1CTR]y (A.15)
t+1 t+1 x x
=µˆ −KCµˆ +K[(CΣ CT +R)C−TΣ [Σ−1CTR]−CΣ CTR]y (A.16)
t+1 t+1 x x x x
=µˆ −KCµˆ +K[(CΣ CT +R−1)R−CΣ CTR]y (A.17)
t+1 t+1 x x
=µˆ −KCµˆ +Ky (A.18)
t+1 t+1
=µˆ +K[y−Cµˆ ] (A.19)
t+1 t+1
WhereK =Σ−1CT[CΣ CT +R]−1 andistheKalmangainandµˆ =Aµ +Bu isthe
x x t+1 t t
projectedmean.
Thefirstfewstepsrearrangethelossfunctionintoaconvenientformandthenderivean
expressionforµ directly. Step 22appliesthe Woodbury matrixinversion lemma to
t+1
the[CTRC+Σ ]−1 term. ThenextsteprewritestheformulaintermsoftheKalmangain
x
matrixKandmultipliesitthrough. Theothermajormanipulationisthemultiplication
ofthelasttermofequation23byKK−1 whichisvalidsinceKK−1 =I.
AppendixA. DerivationofKalmanFilteringEquationsfromBayes’Rule 369
Thisderivesthe optimalposteriormeanas theanalyticalsolutionto theoptimization
problem. Derivingtheoptimalcovarianceisstraightforwardanddoneasfollows,
E[µ µT ]=E[(muˆ +Ky−KCmuˆ )(µˆ +Ky−KCµˆ )T] (A.20)
t+1 t+1 t+1 t+1 t+1 t+1
=E[µˆ µˆ T]−E[µˆ µˆ T]CTKT −KCE[µˆ µˆ T] (A.21)
t+1 t+1 t+1 t+1 t+1 t+1
+KE[yyT]KT +KCE[µˆ µˆ T]CTKT (A.22)
t+1 t+1
=Σ −Σ CTKT −KCΣ +K[R+CΣ CT]KT (A.23)
µ tˆ+1 µ tˆ+1 µ tˆ+1 µ tˆ+1
=Σ −Σ CTKT −KCΣ +Σ CT[CΣ CT +R]−1[R+CΣ CT]KT
µ tˆ+1 µ tˆ+1 µ tˆ+1 µ tˆ+1 µ tˆ+1 µ tˆ+1
(A.24)
=Σ −Σ CTKT −KCΣ +Σ CTKT (A.25)
µ tˆ+1 µ tˆ+1 µ tˆ+1 µ tˆ+1
=Σ −KCΣ (A.26)
µ tˆ+1 µ tˆ+1
=[I−KC]Σ (A.27)
µ tˆ+1
WhichistheKalmanupdateequationfortheoptimalvariance. Thesecondlinefollows
ontheassumptionthatE[xyT]=0. Onequation32thedefinitionoftheKalmangainis
substitutedbackinandthetwoCΣ CT +Rtermscancel.
x
Appendix B
Appendix B: Equations of the LSTM
cell
TheequationsthatspecifythecomputationgraphoftheLSTMcellareasfollows.
v =h ⊕x
1 t t
v =σ(θv )
2 i 1
v =c v
3 t 2
v =σ(θ v )
4 inp 1
v =tanh(θ v )
5 c 1
v =v v
6 4 5
v =v +v
7 3 6
v =σ(θ v )
8 o 1
v =tanh(v )
9 7
v =v v
10 8 9
y=σ(θ v )
y 10
The recipe to convert this computation graph into a predictive coding algorithm is
370
AppendixB. AppendixB:EquationsoftheLSTMcell 371
straightforward. We first rewire the connectivity so that the predictions are set to the
forwardfunctionsoftheirparents. Wethencomputetheerrorsbetweentheverticesand
thepredictions.
vˆ =h ⊕x
1 t t
vˆ =σ(θv )
2 i 1
vˆ =c v
3 t 2
vˆ =σ(θ v )
4 inp 1
vˆ =tanh(θ v )
5 c 1
vˆ =v v
6 4 5
vˆ =v +v
7 3 6
vˆ=σ(θ v )
o 1
vˆ =tanh(v )
9 7
vˆ =v v
10 8 9
vˆ =σ(θ v )
y y 10
ε =v −vˆ
1 1 1
ε =v −vˆ
2 2 2
ε =v −vˆ
3 3 3
ε =v −vˆ
4 4 4
ε =v −vˆ
5 5 5
ε =v −vˆ
6 6 6
ε =v −vˆ
7 7 7
ε =v −vˆ
8 8 8
ε =v −vˆ
9 9 9
ε =v −vˆ
10 10 10
Appendix C
Appendix C: Predictive Coding Under
the Laplace Approximation
In the main derivation in Chapter 3 of the variational free energy F, we used the
assumption that the variational density is a dirac delta function: q(x|o;µ)=δ(x−µ).
However, the majority of derivations, including the original derivations in (Friston,
2005)insteadappliedtheLaplaceapproximationtothevariationaldistributionq. This
approximationdefinesqtobeaGaussiandistributionwithavariancewhichisafunction
of the mean µ: q(x|o;µ)=N(x;µ,σ(µ)). Notationally, it is important to distinguish
between the generative model Σ, and the variational distribution σ. Here we use the
lower-case σ to denote the parameter of the variational distribution. The lower-case
is not meant to imply it is necessarily scalar. As we shall see, the optimal σ will be
becometheinverse-Hessianofthefreeenergyatthemode.
Intuitively,thisisbecausethecurvatureatthemodeofaGaussiandistributiongivesa
goodindicationofthevarianceoftheGaussian,sinceaGaussianwithhighcurvatureat
themode(i.e. themean)willbehighlypeakedandthushaveasmallvariance,whilea
Gaussianwithlowcurvaturewillbebroad,andthushavealargevariance. Whileour
derivationusingadirac-deltaapproximationandthestandardderivationusingaLaplace
372
AppendixC. AppendixC:PredictiveCodingUndertheLaplaceApproximation 373
approximation obviously differ, they ultimately arrive at the same expression for the
variationalfreeenergyF. Thisisbecausebothapproximationseffectivelyremovethe
variationalvariances fromconsiderationand onlyusethe variationalmean inpractice.
Under the Laplace approximation, the variationalcoveriance has an analytical optimal
formandthusdoesnotneedtobeoptimized,andplaysnorealroleintheoptimization
process for the µs either. We chose to present our derivation using dirac-deltas in the
interests of simplicity, since as we shall see the Laplace approximation derivation is
somewhatmoreinvolved.
Tobegin,wereturntothestandardenergyfunctioninmultilayercase, thistimeunder
theassuptiontheLaplaceapproximation.
L
F =∑E (cid:2) lnp(x|x ) (cid:3) +E (cid:2) lnq(x|o;µ) (cid:3)
q(x|o;µ) i i+1 q(x|o;µ)
i=(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Energy Entropy
L
=∑E (cid:2) lnN(x; f(µ ,Σ(µ))) (cid:3) −ln2πσ (C.1)
N(x;µ,σ(µ)) i i+1 i
i=
Where we have used the analytical result that the entropy of a gaussian distribution
H[N]=ln2πσ. Then, we applya Taylorexpansion aroundx =µ to eachelement in
i i
thesum,
F ∝∑
L
E (cid:2) lnp(µ|µ ) (cid:3) +E
(cid:2)∂lnp(x
i
|xi+1)
(x −µ) (cid:3) +E
(cid:2)∂2lnp(x
i
|xi+1)
(x −µ)2(cid:3) −ln2πσ
q i i+1 q
∂x
i i q
∂x2
i i i
i= i i
L ∂2lnp(x|xi+1)
= ∑lnp(µ|µ )+ i σ −ln2πσ (C.2)
i i+1
∂x2
i i
i=0 i
Whereinthesecondline,wehaveusedthefactthatE (cid:2) x −µ (cid:3) =E (cid:2) x]−µ =µ −µ =
q i i q i i i i
0andthatE (cid:2) (x −µ)2]=Σ,whichisthattheexpectedsquaredresidualsimplyisthe
q i i i
variance. Wealsodroptheexpectationaroundthefirstterm,sinceasafunctiononlyof
µandµ ,itisnolongerafunctionofx whichisthevariabletheexpectationisunder.
i+1 i
Wecanthendifferentiatethisexpressionwithrespecttoσ andsolvefor0toobtainthe
i
AppendixC. AppendixC:PredictiveCodingUndertheLaplaceApproximation 374
optimalvariance.
∂F ∂2lnp(x|xi+1)
= i −σ−1
∂σ ∂x2 i
i i
∂F ∂2lnp(x|xi+1) −1
i
=0 =⇒ σ = (C.3)
i
∂σ ∂x2
i i
Giventhisanalyticalresult,thereisnopointoptimizingF withrespecttothevariational
variancesσ,soourobjectivesimplybecomes,
i
L
F = ∑lnp(µ|µ ) (C.4)
i i+1
i=0
whichisexactlythesameresultasobtainedthroughthediracdeltaapproximation.
References
Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., & Ried-
miller, M. (2018). Maximum a posteriori policy optimisation. arXiv preprint
arXiv:1806.06920.
Adams, R. A., Perrinet, L. U., & Friston, K. (2012). Smooth pursuit and visual
occlusion: activeinferenceand oculomotorcontrolinschizophrenia. PloSone,
7(10),e47502.
Aitchison, L., & Lengyel, M. (2017). With or without you: predictive coding and
bayesianinferenceinthebrain. CurrentOpinioninNeurobiology,46,219–227.
Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., & Tweed, D. B. (2019). Deep
learningwithoutweighttransport. InAdvancesinneuralinformationprocessing
systems(pp.974–982).
Amari, S.-I. (1995). Information geometry of the em and em algorithms for neural
networks. Neuralnetworks,8(9),1379–1408.
Amit, Y. (2019). Deep learning with asymmetric connections and hebbian updates.
Frontiersincomputationalneuroscience,13,18.
Andrews,M. (2020). Themathisnottheterritory: Navigatingthefreeenergyprinciple.
Arulampalam,M.S.,Maskell,S.,Gordon,N.,&Clapp,T. (2002). Atutorialonparticle
filtersforonlinenonlinear/non-gaussianbayesiantracking. IEEETransactions
onSignalProcessing,50(2),174–188.
Attias,H. (2003). Planningbyprobabilisticinference. InAistats.
Auksztulewicz, R., & Friston, K. (2016). Repetition suppression and its contextual
375
References 376
determinantsinpredictivecoding. cortex,80,125–140.
Baird,L. (1995). Residualalgorithms: Reinforcementlearningwithfunctionapproxi-
mation. InMachinelearningproceedings1995(pp.30–37). Elsevier.
Baldi,P.,&Sadowski,P. (2016). Atheoryoflocallearning,thelearningchannel,and
theoptimalityofbackpropagation. NeuralNetworks,83,51–74.
Baltieri,M.,&Buckley,C.L. (2017). Anactiveinferenceimplementationofphototaxis.
InArtificiallifeconferenceproceedings14(pp.36–43).
Baltieri,M.,&Buckley,C.L. (2018). Themodularityofactionandperceptionrevisited
usingcontroltheoryandactiveinference. InArtificiallifeconferenceproceedings
(pp.121–128).
Baltieri, M., & Buckley, C. L. (2019). Pid control as a process of active inference
with linear generative models. Entropy, 21(3), 257. Retrieved from https://
www.mdpi.com/1099-4300/21/3/257
Baltieri, M., & Buckley, C. L. (2020). On kalman-bucy filters, linear quadratic
controlandactiveinference. arXivpreprintarXiv:2005.06269. Retrievedfrom
https://arxiv.org/abs/2005.06269
Baltieri, M., Buckley, C. L., & Bruineberg, J. (2020). Predictions in the eye
of the beholder: an active inference account of watt governors. In Artifi-
cial life conference proceedings (pp. 121–129). Retrieved from https://
www.mitpressjournals.org/doi/abs/10.1162/isal_a_00288
Barlow, H. B., et al. (1961). Possible principles underlying the transformation of
sensorymessages. Sensorycommunication,1,217–234.
Bartunov,S.,Santoro,A.,Richards,B.,Marris,L.,Hinton,G.,&Lillicrap,T. (2018).
Assessing the scalability of biologically-motivated deep learning algorithms
and architectures. In Advances in neural information processing systems (pp.
9368–9378).
Bastos, A. M., Lundqvist, M., Waite, A. S., Kopell, N., & Miller, E. K. (2020).
Layerandrhythmspecificityforpredictiverouting. ProceedingsoftheNational
References 377
AcademyofSciences,117(49),31459–31469.
Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K.
(2012). Canonicalmicrocircuitsforpredictivecoding. Neuron,76(4),695–711.
Bastos,A.M.,Vezoli,J.,Bosman,C.A.,Schoffelen,J.-M.,Oostenveld,R.,Dowdall,
J.R.,... Fries,P. (2015). Visualareasexertfeedforwardandfeedbackinfluences
throughdistinctfrequencychannels. Neuron,85(2),390–401.
Baydin,A.G.,Pearlmutter,B.A.,Radul,A.A.,&Siskind,J.M. (2017). Automatic
differentiation in machine learning: a survey. The Journal of Machine Learning
Research,18(1),5595–5637.
Beal,M.J. (2003). Variationalalgorithmsforapproximatebayesianinference(Unpub-
lisheddoctoraldissertation). UCL(UniversityCollegeLondon).
Bear,M.,Connors,B.,&Paradiso,M.A. (2020). Neuroscience: Exploringthebrain.
Jones&BartlettLearning,LLC.
Bellec,G.,Scherr,F.,Subramoney,A.,Hajek,E.,Salaj,D.,Legenstein,R.,&Maass,
W. (2020). Asolutiontothelearningdilemmaforrecurrentnetworksofspiking
neurons. bioRxiv,738385.
Bellman, R. (1952). On the theory of dynamic programming. Proceedings of the
NationalAcademyofSciencesoftheUnitedStatesofAmerica,38(8),716.
Bengio, Y. (2020). Deriving differential target propagation from iterating approximate
inverses. arXivpreprintarXiv:2007.15139.
Bengio,Y.,&Fischer,A. (2015). Earlyinferenceinenergy-basedmodelsapproximates
back-propagation. arXivpreprintarXiv:1510.02777.
Bengio,Y., Mesnard,T.,Fischer,A., Zhang,S.,& Wu, Y. (2017). Stdp-compatibleap-
proximationof backpropagation inanenergy-basedmodel. NeuralComputation,
29(3),555–577.
Berger-Tal,O.,Nathan,J.,Meron,E.,&Saltz,D. (2014). Theexploration-exploitation
dilemma: amultidisciplinaryframework. PloSOne,9(4),e95693.
Betancourt, M. (2017). Aconceptual introduction to hamiltonian montecarlo. arXiv
References 378
preprintarXiv:1701.02434.
Betancourt,M.J. (2013). Generalizingtheno-u-turnsamplertoriemannianmanifolds.
arXivpreprintarXiv:1304.1920.
Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A
reviewforstatisticians. JournaloftheAmericanstatisticalAssociation,112(518),
859–877.
Bostrom,N. (2017). Superintelligence. Dunod.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., &
Zaremba,W. (2016). Openaigym. arXivpreprintarXiv:1606.01540.
Brooks,S.,Gelman,A.,Jones,G.,&Meng,X.-L. (2011). Handbookofmarkovchain
montecarlo. CRCpress.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ...
others (2020). Language models are few-shot learners. arXiv preprint
arXiv:2005.14165.
Bruineberg, J., Dolega, K., Dewhurst, J., & Baltieri, M. (2020). The emperor’s new
markovblankets.
Buckley,C.L.,Kim,C.S.,McGregor,S.,&Seth,A.K. (2017). Thefreeenergyprinci-
pleforactionandperception: Amathematicalreview. JournalofMathematical
Psychology,81,55–79.
Buzsaki,G. (2006). Rhythmsofthebrain. OxfordUniversityPress.
Çatal,O.,Nauta,J.,Verbelen,T.,Simoens,P.,&Dhoedt,B. (2019). Bayesianpolicy
selectionusingactiveinference. arXivpreprintarXiv:1904.08149.
Çatal,O.,Verbelen,T.,Nauta,J.,DeBoom,C.,&Dhoedt,B. (2020). Learningpercep-
tionandplanningwithdeepactiveinference. arXivpreprintarXiv:2001.11841.
Caticha,A. (2015). Thebasicsofinformationgeometry. InAipconferenceproceedings
(Vol.1641,pp.15–26).
Cesa-Bianchi,N.,Gentile,C.,Lugosi,G.,&Neu,G. (2017). Boltzmannexploration
done right. In Advances in neural information processing systems (pp. 6284–
References 379
6293).
Che, T., Lu, Y., Tucker, G., Bhupatiraju, S., Gu, S., Levine, S., & Bengio, Y. (2018).
Combiningmodel-basedandmodel-freerlviamulti-stepcontrolvariates.
Chen,T.,Fox,E.,&Guestrin,C. (2014). Stochasticgradienthamiltonianmontecarlo.
InInternationalconferenceonmachinelearning(pp.1683–1691).
Child,R. (2020). Verydeepvaesgeneralize autoregressivemodelsand canoutperform
themonimages. arXivpreprintarXiv:2011.10650.
Chua, K., Calandra, R., McAllister, R., & Levine, S. (2018). Deep reinforcement
learninginahandfuloftrialsusingprobabilisticdynamicsmodels. InAdvances
inneuralinformationprocessingsystems(pp.4754–4765).
Clark, A. (2013a). Whatever next? predictive brains, situated agents, and
the future of cognitive science. Behavioral and brain sciences, 36(3),
181–204. Retrieved from https://www.cambridge.org/core/journals/
behavioral-and-brain-sciences/article/whatever-next-predictive
-brains-situated-agents-and-the-future-of-cognitive-science/
33542C736E17E3D1D44E8D03BE5F4CD9
Clark,A. (2013b). Whatevernext? predictivebrains,situatedagents,andthe futureof
cognitivescience.,36(3),181–204. doi: 10.1017/S0140525X12000477
Clark, A. (2015). Surfing uncertainty: Prediction, action, and the em-
bodied mind. Oxford University Press. Retrieved from https://
books.google.co.uk/books?hl=en&lr=&id=TnqECgAAQBAJ&oi=fnd&pg=
PP1&dq=andy+clark+surfing+uncertainty&ots=aurm4jE3NO&sig=
KxeHGJ6YJJdN9tKyr6snwDyBBKg&redir_esc=y#v=onepage&q=andy%
20clark%20surfing%20uncertainty&f=false
Cohen,J.D.,McClure,S.M.,&Yu,A.J. (2007). Shouldistayorshouldigo? howthe
humanbrain manages thetrade-off between exploitationand exploration. Philo-
sophical Transactions of the Royal Society B: Biological Sciences, 362(1481),
933–942.
References 380
Conant, R.C.,& RossAshby, W. (1970). Everygoodregulator ofa systemmust bea
modelofthatsystem. Internationaljournalofsystemsscience,1(2),89–97.
Crick, F. (1989). The recent excitement about neural networks. Nature, 337(6203),
129–132.
Cullen, M., Davey, B., Friston, K., & Moran, R. J. (2018). Active inference in
openaigym: Aparadigmforcomputationalinvestigationsintopsychiatricillness.
Biological psychiatry: Cognitive Neuroscience and neuroimaging, 3(9), 809–
818.
DaCosta,L.,Parr,T.,Sajid,N.,Veselic,S.,Neacsu,V.,&Friston,K. (2020). Active
inferenceondiscretestate-spaces: asynthesis. arXivpreprintarXiv:2001.07203.
Da Costa, L., Sajid, N., Parr, T., Friston, K., & Smith, R. (2020). The relationship
betweendynamicprogrammingandactiveinference: Thediscrete,finite-horizon
case. arXivpreprintarXiv:2009.08111. Retrievedfromhttps://arxiv.org/
abs/2009.08111
Daw,N.D.,O’doherty,J.P.,Dayan,P.,Seymour,B.,&Dolan,R.J. (2006). Cortical
substratesforexploratorydecisionsinhumans. Nature,441(7095),876–879.
Dayan, P. (2009). Goal-directed control and its antipodes. Neural Networks, 22(3),
213–219.
Dayan,P.,&Daw,N.D. (2008). Decisiontheory,reinforcementlearning,andthebrain.
Cognitive,Affective,&BehavioralNeuroscience,8(4),429–453.
Dayan, P., & Hinton, G. (1997). Using expectation-maximization for reinforcement
learning. NeuralComputation,9(2),271–278.
Dayan, P., Hinton, G., Neal, R. M., & Zemel, R. S. (1995). The helmholtz machine.
NeuralComputation,7(5),889–904.
DeBoer,P.-T.,Kroese,D.P.,Mannor,S.,&Rubinstein,R.Y. (2005). Atutorialonthe
cross-entropymethod. AnnalsofOperationsResearch,134(1),19–67.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the em algorithm. Journal of the Royal Statistical Society:
References 381
SeriesB(Methodological),39(1),1–22.
de Xivry, J.-J. O., Coppe, S., Blohm, G., & Lefevre, P. (2013). Kalman filtering
naturallyaccountsforvisuallyguidedandpredictivesmoothpursuitdynamics.
JournalofNeuroscience,33(44),17301–17313.
Doucet, A., Godsill, S., & Andrieu, C. (2000). On sequential monte carlo sampling
methodsforbayesianfiltering. StatisticsandComputing,10(3),197–208.
Doya, K. (2000). Reinforcement learning in continuous time and space. Neural
Computation,12(1),219–245.
Esposito,M.,&VandenBroeck,C. (2010). Threefacesofthesecondlaw.i.master
equationformulation. PhysicalReviewE,82(1),011143.
Farshidian,F.,Neunert,M.,&Buchli,J. (2014). Learningofclosed-loopmotioncontrol.
In2014ieee/rsjinternationalconferenceonintelligentrobotsandsystems(pp.
1441–1446).
Feldman,H.,&Friston,K. (2010). Attention,uncertainty,andfree-energy. Frontiersin
humanneuroscience,4,215. Retrievedfromhttps://www.frontiersin.org/
articles/10.3389/fnhum.2010.00215/full
Felleman,D.J., &Van Essen,D.C. (1991). Distributedhierarchicalprocessinginthe
primatecerebralcortex. InCerebcortex.
Feng, S., Whitman, E., Xinjilefu, X., & Atkeson, C. G. (2014). Optimization based
fullbodycontrolfortheatlasrobot. In2014ieee-rasinternationalconferenceon
humanoidrobots(pp.120–127).
Feynman,R. (1998). Statisticalmechanics: asetoflectures(advancedbookclassics).
Fodor,J.A. (1983). Themodularityofmind. MITpress.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active inference
agentsusingmonte-carlomethods. arXivpreprintarXiv:2006.04176.
Fox, C. W., & Roberts, S. J. (2012). A tutorial on variational bayesian inference.
Artificialintelligencereview,38(2),85–95.
Friston, K. (2003). Learning and inference in the brain. Neural Networks, 16(9),
References 382
1325–1352.
Friston,K. (2005). Atheoryofcorticalresponses. PhilosophicalTransactionsofthe
RoyalSocietyB:Biologicalsciences,360(1456),815–836.
Friston,K. (2008a). Hierarchicalmodelsinthe brain. PLoSComputationalBiology,
4(11).
Friston,K. (2008b). Variationalfiltering. NeuroImage,41(3),747–766.
Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in
CognitiveSciences,13(7),293–301.
Friston,K. (2010). Thefree-energyprinciple: aunifiedbraintheory? Naturereviews
neuroscience,11(2),127–138.
Friston, K. (2012). The history of the future of the bayesian brain. NeuroIm-
age, 62(2), 1230–1233. Retrieved from https://www.sciencedirect.com/
science/article/pii/S1053811911011657
Friston,K. (2013). Lifeasweknow it. JournaloftheRoyal SocietyInterface,10(86),
20130475.
Friston, K. (2019a). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
Friston, K. (2019b). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184. Retrieved from https://arxiv.org/pdf/1906.10184
.pdf
Friston, K. (2019c). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
Friston, K. (2019d). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
Friston,K.,&Ao,P. (2012a). Freeenergy,value,andattractors. Computationaland
mathematicalmethodsinmedicine,2012.
Friston,K., &Ao,P. (2012b). Freeenergy, value, andattractors. Computationaland
mathematicalmethodsinmedicine,2012.
References 383
Friston, K., Da Costa, L., Hafner, D., Hesp, C., & Parr, T. (2020). Sophisticated
inference. arXiv preprint arXiv:2006.04120. Retrieved from https://arxiv
.org/abs/2006.04120
Friston,K.,DaCosta,L.,&Parr,T. (2020). Someinterestingobservationsonthefree
energyprinciple. arXivpreprintarXiv:2002.04501.
Friston, K., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active
inference? PloSone,4(7).
Friston,K.,Daunizeau,J.,Kilner,J.,&Kiebel,S.J. (2010). Actionandbehavior: afree-
energyformulation. BiologicalCybernetics,102(3),227–260. Retrievedfrom
https://link.springer.com/article/10.1007/s00422-010-0364-z
Friston, K., Fagerholm, E. D., Zarghami, T. S., Parr, T., Hipólito, I., Magrou, L., &
Razi, A. (2007). Parcels and particles: Markov blankets in the brain. Network
Neuroscience(JustAccepted),1–76.
Friston,K.,Fagerholm,E.D.,Zarghami,T.S.,Parr,T.,Hipólito,I.,Magrou,L.,&Razi,
A. (2020). Parcels and particles: Markov blankets in the brain. arXiv preprint
arXiv:2007.09704. Retrievedfromhttps://arxiv.org/abs/2007.09704
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,&Pezzulo,G. (2017a). Active
inference: aprocesstheory. NeuralComputation,29(1),1–49.
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,&Pezzulo,G. (2017b). Active
inference: aprocesstheory. NeuralComputation,29(1),1–49.
Friston, K., & Frith, C. (2015). A duet for one. Consciousness and Cognition, 36,
390–405.
Friston, K., & Kiebel, S. (2009). Predictive coding under the free-energy princi-
ple. Philosophical Transactions of the Royal Society B: Biological Sciences,
364(1521),1211–1221.
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain.
JournalofPhysiology-Paris,100(1-3),70–87.
Friston, K., Levin, M., Sengupta, B., & Pezzulo, G. (2015). Knowing one’s place: a
References 384
free-energyapproachtopatternregulation. JournaloftheRoyalSocietyInterface,
12(105),20141383. Retrievedfromhttps://royalsocietypublishing.org/
doi/full/10.1098/rsif.2014.1383
Friston,K.,Lin,M.,Frith,C.D.,Pezzulo,G.,Hobson,J.A.,&Ondobaka,S. (2017).
Activeinference,curiosityandinsight. NeuralComputation,29(10),2633–2683.
Friston,K.,Parr,T.,&Zeidman,P. (2018). Bayesianmodelreduction. arXivpreprint
arXiv:1805.07092.
Friston,K.,Rigoli,F.,Ognibene,D.,Mathys,C.,Fitzgerald,T.,&Pezzulo,G. (2015a).
Activeinferenceandepistemicvalue. CognitiveNeuroscience,6(4),187–214.
Friston,K.,Rigoli,F.,Ognibene,D.,Mathys,C.,Fitzgerald,T.,&Pezzulo,G. (2015b).
Activeinferenceandepistemicvalue.,6(4),187–214. doi: 10.1080/17588928
.2015.1020053
Friston,K.,Rosch,R.,Parr,T.,Price,C.,&Bowman,H.(2018a).Deeptemporalmodels
andactiveinference. Neuroscience&BiobehavioralReviews,90,486–501.
Friston, K., Rosch, R., Parr, T., Price, C., & Bowman, H. (2018b). Deep temporal
modelsandactiveinference.,90,486–501. Retrieved2019-11-15,fromhttp://
www.sciencedirect.com/science/article/pii/S0149763418302525 doi:
10.1016/j.neubiorev.2018.04.004
Friston, K., Samothrakis, S., & Montague, R. (2012). Active inference and agency:
optimalcontrolwithoutcostfunctions. BiologicalCybernetics,106(8-9),523–
541.
Friston,K.,Schwartenbeck,P.,FitzGerald,T.,Moutoussis,M.,Behrens,T.,&Dolan,
R. J. (2014). The anatomy of choice: dopamine and decision-making. Philo-
sophical Transactions of the Royal Society B: Biological Sciences, 369(1655),
20130481.
Friston, K., Stephan, K., Li, B., & Daunizeau, J. (2010). Generalised filter-
ing. Mathematical Problems in Engineering, 2010. Retrieved from https://
www.hindawi.com/journals/mpe/2010/621670/
References 385
Friston, K., & Stephan, K. E. (2007). Free-energy and the brain. Synthese, 159(3),
417–458.
Friston, K., Trujillo-Barreto, N., &Daunizeau, J. (2008). Dem: avariational treatment
ofdynamicsystems. Neuroimage,41(3),849–885.
Friston,K.,Wiese,W.,&Hobson,J.A. (2020). Sentienceandtheoriginsofconscious-
ness: Fromcartesiandualitytomarkovianmonism. Entropy,22(5),516.
Fujimoto,S.,vanHoof,H.,&Meger,D. (2018). Addressingfunctionapproximation
errorinactor-criticmethods. arXivpreprintarXiv:1802.09477.
Gaissmaier, W., & Schooler, L. J. (2008). The smart potential behind probability
matching. Cognition,109(3),416–422.
Gal, Y., McAllister, R., & Rasmussen, C. E. (2016). Improving pilco with bayesian
neuralnetworkdynamicsmodels. InData-efficientmachinelearningworkshop,
icml (Vol.4,p.25).
Garivier,A.,&Moulines,E. (2011). Onupper-confidenceboundpoliciesforswitching
banditproblems. InInternationalconferenceonalgorithmiclearningtheory(pp.
174–188).
Gershman,S.J. (2018). Uncertaintyandexploration. bioRxiv,265504.
Gerstner,W.,&Kistler,W.M. (2002). Mathematicalformulationsofhebbianlearning.
BiologicalCybernetics,87(5),404–415.
Geweke,J. (2007). Bayesianmodelcomparisonandvalidation. AmericanEconomic
Review,97(2),60–64.
Ghahramani,Z., &Beal,M.J. (2001). Propagation algorithmsforvariationalbayesian
learning. InAdvancesinneuralinformationprocessingsystems(pp.507–513).
Ghahramani,Z.,Beal,M.J.,etal. (2000). Graphicalmodelsandvariationalmethods.
Advancedmeanfieldmethods-theoryandpractice.MITPress.
Gibson,J.J. (2002). Atheoryofdirectvisualperception. VisionandMind: selected
readingsinthephilosophyofperception,77–90.
Girolami,M., &Calderhead, B. (2011). Riemannmanifoldlangevinandhamiltonian
References 386
montecarlomethods.JournaloftheRoyalStatisticalSociety: SeriesB(Statistical
Methodology),73(2),123–214.
Gold, J. I., & Shadlen, M. N. (2003). The influence of behavioral context on the
representation of a perceptual decision in developing oculomotor commands.
JournalofNeuroscience,23(2),632–651.
Goodfellow,I.,Bengio,Y.,&Courville,A. (2016). Deeplearning. MITpress.
Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,...
Bengio,Y. (2014). Generativeadversarialnets. AdvancesinneuralInformation
ProcessingSystems,27,2672–2680.
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In
Machinelearningproceedings1995(pp.261–268). Elsevier.
Gordon,N.J.,Salmond,D.J.,&Smith,A.F. (1993). Novelapproachtononlinear/non-
gaussian bayesian state estimation. In Ieee proceedings f (radar and signal
processing)(Vol.140,pp.107–113).
Grewal,M.S.,&Andrews,A.P. (2010). Applicationsofkalmanfilteringinaerospace
1960 to the present [historical perspectives]. IEEE Control Systems Magazine,
30(3),69–78.
Griewank,A.,etal. (1989). Onautomaticdifferentiation. MathematicalProgramming:
recentdevelopmentsandapplications,6(6),83–107.
Grill-Spector,K.,&Malach,R. (2004). Thehumanvisualcortex. Annu.Rev.Neurosci.,
27,649–677.
Gu, S., Lillicrap, T., Sutskever, I., & Levine, S. (2016). Continuous deep q-learning
withmodel-basedacceleration. In Internationalconferenceon machinelearning
(pp.2829–2838).
Gupta,S.,Agrawal,A.,Gopalakrishnan,K.,&Narayanan,P. (2015). Deeplearning
withlimitednumericalprecision.InInternationalconferenceonmachinelearning
(pp.1737–1746).
Ha,D.,&Schmidhuber,J. (2018). Worldmodels. arXivpreprintarXiv:1803.10122.
References 387
Haarnoja, T. (2018). Acquiring diverse robot skills via maximum entropy deep rein-
forcementlearning(Unpublisheddoctoraldissertation). UCBerkeley.
Haarnoja,T.,Tang,H.,Abbeel,P.,&Levine,S. (2017). Reinforcementlearningwith
deepenergy-basedpolicies. InProceedingsofthe34thinternationalconference
onmachinelearning-volume70(pp.1352–1361).
Haarnoja,T.,Zhou,A.,Abbeel,P.,&Levine,S. (2018). Softactor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. arXiv
preprintarXiv:1801.01290.
Haarnoja,T.,Zhou,A.,Hartikainen,K.,Tucker,G.,Ha,S.,Tan,J.,... others (2018).
Softactor-criticalgorithmsandapplications. arXivpreprintarXiv:1812.05905.
Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2019). Dream to control: Learning
behaviorsbylatentimagination. arXivpreprintarXiv:1912.01603.
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J.
(2018). Learning latent dynamics for planning from pixels. arXiv preprint
arXiv:1811.04551.
Hafner,D.,Ortega,P.A.,Ba,J.,Parr,T.,Friston,K.,&Heess,N. (2020). Actionand
perceptionasdivergenceminimization. arXivpreprintarXiv:2009.01791.
Harvey,A.C. (1990). Forecasting, structuraltimeseries modelsandthe kalman filter.
Cambridgeuniversitypress.
Hawkins,J.,&Blakeslee,S. (2007). Onintelligence: Howanewunderstandingofthe
brainwillleadtothecreationoftrulyintelligentmachines. Macmillan.
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image
recognition. In Proceedings of the ieee conference on computer vision and
patternrecognition(pp.770–778).
Hebb, D. O. (1949). The first stage of perception: growth of the assembly. The
OrganizationofBehavior,4,60–78.
Heins,R.C.,Mirza,M.B.,Parr,T.,Friston,K.,Kagan,I.,&Pooresmaeili,A. (2020).
Deep active inference andscene construction. Frontiersin Artificial Intelligence,
References 388
3,81.
Helmholtz,H.v. (1866). Concerningtheperceptionsingeneral. Treatiseonphysiologi-
caloptics,.
Henderson, J. M. (2017). Gaze control as prediction. Trends in Cognitive Sciences,
21(1),15–23.
Hesp, C., Tschantz, A., Millidge, B., Ramstead, M., Friston, K., & Smith, R. (2020).
Sophisticated affective inference: Simulating anticipatory affective dynamics
of imagining future events. In International workshop on active inference (pp.
179–186).
Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., ...
Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement
learning. InThirty-secondaaaiconferenceonartificialintelligence.
Hinton,G.,Srivastava,N.,&Swersky,K. (2012). Neuralnetworksformachinelearning
lecture 6a overview of mini-batch gradient descent. Powerpoint Presentation,
14(8).
Hinton, G., & Zemel, R. S. (1994). Autoencoders, minimum description length and
helmholtzfreeenergy. InAdvancesinneuralinformationprocessingsystems(pp.
3–10).
Hochreiter,S.,&Schmidhuber,J. (1997). Longshort-termmemory. NeuralComputa-
tion,9(8),1735–1780.
Hohwy,J. (2016). Theself-evidencingbrain. Noûs,50(2),259–285.
Hohwy,J.,Roepstorff,A.,&Friston,K. (2008). Predictivecodingexplainsbinocular
rivalry: Anepistemologicalreview. Cognition,108(3),687–701.
Huang, Y., & Rao, R. P. (2011). Predictive coding. Wiley Interdisciplinary Reviews:
Cognitive Science, 2(5), 580–593. Retrieved from https://onlinelibrary
.wiley.com/doi/pdf/10.1002/wcs.142?casa_token=TJvdr2nDbr8AAAAA:
0T3LOAIXt6I7YYpJIqOs204qnwU0FFQiVC976sVifVv0XB4wFlrLZ7WvALY9x
_qdoIGciEZWd12hfNQ
References 389
Hubel, D. H., & Wiesel, T. N. (1962). Receptive fields, binocular interaction and
functional architecture in the cat’s visual cortex. The Journal of Physiology,
160(1),106.
Isomura, T., Parr, T., & Friston, K. (2019). Bayesian filtering with multiple internal
models: toward a theory of social intelligence. Neural Computation, 31(12),
2390–2431.
Jaswinski,A. (1970). Stochasticprocessesandfilteringtheory,1970. AcademicPress.
Johnson,M.A.,&Moradi,M.H. (2005). Pidcontrol. Springer.
Jordan,M., Ghahramani,Z.,Jaakkola,T. S.,&Saul,L.K. (1998). Anintroductionto
variationalmethodsforgraphical models. InLearningingraphicalmodels(pp.
105–161). Springer.
Jordan,M.I.,Ghahramani,Z.,Jaakkola,T.S.,&Saul,L.K. (1999). Anintroduction
tovariationalmethodsforgraphicalmodels. Machinelearning,37(2),183–233.
Jordan, R., Kinderlehrer, D., & Otto, F. (1998). The variational formulation of the
fokker–planckequation. SIAMjournalonmathematicalanalysis,29(1),1–17.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partiallyobservablestochasticdomains. ArtificialIntelligence,101(1-2),99–134.
Kaelbling,L.P.,Littman,M.L.,&Moore,A.W. (1996). Reinforcementlearning: A
survey. Journalofartificialintelligenceresearch,4,237–285.
Kaiser, J., Mostafa, H., & Neftci, E. (2020). Synaptic plasticity dynamics for deep
continuouslocallearning(decolle). FrontiersinNeuroscience,14,424.
Kalman,R.E. (1960). Anewapproachtolinearfilteringandpredictionproblems.
Kalman, R. E., & Bucy, R. S. (1961). New results in linear filtering and prediction
theory. JournalofBasicEngineering,83(1),95–108.
Kalman,R.E.,etal. (1960). Contributionstothetheoryofoptimalcontrol. Bol.soc.
mat.mexicana,5(2),102–119.
Kanai,R.,Komura,Y.,Shipp,S.,&Friston,K. (2015). Cerebralhierarchies: predictive
processing,precisionandthepulvinar. PhilosophicalTransactionsoftheRoyal
References 390
SocietyB:BiologicalSciences,370(1668),20140169.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ...
Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361.
Kaplan,R.,&Friston,K.(2018).Planningandnavigationasactiveinference.Biological
Cybernetics,112(4),323–343.
Kappen,H.J. (2005). Pathintegralsandsymmetrybreakingforoptimalcontroltheory.
Journalofstatisticalmechanics: theoryandexperiment,2005(11),P11011.
Kappen,H.J. (2007). Anintroductiontostochasticcontroltheory,pathintegralsand
reinforcementlearning. InAipconferenceproceedings(Vol.887,pp.149–181).
Kappen,H.J.,Gómez,V.,&Opper,M. (2012). Optimalcontrolasagraphicalmodel
inferenceproblem. Machinelearning,87(2),159–182.
Keller,G.B.,&Mrsic-Flogel,T.D. (2018). Predictiveprocessing: acanonicalcortical
computation. Neuron,100(2),424–435.
Kim,H.,Kim,J.,Jeong,Y.,Levine,S.,&Song,H.O. (2018). Emi: Explorationwith
mutualinformation. arXivpreprintarXiv:1810.01176.
Kim,Y.,Wiseman,S.,Miller,A.C.,Sontag,D.,&Rush,A.M. (2018). Semi-amortized
variationalautoencoders. arXivpreprintarXiv:1802.02550.
Kingma,D.P.,&Ba,J. (2014). Adam: Amethodforstochasticoptimization. arXiv
preprintarXiv:1412.6980.
Kingma,D.P.,&Welling,M. (2013). Auto-encodingvariationalbayes. arXivpreprint
arXiv:1312.6114.
Kirk,D.E. (2004). Optimalcontroltheory: anintroduction. CourierCorporation.
Klyubin,A.S.,Polani,D.,&Nehaniv,C.L. (2005). Empowerment: Auniversalagent-
centricmeasure ofcontrol. In 2005ieee congresson evolutionarycomputation
(Vol.1,pp.128–135).
Kocsis,L.,&Szepesvári,C. (2006). Banditbasedmonte-carloplanning. InEuropean
conferenceonmachinelearning(pp.282–293).
References 391
Kondepudi,D.,&Prigogine,I. (2014). Modernthermodynamics: fromheatenginesto
dissipativestructures. JohnWiley&Sons.
Kopp, R. E. (1962). Pontryagin maximum principle. In Mathematics in science and
engineering(Vol.5,pp.255–279). Elsevier.
Krebs,J.R.,Kacelnik,A.,&Taylor,P. (1978). Testofoptimalsamplingbyforaging
greattits. Nature,275(5675),27–31.
Kriegeskorte, N. (2015). Deep neural networks: a new framework for modeling
biological vision and brain information processing. Annual review of vision
science,1,417–446.
Krizhevsky,A.,Sutskever,I.,&Hinton,G. (2012). Imagenetclassificationwithdeep
convolutional neural networks. In Advances in neural information processing
systems(pp.1097–1105).
Kutschireiter,A. (2018). Nonlinearfilteringinneuroscience: theoryandapplication
(Unpublisheddoctoraldissertation). UniversityofZurich.
Kutschireiter, A., Surace, S. C., & Pfister, J.-P. (2020). The hitchhiker’s guide to
nonlinearfiltering. JournalofMathematicalPsychology,94,102307.
Kutschireiter,A.,Surace,S.C.,Sprekeler,H.,&Pfister,J.-P. (2015). Theneuralparticle
filter. arXivpreprintarXiv:1508.06818.
Kwakernaak,H.,&Sivan,R. (1972). Linearoptimalcontrolsystems(Vol.1). Wiley-
interscienceNewYork.
Lanczos,C. (2012). Thevariationalprinciplesofmechanics. CourierCorporation.
Launay,J.,Poli,I.,&Krzakala,F. (2019). Principledtrainingofneuralnetworkswith
directfeedbackalignment. arXivpreprintarXiv:1906.04554.
Lawson, R. P., Rees, G., & Friston, K. (2014). An aberrant precision account of
autism. Frontiers in human neuroscience, 8, 302. Retrieved from https://
www.frontiersin.org/articles/10.3389/fnhum.2014.00302/full
Lee,D.-H.,Zhang,S.,Fischer,A.,&Bengio,Y. (2015). Differencetargetpropagation.
In Joint european conference on machine learning and knowledge discovery in
References 392
databases(pp.498–515).
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., & Salakhutdinov, R.
(2019). Efficient exploration via state marginal matching. arXiv preprint
arXiv:1906.05274.
Leondes, C. T. (1970). Theory and applications of kalman filtering (Tech. Rep.).
Advisory Group for Aerospace Research and Development Neuilly-Sur-Seine
(France).
Levine, S. (2018). Reinforcement learning and control as probabilistic inference:
Tutorialandreview. arXivpreprintarXiv:1805.00909.
Levine, S., Kumar, A., Tucker, G., & Fu, J. (2020). Offline reinforcement learn-
ing: Tutorial, review, and perspectives on open problems. arXiv preprint
arXiv:2005.01643.
Li,S. (2020). Robot playingkendamawithmodel-basedand model-free reinforcement
learning. arXivpreprintarXiv:2003.06751.
Li,W.,&Todorov,E. (2004). Iterativelinearquadraticregulatordesignfornonlinear
biologicalmovementsystems. InIcinco(1)(pp.222–229).
Liao, Q., Leibo, J. Z., & Poggio, T. (2016). How important is weight symmetry in
backpropagation? InThirtiethaaaiconferenceonartificialintelligence.
Lillicrap, T. P., Cownden, D., Tweed, D. B., & Akerman, C. J. (2014). Random
feedback weights support learning in deep neural networks. arXiv preprint
arXiv:1411.0247.
Lillicrap,T.P.,Cownden,D.,Tweed,D.B.,&Akerman,C.J. (2016). Randomsynap-
tic feedback weights support error backpropagation for deep learning. Nature
communications,7(1),1–10.
Lillicrap, T. P., & Santoro, A. (2019). Backpropagation through time and the brain.
CurrentOpinioninNeurobiology,55,82–89.
Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J., & Hinton, G. (2020). Back-
propagationandthebrain. NatureReviewsNeuroscience,1–12.
References 393
Linnainmaa, S. (1970). The representation of the cumulative rounding error of an
algorithmasataylorexpansionofthelocalroundingerrors. Master’sThesis(in
Finnish),Univ.Helsinki,6–7.
Lotter,W.,Kreiman,G.,&Cox,D. (2016). Deeppredictivecodingnetworksforvideo
predictionandunsupervisedlearning. arXivpreprintarXiv:1605.08104.
Ma, Y.-A., Chen, T., & Fox, E. B. (2015). A complete recipe for stochastic gradient
mcmc. arXivpreprintarXiv:1506.04696.
Marino,J.,Yue, Y.,&Mandt,S. (2018). Iterativeamortizedinference. arXivpreprint
arXiv:1807.09356.
Marr,D. (1982). Vision: Acomputationalinvestigationintothehumanrepresentation
andprocessingofvisualinformation.
Maturana,H.R.,&Varela,F.J. (2012). Autopoiesisandcognition: Therealizationof
theliving(Vol.42). SpringerScience&BusinessMedia.
Mehlhorn,K.,Newell,B.R.,Todd,P.M.,Lee,M.D.,Morgan,K.,Braithwaite,V.A.,
... Gonzalez, C. (2015). Unpacking the exploration–exploitation tradeoff: A
synthesisofhumanandanimalliteratures. Decision,2(3),191.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E.
(1953). Equationofstatecalculationsbyfastcomputingmachines. TheJournal
ofChemicalPhysics,21(6),1087–1092.
Meulemans,A.,Carzaniga,F.S.,Suykens,J.A.,Sacramento,J.,&Grewe,B.F. (2020).
Atheoreticalframeworkfortargetpropagation.arXivpreprintarXiv:2006.14331.
Millidge,B. (2019a). Combiningactiveinferenceandhierarchicalpredictivecoding: A
tutorialintroductionandcasestudy.
Millidge, B. (2019b). Deep active inference as variational policy gradients. arXiv
preprintarXiv:1907.03876.
Millidge,B. (2019c). Implementingpredictiveprocessingandactiveinference: Prelim-
inarystepsandresults.
Millidge,B. (2020). Deepactiveinferenceasvariationalpolicygradients. Journalof
References 394
MathematicalPsychology,96,102348.
Millidge,B.,Tschantz,A.,&Buckley,C.L. (2020a). Predictivecodingapproximates
backpropalongarbitrarycomputationgraphs. arXivpreprintarXiv:2006.04182.
Millidge, B., Tschantz, A., & Buckley, C. L. (2020b). Whence the expected free
energy? arXivpreprintarXiv:2004.08128.
Millidge,B.,Tschantz,A.,Buckley,C.L.,&Seth,A. (2020). Activationrelaxation: A
localdynamicalapproximationtobackpropagationinthebrain. arXivpreprint
arXiv:2009.05359.
Millidge,B.,Tschantz, A.,Seth,A.,&Buckley,C. (2021). Understandingtheorigin
ofinformation-seekingexplorationinprobabilisticobjectivesforcontrol. arXiv
preprintarXiv:2103.06859.
Millidge, B., Tschantz, A., Seth, A., & Buckley, C. L. (2020a). Investigating the
scalabilityandbiologicalplausibilityoftheactivationrelaxationalgorithm. arXiv
preprintarXiv:2010.06219.
Millidge,B.,Tschantz,A.,Seth,A.,&Buckley,C.L. (2020d). Relaxingtheconstraints
onpredictivecodingmodels. arXivpreprintarXiv:2010.01047.
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020b). On the re-
lationship between active inference and control as inference. arXiv preprint
arXiv:2006.12964.
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020c). Reinforcement
learningasiterativeandamortisedinference. arXivpreprintarXiv:2006.10524.
Mirchev, A., Kayalibay, B., Soelch, M., van der Smagt, P., & Bayer, J. (2018).
Approximate bayesian inference in spatial environments. arXiv preprint
arXiv:1805.07206.
Mirza, M. B., Adams, R. A., Parr, T., & Friston, K. (2019). Impulsivity and active
inference. JournalofCognitiveNeuroscience,31(2),202–220.
Mnih, A., & Gregor, K. (2014). Neural variational inference and learning in belief
networks. arXivpreprintarXiv:1402.0030.
References 395
Mnih,V.,Badia,A.P.,Mirza,M.,Graves,A.,Lillicrap,T.,Harley,T.,... Kavukcuoglu,
K. (2016). Asynchronousmethodsfordeepreinforcementlearning. InInterna-
tionalconferenceonmachinelearning(pp.1928–1937).
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &
Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv
preprintarXiv:1312.5602.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ...
others (2015). Human-levelcontrolthroughdeepreinforcementlearning. Nature,
518(7540),529–533.
Mobbs, D., Trimmer, P. C., Blumstein, D. T., & Dayan, P. (2018). Foraging for
foundations in decision neuroscience: insights from ethology. Nature Reviews
Neuroscience,19(7),419–427.
Mumford,D. (1992). Onthecomputationalarchitectureoftheneocortex. Biological
Cybernetics,66(3),241–251.
Munuera, J., Morel, P., Duhamel, J.-R., & Deneve, S. (2009). Optimal sensorimotor
control in eye movement sequences. Journal of Neuroscience, 29(10), 3026–
3035.
Nagabandi,A.,Kahn,G.,Fearing,R.S.,&Levine,S. (2018). Neuralnetworkdynamics
for model-based deep reinforcement learning with model-free fine-tuning. In
2018ieeeinternationalconferenceonroboticsandautomation(icra) (pp.7559–
7566).
Nagabandi,A.,Konoglie,K.,Levine,S.,&Kumar,V. (2019). Deepdynamicsmodels
forlearningdexterousmanipulation. arXivpreprintarXiv:1909.11652.
Neal,R.M.,&Hinton,G. (1998). Aviewoftheemalgorithmthatjustifiesincremental,
sparse, and other variants. In Learning in graphical models (pp. 355–368).
Springer.
Neal, R.M., et al. (2011). Mcmcusing hamiltonian dynamics. Handbook ofMarkov
ChainMonteCarlo,2(11),2.
References 396
Neftci,E.O.,Mostafa,H.,&Zenke,F. (2019). Surrogategradientlearninginspiking
neuralnetworks: Bringingthe power ofgradient-basedoptimization tospiking
neuralnetworks. IEEESignalProcessingMagazine,36(6),51–63.
Nesterov, Y. (1983). A method of solving a convex programming problem with
convergencerateo(1/kˆ2)o(1/k2). InSov.math.dokl (Vol.27).
Nøkland, A. (2016). Direct feedback alignment provides learning in deep neural
networks. In Advances in neural information processing systems (pp. 1037–
1045).
Okada,M.,Kosaka,N.,&Taniguchi,T. (2020). Planetofthebayesians: Reconsidering
andimprovingdeepplanningnetworkbyincorporatingbayesianinference. arXiv
preprintarXiv:2003.00370.
Okada,M.,&Taniguchi,T. (2020). Variationalinferencempcforbayesianmodel-based
reinforcementlearning. InConferenceonrobotlearning(pp.258–272).
Olah,C.,Mordvintsev,A.,&Schubert,L. (2017). Featurevisualization. Distill,2(11),
e7.
Ollivier,Y. (2019). Theextendedkalmanfilterisanaturalgradientdescentintrajectory
space. arXivpreprintarXiv:1901.00696.
Ollivier, Y., Arnold, L., Auger, A., & Hansen, N. (2017). Information-geometric
optimizationalgorithms: Aunifyingpictureviainvarianceprinciples. Journalof
MachineLearningResearch,18(18),1–65.
Ollivier, Y., Tallec, C., & Charpiat, G. (2015). Training recurrent networks online
withoutbacktracking. arXivpreprintarXiv:1507.07680.
Orchard,J.,&Sun,W. (2019). Makingpredictivecodingnetworksgenerative. arXiv
preprint arXiv:1910.12151. Retrieved from https://arxiv.org/abs/1910
.12151
O’Reilly, R. C., Wyatte, D. R., & Rohrlich, J. (2017). Deep predictive learning: a
comprehensivemodelofthreevisualstreams. arXivpreprintarXiv:1709.04654.
Ororbia,A.G.,&Mali,A. (2019). Biologicallymotivatedalgorithmsforpropagating
References 397
localtargetrepresentations. InProceedingsoftheaaaiconferenceonartificial
intelligence(Vol.33,pp.4651–4658).
OrorbiaII,A.G.,Haffner,P.,Reitter,D.,&Giles,C.L. (2017). Learningtoadaptby
minimizingdiscrepancy. arXivpreprintarXiv:1711.11542.
Osband,I.,&VanRoy,B. (2015). Bootstrappedthompsonsamplinganddeepexplo-
ration. arXivpreprintarXiv:1507.00300.
Osband, I., Van Roy, B., Russo, D. J., & Wen, Z. (2019). Deep exploration via
randomized value functions. Journal of Machine Learning Research, 20(124),
1–62.
Oudeyer, P.-Y., & Kaplan, F. (2009). What is intrinsic motivation? a typology of
computationalapproaches. Frontiersinneurorobotics,1,6.
Ovchinnikov, I. V. (2016). Introduction to supersymmetric theory of stochastics.
Entropy,18(4),108.
O’Reilly,R.C., Braver, T.S., Cohen,J.D., etal. (1999). Abiologically basedcompu-
tationalmodelofworkingmemory. Modelsofworkingmemory: Mechanismsof
activemaintenanceandexecutivecontrol,375–411.
Palacios,E.R.,Razi,A.,Parr,T.,Kirchhoff,M.,&Friston,K. (2017). Biologicalself-
organisationandmarkovblankets. BioRxiv,227181. Retrievedfromhttps://
www.biorxiv.org/content/10.1101/227181v1.abstract
Parr, T. (2019). Thecomputational neurologyof activevision (Unpublisheddoctoral
dissertation). UCL(UniversityCollegeLondon).
Parr,T., DaCosta,L.,&Friston,K. (2020). Markovblankets,informationgeometry
andstochasticthermodynamics. PhilosophicalTransactionsoftheRoyalSociety
A,378(2164),20190159.
Parr, T., & Friston, K. (2017a). The active constructionof the visual world. Neuropsy-
chologia,104,92–101.
Parr,T.,&Friston,K. (2017b). Uncertainty,epistemicsandactiveinference. Journal
ofTheRoyalSocietyInterface,14(136),20170376.
References 398
Parr, T., & Friston, K. (2018a). Active inference and the anatomy of oculomotion.
Neuropsychologia,111,334–343.
Parr,T.,&Friston,K. (2018b). Theanatomyofinference: Generativemodelsandbrain
structure. Frontiersincomputationalneuroscience,12.
Parr,T.,&Friston,K. (2018c). Thecomputationalanatomyofvisualneglect. Cerebral
Cortex,28(2),777–790.
Parr,T.,Markovic,D.,Kiebel,S.J.,&Friston,K. (2019). Neuronalmessagepassing
using mean-field, bethe, and marginal approximations. Scientific reports, 9(1),
1–18.
Parr, T., Sajid,N.,& Friston,K. (2020). Modulesor mean-fields? Entropy, 22(5),552.
Retrievedfromhttps://www.mdpi.com/1099-4300/22/5/552
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., ... Lerer, A.
(2017). Automaticdifferentiationinpytorch.
Pathak,D.,Agrawal,P.,Efros,A.A.,&Darrell,T. (2017). Curiosity-drivenexploration
byself-supervisedprediction. InProceedingsoftheieeeconferenceoncomputer
visionandpatternrecognitionworkshops(pp.16–17).
Pearl,J. (2011). Bayesiannetworks.
Pearl,J. (2014). Probabilisticreasoninginintelligentsystems: networksofplausible
inference. Elsevier.
Peters,J.,&Schaal,S. (2007). Reinforcementlearningbyreward-weightedregression
foroperationalspacecontrol. InProceedingsofthe24thinternationalconference
onmachinelearning(pp.745–750).
Pinker, S. (2003). The language instinct: How the mind creates language. Penguin
UK.
Pio-Lopez, L., Nizard, A., Friston, K., & Pezzulo, G. (2016). Active inference and
robot control: a case study. Journal of The Royal Society Interface, 13(122),
20160616.
Pozzi,I.,Bohté,S.,&Roelfsema,P. (2018). Abiologicallyplausiblelearningrulefor
References 399
deeplearninginthebrain. arXivpreprintarXiv:1811.01768.
Prigogine, I. (2017). Non-equilibrium statistical mechanics. Courier Dover Publica-
tions.
Prigogine,I.,&Lefever,R. (1973). Theoryofdissipativestructures. InSynergetics(pp.
124–135). Springer.
Pyke, G. H. (1984). Optimal foraging theory: a critical review. Annual review of
ecologyandsystematics,15(1),523–575.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... others
(2021). Learningtransferablevisualmodelsfromnaturallanguagesupervision.
arXivpreprintarXiv:2103.00020.
Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,&Sutskever,I. (2019). Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8),9.
Rao,R.P.,&Ballard,D.H. (1999). Predictivecodinginthevisualcortex: afunctional
interpretationofsomeextra-classicalreceptive-fieldeffects. NatureNeuroscience,
2(1),79–87.
Rawlik,K.,Toussaint,M.,&Vijayakumar,S. (2013). Onstochasticoptimalcontroland
reinforcementlearningbyapproximateinference. InTwenty-thirdinternational
jointconferenceonartificialintelligence.
Rawlik, K. C. (2013). On probabilistic inference approaches to stochastic optimal
control.
Roelfsema, P. R., & Ooyen, A. v. (2005). Attention-gated reinforcement learning of
internal representations for classification. Neural Computation, 17(10), 2176–
2214.
Rubinstein,R.Y. (1997). Optimizationofcomputersimulationmodelswithrareevents.
EuropeanJournalofOperationalResearch,99(1),89–112.
Rumelhart,D.E.,Hinton,G.,&Williams,R.J. (1986). Learningrepresentationsby
back-propagatingerrors. nature,323(6088),533–536.
Rumelhart, D. E., & Zipser, D. (1985). Feature discovery by competitive learning.
References 400
Cognitivescience,9(1),75–112.
Russo, D., & Van Roy, B. (2016). An information-theoretic analysis of thompson
sampling. TheJournalofMachineLearningResearch,17(1),2442–2471.
Sacramento, J., Costa, R. P., Bengio, Y., & Senn, W. (2018). Dendritic cortical
microcircuits approximate the backpropagation algorithm. In Advances in neural
informationprocessingsystems(pp.8721–8732).
Salimans,T.,Ho,J.,Chen,X.,Sidor,S.,&Sutskever,I. (2017). Evolutionstrategiesasa
scalablealternativetoreinforcementlearning. arXivpreprintarXiv:1703.03864.
Sanborn,A.N.,&Chater,N. (2016). Bayesianbrainswithoutprobabilities. Trendsin
CognitiveSciences,20(12),883–893.
Särkkä, S. (2013). Bayesian filtering and smoothing (No. 3). Cambridge University
Press.
Scellier, B., & Bengio, Y. (2016). Towards a biologically plausible backprop. arXiv
preprintarXiv:1602.05179,914.
Scellier, B., & Bengio, Y. (2017). Equilibrium propagation: Bridging the gap be-
tween energy-based models and backpropagation. Frontiers in computational
neuroscience,11,24.
Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y. (2018a). Extending the
frameworkofequilibriumpropagationtogeneraldynamics.
Scellier, B., Goyal, A., Binas, J., Mesnard, T., & Bengio, Y. (2018b). General-
ization of equilibrium propagation to vector field dynamics. arXiv preprint
arXiv:1808.04873.
Schiess,M.,Urbanczik,R.,&Senn,W. (2016). Somato-dendriticsynapticplasticity
and error-backpropagation in active dendrites. PLoS Computational Biology,
12(2).
Schmidhuber, J. (1991). A possibility for implementing curiosity and boredom in
model-building neural controllers. In Proc. of the international conference on
simulationofadaptivebehavior: Fromanimalstoanimats(pp.222–227).
References 401
Schmidhuber, J. (1999). Artificial curiosity based on discovering novel algorithmic
predictability through coevolution. In Proceedings of the 1999 congress on
evolutionarycomputation-cec99(cat.no.99th8406)(Vol.3,pp.1612–1618).
Schmidhuber,J. (2007). Simplealgorithmicprinciplesofdiscovery,subjectivebeauty,
selectiveattention,curiosity&creativity.InInternationalconferenceondiscovery
science(pp.26–38).
Schneider,W. (1988). Analyticalusesofkalmanfilteringineconometrics—asurvey.
StatisticalPapers,29(1),3–33.
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., ...
others (2019). Masteringatari,go,chessandshogibyplanningwithalearned
model. arXivpreprintarXiv:1911.08265.
Schulman,J.,Levine,S.,Abbeel,P.,Jordan,M.,&Moritz,P. (2015). Trustregionpolicy
optimization. InInternationalconferenceonmachinelearning(pp.1889–1897).
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347.
Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of
neurophysiology,80(1),1–27.
Schultz, W., Tremblay, L., & Hollerman, J. R. (1998). Reward prediction in primate
basalgangliaandfrontalcortex. Neuropharmacology,37(4-5),421–429.
Schwartenbeck,P.,FitzGerald,T.,Dolan,R.,&Friston,K. (2013). Exploration,novelty,
surprise,andfreeenergyminimization. FrontiersinPsychology,4,710.
Schwartenbeck,P.,FitzGerald,T.H.,Mathys,C.,Dolan,R.,Wurst,F.,Kronbichler,M.,
&Friston, K. (2015). Optimalinferencewithsuboptimal models: addictionand
activebayesianinference. MedicalHypotheses,84(2),109–117.
Schwartenbeck,P.,Passecker,J.,Hauser,T.U.,FitzGerald,T.H.,Kronbichler,M.,&
Friston, K. (2019). Computational mechanisms of curiosity and goal-directed
exploration. , 8, e41703. Retrieved 2019-11-15, from https://doi.org/10
.7554/eLife.41703 doi: 10.7554/eLife.41703
References 402
Schwöbel,S.,Kiebel,S.,&Markovic´,D. (2018). Activeinference,beliefpropagation,
andthebetheapproximation. NeuralComputation,30(9),2530–2567.
Seifert, U. (2008). Stochastic thermodynamics: principles and perspectives. The
EuropeanPhysicalJournalB,64(3),423–431.
Seifert, U. (2012). Stochastic thermodynamics, fluctuation theorems and molecular
machines. ReportsonProgressinPhysics,75(12),126001.
Seth, A. K. (2014). The cybernetic bayesian brain. Open MIND. Frankfurt am
Main: MIND Group. Retrieved from https://open-mind.net/papers/the
-cybernetic-bayesian-brain
Sethi,S.P.,&Thompson,G.L. (2000). Whatisoptimalcontroltheory? Springer.
Seung,H.S. (2003). Learninginspikingneuralnetworksbyreinforcementofstochastic
synaptictransmission. Neuron,40(6),1063–1073.
Shanks,D.R.,Tunney,R.J.,&McCarthy,J.D. (2002). Are-examinationofprobability
matching and rational choice. Journal of Behavioral Decision Making, 15(3),
233–250.
Shannon, C. E. (1948). A mathematical theory of communication. The Bell system
technicaljournal,27(3),379–423.
Shipp, S. (2016). Neural elements for predictive coding. Frontiers in Psychology, 7,
1792.
Shipp, S.,Adams, R.A., &Friston, K. (2013). Reflectionson agranulararchitecture:
predictivecodinginthemotorcortex. Trendsinneurosciences,36(12),706–716.
Shyam, P., Jas´kowski, W., & Gomez, F. (2019). Model-based active exploration.
In International conference on machine learning (pp. 5779–5788). Retrieved
2019-10-11,fromhttp://proceedings.mlr.press/v97/shyam19a.html
Silver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L.,VanDenDriessche,G.,...
others (2016). Mastering the game of go with deep neural networks and tree
search. nature,529(7587),484.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ...
References 403
others (2017). Mastering the game of go without human knowledge. Nature,
550(7676),354–359.
Simoncelli, E. P. (2009). Optimal estimation in sensory systems. The Cognitive
Neurosciences,IV,525–535.
Singh,S.P.,&Sutton,R.S. (1996). Reinforcementlearningwithreplacingeligibility
traces. MachineLearning,22(1),123–158.
Spratling,M.W. (2008). Reconcilingpredictivecodingandbiasedcompetitionmodels
ofcorticalfunction. FrontiersinComputationalNeuroscience,2,4.
Spratling,M.W. (2017). Areviewofpredictivecodingalgorithms. BrainandCognition,
112, 92–97. Retrieved from https://www.sciencedirect.com/science/
article/pii/S027826261530035X?casa_token=zzTchZsrFesAAAAA:
5bJNguAnRfn4BOjlCtmGvjiQT0Mkk3CE1By9JsrGrDIT0qY-CUKLUwVROkHB9S
_kUx6mtH-nc74
Steil,J.J. (2004). Backpropagation-decorrelation: onlinerecurrentlearningwitho(n)
complexity. In2004ieeeinternationaljointconferenceonneuralnetworks(ieee
cat.no.04ch37541)(Vol.2,pp.843–848).
Stengel,R.F. (1994). Optimalcontrolandestimation. CourierCorporation.
Still,S.,&Precup,D. (2012). Aninformation-theoreticapproachtocuriosity-driven
reinforcementlearning. TheoryinBiosciences,131(3),139–148.
Stuart,G.,Spruston,N.,Sakmann,B.,&Häusser,M. (1997). Actionpotentialinitiation
andbackpropagationinneuronsofthemammaliancns. TrendsinNeurosciences,
20(3),125–131.
Such, F. P., Madhavan, V., Conti, E., Lehman, J., Stanley, K. O., & Clune, J.
(2017). Deep neuroevolution: Genetic algorithms are a competitive alterna-
tivefortrainingdeepneuralnetworksforreinforcementlearning. arXivpreprint
arXiv:1712.06567.
Sun, Y., Gomez, F., & Schmidhuber, J. (2011). Planning to be surprised: Optimal
bayesian exploration in dynamic environments. In International conference on
References 404
artificialgeneralintelligence(pp.41–51).
Sussman, G. J., & Wisdom, J. (2015). Structure and interpretation of classical
mechanics. TheMITPress.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences.
MachineLearning,3(1),9–44.
Sutton,R.S. (1990). Integratedarchitecturesforlearning,planning,andreactingbased
onapproximatingdynamicprogramming. InMachinelearningproceedings1990
(pp.216–224). Elsevier.
Sutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and
reacting. ACMSigartBulletin,2(4),160–163.
Sutton,R.S. (1996). Generalizationinreinforcementlearning: Successfulexamples
usingsparsecoarsecoding. AdvancesinNeuralInformationProcessingSystems,
1038–1044.
Sutton,R.S.,&Barto,A.G. (2018). Reinforcementlearning: Anintroduction. MIT
press.
Sutton, R. S., Barto, A. G., et al. (1998). Introduction to reinforcement learning
(Vol.135). MITpressCambridge.
Tallec,C.,&Ollivier,Y. (2017). Unbiasedonlinerecurrentoptimization. arXivpreprint
arXiv:1702.05043.
Tesauro, G. (1994). Td-gammon, a self-teaching backgammon program, achieves
master-levelplay. NeuralComputation,6(2),215–219.
Theodorou, E., & Todorov, E. (2012). Relative entropy and free energy dualities:
Connectionstopath integralandklcontrol. In2012ieee 51stieeeconference on
decisionandcontrol(cdc)(pp.1466–1473).
Theodorou,J.,EvangelosndBuchli,&Schaal,S. (2010a). Ageneralizedpathintegral
controlapproachtoreinforcementlearning. journalofmachinelearningresearch,
11(Nov),3137–3181.
Theodorou, J., Evangelosnd Buchli, & Schaal, S. (2010b). Reinforcement learning
References 405
of motor skills in high dimensions: A path integral approach. In 2010 ieee
internationalconferenceonroboticsandautomation(pp.2397–2403).
Tishby, N., Pereira, F. C., & Bialek, W. (2000). The information bottleneck method.
arXivpreprintphysics/0004057.
Tishby, N., & Polani, D. (2011). Information theory of decisions and actions. In
Perception-actioncycle(pp.601–636). Springer.
Todorov,E. (2004). Optimalityprinciplesinsensorimotorcontrol. NatureNeuroscience,
7(9),907.
Todorov,E. (2008). General dualitybetweenoptimal controlandestimation. In2008
47thieeeconferenceondecisionandcontrol(pp.4286–4292).
Toussaint, M., & Storkey, A. (2006). Probabilistic inference for solving discrete
and continuous state markov decision processes. In Proceedings of the 23rd
internationalconferenceonmachinelearning(pp.945–952).
Tran,D.,Dusenberry,M.W.,vanderWilk,M.,&Hafner,D. (2018). Bayesianlayers:
Amoduleforneuralnetworkuncertainty. arXivpreprintarXiv:1812.03973.
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020). Scaling active
inference. In2020internationaljointconferenceonneuralnetworks(ijcnn)(pp.
1–8).
Tschantz,A.,Millidge,B.,Seth,A.K.,&Buckley,C.L. (2020a). Controlashybrid
inference. arXivpreprintarXiv:2007.05838.
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020b). Reinforcement
learningthroughactiveinference. arXivpreprintarXiv:2002.12636.
Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and
biases. science,185(4157),1124–1131.
Ueltzhöffer,K. (2018). Deepactiveinference.,112(6),547–573. Retrieved2019-10-19,
fromhttp://arxiv.org/abs/1709.02341 doi: 10.1007/s00422-018-0785-7
Van Merriënboer, B., Breuleux, O., Bergeron, A., & Lamblin, P. (2018). Automatic
differentiationinml: Whereweareandwhereweshouldbegoing. InAdvances
References 406
inneuralinformationprocessingsystems(pp.8757–8767).
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,... Polo-
sukhin,I. (2017). Attentionisallyouneed. InAdvancesinneuralinformation
processingsystems(pp.5998–6008).
Vinyals,O., Babuschkin,I., Czarnecki,W.M., Mathieu,M., Dudzik,A., Chung,J., ...
others (2019). Grandmaster levelinstarcraftii usingmulti-agentreinforcement
learning. Nature,575(7782),350–354.
Vulkan, N. (2000). An economist’s perspective on probability matching. Journal of
economicsurveys,14(1),101–118.
Wainwright, M. J., & Jordan, M. I. (2008). Graphical models, exponential families,
andvariationalinference. NowPublishersInc.
Walsh,K. S.,McGovern,D.P., Clark,A.,&O’Connell,R.G. (2020). Evaluatingthe
neurophysiological evidence for predictive processing as a model of perception.
AnnalsoftheNewYorkAcademyofSciences,1464(1),242.
Wan,E.A.,&VanDerMerwe,R. (2000). Theunscentedkalmanfilterfornonlinear
estimation.InProceedingsoftheieee2000adaptivesystemsforsignalprocessing,
communications,andcontrolsymposium(cat.no.00ex373)(pp.153–158).
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., & Freitas, N. (2016).
Duelingnetworkarchitecturesfordeepreinforcementlearning. InInternational
conferenceonmachinelearning(pp.1995–2003).
Watanabe,E.,Kitaoka,A.,Sakamoto,K.,Yasugi,M.,&Tanaka,K. (2018). Illusory
motionreproducedbydeepneuralnetworkstrainedforprediction. Frontiersin
Psychology,9,345.
Watkins,C.J.,&Dayan,P. (1992). Q-learning. Machinelearning,8(3-4),279–292.
Watter, M., Springenberg, J. T., Boedecker, J., & Riedmiller, M. (2015). Embed to
control: A locally linear latent dynamics model for control from raw images.
arXivpreprintarXiv:1506.07365.
Weilnhammer, V., Stuke, H., Hesselmann, G., Sterzer, P., & Schmack, K. (2017). A
References 407
predictive coding accountof bistable perception-a model-basedfmri study. PLoS
ComputationalBiology,13(5),e1005536.
Welling,M.,&Teh, Y.W. (2011). Bayesianlearningviastochasticgradientlangevin
dynamics. In Proceedings of the 28th international conference on machine
learning(icml-11)(pp.681–688).
West, R. F., & Stanovich, K. E. (2003). Is probability matching smart? associations
between probabilisticchoices and cognitiveability. Memory & Cognition, 31(2),
243–251.
Whittington,J.C.,&Bogacz,R. (2017). Anapproximationoftheerrorbackpropagation
algorithminapredictivecodingnetworkwithlocalhebbiansynapticplasticity.
NeuralComputation,29(5),1229–1262.
Wiener,N. (2019). Cyberneticsorcontrolandcommunicationintheanimalandthe
machine. MITpress.
Williams, D. (2018). Predictive processing and the representation wars. Minds and
Machines, 28(1), 141–172. Retrieved from https://link.springer.com/
article/10.1007/s11023-017-9441-6
Williams,D. (2020). Isthebrainanorganforpredictionerrorminimization?
Williams, G., Aldrich, A., & Theodorou, E. (2017). Model predictive path integral
control: Fromtheorytoparallelcomputation. JournalofGuidance,Control,and
Dynamics,40(2),344–357.
Williams,G.,Drews,P.,Goldfain,B.,Rehg,J.M.,&Theodorou,E. (2016). Aggressive
driving with model predictive path integral control. In 2016 ieee international
conferenceonroboticsandautomation(icra)(pp.1433–1440).
Williams,G.,Wagener,N.,Goldfain,B.,Drews,P.,Rehg,J.M.,Boots,B.,&Theodorou,
E. (2017). Information theoretic mpc for model-based reinforcement learning.
In 2017 ieee international conference on robotics and automation (icra) (pp.
1714–1721).
Williams,R.J.,&Zipser,D. (1989a). Experimentalanalysisofthereal-timerecurrent
References 408
learningalgorithm. ConnectionScience,1(1),87–111.
Williams, R. J., & Zipser, D. (1989b). A learning algorithm for continually running
fullyrecurrentneuralnetworks. NeuralComputation,1(2),270–280.
Wolpert,D.M. (1997). Computationalapproachestomotorcontrol. TrendsinCognitive
Sciences,1(6),209–216.
Xiao,H., Rasul,K.,&Vollgraf,R. (2017a). Fashion-mnist: anovelimagedatasetfor
benchmarkingmachinelearningalgorithms. arXivpreprintarXiv:1708.07747.
Xiao,H.,Rasul,K., &Vollgraf,R. (2017b). Fashion-mnist: a novelimagedatasetfor
benchmarkingmachinelearningalgorithms.
Yedidia, J. S. (2011). Message-passing algorithms for inference and optimization.
JournalofStatisticalPhysics,145(4),860–890.
Yedidia,J.S.,Freeman,W.T.,&Weiss,Y. (2001). Generalizedbeliefpropagation. In
Advancesinneuralinformationprocessingsystems(pp.689–695).
Yedidia,J. S.,Freeman,W.T., &Weiss,Y. (2005). Constructing free-energyapproxi-
mations and generalized belief propagation algorithms. IEEE Transactions on
informationtheory,51(7),2282–2312.
Yuan, R., & Ao, P. (2012). Beyond itô versus stratonovich. Journal of Statistical
Mechanics: TheoryandExperiment,2012(07),P07010.
Yuan, R., Ma, Y., Yuan, B., & Ao, P. (2010). Constructive proof of global lyapunov
functionaspotentialfunction. arXivpreprintarXiv:1012.2721.
Yuan,R.,Ma,Y.,Yuan,B.,&Ao,P. (2011). Potentialfunctionindynamicalsystems
and the relation with lyapunov function. In Proceedings of the 30th chinese
controlconference(pp.6573–6580).
Yuan,R.,Tang,Y.,&Ao,P. (2017). Sdedecompositionanda-typestochasticinterpre-
tationinnonequilibriumprocesses. FrontiersofPhysics,12(6),1–9.
Zago, M., McIntyre, J., Senot, P., & Lacquaniti, F. (2008). Internal models and
predictionofvisualgravitationalmotion. VisionResearch,48(14),1532–1538.
Zenke,F., &Ganguli,S. (2018). Superspike: Supervisedlearninginmultilayerspiking
References 409
neuralnetworks. NeuralComputation,30(6),1514–1541.
Zwanzig,R. (2001). Nonequilibriumstatisticalmechanics. OxfordUniversityPress.