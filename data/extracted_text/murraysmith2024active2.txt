Active Inference and Human‚ÄìComputer Interaction
RODERICK MURRAY-SMITH, JOHN H. WILLIAMSON, and SEBASTIAN STEIN,School of Computing
Science, University of Glasgow, Scotland
Active Inference is a closed-loop computational theoretical basis for understanding behaviour, based on agents with internal probabilistic
generative models that encode their beliefs about how hidden states in their environment cause their sensations. We review Active
Inference and how it could be applied to model the human-computer interaction loop. Active Inference provides a coherent framework
for managing generative models of humans, their environments, sensors and interface components. It informs off-line design and
supports real-time, online adaptation. It provides model-based explanations for behaviours observed in HCI, and new tools to measure
important concepts such as agency and engagement. We discuss how Active Inference offers a new basis for a theory of interaction in
HCI, tools for design of modern, complex sensor-based systems, and integration of artificial intelligence technologies, enabling it to
cope with diversity in human users and contexts. We discuss the practical challenges in implementing such Active Inference-based
systems.
Additional Key Words and Phrases: Active Inference, Human‚ÄìComputer Interaction, Computational Interaction, Free Energy
1 Introduction
This paper will examine the incorporation ofActive Inference in the theory and practice of human‚Äìcomputer interaction.
Active Inference (AIF) is a closed-loop computational theory for modelling agent behaviour [47, 105]. AIF assumes a
(biological or artificial) agent has an internal generative, probabilistic model which encodes beliefs about how hidden
states in the environment cause its sensations, and that it acts to reduce uncertainty to minimise ‚Äòexpected surprise‚Äô.
Such agents establish and preserve favourable homeostasis by minimising surprise over all predicted futures [48].
Active Inference has become the source of intense scientific interest across a broad set of areas, including consciousness
research, philosophy and neuroscience [14, 27, 29, 128]. We intend this paper to open up links between HCI and this
frontier of theories. We argue that there is a pressing need for a broad-ranging and rigorous theory for interaction
design and analysis; one which we believe Active Inference affords us. Hornb√¶k and Oulasvirta [67] highlight: ‚ÄúWhile
we have concepts of interaction that talk about design ..., we have failed to produce theories and concepts that have both
high determinacy and adequate scope. ‚Äù and go on to discuss that classical mathematical and simulation models of users
often have high determinacy but inadequate scope.
Active Inference is an elegant, unifying theory which incorporates machine learning, Bayesian inference, probabilistic
programming, and dynamic systems, and can be applied to design decisions from low-level physical interactions to
high-level reasoning and decision tasks. It is a precise, mathematical theory that leads to implementable algorithms,
but its scope ranges from the chemotaxis of single-celled organisms to human psychology. It is an exciting scientific
paradigm, and the implications for interaction are profound and stimulating. It is also a theory which many find
notoriously difficult to understand [4, 88] and carries deeper significance than it might initially appear.1
Active Inference provides a basis for scientific, quantitative analysis and engineering of interactive systems which
furnishes detailed predictions at a range of levels and with broad application. This paper will present a first proposal
for the integration of Active Inference theory into interaction design, viewing the interaction loop as a negotiated,
mutual control process between human and computer, with both acting to control their perceptions to minimise their
1Despite the similar name, Active Inference is not directly related to Active Learning in the machine learning literature.
Authors‚Äô Contact Information: Roderick Murray-Smith, Roderick.Murray-Smith@glasgow.ac.uk; John H. Williamson, JohnH.Williamson@glasgow.ac.uk;
Sebastian Stein, School of Computing Science, University of Glasgow, Glasgow, Scotland.
1
arXiv:2412.14741v1  [cs.HC]  19 Dec 2024
2 Murray-Smith, Williamson, Stein
sensory surprise. It will explore the potential impact on HCI theory for off-line analysis, as well as on-line use, creating
interactive systems that are better aligned with users‚Äô preferences, and can adapt to their needs, ability and context
more rapidly.
1.1 Contributions
Active inference as a  model of  HCI
Core elements of  AIF HCI
Outlook Challenges
Conclusions
Related work
AIF background
Vignettes
AppendicesMain body
This section relates active inference to other theories of  HCI, 
particularly computational approaches, and relates the active 
inference literature to HCI.
A step-by-step introduction 
to active inference from an 
HCI perspective, focusing 
on the algorithmic view.
Mathematical formulation
A summary of  the relevant 
equations in active inference.
Three worked-out scenarios illustrating how active inference could 
be used in interaction design.
Overview
Emotional-
support robot
Semi-autonomous 
vehicle
Intelligent 
music player
Computational AIF
Agents and 
environment
Forward
models
Preference
priors
Surprise minimisationMarkov blankets
Computational 
models to predict 
future beliefs via
Bayesian inference.
Specification of  goals 
and constraints via 
probability distributions.
Action selection via minimising 
free energy; combining 
exploitation and exploration.
Defining states that define 
the actuation and sensory 
boundaries of  agents.
Representing users 
and systems as 
agents within an 
envrionment.
How will active inference 
develop in HCI? What is the 
roadmap for practical 
implementations?
Implications
What are the limitations of  
active inference? How do 
computational issues impact 
AIF approaches? 
What theoretical implications 
does AIF have? How would it 
impact design? What new 
questions can it answer?
Why is it worth applying active 
inference to HCI? 
How can HCI problems be framed as active inference? How are 
users, systems and environments and their joint configurations 
represented?
Fig. 1. The overall structure of the paper.
This is a theory paper without implementations, evaluations or results. Our contributions are:
(1) An introduction to Active Inference theory for an HCI audience (¬ß2), where a tutorial on the algorithms (¬ßB)
and mathematical detail (¬ßC) is limited to the Appendix. This includes a review of the AIF literature and how
Active Inference relates to other theories of interaction in (¬ßA) and through illustrative scenarios (¬ßD);
(2) An analysis of how AIF principles may be applied in interaction, presenting the general configurations of AIF
models in the interaction loop (¬ß2.4);
(3) A discussion of the hallmark characteristics of AIF theory in interaction, including interaction withEnvironments,
Markov Blankets , Forward Models (¬ß3) and the role of Prediction, and representing an agent‚Äôs goals or desires via
Preference priors;
Active Inference and Human‚ÄìComputer Interaction 3
(4) A discussion of the ramifications of AIF approaches. The theory offers novel perspectives into questions of
agency, freedom and resilience as well as practical techniques for interaction engineering (¬ß4), but it is subject
to challenges in modelling, computational issues and the limited development of software (¬ß5).
2 Active Inference
2.1 Principles, theories, models and heuristics
What is Active Inference? One reading of it is as simply a principle ‚Äì minimise long-term surprise via a computable
quantity called the Free Energy 2 that is hypothesised to explain the behaviour of sentient organisms. It could also be
characterised as a theory to frame and represent problems of goal-directed behaviour (in the same sense as ‚Äúinformation
theory‚Äù or ‚ÄúBayesian theory‚Äù). This theory unifies perception and action by considering agents as if they stood at the
end of predicted paths into all possible futures and looked back to infer which actions would be most likely to have
taken along their preferred paths. It is not in itself a model, but leads to specific instantiations of models that can be
implemented and tested. Many component parts need to be specified to instantiate a concrete model (Section 2.4).
Active Inference leads to the heuristic of ‚Äúminimising surprise‚Äù as a way to approach building intelligent systems.3 This
is a powerful but subtle way of thinking about interaction, which we develop further in ¬ß3.4.1. Active Inference does
not specify or propose any specific algorithm but specifies how to combine probabilistic reasoning components to build
agents. In that sense, it can be viewed as a framework for tying together particular machine learning algorithms into
models that can be applied to interaction problems.
2.2 Active Inference Agents
We consider an agent ‚Äì an agent being a entity distinct from its environment that perceives and acts with purpose ‚Äì
embedded within a wider environment (Fig. 2, top). An Active Inference agent (AIF agent) is an agent whose actions
are driven by the principles of Active Inference. Active Inference has three key aspects:
‚Ä¢ Probabilistic Active Inference is wholly probabilistic, and conceives of agents as entities that perform Bayesian
inference to update their model of the world and to decide upon actions. Both beliefs and preferences are
encoded as distributions, not points.
‚Ä¢ Predictive Active Inference presumes agents who act based on predictions rather than on immediate perceptions.
Predictive models decouple behaviour from immediate reactions to stimuli. Active Inference agents predict
future beliefs over the environment and use their generative models to predict the sensations those beliefs
would be expected to evoke.
‚Ä¢ Unified Active Inference combines reasoning about perception with reasoning about actions in a single unified
inference problem. Millidge et al. [91] summarise it thus: ‚ÄúIntuitively, [Active Inference] turns the action selection
problem on its head. Instead of saying: I have some goal, what do I have to do to achieve it? The Active Inference
agent asks: Given that my goals were achieved, what would have been the most probable actions that I took?‚Äù
2The Free Energy Principle (FEP) [47] suggests that any self-organizing system that is at equilibrium with its environment must minimize free energy, and
is a mathematical formulation of how adaptive systems resist a natural tendency to disorder. It is composed of two termsEùë•‚àºùëÑ (ùë•)ln ùëÉ(ùë¶,ùë•)‚àíùêª(ùëÑ(ùë•)).
The first is the energy, which is the surprise or information about the joint occurrence of the sensory input ùë¶and its causes ùë•. The second term is the
negative entropy of the density.
3It acts to minimise future surprise , where surprise is defined idiosyncratically to have the pejorative sense of being both unexpected and undesirable. In
this framing, surprise implies being out of equilibrium; a human is ‚Äúsurprised‚Äù to find their body temperature to be 35¬∞C and is motivated to avoid being
surprised in this way.
4 Murray-Smith, Williamson, Stein
Information gain
The Markov blanket captures all 
of  the variables on the boundary 
between the agent and the 
environment.
Planning involves 
searching policies -- 
action sequences -- 
scoring them and 
selecting a next 
action.
The agent uses aforward model to predict 
future states of  the world compatible with its 
previous belief  (and the selected action)
Observations from the environment are used 
to correct the belief  to be compatible with 
what is sensed. This is a Bayesian update, 
posterior ‚àù prior x evidence.
Expected free energy is the average free energy over 
all states in the policy tree that start with a specific 
action (shown in dotted bubble for action at=0).
Rollout searches possible action 
sequences up to a time horizon; generally 
this is a tree of  possible actions. Each 
rollout is one path (orange) in the tree.
Rollout produces action sequences, 
and each sequence is grouped by its 
initial action. Each of  these 
imminent actions gets an EFE score.
Softmaxturns the EFE scores into 
a Boltzmann distribution over the 
imminent actions.
A specific imminent action 
is selected, e.g. by random
sampling weighted by the 
action distribution.
The preference prior
defines what the agent 
wants by assigning higher 
likelihood to more 
favourable states.
Pragmatic valueis a measure of  
the difference between the belief  
distribution and the preferred 
distribution.
Each arrow involves a 
prediction with the
forward model.
Possible observations are simulated. 
Each gets a score -- the information 
gain for that imagined observation.
The average information gain 
is summed with the pragmatic 
value to produce thefree 
energy.
The predicted state is used to 
sample a synthetic observation 
that might be observed.
The synthetic observation is 
"perceived" to update the state, 
as in the perception/action loop.
Comparing the distribution before and 
after the dreamt observation scores 
how much information could be gained.
Beliefs are always 
represented as
distributions over 
possible configurations.
A schematic guide to a practical implementation of  active inference
Time 
horizon
Rollout
EFE
Softmax Action
sampler
Action
distribution
at
Dream Perceive
Compare
KL divergence
Environment
Markov blanket
Observationot+1at ActionPlan
Predict Correct
Next timestep
p(st)
p(st)
EFE[a
t=0]
p(st) p(sT)
p(sT)
preference pragmatic
freeenergy
F[Q(sT)]
Mean
exp.
inf.
gain
inf. gain
inf. gain
inf. gain
o'T p(sT|o'T)
p(st+1|at) p(st+1|ot+1, at)
at=0
at=2
at=3
Agent
p(st+1)
Actuation Sensing
The environment is 
inaccessible to the 
agent, except via 
the Markov  
P(at)
Action-perception loop
blanket.
Plan
Rollout
Free energy
Fig. 2. A schematic diagram of the Active Inference algorithm. An agents plans via a rollout process, scoring the tree of possible
future states according to their information gain ("informativeness") and pragmatic value ("goodness") (See ¬ß3.4.1 and App.B for
details). This is summarised into a single quantity for each imminent action, the Expected Free Energy (EFE) and an action is sampled
to minimise this value. The agent then updates its belief states using a Bayesian update given the sampled action and its perceptions
from sensing.
Active Inference and Human‚ÄìComputer Interaction 5
The agent is explicitly isolated from its environment, except via a defined set of mediating states, the Markov blanket
(Fig 6). This blanket is demarcated into states which actuate (modify the environment) and sense (respond to the
environment). The behaviour of the agent is governed by three elements:
‚Ä¢ a preference prior that defines which states the agent would prefer to be in. This means that its goals or desires
are defined as a distribution rather than a single reference.
‚Ä¢ a forward model 4 that predicts how the environment evolves over time, conditioned on actions that the agent
might take;
‚Ä¢ an observation model that predicts what might be observed (by the agent‚Äôs sensors) in a particular state. Note
that it does not react directly to sensation it perceives, but to its belief about the future states of the world. The
only role of observations is to correct estimates of beliefs about the environment.
The agent acts by predicting future beliefs, scoring each immediate action by the average free energy of all beliefs
consequent to that imminent action (the Expected Free Energy , EFE) and then selecting an action to minimise the EFE,
and thus expected future surprise.
2.3 Active Inference in HCI
Active Inference is a computational approach that proposes a mechanism that drives intelligent behaviour. In the context
of human‚Äìcomputer interaction, it is a computational interaction [104] approach, an approach focused on building and
applying implementable models in interactive systems. Like all computational interaction approaches, an AIF approach
to HCI emphasises explicit and precise description of the interaction in a form that can be analysed in silico . As Active
Inference is not familiar to most HCI researchers, we provide a one-page visual summary of the core computational
elements of AIF is presented in Figure 2. We explicitly conceive of Active Inference from an algorithmic perspective as
we intend these ideas to be implemented as practical software models. The details of the components in Figure 2 and an
explanation of the algorithm are addressed in Appendix B. The mathematical exposition is delegated to Appendix C.
One of the appealing aspects of Active Inference is that it unifies many strands of thinking in computational
interaction. Active Inference is fundamentally a Bayesian approach, which conceives of reasoning as probabilistic
updates, providing a guide for how rational agents should solve problems of induction, integrating prior knowledge
with new data. This is well aligned with developments over recent decades in the cognitive science community, working
on reverse-engineering the mind from Bayesian first principles [56].
With respect to HCI, the probabilistic nature of Active Inference makes it well suited to deal with the uncertainty rife
in interaction problems. Users and contexts of use are fundamentally uncertain; failure to reason about this uncertainty
leads to fragility. The predictive nature of Active Inference is both useful when applied as a model of human cognitive
processes (humans clearly exhibit predictive elements in their actions) and in also a systematic way to address latencies
inherent in interactive closed-loops.
2.4 Human‚ÄìComputer Interaction
Active Inference describes how agents may behave. But in an HCI context, who is the agent? What is the environment?
2.4.1 Principles for applying Active Inference in interaction. We now adapt the standard AIF model to represent humans
and computer systems from an Active Inference perspective:
4The use of a forward model in Active Inference is akin to a ‚Äòdigital twin‚Äô which allows the agent to monitor activity and infer hidden states (such as
possible user intentions or goals), and predict possible outcomes to adapt the interface, make decisions, and act on the world.
6 Murray-Smith, Williamson, Stein
AIF Human model: Our model of the human agent can predict what their senses will observe following actions
they can make. The human agent can mentally make future plans and, using the expected free energy metric, pick
actions which will reduce the surprise emanating from the computer, to bring the unobservable internal states of the
computer to preferred values. Users observe computer displays as part of their observation vector.
AIF Computer model: In turn, human actions are observed by the sensors of the computer. The computer agent
can generate its own candidate action plans which parameterise the computer‚Äôs display to the human (e.g. change the
display, make a sound, send a vibration), or actions on the shared environment. This could include manipulating robot
limbs or environmental adjustments (e.g. lighting, heating or air conditioning). The computer‚Äôs display actions feed
into human agent‚Äôs observations with a fidelity which depends on the quality of the computer‚Äôs display, the human‚Äôs
perceptual systems and attention, and any disturbances from the environment. The AIF needs probabilistic forward
models of sensors, interface dynamics and displays (video, audio, tactile).
AIF Interface model (transduction): a variation on the general Active Inference model of the computer is to
model an interface that looks in both directions to black box user and system models. This may be appropriate when
the states of the computer are not accessible or reliably predictable for the interface elements, forcing it to view the rest
of the system as an uncertain environment.
2.5 Virtues of AIF for HCI
What is the point? Why would one wish to build systems that are driven by Active Inference principles or to model
users as Active Inference agents? When is it appropriate to give an interaction object a purpose, and how will human
users respond to that? Designing interaction objects as agents with purpose or goals has the potential to make them
more resilient to changes in the context and, as William James et al. [73] said, enable them to achieve stable goals via
flexible means,5 and can reduce the complexity that the user needs to control directly.
There are several virtues that typify Active Inference. Active Inference proposes a principle for action that is robust,
in the sense that Active Inference agents apply Bayesian reasoning with appropriately quantified and propagated
uncertainty. Much unmodelled complexity can be absorbed into uncertainty, as opposed to building increasingly
intricate but fragile models. Active Inference emphasises resilience, specifically the framing of goal-directed behaviour
as the drive to remain in ‚Äúcomfortable homeostasis‚Äù. We would therefore expect AIF-powered interfaces to have superior
qualities in remaining stable and controllable in a wide variety of conditions. As a Bayesian approach, Active Inference
is parsimonious ‚Äì especially powerful in small data regimes, where sparse and noisy data can be efficiently combined
with strong prior models. This is in sharp contrast to conventional deep learning methods that rely on extensive
historical datasets for learning; this restricts conventional ML in interaction contexts where such data is hard to come
by. It is particularly challenging for traditional ML to deal with closed-loop adaptation because observations of a
particular user in a particular context will always be sparse. Active Inference models are also fundamentallypredictive;
they operate exclusively on predictions of possible futures. This is a useful feature when modeling human users, to
emulate the predictive nature of human behaviour. It is also of interest in building interactive systems, where predictive
aspects can account for latency and beyond that lead to anticipatory interfaces. An Active Inference based approach is
also adaptive in that Active Inference involves reasoning ‚Äúin the moment‚Äù ‚Äì updating models and their parameters
5‚ÄúAlter the preexisting conditions, and with inorganic materials you bring forth each time a different apparent end. But with intelligent agents, altering the
conditions changes the activity displayed, but not the end reached; for here the idea of the yet unrealised end cooperates with the conditions to determine what
the activities shall be. ‚Äù [73].
Active Inference and Human‚ÄìComputer Interaction 7
not solely in an offline training regime but during the evolution of behaviour itself. This affords a class of intelligent
behaviour that might be harder to realise with approaches that rely on amortised inference.
2.6 Active Inference configurations in Interactive Systems
Environment
Userinternalstates
Motorstates
Perceptionstates
Sensorstates
Actionstates
System
internalstatesUser System
Markov blankets
Active inference user agent Active inference system agent
Fig. 3. The Active Inference representation of the entire human‚Äìcomputer interaction loop as a dyad of mutually interacting agents.
Each agent can only perceive or act on the environment via those variables that form its Markov blanket. Each agent is embedded in
the environment of the other, and its actions impinge upon its partner‚Äôs Markov blanket only via this environment.
The ‚ÄòMutual Interaction‚Äô offline simulation scenario, shown in Figure 3, takes an AIF approach to model the entire
interaction loop with a dyad of a human user and a computer system, both of which are modelled as AIF agents. Each
agent has its own goals/preferences and can learn during their interaction. The user and system become each other‚Äôs
environment (we discuss the environment in more detail in ¬ß3.1). Both agents dynamically interact with each other to
align their observations with preferences, but also engage in the necessary precursor of minimising uncertainty about
hidden states in the partner agent.
By modelling the whole loop, we introduce the benefits of simulation (¬ßA.3.2) and open up the possibility of
computational solutions to explain interaction (¬ß4.2), including open challenges such as measures ofagency, engagement
(¬ß4.3 and ¬ßA.2.3), the boundaries in Human‚ÄìComputer interaction (¬ß3.2), and how best to represent human preferences
and goals (¬ß3.5).
We can, however, apply ideas from active inference to HCI in other configurations where non AIF systems or real
users are included in the loop, as shown in Figure 4, and described below.6
2.6.1 Offline simulation and analysis of interactive systems via AIF. There are configurations intended for offline
simulation and analysis, where the user is modelled by an Active Inference model:
‚Ä¢(U‚Äô) SDescription we can model a user Uas operating under AIF principles to reason about how users might
behave. The system Sis part of the environment. This formalism can be used at design-time to simulate user
behaviour and thus optimise designs, or at evaluation-time to interpret observed interaction behaviours.
6Our textual descriptions of these configurations use parentheses to indicate the part of the user-system loop modelled as an Active Inference agent.
Primes (like U‚Äô) indicate ‚Äúmodel of‚Äù (for example, U‚Äômeans ‚Äúmodel of the user‚Äù).
8 Murray-Smith, Williamson, Stein
Online
Offline Level 1
Level 2
Description (U') S
Mutual Interaction 
(U') (S)
Construction U (S)
Transduction U (I) S
Reflection U (S (U'))
Online
System
Active inference modeling used in the design and evaluation of  
systems via simulation of  user behaviour.
Userinternal
states
Motorstates
Perception
states
User
Active inference user agent
Userinternal
states
Motorstates
Perceptionstates
User
Active inference user agent
Sensor
states
Action
states
System
internalstates
System
Active inference system agent
Sensor
states
Action
states
System
internal
states
System
Active inference system agent
User
User
System
Sensor
states
Action
states
System
internal
states
System
Active inference system agent
Active inference "in the loop", applied at run-
time to improve interaction.
Sensor
states
Action
states
System
internal
states
System
Active inference system agent
Userinternal
states
Motor
states
Perception
states
User
Active inference user agent
One level of  recursive theory of  mind.
E
U S
E
U S
E
U S
E
U SI 
E
SU U 
No recursive theory of  mind.
User
Fig. 4. Different application modes of Active Inference. Active Inference can be used to simulate user behaviour, or joint user-
system behaviour (‚Äúoffline‚Äù). Alternatively, systems can be built that apply AIF in the interaction loop (‚Äúonline‚Äù). Increasing levels
of sophistication involve additional nested active inference agents, as in the reflective agent that operates using an internal AIF
simulator of a user, as shown in the lower box. E indicates environment. Dotted circles are non-AIF units.
Active Inference and Human‚ÄìComputer Interaction 9
‚Ä¢((U‚Äô) S)Mutual interaction We can model the whole interaction loop, where both the user and the system
are represented as AIF agents and the joint behaviour of the mutually interacting agents is simulated.
2.6.2 Online use of AIF in interactive systems. There are also configurations where the system or its interface are
represented by an active inference model interacting with a real user. These are:
‚Ä¢ U (S) Construction we can build an interactive system S that acts by implementing Active Inference. Such
a system would be a predictive, probabilistic interface. The user U is then a part of the environment but not
explicitly modeled.
‚Ä¢ U (I) STransduction We can construct an active inference agent Iwho lies between a user and an existing
system and mediates their interaction (neither of whom are assumed to be AIF agents). The user and the system
jointly form the environment of the mediating agent. Our earlier work [135] provides an example of such a
model.
2.6.3 Reflective Active Inference. All of the active inference models above can be augmented by explicitly incorporating
an active inference model of their partner within their generative forward model.
‚Ä¢ U (S (U‚Äô))Reflection We can construct an AIF interactive system S that incorporates an AIF model of a user
U.7 The interface plans actions informed by a forward model that includes a user model, that is in turn an AIF
model.
Such reflective interfaces are examples of mutual models. For the human to model what they will observe given
a specific action, they need to model the computer‚Äôs likely responses to their actions; which, in turn, are based on
the computer‚Äôs model of how the human will respond to its actions. Similarly the computer‚Äôs model of its path from
action to observation must include a model of the user‚Äôs likely response, which will also include the user‚Äôs model of the
computer. Mutual modelling and recursive theories of mind in interactive systems are reviewed by Keurulainen et al.
[77], who describe a series of levels of mutual modelling.8 This is shown in the lower box of Figure 4.
‚ÄòSophistication‚Äô is used in the economics literature to refer to having beliefs about one‚Äôs own or another‚Äôs beliefs, and
Friston et al. [46] point out that ‚Äúmost current illustrations of Active Inference can be regarded as unsophisticated or naive,
in the sense that they only consider beliefs about the consequences of action, as opposed to the consequences of action for
beliefs‚Äù. The ability to reason about beliefs will be important in interactive systems because the system has uncertain
beliefs about user goals and preferences, and a key goal of interaction is to reduce the uncertainty the system has about
these beliefs, so that it can act to support the user, given its knowledge about their goals and preferences.
3 Core elements of Active Inference in interaction
Section 2.2 introduced the key features of general AIF agents. This section explores how these general elements need to
be adapted to apply AIF thinking to HCI design, construction and analysis tasks. We develop each point in more detail
in the following linked subsections.
7The basic Construction model above changes from U (S)to U (S(U‚Äô))when the system Shas an explicit AIF model of the user. The mutual interaction
case becomes (U‚Äô (S‚Äô)) (S(U‚Äô)), and if we added a further level of mutual prediction it would become (U‚Äô (S‚Äô(U‚Äù))) (S (U‚Äô(S‚Äù))).
8Summarising the presentation in [77]: A Level 1 agent is invariant to the internal state of the partner agent. An agent which adapts its interaction
based on beliefs about the level 1 partner model is a Level 2 agent. A Level 3 agent acts conditional on second-order nested beliefs about its partner, e.g.
incorporating a model of their partner‚Äôs model of themselves. This would allow a human to infer the knowledge an AIF system has about them by the
actions it makes. A Level 4 system modelling the user at Level 3 could use Theory of Mind to understand the human‚Äôs responses, and can act in ways that
make the agent‚Äôs beliefs identifiable to the human. Level 5 users are able to infer the intention behind system actions.
10 Murray-Smith, Williamson, Stein
‚Ä¢ Agents As discussed in ¬ß2.4, we must decide which entities will be modelled or implemented as AIF agents:
users, systems, both, neither?
‚Ä¢ Environment We need to establish the role of the environment. Is it just a ‚Äútransmission medium‚Äù? Is the user
applying the system as an instrument to act upon or better sense the environment? Are both agents engaged in
fighting environmental flux cooperatively? We discuss this in ¬ß3.1.
‚Ä¢ Markov blankets ‚Äì Separating Agents from their Environment Identify the actions and sensing each
agent is capable of, to be able to define the boundaries between the user and system, the ‚ÄòMarkov blanket‚Äô. ¬ß3.2.
‚Ä¢ Forward model Construct a generative forward model. This is generally the most challenging implementation
step, and involves both building a model that can predict future states and synthesise the observations that
would occur under those conditions. In an interaction setting this includes models of human action, perception
and cognition, as well as the system‚Äôs sensing, computation and action. ¬ß3.3
‚Ä¢ Prediction and reasoning about the best action ¬ß3.4 Active Inference relies on these forward, predictive
models to estimate future states, and predicates actions on those future estimates, driven by a intrinsic goal to
reduce prediction error andminimise surprise. We need to frame the problem in terms of surprise minimisation
¬ß3.4.1. This is the most challenging conceptual step ‚Äì surprise minimisation is a powerful tool for thought, but
it is sometimes counter-intuitive to apply. Computationally, each agent generates candidate policies for action
sequences over their prediction horizon and selects one that minimises the Expected Free Energy.
‚Ä¢ Preference priors ¬ß3.5. An agent‚Äôs goals or desires are defined as a distribution ‚Äì a preference prior ‚Äì rather
than a single reference. We therefore must define a preference prior to shape the actions of the system‚Äôs agent
to align with the goal of the interaction design. While surprise minimisation provides a degree of intrinsic
motivation, aligning that with the intended purpose of the system requires explicit modelling of preferences.
This can be seen as a process of constructing attractors to drive the agent into preferable states.
‚Ä¢ Implement This will involve building/training the forward model, implementing a rollout policy evaluator,
and an inference mechanism to perform the Bayesian observation updates. We discuss the challenges involved
in this in ¬ß5.
3.1 The role of the environment
U S
E
SEU S EU
Cooperative Transmission medium Instrument
Fig. 5. The environment in an AIF‚ÄìHCI loop can be a) Something which is jointly controlled by a human and AIF system. b) A
transmission medium c) Something which the user observes or controls via the AIF system
Active Inference and Human‚ÄìComputer Interaction 11
Our approach in ¬ß2.6 differed from traditional presentations of Active Inference which describe a single agent in an
environment. In our mutual interaction models, the computer and human AIF agents become the environments for
each other. However, we may still need to explicitly model the wider environment beyond the simple human‚Äìcomputer
dyad.
3.1.1 Transmission. The simplest extension is to include the transmission aspects of the environment between the
system and user. This would involve forward models of the environmental impact from user action to system sensor
and from system display to user perception.
3.1.2 Intermediary instruments/actuators. Is the user applying the system as an intermediary instrument to act upon or
better sense the environment? Systems acting asintermediary instruments or actuators can support a user by transducing
user-unobservable information from the environment into human-compatible percepts (as a microscope does), or it can
act upon the environment at the behest of the user (as a robot arm does).9
3.1.3 Direct engagement with the environment. Can both agents engage with the environment directly in fighting
environmental flux cooperatively? For example, a smart speaker might be operated by a user to change the mood and
observable behaviour of a party, but the user can also interact directly with others in the party. A central heating system
application might be for controlling the temperature in a building, but the user can make other changes such as opening
or closing windows.
In some interactions with the environment, agents will act to change the environment in future-oriented ways
which maximise Expected Free Energy. This can be viewed as an example of niche construction , which is commonly
observed in biological agents [15, 101], where we can put effort into current actions which will simplify our future
environment and make our existence more predictable and controllable. Such environmental action and perception also
permits stigmergy [140], where the environment is configured as augmentation to cognition or as an indirect mode
of communication between the agents, as it can stimulate the same agent or its partner(s) to behave in a particular
manner. HCI examples include XR applications, or even simple clustering of documents on a desktop.
3.1.4 Multi-agent environments. The environment models discussed so far have not had goals or preferences. However,
in some cases the environment could include other humans or active, intelligent agents, which could be explicitly
modelled by expanding the dyadic relationships in ¬ß2.4 to multi-agent relationships, or the complexity could be just
kept as an undifferentiated part of the general environment forward model without representing individual agents.
3.2 Markov blankets ‚Äì separating agents from their environments
Finding meaningful boundaries between an agent and its environment is a core aspect of Active Inference, and is a
fundamental and controversial question of general interest for HCI, especially when AI assistants can take agency from
humans for certain tasks. We will discuss what Markov blankets are, how they change, and why that is of conceptual
and practical interest for HCI.
3.2.1 Markov Blankets. A Markov blanket defines the boundaries of a system in a statistical sense [109]. The existence
of a Markov blanket means that there are external states that are conditionally independent of internal states and vice
versa (Figure 6), and there are mature algorithms to support the inference of these states in Bayesian networks, e.g.
9Inverse instruments can also be imagined, where the human acts as a sensory organ for the system, relaying unobservable features of the environment,
or translating digital displays into physical actions that the computer is unable to actuate.
12 Murray-Smith, Williamson, Stein
[5, 110]. In AIF, a Markov blanket is the set of variables which mediate all statistical interactions between an agent
and its environment. Blanket states sit on the boundary, and within it we further distinguish sensory and active states.
Internal states only affect external states via active states. Similarly, external states must pass through sensory states to
affect internal states. This use of quantitative approaches to infer Markov blankets to represent the boundaries between
an agent and its environment, could potentially be used to infer boundaries within the human‚Äìcomputer interaction
loop.
Fig. 6. A Markov blanket statistically separates internal from external states; information flows exclusively via the active and sensory
states. Attention can be seen as selective weighting/disabling of blanket states (center). Complex agents may have hierarchical Markov
blankets (right).
3.2.2 Where are the boundaries in human‚Äìcomputer interaction? Hornb√¶k and Oulasvirta [67] highlight that different
approaches to interaction vary in how they portray the human‚Äìcomputer boundary, and how important that is for
analysis and design, and we review that literature and discussion in ¬ßA.1.2
Technology can change the boundary. By giving a user a tool or assistive technology we transform their action
and sensing capabilities. Modern tools will include context-sensitive algorithmic support from artificial intelligence,
augmenting the user cognitively, or relieving them of the need to attend to, or act on mundane tasks.10 In cases where
a user‚Äôs behaviour indicates they wish to hand over some share of autonomy to an intelligent tool, it effectively ‚Äòpulls
in‚Äô its Markov blanket, no longer controlling certain action states, or attending to (and having to predict the values of)
certain sensory states.11
As discussed in [65] the role of boundaries is a core and disputed aspect of competing theories of interaction, but they
give no concrete way to objectively analyse such boundaries. The dynamic reconfiguration of boundaries by intelligent
technologies can be associated with loss of agency, autonomy and freedom, motivating analytic tools which can infer
boundaries, and map how they change in different designs and contexts.
3.2.3 Dynamically changing boundaries and generalised tools. The Markov blanket is not necessarily identified with
the physical extent of the agent (the ‚Äúskin-bag‚Äù [25]). Consider a skilled craftsman with specialised tools, where does
the boundary lie when they perform a task? Kirchhoff et al. [79] argue that for complex systems to survive they need to
be able to hierarchically assemble Markov blankets of Markov blankets (Figure 6, right), and that these do not need
10Technologies such as deep neural networks and large language models are powerful ways for technical systems to interpret images, sound and text. This
complex algorithmic manipulation of sensor observations has the potential to dramatically extend our perception, cognition and actuation capabilities,
but it needs to be controllable by the user for successful interaction. Active Inference‚Äôs standard mechanisms can model and predict such algorithmic
transformations of the action‚Äìperception loop, allowing the agent to adjust its uncertainty in managing the complexity involved (e.g. how well does the
ML-based classification of a file‚Äôs music genre map to the current user‚Äôs subjective assessment?).
11We discuss how shared autonomy could arise in semi-autonomous driving in Appendix D.1.
Active Inference and Human‚ÄìComputer Interaction 13
to align with the physical boundaries of the organism, but can extend to include sensory or cognitive aids. Kirchhoff
and Kiverstein [80] say that the ‚ÄòMarkov blanket formalism specifies a boundary for the mind that is negotiable and
can move inwards and outwards over time. We show how the Markov blanket concept can ... include elements external to
the individual‚Äôs body‚Äô . As agents learn and adapt over their lives, change their environments and, in the human case,
create tools and technologies that change the boundaries themselves, Clark [28] argues for ‚Äúmultiplicity, flexibility,
and transformability of those boundaries‚Äù [25, 26], and proposes that ‚Äúconsidered as cognitive agents ... the sets of tools,
strategies, and devices (neural, bodily, and bio-external) that constitute us as the mindful beings we are undergoes dramatic
alteration. ‚Äùand that we are ‚Äúnature‚Äôs experts at knitting their own Markov blankets ... self-organizing processes that
constantly re-invent themselves, repeatedly re-defining their own cognitive, bodily, and sensory forms‚Äù [28].
In the previous section we discussed how shared autonomy could relieve people from mundane tasks. However,
relieving people of certain tasks is also effectively reducing their freedom, as it relates to that task. Satyanarayan and
Jones [120] view intelligence as agency (which they define as ‚Äòthe ability to meaningfully act‚Äô), and design as delegating
constrained agency. They illustrate dyads of ‚Äòco-fused agents‚Äô, with a dynamically shifting ‚Äòline-of-control‚Äô which can
give or take agency from the human or AI agent. They discuss the design challenge for drawing a line of control between
human and artificial agents, and highlight that ‚Äòthe form and balance of agency is highly dependent on cultural attitudes,
individual preferences, and the specific tasks that are being jointly performed‚Äô . In his attempt to compare the nature of
human and machine intelligence, Lawrence [86] argues that it is important for us to understand what the ‚Äòatomic‚Äô core
of humanity is, the elements which cannot be replaced by AI. So we can see that getting to the core of interaction
concepts is coupled to understanding how the effective boundaries between agents change during interaction. Mutual
interaction models, implemented with AIF agents allow us to computationally analyse interaction loops in a variety of
contexts and counterfactual conditions to gain insight into how boundaries change under different conditions, and
what implications that might have for design.
3.2.4 Attention. In agents with limited capacity, attention is a key means to allocate processing, and has revolutionised
deep learning [148]. We may be physically able to sense a stimuli but not be attending to it. Attention focuses transient
information flows specialised for the current task with changing Markov blankets associated with temporary conditional
statistical independencies (Fig. 6, centre).‚ÄòAttentional mechanisms may thus be seen as driving the formation and dissolution
of a short-lived Markov partitioning within the neural economy itself, temporarily insulating some aspects of on-board
processing from others according to the changing demands of task and context‚Äô [28]. We therefore need to model Markov
blankets that can grow and shrink depending on the mode of interaction, and the consequent shifts of attention.
3.2.5 Summary ‚Äì What Markov Blankets can do for HCI. Markov blankets have clear value as sharp intellectual tools
for segmenting functional roles in interactive systems. Markov blankets could bedetected; we can use formal statistical
measures to identify blankets from data,12 informing us how the boundary between human and computer shifts in
different contexts, offering a computational instrument to quantify what happens to users of sensory and cognitive
augmentative technology as their boundaries shift over time. 13
12How much data is needed depends on the task complexity. This is potentially challenging for small groups of users, but more feasible once instrumented
systems are deployed with end-users.
13We can also potentially define constraints on Markov blankets to support interaction. However, it is not yet clear how best to represent this in
interaction engineering, and to what degree it can be used to constrain adaptation or help support calibration.
14 Murray-Smith, Williamson, Stein
3.3 Forward and inverse models in interaction
Active Inference agents use predictive probabilistic generative models which can predict belief distributions over future
states. Predictive models propagate beliefs through time; observation models grant agents the power to dream of the
sensations they would expect in those states. Useful predictive models may be very impoverished simulacra of the
true environment; it is in no sense necessary (nor likely) that an AIF agent have a completely accurate model of its
environment.
In the interaction case we need forward models of human perception (what would the human perceive for a given
state of the world) and the computer‚Äôs sensing (what would the sensors read for a given human action). We also model
the human motor control system (e.g. how reliable muscle actions are, such as signal-dependent noise or fatigue effects),
and the reliability of the computer displays (taking into account environmental disturbances, such as bright sunlight
obscuring a screen, or audio displays being drowned out by loud sounds).
Generates
Forward model Inverse model
Time
 Time
Estimates
Sensor
observation
Observation
distribution
Hypotheticalmotion Estimated
 motion
Fig. 7. Imagine detecting a cursor swipe gesture. Forward models (left) hypothesise what sensations (pointer trajectories, below)
would be generated if a swipe were to be performed. Inverse models observe pointer trajectories and try to decode a specific swipe.
Active Inference makes extensive use of forward models, so it is worth reviewing the contrast between forward
and inverse modelling (Figure 7). The inverse problem is the problem of using observed measurements to infer latent
parameters that characterise a system. This is a key problem in experimental science and engineering [59, 138, 141].
The forward problem is using models and their latent parameters to predict the observed measurements. Murray-Smith
et al. [98] outline the role of forward and inverse modelling to support inference of human intention from sensors in
HCI. Causal, forward models tend to be easier to specify, simulate14 and to gather experimental data for than inverse
problems; but the inverse problem is what needs to be solved in an interaction context where we need to go from sensor
readings to an estimate of the intention of a user. Bayesian approaches offer a general way to invert systems from
forward models, using prior beliefs to constrain the solution set. Murray-Smith et al. [98] point out that the inversion
from sensing to intention may be robust to some variations in input, but very delicate in others because the inverse
problem is ill-posed. For example, a nonlinear effect is present in the forward model of a mobile radar sensor (Figure
8). As a hand moves away from the sensor we see a flattening response as the hand exceeds the range of the sensor.
While we can predict the forward values with precision, there are infinitely many solutions to the inverse problem in
saturated areas.
14In many cases, where the scientific knowledge is mature, scientists can simulate a system better than they can effectively observe it.
Active Inference and Human‚ÄìComputer Interaction 15
Reflector height
Sensor response
Mobile radar sensor
Reflecting finger
Saturation
Response curve
Fig. 8. An example of ill-posedness in sensing. A mobile mm-wave radar detects the proximity of a hand. Close to the device, it is easy
to resolve the true height of the hand from the radar signal. As the hand exceeds the sensitive range, inverting the signal becomes
increasingly ill-posed.
3.3.1 Active Inference does not need inverse models. Active Inference, in common with Bayesian approaches, sidesteps
some of these invertibility issues with its generative approach which tracks latent states of the environment. Active
inference loops are exclusively built around forward models, and Pio-Lopez et al. [111] observe that a distinguishing
feature is that active inference dispenses with inverse models: "In active inference, there is no inverse model or cost
function and the resulting trajectories are Bayes optimal ... This contrasts with optimal control, which calls on the inverse
model to finesse problems incurred by sensorimotor noise and delays. Inverse models are not required in active inference,
because the robot‚Äôs generative (or forward) model is inverted during the inference. ... optimal control formulations start with
a desired endpoint (consequence) and tried to reverse engineer the forces (causes) that produce the desired consequences. It
is this construction that poses a difficult inverse problem with solutions that are not generally robust ... Active Inference
finesses this problem by starting with the causes of movement, as opposed to the consequences. "
Furthermore, the closed-loop nature of the active inference-based interaction means that if there are regions of the
state space where the agent can end up uncertain, the agent implicitly acts to avoid these states to reduce its future
uncertainty and concomitant sensory surprise. However, there is no free lunch. The success of this inferential approach
will depend on the quality of the generative model and the computational cost of applying it.
3.3.2 Sensing for interaction. The impact of the previous section is that when using Active Inference, we do not build
models to map sensors onto unknown states; we build models to map unknown states onto sensors. An AIF agent
combines top-down prior information about likely causes of sensations, with bottom-up sensory stimuli, rather than
mere bottom-up transduction of sensory states. This is a constructive process where sensations test causal hypotheses
about how they were generated. The agent chooses actions which will, over time, reduce the level of surprise at its
sensory organs.
We anticipate that generative modelling will help to address future interaction challenges, where we will need to
deal with richer, high-dimensional, uncertain sensors which are less optimised for human input. Existing HCI methods
often lead to awkward and inefficient interactions, tightly bound to specific implementation details. This is unlikely to
16 Murray-Smith, Williamson, Stein
be resolved by minor tweaks to established interaction paradigms. Active inference implies fundamental rethinking of
what interaction is and how it should unfold.
3.4 The role of prediction and minimising surprise
An AIF agent does not react directly to sensation it perceives, but to its belief about the future states of the world. The
only role of observations is to correct estimates of beliefs about the environment.
3.4.1 Minimise surprise. Active Inference relies on the forward, predictive models discussed in ¬ß3.3 that estimatefuture
states, and predicates actions on those future estimates, driven by a intrinsic goal to reduce prediction error. The agent
avoids surprise by ascribing a value, the expected free energy (EFE) to future states and then selecting an action to
minimise the EFE, and thus expected future surprise. The EFE is an upper bound on surprise [41] that is computable. This
can be decomposed into two components (see eq. (4) in Appendix C): 1. the information gain characterises how much
more could be learned (the epistemic gain) about the environment in a hypothetical future; how much information could
leak in through the agent‚Äôs Markov blanket, and 2. the pragmatic value quantifies the degree to which the hypothesised
state of the environment agrees with the preference prior ‚Äì a measure of how ‚Äúgood‚Äù it is from the agent‚Äôs perspective.15
We can see these as a reflection of the classic exploration‚Äìexploitation trade-off, and their relative magnitudes determine
whether an agent‚Äôs behaviour is predominantly exploratory or exploitative. Paradoxically, the goal of minimising
surprise in the long run leads to curiosity-like information gathering behaviours in the short term. AIF agents will
naturally actively acquire information about their environment to mitigate the risk of future surprises. It is better to
turn on the lights in a dark room, even if what appears may be unpredictable, than to risk later surprise from that
which lies unseen.
Expected free energy summarises the value of all the paths through future consequences of an action in a single
number that bounds this surprise. An AIF agent is therefore continually engaged in running counterfactual, ‚Äòwhat if?‚Äô
simulations, scoring them consistently via EFE. A compelling aspect of Active Inference is that action and perception
are unified to cooperate in optimising this single objective, rather than being treated separately, which is an inelegance
in other approaches.16
An interesting observation is that actions based on EFE minimisation use predicted future outcomes as causes for
the decisions and actions in the present. This is relevant for interpretation of human actions, as people‚Äôs actions often
only make sense when explained by their anticipation of achieving some future state of the world. We relate this to
traditional causality discussions in appendix A.2.3.
3.4.2 Coping with latency. Interactive systems have notable delays, particularly in human perception and motor
execution. In systems with delays, we need to be able to predict to retain stability. The predictive element of Active
Inference simplifies dealing with and reasoning about latency in interaction. An AIF agent‚Äôs actions are driven purely
by its predictions of what will happen, and not directly by what is happening now. Friston [43] notes, for example, that
an Active Inference approach sidesteps some of the issues with modelling delay in human motor control, and compares
the approach to optimal control methods which also include forward models [142, 143].
3.4.3 Adaptation via curiosity supports diversity. The goal of an interaction engineer is to enable user and system to
communicate intention via interaction, accommodating diversity in human performance and preferences. An AIF agent
15There are other ways of decomposing free energy [105], p28-33, but this is the most insightful in this context.
16It should be noted that while we have focused on EFE as the most common bound on surprise, there are free energies other than EFE that also bound
surprise, such as the free energy of the expected future (FEEF). The specific choice of these quantities is discussed in detail in Millidge et al. [91].
Active Inference and Human‚ÄìComputer Interaction 17
uses predictive models to estimate the expected free energy, which imbues it with curiosity to test interaction options
(whether sensor interpretations, interface parameters, autonomy level or preference settings). The user experiences this
as a proactive system offering them personalised ways to interact. Such exploration makes a system more resilient to
inter-user variability, context or varying preferences. This comes at a cost, as generative models must acquire more
information to reduce their uncertainty about hidden states. Active Inference offers us a clear mathematical framework
for analysis of these trade-offs.
3.4.4 Prediction Horizons. Identifying appropriate prediction horizons helps prune the planning to make simulation
computationally tractable, and to terminate the recursion of mutual theories of mind (this is associated with the levels of
recursion for the mutual prediction discussed in ¬ß2.6.3). This will be an important practical parameter when designing
interactive systems using AIF models.
3.5 Preference priors
Hornb√¶k and Oulasvirta [67] highlight that ‚Äòmost [concepts of interaction] say little about how intentions are formed or
affected by interaction‚Äô . In AIF, intentions are encoded via preference priors17 over observations that the agent prefers
to sense or, alternatively, over states that the agent prefers to be in. The preference prior biases the agent‚Äôs model of its
environment, making it appear less surprising to transition into preferred states and more surprising to transition into
others. AIF uses cross-entropy to evaluate the alignment of estimated beliefs with preferences.18 This is a flexible but
easily computed measure of agreement. Preference priors are not necessarily static and can change over time as an
agent adopts different priorities. In a simple example, an AIF model of a pointing task might have a preference prior
that changes to favour a new target once a target has been selected.
3.5.1 Rewards are not preferences. AIF researchers emphasise the difference between objective rewards in RL and
surprise minimisation. Rewards are conceptually regarded as a property of the environment and are therefore assumed
to be static in nature. A subtle but important distinction is that rewards are related to outcomes, while preferences are
an attribute of an agent‚Äôs beliefs.19 Preference priors are commonly found to be more intuitive to set, without giving
name to an ideal. In standard AIF work focused on biological systems the preferences are often relatively easily-defined,
reflecting observations consistent with states that the agent naturally thrives in. Defining preferences for human
behaviour is less trivial, and we anticipate that as AIF‚ÄìHCI develops there will be significant research into eliciting and
calibrating preference priors.
3.5.2 Eliciting and learning preference priors. Preference priors can be learned from data, either online or offline. For
example Shin et al. [133] present an approach to learn a prior preference of an AIF agent from expert simulation. Sajid
et al. [118] generalise reward learning, in which preferences are themselves learned, ‚Äòlearning to prefer‚Äô, to Active
Inference. The interaction with an agent‚Äôs niche and preference learning is discussed in [15]. Sajid et al. [119] present
Pepper, an AIF approach to a reward-free preference learning mechanism which allows an agent to interact with the
environment and develop preferences that it acts to satisfy without an extrinsic reward signal.
17Sometimes simply referred to as ‚Äúpriors‚Äù. This overloaded use is confusing and we avoid it.
18As Alexander [3] (p.22) notes, it is much easier to define misfit than identify an objective norm: ‚ÄúWe should find it almost impossible to characterize a
house which fits its context. Yet it is the easiest thing in the world to name the specific kinds of misfit which prevent good fit. ‚Äù
19Pio-Lopez et al. [111] discuss this in the robotics context: "Active Inference dispenses with cost functions, as these are replaced by the robot‚Äôs (prior) beliefs
... replacing the cost function with prior beliefs means that minimizing cost corresponds to maximizing the marginal likelihood of a generative model"
18 Murray-Smith, Williamson, Stein
3.5.3 Misaligned preferences. The simple one-agent setting has a single set of prior preference distributions. In the
AIF‚ÄìHCI case, we can model prior preferences in both elements of the interacting dyad. These preferences should be
aligned, to ensure that the computer is acting in the interests of the human, as the purpose of the joint human-computer
system is normally to support the user in achieving their goals.
However, it is not necessary that preference distributions be mirrored perfectly. The timescale over which we evaluate
performance justifies differential preference prior structures. For example, in educational contexts, learning often
involves suffering in the short-term for longer-term payoff. Actions that benefit the user may become apparent after
many episodes of use, but the instructor agent would need preferences that appropriately shape behaviour within a
single episode of use. Preference may account for others or society generally, as in a courteous and safe autonomous
driving system or a preference to avoid expending energy. Misaligned preference may also arise because one of the
agents has lower-level capacities with which it must concern itself, such as a vehicle that needs to maintain stability.
4 Theoretical contributions of AIF‚ÄìHCI
We now discuss how Active Inference can offer a general theoretical framework for HCI, structuring our discussion
along dimensions used by Hornb√¶k et al. [66] to evaluate the contribution of a ‚Äòtheory of HCI‚Äô.
4.1 Predictive power ‚Äì inherent to AIF
As a computational simulation of all aspects of the interaction loop, AIF inherently supports prediction of observable
and latent states over time, at a range of levels of abstraction and timescales, and can extrapolate to generate predictions
beyond regions covered in experiments. Prediction in such a probabilistic approach also includes the prediction
uncertainty, which can be used both on- and off-line to improve the resilience of the interaction loop to disturbances, or
new contexts.
4.2 Explanatory power: Studying the interaction loop with model-based analytic tools
AIF can, from some very basic assumptions, provide coherent explanations for behaviours or phenomena observed in
HCI. A specific novel explanatory strength is formally analysing effective boundaries between humans and computers
via Markov blankets and using the models to measure agency or freedom in different contexts. AIF agents adapt their
behaviour when uncertain about their environment, and this can often be a key factor in interpreting and understanding
the behaviour of an intelligent agent, whether human or artificial. This is of particular value in modelling shared control
scenarios, where humans interact with AI algorithmic support.
The closed-loop, predictive structure of the Active Inference action‚Äìperception loop frames the human‚Äìcomputer
interaction problem through a lens distinct from that in most textbooks (which often have separate chapters on ‚ÄòInput‚Äô
and ‚ÄòOutput‚Äô), highlighting the irreducible interdependence of action and sensing. Section A.2.3 argues that interaction
cannot be fully understood either from users, technology or their environment in isolation. Fundamental aspects of
interaction can only be quantitatively modelled by considering the whole interaction loop.
4.3 Evaluation power
Active Inference uses explicit computational generative models of each component of the loop. This allows us to take
any state of the system and run counterfactual experiments to evaluate degrees of freedom available to the agent.
Active Inference and Human‚ÄìComputer Interaction 19
This can support conceptual breakthroughs and provide quantitative measures for concepts which were previously
challenging to represent, such as agency, engagement, interaction and freedom.20
As we discuss in more detail in ¬ßA.2.3, agency relates to the degree of control one has over their environment, but
it is tricky to model computationally. Friston et al. [49] argue that when cost functions (as used in optimal control)
are replaced with the free energy principle this grants agents a ‚Äúsense of agency‚Äù . Because Active Inference agents
form a distribution over the future actions they may take, they are directly computing their freedom to act. Their
reasoning naturally includes predictions of futures which are highly constrained and few actions are reasonable, and
other futures where the choice of action is broadly free. This highlights that the ‚Äòcauses‚Äô for predictive AIF agents are
often directly attributable to their modelled distribution on future states. This approach has the potential to give us
a more coherent theoretical basis for analysing interaction itself ‚Äì recently Hornb√¶k and Oulasvirta [67] observed
that ‚Äòthe term interaction is field-defining yet easily confused‚Äô .21 Pattisapu et al. [106] mapped emotional valence to
utility minus expected utility and arousal to the entropy of posterior beliefs. This, along with the measures of attention
discussed in ¬ß3.2.4 can be used to support new measures of engagement, where we can track which aspects of the
environment the user is sensitive to, and how that affects their control behaviour.
Interaction freedom can manifest itself in two ways: a free interaction loop should lead to the joint system being
able to reach many distinct preferred states (diverse ends). But a free interaction should also grant the user many ways
to reach states which are compatible with the user‚Äôs preferences (diverse means). Systems maximising freedom are
likely to have common features, such as the freedom to use sensors or actuators in different, personalised ways, and the
freedom to choose the level of autonomy in a system with algorithmic support. AIF is based on minimising long-term
surprise, but this does not force us into a narrow homeostasis; the principle in fact motivates novelty-seeking, curious
or exploratory behaviour, visiting possible states of the environment where it is uncertain to reduce future surprise. AIF
therefore provides a formal, rigorous framework for comparing competing adaptive models to expand users‚Äô freedom
of interaction [124, 131].
4.4 Guiding measurement
Many traditional HCI experimental methods use summary data such as time-to-completion or error rates for comparison.
In contrast, AIF uses generative modelling of phenomena at every level of granularity and can thus use a wide range
of empirical behavioural and sensor data to refine and calibrate its models. In addition, when using Active Inference
on the system side of things, the process of minimising the Expected Free Energy includes the information gain term
which automatically picks actions which balance exploitation with the highest epistemic gains to reduce model and
state uncertainty.
4.5 Informing design
Our task as interaction engineers is to fabricate the inference, sensors and display mechanisms such that the two
agents (human and computer) can interact constructively to achieve a goal. Relying primarily on forward models, AIF
concepts can help us reason offline in a data-efficient, model-based, computationally implementable fashion about the
20A model-based approach is arguably the only way to realise quantitative measures of concepts such as engagement or agency, because of the need to
perform counterfactual simulations of the consequences of different action options.
21It is instructive to consider how a theory of Active Inference in HCI would fit the classifications of theories of HCI provided by Oulasvirta and Hornb√¶k
[102]. Active Inference immediately fits their classes ‚ÄòInteraction as Control‚Äô and ‚ÄòInteraction as Information transmission‚Äô , but because of the consequences
of the predictive agent-focused approach, it would also incorporate Interaction as tool use, Interaction as Optimal Behaviour , and Interaction as Embodied
Action.
20 Murray-Smith, Williamson, Stein
interaction loop, and can be used online for real-time inference, adaptation and exploration. Design is hard, involving
many trade-offs. Forcing designers to decide everything in advance, in ignorance of the specific user, context and task
is excessively constraining. This can be mitigated by leaving some adaptation to deployment where users and systems
can negotiate how best to interact. It is also becoming clearer that purely offline development reaches a limit because
the historical data gathered with an earlier version of the system to calibrate the models no longer represent the reality
once the loop is closed with the new interface. In recommender systems, for example, a gap opened between research
and practice, due to the vulnerability of the traditional approach of testing on historical logs of user interactions [71, 72].
Recommender systems that perform well on historical data often rapidly deteriorate when they engage with real users.
Simulations with user models can help mitigate this risk at design [69, 156], but being able to actively adapt models
once deployed will often be critical to achieve closed-loop success.
5 Current challenges and next steps
5.1 General Challenges
Active Inference will face similar criticisms to computational simulation approaches to HCI. These include the cost
and complexity of developing models, and the (claimed) inability of models to adequately represent the cognitive and
perceptual complexity of humans, especially the sensitivity of behavior to details of context. Other critiques have
included the perceived failure of models to capture the physical and social context of interaction [97]. Both sides of
this argument agree that understanding humans is core to HCI and the argument is fundamentally about the level of
description involved. Sheridan emphasises the importance of denotative model descriptions [132], which minimise the
variability of interpretation, so that the field can agree what it is talking about. ‚ÄúThe process of modelling forces one to
think hard about the slice of nature under consideration, to ask and answer the question of what are the essential features of
the structure and function" . Modelling in AIF terms requires precise thinking that throws into sharp relief questions that
are often fuzzily-defined. Building generative models that validate against user behavior is an acid test of whether we
really understand an interactive system.
Humans are complex, social, diverse and ever-changing. The evolution of mobile, wearable and XR interaction means
environments for computing systems have become rapidly more diverse, and thus less knowable. These are challenges
to all simulation-based approaches. However, AIF systems do not require a complete and perfect representation of
the external environment. The agent‚Äôs form allows it to become a statistical model of its niche, embodying statistical
regularities of its world [28, 79]. A pair of AIF agents will bring their joint system into a mutually predictable region of
the state space, rather than having to accurately model a wide range of ‚Äòoutlier‚Äô behaviour.
Any adaptive intelligent system faces challenges in dealing with co-adaptation of users, and the need to account
for recursive theories of mind [77]. AIF faces these challenges, but its capacity to model both user and computer in an
interaction loop means that it can approach these problems computationally.
5.2 Active Inference Challenges
There are other challenges specific to Active Inference itself:
‚Ä¢ Computation AIF is computationally intensive, particularly in action rollouts, and engineering effort will
be needed to balance planning with real-time requirements of online applications, for example via amortised
algorithms [78]. Even in offline settings, computational developments like dynamic programming [107] and
deep learning approximations [107] will be required to realise agents with long predictive horizons.
Active Inference and Human‚ÄìComputer Interaction 21
‚Ä¢ Implementation AIF is an elegant theoretical framework, but we do not yet have the software libraries and
computational abstractions to build, reason about, and deploy interfaces. Current implementations of AIF
like RxInfer [7] and pymdp [60] are nascent and not yet easily applied to interaction problems. We also lack
developed workflows [23, 53] to apply Active Inference to interaction. We need principled, computational
human-centric engineering design to capture human behaviour and acquire the data to build preference priors
and forward models.
‚Ä¢ Preference modelling AIF eschews rewards for the flexibility of preference priors. Casting interaction problems
in terms of these novel approaches to preference is flexible but unfamiliar. This will require developments in
how preference priors should be elicited [133] and validated, and how generalised preferences can be fused
from individual preferences.
‚Ä¢ Uncertainty and prediction Representing uncertainty can make interfaces robust but users can struggle to
comprehend uncertainty or understand systems that represent it (Section A.3.1). Prediction is a vital component
of AIF and can mitigate latency in the interaction loop. But prediction can be unsettling for users. The predictive,
pro-active nature may make interfaces endowed with AIF models appear to be in the ‚Äòuncanny valley‚Äô [96].
5.3 Anticipated progress
What do we expect the immediate impact of AIF on HCI to be? Online use may be one of the first areas to show results,
particularly the transduction approaches introduced in 2.6 which mediates the interaction between users and systems.
First simulation results for these have already been presented [135]. Use of AIF in system construction is likely to first
appear as part of a more general system, where the AIF approach is used to improve a specific aspect of interaction,
while others are designed with classical approaches.
Offline use of AIF will be relevant for systems development and basic research. Descriptive AIF models of user
behaviour for standard interaction mechanisms are likely to be made available over the coming years and can be used
to test candidate systems designs. Simulating mutual interaction is a more advanced topic, requiring the complexity of
Active Inference to work for both agents in the dyad, and is likely to be initially more relevant for fundamental research
into interaction and agency. Reflection with mutually predicting AIF models and higher levels of recursion in mutual
mental models will push the conceptual and computational limits even further, but will provide a valuable framework
for the future to develop concepts and tools about hows humans interact with ever more intelligent machines.
6 Outlook
6.1 Active Inference in HCI
We introducedActive Inference to an HCI readership, and presented how this can be a coherent framework for the human‚Äì
computer interaction loop, representing users and systems with probabilistic generative models driven by intrinsic
goals of minimising surprise. It opens up connections between HCI and modern thinking in cognitive neuroscience,
philosophy and theoretical biology. Active Inference in HCI builds on model-basedcomputational interaction approaches,
but brings specific novelties in preference distributions, forward modelling, surprise minimisation, and associated
analytic techniques such as Markov blankets. There are extensive challenges to be addressed to bring Active Inference
into interaction: modelling of human cognitive and perceptual processes at a sufficient level; developing software tools
to build, evaluate and debug AIF models; and computational techniques to implement AIF efficiently.
22 Murray-Smith, Williamson, Stein
6.2 Why now?
Control-based approaches to HCI have been limited by the need to replicate human perception, especially visual
perception. Recent breakthroughs in near human-level perception via machine learning have changed this dramatically
[92]. However, Lake et al. [84] observe ‚Äòtruly human-like learning and thinking machines will have to reach beyond
current engineering trends in both what they learn and how they learn it. ‚Äô and propose combining the strengths of recent
machine learning advances with more structured Bayesian cognitive models. Cognitive science researchers have found
growing evidence of the benefits of this approach to structuring the research agenda, [56], and we anticipate it will also
be relevant for HCI. Active Inference provides a coherent framework for integrating the benefits of machine learning
into HCI. Computational advances have rendered the appealing theoretical properties of Active Inference manifestly
implementable. Theoretical developments in the fast-moving AIF community have developed the basic principle to a
sophisticated and widely-applicable theory in a remarkably short period of time.
6.3 Why is this exciting?
Active Inference has aroused extraordinary interest across diverse scientific fields. Partly, this is because it is an elegant
and internally-consistent theoretical model that unifies many strands of thought. It is compelling and satisfying from a
purely intellectual point-of-view. But it can also be expressed mathematically and directly implemented in code: it is
actionable at every level. Applying Active Inference in HCI stimulates new ways of thinking, and particularly once
we step out of the envelope of the conventional interaction modes where user behaviour and design have sedimented.
Active Inference offers promise in engineering interactions which are truly new; when exotic sensors are integrated;
when new modes of interaction are brought online; when new ML algorithms are developed.
We believe Active Inference will lead to resilient interactive systems that are far less fragile than we have come to
accept, especially in unpredictable contexts. Interfaces built on AIF principles will exhibit rich, adaptive and robust
behaviour with ‚Äúflexible means to achieve stable goals‚Äù . Active Inference is an online way of reasoning. AIF-powered
systems do not bake in pre-canned responses or pre-learned behaviours; they reason and act in the moment and provide
a level of dynamism that ML approaches to intelligent agents struggle with. As a tool for quantitatively modelling the
entire interaction loop, its promise is unparalleled. An AIF account of the interaction loop gives precise and actionable
routes to understand fundamental questions about interaction, agency, freedom, and engagement. In many ways, the
advantages and challenges mirror those of researchers applying Bayesian methods in cognitive science [56].
There is such a rich and deep seam of ideas that this paper can only lay out the first steps in the broad intellectual
landscape that lies before us. From visualisation to UI evaluation, user modeling to critical design, social signal processing
to augmented reality, these ideas offer a fresh and inspiring perspective for every research area in interaction.
Acknowledgments
The authors received funding from the Designing Interaction Freedom via Active Inference (DIFAI) ERC Advanced Grant
(proposal 101097708, funded by the UK Horizon guarantee scheme as EPSRC project EP/Y029178/1). R.M-S. also received
funding from EPSRC projects EP/T00097X/1, EP/R018634/1, and EP/T021020/1. We are also grateful for research gifts
from Google and Aegean Airlines. The authors would also like to thank the following colleagues for valuable feedback
on earlier drafts: Daniel Buschek, Karl Friston, Conor Heins, Kasper Hornb√¶k, Markus Klar, Antti Oulasvirta, Aini
Putkonen.
Active Inference and Human‚ÄìComputer Interaction 23
References
[1] Rick A. Adams, Stewart Shipp, and Karl J. Friston. 2013. Predictions not commands: active inference in the motor system. Brain Structure and
Function 218 (2013), 611‚Äì643.
[2] Stephen Adams, Tyler Cody, and Peter A Beling. 2022. A survey of inverse reinforcement learning. Artificial Intelligence Review 55, 6 (2022),
4307‚Äì4346.
[3] Christopher Alexander. 1964. Notes on the Synthesis of Form . Harvard University Press.
[4] Scott Alexander. 2018. God Help Us, Let‚Äôs Try To Understand Friston On Free Energy. https://slatestarcodex.com/2018/03/04/god-help-us-lets-try-
to-understand-friston-on-free-energy/.
[5] Constantin F Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D Koutsoukos. 2010. Local Causal and Markov
Blanket induction for causal discovery and feature selection for classification. Part I: algorithms and empirical evaluation. Journal of Machine
Learning Research 11, 1 (2010).
[6] Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. 2021. A Survey of Exploration Methods in Reinforcement
Learning. CoRR abs/2109.00157 (2021). arXiv:2109.00157 https://arxiv.org/abs/2109.00157
[7] Dmitry Bagaev, Albert Podusenko, and Bert de Vries. 2023. RxInfer: A Julia Package for Reactive Real-Time Bayesian Inference. Journal of Open
Source Software 8, 84 (April 2023), 5161. https://doi.org/10.21105/joss.05161
[8] Lisa Feldman Barrett. 2017. The Theory of Constructed Emotion: An Active Inference Account of Interoception and Categorization. Social
Cognitive and Affective Neuroscience 12, 1 (Jan. 2017), 1‚Äì23. https://doi.org/10.1093/scan/nsw154
[9] Michel Beaudouin-Lafon. 2004. Designing interaction, not interfaces. In Proceedings of the working conference on Advanced visual interfaces . 15‚Äì22.
[10] Dan Bennett, Oussama Metatla, Anne Roudaut, and Elisa D Mekler. 2023. How does HCI understand human agency and autonomy?. InProceedings
of the 2023 CHI Conference on Human Factors in Computing Systems . 1‚Äì18.
[11] Joanna Bergstr√∂m and Kasper Hornb√¶k. 2025. DIRA: A model of the user interface. International Journal of Human-Computer Studies 193 (2025),
103381.
[12] Joanna Bergstrom-Lehtovirta, David Coyle, Jarrod Knibbe, and Kasper Hornb√¶k. 2018. I really did that: Sense of agency with touchpad, keyboard,
and on-skin interaction. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1‚Äì8.
[13] Martin Biehl, Christian Guckelsberger, Christoph Salge, Sim√≥n C Smith, and Daniel Polani. 2018. Expanding the active inference landscape: more
intrinsic motivations in the perception-action loop. Frontiers in neurorobotics 12 (2018), 387187.
[14] Rafal Bogacz. 2017. A tutorial on the free-energy framework for modelling perception and learning. Journal of mathematical psychology 76 (2017),
198‚Äì211.
[15] Jelle Bruineberg, Erik Rietveld, Thomas Parr, Leendert van Maanen, and Karl J. Friston. 2018. Free-energy minimization in joint agent-environment
systems: A niche construction perspective. Journal of Theoretical Biology 455 (2018), 161‚Äì178.
[16] Mario Bunge. 2017. Causality and modern science . Routledge.
[17] Daniel Buschek and Florian Alt. 2017. ProbUI: Generalising Touch Target Representations to Enable Declarative Gesture Definition for Probabilistic
GUIs. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM, Denver Colorado USA, 4640‚Äì4653. https:
//doi.org/10.1145/3025453.3025502
[18] Gy√∂rgy Buzs√°ki. 2019. The Brain from Inside Out . Oxford University Press.
[19] Stuart Card, Thomas P. Moran, and Allen Newell. 1986. The model human processor- An engineering model of human performance. Handbook of
perception and human performance. 2, 45‚Äì1 (1986).
[20] Stuart K. Card, Thomas P. Moran, and Allen Newell. 1983. The psychology of human-computer interaction . Lawrence Erlbaum.
[21] Micah Carroll, Dylan Hadfield-Menell, Stuart Russell, and Anca Dragan. 2021. Estimating and penalizing preference shift in recommender systems.
In Proceedings of the 15th ACM Conference on Recommender Systems . 661‚Äì667.
[22] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J√©r√©my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
Lindner, Pedro Freire, et al. 2023. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint
arXiv:2307.15217 (2023).
[23] Suyog Chandramouli, Danqing Shi, Aini Putkonen, Sebastiaan De Peuter, Shanshan Zhang, Jussi Jokinen, Andrew Howes, and Antti Oulasvirta.
2024. A Workflow for building Computationally Rational Models of Human Behavior. Computational Brain & Behavior (2024).
[24] Nuttapong Chentanez, Andrew Barto, and Satinder Singh. 2004. Intrinsically motivated reinforcement learning. Advances in neural information
processing systems 17 (2004).
[25] A Clark. 2003. Natural-born cyborgs: Minds, technologies, and the future of human intelligence. Oxford University Press.
[26] Andy Clark. 2008. Supersizing the mind: Embodiment, action, and cognitive extension . OUP USA.
[27] Andy Clark. 2015. Surfing uncertainty: Prediction, action, and the embodied mind . Oxford University Press.
[28] Andy Clark. 2017. How to knit your own Markov blanket: Resisting the second law with metamorphic minds. In Philosophy and predictive
processing. Frankfurt am Main: MIND Group., 3.
[29] Andy Clark. 2023. The Experience Machine: how our minds predict and shape reality . Pantheon.
[30] Patricia Cornelio, Patrick Haggard, Kasper Hornbaek, Orestis Georgiou, Joanna Bergstr√∂m, Sriram Subramanian, and Marianna Obrist. 2022. The
sense of agency in emerging technologies for human‚Äìcomputer integration: A review. Frontiers in Neuroscience 16 (2022), 949138.
24 Murray-Smith, Williamson, Stein
[31] R√©mi Coulom. 2006. Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search. In Computers and Games . Vol. 4630. Springer Berlin
Heidelberg, Berlin, Heidelberg, 72‚Äì83. https://doi.org/10.1007/978-3-540-75538-8_7
[32] David Coyle, James Moore, Per Ola Kristensson, Paul Fletcher, and Alan Blackwell. 2012. I did that! Measuring users‚Äô experience of agency in their
own actions. In Proceedings of the SIGCHI conference on human factors in computing systems . 2025‚Äì2034.
[33] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. 2022. How Active Inference Could Help Revolutionise Robotics.
Entropy 24, 3 (2022), 361.
[34] Daniel Damb√∂ck, Martin Kienle, Klaus Bengler, and Heiner Bubb. 2011. The H-Metaphor as an Example for Cooperative Vehicle Driving. In
Human-Computer Interaction. Towards Mobile and Intelligent Interaction Environments , Julie A. Jacko (Ed.). Vol. 6763. Springer Berlin Heidelberg,
Berlin, Heidelberg, 376‚Äì385. https://doi.org/10.1007/978-3-642-21616-9_42
[35] Marc Deisenroth and Carl E Rasmussen. 2011. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th
International Conference on machine learning (ICML-11) . 465‚Äì472.
[36] Michelle Drouin, Daren H. Kaiser, and Daniel A. Miller. 2012. Phantom vibrations among undergraduates: Prevalence and associated psychological
characteristics. Computers in Human Behavior 28, 4 (2012), 1490‚Äì1496.
[37] Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, and Anca Dragan. 2020. AvE: Assistance via Empowerment. Advances in
Neural Information Processing Systems 33 (2020), 4560‚Äì4571.
[38] Johan Engstr√∂m, Ran Wei, Anthony D. McDonald, Alfredo Garcia, Matthew O‚ÄôKelly, and Leif Johnson. 2024. Resolving Uncertainty on the Fly:
Modeling Adaptive Driving Behavior as Active Inference. Frontiers in Neurorobotics 18 (March 2024). https://doi.org/10.3389/fnbot.2024.1341750
[39] Jean-Daniel Fekete, Niklas Elmqvist, and Yves Guiard. 2009. Motion-pointing: target selection using elliptical motions. In Proceedings of the SIGCHI
conference on Human factors in computing systems . 289‚Äì298.
[40] Frank O. Flemisch, Catherine A. Adams, Sheila R. Conway, Ken H. Goodrich, Michael T. Palmer, and Paul C. Schutte. 2003. The H-Metaphor as a
Guideline for Vehicle Automation and Interaction . Technical Memorandum 20040031835. NASA.
[41] Karl Friston. 2009. The Free-Energy Principle: A Rough Guide to the Brain? Trends in Cognitive Sciences 13, 7 (July 2009), 293‚Äì301. https:
//doi.org/10.1016/j.tics.2009.04.005
[42] Karl Friston. 2011. Embodied inference: or ‚ÄúI think therefore I am, if I am what I think ‚Äù. The implications of embodiment (Cognition and
Communication) (2011), 89‚Äì125.
[43] Karl Friston. 2011. What Is Optimal about Motor Control? Neuron 72, 3 (2011), 488‚Äì498.
[44] Karl Friston. 2012. A free energy principle for biological systems. Entropy 14, 11 (2012), 2100‚Äì2121.
[45] Karl Friston. 2013. Life as we know it. Journal of the Royal Society Interface 10, 86 (2013), 20130475.
[46] Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. 2021. Sophisticated inference. Neural Computation 33, 3 (2021),
713‚Äì763.
[47] Karl Friston, James Kilner, and Lee Harrison. 2006. A free energy principle for the brain. Journal of physiology-Paris 100, 1-3 (2006), 70‚Äì87.
[48] Karl Friston, J√©r√©mie Mattout, and James Kilner. 2011. Action understanding and active inference. Biological cybernetics 104 (2011), 137‚Äì160.
[49] Karl Friston, Spyridon Samothrakis, and Read Montague. 2012. Active Inference and Agency: Optimal Control without Cost Functions. Biological
Cybernetics 106, 8-9 (Oct. 2012), 523‚Äì541.
[50] Karl Friston, Spyridon Samothrakis, and Read Montague. 2012. Active inference and agency: optimal control without cost functions. Biological
cybernetics 106, 8 (2012), 523‚Äì541.
[51] Karl Friston, Philipp Schwartenbeck, Thomas FitzGerald, Michael Moutoussis, Timothy Behrens, and Raymond J Dolan. 2014. The anatomy of
choice: dopamine and decision-making. Philosophical Transactions of the Royal Society B: Biological Sciences 369, 1655 (2014), 20130481.
[52] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. 2009. Reinforcement learning or active inference? PloS one 4, 7 (2009), e6421.
[53] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian
B√ºrkner, and Martin Modr√°k. 2020. Bayesian workflow. arXiv preprint arXiv:2011.01808 (2020).
[54] Richard Langton Gregory. 1980. Perceptions as hypotheses. Philosophical Transactions of the Royal Society of London. B, Biological Sciences 290,
1038 (1980), 181‚Äì197.
[55] Miriam Greis. 2017. A Systematic Exploration of Uncertainty in Interactive Systems . Ph. D. Dissertation. University of Stuttgart.
[56] Thomas L. Griffiths, Nick Chater, and Joshua B. Tenenbaum. 2024. Bayesian models of cognition: reverse engineering the mind . MIT Press.
[57] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J. Russell, and Anca Dragan. 2017. Inverse reward design.Advances in neural information
processing systems 30 (2017).
[58] Dylan Hadfield-Menell, Stuart J. Russell, Pieter Abbeel, and Anca Dragan. 2016. Cooperative inverse reinforcement learning. Advances in neural
information processing systems 29 (2016).
[59] Per Christian Hansen. 2010. Discrete inverse problems: insight and algorithms . SIAM.
[60] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl Friston, Iain Couzin, and Alexander Tschantz. 2022. Pymdp: A Python
Library for Active Inference in Discrete State Spaces. Journal of Open Source Software 7, 73 (May 2022), 4098. https://doi.org/10.21105/joss.04098
arXiv:2201.03904 [cs, q-bio]
[61] Jakob Hohwy. 2013. The predictive mind . OUP Oxford.
[62] Erik Hollnagel. 1999. Modelling the controller of a process. Trans Inst MC 2, 4/5 (1999), 163‚Äì170.
Active Inference and Human‚ÄìComputer Interaction 25
[63] Erik Hollnagel. 2017. The diminishing relevance of Human‚ÄìMachine Interaction. In The Handbook of Human-Machine Interaction . CRC Press,
417‚Äì429.
[64] Erik Hollnagel and David D Woods. 2005. Joint cognitive systems: Foundations of cognitive systems engineering . CRC press.
[65] Kasper Hornb√¶k and Antti Oulasvirta. 2017. What Is Interaction?. InProceedings of the SIGCHI Conference on Human Factors in Computing Systems .
[66] Kasper Hornb√¶k, Per Ola Kristensson, and Antti Oulasvirta. In press, 2024. Introduction to Human‚ÄìComputer Interaction . Oxford University Press.
[67] Kasper Hornb√¶k and Antti Oulasvirta. 2017. What is interaction?. In Proceedings of the 2017 CHI conference on human factors in computing systems .
5040‚Äì5052.
[68] Andrew Howes, Jussi PP Jokinen, and Antti Oulasvirta. 2023. Towards machines that understand people. AI Magazine 44, 3 (2023), 312‚Äì327.
[69] Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A configurable
simulation platform for recommender systems. arXiv preprint arXiv:1909.04847 (2019).
[70] Aleksi Ikkala, Florian Fischer, Markus Klar, Miroslav Bachinski, Arthur Fleig, Andrew Howes, Perttu H√§m√§l√§inen, J√∂rg M√ºller, Roderick Murray-
Smith, and Antti Oulasvirta. 2022. Breathing life into biomechanical user models. In Proceedings of the 35th Annual ACM Symposium on User
Interface Software and Technology . 1‚Äì14.
[71] Amir H. Jadidinejad, Craig Macdonald, and Iadh Ounis. 2020. Using Exploration to Alleviate Closed Loop Effects in Recommender Systems. In
Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR ‚Äô20) .
Association for Computing Machinery, New York, NY, USA, 2025‚Äì2028. https://doi.org/10.1145/3397271.3401230
[72] Amir Hossein Jadidinejad, Craig Macdonald, and Iadh Ounis. 2022. The Simpson‚Äôs Paradox in the Offline Evaluation of Recommendation Systems.
ACM Trans. Inf. Syst. 40, 1 (2022), 4:1‚Äì4:22. https://doi.org/10.1145/3458509
[73] William James, Frederick Burkhardt, Fredson Bowers, and Ignas K Skrupskelis. 1890. The principles of psychology . Vol. 1. Macmillan London.
[74] Shunichi Kasahara, Jun Nishida, and Pedro Lopes. 2019. Preemptive action: Accelerating human reaction using electrical muscle stimulation
without compromising agency. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1‚Äì15.
[75] Shunichi Kasahara, Kazuma Takada, Jun Nishida, Kazuhisa Shibata, Shinsuke Shimojo, and Pedro Lopes. 2021. Preserving agency during electrical
muscle stimulation training speeds up reaction time directly after removing EMS. In Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems . 1‚Äì9.
[76] Rafael Kaufmann, Pranav Gupta, and Jacob Taylor. 2021. An active inference model of collective intelligence. Entropy 23, 7 (2021), 830.
[77] Oskar Keurulainen, Gokhan Alcan, and Ville Kyrki. 2024. The Role of Higher-Order Cognitive Models in Active Learning. arXiv preprint
arXiv:2401.04397 (2024).
[78] Diederik P. Kingma and Max Welling. 2013. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114 (2013), 1‚Äì14. https://doi.org/10.
48550/arXiv.1312.6114
[79] Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. 2018. The Markov blankets of life: autonomy, active inference
and the free energy principle. Journal of The royal society interface 15, 138 (2018), 20170792.
[80] Michael D Kirchhoff and Julian Kiverstein. 2021. How to determine the boundaries of the mind: A Markov Blanket proposal. Synthese 198, 5 (2021),
4791‚Äì4810.
[81] A.S. Klyubin, D. Polani, and C.L. Nehaniv. 2005. Empowerment: A Universal Agent-Centric Measure of Control. In 2005 IEEE Congress on
Evolutionary Computation , Vol. 1. 128‚Äì135. https://doi.org/10.1109/CEC.2005.1554676
[82] David C. Knill and Alexandre Pouget. 2004. The Bayesian Brain: the role of uncertainty in neural coding and computation.TRENDS in Neurosciences
27, 12 (2004), 712‚Äì719.
[83] P. O. Kristensson and Th. M√ºllners. 2021. Design and Analysis of Intelligent Text Entry Systems with Function Structure Models and Envelope
Analysis. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1‚Äì12.
[84] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people.
Behavioral and brain sciences 40 (2017), e253.
[85] Alexander Lavin, Hector Zenil, Brooks Paige, David Krakauer, Justin Gottschlich, Tim Mattson, Anima Anandkumar, Sanjay Choudry, Kamil Rocki,
Atƒ±lƒ±m G√ºne≈ü Baydin, et al. 2021. Simulation Intelligence: Towards a New Generation of Scientific Methods. arXiv preprint arXiv:2112.03235 (2021).
[86] Neil D Lawrence. 2024. The Atomic Human: Understanding ourselves in the age of AI . Random House.
[87] Hannah Limerick, David Coyle, and James W. Moore. 2014. The experience of agency in human-computer interactions: a review. Frontiers in
human neuroscience 8 (2014), 643.
[88] Alianna J. Maren. 2017. How to Read Karl Friston (in the Original Greek). https://www.aliannajmaren.com/2017/07/27/how-to-read-karl-friston-in-
the-original-greek/.
[89] D T McRuer and H R Jex. 1967. A review of quasi-linear pilot models. IEEE Trans. on Human Factors in Electronics 8, 3 (1967), 231‚Äì249.
[90] Beren Millidge, Anil Seth, and Christopher L Buckley. 2021. Predictive coding: a theoretical and experimental review.arXiv preprint arXiv:2107.12979
(2021).
[91] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. 2021. Whence the Expected Free Energy? Neural Computation 33, 2 (Feb. 2021),
447‚Äì482. https://doi.org/10.1162/neco_a_01354
[92] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K.
Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529‚Äì533. https:
26 Murray-Smith, Williamson, Stein
//doi.org/10.1038/nature14236
[93] Hee-Seung Moon, Seungwon Do, Wonjae Kim, Jiwon Seo, Minsuk Chang, and Byungjoo Lee. 2022. Speeding up inference with user simulators
through policy modulation. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . 1‚Äì21.
[94] Hee-Seung Moon, Yi-Chi Liao, Chenyu Li, Byungjoo Lee, and Antti Oulasvirta. 2024. Real-time 3D Target Inference via Biomechanical Simulation.
In Proceedings of the CHI Conference on Human Factors in Computing Systems . 1‚Äì18.
[95] Hee-Seung Moon, Antti Oulasvirta, and Byungjoo Lee. 2023. Amortized inference with user simulations. In Proceedings of the 2023 CHI Conference
on Human Factors in Computing Systems . 1‚Äì20.
[96] Masahiro Mori. 1970. Bukimi no tani [The Uncanny Valley]. Energy 7 (1970), 33‚Äì35.
[97] Roderick Murray-Smith, Antti Oulasvirta, Andrew Howes, J√∂rg M√ºller, Aleksi Ikkala, Miroslav Bachinski, Arthur Fleig, Florian Fischer, and Markus
Klar. 2022. What simulation can do for HCI research. Interactions 29, 6 (2022), 48‚Äì53.
[98] R. Murray-Smith, J. H. Williamson, and F. Tonolini. 2022. Human‚ÄìComputer Interaction Design and Inverse problems. In Bayesian Methods for
Interaction and Design , J. H. Williamson, A. Oulasvirta, P. O. Kristensson, and N. Banovic (Eds.). Cambridge University Press.
[99] Vivek Myers, Evan Ellis, Benjamin Eysenbach, Sergey Levine, and Anca Dragan. 2024. Learning to Assist Humans without Inferring Rewards. In
ICML 2024 Workshop on Models of Human Feedback for AI Alignment .
[100] Andrew Y. Ng and Stuart Russell. 2000. Algorithms for inverse reinforcement learning.. In ICML, Vol. 1. 2.
[101] John Odling-Smee. 2024. Niche Construction: How Life Contributes to Its Own Evolution . MIT Press.
[102] Antti Oulasvirta and Kasper Hornb√¶k. 2022. Counterfactual thinking: What theories do in design. International Journal of Human‚ÄìComputer
Interaction 38, 1 (2022), 78‚Äì92.
[103] Antti Oulasvirta, Jussi P.P. Jokinen, and Andrew Howes. 2022. Computational rationality as a theory of interaction. In ACM CHI‚Äô22 Proceedings of
the CHI Conference on Human Factors in Computing Systems .
[104] Antti Oulasvirta, Per Ola Kristensson, Xiaojun Bi, and Andrew Howes. 2018. Computational interaction . Oxford University Press.
[105] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. 2022. Active Inference. MIT Press.
[106] Candice Pattisapu, Tim Verbelen, Riddhi J. Pitliya, Alex B. Kiefer, and Mahault Albarracin. 2024. Free Energy in a Circumplex Model of Emotion. In
International Workshop on Active Inference (IWAI) .
[107] Aswin Paul, Noor Sajid, Lancelot Da Costa, and Adeel Razi. 2024. On Efficient Computation in Active Inference. Expert Systems with Applications
253 (2024), 124315.
[108] Stephen J. Payne and Andrew Howes. 2022. Adaptive interaction: A utility maximization approach to understanding human interaction with
technology. Springer Nature.
[109] J Pearl. 1988. Probabilistic Reasoning in Intelligent Systems . Morgan Kaufmann.
[110] Jean-Philippe Pellet and Andr√© Elisseeff. 2008. Using Markov Blankets for causal structure learning. Journal of Machine Learning Research 9, 7
(2008).
[111] L√©o Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo. 2016. Active inference and robot control: a case study. Journal of The Royal
Society Interface 13, 122 (2016), 20160616.
[112] William T. Powers. 1973. Behavior: The Control of Perception . Aldine, Chicago.
[113] Deepak Ramachandran and Eyal Amir. 2007. Bayesian Inverse Reinforcement Learning.. In IJCAI, Vol. 7. 2586‚Äì2591.
[114] Rajesh PN Rao and Dana H Ballard. 1999. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field
effects. Nature neuroscience 2, 1 (1999), 79‚Äì87.
[115] Michael B. Rothberg, Ashish Arora, Jodie Hermann, Reva Kleppel, Peter St Marie, and Paul Visintainer. 2010. Phantom vibration syndrome among
medical staff: a cross sectional survey. BMJ 341 (2010).
[116] James A. Russell. 1980. A Circumplex Model of Affect. Journal of personality and social psychology 39, 6 (1980), 1161.
[117] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. 2021. Active inference: demystified and compared. Neural computation 33, 3 (2021),
674‚Äì712.
[118] Noor Sajid, Panagiotis Tigas, and Karl Friston. 2022. Active inference, preference learning and adaptive behaviour. In IOP Conference Series:
Materials Science and Engineering , Vol. 1261. IOP Publishing, 012020.
[119] Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas, and Karl Friston. 2021. Exploration and preference satisfaction trade-off in
reward-free learning. In ICML 2021 Workshop on Unsupervised Reinforcement Learning .
[120] Arvind Satyanarayan and Graham M. Jones. 2024. Intelligence as Agency: Evaluating the Capacity of Generative AI to Empower or Constrain
Human Action. An MIT Exploration of Generative AI (2024). https://doi.org/10.21428/e4baedd9.2d7598a2
[121] Vera J. Sauer, Sabrina C. Eimler, Sanaz Maafi, Michael Pietrek, and Nicole C. Kr√§mer. 2015. The phantom in my pocket: Determinants of phantom
phone sensations. Mobile Media & Communication 3, 3 (2015), 293‚Äì316.
[122] Felix Schoeller, Mark Miller, Roy Salomon, and Karl J Friston. 2021. Trust as Extended Control: Human-Machine Interactions as Active Inference.
Frontiers in Systems Neuroscience (2021), 93.
[123] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR
abs/1707.06347 (2017). arXiv:1707.06347 http://arxiv.org/abs/1707.06347
[124] Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald, Martin Kronbichler, and Karl J Friston. 2019. Computational
mechanisms of curiosity and goal-directed exploration. Elife 8 (2019), e41703.
Active Inference and Human‚ÄìComputer Interaction 27
[125] Julia Schwarz, Scott Hudson, Jennifer Mankoff, and Andrew D. Wilson. 2010. A Framework for Robust and Flexible Handling of Inputs with
Uncertainty. In Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology . ACM, New York New York USA, 47‚Äì56.
[126] Julia Schwarz, Jennifer Mankoff, and Scott Hudson. 2011. Monte Carlo Methods for Managing Interactive State, Action and Feedback under
Uncertainty. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology . ACM, Santa Barbara California USA,
235‚Äì244.
[127] Julia Schwarz, Jennifer Mankoff, and Scott E. Hudson. 2015. An Architecture for Generating Interactive Feedback in Probabilistic User Interfaces.
In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . ACM, Seoul Republic of Korea, 2545‚Äì2554.
[128] Anil Seth. 2021. Being you: A new science of consciousness . Penguin.
[129] Ajay Seth, Michael Sherman, Jeffrey A. Reinbolt, and Scott L. Delp. 2011. OpenSim: a musculoskeletal modeling and simulation framework for in
silico investigations and exchange. Procedia Iutam 2 (2011), 212‚Äì232.
[130] Anil K Seth. 2015. The Cybernetic Bayesian Brain. In Open MIND, T. Metzinger & J. M. Windt (Ed.). Open MIND. Frankfurt am Main: MIND Group.
[131] Anil K Seth and Karl J Friston. 2016. Active interoceptive inference and the emotional brain. Philosophical Transactions of the Royal Society B:
Biological Sciences 371, 1708 (2016), 20160007.
[132] Thomas B. Sheridan. 2016. Modeling Human‚ÄìSystem Interaction: Philosophical and Methodological Considerations, with Examples . John Wiley &
Sons.
[133] Jin Young Shin, Cheolhyeong Kim, and Hyung Ju Hwang. 2022. Prior preference learning from experts: Designing a reward with active inference.
Neurocomputing 492 (2022), 508‚Äì515. https://doi.org/10.1016/j.neucom.2021.12.042
[134] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. Defining and characterizing reward gaming. Advances in Neural
Information Processing Systems 35 (2022), 9460‚Äì9471.
[135] Sebastian Stein, John H. Williamson, and Roderick Murray-Smith. 2024. Towards Interaction Design with Active Inference: A Case Study on Noisy
Ordinal Selection. In 5th International Workshop on Active Inference (IWAI 24) .
[136] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press.
[137] Daisuke Tajima, Jun Nishida, Pedro Lopes, and Shunichi Kasahara. 2022. Whose touch is this?: understanding the agency trade-off between
user-driven touch vs. computer-driven touch. ACM Transactions on Computer-Human Interaction 29, 3 (2022), 1‚Äì27.
[138] Albert Tarantola. 2005. Inverse problem theory and methods for model parameter estimation . Vol. 89. SIAM.
[139] Alex Taylor. 2015. After interaction. interactions 22, 5 (2015), 48‚Äì53.
[140] Guy Theraulaz and Eric Bonabeau. 1999. A Brief History of Stigmergy. Artificial life 5, 2 (1999), 97‚Äì116.
[141] A N Tikhonov and V Y Arsenin. 1977. Solutions of Ill-posed problems . Winston, Washington DC.
[142] Emanuel Todorov. 2004. Optimality principles in sensorimotor control. Nature neuroscience 7, 9 (2004), 907‚Äì915.
[143] Emanuel Todorov and Michael I Jordan. 2002. Optimal feedback control as a theory of motor coordination. Nature neuroscience 5, 11 (2002),
1226‚Äì1235.
[144] Dari Trendafilov. 2017. An information-theoretic account of human‚Äìcomputer interaction . Ph. D. Dissertation. University of Glasgow.
[145] Dari Trendafilov and Roderick Murray-Smith. 2013. Information-theoretic characterization of uncertainty in manual control. Proceedings - 2013
IEEE International Conference on Systems, Man, and Cybernetics, SMC 2013 March (2013), 4913‚Äì4918. https://doi.org/10.1109/SMC.2013.835
[146] Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. 2021. Optimal policies tend to seek power. In Proceedings
of the 35th International Conference on Neural Information Processing Systems . 23063‚Äì23074.
[147] Beatrix Vad, Daniel Boland, John Williamson, Roderick Murray-Smith, and Peter Berg Steffensen. 2015. Design and Evaluation of a Probabilistic
Music Projection Interface. In 16th Int. Society for Music Information Retrieval Conference (ISMIR) . 134‚Äì140.
[148] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. Advances in neural information processing systems 30 (2017).
[149] John P Veillette, Pedro Lopes, and Howard C Nusbaum. 2023. Temporal Dynamics of Brain Activity Predicting Sense of Agency over Muscle
Movements. Journal of Neuroscience 43, 46 (2023), 7842‚Äì7852.
[150] Eduardo Velloso, Marcus Carter, Joshua Newn, Augusto Esteves, Christopher Clarke, and Hans Gellersen. 2017. Motion correlation: Selecting
objects by matching their movement. ACM Transactions on Computer-Human Interaction (TOCHI) 24, 3 (2017), 1‚Äì35.
[151] Eduardo Velloso and Carlos H. Morimoto. 2021. A probabilistic interpretation of motion correlation selection techniques. In Proceedings of the 2021
CHI Conference on Human Factors in Computing Systems . 1‚Äì13.
[152] Kazuyoshi Wada and Takanori Shibata. 2007. Living with seal robots‚Äìits sociopsychological and physiological influences on the elderly at a care
house. IEEE transactions on robotics 23, 5 (2007), 972‚Äì980.
[153] Daryl Weir. 2014. Modelling Uncertainty in Touch Interaction . Ph. D. Dissertation. University of Glasgow.
[154] J. Williamson and R. Murray-Smith. 2004. Pointing without a pointer. In ACM SIG CHI . ACM, 1407‚Äì1410.
[155] John H. Williamson. 2022. An Introduction to Bayesian Methods for Interaction Design. In Bayesian Methods for Interaction and Design . Cambridge
University Press.
[156] Yaxiong Wu, Craig Macdonald, and Iadh Ounis. 2023. Goal-oriented multi-modal interactive recommendation with verbal and non-verbal relevance
feedback. In Proceedings of the 17th ACM Conference on Recommender Systems . 362‚Äì373.
28 Murray-Smith, Williamson, Stein
A Related work
We review the theory of Active Inference and contrast it with closely related interaction theories. Active Inference is an
example of a closed-loop, feedback-based approach to modelling agents. We therefore review cybernetic approaches.
Perceptual Control Theory focusses on how actions change an agent‚Äôs perceptions. Empowerment-based methods
(Sec. A.2.2) use information theory to create probabilistic models of the channel describing the agent‚Äôs action‚Äìperception
loop. Closed-loop systems make it harder to determine causality (Sec. A.2.3). These challenges also affect users‚Äô sense of
agency in interaction loops.
Active Inference approaches to HCI are a specific example of Computational Interaction (Sec. A.3), and lean heavily
on Bayesian approaches to the interaction loop [155]. They are also closely related, but distinct from, the recent work
in reinforcement learning (RL) in HCI (Section A.4), as well as simulation-based approaches (Section A.3.2).
A.1 Active Inference
The core ideas of Active Inference were originally developed by Friston [47] under the name ‚Äúthe free energy principle‚Äù .
Friston and his collaborators extensively published on applications of Active Inference in biology [44, 45], motor control
[1, 42], robotics [33, 111] and neuroscience [14, 51]. Active Inference has also seen growing interest in consciousness
research [128] and philosophy of mind [ 27, 29]. These theories of consciousness posit that the mind continuously
constructs its own experience of reality and updates it via discrepancies with real sensory input. The first textbook on
the subject, Parr et al. [105] (p57) makes the case that Active Inference is a unifying theory which encapsulates earlier
frameworks from cybernetics, neuroscience and psychology. Specifically, they argue that Active Inference unites and
extends three theoretical approaches:
(1) Enactive theories of life and cognition, which emphasise the self-organisation of behaviour as agents interact
with their environments.
(2) Cybernetic, or control-based approaches where behaviour is teleological, i.e. is seen as being driven by goals.
(3) Agents contain predictive models of their environment which guide their perception and action. These ap-
proaches are compatible with active perception [54] and top-down predictive coding [61, 114]
In neuroscience, predictive coding/predictive processing theory postulates that the brain is constantly generating and
updating a mental model of the environment [61, 90]. The model predicts input signals from the senses that are then
compared with the actual input signals from those senses. Predictive coding is member of a wider set of theories that
follow the Bayesian brain hypothesis [82]. A recent overview of applications of Bayesian inference in cognitive science
is given in [56]. Seth [130] expands the predictive processing approach in hisCybernetic Bayesian Brain work, placing an
emphasis on predictive modelling of internal physiological states and engaging with enactive and embodied cognitive
science approaches. Buzs√°ki [18] advocates that the brain‚Äôs fundamental function is to induce actions and predict the
consequences of those actions to support its owner‚Äôs survival. Examples of predictive perception in interaction settings
include phantom vibrations from undergraduates‚Äô phones [36] or pagers for medical interns [115, 121].
A.1.1 Active Inference in HCI. Although it has not yet been widely adopted, the first explorations of Active Inference
in HCI are being published. A recent example of its application in HCI is [135], which implemented an AIF approach for
1-of-ùëÅ selection, a fundamental building block of interactive systems, in the context of noisy sensors (relevant for Brain
Computer Interfaces), and the authors formulated the interface as an independent agent charged with facilitating the
Active Inference and Human‚ÄìComputer Interaction 29
flow of information, acting as an active transducer22 able to reason about the environment and user characteristics to
optimise this flow, and therefore used Active Inference to enable reliable selection to be made with unreliable sensors.
Other early explorations are emerging in viewing trust as extended predictive control, such that it could be applied
to analyse trust between humans and technological artifacts [122], and in multi-agent collective intelligence at multiple
scales [76]. However, to date there have been no publications reviewing the broad potential for Active Inference in HCI.
A.1.2 Boundaries in HCI. Hornb√¶k and Oulasvirta [67] highlight that different approaches to interaction vary in
how they portray the human‚Äìcomputer boundary. Hollnagel [63] highlights that the very name human‚Äìcomputer
interaction assumes that there is a clear boundary between human and machine, and that ‚Äúsystem boundaries are
well-defined, internal and external interactions are similar, and that humans and machines are reactive ... these assumptions
... are not tenable for intractable systems with tight couplings to their environment‚Äù . So, because interaction links human
and computer, it makes it challenging to analyse them separately. Beaudouin-Lafon[9] argues ‚ÄúDesigning interaction
rather than interfaces means that our goal is to control the quality of the interaction between user and computer: user
interfaces are the means, not the end. ‚Äù . This is taken up further in Taylor [139] and, as discussed in ¬ßA.2.3, has been a
concern in engineering since the 1960‚Äôs, e.g. the crossover effect in flight control [89]. Bergstr√∂m and Hornb√¶k [11]
argue that the user interface is currently a fuzzy concept, saying ‚ÄúWe also think that it is useful to separate some aspects
of the user and some aspects of the UI‚Äù and propose a model composed of four elements: Devices, Interaction Techniques,
Representations, and Assemblies (DIRA).
A.2 Cybernetics, closed loops, perceptual control theory, agency, and empowerment
A.2.1 Perceptual control theory. One cybernetic approach to modelling agent behaviour related to active inference
is Perceptual Control Theory (PCT). Powers [112] argues that we control our ‚Äòinputs‚Äô, our perception, rather than our
‚Äòoutputs‚Äô or motor actions. Closed-loop interaction techniques inspired by PCT were used in HCI contexts by Williamson
and Murray-Smith [154], which were subsequently further developed by others [39, 150]. An explicitly probabilistic
interpretation is given by Velloso and Morimoto[151]. Parr et al. [105] observed "While in both Active Inference and
perceptual control theory it is a perceptual (and specifically a proprioceptive) prediction that controls action, the two theories
differ in how control is operated. In Active Inference but not perceptual control theory, action control has anticipatory or
feedforward aspects" .
A.2.2 Empowerment. Another probabilistic, closed-loop approach to analysing interaction is ‚Äòempowerment‚Äô, proposed
by Klyubin et al. [81], which combines control theory and information theory to measure how much information an
agent can send from their actuators, through their environment to their perception (the channel capacity). An agent
which cannot perceive the impact of its actions has low empowerment. Maximising empowerment is a behavioural
heuristic for selecting actions. This was applied to interaction in [144, 145].
A.2.3 Causality and agency in closed-loops. Human predictive abilities mean that human behaviour is deeply inter-
twined with the systems they control. This is not easy to compartmentalise in the style of a classical control system
block diagram. For example, McRuer and Jex [89] demonstrated in their crossover model research that human pilots
adapt their behaviour to shape the properties of the whole closed-loop, changing their control behaviour when the
properties of the aircraft change. The joint cognitive systems approach used by Hollnagel and Woods [64] highlights the
22An example of the Transduction model in Section 2.4.
30 Murray-Smith, Williamson, Stein
need to examine the joint human‚Äìsystem interaction, rather than only the individual blocks.23 We will return to this
when trying to define boundaries in interactive systems, in Section 3.2.2.
The mutually predictive nature of interaction loops makes it challenging to reason about causality. Bunge [16] sees
interaction as reciprocal causation , as in the gravitational attraction of two bodies, and proposed approaches based on
a special type of causal relationship: mutual determination , rather than a traditional causal determination approach.
Interaction cannot be divided and attributed to a human or a system alone. This makes analysis of interactive systems
more challenging, and also affects users‚Äô perceptions of agency.
Bennett et al. [10] and Limerick et al. [87] review how the HCI literature understands agency and autonomy , because
of the importance of these for informing design, giving users a sense of ownership and control and the relevance for the
modern opportunities provided by artificial intelligence. Satyanarayan and Jones[120] reviews the role of the concept of
agency in interaction with Generative AI systems. [12, 32] explore users‚Äô perception of ownership of actions. Cornelio
et al. [30] look at agency in emerging technologies. Kasahara et al. [74, 75], Tajima et al. [137] and Veillette et al. [149]
present a series of experiments examining how timing of interventions such as electrical muscle stimulation affect the
user‚Äôs perception of agency. Friston et al. [50] highlight how in active inference a sense of agency can be linked to a
probabilistic representation of control which is independent of the actual actions emitted, but which allows the agent
to evaluate the consequences of possible actions. As pointed out in section 3.4.1, by conditioning current actions on
future predicted distributions, once the past history has been incorporated into the generative model and an initial
state estimate, Active Inference has a causal structure which is based on future predicted states, rather than past events.
A.3 Computational interaction
Computational interaction uses algorithms and computationally implemented mathematical models to predict, explain
and enhance interactive behaviour [104]. It includes formal representations of designs as well as predictive models of
users, environment and technical systems.
A.3.1 Uncertainty and probabilistic interfaces. Active Inference puts uncertainty at the heart of interaction, and uses
probabilistic, Bayesian methods to manipulate and represent it. Schwarz and her collaborators investigated probabilistic
interfaces based on Monte Carlo sampling in a series of papers [ 125‚Äì127], showing that modelling uncertainty can
improve robustness. Buschek and Alt [17] created a framework for probabilistic touch interfaces using hidden Markov
models to improve resilience in touch-based UIs. Weir[153] shows how appropriate probabilistic modelling of uncertainty
in interaction can improve interpretation of user intention. Greis‚Äôs thesis presents a wide-ranging review of the role of
uncertainty in interaction [55], including the challenges in communicating uncertainty to users.
A.3.2 Simulation in HCI. Active Inference is predicated on simulation models. In HCI, pioneering work in simulations
was driven by Card et al. [20], whose Model Human Processor divided the user aspect into cognitive, motor-behavioural
and perceptual components [19]. The increasing use of computational models has led to a greater role of simulation
[97] in HCI. The term Simulation Intelligence , as proposed in [ 85] involves the development and integration of the
key algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. The
original paper focused on other areas of science and engineering, but their methods are highly relevant to the complex
modelling challenges of HCI. Kristensson and M√ºllners [83] use modelling to replace extensive experimentation in
optimising text entry system parameters.
23Hollnagel [62] views the human as a model of the process they are controlling.
Active Inference and Human‚ÄìComputer Interaction 31
OpenSim enables the modelling, simulating, controlling, and analysis of the neuromusculoskeletal system [ 129].
Ikkala et al. [70] used a realistic biomechanical simulation of a human body, learning muscle-actuated control policies
based on perceptual feedback in interaction tasks with physical input devices, which allowed modelling of more realistic
interaction tasks with cognitively plausible visuomotor control. This approach shares the model-driven approach to
modelling the human motor system, perceptual system and the sensing system, but differs from AIF in the reward
structure, inference mechanisms and learns a direct control policy rather than optimising over possible future states.
Moon et al. [94] used simulated models which matched the accuracy of human 3D selections to speed up selection
and reduce errors. Computational issues of human simulation models are addressed in [93, 95].
A.3.3 Utility maximisation and bounded rationality. Related approaches involve utility maximisation [108]. Oulasvirta
et al. [103] propose ‚ÄòComputational Rationality‚Äô as a theory of interaction. The core assumption is that users act in
accordance with what is best for them, given the limits imposed by their cognitive architecture and their experience of
the task environment. This theory can be expressed in computational models that explain and predict interaction. It
offers a principled, model-based, basis for algorithms to drive both inference and planning in cooperative agents [68].
A.4 Reinforcement Learning
Reinforcement Learning (RL) [ 136] is concerned with learning agent behavior that maps directly from observable
state to (a probability distribution over) next actions such that, in expectation, a cumulative reward (value or utility)
is maximised. Value maximisation is performed using an estimated value function in model-free RL [123], or using a
model of the environment in model-based RL [35]. RL relies on manually specifying an auxiliary exploration objective
(e.g., ùúñ-greedy, entropy regularisation or intrinsic motivation) to trade-off exploration and exploitation over the course
of training [6], which often requires careful fine-tuning in practice. In contrast, free energy minimisation in Active
Inference provides a Bayes-optimal trade-off between exploration and exploitation [117]. In RL, behavior is shaped by
specifying rewarding state-action pairs, which turns out to be hard to do correctly in practice, and goals may change
over time [21]. Approaches for training RL agents without explicit reward function (Inverse RL) include behavior
cloning, reward function learning [2, 100, 113], learning from human feedback [22], and maximising empowerment
[37, 99]. Mis-specified rewards lead to unintended, undesirable behavior [134, 146] and have motivated methods for
reward refinement [57, 58]. An active inference agent is encouraged to generate preferred outcomes in proportion to
their probabilities, which is distinct from repeatedly producing the most valuable ones.
In RL, latent states are commonly treated as environment stochasticity instead of being inferred, as is done in
Active Inference. In this case, the only mechanism for adapting behavior to unobservable changes is policy retraining.
Model-free RL agents have to visit safety-critical states before it can learn to avoid them. By contrast, model-based RL
and Active Inference can quantify the risk of visiting those states and plan to avoid them. The combination of Bayesian
inference, Bayes-optimal exploration-exploitation trade-offs, and safe exploration makes Active Inference appealing
for modelling interaction with humans. In principle, all RL algorithms can be recast as active inference algorithms
with some aspects removed, [52] and close parallels can be drawn to RL with intrinsic motivation [13, 24] and reward
learning [133].
32 Murray-Smith, Williamson, Stein
Environment
Markov blanket
Observationot+1
at ActionPlan
Predict Correct
Next timestep
p(st) p(st+1|at) p(st+1|
ot+1, at)
Agent
p(st+1)
Actuation Sensing
(1.a)
(1.b) (1.e)
(1.d)
(1.c)
(1.f)
Fig. 9. An Active Inference agent, embedded within its environment. Beliefs are recursively updated using prediction and Bayesian
updates, while actions are planned by rolling out hypothetical futures. The annotation labels refer to the discussion in Section B.
Figure 2 shows a diagram of how all of these components fit together.
B An introduction to the Active Inference algorithm
B.1 Predict-correct
The gross behaviour of an AIF agent is akin to a Bayesian filter (such as a Kalman filter) with an additional planning step
that evaluates a tree of possible future actions and scores them according to expected free energy. The agent follows
this pattern:
‚Ä¢ It begins with a distribution over possible states ùëù(ùë†ùë°)(Fig. 9 (1.a)) representing how it thinks the environment
might be configured at time ùë°.
‚Ä¢ It evaluates possible futures to decide on an action ùëéùë° (see Plan below).
‚Ä¢ It executes this action ùëéùë° on the environment (Fig. 9 (1.f)).
‚Ä¢ It uses its forward model to predict what the state of the world will be after this action. ùëù(ùë†ùë°+1|ùëéùë°,ùë†ùë°)(Fig. 9
(1.c))
‚Ä¢ It collects an observation from the environment, ùëúùë°+1.
‚Ä¢ It uses its observation model to correct its estimate of the state of the world based on the actual observation
ùëù(ùë†ùë°+1|ùëéùë°,ùë†ùë°,ùëúùë°+1), by performing a Bayesian belief update using its estimated state as the prior and the obser-
vation as the evidence to update on. (Fig. 9 (1.d)). This update is classically performed with variational methods,
though it does not have to be.
‚Ä¢ This becomes the prior belief distribution ùëù(ùë†ùë°+1)for the next step of the reasoning process
Active Inference does not prescribe the specific algorithms to implement its component parts: forward models may be
first principles algorithms, deep neural networks such as VAEs[78], transformers[148] or any other learning algorithm;
Active Inference and Human‚ÄìComputer Interaction 33
(2.a)
Rollout
EFE
Softmax Action
sampler
Action
distribution
at
p(st)
p(at)
(2.b)
(2.c)
Fig. 10. The planning phase involves rolling out sequences of possible future actions, assigning them expected free energy scores and
then sampling actions based on these scores.
inference can be performed with variational, exact or Monte Carlo methods; rollouts can use fixed policy sequences,
exhaustive search or Monte Carlo Tree Search [31].
B.2 Plan
Planning (Fig. 9 (1.b)) involves deciding upon an immediate action by estimating the consequences of all future states
and potential actions consequent to it. In practice, the tree of future states is pruned to a limited set, for example using
fixed time horizon.
‚Ä¢ The rollout phase enumerates possible future action sequences (Fig. 10 (2.a)).
‚Ä¢ Each imminent action is assigned an expected free energy (EFE) (see Rollout below)
‚Ä¢ These scores are negated and transformed into a probability distribution using softmax (Fig. 10 (2.b)).
‚Ä¢ An imminent action is selected by drawing a sample from this distribution (Fig. 10 (2.c)).
B.3 Rollout
During rollout, policies24 are evaluated and scored according to their expected free energy.
‚Ä¢ For each possible imminent action ùëéùë° (or sample them, if the actions are unbounded). (Fig. 11 (3.a))
‚Ä¢ For ùëò until some time horizon ùëá:
‚Äì For each action ùëéùë°+ùëò
‚àó Predict Using the forward model, estimate the distribution over hypothetical future statesùëù(ùë†ùë°+ùëò|ùëéùë°+ùëò).
(Fig. 11 (3.c))
‚àó A free energy is computed for each future state (Fig. 11 (3.b)). See Free energy below.
‚àó The search branches over the new set of available actions for the next timestep ùëéùë°+ùëò+1.
‚Äì Then we average the values over all of these futures, grouped by the immediate next action ùëéùë° ‚Äì giving the
expected free energy (EFE) for that action. (Fig. 11 (3.b))
24Policies are simply action sequences in the Active Inference lexicon, rather than functions mapping states to actions, as in RL.
34 Murray-Smith, Williamson, Stein
Time 
horizon
p(st)
at=0
at=1
at=2
(3.b)
(3.c)
(3.d)
(3.a)
EFE[at=0]
EFE[at=1]
EFE[at=2]
Fig. 11. Rollout traverses the tree of future actions. Each future state has a free energy computed. The expected free energy for an
imminent action is the average of all free energies over all consequent states.
B.4 Free energy
The free energy plays a fundamental role in Active Inference. It is a value that bounds the ‚Äúsurprise‚Äù of a belief
distribution. It is computed by:
‚Ä¢ Comparing the predicted state distributionùëù(ùë†ùë°)to a preference priorùëû(ùë†ùë°)(Fig. 12) to get thepragmatic value
of a state.25 The preference prior is the key component that defines the behaviour of the agent.
‚Ä¢ Synthesizing observations that would be plausible under the predicted state distribution (Fig. 12 (4.d))
‚Ä¢ Computing the information gain for those hypothetical observations (see below)
‚Ä¢ Averaging all of the information gains to estimate the expected information gain (Fig. 12 (4.e))
‚Ä¢ Summing the pragmatic value and the information gain to form the free energy for this hypothesised state
B.5 Information gain
Information gain is a measure of how much could be learned from observations in future hypothesised states. This
obviously depends on a model that can synthesise plausible observations from state distributions. This generative
approach is a distinctive element of Active Inference. The information gain can be computed by synthesising an
observation (Fig. 13 (5.a)), performing a Bayesian update (Fig. 13 (5.b)) and then comparing the observation-updated
state distribution to the original state distribution (Fig. 13 (5.c)). This process can then be iterated to reliably estimate
the expectation of the information gain.26
C Mathematical definition
Formally, we assume an Active Inference agent interacts with an environment in discrete timesteps that we denote by
index ùëñ. In each timestep, the environment is in a particular unobservable state ùë†ùëñ ‚ààS and the agent interacts with its
environment by choosing an action ùëéùëñ ‚ààA. The effect of action ùëéùëñ on the environment is modeled by a probability
distribution over successor states ùëÉ(ùë†ùëñ+1 |ùë†ùëñ,ùëéùëñ)commonly referred to as the transition model . The new environment
state can only be observed by the agent via a sensor state ùëúùëñ ‚ààùëÇ. The relationship between unobservable states ùë†ùëñ and
25This comparison is the cross-entropy between the state distribution and the preference prior.
26In some cases the (expected information gain of the) observation distribution can be computed analytically without sampling.
Active Inference and Human‚ÄìComputer Interaction 35
p(sT)
preference pragmatic
freeenergy
FE[sT]
Mean
exp.
inf.gain
inf. gain
inf. gain
inf. gain
(4.a) (4.b) (4.c)
(4.d)
(4.e)
Fig. 12. The free energy of a predicted future state is a combination of how well the hypothesised distribution over states corresponds
to the preferences of the agent (pragmatic value) and how much the agent expects to be able to learn from the environment in this
state (information gain), estimated by sampling many synthetic observations.
Information gain
Dream Perceive
Compare
KL divergence
p(sT) o'T p(sT|o'T)
(5.a) (5.b)
(5.c)
Belief
Fig. 13. The information gain for a future hypothesised state is computed by synthesising an observation that might be observed,
performing a Bayesian update (exactly as the agent would if it really encountered this observation) and then computing how much
the distribution over states has changed.
observations ùëúùëñ is modeled by a conditional probability distribution ùëÉ(ùëúùëñ |ùë†ùëñ), commonly referred to as the observation
model or emission distribution . The Active Inference agent holds a belief ùëÑùëñ(ùë†), i.e. a probability distribution over
plausible states of the environment, which is initialised from a prior belief ùëÑ0 and updated using Bayesian filtering.
Upon selecting an action ùëéùëñ, the agent predicts the next environment state by updating its belief using its transition
model as in Equation (1). Upon making a new observation ùëúùëñ, the agent corrects its prediction using its observation
model as in Equations (2) and (3).
36 Murray-Smith, Williamson, Stein
ùëÑ(ùë†ùëñ)=
‚à´
ùëÉ(ùë†ùëñ |ùë†ùëñ‚àí1,ùëéùëñ‚àí1)ùëÑ(ùë†ùëñ‚àí1|ùëúùëñ‚àí1)dùë†ùëñ‚àí1 predict (1)
ùëÑ(ùë†ùëñ|ùëúùëñ)= ùëÉ(ùëúùëñ |ùë†ùëñ)ùëÑ(ùë†ùëñ)
ùëÉ(ùëúùëñ) correct (2)
ùëÉ(ùëúùëñ)=
‚à´
ùëÉ(ùëúùëñ |ùë†ùëñ)ùëÑ(ùë†ùëñ)dùë†ùëñ normalising constant (3)
The Active Inference agent‚Äôs behavior is informed by a potentially time-varying prior preference distribution ùëÉùëê
ùëñ (ùëúùëñ)
that characterises the distribution over sensor states the agent would prefer to observe. The agent selects actions by
approximating the expected free energy ùê∫(ùúã)over a range of alternative sequences of actions ùúã : (ùëéùëñ+1,...ùëé ùëñ+ùëá)with
some fixed time horizon ùëá, referred to as policies, as in Equation (4). Note that ùëÑ(ùë†ùëò)and ùëÑ(ùë†ùëò|ùëúùëò)are hypothetical
beliefs the agent anticipates to hold in the future, imagining that it followed the sequence of actions in ùúã, that the
environment follows the agent‚Äôs internal transition model ùëÉ(ùë†ùëñ+1 |ùë†ùëñ,ùëéùëñ), and that the environment will be observed
according to the agent‚Äôs internal observation model ùëÉ(ùëúùëñ |ùë†ùëñ). Because these beliefs are inferred from imagined futures,
the correction step (2) is skipped when constructing the sequence of hypothetical beliefsùëÑ(ùë†ùëò). This step is instead only
used to estimate the expected information gain. The agent defines a probability distribution ùëÉ(ùúã)over policies using
the softmax over negative expected free energy (6), samples a policy from this distribution, and executes the first action
of the sampled policy. It is sometimes convenient to define preferencesùëÉùëê(ùë†ùëò)over latent states of the environment
instead of sensor states, particularly when some aspects of the environment‚Äôs state space are known and represented in
the transition model. In this case, the expected free energy can be approximated using Eq. (5).
ùê∫(ùúã)‚âà 1
ùëá
ùëñ+ùëá‚àëÔ∏Å
ùëò=ùëñ+1
‚àíEùëÉ(ùëúùëò |ùë†ùëò )ùëÑ(ùë†ùëò )[ùê∑KL (ùëÑ(ùë†ùëò|ùëúùëò)‚à•ùëÑ(ùë†ùëò))]
|                                                 {z                                                 }
Information gain
‚àíEùëÉ(ùëúùëò |ùë†ùëò )ùëÑ(ùë†ùëò )

ln ùëÉùëê
ùëò(ùëúùëò)

|                              {z                              }
Pragmatic value
expected free energy (4)
‚âà1
ùëá
ùëñ+ùëá‚àëÔ∏Å
ùëò=ùëñ+1
EùëÑ(ùë†ùëò )
h
ùê∑KL

ùëÑ(ùë†ùëò)
ùëÉùëê
ùëò (ùë†ùëò)
i
|                                   {z                                   }
Risk
‚àíEùëÑ(ùë†ùëò )[ln ùëÉ(ùëúùëò |ùë†ùëò)]
|                        {z                        }
Ambiguity
(5)
ùëÉ(ùúã)= exp (‚àíùê∫(ùúã)))
Œ£ùúã‚Ä≤exp (‚àíùê∫(ùúã‚Ä≤)) policy distribution (6)
The agent interacts with the environment by executing the steps above in a closed feedback loop. First, it initialises a
prior belief over environment states. Second, it uses its observation from the environment to correct its belief using (2).
Then, it selects an action using Equations (4) and (6). Finally, it predicts the next environment state (1). This prediction
is used as the prior belief for the next state and the cycle repeats.
D Vignettes
This appendix presents three short vignettes illustrating how Active Inference could be used in practical human‚Äì
computer interaction problems. They are intended to make concrete some of the abstract concepts involved in Active
Inference, and to highlight the challenges involved in thinking about interaction design from this perspective.
The vignettes represent different points on the spectrum of use-cases. Semi-autonomous driving is a safety-critical
application in a variable, challenging environment, with a clear task structure, emphasizing shared autonomy issues
bringing sensing and skill of the driver together with the sensing and processing of the car. The soft companion robot
Active Inference and Human‚ÄìComputer Interaction 37
highlights hedonic and well-being focussed interactions with little formal task structure, the importance of uncertain
models of emotion, and the importance of engaging and disengaging appropriately. The intelligent music loudspeaker
highlights the challenge of low-dimensional interactions with enormous amounts of data, and the use of artificial
intelligence to predict human subjective responses.
We anticipate that readers may disagree with our suggestions, and in such brief descriptions of subtle tasks a brief
analysis will inevitably appear superficial. However, we believe that reasoning about how an AIF agent should be
designed for tasks like these can inspire new ways of thinking about the problem. The answer to the question ‚ÄòWhat
should the preferences of an AIF-based interactive agent be?‚Äô is often far from trivial, and can force us to think deeply
about the design task, and potentially enable us to find new perspectives for design.
D.1 Vignette 1: Semi-autonomous driving U (S (U‚Äô))
Scenario We imagine a vehicle with a driver support system, short of autonomous driving but with a repertoire of
autonomous capabilities like cruise control, lane following, automated merging and route guidance. In this scenario, we
assume that the system acts under Active Inference and has an AIF-based model of the user.
Markov blanket The system can set the steering, acceleration and braking input to the vehicle. It can sense the
road environment via LIDAR and the car‚Äôs position and motion (via GNSS, IMUs and odometry). It can also observe
the driver through the inputs from the steering wheel, brake and accelerator pedals. The system‚Äôs model assumes the
driver senses the environment around them (other vehicles, road markings) visually, as well as sensing acceleration
forces. The system‚Äôs model driver can turn the steering wheel and press the accelerate or brake pedals. We assume that
other road users are merely part of the environment.27
Forward model The car needs to make several predictions: what the vehicle itself will do (e.g. turn left 5¬∞); what
the mobile elements of the environment will do (e.g. another driver approaches); and what the user will do (e.g. turn
steering wheel to neutral). It further must predict how these will be sensed. If the user is concerned by a car approaching
on the right, how will the control inputs change? What will its LIDAR show if the car closes distance? If the accelerator
is engaged, how will the IMUs respond? As a safety-critical system a real car will require hardcoded fallback behaviours
(wholly outside of the inference process) that protect the vehicle from danger. But the vehicle‚Äôs predictive model should
include knowledge of those fallback behaviours so that it can model when its own control will be suppressed, just as a
biological organism has awareness of its own reflexes.
Preference prior The system assumes the user model has a preference prior that (a) prefers safe and comfortable
vehicle states (even, legal speed; appropriate distance to other drivers; steady turns) and (b) prefers to go to a certain
destination. Without a preference to go to a destination, the car might minimise surprise entirely by refusing to leave its
garage. The car‚Äôs own preference prior is to align with the user‚Äôs preferences, though it may have additional lower-level
preferences such as to minimise mechanical wear.
How is surprise minimised? The car‚Äôs autonomous function works to reduce surprise at the most basic level;
e.g. its preference over a stable road speed should imply consistent measurements from odometry. Deviations (e.g.
caused by changing road grade) should be planned for and acted upon to preserve the homeostasis of cruise control. It
also needs to minimise surprising inputs from the driver. The driver thrashing the wheel about unexpectedly is one
form of surprise and this behooves the car to carry out the implied maneuvers of the driver. For example, changing
lane smoothly when the driver steers left by anticipating the control. The H-metaphor [34, 40] proposed a model for
27An obvious extension (section 3.1.4) would be to model them as AIF agents [38].
38 Murray-Smith, Williamson, Stein
vehicular control inspired by a horse‚Äôs response to its rider that addresses how shared autonomy could be developed.
When control is definitive and meaningful, the horse does as it is bidden. When there is no meaningful control, the
horse maintains comfortable homeostasis, keeping to sensible routes and returning to known locations. In much the
same way, an AIF-based vehicle will execute commands compatible with its model but will autonomously act once its
predictions indicate a lack of agency on the part of the driver; conversely, an increase in agency expressed through
definitive but unpredicted control inputs will lead to a natural reduction of driver support.
The pragmatic values (the value derived from the preference prior) cause the car to be attracted to routes that will
take it to what it believes the driver‚Äôs destination to be. Driver inputs to make navigational changes counter to the car‚Äôs
route planning should be minimised, so the car should not only take a route that leads them to their destination, but
one that the car believes the driver will expect to be taken ‚Äì and not a clever detour through an industrial area.
Why is Active Inference appropriate? Driving is clearly a task where predictive models are vital. Driving involves
predictions on the scale of milliseconds to predict vehicle dynamics, on the scale of seconds for how other road users
will behave, and on the scale of minutes for how route decisions will impact navigation. Likewise, uncertainty is
clearly relevant. Vehicle dynamics are relatively predictable but other drivers are not. Minimising surprise at the
vehicle level has obvious correlates with safe driving behaviour, but the reflective component of surprise minimisation
is particularly interesting here. Autonomous driving supplants control that the driver used to have to provide and
taking on this workload without constraining what the driver can do is a challenge in shared autonomy. Reflective
surprise minimisation supports this handover. The vehicle plans to minimise the surprise from driver controls, which it
predicts based on a user model that wants to minimise surprise about what they in turn observe, given their intentions.
This is intrinsic motivation for the car to establish what the driver‚Äôs intentions are and to follow them correctly and
visibly. When the driver‚Äôs inputs are poorly predicted, one way for the car to minimise future surprise is to temporarily
propagate inputs more directly; ‚Äútightening the reins‚Äù in the H-metaphor sense, and making the driver more responsible
for direct control.
D.2 Vignette 2: A soft companion robot U (S)
Scenario We imagine an artificial companion fulfilling roughly the role of a domestic cat, for example supporting older
populations who cannot care for a live animal [152].
Markov blanket Physically, the imaginary device is a robot with a pliable exoskeleton, limited but high-degree-
of-freedom actuation that can flex its soft exterior, and a range of surface-based touch sensors distributed over its
body.
Preference prior What does it prefer? It prefers its companion human to be happy and relaxed. It might have a
dimensional model of its companion‚Äôs emotional state (e.g. an arousal-valence model [ 116]).28 Its preference prior
would then be close to zero for emotional coordinates representing sadness, anger, agitation, etc.
Forward model The robot makes predictions over the mental state of the human it supports, driven by a forward
model that relates sensing and actuation to emotional responses. This in turn depends on a predictive model of how its
movements affect its own sensors; for example, a physical model of contact that can predict that flexing against an
opposing surface will increase activity at those touch sensors pressed against the surface.
How is surprise minimised? The robot minimises surprise, where it is surprised by having an unhappy human. Its
actions will be motivated by a combination of keeping the user in preferred mental states and monitoring what those
28While modelling emotion might appear challenging, AIF approaches have made significant strides in computational modelling emotional responses.
[8, 106]
Active Inference and Human‚ÄìComputer Interaction 39
are. As a top-level behavior, it may act to stabilise what it believes to be a comforting state ("keep moving gently"); it
may act to stimulate a bored or disengaged user ("move in an amusing way") or back off from a tired or irritated user
("go to sleep"). But it may also act to elicit how the companion feels, or even if their companion is present ‚Äì ("squirm
and see if something happens").
At a lower level, it needs to act to make the most of its own body. It might contort itself such that its sensory
apparatus ‚Äì its active skin ‚Äì is exposed to touch to engage with the user (minimising surprise through inquisitive
kinematics). It may also act to establish to what degree it can move ("am I being hugged?") by actuating its flexors
and seeking change at its sensors. It must predict both how these self-calibrating actions will inform the agent of its
immediate environment and how those actions will be perceived by the user. Extreme flexes are likely to distress the
user, even if they are highly informative for the robot.
Why is Active Inference appropriate? Surprise minimisation naturally drives a trade-off between soothing the
companion‚Äôs emotional states, establishing what they are, and optimising how they can be sensed and acted on. The
automatic balance between inquisitiveness and utility maximisation at multiple levels is a notable virtue of Active
Inference that would be hard to replicate otherwise. Any model of a human‚Äôs emotional response will be incomplete.
Humans are complex, and the robot‚Äôs sensorium has limited engagement with its companion‚Äôs internal states. This
makes it vital to adequately represent uncertainty. It also motivates predictive models, since the lag between movement
and emotional response is substantial. A purely reactive system would be unstable and difficult to engineer.
D.3 Vignette 3: An intelligent music loudspeaker U (S (U‚Äô))
Scenario This vignette presents a hypothetical AIF-powered consumer device, a smart music speaker. Its actions are to
change the music played and a small two-dimensional display of a ‚Äòmusic map‚Äô which presents an abstract display of
the space of all possible musical tracks. Its sensorium (the totality of its sensing) comprises a room-level motion sensor,
an internal clock and a radar proximity sensor of the style described in Figure 8, which can detect the distance and
velocity of the user‚Äôs hand in the 20cm directly above the device. It can also detect whether the user‚Äôs hand has spread
fingers or not.
User satisfaction represented via preferences/goals: The user can be assumed to have long-term musical
preferences (we will assume the system has an uncertain and incomplete knowledge of these), but might be affected by
their immediate context, e.g. they may also play music for social reasons,29 and might have other short-term goals such
as learning more about a particular genre, and hence their preferences can adapt over time. Preferences might also
include having no music at all (zero volume) in certain contexts (e.g. tired, needing to work, social context).
How is surprise minimised? The speaker‚Äôs apparent purpose is to play music, but a more fundamental purpose
is to keep its user satisfied. As an AIF agent, the speaker achieves its purpose by adjusting the state in music space,
and hence changing the intermediate visualisation and the music played. By affecting the user in an appropriate way,
this makes the system‚Äôs future sensory predictions of its motion and proximity sensors as accurate as possible. This
is the principle of surprise minimisation. It cannot perceive satisfaction directly, but only via the effect of its actions
as reflected through the lens of the user‚Äôs sensed actions. If we assume that the user behaves like an AIF system their
actions will be to behave in such a way that the system responds predictably to their actions and in a manner aligned
with their preferences.
29Environment: If a user were listening to music on their own, with headphones on, the environment would be limited in its impact on the system.
However, if the user were controlling the music to change the mood of a party of other people in their house, the nature of the task is different, leading to
different effective preferences from the single use case, and the user‚Äôs attention is divided between the interface and the broader audience.
40 Murray-Smith, Williamson, Stein
So let us examine how this mutual prediction could be realised in practice, based on user actions and feedback:
User Actions: The music system needs to be able to take low-dimensional inputs from the user and use these to
navigate a high-dimensional space of music tracks, akin to the semi-autonomous driving example from the previous
section. However, in this case there are not enough degrees of freedom or accuracy in the input to select arbitrary
tracks, at the resolution needed, from sensed absolute position or hand spread alone, so the extra information must
come from the interaction between the human and the music system.30
D.3.1 Forward models. To enable interaction requires multiple levels of feedback loops, and each of these loops will
have associated predictive forward models. Prediction: We need to learn four groups of forward probabilistic models.
We need to (1) be able to predict mappings from human action to the speaker‚Äôs sensor, and hence to the state of the
input device. We then need (2) to map from that state to the change in music space state, (3) from that state to the
human subjective music experience and (4) from the subjective experience of the user to a user action.
1. System Forward Model ‚Äì sensing: We need forward models of the sensor system, in order to infer the user‚Äôs actual
physical movement, as described in 3.3, so that the basic low-dimensional input can work as expected by the user. Hand
proximity is used to control the state of the input device, and the finger spread can indicate uncertainty or vagueness.
2. User Forward Model ‚Äì intermediate display: The state of the low-dimensional input device will be fed into the
music state space transition dynamics, and will change the state in music space in a deterministic manner. The user
model will have a probabilistic forward model which attempts to predict the change in this display given the state of
the input device (which is driven by their low-dimensional actions).
3. User Forward Model ‚Äì human subjective music experience: The features of the music space correspond to an
implicit model of how the user will subjectively experience those tracks. Particular intermediate display settings should
lead to a predictable musical experience for the user. 31
4. User Forward Model ‚Äì human control of music space: Once the human has perceived the nature of the current
track playing, they may be satisfied, and not make further action, or might want to change, and must decide where to
move to in music space. This will require the user to be able to predict where in the music space they would be happier,
and use User Forward Model 2 to predict what actions would take them there. The user action model will also have the
user‚Äôs own approximate model of System Forward Model 1 which describes the reliability of action sensing.
D.3.2 Designing a system. Incorporating Machine learning methods: We now assume that machine learning can
be used to learn offline from training sets of human-labelled data, and create an ordinal regression model which can
analyse the audio tracks and probabilistically predict human subjective responses for multiple features such as mood,
genre, tempo etc. This can be used to pre-index hundreds of thousands of tracks with these subjective responses in
advance. This enables the creation of a 2D embedding of the multidimensional music space to visualise the location of
any of the tracks. This simple visual display shows a summary of responses to human sensed motion, and the projection
of the current location in ‚Äòmusic space‚Äô which is intended to help provide the user with a bridge to understand how the
music is going to be generated. This could be a coloured two-dimensional ‚Äòmusic map‚Äô, as in [147]. From an individual
30There are many possible ways to envisage the implementation such a system (e.g. learning low-dimensional projections of the high-dimensional space,
and combining these with local searches, based on sojourn time in an area), but for any given candidate system, we should be able to act in such a way
that we minimise the EFE, where actions can include learning parameterisations of embeddings or local dynamics.
31This assumes that the intermediate display setting corresponds to where the music is actually selected. In low-density areas, however, there may be no
tracks nearby, so we may end up playing inaccurate content, and surprising the user, leading to a poor free energy measure. A further subtlety is that the
user‚Äôs ability to differentiate different classes of content may vary, in which case deviations between the intermediate display setting and played content
might not be noticed by one user, but would be by others. This can be represented by uncertainty in the AIF model of the user.
Active Inference and Human‚ÄìComputer Interaction 41
point in the music space, the system can sample the nearest ùëÅùë° points from this location, and generate playlists. It has a
sense of time, and can sense when the user is in the room, so can initiate music playing, when appropriate.
D.3.3 Preference alignment. The previous section described how the free energy approach would support making an
interface predictable, but does not yet take user preferences into account. If the system side had an internal AIF model
of the user, including learned preference priors for musical taste, then the system could combine the density of content
with the preference weighting to move to regions of musical content the user most likes to listen to. This might be
experienced by the user as them controlling one aspect of the input, and the music system responding proactively
with a sequence of ‚Äòoffers‚Äô of movement, which it could accept or reject via subtle movements of the input. We give
illustrations of interaction in section D.3.4.
D.3.4 Illustrative interaction histories. In Section 2.1 we observed that Active Inference ‚Äòunifies perception and action by
considering agents as if they stood at the end of predicted paths into all possible futures and looked back to infer which
actions would be most likely to have taken along their preferred paths‚Äô , so let us imagine several cases.
Paths to preferred and non-preferred tracks. If we assume that the smart speaker‚Äôs predictive model includes an approxi-
mate model of the user‚Äôs music preferences, how would the interaction history evolve differently for the two cases of a
user with a style of music in mind wishing to select content which has 1. a high preference versus 2. a low preference?
In this case, we assume an experienced user who understands the genre/mood mappings implicit in the 2D map. In
case 1., if they are aiming for a region with their preferred content then a rapid movement to the approximate area,
although being less informative because of its higher error rate, may still be sufficient to rapidly select preferred content,
as even if they do not get to the ideal region after the first action, the interface is more likely to offer preferred content
nearby, so with a small number of interactions they will enter an area with their desired content and stop interacting to
listen. The overall effect is similar to that observed in Fitts‚Äô Law tasks, where users move more rapidly to larger targets
because of the greater leeway for error.
However, if, as in case 2, the target track is unusual for them, the user will predict that they will get little useful
support from the system to find it, as it will initially assume more preferred content, and they will need to generate a
flow of precise (and hence informative) movements. As with smaller spatial targets in a traditional interface, they will
make more careful, precise movements, with more time to observe where they are in the music space, or listen to the
content for feedback about how to change their position with their next action. The system can observe this unusual
behaviour, and infer that it is compatible with a user acting carefully because they are aiming for a less likely target, and
it can increase the uncertainty on the preference structure, making less preferred tracks more likely to be presented.
D.3.5 Why less is often more, with Active Inference. When comparing possible paths to the goal, the system will tend
to see as less likely paths which generate more uncertainty than required. For example: If the system knows that the
user‚Äôs perceptual model is less able to distinguish certain genres, it will be less likely to offer contrasts between those
genres, as any responses will be less informative than between genres/moods that the user can more reliably separate.
Similarly the user will be less likely to choose to navigate through such regions where they have poor acuity, if routes
through other areas are more predictable for them.
The timing and execution of user interactions in general is variable, meaning that the sensations at its sensor are
inherently unpredictable; and they represent a breach of the agent‚Äôs comfortable homeostasis. So if a user does interact
with the speaker the system will be motivated (by its intrinsic goal to minimise surprise) to make the interactions as
42 Murray-Smith, Williamson, Stein
short and simple as possible, and entrain the user to move predictably, to reduce its sensory surprise. It also becomes
more likely to act pre-emptively when it predicts a user intervention.
Although its only external action is to adjust the music played, and its display, internally it can build models of what
percept influences changes in behaviour and update its model to improve its predictions. If it detects the user moving
out of range, it may learn to turn down the sound to avoid an unpredicted touch. It may learn activity patterns, so that
certain movements ‚Äì say, typing or working out ‚Äì can be recognised and responded to. It may learn a temporal model,
adjusting the volume and type of music as evening progresses, if it has had to endure frequent manual adjustments in
the past. If its touch sensor is poorly calibrated to the user‚Äôs expectations, they may spend time fiddling with it. And
so it has an implicit objective to calibrate its touch to suit the user, to minimise these interactions. If its predictions
become weak it may actively elicit a response to reinforce or contradict them. For example, if the motion sensor reads
nothing, it may be ambiguous as to whether the user is still or has departed; varying the audio slightly will provoke a
response from the user that will quickly disambiguate the uncertainty.
D.3.6 Why is Active Inference appropriate? This approach to design is a novel one. The speaker is designed to minimise
surprise. From this simple principle we can envision intelligent, adaptive behaviour where many of the ways it ‚Äúshould
work‚Äù are not manually designed but arise naturally from the free energy principle. This responsive behaviour adapts in
the moment by continuously planning over anticipated sensation; it is not a collection of pre-baked responses. Proactive,
dynamic interaction can bridge the gap between the low-dimensional input and the high dimensional content space.
Active Inference can also provide a clear structure for using machine learned models of human subjective experience
of content, and associated preferences. This means that models of user preference, of user search and of foraging
behaviour can be cleanly included to inform and improve the interaction.