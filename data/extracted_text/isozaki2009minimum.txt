Minimum Free Energy Principle for
Constraint-Based Learning Bayesian Networks
Takashi Isozaki1,2 and Maomi Ueno1
1 Graduate School of Information Systems, The University of
Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo 182-8585, Japan
2 Research and Technology Group, Fuji Xerox Co., Ltd.
3-1-1 Roppongi, Minato-ku, Tokyo 106-0032, Japan
{t-isozaki,ueno}@ai.is.uec.ac.jp
Abstract. Constraint-based search methods, which are a major ap-
proach to learning Bayesian networks, are expected to be eﬀective in
causal discovery tasks. However, such methods often suﬀer from imprac-
ticality of classical hypothesis testing for conditional independence when
the sample size is insuﬃciently large. We propose a new conditional in-
dependence (CI) testing method that is eﬀective for small samples. Our
method uses the minimum free energy principle, which originates from
thermodynamics, with the “Data Temperature” assumption recently pro-
posed for relating probabilistic ﬂuctuation to virtual thermal ﬂuctuation.
We deﬁne free energy using Kullback–Leibler divergence in a manner
corresponding to an information-geometric perspective. This CI method
incorporates the maximum entropy principle and converges to classical
hypothesis tests in asymptotic regions. We provide a simulation study,
the results of which show that our method improves the learning perfor-
mance of the well known PC algorithm in some respects.
Keywords: Bayesian networks, Structure learning, Conditional inde-
pendence test, Minimum free energy principle.
1 Introduction
Bayesian networks (BNs) [1] are graphical models and compact representations
of joint probability distributions. This combination is suitable for modeling the
uncertainty surrounding random variables. Actually, BNs are widely studied
for various applications, such as expert systems, human modeling, autonomous
agents, natural language processing, and computational biology. The network
structure expresses conditional dependence–independence relations among ran-
dom variables. A wide range of applications is considered. Therefore, structure
discovery of BNs from observational data has become an attractive problem
tackled by many researchers over the past decade.
The many proposed methods of struct ure learning can be categorized into
two major approaches: score-and-search based methods (e.g. [2]) and constraint-
based methods (e.g. [3]). Score-and-search based methods describe the ﬁtness of
W. Buntine et al. (Eds.): ECML PKDD 2009, Part I, LNAI 5781, pp. 612–627, 2009.
c⃝ Springer-Verlag Berlin Heidelberg 2009
Minimum Free Energy Principle for Constraint-Based Learning BNs 613
each possible structure to the observed data while retaining appropriate complex-
ity of models, for which scores such as AIC, BIC, MDL, and BDeu are typically
used (e.g. [2]). Constraint-based methods are designed to estimate properties of
conditional independence among the variables in the data. Both methods present
distinct advantages and disadvantages. Constraint-based methods are computa-
tionally eﬃcient and expected to ﬁnd causal models and latent common causes
under certain conditions [3].
However, in contrast to recent development of score-and-search based meth-
ods, disadvantages of constraint-based methods have increasingly achieved promi-
nence [4]. Instances of conditional independence are often decided using classical
hypothesis tests with χ2 or G2 test [3,4]. One disadvantage of the classical tests
is their impracticality for use with small samples because of their use of asymp-
totic approximation of the statistic to theχ2 distribution, which is justiﬁed for a
suﬃciently large sample size.
As described in this paper, we propose an alternative conditional indepen-
dence testing method that presents an advantage of eﬀectiveness for small sam-
ples and which has connectivity with the classical hypothesis tests. To realize
this, we use the minimum free energy principle , which originated from ther-
modynamics (e.g. [5]). In fact, the minimum free energy (MFE) principle and
similar ideas have recently attracted the attention of resea rchers in some do-
mains of computer science such as clustering [6] and learning [7,8]. In thermal
physics, free energy consists of an internal energy, an entropy, and temperature.
All of these are expected to play import ant roles. Nevertheless, many studies
that have applied free energy to statistical science have treated temperature as
a ﬁxed parameter or a free parameter, apparently because of its lack of clarity
of the meaning in data science. Consequently, we consider that the potentials of
free energies have not been well extracted.
We remain acutely aware that an advantage of using free energies in statis-
tical science is their capability of expressing a tradeoﬀ between the maximum
likelihood (ML) [9] and the maximum entropy (ME) [10] principles, which are
best used, respectively, for suﬃciently large datasets and small datasets. As de-
scribed in this paper, for solving this problem, we use a metaphor of the tradeoﬀ
between minimizing internal energies, which are dominant for low temperatures,
and maximizing entropies, which are dominant for high temperatures in the
MFE principle on thermodynamics. We can regard the temperature in free en-
ergies as a tradeoﬀ parameter that det ermines the compon ent fraction of the
ML and ME concepts. Therefore, if we pursue the program, it is reasonable to
relate temperature with the available data size. Consequently, we recently pro-
posed the “Data Temperature” assumption by which the inverted temperature
is a monotonic increasing function of the available data size. We demonstrated
its eﬀectiveness in parameter learning of BNs [11].
For this work, we adopt this assumption for learning structure BNs. However,
a new manner of deﬁnition of free energi es must be developed for constraint-
based learning. These new deﬁnition s are related to a fact described in infor-
mation geometry [12]. As a result, we obtain a new uniﬁed manner between
614 T. Isozaki and M. Ueno
the constraint-based structure and parameter learning of BNs using the MFE
principle with the “Data Temperature” assumption, in which we use only one
hyperparameter introduced in our previous work.
Advantages of the proposed method are the following.
– Improving accuracy in some respects for constraint-based learning methods
on BNs, especially for small samples, and having connectivity with the classi-
cal hypothesis tests for asymptotic regions by introducing the minimum free
energy principle, which can treat t he tradeoﬀ of the maximum likelihood
and maximum entropy principles explicitly.
– Developing a novel theoretical framework of unifying structure and parame-
ter learning methods of BNs, which corresponds to an information-
geometrical perspective.
Furthermore, we demonstrated the performance of our methods incorporated
with the PC algorithm [3], which is a typical benchmark constraint-based algo-
rithm. Our robust independence testing method turns out to be more eﬀective
on one level than the standard hypothesis tests for small or medium data sizes.
2 Background
2.1 Bayesian Networks
A Bayesian network (BN) is a set B = ⟨G,P ⟩. Actually, G = ⟨V, E⟩denotes a
directed acyclic graph (DAG) with nodes representing random variables V. In
addition, P is a joint probability distribution on V. Furthermore,G and P must
satisfy the Markov condition: all variablesX ∈ V are independent of any subset
of its non-descendant variables conditioned on the set of its parents [1]. The set
of the parents of a variableXi is in the graph G as Πi. For a distributionP of n
variables V ={X1,...,X n},aB N B can be factorized as conditional probability
distributions
P(X1,...,X n)=
n∏
i=1
P(Xi |Πi) , (1)
as proved easily using the Markov condition.
The graph of a BN presents some instances of entailed independence of the
probability distribution. The d-separation [1] is a concept in DAGs, which char-
acterizes entailed conditional independence in the graph. Two nodes X and
Y are d-separated by Z in graph G, denoted as DsepG(X; Y |Z). In addition,
Ind (X; Y |Z) is denoted as the conditional independence of X and Y given Z
in P.I ti sk n o w nt h a tDsepG(X; Y |Z) ⇒ Ind (X; Y |Z)f o raB N B = ⟨G,P ⟩.
AB N B = ⟨G,P ⟩satisﬁes the faithfulness condition if the Markov condi-
tion entails all and only the instances of conditional independence inP [3]. In a
faithful BN B = ⟨G,P ⟩, DsepG(X; Y |Z) ⇔ Ind (X; Y |Z) [1]. This equivalence
relation enables us to infer structures of BNs from conditional independence re-
lations. We assume for this discussion that BNs satisfy the faithfulness condition.
We describe some other assumptions used to conduct DAG structure inference
in this paper, further to i.i.d., as follows.
Minimum Free Energy Principle for Constraint-Based Learning BNs 615
– Discrete Variables: each variable inV has a ﬁnite, discrete number of possible
values.
– No Missing Values: let D be a database set with sample size N such that
each sample has no missing values for all variables in set V.
– No Latent Variables: we consider DAGs without latent variables.
2.2 Constraint-Based Learning on BNs
Here we state a basic concept of the constraint-based structure learning methods
using the preceding notation. For a BN that satisﬁes the faithfulness condition,
the basic concepts are the following [13].
– Search for a set Z for each pair of variables X and Y in V such that
Ind (X; Y |Z)h o l d si nP. Therefore, X and Y are conditionally indepen-
dent given a set Z in P. Construct an undirected graph such that nodes X
and Y are connected with an undirected edge if and only if no setZ can be
found.
– For each pair of nonadjacent variables X and Y with a common neighbor
W,c h e c ki fW ∈ Z. If not, then add arrowheads pointing at W (i.e., X →
W ← Y ), the type of which is called a v-structure.
– Orient as many of the undirected edges as possible subject to two condi-
tions: (i) the orientation should not create a new v-structure; and (ii) the
orientation should not create a directed cycle graph.
For checking conditional independence, we describe classical hypothesis test-
ing, which is frequently used within BN learning algorithm under a faithfulness
assumption [3,4]; we also use it afterwards. Statistics such as χ2 or G2 are ex-
pressed here as S2.I f S2 can be approximated to a χ2 distribution with degrees
of freedom df: χ2
df and S2 <χ 2
α,d f,w h e r eχ2
α,d f is a threshold value such that
P(χ2
df ≥ χ2
α,d f)= α,i nw h i c hα is a ﬁxed conﬁdence level, then we do not reject
the null hypothesis of (conditional) independence between two selected variables
given selected conditional sets; otherwise we reject it. The validity of approxima-
tion of statistics such asχ2 or G2 is proved in asymptotic regions [14]. However,
for a small sample size, it is not justiﬁed. Spirtes et al. [3] have used, in their
PC algorithm, a criterion for the validity: the algorithm does not perform an
independence test if the sample size is less than 10 times the number of diﬀerent
possible joint patterns of the two variables and conditional sets, which means
the variables are assumed to be conditionally dependent. This impracticality is
a weak point of the constraint-based learning methods of BNs because learning
BNs often process insuﬃcient data.
3 Conditional Independence Testing Using the MFE
Principle
In this section, we describe a new conditional independence testing method that
is designed to be especially eﬀective for small data size, and which is designed
616 T. Isozaki and M. Ueno
to be connected asymptotically with cla ssical hypothesis testing. We take an
approach from the metaphor of thermodynamics, where entropy, energy, and
temperature play important roles.
3.1 Why Is the MFE Principle Needed?
Many studies of learning BNs have often used mutual information (e.g., [15])
for measuring dependence, which often means minimizing entropy , as described
below. For asymptotic regions, for which suﬃciently large samples are available,
the guiding principle in statistics is the maximum likelihood (ML) principle [9].
Friedman et al. [15] derived that, for a BN G, given a dataset D with N data
size for n random variables, maximizing the log likelihood LL(G|D)i se q u i v a -
lent to maximizing empirical mutual information between a node and its par-
ent nodes (represented as Πi for a node Xi): LL(G|D)= N(∑ n
i=1 ˆI(Xi; Πi) −∑ n
i=1 ˆH(Xi)), where ˆI and ˆH denotes empirical mutual information and Shan-
non entropy [10], and the second term of the right-hand-side of the equation
has nothing to do with the learning structure. Therefore, from the deﬁnition of
mutual information, it is readily derived that
LL(G|D)= −N
n∑
i=1
ˆH(Xi |Πi)= −N ˆH(X1,...,X n) . (2)
The last equation is derived from the d eﬁnition of BNs described in (1). This
equation means that maximizing the log likelihood is equivalent to minimizing
the entropy of BNs. This equation also implies that maximizing the log likeli-
hood for constructing the DAG structures engenderscomplete DAG because the
following inequality is justiﬁed: −N ∑
i ˆH(Xi |Πi) ≥− N ∑
i ˆH(Xi), because
0 ≤ H(X|Y ) ≤ H(X)[ 1 0 ] .
In contrast, when we obtain insuﬃcient data, it is reasonable to use the max-
imum entropy (ME) principle [10], which states that the most preferable prob-
abilistic model should maximize its entropies under some constraint related to
available data. Consequently, with no constraint, maximizing entropies of BNs
engenders the DAG with no edges , which means that a BN is a collection of
complete independent distributions: P(X)= ∏
i P(Xi).
A tradeoﬀ exists between maximum likelihood and maximum entropy for
obtaining the valid structures. In the asymptotic region, the ML principle is
expected to be dominant; in an insuﬃcient sample region, the ME principle is
expected to be dominant. Therefore, we can set a problem of how to decide the
tradeoﬀ between the ML and ME principl es according to an arbitrarily given
sample size. We regard the situation as a metaphor of thermodynamics. The
tradeoﬀ between minimizing internal en ergy and maximizing entropy in ther-
modynamics seems to correspond to the tradeoﬀ between maximizing likelihood
and entropy in statistics, and temperature can be regarded as a parameter that
brings harmony of the two amounts. These amounts can be treated in a uniﬁed
manner as afree energy that is well known in thermal physics. Furthermore, this
tradeoﬀ can be determined using the minimum free energy (MFE) principle.
Minimum Free Energy Principle for Constraint-Based Learning BNs 617
During recent decades, many researchersinvestigated Bayesian methods, which
can avoid overﬁtting derived from using the ML with insuﬃcient data. This can be
regardedas the same problem setting described in this paper. For example, Dash et
al. proposed a robust conditional independence testing procedure using Bayesian
Dirichlet smoothing [16]. However, the Bayesian method presents the diﬃculty of
deciding optimal hyperparameters simultaneously in both theoretical (e.g. [17])
and practical [18] perspectives, when no prior knowledge exists.
Similarly, temperature is an unknown parameter in the MFE principle. How-
ever, we recently presented a model of inverted temperature that is a monotonic
increasing function of available data size, which we call the “Data Temperature”
assumption [11]. Using this approach, we give the meaning of temperature of
free energy in data/statistical science by regarding the probability ﬂuctuation
as a virtual thermal ﬂuctuation. Furthermore, we showed that the approach is
eﬀective for parameter learning of BNs, a nd that the eﬀect is not sensitive for
selecting a hyperparameter. We consider that the approach can also be useful in
structure learning BNs for estimating optimal entropies of the network structure.
To realize this, we regard that remaining problems are how to deﬁne amounts
corresponding to energies, entropies, and temperature.
3.2 Minimum Free Energy Principle
The (Helmholtz) free energies were introduced originally into the ﬁeld of thermo-
dynamics. The energies are deﬁned such that a maximum thermodynamical work
is the diﬀerence between values of free energies in two distinct states, [5] where
the maximum work is obtained using an isothermal quasistatic operation from a
closed system under the condition of a constant temperature. Therefore, the free
energy can be regarded as an amount, in a constant temperature, correspond-
ing to a potential energy in dynamics (e.g., gravitational and electro-magnetic
potential energy). In this meaning, the free energy is viewed as an amount that
is extracted freely from a thermodynamical system.
We regard the free energy in information systems as an amount that has a
similar eﬀect in thermody namical systems: it is ext racted freely from a data
system under a given data size (corresponding to inverted temperature). This
property is apparently preferred for various tasks such as inference, learning, and
estimation under a ﬁnite available data size because we wish to obtain maximum
eﬀective information from limited exploitable data.
We use a principle of minimum free ener gy (MFE) for statistical testing of
conditional independence. A free energy F is deﬁned by an internal energy U,
an entropy H, and inverted temperature β0 (=1 /temperature)a s
F := U − H
β0
, (3)
where (inverted) temperature β0, which is a parameter, balances the respective
contributions to F of U and H. According to the principle of MFE, given some
temperature β0, the stable state of the system is realized to minimize F [5],
where minimizing U and maximizing H are balanced.
618 T. Isozaki and M. Ueno
3.3 Representation of Free Energy in Probabilistic Models
Diﬀerent from usual application of the MFE principle in data science, we start
with description of the free energy deﬁnitely as a function of internal energy,
entropy, and temperature to recognize clearly important properties of tempera-
ture and use eﬀectively free energies. Fortunately, entropy was introduced into
information theory by Shannon. It has since become a fundamental concept in
computer science and statistical science[10]. Therefore, we deﬁne the entropy of a
random variableX as Shannon entropy. We hope that the entropy serves to avoid
overﬁtting for small samples. We adopt the Kullback–Leibler (KL) divergence
between two probabilistic distributions, which are an empirical distribution and
a true distribution. Here, we follow the “Data Temperature” assumption [11],
which makes the MFE principle express a harmony between the ML and ME
principle according to available data size:temperature is deﬁned as a monotonic
function of the available data size such that temperature β0 →∞ if data size
N →∞ ,a n d β0 → 0 if N → 0.
3.4 An MFE Representation of Hypothesis Testing on BNs
We represent the conditional independence tests using the MFE principle. To do
so, as in the usual manner [3,4], we represent the null hypothesis as conditional
independent relations, and the opposite hypothesis as conditional dependent
relations between two variables X and Y given conditional sets Z.
We deﬁne the internal energies for each hypothesis. First, we represent the
internal energy U such that the relative entropy (KL divergence) between the
graphs expressing the null hypothesis (expressed as H1, corresponding distribu-
tions as ˆP1) and the true graphs (as H0, corresponding as P0), where ˆP1 of the
null hypothesis is deﬁned as a maximum likelihood distribution. Therefore, we
can deﬁne an internal energy U1 which expresses the null hypothesis such as
U1(X,Y, Z):= −D( ˆP1(X,Y, Z)||P0(X,Y, Z))
=
∑
x,y,z
ˆP(x, y,z)l o g P(x, y|z)
ˆP(x|z) ˆP(y |z)
, (4)
where ˆP is a maximum likelihood distribution and P is a distribution that will
be estimated using the MFE principle with a “Data Temperature” model. In
turn, the internal energy U2 expresses the opposite hypothesis, which expresses
a dependent relation as
U2(X,Y, Z):= −D( ˆP2(X,Y, Z)||P0(X,Y, Z))
=
∑
x,y,z
ˆP(x, y,z)l o gP(x, y|z)
ˆP(x, y|z)
. (5)
In the next step, we deﬁne the entropy term with respect to each hypothesis.
Probability distributions which constitute the entropy are estimated under given
available samples. We describe the entropy of the null hypothesis as
Minimum Free Energy Principle for Constraint-Based Learning BNs 619
H1(X,Y, Z): = −
∑
x,y,z
P(x, y,z)l o g(P(x|z)P(y |z)P(z)) . (6)
The other entropy, that of the opposite hypothesis, is
H2(X,Y, Z): = −
∑
x,y,z
P(x, y,z)l o g(P(x, y|z)P(z)) . (7)
Now we are almost prepared to express the free energy of each hypothesis. We
regard the temperature in each hypothesis ( β1 and β2)a sa global temperature
over related variables. According to t he “Data Temperature” assumption, we
can consider that β1 = β2 = β0, which means the same sample size. Therefore,
the diﬀerence of the free energy of each hypothesis is
F1(X,Y, Z) − F2(X,Y, Z)= ˆI(X; Y |Z) − 1
β0
I(X; Y |Z) , (8)
where
ˆI(X; Y |Z)=
∑
x,y,z
ˆP(x, y,z)l o g
ˆP(x, y|z)
ˆP(x|z) ˆP(y |z)
, (9)
and
I(X; Y |Z)=
∑
x,y,z
P(x, y,z)l o g P(x, y|z)
P(x|z)P(y |z) . (10)
According to the notation used in our previous work, we deﬁne a new parameter
β, which we call “Data Temperature” hereinafter as
β := β0/(β0 +1 ) , (11)
where if β0 → 0, then β → 0 (high temperature limit); if β0 →∞ ,t h e nβ → 1
(low temperature limit).
For estimating the non-empirical conditional mutual information I(X,Y |Z),
as described above, we follow our previous work associated with parameter learn-
ing method, for which a diﬀerent deﬁnition of internal energiesU is needed [11].
Let P(X)a n d ˆP(X) respectively represent probability distributions of joint
random variables X to be estimated from the MFE principle and ML principle.
Internal energies U(X) are deﬁned for parameter learning as
U(X)= D(P(X)|| ˆP(X))=
∑
x
P(x)l o gP(x)
ˆP(x)
. (12)
Then, using Lagrange multipliers corresponding to minimizing the free energy
with a constraint as ∑
X=x P(X = x) = 1, the estimated probability Pβ(x)i s
expressed in Boltzmann’s formula, as shown below [11].
Pβ(x)= exp(−β(−log ˆP(x)))
∑
x′ exp(−β(−log ˆP(x′)))
= [ ˆP(x)]β
∑
x′ [ ˆP(x′)]β (13)
620 T. Isozaki and M. Ueno
Therein, ˆP is a relative frequency: the ML estimator.
Finally, we obtain the condition of conditional independence as
ˆI(X; Y |Z) < 1 − β
β Iβ(X; Y |Z) , (14)
where Iβ is deﬁned as
Iβ(X; Y |Z)=
∑
x,y,z
Pβ(x, y,z)l o g Pβ(x, y|z)
Pβ(x|z)Pβ(y |z)
=
∑
x,y,z
Pβ(x, y,z)l o gPβ(x, y,z)Pβ(z)
Pβ(x,z)Pβ(y, z) . (15)
Therein, β plays only the role of a symbolic index; it does not represent a sole
parameter. In each estimator, β should be calculated using the explicit model
of “Data Temperature,” as described in the next subsection. Therefore, β in
(15) represents local temperature. In (14), the left-hand-side corresponds to the
likelihood term, which is dominant for a large data size (largeβ), and the right-
hand-side corresponds to the entropy term, which is dominant for a small data
size (small β). We designateg2
β and represent the conditional independence (CI)
condition with it as
g2
β = ˆI(X; Y |Z) − 1 − β
β Iβ(X; Y |Z) < 0 . (16)
This is useful for combination with the classical hypothesis tests.
In these formulations, it might seem strange that two distinct deﬁnitions of
internal energies exist between hypothesis tests and parameter estimation. We
express internal energies in the hypothesis tests as D( ˆP ||Q), where ˆP is an
ML distribution. That diﬀers from our formula in parameter learning, where we
expressed the internal energies as D(P || ˆQ), where ˆQ is an ML distribution.
This diﬀerence is pointed out from the perspective ofinformation geometry [12].
From an information theoretical viewpoint, hypothesis testing is related to the
large deviation theorem via Sanov’s theorem [10], and then, from an information
geometrical perspective, it can be interpreted as a ∇(e)-projection, whereas the
ML estimation can be interpreted as ∇(m)-projection [12]. In other words, the
hypothesis testing and ML estimation are then diﬀerent concepts in view of
information theory. Consequently, in the deﬁnitions for learning structures and
parameters, the diﬀerence is reasonable from this perspective.
3.5 “Data Temperature” Model
In searching for the values ofβ, we use our simple model of temperature, which
is proposed as a function of data sizeN [11]. The model function of β is deﬁned
as
β := 1 − exp
(
− N
γNc
)
,
γ := |X|− 1 ,
(17)
Minimum Free Energy Principle for Constraint-Based Learning BNs 621
where γ is deﬁned as the degrees of freedom of related random variablesX,a n d
where Nc is a decoupling constant, which can be regarded as a hyperparameter
for β.W eu s eNc as only onecommon hyperparameter in learning of both param-
eter and structure. This explicit model shows good performance and robustness
against selected hyperparameters Nc in classiﬁcation tasks using Bayesian net-
work classiﬁers with structure learning [11].
3.6 Asymptotic Theoretical Analysis
We hope that the proposed method has consistency with the classical hypothesis
test for an asymptotic region because it i s theoretically justiﬁed. However, our
conditional independence conditions using the inequality (14) cannot be used
straightforwardly for large data sizes becauseg2
β ≥ 0 always for suﬃciently large
data size. That is true becauseˆI(X; Y |Z) ≥ 0a n d[ ( 1−β)/β] Iβ(X; Y |Z) → 0a s
β goes to 1 (asN becomes suﬃciently large), which means that our method would
produce an overly dense graph for suﬃciently large data size. In such regions,
the eﬀect of enlarging the entropy term has vanished and the likelihood term
has become dominant. However, diﬀerent from parameter learning, hypothesis
testing for BNs means that extra edges should be removed even for a large
sample size, based on Occam’s razor [13]. This connecting problem is solved as
described below.
For a large sample size region, we wish to use theG2 statistic for conditional
independence testing, which is often used [3,4]. The G2 test is used to identify
Ind (X,Y |Z), by which the null hypothesis of conditional independence is repre-
sented. Let Nxyz represent the number of times in the data whereX = x, Y = y
and Z = z. We deﬁneNxz ,N yz , and Nz similarly. Consequently, theG2 statistic
is deﬁned as follows:
G2 =2
∑
x,y,z
Nxyz log Nxyz Nz
Nxz Nyz
. (18)
The degrees of freedom df are deﬁned as
df =( |X|− 1)(|Y |− 1)
∏
Z∈Z
|Z| , (19)
w h e r ew ed e s i g n a t e|X| as the number of states in X. It is noteworthy that the
G2 statistics have a relation with the empirical mutual information with data
size N [14] as
G2 =2 N ˆI(X; Y |Z) . (20)
The statistic is proven to be approximated asymptotically to a χ2 distribution
with degrees of freedomdf[14]. Therefore, in a large sample size region, we should
set the condition in which the null hypothesis (i.e., conditional independence) is
not rejected, as
G2 <χ 2
α,d f, (21)
622 T. Isozaki and M. Ueno
where α is a signiﬁcance level such as 0.05, and where df are the degrees of
freedom, as deﬁned in (19).
Here, we intend to connect the classical condition with the MFE condition.
We deﬁne a formal correspondence amount G2
β to G2, using (16) and (20) as
G2
β := 2Ng2
β = G2 − 2N 1 − β
β Iβ(X; Y |Z) . (22)
Using the explicit model of β expressed in (17), in an asymptotic region,
G2
β = G2 − 2N exp(−N/γNc)
1 − exp(−N/γNc) Iβ(X; Y |Z) → G2 . (23)
Then, we can treat the MFE and the classical condition uniformly because a
condition G2
β <χ 2
α,d fcan include the CI condition (16) and the classical condi-
tion (21). Even when the data size is small andG2
β ≥ 0, we conduct the classical
hypothesis tests because our method was shown to generate pseudo-samples sim-
ilarly to the Bayesian methods [19]. Consequently, we can conduct conditional
independence tests on variables X and Y given Z using the MFE principle and
G2 tests as described below.
– If G2
β < 0, because of MFE principle, then we set X ⊥⊥ Y |Z (conditional
independence),
– else if 0 ≤ G2
β <χ 2
α,d f, because of the classical test, then we setX ⊥⊥ Y |Z,
– else, we set X/⊥⊥ Y |Z (conditional dependence).
We designate this conditional independence method as MFE-CI.
4 Experiments
We next demonstrate the performances of our approach compared with tradi-
tional statistical testing methods. So me experiments of learning BNs can be
done using the PC algorithm [3], which is a well known benchmark algorithm
of constraint-based methods, embedding our conditional independence tests or
classical independence tests using χ2 distributions with ﬁxed signiﬁcant level
α =0 .05 for each hypothesis test of conditional independence. We implemented
the PC algorithm for embedding the MFE-CI method using C++ programming
language. The PC algorithm, which constructs partial DAGs (PDAGs), is the
following [20]:
The PC algorithm
1. Assume a non-negative integer m =0 .
2. Let G be a complete undirected graph.
3. Repeat:
(a) For all pairs of variables (X,Y ),c h e c k Ind (X,Y |Z) for all subsets Z
such that |Z| = m and Z ⊂ Adj(X) or Z ⊂ Adj(Y ).
If there exists a Z such as Ind (X,Y |Z),
Minimum Free Energy Principle for Constraint-Based Learning BNs 623
then remove the edge X − Y from G, and add Z to SepSet(XY ).
(b) Set m = m +1 .
Until no variable has more than m adjacencies,
or a stopping condition is satisﬁed.
4. Orientation rules are performed.
5. Return the partially directed acyclic graph G.
Therein, |X| denotes the size of members in X; Adj(X)i sas e to fa d j a c e n t
nodes to X. The orientation rules [21], described in step 4 of the algorithm, are
as follows:
4-1. If U/∈ SepSet(XY ),o r i e n t X − U − Y as X → U ← Y (v-structure)
for each uncoupled set of X and Y such as X − U − Y .
4-2. Repeat this step while more edges can be oriented.
4-2-1. Orient U − Y as U → Y for each uncoupled set of X and Y such as
X → U − Y .
4-2-2. Orient X − Y as X → Y for each set of X and Y such that a path exists
from X to Y .
4-2-3. Orient U − W as U → W for each uncoupled set of X and Y such as
X − U − Y , X → W, Y → W,a n d U − W.
The completeness of the rules was proved by Meek [22].
The PC algorithm is performed under the faithfulness assumption described
in section 2. Consequently, the algorithm can infer correct graph structures by
ﬁnding conditional independence for probability distributions. However, if the
assumption is violated, even though the true graph means Ind (X,Y |Z)f o rX
and Y and a conditional setZ, the algorithm might ﬁnd another false conditional
set Z′ for the test betweenX and Y ,a n dt h e na d dZ′ to SepSet(XY).T h i sf a l s e
detection has no inﬂuence on removing the edge between X and Y correctly.
However, the algorithm decides the wrong direction of edges using the orientation
rules described above. In this situation, ﬁnding correctly conditional sets has a
large inﬂuence on the directionality of edges in BNs.
We conducted the simulation study with various quantities of variables:{10,
20, 40, 80}, where each variable has all four possible states, and with networks
of two types, i.e. the sparser and denser graphs, where sparser cases have the
same number of edges as variables; the denser cases have twice. For each such
graph, a random structure network was constructed with conditional probability
tables (CPTs) of ﬁve types that were set by random numbers. The available
sample size is varied in a range of {500, 1000, 2500, 5000, 10000 }. When the
number of conditional sets |Z| is large, the number of CI tests is intractably
large because of a combinatorial explosion. Therefore, we did not perform CI
tests and assume conditional dependence when |Z|≥ 5. We selected a value of
the hyperparameter Nc for β in (17) as 2.0, which shows good performance in
preliminary experiments.
624 T. Isozaki and M. Ueno
We set the performance criterion as counting added edges , removed edges ,
and reversed edges. Counting added edges expresses the consequence where two
variables X and Y are not adjacent in original BNs but where an edge exists
between them in reconstructed BNs. On the other hand, counting removed edges
mean the opposite. Counting reversed edges means that ifX → Y in the original,
then Y → X in the output. The results are presented in Table 1 for sample sizes
of 500, 1000, and 2500, and in Table 2 for sample sizes of 5000 and 10000. The
values in the tables are averaged values of simulations for ﬁve diﬀerent randomly
set CPTs. We designate the PC algorithm with a standard G2 test as Std-
PC or Std, and the PC embedded with the MFE-IC as MFE-PC. These tables
show that the counted quantities of ex tra added edges were very small, even
for a small sample size such as 500 and even for denser structures. In contrast,
quantities of removed edges are very large in both Std-PC and MFE-PC. The
MFE-PC removed true edges more than Std-PC. In reversed checks, many errors
were found in Std-PC. These characteristics were noticeable in large and denser
networks. We discuss these resul ts later. A key is apparently the faithfulness
condition for understanding the results.
The MFE-PC seemed to underscore the eﬀectiveness for deciding the direction
of edges. It might be unfair, however, to conclude that because the MFE-PC re-
moved more edges than Std-PC. Therefore, we deﬁnedreversed ratio as (number
of reversed edges)/((true number of edges) – (number of removed)). Results of
reversed ratio for denser networks are portrayed in Fig. 1, where the horizontal
axis expresses the true number of edges, the vertical axis expresses the reversed
ratio, and G2 and MFE respectively signify Std-PC and MFE-PC. These ﬁgures
show that the MFE-PC outperforms Std-PC in deciding the direction of edges,
especially for denser networks, even using samples such as 5000, which are not
small.
We discuss these comparative results. As designed using the MFE principle,
MFE-PC is expected to remove edges more than PC does for two reasons. The
ﬁrst is that MFE-PC performs CI tests in more cases than Std-PC, which does
the test only for suﬃciently large data size. For example, even when the data
Table 1. Results for the simulation using data sizes of 500, 1000, and 2500
500
 1000
 2500
Sparser
 Denser
 Sparser
 Denser
 Sparser
 Denser
Type Nodes
 Std MFE
 Std MFE
 Std MFE
 Std MFE
 Std MFE
 Std MFE
Added 10
 00
 0.6 0.2
 0.4 0
 1.8 0
 00
 00
20
 00
 0.4 0.4
 0.2 0
 1.2 0
 00
 0.2 0
40
 0.2 0
 0.4 0
 0.6 0.4
 0.8 0
 00
 00
80
 0.6 0.4
 1.4 0.8
 0.4 0.2
 2.2 0.2
 0.2 0.2
 00
Removed 10
 3.0 3.4
 9.0 14.0
 2.4 2.8
 3.6 11.6
 1.8 1.8
 4.4 8.6
20
 4.2 7.6
 19.0 26.2
 3.0 5.8
 11.4 22.0
 2.0 2.8
 12.8 18.6
40
 11.2 15.8
 41.6 53.6
 6.4 12.0
 25.0 46.6
 6.0 7.2
 26.0 37.2
80
 21.4 32.0
 81.2 109
 11.0 21.2
 48.6 93.8
 9.0 12.0
 54.4 73.8
Reversed 10
 1.8 1.4
 7.4 2.2
 0.8 0.8
 12.2 4.2
 0.6 0.6
 9.2 5.2
20
 5.4 2.0
 14.6 6.4
 5.2 2.6
 22.2 7.6
 2.8 2.4
 13.2 7.6
40
 10.6 5.2
 26.6 11.8
 10.2 4.0
 42.6 16.8
 6.6 6.4
 31.8 19.4
80
 18.8 10.6
 54.2 26.6
 21.0 11.2
 86.4 26.0
 11.6 10.8
 56.6 38.8

Minimum Free Energy Principle for Constraint-Based Learning BNs 625
Table 2. Results for simulations using data sizes 5000 and 10000
5000
 10000
Sparser
 Denser
 Sparser
 Denser
Type Nodes
 Std MFE
 Std MFE
 Std MFE
 Std MFE
Added 10
 00
 1.2 0.2
 00
 00
20
 00
 0.4 0.0
 00
 00
40
 0.4 0.4
 0.0 0.0
 00
 0.2 0.2
80
 0.4 0.4
 0.0 0.0
 0.4 0.4
 00
Removed 10
 1.0 1.0
 1.8 6.4
 0.6 0.6
 2.6 4.6
20
 0.4 1.4
 6.2 14.6
 0.6 0.6
 8.6 10.4
40
 2.6 4.6
 16.0 30.0
 1.8 2.0
 20.2 24.4
80
 4.6 7.2
 24.2 59.2
 4.4 4.6
 40.0 48.0
Reversed 10
 0.6 0.6
 7.2 4.4
 4.0 4.0
 4.2 3.6
20
 2.6 2.4
 20.8 9.6
 6.2 6.2
 12.0 11.4
40
 5.0 3.8
 36.6 20.8
 14.0 14.2
 21.0 19.0
80
 8.4 7.8
 78.0 38.0
 24.0 24.2
 48.5 44.3
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 20  60  100  140  180
Reversed ratio
Number of true edges
G2 500
MFE 500
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 20  60  100  140  180
Reversed ratio
Number of true edges
G2 1000
MFE 1000
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 20  60  100  140  180
Reversed ratio
Number of true edges
G2 2500
MFE 2500
(a) Sample size = 500. (b) Sample size = 1000. (c) Sample size = 2500.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 20  60  100  140  180
Reversed ratio
Number of true edges
G2 5000
MFE 5000
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 20  60  100  140  180
Reversed ratio
Number of true edges
G2 10000
MFE 10000
(d) Sample size = 5000. (e) Sample size = 10000.
Fig. 1. Ratio of reversed edges in the resultant graphs with denser BNs from use of a
standard PC and PC embedded with the MFE-IC method
size N = 5000, Std-PC was unable to perform CI tests for |Z|≥ 3i nt h i s
simulation, which implies that Std-PC might sometimes correctly happen to
maintain some existing edges. The second is that, when conﬁdence of existing
edges is small because the data size is insuﬃciently large, the null hypothesis
is not rejected because of the eﬀect of ma ximum entropy. Therefore, MFE-IC
method seemingly tends to prefer sparser graphs. These mean that MFE-IC
method does draw edges only when high conﬁdence for dependence is obtained.
Additionally, we can comment on the fact that Std-PC incorrectly decided the
direction of edges more than MFE-PC. This fact means that Std-PC detected
conditional independence for invalid conditional sets Z. In this case, wrong
626 T. Isozaki and M. Ueno
v-structures, divergence connections such as X ← U → Y , and serial connec-
tions such as X → U → Y were generated. This suggests that the simulation
had diﬃculty realizing the faithfulness assumption. In other words, unfortu-
nately, Ind (X; Y |Z) ⇒ DsepG(X; Y |Z) was often violated in this simulation.
This situation was also reported by Ramsey et al. for linear Gaussian mod-
els of DAGs [23]. However, we found the large diﬀerence of the reversed ratio,
which shows that MFE-PC correctly found the conditional sets more than Std-
PC. According to the discussion, we regard that MFE-IC method is especially
preferable for causal discovery, where ex isting edges are expected to represent
deﬁnite direct dependence between variables, and where direction has impor-
tant meanings. In contrast, MFE-IC seems to be unsuitable for ﬁnding BNs in
view of the predictive sense, for which the edges are allowed to be reversed for
maintaining adequate parametric space size.
5C o n c l u s i o n
We proposed a method for improvement of conditional independence (CI) testing
in small samples, which is a weak point of constraint-based learning Bayesian
networks using the classical hypothesis tests. To do this, we introduced the
minimum free energy principle with a “Data Temperature” assumption that re-
lates probabilistic ﬂuctuation to virtual thermal ﬂuctuation. We deﬁned a free
energy using Kullback–Leibler divergence, which corresponds to an information-
geometric view. This CI method incorporates the maximum entropy and max-
imum likelihood principles and converges to the classical hypothesis tests in
asymptotic regions.
We also demonstrated the eﬀectiveness of our method by embedding it in the
well known PC algorithm. The results show that our method correctly identiﬁed
the direction of the edges better than the standard tests did, which is expected
to be eﬀective for causal discovery where the orientation of edges is signiﬁcant.
Acknowledgements
One author (T.I.) particularly thanks M. Kyojima, K. Shinozaki and T. Tanaka
of Fuji Xerox Co., Ltd. for support and encouragement, and thanks T. Ogawa
of the University of Electro-Communications for advice related to information
geometry. The authors thank anonymous reviewers for fruitful comments, which
help them improve the paper.
References
1. Pearl, J.: Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Mateo (1988)
2. Heckerman, D., Geiger, D., Chickering, D.: Learning Bayesian networks: The com-
bination of knowledge and statistical data. Machine Learning 20, 197–243 (1995)
Minimum Free Energy Principle for Constraint-Based Learning BNs 627
3. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction and Search, 2nd edn.
MIT Press, Cambridge (2000)
4. Tsamardinos, I., Brown, L.E., Aliferis, C.F.: The max–min hill-climbing Bayesian
network structure learning algorithm. Machine Learning 65(1), 31–78 (2006)
5. Callen, B.H.: Thermodynamics and An Introduction to Thermostatistics, 2nd edn.
John Wiley & Sons, Hoboken (1985)
6. Pereira, F., Tishby, N., Lee, L.: Distributional clustering of English words. In: Proc.
of Annual Meeting on Association for Computational Linguistics (ACL 1993), pp.
183–190 (1993)
7. Hofmann, T.: Probabilistic latent semantic analysis. In: Proc. of Conference on
Uncertainty in Artiﬁcial Intelligence (UAI 1999), pp. 289–296 (1999)
8. LeCun, Y., Huang, F.J.: Loss functions for discriminative training of energy-based
models. In: Proc. of International Workshop on Artiﬁcial Intelligence and Statistics
(AISTATS 2005), pp. 206–213 (2005)
9. Lehmann, L.E.: Testing Statistical Hypotheses, 2nd edn. John Wiley & Sons, New
York (1986)
10. Cover, T.M., Thomas, J.A.: Elements of Information Theory, 2nd edn. John Wiley
& Sons, Hoboken (2006)
11. Isozaki, T., Kato, N., Ueno, M.: Minimum free energies with “data temperature”
for parameter learning of Bayesian networks. In: Proc. of IEEE International Con-
ference on Tools with Artiﬁcial Intelligence (ICTAI 2008), pp. 371–378 (2008)
12. Amari, S., Nagaoka, H.: Method of Information Geometry. Oxford University Press,
New York (2000)
13. Pearl, J.: Causality, models, reasoning, and inference. Cambridge University Press,
New York (2000)
14. Kullback, S.: Information Theory and Statistics. Dover Publications, Mineola
(1968)
15. Friedman, N., Geiger, D., Goldszmidt, M.: Bayesian network classiﬁers. Machine
Learning 29(2-3), 131–163 (1997)
16. Dash, D., Druzdzel, M.J.: Robust independence testing for constraint-based learn-
ing of causal structure. In: Proc. of Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI 2003), pp. 167–174 (2003)
17. Clarke, B., Barron, A.: Jeﬀreys’ prior is asymptotically least favorable under en-
tropy risk. Journal of Statistical Planning and Inference 41, 37–60 (1994)
18. Silander, T., Kontkane, P., Myllymaki, P.: On sensitivity of the map Bayesian
network structure to the equivalent sample size parameter. In: Proc. of Conference
on Uncertainty in Artiﬁcial Intelligence (UAI 2007), pp. 360–367 (2007)
19. Isozaki, T., Kato, N., Ueno, M.: “Data temperature” in minimum free energies for
parameter learning of Bayesian networks (to appear)
20. Neapolitan, R.E.: Learning Bayesian Networks. Prentice-Hall, Upper Saddle River
(2004)
21. Verma, T., Pearl, J.: An algorithm for deciding if a set of observed independen-
cies has a causal explanation. In: Proc. of Conference on Uncertainty in Artiﬁcial
Intelligence (UAI 1992), pp. 323–330 (1992)
22. Meek, C.: Causal inference and causal explanation with background knowledge.
In: Proc. of Conference on Uncertainty in Artiﬁcial Intelligence (UAI 1995), pp.
403–410 (1995)
23. Ramsey, J., Spirtes, P., Zhang, J.: Adjacency-faithfulness and conservative causal
inference. In: Proc. of Conference on Uncertainty in Artiﬁcial Intelligence (UAI
2006), pp. 401–408 (2006)