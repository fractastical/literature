Autonomous learning and chaining of motor
primitives using the Free Energy Principle
Louis Annabi Alexandre Pitti Mathias Quoy
ETIS UMR 8051 ETIS UMR 8051 ETIS UMR 8051
CY University, ENSEA, CNRS CY University, ENSEA, CNRS CY University, ENSEA, CNRS
F-95000, Cergy-Pontoise, France F-95000, Cergy-Pontoise, France F-95000, Cergy-Pontoise, France
louis.annabi@ensea.fr alexandre.pitti@ensea.fr mathias.quoy@ensea.fr
Abstract—Inthisarticle,weapplytheFree-EnergyPrincipleto present a variational formulation of our problem that allows
thequestionofmotorprimitiveslearning.Anecho-statenetwork us to translate motor primitives learning into a free energy
is used to generate motor trajectories. We combine this network
minimisation problem.
with a perception module and a controller that can influence its
In previous works, we applied the principle of free energy
dynamics.
Thisnewcompoundnetworkpermitstheautonomouslearning minimisation in a spiking recurrent neural network for the
of a repertoire of motor trajectories. To evaluate the repertoires generation of long range sequences [8] but not associated to
built with our method, we exploit them in a handwriting task sensorimotor control. The presented model was able to gen-
where primitives are chained to produce long-range sequences.
erate long range sequences minimising free energy functions
Index Terms—Unsupervised learning, self-organizing feature
corresponding to several random goals. We used a randomly
maps,inferencealgorithms,predictivecoding,intelligentcontrol,
connectedrecurrentneuralnetworkinordertogeneratetrajec-
nonlinear dynamical systems
tories, and combined it with a second population of neurons
I. INTRODUCTION in charge of driving its activation into directions minimising
the distance towards a randomly sampled goal.
We consider the problem of building a repertoire of motor
Using randomly connected recurrent neural networks to
primitives from an open-ended, task agnostic interaction with
generate sequences is at the core of reservoir computing (RC)
theenvironment.Wesuggestthatasuitablerepertoireofmotor
techniques [9], [10]. In particular, there is work using RC for
primitives should enable the agent to reach a set of states that
thegenerationofmotortrajectories,seeforinstance[11],[12].
covers best its state space. Based on this hypothesis, we train
IntheRCframework,inputsaremappedtoahigh-dimensional
an agent to learn a discrete representation of its state space,
space by a recurrent neural network called reservoir, and
as well as motor primitives driving the agent in the learned
decoded by an output layer. The reservoir weights are fixed
discrete states. In a fully observable environment, a clustering
and the readout weights are regressed, usually with gradient
algorithm such as Kohonen self-organising maps [1] applied
descent. In [8], we proposed to alter the learning problem by
to the agent’s sensory observations make it possible to learn a
fixing the readout weights to random values as well, and by
set of discrete states that covers well the agent’s state space.
optimising the input of the reservoir network instead.
Using this set of discrete states as goals, an agent can learn
Inthisarticle,weproposetocombinetheideasdevelopedin
policies that drive it towards those goals, thus building for
our previous work with a perception module in order to learn
itself a repertoire of motor primitives. Our main contribution
a repertoire of motor primitives. We train and evaluate our
is to address this twofold learning problem in terms of free
model in an environment designed for handwriting, where the
energy minimisation.
agentcontrolsa2degreesoffreedomarm.Theagentrandomly
The Free Energy Principle [2] (FEP) suggests that the
exploresitsenvironmentbydrawingrandomtrajectoriesonthe
computing mechanisms in the brain accounting for percep-
canvas. By clustering its visual observations of the resulting
tion, action and learning can be explained as a process of
trajectories, the agent learns a set of prototype observations
minimisationofanupperboundonsurprisecalledfreeenergy.
which it will sample as goals to achieve during its future
On the one hand, FEP applied to perception [3] translates the
drawing episodes. This learning method is interesting from
inferenceonthecausesofsensoryobservationsintoagradient
a developmental point of view since it implements goal-
descent on free energy, and aligns nicely with the predictive
babbling. Developmental psychology tells us that learning
coding [4] and Bayesian brain hypotheses [5]. On the other
sensorimotor contingencies [13] plays a key role in the de-
hand, FEP applied to action, or active inference [6], [7], can
velopment of young infants. In [14], the authors present a
explain motor control and decision making as an optimisation
review about sensorimotor contingencies in the fields of de-
of free energy constrained by prior beliefs. In this work, we
velopmentalpsychologyanddevelopmentalrobotics,inwhich
they propose a very general model on how a learning agent
ThisworkwasfundedbytheCergy-ParisUniversityFoundation(Facebook
grant)andpartiallybyLabexMME-DII,France(ANR11-LBX-0023-01). should organise its exploration of the environment to develop
0202
yaM
11
]EN.sc[
1v15150.5002:viXra
its sensorimotor skills. They suggest that the agent should weights. However we showed in a previous work [8] that it is
continuously sample goals from its state space and practice possibletocontroltheRNNdynamicsbyoptimisingitsinitial
achieving these goals. The work we propose in this article state.Inthisarticlethisisthestrategyweuseinordertolearn
aligns nicely with their suggestion, as our agent randomly motor primitives.
samples goals from a discrete state space, and optimises the Our implementation of the reservoir network is based on
motor sequences leading to these discrete states. an existing model for handwriting trajectory generation using
In the following we will first present the model, then the RC [15], using the following equations :
results on motor primitives learning. In a third part, we will 1 1
evaluate the learned repertoires on a task of motor primitive u(t)=(1− τ )·u(t−1)+ τ (W r ·r(t−1)) (1)
chaining to draw complex trajectories.
r(t)=tanh(u(t)) (2)
II. METHODS a(t)=tanh(W
o
·r(t)) (3)
A. Model for motor primitives learning
where u and r denote the network activation respectively
Ourneuralnetworkarchitecture,representedinfigure1,can before and after application of the non-linearity. We denote
besegmentedintothreesubstructures.Onthesensorypathway by τ the time constant of the network.
(top), a Kohonen map is used to cluster the observations. For the recurrent network to have a self-sustained activity,
On the motor pathway (bottom), a reservoir network and a wereferredtotheweightsinitialisationusedin[15].Therecur-
controller are used to model the generation and optimisation rent weights matrix W is made sparse with each coefficient
r
of motor sequences. having a probability p of being non-null. When non-null, the
r
coefficients are sampled from a normal distribution N(0,σ r 2 )
nr
Kohonen with a variance scaled according to the network size n . The
r
q(s) readout weights W are sampled from a normal distribution
o
o s N(0,σ2).
o
p(o|s) 2) Kohonen map: The Kohonen map [1] takes as input a
64x64 gray scale image. Each filter learned by the Kohonen
env F p(s) map has to correspond to a distinct motor primitive. Since
we expected to learn motor primitives corresponding mainly
Reservoir Controller
to movement orientations, we used a Kohonen map topology
withonlyonecyclicdimension.Wealsoranexperimentswith
[a] [r] x k
2-d and 3-d topologies with and without cyclic dimensions.
We chose to stick with the 1-d cyclic topology because it
Fig. 1: Model for motor primitives learning. x denotes the presented a fast learning and a balanced use of all the learned
activation signal. [r] denotes the sequence of activations filters. Here are the equations of the Kohonen network:
{r } of the reservoir network. [a] denotes the sequence
t t=1..T i (t)=argmin((cid:107)W [i,:](t)−o(t)(cid:107)2) (4)
of atomic motor commands {a t } t=1..T decoded from the w i k 2
reservoir dynamics. o denotes visual observation. s denotes
W (t+1)=(1−λ )·W (t)+λ ·N(i (t))(cid:12)o(t) (5)
k k k k w
the hidden state. k denotes the primitive index on which
depend the activation signal x and the prior probability over where W k denotes the Kohonen weights, each row W k [i,:]
hidden state p(s). F denotes the free energy, it is used as an corresponding to the filter associated with neuron of index
optimisation signal for the controller. i. i w denotes the winner neuron index, i.e. the index of
the Kohonen neuron whose associated filter is closest to
The activation signal x stimulates the random recurrent the input stimulus o. The neighbourhood function N(i (t))
w
neural network (RNN), that exhibits a self-sustained activity depends on the chosen topology. It is maximum in i (t) and
w
r during T time steps. T atomic motor commands a are decreases exponentially according to the distance with regard
readout from the T activations of the recurrent network. The to the winner neuron index i (t). This exponential decay is
w
environmentprovidesanobservationoafterbeingimpactedby parameterised by a neighbourhood width σ2. The operator (cid:12)
k
the sequence of actions. This observation is then categorised denotes the element-wise product.
in s∈S ={s } by a Kohonen network. 3) Free energy derivations: Our goal is to learn the right
i i<n
1) Reservoir network: Motor primitives are sequences of activation signals x that stimulate the random recurrent net-
atomic motor commands read out from the dynamics of the work in a way that leads to desired categorisations of the
reservoir network. To optimise these sequences according to observationobytheKohonennetwork.Letnbethenumberof
somecriteria,onecaneitheroptimisethereadoutweights,the motor primitives that we want to learn. We set the size of the
dynamics of the reservoir or control its activity (our case). Kohonen network as well as the number of activation signals
Updating the recurrent weights of the RNN can have the tolearnton.Forthetrajectoryofindexk,wewanttolearnthe
undesirable effect of limiting its dynamics. Thus in RC, the stimulusx∗ thatbetteractivatesthecorrespondingcategoryin
k
focus is usually put on the learning of appropriate readout theKohonennetwork(i.e.suchthatp(o|s=s )≈1).Wecan
k
observe that the optimal activation signal x∗ depends on the close to the observation. Maximising accuracy induces the
k
weights of the Kohonen map. Because of this dependence, it network to generate trajectories that are as close as possible
wouldbeeasiertolearnandfixtheKohonenmapbeforeopti- to one of the Kohonen filter. For simplicity, we will call this
misationofthecontroller.However,itismorerealisticfroma quantity inaccuracy instead of opposite of accuracy.
developmentalpointofviewtotrainthesetwostructuresatthe Summing those two quantities, minimising free energy
same time. For this reason, we learn both networks in parallel would result in observations that are close to one of the
but control their learning parameters over time, to be able to Kohonen filter, and in this Kohonen filter being the one with
favor the learning of one network compared to the other. the highest prior probability.
Weusefree-energyminimisationasthestrategytotrainthe 4) Optimisation method: Our optimisation problem is the
controller.Whatfollowsisaformalisationofourmodelusing following. For each primitive k, we want to find an activation
a variational approach: signal x that generates an observation o resulting in a low
k
• p(s) is the prior probability over states. Here we propose free energy F 1 (o). To use gradient based methods, we would
using a softmax, parameterised by β > 0, around the need to have a diiferentiable model of how the activation
index k of the current primitive. signals x k impacts the resulting free energy F 1 (o). Since
we do not have a model of how the environment produces
exp(−β·|k−i|)
p(s=s i )= (cid:80) exp(−β·|k−j|) (6) observations, the whole x → r → a → o → F 1 (o) chain
j is not differentiable. To solve our problem, we instead use a
• p(o|s)isthestateobservationmapping.Theobservations randomsearchoptimisationmethod,detailedbythefollowing
areimagesofsized.Forsimplicity,wemaketheapprox- algorithm.
imation of considering all pixel values as independent. {Random initialisation of the controller}
We choose to use Bernoulli distributions for all pixel for k <n do
valueso l<d ∈{0,1}.Sinceallpixelvaluesareconsidered x k ←N(0,1)
independent, the probability distribution over the whole end for
observation can be factorised as: {Training}
(cid:89) for e<E do
p(o|s=s
i
)= W
k
[i,l]ol ·(1−W
k
[i,l])1−ol (7)
k ←U(n)
l<d δx∼N(0,σ2(e))
where W [i,l] is the value of pixel l of the filter i of the u ←x +δx
k + k
Kohonen map. u ←x −δx
− k
• q(s) is the approximate posterior probability over states [a 1 ,...,a T ] + ← simulate action(u + )
knowing the observation o. Here we define q(s) to be [a ,...,a ] ← simulate action(u )
1 T − −
theone-hotdistributionoverstatessuchthatq(s=s )= o ← env([a ,...,a ] )
i + 1 T +
δ , where i is the index of the Kohonen neuron with o ← env([a ,...,a ] )
iw,i w − 1 T −
the highest activation (i.e. whose filter is the closest to i ← simulate kohonen(o )
w,+ +
the observation): i =argmin ((cid:107)W [j,:]−o(cid:107)2). i ← simulate kohonen(o )
w j k 2 w,− −
We can now derive the free-energy computations using this f + ← free energy(i w,+ ,o + )
model: f − ← free energy(i w,− ,o − )
(cid:88) x k ←x k −λ·(f + −f − )·δx
F (o)=KL(q(s)||p(s))− q(s )log(p(o|s )) (8)
1 i i end for
i<n The parameter e in the search standard deviation σ2(e)
= (cid:88) q(s )log q(s i ) − (cid:88) q(s )log(p(o|s )) (9) indicates that this coefficient can depend on the training
i p(s ) i i
i episode e. The ”simulate action” function used in the code
i<n i<n
=−log(p(s ))−log(p(o|s )) (10) above corresponds to the iterative application of equations
iw iw
(1), (2), (3) for the duration T of the motor primitives. The
The first term of the free energy in eq. (8) is a quantity called
”env”functioncorrespondstothegenerationofanobservation
complexity. It scores how complex the approximate posterior
by the environment after being impacted by the sequence of
is compared to the prior. It decreases when q(s) and p(s)
actions. This computation is performed by the environment
are close. In our case, it is minimal when i = k, meaning
w and unknown to the agent. The ”simulate kohonen” function
that the category chosen by the Kohonen map is the one
corresponds to the application of equations (4) and (5). The
withthehighestpriorprobability.Minimisingcomplexitythus
”free energy”functioncorrespondstotheapplicationofequa-
induces the network to generate trajectories that activate the
tion (10).
right Kohonen category.
B. Experimental setup
Thesecondtermistheoppositeofthequantitycalledaccu-
racy. Accuracy measures how good the approximate posterior 1) Environment: The environment is an initially blank
probability q(s) is at predicting the observation o. Here, it canvas on which the agent can draw. The initial position of
increases when the Kohonen filter of the winner neuron is the pen is at the center of the canvas. The agent can act on
the environment via 2D actions. The actions are the angle
velocities of a 2 degrees of freedom arm as represented in
figure 2.
Fig. 2: Initial (left) and final (right) arm position for a
trajectory taken from a data set of handwriting trajectories.
2) Training: We start from a random initialisation of the
Kohonen map. Over time, the Kohonen map self-organises
when being presented with the trajectories generated by the
random search algorithm. Simultaneously, the random search
algorithms learns to generate motor trajectories that lead to
the different Kohonen prototype observations.
Training was performed using the following set of param-
eters for the different components described in the previous
section:
• RNN: n r =100, τ =10, p r =0.1, σ r 2 =1,5.
• Readout layer: n o =2, σ o 2 =1.
• Kohonenmap:n=50,λ k =0.01,Kohonenwidthσ k 2(e)
varies over time, see figure 3.
• Freeenergy:β ∈{2−5,2−4,2−3,2−2,2−1,1,2,4,8,16}.
• Random optimisation: λ=0.01, σ2(e) varies over time,
see figure 3.
10
8
6
4
2
0
0 2500 5000 7500 10000 12500 15000 17500 20000
Training iteration
htdiw
nenohoK
Search variance 1.0
Kohonen width
0.8
0.6
0.4
0.2
0.0
ecnairav
hcraeS
[1,n]. We train on the kth primitive by adjusting the prior
probability as in (6) and optimising x . On average, each
k
activation signal x is trained on 400 iterations. Figure 3
k
displays the evolution of the random optimisation search
variance and of the Kohonen width over the 20000 iterations.
We discuss this choice in the light of the following results.
175
150
125
100
75
50
25
0
0 50 100 150 200 250 300 350
Training iteration
Fig. 3: Search variance σ2(e) and Kohonen width σ2(e)
k
according to training iteration e.
C. Results
We trained our model for E =20000 iterations on n=50
primitives. At each iteration, we uniformly sample k from
)stan(
ytixelpmoC/ycaruccanI
Inaccuracy
Complexity
Fig. 4: Inaccuracy and complexity averaged on the number
of primitives n = 50, with β = 8. Each primitive has been
sampled on at least 360 iterations, and on average on 400
iterations.
Figure 4 displays the evolution of inaccuracy and complex-
ity during training.
During the first phase, when e < 12500, the random
searchhasaveryhighvariance.Consequently,thetrajectories
generated and fed to the Kohonen map are very diverse and
thisallowstheKohonenmaptoself-organise.Inaccuracydoes
not seem to decrease in this early phase. This is because
the Kohonen filters, initially very broad, are becoming more
precise. The high variance in the random search allows for
a diminution of complexity but still generates trajectories that
aretoonoisytoaccuratelyfitthemorepreciseKohonenfilters.
During the second phase, we decrease the variance of the
random search. The system can now converge more precisely
and this causes a faster decrease of both inaccuracy and
complexity.
We can question whether it is necessary for the random
search variance to remain high for such a long time, since
it slows down learning. We observed that if we reduce the
duration of the first phase, the Kohonen does not have the
timetoselforganiseandthisresultsinanentangledtopology.
Because of the complexity term in the free energy computa-
tions, the topology of the Kohonen has an influence over the
learning. For instance, with an entangled topology, a search
direction for x that activates a neuron closer to k might not k
maketheactualtrajectoryclosertotheonesrecognisedbythe
kth Kohonenneuron.Inotherwords,havingapropertopology
smooths the loss function.
We also notice that the inaccuracy cannot decrease below a
certain value. At first, we could think that this is because the
optimisationstrategyisstuckinalocaloptimum.However,we
obtained the same lower bound on inaccuracy over different
training sessions. Since the optimisation strategy relies on
random sampling, there is no evident reason to encounter the
samelocalminimum.Ourexplanationisthatthislowerbound
is imposed by the Kohonen network neighbourhood function.
Because the Kohonen width does not reach 0, the Kohonen
centroids are still attracting each other and this prevents
them from completely fitting the presented observations. In
consequence, the filters are always partly mixed with their
neighboursandthiscausestheinaccuracytoplateauatavalue
that depends on σ2.
k
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
10 1 100
Beta
gniniart
fo
dne
ta
ecnatsid
egarevA
Fig.6:9ofthe50learnedmotorprimitivesandcorresponding
Kohonenfilters:k ∈{0,5,10,15,20,25,35,40,45}.Theblue
component of the image corresponds to the trajectory that is
actually being generated by the network for the activation
signal x . The red component of the image corresponds to
k
the Kohonen filter of index k.
Fig.5:Averagedistance|i −k|betweentheactivatedneuron
w
in the Kohonen i and the primitive index k, according to β.
w that the minimisation of complexity successfully enforced the
mapping between i and k.
Figure 5 shows the impact of the parameter β over the w
convergence. Looking at the equations (6) and (10) we can III. CHAININGOFMOTORPRIMITIVES
see that this parameter directly scales the overall complexity.
To validate our approach, we still need to show that this
For low values of β, the random search is more likely to
repertoire of motor primitives is efficient at constructing more
be stuck in local minima of free-energy, when activating
complex movements. To perform this evaluation, we propose
a Kohonen neuron closer to k corresponds to an increase
to extend the model presented in section II.
in inaccuracy that exceeds the decrease in complexity. We
First, we define a new perception module meant to classify
measuredtheaveragedistancebetweentheactivatedneuronin
sensory observations into states corresponding to more com-
the Kohonen and the primitive index k at the end of training
plex trajectories. To avoid confusion, we will denote this new
for β ∈ {2−5,2−4,2−3,2−2,2−1,20}. The results, presented
hidden state σ.
in figure 5, confirm that the final states obtained with higher
Second, we enable in this revisited model the chaining
values of β correspond to a more precise mapping between
of motor primitives. In the previous model, each drawing
i (winner index of the Kohonen map) and k (state index
w episodecorrespondstoonemotortrajectoryoflengthT being
enforced by the prior probability).
generated. Here in each drawing episode the agent will draw
Figure6displayssomeofthelearnedmotorprimitives.The
a trajectory corresponding to M motor primitives of length T
bluecomponentoftheimagecorrespondstothetrajectorythat
chained together.
is actually being generated by the reservoir network for the
Sincewearesimplytryingtoevaluatetheprimitiveslearned
activation signal x . The red component of the image corre-
k inthefirstmodel,wewon’taddressthetrainingofthenetwork
sponds to the Kohonen filter of index k. This figures allows
used to classify sensory observations. Instead, our focus will
visual confirmation of several points. First, the inaccuracy at
be on the decision making process, i.e. the selection of the
theendoftrainingseemstoindeedcomefromtheblurrinessof
primitives {k ,...,k } to chain in order to reach a certain
1 M
theKohonenfilters.Second,thefiltersandmotorprimitivetra- desired state σ∗.
jectoriesseemtofollowatopology:theindexoftheprimitive
A. Model
seemshighlycorrelatedwiththeorientationoftheroutetaken
bythearmendeffector.Finally,everytrajectoryseemstobein The model for motor primitives chaining is presented in
thecenterofthecorrespondingKohonenfilter,whichsuggests figure 7.
Learned classifier motorprimitivekwillinfluencethestate.Thisprobability
q(σ)
distribution over hidden states corresponds to the output
o σ
of the learned classifier when fed with the observation
p(o|σ)
p(σ) resultingfromtheapplicationofthekth motorprimitive.
env F π • AsintheprimitivelearningmodelpresentedinsectionII,
thestateobservationmappingp(o|σ)isbuiltusingequa-
Reservoir Controller
tion (7). The filters used for each category correspond to
[a] [r] [x] [k] the average of the observations belonging to this class
obtained from the data set.
Fig. 7: Model for motor primitives chaining. π = p(σ) We can now derive the expected free-energy using this
denotes the prior probability over states σ. [k] denotes model:
the sequence of motor primitives indices {k } . [x]
m m=1..M (cid:88)
denotes the sequence of corresponding activation signals E[F 2 (k)]=KL(q k (σ)||p(σ))− q k (σ i )H(p(o|σ i )) (11)
{x } .[r]denotestheresultingsequenceofactivations i<n
m m=1..M
{r } of the reservoir network. [a] denotes the
m,t m=1..M,t=1..T whereH(p(o|σ ))denotestheentropyofthestateobservation
i
sequence of atomic motor commands {a }
m,t m=1..M,t=1..T mapping for state i.
decoded from the reservoir dynamics. o denotes the visual
When the agent has to make a decision about which motor
observation provided by the environment after being modified
primitive to use, it computes its expected future free energy
by the sequence of atomic motor commands. σ denotes the
E[F (k)] for each possible primitive and selects the primitive
hiddenstate,itisdifferentfromthehiddenstateoftheprevious 2
with the minimum expected free energy.
section. F denotes the expected free energy, it is used as
This time again, expected free energy can be segmented
an optimisation signal for the choice of sequence of motor
as complexity and inaccuracy. Minimising the first term of
primitives indices {k } .
m m=1..M equation(11)willresultinchoosingmotorprimitivesthatlead
to hidden states matching our prior preferences. This can be
seenasdirectlyoptimisingreward.Minimisinginaccuracywill
1) New hidden state: The hidden state σ corresponds to
result in choosing actions that lead to hidden states with high
new categories representing complex motor trajectories. We
precision(lowentropy)inthestateobservationmapping.This
used the Character Trajectories Data Set from [16] composed
connectstothedriveofsurpriseavoidanceinherenttothefree
ofapproximately70trajectoriesforeachletterofthealphabet
energy principle.
that can be drawn without lifting the pen. The trajectories are
3) Action selection: The following algorithm details the
provided as sequences of pen positions. We drew these trajec-
actionselectionprocessonatrajectorycomposedofM motor
tories using our drawing environment and used the resulting
primitives using free energy minimisation :
observations to build the new state observation mapping.
p(σ)← init()
2) Expectedfreeenergyderivations: Themodelusesactive
for m<M do
inferencefordecisionmaking.Itselectsactionsthatminimise
for k <n do
afreeenergyfunctionconstrainedbypriorbeliefsoverhidden
u←x
states. According to active inference, constraining the prior k
[a ,...,a ]← simulate action(u)
probability over states to infer a state distribution that favors 1 T
o← env model([a ,...,a ])
a target state σ∗ will force the agent to perform actions 1 T
q (σ)← classifier(o)
that fulfill this prediction. In this sense, the prior probability k
f ← expected free energy(q (σ),p(σ),o)
over states can be compared to the definition of a reward in k k
end for
reinforcement learning. For instance, a rewarding state would
k∗ ← argmin (f )
be a state that is more likely under the prior probability over m k k
u∗ ←x
states. k m ∗
[a ,...,a ]← simulate action(u)
1 T
Here is the formalisation of this model in the variational
env([a ,...,a ])
1 T
framework:
end for
• Prior probability over states p(σ) acts similarly to a Thefunction”env model”correspondstoalearnedforward
reward function in reinforcement learning. The prior model that allows estimating the future observations for
beliefs (or prior preferences) over σ will be set manually any sequence of actions. In our experiments, we simply
to different values during testing to guide the agent into simulated the actions and rewound the environment, which
a desired state. (in a deterministic environment) corresponds to having a
• The other probability distribution over states is one that perfect forward model (env = env model). The function
the agent has control on. By choosing one primitive in ”classifier” corresponds to the classification of the ob-
the learned repertoire, the agent selects one resulting servation by a learned classifier. It outputs a probability
distributionoverstatesq (σ)modelinghowthechoiceof distribution over hidden states σ. Finally, the function
k
”expected free energy” corresponds to the application of
eq. (11).
B. Results
(a) (b) (c)
Fig.8:Exampleoffilterandproducedtrajectories.Left:Filter
for the category ’s’. Middle and right: trajectories produced
by chaining five motor primitives belonging to two different
learned repertoires.
In our tests, the hidden state σ was build using five classes
corresponding to the letters ’c’, ’h’, ’i’, ’s’, ’r’. We chose
theselettersbecausethetrajectoriesinsideeachcategorywere
relatively close and this allowed for filters of low entropy.
Figure8displaysonefilterofthelearnedclassifierandtwo
trajectories generated by chaining five motor primitives from
twodifferentprimitiverepertoireslearnedwithourmodel.The
trajectories were obtained by setting the prior preferences to
0.96forthecategory’s’and0.01forthefourothercategories.
To verify that our model learns a valuable repertoire of
motor primitives, we compare the quality of the constructed
complex trajectories (as in 8b and 8c) with our model and
with random repertoires.
Random repertoires are built using the same RNN and
readout layer initialisations. They differ from the learned
repertoires in the fact that we do not optimise the activation
signals x of the reservoir. The initial states of the reservoir
k
used to generate the primitives are taken as x ∼N(0,1). In
k
other words, they are equivalent to the repertoires our model
would provide before learning.
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
50 25 10
Repertoire size
)stan(
ytixelpmoc
egarevA
Figure 9 displays the average complexities measured at
the end of the episode. For each episode, we set the prior
preferencesofoneofthelettercategoryto0.96andtheothers
to 0.01. The values are averaged over the different letters and
for 5 different repertoires, learned or random.
The complexity scores how close the recognition probabil-
ity q(σ) (provided by the learned classifier) is to the prior
preferences p(σ) and thus constitutes a suitable indicator for
comparison. For low complexities, the constructed images are
close to the filter, as in 8.
We observe that the average complexity tends to be lower
for repertoires of larger sizes, independently of the type of
repertoire. Having a larger repertoire of primitives indeed
should be an asset in order to reconstruct more complex
trajectories. For every repertoire size, we measure a lower
complexitywithrepertoireslearnedusingthemodeldescribed
in section II.
IV. CONCLUSION
The results displayed in section III show that our model is
able to learn repertoires of motor primitives that are efficient
at building more complex trajectories when combined.
To further validate our approach, it would be interesting to
compare our results with other strategies for motor primitive
learning. On the one hand, there is existing work in devel-
opmental robotics prescribing guidelines to build repertoires
of motor primitives [14], [17], but they don’t provide neural
network implementation to be used for comparison.
On the other hand, the option discovery literature in hier-
archical reinforcement learning provides practical methods to
build repertoires of relevant options. Options were introduced
in[18]asacandidatesolutiontoaddresstheissueoftemporal
scaling in reinforcement learning. An option is defined as a
temporally extended action, and thus is conceptually similar
to a motor primitive. It would be interesting to measure how
ourapproachcompareswithcurrentstateofthearttechniques
for unsupervised option learning such as [19].
ACKNOWLEDGMENT
This work was funded by the Cergy-Paris University
Foundation (Facebook grant) and Labex MME-DII, France
(ANR11-LBX-0023-01).
Learned repertoire
Random repertoire REFERENCES
[1] T.Kohonen,“Self-organizedformationoftopologicallycorrectfeature
maps,”BiologicalCybernetics,vol.43,pp.59–69,1982.
[2] K. Friston and J. Kilner, “A free energy principle for the brain,” J.
Physiol.Paris,vol.100,pp.70–87,2006.
[3] K. Friston, “Hierarchical models in the brain,” PLOS Computational
Biology,vol.4,no.11,pp.1–24,112008.
[4] R. Rao and D. Ballard, “Predictive coding in the visual cortex a
functionalinterpretationofsomeextra-classicalreceptive-fieldeffects,”
NatNeurosci,vol.2,pp.79–87,1999.
[5] P.Dayan,G.E.Hinton,R.M.Neal,andR.S.Zemel,“Thehelmholtz
machine,”NeuralComputation,vol.7,pp.889–904,1995.
[6] K. Friston, J. Daunizeau, and S. Kiebel, “Reinforcement learning or
activeinference?”PLoSONE,vol.4,no.7,p.e6421,2009.
Fig.9:Averagecomplexityforlearnedandrandomrepertoires [7] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,J.ODoherty,and
G.Pezzulo,“Activeinferenceandlearning,”Neuroscience&Biobehav-
of different sizes.
ioralReviews,vol.68,pp.862–879,2016.
[8] A. Pitti, P. Gaussier, and M. Quoy, “Iterative free-energy optimization
for recurrent neural networks (inferno),” PLoS ONE, vol. 12, no. 3, p.
e0173684,2017.
[9] D. Verstraeten, B. Schrauwen, M. DHaene, and D. Stroobandt, “An
experimentalunificationofreservoircomputingmethods,”NeuralNet-
work,vol.20,pp.391–403,2007.
[10] M. Lukoeviius and H. Jaeger, “Reservoir computing approaches to
recurrent neural network training,” Computer Science Review, vol. 3,
no.3,pp.127–149,2009.
[11] J.NamikawaandJ.Tani,“Learningtoimitatestochastictimeseriesin
a compositional way by chaos,” Neural Networks, vol. 23, no. 5, pp.
625–638,2010.
[12] F.MannellaandG.Baldassare,“Selectionofcorticaldynamicsformotor
behaviour by the basal ganglia,” Biological Cybernetics, vol. 109, pp.
575–595,2015.
[13] J. K. O’Regan and A. No, “A sensorimotor account of vision and
visual consciousness,” Behavioral and Brain Sciences, vol. 24, no. 5,
p.939973,2001.
[14] L. Jacquey, G. Baldassare, V. Santucci, and O. J.K., “Sensorimotor
contingencies as a key drive of development: From babies to robots,”
FrontiersinNeuroRobotics,vol.13,no.98,2019.
[15] R.LajeandD.Buonomano,“Robusttimingandmotorpatternsbytam-
ingchaosinrecurrentneuralnetworks,”NatureNeuroscience,vol.16,
no.7,pp.925–935,2013.
[16] D. Dua and C. Graff, “Uci machine learning repository,”
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
SchoolofInformationandComputerScience,2019.
[17] E.Ugur,E.Sahin,andE.O¨ztop,“Self-discoveryofmotorprimitivesand
learninggraspaffordances,”2012IEEE/RSJInternationalConferenceon
IntelligentRobotsandSystems,pp.3260–3267,2012.
[18] R.S.Sutton,D.Precup,andS.Singh,“Betweenmdpsandsemi-mdps:A
frameworkfortemporalabstractioninreinforcementlearning,”Artificial
Intelligence,vol.112,no.1,pp.181–211,1999.
[19] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine, “Diversity is all
you need: Learning skills without a reward function,” CoRR, vol.
abs/1802.06070,2018.