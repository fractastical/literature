JournalofMathematicalPsychology107(2022)102632
Contents lists available at ScienceDirect
JournalofMathematicalPsychology
journal homepage: www.elsevier.com/locate/jmp
Tutorial
Astep-by-steptutorialonactiveinferenceanditsapplicationto
empiricaldata
RyanSmitha,1,∗,KarlJ.Friston b,ChristopherJ.Whyte c,1
aLaureate Institute for Brain Research, Tulsa, OK, USA
bWellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, WC1N 3AR, UK
cMRC Cognition and Brain Sciences Unit, University of Cambridge, Cambridge, UK
a r t i c l e i n f o
Article history:
Received1March2021
Receivedinrevisedform11September2021
Accepted15November2021
Availableonline xxxx
Keywords:
Activeinference
Computationalneuroscience
Bayesianinference
Learning
Decision-making
Machinelearning
a b s t r a c t
The active inference framework, and in particular its recent formulation as a partially observable
Markov decision process (POMDP), has gained increasing popularity in recent years as a useful
approach for modeling neurocognitive processes. This framework is highly general and flexible in
itsabilitytobecustomizedtomodelanycognitiveprocess,aswellassimulatepredictedneuronal
responsesbasedonitsaccompanyingneuralprocesstheory.Italsoaffordsbothsimulationexperiments
for proof of principle and behavioral modeling for empirical studies. However, there are limited
resourcesthatexplainhowtobuildandrunthesemodelsinpractice,whichlimitstheirwidespread
use.Mostintroductionsassumeatechnicalbackgroundinprogramming,mathematics,andmachine
learning.Inthispaperweofferastep-by-steptutorialonhowtobuildPOMDPs,runsimulationsusing
standardMATLABroutines,andfitthesemodelstoempiricaldata.Weassumeaminimalbackground
inprogrammingandmathematics,thoroughlyexplainallequations,andprovideexemplarscriptsthat
canbecustomizedforboththeoreticalandempiricalstudies.Ourgoalistoprovidethereaderwiththe
requisitebackgroundknowledgeandpracticaltoolstoapplyactiveinferencetotheirownresearch.
Wealsoprovideoptionaltechnicalsectionsandmultipleappendices,whichoffertheinterestedreader
additionaltechnicaldetails.Thistutorialshouldprovidethereaderwithallthetoolsnecessarytouse
thesemodelsandtofollowemergingadvancesinactiveinferenceresearch.
©2021TheAuthor(s).PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-ND
license(http://creativecommons.org/licenses/by-nc-nd/4.0/).
Introduction
Active inference,andinparticularitsrecentapplicationto
partially observable Markov decision processes (POMDPs; de-
finedbelow),offersaunifiedmathematicalframeworkformod-
elingperception,learning,anddecisionmaking(DaCosta,Parr
etal.,2020;Friston,Parr,&deVries,2017c;Friston,Rosch,Parr,
Price,&Bowman,2018;Parr&Friston,2018b).Thisframework
treatseachofthesepsychologicalprocesses,andtheirinterac-
tions, as interdependent forms of inference. Namely, decision-
makingagentsareassumedtoinfertheprobabilityofdifferent
externalstatesandeventsintheenvironment–includingtheir
own actions – by combining prior beliefs with sensory input.
Unlike ‘passive’, perceptual inference processes (e.g., inferring
the presence of an external object based on patterns of light
∗ Correspondence to: Laureate Institute for Brain Research,6655 S Yale
Ave,Tulsa,OK74136,USA
E-mail address: rsmith@laureateinstitute.org(R.Smith).
1 Theseauthorscontributedequally.
impinging on the retina), the inferences underlying decision-
making are ‘active’, in the sense that the agent infers the ac-
tions most likely to generate preferred sensory input (e.g., in-
ferringthateatingsomefoodwillreduceafeelingofhunger).
Agentsalsoinfertheactionsmostlikelytoreduceuncertainty
and facilitate learning (e.g., inferring that opening the fridge
will reveal available food options). This leads decision-making
to favor actions that optimize a trade-off between maximiz-
ingrewardandinformationgain.Theresultingpatternsofper-
ceptionandbehaviorpredictedbyactiveinferencematchwell
with those observed empirically (e.g., see Smith et al., 2021d,
2021c,2020b;Smith,Kuplicki,Teed,Upshaw,&Khalsa,2020c;
Smithetal.,2021e,2020e).Theneuralprocesstheoryassociated
with active inference has also successfully reproduced empiri-
callyobservedneuralresponsesinmultipleresearchparadigms
and generated novel, testable predictions (Friston, FitzGerald,
Rigoli,Schwartenbeck,&Pezzulo,2017a;Schwartenbeck,FitzGer-
ald,Mathys,Dolan,&Friston,2015;Whyte&Smith,2020).Due
totheseandotherconsiderations,thisframeworkhasbecome
increasinglyinfluentialinrecentyearswithinpsychology,neu-
roscience,andmachinelearning.
https://doi.org/10.1016/j.jmp.2021.102632
0022-2496/© 2021TheAuthor(s). PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
nc-nd/4.0/).
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Over the last decade there have been many articles that
offer either (1) broad intuitions about the workings and po-
tentialimplicationsofactiveinference(e.g., (Badcock,Friston,
Ramstead, Ploeger, & Hohwy, 2019; Clark, 2013, 2015; Clark,
Watson,&Friston,2018;Hohwy,2014;Pezzulo,Rigoli,&Friston,
2015, 2018; Smith, Badcock, & Friston, 2020a)), or (2), tech-
nical presentations of the mathematical formalism and how it
continues to evolve (e.g., Da Costa, Parr et al., 2020; Friston
etal.,2016a,2017a,2017c;Hesp,Smith,Allen,Friston,&Ram-
stead, 2020; Parr & Friston, 2018b). However, for those first
becoming acquainted with this field, the former class of ar-
ticles does not provide sufficient detail to instill a thorough
understanding of the framework, leading to potential misun-
derstandingandpotentiallyinaccurateempiricalpredictions.At
theotherextreme,thelatterclassofarticlesishighlytechnical
and requires considerable mathematical expertise, familiarity
withnotationalconventions,andthebroaderabilitytotranslate
themathematicalformalismintoempiricalpredictionsrelevant
to a given field of study. This has made the active inference
literaturelessaccessibletoabroaderaudiencewhomightoth-
erwise benefit from engaging with it. To date, there are also
relatively few materials available for students seeking to gain
the practical skills necessary to build active inference models
and apply them to their own research aims (although some
veryhelpfulmaterialhasbeenpreparedbyothers;e.g.,Philipp
Schwartenbeck [https://github.com/schwartenbeckph] and Oleg
Solopchuk [https://medium.com/@solopchuk/tutorial-on-active-
inference-30edcf50f5dc]).
The goal of this paper is to provide an accessible tutorial
on the POMDP formulation of active inference that is easy to
followforreaderswithoutupper-levelundergraduate/graduate-
level training in mathematics and machine learning, while si-
multaneously offering basic mathematical understanding — as
well as the practical tools necessary to build and use active
inference models for their own purposes. We review the con-
ceptualandformalfoundationsandprovideastep-by-stepguide
on how to use code in MATLAB (provided in theappendices
and supplementary code) to build active inference (POMDP)
models,runsimulations,fitmodelstoempiricaldata,perform
modelcomparison,andperformfurtherstepsnecessarytotest
hypothesesusingbothsimulatedandfittedempiricaldata(all
supplementarycode canalsobefoundat:https://github.com/
rssmith33/Active-Inference-Tutorial-Scripts; we note here that
there is also a recently developed python implementation of
activeinferencethatcanbefoundat:https://github.com/infer-
actively/pymdp).Wehavetriedtoassumeaslittleaspossible
about the reader’s background knowledge in hopes of making
thesemethodsaccessibletoresearchers(e.g.,psychologistsand
neuroscientists)withoutastrongbackgroundinmathematicsor
machinelearning.However,wehavealsoincludedsectionsthat
provideadditionaltechnicaldetail,whichthepragmaticreader
can safely skip over and still follow the practical tutorial as-
pectsofthepaper.Wehavealsoprovidedadditionalmaterialin
appendicesandsupplementarycode with:
(1) Definitional material to help the non-expert reader who
wouldliketoattemptthetechnicalsections.
(2) Additionalmathematicaldetailforinterestedreaderswith
astrongertechnicalbackground.
(3) Pencil-and-paperexercisesthathelpbuildanintuitionfor
thebehaviorofthesemodels.
(4) Astrippeddownbutwellcommentedversionofthemost
commonlyusedmodelinversionscript(describedbelow)
forrunningsimulations,whichcanserveasaspringboard
forreadersseekingadeeperunderstandingofthecodethat
implementsthesemodels.
Throughoutthearticle,wewillrefertotheassociatedMATLAB
code,assumingthereaderisworkingthroughthepaperandthe
codeinparallel.
Whileweassumeaslittlemathematicalbackgroundaspos-
sible, some limited knowledge of probability theory, calculus,
and linear algebra will be necessary to fully appreciate some
sections of the tutorial. Building models in practice also re-
quires some basic familiarity with the MATLAB programming
environment.Werealizethatthisbackgroundknowledgeisnon-
trivial. However, to minimize these potential hurdles, we (1)
providethoroughexplanationswhenpresentingthemathemat-
ics and programming (with further expansion within optional
technical sections and in Appendix A), (2) include hands-on
examples/exercises in the companion MATLAB code, and (3)
provide pencil-and-paper exercises (see Appendix B and
Pencil_and_paper_exercise_solutions.mcode)thatreaderscan
workthroughthemselves.Intotal,thistutorialshouldofferthe
readerthenecessaryresourcesto:
(1) Acquireabasicunderstandingofthemathematicalformal-
ism.
(2) Buildgenerativemodelsofbehavioraltasksandrunsimu-
lationsofbothbehavioralandneuralresponses.
(3) Fitmodelstobehavioraldataandrecovermodelparam-
etersonanindividualbasis,whichcanthenbeusedfor
subsequent(e.g.,between-subjects)analyses.
Ourhopeisthatthiswillincreasetheaccessibilityanduseof
thisframeworktoabroaderaudience.Note,however,thatour
focus is specifically on the POMDP formulation, which models
timeindiscretestepsandtreatsbeliefsandactionsasdiscrete
categories(referredtoas‘discretestate–space’modelswith‘dis-
crete time’). This means that we do not cover a number of
othertopicsassociatedwithactiveinferenceandthebroaderfree
energyprinciplefromwhichitisderived.Forexample,wedo
notcover‘continuousstate–space’models,whichcanbeusedto
modelperceptionofcontinuousvariables(e.g.,brightness;fora
tutorial, see Bogacz (2017) as well as motor control processes
(e.g., controlling continuous levels of muscle contraction; see
(Adams,Shipp,&Friston,2013)andBuckley,SubKim,McGregor,
and Seth (2017)). Nor do we cover ‘mixed’ models, in which
discreteandcontinuousstate–spacemodelscanbelinked— al-
lowingdecisionstobetranslatedintomotorcommands(e.g.,see
Fristonetal.(2017c),Millidge(2019)andTschantzetal.(2021).
Wealsodonotcoverworkonfreeenergyminimizationinself-
organizingsystemsorthebasisofthefreeenergyprinciplein
physics.Themostthoroughtechnicalintroductiontothephysics
perspectivecanbefoundinFriston(2019);alesstechnical(but
stillrigorous)introductionispresentedinAndrews(2020). 2Thus,
thefocusofthistutorialissomewhatnarrowandpractical.Our
aim is to equip the reader with the understanding and tools
necessarytobuildmodelsinpracticeandapplythemintheirown
research.
2 Thisotherworkappealstoanumberofcommonconstructsdiscussedin
the free energy principle literature that are also not covered here, but which
thereadermayhavecomeacrosspreviously.Onesuchconstructisa‘Markov
blanket’,whichisamathematicalwayofdescribingtheboundarythatseparates
theinternalstatesofanorganismfromtheexternalenvironment(althoughnote
that this term is sometimes used in different ways; see Bruineberg, Dolega,
Dewhurst, and Baltieri (2021). Another related construct is a ‘non-equilibrium
steadystate(NESS)density’,whichdescribesthestatesanorganismmusthavea
highprobabilityofoccupyingifitistomaintainitsexistence— wherethiscan
beunderstoodasmaintainingtheintegrityofitsMarkovblanket(i.e.,keeping
the boundary intact that separates an organism from its environment). The
POMDP scheme described in this tutorial does not explicitly appeal to these
constructs; however, one can think of an agent’s preferred observations in
POMDPsasthosethatkeepitwithinthehigh-probabilitystatesconsistentwith
itscontinuedexistence(i.e.,thosethatwouldkeepitsMarkovblanketintact).
2
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
The paper is organized as follows. InPart 1, we introduce
the reader to the terms, concepts, and mathematical notation
usedwithintheactiveinferenceliterature,andpresentthemin-
imummathematicsnecessaryforabasicunderstandingofthe
formalism (as applied in a practical experimental setting). In
Part 2, we introduce the reader to the concrete structure and
elements of POMDPs and how they are solved. InPart 3, we
provideastep-by-stepdescriptionofhowtobuildagenerative
modelofabehavioraltask(avariantoncommonlyusedexplore–
exploittasks),runsimulationsusingthismodel,andinterpretthe
outputsofthosesimulations.In Part4,weintroducethereader
tolearningprocessesinactiveinference.In Part5,weintroduce
thereadertotheneuralprocesstheoryassociatedwithactive
inferenceandwalkthereaderthroughgeneratingandinterpret-
ingtheoutputsofneuralsimulationsthatcanbeusedtoderive
empiricalpredictions.In Part6,weintroducehierarchicalmodels
andillustratehow,basedontheneuralprocesstheory,theycan
beusedtosimulateestablishedelectrophysiologicalresponsesin
acommonlyusedauditorymismatchparadigm.Finally,in Part
7wedescribehowtofitbehavioraldatatoamodelandderive
individual-levelparameterestimatesandhowtheycanbeused
forfurthergroup-levelanalyses.
1. Basicterminology,concepts,andmathematics
1.1. Mathematical foundations: Bayes’ theorem and active inference
Theactiveinferenceframeworkisbasedonthepremisethat
perceptionandlearningcanbeunderstoodasminimizingaquan-
tity known asvariational free energy(VFE), and that action
selection,planning,anddecision-makingcanbeunderstoodas
minimizing expected free energy(EFE), which quantifies the
VFE of various actions based on expected future outcomes. To
motivatetheuseandderivationofthesequantities,weneedto
firstintroducethereadertoBayesianinferenceandexploreits
relationtothenotionofactiveinference.Wewillcoverthese
foundationalprincipleshere.Bytheendofthissubsection,the
readershouldhaveaworkingknowledgeofthebasicbuilding
blocks of active inference. This includes understanding what a
model is, how rules within probability theory can be used to
performinferencewithinamodel,andhowthisinferenceprocess
canbeextendedtoperformactionselection.
As an initial note to readers with less mathematical back-
ground,afullunderstandingoftheequationspresentedbelow
willnotbenecessarytobeginbuildingmodelsandapplyingthem
tobehavioraldata.Often,buildingandworkingwithmodelsin
practiceisagreatwaytogetanintuitivegraspoftheunderlying
mathematics.So,ifsomeoftheequationsbelowhaveunfamiliar
notation and become hard to follow, do not get discouraged.
Anintuitivegraspoftheconceptsdescribedinthissectionwill
beenoughtolearnthepracticalapplicationsinthesubsequent
sections.Thatsaid,wealsoexplaintheequationsandnotationin
thissectionassumingminimalmathematicalbackground.
We start by highlighting that the term ‘active inference’ is
basedontwoconcepts.Thefirstistheideathatorganisms ac-
tively engagewith(e.g.,movearoundin)theirenvironmentsto
gatherinformation,seekout‘preferred’observations(e.g.,food,
water,shelter,socialsupport,etc.),andavoidnon-preferredob-
servations (e.g., tissue damage, hunger, thirst, social rejection,
etc.). The second concept isBayesian inference, a statistical
procedurethatdescribestheoptimalwaytoupdateone’sbeliefs
(understoodasprobabilitydistributions)whenmakingnewob-
servations(i.e.,receivingnewsensoryinput)basedontherules
ofprobability(forabriefintroductiontotherulesofprobability,
seeAppendixA).Specifically,beliefsareupdatedinlightofnew
observations usingBayes’ theorem, which can be written as
follows:
p (s|o, m) = p (o|s, m) p (s|m)
p (o|m)
(1)
Startingontheright-handsideoftheequation,theterm p(s|m)
indicatestheprobability( p)ofdifferentpossible states(s)under
amodeloftheworld( m).This‘priorbelief’(the ‘prior’)encodesa
probabilitydistribution(‘Bayesianbelief’)withrespectto s before
makinganewobservation( o).Ingeneral,theconceptofa‘state’is
abstractandcanrefertoanythingonemighthaveabeliefabout.
Forexample, s mightrefertothedifferentpossibleshapesofan
object,suchasasquarevs.acirclevs.atriangle,andsoforth.The
termp (o|s, m) isthe ‘likelihood’termandencodestheprobabil-
itywithinamodelthatone would makeaparticularobservation
if somestatewerethetruestate(e.g.,observingastraightline
isconsistentwithasquareshapebutnotwithacircularshape).
Thesymbol (|) means‘conditionalon’andisalsooftenreadas
‘given’ (e.g., the probability ofo given s). The termp (o|m) is
the‘modelevidence’(alsocalledthe‘ marginallikelihood’)and
indicateshowconsistentanobservationiswithamodelofthe
worldingeneral(i.e.,acrossallpossiblestates).Finally,theterm
p (s|o, m) isthe ‘posterior’belief,whichencodeswhatone’snew
belief(i.e.,adjustedprobabilitydistributionoverpossiblestates)
optimallyshould be aftermakinganewobservation.
Inessence,Bayesruledescribeshowtooptimallyupdateone’s
beliefsinlightofnewdata.Specifically,toarriveatanewbelief
(yourposterior),youmust:(1)takewhatyoupreviouslybelieved
(your prior), (2) combine it with what you believe about how
consistent a new observation is with different possible states
(yourlikelihood),and(3)considertheoverallconsistencyofthat
observation with your model (i.e., how likely that observation
isunder any setofpossiblestatesincludedinyourmodel;the
modelevidence, p (o|m)).Thelaststep(i.e.,dividingby p (o|m))
ensuresthatyourposteriorbeliefremainsaproperprobability
distributionthatsumsto1(i.e.,itaccomplishes‘normalization’).
For a simple numerical example of Bayesian inference in the
contextofperception,seeFig.1.
In this tutorial, the concept of a model is key. As briefly
introducedabove,weherefocusspecificallyon generativemod-
els,whicharemodelsofhowobservations(sensoryinputs)are
generatedbyobjectsandeventsoutsideofthebrainthatcannot
beknowndirectly(typicallytermed‘ hiddenstates’or‘ hidden
causes’;e.g.,abaseballgeneratingaspecificpatternofactiva-
tion on the retina). In simple generative models (i.e., not yet
incorporatingaction),thenecessaryvariablescorrespondtothose
presentedwithinBayes’theoremabove(althoughnotethatcon-
ditioningonthemodelvariable m isoftenleftimplicit,aswe
willalsodogoingforward).Thatis,eachmodelincludesaset
ofpossiblehiddenstates( s),priorsoverthosestates p(s),asetof
possibleobservations( o;alsocalled‘ outcomes’),andalikelihood
thatspecifieshowstatesgenerateobservations p(o|s).Thenotion
ofhiddenorunobservablestatescausingobservableoutcomesil-
lustrateshowinferencecanbeseenasatypeof modelinversion.
Namely,updatingone’sbeliefsfrompriortoposteriorbeliefsis
like inverting the likelihood mapping — that is, moving from
p(o|s) top(s|o). In other words, starting with a mapping from
causesto consequencesandthenusingittoinferthecauses from
consequences.
Importantly, models can also include multiple types/sets of
states(i.e.,differentstate-spaces).Forexample,onesetofstates
couldencodepossibleshapes,whileanothersetofstatescould
encodepossibleobjectlocations.Whendifferentsetsofstatesare
independentinthisway,eachsetiscalledadifferent‘ hidden
statefactor’.Similarly,modelscanincludemultipletypes/sets
ofobservableoutcomes.Forexample,onesetofpossibleobser-
vations could come from vision, while another set of possible
3
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.1. SimpleexampleofperceptionasBayesianinference(basedonRamachandran,1988).Pleasenotethat,whilewehavenotexplicitlyconditionedonamodel( m)
intheexpressionofBayes’ruleshownintheleftpanel(asinthetext),thisshouldbeunderstoodasimplicit(i.e.,priorsandlikelihoodsarealwaysmodel-dependent).
Inthisexample,wetakea‘brain’seyeview’andimaginethatwearepresentedwiththeshadedgraydisk(the‘observation’)intheleftpanelofthefigure.Due
totheshadingpattern,thecentralportionofthediskistypicallyperceivedasconcave(asshowninthebottom-left),butitcanalsobeperceivedasconvex(and
typicallyisperceivedasconvexifrotated180 o).Thisisbecausethebrainisequippedwithastrong(unconscious)beliefthatlightsourcestypicallycomefromabove.
Giventhisassumption,theapparentshadowintheupperportionofthediskismuchmorelikelytoarisefromaconcavesurface.Tocapturethismathematically,
ontherightweconsider‘concave’and‘convex’asthetwopossiblehiddenstatesor‘causes’ofsensoryinput(i.e.,theshadowonthegraydisk).Wewanttoknow
whethertheshadowpatternonthedisk(observation)iscausedbyaconcaveorconvexsurface.Theoptimalwaytoinferthehiddenstate(concaveorconvex)is
touseBayes’theorem.Forthesakeofthisexample,assumewebelievethechancesofobservingaconcavevs.convexsurfaceingeneralarealmostequal,witha
slightbiastowardexpectingaconvexsurface(e.g.,perhapswehavecomeacrossconvexsurfacesslightlymoreofteninthepast;encodedinthepriordistribution
shownabove).Thelikelihoodisadifferentstory.Theapparentshadowismuchmoreconsistentwithaconcavesurfaceiflightiscomingfromabove(i.e.,encoded
inthelikelihooddistribution).Toinfertheposteriorprobability,wemultiplythelikelihoodandpriorprobabilities,givingusthejointdistribution.Wethensum
the probabilities in the joint distribution, yielding the total probability of the observation across the possible hidden states (i.e., the marginal likelihood). Finally,
wedividethejointdistributionbythemarginallikelihoodtoreachtheposterior.Theposteriortellsusthatthemostprobablehiddenstateisaconcavesurface
(i.e.,correspondingtowhatismostoftenperceived).Thus,eventhoughthetwo-dimensionalgraydiskaloneisequallyconsistentwithaconvexorconcavesurface,
theassumptionthatlightiscomingfromabove(encodedinthelikelihood)mostoftenleadsustoperceiveaconcave3-dimensionalshape.
observations could come from audition. When sets of observ-
able outcomes are independent in this way, each set is called
adifferent‘ outcomemodality’.Onceallsetsofpossiblestates
andobservationsarespecified,thegenerativemodelisdefined
intermsofthe jointdistribution p(o, s)— thatis,theprob-
abilitydistributionoverallpossiblecombinationsofstatesand
observations.Basedonthe productruleinprobabilitytheory(see
AppendixA),thiscanbedecomposedintotheseparatetermsjust
mentioned:
p(o, s)= p(o|s)p(s) (2)
Ifthereisonlyonesetofstatesandobservations,thisjoint
distribution is a 2-dimensional distribution. If there are more
sets, it becomes a higher-dimensional distribution that, while
hardertovisualize(andmoretimeconsumingtocompute),can
betreatedinthesameway.
Acrucialpointtokeepinmindatthispointisthedistinction
between a generative model and thegenerative process(see
Fig. 2). A generative model, as discussed above, is constituted
bybeliefs abouttheworldandcanbeinaccurate(sometimesre-
ferredtoas‘fictive’).Inotherwords,explanationsfor(i.e.,beliefs
about)howobservationsaregenerateddonothavetorepresent
averidicalaccountofhowtheyareactuallygenerated.Indeed,
explanationsforsensorydatawithinmodelsareoftensimpler
thanthetrueprocessesgeneratingthosedata.Incontrast,the
generativeprocessreferstowhatisactuallygoingonoutinthe
world— thatis,itdescribestheveridical‘groundtruth’about
thecausesofsensoryinput.Forexample,amodelmightholdthe
priorbeliefthattheprobabilityofseeingapigeonvs. ahawk
while at a city park is [.9 .1], whereas the true probability in
thegenerativeprocessmayinsteadbe[.7.3].Thisdistinctionis
importantinpracticalusesofmodelingwhenonewantstosim-
ulatebehaviorunderfalsebeliefsandunexpectedobservations
(e.g.,whenmodelingdelusionsorhallucinations).
WhileBayesianinferencerepresentstheoptimalwaytoinfer
posterior beliefs within a generative model, Bayes theorem is
computationallyintractableforanythingbutthesimplestdistri-
butions.Thisisbecauseevaluating p(o|m)–themarginallike-
lihood (denominator) in Bayes’ theorem – requires us to sum
theprobabilitiesofobservationsunderallpossiblestatesinthe
generativemodel(i.e.,basedonthe sumrule ofprobability;see
AppendixAandFig.1).Fordiscretedistributions,asthenumber
of dimensions (and possible values) increases, the number of
termsthatmustbesummedincreasesexponentially.Inthecase
ofcontinuousdistributions,itrequirestheevaluationofintegrals
thatdonotalwayshaveclosed-form(analytic)solutions.Assuch,
approximationtechniquesarerequiredtosolvethisproblem.This
iswhere VFE iscrucial,asitprovidesacomputationallytractable
quantitythatallowsforapproximateinference.Onecommonway
this is explained requires the introduction of an information-
theoreticquantityknownas self-informationorsurprisal(often
alsojustcalled‘surprise’,butweavoidthistermheretominimize
confusion with the distinct concept of psychological surprise).
Surprisal reflects a deviation between observed outcomes and
thosepredictedbyamodel.Itistypicallywrittenastheneg-
ativelog-probabilityofthatobservation, − lnp (o|m),whereln
is the natural logarithm. Consistent with the intuitive notion
of surprise, lower probability events generate higher surprisal
values(e.g., − ln(.5) = 0.69, while− ln(.9)= 0.1).Ittherefore
followsthatminimizingsurprisalisequivalenttomaximizingthe
evidenceanobservationprovidesforamodel;i.e., p(o|m).Aswill
bedemonstratedinthenextsection, VFE isalwaysgreaterthan
orequaltosurprisal,whichmeansthatminimizing VFE isalsoa
waytomaximizemodelevidence.Thisgetsaroundtheproblem
ofcomputationalintractabilitymentionedaboveandallowsfor
inferenceofposteriorbeliefsoverstates.Fig.3providesanex-
ampleofinferenceusingBayes’theoremandusingminimization
4
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.2. Visualdepictionofthedistinctionbetweenthegenerativeprocessandthegenerativemodel,aswellastheirimplicitcouplingintheperception–actioncycle.
Thegenerativeprocessdescribesthetruecausesoftheobservationsthatarereceivedbythegenerativemodel,whichinformposteriorbeliefsaboutthosecauses
(i.e.,perception).Inactiveinference,agenerativemodelalsoincludespolicies( π),whereeachpolicyisapossiblesequenceofactions( u)thatcouldbeselected.
Thepolicywiththehighestposteriorprobability(givenpreferredoutcomes)istypicallychosen,whichcouplestheagentbacktothegenerativeprocessbychanging
thetruestateofworldthroughaction.
Fig.3. SimpleexampleofexactversusapproximateBayesianinference.AswiththeexampleinFig.1,wearegivenapriorbeliefoverstates p (s) andthelikelihood
ofanewobservation p (o|s),andwewishtoinfertheposteriorprobabilityoverstatesgiventhatnewobservation p (s|o).Exactinferencerequirestheevaluation
ofthemarginallikelihood p (o),which,foranythingbutthesimplestdistributions,iseithercomputationallyintensiveorintractable.Instead,variationalinference
minimizesVFE (heredenotedby F),whichscoresthedifferencebetweenan(initiallyarbitrary) approximateposteriordistribution q(s)andatargetdistribution
(heretheexactposterior;foranintroductiontovariationalinference,seeAppendixA).Byiterativelyupdatingtheapproximateposteriortominimize F (usually
viagradientdescent,seemaintextfordetails),adistributioncanbefoundthatapproximatestheexactposterior.Thatis, q(s)willapproximatethetrueposterior
whenitproducesaminimumvaluefor F.Here,wehaveshownanexampleofiterativeupdatingforthesimplestdistributionpossibletoillustratetheconcept.As
showninthebottom-left,inthisexamplewestarttheagentwithanapproximateposteriordistribution q (s) = p (s) = [.5.5]T — whichcanbethoughtofasan
initialguessaboutwhatthetrueposteriorbelief p(s|o)shouldbeaftermakinganewobservation( o).Wethendefineagenerativemodelwiththejointprobability,
p (o, s),wherethetrueposteriorwewishtofindis p (s|o) = [.8.2]T fortheobservation o (ascalculatedusingexactinferenceinthetoppanel).Onthebottom-right,
under‘Initial F’,wefirstsolvefor F usingourinitial q(s).Under‘Update1’,wethenfind(bysearchingneighboringvalues)anearbyvaluefor q(s)thatleadstoa
lowervaluefor F,andwerepeatthisprocessin‘Update2’.In‘Update3’, q (s) isequalto p(s|o),whichcorrespondstofindingaminimumvaluefor F (i.e.,where
theremainingvalueabovezerocorrespondstosurprisal, − lnp(o)).Thefactthat F hasreachedaminimumcanbeseenin‘Update4’,wherecontinuingtochange
q(s)causes F toagainincreaseinvalue.Thus,byfindingtheposterior q (s) overstatesthatgeneratestheminimumfor F,that q(s)willalsobestapproximatethe
trueposterior.Inthe supplementarycode,wehaveincludedascript VFE_calculation_example.mthatwillallowyoutodefineyourownpriors,likelihoods,and
observationsandcalculate F fordifferent q (s) values.
5
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
ofVFE (seethenextsubsectionforanintroductiontotherelevant
mathematics).
AlthoughBayesianinferenceisusedtomodelperceptionand
learninginrelatedframeworks(e.g.,predictivecoding;see(Bo-
gacz,2017),theactiveinferenceapproachcoveredinthistutorial
extendsthisapplicationofBayesianinferenceintwoways.First,
itmodelscategoricalinference(e.g.,thepresenceofacatvs.a
dog),asopposedtocontinuousinference(i.e.,variablesthattake
acontinuousrangeofvalues,suchasspeed,directionofmotion,
brightness,etc.).Second,itmodelstheinferenceofoptimalaction
sequences during decision-making (i.e., inferring a probability
distributionoverpossibleactionoptions,whichcanbethought
ofasencodingtheestimatedprobabilityofachievingone’sgoals
ifeachactionwerechosen).Inplanning,possiblesequencesof
actions(called‘ policies’)3aredenotedbytheGreekletterpi( π),
sothegenerativemodelisextendedto:
p(o, s, π)= p(o|s, π)p(s|π)p(π) (3)
Wewillreturntotheprioroverpolicies p(π)later.Fornow,
wesimplynotethatactiveinferencemodelscanincludeaddi-
tionalelementsthatcontrol(forexample)howmuchrandomness
ispresentindecision-makingandhowhabitscanbeacquired
and influence decisions. They can also be extended to include
learning.Wewillreturntotheseextensionsinlatersections.
To make decisions, an agent requires a means of assigning
higher value to one policy over another. This in turn requires
that some observations are preferred over others. One of the
more(superficially)counterintuitiveaspectsofactiveinference
is the way it formalizes preferences. This is because there are
noadditionalvariableslabeledas‘rewards’or‘values’.Instead,
preferencesareencodedwithinaspecifictypeofpriorprobability
distribution— whichisoftencalleda‘ priorpreferencedistribu-
tion’.Thisdistributionisoftensimplydenotedas p(o);however,
theterm p(o)isalsousedinotherways,whichcanbeasourceof
confusion.Therefore,wewillinsteadrepresentthisdistribution
asp(o|C),wherethevariable C denotestheagent’spreferences
(Parr,Pezzulo,&Friston,2022).Inthisdistribution,observations
withhigherprobabilitiesaretreatedasmorerewarding.Notethat
thisisdistinctfrompriorsoverstates, p(s),whichencodebeliefs
aboutthetruestatesoftheworld(i.e.,irrespectiveofwhatis
preferred).
Thevalueofeachpolicyinactiveinferenceisalsospecified
within a probability distribution, where a higher value corre-
spondstoahigherprobabilityofbeingselected.Thisprobability
isbasedon EFE (i.e.,lower EFE indicateshighervalue)andreflects
beliefs about how likely each policy is to generate preferred
observations (and how effective it is expected to be at maxi-
mizinginformationgain;discussedfurtherbelow).Inonesense,
the use of probability distributions to encode preferences and
policyvaluescansimplybeconsideredakindofmathematical
‘trick’tobringallelementsofactionselectionwithinthedomain
ofBayesianbeliefupdating—akindof planningasinference
(Attias, 2003; Botvinick & Toussaint, 2012; Kaplan & Friston,
2018).However,manyarticleshaveconsideredthepossibility(or
use language suggesting) that the formalism may have deeper
implications.Specifically,theactiveinferenceliteratureoftendis-
cusses how prior preferences may be thought of as encoding
the observations that are implicitly ‘expected’ by an organism
3 It is important to note that the term ‘policy’ in active inference is used
inadifferentwaythaninmodel-freereinforcementlearning.Asstatedinthe
maintext,apolicyinactiveinferencereferstoanallowablesequenceofactions
(e.g.,aplantomovetostate1,thentostate2,thentostate3).Incontrast,a
policyinmodel-freereinforcementlearningtypicallyconsistsofamappingfrom
statestoactions.Thatis,apolicyspecifiestheactionthatshouldbechosenfor
eachpossiblestateanagentmightoccupy(e.g.,ifinstate1,movetostate2;
ifinstate3,movetostate1,etc.).
in virtue of its phenotype (i.e., the observations an organism
mustseekouttomaintainitssurvivaland/orreproduction).For
example,considerbodytemperature.Humanscanonlysurviveif
bodytemperaturescontinuetobeobservedwithintherangeof
36.5–37.5degreesCelsius.Thus,thehumanphenotypeimplicitly
entailsahighpriorprobabilityofmakingsuchobservations.If
ahumanperceives(i.e.,infers)thattheirbodytemperaturehas
(orisgoingto)deviatefrom‘expected’temperatures,theywill
inferwhichpoliciesaremostlikelytominimizethisdeviation
(e.g., seeking shelter when they are cold or expect to become
cold).Inthissense,bodytemperatureswithinsurvivableranges
(forthehumanphenotype)aretheleast‘surprising’.Informal
terms,thevariable C inpriorpreferences p(o|C)canthereforebe
thoughtofasstandinginforamodelofanorganism’sphenotype,
where this model predicts specific (internal and external) ob-
servationsconsistentwiththatphenotypeandmotivatesactions
expectedtomaintainthoseobservations.
However,itisimportanttohighlightthatthisformaltreat-
mentofpreferencesandvaluesas‘Bayesianbeliefs’(i.e.,prob-
abilitydistributions)neednotbeunderstoodasapsychological
description,normustitbeifonewishestouseactiveinference
modelsinpractice.Inotherwords,notallbeliefsatthemath-
ematicallevelofdescriptionneedtobeequatedwithbeliefsat
thepsychologicallevel;someBayesianbeliefsintheformalism
caninsteadcorrespondtorewardingordesiredoutcomesatthe
psychologicallevel(Smith,Ramstead,&Kiefer,2022).Similarly,
thenotionof‘surprise’withrespecttopriorpreferencesisnot
equivalenttotheconsciousexperienceofsurprise;minimizing
thetypeof‘phenotypicsurprise’discussedinactiveinferenceis
bettermappedontopsychologicalstatesassociatedwithachiev-
ingone’sgoals.Traditionalbeliefs(inthepsychologicalsense)can
insteadbeidentifiedwithothermodelelements,suchaspriors
overstates, p(s),andobservationsexpectedgivenpolicies, p(o|π).
However,regardlessofthewayoneviewsthemappingbetween
mathematicalandpsychologicallevelsofdescription,activein-
ferencecanmorebroadly(andlesscontroversially)beseenas
suggestingthatthebrainjust is (orthatit‘implements’or‘en-
tails’)agenerativemodelofthebodyandexternalenvironment
oftheorganism.
1.2. Non-technical introduction to solving partially observable
Markov decision processes via free energy minimization
Inthissubsection,weexpandontheneedforapproximate
inference in cases where Bayes’ theorem cannot be computed
directlyandexplainhowthismotivatestheuseof VFE andEFE.
By the end of this subsection, the reader should have a basic
understandingofhow VFE canbeusedtoperformapproximate
Bayesian inference within a generative model and of howEFE
extendsthisapproachtoinferoptimalchoices.
The specific type of generative model used here is apar-
tiallyobservableMarkovdecisionprocess (POMDP).AMarkov
decision process describes beliefs about abstract states of the
world, how they are expected to change over time, and how
actionsareselectedtoseekoutpreferredoutcomesorrewards
basedonbeliefsaboutstates.Thisclassofmodelsassumesthe
‘Markov property’, which simply means that beliefs about the
currentstateoftheworldareallthatmatterforanagentwhen
deciding which actions to take (i.e., that all knowledge about
past states is implicitly ‘packed into’ beliefs about the current
state).Theagentthenusesitsmodel,combinedwithbeliefsabout
thecurrentstate,toselectactionsbymakingpredictionsabout
possiblefuturestates.Tobe‘partiallyobservable’meansthatthe
agentcanbeuncertaininitsbeliefsaboutthestateoftheworldit
isin.Inthiscase,statesarereferredtoas‘hidden’(asintroduced
above).Theagentmustinferhowlikelyitistobeinonehidden
6
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
stateoranotherbasedonobservations(i.e.,sensoryinput)and
usethisinformationtoselectactions.
In active inference, these tasks are solved using a form of
approximateinferenceknownas variationalinference (Attias,
2000;Beal,2003;Markovic,Stojic,Schwoebel,&Kiebel,2021)
forabriefintroduction,seeAppendixA).Broadlyspeaking,the
idea behind variational inference is to convert the intractable
sum or integral required to perform model inversion into an
optimization problem that can be solved in a computationally
efficientmanner.Thisisaccomplishedbyintroducingan approxi-
mateposteriordistribution overstates,denoted q(s),thatmakes
simplifyingassumptionsaboutthenatureofthetrueposterior
distribution.Forexample,itiscommontoassumethathidden
statesundertheapproximatedistributiondonotinteract(i.e.,are
independent),whichgivestheapproximatedistributionamuch
simplermathematicalform.Suchassumptionsareoftenviolated,
buttheapproximationisusuallygoodenoughinpractice.Note
that,despiteitsaimofapproximatingthetrueposterior p (s|o),
theapproximateposterior q(s)istypicallynotwrittenasbeing
conditionedonobservations.Thisisbecauseitdoesnotdirectly
dependonobservations— itissimplyan(initiallyarbitrary)dis-
tributionoverstatesthatisiterativelyupdatedtomatchthetrue
posteriordistributionascloselyaspossible(describedbelow).
Afterintroducing q(s),thenextstepinvariationalinference
is to measure the similarity between this distribution and the
generativemodel, p (o, s),usingameasurecalledthe Kullback–
Leibler(KL)divergence .WewilldiscusstheKLdivergencein
more detail in the following (technical) section. For now, it is
sufficient to think of the KL divergence as a measure of the
dissimilaritybetweentwodistributions.Itiszerowhenthedis-
tributionsmatch,anditgetslargerthemoredissimilarthedis-
tributionsbecome. VFE correspondstothesurprisalwewantto
minimizeplustheKLdivergencebetweentheapproximateand
trueposteriordistributions.Invariationalinference,wesystem-
aticallyupdate q (s) untilwefindthevaluethatminimizes VFE,
atwhichpoint q (s) willapproximatethetrueposterior, p (s|o).
InFig.3weprovideasimpleexampleofcalculating VFE under
differentvaluesfor q (s) toprovidethereaderwithanintuition
for how this works (this example can also be reproduced and
customizedinthe VFE_calculation_example.mscriptprovided
inthe supplementarycode).Foreaseofcalculation,thisfigure
usesthefollowingexpressionfor VFE (notethat VFE isdenoted
byF whenpresentedinequations):
F =
∑
s∈S
q (s) ln q (s)
p (o, s)
(4)
However,thisexpressiondoesnotmakeitobvioushowmin-
imizingVFE willlead q (s) toapproximatethetrueposterior.As
discussedfurtherinthetechnicalsections,thiscanbeseenmore
clearlybyalgebraicallymanipulating VFE intothefollowingform,
whichismoreoftenseenintheactiveinferenceliterature:
F = Eq(s)[ln q (s)
p (s|o)] −lnp (o) (5)
Fordetailsonhowwemovebetweendifferentexpressions
forF,seetheoptionaltechnicalsection(Section1.3).Herethe
Eq(s) term indicates theexpected valueor ‘expectation’ of a
distributionandisequivalenttothe
∑
s∈S q (s) terminEq.(4).It
indicatesthatq (s) [ln q(s)
p(s|o) ] isevaluatedforeachvalueof q (s) and
thentheresultingvaluesaresummed(seenumericalexample
inFig.3).Basedonthisformoftheequation,wecanseethat,
because lnp(o) does not depend onq (s), the value ofF will
becomesmallerasthevalueof q (s) approachesthevalueofthe
trueposterior, p (s|o) — sincetheformerisdividedbythelatter
andthelogofoneiszero.
Withintheactiveinferenceframework,thetaskofbothper-
ceptionandlearningistominimize VFE inordertofind(approx-
imately) optimal posterior beliefs after each new observation.
Perception corresponds to posterior state inference after each
newobservation,whilelearningcorrespondstomoreslowlyup-
datingthepriorsandlikelihooddistributionsinthemodelover
manyobservations(whichfacilitatesmoreaccuratestateinfer-
ence in the long run). It is important to note, however, that
minimizingVFE isnotsimplyaprocessoffindingthebest-fitting
approximationoneverytrial.Sensoryinputisinherentlynoisy,
andsimplyfindingthebest-fittingposterioroneachtrialwould
lead to fitting noise, which would result in exaggerated and
metabolicallycostlyupdates.Instatistics,thisisknownasoverfit-
ting.Fortunately,VFE minimizationnaturallyavoidsthisproblem.
Putintowords, VFE measuresthecomplexity ofamodelminusthe
accuracy ofthatmodel.Here,theterm‘accuracy’referstohow
wellamodel’sbeliefspredictsensoryinput(i.e.,thegoodnessof
fit),whiletheterm‘complexity’referstohowmuchbeliefsneed
tochangetomaintainhighaccuracywhennewsensoryinputis
received(i.e., VFE remainshigherifbeliefsneedtochangealotto
accountfornewsensoryinput).Perceptionthereforeseekstofind
the most parsimonious (smallest necessary) changes in beliefs
aboutthecausesofsensoryinputthatcanadequatelyexplainthat
input.
Analogously,thetaskofactionselectionandplanningisto
selectpoliciesthatwillbringaboutfutureobservationsthatmin-
imizeVFE.Theproblemis,ofcourse,thatfutureoutcomeshave
notyetbeenobserved.Actionsmustthereforebeselectedsuch
that they minimizeexpected free energy (EFE). Crucially,EFE
scores the expected cost (i.e., a lower value indicates higher
reward)minustheexpectedinformationgainofanaction.This
meansthatdecisionsthatminimize EFE seektobothmaximize
reward andresolve uncertainty. When beliefsabout statesare
veryimpreciseoruncertain,actionswilltendtobeinformation-
seeking. Conversely, selected actions will tend to be reward-
seekingwhenconfidenceinbeliefsaboutstatesishigh(i.e.,when
thereisnomoreuncertaintytoresolveandtheagentisconfident
aboutwhattodotobringaboutpreferredoutcomes).
However,aswewillseelater,ifthemagnitudeofexpected
reward is sufficiently high (i.e., if a preference distribution is
highly precise), actions that minimizeEFE can become ‘risky’
— in that they seek out reward in the absence of sufficient
information(i.e.,rewardvalueoutweighsinformationvalue).In
general,theimperativetominimize EFE isespeciallypowerfulin
accountingforcommonlyobservedbehaviorsinwhich,instead
of seeking immediate reward, organisms first gather informa-
tionandthenmaximizerewardoncetheyareconfidentabout
states of the world (e.g., turning on a light before trying to
findfood).Itcanalsocaptureinterestingbehaviorsthatoccur
in the absence of opportunities for reward, where organisms
appear to act simply out of ‘curiosity’ (Barto, Mirolli, & Bal-
dassarre, 2013; Oudeyer & Kaplan, 2007; Schmidhuber, 2006).
Further, variations in the precision of preferences duringEFE
minimization can capture interesting individual differences in
behavior,asexemplifiedintheexampleof‘risky’behaviorjust
mentioned.Notethatthiscrucialaspectofactiveinferenceeffec-
tivelyaddressesthe‘explore–exploitdilemma’(discussedfurther
below), because the imperatives for exploration (information-
seeking)andexploitation(reward-seeking)arejusttwoaspects
ofexpectedfreeenergy,andwhetherexploratoryorexploitative
behaviors are favored in a given situation depends on current
levelsofuncertaintyandthelevelofexpectedreward.
Asweshallseeinlatersections,theposterioroverpolicies
canbeinformedbyboth VFE andEFE.Fornow,wesimplynote
thatthisisbecause VFE isameasureofthefreeenergyofthe
present(andimplicitlythepast),while EFE isameasureofthe
7
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
freeenergyofthefuture.Thisisimportant,becausewhilesome
policiesmayleadtoaminimizationoffreeenergyinthefuture,
theymaynothaveledtoaminimizationoffreeenergyinthepast
(andarethereforesuboptimalpolicieswhenevaluatedoverall).In
otherwords, EFE scoresthelikelihoodofpursuing(i.e.,thevalue
assignedto)aparticularcourseofactionbaseduponexpected
future outcomes, whileVFE reflects the likelihood of (i.e., the
value assigned to) a course of action based upon past/present
outcomes.Thismeanstheposteriordistributionoverpoliciesisa
functionofboth VFE andEFE,wherethesequantitiesrespectively
furnishretrospectiveandprospectivepolicyevaluations.Atthe
psychological level, one can intuitively think ofVFE as asking
‘howgoodhasthisactionplanturnedoutsofar?’,while EFE asks
‘howgooddoIexpectthingstogoifIcontinuetofollowthis
actionplan?’.
Whenviewedfromtheperspectiveofpotentialneuroscientific
applications,anothercrucialbenefitof VFE andEFE isthatthey
can be computed in a biologically plausible manner. This has
inspiredneuralprocesstheories thatspecifywaysinwhichsets
ofneuron-likenodes(e.g.,neuronalpopulations),withparticular
patterns of synapse-like connection strengths, can implement
perception,learning,anddecision-makingthroughtheminimiza-
tionofthesequantities.Theseneuralprocesstheoriespostulate
severalneuronalpopulationswhoseactivityrepresents:
(1)Categorical probability distributions(a special case of
the multinomial distribution) over the possible states of
the world. For those without background in probability
theory,thesedistributionsassignoneprobabilityvalueto
eachpossibleinterpretationofsensoryinput,andallthese
probabilityvaluesmustadduptoavalueof1.Inother
words, this assumes the world must be in this state or
thatstate,butnotbothatthesametime,andassignsone
probabilityvaluetotheworldbeingineachpossiblestate.
(Note:inotherpapersyoumaycomeacrossthenotation
Cat(x), which simply indicates that a distributionx is a
categoricaldistribution).
(2)Prediction-errors,whichsignalthedegreetowhichsen-
soryinputisinconsistentwithcurrentbeliefs.Prediction
errors drive the system to find new beliefs – that is,
adjustedprobabilitydistributions–sothattheyaremore
consistent with sensory input, and therefore minimize
theseerrorsignals.
(3) Categoricalprobabilitydistributionsoverpossible policies
theagentmightchoose.
These neural process theories also include simple,
coincidence-based learning mechanismsthat can be under-
stoodintermsofHebbiansynapticplasticity(i.e.,whichinvolve
adjusting the strength of the connections between two neu-
ronswhenbothneuronsareactivatedsimultaneously;(Brown,
Zhao,&Leung,2010).Theyfurtherincorporate messagepass-
ingalgorithms (discussedindetailbelow)thatcanmodelthe
connectivity and firing rate patterns of neuronal populations
organized into cortical columns. Such theories afford precise
quantitativepredictionsthatcanbetestedusingneuroimaging
andotherelectrophysiologicalmeasuresofneuronalactivation
duringspecificexperimentalparadigms.
1.3. Technical introduction to variational free energy and expected
free energy (optional)
Intheprevioussubsectionsweintroducedtwoquantities, VFE
andEFE, which were described in largely qualitative terms. In
thissubsection,weconsidertheformaldetailsbehindtheabove
descriptions. By the end of this subsection, the reader should
haveaworkingunderstandingofthedifferentwaysthat VFE and
EFE areoftenexpressedintheliterature,howtheseexpressions
are derived, and the theoretical insights that each expression
provides.Readerswithoutstrongmathematicalbackgroundcan
safelyskipmuchofthissection(ifsodesired)andmoveonto
thenextsectionwithoutsignificantlossofunderstanding.That
is,theyshouldstillbeabletofollowtherestofthepaper.For
those who choose to read this section, we provide accessible
explanationsofeachequationinthehopethatasmanypeople
aspossiblewillbeabletofollowandlearnfromthematerial.
Beforeformallydefining VFE andEFE,itwillbehelpfultofirst
familiarizethereaderwiththerelevantmathematicalmachinery.
Wewillfirstexpandonthe KLdivergence (DKL;alsosometimes
called relative entropy), which was briefly introduced in the
previoussubsection.Toremindthereader,thisisameasureof
thesimilarity,ordissimilarity,betweentwodistributions.TheKL
divergencebetweentwodistributions, q(x)and p(x),iswrittenas
follows:
DKL[q(x)∥ p(x)] =
∑
x∈X
q(x)ln q(x)
p(x)
(6)
ThisequationstatesthattheKLdivergenceisfoundbytaking
each value ofx in rangeX (for whichp andq assign values),
calculatingthevalueoftheright-handquantity,andthensum-
mingtheresultingvalues(foraconcretenumericalexample,see
thecalculationof F inFig.3).Fromtheperspectiveofinformation
theory,theKLdivergencecanbethoughtofasscoringtheamount
of information one would need to reconstructp(x) given full
knowledgeof q(x).Inthiscontext,‘information’ismeasuredin
aquantitycalled nats becauseitdependsonthenaturallog,as
opposedtologbase2whereinformationwouldbequantifiedin
bits.
Asmentionedabove,becausecalculatingmodelevidenceis
generallynotpossible,weinsteadminimize VFE,whichiscon-
structedtobeanupperboundonnegative(log)modelevidence.
Aswenotedabove,thisisalsocalled‘surprisal’ininformation
theory:− lnp (o).Inotherwords,byminimizing VFE,onecan
minimizethenegativemodelevidence— or,moreintuitively,
onecanmaximizemodelevidence(i.e.,byfindingbeliefsina
modelforwhichobservationsprovidethemostevidence).
Beforeturningtotheformaldefinitionof VFE,itwillbehelpful
toclarifysomenotationalconventionsandconcepts.Specifically,
usingthesumruleofprobabilitywecanexpressmodelevidence
in terms of our generative model as:p (o) = ∑
s,π p (o, s, π).
Thatis, p (o) isthesumoftheprobabilitiesofobservationsfor
everycombinationofstatesandpoliciesinthemodel.Next,one
canmultiplyanddividethejointdistribution, p (o, s, π),bythe
(initiallyarbitrary)approximatedistribution q(s, π).Bydefinition,
multiplying and then dividing byq(s, π) does not change the
value of this distribution. However, this trick ends up being
quiteusefulaswewillsee.Formathematicalconvenience,one
can take the negative logarithm of the resulting term, leading
to:
− lnp (o) = −ln
∑
s,π
p (o, s, π) q (s, π)
q (s, π) = −lnEq(s,π)
[
p (o, s, π)
q (s, π)
]
(7)
As briefly mentioned in the previous subsection, Eq(s,π) de-
notestheexpectedvalueorexpectationofadistribution.This
canbethoughtasakindofweightedaverage,whereeachvalue
ofonedistribution(here q (s, π))ismultipliedbytheassociated
value in another distribution (here
[
p(o,s,π)
q(s,π)
]
), and each of the
resultingvaluesissummedtogettheexpectedvalueofthelatter
distribution. Note that, although written as a summation, the
calculationof F inFig.3representsanumericalexample.Readers
shouldnotetheformalsimilaritybetweentheKLdivergenceand
8
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
theexpectationvalue.Thisisnoaccident,astheKLdivergence
issimplytheexpecteddifferenceofthelogoftwodistributions,
wheretheexpectationistakenwithrespecttothedistributionin
thenumerator.Notethat,becauseln x
y = −lny
x ,thedistributions
inthenumeratorareoftenswappedaround,aswehavedone
below.Intheactiveinferenceliterature,itisverycommonto
movebetweenKLdivergencenotationandexpectationnotation.
As such, it can be useful to keep the following identities in
mind:
DKL[q(x)∥ p(x)] =Eq(x)
[
lnq (x)
p (x)
]
=
∑
x∈X
q (x) lnq(x)
p(x)
= −
∑
x∈X
q (x) lnp(x)
q(x)
(8)
WithaclearideaofexpectationvaluesandtheKLdivergence
nowinmind,wemoveontothedefinitionof VFE.Specifically,
leveraging the mathematical result referred to asJensen’s in-
equality(Kuczma&Gilányi,2009)–whichstatesthattheexpec-
tationofalogarithmisalwayslessthanorequaltothelogarithm
ofanexpectation–wearriveatthefollowinginequality:
− lnp (o) = −lnEq(s,π)
[
p (o, s, π)
q (s, π)
]
≤ −Eq(s,π)
[
lnp (o, s, π)
q (s, π)
]
= F (9)
Ontheright-handsideofthisequationis VFE,whichisdefined
intermsoftheKLdivergence–thatis,expecteddifferenceof
the respective logs – between the generative modelp (o, s, π)
andtheapproximateposteriordistribution q (s, π).Whentheap-
proximateposteriordistributionandthegenerativemodelmatch,
VFE is equal to zero (i.e., whenq = p, ln
(
p(o, s,π)
q(s,π)
)
= 0). To
understandtheequalityontheleft-handside,considerthatthe
expectation,− lnEq(s,π)
[
p(o,s,π)
q(s,π)
]
,entailssummingoverallvalues
ofs andπ;thatis, − ln
∑
s,π q (s, π)
[
p(o,s,π)
q(s,π)
]
. Thisremoves s
andπ from the expression in both the denominator and the
numerator, leaving− lnp (o). From this, we can see that, by
minimizingVFE,weminimizeanupperboundonnegativelog
modelevidence(here,withrespecttostatesundereachpolicy).
This means thatVFE will always be greater than or equal to
− lnp (o),whichentailsthatbyminimizingthevalueof VFE,the
model evidencep (o) will either increase or remain the same
(i.e.,itwillbemaximized,asthelogarithmisamonotonically
increasingfunction).
Therefore,allthatisneededtoperformapproximateBayesian
inference in perception, learning, and decision-making is a
tractableapproachtofindingthevalueof s (i.e.,theapproximate
posterior distribution overs) that minimizesVFE. This can be
accomplishedbyperforminga gradientdescent onVFE.Gradient
descentisatechniquethatstartsbypickingsomeinitialvaluefor
s andthencalculates VFE forthisvalue.Itthencalculates VFE for
neighboringvaluesof s andidentifiestheneighboringvaluesfor
whichVFE decreasesmost.Itthensamplesfromvaluesof s that
neighborthosevaluesandcontinuestodosoiterativelyuntila
minimumVFE valueisfound(i.e.,where VFE nolongerdecreases
foranyneighboringvalues).Atthispoint,anapproximationto
the optimal beliefs fors has been found (given some set of
observationso).Onafinalterminologicalnote,because VFE is
afunctionthatisdefinedintermsofprobabilitydistributions,
which are themselves functions,VFE is sometimes referred to
asa‘ functional’,whichissimplythemathematicaltermfora
functionofafunction.
Notethatinactiveinferencewecalculate VFE withrespectto
eachavailablepolicyindividually(denotedby Fπ ).Thisisbecause
differentpolicies,throughtheirimpactonhiddenstatesinthe
generativeprocess,makecertainobservationsmorelikelythan
others.Forexample,considerasituationwhereIbelievethereis
achairtomyleftandatabletomyright.Conditionalonhaving
chosentolookleft,itismorelikelythatIwillobserveachair
thanatable.Thismeansthatobservingthechairactsasevidence
thatIhavechosenthepolicyoflookingleft.Becauseobservations
provideevidenceforpoliciesinthisway,boththeapproximate
posteriorq (s|π),andthegenerativemodel p (o, s|π),arecondi-
tionedonpolicies.Thismaybeusefulinsomecaseswhere,for
example,oneisconsideringthepossibilitythatanagentcould
havefalsebeliefsabouttheactionstheyarecarryingoutorcould
besurprisedwhentheirintendedpolicydoesnotmatchthetrue
observedactions.Goingforward, VFE willbepresentedinterms
ofFπ (asshowninthefollowingparagraph).
Aswewilldiscussbelow,onewayinwhichthebrainmay
accomplishgradientdescenton VFE duringperceptionisthrough
theminimizationofpredictionerror.Thereasonforthiscanbe
broughtoutbydoingsomealgebraicrearrangementtoexpress
VFE asameasureof complexityminusaccuracy(i.e.,astouched
uponinformallyintheprevioussubsection):
Fπ = Eq(s|π)
[
ln q (s|π)
p (o, s|π)
]
(10)
= Eq(s|π)[lnq (s|π) − lnp (o, s|π)] (L2)
= Eq(s|π)[lnq (s|π) − lnp (s|π)] −Eq(s|π)[lnp (o|s, π)] (L3)
= DKL[q (s|π) ∥ p (s|π)] −Eq(s|π) [lnp (o|s)] (L4)
The first line expressesVFE in terms of the expected log
difference (KL divergence) between the approximate posterior
andthegenerativemodel.Inthesecondlineweuselogalgebra
toexpressthedivisionasasubtraction(ln x
y = lnx − lny).In
thethirdlineweusetheproductruleofprobability( p (o, s|π) =
p (s|π) p (o|s, π) totakethelikelihoodtermoutofthefirstex-
pectationterm.Thefourthlinere-expressesthethirdline,but
uses more compact notation for the first term and drops the
dependency on policies in the second term (i.e., because we
assume here that the likelihood mapping does not depend on
policies).Thefirstterminline4istheKLdivergencebetween
priorandposteriorbeliefs.Thisvaluewillbelargerifoneneeds
tomakelargerrevisionstoone’sbeliefs,whichisthemeasure
of complexity introduced earlier. A greater complexity means
thereisagreaterchanceofchangingbeliefstoexplainrandom
aspectsofone’sobservations,whichcanreducethefuturepre-
dictivepowerofamodel(analogousto‘overfitting’instatistics).
Thesecondterminline4reflectspredictiveaccuracy(i.e.,the
probabilityofobservationsgivenmodelbeliefsaboutstates).The
brainwillthereforeminimize VFE ifitminimizespredictioner-
ror(maximizingaccuracy)whilenotchangingbeliefsmorethan
necessary(minimizingcomplexity).
Another common way thatVFE is expressed is in terms of
placingaboundonsurprisal:
Fπ = Eq(s|π)[lnq (s|π) − lnp (s|o, π)] −lnp (o|π) (11)
Thisequationrearrangesline1inEq.(10)(againusingthe
productrule: p (o, s|π) = p(s|o, π)p(o|π))toshowthat VFE isal-
waysgreaterthanorequaltosurprisal(i.e.,isanupperboundon
surprisal)withrespecttoapolicy(i.e.,greaterthan − lnp (o|π)).
Inmachinelearning,thesignof VFE isusuallyswitched,sothat
it becomes an evidence lower bound, also known as an ELBO.
MaximizingtheELBOisacommonlyusedoptimizationapproach
inmachinelearning(Winn&Bishop,2005).
9
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
From a slightly different perspective, the gradients ofVFE
leveraged in active inference can also be expressed as a mix-
ture of prediction errors (i.e., when thought of more broadly
as differences to be minimized). This is because complexity is
theaveragedifferencebetweenposteriorandpriorbeliefs,while
accuracyisthedifferencebetweenpredictedandobservedout-
comes.Thisthereforealsolicensesadescriptionofactiveinfer-
enceaspredictionerrorminimization(Burr&Jones,2016;Clark,
2017;Fabry,2017;Hohwy,Paton,&Palmer,2016),corresponding
to the minimization of these two differences (minimizingVFE
throughpredictionerrorminimizationisdescribedinmoredetail
inSections2.4and5).
However,activeinferenceisnotsolelyconcernedwithmin-
imizingpredictionerrorinperception.Itisalsoamodelofac-
tionselection.Wheninferringoptimalactions,onecannotsim-
plyconsidercurrentobservations,becauseactionsarechosento
bring about preferred future observations. As described infor-
mallyabove,thismeansthat,toinferoptimalactions,amodel
mustpredictsequencesof future statesandobservationsforeach
possiblepolicy,andthencalculatetheexpectedfreeenergy( EFE)
associated with those different sequences of future states and
observations. As a model of decision-making,EFE also needs
to be calculated relative to preferences for some sequences of
observationsoverothers(i.e.,howrewardingorpunishingthey
will be). In active inference, this is formally accomplished by
equipping a model with prior expectations over observations,
p(o|C),thatplaytheroleofpreferences. 4 Foraninitialintuition
ofhowthisworks,considertwopossiblepoliciesthatcorrespond
totwodifferentsequencesofstatesandobservations,whereone
sequenceofobservationsispreferredmorethantheother.Since
‘preferred’hereformallytranslatesto‘expectedbythemodel’,
thenthepolicyexpectedtoproducepreferredobservationswill
betheonethatmaximizestheaccuracyofthemodel(andhence
minimizesEFE).Thismeansthattheprobability(orvalue)ofeach
policy can be inferred based on how much expected observa-
tionsunderapolicywillmaximizemodelaccuracy(i.e.,match
preferredobservations).Whenpreferredobservationsaretreated
as implicit expectations definitive of an organism’s phenotype
(e.g.,thoseconsistentwithitssurvival,suchasseekingwarmth
whencold,orwaterwhenthirsty)thishasalsobeendescribedas
‘self-evidencing’(Hohwy,2016).
Toscoreeachpossiblepolicyinthisway, EFE (denotedGπ in
equations)canbeexpressedasfollows:
Gπ = Eq(o,s|π)[lnq (s|π) − lnp (o, s|π)] (12)
= Eq(o,s|π)[lnq (s|π) − lnp (s|o, π)] −Eq(o|π)[lnp (o|π)] (L2)
≈ Eq(o,s|π)[lnq (s|π) − lnq (s|o, π)] −Eq(o|π)[lnp (o|C)] (L3)
= −Eq(o,s|π)[lnq (s|o, π) − lnq (s|π)] −Eq(o|π)[lnp (o|C)] (L4)
Thefirstlineexpresses EFE astheexpecteddifferencebetween
theapproximateposteriorandthegenerativemodel.Notethat
because EFE is calculated with respect to expected outcomes
that(bydefinition)havenotyetoccurred,observationsenterthe
expectation operator Eq as random variables (i.e., otherwise it
isidenticalinformtotheexpressionfor VFE inEq.(10)).The
4 Note that in some papers, preferences are formulated over states instead
of observations. In this case, one might wonder how an agent can have two
priors over states at the same time (one for beliefs and one for preferences).
Althoughthetechnicaldetailsarebeyondthescopeofthispaper,inthiscase
onemustthinkmoreexplicitlyintermsofanagenthavingtwomodels—one
oftruestatesoftheworldandoneofpreferredstates(castaspriorsineach
model,respectively).Policyselectionthenattemptstominimizethedivergence
betweenthetwobybringingtruestatestomatchpreferredstates(fordetails,
seeDaCosta,Parretal.,2020).
second line uses the product rule of probability,p (o, s|π) =
p (s|o, π) p(o|π)torearrange EFE intotwotermsthatcanbeas-
sociatedwithinformation-seekingandreward-seeking.Tomake
thisclear,thethirdlinedoestwothings.First,itreplacesthetrue
posterior(lnp (s|o, π))withanapproximateposterior(ln q (s|o, π)).
Second,itdropstheconditionalizationon π inthesecondterm
andinsteadconditionsonthevariable C describedabovethat
encodespreferences(i.e.,E q(o|π)[lnp (o|π)]→ Eq(o|π)[lnp (o|C)]).
Thisisacentralmovewithinactiveinference.Namely, p (o|C) is
usedtoencodepreferredobservations,andtheagentseekstofind
policiesexpectedtoproducethoseobservations.Theagent’spref-
erencescanbeindependentofthepolicybeingfollowed,which
allows us to drop the conditionalization onπ. As mentioned
earlier,inmostpapersonactiveinferencepriorpreferencesare
simplywrittenasE q(o|π)[lnp (o)];however,toclearlydistinguish
thisfromtheln p (o) termwithin VFE (i.e.,where o isanobserved
variable),wewritethetermhereasexplicitlyconditionedon C
(Parretal.,2022).
Thefirsttermontheright-handsideofline3iscommonly
referredtoasthe epistemic value,ortheexpected information gain
ofastatewhenitisconditionedonexpectedobservations.The
secondtermiscommonlyreferredtoas pragmatic value,which,
as just mentioned, scores the agent’s preferences for particu-
larobservations.Tomaketheintuitionbehindepistemicvalue
more apparent, the fourth line flips the terms inside the first
expectation so that it becomes prefixed with a negative sign
(i.e.,p(x)[lnp(x)− lnq(x)] = −p(x)[lnq(x)− lnp(x)]). Because
theepistemicvaluetermissubtractedfromthetotal,itisclear
thattominimize EFE overallanagentmustmaximizethevalue
of this term by selecting policies that take it into states that
maximizethedifferencebetweenpriorandposteriorbeliefs;that
is,maximizethedifferencebetweenln q (s|o, π) andlnq (s|π).In
otherwords,theagentisdriventoseekoutobservationsthatre-
duceuncertaintyabouthiddenstates(Parr&Friston,2017a).For
example,ifanagentwereinadarkroom,themappingbetween
hiddenstatesandobservationswouldbeentirelyambiguous,so
itwouldbedriventomaximizeinformationgainbyturningona
lightbeforeseekingoutpreferredobservations(i.e.,asitwouldbe
unclearhowtobringaboutpreferredoutcomesbeforethelight
wasturnedon).
Anotherverycommonexpressionof EFE intheactiveinfer-
enceliteratureis:
Gπ = DKL [q(o|π)||p(o|C)]+ Eq(s|π) [H[p(o|s)]] (13)
Forafulldescriptionofhowyougetfromline1ofEq.(12)to
thisdecomposition,seeAppendixA.Thefirsttermontheright-
handsideofthisequationscorestheanticipateddifference(KL
divergence) between (1) beliefs about the probability of some
sequenceofoutcomesgivenapolicy,and(2)preferredoutcomes
(i.e., those expected a priori within the model). This term is
sometimesreferredtoas‘risk’(orexpectedcomplexity),butit
canmoreintuitivelybethoughtofasbeliefsabouttheprobability
of reward for each choice one could make. That is, the lower
theexpecteddivergencebetweenpreferredoutcomesandthose
expected under a policy, the higher the chances of attaining
rewardingoutcomesifonechosethatpolicy.Thesecondterm
onetheright-handsideoftheequationistheexpectedvalueof
theentropy(H)ofthelikelihoodfunction,whereH [p (o|s)] =
− ∑
p(o|s)lnp(o|s). Entropy is a measure of the dispersion of
adistribution,whereaflatter(lowerprecision)distributionhas
higherentropy.Ahigher-entropylikelihoodmeansthereareless
precisepredictionsaboutoutcomesgivenbeliefsaboutthepossi-
blestatesoftheworld.Thistermisthereforecommonlyreferred
toasameasureof‘ambiguity’.Policiesthatminimizeambiguity
willtrytooccupystatesthatareexpectedtogeneratethemost
10
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
precise(i.e.,mostinformative)observations,becausethoseob-
servationswillprovidethemostevidenceforonehiddenstate
overothers.Puttingtheriskandambiguitytermstogethermeans
thatminimizing EFE willdriveselectionofpoliciesthatmaximize
bothrewardandinformationgain(forsimplenumericalexam-
plesofcalculatingtheriskandambiguityterms,seediscussion
of‘outcomepredictionerrors’inSection2.4).Typically,seeking
informationwilloccuruntilthemodelisconfidentabouthow
to achieve preferred outcomes, at which point it will choose
reward-seekingactions.Importantly,asbrieflymentionedearlier,
theexpressionfor EFE aboveentailsthatstronger(moreprecise)
preferences for one outcome over others will have the effect
ofdown-weightingthevalueofinformation,leadingtoreduced
information-seeking(andvice-versaifpreferencesaretooweak
orimprecise).Thisaffectshowamodelresolvesthe‘explore–
exploit dilemma’ (Addicott, Pearson, Sweitzer, Barack, & Platt,
2017;Fristonetal.,2017b;Parr&Friston,2017a;Schwartenbeck
et al., 2019; Wilson, Geana, White, Ludvig, & Cohen, 2014) —
that is, the difficult judgement of whether or not one ‘knows
enough’ to trust their beliefs and act on them to seek reward
orwhethertofirstacttogathermoreinformation(seeexample
simulationsbelow).Foramoredetaileddescriptionandstep-by-
stepderivationofthemostcommonformulationsof EFE inthe
activeinferenceliterature,seeAppendixA.
2. BuildingandsolvingPOMDPs
2.1. Formal POMDP structure
Inthisfirstsubsection,weintroducethereadertotheabstract
structureandelementsofanactiveinferencePOMDP,whichis
thestandardmodelingapproachinactiveinferenceresearchat
present. In a POMDP, one is given a specific type of genera-
tivemodel,includingobservations,states,andpolicies,andthe
goalistoinferposteriorbeliefsoverstatesandpolicieswhen
conditioningonobservations.Bytheendofthissubsection,the
reader should be able to identify and interpret each type of
variableinthesemodelsandunderstandtheroletheyplayin
performinginference.Awarning:uponinitialexposure,gaining
afullunderstandingofthisabstractstructurecanfeeldaunting.
However,afterweputtogetheramodelofaconcretebehavioral
task(inSection3),thisstructure–andhowtopracticallyuse
it–tendstobecomemuchclearer.Thetaskwewillmodelis
an‘explore–exploit’tasksimilartocommonlyusedmulti-armed
bandittasksemployedincomputationalpsychiatryresearch(see
Fig.4).Inthisvariant,therearetwoslotmachineswithunknown
probabilitiesofpayingout.Aparticipantcansimplyguess,re-
sultingineitheralargerewardornoreward,ortheycanask
for a hint (which may or may not be accurate). If they get it
rightaftertakingthehint,theyreceiveasmallerreward.This
allowsforcompetitionbetweenaninformation-seekingdriveand
areward-seekingdrive.Thistaskwillbedescribedindetailin
Section3,butwewillusepartsofthisbroad-strokesdescription
belowtoexemplifyusesofthemoreabstractelementsmaking
upPOMDPs.
ThetermPOMDPdenotestwomajorconcepts.Asdescribed
above,thefirstispartialobservability,whichmeansthatobser-
vationsmayonlyprovideprobabilisticinformationabouthidden
states(e.g.,observingahintmayindicatethatoneoranotherslot
machineismorelikelytopayout).ThesecondistheMarkov
property,whichsimplymeansthat,whenmakingdecisions,all
relevant knowledge about distant past states is implicitly in-
cluded within beliefs about the current state. This assumption
canbeviolated,butitallowsmodelingtobemoretractableand
is‘goodenough’inmanycases.Whendealingwithviolationsof
theMarkovianassumption–suchaswhenmodelingmemory–
itisnecessarytomodelseveralinterconnectedMarkovianpro-
cessesthatevolveoverdifferenttimescales.Wewillseerelated
examplesinlatersectionscoveringbothhierarchicalmodelsand
howtheparametersofaPOMDPcanbelearnedthroughrepeated
observations.Herewestartwithasimple,single-levelPOMDP
whereMarkovianassumptionsarenotviolated.Inthepresen-
tationbelow,notethatvectors(i.e.,singlerowsorcolumnsof
numbers)aredenotedwithitalics,whilematrices(i.e.,multiple
rows and columns of numbers) are not italicized and denoted
withbold.
APOMDPincludesbothtrialsandtimepoints(tau; τ)within
eachtrial(sometimescalled‘epochs’).Animportantthingtonote
hereisthat τ indexesthetimepoints about whichagentshave
beliefs. This is distinct from the variablet, which denotes the
timepoints at whicheachnewobservationispresented.Thisis
acommon(andunderstandable)sourceofconfusionforthose
newtotheactiveinferenceliterature(perhapsexacerbatedby
the fact thatt andτ look so similar). To appreciate the need
for this distinction, consider cases in which an observation in
thepresentcanchangeone’sbeliefsaboutthepast.Forexample,
imaginethatyoustartoutinoneoftworooms(agreenroom
orablueroom),butyoudonotknowwhatcolorthewallsare.
Later,whenyouopenyoureyesandfindouttheroomispainted
blue, you will change your beliefnow about where you were
earlier beforeyouopenedyoureyes(i.e.,youhadbeeninthe
blueroomthewholetime).Inaformalmodel,thiswouldbea
caseinwhichbeliefs about one’sstateattime τ = 1changeafter
makinganewobservation at timet = 2.Thus,theinclusionof
botht andτ inactiveinferenceentailsthattheagentupdatesits
beliefsaboutstatesatalltimepoints τ withnewobservationsat
eachtimepoint t.Thisallowsfor retrospective inference,asinthe
previousexample,aswellasfor prospective inference,inwhich
anagentupdatesbeliefsaboutthefuture(e.g., τ = 3),when
makingnewobservationsinthepresent(e.g., t = 2).Thiswould
bethecaseintheexplore–exploittaskexample,whereobserving
ahintatonetimepointcouldupdatebeliefsaboutwhichslot
machinewillbebetteratthenexttimepoint.Thisisanimportant
distinctiontokeepinmindwhentryingtounderstandsimulation
results(e.g.,intermsofworkingmemoryforthepastandfuture;
i.e.,postdictionandprediction).
Inpractice,thisisaccomplishedbyhavingentriesof0forall
elementsofanobservationvectorwhen t < τ.Toillustratethis
formally,wewillusethesimplerexampleofbeinginoneoftwo
roomsdescribedabove.Inthiscase,therewillbea‘color’ob-
servationmodalitywhereobservationscouldbe‘blue’or‘green’
(i.e.,avectorwithoneelementforeachcolor).Attime t = 1,the
observedcolorfortime τ = 2hasnotyetoccurred.So,at t = 1:
oτ=2 =
[
0
0
]
Ifbluewerethenobservedat t = 2,theobservationfor τ = 2
wouldbeupdatedto:
oτ=2 =
[
1
0
]
Thisvectorwouldthenremainunchangedforallfuturetime
pointst > 2(i.e.,theobservationisnever‘forgotten’onceithas
takenplace).Thesamethingwouldthenoccurforallsubsequent
observations(e.g., oτ=3 =
[
0
0
]
att = 1and2;butifgreen
wereobservedat t = 3 thenthevectorwouldbeupdatedto
oτ=3 =
[
0
1
]
andremainthatwayfor t > 3,etc.).Thisallows
beliefsaboutstatesforalltimepointstobeupdatedat eachtime
pointt whentheseobservationvectorsareupdated.
11
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.4. Depictionoftheexplore–exploittaskexample.Notethatthestatesandoutcomesshownontherightareonlyexamples.Table1andSection3listallstates,
outcomes,andpoliciesrequiredtobuildagenerativemodelforthistask.
Havingnowclarifiedtimeindexing,wewillmoveontoother
model elements. At the first time point in a trial(τ = 1), the
model starts out with a prior over categorical states,p (sτ=1),
encodedinavectordenotedby D –onevalueperpossiblestate
(e.g.,whichslotmachineismorelikelytopayout).Whenthere
aremultiplestatefactors,therewillbeone D vectorperfactor.As
touchedonabove,multiplestatefactorsarenecessarytoaccount
formultipletypesofbeliefsonecanholdsimultaneously.One
common example is holding separate beliefs about an object’s
location and its identity. In the explore–exploit task example,
thiscouldincludebeliefsaboutwhichslotmachineisbetterand
beliefsaboutavailablechoicestates(e.g.,thestateofhavingtaken
thehint).
At each subsequent time point, the model has prior beliefs
abouthowonestatewillevolveintoanotherdependingonthe
chosenpolicy, p (sτ+1|sτ , π),encodedina‘transitionmatrix’de-
notedby Bπ,τ — onecolumnperstateat τ andonerowper
stateat τ + 1.Iftransitionsforagivenstatefactorareidentical
acrosspolicies,theycanberepresentedbyasinglematrix.When
transitionsforastatefactorarepolicy-dependent,therewillbe
oneBπ,τ matrixperpossibleaction(i.e.,oneforeachpossible
statetransitionunderapolicy).Inotherwords,thecombination
ofapolicyandatimespecifiesatransitionmatrix(i.e.,encoding
theactionthatwouldbetakenunderthatpolicyatthatpoint
in time; described further below). In the explore–exploit task
example,thiscouldincludetransitioningtothestateassociated
withgettingthehintortransitioningtothestateassociatedwith
selectingoneofthetwomachines(dependingonthepolicy).
Thelikelihoodfunction, p(oτ |sτ ),isencodedinamatrixde-
noted byA — one column per state atτ and one row per
possible observation atτ. When there are multiple outcome
modalities,therewillbeone Amatrixperoutcomemodality.As
toucheduponabove,multipleoutcomemodalitiesarenecessary
to account for parallel channels of sensory input (e.g., one for
possiblevisualinputsandoneforpossibleauditoryinputs).Inthe
explore–exploittaskexample,thiscouldincludeonemodalityfor
observingthehintandanothermodalityforobservingrewardvs.
noreward.
Preferred outcomes, lnp(oτ |C), are specified using a matrix
denotedby C— onecolumnpertimepointandonerowperpos-
sibleobservation.Whentherearemultipleoutcomemodalities,
therewillbeone Cmatrixpermodality.Intheexplore–exploit
taskexample,thiscouldencodeastrongpriorpreferencefora
largereward,amoderatepreferenceforasmallreward,andlow
preferencefornoreward.
Priorbeliefsaboutpolicies p (π) areencodedina(column)
vectorE (onerowperpolicy)— increasingtheprobabilitythat
some policies will be chosen over others (i.e., independent of
observed/expected outcomes). This can be used to model the
influenceofhabits.Forexample,ifanagenthaschosenapar-
ticularpolicymanytimesinthepast,thiscanleadtoastronger
expectationthatthispolicywillbechosenagain.Intheexplore–
exploittaskexample, E couldbeusedtomodelasimplechoice
bias in which a participant is more likely to choose one slot
machineoveranother(independentofpreviousrewardlearning).
However,itisimportanttodistinguishbetweenthistypeofprior
beliefandtheinitialdistributionoverpoliciesfromwhichactions
are sampled before making an observation (π0). As explained
furtherbelow(andinTable2),thislatterdistributiondependson
E,G,and γ ,wheretheinfluencesofhabitsandexpectedfuture
outcomeseachhaveaninfluenceoninitialchoices.
Eachallowableaction( u)isencodedasapossiblestatetran-
sition(oneofseveral Bmatricesthatcanbechosenforastate
factor).Inthiscase,eachpossibleactionisencodedinavector
U,andthepossiblesequencesofactions(whereeachallowable
sequencedefinesapolicy)areencodedinamatrixdenotedby
V(onerowpertimepoint,onecolumnperpolicy,andathird
dimensionforstatefactor).Intheexplore–exploittaskexample,
U couldincludethechoicetotakethehintandthechoicetose-
lecteachofthetwomachines,while Vcouldincludethepossible
actionsequences,suchas,forexample,takingthehintandthen
choosingtheleftmachinevs.takingthehintandthenchoosing
therightmachine.Notethatthepossibleactionsencodedinthe
vectorU arealsosometimesreferredtoas‘ controlstates’inthe
activeinferenceliterature.
As we have done in previous sections, the free energy and
expectedfreeenergyforeachpolicyaredenotedbyvectors F and
12
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
G,respectively.Thedegreetowhich G controlspolicyselection
ismodulatedbyafurtherparameter γ (asinglenumber;i.e.,a
scalar).Thisparameterisa precisionestimatefortheexpected
freeenergy overpolicies.Itcanbethoughtofasencodingaprior
beliefabouttheconfidencewithwhichpoliciescanbeinferred
(i.e.,howreliablebeliefsaboutthebestpolicyareexpectedtobe).
Itisoftencalledthe‘priorpolicyprecision’parameter;however,
it is important to note that this is not the same thing as the
precisionofposteriorbeliefsoverpolicies( π).Thisisbecause π
alsodependsonthevectors E (habits)and F (showninTable2
furtherbelow)— whichmeans,forexample,that π couldbe
preciseevenif γ werelow(Hespetal.,2020).Forthisreason,
itisbettertothinkof γ asan‘expectedfreeenergy( G)precision’
parameterasopposedtoapolicyprecisionparameterperse.Ifno
habitsarepresent(i.e.,if E isaflatdistribution ),lower γ values
leadtomorerandomnessinpolicyselection.Inthepresenceof
stronghabits,lower γ valuesincreasehowmuchhabitsinfluence
policyselection,becausetheinfluenceof G isreducedrelativeto
E (seeequationfor π inTable2furtherbelow).Youcansimulate
these dynamics yourself by specifying values forγ , E, F, and
G within theEFE_Precision_Updating.mcode provided in the
supplementarycode.
Insomemodelswewilldiscussbelowwithmulti-steppoli-
cies,thepriorvaluefor γ isupdatedintoaposterior γ estimate
(viaupdatestoitshyperparameter β).Table2andFig.9explain
thisindetail.Briefly,thevalueofthisprecisionparameterisin-
creasedaftereachobservationifthevariationalfreeenergyover
policiesisconsistentwiththeexpectedfreeenergyoverpolicies
priortothatobservation.Ifthesefreeenergiesareinconsistent,
thisprecisionisdecreased(i.e.,theagentbecomeslessconfident
initsestimatesof G).However,itisimportanttonotethatthere
are situations in which policies are only considered from the
current time step into the future (such as ‘shallow’ or ‘short-
sighted’ policies that, at each time step, only ‘look ahead’ one
timesteptoconsidertheimmediateconsequencesofdifferent
actions).Insuchcases,previousobservationsdonotinformthe
relativeprobabilitiesofpolicies(i.e.,theyarejust‘reset’ateach
timestep)— andtheexpectedprecisionreducestotheprior
valuefor γ (andisnotupdated).(Note:below,andinthe sup-
plementarycode,weshowhow‘shallow’,one-steppoliciesvs.
‘deep’,multi-steppoliciescanbeincludedbyspecifyingpolicies
withthevariable U vs.V ).
AllthemodelvariablesaresummarizedinTable1.Solutions
forinferenceinthePOMDPareshowninTable2attheendof
thissection.Ineachofthesetableswealsoprovideadescription
ofthewayeachmodelelementcanbeusedtoimplementthe
explore–exploittaskexample,whichwebuildinSection3.
2.2. Graphical models
In many papers in the active inference literature, POMDPs
are represented usinggraphical models. We will now discuss
theserepresentationsandwhattheirroleisinactiveinference.
By the end of this subsection, the reader should be able to
interpretgraphicalmodelsandunderstandthedifferentbenefits
theyprovide.
Graphical models, such as the graphs shown in Figs. 5–7,
are a useful method for visually depicting how variables in a
modeldependononeanother.Whenmodelsincludeprobability
distributionsovervariables,thesegraphscanbeusedtorepre-
senttheconditionalrelationshipsbetweenthesevariables.These
types of probabilistic graphical models are particularly useful
inthecontextofactiveinferencebecausetheyprovideaclear
visualsummaryofthecomputationalarchitectureofthemodels,
andtheway(biologicallyplausible)messagepassingalgorithms
(describedbelow)canbeusedtoupdatebeliefs.Hereweconsider
twotypesofgraphicalmodels– Bayesiannetworks (or‘Bayes
nets’,seeFig.5;andforanintroduction,seechapter8of(Bishop,
2006)and Forney-style(normal)factorgraphs (Dauwels,2007;
Loeliger, 2004). For readers interested in a more detailed in-
troduction to the use of Forney-style factor graphs in active
inference,werecommendtheexcellenttutorialintroductionby
deVriesandFriston(2017).
When depicting active inference models with Bayes nets
(Fig.5),thecircles(‘nodes’)correspondtovariables(e.g.,obser-
vations,hiddenstates,andpolicies),whilethearrowsconnecting
nodes(‘edges’)showthedependenciesbetweenvariablesrepre-
sentedbyeachnode.Forexample,thearrowsinFig.5goingfrom
the‘sτ node’(i.e.,statesattime τ)tothe‘ oτ node’(i.e.,outcomes
attime τ)meansthatthevalueof oτ dependsonthevalueof
sτ .Thisentailsthatifoneknowssomethingaboutobservations,
thenonecaninfersomethingaboutthehiddenstatesthatcause
them(i.e.,thehiddenstatesthatgeneratethoseobservationsin
thegenerativemodel).ReadersfamiliarwithBayesiannetworks
willnotethattheformofthegraphicalmodelshowninFig.5is
slightlyunusual,assquaresdenotingthefactorsthatmediatethe
conditionalrelationshipshavebeenplacedontopoftheedges
(e.g.,the AandBπ,τ matrices).
Thisgraphicalmodelservesasaconcisevisualdepictionof
therelationshipsbetweenmodelelementscoveredindetailin
theprevioussubsection.Itillustrateshowobservationsateach
timestep(purple)aregeneratedbyhiddenstates(green)viaa
mappingencodedbythelikelihoodmatrix A.The Bπ,τ matrixis
shownasmediatingthedependenciesbetweenstatesatdifferent
timepoints(i.e.,encodingbeliefsabouthowstateschangeover
time).Theprobabilityoverstatesatthefirsttimepointisshown
to depend on theD vector. State transitions (Bπ,τ ) are shown
to depend upon policies (π). The probability distribution over
policiesinturndependsonlearnedpriorsoverpolicies( E)and
theEFE ofeachallowablepolicy( G).The EFE isshowntodepend
onthepriordistributionoverobservations( C),whichencodesthe
agent’spreferencesforsomeobservationsoverothers(i.e.,this
dependency entails that policies with the lowestEFE will be
those expected to generate the most preferred observations).
Theinfluenceof EFE onpoliciesalsodependsuponitsprecision
term (γ ), which encodes confidence in currentEFE estimates.
Thisinturndependsonthevalueof β (aninitialpriorover γ
thatcansubsequentlybeupdated;seeTable2foradescription).
Tohelpreadersgainanintuitionforinferenceusinggraphical
models,Fig.5buildsupafullPOMDPstartingfromagraphical
representationofperceptionatasingletimepointusingBayes
theorem(withaworkedexample).Itthenaddstheevolutionof
statesovertime,followedbytheirdependenceonpolicies,and
thedependenceofpoliciesonthevariablesjustdescribed.
Thedefiningcharacteristicofagenerativemodelsuchasthat
showninFig.5isthatitcanbeusedtogeneratedata(i.e.,obser-
vations).TheconditionaldependenciesdepictedintheBayesian
networkinthebottom-rightofthisfigureshowhowobservations
aregeneratedbyaPOMDP.Startingatthetopofthenetwork,a
policy(π)isfirstselectedviaasoftmax(normalizedexponential)
function(σ)oftheaforementionedvariables(foranintroduction
tothesoftmaxfunction,seeAppendixA).Theinitialdistribution
overpoliciespriortoreceivinganobservationisdenoted π0 =
σ(lnE − γ G),whiletheposterioroverpoliciesafterreceivingan
observationalsoincorporatesthe VFE:π = σ(lnE−F −γ G).Next,
policy-dependenttransitionprobabilitiesencodedinthe Bπ,τ ma-
trix(orthe D vectorat τ = 1)generatehiddenstates,which
inturngenerateobservationsateachtimepoint.Thelikelihood
(A)matrixdetermineswhichobservationsaregeneratedbyeach
hiddenstate.
Recallthattoperforminferencewemustinvertthegenerative
model(i.e.,inferthemostlikelystatesandpoliciesgiveneach
13
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.5. Bayesiannetworkrepresentationsofstateestimation(perception)andpolicyselection.Eachgraphdepictsagenerativemodelofthecausesofobservations,
which can be inverted to perform inference.Top leftStatic perception with a worked example. Variables:s = states,o = observations,A= likelihood mapping
betweenstatesandoutcomes, D = initialstatepriors.ThisisequivalenttoexactBayesianinference. Bottomleft:Dynamic perception.Transitionmatrices( Bτ )have
been added to describe (beliefs about) how states change over time. Subscripts for observations and states correspond to time point in a trial (denoted bytau;
τ).Importantly,when τ >1,the Bτ matrixfromtheprevious τ (i.e.,Bτ−1)functionsasanempiricalprior,playingthesameroleasthe D vectorat τ = 1.Top
right: Dynamic perception with policy selection. Each policy (π) entails a different sequence of actions, which corresponds to different transitions between states
(i.e.,different Bπ,τ matrices).Basedonexpectedfreeenergy( G;whichinturndependsonpriorpreferences, C),thehighestprobabilitywillbeassignedtopolicies
expectedtominimizeuncertaintyoverstatesandmaximizetheprobabilityofpreferredobservations. Bottomright:Dynamic perception with flexible policy selection.
Thisfinalmodelincludesanexpectedfreeenergyprecisionterm γ = E[γ],where p (γ) correspondstoagammadistribution( Γ )witha‘shape’parameterequalto
1anda‘rate’parameterspecifiedby β.Notethatthenon-italicized γ inthegenerativemodelisarandomvariable,whereastheitalicized γ isastatistic(expected
value)ofthegammadistribution(i.e.,afixedscalar)thatisupdatedbasedontheequationsshowninthefigure.Thevalueof γ encodestheagent’sconfidencein
policyselectionandadjuststhecontributionof G totheposteriordistributionoverpolicies( π).Thisprecisionvalueisalsooptimizedbyupdatingthevalueof β
afternewobservations,basedonthevariationalfreeenergy( F)overpoliciesassociatedwiththoseobservations.Inshort,whenanewobservationisinconsistent
withpriorbeliefsaboutpolicies( π0;i.e.,basedon G),theagentassignsalowerexpectedprecision( γ )to G whenarrivingatposteriorsoverpolicies( π;seeFig.9
andTable2formoredetails).Aprioroverpolicies( E)isalsoincluded,whichcanbeusedtomodelhabitformation.Alower γ (i.e.,lessconfidenceinmodel-based
beliefsabout G)alsoentailsastrongerinfluenceofthehabitsencodedin E onpolicyselection.Notethatthedependencyof π onF hasbeenomittedfromthe
graphicaldepictionofthegenerativemodelinthispanel.Seemaintextforfurthervariabledescriptions.SeeTable2forfurtherexplanationoftheseequations.
newobservation).Thisiswherenormalfactorgraphs(Fig.6)are
crucial, as they can be leveraged to both derive and visualize
asuiteofmessagepassingalgorithms(seebelow)forBayesian
inference.Normalfactorgraphsaremadeupof squarenodes
andedges(linesconnectingsquarenodes).Squarenodescanbe
thought of as functions (i.e., factors or conditional probability
distributions;seebelow)thattakeinsomeinput(e.g.,sufficient
statistics5 ofbeliefsoverstatesorobservations)andtransform
that information in some way to produce an output (e.g., the
sufficientstatisticsoftheconditionalprobabilitiesspecifiedby
thefactor).Theseinputsandoutputsarecalled‘messages’and
arerepresentedbytheedgesconnectingthesquarenodes.When
an edge connects to only one square node it is called ahalf
edge,anditonlycarriesmessagestoandfromthatnode.When
an edge connects two square nodes, this indicates that these
nodesexchangemessagesandthateachcontributestothevalue
represented on that edge. When three square nodes exchange
messages,thegraphisadjustedtocontainthreeedges(onecon-
nectedtoeachsquarenode)thatconvergeontoan equalitynode
(asmallsquarenodewithan‘=’sign),whichcombinesthemes-
sagesinaspecificmanner(describedinthefollowingtechnical
5 Note that, for the categorical distributions we use in this tutorial, the
sufficientstatisticswillcorrespondtotheprobabilityofeachpossiblevalueof
arandomvariable(e.g.,theprobabilityofeachpossiblestateorobservation).
section).Asdescribedfurtherbelow,themessagesrepresentedon
eachedgecorrespondtothevariablesrepresentedbythecircular
nodesintheBayes’netdepictioninFig.5,whilethesquarenodes
inafactorgraphcorresponddirectlytothesquarenodesinthis
figure.Notethat,insomecases,theedgesinfactorgraphsare
alsosupplementedbyincludingcirclenodestorepresent(thesuf-
ficientstatisticsof)hiddenvariables(asinFig.5),butwedepict
themwithoutcirclenodesinFigs.6and7togivethereadersome
familiaritywiththiscommonlypresentedform.Alsonotethat,
unlikeBayesnetdepictions,normalfactorgraphshaveundirected
edges,whichhighlightsthebidirectionalnatureofmessagepass-
ing(seelightpurplearrowsinthebottomportionofFig.6).
More technically, normal factor graphs represent a factor-
ization of the generative model. Recall that generative models
are formally defined as the joint probability distribution over
observations, states, and policies of the POMDP across time,
p (o1: T , s1: T , π).Factorizationmeansthatthisjointprobability
canbedefinedastheproductofseveralconditionallyindepen-
dent distributions. In POMDPs, the factorization assumes that
eachstateonlydependsonthestateattheprevioustimestepand
policy(i.e.,theso-calledMarkovproperty).Thisisdescribedby
thefollowingequation,whichshowsafactorizationofthejoint
distributionintopriordistributionsoverstatesandpolicies,and
14
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table1
Modelvariables.
Modelvariable* Generaldefinition Modelspecificationforexplore–exploittask(describedin
detailSection3)
oτ Observableoutcomesattime τ. Outcomemodalities:
1.Hints(nohint,hint-left,hint-right)
2.Reward(start,lose,win)
3.Observedbehavior(start,takehint,
chooseleft,chooseright)
sτ Hiddenstatesattime τ.Onevectorofpossiblestatevalues
foreachstatefactor(i.e.,eachindependentsetofstates;e.g.,
visualvs.auditorystates).
Hiddenstatefactors:
1.Context(leftmachineisbettervs.
rightmachineisbetter)
2.Choices(start,takehint,chooseleft,chooseright)
π Avectorencodingthedistributionoverpoliciesreflectingthe
predictedvalueofeachpolicy.Eachpolicyisaseriesof
allowableactionsinavector U,whereactionscorrespondto
differentstatetransitions(i.e.,different Bπ,τ matrices)thatcan
bechosenbytheagentforeachstatefactor.Policiesare
chosenbysamplingfromthisdistribution.
Allowablepoliciesincludethedecisionto:
1.Stayinthestartstate
2.Getthehintandthenchoosetheleftmachine
3.Getthehintandthenchoosetherightmachine
4.Immediatelychoosetheleftmachine(andthen
returntothestartstate)
5.Immediatelychoosetherightmachine(andthen
returntothestartstate)
Amatrix:p(oτ |sτ ) Amatrixencodingbeliefsabouttherelationshipbetween
hiddenstatesandobservableoutcomesateachtimepoint τ
(i.e.,theprobabilitythatspecificoutcomeswillbeobserved
givenspecifichiddenstatesatspecifictimes).Notethatinthe
POMDPstructuretypicallyusedintheactiveinference
literature(andwhichwedescribeinthistutorial),the
likelihoodisassumedtoremainconstantacrosstimepointsin
atrial,andhencewillnotdifferatdifferentvaluesfor τ
(althoughonecouldadjustthisifsodesired).Thelikelihoodis
alsoassumedtobeidenticalacrosspolicies,andsothereisno
indexingwithrespectto π.
Whenthereismorethanoneoutcomemodality,thereisone
Amatrixperoutcomemodality.Whenthereismorethanone
statefactor,thesematricesbecomehigh-dimensionalandare
technicallyreferredtoas tensors.Forexample,asecondstate
factorcorrespondstoathirdmatrixdimension,athirdstate
factorcorrespondstoafourthmatrixdimension,andsoforth.
Encodesbeliefsabouttherelationshipbetween:
1.Probabilitythatthehintisaccurateineachcontext
2.Probabilityofrewardineachcontext
3.Identitymappingbetweenchoicestatesand
observedbehavior
Bπ,τ matrix:p(sτ+1|sτ , π) Amatrixencodingbeliefsabouthowhiddenstateswillevolve
overtime(transitionprobabilities).Forstatesthatareunder
thecontroloftheagent,therearemultiple Bπ,τ matrices,
whereeachmatrixcorrespondstooneaction(statetransition)
thattheagentmaychooseatagiventimepoint(ifconsistent
withanallowablepolicy).Whenthereismorethanone
hiddenstatefactor,thereisoneormore Bπ,τ matricesper
statefactor(dependingonpolicies).
Encodesbeliefsthat:
1.Contextdoesnotchangewithinatrial
2.Transitionsfromanychoicestatetoanyother
arepossible,dependingonthepolicy.
Cmatrix:p(oτ |C) Amatrixencodingthedegreetowhichsomeobserved
outcomesarepreferredoverothers(technicallymodeledas
priorexpectationsoveroutcomes).Whenthereismorethan
oneoutcomemodality,thereisone Cmatrixperoutcome
modality.Rowsindicatepossibleobservations;columns
indicatetimepoints.NotethateachcolumnofvaluesinCis
passedthroughasoftmaxfunction(transformingitintoa
properprobabilitydistribution)andthenlog-transformed
(usingthenaturallog).Thus,preferencesbecome
log-probabilitiesoveroutcomes.
Encodesthestrongerpreferenceforwinsthanlosses.Wins
arealsomorepreferredatthesecondtimepointthanthe
thirdtimepoint.
(continued on next page)
thedistributionsrepresentingthelikelihoodandstatetransitions.
p (o1: T , s1: T , π) = p (s1) p (π)
T∏
τ=1
p (oτ |sτ )
T∏
τ=2
p (sτ |sτ−1, π) (14)
p (o1: T , s1: T |π) = p (s1)
T∏
τ=1
p (oτ |sτ )
T∏
τ=2
p (sτ |sτ−1, π) (L2)
= s1· D
T∏
τ=1
oτ · Asτ
T∏
τ=2
sτ · Bπ,τ sτ−1 (L3)
15
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table1 (continued).
Modelvariable* Generaldefinition Modelspecificationforexplore–exploittask(describedin
detailSection3)
D vector:p(s1) Avectorencodingbeliefsabout(aprobabilitydistribution
over)initialhiddenstates.Whenthereismorethanone
hiddenstatefactor,thereisone D vectorperstatefactor.
Theagentbeginsinaninitialstateofmaximaluncertainty
aboutthecontextstate(priortolearning),butcomplete
certaintythatitwillstartinthe‘start’choicestate.
E vector:p(π) Adistributionencodingbeliefsaboutwhatpolicieswillbe
chosenapriori(apriorprobabilitydistributionoverpolicies,
implementedasavectorassigningonevaluetoeachpolicy),
basedonthenumberoftimesdifferentactionshavebeen
choseninthepast.
Theagenthasnoinitialhabitstochooseoneslotmachineor
another(priortolearning).
*While,forconsistency,wehaveusedthestandardnotationfoundintheactiveinferenceliterature,itisimportanttonotethatitdoesnotalwaysclearlydistinguish
betweendistributionsandthepossiblevaluestakenbyrandomvariablesunderthosedistributions.Forexample, π referstothedistributionoverpolicies,butwhen
usedasasubscriptitindexeseachindividualpolicy(e.g., Bπ,τ indicatesadistinctmatrixforeachdifferentpolicy).Thissameconventionholdsfor s ando.
Fig.6. Top:Equationsspecifyingthefactorsthatconstitutethefactorizedgenerativemodel.Numbersinthegreensquareshighlightthecorrespondencebetween
theequationsandthefactorsinthegenerativemodelthatarerepresentedwithinthenormalfactorgraphinthebottompanel.Here, Cat()indicatesacategorical
distribution.Middle:Beliefupdateequationforapproximateposteriorsoverstatesthatisderivedfromvariationalmessagepassing(notethedifferencebetween
thismessagepassingschemeandthemarginalmessagepassingapproachdescribedinthetext).Purplenumbersindicatethecorrespondencebetweentermswithin
theupdateequationandthemessagespassedbetweeneachfactorshowninthefactorgraphinthebottompanel. Bottom:Normalfactorgraphrepresentationof
thefactorizedPOMDP.IncontrasttotheBayesnetrepresentationshowninFig.5,nodes(largewhiteboxes)representfactors,whereastheedges(linesconnecting
eachbox)representthesufficientstatisticsofapproximateposteriors,whicharepassedasmessagesbetweenfactors(i.e.,edgesrepresentthecommonvariables
thatparticipateinthefactorstheyconnect,suchasposteriorsoverstatesundereachpolicyforeachtimepoint, sπ,τ ).(Forinterpretationofthereferencestocolor
inthisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
Source: AdaptedfromFristonetal.(2017c).
Fortheunfamiliarreader,pleasenotethatthesymbol
∏T
τ (·)
indicatestakingtheproductofeachofthedistributionstothe
rightofitforeachtimepoint τ tothefinaltimepoint T .Line2
showstheformofthedistributionconditionedonpolicies.Line
3 rewrites line 2 in matrix form by replacing each categorical
distribution with the above-described matrices/vectors whose
columnscontaintheparametersoftherespectivedistributions.
Specifically,p (s1) = s1·D,p (oτ |sτ ) = oτ ·Asτ ,and p (sτ |sτ−1, π) =
sτ · Bπ,τ sτ−1.Here oτ andsτ−1 arevectorsofzeroswithaone
placedintheelementcorrespondingtothestate/observationof
interest.Theirroleissimplytoselectouttheelementsofthe
AorBmatrixcorrespondingtotherelevantstate-outcomepair
or current state-previous state pair. Once in matrix form, it is
easy to see the direct correspondence between the factorized
distributionshownintheequationaboveandthefactorsincluded
inthenormalfactorgraphinFig.6.Eachofthesedistributionsis
associatedwithafactornode.Eachedgerepresentstheprobabil-
itydistributionoverthevariablethatneedstobeinferred(i.e.,the
approximateposteriorsoverstates sπ,τ ,andpolicies π).Edges
16
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.7. ThisfigurereproducesthesamegraphshowninFig.6toillustratethelinkbetweenmessagepassingandpolicyselectioninactiveinference. Top:AsinFig.6,
theseequationsspecifythefactorsthatconstitutethefactorizedgenerativemodel,andthenumbersinthegreensquareshighlightthecorrespondencebetweenthe
equations and the factors represented by the normal factor graph below.MiddleBelief update equation for inferring the posterior over policies. Purple numbers
indicatethecorrespondencebetweentermswithintheupdateequationandthemessagespassedbetweeneachfactorshowninthefactorgraph. Bottom:Normal
factor graph representation of message passing in the context of inference over policies. Red dotted lines show partition functions of the graph, which are used
toconstructthefreeenergyapproximationstotheprobabilityofcurrentobservationsconditionedonpolicies, − lnp (oτ |π) ≈ Fπ,τ ,andtheexpectedprobabilityof
futureobservationsconditionedonpolicies, −Eq(oτ>t ,sτ>t |π)[lnp (oτ>t |π)]≈ Gπ,τ .Thefactors Fπ,τ andGπ,τ thenbecomethemessages(shownbythepurplearrows)
sentfromthetwotransitionprobabilityfactors( Bπ,τ −1 andBπ,τ )thatconvergeontheequalityconstraintnode(connectingthe Bπ,τ nodesandthe E node).When
combined with the message sent fromE, and after the application of a softmax function, this becomes the posterior over policies (adapted from Friston et al.
(2017c),ParrandFriston(2018a)).Asnotedinthetext,thisrepresentationofinferenceoverpoliciesasmessagepassingisheuristicandonlymeantasananalogy
tomessagepassingwithrespecttoposteriorsoverstates.Thisisbecauseitisnotcarriedoutiteratively(i.e.,theposteriorisarrivedatusingasingleiteration),
themessagesarenotbidirectional,and Fπ,τ andGπ,τ arenotfactors.Thisgraphicalrepresentationalsocannotillustratealldependencieswithrespectto Gπ,τ .This
isbecause Gπ,τ dependsontwodifferenttypesofpredictedfutureobservations– p (oτ>t |π) andp (oτ>t |C) — onlythefirstofwhichisdepictedhere(i.e.,with
respectto o3).Foraproposedschemeforcarryingoutiterativemessagepassingwithrespecttoinferenceoverpolicies,seeChampion,Grześ,andBowman(2021).
(Forinterpretationofthereferencestocolorinthisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
connectfactorsthatexchangemessageswiththesamevariables
(e.g.,D,A,and Bπ,τ areallconnectedbythevariable sπ,1).
2.3. Technical section on variational and marginal message passing
(optional)
In this subsection we will introducevariational message
passing(Winn&Bishop,2005),whichisfoundationaltotheway
active inference performs approximate inference of posteriors
over states. By the end of this subsection, the reader should
understandthegeneralstepsforperformingvariationalmessage
passingandhowtheycanbecarriedoutusingthefactorgraph
representationofPOMDPsinactiveinference.Forreaderswith
less mathematical background, this section can also be safely
skippedwithoutcompromisingtheabilitytounderstandtherest
ofthetutorial.Although,asalways,wehavemadeeffortstofully
explain all equations. For readers with specific interest in this
topic, we also note here that more recent implementations of
activeinferencehaveusedarefinedalgorithm–called marginal
messagepassing –thatismorerobusttoproblemsofovercon-
fidencethatarisewithvariationalmessagepassing(i.e.,where
posterior beliefs can become too precise too quickly; see Parr
etal.(2019).However,understandingmarginalmessagepassing
requires us to first understanding variational message passing.
Therefore,wewillfocusonthisapproachhere,andreturntohow
ithasbeenrefinedattheendofthesection.
Toinvertthemodel(i.e.,conditiononobservationstoinfer
approximate posteriors over states and policies) via the mini-
mizationof VFE,somesimplifyingassumptionsneedtobemade
(i.e.,sinceexactinferenceisintractableinmostreal-wordcases).
Variationalmessagepassingisbasedonthe mean-fieldapproxi-
mation,whichassumesthattheapproximateposteriorfactorizes
intotheproductof(independent)distributions(Bishop,2006).
Thisapproximationoftenworkswellinpractice,butithasthe
limitationofignoringpossiblepairwise(ormorecomplex)inter-
actionsbetweenvariables.InthePOMDPsunderdiscussionhere,
themean-fieldapproximationassumesthattheapproximatepos-
terior factorizes into a prior distribution over policies and the
distributionsoverstatesexpectedundereachpolicyateachtime
17
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table2
Matrixformulationofequationsusedforinference.
Modelupdate
component
Updateequation Explanation Model-specificdescriptionfor
explore–exploittask(described
indetailSection3)
Updatingbeliefsabout
initialstatesexpected
undereachallowable
policy.
επ,τ =1 ← 1
2
(
lnD + ln(B†
π,τ sπ,τ +1)
)
+ lnAToτ − lnsπ,τ =1
sπ,τ =1 = σ
( 1
2
(
lnD + ln(B†
π,τ sπ,τ +1)
)
+ lnAToτ
)
First equation:Thevariable επ,τ =1 isthestateprediction
errorwithrespecttothefirsttimepointinatrial.
Minimizingthiserrorcorrespondstominimizing VFE
(viagradientdescent)andisusedtoupdateposterior
beliefsoverstates.Theterm
(
lnD + ln(B†
π,τ sπ,τ +1)
)
correspondstopriorbeliefsinBayesianinference,based
onbeliefsabouttheprobabilityofinitialstates, D,and
theprobabilityoftransitionstofuturestatesundera
policy,ln(B†
π,τ sπ,τ +1).Theterm AToτ correspondstothe
likelihoodterminBayesianinference,evaluatinghow
consistentobservedoutcomesarewitheachpossible
state.Thetermln sπ,τ =1 correspondstoposteriorbeliefs
overstates(forthefirsttimepointinatrial)atthe
currentupdateiteration.
Second Equation:Wemovetothesolutionforthe
posteriorsπ,τ =1 bysetting επ,τ =1 = 0,solvingfor
lnsπ,τ =1,andthentakingthesoftmax(normalized
exponential)function(denoted σ)toensurethatthe
posterioroverstatesisaproperprobabilitydistribution
withnon-negativevaluesthatsumsto1.Thisequation
isdescribedinmoredetailinthemaintext.A
numericalexampleofthesoftmaxfunctionisalso
showninAppendixA.
Updatingbeliefsabout:
1.Whethertheleftvs.right
slotmachineismore
likelytopayout
onagiventrial.
2.Theinitialchoicestate
(here,alwaysthe
‘start’state).
Updatingbeliefsabout
allstatesafterthefirst
timepointinatrialthat
areexpectedundereach
allowablepolicy.
επ,τ> 1 ← 1
2
(
ln
(
Bπ,τ −1sπ,τ −1
)
+ ln
(
B†
π,τ sπ,τ +1
))
+ lnAToτ − lnsπ,τ> 1
sπ,τ> 1 = σ
( 1
2
(
ln(Bπ,τ −1sπ,τ −1)
+ ln(B†
π,τ sπ,τ +1)
)
+ lnAToτ
)
First equation:Thevariable επ,τ> 1 isthestateprediction
errorwithrespecttoalltimepointsinatrialafterthe
firsttimepoint.Minimizingthiserrorcorrespondsto
minimizingVFE (viagradientdescent)andisusedto
updateposteriorbeliefsoverstates.Theterm(
ln(Bπ,τ −1sπ,τ −1)+ ln(B†
π,τ sπ,τ +1)
)
correspondstoprior
beliefsinBayesianinference,basedonbeliefsaboutthe
probabilityoftransitionsfrompaststates,
ln(Bπ,τ −1sπ,τ −1),andtheprobabilityoftransitionsto
futurestates,ln( B†
π,τ sπ,τ +1),underapolicy.Theterm
lnAToτ correspondstothelikelihoodterminBayesian
inference,evaluatinghowconsistentobservedoutcomes
arewitheachpossiblestate.
Second Equation:Asinthepreviousrow,wemoveto
thesolutionfortheposterior, sπ,τ> 1,bysetting
επ,τ> 1 = 0,solvingforln sπ,τ> 1,andthentakingthe
softmaxfunction( σ).Thisequationisdescribedinmore
detailinthemaintext.
Updatingbeliefsabout:
1.Whethertheleftvs.
rightslotmachineis
morelikelytopay
outonagiventrial.
2.Beliefsaboutchoice
statesafter the
initialtimepoint
(here,thisdepends
onthechoiceto
takethehintor
toselectoneof
theslotmachines).
Probabilityofselecting
eachallowablepolicy
π0 = σ(lnE − γ G)
π = σ(lnE − F − γ G)
Theinitialdistributionoverpoliciesbeforemakingany
observations(π0),andtheposteriordistributionover
policiesafteranobservation( π).Theinitialdistribution
ismadeupofthelearnedprioroverpoliciesencodedin
theE vector(reflectingthenumberoftimesapolicy
haspreviouslybeenchosen)andtheexpectedfree
energyofeachallowablepolicy( G).Theposterior
distributionisdeterminedby E,G,andthevariational
freeenergy( F)undereachpolicyaftermakinganew
observation.Theinfluenceof G isalsomodulatedbyan
expectedprecisionterm( γ ),whichencodesprior
confidenceinbeliefsabout G (describedfurtherinthe
maintext;alsoseeFig.9).Seerow1foranexplanation
ofthefunctionofthe σ symbol.
Wenote,however,thatincorporationof E,F,and/or γ
whencomputing π isamodelingchoice.Theseneed
notbeincludedinallcases(e.g.,seetop-leftportionof
Fig.5;alsoseeDaCosta,Parretal.,2020).Insome
contexts,onemightchoosetoincludesomeofthese
termsbutnotothers,ortoonlyinclude G.Thisdepends
ontheresearchquestion.(e.g., E willbeusefuliftask
behaviorisinfluencedbyhabits,while F/γ canbe
usefulwhentherearemanypossibledeeppoliciesto
choosefrom).Seetherowinthistableon‘Expected
freeenergyprecision’formoredetailsaboutinference
overpolicieswhen F/γ areincluded.Thisisalso
discussedfurtherinthemaintext.
Updatingoverallbeliefsabout
whetherthebestcourseof
actionistotakethehint
and/ortochoosetheleftvs.
rightslotmachine.
(continued on next page)
18
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table2 (continued).
Modelupdate
component
Updateequation Explanation Model-specificdescriptionfor
explore–exploittask(described
indetailSection3)
Expectedfreeenergyof
eachallowablepolicy
Gπ = DKL [q(o|π)∥ p (o|C)]
+ Eq(s|π)[H[p(o|s)]]
Gπ =
∑
τ
(
Asπ,τ ·
(
lnAsπ,τ − lnCτ
)
− diag
(
ATlnA
)
· sπ,τ
)
Thefirstequationreproducesthe‘risk+ambiguity’
expressionfortheexpectedfreeenergyofeachpolicy
(Gπ )thatisexplainedinthemaintext.Thesecond
equationshowsthissameexpressionintermsofthe
elementsinthePOMDPmodelusedinthistutorial(i.e.,
inmatrixnotation).
Expectedfreeenergyevaluatesthevalueofeachpolicy
basedontheirexpectedabilityto:(1)generatethe
mostdesiredoutcomes,and(2)minimizeuncertainty
abouthiddenstates.Achievingthemostdesired
outcomescorrespondstominimizingtheKLdivergence
betweenpreferredobservations, p (o|C) = Cτ ,andthe
observationsexpectedundereachpolicy,
q (o|π) = Asπ,τ = oπ,t .Minimizinguncertainty
correspondstominimizingtheexpectedentropyofthe
likelihood(Eq(s|π)[H[p(o|s)]]= −diag
(
ATlnA
)
· sπ,τ ).
Notethatthe diag()functionsimplytakesthediagonal
elementsofamatrixandplacestheminarowvector.
Thisissimplyaconvenientmethodforextractingand
operatingonthecorrectmatrixentriestocalculatethe
entropy,H [p (o|s)]= −∑
p(o|s)lnp (o|s),ofthe
distributionsencodedwithineachcolumnin A.For
simplenumericalexamplesofcalculatingtheriskand
ambiguityterms,seediscussionof‘outcomeprediction
errors’inSection2.4.
The‘risk’term–
DKL [q(o|π)∥ p (o|C)]=
Asπ,τ ·
(
lnAsπ,τ − lnCτ
)
–
drivestheagenttoselectthe
slotmachineexpectedtobe
mostlikelytopayout.Ifthe
valueofwinningmoneyin Cτ
ishighenough(i.e.,if p (o|C) is
sufficientlyprecise),thiswill
detertheagentfromchoosing
toaskforthehint.
The‘ambiguity’term–
Eq(s|π)[H[p(o|s)]]=
−diag
(
ATlnA
)
· sπ,τ –drives
theagenttominimize
uncertaintybyaskingforthe
hint.
Marginalfreeenergyof
eachallowablepolicy
Fπ = Eq(s|π)[lnq (s|π)
− 1
2
Eq(sτ−1|π)[lnp (sτ |sτ−1, π)]
− 1
2
Eq(sτ+1|π)[lnp (sτ |sτ+1, π)]
− lnp (oτ |sτ )]
Fπ =
∑
τ
sπ,τ ·
(
lnsπ,τ
− 1
2
(
ln(Bπ,τ −1sπ,τ −1)
+ ln(B†
π,τ sπ,τ +1)
)
− lnAToτ
)
Thefirstequationshowsthe marginal (asopposedto
variational)freeenergy,whichisnowusedinthemost
recentimplementationsofactiveinference.Thesecond
equationshowsthissameexpressionintermsofthe
elementsinthePOMDPmodelusedinthistutorial(i.e.,
inmatrixnotation).Marginalfreeenergyhasasightly
differentformthantheexpressionsfor VFE thatarealso
showninthetext(andwhichhavebeenusedinmany
previouspapersintheactiveinferenceliterature).This
updatedformimprovesoncertainlimitationsofthe
messagepassingalgorithmsderivedfromminimization
ofVFE (seeSection2.3;alsosee(Parr,Markovic,Kiebel,
&Friston,2019).
Marginalfreeenergyevaluatestheevidencethat
inferredstatesprovideforeachpolicy(basedonnew
observationsateachtimepoint).Seethefirsttworows
inthistableonupdatingbeliefsaboutstatesforan
explanationofhoweachtermintheequationrelatesto
Bayesianinference.
Thiswouldencodetheamount
ofsurprise(givenachoiceof
policy)whenobservingahint
orawin/lossafterselectinga
specificslotmachine.
(continued on next page)
point:
p (s1: T |o1: T , π) ≈ q(s1: T , π)= q(π)
T∏
τ
q(sτ |π) (15)
Notethat,byconvention,approximateposteriordistributions
aredenotedwiththevariable q.Alsoagainrecallthat T corre-
spondstothefinaltimepointinatrial,suchthatthisposterior
distributionisoverthevaluesofstatesacrosstimepointsunder
eachpolicy— andthisdistributionitselfevolvesovertimewith
eachnewobservation.Thismeansthatanobservationatalater
time can change posterior beliefs about states at earlier times
(i.e.,retrospectiveinference).
With this factorization in hand, we can employ variational
message passing to infer the approximate posteriorq(sτ |π) at
each edge of the graph, and then combine them into a global
posteriorq(s1: T |π)usingtheequationjustpresented.Variational
messagepassingcanbesummarizedintermsofthefollowing
steps:
1. Initializethevaluesoftheapproximateposteriors q(sπ,τ )
forallhiddenvariables(i.e.,alledges)inthegraph.
2. Fixthevalueofobservedvariables(here, oτ ).
3. Chooseanedge( V )correspondingtothehiddenvariable
youwanttoinfer(here, sπ,τ ).
4. Calculatethemessages,µ(sπ,τ ),whichtakeonvaluessent
byeachfactornodeconnectedto V .
5. Passamessagefromeachconnectedfactornode N toV
(oftenwrittenas µN→V ).
6. Update the approximate posterior represented byV ac-
cordingtothefollowingrule: q(sπ,τ )∝ ⃗µ(sπ,τ )
←
µ(sπ,τ ).The
arrownotationhereindicatesmessagesfromtwodifferent
factorsarrivingatthesameedge.
a. Normalize the product of these messages so that
q(sπ,τ )correspondstoaproperprobabilitydistribu-
tion.
b. Usethisnewq(sπ,τ )toupdatethemessagessentby
connectedfactors(i.e.,forthenextroundofmessage
passing).
7. Repeatsteps4–6sequentiallyforeachedge.
8. Steps3–7arethenrepeateduntilthedifferencebetween
updatesconvergestosomeacceptablylowvalue(i.e.,re-
sultinginstableposteriorbeliefsforalledges).
19
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table2 (continued).
Modelupdate
component
Updateequation Explanation Model-specificdescriptionfor
explore–exploittask(described
indetailSection3)
Expectedfreeenergy
precision
p(γ)= Γ (1, β)
E[γ] =γ = 1/β
Iteratedtoconvergence:
π0 ← σ(lnE − γ G)
π ← σ(lnE − F − γ G)
Gerror ← (π − π0) · (−G)
βupdate ← β − β0+ Gerror
β ← β − βupdate/ψ
γ ← 1/β
Theβ term,anditspriorvalue β0,isahyperparameter
ontheexpectedfreeenergyprecisionterm( γ).
Specifically,β isthe’rate’parameterofagamma
distribution(Γ )witha‘shape’parametervalueof1.
Theexpectedvalueofthisdistribution,E [γ]= γ ,is
equaltothereciprocalof β.Notethatweusethe
non-italicizedγ torefertotherandomvariableanduse
theitalicized γ torefertothescalarvalueofthat
variable.Thisscalariswhatissubsequentlyupdated
basedontheequationsshownhere.
Theγ termcontrolstheprecisionof G,basedonthe
agent’sconfidenceinitsestimatesofexpectedfree
energy.Thisconfidencechangeswhennewobservations
areconsistentorinconsistentwith G.Morespecifically,
γ modulatestheinfluenceof G onpolicyselectionbased
upona G predictionerror( Gerror ).Thisiscalculated
basedonthedifferencebetweentheinitialdistribution
overpolicies( π0)andtheposteriordistributionafter
makinganewobservation( π).Thedifferencebetween
thesetermsreflectstheextenttowhichnew
observations(scoredby F)makepoliciesmoreorless
likely.Ifthevectorencodingtheposterioroverpolicies
increasesinmagnitudeincomparisontotheprior,and
stillpointsinthesamedirection,thedifferencevector
betweentheposteriorandthepriorwillpointinthe
samedirectionasthe −G vector(i.e.,lessthana90 ◦
angleapart;seeFig.9).Ifso,thevalueof γ will
increase,therebyincreasingtheimpactof G onpolicy
selection.Incontrast,ifthedifferencevectorbetween
theposteriorandthepriordoesnotpointinthesame
directionasthe −G vector(i.e.,greaterthana90 ◦ angle
apart),γ willdecreaseandtherebyreducetheimpact
ofG onpolicyselection(i.e.,astheagent’sconfidence
initsestimatesofexpectedfreeenergyhasdecreased).
Notethatthe βupdate termmediatingtheseupdates
technicallycorrespondstothegradientoffreeenergy
withrespectto γ (∇γ F).Thesubsequentupdateinthe
valueof γ issuchthat G contributestotheposterior
overpoliciesinanoptimalmanner. β andGerror are
oftendiscussedinrelationtodopamineintheactive
inferenceliterature.
Notethat β0 istheinitialprior(whichisnotupdated),
andβ istheinitialposterior,whichissubsequently
updatedtoprovideanewestimatefor γ = 1/β.The
variableψ isastepsizeparameterthatreducesthe
magnitudeofeachupdateandpromotesstable
convergencetofinalvaluesof γ .Foraderivationof
theseequations,seeAppendixinSales,Friston,Jones,
Pickering,andMoran(2019).
Ahighervaluefor β would
reduceanagent’sconfidencein
thebestpolicybasedonthe
valuesin G.Thismightlead
theagenttoselectaslot
machinemorerandomlyor
basedtoagreaterextenton
itspastchoices(i.e.,ifithasa
preciseprioroverpoliciesin
thevector E).
Table note: The termB†
π,τ denotes the transpose ofBπ,τ with normalized columns (i.e., columns that sum to 1). Note that you may commonly see the dot (·)
notation used in the active inference literature to denote transposed matrix multiplication, such asA· oτ , which meansAToτ (we use the latter notation here).
WhenAmatriceshavemorethantwodimensions(i.e.,whentheyaretensors),thetransposeisappliedtothetwo-dimensionalmatrixassociatedwitheachvalue
oftheotherdimensions.The σ symbolindicatesasoftmaxoperation(foranintroductionsee AppendixA),whichtransformsvectorvaluestomakeupaproper
probabilitydistribution(i.e.,withnon-negativevaluesthatsumto1).Italicizedvariablesindicatevectors(orsinglenumbers[scalars]inthecaseof β andγ ).Bold,
non-italicizedvariablesindicatematrices.Subscriptsindicateconditionalprobabilities;e.g., sπ,τ = p(sτ |π).
Forunfamiliarreaders,the‘ ∝’symbolinstep6denotespro-
portionality,meaningthattheratiobetweenvariablesisalways
constant. We can change from the proportionality sign to an
equals(‘=’)signbyexplicitlyintroducingaconstant( k)intothe
equation,so x ∝ y becomesx = k × y.Forprobabilitydistri-
butions,theconstantisthenormalizationfactorthatensuresa
distributionsumsto1.Alsonotethat,whilethearrowsabove
eachµ instep6areusedtodistinguishmessagesconveyedfrom
twodifferentfactornodesontothesameedge,thefactorgraphs
inactiveinferencemodelsrequirethreefactornodestoexchange
messages. As mentioned earlier, when more than two factors
exchangemessages,thisrequiresedgesfromeachfactornodeto
convergeontoanequalitynode.Inthiscase,themessagecon-
veyedtoeachedgeistheproductofthemessagesfromtheother
connectedfactors: ⃗µ
(
sπ,τ
)
∝ ⃗µ1
(
sπ,τ
)
⃗µ2
(
sπ,τ
)
. . .⃗µN
(
sπ,τ
)
.
Forhiddenstates sπ,τ ,eachmessageconveystheexponenti-
atedexpectedlogvalueofeachfactor ⃗µ(sπ,τ )∝ expEq[lng(sπ,τ )],
whereg(sπ,τ ) denotes the function represented by each factor
(Dauwels,2007).Forobservedvariables,themessagesimplycon-
veys the known value of the factor, which is easily calculated
(e.g., in the POMDPs considered in this tutorial, the message
is simplyATo). When combined, these messages allow for ap-
proximationoftheposteriorrepresentedbytheassociatededge.
Theposteriorateachedge q
(
sπ,τ
)
isnormalizedbyapplyinga
softmaxfunctionpriortothenextroundofmessagepassing.
20
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Usingtheseupdaterules,wearriveatthefollowingupdate
equationsforapproximateposteriorsoverstatesinourPOMDP
models,whichwewillcallmessages1–3(i.e.,performedinlog-
space;representedbypurplecirclesonthefactorgraphateach
locationinFig.6whereedgesmeetfactors):
Message1 : ln⃗µBπ,τ −1→sπ,τ = Eq(sτ−1|π)[lnp (sτ |sτ−1, π)]
Message2 : ln
←
µBπ,τ →sπ,τ = Eq(sτ+1|π)[lnp (sτ |sτ+1, π)]
Message3 : lnµA→sπ,τ = lnp(oτ |sτ )
Notethestraightforwardrelationbetweenthesemessagesand
Bayes’theorem.AsdepictedinFig.6,message1correspondsto
the prior from the previous time point (denoted by the right-
facingarrow).Message2correspondstopriorinformationfrom
thefuturetimepoint(denotedbytheleft-facingarrow;e.g.,al-
lowing retrospective inference about the state at time point 1
afterreceivinganobservationattimepoint2).Message3cor-
respondstothelikelihoodofanobservationatthecurrenttime
point(noarrownotation;hereassumedtobethesameforall
valuesof π).So,forexample,ifwetaketheedgecorresponding
totheposteriorfor sπ,2(inthemiddleofthegraph),thisposterior
willthencorrespondtointegratingpriors( Bπ,τ −1 andBπ,τ )with
the likelihood (A) and then normalizing to convert back to a
properprobabilitydistribution(i.e.,asinBayes’theorem).When
adoptingthematrixnotationforthesemessages,beliefupdating
canbewrittenas:
sπ,τ = σ
(
lnBπ,τ −1sπ,τ −1+ lnBT
π,τ sπ,τ +1+ lnAToτ
)
(16)
Note thatBπ,τ −1sπ,τ −1 is replaced by the prior over initial
statesD forτ = 1.Asdescribedinthefollowingsection,these
updates (or their marginal message passing counterparts) can
alsobereformulatedintermsofpredictionerrorsthatillustrate
the biological plausibility of this message passing scheme. To
helpthereadergetaconcretesenseofthedynamicsofmessage
passing,wehaveprovidedsomesimpleexamplecodewithinthe
accompanyingMATLABscripts( Message_passing_example.m).
Inferringpoliciescanbethoughtofasmakinguseofananalo-
gousprocess.Althoughitshouldbeemphasizedthatposteriorin-
ferenceoverpoliciesinthecurrentimplementation( spm_MDP_
VB_X_tutorial.m)isnotexplicitlydoneinthismanner.Still,we
willstaywiththemessagepassingnotationfordidacticpurposes.
Recallthat,underactiveinference,policiesareselectedbasedon
their (expected) ability to generate preferred observations and
maximizeinformationgain.Inaddition,recallfromtheprevious
two sections that preferred observations are formally treated
asbeingmoreprobable a priori.Statetransitionsunderapol-
icy can then be seen as more probable if they maximize the
probabilityofcurrentobservations,ln p(oτ |π),andtheexpected
probability of future observations conditioned upon the policy
inquestion,E q(oτ ,sτ |π)[lnp (oτ |π)].Noticethatfutureobservations
are here treated as random variables that need to be inferred
(i.e.,becausetheyhavenotyetbeengiventothemodel).Also
notice the similarity between this and the expression forEFE.
This similarity is no accident, as we shall see shortly (Parr &
Friston,2018a).Inferringthesedistributionsrequiresustoevalu-
atepartitionfunctions ofthenormalfactorgraph.Thismeans
summing over the variables (i.e., probabilities) represented by
theedgesenclosedinthereddottedlinesinFig.7.Thisoper-
ationisalsosometimescalled‘closingthebox’(Loeliger,2004).
For example, to obtain the probability of current observations
conditioneduponpolicies,andthatofexpectedfutureobserva-
tionsconditioneduponpolicies,wemustevaluatethefollowing
summations:
lnp(oτ |π)= ln
∑
s
p(oτ , sτ |π) (17)
Eq(oτ>t ,sτ>t |π)[lnp (oτ>t |π)]= Eq(oτ>t ,sτ>t |π)[ln
∑
s
p (oτ>t , sτ>t |π)]
(18)
As we have seen, however, such summations are often in-
tractable. Instead,weevaluatethefreeenergyfunctionals VFE
andEFE,astheyapproximatetherequiredprobabilities(aswe
sawinSection1)andcanbecomputedefficiently:
− lnp (oτ |π) ≈ Fπ,τ (19)
− Eq(oτ>t ,sτ>t |π)[lnp (oτ>t |π)]≈ Gπ,τ (20)
Theposterioroverpoliciescanthenbecomputedinasimilar
mannerastheposterioroverstates.Specifically,wecanexpress
themessagessentfromthe Bπ,τ −1 andBπ,τ matrixfactornodes,
and theE vector factor node, to the edges representing the
posterioroverpoliciesasfollows(messages1–3showninFig.7).
q (π) ∝ µE→π · ⃗µBπ,τ −1→π ·
←
µBπ,τ →π (21)
Message1 : lnµE→π = lnE
Message2 : ln⃗µBπ,τ −1→π = Fπ,τ
Message3 : ln
←
µBπ,τ →π = Gπ,τ
Hereagain,messagesfrompastandfuturetimepointsarede-
notedwithright-pointingandleft-pointingarrows(respectively),
while the message conveying priors over policies is denoted
withoutarrownotation.Oncethesemessagesarepassed,ifwe
normalizetheresultbyapplyingasoftmaxfunction,wearriveat
anexpressionfortheposterioroverpoliciesthat(suppressingthe
precisionterm γ )correspondstotheequationshowninTable2:
π = σ(lnE − F − G) (22)
It is important to note, however, that unlike the state in-
ference process shown in Fig. 6, there is no need for iterative
messagepassinginthiscase.Asingleroundofmessagepassing
isequivalenttotheequationabove.Thus,whileinferenceover
policiescanbeheuristicallyviewedintermsofmessagepassing
(forillustrativeconsistencywithvariationalmessagepassingin
state inference), it need not be described in this manner (and
therearedifferences;e.g.,themessagesarenotbidirectional).
Atthisjuncture,wereturntothemorerecentdevelopment
ofmarginalmessagepassingthatwasmentionedatthebegin-
ning of this section. Because variational message passing and
themean-fieldapproximationhaveknownlimitations,thisim-
provedalgorithmhasbeenadopted,andisincorporatedintothe
most recent software implementation (spm_MDP_VB_X.m; as
wellasinthetutorialversionincludedas supplementarycode:
spm_MDP_VB_X_tutorial.m).Briefly,marginalmessagepassing
represents a type of compromise between the computational
efficiency of variational message passing and another widely
usedalgorithm–calledbeliefpropagation–thatismorecom-
putationally expensive but can perform exact (as opposed to
approximate)inferenceundersuitableconditions(fordetails,see
Parretal.,2019).Afullexplanationofmarginalmessagepassing
isbeyondthescopeofthistutorial,asitfirstrequiresamore
thoroughintroductiontobothvariationalmessagepassingand
belief propagation. Here, we chose to introduce the reader to
the mean-field approximation in combination with variational
messagepassingduetoitssimplicityandwideusagewithinthe
activeinferenceliterature,andtoprovidetheinterestedreader
withafoundationtobuildfromwhenpursuingmoredetailson
thesetopicselsewhere.
However,themajorresultingadjustmentundermarginalmes-
sagepassingisthattheposterioroverstatesbecomes:
sπ,τ = σ
(
1
2
(
lnBπ,τ −1sπ,τ −1+ lnB†
π,τ sπ,τ +1
)
+ lnAToτ
)
(23)
21
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Asabove,notethat Bπ,τ −1sπ,τ −1 isreplacedbythepriorover
initialstates D forτ = 1.Theresultofaddingthe 1
2 toscale
the influence of transition beliefs (Bπ,τ ) is that the precision
oftransitionprobabilitiesisreduced.Thispreventsoverestima-
tionoftheprecisionofposteriors— somethingthatcanoccur
with variational message passing. Also note thatB†
π,τ denotes
the transpose ofBπ,τ with normalized columns (i.e., columns
thatsumto1).Aspresentedhere,thismodificationmaycome
acrosssomewhatadhoc.However,aswithvariationalmessage
passing,theupdateequationsformarginalmessagepassingcan
bederivedinaprincipledmanner(describedinParretal.,2019).
2.4. Prediction error formulation
One strength of active inference is that it comes equipped
withabiologicallyplausibleinstantiationintermsofprediction-
error minimization. In this subsection, we will introduce the
readertothedifferenttypesofpredictionerrorsdescribedinthe
active inference literature and how their minimization affords
stateinferenceandpolicyselection.Wewillalsoprovideexplicit
numerical examples. By the end of this subsection, the reader
shouldunderstandthebasisofthesepredictionerrors,therole
ofthedifferenttermswithintheirrespectiveequations,andhow
theyrelateto VFE andEFE.
Therearetwotypesofpredictionerrorsdescribedinactive
inference – ‘state’ and ‘outcome’ prediction errors – based on
the equations forFπ and Gπ , respectively (see Table 2). State
predictionerrorsdrivebeliefupdatingwithrespecttostatesand
are based on message passing algorithms. Outcome prediction
errorsdrivepolicyselection.Theyarenotbasedonanexplicit
message passing algorithm, but they illustrate howGπ can be
formulatedwithinthesametypeofbiologicallyplausibleerror-
minimizationscheme.Wewillnowcovereachofthesetypesof
predictionerrorsinturn.
Statepredictionerrorstrackhow Fπ changesovertimeas be-
liefs about states sπ,τ areupdated(i.e.,reductionsin Fπ correspond
toreductionsinstatepredictionerror).Thesepredictionerrors
arebasedonthemarginalmessagepassingschemedescribedin
theprevioustechnicalsection.Forthosewhoskippedthissection,
please briefly review Eq. (23), which corresponds to message
passingbetweenthesquarenodesandedgesinthefactorgraph
foranactiveinferencePOMDP(showninthebottompanelof
Fig.6).Aswenowdescribe,itisthisequationthatcanberefor-
mulatedintermsofastateprediction-errorsignalthatthebrain
seekstominimizeinordertoinferposteriorsoverstates sπ,τ ,
usingthreetypesofmessages(i.e.,message1:ln Bπ,τ −1sπ,τ −1,
message 2: lnB†
π,τ sπ,τ +1, and message 3: lnAToτ ). This is part
of a more general mapping proposed between the variables
included in active inference and both neuronal and synaptic
activity.
In the proposed mapping, firing rates in specific neuronal
populations represent the continually updated posteriors over
statessπ,τ –correspondingtoedgesinthefactorgraphshown
inFig.6orcirclenodesintheBayes’netdepictioninFig.5.Pat-
ternsofsynapticconnectionstrengthsimplementfactors(i.e.,the
squarenodeswithingraphs),suchasthe AandBπ,τ matrices,
thatimplementfunctionsandtransformtheincomingmessages
encoded within firing rates; see Parr and Friston (2018a). To
simulateneuronaldynamics,onecansetupanordinarydifferen-
tialequation,basedonmarginalmessagepassing,thatperforms
a gradient descent onVFE by introducing the state prediction
error(επ,τ )asanauxiliaryvariable.Thispredictionerrorscores
thedifferencebetweenthelogpriorprobabilityofeachhidden
state(i.e.,theposteriorfromtheprevioustimestep)andthelog
probabilityofeachhiddenstatefollowingaroundofmessage
passing (i.e., when a new observation has been received). As
describedintheprevioussection,witheachobservationthere
will be many rounds (iterations) of message passing (i.e., the
message passing equation will be repeated many times) until
posteriorbeliefsoverstatesconvergetoastablevalue.Allofthis
isconditioneduponaspecificpolicy(denotedbythesubscript π),
becausetheagentistryingtoinferthestatesitwilloccupyifit
choosesonepolicyvs.another.Toarriveatempiricalpredictions
about measurable neural responses, we can then substitute in
a‘depolarization’or‘voltage’variable, vπ,τ ,tostandinforthe
log posterior over states;vπ,τ = lnsπ,τ . The resulting state
predictionerrorequationandbeliefupdatingarethenwrittenas
follows.
StatePredictionErrors
επ,τ ← 1
2
(
ln
(
Bπ,τ −1sπ,τ −1
)
+ ln
(
B†
π,τ sπ,τ +1
))
+ lnAToτ
− lnsπ,τ (24)
vπ,τ ← vπ,τ + επ,τ (25)
sπ,τ ← σ(vπ,τ ) (26)
Forthosewhoskippedthetechnicalsection,notethat B†
π,τ in
Eq.(24)denotesthetransposeof Bπ,τ withnormalizedcolumns
(i.e.,columnsthatsumto1).Inthisequation,thecombination
ofthetwo Bmatrices(combinedwithstatebeliefs)correspond
to priors, whereas theAmatrix (combined with observations)
correspondstothelikelihood.Thearrownotationindicatesup-
datestothevalueofavariableateachiteration.Eq.(25)states
thatthechangeinlevelofdepolarization vπ,τ witheachiterative
updatecorrespondstothepredictionerror επ,τ .Notethatthis
error term corresponds to the rate of change inVFE; επ,τ =
− ∂Fπ
∂sπ,τ .Theupdatedvalueof vπ,τ issubsequentlyputthrougha
softmaxfunction( σ)inEq.(26)toreturnanupdatedposterior
distributionoverstates sπ,τ .Thekeyaspectofthissetofupdate
equationsisthatthevalueof sπ,τ continuestochange(i.e.,the
equationsarecontinuallyrepeated)untilthevalueofthestate
prediction error termεπ,τ is minimized. In other words, the
equationsaresetupsuchthattheychangethevalueof sπ,τ (inthe
directionofsteepestdescent)untilthisproducesthelowestvalue
ofεπ,τ ,atwhichpointtheresultingvalueof sπ,τ willcorrespond
toanapproximateposterioroverstates.Thisisbecause επ,τ = 0
istheattractingfixedpoint,meaningthatthesystemtendsto
evolve towardsεπ,τ = 0, and that once it reaches this value
itwillremainthere.Thisleavesuswithabiologicallyplausible
prediction-errorminimizationschemethatcanperformposterior
inferenceoverstatesandcanbeinstantiatedinarelativelysimple
neuralnetwork(seeSection5;formoredetails,seeParrandFris-
ton(2018a).Thatis,byfindingposteriorbeliefsoverstates sπ,τ
(onthefarrightofEq.(24))thatminimize επ,τ ,Fπ isminimized
andsπ,τ becomesastableposteriorbelief.
Asdescribedinmoredetailbelow(andelsewhere;DaCosta,
Parr,Sengupta,&Friston,2021),thevariable vπ,τ isusedtomodel
theaveragevoltageormembranepotentialofaneuralpopulation
(i.e., by taking either positive or negative values), where one
populationisassumedtoencodeinformationabouteachstate
factor (i.e., the probability of each state within that factor for
eachtime τ undereachpolicy).Thestatevariable sπ,τ thencorre-
spondstothefiringratesofthatpopulation,whicharedrivenby
theirmembranepotential.Thisisbecause sπ,τ isthesoftmax( σ)
ofthevoltageandtherefore,similartoafiringrate,takesonly
non-negative values (i.e., between 0 and 1). This follows from
theassumptionmadeinmean-fieldmodelsofneuraldynamics
thattheaveragefiringrateofapopulationcanbetreatedasa
sigmoidfunctionoftheaveragemembranepotential(Breakspear,
2017; Da Costa et al., 2021). Local field potentials (LFPs) and
event-relatedpotentials(ERPs)inelectroencephalography(EEG)
22
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
researcharethenmodeledasthetimederivative(rateofchange)
inthefiringrates sπ,τ .
Tomaketheseequationsmoreconcrete,consideraworked
examplewiththefollowinggenerativemodelentailedbyapolicy,
combinedwithaspecificobservationandinitializedvalueofthe
approximateposterior:
A=
[
.8 .4
.2 .6
]
; Bπ,τ −1 =
[
.9 .2
.1 .8
]
; Bπ,τ =
[
.2 .3
.8 .7
]
;
oτ =
[
1
0
]
; sπ,τ =
[
.5
.5
]
vπ,τ = lnsπ,τ =
[
−.6931
−.6931
]
Ascanbeseenhere,thelikelihood( A)matrixindicatesthat
outcome 1 (row 1) is more likely (i.e.,p = .8) under state 1
(column1).Underthepolicybeingconsidered,theagentbelieves
itwillmostlikelyremaininitscurrentstateinthefirststate
transition(i.e., p = .9and.8onthediagonalin Bπ,τ −1;columns
indicatestatesattime τ − 1,rowsindicatestateattime τ)and
morelikelytomovetostate2duringthesecondstatetransition
(i.e.,p = .8and.7inthebottomrowin Bπ,τ ).Further,theagent
observesoutcome1attime τ (oτ )andhasapriorexpectation
thatbothstatesareequallylikely( sπ,τ andv).Herewewillalso
setsπ,τ −1 = sπ,τ = sπ,τ +1, which is a common initialization
atthebeginningofatrial.However,thevaluesofthesethree
variableswilloftennotremainequalafternewobservationsas
atrialprogresses.Inthiscase,theerrorsignalwillbe:
επ,τ ← 1
2
(
ln
([
.9 .2
.1 .8
][
.5
.5
])
+ ln
([
.475 .525
.525 .475
][
.5
.5
]))
+ ln
([
.8 .2
.4 .6
][
1
0
])
− ln
[
.5
.5
]
= 1
2
(
ln
[
.55
.45
]
+ ln
[
.5
.5
])
+ ln
[
.8
.4
]
− ln
[
.5
.5
]
=
[
−.6455
−.7458
]
+
[
.4700
−.2231
]
=
[
−.1755
−.9690
]
Thiserrorsignalwillthenupdatebeliefsoverstatesthrough
thedepolarizationvariable vπ,τ asfollows:
vπ,τ ← vπ,τ + επ,τ =
[
−.6931
−.6931
]
+
[
−.1755
−.9690
]
=
[
−.8686
−1.6621
]
sπ,τ ← σ
(
vπ,τ
)
=
[
e−.8686
e−.8686+e−1.6621
e−1.6621
e−.8686+e−1.6621
]
=
[
0.6886
0.3114
]
Noticethatinthisexamplethevariationalupdate(i.e.,single
stepofgradientdescent)resultsinanegativevalueforthestate
prediction error term (i.e.,επ,τ ←
[
−.1755
−.9690
]
). As can be
seen,thisshiftstheapproximateposteriorsuchthatitwill better
minimizepredictionerrorinthenextvariationalupdate(i.e.,here
increasingtheprobabilityofoccupyingstate1).
Incontrasttostatepredictionerrors,theoutcomeprediction
errorsmentionedabovetrackhow Gπ changesovertimeas beliefs
about policies areupdated(i.e.,reductionsin Gπ correspondto
reductionsinoutcomepredictionerror).Inotherwords,when
thistypeofpredictionerrorisminimized,policiesareidentified
thatminimizebothuncertaintyoverstates(i.e.,ambiguity)and
the expected difference between predicted and preferred out-
comes.However,asnotedabove,itisimportanttoclarifythat,
unlikestatepredictionerrors,outcomepredictionerrorsarenot
directly tied to the message passing schemes described above
— andthecurrentlyavailableroutinesinSPMforperforming
active inference do not explicitly calculate outcome prediction
errors.Thecurrentimplementationinsteadcalculates Gπ directly.
When this prediction error formulation has been presented in
previousliterature,ithaslargelybeenforillustrativepurposes
withrespecttodemonstrationofbiologicalplausibility(Parr&
Friston,2018a).However,calculatingoutcomepredictionserrors
couldfeasiblybeaddedifonewereinterestedinmodelingthe
associated neuronal responses predicted by this aspect of the
processtheory(foronerecentlyproposedschemeforinferring
policiesthroughmessagepassing,seeChampionetal.(2021).In
contrast,thecurrentroutinesdocalculatestatepredictionerrors,
whichcanbeusedwithoutmodificationforpurposesofempirical
prediction.
Theupdateequationforoutcomepredictionerrorisasfollows.
OutcomePredictionErrors:
ςπ,τ = Asπ,τ ·
(
lnAsπ,τ − lnCτ
)
− diag
(
ATlnA
)
· sπ,τ (27)
Thispredictionerrorisbestunderstoodasamixtureoftwo
types of expected predictions errors. The first term,
Asπ,τ · (lnAsπ,τ − lnCτ ),correspondstotheexpecteddifference
between preferred outcomes (i.e., the probability distribution
encoding preferences over outcomes specified byCτ ) and the
outcomesexpectedunderapolicy(i.e., Asπ,τ correspondstothe
observationsexpectedunderapolicy,o π,τ ).Thiscanthereforebe
thoughtofastheexpectedpredictionerror(undereachpolicy)
withrespecttotheobservationspredictedbypriorpreferences.
Thesecondterm, diag
(
ATlnA
)
· sπ,τ ,correspondstohowmuch
observationsareexpectedtoupdatebeliefsifadoptingaparticu-
larpolicy(i.e.,itistheentropyterm,wherelowerentropyentails
greater information gain). Therefore, as with state prediction
error,minimizingthistermminimizesuncertainty— butinthis
caseitisuncertaintywithrespecttopolicies.Notethatthe diag()
function simply takes the diagonal elements of a matrix and
placestheminarowvector.Notealsothat,unlikewithstate
prediction errors, we have not used the update (←) notation
for outcome prediction errors. This is because, in the current
formulation of active inference, outcome prediction errors are
not iteratively minimized; they are simply computed once for
eachpolicy.Thosewhoreadthetechnicalsectionon VFE andEFE
willrecognizethesetwotermsasthematrixformsoftherisk
(DKL [q(oτ |π)||p(oτ )] ≈ Asπ,τ · (lnAsπ,τ − lnCτ ),andambiguity
(Eq(s|π) [H[p(oτ |sτ )]]≈ −diag
(
ATlnA
)
· sπ,τ )termsin EFE.
Again,tomakethismoreconcreteweprovideaworkedex-
ampleofeachtermundertwopossiblepolicies,assumingthe
followingvariablevalues:
A=
[
.9 .1
.1 .9
]
; Cτ =
[
1
0
]
; sπ=1,τ =
[
.9
.1
]
;
sπ=2,τ =
[
.5
.5
]
Inotherwords,theagentprefersoutcome1(row1),andthe
likelihood(A)matrixindicatesthatstate1(column1)ismore
likelytogenerateoutcome1(i.e., p = .9).Further,statebeliefs
under policy 1 (sπ=1,τ ) entail a higher probability of being in
state1(i.e., p = .9)thanstatebeliefsunderpolicy2( sπ=2,τ ;
23
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
i.e.,p = .5).Wecanfirstcalculatetherisk(reward-seeking)term
withintheoutcomepredictionerrorforeachpolicy:
Policy1:
oπ=1,τ = Asπ=1,τ =
[
.82
.18
]
Asπ=1,τ ·
(
lnAsπ=1,τ − lnCτ
)
=[
.82
.18
]
·
(
ln
[
.82
.18
]
− ln
[
1
0
])
= 2.4086
Policy2:
oπ=2,τ = Asπ=2,τ =
[
.5
.5
]
Asπ=2,τ · (lnAsπ=2,τ − lnCτ )=[
.5
.5
]
·
(
ln
[
.5
.5
]
− ln
[
1
0
])
= 7.3069
Notethatanegligiblysmallnumber(here, e−16)isaddedto
thevaluesin Cτ becauseln(0) isundefined.Asexpected,because
the approximate posterior over states for policy 1 makes the
generation of preferred observations more likely, policy 1 has
lower values for the risk term (i.e., leading to lower outcome
predictionerror,allelsebeingequal).
Movingontotheambiguity(information-seekingterm),con-
sideranotherexamplewiththefollowingvariables:
A=
[
.4 .2
.6 .8
]
; sπ=1,τ =
[
.9
.1
]
; sπ=2,τ =
[
.1
.9
]
Inthiscase,thelikelihood( A)matrixindicatesthatstate2
(column2)hasamoreprecisedistributionthanstate1(column
1).Inotherwords,observationsareexpectedtoprovidemore
precise information about states when in state 2. As such, we
expectoutcomepredictionerrorstodriveselectionofthepolicy
that will lead the agent toward state 2. In this case, policy 2
assignsahigherprobabilitytostate2(i.e., = .9inrow2).Wecan
confirmthisbycalculatingtheambiguitytermforeachpolicyas
follows:
Policy1:
− diag
(
ATlnA
)
· sπ=1,τ = −diag
([
.4 .6
.2 .8
]
ln
[
.4 .2
.6 .8
])
·
[
.9
.1
]
= −diag
([
−.67 −.78
−.59 −.50
])
·
[
.9
.1
]
=
[
−.67
−.50
]
·
[
.9
.1
]
= −(−.66) = .66
Policy2:
− diag
(
ATlnA
)
· sπ=2,τ = −diag
([
.4 .6
.2 .8
]
ln
[
.4 .2
.6 .8
])
·
[
.1
.9
]
= −diag
([
−.67 −.78
−.59 −.50
])
·
[
.1
.9
]
=
[
−.67
−.50
]
·
[
.1
.9
]
= −(−.52) = .52
Asexpected,becausetheoutcomesgeneratedbystate1are
more ambiguous (i.e., less informative), and policy 2 assigns
a higher probability to state 2 than policy 1, policy 2 better
minimizesambiguity.
It is important to stress that the risk and ambiguity terms
foroutcomepredictionerrorsworksynergistically,andoneoften
haspoliciesthatminimizebothriskandambiguity.Ascanbe
seenintheoutcomepredictionerrorequationabove,subtracting
the ambiguity term from the risk term corresponds to adding
(i.e., note the double negative) the entropy of the likelihood
mappingunderapolicytotheriskofthepolicy,whichwehave
calculatedseparatelyhere.Thisdrivesselectionofpoliciesthat
maximizebothreward-andinformation-seekingbyminimizing
theoverallresultingerror.
Whiletheseexamplecalculationsmayappearsomewhatin-
volved(evenforasinglepolicy),anintuitivewaytothinkabout
thesetwopredictionerrorsisthatminimizingstateprediction
errormaximizesconfidenceinposteriorbeliefs,whileminimiz-
ing outcome prediction error maximizes confidence in how to
achieve goals or desires. To reproduce the worked examples
aboveandallowthereadertocalculatestateandoutcomepredic-
tionerrorsunderdifferentmodelparameters,wehaveprovided
thePrediction_error_example.mscript in thesupplementary
code.
3. Buildingspecifictaskmodels
3.1. Explore-exploit task
TomakethestructureofaPOMDPmoreconcrete,inthisand
subsequentsectionswewillbuildmodelsofspecificbehavioral
taskscommonlyusedinempiricalstudies.Thiswillprovidethe
readerwiththenecessarytoolstobuildtheirownmodelsand
use them in both simulation work and empirical studies. This
willalsoallowustoconcretelydemonstratesomeoftheunique
resources offered by active inference when modeling behavior
inasimplereinforcementlearningcontext.Tobesure,inmany
task contexts (e.g., when there is no uncertainty about states)
activeinferencemodelscanperformsimilarlytoreinforcement
learningmodels,andtheydonotalwaysgenerateoptimalbe-
havior (Da Costa, Sajid, Parr, Friston, & Smith, 2020; Markovic
etal.,2021). 6 However,asdiscussedabove,whentasksinvolve
various types of uncertainty (e.g., about task condition or re-
ward probabilities), active inference offers a unique approach
formodelinginformation-seekingbehaviorthatcanleadtosu-
periorperformanceinsomecases(Markovicetal.,2021;Sajid,
Ball,Parr, &Friston, 2021). Anotherresource offeredbyactive
inference, even when it performs similarly to other types of
models, is its associated neural process theory (i.e., describing
howneuralsignalingmightimplementvariationalormarginal
messagepassing;seeSection5).Thetaskmodelswebuildinthis
tutorialwillfurtherallowustoillustratehowactiveinference
canbeusedtomaketestableempiricalpredictionsaboutneural
responses.Aswewillsee,becauseactiveinferencemodelsinte-
grateperception,learning,anddecision-makingwithinasingle
model architecture, this affords the generation of predictions
aboutneuralresponsesacrossawiderangeofperceptualtasks
inadditiontoreinforcementlearninganddecision-makingtasks.
Inthissubsection,wewillbuildamodeloftheexplore–exploit
task briefly described in the previous section. Every step we
outlineinthissectionforbuildingtheexplore–exploittaskmodel
islaidoutintheaccompanyingMATLABcode( Step_by_Step_AI_
Guide.m).Thiscodeisincludedinthe supplementarycode files
6 Although note that, even in task contexts where they perform similarly,
active inference models and reinforcement learning models make decisions
in a different way. Specifically, while reinforcement learning models seek to
maximize a reward signal, active inference models instead seek to reach a
target distribution that is treated as rewarding (i.e., the distribution encoding
theagent’spreferredobservations).
24
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
andcanalsobefoundat:https://github.com/rssmith33/Active-
Inference-Tutorial-Scripts.Whilegoingthroughthissection,we
encouragethereadertoworkthroughthiscodeinparallel.Here
wewillusenon-bolditalicswhenpresentingthegeneralmathe-
maticalnotationandsubsequentlyshowtheassociatedMATLAB
syntaxinbold.
Inthebeginningoftheexplore–exploittask,theparticipant
istoldthatoneachtrialonemachinewilltendtopayoutmore
often,buttheywillnotknowwhichone.Theyarealsotoldthat
thebettermachinewillnotalwaysbethesameoneachtrial.
Theycanchoosetoselectonerightawayandpossiblywin$4.
Ortheycanchoosetopressabuttonthatgivesthemahintabout
whichslotmachineisbetteronthattrial.However,iftheychoose
totakethehint,theycanonlywin$2iftheypickthecorrect
machine.Overmanytrials,theparticipantcanlearnwhichslot
machinetendstopayoutmoreoftenandeithermakesafeor
riskychoices(i.e.,takethehintornot).
Tomodelthistask,itcanbehelpfultostartbyspecifyingthe
setsofpossiblehiddenstates(statefactors).Inthiscase,onestate
factorcorrespondstowhethertheleftorrightslotmachineis
more likely to win (‘left-better context’, ’right-better context’).
The second state factor corresponds to the choice state (‘start
state’,‘askingforthehint’,‘choosingtheleftmachine’,‘choosing
therightmachine’).MovingtoMATLABcode,wecansettheseup
byspecifyingthepriorsoverinitialstateswithasetofvectors D,
withonevectorperstatefactor(i.e.,wherethefactornumberis
specifiedinbrackets).Thegeneralstructureforthesevectorsis:
p
(
s
factor
τ=1
)
=
D {statefactor} (state, 1)= [vector]
Inthiscase,wespecify:
p
(
scontext
τ=1
)
=
D {1} = [0.5 0.5 ]′
p
(
schoice
τ=1
)
=
D {2} = [1 0 0 0 ]′
Notethat,tomatchMATLABsyntax,weusetheapostrophe
(‘)toindicateatranspose.Thissaysthattheparticipantbegins
with the belief that the ‘left-better’ and ‘right-better’ contexts
haveequalprobability(leftandrightentries,respectively),and
withafullyprecisebeliefthathe/shewillstartthetrialinthe
startstate(fromlefttoright:‘start’,‘gethint’,‘chooseleft’,and
‘chooseright’states).
Itisimportanttobrieflynote,however,thatthingschange
slightlyifwewishtosimulatelearningasopposedtojustinfer-
ence,becauseweneedtoseparatethegenerativeprocessfrom
thegenerativemodel.Inthiscase,capital D standsforinitialstate
probabilitiesinthegenerativeprocess,whilelowercase d stands
for the initial state priors in the generative model (which are
learned).Forexample,onecouldspecify D{1} = [1 0 ]’and
d{1} = [.5 .5 ]’,whichwouldmeanthetruecontextis‘left-
better’buttheagentbelieveseachcontextisequallylikely.In
thesupplementarycode accompanyingthissection,wedothis
asawayofcontrollingwhichcontextwewanttosimulate.The
samecapitalvs.lowercaseletterconventionholdsforallother
matrices/vectorsusedhere.Wereturntothisinthesectionon
learningfurtherbelow(Section4).
Movingforward,wemustnextspecifythe(inthiscasethree)
setsofpossibleobservations(outcomemodalities).Here,thefirst
setofobservationscorrespondstothehint(‘nohint’,‘machine-
left hint’, ‘machine-right hint’). The second set of observations
corresponds to decision outcomes (‘start’, ‘lose’, and ‘win’). Fi-
nally,theparticipantalsoobservestheirownbehavior;namely,
the observed action (‘start’, ‘asking for the hint’, ‘choosing the
left machine’, ‘choosing the right machine’). This last outcome
modalitycanbeimportantinactiveinferencemodelsbecause
choice states must be inferred just like any other state. Ob-
servingtheirownbehaviorthereforeallowsaparticipanttobe
moreconfidentaboutwhethertheirintendedactionshavebeen
successfully carried out. In MATLAB, we can set these up by
specifyingthelikelihood( A)matrices.Therewillalwaysbeone
Amatrixforeachoutcomemodality. Rowscorrespondtoout-
comes, columns correspond to the states in the first state
factor,andthereis anadditionaldimensionforeachadditional
statefactor (notethat,asmentionedinTable1,the Amatrices
aremorecorrectlyreferredtoas tensors iftheyincludemorethan
twodimensions).Thegeneralstructureistherefore:
p
(
omodality
τ |sfactor
τ
)
=
A{outcomemodality} (outcome, factor1, factor2,
. . . ,factorN) = [matrix]
Inthiscase,onlythe‘gethint’state(state2)instatefactor
2 generates a hint observation, so for the third dimension we
specifya‘2’asfollows:
p
(
ohint
τ |scontext, choice=get hint
τ
)
=
A{1} (: , : , 2)=
[ 0 0
1 0
0 1
]
Here,columnsfromlefttorightcorrespondtothe‘left-better’
and‘right-better’states,whilerowsfromtoptobottomcorre-
spond to the ‘no hint’, ‘machine-left hint’, ‘machine-right hint’
observations.Thismatrixindicatesthatthehintisaccuratewith
aprobabilityof1(100%accuracy).Forexample,a‘machine-left
hint’observation(rowtwo)willbegeneratedbythe‘left-better
context’(columnone)withprobability = 1.Hereeachcolumn
mustaddupto1.
Fortheotherdimensionsofstatefactor2(i.e.,matrixdimen-
sion3):
p
(
ohint
τ |scontext, choice=start, choose left, choose right
τ
)
=
for i = 1, 3, 4:
A{1} (: , : , i)=
[ 1 1
0 0
0 0
]
end
This indicates that all other choice states will generate the
‘nohint’observation(i.e.,ahintwillneverbeobservedinthose
states).
For the second outcome modality, the ‘start’ and ‘get hint’
statesgenerate‘start’observations(row1):
p
(
owin
τ |scontext, choice=start, get hint
τ
)
=
for i = 1, 2
A{2} (: , : , i)=
[ 1 1
0 0
0 0
]
end
Wewillnowspecifytheprobabilityofwinninginthe‘left-
bettercontext’vs.the‘right-bettercontext’dependingonchoice
state.First,wespecifythatchoosingtheleftmachine(i.e.,tran-
sitioningtothe‘chooseleftmachine’state;state3infactor2)
willleadtoawin80%ofthetime(row3)ifinthe‘left-better
context’(column1)andleadtoawin20%ofthetimeifinthe
‘right-bettercontext’(column2),withinverseprobabilitiesfora
loss(row2),andaprobabilityof0ofcontinuingtoobservethe
25
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
‘start’observation(row1):
p
(
owin
τ |scontext, choice=choose left
τ
)
=
A{2} (: , : , 3)=
[ 0 0
.2 .8
.8 .2
]
We will then specify that the probabilities of winning are
reversedifchoosingtherightmachine(i.e.,transitioningtothe
‘chooserightmachine’state;state4infactor2):
p
(
owin
τ |scontext, choice=choose right
τ
)
=
A{2} (: , : , 4)=
[ 0 0
.8 .2
.2 .8
]
Remember,thefirstcolumnisthecontextwheretheleftma-
chineisbetter,andthesecondcolumnisthecontextwherethe
rightmachineisbetter.Itisthethirddimensionthatcorresponds
tochoicestates(inthiscase,choicestate3and4,corresponding
tochoosingtheleftvs.rightmachine).
Finally,forthethirdobservationmodality(observedaction),
statessimplymap1-to −1tooutcomes(acrossallotherstate
combinations):
p
(
oobserved action
τ |scontext, choice=start
τ
)
=
A{3} (: , : , 1)=
⎡
⎢⎣
1 1
0 0
0 0
0 0
⎤
⎥⎦
p
(
oobserved action
τ |scontext, choice=get hint
τ
)
=
A{3} (: , : , 2)=
⎡
⎢⎣
0 0
1 1
0 0
0 0
⎤
⎥⎦
p
(
oobserved action
τ |scontext, choice=choose left
τ
)
=
A{3} (: , : , 3)=
⎡
⎢⎣
0 0
0 0
1 1
0 0
⎤
⎥⎦
p
(
oobserved action
τ |scontext, choice=choose right
τ
)
=
A{3} (: , : , 4)=
⎡
⎢⎣
0 0
0 0
0 0
1 1
⎤
⎥⎦
Thissimplyallowstheindividualtoinferwhattheirchoice
waswithcompletecertainty(rowstoptobottom:‘start’,‘asking
for the hint’, ‘choosing the left machine’, ‘choosing the right
machine’observations).
Nowthatwehavethelikelihood,thenextstepistospecifythe
(policy-dependent)statetransition( Bπ,τ )matrices.Thegeneral
structureforthesematricesis:
p
(
s
factor
τ+1 |sfactor
τ , U
)
=
B{statefactor} (stateattime τ + 1, stateattime τ, actionnumber)
= [matrix]
Herethevector U containsindicesspecifyingthe actionnum-
berassignedtoeachmatrix,whereeachpolicy π subsequently
specifiesasequenceoftheseactionnumbers(describedfurther
below).
Becausewehavetwostatefactors,weneedtwosetsofmatri-
ces.Thefirstmatrixisforthecontextfactor.Inthiscase,because
acontextremainsthesamewithineachtrial,thisissimplyan
identitymatrixthatsays statesattime τ (columns)remainthe
sameatτ + 1(rows):
p
(
scontext
τ+1 |scontext
τ , U
)
=
B{1} (: , : , 1)=
[
1 0
0 1
]
Columnsfromlefttorightandrowsfromtoptobottomboth
correspondtothe‘left-better’and‘right-better’states.Thereis
only one ‘action’ (possible transition from each state) for this
factor, so the third dimension is a 1 and has a length of 1;
i.e.,thereisnoB {1} (: , : , 2).
Incontrast,thesecondstatefactorischoicestate,wherefour
differenttransitions(actions)arepossibleateachtimestep.In
thiscase,eachmatrixbelowindicatesthatonecouldmovefrom
anystatetothechosenstate:
p
(
schoice
τ+1 |schoice
τ , U = start
)
=
B{2} (: , : , 1)=
⎡
⎢⎣
1 1 1 1
0 0 0 0
0 0 0 0
0 0 0 0
⎤
⎥⎦
p
(
schoice
τ+1 |schoice
τ , U = get hint
)
=
B{2} (: , : , 2)=
⎡
⎢⎣
0 0 0 0
1 1 1 1
0 0 0 0
0 0 0 0
⎤
⎥⎦
p
(
schoice
τ+1 |schoice
τ , U = choose left
)
=
B{2} (: , : , 3)=
⎡
⎢⎣
0 0 0 0
0 0 0 0
1 1 1 1
0 0 0 0
⎤
⎥⎦
p
(
schoice
τ+1 |schoice
τ , U = choose right
)
=
B{2} (: , : , 4)=
⎡
⎢⎣
0 0 0 0
0 0 0 0
0 0 0 0
1 1 1 1
⎤
⎥⎦
Inotherwords,action1,B {2} (: , : , 1),entailsmovingtothe
‘start’statefromanyotherstate,action2,B {2} (: , : , 2),entails
movingtothe‘gethint’statefromanyotherstate,andsoforth.
Thethirddimensionlabelstheseas actionnumbers1,2,3,and
4.
Next, we need to specify preferences over each set of out-
comes(C),withonematrixperoutcomemodality.Here, rows
indicate observations(same order as in the correspondingA
matrices)and columnsindicatetimepoints inatrialfromleft
toright.Inotherwords:
Cmodality =
C{outcomemodality} (outcome, timepoint) = [matrix]
Inthiscase,themodelhasnodirectpreferenceforgetting
a hint (i.e., preferences for outcome modality 1:C {1}) or for
observingthechoiceofaparticularaction(i.e.,preferencesfor
outcomemodality3: C {3}).So:
Chint =
C{1} =
[ 0 0 0
0 0 0
0 0 0
]
26
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Cobserved action =
C{3} =
⎡
⎢⎣
0 0 0
0 0 0
0 0 0
0 0 0
⎤
⎥⎦
Here,columnsfromlefttorightindicate τ = 1, 2, 3inthe
trial.Themodeldoeshavepreferencesforwinningandlosing
(outcomemodality2),whicharespecifiedasfollows:
Cwin =
C{2} =
[ 0 0 0
0 −1 −1
0 4 2
]
This indicates that, at the second and third time points, a
win(row3)hasavalueof4and2,respectively.Remember,the
valueislessatthethirdtimepoint(thirdcolumn)becausethe
individualwinslessmoneyiftheyinsteadchoosetotakethehint
atthesecondtimepoint(i.e.,andthuscontinuestoobservethe
‘start’outcomeinrow1forthisoutcomemodality).Thevalues
of−1inrow2indicateapreferenceagainstobservingalossat
timepoints2and3.
Notethatweonlyinitiallyspecifythe Cmatrixvaluesinthis
formforconvenience.Thesepreferencedistributionsarepassed
through a softmax (normalized exponential) function (σ) such
thateachcolumn(i.e.,thepreferencedistributionforeachtime
τ) in theC matrix encodes a proper probability distribution
ofnon-negativevaluesthatsumsto1,atwhichpointanatu-
ral log is applied. This means that the values are transformed
intolog-probabilitiesasfollows(i.e.,lessnegativeindicatesmore
preferred):
lnp
(
omodality
τ |Cmodality
τ
)
= ln(σ(Cmodality
τ ))
Forexample,inthecaseofthepreferencesfor Cwin specified
above:
lnp
(
owin
τ |Cwin
τ
)
=
ln(σ(C{2}))=
[ −1.1 −4.0 −2.1
−1.1 −5.0 −3.2
−1.1 −0.02 −0.2
]
To be clear, in the line immediately above, the softmax is
applied to each column separately (i.e., corresponding to the
preferencedistributionoveroutcomesateachtimepoint).
Next,weneedtospecifyallowablepolicies.Therearethree
time points in a trial for this task, which means a policy will
consistoftwoactions.Ifwewanttoinclude ‘shallow’policies,
wherethemodelonlylooksonestepahead,weneedtospecify
asetofvectors U thatindexeachaction(asalreadyreferredto
above). Technically, this set of vectors is specified as a matrix
includingonerow, onecolumnforeachallowableaction ,and a
thirddimensionspecifyingeachstatefactor .Thus,thestructure
is:
Ufactor =
U(1, actionnumber, statefactor)= [vector]
Inthiscase,wecanincludeallactions:
Ucontext =
U(: , : , 1)= [1 1 1 1 ]
Uchoice =
U(: , : , 2)= [1 2 3 4 ]
Entriesfor U(: , : , 2)allowallfourpossibletransitions(ac-
tions)betweenchoicestates(factor2)ateachtimepoint.The
entries forU(: , : , 1) are all ones because there is only one
possible‘action’–thatis,onetransitionmatrix Bπ,τ –forstate
factor1.Therestillneedstobefouroneswithin U(: , : , 1)to
matchthenumberofactionsin U(: , : , 2).Inotherwords,each
overallactionoptioncorrespondstothecombinedentriesina
givencolumnforbothstatefactors. 7
Ifweinsteadwanttoinclude ‘deep’policies,wherethesimu-
latedparticipantplansaheaduntiltheendofthetrial,thismeans
thatweneedtospecify onecolumnforeachallowablepolicy
(witheachentryindicatinganactionnumber) inamatrix V,
withonerowpertimepoint anda thirddimensionspecifying
eachstatefactor .Thus,thegeneralstructureis:
πfactor =
V(timepoint, policy, statefactor)= [matrix]
Inthiscase,wemightreasonablyincludefivepolicies:
πcontext =
V(: , : , 1)=
[
1 1 1 1 1
1 1 1 1 1
]
πchoice =
V(: , : , 2)=
[
1 2 2 3 4
1 3 4 1 1
]
Aswith U,allpoliciesheredonotchangestatefactor1at
eithertimepoint(hence,allentriesareones 8).Forstatefactor
2, we have included policies in which the model chooses to
remaininthe‘startstate’(i.e.,choosingaction1twice;column
1),choosestotakethehint(action2)andthenselecteitherof
theslotmachines(i.e.,actions3or4;columns2–3),ordecides
tochooseaslotmachinerightaway(columns4–5;note,these
policiessubsequentlyreturntostate1,sinceitisnotpossible
to win twice in one trial). We will use deep policies in the
simulationsbelow.
Ifsodesired,onecanalsospecifyafixedprioroverpolicies E
toincorporateabiasor‘habit’toselectsomepoliciesoverothers.
Thisissimplya columnvectorwithoneentryperpolicy that
encodestheprobabilityofthatpolicy.Herewewillnotinclude
suchabias,whichmeansthat E willsimplybeaflatdistribution
overour5allowablepoliciesin V:
p(π)=
E = [1
5
1
5
1
5
1
5
1
5 ]′
Finally,thereareseveralscalar(single-value)parametersone
can set. One parameter is beta (β), which is the prior on the
expectedfreeenergyprecisionterm γ discussedabove(which
encodestheprecisionestimatefortheexpectedfreeenergyover
policies).Alow β value(around1)indicateshighexpectedpreci-
sion,whereashighervalues(e.g.,3,5,10)indicatelowerexpected
precision.Higher β valueswillincreaserandomnessinpolicyse-
lectionandalsomakepolicyselectionmoreinfluencedbyhabits
encodedinthe E vector(foranexampleofthesedynamics,see
Smith,Khalsa,&Paulus,2021b.Thisfollowsfromthefactthatit
implieslessconfidencethatmodelbeliefswillgeneratepreferred
outcomes (Hesp et al., 2020). Another parameter is alpha (α),
whichisastandard‘inversetemperature’(or‘actionprecision’)
parameterthatcontrolsrandomness(e.g.,motorstochasticity)in
7 Although not shown in detail here, this also affords the possibility of
multidimensionalpoliciesinmorecomplexmodels.Forexample,iftherewere
multiplepossibleactions(transitionmatrices)fortwodifferentstatefactors,one
mightspecifythataction2forstatefactor1canbechosentogetherwithaction
2forstatefactor2,butthataction2forstatefactor1cannotbechosentogether
withaction3forstatefactor2(simplybyincludingacolumnthathasentries
of2forbothstatefactorsbutnocolumnthathasanentryof2forstatefactor
1andanentryof3forstatefactor2).
8 However, note that they need not all be ones in a more complex model
withmultidimensionalpolicies,asalsodescribedfor U inthepreviousfootnote.
27
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
actionselectionunderachosenpolicy(highervaluesindicateless
randomness;typicalrangeisbetweenaround1–32,butvery
highvaluescanbechosentoremovechoicestochasticity).
p (Action|α) = σ(α × lnp(Action|π)) (28)
Here,sigma( σ)indicatesasoftmaxfunctionthattransforms
thequantityontherightintoaproperprobabilitydistribution
that sums to 1 (see Appendix A for more detail). Bothβ and
α must be positive numbers. Here we will makeβ = 1 and
α = 32, specifying reasonable amounts of indeterminacy in
actionselection.
3.2. Running and plotting simulations
We have now specified a generative model and are ready
to run single-trial simulations. To do so in MATLAB, we will
assigneachofourvariablestoastructurecalledmdp(forMarkov
decision process). Concretely, this means assigningmdp.D =
D, mdp.V = V, mdp.beta = beta, and so forth for all the
scalars,vectors,andmatricesconstructedabove.Wecanthenrun
thisstructurethroughthestandardactiveinferenceestimation
functionspm_MDP_VB_X.m(availableintheDEMtoolboxofthe
mostrecentversionsofSPMacademicsoftware:http://www.fil.
ion.ucl.ac.uk/spm/).Thisjustmeansentering:
MDP= spm_MDP_VB_X(mdp)
However,becauseSPMsoftwareisoftenupdated,weinclude
aspecificversionforthistutorial.Sohereyoushouldrun:
MDP= spm_MDP_VB_X_tutorial(mdp)
This function will simulate behavior based on an POMDP
structure (i.e., it runs the equations in Table 2), and the out-
put MDP (capital letters) structure will contain the simulation
results. As we have specified it here, it assumes the gener-
ative process and generative model are identical (see section
on learning below where we remove this assumption). It will
thus generate outcomes based on the generative process and
simulatethesubsequentinferenceanddecisiondynamicswithin
thegenerativemodelwhenobservingthoseoutcomes.Because
theabove-mentionedsimulationscriptisquitecomplex,wealso
directreadersinterestedinthedetailsofhowthebeliefupdating
schemeisimplementedtothe Simplified_simulation_script.m
scriptincludedinthe supplementarycode, whichisastripped
down(butheavilycommented)versionofthestandardmodel
inversion scheme used inspm_MDP_VB_X.m. For clarity, this
additionaltutorialscriptinvertsthesamegenerativemodelof
theexplore–exploittaskintroducedabove.
Single-trialbehaviorcanbeplottedwithsomedefaultplotting
routines. The primary single-trial plotting routine available in
SPMcanberuninMATLABbyentering:
spm_figure(′GetWin′,′ Figure1′); clf; spm_MDP_VB_trial(MDP);
subplot(3, 2, 3)
Thisplottingroutinecanalsotakeadditionaloptionalinputs:
spm_MDP_VB_trial(MDP, Gf, Gg).
Gf:statefactorstoplot.
Gg:outcomemodalitiestoplot.
Forexample, spm_MDP_VB_trial(MDP, 1: 2, 2: 3)wouldplot
the first two state factors and the second and third outcome
modalities.
At this point, the reader is encouraged to set the variable
Sim inthefirstsectionoftheaccompanyingtutorialcode(i.e.,
Step_by_Step_AI_Guide.m,line51)to Sim = 1andthenclick
‘Run’,whichwillrunthemodelandthisplottingscript.Before
runningthisscript,remembertomakesureSPM12isinstalled
andthatthe‘DEM’folderwithintheSPMfolderstructureisadded
asapathinMATLAB(...spm12\toolbox\DEM).
Basedonthecurrentmodelspecification,arepresentativeplot
ofsimulationresultsisshowninFig.8A.Thisandsimilarplots
aregeneratedfromspecificoutputfieldsintheMDPstructure
(Table 3 describes each output field). The two panels in the
top-leftofFig.8Ashowposteriorsoverstates at the end of the
trial (i.e.,thestatesthemodelbelievesitwasinateachtime
pointτ whenatthelasttimepoint t).Heretimegoesfromleft
toright,darkerindicateshigherprobability,andthecyandots
denotethetruestates.Here,themodelbelievesitwasinthe‘left-
bettercontext’andthatitchosetotakethehintandthenchose
theleftslotmachine.Thetop-rightshowstheactionprobabilities
andtrueactions.Heretheagentishighlyconfidentthattaking
the hint and choosing the left machine were the best choices
(andcyandotsindicatethatthesewerealsotheactualactions
taken).Theleft-middlepaneljustshowsthedifferentpossible
two-stepaction-sequencesspecifiedinthemodel(fromleftto
right).Notethatlightershadesinthispaneljustindicatehigher
actionnumbers(e.g.,action1isblack,action2isdarkgray,etc.).
Theright-middlepanelshowstheevolutionoftheposteriordis-
tributionoverpoliciesovertime(fromlefttoright).Here,itcan
beseenthatatthesecondtimepointthemodelbecamehighly
confident in policy 2 (i.e., the ‘take the hint and then choose
theleftslotmachine’policy).Thethreepanelsinthebottom-left
showtheoutcomes(cyandots)andpreferencedistributions.The
first‘hint’modalityshowsthatthemodelreceivedthehintat
timepoint2.Theplotisgraybecausethereisnopreferencefor
oneobservationoverothers.Thisisalsothecaseforthethird
‘observed action’ plot, which simply confirms what the model
chose. The second ‘win/lose’ modality shows that a win was
observedatthethirdtimepoint.Thepreferencedistributionhere
indicatesthestrongpreferenceforthewinattimepoints2and
3(darkervalue),andapreferencenottolose(lightervalue).The
‘null’(starting)outcomeinthetoprowisanintermediategray
attimepoint2(nopreferencefororagainstthisoutcome);the
distributionbecomesdarkergrayatthethirdtimepointbecause
thevalueofthewinattimepoint3wasrelativelyless(i.e.,$2
vs.$4)andsotheoveralldistributionoveroutcomesatthethird
timepointislessprecise.
Asmentionedearlier,however,optimalinformation-seeking
(in the sense of maximizing preferred outcomes in the long-
run)dependsonhavingtherightbalanceofrewardvalueand
informationvalue.Toillustratethis,Fig.8Bshowssimulations
inwhichthemagnitudeofthepreferencedistributionforawin
hasbeenmultipliedby2:C {2} (3, : ) = [084].Ascanbeseen
there, the model instead decided to make a guess right away
aboutwhichmachinewillwin(inthiscase,choosingright)and
unfortunately observed a loss (bottom-left, middle sub-panel).
Ascanbeseenintheupperright,itsconfidenceintheleftvs.
right action is equal (equally gray over each). As can be seen
intheupperleft,themodel’sposterioroverstatesshowshigh
confidencethatitwasinfactinthe‘left-better’contextatevery
timepoint,because(retrospectively)thiswasmostlikelythecase
ifitlostafterchoosingthemachineontheright.
Finally,thebottom-rightplotsinthesepanelsdisplaypredic-
tions about dopamine responses in the neural process theory,
whichwehavenotyetdiscussed.Theseresponsesarebasedon
changes in confidence in expected free energy estimates after
receivingnewobservations(i.e.,theupdatestotheexpectedfree
energyprecisionparameter γ ;explainedinthebottomrowof
Table2).Inthiscase,thelarge‘dopaminespike’shownatthe
secondtimestepisbecause EFE atthattimestepfavoredpolicy
2(i.e.,takingthehintandthenchoosingtheleftmachine)and
28
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig. 8.Example simulation plots that can be generated with the code included in this tutorial. A detailed walk-through is provided in the main text.Panel A
Examplesimulationofarisk-averseagentperformingtheexplore–exploittask.Theagenttakesthehint,thenchoosestheleftslotmachine,andobservesawin.
PanelB: Examplesimulationofarisk-seekingagentperformingtheexplore–exploittask.Theagentforegoesthehintandimmediatelychoosestherightslotmachine,
observesaloss,andthenreturnstothestartstate. PanelC: Exampleneuronalsimulationsbasedontherisk-averseagentin PanelA.Thisillustratestheneuronal
firingrates,localfieldpotentials,anddopamineresponsespredictedbytheneuralprocesstheory.Forallpanels,notethatdarkercolorsindicatehigherprobability
valuesforbeliefsaboutstates,actions,andpoliciesovertime.Foroutcomes,darkervaluesindicatestrongerpriorpreferences.The‘allowablepolicies’plotsinthe
firsttwopanelssimplydisplaytheactionsequencescorrespondingtoeachpolicy(darkerindicateslowernumbers,whereeachnumberdenotesanavailableaction).
Thedopamineresponseplots(lower-rightplotsineachpanel)correspondtoupdatesintheexpectedprecisionofthe EFE distributionoverpolicies( γ );cyanlines
indicateγ values while black spikes correspond to their rate of change. In the upper- and lower-left plots (‘contexts’ and ‘firing rates’) of panelC, each column
(movinglefttorightalongthex-axis)correspondstobeliefsaboutcontextstates at thetimewhenanobservationwasreceived( t),whilerowsfromtoptobottom
onthe y-axiscorrespondtothetimepointforwhichbeliefsareupdated(tau; τ).Forexample,thetop-rightquadrantcorrespondstobeliefsattime t = 3about
timeτ = 1(notethat,unfortunately,thisstandardSPMplottingroutineinappropriatelylabelseachrowwith tsinsteadof τs).Firingrates(upper-right)correspond
tothemagnitudeofposteriorsovereachstate(inthiscase,thestatesinthe‘context’statefactor),whilelocalfieldpotentials(middle-right)correspondtotheir
rateofchange(inbothcases,thereisonelineplottedforeachrowintheplotsintheupper-andlower-left).Seemaintextforinterpretationoftimefrequency
response plots and their motivation. These simulations can be reproduced by running theSim = 1 option in the supplementaryStep_by_Step_AI_Guide.mcode
(althoughnotethat,becauseoutcomesaresampledfromprobabilitydistributions,resultswillnotbeidenticaleachtime)
VFE atthattimestep(basedonobservations)providedsupport
forpolicies2and3(i.e.,afterobservationsatthesecondtime
step,onlythetwopoliciesthatincludedtakingthehintremained
plausible). Because the policy favored byEFE was supported,
theprecisionestimatefor EFE increased(correspondingtothe
positivedopaminespike).
As a numerical example to help offer an intuition for how
theseupdatesoperate,wecanplugthe VFE andEFE valuesat
timepoint2inthissimulationintothepolicydistributionand
precisionupdateequationsshowninTable2.Intheseequations,
γ istheexpectedfreeenergyprecisionterm, β0istheinitialprior
forthisprecisionatthestartofatrial( γ = 1/β),and β isthe
posteriorvaluethatiscontinuouslyupdatedovertimebyaterm
welabel βupdate.Thistermtechnicallyreflectsthegradientoffree
energywithrespectto γ (∇γ F)andisinformedbyavaluescoring
the level of (dis)agreement between expected free energy and
observed(variational)freeenergyaftermakinganewobservation
— whichcanbethoughtofasatypeofpredictionerror( Gerror ;
withproposedassociationswithemotion;see(Hespetal.,2020).
Forthesakeofillustration,wecanset γ ,β0,and β equaltoone
andspecifythedistributionsoverpoliciesasfollows 9:
E =
[
1 1 1 1 1
]T
G ≈
[
12.505 9.51 12.5034 12.505 12.505
]T
F ≈
[
17.0207 1.7321 1.7321 17.0387 17.0387
]T
Ascanbeseenhere,theagenthasnohabit-likepriorexpecta-
tionsoverpolicies(i.e.,the E distributionisflat),andtheexpected
freeenergyoverpolicies( G)favorspolicy2(i.e.,entry2hasthe
lowestvalue).Thevariationalfreeenergyafteranewobservation
(F)providespreciseevidenceforpolicies2and3(valuesmuch
closerto0,indicatingthatthenewobservationisinconsistent
withpolicies1,4,and5).Giventhissetup,oneroundofiterative
updatingwouldbe:
π0 ← σ (lnE − γ G)
=
[
0.0417 0.8332 0.0418 0.0417 0.0417
]T
π ← σ (lnE − F − γ G) =
[
0 .9523 .0477 0 0
]T
9 Note that thespm_MDP_VB_X.m script (and the tutorial version here)
works with negative free energies, and so theseF and G values are made
negativeintheMDPoutputstructureinMATLAB.
29
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table3
Outputfieldsforspm_MDP_VB_X_tutorial.msimulationscript.
MDPField ModelElement Structure Description
MDP.F Negativevariationalfreeenergyofeachpolicy
overtime.
Rows= policies.
Columns= timepoints.
Negativevariationalfreeenergyofeachpolicy
ateachtimepointinthetrial.Forexample,if
thereare2policiesand6timepointsthere
willbea2 × 6matrixcontainingthenegative
variationalfreeenergyofeachpolicyateach
pointinthetrial.
MDP.G Negativeexpectedfreeenergyofeachpolicy
overtime.
Rows= policies.
Columns= timepoints.
Negativeexpectedfreeenergyofeachpolicy
ateachtimepointinthetrial.Forexample,if
thereare2policiesand6timepointsthere
willbea2 × 6matrixcontainingthenegative
expectedfreeenergyofeachpolicyateach
pointinthetrial.
MDP.H Totalnegativevariationalfreeenergyover
time.
Columns= timepoints. Totalnegativevariationalfreeenergyaveraged
acrossstatesandpoliciesateachtimepoint.
Forexample,ifthereare8timepointsthere
willbea1 × 8rowvectorcontainingthe
totalnegativefreeenergyateachtimepoint.
MDP.Fa
MDP.Fd
MDP.Fb
...
MDP.Faisthenegativefreeenergyof
parameter‘a’(iflearning Amatrix).Thereare
alsoanalogousfieldsiflearningother
matrices/vectors(e.g.,MDP.Fdforlearningthe
parametersofthe D vector,etc.).
Columns= oneperoutcomemodalityor
hiddenstatefactor(i.e.,dependingonthe
specificparametersbeinglearned).Iftheagent
islearningparametersofasinglevector(e.g.,
E),thiswillbeasinglecolumn.
KLdivergencebetweentheparametersofthe
matrix/vectorthatisbeinglearnedatthe
beginningofeachtrialandattheendofeach
trial.Eachcolumninthevectormayrepresent
anoutcomemodality(i.e.,inthecaseofthe A
matrix),ahiddenstatefactor(i.e.,inthecase
ofthe Bmatrixand D vector),oranyother
vector(e.g.,the E vector).
MDP.O Outcomevectors. Rows = outcomemodalities.
Columns= timepoints.
Vectors(onepercell)specifyingtheoutcomes
foreachmodalityateachtimepoint.Observed
outcomesareencodedas1s,with0s
otherwise.
MDP.P Probabilityofemittinganaction. Rows = onepercontrollablestatefactor.
Columns= actions.
Thirddimension = timepoint.
Theprobabilityofemittingeachparticular
action,expressedasasoftmaxfunctionofa
vectorcontainingtheprobabilityofeach
actionsummedovereachpolicy.Forexample,
assumethattherearetwopossibleactions,
withaposterioroverpoliciesof[.4.4.2],with
policy1and2leadingtoaction1,andpolicy
3leadingtoaction2.Theprobabilityofaction
1and2istherefore[.8.2].Thisvectoristhen
passedthroughanothersoftmaxfunction
controlledbytheinversetemperature
parameterα,whichbydefaultisextremely
large(α = 512).Actionsarethensampled
fromtheresultingdistribution,wherehigher α
valuespromotemoredeterministicaction
selection(i.e.,bychoosingtheactionwiththe
highestprobability).
MDP.Q Posteriorsoverstatesundereachpolicyatthe
endofthetrial.
1cellperstatefactor.
Rows= states.
Columns= timepoints.
Thirddimension = policynumber.
Posteriorprobabilityofeachstateconditioned
oneachpolicyattheendofthetrialafter
successiveroundsofupdatingateachtime
point.
MDP.R Posteriorsoverpolicies. Rows = policies.
Columns= timepoints.
Posterioroverpoliciesateachtimepoint.
MDP.X Overallposteriorsoverstatesattheendofthe
trial.TheseareBayesianmodelaveragesofthe
posteriorsoverstatesundereachpolicy.
1cellperstatefactor.
Rows= states.
Columns= timepoints.
Thismeanstakingaweightedaverageofthe
posteriorsoverstatesundereachpolicy,
wheretheweightingisdeterminedbythe
posteriorprobabilityofeachpolicy.
MDP.un Neuronalencodingofpolicies. 1cellperpolicydimension.
Rows= policies.
Columns= iterationsofmessagepassing(16
pertimepoint).Forexample,16iterations,
and8timepointsgivesavectorwith128
columns).
Simulatedneuronalencodingoftheposterior
probabilityofeachpolicyateachiterationof
messagepassing.
MDP.vn Neuronalencodingofstatepredictionerrors. 1cellperstatefactor.
Rows= iterationsofmessagepassing(16per
timepoint).
Columns= states.
ThirdDimension:timepointthebeliefis about
(τ).
FourthDimension:timepointthebeliefis at
(t).
Bayesianmodelaverageofstateprediction
errorsateachiterationofmessagepassing
(weightedbytheposteriorprobabilityofthe
associatedpolicies).
(continued on next page)
30
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Table3 (continued).
MDPField ModelElement Structure Description
MDP.xn Neuronalencodingofhiddenstates. 1cellperstatefactor.
Rows= iterationsofmessagepassing(16per
timepoint).
Columns= states.
ThirdDimension:timepointthebeliefis about
(τ).
FourthDimension:timepointthebeliefis at
(t).
Bayesianmodelaverageofnormalizedfiring
rates,whichreflectposteriorsoverstatesat
eachiterationofmessagepassing(weighted
bytheposteriorprobabilityoftheassociated
policies).
MDP.wn Neuronalencodingoftonicdopamine,
reflectingthecurrentvalueof γ .
Rows= numberofiterativeupdates(16per
timepoint).Forexample,ifthereweretwo
timepointsinatrialthiswouldbe1column
with32rows.
Thisreflectsthevalueoftheexpected
precisionoftheexpectedfreeenergyover
policies(γ )ateachiterationofupdating.
MDP.dn Neuronalencodingofphasicdopamine
responses,reflectingtherateofchangein γ .
Rows= numberofiterativeupdates(16per
timepoint).Forexample,ifthereweretwo
timepointsinatrialthiswouldbe1column
with32rows.
Thisvariablereflectstherateofchangeinthe
expectedprecisionofexpectedfreeenergy
overpolicies( γ )ateachiterationofupdating.
MDP.rt Simulatedreactiontimes. Columns = timepoints. Computationtime(i.e.,timetoconvergence)
foreachroundofmessagepassingandaction
selection.
Gerror ← (π − π0) · (−G)= .3567
βupdate ← β − β0+ Gerror = .3567
β ← β − βupdate/ψ = 1− .3567/2= .8216
γ ← 1
β = 1.2171
Here we have included astep size parameterof ψ = 2,
whichreducesthemagnitudeofeachupdateandpromotesstable
convergence.Notethat,whilewehavehereshownanexample
of a single round of updating, there will be many rounds of
updatingtoconvergenceforeachnewobservation.Noticethat
the increase in the probability of policy 2 and 3 between the
priorandposterioroverpoliciesisdrivenby F,whichscoresthe
evidenceaffordedeachpolicygivencurrentobservations.Policy
2 and 3 better minimizeF and are therefore more plausible.
Taking the dot product between the vector encoding the dif-
ferencebetweenthepriorandposterioroverpolicies( π − π0)
andthe −G vectorisequivalenttoscalingeachelementofthe
differencevectorbytheassociated G valueandthensumming
theresults,whichinthiscasecreatesapositiveupdate.Thisis
because the difference vector is pointing in roughly the same
directionasthe −G vector. Thisisapparentinthat G initially
indicated the highest probability for policy 2, andF also pro-
vided evidence for policy 2. As a result, the prediction error
(Gerror )ispositiveandtheupdated γ valueincreasestheimpact
of G on the posterior over policies (i.e., because the agent is
now more confident in its beliefs aboutG). In contrast, if the
policiesfavoredby G werenotsupportedby F (asinFig.8B),
the Gerror term would be negative and the updated value of
γ would decrease the impact ofG on the posterior over poli-
cies, as the agent has lost confidence in its beliefs aboutG.
For a derivation of these update equations, see Appendix in
(Salesetal.,2019).
Note that the cyan line in the dopamine plot corresponds
to the stable expected free energy precision value (γ ; with a
hypothesizedlinktotonicdopaminelevels),asopposedtothe
rateofchangeinthisprecision(inblack;withahypothesizedlink
tophasicdopamineresponses).Tohelpthereadergainabetter
intuitionforthedynamicsoftheseupdates,wehaveprovided
supplementarycode( EFE_Precision_Updating.m),whichallows
thereadertospecifythenumberofpolicies,thevaluesforthe
vectorsE,F,and G,andthevalueof β0,andthensimulatethese
updates.Fig.9alsoillustratesahelpfulgeometricinterpretation
ofthefactorsthatdeterminethedirectionof β updates.Namely,
whenthedifferencevector( π − π0)andthe −G vectorpointin
asimilardirection(i.e.,anangleoflessthan90 ◦ apart),thedot
productofthetwowillresultinanincreasein γ .Thefactthat
thesevectorspointinthesamedirectionisawaytovisualize
hownewobservations(through F)provideevidencesupporting
thereliabilityof G,andthereforeincreaseitsprecisionweight-
ing.Incontrast,whenthesevectorspointindifferentdirections
(i.e.,theangleseparatingthemisgreaterthan90 ◦),thissuggests
that G is less reliable; its precision (γ ) is therefore reduced
anditcontributeslesstotheposteriordistributionoverpolicies
(π).
Now that we have gone through an example of these dy-
namics, an important question concerns the settings in which
they may be useful. Here, it is important to highlight thatE,
F,and β/γ canbeviewedasoptionalelements(e.g.,theyare
notincorporatedinthepolicyselectionmodelwithintheupper
right portion of Fig. 5, or in other examples of active infer-
ence(DaCosta,Parretal.,2020)).Forexample,incorporating E
maynotbeusefulunlessmodelingataskinwhichyoususpect
thatparticipantsenterastudywithaparticularchoicebiasor
thathabitualchoicebehaviorcouldbelearnedovertime.Incor-
porating F and β/γ updating is only useful in the context of
deep policies. As mentioned elsewhere,β/γ can optimize the
relativeinfluenceofgoalsandhabits( E andG).Amongothers,
onebenefitofincorporating F emergeswhentherearealarge
numberofdeeppoliciestochoosefrom.Thisisbecauseitallows
observations to render some policies highly implausible early
in a trial, which has the effect of narrowing the search space
fortheoptimalpolicy.Thistypicallyworksinconjunctionwith
an‘Occam’swindow’parameterthatremovespoliciesfromthe
searchspaceiftheirprobabilitybecomestoolowrelativetothe
mostprobablepolicy(thisparameterisspecifiedas mdp.zetaand
explainedinmoredetailwithintheaccompanyingtutorialcode
Step_by_Step_Hierarchical_Model.m.
4. Modelinglearning
Inthissectionwewilldiscusshowlearningisimplemented
inactiveinferenceandhow thiscanbeusedtomodelmulti-
trialbehavioraldata.Asaconcreteexample,wewillreturnto
theexplore–exploittaskmodelandallowtheagenttolearnprior
beliefs(i.e.,withinthevector D)abouthowoftentheleftslot
machineandrightslotmachinetendtopayout.Moregenerally,
wewilldiscusshowanysetofmodelparametervalues–such
asthoseencodingdistributionswithinthelikelihood( A),tran-
sitionbeliefs( Bπ,τ ),orpriorsoverpolicies( E)–canbelearned
31
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.9. Illustrationofageometricinterpretationofthefactorscontributingtoupdatesinexpectedfreeenergyprecisionestimates( γ ).Intheseexamplesweinclude
twopoliciesandspecifythevaluesofpriorsoverpolicies( E),expectedfreeenergyoverpolicies( G),andthevariationalfreeenergyoverpoliciesfollowinganew
observation (F). Based on these, a prior and posterior over policies (π0 and π) are computed (calculations shown in each of the top panels). Updates are then
computed forβ, whereγ = 1/β, constrained by a fixed prior valueβ0 and a step size parameter (ψ) that promotes convergence to a stable posterior across
iterations.Thedirectionandmagnitudeoftheresultingupdatein γ isdrivenbythedotproductbetweenthedifferencevectorforpriorandposteriorpolicybeliefs
(π − π0)andthe −G vector.Thisdotproductcanbethoughtofasapredictionerror( Gerror )reflectingthelevelof(dis)agreementbetweenexpectedfreeenergy
andthevariationalfreeenergyofasubsequentobservation.Theplotsinthetoppanelsshowcaseswhere γ ispositivelyupdated(left)andnegativelyupdated
(right).Inthecaseontheleft,thetwovectors( π − π0 and−G)pointinasimilardirection(i.e.,lessthan90 ◦ apart),whichrepresentsawaytovisualizehownew
observations(through F)provideevidenceforthereliabilityof G (leadingtoanincreaseinitsprecisionweighting γ ).Inthecaseontheright,thevectorsaregreater
than90◦ apart,providingevidenceagainstthereliabilityof G (leadingtoadecreaseinitsprecisionweighting γ ).Notethat,forreasonsofclarity,theendpointsof
thevectorsshownherearenottheactualvaluesof π − π0 and−G;theyinsteadcorrespondtoscaledvaluesofthesevectors,whichmakesthemsimilarinlength
andmoreclearlyillustratestheangleseparatingthem.Themiddlepanelsshow16iterationsof γ updating,asisdonepertimepoint(i.e.,observation)inatrialin
thesupplementarycode (andinthestandardSPMroutines)untilastableposteriorestimateisreached.Thisissimilartopredictionerrorminimizationdynamics
forthestateandoutcomepredictionserrorsdescribedearlier.Thebottompanelsshowtherateofchangein γ ,whichbearssomesimilaritytopredictionerror
responses.Theupdatesshowninthisfigurehavebeenassociatedwithdopamineintheneuralprocesstheoryaccompanyingactiveinference.Thesesimulationscan
bereproducedusingthe EFE_Precision_Updating.mcodeprovidedinthe supplementarycode.
overrepeatedtrials.Thisisbasedonupdatingpriorbeliefsover
theseparameterswithinaclassofdistributionscalled Dirichlet
distributions. We first provide a technical introduction to the
generalmathematicalfoundationsofDirichletdistributions.Then
wediscusslearninginmoreintuitiveterms,providenumerical
examples,anddemonstratehowtorunsimulationsinpractice.
4.1. Technical introduction to Dirichlet priors (optional)
Inthissubsectionweprovideatechnicalintroductiontothe
Dirichletdistributionusedtoimplementlearninginactiveinfer-
ence.Aftercompletingthissection,thereadershouldhavean
understandingofhowtheparametersinaDirichletdistribution
can: (1) act as priors on the categorical distributions used in
thePOMDPmodelscoveredabove,and(2)beupdatedbasedon
posteriorbeliefsattheendofatrial.Forreaderslessinterestedin
thesetechnicaldetails,thissectioncanbeskipped.Asmentioned
above,wewillprovideamoreintuitiveconceptualintroduction
in the next subsection. We encourage all readers to consider
theformaldetailsbelow,butacompleteunderstandingofthis
subsectionwillnotberequiredtofollowsubsequentsections.
Learning in active inference is formulated in terms of a
Dirichlet-categorical model. Specifically, Bayesian inference is
performedusingacategoricaldistribution(whichwasdiscussed
32
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
earlier)asthelikelihood,andaDirichletdistributionastheprior.
TheDirichletdistributionisadistributiondefinedoveravector
ofvaluesthatsitontheinterval [0, 1],andsumto1.Thatis,the
valuesofthevectorhavethesamepropertiesasaprobability
distribution.Assuch,theDirichletdistributionisoftendescribed
asa distribution over a distribution.Inthiscase,itcanbeused
to encode beliefs about model parameters (e.g., confidence in
parametersinthelikelihoodortransitionmatricesofaPOMDP).
TheDirichletdistributionisusedasthepriorovertheparam-
etersofthecategoricaldistributionbecauseitistheconjugate
priorforthecategoricaldistribution.Thismeansthatifwemul-
tiply a categorical distribution by a Dirichlet distribution, and
thennormalizetoobtaintheposteriordistributionoverthepa-
rametersofthecategoricaldistribution,weendupwithanother
Dirichlet distribution — allowing it to be used as a prior in
thenextroundofinference.Thisallowsactiveinferenceagents
tosequentiallyupdatetheirbeliefsaboutmodelparametersas
theyreceivenewobservations.TheDirichletdistribution,denoted
Dir (θ|α),isdefinedasfollows:
p (θ|α) = Dir (θ|α) = Γ (
∑K
k=1αk)
∏K
k=1Γ (αk)
K∏
k=1
θαk−1
k (29)
Where
Γ (
∑K
k=1αk)
∏K
k=1Γ (αk)
isanormalizationconstantthatensuresthe
distributionsumsto1,and Γ denotesthegammafunction(fora
briefintroductiontothegammafunction,seeAppendixA).The
variableθ = (θ1, . . . , θK ) isavectoroflength K containingthe
parametersofacategoricaldistribution,and α = (α1, . . . , αK )
isthesetof concentrationparameters oftheDirichletdistri-
bution, which satisfy the condition thatαk > 0. The gamma
functionisusedinthenormalizationconstanttoaccountforthe
combinatoricsofdrawingarandomvariablefromacategorical
distribution.Thatis,itcountsthenumberofwaysinwhichwe
canplacethevariable α inoneof K mutuallyexclusivestates.
Similarly,thecategoricaldistributionisdefinedas:
p (x|θ) = Cat (x|θ) = 1
x1!x2! . . .xK !
K∏
k=1
θ
xk
k (30)
Wherex = (x1, . . . ,xK )isacategoricalvariablethatoccupies
oneof K mutuallyexclusivestates(e.g., x =
[
0 1 0
]T
).Here
θ = (θ1, . . . , θK ) aretheparametersofthedistributionandsatisfy
theconditions θk ≥ 0,and
∑
k θk = 1.Theterm 1
x1!x2!...xK ! isthe
normalizationconstant.
IfwemultiplytheDirichletandcategoricaldistributionsto
arrive at the posterior distribution over the parametersθ =
(θ1, . . . , θK ) ofthecategoricaldistribution,weobtainthefollow-
ing(ignoringthenormalizationconstantforthesakeofbrevity):
p (θ|x, α) = Dir (θ|x, α + x) ∝
K∏
k=1
θαk+xk−1
k (31)
Noticethatthishasexactlythesameformasthepriordefined
above,exceptthatwehaveaddeda‘count’(i.e., xk)of1tothe
concentrationparameterscorrespondingtotheobservedvariable,
while a ‘count’ of 0 is added to those corresponding to the
non-observedvariables.Itistheconcentrationparametersofthe
DirichletdistributionsinthePOMDPstructurethatareupdated
duringlearning.Theexactwaytheyareupdateddependsonthe
modelelementinquestion(e.g., AorBmatrix,or D vector)which
wediscussinmoreintuitivetermsbelow.Foramoredetailed
introductiontotheDirichlet-categoricalmodel,seeTu(2014).
4.2. Non-technical continuation on Dirichlet priors
Inthissubsectionwewillintroducelearninginmoreconcrete
andintuitiveterms.Thiswillbuildonwhatwaspresentedinthe
previoustechnicalsection,butitdoesnotrequireanunderstand-
ingofthedetailspresentedthere.Attheendofthissubsection,
readersshouldhaveapracticalunderstandingofwhatchangesin
amodelduringlearningandwhatcausesthesechangestooccur.
Although the form of the Dirichlet distribution shown in the
previoussectioncanseemcomplex,theresultinglearningprocess
turnsouttobequiteintuitive.Essentially,itjustinvolves adding
countstoavectorormatrixbasedonposteriorbeliefs,where
largernumbersofcountsindicatehigherconfidence.Toillustrate
this,wewillfirstconsideraDirichletdistributionforinitialstate
priors over two possible states. As is standard notation in the
active inference literature, we represent Dirichlet distributions
withthelowercaselettersassociatedwitheachvectorormatrix.
Forexample,wewilldenotetheDirichlet( Dir)distributionfor
theinitialstateprior D asd.Expressedformally:
p (D) = Dir(d) (32)
d = p(sτ=1)=
[
d1 d2
]T
(33)
Here,the concentrationparameters forD –denotedbylow-
ercased = d1, d2–aretheindividualparametersthatwillchange
during learning. In other words, the process of adding counts
mentionedabovewillapplytothevaluesofthesevariables.This
isbasedonthefollowinglearningequation:
dtrial+1 = ω × dtrial + η × sτ=1 (34)
Theeta( η)parameterisa learningrate (scalarfrom0–1),
which controls how much the values ind change after each
trial.Theomega( ω)parameterisa forgettingrate (scalarfrom
0–1),whichinfluenceshowquicklylearninginrecenttrialcan
‘overwrite’thechangesin d thatoccurredinearliertrials.We
willreturntothesebelow(fornowwewillassumetheyareboth
equalto1).
Togetanintuitionforhowthis‘learningbycounting’process
works,consideracasewhereyoustartoutwithaninitialstate
priorof d = [0.50.5]T onthefirsttrial,andyourposteriorbelief
attheendofthattrialisthatyouwereinstate1(withprobability
= 1).Inthiscase,yourprioronthesecondtrialwouldbecome
d = [1.50.5]T.Inotherwords,acountof1wasaddedtothefirst
entry(i.e.,theentryforstate1).Ifthishappened3moretimes,
thenitwouldbecome d = [4.50.5]T.Incasesofuncertainty,you
insteadadd proportions ofcounts.Forexample,ifyoustartout
withaninitialstatepriorof d = [11]T onthefirsttrial,and
yourposteriorbeliefattheendofthetrialwas s = [0.70.3]T,
thenyourpriorwouldbeupdatedonthesecondtrialtobe d =
[1.71.3]T. During within-trial inference, these distributions are
putthroughasoftmaxfunctionsothattheyretaintheirsame
shapebutagainaddupto1.However,largernumbersindicate
greaterconfidenceintheshapeofthedistribution.Thiscanbe
seen by comparingd = [11]T to d = [5050]T. While both
distributionshavethesame(inthiscaseflat)shape,itwouldtake
many more(new)observationstomeaningfullychangetheshape
of the second distribution compared to the first. For example,
afteronefurthertrialwithapreciseposterioroverstate1,the
resultingshapeofthedistribution d = [21]T haschangedquite
a bit more thand = [5150]T. This is an important aspect of
activelearningbecauseitmeansthattheinitial(prior)counts
determinehow‘open’anagentistonewexperience.Typically,in
anovelenvironmentortask,theinitialcountsaresettoverylow
values(e.g.,.25)–sothatexperiencehasasubstantialeffecton
anagent’spriorbeliefs.Thismakesinferenceandplanningmore
context-sensitive,asopposedtoanagentwithhighinitialcounts
33
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
whois‘stuckinitsways’andwouldrequiremuchmoreevidence
to‘changeitsmind’.
As mentioned above, the agent also has a learning rateη.
Thiscontrolshowquicklyitgets‘stuckinitsways’duringlearn-
ing(thisalsoinfluenceshowquicklytheagentceasestoselect
information-seeking policies; see below for more details). For
example,ifη = 0.5,thenanupdatefrom d = [11]Tafterinferring
state1wouldnotleadto d = [21]T asshownabove.Instead,it
wouldbe d = 1× [11]T + 0.5× [10]T = [1.51]T.Thus,counts
(andhenceconfidence)willincreasemoreslowlyaftereachtrial.
Asalsomentionedabove,learningcanbefurthermodulated
(multiplied)byaforgettingrate( ω).Thisparametercontrolshow
strongly recent experience is able to ‘overwrite’ what one has
learned in the more distant past. A value ofω = 1 indicates
noforgetting(i.e.,recentexperienceisunabletooverwritewhat
has been learned previously), while values less than 1 allow
increasinglevelsofforgetting(essentially,witheachnewobser-
vationtheagentbecomeslessconfidentinwhatithaspreviously
learned). This is important because, as counts increase during
learning, an agent’s beliefs can become rigid and resistant to
change,whichissuboptimalinchangingenvironments.Higher
counts also reduce information-seeking, because the agent is
highlyconfidentinitsbeliefs(describedinmoredetailbelow),
whichfurtherhinderstheopportunitytolearnthat(andhow)
theenvironmenthaschanged.Assuch,ifanagentbelievesthat
theenvironmentisvolatile(e.g.,thattheprobabilitiesofrewards
undereachpolicycanchangeovertime),thenitisappropriate
toadoptalowvaluefor ω (i.e.,ahighforgettingrate).Inother
words,alowvaluefor ω canbeunderstoodasencodinganagent’s
priorbeliefthatthecontingenciesintheworldareunstable.It
is worth noting that inferences about changes in the environ-
ment (and about the volatility of the environment) could also
beimplementedinamoreprincipledmannerinahierarchical
model(oneexampleofamodelwithdynamicallyupdatedbeliefs
aboutenvironmentalvolatilityistheHierarchicalGaussianFilter;
(Mathysetal.,2014).However,includingthesimplerforgetting
rateparameterdescribedherecouldbesufficientformodeling
taskbehaviorinmanycases.
To get an intuition for how this works, glance back at the
equationforlearningattheendoftheprevioussection(Eq.(34))
andthenconsideracasewhere d = [5050]T andη = 1.If ω = 1,
andtheagentinfersthatitisinstate2,thentheupdatewillbe
d = 1× [5050]T+ 1× [01]T = [5051]T.Incontrast,if ω = 0.1,
thentheupdatewillbe d = 0.1×[5050]T+1×[01]T = [56]T.In
thislattercase,theagentthereforebecomesmuchlessconfident
in its prior beliefs, and the shape of the posterior (Dirichlet)
distributionischangedtoagreaterdegreeattheendofthattrial.
Asaslightlymorecomplexexampleoflearning,theupdates
foranexample Amatrixbecome:
p (A) = Dir(a) (35)
a = p(oτ |sτ )=
[ a1 a2
a3 a4
a5 a6
]
(36)
atrial+1 = ω × atrial + η ×
∑
τ
oτ ⊗ sτ (37)
Here,theconcentrationparametersfor A–denotedbylower-
casea = a1.., a6 –aretheindividualparametersinthematrixto
beupdated.The ⊗ symbolindicatestheouterproduct.Thisagain
justinvolvesaccumulating(proportionsof)counts,modulatedby
a learning rate and a forgetting rate. But in this case, what is
beingcountedare coincidences betweenstatesandobservations.
Forexample,assumeyouhaveaposterioroverstatesof s = [10]T
andyoumadetheobservationassociatedwithrow1, o = [100]T.
Becauseyoubelievedyouwereinstate1whenyouobservedout-
come1,thisindicatesthattheirassociationin a shouldincrease.
Thatis,acountshouldbeaddedto a1(i.e.,theintersectionofstate
1andoutcome1)beforethesubsequenttrial.Ifyouinsteadhave
aposterioroverstatesof s = [0.70.3]T andyoumadetheobser-
vationassociatedwithrow2, o = [010]T,thisindicatesthattheir
associationin a shouldincreaseproportionally.Thatis,updatesof
a3+0.7and a4+0.3shouldoccurbeforethenexttrial.Analogous
updaterulesapplytotheothermodelparameters( B, C, E).This
generaltypeof‘coincidencedetection’learningisanalogousto
Hebbiansynapticplasticity,whereneuronswithcoincidentfiring
ratesincreasetheirsynapticconnectionstrengths(Brownetal.,
2010). As discussed further below, the neural process theory
associatedwithactiveinferenceproposesthateachconcentration
parameter can therefore be associated with the strength of a
synapticconnection.
Anotherimportantchangewhenlearningisincorporatedis
that the expected free energy gains an extra term, depending
onwhichparameterisbeinglearned.Thisisbecauselearningis
alsobasedonminimizingexpectedfreeenergy.Forexample,if
learningA,theequationforexpectedfreeenergybecomes:
Gπ = DKL [q (oτ |π) ||p (oτ )]+ Eq(sτ |π)[H[p (oτ |sτ )]]
− Ep(oτ |sτ )q(sτ |π)[DKL [q (A|oτ , sτ ) ||q (A)]] (38)
≈
∑
τ
(
Asπ,τ ·
(
lnAsπ,τ − lnCτ
)
− diag
(
ATlnA
)
· sπ,τ − Asπ,τ · Wsπ,τ
)
(39)
W:= 1
2
(a
⨀(−1) − a
⨀(−1)
sums ) (40)
Notethatthe := symboljustmeansthattwothingsarede-
finedtobeequivalent.The
⨀
symbolindicatestheelement-wise
power(i.e.,separatelyraisingeachelementinamatrixtothe
powerofsomenumber).Theterm asums isamatrixofthesame
sizeas a whereeachentrywithinacolumncorrespondstothe
sum of the values of the associated column ina. For exam-
ple,if a =
[
.25 1
.75 3
]
, thenasums =
[
.25+ .75 1+ 3
.25+ .75 1+ 3
]
=
[
1 4
1 4
]
.
Althoughthisupdatedequationfor EFE mayappearcomplex,
it simply adds one additional term — often called the ‘nov-
elty’term(i.e.,E p(oτ |sτ )q(sτ |π)[DKL [q(A|oτ , sτ )||q (A)]] inEq.(38),or
Asπ,τ · Wsπ,τ inthematrixformulationinEq.(39)).Thisterm
scores how much beliefs within theAmatrix are expected to
changeafterreceivinganewobservation.Becausethenovelty
termisapositivevalue(andsubtractedfromthetotalvalue),
thisentailsthatminimizingexpectedfreeenergywillnowalso
driveinformation-seekingaboutparametervaluesinthe Amatrix
(i.e.,asopposedtosimplyseekingoutinformationaboutstates).
In other words, the agent will also seek out observations to
increaseconfidenceinitsbeliefsabout p(oτ |sτ ).Todothis,the
agentwillseekoutstate-observationpairingsthatwillmaximize
thedifferenceinconcentrationparametersbetweenposteriorand
priordistributionsover A.Thisdifferencequantifiesthedriveor
epistemicaffordanceoffindingout‘whatwouldhappenifIdo
that?’. Although we do not show them explicitly here, similar
termscanalsobeaddedtothe EFE iftheagentislearningany
oftheothermatricesorvectorsinthemodel(e.g.,learningthe
transitionprobabilitiesin Bπ,τ ).
Notethatthevalueofthenoveltytermisinverselyrelated
toconcentrationparametervalues.Whentheconcentrationpa-
rameters have large values, this term will have a small value,
andwhentheconcentrationparametershavesmallvalues,this
termwillhavealargevalue.Therefore,whentheconcentration
34
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
parametervaluesarehigh(i.e.,noveltyislow),themodelwill
becomeprimarilyreward-seeking,asitwillbehighlyconfident
initsbeliefs.Incontrast,theagentwillbeinformation-seeking
when concentration parameter values are low. Analogous dy-
namicsoccurwhenupdatingconcentrationparametersforother
modelparameters.
Tomakethismoreconcrete,weshowaworkedexampleof
thenoveltytermfortwo Amatrices.Onewithsmallconcentra-
tionparametervalues(i.e.,lowconfidenceinbeliefsaboutthe
outcomesgeneratedbyhiddenstates),andtheotherwithlarge
concentrationparametervalues(i.e.,highconfidenceinbeliefs
abouttheoutcomesgeneratedbyhiddenstates).
Smallconcentrationparametervalues(lowconfidence)
a =
[
.25 1
.75 1
]
; A= σ (a) =
[
.25 .5
.75 .5
]
; sπ,τ =
[
.9
.1
]
asums =
[
.25+ .75 1+ 1
.25+ .75 1+ 1
]
=
[
1 2
1 2
]
;
Asπ,τ =
[
.275
.725
]
W:= 1
2
(a
⨀(−1) − a
⨀(−1)
sums )
W= 1
2
([
.25−1 1−1
.75−1 1−1
]
−
[
1−1 2−1
1−1 2−1
])
= 1
2
([
4 1
1.3333 1
]
−
[
1 .5
1 .5
])
= 1
2
([
3 .5
.3333 .5
])
=
[
1.5 .25
.1667 .25
]
Wsπ,τ =
[
1.5 .25
.167 .25
][
.9
.1
]
=
[
1.375
.175
]
Novelty = Asπ,τ · Wsπ,τ =
[
.275
.725
]
·
[
1.375
.175
]
= .505
Largeconcentrationparametervalues(highconfidence):
a =
[
25 100
75 100
]
; A= σ (a) =
[
.25 .5
.75 .5
]
; sπ,τ =
[
.9
.1
]
asums =
[
25+ 75 100+ 100
25+ 75 100+ 100
]
=
[
100 200
100 200
]
;
Asπ,τ =
[
.275
.725
]
W:= 1
2
(a
⨀(−1) − a
⨀(−1)
sums )
W= 1
2
([
25−1 100−1
75−1 100−1
]
−
[
100−1 200−1
100−1 200−1
])
= 1
2
([
.04 .01
.0133 .01
]
−
[
.01 .005
.01 .005
])
= 1
2
([
.03 .005
.0033 .005
])
=
[
.015 .0025
.0017 .0025
]
Wsπ,τ =
[
.015 .0025
.00167 .0025
][
.9
.1
]
=
[
.01375
.00175
]
Novelty = Asπ,τ · Wsπ,τ =
[
.275
.725
]
·
[
.01375
.00175
]
= .00505
Inbothexamples,thepolicyassignshighprobabilitytooccu-
pyingstate1( p = .9).Thenormalizedshapeofthedistribution
foreachcolumnin Aisalsothesameinbothexamples.However,
thenoveltytermislargerinthefirstexamplewheretheassoci-
atedDirichletprior a hassmallerconcentrationparametervalues
(which,whensubtractedfromthetotal,willleadtoalower EFE).
Thismeanstheagentwilllearnmore(i.e.,changeitsbeliefsmore)
whenmovingtostateswhereitislessconfidentinitsbeliefs
(encoded as smallerconcentration parameter values). Toget a
moreintuitivesenseforthesecomputations,youcanreproduce
theseresultsandadjusttheconcentrationparametervaluesin
thesupplementaryscript EFE_learning_novelty_term.m.
4.3. Simulating learning
Inthissubsectionwewillbuildontheexplore–exploittask
modelwespecifiedaboveanddemonstratehowitcanalsobe
usedtosimulatelearning.Bytheendofthissection,thereader
shouldbeequippedtorunthesesimulationsindependently,and
toplotandinterprettheirresults.Withtheexplore–exploittask
modelinplace,weonlyneedafewadditionstothecode.First,
a lowercase version of the to-be-learned model element must
be created. For example, to enable learning within theAma-
trix,onemustspecifyan mdp.awiththesamedimensionsas
mdp.A. The same goes for other parameters (mdp.b, mdp.d,
etc.).Typically,theinitialconcentrationparameterswouldbeset
tolow-confidence(i.e.,low-magnitude),flatdistributionsbefore
learningbegins;forexample: Dir (d) = d {1} = [.25.25]′.Note
herethatthegenerativeprocesscontinuestocorrespondtothe
capital-lettermatrices(i.e.,whichwillgeneratethepatternsof
observations),whilethelowercase-lettermatricesarenowthe
generative model. Next, we can specify a learning rate and a
forgetting rate by settingmdp.eta and mdp.omega equal to
valuesbetween0and1.Finally,weneedtoreplicatethe mdp
structuretoincludemanytrials;forexample,usingcodesuchas:
N_Trials = 30;
[mdp(1: N_Trials)] =deal(mdp);
Then, we can simply run themdp structure through the
spm_MDP_VB_X_tutorial.m function as before. Here, we will
simulatetwodifferentversionsofthetask.Inthefirstversion,
there are 30 trials, and the better slot machine is the same
foralltrials(leftmachine).Wewillallowthesimulatedagent
tolearnpriorexpectationsaboutwhichcontextismorelikely
(i.e.,whethertheleftorrightmachinetendstoleadtowinsmore
often).Toenablethistypeoflearning,wewillinclude mdp.d.
Asshowninthe Step_by_Step_AI_Guide.mcode,wespecifylow
confidenceininitialstatepriors, d {1} = [.25.25]′.Wealsoset
the learning rate tomdp.eta = .5 and the forgetting rate to
mdp.omega = 1 (i.e., no forgetting). We then define a ‘risk-
seeking’ (RS) parameter that corresponds to how precise the
preferenceistowinthehigheramountofmoneyinthe Cmatrix:
Cwin =
C{2} =
⎡
⎣
0 0 0
0 −1 −1
0 RS RS
2
⎤
⎦
Inourfirstsimulation,weset RS = 3andinoursecondweset
RS = 4.Weexpecttheagentwillbelessinformation-seeking,and
morerisk-seeking,inthelattercase.AscanbeseeninFig.10( top-
left),theagentwith RS = 3choosestotakethehintonthefirst
severaltrials,andslowlybeginstoforegothehintonlatertrials
(withsomechoicestochasticity).Unexpectedlosses,showninthe
paneljustbelow,alsocausetheagenttoreturnto‘playingitsafe’
andagainaskforthehint.Incontrast,theagentwith RS = 4
35
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.10. Simulatedlearningontheexplore–exploittaskandpredictedneuronalresponses.Bluecirclesinthetoppanelindicatechosenactions(i.e.,theagent’s first
choice on each trial); darker shading indicates a higher action probability. Wins/losses, free energies, simulated neuronal responses, and changes in prior beliefs
aboutcontextovertime(darker = strongerpriorbelief)areshowninthelowerpanels.Seemaintextformoredetails. Left:Examplesimulationofarisk-averse
agent(i.e.,definedbyamoderatelyprecisepreferencedistribution; RS = 3)learningfromrepeatedtrialsoftheexplore–exploittask.Heretheagentslowlygains
confidencethattheleftmachinewillalwaysbebetterandbeginstochoosethatoptionwithouttakingthehint.Thisagentoftenreturnstotakingthehintafter
unexpectedlosses. Right:Examplesimulationofarisk-seekingagent(i.e.,definedbyahighlyprecisepreferencedistribution; RS = 4)learningfromrepeatedtrials
oftheexplore–exploittask.Thisagentonlyrequiredseeingthehintonetimebeforeattemptingtopickthecorrectoptiondirectly(andwinmoremoney).Itdidnot
returntotakingthehintwithoccasionalunexpectedlosses.Notethatbothagentsalsoshowsomestochasticityinchoice.Thesesimulationscanbereproducedby
runningthe Sim = 2optioninthesupplementary Step_by_Step_AI_Guide.mcode(althoughnotethat,becauseoutcomesaresampledfromprobabilitydistributions,
resultsmaynotbeidenticaleachtime).(Forinterpretationofthereferencestocolorinthisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
choosestotakethehintonlyonce(Fig.10, top-right)andthen
takesthatassufficientevidencethattheleftmachinemustbethe
betterone(andcontinuestochoosethisonethroughout,despite
occasionallosses).Thelowerpanelsinthefigureshowsimulated
event-relatedpotentials(ERPs),simulateddopamineresponses,
and how beliefs change over time regarding which context is
morelikely(darker = higherprobability).Whenthe Sim variable
issetto Sim = 2inthe Step_by_Step_AI_Guide.mcode,you
canreproducethesesimulations(andadjustthe RS parameterto
otherpossiblevalues).
Next,wesimulateareversallearningparadigm.Here,there
are32trialsintotal.Unbeknownsttotheagent,inthefirst4
trialstheleftmachinewillbebetter,butintherestofthetrials
the right machine will be better. Again, we examine an agent
withRS = 3and RS = 4. As shownin Fig.11, theRS = 3
agentchosetotakethehintonalltrialsinthissimulation.In
contrast,the RS = 4agentquicklylockedontotheleftmachine,
butitthenreturnedtotakingthehintafterthereversal.After
severaltrialsofagainchoosingthehint,itbecomesconfidentin
directlychoosingtherightmachineinthefinaltrials.Whenthe
Sim variableissetto Sim = 3inthe Step_by_Step_AI_Guide.m
code, you can reproduce these simulations (and adjust theRS
parametertootherpossiblevalues).
Eachoftheseexamplesismeantprimarilytogivethereader
a sense of how to work with these types of simulations. But
therearemanyotherparametersthatcouldbemanipulated.In
the accompanying MATLAB code, the reader can easily re-run
these simulations while changing the learning rate, forgetting
rate, action precision, or any other parameters in the model.
Weencouragethereadertodosotogetasenseoftheunique
influencesofdifferentparametersontaskbehavior.Forexamples
ofpapersthatmodellearningusingactiveinference,seeFriston
etal.(2016a,2017b),Schwartenbecketal.(2019),Smithetal.
(2021e),Smith,Parr,andFriston(2019b),Smith,Schwartenbeck,
Parr,andFriston(2020d),Smithetal.(2020e)andTschantz,Seth,
andBuckley(2020).
5. Neuralprocesstheory
Inmanyactiveinferencepapers,oneseesfiguressimilarto
Fig.12.Thesefigurestypicallydepictaseriesofupdateequations,
severalcolumnsof‘ball’neurons,aspecificpatternofsynaptic
connections,andlabelsassigningmodelvariablestothoseneu-
ronsandsynapticconnections.Suchfiguresaremeanttodepict
one possible neural implementation of active inference, which
servesasaconcreteillustrationofthegeneralbiologicalplausibil-
ityofthetheory.Thistypeofbiologicalplausibilityisconsidered
an important strength of active inference models, due to the
resultingabilitytomakeempiricalpredictionsaboutneurophys-
iologicalresponses.Inturn,thesepredictionsallowonetoassess
theevidencefordistinctmessagepassingalgorithmsandpossible
neuronalimplementations(Parretal.,2019).Someofthesimula-
tionoutputsinTable3containpredictedneuronalresponsesthat
canbeassignedtodistinctneuronalpopulationsorsynapticcon-
nections(someexamplesareplottedinFig.8).Tobeclear,some
neurophysiologicalpredictionsinactiveinferencearenotspecific
toasingleneuralimplementation(i.e.,theyarebasedonlyon
thegenericpredictionerrorminimizationandprecisionupdating
equationsdescribedabove);andthesetypesofpredictionshave
beensuccessfullyassociatedwithneuralresponsesobservedin
previousfunctionalmagneticresonanceimaging[fMRI]andEEG
studies(e.g.,see(Schwartenbecketal.,2015;Smithetal.,2021e;
36
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.11. Simulatedreversallearningontheexplore–exploittaskandpredictedneuronalresponses.Here,thebettermachinewasontheleftforthefirst4trialsand
thenontherightforallsubsequenttrials(withouttheagentexpectingthis).Bluecirclesinthetoppanelindicatechosenactions(i.e.,theagent’s first choiceon
eachtrial);darkershadingindicatesahigheractionprobability.Wins/losses,freeenergies,simulatedneuronalresponses,andchangesinpriorbeliefsaboutcontext
overtime(darker = strongerpriorbelief)areshowninthelowerpanels.Seemaintextformoredetails. Left:Examplesimulationofarisk-averseagent(i.e.,defined
byamoderatelyprecisepreferencedistribution; RS = 3),whoalwayschosetotakethehint. Right:Examplesimulationofarisk-seekingagent(i.e.,definedbya
highlyprecisepreferencedistribution; RS = 4),whoquicklybecameconfidentinchoosingtheleftmachinewithouttakingthehint.Aftertheunexpectedreversal,
itdecidedtoagaintakethehintformanytrialsbeforebecomingconfidentinchoosingtherightmachinedirectly.Thesesimulationscanbereproducedbyrunning
theSim = 3optioninthesupplementary Step_by_Step_AI_Guide.mcode(althoughnotethat,becauseoutcomesaresampledfromprobabilitydistributions,results
maynotbeidenticaleachtime).(Forinterpretationofthereferencestocolorinthisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
Whyte, Hohwy, & Smith, 2021; Whyte & Smith, 2020). How-
ever,corticalcolumnproposals–suchasthatshowninFig.12
– can also motivate targeted research using methods (such as
laminarfMRI)thathavebeenappliedtotestsimilarcolumnar
implementationsproposedforpredictivecoding(Stephanetal.,
2019).Topreparethereaderforunderstanding–andpotentially
contributingto–thisimportantareaofactiveinferenceresearch,
wewillwalkthereaderthroughFig.12step-by-step.Thiswill
alsobeimportantwhenwediscusshierarchicalmodelsinthe
nextsection(Section6),inwhichsimulatedEEGresponsesoccur
overdifferenttimescalesandarepredictedtooccuratdistinct
levelsofprocessingwithinthebrain.
Inthedepictedneuralnetwork,eachcolumnofneurons(in
this case, 3 columns) represents beliefs and prediction errors
about eachpointintime(fromlefttoright,indicatedbysub-
scripts forτ = 1, 2, 3). With each new observation, beliefs
aboutalltimepoints(i.e.,aboutthepast,present,andfuture)
areupdated,correspondingtochangesinneuralactivationacross
allneurons.Theupwardarrowsfromobservations(purplenodes
atthebottom)tolayer3(denotedby επ,τ forstateprediction
errors) are depicted as conveying excitatory (red) observation
signals (e.g., sensory input) to granular cells in each cortical
column,wheretheseobservationscandifferateachtimepoint.
Thereceiving(pink)statepredictionerrorneuronscalculatetheir
predictionerrorsbycombiningobservationsignalswithpredic-
tionsignalsfromthestaterepresentationsinthecyanneuronsof
layer2(supragranularneuronsdenotedby sπ,τ forstaterepresen-
tations).Notethatexcitatory(red)downwardsignalsfromthese
neuronstolayer3areconveyedbothforward(fromthe τ = 1,
leftneurons)andbackward(fromthe τ = 3,rightneurons)–in-
dicatingbothprospectiveandretrospectivepredictiveinfluences
onstaterepresentations about atimepoint.Incontrast,inhibitory
(blue)signalsareconveyedbythesestaterepresentationneurons
to layer three neurons for the current time point, leading to
minimization of prediction error when predictions from state
representationsmatchobservationsignals.
Notenextthateachofthesestateandstatepredictionerror
representationsarecalculatedinparallelforeachpolicy(denoted
byoneexampleneuralcolumninfrontofanother).Thetop(red)
layer1neurons,however,donothaveanothersetofneurons
behind them. This is because they perform a Bayesian model
averageasanoverallbestguessaboutstates.Theydothisby
takingstaterepresentationsforeachpolicy,multiplyingthemby
theprobabilityofthatpolicy,andthenaveragingthemtogeta
finalposterioroverstates.Thisisaccomplishedinconjunction
with the policy representation (π) neuron on the left (meant
to represent a subcortical neural population), the signals from
whichmultiply(viathegreenmodulatoryconnection)theex-
citatory(red)signalsfromthestaterepresentationneuronsfor
each policy in layer 2. After each new observation, activity in
thesepolicyrepresentationneuronsalsopromotessomeactions
(u)overothers.
Policy representation (π) neurons are in turn activated by
neurons encoding habits (E) and inhibited by those encoding
expectedfreeenergy( G;i.e.,greaterexpectedfreeenergyreduces
theprobabilityofapolicy).Theinfluenceof G onπ ismodulated
bytheexpectedfreeenergyprecisionterm( γ )— depictedhere
asbeingconveyedbysubcorticaldopamineneurons.Theactivity
of G neurons is increased by outcome prediction errors (ζπ,τ )
inlayer5,multiplied(greenconnections)bytheprobabilityof
thoseoutcomesundereachpolicy( oπ,τ ,cyanneuronsinlayer4;
37
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig. 12.Common depiction of the neural process theory associated with active inference. This includes the update equations on the left and an example neural
networkthatcouldimplementthemontheright(onlyexemplarsynapticconnectionsareshowntoavoidvisualclutter).Seemaintextforanin-depthwalk-through.
Notethatwhilemostequationsarepresentedinthesameformasinthemaintext,wehaveupdatedtheoutcomepredictionerror( ςπ,τ )equationtoincludethe
‘novelty’term( Asπ,τ ·Wsπ,τ )within Gπ thatwasintroducedintheprevioussectiononlearning.Wealsodepictiterativepredictionerrorminimizationandprecision
updatingprocessesusingarrowsindicatingtheorderofrepeatedupdatinguntilconvergence.(Forinterpretationofthereferencestocolorinthisfigurelegend,the
readerisreferredtothewebversionofthisarticle.)
notethatthesearecalculatedfrom Asπ,τ ).Theseoutcomepre-
dictionerrorsreflecttheexpecteddifferencebetweenpreferred
outcomesandpredictedoutcomesundereachpolicy(fromlayer
4; downward excitatory connections between layers 4 and 5),
andwherethosepredictedoutcomesareinturnbasedonstate
representationsinlayer2(excitatoryredconnectionsfromlayer
2tolayers4and5).Notethatthematrix W(i.e.,calculatedbased
onthesynapticstrengthsencodedin A)alsoinfluences G through
interactionwithactivityintheneuronsencoding sπ,τ ,butthese
connectionsarenotexplicitlyshowntominimizevisualclutter.
Basedonthisdescription,we assumethereadershouldbe
abletofollowtheequationsonthelefttoidentifyeachassociated
connectioninthenetworkontheright.Notethatthisfigureonly
shows example connections, assuming two policies and three
timepoints.However,thebasicideaisthat,ifeachexcitatory
connectioncorrespondstoaddition,eachinhibitoryconnection
correspondstosubtraction,andeachmodulatoryconnectioncor-
respondstomultiplication,theneachoftheupdateequationson
theleft(inthe‘Beliefupdating’,‘Policyinferenceandexpected
freeenergyprecision’,‘Actionselection’,and‘Learning’boxes)can
beimplementedinastraightforwardmannerwithinarelatively
simpleneuralnetwork.Itisworthmentioningthattheseupdate
equations are not always presented in identical form in such
figuresacrosstheliterature.However,thesevariationsareeither
algebraicallyequivalentorhavebeenpresentedwithorwithout
certainelements(e.g.,withorwithoutlearning,withorwithout
expectedfreeenergyprecision,etc.),dependingonthegoalsof
thepaper.
Oneotheraspectoftheneuralprocesstheorythatwewillnot
discussindetail–butthatwewouldliketobrieflypointthein-
terestedreadertoward–pertainstotheideathatBayesianmodel
reduction(i.e.,comparingmodelstofindthesimplestonethat
canaccountforavailabledata)isimplementedbyhomeostatic
synaptic adjustment processes during sleep and resting wake-
fulness(Bucci&Grasso,2017;Fristonetal.,2017b;Hobson&
Friston,2012;Hobson,Hong,&Friston,2014;Smithetal.,2020d;
Tononi&Cirelli,2014).Inshort,thisliteraturesuggeststhat,dur-
ingsleep/rest,thebraincanalsominimize VFE byfindingsimpler
models (with fewer parameters) that can successfully account
forpreviousexperience.Thiscanbeaccomplishedinpartviaa
‘synapticdownscaling’process–knowntooccurduringsleep–
inwhichsynapticconnectionsthathavegottenstrongerduring
recentlearningaresubsequentlyattenuated.Synapticdownscal-
ingcanbehelpfulinremovinganysmallsynapticchangesdriven
bynoise(e.g.,uninformativecoincidencesinthepresenceofmul-
tiplestimuli),leavingonlythelargersynapticchangesneededto
accountforconsistentpatternsinrecentexperience.Althoughwe
donotcoverithere,wenotethatthe spm_MDP_VB_X.mscript
(andourtutorialversion)doeshaveanadditional‘ BMR’option
that can be turned on, which will implement Bayesian model
reduction by calling a further SPM script written to simulate
thisproposedfunctionofsleep/rest( spm_MDP_VB_sleep.m).In
this case, the entries in each matrix (e.g., theA matrix) are
assumed to represent synaptic connections, and model reduc-
tioninvolveseliminatinganychangesintheconcentrationpa-
rameters for those matrices during learning that did not im-
provetheexplanatorypowerofthemodel(i.e.,intermsofthe
accuracy/complexitytrade-off).
To facilitate the reader’s ability to use the neural process
theory in their own research, thespm_MDP_VB_X_tutorial.m
script automatically generates simulated responses for several
of the neuronal populations described above (see Table 3 for
specificoutputdescriptions).Thisincludesthesimulatedfiring
ratesofneuronalpopulationsrepresentingposteriorsoverstates
(i.e.,whosepredictedlocationinthebrainwoulddependonthe
task).Here,distinctfiringratesaregeneratedforthestateswithin
each state factor of a model (e.g., in the explore–exploit task,
firingratesinonepopulationwouldencodetheprobabilityof
the‘left-bettercontext’,whilethoseinanotherpopulationwould
encodetheprobabilityofthe‘right-bettercontext’).Thescript
38
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
alsogeneratesvectorsencodingtheassociatedstateprediction
errorsdiscussedabove.Simulatedelectrophysiologicalresponses
are based on the rates of change (derivatives) in firing rates
duringprediction-errorminimization(interpretedaslocalfield
potentials [LFPs] or event-related potentials [ERPs], depending
on the context). Separate neural populations are postulated to
encodeposteriorsoverstatesandpredictionerrorsforeachtime
pointτ (i.e.,eachofwhichareupdatedwithnewobservations
overtime).
Another set of potentially useful outputs generated by the
spm_MDP_VB_X_tutorial.m script corresponds to theVFE of
changesinconcentrationparameters(i.e.,higher = greatersur-
prise during learning; see Table 3). Recall here that another
important aspect of the neural process theory is that synaptic
inputswithdifferentstrengthsarepostulatedtocarrytheprior
and conditional probabilities encoded in each of the vectors/
matrices that define a model (e.g., the probabilities in theA
or Bπ,τ matrices, or in theD or E vectors). More specifically,
thevalueofeachentryinparticularmatricesorvectorscanbe
thoughtofascorrespondingtothestrengthofasynapticconnec-
tionbetweentwoneurons.Updatingconcentrationparameters
overrepeatedtrials(i.e.,overaslowertimescalethanperception)
can therefore be linked to synaptic plasticity — where the
specificchangesinvolveddependonthepatternofobservations,
belief updates, and/or policies chosen on each trial). TheVFE
of changes in concentration parameters can therefore be used
to quantify the magnitude of change in a model on a trial by
trialbasis,whichcouldbeusedtoidentifyneuralcorrelatesof
suchchanges.Thepredicteddynamicsofwithin-trialprediction
errorsandbeliefupdateswillalsobemodulatedbythesesynaptic
changes,allowingforpredictedtimecoursesthatcouldbeused
forsimilarexperimentalpurposes.Inaddition,synapticstrength
changeswillalsoinformthematrix W–dynamicallyadjusting
motivationtoseekinformationaboutmodelparameters.Thus,
several experimentally useful outputs are provided to test the
neuralprocesstheory.
AnavailableSPMfunctionforplottingsingle-trialneuralsim-
ulationscanberunbyinputtingthefollowingintoMATLAB:
spm_figure(′GetWin′,′ Figure2′); clf; spm_MDP_VB_LFP(MDP);
subplot(3, 2, 3)
Setting the variableSim in the accompanying tutorial code
(i.e.,Step_by_Step_AI_Guide.m, line 51) toSim = 1 will also
generatesimulationplotsusingthisfunction.
Basedonthecurrentmodelspecification,arepresentativeplot
ofsimulationresultsisshowninFig.8C,basedonthesingletrial
depictedinFig.8A.The top-left paneldepictsthebeliefupdates
at eachtimepoint t (columns)about eachtimepoint τ (rows).
Asbefore,darkerindicateshigherprobability.Inthiscase,after
presentation of the first observation (t = 1, column 1), the
modelisfullyuncertainaboutcurrentorfuturestates(eachrow).
Afterpresentationofthesecondobservation( t = 2,column2),
whenthemodelreceivesthehint,itbecomeshighlyconfident
that it was, currently is, and will continue to be in the ‘left-
better’state(i.e.,basedonitstransitionbeliefsthatthisstatedoes
notchangewithinatrial).Thesebeliefsremainstablewhenit
receivesthethirdobservation(column3;i.e.,uponobservingthe
expectedwin).The top-right paneldepictsthesebeliefupdates
(i.e.,changesinbeliefsovertimeaboutthestateateachtime
point;2possiblestatesforeachofthe3timepoints).Thesebelief
updatesaredepictedastracesofchangesinneuralfiringrates,
with2firingratesperdistribution(i.e.,encodingtheprobability
of the left- vs. right-better context), resulting in 6 firing rate
tracesintotal(notethat,duetooverlap,notall6tracesareclearly
visibleinthisexample).The bottom-left plotdepictsthissame
information,butheredisplayedintermsofasimulatedraster
plot(i.e.,onetickperactionpotentialperneuroninasimulated
population).The middle-right paneldepictspredictedlocalfield
potentials(orevent-relatedpotentials),whichreflecttherateof
changeinthesimulatedfiringrates.The middle-left paneldepicts
theneuralresponsesassociatedwithcontextstatebeliefsbefore
(dottedline)andafter(solidline)filteringat4Hz,superimposed
onatime–frequencydecompositionofthelocalfieldpotential
(averagedoverallsimulatedneurons).Thistypeofplothasbeen
usedinpreviousworktoexplainhow/whysimulateddepolar-
izationinspecificfrequencyrangesmaycoincidewithspecific
stimulus-induced neural responses (Friston et al., 2017a). The
bottom-right paneldepictssimulateddopamineresponsesafter
eachnewobservation(asalsodepictedinaslightlydifferentway
inFig.8A).ThesewerecoveredindetailinSection3(seeFig.9
andthelastrowofTable2).
Thisplottingfunctionalsohasseveraloptionsasadditional
functionentriesandoutputsasfollows:
[u, v] =spm_MDP_VB_LFP(MDP, UNITS, FACTOR, SPECTRAL).
UNITS:amatrixwith2rowsandoneormorecolumns.The
firstrowindicateswhichhiddenstate(s)toplotovertimefor
thespecifiedstatefactor.Thesecondrowspecifiesthetimepoint
beingrepresented.Forexample: UNITS = [1212 ; 1133 ] would
plotfiringratesforthefirsttwohiddenstatesoftheselectedstate
factorovertimewithregardtobeliefsabouttimepoints1and3.
Bydefault,allunitsareselected.
FACTOR:asinglenumberdenotingwhichstatefactortoplot
(default= 1).
SPECTRAL:eithera0or1(default = 0).If1,the top-left plot
isreplacedbyaplotofthepowerofsimulatedneuralresponses
indifferentfrequencyranges.
Theoptional outputsuandv correspondtovectorsencoding
simulatedevent-relatedpotentialsandfiringrates,respectively
(forselectedunits).Aswasmentionedabove,theevent-related
potentials correspond to the temporal derivative of the firing
rates, while the firing rates reflect the magnitude of posterior
beliefs over each state at each iteration of marginal message
passing.
6. Buildinghierarchicalmodels
6.1. Hierarchical model structure
Nowthatwehaveaclearideaofhowtospecifyagenera-
tive model of a behavioral task, how to interpret the relevant
outputs,andhowwecangeneratetestablepredictionsregarding
neuralresponses,wewillnowextendthisfoundationbybuild-
ingahierarchicalor‘deeptemporal’modelanddemonstrating
howitcanbeusedtoreproduceestablishedneurophysiological
results (for examples, see Friston et al., 2018; Parr & Friston,
2017b;Smith,Lane,Parr,&Friston,2019a;Whyteetal.,2021;
Whyte&Smith,2020).Specifically,wewillreproducetheresults
of experiments examining ERPs in a commonly used auditory
mismatchparadigmdesignedtostudytheneuralbasisofper-
ceptuallearningandexpectationviolation.Thisshouldhelpthe
readertogeneralizetheirunderstandingbyseeinghowtobuild
amodelwithadifferentstructure.Itwillalsodemonstratethe
versatility and wide range of applications of active inference
models.Forexample,whilereinforcementlearningmodelsare
oftenusedtosolvetasksliketheslotmachinetask,suchmodels
arenotreadilyapplicabletoperceptualtasks,liketheauditory
mismatch paradigm, that do not include rewards or produce
notable variability in behavior (e.g., when performance tends
39
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
to be near ceiling across participants). By the end of this sec-
tion,thereadershouldhaveasolidunderstandingofbothhow
hierarchicalmodelsworkandhowtoimplementthem.
The steps for building hierarchical models are quite simi-
lar to what we have already covered, because this primarily
just involves building two models, and then placing one be-
low the other. The further step is figuring out how to link
thetwomodelstogether.Thisisbecause,inhierarchicalmod-
els, the states at the lower level exchange information with
statesatthehigherlevelinaveryspecificbidirectionalmanner
(see Fig. 13). First, for each time point in a second-level trial,
thehiddenstatesatthesecondlevel( s{2}
τ ;superscriptindicates
hierarchical level) provide prior beliefs over the initial states
ofafirst-leveltrial(i.e., D{1} = A2TD{2} forthefirstfirst-level
trial;D{1} = A2Ts{2}
τ,t=τ−1 forallsubsequentfirst-leveltrials).In
turn, posterior beliefs over initial states at the end of a first-
level trial (i.e.,s{1}
τ=1,t=T ) are treated as observations at a time
pointinasecond-leveltrial( o{2}
τ ).Thismeansthatthesecond-
levelAmatrix(likelihoodmapping)mediatestheascendingand
descendingmessagesbetweenhierarchicallevels.Thisstructure
alsoentailsthatthesecond-levelmodelmustoperateataslower
timescalethanthefirst-levelmodel,becauseeachobservationin
thesecond-levelmodel(i.e.,eachtimepointinasecond-level
trial)correspondstotheresultsof(i.e.,posteriorbeliefsafter)a
completetrialinthefirst-levelmodel.Thus,thereareasmany
first-leveltrialsastherearetimepointsinasecond-leveltrial.
Forexample,iftherearefourtimepointsinasecond-leveltrial,
thismeanstherewillneedtobeacorrespondingsequenceoffour
first-leveltrials(i.e.,whereeachfirst-leveltrialcoulditselfhave
severaltimepoints).Thisiswhysuchmodelsareoftencalled
deeptemporalmodels.
Thistypeofmodelarchitectureisessentialforcapturingper-
ceptualphenomenawithnesteddynamics,orwhereobjectsmust
berecognizedbeforeregularitiesinthebehaviorofthoseobjects
canbedetected.Forexample,toperceiveabaseballflyingina
leftwarddirection,alower-levelmodelwouldfirstneedtoinfer
thebaseball’sidentityandposition(i.e.,oneinferredpositionper
lower-leveltrial),andahigher-levelmodelwouldthenneedto
accumulateevidenceforaleftwardtrajectoryofmotionbased
on how the baseball’s inferred position changes across several
lower-level trials. As another example, to recognize a melody,
alower-levelmodelwouldbeneededtoinferthepresenceof
eachnote,andahigher-levelmodelwouldthenbeneededto
accumulateevidenceforaspecificmelody,basedonaspecific
sequenceofinferrednotesovertime.Afurtherintuitiveexam-
ple is reading, where the first level infers single words, while
the second level infers the narrative entailed by sequences of
words (Friston et al., 2018). Note, as soon as we start to use
deep or hierarchical generative models, we are essentially re-
laxingtheMarkovianassumptionbyintroducingaseparationof
temporalscalestoproducewhatareknownassemi-Markovian
models.Theseareessentialforinferringnarratives,language,or
anydeeplystructuredsequenceofstatetransitions.
Asidefromtheseexamples,thehierarchicalPOMDPsetupis
quiteflexibleandcanbeusedtomodelawiderangeoftempo-
rallystructuredphenomena.Forexample,apolicyspacecould
beincludedateitherlevelalone,orbothlevels,dependingon
thetargetphenomenontobemodeled(e.g.,verbalreportata
higherlevelvs.reflexivebehavioratalowerlevel).Onecould
also specify several time points in each lower-level trial, such
thathigher-levelstatesgeneratesequencesortrajectoriesofstate
transitionsatthelowerlevel(i.e.,within-trial).Inpreviouswork,
hierarchicalmodelshavebeenusedtomodelworkingmemory,
reading,visualconsciousness,andemotionalawareness,among
otherphenomena(Fristonetal.,2017c,2018;Hespetal.,2020;
Parr&Friston,2017b,2018b;Sandved-Smithetal.,2021a;Smith
etal.,2019a;Whyteetal.,2021;Whyte&Smith,2020).Hierarchi-
calPOMDPsalsoaffordfurtheropportunitiesforsimulatingneu-
ronalprocesses.Todate,simulationsassociatedwiththefaster
and slower timescales of belief updating have been shown to
reproduceanimpressivenumberoftask-basedelectrophysiolog-
icalfindings.Forexample,empiricallyobservedpatternsofERPs
associatedwithspecificcognitiveandperceptualprocesses,such
astheP300andmismatchnegativity(MMN),emergenaturallyin
simulationsofdifferentexperimentalparadigms,whichsupports
the face validity of both the model structure and the neural
processtheory(e.g.,Fristonetal.,2017a;Parr&Friston,2017b;
Whyteetal.,2021;Whyte&Smith,2020).
6.2. Building a hierarchical model
As a concrete, empirically relevant example, we will now
demonstrate how one could build a hierarchical model to
simulate a simplified version of the auditory mismatch ‘local–
global’ paradigm introduced in Bekinschtein et al. (2009); see
bottompanelofFig.13.Inoursimplifiedversionofthisparadigm,
eachtrialconsistsofasequenceoffourtones(witheitherlowor
highfrequency),thefirstthreehavethesamefrequency,andthe
fourthtoneeitherconformstothepredictedpattern(e.g.,high-
high-high-high;local standard)orviolatesthepredictedpattern
by presenting a different tone (e.g., high-high-high-low;local
deviation). During EEG, local deviations elicit a mismatch
negativity component in ERPs (i.e., a negative component
obtainedbysubtracting‘localstandard’trialsfrom‘localdevia-
tion’trials),whichappearsafterapproximately130ms.Impor-
tantly,thesequenceoflocalstandardandlocaldeviationtrials
establishesaglobalpatternthatcanitselfbeconfirmed( global
standard)orviolated( global deviation).Forexample,thiscould
includeseverallocaldeviationtrialsinarowfollowedbyanun-
expectedlocalstandardtrial.Globaldeviationsareknowntoelicit
aP300ERPcomponent(i.e.,apositivecomponentthatappears
after approximately 300 ms). Unlike other auditory mismatch
paradigms, this design also allows local and global violation
responses to be dissociated. That is, the factorial design leads
tofourconditions local standard + global standard,local standard
+ global deviation , local deviation + global standard , andlocal
deviation + global deviation.Forbrevity,hereweonlysimulate
two of the four possible combinations,local deviation + global
deviation,and local deviation + global standard.Inoursimulated
version of the task, we presented an active inference agent
with sequences of 4 tones, where each tone could be either
low or high, in an analogous manner to the empirical task.
We then had the agent report whether the last stimulus on
each trial was the same as, or different from, the established
pattern.
Atthispoint,thereaderisencouragedtoopentheaccompa-
nyingMATLABscriptandfollowalonginparallel( Step_by_Step_
Hierarchical_Model.m). As with the previous model, we will
startbysettingupthepriorsoverinitialstatesforeachhidden
statefactoratthefirstlevel:
p
(
stone
τ=1
)
=
D {1} = [1 1 ]′
Dir (d) =
d = D
Thespecificationfor D {1} meansthatthereisanequalproba-
bilityofahighoralowtone(leftandrightentries,respectively).
Notethat,becausethecolumnsofallthevectorsandmatricesof
thegenerativeprocessarerunthroughasoftmaxfunctioninthe
40
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.13. TopLeft:Bayesiannetworkdepictionofa2-levelPOMDP.Observationsdependonhiddenstatesatthefirstlevel.Inturn,hiddenstatesatthefirstlevel
dependonhiddenstatesatthesecondlevel.Specifically,second-levelhiddenstatesprovidepriorbeliefsoverinitialstatesatthefirstlevel,whileposteriorbeliefs
overinitialstatesatthefirstlevelaretreatedasobservationsbythesecondlevel.Intheexampleshownhere,thefirstlevelhastwostatetransitionspertrial.This
entailsthatthefirstlevelhastwostatetransitionsforeveryonestatetransitionatthesecondlevel.Thus,beliefsatthesecondlevelevolveoveraslowertimescale.
TopRight:ExampleneuralnetworkimplementingthehierarchicalPOMDPshownontheleft. Bottom:Illustrationoftheauditorymismatchparadigmforwhich
webuildagenerativemodelandrunsimulationsinthissection(Section6).Inthemodelspecifiedinthemaintext,beliefsaboutsingletonesareencodedatthe
firstlevel,whilebeliefsaboutsequencesoftonesareencodedatthesecondlevel.Acrosstrials,theagentthenbuildsuppriorexpectationsthroughpresentation
ofrepeatedtonesandtonesequences.Inlinewithempiricalresults,thepost-learningmodelsimulationsshowninFigs.14–16predictearlierERPsforunexpected
tones(localdeviations)andlaterERPsforunexpectedtonesequences(globaldeviations).
spm_MDP_VB_X_tutorial.mscript,D {1} = [11]′ isequivalent
toD {1} = [.5.5]′.Asthesimulationinvolveslearning,wealso
need to separate the generative process from the generative
model by including the lowercased for the generative model.
Here we simply setd = D, as the agent will also start out
withthebeliefthatahighandalowtoneareequallyprobable
(butwithfairlylowconfidence).However,including d willallow
theagenttoaccumulateconcentrationparameters(changingthe
shapeofitsinitialstatepriors)overtrialsbasedonpatternsinits
observations.
41
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.14. SimulatedbeliefsandbehaviorinthehierarchicalPOMDP(analogoustothesingle-levelmodelplotsshowinFig.8)describedinthemaintask.Thetop
andbottompanelssimulatethetwotaskconditionsdescribedinthemaintextbothbeforeandafterlearning(Trial1andTrial10,respectively).Thesesimulations
demonstratedthattheagentperformedthetaskappropriately.Thethreepanelsinthe top-left ofeachplotshowposteriorsoverstatesattheendofthetrial.That
is,thestatesthemodelbelievesitwasinateachtimepoint τ atthelasttimepoint t (i.e.,afterreceivingthelastobservation).Here,timegoesfromlefttoright,
darkerindicateshigherprobability,andthecyandotsdenotethetruestates.The top-right panelsineachplotshowtheactionprobabilitiesandtrueactions.The
left-middle panelineachplotjustshowsthedifferentpossibleaction-sequences/policiesinthespecifiedmodel(encodednumericallyfromlefttoright).Adarker
colorindicatesalowernumber.The right-middle panelineachplotshowstheprogressionofposteriorbeliefsineachpolicyovertime(fromlefttoright,darker
= higherconfidence).Policies(rows)lineupwiththeactionsequencesintheplotonthe middle-left.Thetwopanelsinthe bottom-left ofeachplotdisplaythe
outcomes in cyan dots and the agent’s preference for each outcome, where darker colors indicate a greater preference (i.e., higher prior probability). Lastly, the
bottom-right plot displays predictions about dopamine responses based on the neuronal process theory (i.e., encoding changes in expected precision ofEFE; see
lastrowofTable2andFig.9).Intermsofbehavior,noticethatallmodelsselectedthecorrectactionsandreceived‘correct’feedback,asindicatedinthelower
outcomeplot.Thesesimulationscanbereproducedusingthe Step_by_Step_Hierarchical_Model.mscriptincludedas supplementarycode.Notethat,duetorandom
sampling,resultsmaynotbeidenticaleachtime.
42
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig. 15.Simulated ERPs and firing rates extracted from the hierarchical POMDP model of the auditory mismatch paradigm during each task condition (top vs.
bottom) both before and after learning (Trial 1 vs. Trial 10). As described in the main text (and illustrated further in Fig. 16), these simulations reproduce ERP
resultsobservedinpreviousempiricalstudies(e.g.,strongershort-latencyERPsinresponsetolocaldeviationsandstrongerlonger-latencyERPsinresponsetoglobal
deviations).Theunitresponseplotsshowtheposteriorprobabilityoverstates( sπ,τ )ateachlevelofthemodel(asusual,darkercolors = higherposterior = higher
firingrates).Asdescribedintheneuronalprocesstheorysection,normalizedfiringratesaregeneratedbypassingthedepolarizationvariable vπ,τ throughasoftmax
functionsπ,τ = σ(vπ,τ ).TheERPplotsshowtherateofchange(firstderivative)ofposteriorbeliefsoverstatessummedoverallstatesateachlevelofthemodel
(analogoustotheaggregatesignalmeasuredatthelevelofthescalpbyEEG).Notethateachincrementof0.5alongthe x-axiscorrespondstoatrialatthelower
level,andtoatimepointatthehigherlevel(i.e.,6timepointsinahigher-leveltrial,with6correspondinglower-leveltrials).Foradetaileddescriptionofeachplot
anditsmeaninginrelationtothetask,seethemaintext.Thesesimulationscanbereproducedusingthe Step_by_Step_Hierarchical_Model.mscriptincludedas
supplementarycode.Notethat,duetorandomsampling,resultsmaynotbeidenticaleachtime.(Forinterpretationofthereferencestocolorinthisfigurelegend,
thereaderisreferredtothewebversionofthisarticle.)
Next,wemustspecifythelikelihoodmappingsforthefirst
levelinthe Amatrix.
p
(
otone
τ |stone
τ
)
=
A{1} =
[
1 0
0 1
]
43
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.16. CustomERPplotsgeneratedfromthesimulatedauditorymismatchparadigm.Ontheleftwehavethe‘raw’first-andsecond-levelERPwaveformscentered
onthefourthtimestepofthetenthtrialfromeachcondition.Therightsideofthefigureshowshowthesubtractionofthedevianttrialsfromthestandardtrials
reproducesboththeMMNandP300.Atboththefirstandsecondlevel,‘deviant’ERPshaveasubstantiallylargeramplitudethan‘standard’ERPs.Notethat,although
therelativedifferencesintimingbetweensimulatedERPs(andtherelativedifferencesintimingbetweenERPsatdifferentlevelsofthemodel)aremeaningful,the
unitsweascribetotimeare(usually)somewhatarbitrary.So,forclarity,wehavenotincludedanyunitsoftimeonthe x-axis.Thesesimulationscanbereproduced
usingthe Step_by_Step_Hierarchical_Model.mscriptincludedas supplementarycode.Notethat,duetorandomsampling,resultsmaynotbeidenticaleachtime.
Thisissimplyanidentitymatrixindicatingthatthetonestates
correspond 1-to−1 with tone observations (columns [left to
right]: high tone, low tone states; rows [top to bottom]: high
tone,lowtoneobservations).However,inthegenerativemodel
wemaywanttointroducesomenoiseintotoneperception.One
convenientwaytodothisistofirstspecify:
Dir (a) =
a = A
Then,wecanuseasoftmaxfunctiontocontroltheexpected
precisionofthestate-observationmappingwitha precision pa-
rameter:
precision = 2;
a{1} =spm_softmax(precision ∗ log(A{1} +exp(−4)))
Notethatthe exp(−4)issimplyaverysmallnumberadded
toA{1} topreventthepossibilityof log(0),whichisundefined
(alsonotethat,while −4isareasonablevalue,othervaluescould
bechosen).Dependingonthevalueofthe precision parameter
(higher= moreprecise),thiswillresultinalikelihoodmapping
thatspecifiesdifferentamountsofsensorynoise.Forexample:
Dir (a) =
a{1} =
[
.92 .08
.08 .92
]
Notethat,forclarity,thisexampleshowsalowerprecision
thanwhatresultsfromsetting precision = 2intheaccompany-
ingtutorialcode.
As we are mainly interested in simulating the learning of
priorexpectations( D vector),wealsomultiplytheconcentration
parameterswithin aby100(anarbitrarylargenumber)toef-
fectivelypreventthelearningoftheseotherparameters.Thisis
becausewewantthelevelofsensorynoisetoremainconsistent
acrosstrials.
Next,wecanspecifytransitionprobabilitiesinthe Bmatrixas
identitymatrices,10 astonesdonotchangewithinalower-level
trial.
p
(
stone
τ+1|stone
τ
)
=
B{1} =
[
1 0
0 1
]
Herecolumns(lefttoright)arehightoneandlowtonestates
at timeτ, while rows (top to bottom) are high tone and low
tone states at timeτ + 1. Here there is no need to separate
thegenerativeprocessfromthegenerativemodel,sowedonot
specifyaseparate bmatrix.
We do not include preferences or policies at this level, so
wenowsimplyassigneachvariabletothe mdpstructure.For
convenience when later linking this to the higher-level model
below,wewilldenotethisstructurewithan‘ _1’asfollows:
mdp_1.D= D
mdp_1.d= d
mdp_1.A= A
mdp_1.a= a
mdp_1.B= B
Forconsistencywithhowwelinkfirst-andsecond-levelmod-
elsbelow,wethenset MDP_1= mdp_1andclear mdp_1.
Now we move on to specifying the second-level model. To
keepthevariablesseparate,wewilldenoteeachmodelvariable
witha‘ _2’forthislevel.
10 As a brief general note, the columns of all matrices are put through a
softmax function in thespm_MDP_VB_X_tutorial.m script after the addition
of negligibly low values to each entry to prevent the problem that log(0) is
undefined.Assuch,evenspecificationofidentitymatricesfortransitionbeliefs
willnotcompletelyruleoutthepossibilitythatstatescouldchangeovertime
inthefaceofverystrongobservationalevidencetothecontrary.
44
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Here,wewillincludeonehiddenstateforeachpossible se-
quence oftones:
p
(
s
sequence
τ=1
)
=
D_2{1} = [1 1 1 1 ]′
This indicates that initially there is an equal probability of
eachtonesequence(lefttoright:‘allhigh’,‘alllow’,‘high-low’,
‘low-high’).Noteagainthatusingfour1shereisjustforconve-
nience,asthisvectorwillsubsequentlyberunthroughasoftmax
functionandeach1willbecomea.25.
Herewemustalsoincludeasecondhiddenstatefactorthat
encodesbeliefsaboutthe time point within a trial(e.g.,‘firsttone’,
‘secondtone’,etc.).Thisincludes(fromlefttoright)timepoints
for4tones,adelayperiod,andthenareportingperiod:
p
(
stime
τ=1
)
=
D_2{2} = [1 0 0 0 0 0 ]′
Thisindicatesthattheagentalwaysstartsinthe‘time1’state.
Wealsoincludeareportingstatefactor,correspondingtothe
agenteithernotyetreporting(leftentry),reporting‘sametone’
(middleentry),andreporting‘differenttone’(rightentry)atthe
endofthetrial:
p
(
s
report
τ=1
)
=
D_2{3} = [1 0 0 ]′
Thisindicatesthattheagentalwaysstartsinastateofnotyet
havingmadeareport.
Finally, we allow the agent to build up prior beliefs over
timewithrepeatedtrials.Inthiscase,becausetheagent’sbeliefs
initiallymatchthegenerativeprocess,wecansimplyset:
Dir(d)=
d_2= D_2
Next,wemustspecifythelikelihoodmappingsforthesecond-
levelAmatrix.Because time in trialisastatefactor,thisbecomes
somewhatmorecomplex.Specifically,wearenowrequiredto
specifythetypeoftone at each time pointthatisexpected under
each sequence.Todoso,wecanspecifythematricesasfollows.
Forconvenience,wecanfirstspecify:
p
(
olevel 1tone state
τ |ssequence, time, report
τ
)
=
for i = 1: 6
for j = 1: 3
A_2{1} (: , : , i, j)=
[
1 0
0 1
1 0
0 1
]
end
end
Thissaysthatforthefirstsixtimepoints( i = 1: 6),andfor
allthreechoicestates( j = 1: 3),thefirstandthirdsequence
states(i.e.,‘allhightones’and‘hightonesfollowedbylowtone’;
columns1and3)areassociatedwiththe‘hightone’observation
(toprow),whereasthesecondandfourthsequencestates(i.e.,‘all
lowtones’and‘lowtonesfollowedbyhightone’;columns2and
4)areassociatedwiththe‘lowtone’observation(bottomrow).
Then,wecanadjustthissothatthedeviationsequences(‘low
tones followed by high tone’ and ‘high tones followed by low
tone’;columns3and4)areassociatedwiththeoppositetone
mappingatthefourthtimepoint( i = 4):
p
(
olevel 1tone state
τ |ssequence, time=4, report
τ
)
=
for i = 4
for j = 1: 3
A_2{1} (: , : , i, j)=
[
1 0
0 1
0 1
1 0
]
end
end
Thesecondoutcomemodalityatthehigherlevel(whichdoes
notcorrespondtoalower-levelstatefactor)isfeedbackabout
whetherachosenreportwascorrectorincorrect.Here,weneed
tospecifythattheagentwillobserve‘correct’feedback(atthefi-
naltimepoint)incaseswhereitsreportmatchestheappropriate
sequence(row3),and‘incorrect’feedbackotherwise(row2).To
dothis,wecaninitiallyspecifythatnofeedback(‘null’;row1)
willbeobservedacrossalltimepoints:
p
(
ofeedback
τ |ssequence, time, report
τ
)
=
for i = 1: 6
for j = 1: 3
A_2{2} (: , : , i, j)=
[ 1 1
0 0
0 0
1 1
0 0
0 0
]
end
end
Thenwecanspecifythatattimepoint i = 6,iftheagent
reports‘same’( j = 2),thenitwillreceivecorrectfeedbackwhen
itisoneofthefirsttwo(standard)sequencesandincorrectfor
eitherofthesecondtwo(deviant)sequences:
p
(
ofeedback
τ |ssequence, time=6, report=′same′
τ
)
=
for i = 6
for j = 2
A_2{2} (: , : , i, j)=
[ 0 0
0 0
1 1
0 0
1 1
0 0
]
end
end
The we specify the opposite mapping if the agent reports
‘different’(j = 3):
p
(
ofeedback
τ |ssequence, time=6, report=′different′
τ
)
=
for i = 6
for j = 3
A_2{2} (: , : , i, j)=
[ 0 0
1 1
0 0
0 0
0 0
1 1
]
end
end
Aswiththefirst-levelmodel,tocontroltheprecisionofthe
mappingbetweenfirst-andsecond-levelstatesinthegenerative
model, we can use aprecision_2 parameter. To do so, we can
specify:
Dir(a)=
a_2= A_2
precision_2= 2;
a_2{1} =spm_softmax(precision_2∗log (A_2{1} +exp(−4)))
Thisresultsinaminoramountofnoiseinthemessagespassed
between levels. As with the first level, we also multiply this
parameterby100to(effectively)preventlearning.
Next,wemustspecifythetransitionmatricesforthesecond
level.Inthiscase,thesequencetypeisstablewithinatrial,so
45
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
thisshouldbeanidentitymatrix(columns:statesattime τ,rows:
statesat τ + 1):
p
(
s
sequence
τ+1 |ssequence
τ
)
=
B_2{1} =
⎡
⎢⎣
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
⎤
⎥⎦
Time in trial should progress forward (e.g., ‘Time 1’ should
‘transitionto‘Time2’,andsoforth.Assuch:
p
(
stime
τ+1|stime
τ
)
=
B_2{2} =
⎡
⎢⎢⎢⎢⎢⎣
0 0
1 0
0 1
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
1 0
0 1
0 0
0 0
0 0
1 1
⎤
⎥⎥⎥⎥⎥⎦
Finally,reportstatesareundercontroloftheagent.Inthis
case,therearethreeactions:
p
(
s
report
τ+1 |sreport
τ , U = no report
)
=
B_2{3} (: , : , 1)=
[ 1 1 1
0 0 0
0 0 0
]
p
(
s
report
τ+1 |sreport
τ , U = report ′same′)
=
B_2{3} (: , : , 2)=
[ 0 0 0
1 1 1
0 0 0
]
p
(
s
report
τ+1 |sreport
τ , U = report ′different′)
=
B_2{3} (: , : , 3)=
[ 0 0 0
0 0 0
1 1 1
]
Thesethreematrices(from1–3indimension3)correspondto
theactions( U)ofmoving(fromanystate)tothe‘noreport’state,
‘reportsame’state,and‘reportdifferent’state,respectively.Next,
wemustspecifytheallowablesequencesofactions(i.e.,policies).
Inthiscase,weincludetwopolicies(twocolumns)andonerow
pertimepoint.Therearenoactionsforthefirststatefactor,so
(number= action,column = policy,row = timepoint):
πsequence =
V_2(: , : , 1) =
⎡
⎢⎢⎢⎣
1 1
1 1
1 1
1 1
1 1
⎤
⎥⎥⎥⎦
Therearealsonoactionsforthesecondstatefactor:
πtime =
V_2(: , : , 2)=
⎡
⎢⎢⎢⎣
1 1
1 1
1 1
1 1
1 1
⎤
⎥⎥⎥⎦
Forthethirdstatefactor,theagentmustwaituntilthelast
time point and then either select the ‘report same’ or ‘report
different’actions:
πreport =
V_2(: , : , 3)=
⎡
⎢⎢⎢⎣
1 1
1 1
1 1
1 1
2 3
⎤
⎥⎥⎥⎦
Lastly,wemustprovidetheagentwithpreferencesthatwill
motivate accurate reporting. For the first outcome modality
(tones),theagenthasnopreferences(columns = timepoint,rows
[toptobottom] = hightone,lowtoneobservation):
Clevel 1tone state =
C_2(: , : , 1)=
[
0 0 0
0 0 0
0 0 0
0 0 0
]
For the second outcome modality (accuracy feedback), the
agentpreferstoreceive‘correct’feedbackatthelasttimepoint
(column6,row3)andfinds‘incorrect’feedbacktobeaversiveat
thelasttimepoint(column6,row2):
Cfeedback =
C_2(: , : , 2)=
[ 0 0 0
0 0 0
0 0 0
0 0 0
0 0 −1
0 0 1
]
Asalreadymentionedwhenbuildingtheexplore–exploittask
model, each column in this matrix is put through a softmax
functionandthenconvertedintolog-probabilities.Havingnow
specified the second-level model, we will place each of these
matricesintoitsownmdpstructure:
mdp.D= D_2
mdp.d= d_2
mdp.A= A_2
mdp.a= a_2
mdp.B= B_2
mdp.C= C_2
mdp.V= V_2
We then need to connect the lower-level model with the
higher-levelmodelasfollows:
mdp.MDP= MDP_1
Next,weneedtoprovideamatrixspecifyingwhichoutcome
modalitiesatthesecondlevel(columns)correspondstowhich
statefactorsatthelowerlevel(rows)withina‘link’field.Here,
thefirstoutcomeatthesecondlevel(‘tones’)correspondstothe
firststatefactoratthefirstlevel:
mdp.link= [10]
Inthiscase,thematrixonlyhasasinglerowbecausethereis
onlyonestatefactoratthelowerlevel.
Lastly,weneedtosetthevalueoftheERP‘reset’or‘decay’
parametermdp.erp,whichatthestartofeveryepochofgradient
descentisusedtoresettheposterioroverstatesbydividingthe
posteriorbythevalueoftheparameter(i.e.,highervalues = more
resetting).Settingthevalueoftheparameterisentirelyupto
thediscretionofthemodeler,dependingonassumptionsabout
theparticularneurocognitiveprocessunderstudy.Inempirical
work, this parameter could be fit to observed ERP responses.
Intheexperimentalparadigmwesimulatehere,thetonesare
playedtotheparticipantinquicksuccession,soweassumethat
theposteriorateachtimestepcarriesoveranddoesnotdecay
betweenpresentations.Assuch,weset mdp.erp= 1.If,however,
weweretryingtomodelataskwithlongertimeperiodsbetween
updating(e.g.,asubjectnavigatingamaze),somedegreeofdecay
46
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
couldbeappropriate( mdp.erp = 4isthedefaultvalueinthe
currentspm_MDP_VB_X.mscript).
Asbefore,wecannowrunthisstructurethroughthestandard
routinetogeneratesimulatedbehavioralandneuronalresponses
ofanexampletrial.
MDP= spm_MDP_VB_X_tutorial(mdp);
Tosimulateourtwoconditionsofinterest, local deviation +
global deviation, andlocal deviation + global standard, we will
simulate10sequentialtrialsforeachcondition.Forbrevity,we
willnotdescribethecodethatimplementsthishere.Instead,we
directreaderstothe Step_by_Step_Hierarchical_Model.mscript,
whichhasdetailedcommentsdescribingeachofthenecessary
steps.Forbothconditions,thefirstninetrialsconsistofthree
hightonesandafourthlowtone.Onthetenthtrialofthe local
deviation + global standard condition,thetrialagainconsistsof
three high tones and a fourth low tone. On the tenth trial of
the local deviation + global deviation condition, the tenth trial
insteadconsistsoffourhightones,therebyviolatingtheglobal
regularity.Fig.14showsplotsofsecond-levelbeliefupdatingand
policyselection,analogoustothesingle-levelmodelplotsshown
in Fig. 8. The model performed at ceiling with 100% accuracy
whenclassifyingthelaststimulusinthesequenceassameor
different.
Thenextstepistovisualizetheresultingsimulationsinor-
dertomakeempiricalpredictionsaboutbehaviorandneuronal
responses.
6.3. Plotting hierarchical models
Tovisualizetheresultingbeliefupdatingandsimulatedneu-
ronal responses for this hierarchical model, we have provided
amodifiedversionoftheplottingscriptprovidedinthefreely
availableSPMroutines:
MDP= spm_MDP_VB_ERP_tutorial(MDP);
Thisplottingscriptshowsthesimulatedfiringratesassociated
withbeliefupdatingateachlevel.Italsoshowsthesimulated
ERPs (summed first derivative of the firing rates) that would
beexpectedtobegeneratedduringthesimulatedtask.Fig.15
showsthefirstandtenthtrialfromeachcondition(i.e.,before
and after building up prior beliefs favoring some states over
others).Noticethatonthefirsttrialofboththe local deviation +
global deviationconditionandthe local deviation + global standard
condition, the high firing rates at the first level reflect a high
posterior confidence in the tone each time it is presented. In
contrast, after hearing the first tone, the firing rate is evenly
spreadbetweenboththe‘high’and‘high-low’hiddenstatesat
thesecondlevel,reflectingtheagent’suncertaintyaboutwhich
typeofsequenceisbeingpresented(i.e.,sincebothsequences
predict‘high’tonesforthefirstthreetimesteps).Note,however,
thatthefourthtoneonthefirsttrialgeneratesanincreasedfiring
rateforoneofthehiddenstates,andthefiringratedrastically
decreasesfortheotherstate(dependingonwhethera‘high’or
‘low’toneispresented).Bythetenthtrial,theagenthasahigh
priorexpectationthatitwillexperiencea‘high-low’sequence,
reflectedinthehighfiringratesforthissecond-levelhiddenstate
from early time points. In thelocal deviation + global standard
condition,thisexpectationisconfirmed,leadingtolittlechange
in second-level firing rates. Importantly, however, in thelocal
deviation + global deviationcondition,thisexpectationisviolated,
becausetheagentispresentedwithfourhightones.Thiscreates
arapidswitchinposterior confidencefromthe‘high-low’hidden
statetothe‘high’hiddenstateatthefourthtimestep,generating
averystrongandrapidshiftinsecond-levelbeliefs.
This brings us to simulated ERPs, which reflect the rate at
whichposteriorbeliefschangewithineachepochofbeliefup-
dating,summedoverhiddenstatesateachlevelofthemodel
(i.e., similar to the aggregate signal measured by EEG). At the
firstlevel,therepeatedpresentationofhightonesgeneratessmall
ERPs,whereasdeviationsfromthispatternatthefourthtimestep
(localdeviations)generatealargeramplitudemismatchresponse
becauseposteriorbeliefsaboutthetonechangerapidly.Impor-
tantly,whenthelocaldeviationresponseissubtractedfromthe
localstandardresponse,wecanreproducetheclassicmismatch
negativity effect (MMN; see Fig. 16). At the second level, the
repeatedoccurrenceofthreehightonesandonelowtonefor
thefirstninetrialscreatesastrongpriorexpectation(through
the increase in concentration parameters in theD vector) for
the‘high-low’sequencestate.Whenthemodelisunexpectedly
presentedwithfourhightonesonthetenthtrial,thisexpectation
violationgeneratesarapidchangeinbeliefsandacorrespond-
inglylargesecond-levelERPresemblingtheP300(seeFigs.15
and16).Fig.16showscustom-madeERPplots,whichisolatethe
contrastsofinterest.Again,forthesakeofbrevity,wewillnotde-
scribethecodethatgeneratesthisplotinthemaintext,butdirect
interested readers to theStep_by_Step_Hierarchical_Model.m
scriptformoredetails.
7. Fittingmodelstobehavior
Sofar,wehavefocusedonsimulatingbehaviorandestablish-
ingthefacevalidityofactiveinferenceusingcanonicalexamples
fromthedecision-makingandelectrophysiologicalliterature.In
thissection,wedemonstratehowactiveinferencemodelscan
beusedinempiricalstudies.Morespecifically,wewilldescribe
how one can estimate the model parameter values that best
explain participant behavior during an experimental task. This
approachhasbeenemployedinseveralrecentstudiesthathave
usedactiveinferencemodelstoaccountforbehaviorduringtasks
designedtostudyawiderangeofphenomena— suchasat-
tention,risk-taking,approach-avoidanceconflict,explore–exploit
behavior, and interoception (Mirza, Adams, Mathys, & Friston,
2018; Schwartenbeck et al., 2015; Smith et al., 2021d, 2021c,
2020b,2020c,2021e,2020e).Ineachofthesestudies,amodel
wasusedtoevaluatethepriorbeliefsthatparticipantsmostlikely
heldwhenperformingatask(i.e.,thepriorbeliefsthatwould
havegeneratedtheirbehaviorinamodel).Allnecessarystepsfor
carryingoutthisapproacharedescribedbelow,withthegoalof
preparingthereadertouseactiveinferencemodelsintheirown
empiricalstudies.Bytheendofthissection,thereadershould
understandhowtofitamodeltoparticipantbehavior,howto
performanumberofdiagnosticcheckstoensurethevalidityof
parameterestimates,andhowtouseparameterestimateswithin
group-levelBayesianmodels.
As stated above, empirical applications of active inference
requirefittingataskmodeltoparticipantbehavior.Whenfitting
a model to behavior, one would like to find the parameters
that maximize the posterior probability of a model given that
behavior,p(model|participantbehavior).Assumingaflatpriorbe-
liefovermodels,thisposteriorisproportionaltothelikelihood
term,p(participantbehavior|model).Thus,manyfittingapproaches
(estimationalgorithms)trytofindtheparametersthatmaximize
thislikelihood— referredtoas maximum likelihood estimation
(MLE).Insomecases,onemightalsohavereasontoexpectthat
certainmodelsaremorelikelythanothersapriori.Inthiscase,
onecanalsoincorporateaninformativepriorbeliefovermodels,
47
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.17. Illustrationofdifferentstepsinmodelfitting,modelchecking,andsubsequentanalysismethods. TopLeft:Single-subjectparameterestimatesgeneratedby
thescriptsprovidedwiththistutorial.Thisexampleincludesestimationoftwoparameters: alpha (actionprecision)and RS (risk-seeking).Theleftsubplotshows
thetrajectoryofparametervaluesasfittingprogressed(fromredtoblue).Themiddlesubplotshowshowevidenceforthemodel(givenparticipantdata)increased
during estimation (via gradient descent on free energy; here shown as ascent on negative free energy). The right subplot depicts how parameters moved from
estimation priors to posterior values (pink lines indicate 95% Bayesian confidence intervals). Note that, for clarity, prior values for all parameters in these plots
are given a common reference value of 0, such that posterior values are shown as deviations from those priors. For example, the prior values foralpha andRS
were16and5,respectively.Theplotsthereforeshowthatposteriorsfor alpha deviatedtoavaluebelow16andthatthosefor RS deviatedtoavalueabove5.
TopRight: Priorandposteriormeanestimatesfor alpha andRS forasinglesimulatedparticipant.Thisshowshowposteriorestimatesforbothparametersmove
towardthetrueparametervalues. BottomLeft: Posteriorvariancesandco-variancesforparametersatthesingle-subjectlevel,hereshowingthatbothvariances
andco-varianceswerelow.Theseresultscanbereproducedbyrunningthe Sim = 4optioninthesupplementary Step_by_Step_AI_Guide.mcode.BottomMiddle:
Exampleofarecoverabilityanalysisfor alpha in6simulatedparticipants.Thisplotshowsthe(inthiscasestrong)correlationbetween alpha valuesestimatedfrom
simulatedbehaviorineachparticipantandthetrueparametervaluesusedtogeneratethatsimulatedbehavior. BottomRight: Outputofgroup-levelparametric
empirical Bayes’ (PEB) analyses (scripts provided insupplementary code). This example included estimation of three parameters for six simulated participants,
whereparameters1,2,and3correspondto alpha (actionprecision), RS (risk-seeking),and eta (learningrate),respectively.Thetoprow(lefttoright)showsthe
evidence(foreachparameter)fordifferencesfrom0,differencesbetweentwogroups(simulatedashavingdifferent RS values,i.e.,parameter2),andrelationships
to(arbitrarilyspecified)agevaluesinmodelsassumingeffectsforallparameters.Thebottomrowshowsanalogousresultsforreducedmodelsthathavegreater
evidencethanthefullmodels(here,nogroupdifferencein alpha [parameter1]or eta [parameter3],andsomeeffectsofageon alpha andeta).Recallthatthese
results are based on a very small sample of only 6 simulated participants, which will generally be unreliable. They are for illustration only, and the identified
relationshipswiththearbitrary‘age’valuesshouldnotbetakenseriously.Notethatwehaveomittedsomeadditionalplotsthatarealsogenerated.Thisisbecause
thesearetheautomaticoutputofscriptsoriginallydesignedfordynamiccausalmodelinginneuroimaging,andnotallofthemareusefulinthepresentcontext.
Theseresultscanbereproducedbyrunningthe Sim = 5optioninthesupplementary Step_by_Step_AI_Guide.mcode,whilealsosettingtheoption PEB = 1(note
that,becauseoutcomesaresampledfromprobabilitydistributions,resultswillnotbeidenticaleachtime)..(Forinterpretationofthereferencestocolorinthisfigure
legend,thereaderisreferredtothewebversionofthisarticle.)
p(model).11 Regardlessofthespecificapproach,thegoalofany
fittingprocedureistofindthesetofparametersthatwouldbest
reproduce/predicttheactualbehaviorofaparticipant(i.e.,with
thehighestprobability)whenrunningsimulationsusingamodel
(whileinsomecasesalsoincorporatinganypriorknowledgeone
mighthave).
Todothis,oneneedstofeedthetrial-by-trialobservations
madebyparticipants(e.g.,cues,wins/losses,etc.)intothemodel
and look at the actions predicted by the model (i.e., posterior
probabilitiesoveractions).Onecanthencomparethesepredic-
tionstotheactionsaparticipantactuallychose.Undersomesets
ofparametervalues(e.g.,priorexpectations,precisions,etc.),the
model’spredictionsmaynotmatchbehaviorwell(i.e.,theprob-
abilityofaparticipant’sactionsunderthemodelmaybelow).
However,bysearchingthroughdifferentpossiblecombinationsof
parametervalues,thebestcombinationcanbefoundforagiven
11 An example of this approach is maximum a posteriori (MAP) estimation,
inwhichanalgorithmtriestofindtheparametervalues(i.e.,pointestimates)
thatmaximizethevalueoftheposteriorasopposedtothelikelihood.
participant.Note,however,thatthiswillonlybethebestcombi-
nationpossible for that model.Thisdoesnotmeanthatthemodel
hashighexplanatorypower(e.g.,thebestparametercombination
foronemodelmightleadtoanaverageactionprobabilityof0.4,
whileanothermodelmightreach0.7,etc.).Thisiswhyitisalso
importanttocomparetheexplanatorypowerofdifferentmodels.
Throughout this section, we encourage the reader to fol-
lowalonginthecompanionMATLABscript( Step_by_Step_AI_
Guide.m).Thiscanbefoundinthe supplementarycode files,
as well as at: https://github.com/rssmith33/Active-Inference-Tu
torial-Scripts. At the top of this script, if you setSim = 4 it
willperformparameterestimationonasinglesetofsimulated
behavioraldatafromtheexplore–exploittaskmodel(seethetop
panelsandbottom-leftpanelofFig.17forexampleoutputs).If
youset Sim = 5itwillsimulatebehavioraldatafromtwomodels
(onewithtwoparametersandonewiththreeparameters;see
below)forafewsyntheticparticipantsandthenperformmodel
comparison.Itwillalsoassesshowwelltheestimatedparameter
valuesmatchwiththetrueparametervaluesusedtogeneratethe
48
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
simulatedtaskbehavior(i.e.,itwilloutputcorrelationmatrices
andassociated p-values;bottom-middlepanelofFig.17).Thisis
importanttocheck(asdescribedbelowinrelationtoparameter
recoverability)toensurethatparameterscanbeestimatedreli-
ably(thisissometimesreferredtoas modelidentifiability).All
the code for performing these steps is included and described
in the script comments. All examples here will be based on
theexplore–exploittaskmodel,whensimulatingbehaviordur-
ing the case of reversal learning used above (see Fig. 11). We
willestimatethealphaparameter( α)encodingactionprecision
(i.e., inverse temperature) as well as a risk-seeking parameter
(describedbelow).Formodelcomparison,wewillalsoestimate
learningrate( η)inasecondmodel.
Asmentionedabove,findingtheoptimalcombinationofpa-
rametervaluestoaccountforaparticipant’sbehaviorrequiresa
modelfitting(parameterestimation)procedure.Therearemany
differentprocedures(estimationalgorithms)thatareavailable,
eachwiththeirstrengthsandlimitations.Thesimplestapproach
iscalledagridsearch,whereeachpossiblecombinationofpa-
rametervalues(withinsomespecifiedrangeofvalues)istried
one-by-one,andthentheonethatbestreproduces(e.g.,max-
imizesthelikelihoodof)behaviorischosen.However,thisap-
proachislimitedtofairlysimplemodelswithasmallnumber
of parameters, and it can also lead to overfitting (i.e., finding
parameter values that reproduce random aspects of behavior).
Inmorecomplexmodelswithalargernumberofparameters,
exhaustivesearchoftheparameterspacebecomesintractable.
Thus, a number of other algorithms have been developed. In
this section, we will focus on an estimation technique called
variationalBayes(i.e.,aspecificvariantofthisapproachcalled
variationalLaplace;(Friston,Mattout,Trujillo-Barreto,Ashburner,
&Penny,2007)),whichisbasedonexactlythesamefreeenergy
minimizationideasdescribedabove. 12Namely,variationalBayes
startswithapriordistributionoverparametervalues,whichacts
asthestartingvalueofanapproximateposterior(or‘proposal’)
distribution.Viagradientdescentonvariationalfreeenergy,this
approximateposteriorisadjusteduntilconvergencetoastable
minimumvalue,providingposteriorestimatesofmodelparam-
etervalues.However,thedetailedmathematicsunderlyingthis
techniquearequiteadvancedandbeyondthescopeofthistuto-
rial.Therefore,ourfocushereisonpracticalapplications,withthe
aimofequippingreaderswithoutadvancedmathematicaltrain-
ingtostillbeabletomakeuseofsuchapproachesappropriately.
Wewillnowgooverasetofconcretestepsthatcanbetaken
tousevariationalBayesformodelfitting.Aswearefocusingon
theexplore–exploittaskmodel,itmayhelptoglancebackatthe
model/taskdescriptioninearliersectionsbeforemovingon.
Thefirststepistoplacetrial-bytrial-observationsandpartic-
ipantactionsintoan mdp structure.Intheexplore–exploittask
model,foreachtrialthiswouldmeanspecifyingtheobservations
(mdp.o)aparticipantmadeforthethreeoutcomemodalitiesat
thethreetimepointsineachtrial,andthenspecifyingthetwo
actionstaken( mdp.u).Forexample,ifontrial#1theparticipant
chosetotakethehintandthenchosetheleftmachine,thiswould
be:
mdp(1).u =
[
1 1
2 3
]
12 Another important class of estimation algorithms uses Markov Chain
MonteCarlo(MCMC)methods(Neal,1993).Thesemethodsinvolvesequentially
sampling from a distribution according to specific sets of rules that try to
find locations under that distribution with high probability. In this case, the
probability of a participant’s actions would be evaluated under a sequence of
parametervaluecombinations(withaninitiallyrandomstartingpoint)untila
sufficientapproximationtothetruedistributionwasgenerated.
mdp(1).o =
[ 1 2 1
1 1 3
1 2 3
]
Here,timegoesfromlefttoright.For mdp.u,actionsforeach
state factor correspond to each row from top to bottom. For
mdp.o,timealsogoesfromlefttorightandrowscorrespondto
outcomemodalities.Theparenthetical‘(1)’forthe mdp denotes
trial#1.Here,for mdp.u,rememberthereisonlyone‘action’for
thefirststatefactor(i.e.,theparticipantdoesnothavecontrol
of which machine will most likely lead to a win). So, the top
row is simply a 1 for both time points. For the second state
factor (choice), the participant first chose the hint (action #2)
and then chose the left machine (action #3). Formdp.o, the
firstrowindicates‘null’(observation1),‘lefthint’(observation
#2),andthenbackto‘null’.Thesecondrowindicatestwo‘null’
observationsfollowedbya‘win’(observation#3).Thelastrow
simply corresponds to the choices (‘start’, ‘take hint’, ‘choose
left’).Observationsandbehaviorforeachtrialneedtobeinserted
inthissameway(e.g., mdp(2),mdp(3),etc.untilthefinaltrial
number).
Thesecondstepistochoosethemodelparametersyouwant
toestimateandwhichyouwanttoholdfixed.Ultimately,one
may want to try estimating different models and/or different
numbersofparametersandthencomparethemtofindwhich
best accounts for behavior, as we describe further below. In
thisexample,wewillfirstconsiderestimatingtwoparameters:
learningrate( η)and‘risk-seeking’( RS).Thelattercorrespondsto
howstrongthepreferenceistowinmoneyinthe Cmatrix:
Cwin =
C{2} =
⎡
⎣
0 0 0
0 −1 −1
0 RS RS
2
⎤
⎦
Asreviewedabove,learningratescalesthesizeofthe‘count’
thatisaddedtothe(Dirichlet)concentrationparametersaftera
newobservation.The RS parametercontrolstheexplore–exploit
trade-off.InthesimulationsshowninFigs.10and11,wesaw
that,as RS valuesgoup,theprobabilitythataparticipantwill
takethehintgoesdown.Thatis,theywilltendto‘riskit’and
simplyguessleftorrightinhopesofwinningthelargeramount
ofmoney.
WhenusingvariationalBayes,thenextstepistochooseaset
ofestimationpriors.Thesearenotthepriorbeliefsofapartic-
ipant,buttheinitialparametervaluesthatareevaluatedduring
modelfitting.Estimationpriorsincludebothapriormeananda
priorvariance,eachofwhichcanbesetwithinthesupplementary
‘Estimate_parameters.m’ script, as described further below. A
simplewayofthinkingaboutthevariationalBayesalgorithmis
that it starts from the chosen estimation priors (i.e., the prior
means) and slowly moves away from them to find the com-
bination of parameter values that best explains a participant’s
behavior,whilealsobalancingtheassociatedcostofincreased
modelcomplexitythatisincurredwhentheparametersmove
toofarawayfromthepriormeans.Morespecifically,variational
Bayesisaccomplishedbyperformingagradientdescenton VFE
(i.e.,thesameprocessactiveinferencemodelsusetoaccomplish
perception,learning,andactionselection).Inthiscase,thegra-
dient descent starts with the prior means, and then evaluates
the log-probability of a participant’s actions (e.g., sequence of
choices). This log-probability is evaluated using the posterior
beliefsaboutactionthatthesubjectwouldhavehad,giventhe
priorsinquestionandtheoutcomestheyobserved.Thescheme
thenevaluatesneighboringparametervaluesandcontinuesinthe
directionofincreasinglikelihood,untilacombinationisfound
withnoneighboringvaluesthatimprovethefit.
49
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
However,becauseweseeka VFE minimum,whichincludes
bothcomplexityandaccuracyterms,thealgorithmwillnotsim-
plymaximizemodelaccuracy.Itwillalsotrytominimizehow
far parameter estimates move away from the mean values of
theestimationpriors.Inotherwords,parameterestimatesfora
givenparticipantwillrepresentvaluesthataccuratelyreproduce
that participant’s behavior while moving as little as possible
fromthepriorvalueschosenbytheexperimenter.Importantly,
complexityminimizationalsodependsonthepriorvariancesthat
arechosen.Settingasmallpriorvariancewilllendstrongweight
tocomplexityminimization;incontrast,settingalargevariance
forestimationpriorswillleadastrongerweighttobeplacedon
maximizingaccuracy(butwithagreaterchanceforoverfitting).
Howtosetthepriorvariancecanbeacrucialchoiceinvariational
Bayes,becauseposteriorparameterestimatescaninsomecases
besensitivetothechoiceofpriormeans.Whenavailable,prior
meanscanbebasedonpreviousstudies.However,ifprevious
literatureisnotavailabletodrawfrom,onemightconsiderspec-
ifyingalargepriorvariancetoreflectthisuncertainty(inwhich
case the complexity cost will play little-to-no role). This issue
of setting appropriate estimation priors, and diagnosing when
they may be inappropriate, is one of several important model
checkingproceduresthatshouldbetakentoconfirmthevalidity
ofparameterestimates,whichwereturntobelow.
Beforemovingforward,however,itisimportanttohighlight
aspecificsubtletyintheabovescheme.Namely,weareusing
variational Bayes to estimate the parameters that underwrite
variationalinferencewithinthebrainofastudyparticipant.In
otherwords,therearetwogenerativemodels:first,a subjective
model,whichisthePOMDPweassumetheparticipantisusing
toplantheirresponses.Second,thereisan objectivemodel that
theexperimenterspecifiesintermsoftheestimationpriorson
parametersofthesubjectivemodel.Akeyconceptualpointhere
is that the parameters of the subjective model can always be
interpretedaspriors;eitherexplicitlyorimplicitlyintermsof
thestructureorformofthesubjectivemodel.Thisisimportant
becauseofsomethingcalledthecompleteclasstheorem(L.D.
(Brown,1981;Wald,1947).Thistheoremsaysthatforanypairof
rewardfunctions(i.e.,preferences)andchoicebehavior,thereex-
istsomepriorbeliefsthatrenderthechoicesBayesoptimal.This
meansthatanybehaviorcanbedescribed,underidealBayesian
assumptions,giventherightsetofpriorbeliefs.Itistheseprior
beliefsthatprovideatheoreticallycompletecharacterizationof
anygivenparticipant,inanygivenexperimentalparadigm.
Afterchoosingafinalsetofestimationpriors,andobtaining
parameterestimatesforeachparticipant,itisalsoimportantto
confirmwhatiscalled parameterrecoverability (alsosometimes
referred to as testing whether the model isidentifiable or in-
vertible). What this means is that you need to be sure that, if
simulatedbehaviorweregeneratedfromamodelwithagivenset
ofparametervalues,thattheestimationalgorithmwouldprovide
reasonablypreciseestimatesthatapproachtheparametervalues
thatgeneratedthebehavior.Forexample,assumeagivenpartic-
ipant’sposteriorparameterestimateswere RS = 3.2and α = 3.7
(e.g., using estimation priors ofRS = 4 andα = 2). Assess-
ingrecoverabilitywouldrequire:1)puttingthevaluesofthese
posteriorparameterestimatesintothetaskmodel,2)generating
simulatedbehaviorusingthosevalues,3)feedingthatsimulated
behaviorintothefittingalgorithm,and4)examininghowsimilar
the resulting parameter estimates are to the parameter values
that you used to generate the simulated behavior in the first
place.Ifthealgorithmreturnsestimatesthatmoveintheright
directionfromtheestimationpriorsandapproachthevaluesthat
generatedthedata(e.g., RS = 3.3and α = 3.5),thenwecan
bemoreconfidentthattheparticipant’sestimatesarecapturing
somethingmeaningfulandreliableabouttheirdecisionprocess.
Theplotonthetop-rightofFig.17illustratesacaseinwhich
posteriorestimatesmovefromestimationpriorstowardthetrue
values(thisplotcanbereproducedbysetting Sim = 4whenyou
runthe Step_by_Step_AI_Guide.mcode).Ifthefittingalgorithm
insteadreturnedestimatesthatwerefartherawayandmovedin
thewrongdirectionfromtheestimationpriors(e.g., RS = 4.9
andα = 1.3),thiswouldsuggestthattheparameterestimatesfor
thatparticipantmaynotbereliable(e.g.,adifferentcombination
ofparametervaluesmightreproducetheirbehaviorequallywell).
Thus, before interpreting parameter estimates in real par-
ticipants – and using them in subsequent group analyses – it
is important to confirm in simulated data that the estimates
providedbythefittingalgorithmmatchwellwith(e.g.,aresig-
nificantlycorrelatedwith)theparametersusedtogeneratethe
simulatedbehaviorinthefirstplace.Thisshouldbecheckedusing
thecombinationsofparameterestimatesfoundwhenfittinga
model to the true participant data in a study. For example, if
youhadthreeparticipantswithestimatedparametervaluesof
[RS, α]= [2.8, 4.1], [3.5, 6.2], [3.8, 8.1],thenthesethreecom-
binationscouldbeusedtogeneratesimulatedbehavior,andthis
simulatedbehaviorcouldthenbefedintothefittingalgorithm
to see if it returned results similar to the generative values
(e.g.,ifestimatedvalueswerehighlycorrelatedwiththoseused
togeneratethesimulatedbehavior).
Itisimportanttokeepinmind,however,thattheproducts
of Bayesian model inversion (here, parameter estimation) will,
generallyspeaking,neverbethesameastheparametersused
togeneratedata.Thisisbecausethereisusuallyasimplerway
ofgeneratinganygivensetofdata,whichthemodelinversion
willidentify.Inotherwords,inacertainsensethereareno‘true’
parameters— onlythebestexplanationinthesenseofOccam’s
principle.
If theSim variable is set toSim = 5 when you run the
Step_by_Step_AI_Guide.mcode,simulatedbehaviorwillbegen-
erated under several parameter values for the explore–exploit
taskmodelanditwillruncorrelationsbetweentheestimatedpa-
rametervaluesandtheparametervaluesthatactuallygenerated
thesimulatedbehavior.Inarepresentativeexamplesimulation,
recoverabilityappearedhighfor RS (r = .95)and α (r = .94).
In a model that also included learning rate (η), this parame-
terappearedrecoverableaswell( r = .75).Thebottom-middle
panelofFig.17showsascatterplotillustratingthecorrelation
betweengenerativeandestimatedparametervaluesfor α,which
theabove-mentionedcodewillreproduce.
Before assessing parameter recoverability (i.e., model iden-
tifiability), however, it is important to first understand more
abouttheconcretestepsforfitting.Asmentionedabove,ifyou
set Sim = 4 in theStep_by_Step_AI_Guide.m script, it will
generate simulated explore–exploit task behavior for a single
participant. It will feed this simulated behavior into a ‘DCM’
(forDynamicCausalModeling)datastructureintheappropri-
ate format for model fitting. It then calls the supplementary
Estimate_parameters.mscript we have provided, which takes
theDCMstructureasinput,runsthevariationalBayesroutinein
SPM12(spm_nlsi_Newton.m),andcalculatesthelog-likelihood
— that is, the sum of the log-probabilities of chosen actions
underthemodel.Inourscript,theDCMstructureincludesthe
following:
DCM.MDP(generativemodel)
DCM.U(participantobservations)
DCM.Y(participantactions)
DCM.field(specifiestheparameterstobefit)
50
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Withinthe Estimate_parameters.mfile,youwillseelocations
(startingonline47)wherethescriptsearchesfortheto-be-fitpa-
rameternames.Here,youcanentertheestimationpriors(means
andvariances)foreachparameter.Notethatsomeparameters
(e.g., learning rate) need to be between 0–1. For this reason,
theyareheretransformedintologit-spacesothattheestimation
routinedoesnotassessvaluesoutsideofthatrange.Similarly,pa-
rametersthatcanonlytakeonpositivevaluescanbetransformed
intolog-spacetoprecludenegativevalues.Forillustration,we
haveassumedthat RS canonlytakepositivevaluesandisthere-
forelog-transformed.Fartherdowninthescript(startingonline
133),thesevaluesarere-transformedoutoflogit-orlog-space
whentheyarefedintothemodelandoutputasfinalestimates.
Startingonline188,thelog-likelihoodfunctionloopsthrough
eachtrial,takestheprobabilityofaparticipant’sactionsinthe
model(storedin MDP.P;seeTable3),log-transformsit,andthen
sumsthelog-probabilities.Thecloserthisvalueisto0,thebetter
themodelfitsthebehavior.
When you run theStep_by_Step_AI_Guide.mscript (which
calls the others automatically) withSim = 4, a display will
alsoappear(asinthetop-leftpanelofFig.17)thatshowshow
the free energy changes over iterations. Note that these val-
uesincreasebecausetheyarenegativeandwillapproach0as
modelfitimproves.Ifthesevaluessteadilyincrease,thissuggests
thatfittingisconvergingtoareliableestimate.Iftheyinstead
fluctuateupanddowninconsistently,thiscouldsuggestoneor
moreproblemsthatneedtobeaddressedbeforemodelestimates
are considered reliable (e.g., poor choice of estimation priors,
problemswithparametervaluesremaininginvalidranges,es-
timatesgettingstuckinlocally[butnotglobally]optimalvalues).
Afailuretoconvergeisusuallyreadasausefuldiagnosticthatthe
estimationpriorsaresomehowmis-specified.Inotherwords,if
modelinversiondoesnotconvergegracefullywithinafewtensof
iterations,youmaywanttothinkaboutwhetheryourpriorsare
appropriate— orwhetheryouaretryingtofittoomanyparam-
eterstorathersparsedata.Thistypeofmodelcheckingiscrucial
toensurethatparameterestimatesarevalidandinformative.
Theestimatesresultingfrominitiallychosenpriormeanscan
also sometimes offer guidance in this regard. For example, if
estimatesforallparticipantstendtomovefarfromthechosen
priormeansinthesamedirection,thiscouldsuggestthatthese
estimationpriorswerepoorlychosen.Ifyouinitiallysetaprior
meanof RS = 1,forexample,andthennoticethatestimatesfor
allparticipantstendtomoveuptobetween3and5,thiscould
indicatethatapriorof RS = 4wouldbemoreappropriate(asit
appearstobeabetterpriorforthegroupasawhole)andmight
helppreventover-weightedcomplexitycoststhatcouldhinder
identification of individual differences. It is worth noting that
lettingdataguidethewayonechoosesestimationpriorsinthis
waycouldbeviewedassuspectundersomeinterpretationsof
Bayesianstatistics.However,itisalsopossibletoviewestimation
priors in variational Bayes as simply being starting values for
estimationthatcanbeoptimized(andmayneedtobeincases
wheresomestartingvalueswillleadgradientdescenttogetstuck
insuboptimallocalminima).Insomecasesitmaybeusefulto
trymultiplestartingvaluesandthenexaminewhetherposterior
estimatestendtoconvergetowardsimilarresults.Notealsothat,
althoughwedonotgointodetailhere,therearealsohierarchical
‘empiricalBayes’methodsthatcanbeusedtorigorouslyestimate
group-levelpriorsandthenre-estimateindividual-levelparam-
eters based on those group-level priors (Carlin & Louis, 1998;
Fristonetal.,2016b).
After convergence to the best estimates, the outputDCM
structurewillcontainadditionalfields.Thefollowingarerelevant
toourcurrentuse:
DCM.Ep:posteriormeanestimates(i.e.,expectations)foreach
parameter.
DCM.Cp:posteriorcovariancematrix(withposterior
parametervariancesonthediagonal).
DCM.F:thefinalfreeenergyvalueofthebestfitmodel.
Thefreeenergyvaluesforeachparticipantwilllaterbeused
for model comparison. The covariance values (i.e., the off-
diagonals inDCM.Cp) should be checked to make sure they
arenottoohigh(e.g.,>.8),oritwouldsuggesttheywerenot
independentlyestimatedandthateachestimatemaynotcarry
unique/reliableinformation.Thisisthesamekindofcheckyou
wouldapplytoanexperimentaldesign— toensureexplana-
tory variables are orthogonal and the model parameters can
beestimatedefficiently.Thebottom-leftpanelofFig.17plots
an example co-variance matrix (which will be reproduced by
the accompanying tutorial scripts;Sim = 4, as with previous
plots).
Whentestinghypothesesaboutwhichofseveralmodelsbest
explainsyourdata,onemustfiteachmodelandthencompare
howwelltheyeachfitbehavior.ForBayesianmodelcomparison
usingvariationalBayes,thismeanscomparingthefreeenergies
atthegrouplevel.Onecommonapproachistousearandom
effectsmodel,whichcanbedoneusingthe spm_BMS.mfunction
(availablewithinSPM12).Thisfunctiontakesasinputamatrix
containingthefreeenergiesforeachparticipantforeachmodel
(onecolumnpermodel).Forthedetailsofthisimplementation
of Bayesian model selection, see (Rigoux, Stephan, Friston, &
Daunizeau, 2014) and Stephan, Penny, Daunizeau, Moran, and
Friston(2009).Animportantoutputofthisfunctionisthe pro-
tectedexceedanceprobability (pxp)ofeachmodel.Inthiscase,
the model with the highestpxp has the most evidence. Often
therewillbeaclearwinner,with pxp = 1forasinglemodel
and0sfortheothers.Incaseswherethereisnoclearwinner
(e.g., pxp = [.48.52]), this will be important to note, as it
mayreflectinsufficientevidencefora‘best’model.Ifparameters
inbothmodelsarerecoverable,onemaywishtoconsiderthe
parametersofeachmodelinfurtherbetween-subjectsanalyses
(e.g., parameters in one model may have higher explanatory
valueforspecifictheoreticalquestions).Anotherusefulmodel-
checkingstepthatcouldbeinformativeinthiscaseistogenerate
simulateddatasetsfrommodelsthatincludedifferentnumbersof
parameters(e.g.,frommodelsthatdovs.donotincludealearning
rate) and then confirm that the correct model is identified by
modelcomparison(e.g.,ifthedataweregeneratedbyamodel
withonlytheRSand α parameters,andthenseveralpossible
modelswerefittothisdata,onewouldconfirmthatthemodel
including only these two parameters was identified as having
thehighest pxp).Asmentionedabove,setting Sim = 5inthe
Step_by_Step_AI_Guide.mscriptwillgenerate/estimatebehavior
forafewsimulatedparticipantsformodelswithandwithoutthe
learningrateparameter.ItwillthendoBayesianmodelcompar-
ison(thiswilltakeseveralminutes).Thiswilloutputthe pxp for
eachmodel,aswellasthemodelprobabilitiesandafewother
diagnosticoutputswewillnotcoverindetailhere;forfurther
information,seeRigouxetal.(2014)andStephanetal.(2009)
aswellasthedocumentationwithinthe spm_BMS.mfunction.
As one example, you could input the free energies for the 2-
parameter(F_2_params)and3-parameter( F_3_params)models
asfollows:
[alpha,exp_r,xp,pxp,bor] = spm_BMS([F_2_params F_3_
params])
Iftheoutputwere pxp = [10],thiswouldmeanthattheprob-
abilityisequalto1thatthe2-parametermodelisabetterfitthan
51
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
the3-parametermodel.Inarepresentativeexamplesimulation,
thepxp = [.37.63],weaklyfavoringthe3-parametermodel.After
identifyingthebestmodel,itisalsoimportanttomakesureit
capturesthedatawell,whichcouldbedoneby(forexample)
calculatingtheaverageprobabilityofparticipants’choicesunder
thewinningmodelorthepercentageoftrialswhereparticipants’
choices were assigned the highest probability. If the winning
modelstilldoesn’tcapturethedatawell,thissuggestsonemay
needtoconsiderotherpossiblemodels.
Once(recoverable)parametersforawinningmodelhavebeen
estimated,andwehaveconfirmedthatthismodelcapturesthe
datawell,onesimpleapproachforgroup-levelanalysiswould
betoanalyzetheposteriorparametermeansacrossparticipants
usingstandardfrequentistanalyses.However,oneadvantageof
usingvariationalBayesisthatitalsoprovidesinformationabout
posteriorparametervariancesforeachindividual(i.e.,asopposed
to only the posterior means). This allows for between-subject
Bayesiananalysesthattakethevariancesintoaccount.Oneap-
proachthattakestheoutput DCMstructuresforeachparticipant
asinputis parametricempiricalBayes (PEB),describedindetail
in Friston et al. (2016b) and Zeidman et al. (2019). PEB uses
a general linear model and effectively down-weights the con-
tributionofindividualsubjectparameterestimateswhenthose
estimateshavelargerposteriorvariances(i.e.,thosewithgreater
uncertainty). PEB can be run using thespm_dcm_peb.m and
spm_dcm_peb_bmc.mfunctions.Thesefunctionsestimateand
comparegroupmodels,respectively.Inshort,theyallowatest
ofwhetherthereisevidenceforamodelthatdoesincludegroup
differencesorwhetherthereismoreevidenceforamodelthat
does not include those differences. When including covariates
(e.g.,age),italsoallowscomparingtheevidencefororagainst
a relationship between parameters and those covariates. The
spm_dcm_peb_review.m function can be used to inspect the
resultsreturnedfromthePEBscripts.
ExamplecodetoimplementsuchempiricalBayesian(random
effects)analysesisalsoincludedintheaccompanyingMATLAB
codeStep_by_Step_AI_Guide.m.Thecodeshowsexamplesabout
(withcommenteddescriptionsof)howtosetupinputstoPEB.In
ourexample,thecodesetsseveralparameterstodefaultvalues
(formoreinformationonthese,seeFristonetal.,2016b;Zeidman
etal.,2019)andtheninputsamatrix( M.X)foragenerallinear
model,withacolumnforthemean,acolumnseparatingpartici-
pantsintotwogroups(here,withlowvs.highlearningrates),and
acolumnwithrandomlygeneratedparticipantagevalues).This
willallowustoassesstheevidenceformodelsincludingeffects
ofgroupand/orage(andthestrengthoftheseeffects).
TorunPEB,set Sim = 5andset PEB = 1.Thismaytake
sometime,asitwillfirstgenerateandestimateparametersfor
several(six)simulatedparticipantsbeforefeedingthemintothe
PEBscripts(notethatthescriptwillsavetheoutputsof Sim = 5
sothatthisdoesnotneedtoberepeatedeachtimeyouwantto
practiceusingPEB).InbeginningofSection10ofthescript,you
canalsospecifywhetheryouwanttousePEBonthe2-parameter
or3-parametermodel.Whencomplete,someoutputplotsand
thePEBresultsviewerwindowwillappear.Exampleoutputsare
showninthebottom-rightpanelofFig.17).Thefigurelegend
describeshowtointerprettheseoutputs.
Toreview,wehavecoveredseveralstepsformodelfitting,
model checking, and subsequent analysis. These steps are as
follows:
1. Selectestimationpriors,includingpriormeansandvari-
ances.
2. RunvariationalBayesonparticipantdataforeachmodel
underconsideration.
a. Checkthatgradientdescentshowssmoothconver-
gencetowardafreeenergyminimum(Fig.17,top-
leftpanel).Notethatthisisdisplayedasmaximiza-
tionbecausenegativefreeenergiesareused.
3. PerformBayesianmodelcomparisontoidentifythebest
model,andcheckthatthismodelcapturesthedatawell
(e.g., by evaluating the percentage of trials in which it
assignsthehighestprobabilitytoparticipants’choices).
4. Generatesimulatedbehaviorinthebestmodelusingthe
parametervaluesestimatedinyourparticipants,andthen
run variational Bayes to estimate parameters using this
simulatedbehavior.
a. Confirmthatposteriorestimatesapproachthepa-
rametervaluesusedtogeneratethesimulatedbe-
havior(Fig.17,top-rightpanel).
b. Confirm that posterior co-variances are not high
(Fig.17,bottom-leftpanel).
c. Confirm that generative and estimated parameter
values are strongly correlated (Fig. 17, bottom-
middlepanel).
5. UsePEBtorunregressionorgroupcomparisonanalyses
onposteriormeansandvariancesatthebetween-subjects
level(Fig.17,bottom-rightpanel).
8. Concludingremarks
Thisconcludesthetutorial.Forreadersseekingmorehands-
on practice, we have also provided pencil-and-paper exercises
in Appendix B (as well as solutions to check your work; see
Pencil_and_paper_exercise_solutions.m code). In our experi-
ence, doing a few practice problems of this sort, and working
withthecode,arethebestwaytogainusefulintuitionsabout
the dynamics of these models and how they can be tailored
forspecificstudies.Wenotethat,whilewehavestrivedtobe
comprehensive,therearemanynewdirectionsinactiveinference
(andassociatedfunctionalityinthestandardSPMroutines)incor-
porating,forexample,multi-agentinteractions,deepparametric
models(e.g.,2nd-orderparametersonhabits,preferences,preci-
sions,etc.),andmixedmodelsthatlinkPOMDPstocontinuous
motorcontrolprocesses,amongothers(e.g.,see(Fristonetal.,
2017c;Hespetal.,2020)).Wehopethatworkingthroughthe
materialsprovidedherewilloffera‘launchingpoint’thatwill
providethereaderwithasufficientfoundationtoindependently
extendtheirworkwithadvancesinthefieldastheyemerge.
Declarationofcompetinginterest
Theauthorsdeclarethattheyhavenoknowncompetingfinan-
cialinterestsorpersonalrelationshipsthatcouldhaveappeared
toinfluencetheworkreportedinthispaper.
Acknowledgments
TheauthorswouldliketothankLanceDaCosta,ThomasParr,
GiuseppePagnoni,andConorHeinsforofferingusefulcomments
andsuggestionsduringpreparationofthemanuscript.Theau-
thorswouldalsoliketoacknowledgeSamuelTaylorforadditional
suggestionsthatimprovedtheclarityofthemanuscript.
Funding
R.S.issupportedbytheWilliamK.WarrenFoundationandthe
NationalInstituteofGeneralMedicalSciences(P20GM121312).C.
W.issupportedbytheUniversityofCambridgeHardingDistin-
guishedPostgraduateScholarsProgramme.
52
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Fig.A.1. Venndiagramdepictionofajointprobabilitydistributionandhowitisusedtoinferaconditionalprobability(suchasaposteriorprobabilityinBayesian
inference).Thetotalspacewithintherectanglerepresentsallpossiblesituations.Thesolidcirclecorrespondstothesubsetofallpossiblesituationswhere a istrue.
Thedashedcirclecorrespondstothesubsetofallpossiblesituationswhere b istrue.Theoverallprobabilityof a relatestohowlargethecircleisfor a relative
tothetotalareawithinthesurroundingrectangle(andlikewisefortheoverallprobabilityof b).Theblueareawherethecirclesoverlaprepresentsthesituations
whereboth a andb aretrue.Toinfer p(b|a),wesimplyremovethecirclefor b (sinceweareconditioningon a beingtrue).Thenweevaluatehowmuchofthe
circlefor a isoccupiedbytheblueregionwhereboth a andb aretrue.Iftheblueregioncoversthemajorityofthecirclefor a,then p(b|a)willbelarge,whereasif
theblueregiononlycoversasmallpartofthecirclefor a,then p(b|a)willbesmall.Thus, p(b|a)canbethoughtofastheproportionofsituationswhere a istrue
inwhich b isalsotrue.(Forinterpretationofthereferencestocolorinthisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
AppendixA. Additionalmathematicaldetails
Introductiontotherulesofprobability
Therearetwobasicrulesinthemathematicsofprobability
thatwillunderpinmuchofthematerialcoveredinthistutorial.
Thefirstruleiscalledthe sumrule:
p (a) = p (a, b) + p (a, ∼ b) (A.1)
Hereweusethetilde( ∼)symboltoindicatenegation.Sothis
statesthattheprobabilityof a beingtrue, p (a),isequaltothe
jointprobability thatboth a andb aretrue, p (a, b),plusthejoint
probabilitythat a istrueand b isfalse, p (a, ∼ b).Ifthevariable b
cantakeonseveralvalues(i.e.,morethanjust true andfalse),then
calculatingp (a) requiressumming p (a, b) foreachpossiblevalue
ofb.Notethat p (a) isoftenreferredtoasa marginalprobability
inthiscontextwhencalculatedbysummingoverallthevaluesof
anothervariableinajointprobabilitydistribution.Inthecontext
ofBayesianinference,itisalsosometimesreferredtoasa prior
probabilitywhenusedtodescribebeliefsabout a beforemaking
anewobservation(describedfurtherbelow).
Thesecondruleiscalledthe productrule:
p(a|b)p(b)= p(a, b)= p(b|a)p(a) (A.2)
Thissaysthatthejointprobabilityof a andb isequivalentto
theconditionalprobability ofa givenb,p (a|b),multipliedby
theprobabilityof b,p (b).Here,thejointprobablyindicatesthe
likelihoodthat a andb willoccurtogether(e.g.,thatitiscloudy
and thatitisraining).Theconditionalprobabilityindicateshow
likelya isifwearetold b (e.g.,howlikelyitistorain if weknow
thatitiscloudy).Themarginalprobability, p (b),indicateshow
likelysomethingistooccuringeneral(e.g.,howoftenitiscloudy
overall).
Symmetrically,theproductrulesaysthat p (a, b) isalsoequiv-
alenttotheconditionalprobabilityof b givena,p (b|a),multiplied
by the marginal probability ofa, p (a). We can also use the
productruletodostandardalgebraicmanipulations.Forexample,
wecantake p (a, b) = p(b|a)p(a)andthendividebothsidesby
p (a) togettheconditionalprobabilityof b givena,p (b|a):
p (b|a) = p (a, b)
p (a)
(A.3)
We can then deriveBayes’ theoremby simply replacing
p (a, b) in Eq. (A.3) withp(a|b)p(b), where this equivalence is
showninEq.(A.2):
p (b|a) = p (a|b) p(b)
p (a)
(A.4)
Inscientificpractice,wecanrepresentobservationaldatawith
the variablea and then use the variableb to represent some
theory.Inthiscase, p(b)isreferredtoasthe prior probability
of the theory being true before observing the data. Eq. (A.4)
thenmeansthatifwegetsomenewdata a,thenwecanuse
Bayes’theoremtoinferwhetheritincreasestheprobabilitythat
some theoryb is true. In other words, we can infer whether
theposteriorprobability ofthetheorygiventhedata, p (b|a),is
higherthanthepriorprobabilitythatthetheorywastruebefore
observingthenewdata, p(b).Thisjustrequiresthatweknowhow
stronglythetheorypredictsthatnewpieceofdata— typically
referredtoasthe likelihoodthatthedatawouldbeobservedif
thetheoryweretrue, p (a|b).
TheuseofVenndiagramsisoftenhelpfulforgainingintuitions
about probabilistic inference. We illustrate this in Fig. A.1 by
showingonecircleontheleftthatcontainsallsituationsinwhich
a istrue(solidoutline)andanothercircleontherightcontain-
ingallsituationsinwhich b istrue(dashedoutline).Thetotal
areawithinthesurroundingrectanglecorrespondstoallpossible
situations.Thus,theareaoutsideofbothcirclescorrespondsto
thesituationswhereneither a orb istrue.Thesetofsituations
whereboth a andb aretruecorrespondstotheshadedbluearea
inthemiddlewherethecirclesoverlap.Theoverallprobability
ofa correspondstohowlargethecircleisfor a relativetothe
totalareawithinthesurroundingrectangle(andlikewiseforthe
overallprobabilityof b).Whenweinfertheposteriorprobability,
p (b|a), we first simply remove the circle forb (i.e., imagine
erasingthedashedoutlineandtheseparate b variable,sincewe
know from our new data that we are in a situation wherea
istrue).However,weretaintheblueshadedregionwithinthe
circlefor a correspondingtowhen a andb arebothtrue.Then
welookathowlargetheproportionofthecircleisfor a that
correspondstowhen b isalsotrue(i.e.,howlargetheblueshaded
areais).Ifthisarearepresentsalargeportionofthecirclefor a,
thenthismeanstheposteriorprobability, p (b|a),willbehigh.
Thisproportioniswhatiscapturedby p(a,b)
p(a) (rememberthisis
equivalentto p(a|b)p(b)
p(a) inBayes’theorem;seeEq.(A.2)).
Afurtherfacttokeepinmindabouttherulesofprobabilityis
thattheydonotchangeifotherconstantsareincluded.Forex-
ample,thesumruleremainsthesameifalltermsareconditioned
53
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
TableA.1
Priorandconditionalprobabilities.
cloudy (p = .7) not cloudy (p = .3)
raining p (raining|cloudy)
= .6
p (raining|not cloudy)
= .01
not raining p (not raining|cloudy)
= .4
p (not raining|not cloudy)
= .99
TableA.2
Jointprobabilities.
cloudy not cloudy Marginalprobabilities
raining p (raining, cloudy)
= .7× .6= .42
p (raining, not cloudy)
= .3× .01= .003
p (raining)
= .42+ .003= .423
not raining p (not raining, cloudy)
= .7× .4= .28
p (not raining, not cloudy)
= .3× .99= .297
p (not raining)
= .28+ .297= .577
TableA.3
Posteriorprobabilities.
cloudy not cloudy
raining p (cloudy|raining)
= .42
.423 = .993
p (not cloudy|raining)
= .003
.423 = .007
not raining p (cloudy|not raining)
= .28
.577 = .485
p (not cloudy|not raining)
= .297
.577 = .515
onsomeothervariable c:
p (a|c) = p (a, b|c) + p (a, ∼ b|c) (A.5)
Thisalsoholdsfortheproductrule:
p (a, b|c) = p (a|b, c) p (b|c) (A.6)
As a numerical illustration of these rules, we will leta =
"raining" andb = "cloudy" and use Table A.1 to indicate the
followingpriorandconditionalprobabilities(i.e.,theprobability
ofitrainingifitiscloudy):
Note that while the values of each column sum to 1, the
valuesoftherowsdonot,whichiswhythelikelihoodtermin
Bayes’theoremisnottechnicallyconsideredaproperprobability
distribution(i.e.,ifonewantedtotreatitassuch,therowswould
needtobenormalizedsothattheykeptthesameproportionsbut
didsumto1).
Togetthejointprobabilities,wesimplymultiplyeachprior
byeachconditional(i.e.,theproductrule):
Inthiscase,allthejointprobabilitiessumto1,asthesefour
cells(i.e.,inthefirstandsecondcolumns)describeallpossible
outcomes.Asshowninthethirdcolumn,summingacrosseach
rowgivesusthemarginalprobabilitiesforeachpossiblevalue
of p (a) (i.e., raining vs. not raining), based on the sum rule.
Thesearethevaluesthatendup‘inthemargin’(whichiswhy
theyarecalledmarginalprobabilities)whensummingoverthe
probabilitiesundereachpossiblevalueof b (i.e.,cloudyvs.not
cloudy).
We can then find the posterior probabilities (Table A.3) by
dividingthejointprobabilities(i.e.,thevalueineachcellwithin
thefirstandsecondcolumnsofTableA.2)bythemarginalprob-
abilities (i.e., the value of the associated row within the third
columninTableA.2):
Thistellsus,forexample,thatifyoulookoutyourwindow
andseethatitisraining,thenyoucanconfidentlyinferthatitis
cloudy.
Afinalrulethatwillbeusefultoknowforthistutorialpertains
tologarithmic(log)transformations .Specifically,logtransfor-
mationsofprobabilitiesallowmultiplicationanddivisiontobe
expressedintermsofadditionandsubtraction(respectively).For
example,usingthenaturallogarithm(ln),theproductrulecanbe
expressedasfollows:
lnp (a, b) = lnp (b|a) + lnp (a) (A.7)
lnp (b|a) = lnp (a, b) − lnp (a) (A.8)
Performingthesetransformationsisoftenbeneficialinprac-
tice because it simplifies the necessary computations and can
preventtheneedtoworkwithverysmallprobabilities(whichcan
happenwhenmanyprobabilitiesmustbemultipliedtogetherin
complexreal-worldproblems).Log-probabilitiescanalsobeeas-
ilyconvertedbackintostandardprobabilitiesby exponentiating
them.Thatis, elnp(a) = p (a),where e ≈ 2.71828(oftencalled
Euler’snumber).
Thisconcludesourbriefintroductiontotherulesofproba-
bility.Manyderivationsinthistutorialworkprimarilybasedon
usingtheserulesincombinationwithalgebraicmanipulation.
IntroductiontoVariationalInference
The typical goal of Bayesian inference is to find the poste-
riordistribution p (s|o) –thatis,toinferhowthestatesofthe
world (s) have changed based on new observations (o). How-
ever,thisrequiresonetocalculatethemarginallikelihood p (o),
whichofteninvolvesintractablesums(orintegralsinthecaseof
states/observationswithcontinuousvalues).Thekeyideabehind
variational inference is to convert this inference problem into
an optimization problem. To do so, instead of evaluating the
marginallikelihood,weoptimizeanauxiliarydistribution q (s)
(sometimescalledtherecognitiondistribution,orvariationalpos-
terior)toapproximatethetrueposterior p (s|o).Thisisdoneby
usingtheKLdivergenceasameasureoftherelativedifference(in
theinformation-theoreticunit nats –thenaturallogequivalentof
bits)betweenthetwodistributions:
DKL(q (s) ∥ p (s|o))=
∑
s
q (s) ln q (s)
p (s|o)
(A.9)
TheKLdivergencesumsoverthestatesofthetwodistribu-
tionssotheoutputisalwaysgreaterthanorequaltozero.When
therecognitiondistributionandthetrueposteriormatch,theKL
divergenceiszero(i.e.,when q(s)= p(s|o), DKL(q(s)∥ p(s|o))= 0).
Although,aswedonotknowthetrueposteriordistribution,this
sumalsocannotbeevaluated.Crucially,however,workingfrom
thedefinitionofconditionalprobabilities, p (s|o) = p(o,s)
p(o) ,wecan
54
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
introduceaquantitythatcanbeevaluateddirectly(notingthat
= 1
x/1
y = y
x):
DKL(q (s) ∥ p (s|o))=
∑
s
q (s) lnq (s)
p(o,s)
p(o)
(A.10)
=
∑
s
q (s) lnq (s) p (o)
p (o, s)
(L2)
=
∑
s
q (s) ln q (s)
p (o, s) + ln(p (o)) (L3)
Eq.(A.10)substitutesthealternatedefinitionoftheposterior
distributionintoEq.(A.9).Withsomeminorrearrangement,we
seethattheKLdivergencebetweenourapproximateposterior
andthetrueposteriorisnowequaltotheKLdivergencebetween
ourapproximateposteriorand p (o, s) –whichcanbeviewedas
agenerativemodelofhowstatesoftheworldgenerateobserva-
tions–plusthelogprobabilityofobservations(i.e.,thelogofthe
marginallikelihood).Thisisthecriticalmove.Becausewearefree
tospecifythegenerativemodel,and q (s) isthevariableweseek
tooptimize(andcanthusbeinitiallysettoanarbitraryvalue;
seebelow),wehaveaccesstoboththequantitiesweneedto
computetheKLdivergence.Wenowintroduceanewquantity,
variationalfreeenergy( VFE),denoted F,anddefineitinterms
of this KL divergence:F := ∑
s q (s) ln q(s)
p(o,s) . The value of the
approximateposterior q (s) thatminimizes VFE willthenbethe
q (s) thatbestapproximatesthetrueposteriordistribution.
DKL(q (s) ∥ p (s|o))= F + ln(p (o)) (A.11)
EquationA.11rewritesline3ofEq.(A.10),butsubstitutesin
VFE asanexplicitvariable.Fromthisvantagepoint,weseethat
whenVFE isminimizedtheKLdivergencebetweentheapproxi-
mateposteriorandthetrueposteriorisalsominimized,meaning
that the approximate posterior is close to the true posterior.
Hence,minimizing VFE allowsatractablemeansofperforming
approximate Bayesian inference. One way to do this is using
gradientdescent.Thatis,onecanstart q(s)atanarbitraryvalue
andthentestneighboringvaluestofindtheonethatreduces VFE
most.Thenonecanmovetothatvalue,searchneighboringvalues
again(etc.),andrepeatthisprocessuntilavaluefor q(s)isfound
forwhichnoneighboringvaluesfurtherreduce VFE.
ExpectedFreeEnergy
Activeinferencereversestheusuallogicofactionselection.
Insteadofasking‘whichsequenceofactionswillbringaboutmy
preferredoutcomes?’,itformallyasks,‘giventheassumptionthat
Iwillachievemypreferredoutcomes,whatcourseofactionam
Imostlikelytopursue?’(Millidge,Tschantz,&Buckley,2021).
Withinactiveinference,theanswertothisquestionisthepolicy
(π;i.e.,actionsequence)thatbestminimizesaquantitytermed
expected free energy (EFE). Here we show the most common
decompositionsof EFE thatappearintheactiveinferenceliter-
atureanddescribetheworkingsandtheintuitionbehindeach
decomposition.EFE isdefinedintermsoftheexpecteddifference
betweenthelogofthegenerativemodel p (o, s|π) andthelog
of the approximate posterior given a choice of policyq (s|π).
Eq.(A.12)showsthedecompositionofthe EFE ofeachpolicy( Gπ )
intotermsoftenreferredtoas epistemic andpragmatic value(in-
tuitively,expectedinformationgainandrewardprobabilityunder
eachpolicy,respectively).Italsoshowsanothercommondecom-
positionintotermsreferredtoas risk andambiguity (similarly
correspondingtoexpectedrewardanduncertaintyminimization
undereachpolicy).Notebelowthat,because EFE iscalculated
withrespecttoexpectedoutcomesthat(bydefinition)havenot
yetoccurred,observationsentertheexpectationoperatorE q as
randomvariables.
Gπ = Eq(o,s|π)[lnq (s|π) − lnp (o, s|π)] (A.12)
= Eq(o,s|π)[lnq (s|π) − lnp (s|o, π)] −Eq(o|π)[lnp (o|π)] (L2)
≈ Eq(o,s|π)[lnq (s|π) − lnq (s|o, π)] −Eq(o|π)[lnp (o|C)] (L3)
= −Eq(o,s|π)[lnq (s|o, π) − lnq(s|π)] −Eq(o|π)[lnp (o|C)] (L4)
= Eq(o,s|π)[lnq (o|π) − lnq (o|s, π)] −Eq(o|π)[lnp (o|C)] (L5)
= DKL(q (o|π) ∥ p (o|C))+ Eq(s|π) [H [p (o|s)]] (L6)
Thesecondlineusestheproductruleofprobability, p (o, s|π)
= p (s|o, π) p (o|π), to rearrangeEFE into the epistemic and
pragmaticvaluetermsdescribedinthemaintext.Inthethird
line,thedependenceonpoliciesisdroppedfromthepragmatic
valuetermsothatitcanbeusedtoencodepreferences(i.e.,this
is a key move in active inference). Note that, in most papers
on active inference, this is simply written as Eq(o|π)[lnp (o)];
however,toclearlydistinguishthisfromtheln p (o) termwithin
VFE (i.e.,where o isanobservedvariable),wewritethepragmatic
valuetermhereasexplicitlyconditionedonapreferencevariable
C (Parr et al., 2022). Line 3 also replaces the true posterior
(lnp (s|o, π))withanapproximateposterior(ln q (s|o, π)).Line
4 offers a clearer intuition for epistemic value by flipping the
terms inside the first expectation so that it becomes prefixed
withanegativesign(i.e., p(x)[lnp(x)− lnq(x)] = −p(x)[lnq (x) −
lnp (x)]). Because the epistemic value term is now subtracted
fromthetotal,itisclearthat,tominimize EFE overall,anagent
mustmaximizethevalueofthistermbyselectingpoliciesthat
takeitintostatesthatmaximizethedifferencebetweenln q (s|o, π)
andln q (s|π).Inotherwords,theagentisdriventoseekoutob-
servationsthatreduceuncertaintyabouthiddenstates(i.e.,max-
imize the change from prior to posterior beliefs after a new
observation).Forexample,ifyouareinadarkroom,thenthe
mapping between hidden states and observations is entirely
ambiguous.Thebestwaytominimizeuncertaintyistoturna
lighton.
Movingfromtheexpressioninline3,line5usesBayesrule
inthedenominator q(s|π)
q(s|o,π) = q(s|π)q(o|π)
p(o|s,π)q(s|π) = q(o|π)
p(o|s,π) toexpress
thesameepistemicimperative,butwiththeconditionalproba-
bilitiesflipped.Noteherethat,althoughalgebraically q (s|o, π) =
q(o|s,π)q(s|π)
q(o|π) ,inthiscase q (o|s, π) andp (o|s, π) refertothesame
distribution.Theepistemicvaluetermsinlines3,4,and5are
formallyequivalentsincetheyeachexpressthe mutualinfor-
mation between hidden states and observations. First noting
thatH[p (x)]denotestheentropyofadistribution p (x),where
H[p (x)] = −∑
x p (x) lnp (x) = −Ep(x)[lnp (x)], mutual infor-
mation can be written asI (x, y) = H[p (x)]− H[p (x|y)] =
H[p (y)] − H[p (y|x)]. This quantityI (x, y) is symmetric and
scoresthereductioninuncertainty(entropy)aboutthevalueof
avariable x affordedbyknowledgeofanothervariable y.Iftwo
variables are independent, mutual information is zero. Finally,
line6expresses EFE intheformshowninthemaintextas risk
plusambiguity.Thefirstterm,risk,istheKLdivergencebetween
theobservationsexpectedunderapolicyandpriorpreferences.
Thesecondterm,ambiguity,scorestheuncertaintyinthelike-
lihood mapping between states and observations. Minimizing
EFE thus requires agents to select policies that minimize the
differencebetweenexpectedobservationsandpreferredobserva-
tions(e.g.,seekingwarmthwhenitiscold,maximizingreward,
etc.)andtotakeactionsthatreduceuncertainty(i.e.,ambigu-
ity)aboutthemappingbetweenhiddenstatesandobservations
(i.e.,anotherwayofexpressingthedrivetomaximizeinformation
gain).Theworktomovebetweenthefifthandthesixthlineis
55
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
somewhatconvoluted,soEq.(A.13)showsitstepbystep:
Gπ =
∑
o,s
p (o|s) q (s|π) (ln q (o|π)
p (o|s, π)
)
−
∑
o,s
p (o|s) q (s|π) lnp (o|C) (A.13)
=
∑
o,s
p (o|s) q (s|π) (ln q (o|π)
p (o|C) p (o|s, π)
) (L2)
=
∑
o,s
q (o, s|π) (lnq (o|π)
p (o|C)
)
−
∑
s
q (s|π)
∑
o
p (o|s) lnp (o|s, π) (L3)
=
∑
o
q (o|π) (lnq (o|π)
p (o|C)
)−
∑
s
q (s|π)
∑
o
p (o|s) lnp (o|s)
(L4)
=
∑
o
q (o|π) (lnq (o|π)
p (o|C)
)+
∑
s
q(s|π) H[p (o|s)] (L5)
= DKL(q (o|π) ∥ p (o|C))+ Eq(s|π)[H[p (o|s)]] (L6)
Line1ofEq.(A.13)rewritesline5ofEq.(A.12),butmakesthe
summationsimpliedbytheexpectationoperatorsexplicit.Line
2moves p(o|C)backintothesameexpectation.Line3expresses
theexpectationinthefirsttermintermsofanapproximatejoint
distributionp (o|s) q (s|π) = q (o, s|π),andinthesecondterm
separatesout p (o|s, π).Inthefirsttermofline4weevaluatethe
summationoverstatesinthejointdistribution
∑
o,sq (o, s|π) =∑
oq (o|π), leaving the fraction inside the log untouched as it
does not depend on states. In the second term of line 4 we
dropthedependencyonpoliciessincethelikelihoodmappingis
constantacrosschoicesofpolicy.Inline5thisthenallowsusto
express− ∑
op (o|s) lnp (o|s) intermsofentropy H[p (o|s)],and
rewrite
∑
o q(o|π)ln q(o|π)
p(o|C) intermsoftheKLdivergence(defined
in the main text) between prior preferences and observations
expectedundereachpolicy DKL(q(o|π)∥ p (o|C)).Finally,because
entropyisanegativequantity,weswapthesignbetweenthetwo
terms,leavinguswiththecanonicalformof EFE asrisk plusambi-
guity inline6–wherelowerriskindicatesahigherprobabilityof
preferredoutcomesunderapolicyandlowerambiguityindicates
moreprecise(informative)observationsexpectedunderapolicy.
It is important to highlight here that generative models in
activeinferencealsomaintainconfidenceestimatesforthemodel
parametersthemselves,viaaformofdistributioncalledaDirich-
letdistributionthatencodespriorsovertheseparameters(see
maintextforanintroductiontothistypeofdistribution).Dirich-
letdistributionscontainwhatarecalledconcentrationparame-
ters,wherehigherconcentrationparametervaluesindicatelower
uncertainty in the parameters of each distribution. The above
expressions ofEFE assume that the concentration parameters
aresaturated(i.e.,thattheyaremaximallyprecise),andhence
thatthereisnouncertainty.However,whenthereisuncertainty
in parameters (as is the case for real organisms), agents must
alsolearnthevaluesforthoseparametersviatheselectionof
appropriatepolicies.Thismeansthatparameteruncertaintynow
enterstheequationfor EFE.Forexample,Eq.(A.14)showsthe
formof EFE whentheparametersofthelikelihood p (o|s) must
belearned.Notethatthislikelihoodistermedthe Amatrixin
activeinferencemodels(seeTable1).
Gπ = Eq[lnq (s, A|π) − lnp (o, s, A|π)] (A.14)
= Eq[lnq (s|π) + lnq (A) − lnp (A|s, o, π)
− lnp (s|o, π) − lnp (o|π)] (L2)
≈ Eq[lnq (s|π) + lnq (A) − lnq (A|s, o, π)
− lnq (s|o, π) − lnp (o|π)] (L3)
= Eq[lnq (s|π) + lnq (A) − lnq (A|s, o, π)
− lnq (s|o, π)]− Eq[lnp (o|C)] (L4)
= Eq[lnq (s|π) − lnq (s|o, π)]+ Eq[lnq (A)
− lnq (A|s, o, π)]− Eq[lnp (o|C)] (L5)
= −Eq[lnq (s|o, π) − lnq (s|π)]− Eq[lnq (A|s, o, π)
− lnq (A)] −Eq[lnp (o|C)] (L6)
Hereq = q (o, s, A|π).Line1showstheformof EFE whenAis
treatedasarandomvariable.Line2breakstheapproximatepos-
teriorandgenerativemodelintoseparatetermsusingtheproduct
ruleofprobability p (o, s, A|π) = p (A|s, o, π) p (s|o, π) p (o|π).
Italsousesthemean-fieldapproximation–whichassumesthat
theapproximateposteriorfactorizesintotheproductofindepen-
dentmarginaldistributions–toexpresstheapproximatejoint
distributionas q (s, A|π) = q (s|π) q (A).Line3approximatesline
two,replacingtheexactposteriors p (A|s, o, π) andp (s|o, π) with
approximateposteriors q (A|s, o, π) andq (s|o, π).Line4takes
pragmaticvalueE q[lnp (o|π)] outofthefirstexpectationterm
andthen(asabove)conditionsonthepreferencevariable C in-
steadof π;i.e.,E q[lnp (o|C)].Line5breaksthefirstexpectationin
line4intotwoquantities.Thefirst,E q[lnq (s|π) − lnq (s|o, π)])
is the epistemic value term that we saw in the previous ex-
pression ofEFE without uncertainty in the parameters. As we
nowhavetwotypesofepistemicvalue,todistinguishthemwe
callthefirst salience andthesecond novelty.Saliencescoresthe
reduction of uncertainty about states afforded by observations
(driving‘stateexploration’behavior),whilenoveltyE q[lnq (A) −
lnq (A|s, o, π)] scoresthereductioninuncertaintyaboutparam-
etersofthegenerativemodelaffordedbystatesandobservations
(driving ‘parameter exploration’ behavior; see (Schwartenbeck
etal.,2019).Aswithline4inEq.(A.12),line6hereflipstheterms
insidethefirstandsecondexpectationtomakethesalienceand
noveltytermsnegative(i.e.,suchthatmaximizingtheseterms
brings their value closer to zero). This makes it clearer why
maximizingthedifferencebetweenpriorandposteriorbeliefs–
hereaboutbothstatesandparameters–willminimize EFE.The
parametersofthemodelarethesufficientstatisticsofDirichlet
distributions,which,inthecaseoflearning A,essentiallycount
thenumberoftimesaparticularcategoricalstateisinferredwhen
aparticularoutcomeisobserved(proportionaltotheposterior
probabilityovereachstate).Liketheepistemicvalueterminthe
previousexpressionof EFE,tominimize EFE hereagentsmust
maximizebothsalienceandnoveltybyseekingoutobservations
that(1)reduceuncertaintyabouthiddenstates,and/or(2)reduce
uncertaintyaboutparameters.Thereductionofuncertaintyofthe
parametersviathemaximizationofnoveltyencouragesagents
toexplorenovelpartsofthestatespacethatarelessfamiliar
(i.e.,statesthathavelowconcentrationparameters).
TheSoftmaxFunction
Thesoftmax(ornormalizedexponential)function,denotedby
σ,takesavector x oflength k andnormalizesthevectorsuchthat
theelements(1)haveamonotonicrelationshipwiththeelements
oftheinputvector,and(2)sumto1andcanthusbetreated
asacategoricalprobabilitydistributionover1tokmutuallyex-
clusivestates.Importantly,thevectorisweightedbyaprecision
parameter denoted byγ , which controls the extent to which
differencesbetweentheelementsareamplifiedordampenedby
theexponential.
σ (x) = eγ xi
∑
keγ xk
(A.15)
56
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
For example, forx = [1 2 3 4 ]T, whenγ = 1, σ(x) =
[0.0321 0.0871 0.2369 0.6439]T.
Whenγ = 0.1, σ(x) = [0.2138 0.2363 0.2612 0.2887]T.
Whenγ = 2, σ(x) = [0.0021 0.0158 0.1171 0.8650]T.
TheGammaFunction
Thegammafunction(denotedby Γ )isageneralizationofthe
factorialfunctionthat,unlikethefactorialfunction(whosedo-
mainisrestrictedtopositiveintegers),iswelldefinedforcomplex
andrealvalued(i.e.,non-integer)inputs(exceptforthenegative
integers). For positive, real-valued, and complex numbers, the
gammafunctionisdefinedbythefollowingdefiniteintegral.
Γ (z) =
∫ ∞
0
xz−1e−xdx (A.16)
Forthepositiveintegers,thegammafunctionreducestothe
factorialfunction(i.e.,n ! =n × (n− 1) × (n− 2) × (n− 3) ×
· · · ×3 × 2× 1)butisshiftedby1.
Γ (n) = (n − 1)! (A.17)
Γ (2) = (2–1)! =1× 1= 1 (L2)
Γ (3) = (3–1)! =2× 1= 2 (L3)
Γ (4) = (4–1)! =3× 2× 1= 6 (L4)
To gain an intuition for how the gamma function relates
tothefactorialfunction,wewilluseintegrationbyparts(i.e.,∫a
b f (x) g′ (x) dx = [f (x) g (x)]a
b −
∫a
b f ′ (x) g (x) dx)toshowthat
Γ (n+ 1) = (n)!.
Γ (z + 1) =
∫ ∞
0
xz e−xdx = [−xz e−x]∞
0
+
∫ ∞
0
zxz−1e−xdx (A.18)
= lim
x→∞
(−xz e−x)−
(
−0e−0)
+ z
∫ ∞
0
xz−1e−xdx (L2)
Becausee−x growsfasterthan xz ,thefirsttermissenttozero
leavinguswiththefollowing.
Γ (z + 1) = z
∫ ∞
0
xz−1e−xdx (A.19)
= zΓ (z) (L2)
Ifwepluginsomeexamples,weseeimmediatelythatthisis
equivalenttothefactorialfunction.
Γ (2+ 1) = 2Γ (2) = 2(2− 1)! =2= 2! (A.20)
Γ (3+ 1) = 3Γ (3) = 3(3− 1)! =6= 3! (L2)
Γ (4+ 1) = 4Γ (4) = 4(4− 1)! =24= 4! (L3)
InthecontextoftheDirichletdistribution,thegammafunc-
tion is used to define a normalization constant that accounts
for the combinatorics of the concentration parameters, which
arepositivereal-valuednumbers(i.e.,theycantakenon-integer
values),mostofwhicharenotdefinedwhenusingthefactorial
function, which is why the gamma function is employed. We
canthinkofthegammafunctionasinterpolatingbetweenthe
valuesofthefactorialfunction(i.e.,becauseitisdefinedwith
respecttonon-negativerealnumbers,whilethefactorialfunction
is only defined with respect to the non-negative integers). In
otherwords,whenthefactorialfunctionandgammafunctionare
giventhesameinputs,theyproduceconsistentoutputs(shifted
by one), but the gamma function outputs values between the
integeroutputsgivenbythefactorialfunction.
AppendixB. Pencilandpaperexercises
Thepurposeofthissupplementarysectionistoprovideaset
ofexercisesthatcanbeworkedthroughusingpencilandpaper,
withtheaimofbuildinganintuitionforhowactiveinference
modelsoperate.Thefirstexerciseisasimpleexampleofstatic
perception.Thesecondexampleextendsthisbymodelinghow
states(andtheobservationstheygenerate)changeacrosstime,
whichisanexampleofahiddenMarkovmodel(HMM).HMMs
are the perceptual component of partially observable Markov
decisionprocesses(POMDPs).Wehavenotincludedanexample
ofpolicyselection,asperformingthecalculationsforeventwo
policies involves too many computations to reasonably expect
readerstoperformthembyhand.
Tohelpthereaderbuildaconceptualbridgebetweentheup-
dateequationsandtheirimplementationincode,wehavealsoin-
cludedMATLABcode( Pencil_and_paper_exercise_solutions.m)
thathassolutionstoeachoftheexercisesshownbelow.Finally,
readers should note that, to ensure that the exercises can be
solvedbyhand,wehaveexcludedakeyaspectofactiveinference
models as they are usually implemented. Namely, instead of
inferringtheposterioroverhiddenstatesusinggradientdescent,
we use only a single round of marginal message passing. For
readersseekingtounderstandhowmessagepassingandpolicy
selectionoperateinthemodelinversionprocedureimplemented
inspm_MDP_VB_X.m,pleaseseethestand-aloneMATLABscript
Simplified_simulation_script.mthatisalsoprovided,whichisa
strippeddown,butthoroughlycommented,versionofthemodel
inversionschemeusedin spm_MDP_VB_X.m.
Exercises
Static Perception
Forthisfirstexamplewewillkeepthingsassimpleaspossible.
Example 1
Update Equation
s= σ(lnD + lnATo)
Generative Model and Observation
D =
[
.5
.5
]
; A=
[
.9 .3
.1 .7
]
; o=
[
1
0
]
Model Inversion
s= σ
(
ln
[
.5
.5
]
+ ln
[
.9 .1
.3 .7
][
1
0
])
= σ
(
ln
[
.5
.5
]
+ ln
[
.9
.3
])
= σ
(
ln
[
.5× .9
.5× .3
])
= σ
(
ln
[
.45
.15
])
=
[
eln(.45)
eln(.45)+eln(.15)
eln(.15)
eln(.45)+eln(.15)
]
=
[ .45
.45+.15
.15
.45+.15
]
=
[
.75
.25
]
Exercise 1
BasedontheupdateequationinExample1andtheobserva-
tionlistedbelow,invertthefollowinggenerativemodel:
D =
[
.75
.25
]
; A=
[
.8 .2
.2 .8
]
; o=
[
1
0
]
57
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Dynamic Perception
In the second example below we move from a static en-
vironment to a dynamic one in which hidden states, and the
observations generated by these states, change across time. In
addition to including aB matrix that encodes the transition
probabilities,wemustinitializetheapproximateposteriorsfor
eachtau(τ)beforestartingmodelinversion(recall,taureferences
a timeabout which one has beliefs, not a timeat which one
updates beliefs with a new observation). It is also important
tonotethat,becausethelogofzeroisnotdefined,themodel
inversionprocedureimplementedinthecodeaddsaverysmall
number(e−16 = 0.00000011253)toallinputswhichturnsthe
logofzerointothelogofaverysmallnumber.Sinceweexpect
readerstobeabletodothisexercisebyhand,weapproximate
thisbyadding0.01totheinputofalllogs.
Example 2
Update Equations
sτ=1 = σ
(
1
2
(
lnD+ lnB†
τ sτ+1
)
+ lnAToτ
)
s1<τ<T = σ
(
1
2
(
lnBτ−1sτ−1+ lnB†
τ sτ+1
)
+ lnAToτ
)
sτ=T = σ
(
1
2(lnBτ−1sτ−1) + lnAToτ
)
*Recallthat B† denotesthetransposeof Bwithnormalized
columns(i.e.,columnsthatsumto1).Alsonotethatbecausethe
examplebelowonlyincludes2timepoints,onlythefirstand
thirdequationswillapply.
Generative Model and Observations
D =
[
.75
.25
]
; A=
[
.8 .2
.2 .8
]
; B=
[
0 1
1 0
]
;
oτ=1 =
[
1
0
]
; oτ=2 =
[
0
1
]
Initialize Approximate Posteriors
sτ=1 =
[
.5
.5
]
; sτ=2 =
[
.5
.5
]
Model Inversion: Time Step 1
sτ=1 = σ
(
1
2
ln
[
.75
.25
]
+ 1
2
ln
[
0 1
1 0
][
.5
.5
]
+ ln
[
.8 .2
.2 .8
][
1
0
])
= σ
([
−.1372
−.6735
]
+
[
−.3367
−.3367
]
+
[
−.2107
−1.5606
])
= σ
([
−.6846
−2.5709
])
=
[
.8683
.1317
]
sτ=2 = σ
(
1
2
ln
[
0 1
1 0
][
.8683
.1317
]
+ ln
[
.8 .2
.2 .8
][
0
0
])
= σ
([
−.9771
−.0649
]
+
[
−4.6052
−4.6052
])
= σ
([
−5.5823
−4.6700
])
=
[
.2865
.7135
]
Model Inversion: Time Step 2
sτ=1 = σ
(
1
2
ln
[
.75
.25
]
+ 1
2
ln
[
0 1
1 0
][
.2865
.7135
]
+ ln
[
.8 .2
.2 .8
][
1
0
])
= σ
([
−.1372
−.6735
]
+
[
−.1619
−.6078
]
+
[
−.2107
−1.5606
])
= σ
([
−.5098
−2.8420
])
=
[
.9115
.0885
]
sτ=2 = σ
(
1
2
ln
[
0 1
1 0
][
.9115
.0885
]
+ ln
[
.8 .2
.2 .8
][
0
1
])
= σ
([
−1.1589
−.0409
]
+
[
−1.5606
−0.2107
])
= σ
([
−2.7195
−0.2516
])
=
[
.0781
.9219
]
Exercise 2
UsingtheequationspresentedinExample2,andtheobser-
vationslistedbelow,invertthefollowinggenerativemodel.Note
againthatbecausethisexampleonlyincludes2timepoints,only
thefirstandthirdequationswillapply.
D =
[
.5
.5
]
; A=
[
.9 .1
.1 .9
]
; B=
[
1 0
0 1
]
;
oτ=1 =
[
1
0
]
; oτ=2 =
[
1
0
]
Answers
Answer: Exercise 1
s= σ
(
ln
[
.75
.25
]
+ ln
[
.8 .2
.2 .8
][
1
0
])
= σ
(
ln
[
.75
.25
]
+ ln
[
.8
.2
])
= σ
(
ln
[
.75× .8
.25× .2
])
= σ
(
ln
[
.6
.05
])
=
[
eln(.6)
eln(.6)+eln(.05)
eln(.05)
eln(.6)+eln(.05)
]
=
[ .6
.6+.05
.05
.6+.05
]
=
[
.9231
.0769
]
Answer: Exercise 2
Model Inversion: Time Step 1
sτ=1 = σ
(
1
2
ln
[
.5
.5
]
+ 1
2
ln
[
1 0
0 1
][
.5
.5
]
+ ln
[
.9 .1
.1 .9
][
1
0
])
= σ
([
−.3367
−.3367
]
+
[
−.3367
−.3367
]
+
[
−.0943
−2.2073
])
58
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
= σ
([
−.7677
−2.8806
])
=
[
.8922
.1078
]
sτ=2 = σ
(
1
2
ln
[
1 0
0 1
][
.8922
.1078
]
+ ln
[
.9 .1
.1 .9
][
0
0
])
= σ
([
−0.0515
−1.0692
]
+
[
−4.6052
−4.6052
])
= σ
([
−4.6567
−5.6744
])
=
[
.7345
.2655
]
Model Inversion: Time Step 2
sτ=1 = σ
(
1
2
ln
[
.5
.5
]
+ 1
2
ln
[
1 0
0 1
][
.7345
.2655
]
+ ln
[
.9 .1
.1 .9
][
1
0
])
= σ
([
−.3367
−.3367
]
+
[
−.1475
−.6446
]
+
[
−.0943
−2.2073
])
= σ
([
−.5785
−3.1886
])
=
[
.9315
.0685
]
sτ=2 = σ
(
1
2
ln
[
1 0
0 1
][
.9315
.0685
]
+ ln
[
.9 .1
.1 .9
][
1
0
])
= σ
([
−0.0301
−1.2724
]
+
[
−0.943
−2.2073
])
= σ
([
−0.1244
−3.4797
])
=
[
.9663
.0337
]
AppendixC. Supplementarydata
Supplementarymaterialrelatedtothisarticlecanbefound
onlineathttps://doi.org/10.1016/j.jmp.2021.102632.
References
Adams, R., Shipp, S., & Friston, K. (2013). Predictions not commands: active
inferenceinthemotorsystem. Brain Structure and Function,218,611–643.
Addicott,M.,Pearson,J.,Sweitzer,M.,Barack,D.,&Platt,M.(2017).Aprimeron
foragingandtheexplore/exploittrade-offforpsychiatryresearch.. 42,(pp.
1931–1939).
Andrews, M. (2020).The math is not the territory: Navigating the free energy
principle (p.18315).Pittphilsci.
Attias,H.(2000).Avariationalbaysianframeworkforgraphicalmodels.
Attias,H.2003.Planningbyprobabilisticinference.In Paper presented at the proc.
of the 9th int. workshop on artificial intelligence and statistics.
Badcock,P.B.,Friston,K.J.,Ramstead,M.J.D.,Ploeger,A.,&Hohwy,J.(2019).
The hierarchically mechanistic mind: an evolutionary systems theory of
thehumanbrain,cognition,andbehavior. Cognitive, Affective, & Behavioral
Neuroscience,19(6),1319–1351.
Barto,A.,Mirolli,M.,&Baldassarre,G.(2013).Noveltyorsurprise? Frontiers in
Psychology,4.
Beal, M. J. (2003).Variational algorithms for approximate bayesian inference .
London:UniversityofLondon.
Bekinschtein, T., Dehaene, S., Rohaut, B., Tadel, F., Cohen, L., & Naccache, L.
(2009).Neuralsignatureoftheconsciousprocessingofauditoryregularities.
Proceedings of the National Academy of Sciences of the United States of America,
106,1672–1677.
Bishop, C. M. (2006).Pattern recognition and machine learning . New York:
Springer.
Bogacz, R. (2017). A tutorial on the free-energy framework for modelling
perceptionandlearning. Journal of Mathematical Psychology,76,198–211.
Botvinick,M.,&Toussaint,M.(2012).Planningasinference. Trends in Cognitive
Sciences,16(10),485–488.
Breakspear, M. (2017). Dynamic models of large-scale brain activity.Nature
Neuroscience,20(3),340–352.
Brown, L. D. (1981). A complete class theorem for statistical problems with
finite-samplespaces. The Annals of Statistics,9(6),1289–1300.
Brown,T.H.,Zhao,Y.,&Leung,V.(2010).Hebbianplasticity.In Encyclopedia of
neuroscience (pp.1049–1056).
Bruineberg,J.,Dolega,K.,Dewhurst,J.,&Baltieri,M.(2021).TheEmperor’snew
markov blankets.Behavioral and Brain Research, 1–63. http://dx.doi.org/10.
1017/S0140525X21002351,Epubaheadofprint.PMID:34674782.
Bucci,A.,&Grasso,M.(2017).Sleepanddreaminginthepredictiveprocessing
framework. In T. Metzinger, & W. Wiese (Eds.),Philosophy and predictive
processing.JohannesGutenberg-UniversitätMainz.
Buckley,C.L.,SubKim,C.,McGregor,S.,&Seth,A.K.(2017).Thefreeenergy
principle for action and perception: A mathematical review.Journal of
Mathematical Psychology,81,55–79.
Burr, C., & Jones, M. (2016). The body as laboratory: Prediction-error mini-
mization, embodiment, and representation.Philosophical Psychology, 29(4),
586–600.
Carlin, B. P., & Louis, T. A. (1998).Bayes and empirical bayes methods for data
analysis.BocaRaton:Chapman&Hall/CRC.
Champion, T., Grześ, M., & Bowman, H. (2021). Realising active inference in
variational message passing: the outcome-blind certainty seeker.Neural
Computation.
Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the
futureofcognitivescience. The Behavioral and Brain Sciences,36,181–204.
Clark, A. (2015).Surfing uncertainty: Prediction, action, and the embodied mind.
NewYork:OxfordUniversityPress.
Clark, A. (2017). How to knit your own Markov blanket. In T. K. Metzinger,
&W.Wiese(Eds.), Philosophy and predictive processing.FrankfurtamMain:
MINDGroup.
Clark,J.E.,Watson,S.,&Friston,K.J.(2018).Whatismood?Acomputational
perspective.Psychologicalmedicine.(pp.1–8).
Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., & Friston, K. J. (2020).
Activeinferenceondiscretestate-spaces:Asynthesis. Journal of Mathematical
Psychology,99,Article102447.
Da Costa, L., Parr, T., Sengupta, B., & Friston, K. (2021). Neural dynamics
underactiveinference:Plausibilityandefficiencyofinformationprocessing.
Entropy,23(4),454.
Da Costa, L., Sajid, N., Parr, T., Friston, K. J., & Smith, R. (2020). The rela-
tionshipbetweendynamicprogrammingandactiveinference:thediscrete,
finite-horizoncase.arXiv,arXiv:2009.08111.
Dauwels, J. (2007). On variational message passing on factor graphs.IEEE
International Symposium on Information Theory,2546–2550.
deVries,B.,&Friston,K.J.(2017).Afactorgraphdescriptionofdeeptemporal
activeinference. Frontiers in Computational Neuroscience,11(95).
Fabry, R. E. (2017). Transcending the evidentiary boundary: Prediction error
minimization,embodiedinteraction,andexplanatorypluralism. Philosophical
Psychology,30(4),395–414.
Friston, K. J. (2019). A free energy principle for a particular physics. arXiv:
1906.10184.
Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Doherty, J. O.,
& Pezzulo, G. (2016a). Active inference and learning.Neuroscience and
Biobehavioral Reviews,68,862–879.
Friston,K.J.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,&Pezzulo,G.(2017a).
Activeinference:Aprocesstheory. Neural Computation,29,1–49.
Friston,K.J.,Lin,M.,Frith,C.,Pezzulo,G.,Hobson,J.,&Ondobaka,S.(2017b).
Activeinference,curiosityandinsight. Neural Computation,29,2633–2683.
Friston,K.J.,Litvak,V.,Oswal,A.,Razi,A.,Stephan,K.E.,vanWijk,B.C.M.,et
al.(2016b).BayesIanmodelreductionandempiricalBayesforgroup(DCM)
studies.NeuroImage,128,413–431.
Friston,K.J.,Mattout,J.,Trujillo-Barreto,N.,Ashburner,J.,&Penny,W.(2007).
Variational free energy and the Laplace approximation.NeuroImage, 34,
220–234.
Friston, K. J., Parr, T., & de Vries, B. (2017c). The graphical brain: Belief
propagationandactiveinference. Network Neuroscience,1,381–414.
Friston,K.J.,Rosch,R.,Parr,T.,Price,C.,&Bowman,H.(2018).Deeptemporal
models and active inference.Neuroscience & Biobehavioral Reviews , 90,
486–501.
Hesp, C., Smith, R., Allen, M., Friston, K. J., & Ramstead, M. J. D. (2020).
Deeplyfeltaffect:Theemergenceofvalenceindeepactiveinference. Neural
Computation,1–49.
Hobson, J., & Friston, K. (2012). Waking and dreaming consciousness: neu-
robiological and functional considerations.Progress in Neurobiology , 98,
82–98.
59
R. Smith, K.J. Friston and C.J. Whyte Journal of Mathematical Psychology 107 (2022) 102632
Hobson, J., Hong, C.-H., & Friston, K. (2014). Virtual reality and consciousness
inferenceindreaming. Frontiers in Psychology,5(1133).
Hohwy,J.(2014).Thepredictivemind.
Hohwy,J.(2016).Theself-evidencingbrain. Noû,50,259–285.
Hohwy,J.,Paton,B.,&Palmer,C.(2016).Distrustingthepresent. Phenomenology
and the Cognitive Sciences,15(3),315–335.
Kaplan,R.,&Friston,K.J.(2018).Planningandnavigationasactiveinference.
Biological Cybernetics,112(4),323–343.
Kuczma, M., & Gilányi, A. (2009).An introduction to the theory of functional
equations and inequalities : Cauchy’s equation and Jensen’s inequality(2nded.).
Basel,Boston,MA:Birkäeuser.
Loeliger, H. A. (2004). An introduction to factor graphs.IEEE Signal Processing
Magazine,21(1),28–41.
Markovic,D.,Stojic,H.,Schwoebel,S.,&Kiebel,S.(2021).Anempiricalevaluation
ofactiveinferenceinmulti-armedbandits.arXiv:2101.08699.
Mathys, C. D., Lomakina, E. I., Daunizeau, J., Iglesias, S., Brodersen, K. H.,
Friston, K. J., et al. (2014). Uncertainty in perception and the hierarchical
Gaussianfilter. Frontiers in Human Neuroscience,8(825).
Millidge, B. (2019). Combining active inference and hierarchical predictive
coding:Atutorialintroductionandcasestudy.PsyArXiv.
Millidge, B., Tschantz, A., & Buckley, C. L. (2021). Whence the expected free
energy?.Neural Computation,33(2),447–482.
Mirza, M. B., Adams, R. A., Mathys, C., & Friston, K. J. (2018). Human visual
exploration reduces uncertainty about the sensed world.PLOS ONE, 13,
Articlee0190429.
Neal,R.M.(1993). Probabilistic inference using Markov chain Monte Carlo meth-
ods. Department of Computer Science, University of Toronto. Toronto, ON,
Canada.
Oudeyer,P.-Y.,&Kaplan,F.(2007).Whatisintrinsicmotivation?atypologyof
computationalapproaches. Frontiers in Neurorobotics,1(6).
Parr, T., & Friston, K. J. (2017a). Uncertainty, epistemics and active inference.
Journal of the Royal Society, Interface,14.
Parr, T., & Friston, K. J. (2017b). Working memory, attention, and salience in
activeinference. Scientific Reports,7,14678.
Parr,T.,&Friston,K.J.(2018a).Theanatomyofinference:Generativemodels
andbrainstructure. Frontiers in Computational Neuroscience,12,90.
Parr, T., & Friston, K. J. (2018b). The discrete and continuous brain: From
decisionstomovement—andbackagain. Neural Computation,1–29.
Parr,T.,Markovic,D.,Kiebel,S.,&Friston,K.J.(2019).Neuronalmessagepassing
usingmean-field,bethe,andmarginalapproximations. Scientific Reports,9,
1889.
Parr, T., Pezzulo, G., & Friston, K. J. (2022).Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior.MITPress.
Pezzulo, G., Rigoli, F., & Friston, K. J. (2015). Active inference, homeostatic
regulation and adaptive behavioural control.Progress in Neurobiology, 134,
17–35.
Pezzulo, G., Rigoli, F., & Friston, K. J. (2018). Hierarchical active inference: A
theoryofmotivatedcontrol. Trends in Cognitive Sciences,22,294–306.
Ramachandran,V.S.(1988).Perceivingshapefromshading. Scientific American,
259(2),76–83.
Rigoux,L.,Stephan,K.E.,Friston,K.J.,&Daunizeau,J.(2014).BayesIanmodel
selectionforgroupstudies-revisited. Neuroimage,84,971–985.
Sajid,N.,Ball,P.,Parr,T.,&Friston,K.(2021).Activeinference:Demystifiedand
compared.Neural Computation,1–39.
Sales,A.C.,Friston,K.J.,Jones,M.W.,Pickering,A.E.,&Moran,R.J.(2019).Locus
coeruleus tracking of prediction errors optimises cognitive flexibility: An
activeinferencemodel. PLoS Computational Biology,15(1),Articlee1006267.
Sandved-Smith, L., Hesp, C., Mattout, J., Friston, K., Lutz, A., & Ramstead, M.
J. (2021a). Towards a computational phenomenology of mental action:
modelling meta-awareness and attentional control with deep parametric
activeinference. Neuroscience of Consciousness,2021(2),niab018.
Schmidhuber, J. (2006). Developmental robotics, optimal artificial curiosity,
creativity, music, and the fine arts.Journal of Connection Science , 18(2),
173–187.
Schwartenbeck, P., FitzGerald, T., Mathys, C., Dolan, R., & Friston, K. J. (2015).
The dopaminergic midbrain encodes the expected certainty about desired
outcomes.Cerebral Cortex,25,3434–3445.
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbich-
ler,M.,&Friston,K.J.(2019).Computationalmechanismsofcuriosityand
goal-directedexploration. Elife,8.
Smith, R., Badcock, P. B., & Friston, K. J. (2020a). Recent advances in the
applicationofpredictivecodingandactiveinferencemodelswithinclinical
neuroscience.Psychiatry and Clinical Neurosciences.
Smith,R.,Khalsa,S.S.,&Paulus,M.P.(2021b).Anactiveinferenceapproachto
dissectingreasonsfornonadherencetoantidepressants. Biological Psychiatry
Cognitive Neuroscience Neuroimaging,6(9),919–934.
Smith, R., Kirlic, N., Stewart, J. L., Touthang, J., Kuplicki, R., Khalsa, S. S., et
al.(2021d).Greaterdecisionuncertaintycharacterizesatransdiagnosticpa-
tientsampleduringapproach-avoidanceconflict:Acomputationalmodeling
approach.Journal of Psychiatry & Neuroscience,46(1),E74–E87.
Smith, R., Kirlic, N., Stewart, J., Touthang, J., Kuplicki, R., McDermott, T., et al.
(2021c).Long-termstabilityofcomputationalparametersduringapproach-
avoidanceconflictinatransdiagnosticpsychiatricpatientsample. Scientific
Reports,11.
Smith, R., Kuplicki, R., Feinstein, J., Forthman, K. L., Stewart, J. L., Paulus, M.
P., et al. (2020b). A Bayesian computational model reveals a failure to
adapt interoceptive precision estimates across depression, anxiety, eating,
and substance use disorders.PLoS Computational Biology , 16(12), Article
e1008484.
Smith,R.,Kuplicki,R.,Teed,A.,Upshaw,V.,&Khalsa,S.S.(2020c).Confirmatory
evidence that healthy individuals can adaptively adjust prior expectations
andinteroceptiveprecisionestimates.InT.Verbelen,P.Lanillos,C.Buckley,
&C.DeBoom(Eds.), Active inference.IWAI2020,Cham:Springer.
Smith,R.,Lane,R.D.,Parr,T.,&Friston,K.J.(2019a).Neurocomputationalmech-
anisms underlying emotional awareness: Insights afforded by deep active
inferenceandtheirpotentialclinicalrelevance. Neuroscience & Biobehavioral
Reviews,107,473–491.
Smith,R.,Mayeli,A.,Taylor,S.,AlZoubi,O.,Naegele,J.,&Khalsa,S.S.(2021e).
Gut inference: A computational modelling approach.Biological Psychology,
Article108152.
Smith, R., Parr, T., & Friston, K. J. (2019b). Simulating emotions: An active
inferencemodelofemotionalstateinferenceandemotionconceptlearning.
Frontiers in Psychology,10(2844).
Smith,R.,Ramstead,M.J.D.,&Kiefer,A.(2022).Activeinferencemodelsdonot
contradictfolkpsychology. Synthese,InPress.
Smith,R.,Schwartenbeck,P.,Parr,T.,&Friston,K.J.(2020d).Anactiveinference
approach to modeling structure learning: Concept learning as an example
case.Frontiers in Computational Neuroscience,14(41).
Smith, R., Schwartenbeck, P., Stewart, J. L., Kuplicki, R., Ekhtiari, H., Investiga-
tors,T.,etal.(2020e).Impreciseactionselectioninsubstanceusedisorder:
Evidenceforactivelearningimpairmentswhensolvingtheexplore-exploit
dilemma.Drug and Alcohol Dependence,215,Article108208.
Stephan, K., Penny, W. D., Daunizeau, J., Moran, R. J., & Friston, K. J. (2009).
BayesIanmodelselectionforgroupstudies. Neuroimage,46(4),1004–1017.
Stephan,K.E.,Petzschner,F.H.,Kasper,L.,Bayer,J.,Wellstein,K.V.,Stefanics,G.,
et al. (2019). Laminar fMRI and computational theories of brain function.
Neuroimage,197,699–706.
Tononi,G.,&Cirelli,C.(2014).Sleepandthepriceofplasticity:Fromsynaptic
andcellularhomeostasistomemoryconsolidationandintegration. Neuron,
81,12–34.
Tschantz,A.,Barca,L.,Maisto,D.,Buckley,C.L.,Seth,A.K.,&Pezzulo,G.(2021).
Simulating homeostatic, allostatic and goal-directed forms of interoceptive
controlusingactiveinference.bioRxiv,2021.2002.2016.431365.
Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). Learning action-oriented
models through active inference. PLoS Computational Biology , 16(4),
Articlee1007805.
Tu, S. (2014).The dirichlet-multinomial and dirichlet-categorical models for
bayesian inference.Berkeley,CA:ComputerScienceDivision,UCBerkeley.
Wald,A.(1947).Anessentiallycompleteclassofadmissibledecisionfunctions.
The Annals of Mathematical Statistics,54,9–555.
Whyte,C.,Hohwy,J.,&Smith,R.(2021).Anactiveinferencemodelofconscious
access:Howcognitiveactionselectionreconcilestheresultsofreportand
no-reportparadigms.http://dx.doi.org/10.31234/osf.io/mkzx8,PsyArXiv.
Whyte,C.,&Smith,R.(2020).Thepredictiveglobalneuronalworkspace:Afor-
malactiveinferencemodelofvisualconsciousness. Progress in Neurobiology,
2020.2002.2011.944611.
Wilson, R., Geana, A., White, J., Ludvig, E., & Cohen, J. (2014). Humans use
directed and random exploration to solve the explore-exploit dilemma.
Journal of Experimental Psychology. General,143,2074–2081.
Winn,J.,&Bishop,C.M.(2005).Variationalmessagepassing. Journal of Machine
Learning Research,6,661–694.
Zeidman,P.,Jafarian,A.,Seghier,M.L.,Litvak,V.,Cagnan,H.,Price,C.J.,etal.
(2019).Aguidetogroupeffectiveconnectivityanalysis,Part2:Secondlevel
analysiswithPEB. Neuroimage,200,12–25.
60