Heliyon 10 (2024) e39129
Available online 18 October 2024
2405-8440/Â© 2024 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC license
(http://creativecommons.org/licenses/by-nc/4.0/).
Contentslistsavailableat ScienceDirect
Heliyon
journal homepage: www.cell.com/heliyon
Research article
Slow but ï¬‚exible or fast but rigid? Discrete and continuous 
processes compared
MatteoPriorelli, Ivilin PeevStoianov âˆ—
Institute of Cognitive Sciences and Technologies, National Research Council of Italy, Padova, Italy
A B S T R A C T
A tradeoï¬€ exists when dealing with complex tasks composed of multiple steps. High-level cognitive processes can ï¬nd the best sequence of actions 
to achieve a goal in uncertain environments, but they are slow and require signiï¬cant computational demand. In contrast, lower-level processing 
allows reacting to environmental stimuli rapidly, but with limited capacity to determine optimal actions or to replan when expectations are not met. 
Through reiteration of the same task, biological organisms ï¬nd the optimal tradeoï¬€: from action primitives, composite trajectories gradually emerge 
by creating task-speciï¬c neural structures. The two frameworks of active inference â€“ a recent brain paradigm that views action and perception 
as subject to the same free energy minimization imperative â€“ well capture high-level and low-level processes of human behavior, but how task 
specialization occurs in these terms is still unclear. In this study, we compare two strategies on a dynamic pick-and-place task: a hybrid (discrete-
continuous) model with planning capabilities and a continuous-only model with ï¬xed transitions. Both models rely on a hierarchical (intrinsic and 
extrinsic) structure, well suited for deï¬ning reaching and grasping movements, respectively. Our results show that continuous-only models perform 
better and with minimal resource expenditure but at the cost of less ï¬‚exibility. Finally, we propose how discrete actions might lead to continuous 
attractors and compare the two frameworks with diï¬€erent motor learning phases, laying the foundations for further studies on bio-inspired task 
adaptation.
1. Introduction
How does the brain support the eï¬ƒcient execution of tasks comprising multiple steps, such as picking and placing an object? 
While a sequence of movements is easy to plan in static contexts, a diï¬ƒculty emerges when acting in dynamic environments, e.g., 
when the object has to be grasped on the ï¬‚y. Tackling complex multi-step tasks is known to occur deep in the cortical hierarchy, by 
areas processing discrete, slow-varying entities [1]. But the advantage of discrete representations useful for planning [2]comes at a 
cost: if an object moves too fast, deep processing might be too slow for the grasping action to succeed.
Motor skill consolidation is a well-known phenomenon [3], clearly evident in athletes [4â€“6]: during repeated exposure to a task, 
an initial learning phase gives way to autonomous movements as the athlete becomes more proï¬cient [7]. Cortical involvement is 
gradually reduced as specialized neural structures are constructed in lower, fast subcortical regions [8], including ï¬ne adjustment 
in the spinal cord reï¬‚exes [9]. These structures cannot extract invariant representations to perform high-level decision-making but 
they react rapidly to the sensorium. The basal ganglia are known to be central to task specialization as the striatum encodes chunked 
representations of action steps composing a learned habit [10,11]. But this change seems much more pervasive, involving informa-
tion processing within the cortex itself and a general activity shift from anterior to posterior regions [12], as evidenced by plastic 
changes in the primary motor cortex [13]. The brain mechanisms underlying such changes are still unclear, and understanding their 
computational basis is compelling.
* Corresponding author.
E-mail address: ivilinpeev.stoianov@cnr.it(I.P.Stoianov).
https://doi.org/10.1016/j.heliyon.2024.e39129
Received 5 January 2024; Received in revised form 29 September 2024; Accepted 8 October 2024
Heliyon 10 (2024) e39129
2
M. Priorelli and I.P. Stoianov
An interesting proposal is that the brain maintains a model of the task dynamics, and skill consolidation consists of ï¬ne-tuning 
this model to achieve a stable behavior accounting for environmental uncertainty [14,15]. This hypothesis ï¬ts with a recent theory 
called active inference, which brings insights of increasing appeal into the computational role of the nervous system [16â€“18]. Active 
inference provides a formalization of the two essential components of control, sensing and acting, which supposedly aim to resolve the 
critical goal of all organisms: to survive in uncertain environments by operating within preferred states (e.g., maintaining a constant 
temperature). As in predictive coding [19], active inference assumes that organisms perceive the environment through an internal 
generative model constructed by inferring how hidden causes produce sensations [17,20]. Crucially, perception and action jointly 
minimize a quantity called free energy: while perception gradually adjusts internal expectations to match sensory evidence, action 
gradually samples those sensations that make the expectations true [17,20].
The two frameworks of this theory have been used to analyze high-level and low-level processes of human behavior under a 
uniï¬ed perspective. Active inference in continuous time deï¬nes an internal dynamic model about the self and external targets in 
generalized coordinates of motion (position, velocity, acceleration, and so on) [20,21]. Elementary movements are then resolved by 
local suppression of proprioceptive prediction errors in classical reï¬‚ex arcs [22], forcing a physical change in the actuators. This 
framework is ideal for interacting with real environments; however, it falls short when it comes to multi-step actions, since free 
energy minimization only deals with current sensory signals.
At the other extreme, decision-making is addressed by the discrete formulation [23â€“25], which views planning as an inferential 
mechanism [26,27]. Discrete active inference models leverage Partially Observed Markov Decision Processes (POMDPs) to plan 
abstract actions over (yet unobserved) outcomes thanks to the minimization of the expected free energy, i.e., the free energy that the 
agent expects to perceive in the future [28â€“31]. This framework seems well suited to analyze cortical operations [1], with higher 
biological plausibility compared to reinforcement learning [32,33]. However, the downside of discrete computations is the lack of 
real-time interaction with the environment.
Between the two stands a third, hybrid, framework, which speciï¬es a discrete model combined with its continuous counterpart 
[34,35]. This involves the transformation of continuous signals into discrete messages through Bayesian Model Reduction [36,37]. 
Solving multi-step tasks with a hybrid model implies associating elementary continuous trajectories to discrete states for planning. 
The mathematical link between prediction errors in continuous formulations and variational free energy in discrete formulations 
is straightforward: precision-weighted prediction errors are variational free energy gradients, which are minimized at free energy 
minima, where the gradients are destroyed. This approach has not received adequate attention yet, as not many implementations can 
be found in the literature [2,18,34,38,35,39,40], none simulating applications in dynamic contexts.
Importantly, it is not entirely clear how active inference could simulate the virtuous cycle of motor skill acquisition, which begins 
with the rigid information processing of subcortical structures and culminates with the emergence of task-speciï¬c structures. To shed 
light on this issue, the hybrid framework could be highly relevant: discrete computations can simulate the behavior of an agent 
that has no prior idea of what actions to take. However, as soon as the agent has learned a policy that corresponds to a sequence 
of a priori-deï¬ned actions, it can adapt by encoding the transitions between discrete states into continuous dynamics, which then 
represent a learned motor skill.
In summary, this paper foregrounds the distinction between active inference under continuous and hybrid generative models, with 
special reference to their neurobiological implementation and the biomimetic advantages aï¬€orded by both kinds of models. Several 
perspectives might help understand the key distinction. Active inference under continuous state-space models can be regarded as a 
generalization of â€œcontrol as inferenceâ€; e.g., [41â€“43]. Control as inference itself can be thought of as generalizing linear quadratic 
control to include nonlinear and deep (hierarchical) state space models. In neurobiology, this kind of control is often referred to in 
terms of equilibrium point or trajectory hypotheses [44,45]; namely, using motor reï¬‚exes to eliminate proprioceptive or actuator pre-
diction errors â€“ where the predictions provide setpoints or trajectories that replace motor commands. In the active inference literature, 
these schemes are sometimes referred to as â€œmerely reï¬‚exiveâ€ and are apt to describe various forms of homeostatic behavior or be-
haviors driven by autonomous dynamics (e.g., central pattern generators). Merely reï¬‚exive active inference should be compared with 
the â€œplanning as inferenceâ€ [46,26,35]aï¬€orded by equipping continuous state space models with discrete state-spaces that include 
the consequences of sequential policies. This brings a future-pointing aspect to behavior, aï¬€orded by rolling out sequences of actions 
and evaluating them under posterior predictive distributions over outcomes. In neurobiology, this can be framed as a move from 
homeostasis to allostasis [47,39]and aï¬€ords the capacity for information-seeking and preference-seeking behaviors that go beyond 
merely reï¬‚exive behavior. The price paid for this prospective kind of active inference is the computational cost and requisite course 
graining (into discrete states). This trade-oï¬€ speaks to the integration of continuous and discrete models of the sort described below.
2. Methods
2.1. Perception, control, and the variational free energy
Active inference assumes that organisms perceive the environment through an internal generative model that infers how external 
causes produce sensations and how they evolve [17,20]. In the continuous domain, this model is usually factorized into probability 
distributions over hidden states Ìƒğ’™, hidden causes Ìƒğ’— (i.e., causal variables over the hidden states dynamics), and observable outcomes 
Ìƒğ’š:
ğ‘(Ìƒğ’™, Ìƒğ’—, Ìƒğ’š)= ğ‘(Ìƒğ’š|Ìƒğ’™)ğ‘(Ìƒğ’™|Ìƒğ’—)ğ‘(Ìƒğ’—) (1)
The symbol âˆ¼ indicates variables encoded in generalized coordinates of motion, representing instantaneous trajectories (position, 
velocity, acceleration, and so on), e.g., Ìƒğ’™=[ğ’™, ğ’™â€², ğ’™â€²â€², â€¦ ]. These distributions are assumed to be Gaussian:
Heliyon 10 (2024) e39129
3
M. Priorelli and I.P. Stoianov
ğ‘(Ìƒğ’š|Ìƒğ’™)= îˆº(Ìƒğ’ˆ(Ìƒğ’™), Ìƒğš·âˆ’1
ğ‘¦ )
ğ‘(îˆ°Ìƒğ’™|Ìƒğ’—)= îˆº(Ìƒğ’‡(Ìƒğ’™, Ìƒğ’—), Ìƒğš·âˆ’1
ğ‘¥ )
ğ‘(Ìƒğ’—|ğœ¼)= îˆº(ğœ¼, Ìƒğš·âˆ’1
ğ‘£ )
corresponding to the following non-linear stochastic equations that represent how the environment evolves:
Ìƒğ’š = Ìƒğ’ˆ(Ìƒğ’™)+ ğ’˜ğ‘¦
îˆ°Ìƒğ’™= Ìƒğ’‡(Ìƒğ’™, Ìƒğ’—)+ ğ’˜ğ‘¥
Here, îˆ° is the diï¬€erential shift operator that shifts all the temporal orders by one, i.e.: îˆ°Ìƒğ’™ =[ ğ’™â€², ğ’™â€²â€², ğ’™â€²â€²â€², â€¦ ], and we indicated 
with the symbol Ìƒğš· the generalized precisions (or inverse variances) of the distributions. Directly evaluating the posterior ğ‘(Ìƒğ’™|Ìƒğ’š)
is intractable since it involves the computation of the inaccessible marginal ğ‘(Ìƒğ’š). The variational solution is to approximate the 
posteriors with recognition distributions of a more tractable form. Perceptual inference then turns into a minimization of the diï¬€erence 
between the approximate and real posteriors, which can be formalized in terms of a KL divergence â€“ equivalent to the expectation, 
over the approximate posterior, of the diï¬€erence between the two log-probabilities:
ğ·ğ¾ğ¿[ğ‘(Ìƒğ’™)||ğ‘(Ìƒğ’™|Ìƒğ’š)]= âˆ«
ğ‘¥
ğ‘(Ìƒğ’™)ln ğ‘(Ìƒğ’™)
ğ‘(Ìƒğ’™|Ìƒğ’š) = ğ”¼
ğ‘(Ìƒğ’™)
[lnğ‘(Ìƒğ’™)âˆ’ln ğ‘(Ìƒğ’™|Ìƒğ’š)]
Given that the denominator ğ‘(Ìƒğ’™|Ìƒğ’š) still depends on the marginal ğ‘(Ìƒğ’š), we express the KL divergence in terms of the log-evidence and 
the Variational Free Energy (VFE), and minimize the latter quantity instead:
îˆ² = ğ”¼
ğ‘(Ìƒğ’™)
[
ln ğ‘(Ìƒğ’™)
ğ‘(Ìƒğ’™, Ìƒğ’š)
]
= ğ”¼
ğ‘(Ìƒğ’™)
[
ln ğ‘(Ìƒğ’™)
ğ‘(Ìƒğ’™|Ìƒğ’š)
]
âˆ’ln ğ‘(Ìƒğ’š)
Since the KL divergence is non-negative (due to Jensenâ€™s inequality), the VFE provides an upper bound on surprise â€“ thus, its op-
timization improves both evidence and model ï¬t. One common assumption about the form of the recognition distributions is the 
Laplace approximation [16]:
ğ‘(Ìƒğ’™)= îˆº(Ìƒğ’™; Ìƒğ, Ìƒğ‘·
âˆ’1
ğ‘¥ )
ğ‘(Ìƒğ’—)= îˆº(Ìƒğ’—; Ìƒğ‚, Ìƒğ‘·
âˆ’1
ğ‘£ )
where the parameters Ìƒğ and Ìƒğ‚ are called beliefs over hidden states and hidden causes, while Ìƒğ‘·ğ‘¥ and Ìƒğ‘·ğ‘£ are their precisions. Under 
this assumption, the free energy breaks down to a simple formula (see [18]for more details):
îˆ² = 1
2
[
Ìƒğš·ğ‘¦Ìƒğœº2
ğ‘¦ + Ìƒğš·ğ‘¥Ìƒğœº2
ğ‘¥ + Ìƒğš·ğ‘£Ìƒğœº2
ğ‘£ +ğ¶
]
where Ìƒğœºğ‘ , Ìƒğœºğ‘¥ and Ìƒğœºğ‘£ are respectively prediction errors of sensations, dynamics and prior:
Ìƒğœºğ‘¦ = Ìƒğ’šâˆ’ Ìƒğ’ˆ(Ìƒğ)
Ìƒğœºğ‘¥ =îˆ°Ìƒğâˆ’ Ìƒğ’‡(Ìƒğ, Ìƒğ‚)
Ìƒğœºğ‘£ = Ìƒğ‚âˆ’ğœ¼
(2)
and we included in ğ¶ the terms that disappear when computing the gradients. Then, minimizing the free energy with respect to 
hidden states and hidden causes turns into an iterative parameter update through the following expressions:
Ì‡Ìƒğâˆ’îˆ°Ìƒğ=âˆ’ğœ•ğœ‡îˆ² =ğœ•Ìƒğ’ˆğ‘‡ Ìƒğš·ğ‘¦Ìƒğœºğ‘¦ +ğœ•ğœ‡ Ìƒğ’‡ğ‘‡ Ìƒğš·ğ‘¥Ìƒğœºğ‘¥ âˆ’îˆ°ğ‘‡ Ìƒğš·ğ‘¥Ìƒğœºğ‘¥
Ì‡Ìƒğ‚âˆ’îˆ°Ìƒğ‚ =âˆ’ğœ•ğœˆîˆ² =ğœ•ğœˆ Ìƒğ’‡ğ‘‡ Ìƒğš·ğ‘¥Ìƒğœºğ‘¥ âˆ’ Ìƒğš·ğ‘£Ìƒğœºğ‘£
(3)
Notably, the VFE can also be minimized by acting, computing the following motor control signals:
Ì‡ğ’‚=âˆ’ğœ•ğ’‚îˆ² =âˆ’ğœ•ğ’‚Ìƒğ’ˆğ‘‡ Ìƒğš·ğ‘¦Ìƒğœºğ‘¦
In continuous-time active inference, goals are generally encoded as prior beliefs in the hidden causes Ìƒğ’—. These generate sensory predic-
tions about a speciï¬c evolution of the world in accordance with the agentâ€™s beliefs. If these predictions are realized by movement, the 
agent eventually looks for those states that make his or her belief true. This process â€“ known as self-evidencing â€“ allows implementing 
goal-directed behavior through VFE minimization, keeping the agent in predictable and safer spaces [17]. Note that this process can 
be scaled up to construct a hierarchical model, wherein each level is involved in computing a prediction, comparing it with the state 
below, and updating its state with the generated prediction error. These simple steps are repeated throughout the whole hierarchy 
so that, from a prior belief encoding a goal, a cascade of proprioceptive trajectories is generated that is eventually realized by the 
lowest levels of the motor system.
Heliyon 10 (2024) e39129
4
M. Priorelli and I.P. Stoianov
2.2. Planning with the expected free energy
Although active inference in continuous time can deal with real-world problems by keeping track of instantaneous trajectories, it 
has several limitations and a narrow use, as it cannot easily handle more general types of actions arising from decision-making. VFE 
minimization can only adjust the internal generative model depending on current (or past) observations, and it does not evaluate 
future states and outcomes. To endow an agent with this ability, a quantity called Expected Free Energy (EFE) is considered [23,24]. 
We consider a generative model similar to Equation (1), but controlled by policies ğ…:
ğ‘(ğ’”,ğ’,ğ…)= ğ‘(ğ’|ğ’”,ğ…)ğ‘(ğ’”|ğ…)ğ‘(ğ…)
where ğ’” and ğ’ are discrete hidden states and outcomes. Note that policies are not simple stimulus-response mappings â€“ as in Rein-
forcement Learning schemes â€“ but sequences of actions. The agentâ€™s generative model is factorized as in POMDPs:
ğ‘(ğ’”1âˆ¶ğœ,ğ’1âˆ¶ğœ,ğ…)= ğ‘(ğ’”1)â‹… ğ‘(ğ…)â‹…
ğœâˆ
ğœ=1
ğ‘(ğ’ğœ|ğ’”ğœ)â‹…
ğœâˆ
ğœ=2
ğ‘(ğ’”ğœ|ğ’”ğœâˆ’1,ğ…)
where ğœ is the total number of discrete steps. These elements are represented with categorical distributions:
ğ‘(ğ’”1)= ğ¶ğ‘ğ‘¡(ğ‘«)
ğ‘(ğ…)= ğ¶ğ‘ğ‘¡(ğ‘¬)
ğ‘(ğ’ğœ|ğ’”ğœ)= ğ¶ğ‘ğ‘¡(ğ‘¨)
ğ‘(ğ’”ğœ|ğ’”ğœâˆ’1,ğ…)= ğ¶ğ‘ğ‘¡(ğ‘©ğœ‹,ğœ)
where ğ‘« encodes beliefs about the initial state, ğ‘¬ encodes the prior over policies, ğ‘¨is the likelihood matrix and ğ‘©ğœ‹,ğœ is the transition 
matrix. Similar to the continuous case, perceptual inference relies on an approximate posterior distribution ğ‘(ğ’”1âˆ¶ğœ, ğ…), and minimizes 
the VFE â€“ which in turn minimizes the Kullback-Leibler (KL) divergence between the approximate and real posteriors:
0â‰¤ =ğ·ğ¾ğ¿[ğ‘(ğ’”1âˆ¶ğœ,ğ…)||ğ‘(ğ’”1âˆ¶ğœ,ğ…|ğ’1âˆ¶ğœ)]
=ğ”¼ğ‘[lnğ‘(ğ’”1âˆ¶ğœ,ğ…)âˆ’ln ğ‘(ğ’1âˆ¶ğœ,ğ’”1âˆ¶ğœ,ğ…)]+ln ğ‘(ğ’1âˆ¶ğœ)
=îˆ² +ln ğ‘(ğ’1âˆ¶ğœ)
If we assume, under the mean-ï¬eld approximation, that the approximate posterior factorizes into independent distributions:
ğ‘(ğ’”1âˆ¶ğœ|ğ’1âˆ¶ğœ,ğ…)â‰ˆ ğ‘(ğ’”1âˆ¶ğœ,ğ…)= ğ‘(ğ…)
ğœâˆ
ğœ
ğ‘(ğ’”ğœ|ğœ‹)
ğ‘(ğ…)= ğ¶ğ‘ğ‘¡(ğ…)
ğ‘(ğ’”ğœ|ğ…)= ğ¶ğ‘ğ‘¡(ğ’”ğœ‹,ğœ)
we can use the standard technique of variational message passing to infer each posterior ğ‘(ğ’”ğœ|ğ…) and combine them into a global 
posterior ğ‘(ğ’”1âˆ¶ğœ|ğ…). In order to update the posterior about hidden states, we combine the messages from past states, future states, and 
outcomes, express each term with its suï¬ƒcient statistics, and ï¬nally apply a softmax function to get a proper probability distribution:
ğ’”ğœ‹,ğœ =ğœ(lnğ‘©ğœ‹,ğœâˆ’1ğ’”ğœ‹,ğœâˆ’1 +ln ğ‘©ğ‘‡
ğœ‹,ğœğ’”ğœ‹,ğœ+1 +ln ğ‘¨ğ‘‡ ğ’ğœ)
Instead, action planning involves inferring those policies that lead to the desired outcomes. Hence, the EFE is speciï¬cally constructed 
by considering future outcomes as random variables, and by conditioning over them:
îˆ³ğœ‹ = ğ”¼
ğ‘(ğ’”,ğ’|ğ…)
[
ln ğ‘(ğ’”|ğ…)
ğ‘(ğ’”,ğ’|ğ…)
]
â‰ˆ ğ”¼
ğ‘(ğ’”,ğ’|ğ…)
[
ln ğ‘(ğ’”)
ğ‘(ğ’”|ğ’,ğ…)
]
âˆ’ ğ”¼
ğ‘(ğ’|ğ…)
[lnğ‘(ğ’|ğ‘ª)]
where the probability distribution ğ‘(ğ’|ğ‘ª) encodes preferred outcomes. The last two RHS terms are respectively called epistemic
(uncertainty-reducing) and pragmatic (goal-seeking). In order to update ğ‘(ğ…), we combine the messages from the prior over policies 
given by the matrix ğ‘¬, and from future observations conditioned upon policies; we can approximate the latter by the EFE conditioned 
at a particular time ğœ:
îˆ³ğœ‹ â‰ˆ
âˆ‘
ğœ
ğ·ğ¾ğ¿[ğ‘(ğ’ğœ|ğ…)||ğ‘(ğ’ğœ|ğ‘ª)]+ ğ”¼
ğ‘(ğ’”ğœ|ğ’”ğœâˆ’1,ğ…)
[ğ»[ğ‘(ğ’ğœ|ğ’”ğœ)]]
=
âˆ‘
ğœ
ğ’ğœ‹,ğœ(lnğ’ğœ‹,ğœ âˆ’ğ‘ªğœ)+ ğ’”ğœ‹,ğœğ‘¯ğ´
where:
ğ’ğœ‹,ğœ =ğ‘¨ğ’”ğœ‹,ğœ ğ‘ªğœ =ln ğ‘(ğ’ğœ|ğ‘ª) ğ‘¯ğ´ =âˆ’ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘¨ğ‘‡ lnğ‘¨)
Finally, we select the action ğ‘¢ that is the most likely under all policies:
Heliyon 10 (2024) e39129
5
M. Priorelli and I.P. Stoianov
ğ… =ğœ(lnğ‘¬ âˆ’î‰)
ğ‘¢ğ‘¡ =argmax
ğ‘¢
ğ…â‹… [ğ‘ˆğœ‹,ğ‘¡ =ğ‘¢]
As continuous-time active inference, discrete models can be scaled as well. This leads to a hierarchy of temporal dynamics, wherein 
low levels adapt more frequently to sensory observations, while high levels construct increasingly invariant representations of the 
environment. Diï¬€erently from continuous models, active inference in discrete state-spaces allows to operate with abstract actions at 
diï¬€erent levels, creating a hierarchical plan composed of several subgoals [2].
2.3. Bayesian model reduction in hybrid models
To get a discrete model working with the richness of continuous signals, a form of communication with the continuous framework 
is required. Since the two frameworks operate in diï¬€erent domains, we need a way to obtain a continuous prior from a discrete 
prediction and, concurrently, to estimate a discrete outcome based on continuous evidence. Both problems can be easily addressed 
through Bayesian model reduction [37,36]. Consider a generative model ğ‘(ğœ½, ğ’š) with parameters ğœ½ and data ğ’š:
ğ‘(ğœ½,ğ’š)= ğ‘(ğ’š|ğœ½)ğ‘(ğœ½)
and an additional distribution Ìƒğ‘(ğ’š, ğœ½), which is a reduced version of the ï¬rst model if the likelihood of some data is the same under 
both models and the only diï¬€erence rests upon the speciï¬cation of the priors Ìƒğ‘(ğœ½). We can express the posterior of the reduced model 
as:
Ìƒğ‘(ğœ½|ğ’š)= ğ‘(ğœ½|ğ’š) Ìƒğ‘(ğœ½)ğ‘(ğ’š)
ğ‘(ğœ½)Ìƒğ‘(ğ’š)
The procedure for computing the reduced posterior is the following: ï¬rst, we integrate over the parameters to obtain the evidence 
ratio of the two models:
Ìƒğ‘(ğ’š)= ğ‘(ğ’š)âˆ« ğ‘(ğœ½|ğ’š) Ìƒğ‘(ğœ½)
ğ‘(ğœ½)ğ‘‘ğœ½
Then, we deï¬ne an approximate posterior ğ‘(ğœ½) and we compute the reduced VFE in terms of the full model:
îˆ²[Ìƒğ‘(ğœ½)]â‰ˆ îˆ²[ğ‘(ğœ½)]+ln ğ”¼ğ‘
[ Ìƒğ‘(ğœ½)
ğ‘(ğœ½)
]
(4)
Correspondingly, the approximate posterior of the reduced model can be written in terms of the full model:
ln Ìƒğ‘(ğœ½)=ln ğ‘(ğœ½)+ln Ìƒğ‘(ğœ½)
ğ‘(ğœ½) âˆ’ln ğ”¼
ğ‘
[ Ìƒğ‘(ğœ½)
ğ‘(ğœ½)
]
In order to apply this technique to the communication between discrete and continuous models, we ï¬rst consider the hidden causes 
ğ’— as parameters and denote with ğ’š the continuous observations. We then assume that the full prior probability depends on a discrete 
outcome:
ğ‘(ğ’—|ğ’)= îˆº(ğœ¼,ğš·âˆ’1
ğ‘£ )
In this way, the parameter ğœ¼ acts as a prior over the hidden causes. We also assume that the agent maintains ğ‘€ (Gaussian) reduced 
models:
Ìƒğ‘(ğ’—|ğ’,ğ‘š)= îˆº(ğœ¼ğ‘š,ğš·âˆ’1
ğ‘š ) (5)
with prior ğœ¼ğ‘š. These priors correspond to the agentâ€™s hypotheses about the causes of its sensorium, i.e., they encode a speciï¬c 
conï¬guration of the continuous model, and each of them is associated with a particular discrete outcome ğ‘œğœ,ğ‘š, i.e., ğ’ğœ =[ğ‘œğœ,1, â€¦ , ğ‘œğœ,ğ‘€ ]. 
The latter is computedby marginalizing ğ’ğœ‹,ğœ over all policies:
ğ’ğœ =
âˆ‘
ğœ‹
ğœ‹ğœ‹ğ’ğœ‹,ğœ
Then, the prior ğœ¼ of the full model, which represents the actual agentâ€™s parameters of the current state of the world, is obtained by 
simply performing a Bayesian model average between every speciï¬ed model:
ğœ¼=
âˆ‘
ğ‘š
ğ‘œğœ,ğ‘šğœ¼ğ‘š
thus transforming discrete probabilities into a continuous value. As a result, this quantity biases the belief ğ‚ over hidden causes 
following Equations (3)-(2). Similarly, having deï¬ned the full and reduced approximate posteriors:
ğ‘(ğ’—)= îˆº(ğ‚,ğ‘·âˆ’1
ğ‘£ )
Ìƒğ‘(ğ’—|ğ‘š)= îˆº(ğ‚ğ‘š,ğ‘·âˆ’1
ğ‘š )
Heliyon 10 (2024) e39129
6
M. Priorelli and I.P. Stoianov
we compute an ascending message (acting as an observation for the discrete model) through Bayesian model comparison, i.e., by 
comparing the agentâ€™s expectations with the observed evidence. More formally, the free energy computed in Equation (4) acts as a 
hint to how well a reduced model explains continuous observations, and it corresponds to the prior surprise plus the log-evidence for 
each outcome model accumulated over time:
îˆ²ğ‘š =âˆ’ln ğ‘œğœ,ğ‘š âˆ’
ğ‘‡
âˆ«
0
îˆ¸ğ‘šğ‘‘ğ‘¡
We refer to ğ‘‡ as sampling time window: every discrete time step ğœ corresponds to a continuous period ğ‘‡ during which evidence is 
accumulated. The Laplace approximation [48]leads to a simple form of the second RHS term:
îˆ¸ğ‘š = 1
2 ln|ğš·ğ‘šğ‘·ğ‘£ğ‘·âˆ’1
ğ‘š ğšºğ‘£| âˆ’ 1
2(ğ‚ğ‘‡ ğ‘·ğ‘£ğ‚âˆ’ğ‚ğ‘‡
ğ‘šğ‘·ğ‘šğ‚ğ‘š âˆ’ğœ¼ğ‘‡ ğš·ğ‘£ğœ¼+ğœ¼ğ‘‡
ğ‘šğš·ğ‘šğœ¼ğ‘š) (6)
where the mean and precision of the ğ‘šth reduced model are:
ğ‚ğ‘š =ğ‘·âˆ’1
ğ‘š (ğ‘·ğ‘£ğ‚âˆ’ğš·ğ‘£ğœ¼+ğš·ğ‘šğœ¼ğ‘š)
ğ‘·ğ‘š =ğ‘·ğ‘£ âˆ’ğš·ğ‘£ +ğš·ğ‘š
After having computed the free energy associated with each model, they are normalized through a softmax function to get a proper 
probability, which is ï¬nally used to estimate the current discrete state:
ğ’“ğœ =ğœ(âˆ’î‰Œ)
ğ’”ğœ‹,ğœ =ğœ(lnğ‘©ğœ‹,ğœâˆ’1ğ’”ğœ‹,ğœâˆ’1 +ln ğ‘©ğ‘‡
ğœ‹,ğœğ’”ğœ‹,ğœ+1 +ln ğ‘¨ğ‘‡ ğ’“ğœ)
(7)
In summary, each value of ğ’“ğœ indicates how probable an agentâ€™s hypothesis is based on the accumulated continuous evidence. In 
contrast to the discrete-only case, the discrete actions in a hybrid model are only implicitly used to compute the posterior over policies, 
and the link between the two frameworks is possible by associating each discrete outcome model ğ‘œğœ,ğ‘š to a speciï¬c continuous prior 
ğœ¼ğ‘š. These priors are combined to realize an average of the agentâ€™s hypotheses, and are concurrently compared with the estimated 
continuous trajectories to infer the true cause of the sensorium.
3. Results
To shed light on task specialization under active inference, we ï¬rst delineate how to specify multiple hidden state dynamics in the 
agentâ€™s generative model. These dynamics are related to the agentâ€™s desires (e.g., reaching or grasping an object) or intentions [49,50]. 
The deï¬ned method is used as the basis for tackling dynamic multi-step tasks in both hybrid or continuous-only frameworks, showing 
that discrete actions and continuous trajectories work under similar mechanisms. We then compare the behavioral diï¬€erences between 
hybrid and continuous frameworks in terms of planning capabilities and reaction times. To this aim, we consider a pick-and-place 
operation. The agentâ€™s body is an 8-DoF simulated arm with two ï¬ngers. At each trial, a target object spawns with a random and 
unknown position; the agentâ€™s goal is to reach the object, grasp it, reach a random goal position, and open the ï¬ngers to place the 
object. To assess the modelâ€™s capabilities in dynamic environments, at each trial a velocity with random direction is assigned to the 
object, so that the grasping action might fail if the object moves too fast.
3.1. Flexible intentions for dynamic multi-step tasks
Whether an agent maintains a simple continuous generative model or makes use of a discrete model for planning, to operate 
in realistic scenarios it needs to specify beliefs over environmental entities. Maintaining such additional beliefs has been used to 
actively infer object-centric representations [51,52]o r for simulating oculomotor behavior [53]; here, we turn to the problem of 
tackling dynamic tasks that comprise multiple steps or composite actions. If the agentâ€™s goal is to reach a moving object, it must 
maintain an estimate of its hand position as well as of the object position [54]. Designing a dynamics function with an attractor 
toward a static prior conï¬guration is not helpful in dynamic contexts, and directly embedding exteroceptive observations of the 
object (which are low-level representations) in the internal model is not biologically plausible, since it would require to deï¬ne the 
sensory mapping in the model dynamics as well.
Formally, we consider beliefs ğ about diï¬€erent environmental entities operating in the same domain, i.e., ğ=[ğ1, â€¦ , ğğ‘], where 
ğ‘ is the number of entities. When we talk about beliefs, we mean the expected value of hidden or latent states generating observations. 
We assume that these beliefs generate observations ğ’š in parallel, through a likelihood function ğ’ˆ(ğ) with the same factorization:
ğ’š =[ğ’š1 â€¦ ğ’šğ‘
] =[ğ’ˆ1(ğ1)+ ğ’˜1 â€¦ ğ’ˆğ‘(ğğ‘)+ ğ’˜ğ‘
]
where the letter ğ‘¤ indicates a (Gaussian) noise term. Each belief can then be inferred by inverting this mapping:
ğœ•ğœ‡ğ’š =[ğœ•ğœ‡1ğ’ˆ1 â€¦ ğœ•ğœ‡ğ‘ ğ’ˆğ‘
]
and by following the update rules deï¬ned in Equation (3). In this way, the agent can track the trajectory of its body along with the 
trajectories of moving objects or other agents. At this point, the dynamic nature of a task can be tackled through the deï¬nition of 
Heliyon 10 (2024) e39129
7
M. Priorelli and I.P. Stoianov
appropriate intentions, or functions that generate a possible future state by combining the beliefs over some environmental entities 
[54]:
ğâˆ— =ğ’Š(ğ)= ğ‘¾ğ +ğ’ƒ (8)
Here, we speciï¬ed a form that could be realized through a simple combination of neurons (although more advanced behaviors 
can be realized with nonlinear functions). In particular, the matrix ğ‘¾ performs a linear transformation of the beliefs, aï¬€ording 
dynamic behavior since every entity is constantly updated through the observations. On the other hand, the vector ğ’ƒ imposes a static 
conï¬guration, e.g., a ï¬xed position to reach. Note that diï¬€erently from the likelihood function ğ’ˆ(ğ), the function ğ’Š(ğ) manipulates 
and combines all the environmental entities.
To clarify how Equation (8)works, it is useful to model the pick-and-place operation considered before. To focus on the control 
aspects, we assume a simulated agent endowed with the following simpliï¬ed sensory modalities: (i) a proprioceptive observation 
of the armâ€™s joint angles (with dimension ğ‘›ğ‘ =8 ); (ii) a visual observation encoding the positions of hand, object, and goal (with 
dimension ğ‘›ğ‘£ =3); (iii) a tactile observation:
ğ’š ={ğ’šğ‘,ğ’šğ‘£,ğ‘¦ğ‘¡}
For simplicity, we directly provide the 2D Cartesian coordinates of the hand, object, and goal as visual input, i.e., ğ’šğ‘£ =[ğ’šğ‘£,ğ‘, ğ’šğ‘£,ğ‘œ, ğ’šğ‘£,ğ‘”]. 
Tactile observations are deï¬ned through a Boolean function and inform the agent whether all the ï¬ngers â€“ the last four joints â€“ touch 
an external object. Here, we only consider the Cartesian position of the last level (i.e., the hand). However, a more realistic scenario 
should comprise information about all intermediate limb positions, which may be used to infer the correct posture from exteroceptive 
sensations only [55].
Although limb trajectories are generated by intrinsic kinematic information (e.g. joint angles), the movements that the agent wants 
to realize are usually expressed in an extrinsic domain (e.g., 3D visual space). These two continuous modalities are needed in pick-
and-place operations because reaching movements are better described in an extrinsic reference frame, while opening and closing the 
hand are operations better understood from an intrinsic reference frame. We thus consider beliefs in both intrinsic (ğğ‘–) and extrinsic 
(ğğ‘’) domains. It is also convenient to express in these two domains the objects the agent interacts with. In this way, for each object the 
agent can automatically infer a possible joint conï¬guration, which can be further restricted by appropriate priors (e.g., a particular 
grip depending on the objectâ€™s aï¬€ordances). We further assume that the agent maintains a belief over limb lengths ğğ‘™ in order to 
compute the extrinsic Cartesian coordinates of the hand and every object. Here, this belief is kept ï¬xed, although it is possible to 
infer it [55,56]. Finally, we also keep a belief over tactile observations ğœ‡ğ‘¡. Summing up:
ğ={ğğ‘–,ğğ‘’,ğğ‘™,ğœ‡ğ‘¡}
where the ï¬rst two beliefs comprise components about the arm, object, and goal, reï¬‚ecting the decomposition of the visual sensory 
signal:
ğğ‘– =[ğğ‘–,ğ‘ ğğ‘–,ğ‘œ ğğ‘–,ğ‘”
] ğğ‘’ =[ğğ‘’,ğ‘ ğğ‘’,ğ‘œ ğğ‘’,ğ‘”
]
Since joint angles generate Cartesian coordinates with a one-to-one mapping, it is natural to place the intrinsic belief at the top of 
this hierarchy, and deï¬ne the following likelihood function ğ’ˆğ‘’:
ğ’ˆğ‘’(ğğ‘–)= [ğ‘»(ğğ‘–,ğ‘,ğğ‘™) ğ‘»(ğğ‘–,ğ‘œ,ğğ‘™) ğ‘»(ğğ‘–,ğ‘”,ğğ‘™)] +ğ’˜ğ‘’
ğ‘»(ğœ½,ğ’)=
[ ğ‘™1ğ‘1 +ğ‘™2ğ‘12 +ğ‘™3ğ‘123 +(ğ‘™4 +ğ‘™âˆ—)ğ‘1234
ğ‘™1ğ‘ 1 +ğ‘™2ğ‘ 12 +ğ‘™3ğ‘ 123 +(ğ‘™4 +ğ‘™âˆ—)ğ‘ 1234
]
where ğ‘»(ğœ½, ğ’) is the forward kinematics. As a result, the intrinsic belief is found by inferring the most likely kinematic conï¬guration 
that may have generated the extrinsic belief [55]; this naturally performs inverse kinematics without explicitly requiring it in the 
dynamics function of the intrinsic belief. Here, ğ‘™ğ‘› is the length of the ğ‘›th segment, and we used a compact notation to indicate the 
sine and cosine of the sum of angles. Since we are not interested in the positions of the ï¬ngers during the reaching task, the kinematic 
likelihood function only computes the Cartesian position of the hand, which is found by extending the length of the last limb by a 
grasping distance ğ‘™âˆ—. Finally, proprioceptive predictions are generated through a mapping ğ’ˆğ‘ that extracts the armâ€™s joint angles from 
the intrinsic belief, while exteroceptive sensations â€“ position and touch â€“ are generated by an identity function:
ğ’ˆğ‘(ğğ‘–)= ğğ‘–,ğ‘ ğ’ˆğ‘£(ğğ‘’)= ğğ‘’ ğ‘”ğ‘¡(ğœ‡ğ‘¡)= ğœ‡ğ‘¡
The belief dynamics can account for many factors including friction, gravity, etc. â€“ but for simplicity we deï¬ne them as a combi-
nation of the agentâ€™s intentions. For a pick-and-place task, it is useful to specify an intention for each step: (i) reach the object; (ii) 
reach the goal position; (iii) close the ï¬ngers to grasp the object; (iv) open the ï¬ngers to release the object. The ï¬rst two intentions 
(indexed by a second subscript) can be realized in an extrinsic reference, setting the hand position equal to the inferred object/goal 
positions:
ğâˆ—
ğ‘’,ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ =ğ’Šğ‘–,ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğğ‘’)= ğ‘¾ğ‘’,ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ â‹… ğğ‘’ =[ğğ‘’,ğ‘œ ğğ‘’,ğ‘œ ğğ‘’,ğ‘”
]
ğâˆ—
ğ‘’,ğ‘”ğ‘œğ‘ğ‘™ =ğ’Šğ‘–,ğ‘”ğ‘œğ‘ğ‘™(ğğ‘’)= ğ‘¾ğ‘’,ğ‘”ğ‘œğ‘ğ‘™ â‹… ğğ‘’ =[ğğ‘’,ğ‘” ğğ‘’,ğ‘œ ğğ‘’,ğ‘”
]
Heliyon 10 (2024) e39129
8
M. Priorelli and I.P. Stoianov
Fig. 1.Graphical representation of the relationship between environmental beliefs and intentions. Intrinsic and extrinsic beliefs are linked by a function ğ‘» computing 
forward kinematics. Both beliefs have components representing the arm, an object, and a goal position. Orange and green circles represent proprioceptive and visual 
observations, respectively (note that only the arm component generates proprioceptive predictions). Future states are computed through intentions ğ’Šğ‘– and ğ’Šğ‘’ that 
manipulate and combine all the environmental entities from a given domain.
with the help of the following transformations:
ğ‘¾ğ‘’,ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ =
â¡
â¢
â¢â£
ğŸ ğ‘° ğŸ
ğŸ ğ‘° ğŸ
ğŸğŸ ğ‘°
â¤
â¥
â¥â¦
ğ‘¾ğ‘’,ğ‘”ğ‘œğ‘ğ‘™ =
â¡
â¢
â¢â£
ğŸğŸ ğ‘°
ğŸ ğ‘° ğŸ
ğŸğŸ ğ‘°
â¤
â¥
â¥â¦
where the elements ğŸ and ğ‘° are 2x2 zero and identity matrices, corresponding to the x and y coordinates. As explained later, the 
extrinsic belief over the hand, steered toward one of the environmental entities, generates a prediction error that propagates back to 
the intrinsic level, inferring a kinematic conï¬guration that is realized by acting. Intentions with a similar form can also be deï¬ned 
at the intrinsic level, setting the armâ€™s joint angles equal to the inferred object/goal conï¬gurations (recall that the agent constantly 
maintains potential conï¬gurations related to the entities): this results in faster dynamics as the intentions directly operate on the 
intrinsic domain required for movement.
Grasping/placing intentions are accomplished by generating a future intrinsic belief with the ï¬ngers in a ï¬xed closed/open 
conï¬guration:
ğâˆ—
ğ‘–,ğ‘œğ‘ğ‘’ğ‘› =ğ’Šğ‘–,ğ‘œğ‘ğ‘’ğ‘›(ğğ‘–)= ğ‘¾ğ‘–,ğ‘œğ‘ğ‘’ğ‘› â‹… ğğ‘– +ğ’ƒğ‘–,ğ‘œğ‘ğ‘’ğ‘› =
[
ğâˆ—
ğ‘–,ğ‘œğ‘ğ‘’ğ‘›,ğ‘ ğğ‘–,ğ‘œ ğğ‘–,â„
]
ğâˆ—
ğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ =ğ’Šğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’(ğğ‘–)= ğ‘¾ğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ â‹… ğğ‘– +ğ’ƒğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ =
[
ğâˆ—
ğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’,ğ‘ ğğ‘–,ğ‘œ ğğ‘–,â„
]
Here, ğ‘¾ğ‘–,ğ‘œğ‘ğ‘’ğ‘› and ğ‘¾ğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ ensure that the ï¬rst 4 components of the arm (thus, excluding the ï¬ngers) maintain the current conï¬gu-
ration:
ğ‘¾ğ‘–,ğ‘œğ‘ğ‘’ğ‘› =ğ‘¾ğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ =
â¡
â¢
â¢â£
ğ‘¾ğ‘–,ğ‘ ğŸğŸ
ğŸğŸ ğŸ
ğŸğŸ ğŸ
â¤
â¥
â¥â¦
ğ‘¾ğ‘–,ğ‘ =
â¡
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢â£
10000000
01000000
00100000
00010000
00000000
00000000
00000000
00000000
â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥â¦
where the elements ğŸ and ğ‘° are 8x8 zero and identity matrices, corresponding to the armâ€™s joint angles. Instead, ğ’ƒğ‘–,ğ‘œğ‘ğ‘’ğ‘› and ğ’ƒğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’
impose closed/open angles ğœƒğ‘ and ğœƒğ‘œ to the ï¬ngers:
ğ’ƒğ‘–,ğ‘œğ‘ğ‘’ğ‘› =[ğ’ƒğ‘–,ğ‘œğ‘ğ‘’ğ‘›,ğ‘ ğŸğŸ ]
ğ’ƒğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ =[ğ’ƒğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’,ğ‘ ğŸğŸ ]
ğ’ƒğ‘–,ğ‘œğ‘ğ‘’ğ‘›,ğ‘ =[000000âˆ’ ğœƒğ‘ ğœƒğ‘
]
ğ’ƒğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’,ğ‘ =[0000 ğœƒğ‘œ âˆ’ğœƒğ‘œ 00 ]
where ğŸ is a zero vector. In summary, we deï¬ned two sets of competing intentions operating in diï¬€erent domains: reaching an object 
or reaching a goal position (in extrinsic reference frames), and closing or opening the hand (in intrinsic reference frames). Fig. 1
provides a graphical representation of the link between environmental beliefs and ï¬‚exible intentions. In the following, we turn to the 
deï¬nition of the two (hybrid and continuous-only) control methods, showing how these intentions are used in practice. The diï¬€erence 
between the two methods regards where intentions are embedded and how they aï¬€ect the system dynamics.
Heliyon 10 (2024) e39129
9
M. Priorelli and I.P. Stoianov
3.2. Hybrid models and discrete goals
In this section, we delineate how the deï¬ned intentions can be used to simulate a pick-and-place operation with a hybrid model, 
which we then compare to a speciï¬c phase of motor skill learning. Designing a discrete model involves the speciï¬cation of discrete 
states ğ’”, a likelihood matrix ğ‘¨, a transition matrix ğ‘©, and actions ğ’–. In the task considered, the discrete hidden states encode: (i) 
whether the hand and object are in the same position, in diï¬€erent positions, or both in the goal position; and (ii) whether the hand is 
open, closed or has grasped the object. In total, these factors combine in 9 possible process states. There are two likelihood matrices: 
(i) a matrix ğ‘¨ performs a simple identity mapping from the discrete to the continuous model; and (ii) a matrix ğ‘¨ğ‘¡ returns a discrete 
tactile observation ğ’ğ‘¡ â€“ encoded by a Bernoulli distribution â€“ signaling whether or not the object is grasped (hence, the continuous 
observation ğ‘¦ğ‘¡ is not used). The transition matrix ğ‘© is deï¬ned such that the object can be grasped only when both hand and object 
are in the same position. Note that every state has a certain probability â€“ e.g., depending on sampling time window ğ‘‡ â€“ that the 
transition might fail. Finally, the discrete actions ğ’– are equivalent to the intentions deï¬ned before, with the addition of a stay action. 
The trial is successful only if the agent has placed the object in the goal position and the hand is open.
For the discrete and continuous models to communicate, each discrete outcome has to be associated with a reduced continuous 
prior. Since the latter is usually static, to deal with a dynamic environment we let it depend on a combination of the intentions. 
This has the advantage that if the reduced priors are generated at each discrete time step, a correct conï¬guration over the hidden 
states can be imposed even with a moving object â€“ whose position will be dynamically inferred by the continuous model. However, 
note that a richer design would be to link the reduced priors to the hidden causes and encode the intentions in the latter [57]. With 
this method, dynamic inference of discrete variables emerges naturally by generating the reduced priors from diï¬€erent dynamics 
functions maintained by the agent. Crucially, this means that not just the position, but the whole instantaneous trajectory is used for 
inference and planning.
Since the agent maintains intrinsic and extrinsic beliefs, a single discrete outcome ğ‘œğœ,ğ‘š generates two diï¬€erent sets of reduced 
priors, ğœ¼ğ‘–,ğ‘š and ğœ¼ğ‘’,ğ‘š. For example, if the hand is open and in the object position, the reduced priors are:
ğœ¼ğ‘–,0 =ğ’Šğ‘–,ğ‘œğ‘ğ‘’ğ‘›(ğğ‘–) ğœ¼ğ‘’,0 =ğ’Šğ‘’,ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğğ‘’)
Note that some outcomes generate the same reduced priors (e.g., the open-hand condition does not aï¬€ect the extrinsic prior), or do 
not impose any bias over the hidden states (e.g., if the hand and the object are in diï¬€erent positions, the mapping of the extrinsic 
level reduces to an identity and the corresponding reduced prior will be equal to the posterior). Through Bayesian Model Average 
(BMA), the full priors ğœ¼ğ‘– and ğœ¼ğ‘’ are then computed, which act over the continuous models as prior prediction errors ğœºğœ‚,ğ‘– and ğœºğœ‚,ğ‘’:
ğœ¼ğ‘– =
âˆ‘
ğ‘š
ğ‘œğœ,ğ‘š â‹… ğœ¼ğ‘–,ğ‘š
ğœ¼ğ‘’ =
âˆ‘
ğ‘š
ğ‘œğœ,ğ‘š â‹… ğœ¼ğ‘’,ğ‘š
ğœºğœ‚,ğ‘– =ğğ‘– âˆ’ğœ¼ğ‘–
ğœºğœ‚,ğ‘’ =ğğ‘’ âˆ’ğœ¼ğ‘’
Finally, the updates for the hidden states are found by the following update rules:
Ì‡ğğ‘– =âˆ’ğœ•ğğ‘–îˆ² =ğœ•ğ’ˆğ‘‡
ğ‘ ğœ‹ğ‘ğœºğ‘ +ğœ•ğ’ˆğ‘‡
ğ‘’ ğœ‹ğ‘’ğœºğ‘’ âˆ’ğœ‹ğœ‚,ğ‘–ğœºğœ‚,ğ‘–
Ì‡ğğ‘’ =âˆ’ğœ•ğğ‘’îˆ² =ğœ•ğ’ˆğ‘‡
ğ‘£ ğœ‹ğ‘£ğœºğ‘£ âˆ’ğœ‹ğ‘’ğœºğ‘’ âˆ’ğœ‹ğœ‚,ğ‘’ğœºğœ‚,ğ‘’
(9)
where îˆ² is the variational free energy of the continuous models, ğœºğ‘, ğœºğ‘’, ğœºğ‘£ are respectively the proprioceptive, extrinsic, and visual 
prediction errors, and we indicated with the letter ğœ‹ the precisions of the related likelihood functions. Note that both updates have a 
similar form, with the only diï¬€erences lying in the type of sensory observations, and in the direction of the extrinsic prediction error 
(backward vs forward). In particular, the gradient ğœ•ğ’ˆğ‘’ performs kinematic inversion from intrinsic to extrinsic beliefs [55].
On the other side, ascending messages are found by comparing, for a sampling time window ğ‘‡, the models of both intrinsic and 
extrinsic posteriors through the free energies Ìƒîˆ²ğ‘š of the reduced models:
Ìƒîˆ²ğ‘š =âˆ’ln ğ‘œğœ,ğ‘š âˆ’ğ›¿ğ‘–
ğ‘‡âˆ‘
0
îˆ¸ğ‘š,ğ‘–ğ‘‘ğ‘¡ âˆ’ğ›¿ğ‘’
ğ‘‡âˆ‘
0
îˆ¸ğ‘š,ğ‘’ğ‘‘ğ‘¡ (10)
where îˆ¸ğ‘š,ğ‘– and îˆ¸ğ‘š,ğ‘’ are the log-evidences of intrinsic and extrinsic beliefs â€“ computed via Equation (6)â€“ respectively weighted by 
gains ğ›¿ğ‘– and ğ›¿ğ‘’. The latter quantities modulate the contributions of diï¬€erent domains. Here, they are kept ï¬xed throughout the tasks 
but it is assumed that higher values result in faster reaction times and evidence accumulation during perceptual decision-making 
[58]. Note that a long enough sampling time window ğ‘‡ is needed to accumulate evidence from the lower modalities and plan the 
next movement, as will be clear later. At this point, the discrete hidden states are inferred by combining the likelihoods of both the 
ascending message ğ’“ğœ and the tactile observation ğ’ğ‘¡:
ğ’”ğœ‹,ğœ =ğœ(lnğ‘©ğœ‹,ğœâˆ’1ğ’”ğœâˆ’1 +ln ğ‘¨ğ‘‡ ğ’“ğœ +ln ğ‘¨ğ‘‡
ğ‘¡ ğ’ğ‘¡)
similar to Equation (7). In summary, the discrete model ï¬rst computes policy probabilities depending on some prior preferences, 
and combines them to obtain a discrete outcome. The latter is used to compute â€“ through ï¬‚exible intentions â€“ the reduced priors, 
which are further weighted to get a ï¬nal conï¬guration that the continuous models have to realize. This conï¬guration acts as a prior 
over the 0th-order (i.e., position) hidden states, and the belief update breaks down to the computation of sensory gradients and 
Heliyon 10 (2024) e39129
10
M. Priorelli and I.P. Stoianov
Fig. 2.Graphical representation of the ï¬rst cognitive phase. In the ï¬rst trials of a novel task, the state transitions generally encode elementary actions, such as reaching, 
grasping, and releasing. The cognitive eï¬€ort needed by high-level processes to learn the correct sequence of elementary actions translates to the task dynamics being 
only visible in the discrete model, while the continuous levels only receive a prior representation (e.g., target states) and their job is mostly to realize the corresponding 
trajectories. This is in line with the hypothesis that in the ï¬rst phase of motor learning a high cortical activity is recorded, especially in anterior (e.g., prefrontal) 
regions.
prediction errors. Note that the extrinsic belief ğğ‘’ can be biased through two diï¬€erent pathways: directly from the reduced prior of 
the discrete model, or indirectly through forward kinematics of the intrinsic belief ğğ‘–. Correspondingly, the latter can be changed 
either by imposing a particular conï¬guration in the intrinsic domain through the discrete model, or by the backward pathway of the 
extrinsic belief, through computation of the gradient ğœ•ğ’ˆğ‘’ [55].
What are the implications from a biological perspective? It has been hypothesized that motor skill learning is roughly composed 
of three diï¬€erent phases [59]: a ï¬rst cognitive phase where movements are ineï¬ƒcient and one has to consciously explore diï¬€erent 
strategies to achieve a goal; an associative stage where the learner becomes more proï¬cient and transitions between movements are 
more ï¬‚uid; a ï¬nal autonomous phase where the learner has reached almost complete autonomy from conscious task-speciï¬c thinking 
and can move without cognitive eï¬€ort. Although the reality is likely to be far more complex than this simple decomposition, with 
diï¬€erent degrees of proï¬ciency corresponding to speciï¬c neural processing, it may be useful for analyzing the key principles that 
underlie the change in neural activity about task specialization. In this view, active inference provides interesting insights about this 
process because: (i) it builds upon an internal generative model of the task dynamics, which can be constantly reï¬ned by experience; 
(ii) it assumes that information processing starts and unfolds through the same principle of prediction error minimization, either with 
continuous or discrete representations. Under this perspective, the hybrid model described can be compared with the ï¬rst cognitive 
phase â€“ shown in Fig.2â€“ wherein the agent has not yet developed an appropriate automatism for pick-and-place operations, and has 
to constantly plan the next action to take based on the new evidence observed. The resulting goal-directed behavior is composed of 
simple action primitives (reaching, picking, or placing), which is ineï¬ƒcient and requires non-negligible energy demands.
3.3. Continuous models and ï¬‚exible intentions
Whether or not a discrete model is necessary depends on the nature of the task, even when considering actions composed of 
diï¬€erent steps. In some cases, an a-priori-deï¬ned sequence of movements (i.e., a habitual behavior) is all that is needed to solve a 
particular task, e.g., in rhythmic movements or, as in this case, a simple pick-and-place operation. Such scenarios may present little 
uncertainty about the order of elementary actions that does not necessarily involve repeated online decision-making.
But how can transitions between continuous trajectories be encoded in practice? To elucidate, we reveal a parallelism between 
high-level and low-level processes, shown in Fig.3a. In the discrete model, hidden states are conditioned on a policy that the agent 
can follow in a speciï¬c instant, and the overall state ğ’”ğœ is found by weighting the expected states from all policies. The same technique 
Heliyon 10 (2024) e39129
11
M. Priorelli and I.P. Stoianov
Fig. 3.(a) Bayesian model average in discrete, hybrid, and continuous models. In discrete models, future states ğ’”ğœ‹,ğœ are weighted by the probability of each policy ğœ‹ğœ‹, 
generating a combined discrete state ğ’”ğœ. In hybrid models, continuous (reduced) priors ğœ¼ğ‘š are weighted by the probability of each discrete outcome modality ğ‘œğœ,ğ‘š, 
generating a combined (full) continuous prior ğœ¼, which acts as a bias for the continuous model. In continuous models, future dynamic trajectories Ìƒğ are weighted by 
the hidden causes ğ’— and dynamics precisions ğœ‹ğ‘¥, generating a combined trajectory realized through movement. (b) Graphical representation of the third autonomous 
phase. The process of task specialization is complete. The discrete model â€“ not shown here â€“ has learned a representation abstract enough to include all the four main 
movements of the pick-and-place operation in a combined trajectory, and its role is only to command its execution and accumulate evidence from both modalities to 
infer when it has been completed.
is applied to hybrid models, where the probability of every possible discrete outcome is combined with a continuous state to compute 
the average signal that will bias the lower levels. Along the same line, we propose that composite continuous dynamics can be 
generated by weighting the results from independent contributions, each encoding a simple trajectory. Speciï¬cally, we let the agentâ€™s 
intentions depend on hidden causes ğ’—, i.e., with beliefs ğ‚ğ‘– =[ğœˆğ‘–,1, â€¦ , ğœˆğ‘–,ğ½] and ğ‚ğ‘’ =[ğœˆğ‘’,1, â€¦ , ğœˆğ‘’,ğ¾], where ğ½ and ğ¾ are the number of 
intrinsic and extrinsic intentions. Next, we factorize the prior distribution over the generalized beliefs Ìƒğğ‘– and Ìƒğğ‘’ (in this case, up to 
the 1st order) into diï¬€erent probability distributions for each hidden cause:
ğ‘(Ìƒğğ‘–)= ğ‘(ğğ‘–)
ğ½âˆ
ğ‘—
ğ‘(ğâ€²
ğ‘–,ğ‘—|ğğ‘–) ğ‘(Ìƒğğ‘’)= ğ‘(ğğ‘’)
ğ¾âˆ
ğ‘˜
ğ‘(ğâ€²
ğ‘’,ğ‘˜|ğğ‘’)
where:
ğ‘(ğâ€²
ğ‘–,ğ‘—|ğğ‘–)= îˆº(ğâ€²
ğ‘–,ğ‘—|ğ’‡ğ‘–,ğ‘—(ğğ‘–,ğœˆğ‘–,ğ‘—),ğœ‹âˆ’1
ğ‘¥,ğ‘–,ğ‘—) ğ‘(ğâ€²
ğ‘’,ğ‘˜|ğğ‘’)= îˆº(ğâ€²
ğ‘’,ğ‘˜|ğ’‡ğ‘’,ğ‘˜(ğğ‘’,ğœˆğ‘’,ğ‘—),ğœ‹âˆ’1
ğ‘¥,ğ‘’,ğ‘˜) (11)
and ğœ‹ğ‘¥,ğ‘–,ğ‘— and ğœ‹ğ‘¥,ğ‘’,ğ‘˜ are the corresponding precisions. Each function acts as an attractive force proportional to the error between the 
current belief and desired states, the latter computed with the intentions deï¬ned before:
ğ’‡ğ‘–,ğ‘—(ğğ‘–,ğœˆğ‘–,ğ‘—)= ğœˆğ‘–,ğ‘—ğ’†ğ‘–,ğ‘—
ğ’‡ğ‘’,ğ‘˜(ğğ‘’,ğœˆğ‘’,ğ‘—)= ğœˆğ‘’,ğ‘—ğ’†ğ‘’,ğ‘˜
ğ’†ğ‘–,ğ‘— =ğ’Šğ‘–,ğ‘—(ğğ‘–)âˆ’ ğğ‘–
ğ’†ğ‘’,ğ‘˜ =ğ’Šğ‘’,ğ‘˜(ğğ‘’)âˆ’ ğğ‘’
Heliyon 10 (2024) e39129
12
M. Priorelli and I.P. Stoianov
Fig. 4.(a) Sequence of time frames of the grasping task with static object and goal. Real and estimated object positions are represented by red and purple circles, 
real and estimated goal positions by gray and dark gray squares, and real and estimated limb positions in blue and cyan. Hand, object, and goal belief trajectories 
are represented respectively with a blue, red, and graydotted line. The object position is rapidly inferred while the hand reaches it. As soon as the object is grasped, 
the conditions over the hidden causes force the agent to switch dynamics; as a result, the object belief is pulled toward the goal position while the agent continues to 
track it. When the object is in the goal position, another transition between dynamics forces the agent to place the object. (b) Sequence of time frames of the grasping 
task with aï¬€ordances and a moving object. The arm starts with the ï¬ngers closed and the object belief is initialized to the hand position. It starts tracking the object 
while slowly opening the hand and preparing a counterclockwise wrist rotation. Being subject to multi-domain constraints, this conï¬guration is maintained until the 
moving object is grasped. When this happens, the agent rotates the wrist by 180 degrees clockwise and correctly places the object in the goal position.
Note that the hidden causes can either act as attractor gains, or specify the relative strength of one dynamics over the other [54,60]; 
as a result, modulation of the hidden causes achieves a multi-step behavior. In fact, since the belief dynamics already store and embed 
every future goal, the belief follows the contribution of all active dynamics. In particular, we want the following behavior: (i) until 
the hand has not reached the object, execute only ğ’Šğ‘’,ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ (i.e., reach the object) and ğ’Šğ‘–,ğ‘œğ‘ğ‘’ğ‘› (i.e., open the hand); (ii) as soon as the 
hand reaches the object, ğ’Šğ‘–,ğ‘ğ‘™ğ‘œğ‘ ğ‘’ should start (i.e., close the hand); (iii) when the agent has grasped the object, the object-reaching 
intention should be replaced by the ğ’Šğ‘’,ğ‘”ğ‘œğ‘ğ‘™ (i.e., move the hand toward the goal position along with the object); (iv) when the agent 
has reached the goal position, execute ğ’Šğ‘–,ğ‘œğ‘ğ‘’ğ‘› again to release the object. Note that the object-reaching intention should be active until 
the agent has correctly grasped the object, since the latter may be moving and the grasping intention fail. The desired behavior can be 
easily implemented by computing and combining Boolean functions (or sigmoid functions to have smooth transitions) of the beliefs. 
Having deï¬ned the dynamics prediction errors for each intention:
ğœºğ‘¥,ğ‘–,ğ‘— =ğâ€²
ğ‘– âˆ’ğ’‡ğ‘–,ğ‘—(ğğ‘–,ğœˆğ‘–,ğ‘—) ğœºğ‘¥,ğ‘’,ğ‘˜ =ğâ€²
ğ‘’ âˆ’ğ’‡ğ‘’,ğ‘˜(ğğ‘’,ğœˆğ‘’,ğ‘˜)
the update rules for the intrinsic and extrinsic beliefs will be a precision-weighted combination of sensory prediction errors and 
multiple attractor dynamics:
Ì‡Ìƒğğ‘– =îˆ°Ìƒğğ‘– âˆ’ğœ•ğœ‡ğ‘–îˆ² =
[ğâ€²
ğ‘– +ğœ•ğ’ˆğ‘‡
ğ‘ ğœ‹ğ‘ğœºğ‘ +ğœ•ğ’ˆğ‘‡
ğ‘’ ğœ‹ğ‘’ğœºğ‘’ +âˆ‘ğœ•ğ’‡ğ‘‡
ğ‘–,ğ‘—ğœ‹ğ‘¥,ğ‘–,ğ‘—ğœºğ‘¥,ğ‘–,ğ‘—
âˆ’âˆ‘ğœ‹ğ‘¥,ğ‘–,ğ‘—ğœºğ‘¥,ğ‘–,ğ‘—
]
Ì‡Ìƒğğ‘’ =îˆ°Ìƒğğ‘’ âˆ’ğœ•ğœ‡ğ‘’îˆ² =
[ğâ€²
ğ‘’ +ğœ•ğ’ˆğ‘‡
ğ‘£ ğœ‹ğ‘£ğœºğ‘£ âˆ’ğœ‹ğ‘’ğœºğ‘’ +âˆ‘ğœ•ğ’‡ğ‘‡
ğ‘’,ğ‘˜ğœ‹ğ‘¥,ğ‘’,ğ‘˜ğœºğ‘¥,ğ‘’,ğ‘˜
âˆ’âˆ‘ğœ‹ğ‘¥,ğ‘’,ğ‘˜ğœºğ‘¥,ğ‘’,ğ‘˜
] (12)
Note here the dynamics prediction errors acting as a forward message to the 1st-order of the beliefs. Finally, we update the beliefs 
over tactile sensations:
Ì‡ğœ‡ğ‘¡ =ğœ•ğ‘”ğ‘‡
ğ‘¡ ğœ‹ğ‘¡ğœ€ğ‘¡
Fig. 3b shows a graphical representation of the continuous-only model, compared to the third phase of motor learning. The 
continuous models now encode the dynamics of the entire task in the most eï¬ƒcient way because the mechanism is fully autonomous 
and does not require the repeated activation of higher levels for planning the next action. Diï¬€erently than the previous model 
(Fig.2), which had to encode every possible elementary movement â€“ either intrinsic or extrinsic â€“ the increased eï¬ƒciency is also the 
consequence of each modality encoding its dynamics independently from the others, translating to grasping (or reaching) movements 
being mapped mostly into the intrinsic (or extrinsic) modality. In fact, the dimensions of hidden causes ğ½ and ğ¾ generally diï¬€er.
A sequence of time frames is shown in Fig.4a for a sample grasping trial with a continuous-only control. Note that the intrinsic 
and extrinsic beliefs comprise estimates of the arm, the object to be grasped, and the goal position. Deï¬ning continuous intentions 
in the intrinsic domain has the advantage that high hierarchical levels can impose priors not only over hand positions, but also over 
speciï¬c conï¬gurations of the arm and ï¬ngers. This is useful when the agent has to grasp an object in a particular manner depending 
on the task considered (e.g., power grip vs precision grip). Fig.4b shows a simple demonstration of this behavior, wherein the agent 
Heliyon 10 (2024) e39129
13
M. Priorelli and I.P. Stoianov
is required to grasp the object with a counterclockwise wrist rotation, and then place it in the goal position with a clockwise rotation. 
Here, there is a tradeoï¬€ between reaching the object and maintaining the imposed constraint on the wrist; this is evident in the 
middle of the movement when the agent waits until the object can be grasped with the correct rotation even if the latter is already 
in its peripersonal space.
3.4. Discrete and continuous processes compared
In summary, goal-directed behavior can be achieved through specular mechanisms operating at diï¬€erent hierarchical levels, as 
shown in Fig. 3a. Comparing the inference of continuous and discrete hidden states, we note a relationship between the policy-
dependent states ğ’”ğœ‹,ğœ computed through the discrete transition matrix ğ‘©, and the trajectories ğâ€²
ğ‘–,ğ‘— and ğâ€²
ğ‘’,ğ‘˜ computed through the 
continuous intentions. While a policy-independent discrete state ğ’”ğœ is found via Bayesian model average of each policy probability 
ğœ‹ğœ‹, in the continuous case the hidden states ğğ‘– and ğğ‘’ are computed by weighting each trajectory with the corresponding hidden 
causes ğœˆğ‘–,ğ‘— and ğœˆğ‘’,ğ‘˜ at each step of the task.
If we consider the hybrid model, we also note a similarity between the probability distributions of the reduced priors ğœ¼ğ‘š of 
Equation (5)a n d the probabilities of Equation (11)t h a t compose the dynamics function. In the former case, each reduced prior is 
averaged through each outcome model probability ğ‘œğœ,ğ‘š. In this case, however, the full prior biases the continuous belief through the 
overall prediction error ğœ€ğœ‚,ğ‘– (or ğœ€ğœ‚,ğ‘’), which already contains the ï¬nal conï¬guration that the continuous model must realize until 
the next discrete step. In the continuous-only model, the intentions are instead used to compute diï¬€erent directions of update that 
independently pull the belief over the continuous hidden states. Also, the prior precision ğœ‹ğœ‚,ğ‘– of Equation (9)a n d the dynamics 
precisions ğœ‹ğ‘¥,ğ‘–,ğ‘— and ğœ‹ğ‘¥,ğ‘’,ğ‘˜ of Equation (12)p l a y the same role of encoding the strength of the intention dynamics â€“ that steers the 
belief toward a desired state â€“ in contrast to the sensory likelihood â€“ that keeps the agent close to what it is currently perceiving. 
Crucially, while the (intrinsic) weighted prediction error of the hybrid model is:
ğœ‹ğœ‚,ğ‘–(ğğ‘– âˆ’
âˆ‘
ğ‘œğœ,ğ‘šğœ¼ğ‘–,ğ‘š)
if the dynamics precisions have the same value ğœ‹ğ‘¥,ğ‘–, the combination of the weighted dynamics prediction error of the continuous-only 
model takes the form:
ğâ€²
ğ‘– +ğœ‹ğ‘¥,ğ‘–(ğğ‘– âˆ’
âˆ‘
ğœˆğ‘–,ğ‘—ğâˆ—
ğ‘–,ğ‘—)
Hence, encoding diï¬€erent precisions for each intention permits an additional modulation. As a result, in the continuous-only model 
the future trajectories are subject to two diï¬€erent processes [57]. A fast process that imposes (and infers) future states based on the 
hidden causes ğ‚ğ‘– and ğ‚ğ‘’; but also a slow process that learns the precisions ğœ‹ğ‘¥,ğ‘–,ğ‘— and ğœ‹ğ‘¥,ğ‘’,ğ‘˜ of every trajectory. This result lends itself to 
an intuitive interpretation: in active inference, a low precision in a speciï¬c sensory modality implies that it cannot be trusted and the 
agent should update its internal state relying on other signals. Correspondingly, a trajectory with high precision is a good option for 
minimizing prediction errors. In other words, the agent is more conï¬dent in using an intention either for solving the task considered 
or for explaining the current context â€“ given the reciprocal interactions between action and perception.
In order to evaluate the capacity of the continuous and hybrid models to solve a grasping task in static and dynamic conditions, 
we ï¬rst run an experiment with 9 target velocities ranging from 0 to 80 pixels per time step, 500 grasping trials per condition (i.e., 
4500 trials in total per model). The sampling time window of the hybrid model â€“ the period ğ‘‡ in Equation (10)â€“  was ï¬xed at 50 
time steps. The positions of the target and the goal, as well as the direction of the target, were randomly sampled at the beginning 
of every trial. Also, the target dimension was randomly chosen at each trial, within a deï¬ned range. For each condition, accuracy 
was computed as the average number of successful trials in which the agent successfully picked the object and placed it in the goal 
position before the deadline of 6000 time steps. The simulation results are displayed in Fig.5a.
In static grasping (i.e., zero object velocity), the continuous-only model achieves better performances compared to the hybrid 
model, with a diï¬€erence in accuracy of 18.4%. This diï¬€erence increases considerably when it comes to grasping a moving object, up 
to 33.8% for the highest target velocity. A similar diï¬€erence can also be seen when considering the time needed to complete the task 
(the right plot of Fig.5a). We investigated the performance drop of the hybrid model in a second experiment illustrated in Fig.5b, 
showing a tradeoï¬€ between response time and computational complexity. In this experiment, we assessed the performances of the 
hybrid model by varying the sampling time window ğ‘‡ from 15 to 200 time steps, with 500 trials per condition (i.e., 5000 trials in 
total). Here, we analyzed the agentâ€™s accuracy and total simulation time, measured in seconds.
Greater sampling time allows the agent to accumulate more information and accurately estimate the discrete hidden states. 
However, longer sampling time also implies chunking the action into a smaller number of longer action primitives, which ultimately 
makes the grasping action fail since the object continuously changes its location during the sampling period. On the contrary, reducing 
the sampling time has the advantage of speeding up the agentâ€™s planning process (i.e., faster reaction to environmental changes) so 
that it can grasp the object with greater accuracy, but only up to a certain point â€“ after which there is a considerable performance 
drop. In this case, the agent would require longer policies since a reaching movement planned by the discrete model breaks down 
to a greater number of continuous trajectories. At the same time, since the discrete model is activated at a higher rate for action 
replanning, the simulation time steadily increases too, which can be associated with higher energy demands.
Figs.6a and 6bshow the interplay between reaching and grasping actions in the hybrid and continuous models, using a constant 
sampling time window of 50 steps, and target velocity of 30 px/t. For both models, we initialized the target position and direction 
with the same values, as well as for the goal position. Here, ï¬ve diï¬€erent phases can be distinguished: a pure reaching movement, an 
Heliyon 10 (2024) e39129
14
M. Priorelli and I.P. Stoianov
Fig. 5.(a) Performance comparison between the hybrid and continuous-only models for diï¬€erent object velocities (left), and time needed to complete the task (right). 
Accuracy measures the number of successful trials. The right plot also shows the 95% conï¬dence interval of every trial. As the objectâ€™s velocity increases, the grasping 
action becomes more diï¬ƒcult for the hybrid model since faster response planning is needed. Instead, the continuous-only agent adjusts its hidden causes based on 
the evidence accumulated at each time step. Despite using an a-priori (assumed to be learned) sequence of actions that does not permit replanning, this semi-ï¬‚exible 
dynamics handles dynamic tasks more smoothly and eï¬ƒciently. (b) Behavioral eï¬€ects of the sampling time in the hybrid model with an object moving at a moderate 
velocity of 50 px/t. Accuracy (left) and average simulation time (right) are shown. Every condition is averaged over 500 trials. The simulation time is measured in 
seconds. The right plot also displays the 95% conï¬dence interval of the time needed to complete the task.
intermediate phase when the agent slowly approaches the object and prepares the grasping action, a grasping phase, another reaching 
movement and, ï¬nally, the object release. The computation of the action probabilities by the discrete model thus realizes a smooth 
transition between reaching and grasping actions, although at this stage still encoded as separate representations.
This ï¬nding might provide a clue on how discrete actions lead to the emergence of smoother composite trajectories in the con-
tinuous domain, typical of intermediate phases of motor skill learning. In the second (associative) phase, the cortical activity begins 
to shift toward posterior areas. The process can be modeled as in Fig. 6c, with a balanced work between discrete and continuous 
models in which the latter start to construct their own task dynamics, eventually leading to the model of Fig.3b. Note the diï¬€erences 
with Fig.2, wherein the continuous models only receive a static prior resulting from a combination of all discrete states. In summary, 
while in the cognitive phase the task would be comprised of a reaching movement, an eï¬€ort to ï¬nd the next optimal action, and then 
a pure grasping action, in the associative phase the continuous models would handle the transition generating a ï¬‚uid movement that 
makes the hand close as soon as it approaches the object. The discrete model could hence represent the task with a reaching-grasping 
action and a reaching-releasing action, with reduced computational demand.
4. Discussion
Understanding the mechanisms that support task specialization and motor skill learning is compelling for making advances with 
robots and intelligent systems. A major issue is that high-level and low-level processes are often analyzed and developed with dif-
ferent perspectives and techniques, as in optimal control algorithms [61]or deep neural networks [62]. Instead, the ï¬‚exibility and 
robustness behind human and animal motor systems reside in that the learning of a new skill is a uniï¬ed process that initially involves 
prevalent activation of prefrontal brain areas, and gradually shifts toward posterior and subcortical regions [12]. Indeed, constantly 
relying on the whole brain hierarchy is computationally demanding, and high-level areas are unable to track environmental changes 
rapidly. Highly dynamic tasks can be solved more eï¬ƒciently by oï¬„oading the learned action transitions and policies to lower, faster 
hierarchical levels operating in continuous domains [63,12]. For example, evidence suggests that repetitive and rhythmic movements 
Heliyon 10 (2024) e39129
15
M. Priorelli and I.P. Stoianov
Fig. 6.(a) Distance between hand and target (top) and action probabilities (bottom) for the hybrid model. (b) Distance between hand and target (top) and hidden causes 
(bottom) for the continuous model. (c) Graphical representation of the second associative phase. Continuous dynamics are represented in generalized coordinates of 
motion including position, velocity, acceleration, etc., allowing one to express with increasing ï¬delity the true environmental dynamics. Further, the outcomes of 
the discrete model not only bias the 0th temporal order (i.e., position) state, but steer the whole continuous trajectory toward a particular direction. In our reach-to-
grasp task, this might correspond to considering the elementary actions of reaching and grasping as single discrete states that generate composite trajectories in the 
continuous domain.
do not involve activation of prefrontal areas but only rely on sensorimotor circuits [64]. Among those, the Posterior Parietal Cortex 
(PPC) is known for its involvement in reaching and grasping movements [65,66]a n d encoding multiple goals in parallel during 
sequences of actions as in multi-step reaching [67], even when there is a considerable delay between goal states [68].
In this study, we show how the computations performed by hybrid and continuous models in active inference can be compared 
to diï¬€erent phases of motor skill learning. When the agent is required to interact with a novel situation, high-level planning is 
essential because the low levels alone cannot minimize the generated prediction errors, which then climb up the cortical hierarchy. 
As the agent practices the task and learns transitions that account for environmental uncertainty and possible dynamic elements, the 
Heliyon 10 (2024) e39129
16
M. Priorelli and I.P. Stoianov
prediction errors arising during the unfolding of the task can be explained away more eï¬ƒciently by lower-level specialized neurons; as 
a result, repeated calling of high levels becomes redundant and ceases. Even using elementary discrete actions, a composite movement 
corresponding to an approaching phase between reaching and grasping naturally arises from Bayesian model reduction. This is due 
to the continuous evidence accumulation providing a smooth transition between discrete hidden states, as shown in Figs. 6a and 
6b. Therefore, we propose that the dynamics of a continuous model may have a specular structure to their discrete counterpart, 
that is, the ï¬nal trajectory is generated by weighting independent distributions related to some action primitives. As a result, the 
composite movement might be embedded into the continuous dynamics with two parallel processes: while a discrete model can 
rapidly impose and infer speciï¬c trajectories, the dynamics precisions are additionally adapted through the same mechanisms of free 
energy minimization, so that the agent can score how well the action primitives perform. Importantly, the dynamics precisions act not 
just as modulatory signals but represent the agentâ€™s conï¬dence about the current state of the task â€“ in a specular manner to sensory 
precisions [57]. The higher the precision of a speciï¬c trajectory, the more useful it is in explaining the agentâ€™s ï¬nal goal; on the other 
hand, a low trajectory precision means that the agent is not conï¬dent about it for a particular context. Nonetheless, this process does 
not mean that action primitives cannot be used anymore, but only that task-speciï¬c structures are constructed at low levels for the 
particular task considered. In fact, action primitives are maintained and can be used to create new combined trajectories to be applied 
to similar tasks.
Further, the results in Fig. 5b show an interesting tradeoï¬€ between reaction times, planning capabilities, and computational 
demand. If the environment changes with high frequency, occasional calling of the discrete model does not permit to respond in 
time to novel sensory observations. On the other hand, periodic action replanning is counterproductive beyond a certain limit since, 
although allowing the agent to react more rapidly to environmental changes, it increases the eï¬€ort of the discrete model to ï¬nd the 
correct policy, as a higher number of discrete steps are needed. Instead, a continuous-only model achieves the best performances in 
less time and with minimal resource expenditure. However, this comes at the cost of being unable to adapt to changes in the hidden 
causes of reï¬‚exive control as inference when the context changes, resulting in an inability to plan in a context-sensitive manner.
The presented work introduces two novelties compared to state-of-the-art hybrid models. First, the reduced priors are generated 
dynamically at the beginning of each discrete step through the speciï¬cation of ï¬‚exible intentions â€“ which roughly correspond to the 
discrete actions â€“ enabling the agent to operate in dynamic environments [57]. Second, the discrete model can impose priors and 
accumulate evidence over multiple continuous modalities, e.g., intrinsic and extrinsic. This has the advantage that one can realize 
more complex goals (e.g., reaching with speciï¬c aï¬€ordances, as shown in Fig.4b) and has access to more information for inferring 
the discrete hidden states (e.g., extrinsic for position, and intrinsic for hand status).
Finally, the analogies we presented between discrete, mixed, and continuous models may shed light on the information processing 
within cortical regions. It has been hypothesized that the cortex works within a system of discrete models of increasing hierarchi-
cal depth (e.g., motor, associative, limbic) [11], and various studies that recorded neural populations during generation of smooth 
movements seem to point at this direction [69,70,65]. Instead, the interface with the continuous models that receive sensory obser-
vations is supposed to be achieved through subcortical structures, such as the superior colliculus [35]o r the thalamus [34]. Since 
deeper cortical levels are found to encode increasingly discretized representations [18], the latter could also be the consequence of 
the invariability of neural activity that results during inference when the hierarchy is spatially deep. Cortical computations have been 
simulated by continuous-time active inference [71,72]too, but as well noted in [18], even the ï¬rst computations of neural processing 
could be viewed as dealing with lots of little categories. In this view, whether it makes more sense to consider a speciï¬c level as 
discrete or continuous, from a high-level perspective a gradual transition between the two modalities could occur. Therefore, since 
all regions eventually have to deal with continuous signals, looking for analogies between the two processes might be helpful (e.g., 
the policy optimization of discrete models is generally compared to corticostriatal pathways [34], but reward-related activity has also 
been recorded in the Primary Visual Cortex V1 [73]).
Overall, our results provide some hints into how task specialization may occur in biologically plausible ways. However, in this 
preliminary study, the diï¬€erent phases were analyzed separately to show the relationships between policies and dynamic trajectories. 
Further studies are needed to understand how the generation of prediction errors throughout the hierarchy leads to the adaptation 
from discrete to continuous processes. Two promising directions would be to implement deep hierarchical models [55], and to hidden 
causes and dynamics precisions of the continuous models â€“ which in our simulations were kept ï¬xed throughout the task â€“ adapt 
to the prediction errors, thus allowing self-modeling based on the current task and environmental state. Finally, future works will 
assess the capabilities of the hybrid active inference model to simulate human movements. For instance, a more realistic model would 
include a belief of real object dynamics, along with the dynamics desired by the agent. This would possibly account for anticipatory 
eï¬€ects, which are required when modeling experiments that consist of catching moving objects on the ï¬‚y, such as when baseball 
players try to intercept a thrown ball [74]. The proposed model may then be assessed against kinematic and neural data collected 
from humans and primates.
CRediT authorship contribution statement
Matteo Priorelli: Writing â€“ review & editing, Writing â€“ original draft, Visualization, Validation, Software, Methodology, Formal 
analysis, Data curation, Conceptualization. Ivilin Peev Stoianov: Writing â€“ review & editing, Supervision, Funding acquisition, 
Conceptualization.
Heliyon 10 (2024) e39129
17
M. Priorelli and I.P. Stoianov
Declaration of competing interest
The authors declare the following ï¬nancial interests/personal relationships which may be considered as potential competing 
interests: Ivilin Peev Stoianov reports ï¬nancial support was provided by EU Framework Programme for Research and Innovation 
Future and Emerging Technologies under Grant Agreement 951910. If there are other authors, they declare that they have no known 
competing ï¬nancial interests or personal relationships that could have appeared to inï¬‚uence the work reported in this paper.
Acknowledgements
This research received funding from the European Unionâ€™s Horizon H2020-EIC-FETPROACT-2019 Programme for Research and 
Innovation under Grant Agreement 951910 â€œMAIAâ€ to I.S. and the Italian National Recovery and Resilience Plan (NRRP) Project 
PE0000006, CUP J33C22002970002 â€œMNESYSâ€ and Project PE0000013, CUP B53C22003630006, â€œFAIRâ€. The funders had no role 
in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
Data availability
Code and data have been deposited in GitHub (https://github .com /priorelli /discrete -continuous).
References
[1] Thomas Parr, Rajeev Vijay Rikhye, Michael M. Halassa, Karl J. Friston, Prefrontal computation as active inference, Cereb. Cortex 30(2) (2020) 682â€“695.
[2] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, Howard Bowman, Deep temporal models and active inference, Neurosci. Biobehav. Rev. 77 (2017) 
388â€“402, (November 2016).
[3] Thomas Brashers-Krug, Reza Shadmehr, Emilio Bizzi, Consolidation in human motor memory, Nature 382(5505) (1996) 252â€“255.
[4] Daniel E. Callan, Eiichi Naito, from Expert and Novice Athletes 27(4) (2014) 183â€“188.
[5] Francesco Di Russo, Sabrina Pitzalis, Teresa Aprile, Donatella Spinelli, Eï¬€ect of practice on brain activity: an investigation in top-level riï¬‚e shooters, Med. Sci. 
Sports Exerc. 37(9) (2005) 1586â€“1593.
[6] F. Fattapposta, G. Amabile, M.V. Cordischi, D. Di Venanzio, A. Foti, F. Pierelli, C. Dâ€™Alessio, F. Pigozzi, A. Parisi, C. Morrocutti, Long-term practice eï¬€ects on a 
new skilled motor learning: an electrophysiological study, Electroencephalogr. Clin. Neurophysiol. 99(6) (1996) 495â€“507.
[7] Lucio Marinelli, Angelo Quartarone, Mark Hallett, Giuseppe Frazzitta, Maria Felice Ghilardi, The many facets of motor learning and their relevance for Parkinsonâ€™s 
disease, Clin. Neurophysiol. 128(7) (2017) 1127â€“1141.
[8] Elisenda BueichekÃº, Anna MirÃ³-Padilla, MarÃ­a Ãngeles Palomar-GarcÃ­a, Noelia Ventura-Campos, MarÃ­a Antonia Parcet, Alfonso BarrÃ³s-Loscertales, CÃ©sar Ãvila, 
Reduced posterior parietal cortex activation after training on a visual search task, NeuroImage 135 (2016) 204â€“213.
[9] J.R. Wolpaw, Spinal cord plasticity in acquisition and maintenance of motor skills, Acta Physiol. 189(2) (2007) 155â€“169.
[10] Alice Nieuwboer, Lynn Rochester, Liesbeth MÃ¼ncks, Stephan P. Swinnen, Motor learning in Parkinsonâ€™s disease: limitations and potential for rehabilitation, 
Parkinsonism Relat. Disord. 15 3 (2009) 53â€“58.
[11] Giovanni Pezzulo, Francesco Rigoli, Karl J. Friston, Hierarchical active inference: a theory of motivated control, Trends Cogn. Sci. 22(4) (2018) 294â€“306.
[12] Ann M. Graybiel, Habits, rituals, and the evaluative brain, Annu. Rev. Neurosci. 31 (2008) 359â€“387.
[13] R.J. Nudo, G.W. Milliken, W.M. Jenkins, M.M. Merzenich, Use-dependent alterations of movement representations in primary motor cortex of adult squirrel 
monkeys, J. Neurosci. 16(2) (1996) 785â€“807.
[14] Reza Shadmehr, Henry H. Holcomb, Neural correlates of motor memory consolidation, Science 277(5327) (1997) 821â€“825.
[15] D.M. Wolpert, Z. Ghahramani, M. Jordan, An internal model for sensorimotor integration, Wolpert et al Science 269(5) (1995) 1880â€“1882, (1995).pdf.
[16] Karl Friston, Stefan Kiebel, Predictive coding under the free-energy principle, Philos. Trans. -R. Soc. B, Biol. Sci. 364(1521) (2009) 1211â€“1221.
[17] Karl J. Friston, Jean Daunizeau, James Kilner, Stefan J. Kiebel, Action and behavior: a free-energy formulation, Biol. Cybern. 102(3) (2010) 227â€“260.
[18] Thomas Parr, Giovanni Pezzulo, Karl J. Friston, Active Inference: the Free Energy Principle in Mind, Brain, and Behavior, MIT Press, Cambridge, MA, 2021.
[19] Rajesh P.N. Rao, Dana H. Ballard, Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-ï¬eld eï¬€ects, Nat. Neurosci. 
2(1) (1999) 79â€“87.
[20] Karl Friston, The free-energy principle: a uniï¬ed brain theory?, Nat. Rev. Neurosci. 11(2) (2010) 127â€“138.
[21] Matteo Priorelli, Federico Maggiore, Antonella Maselli, Francesco Donnarumma, Domenico Maisto, Francesco Mannella, Ivilin Peev Stoianov, Giovanni Pezzulo, 
Modeling motor control in continuous-time active inference: a survey, Trans. Cogn. Dev. Syst. (2023) 1â€“15.
[22] Rick A. Adams, Stewart Shipp, Karl J. Friston, Predictions not commands: active inference in the motor system, Brain Struct. Funct. 218(3) (2013) 611â€“643.
[23] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, Karl Friston, Active inference on discrete state-spaces: a synthesis, J. Math. 
Psychol. 99 (2020).
[24] Ryan Smith, Karl J. Friston, Christopher J. Whyte, A step-by-step tutorial on active inference and its application to empirical data, J. Math. Psychol. 107 (2022) 
102632.
[25] Raphael Kaplan, Karl J. Friston, Planning and navigation as active inference, Biol. Cybern. 112(4) (2018) 323â€“343.
[26] Matthew Botvinick, Marc Toussaint, Planning as inference, Trends Cogn. Sci. 16(10) (2012) 485â€“488.
[27] Marc Toussaint, Probabilistic inference as a model of planned behavior, KÃ¼nstl. Intell. 3(09) (2009) 23â€“29.
[28] Sherin Grimbergen, S.S. Grimbergen, Â· C. Van Hoof, Â· P. Mohajerin Esfahani, Â· M. Wisse, Act. Inference State Mod. Tutor. (2019).
[29] Noor Sajid, Philip J. Ball, Thomas Parr, Karl J. Friston, Active inference: demystiï¬ed and compared, Neural Comput. 33(3) (2021) 674â€“712.
[30] Philipp Schwartenbeck, Thomas H.B. Fitzgerald, Christoph Mathys, Ray Dolan, Martin Kronbichler, Karl Friston, Evidence for surprise minimization over value 
maximization in choice behavior, Sci. Rep. 5 (2015) 1â€“14.
[31] Philipp Schwartenbeck, Johannes Passecker, Tobias U. Hauser, Thomas H.B. Fitzgerald, Martin Kronbichler, Karl J. Friston, Computational mechanisms of 
curiosity and goal-directed exploration, eLife 8 (2019) 1â€“45.
[32] Karl J. Friston, Jean Daunizeau, Stefan J. Kiebel, Reinforcement learning or active inference?, PLoS ONE 4(7) (2009).
[33] Karl J. Friston, S. Samothrakis, Read Montague, Active inference and agency: optimal control without cost functions, Biol. Cybern. 106 (2012) 523â€“541.
[34] Karl J. Friston, Thomas Parr, Bert de Vries, The graphical brain: belief propagation and active inference 1(4) (2017) 381â€“414.
[35] Thomas Parr, Karl J. Friston, The Discrete and Continuous Brain: from Decisions to Movementâ€”and Back Again. (September):2319â€“2347, 2018.
[36] Karl Friston, Thomas Parr, Peter Zeidman, Bayesian model reduction, 2018, pp.1â€“32.
[37] Karl Friston, Will Penny, Post hoc Bayesian model selection, NeuroImage 56(4) (2011) 2089â€“2099.
Heliyon 10 (2024) e39129
18
M. Priorelli and I.P. Stoianov
[38] T. Parr, K.J. Friston, The computational pharmacology of oculomotion, Psychopharmacology 236(8) (August 2019), (Berl.).
[39] Alexander Tschantz, Laura Barca, Domenico Maisto, Christopher L. Buckley, Anil K. Seth, Giovanni Pezzulo, Simulating homeostatic, allostatic and goal-directed 
forms of interoceptive control using active inference, Biol. Psychol. 169 (2022).
[40] K.J. Friston, N. Sajid, D.R. Quiroga-Martinez, T. Parr, C.J. Price, E. Holmes, Active listening, Hear. Res. 399 (January 2021) 107998.
[41] Manuel Baltieri, Christopher L. Buckley, PID control as a process of active inference with linear generative models, Entropy 21(3) (2019).
[42] Hilbert J. Kappen, VicenÃ§ GÃ³mez, Manfred Opper, Optimal control as a graphical model inference problem, Mach. Learn. 87(2) (February 2012) 159â€“182.
[43] Pablo Lanillos, Marcel van Gerven, Neuroscience-inspired perception-action in robotics: applying active inference for state estimation, control and self-perception, 
CoRR, arXiv :2105 .04261[abs], 2021.
[44] Anatol G. Feldman, Mindy F. Levin, The origin and use of positional frames of reference in motor control, Behav. Brain Sci. 18(4) (1995) 723â€“744.
[45] Anatol G. Feldman, Mindy F. Levin, The Equilibrium-Point Hypothesis â€“ Past, Present and Future, Springer US, 2009, pp.699â€“726.
[46] Hagai Attias, Planning by probabilistic inference, in: Christopher M. Bishop, Brendan J. Frey (Eds.), Proceedings of the Ninth International Workshop on Artiï¬cial 
Intelligence and Statistics, in: Proceedings of Machine Learning Research, vol.R4, Jan 2003, pp.9â€“16, PMLR, 03â€“06, Reissued by PMLR on 01 April 2021.
[47] Andrew W. Corcoran, Giovanni Pezzulo, Jakob Hohwy, From allostatic agents to counterfactual cognisers: active inference, biological regulation, and the origins 
of cognition, Biol. Philos. 35(3) (April 2020).
[48] Karl Friston, JÃ©rÃ©mie Mattout, Nelson Trujillo-Barreto, John Ashburner, Will Penny, Variational free energy and the Laplace approximation, NeuroImage 34(1) 
(2007) 220â€“234.
[49] Bertram F. Malle, Joshua Knobe, The folk concept of intentionality, J. Exp. Soc. Psychol. 33(2) (1997) 101â€“121.
[50] John R. Searle, Intentionality: An Essay in the Philosophy of Mind, Cambridge University Press, 1983.
[51] Stefano Ferraro Toon Van de Maele, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Disentangling shape and pose for object-centric deep active inference models, 
2022.
[52] Toon Van de Maele, Tim Verbelen, Pietro Mazzaglia, Stefano Ferraro, Bart Dhoedt, Object-centric scene representations using active inference, 2023.
[53] Rick A. Adams, Eduardo Aponte, Louise Marshall, Karl J. Friston, Active inference and oculomotor pursuit: the dynamic causal modelling of eye movements, J. 
Neurosci. Methods 242 (2015) 1â€“14.
[54] Matteo Priorelli, Ivilin Peev Stoianov, Flexible intentions: an active inference theory, Front. Comput. Neurosci. 17 (2023) 1â€“41.
[55] Matteo Priorelli, Giovanni Pezzulo, Ivilin Peev Stoianov, Deep kinematic inference aï¬€ords eï¬ƒcient and scalable control of bodily movements, Proc. Natl. Acad. 
Sci. 120 (2023).
[56] Matteo Priorelli, Ivilin Peev Stoianov, Eï¬ƒcient motor learning through action-perception cycles in deep kinematic inference, in: Active Inference, Springer Nature, 
Switzerland, 2024, pp.59â€“70.
[57] M. Priorelli, I.P. Stoianov, Dynamic inference by model reduction, 2023, bioRxiv.
[58] Thomas H.B. FitzGerald, Rosalyn J. Moran, Karl J. Friston, Raymond J. Dolan, Precision and neuronal dynamics in the human posterior parietal cortex during 
evidence accumulation, NeuroImage 107 (2015) 219â€“228.
[59] Janelle Weaver, Motor learning unfolds over diï¬€erent timescales in distinct neural systems, PLoS Biol. 13(12) (2015) 1â€“2.
[60] Matteo Priorelli, Ivilin Peev Stoianov, Intention modulation for multi-step tasks in continuous time active inference, in: Active Inference, Third International 
Workshop, IWAI 2022, Grenoble, France, Sept 19, 2022, 2022.
[61] Emanuel Todorov, Optimality principles in sensorimotor control, Nat. Neurosci. 7 (2004) 907â€“915.
[62] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, 
Thore Graepel, Demis Hassabis, Mastering the game of Go with deep neural networks and tree search, Nature 529(7587) (2016) 484â€“489.
[63] Jakob Hohwy, The Predictive Mind, Oxford University Press, UK, 2013.
[64] Stefan Schaal, Dagmar Sternad, Rieko Osu, Mitsuo Kawato, Rhythmic arm movement is not discrete, Nat. Neurosci. 7(10) (2004) 1137â€“1144.
[65] S. Diomedi, F.E. Vaccari, C. Galletti, K. Hadjidimitrakis, P. Fattori, Motor-like neural dynamics in two parietal areas during arm reaching, Prog. Neurobiol. 205 
(2021) 102116.
[66] Stefano Diomedi, Francesco E. Vaccari, Matteo Filippini, Patrizia Fattori, Claudio Galletti, Mixed selectivity in macaque medial parietal cortex during eye-hand 
reaching, iScience 23(10) (Oct 2020).
[67] Yuhui Li, He Cui, Dorsal parietal area 5 Encodes immediate reach in sequential arm movements, J. Neurosci. 33(36) (2013) 14455â€“14465.
[68] Daniel Baldauf, He Cui, Richard A. Andersen, The posterior parietal cortex encodes in parallel both goals for double-reach sequences, J. Neurosci. 28(40) (2008) 
10081â€“10089.
[69] Kevin A. Mazurek, Adam G. Rouse, Marc H. Schieber, Mirror neuron populations represent sequences of behavioral epochs during both execution and observation, 
J. Neurosci. 38(18) (2018) 4441â€“4455.
[70] Caleb Kemere, Gopal Santhanam, Byron M. Yu, Afsheen Afshar, Stephen I. Ryu, Teresa H. Meng, Krishna V. Shenoy, Detecting neural-state transitions using 
hidden Markov models for motor cortical prostheses, J. Neurophysiol. 100(4) (October 2008) 2441â€“2452.
[71] A. Maselli, P. Lanillos, G. Pezzulo, Active inference uniï¬es intentional and conï¬‚ict-resolution imperatives of motor control, PLoS Comput. Biol. 18(6) (2022).
[72] Francesco Mannella, Federico Maggiore, Manuel Baltieri, Giovanni Pezzulo, Active inference through whiskers, Neural Netw. 144 (2021) 428â€“437.
[73] Marshall G. Shuler, Mark F. Bear, Reward timing in the primary visual cortex, Science (March 2015) 1606â€“1610.
[74] M.M. Hayhoe, N. Mennie, K. Gorgos, J. Semrau, B. Sullivan, The role of prediction in catching balls, J. Vis. 4(8) (August 2004) 156.