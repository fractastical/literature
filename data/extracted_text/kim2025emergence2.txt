Emergence of Goal-Directed Behaviors
via Active Inference with Self-Prior
Dongmin Kim Hoshinori Kanazawa‚àó Yasuo Kuniyoshi‚àó
Graduate School of Information Science and Technology
The University of Tokyo
Tokyo, Japan
{d-kim,kanazawa,kuniyosh}@isi.imi.i.u-tokyo.ac.jp
Naoto Yoshida
Graduate School of Informatics
Kyoto University
Kyoto, Japan
yoshida.naoto.8x@kyoto-u.ac.jp
Abstract
Infants often exhibit goal-directed behaviors, such as reaching for a sensory stimu-
lus, even when no external reward criterion is provided. These intrinsically mo-
tivated behaviors facilitate spontaneous exploration and learning of the body and
environment during early developmental stages. Although computational modeling
can offer insight into the mechanisms underlying such behaviors, many existing
studies on intrinsic motivation focus primarily on how exploration contributes to
acquiring external rewards. In this paper, we propose a novel density model for an
agent‚Äôs own multimodal sensory experiences, called the ‚Äúself-prior,‚Äù and investi-
gate whether it can autonomously induce goal-directed behavior. Integrated within
an active inference framework based on the free energy principle, the self-prior
generates behavioral references purely from an intrinsic process that minimizes mis-
matches between average past sensory experiences and current observations. This
mechanism is also analogous to the acquisition and utilization of a body schema
through continuous interaction with the environment. We examine this approach in
a simulated environment and confirm that the agent spontaneously reaches toward
a tactile stimulus. Our study implements intrinsically motivated behavior shaped
by the agent‚Äôs own sensory experiences, demonstrating the spontaneous emergence
of intentional behavior during early development.
Research Highlights
‚Äì Suggests a computational model for early intentional behavior, integrating body-schema formation
and goal-directed actions under the free energy principle.
‚Äì Introduces a self-prior as an internal density model that drives the emergence of goal-directed
behaviors without external rewards.
‚Äì Demonstrates spontaneous reaching for a sticker by minimizing mismatches between observed
multimodal sensory inputs and the empirically acquired self-prior.
‚àóCo-corresponding authors: Hoshinori Kanazawa, Yasuo Kuniyoshi
Code is available athttps://github.com/kim135797531/self-prior.
Preprint. Under review.
arXiv:2504.11075v2  [cs.AI]  11 Nov 2025
1 Introduction
Infants spontaneously exhibit goal-directed behaviors, such as reaching for a sensory stimulus or
actively exploring their surroundings, even in the absence of external rewards essential for survival.
These behaviors are highly motivated by internal satisfaction and are referred to as intrinsically moti-
vated behaviors (Czikszentmihalyi, 1990; Ryan & Deci, 2000). Such behaviors facilitate unsupervised
learning without explicit external rewards or punishments, which is known to offer various advantages
in early developmental stages (Gopnik, 2009; Kanazawa et al., 2023; White, 1959; Zaadnoordijk
et al., 2022). For example, sensory experiences arising from an infant‚Äôs spontaneous movements, such
as self-touch, not only promote learning about one‚Äôs own body (e.g., acquiring body representations)
(Hoffmann et al., 2017), but also contribute to the formation of an early sense of self (Rochat, 1998).
Traditional observational studies and neuroscience methodologies have been employed to investigate
the mechanisms underlying intrinsically motivated behavioral development (Di Domenico & Ryan,
2017). In particular, computational modeling has also been utilized to quantitatively interpret and
predict behavioral development (Oudeyer & Kaplan, 2009; Shultz, 2013). Experiments using com-
puter simulations allow researchers to freely manipulate and control specific variables to investigate
various scenarios, making them advantageous for uncovering latent effects and interactions that may
be difficult to observe through conventional behavioral studies alone.
Although computational modeling is a useful approach for studying infant behavioral development,
many models proposed in modern robotics and machine learning tend to treat intrinsic motivation
primarily as an exploratory mechanism for improving reward acquisition efficiency in environments
with sparse external rewards (Aubret et al., 2023). Indeed, such approaches can enhance reinforcement
learning agents‚Äô ability to improve explicit task performance, such as maximizing video game scores.
However, they do not sufficiently address how entirely new behaviors emerge when no explicit
external reward objective is set, meaning that no performance criterion or information gain is
provided.
To address this limitation, we propose a novel computational model of intrinsic motivation based on
the expected free energy (Friston, 2010; Friston et al., 2016) (Figure 1). We introduce an internal
density model, termed theself-prior,which enables an agent to learn a statistical representation of
incoming multimodal sensory signals. When a mismatch between the learned probabilistic model and
actual observations is detected, goal-directed behavior emerges without any explicit reward criterion.
Our approach combines several key features that, to our knowledge, have not been integrated in
previous computational models of developmental behavior: (i) the behavioral setpoint is autonomously
formed from the agent‚Äôs own experience rather than externally predefined; (ii) specific goal-directed
behaviors such as reaching emerge independently, rather than serving merely as auxiliary mechanisms
for reward exploration; (iii) behaviors persist even without information gain, as long as there is
a mismatch with familiar states; and (iv) the mechanism is theoretically integrable with existing
extrinsic and intrinsic motivations within the active inference framework. We implement a simulated
agent that perceives a stimulus (such as a sticker) on its arm and spontaneously exhibits reaching
behavior toward it through the self-prior mechanism. In the latter part of this paper, we discuss
how the proposed mechanism may contribute to developmental science, positioning this study as a
computational investigation into the early development of intentional behaviors in infants.
1.1 Free Energy Principle
Thefree energy principleposits that an agent can learn to perceive and act by minimizing the
"surprise" induced by sensory inputs (Friston, 2010). This surprise refers to the prediction error
that arises when the agent encounters unexpected sensory observations, and reducing this surprise
becomes the agent‚Äôs primary objective.
At each time step t, the agent receives information from the world through its sensory organs. We
denote this sensory input by ot, referring to it as the observation. Although ot is all the agent can
directly observe, there may exist underlying factors in the world that generate these observations.
Such factors are called hidden states, denoted by st at time t. For example, given a properly learned
model, an agent can infer from a tactile pattern on its arm (observation ot) that there is a sticker
attached to its arm (hidden states t).
2
(b)
(c)
(a)
Current Future Planning
Acquired
Self-prior
Sticker
&‡∑§ùëù( ) = Low!
Sensory Experiences
Initial
Self-prior
ùëú
‡∑§ùëù(ùëú)
Proprioceptive Tactile
Time
Action
Sensory data ùëú
Expected Free Energy
Figure 1: Emergence of reaching behavior via the self-prior and active inference. (a) When a sticker
is placed on the left arm of the simulated agent, it detects a mismatch with its prior experience of
not having a sticker, and reaches toward the sticker with its right hand to minimize the discrepancy.
(b) Development of the self-prior through experience: as sensory experiences are collected, the
probability distribution over sensory patterns gradually develops. (c) The active inference process in
which the agent plans future actions to minimize expected free energy by aligning sensory inputs
with the learned self-prior. As a result, the agent performs a reaching action toward the sticker. A
full-body infant illustration is used for clarity, the actual experiment was conducted in a pseudo-3D
environment.
Consider an agent that must satisfy certain biological criteria for survival in a stable manner. To do so,
the agent must accurately understand its environment, which amounts to inferring p(ot). According
to the free energy principle, an agent maintains its own probabilistic model of these hidden states
because doing so enables it to approximate p(ot) and thus make inferences about the world. In
practice, we often maximize the log of p(ot) (i.e., logp(o t)) for computational convenience, which is
referred to as maximizing the model evidence. Minimizing the negative term, ‚àílogp(o t), known as
surprise, is therefore equivalent to maximizing this model evidence.
In reality, calculating p(ot) requires marginalization over hidden states (i.e., p(ot, st)), which is
often intractable. The variational free energy principle tackles this by introducing a variational
approximation of Bayesian inference, wherein the agent indirectly reduces surprise by minimizing
the variational free energy F, an upper bound on ‚àílogp(o t). Formally, F is defined as follows (see
(Parr et al., 2022) for details):
‚àílogp(o t)‚â§ F=E q(st)[logq(s t)‚àílogp(o t, st)]
=D KL[q(st)‚à•p(st)]| {z }
complexity
‚àíEq(st)[logp(o t |s t)]| {z }
accuracy
(1)
where DKL(¬∑‚à•¬∑) denotes the Kullback‚ÄìLeibler divergence, and q(¬∑) represents the variationally
approximated distribution. That is, q(st) is the variational distribution that approximates the posterior
distribution over hidden states. Therefore, to reduce surprise at time t, the agent must keep the
posterior and prior distributions close (reducing complexity) while maximizing the likelihood of
observations (improving accuracy).
1.2 Active Inference and Expected Free Energy
While the free energy principle describes how an agent updates its model to reduce surprise based
on current sensory observations, the agent can also directly change its observations through actions
at to make them align better with its internal model. This is the essence ofactive inference, and
more concretely, the approximate posterior q(st) is extended to include actions as q(st, at) =q(at |
st)q(s t).
3
To obtain optimal actions at for both present and future steps, a method has been proposed that
computesexpected free energyby considering the expected free energy at future time points (Friston
et al., 2016). That is, the agent predicts multiple possible future states and selects the trajectory that
is expected to minimize free energy most effectively.
However, since future observations cannot be obtained before the corresponding time actually arrives,
the agent cannot directly compute the free energy of future states. Therefore, the expected free
energy assumes that the agent possesses apreferred prior Àúp(ot) over desired observations, which
acts as a setpoint. As a result, p(ot, st) is extended to include actions and is biased toward preferred
observations, expressed as Àúp(ot, st, at) =p(at)p(s t |o t) Àúp(ot). Thus, the expected free energy G is
defined as follows (see (Mazzaglia et al., 2021; Millidge et al., 2020) for details):
G=E q(ot,st,at)[logq(s t, at)‚àílog Àúp(ot, st, at)]
‚âà ‚àíEq(ot)[log Àúp(ot)]| {z }
extrinsic value
‚àíEq(ot)[DKL[q(st |o t)‚à•q(st)]]| {z }
intrinsic value
‚àíEq(st)[H(q(at |s t))]| {z }
action entropy
(2)
where H denotes the information entropy. That is, to reduce surprise at a future time t, the agent must
(i) increase the probability of preferred observations, (ii) obtain high information gain from those
observations, and (iii) maintain diverse actions. As shown above, expected free energy integrates
both pragmatic (extrinsic) and epistemic (intrinsic) value into a single expression, thereby naturally
resolving the exploration‚Äìexploitation dilemma.
2 Self-Prior: Acquired Preference to Induce Goal-Directed Behavior
Under the expected free energy, agents do not require scalar rewards to be explicitly defined by the
environment. This is because the agent is assumed to possess information about preferred observations
as a preferred prior distribution, which serves as the setpoint for action planning.
However, since the preferred prior is typically given in a fixed form, the removal of external reward
signals does not imply that the agent hasautonomouslygenerated its goal-directed behavior. Therefore,
in this study, we allow the agent to autonomously determine the setpoint for its action planning by
additionally learning a density model, which we refer to as theself-prior.
The self-prior represents "the probability density over the frequency of observed sensations that the
agent has experienced," and is defined as a variable preferred prior that induces behavioral tendencies
for the agent to maintain or re-experience familiar states. Unlike fixed preferred priors, this means
that models with the same structure can develop different priors depending on different experiences,
leading to the emergence of different behaviors. To provide an intuitive understanding of how the
self-prior operates, this paper presents the example of spontaneous reaching toward and removal of
a sticker attached to one‚Äôs own body. This is similar to the experimental setup of (Bigelow, 1986),
where a silent toy was placed on the body of a blind infant to examine reaching behavior. Here, we
interpret the reason why the infant agent removes a sticker attached to its body as being due to a
mismatch between the prior of "oneself without a sticker" and "the currently observed self." Moreover,
this prior has been autonomously and gradually developed by the agent through everyday experiences
(in which no sticker was attached). Since this self-prior does not contain specific instructions to
remove the sticker, it is distinguished from approaches that directly command reaching by providing
fixed setpoints.
The implementation of the self-prior in the active inference framework is derived from a simple
modification to the standard expected free energy formulation, separating the self-prior term from the
original preferred prior term. Specifically, this is achieved by redefining each preferred observation
o in terms of two components: (i) anextrinsicobservation oE that must be maintained to satisfy
survival requirements, and (ii) anintrinsicobservation oI related to the self-prior. Here, oI refers to
bodily states or sensations such as a tactile pattern on the arm, that can vary without threatening the
agent‚Äôs essential functions. Consequently, we decompose the preferred prior Àúp(o)into Àúp(oE, oI), and
in this study, we assume independence so thatÀúp(oE, oI)‚âàÀúp(oE) Àúp(oI):
Preferred prior:Àúp(o) = Àúp(oE, oI)‚âàÀúp(oE) Àúp(oI)| {z }
Self-Prior
(3)
4
In our approach, we choose to define the self-prior over observations o rather than internal states s.
This is because, as mentioned earlier, the self-prior can be derived through simple manipulation of the
standard expected free energy formulation, enabling natural integration of fixed extrinsic preferences
and experience based intrinsic preferences within the same framework. The fact that the self-prior is
related to intrinsic motivation yet can emerge from external sensory data may seem counterintuitive.
This confusion stems from inconsistent use of "external-extrinsic" and "internal-intrinsic" terminology.
To resolve this confusion and rigorously classify intrinsic motivation, we adopt the computational
classification of intrinsic motivation proposed by (Oudeyer & Kaplan, 2009):
‚Ä¢ External vs. Internal: Referring to whether the rewardcomputationoccurs in the environ-
ment or within the agent.
‚Ä¢ Extrinsic vs. Intrinsic: Referring to whether the rewardcriterionis determined externally
or internally.
According to these definitions, any reward provided from the external environment, such as a game
score, qualifies as extrinsic motivation. Meanwhile, internal motivation can be either extrinsic or
intrinsic. An internal motivation is considered intrinsic if the agent sets its reward criterion according
to its internal model (e.g., measuring information gain based on a learned world model). By contrast,
an internal yet extrinsic motivation is one in which the agent checks its internal state and computes a
reward based on an externally fixed criterion (e.g., "maintaining a battery level of 50%").
Therefore, the key to distinguishing extrinsic and intrinsic motivation is not the source of information,
but rather who sets the reward criterion. In this sense, expected free energy is an internal framework
since the agent itself computes both extrinsic and intrinsic components. Furthermore, the self-prior
proposed in this paper qualifies asinternal intrinsic motivationbecause the agent‚Äôs internal model
directly calculates and updates the reward criterion.
Additionally, (Oudeyer & Kaplan, 2009) proposed that motivation can also be distinguished as
homeostatic vs. heterostatic. For example, if there is an intrinsic motivation to pursue new information
acquisition, this is heterostatic, whereas if there is an intrinsic motivation to maintain a familiar state,
this is defined as homeostatic.
In active inference, the preferred prior has traditionally been interpreted as a fixed setpoint representing
observations essential for survival (e.g., blood sugar level, battery level). These setpoints have been
considered extrinsic values since they are externally specified, but we regard this term as strictly
representing a homeostatic value characteristic. Therefore, the decomposition in Equation (3)
separates the homeostatic value term in Equation (2) into extrinsic and intrinsic terms:
‚àíEq(ot)[log Àúp(ot)]| {z }
homeostatic value
=‚àíE q(oE
t )[log Àúp(oE
t )]
| {z }
extrinsic (homeostatic)
‚àíEq(oI
t )[log Àúp(oI
t )]
| {z }
intrinsic (homeostatic)
(4)
Note that unlike Equation (2), where Eq(ot)[log Àúp(ot)] is labeled as extrinsic value, Equation (4)
labels it as homeostatic. Thus, the expected free energy incorporating the self-prior becomes:
G ‚âà ‚àíEq(oE
t )[log Àúp(oE
t )]
| {z }
extrinsic (homeo-, fixed)
‚àíE q(oI
t )[log Àúp(oI
t )]
| {z }
intrinsic (homeo-, familiarity)
‚àíEq(ot)[DKL[q(st|ot)‚à•q(st)]]| {z }
intrinsic (hetero-, novelty)
‚àíH‚Ä≤
(5)
Here,H ‚Ä≤ denotes the entropy of the action distribution.
In summary, the self-prior in this study aims to "maintain or restore a reference pattern" learned
from the agent‚Äôs sensory experiences, thus qualifying asinternal homeostatic intrinsic motivation.
This most closely resembles distributional familiarity motivation (DFM) among the classifications of
intrinsic motivation proposed by (Oudeyer & Kaplan, 2009). Table 1 summarizes the positioning of
the self-prior within this motivational taxonomy.
As an alternative approach to implementing the self-prior concept, there also exist approaches that
construct preferences over internal states. This is the approach proposed by (Sajid et al., 2021), but it
could not explicitly integrate extrinsic and intrinsic motivations. Another approach could consider
hierarchical models wherein internal representations learned at higher levels are transformed into
self-priors for lower levels. We revisit these potential future extensions in the Discussion section.
5
Table 1: Positioning of the self-prior within a taxonomy of motivations
Extrinsic Intrinsic
Homeostatic Heterostatic
External Score in video game (N/A) (N/A)
Internal Gap of sensory data
from fixed value
Gap of sensory data
from self-prior
Information gain
by sensory data
3 Computational Model of Self-Prior
In this section, we introduce the concrete implementation of our model for computational simulations
based on the active inference framework incorporating the self-prior. The overall framework is
constructed using small-sized matrices in the discrete environment to allow direct inspection of model
behavior, and deep neural networks using techniques developed in deep reinforcement learning in the
continuous environment.
We implement this as a general-purpose structure applicable to a wide range of behaviors beyond
specific tasks, using an end-to-end learning model that requires no prior preparation such as filtering
specific parts of sensory data for the self-prior. Specifically, in the discrete environment model,
we learn maximum likelihood estimation via empirical frequency normalization (plug-in MLE),
while in the continuous environment model, we train Normalizing Flow based density estimators via
maximum likelihood (Durkan et al., 2019).
3.1 Model for Discrete Environment
The model for the discrete environment uses categorical distributions as the generative model,
handling discrete random variables:
Prior:P(s t |s t‚àí1, at‚àí1) =Cat(Bat‚àí1 )
Posterior:Q(s t) =Cat(œït)
Likelihood:P(o t |s t) =Cat(A)
(6)
Here, uppercase letters P, Qdenote discrete distributions, and A,B, œït are parameter matrices and
vectors of categorical distributions. B implements the transition model, with separate Ba matrices
for each action a. Using this notation, minimizing variational free energy yields the optimal posterior
distribution parameter œït for the current observation ot through the Softmax normalization function
œÉas follows (derivation process see Appendix A.1.1):
œït ‚âàœÉ(log (A¬∑o t) + log (Bat‚àí1 œït‚àí1)) (7)
We assume that the likelihood distribution Cat(A) and the prior distribution Cat(B) have already
been learned accurately. That is, the parameter A perfectly knows which observation ot corresponds
to each possible hidden state st, and B perfectly knows the outcome of each possible action at for
each state. This simplifying assumption is intended to exclude side effects that may arise from other
parts of the generative model when investigating changes in behavior due to changes in the self-prior
(the learning of both likelihood and prior distributions is included in the model for the continuous
environment discussed later).
The distribution ÀúP(ot) that serves as the setpoint for future observations is given in the form of
a categorical distribution Cat(C), where C is a column vector with a row for each possible ot.
According to the definition in Eq. (3), the preferred prior Àúp(ot) should decompose into an extrinsic
component Àúp(oE
t ) and an intrinsic component Àúp(oI
t ), but in this study, we excluded the extrinsic
component in order to investigate purely intrinsic motivational behavior driven by the self-prior.
Therefore, all components of the observation vector ot correspond to intrinsic observations oI
t , and
the entire Cat(C)is defined as the self-prior ÀúP(oI
t ):
Self-prior: ÀúP(oI
t ) =Cat(C) (8)
6
The agent autonomously learns the self-prior by recording the frequency of each observation experi-
enced so far and storing the corresponding observation probabilities in the parameter C. The initial
value of C is set so that all observations have equal probability. To determine the optimal actionat
based on this, we compute the expected free energy for each action (derivation process see Appendix
A.1.2):
G ‚âà(Batœït)¬∑ H[A] +DKL[ABatœït‚à•C] (9)
To evaluate future policies, we propagate the current beliefœït through the transition model B into
future beliefs œït+1:t+N along "imagined time": œït+œÑ =B at+œÑ‚àí1 œït+œÑ‚àí1 . The optimal policy is
selected by summing expected free energies for each candidate policy œÄ={a t, ..., at+N }. The
policy is expressed as a categorical distribution, from which actions are sampled:
Policy:P(œÄ) =œÉ(‚àíG) (10)
3.2 Model for Continuous Environment
In prior research attempting to apply the free energy principle to (high-dimensional) continuous
environments, many implementations use joint angles directly as internal states and generate actions
through analytically derived backpropagation (e.g., (Priorelli et al., 2023; Sancaktar et al., 2020)).
While these approaches offer the advantage of analytical computation, they are limited in that
the representation is constrained to joint angles, making it difficult to implicitly encode complex
sensorimotor information in high-dimensional latent spaces and challenging to apply to complex
tasks requiring long-term planning.
In contrast, approaches utilizing deep neural networks do not directly mimic biological details, but
they provide a practical method to extend the core computational principles of active inference to
high-dimensional problems (Millidge, 2020). The focus of this study is to propose and demonstrate
the conceptual mechanism of the self-prior, which is not bound to any specific neural implementation.
Therefore, we follow the deep neural network based approach to enable representation of complex sen-
sorimotor information in high-dimensional latent spaces and to perform future planning. Specifically,
in this study, we follow the approach of prior research that interprets the Recurrent State-Space Model
(RSSM) in PlaNet (Hafner et al., 2019) as optimizing variational free energy (√áatal et al., 2020), and
interprets policy gradient methods as minimizing expected free energy (Millidge, 2020). The RSSM
is essentially an extension of the Variational Autoencoder (V AE, (Kingma, 2013)) architecture to
sequential data, directly implementing the principle of variational free energy minimization.
Whereas the discrete model fixed the parameters of the likelihood and prior distributions in variational
free energy, the continuous model follows the standard RSSM structure and trains the parameters œï
using deep neural networks. The RSSM separates the deterministic latent state ht and the stochastic
latent states t, enabling proper sequential decomposition:
Deterministic state:h t =f œï(ht‚àí1, st‚àí1, at‚àí1)
Stochastic prior:p œï(ÀÜst |h t)
Stochastic posterior:q œï(st |h t, ot)
Likelihood:p œï(ot |h t, st)
(11)
The deterministic state captures temporal dependencies through a GRU cell, while the stochastic
prior and posterior each generate Gaussian distribution parameters from the deterministic states
(and observations). This separation ensures proper variational decomposition at each time step
(implementation details in Appendix A.2.2).
The continuous model also investigates purely intrinsic motivational behavior, thusÀúp(ot) = Àúp(oI
t ).
The self-prior is learned using Neural Spline Flows (NSF, (Durkan et al., 2019)) to maximize
observation log-likelihood. This is done by sampling observations o from a replay buffer D that
stores recently experienced trajectories:
Self-prior:Àúp Œæ(ot)
arg min
Œæ
Lself = arg min
Œæ
Eo‚àºD[‚àílog ÀúpŒæ(o)] (12)
7
Future Planning(Minimize )Current(Minimize )
Preferred Prior
Log Prob.
Entropy
KL Div.
(a) Original active inference
Future Planning(Minimize )Current(Minimize )
(Preferred Prior)
Self-Prior
Log Prob.
Self-Prior (b) Active inference with self-prior
Figure 2: Graphical model of active inference using deep neural networks that minimize variational
free energy F and expected free energy G. The self-prior Àúp(oI
t ) is trained to maximize the log-
likelihood of observations oI
t . In the expected free energy calculation (highlighted in blue), the
learned self-prior serves as the behavioral setpoint alongside the fixed preferred prior. Although the
preferred prior and self-prior can theoretically be applied simultaneously, we use only the self-prior
in this study for clarity of exposition; thus, the preferred prior is shown faded in the figure.
Since action candidates cannot be enumerated in continuous environments, we use policy gradient
methods. Following (Millidge, 2020), we train a policy network that predicts actions from states and
a value network that estimates expected free energy over an infinite horizon:
Policy:q Œ∏(at |s t)
Expected utility:g œà(st) (13)
This is a similar approach to Soft Actor-Critic (Haarnoja et al., 2018), and approximates expected
utility through GAE(Œª) estimation (Schulman et al., 2015). Training details and hyperparameters are
provided in Appendix A.2.3 and (Mazzaglia et al., 2021).
4 Experiments and Results
In this section, we validate the emergence of goal-directed behavior driven by the self-prior, which is
learned from multimodal sensory experiences, through computer simulations. As briefly mentioned
earlier, our experiment is based on the following assumptions: (i) since an infant typically does not
have stickers attached to its body, it acquires a self-prior that "there are no stickers on my body"; (ii)
when a sticker is attached, the mismatch between sensory observations and the self-prior increases
free energy; and (iii) to reduce free energy, the agent spontaneously generates reaching behavior
aimed at the sticker. Importantly, we did not preset any preferred states such as removing stickers
or adopting specific postures; all goal-directed behaviors emerge from the self-prior autonomously
formed from the sensations the agent experiences through motor babbling.
Through these experiments, we (i) verify the process by which the self-prior, representing the agent‚Äôs
own preference, is autonomously formed, (ii) confirm the emergence of reaching behavior within
the free energy minimization framework incorporating the self-prior, and (iii) in experiments in the
discrete environment, additionally show that behavior can vary for the same stimulus depending on
changes in the self-prior.
4.1 Simulation in Discrete Environment
In the discrete environment, the agent receives multimodal observations composed of discrete
proprioceptive and tactile inputs (Figure 3). The agent‚Äôs right hand can occupy one of five positions,
labeled 0,1,2,3, or 4, representing its proprioceptive state. The left arm has three tactile sensors,
located at positions 1,2, and 3 in the right hand‚Äôs coordinate frame, and each sensor can be activated
independently, yielding 23 = 8possible tactile states. Consequently, the observation variable ot can
8
Sticky note
Left arm
Right hand
<Proprioceptive>
Joint position (0~4)
<Tactile>
Skin touch (000~111)
1
0
1
0
1
2
[3]
4
ùëúùë°
ùëéùë°
Figure 3: Overview of the discrete environ-
ment. The right hand can move left or right
either above the left arm or outside of it, and
tactile input occurs either where the right hand
is located or where the sticker is attached.
Table 2: Possible combinations of sensory
observations in the discrete environment
ot Touch Hand position
(000‚àº111) (0‚àº4)
0 000 0
1 000 1
... ... ...
28 101 3
... ... ...
39 111 4
take 5√ó8 = 40 distinct values and is represented as a 40-dimensional one-hot column vector (Table
2).
A caregiver can attach a sticker to the agent‚Äôs left arm. If a sticker is placed at a specific position (e.g.,
position x), the agent continuously receives a tactile signal from position x, regardless of the right
hand‚Äôs position. If the right hand exactly matches the sticker‚Äôs location, the agent perceives a single
tactile signal.
At each time step, the agent can choose one of three actions at: (i) remain stationary, (ii) move one
step to the left, (iii) move one step to the right. Thus, at is represented as a 3-dimensional one-hot
column vector. In the discrete environment experiment, there are 34 = 81possible policies œÄ, which
are sequences of actions up to 4 steps into the future, and are probabilistically determined by Equation
10 according to the sum of expected free energies for each.
At the beginning of the experiment, the self-prior was uniformly initialized across all possible
observations. This makes the expected free energy equal for all policies, causing random policy
selection. That is, no special response occurs even when a sticker is attached (Figure 5a).
Subsequently, we let the agent perform motor babbling without constraints for a sufficiently long
period (10,000 steps). In the case of real infants, such experiences may correspond to a variety of
everyday experiences that fulfill extrinsic preferences. However, for simplicity, we performed motor
babbling in which the agent randomly selects its actions a. Through this process, the pattern that "a
single tactile signal occurs when the right hand is over the left arm" is learned, and as a result, "no
tactile signal occurs when the right hand is off the left arm" (Left of Figure 4).
At this point, when a caregiver attaches a sticker to position 1 on the left arm, a tactile signal occurs
at a location different from the current hand position (position 4). According to the newly learned
self-prior, this is a low-probability situation, resulting in an increase in free energy (i.e., attention
is drawn due to the mismatch with the model). Accordingly, the agent begins planning actions to
reduce expected free energy and concludes that touching the sticker aligns best with its self-prior. As
a result, the agent shows goal-directed movement toward position 1 (Figure 5b). Similarly, when the
caregiver moves the sticker to position 3, the agent detects this new signal as incongruent with its
model and immediately reaches toward position 3.
Next, we let the agent undergo another round of motor babbling (20,000 steps) while the sticker
remained at position 3. Over time, the self-prior was gradually updated so that the probability of
tactile signal at position 3 increased relative to when the sticker was absent. In other words, the agent
came to expect tactile input at position 3 regardless of its hand position (Right of Figure 4).
After this extended babbling, if the sticker was moved to position 1, the agent still noticed the
mismatch (as a tactile signal at position 1 is inconsistent with the self-prior) and reached out to touch
it. However, if the sticker was placed back at position 3, the agent no longer exhibited consistent
9
0
10k
20k
30k
0
39
0.05
0.1
0.15
0.2
0.25
Time
Observation ID
Put sticker
Figure 4: Change in self-prior over time. Before the sticker is attached, the probability increases for
situations where no sticker is present on the arm (t <10,000). After the sticker is attached, the agent
gradually adapts to the new situation where the sticker is present (t‚â•10,000).
0
5
10
15
20
25
0
5
10
15
20
25
30
35
40
Time
Expected free energy
0
5
10
15
20
25
0
1
2
3
4
Time
Env. position
Sticker on 1
Sticker on 3
Sticker on 1
(a) Before acquiring the self-prior (t= 0)
0
5
10
15
20
25
0
5
10
15
20
25
30
35
40
Time
Expected free energy
0
5
10
15
20
25
0
1
2
3
4
Time
Env. position
Sticker on 1
Sticker on 3
Sticker on 1
 (b) After acquiring the self-prior (t= 10,000)
Figure 5: Comparison of the agent‚Äôs behavior before and after acquiring the self-prior. The top panel
illustrates environmental changes over time: the red line indicates the hand position, the yellow
dashed line indicates the sticker position. White areas denote where tactile feedback occurred, black
areas indicate no tactile feedback, and gray areas represent regions outside the arm where tactile
feedback never occurs. The bottom panel shows expected free energy over time. Each green dot
represents the free energy of a candidate policy, with lower free energy policies being more likely to
be selected. The red line connects the actually selected policies. (a) Before acquiring the self-prior,
the agent does not respond even when a sticker is attached to the arm. (b) After acquiring the
self-prior, goal-directed behavior emerges: the agent moves its hand to the sticker‚Äôs location when it
appears on the arm.
goal-directed behavior (Figure 6). Because the self-prior had already adapted to position 3, no
additional mismatch arose. Consequently, the difference in expected free energy between reaching or
not was negligible, so the agent simply behaved randomly.
This situation, in which the self-prior does not induce any motivation, allows us to isolate the
influence of the traditional information gain component of intrinsic motivation in the expected free
energy framework. In this experiment, since the parameters A and B are already fixed in a fully
accurate state, there is no new information to be gained from observations. As a result, no epistemic
(information-seeking) drive emerges, and all policies yield nearly identical expected free energy,
causing the agent to behave randomly.
10
0
5
10
15
20
25
0
5
10
15
20
25
30
35
40
Time
Expected free energy
0
5
10
15
20
25
0
1
2
3
4
Time
Env. position
Sticker on 1
Sticker on 3
Sticker on 1
Figure 6: Agent‚Äôs behavior when a sticker has persistently been attached to position 3 (t= 30,000).
When the sticker is placed on other positions, the agent still reaches toward them; however, it no
longer shows interest when the sticker is placed at position 3.
In contrast, we observed that actions arising from the self-prior could be generated even when no
additional information gain was possible. In other words, because the parameters of the self-prior, C,
continuously evolved with experience, intrinsic motivation related to familiarity based on experience
was still induced. As a result, the agent was driven to act in ways that would align the newly updated
self-prior with the observations.
4.2 Simulation in Continuous Environment
Up to this point, we have introduced the core idea that goal-directed behavior can emerge from the
self-prior using a simple discrete setup. We now extend this to an environment with continuous-valued
variables, inspired by the environment setup of (Marcel et al., 2022) (Figure 7).
Initially, the agent‚Äôs two arms lie on the xy-plane. The lengths of the left forearm, left arm, torso,
right arm, and right forearm are 80, 70, 140, 70, and 80mm , respectively. The left forearm features a
30mm -wide two-dimensional tactile sensor array, placed laterally relative to the forearm itself. At
the endpoint of the right forearm is a circular "right hand" of radius 6mm , which can generate tactile
input on the left forearm.
All joint angles are expressed in radians. The left elbow and left shoulder angles are fixed at œÄ/3
and 2œÄ/3, while the right shoulder and right elbow angles have respective ranges of[0,2œÄ/3] and
[0,3œÄ/4] . Additionally, the right hand can move along a pseudo-z (height) axis within the interval
[0,20]mm.
Similar to the discrete case, the agent perceives both proprioception and touch; however, these
quantities are now continuous. The proprioceptive input is a three-dimensional real-valued vector
(shoulder,elbow,hand) , while tactile input is represented by an80√ó30 real-valued matrix normalized
to [0,1] . Whenever the right hand is above the left forearm at a height of ‚â§10mm , the tactile sensor
on the left forearm is activated with intensity inversely proportional to the hand‚Äôs height.
To represent both tactile and proprioceptive modalities as a unified observationot in the model for
the continuous environment, we convert each modality into an embedding of the same dimensionality
and integrate them. The tactile matrix is transformed into an embedding using a CNN, and the
proprioceptive vector is transformed into an embedding using a MLP. We combine the two embeddings
via element-wise addition to yield the final integrated observation embedding ot. To decode from the
embedding back to the original sensations, we use inverse transformation modules (see Appendix
A.2.1 for detailed hyperparameters).
A caregiver can attach a sticker of radius 4mm to the left forearm at a fixed height of 0mm . As a
simplified environmental rule to demonstrate the core self-prior mechanism, the agent can remove the
sticker if the center of its right hand remains within the sum of the hand‚Äôs and sticker‚Äôs radii for10
11
70mm
70mm70mm
80mm
80mm
30mm
ùúã/3
2ùúã/3
[0~2ùúã/3]
[0~3ùúã/4]
Hand
Sticker
70mm
Figure 7: Overview of the continuous environment. As in the discrete environment, the right hand
can move over and around the left arm, and tactile sensations are generated where the hand or the
sticker is located.
consecutive time steps. This represents a simplified assumption wherein sustained contact between
the hand and sticker leads to object removal, without modeling complex manipulation actions such as
grasping or pushing.
At every time step, the right shoulder and right elbow angles can each rotate between ‚àí0.05 and
+0.05rad , and the hand‚Äôs height can shift by ‚àí0.5 to +0.5mm . To keep the hand near the forearm,
we impose an additional constraint that the hand‚Äôs center must remain within 15mm of the forearm‚Äôs
surface. If a proposed movement would place the hand outside this region, the agent‚Äôs action is
disregarded and its previous position is retained.
We conducted the continuous environment experiments on a machine running Ubuntu 22.04.5 LTS 64-
bit (Linux 5.15.0-1066) with an Intel Xeon E5-2698 v4 CPU and an NVIDIA Tesla V100-SXM2 GPU,
using Python 3.11.10 and PyTorch 2.5.1. For our normalizing flows based self-prior, we employed
the ‚Äúzuko‚Äù library (version 1.3.0), and we based much of our overall algorithmic implementation on
Dreamer (Hafner et al., 2020) and Contrastive Active Inference (Mazzaglia et al., 2021).
To train and evaluate our deep model, we stored episodes in a replay buffer and randomly sampled
them during model training. Each episode was collected either by using random actions or by
following the agent‚Äôs policy from active inference, chosen with equal probability (50%). Likewise,
for half of the episodes, a sticker was placed at a random location on the arm; for the other half, no
sticker was used.
After collecting an initial batch of 100 episodes, we trained the model for 100 epochs at the end of
each episode. In each epoch, we sampled B= 50 trajectories of length L= 50 from the replay buffer,
and generated imagined rollouts with a planning horizon H= 15 for policy learning. For variational
free energy minimization, trajectories were sampled from all available data, regardless of the presence
of a sticker or whether the behavior was random or policy-driven. This was intended to ensure the
acquisition of an accurate world model, similar to the assumption in the discrete environment that A
and B were already known. Likewise, all data were used for expected free energy minimization, so
that the agent could learn which actions reduce free energy effectively across various conditions.
For self-prior learning, we constructed the dataset such that only about 5% of the episodes included
a sticker, while the rest were sampled without any sticker present. Consequently, as in the discrete
environment experiment, the self-prior came to assign high probability to observations like ‚Äúa single-
point touch occurs when the right hand is above the left arm‚Äù and ‚Äúno touch occurs when the right
hand is away‚Äù.
After collecting and training for 2,000 episodes, we attached a sticker to the agent‚Äôs left arm. As a
result, the agent exhibited goal-directed behavior of reaching toward the sticker (Figure 8a). This
occurred because touching the sticker aligned with the self-prior, minimizing free energy (Figure 8b).
Moreover, after reaching the sticker, the agent continued touching it and eventually removed it,
indicating that it had a persistent motivation to reduce the mismatch rather than simply losing interest
after contact.
Interestingly, the agent exhibited reaching behavior toward the sticker even when the right hand was
not initially touching the arm. If the self-prior had been learned based solely on tactile information,
(i) the case where the left arm is touched by the right hand and (ii) the case where a sticker is attached
12
t = 0
t = 5
t = 10
t = 15
t = 20
t = 25
t = 30
t = 35
t = 40
t = 45
t = 50
t = 55
(a) Time series of reaching behavior
 (b) Change in expected free energy
Figure 8: Agent behavior when a sticker is placed in the continuous environment. (a) When a sticker
(blue circle) is attached, the hand (red circle) moves toward it, illustrating goal-directed reaching
behavior. The figure visualizes the agent‚Äôs tactile matrix, where grayscale tactile data is overlaid with
colored markers for the hand and sticker for clarity. (b) Reaching for the sticker reduces expected
free energy, and removing the sticker leads to its minimization. Shaded areas represent the standard
deviation across 64 experiments using 8 model training seeds (0~7) tested on 8 environment seeds
(0~7). The figure uses seed 4, selected for clearly demonstrating the reaching behavior.
might not be distinguishable, since both yield a single tactile point on the arm. In that case, no
reaching behavior toward the sticker would emerge. However, the agent did reach toward the sticker,
suggesting that the self-prior was learned by integrating both tactile and proprioceptive inputs. In
this sense, our system, which utilizes a self-prior learned from multimodal sensory inputs to generate
reaching behavior, resembles the concept of a body schema mentioned in (Gallagher, 1986; Hoffmann,
2021), which supports action planning and control.
Additionally, even if extra touches arose mid-trajectory (e.g., from the right hand itself) thus mo-
mentarily increasing mismatch relative to a single-point self-prior, the agent accepted this short-term
deviation to achieve the ultimate goal of minimizing overall free energy. This robust, noise-tolerant se-
quence of goal-directed action demonstrates that our agent maintained a coherent intention throughout
its interactions.
5 Discussion
We proposed the self-prior as an intrinsic preference that isautonomouslyformed from an agent‚Äôs
sensory experiences. We then applied it to a simulated infant agent within the active inference
framework and confirmed the emergence of goal-directed behavior, specifically reaching for a sticker.
In this section, to clarify the contributions of our work to developmental science, we explain how our
approach differs from previous studies that attempted to generate spontaneous behaviors, propose
the role of the self-prior as a potential origin of early intentional behavior, and finally discuss the
limitations of this work and future research directions.
5.1 Computation Models of Intrinsic Motivation
Approaches that aim to computationally address intrinsic motivation have primarily been explored in
reinforcement learning, where designing high-quality reward functions remains a significant challenge.
As a result, intrinsic motivation has been proposed as a means for agents to more efficiently explore
the environment and better infer reward functions. However, a fundamental question persists: how
can an agent autonomously form motivation and generate behavior in environments where no goals
13
are given, and what kind ofvalueguides the agent‚Äôs actions under such conditions (Juechems &
Summerfield, 2019)?
Extrinsic Homeostatic Reinforcement LearningAn alternative approach is homeostatic rein-
forcement learning, in which the agent can generate a range of behaviors simply by maintaining
certain sensory channels to specified thresholds, without environmental goal (Keramati & Gutkin,
2011). Recent studies have extended this concept to complex settings using deep neural networks,
examining how internal physiological states drive behavior (Yoshida et al., 2024). Under active
inference, a similar mechanism emerges: by minimizing free energy with respect to an internal
setpoint (expressed as a preferred prior), an agent can generate a single goal-directed act like reaching
for a target (Oliver et al., 2022) or orchestrate repeated behaviors (Matsumoto et al., 2022).
However, although these approaches appear intrinsic because behaviors occur without external
rewards, most have a limitation in that the experimenterfixesthe reference values (e.g., setpoints or
preferred priors) in advance. That is, even though the agent calculates rewards internally, the reference
itself is externally provided, so it ultimately has an extrinsic nature. Therefore, these approaches
do not fully resolve the fundamental question of "Where does value come from, and can the agent
establish it by itself?"
Reward-auxiliary Intrinsic MotivationTo allow the agent to establish its own value function
independently, neither external rewards nor internal setpoints can be predetermined. But without
these metrics, it is difficult to track learning progress or judge the utility of behaviors, which is
why many studies on intrinsic motivation still tend to analyze it primarily as a means to facilitate
exploration for extrinsic tasks (Aubret et al., 2023). That is, the process by which agents generate and
maintain goals themselves has been relatively less illuminated.
Heterostatic Intrinsic MotivationSome works do investigate the behaviors arising purely from
intrinsic motivation and qualitatively evaluate them in scenarios with no externally defined reward
(Eysenbach et al., 2018; Pathak et al., 2017). While these highlight the agent‚Äôs acquisition of diverse
skills or behaviors, they typically involve a heterostatic approach, continuously seeking novel states.
Consequently, once uncertainty is sufficiently reduced, further exploration tends to diminish, and
fewer new behaviors emerge.
Active inference offers a unified theoretical framework that can integrate both extrinsic and intrinsic
motivations under the single quantitative measure of free energy (Parr et al., 2022). Furthermore,
(Biehl et al., 2018) showed that many existing methods for intrinsic motivation can be cast within
active inference as processes for improving the posterior, a perspective reminiscent of (Schmidhuber,
2010), who defined intrinsic motivation in terms of seeking ‚Äúbetter world models.‚Äù However, such
posterior-improving motives are likewise heterostatic: they promote exploration until uncertainty is
resolved, after which they elicit fewer notable actions.
Homeostatic Intrinsic MotivationRecently, studies have begun to emerge that address homeostatic
intrinsic motivation, in which agents autonomously define their own setpoints but aim to preserve
already learned states or past experiences, rather than continuously seeking novelty. These studies
have made important contributions by demonstrating that goal-directed behaviors can emerge without
external rewards. For example, (Marcel et al., 2022) proposed a process in which latent representations
of self-touch observations are used as setpoints to induce reaching behavior, and (Kim et al., 2023;
Takemura et al., 2018) demonstrated that forward and inverse models learned from motor babbling
can generate reaching toward target positions without explicit rewards. Most similar to our approach,
(Sajid et al., 2021) introduced the concept of "learned preferences" within the free energy framework
and semantically analyzed how a drive to reduce mismatches between observation and preference can
lead to new behaviors. While these studies have demonstrated the potential of homeostatic intrinsic
motivation, they operate within independent frameworks focused on a single intrinsic motivation
mechanism, and integration with extrinsic motivations remains a future challenge.
SummaryIn summary, our approach is differentiated from existing research in the following ways:
1. Intrinsic: The agent autonomously establishes criteria based on its own experience rather
than adhering to externally fixed criteria.
14
2. Independent behavior generation: It independently generates specific goal-directed be-
haviors such as reaching, rather than serving an auxiliary role to facilitate exploration.
3. Homeostatic: Even in situations without information gain, behaviors persist without stop-
ping as long as there is a mismatch with familiar states.
4.FEP with active inference: Defined within the active inference framework, it can be theo-
retically integrated with existing extrinsic motivations and heterostatic intrinsic motivations
(information gain).
5.2 Origin of Early Intentional Behaviors
Infants initially depend heavily on reflex mechanisms for action. Over time, however, they gradually
develop what (Mele & Moser, 1994) describes as ‚Äúintentional behavior,‚Äù comprising (i) a motivational
system that judges how to handle incoming stimuli and (ii) the capability to execute genuinely
goal-directed actions. According to (Zaadnoordijk & Bayne, 2020), stimulus-driven intentions arise
first, and then, with accumulating experience, more endogenous intentions emerge. For instance,
newborns rely on reflexes such as rooting to acquire feeding experience; later, merely seeing a bottle
prompts them to reach out and drink (stimulus-driven intention). As they become aware of internal
states like hunger, they begin actively searching for the bottle themselves (endogenous intention).
Over time, to recognize themselves as independent entities, infants must undergo abundant ex-
periences featuring multimodal redundancy and temporal‚Äìspatial contingencies (Rochat, 1998).
Sequentially developing intentional actions may spontaneously provide opportunities for such experi-
ences. (Thelen et al., 1993) similarly noted that infants initially lack both the ability and the intention
to perform reaching, and that the formation of stable intentions, coupled with active exploration
driven by those intentions, facilitates the learning of reaching.
However, prior studies have not discussed how intentions, including stimulus-driven intentions, and
intentional behaviors develop through what structure. Even (Priorelli & Stoianov, 2023), which
implemented switchable intentions within the active inference framework, used intentions that were
discretely predefined rather than self-developing. Meanwhile, neurophysiological evidence suggests
that the anterior cingulate cortex primarily deals with reward-related errors in individual decisions,
whereas the posterior cingulate cortex processes discrepancy signals related to autobiographical
information such as one‚Äôs own body or past experiences, and triggers new behaviors (Brewer et al.,
2013; Pearson et al., 2011).
Our proposed self-prior aligns with this notion of ‚Äúself-relevant predictions,‚Äù indicating that when
new input fails to satisfy these predictions, the system initiates action. In other words, even without
any externally designed goal, an intrinsic drive to preserve a self-prior learned from past experiences
can manifest behaviors akin to an infant persistently touching a newly discovered sticker on their
arm. This mirrors Piaget‚Äôs concept of primary circular reactions, where repeated attempts can refine
intentions further, offering a theoretical foundation for computational studies on the origins of infant
intentional behavior.
In our experiments, we fixed the self-prior‚Äôs learning rate. However, an excessively high rate could
lead to overreactions (compulsive-like responses) to minor changes, whereas an overly low rate might
cause indifference (apathy) toward new stimuli. Future work could investigate how more flexible
learning rates shape different behavioral styles via self-relevant prediction-error processing.
5.3 Limitations and Future Directions
Our study employs a constructivist approach, focusing on demonstrating how the self-prior mechanism
operates under minimal conditions. The infant sticker-reaching task serves as one example to clearly
illustrate this mechanism, showing what behaviors emerge under the specific condition of tactile and
proprioceptive body reaching. Since the self-prior is modality-independent in principle, the same
mechanism can be applied to various sensory modalities. For example, using visual modality would
be expected to produce reaching behavior toward rarely seen external objects outside the agent‚Äôs
body. Indeed, several phenomena observed in actual infant development may also be explained by
the self-prior mechanism. For instance, according to (Eskandari et al., 2020), preterm infants nested
in boundaries showed a significant reduction in unstable extension movements and maintained stable
flexed postures, which can be reinterpreted as the infants preferring uterine-like environments and
15
maintaining stable behavioral states to resolve mismatches with a self-prior learned from the flexed
posture and tactile boundary sensations experienced during fetal stages.
Furthermore, behaviors that emerge in more complex cognitive contexts beyond simple sensorimotor-
level adjustments, such as attention or social interactions, may also be explained by the self-prior.
(Warneken & Tomasello, 2006) presented tasks in which infants help adults who are having difficulty
achieving goals, including results where infants opened a door for an adult who was having difficulty
putting a magazine into a closet. Reinterpreting this in terms of the self-prior approach, it is possible
that the infant opened the door to resolve a mismatch with a self-prior learned from visual information
that adults normally open the closet door and put magazines in. Thus, the same principle of learning
the probability density over experienced observations can explain behaviors across diverse contexts.
Our approach has only been validated in discrete environments and low-degree-of-freedom (3-DoF)
continuous environments, but real infants or robots exhibit far more complex and varied sensorimotor
interactions. Therefore, it is necessary to extend this study to high-dimensional state and action
environments. In particular, by utilizing simulations of early human sensorimotor experiences
including fetal stages (Kim et al., 2022), it would be possible to explore how rich sensorimotor inputs
provided in utero shape the self-prior and goal-directed behaviors, thereby revealing what behaviors
can develop under physical constraints (Kuniyoshi, 2019).
In such complex environments, achieving generalizations such as reusing priors acquired across
diverse contexts would require integrating all available modalities, learning various scenes over
extended periods, and employing high-dimensional self-prior models capable of probabilistically
modeling all of these. Such models could potentially be achieved through hierarchical active
inference (Friston et al., 2017), or deep generative models like Transformers for handling long time
series (Chen et al., 2022; Vaswani et al., 2017). Since the self-prior learns experienced sensations in a
density model without prior knowledge, such extensions are possible in principle through scaling up
generative models.
For example, generalizations such as transferring reaching behavior learned through touch to visual
stimuli, generalizing responses to a specific shape of sticker to other shapes, or extending behaviors
toward objects attached to one‚Äôs own body to external objects are all expected to be possible through
large-scale generative models capable of learning statistical regularities across diverse sensory
experiences. Such generalization can be understood not simply as repetition of the same action, but
as a process of recognizing and utilizing structural similarities among self-priors formed in different
contexts. Additionally, we showed behaviors driven solely by familiarity based intrinsic motivation
by excluding externally imposed reward criteria, but in reality, infants or robots must balance multiple
motivations such as energy homeostasis and novel information exploration. Therefore, designing
experiments that systematically assess how these motivations complement or conflict and organize
behavior in large-scale tasks will be an important task for future research.
Meanwhile, our study demonstrated that, by separating an intrinsic homeostatic term using a learnable
self-prior from the extrinsic term using a fixed preferred prior that was commonly provided in prior
studies using expected free energy, an agent can not only generate actions aimed at achieving
predefined goals but also spontaneously generate goal-directed behaviors. However, a current
limitation is that while theuseof the self-prior is integrated into policy generation via expected free
energy, thelearningof the self-prior is performed separately from the learning of variational free
energy.
One potential clue to resolving this separation may also lie in a hierarchical architecture. Specifically,
a higher-level hidden state that reflects long-term experiences may be gradually learned, and the
likelihood distribution inferred from it could serve as the self-prior for the lower level. This differs
from conventional hierarchical inference, where the higher level simply learns priors and determines
actions on a slower timescale to influence the lower level (e.g., P(st|st‚àí1, at‚àí1) =Cat(Bat‚àí1 )).
Instead, it continuously receives information from the lower level to learn a prior, serving only to form
a self-prior over a long duration (e.g., P(s) =Cat(B) ). This hierarchical approach is also expected
to be closely connected to building preferences over internal states. While the current implementation
learns the self-prior in observation space, through a hierarchical approach, there is a possibility that
higher-level internal latent representations may be formed through experience.
Another possibility is that the learning of the self-prior is handled not by the cortex but by a separate
memory system such as the hippocampus, and thus may require structural separation from the
16
inference system proposed under the free energy principle. Concrete validation of these open
hypotheses is left for future work.
Finally, in the discrete environment, we could observe that as time passed with a sticker attached, the
self-prior gradually changed and, accordingly, behavior also changed. By contrast, in the continuous
environment, we conducted the experiment by fixing the probability of stickers being included in the
observations used for self-prior learning at 5%. This was because the self-prior model stored past data
in a replay buffer and learned without considering temporal order, making it difficult to investigate
gradual changes in the self-prior according to changes in observation frequency. To reproduce the
process by which stimulus-driven intention gradually strengthens into a more endogenous form of
intention in real infants or robots, a self-prior learning mechanism that considers temporal order is
necessary. This could potentially be implemented through the hierarchical architecture mentioned
earlier, where higher levels integrate long-term experiences.
6 Conclusion
Building on the free energy principle and the active inference framework, we have demonstrated
that an agent‚Äôs intrinsic drive to learn and maintain a "self-prior", which is akin to a body schema,
can induce goal-directed behaviors even in the absence of externally specified rewards. In particular,
whereas the conventional active inference literature has emphasized heterostatic intrinsic motivation
(i.e., seeking new information), our work highlights a homeostatic form of intrinsic motivation, one
that actively strives to preserve familiar sensory experiences.
Using a simulated infant touching and examining a sticker on its arm as an illustrative example,
we showed how an internally derived drive to resolve mismatches between the self model and
incoming data can manifest in behaviors such as reaching and sticker removal, all without any explicit
reward criteria. This offers a computational interpretation of stimulus-driven intentional actions
in developmental psychology and suggests potential relevance to the neurobiological literature on
posterior cingulate cortex, which processes errors linked to autobiographical information.
Future work should extend our approach to complex physical environments such as full-scale infant
simulations or high-DoF robots to investigate the long-term formation and refinement of intentions
and behaviors. Additionally, we must experimentally validate how multiple motivations interact
within a single free energy framework, incorporating not only familiarity based motivation but also
extrinsic and information-seeking motivations. Such research will play an important role in analyzing
the spontaneous developmental trajectory of intentional behavior.
Acknowledgments and Disclosure of Funding
This work was supported by JST, PRESTO Grant Number JPMJPR23S4, Japan.
References
Aubret, A., Matignon, L., & Hassas, S. (2023). An Information-Theoretic Perspective on Intrinsic
Motivation in Reinforcement Learning: A Survey.Entropy, 25(2), 327.
Biehl, M., Guckelsberger, C., Salge, C., Smith, S. C., & Polani, D. (2018). Expanding the Active
Inference Landscape: More Intrinsic Motivations in the Perception-Action Loop.Frontiers in
neurorobotics.
Bigelow, A. E. (1986). The development of reaching in blind children.British Journal of Develop-
mental Psychology, 4(4), 355‚Äì366.
Brewer, J., Garrison, K., & Whitfield-Gabrieli, S. (2013). What about the ‚ÄúSelf‚Äù is Processed in the
Posterior Cingulate Cortex?Frontiers in Human Neuroscience, 7.
Chen, C., Wu, Y .-F., Yoon, J., & Ahn, S. (2022). Reinforcement Learning with Transformer World
Models.arXiv:2202.09481.
Chung, J., Gulcehre, C., Cho, K., & Bengio, Y . (2014). Empirical Evaluation of Gated Recurrent
Neural Networks on Sequence Modeling.arXiv:1912.01603.
17
Czikszentmihalyi, M. (1990).Flow: The psychology of optimal experience. New York: Harper &
Row.
Di Domenico, S. I. & Ryan, R. M. (2017). The Emerging Neuroscience of Intrinsic Motivation: A
New Frontier in Self-Determination Research.Frontiers in Human Neuroscience, 11.
Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G. (2019). Neural spline flows.Advances in
neural information processing systems, 32.
Eskandari, Z., Seyedfatemi, N., Haghani, H., Almasi-Hashiani, A., & Mohagheghi, P. (2020). Effect
of nesting on extensor motor behaviors in preterm infants: A randomized clinical trial.Iranian
Journal of Neonatology, 11(3), 64‚Äì70.
Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Diversity is All You Need: Learning Skills
without a Reward Function123.arXiv:1802.06070.
Friston, K. (2010). The free-energy principle: a unified brain theory?Nature Reviews Neuroscience,
11(2), 127‚Äì138.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O‚ÄôDoherty, J., & Pezzulo, G. (2016). Active
inference and learning.Neuroscience & Biobehavioral Reviews, 68, 862‚Äì879.
Friston, K. J., Parr, T., & de Vries, B. (2017). The graphical brain: Belief propagation and active
inference.Network Neuroscience, 1(4), 381‚Äì414.
Gallagher, S. (1986). Body Image and Body Schema: A Conceptual Clarification.The Journal of
Mind and Behavior, 7(4), 541‚Äì554.
Gopnik, A. (2009).The philosophical baby: What children‚Äôs minds tell us about truth, love & the
meaning of life. Random House.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. InInternational conference on
machine learning(pp. 1861‚Äì1870).: PMLR.
Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2020). Dream to Control: Learning Behaviors by
Latent Imagination.arXiv:1912.01603.
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J. (2019). Learning
latent dynamics for planning from pixels. InInternational conference on machine learning(pp.
2555‚Äì2565).: PMLR.
Hoffmann, M. (2021). Body models in humans, animals, and robots: Mechanisms and plasticity.
InBody schema and body image: New directions.(pp. 152‚Äì180). New York, NY , US: Oxford
University Press.
Hoffmann, M., Chinn, L. K., Somogyi, E., Heed, T., Fagard, J., Lockman, J. J., & O‚ÄôRegan, J. K.
(2017). Development of reaching to the body in early infancy: From experiments to robotic models.
In2017 Joint IEEE International Conference on Development and Learning and Epigenetic
Robotics (ICDL-EpiRob)(pp. 112‚Äì119).
Juechems, K. & Summerfield, C. (2019). Where Does Value Come From?Trends in Cognitive
Sciences, 23(10), 836‚Äì850.
Kanazawa, H., Yamada, Y ., Tanaka, K., Kawai, M., Niwa, F., Iwanaga, K., & Kuniyoshi, Y . (2023).
Open-ended movements structure sensorimotor information in early human development.Proceed-
ings of the National Academy of Sciences, 120(1), e2209953120.
Keramati, M. & Gutkin, B. (2011). A Reinforcement Learning Theory for Homeostatic Regulation.
InAdvances in Neural Information Processing Systems, volume 24: Curran Associates, Inc.
Kim, D., Kanazawa, H., & Kuniyoshi, Y . (2022). Simulating a Human Fetus in Soft Uterus. In2022
IEEE International Conference on Development and Learning (ICDL)(pp. 135‚Äì141).
Kim, D., Kanazawa, H., & Kuniyoshi, Y . (2023). Emergence of Reaching using Predictive Learning
as Sensorimotor Development in Complex Dynamics. InThe 11th International Symposium on
Adaptive Motion of Animals and Machines (AMAM2023)(pp. 144‚Äì145).: Adaptive Motion of
Animals and Machines Organizing Committee.
Kingma, D. P. (2013). Auto-encoding variational bayes.arXiv:1312.6114.
18
Kuniyoshi, Y . (2019). Fusing autonomy and sociality via embodied emergence and development of
behaviour and cognition from fetal period.Philosophical Transactions of the Royal Society B:
Biological Sciences, 374(1771), 20180031.
Marcel, V ., O‚ÄôRegan, J. K., & Hoffmann, M. (2022). Learning to reach to own body from spontaneous
self-touch using a generative model. In2022 IEEE International Conference on Development and
Learning (ICDL)(pp. 328‚Äì335).
Matsumoto, T., Ohata, W., Benureau, F. C. Y ., & Tani, J. (2022). Goal-Directed Planning and Goal
Understanding by Extended Active Inference: Evaluation through Simulated and Physical Robot
Experiments.Entropy, 24(4), 469.
Mazzaglia, P., Verbelen, T., & Dhoedt, B. (2021). Contrastive Active Inference.Advances in neural
information processing systems, 34, 13870‚Äì13882.
Mele, A. R. & Moser, P. K. (1994). Intentional Action.No√ªs, 28(1), 39‚Äì68.
Millidge, B. (2020). Deep active inference as variational policy gradients.Journal of Mathematical
Psychology, 96, 102348.
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). On the Relationship Between Active
Inference and Control as Inference. In T. Verbelen, P. Lanillos, C. L. Buckley, & C. De Boom
(Eds.),Active Inference, Communications in Computer and Information Science (pp. 3‚Äì11). Cham:
Springer International Publishing.
Oliver, G., Lanillos, P., & Cheng, G. (2022). An Empirical Study of Active Inference on a Humanoid
Robot.IEEE Transactions on Cognitive and Developmental Systems, 14(2), 462‚Äì471.
Oudeyer, P.-Y . & Kaplan, F. (2009). What is intrinsic motivation? A typology of computational
approaches.Frontiers in neurorobotics, 1, 6.
Parr, T., Pezzulo, G., & Friston, K. J. (2022).Active Inference: The Free Energy Principle in Mind,
Brain, and Behavior. The MIT Press.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven exploration by self-
supervised prediction. InInternational conference on machine learning(pp. 2778‚Äì2787).: PMLR.
Pearson, J. M., Heilbronner, S. R., Barack, D. L., Hayden, B. Y ., & Platt, M. L. (2011). Posterior
cingulate cortex: adapting behavior to a changing world.Trends in Cognitive Sciences, 15(4),
143‚Äì151.
Priorelli, M., Pezzulo, G., & Stoianov, I. P. (2023). Deep kinematic inference affords efficient and
scalable control of bodily movements.Proceedings of the National Academy of Sciences, 120(51),
e2309058120.
Priorelli, M. & Stoianov, I. P. (2023). Flexible intentions: An active inference theory.Frontiers in
Computational Neuroscience, V olume 17 - 2023.
Rochat, P. (1998). Self-perception and action in infancy.Experimental Brain Research, 123(1),
102‚Äì109.
Ryan, R. M. & Deci, E. L. (2000). Intrinsic and Extrinsic Motivations: Classic Definitions and New
Directions.Contemporary Educational Psychology, 25(1), 54‚Äì67.
Sajid, N., Tigas, P., Zakharov, A., Fountas, Z., & Friston, K. (2021). Exploration and preference
satisfaction trade-off in reward-free learning.arXiv:2106.04316.
Sancaktar, C., van Gerven, M. A. J., & Lanillos, P. (2020). End-to-end pixel-based deep active
inference for body perception and action. In2020 Joint IEEE 10th International Conference on
Development and Learning and Epigenetic Robotics (ICDL-EpiRob)(pp. 1‚Äì8).
Schmidhuber, J. (2010). Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990‚Äì2010).
IEEE Transactions on Autonomous Mental Development, 2(3), 230‚Äì247.
Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous
control using generalized advantage estimation.arXiv:1506.02438.
Shultz, T. R. (2013). Computational Models in Developmental Psychology. In P. D. Zelazo (Ed.),
The Oxford Handbook of Developmental Psychology, Vol. 1: Body and Mind. Oxford University
Press.
19
Takemura, N., Inui, T., & Fukui, T. (2018). A neural network model for development of reaching and
pointing based on the interaction of forward and inverse transformations.Developmental Science,
21(3), e12565.
Thelen, E., Corbetta, D., Kamm, K., Spencer, J. P., Schneider, K., & Zernicke, R. F. (1993). The
Transition to Reaching: Mapping Intention and Intrinsic Dynamics.Child Development, 64(4),
1058‚Äì1098.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin,
I. (2017). Attention Is All You Need.arXiv:1706.03762.
Warneken, F. & Tomasello, M. (2006). Altruistic helping in human infants and young chimpanzees.
Science, 311(5765), 1301‚Äì1303.
White, R. W. (1959). Motivation reconsidered: the concept of competence.Psychological review,
66(5), 297.
Yoshida, N., Daikoku, T., Nagai, Y ., & Kuniyoshi, Y . (2024). Emergence of integrated behaviors
through direct optimization for homeostasis.Neural Networks, 177, 106379.
Zaadnoordijk, L. & Bayne, T. (2020). The Origins of Intentional Agency.psyArXiv:wa8gb.
Zaadnoordijk, L., Besold, T. R., & Cusack, R. (2022). Lessons from infant learning for unsupervised
machine learning.Nature Machine Intelligence, 4(6), 510‚Äì520.
√áatal, O., Wauthier, S., De Boom, C., Verbelen, T., & Dhoedt, B. (2020). Learning Generative State
Space Models for Active Inference.Frontiers in Computational Neuroscience, 14.
20
A Appendix
A.1 Detailed Description of the Model for the Discrete Environment
A.1.1 Variational Free Energy Derivation
In the discrete model, observation ot and hidden state st are one-hot column vectors with No and
Ns rows, respectively. Categorical distribution parameters are implemented as matrices: A is an
No √óN s matrix, and each Ba is an Ns √óN s square matrix. For instance, if actions are {LEFT,
STOP, RIGHT}, separate matricesBLEFT,BSTOP,BRIGHT exist.
The variational free energyFexpands as:
F=E Q(st)[logQ(s t)‚àílogP(o t, st)]
=E Q(st)[logQ(s t)‚àílogP(s t)]‚àíE Q(st)[logP(o t |s t)]
=œï t ¬∑(logœï t ‚àílog (Bat‚àí1 œït‚àí1))‚àíœï t ¬∑(log (A¬∑o t))
(14)
The parameterœïthat minimizes free energy is derived by differentiation:
‚àÇF
‚àÇœït
= logœït ‚àílog (Bat‚àí1 œït‚àí1) + 1‚àílog (A¬∑ot) = 0
‚à¥logœï t ‚âàlog (A¬∑o t) + log (Bat‚àí1 œït‚àí1)
œït ‚âàœÉ(log (A¬∑o t) + log (Bat‚àí1 œït‚àí1))
(15)
This corresponds to Eq. (7) in the main text.
A.1.2 Expected Free Energy Derivation
The expected free energy can be derived through step-by-step approximations into a computationally
tractable form:
G=E Q(st+1,ot+1|at)[logQ(s t+1 |a t)‚àílog ÀúP(ot+1, st+1 |a t)]
=E Q(st+1,ot+1|at)[logQ(s t+1 |a t)‚àílogP(s t+1 |o t+1, at)‚àílog ÀúP(ot+1)]
‚âàE Q(st+1,ot+1|at)[logQ(s t+1 |a t)‚àílogQ(s t+1 |o t+1, at)‚àílog ÀúP(ot+1)]
=E Q(st+1,ot+1|at)[logQ(o t+1 |a t)‚àílogQ(o t+1 |s t+1, at)‚àílog ÀúP(ot+1)]
(16)
The approximation in the third line replaces the true posterior P(st+1 |o t+1, at) with the ap-
proximate posterior Q(st+1 |o t+1, at). The fourth line is derived by applying Bayes‚Äô rule:
Q(s, o) =Q(o|s)Q(s) =Q(s|o)Q(o). Reformulating for computation:
G ‚âàEQ(st+1|at)Q(ot+1|st+1,at)[‚àílogQ(o t+1 |s t+1, at)]
+E Q(ot+1|at)[logQ(o t+1 |a t)‚àílog ÀúP(ot+1)]
=E Q(st+1|at)P(ot+1|st+1)[‚àílogP(o t+1 |s t+1)]
+E Q(ot+1|at)[logQ(o t+1 |a t)‚àílog ÀúP(ot+1)]
=E Q(st+1|at)[H[P(o t+1 |s t+1)]] +DKL[Q(ot+1 |a t)‚à• ÀúP(ot+1)]
= (Batœït)¬∑ H[A] +DKL[ABatœït‚à•C]
(17)
This corresponds to Eq. (9) in the main text. In our setup, since we assume that A and B are already
known accurately, the action entropy term for information gain about parameters is omitted in the
discrete environment experiment (see (Parr et al., 2022) for more details on the above equations).
A.2 Detailed Description of the Model for the Continuous Environment
A.2.1 Embedding Transformation Architecture for Continuous Environment Model
To represent both tactile and proprioceptive modalities as a unified observationot in the model for
the continuous environment, we convert each modality into an embedding of the same dimensionality.
21
The tactile matrix ( 80√ó30 ) is transformed into an embedding using a Conv2D module, and the
proprioceptive vector (3-dimensional) is transformed into an embedding using a MLP. We combine
the two embeddings via element-wise addition to yield the final integrated observation embedding ot.
To decode from the embedding back to the original sensations, we use a ConvTranspose2D for tactile
and a MLP for proprioception.
A.2.2 RSSM Computation
The deterministic stateh t is computed as follows:
1. Encode previous stochastic state st‚àí1 and action at‚àí1 into a single vector via a linear layer
2. Feed the encoded vector together with previous deterministic state ht‚àí1 into a GRU
cell (Chung et al., 2014)
3. The GRU cell output becomes the new deterministic stateh t
The stochastic prior pœï(ÀÜst |h t) passes ht through a MLP to output mean and variance parameters of
a multivariate Gaussian, from which the stochastic state is sampled.
The stochastic posterior qœï(st |h t, ot) receives a concatenated vector of deterministic state ht and
embedded observation ot. The concatenated vector is passed through a MLP to output mean and
variance parameters of a multivariate Gaussian, from which the stochastic state is sampled.
The likelihood pœï(ot |h t, st) transforms the concatenated ht and st into an embedded ot through a
linear layer, which is then decoded into individual modality-specific observations.
All parameters œï constituting this RSSM structure are optimized via gradient descent to minimize the
following variational free energy:
F=E q(st)[logq(s t)‚àílogp(o t, st)]
=E q(st)[logq(s t)‚àílogp(s t)]‚àíE q(st)[logp(o t |s t)]
=D KL[q(st)‚à•p(st)]‚àíE q(st)[logp(o t |s t)]
‚à¥arg min
œï
F= arg min
œï

DKL[qœï(st |h t, ot)‚à•pœï(st |h t)]‚àíE qœï(st|ht,ot)[logp œï(ot |h t, st)]

(18)
A.2.3 Policy and Value Network Training
Both the policy network qŒ∏(at |s t) and utility (value) network gœà(st) take the stochastic state st as
input, use a MLP to output multivariate Gaussian parameters, and sample from this distribution.
The training objectives are:
arg min
Œ∏
Lpolicy = arg min
Œ∏
X
t
GŒª
t
arg min
œà
Lutility = arg min
œà
X
t
(gœà(st)‚àíG Œª
t )
GŒª
t =G(s t) +Œ≥t
(1‚àíŒª)g œà(st+1) +ŒªGŒª
t+1,ift < H
gœà(sH),ift=H
(19)
Here, GŒª
t is the GAE(Œª)-estimated expected utility approximation. Œ≥t is the discount factor and H is
the simulation horizon.
A.3 Hyperparameters for Continuous Environment Experiments
Table 3 presents all hyperparameters used in the continuous environment experiments.
22
Table 3: Hyperparameters for continuous environment experiments
Parameter Value Description
Environment & Data Collection
Episode length (L)1000Time steps per episode
Initial random episodes100Random episodes before training
Exploration noise0.3Noise added to policy during training
Training Schedule
Replay buffer size450Max episodes stored in memory
Training epochs per episode100Training iterations after each episode
Batch size (B)50Number of trajectories sampled per epoch
Trajectory length (L)50Time steps per sampled trajectory
Planning horizon (H)15Future prediction horizon for policy learning
Gradient clipping100.0Maximum gradient norm
Network optimizer AdamW Optimizer for training
Observation Embedding
CNN encoder channels4,8,16,16Output channels for each encoder layer
CNN decoder channels16,8,4,3Output channels for each decoder layer
CNN encoder kernel sizes4,4,4,2Kernel size for each encoder layer
CNN decoder kernel sizes2,4,4,4Kernel size for each decoder layer
CNN stride2Stride value for all layers
MLP hidden layer size32Units in MLP hidden layer
MLP hidden layers2Number of hidden layers in MLP
Integrated embedding size64Final embedding dimension
Activation layer ELU Activation layer
World Model (RSSM)
Deterministic state size (dim(ht))200Dimension of GRU hidden state
Stochastic state size (dim(st))30Dimension of stochastic latent variable
Hidden layer size200Units in hidden layers
Number of hidden layers1Number of hidden layers
Activation layer ELU Activation layer
Learning rate (Œ±œï)10 ‚àí3 Learning rate for world model parameters
Free nats3.0Minimum threshold for KL divergence
KL scale (Œ≤)2.0KL divergence weight (Œ≤-V AE)
Policy & Value Networks
Hidden layer size200Units in hidden layers
Number of hidden layers3Number of hidden layers
Activation layer ELU Activation layer
Learning rate (Œ±Œ∏, Œ±œà)10 ‚àí3 Learning rate for parameters
Entropy temperature10 ‚àí4 Entropy regularization coefficient
Discount factor (Œ≥)0.99Discount rate for future rewards
GAE lambda (Œª)0.95Smoothing parameter for GAE estimation
Self-Prior (Neural Spline Flow)
Number of spline transforms3Number of transform layers in the flow
Hidden layer size64Units in spline network hidden layers
Activation layer ReLU Activation layer
Learning rate (Œ±Œæ)10 ‚àí3 Learning rate for self-prior parameters
23