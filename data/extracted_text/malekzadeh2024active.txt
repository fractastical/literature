1
Active Inference and Reinforcement Learning: A unified infer-
ence on continuous state and action spaces under partial observabil-
ity
Parvin Malekzadeh
p.malekzadeh@mail.utoronto.ca
Konstantinos N. Plataniotis
The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, Uni-
versity of Toronto, Toronto, ON, M5S 3G8, Canada.
Keywords: Active inference; Expected free energy; Partially observability; Policy;
Reinforcement learning
Abstract
Reinforcement learning (RL) has garnered significant attention for developing decision-
making agents that aim to maximize rewards, specified by an external supervisor, within
fully observable environments. However, many real-world problems involve partial
or noisy observations, where agents cannot access complete and accurate informa-
tion about the environment. These problems are commonly formulated as partially
observable Markov decision processes (POMDPs). Previous studies have tackled RL
in POMDPs by either incorporating the memory of past actions and observations or by
inferring the true state of the environment from observed data. Nevertheless, aggre-
gating observations and actions over time becomes impractical in problems with large
decision-making time horizons and high-dimensional spaces. Furthermore, inference-
based RL approaches often require many environmental samples to perform well, as
they focus solely on reward maximization and neglect uncertainty in the inferred state.
Active inference (AIF) is a framework naturally formulated in POMDPs and directs
agents to select actions by minimizing a function called expected free energy (EFE).
This supplies reward-maximizing (or exploitative) behaviour, as in RL, with information-
This preprint has been accepted by Neural Computation.
arXiv:2212.07946v3  [cs.LG]  31 May 2024
seeking (or exploratory) behaviour. Despite this exploratory behaviour of AIF, its usage
is limited to problems with small time horizons and discrete spaces, due to the computa-
tional challenges associated with EFE. In this paper, we propose a unified principle that
establishes a theoretical connection between AIF and RL, enabling seamless integration
of these two approaches and overcoming their aforementioned limitations in continuous
space POMDP settings. We substantiate our findings with rigorous theoretical analysis,
providing novel perspectives for utilizing AIF in designing and implementing artifi-
cial agents. Experimental results demonstrate the superior learning capabilities of our
method compared to other alternative RL approaches in solving partially observable
tasks with continuous spaces. Notably, our approach harnesses information-seeking ex-
ploration, enabling it to effectively solve reward-free problems and rendering explicit
task reward design by an external supervisor optional.
1 Introduction
Decision-making is the process of evaluating and selecting a course of action from
various alternatives based on specific criteria or goals. This process occurs within an
environment where an agent interacts and influences its state through actions. When
making decisions, 1 the agent carefully assesses available actions and their potential
outcomes. Once a decision is made, the agent translates it into action by executing a
specific course of action. The effectiveness of a decision is determined by examining
its outcomes, which can involve achieving desired results or mitigating undesirable
consequences (Puterman, 2014).
Reinforcement learning (RL) (Sutton & Barto, 2018) is a framework that models
agents to interact with an environment typically represented as a Markov decision pro-
cess (MDP), where the agent has complete and accurate observation of the true state
of the environment. In RL, outcomes are often associated with rewards designed by
an external supervisor and received by the agent based on its actions. The main objec-
tive of RL is to learn optimal policies that define an agent‚Äôs decision-making strategy to
maximize a value function, which represents the expected long-term reward. Recent ad-
vancements in using deep neural networks (DNNs) as parametric function approxima-
tors enabled RL algorithms to successfully solve tasks with continuous state and action
spaces (Haarnoja, Zhou, Abbeel, & Levine, 2018; Schulman, Wolski, Dhariwal, Rad-
ford, & Klimov, 2017; Dai, Du, Fang, & Bharath, 2022). Among the popular RL meth-
ods designed for continuous state and action spaces, actor-critic methods (Haarnoja,
Zhou, Abbeel, & Levine, 2018; Mnih et al., 2016; Schulman et al., 2017) have gained
1We use the terms decision-making and action selection interchangeably throughout the work.
significant attention. These methods employ a policy iteration algorithm to learn the
optimal policy. However, in many real-world scenarios, the true and complete state of
the environment is often inaccessible, leading to a situation known as learning under
uncertainty or partial observability. Partial observability can arise from various sources,
such as temporary information like a way point sign in a navigation task, sensor limita-
tions, noise, and a limited view of the environment. Consequently, the agent must select
optimal actions based on incomplete or noisy information about the environment‚Äôs true
(hidden or latent) states. Such environments can be formulated as partially observ-
able Markov decision processes (POMDPs). Unfortunately, identifying the objectives
on which an optimal policy is based for POMDPs is generally undecidable (Madani,
Hanks, & Condon, 1999) because partial and noisy observations lack the necessary
information for decision-making.
RL algorithms typically assume complete and accurate observation of the environ-
ment‚Äôs states, which limits their performance on partially observable tasks. To address
this, various approaches have been proposed to extend RL methods for solving partially
observable tasks. One popular approach is the use of memory-based methods, where
recurrent neural networks (RNNs) are employed to remember past observations and ac-
tions (Zhu, Li, Poupart, & Miao, 2017; Nian, Irissappane, & Roijers, 2020; Haklidir
& Temeltas ¬∏, 2021; Ni, Eysenbach, & Salakhutdinov, 2022). However, these methods
may become impractical when dealing with large dimensions of actions and/or obser-
vations, or when the decision-making time horizon is extensive, due to the demands of
maintaining long-term memory. Consequently, these approaches are mostly applicable
to environments with finite and countable action and observation spaces. Addition-
ally, training an RNN is more challenging than training a feed-forward neural network
since RNNs are relatively more sensitive to the hyperparameters and structure of the
network (Pascanu, Mikolov, & Bengio, 2013). To address these issues, some recent
work (Ramicic & Bonarini, 2021; Igl, Zintgraf, Le, Wood, & Whiteson, 2018; Lee,
Nagabandi, Abbeel, & Levine, 2020; Han, Doya, & Tani, 2020; Hafner, Lillicrap, Ba,
& Norouzi, 2020) has proposed inferring the belief state, which is a representation of
the hidden states given the past observations and actions. From the inferred belief state,
an optimal policy can be derived to maximize the expected long-term reward. How-
ever, due to the partial observability and limitations of the available observations, the
agent cannot accurately determine the exact hidden state with certainty. As a result,
the inferred belief state is often represented as a probability distribution, indicating the
likelihood of different hidden states given past observations and actions.
Given the uncertainty surrounding the inferred state, it is crucial for the agent not to rely
solely on its existing knowledge (exploitation). Instead, in a partially observable envi-
ronment, the agent should actively engage in an information-directed interaction with
the environment, aiming to minimize uncertainty and maximize information about the
true state of the environment, given only partial or noisy observations of states (Mavrin,
Yao, Kong, Wu, & Yu, 2019; Dong, Zhang, Liu, Zhang, & Shen, 2021; Likmeta,
Sacco, Metelli, & Restelli, 2022; Maddox, Izmailov, Garipov, Vetrov, & Wilson, 2019;
Malekzadeh, Salimibeni, Hou, Mohammadi, & Plataniotis, 2022; Yin, Chen, Pan, &
Tschiatschek, 2021; Malekzadeh, Salimibeni, Mohammadi, Assa, & Plataniotis, 2020).
Active inference (AIF) (K. Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo,
2017; K. J. Friston, Daunizeau, Kilner, & Kiebel, 2010) is a framework derived from the
free energy principle that models and explains the decision-making behaviour of agents
in environments modelled as POMDPs. AIF optimizes two complementary objective
functions: variational free energy (VFE) and expected free energy (EFE). The VFE ob-
jective is minimized with respect to past interactions with the environment, enabling the
agent to learn a generative model of the environment and infer the belief state used for
action selection. This process of learning the generative model and inferring the belief
state is called perceptual learning and inference. Action selection involves minimizing
the EFE objective with respect to the future, aiming to find an optimal plan‚Äîsequences
of actions.2 By minimizing the EFE, AIF simultaneously maximizes the agent‚Äôs infor-
mation gain about the hidden states (exploration) and optimizing its expected long-term
reward (exploitation). The information gain represents the enhancement in the agent‚Äôs
knowledge of the environment, allowing it to make more informed decisions as it in-
teracts with the world. Notably, AIF agents select actions that maximize the expected
long-term reward, akin to classical RL, while maximizing information about the envi-
ronment‚Äôs hidden states.
While AIF is naturally modelled in POMDPs and provides intrinsic information-directed
exploration, its applications are limited to environments with discrete state, observation,
and action spaces due to computational issues with the EFE (Millidge, 2020; Da Costa,
Sajid, Parr, Friston, & Smith, 2023; Lanillos et al., 2021; Sajid, Tigas, Zakharov, Foun-
tas, & Friston, 2021). This limitation arises because AIF finds the optimal plan by
computing the EFE for each possible plan and subsequently selecting the plan that min-
imizes the EFE (Tschantz, Baltieri, Seth, & Buckley, 2020). Recent studies have pro-
posed approaches to calculate the EFE in a more tractable manner. These approaches
include limiting the future decision-making time horizon (Tschantz et al., 2020), em-
ploying bootstrapping estimation techniques (Millidge, 2020; Hafner et al., 2022), and
utilizing Monte Carlo tree search (MCTS) methods (Maisto, Gregoretti, Friston, & Pez-
2In this paper, we will use the term ‚Äòplan‚Äô to denote a sequence of actions, distinguishing it from a
(state-action) policy typically defined in RL.
zulo, 2021; Fountas, Sajid, Mediano, & Friston, 2020). However, it‚Äôs important to note
that these methods are (partially) heuristic and typically applicable only to finite time
horizons or discrete action spaces.
Considering the recent advancements in scaling RL for complex, fully observable
environments with continuous state and action spaces, and acknowledging the capa-
bility of AIF to perform belief state inference and information-seeking exploration in
partially observable environments, a compelling question emerges: Is there a relation-
ship between AIF and RL that enables the development of integrated solutions, lever-
aging ideas from both fields and surpassing the limitations of individual AIF or RL ap-
proaches? In this paper, we propose ‚Äùunified inference‚Äù‚Äîa convergence of AIF and RL
on a common theoretical ground‚Äîto harness the mutual advantages of both paradigms
and address the challenges posed by realistic POMDP settings with high-dimensional
continuous spaces.
The key contributions of our proposed unified inference framework are summarized as
follows:
‚Ä¢ Extension of EFE to stochastic belief state-action policy: We extend the EFE,
originally defined for plans (K. Friston et al., 2017) in AIF, to accommodate the
learning of a stochastic policy in an infinite time horizon POMDP setting with
continuous state, action, and observation spaces. This extension of the EFE to a
belief state-action policy allows actions to be chosen at each time step based on
the inferred belief state, eliminating the need to enumerate every possible plan
into the future. This extension integrates information-seeking exploration and
reward-maximization under partial observability learning, making it a practical
and effective objective function for action selection in both AIF and RL frame-
works within POMDP settings with continuous spaces. We hence refer to this
extension as the unified objective function. Our experiments demonstrate that
introducing stochasticity to the policy significantly improves the stability and ro-
bustness of our algorithm in tasks with continuous spaces, where challenges such
as belief state inference and hyperparameter tuning pose significant obstacles.
‚Ä¢ Unified policy iteration in continuous space POMDPs: To optimize the pro-
posed unified objective function and find the optimal policy, we introduce a com-
putationally efficient algorithm called unified policy iteration. This algorithm
generalizes the policy iteration guarantees in MDPs (Haarnoja, Zhou, Abbeel, &
Levine, 2018) to POMDPs and provides theoretical proof of its convergence to
the optimal policy. Within the unified policy iteration framework, the extended
EFE can be treated as a negative value function from an RL perspective. This
theoretical connection demonstrates three important aspects: (i) AIF can be an-
alyzed within the framework of RL algorithms, enabling insights from scaling
RL to infinite time horizons and continuous space MDPs to be directly applied to
scaling AIF for use in infinite time horizons and continuous space POMDPs. This
provides deep learning practitioners with a starting point to leverage RL findings
and further advance AIF in challenging tasks. (ii) We can generalize a range
of RL approaches designed for continuous state and action MDPs to continuous
state, action, and observation space POMDPs while incorporating an inherent
information-seeking exploratory term. This bridges the gap between MDPs and
POMDPs and also opens up new possibilities for applying state-of-the-art RL
techniques to challenging decision-making problems. (iii) We can extend a range
of reward-dependent RL methods to a setting where the reward function is not de-
termined by an external supervisor. This aspect of the proposed policy iteration is
significant as it eliminates the challenges associated with designing reward func-
tions. For instance, a poorly designed reward function may lead to slow learning
or even convergence to sub-optimal policies, highlighting the importance of mit-
igating the necessity of defining task rewards through unified policy iteration.
‚Ä¢ Unified actor-critic for continuous space POMDPs: Building upon the para-
metric function approximations in the unified policy iteration, we present a novel
unified actor-critic algorithm that unifies actor-critic methods for both MDPs and
POMDPs. Our approach stands out as one of the few computationally feasi-
ble methods for addressing POMDPs and opens new possibilities for enhancing
the performance of the popular reward-maximizing actor-critic RL algorithms,
such as soft actor-critic (SAC) (Haarnoja, Zhou, Abbeel, & Levine, 2018) and
Dreamer (Hafner et al., 2020) when applied to real-world scenarios that inher-
ently involve partial observability. To evaluate the effectiveness of our unified
actor-critic algorithm in addressing high-dimensional continuous space POMDPs,
we conduct experiments on modified versions of Roboschool tasks (Brockman et
al., 2016), where agents have access to partially observed and noisy states. The
experimental results demonstrate that our proposed unified actor-critic method
achieves superior sample efficiency and asymptotic performance compared to ex-
isting frameworks in the literature.
A comparative overview of foundational elements and decision-making strategies within
RL, AIF, and our proposed unified inference approach, with a particular focus on their
application in continuous decision spaces, is presented in Table 1. This summary encap-
sulates the methodologies employed by these frameworks, highlighting their distinctive
Table 1: Comparative overview of foundational and decision-making aspects in RL,
AIF, and unified inference under continuous decision spaces.
Aspect RL AIF Unified Inference
Decision Space Continuous
MDP
Continuous
POMDP
Continuous
POMDP
Foundation for
Decision-Making N/A Perceptual inference
& learning via VFE
Perceptual inference
& learning via VFE
Decision-Making
Strategy
State-action
policy learning Plan learning Belief state-action
policy learning
Decision-Making
Objective
Value function
(reward
maximization)
EFE (reward +
information gain
maximization)
Unified objective
(reward +
information gain +
policy entropy
maximization)
Optimization
Mechanism Policy iteration Enumeration Unified policy
iteration
Learning
Approach Actor-critic Bootstrapping,
MCTS Unified actor-critic
approaches to learning and decision-making in environments characterized by continu-
ous decision spaces.
1.1 Overview
In Section 2, we present a comprehensive introduction to the problem‚Äôs nature, which
serves as the foundation for our study. Section 3 offers an introductory tutorial on
POMDPs and covers essential concepts like policies, generative models, and inference.
We review the RL paradigm for MDPs and the AIF paradigm for POMDPs in Sec-
tion 4. Section 5 introduces the proposed unified inference framework and establishes
its convergence to the optimal policy. In Section 6, we delve into the implementa-
tion and modeling aspects of the proposed unified inference, demonstrating how our
formulation extends various existing MDP-based actor-critic methods directly to their
respective POMDP cases. Section 7 discusses existing works related, and Section 8
presents the experiments utilized to evaluate the performance of our method. Finally,
we conclude the paper and outline potential avenues for future research in Section 9.
2 Problem characteristics
This section outlines our assumptions concerning various key elements of the decision-
making problem addressed in this paper.
1. Finite or infinite horizon: We assume agents interact with environments over
an infinite horizon, appropriate in scenarios where decisions have long-term im-
pacts. Infinite horizon problems, requiring analysis of endless action sequences,
are inherently more complex than their finite counterparts.
2. Fully or partially observable: We tackle sequential decision-making in partially
observable environments. Partial observability is prevalent in many real-world
tasks and challenges decision-making compared to fully observable settings (Igl
et al., 2018).
3. Discrete or continuous state, observation, and action spaces: We consider envi-
ronments with continuous state, observation, and action spaces. These continuous
spaces are common in complex applications, as they enhance the representation
of the environment and actions, and better handle complexity.
4. Stochastic or deterministic environment: We consider stochastic environments
introducing randomness in action outcomes. Unlike deterministic environments,
stochastic environments require strategies that account for outcome variability,
enhancing decision robustness and capturing real-world complexity.
5. Stationary or non-stationary environment: We assume stationary environments
with consistent statistical properties, a common simplification to manage the
computational complexity of non-stationary environments (Sutton & Barto, 2018;
Puterman, 2014). While focused on stationary settings, the methodologies pro-
posed in this paper also apply to non-stationary environments.
6. Markovian or non-Markovian policy: We focus on Markovian policies that rely
solely on the current belief state, streamlining computation and memory com-
pared to non-Markovian (history-dependent) policies.
7. Stochastic or deterministic policy: We adopt stochastic policies for their en-
hanced ability to explore environments and handle uncertainties, facilitating adap-
tive responses. These features promote better generalization and task-specific
fine-tuning (Ramicic & Bonarini, 2021; Haarnoja, Zhou, Abbeel, & Levine, 2018).
Table 2 provides a comprehensive comparison between the key characteristics of
our problem and the problems addressed by most RL and AIF paradigms. This com-
parison covers aspects such as the observability of environment states, decision-making
horizon, stochastic properties of the environment, and policy class. By analyzing these
elements in comparison to typical RL and AIF problems, we gain insights into the dis-
tinct nature and unique challenges of our problem setting.
Table 2: Comparison of our problem setting with common problem settings addressed
by RL and AIF algorithms.
Method Horizon Observ-
ability
State &
observation
space
Action
space
Environ-
ment Policy
RL Finite/
Infinite Full Discrete/
Continuous
Discrete/
Continuous
Stochastic
&
Stationary
Markovian &
Stochastic
AIF Finite Partial Discrete Discrete
Stochastic
&
Stationary
Non-
Markovian &
Deterministic
Ours Infinite Partial Continuous Continuous
Stochastic
&
Stationary
Markovian &
Stochastic
3 Preliminaries and Problem modeling
In the previous section, we outlined the characteristics of the problem addressed in this
paper, involving an agent making decisions within a stochastic environment with con-
tinuous state, observation, and action spaces. This section delves into modeling these
characteristics, beginning with an overview of relevant concepts including POMDPs,
policies, generative models, and belief state inference, and then detailing the specific
models applied to our problem.
We assume the agent receives observations and makes decisions at discrete time steps
t, continuing indefinitely, where t ‚àà {0, 1, 2, . . .}. For notation, xt represents vari-
able x at time step t, and xf:h includes all elements from t = f to t = h, i.e.,
xf:h = ( xf , xf+1, ..., xh). Additionally, ‚àÜ(X) denotes the set of all probability dis-
tributions3 over set X, and |X| represents its cardinality. For further clarification, a
comprehensive list of all notations used is provided in Table D.1 in the appendix.
3.1 Partially observed Markov decision processes (POMDPs)
A POMDP is fully specified by M = (S, O, A, d0, ‚Ñ¶, R, U,Œò), where S ‚ààRDS rep-
resents the state space encompassing all possible (hidden or latent) states of the envi-
ronment. Here, R denotes the set of real numbers, and DS is the dimension of the state
space. O ‚ààRDO denotes the observation space, the set of all possible observations
the agent can receive, with DO as the dimension of the observation space. A ‚ààRDA
3We use the probability distribution function for both the probability mass function (PMF) and prob-
ability density function (PDF).
is the action space, indicating all possible actions the agent can take, where DA is the
dimension of action space. These spaces can be either discrete or continuous. At time
step t, st, ot, and at denote elements from S, O, and A, respectively.
d0 : S ‚Üí‚àÜ(S) denotes the probability distribution of the initial latent state, withd0(s0)
specifying the probability of the environment starting in state s0 ‚àà S. ‚Ñ¶ : S √ó A ‚Üí
‚àÜ(S) is the (forward) transition function, such that ‚Ñ¶(st+1|st, at) specifies the proba-
bility of transitioning to st+1 ‚àà Sfrom st after acting at.
R is the set of possible rewards and is called the reward space. We will usert to denote
an element of the reward setR at time t. Adhering to RL and AIF standards (K. Friston
et al., 2017; Sutton & Barto, 2018), we assume that R ‚ààR, although more general
vector-valued reward functions are also possible. U : S √ó A ‚Üí‚àÜ(R) is the reward
function, such that U(rt|st, at) indicates the probability of a reward value rt given state
st and action at. The reward function U is also known as the external (or extrinsic)
reward function since its value is determined by an external supervisor or designer.4
Œò : S ‚Üí‚àÜ(O) is the observation function, such that Œò(ot|st) specifies the probabil-
ity of observing ot given state st. This distinguishes POMDPs from MDPs, where the
agent indirectly observes st through ot. In MDPs, observations directly reflect the true
state, making ot = st. A POMDP is deemed deterministic if ‚Ñ¶, U, and Œò are all de-
terministic. Otherwise, it is considered stochastic. Furthermore, the POMDP is termed
stationary if these functions remain constant over time and non-stationary if they vary.
Following assumptions 2-5 in Section 2, we represent our environment as a station-
ary stochastic POMDP with continuous state, observation, and action spaces.
3.2 Policy
A policy is a sequence of decision rules denoted by œÄ0:‚àû = (œÄ0, œÄ1, œÄ2, ..., œÄt, ...). Each
decision rule œÄt models how an agent selects action at ‚àà Aat time step t. To identify
an optimal policy, a specific optimality criterion is defined.
A Markovian decision ruleœÄt(at|ot) operates solely on the current observationot, whereas
a history-dependent decision rule œÄt(at|a0:t‚àí1, o0:t) utilize the complete history of ac-
tions a0:t‚àí1 and observations o0:t‚àí1 until time t. Decision rules can be deterministic,
selecting actions with certainty, or stochastic, introducing probability into the selection
process (Puterman, 2014).
The distinctions between Markovian, history-dependent, deterministic, and stochas-
tic decision rules give rise to corresponding classes of policies.
4In this work, we interchange the terms reward function and extrinsic reward function when referring
to U.
Moreover, a policy œÄ0:‚àû is considered stationary if the decision rule œÄt remains con-
stant for all time instants t ‚àà {0, 1, 2, . . .}. Otherwise, it is called non-stationary. In
the case of a stationary POMDP with an infinite time horizon, it is common to assume
a stationary policy (Puterman, 2014; Russell, 2010). This assumption arises from the
understanding that at each time step, an infinite number of future time steps exist, and
the statistical properties of the environment remain unchanged. Adopting a stationary
policy can simplify the agent‚Äôs decision-making process.
In an MDP where ot = st and states exhibit the Markovian property, all policies
are Markovian (Puterman, 2014). A Markovian policy within an MDP selects action
at based solely on the current state, denoted as œÄ(at|st), and is thus referred to as a
state-action policy. Conversely, in POMDPs where observations lack the Markovian
property, history-dependent policies may outperform Markovian ones by enabling more
informed decisions (Igl et al., 2018; Montufar, Ghazi-Zahedi, & Ay, 2015; Puterman,
2014). However, each time step in POMDPs introduces one new action and one new
observation to the history, adding DO + DA-dimensional data to the existing history.
Furthermore, the addition of one observation and one action to the history at each time
step leads to an exponential increase in the number of possible histories. Specifically,
the number of possible histories expands at each step by a factor of(|‚Ñ¶| √ó |A|), reflect-
ing the agent‚Äôs |A| possible actions and |‚Ñ¶| possible observations. Consequently, the
size of the policy space, which represents the number of possible policies derived from
these histories, also undergoes the same exponential growth.
To tackle these challenges, certain methods (Igl et al., 2018; Kochenderfer, 2015; Mont-
ufar et al., 2015) employ belief state-action policies, where decisions at timet are based
on a belief state bt ‚àà B, i.e., œÄt(at|bt). Here, the belief state bt represents a probability
distribution over potential states of the environment at time t, and B ‚ààR|S| encom-
passes all possible belief states. Derived using Bayes‚Äô rule through a process known as
inference, the belief state consolidates all pertinent historical information necessary for
action selection, effectively rendering belief state-action policies Markovian.
Unlike history-dependent policies, belief states in belief state-action policies main-
tain a constant size, |S|, at each time step. Consequently, the memory and com-
putation required to store and evaluate each belief state and its corresponding policy
does not inherently increase over time, providing significant efficiency advantages over
history-dependent approaches, particularly in environments with infinite horizons or
high-dimensional action and observation spaces. However, inferring and storing belief
states can be computationally and memory-intensive, particularly in large or continuous
state spaces. Nevertheless, this memory and computational requirement generally falls
below that of history-dependent policies (Yang & Nguyen, 2021). Further details com-
paring the complexity of history-based versus belief state-action policies are provided
in Appendix A.
3.3 Generative model
With policy œÄ and initial state distribution d0, interactions between an agent and a sta-
tionary POMDP M gives the following sequence (s0, o0, a0, s1, o1, a1, . . .) with the
probability distribution:
p(s0, o0, a0, s1, o1, a1, ...|œÄ)5 = p(s0, o0, s1, o1, ...|a0:‚àû)
‚àûY
k=1
p(ak‚àí1|œÄ), (1)
where p(ak‚àí1|œÄ) denotes the probability that policy œÄ chooses action ak‚àí1. Given Œò
and ‚Ñ¶, p(s0, o0, s1, o1, ...|œÄ) in Eq. (1) simplifies to:
p(s0, o0, s1, o1, ...|a0:‚àû) = d0(s0)p(o0|s0)
‚àûY
k=1
p(ok|sk)p(sk|sk‚àí1, ak‚àí1)
= d0(s0)Œò(o0|s0)
‚àûY
k=1
Œò(ok|sk)‚Ñ¶(sk|sk‚àí1, ak‚àí1). (2)
As explained in Sub-section 3.2, the belief state is a variable estimated by the agent.
To infer the belief state using Bayesian inference (as detailed in Sub-section 3.4), the
agent relies on p(s0:‚àû, o0:‚àû|a0:‚àû) = p(s0, o0, s1, o1, . . .|a0:‚àû). However, the agent
lacks direct access to ‚Ñ¶ and Œò; thus, it constructs a generative model of the POMDP
denoted as P(s0:‚àû, o0:‚àû|a0:‚àû) = P(s0, o0, s1, o1, . . .|a0:‚àû), which is decomposed as
follows (Han et al., 2020; K. Friston et al., 2017; Lee et al., 2020): 6
P(s0:‚àû, o0:‚àû|a0:‚àû) = d0(s0)P(o0|s0)
‚àûY
k=1
P(ok|sk)P(sk|sk‚àí1, ak‚àí1), (3)
where P(ot|st) and P(st|st‚àí1, at‚àí1) are the agent‚Äôs models of observation function Œò
and the transition function ‚Ñ¶, respectively. These are referred to as the likelihood func-
tion or the observation model, and the transition model, respectively.
The generative model of an MDP is derived by settingP(ot|st) = 1 in the POMDP‚Äôs
model. Fig. 1 shows the dependency relationships in the generative models for POMDPs
5We abuse notation by writing p(.) for both the PMF and PDF. Moreover, we use p(xt) as shorthand
for p(Xt = xt), where Xt is a random variable at time t taking on values xt ‚àà X.
6We use P to represent the agent‚Äôs probabilistic model of the POMDP, which is a learned approxima-
tion, while p denotes the true probabilistic components of the POMDP. Since the agent is unaware of the
true probabilistic components, we solely refer to the agent‚Äôs generative model from now on.
Fig. 1. Relationship between generative models of MDPs and POMDPs. Arrows indi-
cate dependence.
and MDPs, illustrating how a POMDP‚Äôs model encompasses that of an MDP.
3.4 Inference
The belief state bt utilized by a Markovian belief state-action policy œÄ(at|bt) signifies a
probability distribution over the state spaceS at time t, where bt(st) = P(st|o0:t, a0:t‚àí1).
Through a generative model for the environment, the agent estimates bt, a process
known as inference (Igl et al., 2018). Inference is performed using Bayes‚Äô rule:
bt =
R
bt‚àí1(st‚àí1)P(ot|st)P(st|st‚àí1, at‚àí1)dst‚àí1R R
bt‚àí1(st‚àí1)P(ot|st)P(st|st‚àí1, at‚àí1)dstdst‚àí1| {z }
BeliefUpdate(bt‚àí1,st,ot,at‚àí1)
. (4)
Eq. (4) presents Bayes‚Äô rule for inference in POMDPs with continuous state and obser-
vation spaces. For discrete spaces, summations would be used instead. Given the belief
state bt‚àí1, an action at‚àí1, an observation ot, and the latent state st, the belief state bt is
fully determined via the recurrent update function BeliefUpdate. Consequently, the be-
lief state forward distribution P(bt|st, ot, bt‚àí1, at‚àí1), representing the distribution over
the subsequent belief state, is Œ¥(bt ‚àí BeliefUpdate(bt‚àí1, st, ot, at‚àí1)), where Œ¥ denotes
the Dirac delta function. Therefore, the belief state bt depends solely on the previous
belief state bt‚àí1, exhibiting the Markovian property.
In practice, exact inference often becomes computationally intractable in continuous
spaces due to the integrals involved. To address this, we use variational inference, a
common approximation technique in the RL and AIF literature (Hafner et al., 2020;
Lee et al., 2020; Millidge, 2020; Mazzaglia, Verbelen, & Dhoedt, 2021). Details on
variational inference are provided in Sub-section 4.2.1.
4 Review of the State-of-The-Art algorithms
RL and AIF are the primary frameworks for decision-making problems. RL algorithms
typically assume full observability and model environments as MDPs, whereas AIF
assumes partial observability and models them as POMDPs. This section overviews
their fundamental concepts and underscores their key distinctions.
Note 4.1: In the rest of this paper, we fix the current time t and assume that the
agent focuses on optimizing future actions for time steps œÑ ‚àà {t, t+ 1, ...}.
4.1 Reinforcement learning (RL)
As RL algorithms are formulated within MDPs, it is sufficient to consider a Markovian
state-action policy œÄ(aœÑ |sœÑ ) (Sutton & Barto, 2018). The goal of RL is to find an op-
timal state-action policy œÄ‚àó maximizing the expected long-term reward JœÄ, expressed
as JœÄ = EQ‚àû
œÑ=t P(sœÑ |sœÑ‚àí1,aœÑ‚àí1)P(rœÑ |sœÑ ,aœÑ )œÄ(aœÑ |sœÑ ) [P‚àû
œÑ=t Œ≥œÑ‚àítrœÑ ], where Œ≥ ‚àà [0, 1) is the dis-
count factor used to ensure the sum is bounded. RL employs various methods to find
the optimal policy œÄ‚àó. Three common categories relevant to our context are as follows:
‚Ä¢ Policy-based approaches: These algorithms directly find œÄ‚àó by maximizing JœÄ
using gradient ascent methods. Policy-based approaches have shown success
in dealing with continuous state and action spaces (Y . Ma, Zhao, Hatano, &
Sugiyama, 2016; Schulman, Levine, Abbeel, Jordan, & Moritz, 2015), but they
often suffer from high variance in gradient estimates (Tucker et al., 2018).
‚Ä¢ Value-based methods: Value-based algorithms rely on learning either the state
value function V (œÄ)(st) or the state-action value function Q(œÄ)(st, at), which are
obtained by conditioning JœÄ on the state St = st and the state-action pair (St =
st, At = at), respectively:
V (œÄ)(st) = EQ‚àû
œÑ=t P(sœÑ+1|sœÑ ,aœÑ )P(rœÑ |sœÑ ,aœÑ )œÄ(aœÑ |sœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àítrœÑ |st

, (5)
Q(œÄ)(st, at) = EQ‚àû
œÑ=t P(sœÑ+1|sœÑ ,aœÑ )P(rœÑ |sœÑ ,aœÑ )œÄ(aœÑ |sœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àítrœÑ |st, at

, (6)
Value-based algorithms calculateV (œÄ)(st) and Q(œÄ)(st, at) recursively using Bell-
man equations:
V (œÄ)(st) = EœÄ(at|st)P(rt|st,at)

rt + Œ≥EP(st+1|st,at)[V œÄ(st+1)]

, (7)
QœÄ(st, at) = EP(rt|st,at)[rt] + EP(st+1|st,at)œÄ(at+1|st+1)[QœÄ(st+1, at+1)]. (8)
Using the Bellman equations, RL iteratively updates these value functions using
the Bellman operators (Sutton & Barto, 2018). The Bellman operator for state-
action value function is denoted as TRL
œÄ and is defined as:
TRL
œÄ Q(st, at):=EP(rt|st,at)[rt] + EP(st+1|st,at)œÄ(at+1|st+1)[Q(st+1, at+1)]. (9)
The optimal policy is then foundœÄ‚àó = arg maxœÄ V (œÄ)(st) = arg maxœÄ Q(œÄ)(st, at).
It has been shown that the following Bellman optimality equations on the opti-
mal state value function V ‚àó(st+1) := V (œÄ‚àó)(st) and the optimal state-action value
function Q‚àó(st, at) := Q(œÄ‚àó)(st, at) hold (Sutton & Barto, 2018):
V ‚àó(st) = max
a‚ààA

EP(rt|st,a)[rt] + Œ≥EP(st+1|st,a)[V ‚àó(st+1)]

, (10)
Q‚àó(st, at) = EP(rt|st,a)[rt] + Œ≥EP(st+1|st,at)[max
a‚Ä≤‚ààA
Q‚àó(st+1, a‚Ä≤)]. (11)
Thus, value-based algorithms can calculate the optimal state value functionV ‚àó(st)
or state-action value function Q‚àó(st, at), and derive the optimal action a‚àó as
a‚àó = arg maxa Q‚àó(st, a) or a‚àó = arg maxa V ‚àó(st). However, when dealing with
large action spaces, using the max operator to select the best action can become
computationally expensive (Okuyama, Gonsalves, & Upadhay, 2018).
‚Ä¢ Actor-critic methods: These hybrid approaches blend policy-based and value-
based techniques. The policy, or actor, selects actions, while the critic estimates
the state or state-action value function, evaluating the actor‚Äôs decisions. Using
policy gradients, the actor improves its policy, while the critic assesses this en-
hanced policy by estimating its corresponding value function. This iterative pro-
cess, known as policy iteration, guarantees convergence to the optimal policy
under certain conditions with sufficient iterations. Additionally, policy iteration
is computationally efficient since it updates the policy at each iteration, making it
suitable for problems with continuous state and action spaces.
RL algorithms, including value-based, policy-based, and actor-critic methods, can be
either model-free or model-based, depending on how they learn the optimal policy.
Model-based algorithms learn a generative model of the MDP and a reward model, of-
fering improved sample efficiency (Hafner et al., 2020). Conversely, model-free meth-
ods, while not relying on explicit models, often require more environment interactions
for optimization. Recent approaches, like CVRL (X. Ma, Chen, Hsu, & Lee, 2021), in-
tegrate model-free and model-based components to leverage their respective strengths.
Despite RL‚Äôs advancements in continuous spaces, it primarily operates in fully ob-
servable environments. As such, its applicability in partially observable scenarios re-
mains limited (Da Costa et al., 2023; Han et al., 2020; Igl et al., 2018).
4.2 Active inference (AIF)
AIF has gained attention as a framework unifying inference and action selection under
the free energy principle in POMDPs (K. Friston et al., 2017; K. Friston, Mattout, &
Kilner, 2011). In AIF, the agent engages in inferring and learning a generative model
of the POMDP and then utilizes the inferred belief state and generative model to seek
an optimal plan, a sequence of future actions, that minimizes the EFE.
4.2.1 Perceptual inference and learning
AIF assumes that the agent has access to past observations and actions and models
the instant generative model as P(st, ot|at‚àí1, st‚àí1) (K. Friston et al., 2017; Millidge,
Tschantz, & Buckley, 2021). Upon receiving the observationot, AIF learnsP(st, ot|at‚àí1, st‚àí1)
and utilizes variational inference to approximate the belief state bt using the variational
posterior q(st|ot).7 This is achieved by minimizing the VFE at time t, denoted as Ft.
The VFE is an upper bound on the current Bayesian surprise, defined as ‚àílog p(ot),
where log represents the natural logarithm function. In machine learning, the VFE
corresponds to the negative Evidence Lower Bound (ELBO) within the framework of
Variational AutoEncoder (V AE) (Kingma & Welling, 2013).Ft is defined as follows:
Ft = Eq(st|ot) [logq(st|ot) ‚àí logP(st, ot|at‚àí1, st‚àí1)] . (12)
By factoring P(st, ot|at‚àí1, st‚àí1) into P(ot|st) and P(st|st‚àí1, at‚àí1), Ft can be rewritten as
Ft = ‚àíEq(st|ot) [logp(ot|st)] + DKL[q(st|ot), P(st|st‚àí1, at‚àí1)], (13)
where DKL is Kullback Leibler (KL)-divergence. By minimizing Ft, the agent ensures
that its generative model aligns with the received observations and the inferred states.
The process of inferring q(st|ot) by minimizing Ft is called perceptual inference, and
minimizing Ft with respect to the transition model P(st|st‚àí1, at‚àí1) and the likelihood
function P(ot|st) is known as perceptual learning in AIF (K. Friston et al., 2017).
7Throughout this paper, the notation q is used to represent variational probability distributions.
4.2.2 Plan selection
AIF assumes that the agent has a desired distribution ÀúP(oœÑ+1) over future observations
oœÑ for œÑ ‚àà {t, t+ 1, ..., T}, where T represents a finite time horizon. This desired dis-
tribution, known as the prior preference, encodes the agent‚Äôs goals.
Looking ahead to future time steps, the agent in AIF aims to minimize its expected
future Bayesian surprise based on its prior preference ÀúP(oœÑ+1). This is achieved by
minimizing the EFE given the current observation ot over all possible plans Àúa :=
at:T‚àí1 (Millidge et al., 2021). The EFE is denoted as GAIF(ot) and is defined as fol-
lows:
GAIF(ot) = Eq(st:T ,ot+1:T ,Àúa|ot)

log q(st+1:T , Àúa|st)
P(st+1:T , ot+1:T |st, Àúa)

, (14)
where the variational distribution q(st+1:T , Àúa) infers future states and actions and the
variational distribution q(st:T , ot+1:T , Àúa|ot) considers expectations for future observa-
tions. AIF factors q(st+1:T , Àúa|st) as q(st+1:T , Àúa|st) = QT‚àí1
œÑ=t q(Àúa)P(sœÑ+1|sœÑ , aœÑ ) and
approximates the generative modelP(st+1:T , ot+1:T |st, Àúa) in Eq. (14) with a biased gen-
erative model ÀúP(st+1:T , ot+1:T |st, Àúa), defined as follows:
Definition 4.1 (Biased generative model)
Given the prior preference ÀúP(oœÑ+1), the generative model P(st+1:T , ot+1:T |st, Àúa) in
Eq. (14) is approximated by a biased generative model ÀúP(st+1:T , ot+1:T |st, Àúa), which
factors as:
ÀúP(st+1:T , ot+1:T |st, Àúa) =
T‚àí1Y
œÑ=t
ÀúP(oœÑ+1|sœÑ , aœÑ )q(sœÑ+1|oœÑ+1). (15)
By replacing the generative model P(st+1:T , ot+1:T |st, Àúa) with the biased generative
model ÀúP(st+1:T , ot+1:T |st, Àúa), GAIF(ot) in Eq. (14) can be rewritten as:
GAIF(ot) = Eq(st:T ,ot+1:T ,Àúa|ot)
T‚àí1X
œÑ=t
log q(Àúa)P(sœÑ+1|sœÑ , aœÑ )
ÀúP(oœÑ+1|sœÑ , aœÑ )q(sœÑ+1|oœÑ+1)

. (16)
Focusing on deriving the optimal variational distribution q‚àó(Àúa) (Millidge et al., 2021;
Mazzaglia, Verbelen, C ¬∏ atal, & Dhoedt, 2022) that minimizes GAIF(ot), it has been
shown that q‚àó(Àúa) = œÉ

‚àíG(Àúa)
AIF(ot)

, where œÉ is the Softmax function and G(Àúat)
AIF (ot)
is the EFE for a fixed plan Àúa, defined as:
G(Àúa)
AIF(ot) =Eq(st:T ,ot+1:T |ot,Àúa)
T‚àí1X
œÑ=t
logP(sœÑ+1|sœÑ , aœÑ )
q(sœÑ+1|oœÑ+1) ‚àí log ÀúP(oœÑ+1|sœÑ , aœÑ )

. (17)
As q‚àó(Àúa) = œÉ

‚àíG(Àúa)
AIF(ot)

, the most probable plan, denoted as Àúa‚àó, corresponds to the
one that minimizes G(Àúa)
AIF(ot). Thus, Àúa‚àó is referred to as the optimal plan and can be
found as Àúa‚àó = arg minÀúa G(Àúa)
AIF(ot). AIF achieves this optimal plan by running its biased
generative model forward from the current time t to the time horizon T, generating
hypothetical future states and observations for all possible plans Àúa. It then selects the
plan Àúa‚àó that results in the minimum value of G(Àúa)
AIF(ot).
Note 4.2: It should be noted that AIF (K. Friston et al., 2017) chooses its actions
according to the optimal plan Àúa‚àó. This is in contrast to the optimal state-action policy
œÄ‚àó in RL, which maps states to probability distributions over the action space.
According to the complete class theorem in AIF (K. Friston, Samothrakis, & Mon-
tague, 2012), EP(rœÑ |sœÑ ,aœÑ )[rœÑ ] can be encoded as log ÀúP(oœÑ+1|sœÑ , aœÑ ). Therefore, G(Àúa)
AIF(ot)
in Eq. (17) can be rewritten as
G(Àúa)
AIF(ot) = ‚àíEQT‚àí1
œÑ=t q(sœÑ |oœÑ ) ÀúP(oœÑ+1|sœÑ ,aœÑ )P(rœÑ |sœÑ ,aœÑ )
"T‚àí1X
œÑ=t
rœÑ
#
| {z }
Expected long-term reward
‚àíEQT‚àí1
œÑ=t q(sœÑ |oœÑ ) ÀúP(oœÑ+1|sœÑ ,aœÑ )
"T‚àí1X
œÑ=t
DKL[q(.|oœÑ+1), P(.|sœÑ , aœÑ )
#
.
| {z }
Expected information gain
(18)
The first term in Eq. (18) represents the expected long-term (extrinsic) reward, which is
known as the goal-directed term in AIF (Millidge et al., 2021). The second term, KL-
divergence between the variation posterior q(.|oœÑ ) and the transition model P(.|sœÑ , aœÑ ),
represents the expected information gain, referred to as the epistemic (intrinsic) value,
which measures the amount of information acquired by visiting a particular state. By
following the optimal plan Àúa‚àó that minimizes G(Àúa)
AIF(ot), the agent indeed maximizes the
expected long-term extrinsic reward (exploitation) while reducing its uncertainty and
maximizing the expected information gain (exploration) about the hidden states.
In this section, we have explored the strengths and weaknesses of RL and AIF al-
gorithms. RL excels in fully observable problems with high-dimensional continuous
spaces, utilizing policy iteration and function approximations like DNNs. However, it
struggles with partial observability and relies heavily on well-defined extrinsic reward
signals. As a result, RL can face difficulties when the extrinsic reward function is absent
or provides sparse or zero rewards from the environment. In contrast, AIF addresses
partial observability by integrating belief state inference and information-seeking ex-
ploratory behavior, which is often lacking in traditional RL algorithms (Haarnoja, Zhou,
Abbeel, & Levine, 2018; Okuyama et al., 2018). The information gain term allows AIF
agents to learn even without explicitly defined extrinsic reward functions, as any plan
inherently possesses intrinsic value. This aspect of AIF is particularly valuable when
designing appropriate reward functions is challenging. However, the computational
burden of considering all possible plans limits many AIF methods to discrete spaces or
finite horizon POMDPs. Combining RL and AIF can leverage their respective strengths,
offering more effective decision-making in POMDPs. The following section introduces
a unified framework that merges RL and AIF, addressing partially observable problems
with infinite time horizons and continuous spaces.
5 Unified inference integrating AIF and RL in continu-
ous space POMDPs
In this section, we present a unified inference framework that combines the strengths
of AIF and RL. This framework enables an agent to learn an optimal belief state-action
policy in an infinite time horizon POMDP with continuous spaces, while promoting
exploration through information-seeking. The proposed unified inference approach in-
troduces a unified objective function that formulates the action selection problem and
establishes the optimality criteria for the policy. This unified objective function com-
bines the objective functions of both the AIF and RL frameworks within the context
of a POMDP with continuous state and action spaces. Next, we show that the unified
objective function obeys the so-called unified Bellman equation and the unified Bell-
man optimality equation. These equations generalize the standard Bellman equation
and Bellman optimality equation from MDPs (i.e., Eqs. (7), (8), (10), and (11)) to the
broader class of POMDPs. By utilizing these unified Bellman equations, we derive a
unified policy iteration framework that iteratively optimizes the proposed unified objec-
tive function.
The proposed unified policy iteration serves two main purposes. Firstly, it extends the
guarantees of policy iteration from MDPs (Puterman, 2014) to POMDPs, allowing us to
apply insights from MDP-based RL algorithms, such as actor-critic algorithms, in the
context of POMDP-based AIF. This extension enables the generalization of these MDP-
based algorithms to POMDPs and facilitates computationally feasible action selection
through AIF in problems with continuous spaces and an infinite time horizon. Secondly,
this approach enables us to leverage recent breakthroughs in RL methods that rely on
extrinsic rewards and apply them to tasks without pre-specified reward functions. By in-
corporating this capability, we can address a broader range of problem domains beyond
those limited to explicit reward-based learning.
Prior to detailing the unified objective function, we must outline assumptions re-
garding our decision-making model. To establish the foundational formulation of the
performance criterion in a POMDP with continuous state, action, and observation spaces,
it is necessary to consider specific regularity assumptions outlined by Puterman (2014),
known as the POMDP regularity.
Assumption 5.1 (POMDP regularity)
(A1) The state space S is a compact set in RDS .
(A2) The action space A is a compact set in RDA.
(A3) The observation space O is a compact set in RDO.
(A4) The extrinsic reward space R is a compact set in R
(A5) The transition function ‚Ñ¶, observation function Œò, and reward function U are
Lipschitz continuous.
Appendix C provides detailed explanations of each regularity assumption. These condi-
tions enable us to focus on deriving an objective function as an optimality criterion for
an optimal policy. However, as discussed in Sub-section 3.2, to mitigate the high mem-
ory requirement associated with history-dependent policies, we adopt a belief state-
action policy œÄ(at|bt), requiring the agent to infer the belief state bt before selecting an
action at time step t. In line with common practices in RL and AIF approaches applied
to POMDPs (von Helmholtz, 2001; Hafner et al., 2020; Millidge et al., 2021; Han et
al., 2020), we assume that the agent performs perceptual inference and learning prior to
action selection at time step t as follows:
Assumption 5.2 (Perceptual inference and learning)
Prior to action selection at time step t, given the current observation ot, the agent per-
forms inference by approximating its belief state bt with a variational posterior distri-
bution. This perceptual inference is performed concurrently with the perpetual learning
of the generative model through the minimization of the VFE (ELBO in VAEs (Kingma
& Welling, 2013)).
Appendix E.1 offers further details on the learning process of variational inference and
the generative model.
5.1 Problem formulation: Unified objective function for AIF and
RL in POMDPs
Given the regularity assumption in Assumption 5.1 and the Perceptual Inference and
Learning assumption in Assumption 5.2, our attention now turns to defining a perfor-
mance criterion, or objective function, for learning the optimal belief state-action policy.
In POMDPs, where agents lack complete knowledge of true environmental states,
devising a performance criterion presents a challenge (Nian et al., 2020; Krishnamurthy,
2015; Chatterjee, Chmelik, & Tracol, 2016). The RL literature concerning POMDPs of-
ten focuses on maximizing the expected long-term extrinsic reward (Haklidir & Temeltas ¬∏,
2021; Montufar et al., 2015) as the optimality criterion. The EFE in AIF, as expressed
in Eq. (18), inherently balances information-seeking exploration and reward maximiza-
tion in POMDPs. Hence, if we can extend the EFE to a stochastic belief state-action
policy œÄ and reduce the computational requirements for learning the optimal policy, the
extended EFE can serve as a suitable objective function for our infinite time horizon
POMDP with continuous state, action, and observation spaces.
By extending the EFE to a stochastic belief state-action policy œÄ, action at is sampled
from policy œÄ(at|bt)8 instead of being selected from a predetermined plan Àúa. The main
idea is to learn a mapping from belief states to actions without explicitly searching
through all possible plans. Thus, our goal becomes finding an optimal stochastic belief
state-action policy œÄ‚àó that minimizes G(œÄ)
Unified(bt), which represents the EFE correspond-
ing to œÄ(at|bt), i.e., œÄ‚àó = arg minœÄ G(œÄ)
Unified(bt). Learning the optimal belief state-action
policy is computationally more efficient than finding the optimal plan in continuous
spaces, facilitating quicker decision-making. Moreover, by selecting actions individu-
ally from the policy at each time step instead of a pre-determined plan, the agent can
update its policy iteratively based on new observations over time, allowing it to adapt to
environmental dynamics, avoid repeating errors, and generalize better to new scenarios.
Theorem 5.3 states the extended EFE for the belief state-action policy œÄ.
Theorem 5.3 Let œÄ be a stochastic belief state-action policy selecting action aœÑ ac-
cording to œÄ(aœÑ |bœÑ ) for œÑ = {t, t+ 1, t+ 2, ...} in a POMDP satisfying Assumption 5.1.
G(œÄ)
Unified(bt), the EFE corresponding to the policy œÄ, can be achieved as
G(œÄ)
Unified(bt) = EQ‚àû
œÑ=t P(bœÑ+1|oœÑ+1,bœÑ ,aœÑ ) ÀúP(oœÑ+1|aœÑ ,bœÑ )œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ )
 ‚àûX
œÑ=t
logœÄ(aœÑ |bœÑ )
8In the context of our proposed unified inference framework, when we refer to the term ‚Äùpolicy‚Äù
denoted as œÄ, we mean a Markovian belief state-action policy.
‚àí log ÀúP(oœÑ+1|aœÑ , bœÑ ) + log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

. (19)
Proof. See Appendix D.1. 2
The expression P(bœÑ+1|sœÑ+1, oœÑ+1, bœÑ , aœÑ ) in Eq. (19) represents the agent‚Äôs model of
the belief state forward distribution, as defined in Sub-section 3.4. P(sœÑ+1|aœÑ , bœÑ ) rep-
resents the belief state-conditioned transition model bœÑ . Furthermore, q(sœÑ+1|oœÑ+1, bœÑ )
and ÀúP(oœÑ+1|bœÑ , aœÑ ) denotes the variational posterior q(sœÑ+1|oœÑ+1) and the prior prefer-
ence ÀúP(oœÑ+1|sœÑ , aœÑ ) in AIF conditioned on the belief state bœÑ .9 Thus, Eq. (19) com-
putes the EFE, where the agent possesses a biased generative model over the sequence
(st+1, ..., bœÑ , sœÑ+1, oœÑ+1, bœÑ+1, ...) given bt. This sequence involvesoœÑ+1 ‚àº ÀúP(oœÑ+1|aœÑ , bœÑ ),
sœÑ+1 ‚àº q(sœÑ+1|oœÑ+1, bœÑ ), and bœÑ+1 ‚àº P(bœÑ+1|sœÑ+1, oœÑ+1, bœÑ , aœÑ ). We can regard this bi-
ased generative model over this sequence as an extension of the biased generative model
of AIF defined in Eq. (15) to encompass a stochastic belief state-action policyœÄ. Hence,
we denote this biased generative model as ÀúP(st+1:‚àû, ot+1:‚àû, bt+1:‚àû|bt, at:‚àû) and refer
to it as the biased belief state generative model, which is formally defined as follows:
Definition 5.1 (Biased belief state generative model)
Given the prior preference ÀúP(oœÑ+1|bœÑ , aœÑ ) for œÑ = {t, t+ 1, . . .}, the biased belief state
generative model is ÀúP(st+1:‚àû, ot+1:‚àû, bt+1:‚àû|bt, at:‚àû), which factorizes as:
ÀúP(st+1:‚àû, ot+1:‚àû, bt+1:‚àû|bt, at:‚àû) =
‚àûY
œÑ=t
ÀúP(sœÑ+1, oœÑ+1, bœÑ+1|bœÑ , aœÑ )
=
‚àûY
œÑ=t
ÀúP(oœÑ+1|bœÑ , aœÑ )q(sœÑ+1|oœÑ+1, bœÑ )P(bœÑ+1|sœÑ+1, oœÑ+1, bœÑ , aœÑ ). (20)
The biased belief state generative model approximatesP(st+1:‚àû, ot+1:‚àû, bt+1:‚àû|bt, at:‚àû),
which extends the generative model of a POMDP defined in Eq. (3) to account for a
stochastic belief state-action policy. Hence, we refer toP(st+1:‚àû, ot+1:‚àû, bt+1:‚àû|bt, at:‚àû)
as the belief state generative model, which can be factorized as:
P(st+1:‚àû, ot+1:‚àû, bt+1:‚àû|bt, at:‚àû) =
‚àûY
œÑ=t
P(sœÑ+1, oœÑ+1, bœÑ+1|bœÑ , aœÑ )
=
‚àûY
œÑ=t
P(sœÑ+1|aœÑ ,bœÑ)P(oœÑ+1|sœÑ+1,bœÑ)P(bœÑ+1|sœÑ+1, oœÑ+1, bœÑ , aœÑ), (21)
where P(oœÑ+1|sœÑ+1, bœÑ ) represents the belief state-conditioned likelihood model.
9As the belief state bœÑ encapsulates the agent‚Äôs knowledge about the underlying state sœÑ based on all
available observations and actions, the direct conditioning onsœÑ can be omitted when conditioning on bœÑ
To accommodate the reward-maximizing RL objective, following the complete class
theorem in AIF (K. Friston et al., 2012), we employ the reparameterization logÀúP(oœÑ+1|aœÑ , bœÑ ) =
EP(rœÑ |bœÑ ,aœÑ )[rœÑ ], where P(rœÑ |bœÑ , aœÑ ) represents the belief state-conditioned reward model.
Additionally, to ensure that the value ofG(œÄ)
Unified(bt) in Eq. (19) remains bounded, we use
the discount factor Œ≥ ‚àà [0, 1), following the convention in RL literature. We denote the
modified version of G(œÄ)
Unified(bt) as G(œÄ)(bt), which can be expressed as follows:
G(œÄ)(bt)= EQ‚àû
œÑ=t P(bœÑ+1|sœÑ+1,oœÑ+1,bœÑ ,aœÑ ) ÀúP(oœÑ+1|aœÑ ,bœÑ )œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àít

logœÄ(aœÑ |bœÑ)
‚àílog ÀúP(oœÑ+1|bœÑ , aœÑ ) + log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

(22)
= EQ‚àû
œÑ=t P(oœÑ+1,bœÑ+1,sœÑ+1|bœÑ ,aœÑ )
 ‚àûX
œÑ=t
‚àíŒ≥œÑ‚àítH(œÄ(.|bœÑ ))

| {z }
Expected entropy
‚àíEQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )P(sœÑ+1,oœÑ+1,bœÑ+1|bœÑ ,aœÑ )P(rœÑ |bœÑ ,aœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àítrœÑ

| {z }
Expected long-term extrinsic reward
‚àíEQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )P(oœÑ+1,bœÑ+1|bœÑ ,aœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àítDKL[q(.|oœÑ+1, bœÑ ), P(.|aœÑ , bœÑ )]

| {z }
Expected information gain
, (23)
where H(œÄ(.|bœÑ )) is the Shannon entropy of œÄ(.|bœÑ ) and is calculated as H(œÄ(.|bœÑ )) =
‚àíEœÄ(aœÑ |bœÑ )[logœÄ(aœÑ |bœÑ )]. This entropy term provides a bonus for random exploration,
encouraging the agent to distribute the probability across all possible actions as evenly
as possible. This helps prevent the phenomenon of policy collapse (Millidge, 2020),
where the policy quickly converges to a degenerate distribution (i.e., a deterministic
policy). The second term in Eq. (23) corresponds to the agent‚Äôs objective of maximiz-
ing the expected long-term extrinsic reward, emphasizing exploitation. Including this
term encourages the agent to focus on immediate rewards and exploit its current knowl-
edge. The third term represents the expected information gain. By incorporating this
term into the objective function, the agent is motivated to actively seek out informa-
tion to reduce uncertainty about the hidden state of the environment. By combining
both reward maximization and information-seeking, G(œÄ)(bt) serves as a unified objec-
tive function for both RL and AIF in continuous space POMDPs. The problem is thus
formulated as finding the optimal belief state-action policy œÄ‚àó that minimizes G(œÄ)(bt).
Since G(œÄ)(bt) is a composite objective function that comprises multiple terms, in-
cluding the expected long-term extrinsic reward, the entropy of the policy, and the infor-
mation gain, we introduce the scaling factors0 ‚â§ Œ± <‚àû, 0 ‚â§ Œ≤ <‚àû, and 0 ‚â§ Œ∂ <‚àû
in G(œÄ)(bt) to adjust the relative weights of these components in deriving the optimal
policy. This flexibility allows us to prioritize specific objectives according to the task
and desired behaviour. By incorporating these scaling factors, we rewrite G(œÄ)(bt) in
Eq. (22) as follows:
G(œÄ)(bt) = EQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )P(sœÑ+1,oœÑ+1,bœÑ+1|bœÑ ,aœÑ )P(rœÑ |bœÑ ,aœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àít 
Œ≤ logœÄ(aœÑ |bœÑ )
‚àí Œ± rœÑ + Œ∂ log P(sœÑ+1|bœÑ , aœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

. (24)
Note 5.1: Throughout this paper, when we refer to G(œÄ)(bt), we specifically mean
the scaled version expressed in Eq. (24). Furthermore, as G(œÄ)(bt) represents the EFE
as a function of a given belief state bt, we refer to G(œÄ)(bt) as the belief state EFE to
distinguish it from the EFE G(Àúa)(ot) defined in AIF (Eq. (17)).
Given the the belief state EFE G(œÄ)(bt), the problem can now be formulated as
œÄ‚àó ‚àà arg min
œÄ
G(œÄ)(bt). (25)
The belief state EFE corresponding to the optimal policy œÄ‚àó is referred to as the op-
timal belief state EFE and is denoted as G‚àó(bt). In other words, we have G‚àó(bt) :=
G(œÄ‚àó)(bt) = min œÄ G(œÄ)(bt). It should be noted that œÄ‚àó ‚àà arg minœÄ G(œÄ)(bt) and not
œÄ‚àó = arg minœÄ G(œÄ)(bt) because the optimization problem in Eq. (25) may admit more
than one optimal policy.
Considering that the action space A in our problem is continuous, the space of possi-
ble policies œÄ representing probability distribution functions over the continuous action
space A is vast and continuous. Therefore, solving the optimization problem stated in
Eq. (25) is a challenging and non-trivial task. In the next sub-section, we will demon-
strate that G(œÄ)(bt) exhibits a recursive relationship, which offers a pathway for solving
the optimization problem in Eq. (25).
5.2 Unified Bellman equation
In this sub-section, we introduce a recursive solution for the problem in Eq. (25) in
the spirit of the classical Bellman approach in MDPs (Bellman, 1952). This recursion
allows us to solve the optimization problem presented in Eq. (25) by breaking it down
into smaller sub-problems.
We begin by demonstrating that the belief state EFE G(œÄ)(bt), defined in Eq. (24), fol-
lows a Bellman-like recursion in the context of a POMDP. Since this recursion is de-
fined for the belief state EFE G(œÄ)(bt), which serves as a unified objective function for
both RL and AIF in the POMDP setting, we refer to this Bellman-like recursion as the
unified Bellman equation.
Proposition 5.4 (Unified Bellman equation for G(œÄ)(bt))
The belief state EFE G(œÄ)(bt) defined in Eq. (24) for a POMDP satisfying Assump-
tion 5.1 can be computed recursively starting from the belief state bt and following
policy œÄ, as follows:
G(œÄ)(bt) = EœÄ(at|bt)P(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt (26)
+ EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G(œÄ)(bt+1)

.
Proof. See Appendix D.2. 2
The unified Bellman equation establishes a recursive relationship between the belief
state EFE at a given belief state bt and the belief state EFE at its successor belief state
bt+1. This recursive relationship allows us to calculate the belief state EFE G(œÄ)(bt)
at time instant t by considering the belief state EFE for the remaining time horizon,
starting from the next belief state bt+1.
To demonstrate how the unified Bellman equation in Proposition 5.4 facilitates the
optimization problem in Eq. (25), we present the following theorem, which proves that
the optimal belief state EFEG‚àó(bt) can be computed recursively using the optimal belief
state EFE at the successor belief state bt+1, i.e., G‚àó(bt+1). We refer to this recursive
computation of the optimal belief state EFE as the unified Bellman optimality equation.
Theorem 5.5 (Unified Bellman optimality equation for G‚àó(bt))
The optimal belief state EFE G‚àó(bt) = minœÄ G(œÄ)(bt) in a POMDP satisfying Assump-
tion 5.1 can be calculated recursively starting from the belief state bt and following the
optimal policy œÄ‚àó as follows:
G‚àó(bt) = min
œÄ(at|bt)
EœÄ(at|bt)

Ep(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt (27)
+ EP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G‚àó(bt+1)]

.
Proof. See Appendix D.3. 2
Using the unified Bellman optimality equation, the following corollary demonstrates
how finding the optimal instant policy œÄ‚àó(at|bt) ‚àà arg minœÄ(at|bt) G‚àó(bt) for selecting
the instant action at based on the belief state bt, involves finding an optimal policy
ÀúœÄ‚àó ‚àà arg minÀúœÄ G(ÀúœÄ)(bt+1) for the remaining actions, starting from the belief state bt+1
resulting from the first action at.
Corollary 5.6 The instant optimal policy œÄ‚àó(at|bt) ‚àà arg minœÄ(at|bt) G‚àó(bt) for choos-
ing instant actionat can be achieved in terms of the optimal policyÀúœÄ‚àó ‚àà arg minÀúœÄ G(ÀúœÄ)(bt+1)
with regard to the next belief statebt+1 resulting from at:
œÄ‚àó(at|bt) ‚àà arg min
œÄ(at|bt)
EœÄ(at|bt)

EP(rt|bt,at)[Œ≤ logœÄ(at|bt) ‚àí Œ± rt]
+ EP(bt+1,ot+1,st+1|st,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥ min
ÀúœÄ
G(ÀúœÄ)(bt+1)

. (28)
Proof. See Appendix D.4. 2
Eq. (28) demonstrates how the optimization problem œÄ‚àó ‚àà arg minœÄ G‚àó(bt) in Eq. (25)
with an infinite time horizon, can be decomposed into two sub-problems with sepa-
rate time horizons. The first sub-problem has a time horizon equal to 1 and involves
finding an instant optimal policy œÄ‚àó(at|bt) based on the belief state bt as œÄ‚àó(at|bt) ‚àà
arg minœÄ(at|bt) G‚àó(bt). The second sub-problem determines the optimal policy for the
remaining time horizon from t + 1 onwards based on the next belief state bt+1 as
ÀúœÄ‚àó ‚àà arg minÀúœÄ G(ÀúœÄ)(bt+1). In other words, the optimal policy œÄ‚àó ‚àà arg minœÄ G(œÄ)(bt)
can be decomposed as œÄ‚àó = ( œÄ‚àó(.|bt), ÀúœÄ‚àó), and it can be constructed by recursively
combining the policies of the sub-problems with one-time horizons over time, namely
œÄ‚àó(aœÑ |bœÑ ) for œÑ = {t, t+ 1, ...}. The crucial improvement achieved from Corollary 5.6
over current AIF methods is that choosing the action at from œÄ‚àó(at|bt) is now per-
formed based on subsequent counterfactual action at+1 from œÄ‚àó(at+1|bt+1), as opposed
to considering all future courses of actions. This approach allows for more efficient
decision-making by focusing on the immediate consequences of the selected action,
rather than exploring all possible future trajectories.
5.3 Existence of a unique optimal policy
In Theorem 5.5, we have shown that the optimal belief state EFE, G‚àó(bt), follows the
unified Bellman optimality recursion. This recursion allows for the recursive combina-
tion and computation of œÄ‚àó(aœÑ |bœÑ ) for œÑ = {t, t+ 1, ...}, enabling the derivation of an
optimal policy œÄ‚àó ‚àà arg minœÄ G(œÄ)(bt). In this sub-section, we aim to demonstrate the
existence of a unique optimal policy œÄ‚àó(at|bt) that satisfies Eq. (28), as well as provide
a method for computing it.
Theorem 5.7 (Existence of a unique optimal policy)
Minimizing the right-hand side of the unified Bellman optimality equation (i.e., Eq.(27))
with respect to œÄ(at|bt) in a POMDP satisfying Assumption 5.1 leads to the following
unique minimizer:
œÄ‚àó(at|bt)=arg min
œÄ(at|bt)
G‚àó(bt)
=œÉ
Ô£´
Ô£≠
EP(rt|bt,at)[Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)
h
Œ∂ log P(st+1|bt,at)
q(st+1|ot+1,bt) +Œ≥G‚àó(bt+1)]
i
Œ≤
Ô£∂
Ô£∏.(29)
Proof. See Appendix D.5. 2
Theorem 5.7 showed that œÄ‚àó(at|bt) (the unique minimizer of Eq. (27)) is a Softmax
function of the following term:
EP(rt|bt,at)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) +Œ≥G‚àó(bt+1)

, (30)
which is a function of a given belief state-action pair (bt, at). To gain intuition about
this expression, we rewrite it as follows
EP(rt|bt,at)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) +Œ≥G‚àó(bt+1)

=‚àíEP(rt|bt,at)

‚àíŒ±rt+EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) +Œ≥G‚àó(bt+1)

| {z }
G‚àó(bt,at)
.(31)
We can now simplify and rewriteœÄ‚àó(at|bt) in Eq. (29) in terms of G‚àó(bt, at) as follows:
œÄ‚àó(at|bt) =œÉ
Ô£´
Ô£≠
EP(rt|bt,at)
h
Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt,at)
q(st+1|ot+1,bt) +Œ≥G‚àó(bt+1)
i
Œ≤
Ô£∂
Ô£∏
= œÉ
‚àíG‚àó(bt, at)
Œ≤

(32)
=
exp

‚àíG‚àó(bt,at)
Œ≤

Z‚àó(bt) , (33)
where exp(.) is the exponential function, andZ‚àó(bt) =
R
A exp

‚àíG‚àó(bt,a‚Ä≤)
Œ≤

da‚Ä≤. We can
control the stochasticity of œÄ‚àó(at|bt) by adjusting the value of Œ≤. As Œ≤ ‚Üí 0, œÄ‚àó(at|bt)
converges to a deterministic policy, while as Œ≤ ‚Üí ‚àû, œÄ‚àó(at|bt) approaches a uniform
distribution over the action space A.
However, computing Z‚àó(bt) =
R
A exp

‚àíG‚àó(bt,a‚Ä≤)
Œ≤

da‚Ä≤ in the denominator of the Soft-
max function in Eq. (33) is computationally challenging or intractable. This is because
it involves the integration of the complex function G‚àó(bt, at) over the continuous ac-
tion space A. To address this issue, common practices in handling intractable integrals
over continuous spaces (Wright, 2006) involve methods like discretization of the action
space into a finite number of points or regions, or employing numerical integration tech-
niques such as Monte Carlo integration, numerical quadrature, or adaptive integration
algorithms. These methods aim to approximate the integral over the continuous space
numerically. However, discretization and numerical integration methods can introduce
errors and lead to a loss of information. Problem reformulation (Boyd & Vandenberghe,
2004), on the other hand, allows us to maintain the inherent continuity of the origi-
nal problem, enabling a more accurate representation of the underlying optimization
problem. By reformulating the problem and imposing constraints, we can optimize the
policy within a set of stochastic belief state-action policies¬ØŒ† that are tractable over con-
tinuous action spaces. These constraints limit the search space of policies to a subset of
feasible solutions ¬ØŒ†, ensuring that the optimization problem remains computationally
tractable. Consequently, using the problem formulation technique, our objective func-
tion in Eq. (25) is transformed into finding an optimal stochastic policy ¬ØœÄ‚àó ‚àà ¬ØŒ† that
minimizes the belief state EFE corresponding to the constrained policy ¬ØœÄ ‚àà ¬ØŒ†, denoted
as G(¬ØœÄ)(bt):
¬ØœÄ‚àó ‚àà arg min
¬ØœÄ‚àà¬ØŒ†
G(¬ØœÄ)(bt). (34)
The choice of the set ¬ØŒ† depends on the specific problem and its constraints. It is driven
by factors such as the complexity of the problem, the desired level of policy expres-
siveness, and computational tractability. In some cases, a more complex policy may
be necessary to capture the nuances and intricacies of the problem at hand. This could
involve using flexible parametric distributions or even non-parametric representations
for the policies. These complex policies allow for more expressive modelling of the
belief state-action policies, accommodating a wide range of possible behaviors. On the
other hand, in certain scenarios, a simpler distribution may be sufficient to effectively
model the belief state-action policies. This could involve using parametric distributions
with fewer parameters or selecting a specific functional form that is well-suited to the
problem‚Äôs characteristics. These simpler policies offer computational advantages, as
they typically require fewer computational resources and can be easier to optimize.
Since the unified Bellman equation and unified Bellman optimality equation pre-
sented in Eqs. (26) and (27) hold for the belief state EFE under any unconstrained policy
œÄ, these equations also hold for the belief state EFE corresponding to the constrained
policy ¬ØœÄ, i.e., G(¬ØœÄ)(bt), and the belief state EFE corresponding to the constrained opti-
mal policy ¬ØœÄ‚àó ‚àà ¬ØŒ†, denoted as G(¬ØœÄ‚àó)(bt) = min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt). Therefore, we can express
the unified Bellman equation and the unified Bellman optimality equation for G(¬ØœÄ)(bt)
and G( ¬ØœÄ‚àó)(bt) as
G(¬ØœÄ)(bt) = E¬ØœÄ(at|bt)P(rt|bt,at)

Œ≤ log¬ØœÄ(at|bt) ‚àí Œ± rt (35)
+ EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G(¬ØœÄ)(bt+1)

.
and
G(¬ØœÄ‚àó)(bt) = min
¬ØœÄ(at|bt)‚àà¬ØŒ†
E¬ØœÄ(at|bt)

EP(rt|bt,at)

Œ≤ log¬ØœÄ(at|bt) ‚àí Œ± rt (36)
+ EP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G(¬ØœÄ‚àó)(bt+1)]

.
Using the unified Bellman optimality equation in Eq. (36), the instant constrained opti-
mal policy ¬ØœÄ‚àó(at|bt) ‚àà arg min¬ØœÄ(at|bt)‚àà¬ØŒ† G(¬ØœÄ‚àó)(bt) can be achieved as
¬ØœÄ‚àó(at|bt) ‚àà arg min
¬ØœÄ(at|bt)‚àà¬ØŒ†
G(¬ØœÄ‚àó)(bt) (37)
= arg min
¬ØœÄ(at|bt)‚àà¬ØŒ†
E¬ØœÄ(at|bt)

EP(rt|bt,at)[Œ≤ log¬ØœÄ(at|bt) ‚àí Œ± rt]
+ EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G(¬ØœÄ‚àó)(bt+1)

. (38)
Solving the constrained minimization problem in Eq. (37) is non-trivial as it may not
have a closed-form solution for ¬ØœÄ‚àó(at|bt). However, according to Theorem 5.7, we
know that the global solution of the unconstrained problem arg min¬ØœÄ(at|bt) G(¬ØœÄ‚àó)(bt) is
œÉ

‚àíG(¬ØœÄ‚àó)(bt,at)
Œ≤

, where G(¬ØœÄ‚àó)(bt, at) is defined as
G(¬ØœÄ‚àó)(bt, at) = EP(rt|bt,at)

‚àíŒ±rt+EP(bt+1,ot+1,st+1|bt,at)[Œ∂log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥G(¬ØœÄ‚àó)(bt+1)]

. (39)
If we choose the policy set ¬ØŒ† to be convex, like a set of Gaussian or T-distributions
over the compact action space A, the constrained optimization problem in Eq. (37) for
a POMDP satisfying Assumption 5.1 meets Slater‚Äôs condition (Boyd & Vandenberghe,
2004).10 This implies the existence of a unique minimizer ¬ØœÄ‚àó(at|bt) ‚àà ¬ØŒ† that is ob-
tained by projecting the solution of the unconstrained problem, œÉ

‚àíG(¬ØœÄ‚àó)(bt,at)
Œ≤

, onto
the constrained set ¬ØŒ†. This projection operation involves finding the closest distribution
in ¬ØŒ† to œÄ‚àó(at|bt) = œÉ

‚àíG(¬ØœÄ‚àó)(bt,at)
Œ≤

, according to some distance metric or divergence.
Consistent with standard practices in RL and AIF formulations (Haarnoja, Zhou, Abbeel,
& Levine, 2018; K. Friston et al., 2017), we use information projection based on the
KL-divergence to perform this mapping. Therefore, the unique solution of the con-
strained optimization problem in Eq. (37) is:
¬ØœÄ‚àó(.|bt) = arg min
¬ØœÄ‚àà¬ØŒ†
DKL

¬ØœÄ(.|bt), œÉ
‚àíG(¬ØœÄ‚àó)(bt, .)
Œ≤

. (40)
As the intractable term Z‚àó(bt) is independent of action at, Eq. (40) can be simplified as:
¬ØœÄ‚àó(.|bt) = arg min
¬ØœÄ‚àà¬ØŒ†
DKL

¬ØœÄ(.|bt), œÉ
‚àíG(¬ØœÄ‚àó)(bt, .)
Œ≤

(41)
= arg min
¬ØœÄ‚àà¬ØŒ†
E¬ØœÄ(at|bt)

log¬ØœÄ(at|bt) + G(¬ØœÄ‚àó)(bt, at)
Œ≤

. (42)
Eq. (42) demonstrates that by selecting actions from ¬ØœÄ‚àó(.|bt) instead of œÄ‚àó(.|bt) =
œÉ

‚àíG(œÄ‚àó)(bt,¬∑)
Œ≤

, the intractable term Z‚àó(bt) is no longer involved in the action selec-
tion process.
Note 5.2: Throughout the rest of this paper, any policyœÄ and any optimal policy œÄ‚àó
mentioned are considered unconstrained. Conversely, any policy ¬ØœÄ and optimal policy
¬ØœÄ‚àó mentioned are assumed to belong to the constrained policy space ¬ØŒ†.
5.4 Existence of a unique optimal belief state-action EFE
In the previous sub-section, assuming the existence of G(¬ØœÄ‚àó)(bt, at), we have demon-
strated that the solution to the minimization problem in Eq. (37) is unique and can be
computed as follows:
¬ØœÄ‚àó(.|bt) = arg min
¬ØœÄ‚àà¬ØŒ†
DKL

¬ØœÄ(.|bt), œÉ
‚àíG(¬ØœÄ‚àó)(bt, .)
Œ≤

, (43)
Our objective in this sub-section is to demonstrate the existence of a unique value for
G(¬ØœÄ‚àó)(bt, at) that leads to deriving the optimal policy¬ØœÄ‚àó via Eq. (43). This demonstration
involves three steps.
First, we show through Lemma 5.8 that G(¬ØœÄ‚àó)(bt, at) in Eq. (39) can be expressed as
10For more details about Slater‚Äôs condition, we refer the reader to (Boyd & Vandenberghe, 2004).
G(¬ØœÄ‚àó)(bt, at) = min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt, at), where
G(¬ØœÄ)(bt, at) = EP(rt|bt,at)

‚àíŒ±rt+EP(bt+1,ot+1,st+1|bt,at)

Œ∂log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥G(¬ØœÄ)(bt+1)

. (44)
Lemma 5.8 G(¬ØœÄ‚àó)(bt, at) in Eq. (39) can be rewritten asG(¬ØœÄ‚àó)(bt, at) = min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt, at),
where G(¬ØœÄ)(bt, at) is given by
G(¬ØœÄ)(bt, at) = EP(rt|bt,at)

‚àíŒ±rt+EP(bt+1,ot+1,st+1|bt,at)

Œ∂log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥G(¬ØœÄ)(bt+1)

. (45)
Proof. See Appendix D.6. 2
While G(¬ØœÄ)(bt) represents the belief state EFE for a given belief state bt following a
given policy ¬ØœÄ, G(¬ØœÄ)(bt, at) in Eq. (45) quantifies the belief state EFE G(¬ØœÄ)(bt) condi-
tioned on a given action at ‚àà Afollowed by the policy ¬ØœÄ afterwards. Henceforth,
we refer to G(¬ØœÄ)(bt, at) and G(¬ØœÄ‚àó)(bt, at) = min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt, at) as the belief state-action
EFE and the optimal belief state-action EFE, respectively.
Second, by utilizing the result of Lemma 5.8, we establish the following proposition,
which introduces the unified Bellman equation for the belief state-action G(¬ØœÄ)(bt, at).
Proposition 5.9 (Unified Bellman equation for G(œÄ)(bt, at))
The belief state-action G(¬ØœÄ)(bt, at) in a POMDP satisfying Assumption 5.1 can be cal-
culated recursively starting from the belief statebt and action at and then following the
policy ¬ØœÄ ‚àà ¬ØŒ† as follows:
G(¬ØœÄ)(bt, at) = EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ(at+1|bt+1)[Œ≤ log¬ØœÄ(at+1|bt+1) + G(¬ØœÄ)(bt+1, at+1)]

. (46)
Proof. See Appendix D.7. 2
Third, by utilizing the result of Proposition 5.9, we establish through Theorem 5.10 that
the belief state-action EFE G(¬ØœÄ)(bt, at), for a given policy ¬ØœÄ ‚àà ¬ØŒ†, exists and is a unique
fixed point of the operator Tunified
¬ØœÄ G(bt, at). This operator, which we refer to as the
unified Bellman operator, is defined as follows:
Tunified
¬ØœÄ G(bt, at) :=‚àíEP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ(at+1|bt+1)[Œ≤ log¬ØœÄ(at+1|bt+1) + G(bt+1, at+1)]

. (47)
Theorem 5.10 (Existence of a unique belief state-action G(¬ØœÄ)(bt, at) for a given ¬ØœÄ)
The belief state-action EFE G(¬ØœÄ)(bt, at) for all bt ‚àà Band at ‚àà Ain a POMDP satisfy-
ing Assumption 5.1 exists and is a unique fixed point G(¬ØœÄ)(bt, at) = Tunified
¬ØœÄ G(¬ØœÄ)(bt, at)
of the unified Bellman operator Tunified
¬ØœÄ in Eq. (47).
Proof. See Appendix D.8. 2
Finally, using Theorem 5.10, Corollary 5.11 shows that that the optimal belief state-
action EFE G(¬ØœÄ‚àó)(bt, at) = min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt, at) exists and is the unique fixed point given
by G(¬ØœÄ‚àó)(bt, at) = Tunified
¬ØœÄ‚àó G(¬ØœÄ‚àó)(bt, at).
Corollary 5.11 (Existence of a unique optimal belief state-action G(¬ØœÄ‚àó)(bt, at))
The optimal belief state-action EFE G(¬ØœÄ‚àó)(bt, at) = min ¬ØœÄ‚ààŒ† G(¬ØœÄ)(bt, at) defined in a
POMDP satisfying Assumption 5.1 exits and is the unique fixed point of the following
equation:
G(¬ØœÄ‚àó)(bt, at) =Tunified
¬ØœÄ‚àó G(¬ØœÄ‚àó)(bt, at) (48)
= EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ‚àó(at+1|bt+1)[Œ≤ log¬ØœÄ‚àó(at+1|bt+1) + G(¬ØœÄ‚àó)(bt+1, at+1)]

. (49)
Proof. See Appendix D.9. 2
Corollary 5.11 demonstrates the existence and uniqueness ofG(¬ØœÄ‚àó)(bt, at), which plays a
crucial role in Eq. (43) to determine the optimal policy¬ØœÄ‚àó(at|bt). Furthermore, Eq. (49)
extends the unified Bellman optimality equation to incorporate the optimal belief state-
action EFE G(¬ØœÄ‚àó)(bt, at), and therefore, we refer to it as the unified Bellman optimality
equation for G(¬ØœÄ‚àó)(bt, at).
5.5 Derivation of unified policy iteration
So far, we have proven the existence and uniqueness of the optimal policy ¬ØœÄ‚àó(.|bt) as
¬ØœÄ‚àó(.|bt) = arg min
¬ØœÄ‚àà¬ØŒ†
DKL

¬ØœÄ(.|bt), œÉ
‚àíG(¬ØœÄ‚àó)(bt, .)
Œ≤

, (50)
where
G(¬ØœÄ‚àó)(bt, at) = EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ‚àó(at+1|bt+1)[Œ≤ log¬ØœÄ‚àó(at+1|bt+1) + G(¬ØœÄ‚àó)(bt+1, at+1)]

. (51)
From Eqs. (50) and (51), it is evident that the learning of the optimal policy¬ØœÄ‚àó(at|bt) at
time t depends on the optimal belief state-action G(¬ØœÄ‚àó)(bt, at), which, in turn, depends
on the optimal policy ¬ØœÄ‚àó(at+1|bt+1) at the subsequent time step. As a result, ¬ØœÄ‚àó(at|bt)
and its corresponding belief state-action G(¬ØœÄ‚àó)(bt, at) follow a recursive pattern over
time, enabling their recursive determination. To initiate this recursive process, we can
start with an arbitrary policy, which serves as the initial optimal policy (baseline policy),
and initialize its corresponding belief state-action EFE with arbitrary values. We then
update the belief state-action EFE corresponding to the baseline policy using Eq. (51).
Next, we update the baseline policy using Eq. (50). This updated policy becomes the
new baseline policy, and the process is repeated iteratively over k ‚àà {0, 1, 2, ...}. In-
spired by this recursive procedure, we propose an iterative algorithm called unified
policy iteration for concurrent learning of ¬ØœÄ‚àó and G(¬ØœÄ‚àó). The algorithm involves up-
dating the baseline policy and its corresponding belief state-action EFE iteratively in a
computationally efficient manner. We provide a proof of convergence for the proposed
unified policy iteration, showing that it converges to the optimal policy ¬ØœÄ‚àó and its cor-
responding belief state-action EFE G(¬ØœÄ‚àó). This algorithm offers an effective approach
for solving the POMDP problem with continuous state, action, and observation spaces,
providing a practical framework for decision-making in complex environments.
Consider a randomly selected policy ¬ØœÄ(old) ‚àà ¬ØŒ† and its randomly initialized be-
lief state-action EFE G(¬ØœÄ(old)). We can update G(¬ØœÄ(old)) by applying the unified Bellman
operator Tunified
¬ØœÄ(old) G(¬ØœÄ(old))(bt, at), as follows
G(¬ØœÄ(old))(bt, at) ‚ÜêTunified
¬ØœÄ(old) G(¬ØœÄ(old))(bt, at) (52)
= EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ(old)(at+1|bt+1)[Œ≤ log¬ØœÄ(old)(at+1|bt+1) + G(¬ØœÄ(old))(bt+1, at+1)]

. (53)
Next, let‚Äôs consider a new policy ¬ØœÄ(new)(.|bt) resulting from the following equation:
¬ØœÄ(new)(.|bt)=arg min
¬ØœÄ‚àà¬ØŒ†
DKL
"
¬ØœÄ(.|bt), œÉ
 
‚àíG(¬ØœÄ(old))(bt, .)
Œ≤
!#
. (54)
The next lemma proves that the belief state-action EFE corresponding to ¬ØœÄ(new) has a
lower value than the belief state-action EFE corresponding to the baseline policy ¬ØœÄ(old).
We thus refer to the new policy ¬ØœÄ(new) as an improved version of the baseline policy
¬ØœÄ(old), and we refer to the next lemma as the unified policy improvement.
Lemma 5.12 (Unified policy improvement)
Let ¬ØœÄ(old) ‚àà Œ† be a randomly selected policy, and let ¬ØœÄ(new)(.|bt) be ¬ØœÄ(new)(.|bt) =
arg min¬ØœÄ‚àà¬ØŒ† DKL[¬ØœÄ(.|bt), œÉ(‚àíG(¬ØœÄ(old))(bt,.)
Œ≤ )] in a POMDP satisfying Assumption 5.1. Then:
G(¬ØœÄ(new))(bt, at) ‚â§ G(¬ØœÄ(old))(bt, at) (55)
for all (bt, at) ‚àà B √ó A.
Proof. See Appendix D.10. 2
Using Lemma 5.12, we formally state our proposed unified policy iteration algorithm
through the following theorem.
Theorem 5.13 (Unified policy iteration)
Consider a POMDP satisfying Assumption 5.1. By starting from an initial policy ¬ØœÄ0 ‚àà
¬ØŒ† and an initial mapping G0 : B √ó A ‚ÜíR and recursively applying the following
updates for k = 0, 1, 2, . . .:
Gk+1(bt, at) = Tunified
¬ØœÄk Gk(bt, at), (56)
¬ØœÄk+1(¬∑|bt) = arg min
¬ØœÄ‚àà¬ØŒ†
DKL

¬ØœÄ(¬∑|bt), œÉ
‚àíGk+1(bt, ¬∑)
Œ≤

, (57)
the iterative process converges to ¬ØœÄ‚àó = arg min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt).
Proof. See Appendix D.11. 2
Theorem 5.13 demonstrates that the optimal policy ¬ØœÄ‚àó = arg min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt) can be
obtained by recursively alternating between the belief state-action EFE update step,
given by Eq. (56), and the policy improvement step, given by Eq. (57).
5.6 Unified reward function for AIF and RL in POMDPs
The unified Bellman equation for the belief state EFE G(¬ØœÄ)(bt) and the Bellman op-
timality equation for G(¬ØœÄ‚àó)(bt) in a POMDP, as stated in Eqs. (35) and (36) respec-
tively, exhibit a strong resemblance to the Bellman equation for the state value function
V (œÄ)(st) and the Bellman optimality equation for V ‚àó(st) in an MDP, as represented by
Eqs. (7) and (10) respectively. Given this similarity and considering that the proposed
unified inference algorithm aims to minimize the belief state EFE while RL endeavours
to maximize the state value function, we can interpret G(¬ØœÄ)(bt) as ‚àí¬ØV (¬ØœÄ)(bt), where
¬ØV (¬ØœÄ)(bt) is referred to as the belief state value function, defined as follows:
¬ØV (¬ØœÄ)(bt) = EQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )P(sœÑ+1,rœÑ ,oœÑ+1,bœÑ+1|bœÑ ,aœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àít

Œ± rœÑ ‚àí Œ∂ log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )
‚àí Œ≤ log¬ØœÄ(aœÑ |bœÑ )

. (58)
The belief state value function ¬ØV (¬ØœÄ)(bt) in Eq. (58) can be considered as a value function
for a POMDP with the reward function runified
œÑ at time step œÑ defined as:
runified
œÑ = Œ± r œÑ|{z}
rextrinsicœÑ
+Œ∂

‚àílog P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bt)

| {z }
rintrinsicœÑ
+Œ≤ (‚àílog¬ØœÄ(aœÑ |bœÑ ))| {z }
rentropy
œÑ
. (59)
The reward function runified
œÑ represents the agent‚Äôs total reward at time step œÑ after per-
forming action aœÑ in belief state bœÑ . It encompasses the reward rœÑ obtained, the transi-
tion to the next statesœÑ+1, and the observationoœÑ+1 within the POMDP framework. The
first term ofrunified
œÑ (i.e., rextrinsic
œÑ ) corresponds to the extrinsic reward, which is the reward
function commonly used in MDP-based RL. The second term ( rintrinsic
œÑ ) represents the
intrinsic reward that encourages the agent to visit states providing the most information
about the hidden states of the environment. The combination of the extrinsic reward
rextrinsic
œÑ and the intrinsic reward rintrinsic
œÑ balances exploration for information gain with
the exploitation of extrinsic rewards. The third term in Eq. (59) arises from taking
the expected value over future actions and corresponds to entropy maximization. This
term, referred to as the entropy reward rentropy
œÑ , promotes random exploration and pre-
vents the policy from becoming overly deterministic, thereby encouraging the learning
of stochastic policies ¬ØœÄ. As explained in Section 2, learning a stochastic policy enables
the agent to adapt to different situations resulting from environmental changes, lead-
ing to improved stability in the proposed unified policy iteration approach. The reward
function runified
œÑ combines both the extrinsic reward employed in RL and the intrinsic
reward that promotes information-seeking exploration as used in AIF. Hence, we des-
ignate runified
œÑ as a unified reward function applicable to both AIF and RL in POMDPs.
Given that the state-action value functionQ(œÄ)(st, at) and the belief state-action EFE
G(¬ØœÄ)(bt, at) are obtained by conditioning V (œÄ)(st) and G(¬ØœÄ)(bt) on the current action
At = at, we can similarly interpret G(¬ØœÄ)(bt, at) as negative counterpart of the belief
state-action value function ¬ØQ(¬ØœÄ)(bt, at), defined as follows:
¬ØQ(¬ØœÄ)(bt, at) =EQ‚àû
œÑ=t œÄ(aœÑ+1|bœÑ+1)P(sœÑ+1,rœÑ ,oœÑ+1,bœÑ+1|bœÑ ,aœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àít

Œ± rœÑ ‚àí Œ∂ log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )
‚àíŒ≤ log¬ØœÄ(aœÑ+1|bœÑ+1)

. (60)
By setting Œ± = 1, Œ≤ = 0, and Œ∂ = 0 in the unified reward functionrunified
œÑ in Eq. (59),
the optimal policy ¬ØœÄ‚àó = arg max¬ØœÄ‚àà¬ØŒ† ¬ØV (¬ØœÄ)(bt) aims to maximize the expected long-term
extrinsic reward alone, which is the objective in MDP-based RL algorithms. However,
by setting Œ± = 1, Œ≤ = 0, and Œ∂ = 1, the optimal policy ¬ØœÄ‚àó = arg max¬ØœÄ‚àà¬ØŒ† ¬ØV (¬ØœÄ)(bt) aims
to maximize both the expected long-term extrinsic reward through rextrinsic
œÑ and the ex-
pected long-term information gain through rintrinsic
œÑ , which is crucial in the context of
POMDPs. Therefore, we can extend RL methods developed for MDPs to POMDPs
by augmenting the external reward rœÑ in these RL approaches with the intrinsic reward
‚àílog P(sœÑ+1|bœÑ ,aœÑ )
q(sœÑ+1|oœÑ+1,bœÑ ).
Furthermore, by setting Œ± = 0, Œ≤ = 0, and Œ∂ = 1 in the unified reward function runified
œÑ
in Eq. (59), the optimal policy ¬ØœÄ‚àó = arg max¬ØœÄ‚àà¬ØŒ† ¬ØV (¬ØœÄ)(bt) solely focuses on maximiz-
ing the expected information gain through the intrinsic reward termrintrinsic
œÑ . Therefore,
by replacing the extrinsic reward rœÑ in these RL approaches with the intrinsic reward
‚àílog P(sœÑ+1|bœÑ ,aœÑ )
q(sœÑ+1|oœÑ+1,bt), we can extend extrinsic reward-dependent RL algorithms to a set-
ting where extrinsic reward values have not been determined by an external supervisor.
This extension is particularly important in scenarios where designing extrinsic reward
values is costly or specifying them is challenging.
6 Unified inference model
As mentioned in Assumption 5.2, the proposed unified inference assumes that the agent
performs perceptual inference and learning before action selection at each time step by
minimizing the VFE. The action selection phase is facilitated by the proposed unified
policy iteration method outlined in Theorem 5.13.
However, in scenarios with continuous state, action, and observation spaces, the infinite
number of possible states, actions, and observations makes it impractical to explic-
itly represent the generative model, variational posterior distribution, belief state-action
EFE, and policy using a finite set of values (Sutton & Barto, 2018; Haarnoja, Zhou,
Abbeel, & Levine, 2018). To overcome this challenge, following common practices
in both RL and AIF (Haarnoja, Zhou, Abbeel, & Levine, 2018; Mnih et al., 2013;
Ueltzh¬®offer, 2018; Lee et al., 2020), we utilize DNNs to approximate the generative
model, variational posterior distribution, belief state-action EFE, and policy.
Considering that perceptual inference and learning are derived from existing literature,
detailed explanations are provided in Appendix E.1. This section primarily focuses on
our contributions: learning the optimal belief state-action policy through our proposed
unified policy iteration method.
6.1 Unified actor-critic
In this sub-section, we focus on approximating the belief state-action EFE G(¬ØœÄ)(bt, at)
and the belief state-action policy ¬ØœÄ within the context of the proposed unified policy
iteration, which involves alternating between the belief state-action EFE update step,
as given by Eq. (56), and the policy update step, as given by Eq. (57). Inspired by
actor-critic algorithms in RL, we refer to the approximated policy as the actor and the
approximated belief state-action EFE as the critic. Hence, we term our approximation
of the unified policy iteration framework as the unified actor-critic.
By utilizing the relationshipG(¬ØœÄ)(bt) = ‚àí¬ØV (¬ØœÄ)(bt), which arises from the unified reward
function runified
t in Eq. (59), we can leverage and adapt a wide range of advanced MDP-
based actor-critic algorithms to learn the critic G(¬ØœÄ)(bt, at) and the actor ¬ØœÄ(at|bt) in our
unified actor-critic algorithm. This enables us to take advantage of the existing methods
and techniques developed for MDPs and extend them to address the challenges posed
by partial observability in POMDPs.
To approximate G(¬ØœÄ)(bt, at) and ¬ØœÄ(at|bt), we parameterize them with œà and œï, re-
spectively. However, directly inputting the infinite-dimensional continuous belief state
bt to the DNNs modeling the policy and the belief state-action EFE function is impracti-
cal. Instead, we use a belief state representation ht ‚àà RDH, where DH is the dimension
of ht. This representation is explained in Appendix E.1. We then model the belief state-
action EFE and the policy as G(¬ØœÄ)
œà (ht, at) and ¬ØœÄœï(at|ht), respectively. Thus, the belief
state-action EFE update and the policy improvement steps can be expressed as follows
G(¬ØœÄ)
œà (ht, at) = EP(rt|ht,at)

‚àí Œ± rt + EP(ht+1,ot+1,st+1|ht,at)

Œ∂ logP(st+1|ht, at)
q(st+1|ot+1)
+ Œ≥E¬ØœÄ(at+1|ht+1)[Œ≤ log¬ØœÄ(at+1|ht+1) + G(¬ØœÄ)
œà (ht+1, at+1)]

, (61)
and
¬ØœÄœï(.|ht)=arg min
¬ØœÄ‚àà¬ØŒ†
DKL
"
¬ØœÄ(.|ht), œÉ
 
‚àíG(¬ØœÄ)
œà (ht, .)
Œ≤
!#
. (62)
We train œà to minimize the squared residual error derived from Eq. (61) using
batches of size B. Each batch can consist of M sequential data points denoted as
{(ak‚àí1, rk, ok)M
k=1}B
i=1, sampled from a replay buffer D containing real environment
interactions. Alternatively, batches may include N simulated interaction data points
{(aœÑ , rœÑ , oœÑ+1)t+N
œÑ=t }B
i=1, starting from ht and simulating interactions up to N steps. We
can also use a combination of both real and imagined interactions for training œà. De-
pending on the type of data used, we devise three unified actor-critic approaches: model-
based unified actor-critic, model-free unified actor-critic, and hybrid unified actor-critic.
6.1.1 Model-free unified actor-critic
Inspired by model-free RL algorithms, our proposed model-free unified actor-critic
learns œà by minimizing the squared residual error from Eq. (61) over a batch of real
data {(ak, rk, ok+1)M
k=1}B
i=1 sampled from the replay buffer D. This involves minimiz-
ing the squared residual error LMF
G (œà), which is defined as
LMF
G (œà) = ED(ak,rk,ok+1)
1
2

G(¬ØœÄ)
œà (ht, ak) + Œ± rk ‚àí Eq(st+1|ht,ok+1)

Œ∂ log P(st+1|ht, at)
q(st+1|ot+1, ht)
‚àíŒ≥EP(ht+1|ht,ak,ok+1,st+1)¬ØœÄœï(at+1|ht+1)[Œ≤ log¬ØœÄœï(at+1|ht+1) + G(¬ØœÄ)
œà (ht+1, at+1)]
2
, (63)
After updating œà, the parameter œï of the policy ¬ØœÄœï(at|ht) can be trained by minimizing
the right-hand side of Eq. (62):
LMF
¬ØœÄ (œï) = DKL
"
¬ØœÄœï(.|ht), œÉ
 
‚àíG(¬ØœÄ)
œà (ht, .)
Œ≤
!#
(64)
= E¬ØœÄœï(at|ht)
h
Œ≤ log¬ØœÄœï(at|ht) + G(¬ØœÄ)
œà (ht, at)
i
. (65)
To minimize Eq. (65), we need to calculate its gradient with respect to œï by sam-
pling from ¬ØœÄœï(at|ht). However, computing this gradient involves differentiating with
respect to ¬ØœÄœï(at|ht), which we sample from. Therefore, we use the reparameterization
trick (Kingma & Welling, 2013) for sampling from ¬ØœÄœï(at|ht).
By employing the relationship G(¬ØœÄ)
œà (ht) = ‚àí¬ØV (¬ØœÄ)
œà (ht) and considering a specific
case of the model-free unified actor-critic algorithm, where Œ± = 1 and Œ∂ = 0, we re-
cover the variational recurrent model (VRM) algorithm proposed in (Han et al., 2020)
as an extension of SAC (Haarnoja, Zhou, Abbeel, & Levine, 2018) to POMDPs. Addi-
tionally, by assigning Œ± = 0 and Œ≤ Ã∏= 0 in the model-free unified actor-critic algorithm,
we can formulate the so-called reward-free variants of SAC (Haarnoja, Zhou, Abbeel,
& Levine, 2018; Haarnoja, Zhou, Hartikainen, et al., 2018) in settings without an ex-
trinsic reward function. Moreover, by setting Œ± = 1 and assigning non-zero values to
Œ∂ (i.e., Œ∂ Ã∏= 0), our model-free unified actor-critic algorithm is transformed into a gen-
eralized version of SAC for POMDPs. This generalized version maximizes both the
expected future extrinsic reward and information gain. We refer to this extension as the
generalized SAC (G-SAC).
Generalized SAC (G-SAC) generalizes model-free SAC (Haarnoja, Zhou, Abbeel, &
Levine, 2018) to POMDPs with the following loss functions:11
LG-SAC
G (œà) = ED(ak,rk,ok+1)
1
2

G(¬ØœÄ)
œà (ht, ak) + rk ‚àí Eq(st+1|ht,ok+1)

Œ∂ log P(st+1|ht, at)
q(st+1|ot+1, ht)
‚àíŒ≥EP(ht+1|ht,ak,ok+1,st+1)¬ØœÄœï(at+1|ht+1)[Œ≤ log¬ØœÄœï(at+1|ht+1) + G(¬ØœÄ)
œà (ht+1, at+1)]
2
, (66)
and
LG-SAC
¬ØœÄ (œï) = E¬ØœÄœï(at|ht)
h
Œ≤ log¬ØœÄœï(at|ht) + G(¬ØœÄ)
œà (ht, at)
i
. (67)
Note 6.1: It is important to clarify that the proposed model-free unified actor-critic
algorithm incorporates a learned belief state generative model to compute the intrinsic
reward rintrinsic
t and the belief state representation ht. Therefore, from a strict RL per-
spective, it is not considered a purely model-free algorithm. However, in this context,
the term ‚Äùmodel-free‚Äù refers to the fact that the generative model is not used for data se-
lection during belief state-action EFE (critic) learning. Instead, the agent directly learns
the policy and belief state-action EFE from observed interactions with the environment.
During the iterative process of the model-free unified actor-critic, the agent interacts
with the environment, updates its belief state-action EFE based on observed rewards and
observations, and learns through trial and error. Model-free methods, not relying on a
learned model for data selection, are more robust to modeling errors or inaccuracies.
However, this trial-and-error approach can be time-consuming and sample inefficient,
11In the original formulation of SAC (Haarnoja, Zhou, Abbeel, & Levine, 2018), they introduced
an additional function approximator for the state value function, but later they found it to be unneces-
sary (Haarnoja, Zhou, Hartikainen, et al., 2018).
often requiring numerous samples to converge to an optimal or near-optimal policy. To
tackle this, we introduce a model-based version of the unified actor-critic method in the
next phase to learn the optimal policy.
6.1.2 Model-based unified actor-critic
The proposed model-based unified actor-critic algorithm utilizes the learned belief state
generative model to simulate and predict future observations and rewards resulting from
different actions. This enables the agent to make decisions and optimize its policy with-
out needing direct interaction with the environment.
To implement the model-based unified actor-critic approach, the agent collects trajecto-
ries {(aœÑ , rœÑ , oœÑ+1)t+N
œÑ=t }B
i=1 starting from the initial belief state ht and forwarding till N.
These trajectories are generated by imagining following the policyaœÑ ‚àº ¬ØœÄœï(aœÑ |hœÑ ), uti-
lizing the transition model P(sœÑ+1|hœÑ , aœÑ ), the reward modelP(rœÑ |hœÑ , aœÑ ), and the like-
lihood model P(oœÑ+1|hœÑ , sœÑ+1). It‚Äôs important to note that the reward modelP(rt|st, at)
also needs to be learned, with the process detailed in Appendix E.2.
Next, the critic parameter œà is updated by minimizing the following loss function:
LMB
G (œà) =E¬ØœÄœï(at|ht)P(rt|ht,at)
1
2

G(¬ØœÄ)
œà (ht, at) + Œ± rt
‚àíEP(st+1|ht,at)P(ot+1|ht,st+1)

Œ∂ log P(st+1|ht, at)
q(st+1|ot+1, ht) (68)
‚àí Œ≥ EP(ht+1|ht,at,ot+1,st+1)¬ØœÄœï(at+1|ht+1)[Œ≤ log¬ØœÄœï(at+1|ht+1) + G(¬ØœÄ)
œà (ht+1, at+1)]
2
,
where the expectations are estimated under the imagined trajectories{(aœÑ , rœÑ , oœÑ+1)t+N
œÑ=t }B
i=1.
The actor parameter œï is estimated by minimizing the following loss function:
LMB
¬ØœÄ (œï) = E¬ØœÄœï(at|ht)

Œ≤ log¬ØœÄœï(at|ht) + G(¬ØœÄ)
œà (ht, at)

. (69)
Note 6.2: This model-based unified actor-critic approach reduces memory usage
and improves sample efficiency compared to the model-free unified actor-critic. How-
ever, it requires learning the reward model P(rœÑ |hœÑ , aœÑ ), unlike the model-free unified
actor-critic. Learning the reward model of an environment is typically more challeng-
ing than learning the transition and likelihood models since prediction errors in the state
and observation of the environment provide a richer source of information than reward
prediction errors. This is because rewards are usually scalar, while the environment‚Äôs
state and observation are typically characterized by high dimensionality.
By relating G(¬ØœÄ)
œà (ht) to ‚àí¬ØV (¬ØœÄ)
œà (ht), we can recover the model-based actor-critic
Dreamer algorithm (Hafner et al., 2020), which focuses on maximizing rewards, as a
special case of our model-based unified actor-critic algorithm whenŒ± = 1 and Œ∂ = 0.12
Additionally, we can create a reward-free version of the Dreamer by assigning Œ± = 0
and Œ≤ Ã∏= 0 in the model-based unified actor-critic algorithm. Furthermore, by selecting
Œ± = 1 and assigning non-zero values to Œ∂, we extend Dreamer to POMDPs, aiming
to maximize both the expected long-term extrinsic reward and the expected long-term
information gain during action selection. This extended version is referred to as the
Generalized Dreamer (G-Dreamer).
Generalized Dreamer (G-Dreamer)generalizes model-based Dreamer framework (Hafner
et al., 2020) to POMDPs with the following loss functions:
LG-Dreamer
G (œà) =E¬ØœÄœï(at|ht)P(rt|ht,at)
1
2

G(¬ØœÄ)
œà (ht, at) + rt
‚àíEP(st+1|ht,at)P(ot+1|ht,st+1)

Œ∂ log P(st+1|ht, at)
q(st+1|ot+1, ht) (70)
‚àí Œ≥ EP(ht+1|ht,ak,ot+1,st+1)¬ØœÄœï(at+1|ht+1)[Œ≤ log¬ØœÄœï(at+1|ht+1) + G(¬ØœÄ)
œà (ht+1, at+1)]
2
,
and
LG-Dreamer
¬ØœÄ (œï) = E¬ØœÄœï(at|ht)

Œ≤ log¬ØœÄœï(at|ht) + G(¬ØœÄ)
œà (ht, at)

. (71)
6.1.3 Hybrid unified actor-critic
While model-based unified actor-critic methods offer the potential for higher sample
efficiency and advanced trajectory integration, their reliance on accurate models for
performance is crucial. Errors in learned generative models can accumulate over time,
leading to significant deviations from desired behavior (X. Ma et al., 2021). Learn-
ing precise models, especially with complex observations, is challenging. Inspired
by (X. Ma et al., 2021), to mitigate model errors, we adopt a hybrid unified actor-
critic learning scheme. This approach combines the sample efficiency of model-based
learning with the robustness of model-free learning to address inaccuracies.
In the hybrid unified actor-critic scheme, the critic parameter œà is learned using both
real trajectories {(ak, rk, ok+1)M
k=1}B
i=1 and imagined trajectories{(aœÑ , rœÑ , oœÑ+1)t+N
œÑ=t }B
i=1:
LHybrid
G (œà) = LMB
G (œà) + c LMF
G (œà), (72)
12Dreamer computes estimates of G(¬ØœÄ)(ht) instead of G(¬ØœÄ)(ht, at). However, these two estimates
are equivalent due to the relationship between G(¬ØœÄ)(ht) and G(¬ØœÄ)(ht, at) illustrated in Appendix D.7.
Specifically, we have G(¬ØœÄ)(ht) = E¬ØœÄ(at|ht)

Œ≤ log¬ØœÄ(at|ht) + G(¬ØœÄ)(ht, at)

.
where c represents the scaling factor that determines the relative importance of the
model-free critic loss functionLMF
G (œà) compared to the model-based critic loss function
LMB
G (œà). The actor parameter œï is learned by minimizing the following loss function:
LHybrid
¬ØœÄ (œï) = E¬ØœÄœï(at|ht)

Œ≤ log¬ØœÄœï(at|ht) + G(¬ØœÄ)
œà (ht, at)

. (73)
The complete learning process of the unified inference model is outlined in Algo-
rithm E.1 of the appendix.
7 Related work
This section provides a concise overview of relevant literature, specifically addressing
the extension of RL and AIF techniques to POMDPs featuring continuous spaces.
7.1 RL approaches for continuous space POMDPs
While RL conventionally targets MDPs, recent progress has broadened its application
to POMDPs. These advancements frequently employ memory-based neural networks
to encode past observations and actions or employ belief state inference.
7.1.1 Memory-based approaches
Hausknecht and Stone (2015) developed a variant of (Mnih et al., 2013) to handle
POMDPs by incorporating a recurrent layer, such as Long Short-Term Memory (LSTM)
or Gated Recurrent Unit (GRU), to capture the history of observations. However, this
method did not consider the history of actions, focusing solely on the observation se-
quence. Later, Zhu et al. (2017); Heess, Hunt, Lillicrap, and Silver (2015); Nian et
al. (2020) utilized recurrent layers to capture both the observation and action history.
They demonstrated that it is possible to store only the necessary statistics of the history
using recurrent layers instead of storing the entire preceding history. It is worth noting
that these works primarily focused on tasks with discrete action spaces rather than con-
tinuous ones. Haklidir and Temeltas ¬∏ (2021) proposed the guided SAC approach, which
augments the original SAC with a guiding policy. The guided SAC architecture consists
of two actors and a critic, where the original actor incorporates the history of observa-
tions and actions, while the guiding actor uses the true state as input. Although the
guided SAC has been applied to tasks with continuous observation and action spaces,
it still requires storing the history of observations and actions and relies on an external
supervisor to provide additional information about the true state of the environment,
which remains a challenge. Meng, Gorbet, and Kuli¬¥c (2021); Ni et al. (2022); Yang and
Nguyen (2021) extended actor-critic algorithms to POMDPs by adding recurrent lay-
ers to both the actor and critic components to compress history into a into a fixed-size
representation. This compresses history into a manageable form, passing to the actor
and critic. This adaptation allowed the models to effectively handle continuous action
spaces.
Although memory-based approaches have demonstrated promise in tasks involving
continuous state, action, and observation spaces, our proposed unified inference frame-
work, which expands AIF into continuous spaces through the extension of the EFE to
stochastic belief state-action policies, offers three main advantages:
i) Computational and memory efficiency: Firstly, although we also transform the con-
tinuous belief state bt into a fixed-length representation ht using an LSTM, the LSTM
in our method updates the belief state representation based on fixed-size inputs ht‚àí1,
at‚àí1, ot, and st (see Appendix E.1 for more details). In contrast, the memory-based
methods in (Meng et al., 2021; Ni et al., 2022; Yang & Nguyen, 2021) require passing
the entire history to their LSTM, leading to substantial memory demands in large or
infinite time horizon problems. Although our belief state inference-based method may
not require as much memory since it does not need to store extensive historical data, it
still demands significant computational resources. These demands arise primarily from
the complexity involved in learning the generative model, belief state, and belief state
representation. However, belief state inference-based algorithms, including ours, often
become more computationally efficient over time compared to memory-based methods.
This efficiency stems from two main factors: a) Belief state inference-based approaches
learn generative models and belief states through feed-forward neural networks, which
are less computationally demanding than RNNs (Pascanu et al., 2013). b) In memory-
based methods, the dimension of inputs to LSTMs grows as history accumulates, even-
tually surpassing the fixed input dimension of our LSTM for belief state representation
learning in our infinite horizon POMDP setting. This increase in the input dimension of
RNNs leads to increasingly computationally expensive matrix multiplications at each
step, while the computational complexity of our algorithms remains fixed across time
steps. This illustrates one of the advantages of basing our proposed unified inference
on AIF, as it allows us to leverage inference from AIF for decision-making through a
belief state-action policy.
Therefore, over time, the rapid increase in input dimensions in memory-based meth-
ods surpasses the memory and computational demand of learning generative models,
belief states, and belief state representations in our method, especially in scenarios
with large/infinite horizons. A detailed comparison of the memory and computational
complexities between our algorithm and memory-based baselines is provided in Ap-
pendix H.
ii) Enhanced exploration: Beyond the computational and memory efficiencies, our ap-
proach incorporates an information gain exploratory term when making decisions, high-
lighting another necessity of AIF for managing the uncertainty inherent in partially ob-
servable environments. In recent years, extensive exploration methods for RL in fully
observable environments have emerged (Pathak, Agrawal, Efros, & Darrell, 2017; Choi
et al., 2018; Savinov et al., 2019). However, due to the limited observability of states
in partially observable tasks, designing intrinsic exploration methods is non-trivial and
challenging. For tasks with partial observability, one prominent line of exploration uti-
lized in memory-based RL relates to prediction error-based approaches, such as the
intrinsic curiosity model (ICM) (Pathak et al., 2017) and random network distillation
(RND) (Burda, Edwards, Storkey, & Klimov, 2018). These methods, which can be
enhanced by incorporating memory mechanisms such as RNNs to handle partial ob-
servability, learn a forward model to predict the next state and sometimes a backward
model to infer past states. The prediction loss from these models serves as intrinsic mo-
tivation for the agent to explore new and informative states. Oh and Cavallaro (2019)
introduced a triplet ranking loss to push the prediction output of the forward dynamics
model to be far from the output generated by taking alternative actions. However, these
prediction error-based approaches face challenges in accurately discerning the novelty
of an agent‚Äôs state in partially observable settings for two main reasons: 1) As states are
not fully observable, applying these methods in partially observable environments re-
quires constructing forward and backward models based solely on observations, which
are noisy or incomplete representations of states. Relying solely on local observations
is insufficient for accurately inferring novelty over the true world state in tasks with
partial observability. This is because two observations might appear identical at vari-
ous locations on the map, although their underlying true world states are fundamentally
different. 2) Prediction errors might restrict the expressiveness of the inferred intrinsic
reward scores. This challenge is especially prominent in environments characterized
by continuous state spaces, where state changes occur subtly over short intervals. Con-
sequently, agents may incorrectly perceive such states as familiar or well-understood,
leading to minimal prediction errors and inadequate novelty detection. Moreover, in
stochastic environments where outcomes following the same action can vary unpre-
dictably, prediction errors may persistently remain high due to the intrinsic randomness
of the environment. This situation often results in the agent receiving high intrinsic
rewards for exploring parts of the environment it may already comprehend, albeit ap-
pearing different due to randomness. As a result, these methods often fall short in
generating a robust novelty measure capable of accurately distinguishing novel states
from those previously encountered by the agent (Yin et al., 2021).
Apart from such prediction error-based approaches, Houthooft et al. (2016) proposed
variational information maximizing exploration (VIME), which utilizes a Bayesian neu-
ral network to learn a state forward model and considers the information gain from the
network parameters as an intrinsic reward function. Although this method does not
suffer from the issue of subtle changes in states and high variability in stochastic envi-
ronments (issue 2) found in other prediction error-based methods, it still struggles with
issue 1: it relies on observations, which contain noisy or incomplete information about
the true states of the environment.
In contrast, our unified inference approach actively engages with the environment to
sequentially infer the information gain, which then determines the intrinsic reward of a
state. Our method not only encourages exploration but does so in a manner inherently
aligned with the agent‚Äôs imperative to reduce uncertainty and enhance its understanding
of the true states of the environment. Even when states exhibit similarities, the identifi-
cation of subtle differences that offer new insights about the environment contributes to
increased information gain. This approach provides a more nuanced and sensitive mea-
sure compared to traditional prediction error-based methods. Moreover, the incorpora-
tion of information gain encompasses the entire distribution of potential states, rather
than solely predicting the most probable next state. This broader perspective renders our
method more resilient to the variability introduced by stochasticity. The inclusion of in-
formation gain in our generalized actor-critic methods, as detailed in Sub-sections 8.1
and 8.2, not only enhances robustness to noisy observations but also improves sample
efficiency compared to other memory-based and exploration baselines.
iii) Flexibility in extrinsic reward design:Another significant advantage of our approach
over memory-based algorithms in POMDPs is its flexibility in designing extrinsic re-
wards. This flexibility stems from AIF‚Äôs inherent capability to integrate the information
gain term. In contrast, memory-based POMDP methods often rely heavily on well-
defined extrinsic reward functions, which can be limiting in complex environments
where specifying every desired outcome is impractical. Our method allows for a more
arbitrary design of extrinsic rewards, reducing dependence on explicit reward structures.
By reducing reliance on rigid reward structures, our method provides a robust frame-
work for developing more adaptable decision-making agents capable of operating in a
broader array of environments, especially where crafting specific rewards is challenging
or rewards are inherently sparse.
In conclusion, our proposed unified inference framework inherently embodies the
three aforementioned advantages: computational and memory efficiency, improved ex-
ploration through state information gain, and the ability to function without extrinsic
rewards. These benefits, derived from AIF, provide a comprehensive approach within
the free energy principle, surpassing memory-based RL methods.
7.1.2 Belief state inference-based approaches
Igl et al. (2018) introduced deep variational RL, which utilizes particle filtering to in-
fer belief states and learn an optimal Markovian policy. This approach minimizes the
ELBO to maximize the expected long-term extrinsic reward using the A2C algorithm.
Building upon this work, Lee et al. (2020) and Han et al. (2020) proposed stochastic
latent actor-critic (SLAC) and VRM, respectively, to extend SAC to POMDPs. SLAC
and VRM also employ the ELBO objective to learn belief states and generative mod-
els. SLAC focuses on pixel-based robotic control tasks, where velocity information is
inferred from third-person images of the robot. While SLAC utilizes the inferred belief
state solely in the critic network, VRM incorporates the belief state in both the actor
and critic networks. VRM does not utilize the generative model for action selection,
and as shown in Sub-section 6.1.1, VRM can be derived from our proposed model-free
actor-critic. It is important to note that these methods do not explicitly consider the
information gain associated with the inferred belief state for action selection.
Some other works, including Dreamer (Hafner et al., 2020) and PlaNet (Hafner et
al., 2019), learn belief state and generative model along with the extrinsic reward func-
tion model. These methods adopt a model-based learning approach that utilizes image
observations to maximize the expected long-term reward. However, unlike our infer-
ence model, Dreamer does not consider the belief state representation transition model
and, therefore, does not incorporate this representation into its variational distribution,
generative model, and reward model. As discussed in Sub-section 6.1.2, Dreamer can
be viewed as a special case of our proposed model-based unified actor-critic. A recent
evolution of Dreamer, called Dreamer-v2 (Hafner, Lillicrap, Norouzi, & Ba, 2021), has
been proposed; however, it is applicable only to discrete state spaces. It is important
to note that these model-based RL methods do not explicitly take into account the in-
formation gained regarding the inferred belief state for action selection. In contrast,
our proposed model-based unified actor-critic approach encourages the agent to engage
in information-seeking exploration, enabling it to leverage that information to reduce
uncertainty about the true states of the environment.
Additionally, X. Ma, Chen, Hsu, and Lee (2020) and Laskin, Srinivas, and Abbeel
(2020) focused on learning the generative model and belief state in pixel-based envi-
ronments with image-based observations, utilizing a form of consistency enforcement
known as contrastive learning between states and their corresponding observations.
However, contrastive learning poses distinct challenges that are beyond the scope of
our work. Nevertheless, our idea of using a hybrid model-based and model-free ap-
proach in our proposed unified actor-critic is inspired by their work.
Recently, there have been several RL works (Yin et al., 2021; Mazzaglia, Catal,
Verbelen, & Dhoedt, 2022; Klissarov, Islam, Khetarpal, & Precup, 2019) that have in-
troduced information gain as an intrinsic reward function for exploration. These meth-
ods involve inferring the belief state and learning a generative model by minimizing
the ELBO. They then use actor-critic RL methods to learn an optimal policy that max-
imizes both the expected long-term extrinsic reward and information gain. In these
approaches, the information gain term is added in an ad-hoc manner. In contrast, our
method provides a theoretical justification by extending the EFE within AIF to infinite
horizon POMDPs with continuous state, action, and observation spaces to incorporate
stochastic belief state-action policies.
Therefore, our unified inference method frames belief state inference, generative model
learning, and the integration of information gain as an intrinsic reward function, as-
pects that have previously been considered (partially) heuristic in those studies, under a
comprehensive interpretation of free energy principle (K. Friston et al., 2017).
7.2 AIF approaches for continuous space POMDP
As mentioned earlier, AIF approaches are mostly limited to discrete spaces or short,
finite-horizon POMDPs. This limitation arises from the computational expense of eval-
uating each plan based on its EFE and selecting the next action from the plan with the
lowest EFE. However, recent efforts have been made to extend AIF to POMDPs with
continuous observation and/or action spaces. These efforts include approaches that fo-
cus on deriving the optimal distribution over actions (i.e., state-action policy) instead
of plans or utilizing MCTS for plan selection. These advancements aim to address the
computational challenges associated with scaling AIF to continuous spaces.
7.2.1 State-action policy learning
Ueltzh¬®offer (2018) introduced a method where action at is sampled from a state-action
policy œÄ(at|st) instead of placing probabilities over a number of plans Àúa. They min-
imize the EFE by approximating its gradient with respect to œÄ(at|st). However, this
approach requires knowledge of the partial derivatives of the observation given the ac-
tion, which involves propagating through the unknown transition modelP(sœÑ+1|sœÑ , aœÑ ).
To address this challenge, Ueltzh¬®offer (2018) used a black box evolutionary genetic op-
timizer, which is considerably sample-inefficient.
Later, Millidge (2020) proposed a similar scheme that includes the transition model
P(sœÑ+1|sœÑ , aœÑ ). They heuristically set the optimal state-action policy as a Softmax func-
tion of the EFE. They introduced a recursive scheme to approximate the EFE through
a bootstrapping method inspired by deep Q-learning (Mnih et al., 2013, 2016), which
is limited to discrete action spaces. However, our approach is specifically designed for
continuous action spaces. In addition, it is important to note that their approach as-
sumes that the current hidden state is fully known to the agent after inference and does
not consider the agent‚Äôs belief state for action selection in the EFE and policy. In con-
trast, our approach focuses on learning the belief state representation, which is utilized
for action selection in the belief state-action EFE and belief state-action policy. We also
provide an analytical demonstration of the recursion in our proposed belief state-action
EFE and establish its convergence in Proposition 5.9 and Theorem 5.10. Furthermore,
we prove the expression œÄ‚àó(at|bt) = œÉ

‚àíG‚àó(bt,at)
Œ≤

in Theorem 5.7.
In a related context, K. Friston, Da Costa, Hafner, Hesp, and Parr (2021) explored a
recursive form of the EFE in a problem with discrete state and action spaces, consid-
ering it as a more sophisticated form of AIF. This approach involves searching over
sequences of belief states and considering the counterfactual consequences of actions
rather than just states and actions themselves. Building upon the work of Millidge
(2020), Mazzaglia et al. (2021) assumed that DKL[q(.|ot), P(.|st‚àí1, at‚àí1)] = 0 and fo-
cused on contrastive learning for the generative model and belief state in environments
with image-based observations.
In a subsequent study, Da Costa et al. (2023) investigated the relationship between
AIF and RL methods in finite-horizon fully observable problems modeled as MDPs.
They demonstrated that the optimal plan Àúa‚àó that minimizes the EFE also maximizes the
expected long-term reward in the RL setting, i.e.,arg minÀúa G(Àúa)
AIF(ot) ‚äÇ arg maxœÄ V (œÄ)(st).
Shin, Kim, and Hwang (2022) extended AIF to continuous observation spaces by show-
ing that the minimum of the EFE follows a recursive form similar to the Bellman op-
timality equation in RL. Based on this similarity, they derived a deterministic optimal
policy akin to deep Q-learning. However, their method is limited to discrete action
spaces and only provides a deterministic policy. In contrast, our approach is designed
for continuous action spaces and learns a stochastic policy, which enhances robustness
to environmental changes.
7.2.2 MCTS plan selection
Tschantz et al. (2020) extended AIF to continuous observation and action space prob-
lems by limiting the decision-making time horizon. They parametrized a probabil-
ity distribution over all possible plans and sampled multiple plans. Each sample was
weighted proportionately to its EFE value, and the mean of the sampling distribution
was then returned as the optimal plan Àúa‚àó. However, this solution cannot capture the
precise shape of the plans in AIF, as q‚àó(Àúa) = œÉ

‚àíG(Àúa)
AIF(ot)

, and is primarily suitable
for short time horizon problems.
Fountas et al. (2020) and Maisto et al. (2021) proposed an amortized version of MCTS
for plan selection. They consider the probability of choosing action at as the sum of
the probabilities of all the plans that begin with action at. However, these methods are
limited to discrete action spaces and are not applicable to continuous action spaces.
8 Experimental results
This section describes the experimental design used in our study to evaluate the ef-
fectiveness of our unified inference approach for extending RL and AIF methods to
POMDP settings with continuous state, action, and observation spaces. Our principal
aim is to compare the performance of our approach with state-of-the-art techniques
proposed in the literature. We assess our approach across various tasks character-
ized by partially observable continuous spaces, which are modeled as continuous space
POMDPs.
Furthermore, by focusing on the exploration behaviors, we compare the information
gain intrinsic reward in our unified inference approach with other exploration methods
in the RL literature.
Furthermore, we delve into the individual contributions of different components within
our framework, the extrinsic reward, intrinsic reward, and entropy reward term, to the
overall performance. To achieve this, we carry out a series of ablation studies on these
tasks, analyzing the effects of removing or modifying these components.
8.1 Comparative evaluation on partially observable continuous space
tasks
In this sub-section, we examine the effectiveness of our unified inference approach in
overcoming the limitations of existing RL and AIF algorithms when applied to partially
observable problems with continuous state, action, and observation spaces. To evaluate
its performance, we conducted experiments under two specific conditions of partial ob-
servability: (i) Partial state information (partial observations): In this setting, the agent
does not have access to complete information about the environment states. This con-
dition emulates scenarios where the agent has limited visibility into the true state of
the environment. (ii) Noisy state information (noisy observations): In this condition,
all observations received from the environment are noisy or contain inaccuracies. This
situation replicates real-world scenarios where observations are affected by noise or
errors, making it challenging to accurately estimate the underlying state of the environ-
ment.
Environments and tasks: The experimental evaluations encompassed four continu-
ous space Roboschool tasks (HalfCheetah, Walker2d, Hopper, and Ant) from the Py-
Bullet (Coumans & Bai, 2016), which is the replacement of the deprecated OpenAI
Roboschool (Brockman et al., 2016). These tasks have high-dimensional state spaces
characterized by quantities such as positions, angles, velocities (angular and linear),
and forces. Each episode in these tasks terminates on failure (e.g. when the hopper
or walker falls over). The choice of these environments is driven by two key factors:
(i) they offer challenging tasks with high-dimensional state spaces and sparse reward
functions, and (ii) recent efforts have been made to enhance the sample efficiency of
model-free and model-based RL methods in the partially observable variants of these
benchmarks, providing suitable baselines for comparison.
To create partially observable versions of these tasks, we made modifications to the
task environments. The first modification restricts the agent‚Äôs observations to velocities
only, transforming the tasks into partial observation tasks. This modification proposed
by Han et al. (2020) is relevant in real-world scenarios where agents may estimate their
speed but not have direct access to their position. For the noisy observation versions of
the tasks, we added zero-mean Gaussian noise with a standard deviation of œÉ = 0.05 to
the original states returned from the environment. This modification allows us to simu-
late realistic sensor noise in the environment. Further details of the modifications made
to create partially and noisy observable environments are provided in Appendix F. We
denote the partial observation modification and noisy observation modification of the
four tasks as {Hopper, Ant, Walker-2d, Cheetah }-{P} and {Hopper, Ant, Walker-2d,
Cheetah}-{N}, respectively.
Baselines: To evaluate the potential of the proposed unified inference framework in
extending MDP-based RL methods to POMDPs, we compare the performance of our
proposed model-free G-SAC and model-based G-Dreamer algorithms on the {Hopper,
Ant, Walker, Cheetah}-{P,N} tasks with the following state-of-the-art model-based and
model-free algorithms from the literature:
‚Ä¢ SAC: SAC (Haarnoja, Zhou, Abbeel, & Levine, 2018) is a model-free actor-critic
RL algorithm designed for MDPs. We include experiments showing the perfor-
mance of SAC based on true states (referred to as State-SAC) as an upper bound
on performance. The State-SAC serves as an oracle approximation representing
an upper bound on the performance that any POMDP method should strive to
achieve.
‚Ä¢ VRM: VRM (Han et al., 2020) is a POMDP-based actor-critic RL algorithm that
learns a generative model, infers the belief state, and constructs the state value
function in a model-free manner. By comparing our approach with VRM, we
can evaluate the impact of the information gain exploration term on model-free
learning in POMDP settings.
‚Ä¢ Dreamer: Dreamer (Hafner et al., 2020) is a model-based actor-critic RL method
designed for image observations. It learns a generative model, infers the belief
state, and learns the state value function through imagined trajectories using the
generative model. To compare our approach with Dreamer, we made some mod-
ifications to its implementation (see Appendix G). Despite these modifications,
we expect the comparison to demonstrate the effects of the information gain ex-
ploration term on model-based learning in POMDP settings.
‚Ä¢ Recurrent Model-Free: Recurrent Model-Free (Ni et al., 2022) is a model-free
memory-based RL algorithm for POMDPs. It explores different architectures, hy-
perparameters, and input configurations for the recurrent actor and recurrent value
function across Roboschool environments and selects the best-performing config-
uration. We consider Recurrent Model-Free as a baseline to compare the perfor-
mance of memory-based approaches with belief state inference-based methods in
partially observable environments.
Evaluation metrics: The performance of RL and AIF algorithms can be evaluated
using multiple metrics. In this sub-section, we assess the performance based on the
commonly used metric of cumulative extrinsic reward (return) after 1 million steps.
This metric quantifies the sum of all extrinsic rewards obtained by the agent up to that
point, providing an indication of the agent‚Äôs overall success in accomplishing its task
within the given time frame. Additionally, we evaluate the sample efficiency of each
baseline algorithm by measuring the number of steps required for them to reach the best
performance achieved at 1 million steps. It should be noted that the number of environ-
ment steps in each episode is variable, depending on the termination.
Table 3: The mean of the return on Roboschool tasks (partial and noisy observations)
averaged at the last 20% of the total 1 million environment steps across 5 seeds. The
model-free baseline State-SAC is used as a reference for the performance.
Task State-
SAC
SAC Dreamer VRM G-
SAC
G-
Dreamer
Recurrent
Model-Free
HalfCheetah-P 2994 193 1926 2938 3859 3413 1427
HalfCheetah-N 2994 ‚àí512 1294 2170 3655 3226 408
Hopper-P 2434 724 2043 1781 2468 2766 1265
Hopper-N 2434 466 1674 1343 2308 2448 771
Ant-P 3394 411 847 1503 2743 2325 994
Ant-N 3394 328 762 1256 2545 1996 367
Walker2d-P 2225 305 821 761 1837 2173 210
Walker2d-N 2225 123 578 424 1698 1956 116
Experimental setup: We implemented SAC, VRM, and Recurrent Model-Free us-
ing their original implementations on the Hopper, Ant, Walker, Cheetah-P,N tasks.
For state-SAC, we utilized the results from (Raffin, Kober, & Stulp, 2022). Regard-
ing Dreamer, we mostly followed the implementation described in the original pa-
per (Hafner et al., 2020). However, there was one modification: since their work
employed pixel observations, we replaced the convolutional neural networks (CNNs)
and transposed CNNs with two-layer multi-layer perceptrons (MLPs) consisting of256
units each for the variational posterior and likelihood model learning. Feed-forward
neural networks were used to represent the actors (policies) and critics (state-action
value functions or the negative of belief state-action EFE). To ensure a fair comparison,
we maintained identical hyperparameters for the actor and critic models of Dreamer,
G-SAC, G-Dreamer, and VRM.
The same set of hyperparameters was used for both the noisy observation and partial
observation versions of each task. We trained each algorithm for a total of 1 million
environment steps on each task and conducted each experiment with5 different random
seeds. For additional information regarding the model architectures, please refer to Ap-
pendix G.
Results: Table 3 provides a summary of the results, displaying the mean return aver-
aged over the last 20% of the total 1 million environment steps across the five random
seeds. The complete learning curves can be found in Appendix I. We will now analyze
the quantitative results in the following:
(i) Unified inference model successfully generalizes model-free and model-based RL to
POMDPs. As expected, SAC encountered difficulties in solving the tasks with partial
and noisy observations due to its MDP-based implementation. In contrast, our proposed
G-SAC algorithm demonstrated superior performance compared to SAC. Furthermore,
while Dreamer and VRM are regarded as state-of-the-art methods for POMDP tasks,
G-Dreamer and G-SAC consistently outperformed them in all scenarios. This empha-
sizes the advantages of leveraging the belief state representation in both the actor and
critic, along with the information gain term in our G-Dreamer and G-SAC algorithms.
The number of steps required for Dreamer and VRM to match the performance of G-
Dreamer and G-SAC at the end of 1 million steps was significantly higher, indicating
that the information gain intrinsic term in G-SAC and G-Dreamer enhances sample ef-
ficiency. It should be noted that while G-SAC and G-Dreamer outperform Dreamer
and VRM algorithms in terms of final performance and sample efficiency in both the
partial observation and noisy observation cases, they achieve these improvements with
comparable computational requirements to the compared algorithms.
Remarkably, both our proposed methods, G-SAC and G-Dreamer, performed on par
with, or even surpassed, the oracle State-SAC framework, which has access to the true
state of the environment. This further confirms: 1) Effectiveness of the perceptual infer-
ence and learning method for inferring the belief state-conditioned variational posterior
q(st|ot, bt‚àí1) and the belief state generative model P(ot, st, bt|at‚àí1, bt‚àí1), as well as the
belief state representation model for learning a belief representation ht, used in our
approaches. 2) Effectiveness of the information-seeking exploration facilitated by the
intrinsic reward term in our unified objective function G(œÄ)(bt).
(ii) Belief state inference-based approaches are more robust than memory-based base-
line given noisy observations. Belief state inference-based approaches, such as VRM,
Dreamer, and G-SAC, consistently outperform the memory-based baseline, Recurrent
Model-Free, in both partial observations and noisy observations settings. This supe-
riority can be attributed to the ability of VRM, Dreamer, and G-SAC to encode the
observations and actions history into the variational posterior, enabling more effective
encoding of underlying states compared to Recurrent Model-Free when dealing with
velocity observations or noisy observations. The performance gap between these ap-
proaches becomes more pronounced in the case of noisy observations, highlighting the
advantage of inferring the belief state for use in the actor and critic, especially in the
presence of observation noise. This advantage is linked to the complexity inherent in
noisy environments, where various observations may correspond to a single underly-
ing state due to noise interference. Belief state methods address this uncertainty by
maintaining a probability distribution over possible states, allowing them to consolidate
information from multiple noisy inputs and update their beliefs accordingly. Thus, even
when faced with differing noisy observations that might relate to the same state, belief
state methods can assimilate this uncertainty and assign probabilities to each possible
state based on the collected data. Conversely, memory-based methods often falter un-
der such uncertainty. These methods typically depend on storing and processing an
extensive history of observations and actions, which can become cumbersome in noisy
settings where observations might be ambiguous or misleading. Lacking a structured
way to handle and update uncertainty, memory-based approaches may struggle to differ-
entiate between observations leading to the same underlying state, potentially resulting
in inferior decision-making.
It is noteworthy that while memory-based methods like Recurrent Model-Free typically
result in higher memory consumption due to storing extensive past observations and
actions, belief state inference methods such as VRM, G-SAC, and G-Dreamer often re-
quire less memory. However, they may necessitate additional computational resources
for inference and learning the generative model. Nonetheless, as elaborated in Ap-
pendix H, the computational burden induced by ongoing belief state inference and gen-
erative model learning in VRM and our proposed G-SAC and G-Dreamer frameworks
is generally lower than that of processing a sequence of past actions and observations
through an RNN, as seen in Recurrent Model-Free. This contrast is especially no-
ticeable in large or infinite time horizon problems with high-dimensional action and
observation spaces.
(iii) Maximizing expected long-term information gain improves robustness to the noisy
observations. While the performance of VRM and Dreamer degraded from the par-
tial observation setting to the noisy observation setting, G-SAC and G-Dreamer were
able to maintain comparable performance in the presence of observation noise. This
robustness to observation noise can be attributed to the KL-divergence term in the in-
trinsic information-seeking term, as highlighted by Hafner et al. (2022) in the context
of divergence minimization frameworks.
The results presented in this sub-section highlight the effectiveness of the proposed
unified inference algorithm in various aspects: (i) It successfully generalizes MDP
actor-critic methods to the POMDP setting, allowing for more effective exploration
and learning under partial observability. (ii) It outperforms memory-based approaches
in scenarios with noisy observations, indicating the advantage of leveraging the belief
state representation in handling observation noise.(iii) The inclusion of the information
gain intrinsic term into the generalized actor-critic methods improves their robustness
to noisy observations.
8.2 Comparative evaluation of exploration methods
In this sub-section, we evaluate the performance of our unified inference approach,
employing information gain as an intrinsic reward for exploration, in contrast to RL
methods incorporating alternative exploratory intrinsic rewards. We particularly inves-
tigate its efficacy across deterministic and stochastic partially observable environments.
To facilitate this analysis, we utilize extrinsic reward-free agents dedicated solely to
exploration. This emphasis enables us to comprehensively scrutinize and highlight the
exploratory behaviors exhibited by the agents.
Environments and tasks: We conduct a series of experiments on a partially observ-
able variant of the MountainCarContinuous-v0 environment, where only the velocity
is observable. The problem‚Äôs state space is continuous and includes the car‚Äôs position
and velocity along the horizontal axis. The action space is one-dimensional, allowing
control over the force applied to the car for movement, and the transition function is
deterministic. We chose the MountainCarContinuous-v0 task as it is relatively easy to
solve when extrinsic rewards are available. Thus, by considering the reward-free case,
we can emphasize the exploration challenge and evaluate the effectiveness of different
exploration methods. As stated in Sub-section 7.1.1, exploration based on the prediction
error of a state forward model is sensitive to the inherent stochasticity of the environ-
ment (Burda et al., 2018). Therefore, we also performed an experiment on the same
task but with a stochastic transition function. We used the stochastic version of the en-
vironment introduced in (Mazzaglia, Catal, et al., 2022), which adds a one-dimensional
state referred to as the NoisyState, and a one-dimensional action ranging from [‚àí1, 1]
that acts as a remote for the NoisyState. When this action‚Äôs value is higher than 0,
the remote is triggered, updating the NoisyState value by sampling uniformly from the
[‚àí1, 1] interval.
Baselines: We compare G-SAC against the following RL frameworks that incorporate
exploratory terms as intrinsic rewards in SAC:
‚Ä¢ ICM: ICM (Pathak et al., 2017) is a prediction error-based RL algorithm that gen-
erates intrinsic rewards through a state forward-backward dynamics model. The
main source of intrinsic reward is the prediction error from the forward model.
The backward transition model supports state feature learning by reconstructing
the previous state‚Äôs features.
‚Ä¢ RND: RND (Burda et al., 2018) is a prediction error-based RL method where
state features are learned using a fixed, randomly initialized neural network. In-
trinsic rewards are calculated based on the prediction errors between the next
state features and the outputs of a distillation network. This distillation network
is continuously trained to emulate the outputs of the randomly initialized feature
network.
‚Ä¢ VIME VIME (Houthooft et al., 2016) employs a Bayesian neural network to
learn the forward model. It utilizes intrinsic rewards based on the information
gain about the parameters of the Bayesian network. These rewards are quantified
by the change in information before and after updating the network with data
from new interactions.
Performance metrics: We measure exploration ability directly by calculating an agent‚Äôs
environment coverage. Following the approach in (Mazzaglia, Catal, et al., 2022), we
discretize the state space into 100 bins and evaluate the coverage percentage of the
number of bins explored. An agent visiting a certain bin corresponds to the agent suc-
cessfully accomplishing a task that requires reaching that particular area of the state
space. Hence, it is crucial that a good exploration method can explore as many bins as
possible.
Experimental Setup: For G-SAC, we use the same model architectures and hyper-
parameters described in Sub-section 8.1 of the main manuscript. Since ICM, RND,
and VIME were originally developed for fully observable environments, we adapted
them for our partially observable setting of MountainCarContinuous-v0. Specifically,
the forward and backward models are reconstructed based on observations rather than
fully observable states. Consequently, the state-action policies are changed to history-
dependent policies, as the observations no longer have the Markovian property. This
adaptation allows both our method and these three baselines to incorporate exploratory
intrinsic rewards, enabling a fair comparison.
We then train G-SAC and the baselines on the deterministic and stochastic partially ob-
servable MountainCarContinuous-v0 for a total of100 episodes. An episode terminates
when the agent reaches the goal or the episode length exceeds 1000 steps.
Results: Fig. 2 presents training curves averaged over 10 different random seeds. The
results show that methods utilizing intrinsic rewards based on information gain, specif-
ically G-SAC and VIME, learn significantly faster in deterministic environments. This
accelerated learning suggests that their exploration mechanisms are more effective than
those of ICM and RND, which rely on prediction error-based exploration. The superior-
ity of information gain-based methods largely stems from their effectiveness in handling
state similarities within continuous state spaces. Unlike prediction error-based methods
that struggle to differentiate between subtly different states, information gain methods
assess the entire distribution of possible states, enabling more precise and meaningful
exploration even in the presence of similar states. Notably, G-SAC performs slightly
better than VIME because it directly assesses uncertainty reduction based on inferred
states, while VIME considers uncertainty reduction based on observations, which in-
clude the car‚Äôs velocity but not its position. Therefore, VIME cannot effectively capture
uncertainty reduction in the car‚Äôs position for exploration.
Moreover, although VIME explores less than G-SAC, both methods demonstrate re-
silience to stochasticity. In contrast, the performance of ICM and RND is significantly
compromised by randomness, with ICM being the most adversely affected. It is well-
documented that intrinsic motivation strategies based on the prediction error of a for-
ward model are vulnerable to the inherent stochasticity of the environment (Burda et
al., 2018). This degradation is largely due to the inherent stochasticity of environments
where outcomes vary unpredictably following the same action, causing persistently high
prediction errors and leading to misguided exploration efforts. This highlights the vul-
nerability of prediction error-based intrinsic motivation strategies in stochastic settings.
In conclusion, information gain-based methods like G-SAC and VIME, which re-
spectively assess the entire distribution of possible latent states and parameters of neural
networks generating next states rather than just the most likely ones, demonstrate ro-
bustness against subtle changes in an agent‚Äôs state and the variability introduced by
stochastic conditions. This comprehensive consideration of potential states and param-
eters significantly improves their effectiveness, especially in stochastic environments
with continuous state and action spaces.
(a) Partially observable deterministic mountain
car
(b) Partially observable stochastic mountain
car
Fig. 2. The average state-space coverage in terms of percentage of bins visited by the
agents for deterministic and stochastic partially observable MountainCarContinuous-
v0. The more state space coverage in an episode, the better the agent explores the
environment and thus performs in that episode.
8.3 Ablation studies
In this sub-section, we conduct a comprehensive ablation study on the partial obser-
vation variants of the four Roboschool tasks discussed in Sub-section 8.1, namely
{Hopper, Ant, Walker-2d, Cheetah }-{P}. The primary objective of this study is to
gain a deeper understanding of the contribution of each individual component in the
proposed unified actor-critic framework. We evaluate the performance based on the
average return across 5 different random seeds.
8.3.1 Stochastic policy versus deterministic policy
The proposed G-SAC framework learns an optimal stochastic policy by minimizing the
loss functions corresponding to the belief state-action EFE update step (Eq. (66)) and
policy update step (Eq. (67)), with the policy entropy term included in both of these
loss functions. In the belief state-action EFE update step, the entropy term encourages
exploration by reducing the belief state-action EFE in regions of the state space that
lead to high-entropy behavior. In the policy update step, the entropy term helps prevent
premature policy convergence.
To assess the impact of policy stochasticity (policy entropy term) on G-SAC‚Äôs perfor-
mance, we compare it to an algorithm called G-DDPG, obtained by setting Œ≤ = 0 in
G-SAC. G-DDPG is a generalization of the MDP-based Deep Deterministic Policy Gra-
dient (DDPG) algorithm (Silver et al., 2014) to POMDPs, where a deterministic policy
is learned by removing the policy entropy term from the belief state-action EFE.
Experimental setup: For G-DDPG, we utilize identical hyperparameters and network
architectures as those employed in G-SAC. We train G-DDPG on Hopper, Ant, Walker-
2d, Cheetah-P for a total of 1 million time steps using 5 different random seeds.
Results: Fig. 3 presents the performance comparison between G-SAC and G-DDPG.
The results indicate that G-DDPG suffers from premature convergence due to the ab-
sence of the entropy term. Furthermore, G-DDPG exhibits a higher standard deviation,
resulting in reduced stability compared to G-SAC. This finding suggests that learn-
ing a stochastic policy with policy entropy maximization in POMDPs with uncertain
environmental states can significantly enhance training stability, particularly for more
challenging tasks where hyperparameter tuning can be difficult.
8.3.2 Model-based actor-critic vs hybrid actor-critic
When Œ± = 1 in the hybrid unified actor-critic method, the critic loss functionLHybrid
¬ØœÄ (œà)
combines the critic loss functions of G-SAC (LG-SAC
G (œà)) and G-Dreamer (LG-Dreamer
G (œà))
(a) HalfCheetah-P
 (b) Hopper-P
(c) Ant-P
 (d) Walker2d-P
Fig. 3. Ablation study comparing the average return of G-SAC and G-DDPG algorithms
across the partial observation version of Roboschool tasks.
using a scaling factor c. We adopt a hybrid G-Dreamer-SAC approach by setting c to
1 in LHybrid
¬ØœÄ (œà). To assess the influence of the G-SAC component on the performance
of the hybrid G-Dreamer-SAC, we compare its performance with that of G-Dreamer,
which can be considered a special case of the hybrid unified actor-critic when c = 0.
Results: Fig. 4 presents a performance comparison between G-Dreamer and G-Dreamer-
SAC. G-Dreamer-SAC outperforms G-Dreamer on HalfCheetah-P and Ant-P, while
performing on par with G-Dreamer on Hopper-P and Walker2d-P. This discrepancy can
be attributed to the higher number of hidden values in the state vector that need to be
inferred solely from velocities in HalfCheetah-P and Ant-P, compared to Hopper-P and
Walker2d-P. Accurately learning the belief state and generative model becomes more
challenging when a larger number of states are unknown. Therefore, relying solely on
the learned generative model for actor and critic learning in HalfCheetah-P and Ant-
P leads to inaccurate data trajectories. However, by utilizing ground-truth trajectories
from the replay buffer, the G-SAC component of G-Dreamer-SAC can provide accu-
rate data to compensate for the compositional errors of the generative models. As a
(a) HalfCheetah-P
 (b) Hopper-P
(c) Ant-P
 (d) Walker2d-P
Fig. 4. Ablation study comparing the average return of the hybrid G-Dreamer-SAC
algorithm and the model-based G-Dreamer algorithm across four partial observation
versions of Roboschool tasks.
result, G-Dreamer-SAC benefits from the sample efficiency of model-based learning
while maintaining the robustness to complex observations, which are characteristic of
model-free learning.
9 Conclusion and Perspectives
In this paper, we extended the EFE formulation to stochastic Markovian belief state-
action policies, which allowed for a unified objective function formulation encom-
passing both exploitation of extrinsic rewards and information-seeking exploration in
POMDPs. We then introduced a unified policy iteration framework to optimize this
objective function and provided proof of its convergence to the optimal solution. Our
proposed unified policy iteration framework not only generalized existing RL and AIF
algorithms but also revealed a theoretical relationship between them, showing that the
belief state EFE can be interpreted as a negative state value function. Additionally, our
method successfully scaled up AIF to tasks with continuous state and action spaces
and enhanced actor-critic RL algorithms to handle POMDPs while incorporating an
inherent information-seeking exploratory term. We evaluated our approach on high-
dimensional Roboschool tasks with partial and noisy observations, and our unified pol-
icy iteration algorithm outperformed recent alternatives in terms of expected long-term
reward, sample efficiency, and robustness to estimation errors, while requiring com-
parable computational resources. Furthermore, our experimental results indicated that
the proposed information-seeking exploratory behavior is effective in guiding agents
towards their goals even in the absence of extrinsic rewards, making it possible to op-
erate in reward-free environments without the need for external supervisors to define
task-specific reward functions.
However, there are still challenges to overcome, particularly in scaling AIF to high-
dimensional environments, such as those based on images. Accurate variational pos-
terior and generative models are required to reconstruct observations in detail. One
potential solution is to incorporate a state-consistency loss that enforces consistency
between states and their corresponding observations, which has shown promise in self-
supervised learning methods (He, Fan, Wu, Xie, & Girshick, 2020; Grill et al., 2020).
We plan to explore the combination of our unified inference agents with such learning
methods in RL.
Another exciting direction for future research is to investigate the impact of information-
seeking exploration in a multi-task setting, where initially explored agents may benefit
from transferring knowledge across tasks.
References
Bellman, R. (1952). On the theory of dynamic programming. Proceedings of
the National Academy of Sciences of the United States of America , 38(8),
716.
Boyd, S. P., & Vandenberghe, L. (2004). Convex optimization. Cambridge
university press.
Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
& Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.
Burda, Y ., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by
random network distillation. arXiv preprint arXiv:1810.12894.
Chatterjee, K., Chmelik, M., & Tracol, M. (2016). What is decidable about
partially observable markov decision processes with œâ-regular objectives.
Journal of Computer and System Sciences, 82(5), 878‚Äì911.
Choi, J., Guo, Y ., Moczulski, M., Oh, J., Wu, N., Norouzi, M., & Lee, H. (2018).
Contingency-aware exploration in reinforcement learning. arXiv preprint
arXiv:1811.01483.
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., & Bengio, Y . (2015).
A recurrent latent variable model for sequential data. Advances in neural
information processing systems, 28.
Coumans, E., & Bai, Y . (2016). Pybullet, a python module for physics simula-
tion for games, robotics and machine learning.(2016). URL http://pybullet.
org.
Da Costa, L., Sajid, N., Parr, T., Friston, K., & Smith, R. (2023). Reward max-
imization through discrete active inference. Neural Computation, 35(5),
807‚Äì852.
Dai, T., Du, Y ., Fang, M., & Bharath, A. A. (2022). Diversity-augmented
intrinsic motivation for deep reinforcement learning.Neurocomputing, 468,
396‚Äì406.
Dong, Y ., Zhang, S., Liu, X., Zhang, Y ., & Shen, T. (2021). Variance aware
reward smoothing for deep reinforcement learning. Neurocomputing, 458,
327‚Äì335.
Dufour, F., & Prieto-Rumeau, T. (2012). Approximation of markov decision
processes with general state space. Journal of Mathematical Analysis and
applications, 388(2), 1254‚Äì1267.
Dufour, F., & Prieto-Rumeau, T. (2013). Finite linear programming approxima-
tions of constrained discounted markov decision processes. SIAM Journal
on Control and Optimization, 51(2), 1298‚Äì1324.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active infer-
ence agents using monte-carlo methods. arXiv preprint arXiv:2006.04176.
Friston, K., Da Costa, L., Hafner, D., Hesp, C., & Parr, T. (2021). Sophisticated
inference. Neural Computation, 33(3), 713‚Äì763.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017).
Active inference: a process theory. Neural computation, 29(1), 1‚Äì49.
Friston, K., Mattout, J., & Kilner, J. (2011). Action understanding and active
inference. Biological cybernetics, 104(1), 137‚Äì160.
Friston, K., Samothrakis, S., & Montague, R. (2012). Active inference and
agency: optimal control without cost functions. Biological cybernetics,
106(8), 523‚Äì541.
Friston, K. J., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Action and
behavior: a free-energy formulation. Biological cybernetics, 102(3), 227‚Äì
260.
Gregor, K., Jimenez Rezende, D., Besse, F., Wu, Y ., Merzic, H., & van den Oord,
A. (2019). Shaping belief states with generative environment models for
rl. Advances in Neural Information Processing Systems, 32.
Grill, J.-B., Strub, F., Altch¬¥e, F., Tallec, C., Richemond, P., Buchatskaya, E., . . .
others (2020). Bootstrap your own latent-a new approach to self-supervised
learning. Advances in neural information processing systems , 33, 21271‚Äì
21284.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning with a stochastic
actor. In International conference on machine learning (pp. 1861‚Äì1870).
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., . . . oth-
ers (2018). Soft actor-critic algorithms and applications. arXiv preprint
arXiv:1812.05905.
Hafner, D., Lillicrap, T., Ba, J., & Norouzi, M. (2020). Dream to control:
Learning behaviors by latent imagination. In International conference on
learning representations.
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J.
(2019). Learning latent dynamics for planning from pixels. InInternational
conference on machine learning (pp. 2555‚Äì2565).
Hafner, D., Lillicrap, T. P., Norouzi, M., & Ba, J. (2021). Mastering atari with
discrete world models. In International conference on learning represen-
tations.
Hafner, D., Ortega, P. A., Ba, J., Parr, T., Friston, K., & Heess, N.
(2022). Action and perception as divergence minimization. arXiv preprint
arXiv:2009.01791.
Haklidir, M., & Temeltas ¬∏, H. (2021). Guided soft actor critic: A guided deep
reinforcement learning approach for partially observable markov decision
processes. IEEE Access, 9, 159672‚Äì159683.
Han, D., Doya, K., & Tani, J. (2020). Variational recurrent models for solving
partially observable control tasks. In International conference on learning
representations.
Hausknecht, M., & Stone, P. (2015). Deep recurrent q-learning for partially
observable mdps. In 2015 aaai fall symposium series.
He, K., Fan, H., Wu, Y ., Xie, S., & Girshick, R. (2020). Momentum contrast for
unsupervised visual representation learning. In Proceedings of the ieee/cvf
conference on computer vision and pattern recognition (pp. 9729‚Äì9738).
Heess, N., Hunt, J. J., Lillicrap, T. P., & Silver, D. (2015). Memory-based
control with recurrent neural networks. arXiv preprint arXiv:1512.04455.
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural
computation, 9(8), 1735‚Äì1780.
Houthooft, R., Chen, X., Duan, Y ., Schulman, J., De Turck, F., & Abbeel, P.
(2016). Vime: Variational information maximizing exploration. Advances
in neural information processing systems, 29.
Igl, M., Zintgraf, L., Le, T. A., Wood, F., & Whiteson, S. (2018). Deep varia-
tional reinforcement learning for pomdps. In International conference on
machine learning (pp. 2117‚Äì2126).
Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv
preprint arXiv:1312.6114.
Klissarov, M., Islam, R., Khetarpal, K., & Precup, D. (2019). Variational
state encoding as intrinsic motivation in reinforcement learning. In Task-
agnostic reinforcement learning workshop at proceedings of the interna-
tional conference on learning representations (V ol. 15, pp. 16‚Äì32).
Kochenderfer, M. J. (2015). Decision making under uncertainty: theory and
application. MIT press.
Krishnamurthy, V . (2015). Structural results for partially observed markov de-
cision processes. arXiv preprint arXiv:1512.03873.
Lanillos, P., Meo, C., Pezzato, C., Meera, A. A., Baioumy, M., Ohata, W., . . .
others (2021). Aif in robotics and artificial agents: Survey and challenges.
arXiv preprint arXiv:2112.01871.
Laskin, M., Srinivas, A., & Abbeel, P. (2020). Curl: Contrastive unsupervised
representations for reinforcement learning. In International conference on
machine learning (pp. 5639‚Äì5650).
Lee, A. X., Nagabandi, A., Abbeel, P., & Levine, S. (2020). Stochastic la-
tent actor-critic: Deep reinforcement learning with a latent variable model.
Advances in Neural Information Processing Systems, 33, 741‚Äì752.
Leibfried, F., Pascual-Diaz, S., & Grau-Moya, J. (2019). A unified bellman
optimality principle combining reward maximization and empowerment.
In Advances in neural information processing systems (V ol. 32). Curran
Associates, Inc.
Likmeta, A., Sacco, M., Metelli, A. M., & Restelli, M. (2022). Directed ex-
ploration via uncertainty-aware critics. In Decision awareness in reinforce-
ment learning workshop at icml.
Ma, X., Chen, S., Hsu, D., & Lee, W. S. (2020). Contrastive variational model-
based reinforcement learning for complex observations. In In proceedings
of the 4th conference on robot learning, virtual conference.
Ma, X., Chen, S., Hsu, D., & Lee, W. S. (2021). Contrastive variational re-
inforcement learning for complex observations. In Conference on robot
learning (pp. 959‚Äì972).
Ma, Y ., Zhao, T., Hatano, K., & Sugiyama, M. (2016). An online policy gra-
dient algorithm for markov decision processes with continuous states and
actions. Neural Computation, 28(3), 563‚Äì593.
Madani, O., Hanks, S., & Condon, A. (1999). On the undecidability of proba-
bilistic planning and infinite-horizon partially observable markov decision
problems. In Aaai/iaai (pp. 541‚Äì548).
Maddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., & Wilson, A. G. (2019).
A simple baseline for bayesian uncertainty in deep learning. Advances in
Neural Information Processing Systems, 32.
Maisto, D., Gregoretti, F., Friston, K., & Pezzulo, G. (2021). Active tree search
in large pomdps. arXiv preprint arXiv:2103.13860.
Malekzadeh, P., Salimibeni, M., Hou, M., Mohammadi, A., & Plataniotis, K. N.
(2022). Akf-sr: Adaptive kalman filtering-based successor representation.
Neurocomputing, 467, 476‚Äì490.
Malekzadeh, P., Salimibeni, M., Mohammadi, A., Assa, A., & Plataniotis, K. N.
(2020). Mm-ktd: multiple model kalman temporal differences for rein-
forcement learning. IEEE Access, 8, 128716‚Äì128729.
Mavrin, B., Yao, H., Kong, L., Wu, K., & Yu, Y . (2019). Distributional rein-
forcement learning for efficient exploration. InInternational conference on
machine learning (pp. 4424‚Äì4434).
Mazzaglia, P., Catal, O., Verbelen, T., & Dhoedt, B. (2022). Curiosity-driven
exploration via latent bayesian surprise. In Proceedings of the aaai confer-
ence on artificial intelligence (V ol. 36, pp. 7752‚Äì7760).
Mazzaglia, P., Verbelen, T., C ¬∏ atal, O., & Dhoedt, B. (2022). The free energy
principle for perception and action: A deep learning perspective. Entropy,
24(2), 301.
Mazzaglia, P., Verbelen, T., & Dhoedt, B. (2021). Contrastive active inference.
Advances in Neural Information Processing Systems, 34, 13870‚Äì13882.
Meng, L., Gorbet, R., & Kuli ¬¥c, D. (2021). Memory-based deep reinforcement
learning for pomdps. In 2021 ieee/rsj international conference on intelli-
gent robots and systems (iros) (pp. 5619‚Äì5626).
Millidge, B. (2020). Deep active inference as variational policy gradients. Jour-
nal of Mathematical Psychology, 96, 102348.
Millidge, B., Tschantz, A., & Buckley, C. L. (2021). Whence the expected free
energy? Neural Computation, 33(2), 447‚Äì482.
Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., . . .
Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforce-
ment learning. In International conference on machine learning(pp. 1928‚Äì
1937).
Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
& Riedmiller, M. (2013). Playing atari with deep reinforcement learning.
arXiv preprint arXiv:1312.5602.
Montufar, G., Ghazi-Zahedi, K., & Ay, N. (2015). Geometry and determin-
ism of optimal stationary control in partially observable markov decision
processes. arXiv preprint arXiv:1503.07206.
Ni, T., Eysenbach, B., & Salakhutdinov, R. (2022). Recurrent model-free rl
can be a strong baseline for many pomdps. In International conference on
machine learning (pp. 16691‚Äì16723).
Nian, X., Irissappane, A. A., & Roijers, D. (2020). Dcrac: Deep conditioned
recurrent actor-critic for multi-objective partially observable environments.
In Proceedings of the 19th international conference on autonomous agents
and multiagent systems (pp. 931‚Äì938).
Ogishima, R., Karino, I., & Kuniyoshi, Y . (2021). Reinforced imitation learning
by free energy principle. arXiv preprint arXiv:2107.11811.
Oh, C., & Cavallaro, A. (2019). Learning action representations for self-
supervised visual exploration. In 2019 international conference on robotics
and automation (icra) (pp. 5873‚Äì5879).
Okuyama, T., Gonsalves, T., & Upadhay, J. (2018). Autonomous driving system
based on deep q learnig. In 2018 international conference on intelligent
autonomous systems (icoias) (pp. 201‚Äì205).
Pascanu, R., Mikolov, T., & Bengio, Y . (2013). On the difficulty of training
recurrent neural networks. InInternational conference on machine learning
(pp. 1310‚Äì1318).
Paternain, S., Bazerque, J. A., & Ribeiro, A. (2020). Policy gradient for con-
tinuing tasks in non-stationary markov decision processes. arXiv preprint
arXiv:2010.08443.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven
exploration by self-supervised prediction. In International conference on
machine learning (pp. 2778‚Äì2787).
Pezzulo, G., Rigoli, F., & Friston, K. (2015). Active inference, homeostatic
regulation and adaptive behavioural control.Progress in neurobiology, 134,
17‚Äì35.
Puterman, M. L. (2014). Markov decision processes: discrete stochastic dy-
namic programming. John Wiley & Sons.
Raffin, A., Kober, J., & Stulp, F. (2022). Smooth exploration for robotic rein-
forcement learning. In Conference on robot learning (pp. 1634‚Äì1644).
Ramicic, M., & Bonarini, A. (2021). Uncertainty maximization in par-
tially observable domains: A cognitive perspective. arXiv preprint
arXiv:2102.11232.
Russell, S. J. (2010). Artificial intelligence a modern approach. Pearson Edu-
cation, Inc.
Sajid, N., Tigas, P., Zakharov, A., Fountas, Z., & Friston, K. (2021). Exploration
and preference satisfaction trade-off in reward-free learning.arXiv preprint
arXiv:2106.04316.
Savinov, N., Raichuk, A., Marinier, R., Vincent, D., Pollefeys, M., Lillicrap, T.,
& Gelly, S. (2019). Episodic curiosity through reachability. In Interna-
tional conference on learning representations.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust re-
gion policy optimization. In International conference on machine learning
(pp. 1889‚Äì1897).
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Prox-
imal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
Shin, J. Y ., Kim, C., & Hwang, H. J. (2022). Prior preference learning from
experts: Designing a reward with active inference. Neurocomputing, 492,
508‚Äì515.
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M.
(2014). Deterministic policy gradient algorithms. In International con-
ference on machine learning (pp. 387‚Äì395).
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.
MIT press.
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020). Scaling active
inference. In 2020 international joint conference on neural networks (ijcnn)
(pp. 1‚Äì8).
Tucker, G., Bhupatiraju, S., Gu, S., Turner, R., Ghahramani, Z., & Levine, S.
(2018). The mirage of action-dependent baselines in reinforcement learn-
ing. In International conference on machine learning (pp. 5015‚Äì5024).
Ueltzh¬®offer, K. (2018). Deep active inference. Biological cybernetics, 112(6),
547‚Äì573.
von Helmholtz, H. (2001). Concerning the perceptions in general. Visual per-
ception, 24‚Äì44.
Wright, J. N. S. J. (2006). Numerical optimization. springer publication.
Yang, Z., & Nguyen, H. (2021). Recurrent off-policy baselines for memory-
based continuous control. arXiv preprint arXiv:2110.12628.
Yin, H., Chen, J., Pan, S. J., & Tschiatschek, S. (2021). Sequential genera-
tive exploration model for partially observable reinforcement learning. In
Proceedings of the aaai conference on artificial intelligence (V ol. 35, pp.
10700‚Äì10708).
Zhu, P., Li, X., Poupart, P., & Miao, G. (2017). On improving deep reinforce-
ment learning for pomdps. arXiv preprint arXiv:1704.07978.
Appendices
Appendix A Markovian belief state-action versus history-dependent policies: mem-
ory and computational complexity
In POMDPs, each time step introduces one new action and one new observation to the
history, adding DO + DA-dimensional data to the existing history. Consequently, start-
ing with a DO-dimensional observation o0, the total dimensionality of the history by
time t is given by DO + (DO + DA) √ó t. This increase in the dimensionality of inputs
necessitates more substantial memory for storage and larger matrix operations at each
step, escalating computational demands.
Furthermore, the addition of one observation and one action to the history at each time
step leads to an exponential increase in the number of possible histories. Specifically,
this number expands at each step by a factor of (|‚Ñ¶| √ó |A|), reflecting the agent‚Äôs |A|
possible actions and |‚Ñ¶| possible observations. Thus, starting from any of |‚Ñ¶| poten-
tial initial observations o0 ‚àà ‚Ñ¶, the number of possible histories grows exponentially
as (|‚Ñ¶| √ó |A|)t across t time steps, resulting in a total number of histories at time t
of |‚Ñ¶| √ó(|‚Ñ¶| √ó |A|)t. Consequently, the size of the policy space, which represents
the number of possible policies derived from these histories, also undergoes the same
exponential growth. This exponential growth over time significantly heightens mem-
ory and computational requirements to store and evaluate each possible history and its
corresponding policy. These challenges are particularly pronounced in scenarios with
high-dimensional action and observation spaces, where DA and DO are large, and in
environments with extensive or continuous action and observation spaces, where |A|
and |‚Ñ¶| are also large. Additionally, these challenges are accentuated in environments
with long time horizons, where t is substantial.
Unlike history-dependent policies, where the length of history grows linearly, be-
lief states in belief state-action policies maintain a constant size, |S|, at each time step.
Moreover, the size of all possible belief states (policy space) remains fixed as|B|. Con-
sequently, the memory required to store each belief state and its corresponding pol-
icy does not inherently increase over time, offering significant memory efficiency ad-
vantages over history-dependent approaches, particularly in environments with infinite
horizons or high-dimensional action and observation spaces. However, storing belief
states can be memory-intensive, especially in large or continuous state spaces where
|S| is large. Nonetheless, this memory requirement is generally more manageable com-
pared to the expansion of history dimension and policy space in history-dependent poli-
cies (Yang & Nguyen, 2021).
To infer the belief state using Bayesian inference (as detailed in Sub-section 3.4), the
agent must learn the structure of the environment‚Äôs observation and transition functions
through a generative model. While belief state-action policies offer memory efficiency
advantages, their computational complexity introduces distinct challenges, primarily
contingent on the learning process for the generative model and the belief state update
mechanism. Nonetheless, particularly in environments with infinite horizons, belief
state-action policies often achieve greater computational efficiency over time by utiliz-
ing fixed-dimension inputs, which streamline processing and ensure consistent compu-
tational demands regardless of the duration of operation.
Appendix B Reward model
As mentioned in Sub-section 3.3, interacting with a POMDP through a policy œÄ gen-
erates the sequence (s0, o0, a0, s1, o1, a1, . . .). During the interaction, the agent receives
reward values (r0, r1, . . . , rt, . . .), where rt is generated according to the reward func-
tion U(rt|st, at). Given the trajectory (s0, a0, s1, a1, . . .), we can calculate the probabil-
ity distribution of receiving the reward sequence (r0, r1, . . .) as follows:
p(r0, r1, ...|s0, a0, s1, a1, , ...) =
‚àûY
k=0
p(rk|sk, ak) =
TY
k=0
U(rk|sk, ak). (B.1)
Due to the agent‚Äôs lack of knowledge about the true reward function U(rt|st, at), it
learns a reward model P(rt|st, at) in a supervised manner through environmental inter-
actions, enabling the agent to estimate the reward sequence‚Äôs distribution:
p(r0, r1, ...|s0, a0, s1, a1, , ...) ‚âà
‚àûY
k=0
P(rk|sk, ak). (B.2)
It is worth noting that learning the reward model poses challenges due to reward
sparsity and its scalar nature. While the generative model‚Äôs learning is crucial for a
belief state-action policy, our study explores scenarios with and without reward model
learning, as detailed in Sub-section 6.1.
Appendix C The POMDP regularity assumptions
The regularity assumptions play a critical role in establishing the convergence of al-
gorithms that solve POMDPs (Puterman, 2014; Paternain, Bazerque, & Ribeiro, 2020;
Dufour & Prieto-Rumeau, 2012, 2013). These assumptions ensure the existence of
solutions for specific types of optimization problems that arise in POMDPs. The com-
pactness assumptions on the state, action, observation, and reward spaces are standard
requirements in the continuous space MDP and POMDP literature (Puterman, 2014;
Paternain et al., 2020; Dufour & Prieto-Rumeau, 2012, 2013). They also facilitate the
numerical implementation of MDP and POMDP problems with continuous spaces. For
instance, in grid-based methods, which involve dividing the continuous state or action
space into a finite set of discrete grid points, the compactness assumption ensures that
the discretization is not excessively coarse, which could lead to inaccuracies in the so-
lutions. It‚Äôs worth noting that the compactness assumption of the extrinsic reward space
R is not a strict requirement as it is user-defined.
The Lipschitz continuity assumption ensures that the transition function, observation
function, and reward function are well-behaved and exhibit smoothness and bounded-
ness. This assumption prevents overly steep or abrupt changes in these functions, which
could result in instability and convergence issues. The Lipschitz continuity assumption
is crucial for effective and reliable decision-making in continuous spaces, as it guaran-
tees the stability and convergence of the algorithms (Puterman, 2014; Paternain et al.,
2020; Dufour & Prieto-Rumeau, 2012, 2013).
Appendix D Theoretical analysis
This section presents a detailed theoretical analysis of the main paper. To facilitate un-
derstanding, Table D.1 lists the relevant notations used in our proposed unified inference
framework.
Table D.1: Overview of the notations used in our proposed unified inference framework.
Expression Explanation
T ‚àà {0, N} Finite time horizon in AIF
t ‚àà {0, 1, ...,} Current time step
œÑ ‚àà {t, t+ 1, ...,} Future time step
st ‚àà S Current hidden state
DS ‚àà N Dimension of state space
S ‚äÇRDS Continuous state space
bt ‚àà B Current belief state
B Continuous belief state space
ht ‚àà B Representation of belief state bt
DH ‚àà N Dimension of belief state representation
H ‚äÇRDH Continuous belief state representation space
ot ‚àà O Current observation
DO ‚àà N Dimension of observation space
O ‚äÇRDO Continuous observation space
at ‚àà A Current action (decision)
DA ‚àà N Dimension of action space
A ‚äÇRDA Continuous action space
rt ‚àà R Current (extrinsic) reward
R ‚äÇR Continuous (extrinsic) reward space
Àúa = (at, at+1, ..., aT‚àí1) Plan in AIF (sequence of future actions including current time step t)
¬ØŒ† A set of desired stochastic Markovian belief state-action policies ¬ØœÄ
E[.] Expectation function
log Natural log
P(.) Agent‚Äôs probabilistic model
H(.) Shannon entropy
DKL(.) Kullback Leibler divergence
œÉ(.) Boltzmann (softmax) function
P(ot, st, bt|at‚àí1, bt‚àí1, ) Belief state generative model
P(ot|st, bt‚àí1) Belief state-conditioned observation (likelihood) model
P(st|bt‚àí1, at‚àí1) Belief state-conditioned transition model
q(st|ot, bt‚àí1) Belief state-conditioned variational posterior distribution
Ft Original definition of VFE (i.e., ELBO) in AIF (K. Friston et al., 2017)
FVRNN Extension of VFE (i.e., ELBO) in VRNN (Chung et al., 2015)
GAIF(ot) Original definition of the EFE in AIF (K. Friston et al., 2017)
G(Àúa)
AIF(ot) Original definition of the EFE (K. Friston et al., 2017) for plan Àúa
G(œÄ)(bt) Our proposed belief state EFE for belief state-action policy œÄ
G(œÄ)(bt, at) Our proposed belief state-action EFE for belief state-action policy œÄ
œÄ‚àó = arg minœÄ G(œÄ)(bt) Optimal unconstrained belief state-action policy
¬ØœÄ‚àó = arg min¬ØœÄ‚àà¬ØŒ† G(¬ØœÄ)(bt) Optimal constrained belief state-action policy
V (œÄ)(st) State value function in RL for state-action policy œÄ
Q(œÄ)(st, at) State-action value function in RL for state-action policy œÄ
Œ≥ ‚àà [0, 1) Discount factor
0 ‚â§ Œ±, Œ≤, Œ∂ <‚àû Scaling factors
‚àÜ(.) The set of all probability distributions over the specified set
|.| Cardinality operator
D.1 Proof of Theorem. 5.3
Proof. By conditioning the EFE GAIF(ot) in Eq. (16) on the belief state-action policy
œÄ and taking the limit as T ‚Üí ‚àû, we obtain the resulting expression G(œÄ)
Unified(ot) as:
G(œÄ)
Unified(ot) = Eq(st:‚àû,ot+1:‚àû,at:‚àû|ot,œÄ)
 ‚àûX
œÑ=t
logq(at:‚àû|œÄ)P(sœÑ+1|sœÑ , aœÑ )
ÀúP(oœÑ+1|aœÑ )q(sœÑ+1|oœÑ+1)

. (D.1)
Using the biased generative model from AIF literature (K. Friston et al., 2017; Pezzulo,
Rigoli, & Friston, 2015), the variational distribution q(st:‚àû, ot+1:‚àû, at:‚àû|ot, œÄ) can be
factored as:
q(st:‚àû, ot+1:‚àû, at:‚àû|ot, œÄ) = q(at:‚àû|œÄ)
‚àûY
œÑ=t
q(sœÑ |oœÑ ) ÀúP(oœÑ+1|sœÑ , aœÑ ). (D.2)
Since action at is chosen based on the stochastic belief state-action policy œÄ and does
not depend on the other actions, we have q(at:‚àû|œÄ) = Q‚àû
œÑ=t q(aœÑ |œÄ). Consequently,
Eq. (D.2) can be rewritten as:
q(st:‚àû, ot+1:‚àû, at:‚àû|ot, œÄ) =
‚àûY
œÑ=t
q(aœÑ |œÄ)q(sœÑ |oœÑ ) ÀúP(oœÑ+1|sœÑ , aœÑ ). (D.3)
Now, by substituting q(at:‚àû|œÄ) with Q‚àû
œÑ=t q(aœÑ |œÄ) and q(st:‚àû, ot+1:‚àû, at:‚àû|ot, œÄ) with
the right-hand side of Eq. (D.3), G(œÄ)
Unified(ot) in Eq. (D.1) can be rewritten as:
G(œÄ)
Unified(ot) = EQ‚àû
œÑ=t q(aœÑ |œÄ)q(sœÑ |oœÑ ) ÀúP(oœÑ+1|sœÑ ,aœÑ )
T‚àí1X
œÑ=t
logq(aœÑ |œÄ) (D.4)
‚àí log ÀúP(oœÑ+1|œÄ, sœÑ ) + logP(sœÑ+1|aœÑ , sœÑ )
q(sœÑ+1|oœÑ+1)

. (D.5)
Furthermore, since policy œÄ selects action aœÑ based on the belief state bœÑ , i.e., aœÑ ‚àº
œÄ(aœÑ |bœÑ ), we can express G(œÄ)
Unified(ot) using the law of total expectation as follows:
G(œÄ)
Unified(ot) = EP(bt:‚àû)

Eq(st|ot) Q‚àû
œÑ=t q(aœÑ |œÄ,bœÑ )q(sœÑ+1|oœÑ+1,bœÑ ) ÀúP(oœÑ+1|sœÑ ,aœÑ ,bœÑ )
 ‚àûX
œÑ=t
logq(aœÑ |œÄ, bœÑ )
‚àí log ÀúP(oœÑ+1|aœÑ , sœÑ , bœÑ ) + logP(sœÑ+1|aœÑ , sœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

(D.6)
(a)
= EP(bt:‚àû)

Eq(st|ot) Q‚àû
œÑ=t œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ ) ÀúP(oœÑ+1|sœÑ ,aœÑ ,bœÑ )
 ‚àûX
œÑ=t
logœÄ(aœÑ |bœÑ )
‚àí log ÀúP(oœÑ+1|aœÑ , sœÑ , bœÑ ) + logP(sœÑ+1|aœÑ , sœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

(D.7)
(b)
= EP(bt:‚àû)

EQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ ) ÀúP(oœÑ+1|aœÑ ,bœÑ )
 ‚àûX
œÑ=t
logœÄ(aœÑ |bœÑ )
‚àí log ÀúP(oœÑ+1|aœÑ , bœÑ ) + log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

(D.8)
(c)
= EP(bt|st,ot,bt‚àí1,at‚àí1) Q‚àû
œÑ=t P(bœÑ+1|sœÑ+1,oœÑ+1,bœÑ ,aœÑ )œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ ) ÀúP(oœÑ+1|aœÑ ,bœÑ )
 ‚àûX
œÑ=t
logœÄ(aœÑ |bœÑ ) ‚àí log ÀúP(oœÑ+1|aœÑ , bœÑ ) + log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

, (D.9)
where (a) follows from q(aœÑ |œÄ, bœÑ ) = œÄ(aœÑ |bœÑ ), and (b) follows because the belief state
bœÑ contains the necessary information about the hidden state sœÑ that is relevant for pre-
dicting sœÑ+1 and oœÑ+1. As a result, the direct dependence on sœÑ becomes redundant
when conditioned on the belief state bœÑ . (c) follows due to the factorization resulting
from the belief state update function in Eq. (4) of the main manuscript:
P(bt:‚àû) =
‚àûY
œÑ=t
P(bœÑ |bœÑ‚àí1)] = P(bt|st, ot, bt‚àí1, at‚àí1) (D.10)
√ó
‚àûY
œÑ=t
EQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ ) ÀúP(oœÑ+1|aœÑ ,bœÑ )[P(bœÑ+1|sœÑ+1, oœÑ+1, bœÑ , aœÑ )].
As stated in Assumption 5.2, following AIF, we assume that prior to action selection
at time step t, the agent performs inference and approximates its belief state bt by a
variational posterior (i.e., here q(.|ot, bt‚àí1)). Hence, the belief state bt is uniquely deter-
mined, and thus P(bt|st, ot, bt‚àí1, at‚àí1) is equivalent to Œ¥(bt ‚àí q(.|ot, bt‚àí1)). Therefore,
taking the expected value with respect to P(bt|st, ot, bt‚àí1, at‚àí1) in Eq. (D.9) simply
evaluates G(œÄ)
Unified(ot) at the given bt = q(.|ot, bt‚àí1). Therefore, G(œÄ)
Unified(ot) can be ex-
pressed as a function of both ot and bt. Since bt contains sufficient information about
ot, it is adequate to express G(œÄ)
Unified(ot) in Eq. (D.9) solely as a function of bt, i.e.,
G(œÄ)
Unified(bt) = EQ‚àû
œÑ=t P(bœÑ+1|sœÑ+1,oœÑ+1,bœÑ ,aœÑ ) ÀúP(oœÑ+1|aœÑ ,bœÑ )œÄ(aœÑ |bœÑ )q(sœÑ+1|oœÑ+1,bœÑ )
 ‚àûX
œÑ=t
logœÄ(aœÑ |bœÑ )
‚àí log ÀúP(oœÑ+1|aœÑ , bœÑ ) + log P(sœÑ+1|aœÑ , bœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

. (D.11)
2
D.2 Proof of Proposition 5.4
Proof. The result is obtained by taking out the termsŒ≤œÄ(at|bt), Œ±rt, and Œ∂log P(st+1|bt,at)
q(st+1|ot+1,bt)
from G(œÄ)(bt) in Eq. (24):
G(œÄ)(bt) = EQ‚àû
œÑ=t œÄ(aœÑ |bœÑ )P(sœÑ+1,oœÑ+1,bœÑ+1|bœÑ ,aœÑ )P(rœÑ |bœÑ ,aœÑ )
 ‚àûX
œÑ=t
Œ≥œÑ‚àít 
Œ≤ logœÄ(aœÑ |bœÑ )
‚àí Œ± rœÑ + Œ∂ log P(sœÑ+1|bœÑ , aœÑ )
q(sœÑ+1|oœÑ+1, bœÑ )

(a)
= EœÄ(at|bt)P(st+1,ot+1|bt,at)P(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt + Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥ EP(bt+1|st+1,bt,at,ot+1)

EQ‚àû
œÑ‚Ä≤=t+1 œÄ(aœÑ‚Ä≤|bœÑ‚Ä≤)P(sœÑ‚Ä≤+1,oœÑ‚Ä≤+1,bœÑ‚Ä≤+1|bœÑ‚Ä≤,aœÑ‚Ä≤)P(rœÑ‚Ä≤|bœÑ‚Ä≤,aœÑ‚Ä≤)
[
‚àûX
œÑ‚Ä≤=t+1
Œ≥œÑ‚Ä≤‚àít 
Œ≤ logœÄ(aœÑ‚Ä≤|bœÑ‚Ä≤) ‚àí Œ± rœÑ‚Ä≤ + Œ∂ log P(sœÑ‚Ä≤+1|aœÑ‚Ä≤, bœÑ‚Ä≤)
q(sœÑ‚Ä≤+1|oœÑ‚Ä≤+1, bœÑ‚Ä≤)

]

(b)
= EœÄ(at|bt)P(rt|bt,at)P(st+1,ot+1|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt + Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥EP(bt+1|bt,at,ot+1)

G(œÄ)(bt+1)

(c)
= EœÄ(at|bt)P(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt + EP(st+1,ot+1,bt+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥G(œÄ)(bt+1)

,
where (a) follows by applying the law of total expectation, (b) is because of the defi-
nition of the belief state EFE, and (c) follows since œÄ(at|bt) and rt are independent of
P(st+1, ot+1, bt+1|bt, at). 2
D.3 Proof of Theorem 5.5
Proof. According to Proposition 5.4, we have:
G(œÄ)(bt) = EœÄ(at|bt)P(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt (D.12)
+ EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G(œÄ)(bt+1)

.
Therefore:
G‚àó(bt) = min
œÄ
G(œÄ)(bt) = min
œÄ
EœÄ(at|bt)P(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt
+EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G(œÄ)(bt+1)

(a)
= min
œÄ(.|bt),ÀúœÄ
EœÄ(at|bt)

EP(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt (D.13)
+EP(bt+1,ot+1,st+1|bt,at)[Œ∂ logp(st+1|st, at)
q(st+1|ot+1) + Œ≥G(ÀúœÄ)(st+1)]

(b)
= min
œÄ(.|bt)
EœÄ(at|bt)

EP(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt (D.14)
+EP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥ min
ÀúœÄ
G(ÀúœÄ)(bt+1)]

(c)
= min
œÄ(.|bt)
EœÄ(at|bt)

EP(rt|bt,at)

Œ≤ logœÄ(at|bt) ‚àí Œ± rt (D.15)
+EP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G‚àó(bt+1)]

,
where (a) is because of the decomposition of the policy œÄ as œÄ = ( œÄ(.|st), ÀúœÄ). Let
ÀúœÄ‚àó = arg minÀúœÄ G(ÀúœÄ)(bt+1), then (b) follows because of the trivial inequality:
min
ÀúœÄ
EP(bt+1,ot+1,st+1|bt,at)[G(ÀúœÄ)(bt+1)] ‚â• EP(bt+1,ot+1,st+1|bt,at)[min
ÀúœÄ
G(ÀúœÄ)(bt+1)],
and
EP(bt+1,ot+1,st+1|bt,at)[min
ÀúœÄ
G(ÀúœÄ)(bt+1)] = EP(bt+1,ot+1,st+1|bt,at)[GÀúœÄ‚àó
(bt+1)]
‚â§ min
ÀúœÄ
Ep(st+1,ot+1|at)[G(ÀúœÄ)(bt+1)]. (D.16)
Finally, (c) follows from the definition of the optimal belief state EFE. 2
D.4 Proof of Corollary. 5.6
Proof. Proof of this corollary is straightforward by taking the argument of Eq. (27). 2
D.5 Proof of Theorem 5.7
Proof. From Corollary 5.6, we have:
œÄ‚àó(at|bt) ‚àà arg min
œÄ(at|bt)
EœÄ(at|bt)

EP(rt|bt,at)[Œ≤ logœÄ(at|bt) ‚àí Œ± rt] (D.17)
+ EP(bt+1,ot+1,st+1|st,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥ min
ÀúœÄ
G(ÀúœÄ)(bt+1)

.
The minimization in the equation above subject to the constraint
R
A œÄ(a‚Ä≤|bt) da‚Ä≤ = 1
yields minimizing the Lagrangian function L(Œª, œÄ(at|bt)) with the Lagrange multiplier
Œª:
L(Œª, œÄ(at|bt))=EœÄ(at|bt)

EP(rt|bt,at)[Œ≤ logœÄ(at|bt) ‚àí Œ± rt] + EP(bt+1,ot+1,st+1|st,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+Œ≥G‚àó(bt+1)

‚àí Œª
Z
A
œÄ(a‚Ä≤|bt) da‚Ä≤ ‚àí 1

. (D.18)
The derivative of L(Œª, œÄ(at|bt)) with respect to œÄ(at|bt) is given by:
‚àÇL(Œª, œÄ(at|bt))
‚àÇœÄ(at|bt)) =EP(rt|bt,at)[Œ≤ logœÄ(at|bt) + Œ≤ ‚àí Œ± rt] (D.19)
+EP(bt+1,ot+1,st+1|st,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt) + Œ≥G‚àó(bt+1)

‚àí Œª.
Setting ‚àÇL(Œª,œÄ(at|bt))
‚àÇœÄ(at|bt)) = 0 gives the optimal œÄ‚àó(at|bt) for the specific action at as:
œÄ‚àó(at|bt)=exp
 
Œª‚àíŒ≤ ‚àíEP(rt|bt,at)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt,at)
q(st+1|ot+1,bt) +Œ≥G‚àó(bt+1)]

Œ≤
!
.
(D.20)
By accounting the constraint
R
A œÄ(a‚Ä≤|bt) da‚Ä≤ = 1:
Z
A
œÄ(a‚Ä≤|bt) da‚Ä≤ = exp(Œª ‚àí Œ≤
Œ≤ ) (D.21)
Z
A
exp
 
EP(rt|bt,at)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt,at)
q(st+1|ot+1,bt) +Œ≥G‚àó(bt+1)]

Œ≤
!
= 1.
Putting together Eqs. (D.20) and (D.21) results in:
œÄ‚àó(at|bt) =
exp
 
EP(rt|bt,at)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)[Œ∂ log
P(st+1|bt,at)
q(st+1|ot+1,bt)+Œ≥G‚àó(bt+1)]

Œ≤
!
R
A exp
 
EP(rt|bt,a‚Ä≤)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,a‚Ä≤)[Œ∂ log
P(st+1|bt,a‚Ä≤)
q(st+1|ot+1,bt)+Œ≥G‚àó(bt+1)]

Œ≤
!
da‚Ä≤
= œÉ
 
EP(rt|bt,at)

Œ±rt‚àíEP(bt+1,ot+1,st+1|bt,at)[Œ∂ log P(st+1|bt,at)
q(st+1|ot+1,bt) +Œ≥G‚àó(bt+1)]

Œ≤
!
.
(D.22)
Taking the second derivative ofL(Œª, œÄ(at|bt)) with respect to œÄ(at|bt) yields:
‚àÇ2L(Œª, œÄ(at|bt))
‚àÇœÄ2(at|bt) = Œ≤
œÄ(at|bt), (D.23)
which is a positive value for the compact action space A. Therefore, the expression
in Eq. (D.22) is the unique minimizer of L(Œª, œÄ(at|bt)) and, consequently, the unique
minimizer of the unified Bellman equation. 2
D.6 Proof of Lemma 5.8
Proof. From the definition of G(¬ØœÄ‚àó)(bt, at) in Eq. (39):
G(¬ØœÄ‚àó)(bt, at) = EP(rt|bt,at)

‚àíŒ±rt +EP(bt+1,ot+1,st+1|bt,at)

Œ∂log P(st+1|bt, at)
q(st+1|ot+1, bt) +Œ≥G(¬ØœÄ‚àó)(bt+1)

(a)
= EP(rt|bt,at)

‚àíŒ±rt +EP(bt+1,ot+1,st+1|bt,at)

Œ∂log P(st+1|bt, at)
q(st+1|ot+1, bt) +Œ≥ min
¬ØœÄ‚ààŒ†
G(¬ØœÄ)(bt+1)

(b)
= min
¬ØœÄ‚ààŒ†
EP(rt|bt,at)

‚àíŒ±rt +EP(bt+1,ot+1,st+1|bt,at)

Œ∂log P(st+1|bt, at)
q(st+1|ot+1, bt) +Œ≥G(¬ØœÄ)(bt+1)

| {z }
G(¬ØœÄ)(bt,at)
= min
¬ØœÄ‚ààŒ†
G(¬ØœÄ)(bt, at), (D.24)
where (a) follows from the definition of G(¬ØœÄ‚àó)(bt+1), and (b) follows since P(rt|bt, at),
rt, P(bt+1, ot+1, st+1|bt, at), and log P(st+1|bt,at)
q(st+1|ot+1,bt) are independent of ¬ØœÄ. 2
D.7 Proof of Proposition 5.9
Proof. The proof follows two steps:
First, by replacingG(¬ØœÄ)(bt, at) with EP(rt|bt,at)

‚àíŒ±rt+EP(bt+1,ot+1,st+1|bt,at)[Œ∂log P(st+1|bt,at)
q(st+1|ot+1,bt)+
Œ≥G(¬ØœÄ)(bt+1)]

in Eq. (35), we can express the belief state EFE G(¬ØœÄ)(bt) as a function of
the belief state-action EFE G(¬ØœÄ)(bt, at) as follows:
G(¬ØœÄ)(bt) = E¬ØœÄ(at|bt)

Œ≤ log¬ØœÄ(at|bt) + G(¬ØœÄ)(bt, at)

. (D.25)
Now, by plugging G(¬ØœÄ)(bt+1) = E¬ØœÄ(at+1|bt+1)

log¬ØœÄ(at+1|bt+1) + G(¬ØœÄ)(bt+1, at+1)

ob-
tained from Eq. (D.25) into the result of Lemma 5.8, the proof is completed:
G(¬ØœÄ)(bt, at) = EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ(at+1|bt+1)[Œ≤ logœÄ(at+1|bt+1) + G(¬ØœÄ)(bt+1, at+1)]

. (D.26)
2
D.8 Proof of Theorem 5.10
Proof. Following (Sutton & Barto, 2018; Leibfried, Pascual-Diaz, & Grau-Moya,
2019), let‚Äôs define PœÄ(bt+1, ot+1, st+1, bt, at) and Runified (bt, at) as
P¬ØœÄ(bt, at, bt+1, at+1) := EP(ot+1,st+1|bt,at)[P(bt+1|st+1, ot+1, bt, at)¬ØœÄ(at+1|bt+1)], (D.27)
Runified (bt, at) := EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥E¬ØœÄ(at+1|bt+1)[Œ≤ logœÄ(at+1|bt+1)]

. (D.28)
The compactness assumptions on the state space S, action space A, and observation
space O guarantee that the space of all belief states bœÑ ‚àà Bis a compact subset of a
separable Hilbert space. As a result, both P¬ØœÄ(bt, at, bt+1, at+1) and Runified (bt, at) are
bounded. With these assumptions in place, we can expressTunified
¬ØœÄ G in a compact form
as follows:
Tunified
¬ØœÄ G = Runified + Œ≥P œÄG,
By starting from an initial bounded mapping G0 : B √ó A ‚ÜíR and recursively applying
Gk+1 = Tunified
¬ØœÄ Gk for k = 0, 1, 2, . . ., we have
G¬ØœÄ := lim
k‚Üí‚àû
Tunified
¬ØœÄ Gk = limk‚Üí‚àû
k‚àí1X
i=0
Œ≥iPi
¬ØœÄRunified + Œ≥kPk
¬ØœÄ G0
(a)
= limk‚Üí‚àû
k‚àí1X
i=0
Œ≥iPi
¬ØœÄRunified , (D.29)
where (a) is because G0 and P¬ØœÄ are bounded and thus the term Œ≥kPk
¬ØœÄ G0 will converge to
zero. Therefore the convergence of Tunified
¬ØœÄ G¬ØœÄ does not depend on the initial value G0.
Now, by substituting G¬ØœÄ with limk‚Üí‚àû
Pk‚àí1
i=0 Œ≥iPi
¬ØœÄRunified , we have:
Tunified
¬ØœÄ G¬ØœÄ = Runified + Œ≥P œÄ lim
k‚Üí‚àû
k‚àí1X
i=0
Œ≥iPi
¬ØœÄRunified = lim
k‚Üí‚àû
kX
i=0
Œ≥iPi
¬ØœÄRunified
(a)
= lim
k‚Üí‚àû
k‚àí1X
i=0
Œ≥iPi
¬ØœÄRunified + Œ≥kPk
¬ØœÄ Runified
| {z }
0
= G¬ØœÄ, (D.30)
where (a) follows since Runified is bounded. Eq. (D.30) demonstrates that G¬ØœÄ is a fixed
point of Tunified
¬ØœÄ G¬ØœÄ.
To establish the uniqueness of this fixed point, let‚Äôs assume there exists another fixed
point G‚Ä≤ of Tunified
¬ØœÄ such that Tunified
¬ØœÄ G‚Ä≤ = G‚Ä≤. Then, G‚Ä≤ = lim k‚Üí‚àû Tunified
¬ØœÄ Gk with
G0 = G‚Ä≤ converges to G¬ØœÄ since the convergence behavior of Tunified
¬ØœÄ is independent of
the initial value G‚Ä≤. Consequently, we can conclude that G‚Ä≤ = G¬ØœÄ. 2
D.9 Proof of Corollary 5.11
Proof. This result can be easily obtained by replacing ¬ØœÄ and G(¬ØœÄ)(bt, at) in the result of
Theorem 5.10 with the optimal policy ¬ØœÄ‚àó and its corresponding belief state-action value
function G(¬ØœÄ‚àó)(bt, at), i.e., the optimal belief state-action EFE. 2
D.10 Proof of Lemma 5.12
Proof. Based on the definition of ¬ØœÄ(new), for any ¬ØœÄ ‚àà ¬ØŒ†:
DKL
"
¬ØœÄ(new)(.|bt), œÉ
 
‚àíG(¬ØœÄ(old))(bt, .)
Œ≤
!#
‚â§ DKL
"
¬ØœÄ(.|bt), œÉ
 
‚àíG(¬ØœÄ(old))(bt, .)
Œ≤
!#
. (D.31)
By choosing ¬ØœÄ = ¬ØœÄ(old) in Eq. (D.31) and using definition of KL-divergence:
‚àíŒ≤ H(¬ØœÄ(new)(.|bt)) + E¬ØœÄ(new)(at|bt)
h
G(¬ØœÄ(old))(bt, at)
i
(D.32)
‚â§ ‚àíŒ≤ H(¬ØœÄ(old)(.|bt)) + E¬ØœÄ(old)(at|bt)
h
G(¬ØœÄ(old))(bt, at)
i
= G(¬ØœÄ(old))(bt).
Moreover, from Eq. (45), we have:
G(¬ØœÄ(old))(bt, at) = EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥G(¬ØœÄ(old))(bt+1)

(D.33)
Now, by repeatedly applying Eq. (D.33) and the bound in Eq. (D.32):
G(¬ØœÄ(old))(bt, at) = EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
+ Œ≥G(¬ØœÄ(old))(bt+1)

‚â• EP(rt|bt,at)

‚àí Œ± rt + EP(bt+1,ot+1,st+1|bt,at)

Œ∂ log P(st+1|bt, at)
q(st+1|ot+1, bt)
‚àí Œ≤ H(¬ØœÄ(new)(.|bt)) + E¬ØœÄ(new)(at|bt)
h
G(¬ØœÄ(old))(bt, at)
i
(D.34)
...
(a)
‚â• G(¬ØœÄ(new))(bt, at), (D.35)
where (a) results from Theorem 5.10. 2
D.11 Proof of Theorem 5.13
Proof. Let ¬ØœÄk be the policy at iteration k of the policy iteration process. According
to Lemma 5.12, the sequence G(¬ØœÄk) is monotonically decreasing. Since Runified defined
in Eq. (D.28) is bounded, G(¬ØœÄ) = E[P‚àû
œÑ=t Œ≥œÑ‚àítRunified (bœÑ , aœÑ ))] is also bounded. The
sequence G(¬ØœÄk) and consequently sequence ¬ØœÄk converge to some G(¬ØœÄ‚àó) and policy ¬ØœÄ‚àó.
We still need to show that ¬ØœÄ‚àó is the optimal policy. To do so, we need to show that
the belief state-action of the converged policy is lower than any other policy in ¬ØŒ†, i.e.,
G(¬ØœÄ‚àó)(bt, at) < G(¬ØœÄ)(bt, at) for all ¬ØœÄ ‚àà Œ† and all (bt, at) ‚àà B √ó A. This can be achieved
by following the same iterative steps outlined in proof of Lemma 5.12. 2
Appendix E Unified inference model
In this section, we elaborate on the proposed unified inference model, encompassing
perceptual inference and learning, as well as the unified actor-critic framework. We
delve into three training methods for the unified actor-critic: model-based, model-free,
and hybrid approaches. Furthermore, we present the pseudocode for the unified infer-
ence model.
E.1 Perceptual inference and learning model
As stated in Sub-section 4.2.1, AIF approximates generative modelP(ot, st|at‚àí1, st‚àí1) =
P(ot|st)P(st|st‚àí1, at‚àí1) and the variational distributionq(st|ot) by minimizing the VFE
(negative of ELBO in V AE). However, in our proposed unified inference, we need to ap-
proximate the belief state-conditioned variational posterior q(st|ot, bt‚àí1) and the belief
state generative model defined in Eq. (21), i.e.,
P(ot, st, bt|at‚àí1, bt‚àí1) = P(ot|st, bt‚àí1)P(st|bt‚àí1, at‚àí1)P(bt|st, ot, bt‚àí1, at‚àí1). (E.1)
To achieve this, we use the Variational Recurrent Neural Networks (VRNN) model (Chung
et al., 2015). VRNNs combine the variational inference of V AEs with the sequential
modeling capabilities of RNNs. They minimize the VFE conditioned on a variable of
an RNN, enabling the modeling of temporal dependencies and generating sequential
data. However, in our case, where the POMDP assumes a continuous state space (i.e.,
an infinite number of latent states), the corresponding belief space bt‚àí1 is a continuous
probability distribution function with an infinite number of dimensions. Therefore, we
need to approximate bt‚àí1 to use it as an input to the VRNN model for approximating
P(ot|st, bt‚àí1), P(st|bt‚àí1, at‚àí1), P(bt|st, ot, bt‚àí1, at‚àí1), and q(st|ot, bt‚àí1).
Various approaches can be used to approximate a continuous belief state for input
to a neural network model, including vector representation, kernel density estimation
(KDE), discretization, and particle filtering (Igl et al., 2018). These methods aim to
transform the continuous belief state into a fixed-length representation that can be fed
into the neural network. One common approach is to learn a belief representation
through the belief state update function in Eq. (4) (Gregor et al., 2019). Following
established practices, we learn an explicit belief state representation ht ‚àà RDH, where
DH represents the dimension of the belief state representation ht.13 We learn this belief
state representation through a deterministic non-linear function f that updates the be-
lief state representation based on the previous belief state representation, the previous
action, the current observation, and the current state:
ht = f(ht‚àí1, at‚àí1, ot, st). (E.2)
By iteratively applying the update function f, the belief state representation can adapt
and evolve over time as new observations and actions are encountered. This approach
allows us to approximate the belief state representation in a continuous space and uti-
13While our unified inference framework can be combined with various algorithms for approximating
the belief state, we employ a commonly used approximation method that has demonstrated effectiveness
in many recent approaches.
lize it as an input to a VRNN model for approximatingP(ot|st, ht‚àí1), P(st|ht‚àí1, at‚àí1),
P(ht|st, ot, ht‚àí1, at‚àí1), and q(st|ot, ht‚àí1). Given the transition model P(st|bt‚àí1, at‚àí1)
and the update function f(ht‚àí1, at‚àí1, ot, st), we can intuitively say that the states are
split into a stochastic part st and a deterministic part ht. This aligns with the modifi-
cations made in previous RL and AIF works that heuristically incorporate this separa-
tion (Hafner et al., 2020; Ogishima, Karino, & Kuniyoshi, 2021; Lee et al., 2020; Han
et al., 2020).
We parameterize the posterior distribution with ŒΩ, denoted as qŒΩ(st|ot, ht‚àí1), and the
agent‚Äôs belief state generative model with Œ∏, i.e., PŒ∏(ot|st, ht‚àí1), PŒ∏(st|ht‚àí1, at‚àí1), and
PŒ∏(ht|st, ot, ht‚àí1, at‚àí1) = Œ¥(ht‚àífŒ∏(ht‚àí1, at‚àí1, ot, st)). To modelqŒΩ(st|ot, bt‚àí1), PŒ∏(ot|st.bt‚àí1),
and PŒ∏(st|bt‚àí1, at‚àí1) we use DNNs that output the mean and standard deviation of
the random variables according to the Gaussian distribution. For example, we model
qŒΩ(st|ot, bt‚àí1) as a network that takes ot and bt‚àí1 as inputs, calculates through several
hidden layers, and outputs the Gaussian distribution of st. The update function fŒ∏ can
be implemented using gated activation functions such as Long Short-Term Memory
(LSTM) or Gated Recurrent Unit (GRU). We use LSTM (Hochreiter & Schmidhuber,
1997) as it has shown good performance in general cases.
The parameters ŒΩ and Œ∏ are trained by minimizing the objective function of VRNN,
given by
FVRNN
Œ∏,ŒΩ = ‚àíEq(st|ot,ht‚àí1) [logP(ot|st, ht‚àí1)] + DKL[qŒΩ(.|ot, ht‚àí1), PŒ∏(.|bt‚àí1, at‚àí1)].(E.3)
The VRNN objective function FVRNN
Œ∏,ŒΩ is minimized using gradient descent on batches
of data sampled from a dataset called a replay buffer, denoted as D. The replay buffer
contains quadruples consisting of the agent‚Äôs action ak, the received extrinsic reward
rk+1 (caused by the action), and the subsequent observation ok+1. The VFE can be
expressed as an expectation over a batch of size B, consisting of M sequential data
points, denoted as {(ak‚àí1, ok)M
k=1}B
i=1, which are sampled from the replay buffer D.
FVRNN
Œ∏,ŒΩ = ED(ak‚àí1,ok)

‚àí EqŒΩ(st|ok,ht‚àí1) [logPŒ∏(ok|st, ht‚àí1)]
+ DKL[qŒΩ(.|ot, ht‚àí1), PŒ∏(.|ht‚àí1, ak‚àí1)]

(E.4)
The expectation with respect toqŒΩ(st|ok, ht‚àí1) in Eq. (E.4) involves integrating over the
continuous state space S, which is not analytically tractable. To estimate FVRNN
Œ∏,ŒΩ , we
use Monte Carlo estimation, which provides an unbiased estimation by sampling sl
t for
l ‚àà {1, 2, ..., L} from qŒΩ(st|ok, ht‚àí1) and evaluating FVRNN
Œ∏,ŒΩ for each sampled state. The
estimate of FVRNN
Œ∏,ŒΩ is obtained by taking the average over the samples. However, com-
puting the gradient of FVRNN
Œ∏,ŒΩ with respect to ŒΩ requires differentiating with respect to
qŒΩ(st|ok, ht‚àí1), which is the distribution we sampled from. To enable gradient-based op-
timization, it is necessary to have a differentiable sampling process that generates sam-
ples from a probability distribution. The reparameterization trick (Kingma & Welling,
2013) provides a solution to this problem by reparameterizing the original distribution
and introducing an auxiliary variable that follows a fixed distribution, such as a stan-
dard Gaussian. This reparameterization allows us to obtain differentiable samples by
applying a deterministic transformation to the auxiliary variable. As a result, we can
backpropagate gradients through the sampling operation and compute gradients with
respect to the distribution parameters. In our approach, we draw inspiration from es-
tablished practices in the field (Lee et al., 2020; Han et al., 2020; Hafner et al., 2020)
and leverage the reparameterization trick (Kingma & Welling, 2013) to sample sl
t from
the distribution qŒΩ(st|ok, ht‚àí1). Given samples s(l)
t , belief state representation ht is then
computed as ht = 1
L
PL
l=1 fŒ∏(ht‚àí1, ak‚àí1, ok, sl
t).
E.2 Reward model learning
We approximate P(rt|ht, at) using a DNN with parameter Œæ, denoted as PŒæ(rt|ht, at).
The reward model approximation is a supervised learning problem that can be addressed
through likelihood maximization, using samples from the replay buffer D. This results
in minimizing the following loss function:
Lr(Œæ) = ‚àíED(ak,rk)[logPŒæ(rk|ht, ak)]. (E.5)
E.3 Pseudocode for the unified inference
Algorithm E.1 outlines the pseudocode for the proposed unified inference framework.
We use a variable mode to represent the approach (model-based, model-free or hybrid)
used for training the proposed unified actor-critic model.
Appendix F Environments
We utilize the PyBullet (Coumans & Bai, 2016) benchmarks, replacing the deprecated
Roboschool as recommended by the official GitHub repository14. To convert these Py-
Bullet benchmarks into tasks with partial observations, we made modifications by re-
taining the velocities while removing all position/angle-related entries from the obser-
14https://github.com/openai/roboschool
Algorithm E.1 UNIFIED INFERENCE MODEL
1: Model components: Likelihood PŒ∏(ot|st, ht‚àí1), variational posterior
qŒΩ(st|ot, ht‚àí1), transition model PŒ∏(st+1|ht, at), belief state representation update
function fŒ∏(ht‚àí1, at‚àí1, ot, st), reward model PŒæ(rt|ht, at), belief state-action EFE
Gœà(ht, at), belief state-action policy ¬ØœÄœï(at|ht).
2: Hyperparameters: Scaling parameters Œ±, Œ≤, Œ∂, and c, discount factor Œ≥, learning
rate Œª, actor-critic learning mode indicatorImode, batch size B, sequence length M,
imagination horizon N.
3: Initialize: Neural network parameters Œ∏, ŒΩ, Œæ, œà, and œï, global step t ‚Üê 0.
4: while not converged do:
5: Reset the environment and initialize h0 randomly.
6: for each environment step t do:
7: Receive observation ot
8: Sample st ‚àº qŒΩ(st|ot, ht‚àí1)
9: Sample action at ‚àº ¬ØœÄœï(at|ht) and execute at in the environment.
10: Receive observation ot+1 and extrinsic reward rt.
11: Record (at, rt, ot+1) into D.
12: end for
13: for each gradient step do:
14: Sample a minibatch {(ak‚àí1, ok)M
k=1}B
i=1 from D.
15: Compute FVRNN
Œ∏,ŒΩ from Eq. (E.4).
16: Update Œ∏ ‚Üê Œ∏ ‚àí Œª‚àáŒ∏FVRNN
Œ∏,ŒΩ .
17: Update ŒΩ ‚Üê ŒΩ ‚àí Œª‚àáŒΩFVRNN
Œ∏,ŒΩ .
18: if Imode == model ‚àí free then: ‚ñ∑ Model-free unified actor-critic.
19: Sample a minibatch {(ak, rk, ok+1)M
k=1}B
i=1 from D.
20: Compute LMF
G (œà) from Eq. (63) and set LG(œà) = LMF
G (œà).
21: end if
22: if Imode == model ‚àí based then: ‚ñ∑ Model-based unified actor-critic.
23: Compute Lr(Œæ) from Eq. (E.5).
24: Update Œæ ‚Üê Œæ ‚àí Œªr‚àáŒæLr(Œæ).
25: Imagine trajectories {(aœÑ , rœÑ , oœÑ+1)t+N
œÑ=t }B
i=1 starting from ht.
26: Compute LMB
G (œà) from Eq. (68) and set LG(œà) = LMB
G (œà).
27: end if
28: if Imode == hybrid then: ‚ñ∑ Hybrid unified actor-critic.
29: Sample a minibatch {(ak‚àí1, ok)M
k=1}B
i=1 from D.
30: Compute LMF
G (œà) from Eq. (63).
31: Imagine trajectories {(aœÑ , rœÑ , oœÑ+1)t+N
œÑ=t }B
i=1 starting from ht.
32: Compute LMB
G (œà) from Eq. (68).
33: Set LG(œà) = cLMF
G (œà) + LMB
G (œà).
34: end if
35: œà ‚Üê œà ‚àí Œª‚àáœàLG(œà).
36: Compute L¬ØœÄ(œï) from Eq. (65).
37: œï ‚Üê œï ‚àí Œª‚àáœïL¬ØœÄ(œï).
38: t ‚Üê t + 1.
39: end for
40: end while
vation vector. Table F.1 provides a summary of key information for each environment.
Table F.1: Information of the Roboschool PyBullet environments we used.
Task DS DO (velocities only)
HalfCheetahPyBulletEnv 26 9
HopperPyBulletEnv 15 6
AntPyBulletEnv 28 9
Walker2DPyBulletEnv 22 9
Appendix G Implementation details
In this section, we describe the implementation details for our algorithm and the alter-
native approaches.
For Recurrent Model-Free (Ni et al., 2022), we followed its original implementation,
including the hyperparameters. The official implementation of VRM (Han et al., 2020)
was also used for the inference, encoding, decoding, and actor-critic networks. We
ensured a fair comparison for G-Dreamer and G-SAC by adopting the same network
structure as VRM, including VRNN and actor-critic networks.
While the original Dreamer framework (Hafner et al., 2020) used pixel observations
and employed convolutional encoder and decoder networks, we made adaptations to suit
our partially observable case. Specifically, we replaced CNNs and transposed CNNs
with two-layer MLPs, each containing 256 units. Additionally, we adjusted Dreamer‚Äôs
actor and critic network parameters to match those of VRM, G-Dreamer, and G-SAC,
ensuring a consistent configuration for fair comparison.
G.1 VRNN
In accordance with (Han et al., 2020), we set the dimensionality of the belief state
representation h to 256. To model the variational posterior qŒΩ(st|ot, ht‚àí1), we employ
a one-hidden-layer fully-connected network with 128 hidden neurons. The likelihood
function PŒ∏(ot|st, ht‚àí1), the transition function PŒ∏(st+1|ht, at), and the reward function
PŒæ(rt|ht, at) are modelled using two-layer MLPs with 128 neurons in each layer. All of
these networks are designed as Gaussian layers, wherein the output is represented by
a multivariate normal distribution with diagonal variance. The output functions for the
mean are linear, while the output functions for the variance employ a non-linear softplus
activation. For the belief state representation update model fŒ∏(ht‚àí1, at‚àí1, ot, st), we
utilize an LSTM architecture.
G.2 Actor-Critic
We represent the actor ¬ØœÄœï(at|ht) as a diagonal multivariate Gaussian distribution. Both
the actor network¬ØœÄœï(at|ht) and the critic networkGœà(ht, at) consist of 4 fully connected
layers with an intermediate hidden dimension of256. The actor model generates a linear
mean for the Gaussian distribution, while the standard deviation is computed using the
softplus activation function. The resulting standard deviation is then transformed using
the tanh function. On the other hand, the critic model utilizes a linear output layer,
which provides the estimated value of the given belief state-action pair.
G.3 Model Learning
The VRNN parameters are trained with a learning rate of 0.0008. The training is con-
ducted using batches of 4 sequences, each with a length of 64. The critic and actor pa-
rameters are trained with a learning rate of 0.0003 for G-SAC. Batches of 4 sequences,
each with a length of 64, are used during training. For G-Dreamer, the actor and critic
are trained with a learning rate of 0.00008, and batches of 50 sequences, each with
a length of 50, are employed. All of the model parameters are optimized using the
Adam optimizer. During training, a single gradient step is taken per environment step.
The imagination horizon, N, is set to 15, and the discount factor Œ≥ is set to 0.99 for
both G-Dreamer and G-SAC. In addition, the scaling parameter Œ≤ and Œ∂ is set to 1 for
both algorithms. For a comprehensive summary of the hyperparameters, please refer to
Tables G.1.
Table G.1: Hyperparameters and network setup used to implement the proposed G-SAC
and G-Dreamer algorithms.
Hyperparameter G-SAC G-Dreamer
Discount factor Œ≥ 0.99 0.99
Scaling factor Œ≤ and Œ∂ 1 1
Optimizer for all the networks Adam Adam
Learning rate for VRNN parameters ŒΩ and Œ∏ 0.0008 0.0008
Learning rate for the reward parameter Œæ ‚àí 0.0008
Batch size for ŒΩ, Œ∏, and Œæ 4 4
Sequence length for ŒΩ, Œ∏, and Œæ 64 64
Learning rate for Gœà and ¬ØœÄœï 0.0003 0.00008
Batch size for œà and œï 4 50
Sequence length for for œà and œï 64 50
The imagination horizon N ‚àí 15
Appendix H Comparison of computational and memory complexity
Table H.1 compares the computational speeds and memory usage of VRM, Recurrent
Model-Free, G-SAC, and G-Dreamer when implemented over 1 million steps in the
Hopper-P environment, utilizing an Nvidia V100 GPU and 10 CPU cores for each train-
ing run. This comparison allows us to assess the computational time required for each
method as follows:
VRM has the lowest computing time and memory. VRM involves perceptual learning
and inference by learning a generative model and inferring belief states via feed-forward
neural networks. It also learns belief state representations through an LSTM. Our pro-
posed model-free G-SAC algorithm exhibits similar computational efficiency, with the
addition of KL divergence calculation in the information gain term of its unified objec-
tive function (referenced in Eq. (24)), leading to slightly increased computational and
memory demands compared to VRM. Our proposed model-based G-Dreamer requires
more computation and memory than both VRM and G-SAC, as it not only encompasses
the tasks performed by G-SAC but also learns the reward function, further elevating its
computational load.
Recurrent Model-Free, on the other hand, showcases the highest computational time,
primarily attributed to its RNN layer (LSTM), which processes the complete history of
observations and actions as inputs. At each time step t, an additional 7-dimensional
input (comprising a 6-dimensional observation ot and a scalar action at in Hopper-P) is
incorporated into the LSTM used in Recurrent Model-Free, posing challenges in RNN
training.
It should be noted that while VRM, G-SAC, and G-Dreamer also entail significant com-
putational demands arising from generative model learning, belief state inference, and
representation via ht = f(ht‚àí1, at, ot, st), with ht being 256-dimensional, their com-
plexity in our experiments is comparatively lower than that of Recurrent Model-Free.
This disparity can be attributed to two key factors:
i) VRM, G-SAC, and G-Dreamer learn the generative model and belief state via feed-
forward neural networks, which are less computationally demanding than RNNs.
ii) Although belief state representation learning ht = f(ht‚àí1, at, ot, st) in VRM, G-
Table H.1: Time and memory cost comparison in Hopper-P for 1 million steps.
Method Time Memory
G-Dreamer (ours) 8.7 hours 840 MB
VRM (Han et al., 2020) 3 hours 600 MB
G-SAC (ours) 4.8 hours 720 MB
Recurrent Model-Free (Ni et al., 2022) 17.5 hours 1.2 GB
SAC, and G-Dreamer is conducted via an LSTM with a high-dimensional input (a278-
dimensional combination, including a256-dimensional ht‚àí1, a scalarat, a 6-dimensional
ot, and a15-dimensional st in Hopper-P), the input length remains fixed over all decision-
making steps into the future. In contrast, the LSTM in Recurrent Model-Free starts
with an initial 6-dimensional observation o0, with an additional 7 dimensions added
after each step. Consequently, after about 39 time steps, the input dimension of the
LSTM in Recurrent Model-Free ( 6 + 39 √ó 7 = 279 ) surpasses the 278-dimensional
input in VRM, G-SAC, and G-Dreamer. Table H.1 shows that this increase in input
dimension leads to an increase in computation that exceeds that of VRM, G-SAC, and
G-Dreamer. The subsequent increase in input dimensionality leads to higher compu-
tational demands, surpassing those of VRM, G-SAC, and G-Dreamer. This is because
inputs with higher dimensions in a network require larger matrix multiplications per
step, leading to increased computational costs. This highlights the significant impact of
increased input dimensions in RNNs on computational demand, especially relevant in
our infinite horizon POMDP setting, where the agent may operate over a vast number
of steps. Furthermore, more memory is required to store inputs, outputs, intermediate
states, and gradients.
In conclusion, within an infinite horizon POMDP setting where the agent consis-
tently interacts with its environment, the escalating memory and computation demands
per step, driven by increased input dimensions, become increasingly significant. Em-
ploying belief state inference-based methods, rather than history-based approaches, not
only substantially reduces the memory footprint by abstracting detailed histories into
compact probabilistic representations but also improves computational efficiency, de-
spite the initial complexities associated with learning the underlying generative model
and belief state representation.
Appendix I Visualizations of results in Sub-section 8.1
In this sub-section, we provide the learning curves for all the compared methods in sub-
section 8.1. The average returns are shown in Fig. I.1, with the shaded areas indicating
the standard deviation.
(a) HalfCheetah-P
 (b) HalfCheetah-N
(c) Hopper-P
 (d) Hopper-N
(e) Ant-P
 (f) Ant-N
(g) Walker-2d-P
 (h) Walker-2d-N
Fig. I.1. Mean return for four Roboschool benchmarks with partial observations (left),
and noisy observations (right). Shaded areas indicate standard deviation.