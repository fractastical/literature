ARTICLE Communicated by Tim Verbelen
Reward Maximization Through Discrete Active Inference
Lancelot Da Costa
l.da-costa@imperial.ac.uk
DepartmentofMathematics,ImperialCollegeLondon,LondonSW72AZ,U.K.
Noor Sajid
noor.sajid.18@ucl.ac.uk
Thomas Parr
thomas.parr.12@ucl.ac.uk
Karl Friston
k.friston@ucl.ac.uk
WellcomeCentreforHumanNeuroimaging,UniversityCollegeLondon,
London,WC1N3AR,U.K.
Ryan Smith
rsmith@laureateinstitute.org
LaureateInstituteforBrainResearch,Tulsa,OK74136,U.S.A.
Active inference is a probabilistic framework for modeling the behavior
of biological and artificial agents, which derives from the principle of
minimizingfreeenergy.Inrecentyears,thisframeworkhasbeenapplied
successfullytoavarietyofsituationswherethegoalwastomaximizere-
ward,oftenofferingcomparableandsometimessuperiorperformanceto
alternativeapproaches.Inthisarticle,weclarifytheconnectionbetween
reward maximization and active inference by demonstrating how and
when active inference agents execute actions that are optimal for max-
imizing reward. Precisely, we show the conditions under which active
inference produces the optimal solution to the Bellman equation, a
formulation that underlies several approaches to model-based rein-
forcement learning and control. On partially observed Markov decision
processes, the standard active inference scheme can produce Bellman
optimal actions for planning horizons of 1 but not beyond. In contrast,
a recently developed recursive active inference scheme (sophisticated
inference) can produce Bellman optimal actions on any finite tempo-
ral horizon. We append the analysis with a discussion of the broader
relationship between active inference and reinforcement learning.
1 Introduction
1.1 Active Inference. Active inference is a normative framework for
modelingintelligentbehaviorinbiologicalandartificialagents.Itsimulates
NeuralComputation 35, 807–852(2023) © 2023 Massachusetts Institute of Technology
https://doi.org/10.1162/neco_a_01574
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

808 L. Da Costa et al.
behavior by numerically integrating equations of motion thought to de-
scribe the behavior of biological systems, a description based on the free
energyprinciple(Barpetal.,2022;Fristonetal.,2022).Activeinferencecom-
prisesacollectionofalgorithmsformodelingperception,learning,andde-
cision making in the context of both continuous and discrete state spaces
(Barp et al., 2022; Da Costa et al., 2020; Friston et al., 2021, 2010; Friston,
Parr, et al., 2017). Briefly, building active inference agents entails (1) equip-
ping the agent with a (generative) model of the environment, (2) fitting the
model to observations through approximate Bayesian inference by mini-
mizing variational free energy (i.e., optimizing an evidence lower bound
Beal, 2003; Bishop, 2006; Blei et al., 2017; Jordan et al., 1998) and (3) se-
lecting actions that minimize expected free energy, a quantity that that can
be decomposed into risk (i.e., the divergence between predicted and pre-
ferredpaths)andambiguity,leadingtocontext-specificcombinationsofex-
ploratory and exploitative behavior (Millidge, 2021; Schwartenbeck et al.,
2019).Thisframeworkhasbeenusedtosimulateandexplainintelligentbe-
haviorinneuroscience(Adamsetal.,2013;Parr,2019;Parretal.,2021;Sajid
et al., 2022), psychology and psychiatry (Smith, Khalsa, et al., 2021; Smith,
Kirlic,Stewart,Touthang,Kuplicki,Khalsa,etal.,2021;Smith,Kirlic,Stew-
art,Touthang,Kuplicki,McDermott,etal.,2021;Smith,Kuplicki,Feinstein,
et al., 2020; Smith, Kuplicki, Teed, et al., 2020; Smith, Mayeli, et al., 2021;
Smith, Schwartenbeck, Stewart, et al., 2020; Smith, Taylor, et al., 2022), ma-
chinelearning(Çataletal.,2020;Fountasetal.,2020;Mazzagliaetal.,2021;
Millidge, 2020; Tschantz et al., 2019; Tschantz, Millidge, et al., 2020), and
robotics (Çatal et al., 2021; Lanillos et al., 2020; Oliver et al., 2021; Pezzato
et al., 2020; Pio-Lopez et al., 2016; Sancaktar et al., 2020; Schneider et al.,
2022).
1.2 Reward Maximization through Active Inference?In contrast, the
traditional approaches to simulating and explaining intelligent behavior—
stochastic optimal control (Bellman, 1957; Bertsekas & Shreve, 1996) and
reinforcement learning (RL; Barto & Sutton, 1992)—derive from the nor-
mative principle of executing actions to maximize reward scoring the util-
ity afforded by each state of the world. This idea dates back to expected
utility theory (Von Neumann & Morgenstern, 1944), an economic model of
rational choice behavior, which also underwrites game theory (Von Neu-
mann & Morgenstern, 1944) and decision theory (Berger, 1985; Dayan &
Daw, 2008). Several empirical studies have shown that active inference can
successfullyperformtasksthatinvolvecollectingreward,often(butnotal-
ways) showing comparative or superior performance to RL (Cullen et al.,
2018;Markovi ´cetal.,2021;Mazzagliaetal.,2021;Millidge,2020;Pauletal.,
2021; Sajid, Ball, et al., 2021; Smith, Kirlic, Stewart, Touthang, Kuplicki,
Khalsa, et al., 2021; Smith, Kirlic, Stewart, Touthang, Kuplicki, McDermott,
etal.,2021;Smith,Schwartenbeck,Stewart,etal.,2020;Smith,Taylor,etal.,
2022; van der Himst & Lanillos, 2020) and marked improvements when
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 809
interacting with volatile environments (Markovi´c et al., 2021; Sajid, Ball,
et al., 2021). Given the prevalence and historical pedigree of reward maxi-
mization, we ask:Howandwhendoactiveinferenceagentsexecuteactionsthat
areoptimalwithrespecttorewardmaximization?
1.3 Organization of Paper. In this article, we explain (and prove)
how and when active inference agents exhibit (Bellman) optimal reward-
maximizing behavior.
For this, we start by restricting ourselves to the simplest problem: max-
imizing reward on a finite horizon Markov decision process (MDP) with
known transition probabilities—a sequential decision-making task with
completeinformation.Inthissetting,wereviewthebackward-inductional-
gorithmfromdynamicprogramming,whichformstheworkhorseofmany
optimalcontrolandmodel-basedRLalgorithms.Thisalgorithmfurnishesa
Bellman optimal state-action mapping, which means that it provides prov-
ably optimal decisions from the point of view of reward maximization
(see section 2).
We then introduce active inference on finite horizon MDPs (see section
3)—a scheme consisting of perception as inference followed by planning
as inference, which selects actions so that future states best align with pre-
ferred states.
Insection4,weshowhowandwhenactiveinferencemaximizesreward
in MDPs. Specifically, when the preferred distribution is a (uniform mix-
tureof)Diracdistribution(s)overreward-maximizingtrajectories,selecting
action sequences according to active inference maximizes reward (see sec-
tion 4.1). Yet active inference agents, in their standard implementation, can
select actions that maximize reward only when planning one step ahead
(see section 4.2). It takes a recursive, sophisticated form of active inference
to select actions that maximize reward—in the sense of a Bellman optimal
state-action mapping—on any finite time-horizon (see section 4.3).
In section 5, we introduce active inference on partially observable
Markov decision processes with known transition probabilities—a se-
quential decision-making task where states need to be inferred from
observations—andexplainhowtheresultsfromtheMDPsettinggeneralize
to this setting.
In section 6, we step back from the focus on reward maximization and
brieflydiscussdecisionmakingbeyondrewardmaximization,learningun-
known environments and reward functions, and outstanding challenges
in scaling active inference. We append this with a broader discussion of
the relationship between active inference and reinforcement learning in
appendix A.
Our findings are summarized in section 7.
All of our analyses assume that the agent knows the environmental dy-
namics (i.e., transition probabilities) and reward function. In appendix A,
we discuss how active inference agents can learn their world model and
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

810 L. Da Costa et al.
rewarding states when these are initially unknown—and the broader rela-
tionship between active inference and RL.
2 Reward Maximization on Finite Horizon MDPs
Inthissection,weconsidertheproblemofrewardmaximizationinMarkov
decision processes (MDPs) with known transition probabilities.
2.1 Basic Definitions. MDPs are a class of models specifying environ-
mental dynamics widely used in dynamic programming, model-basedRL,
andmorebroadlyinengineeringandartificialintelligence(Barto&Sutton,
1992; Stone, 2019). They are used to simulate sequential decision-making
taskswiththeobjectiveofmaximizingarewardorutilityfunction.AnMDP
specifiesenvironmentaldynamicsunfoldingindiscretespaceandtimeun-
der the actions pursued by an agent.
Definition 1 (Finite Horizon MDP). Afinite horizon MDP comprises the fol-
lowingcollectionofdata:
• S,afinitesetofstates.
• T ={ 0,..., T}, a finite set that stands for discrete time. T is the temporal
horizon(a.k.a.planninghorizon).
• A,afinitesetofactions.
• P(st = s′ | st−1 = s,at−1 = a),theprobabilitythatactiona ∈ A instates ∈
S attimet − 1willleadtostates ′ ∈ S attimet.s t arerandomvariablesover
S thatcorrespondtothestatebeingoccupiedattimet = 0,..., T.
• P(s0 = s),theprobabilityofbeingatstates ∈ S atthestartofthetrial.
• R(s),thefiniterewardreceivedbytheagentwhenatstates ∈ S.
ThedynamicsaffordedbyafinitehorizonMDP(seeFigure1)canbewrittenglob-
ally as a probability distribution over state trajectories s0:T := (s0,..., sT),g i v e n
asequenceofactionsa 0:T−1 := (a0,..., aT−1),whichfactorizesas
P(s0:T | a0:T−1) = P(s0)
T∏
τ=1
P(sτ | sτ−1,aτ−1).
Remark1 (OntheDefinitionofReward).Moregenerally,therewardfunc-
tioncanbetakentobedependentonthepreviousactionandpreviousstate:
Ra (s′ | s)istherewardreceivedaftertransitioningfromstate stostate s′ due
toaction a(Barto&Sutton,1992;Stone,2019).However,givenanMDPwith
such a reward function, we can recover our simplified setting by defining
a new MDP where the new states comprise the previous action, previous
state, and current state in the original MDP. By inspection, the resulting re-
wardfunctiononthenewMDPdependsonlyonthecurrentstate(i.e., R(s)).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 811
Figure 1: Finite horizon Markov decision process. This is a Markov decision
processpicturedasaBayesiannetwork(Jordanetal.,1998;Pearl,1998).Afinite
horizonMDPcomprisesafinitesequenceofstates,indexedintime.Thetransi-
tionfromonestatetothenextdependsonaction.Assuch,foranygivenaction
sequence,thedynamicsoftheMDPformaMarkovchainonstate-space.Inthis
fullyobservedsetting,actionscanbeselectedunderastate-actionpolicy, /Pi1,in-
dicatedwithadashedline:thisisaprobabilisticmappingfromstate-spaceand
time to actions.
Remark2 (AdmissibleActions).Ingeneral,itispossiblethatonly someac-
tionscanbetakenateachstate.Inthiscase,onedefines As tobethefiniteset
of (allowable) actions from states∈ S. All forthcoming results concerning
MDPs can be extended to this setting.
Toformalizewhatitmeanstochooseactionsineachstate,weintroduce
the notion of a state-action policy.
Definition 2 (State-action Policy). Astate-action policy/Pi1is aprobability dis-
tributionoveractionsthatdependsonthestatethattheagentoccupies,andtime.
Explicitly,
/Pi1: A × S × T → [0,1]
(a,s,t) ↦→ /Pi1(a| s,t)
∀(s,t) ∈ S × T :
∑
a∈A
/Pi1(a| s,t) = 1.
When st = s, we will write/Pi1(a| st): = /Pi1(a| s,t). Note that the action at the
temporal horizon T is redundant, as no further can be reaped from the environ-
ment. Therefore, one often specifies state-action policies only up to time T− 1,a s
/Pi1: A × S ×{ 0,..., T − 1}→ [0,1]. The state-action policy—as defined here—
can be regarded as a generalization of a deterministic state-action policy that as-
signstheprobabilityof1toanavailableactionand0otherwise.
Remark 3 (Time-Dependent State-Action Policies). The way an agent
choosesactionsattheendofitslifeisusuallygoingtobeverydifferentfrom
thewayitchoosesthemwhenithasalongerlifeaheadofit.In finitehorizon
decision problems, state-action policies should generally be considered to
be time-dependent, as time-independent optimal state-action policies may
not exist. To see this, consider the following simple example:S = Z/5Z
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

812 L. Da Costa et al.
(integers mod 5), T ={ 0,1,2},A ={ −1,0,+1},R(0) = R(2) = R(3) =
0,R(1) = 1,R(4) = 6. Optimal state-action policies are necessarily time-
dependent as the reward-maximizing trajectory from state 2 at time 0
consistsofreachingstate4,whiletheoptimaltrajectoryfromstate2attime
1 consists of reaching state 1. This is particular to finite-horizon decisions,
as, in infinite-horizon (discounted) problems, optimal state-action policies
can always be taken to be time-independent (Puterman, 2014, theorem
6.2.7).
Remark4 (ConflictingTerminologies:PolicyinActiveInference).Inactive
inference, apolicy is defined as a sequence of actions indexed in time.1 To
avoidterminologicalconfusion,weuseactionsequencestodenotepolicies
under active inference.
Attime t,thegoalistoselectanactionthatmaximizesfuturecumulative
reward:
R(st+1:T): =
T∑
τ=t+1
R(sτ ).
Specifically,thisentailsfollowingastate-actionpolicy /Pi1thatmaximizes
the state-valuefunction:
v/Pi1(s,t): = E/Pi1[R(st+1:T) | st = s]
for any (s,t) ∈ S × T. The state-value function scores the expected cumula-
tive reward if the agent pursues state-action policy/Pi1from the statest = s.
When the statest = s is clear from context, we will often writev/Pi1(st): =
v/Pi1(s,t). Loosely speaking, we will call the expected reward thereturn.
Remark 5 (Notation E/Pi1). While standard in RL (Barto & Sutton, 1992;
Stone, 2019), the notationE/Pi1[R(st+1:T) | st = s] can be confusing. It denotes
the expected reward, under the transition probabilities of the MDP and a
state-action policy/Pi1,t h a ti s ,
EP(st+1:T|at:T−1,st=s)/Pi1(at:T−1|st+1:T−1,st=s)[R(st+1:T)].
It is important to keep this correspondence in mind, as we will use both
notations depending on context.
Remark 6 (Temporal Discounting). In infinite horizon MDPs (i.e., whenT
is infinite), RLoften seeks to maximize the discounted sum of rewards,
1
Theseareanalogoustotemporallyextendedactionsoroptionsintroducedunderthe
options framework in RL(Stolle & Precup, 2002).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 813
v/Pi1(s,t): = E/Pi1
[ ∞∑
τ=t
γτ−tR(sτ+1) | st = s
]
,
foragiventemporaldiscountingterm γ ∈ (0,1)(Barto&Sutton,1992;Bert-
sekas&Shreve,1996;Kaelblingetal.,1998).Infact,temporaldiscountingis
addedtoensurethattheinfinitesumoffuturerewardsconvergestoafinite
value(Kaelblingetal.,1998).InfinitehorizonMDPs,temporaldiscounting
is not necessary so we setγ = 1 (see Schmidhuber, 2006, 2010).
Tofindthebeststate-actionpolicies,wewouldliketoranktheminterms
of theirreturn. We introduce a partial ordering such thata state-actionpol-
icy isbetterthan another if it yields a higher return in any situation:
/Pi1≥ /Pi1′ ⇐⇒ ∀(s,t) ∈ S × T : v/Pi1(s,t) ≥ v/Pi1′(s,t).
Similarly,astate-actionpolicy /Pi1isstrictlybetterthananother /Pi1′ ifityields
strictly higher returns:
/Pi1>/Pi1′ ⇐⇒ /Pi1≥ /Pi1′ and∃(s,t) ∈ S × T : v/Pi1(s,t) > v/Pi1′(s,t).
2.2 Bellman Optimal State-Action Policies. A state-action policy is
Bellman optimal if it is better than all alternatives.
Definition3 (BellmanOptimality).Astate-actionpolicy /Pi1∗ isBellmanoptimal
ifandonlyifitisbetterthanallotherstate-actionpolicies:
/Pi1∗ ≥ /Pi1,∀/Pi1.
In other words, it maximizes the state-value functionv/Pi1(s,t) f o ra n ys t a t esa t
timet.
It is important to verify that this concept is not vacuous.
Proposition 1 (Existence of Bellman Optimal State-Action Policies).G i v e n
a finite horizon MDP as specified in definition 1, there exists a Bellman optimal
state-actionpolicy /Pi1∗.
A proof is found in appendix B.1. Note that the uniqueness of the
Bellman optimal state-action policy is not implied by proposition 1; in-
deed, multiple Bellman optimal state-action policies may exist (Bertsekas
& Shreve, 1996; Puterman, 2014).
Now that we know that Bellman optimal state-action policies exist, we
cancharacterizethemasareturn-maximizingactionfollowedbyaBellman
optimal state-action policy.
Proposition2 (CharacterizationofBellmanOptimalState-ActionPolicies) .
Forastate-actionpolicy /Pi1,thefollowingareequivalent:
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

814 L. Da Costa et al.
1. /Pi1isBellmanoptimal.
2. /Pi1isboth
a. Bellmanoptimalwhenrestrictedto {1,..., T}.Inotherwords, ∀ state-
actionpolicy /Pi1′ and(s,t) ∈ S ×{ 1,... T}
v/Pi1(s,t) ≥ v/Pi1′(s,t).
b. Attime0, /Pi1selectsactionsthatmaximizereturn:
/Pi1(a| s,0)> 0 ⇐⇒ a∈ argmax
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a],
∀s∈ S. (2.1)
AproofisinappendixB.2.Notethatthischaracterizationoffersarecur-
sive way to construct Bellman optimal state-action policies by successively
selecting the best action, as specified by equation 2.1, starting fromT and
inducting backward (Puterman, 2014).
2.3 Backward Induction. Proposition 2 suggests a straightforward re-
cursivealgorithmtoconstructBellmanoptimalstate-actionpoliciesknown
asbackwardinduction (Puterman,2014).Backwardinductionhasalonghis-
tory. It was developed by the German mathematician Zermelo in 1913 to
provethatchesshasBellmanoptimalstrategies(Zermelo,1913).Instochas-
tic control, backward induction is one of the main methods for solving the
Bellmanequation(Adda&Cooper,2003;Miranda&Fackler,2002;Sargent,
2000).Ingametheory,thesamemethodisusedtocomputesubgameperfect
equilibria in sequential games (Fudenberg & Tirole, 1991).
Backwardinductionentailsplanningbackwardintime,fromagoalstate
attheendofaproblem,byrecursivelydeterminingthesequenceofactions
that enables reaching the goal. It proceeds by first considering the last time
at which a decision might be made and choosing what to do in any situa-
tionatthattimeinordertogettothegoalstate.Usingthisinformation,one
canthendeterminewhattodoatthesecond-to-lastdecisiontime.Thispro-
cesscontinuesbackwarduntilonehasdeterminedthebestactionforevery
possible situation or state at every point in time.
Proposition 3 (Backward Induction: Construction of Bellman Optimal
State-Action Policies).Backwardinduction
/Pi1(a| s,T − 1) > 0 ⇐⇒ a∈ argmax
a∈A
E[R(sT) | sT−1 = s,aT−1 = a], ∀s∈ S
/Pi1(a| s,T − 2) > 0 ⇐⇒ a∈ argmax
a∈A
E/Pi1[R(sT−1:T) | sT−2 = s,aT−2 = a],
∀s∈ S
...
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 815
/Pi1(a| s,0) > 0 ⇐⇒ a∈ argmax
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a], ∀s∈ S
(2.2)
definesaBellmanoptimalstate-actionpolicy /Pi1.Furthermore,thischaracterization
iscomplete:allBellmanoptimalstate-actionpoliciessatisfythebackwardinduction
relation,equation2.2.
Aproof is in appendix B.3.
Intuitively, the backward induction algorithm 2.2 consists of planning
backward, by starting from the end goal and working out the actions
neededtoachievethegoal.Togiveaconcreteexampleofthiskindofplan-
ning,backwardinductionwouldconsiderthefollowingactionsintheorder
shown:
1. Desired goal: I would like to go to the grocery store.
2. Intermediate action: I need to drive to the store.
3. Current best action: I should put my shoes on.
Proposition 3 tells us that to be optimal with respect to reward maxi-
mization, one must plan like backward induction. This will be central to
our analysis of reward maximization in active inference.
3 Active Inference on Finite Horizon MDPs
WenowturntointroducingactiveinferenceagentsonfinitehorizonMDPs
withknowntransitionprobabilities.Weassumethattheagent’sgenerative
model of its environment is given by the previously defined finite horizon
MDP (see definition 1). We do not consider the case where the transitions
have to be learned but comment on it in appendix A.2 (see also Da Costa
et al., 2020; Friston et al., 2016).
Inwhatfollows,wefixatime t ≥ 0andsupposethattheagenthasbeen
in statess0,..., st. To ease notation, we let⃗s:= st+1:T,⃗a:= at:T be the future
statesandfutureactions.Wedefine Qtobethe predictivedistribution,which
encodes the predicted future states and actions given that the agent is in
state st:
Q(⃗s,⃗a| st): =
T−1∏
τ=t
Q(sτ+1 | aτ ,sτ )Q(aτ | sτ ).
3.1 Perception as Inference.In active inference, perception entails in-
ferences about future, past, and current states given observations and a se-
quence of actions. When states are partially observed, this is done through
variational Bayesian inference by minimizing a free energy functional also
known as an evidence bound (Beal, 2003; Bishop, 2006; Blei et al., 2017;
Wainwright & Jordan, 2007).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

816 L. Da Costa et al.
In the MDPsetting, past and current states are known, so it is necessary
only to infer future states given the current state and action sequenceP(⃗s|
⃗a,st). These posterior distributionsP(⃗s| ⃗a,st) can be computed exactly in
virtue of the fact that the transition probabilities of the MDP are known;
hence, variational inference becomes exact Bayesian inference:
Q(⃗s| ⃗a,st): = P(⃗s| ⃗a,st) =
T−1∏
τ=t
P(sτ+1 | sτ ,aτ ). (3.1)
3.2 PlanningasInference. Nowthattheagenthasinferredfuturestates
given alternative action sequences, we must assess these alternative plans
by examining the resulting state trajectories. The objective that active in-
ferenceagentsoptimize—inordertoselectthebestpossibleactions—isthe
expected free energy(Barp et al., 2022; Da Costa et al., 2020; Friston et al.,
2021). Under active inference, agents minimize expected free energy in or-
der to maintain themselves distributed according to a target distributionC
over the state-spaceS encoding the agent’s preferences.
Definition 4 (Expected Free Energy on MDPs). On MDPs, the expected free
energyofanactionsequence ⃗astartingfr omst isdefinedas(Barpetal.,2022,see
section5):
G(⃗a| st) = DKL[Q(⃗s| ⃗a,st) | C(⃗s)], (3.2)
whereDKL is the KL-divergence. Therefore, minimizing expected free energy cor-
responds to making the distribution over predicted states close to the distribution
C that encodes prior preferences. Note that the expected free energy in partially
observed MDPs comprises an additional ambiguity term (see section 5), which is
droppedhereasthereisnoambiguityaboutobservedstates.
Since the expected free energy assesses the goodness of inferred fu-
ture states under a course of action, we can refer to planning as inference
(Attias, 2003; Botvinick & Toussaint, 2012). The expected free energy may
be rewritten as
G(⃗a| st) = EQ(⃗s|⃗a,st)[−logC(⃗s)]  
Expected surprise
− H[Q(⃗s| ⃗a,st)]  
Entropy of future states
. (3.3)
Hence, minimizing expected free energy minimizes the expected surprise
ofstates2 accordingto CandmaximizestheentropyofBayesianbeliefsover
2
The surprise (also known as self-information or surprisal) of states—logC(⃗s)i s
information-theoretic nomenclature (Stone, 2015) that scores the extent to which an
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 817
futurestates(amaximumentropyprinciple(Jaynes,1957a),whichissome-
times cast as keeping options open (Klyubin et al., 2008)).
Remark 7 (Numerical Tractability). The expected free energy is straight-
forward to compute using linear algebra. Given an action sequence⃗a,C(⃗s)
and Q(⃗s| ⃗a,st) are categorical distributions overST−t. Let their parameters
be c,s⃗a ∈ [0,1]|S|(T−1), where|·| denotes the cardinality of a set. Then the
expected free energy reads
G(⃗a| st) = sT
⃗a(logs⃗a − logc). (3.4)
Notwithstanding, equation 3.4 is expensive to evaluate repeatedly when
all possible action sequences are considered. In practice, one can adopt a
temporal mean field approximation over future states (Millidge, Tschantz,
& Buckley, 2020):
Q(⃗s| ⃗a,st) =
T∏
τ=t+1
Q(sτ | ⃗a,sτ−1) ≈
T∏
τ=t+1
Q(sτ | ⃗a,st),
which yields the simplified expression
G(⃗a| st) ≈
T∑
τ=t+1
DKL[Q(sτ | ⃗a,st) | C(sτ )]. (3.5)
Expression 3.5 is much easier to handle: for each action sequence⃗a,o n e
evaluates the summands sequentiallyτ = t+ 1,..., T, and if and when
the sum up toτ becomes significantly higher than the lowest expected
free energy encountered during planning,G(⃗a| st) is set to an arbitrarily
high value. SettingG(⃗a| st) to a high value is equivalent to pruning away
unlikely trajectories. This bears some similarity to decision tree pruning
proceduresusedinRL(Huysetal.,2012).Itfinessesexplorationofthedeci-
siontreeinfulldepthandprovidesanOccam’swindowforselectingaction
sequences.
Complementary approaches can help make planning tractable. For ex-
ample,hierarchicalgenerativemodelsfactorizedecisionsintomultiplelev-
els. By abstracting information at a higher-level, lower levels entertain
fewer actions (Friston et al., 2018), which reduces the depth of the decision
tree by orders of magnitude. Another approach is to use algorithms that
searchthedecisiontreeselectively,suchasMonteCarlotreesearch(Cham-
pion,Bowman,etal.,2021;Champion,DaCosta,etal.,2021;Fountasetal.,
observation is unusual underC. It does not imply that the agent experiences surprise
in a subjective or declarative sense.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

818 L. Da Costa et al.
2020; Maisto et al., 2021; Silver et al., 2016) and amortizing planning using
artificial neural networks (i.e., learning to plan) (Çatal et al., 2019; Fountas
et al., 2020; Millidge, 2019; Sajid, Tigas, et al., 2021).
4 Reward Maximization on MDPs through Active Inference
Here, we show how active inference solves the reward maximization
problem.
4.1 Reward Maximization as Reaching Preferences.From the defini-
tion of expected free energy, equation 3.2, active inference on MDPs can
be thought of as reaching and remaining at a target distributionC over
state-space.
The basic observation that underwrites the following is that the agent
willmaximizerewardwhenthestationarydistributionhasallofitsmasson
reward maximizing states. To illustrate this, we define a preference distri-
butionCβ,β > 0overstate-space S,suchthatpreferredstatesarerewarding
states:3
Cβ (σ ): = expβR(σ )∑
ς∈S expβR(ς) ∝ exp(βR(σ )), ∀σ ∈ S
⇐⇒ − logCβ (σ ) =− βR(σ )− c(β),
∀σ ∈ S, for somec(β) ∈ R constant w.r.tσ.
The (inverse temperature) parameterβ> 0 scores how motivated the
agent is to occupy reward-maximizing states. Note that statess∈ S that
maximizethereward R(s)maximize Cβ (s)andminimize −logCβ (s)forany
β> 0.
Usingtheadditivepropertyoftherewardfunction,wecanextend Cβ toa
probabilitydistributionovertrajectories ⃗σ := (σ1,...,σ T) ∈ ST.Specifically,
Cβ scores to what extent a trajectory is preferred over another trajectory:
Cβ (⃗σ ): = expβR(⃗σ )∑
⃗ς∈ST expβR(⃗ς) =
T∏
τ=1
expβR(στ )∑
ς∈S expβR(ς) =
T∏
τ=1
Cβ (στ ), ∀⃗σ ∈ ST
⇐⇒ − logCβ (⃗σ ) =− βR(⃗σ )− c′(β) =−
T∑
τ=1
βR(στ )− c′(β), ∀⃗σ ∈ ST,
(4.1)
wherec′(β): = c(β)T ∈ R is constant with regard to⃗σ.
3
Note the connection with statistical mechanics:β is an inverse temperature parame-
ter,−Ris a potential function, andCβ is the corresponding Gibbs distribution (Pavliotis,
2014; Rahme & Adams, 2019).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 819
When preferences are defined in this way, the preference distribution
assigns exponentially more mass to states or trajectories that have a higher
reward. Put simply, for trajectories⃗σ, ⃗ς ∈ ST with rewardR(⃗σ ) > R(⃗ς), the
ratio of preference mass will be the exponential of the weighted difference
in reward, where the weight is the inverse temperature:
Cβ (⃗σ )
Cβ (⃗ς) = exp(βR(⃗σ ))
exp(βR(⃗ς)) = exp(β(R(⃗σ )− R(⃗ς))). (4.2)
As the temperature tends to zero, the ratio diverges so thatCβ (⃗σ ) becomes
infinitelylargerthan Cβ (⃗ς).As Cβ isaprobabilitydistribution(withamax-
imal value of one), we must haveCβ (⃗ς)
β→+∞
−→ 0 for any suboptimal trajec-
tory ⃗ς and positive preference for reward maximizing trajectories (as all
preferences must sum to one). In addition, all reward maximizing trajecto-
ries have the same probability mass by equation 4.2. Thus, in the zero tem-
peraturelimit,preferencesbecomeauniformmixtureofDiracdistributions
over reward-maximizing trajectories:
lim
β→+∞
Cβ ∝
∑
⃗σ∈IT−t
Dirac⃗σ , I := argmax
s∈S
R(s). (4.3)
Of course, the above holds for preferences over individual states as it does
for preferences over trajectories.
Wenowshowhowreachingpreferredstatescanbeformulatedasreward
maximization:
Lemma 1. Thesequenceofactionsthatminimizesexpectedfreeenergyalsomax-
imizesexpectedrewardinthezerotemperaturelimit β →+ ∞ (seeequation4.3):
lim
β→+∞
argmin
⃗a
G(⃗a| st) ⊆ argmax
⃗a
EQ(⃗s|⃗a,st)[R(⃗s)].
Furthermore,ofthoseactionsequencesthatmaximizeexpectedreward,theexpected
free energy minimizers will be those that maximize the entropy of future states
H[Q(⃗s| ⃗a,st)].
A proof is in appendix B.4. In the zero temperature limitβ →+ ∞,
minimizing expected free energy corresponds to choosing the action se-
quence⃗asuch thatQ(⃗s| ⃗a,st) has most mass on reward-maximizing states
or trajectories (see Figure 2). Of those reward-maximizing candidates, the
minimizer of expected free energy maximizes the entropy of future states
H[Q(⃗s| ⃗a,st)], thus keeping options open.
4.2 Reward Maximization on MDPs with a Temporal Horizon of 1.
In this section, we first consider the case of a single-step decision problem
(i.e.,atemporalhorizonof T = 1)anddemonstratehowthestandardactive
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

820 L. Da Costa et al.
Figure 2: Reaching preferences and the zero temperature limit. We illustrate
how active inference selects actions such that the predictive distributionQ(⃗s|
⃗a, st) most closely matches the preference distributionCβ (⃗s) (top right). We il-
lustrate this with a temporal horizon of one, so that state sequences are states,
which are easier to plot, but all holds analogously for sequences of arbitrary
finite length. In this example, the state-space is a discretization of a real inter-
val,andthepredictiveandpreferencedistributionshaveagaussianshape.The
predictive distributionQ is assumed to have a fixed variance with respect to
action sequences, such that the only parameter that can be optimized by ac-
tion selection is its mean. In the zero temperature limit, equation 4.3,Cβ be-
comes a Dirac distribution over the reward-maximizing state (bottom). Thus,
minimizing expected free energy corresponds to selecting the action such that
the predicted states assign most probability mass to the reward-maximizing
state (bottom-right). Here,Q∗ := Q(⃗s| ⃗a∗, st) denotes the predictive distribu-
tion over states given the action sequence that minimizes expected free energy
⃗a∗ = argmin⃗a G(⃗a| st).
inference scheme maximizes reward on this problem in the limitβ →+ ∞.
Thiswillactasanimportantbuildingblockforwhenwesubsequentlycon-
sider more general multistep decision problems.
The standard decision-making procedure in active inference consists of
assigning each action sequence with a probability given by the softmax of
the negative expected free energy (Barp et al., 2022; Da Costa et al., 2020;
Friston, FitzGerald, et al., 2017):
Q(⃗a| st) ∝ exp(−G(⃗a| st)).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 821
Table1: StandardActiveInferenceSchemeonFiniteHorizonMDPs(Barpetal.,
2022, section 5).
Process Computation
Perceptual inference Q(⃗s| ⃗a, st) = P(⃗s| ⃗a,st) = ∏ T−1
τ=t P(sτ+1 | sτ , aτ )
Planning as inference G(⃗a| st) = DKL[Q(⃗s| ⃗a, st) | C(⃗s)]
Decision making Q(⃗a| st) ∝ exp(−G(⃗a| st))
Action selection at ∈ argmaxa∈A
[
Q(at = a| st) = ∑
⃗a Q(at = a| ⃗a)Q(⃗a| st)
]
Agents then select the most likely action under this distribution:
at ∈ argmax
a∈A
Q(a| st) = argmax
a∈A
∑
⃗a
Q(a| ⃗a)Q(⃗a| st)
= argmax
a∈A
∑
⃗a
Q(a| ⃗a)exp(−G(⃗a| st)) = argmax
a∈A
∑
⃗a
(⃗a)t=a
exp(−G(⃗a| st)).
In summary, this scheme selects the first action within action sequences
that, on average, maximize their exponentiated negative expected free
energies. As a corollary, if the first action is in a sequence with a very
low expected free energy, this adds an exponentially large contribution
to the selection of this particular action. We summarize this scheme in
Table 1.
Theorem 1. In MDPs with known transition probabilities and in the zero tem-
peraturelimit β →+ ∞ (4.3),theschemeofTable1,
at ∈ lim
β→+∞
argmax
a∈A
∑
⃗a
(⃗a)t=a
exp(−G(⃗a| st)),
G(⃗a| st) = DKL[Q(⃗s| ⃗a,st) | Cβ (⃗s)], (4.4)
isBellmanoptimalforthetemporalhorizonT = 1.
A proof is in appendix B.5. Importantly,the standard active inference
scheme,equation4.4,fallsshortintermsofBellmanoptimalityonplanninghori-
zons greater than one; this rests on the fact that it does not coincide with
backward induction. Recall that backward induction offers a complete de-
scriptionofBellmanoptimalstate-actionpolicies(seeproposition3).Incon-
trast, active inference plans by adding weighted expected free energies of
each possible future course of action. In other words, unlike backward in-
duction, it considers future courses of action beyond the subset that will
subsequently minimize expected free energy, given subsequently encoun-
tered states.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

822 L. Da Costa et al.
4.3 Reward Maximization on MDPs with Finite Temporal Hori-
zons. To achieve Bellman optimality on finite temporal horizons, we turn
to the expected free energy of an action given future actions that also mini-
mizeexpectedfreeenergy.Todothis,wecanwritetheexpectedfreeenergy
recursively, as the immediate expected free energy, plus the expected free
energy that one would obtain by subsequently selecting actions that mini-
mize expected free energy (Friston et al., 2021). The resulting scheme con-
sists of minimizing an expected free energy defined recursively, from the
last time step to the current time step. In finite horizon MDPs, this reads
G(aT−1 | sT−1) = DKL[Q(sT | aT−1,sT−1) | Cβ (sT)]
G(aτ | sτ ) = DKL[Q(sτ+1 | aτ ,sτ ) | Cβ (sτ+1)]
+ EQ(aτ+1,sτ+1|aτ ,sτ )[G(aτ+1 | sτ+1)],τ = t,..., T − 2,
where, at each time step, actions are chosen to minimize expected free
energy:
Q(aτ+1 | sτ+1) > 0 ⇐⇒ aτ+1 ∈ argmin
a∈A
G(a| sτ+1). (4.5)
To make sense of this formulation, we unravel the recursion,
G(at | st) = DKL[Q(st+1 | at,st) | Cβ (st+1)]+ EQ(at+1,st+1|at,st)[G(at+1 | st+1)]
= DKL[Q(st+1 | at,st) | Cβ (st+1)]
+ EQ(at+1,st+1|at,st)
[
DKL[Q(st+2 | at+1,st+1) | Cβ (st+2)]
]
+ EQ(at+1:t+2,st+1:t+2|at,st)[G(at+2 | st+2)]
= ... = EQ(⃗a,⃗s|at,st)
T−1∑
τ=t
DKL[Q(sτ+1 | aτ ,sτ ) | Cβ (sτ+1)]
= EQ(⃗a,⃗s|at,st)DKL[Q(⃗s| ⃗a,st) | Cβ (⃗s)], (4.6)
which shows that this expression is exactly the expected free energy under
action at, if one is to pursue future actions that minimize expected free en-
ergy, equation 4.5. We summarize this “sophisticated inference” scheme in
Table 2.
Thecrucialimprovementoverthestandardactiveinferencescheme(see
Table1)isthatplanningisnowperformedbasedonsubsequentcounterfac-
tual actions that minimize expected free energy as opposed to considering
allfuturecoursesofaction.Translatingthisintothelanguageofstate-action
policies yields∀s∈ S:
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 823
Table2: Sophisticatedactiveinferencescheme onfinitehorizonMDPs(Friston
et al., 2021).
Process Computation
Perceptual inference Q(sτ+1 | aτ , sτ ) = P(sτ+1 | aτ , sτ )
Planning as inference G(aτ | sτ ) = DKL[Q(sτ+1 | aτ , sτ ) | Cβ (sτ+1)]...
... + EQ(aτ+1,sτ+1|aτ ,sτ )[G(aτ+1 | sτ+1)]
Decision making Q(aτ | sτ ) > 0 ⇐⇒ aτ ∈ argmina∈A G(a| sτ )
Action selection at ∼ Q(at | st)
aT−1(s) ∈ argmin
a∈A
G(a| sT−1 = s)
aT−2(s) ∈ argmin
a∈A
G(a| sT−2 = s)
...
a1(s) ∈ argmin
a∈A
G(a| s1 = s)
a0(s) ∈ argmin
a∈A
G(a| s0). (4.7)
Equation 4.7 is strikingly similar to the backward induction algorithm
(proposition3),andindeedwerecoverbackwardinductioninthelimit β →
+∞.
Theorem2 (BackwardInductionasActiveInference) .InMDPswithknown
transitionprobabilitiesandinthezerotemperaturelimit β →+ ∞,equation4.3,
theschemeofTable2,
Q(aτ | sτ )> 0 ⇐⇒ at ∈ lim
β→+∞
argmin
a∈A
G(a| sτ )
G(aτ | sτ ) = DKL[Q(sτ+1 | aτ ,sτ ) | Cβ (sτ+1)]
+ EQ(aτ+1,sτ+1|aτ ,sτ )[G(aτ+1 | sτ+1)], (4.8)
is Bellman optimal on any finite temporal horizon as it coincides with the back-
ward induction algorithm from proposition 3. Furthermore, if there are multiple
actionsthatmaximizefuturereward,thosethatareselectedbyactiveinferencealso
maximizetheentropyoffuturestates H[Q(⃗s| ⃗a,a,s0)].
Note that maximizing the entropy of future states keeps the agent’s op-
tions open (Klyubin et al., 2008) in the sense of committing the least to a
specified sequence of states. Aproof of theorem 2 is in appendix B.6.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

824 L. Da Costa et al.
5 Generalization to POMDPs
Partially observable Markov decision processes (POMDPs) generalize
MDPs in that the agent observes a modalityot, which carries incomplete
informationaboutthecurrentstate st,asopposedtothecurrentstateitself.
Definition 5 (Finite Horizon POMDP).Afinite horizon POMDPisan MDP
(seedefinition1)withthefollowingadditionaldata:
• O isafinitesetofobservations.
• P(ot = o| st = s)istheprobabilitythatthestates ∈ S attimet willleadto
the observation o∈ O at timet. ot are random variables overO that corre-
spondtotheobservationbeingsampledattimet = 0,..., T.
5.1 ActiveInferenceonFiniteHorizonPOMDPs. Webrieflyintroduce
active inference agents on finite horizon POMDPs with known transition
probabilities (for more details, see Da Costa et al., 2020; Parr et al., 2022;
Smith,Friston,etal.,2022).Weassumethattheagent’sgenerativemodelof
its environment is given by POMDP(see definition 5).4
Let⃗s:= s0:T,⃗a:= a0:T−1 be all states and actions (past, present, and fu-
ture),let ˜o:= o0:t betheobservationsavailableuptotime t,andlet ⃗o:= ot+1:T
be the future observations. The agent has a predictive distribution over
states given actions
Q(⃗s| ⃗a, ˜o): =
T−1∏
τ=0
Q(sτ+1 | aτ ,sτ , ˜o),
which is continuously updated following new observations.
5.1.1 Perception as Inference.In active inference, perception entails in-
ferences about (past, present, and future) states given observations and a
sequenceofactions.Whenstatesarepartiallyobserved,theposteriordistri-
butionP(⃗s| ⃗a, ˜o) is intractable to compute directly. Thus, one approximates
it by optimizing a variational free energy functionalF⃗a (also known as an
evidence bound; Beal, 2003; Bishop, 2006; Blei et al., 2017; Wainwright &
Jordan, 2007) over a space of probability distributionsQ(·| ⃗a, ˜o) called the
variationalfamily:
P(⃗s| ⃗a, ˜o) = argmin
Q
F⃗a[Q(⃗s| ⃗a, ˜o)] = argmin
Q
DKL[Q(⃗s| ⃗a, ˜o) | P(⃗s| ⃗a, ˜o)]
F⃗a[Q(⃗s| ⃗a, ˜o)] := EQ(⃗s|⃗a,˜o)[logQ(⃗s| ⃗a, ˜o)− logP(˜o,⃗s| ⃗a)].
(5.1)
4
Wedonotconsiderthecasewherethemodelparametershavetobelearnedbutcom-
ment on it in appendix A.2 (details in Da Costa et al., 2020; Friston et al., 2016).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 825
Here,P(˜o,⃗s| ⃗a)isthePOMDP,whichissuppliedtotheagent,and P(⃗s| ⃗a, ˜o).
When the free energy minimum (see equation 5.1) is reached, the inference
is exact:
Q(⃗s| ⃗a, ˜o) = P(⃗s| ⃗a, ˜o). (5.2)
For numerical tractability, the variational family may be constrained to a
parametricfamilyofdistributions,inwhichcaseequalityisnotguaranteed:
Q(⃗s| ⃗a, ˜o) ≈ P(⃗s| ⃗a, ˜o). (5.3)
5.1.2 PlanningasInference. Theobjectivethatactiveinferenceminimizes
inordertheselectthebestpossiblecoursesofactionisthe expectedfreeenergy
(Barpetal.,2022;DaCostaetal.,2020;Fristonetal.,2021).InPOMDPs,the
expected free energy reads (Barp et al., 2022, section 5)
G(⃗a| ˜o) = DKL[Q(⃗s| ⃗a, ˜o) | Cβ (⃗s)]  
Risk
+EQ(⃗s|⃗a,˜o)H[P(⃗o|⃗s)]  
Ambiguity
.
TheexpectedfreeenergyonPOMDPsistheexpectedfreeenergyonMDPs
plusanextratermcalled ambiguity.Thisambiguitytermaccommodatesthe
uncertainty implicit in partially observed problems. The reason that this
resulting functional is called expected free energy is because it comprises
a relative entropy (risk) and expected energy (ambiguity). The expected
freeenergyobjectivesubsumesseveraldecision-makingobjectivesthatpre-
dominate in statistics, machine learning, and psychology, which confers it
with several useful properties when simulating behavior (see Figure 3 for
details).
5.2 Maximizing Reward on POMDPs. Crucially, our reward maxi-
mization results translate to the POMDP case. To make this explicit, we
rehearse lemma 1 in the context of POMDPs.
Proposition 4 (Reward Maximization on POMDPs). In POMDPs with
known transition probabilities, provided that the free energy minimum is reached
(seeequation5.2),thesequenceofactionsthatminimizesexpectedfreeenergyalso
maximizes expected reward in the zero temperature limitβ →+ ∞ (see equation
4.3):
lim
β→+∞
argmin
⃗a
G(⃗a| ˜o) ⊆ argmax
⃗a
EQ(⃗s|⃗a,˜o)[R(⃗s)].
Furthermore, of those action sequences that maximize expected reward, the ex-
pected free energy minimizers will be those that maximize the entropy of future
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

826 L. Da Costa et al.
Figure 3: Active inference. The top panels illustrate the perception-action loop
inactiveinference,intermsofminimizationofvariationalandexpectedfreeen-
ergy.Thelowerpanelsillustratehowexpectedfreeenergyrelatestoseveralde-
scriptionsofbehaviorthatpredominateinthepsychological,machinelearning,
and economics. These descriptions are disclosed when one removes particular
terms from the objective. For example, if we ignore extrinsic value, we are left
with intrinsic value, variously known as expected information gain (Lindley,
1956;MacKay,2003).Thisunderwritesintrinsicmotivationinmachinelearning
androbotics(Bartoetal.,2013;Deci&Ryan,1985;Oudeyer&Kaplan,2007)and
expected Bayesian surprise in visual search (Itti & Baldi, 2009; Sun et al., 2011)
andtheorganizationofourvisualapparatus(Barlow,1961,1974;Linsker,1990;
Optican&Richmond,1987).Intheabsenceofambiguity,weareleftwithmini-
mizing risk, which corresponds to aligning predicted states to preferred states.
Thisleadstorisk-aversedecisionsinbehavioraleconomics(Kahneman&Tver-
sky, 1979) and formulations of control as inference in engineering such as KL
control(vandenBroeketal.,2010).Ifwethenremoveintrinsicvalue,weareleft
with expected utility in economics (Von Neumann & Morgenstern, 1944) that
underwritesRLandbehavioralpsychology(Barto&Sutton,1992).Bayesianfor-
mulations of maximizing expected utility under uncertainty are also the basis
of Bayesian decision theory (Berger, 1985). Finally, if we only consider a fully
observed environment with no preferences, minimizing expected free energy
corresponds to a maximum entropy principle over future states (Jaynes, 1957b,
1957a). Note that hereC(o) denotes the preferences over observations derived
from the preferences over states. These are related byP(o| s)C(s) = P(s| o)C(o).
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 827
states minus the (expected) entropy of outcomes given statesH[Q(⃗s| ⃗a, ˜o)]−
EQ(⃗s|at,˜o)H[P(⃗o|⃗s)]].
From proposition 4, we see that if there are multiple maximize reward
action sequences, those that are selected maximize
H[Q(⃗s| ⃗a, ˜o)]  
Entropy of future states
− EQ(⃗s|at,˜o)[H[P(⃗o|⃗s)]]  
Entropy of observations given future states
.
In other words, they least commit to a prespecified sequence of future
states and ensure that their expected observations are maximally informa-
tive of states. Of course, when inferences are inexact, the extent to which
proposition 4 holds depends on the accuracy of the approximation, equa-
tion 5.3. Aproof of proposition 4 is in appendix B.7.
The schemes of Tables 1 and 2 exist in the POMDP setting, (e.g., Barp
et al., 2022, section 5, and Friston et al., 2021, respectively). Thus, in
POMDPswithknowntransitionprobabilities,providedthatinferencesare
exact (see equation 5.2) and in the zero temperature limitβ →+ ∞ (see
equation 4.3), standard active inference (Barp et al., 2022, section 5) max-
imizesrewardontemporalhorizonsofonebutnotbeyond,andarecursive
scheme such as sophisticated active inference (Friston et al., 2021) max-
imizes reward on finite temporal horizons. Note that for computational
tractability, the sophisticated active inference scheme presented in Friston
et al. (2021) does not generally perform exact inference; thus, the extent to
which it will maximize reward in practice will depend on the accuracy of
its inferences. Nevertheless, our results indicate that sophisticated active
inference will vastly outperform standard active inference in most reward-
maximization tasks.
6 Discussion
In this article, we have examined a specific notion of optimality, namely,
Bellman optimality, defined as selecting actions to maximize future ex-
pected rewards. We demonstrated how and when active inference is Bell-
manoptimalonfinitehorizonPOMDPswithknowntransitionprobabilities
and reward function.
These results highlight important relationships among active inference,
stochastic control, and RL, as well as conditions under which they would
and would not be expected to behave similarly (e.g., environments with
multiple reward-maximizing trajectories, those affording ambiguous ob-
servations). We refer readers to appendix Afor a broader discussion of the
relationship between active inference and reinforcement learning.
6.1 Decision Making beyond Reward Maximization.More broadly,
it is important to ask if reward maximization is the right objective
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

828 L. Da Costa et al.
underwritingintelligentdecisionmaking.Thisisanimportantquestionfor
decision neuroscience. That is, do humans optimize a reward signal, ex-
pected free energy, or other planning objectives? This can be addressed by
comparingtheevidenceforthesecompetinghypothesesbasedonempirical
data (Smith, Kirlic, Stewart, Touthang, Kuplicki, Khalsa, et al., 2021; Smith,
Kirlic, Stewart, Touthang, Kuplicki, McDermott, et al., 2021; Smith,
Schwartenbeck,Stewart,etal.,2020;Smith,Taylor,etal.,2022).Currentem-
pirical evidence suggests that humans are not purely reward-maximizing
agents; they also engage in both random and directed exploration (Daw
et al., 2006; Gershman, 2018; Mirza et al., 2018; Schulz & Gershman, 2019;
Wilson et al., 2021, 2014; Xu et al., 2021) and keep their options open
(Schwartenbeck, FitzGerald, Mathys, Dolan, Kronbichler, et al., 2015). As
we have illustrated, active inference implements a clear form of directed
exploration through minimizing expected free energy. Although not cov-
ered in detail here, active inference can also accommodate random explo-
ration by sampling actions from the posterior belief over action sequences,
as opposed to selecting the most likely action as presented in Tables 1
and 2.
Note that behavioral evidence favoring models that do not solely maxi-
mizerewardwithinreward-maximizationtasks—thatis,where“maximize
reward”istheexplicitinstruction—isnotacontradiction.Rather,gathering
information about the environment (exploration) generally helps to reap
more reward in the long run, as opposed to greedily maximizing reward
based on imperfect knowledge (Cullen et al., 2018; Sajid, Ball, et al., 2021).
This observation is not new, and many approaches to simulating adaptive
agents employed today differ significantly from their reward-maximizing
antecedents (see appendix A.3).
6.2 Learning. When the transition probabilities or reward function are
unknowntotheagent,theproblembecomesoneofreinforcementlearning
(RL; Shoham et al., 2003 as opposed to stochastic control. Although we did
not explicitly consider it above, this scenario can be accommodated by ac-
tive inference by simply equipping the generative model with a prior and
updatingthemodelusingvariationalBayesianinferencetobestfitobserved
data. Depending on the specific learning problem and generative model
structure, this can involve updating the transition probabilities and/or the
target distributionC. In POMDPs it can also involve updating the prob-
abilities of observations under each state. We refer to appendix A.2 for
discussion of reward learning through active inference and connections to
representative RL approaches, and Da Costa et al. (2020) and Friston et al.
(2016) for learning transition probabilities through active inference.
6.3 ScalingActiveInference. WhencomparingRLandactiveinference
approachesgenerally,oneoutstandingissueforactiveinferenceiswhether
it can be scaled up to solve the more complex problems currently handled
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 829
by RL in machine learning contexts (Çatal et al., 2020, 2021; Fountas et al.,
2020; Mazzaglia et al., 2021; Millidge, 2020; Tschantz et al., 2019). This is an
area of active research.
Oneimportantissuealongtheselinesisthatplanningaheadbyevaluat-
ingallormanypossiblesequencesofactionsiscomputationallyprohibitive
in many applications. Three complementary solutions have emerged:
(1) employing hierarchical generative models that factorize decisions into
multiple levels and reduce the size of the decision tree by orders of magni-
tude (Çatal et al., 2021; Friston et al., 2018; Parr et al., 2021); (2) efficiently
searching the decision tree using algorithms like Monte Carlo tree search
(Champion, Bowman, et al., 2021; Champion, Da Costa, et al., 2021; Foun-
tas et al., 2020; Maisto et al., 2021; Silver et al., 2016); and (3) amortizing
planning using artificial neural networks (Çatal et al., 2019; Fountas et al.,
2020; Millidge, 2019; Sajid, Tigas, et al., 2021).
Another issue rests on learning the generative model. Active inference
may readily learn the parameters of a generative model; however, more
work needs to be done on devising algorithms for learning the structure of
generative models themselves (Friston, Lin, et al., 2017; Smith, Schwarten-
beck,Parr,etal.,2020).Thisisanimportantresearchproblemingenerative
modeling,calledBayesianmodelselectionorstructurelearning(Gershman
& Niv, 2010; Tervo et al., 2016).
Note that these issues are not unique to active inference. Model-based
RL algorithms deal with the same combinatorial explosion when evaluat-
ingdecisiontrees,whichisoneprimarymotivationfordevelopingefficient
model-freeRLalgorithms.However,otherheuristicshavealsobeendevel-
opedforefficientlysearchingandpruningdecisiontreesinmodel-basedRL
(Huys et al., 2012; Lally et al., 2017). Furthermore, model-based RL suffers
the same limitation regarding learning generative model structure. Yet RL
may have much to offer active inference in terms of efficient implementa-
tionandtheidentificationofmethodstoscaletomorecomplexapplications
(Fountas et al., 2020; Mazzaglia et al., 2021).
7C o n c l u s i o n
In summary, we have shown that under the specification that the active
inference agent prefers maximizing reward, equation 4.3:
1. On finite horizon POMDPs with known transition probabilities, the
objective optimized for action selection in active inference (i.e., ex-
pected free energy) produces reward-maximizing action sequences
when state estimation is exact. When there are multiple reward-
maximizing candidates, this selects those sequences that maximize
the entropy of future states—thereby keeping options open—and
that minimize the ambiguity of future observations so that they are
maximally informative. More generally, the extent to which action
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

830 L. Da Costa et al.
sequences will be reward maximizing will depend on the accuracy
of state estimation.
2. The standard active inference scheme (e.g., Barp et al., 2022, section
5) produces Bellman optimal actions for planning horizons of one
when state estimation is exact but not beyond.
3. Asophisticatedactiveinferencescheme(e.g.,Fristonetal.,2021)pro-
duces Bellman optimal actions on any finite planning horizon when
state estimation is exact. Furthermore, this scheme generalizes the
well-known backward induction algorithm from dynamic program-
ming to partially observed environments. Note that for computa-
tional efficiency, the scheme presented in Friston et al. (2021), does
notgenerallyperformexactstateestimation;thus,theextenttowhich
itwillmaximizerewardinpracticewilldependontheaccuracyofits
inferences.Nevertheless,itisclearfromourresultsthatsophisticated
active inference will vastly outperform standard active inference in
most reward-maximization tasks.
Note that for computational tractability, the sophisticated active infer-
ence scheme presented in Friston et al. (2021) does not generally perform
exact inference; thus, the extent to which it will maximize reward in prac-
tice will depend on the accuracy of its inferences. Nevertheless, it is clear
fromtheseresultsthatsophisticatedactiveinferencewillvastlyoutperform
standard active inference in most reward-maximization tasks.
In conclusion, the sophisticated active inference scheme should be the
method of choice when applying active inference to optimally solve the
reward-maximization problems considered here.
Appendix A: Active Inference and Reinforcement Learning
This article considers how active inference can solve the stochastic control
problem.Inthisappendix,wediscussthebroaderrelationshipbetweenac-
tive inference and RL.
Loosely speaking, RL is the field of methodologies and algorithms that
learn reward-maximizing actions from data and seek to maximize reward
in the long run. Because RL is a data-driven field, algorithms are selected
based on how well they perform on benchmark problems. This has pro-
duced a plethora of diverse algorithms, many designed to solve specific
problems, each with its own strengths and limitations. This makes RL dif-
ficult to characterize as a whole. Thankfully, many approaches to model-
based RL and control can be traced back to approximating the optimal
solution to the Bellman equation (Bellman & Dreyfus, 2015; Bertsekas &
Shreve, 1996) (although this may become computationally intractable in
highdimensions;Barto&Sutton,1992).Ourresultsshowedhowandwhen
decisions under active inference and such RLapproaches are similar.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 831
This appendix discusses how active inference and RL relate and differ
more generally. Their relationship has become increasingly important to
understand, as a growing body of research has begun to (1) compare the
performanceofactiveinferenceandRLmodelsinsimulatedenvironments
(Cullen et al., 2018; Millidge, 2020; Sajid, Ball, et al., 2021), (2) apply active
inference to model human behavior on reward learning tasks (Smith, Kir-
lic, Stewart, Touthang, Kuplicki, Khalsa, et al., 2021; Smith, Kirlic, Stew-
art, Touthang, Kuplicki, McDermott, et al., 2021; Smith, Schwartenbeck,
Stewart,etal.,2020;Smith,Taylor,etal.,2022),and(3)considerthecomple-
mentary predictions and interpretations each offers in computational neu-
roscience,psychology,andpsychiatry(Cullenetal.,2018;Huysetal.,2012;
Schwartenbeck,FitzGerald,Mathys,Dolan,&Friston,2015;Schwartenbeck
et al., 2019; Tschantz, Seth, et al., 2020).
A.1 Main Differences between Active Inference and Reinforcement
Learning.
A.1.1 Philosophy. ActiveinferenceandRLdifferprofoundlyintheirphi-
losophy. RL derives from the normative principle of maximizing reward
(Barto & Sutton, 1992), while active inference describes systems that main-
taintheirstructuralintegrityovertime(Barpetal.,2022;Fristonetal.,2022).
Despite this difference, these frameworks have many practical similarities.
For example, recall that behavior in active inference is completely deter-
mined by the agent’s preferences, determined as priors in their generative
model.Crucially,logpriorscanbeinterpretedasrewardfunctionsandvice
versa,whichishowbehaviorunderRLandactiveinferencecanberelated.
A.1.2 Model Based and Model Free.Active inference agents always em-
body a generative (i.e., forward) model of their environment, while RL
comprises both model-based and simpler model-free algorithms. In brief,
“model-free” means that agents learn a reward-maximizing state-action
mapping, based on updating cached state-action pair values through ini-
tially random actions that do not consider future state transitions. In
contrast, model-based RL algorithms attempt to extend stochastic control
approaches by learning the dynamics and reward function from data. Re-
call that stochastic control calls on strategies that evaluate different actions
on a carefully handcrafted forward model of dynamics (i.e., known transi-
tion probabilities) to finally execute the reward-maximizing action. Under
this terminology, all active inference agents are model-based.
A.1.3 Modeling Exploration.Exploratory behavior—which can improve
reward maximization in the long run—is implemented differently in the
two approaches. In most cases, RL implements a simple form of explo-
ration by incorporating randomness in decision making (Tokic & Palm,
2011; Wilson et al., 2014), where the level of randomness may or may not
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

832 L. Da Costa et al.
change over time as a function of uncertainty. In other cases, RL incorpo-
rates ad hoc information bonuses in the reward function or other decision-
making objectives to build in directed exploratory drives (e.g., upper-
confidence-boundalgorithmsorThompsonsampling).Incontrast,directed
exploration emerges naturally within active inference through interactions
betweentheriskandambiguitytermsintheexpectedfreeenergy(DaCosta
et al., 2020; Schwartenbeck et al., 2019). This addresses the explore-exploit
dilemma and confers the agent with artificial curiosity (Friston, Lin, et al.,
2017; Schmidhuber, 2010; Schwartenbeck et al., 2019; Still & Precup, 2012),
as opposed to the need to add ad hoc information bonus terms (Tokic &
Palm, 2011). We expand on this relationship in appendix A.3.
A.1.4 Control and Learning as Inference.Active inference integrates state
estimation, learning, decision making, and motor control under the single
objective of minimizing free energy (Da Costa et al., 2020). In fact, active
inferenceextendspreviousworkonthedualitybetweeninferenceandcon-
trol(Kappenetal.,2012;Rawliketal.,2013;Todorov,2008;Toussaint,2009)
to solve motor control problems via approximate inference (i.e., planning
as inference: Attias, 2003; Botvinick & Toussaint, 2012; Friston et al., 2012,
2009; Millidge, Tschantz, Seth, et al., 2020). Therefore, some of the clos-
est RL methods to active inference are control as inference, also known as
maximum entropy RL (Levine, 2018; Millidge, Tschantz, Seth, et al., 2020;
Ziebart, 2010), though one major difference is in the choice of decision-
making objective. Loosely speaking, these aforementioned methods min-
imize the risk term of the expected free energy, while active inference also
minimizes ambiguity.
Useful Features of Active Inference
1. Activeinferenceallowsgreatflexibilityandtransparencywhenmod-
elingbehavior.Itaffordsexplainabledecisionmakingasamixtureof
information-andreward-seekingpoliciesthatareexplicitlyencoded
(and evaluated in terms of expected free energy) in the generative
model as priors, which are specified by the user (Da Costa, Lanillos,
et al., 2022). As we have seen, the kind of behavior that can be pro-
duced includes the optimal solution to the Bellman equation.
2. Activeinferenceaccommodatesdeephierarchicalgenerativemodels
combining both discrete and continuous state-spaces (Friston, Parr,
et al., 2017; Friston et al., 2018; Parr et al., 2021).
3. The expected free energy objective optimized during planning sub-
sumesmanyapproachesusedtodescribeandsimulatedecisionmak-
inginthephysical,engineering,andlifesciences,affordingitvarious
interesting properties as an objective (see Figure 3 and Friston et al.,
2021).Forexample,exploratoryandexploitativebehaviorarecanon-
icallyintegrated,whichfinessestheneedformanuallyincorporating
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 833
adhocexplorationbonusesintherewardfunction(DaCosta,Tenka,
et al., 2022).
4. Active inference goes beyond state-action policies that predominate
intraditionalRLtosequentialpolicyoptimization.Insequentialpol-
icy optimization, one relaxes the assumption that the same action is
optimal given a particular state and acknowledges that the sequen-
tialorderofactionsmaymatter.Thisissimilartothelinearlysolvable
MDP formulation presented by Todorov (2006, 2009), where transi-
tion probabilities directly determine actions and an optimal policy
specifies transitions that minimize some divergence cost. This way
of approaching policies is perhaps most apparent in terms of explo-
ration.Putsimply,itisclearlybettertoexploreandthenexploitthan
the converse. Because expected free energy is a functional of beliefs,
explorationbecomesanintegralpartofdecisionmaking—incontrast
withtraditionalRLapproachesthattrytooptimizearewardfunction
of states. In other words, active inference agents will explore until
enoughuncertaintyisresolvedforreward-maximizing,goal-seeking
imperatives to start to predominate.
Such advantages should motivate future research to better characterize
the environments in which these properties offer useful advantages—such
aswhereperformancebenefitsfromlearningandplanningatmultipletem-
poralscalesandfromtheabilitytoselectpoliciesthatresolvebothstateand
parameter uncertainty.
A.2 Reward Learning. Given the focus on relating active inference to
the objective of maximizing reward, it is worth briefly illustrating how ac-
tiveinferencecanlearntherewardfunctionfromdataanditspotentialcon-
nectionstorepresentativeRLapproaches.Onecommonapproachforactive
inference to learn a reward function (Smith, Schwartenbeck, Stewart, et al.,
2020;Smith,Taylor,etal.,2022)istosetpreferencesoverobservationsrather
thanstates,whichcorrespondstoassumingthatinferencesoverstatesgiven
outcomes are accurate:
DKL [Q(⃗s| ⃗a, ˜o) | C(⃗s)]  
Risk (states)
= DKL [Q(⃗o| ⃗a, ˜o) | C(⃗o)]  
Risk (outcomes)
+ EQ(⃗o|⃗a,˜o) [DKL [Q(⃗s|⃗o, ˜o,⃗a) | P(⃗s|⃗o)]]  
≈0
≈ DKL [Q(⃗o| ⃗a, ˜o) | C(⃗o)]  
Risk (outcomes)
,
that is, equality holds whenever the free energy minimum is reached
(see equation 5.2). Then one sets the preference distribution such that the
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

834 L. Da Costa et al.
observations designated as rewards are most preferred. In the zero tem-
perature limit (see eqnarray 4.3), preferences only assign mass to reward-
maximizing observations. When formulated in this way, the reward signal
is treated as sensory data, as opposed to a separate signal from the envi-
ronment. When one sets allowable actions (controllable state transitions)
to be fully deterministic such that the selection of each action will transi-
tion the agent to a given state with certainty, the emerging dynamics are
such that the agent chooses actions to resolve uncertainty about the prob-
ability of observing reward under each state. Thus, learning the reward
probabilities of available actions amounts to learning the likelihood matrix
P(⃗o|⃗s): = ot · Ast, whereA is a stochastic matrix. This is done by setting a
prior a over A, that is, a matrix of nonnegative components, the columns
of which are Dirichlet priors over the columns ofA. The agent then learns
by accumulating Dirichlet parameters. Explicitly, at the end of a trial or
episode, one sets (Da Costa et al., 2020; Friston et al., 2016)
a ← a+
T∑
τ=0
oτ ⊗ Q(sτ | o0:T). (A.1)
InequationA.1, Q(sτ | o0:T)isseenasavectorofprobabilitiesoverthestate-
space S, corresponding to the probability of having been in one or another
stateattime τ afterhavinggatheredobservationsthroughoutthetrial.This
rule simply amounts to counting observed state-outcome pairs, which is
equivalent to state-reward pairs when the observation modalities corre-
spond to reward.
Oneshouldnotconflatethisapproachwiththeupdateruleconsistingof
accumulating state-observation counts in the likelihood matrix,
A← A+
T∑
τ=0
oτ ⊗ Q(sτ | o0:T), (A.2)
and then normalizing its columns to sum to one when computing proba-
bilities.Thelattersimplyapproximatesthelikelihoodmatrix Abyaccumu-
latingthenumberofobservedstate-outcomepairs.Thisisdistinctfromthe
approach outlined above, which encodes uncertainty over the matrixA,a s
a probability distribution over possible distributionsP(ot | st). The agent
is initially very unconfident aboutA, which means that it doesn’t place
high-probability mass on any specification ofP(ot | st). This uncertainty is
gradually resolved by observing state-observation (or state-reward) pairs.
Computationally, it is a general fact of Dirichlet priors that an increase
in elements ofa causes the entropy ofP(ot | st) to decrease. As the terms
added in equation A.1 are always positive, one choice of distribution
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 835
P(ot | st)—which best matches available data and prior beliefs—is ulti-
mately singled out. In other words, the likelihood mapping is learned.
The update rule consisting of accumulating state-observation counts in
the likelihood matrix (see equation A.2) (i.e., not incorporating Dirichlet
priors) bears some similarity to off-policy learning algorithms such as Q-
learning.InQ-learning,theobjectiveistofindthebestactiongiventhecur-
rent observed state. For this, the Q-learning agent accumulates values for
state-action pairs with repeated observation of rewarding or punishing ac-
tion outcomes—much like state-observation counts. This allows it to learn
the Q-value function that defines a reward maximizing policy.
Asalwaysinpartiallyobservedenvironments,wecannotguaranteethat
the true likelihood mapping will be learned in practice. Smith et al. (2019)
providesexampleswhere,althoughnotinanexplicitreward-learningcon-
text, learning the likelihood can be more or less successful in different sit-
uations. Learning the true likelihood fails when the inference over states
is inaccurate, such as when using too severe a mean-field approximation
to the free energy (Blei et al., 2017; Parr et al., 2019; Tanaka, 1999), which
causestheagenttomisinferstatesandtherebyaccumulateDirichletparam-
etersinthewronglocations.Intuitively,thisamountstojumpingtoconclu-
sions too quickly.
Remark 8. If so desired, reward learning in active inference can also be
equivalently formulated as learning transition probabilitiesP(st+1 | st,at).
In this alternative setup (as exemplified in Sales et al. (2019)), mappings
between reward states and reward outcomes inAare set as identity matri-
ces, and the agent instead learns the probability of transitioning to states
that deterministically generate preferred (rewarding) observations given
the choice of each action sequence. The transition probabilities under each
actionarelearnedinasimilarfashionasabove(seeequationA.1),byaccu-
mulatingcounts onaDirichletprior over P(st+1 | st,at).SeeDaCostaetal.,
2020, appendix, for details.
Given the model-based Bayesian formulation of active inference, more
direct links can be made between the active inference approach to reward
learning described above and other Bayesian model-based RLapproaches.
For such links to be realized, the Bayesian RL agent would be required to
have a prior over a prior (e.g., a prior over the reward function prior or
transitionfunctionprior).Onewaytoimplicitlyincorporatethisisthrough
Thompson sampling (Ghavamzadeh et al., 2016; Russo & Van Roy, 2014,
2016; Russo et al., 2017). While that is not the focus of this article, future
work could further examine the links between reward learning in active
inference and model-based Bayesian RLschemes.
A.3 Solving the Exploration-Exploitation Dilemma. An important
distinction between active inference and reinforcement learning schemes
is how they solve the exploration-exploitation dilemma.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

836 L. Da Costa et al.
The exploration-exploitation dilemma (Berger-Tal et al., 2014) arises
wheneveranagenthasincompleteinformationaboutitsenvironment,such
aswhentheenvironmentispartiallyobservedorthegenerativemodelhas
to be learned. The dilemma is then about deciding whether to execute ac-
tions aiming to collect reward based on imperfect information about the
environment or to execute actions aiming to gather more information—
allowing the agent to reap more reward in the future. Intuitively, it is al-
ways best to explore and then exploit, but optimizing this trade-off can be
difficult.
Active inference balances exploration and exploitation through mini-
mizing the risk and ambiguity inherent in the minimization of expected
free energy. This balance is context sensitive and can be adjusted by mod-
ifying the agent’s preferences (Da Costa, Lanillos, et al., 2022). In turn,
the expected free energy is obtained from a description of agency in bi-
ological systems derived from physics (Barp et al., 2022; Friston et al.,
2022).
Modern RL algorithms integrate exploratory and exploitative behavior
in many different ways. One option is curiosity-driven rewards to en-
courage exploration. Maximum entropy RLand control-as-inference make
decisions by minimizing a KLdivergence to the target distribution (Eysen-
bach&Levine,2019;Haarnojaetal.,2017,2018;Levine,2018;Todorov,2008;
Ziebart etal., 2008),which combinesrewardmaximizationwithmaximum
entropy over states. This is similar to active inference on MDPs (Millidge,
Tschantz, Seth, et al., 2020). Similarly, the model-free Soft Actor-Critic
(Haarnoja et al., 2018) algorithm maximizes both expected reward and
entropy. This outperforms other state-of-the-art algorithms in continuous
controlenvironmentsandhasbeenshowntobemoresampleefficientthan
itsreward-maximizingcounterparts(Haarnojaetal.,2018).Hyper(Zintgraf
et al., 2021) proposes reward maximization alongside minimizing uncer-
tainty over both external states and model parameters. Bayes-adaptive
RL (Guez et al., 2013a, 2013b; Ross et al., 2008, 2011; Zintgraf et al., 2020)
provides policies that balance exploration and exploitation with the aim
of maximizing reward. Thompson sampling provides a way to balance
exploitingcurrentknowledgetomaximizeimmediateperformanceandac-
cumulating new information to improve future performance (Russo et al.,
2017).Thisreducestooptimizingdualobjectives,rewardmaximizationand
informationgain,similartoactiveinferenceonPOMDPs.Empirically,Sajid,
Ball,etal.(2021)demonstratedthatanactiveinferenceagentandaBayesian
model-based RLagent using Thompson sampling exhibit similar behavior
when preferences are defined over outcomes. They also highlighted that
when completely removing the reward signal from the environment, the
two agents both select policies that maximize some sort of information
gain.
Ingeneral,thewayeachoftheseapproachestheexploration-exploitation
dilemma differs in theory and in practice remains largely unexplored.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 837
Appendix B: Proofs
B.1 Proof of Proposition 1.Note that a Bellman optimal state-action
policy /Pi1∗ is a maximal element according to the partial ordering≤.E x i s -
tence thus consists of a simple application of Zorn’s lemma. Zorn’s lemma
states that if any increasing chain
/Pi11 ≤ /Pi12 ≤ /Pi13 ≤ ... (B.1)
has an upper-bound that is a state-action policy, then there is a maximal
element /Pi1∗.
GiventhechainequationB.1,weconstructanupperbound.Weenumer-
ate A × S × T by (α1,σ 1,t1),..., (αN,σ N,tN). Then the state-action policy
sequence
/Pi1n(α1 | σ1,t1), n= 1,2,3,...
is bounded within [0, 1]. By the Bolzano-Weierstrass theorem, there ex-
ists a sub-sequence/Pi1nk(α1 | σ1,t1), k= 1,2,3,... that converges. Similarly,
/Pi1nk(α2 | σ2,t2) is also a bounded sequence, and by Bolzano-Weierstrass, it
has a sub-sequence/Pi1nkj
(a2 | σ2,t2) that converges. We repeatedly take sub-
sequences until N. To ease notation, call the resulting sub-sequence/Pi1m,
m= 1,2,3,...
With this, we defineˆ/Pi1= limm→∞ /Pi1m. It is straightforward to see thatˆ/Pi1
is a state-action policy:
ˆ/Pi1(α | σ, t) = lim
m→∞
/Pi1m(α | σ, t) ∈ [0,1], ∀(α,σ, t) ∈ A × S × T,
∑
α∈A
ˆ/Pi1(α | σ, t) = lim
m→∞
∑
α∈A
/Pi1m(α | σ, t) = 1, ∀(σ, t) ∈ S × T.
To show thatˆ/Pi1is an upper bound, take any/Pi1in the original chain of
state-action policies, equation B.1. Then by the definition of an increasing
sub-sequence,thereexistsanindex M∈ N suchthat ∀k≥ M:/Pi1k ≥ /Pi1.Since
limits commute with finite sums, we havev ˆ/Pi1(s,t) = limm→∞ v/Pi1m(s,t) ≥
v/Pi1k(s,t) ≥ v/Pi1(s,t) for any (s,t) ∈ S × T. Thus, by Zorn’s lemma, there ex-
ists a Bellman, optimal state-action policy/Pi1∗. □
B.2 Proof of Proposition 2.1) ⇒ 2) : We only need to show assertion
(b). By contradiction, suppose that∃(s,α ) ∈ S × A such that/Pi1(α | s,0) > 0
and
E/Pi1[R(s1:T) | s0 = s,a0 = α] < max
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a].
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

838 L. Da Costa et al.
We letα′ be the Bellman optimal action at statesand time 0 defined as
α′ := argmax
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a].
Then we let/Pi1′ be the same state-action policy as/Pi1except that/Pi1′(·| s,0)
assigns α′ deterministically. Then
v/Pi1(s,0) =
∑
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a]/Pi1(a| s,0)
< max
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a]
= E/Pi1′[R(s1:T) | s0 = s,a0 = α′]/Pi1′(α′ | s,0)
=
∑
a∈A
E/Pi1′[R(s1:T) | s0 = s,a0 = a]/Pi1′(a| s,0)
= v/Pi1′(s,0).
So /Pi1is not Bellman optimal, which is a contradiction.
1) ⇐ 2) : We only need to show that/Pi1maximizes v/Pi1(s,0),∀s∈ S.B y
contradiction,thereexistsastate-actionpolicy /Pi1′ andastate s∈ S suchthat
v/Pi1(s,0) < v/Pi1′(s,0),
⇐⇒
∑
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a]/Pi1(a| s,0)
<
∑
a∈A
E/Pi1′[R(s1:T) | s0 = s,a0 = a]/Pi1′(a| s,0).
Bya, the left-hand side equals
max
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a].
Unpacking the expression on the right-hand side,
∑
a∈A
E/Pi1′[R(s1:T) | s0 = s,a0 = a]/Pi1′(a| s,0)
=
∑
a∈A
∑
σ∈S
E/Pi1′[R(s1:T) | s1 = σ]P(s1 = σ | s0 = s,a0 = a)/Pi1′(a| s,0)
=
∑
a∈A
∑
σ∈S
{
E/Pi1′[R(s2:T) | s1 = σ]+ R(σ )
}
P(s1 = σ | s0 = s,a0 = a)/Pi1′(a| s,0)
=
∑
a∈A
∑
σ∈S
{
v/Pi1′(σ, 1) + R(σ )]P(s1 = σ | s0 = s,a0 = a)/Pi1′(a| s,0). (B.2)
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 839
Since /Pi1is Bellman optimal when restricted to {1,..., T},w eh a v e
v/Pi1′(σ, 1) ≤ v/Pi1(σ, 1),∀σ ∈ S. Therefore,
∑
a∈A
∑
σ∈S
{
v/Pi1′(σ, 1) + R(σ )]P(s1 = σ | s0 = s,a0 = a)/Pi1′(a| s,0)
≤
∑
a∈A
∑
σ∈S
{
v/Pi1(σ, 1) + R(σ )]P(s1 = σ | s0 = s,a0 = a)/Pi1′(a| s,0).
Repeating the steps above equation B.2, but in reverse order, yields
∑
a∈A
E/Pi1′[R(s1:T) | s0 = s,a0 = a]/Pi1′(a| s,0)
≤
∑
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a]/Pi1′(a| s,0).
However,
∑
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a]/Pi1′(a| s,0) < max
a∈A
E/Pi1[R(s1:T) | s0 = s,a0 = a],
which is a contradiction. □
B.3 Proof of Proposition 3.We first prove that state-action policies/Pi1
defined as in equation 2.2 are Bellman optimal by induction onT.
T = 1:
/Pi1(a| s,0) > 0 ⇐⇒ a∈ argmax
a
E[R(s1) | s0 = s,a0 = a], ∀s∈ S
is a Bellman optimal state-action policy as it maximizes the total reward
possible in the MDP.
LetT > 1befiniteandsupposethatthepropositionholdsforMDPswith
a temporal horizon ofT − 1. This means that
/Pi1(a| s,T − 1) > 0 ⇐⇒ a ∈ argmax
a
E[R(sT) | sT−1 = s,aT−1 = a], ∀s∈ S,
/Pi1(a| s,T − 2) > 0 ⇐⇒ a ∈ argmax
a
E/Pi1[R(sT−1:T) | sT−2 = s,aT−2 = a],
∀s∈ S,
...
/Pi1(a| s,1) > 0 ⇐⇒ a ∈ argmax
a
E/Pi1[R(s2:T) | s1 = s,a1 = a], ∀s∈ S,
is a Bellman optimal state-action policy on the MDPrestricted to times 1 to
T. Therefore, since
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

840 L. Da Costa et al.
/Pi1(a| s,0) > 0 ⇐⇒ a∈ argmax
a
E/Pi1[R(s1:T) | s0 = s,a0 = a], ∀s∈ S.
Proposition 2 allows us to deduce that/Pi1is Bellman optimal.
We now show that any Bellman optimal state-action policy satisfies the
backward induction algorithm equation 2.2.
Suppose by contradiction that there exists a state-action policy/Pi1that is
Bellman optimal but does not satisfy equation 2.2. Say,∃(a,s,t) ∈ A × S ×
T,t < T, such that
/Pi1(a| s,t) > 0a n da /∈ argmax
α∈A
E/Pi1[R(st+1:T) | st = s,at = α].
This implies
E/Pi1[R(st+1:T) | st = s,at = a] < max
α∈A
E/Pi1[R(st+1:T) | st = s,at = α].
Let ˜a∈ argmaxα E/Pi1[R(st+1:T) | st = s,at = α].Let ˜/Pi1beastate-actionpol-
icysuchthat ˜/Pi1(·| s,t)assigns ˜a∈ A deterministicallyandsuchthat ˜/Pi1= /Pi1
otherwise. Then we can contradict the Bellman optimality of/Pi1as follows:
v/Pi1(s,t) = E/Pi1[R(st+1:T) | st = s]
=
∑
α∈A
E/Pi1[R(st+1:T) | st = s,at = α]/Pi1(α | s,t)
< max
α∈A
E/Pi1[R(st+1:T) | st = s,at = α]
= E/Pi1[R(st+1:T) | st = s,at = ˜a]
= E ˜/Pi1[R(st+1:T) | st = s,at = ˜a]
=
∑
α∈A
E˜/Pi1[R(st+1:T) | st = s,at = α]˜/Pi1(α | s,t)
= v ˜/Pi1(s,t).
□
B.4 Proof of Lemma 1.
lim
β→+∞
argmin
⃗a
DKL[Q(⃗s| ⃗a,st) | Cβ (⃗s)]
= lim
β→+∞
argmin
⃗a
−H[Q(⃗s| ⃗a,st)]+ EQ(⃗s|⃗a,st)[−logCβ (⃗s)]
= lim
β→+∞
argmin
⃗a
−H[Q(⃗s| ⃗a,st)]− βEQ(⃗s|⃗a,st)[R(⃗s)]
= lim
β→+∞
argmax
⃗a
H[Q(⃗s| ⃗a,st)]+ βEQ(⃗s|⃗a,st)[R(⃗s)]
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 841
⊆ lim
β→+∞
argmax
⃗a
βEQ(⃗s|⃗a,st)[R(⃗s)]
= argmax
⃗a
EQ(⃗s|⃗a,st)[R(⃗s)].
The inclusion follows from the fact that, asβ →+ ∞, a minimizer of the
expectedfreeenergyhastomaximize EQ(⃗s|⃗a,st)[R(⃗s)].Amongsuchactionse-
quences, the expected free energy minimizers are those that maximize the
entropy of future states H[Q(⃗s| ⃗a,st)]. □
B.5 Proof of Theorem 1.When T = 1, the only action isa0.W efi xa n
arbitrary initial states0 = s∈ S. By proposition 2, a Bellman optimal state-
action policy is fully characterized by an actiona∗
0 that maximizes immedi-
ate reward:
a∗
0 ∈ argmax
a∈A
E[R(s1) | s0 = s,a0 = a].
Recall that by remark 5, this expectationstands for return under the transi-
tion probabilities of the MDP:
a∗
0 ∈ argmax
a∈A
EP(s1|a0=a,s0=s)[R(s1)].
Sincetransitionprobabilitiesareassumedtobeknown(seeequation3.1),
this reads
a∗
0 ∈ argmax
a∈A
EQ(s1|a0=a,s0=s)[R(s1)].
On the other hand,
a0 ∈ lim
β→+∞
argmax
a∈A
exp(−G(a| st))
= lim
β→+∞
argmin
a∈A
G(a| st).
By lemma 1, this implies
a0 ∈ argmax
a∈A
EQ(s1|a0=a,s0=s)[R(s1)],
which concludes the proof. □
B.6 Proof of Theorem 2.We prove this result by induction on the tem-
poral horizonT of the MDP.
The proof of the theorem whenT = 1 can be seen from the proof of the-
orem 1. Now suppose thatT > 1 is finite and that the theorem holds for
MDPs with a temporal horizon ofT − 1.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

842 L. Da Costa et al.
Ourinductionhypothesissaysthat Q(aτ | sτ ),asdefinedinequation4.8,
is a Bellman optimal state-action policy on the MDPrestricted to timesτ =
1,..., T. Therefore, by proposition 2, we only need to show that the action
a0 selected under active inference satisfies
a0 ∈ argmax
a∈A
EQ[R(⃗s) | s0,a0 = a].
This is simple to show as
argmax
a∈A
EQ[R(⃗s) | s0,a0 = a]
= argmax
a∈A
EP(⃗s|a1:T,a0=a,s0)Q(⃗a|s1:T)[R(⃗s)] (by remark 4)
= argmax
a∈A
EQ(⃗s,⃗a|a0=a,s0)[R(⃗s)] (as the transitions are known)
= lim
β→+∞
argmax
a∈A
EQ(⃗s,⃗a|a0=a,s0)[βR(⃗s)]
⊇ lim
β→+∞
argmax
a∈A
EQ(⃗s,⃗a|a0=a,s0)[βR(⃗s)]− H[Q(⃗s| ⃗a,a0 = a,s0)]
= lim
β→+∞
argmin
a∈A
EQ(⃗s,⃗a|a0=a,s0)[−logCβ (⃗s)]− H[Q(⃗s| ⃗a,a0 = a,s0)]
(by equation 4.1)
= lim
β→+∞
argmin
a∈A
EQ(⃗s,⃗a|a0=a,s0)DKL[Q(⃗s| ⃗a,a0 = a,s0) | Cβ (⃗s)]
= lim
β→+∞
argmin
a∈A
G(a0 = a| s0) (by equation 4.6).
Therefore,anaction a0 selectedunderactiveinferenceisaBellmanoptimal
state-action policy on finite temporal horizons. Furthermore, the inclusion
follows from the fact that if there are multiple actions that maximize ex-
pectedreward,thatwhichisselectedunderactiveinferencemaximizesthe
entropy of beliefs about future states. □
B.7 Proof of Proposition 4.Unpacking the zero temperature limit,
lim
β→+∞
argmin
⃗a
G(⃗a| ˜o)
= lim
β→+∞
argmin
⃗a
DKL[Q(⃗s| ⃗a, ˜o) | Cβ (⃗s)]+ EQ(⃗s|⃗a,˜o)H[P(⃗o|⃗s)]
= lim
β→+∞
argmin
⃗a
−H[Q(⃗s| ⃗a, ˜o)]+ EQ(⃗s|⃗a,˜o)[−logCβ (⃗s)]+ EQ(⃗s|⃗a,˜o)H[P(⃗o|⃗s)]
= lim
β→+∞
argmin
⃗a
−H[Q(⃗s| ⃗a, ˜o)]− βEQ(⃗s|⃗a,˜o)[R(⃗s)]+ EQ(⃗s|⃗a,˜o)H[P(⃗o|⃗s)]
(by equation 4.1)
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 843
⊆ lim
β→+∞
argmax
⃗a
βEQ(⃗s|⃗a,˜o)[R(⃗s)]
= argmax
⃗a
EQ(⃗s|⃗a,˜o)[R(⃗s)].
The inclusion follows from the fact that asβ →+ ∞, a minimizer of
the expected free energy has first and foremost to maximizeEQ(⃗s|⃗a,˜o)[R(⃗s)].
Among such action sequences, the expected free energy minimizers are
thosethatmaximizetheentropyof(beliefsabout)futurestatesH[ Q(⃗s| ⃗a, ˜o)]
andresolveambiguityaboutfutureoutcomesbyminimizing EQ(⃗s|⃗a,˜o)H[P(⃗o|
⃗s)].
Acknowledgments
WethankDimitrijeMarkovicandQuentinHuysforprovidinghelpfulfeed-
back during the preparation of the manuscript.
Funding Information
L.D. is supported by the Fonds National de la Recherche, Luxembourg
(Project code: 13568875). N.S. is funded by the Medical Research Coun-
cil (MR/S502522/1) and 2021-2022 Microsoft PhD Fellowship. K.F. is sup-
ported by funding for the Wellcome Centre for Human Neuroimaging
(Ref: 205103/Z/16/Z), a Canada-U.K. Artificial Intelligence Initiative (Ref:
ES/T01279X/1), and the European Union’s Horizon 2020 Framework Pro-
gramme for Research and Innovation under the Specific Grant Agreement
945539 (Human Brain Project SGA3). R.S. is supported by the William K.
Warren Foundation, the Well-Being for Planet Earth Foundation, the Na-
tional Institute for General Medical Sciences (P20GM121312), and the Na-
tionalInstituteofMentalHealth(R01MH123691).Thispublicationisbased
on work partially supported by the EPSRC Centre for Doctoral Training
in Mathematics of Random Systems: Analysis, Modelling and Simulation
(EP/S023925/1).
Author Contributions
L.D.:conceptualization,proofs,writing:firstdraft,reviewandediting.N.S.,
T.P., K.F., R.S.: conceptualization, writing: review and editing.
References
Adams, R. A., Stephan, K. E., Brown, H. R., Frith, C. D., & Friston, K. J. (2013).
The computational anatomy of psychosis. Frontiers in Psychiatry, 4.1 0.3389/
fpsyt.2013.00047
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

844 L. Da Costa et al.
Adda, J., & Cooper, R. W. (2003).Dynamic economics: Quantitative methods and appli-
cations. MIT Press.
Attias, H. (2003). Planning by probabilistic inference. InProceedings of the 9th Int.
WorkshoponArtificialIntelligenceandStatistics .
Barlow, H. B. (1961).Possible principles underlying the transformations of sensory mes-
sages. MIT Press.
Barlow, H. B. (1974). Inductive inference, coding, perception, and language.Percep-
tion,3(2), 123–134. 10.1068/p030123, PubMed: 4457815
Barp, A., Da Costa, L., França, G., Friston, K., Girolami, M., Jordan, M. I., & Pavli-
otis, G. A. (2022). Geometric methods for sampling, optimisation, inference and
adaptive agents. In F. Nielsen, A. S. R. Srinivasa Rao, & C. Rao (Eds.),Geometry
andstatistics. Elsevier.
Barto,A.,Mirolli,M.,&Baldassarre,G.(2013).Noveltyorsurprise? FrontiersinPsy-
chology, 4.1 0.3389/fpsyg.2013.00907
Barto, A., & Sutton, R. (1992).Reinforcementlearning:Anintroduction . MIT Press.
Beal, M. J. (2003).Variational algorithms for approximate Bayesian inference. PhD diss.,
University of London.
Bellman, R. E. (1957).Dynamicprogramming. Princeton University Press.
Bellman, R. E., & Dreyfus, S. E. (2015).Applieddynamicprogramming . Princeton Uni-
versity Press.
Berger,J.O.(1985). StatisticaldecisiontheoryandBayesiananalysis (2nded.).Springer-
Verlag.
Berger-Tal,O.,Nathan,J.,Meron,E.,&Saltz,D.(2014).Theexploration-exploitation
dilemma: A multidisciplinary framework. PLOS One, 9(4), e95693. 10.1371/
journal.pone.0095693
Bertsekas,D.P.,&Shreve,S.E.(1996). Stochasticoptimalcontrol:Thediscretetimecase .
Athena Scientific.
Bishop, C. M. (2006).Patternrecognitionandmachinelearning .S p r i n g e r .
Blei,D.M.,Kucukelbir,A.,&McAuliffe,J.D.(2017).Variationalinference:Areview
for statisticians.Journal of the American Statistical Association, 112(518), 859–877.
10.1080/01621459.2017.1285773
Botvinick, M., & Toussaint, M. (2012). Planning as inference.TrendsinCognitiveSci-
ences,16(10), 485–488. 10.1016/j.tics.2012.08.006, PubMed: 22940577
Çatal, O., Nauta, J., Verbelen, T., Simoens, P., & Dhoedt, B. (2019).Bayesian policy
selectionusingactiveinference . http://arxiv.org/abs/1904.08149
Çatal, O., Verbelen, T., Nauta, J., Boom, C. D., & Dhoedt, B. (2020). Learning per-
ception and planning with deep active inference. InProceedings of the IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing(pp. 3952–3956).
10.1109/ICASSP40776.2020.9054364
Çatal, O., Verbelen, T., Van de Maele, T., Dhoedt, B., & Safron, A. (2021). Robot nav-
igation as hierarchical active inference.Neural Networks, 142, 192–204. 10.1016/
j.neunet.2021.05.010
Champion, T., Bowman, H., & Grz ´s, M. (2021). Branching time active infer-
ence: Empirical study and complexity class analysis. http://arxiv.org/abs/2111
.11276
Champion, T., Da Costa, L., Bowman, H., & Grze´s, M. (2021).Branching time active
inference:Thetheoryanditsgenerality . http://arxiv.org/abs/2111.11107
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 845
Cullen, M., Davey, B., Friston, K. J., & Moran, R. J. (2018). Active inference in
OpenAI Gym: Aparadigm for computational investigations into psychiatric ill-
ness.BiologicalPsychiatry:CognitiveNeuroscienceandNeuroimaging ,3(9), 809–818.
10.1016/j.bpsc.2018.06.010, PubMed: 30082215
Da Costa, L., Lanillos, P., Sajid, N., Friston, K., & Khan, S. (2022). How active infer-
ence could help revolutionise robotics.Entropy, 24(3), 361. 10.3390/e24030361
Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., & Friston, K. (2020). Active
inferenceondiscretestate-spaces:Asynthesis. JournalofMathematicalPsychology ,
99, 102447. 10.1016/j.jmp.2020.102447
Da Costa, L., Tenka, S., Zhao, D., & Sajid, N. (2022). Active inference as a model
of agency. Workshop on RL as a Model of Agency. https://www.sciencedirect
.com/science/article/pii/S0022249620300857
Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., & Dolan, R. J. (2006). Corti-
cal substrates for exploratory decisions in humans.Nature, 441(7095), 876–879.
10.1038/nature04766, PubMed: 16778890
Dayan, P., & Daw, N. D. (2008). Decision theory, reinforcement learning, and the
brain. Cognitive, Affective, and Behavioral Neuroscience, 8(4), 429–453. 10.3758/
CABN.8.4.429
Deci, E., & Ryan, R. M. (1985).Intrinsicmotivationandself-determinationinhumanbe-
havior.S p r i n g e r .
Eysenbach, B., & Levine, S. (2019).If MaxEnt Rl is the answer, what is the question?
arXiv:1910.01913.
Fountas,Z.,Sajid,N.,Mediano,P.A.M. DeepactiveinferenceagentsusingMonte-Carlo
methods. http://arxiv.org/abs/2006.04176
Friston,K.,DaCosta,L.,Sajid,N.,Heins,C.,Ueltzhöffer,K.,Pavliotis,G.A.,&Parr,
T.(2022). Thefreeenergyprinciplemadesimplerbutnottoosimple .http://arxiv.org/
abs/2201.06387
Friston, K., Da Costa, L., Hafner, D., Hesp, C., & Parr, T. (2021). Sophis-
ticated inference. Neural Computation, 33(3), 713–763. 10.1162/neco_a_01351,
PubMed: 33626312
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O’Doherty, J., & Pezzulo, G.
(2016). Active inference and learning.Neuroscience and Biobehavioral Reviews, 68,
862–879. 10.1016/j.neubiorev.2016.06.022, PubMed: 27375276
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017). Ac-
tiveinference:Aprocesstheory NeuralComputation,29(1),1–49.10.1162/NECO_a
_00912, PubMed: 27870614
Friston, K., Samothrakis, S., & Montague, R. (2012). Active inference and agency:
Optimal control without cost functions.Biological Cybernetics, 106(8), 523–541.
10.1007/s00422-012-0512-8, PubMed: 22864468
Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active
inference?PLOSOne,4(7), e6421. 10.1371/journal.pone.0006421
Friston, K. J., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Action and behav-
ior: A free-energy formulation.Biological Cybernetics, 102(3), 227–260. 10.1007/
s00422-010-0364-z, PubMed: 20148260
Friston,K.J.,Lin,M.,Frith,C.D.,Pezzulo,G.,Hobson,J.A.,&Ondobaka,S.(2017).
Active inference, curiosity and insight.Neural Computation, 29(10), 2633–2683.
10.1162/neco_a_00999, PubMed: 28777724
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

846 L. Da Costa et al.
Friston, K. J., Parr, T., & de Vries, B. (2017). The graphical brain: Belief propaga-
tion and active inference.Network Neuroscience, 1(4), 381–414. 10.1162/NETN_a
_00018, PubMed: 29417960
Friston, K. J., Rosch, R., Parr, T., Price, C., & Bowman, H. (2018). Deep temporal
models and active inference.Neuroscience and Biobehavioral Reviews, 90, 486–501.
10.1016/j.neubiorev.2018.04.004, PubMed: 29747865
Fudenberg, D., & Tirole, J. (1991).Gametheory. MIT Press.
Gershman, S. J. (2018). Deconstructing the human algorithms for exploration.Cog-
nition,173, 34–42. 10.1016/j.cognition.2017.12.014, PubMed: 29289795
Gershman, S. J., & Niv, Y. (2010). Learning latent structure: Carving nature at its
joints.CurrentOpinioninNeurobiology ,20(2),251–256.10 .1016/j.conb.2010.02.008,
PubMed: 20227271
Ghavamzadeh, M., Mannor, S., Pineau, J., & Tamar, A. (2016).Bayesianreinforcement
learning:Asurvey . arXiv:1609.04436.
Guez, A., Silver, D., & Dayan, P. (2013a). Scalable and efficient Bayes-adaptive rein-
forcement learning based on Monte-Carlo tree search.Journal of Artificial Intelli-
genceResearch,48, 841–883. 10.1613/jair.4117
Guez,A.,Silver,D.,&Dayan,P.(2013b). EfficientBayes-adaptivereinforcementlearning
usingsample-basedsearch . http://arxiv.org/abs/1205.3109
Haarnoja,T.,Tang,H.,Abbeel,P.,&Levine,S.(2017). Reinforcementlearningwithdeep
energy-basedpolicies. arXiv:1702.08165.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR,
abs/1801.01290. http://arxiv.org/abs/1801.01290
Huys, Q. J. M., Eshel, N., O’Nions, E., Sheridan, L., Dayan, P., & Roiser, J. P. (2012).
Bonsai trees in your head: How the Pavlovian system sculpts goal-directed
choices by pruning decision trees.PLOS Computational Biology, 8(3), e1002410.
10.1371/journal.pcbi.1002410
Itti,L.,&Baldi,P.(2009).Bayesiansurpriseattractshumanattention. VisionResearch,
49(10), 1295–1306. 10.1016/j.visres.2008.09.007, PubMed: 18834898
Jaynes, E. T. (1957a). Information theory and statistical mechanics.Physical Review,
106(4), 620–630. 10.1103/PhysRev.106.620
Jaynes,E.T.(1957b).Informationtheoryandstatisticalmechanics.II. PhysicalReview,
108(2), 171–190. 10.1103/PhysRev.108.171
Jordan,M.I.,Ghahramani,Z.,Jaakkola,T.S.,&Saul,L.K.(1998).Anintroductionto
variationalmethodsforgraphicalmodels.InM.I.Jordan(Ed.), Learningingraph-
icalmodels (pp. 105–161). Springer Netherlands. 10.1007/978-94-011-5014-9_5
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting
in partially observable stochastic domains.Artificial Intelligence, 101(1), 99–134.
10.1016/S0004-3702(98)00023-X
Kahneman,D.,&Tversky,A.(1979).Prospecttheory:Ananalysisofdecisionunder
risk. Econometrica,47(2), 263–291. 10.2307/1914185
Kappen, H. J., Gómez, V., & Opper, M. (2012). Optimal control as a graphical model
inference problem.MachineLearning, 87(2), 159–182. 10.1007/s10994-012-5278-7
Klyubin, A. S., Polani, D., & Nehaniv, C. L. (2008). Keep your options open: An
information-based driving principle for sensorimotor systems.PLOSOne, 3(12),
e4018. 10.1371/journal.pone.0004018
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 847
L a l l y ,N . ,H u y s ,Q .J .M . ,E s h e l ,N . ,F a u l k n e r ,P . ,D a y a n ,P . ,&R o i s e r ,J .P .
(2017). The neural basis of aversive Pavlovian guidance during planning
Journal of Neuroscience, 37(42), 10215–10229. 10.1523/JNEUROSCI.0085-17.2017,
PubMed: 28924006
Lanillos, P., Pages, J., & Cheng, G. (2020). Robot self/other distinction: Active infer-
ence meets neural networks learning in a mirror. InProceedings of the European
ConferenceonArtificialIntelligence .
Levine, S. (2018, May 20).Reinforcement learning and control as probabilistic inference:
tutorialandreview . http://arxiv.org/abs/1805.00909
Lindley, D. V. (1956). On a measure of the information provided by an ex-
periment. Annals of Mathematical Statistics, 27(4), 986–1005. 10.1214/aoms/
1177728069
Linsker, R. (1990). Perceptual neural organization: Some approaches based on net-
work models and information theory.Annual Review of Neuroscience, 13(1), 257–
281. 10.1146/annurev.ne.13.030190.001353, PubMed: 2183677
MacKay, D. J. C. (2003, September 25).Informationtheory,inferenceandlearningalgo-
rithms. Cambridge University Press.
Maisto,D.,Gregoretti,F.,Friston,K.,&Pezzulo,G.(2021,March25). Activetreesearch
inlargePOMDPs . http://arxiv.org/abs/2103.13860
Markovi´c, D., Stoji´c, H., Schwöbel, S., & Kiebel, S. J. (2021). An empirical evalua-
tion of active inference in multi-armed bandits.Neural Networks, 144, 229–246.
10.1016/j.neunet.2021.08.018
Mazzaglia, P., Verbelen, T., & Dhoedt, B. (2021).Contrastiveactiveinference . https://
openreview.net/forum?id=5t5FPwzE6mq
Millidge, B. (2019, March 11).Implementing predictive processing and active inference:
Preliminarystepsandresults .P s y A r X i v .1 0.31234/osf.io/4hb58
Millidge, B. (2020). Deep active inference as variational policy gradients.Journal of
MathematicalPsychology, 96, 102348. 10.1016/j.jmp.2020.102348
Millidge, B. (2021).Applicationsofthefreeenergyprincipletomachinelearningandneu-
roscience. http://arxiv.org/abs/2107.00140
Millidge, B., Tschantz, A., & Buckley, C. L. (2020, April 21).Whence the expected free
energy?http://arxiv.org/abs/2004.08128
Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). On the relationship
betweenactiveinferenceandcontrolasinference.InT.Verbelen,P.Lanillos,C.L.
Buckley, & C. De Boom (Eds.),Activeinference (pp. 3–11). Springer.
Miranda, M. J., & Fackler, P. L. (2002, September 1).Applied computational economics
andfinance. MIT Press.
Mirza, M. B., Adams, R. A., Mathys, C., & Friston, K. J. (2018). Human visual explo-
ration reduces uncertainty about the sensed world.PLOS One, 13(1), e0190429.
10.1371/journal.pone.0190429
Oliver, G., Lanillos, P., & Cheng, G. (2021). An empirical study of active inference
on a humanoid robot.IEEE Transactions on Cognitive and Developmental Systems
PP(99), 1–1. 10.1109/TCDS.2021.3049907
Optican, L. M., & Richmond, B. J. (1987). Temporal encoding of two-dimensional
patternsbysingleunitsinprimateinferiortemporalcortex.III.Informationtheo-
reticanalysis. JournalofNeurophysiology ,57(1),162–178.10 .1152/jn.1987.57.1.162,
PubMed: 3559670
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

848 L. Da Costa et al.
Oudeyer, P.-Y., & Kaplan, F. (2007). What is intrinsic motivation? A typology of
computational approaches.Frontiers in Neurorobotics, 1, 6. 10.3389/neuro.12.006
.2007
Parr,T.(2019). Thecomputationalneurologyofactivevision (PhDdiss.).UniversityCol-
lege London.
Parr,T.,Limanowski,J.,Rawji,V.,&Friston,K.(2021).Thecomputationalneurology
of movement under active inference.Brain, 144(6), 1799–1818. 10.1093/brain/
awab085, PubMed: 33704439
Parr, T., Markovic, D., Kiebel, S. J., & Friston, K. J. (2019). Neuronal message pass-
ingusingmean-field,Bethe,andmarginalapproximations. ScientificReports,9(1),
1889. 10.1038/s41598-018-38246-3
Parr, T., Pezzulo, G., & Friston, K. J. (2022, March 29).Activeinference:Thefreeenergy
principleinmind,brain,andbehavior . MIT Press.
Paul, A., Sajid, N., Gopalkrishnan, M., & Razi, A. (2021, August 27).Activeinference
forstochasticcontrol . http://arxiv.org/abs/2108.12245
Pavliotis, G. A. (2014).Stochastic processes and applications: Diffusion processes, the
Fokker-PlanckandLangevinequations .S p r i n g e r .
Pearl, J. (1998). Graphical models for probabilistic and causal reasoning. In P.
Smets (Ed.),Quantified representation of uncertainty and imprecision(pp. 367–389).
Springer Netherlands.
Pezzato,C.,Ferrari,R.,&Corbato,C.H.(2020).Anoveladaptivecontrollerforrobot
manipulatorsbasedonactiveinference. IEEERoboticsandAutomationLetters ,5(2),
2973–2980. 10.1109/LRA.2020.2974451
Pio-Lopez, L., Nizard, A., Friston, K., & Pezzulo, G. (2016). Active inference and
robotcontrol:Acasestudy. JournaloftheRoyalSocietyInterface ,13(122),20160616.
10.1098/rsif.2016.0616
Puterman, M. L. (2014, August 28).Markov decision processes: Discrete stochastic dy-
namicprogramming. Wiley.
Rahme, J., & Adams, R. P. (2019, June 24).A theoretical connection between statistical
physicsandreinforcementlearning . http://arxiv.org/abs/1906.10228
Rawlik, K., Toussaint, M., & Vijayakumar, S. (2013). On stochastic optimal con-
trol and reinforcement learning by approximate inference. In Proceedings of
the Twenty-Third International Joint Conference on Artificial Intelligence. https://
www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6658
Ross, S., Chaib-draa, B., & Pineau, J. (2008). Bayes-adaptive POMDPs. In J. C.
Platt, D. Koller, Y. Singer, & S. T. Roweis (Eds.),Advances in neural informa-
tionprocessingsystems ,20(pp.1225–1232).Curran.http://papers .nips.cc/paper/
3333-bayes-adaptive-pomdps.pdf
Ross, S., Pineau, J., Chaib-draa, B., & Kreitmann, P. (2011). ABayesian approach for
learningandplanninginpartiallyobservableMarkovdecisionprocesses. Journal
ofMachineLearningResearch , 12(2011).
Russo, D., & Van Roy, B. (2014). Learning to optimize via posterior sampling.Math-
ematicsofOperationsResearch ,39(4), 1729–1770. 10.1287/moor.2014.0650
Russo,D.,&VanRoy,B.(2016).Aninformation-theoreticanalysisofThompsonsam-
pling.JournalofMachineLearningResearch , 17(1), 2442–2471.
Russo, D., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2017).At u t o r i a lo n
Thompsonsampling. arXiv:1707.02038.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 849
Sajid, N., Ball, P. J., Parr, T., & Friston, K. J. (2021). Active inference: Demysti-
fied and compared.Neural Computation, 33(3), 674–712. 10.1162/neco_a_01357,
PubMed: 33400903
Sajid, N., Holmes, E., Costa, L. D., Price, C., & Friston, K. (2022).Amixed generative
modelofauditorywordrepetition .1 0.1101/2022.01.20.477138
Sajid, N., Tigas, P., Zakharov, A., Fountas, Z., & Friston, K. (2021, July 18).Explo-
ration and preference satisfaction trade-off in reward-free learning. http://arxiv.org/
abs/2106.04316
Sales, A. C., Friston, K. J., Jones, M. W., Pickering, A. E., & Moran, R. J. (2019). Lo-
cus Coeruleus tracking of prediction errors optimises cognitive flexibility: An
active inference model.PLOS Computational Biology, 15(1), e1006267. 10.1371/
journal.pcbi.1006267
Sancaktar, C., van Gerven, M., & Lanillos, P. (2020, May 29).End-to-end pixel-based
deepactiveinferenceforbodyperceptionandaction .http://arxiv.org/abs/2001.05847
Sargent, R. W. H. (2000). Optimal control.JournalofComputationalandAppliedMath-
ematics, 124(1), 361–371. 10.1016/S0377-0427(00)00418-0
Schmidhuber, J. (2006). Developmental robotics, optimal artificial curiosity, cre-
ativity, music, and the fine arts. Connection Science, 18(2), 173–187. 10.1080/
09540090600768658
Schmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation
(1990–2010). IEEE Transactionson Autonomous Mental Development, 2(3), 230–247.
10.1109/TAMD.2010.2056368
Schneider, T., Belousov, B., Abdulsamad, H., & Peters, J. (2022, June 1).Active infer-
enceforroboticmanipulation .1 0.48550/arXiv.2206.10313
Schulz, E., & Gershman, S. J. (2019). The algorithmic architecture of explo-
ration in the human brain.Current Opinion in Neurobiology, 55, 7–14. 10.1016/
j.conb.2018.11.003, PubMed: 30529148
Schwartenbeck, P., FitzGerald, T. H. B., Mathys, C., Dolan, R., & Friston, K.
(2015). The dopaminergic midbrain encodes the expected certainty about
desired outcomes. Cerebral Cortex, 25(10), 3434–3445. 10.1093/cercor/bhu159,
PubMed: 25056572
Schwartenbeck, P., FitzGerald, T. H. B., Mathys, C., Dolan, R., Kronbichler, M., &
Friston, K. (2015). Evidence for surprise minimization over value maximization
in choice behavior.ScientificReports.5, 16575. 10.1038/srep16575
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., &
Friston, K. J. (2019). Computational mechanisms of curiosity and goal-directed
exploration. eLife,45.
Shoham, Y., Powers, R., & Grenager, T. (2003).Multi-agent reinforcement learning: A
criticalsurvey. Computer Science Department, Stanford University.
S i l v e r ,D . ,H u a n g ,A . ,M a d d i s o n ,C .J . ,G u e z ,A . ,S i f r e ,L . ,v a nd e nD r i e s s c h e ,G . ,...
Hassabis, D. (2016). Mastering the game of Go with deep neural networks and
tree search.Nature,529(7587), 484–489. 10.1038/nature16961, PubMed: 26819042
Smith,R.,Friston,K.J.,&Whyte,C.J.(2022).Astep-by-steptutorialonactiveinfer-
enceanditsapplicationtoempiricaldata. JournalofMathematicalPsychology ,107,
102632. 10.1016/j.jmp.2021.102632
Smith, R., Khalsa, S. S., & Paulus, M. P. (2021). An active inference approach to
dissecting reasons for nonadherence to antidepressants. Biological Psychiatry.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

850 L. Da Costa et al.
Cognitive Neuroscience and Neuroimaging, 6(9), 919–934. 10.1016/j.bpsc.2019.11
.012, PubMed: 32044234
Smith, R., Kirlic, N., Stewart, J. L., Touthang, J., Kuplicki, R., Khalsa, S. S. F., . . .
Aupperle,R.L.(2021).Greaterdecisionuncertaintycharacterizesatransdiagnos-
tic patient sample during approach-avoidance conflict: A computational mod-
eling approach. Journal of Psychiatry an Neuroscience, 46(1), E74–E87. 10.1503/
jpn.200032
Smith, R., Kirlic, N., Stewart, J. L., Touthang, J., Kuplicki, R., McDermott, T. J., . . .
Aupperle, R. L. (2021). Long-term stability of computational parameters during
approach-avoidance conflict in a transdiagnostic psychiatric patient sample.Sci-
entificReports,11(1), 11783. 10.1038/s41598-021-91308-x
Smith, R., Kuplicki, R., Feinstein, J., Forthman, K. L., Stewart, J. L., Paulus, M. P.,
. . . Khalsa, S. S. (2020). A Bayesian computational model reveals a failure to
adapt interoceptive precision estimates across depression, anxiety, eating, and
substance use disorders.PLOSComputationalBiology , 16(12), e1008484. 10.1371/
journal.pcbi.1008484
Smith, R., Kuplicki, R., Teed, A., Upshaw, V., & Khalsa, S. S. (2020, September 29).
Confirmatoryevidencethathealthyindividualscanadaptivelyadjustpriorexpectations
andinteroceptiveprecisionestimates. 10.1101/2020.08.31.275594
Smith, R., Mayeli, A., Taylor, S., Al Zoubi, O., Naegele, J., & Khalsa, S. S. (2021). Gut
inference:Acomputationalmodelingapproach. BiologicalPsychology,164 108152.
10.1016/j.biopsycho.2021.108152
Smith,R.,Schwartenbeck,P., Parr,T., &Friston,K. J.(2019). Anactiveinferencemodel
ofconceptlearning . bioRxiv:633677. 10.1101/633677
Smith, R., Schwartenbeck, P., Parr, T., & Friston, K. J. (2020). An active inference ap-
proachtomodelingstructurelearning:conceptlearningasanexamplecase. Fron-
tiersinComputationalNeuroscience,14 .1 0.3389/fncom.2020.00041
Smith,R.,Schwartenbeck,P.,Stewart,J.L.,Kuplicki,R.,Ekhtiari,H.,&Paulus,M.P.
(2020). Imprecise action selection in substance use disorder: evidence for active
learningimpairmentswhensolvingtheexplore-exploitdilemma. DrugandAlco-
holDependence, 215, 108208. 10.1016/j.drugalcdep.2020.108208
Smith,R.,Taylor,S.,Stewart,J.L.,Guinjoan,S.M.,Ironside,M.,Kirlic,N.,...Paulus,
M. P. (2022). Slower learning rates from negative outcomes in substance use dis-
order over a 1-year period and their potential predictive utility.Computational
Psychiatry, 6(1), 117–141. 10.5334/cpsy.85
Still, S., & Precup, D. (2012). An information-theoretic approach to curiosity-
driven reinforcement learning.Theory in Biosciences, 131(3), 139–148. 10.1007/
s12064-011-0142-z, PubMed: 22791268
Stolle, M., & Precup, D. (2002). Learning options in reinforcement learning.Lecture
NotesinComputerScience , 212–223. Springer. 10.1007/3-540-45622-8_16
Stone, J. V. (2015, February 1).Informationtheory:Atutorialintroduction . Sebtel Press.
Stone,J.V.(2019). Artificialintelligenceengines:Atutorialintroductiontothemathematics
ofdeeplearning . Sebtel Press.
Sun,Y.,Gomez,F.,&Schmidhuber,J.(2011,March29). Planningtobesurprised:Opti-
malBayesianexplorationindynamicenvironments .http://arxiv.org/abs/1103.5708
Tanaka, T. (1999). A theory of mean field approximation. In S. Solla, T. Leen, & K.
Müller (Eds.),Advancesinneuralinformationprocessingsystems , 11. MIT Press.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

Reward Maximization Active Inference 851
Tervo, D. G. R., Tenenbaum, J. B., & Gershman, S. J. (2016). Toward the neural im-
plementation of structure learning.Current Opinion in Neurobiology, 37, 99–105.
10.1016/j.conb.2016.01.014, PubMed: 26874471
Todorov,E.(2006).Linearly-solvableMarkovdecisionproblems.In Advancesinneu-
ral information processing systems, 19. MIT Press. https://papers.nips.cc/paper/
2006/hash/d806ca13ca3449af72a1ea5aedbed26a-Abstract.html
Todorov, E. (2008). General duality between optimal control and estimation. In
Proceedings of the 47th IEEE Conference on Decision and Control(pp. 4286–4292).
10.1109/CDC.2008.4739438
Todorov,E.(2009).Efficientcomputationofoptimalactions.In ProceedingsoftheNa-
tionalAcademyofSciences ,106(28), 11478–11483. 10.1073/pnas.0710743106
Tokic, M., & Palm, G. (2011). Value-difference based exploration: Adaptive Con-
trol between epsilon-greedy and Softmax. In J. Bach & S. Edelkamp (Eds.),
KI 2011: Advances in artificial intelligence (pp. 335–346). Springer. 10 .1007/
978-3-642-24455-1_33
Toussaint, M. (2009). Robot trajectory optimization using approximate inference. In
Proceedings of the 26th Annual International Conference on Machine Learning(pp.
1049–1056). 10.1145/1553374.1553508
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2019, November 24).Scaling
activeinference. http://arxiv.org/abs/1911.10601
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020).Reinforcementlearning
throughactiveinference. http://arxiv.org/abs/2002.12636
Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). Learning action-oriented models
through active inference.PLOS Computational Biology, 16(4), e1007805. 10.1371/
journal.pcbi.1007805
van den Broek, B., Wiegerinck, W., & Kappen, B. (2010).Risk sensitive path integral
control. https://arxiv.org/ftp/arxiv/papers/1203/1203.3523.pdf
van der Himst, O., & Lanillos, P. (2020).Deep active inference for partially observable
MDPs.1 0.1007/978-3-030-64919-7_8
Von Neumann, J., & Morgenstern, O. (1944).Theory of games and economic behavior.
Princeton University Press.
Wainwright, M. J., & Jordan, M. I. (2007). Graphical models, exponential families,
and variational inference.Foundations and Trend in Machine Learning, 1(1–2), 1–
305. 10.1561/2200000001
Wilson, R. C., Bonawitz, E., Costa, V. D., & Ebitz, R. B. (2021). Balancing exploration
and exploitation with information and randomization.CurrentOpinioninBehav-
ioralSciences, 38, 49–56. 10.1016/j.cobeha.2020.10.001, PubMed: 33184605
Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., & Cohen, J. D. (2014). Humans
usedirectedandrandomexplorationtosolvetheexplore–exploitdilemma. Jour-
nalofExperimentalPsychology.General ,143(6), 2074–2081. 10.1037/a0038199
Xu, H. A., Modirshanechi, A., Lehmann, M. P., Gerstner, W., & Herzog, M. H.
(2021). Noveltyis notsurprise:Human exploratoryandadaptive behaviorinse-
quentialdecision-making. PLOSComputationalBiology ,17(6), e1009070. 10.1371/
journal.pcbi.1009070
Zermelo, E. (1913). Über eine Anwendung der Mengenlehre auf die Theorie des
Schachspiels. https://www .mathematik.uni-muenchen.de/∼spielth/artikel/
Zermelo.pdf
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025

852 L. Da Costa et al.
Ziebart, B. (2010).Modeling purposeful adaptive behavior with the principle of maximum
causalentropy. Carnegie Mellon University.
Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008). Maximum entropy
inverse reinforcement learning. InProceedingsoftheAAAIConferenceonArtificial
Intelligence.
Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., & Whiteson, S.
(2020, February 27).VariBAD: A very good method for Bayes-adaptive deep RL via
meta-learning. http://arxiv.org/abs/1910.08348
Zintgraf, L. M., Feng, L., Lu, C., Igl, M., Hartikainen, K., Hofmann, K., & Whiteson,
S. (2021). Exploration in approximate hyper-state space for meta reinforcement
learning.InternationalConferenceonMachineLearning (pp. 12991–13001).
Received September 13, 2022; accepted December 17, 2022.
Downloaded from http://direct.mit.edu/neco/article-pdf/35/5/807/2079473/neco_a_01574.pdf by guest on 12 December 2025
