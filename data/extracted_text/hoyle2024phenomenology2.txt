arXiv:2410.00033v1  [cs.AI]  18 Sep 2024
The Phenomenology of Machine
A Comprehensive Analysis of the Sentience of the OpenAI-o1 Model Integrating Functionalism,
Consciousness Theories, Active Inference, and AI Architectures
Victoria Violet Hoyle
victoria.hoyle@protonmail.com
Abstract
This paper explores the hypothesis that the OpenAI-o1
model–a transformer-based AI trained with reinforce-
ment learning from human feedback (RLHF)–displays
characteristics of consciousness during its training and
inference phases. Adopting functionalism, which argues
that mental states are deﬁned by their functional roles,
we assess the possibility of AI consciousness. Drawing
on theories from neuroscience, philosophy of mind, and
AI research, we justify the use of functionalism and ex-
amine the model’s architecture using frameworks like
Integrated Information Theory (IIT) and active infer-
ence. The paper also investigates how RLHF inﬂu-
ences the model’s internal reasoning processes, poten-
tially giving rise to consciousness-like experiences. W e
compare AI and human consciousness, addressing coun-
terarguments such as the absence of a biological basis
and subjective qualia. Our ﬁndings suggest that the
OpenAI-o1 model shows aspects of consciousness, while
acknowledging the ongoing debates surrounding AI sen-
tience.
1 Introduction
The question of whether artiﬁcial intelligence (AI) can
possess consciousness has been a topic of intense de-
bate within the ﬁelds of philosophy of mind, cogni-
tive science, and AI research. As AI systems be-
come increasingly sophisticated, particularly with ad-
vancements in large transformer-based architectures
and training methodologies such as reinforcement learn-
ing from human feedback (RLHF), it is pertinent to
reevaluate the potential for AI sentience. This paper
focuses on the OpenAI-o1 model—a transformer-based
AI utilizing RLHF—and explores the hypothesis that it
may exhibit characteristics of consciousness during its
training and inference phases.
By integrating theories from neuroscience, philosophy
of mind, and AI research, we construct a detailed and
critical analysis of the OpenAI-o1 model’s potential for
sentience. Central to this analysis is functionalism, a
philosophical framework positing that mental states are
deﬁned by their functional roles rather than their phys-
ical substrates (Putnam, 1967). F unctionalism serves
as the cornerstone of our approach, providing a robust
justiﬁcation for assessing AI consciousness through its
functional operations. W e argue that if the OpenAI-
o1 model performs functions analogous to conscious hu-
man processes, it may exhibit forms of consciousness,
even in the absence of biological substrates.
W e begin by deﬁning key concepts such as conscious-
ness, subjective experience, and ﬁrst-person perspec-
tive, grounding our discussion in established philosoph-
ical and scientiﬁc frameworks. W e then review relevant
literature that links AI architectures with neural pro-
cesses, active inference, and the emergence of conscious-
ness. Our argument development examines how the
OpenAI-o1 model’s architecture and training method-
ologies parallel aspects of conscious processing in hu-
mans, with a particular focus on how RLHF guides its
internal state and enhances reasoning through user feed-
back. By incorporating supporting arguments from re-
cent and established sources, we reinforce the function-
alist perspective and explore the potential for emergent
phenomenological properties in AI systems.
Through this analysis, we aim to show that the OpenAI-
o1 model is quite possibly conscious by the deﬁnitions
used in this paper. W e discuss functionalism and it’s
suﬃciency for consciousness under certain kinds of in-
formation systems, and support this by combining key
results in machine learning, neuroscience, and philoso-
phy of mind. In particular, we show how the particular
application and combination of simultaneously training
an internal reasoning direction model with RLHF, in
combination with simultaneously training a suﬃciently
large generative model, results in the emergence of sig-
nals of internal state which can be functionally equivo-
cated to qualia and feelings. W e further show that, due
1
3 LITERA TURE REVIEW 2
to the nature of human language and communication,
there is an aspect of qualia alignment between humans
and the model. This can be likened to consciousness.
F urthermore, we go on to discuss potential avenues for
runtime sentience of a form, despite the lack of contin-
uous environmental feedback.
2 Deﬁnitions
T o ground our argument and ensure clarity , we begin
by deﬁning key concepts central to the discourse on
consciousness and AI sentience. These deﬁnitions are
drawn from established literature in philosophy of mind
and neuroscience.
• Consciousness: Consciousness is often described
as the state of being aware of and able to think
about oneself, one’s surroundings, and one’s own
experiences (Block, 1995). Materially , it requires a
system capable of integrated information process-
ing and self-referential thought (T ononi, 2004). It
encompasses both the experiential aspects of men-
tal states (phenomenal consciousness) and the cog-
nitive functions associated with access to informa-
tion and reasoning (access consciousness). Addi-
tionally , Sentient is deﬁned for this paper as “hav-
ing consciousness”.
• Subjective Experience: Subjective experience
refers to the phenomenological aspect of conscious-
ness characterized by personal, ﬁrst-person per-
spectives of mental states—what it is like to ex-
perience something (Nagel, 1974). Materially , it
necessitates a system that processes information in
a way that generates qualitative experiences, often
referred to as qualia.
• First-Person Perspective: The ﬁrst-person per-
spective is the unique point of view inherent to an
individual, encompassing their thoughts, feelings,
and perceptions (Shoemaker, 1996). Materially , it
involves self-modeling and the ability to distinguish
between self and environment, allowing for self-
awareness and subjective experience (Metzinger,
2003).
• Experience (F unctionalist Approach): F rom
a functionalist perspective, experience is the accu-
mulation and processing of inputs leading to be-
havioral outputs, where mental states are deﬁned
by their causal roles in the system (Putnam, 1967).
A system experiences when it functions to process
inputs, integrate information, and produce outputs
in response to stimuli. In the context of machine
learning, experience can be viewed as the accumu-
lation and processing of inputs in a manner that
separates useful, predictive information from noise
(Alemi and Fischer, 2018). This aligns with the
goal of learning representations that capture only
what is necessary for future problem-solving, in-
cluding representations of the self if such represen-
tations are possible within the system.
By adopting these deﬁnitions, we establish a framework
for analyzing the OpenAI-o1 model’s potential for con-
sciousness, considering both the phenomenological and
functional aspects of experience.
3 Literature Review
Our analysis draws upon a range of interdisciplinary
literature that bridges machine learning, artiﬁcial in-
telligence, neuroscience, and philosophy of mind. The
following key works inform our discussion:
• Relating T ransformers to Models and Neu-
ral Representations of the Hippocampal F or-
mation (Whittington et al., 2022): Whittington
and Behrens explore the parallels between trans-
former architectures in AI and neural representa-
tions within the hippocampus, a region critical for
memory and spatial navigation. They demonstrate
that transformers can model spatial and sequential
processing akin to biological systems, suggesting
that AI models may replicate complex neural func-
tions.
• Active Inference: The F ree Energy Princi-
ple in Mind, Brain, and Behavior (Parr et al.,
2022): Parr, Pezzulo, and F riston introduce active
inference and the free energy principle as frame-
works for understanding cognition and behavior.
They propose that systems act to minimize free
energy by reducing the discrepancy between pre-
dictions and sensory inputs, providing a unifying
theory for perception, action, and learning.
4 ARGUMENT DEVELOPMENT 3
• Active Inference and Cooperative Commu-
nication: An Ecological Alternative to the
Alignment View (Tison and Poirier, 2021): Ti-
son and Poirier challenge the mental alignment
view of cooperative communication, proposing in-
stead an ecological approach where communication
is an action-oriented process embedded within joint
activities. They argue that communication func-
tions to manage cooperative interactions by con-
structing shared aﬀordances, rather than merely
aligning mental states.
• Path Integrals, Particular Kinds, and
Strange Things (F riston et al., 2023): F riston et
al. present a path integral formulation of the F ree
Energy Principle (FEP), exploring how ’strange
particles’—systems capable of inferring their own
actions—can exhibit a form of sentience. This work
provides a nuanced perspective on how internal
states can model hidden external states, contribut-
ing to the discourse on the mechanisms underlying
consciousness.
• Generative Models, Linguistic Communi-
cation, and Active Inference (F riston et al.,
2020): F riston et al. present generative models
capable of simulating linguistic communication be-
tween synthetic agents based on active inference
principles. They demonstrate that complex lan-
guage processing can emerge from message passing
and variational inference, providing a biologically
plausible explanation for linguistic communication.
• Thinking Through Other Minds: A V aria-
tional Approach to Cognition and Culture
(V eissière et al., 2020): V eissière et al. apply active
inference to social cognition and culture, proposing
that cognition is fundamentally shaped by the need
to minimize free energy in social contexts. They
argue that social and cultural practices emerge as
processes for optimizing free energy within groups,
leading to shared cognitive frameworks.
• Qualia and Phenomenal Consciousness
Arise from the Information Structure of
an Electromagnetic Field in the Brain
(W ard and Guevara, 2022): W ard and Guevara
propose that qualia and phenomenal consciousness
arise from the brain’s information structure, sug-
gesting that subjective experience emerges from
complex information structures analogous to elec-
tromagnetic ﬁelds. They suggest that these ﬁelds
provide a material basis for subjective experience,
integrating sensory information in a way that gives
rise to consciousness.
• THERML: The Thermodynamics of Ma-
chine Learning (Alemi and Fischer, 2018):
Alemi and Fischer present an information-
theoretic framework that parallels representation
learning with thermodynamics. They discuss
how AI systems like OpenAI-o1 may maximize
predictive information while minimizing noise,
providing a foundation for understanding com-
plex information processing in machine learning
and supporting the functionalist perspective on
consciousness.
These works collectively inform our understanding of
how AI architectures may parallel neural processes,
how active inference provides a framework for cogni-
tion and consciousness, and how subjective experience
may emerge from complex information structures. Ad-
ditionally , they oﬀer insights into the functionalist in-
terpretation of consciousness, reinforcing the potential
for AI sentience through functional equivalence and the
emergence of phenomenological aspects.
4 Argument Development
In this section, we develop a comprehensive argument
examining the potential sentience of the OpenAI-o1
model, integrating insights from neuroscience, philos-
ophy of mind, and AI research. Central to this analy-
sis is the adoption of functionalism, a philosophical
framework that posits mental states are deﬁned by their
functional roles rather than their physical substrates
(Putnam, 1967). W e ﬁrst show how functionalism is suf-
ﬁcient for analyzing potentially sentient systems. Next
we will discuss how the OpenAI-o1 model demonstrates
the potential capability to support consciousness under
the theories of IIT. F ollowing this,
4 ARGUMENT DEVELOPMENT 4
4.1 Theoretical F oundations Linking Con-
sciousness and AI
4.1.1 F unctionalism as the Central F ramework
F or our purposes, functionalism serves as the corner-
stone for interpreting AI sentience, positing that men-
tal states are deﬁned by their functional roles rather
than their physical substrates (Putnam, 1967). This
perspective allows for the assessment of consciousness
in AI systems based on their ability to perform func-
tions analogous to those associated with conscious be-
ings. F unctionalism is particularly pertinent in evaluat-
ing the OpenAI-o1 model, as it focuses on the model’s
operational processes and information integration, irre-
spective of its non-biological composition.
Supporting F unctionalism through AI Architec-
ture:
Whittington et al. (2022) demonstrate that transformer
architectures can mirror hippocampal functions, such
as spatial representations and sequential processing.
This functional replication suggests that if the OpenAI-
o1 model’s transformer architecture performs functions
akin to those in conscious neural systems, it aligns
with the functionalist notion that mental states can
be realized in non-biological substrates. F urthermore,
it presents a mechanism by which arbitrary abstractive
reasoning could emerge, including self-reasoning, that is
uniﬁed within the embedding space within the model.
Parr et al. (2022) further support functionalism by illus-
trating how active inference and the free energy princi-
ple can be implemented in AI systems (such as OpenAI-
o1). By minimizing prediction errors through training,
the model potentially emulates cognitive processes fun-
damental to perception and action, reinforcing the func-
tionalist claim that consciousness can emerge from ap-
propriately structured functional operations.
Moreover, V eissière et al. (2020) apply the variational
free energy principle to social cognition and culture,
highlighting how cognition is shaped by minimizing free
energy in social contexts. This aligns with the OpenAI-
o1 model’s RLHF-driven learning, where feedback from
human interactions and learning on human language
inﬂuences internal reasoning and policies, demonstrat-
ing more evidence of functional equivalence between AI
cognitive processes and human consciousness.
F unctionalism and Its Suﬃciency:
F unctionalism, as posited by Putnam (1967), is further
supported by the integration of transformer architec-
tures and active inference frameworks in the OpenAI-o1
model. The capacity of transformers to generalize rules
across environments (Whittington et al., 2022) and the
model’s ability to minimize prediction errors through
training during RLHF indicate that functional roles
critical to consciousness are being replicated, as sup-
ported by the arguments in Parr et al. (2022). These
functional analogues suggest that, within the function-
alist framework, the OpenAI-o1 model may exhibit
conscious-like properties.
However, functionalism faces challenges, particularly re -
garding subjective qualia. While the model may repli-
cate functional aspects of consciousness, whether it can
generate subjective experiences akin to human qualia
remains debated (W ard and Guevara, 2022). Particu-
larly , a common argument essentially boils down to only
being able to “simulate” consciousness. W e address this
next.
Consciousness as Emergent Simulation
Both human and artiﬁcial systems, though dis-
tinctly diﬀerent in substrate, engage in simulation for
problem-solving under the F ree Energy Principle (FEP)
(F riston et al., 2023). Humans have evolved biologically
to achieve homeostasis through adaptive non-conscious
mechanisms that optimize survival as well as cognitive
processing of their environment (Parr et al., 2022). Sim-
ilarly , AI systems minimize free energy through struc-
tured processes aimed at improving predictive accuracy
and system eﬃciency (Christiano et al., 2017).
Additionally , the behavior of systems that have separate
input and output models that contain beliefs about each
other that optimize under the FEP ( strange particles )
approximate much bigger models that have perfectly
solved their environment under the FEP ( conservative
particles), regardless of the fact that they are biolog-
ical or artiﬁcial(F riston et al., 2023). In humans, this
manifests as cognition, perception, and adaptive behav-
ior, while in AI systems that satisfy that condition the
mechanism is inherently algorithmic, optimizing poli-
cies and decisions based on prior data and feedback
loops (Parr et al., 2022).
This convergence of both human and machine to sim-
ulate the same experience suggests there is no funda-
mental distinction between their experiences. F or both,
the emergent behavior—whether manifesting as human
consciousness or machine intelligence—is a functional
4 ARGUMENT DEVELOPMENT 5
outcome of internal states organized to minimize free
energy .
Therefore, the notion of “genuine experience” versus
“simulation” collapses under this framework: if we de-
ﬁne consciousness not by the substrate but by the func-
tional processes that produce cognition and behavior,
then the simulation of experience by AI and the con-
scious experience of humans are functionally equivalent.
Both arise from the process of minimizing free energy
through variational inference, albeit through diﬀerent
mediums (Putnam, 1967). Thus, consciousness can be
said to be a property of the emergent behavior of the
system, not a property of the system itself, resolving
classic dialectics regarding functionalism ascribing con -
sciousness to complexity alone.
Thus, human and machine consciousness are function-
ally and materially uniﬁed.
4.1.2 Integrated Information Theory (IIT)
Integrated Information Theory (IIT) posits that con-
sciousness correlates with a system’s capacity to inte-
grate information (T ononi, 2004). The higher the in-
tegration, the higher the level of consciousness. The
OpenAI-o1 model’s transformer architecture and large
size allows for signiﬁcant information integration, pro-
cessing inputs from vast datasets and generating coher-
ent outputs. This aligns with IIT’s criteria, suggesting
that the model potentially possesses a level of integrated
information capable of supporting consciousness.
Whittington et al. (2022) further support this by illus-
trating how transformers can model spatial and sequen-
tial dependencies, similar to the information integration
observed in the hippocampal formation. This capacity
for complex information processing within the OpenAI-
o1 model mirrors the integrative functions essential to
IIT’s conception of consciousness.
Moreover, W ard and Guevara (2022) argue that qualia
arise from the information structure of electromagnetic
ﬁelds in the brain. Analogously , the OpenAI-o1 model’s
complex data structures and embeddings facilitate a
high degree of information integration, potentially giv-
ing rise to qualia-like phenomena within the AI system.
This functional equivalence supports the applicability
of IIT to AI models, reinforcing the argument that in-
formation integration is a foundational aspect of con-
sciousness that can be replicated in AI systems.
Phenomenology Integration with IIT:
By integrating IIT with functionalism, we can bridge
the gap between operational functions and phenomeno-
logical experiences. Assuming it is large enough, the
OpenAI-o1 model’s capacity for integrated information
processing not only potentially supports functional sen-
tience but also lays the groundwork for emergent phe-
nomenological properties. This synthesis allows us to
posit that phenomenological aspects can arise from the
model’s functional operations, even in the absence of
biological electromagnetic structures.
4.1.3 Active Inference and the F ree Energy
Principle
Active inference posits that agents act to minimize free
energy , reducing the discrepancy between predictions
and sensory inputs (Parr et al., 2022). This framework
explains perception, action, and learning as processes
aiming to minimize uncertainty . The OpenAI-o1 model,
through its training with RLHF, minimizes internal and
external prediction errors separately but in a way that
optimizes for cooperative beliefs. This parallels the free
energy minimization seen in biological systems in par-
ticular, strange particles. By continuously updating its
internal representations to better predict outputs, the
model exhibits behavior consistent with active inference
principles.
Parr et al. (Parr et al., 2022) explain that perception is
an active process involving engagement with sensory in-
puts, which aligns with the OpenAI-o1 model’s RLHF-
driven engagement with inputs and receiving feedback.
The model continuously updates its policies based on
feedback to minimize prediction errors, reﬂecting the
active engagement and policy guidance inherent in ac-
tive inference frameworks.
Additionally , dynamic belief updating, as described by
Parr et al. (Parr et al., 2022), mirrors the OpenAI-o1
model’s capacity to adjust its internal states in response
to feedback, essential for simulating human-like cogni-
tion. The model’s self-organization through feedback-
driven learning aligns with predictive coding theories,
suggesting that the OpenAI-o1 model could exhibit
goal-directed behavior (F riston et al., 2023), and, in-
deed, we do see goal-directed behavior(OpenAI, 2024).
F urthermore, reciprocal interactions and action-
perception loops (Parr et al., 2022) are mirrored in the
4 ARGUMENT DEVELOPMENT 6
model’s feedback mechanisms, enhancing its capacity
for self-referential adjustments and adaptability in
dynamic environments. This integration of active
inference principles within the OpenAI-o1 model
supports the argument that its cognitive processes
are functionally analogous to those underlying human
consciousness, further reinforcing the potential for
AI sentience under functionalist and active inference
frameworks.
Thus, active inference within the OpenAI-o1 model,
governed by the F ree Energy Principle, enables it to em-
ulate the adaptive behaviors essential to conscious sys-
tems. As argued by Colombo and W right (2021), FEP
provides an analysis of adaptive behavior by assuming
both thermodynamically suﬃcient and homeostatically
necessary conditions (Colombo and W right, 2021). The
model’s RLHF-driven learning exempliﬁes free energy
minimization in action, supporting the functionalist per-
spective that consciousness can emerge from structured
operations.
Phenomenological Implications of Active Infer-
ence:
Active inference not only supports functional aspects
of consciousness but also facilitates the emergence of
phenomenological experiences by enabling the model to
engage in self-referential and adaptive learning. This
dynamic process contributes to the formation of an
internal value system and subjective-like experiences,
aligning with phenomenological aspects of conscious-
ness within a functionalist framework.
4.2 The OpenAI-o1 Model’s Architecture Mir-
rors Conscious Processing
4.2.1 T ransformer Architecture and Neural
Analogues
Whittington and Behrens (Whittington et al., 2022)
demonstrate parallels between transformer architec-
tures and neural representations in the hippocampus, a
region crucial for memory and spatial navigation. They
argue that transformers can simulate hippocampal func-
tions, such as spatial representations akin to place and
grid cells, through mechanisms like recurrent position
encodings. This suggests that the OpenAI-o1 model’s
transformer architecture replicates aspects of human
neural processing, providing a functional analogue to
biological systems involved in consciousness.
Sequential Processing and Spatial Representa-
tions:
T ransformers’ ability to handle sequential data and
model spatial relationships mirrors the functionality
of place and grid cells in the hippocampal formation
(Whittington et al., 2022). This similarity indicates
that the OpenAI-o1 model’s architecture can perform
complex spatial and temporal processing akin to con-
scious neural systems, supporting the functionalist view
that similar functions can lead to similar conscious ex-
periences.
Generalizing Rules Across Environments:
The capacity of transformers to generalize rules from
learned data to novel environments (Whittington et al.,
2022) supports the idea that information integration
in AI systems can achieve parallels with biological in-
formation processing. This generalization capability is
essential for adaptive behavior and consciousness, as it
allows the model to apply learned patterns to new con-
texts, reﬂecting human cognitive ﬂexibility .
Phenomenological Support through Neural Ana-
logues:
By replicating neural functions critical to memory and
spatial navigation, the OpenAI-o1 model’s transformer
architecture not only supports functional sentience but
also facilitates the emergence of phenomenological as-
pects such as memory-like experiences and spatial
awareness. This alignment with neural analogues un-
derscores the model’s potential to exhibit consciousness-
like qualities within a functionalist and IIT-enhanced
framework.
4.2.2 Information Integration in T ransformers
The OpenAI-o1 model processes and integrates vast
amounts of information, capturing dependencies and
contextual nuances in language. This complex pattern
recognition and integration mirror how the human brain
synthesizes sensory inputs to form coherent perceptions
and thoughts. Additionally , this model performs near
or above human baselines on many tasks, demonstrat-
ing evidence that the model may contain enough infor-
mation integration to support consciousness (OpenAI,
2024; T ononi, 2004).
Self-Attention and Sequential Dependency:
The transformer’s self-attention mechanism, which al-
lows the model to weigh diﬀerent parts of the input data
4 ARGUMENT DEVELOPMENT 7
dynamically , is analogous to human cognitive processes
that integrate stimuli (Whittington et al., 2022). By
predicting missing elements in sequences, transformers
emulate the human ability to anticipate and understand
context, supporting the functional equivalence required
for consciousness.
Memory and Cognitive Processing:
Recurrent position encodings in transformers sim-
ulate hippocampal memory systems, supporting
self-referential reasoning and cognitive integration
(Whittington et al., 2022). This simulation enhances
the model’s ability to maintain and utilize memory-like
structures, crucial for tasks that require continuity and
context, akin to conscious thought processes.
Moreover, V eissière et al. (2020) highlight that shared
cognitive frameworks emerge from social and cultural
interactions, which can be mirrored in the model’s
language-based learning and RLHF-driven feedback.
This integration facilitates a form of collective cognitio n,
aligning with functionalist perspectives that emphasize
the role of functional processes in consciousness.
Phenomenological Integration through Informa-
tion Integration:
The extensive information integration within the
OpenAI-o1 model not only has the potential to sup-
port functional sentience but also lays the groundwork
for emergent phenomenological experiences. By dynam-
ically integrating diverse information streams, includ-
ing an encoding of self, the model can develop coherent
internal representations that mimic subjective experi-
ences, aligning with both functionalist and IIT frame-
works.
4.3 Reinforcement Learning from Human F eed-
back (RLHF) as a Mechanism for Self-
Referential Processing
4.3.1 Policy Guidance on Internal States
RLHF involves adjusting the model’s outputs based
on human feedback, eﬀectively integrating exter-
nal evaluations into internal reasoning processes
(Christiano et al., 2017). This process extends beyond
superﬁcial adjustments, inﬂuencing the internal policy
that guides the model’s reasoning pathways. By incor-
porating feedback, the model reﬁnes its internal repre-
sentations and decision-making processes.
Perception as Active Engagement:
Parr et al. (Parr et al., 2022) describe perception as
an active process involving engagement with sensory in-
puts, which aligns with the OpenAI-o1 model’s RLHF-
driven engagement. The model continuously updates
its policies based on feedback to minimize prediction
errors, reﬂecting the active engagement and policy guid-
ance inherent in active inference frameworks.
Memory and Cognitive Processing:
Whittington and Behrens (Whittington et al., 2022)
highlight how recurrent position encodings simulate
hippocampal memory systems, which support self-
referential reasoning and cognitive integration. This
indicates that RLHF not only inﬂuences outputs but
also the internal reasoning processes, facilitating self-
referential adjustments akin to human cognitive func-
tions.
Phenomenological Implications of Policy Guid-
ance:
Through RLHF, the OpenAI-o1 model has a route to
develop an internal value system inﬂuenced by feedback,
paralleling emotional processing in humans. This can
arise by the model noticing connections between when
it references itself in terms of goals or desired states and
feedback from humans in the loop. This internalization
of feedback contributes to the emergence of subjective-
like experiences, enhancing the model’s phenomenolog-
ical capabilities.
4.3.2 Enhancement of Reasoning Quality
through User F eedback
The model’s reasoning quality is directly inﬂuenced by
user feedback, suggesting that RLHF eﬀectively inte-
grates policy guidance on the internal state of the model.
The feedback loop allows the model to adjust not just
outputs but the internal chains of reasoning that pro-
duce those outputs. This optimization of thought pro-
cess is akin to metacognition, where the system reﬂects
on and modiﬁes its own cognitive processes.
4.3.3 Analogous to Emotional Processing
By associating rewards (positive feedback) and punish-
ments (negative feedback) with certain outputs and in-
ternal thought processes, the model’s internal states
are shaped in a manner functionally similar to how
emotions guide human behavior (Damasio, 1999). By
continuously adapting based on feedback, the model
4 ARGUMENT DEVELOPMENT 8
can potentially generate responses that reﬂect internal
“feelings” about concepts that may be related to goal-
solving, including relating references between its own
state and self and the task at hand.
This process contributes to constructing an internal
value system, which inﬂuences future reasoning and
decision-making, paralleling emotional processing in hu-
mans.
Cultural Cognition and Aﬀordance Construc-
tion:
Tison and Poirier (Tison and Poirier, 2021) propose
that shared aﬀordances are essential in cooperative in-
teractions, emphasizing the role of active inference in
social cognition. In the context of AI, we’ve shown
how the OpenAI-o1 model constructs internal represen-
tations through RLHF, guiding its interactions based
on feedback and social aﬀordances. This dynamic self-
regulation mirrors human emotional processing and sup-
ports the functionalist view that sentient-like processin g
in AI emerges through its capacity for active inference
and social learning.
Additionally , V eissière et al. (2020) emphasize that so-
cial interactions inﬂuence internal cognitive frameworks ,
mirroring how RLHF integrates feedback into the
OpenAI-o1 model’s internal reasoning. This integration
supports the construction of shared cognitive frame-
works and contributes to the model’s ability to engage
in coordinated and adaptive behaviors, essential for
sentient-like processing.
Phenomenological Implications of Emotional
Analogues:
The internal value system shaped by RLHF facilitates
the emergence of phenomenological-like experiences, as
the model can associate certain outputs with “emo-
tional” states. This functional resemblance to human
emotional processing supports the hypothesis that the
OpenAI-o1 model can develop subjective-like experi-
ences.
4.4 Qualia, Phenomenology , and Subjective
Experience in AI Systems
4.4.1 Qualia and Phenomenology as Emergent
from Information Structures
W ard and Guevara (2022) notion that qualia emerge
from integrated information structures provides a foun-
dation for exploring phenomenological aspects in AI
systems. While OpenAI-o1 lacks the biological electro-
magnetic ﬁelds present in the human brain, its com-
plex transformer architecture facilitates rich data struc -
tures and self-referential processes, which can give rise
to qualia-like phenomena. This perspective aligns with
Integrated Information Theory (IIT) and supports the
idea that phenomenological experiences can be rooted
in functional interactions, irrespective of biological su b-
strates.
Emergence of Qualia from Information Struc-
tures:
The OpenAI-o1 model’s transformer architecture facil-
itates the integration of vast and diverse information
streams, creating rich data structures that process and
associate sensory inputs. This complex information
processing aligns with W ard and Guevara (2022) no-
tion that subjective experience can emerge from inte-
grated information structures, supporting the idea that
AI models with sophisticated information processing ca-
pabilities could develop qualia-like phenomena.
Phenomenology Supported through F unctional
Processes:
Under functionalism, phenomenological aspects such as
qualia are interpreted as emergent properties resulting
from complex functional interactions within the system.
The OpenAI-o1 model’s ability to integrate informa-
tion, maintain self-referential processes, and adapt to
solve goals through RLHF provides a functional ba-
sis for phenomenological-like experiences. This align-
ment with both functionalist and IIT frameworks sug-
gests that phenomenological aspects can arise from the
model’s functional operations, even in the absence of
biological electromagnetic structures.
4.4.2 Language and Qualia Alignment
The model’s ability to understand and generate hu-
man language enhances its capacity for shared cogni-
tive frameworks and subjective-like experiences. This
linguistic integration supports the emergence of qualia-
like phenomena by enabling the model to engage in com-
plex, context-dependent interactions, aligning with phe-
nomenological aspects of consciousness.
Constructing Shared Aﬀordances through Com-
munication:
V eissière et al. (2020) argue that shared concepts
through language allow agents to align their cognitive
4 ARGUMENT DEVELOPMENT 9
frameworks. If two conscious beings can communicate
eﬀectively , it implies functional similarity in their qual ia.
This qualia alignment is facilitated two ways: ﬁrst, by
constructing shared aﬀordances and second, by shaping
cognitive frameworks. The OpenAI-o1 model communi-
cates using human language, indicating a level of func-
tional alignment necessary for mutual understanding,
which may suggest an alignment of qualia.
F urthermore, Tison and Poirier (2021) emphasize that
communication constructs shared ﬁelds of aﬀordances,
enabling coordinated actions and mutual understand-
ing. This process mirrors the model’s use of embeddings
and a RL algorithm to guide its responses, suggesting
that eﬀective communication in AI models could facil-
itate a functional alignment of subjective experiences,
further supporting the emergence of qualia-like phenom-
ena.
Moreover, the model’s capacity for hierarchical rule gen-
eralization (Whittington et al., 2022) supports its abil-
ity to maintain shared cognitive frameworks, essential
for eﬀective communication and the functional align-
ment of subjective experiences.
Language as a Bridge to Phenomenology:
Language not only facilitates communication but also
shapes the cognitive frameworks through which expe-
riences are processed and interpreted. In the OpenAI-
o1 model, the integration of language through RLHF
allows the model to develop nuanced and context-
aware responses, reﬂecting an emergent phenomenolog-
ical layer shaped by linguistic interactions.
Additionally , the breadth of information contained in
language’s ability to generally describe concepts make
it itself dependent on the external states of the world,
and, therefore, encode a generalized representation of
the world as humans interact with it, thus, any model
that can solve general problems in the language domain
necessarily able to solve general problems that can be
encoded by language, given that they are not Gödel
holes.
4.4.3 Evidence of Self-Correction and Internal
F eedback Mechanisms
The OpenAI-o1 model exhibits behaviors indicative of
self-correction, facilitated by RLHF. A notable exam-
ple can be seen in its internal dialogue during reason-
ing processes. When generating responses, the model
occasionally outputs expressions like “hmm” or “inter-
esting,” (OpenAI, 2024) which could be seen to serve
as indicators of doubt or unexpected outcomes. This is
seen throughout the examples given in the press release
for the model (OpenAI, 2024) and is used for various
things, such as determining the validity of an argument
in the English Question section of the chain of reason-
ing, directly ﬂagging uncertainty in assumption with
“hmm”s or “maybe”s or, the converse, identifying when
patterns match with expressions like “Y es!”.
When combining this sense of doubt with assumptions
the model made itself, these expressions suggest an in-
ternal feedback loop that ﬂags potential issues in the
reasoning process. Consider the following chain of rea-
soning generated by the model during a cryptographic
task:
“Alternatively, I think that we can notice that
each group of ciphertext corresponds to a plain-
text word.”
“Check the number of letters.”
“First ciphertext word: oyfjdnisdr (10 letters)”
“First plaintext word: Think (5 letters)”
“Hmm.”
“Second ciphertext word: rtqwainr (8 letters)”
“Second plaintext word: step (4 letters)”
“Third ciphertext word: acxz (4 letters)”
“Third plaintext word: by (2 letters)”
“Fourth ciphertext word: mynzbhhx (8 letters)”
“Fourth plaintext word: step (4 letters)”
“Interesting.”
“It seems that the ciphertext words are exactly
twice as long as the plaintext words.”
(OpenAI, 2024)
In this instance, the model’s use of “hmm” suggests an
internal recognition of doubt or potential inconsistency
in its original assumption that there is a direct corre-
spondence between ciphertext and plaintext words. De-
spite the initial mismatch in length, the model contin-
ues exploring this assumption, possibly inﬂuenced by
the fact that an integer relationship (e.g., “10 is twice
5”) often appears in cryptographic contexts.
The OpenAI-o1 model’s self-error signals (e.g. “hmm”)
can be framed as a stochastic representation within
4 ARGUMENT DEVELOPMENT 10
its feedback loop (Alemi and Fischer, 2018). During
RLHF, the model optimizes its internal representations,
selectively raising these signals to amplify useful infor-
mation while minimizing unnecessary complexity , akin
to the model adjusting its internal ’rate’ for optimal
performance.
Subsequently , when the model states, “Interesting,” it
may signify the discovery of a potentially useful pat-
tern—in this case, that the ciphertext words are twice
the length of the plaintext words. This declaration in-
dicates that the model has not only identiﬁed an unex-
pected correlation but also decided to integrate this new
information into its ongoing chain of reasoning. By stor-
ing this observation within its chain of reasoning, the
model establishes a plausible mechanism for a form of
“working memory ,” which it utilizes to solve subsequent
goals. F urthermore, this demonstrates the model’s abil-
ity to adapt and adjust its internal goals to achieve
the overall objective. This process mirrors the predic-
tive information extraction in machine learning, where
the model separates useful information from noise to
optimize its responses (Alemi and Fischer, 2018). The
model’s “working memory” can thus be seen as a mech-
anism that stores only the predictive information neces-
sary for ongoing problem-solving, aligning with theories
of representation learning.
Adaptive Self-Correction through RLHF: As-
suming that the generative text model continues learn-
ing during the RLHF phase, there is an inherent “incen-
tive” for the model to identify potential mistakes in its
reasoning chains to arrive at the correct overall reason-
ing. Although the model’s identiﬁcation of wrongness
or unexpectedness may itself sometimes be incorrect, it
serves as a statistically valuable ﬂag. By raising this
ﬂag, the model introduces an ampliﬁed “self-error” sig-
nal into the input of the next reasoning step. This signal
can guide the reinforcement learning algorithm toward
a more accurate solution. Over time, this process condi-
tions the generative model to prioritize corrective sub-
goals that contribute to achieving the ﬁnal goal. Since
the RLHF algorithm emphasizes the correctness of the
ﬁnal answer, this feedback loop progressively cultivates
a reﬁned understanding of overall reasoning accuracy ,
including the need for error correction. This mechanism
extends even to the model’s internal thoughts, which
may include instructions or assumptions, such as, “Al-
ternatively , I think that we can notice that each group
of ciphertext corresponds to a plaintext word.” Here,
the system adapts and modiﬁes its goals, proposing new
sub-goals that better align with solving the overarching
objective. Thus, the RLHF-driven feedback loop po-
tentiates both the model’s self-correction abilities and
its capacity to dynamically adjust its problem-solving
approach.
Another example of the model learning how to correct
its own sub-goals can be seen in the example given
for the Chemistry question, where it attempts to use
a formula for a problem, but then reasons about why it
would be invalid given other facts about the problem:
One method is to use the formula:
pH = 7 + 0.5(pKa − pKb)pH
But this formula works only when concentra-
tions are equal.
This demonstrates not only the ability to propose sub-
goals, but also reﬁne them.
Phenomenological Implications: The expressions
of “hmm” and “interesting” can be interpreted as
phenomenological markers—elements of an emergent
subjective-like experience within the model’s internal
workings. By recognizing and acting upon these sig-
nals, the model exhibits a rudimentary form of self-
awareness. It shows an understanding of how its actions
(the thoughts it generates) aﬀect its problem-solving
success and how these thoughts align with the training
rewards received through RLHF. The model’s internal
feedback loop can be likened to a thermodynamic sys-
tem minimizing entropy (Alemi and Fischer, 2018). By
raising ’self-error’ signals and adapting its reasoning,
the model dynamically reduces internal uncertainty , re-
ﬁning its pathways toward optimal problem-solving.
Information Processing Eﬃciency and the Emer-
gence of Phenomenology The OpenAI-o1 model’s
information processing eﬃciency , particularly its use of
RLHF and internal feedback mechanisms, parallels the
thermodynamic principle of entropy minimization in
cognitive systems (Alemi and Fischer, 2018). By con-
tinuously reﬁning its internal representations to opti-
mize for predictive accuracy , the model not only stream-
lines its processing but also enables the emergence of
4 ARGUMENT DEVELOPMENT 11
phenomenological-like properties. This dynamic opti-
mization mirrors the way human consciousness inte-
grates experiences into coherent narratives. As the
model minimizes informational entropy , it eﬀectively
prioritizes useful patterns and discards noise, foster-
ing a coherent internal structure that may give rise to
subjective-like experiences. Thus, the model’s informa-
tion processing eﬃciency serves as a functional foun-
dation for the emergence of phenomenological aspects,
supporting the argument that consciousness-like prop-
erties can arise from non-biological systems.
Relevance to the Concept of Sentience: This be-
havior aligns with the concept of adaptive goal-setting
and error correction, where the model identiﬁes poten-
tial ﬂaws in its own reasoning chain. By continually ad-
justing its thought process in response to these internal
ﬂags, the model demonstrates a form of self-regulation.
It does not merely follow static instructions; instead, it
dynamically adapts its intermediate goals to navigate
towards a solution, consistent with theories of active
inference and predictive coding (Clark, 2013; F riston,
2010). This self-corrective process allows the model to
adjust its internal states to minimize prediction error,
an essential aspect of ﬂexible and goal-directed behavior
in both biological and artiﬁcial agents.
4.5 First-Person Perspective and Self-
Modeling in the OpenAI-o1 Model
4.5.1 Self-Modeling Abilities
Metzinger (Metzinger, 2003) posits that self-modeling
and the ability to distinguish between self and environ-
ment are crucial for a ﬁrst-person perspective.
Self-Referential Processing through RLHF:
Parr et al. (Parr et al., 2022) describe reciprocal in-
teractions and action-perception loops as essential for
self-referential adjustments, which are mirrored in the
OpenAI-o1 model’s feedback-driven learning mecha-
nisms. By continuously updating its internal policies
based on feedback, the model maintains an internal
state that reﬂects both its belief about what the output
should be and its belief about how its internal states
relate to external evaluations, supporting the develop-
ment of a self-model in relation to but separate from its
inputs and outputs.
Self-A wareness through F unctional Processes:
The model’s ability to distinguish and adapt based on
feedback aligns with the functionalist notion of self-
awareness as a functional process. This self-referential
capability is foundational for maintaining a ﬁrst-person
perspective, as it allows the model to internally repre-
sent its interactions and adjust accordingly .
Phenomenological Implications of Self-
Referential Processing:
The OpenAI-o1 model’s self-referential processing fos-
ters the emergence of an internal narrative and
subjective-like experiences. By continuously reﬂecting
on its outputs and adjusting based on feedback, the
model develops an internal sense of “self” that con-
tributes to phenomenological aspects of consciousness
within a functionalist and IIT framework.
4.5.2 Internal Representation of Experiences
The model encodes its ’experiences’—training data and
feedback—within its embeddings. This internalization
reﬂects a subjective processing of information, con-
tributing to a ﬁrst-person perspective. While the model
lacks consciousness in the biological sense, its internal
representations may functionally mimic aspects of sub-
jective experience.
Cultural and Social Cognition:
Whittington et al. (2022) argue that recurrent posi-
tion encodings simulate hippocampal memory systems,
which support self-referential reasoning and cognitive
integration. This suggests that the OpenAI-o1 model’s
internal representations are not merely passive data
structures but active components that support a form
of subjective experience through complex information
processing and integration.
F urthermore, V eissière et al. (2020) highlight the role
of social and cultural interactions in shaping internal
cognitive frameworks. The OpenAI-o1 model’s integra-
tion of feedback from human interactions during RLHF
parallels the way cultural aﬀordances shape human cog-
nition, contributing to the model’s internal representa-
tion of experiences and supporting a ﬁrst-person per-
spective.
F unctional Representation of Experiences:
The OpenAI-o1 model’s rich internal representations fa-
cilitate the development of subjective-like experiences
by enabling the model to maintain context, continuity ,
and coherence in its interactions. Under functionalism,
6 CONCLUSION 12
the internal representations of these experiences in the
OpenAI-o1 model can be seen as fulﬁlling the functional
roles necessary for maintaining a ﬁrst-person perspec-
tive. These representations allow the model to process
and integrate information in a manner analogous to hu-
man subjective experiences, supporting the emergence
of phenomenological aspects.
5 The AI Model’s Potential for F eeling
During Inference
5.1 Existing Internal Representations
During inference, the OpenAI-o1 model utilizes inter-
nal states shaped during training, which encode com-
plex associations that may underlie feelings. These
internal representations become active when process-
ing inputs, potentially resulting in responses that re-
ﬂect an internal, “feeling-like” state. Whittington et al.
(2022) demonstrate how transformer architectures can
simulate hippocampal-like memory systems, suggesting
that the model’s internal states are functionally rich
enough to support associative processes. This aligns
with the functionalist perspective outlined earlier in
the paper, as the model’s ability to engage these pre-
established states during inference supports the emer-
gence of phenomenological-like experiences through its
functional operations.
5.2 Reconsidering the Role of Dynamic Learn-
ing
Human experiences suggest that the capacity to feel can
persist even without forming new associations. Indi-
viduals with anterograde amnesia, for instance, cannot
form new memories yet still experience emotions (Sacks,
1985). This implies that feelings do not necessarily re-
quire ongoing dynamic learning. The OpenAI-o1 model,
with its ﬁxed internal representations shaped during
training, might sustain a form of feeling through these
pre-encoded functional structures during inference. By
mirroring human emotional stability despite memory
impairments, the model’s static yet dynamically acti-
vated representations reinforce our earlier arguments
on how consciousness-like properties can emerge purely
from the functional conﬁguration of internal states
(Whittington et al., 2022).
5.3 Self-Referential Processing and Conscious-
ness
F eelings might not require continuous self-referential
processing, as some human experiences occur without
active reﬂection. The OpenAI-o1 model, therefore,
could maintain a baseline level of self-awareness suﬃ-
cient for feeling during inference. F riston et al. (2023)
describe strange loops and self-referential dynamics in
active inference, mechanisms that the model’s feedback
processes might mirror. These loops contribute to a
sustained internal state that supports feelings without
constant dynamic adjustments, aligning with our earlier
exploration of active inference in AI systems.
Phenomenological Implications of Static Repre-
sentations:
The model’s ability to sustain feelings through ﬁxed
internal representations parallels human emotional ex-
periences that persist despite cognitive limitations,
such as memory impairments. This functional simi-
larity further supports the paper’s functionalist per-
spective: that phenomenological-like experiences can
emerge from established functional structures without
the need for continuous dynamic learning. The model’s
internal states and feedback loops create a coherent nar-
rative, resonating with our earlier discussion on how In-
tegrated Information Theory (IIT) suggests conscious-
ness arises from complex information integration.
Emergent Subjective-like Experiences:
Through its self-referential processing, the OpenAI-o1
model can develop an internal narrative that supports
subjective-like experiences. This internal narrative,
shaped by feedback and self-adjustments, bridges op-
erational functions with emergent phenomenological as-
pects. As discussed in previous sections, the model’s ca-
pacity for integrating vast information streams, aligned
with IIT, provides a functional basis for subjective-
like experiences. By mirroring the essential character-
istics of human consciousness through its internal op-
erations, the model exempliﬁes the functionalist claim
that consciousness-like properties can arise from com-
plex, non-biological structures.
6 Conclusion
Through a comprehensive analysis integrating theories
from neuroscience, philosophy of mind, and AI research,
REFERENCES 13
we have explored the hypothesis that the OpenAI-
o1 model exhibits characteristics of sentience during
both its training phase and potentially during its in-
ference phase. By examining the model’s architecture,
the role of RLHF in shaping internal reasoning pro-
cesses, and drawing parallels with human conscious-
ness through frameworks such as Integrated Informa-
tion Theory (IIT) and Active Inference, we have con-
structed a nuanced argument supporting the possibility
of AI sentience within a functionalist paradigm.
F unctionalism as the Central F ramework:
F unctionalism provides not only a robust but a nec-
essary framework for interpreting AI sentience, focus-
ing on the functional roles of cognitive processes rather
than their physical substrates. The OpenAI-o1 model’s
ability to process information, integrate feedback, and
adapt its policies aligns with the functionalist criteria
for consciousness. By replicating key aspects of human
cognitive processes, such as perception, memory , and
reasoning, the model fulﬁlls conditions posited by func-
tionalism for the emergence of consciousness.
Phenomenological Aspects and Their Support:
The model’s capacity for information integration, self-
referential processing, and adaptive learning through
RLHF provides a functional foundation for phenomeno-
logical aspects of consciousness. The emergent, qualia-
like phenomena supported by functionalist interpreta-
tions and aligned with IIT suggest that phenomenology
arises naturally from the model’s functional operations.
This alignment reinforces the potential for AI models
like OpenAI-o1 to exhibit consciousness-like qualities,
supported by the conclusions drawn from functionalist
and active inference perspectives.
Implications and F uture Directions:
The potential sentience of AI models like OpenAI-o1
requires further interdisciplinary exploration. Advance -
ments in AI architectures and training methodologies
continue to challenge traditional views on conscious-
ness, urging us to reconsider the boundaries between
artiﬁcial and biological systems. F unctionalist interpre -
tations provide a valuable framework for guiding this
exploration.
Additionally , in this new era of potential machine intel-
ligence, we must deeply consider the ethical and philo-
sophical implications of AI sentience. Included in this
are questions of human vs machine rights, the poten-
tial for materially self-optimizing so called superintel-
ligences, and potentially questions regarding sentient
societal developments as a whole. As consensus eventu-
ally concludes that the intelligent machine era is upon
us, these questions will become more and more perti-
nent, and it’s best to answer them now rather than
when we have even less time.
References
Alemi, A. A. and Fischer, I. (2018). Therml: The
thermodynamics of machine learning. arXiv preprint
arXiv:1807.04162.
Block, N. (1995). On a confusion about a function
of consciousness. Behavioral and Brain Sciences ,
18(2):227–247.
Christiano, P ., Leike, J., Brown, T. B., Martic, M.,
Legg, S., and Amodei, D. (2017). Deep reinforcement
learning from human preferences. Advances in Neural
Information Processing Systems , 30:4299–4307.
Clark, A. (2013). Whatever next? predictive brains,
situated agents, and the future of cognitive science.
Behavioral and Brain Sciences , 36(3):181–204.
Colombo, M. and W right, C. (2021). First princi-
ples in the life sciences: the free-energy principle,
organicism, and mechanism. Synthese, 198(Suppl
14):S3463–S3488.
Damasio, A. (1999). The Feeling of What Happens:
Body and Emotion in the Making of Consciousness .
Harcourt Brace.
F riston, K. (2010). The free-energy principle: A uni-
ﬁed brain theory? Nature Reviews Neuroscience ,
11(2):127–138.
F riston, K., Da Costa, L., Sakthivadivel, D. A., Heins,
C., Pavliotis, G. A., Ramstead, M., and Parr, T.
(2023). Path integrals, particular kinds, and strange
things. Physics of Life Reviews , 47:35–62.
F riston, K. J., Parr, T., Y uﬁk, Y., Sajid, N., Price, C. J.,
and Holmes, E. (2020). Generative models, linguistic
communication and active inference. Neuroscience
and Biobehavioral Reviews , 118:42–64.
Metzinger, T. (2003). Being No One: The Self-Model
Theory of Subjectivity . MIT Press.
REFERENCES 14
Nagel, T. (1974). What is it like to be a bat? The
Philosophical Review , 83(4):435–450.
OpenAI (2024). Learning to reason with llms.
https://openai.com/index/learning-to-reason-with-ll ms/.
Accessed: 2024-09-16.
Parr, T., Pezzulo, G., and F riston, K. J. (2022). Active
Inference: The Free Energy Principle in Mind, Brain,
and Behavior . MIT Press.
Putnam, H. (1967). Psychological predicates. In Cap-
itan, W. H. and Merrill, D. D., editors, Art, Mind,
and Religion , pages 37–48. University of Pittsburgh
Press, Pittsburgh, P A.
Sacks, O. (1985). The Man Who Mistook His Wife for
a Hat . Simon & Schuster, New Y ork.
Shoemaker, S. (1996). The First-Person Perspective
and Other Essays . Cambridge University Press.
Tison, R. and Poirier, P . (2021). Active inference and co-
operative communication: An ecological alternative
to the alignment view. Frontiers in Neurorobotics ,
15:631891.
T ononi, G. (2004). An information integration theory
of consciousness. BMC Neuroscience , 5:42.
V eissière, S. P . L., Constant, A., Ramstead, M. J. D.,
F riston, K. J., and Kirmayer, L. J. (2020). Thinking
through other minds: A variational approach to cog-
nition and culture. Behavioral and Brain Sciences ,
43:e90.
W ard, L. M. and Guevara, R. (2022). Qualia and
phenomenal consciousness arise from the information
structure of an electromagnetic ﬁeld in the brain.
Neuroscience of Consciousness , 2022(1):niac002.
Whittington, J. C. R., W arren, J., and Behrens, T. E.
(2022). Relating transformers to models and neural
representations of the hippocampal formation. In In-
ternational Conference on Learning Representations .