Learning Risk Preferences Through Social
Interaction: An Active Inference Approach
Amir Hossein Tehrani-Safa 
Shahid Beheshti University
Reza Ghaderi 
Shahid Beheshti University
Atiye Sarabi-Jamab 
University of Tehran
Article
Keywords:
Posted Date: October 7th, 2024
DOI: https://doi.org/10.21203/rs.3.rs-4991489/v1
License:   This work is licensed under a Creative Commons Attribution 4.0 International License.  
Read Full License
Additional Declarations: No competing interests reported.
Learning Risk Preferences Through Social
Interaction: An Active Inference Approach
Amir Hossein Tehrani-Safa1,2, Reza Ghaderi1, and Atiye Sarabi-Jamab3,*
1 Institute for Cognitive and Brain Sciences, Shahid Beheshti University , T ehran, P .O. Box 19839-63113, Iran.
2
School of Cognitive Sciences, Institute for Research in Fundamental Sciences (IPM),T ehran, P .O. Box 19395-5746,
Iran
3 Faculty of Governance, University of T ehran, T ehran, 1417633461, Iran
* asarabi@ut.ac.ir
ABSTRACT
Social learning involves the ability to make assumptions about a person’s attitudes and preferences on the basis of their
observed behaviors. These characteristics are then used to make predictions and inﬂuence how one interacts with individuals
in various social contexts. In this research, we used an active inference framework to explore how humans can assess another
person’s risk preferences. The active inference learner uses a probabilistic generative model to model others’ decision making
process and then employs a V ariational Bayesian method based on the free-energy principle to invert the generative model. On
a trial-by-trial basis, the observer updates the posterior estimate of the other’s hidden characteristic via social prediction error, a
mismatch between what we predicted and what they chose. The model begins with a high learning rate and gradually reduces
it as more trials occur. This reﬂects that humans learn from examples sequentially , exploring and using the information as they
progress. As more data are accumulated from someone who follows a consistent decision-making process, the observer’s
faith in his own judgments should grow, and he should be less affected by each new observation. Understanding others’ risk
preference is a complex example of social reasoning. We employed a sequential scenario where an observer watched the other
person’s choices between a high-risk gamble and a guaranteed small reward. We discovered that the inferential uncertainty
that the learner has in trying to estimate the agent’s risk attitude, is largely based on the prediction uncertainty accumulated by
the observer when attempting to guess the agent’s choices.
Introduction
Social learning allows us to gain knowledge through observing others 1 . However, successful social learning requires under-
standing others’ characteristics and preferences, as simply copying someone else’s behavior might not be optimal 2 . This is
particularly true in regard to risk preferences.
Risk preference is a characteristic that inﬂuences decision-making under uncertainty 3 . It reﬂects an implicit belief about
how to evaluate and manage risk 4 . Through social learning mechanisms such as observing and interacting with others in risky
situations, we can infer another person’s risk preference 5 . For example, within the context of a two-alternative gambling task,
individuals who consistently choose options with higher potential rewards despite greater uncertainty might exhibit a greater
risk-taking attitude during this decision-making process.
While observing a peer’s choices in situations involving risk can inﬂuence our own risk preferences 6, 7 , these choices do not
always provide a complete picture of their underlying risk preference. This raises the question of whether and how individuals
infer others’ true risk preferences during social observation, despite the inherent challenge of limited observations.
Moreover, inferring underlying risk preferences poses a challenge due to their non-observability . Generative models offer a
framework for representing how this characteristic might generate observable choices 2, 8, 9 . However, these models face the
challenge of capturing the true characteristics on the basis of limited observations 10 .
In the context of social learning, active inference provides a framework for using these generative models and social
feedback to actively update our inferences about someone’s characteristics (see 8, 11– 13 for some examples). In this context,
social prediction errors, the discrepancies between our predictions and their actual choices, serve as crucial signals for this
ongoing process 14 . By actively revising our models on the basis of this feedback, we continuously reﬁne our understanding of
others’ risk preferences during social observation.
Our modeling framework is built on active inference due to its advantages that align perfectly with our question, making
it an ideal choice. For instance, active inference aims to minimize prediction error by constantly updating its models 9 . This
allows active inference agents to learn even in the absence of explicit rewards, making it particularly suitable for studying social
inference, where rewards might not always be readily available. Moreover, in contrast to reinforcement learning -a famous
alternative framework- , active inference offers a solution to the exploration-exploitation trade-off by taking into account the
agent’s uncertainty 9 .
This research investigates how individuals infer another person’s risk preferences during social interaction, with a speciﬁc
focus on the challenge of limited observations. W e utilize an active inference framework within an online scenario. Participants
acting as observers predict and observe another person’s decisions under risk (high-risk gambles with potentially larger rewards
vs. guaranteed smaller rewards). Crucially , this online scenario allows for trial-by-trial feedback, enabling observers to update
their beliefs about the observed person’s risk preference based on the discrepancies between predictions and actual choices.
By employing active inference and its focus on minimizing prediction error, this research design allows us to examine how
individuals reﬁne their understanding of another person’s risk preferences as they gather more information.
Inferring an individual’s risk preference from their choices is a challenging task in social inference for several primary
reasons. First, risk preferences are typically characterized by nonlinear utility functions 15 , which quantify how individuals
assess gambles on the basis of their potential rewards and probabilities. The nonlinear nature of these utility functions
poses mathematical difﬁculties when inferring a subject’s risk attitude from a limited set of observed choices, especially in a
trial-by-trial setting.
Second, in our employed task, the gambling options presented to the decision maker varied from trial to trial. This
introduced a new level of complexity for observers trying to infer the decision maker’s risk attitude. Owing to the variability in
the nature of gambles, the decision maker may appear to switch between accepting and rejecting the gamble, even though their
risk attitude remains consistent. For instance, let us consider a decision maker with a high risk attitude who is expected to
choose gambling trials in most of the trials. However, when presented with a low-value gamble that offers a very low reward,
this person may choose not to gamble owing to the weakness of the proposed option, despite their willingness to take risks.
Consequently , observers may make incorrect predictions during these switching trials, leading to high prediction error. This
presents the challenge of whether to update the inferred risk attitude or prioritize maintaining the accuracy of the inferred value.
This challenge becomes more pronounced in the early trials because initially , the observer has no knowledge of the true value
of the decision maker’s risk attitude, and these switches further confuse them.
In the theoretical results section, we demonstrate how the framework of active inference, which is rooted in Bayesian theory ,
addresses these challenges associated with the social inference of risk preferences. Speciﬁcally , we show that the combination
of a generative model for decision-making and V ariational Bayes approximation tools effectively handles the non-linearity
challenge and enables learning of risk attitudes in the presence of varying gambling options.
In that section, we provide a step-by-step mathematical explanation of how we incorporate the problem into the active
inference framework. Additionally , we demonstrate the use of V ariational Bayes to effectively handle the non-linearity
challenge.
After performing the mathematical calculation, we obtain a computational model that functions in the following manner:
it begins with an initial, uncertain estimate of the partner’s risk attitude ρ . After each trial, the model adjusts its posterior
expectation of the partner’s risk attitude by taking into account the social prediction error, which measures the disparity between
the predicted choice and the partner’s actual observed choice.
Next, to validate our theoretical ﬁndings, we conduct simulation analyses across different scenarios to evaluate our model’s
performance in inferring risk preferences. Finally , to compare the performance of our model with that of the alternative
reinforcement learner, we set up a comparison using real subject data from the experiment conducted by 6 .
Results
Theoretical Results
In this section, we mathematically illustrate how the active inference framework can be employed to model an observer who
progressively grasps the risk attitude of another individual in real time . The observer infers the decision-maker’s risk attitude
as an internal state, assuming that choices are inﬂuenced by internal cognitive processes. This approach seeks to understand the
underlying mental processes that drive decision-making, rather than solely focusing on ﬁtting a model to observed behavior. T o
better understand our observer model, it is important to acknowledge the key assumptions underlying it. First, we assume that
the decision maker adheres to a speciﬁc set of mathematical rules outlined in the methods section . Second, we assume that the
observer incorporates this rule structure when developing a generative model for the observed individual. Finally , the observer
employs the active inference approach 16 .
Active inference framework facilitates agent learning in dynamic, changing circumstances 16 . It assumes a generative model,
which includes explicit probabilistic assumptions about how observed behavior are made 17 . The agent creates this generative
model of the environment to reduce surprises. Surprise is the measure of how unexpected an outcome is 9 .
Active inference framework is rooted in the free-energy principle 14 . Within this framework, we can optimize the generative
model on a trial-by-trial basis by minimizing the free energy functionals, leading to a credible learning process which is
compatible with online learning settings 9 and can be conceptualized as an approximate Bayesian belief-updating process 2 . It is
2/17
based on the idea that humans perceive sensory input as a statistical inference machine 18 , and that cognition can be described as
probabilistic Bayesian inference 19, 20 which is also known as the Bayesian brain hypothesis 18, 21– 24 .
Although the inversion of the generative model via Bayes’ rule provides optimal estimates of the posterior densities of
internal variables, in complex situations, exact inversion is generally unattainable. Therefore, algorithmic inference requires
additional approximations 25 , which are achieved via the V ariational Bayesian method 26 .
Model inversion
Generally , generative models consist of a joint probability density of all states. Generative models are created by combining
likelihood functions and state priors.
P
(
C(k),ρ (k)
⏐
⏐
⏐
β ,p(k),r(k)
)
= P
(
C(k)
⏐
⏐
⏐
ρ (k),β ,p(k),r(k)
)
× P
(
ρ (k)
⏐
⏐
⏐
β ,p(k),r(k)
)
(1)
In Eq (1), the term P
(
C(k)
⏐
⏐
⏐
ρ (k),β ,p(k),r(k)
)
is the likelihood function which is deﬁned by Eqs. (24), (25), (26) and (27)
(see Method section ).
A model inversion problem involves ﬁnding the term P
(
ρ (k)
⏐
⏐
⏐C(k),
β ,p(k),r(k)
)
which describes the posterior probability of
the state ρ (k) that is of interest. As opposed to likelihood, which predicts choice from hidden parameters and states, posterior
density of the state predicts the desired state from what choices have been observed. However, an analytical solution to the
exact Bayesian model inversion is not feasible for our problem, so we should rely on some fast but sufﬁciently accurate
approximations.
Variational energies
The ﬁrst step in this process is to incorporate what is known as the mean-ﬁeld approximation, which transforms the joint
distribution into the product of marginal posterior distributions 27 :
P
(
C(k),ρ (k)
⏐
⏐
⏐
β ,p(k),r(k)
)
= ˆq(C(k)) × ˆq(ρ (k)) (2)
With the help of mean-ﬁeld approximation, marginal posterior distributions can be expressed as follows in terms of V ariational
energy I(·)27 :
ˆq(C(k)) = 1
ZC
exp
[
I
(
C(k)
)]
,I(C(k)) =
⣨
ln P
(
C(k),ρ (k)
)⟩
q(ρ (k))
ˆq(ρ (k)) = 1
Zρ
exp
[
I
(
ρ (k)
)]
,I(ρ (k)) =
⣨
ln P
(
C(k),ρ (k)
)⟩
q(Ck )
(3)
where,
⟨ f (x)⟩q(x)
def=
∫
q(x) f (x)dx (4)
In Eq ( 3), the normalization factors ZC and Zρ are used to ensure that the total probability equals one.
T o model the binary nature of the choice variable C(k), we select the Bernoulli distribution as it is the appropriate distribution
for single binary events:
ˆq(C(k)) ∼ Bernoulli
(
C(k); µ (k)
C
)
(5)
For the continuous variable ρ (k), we choose the Gaussian distribution because of its well-understood properties and its
ability to capture a wide range of possible values:
ˆq(ρ (k)) ∼ N
(
ρ (k); µ (k)
ρ ,σ (k)
ρ
)
(6)
Having determined the form of the distributions, the inverse problem is relegated to identifying moments in the distributions
that have been represented by µ (k)
C (Eq(5)), µ (k)
ρ , and σ (k)
ρ (Eq(6)). Identifying these moments involves two recursive steps:
3/17
I) During the prediction step, a decision C(k) is predicted on the basis of previously updated posterior beliefs regarding the
agent’s attitude toward risk. This is accomplished by estimating the moment of choice distribution ( µ (k)
C ).
II) After observing the value of C(k) represented by C(k)
o , the update step is taken. In this step, given the current choice data,
the observer computes an updated posterior belief for ρ (k), which includes the updated mean µ (k)
ρ and the updated variance
σ (k)
ρ .
I) Prediction Step
Starting from Eq(
3), the V ariational energy I(C(k)) can be expanded as follows:
I(C(k)) =
⣨
ln P
(
C(k),ρ (k)
)⟩
q(ρ (k))
=
⣨
ln P
(
C(k)
⏐
⏐
⏐
ρ (k)
)⟩
q(ρ (k))
+
⣨
ln P
(
ρ (k)
)⟩
q(ρ k )
(7)
Because
⣨
ln P
(
ρ (k)
)⟩
q(ρ k )
does not depend on C(k), Eq ( 7) can be simpliﬁed as follows:
I(C(k)) =
⣨
ln P
(
C(k)
⏐
⏐
⏐
ρ (k)
)⟩
q(ρ (k)) (8)
If we insert P
(
C(k)
⏐
⏐
⏐
ρ (k)
)
(Eq(27)) into Eq ( 8), we get:
I(C(k)) =
⣨
C(k) ln S
(
F (ρ (k))
)
+ (1 −C(k))ln
(
1 − S
(
F (ρ (k))
))⟩
q(ρ (k))
= C(k)
⣨
ln S
(
F (ρ (k))
)⟩
q(ρ (k))
+ (1 −C(k))
⣨
ln
(
1 − S
(
F (ρ (k))
))⟩
q(ρ (k))
(9)
There is no closed form analytical solution for integrals in Eq (9). Additionally , remember that ρ is updated in accordance
with C(k); at the time when we wish to estimate C(k), the last updated moments of ρ would be those values obtained from the
previous trial ( µ (k−1)
ρ ,σ (k−1)
ρ ). Therefore, it seems reasonable to approximate the integrals via µ (k−1)
ρ in place of ρ (k). This
results in the subsequent structure for ˆ q(C(k)):
ˆq(C(k)) = 1
ZC
exp
[
I
(
C(k)
)]
=
(
S
(
F (µ (k−1)
ρ )
)) C(k)
×
(
1 − S
(
F (µ (k−1)
ρ )
)) 1−C(k) (10)
As a result, Eq( 10) naturally has the Bernoulli distribution form, where:
µ (k)
C = S
(
F (µ (k−1)
ρ )
)
σ (k)
C =
[
S
(
F (µ (k−1)
ρ )
)]
×
[
1 − S
(
F (µ (k−1)
ρ )
)] (11)
The parameter µ (k)
C = S
(
F (µ (k−1)
ρ )
)
determines the shape of ˆq(C(k)) and provides the most accurate estimate regarding the
agent’s choice at trial k, C(k).
II) Update Step
Having observed the value of C(k) = C(k)
o , it is now time to modify the model parameter ρ (k) in accordance with the new
observed value. W e begin by rewriting the deﬁnition of variational energy for ρ (k) as follows:
I(ρ (k)) =
⣨
ln P
(
C(k),ρ (k)
)⟩
q(Ck )
=
⣨
ln P
(
C(k)
⏐
⏐
⏐
ρ (k)
)⟩
q(Ck )
+
⣨
ln P
(
ρ (k)
)⟩
q(Ck )
(12)
4/17
C(k) is an observable variable, and when the observer wishes to update the estimate of ρ , the value of C(k) = C(k)
o is readily
available. Therefore, it is acceptable to substitute C(k) with C(k)
o in Eq (12). As a result, the expectation operator is simply
cancelled:
I(ρ (k)) =ln P
(
C(k)
o
⏐
⏐
⏐
ρ (k)
)
+ ln P
(
ρ (k)
)
(13)
According to Eq (27), P
(
C(k)
o
⏐
⏐
⏐
ρ (k)
)
represents the likelihood function, and P
(
ρ (k)
)
represents the prior knowledge of ρ ’s
distribution. Once we have placed the prior distribution and likelihood into Eq ( 13), we have the following equation:
I(ρ (k)) =Ck
o × ln
[
S
(
F (ρ (k))
)]
+ (1 −C(k)
o ) × ln
[
1 − S
(
F (ρ (k))
)]
−
(
ρ (k) − µ (k−1)
ρ
) 2
2σ (k−1)
ρ
(14)
If we simplify the expression (see Appendix S1), we will arrive at the following result:
I(ρ (k)) =ln
[
S
(
F (ρ (k))
)]
+ β × (C(k)
o − 1) × F (ρ (k)) −
(
ρ (k) − µ (k−1)
ρ
) 2
2σ (k−1)
ρ
(15)
Considering that the obtained equation (Eq (15)) does not have a quadratic form with respect to ρ (k), it cannot be used as an
exponent of a Gaussian distribution. Applying a quadratic approximation to the V ariational energy I(ρ (k)) (see Appendix S2 in
the supplementary information ﬁle), we arrive at the following equations for updating µ (k)
ρ and σ (k)
ρ :
µ (k)
ρ = µ (k−1)
ρ + β ×


∂ F (ρ (k))
∂ ρ (k)
⏐
⏐
⏐
⏐
⏐
ρ (k)=µ (k−1)
ρ

×
σ (k)
ρ ×
(
C(k)
o − S
(
F (µ (k−1)
ρ )
))
(16)
1
σ (k)
ρ
= 1
σ (k−1)
ρ
+ β 2 ×


∂ F (ρ (k))
∂ ρ (k)
⏐
⏐
⏐
⏐
⏐
ρ (k)=µ (k−1)
ρ


2
×
[
S
(
F (
µ (k−1)
ρ )
)]
×
[
1 − S
(
F (µ (k−1)
ρ )
)]
(17)
0.0.1 Update Equations: structural interpretation
Eq (16) clearly shows that inverting the model via the V ariational Bayes approximation provides straightforward rules for
updating µ (k)
ρ on a trial-by-trial basis that are computationally feasible:
µ (k)
ρ
post erior(k)
= µ (k−1)
ρ
post erior(k−1)
+ α (k)

learning rat e(k)
× δ (k)

predict ion error(k)
(18)
According to our model, the prediction error δ (k) refers to the difference between the actual choice C(k)
o and the prediction
µ (k)
C = S
(
F (µ (k−1)
ρ )
)
made prior to the actual choice C(k)
o being observed:
δ (k) = C(k)
o − µ (k)
C (19)
By introducing δ (k) into Eq ( 16) and then comparing it to Eq ( 18), we can derive the expression for the time-varying learning
rate α (k):
α (k) = β × γ (k) × σ (k)
ρ (20)
In Eq (20), γ (k) is the derivative of the function F (ρ (k)) (Eq (25)) relative to ρ (k) at point ρ (k) = µ (k−1)
ρ . γ (k) is inﬂuenced by
both the probability p(k) and reward r(k) of the gamble proposed at trial k. At usual probabilities and rewards, γ (k) remains
positive (please refer to supplementary Information (appendix S3) for more details about γ (k) and refer to the Methods section
for more information about the gambles’ settings).
γ (k) = ∂ F (ρ (k))
∂ ρ (k)
⏐
⏐
⏐
⏐
⏐
ρ (k)=µ (k−1)
ρ
(21)
5/17
In addition, β is also a non-negative term (see Methods Section). Note that since β is not provided to the observer, it is
estimated by ﬁtting to the entire set of choice data.
Additionally , the learning rate α (k) varies depending on the level of the positive measure of variance σ (k)
ρ . Considering
that σ (k)
ρ represents the variance of posterior distribution, it is plausible that the learning rate α (k) is proportional to σ (k)
ρ . It is
expected that upcoming observations will be more impactful on an observer who is less certain of what he or she knows.
Eq ( 17) gives us σ (k)
ρ , which can be rewritten in the following manner:
1
σ (k)
ρ
= 1
σ (k−1)
ρ
+ ˚ β 2 × (γ (k))
2
× σ (k)
C (22)
σ (k)
C (see Eq (11)) represents the degree of uncertainty in the choice prediction. W e can see that the amount of σ (k)
ρ is
inversely proportional to the amount of σ (k)
C . As long as the observer is conﬁdent about his prediction ( σ (k)
C = 0 ), σ ρ (k) remains
the same, unless σ ρ (k) decreases trial to trial. Notably , as a variance of the Bernoulli distribution, σ (k)
C is non-negative and has a
peak value of 0.25.
Owing to the recursive nature of Eq (22), σ (k−1)
ρ can be replaced by the value speciﬁed in the equation. This process can be
repeated for σ (k−2)
ρ as follows:
1
σ (k)
ρ
=
(
1
σ (k−2)
ρ
+ β 2 × (γ (k−1))
2
× σ (k−1)
C
)
+ ˚ β 2 × (γ (k))
2
× σ (k)
C
= 1
σ (0)
ρ
+ β 2 ×
(
(γ (1))
2
× σ (1)
C + (γ (2))
2
× σ (2)
C + ··· + (γ (k))
2
× σ (k)
C
)
= 1
σ (0)
ρ
+ β 2 ×
k
∑
i=1
(γ (i))
2
× σ (i)
C
(23)
Eq ( 23) clearly shows that σ (k)
ρ is inversely proportional to the sum of σ (i)
C weighted by (γ (i))2 .
Considering all of the above theoretical results, it seems that the predictive uncertainty σ (k)
C keeps the learning rate lower
than otherwise. This is understandable since the prediction error should be less magniﬁed when the predictions are less certain.
In summary , with each new trial, the inference process begins by making a prediction of the agent’s choice µ (k)
C . Once
the actual choice of the agent C(k)
o is observed, the social prediction error δ (k) comes into play , capturing any discrepancies
between the predicted and actual choices. The prediction uncertainty σ (k)
C is computed in the next step, and the accumulation of
this uncertainty gives rise to what is known as inferential uncertainty σ (k)
ρ . Next, the learning rate α (k) is dynamically adjusted
on the basis of the level of uncertainty in the inference process, allowing for more adaptive and accurate model updates. Once
the learning rate α (k) is determined, the posterior expectation of the risk attitude µ (k)
ρ is updated at the end by incorporating the
prediction error δ (k) and learning rate α (k), as described in Eq ( 18).
Simulation results
Each run of simulation contains a couple of blocks used in 6 (see Method section). Figure 1 illustrates the computational
trajectories of the learning risk attitude ρ for ﬁve simulated agents. The agents were designed to emulate a range of risk
preferences and behaviors. The simulated agents are characterized by their risk attitudes ρ , the values of which are outlined
below:
ρ = {0.85,0.95,1,1.05,1.15}
T o ensure that the task is at an appropriate level of difﬁculty for the learners, β was set to 1, so that the agent’s actions were
neither too predictable nor too unpredictable. This balance allows the learner to be able to learn successfully , while still facing
a challenging task. In the upcoming sections of the simulation section, we will examine quantitatively how β value affects
learning (see Figure 4).
Figure 1 depicts the Bayesian learning process in the structure of reinforcement learning model, with each panel corre-
sponding to one element of Eq (18). The bottom panel illustrates the observed choices C(k)
o and associated choice prediction
errors δ (k), the middle panel depicts the dynamic learning rate α (k), and the top panel presents the posterior expectation of the
parameter of interest µ (k)
ρ .
As shown in the bottom panel of Figure 1, the simulated agent with the least willingness to take risks ( ρ = 0.85) consistently
turned down the offers. As the risk attitude was increased from its minimum to its highest ( ρ = 1.15), the simulated agents
6/17
gradually became more likely to take risks. This trend reafﬁrms the presumption that agents who are more disposed to risk are
more likely to gamble. Remarkably , the agent with the highest risk attitude accepted the gambles in the majority of trials, with
only rare exceptions. Through these simulations, we can deeply examine the capacity of the learning model to adapt to a variety
of situations in numerous ways.
When the learner’s predictions µ (k)
C match the decision-maker’s choice C(k)
o , the posterior expectation of risk attitude µ (k)
ρ
remains the same. On the other hand, when the learner’s predictions are at odds with the decision-maker’s choice, the posterior
expectation is impacted accordingly . If the learner underestimates the decision-maker’s risk attitude ( δ (k) > 0), the expected
risk attitude increases; contrarily , if the learner overestimates the decision-maker’s risk attitude( δ (k) < 0), the expected risk
attitude decreases.
The Bayesian learner initially posits a prior mean and prior variance for the parameter of interest: µ (0)
ρ = 1, σ (0)
ρ = 0.01.
The Bayesian learner employs a dynamic learning rate technique that starts with a broad exploration of the parameter space
and gradually transits to a more focused exploration of the parameter space as trials progress. As expressed through Eq (18),
the learning rate α (k) governs the impact of prediction errors δ (k) on the model expectations. Through the course of the trials,
the learning rate α (k) is reduced, allowing the learner to build on the knowledge gained from earlier trials and become less
responsive to new data.
W e depicted the y-axis of middle panel on a log scale since the learning rate α (k) in the starting trials was much greater than
that in the last trials. In the ﬁnal trials, a low learning rate does not allow the model to signiﬁcantly alter its expectations in a
signiﬁcant way , even with high prediction error signals. This is because the model is conﬁdent that it has almost found the
exact value of risk attitudes and that any discrepancies in the later trials are likely just noise. This approach is advantageous for
our risk attitude learning problem as it is assumed that the underlying parameter of interest ρ is unchanged.
Figure 1. Computational trajectories of learning risk attitudes ρ for 5 simulated agents. Risk attitude values:
ρ = {0.85,0.95,1,1.05,1.15} and β = 1. Each column corresponds to one agent. T wo blocks (56 trials) make up each
simulation run for each agent. T op:-blue line: Posterior expectation µ (k)
ρ of risk attitude ρ . The regions within one standard
deviation of the mean
√
σ (k)
ρ were shaded in blue. The mild green area is indicative of a risk-taking attitude ( ρ > 1), while the
mild red area is indicative of a risk-averse attitude( ρ < 1). The black dot line indicates neutrality ( ρ = 1). Middle: -orange
line: Graph representing the Baysian dynamic learning rate α (k) modulating the impact of choice prediction errors δ (k) on
µ (k)
ρ .The y-axis is shown in log scale. Bottom: gray dot: simulated agent choices C(k)
o . red bar: Prediction error δ (k), the
difference between the actual choice C(k)
o and the prediction µ (k)
C = S
(
F (µ (k−1)
ρ )
)
.
The top panel in Figure 2 illustrates that there is a close link between learning rates α (k) and the degree of uncertainty in
posterior expectations σ (k)
ρ . This is evident from Eq (20), which demonstrates that α (k) and σ (k)
ρ have a linear relationship.
Consequently , a reduction in learning rate is coupled with a decrease in σ (k)
ρ . As the trials advanced, the model’s risk attitude
7/17
estimates became increasingly certain, as the variance of the posterior expectation of risk attitude decreased. In addition, the
learning rate trajectory showed variations that could not be explained by the variance of the risk attitude, but rather by another
factor, γ (k) (as depicted in Supplementary Figure 1 and described by Eq ( 20)).
When the model felt sure of its prediction ( σ (k)
C = 0), the σ (k)
ρ was kept unchanged to maintain a high learning rate. This
was because if the prediction was incorrect but the model was still highly conﬁdent, it had to be adjusted according to the
error signal that was received. As the model’s certainty in prediction decreased, the σ (k)
ρ decreased in relation to the prediction
uncertainty , and experienced the most drastic decrease when the uncertainty was at its highest ( σ Cmax = 0.25) .The result of the
simulation was in agreement with Eq ( 22), showing that the theoretical and simulated results were consistent
Figure 2. The relationship between the rate of learning and the model’s uncertainties. T op:-orange line (right axis):
Learning rate α (k). -purple line (left axis): V ariance of the posterior expectation σ (k)
ρ . The y-axis is scaled in logarithmic units
on both sides. Middle: -purple line (right axis): V ariance of the posterior expectation σ (k)
ρ . The y-axis is shown in log scale.
black bar (left axis): The bar chart illustrates the collective uncertainty of the learning model in predicting decisions made by
the agent. Bottom: The chart displays the model’s uncertainty in predicting agent’s choices σ (k)
C . The value is conﬁned to a
positive number that is less than one-fourth (0 ≤ σ (k)
C ≤ 0.25), as it reﬂects the variance of a Bernoulli distribution employed to
model predictions. The upper bound σ max is represented by a black dotted line.
Figure 3 illustrates the trajectory of σ (k)
ρ and α (k) across thousands of trials. This graph, with both the x- and y-axes in
log scale, shows that there is a negative linear relationship between σ (k)
ρ and trial number k in log space. In other words,
the logarithm of σ (k)
ρ decreases linearly with the increase of logarithm of trial number k. As the trials continue, the σ (k)
ρ
8/17
decreases until it converges to zero, but at a slower and slower pace. Consequently , the learning rate α (k) follows the same
path as σ (k)
ρ , meaning that the learning process persists even after many trials, albeit at a very slow rate. The ﬁtted line for
the log-transformation of the σ (k)
ρ data points intersects the y-axis at -2, indicating that the initial value σ (0)
ρ was 0.01, as set
initially .
Figure 3. The ﬁgure demonstrates the convergence of σ (k)
ρ and α (k) trajectories towards a stable equilibrium throughout
numerous simulation trials. The line Y=-X-2 serves as an asymptotic line for σ (k)
ρ , depicting the pattern of variation of σ (k)
ρ
over the course of numerous trials. both x and y axis depicted in log scale.
In Figure 4, β is a parameter that controls how random an agent’s decisions are (see Method section ). Near-zero values of
β producing completely unpredictable choices ( σ (k)
C = 0.25). As the level of β rises, the amount of uncertainty in the prediction
lessens, which indicates that less random choices are more likely to be accurately predicted by the learner. In the most extreme
case, σ (k)
C tends to approach zero at the highest values of β , where the agent chooses in a predictable fashion, demonstrating
that deterministic choices are the easiest to comprehend.
Subsequently , we ran a simulation to explore the impact of β on σ (k)
ρ and α (k). The upper panel of Fig 4 illustrates the
outcome of the simulation. σ (k)
ρ and β have an inversely proportional relationship according to Eq (22), which shows that when
β increases, σ (k)
ρ decreases. Furthermore, β ’s inﬂuence on σ (k)
C (bottom panel) also impacts σ (k)
ρ in a complex way: as β rises,
σ (k)
C decreases, which in turn causes σ (k)
ρ to increase (Eq 22). Simulations indicate that when these two opposing forces are in
contention, σ (k)
ρ ultimately diminishes when β increases. Turning next to α (k), Eq (20) elucidates its direct relationship to both
σ (k)
ρ and β . So too, we see an opposition between two forces here. However, the simulations indicate that as β increases, the
opposing forces between σ (k)
ρ and β remain in equilibrium, so the effect on α (k) is minimal. This indicates that while β has a
signiﬁcant impact on σ (k)
ρ , it does not have a signiﬁcant effect on learning rate α (k).
Model comparison
W e compared the Bayesian learner to the reinforcement learner model. This RL model adjusts estimations through a prediction
error weighted by a preset learning rate, whereas the Bayesian model adjusts the learning rate at each trial, eliminating the
need for a predetermined learning rate. As each participant’s data are unique, the social RL learner’s accuracy is reduced if the
optimal learning rate is not found for each individual. Therefore, the Bayesian model only requires one adjustable parameter to
ﬁt β to each set of data, whereas the social RL model requires two adjustable parameters.
T o compare the two learning models, we utilized the data collected by Suzuki et al 6 . W e utilized the data from sessions
1, 3 and 5, in which the risk attitudes of the participants were assessed. Fig 5 displays the learning trajectories of one of the
participants involved in the experiment.
W e employed the BFGS optimization algorithm from the T AP AS (https://tnu.ethz.ch/tapas) 28 to determine the best
parameters for our models, which were then used to calculate the quality measures of the models. This optimization technique
9/17
Figure 4. effect of β on σ (k)
C , σ (k)
ρ and α (k). For each level of β , four blocks of twenty-eight trials were run, with the average
of each variable in the last block represented in the ﬁgure. Risk attitude ρ was 1 in all simulations. T op:-orange line:
Learning rate α (k). -purple line: V ariance of the posterior expectation σ (k)
ρ . The y-axis is scaled in logarithmic units. Bottom:
The model’s uncertainty in predicting agent’s choices σ (k)
C .
maximizes the log-joint posterior density across all parameters, based on the models’ trial-wise predictions. W e used the log
model evidence (LME) and the Bayesian information criterion (BIC) to measure the performance of the models. The Model
Evidence is essentially the probability of the data given the model, which we approximated with the Free Energy 29 . When the
best model is selected, the free energy performs better than the Akaike information criterion (AIC) or the BIC, as the latter two
focus solely on the number of parameters, rather than their covariance 30, 31 .
Figure 5 reveals that the Bayesian learner’s average learning rate ¯α BL is comparable to the ﬁxed learning rate of the
reinforcement learning model α RL , despite a slight downward trend. This accomplishment is attributed to the BL ’s ability
to implement a vague prior and adjust its learning rate according to its uncertainty . This systematic tuning of the learning
rate enables BL to achieve a similar accuracy score to that of RL, while only needing one ﬁtting parameter instead of two.
Furthermore, the Bayesian information criterion (BIC) and the log model evidence (LME) scores conﬁrm that BL with one
free parameter is superior to RL with two free parameters, even if the accuracy likelihood score is marginally lower. T able 1
provides a summary of the results of the models quality assessments.
W e implemented a random-effects Bayesian model selection (RFX-BMS) 29, 32 to compare the alternative models and
determine their relative strength. The posterior probability of each model was then used to calculate the exceedance probability
(EP) and protected exceedance probability (PEP) 29, 32 . W e leveraged the open-source VBA toolbox 33 to conduct the analysis
and the results indicated that the Bayesian learner was superior to the RL model in explaining the participants’ behavior (EF=
0.99, EP=1, PEP=1). Please refer to Figure 6.
Discussion
This research investigated how individuals infer another person’s risk preferences during social interaction, particularly under
the challenge of limited observations. W e employed a Bayesian approach within the framework of active inference. Active
inference allows agents to reﬁne internal models of their environment continuously on the basis of prediction errors. In this
context, the internal model represents the observer’s understanding of the decision-maker’s risk preference. The approach
at hand allows for the estimation of uncertainty , which plays a crucial role in driving exploration within the active inference
10/17
Figure 5. The ﬁgure displays the learning trajectories of both the reinforcement learner (RL) and Bayesian learning (BL)
models. It is based on the choice behavior of one subject from the study conducted by Suzuki et al.6 . The RL trajectories are
represented by the -light blue line and the BL trajectories are represented by the -dark blue line. In the top panel, the -dark
green line illustrates the maximum likelihood estimation (MLE) of the subject’s risk attitude, which was calculated separately
for each session. Here, we present the ﬁtted parameters for each model in each sessions. Additionally , we calculate the average
dynamic learning rate of the Bayesian learner to compare with the ﬁxed rate of the reinforcement learner. Session 1:
¯α BL = 0.08 ; α RL = 0.08 ; β BL = 1.45 ; β RL = 1.44 / Session 3: ¯α BL = 0.03 ; α RL = 0.04 ; β BL = 0.54 ; β RL = 0.55 / Session
5: ¯α BL = 0.06 ; α RL = 0.07 ; β BL = 0.77 ; β RL = 1.10
Figure 6. Bayesian Model Selection RL vs BL
framework.
W e propose a speciﬁc learning algorithm to model how observers update their beliefs about the decision-maker’s risk
preference. This algorithm offers a computationally efﬁcient approach for capturing belief updates on the basis of prediction
errors. In this context, the prediction error refers to the discrepancy between the observer’s prediction of the decision-maker’s
choice and the actual observed choice. This approach aligns with the core principle of active inference, where agents
continuously reﬁne their internal models on the basis of these prediction errors.
Previous research suggests that social learning can be understood through principles similar to those governing nonsocial
learning10, 34– 36 . These principles emphasize the role of prediction errors – the discrepancies between what we expect and what
we observe. In social interactions, these prediction errors can inform us about various social aspects, such as another person’s
generosity37 or their propensity to punish 38 . In our study , the learning algorithm within the active inference framework updates
the observer’s beliefs about the decision-maker’s risk preference on a trial-by-trial basis, on the basis of the discrepancies
between the model’s predictions and the observed choices.
Bayesian learning posits that estimation uncertainty , also called inferential uncertainty 39 , is the impetus for exploration 39, 40 .
Research on human learning demonstrated that such uncertainty plays a role in how humans learn and attain knowledge 41 . The
11/17
T able 1.Comparison of the goodness-of-ﬁt between the Bayesian learner and the Reinforcement learner. For the Bayesian
Information Criterion (BIC), the smaller the value, the better the ﬁt. The Log-likelihood (LL) and Log Model Evidence (LME)
both indicate better ﬁt with bigger values; # prms, number of free parameters. Each session’s best model is bolded.
session Model LL BIC # prms LME
session 1 Bayesian Learner −245.96 571 .89 1 -277.79
Reinforcement Learner −236.02 631 .98 2 −313.44
session 3 Bayesian Learner −173.69 427 .35 1 -200.82
Reinforcement Learner −180.71 521 .36 2 −259.29
session 5 Bayesian Learner −182.54 445 .05 1 -210.40
Reinforcement Learner −168.12 496 .19 2 −234.89
feeling-of-knowing, or subjective conﬁdence 42 , is believed to be associated with the inferential uncertainty that arises during
learning process 39 , leading to the idea of deﬁning conﬁdence as a Bayesian probability 42 . In this framework, the precision of
the distribution (its inverse variance) is seen as the manifestation of conﬁdence 43 . Furthermore, from a biological perspective, it
appears that the brain not only estimates a value, but also the level of certainty of that estimate, along with its entire probability
distribution23 . In our Bayesian risk attitude learner, σ ρ characterizes the inferential uncertainty of the model in estimating
other’s risk attitude. Mathematically , the conﬁdence level is formulated as a negative logarithm of variance in the distribution 39 .
In Figure 2, 3 and 4, we showed the trajectories of σ ρ using the log scale, which is the natural space for illustrating variance 44 ,
making the transition from variance to conﬁdence much simpler.
The Bayesian computational framework gives us the chance to investigate further how conﬁdent humans are when
learning others’ preferences, which can build upon prior research into human conﬁdence in areas such as memorization 45 ,
perceptual decision-making 46 , and probabilistic learning task 39 . In contrast, the reinforcement learner cannot express inferential
uncertainty; it provides only a single point estimate, without giving any indication of the reliability of the estimation.
In our theoretical analysis, we connected the learning rate to the uncertainty the model experiences when predicting an
agent’s choice. Additionally , we showed that the accumulated uncertainty the observer experiences when predicting the agent’s
decisions is what determines the width of the posterior distribution.
T o validate our theoretical ﬁndings, we simulated data to assess the observer’s capability to precisely guess the decision
maker’s risk attitude ρ . Furthermore, since simulation allows us to pick the number of trials, we conducted the simulations with
a larger than usual number of trials to thoroughly explore the properties we discovered in the theoretical section, particularly
concerning the connection between the learning rate and the three types of uncertainty: variance of posterior belief ( σ (k)
ρ ),
uncertainty in prediction ( σ (k)
C ) and decision maker randomness ( β ). The simulation results corroborated the hypothesis that the
learning rate α (k) was contingent on σ (k)
ρ and σ (k)
C , as indicated by the theoretical analysis. Furthermore, the simulation results
demonstrated that an increase in β has an inverse effect on both other types of uncertainty ( σ (k)
ρ and σ (k)
C ), while having no
impact on the learning rate α (k).
Active inference emphasizes the importance of exploration, particularly when faced with high uncertainty . This principle
aligns with human social learning research, which suggests a stronger initial learning rate when there’s greater uncertainty about
others’ behavior
35, 47– 49 . In our Bayesian model, this translates to a high initial learning rate to explore the full range of possible
risk preferences for the decision-maker. The model maintains this high learning rate as long as its conﬁdence in its predictions
remains high. This reﬂects the active inference principle of balancing exploration and exploitation based on uncertainty .
Our research compared a Bayesian learner with an uncertainty-based learning rate to a reinforcement learning model that
employs a constant learning rate. The Bayesian learner achieved superior model quality scores. This success can be attributed
to its ability to dynamically adjust its learning rate based on the inherent uncertainty in its estimates of the decision-maker’s
risk preference. This dynamic learning rate, aligned with the core principles of active inference, allows the Bayesian learner to
effectively balance exploration and exploitation during the learning process. In contrast, the reinforcement learning model
utilizes a ﬁxed learning rate, which requires careful pre-tuning or additional ﬁtting procedures to achieve optimal performance.
Here, the Bayesian learner offers a more ﬂexible and data-driven approach to learning, eliminating the need for separate
parameter ﬁtting for the learning rate.
The Bayesian learner’s dynamic learning rate effectively captures the essence of human sequential learning from examples.
It starts high, prioritizing exploration of different risk-preference possibilities for the decision-maker. As the observer gathers
more data, the learning rate gradually decreases, reﬂecting a shift towards exploiting the accumulated knowledge. This aligns
with the well-established concept that conﬁdence increases with the number of observations made during consistent behavior 39 .
In contrast, the reinforcement learning model, while achieving good accuracy with a learning rate similar to the Bayesian
learner’s average, is limited by its inability to capture this dynamic exploration-exploitation behavior. Humans tend to prioritize
12/17
exploration when faced with high uncertainty , a key aspect of social learning. A ﬁxed learning rate model cannot dynamically
adjust its learning style based on uncertainty , potentially leading to a less accurate representation of human behavior in this
speciﬁc task.
Social learning necessitates the ability to interpret information from others. Generative models, like those employed in
active inference, offer a powerful tool for achieving this. They allow learners to go beyond simply observing actions and infer
the underlying cognitive processes that generate those actions, leading to a richer understanding of the observed behavior.
Our research contributes to the ever-growing body of evidence which suggests that inverting generative models is a vital
computational tool for social inference 50 . Studies examining this phenomenon have implemented a range of formal assumptions,
such as those conducted by 11, 51– 54 . While each of these studies have their own distinct formal assumptions, they all lead to the
same conclusion that generative models are powerful psychological tools for social inference.
This study focused on establishing the core functionalities of our model using a well-established dataset 6 . This approach
facilitated a clear understanding of the model’s behavior without introducing the complexities of incorporating data with
potentially different experimental designs from the outset. W e acknowledge the importance of future research incorporating
data from diverse datasets, such as those used in studies by Russ Poldrack e.g., 55 , to enhance the model’s generalizability and
test its applicability across a wider range of risky decision-making tasks.
Methods
Simulation T ask
T o acquire intentions from actions, there need to be reliable perceptual differences between actions performed with different
intentions, and observers must be able to detect and utilize these differences to make judgments about the actors’ intentions 56 .
T o ensure reliable results, we chose the most suitable experimental task based on our knowledge, as suggested by 6 .
In each simulation run, there are two blocks, each with 28 trials; therefore, there are 56 trials in total. The order in which
trials are presented is random. In each trial, artiﬁcial agents are required to decide whether to accept or reject a risky gamble.
The gamble has two possible outcomes: receiving a certain reward or not receiving it. The result of rejecting a gamble is a
ﬁxed guaranteed payment. As a general rule, the reward of a gamble is greater than the payment that is guaranteed. Essentially ,
gambles are characterized by two parameters: p and r, with p being the probability and r being the amount of the reward. Each
of the 28 trials had its own combination of reward probability p and reward magnitude r. There were three possibilities for
reward probabilities: 0.3, 0.4, and 0.5. W e modiﬁed the actual reward magnitude in each trial by adding a small integer noise,
ranging from -1 to 1. Guaranteed payments are ﬁxed at 10 dollars (Figure 7).
(a)
 (b)
Figure 7. (a) Schematic illustration of a gambling game. The reward probability p and magnitude of the reward r are
presented in the form of a pie chart. In the pie chart, a blue area indicates the likelihood of receiving a reward, and a gray area
indicating the likelihood of receiving nothing. The amount of the reward is displayed within the blue area, which in this
example is 25$. (b) A set of gambles employed in the simulation. Points correspond to gambles, which are described by
reward probability p and reward magnitude r. The red color code is indicative of gambles that risk-neutral individuals prefer to
certain payments, and the blue color code is indicative of gambles risk-neutral individuals do not prefer to certain payments.
The yellow graph illustrates a curve of indifference when risk-neutral attitudes are adopted, in which a gamble is just as
valuable as a certain payment of ten dollars. As shown on the right side of the plot, there are two distinct points. These points
correspond to two risk-free gambles that were used in this experiment.
13/17
Computational model of decision making under risk
Artiﬁcial agent choices were generated via a mathematical algorithm. W e assumed that the simulated agents would make
decisions on the basis of the computational model described here. The model is widely used to study decision making under
risk3, 5, 57– 60 .
Let us imagine that the decision-maker is faced with a choice between gambling or receiving a guaranteed monetary
payment. Choosing between options requires the decision maker to calculate subjective expected values.This evaluation may
differ depending on the decision maker’s attitude toward risk. According to one of the most common approaches based on a
power utility function 15, 61, 62 , the utility of the risky option is denoted by UR as follows:
UR (r,p) =p × rρ (24)
In Eq (24), p is the probability of receiving a reward and r is the magnitude of the reward. W e do not use small probabilities
(p < 0.3), so the subjective probability distortion 61 is not relevant to our analysis. ρ refers to the participant’s risk attitude,
which is the curvature of the utility function. Depending on whether the individual is risk-averse or risk-taking, ρ is lower or
greater than 1.
On the basis of the principles of classical decision theory and behavioral economics, people’s choices may reﬂect their
relative preferences for different outcomes 63 . T o decide between the risky option and the sure option, one must compare their
values via the function F (·) , where the US represents a certain payoff.
F = UR −US (25)
Generally , the greater the value of F (·) is, the greater the likelihood that the decision maker will prefer the risky option to
the certain option. Next, a probabilistic rule called Softmax S(·) calculates the probability of choosing a risky option over a
certain option.
S(F (ρ )) = 1
1 + ex p(−β × F (ρ )) (26)
In Eq (26), β is a non-negative free parameter that measures how much the choice probability relates to utility differences or
how random the subject’s decisions are. The probability of choosing a risky option will always be half if β is zero. Therefore,
the decision is made at random. Conversely , a large β suggests that the person frequently makes nonrandom decisions. The
S(·) value tends to be 1 when F (·) is large and positive. As a result, the decision-maker is certainly willing to choose the risky
option. In contrast, when F (·) is large and negative, S(·) tends to 0, and the decision-maker rejects the risky options.
Finally , a Bernoulli distribution is used to describe the choice of decision-maker, C. The value of C can be either 0 or 1.
One means that the decision-maker chooses the risky option, and zero means that he or she rejects it.
P(C|ρ ) = [S (F (ρ ))]C × [1 − S (F (ρ ))]1−C (27)
Code availability
All custom code and mathematical algorithm used for running simulations, model ﬁtting, and plotting are available on a OSF
repository at OSF link .
Human data
Our research used anonymized data from a prior study 6 that was approved by the Institutional Review Board of the California
Institute of T echnology . On that study , all participants provided written consent prior to participation 6 .
Acknowledgments
W e are thankful to Shinsuke Suzuki for providing us with access to the experimental dataset utilized in this study .
Author contributions statement
AHTS, ASJ, and RG conceived the study and developed the methodology . AHTS wrote the script, ran the simulations,
conducted formal analysis, and visualization, and wrote the initial draft of the manuscript. AHTS and ASJ performed the
investigation and validation, and all the authors contributed to reviewing and editing the manuscript. RG and ASJ oversaw the
project.
14/17
Additional information
The authors declare that they have no competing interests.
Data availability statement
All custom code and mathematical algorithms used for running simulations, model ﬁtting, and plotting are available on an
OSF repository at the OSF link . The dataset used for model comparison in this study was obtained from Suzuki et al6 . Due
to restrictions imposed by the data owners, this dataset is not publicly available. However, the data can be accessed upon
reasonable request and with permission from Suzuki et al.
References
1. Heyes, C. M. Social learning in animals: categories and mechanisms. Biological Reviews 69, 207–231 (1994).
2. Hawkins, R. D. et al. Flexible social inference facilitates targeted social learning when rewards are not observable. Nature
Human Behaviour 1–10 (2023).
3. Blankenstein, N. E., Crone, E. A., van den Bos, W . & van Duijvenvoorde, A. C. Dealing with uncertainty: T esting risk-and
ambiguity-attitude across adolescence. Developmental neuropsychology 41, 77–92 (2016).
4. Defoe, I. N., Dubas, J. S., Figner, B. & V an Aken, M. A. A meta-analysis on age differences in risky decision making:
adolescents versus children and adults. Psychological bulletin 141, 48 (2015).
5. Devaine, M. & Daunizeau, J. Learning about and from others’ prudence, impatience or laziness: The computational bases
of attitude alignment. PLoS computational biology 13, e1005422 (2017).
6. Suzuki, S., Jensen, E. L., Bossaerts, P . & O’Doherty , J. P . Behavioral contagion during learning about another agent’s
risk-preferences acts on the neural representation of decision-risk. Proceedings of the National Academy of Sciences 113,
3755–3760 (2016).
7. T ehrani-Safa, A. H., Ghaderi, R., Herassat, M. & Sarabi-Jamab, A. Peer-mediated social signals alter risk tolerance in
teenage boys depending on their peers. Basic and Clinical Neuroscience 15 (2024).
8. Jara-Ettinger, J., Gweon, H., Schulz, L. E. & T enenbaum, J. B. The naïve utility calculus: Computational principles
underlying commonsense psychology . Trends in cognitive sciences 20, 589–604 (2016).
9. Sajid, N., Ball, P . J., Parr, T . & Friston, K. J. Active inference: demystiﬁed and compared. Neural computation 33, 674–712
(2021).
10. FeldmanHall, O. & Nassar, M. R. The computational challenge of social learning. Trends in Cognitive Sciences 25,
1045–1057 (2021).
11. Baker, C. L., Jara-Ettinger, J., Saxe, R. & T enenbaum, J. B. Rational quantitative attribution of beliefs, desires and percepts
in human mentalizing. Nature Human Behaviour 1, 0064 (2017).
12. Thomas, L., Lockwood, P . L., Garvert, M. M. & Balsters, J. H. Contagion of temporal discounting value preferences in
neurotypical and autistic adults. Journal of autism and developmental disorders 1–14 (2022).
13. Diaconescu, A. O. et al. Inferring on the intentions of others by hierarchical bayesian learning. PLoS computational
biology 10, e1003810 (2014).
14. Lehmann, K., Bolis, D., Ramstead, M. J., Friston, K. & Kanske, P . An active inference approach to second-person
neuroscience (2022).
15. Bernoulli, D. Exposition of a new theory on the measurement of risk. Econometrica 22, 23–36 (1954).
16. Friston, K., FitzGerald, T ., Rigoli, F ., Schwartenbeck, P . & Pezzulo, G. Active inference: a process theory . Neural
computation 29, 1–49 (2017).
17. Behrens, T . E., W oolrich, M. W ., W alton, M. E. & Rushworth, M. F . Learning the value of information in an uncertain
world. Nature neuroscience 10, 1214–1221 (2007).
18. Dayan, P ., Hinton, G. E., Neal, R. M. & Zemel, R. S. The helmholtz machine. Neural computation 7, 889–904 (1995).
19. Schwartenbeck, P . et al. Optimal inference with suboptimal models: addiction and active bayesian inference. Medical
hypotheses 84, 109–117 (2015).
20. Geisler, W . S. & Diehl, R. L. Bayesian natural selection and the evolution of perceptual systems. Philosophical Transactions
of the Royal Society of London. Series B: Biological Sciences 357, 419–448 (2002).
15/17
21. Friston, K., Kilner, J. & Harrison, L. A free energy principle for the brain. Journal of physiology-P aris100, 70–87 (2006).
22. Friston, K. The free-energy principle: a uniﬁed brain theory? Nature reviews neuroscience 11, 127–138 (2010).
23. Knill, D. C. & Pouget, A. The bayesian brain: the role of uncertainty in neural coding and computation. TRENDS in
Neurosciences 27, 712–719 (2004).
24. Pouget, A., Beck, J. M., Ma, W . J. & Latham, P . E. Probabilistic brains: knowns and unknowns. Nature neuroscience 16,
1170–1178 (2013).
25. Piray , P . & Daw , N. D. A simple model for learning in volatile environments. PLoS computational biology 16, e1007963
(2020).
26. Daunizeau, J. et al. Observing the observer (i): meta-bayesian models of learning and decision-making. PloS one 5,
e15554 (2010).
27. Friston, K. J. & Stephan, K. E. Free-energy and the brain. Synthese 159, 417–458 (2007).
28. Frässle, S. et al. T apas: an open-source software package for translational neuromodeling and computational psychiatry .
Frontiers in psychiatry 12, 680811 (2021).
29. Stephan, K. E., Penny , W . D., Daunizeau, J., Moran, R. J. & Friston, K. J. Bayesian model selection for group studies.
Neuroimage 46, 1004–1017 (2009).
30. Zeidman, P ., Friston, K. & Parr, T . A primer on variational laplace (vl). NeuroImage 120310 (2023).
31. Penny , W . D. Comparing dynamic causal models using aic, bic and free energy . Neuroimage 59, 319–330 (2012).
32. Rigoux, L., Stephan, K. E., Friston, K. J. & Daunizeau, J. Bayesian model selection for group studies—revisited.
Neuroimage 84, 971–985 (2014).
33. Daunizeau, J., Adam, V . & Rigoux, L. Vba: a probabilistic treatment of nonlinear models for neurobiological and
behavioural data. PLoS computational biology 10, e1003441 (2014).
34. Burke, C. J., T obler, P . N., Baddeley , M. & Schultz, W . Neural mechanisms of observational learning. Proceedings of the
National Academy of Sciences 107, 14431–14436 (2010).
35. Behrens, T . E., Hunt, L. T ., W oolrich, M. W . & Rushworth, M. F . Associative learning of social value. Nature 456, 245–249
(2008).
36. van den Bos, W ., T alwar, A. & McClure, S. M. Neural correlates of reinforcement learning and social preferences in
competitive bidding. Journal of Neuroscience 33, 2137–2146 (2013).
37. Hackel, L. M., Doll, B. B. & Amodio, D. M. Instrumental learning of traits versus rewards: dissociable neural correlates
and effects on choice. Nature Neuroscience 18, 1233–1235 (2015).
38. FeldmanHall, O., Otto, A. R. & Phelps, E. A. Learning moral values: Another’s desire to punish enhances one’s own
punitive behavior. Journal of Experimental Psychology: General 147, 1211 (2018).
39. Meyniel, F ., Schlunegger, D. & Dehaene, S. The sense of conﬁdence during probabilistic learning: A normative account.
PLoS computational biology 11, e1004305 (2015).
40. Payzan-LeNestour, E. & Bossaerts, P . Risk, unexpected uncertainty , and estimation uncertainty: Bayesian learning in
unstable settings. PLoS computational biology 7, e1001048 (2011).
41. Nassar, M. R., Wilson, R. C., Heasly , B. & Gold, J. I. An approximately bayesian delta-rule model explains the dynamics
of belief updating in a changing environment. Journal of Neuroscience 30, 12366–12378 (2010).
42. Meyniel, F ., Sigman, M. & Mainen, Z. F . Conﬁdence as bayesian probability: From neural origins to behavior. Neuron 88,
78–92 (2015).
43. Y eung, N. & Summerﬁeld, C. Metacognition in human decision-making: conﬁdence and error monitoring. Philosophical
Transactions of the Royal Society B: Biological Sciences 367, 1310–1321 (2012).
44. Gelman, A. et al. Bayesian data analysis (2013).
45. Koriat, A., Sheffer, L. & Ma’ayan, H. Comparing objective and subjective learning curves: judgments of learning exhibit
increased underconﬁdence with practice. Journal of Experimental Psychology: General 131, 147 (2002).
46. Barthelmé, S. & Mamassian, P . Evaluation of objective uncertainty in the visual system. PLoS computational biology 5,
e1000504 (2009).
16/17
47. Apps, M. A., Rushworth, M. F . & Chang, S. W . The anterior cingulate gyrus and social cognition: tracking the motivation
of others. Neuron 90, 692–707 (2016).
48. Apps, M. A. & Sallet, J. Social learning in the medial prefrontal cortex. Trends in Cognitive Sciences 21, 151–152 (2017).
49. FeldmanHall, O. & Shenhav , A. Resolving uncertainty in a social world. Nature human behaviour 3, 426–435 (2019).
50. Jern, A., Lucas, C. G. & Kemp, C. People learn other people’s preferences through inverse decision-making. Cognition
168, 46–64 (2017).
51. Baker, C. L., Saxe, R. & T enenbaum, J. B. Action understanding as inverse planning. Cognition 113, 329–349 (2009).
52. Ullman, T . et al. Help or hinder: Bayesian models of social goal inference. Advances in neural information processing
systems 22 (2009).
53. T auber, S. & Steyvers, M. Using inverse planning and theory of mind for social goal inference. In Proceedings of the
annual meeting of the cognitive science society, vol. 33 (2011).
54. Jern, A. & Kemp, C. A decision network account of reasoning about other people’s choices. Cognition 142, 12–38 (2015).
55. T om, S. M., Fox, C. R., Trepel, C. & Poldrack, R. A. The neural basis of loss aversion in decision-making under risk.
Science 315, 515–518 (2007).
56. Catmur, C. Understanding intentions from actions: Direct perception, inference, and the roles of mirror and mentalizing
systems. Consciousness and cognition 36, 426–433 (2015).
57. Braams, B. R., Davidow , J. Y . & Somerville, L. H. Information about others’ choices selectively alters risk tolerance and
medial prefrontal cortex activation across adolescence and young adulthood. Developmental Cognitive Neuroscience 52,
101039 (2021).
58. T ymula, A. et al. Adolescents’ risk-taking behavior is driven by tolerance to ambiguity . Proceedings of the National
Academy of Sciences 109, 17135–17140 (2012).
59. Levy , I., Snell, J., Nelson, A. J., Rustichini, A. & Glimcher, P . W . Neural representation of subjective value under risk and
ambiguity . Journal of neurophysiology 103, 1036–1047 (2010).
60. Hsu, M., Bhatt, M., Adolphs, R., Tranel, D. & Camerer, C. F . Neural systems responding to degrees of uncertainty in
human decision-making. Science 310, 1680–1683 (2005).
61. Tversky , A. & Kahneman, D. Advances in prospect theory: Cumulative representation of uncertainty . Journal of Risk and
uncertainty 5, 297–323 (1992).
62. Kahneman, D. & Tversky , A. Prospect theory: An analysis of decision under risk. Econometrica 47, 263–292 (1979).
63. Samuelson, P . A. Consumption theory in terms of revealed preference. Economica 15, 243–253 (1948).
17/17
Supplementary Files
This is a list of supplementary  les associated with this preprint. Click to download.
Suppsrep.pdf