Chapter 1 
Introduction 
1.1 Motivation 
Robots manipulating and navigating in unmodelled environments need robust 
geometric cues to recover scene structure. Vision - the process of discovering 
fl'om images what is present in the world and where it is [144] - can provide 
some of the most powerful cues. 
Vision is an extremely complicated sense. Understanding how our visual 
systems recognise familiar objects in a scene as well as describing qualitatively 
the position, orientation and three-dimensional (3D) shape of unfamiliar ones, 
has been the subject of intense curiosity and investigation in subjects as disparate 
as philosophy, psychology, psychophysics, physiology and artificial intelligence 
(AI) for many years. The AI approach is exemplified by computational theories 
of vision [144]. These analyse vision as a complex information processing task 
and use the precise language and methods of computation to describe, debate 
and test models of visual processing. Their aim is to elucidate the information 
present in visual sensory data and how it should be processed to recover reliable 
three-dimensional descriptions of visible surfaces. 
1.1.1 Depth cues from stereo and structure from motion 
Although visual images contain cues to surface shape and depth, e.g. perspective 
cues such as vanishing points and texture gradients [86], their interpretation 
is inherently ambiguous. This is attested by the fact that the human visual 
system is deceived by "trompe d'oeuil" used by artists and visual illusions, e.g. 
the Ames room [110, 89], when shown a single image or viewing a scene from 
a single viewpoint. The ambiguity in interpretation arises because information 
is lost in the projection from the three~dimensional world to two-dimensional 
images. 
Multiple images from different viewpoints can resolve these ambiguities. Vis- 
ible surfaces which yield almost no depth perception cues when viewed from a 
single viewpoint, or when stationary, yield vivid 3D impressions when movement 

2 Chap. 1. Introduction 
(either of the viewer or object) is introduced. These effects are known as stereop- 
sis (viewing the scene from different viewpoints simultaneously as in binocular 
vision [146]) and kineopsis ( the "kinetic depth" effect due to relative motion 
between the viewer and the scene [86, 206]). In computer vision the respective 
paradigms are stereo vision [14] and structure from motion [201]. 
In stereo vision the processing involved can be decomposed into two parts. 
1. The extraction of disparities (difference in image positions). This involves 
matching image features that correspond to the projection of the same 
scene point. This is referred to as the correspondence problem. It concerns 
which features should be matched and the constraints that can be used to 
help match them [147, 10, 152, 171, 8]. 
. The interpretation of disparities as 3D depths of the scene point. This 
requires knowledge of the camera/eye geometry and the relative positions 
and orientations of the viewpoints (epipolar geometry [10]). This is essen- 
tially triangulation of two visual rays (determined by image measurements 
and camera orientations) and a known baseline (defined by the relative 
positions of the two viewpoints). Their intersection in space determines 
the position of the scene point. 
Structure fl'om motion can be considered in a similar way to stereo but with 
the different viewpoints resulting from (unknown) relative motion of the viewer 
and the scene. The emphasis of structure from motion approach has been to 
determine thc number of (image) points and the number of views needed to 
recover the spatial configuration of thc scene points and the motion compatible 
with the views [201,135]. The processing involved can be decomposed into three 
parts. 
1. 
. 
Tracking fi.'atures (usually 2D image structures such as points or "cor- 
nel's ~ ) 9 
Interpreting their image motion as arising from a rigid motion in 3D. This 
can be used to estimate the exact details (translation and rotation) of the 
relative motion. 
. Image velocities and viewer motion can then be interpreted in the same 
way as stereo disparities and epipolar geometry (see above). These are used 
to recover the scene structure which is expressed explicitly as quantitative 
depths (up to a speed-scMe ambiguity). 
The computational nature of these problems has been the focus of a signif- 
icant amount of research during the past two decades. Many aspects are well 
