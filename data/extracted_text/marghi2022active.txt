1
Active recursive Bayesian inference using R ´enyi
information measures
*Yeganeh M. Marghi1, *Aziz Koc ¸anao˘gulları1, Murat Akc ¸akaya2, Deniz Erdo ˘gmus ¸1
1Northeastern University, 2University of Pittsburgh
Abstract—Recursive Bayesian inference (RBI) provides opti-
mal Bayesian latent variable estimates in real-time settings with
streaming noisy observations. Active RBI attempts to effectively
select queries that lead to more informative observations to
rapidly reduce uncertainty until a conﬁdent decision is made.
However, typically the optimality objectives of inference and
query mechanisms are not jointly selected. Furthermore, conven-
tional active querying methods stagger due to misleading prior
information. Motivated by information theoretic approaches, we
propose an active RBI framework with uniﬁed inference and
query selection steps through R ´enyi entropy and α-divergence.
We also propose a new objective based on R ´enyi entropy and
its changes called Momentum that encourages exploration for
misleading prior cases. The proposed active RBI framework
is applied to the trajectory of the posterior changes in the
probability simplex that provides a coordinated active querying
and decision making with speciﬁed conﬁdence. Under certain
assumptions, we analytically demonstrate that the proposed
approach outperforms conventional methods such as mutual
information by allowing the selections of unlikely events. We
present empirical and experimental performance evaluations
on two applications: restaurant recommendation and brain-
computer interface (BCI) typing systems.
Index Terms—Active learning, Bayesian inference, Recursive
state estimation, R ´enyi entropy.
I. I NTRODUCTION
Recursive Bayesian inference (RBI) is a general probabilis-
tic framework to estimate the unknown probability distribution
of latent states through a recursive querying process over
time. Due to advantages of Bayesian approaches, for state
estimation, it is also preferred to fuse the obtained observation
with a prior information in a Bayesian manner. Accordingly,
the state estimation and the estimation conﬁdence depend
on the posterior distribution recursively calculated over the
state. Since, the observations are noisy, to achieve a high
conﬁdence and to decrease ambiguity in the state estimation,
the system probes the environment through multiple recursions
of queries [1], [2]. Throughout this manuscript, we denote each
recursion and corresponding conﬁdence update as a sequence.
The RBI framework consists of two main objectives: evi-
dence collection (potentially through active querying through a
sequence of queries) and inference. The goal of the inference
is to estimate latent state that is the most likely to be the
intended state. The inference objective is deﬁned based on
a Bayes decision policy. Typically the maximum a posteriori
(MAP) estimation is used to minimize zero-one risk. Decision
* Joint ﬁrst authors.
This paper is under consideration at Pattern Recognition Letters.
is constrained with a pre-deﬁned conﬁdence level over the
posterior distribution to prevent premature decisions. To reach
the conﬁdence level, the system attempts to make inference
after observing the evidence based on the queries. However,
the evidence collection process is costly, especially when the
evidence is not collected through carefully selected queries.
Therefore, query optimization are required to make the recur-
sive inference more practical and efﬁcient.
From a state-estimation perspective, query optimization in
the recursive state estimation is often performed by greedy
selection, which can be split into three subgroups of 1)
expected posterior maximization [3], in which the Bayesian
model has been used for the querying process; 2) Fisher
information-based approaches [4]–[6], and 3) information
theory-based approaches such as entropy minimization or
maximum mutual information (MMI) [7]–[11]. It is shown
that all these approaches for optimum sequence design through
query selection lead to the selection of N-best queries with
respect to the current belief [12], [13]. As an example of
posterior maximization, Wilson et al. in [3], proposed a
Bayesian query optimization to elicit the target with as few
queries as possible using a sampling with a Monte Carlo
estimate dependent variational distance measure. Using Fisher
information, Sourati et al. [6] introduced a Fisher information-
based ratio for an active sample selection for training a
classiﬁer with tractable computation. The proposed ratio shifts
the classiﬁer parameters towards the most informative point in
the parameter space. Since these parameters were essentially
obtained through a posterior maximization, this approach can
also be adopted to maximize the information of the argument
in the posterior. Hierarchical sampling proposed by Dasgupta
and Hsu [14] is another active learning framework proposed
for query selection, which forms a query subset partitioned
with different objectives.
All these approaches for optimum sequence design through
query selection would lead to the selection of N-best queries
decided based on the probability distribution deﬁned over
the latent states, under two assumptions: (i) unimodality of
observation distribution and (ii) ﬁnite subset of queries [12],
[13]. N-best selection queries based only on the latest belief
over the states may not always provide the best performance,
especially at the beginning of the RBI process, as they always
exploit the current knowledge and lack further exploration
beyond current belief.
As an example, consider a recommender system. In such a
system, the current belief is always inﬂuenced by the history
of the users’ proﬁles. Accordingly, by employing the N-best
arXiv:2004.03139v2  [cs.LG]  10 Mar 2021
2
selection, the system recommends items similar to the ones
selected by peers, or reminiscent of the ones the user has
previously showed interest in. However, these systems are not
as effective, when the user is looking for a new item, e.g. in the
case of an adventurous diner. In this case, the prior information
behaves in an adversarial manner, causing a longer estimation
procedure, or may lead to an unsatisfactory estimation due
to the limited number of observations. Therefore, instead of
exploiting the current belief, such systems can beneﬁt from
exploration of evidence beyond the current belief.
In addition to the sequence design based on query selection,
in an RBI framework, inference is usually constrained with
a posterior threshold. Such a threshold increases the number
of query sequences to obtain enough evidence for conﬁdent
inference. Therefore, in RBI there is a trade-off between speed
and accuracy. Here, we propose a uniﬁed active RBI frame-
work to balance the speed and accuracy trade-off and enhance
RBI performance. Our framework is particularly effective in
adversarial scenarios where a balance between exploitation
and exploration needs to be achieved.The proposed framework
utilizes the Bayesian approach for updating the probability
distribution deﬁned over the states using posterior trajectory
to enhance speed accuracy trade-off in state estimation. By
deﬁning query selection, inference objectives and conﬁdence
constraints based on R ´enyi entropy, our mathematical formu-
lation uniﬁes the query selection and inference steps of RBI.
Such a formulation enables us to analytically demonstrate
that the proposed policy for query selection that ﬁnds a
compromise between exploitation and exploration performs
at least as good as the methods that make N-best selections
relying on the exploitation of the current belief. Moreover this
formulation enables us to formulate the subset query selection
optimization problem as a tractable problem [15]. Finally, we
provide analytical results to show that the proposed uniﬁed
approach has theoretical guarantees to outperform the methods
based only on mutual information or entropy.
In summary, our novel contributions are (i) unifying the
inference and query selection steps of RBI through R ´enyi
entropy; (ii) proposing a new objective based on R´enyi entropy
and changes in the R ´enyi entropy, i.e. Momentum that encour-
ages exploration for adversarial cases, (iii) proposing a new
stopping criterion for decision making, (iv) providing a short-
term policy evaluation to demonstrate the proposed method has
theoretical performance guarantees under reasonable assump-
tions, and (v) presenting a novel geometrical representation
for RBI problem that allows us to investigate how the query
optimization inﬂuences the estimation process and how the
posterior belief changes over sequences in the probability sim-
plex. To illustrate the performance of the proposed method, we
consider two testbeds, (a) restaurant recommendation system,
and (b) a brain-computer interface (BCI)-based typing system.
II. P RELIMINARIES
In the recursive state estimation, the learner (system) is
tasked to update its belief of the distribution over state
candidates by collecting observations as a result of querying
the environment. The state is denoted by σ. We assume that
Figure 1. Proposed probabilistic graphical model representing the Sth
sequence for the RBI problem. H0 denotes prior information in the estimation.
state σ and the query set Φs generates the evidence εs through a label
assignment ℓs. ℓs deﬁnes the relationship between queries and the state in a
probabilistic manner.
the state is an element of a ﬁnite set denoted by A. The learner
proceeds with the evidence collection for state estimation
through sequences of queries indexed by s which is divided
into N trials indexed by i. The system decides on a subset of
queries Φs ≜ {φ1
s,...,φ N
s }at the beginning of each sequence,
where φi
s ∈Q and Qis the set of queries. After system queries
the environment, corresponding evidence εs ≜ {ε1
s,...,ε N
s }
is observed . Due to the noise in an observation, estimation
of σ requires recursion of multiple sequences. This process
can be well formulated as a RBI problem. By formulating the
problems in a Bayesian estimation framework, we decompose
the estimation into inference (I) and querying (Q) objectives,
which is expressed as follows.
(I) : ˆσ= arg max
σ
fI(σ,Hs)
s.t. gI(σ,Hs) >0
(Q) : Φs+1 = arg max
Φ
fQ(σ,Φ,Hs)
(1)
Here, Hs ≜ {ε1:s,Φ1:s,H0}represents the task history and
H0 represents the prior information. Where f and g are
denoting arbitrary functions to distinctly represent objectives
and the constraint respectively. The system alternates between
(I) and (Q) to collect evidence until the optimality objective
measure, i.e. an equality constraint gI is satisﬁed. Accordingly,
the choice of constraint and query optimization objective is
critical to reduce the cost e.g., number of queries.
To represent the probabilistic relationship among variables,
the graphical model of the RBI problem is illustrated in
Figure 1. In this ﬁgure, ℓs is the label for each observation
at sequence s. In this paper, we assume observations are
independent of each other given the label sequence, therefore
the posterior probability is deﬁned as the following;
p(σ|εs,Φs,Hs−1) = p(σ|H0)
s∏
j=1
p(εj|σ,Φj)
p(εj|Φj,Hj−1)
where p(εj|σ,Φj) and p(εj|Φj,Hj−1) can be obtained trough
3
marginalization over ℓ as,
p(εj|σ,Φj) =
∑
ℓj
p(εj|ℓj)p(ℓj|σ,Φj)
p(εj|Φj,Hj−1) =
∑
ℓj
p(εj|ℓj)p(ℓj|Φj,Hj−1)
Accordingly, we can have:
p(σ|ℓj,Φj,Hj−1) = p(ℓj|σ,Φj)p(σ|Hj−1)
p(ℓj|Φj,Hj−1)
= p(ℓj|σ,Φj)p(σ|Hj−1)∑
v∈A
p(ℓj|v,Φj)p(v|Hj−1)
where p(ℓj|σ,Φj) expresses the probabilistic relationship be-
tween each subset of queries and a state, which is typically
learned by query understanding techniques [16]. Since Query
understanding is out of the scope of this study, it is assumed
that p(ℓj|σ,Φj) is given as an external input.
Conventionally, in the active RBI framework, fI is a
function of posterior probability and changes in probability
domain and state constraint is a conﬁdence level threshold
τ ∈ R on the state posterior probability. Moreover, query
selection is typically achieved through information theory-
based techniques such as MMI [9], which is equivalent to
conditional entropy minimization when εs corresponding to
the upcoming sequence is not observed. Accordingly, fI and
fQ objectives and gI are deﬁned as the following;
(I) : ˆ σ= arg max
σ∈A
p(σ|Hs)
s.t. p(σ|Hs) >τ
(Q) : Φ s+1 = arg max
Φ∈QN
−H(σ|εs,Φ,Hs)
(2)
Since −H(σ|εs,Φ,Hs) is a submodular and non-increasing
set function, the sequential subset query selection can be
achieved via a greedy approach by optimizing fQ for each
query with theoretical guarantees [17]–[19]. Therefore, query
selection objective can be reformulated for each query as the
following;
(Q) : φi
s+1 = arg max
φ
−H(σ|εi
s+1,φ, Hs). (3)
III. I NTEGRATING INFERENCE AND QUERYING
OBJECTIVES
A known challenge in optimizing multi-objective functions
such as (1), is that each individual objective demands its
own evaluation criteria and constraints. Therefore, the optimal
solution for one objective may not be optimum for the other(s).
For instance, the optimal query obtained by maximizing fQ
in (1) may not be the optimal query to be used in fI when
estimating the target state σ. Accordingly, it is ideal to use
a uniﬁed objective function, which conjoins (Q) and (I) by
unifying fQ and gI, such that optimizing fQ results in a query
satisfying gI. In conventional representation of the objectives
as deﬁned in (2), (Q) is designed to select subset of queries
to reduce the ambiguity over the state estimates whereas
the (I), which is constrained with a posterior threshold. We
show the unity between these two identities using R ´enyi
entropy [20]–[22]. Accordingly, the active RBI framework can
be represented as,
(I) : σ= arg max
σ∈A
p(σ|Hs)
s.t. Hα1 (σ|Hs) <τ ′
(Q) : φi
s+1 = arg max
φ∈Q
−Hα2 (σ|εi
s+1,φ, Hs)
(4)
where, τ′ is the upper bound of the acceptable entropy,
α1,α2 ∈[0,
8) are the orders of entropy measures for (I)
and (Q) respectively. α-entropy and conditional entropy are
deﬁned as the following;
Hα1 (σ|Hs) = 1
1 −α1
log
(∑
σ∈A
pα1 (σ|Hs)
)
(5)
Hα2 (σ|ε,φ, Hs) = 1
1 −α2
Ep(ε|φ,Hs)
(
log
(∑
σ∈A
pα2 (σ|ε,φ, Hs)
)) (6)
Conclusively, for α1 =
8and α2 = 1 the objectives in
(2) and (4) are identical. Therefore, α1 and α2 are tuning
hyperparameters that adjust a balance between the maximum
posterior condition ( α1 =
8) and Shannon entropy ( α2 = 1).
Moreover, since Hα2 is also a submodular and non-increasing
set function as shown in [23], [24], the sequential subset query
selection in (4) is performed via a greedy algorithm.
IV. A P OSTERIOR CHANGES -BASED OBJECTIVE
FUNCTION
The proposed framework in (4) is using a measure of
expected information gain with the current belief (posterior) of
an estimate and results in the MMI selection. However, greedy
selections based on posterior might initially suffer if the prior
information is misleading. In contrast to prior belief, if the
system state is one of the least probable scenarios (according to
prior), i.e. the adversarial case, the system needs to overcome
the misleading prior via exploration.
To encourage exploration in the query selection (Q), pre-
viously we have proposed a new objective called Momentum,
which is a function of posterior changes over sequences [25].
We have shown that including Momentum in fQ provides a
trade-off between exploration and exploitation and compared
to the N-best method, it increases the chance of the unlikely
queries to be selected. In [26], we have also demonstrated
that the accuracy/speed ratio can be improved by adding
the Momentum term in the stopping criterion, in a RBI
problem. While the previously introduced Momentum function
has shown promising results, it cannot be utilized in the
integrated RBI framework using a general class of information
measures in (4). To extend the existing methods, in this
study, we redeﬁne the Momentum objective by proposing a
general family for Momentum that can be used for both query
selection and stopping criterion in the uniﬁed RBI framework.
4
Accordingly, the alternative fQ for (4) is expressed as the
following;
φi
s+1 = arg max
φ∈Q
−Hα2 (σ|εi
s+1,φ, Hs) + λ1Mα2 (φ,Hs)
(7)
Mα(φ,Hs) = 1
s
s−1∑
n=0
mα(φ,Hn)
= 1
s
s−1∑
n=0
1
(α−1) log
∑
σ∈A
pα(σ|ε,φ, Hn)
pα−1(σ|Hn) p(l= 1|σ,φ)
(8)
where mα(φ,Hn) is called Q-Momentum, Mα2 (φ,Hs) is the
average Q-Momentum and λ1 is the tuning parameter between
objectives. Momentum term by deﬁnition is a state-directed
average of log-posterior changes over all previous sequences
which allows for a one-step advanced approximation of overall
conditional entropy.
Likewise, inclusion of Momentum in (I) can sped up the
estimation process through constraint relaxation in (4) as
follows.
Dα(Hs) = 1
s
s−1∑
n=0
dα(Hn)
= 1
s
s−1∑
n=0
1
(α−1) log
∑
σ∈A
pα(σ|Hn+1)
pα−1(σ|Hn)
(9)
where dα(Hn) is called I-Momentum and Dα(Hs) is the
average I-Momentum. Accordingly, the uniﬁed framework for
RBI problem can be expressed as;
(I) : σ= arg max
σ∈A
p(σ|Hs)
s.t. Hα1 (σ|Hs) −λ1Dα1 (Hs) <τ ′
(Q) : φi
s+1 = arg max
φ∈Q
−Hα2 (σ|εi
s+1,φ, Hs)+
λ2Mα2 (φ,Hs)
(10)
For a single query φ asked in the RBI framework, the
posterior probability is updated according to Bayes rule ,
which implies that at each sequence, the posterior change
is equivalent to p(σ|Hs−1)
[p(ε|σ,φ)
p(ε|φ) −1
]
. Accordingly, the
posterior change is a function of p(ε|σ,φ)
p(ε|φ) , called likelihood
evidence ratio and based on the Bayes rule, it is equivalent to
p(σ|Hs)
p(σ|Hs−1). In fact, both introduced Momentum terms in (I)
and (Q) are function of posterior changes. In the following
sections, we will show how Momentum enhances the speed
and accuracy of the active RBI process.
Pseudocode of the proposed uniﬁed active RBI can be found
in Algorithm 1. System loops querying until the predeﬁned
stopping criterion is satisﬁed. At sequence s+ 1, N number
of unique queries are selected from the query set Q. After the
query subset is selected, the corresponding evidence εs+1 is
observed as a response to the queries. Based on the obser-
vation, system updates the belief over the states and current
stopping condition. Once a feasible solution arises, system
returns the candidate with maximum of the current posterior.
Algorithm 1 Greedy approach for the active recursive
Bayesian inference framework
1: initialize A,Q,α1 ∈[0,∞),α2 ∈[0,∞),τ′,λ1,λ2
2: s←0,H∫ ←{H0}
3: stop ←τ′+ 1
4: while stop >τ ′ do
5: Q′←Q
6: for i∈{0,1,··· ,N}do
7: φi
s+1 ←arg max
φ∈Q′
−Hα2 (σ|εi
s+1,φ, Hs) + λ2Mα2 (φ,Hs)
8: Q′←Q′\{φi
s+1}
9: εs+1 observe for Φs+1 ⊿ Evidence Collection
10: Hs+1 ←Hs ∪{εs+1,φs+1} ⊿ Update History
11: s←s+ 1
12: stop ←Hα1 (σ|Hs) −λ1Dα1 (Hs) ⊿ (10)
13: ˆσ= arg max
σ∈A
p(σ|Hs)
14: return ˆσ
The initial stop criterion is adjusted such that the system is
required to query at least once to avoid instant termination.
Analytical Evaluation
The main purpose of Q-Momentum objective in the
proposed active query selection is providing exploration and
giving chance to unlikely states to be queried at the beginning
of the RBI process. In fact, for the same given task history,
the proposed fQ in (10) enhances the possibility of the
target state selection in the query subset compared to the fQ
objective in (4), while the system is dealing with adversarial
cases.
Proposition 1. Given p(ℓ = 1 |σ,φ) ∈ {0,1}, ∀(σ,φ) for
α ∈ [0,∞) and λ ≥ 0, given a,b ∈ Aand r,q ∈ Q,
where a is the state of the target, a ̸= b, and r ̸= q, if
∃Hs−1 s.t. p(a|Hs−1) <p(b|Hs−1), then
p
(
−Hα(σ|ε,φ = q,Hs) + λMα(φ= q,Hs)
>−Hα(σ|ε,φ = r,Hs) + λMα(φ= r,Hs)
)
≥p
(
−Hα(σ|ε,φ = q,Hs) >−Hα(σ|ε,φ = r,Hs)
)
Proposition 1 shows that the probability of a (assumed to
be the target state) having a higher fQ value than b is larger
when (7) is used instead of fQ in (4). Although, the probability
of a given the task history is lower than the probability of b
given the task history. This means that even if a is less likely
than b according to the prior (adversarial case) and observed
evidence, using the proposed query selection objective, a has
more chance to appear in the query subset, when compared
to using (3). The proof of Proposition 1 can be found in the
supplementary material.
It should be noted that by collecting more evidence through
a recursive process, λ2 should be dynamically updated such
that the emphasis on Hα2 is increased with the number of
sequences that means the λ2 value should be decreased as the
number of sequences is increasing [27].
5
(a)
 (b)
 (c)
 (d)
Figure 2. Illustration of posterior trajectories changes over sequences in RBI to reach the target vertex a. (a) The dashed areas illustrate an example of a
feasible region of the inference problem and the blue dot represents the initial probability over states. (b) Posterior changes over two sequences (queries). (c)
After collecting sufﬁcient evidence in four sequences, the system starts to move toward the target vertex. (d) Still not located in the feasible region of the
target, however, based on the direction and speed of posterior changes, the system can foresee the location of the next move and terminate the process.
V. G EOMETRICAL REPRESENTATION
To offer better understanding of the proposed active frame-
work, a geometrical illustration of the RBI problem using
probability simplex is discussed in this section. Although
probability simplex is widely used for Bayesian inference
interpretation, to the best of our knowledge, the use of
simplex to illustrate a recursive Bayesian inference through
the querying process has never been studied before. By taking
advantage of such a geometrical representation, we show that
the estimation problem can be conceptualized as a posterior
trajectory movement tracking problem, in which we gain new
probabilistic insights on the decision boundary, direction and
speed of the movement toward a particular state during an
active inference.
Probability mass functions can be visualized geometrically
as points in a vector space, called a probability simplex,
with the axes given by random variables. Figure 2 illustrates
an example of a probability simplex, where each vertex
corresponds to a state with its single probability value set
to 1. For simplicity, here we visualize all the ﬁndings for
ﬁxed cardinality |A|=3. For any given τ′, stopping criterion
(gI) forms a feasible set in the simplex and hence determines
whether the system should keep querying or make a decision.
Therefore, at any sequence s, if p(σ|Hs) is located in any
feasible region (dashed areas), the inference is terminated and
the vertex of that region is selected as the target state.
A. Momentum and Posterior Trajectory Movement
As discussed in Section IV, Momentum functions in (I)
and (Q) are functions of the posterior changes. In the query
optimization, Q-Momentum is a function of the vertex-directed
posterior changes in the simplex, which directly inﬂuences the
direction of the posterior trajectory movement in the simplex.
Let’s consider a simple adversarial example in Figure 2 where
the initial probability point (provided by prior information) is
spatially distant from the corner of interest. Assuming a is the
target state, the goal is to move the posterior toward vertex a
in the simplex. Using the Bayes rule, we can show that the
posterior, at each sequence, can only move along three lines
passing through vertices in the simplex space. In proposition 2,
we are demonstrating this geometrical fact.
Proposition 2. For σ,φ ∈ A= {a1,a2,...,a |A|}, prob-
ability points P(φ), ps, and ps−1 are collinear, where
ps = ( p(a1|Hs),p(a2|Hs),...,p (a|A||Hs)) and P(φ) =
(0,0,..., 1,..., 0) = 1(φ = σ), and 1 is an indicator
operator.
According to Proposition 2, prior, posterior and the vertex
point corresponding to the query are collinear (proof can be
found in the supplementary material). Accordingly, εs and
φs (vertex) determine the direction and movement path of
p(σ|Hs) in the simplex, respectively. By employing these
geometrical properties, we can demonstrate the trajectory of
the posterior changes over sequences in the simplex, which
provides a geometric tool to interpret and assesses the RBI
process.
As discussed before, by minimizing conditional entropy
Hα the querying selects the candidate with highest posterior
probability (closest vertex) and ﬂuctuates between two ver-
tices of non-interest. Alternatively the proposed Q-Momentum
measures the average change over passed sequences for each
query. Going back to the adversarial example in Figure 2, we
can see that at beginning of the process, the system is more
likely to query states c and b according to the latest posterior
probability. Although the initial responses to querying both c
and b are negative, we can see that the system keeps querying
them until s = 4 . The proposed fQ, however, has a higher
probability of selecting a query aligned with vertex a as
we stated in Proposition 1. This presumably yields a faster
estimation and prevents the system from zigzagging between
incorrect estimates.
B. Momentum and Posterior Movement Speed
As another geometric insight to the RBI problem, we can
see that the example presented in Figures 2c and 2d shows
two different querying sequences, where p(a|Hs) < 1 and
Hα <τ ′. Although the posterior is not placed in the feasible
region, in 2d, the system is fairly close to the vertex a. The
proposed I-Momentum objective in gI provides a relaxation
based on the prediction made by Dα for the next position of
ps+1 in the simplex, such that the system can make a decision
even before crossing the decision boundary. This relaxation
accelerates the inference process by including the history of
6
the posterior trajectory and approximating the speed of the
posterior movement. This approximation helps the system to
foresee whether the next location of the posterior will be
located in the feasible region or not. Therefore, it is not always
necessary to spend extra queries only to reach the feasible
region. Detailed discussion about decision boundary can be
found in the next Section.
VI. D ECISION BOUNDARY AND FEASIBLE SET IN RBI
Stopping criterion gI relies on the constraint of the inference
step. This constraint forms a feasible set in the simplex and
hence determines whether the system should continue query-
ing or make a decision. Applying a pre-deﬁned conﬁdence
level τ over posterior probability in fI, constructs a decision
boundary visualized in Figure 3 in the probability domain.
Therefore, for any sequence s, if p(x) ≥ τ at a particular
vertex, that vertex is the estimated target. Shaded region in
Figure 3 corresponds to the feasible set as a function of τ,
where S = {p(X) |p(x∗) ≥τ}. As expected, decreasing τ
enlarges the feasible set in the estimation problem.
In the RBI problem, the query objective attempts to select
a subset of queries by minimizing the conditional entropy
across all directions, where the selected queries should push
the posterior toward one of the triangle vertices in Figure 3.
There exists a sequence s, where Hs(X) ≤τ′′and ps(x) ≥τ.
To investigate the relationship betweenτ′′and τ, again we will
take advantage of geometrical representation in simplex.
Figure 3 shows H(X) = τ′′ and p(a) = τ boundaries.
We can see that, by replacing the posterior constraint in fI
with entropy function, we are in turn enlarging the feasible
set. Furthermore, it illustrates that the Hs(X) contour is
intersecting with p(a) = τ at midline. The following Lemma
proves this fact.
Lemma 1. For τ ≥ 1
|A| and |A|> 1, decision boundaries
p(a) = τ and H(X) ≤τ′′ intersect when entropy is at its
maximum τ′′, at the midpoint of line p(a) = τ, Pm, where
pmk ,∀k∈A is deﬁned as follows;
pmk =



τ k = a
1 −τ
|A|−1 k̸= a
Figure 3. Feasible sets in inference problem for two decision boundaries:
maximum of the posterior and entropy.
and
τ′′ = −τlog (τ) −(1 −τ) log
( 1 −τ
|A|−1
)
.
The proof of Lemma (1) can be found in the supplementary
material.
Subsequently, if n denotes the dimensionality of the state
space, we can deﬁne the intersection point as,
vn(τ) = [τ, 1 −τ
n−1,··· ,1 −τ
n−1]
Additionally, let us deﬁne the following point;
wn(τ) = [τ,1 −τ,0,··· ,0]
where this point can be generalized to the other vertices in the
simplex by changing the position of 1 −τ with any of the 0s.
According to the R ´enyi entropy deﬁnition, we have
Hα(vn(τ)) ≥Hα(wn(τ))
H∞(vn(τ)) = H∞(wn(τ)) = τ
Therefore, the decision boundary is a linear surface in the
simplex. Moreover, Hα(vn(τ))−Hα(wn(τ)) is correlated with
the extent of the difference between feasible regions.
Hα(vn(τ)) −Hα(wn(τ)) =
1
1 −αlog τα + (n−1)1−α(1 −τ)α−1)
τα + (1 −τ)α
(11)
Speciﬁcally for α= 1, where
H(vn(τ)) −H(wn(τ)) = (1 −p) log(n−1).
Figure 4 illustrates the changes of entropy as a function
of α and the state space dimension n. While the inference
objective fI aims to select the most likely state candidate, the
decision criterion gI prevents fI from picking a high-entropy
(ambiguous) state. Choosing an ambiguous candidate can be
easily avoided by making sure that the decision boundary is
intersecting with one of the surfaces.
Figure 5 shows the effect of differentαvalues where n= 30
in (a) and the effect of different nvalues where α= 1. Here, ˜τ
is deﬁned such that for a given α, Hα(vn(τ)) = Hα(wn(˜τ)),
where decision boundary intersects with the surface. It should
be noted that we cannot always ﬁnd a ˜τ value such that for
a given τ, the equi-entropy contours completely contained
within the simplex. It can be observed from both plots in
Figure 5 that for the selected values for ˜τ, equi-entropy balls
reside inside the simplex and can force a termination in the
estimation not getting closer to one vertex. Therefore α can
be chosen accordingly with τ value and the cardinality of the
state space n. For instance, in the case of a high conﬁdence
level, e.g. τ ≈0.8 with n= 30, we should avoid selecting an
α such that α<< 1, to prevent dramatic performance loss.
VII. E XPERIMENTAL RESULTS
To evaluate the performance of the proposed active infer-
ence framework, ﬁrst we study role of α and λ parameters in
the recursive inference process and show how using the uniﬁed
framework can jointly enhance both speed and precision in a
7
0.8 0.9 1.0
0.0
0.5
1.0
1.5H (v30( )) H (w30( ))
0.7
0.9
1
2
10
(a)
0.8 0.9 1.0
0.0
0.5
1.0H(vn( )) H(wn( ))
 3
4
10
20
50 (b)
Figure 4. Difference between entropy values of the probabilities vn,wn for a given maximum value τ. (a) Presents the entropy differences with differing α;
(b) presents the entropy differences with differing n.
0.8 0.9 1.0
0.75
0.80
0.85
0.90
0.95
1.00
0.7
0.9
1
2
10
(a)
0.8 0.9 1.0
0.75
0.80
0.85
0.90
0.95
1.00
3
4
10
20
50 (b)
Figure 5. (a) Presents the corresponding (τ,˜τ) tuples where n = 30 for different α values; (b) presents the corresponding (τ,˜τ) tuples where α = 1 for
different alpha n values.
RBI problem. Here, the accuracy and speed are deﬁned as
the number of correct selections and the inverse number of
queries required for the inference problem. In addition to that,
we compare the performance of the proposed framework to the
other approaches discussed in the introduction such as MMI
and random selection in two applications.
A. Speed-accuracy Trade-off
At a more general level, for the ﬁrst empirical analysis
we show some simulation results regarding the impact of α
in the inference performance. For this study, we employed
a simple target detection task using Monte-Carlo sampling.
The goal of the task is to guess the target state (out of 30
candidates) by querying and collecting samples from two class
conditional distributions, i.e. target and non-target classes. To
simplify the sampling process, we assume evidence is sampled
from Gaussian distributions conditioned on query and state
tuples. Speciﬁcally we select these distributions to be 1D and
these distributions are further used to calculate the posterior
distribution over the states after each recursion.
To evaluate the evolution of the posterior under different
query selection methods, we have compared the proposed
method with ﬁve proposed active query selection methods
in [3], [6], [8], [14], [28] as follows. First, we implemented the
proposed active query selection proposed by Tong and Koller
in [28] that aims to maximize the state-space spanned with a
pre-deﬁned kernel at a given location. For our classiﬁcation
problem, it is assumed that the state space is discrete with
dirac-delta function as the kernel. Then we have had the
Fisher information-based method proposed by Sourati in [6]
and the MMI-based method proposed by Higger in [8]. We
also implemented the expected posterior maximization method
proposed by Wilson in [3], in which it is assumed that the class
conditional evidence distributions are known. Finally, we have
implemented the hierarchical sampling method proposed by
Dasgupta and Hsu in [14]. Here, to balance the exploration
and exploitation process, a percentage of queries are selected
to minimize uncertainty and the rest are selected randomly.
Figure 6 illustrates the target state posterior evolution in
estimation over sequences under three different conditions
on the prior probability (H0) at the beginning of the pro-
cess including uniform, adversarial, and supportive priors.
Taking advantages of the history of the posterior changes,
in the adversarial case, the proposed method outperforms
the other methods in terms of both speed and accuracy.
For the other two cases (non-adversarial), we can observe
similar performance to the MMI method. As another obser-
vation, we can see that depending on the prior probability,
8
Figure 6. The impact of α on decision making. R ´enyi entropy based method for different α values is compared to random and 5 other query selection
methods in adversarial and supportive prior (from left to right) presence. Observe that, proposed methods increase the probability mass on the true estimate
earlier and faster than other methods. Additionally, with increasing α values, estimation curves for the proposed method converge to N-best methods curve.
0.00 0.01 0.02 0.03 0.04 0.05
1
0.0
0.2
0.4
0.6
0.8
1.0Accuracy
0.00 0.01 0.02 0.03 0.04 0.05
1
0.1
0.2
0.3
0.4
0.5Speed
1 = 0.2
1 = 0.5
1 = 1
1 = 10
Speed-accuracy trade-off (impact of 1, 1)
(a)
0.0 0.5 1.0
2
0.5
0.6
0.7
0.8
0.9
1.0Accuracy
0.0 0.5 1.0
2
0.05
0.06
0.07
0.08
0.09
0.10
0.11Speed
2 = 0.2
2 = 0.5
2 = 1
2 = 10
Speed-accuracy trade-off (impact of 2, 2)
 (b)
Figure 7. Impact of α and λ on the speed and accuracy of inference in the proposed RBI framework. (a) Given α2,λ2 = 1, speed-accuracy trade off is
computed as a function of α1 and λ1. (b) Using α1 = ∞ and λ1 = 0, the accuracy and speed (inverse of the number of sequences) were calculated as a
function α2 and λ2.
(a)
 (b)
Figure 8. Geometric representation of the RBI problem in probability simplex for three elements. (a) Recursive posterior transition in the simplex using
Random, MMI, and the proposed query methods. (b) The effect of the prior on the posterior movement trajectories in the simplex. Each blue dot represents
a prior. The reported average trajectory and its variability computed for 500 Monte-Carlo simulations. For all simulations, two 1D Gaussian distributions,
N(0,1), N(3,1.5) are used to model target and non-target distributions.
the inference process can be inﬂuenced differently by using
different values of α2 < 1 and α2 > 1. As a justiﬁcation,
consider the Q-Momentum term in (7). If α2 <1, maximizing
9
1
(α2 −1) log
∑
σ∈A
pα2 (σ|ε,φ, Hn)
pα2−1(σ|Hn) is equivalent to minimizing
1
(1 −α2) log
∑
σ∈A
pα2 (σ|ε,φ, Hn)p1−α2 (σ|Hn). Accordingly,
the system will query a subset of most unlikely states based on
the H0, which helps the posterior to moves toward the unlikely
target state (state 16) faster than α2 ≥1 and MMI. Using the
similar simulation study, Figure 7 shows the speed-accuracy
changes as a function of α1,2 and λ1,2 parameters in (10).
Figures 7a and 7b illustrate speed-accuracy changes as a
function of (α1,λ1) and (α2,λ2), respectively. Here, the size
of markers is a linear function of accuracy and speed and
larger points shows higher speed and accuracy. Comparing the
speed and accuracy of points obtained from uniﬁed method to
the conventional case, i.e. α2 = 1, λ2 = 0, we can see that
adjusting these parameters in RBI allows us to increase speed
and accuracy simultaneously.
Figure 8 demonstrates the generality of the discussed geo-
metric approach in Section V and connects the discussed ge-
ometrical representation with the experimental part. Figure 8a
illustrates recursive posterior transition in the simplex using
different query selection methods. The same experimental
settings used for speed-accuracy analyses is used in generating
8a, except for estimating the target among 3 candidates as
opposed to 30. The analysis uses two 1-dimensional Gaussian
distributions, N(0,1), N(3,1.5) to model target and non-
target evidence distributions. Moreover, Figure 8b shows the
impact of the prior probability on the posterior movement tra-
jectories by showing the average trajectory and its variability
for 500 Monte-Carlo simulations in the simplex.
B. Restaurant Recommender
The primary aim in all recommender systems is to un-
derstand what the customer is looking for. For restaurant
recommendation, assuming that the diner has a certain type of
restaurant in mind, the system needs to discover their preferred
restaurant according to the diner’s proﬁle and their responses
to the asked queries. Given the projection matrix for mapping
the queries to the state (extracted from query understanding),
we can use the proposed inference method to estimate the
diner’s intent. Noted that extracting the projection matrix via
query understanding is outside the scope of this paper. Here,
we used Entree Chicago Recommendation dataset [29], which
consists of list of restaurant, cuisines, users’ scores. Using
the dataset, we learned diners’ proﬁles and projection matrix
between list of queries and places. By mapping diners’ scores
to binary Yes and No answers, we used Monte-Carlo simulation
to draw samples from two conditional distributions over 80
restaurants. Figure 9 shows an example of the performance of
the recommender system for two diners using the proposed
method, MMI and random approaches. Figure 9a belongs to a
diner with supportive proﬁle, which means this diner prefers
similar places that they have tried before (a conventional
diner). Figure 9b depicts an adversarial proﬁle where the diner
is eager to try new places (an adventurous diner). Applying
the proposed method over 50 diners shows that on average we
improve accuracy and speed by 8% and 10% respectively in
5 10 15 20
# Sequences
0.5
0.6
0.7
0.8
0.9
1.0p(R12)
Random
MMI
Proposed-c
Proposed-v
(a)
20 40 60 80 100
# Sequences
0.0
0.2
0.4
0.6
0.8
1.0p(R12) (b)
Figure 9. The intended restaurant probability changes in RBI for two users,
(a) a conventional diner and (b) an adventurous diner.
the user intent estimation. We present our results in Table I.
In all of the studies, we decrease λ2 with a small annealing
rate over sequences down to a minimum value 0.
C. Brain-computer Interface (BCI) Typing System
As another application, we use a language-model-assisted
EEG-based BCI typing interface [30]. User intent detection is
one of the main components of a BCI system, in which the
system estimates the intended user state according to the brain
activities, e.g. electroencephalogram (EEG) evidence collected
under speciﬁed presentation mode and prior knowledge pro-
vided by the language model (LM). In order to estimate the
posterior probability over typing symbols such as English
alphabet, a context prior P(σ|H0) is provided by the LM,
which estimates the conditional probability of every letter in
the alphabet based on n −1 previously typed letters in a
Markov model framework. The BCI-based typing interfaces
typically use a rapid serial visual presentation, in which the
system uses a ﬁnite set of symbols. In this study, both σ and
φ belong to A= Q= {A,B,C,...,Z, ,<}, where and <
represent space and backspace, respectively. As observation,
EEG signals were acquired from 10 sensors according to
International 10-20 System locations: Fp1, Fp2, Fz, F3, F4,
F7, F8, Cz, C3, C4, T3, T4, T5, T6, P3, P4, O1, O2, A1 and
A2. A DSI-24 Wearable Sensing EEG Headset was used for
data acquisition, at a sampling rate of 300 Hz with active dry
electrodes. All participants performed the calibration session
containing 100 sequences; each sequence includes 8 trials
(queries); and one trial in each sequence is the target symbol
which is displayed on the screen prior to each sequence. The
time interval between trials is 200 ms. Optimal parameters
for both target and non-target class distributions were learned
using the calibration data, which are used in simulation studies
and copy phrase task.
BCI typing systems require real-time stimuli sequence
optimization. Query optimization for BCI typing systems is
not a well-studied problem. To the best of our knowledge,
there are limited number of studies that addressed the query
optimization problem for the BCI typing system designs [8],
[11] and all of the proposed query selection methods are using
MMI method that results in the selection of the N-best symbols
based on the posterior distribution conditioned on the LM [12].
10
Table I
PERFORMANCE OF THE PROPOSED FRAMEWORK USING DIFFERENT SETTING OF PARAMETERS IN THE RESTAURANT RECOMMENDER SYSTEM , FOR TWO
TYPES OF DINERS FOR 80 RESTAURANTS ’ REVIEWS . ACC PRESENTS THE ACCURACY OF THE ESTIMATION AND SPEED IS INVERSE NUMBER OF
SEQUENCES . MMI IS EQUIVALENT TO THE PROPOSED METHOD USING (α1 : ∞,α2 : 1,λ1 : 0,λ2 : 0), PROPOSED -CαIS EQUIVALENT TO THE PROPOSED
METHOD USING (α1,2 : 1,λ1 : 0 → 0.01,λ2 : 1 → 0) WITH CONSTANT αVALUES , AND PROPOSED -VαIS EQUIVALENT TO THE PROPOSED METHOD
USING (α1,2 : 0.1 → 1,λ1 : 0 → 0.01,λ2 : 1 → 0), WHERE λ1,2, IN WHICH BOTH αAND λARE VARIABLE AND BEING CHANGED OVER SEQUENCES .
Random MMI Proposed-c α Proposed-vα
Case ACC Speed ACC Speed ACC Speed ACC Speed
Conventional Diner 85.29 0.0500 92.03 0.1400 92.10 0.1400 92.51 0.1600
Adversarial Diner 57.88 0.0104 74.57 0.0172 75.72 0.0185 80.73 0.0189
There are cases that posterior conditioned on the LM and EEG
evidence are misleading especially if the intended symbol is
highly unlikely for the LM. We show that using the proposed
uniﬁed framework enhances the typing performance measures
in compare to MMI method, which only exploits the current
knowledge based on the posterior.
Simulation Study: To evaluate the empirical performance
of the proposed active inference, a copy phrase task was
simulated using EEG data collected during the calibration
sessions. Using class conditional distributions fσ∗,φi
s (target
class) and f̸σ∗,φis (non-target class), we used Monte-Carlo
sampling method to draw samples from class distributions for
typing letter. For instance, the target phrase is ”She needs one
month to convalesce. ”and convalesce is the target word which
needs to be typed to complete the phrase.
Figure 10 shows the average of two standard typing ac-
curacy and speed measures such as accuracy in typing a
letter correctly (ATL) or information transfer rate (ITR) for 10
healthy participants. In this task, Proposed-v α again provided
the highest speed and accuracy. The implemented BCI system
is publicly accessible from https://github.com/BciPy/BciPy.
Random
MMI
Proposed-c
Proposed-v
0.0
0.2
0.4
0.6
0.8
1.0ITR (bits/seq)
(a)
Random
MMI
Proposed-c
Proposed-v
0.0
0.2
0.4
0.6
0.8
1.0ATL (b)
Figure 10. (a) Average of ITR and (b) ATL for 10 users attending the copy
phrase task in RSVP Keyboard experiment.
VIII. C ONCLUSIONS
A new active inference framework unifying the query selec-
tion and decision making objectives of recursive Bayesian in-
ference has been proposed. More speciﬁcally, being motivated
by information theoretic approaches, we have uniﬁed the RBI
with a new objective based on the α-entropy and α-divergence.
We have provided a geometrical interpretation of the RBI
over the probability simplex to demonstrate the progression
of belief over the states during inference and query selection
steps. We showed both theoretically and through three different
testbeds that the proposed framework improves both speed and
accuracy of state estimation.
REFERENCES
[1] N. Bergman, “Recursive bayesian estimation,” Department of Electrical
Engineering, Link ¨oping University, Link ¨oping Studies in Science and
Technology. Doctoral dissertation, vol. 579, p. 11, 1999.
[2] S. S ¨arkk¨a, Bayesian ﬁltering and smoothing . Cambridge University
Press, 2013, vol. 3.
[3] A. Wilson, A. Fern, and P. Tadepalli, “A bayesian approach for policy
learning from trajectory preference queries,” in Advances in neural
information processing systems , 2012, pp. 1133–1141.
[4] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu, “Batch mode active learning
and its application to medical image classiﬁcation,” in Proceedings of
the 23rd international conference on Machine learning . ACM, 2006,
pp. 417–424.
[5] S. P. Chepuri and G. Leus, “Sparsity-promoting sensor selection for non-
linear measurement models,” IEEE Transactions on Signal Processing ,
vol. 63, no. 3, pp. 684–698, 2015.
[6] J. Sourati, M. Akcakaya, D. Erdogmus, T. K. Leen, and J. G. Dy, “A
probabilistic active learning algorithm based on ﬁsher information ratio,”
IEEE transactions on pattern analysis and machine intelligence, vol. 40,
no. 8, pp. 2023–2029, 2017.
[7] D. Golovin, A. Krause, and D. Ray, “Near-optimal Bayesian active
learning with noisy observations,” in Advances in Neural Information
Processing Systems, 2010, pp. 766–774.
[8] M. Higger, F. Quivira, M. Akcakaya, M. Moghadamfalahi, H. Nezamfar,
M. Cetin, and D. Erdogmus, “Recursive bayesian coding for bcis,” IEEE
Transactions on Neural Systems and Rehabilitation Engineering, vol. 25,
no. 6, pp. 704–714, 2017.
[9] B. Jedynak, P. I. Frazier, and R. Sznitman, “Twenty questions with noise:
Bayes optimal policies for entropy loss,” Journal of Applied Probability,
vol. 49, no. 1, pp. 114–136, 2012.
[10] M. Moghadamfalahi, M. Akcakaya, H. Nezamfar, J. Sourati, and D. Er-
dogmus, “An Active RBSE Framework to Generate Optimal Stimulus
Sequences in a BCI for Spelling,” ArXiv e-prints (arXiv:1607.03578
[cs.HC]), Jul. 2016.
[11] B. Mainsah, D. Kalika, L. Collins, S. Liu, and C. Throckmorton,
“Information-based adaptive stimulus selection to optimize communi-
cation efﬁciency in brain-computer interfaces,” in Advances in Neural
Information Processing Systems , 2018, pp. 4820–4830.
[12] A. Koc ¸anaogulları, M. Akc ¸akaya, and D. Erdogmus, “On analysis of
active querying for recursive state estimation,” IEEE Signal Processing
Letters, vol. 25, no. 6, p. 743, 2018.
[13] T. Tsiligkaridis, B. M. Sadler, and A. O. Hero, “Collaborative 20
questions for target localization,” IEEE Transactions on Information
Theory, vol. 60, no. 4, pp. 2233–2252, 2014.
[14] S. Dasgupta and D. Hsu, “Hierarchical sampling for active learning,” in
Proceedings of the 25th international conference on Machine learning ,
2008, pp. 208–215.
[15] Y . Baram, R. E. Yaniv, and K. Luz, “Online choice of active learning
algorithms,” Journal of Machine Learning Research , vol. 5, no. Mar,
pp. 255–291, 2004.
[16] J. Liu, P. Pasupat, Y . Wang, S. Cyphers, and J. Glass, “Query understand-
ing enhanced by hierarchical parsing structures,” in2013 IEEE Workshop
on Automatic Speech Recognition and Understanding . IEEE, 2013, pp.
72–77.
[17] A. R. Barron et al., “Entropy and the central limit theorem.” Ann. Prob.,
vol. 14, no. 1, pp. 336–342, 1986.
11
[18] O. Johnson and A. Barron, “Fisher information inequalities and the
central limit theorem,” Probability Theory and Related Fields , vol. 129,
no. 3, pp. 391–409, 2004.
[19] V . F. Farias and R. Madan, “The irrevocable multiarmed bandit problem,”
Operations Research, vol. 59, no. 2, pp. 383–399, 2011.
[20] A. R ´enyi et al., “On measures of entropy and information,” in Proceed-
ings of the Fourth Berkeley Symposium on Mathematical Statistics and
Probability, Volume 1: Contributions to the Theory of Statistics . The
Regents of the University of California, 1961.
[21] K.-S. Song, “R ´enyi information, loglikelihood and an intrinsic distribu-
tion measure,” Journal of Statistical Planning and Inference , vol. 93,
no. 1-2, pp. 51–69, 2001.
[22] Y . Li and R. E. Turner, “R ´enyi divergence variational inference,” in
Advances in Neural Information Processing Systems , 2016, pp. 1073–
1081.
[23] M. Madiman, “On the entropy of sums,” in 2008 IEEE Information
Theory Workshop. IEEE, 2008, pp. 303–307.
[24] T. van Erven and P. Harremo ¨es, “R ´enyi divergence and majorization,”
in 2010 IEEE International Symposium on Information Theory . IEEE,
2010, pp. 1335–1339.
[25] A. Koc ¸anao˘gulları, Y . M. Marghi, M. Akc ¸akaya, and D. Erdo ˘gmus ¸,
“Optimal query selection using multi-armed bandits,” IEEE Signal
Processing Letters, vol. 25, no. 12, pp. 1870–1874, 2018.
[26] Y . M. Marghi, A. Koc ¸anao˘gulları, M. Akc ¸akaya, and D. Erdo˘gmus ¸, “A
history-based stopping criterion in recursive bayesian state estimation,”
in ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) . IEEE, 2019, pp. 3362–3366.
[27] J. A. Bilmes, “Dynamic bayesian multinets,” in Proceedings of the
Sixteenth conference on Uncertainty in artiﬁcial intelligence . Morgan
Kaufmann Publishers Inc., 2000, pp. 38–45.
[28] S. Tong and D. Koller, “Support vector machine active learning with
applications to text classiﬁcation,” Journal of machine learning research,
vol. 2, no. Nov, pp. 45–66, 2001.
[29] D. Dua and C. Graff, “UCI machine learning repository,” 2017.
[Online]. Available: http://archive.ics.uci.edu/ml
[30] M. Akcakaya, B. Peters, M. Moghadamfalahi, A. R. Mooney, U. Orhan,
B. Oken, D. Erdogmus, and M. Fried-Oken, “Noninvasive brain–
computer interfaces for augmentative and alternative communication,”
IEEE reviews in biomedical engineering , vol. 7, pp. 31–49, 2013.
SUPPLEMENTARY MATERIALS
A. Proof of Proposition 1
Proposition 1. Given, p(ℓ = 1|σ,φ) ∈{0,1}, ∀(σ,φ) for
α ∈ [0,∞) and λ ≥ 0, given a,b ∈ Aand r,q ∈ Q,
where a is the state of the target, a ̸= b, and r ̸= q, if
∃Hs−1 s.t. p(a|Hs−1) <p(b|Hs−1), then
p
(
−Hα(σ|ε,φ = q,Hs) + λMα(φ= q,Hs)
>−Hα(σ|ε,φ = r,Hs) + λMα(φ= r,Hs)
)
≥p
(
−Hα(σ|ε,φ = q,Hs) >−Hα(σ|ε,φ = r,Hs)
)
Proof. Given a,b as two possible states and q,r as two
possible queries such that p(ℓ = 1|σ = a,φ = q) = p(ℓ =
1|σ = b,φ = r) = 1 and p(ℓ = 1|σ ̸= a,φ = q) = p(ℓ =
1|σ ̸= b,φ = r) = 0 . Observe that, according to one to one
correspondence in state query tuples; ∑
σf(σ,q)p(ℓ|σ,q) =
f(a,q) + ∑
σ̸=af(σ,q) where f(·,·) represents an arbitrary
function with arguments from state space and query space
correspondingly. Therefore, objectives for query selection can
be represented only by corresponding state variables. For the
sake of simplicity we denote:
• Hα(q) = Hα(σ= a|φ= q,Hs),
Hα(r) = Hα(σ= b|φ= r,Hs)
• Mα(q) = Mα(φ= q, Hs),
Mα(r) = Mα(φ= r,Hs)
• p(a) = p(a|Hs), p(b) = p(b|Hs)
• p(a|ε,q) = p(a|ε,φ = q,Hs),
p(b|ε,r) = p(b|ε,φ = r,Hs)
We can deﬁne notations for b accordingly and ignore λ as
it holds ∀λ ≥0. Using the probabilistic identity of sum of
random variables, we have:
p(A+ B >C+ D) ≥p(A>C,B >D ) = p(A>C |B >D)
p(A>C |B >D) = p(B >D) ∀A,B,C,D
Thus we can write;
p(−Hα(q) + λMα(q) > −Hα(r) + λMα(r))
≥p(Mα(q) >Mα(r)|−Hα(q) >−Hα(r)) ×
p(−Hα(q) >−Hα(r) )
Since we are dealing with an adversarial case, p(a|Hs) <
p(b|Hs). Given condition −Hα(q) >−Hα(r) and p(a|Hs) <
p(b|Hs), for α> 1, we have:
−Hα(q) >−Hα(r)
where −Hα(σ|Hs) = 1
α−1 log
(∑
σ∈A
pα(σ|Hs)
)
. Accord-
ingly,
1
α−1E
[
log
(∑
σ∈A
pα(σ|ε,q)
)]
>
1
α−1E
[
log
(∑
σ∈A
pα(σ|ε,r)
)]
12
∑
σ∈A
pα(σ|ε,q) >
∑
σ∈A
pα(σ|ε,r)
Since for α> 1, pα−1(a) <pα−1(b), therefore,
∑
σ∈A
pα(σ|ε,q)
pα−1(a) >
∑
σ∈A
pα(σ|ε,r)
pα−1(b)
∑
σ∈A
pα(σ|ε,q)
pα−1(a) >
∑
σ∈A
pα(σ|ε,r)
pα−1(b)
1
α−1 log
∑
σ∈A
pα(σ|ε,q)
pα−1(a) > 1
α−1 log
∑
σ∈A
pα(σ|ε,r)
pα−1(b)
Since, p(ℓ = 1|σ ̸= a,φ = q) = p(ℓ = 1|σ ̸= b,φ = r) = 0,
according to the deﬁnition of Mα, we can conclude Mα(q) >
Mα(r). Since for 0 < α <1, pα−1(a) > pα−1(b), the same
conclusion can be drawn.
Accordingly, when p(a) < p (b) and −Hα(q) >
−Hα(r), we have Mα(q) > M α(r), which means
p(Mα(q) >Mα(r) | −Hα(q) >−Hα(r)) = 1 for the ad-
versarial case, and
p(−Hα(q) + λMα(q) > −Hα(r) + λMα(r))
≥p(−Hα(q) > −Hα(r))
B. Proof of Proposition 2
Proposition 2. for σ,φ ∈ A = {a1,a2,...,a |A|},
probability points P(φ), ps, and ps−1 are collinear, where
ps = ( p(a1|Hs),p(a2|Hs),...,p (a|A||Hs)) and P(φ) =
(0,0,..., 1,..., 0) = 1(φ = σ), and 1 is an indicator
operator.
Proof. Using Bayes’ rule, we have,
p(σ= ai|ε,φ = aj,Hs−1) = p(σ|Hs−1)p(ε|ai,aj)
p(ε|aj)
p(σ= ai|ε,φ = aj,Hs−1) = p(σ|Hs−1)p(ε|l= 0)
p(ε|aj) ,
∀i,j ∈[1,N],i ̸= j
p(σ= ai|ε,φ = ai,Hs−1) = p(σ|Hs−1)p(ε|l= 1)
p(ε|ai) ,
∀i∈[1,N]
Using the fact that all probability points located in the prob-
ability simplex have the property ∑
ip(ai|Hs) = 1,
k1 = p(ε|l= 1)
p(ε|φ= aj), k2 = p(ε|l= 0)
p(ε|φ= aj) (12)
k2 = 1 −p(φ= aj|Hs−1)k1
1 −p(φ= aj|Hs−1) (13)
In geometry, any two points X ∈ RN X1 =
(x11 ,x12 ,...,x 1N ), X2 = ( x21 ,x22 ,...,x 2N ) are trivially
collinear. The line containing these two points is deﬁned as:
x1 −x11
x21 −x11
= x2 −x12
x22 −x12
= ··· = xN −x1N
x2N −x1N
(14)
Accordingly, for |A|= N the line determined by p0 and p1
is deﬁned as:
(l1) : x1 −p(a1|Hs−1)
p(a1|Hs) −p(a1|Hs−1) = ··· =
xj −p(aj|Hs−1)
p(aj|Hs) −p(aj|Hs−1) = ··· =
xN −p(aN,|Hs−1)
p(aN,|Hs) −p(aN,|Hs−1))
(15)
According to the relationship between p(ai|Hs−1) and
p(ai|Hs) in (VIII-B) and (VIII-B),
x1 −p(a1|Hs−1)
p(a1|Hs−1)(k2 −1) = ··· =
xj −p(aj|Hs−1)
p(aj|Hs−1)(k1 −1) = ··· =
xN −p(aN|Hs−1)
p(aN|Hs−1)(k2 −1)
(16)
If x= P(φ= aj) point lies on (l1) line, then
0 −p(a1|Hs−1)
p(a1|Hs−1)(k2 −1) = ··· =
1 −p(aj|Hs−1)
p(aj|Hs−1)(k1 −1) = ··· =
0 −p(aN|Hs−1)
p(aN|Hs−1)(k2 −1)
(17)
Our proof is completed by showing (18) and (19) are correct.
−p(ai|Hs−1)
p(ai|Hs−1)(k2 −1) = 1 −p(aj|Hs−1)
p(aj|Hs−1)(k1 −1), ∀i∈[1,N], i̸= j
(18)
−p(ai|Hs−1)
p(ai|Hs−1)(k2 −1) = −p(aq|Hs−1)
p(aq|Hs−1)(k2 −1) ∀i,q ∈[1,N], i,q̸= j
(19)
Trivially, (19) is correct. Using (13), (18) is also correct.
−1
k2 −1 = 1 −p(aj|Hs−1)
p(aj|Hs−1)(k1 −1) (20)
p(aj|Hs−1)(1 −k1) = (1 −p(aj|Hs−1))(k2 −1)
= (1 −p(aj|Hs−1))
(1 −p(aj|Hs−1)k1
1 −p(aj|Hs−1) −1
)
= (1 −p(aj|Hs−1))(p(aj|Hs−1) −p(aj|Hs−1)k1
1 −p(aj|Hs−1)
)
= p(aj|Hs−1)(1 −k1)
(21)
13
C. Proof of Lemma 1
Lemma 1. For τ ≥ 1
|A|and |A|>1, decision boundaries
p(a) = τ and H(X) ≤τ′′ intersect when entropy is at its
maximum τ′′, at the midpoint of line p(a) = τ, Pm, where
pmk ,∀k∈A is deﬁned as follows;
pmk =



τ k = a
1 −τ
|A|−1 k̸= a
and
τ′′ = −τlog (τ) −(1 −τ) log
( 1 −τ
|A|−1
)
.
Proof. The entropy is upper bounded by a strictly concave
function of the maximum likely element of set i.e., p(x∗)
and cardinality of the set, where the function is monotonically
increasing for ∀x∈[1/|A|,1] where |A|≥ 1.
H(X) = −
∑
x
p(x) log(p(x))
= −p(x∗) log(p(x∗)) −
∑
x̸=x∗
p(x) log(p(x))
≤−p(x∗) log(p(x∗)) −(1 −p(x∗)) log
(1 −p(x∗)
|A|−1
)
(22)
Considering x∗= a, p(x∗) = p(a) = τ, according to (22), we
have
τ′′ = −τlog (τ) −(1 −τ) log
( 1 −τ
|A|−1
)
(23)
and
Pm = arg max{H(X)}
pmk =



τ k = a
1 −τ
|A|−1 k̸= a
(24)
where Pm is the midpoint of line p(a) = τ in the simplex.