Active physical inference via reinforcement learning
Shuaiji Li, Yu Sun, Sijia Liu, Tianyu Wang
Center for Data Science, New York University, 60 Fifth Ave, New York City, NY 10011 USA
Todd M. Gureckis
Department of Psychology, New York University, 6 Washington Pl, New York City, NY 10003 USA
Neil R. Bramley
Department of Psychology, University of Edinburgh, 7 George Square, Edinburgh, Scotland EH8 9JZ
Abstract
When encountering unfamiliar physical objects, children and
adults often perform structured interrogatory actions such as
grasping and prodding, so revealing latent physical properties
such as masses and textures. However, the processes driving
and supporting these curious behaviors are still largely myste-
rious. In this paper, we develop and train an agent able to ac-
tively uncover latent physical properties such as the mass and
force of objects in a simulated physical “micro-world’. Con-
cretely, we used a simulation-based-inference framework to
quantify the physical information produced by observation and
interaction with the evolving dynamic environment. We used
reinforcement learning to train an agent to implement general
strategies for revealing latent physical properties. We compare
the behaviors of this agent to the human behaviors observed in
a similar task.
Keywords: physical simulation; active learning; probabilistic
inference; reinforcement learning
Human adults have an intuitive understanding of the phys-
ical world that supports rapid and accurate predictions, judg-
ments and goal-directed actions. For example, we can quickly
estimate how heavy a door is by how it responds to a push,
judge whether a tower is stable with a glance, or predict where
to stand to catch a baseball by watching its trajectory. These
abilities are so ingrained in everyday life, we are rarely aware
of the complexity of identifying physical properties of objects
from brief experiences and interactions with their dynamics.
Bramley, Gerstenberg, Tenenbaum, and Gureckis (2018)
explored the strategies people use to actively infer masses and
forces of attraction and repulsion that relate objects in a sim-
ple simulated environment. Their learning environment con-
sisted of a bounded two-dimensional “hockey puck” world,
containing four circular objects of varying masses and re-
lated with pairwise attractive or repulsive magnet-like forces.
For subjects, the world was displayed on a computer screen
and updated in real time using a physics engine that approx-
imates Newton’s laws of motion (see Figure 1a). Subjects
could hold-click to “grab” onto any object then move their
cursor to “drag” that object around the scene, so altering how
the scene played out and often better revealing the physical
property they had been tasked with inferring. Bramley et
al. (2018) used a simulation-based inference model to assess
what information was generated by subjects, and contrasted
trials in which the subject’s goal was to identify the relative
masses of two objects against trials in which their goal was to
identify the pairwise force between two objects. They found
B
A
C D
(a) (b)
Figure 1: Visualisation of task environment adapted from
Bramley et al. (2018). Gray circle overlay shows grid of tar-
get locations and dashed line give an example control trajec-
tory in which B is grabbed and moved toward A. (b) Sample
of the interpolation functions used to create smooth control
trajectories.
that learners were able to gather information selectively rel-
evant to their learning goal, and exhibited markedly differ-
ent behaviours dependent on the goal that could be classi-
ﬁed into a classes of frequent experimental strategies such as
staging collisions by throwing objects at one another, shak-
ing them back and forth, bringing pairs of objects close to-
gether (dubbed encroaching). However, they did not model
how such strategies were learned or generalised successfully
across learning instances
In the current work, we take a step toward understanding
this ability. We use reinforcement learning to train an artiﬁ-
cial agent to minimize its own uncertainty about these speciﬁc
physical parameters by composing action sequences from an
action space qualitatively similar to human subjects’ mouse
movements, training this ability across a diverse set of ground
truth and initial conditions. We are interested whether, with
sufﬁcient training and subjective information as a reward sig-
nal, an agent can learn to produce robustly informative ac-
tion sequences, or strategies, for revealing particular physical
properties, and whether these will reﬂect the behaviors previ-
ously seen in human subjects.
Related Work
Understanding what drives information-seeking behavior has
long been a goal in psychology (see Schulz & Gershman,
2019, for a recent review). Some approaches attempt to tie
information seeking directly (e.g., Guez, Silver, & Dayan,
2012) or incidentally (e.g., Thompson, 1933) to the familiar
goal of extrinsic reward maximisation. Explicit approaches
require intensive preposterior planning, that is averaging be-
havioral prescriptions over many potential future learning and
reward trajectories (Raiffa, 1974), so are infeasible for com-
plex problems. Incidental approaches are computationally
cheaper but fail to capture information seeking behaviours
that depart from noisy extrinsic goal seeking (cf., Schulz,
Klenske, Bramley, & Speekenbrink, 2017).
The idea that the brain seeks information as a form of in-
trinsic reward captures a middle ground idea that that for hu-
mans, discovery is an “end in itself” driven by something
we intuitively understand as “curiosity” (Gottlieb, Oudeyer,
Lopes, & Baranes, 2013; Schmidhuber, 2010).
While some learning problems only occur once in a life-
time (e.g., we only need to ﬁgure out the laws of physics
once), many others occur repeatedly (we encounter new ob-
jects daily and would like to be able to rapidly infer their
most important properties). For these problems, there is
space to learn to actively learnby training reusable active
learning strategies through repeated experience,making them
amenable to reinforcement learning (Kober & Peters, 2012).
The task we explore is challenging in part because of the
continuous and complex nature of physical dynamics. How-
ever, combined with adequate function approximators such
as a deep neural network, reinforcement learning has proven
successful for optimising control in rich state spaces (Mnih,
Heess, Graves, et al., 2014). For example, Bachman, Sordoni,
and Trischler (2016) developed agents that can solve a collec-
tion of tasks which require active information seeking using
deep neural networks and reinforcement learning, including
cluttered MNIST (Mnih et al., 2014), Blockworld, CelebA
(Liu, Luo, Wang, & Tang, 2015) and Hangman.
This project sets apart from prior work on intrinsic curios-
ity in that it focuses on interventional rather than observa-
tional information seeking. In our physics learning task, ac-
tions have extended complex and far reaching consequences,
compounding the complexity of inference, but better mimick-
ing the challenges of learning in the natural world.
The task
Our interactive physical environment adapts the two-
dimensional physics-based environment from Bramley et al.
(2018) and is implemented using the pybox2d library.1 The
world is limited to a 6 ×4 meter bounded rectangle contain-
ing four circular objects, each with radius 0.25 meters. The
objects interact with one another and with the static walls
according to Newtonian physics and the latent properties of
mass and the pairwise forces. The complete ﬁxed settings of
the simulator are detailed at https://bit.ly/2B4TOAf.
Possible settings and initial uncertainty
As with Bramley et al. (2018), we will contrast actions fo-
cused on identifying objects’ masses against actions focused
1https://github.com/pybox2d/pybox2d
on identifying the forces relating each pair of objects. How-
ever, we explored a larger set of possible settings of these
values and correspondingly larger initial hypothesis space for
the agent than human participants. To ensure the agent had
equivalent initial uncertainty about both masses and forces,
we deﬁned a discrete space of 2 5 mass combinations and 2 5
force combinations and initialised the agent with a uniform
prior across all 2 10 distinct mass-force combinations. The
space of mass combinations is a subset of R4, in which each
parameter represents the mass of an object and is in the range
of [1, 3]kg. Meanwhile, the space of force combinations is a
subset of R4×4, in which each force parameter takes a value
∈(−3,0,3)m/s2, representing repulsive force, no interactive
force, and attractive force respectively for each pair of ob-
jects. Under the Newton’s second law of motion, different
mass-force combinations yield different accelerations, lead-
ing to distinct simulated trajectories, interactions and colli-
sions.
Like the human participants in Bramley et al. (2018), the
agent could “grab” one object at a time. The cursor would ex-
ert an elastic force (i.e., scaling continuously with the inverse
square of the distance between the object and the cursor) at-
tracting the controlled object to the cursor’s location until it
was released. All interactions and resulting trajectories were
simulated by the Box2D engine.
The Agent
Learning Framework
Reinforcement learning (RL) focuses on how agents learn
sequential control policies to maximize cumulative reward.
We assume (S,A,R,T) deﬁnes a Markov Decision Processes
(MDP) with state space S, action space A, reward function R
and transition dynamics T. Modern RL can be divided into
model-based and model-free approaches, where the former
ﬁrst learns a predictive a world model then uses the model
to learn a policy, while the latter learns an optimal policy di-
rectly from experience. We position our learning agent some-
what like a developing child learning about the physical prop-
erties of the world through experience but without much a
priori knowledge. Thus, we do not assume the agent has a
complete internal physics engine from which to draw samples
(model-based learning) but instead is learning how best to re-
veal information in a model-free manner evaluated against its
own learning progress (Oudeyer, Kaplan, & Hafner, 2007).
Action space
As mentioned above, human subjects exhibited rich and in-
formative behaviors when interacting with the objects in this
environment. However, many of the strategies identiﬁed in
Bramley et al. (2018) appeared to be composed of multiple
simpler movements. For instance, “shaking” involved grab-
bing then moving an object rapidly back and forth between
two locations while “throwing” involved grabbing an object
and releasing it at speed and in a direction such that it went
on to collide with another. To encode an action space ex-
pressive enough to incorporate these realistic and extended
human behaviors, we combined a a grid of target locations
(Figure 1a) with pool of easing (interpolation) functions (see
Figure 1b). Concretely for each action, the agent chose an ob-
ject to control (none, 1,2,3 or 4) and a cardinal direction (up,
down, left ,right), or a quadrant (up-right, up-left, down-left,
down-right) which determined a target location adjacent to its
current location. It then followed a path to that location deter-
mined by its selected interpolation function. Figure 1b shows
these functions in the ﬁrst quadrant such that the path would
be appropriately mirrored in the other three quadrants. 2 To-
gether with horizontal and vertical movements, and no move-
ment — i.e., the cursor pausing at its current position — our
action generator output a contiguous mouse trajectory, fol-
lowing the policy learned by our agent via the learning frame-
work demonstrated in the following section.
A complete learning episode consisted of a sequence of
these actions and would terminate when the agents’ uncer-
tainty about a target property of the environment reached a
threshold, or a timeout limit was reached. For simplicity we
assumed each action occurred across a ﬁxed time window,
and to ensure continuity of mouse movement, the start point
of each action was the end position for the last action. This
resulted in a continuous action trajectory decomposed into a
sequence of discrete action choices.
State deﬁnition
We deﬁned the motion of an object at every time step t with
scalars mt and φt , where mt represents the magnitude and φt
the direction of the object. Given a two-dimensional space,
mt =
√
v2xt +v2yt and φt = arctan vyt
vxt
, where vt = [vxt ,vyt ] is the
velocity vector of the object at time step t. Together with the
location tuple (xt ,yt ), the object state st ∈S is thereby a four-
dimensional vector [mt ,φt ,xt ,yt ].
Intrinsic Reward Signal
Our agents’ goal was to minimise its ﬁnal uncertainty about
either mass or force. Thus its reward signal was based on
computing its reduction in entropy with respect to the target
property (Shannon, 1951). Following Bramley et al. (2018),
we assumed likelihoods were computed based on divergence
between mental simulations and the observed trajectories.
Concretely, to infer how likely a mass-force combination,
w, is we assume actual observed dynamics are compared
against dynamics simulated assuming those properties. Let
w ∈W, where W is the space of all possible mass-force com-
binations. Before an episode starts, the agent has a uniform
prior over settings p(W). After a period of action and obser-
vation d, we assess the likelihood of the observed trajectory
o under all possible w, and use this to update the prior distri-
bution p(W|d).
Following Vul, Frank, Alvarez, and Tenenbaum (2009), we
modelled the likelihood of observing the object trajectories o
2All together there were (stay +→+ ↑+ ←+ ↓+ 31[functions]
×4[quadrants]) ×(4[objects] + no object)=645 possible actions.
given the potential property setting w and the mouse trajec-
tory a using a Gaussian error distribution:
p(o |w,a,β) =
T
∏
t=1
exp−β
2Σ (st −dt )⊤(st −dt ), (1)
where dt is the observed[mt ,φt ] produced by the true environ-
ment w′. The covariance matrix was Σ =
[σ2
m 0
0 σ2
φ
]
where
σ2
m and σ2
φ were set to the empirical standard deviations of
the disparities between simulations and the actual observa-
tions in Bramley et al. (2018). β was a scaling parameter
determining how conﬁdently the agent could perceive diver-
gences between the objects’ true and simulated trajectories.3
According to Bayes’ rule, once we measure the likelihood
through Equation 1, we can calculate the posterior of the po-
tential latent physics properties w by:
p(w |o,a,β) =1
Z p(o |w,a,β)p(w), (2)
where p(w) is the prior and Z is a normalizing constant. Us-
ing Shannon entropy (1951) as a measurement of the amount
of remaining uncertainty about w′, we formulated the imme-
diate reward after action t as:
rt = −( ∑
w∈W
p(w)log p(w)−∑
w∈W
p(w |o,a,β)log p(w |o,a,β)).
(3)
For particular parameters of interest (here masses of the four
objects), the posteriors and priors are marginalized over the
remaining parameters (in this case, the pairwise forces) when
estimating the parameter speciﬁc uncertainty reduction. The
resultant reward signal r embodies the amount of information
about the latent physics parameters (mass or force) a particu-
lar agent’s action has obtained.
Deep Q-Learning
Q-learning is a model-free learning algorithm that estimates
the expected cumulative discounted rewards of performing an
action from a given state (Watkins & Dayan, 1992). These
estimated cumulative discounted rewards are often called re-
turns and represented as Q-values (state-action values). One
way to iteratively learn Q-values is by using a Temporal Dif-
ference (TD) algorithm (Sutton, 1988). TD updates Q-values
by:
ˆQ(s,a) := (1 −α)Q(s,a)+ α(r +γmax
a′
Q(s′,a′)), (4)
where (s,a) is the current state-action pair,(s′,a′) is the state-
action pair at the next time step,γ is the discounted factor, and
α is a learning rate.
3Intuitively, β = 0 would result in no learning and βlim∞ would
lead to an implausibly powerful learner with inﬁnite perceptual pre-
cision. In our experiment, we assumed a constant β of 1
50 . We also
held a ﬁxed size of the time windowT of 1
6 seconds, over which for-
ward simulations were compared against observations before being
corrected.
... 
... 
environment 
(physical simulator) 
environment 
(physical simulator) 
Figure 2: Proposed (a) MLP and (b) RQN structures of our Q-learning framework. For every time period, the network takes a
sequence of object states and return the estimated Q-values. The control policy of the next time period, a sequence of mouse
trajectories, is determined by selecting the action corresponding with the largest Q-values over the action space.
A challenge for conventional Q-learning is to estimate Q-
values over continuous states and action spaces. For our task,
despite creating a discrete action space with interpolation
functions, the object’s locations and motion [m,φ] was con-
tinuous throughout the world. Computing separate Q-values
for every possible (s,a) pair at every time window is infeasi-
ble and na¨ıve given the smooth relationships between nearby
states. We hence adapted the deep Q-learning method, which
instead of updating Q-values of each discrete pair (s,a) in
tabular form, approximates ˆq(s,a) with deep neural network.
Denoting the parameters of the network as θ, a mean squared
error loss of the Q-values approximator can be written as:
l(s,a |θ) =1
2(r +γmax
a′
ˆq(s′,a′|θ)−ˆq(s,a |θ))2, (5)
where r + γmaxa′ ˆq(s′,a′|θ) is the target of our estimation.
Below, we present two neural network models, serving as the
function approximators ˆqθ. Optimal control strategies can be
discovered following this general learning structure.
Multilayer Perceptron First, we used a Multi-Layer Per-
ceptron (MLP) with three layers to approximate Q-values
(Figure 2a). For every time period t, the simulator fed the
agent a sequence of object states st that spanned over T time-
steps and offered a rewardrt to the MLP. The network outputs
a deterministic cursor trajectory that was the action at ∈A
with largest Q-value at t. We used ε-greedy exploration and
optimized Equation 5 via stochastic semi-gradient descent
(Watkins & Dayan, 1992). Note that the same network ˆ q
producing the next state target Q-values was used for com-
puting the loss of the current predictions. Such optimization
can yield erratic gradients when the control policy oscillates.
To deal with this potential instability, we redeﬁned the tar-
get as a target network. Instead of using the same network to
compute the next state target and the current state Q-values,
another network ˆqθ−, sharing the same structure as ˆqθ, was
utilized to compute target Q-values during the update. The
loss function in Equation 5 was then modiﬁed as:
l(s,a |θ,θ−) =1
2
[
r +γmax
a′
ˆq(s′,a′|θ−)−ˆq(s,a |θ)
]2. (6)
Following the ‘hard-copy’ update proposed in Mnih et al.
(2015), we froze θ− and copied θ into θ− once after a few
episodes. Experiments demonstrated that the MLP with tar-
get network model produced convergent and stable policies
faster than the simple MLP model.
Recurrent Q-Network For every forward pass, the func-
tion approximator ˆqθ should receive a sequence of object
states with length 16 T (four objects, each carrying a four-
dimensional vector [m,φ,x,y] that depicts the object’s motion
per time step). An MLP approximator, as introduced in the
previous section, consists of multiple fully-connected layers,
where each layer contains multiple neurons that accept and
send information across the network. The input motion vec-
tors are stacked and reshaped into a 16T-dimensional vector,
and then fed into the ﬁrst layer of the MLP. However, the state
vectors encode the objects’ movement information over a pe-
riod of time, and simply ﬂattening them may fail to capture
some parts of the underlying dynamics or spatial correlations
among the objects. We thereby propose a second RNN-based
function approximator (Figure 2b).
Instead of treating all the motion vectors equally without
order, RNN chronologically receives the motion vectors and
updates its hidden cells accordingly. The output hidden vec-
tor at the last time step included not only the object state infor-
mation but also their latent interconnections with the environ-
ment. To alleviate the vanishing gradient issue, we adapted
the Long Short-Term Memory cell (Hochreiter & Schmidhu-
ber, 1997) as the recurrent unit. A linear mapping function
g : S →A mapped the state representation to actions. We
optimize this recurrent Q-network (RQN) by semi-gradient
descent using the target networkmethod.
Experiments
In this section, we present some preliminary attempts at train-
ing these networks and comparing their behavior to that of
humans performing the same task. We compare the achieved
reward with respect to the agent’s goal against its reward for
the mismatched alternative goal to assess to what extent the
actions were selectively informative.
Training
For our physics simulator, 60 time-steps was equivalent to 1
second. We set the time-windowT as 40 time-steps, implying
that each atomic action would take roughly 0.7 seconds. It is
Figure 3: Reward and loss. (a) and (b) show total target and mismatched reward for each game, for the mass and force
exploration tasks respectively. (c) reports the training loss associated with all proposed models. For each test game, the
cumulative target rewards over actions are recorded and presented in (d).
nontrivial to determine when a simulated game should stop,
since uncertainty typically continues to diminish indeﬁnitely,
approaching but never reaching zero. For our agent, the total
uncertainty of the latent physical parameter of interest could
be obtained by computing the Shannon entropy of the initial
prior distribution p(W)0, denoted H (p(W)0). We denoted a
reward threshold factor γr = 0.95. Then, every time the cu-
mulative reward reached γrH(p(W)0), the game would stop.
Otherwise, the agent would continue playing until the current
game approached a timeout limit.
The three-layer MLP had 150, 250, and 450 neurons per
layer. The input state st of MLP was in R640, while for RQN,
it was R16x40. The approximated Q-values ˆqθ(s,a) ∈R645. We
initialized the exploration rate ε as 0.5, discounted by a factor
of 0.95 every 20 games until ε reached 0.01. The discount
factor γ was 0.99, and the weights of target network were
cloned from ˆqθ every 20 games.
A training set with 60 distinct ground truth w, initial ob-
ject locations, velocities, cursor positions, and initial veloci-
ties was created, and a holdout test set containing 20 different
conﬁgurations, are deﬁned beforehand to ensure the robust-
ness of the models. For each proposed model, we trained the
agent for the mass exploration task and the force exploration
task separately with 1000 episodes, and the training errors
and rewards are illustrated in Figure 3(a)-(c). Within each
task, both the target reward and the mismatched reward (i.e.,
force in a mass exploration task) are recorded. To better illus-
trate the moving trends, we weighted the training rewards and
the loss by exponentially weighted moving average (ewma)
with span 10.
Compared with the MLP-based models, RQN converged
faster with small and stable loss, as shown in Figure 3(c).
Both models produced swinging errors in the training pro-
cess. The intuition is that ‘hard’ copying weights fromθ into
θ−yielded a time interval when θ−was frozen and the pre-
dicted Q-values diverged from the target Q-values. Such os-
cillations would not distort the optimization of the objective
function, as the time interval was small andθ−was constantly
updated, ensuring steady amounts of information explored by
the agent. Given a ﬂexible timeout limit, both methods were
able to uncover the uncertainty of the environment, as de-
picted in Figure 3(a) and (b). Mass reward was more vari-
able. This is in line with the Bramley et al. (2018) ﬁnding that
evidence about mass tends to come in sporadic spikes when
objects collide or are moved rapidly, while force information
typically accumulate more smoothly whenever objects are in
close proximity. Thus, gathering mass information reliably
have depended on more speciﬁc and targeted actions while
moving objects closer together may have been sufﬁcient for
force information. Overall, the RQN exhibited continuing
amounts of achieved information on all latent physics param-
eters. We applied the trained RQN agent on the test conﬁg-
uration set for mass and force exploration separately, each
with 20 games. As reported in Figure 3(d), in most of the
cases the agent could effectively capture the underlying prop-
erties of the environment within 20 steps. Since explorations
were excluded from the testing, few ‘abnormal’ actions ap-
peared. For those rare cases, our agent quickly came back
to the right track and could still complete the learning task
within the timeout limit.
Using the trained models we created a small dataset of
10 force-focused episodes and 10 mass-focused episodes us-
ing a holdout set of new worlds and starting locations. See
https://bit.ly/2FYTjvD for videos. Comparing these
episodes, we found that achieved information reward was
similar for mass or force focused trials 2 .82 ±0.54, 2.96 ±
0.39, t(18) =.98, p = .32 but that signiﬁcantly more informa-
tion was generated for the property matched with the agents’
goal than the alternative property (see also lower orange and
blue lines in Figure 3a and b). That is, the agent generated
more evidence and reward on average about the target prop-
erty 3.1 ±0.35bits than the mismatched property 2 .7 ±0.52
bits t(18) =2.4, p = 0.021 similarly to human subjects in
Bramley et al. (2018).
Trained agent behaviour
The agent frequently moved the controlled object closer to
the other objects, reducing the distance to the closest object
from 1.28 ±0.24m to 1 .01 ±0.23m on average during each
control action t(38) =3.6, p < .001. As we see in Figure 4a,
the agent learned that moving was normally more informative
than staying still. There were also hints of goal dependent
control strategies similar related to those identiﬁed in Bram-
ley et al. (2018). For example, the most frequently selected
Figure 4: Distributions of emerged actions in 20 test games (10 for mass focused and 10 for force focused). (a) Sum of the
quadrant distribution of mass and force tasks. ‘0’ means the agent staying over at its position within the time-windows. (b)
Spline distribution of mass task (c) Spline distribution of force task
interpolation by the mass focused agent was bounceEaseOut
(Figure 4b and green highlight in Figure 1b), a particularly
dynamic motion consistent with the rapid changes of direc-
tion associated with shaking or knocking observed frequently
in the human data on mass focused trials. The intuition both
there and here is that these actions strongly reveal objects
mass by causing rapid changes in objects’ directions. Mean-
while, the most frequent interpolation selected by force fo-
cused agent was cubicEaseInOut (Figure 4c and red highlight
Figure 1b), a smooth motion intuitively consistent with the
“encroaching” behavior observed frequently in force trials in
Bramley et al. (2018) and effective in providing strong evi-
dence about mass.
Discussion and Conclusions
Humans display sophisticated intervention strategies when
actively inferring the properties of physical objects. We used
model-free reinforcement learning, deep function approxima-
tion, and simulation based inference to build an agent able to
efﬁciently reveal the latent physical properties in human-like
ways without external input. Part of the insight gleaned from
this project comes from our solutions to the engineering chal-
lenges involved in creating a successful agent. To produce ex-
tended actions with richness and qualitative correspondence
with humans’, we found success with an action space that
combined a discrete set of target locations with a discrete set
of smoothing splines. We found that learning to associate ac-
tion sequences with successful resolution of uncertainty was
much more effective with a recurrent network architecture,
but that robust strategies could be learned through model-free
Q-learning. Following the predicted optimal control policies,
not only did the agent uncover the latent parameters of inter-
est, but there were also hints of behavioral correspondence
with human subjects in that our mass trained agent would se-
lect more jagged and dynamic trajectories aligned with the
strategies observed in Bramley et al. (2018).
While this study provides a valuable ﬁrst step to under-
standing how humans learn and apply rich interrogatory be-
haviours when interacting with the natural world, it also has
its limitations. One of these is the use of the physics simu-
lator to calculate the reward signal. This is a rational ideal-
isation of physics inference, but embodies the overly strong
assumption that the agent is able to simulate the world ac-
curately and perform approximate Bayesian inference with
its own interaction data. A more plausible and practically
viable approach to rewarding informative control would be
to train a separate prediction network to anticipate upcoming
dynamics, and use some function of its loss over time as a re-
ward signal (cf. Oudeyer et al., 2007; Pathak, Agrawal, Efros,
& Darrell, 2017). A more integrated agent could also use
the predictor network approximation to plan actions that are
likely to be informative through preplay Chentanez, Barto,
and Singh (2005). Following Haber, Mrowca, Fei-Fei, and
Yamins (2018), a complex adversarial-based learning frame-
work may be helpful. In future work we plan to combine
more realistic intrinsic rewards, richer action space and model
based planning to better mimic the ability of humans to create
intuitive physical experiments.
References
Bachman, P., Sordoni, A., & Trischler, A. (2016). Towards information-seeking
agents. arXiv preprint arXiv:1612.02605.
Bramley, N. R., Gerstenberg, T., Tenenbaum, J. B., & Gureckis, T. M. (2018).
Intuitive experimentation in the physical world. Cognitive Psychology, 105, 9 -
38.
Chentanez, N., Barto, A. G., & Singh, S. P. (2005). Intrinsically motivated rein-
forcement learning. In Advances in neural information processing systems(pp.
1281–1288).
Gottlieb, J., Oudeyer, P.-Y ., Lopes, M., & Baranes, A. (2013). Information-seeking,
curiosity, and attention: computational and neural mechanisms. Trends in cogni-
tive sciences, 17(11), 585–593.
Guez, A., Silver, D., & Dayan, P. (2012). Efﬁcient bayes-adaptive reinforcement
learning using sample-based search. In Advances in neural information process-
ing systems(pp. 1025–1033).
Haber, N., Mrowca, D., Fei-Fei, L., & Yamins, D. L. (2018). Learning to play with
intrinsically-motivated self-aware agents. arXiv preprint arXiv:1802.07442.
Hochreiter, S., & Schmidhuber, J. (1997, November). Long short-term memory.
Neural Comput., 9(8), 1735–1780.
Kober, J., & Peters, J. (2012). Reinforcement learning in robotics: A survey. In
Reinforcement learning(pp. 579–610). Springer.
Liu, Z., Luo, P., Wang, X., & Tang, X. (2015). Deep learning face attributes in
the wild. In Proceedings of the ieee international conference on computer vision
(pp. 3730–3738).
Mnih, V ., Heess, N., Graves, A., et al. (2014). Recurrent models of visual attention.
In Advances in neural information processing systems(pp. 2204–2212).
Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., . . .
Hassabis, D. (2015). Human-level control through deep reinforcement learning.
Nature, 518, 529–533.
Oudeyer, P.-Y ., Kaplan, F., & Hafner, V. V . (2007). Intrinsic motivation systems for
autonomous mental development. IEEE transactions on evolutionary computa-
tion, 11(2), 265–286.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven ex-
ploration by self-supervised prediction. In International conference on machine
learning (icml)(V ol. 2017).
Raiffa, H. (1974). Applied statistical decision theory. Wiley.
Schmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation
(1990–2010). IEEE Transactions on Autonomous Mental Development, 2(3),
230–247.
Schulz, E., & Gershman, S. J. (2019). The algorithmic architecture of exploration
in the human brain. Current opinion in neurobiology, 55, 7–14.
Schulz, E., Klenske, E., Bramley, N., & Speekenbrink, M. (2017). Strategic explo-
ration in human adaptive control. bioRxiv, 110486.
Shannon, C. E. (1951, 01). Prediction and entropy of printed english. Bell System
Technical Journal, 30, 50-64.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences.
Machine Learning, 3, 9-44.
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds
another in view of the evidence of two samples. Biometrika, 25(3/4), 285–294.
Vul, E., Frank, M. C., Alvarez, G. A., & Tenenbaum, J. B. (2009). Explaining
human multiple object tracking as resource-constrained approximate inference
in a dynamic probabilistic model. In Advances in neural information processing
systems (p. 1955-1963).
Watkins, C. J., & Dayan, P. (1992). Q-learning.Machine learning, 8(3-4), 279–292.