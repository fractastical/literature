Active Inference in String Diagrams: A Categorical
Account of Predictive Processing and Free Energy
Sean Tull1,2, Johannes Kleiner2,3,4, and Toby St Clere Smithe5,6
1Quantinuum
2Association for Mathematical Consciousness Science
3Ludwig Maximilian University of Munich
4University of Bamberg
5Topos Institute
6VERSES Research
Abstract
We present a categorical formulation of the cognitive frameworks of Predictive Processing (PP) and
Active Inference, expressed in terms of string diagrams interpreted in a monoidal category with copying
and discarding. This includes diagrammatic accounts of generative models, Bayesian updating, percep-
tion, planning, active inference, and free energy. In particular we present a diagrammatic derivation of
the formula for active inference via free energy minimisation, and establish a compositionality property
for free energy, allowing free energy to be applied at all levels of an agent’s generative model. Aside from
aiming to provide a helpful graphical language for those familiar with active inference, we conversely hope
that this article may provide a concise formulation and introduction to the framework.
1 Introduction
Predictive processing (PP) is a framework for modelling cognition and adaptive behaviour in both biolog-
ical and artificial systems [WM17, Hoh20]. A prominent sub-field is the programme of Active Inference,
developed by Friston and collaborators [SFW22, PPF22, FFR+17, SBPF21], which aims to provide a unified
understanding of cognition and action which can be applied at many levels, from a single neuron to an entire
brain or organism. More specifically, active inference gives a proposal for how a cognitive agent represents
its own beliefs about the world, how it updates these beliefs in light of new observations, and how it chooses
the actions it takes, with the latter ultimately leading to new observations.
Central to the framework is that an agent possesses a generative model which explains its observations
causally in terms of both hidden states of the world and its own actions. Note that this model is internal to
the agent, and typically distinct from the ‘true’ causal process in the world which produces the observations.
After receiving an observation, the agent may update this generative model to determine likely hidden states
which caused the observation (the process of perception) and choose its actions (the process of planning). In
active inference, both forms of updating are carried out together through a form of approximate Bayesian
inference, by minimising a quantity known as free energy [FKH06, Fri10].
While active inference seeks a principled account of cognition, at present its formalisation can seem fairly
complex, and there are various aspects which do not follow immediately from simply applying the definitions
to a given generative model. Conceptually clear formal accounts of the framework would be desirable to
simplify the theory and address these issues, as well as for applications within AI.
One hope for such a formal account would be for it to be both compositional and graphical. Indeed the
generative models in PP are highly structured, often given as ‘hierarchical models’ [DVF17] which are best
represented diagrammatically in terms of probabilistic graphical models such as Bayesian networks. While
there has been support for, and steps towards, a graphical account of active inference [FPdV17], so far the
graphical aspects only formally describe the structure of a generative model, while other aspects such as
1
arXiv:2308.00861v1  [math.CT]  1 Aug 2023
updating and free energy are still treated through traditional probabilistic calculations, and only informally
in diagrams.
Recently however, fully formal diagrammatic methods have been developed for both describing Bayesian
networks and carrying out probabilistic reasoning about them. These approaches are based on (monoidal)
category theory and its associated graphical language of string diagrams [PZ23]. Category theory has been
applied across the sciences as a general mathematics of interacting processes, including within probability
theory [CS12, CJ19], causality [JKZ19, FK23, LT23], game theory [GHWZ18], machine learning [FST19,
SGW21], quantum computing [AC04] and natural language processing [CCS08]. In particular a major ongoing
development is in the study of probabilistic processes in terms of cd-categories (and ‘Markov categories’),
which allow one to carry out probabilistic reasoning entirely through string diagrams [Fri20].
In this work we give a full categorical account of predictive processing and active inference in terms of
string diagrams, interpreted in cd-categories. In doing so we aim to give a conceptually clear account of the
main features of the framework: generative models, Bayesian updating (including with soft observations),
perception, action planning, and their combination in active inference, and both variational and expected
free energy.
A highlight is a fully graphical derivation of the well-known formula for active inference in terms of
minimisation of free energy. While this is a central result within active inference, its usual justification is more
heuristic in nature. Here we instead derive the free energy formula purely graphically from a diagrammatic
account of active inference itself, providing what we argue is the most transparent account of this result
known so far.
The categorical perspective also naturally leads us to consider more novel aspects of active inference.
These include the definition of open generative models (essentially from [LT23]) which are generative models
coming with ‘inputs’, allowing them to serve as the building blocks of an overall generative model.
We also introduce a notion of variational free energy for open models which allows us to establish the
desirable property that free energy is compositional. Namely, a system with an overall generative model
composed from sub-models may minimise global VFE by minimising VFE locally within each component.
This is a crucial fact in order to apply free energy as proposed to all levels of a system, say from a whole
brain down to its individual neurons.
Overall, we hope that our diagrammatic accounts of PP can provide a conceptually clear view of the
framework, and also a natural language for reasoning within it. Indeed, as argued for example in [LT23]
and elsewhere [JKZ19] diagrams in cd-categories provide a natural way to both represent causal (generative)
models, as well as reason about them. As we demonstrate here they are also natural for describing the struc-
ture of active inference, including free energy. Aside from aiming to provide a helpful graphical language for
those familiar with active inference, we conversely hope that this article may provide a succinct introduction
to PP for those already familiar with string diagrams and categorical reasoning.
Further motivations Though primarily a framework for cognition, various proposals have been put for-
ward for how predictive processing may be related to consciousness [WM17]. In previous work, two of the
authors developed a categorical account of the Integrated Information Theory of consciousness, again essen-
tially using cd-categories [TK21, KT21] and based on the work here we hope to give a categorical account of
how consciousness may be accounted for within PP [Dea21, HS20]. We also see this work as a piece of the
programme of Compositional Intelligence, which explores how categorically structured models and processes
can be applied to (artificial) intelligence. Specifically, PP may be seen as a proposal for how compositional
intelligence manifests in biology; that is, how biological systems may employ compositionality to carry out
intelligent and adaptive behaviour.
Active inference can also be understood as an alternate proposal to reinforcement learning (RL) for how
agents can learn adaptive behaviour, and shares similar features including the role of probabliistic models
and inference [TMSB20]. It differs from conventional RL by replacing an explicit reward function with the
aim of maximizing evidence for a probabilistic model, where the agent’s preferences are now encoded in the
model’s prior distribution [FDK09].
Related work This work can be seen as a part of the growing field of ‘categorical cybernetics’ [Smi21b,
CGHR21], including previous work from one of the authors on compositional accounts of Bayesian updating
[Smi20] and of active inference in terms of ‘statistical games’ [Smi21a, Smi22]. It differs from previous
2
BA
Ot−1
A
St
Ot
B
A
Ot+1
St−1
St+1
P
ED
Figure 1:
A generative model diagram from the recent book Active Inference by Parr, Pezzulo and Friston [PPF22]
(left) and the equivalent string diagram representation (right) (though replacing the informal EFE term G
with the prior E).
works by directly formalising the active inference framework itself, and by working explicitly graphically
within the simple string-diagrammatic setting of cd-categories, with the aim of supplying a simple abstract
characterization of active inference agents.
In this way the work is a part of a general movement in applying string diagrams in cd-categories to
probability theory and causal reasoning. A categorical account of Bayesian inversion was first given by
Coecke and Spekkens in [CS12], and then within cd-categories by Cho and Jacobs [CJ19], with further
developments in categorical probability by Fritz [Fri20]. Our diagrammatic account of generative models is
precisely that given for causal models in part by one of the authors in [LT23], which builds on the earlier
categorical treatments of (causal) Bayesian networks by Fong [Fon13], Jacobs et al [JKZ19] and others e.g.
[FK23]. Indeed, as an agent’s explanation for the observations it receives from the world, a generative model
is ultimately a causal model [Pea09], though this is not often stressed in the literature.
The two forms of soft Bayesian updating treated here we first studied by Jacobs in [Jac19]. The specific
treatment of conditioning in cd-categories used here is from [LT23]. Cd-categories with (non-unique) condi-
tionals have also been recently studied as ‘partial Markov categories’ in [DLR23], along with both notions
of updating. Our treatment of free energy refers to the KL divergence of distributions; we note that an
axiomatic treatment of Markov categories coming with divergences on their morphisms has recently been
given by Perrone in [Per22].
Within active inference itself, graphical aspects have been increasingly prominent, with discussion of the
‘graphical brain’ in [FPdV17]. In such works it is argued that one may describe models as (non-directed)
Forney Factor Graphs (FFGs) [DVF17]. However, generative models are inherently directed, going from
states to observations with the other direction being intractable to compute exactly. Thus it is more natural
to treat models using (generalisations of) directed Bayesian networks. Nonetheless we note though that FFGs
derived from a model still have a role when minimising VFE via ‘message passing’ algorithms [PMKF19].
Interestingly, while a Bayesian network is typically depicted as a DAG (with only the variables labelled),
one may argue the diagrams in active inference have been naturally ‘converging’ on their string diagrammatic
representation, which also includes labels on the channels; see Figure 1. We claim that the advantage of
string diagrams beyond DAGs is in allowing one to both represent and reason about the model in the same
formalism.
We note that the diagrams in PP are at times only semi-formal, including aspects such as the free energy
which are not strictly part of the generative model. One may see this work as a step towards the shared goal
of representing formally all aspects of PP within one language of diagrams.
Structure of the article We begin in Section 2 by introducing cd-categories and their diagrammatic
account of probability theory. We then apply these to introduce from scratch the key aspects of PP: generative
models as Bayesian networks, and their generalisation to open generative models in a cd-category (Section 3),
3
(Bayesian) updating of generative models from observations (Section 4 ), perception and planning (Section
5) and their combination in exact active inference (Section 6). We then discuss free energy (Section 7) and
give a graphical derivation of active inference via free energy minimisation (Section 8). In Section 9 we then
introduce free energy for open models using a graphical formalism of ‘log-boxes’ and use this to establish the
compositionality property of free energy. Finally we discuss future work in Section 10.
Acknowledgements We thank Robin Lorenz for helpful discussions and development of the treatment
of causal models used here. This research was supported by grant number FQXi-RFP-CPW-2018 from
the Foundational Questions Institute and Fetzer Franklin Fund, a donor advised fund of the Silicon Valley
Community Foundation. Sean Tull would also like to thank Quantinuum for their generous support in this
research. Johannes Kleiner would like to thank the Mathematical Institute of the University of Oxford for
hosting him while working on this research.
2 Categorical Setup
Let us begin by introducing the graphical treatment of probabilistic processes in terms of string diagrams,
developed by numerous authors [CS12, CJ19, Fri20]. Formally, these correspond to working in a ‘monoidal
category’ or more specifically a ‘cd-category’, but in practice one may avoid mathematical details and simply
work with the diagrams themselves. Though cd-categories are very general, in this article it suffices to
consider the category MatR+ of R+ valued finite matrices, introduced shortly in Example 1.
A category C consists of a collection of objects X, Y, . . .and morphisms or processes f : X → Y between
them, which we can compose in sequence. In string diagrams we depict an object X as a wire and a morphism
f : X → Y as a box with lower input wire X and upper output wire Y , read from bottom to top.
f
Y
X
Given another morphism g : Y → Z we can compose them to yield a morphism g ◦ f : X → Z, depicted as:
g ◦ f =
X
X
Z
Z
f
g
Y
Each object X also comes with an identity morphism idX : X → X depicted as a blank wire:
idX =
X X
XX
The identity leaves any morphism alone under composition, that is id Y ◦f = f = f ◦idX for any f : X → Y .
Formally, a symmetric monoidal category (C, ⊗, I) is a category C with a functor ⊗: C × C → C, and
natural transformations which express that ⊗ is suitably associative and symmetric, with a distinguished
unit object I [Coe06]. All of these aspects however are expressed most simply in diagrams.
Firstly, the tensor ⊗ allows us to compose any pair of objects X, Yinto an object X ⊗ Y , depicted by
placing wires side-by-side.
X ⊗ Y
X ⊗ Y
=
X
X
Y
Y
4
Given morphisms f : X → W and g : Y → Z we can similarly form their ‘parallel composite’ f ⊗g : X ⊗Y →
W ⊗ Z as below.
f ⊗ g
X ⊗ Y
W ⊗ Z
= f
X
W
g
Y
Z
In text we will at times omit the tensor symbols and write e.g. ‘ f from X to Y, Z’ or f : X → Y, Zin place
of f : X → Y ⊗ Z.
The tensor is symmetric so we can ‘swap’ pairs of wires past each other, such that swapping twice returns
the identity, and boxes carry along the swaps as below.
f g
X
ZW
Y
= fg
W
Y X
Z
We also have a distinguished unit object I whose identity morphism we depict simply as empty space, and
denote by 1.
I
I
= = 1 (1)
Intuitively, tensoring any object by the unit simply leaves it invariant. The unit allows us to consider
morphisms with ‘no inputs’ and/or ‘no outputs’ in diagrams. A morphism ω : I → X is called a state of X,
depicted with no input. An effect on X is a morphism e: X → I, depicted with no output. A morphism
r: I → I, drawn with no input or output, is called a scalar.
ω
X
e
X
r
In particular the ‘empty space’ diagram (1) is the scalar 1 = id I.
The compositions ◦, ⊗ satisfy axioms which must be considered when working symbolically but are trivial
in the graphical language. An example is associativity of composition (h◦g)◦f = h◦(g◦f), which is automatic
from simply drawing three boxes in sequence on the same wire. Similarly the rule (f ⊗g)◦(f′⊗g′) = (f ◦f′)⊗
(g◦g′), displayed in the left identity below, and the ‘interchange law’ (f⊗id)◦(id⊗g) = f⊗g = (id⊗g)◦(f⊗id),
displayed in the right identity below, amount to letting us freely slide boxes along wires.
f
g′f′
g
=
f
g′f′
g
f
g = f g = f
g
Let us now introduce our primary example category in this article.
Example 1. In the category MatR+ of positive valued matrices, the objects are finite sets X, Y, . . .and the
morphisms M : X → Y are functions M : X × Y → R+ where R+ := {r ∈ R | r ≥ 0}. Equivalently such a
function is given by an ‘ X × Y matrix’ with entries M(y | x) := M(x, y) ∈ R+ for x ∈ X, y ∈ Y .
M
Y
X
:: ( x, y) 7→ M(y | x)
5
Composition of M : X → Y and N : Y → Z is given by summation over Y :
M
Z
X
N
Y :: ( x, z) 7→
X
y∈Y
N(z | y)M(y | x)
The tensor ⊗ is given on objects by the Cartesian product X ⊗ Y = X × Y , and on morphisms by the
Kronecker product, i.e. the usual tensor product of matrices:
M
W
X
N
Z
Y
:: (( x, y), (w, z)) 7→ M(w | x)N(z | y)
The symmetry is simply the isomorphism X × Y ≃ Y × X. The unit object I = {⋆} is the singleton set. A
state of X is then equivalent to a positive function on X:
ω
X
:: x 7→ ω(x)
where ω(x) := ω(x | ⋆). Similarly, an effect e on X is also equivalent to a positive function on X via
e(x) := e(⋆ | x).
e
X :: x 7→ e(x)
Finally, a scalar is precisely a positive real r ∈ R+.
2.1 Cd-categories
Many aspects of probability theory can be treated entirely diagrammatically, by noting that categories such
as MatR+ come with the following further structure.
Definition 2. [CJ19] A cd-category ( copy-discard category) is a symmetric monoidal category in which
each object comes with a specified pair of morphisms
called copying and discarding, respectively, which satisfy the following:
= = = =
The choice of these morphisms is moreover ‘natural’ in that the following hold for all objects X, Y.
X ⊗ Y
=
X Y X ⊗ Y
=
X Y I
= 1 (2)
6
Thanks to these axioms for copying, we can unambiguously define a copying morphism with n output
legs, for any n ≥ 1, via:
. . .
:=
. . .
with the n = 0 case defined to be discarding .
The presence of discarding allows us to identify the truly ‘probabilistic’ processes in a cd-category. We
say that a morphism f is a channel when it preserves discarding, as below.
f =
In particular, we call a state ω normalised when the following holds. For an explanation of why this gives
the usual definition, cf. Example 3 below.
ω
= 1
Here we will often call a normalised state ω of X a distribution of X, even when working in a general
cd-category 1. We also call a normalised state of X ⊗ Y a joint distribution over X, Y.
A cd-category in which every morphism is a channel, or equivalently is the unique effect on any object,
is called a Markov category [Fri20]. Given any cd-category C, its subcategory Cchannel of channels always
forms a Markov category.
Discarding allows us to ‘ignore’ certain outputs of a process. Given any morphism f from X to Y, Z, its
marginal X → Y is the following morphism:
f
Y
Z
X
Let us see how these features describe discrete probability theory within our example category.
Example 3. Mat R+ is a cd-category. Copying on X is given (y, z| x) = δx,y,z with value 1 iff x = y = z
and 0 otherwise. Discarding on X is given by the function with x 7→ 1 for all x ∈ X. Hence a state ω is
normalised, i.e. forms a distribution on X, precisely when it forms a probability distribution over X in the
usual sense, i.e. its values sum to 1.
ω
X =
X
x∈X
ω(x) = 1
More generally, a process M : X → Y is a channel iff it forms a probability channel, or equivalently a
stochastic matrix, meaning that it sends each x ∈ X to a normalised distribution M(y | x) over Y . Indeed
we have that:
M
Y
X
:: x 7→
X
y
M(y | x)
1This is to avoid confusion with the usual use of the term (hidden) ‘state’ in PP.
7
Hence M is a channel iff this effect is constant at 1, i.e. for all x we have
X
y∈Y
M(y | x) = 1
In typical probability theory, such a channel is also often called a ‘conditional probability distribution’
P(Y | X) with values denoted P(y | x) := P(Y = y | X = x) for x ∈ X, y ∈ Y . The subcategory of channels
in MatR+ is the Markov category FStoch of finite Stochastic matrices.
Let us see how a few features of probability theory appear in diagrams. Firstly, for any X, Y, a distribution
ω on X ⊗ Y corresponds to a joint distribution over X, Y (left-hand below). In particular given a pair of
distributions ϕ, σover X, Y, the distribution ϕ ⊗ σ corresponds to the resulting product distribution over
X × Y , with X and Y independent from each-other (right-hand below).
ω
X Y
ϕ σ
X Y
A general channel as below represents a probability channel P(Y1, . . . , Ym | X1, . . . , Xn).
P
Y1 Ym
X1 Xn
. . .
. . .
Marginalisation of any morphism corresponds to the usual notion in probability theory, given by summation
over the discarded object.
ω
X
Y
:: x 7→
X
y∈Y
ω(x, y) M
Y
Z
X
:: (x, y) 7→
X
z∈Z
M(y, z| x)
Finally we observe that for any effect e: X → R+ and distribution ω the scalar e ◦ ω corresponds to the
expectation value of the function e according to the probability distribution ω.
E
x∼ω
e(x) =
ω
e
X =
X
x∈X
e(x)ω(x)
2.2 Sharp states and caps
The copying morphisms in a cd-category allow us to identify those states which are really ‘deterministic’
[Fri20]. We call a state x sharp, and depict it with a triangle as below, when it is copied by , that is:
X X
x
=
x x
X X
(3)
In many categories there is a corresponding effect for each state, playing an important role for sharp states,
thanks to the following feature. We say that C has caps when each object comes with a distinguished effect
8
on X ⊗ X depicted and satisfying:
= = = (4)
and such that the following holds for all objects X, Y:
X ⊗ Y X ⊗ Y
=
X XY Y
Intuitively, the cap is an effect which checks if its two input wires are ‘in the same state’. The first equation
in (4) expresses that this comparison is symmetric, and the remaining two that it is compatible with copying;
for example the second says that each input when copied is equal to its copy.
Practically, caps allow us to ‘turn outputs into inputs’. In particular, for each state ω we can define a
corresponding effect by ‘flipping ω upside-down’:
ωω = (5)
When ω = x is a sharp state, we call this effect sharp also. One may verify that it is the unique effect
satisfying the following.
x
x
= 1
x x
=
x
(6)
Caps are particularly useful in diagrammatic reasoning when they are cancellative, meaning that:
f g= =⇒ f g=
for all morphisms f, g.
Example 4. Mat R+ has cancellative caps. Each point x ∈ X corresponds to a normalised sharp state on X
which we again denote by x, given by the point probability distribution δx at X.
x
X
:: y 7→
(
1 x = y
0 otherwise
The corresponding effect is given by the function δx also. Each cap is given by (x, y) = δx,y. We note a
useful fact that for any morphism M : X → Y its values M(y | x) can be given diagrammatically as below.
M(y | x) =
y
x
M
Every sharp state on X is of the above form for some x ∈ X, or else given by the zero state 0 defined
by 0(x) = 0 for all x ∈ X. The only sharp scalars are 0 and 1. Note that a general state ω, even when
normalised, is not copyable.
X X
ω
̸=
ω ω
X X
9
Indeed the left-hand side is the distribution (x, y) 7→ ω(x)δx,y, while the right is (x, y) 7→ ω(x)ω(y), which
differ unless ω is zero or ω = δx for some x ∈ X.
2.3 Normalisation
In graphical probabilistic reasoning it is also useful to be able to normalise states and processes. We say that
a cd-category C has normalisation when it comes with a rule assigning each morphism f : X → Y a new
morphism called the normalisation of f, depicted by drawing a dashed blue box:
f (7)
such that these normalisations satisfy various axioms, of which we sketch a few here. For a full definition see
[LT23]. Firstly, a general state ω is equal to a scalar multiple of its normalisation. In particular in MatR+
when the state is non-zero, this means that its normalisation is indeed normalised in our above sense, i.e. a
distribution.
ωω = ω (8)
For a general morphism f its normalisation is given on each sharp state x by normalising f ◦ x.
f
x
f
x
= (9)
These two rules combine to give the following equation without explicit reference to states.
f = f f (10)
Note that if f we already a channel then it would be equal to its normalisation, as in this case we can passing
the discarding through f and then the copy map above. In general normalisations satisfy a few graphical
conditions including the following.
f g = f g
f
=
f
(11)
Further, for all morphisms f and channels g we have:
=
f
g
f
g
(12)
10
and for all sharp states x and morphisms f we have the following.
f
x
= f
x
For a full account of the properties of normalisation see [LT23]. We note that for a general morphism f,
its normalisation is not necessarily a channel but only a ‘partial channel’ 2. In terms of states, this is because
its sends each sharp state x either to a normalised state, or else to 0 if f ◦ x = 0. However in MatR+ it will
be a channel provided f has ‘full support’, so that f ◦ x is non-zero for all non-zero sharp states x.
Throughout the article, the following notation will be useful. For any set X and function f : X → R+ let
us write
Norm
x
f(x) := f(x)P
x′∈X f(x′)
whenever this is well-defined, i.e. the denominator is finite and non-zero.
Example 5. Mat R+ has normalisation. On each object X the zero state 0, given by 0(x) = 0 for all x ∈ X,
is defined to have normalisation 0. For any non-zero state ω we indeed have
ω
X
:: x 7→ Norm
x
ω(x)
For a general morphism M : X → Y the normalisation is given by:
M
Y
X
:: ( x, y) 7→
(
Normy M(y | x) if P
y∈Y M(y | x) ̸= 0
0 otherwise
As a result if M has full support, so that M(y | x) ̸= 0 for some y, for all x, then its normalisation is a
probability channel.
2.4 Further cd-categories
Though we will not need them here, we note that the notion of a cd-category is much more general than
MatR+, and give a few examples for those familiar with them. The category Rel whose objects are sets and
morphisms are relations is a cd-category, as are its subcategories PFun of sets and partial functions and Set
of sets and functions, with the latter forming the channels in PFun.
There are also many more cd-categories of a ‘probabilistic’ nature, see for instance [Pan98, CJ19, Fri20].
In particular to treat general probability spaces (including ‘continuous probability channels’) one may work
in the category Kl( G) of measurable spaces X = (X, ΣX) and Markov (sub-)kernels f : X → Y , which send
each x ∈ X to a (sub-)probability measure f(x) over Y . Roughly, this means replacing all instances of
summation Σ in MatR+ above with integration
R
. Of particular interest in PP is the following subcategory,
though we will not work with it in detail in this article.
Example 6. [Fri20, Section 6] In the category Gauss the objects are spaces X = Rn and morphisms
M : X → Y are Markov kernels f : X → Y with densities of the form f(y | x) = η(y − Mx) for some fixed
Gaussian noise distribution η (independent of x) and linear map M : X → Y . This category models linear
processes with Gaussian noise. More general non-linear Gaussian processes are studied in PP under the
so-called ‘Laplace assumption’.
2Such morphisms are called ’quasi-total’ in [DLR23], where morphisms satisfying (10) are also called ‘normalisations’, though
are not uniquely chosen unlike our definition.
11
3 Generative Models
A central feature in PP is that each cognitive agent possesses a generative model which describes their internal
beliefs about how the observations they receives arise from hidden states of the world 3. In its simplest form,
a generative model consists of a channel c: S → O describing how likely c(o | s) a given observation o ∈ O is
for each hidden state s ∈ S, along with a distribution σ over S describing prior beliefs about how likely each
state is.
However, generative models typically come with further compositional structure, relating various spaces
of observations and hidden states, as formalised by a Bayesian network (or more precisely a causal Bayesian
network, see later discussion), a probabilistic graphical model based on a directed acyclic graph (DAG).
There is in fact a close correspondence between DAGs and cd-categories, allowing us to describe and study
such models entirely in terms of string diagrams. This view also leads one to consider more general ‘open
generative models’, which may come with ‘input’ variables. These open models can be used to which describe
the individual components of an overall generative model in the usual sense. For more details on the approach
used here, see [LT23].
We begin by relating DAGs with the following class of string diagrams.
Definition 7. [LT23] A network diagram is a string diagram D built from single-output boxes, copy maps
and discarding:
. . .
with labellings on the wires, such that any wires not connected by a sequence of copy maps are given distinct
labels, and each label appears as an output at most once and as an input to any given box at most once.
Such diagrams are best understood by examples, which we come to shortly. Before this, we note that
network diagrams are in fact equivalent to DAGs in the following sense. By an open DAG we mean a
finite DAG G with vertices V = {X1, . . . , Xn}, along with subsets I, O⊆ V of input and output vertices,
respectively, such that each input vertex has no parents in G.
Given any open DAG G, we may construct an equivalent network diagram featuring a box ci with output
Xi for each non-input vertex Xi. The box ci itself has an input wire for each parent of Xi in G. In the
diagram we copy the output of this box and pass it to each of the children of Xi, as well as an extra time if
Xi ∈ O i.e. Xi is an output vertex of the DAG.
Xi
Y1 Yk
. . .
. . .
7→ ci
Y1 Yk
. . .
. . .
Xi
By construction, this yields a network diagram DG from the inputs I to the outputs O of the DAG.
Conversely, given any such network diagram D we define an open DAG GD = (G, I, O) with a vertex X ∈ V
for each wire X in D, and with X ∈ I, Oiff it is an input (resp. output) to the diagram.
In practice the labellings of the boxes are arbitrary, and we consider any two network diagrams equivalent
when they are the same up to the equations of a cd-category and box re-labellings. Then the above yields a
one-to-one correspondence between open DAGs and network diagrams [LT23, Sections 3,5].
Example 8. Consider the open DAG G over {X1, X2, X3, X4} below, with output vertices O = {X2, X3}
circled, and with no input vertices. The equivalent network diagram DG is shown to the right. Note that the
3Note that is distinct from whatever ‘true’ external process produces the observations in reality, with the latter often called
the ‘generative process’ to distinguish it from the agent’s own ‘generative model’ [PPF22].
12
labels of the boxes are arbitrary.
X1
X2
X3
X4
⇐⇒
a b
c
d
X3
X1
X2
X4
Example 9. The following depicts an open DAG over V = {X1, . . . , X5} with outputs O = {X3, X5} and
with inputs I = {X2, X3} highlighted with special incoming arrows. To the right we show the corresponding
network diagram with the same inputs and outputs.
X1
X4
X2 X3
X5
⇐⇒
X1
X2 X3
X5
a
b
c
X3
X4
We may now define generative models themselves, which involve specifying actual channels corresponding
to the boxes in the network diagram.
An interpretation J−K of a network diagram D in a cd-category C consists of specifying an object JXiK
for each wire Xi and channel JfK : JX1K ⊗ ··· ⊗JXkK → JXK for each box f in D with inputs X1, . . . , Xk and
output X.
Definition 10. [LT23] Let C be a cd-category. An open generative model in C is given by a network diagram
D along with an interpretation J−K in C. We call the objects corresponding to output wires observed and the
rest hidden. We call such a model closed when it has no inputs.
Note that an object of an open model may be both an input and output. In practice, we omit the J−K
symbols and for each wire X in the network diagram of a model denote the corresponding object JXK in C
also by X. Similarly for each box c in the diagram with output X we also write c for the corresponding
channel JcK.
Remark 11. Formally, an open generative model in our sense is the same as an open causal model in the
sense of [LT23]; that is, both have the same mathematical definition. However a ‘generative model’ typically
refers to a causal model with the extra interpretation of being possessed by a cognitive agent.
Indeed, though not often stressed in the literature, a typical generative model in PP may be seen as a
causal Bayesian network, i.e. a causal model in the sense of Pearl [Pea09]. This means that the probability
channels which constitute the network do not represent arbitrary relationships but in fact (beliefs about) causal
ones, such as how observations are caused by (rather than merely correlated with) hidden states of the world.
For more discussion see Section 10.
Given any open generative model M we obtain an overall channel from its inputs to its outputs by
composing the channels of the model, i.e. viewing the (interpreted) network diagram as a single channel in
C. Often it is useful to also consider the following related channel.
Definition 12. Let M = (D, J−K) be an open generative model in C with inputs Iand outputs O. Let S
denote the non-input hidden (non-output) objects of the model. The total channel M of the model is the
13
channel from I to S, O:
M
. . .
. . .
I
O
. . .
S
(13)
given by interpreting the network diagram D′ in which we modify D by adding an extra copy morphism to
each object in S, to make it an output.
Conversely, the usual channel from inputs to outputs is then simply the marginal over S:
M
. . .
. . .
I
O
. . .
S
M
I
O
= (14)
In particular for a closed generative model, with no inputs, we call the total channel the total distribution of
the model. It is a joint distribution over the hidden objects S and observed objects O:
M
. . .
O
. . .
S
(15)
with the original distribution over the observed objects as its marginal.
M
. . .
O
. . .
S
=
. . .
O
M (16)
Let us now consider generative models in our main example category.
Example 13. A closed generative model M in MatR+ is precisely a Causal Bayesian Network (CBN). This
consists of specifying:
• a finite DAG Gwith a subset O ⊆ V of ‘observed’ vertices and the remaining S = V \ O being ‘hidden’;
• for each vertex Xi an associated variable with a finite set of values also denoted Xi, and a mechanism
ci given by a probability channel with density:
P(Xi | Pa(Xi)) (17)
The term ‘causal’ refers to the fact each such mechanism has a causal interpretation.
Indeed, as we have seen, such a DAG G with outputs O is equivalent to a network diagram D with no inputs.
Specifying an interpretation of D is then the same as choosing the sets Xi of values and channels (17) for
each box in the diagram. A CBN defines a joint distribution 4 over all the variables V = {X1, . . . , Xn} with
density
P(V ) :=
nY
i=1
P(Xi | Pa(Xi)) (18)
which is precisely (15). The output distribution of the CBN is given by the marginal P(O) over only the
observed variables, corresponding to (16).
4Often a Bayesian network is instead defined as a distribution P(V ) satisfying the Markov condition (18) in terms of its
conditionals. Since these conditionals may not be unique, and the channels ci are an important component of the model, we
instead include the latter explicitly; for more discussion see [LT23].
14
Example 14. An open generative model M in MatR+ is an ‘open CBN’, where now for the input variables
no channel (17) is specified. This induces via (13) the total channel
P(S, O| I)
from the inputs to the non-input hidden variables S and output variables O, which here we would denote (the
entries of) by M(s, o| i). Its marginal P(O | I) on the observed variables O yields the channel M(o | i) from
(14).
In short, a (closed) generative model in MatR+ specifies the internal structure of an output distribution
P(O) in terms of further variables and channels (17), while an open generative model similarly specifies the
internal structure of a channel P(O | I) from inputs I to outputs O.
For the remainder of this section we will describe some of the common forms of (open) generative models
which appear in PP.
3.1 Simple generative models
By a generative model S → O we mean a generative model M with network diagram:
c
σ
O
S
Thus M consists of objects S, Owith O observed and S hidden, a channel c: S → O, called the likelihood,
and a distribution σ on S, called the prior. As alluded to earlier, we call S the hidden states and O the
observations of the model. The total distribution of the model is given by
c
σ
S O
M
S O
= (19)
More generally, we can consider an open variant of such a generative model which now comes with a
hidden object I of inputs, with the following network diagram:
c
σ
O
S
I
(20)
Hence both the prior and likelihood now take an additional I input. The total channel is given by
c
σ
OS
I
M
OS
I
= (21)
15
Intuitively, such an open model M consists of specifying a particular generative model S → O for each input
in I.
Example 15. A generative model S → O in MatR+ consists of a finite set S of hidden states, O of
observations, a likelihood channel c(o | s) and prior distribution σ(s). Often would often write c(o | s) as
simply P(o | s) and σ(s) as P(s). We interpret c(o | s) as the probability of observing o when in the hidden
state s. Then the total state (19) is the joint distribution over S, Ogiven by
M(s, o) = c(o | s)σ(s)
and typically simply denoted P(s, o). As the notation suggests P(o | s) is the conditional and P(s) the
marginal of the joint distribution P(s, o).
When the generative model is open as in (20) it now comes with a finite set I of input values, with
likelihood c(o | i, s) and prior σ(s | i). The total channel (21) is then given by
M(s, o| i) = c(o | i, s)σ(s | i)
Thus for each input i we obtain a generative model M(i) of the form S → O and an induced joint distribution
M(i) over S, O.
3.2 Discrete time models
For a given n ∈ N, a discrete time generative model is a closed generative model M of the form
B
D
. . .
A
O1
A
S2
O2
B
A
On
S1 Sn
. . .
Thus it consists of observed objects O1, . . . , On, hidden objects S1, . . . , Sn, a prior distribution D over S1,
observation channels {A: St → Ot}n
t=1 and transition channels {B : St → St+1}n−1
t=1 . Typically we mean that
there are fixed objects S, Osuch that Sj = S, Oj = O for all j, and similarly as our notation suggests all the
A and B channels for each time step are taken to be identical.
We interpret the modelM as describing the evolution of a system over discrete time steps fromt = 1, . . . , n.
The system begins in its initial state with prior distribution D and then evolves over each time step according
to the transition channelsB. Independently, we observe the system at each timet via the channelA to produce
an observation in Ot.
Example 16. A discrete time generative model in MatR+ is also called a Hidden Markov Model or partially
observable Markov decision process (POMDP) [PPF22].
3.3 Policy models
We now introduce an explicit ingredient whereby the agent can model its own actions. As in reinforcement
learning [TMSB20], a choice of actions or behaviour is called a policy. In a discrete time setting, a policy
can be thought of as determining likely sequences of actions over the time steps, which in turn influence the
evolution of the states over time.
16
An n-time step model with policies is a generative model M of the form:
B
. . .
A
O1
A
S2
O2
B
A
On
S1
Sn
P
E
. . .
Sn−1
B
S0
D
Thus it now includes a hidden object P of policies which forms an input to each transition channel B from
St, Pto St+1, for t ≤ n − 1. The model also comes with a prior distribution E over P, which are called the
habits of the system. Note that here the policy the system is undertaking is considered hidden.
Again we typically take Sj = S and Oj = O for some fixed objects S, O, with all channels A identical and
all B channels identical.
Example 17. Models of this form, within MatR+, are the central examples used in the active inference
tutorial [SFW22] and book [PPF22].
3.4 Hierarchical models
Central to much of PP is the study of hierarchical generative models [DVF17, PPF22], which have a natural
graphical description. These are generative models given by composing various open generative models in
layers, where the outputs of the open models in one layer match the inputs of the models in the next layer,
such as in the example below.
M3
M2
M3 M3
M2
M3
M1
M0
S(0)
S(1)
S(2)
S(3)
(22)
Here it is understood that each boxMj represents an open generative model, which we may decompose further
in terms of its own network diagram with inputs and outputs as shown. The right-hand labels indicate that
the input wire to M1 has type S(0), the output wires from M1 both have type S(1) etc 5.
5It is also common to introduce a labelling convention for the wires such as S(0,1,2) where the indices represent wire numbers
in each layer as we read up the diagram. However this quickly becomes unwieldy, and in most cases the graphical description
of the network is the most convenient.
17
We interpret the inputs to each (box within a) layer as a ‘control’ signal from the layer below. Note
that because we read diagrams bottom to top, the layers further down the diagram are in fact those usually
referred to as more ‘high-level’ or ‘higher’ in the hierarchy.
The structure of the model tells us that the ‘high-level’ features cause the generation of the ‘lower-level’
features. For example S(0) could describe an overall action policy while the S(3) control more fine-grained
motor actions. Another common example explored in [DVF17] is where the output wires from each box
denote individual time steps. In this case time runs faster in the lower-level layers (higher in the diagram).
For example in the diagram above six time steps occur in layer S(3) for every time step in layer S(1).
Plugging in the network diagrams for each open model corresponding to Mj yields a composite network
diagram for the whole hierarchical model. For example in the following hierarchical model, the network
diagrams for M1 and M2 are shown in the highlighted boxes below and compose to yield the diagram on the
right-hand side.
M2 M2
M1
M0
P
S(1)
S(2)
=
A1
B1
S(1)
S(2)
A2
B2
A2
B2
P
M0
Much of the PP literature concerns such hierarchical models and the passing of these ‘top-down predictions’
(the flow of information up the diagram in this case) are adjusted by ‘bottom-up errors’ passed back down
the model. The latter takes place when a model is updated, which we address next.
4 Updating Models
Consider an agent with be a simple generative model M of the form S → O as in Section 3.1. Recall that this
induces a joint distribution M over S, Oas in (15), whose marginal on S is the prior σ describing ‘beliefs’
about how likely each state in S is to occur.
σ
S
=
M
S
O
Now suppose the agent receives an observation, which in general may be ‘soft’, given by an distribution o
over O. The agent would like to update these beliefs to obtain a new posterior distribution over S, describing
how likely each s ∈ S now is given the observation.
M
S O
,
o
O
7→
update(M, o)
S
How then should the agent update the marginal on S to yield this posterior? For a general soft observation
with distribution o over O there are at least two distinct but natural ways to carry out Bayesian-style
18
updating, as pointed out by Jacobs in [Jac19], which we describe in this section. When the observation o
is sharp, however, corresponding to a single element o ∈ O, there is a canonical way to carry out this belief
updating, usually simply referred to as Bayesian updating, which we introduce first.
4.1 Sharp Updating
Let us begin by describing updates with respect to a sharp observation, given by (a point distribution at)
an element o ∈ O. Such Bayesian updating is closely related to the notion of conditional probabilities,
which have a nice characterisation in cd-categories. Here we follow the approach to conditioning from [LT23],
building on earlier treatments [CS12, CJ19, Fri20]; see also [DLR23].
Definition 18. Let C be a cd-category, and ω a joint distribution over X, Y. Then a conditional of ω by Y
is a morphism ω|Y : Y → X such that the following holds:
ω|Y
X Y
σ
=ω
X Y
(23)
where σ is the marginal of ω on Y . If C has normalisation and cancellative caps, we define the (minimal)
conditional to be the morphism:
ω|Y
X
Y
= ω
X
Y
Each minimal conditional is indeed a conditional as shown in the Appendix of [LT23]. As we saw for
normalisations, a conditional is only a partial channel in general, being a channel only when ω has ‘full
support’.
Example 19. In MatR+ the minimal conditional ω|Y is given by
ω|Y (x | y) := Norm
x
ω(x, y) = ω(x, y)P
x′ ω(x′, y)
whenever the sum in the denominator is non-zero, and ω|Y (x | y) = 0 for all x otherwise. Thus when ω is
normalised with density denoted P(X, Y) this is the usual conditional P(X | Y ). The condition (23) amounts
to the usual ‘chain rule’ P(x, y) = P(x | y)P(y) for the probability distribution P(x, y) = ω(x, y), since σ(y)
is the marginal P(y) and we have:
ω(x, y) =
ω|Y
yx
σ
=ω
yx
ω|Y
y
yx
σ= = ω|Y (x | y)σ(y)
For a generative model M of the form S → O with joint distribution M over S, Owe call the minimal
conditional M|O : O → S the Bayesian inverse of the model. It specifies how to update beliefs about S for
19
each specific sharp observation o ∈ O. Explicitly, given a sharp distribution o = δo over O for some o ∈ O
the updated beliefs are given by the posterior:
M
S
O
o
= M
S
O
o
= M
S
O
o
=update(M, o)
S
(24)
Example 20. In MatR+, for a sharp observation δo for some o ∈ O, the posterior is the distribution over
S given by the usual Bayesian update:
M(s | o) = M(s, o)P
s′ M(s′, o) (25)
4.2 Pearl and Jeffrey Updating
There are two distinct ways to generalise updating to the case of a soft observation given by a distribution
o over O, described in [Jac19]. Diagrammatically these correspond to generalising from either the former or
latter diagrams in (24). For more on both forms of updating in cd-categories see also the treatment by Di
Lavore and Rom´ an [DLR23].
Definition 21. Let C be a cd-category with normalisation and cancellative caps, and M a joint distribution
over S, O. Given a distribution o over O, the Jeffrey update denoted MJ or M|o is given by the composite
M|O ◦ o, i.e.:
:=M|o
S
M
S
O
o
whenever this is normalised, and more generally is given by the normalisation of the above state. The
Pearl update denoted MP or M|o is instead given by the normalisation:
M|o
S
=
M
S
O
o
recalling that the effect o is given by composing o with a cap as in (5).
Example 22. For a generative model M from S to O in MatR+, with joint distribution M over S, O, the
Jeffrey update is given by
MJ(s) = E
o∼o
Norm
s
M(s, o) =
X
o
M(s, o)o(o)P
s′ M(s′, o) (26)
while the Pearl update is
MP (s) = Norm
s E
o∼o
M(s, o) =
P
o M(s, o)o(o)P
s′,o′ M(s′, o′)o(o′) (27)
20
The distinction between both update procedures is not always considered in the literature since for sharp
observations they coincide with the usual Bayesian update. Indeed the following is immediate from (24).
Lemma 23. Let C be a cd-category with normalisation and cancellative caps. Then for each sharp state o
on O the updates coincide: MJ = MP = M|O ◦ o.
In contrast, for a general observation o the two updates differ in the way they apply normalisation,
amounting to whether one normalises with respect to (or separately from) the observation itself.
M
S
o
̸=M
S
o
O
O
The Jeffrey update simply composes the observation o with the Bayesian inverse (partial) channel M|O. If
M|O is only a partial channel the result may not be normalised (such as when o falls outside the support), in
which case the update is then further normalised. The Pearl update instead involves a single normalisation,
taking place after composing with the observation, so that o is inside the normalisation box.
Remark 24. Jacobs has compared the two forms of updating within MatR+ in detail, noting that their in-
ferences can differ considerably, but that both can be considered rational notions of updating [Jac19]. One
difference between the updates is that by definition Jeffrey updating forms a probability channel inO (whenever
M|O is a channel, i.e. M has full support over O). In contrast, the normalisation over o in Pearl updating
means that it does not form a channel in O. The two update procedures can also be characterised by the fol-
lowing respective properties. For a generative model M over S, Owith likelihood c, Jeffrey updating minimises
the KL-divergence between o and the marginal on O of the updated model in which we replace the prior with
the posterior (left-hand below). Pearl updating instead has the property that it maximizes the expected value
of the function o (right-hand below).
MJ minimises: DKL


c
MJ
S
O
,
o
O


MP maximises: c
o
MP
S
O
∈ R+
The PP literature has mostly focused on updating with respect to sharp observations, in which the two notions
coincide. It is an interesting question for the future to determine which (if either) form of updating is most
natural in Bayesian models of cognition.
4.3 Updating Open Models
Since a typical generative model in PP is composed of various open generative models M, it is also important
to describe how an agent may update such open modelsM, now coming with inputsI. In this case we consider
the induced channel M : I → S, O. The prior beliefs about S are now given by the marginal σ : I → S, which
we can think of specifying beliefs over S for each input i ∈ I. Given an observation o over O the agent now
wishes to update this to a posterior channel of the same kind.
σ
S
= M
S
O
I I
7→ update(M, o)
S
I
21
All of the treatment of updating above generalises straightforwardly to such open models, amounting to
updating the corresponding closed model M(i) over S, Ofor each input i ∈ I.
Explicitly, for any morphismf : X → Y ⊗Z in a cd-category, a conditional is any morphism f|Z satisfying
the left-hand equation below, where σ is the corresponding marginal of f. In the presence of normalisation
and cancellative caps, the (minimal) conditional is that given on the right below, as in [LT23].
f|Z
Y Z
σ
=f
Y Z
X
X
f|Z
Y
X Z
= f
Y
X Z
Definition 25. Let M : I → S, Obe the channel induced by an open model M, and o a distribution over O,
in a cd-category with normalisation and cancellative caps. The Jeffrey update denoted MJ or M|o is given
by composing M|O with o as left-hand below (or more generally by its normalisation if the result is not a
partial channel). The Pearl update denoted MP or M|o is instead given as on the right-hand side.
:=M|o
S
I
M
S
I
O
o
M|o
S
= M
S
O
o
I I
By the defining property of normalisations (10) the Pearl update M|o satisfies the following, which will
be useful later.
M|o
S
M=M
S
O
o
O
o
I
I
(28)
Example 26. In MatR+ the minimal conditional of f by Z is given by
f|Z(y | x, z) = Norm
y
f(y, z| x)
and for a probability channel P(Y, Z| X) corresponds to the usual conditional P(Y | X, Z). The formulae
for both updates MJ(s | i), MP (s | i) are the same as (26), (27) simply replacing each M(s, o) term with
M(s, o| i), i.e.
MJ(s | i) = E
o∼o
Norm
s
M(s, o| i)
MP (s | i) = Norm
s E
o∼o
M(s, o| i)
Again both update procedures coincide with M(s | o, i) for sharp observations i ∈ I and all inputs i ∈ I.
22
Remark 27. Di Lavore and Rom´ an also study both forms of updating in cd-categories in which (non-chosen)
conditionals exist in [DLR23], calling them ‘partial Markov categories’. There updating is defined via arbitrary
(non-minimal) conditionals, meaning that M|O can be arbitrarily defined outside the support on O of M.
However since this arbitrary choice can impact the result of a Jeffrey update M|O ◦o when o is also non-zero
outside this support, we instead define updating via the minimal conditional M|O.
5 Perception and Planning
Let us now see how the notion of updating is applied by an agent to govern its behaviour in PP. Two
fundamental uses of updating are the following.
Perception Firstly, as already alluded to, we can consider the case of an agent with a generative model
M from S to O, interpreted as accounting for observations O in terms of hidden states of the world S. For
example, O may be the space of pixel-level descriptions of images while S is a compressed representational
space of possible objects which the images portray.
Given an observation encoded by a (soft or sharp) distribution o over O, the agent can update its prior
over hidden states S to obtain a posterior describing how likely each hidden state is to have caused the
observation. We refer to this general process of updating as perception and view the resulting distribution
as the agent’s specific perception of the observation o. Intuitively perception takes the ‘raw data’ of the
observation o and returns (a distribution over) representations S.
update(M, o)
S
perception
S
=M
S O
, o
O
7→
Intuitively, the update answers the question ‘Given that I have received this observation, how likely is each
possible world state?’. In the literature this is often referred to as inference, in reference to Bayesian inference.
Planning A second application of updating by an agent is in planning its behaviours. Here an agent
possesses a generative model M of the same formal structure but with objects labelled P, Fand interpreted
differently. Now P encodes the action policies, or behaviours, the agent may carry out, while F represents
observations (or states) it may receive in the future. The model M includes a prior over policies which we
can think of as the agent’s habits or typical behaviours.
Here the agent possesses some preferences about which future observations (or states) are most desir-
able, encoded by a distribution C over F. Intuitively, the distribution will have highest density on the
most desirable outcomes. The agent can then plan its actions by updating its habits with respect to these
preferences:
update(M, C)
P
plan
P
=M
P F
, C
F
7→
The process of deriving this distribution can intuitively be called ‘planning’. We can think of this update
as answering the question ‘Given that I will obtain my preferences in the future, how likely is each policy to
have led to this outcome?’ .
The resulting ‘plan’ distribution over P can be used to guide the agent’s future behaviour. For example,
an agent may then sample an policy to pursue from this distribution, so that the more probable policies
according to the distribution are more likely to be carried out.
6 Exact Active Inference
Both uses of updating by an agent, planning and perception, come together in the concept of active inference,
of which we are now able to present a fully formal diagrammatic account.
Consider an agent possessing a generative model describing how its actions, in the form of action policies
P, bring about changes in its observations. These consist of both observations for the present time (and
23
previous times) O and for future time steps F. Thus the agent has a closed generative model M of the
following form.
M
P
O F
E
action
policies
observations
habits
= (29)
Here (abusing notation slightly) we denote by M also the channel from policies to observations induced by
the model, and E is the prior over policies describing the agent’s habits.
Suppose further that the agent’s model explains the observations at each of these time steps through
hidden states, where S denotes the hidden states in the present time and S′ in the future, so that we have:
B
A A′
B′
O F
P
S S′
M
P
=
O F
(30)
for ‘observation’ channels A, A′ and ‘transition’ channels B, B′. The induced distribution on P, O, Fis then
given by:
E
P
M
P
=
O F
M
O F
(31)
The goal of active inference is then the following. The agent receives a current observation given by a
distribution o over O, and also carries a distribution C describing its preferences for future observations F.
The agent then wishes to update its prior E over policies to yield a posterior which describes its plan of
action6:
o
O
C
F
7→ update(M, o, C)
P
plan
P
= (32)
Intuitively, the posterior over policies can be thought of as answering the question ‘Given that I have received
this observation o now, and will attain my preferences C in the future, which action policy am I pursuing?’ .
Note that, perhaps surprisingly, the agent’s own action policy is thus treated as hidden from itself, and
something that it must infer.
Now, typically the objects above all decompose into further structure, as in the following example.
Example 28. A common application of active inference is to the discrete-time models with policies given
in Section 3.3, which we may view as instances of (30) as follows. Consider such a model featuring N
time-steps, where n << Nis considered the current time, and all times m with n ≤ m ≤ N as in the future.
6Ultimately, having derived their ‘plan’ distribution the agent may then sample a single action policy π ∈ P as in Section 5,
and act accordingly. We imagine that via the true ‘generative process’ in the world (distinct from the agent’s model) this leads
to further observations in the future, to which the agent carries out further planning steps, and so on. Our focus is simply on a
single step of how the agent derives their ‘plan’ from o and C.
24
The spaces of ‘current’ hidden states and observations S, Oare the products over all previous time-steps
t = 1, . . . , nup to and including the current time, while the future hidden states and observations S′, Ftake
the product over all future time-steps t = n + 1, . . . , N.
S := S1 ⊗ ··· ⊗Sn S′ := Sn+1 ⊗ ··· ⊗SN
O := O1 ⊗ ··· ⊗On F := On+1 ⊗ ··· ⊗ON
The observation channels in the overall model (30) would then be given by:
A :=
O
S
A
O1
S1
A
On
Sn
. . . A′ :=
F
S′
A
On+1
Sn+1
A
ON
SN
. . .
while the transition channels are as follows:
. . .
S1
B
Sn
P
. . .B
B
S
:=
P
. . .
Sn+1
B
SN
P
. . .B
Sn
B′
S′
:=
PS
S1 Sn−1
. . .
so that the composite (31) yields the network diagram for the overall model for times t = 1, . . . , N.
An agent may employ various update procedures, such as those discussed in Section 4, to calculate its
plan of action (32). Though both forms of updating coincide for sharp inputs, and the observations o in the
active inference literature are typically taken to be sharp, the preferences C are often not; that is, there may
be multiple desirable future observations in F. Thus Pearl and Jeffrey updating can be expected to differ.
Here we will describe an exact active inference procedure based on Pearl updating, allowing both obser-
vations o and preferences C to be soft. We leave the exploration of Jeffrey updating in active inference for
future work.
Now let us consider how the agent can in the ideal case compute its plan (32) via an exact update
procedure. Firstly, let us rewrite the channel in (30) as follows.
M =
O F S
O M2
F
M1
P
=
B
A
A′
B′
O
F
S
S′
S′
P
P
25
Here the channels M1, M2 are the compositions indicated by the highlighted boxes 7. Now applying the
property of Pearl updates (28) to M1 we have the following:
M
P
=
O F
S
P
M2
F
M1
E
P
o
C
O
=
M1|o
S
M1
O
o
P
M2
F
E
P
C
=
M|oM1
O
o
P
F
E
P Co C
(33)
Here we have again denoted by M1, M2 their respective marginals on O, F, given by discarding S, S′ respec-
tively. In the last step we used associativity of copying and the following argument:
M =
o
O
F
P
M|o
F
P
= M1|o
S
M2
F
P
P
S
M2
F
M1
P
o
O
=
S
M2
F
M1
P
o
O
=
where in the middle step we used (11) and (12) to slide the channel M2 and copying out of the normalisation
box, respectively.
Thus we obtain an exact expression for active inference.
Proposition 29. The plan over policies in Pearl-style exact active inference is given by:
M
P
=
O F M|oM1
O
o
P
F
E
P
C
o C
plan
P
= (34)
In MatR+ the plan has density over policies π ∈ P given by:
plan(π) := Norm
π
(E(π)(o ◦ M1(π))(C ◦ M|o(π))) (35)
= Norm
π
M|o
B
o
E
C
π
π π
A
(36)
7While we could define M2 without S′ as an output, the appearance of S′ will be useful later in treating approximate active
inference. Note also that the dashed boxes in this case do not denote normalisation.
26
Proof. The first equality holds by definition, so plan( π) = Normπ f(π) where f is the density of the state in
(33). But this is given by:
M
π
P O F
=
M|oM1
O
o
P
F
E
P
Co C π
=
M|oM1
O
o
P
F
E
P
C
π
π π
P
using that π is sharp, where the three right-hand scalars are precisely the terms in (35). The last line comes
from noting that the given marginal M1 : P → A is precisely B ◦ A.
There is only one problem with this form of active inference: the quantity (35) is completely intractable
to calculate. Along with the normalisation in calculating M|o, calculating the terms in (35) would involve
summation (or integration) over S, Oand S′, Frespectively, requiring us to respectively calculate:
X
s∈S,o∈O
o(o)A(o | s)B(s | π)
X
o′∈F
C(o′)M|o(o′ | π)
To make the calculation of these updates tractable, an agent in active inference is understood to instead
use a special form of approximation scheme, to which we now turn.
7 Free Energy
We have seen that for an agent to perform exact Bayesian updating is computationally intractable. In active
inference, an agent instead carries out approximate updating by minimising a quantity known as free energy
[FKH06, Fri10, PPF22]. In this section for simplicity we work concretely in the categoryC = MatR+, though
the same notions should be similarly defined in continuous settings.
The extra mathematical ingredient8 needed to define free energy will be the following .
Definition 30. For any distribution σ over X and x ∈ X we define the surprise as S(σ)(x) := −log σ(x).
For another distribution ω on X we define the overall surprise of σ relative to ω as the expectation value:
S

ω , σ

:= − E
x∼ω
log σ(x)
The entropy H(ω) of ω is its self-surprise:
H

ω

:= S

ω , ω

while the Kullback-Liebler (KL) divergence D(ω, σ) from σ to ω is the difference between these quantities:
D

ω , σ

:= S

ω , σ

− H

ω

The KL divergence is a commonly used similarity measure on distributions, with D(ω, σ) ≥ 0 and
D(ω, ω) = 0 for all distributions ω, σ.
We may now define the following general notion of free energy. Throughout we consider a distribution M
over S, O, which we imagine to be induced by a generative model from S to O. In this section for simplicity
given any such distribution we denote its marginals on S, Oand conditional channels M|S, M|O again simply
by M.
8To study free energy we will move beyond a purely diagrammatic approach and make use of some probabilistic calculations,
most notably to define ‘surprise’. However later in Section 9 we will see how to represent surprise in diagrams (via ‘log-boxes’).
In future work it would be interesting to represent all of the calculations in this section using such diagrams.
27
Definition 31 (Free Energy). The Free Energy of a distribution Q over S, Orelative to M is defined as:
FE

 Q
S O
, M
S O

 := S

 Q
S O
, M
S O

 − H

 Q
S

 (37)
Explicitly then we can re-write the free energy in the following useful form.
FE(Q, M) = E
(s,o)∼Q
[log(Q(s)) − log(M(s, o))] (38)
= E
(s,o)∼Q
[log(Q(s) − log(M(s | o)) − log M(o)] (39)
= E
o∼Q
S


Q
o
S
, M
o
S


+ S

 Q
O
,
M
O

 − H

 Q
S

 (40)
We now turn to two specific variants of this quantity commonly considered in active inference.
7.1 Variational Free Energy
Suppose an agent receives an observation given by a distribution o over O, and wishes to perform an approx-
imate Bayesian update of its prior beliefs about S as encoded by the marginal of M on S. It may do so by
finding the distribution q over S which minimises the following quantity.
Definition 32 (Variational Free Energy). Given a distribution M over S, Oand distribution o over O, the
Variational Free Energy (VFE) of a distribution q over S is defined as:
F

 q
S

 := FE

 q
S
o
O
, M
S O


An important feature of the VFE is the following. Using the expression (40) and pulling the entropy term
inside the expectation we see that
F(q) = E
o∼o
D

 q
S
, M
o
S

 + S

 o
O
,
M
O

 (41)
≥ D

 q
S
, M
o
S

 + S

 o
O
,
M
O

 (42)
The inequality follows from concavity of the KL divergence and Jensen’s inequality, which states that for any
probability measure ω on X, measurable function f : X → R and concave function ϕ on R we have
E
x∼ω
[ϕ(f(x))] ≤ ϕ( E
x∼ω
[f(x)]) (43)
In particular we see that the inequality (42) will be a strict equality whenever o = δo is given by a sharp
observation o ∈ O. In this case the minimum VFE value is given by the exact Bayesian inverse M|o, with
value F = −log M(o). Hence for a sharp observation o, minimising the VFE minimises the KL-divergence
between q and the Bayesian inverse M|o, achieving approximate inversion q ≈ M|o. Moreover F( q) is an
upper bound on the surprise of the observation o, and when q ≈ M|o we have F(q) ≈ S(o, M).
28
VFE Updating This process of minimising VFE to compute an approximate Bayesian update is central
in active inference, but typically only considered for such sharp observations. Here we can now consider the
more general minimisation of VFE for a soft observation given by a distribution o. In fact we may view this
as another notion of updating for a prior over S, in addition to the two forms of updating met in Section 4.
Firstly, observe that in the expression (41) since the surprise term is constant, the distribution q which
minimises F(q) will be that which minimises the left-hand expected KL term, which is equal to the following.
E
s∼q

log q(s) − E
o∼o
log M(s | o)

This quantity will in turn be minimised when this expression over S is equal to a constant K, so that:
log q(s) = E
o∼o
[log M(s | o)] + K
The distribution q will be given by normalisingq(s) in the above expression, allowing us to ignore the constant
and yielding the following notion of updating motivated by the VFE. Recall that the softmax of a function
f : X → R+ is defined by σ(f)(x) = Normx ef(x).
Definition 33 (VFE Update). Given a joint distribution M over S, Oand distribution o over O the VFE
update is the posterior
MF (s) = Norm
s
eEo∼o log M(s|o) (44)
= σ( E
o∼o
log M(s | o)) (45)
where σ denotes a softmax over S.
Similarly, for any channel M from P to S, Owe define the VFE update of its marginal P → S point-wise,
by MF (s | π) = M(π)F (s) for each π ∈ P.
From the derivation above we see that q = MF is the distribution which minimises F( q). Note that, as
for our other forms of updating, for a sharp observation o = δo we have MF (s) = M(s | o).
To relate general VFE minimisation for a soft observation to expectation values, we will use the following
form of approximation. Firstly, note that by Jensen’s inequality, for any probability measure ω and real
function f over X we have:
eEx∼ω[log f(x)] ≤ E
x∼ω
[f(x)] (46)
Whenever we take both sides of such an inequality to be approximately equal, let us say we are using a log
approximation. In particular for any distributions ω, σon X the follow holds log-approximately:
e−S

ω , σ

⪅
ω
σ
X (47)
Indeed this states precisely (46) for the case f(x) = σ(x). Such approximations can be used to relate free
energy to exact expectation values, as follows.
Proposition 34. Let MF be the VFE update of M relative to a distribution o over O, and F its VFE value.
Then the following holds log-approximately:
MF
S
e−F ≈
M
S
O
o
Proof. Define f(s) := eEo∼o log M(s|o) and the normalisation constant K = P
s f(s), so that KMF (s) = f(s).
Then we have:
F = S(o, M) + E
s∼q
[log MF (s) − E
o∼o
log M(s | o)] (48)
29
= S(o, M) − log K
e−F MF (s) = e−S(o,M)KMF (s)
= e−Eo∼o[log M(o)+log M(s|o)]
= e−Eo∼o[log M(s,o)] ≈ E
o∼o
M(s, o)
where in the last step we used a log-approximation.
Remark 35. Compare the formula for VFE update to the Jeffrey and Pearl updates (26), (27). While the
Jeffrey update composes the conditional O → S with o exactly, the VFE update instead minimises the expected
KL below.
MJ
S
M
S
o
O
= while MF minimises E
o∼o
D

 MF
S
, M
o
S


7.2 Expected Free Energy
A second form of free energy employed in active inference is used by an agent with a model featuring a space
O describing observations in the future. It then has a distribution C over O modelling preferences for these
future observations. Rather than updating its beliefs about future states, the agent simply want to assess
how well the marginal of the model on O will fit these preferences, via the following approximation.
Definition 36. Given a distribution M over S, Oand distribution C over O, the Expected Free Energy
(EFE) is defined as
G

 M
S O
, C
O

 := FE

 M
S O
, M
S O
C


(49)
The EFE compares the given model M to the right hand generative model which perfectly attains the
preferences, via its marginal C over O, whilst making use of the same inverse channel O → S. Writing the
EFE explicitly, and then rewriting in terms of the typically more readily computable channel S → O, we
have
G(M, C) = E
(s,o)∼M
[log(M(s)) − log(M(s | o))] − E
o∼M
[log C(o)]
= E
s∼M
o∼M◦s
[−log(M(o | s)] + E
o∼M
[log M(o) − log C(o)]
= E
s∼M


H


M
s
O




+ D

 M
O
,
C
O


The final line expresses the EFE in terms of a right-hand risk term, which assesses how well the predicted
state over O matches the preferences C, and a left-hand uncertainty term given by the expected entropy in
the observations. Thus minimising EFE requires both matching preferences and reducing uncertainty. For
more interpretations of EFE see [PPF22].
Now using Jensen’s inequality and the concavity of entropy, one may show that for any distribution ω
and channel c we always have:
E
x∼ω
H


c
x

 ≤ H


c
ω


30
Hence the EFE is bounded above by the surprise of the preferences:
G

 M
S O
,
C
O

 ≤ H

 M
O

 + D

 M
O
,
C
O

 = S

 M
O
,
C
O

 (50)
Thus minimising the EFE results in reducing the surprise of the preferences, making them more likely to be
obtained according to the model. Taking the inequality to be an approximation and applying exponentials
to both side along with a log-approximation then gives the following.
Proposition 37. The EFE is bounded above and approximately equal to the expectation value:
e−G

 M
S O
, C
O

 ⪅
C
O
M
7.3 Free Energy in Active Inference
We conclude this section by noting two uses of free energy in approximate active inference, treated in the next
section. For these we now consider a channel M from P to S, O, typically induced by an open model. For
each π ∈ P this specifies a joint distribution M(π) over S, O, to which we may apply free energy calculations.
Corollary 38. Let o and C be distributions over O. Let MF : P → S be the VFE update of M by o,
and for each π ∈ P set F(π) := F( M(π)F ) to the corresponding VFE value. Similarly for each π ∈ P let
G(π) = G(M(π), C). Then we have the following approximations:
M
S
O
o
P
≈MF
S
e−F
P
e−G
P
M
O
≈
P
C
S
In the above the effect e−F is given by π 7→ e−F(π), for π ∈ P, and e−G is defined similarly.
Proof. For the first approximation, plugging in a (sharp state given by) an elementπ ∈ P to both sides shows
that this is equivalent to Proposition 34 holding for each joint distribution M(π) over S, Owith respect to
the observation o. For the second approximation, apply Proposition 37 to the joint distribution M(π) over
S, Ofor each π ∈ P.
8 Active Inference via Free Energy
Let us now return to the situation of an agent carrying out active inference as in Section 6. As before the
agent’s generative model M in (29) consists of its habits E over policies P and a channel M from P to current
and future observations O, F, factoring via current and future hidden states S, S′. Given its observation o
and future preferences C it can now use free energy to give a viable approximation of its updated plan of
31
behaviour from Proposition 29, proceeding in two steps. We saw already in (33) that:
M
P
=
O F
M1|o
S
M1
O
o
P
M2
F
E
P
C
o C
‘Perception’ step In the first step, the agent approximately updates the part of the model pertaining to
the current time, M1, in light of the observation o. For each policy π it computes a distribution q(π) with
(approximately) minimal VFE F( q(π)), thus obtaining a channel q : P → S which approximates the VFE
update of M1 by o. For each π ∈ P denote the corresponding VFE value by F( π). Explicitly:
F(π) = F

 q(π)
S

 := FE

 q(π)
S
o
O
, M1
S O
π
P


‘Prediction’ step In the second step, the agent uses this approximation channel q, to obtain a channel
Mq which approximates the model over future states and observations, defined as follows:
Mq
F
P
= q
S
M2
F
P
P
S′
S′
For each policy π this induces a distribution Mq(π) over S′, F, for which the agent can compute the EFE
with respect to the preferences:
G(π) := G


Mq
FS′
π
, C
F


(51)
Using these free energy quantities, the agent may carry out approximate active inference. The following
formula is central in the active inference literature.
Theorem 39. The agent can carry out approximate active inference given observation o and preferences C
by setting its plan to have density
plan(π) := σ(log E(π) − F(π) − G(π)) (52)
where σ denotes a softmax over π ∈ P.
32
Proof. We have:
=
S
O
o
P
M2
F
E
P
C
≈
P
M
F
E
P
C
q
S
e−F
M
P
O F
o C
E
P
e−F Mq
F
C
≈
E
P
e−F e−G
=
M1
where we used Corollary 38 in both approximation steps. Thus defining our plan as on the left-hand below
yields an approximate update:
plan
P
:= ≈
E
P
e−F e−G
M
P
O F
o C
But finally, note that the left-hand distribution is precisely given by (52). Indeed for each π ∈ P, correspond-
ing to a sharp effect on P, we have:
E
P e−F e−G
π
= π
E
e−F e−G
π π = E(π) e−F(π) e−G(π)
Hence the normalisation of the above is precisely the softmax expression (52).
This formula for active inference via free energy, though frequently used, is usually only justified in a
fairly heuristic manner [PPF22]. Previous accounts rely on the less clear notion of treating EFE as a ‘prior’
to updating 9. Here we have instead seen how the expression can be derived from a direct diagrammatic
argument, directly from the structure of the generative model.
9 Compositionality of Free Energy
A crucial aspect of active inference is the idea that an agent can be understood to minimise free energy at
all levels, so that it may be seen to globally minimise free energy in its generative model by minimising free
energy within each component.
To formalise this idea we must first introduce a notion of free energy for open models. For this we will
make use of the following graphical notation for the surprise.
9Despite the fact EFE is not straightforwardly a component of the generative model, and requires inference over present
states S to be calculated first, rather than prior to them
33
Definition 40. Given any effect e on X in MatR+, corresponding to a function e: X → R+, we denote by
e
X
the function −log e(x): X → (−∞, ∞].
Remark 41. Note that a log-box is no longer an effect within MatR+, since when e(x) = 0 we will have
−log e(x) = ∞. Here we will interpret any diagram involving log-boxes with inputs X1, . . . , Xn and no outputs
as a (formula specifying a) function X1 ×···× Xn → (−∞, ∞]. Composing boxes in the diagram amounts to
summation over wires, as for MatR+. Given two such diagrams D1, D2 we write D1 + D2 for the function
given by their point-wise sum as functions. In future it would be interesting to explore a formal categorical
semantics for log-boxes.
In particular we can apply a log box to any distribution ω in MatR+ by first turning it into an effect,
yielding the surprise S(ω)(x) = −log ω(x).
ω
X
7→
ω
X
ω
X
= 7→
ω
X
Similarly for a pair of distributions ω, σwe have
S

ω , σ

=
σ
ω
. (53)
From the properties of the logarithm, one may verify that log-boxes then satisfy the following composi-
tional properties.
Lemma 42. For all effects d, eand sharp states x the following hold.
1.
d e
= d e+
2.
= 0
3.
d e = d e+
4.
x
d =
x
d
34
Proof. Plugging inputs x, yinto each equation they reduce to the following respective properties of the
logarithm. (1): log( d(x)e(x)) = log(d(x))+log( e(x)). (2): log(1) = 0. (3): log( d(x)e(y)) = log d(x)+log e(y).
(4) holds by definition, since both diagrams are given by y 7→ log d(x, y).
The following properties then follow from diagrammatic reasoning, using the relation between caps and
copying.
Proposition 43.
1. For all effects d, eand normalised states σ, ω:
d
σ
e
ω
=
d
σ
e
ω
+
In particular, entropy is additive across parallel composition: H(σ ⊗ ω) = H(σ) + H(ω).
2. For all effects d, e:
=
d e d e
+
3. For all morphisms f, g:
=f
g
f
+
g
Proof. (1) follows from Lemma 42 (3) since:
σ ω
d e
=
σ ω
d
ωσ
e
+ =
σ
d
ω
e
+
(2) follows from Lemma 42 (1), 2 and 3 since:
d e
d
=
d e
= + e
d= + e
35
(3) is a special case of (2) where we define the effects d, eby composing f, gwith caps on their output,
respectively, since using the relation between caps and copying we see that:
=f
g
f g
Now, by construction, for any joint distributions M, Qover S, Othe free energy is given by:
FE

 Q
S O
,
M
S O

 =
Q
M
−
Q
Q
S
Hence for any joint distribution M over S, Oand distributions q, o over S, Orespectively, the VFE is given
by:
F

 q
S

 =
q
M
−
q
q
S
o
S O
We can use this to define a generalisation of VFE for (the channels induced by) open generative models.
Definition 44 (Open VFE). Given a channel M : I → S, O, distribution q over I, Sand distribution o over
O, we define the open Variational Free Energy as
F


M
S O
I
, q
S I
, o
O


:= M
S O
I
q o
−
I
q
S
q
(54)
In the special case where I is trivial, the open VFE coincides with the usual VFE.
We can use the compositional properties of log boxes to show that this form of free energy is compositional,
in an appropriate sense. First consider the following two ways in which we may compose open models.
Consider a pair of open models M1, M2 with inputs I1, I2, outputs O1, O2 and hidden states S1, S2,
respectively, such that O1 = I2. We can compose these in sequence into a single open model M from I1 to
O2, with S1, S2, O1 as its hidden states, with induced total channel M from I1 to the remaining variables
given as below.
M1
M2
I1
O2
O1M
I1
O2
:= with total channel
I1
O2
M
S1
=
I1
M1
M2
O1
S1S2
S2 O2
O1
(55)
36
Formally, the left-hand diagram is a composition in the category of open causal models; see [LT23, Sec. 5].
We can also compose open models in parallel. Given two open models Mi with inputs Ii, outputs Oi and
hidden states Si, for i = 1, 2 we can define an open model M with both sets of inputs, outputs and hidden
states with induced induced total channel M from I1, I2 to the remaining variables given as below.
M1 M2
I1
O2O1
M
I1
O2
:=
I2
O1
I2
with total channel
I1
M1
O1S1
I1
M2
O2S2
I1
M
O1S1
=
I2
O2S2
(56)
For each of these forms of composition of open models, we wish to establish that free energy is compositional
in that the VFE of the open model M is determined from the VFE of its constituents. This ensures that
locally minimising VFE (within each of sub-component) can achieve global VFE minimisation also.
Theorem 45. For the sequential composite model M in (55) with O1 = I2, with total channel M, and any
distribution o over O2 and distributions q1, q2, the following holds:
F


I1
O2
M
S1 S2O1
, q2
I1
q1
S1 S2I2
, o
O2


= F


I1
M1
O1S1
, q1
S1I1
, o1
O1


+ F


I2
M2
O2S2
, q2
S2I2
, o
O2


(57)
where
o1
O1
=
I1
q2
S2
(58)
Intuitively, (58) expresses the way in which the beliefs about inputs I2 in M2 are passed down to the
model M1 as observations in O1 = I2.
Proof. After rearranging some wires, we have that
F(M, q,o) =
M1
M2
I1 S1 O2S2I2
q1 q2 o
−
I1S1 S2 I2
q1 q2
q1 q2
= M1
I1 S1 S2I2
q1 q2
+ M2
O2S2I2
q2 o
−
I1S1 S2 I2
q1 q2
q1 q2
−
where for the first two terms we apply Proposition 43 with f = M1 and g = M2, along with the fact that
o and q1 are normalised, and the second two terms are from Proposition 43 (1). But this is precisely the
right-hand side of (57).
Next let us turn to the parallel composite of open models.
37
Theorem 46. For any channels and distributions Mi, qi, oi for i = 1, 2, the following holds.
F


I1
M1
O1S1
I2
M2
O2S2
, q2
I1
q1
S1 S2I2
, o1
O1
o2
O2


= F


I1
M1
O1S1
, q1
S1I1
, o1
O1


+ F


I2
M2
O2S2
, q2
S2I2
, o2
O2


(59)
Proof. The left-hand side is given by
F(M, q,o) = M1
S1 O1
I1
q1 o1
−M2
S2 O2
I2
q2 o2
M1
S1 O1
I1
q1 o1
−M2
S2 O2
I2
q2 o2
= +
I1S1 S2 I2
q1 q2
q1 q2
I1S1 S2 I2
q1 q2
q1 q2
−
which is precisely the right-hand side, where we applied Proposition 43 (1).
The above results tell us that an agent with an overall generative model may minimise VFE by minimising
VFE locally within each sub-model, an important property underlying the application of the free energy to
all levels of a system.
10 Outlook
In this article we have aimed to give a concise formulation of active inference in terms of string diagrams
interpreted in a cd-category C, focusing on the case of finite discrete systems as described by C = MatR+.
In particular we were able to derive the formula for approximate active inference via free energy minimisa-
tion purely from the high-level structure of a generative model undertaking active inference, and derived a
compositionality property for free energy.
However these are just the first steps towards a fully compositional account of intelligent behaviour
according to predictive processing, and there are many directions for future work.
Message passing So far we only studied active inference at a high-level, saying that an agent must, for
each observation, arrive at an updated distributionq by free energy minimisation, without discussing how this
is to be carried out. In PP this minimisation is normally achieved via so-called ‘message passing’ algorithms,
such as ‘variational’ and ‘marginal’ message passing [PMKF19]. These are defined on undirected graphical
models described by Forney factor graphs, induced by a generative model. In future it would be interesting
to include a categorical account of message passing within our framework, to complete our description of
active inference.
Continuous settings Another technical matter would be to extend the treatment of PP beyond the
finite case to further cd-categories describing continuous settings, such as a suitable category of Gaussian
probabilistic processes, which are widely employed in PP under the ‘Laplace assumption’. One issue is
38
in extending our treatment of minimal conditionals to such continuous settings, where they are not as
straightforwardly defined.
Causal reasoning We have here pointed out that an generative model may be seen precisely as a causal
model [Pea09]. In future it would be interesting to explore how an agent may carry out causal reasoning on
its model using concepts from the causal model framework such as ‘interventions’, as treated graphically in
[LT23], and how such reasoning relates to active inference.
Approximations The treatment of active inference via free energy in Section 8 relied on applying various
approximation steps from Section 7 to parts of the diagram. Certainly more could be done to set bounds
on how well these approximations hold, including how they extend from part of a diagram to the whole
generative model.
Updating within PP The categorical perspective led us to naturally consider soft observations (given
by distributions) rather than the usual sharp ones (given by points), which come with distinct notions of
Jeffrey and Pearl updating (Section 4), as well our new notion of VFE updating (Section 7). While we were
able to describe active inference via the latter two forms of updating, it would be interesting to compare
against Jeffrey updating and establish which form of exact updating is most naturally considered (and
approximated) in PP. That is, given that both forms of updating have different goals [Jac19], which one (if
either) is approximately carried out by the brain? This question was also raised in [DLR23].
We note that Pearl updating can be more generally defined with respect to any effect (see e.g. [DLR23]),
i.e. any (not necessarily normalised) function. There is disagreement between active inference and rein-
forcement learning (RL) in whether an agent’s preferences should, rather than as a distribution as in active
inference, be simply modelled by a function C : F → R+ assigning a ‘value’ in R+ to each possible future
observation, i.e. as an effect C on F [FDK09, TMSB20]. In this case Pearl updating may be the most natural
to treat planning. In contrast, Jeffrey updating may be most fitting for perception, with an observation o
naturally encoded as a distribution i.e. a ‘fuzzy point’ in O.
Consciousness in PP Various proposals have been put forward for how PP and active inference can be
related to consciousness. Continuing from previous work from two of the authors on IIT [KT21, TK21], in
future we hope to account for these proposals within our graphical account of active inference.
Categorical modifications of PP Beyond simply recasting previous results in PP categorically, in future
one may also study what new insights the compositional perspective may bring to PP and active inference,
and to connect the work to ongoing research within categorical cybernetics [Smi21b, CGHR21] and more
broadly to the research programme of compositional intelligence.
References
[AC04] Samson Abramsky and Bob Coecke. A categorical semantics of quantum protocols. In Proceedings
of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004. , pages 415–425. IEEE,
2004.
[CCS08] Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. A compositional distributional model
of meaning. In Proceedings of the Second Quantum Interaction Symposium (QI-2008) , pages
133–140. Citeseer, 2008.
[CGHR21] Matteo Capucci, Bruno Gavranovi´ c, Jules Hedges, and Eigil Fjeldgren Rischel. Towards founda-
tions of categorical cybernetics. arXiv preprint arXiv:2105.06332 , 2021.
[CJ19] Kenta Cho and Bart Jacobs. Disintegration and bayesian inversion via string diagrams. Mathe-
matical Structures in Computer Science , 29(7):938–971, 2019.
[Coe06] Bob Coecke. Introducing categories to the practicing physicist. In What is category theory, pages
45–74. Polimetrica Monza, 2006.
39
[CS12] Bob Coecke and Robert W Spekkens. Picturing classical and quantum bayesian inference. Syn-
these, 186:651–696, 2012.
[Dea21] George Deane. Consciousness in active inference: Deep self-models, other minds, and the chal-
lenge of psychedelic-induced ego-dissolution. Neuroscience of Consciousness , 2021(2):niab024,
2021.
[DLR23] Elena Di Lavore and Mario Rom´ an. Evidential decision theory via partial markov categories.
arXiv preprint arXiv:2301.12989 , 2023.
[DVF17] Bert De Vries and Karl J Friston. A factor graph description of deep temporal active inference.
Frontiers in computational neuroscience, 11:95, 2017.
[FDK09] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference?
PloS one, 4(7):e6421, 2009.
[FFR+17] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pez-
zulo. Active inference: a process theory. Neural computation, 29(1):1–49, 2017.
[FK23] Tobias Fritz and Andreas Klingler. The d-separation criterion in categorical probability. J. Mach.
Learn. Res, 24(46):1–49, 2023.
[FKH06] Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
physiology-Paris, 100(1-3):70–87, 2006.
[Fon13] Brendan Fong. Causal theories: A categorical perspective on bayesian networks. arXiv preprint
arXiv:1301.6201, 2013.
[FPdV17] Karl J Friston, Thomas Parr, and Bert de Vries. The graphical brain: belief propagation and
active inference. Network neuroscience, 1(4):381–414, 2017.
[Fri10] Karl Friston. The free-energy principle: a unified brain theory? Nature reviews neuroscience,
11(2):127–138, 2010.
[Fri20] Tobias Fritz. A synthetic approach to markov kernels, conditional independence and theorems
on sufficient statistics. Advances in Mathematics, 370:107239, 2020.
[FST19] Brendan Fong, David Spivak, and R´ emy Tuy´ eras. Backprop as functor: A compositional perspec-
tive on supervised learning. In 2019 34th Annual ACM/IEEE Symposium on Logic in Computer
Science (LICS), pages 1–13. IEEE, 2019.
[GHWZ18] Neil Ghani, Jules Hedges, Viktor Winschel, and Philipp Zahn. Compositional game theory.
In Proceedings of the 33rd annual ACM/IEEE symposium on logic in computer science , pages
472–481, 2018.
[Hoh20] Jakob Hohwy. New directions in predictive processing. Mind & Language , 35(2):209–223, 2020.
[HS20] Jakob Hohwy and Anil Seth. Predictive processing as a systematic basis for identifying the neural
correlates of consciousness. Philosophy and the Mind Sciences , 1(II), 2020.
[Jac19] Bart Jacobs. The mathematics of changing one’s mind, via jeffrey’s or via pearl’s update rule.
Journal of Artificial Intelligence Research , 65:783–806, 2019.
[JKZ19] Bart Jacobs, Aleks Kissinger, and Fabio Zanasi. Causal inference by string diagram surgery. In
Foundations of Software Science and Computation Structures: 22nd International Conference,
FOSSACS 2019, Held as Part of the European Joint Conferences on Theory and Practice of
Software, ETAPS 2019, Prague, Czech Republic, April 6–11, 2019, Proceedings 22 , pages 313–
329. Springer, 2019.
[KT21] Johannes Kleiner and Sean Tull. The mathematical structure of integrated information theory.
Frontiers in Applied Mathematics and Statistics , 6:602973, 2021.
40
[LT23] Robin Lorenz and Sean Tull. Causal models in string diagrams. arXiv preprint arXiv:2304.07638,
2023.
[Pan98] Prakash Panangaden. Probabilistic relations. School of Computer Science Research Reports-
University of Birmingham CSR , pages 59–74, 1998.
[Pea09] Judea Pearl. Causality. Cambridge university press, 2009.
[Per22] Paolo Perrone. Markov categories and entropy. arXiv preprint arXiv:2212.11719 , 2022.
[PMKF19] Thomas Parr, Dimitrije Markovic, Stefan J Kiebel, and Karl J Friston. Neuronal message passing
using mean-field, bethe, and marginal approximations. Scientific reports, 9(1):1889, 2019.
[PPF22] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle
in mind, brain, and behavior . MIT Press, 2022.
[PZ23] Robin Piedeleu and Fabio Zanasi. An introduction to string diagrams for computer scientists.
arXiv preprint arXiv:2305.08768 , 2023.
[SBPF21] Noor Sajid, Philip J Ball, Thomas Parr, and Karl J Friston. Active inference: demystified and
compared. Neural computation, 33(3):674–712, 2021.
[SFW22] Ryan Smith, Karl J Friston, and Christopher J Whyte. A step-by-step tutorial on active inference
and its application to empirical data. Journal of mathematical psychology , 107:102632, 2022.
[SGW21] Dan Shiebler, Bruno Gavranovi´ c, and Paul Wilson. Category theory in machine learning. arXiv
preprint arXiv:2106.07032, 2021.
[Smi20] Toby St Clere Smithe. Bayesian updates compose optically. arXiv preprint arXiv:2006.01631 ,
2020.
[Smi21a] Toby St Clere Smithe. Compositional active inference i: Bayesian lenses. statistical games. arXiv
preprint arXiv:2109.04461, 2021.
[Smi21b] Toby St Clere Smithe. Cyber kittens, or some first steps towards categorical cybernetics. arXiv
preprint arXiv:2101.10483, 2021.
[Smi22] Toby St Clere Smithe. Compositional active inference ii: Polynomial dynamics. approximate
inference doctrines. arXiv preprint arXiv:2208.12173 , 2022.
[TK21] Sean Tull and Johannes Kleiner. Integrated information in process theories. Journal of Cognitive
Science, 2021.
[TMSB20] Alexander Tschantz, Beren Millidge, Anil K Seth, and Christopher L Buckley. Reinforcement
learning through active inference. arXiv preprint arXiv:2002.12636 , 2020.
[WM17] Wanja Wiese and Thomas Metzinger. Vanilla pp for philosophers: A primer on predictive pro-
cessing. 2017.
41