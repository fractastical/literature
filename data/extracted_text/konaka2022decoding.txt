1 
 
Decoding rewardâ€“curiosity conflict in decision-making 
from irrational behaviors 
 
Yuki Konaka1 and Honda Naoki1, 2, 3, 4* 
 
1. Laboratory of Data -driven Biology, Graduate School of Integrated Sciences for Life, 
Hiroshima University, Kagamiyama, Higashi-Hiroshima, Hiroshima 739-8526, Japan 
2. Kansei-Brain Informatics Group, Center for Brain, Mind and Kansei Sciences Research 
(BMK Center), Hiroshima University, Kasumi, Minami-ku, Hiroshima 734-8551, Japan 
3. Theoretical Biology Research Group, Exploratory Research Center on Life and Living 
Systems (ExCELLS), National Institutes of Natural Sciences, Okazaki, Aichi, Japan.  
4. Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, 
Yoshidakonoecho, Sakyo, Kyoto 606-8315, Japan 
 
*Corresponding author: Honda Naoki 
Address: Graduate School of Integrated Sciences for Life, 1-3-1 Kagamiyama, Higashi-
Hiroshima City, Hiroshima 739-8526, Japan 
Tel.: +81-82-424-7336 
E-mail: nhonda@hiroshima-u.ac.jp 
  
Keywords: Mental conflict; Curiosity, Reinforcement learning; Reward; Machine learning; 
State-space model.  
 
Abstract 
Humans and animals are not always rational. They not only rationally exploit rewards but also explore 
an environment, even if reward is less expected, owing to their curiosity. However, the mechanism of 
such curiosity-driven irrational behavior is largely unknown.  Here, we developed a novel decision-
making model for a two-choice task based on the free energy principle, which is a theory integrating 
recognition and action selection. The model successfully described irrational behaviors depending on 
the curiosity level. We then proposed a machine learning method to decode temporal curiosity from 
behavioral data, which enables us to quantitatively compare estimated curiosity and neural activities. 
By applying it to rat behavioral data, we found that the irrational choices sticking to one option was 
reflected to the negative curiosity level. Our decoding approach can be a  fundamental tool for 
identifying the neural basis for reward -curiosity conflicts. Specifically, it could be effective in 
diagnosing mental disorders.  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
2 
 
Introduction 
Animals and humans perceive the external world through their sensory systems and make 
decisions accordingly1,2. Generally, they cannot make optimal decisions because of the 
uncertainty of the environment as well as the limited computational capacity of the brain and time 
constraints in decision-making3. In fact, they perform irrational actions. For example, people play 
lotteries and gamble despite low reward expectations. In this case, they face a dilemma between 
low expected reward and curiosity regarding whe ther a reward will be acquired. Thus,  
understanding how animals control the balance between reward and curiosity is important for the 
elucidation of the whole decision-making process. However, no method to quantify the rewardâ€“
curiosity balance has yet been established. In this study, we developed a machine learning method 
to decode the time series of the rewardâ€“curiosity balance from animal behavioral data. 
Some irrational behaviors emerge because of the strength of curiosity 4,5. For example, 
conservative individuals avoid uncertainty and prefer to select an action that leads to predictable 
outcomes. Conversely, inquisitive individuals strongly desire to know the environment rather than 
rewards and prefer to select an action that leads to unpredictable outcomes. Rational individuals 
fall midway between these two extremes; in an ambiguous environment, they select an action to 
efficiently understand the environment, and if the environment becomes clear,  they select an 
action to efficiently exploit the rewards. Thus, curiosity has a significant impact on behavioral 
patterns, and it is naturally thought that animals control the balance between reward and curiosity 
in a context-dependent manner.  
Adaptive learning behaviors have been modeled primarily by reinforcement learning 
(RL), which is a theory for describing reward-seeking behaviors6. However, animals not only 
exploit rewards but also explore the environment , even without rewards , to minimize the 
uncertainty of the environment owing to their curiosity. Recently, the free energy principle (FEP) 
was proposed by Karl Friston under the Bayesian brain hypothesis, in which the brain optimally 
recognizes the outside world according to Bayesian estimation 7â€“9. FEP addresses not only the 
recognition of the external world but also action selection, which minimizes the uncertainty of 
the recognition of the external world. Furthermore, FEP was integrated with RL, and then action 
selection was formulated by maximizing both reward and curiosity. Note that curiosity can be 
regarded as information gain, that is, the extent to which we expect our recognition to be updated 
by new observation through the action10,11. However, FEP assumes that the weighting of rewards 
and curiosity is always constant and thus cannot treat actual animal behaviors in which the 
weights of rewards and curiosity are expected to change over time. Hence, conventional theories 
such as RL and FEP are limited in describing the conflict between reward and curiosity.  
The identification of the temporal variability of curiosity is important for elucidating the 
neural mechanisms of the reward and curiosity conflicts in decision-making. Many neuroscience 
studies have extensively examined the neural mechanisms of decision -making in the context of 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
3 
 
RL12â€“16. RL has worked as a guiding principle in the quantitative understanding of reward-seeking 
behavior. However, RL does not focus on curiosity -driven behavior; therefore, it is difficult to 
quantitatively understand the rewardâ€“curiosity conflict. To overcome this difficulty, a method to 
decode the temporal variability of curiosity from behavioral data is essential. Such a method 
would enable us to analyze neural correlates with the temporal variability of curiosity and, 
consequently, clarify how the brain controls the balance of reward and curiosity in a context -
dependent manner.  
 In this study, we extended FEP by incorporating a meta-parameter that controls the conflict 
dynamics between reward and curiosity, called the rewardâ€“curiosity decision-making (ReCU) 
model. The ReCU model is able to exhibit various behavioral patterns, such as greedy behavior 
toward reward, information -seeking behaviors with high curiosity, and conservative behaviors 
avoiding uncertainty. Moreover, we developed a machine learning method, called the inverse FEP 
(iFEP) method, to estimate the internal variables of decision-making information processing. By 
applying the iFEP method to behavioral time  series in a two -choice task, we successfull y 
estimated the internal variables, such as variations in curiosity, recognition of reward availability, 
and its confidence.  
 
 
Results 
ReCU model: decision-making with rewardâ€“curiosity dilemma in a two-choice 
task 
Animals perceive the environment by inferring causes, such as reward availability from 
observation, and then make decisions based on their own inferences ( Fig. 1A). In this study, we 
developed an ReCU model of a decision -making agent facing a dilemma between reward and 
curiosity in the case of a two -choice task, in which the agent selects either of two choices 
associated with the same rewards but with different reward probabilities. If the agent aims to 
maximize cumulative rewards, the agent must select an option with a higher reward probability. 
However, in animal behavioral experiments, even after they learned which option was associated 
with a reward, they did not exclusively always select the best choice, but also often select ed the 
option with a smaller reward probability, which seems unreasonable. Consequently, we 
hypothesized the following: Animals assume that the reward probability for each option might 
fluctuate over time so that the continuous selection of one option decreases the confidence of the 
reward probability estimation for the other option. Thus, they become curious about the 
ambiguous option even with a smaller reward probability, and then selecting the ambiguous 
option is reasonable to increase the confidence of the estimation for both options. Therefore, we 
considered that the agent should make a decision driven by reward and curiosity in a situation -
dependent manner. 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
4 
 
In this two-choice task, the reward is given as all -or-none, but its intensity depends on 
the agentâ€™s own feeling, as 
ğ‘Ÿ = 0	ğ‘œğ‘Ÿ	log ) ğ‘ƒ!
1 âˆ’ ğ‘ƒ!
- ,  
where ğ‘ƒ! is a parameter that controls the reward intensity felt by the agent. According to the 
Friston formulation, ğ‘ƒ! represents the desired level of how much the agent wants the reward10,11. 
To model decision-making driven by reward and curiosity, we divided information processing in 
the brain into two processes. First, the agent recognizes the world, that is, it estimates the reward 
probability of each option (Fig. 1A; Process 1). Then, based on this recognition, the agent selects 
an action based on reward and curiosity (Fig. 1A; Process 2). 
 In Process 1, we modeled the recognition process of reward probability using sequential 
Bayesian updating. The agent assumed that the reward is probabilistically generated from the 
latent cause w for both options (Fig. 1B, left), and these probabilities are represented by a 
sigmoidal function of ğ‘¤" as 
ğœ†" = ğ‘“(ğ‘¤"),  
where ğœ†"	and ğ‘¤"  denote the recognition of the reward probability of option i and its control 
variable, respectively, and ğ‘“(ğ‘¥) = 1/(1 + ğ‘’#$)	(Fig. 1B, middle). In other words, either reward 
or non-reward was observed at the probabilities ğœ†" and 1 âˆ’ ğœ†" if the agent selected option ğ‘–.  
 In addition, the agent assumed an environment in which the reward probabilities for both 
options could change ambiguously over time (Fig. 1C, upper panel). To describe this, ğ‘¤ is 
fluctuated by a random walk as 
ğ‘¤",& = ğ‘¤",&#' + ğœğœ‰",&,  
where ğ‘¡, ğœ‰",&, and ğœ denote the trial of the two-choice task, Gaussian noise with zero mean and 
unit variance, and the noise intensity, respectively. Accordingly, the reward probabilities for both 
options can be described by ğœ†",& = ğ‘“(ğ‘¤",&). The temporal dynamics of the latent variable ğ‘¤",& and 
reward observation are described by the state-space model from the viewpoint of the agent (agent-
SSM) (Fig. 1C, middle).  
 Given the above environment, the agent aims to estimate the reward probability ğœ†",& of each 
option at each observation. To describe the confidence of ğœ†",&, the agent is assumed to estimate 
the control variable ğ‘¤",& as a distribution ğ‘ƒ(ğ‘¤",&) and update its distribution. In general, ğ‘ƒ(ğ‘¤",&) 
can be updated by action and reward observation ğ‘œ& (Fig. 1C, lower), which can be roughly 
expressed as follows: 
updated	belief	ğ‘ƒ(ğ‘¤",&) = 	previous	belief	ğ‘ƒ(ğ‘¤",&#') Ã—	ğ‘œ&. 
This process was formulated using Bayesâ€™ rule (see Methods). We then derived the updates of the 
parameters as follows: 
ğœ‡!,# = #
ğœ‡!,#$% (ğ‘œğ‘ğ‘¡ğ‘–ğ‘œğ‘›	ğ‘–	ğ‘–ğ‘ 	ğ‘›ğ‘œğ‘¡	ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘)
ğœ‡!,#$% âˆ’ ğ›¼ 3 1
ğ‘!,#$%
+ 1
ğ‘&
6 7ğ‘œ# âˆ’ ğ‘“(ğœ‡!,#$%)9 (ğ‘œğ‘ğ‘¡ğ‘–ğ‘œğ‘›	ğ‘–	ğ‘–ğ‘ 	ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘) , (1) 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
5 
 
ğ‘",& =
â©
â¨
â§
ğ‘",&#'ğ‘(
ğ‘",&#' + ğ‘(
(ğ‘œğ‘ğ‘¡ğ‘–ğ‘œğ‘›	ğ‘–	ğ‘–ğ‘ 	ğ‘›ğ‘œğ‘¡	ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘)
ğ‘",&#'ğ‘(
ğ‘",&#' + ğ‘(
+ ğ‘“(ğœ‡ğ‘–,ğ‘¡)(1 âˆ’ ğ‘“(ğœ‡ğ‘–,ğ‘¡)) (ğ‘œğ‘ğ‘¡ğ‘–ğ‘œğ‘›	ğ‘–	ğ‘–ğ‘ 	ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘)
, (2) 
where	ğ‘& = 1/ğœ';	ğœ‡!,# and ğ‘!,# denote the mean and precision (i.e., inverse of variance) of ğ‘ƒ(ğ‘¤",&) 
(see the details in Methods), respectively. If the ğ‘–-th choice is not selected, its precision decreases 
(i.e.,	ğ‘!,#(% < ğ‘!,#). With observation, belief is updated by the prediction error (i.e., ğ‘œ& âˆ’ ğ‘“(ğœ‡ğ‘–,ğ‘¡)), 
and its precision is improved. Because the reward probability is represented by ğ‘“(ğ‘¤ğ‘–,ğ‘¡), the 
estimated reward probability, that is, the recognized reward probability, should be transformed as 
ğ‘“(ğœ‡ğ‘–,ğ‘¡). Similarly, the confidence of the recognized reward probability should be evaluated not in 
ğ‘¤",& -space, but in ğœ†!,# t-space, and hence the confidence is defined by ğ›¾& = ğ‘",&/ğ‘“)(ğœ‡",&)*	(see 
Methods). 	
 In Process 2, the agent selects actions according to the recognition of the reward probability 
and its confidence in Process 1. Agents have two kinds of motivations in decision-making: the 
desire to maximize the reward, and curiosityâ€”the desire to update their beliefs with more 
confidence (Fig. 1D, upper). This sum, called â€œexpected utilityâ€ in this study, can be expressed 
as  
ğ‘ˆ(ğ‘&+') = ğ‘& âˆ™ ğ¸[ğ¼ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘›	ğ‘”ğ‘ğ‘–ğ‘›&+'] + ğ¸[ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘&+'],  
where â€œinformation gainâ€ represents the information derived from a new observation based on 
previous recognition, ğ¸[ğ‘¥] denotes the expectation value of ğ‘¥ based on current recognition, and 
ğ‘& denotes a meta-parameter describing the intensity of curiosity, which weights the expected 
information gain. We regard ğ‘& as varying over time (Fig. 1D, lower panel). We analytically 
derived ğ‘ˆ(ğ‘&+') as a function of ğœ‡&,,&+'  and ğ‘&,,&+'  (see Methods). In decision-making, the 
agents selected action ğ‘&+' with the highest expected utility with a higher probability as  
ğ‘ƒ(ğ‘&+') = ğ‘’ğ‘¥ğ‘^ğ‘ˆ(ğ‘&+')_
âˆ‘ ğ‘’ğ‘¥ğ‘^ğ‘ˆ(ğ‘)_,
, (3) 
which is a SoftMax function.  
 
Sequential recognition and decision-making in the simulation 
To validate our model, we performed simulations for two cases. In the first case, where reward 
probabilities were constant and different between the two options (Fig. 2A), the agent preferred to 
select the option with the higher reward probability (Fig. 2B). The recognized reward probabilities 
converged to ground truths, indicating that the agent accurately recognized the reward probabilities 
(Fig. 2C). The recognition confidence changed over time depending on the behavior; the confidence 
increased for the option that was selected more (Fig. 2D). The expected information gain was lower 
for the option with higher confidence (Fig. 2E), whereas the expected reward consistently followed 
the recognized reward probability (Fig. 2F). The expected utility, which is the sum of the expected 
information gain and reward, represents the value of each selection (Fig. 2G), resulting in the agent 
preferentially selecting the option with the higher expected utility.  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
6 
 
In the second case, we assumed a time -dependent reward probability ( Fig. 2Aâ€™). In the 
simulation, the agent adaptively changed its recognition of the reward probability following the 
change in the true reward probability and selected the option with the higher estimated reward 
probability (Fig. 2Bâ€™ and Câ€™). The confidence was also affected by the uncertainty of the reward 
probability at each time; the confidence was high  where the reward probabilities were near -
deterministic, around one and zero, and low where the reward probabilities were uncertain , 
approximately 0.5 ( Fig. 2Dâ€™). The expected information gain of an option was negatively 
correlated with the confidence of the option (red line in Fig. 2Eâ€™), suggesting that the agent was 
curious about the uncertain option. As in the above case, the expected reward varied depending 
on the recognized reward probability ( Fig. 2Fâ€™). The expected utility changed similarly to the 
expected reward, but the difference between the left and right options was less pronounced owing 
to curiosity (Fig. 2Gâ€™). These two demonstrations indicate that our model is able to represent the 
process of cognition and decision-making based on reward and curiosity. 
 
Curiosity-dependent irrational behaviors  
Next, we examined how behavioral patterns are regulated by the intensity of curiosity and the degree 
of reward-seeking (Fig. 3). In a situation where the reward probabilities were zero for the left and 0.5 
for the right ( Fig. 3A), we simulated the model by varying the meta-parameters ğ‘ and ğ‘ƒ+ (Fig. 3B). 
When the agent strongly desired the reward (ğ‘ƒ+ = 0.99, nearly equal to one) with no curiosity (ğ‘ = 0), 
the agent preferred the right option with a higher reward probability (Fig. 3C-(a)). If the agent had no 
desire for a reward (ğ‘ƒ+ = 0.5) with high curiosity (ğ‘ = 10), the agent preferred the right option with 
a higher reward probability (Fig. 3C-(b)). Although this behavior seems to be rational at first glance, 
the agent did not seek the reward, but rather sought the information, that is, belief update, driven by 
curiosity, resulting in preference for the uncertain option. When the  agent moderately desired the 
reward (ğ‘ƒ+ = 0.5	to 1) with negative curiosity (ğ‘ = âˆ’10), the agent conservatively selected the same 
option as the previous selection (Fig. 3C-(c)). As a result, the agent continuously selected either of the 
two options depending on the first selection. These results were expected.  
However, we obtained a non-trivial result in another situation, where the reward probabilities 
were 0.5 for the left and 1 for the right ( Fig. 3Aâ€™â€“Câ€™). As in the previous situation, the agents with a 
strong desire for the reward ( ğ‘ƒ+ = 0.99, nearly equal to 1) preferred the right option with a higher 
reward probability (Fig. 3Câ€™-(a)). The agent with no desire for reward ( ğ‘ƒ+ = 0.5) and high curiosity 
(ğ‘ = 10) preferred the left option with a lower reward probability ( Fig. 3Câ€™-(b)). This seemingly 
irrational behavior was the outcome of focusing on satisfying curiosity and not seeking rewards. In 
addition, as seen in the previous situation (Fig. 3C-(c)), the agents with negative curiosity (ğ‘ = âˆ’10), 
irrespective of the desire for the reward, exhibited conservative selection ( Fig. 3Câ€™ -(c)). Taken 
together, these results clearly indicate that behavioral patterns largely depend on the degree of conflict 
between reward and curiosity, which has been difficult to define because of ambiguity (Fig. 3). 
 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
7 
 
inverse FEP: Bayesian estimation of internal state 
In the above cases, we assumed a constant balance between reward and curiosity. However, in reality, 
our feelings swing in a context-dependent manner. Although it is important to decipher the temporal 
swinging of the conflict between reward and curiosity in terms of neuroscience and psychology, it is 
difficult to quantify the conflict because of its temporal dynamics . Here, we developed a machine 
learning method called inverse FEP (iFEP) to quantitatively decipher the temporal dynamics of the 
internal state including curiosity meta-parameter from behavioral data.  
In developing iFEP, we needed to switch the viewpoint from the agent to the observer; that is, 
from animals to us. In the agent-SSM, we described the sequential recognition of reward probabilities 
by the agent ( Fig. 1C, Fig. 4A,B ). Conversely, to determine the internal state of the agent, e.g., 
intensity of curiosity ğ‘&, recognition ğœ†",& and its confidence ğ›¾",&, we developed a state -space model 
from the observerâ€™s eye (observer-SSM) (Fig. 4B). In the observer-SSM, the intensity of curiosity was 
assumed to change continuously over time, and the agent's recognition of the reward probability was 
updated by equations (1) and (2) following FEP, but they were unknown to the observers. In addition, 
the agentâ€™s actions were assumed to be genera ted depending on the intensity of its curiosity, 
recognition, and confidence, as described in equation (3), but the observers can only monitor the 
agentâ€™s action and the presence of  a reward. In iFEP, based on the observer -SSM, we estimate the 
latent internal state of agent z from observation x in a Bayesian manner as follows:  
ğ‘ƒ(ğ‘§':.|ğ‘¥':.) âˆ ğ‘ƒ(ğ‘¥':.|ğ‘§':.)ğ‘ƒ(ğ‘§':.), (4) 
where ğ‘§!,%:- = {ğœ‡!,%:-, ğ‘!,%:-, ğ‘%:-}, ğ‘¥%:- = {ğ‘%:-, ğ›¾%:-}, and the subscript 1: ğ‘‡ indicates steps 1 to ğ‘‡. In 
this Bayesian estimation, a posterior distribution ğ‘ƒ(ğ‘§':.|ğ‘¥':.) represents the observer's recognition 
of the estimated ğ‘§':. given observation ğ‘¥':. with uncertainty. A prior distribution ğ‘ƒ(ğ‘§':.) represents 
our belief, which is expressed as the ReCU model with random motion of the curiosity meta-
parameter c as   
ğ‘& = ğ‘&#' + ğœ–ğœ&,  
where ğœ& indicates white noise with zero mean and unit variance, and ğœ– indicates its noise intensity. 
The likelihood ğ‘ƒ(ğ‘¥':.|ğ‘§':.)	represents the probability that ğ‘¥':. was observed assuming ğ‘§':., which 
also follows the ReCU model. This Bayesian estimation, namely iFEP, was conducted using a particle 
filter and Kalman backward algorithm (see Methods).  
 
Validation of iFEP 
We tested the validity of the iFEP method by applying it to the artificial data generated by the ReCU 
model. Specifically, we simulated a model agent with non-constant curiosity in the two-choice task, 
where reward probabilities varied temporally. We then demonstrated that iFEP estimated the ground 
truth of the internal state of the simulated agent, that is, the agentâ€™s intensity of curiosity, recognition, 
and confidence (Fig. 5). Therefore, iFEP must be powerful in clarifying decision-making processing 
and the accompanying temporal swing in the conflict between reward and curiosity.  
 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
8 
 
Negative curiosity of rat behaviors decoded by iFEP 
Finally, we applied iFEP  to actual rat behavioral data from the two -choice task experiment with 
temporally varying reward probabilities (Fig. 6A)17. In this experiment, once the reward probabilities 
were suddenly changed in a discrete manner, the rat slowly adapted to select the option with the higher 
reward probability (Fig. 6B), suggesting that the rat sequentially updated its recognition of the reward 
probability. Based on the se behavioral data of the rat, iFEP estimated the internal state , that is, the 
intensity of curiosity, the recognized reward probabilities, and their confidence levels (Fig. 6C-E). We 
found that the rat was not perfectly aware  of the true reward probabilities but was able to recognize 
increases and decreases in reward probability  (Fig. 6C and D ). We also found that confidence 
increased with choice and decreased with no choice (Fig. 6E and F). Interestingly, the curiosity held 
by the rat was always estimated to be negative  (Fig. 6G ). In other words, the rat conservatively 
preferred certain choices but did not explore uncertain choices, regardless of the recognition of reward 
probability. 
To further show that negative curiosity was not estimated by chance, we conducted Monte 
Carlo testing (Fig. S1). As a null hypothesis, we considered an agent who has no curiosity and decides 
on a choice only depending on its recognition of the reward probability. Following the null hypothesis, 
we simulated a series of the agentâ€™s choices using iFEP-estimated reward probability recognition and 
its confidence with zero curiosity (i.e., ğ‘& = 0). The same simulations were repeated 1,000 times, and 
the curiosity was estimated using iFEP for each (Fig. S1B). We then plotted the null distribution for 
temporal average of estimated curiosity, as a test statistic (Fig. S1B). Compared to the null distribution, 
the temporal average curiosity estimated from the actual rat behavior was located to the left of the 
significance level (ğ‘ = 0). These results suggest that rats have negative curiosity; they made choices 
based on confidence in reward probability recognition rather than the recognition itself. 
 
 
Discussion 
Previous studies have discussed decision -making subject to rationality and optimality, in which 
irrational behaviors typical of animals cannot be tractable18. By contrast, we proposed the ReCU model, 
which expresses the mental conflict between reward and curiosity. In addition, we developed the iFEP 
method to decode the conflict between reward and curiosity based on the time-series behavioral data 
of a two-choice task. 
 What is the neural basis that controls psychological states, including mental conflict? This 
question cannot be addressed simply by comparing neural activities and animal actions because 
the psychological state is not reflected from each primitive action itself but must be behind a 
series of actions. Thus, it is important to define psychological states and estimate latent 
psychological states, such as curiosity, confidence, and reward prediction error, from animal 
behaviors, which enables us to compare the estimated  psychological states and neural 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
9 
 
activities12,15,19. The iFEP method is an essential tool for this purpose. Therefore, the iFEP 
approach will contribute to the future development of neuroscience for psychological states.  
 Our approach has three novel characteristics compared with previous models. First, we 
addressed the irrationality of decision-making. The expected free energy was originally expressed 
as the sum of the expected information gain and the expected reward, both of which were equally 
weighted10,11, leading to Bayesian optimality. However, animals, including humans, perform 
decision-making under bounded rationality . Consequently, their computational performance is 
restricted by the limited size of the brain and time constraints for needing to make quick decisions. 
Thus, they do not necessarily achieve Bayesian optimization. By contrast, our study formulated 
irrational decision-making by introducing arbitrary weights  on the expected information as a 
meta-parameter controlling curiosity. Therefore, our model represents individuality. Second, we 
addressed temporally changing curiosity, which might be situation -dependent. Animals and 
humans face a dilemma between reward and curiosity, which causes curiosity to vary over time. 
Third, we addressed an inverse problem using the iFEP method, which quantitatively decodes 
psychological states, including the intensity of curiosity, from the viewpoint of an observer of 
animals. This method is able to evaluate not only individual -specific properties, such as curious 
and conservative characters, but also temporal mental swings in a situation-dependent manner. 
 Decision-making on the two-choice task has been computationally addressed based on two 
types of modeling: RL and FEP. Studies based on RL described the temporal update of action 
values, which is called Q -learning6, but with a lack of recognition of the environment. In RL 
reinforcement modeling, it is common to represent the degree of exploration by a temperature 
parameter, which derives randomness in action selection but does not lead to information-seeking 
behavior, which corresponds to curiosity in our approach. Several studies have estimated the 
dynamics and parameters of Q -learning15,17,19,20. By contrast, Schwartenbeck et al. modeled 
decision-making behavior in a two-choice task based on FEP21. In their study, they succeeded in 
representing animal behaviors driven not only by reward but also by curiosity. However, they 
assumed that the intensity of curiosity was fixed at one (ğ‘& = 1), at which the Bayes optimality 
was satisfied, and did not address its te mporal variability as assumed and estimated in our iFEP 
approach. Furthermore, the related modeling differs from RL and FEP. Ortega and Braun 
formulated FEP, which describes irrational decision-making22,23. Interestingly, their formulation 
was based on microscopic thermodynamics, and the temperature parameters controlled this 
irrationality. However, the thermodynamics-based FEP did not treat the sequential update of the 
recognition from the observations.   
 Finally, it is worth discussing future perspectives of our iFEP approach in medicine. 
Generally, mental diagnosis relies on medical interviews and has not been quantitatively 
evaluated. Our iFEP method can quantitatively estimate the psychological state of patients based 
on their behavioral data. For example, patients with social withdrawal, also known as 
â€œHikikomori,â€ have no interest in anything. In this case, social withdrawal would be characterized 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
10 
 
by a negative value of curiosity in our FEP model. Therefore, the iFEP method could be effective 
in diagnosing mental disorders. 
 
 
Methods 
FEP for reward probability recognition 
In the reward probability recognition process, the agent assumes an environment expressed by a 
state-space model (Fig. 1C) as  
ğ‘ƒ(ğ°!|ğ°!"#) = ğ’©(ğ°!|ğ°!"#, ğœ$ğˆ),  
ğ‘ƒ(ğ‘œ&|ğ°ğ‘¡, ğšğ‘¡) = h iğ‘“(ğ‘¤",&)!!j1 âˆ’ ğ‘“(ğ‘¤",&)k
'#!!
l
,",!
,
"
(5) 
where ğ°! and ğ‘œ& denote the latent variables controlling the reward probability of both options at 
step t (ğ°& = (ğ‘¤',&, ğ‘¤*,&). ) and the observation of the presence of the reward (ğ‘œ& âˆˆ {0,1}), 
respectively; ğš& denotes the agentâ€™s action at step ğ‘¡, which is represented by a one -hot vector 
(ğš& âˆˆ {(1,0)., (0,1).}); ğ’©(ğ±|ğ›, Î£) denotes the Gaussian distribution mean ğ› and variance Î£; 
ğœ$ denotes the variance of the transition probability of ğ°; and	ğ‘“(ğ‘¤",&) = 1/(1 + ğ‘’#(",!), which 
represents the probability of reward of option ğ‘– at step ğ‘¡. The initial distribution of ğ°' is given by 
ğ‘ƒ(ğ°') = ğ‘(ğ°'|0, ğœ…ğˆ), where ğœ… denotes variance.  
 Here, we model ed the agentâ€™s recognition process of reward probability using sequential 
Bayesian updating as follows:  
ğ‘ƒ(ğ°&|ğ‘œ':&, ğš':&) âˆ ğ‘ƒ(ğ‘œ&|ğ°&, ğš&) v ğ‘ƒ(ğ°&|ğ°&#')ğ‘ƒ(ğ°&#'|ğ’':&#', ğš':&#')ğ‘‘ğ°&#'.  
Because of the non -Gaussian ğ‘ƒ(ğ‘œ&|ğ°&, ğš&), the posterior distribution of ğ°& , ğ‘ƒ(ğ°&|ğ‘œ':&, ğš':&), 
becomes non -Gaussian and cannot be calculated analytically. To avoid this problem, we 
introduced a simple posterior distribution approximated by a Gaussian distribution, as follows:   
ğ‘„^ğ°ğ‘¡|ğœ‘ğ‘¡_ = ğ’©(ğ°ğ‘¡|ğ›ğ‘¡, ğš²ğ‘¡
âˆ’1) â‰’ ğ‘ƒ(ğ°&|ğ‘œ':&, ğš':&),  
where ğœ‘! = {ğ›!, ğš²(} , and  ğ›!  and ğš²(  denote the mean and precision, respectively (ğ›& =
(ğœ‡',&, ğœ‡*,&).;	ğš²/ = ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘',&, ğ‘*,&)). ğ‘„(ğ°&|ğœ‘&) denotes the recognition distribution. The model 
agent aims to update the recognition distribution through ğœ‘! at each time step by minimizing the 
surprise, which is defined byâˆ’ğ‘™ğ‘›ğ‘ƒ(ğ‘œ&|ğ‘œ':&#'). The surprise can be decomposed as follows: 
âˆ’ğ‘™ğ‘›ğ‘ƒ(ğ‘œ&|ğ‘œ':&#') = v ğ‘„(ğ°ğ­|ğœ‘&)ğ‘™ğ‘›ğ‘„(ğ°ğ­|ğœ‘&)
ğ‘ƒ(ğ‘œ&, ğ°&|ğ‘œ':&#', ğš':&) ğ‘‘ğ°& âˆ’ KL[ğ‘„(ğ°ğ­|ğœ‘&)||ğ‘ƒ(ğ°&|ğ‘œ':&, ğš':&)],  
where KL[ğ‘(ğ±)||ğ‘(ğ±)] denotes the Kullbackâ€“Leibler (KL) divergence between the probability 
distributions ğ‘(ğ±) and ğ‘(ğ±). Because of the non-negativity of KL divergence, the first term is the 
upper bound of the surprise: 
ğ¹(ğ‘œ&, ğœ‘&) = v ğ‘„(ğ°ğ’•|ğœ‘&)ğ‘™ğ‘›ğ‘„(ğ°ğ’•|ğœ‘&)ğ‘‘ğ°& + v ğ‘„(ğ°ğ’•|ğœ‘&)ğ¸(ğ‘œ&, ğ°&)ğ‘‘ğ°& ,  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
11 
 
which is called the free energy, where ğ¸(ğ‘œ&, ğ°&) = âˆ’ğ‘™ğ‘›ğ‘ƒ(ğ‘œ&, ğ°&|ğ‘œ':&, ğš':&). The first term of the 
free energy corresponds to the negative entropy of a Gaussian distribution:	
ğ¹' = v ğ‘„(ğ°ğ’•|ğœ‘&)ğ‘™ğ‘›ğ‘„(ğ°ğ’•|ğœ‘&)ğ‘‘ğ°& . 	
The second term is approximated as  
ğ¹* = v ğ‘„(ğ°ğ’•|ğœ‘&)ğ¸(ğ‘œ&, ğ°&)ğ‘‘ğ°&
â‰… v ğ‘„(ğ°ğ’•|ğœ‘&) Â„ğ¸(ğ‘œ&, ğ›&) + ğ‘‘ğ¸
ğ‘‘ğ‘¤ (ğ°& âˆ’ ğ›&) + 1
2
ğ‘‘*ğ¸
ğ‘‘ğ‘¤* (ğ°& âˆ’ ğ›&)*Â… ğ‘‘ğ°&
= ğ¸(ğ‘œ&, ğ°&)|(!23! + 1
2
ğ‘‘*ğ¸
ğ‘‘ğ‘¤*Â†
(!23!
ğš²&
#'	 (6)
 
Note that ğ¸(ğ‘œ&, ğ°&) is expanded by a second-order Taylor series around ğ›&. At each time step, the 
agent updates ğœ‘& by minimizing ğ¹(ğ‘œ&, ğœ‘&).  
 
Calculation of free energy  
The free energy is derived as follows. ğ¹' simply becomes  
ğ¹' = 1
2 ğ‘™ğ‘›2ğœ‹ğ‘',&
#' + 1
2 ğ‘™ğ‘›2ğœ‹ğ‘*,&
#' + ğ‘ğ‘œğ‘›ğ‘ ğ‘¡. 
For computing ğ¹*,  
ğ‘ƒ(ğ‘œ&, ğ°&|ğ‘œ':&#', ğš':&) = 	ğ‘ƒ(ğ‘œ&|ğ°&, ğš&) v ğ‘ƒ(ğ°&|ğ°&#')ğ‘ƒ(ğ°&#'|ğ‘œ':&#', ğš':&#')ğ‘‘ğ°&#' 
																																				â‰… ğ‘ƒ(ğ‘œ&|ğ°&, ğš&) v ğ‘ƒ(ğ°&|ğ°&#') N(ğ°&#'|ğ›&#', ğš²/#'
#' )ğ‘‘ğ°&#'. 
In the second line of this equation, we use the approximated recognition distribution as the 
previous posterior ğ‘ƒ(ğ°&#'|ğ‘œ':&#', ğš':&#'). This equation can be written as follows:  
ğ‘ƒ(ğ‘œ&, ğ°&|ğ‘œ':&#', ğš':&) â‰… 
h iğ‘“^ğ‘¤",&_
!!
j1 âˆ’ ğ‘“^ğ‘¤",&_k
'#!!
l
,",!
"
ğ‘(ğ‘¤',&|ğœ‡',&#', ğ‘',&
#' + ğœ*	)ğ‘(ğ‘¤*,&|ğœ‡*,&#', ğ‘*,&
#' + ğœ*). 
Then, 
ğ¸(ğ‘œ&, ğ°&)|(!23! = ğ¸'(ğ‘œ&, ğœ‡',&) + ğ¸*(ğ‘œ&, ğœ‡*,&) + ğ‘ğ‘œğ‘›ğ‘ ğ‘¡, 
where  
ğ¸"(ğ‘œ&, ğœ‡",&) = ğ‘",&Â‹ğ‘œ& log ğ‘“(ğœ‡",&) + (1 âˆ’ ğ‘œ&)ğ‘™ğ‘œğ‘”j1 âˆ’ ğ‘“^ğœ‡",&_kÂŒ âˆ’ 1
2
^ğœ‡",& âˆ’ ğœ‡",&#'_
*
ğ‘",&
#' + ğœ*
âˆ’ 1
2 log	(ğ‘",&
#' + ğœ*).																																																																																				(7) 
Thus, ğ¹* is calculated by substituting equation (7) into equation (6). Taken together,  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
12 
 
ğ¹(ğ‘œ&, ğœ‘&) = Â Âğ¸"(ğ‘œ&, ğœ‡",&) + 1
2
ğ‘‘*ğ¸"
ğ‘‘ğ‘¤",&
* Â†
(",!23",!
ğ‘",&
#' + 1
2 ğ‘™ğ‘›2ğœ‹ğ‘",&
#'Â .
"
(8) 
 
Sequential updating of the agentâ€™s recognition 
The updating rule for ğœ‘& was derived by minimizing the free energy. The optimized ğ‘",& can be 
computed by ğœ•ğ¹/ğœ•ğ‘",&
#' = 0, which leads to the following: 
ğ‘",& = ğ‘‘*ğ¸"
ğ‘‘ğ‘¤",&
* Â†
(",!23",!
. 
By substituting ğ‘",&  into equation (8), the second term in the summation becomes constant, 
irrespective of ğœ‡",&. Thus, ğœ‡",& is updated by minimizing only the first term as   
ğœ‡",& = ğœ‡",&#' âˆ’ ğ›¼ğ›¿",,
ğœ•ğ¸"
ğœ•ğœ‡",&
Â†
3",!23",!$%
, 
where ğ›¼ is the learning rate. These two equations finally lead to equations (1) and (2). 
 
Model for action selection   
The agent selects a choice with higher expected utility defined by  
ğ‘ˆ(ğš&+') = ğ‘& âˆ™ ğ¸5(!!&%|ğš!&%)Â‹KL[ğ‘„(ğ°&+'|ğ‘œ&+', ğš&+')||ğ‘„(ğ°&+'|ğš&+')]ÂŒ
+ğ¸5(!!&%|ğš!&%)[lnğ‘ƒ(ğ‘œ&+')]. (9) 
The first and second terms represent the expected information gain and expected reward, 
respectively, and ğ‘& denotes the intensity of curiosity at time ğ‘¡. 
 In the first term, the posterior and prior distributions of ğ°&+' in the KL divergence are 
derived as 
ğ‘„(ğ°&+'|ğš&+') = v ğ‘ƒ(ğ°&+'|ğ°&)ğ‘„(ğ°&)ğ‘‘ğ°&, 
ğ‘„^ğ°&+'|ğ‘œ&+',ğš&+'_ = ğ‘ƒ(ğ‘œ&+'|ğ°&+', ğš&+')ğ‘„(ğ°ğ’•+ğŸ|ğš&+')
ğ‘ƒ(ğ‘œ&+'	|ğš&+') , 
respectively. This KL divergence is called a Bayesian surprise, representing the extent to which 
the agentsâ€™ beliefs are updated by observation. Note that ğ‘œ&+'	 is a future observation, therefore 
the first term was expected by ğ‘ƒ(ğ‘œ&+'	|ğš&+'), which can be calculated by  
ğ‘ƒ(ğ‘œ&+'	|ğš&+') = v ğ‘ƒ^ğ‘œ&+'	|ğ°&+',ğš&+'_ğ‘„(ğ°&+'|ğš&+')ğ‘‘ğ°&+'. 
 In the second term, the reward is quantitatively interpreted as the desired probability of ğ‘œ&+'	. 
For the two-choice task, we use 
ğ‘ƒ(ğ‘œ&+'	) = ğ‘ƒ!
!!&%(1 âˆ’ ğ‘ƒ!)(1âˆ’ğ‘œğ‘¡+1), 
where Po indicates the desired probability of the presence of a reward. According to the 
probabilistic interpretation of reward10,11, the presence and absence of a reward can be evaluated 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
13 
 
by ln ğ‘ƒ!  and ln(1 âˆ’ ğ‘ƒ!) , respectively. Because rewards are relative, in this study, we set 
ln{ğ‘ƒ!/(1 âˆ’ ğ‘ƒ!)} and 0 for the presence and absence of a reward, respectively. 
 
Calculation of the expected utility  
Here, we present the calculation of the expected utility. The KL divergence in the first term of 
equation (9) can be transformed into 
ğ¸;(!!&%|ğš!&%)Â‹ğ¾ğ¿[ğ‘„(ğ°&+'|ğ‘œ&+', ğš&+')||ğ‘„(ğ°&+'|ğš&+')]ÂŒ = H(ğ‘œ&+'|ğ°&+') âˆ’ H(ğ‘œ&+'), 
where the first and second terms represent the conditional and marginal entropies, respectively:  
H(ğ‘œ&+'|ğ°&+') = E5(!!&%|ğ°!&%,ğš!&%)=(ğ’˜!&%|ğ’‚!&%)[âˆ’lnğ‘ƒ(ğ‘œ&+'|ğ°&+', ğš&+')], (10) 
H(ğ‘œ&+') = E5(!!&%|ğš!&%)[âˆ’lnğ‘ƒ(ğ‘œ&+'|ğš&+')]. 
The conditional entropy H(ğ‘œ&+'|ğ°&+') can be calculated by substituting equation (5) into 
equation (10) as 
H(ğ‘œ&+'|ğ°&+') = âˆ’ğ¸=(ğ°!&%|ğš!&%) Â›Â ğ‘",&+'ğ‘”(ğ‘¤",&+')
"
Âœ, 
where 
ğ‘”(ğ‘¤) = ğ‘“(ğ‘¤)lnğ‘“(ğ‘¤) + (1 âˆ’ ğ‘“(ğ‘¤))ln(1 âˆ’ ğ‘“(ğ‘¤)). 
Here, we approximately calculate this equation by using the second-order Taylor expansion as  
H(ğ‘œ&+'|ğ°&+') â‰… âˆ’ğ¸=(ğ°!&%|ğš!&%) ÂÂ ğ‘",&+' Â„ğ‘”(ğœ‡",&+') + ğœ•ğ‘”
ğœ•ğ‘¤",&+'
(ğ‘¤",&+' âˆ’ ğœ‡",&+')
"
+ 1
2
ğœ•*ğ‘”
ğœ•ğ‘¤",&+'	
* ^ğ‘¤",&+' âˆ’ ğœ‡",&+'_
*
Â…Â, 
which leads to  
H(ğ‘œ&+'|ğ°&+') = âˆ‘ ğ‘",&+' Â›ğ‘“(ğœ‡",&+')lnğ‘“(ğœ‡",&+') + ^1 âˆ’ ğ‘“(ğœ‡",&+')_ln^1 âˆ’"
ğ‘“(ğœ‡",&+')_ +
'
* )ğ‘“(ğœ‡",&+')^1 âˆ’ ğ‘“(ğœ‡",&+')_ ÂŸ1 + ^1 âˆ’ 2ğ‘“(ğœ‡",&+')_ln
@(3",!&%)
'#@(3",!&%)Â - (ğ‘",&
#' + ğ›¼)Âœ. 
 The marginal entropy H(ğ‘œ&+') can be calculated as  
H(ğ‘œ&+') = Â ğ‘",&+'{ğ‘ƒ(ğ‘œ&+' = 0|ğš&+')lnğ‘ƒ(ğ‘œ&+' = 0|ğš&+')
"
+ ğ‘ƒ(ğ‘œ&+' = 1|ğš&+')lnğ‘ƒ(ğ‘œ&+' = 1|ğš&+')}, 
where 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
14 
 
ğ‘ƒ(ğ‘œ&+'|ğš&+') = v ğ‘ƒ(ğ‘œ&+'|ğ°&+', ğš&+')ğ‘„(ğ°&+'|ğš&+')ğ‘‘ğ°&+'
= v h )ğ‘“^ğ‘¤",&+'_
!!&%
Â¡1 âˆ’ ğ‘“^ğ‘¤",&+'_Â¢
'#!!&%
-
,",!&%
ğ‘„(ğ°&+'|ğš&+')ğ‘‘ğ°&+'
"
= h Â›ğ‘“(ğœ‡",&+')!!&%^1 âˆ’ ğ‘“(ğœ‡",&+')_
'#!!&%
"
+ 1!!&%(âˆ’1)'#!!&%
1
2 ğ‘“(ğœ‡",&+') Â£1 âˆ’ ğ‘“(ğœ‡",&+')j1 âˆ’ 2ğ‘“(ğœ‡",&+')kÂ¤ (ğ‘",&
#' + ğ›¼)Âœ
,",&+'
. 
 The second term of the expected utility (equation (10)) is calculated as  
ğ¸5(!!&%|ğš!&%)[lnğ‘ƒ(ğ‘œ&+')] = ğ¸5(!!&%|ğš!&%)[ğ‘œ&+'lnğ‘ƒ! + (1 âˆ’ ğ‘œ&+')ln	(1 âˆ’ ğ‘ƒ!)] 
																													= 	ğ‘ƒ(ğ‘œ&+' = 0|ğš&+')ln	(1 âˆ’ ğ‘ƒ!) + ğ‘ƒ(ğ‘œ&+' = 1|ğš&+')ln	(1 âˆ’ ğ‘ƒ!). 
 
The observer-SSM  
We constructed the observer-SSM, which describes the temporal transitions of the latent internal state 
of agent z and the generation of action, from the viewpoint of the observer of the agent. This is depicted 
as a graphical representation in Fig. 4. As prior information, we assumed that the agent acts based on 
the internal state, that is, the intensity of curiosity, the recognized reward probabilities, and their 
confidence levels. The intensity of curiosity was assumed to change temporally as a random walk:  
ğ‘& = ğ‘&#' + ğœ–ğœ&, 
where ğœ& denotes white noise with zero mean and unit variance, and ğœ– denotes its noise intensity. Other 
internal states, that is, ğœ‡" and ğ‘", were assumed to update as equations (1) and (2). The transition of 
the internal state is expressed by the probability distribution,  
ğ‘ƒ(ğ³&|ğ³&#') = ğ’©(ğ³&|ğ…(ğ³&#', ğš&#'), ğšª), (11) 
ğ…(ğ³&#', ğš&#') =
â£
â¢
â¢
â¢
â¢
â¡ 0
â„(ğœ‡',&#', ğ‘',&#', ğ‘œ&, ğ‘')
â„(ğœ‡*,&#', ğ‘*,&#', ğ‘œ&, ğ‘*)
ğ‘˜(ğœ‡',&#', ğ‘',&#', ğ‘')
ğ‘˜(ğœ‡*,&#', ğ‘*,&#', ğ‘*) â¦
â¥
â¥
â¥
â¥
â¤
, 
ğšª = ğœ–*âˆ†',', 
where ğ³# = (ğ‘#, ğ#
-, ğ’‘#
-). ; â„(ğœ‡",&#', ğ‘",&#', ğ‘œ&, ğ‘") and ğ‘˜(ğœ‡",&#', ğ‘",&#', ğ‘") represent the right-hand 
sides of equations (1) and (2), respectively, and âˆ†!,/ denotes a variance-covariance matrix with 1 at 
(ğ‘–, ğ‘—) component and 0 at others. In addition, the agent was assumed to select an action ğš&+' based 
on the expected utilities, as follows:  
ğ‘ƒ(ğš&+') = exp	(ğ‘ˆ(ğš&+'))
âˆ‘ exp	(ğ‘ˆ(ğš))ğš
, 
and the reward was obtained by the following probability distribution:  
ğ‘ƒ(ğ‘œ&|ğš&) = h jğœ†",&
!! (1 âˆ’ ğœ†",&)'#!! k
,",!
.
"
 
 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
15 
 
iFEP by particle filter and Kalman backward algorithm 
Based on the observer-SSM, we estimated the posterior distribution of the latent internal state of agent 
ğ‘§# given all observations from 1 to T (ğ‘¥%:-) in a Bayesian manner, i.e., ğ‘ƒ(ğ‘§#|ğ‘¥%:-). This estimation was 
done by forward and backward algorithms, which are called filtering and smoothing, respectively.  
 In filtering, the posterior distribution of ğ‘§# given observations until t (ğ‘¥%:#) is sequentially updated 
in a forward direction as  
ğ‘ƒ(ğ³#|ğ±%:#) âˆ ğ‘ƒ(ğ±#|ğ³#, ğœƒ) T ğ‘ƒ(ğ³#|ğ³#$%, ğœƒ)ğ‘ƒ(ğ³#$%|ğ±%:#$%)ğ‘‘ğ³#$%, 
where ğ±# = (ğš#
-, ğ‘œ#). and ğœƒ = {ğœ', ğ›¼, ğ‘ƒğ‘œ}. The prior distribution of ğ³% is 
ğ‘ƒ(ğ³%) = ğ’©(ğ›%|ğ›0, ğœ1
'ğˆ)ğ’©(ğ©%|ğ©+, ğœ2
'ğˆ)ğ’©(ğœ%|ğœ+, ğœ3
'ğˆ), 
where ğ›0, ğ©+and ğœ0 denote means, and ğœ1
', ğœ2
' and ğœ3
' denote variances. We used a particle filter24 to 
sequentially calculate the posterior ğ‘ƒ(ğ³#|ğ±%:#), which cannot be analytically derived because of the 
nonlinear transition probability.  
 After the particle filter, the posterior distribution of ğ³# given all observations (ğ±%:-) is sequentially 
updated in a backward direction as  
ğ‘ƒ(ğ³ğ‘¡|ğ±1:ğ‘‡) = v ğ‘ƒ(ğ³ğ‘¡+1|ğ±1:ğ‘¡+1)ğ‘ƒ(ğ³ğ‘¡+1|ğ³ğ‘¡, ğœƒ)ğ‘‘ğ³ğ‘¡âˆ’1 . (12) 
However, this integration is intractable due to non-Gaussian	ğ‘ƒ(ğ³#$%|ğ±%:#$%),	which	was represented 
by particle ensemble in the particle filter, and non-linear relationship between ğ³#$%  and ğ³#  in 
ğ‘ƒ(ğ³#|ğ³#$%, ğœƒ) (equation (11)). Thus, we approximated ğ‘ƒ(ğ³#|ğ±%:#) as ğ’©(ğ³#|ğ¦#, ğ•#), where ğ¦# and ğ•# 
denote sample mean and sample variance of particles at t, whereas we linearized ğ‘ƒ(ğ³#|ğ³#$%, ğœƒ) as  
ğ‘ƒ(ğ³#|ğ³#$%, ğœƒ) â‰… ğ’©(ğ³#|ğ€ğ³#$% + ğ›, ğšª), 
ğ€ = ğœ•ğ…(ğ³ğ‘¡âˆ’1, ğšğ‘¡âˆ’1)
ğœ•ğ³#$%
g
ğ¦!
, 
ğ› = ğ¹(ğ¦#, ğš#$%) âˆ’ ğ€ğ¦#, 
where ğ€ denotes Jacobian matrix. Because these approximations make integration of equation (12) 
tractable, the posterior distribution ğ‘ƒ(ğ³#|ğ±%:-) can computed by Gaussian distribution as  
ğ‘ƒ(ğ³#|ğ±%:-) = ğ’©7ğ³#|ğ¦i#, ğ•j#9. 
Its mean and variance was analytically updated by Kalman backward algorithms25:  
ğ¦i# = ğ¦# + ğ‰#{ğ¦i#(% âˆ’ (ğ€ğ¦# + ğ›)}, 
ğ•j# = ğ•# + ğ‰#{ğ¦i#(% âˆ’ (ğ€ğ•#ğ€. + ğšª)}ğ‰#
., 
where 
ğ‰# = ğ•#ğ€.(ğ€ğ•#ğ€. + ğšª)$ğŸ. 
 
Estimation of parameter in iFEP  
To estimate parameter ğœƒ, we extended the observer-SSM to a self-organizing state-space model26 in 
which ğœƒ was also addressed as constant latent variables:   
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
16 
 
ğ‘ƒ(ğ³#, ğœƒ|ğ±%:#) âˆ ğ‘ƒ(ğ±#|ğ³#) T ğ‘ƒ(ğ³#|ğ³#$%, ğœƒ)ğ‘ƒ(ğ³#$%, ğœƒ|ğ±%:#$%)ğ‘‘ğ³#$%, 
ğ‘ƒ(ğ³#, ğœƒ) = ğ‘ƒ(ğ³%)ğ‘ƒ(ğœƒ), 
where ğ‘ƒ(ğœƒ) = ğ’©(ğœ'|ğ‘š:, ğ‘£:)ğ’©(ğ›¼|ğ‘š;, ğ‘£;)ğ’©(ğ‘ƒ+|ğ‘š2", ğ‘£2"). To sequentially calculate the posterior 
ğ‘ƒ(ğ³#, ğœƒ|ğ±%:#)  using the particle filter, we augmented the state vector of all particles by adding 
parameter ğœƒ, which was not updated from randomly sampled initial values.  
 
Statistical testing with Monte Carlo simulations 
In Fig. S1, we statistical ly tested negative curiosity estimated in Fig. 6. A null hypothesis is that an 
agent who has no curiosity (i.e., ğ‘& = 0) and decides on a choice only depending on its recognition of 
the reward probability. Under the null hypothesis, model simulations were repeated 1,000 times under 
the same experimental conditions as in Fig. 6 and the curiosity was estimated for each using iFEP. We 
adopted temporal average of estimated curiosity as a test statistic and  plotted the null distribution of 
the test statistic. Compared with the estimated curiosity of the rat behavior, we then computed p-value 
for one sided left tailed test.  
 
 
Acknowledgments 
We are grateful to Prof. Kenji Doya and Dr. Makoto Ito for providing rat behavioral data. We thank 
organizers of the tutorial on the free-energy principle in 2019, that inspired this research. This study 
was supported in part by Grant -in-Aid for Transformative Research Areas (B) [grant number 
21H05170], AMED [Grant Number JP21wm0425010], Moonshot R&Dâ€“MILLENNIA Program 
[grant number JPMJMS2024 -9] by JST, the Cooperative Study Program of Exploratory Research 
Center on Life and Living Systems (ExCELLS) [program number 21 -102], and the grant of Joint 
Research by the National Institutes of Natural Sciences [NINS Programme No. 01112102].  
 
Author contributions 
H.N. conceived of the project. Y . K. and H.N. developed the method and Y .K. implemented the model 
simulation. Y .K. and H.N. wrote the manuscript.  
 
Competing interests 
The authors declare no competing interests. 
 
  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
17 
 
Data availability 
We used the rat behavioral data published by Ito and Doya, J Neurosci (2009)17, which is publicly 
available at Prof. Doya Kenjiâ€™s homepage: https://groups.oist.jp/ja/ncu/data. Source codes will be 
open after publication.  
 
 
References 
1. Helmholtz, H. Handbuch der Physiologischen Optik. 1867 (1867). 
2. Yuille, A. & Kersten, D. Vision as Bayesian inference: analysis by synthesis? Trends Cogn. Sci. 
10, 301â€“308 (2006). 
3. Millett, J. D. & Simon, H. A. Administrative Behavior: A Study of Decision-Making Processes in 
Administrative Organization. Polit. Sci. Q. 62, 621 (1947). 
4. Dubey, R. & Griffiths, T. L. Understanding exploration in humans and machines by formalizing 
the function of curiosity. Curr. Opin. Behav. Sci. 35, 118â€“124 (2020). 
5. Kidd, C. & Hayden, B. Y. The Psychology and Neuroscience of Curiosity. Neuron 88, 449â€“460 
(2015). 
6. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction. (MIT Press, 1998). 
7. Friston, K. A theory of cortical responses. Philos. Trans. R. Soc. B Biol. Sci. 360, 815â€“836 
(2005). 
8. Friston, K., Kilner, J. & Harrison, L. A free energy principle for the brain. J. Physiol. Paris 100, 
70â€“87 (2006). 
9. Friston, K. The free-energy principle: A unified brain theory? Nat. Rev. Neurosci. 11, 127â€“138 
(2010). 
10. Friston, K. et al. Active inference and epistemic value. Cogn. Neurosci. 6, 187â€“214 (2015). 
11. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G. Active Inference: A 
Process Theory. Neural Comput. 29, 1â€“49 (2017). 
12. Schultz, W., Dayan, P. & Montague, P. R. A neural substrate of prediction and reward. Science 
(80-. ). 275, 1593â€“1599 (1997). 
13. Montague, P. R., Dayan, P. & Sejnowski, T. J. A framework for mesencephalic dopamine 
systems based on predictive Hebbian learning. J. Neurosci. 16, 1936â€“1947 (1996). 
14. Tanaka, C. S. et al. Prediction of immediate and future rewards differentially recruits cortico-
basal ganglia loops. Nat. Neurosci. 7, 887â€“893 (2004). 
15. Samejima, K., Ueda, Y., Doya, K. & Kimura, M. Neuroscience: Representation of action-specific 
reward values in the striatum. Science (80-. ). 310, 1337â€“1340 (2005). 
16. Ishii, S., Yoshida, W. & Yoshimoto, J. Control of exploitation-exploration meta-parameter in 
reinforcement learning. Neural Networks 15, 665â€“687 (2002). 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
18 
 
17. Ito, M. & Doya, K. Validation of decision-making models and analysis of decision variables in 
the rat basal ganglia. J. Neurosci. 29, 9861â€“9874 (2009). 
18. Montague, P. R., Hyman, S. E. & Cohen, J. D. Computational roles for dopamine in behavioural 
control. Nature 431, 760â€“767 (2004). 
19. Mizoguchi, H. et al. Insular neural system controls decision-making in healthy and 
methamphetamine-treated rats. Proc. Natl. Acad. Sci. U. S. A. 112, E3930â€“E3939 (2015). 
20. Katahira, K. The statistical structures of reinforcement learning with asymmetric value updates. J. 
Math. Psychol. 87, 31â€“45 (2018). 
21. Schwartenbeck, P. et al. Computational mechanisms of curiosity and goal-directed exploration. 
Elife 8, 1â€“45 (2019). 
22. Ortega, P. A. & Braun, D. A. Thermodynamics as a theory of decision-making with information-
processing costs Subject Areasâ€‰: Author for correspondenceâ€‰: Proc. R. Soc. London. Part A 469, 
20120683 (2013). 
23. Gottwald, S. & Braun, D. A. The two kinds of free energy and the Bayesian revolution. PLoS 
Computational Biology vol. 16 (2020). 
24. Kitagawa, G. Monte Carlo Filter and Smoother for Non-Gaussian Nonlinear State Space Models. 
J. Comput. Graph. Stat. 5, 1â€“25 (1996). 
25. Bishop, C. M. Pattern recognition and machine learning. (New Yorkâ€‰: Springer, [2006] Â©2006). 
26. Journal, S., Statistical, A. & Sep, N. A Self-Organizing State-Space Model Author ( s ): Genshiro 
Kitagawa Published byâ€‰: Taylor & Francis , Ltd . on behalf of the American Statistical 
Association Stable URLâ€‰: http://www.jstor.org/stable/2669862 All use subject to 
http://about.jstor.org/terms. Am. Stat. 53, 326â€“331 (1999). 
 
  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
19 
 
 Figures 
 
Fig. 1: Decision-making model for the two-choice task with rewardâ€“curiosity dilemma 
(A) Decision-making in the two -choice task. Reward is provided at different probabilities for each 
option. The agent does not know those probabilities. Through repeated trial and error, the agent 
recognizes the world by inferring the latent reward probability of each option, and decides to choose 
the next action, i.e., option, based on its own inference. (B) Generative model of reward. The agent 
assumes that reward is generated by the latent variable ğ‘¤ (left panel), and the reward probability is 
sigmoidally controlled by w (middle panel). The presence of the reward obeys a Bernoulli distribution 
(right panel). (C) Recognition process by agent. The agent assumes that the reward probabilities 
change over time owing to fluctuation of the latent variable ğ‘¤# (upper). The probabilistic dependence 
among the latent variable ğ‘¤, action ğ‘, and the presence of reward o is graphically described (middle). 
The agent recognizes ğ‘¤  as a probability distribution, whose precision, i.e., inverse of variance, 
represents confidence, and sequentially updates its recognized distribution from previous rewards and 
actions in a Bayesian manner (lower). (D) Action selection process by agent. The agent evaluates the 
utility of each action using the weighted sum of the expected reward an d information gain (upper). 
The agent compares the utilities for both actions and prefers the option with the larger utility (middle). 
The expected information gain can be calculated as the difference between the currently recognized 
distribution and the predicted recognized distribution updated by the future observation (lower).  
w
w
1
0
Latent variable
Recognition
C Recognition process
B
A
Action
D Action process
Reward
o=1
Bernoulli distribution
No Reward
o=0
Observation
(presence of reward)o
Latent
variable
Observation
Action
wt-2
w
time
wt-1 wt
wt=wt-1+noise
wt+1
ot-2 ot-1 ot ot+1
at-2 at-1 at at+1
Ut(at+1=tÉ¾[inomation ain!+[ewa!
Old
Belief
New
Belief
"pdate
R
R
# #
#
R
or

time
t=t-1+noise
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
20 
 
 
Fig. 2: Simulations of the decision-making model  
(A) Two-choice task with constant reward probabilities. (B) Selection probabilities for left and right 
options. These were plotted by moving average 101 window width. (C) Recognized reward 
probabilities for left and right options. Dashed lines represent the ground truths of the reward 
probabilities. (D) Confidences of reward probability recognitions for left and right options. Expected 
brief updates (E), expected reward (F), and expected utility (G) for left and right options. (Aâ€™) Two-
choice task with constant and temporally varying reward probabilities for left a nd right options, 
respectively. (Bâ€™-Gâ€™) The same as (B -G). In these simulations, the curiosity parameter and reward 
seeking parameter were all kept constant at ğ‘ = 1 and ğ‘ƒ+ = 0.9, respectively.  
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
21 
 
 
Fig. 3: Curiosity-dependent irrational behaviors  
(A) Two-choice task with different, constant reward probabilities: 0% for the left, and 50% for the 
right. (B) Heatmap of selection probability of the right option varying the parameters of curiosity and 
reward intensity (upper panel). There are three representative conditions indicated by black dots: ğ‘ =
0, ğ‘ƒ+ = 1  [a],	 ğ‘ = 10, ğ‘ƒ+ = 0.5  [b], and ğ‘ = âˆ’10, ğ‘ƒ+ = 0.75  [c]. Lower fou r panels present the 
selection ratios of the right option for conditions [a], [b], and [c]. (Aâ€™) Two-choice task with different, 
constant reward probabilities: 50% for the left and 100% for the right. (Bâ€™) is the same as (B).  
Ratio
Ratio
Reward intensityPo
Intensity of Curiosity
Ct
aa
a


 a





Reward intensityPo
Intensity of Curiosity
Ct
Ratio
Ratio
   




(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
22 
 
 
Fig. 4: Scheme of inverse FEP by an observer of a decision-making agent.   
(A) An agent performing a two-choice task from the observer's perspective. (B) Observerâ€™s assumption 
about the agentâ€™s decision -making. The agent is assumed to follow the decision -making model, as 
described in Fig. 1. (C) State-space model of the observerâ€™s eye. For the observer, the agentâ€™s rewardâ€“
curiosity conflict, recognized rew ard probabilities, and their uncertainties are temporally varying 
latent variables, whereas the agentâ€™s action and the presence/absence of a reward are observable. The 
observer estimates the latent internal states of the agent. 
 
 
 
 
  
 
Latent
variable
Observation
Action
zt-2 zt-1 zt zt+1
ot-2 ot-1 ot ot+1
at-2 at-1 at at+1
Looking
Obserbation
Action
A
B
Iamwondering
É¾howmuchistherewardprobability.
É¾whichoptionshouldbeselected.
Iamwondering
É¾howtheagentrecognizetherewardprobability.
É¾howmuchconfidencedotheagenthaveinitsrecognition.
Latent
variable
Observation
Observation
Action
	t-2

2t-2
ot-2 ot-1 ot ot+1
t-2 t-1 t t+1
at-2 at-1 at at+1

2t-1
	t-1 	t

2t
	t+1

2t+1
Old
Belief
New
Belief
Update
R
I IR
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
23 
 
 
 
Fig. 5: Estimation of the simulated agentâ€™s internal state by inverse FEP  
(A) Two-choice task with temporally varying reward probabilities and agent having temporally 
varying curiosity. (B) Simulated agentâ€™s behaviors. Blue and red vertical lines indicate selections of 
left and right options, respectively. Long and short vertical lines indicate the presence and absence of 
rewards, respectively. Gray line indicates the moving average of selection probability of the left option 
with 101 window width. (C) Moving average of selection probabilities for left and right options. (Dâ€“
H) Simulated agentâ€™s behavior-driven estimations of agent -recognized reward probabilities for left 
(D) and right (E) options, agentâ€™s confidence about recognized reward probabilities for left (F) and 
right (G) options, and agentâ€™s curiosity (H). Orange and dashed blue lines represent the estimations 
and ground truths, respectively.  
 
rewarded
non-rewarded
left
right
Selectionprobability
Confidence
(Left)
IntensityofCuriosity
Estimatedreward
probability(Right)
Estimatedreward
probability(Left)
Confidence
(Right)
Left
P(L)
Right
Left
Right
Estimation Groundtruth Estimation Groundtruth
Estimation Groundtruth
Trials TrialsTrials
Trials
C
!
"
# E
$ G %
Rewardprobability
Trials
Rewardprobability
Trials
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
24 
 
 
Fig. 6: Estimation of the ratâ€™s internal state by inverse FEP  
(A) A rat in the two-choice task with temporally switching reward probabilities. (B) Ratâ€™s behaviors 
in the two-choice task. A public dataset at https://groups.oist.jp/ja/ncu/data was used 5. Blue and red 
vertical lines indicate selections of left and right options, respectively. Long and short vertical lines 
indicate the presence and absence of rewards, respectively. Gray line indicates the moving average of 
the selection probability of the left option with 25 window-width backward. (C) Moving average of 
selection probabilities for left and right options. (D, E) Rat behavior-driven estimations of agent -
recognized reward probabilities for left (D) and right (E) options. Red and dashed blue lines represent 
the estimations and ground truths, respectively.  (Fâ€“H) Rat behavior-driven estimations of agentâ€™s 
confidence about recognized reward probabilities for left (F) and right (G) options, and agentâ€™s 
curiosity (H).  
 
 
 
 
 
 
 
rewarded
non-rewarded
left
right
(0.9,0.5)(L,R)= (0.5,0.9) (0.5,0.1) (0.1,0.5) (0.5,0.9) (0.9,0.5) (0.1,0.5) (0.5,0.1)
Selectionprobability
Confidence
(Left)
Inten!ity ofC"rio!ity
#!ti$atedreward
probability (Right)
#!ti$atedreward
probability (Left)
Confidence
(Right)
%ro"nd tr"th
Left
#!ti$ation
Right
Left
&(L)
Right
'rial! 'rial!'rial!
'rial!
C
(
)
* #
+ % ,
Rewardprobability
'rial!
Rewardprobability
'rial!
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 
25 
 
Supplementary Information 
 
Fig. S1: Monte Carlo statistical testing of estimated negative curiosity in rat  
(A) Model simulations conducted under the same experimental conditions as in Fig. 6 under a null 
hypothesis that the rat behavior is generated by zero curiosity. Simulations were repeated 1,000 times, 
and the curiosity was estimated using iFEP for each.  
(B) Null distribution for temporal average of estimated curiosity by 1,000 times simulations. The 
estimated curiosity of the rat shown in red circles was on the left side and did not overlap with the 
distribution, which means p=0 (one-sided, left tailed test). 
0.2
0.18
0.12
0.06
0
Selectionprobability
Confidence
(Left)
IntensityofCuriosity
Estimatedreward
probability(Ri!"t)
Estimatedreward
probability(Left)
Confidence
(Ri!"t)
Left
#(L)
Ri!"t
Rewardprobability
$rials
Rewardprobability
$rials
rewarded
non%rewarded
left
ri!"t
(0.&'0.()(L'R)) (0.('0.&) (0.('0.1) (0.1'0.() (0.('0.&) (0.&'0.() (0.1'0.() (0.('0.1)
*randtrut"
Left
Estimation
Ri!"t
repeat
+1000
c)0
estimatedcuriosityoft"erat
temporala,era!eofestimatedcuriosity
by10000timessimulations
-
.
Selectionprobability
Confidence
(Left)
IntensityofCuriosity
Estimatedreward
probability(Ri!"t)
Estimatedreward
probability(Left)
Confidence
(Ri!"t)
Left
#(L)
Ri!"t
Rewardprobability
$rials
Rewardprobability
$rials
rewarded
non%rewarded
left
ri!"t
(0.&'0.()(L'R)) (0.('0.&) (0.('0.1) (0.1'0.() (0.('0.&) (0.&'0.() (0.1'0.() (0.('0.1)
*randtrut"
Left
Estimation
Ri!"t
#robability
Intensityofcuriosity
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted April 26, 2022. ; https://doi.org/10.1101/2022.04.24.489304doi: bioRxiv preprint 