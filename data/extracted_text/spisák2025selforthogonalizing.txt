arXiv:2505.22749v1  [q-bio.NC]  28 May 2025
Self-orthogonalizing attractor neural networks
emerging from the free energy principle
Tamas Spisak1
Center for Translational Neuro- and Behavioral Sciences (C-TNBS), University Medicine Essen, Germany
Karl Friston
Queen Square Institute of Neurology, University College London, WC1N 3AR, UK
VERSES, Los Angeles, CA 90067, USA
Wednesday 28th May, 2025
Abstract
Attractor dynamics are a hallmark of many complex systems, including the brain. Un-
derstanding how such self-organizing dynamics emerge from first principles is crucial for
advancing our understanding of neuronal computations and the design of artificial intelligence
systems. Here we formalize how attractor networks emerge from the free energy principle
applied to a universal partitioning of random dynamical systems. Our approach obviates the
need for explicitly imposed learning and inference rules and identifies emergent, but efficient
and biologically plausible inference and learning dynamics for such self-organizing systems.
These result in a collective, multi-level Bayesian active inference process. Attractors on the
free energy landscape encode prior beliefs; inference integrates sensory data into posterior
beliefs; and learning fine-tunes couplings to minimize long-term surprise. Analytically and
via simulations, we establish that the proposed networks favor approximately orthogonalized
attractor representations, a consequence of simultaneously optimizing predictive accuracy
and model complexity. These attractors efficiently span the input subspace, enhancing
generalization and the mutual information between hidden causes and observable effects.
Furthermore, while random data presentation leads to symmetric and sparse couplings,
sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings offer a
unifying theory of self-organizing attractor networks, providing novel insights for AI and
neuroscience.
Keywords attractor networks, active inference, orthogonal representations, self-organization
Interactive manuscript: https://pni-lab.github.io/fep-attractor-network
Graphical Abstract
1Correspondence to: tamas.spisak@uk-essen.de
1 Highlights
• Attractor networks are derived from the Free Energy Principle (FEP) applied to a universal parti-
tioning of random dynamical systems.
• This approach yields emergent, biologically plausible inference and learning dynamics, forming a
multi-level Bayesian active inference process.
• The networks favor approximately orthogonalized attractor representations, optimizing predictive
accuracy and model complexity.
• Sequentialdatapresentationleadstoasymmetriccouplingsandnon-equilibriumsteady-statedynamics,
extending conventional Boltzmann Machines.
• Simulations demonstrate orthogonal basis formation, generalization, sequence learning, and resistance
to catastrophic forgetting.
2 Introduction
From whirlpools and bird flocks to neuronal and social networks, countless natural systems can be characterized
by dynamics organized aroundattractor states[Haken, 1978]. Such systems can be decomposed into a collection
of - less or more complex - building blocks or “particles” (e.g. water molecules, birds, neurons, or people),
which are coupled through local interactions. Attractors are an emergent consequence of the collective
dynamics of the system, arising from these local interactions, without any individual particles exerting global
control.
Attractors are a key concept in dynamical systems theory, defined as a set of states in the state space of
the system to which nearby trajectories converge [Guckenheimer et al., 1984]. Geometrically, the simplest
attractors are fixed points and limit cycles (representing periodic oscillations). However, the concept extends
to more complex structures like strange attractors associated with chaotic behavior, as well as phenomena
arising in stochastic or non-equilibrium settings, such as probability distributions over states (stochastic
attractors), transient states reflecting past dynamics (ghost attractors or attractor ruins), and trajectories that
cycle through sequences of unstable states (sequential attractors or heteroclinic cycles). Artificial attractor
neural networks [Amit, 1989] represent a class of recurrent neural networks specifically designed to leverage
attractor dynamics. While the specific forms and behaviors of these networks are heavily influenced by the
chosen inference and learning rules, self-organization is a key feature of all variants, as the stable states
emerge from the interactions between network elements without explicit external coordination. This property
makes them particularly relevant as models for self-organizing biological systems, including the brain. It
is clear that the brain is also a complex attractor network. Attractor dynamics have long been proposed
to play a significant role in information integration at the circuit level [Freeman, 1987, Amit, 1989, Deco
and Rolls, 2003, Nartallo-Kaluarachchi et al., 2025, Tsodyks, 1999] and have become established models
for canonical brain circuits involved in motor control, sensory amplification, motion integration, evidence
integration, memory, decision-making, and spatial navigation (see Khona and Fiete [2022] for a review).
For instance, the activity of head direction cells - neurons that fire in a direction-dependent manner - are
known to arise from a circular attractor state, produced by a so-called ring attractor network [Zhang, 1996].
Multi- and metastable attractor dynamics have also been proposed to extend to the meso- and macro-scales
[Rolls, 2009], “accommodating the coordination of heterogeneous elements” [Kelso, 2012], rendering attractor
dynamics an overarching computational mechanism across different scales of brain function. The brain, as an
instance of complex attractor networks, showcases not only the computational capabilities of this network
architecture but also its ability to emerge and evolve through self-organization.
When discussing self-organization in attractor networks, we will differentiate two distinct levels. First, we can
talk aboutoperational self-organization: the capacity of a pre-formed network to settle into attractor states
during its operation. This however does not encompass the network’s ability to “build itself” – to emerge
from simple, local interactions without explicit programming or global control, and to adaptively evolve its
structure and function through learning. This latter level of self-organization is what we will refer to as
adaptive self-organization. Such architectures would mirror the nervous system’s capacity to not just function
as an attractor network, but to become - and remain - one through a self-directed process of development
and learning. Further, adaptive self-organization would also be a highly desirable property for robotics and
artificial intelligence systems, not only boosting their robustness and adaptability by means of continuous
learning, but potentially leading to systems that can increase their complexity and capabilities organically
over time (e.g. developmental robotics). Therefore, characterizingadaptive self-organizationin attractor
2
networks is vital for advancing our understanding of the brain and for creating more autonomous, adaptive,
brain-inspired AI systems.
The Free Energy Principle (FEP) offers a general framework to study self-organization to nonequilibrium
steady states as Bayesian inference (a.k.a., active inference). The FEP has been pivotal in connecting the
dynamics of complex self-organizing systems with computational and inferential processes, especially within
the realms of brain function [Friston et al., 2023, Friston and Ao, 2012, Palacios et al., 2020]. The FEP posits
that any ‘thing’ - in order to exist for an extended period of time - must maintain conditional independence
from its environment. This entails a specific sparsely coupled structure of the system, referred to as a
particular partition, that divides the system into internal, external, and boundary (sensory and active) states
(see Figure 1A). It can be shown that maintaining this sparse coupling is equivalent to executing an inference
process, where internal states deduce the causes of sensory inputs by minimizing variational free energy (see
Friston et al. [2023] or [Friston et al., 2023] for a formal treatment).
Here, we describe the specific class of adaptive self-organizing attractor networks that emerge directly from
the FEP, without the need for explicitly imposed learning or inference rules. First, we show that a hierarchical
formulation ofparticular partitions- a concept that is applicable to any complex dynamical system - can give
rise to systems that have the same functional form as well-known artificial attractor network architectures.
Second, we show that minimizing variational free energy (VFE) with regard to the internal states of such
systems yields a Boltzmann Machine-like stochastic update mechanism, with continuous-state stochastic
Hopfield networks being a special case. Third, we show that minimizing VFE with regard to the internal
blanket or boundary states of the system (couplings) induces a generalized predictive coding-based learning
process. Crucially, this adaptive process extends beyond simply reinforcing concrete sensory patterns; it
learns to span the entire subspace of key patterns by establishing approximately-orthogonalized attractor
representations, which the system can then combine during inference. We use simulations to identify the
requirements for the emergence of quasi-orthogonal attractors and to illustrate the derived attractor networks’
ability to generalize to unseen data. Finally, we highlight, that the derived attractor network can naturally
produce sequence-attractors (if the input data is presented in a clear sequential order) and exemplify its
potential to perform continual learning and to overcome catastrophic forgetting by means of spontaneous
activity. We conclude by discussing testable predictions of our framework, and exploring the broader
implications of these findings for both natural and artificial intelligence systems.
3 Main Results
3.1 Background: particular partitions and the free energy principle
Our effort to characterize self-organizing attractor networks calls for an individuation of ‘self’ from nonself.
Particular partitions, a concept that is at the core of the Free Energy Principle (FEP) [Friston et al., 2022,
2023, Friston and Ao, 2012, Palacios et al., 2020], is a natural way to formalize this individuation. A particular
partition is a partition that divides the states of a systemx into a particle or ‘thing’(s,a,µ ) ⊂x and its
external statesη⊂x, based on their sparse coupling:


˙η(τ)
˙s(τ)
˙a(τ)
˙µ(τ)

=


fη(η,s,a )
fs(η,s,a )
fa(s,a,µ )
fµ(s,a,µ )

+


ωη(τ)
ωs(τ)
ωa(τ)
ωµ(τ)


where µ, s and a are the internal, sensory and active states of the particle, respectively. The fluctuations
ωi,i ∈(µ,s,a,η ) are assumed to be mutually independent. The particular statesµ, s and a are coupled
to each other withparticular flow dependencies; namely, external states can only influence themselves and
sensory states, while internal states can only influence themselves and active states (see Figure 1A). It can be
shown that these coupling constraints mean that external and internal paths are statistically independent,
when conditioned on blanket paths [Friston et al., 2022]:
η⊥µ|s,a. (1)
As shown by Friston et al. [2023], such a particle, in order to persist for an extended period of time, will
necessarily have to maintain this conditional independence structure, a behavior that is equivalent to an
3
inference process in which internal states infer external states through the blanket states (i.e., sensory and
active states) by minimizing free energy [Friston, 2009, 2010, Friston et al., 2023]:
η⊥µ|s,a ⇒ ˙µ= −∇µF(s,a,µ ) (2)
where F(s,a,µ ) is the variational free energy (VFE):
F(s,a,µ ) = Eqµ(η)[ln qµ(η) −ln p(s,a,η )] (3)
with qµ(η) being a variational density over the external states that is parameterized by the internal states
and p(s,a,η ) being the joint probability distribution of the sensory, active and external states, a.k.a. the
generative model [Friston et al., 2016].
Figure 1: Deep Particular Partitions.
A Schematic illustration of a particular partition of a system into internal (µ) and external states (η),
separated by a Markov blanket consisting of sensory states (s) and active states (a). The tuple (µ,s,a )
is called aparticle. A particle, in order to persist for an extended period of time, will necessarily have to
maintain its Markov blanket, a behavior that is equivalent to an inference process in which internal states infer
external states through the blanket states. The resulting self-organization of internal states corresponds to
perception, while actions link the internal states back to the external states.B The internal statesµ⊂xcan
be arbitrarily complex. Without loss of generality, we can consider that the macro-scaleµcan be decomposed
into set of overlapping micro-scalesubparticles (σi,si,ai,sij,aij), so that the internal state of subparticle
σi ⊂µ can be an external state from the perspective of another subparticleσj ⊂µ. Some, or all subparticles
can be connected to the macro-scale external stateη, through the macro-scale Markov blanket, giving a
decomposition of the original boundary states intosi ⊂s and ai ⊂a. The subparticles are connected to each
other by the micro-scale boundary statessij and aij. Note that this notation considers the point-of-view of
the i-th subparticle. Taking the perspective of thej-th subparticle, we can see thatsji = aij and aji = sij.
While the figure depicts the simplest case of two nested partitions, the same scheme can be applied recursively
to any number of (possibly nested) subparticles and any coupling structure amongst them.
3.2 Deep particular partitions and subparticles
Particular partitions provide a universal description of complex systems, in a sense that the internal states
µ behave as if they are inferring the external states under a generative model; i.e., a ‘black box’ inference
process (or computation), which can be arbitrarily complex. At the same time, the concept of particular
partitions speaks to a recursive composition of ensembles (of things) at increasingly higher spatiotemporal
scales [Friston, 2019, Palacios et al., 2020, Clark, 2017], which yields a natural way to resolve the internal
complexity ofµ. Partitioning the “macro-scale” particleµinto multiple, overlapping “micro-scale”subparticles
{πi}n
i=1 - that themselves are particular partitions - we can unfold the complexity of the macro-scale particle
to an arbitrary degree. As subparticles can be nested arbitrarily deep - yielding a hierarchical generative
model - we refer to such a partitioning as adeep particular partition.
As illustrated in Figure 1B, each subparticleπi has internal statesσi, and the coupling between any two
subparticles i and j is mediated by micro-scale boundary states: sensory statessij (representing the sensory
4
information inicoming fromj) and active statesaij (representing the action ofion j). The boundary states
of subparticles naturally overlap; the sensory state of a subparticleσi is the active state ofσj and vice versa,
i.e. aji = sij and sji = aij. This also means that, at the micro-scale, the internal state of a subparticle
σi ⊂µ is part of the external states for another subparticle inσj ⊂µ. Accordingly, the internal state of a
subparticle σi is conditionally independent of any other internal statesσj with j ̸= i, given the blanket states
of the subparticles:
σi ⊥σj |aij,sij (4)
Note that this definition embraces sparse couplings across subparticles, asaij and sij may be empty for
a givenj (no direct connection between the two subparticles), but we require the subparticles to yield a
complete coverageof µ: ⋃n
i=1 πi = µ.
3.3 The emergence of attractor neural networks from deep particular partitions
Next, we establish a prototypical mathematical parametrization for an arbitrary deep particular partition,
shown on Figure 2, with the aim of demonstrating that such complex, sparsely coupled random dynamical
systems can give rise to artificial attractor neural networks.
Figure 2: Parametrization of subparticles in a deep particular partition.
The internal state σi of subparticle πi follows a continuous Bernoulli distribution, (a.k.a. a truncated
exponential distribution supported on the interval[−1,+1] ⊂R, see Appendix 1), with a prior “bias”bi that
can be interpreted as a priori log-odds evidence for an event (stemming from a macro-scale sensory input
si - not shown, or from the internal dynamics ofσi itself, e.g. internal sequence dynamics). The stateσi
is coupled to the internal state of another subparticleσj through the micro-scale boundary statessij and
aij. The boundary states simply apply a deterministic scaling to their respectiveσ state, with a weight (Jij)
implemented by a Dirac delta function shifted byJij (i.e. we deal with conservative subparticles, in the sense
of Friston et al. [2023]). The stateσi is influenced by its sensory inputsi in a way thatsi gets integrated
into its internal bias, updating the level of evidence for the represented event.
In our example parametrization, we assume that the internal states of subparticles in a complex particular
partition arecontinuous Bernoulli states (also known as a truncated exponential distribution, see Appendix 1
for details), denoted byσi ∼CBbi:
p(σi) ∝ebσi (5)
Here, σi ∈[−1,+1] ⊂R, andbi ∈R represents an a priori bias (e.g. the level of prior log-odds evidence for
an event) inσi Figure 2. Zero bias represents a flat prior (uniform distribution over [-1,1]), while positive or
negative biases represent evidence for or against an event, respectively. The above probability is defined up
to a normalization constant,bi/(2 sinh(bi)). See Appendix 2 for details.
Next, we define the conditional probabilities of the sensory and active states, creating the boundary between
two subparticlesσi and σj: sij|σj ∼δJijσj and aij|σi ∼δJjiσi, whereδ is the Dirac delta function andJ is a
weight matrix. The elementsJij contains the weights of the coupling matrix (see Appendix 3).
Expressed as PDFs:
p(sij|σj) = δ(sij −Jijσj) (6)
p(aij|σi) = δ(aij −Jjiσi) (7)
5
The choice of this deterministic parametrization means that we introduce the assumption of the subparticles
being conservative particles, as defined in Friston et al. [2023].
To close the loop, we define how the internal stateσi depends on its sensory inputsij. We assume the sensory
input simply adds to the prior biasσi|sij ∼CBbi+sij:
p(σi|sij) ∝e(bi+sij)σi (8)
With the continuous Bernoulli distribution, this simply means that the sensory evidencesij adds to (or
subtracts from) the prior beliefbi.
We now write up the direct conditional probability describingσi given σj, marginalizing out the sensory and
active states:
p(σi|σj) =
∫
p(σi|sij)p(sij|σj) dsij ∝
∫
e(sij+bi)σiδ(sij −Jijσj)dsij ∝e(bi+Jijσj)σi (9)
1
Given thatp(σi,σj) = p(σi|σj)p(σj), and using equations (5) and (9), we can express the joint distribution
as follows:
p(σi,σj) = e(bi+Jijσj)σiebjσj = ebiσi+Jijσiσj+bjσj (10)
Next, we observe that the statessij and aij (∀i,j) are the ’blanket states’ of the system, forming a Markov
blanket [Hipólito et al., 2021]. The corresponding conditional independence structure implies that the joint
probability for allσ nodes can be written as the product of the individual joint probabilities,∏
i,jP(σi,σj),
which results in:
p(σ) ∝e
∑
ibiσi+
∑
i̸=jJijσiσj
(11)
However, sinceσiσj = σjσi, each undirected pair{i,j}appears twice in the sum over directed pairs(i,j)
with i̸= j. This mathematical fact leads to an important consequence for the steady-state distribution.
Since σiσj = σjσi, we can rearrange the double sum over distinct pairs:
∑
i̸=j
Jijσiσj =
∑
i<j
(
Jij + Jji
)
σiσj. (12)
Thus, even though we started with non-symmetric couplingsJij and Jji, the joint distribution ends up
depending only on the sumJij + Jji:
p(σ) ∝exp
{ ∑
i
biσi
 
bias term
+
∑
ij
J†
ijσiσj
  
interaction term  
-ve energy
}
(13)
with J†= 1
2 (J + J⊺) (meaning that the diagonal elementsJ†
ii = 0 for alli).
This joint probability distribution takes the functional form of a stochastic continuous-state Hopfield network
(a specific type of Boltzmann machines). As known in such systems, regions of high probability density in
this stationary distribution will constitute “stochastic attractors”, which are the regions of the state space
that the system will tend to converge to. Furthermore, in case of asymmetric couplings, the antisymmetric
1The expected value ofp(σi|σj ) is a sigmoid function ofσj Appendix 4, specifically the Langevin function. This
property allows it to function as an activation function in neural networks, enabling the network to model more
complex patterns and decision boundaries.
6
part of the coupling matrix induces “solenoidal flows”, extending the attractor repertoire with “sequence
attractors” (heteroclinic chains).
Importantly, our derivation shows that, while solenoidal flows that emerge with asymmetric couplings can
break detailed balance and induce non-equilibrium dynamics, the stationary distribution retains a Boltzmann-
like form determined by the symmetric partJ†. This relies on the assumption that the solenoidal flows are
divergence-free - i.e. it does not alter the Boltzmann form of the stationary distribution but, instead it drives
persistent, divergence-free probability currents along iso-potential surfaces. This condition is argued to hold
for conservative particles under a particular partition (see Appendix 5 and Friston et al. [2023]; but also Ao
[2004] and Xing [2010]). Under this condition, the antisymmetric part of the coupling acts only tangentially
to the iso-potential surfaces defined by the symmetric part and, therefore, does not alter the form of the
stationary distribution, merely driving probability fluxes along these surfaces. Nevertheless, as we will see in
the next sections, the dynamics derived from local free energy minimization in this system still depend on
the potentially asymmetric couplingsJ.
3.4 Inference
So far our derivation only relied on the sparsely coupled structure of the system (deep particular partition), but
did not utilize the free energy principle itself. We now consider the implications of free energy minimization
on the dynamics of the derived recurrent neural network. We start by expressing variational free energy
(VFE, eq. (3)) from the point of view of a single node of the attractor networkσi, given observations from all
other nodesσ\i:
F = Eq(σi)[ln q(σi) −ln p(σ\i,σi)] = DKL[q(σi)||p(σi)]  
complexity
−Eq(σ)[ln p(σ\i|σi)]  
accuracy
(14)
Where q(σi) is the approximate posterior distribution overσi, which we will parametrize as aCB with
variational biasbq. We are now interested in how nodeσi must update its bias, given the state of all other
nodes, σ\i and the weightsJij. Intuitively, the last part of eq. (14) tells us that minimizing free energy
will tend to pull the system towards a (local) minimum of the Boltzmann-like energy functional (eq. (13))
of the attractor network (accuracy term), with the constraint that this has to lie close to the initial state
(complexity term).
Let us verify this intuition by substituting our parametrization into eq. (14). First, we rearrange eq. (13) to
get:
p(σ\i|σi) ∝exp

∑
j̸=i
(bj + Ji,jσi) σj + 1
2
∑
j̸=i
∑
k̸=i
Jj,kσjσk

 (15)
Then, we express the accuracy term:
Eq(σi)[ln p(σ\i |σi)] = const+
∑
j̸=i
bjσj + L(bq)
∑
j̸=i
Jijσj + 1
2
∑
j̸=i
∑
k̸=i
Jjkσjσk (16)
The complexity term in eq. (14) is simply the KL-divergence term between twoCBdistributions:
DKL[q(σi)∥p(σi)] =
[
ln
( bq
sinh(bq)
)
+ bqL(bq)
]
−
[
ln
( b
sinh(b)
)
+ bL(bq)
]
(17)
where L(·) is the expected value of theCB, a sigmoid function of the bias, specifically the Langevin function
(Appendix 4).
Combining the complexity and accuracy terms leads to the following expression for VFE:
F = (bq −b)L(bq) −
∑
j̸=i
(bj + L(bq)Jij) σj −1
2
∑
j̸=i
∑
k̸=i
Jjkσjσk + C (18)
where C denotes all constants in the equation that are independent ofσ.
7
For details on the derivation, see Appendix 6.
Now we take the partial derivative of the free energy with respect to the variational bias:
∂F
∂bq
=

bq −b−
∑
j̸=i
Jijσj

dL
dbq
(19)
Setting the derivative to zero, solving forbq, and substituting the expected value of theCBdistribution, we
get:
Eq[σi] = L(bq) = L


bi
bias
+
∑
j̸=i
Jijσj
 
weighted input


  
sigmoid (Langevin)
(20)
In the case of symmetric couplings, the above equation reduces to a Boltzmann-style update rule (specifically,
that of a continuous-state stochastic Hopfield network, with the special sigmoid functionL). While the
deterministic variant of the above inference rule can be derived directly as a gradient descent on the negative
exponent of eq. (13), the presented FEP-based derivation naturally extends this to a probabilistic framework,
with an emerging sigmoid function (through the complexity term). Thus, the FEP minimization provides
the full probabilistic machinery, instead of just moving down deterministically on an energy gradient. The
resulting stochastic dynamics leads to the optimal expected belief under variational inference, naturally
incorporating prior biases, state constraints (sigmoid due to the{−1,1}state space) and equals to a local
approximate Bayesian inference, where the approximate posterior beliefbq balances prior information (bi)
and evidence from neighbours (∑ Jijσj). As we will show later in the manuscript, the inherently stochastic
characteristics of inference is what allows the network as a whole as well, to escape local energy minima over
time and thereby perform macro-scale Bayesian inference.
3.5 Learning
At optimum,q would matchp, causing the VFE’s derivative to vanish. Learning happens, when there is a
systematic change in the prior biasbi that counteracts the update process. This can correspond, for instance,
to an external input (e.g. sensory signal representing increased evidence for an external event), but also as
the result of the possibly complex internal dynamics of a subparticle (e.g. internal sequence dynamics or
memory retrieval). In this case, a subparticle can take use of another (slower) process, to decrease local VFE:
it can change the way its action states are generated; and rely on its vicarious effects on sensory signals.
In our parametrization, this can be achieved by changing the coupling strengthJji corresponding to the
action states. Of note, while changingJji corresponds to a change in action-generation at the local level
of the subparticle (micro-scale), at the macro-scale, it can be considered as a change in the whole system’s
generative model.
Let’s revisit VFE from the perspective of nodei:
F = Eq(σi)
[
ln q(σi) −ln p(σ\i |σi)
]
(21)
and parameterize the distributions as:
ln q(σi) ∝bqσi, ln p(σ\i |σi) ∝uiσi (22)
with ui being the net weighted input to nodei: ui = b+ ∑
j̸=iJijσj.
We obtain:
F = Eq(σi)
[
(bq −ui) σi
]
+ ϕ(ui) −ϕ(bq) (23)
8
At equilibrium (i.e. whenbq = ui), we haveEq[σi] = L(ui). To construct a stochastic (sample-based) estimate,
we can replace the expectationEq[σi] with the instantaneous valueσi. A perturbationδJij produces a change
δui = σjδJij, and by applying the chain rule we get:
dF
dJij
= ∂F
∂ui
∂ui
∂Jij
=
[
L(ui) −σi
]
σj (24)
Substituting backui and rearranging we get:
∆Jij ∝ σiσj
observed correlation (Hebbian)
− L(bi +
∑
k̸=i
Jikσk)σj
  
predicted correlation (anti-Hebbian)
(25)
This learning resembles the family of “Hebbian / anti-Hebbian” or “contrastive” learning rules and it explicitly
implements predictive coding (akin toprospective configuration[Song et al., 2024, Millidge et al., 2022a]).
However, as opposed to e.g. contrastive divergence (a common method for training certain types of Boltzmann
machines, Hinton [2002]), it does not require to contrast longer averages of separate “clamped” (fixed inputs)
and “free” (free running) phases, but rather uses the instantaneous correlation between presynaptic and
postsynaptic activation to update the weight, lending a high degree of scalability for this architecture. As
we demonstrate below with Simulation 1, 2 and 4, this learning rule converges to symmetric weights (akin
to a classical stochastic continuous-state Hopfield network), if input data is presented in random order and
long epochs. At the same time, if data is presented in rapidly changing fixed sequences (Simulation 3), the
learning rule results in temporal predictive coding and learns asymmetric weights, akin to [Millidge et al.,
2024]. As discussed above, in this case the symmetric component ofJ encodes fixed-point attractors and
the probability flux induced by the antisymmetric component results in sequence dynamics (conservative
solenoidal flow), without altering the steady state of the system.
Another key feature of this rule is its resemblance to Sanger’s rule [Sanger, 1989], hinting that it imposes an
approximate orthogonality across attractor states. We motivate this theoretically and with simulations in the
next sections.
3.6 Emergence of approximately orthogonal attractors
Under the FEP, learning not only aims to maximize accuracy but also minimizes complexity (14), leading
to parsimonious internal generative models (encoded in the weightsJ and biases b) that offer efficient
representations of environmental regularities. A generative model is considered more complex (and less
efficient) if its internal representations, specifically its attractor statesσ(µ) - which loosely correspond to
the modes ofp(σ) - are highly correlated or statistically dependent. Such correlations imply redundancy, as
distinct attractors would not be encoding unique, independent features of the input space. Our learning rule
(eq. (25)) - by minimizing micro-scale VFE - inherently also minimizes the complexity termDKL[q(σi)||p(σi)],
which regularizes the inferred stateq(σi) to be close to the node’s current priorp(σi). This not only
induces sparsity in the coupling matrix, but - as we motivate below - also penalizes overlapping attractor
representations and favours orthogonal representations. Hebbian learning - in itself - can not implement such
a behavior; as it simply aims to store the current pattern by strengthening connections between co-active
nodes. This has to be counter-balanced by the anti-Hebbian term, that subtracts the variance that is already
explained out by the network’s predictions.
To illustrate how this dynamic gives rise to efficient, (approximately) orthogonal representations of the
external states, suppose the network has already learned a patterns(1), whose neural representation is the
attractor σ(1) and associated weightsJ(1). When a new patterns(2) is presented that is correlated withs(1),
the network’s prediction forσ(2)
i will be ˆσi = L(bi + ∑
k̸=iJ(1)
ik σk). Because inference withJ(1) converges to
σ(1) and σ(2) is correlated withσ(1), the predictionˆσ will capture variance inσ(2) that is ‘explained’ byσ(1).
The learning rule updates the weights based only on the unexplained (residual) component of the variance, the
prediction error. In other words,ˆσ approximates the projection ofσ(2) onto the subspace already spanned by
σ(1). Therefore, the weight update primarily strengthens weights corresponding to the component ofσ(2) that
is orthogonal toσ(1). Thus, the learning effectively encodes this residual,σ(2)
⊥ , ensuring that the new attractor
components being formed tend to be orthogonal to those already established. As we demonstrate in the
next section with Simulation 1-2, repeated application of this rule during learning progressively decorrelates
9
the neural activities associated with different patterns, leading toapproximately orthogonal attractor
states. This process is analogous to online orthogonalization procedures (e.g., Sanger’s rule for PCA [Sanger,
1989]) and results in a powerful stochastic approximation of the “projection network” of Personnaz et al.
[1985], Kanter and Sompolinsky [1987], which offers maximal memory capacity and retrieval without errors.
While orthogonality enhances representational efficiency, it raises the question of how the network retrieves the
original patterns from these (approximately) orthogonalized representations - a key requirement to function as
associative memory. As discussed next, stochastic dynamics enable the network to address this by sampling
from a posterior that combines these orthogonal bases.
3.7 Stochastic retrieval as macro-scale Bayesian inference
As a consequence of the FEP, the inference process described above - where each subparticleσi updates
its state based on local information (its biasbi and input∑
jJijσj) - can be seen as a form of micro-scale
inference, in which the prior - defined by the node’s internal bias, gets updated by the evidence collected
from the neighboring subparticles to form the posterior. However, as the whole network itself is also a
particular partition (specifically, a deep one), it must also perform Bayesian inference, at the macro-scale.
While the above argumentation provides a simple, self-contained proof, the nature of macro-scale inference
can be elucidated by using the equivalence of the derived attractor network - in the special case of symmetric
couplings - to Boltzmann machines (without hidden units). Namely, the ability of Boltzmann machines to
perform macro-scale approximate Bayesian inference through Markov Chain Monte Carlo (MCMC) sampling
has been well established in the literature [Ackley et al., 1985, Hinton, 2002].
Let us consider the network’s learned weightsJ (and potentially its baseline biasesbbase) as defining aprior
distribution over the collective statesσ:
p(σ) ∝exp
{∑
i
bbase
i σi + 1
2
∑
ij
Jijσiσj
}
(26)
Now, suppose the network receives external input (evidence)s, which manifests as persistent modulationsδbi
to the biases, such that the total bias isbi = bbase
i + δbi. This evidence can be associated with alikelihood
function:
p(s|σ) ∝exp
(∑
i
δbiσi
)
(27)
According to Bayes’ theorem, theposterior distributionover the network states given the evidence is
p(σ|s) ∝p(s|σ)p(σ):
p(σ|s) ∝exp
{∑
i
biσi + 1
2
∑
ij
Jijσiσj
}
(28)
As expected, this posterior distribution has the same functional form as the network’s equilibrium distribution
under the influence of the total biasesbi. Thus, the stochastic update rule derived from minimizing local VFE
(eq. (14)) effectively performs Markov Chain Monte Carlo (MCMC) sampling - specifically Gibbs sampling
- from the joint posterior distribution defined by the current VFE landscape. In the presence of evidence
s, the network dynamics thereforesample from the posterior distributionp(σ|s). The significance of the
stochasticity becomes apparent when considering the network’s behavior over time. The time-averaged state
⟨σ⟩converges to the expected value under the posterior distribution:
⟨σ⟩= lim
T→∞
1
T
∫T
0
σ(t)dt≈Ep(σ|s)[σ] (29)
Noise or stochasticity allows the system to explore the posterior landscape, escaping local minima inherited
from the prior if they conflict with the evidence, and potentially mixing between multiple attractors that are
compatible with the evidence. The resulting average activity⟨σ⟩thus represents a Bayesian integration of the
prior knowledge encoded in the weights and the current evidence encoded in the biases. This contrasts sharply
with deterministic dynamics, which would merely settle into a single (potentially suboptimal) attractor within
the posterior landscape.
Furthermore, if the learning process shapes the priorp(σ) to have less redundant (e.g., more orthogonal)
attractors, this parsimonious structure of the prior naturally contributes to a less redundant posterior
10
distribution p(σ|s). When the prior belief structure is efficient and its modes are distinct, the posterior
modes formed by integrating evidences are also less likely to be ambiguous or highly overlapping. This leads
to more robust and interpretable inference, as the network can more clearly distinguish between different
explanations for the sensory data.
With this, the loop is closed. The stochastic attractor network emerging from the FEP framework (summarized
on Figure 3A) naturally implements macro-scale Bayesian inference through its collective sampling dynamics,
providing a robust mechanism for integrating prior beliefs with incoming sensory evidence. This reveals the
potential for a deep recursive application of the Free Energy Principle: the emergent collective behavior of
the entire network, formed by interacting subparticles each minimizing their local free energy, recapitulates
the inferential dynamics of a single, macro-scale particle. This recursion could extend to arbitrary depths,
giving rise to a hierarchy of nested particular partitions and multiple emergent levels of description, each
level performing Bayesian active inference according to the same fundamental principles.
Figure 3: Free energy minimizing, adaptively self-organizing attractor network
A Schematic of the network illustrating inference and learning processes. Inference and learning are two
faces of the same process: minimizing local variational free energy (VFE), leading to dissipative dynamics
and approximately orthogonal attractors. B A demonstrative simulated example (Simulation 1) of the
network’s attractors forming an orthogonal basis of the input data. Training can be performed by introducing
the training data (top left) through the biases of the network. In this example, the input data consists
of two correlated patterns (Pearson’s r = 0.77). During repeated updates, micro-scale (local, node-level)
VFE minimization implements a simultaneous learning and inference process, which leads to approximate
macro-scale (network-level) free energy minimization (bottom graph). The resulting network does not simply
store the input data as attractors, but it stores approximately orthogonalized varieties of it (top right,
Pearson’s r = -0.19). C When the trained network is introduced a noisy version of one of the training
patterns (left), it is internally handled as the Likelihood function, and the network performs an Markov-Chain
Monte-Carlo (MCMC) sampling of the posterior distribution, given the priors defined by the network’s
attractors (top right), which can be understood as a retrieval process.D Thanks to its orthogonal attractor
representation, the network is able to generalize to new patterns - as long as they are sampled from the
sub-space spanned by the attractors - by combining the quasi-orthogonal attractor states (bottom right) by
multistable stochastic dynamics during the MCMC sampling.
11
4 In silico demonstrations
We illustrate key features of the proposed framework with computer simulations. All simulations are based
on a simple python implementation of the network, available at https://github.com/tspisak/fep-attractor-
networks. The implementation favors clarity over efficiency - it implements bothσ and boundary states as
separate classes, and is not optimized for performance. In all simulations, we train an attractor network with
the derived rules in a continuous-learning fashion (i.e simultaneously performing inference and learning). To
be able to control the precision during inference and the speed of learning, we introduce two coefficients for
eq.-s (20) and (25), the inverse temperature parameteriT and a learning-rateα.
4.1 Simulation 1: demonstration of orthogonal basis formation, macro-scale free energy
minimization and Bayesian inference
In Simulation 1, we construct a network with 25 subparticles (representing 5x5 images) and train it with 2
different, but correlated images (Pearson’s r = 0.77, see Figure 3B), with a precision of 0.1 and a learning
rate of 0.01. The training phase consisted of 500 epochs, each showing a randomly selected pattern from the
training set through 10 time steps of simultaneous inference and learning. As shown on Figure 3B, local,
micro-scale VFE minimization performed by the simultaneous inference and learning process leads to a
macro-scale free energy minimization. Next, we obtained the attractor states corresponding to the input
patterns by means of deterministic inference (updating with the expected value, instead of sampling from
the CBdistribution, akin to a vanilla Hopfield network). As predicted by theory, the attractor states were
not simple replicas of the input patterns, but approximately orthogonalized versions of them, displaying a
correlation coefficient of r=-0.19. Next, we demonstrated that the network (with stochastic inference) is not
only able to retrieve the input patterns from noisy variations of them (fig. Figure 3C), but also generalizes
well to reconstruct a third pattern, by combining its quasi-orthogonal attractor states (fig. Figure 3D). Note
that this simulation only aimed to demonstrate some of the key features of the proposed architecture, and a
comprehensive evaluation of the network’s performance, and its dependency on the parameters is presented
in the next simulation.
4.2 Simulation 2: systematic evaluation of learning regimes
In Simulation 2, we trained the network on 10 images of handwritten digits (a single example of each of
the 10 digits from 0 to 9, 8x8 pixels each, as distributed with scikit-learn, see Figure 4C, upper row). The
remaining 1787 images were unseen in the training phase and only used as a test set, in a one-shot learning
fashion. The network was trained with a fixed learning rate of 0.01, through 5000 epochs, each consisting of
10 time steps with the same, randomly selected pattern from the training set of 10 images, while performing
simultaneous inference and learning. We evaluated the effect of the inverse temperature parameteriT (i.e.
precision) and the strength of evidence during training, i.e. the magnitude of the bias changesδbi. The
precision parameteriT was varied with 19 values between 0.01 and 1, and the strength of evidence during
training varied by changing the magnitude of the biases from 1 to 20, with increments of 1. The training
patterns were first preprocessed by squaring the pixel values (to enhance contrast) and normalizing each
image to have zero mean and unit variance. We performed a total of 380 runs, varying these parameters in a
grid-search fashion. All cases were evaluated in terms of (i) stochastic (Bayesian) pattern retrieval from noisy
variations of the training images; and (ii) one-shot generalization to reconstruct unseen handwritten digit
examples. In both types of evaluation, the network was presented a noisy variant of a randomly selected
(training or test) image through its biases. The noisy patterns were generated by adding Gaussian noise with
a standard deviation of 1 to the pixel values of the training images (see “Examples” in Figure 4B C and D).
The network’s response was obtained by averaging 100 time steps of stochastic inference. The performance
was quantified as the improvement in the proportion of variance in the original target pattern (without noise)
explained by the network’s response, compared to that explained by the noisy input pattern. Both for retrieval
and generalization, this approach was repeated 100 times, with a randomly sampled image from the training
(10 images) and test set (1787 images), respectively. The median improvement across these 100 repetition
was used as the primary performance metric. The retreival and 1-shot generalization performance of models
trained with differentiT and α parameters is shown on Figure 4A, top row). We found that, while retrieval
of a noisy training pattern was best with precision values between 0.1 and 0.5, generalization to new data
peferred lower precision during learning (iT<0.1, i.e. more stochatsic dynamics). Furtermore, in all simulation
cases, we seeded the networks with the original test patterns and obtained the corresponding attractor states,
by means of deterministic inference. We then computed the pairwise correlation and dot product between
the attractor states. The dot product was converted to degrees. Orthogonality was finally quantified by the
12
Figure 4: Adaptive self-organization and generalization in a free-energy minimizing attractor
network.
Simulation results from training the network on a single, handwritten example for each of the 10 digits (0-9),
with variations in training precision and evidence strength to explore different learning regimes (Simulation 2).
A: Performance landscapes as a function of inference temperature (inverse precision) and training evidence
strength (bias magnitude). Retrieval performance (reconstructing noisy variants of the 10 training patterns,
top left), one-shot generalization (reconstructing a noisy variants of unseen handwritten digits, top right),
attractor orthogonality (mean squared angular difference from 90° indicating higher orthogonality for lower
values, bottom left), and the number of attractors (when initialized with the 10 training patterns, bottom
right) are shown. Optimal regions (contoured) highlight parameter settings that yield good generalization
and highly orthogonal attractors. Contours in the top left and top right highlight the most efficient parameter
settings for retrieval and generalization, respectively. Both contours are overlaid on the two bottom plots.B:
Conceptual illustration of training regimes. With low temperature (high precision) high model complexity is
allowed (“accuracy pumping”) and attractors will tend to exactly match the training data. On the contrary,
high temperatures (low precision) result in a single fixed point attractor and reduced recognition performance.
However, such networks will be able to generalize to new data, suggesting the existance of “soft attractors”
(e.g. saddle-like structures) that are not local minima on the free energy landscape, yet affect the steady-state
posterior distribution in a non-negligible way (especially with longer mixing-times).
13
Figure 4: (continued) A balanced regime can be found with intermediate precision during training, where
both recognition and generalization performance are high. This is exactly the regime that promotes attractor
orthogonalization, crucial for efficient representation and generalization. The complexity restrictions on
these models cause them to re-use the same attractors to represent different patterns (see e.g. the single
attractor belonging to the digits 5 and 7 in the example on panel D), which eventually leads to approximate
orthogonality. Panels C-E provide examples of network behavior on a handwritten digit task across different
regimes, including (i) training data (same in all cases); (ii) fixed-point attractors (obtained with deterministic
update); (iii) attractor-orthogonality (polar histogram of the pairwise angles between attractors); (iv) retrieval
and 1-shot generalization performance (R2 between the noisy input pattern and the network output after
100 time steps, for 100 randomly sampled patterns) and (v) illustrative example cases from the recognition
and 1-shot generalization tests (noisy input, network output and true pattern). C: High complexity:
Attractors are sharp and similar to training data; good recognition, limited generalization.D: Balanced
complexity (orthogonalization): Attractors are distinct and quasi-orthogonal, enabling strong recognition
and generalization from noisy inputs. The balanced regime clearly demonstrates the network’s ability to
form an orthogonal basis, facilitating effective generalization as predicted by the free-energy minimization
framework. E: Low complexity: There is only a single fixed-point attractor. Recognition performance is
lower, but generalization remains considerable.
mean correlation among attractors and and the mean squared deviation from orthogonality (in degrees). To
establish a reference value, the same procedure was also repeated for the original patterns (after preprocessing),
which displayed a mean correlation a 29.94 degree mean squared deviation from orthogonality. Attractor
orthogonality and the number of attractors for each simulation case is shown on Figure 4A, bottom row.
We found that depending on the temperature of the network during the learning phase, the network can
be in characteristic regimes of high, low and balanced complexity (Figure 4B). With low temperature (high
precision), high model complexity is allowed (“accuracy pumping”) and attractors will tend to exactly match
the training data Figure 4C. On the contrary, high temperatures (low precision) result in a single fixed point
attractor and reduced recognition performance Figure 4E. However, such networks were found to be able to
generalize to new data, suggesting the existence of “soft attractors” (e.g. saddle-like structures) that are not
local minima on the free energy landscape, yet affect the steady-state posterior distribution in a non-negligible
way (especially with longer mixing-times). A balanced regime Figure 4D can be found with intermediate
training precison, where both recognition and 1-shot generalization performance are high (similarly to the
“standard regime” ofprospective configuration[Millidge et al., 2022a]). This is exactly the regime that
promotes attractor orthogonalization, crucial for efficient representation and generalization. The complexity
restrictions on these models cause them to re-use the same attractors to represent different patterns (see e.g.
the single attractor belonging to the digits 5 and 7 in the example on panel D), which eventually leads to less
attractors, with each having more explanatory power, and being approximately orthogonal to each other.
Panels C-E on Figure 4 provide examples of network behavior on a handwritten digit task across different
regimes, including (i) training data (same in all cases); (ii) fixed-point attractors (obtained with deterministic
update); (iii) attractor-orthogonality (polar histogram of the pairwise angles beteen attractors); (iv) retrieval
and 1-shot generalization performance (R2 between the noisy input pattern and the network output after 100
time steps, for 100 randomly sampled patterns) and (v) illustrative example cases from the recognition and
1-shot generalization tests (noisy input, network output and true pattern).
4.3 Simulation 3: demonstration of sequence learning capabilities
In Simulation 3, we demonstrate the sequence learning capabilities of the proposed architecture. We
trained the network on a sequence of 3 handwritten digits (1,2,3), with a fixed order of presentation
(1 →2 →3 →1 →2 →3 →...), for 2000 epochs, each epoch consisting of a single step (Figure 5A). This
rapid presentation of the input sequence forced the network to model the current attractor from the network’s
response to the previous pattern, i.e. to establish sequence attractors. The inverse temperature was set to 1
and the learning rate to 0.001 (in a supplementary analysis, we saw a considerable robustness of our results
to the choice of these parameters). As shown on Figure 5B, this training approach led to an asymmetric
coupling matrix (it was very close to symmetric in all previous simulations). Based on eq.-s (12) and (13), we
decomposed the coupling matrix into a symmetric and antisymmetric part (Figure 5C and D). Retrieving the
fixed-point attractors for the symmetric component of the coupling matrix, we obtained three attractors,
corresponding to the three digits (Figure 5C and E). The antisymmetric component of the coupling matrix, on
the other hand, was encoding the sequence dynamics. Indeed, letting the network freely run (with zero bias)
resulted in a spontanously emerging sequance of variations of the digits1 →2 →3 →1 →2 →3 →1 →...,
14
reflecting the original training order (Figure 5F). This illustrates that the proposed framework is capable of
producing and handling assymetric couplings, and thereby learn sequences.
Figure 5: Sequential Dynamics in Free-Energy Minimizing Attractor Networks.
Simulation results (Simulation 3) demonstrate the framework’s ability to learn temporal sequences. Ordered
training data leads to asymmetric coupling matrices, where the symmetric component establishes fixed-point
attractors for individual patterns, and the antisymmetric component encodes the transitional dynamics,
enabling spontaneous sequence recall.A: Ordered training data (digits 1, 2, 3) presented sequentially to the
network. B: The emergent coupling matrix, displaying asymmetry as a consequence of sequential training.C:
The symmetric component of the coupling matrix. This part is responsible for creating stable, fixed-point
attractors for each pattern in the sequence.D: The antisymmetric component of the coupling matrix. This
part drives the directional transitions between the attractors, encoding the learned order.E: The sequence
attractors themselves – fixed-point attractors (digits 1, 2, 3) derived from the symmetric coupling component.
F: Example of spontaneous network activity with zero external bias, showcasing the autonomous recall of the
learned sequence(1 →2 →3 →...) driven by the interplay of symmetric and antisymmetric couplings.
4.4 Simulation 4: demonstration of resistance to catastrophic forgetting
In Simulation 4, we took a network trained in Simulation 2 with inverse temperature 0.17 and evidence level
11 (the same network that is illustrated on Figure 4D) and let it run for 50000 epochs (the same number of
epochs as the training phase), with unchanged learning rate, but zero bias. We expected that, as the network
spontanously traverses around its attractors, it reinforces them and prevents them from being fully “forgotten”.
Indeed, as shown on Figure 6, the network’s coupling matrix (panel A), retrieval performance (panel B) and
one-shot generalization performance (panel C) were all very similar to the original network’s performance.
However, the network’s attractor states were not exactly the same as the original ones, indicating that some
of the original attractors become “soft attractors” (or “ghost attractors”), which do not represent explicit
local minima on the free energy landscape anymore, but their influence on the network’s dynamics is still
significant (see 06-simulation-digits-catastrophic-forgetting.ipynb).
5 Discussion
The crucial role of attractor dynamics in elucidating brain function mandates a foundational question: what
types of attractor networks arise naturally from the first principles of self-organization, as articulated by
the Free Energy Principle [Friston et al., 2023, Friston, 2010]? Here we aimed to address this question. We
demonstrated mathematically, by recourse to a prototypical parametrization, that the ensuing networks
generally manifest as non-equilibrium steady-state (NESS) systems [Xing, 2010, Ao, 2004] that have a
stationary state probability distribution governed by the symmetric component of their synaptic efficacies,
conforming to a Boltzmann-like form [Amit, 1989, Hochreiter and Schmidhuber, 1997]. This renders the
resulting self-organizing attractor networks a generalization of canonical, single-layer Boltzmann machines or
stochastic Hopfield networks [Hinton, 2002, Hopfield, 1982], but distinguished by their capacity for asymmetric
coupling and continuous-valued neuronal states. The main assumptions underpinning this derivation - namely,
the existence of a (deep) particular partition and the ensuing imperative to minimize variational free energy -
15
Figure 6: Demonstration of resistance to catastrophic forgetting via spontaneous activity.
Simulation results (06-simulation-digits-catastrophic-forgetting.ipynb) illustrate the network’s ability to
mitigate catastrophic forgetting. When allowing the network to “free-run” (e.g. with zero external bias)
while performing continous weight adjustment, spontaneous activity reinforces existing attractors, largely
preserving learned knowledge even in the absence of the repeated presentation of previous training patterns.
A: Coupling matrices. The top panel displays the coupling matrix immediately after training on digit
patterns (50000 steps with bias corresponding to digits, the same simulation case as on D). The bottom
panel shows the coupling matrix after an additional 50000 steps of free-running (zero bias, but active weight
adjustment), indicating that the learned structure is largely maintained.B: Recognition performance. This
panel compares theR2 values for reconstructing noisy versions of the trained digit patterns. It shows the
similarity between the noisy input and the true pattern (left boxplots in each sub-panel) versus the similarity
between the network’s output and the true pattern (right boxplots in each sub-panel). Performance is robustly
maintained after the free-running phase (bottom) compared to immediately after training (top).C: One-shot
generalization performance. This panel shows theR2 values for reconstructing noisy versions ofunseen
handwritten digit patterns (after seeing only a single example per digit). Similar to recognition, the network’s
ability to generalize to novel inputs is well-preserved after the free-running phase (bottom) compared to
immediately after training (top).
are not merely parsimonious, but arguably fundamental for any random dynamical system that maintains its
integrity despite a changing environment [Friston et al., 2023].
Our formulation reveals that the minimization of variational free energy at the micro-scale, i.e., by individual
network nodes (“subparticles”), gives rise to a dual dynamic within the active inference framework [Friston,
2009]. Firstly, it prescribes Bayesian update dynamics for the individual network nodes, homologous to the
stochastic relaxation observed in Boltzmann architectures (e.g. stochastic Hopfield networks), with high
neuroscientific relevance, especially for coarse-grained brain networks, where activity across brain regions have
been reported to “flow” following a similar rule [Cole et al., 2016, Sanchez-Romero et al., 2023, Cole, 2024].
Secondly, it engenders a distinctive coupling plasticity - a local, incremental learning rule - that continuously
adjusts coupling weights to preserve low free energy in anticipation of future sensory encounters, effectively
implementing action selection in the active inference sense [Friston et al., 2016]. The learning rule itself
displays a high neurobiological plausibility, as it resembles both generalized Hebbian-anti-Hebbian learning
[Földiák, 1990, Sanger, 1989] and predictive coding [Rao and Ballard, 1999, Millidge et al., 2022b, 2024].
By contrasting the current correlations of inputs with that predicted by the network’s generative model in
the previous time step, the rule efficiently implements sequence learning. Furthermore, by capitalizing on
its natural tendency to spontaneously (stochastically) revisit its attractors, the network is able to mitigate
catastrophic forgetting[Aleixo et al., 2023] - the tendency of current deep learning architectures to lose
old representations when learning new ones. This phenomenon may serve as a model for the spontaneous
fluctuations observed during resting state brain activity (also related to "day-dreaming") and shows promise
for leveraging similar mechanisms in artificial systems.
Importantly, we show that in our adaptive self-organizing network architecture, micro-scale free energy
minimization manifests in (approximate) macro-scale free energy minimization - as expected from the fact
that the macro-scale network itself is also a particular partition. This entails that the network performs
Bayesian inference not only locally in its nodes, but also on the macro-scale - a result well known from
16
the literature on Boltzmann machines and also spiking neural networks [Ackley et al., 1985, Hinton, 2002,
Buesing et al., 2011]. Our work extends these previous results with a holistic view: the free energy landscape,
sculpted by the coupling efficacies and manifested as the repertoire of (soft-) attractors, encodes the system’s
prior beliefs. Sensory inputs or internal computations, presented as perturbations to the internal biases
of network nodes, constitute thelikelihood, while the network’s inherent stochastic dynamics explore the
posterior landscape via a process akin to Markov Chain Monte Carlo sampling [Gelman and Rubin, 1992].
The stochastic nature of these dynamics is not a mere epiphenomenon; it empowers the network to engage
in active inference by dynamically traversing trajectories in its state space that blend the contributions
of disparate attractor basins [Friston, 2009]. Consequently, for inputs that, while novel, reside within the
subspace spanned by the learned attractors, the network generalizes by engaging in oscillatory activity
- a characteristic signature of neural computations in the brain. While the notion of oscillations being
generated by multistable stochastic dynamics has been previously proposed by e.g. [Liu et al., 2022], the
quasi-orthogonal basis spanned by the attractors of the system could render such a mechanism especially
effective.
The FEP formalisation via deep particular partitions intrinsically accommodates multiple, hierarchically
nested levels of description. One may arbitrarily coarse-grain the system by combining subparticles, ultimately
arriving at a simple particular partition. Drawing upon concepts such as the center manifold theorem [Wagner,
1989], it is posited that rapid, fine-grained dynamics at lower descriptive levels converge to lower-dimensional
manifolds, upon which the system evolves via slower processes at coarser scales. This inherent separation
of temporal scales offers a compelling paradigm for understanding large-scale brain dynamics, where fast
neuronal activity underwrites slower cognitive processes through hierarchical active inference [Man et al.,
2018]. Indeed, empirical investigations have provided manifold evidence for attractor dynamics in large-scale
brain activity [Rolls, 2009, Kelso, 2012, Haken, 1978, Breakspear, 2017, Deco and Jirsa, 2012, Kelso, 2012,
Gosti et al., 2024, Chen et al., 2025].
As motivated mathematically and demonstrated in silico, a pivotal outcome of the FEP-driven simultaneous
learning and inference process is the propensity of the network to establishapproximately orthogonal attractor
states. We argue that this remarkable property is not simply a side effect; it is an unavoidable result of
reducing free energy of conservative particles [Friston et al., 2023]; which entails minimizing model complexity
and maximizing accuracy simultaneously or - eqivalently - minimize redundancy via a maximization of mutual
information. This key characteristic makes free energy minimizing attractor networks naturally approximate
one of the most efficient attractor network architectures, theprojection networksarticulated by Kanter
and Sompolinsky [Kanter and Sompolinsky, 1987, Personnaz et al., 1985]. We propose the approximate
orthogonality of attractor states as a signature of free energy minimizing attractor networks, potentially
detectable in natural systems (e.g. neural data). There is initial evidence from large-scale brain network data
pointing into this direction. Using an attractor network model very similar to the herein presented - a recent
paper has demonstrated that standard resting-state networks (RSNs) are manifestations of brain attractors
that can be reconstructed from fMRI brain connectivity data [Englert et al., 2023]. Most importantly, these
empirically reconstructed large-scale brain attractors were found to be largely orthogonal - the key feature
of the self-organizing attractor networks described here. Future research needs to carefully check if these
large-scale brain attractors, and the computing abilities that come with them, are truly a direct signature of
an underlying FEP-based attractor network.
Besides neuroscientific relevance, our work has also manifold implications for artificial intelligence research.
In general, there is growing interest in predictive coding-based neural network architectures - akin to the
architecture presented herein. Recent studies have demonstrated that such approaches can not only reproduce
backpropagation as an edge case [Millidge et al., 2022c], but scale efficiently - even for cyclic graphs - and
can outperform traditional backpropagation approaches in several scenarios [Salvatori et al., 2023, 2021].
Furthermore, in our framework - in line with recent formulations of predictive coding for deep learning
[Millidge et al., 2022b] - learning and inference are not disparate processes but rather two complementary
facets of variational free energy minimization through active inference. As we demonstrated with simulations,
this unification naturally endows the proposed architecture with characteristics of continual or lifelong
learning - the ability for machines to gather data and fine-tune its internal representation continuously during
functioning [Wang et al., 2023]. As inference in the proposed network involves spontaneously “replaying”
its own attractors (or sequences thereof), even if no external input is introduced (zero bias), the proposed
architecture may naturally overcome catastrophic forgetting [Aleixo et al., 2023]. To examine how these
properties of FEP-based attractor networks scale to more complex, diverse, and extended learning scenarios is
a promising direction for further studies. Stochasticity is another key property of our network, implementing
the precision of inference and allowing it to strike a balance between stability and flexibility. This inherent
17
stochasticity yields an exceptional fit with energy-efficient neuromorphic architectures [Schuman et al., 2022],
particularly within the emerging field of thermodynamic computing [Melanson et al., 2025]. Finally, the
recursive nature of our FEP-based formal framework provides a principled way to build hierarchical, multi-
scale attractor networks which may be exploited for boosting the efficiency, robustness and explainability of
large-scale AI systems. In general, the proposed architecture inherently embodies all the previously discussed
advantages of active inference and predictive coding [Millidge et al., 2022b, Salvatori et al., 2023]. At every
level of description the network dynamically balances accuracy and complexity and may naturally exhibit
“information-seeking” behaviors (curiosity) [Friston et al., 2017]. Furthermore, the architecture may offer a
foundation for exploring long-term philosophical implications of the qualities of active inference associated
with sentience [Pezzulo et al., 2024].
In conclusion, by deriving the emergence of adaptive, self-organizing - and self-orthogonalizing - attractor
networks from the FEP, this work offers a principled synthesis of self-organization, Bayesian inference, and
neural computation. The intrinsic tendency towards attractor orthogonalization, the multi-scale dynamics,
and the continuous learning capabilities present a compelling, theoretically grounded outlook for better
understanding natural intelligence and inspiring artificial counterparts through the lens of active inference.
6 Manuscript Information
6.1 Data availability
Simulation 1 is based on simulated data. Simulation 2-4 is based on the ’handwritten digits’ dataset available
in scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html,
and originally published as part of the thesis of C. Kaynak., 1995:
https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits.
6.1.1 Interactive manuscript
An interactive version of this manuscript (based on Myst Markdown) is available at: https://pni-
lab.github.io/fep-attractor-network/
6.1.2 Simulation source code
• Simulation 1: https://pni-lab.github.io/fep-attractor-network/simulation-demo
• Simulation 2: https://pni-lab.github.io/fep-attractor-network/simulation-digits
• Simulation 3: https://pni-lab.github.io/fep-attractor-network/simulation-digits-continuous-sequence
• Simulation 4: https://pni-lab.github.io/fep-attractor-network/simulation-digits-catastrophic-
forgetting
Acknowledgments
TS was supported by funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
— Project-ID 422744262 - TRR 289; and Project-ID 316803389 – SFB 1280 “Extinction Learning”. KF is
supported by funding from the Wellcome Trust (Ref: 226793/Z/22/Z).
References
Hermann Haken.Synergetics: An Introduction Nonequilibrium Phase Transitions and Self-Organization in
Physics, Chemistry and Biology. Springer Berlin Heidelberg, 1978. ISBN 9783642964695. doi:10.1007/978-
3-642-96469-5. URL http://dx.doi.org/10.1007/978-3-642-96469-5 .
John Guckenheimer, Philip Holmes, and M. Slemrod. Nonlinear Oscillations Dynamical Systems, and
Bifurcations of Vector Fields.Journal of Applied Mechanics, 51(4):947–947, 12 1984. ISSN 1528-9036.
doi:10.1115/1.3167759. URL http://dx.doi.org/10.1115/1.3167759.
Daniel J. Amit.Modeling Brain Function: The World of Attractor Neural Networks. Cambridge University
Press, 9 1989. ISBN 9780511623257. doi:10.1017/cbo9780511623257. URLhttp://dx.doi.org/10.1017/
CBO9780511623257.
18
W. J. Freeman. Simulation of chaotic EEG patterns with a dynamic model of the olfactory system.
Biological Cybernetics, 56(2–3):139–150, 5 1987. ISSN 1432-0770. doi:10.1007/bf00317988. URLhttp:
//dx.doi.org/10.1007/BF00317988.
Gustavo Deco and Edmund T. Rolls. Attention and working memory: a dynamical model of neuronal activity
in the prefrontal cortex.European Journal of Neuroscience, 18(8):2374–2390, 10 2003. ISSN 1460-9568.
doi:10.1046/j.1460-9568.2003.02956.x. URL http://dx.doi.org/10.1046/j.1460-9568.2003.02956.x.
Ramón Nartallo-Kaluarachchi, Morten L. Kringelbach, Gustavo Deco, Renaud Lambiotte, and Alain Goriely.
Nonequilibrium physics of brain dynamics, 2025. URLhttps://arxiv.org/abs/2504.12188.
Misha Tsodyks. Attractor neural network models of spatial maps in hippocampus.Hippocampus, 9(4):
481–489, 1999. ISSN 1098-1063. doi:10.1002/(sici)1098-1063(1999)9:4<481::aid-hipo14>3.0.co;2-s. URL
http://dx.doi.org/10.1002/(SICI)1098-1063(1999)9:4%3C481::AID-HIPO14%3E3.0.CO;2-S .
Mikail Khona and Ila R. Fiete. Attractor and integrator networks in the brain.Nature Reviews Neuroscience,
23(12):744–766, 11 2022. ISSN 1471-0048. doi:10.1038/s41583-022-00642-0. URLhttp://dx.doi.org/10.
1038/s41583-022-00642-0 .
K Zhang. Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble:
a theory.The Journal of Neuroscience, 16(6):2112–2126, 3 1996. ISSN 1529-2401. doi:10.1523/jneurosci.16-
06-02112.1996. URL http://dx.doi.org/10.1523/JNEUROSCI.16-06-02112.1996.
Edmund T. Rolls. Attractor networks.WIREs Cognitive Science, 1(1):119–134, 12 2009. ISSN 1939-5086.
doi:10.1002/wcs.1. URLhttp://dx.doi.org/10.1002/wcs.1.
J. A. Scott Kelso. Multistability and metastability: understanding dynamic coordination in the brain.
Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1591):906–918, 4 2012. ISSN
1471-2970. doi:10.1098/rstb.2011.0351. URLhttp://dx.doi.org/10.1098/rstb.2011.0351.
Karl Friston, Lancelot Da Costa, Dalton A.R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis, Maxwell
Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things.Physics of Life Reviews,
47:35–62, 12 2023. ISSN 1571-0645. doi:10.1016/j.plrev.2023.08.016. URLhttp://dx.doi.org/10.1016/
j.plrev.2023.08.016.
Karl Friston and Ping Ao. Free Energy, Value, and Attractors.Computational and Mathematical Methods
in Medicine, 2012:1–27, 2012. ISSN 1748-6718. doi:10.1155/2012/937860. URLhttp://dx.doi.org/10.
1155/2012/937860.
Ensor Rafael Palacios, Adeel Razi, Thomas Parr, Michael Kirchhoff, and Karl Friston. On Markov blankets
and hierarchical self-organisation.Journal of Theoretical Biology, 486:110089, 2 2020. ISSN 0022-5193.
doi:10.1016/j.jtbi.2019.110089. URL http://dx.doi.org/10.1016/j.jtbi.2019.110089.
Karl Friston, Lancelot Da Costa, Dalton A. R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis,
Maxwell Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things. 2022.
doi:10.48550/ARXIV.2210.12761. URL https://arxiv.org/abs/2210.12761.
Karl Friston. The free-energy principle: a rough guide to the brain?Trends in Cognitive Sciences, 13(7):
293–301, 7 2009. ISSN 1364-6613. doi:10.1016/j.tics.2009.04.005. URLhttp://dx.doi.org/10.1016/j.
tics.2009.04.005.
Karl Friston. The free-energy principle: a unified brain theory?Nature Reviews Neuroscience, 11(2):127–138,
1 2010. ISSN 1471-0048. doi:10.1038/nrn2787. URLhttp://dx.doi.org/10.1038/nrn2787.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John ODoherty, and Giovanni
Pezzulo. Active inference and learning.Neuroscience & Biobehavioral Reviews, 68:862–879, 9 2016. ISSN
0149-7634. doi:10.1016/j.neubiorev.2016.06.022. URLhttp://dx.doi.org/10.1016/j.neubiorev.2016.
06.022.
Karl Friston. A free energy principle for a particular physics, 2019. URLhttps://arxiv.org/abs/1906.
10184.
Andy Clark. How to Knit Your Own Markov Blanket:. Philosophy and Predictive Processing, 2017.
doi:10.15502/9783958573031. URL http://www.predictive-mind.net/DOI?isbn=9783958573031.
Inês Hipólito, Maxwell JD Ramstead, Laura Convertino, Anjali Bhat, Karl Friston, and Thomas Parr. Markov
blankets in the brain.Neuroscience & Biobehavioral Reviews, 125:88–97, 2021.
P Ao. Potential in stochastic differential equations: novel construction.Journal of Physics A: Mathematical
and General, 37(3):L25–L30, 1 2004. ISSN 1361-6447. doi:10.1088/0305-4470/37/3/l01. URL http:
//dx.doi.org/10.1088/0305-4470/37/3/L01.
19
Jianhua Xing. Mapping between dissipative and Hamiltonian systems.Journal of Physics A: Mathematical
and Theoretical, 43(37):375003, 7 2010. ISSN 1751-8121. doi:10.1088/1751-8113/43/37/375003. URL
http://dx.doi.org/10.1088/1751-8113/43/37/375003.
Yuhang Song, Beren Millidge, Tommaso Salvatori, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz.
Inferring neural activity before plasticity as a foundation for learning beyond backpropagation.Nature
Neuroscience, 27(2):348–358, 1 2024. ISSN 1546-1726. doi:10.1038/s41593-023-01514-1. URL http:
//dx.doi.org/10.1038/s41593-023-01514-1 .
Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, and Rafal Bogacz. A Theoretical
Framework for Inference and Learning in Predictive Coding Networks, 2022a. URLhttps://arxiv.org/
abs/2207.12316.
Geoffrey E. Hinton. Training Products of Experts by Minimizing Contrastive Divergence.Neural Computation,
14(8):1771–1800, 8 2002. ISSN 1530-888X. doi:10.1162/089976602760128018. URLhttp://dx.doi.org/
10.1162/089976602760128018.
Beren Millidge, Mufeng Tang, Mahyar Osanlouy, Nicol S. Harper, and Rafal Bogacz. Predictive coding
networks for temporal prediction.PLOS Computational Biology, 20(4):e1011183, 4 2024. ISSN 1553-7358.
doi:10.1371/journal.pcbi.1011183. URL http://dx.doi.org/10.1371/journal.pcbi.1011183.
Terence D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network.
Neural Networks, 2(6):459–473, 1 1989. ISSN 0893-6080. doi:10.1016/0893-6080(89)90044-0. URLhttp:
//dx.doi.org/10.1016/0893-6080(89)90044-0.
L. Personnaz, I. Guyon, and G. Dreyfus. Information storage and retrieval in spin-glass like neural networks.
Journal de Physique Lettres, 46(8):359–365, 1985. ISSN 0302-072X. doi:10.1051/jphyslet:01985004608035900.
URL http://dx.doi.org/10.1051/jphyslet:01985004608035900.
I. Kanter and H. Sompolinsky. Associative recall of memory without errors.Physical Review A, 35(1):380–392,
1 1987. ISSN 0556-2791. doi:10.1103/physreva.35.380. URLhttp://dx.doi.org/10.1103/physreva.35.
380.
D Ackley, G Hinton, and T Sejnoski. A learning algorithm for boltzmann machines.Cognitive Science, 9
(1):147–169, 3 1985. ISSN 0364-0213. doi:10.1016/s0364-0213(85)80012-4. URLhttp://dx.doi.org/10.
1016/S0364-0213(85)80012-4.
Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory.Neural Computation, 9(8):1735–1780,
11 1997. ISSN 1530-888X. doi:10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.
1997.9.8.1735.
J J Hopfield. Neural networks and physical systems with emergent collective computational abili-
ties. Proceedings of the National Academy of Sciences, 79(8):2554–2558, 4 1982. ISSN 1091-6490.
doi:10.1073/pnas.79.8.2554. URL http://dx.doi.org/10.1073/pnas.79.8.2554.
Michael W Cole, Takuya Ito, Danielle S Bassett, and Douglas H Schultz. Activity flow over resting-state
networks shapes cognitive task activations. Nature Neuroscience, 19(12):1718–1726, 10 2016. ISSN
1546-1726. doi:10.1038/nn.4406. URLhttp://dx.doi.org/10.1038/nn.4406.
Ruben Sanchez-Romero, Takuya Ito, Ravi D. Mill, Stephen José Hanson, and Michael W. Cole. Causally
informed activity flow models provide mechanistic insight into network-generated cognitive activations.
NeuroImage, 278:120300, 9 2023. ISSN 1053-8119. doi:10.1016/j.neuroimage.2023.120300. URLhttp:
//dx.doi.org/10.1016/j.neuroimage.2023.120300.
Michael W. Cole. The explanatory power of activity flow models of brain function, 2024. URLhttps:
//arxiv.org/abs/2402.02191.
P. Földiák. Forming sparse representations by local anti-Hebbian learning.Biological Cybernetics, 64(2):165–
170, 12 1990. ISSN 1432-0770. doi:10.1007/bf02331346. URLhttp://dx.doi.org/10.1007/BF02331346.
Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: a functional interpretation
of some extra-classical receptive-field effects.Nature Neuroscience, 2(1):79–87, 1 1999. ISSN 1546-1726.
doi:10.1038/4580. URL http://dx.doi.org/10.1038/4580.
Beren Millidge, Tommaso Salvatori, Yuhang Song, Rafal Bogacz, and Thomas Lukasiewicz. Predictive Coding:
Towards a Future of Deep Learning beyond Backpropagation?, 2022b. URLhttps://arxiv.org/abs/
2202.09467.
Everton L. Aleixo, Juan G. Colonna, Marco Cristo, and Everlandio Fernandes. Catastrophic Forgetting in
Deep Learning: A Comprehensive Taxonomy, 2023. URLhttps://arxiv.org/abs/2312.10549.
20
Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural Dynamics as Sampling: A Model
for Stochastic Computation in Recurrent Networks of Spiking Neurons.PLoS Computational Biology, 7
(11):e1002211, 11 2011. ISSN 1553-7358. doi:10.1371/journal.pcbi.1002211. URLhttp://dx.doi.org/10.
1371/journal.pcbi.1002211.
Andrew Gelman and Donald B. Rubin. Inference from Iterative Simulation Using Multiple Sequences.
Statistical Science, 7(4), 11 1992. ISSN 0883-4237. doi:10.1214/ss/1177011136. URLhttp://dx.doi.org/
10.1214/ss/1177011136.
Zilu Liu, Fang Han, and Qingyun Wang. A review of computational models for gamma oscillation dynamics:
from spiking neurons to neural masses.Nonlinear Dynamics, 108(3):1849–1866, 3 2022. ISSN 1573-269X.
doi:10.1007/s11071-022-07298-6. URL http://dx.doi.org/10.1007/s11071-022-07298-6 .
David H. Wagner. The Existence and Behavior of Viscous Structure for Plane Detonation Waves.SIAM
Journal on Mathematical Analysis, 20(5):1035–1054, 9 1989. ISSN 1095-7154. doi:10.1137/0520069. URL
http://dx.doi.org/10.1137/0520069.
Kenneth K.C. Man, Esther W. Chan, Patrick Ip, David Coghill, Emily Simonoff, Phyllis K.L. Chan,
Wallis C.Y. Lau, Martijn J. Schuemie, Miriam C.J.M. Sturkenboom, and Ian C.K. Wong. Prenatal
antidepressant exposure and the risk of attention-deficit hyperactivity disorder in children: A systematic
review and meta-analysis. Neuroscience & Biobehavioral Reviews, 86:1–11, 3 2018. ISSN 0149-7634.
doi:10.1016/j.neubiorev.2017.12.007. URL http://dx.doi.org/10.1016/j.neubiorev.2017.12.007.
Michael Breakspear. Dynamic models of large-scale brain activity.Nature Neuroscience, 20(3):340–352, 2
2017. ISSN 1546-1726. doi:10.1038/nn.4497. URLhttp://dx.doi.org/10.1038/nn.4497.
Gustavo Deco and Viktor K. Jirsa. Ongoing Cortical Activity at Rest: Criticality, Multistability,
and Ghost Attractors. The Journal of Neuroscience, 32(10):3366–3375, 3 2012. ISSN 1529-2401.
doi:10.1523/jneurosci.2523-11.2012. URL http://dx.doi.org/10.1523/JNEUROSCI.2523-11.2012.
Giorgio Gosti, Edoardo Milanetti, Viola Folli, Francesco de Pasquale, Marco Leonetti, Maurizio Corbetta, Gian-
carlo Ruocco, and Stefania Della Penna. A recurrent Hopfield network for estimating meso-scale effective con-
nectivity in MEG.Neural Networks, 170:72–93, 2 2024. ISSN 0893-6080. doi:10.1016/j.neunet.2023.11.027.
URL http://dx.doi.org/10.1016/j.neunet.2023.11.027.
Ruiqi Chen, Matthew Singh, Todd S. Braver, and ShiNung Ching. Dynamical models reveal anatomically
reliable attractor landscapes embedded in resting-state brain networks.Imaging Neuroscience, 3, 2025.
ISSN 2837-6056. doi:10.1162/imag_a_00442. URLhttp://dx.doi.org/10.1162/imag_a_00442.
Robert Englert, Balint Kincses, Raviteja Kotikalapudi, Giuseppe Gallitto, Jialin Li, Kevin Hoffschlag, Choong-
Wan Woo, Tor D. Wager, Dagmar Timmann, Ulrike Bingel, and Tamas Spisak. Connectome-Based Attractor
Dynamics Underlie Brain Activity in Rest, Task, and Disease. 11 2023. doi:10.1101/2023.11.03.565516.
URL http://dx.doi.org/10.1101/2023.11.03.565516.
Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, and Rafal Bogacz. Backpropagation
at the Infinitesimal Inference Limit of Energy-Based Models: Unifying Predictive Coding, Equilibrium
Propagation, and Contrastive Hebbian Learning, 2022c. URLhttps://arxiv.org/abs/2206.02629.
Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl
Friston, and Alexander Ororbia. A Survey on Brain-Inspired Deep Learning via Predictive Coding, 2023.
URL https://arxiv.org/abs/2308.07870.
Tommaso Salvatori, Yuhang Song, Yujian Hong, Simon Frieder, Lei Sha, Zhenghua Xu, Rafal Bogacz, and
Thomas Lukasiewicz. Associative Memories via Predictive Coding, 2021. URLhttps://arxiv.org/abs/
2109.08063.
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A Comprehensive Survey of Continual Learning:
Theory, Method and Application, 2023. URLhttps://arxiv.org/abs/2302.00487.
Catherine D. Schuman, Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay.
Opportunities for neuromorphic computing algorithms and applications.Nature Computational Science, 2
(1):10–19, 1 2022. ISSN 2662-8457. doi:10.1038/s43588-021-00184-y. URLhttp://dx.doi.org/10.1038/
s43588-021-00184-y .
Denis Melanson, Mohammad Abu Khater, Maxwell Aifer, Kaelan Donatella, Max Hunter Gordon, Thomas
Ahle, Gavin Crooks, Antonio J. Martinez, Faris Sbahi, and Patrick J. Coles. Thermodynamic computing
system for AI applications.Nature Communications, 16(1), 4 2025. ISSN 2041-1723. doi:10.1038/s41467-
025-59011-x. URL http://dx.doi.org/10.1038/s41467-025-59011-x .
21
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka.
Active Inference, Curiosity and Insight.Neural Computation, 29(10):2633–2683, 10 2017. ISSN 1530-888X.
doi:10.1162/neco_a_00999. URL http://dx.doi.org/10.1162/neco_a_00999.
Giovanni Pezzulo, Thomas Parr, and Karl Friston. Active inference as a theory of sentient behavior.
Biological Psychology, 186:108741, 2 2024. ISSN 0301-0511. doi:10.1016/j.biopsycho.2023.108741. URL
http://dx.doi.org/10.1016/j.biopsycho.2023.108741.
22
Appendix
Library imports for inline code.
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
import scipy.integrate as integrate
Appendix 1
Continuous Bernoulli distribution
x∼CB(L) ⇐⇒P(x) ∝eLx, x ∈[−1,1] ⊂R
Below we provide a Python implementation of the continuous Bernoulli distributions:CB(L), parametrized
by the log odds L and adjusted to the [-1,1] interval.
def CB(x, b, logodds=True):
if not logodds:
b=np.log(b/(1 -b))
if np.isclose(b, 0):
return np.ones_like(x)/2
else:
return b * np.exp(b*x) / (2*np.sinh(b))
# Plot some examples
delta = 100
sigma = np.linspace( -1, 1, delta)
for b in np.linspace( -2, 2, 5):
p_sigma = CB(sigma, b)
sns.lineplot(x=sigma, y=p_sigma, label=b)
23
Appendix 2
Continuous Bernoulli distribution full derivation
Derivation of the exponential form of the continuous Bernoulli distribution, parametrized by the log odds L
and adjusted to the [-1,1] interval.
P(x; L) = eLx
∫1
−1
eLx
= eLx
eL−e−L
L
= L eLx
2sinh(L)
Appendix 3
Visualization of the likelihood function with various parameters, in python.
delta = 100
s_i = np.linspace( -1, 1, delta)
fig, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)
for idx, mu in enumerate([0.1, 0.5, 1]):
for w_i in np.linspace( -2, 2, 5):
p_mu = CB(s_i, w_i*mu)
sns.lineplot(x=s_i, y=p_mu, ax=axes[idx], label=w_i).set(title=f"$\\mu={mu}$")
Appendix 4
Expected value of theCB
ECB(b)[σ] =
∫
σ ebσ
2sinh(b)dσ=
b
(
(b−1)eb
b2 + (b+1)e−b
b2
)
2sinh(b) = coth(b) −1
b (30)
bs = np.linspace( -10, 10, 100)
plt.figure(figsize=(4, 1))
sns.lineplot(x=bs, y=1/np.tanh(bs) - 1/bs) # coth(b) - 1/b == 1/2 sech^2(b)
plt.show()
24
Appendix 5
Conservative dynamics
Let x∈Rn denote the system’s states (internal, blanket, and external states). The drift can be decomposed
into a gradient part (from a potential U) and a solenoidal part R:
˙x= −∇U(x) + R(x)
where R= −R⊺ is antisymmetric in state-space.
The probability densityp(x,t) over states evolves according to the Fokker–Planck equation:
∂tp(x,t) = −∇·[(−∇U + R)p(x,t)] + diffusion terms.
To determine the stationary distribution ps(x), we set ∂tp(x,t) = 0 . The Fokker-Planck equation
then implies that the net probability currentJs(x) = (−∇U(x) + R(x))ps(x) −D∇ps(x) (assuming isotropic
diffusion D) must be divergence-free:∇·Js(x) = 0. If we proposeps(x) = Z−1 exp(−U(x)/D) (where Z is a
normalization constant), then the diffusive current−D∇ps(x) becomes ps(x)∇U(x).
Substituting this intoJs(x), we get:
Js(x) = (−∇U(x) + R(x))ps(x) + ps(x)∇U(x) = R(x)ps(x).
Thus, ps(x) ∝ exp(−U(x)/D) is the stationary distribution if and only if the solenoidal component
of the probability current,JR(x) = R(x)ps(x), is itself divergence-free:∇·(R(x)ps(x)) = 0.
It is a key result from the study of non-equilibrium systems under the Free Energy Principle, particularly
those possessing a Markov blanket and involving conservative particles, that this condition∇·(R(x)ps(x)) = 0
holds (see [Friston et al., 2023]; but also related: Ao [2004]; Xing [2010]). The conditional independence
structure imposed by the Markov blanket, along with other FEP-specific assumptions (like conservative
particles), constrains the system’s dynamics such that solenoidal forcesR(x) do not alter the Boltzmann
form of the stationary distribution. Instead, they drive persistent, divergence-free probability currentsJR(x)
along iso-potential surfaces.
In steady state (∂tp = 0) , only the gradient term −∇U influences p(x). The stationary (nonequi-
librium) distribution is
p(x) ∝exp−U(x),
determined solely by the symmetric (potential) part−∇U. The antisymmetricR does not reweightp(x); it
just circulates probability around isocontours ofU.
In most NESS systems, antisymmetric flows do alter the stationary measure. Under particular-partition
(Markov-blanket) constraints, however, the internal–external factorization ensures that solenoidal (antisym-
metric) flows remain divergence-free, leaving the Boltzmann-like steady distribution intact. This is why,
in these Markov-blanketed systems, one can have persistent solenoidal currents (nonequilibrium flows) yet
preserve a stationary distribution that depends only on the symmetric part of the couplings.
Appendix 6
Detailed derivation of∂F
∂bq
1. Subsitute our parametrization into F
Let’s start with substituting our parametrization into eq. .
1a. Accuracy term
25
From the RBM marginalization (eq. ):
E(σ) = −biσi −
∑
j̸=i
Jijσiσj
  
Terms withσi
−
∑
j̸=i
bjσj −1
2
∑
j,k̸=i
Jjkσjσk
  
Terms withoutσi
Here, −biσi becomes constant, sinceσi is fixed. So we get:
P(σ\i|σi) ∝exp
(∑
j̸=i(bj + Jijσi)σj + 1
2
∑
j,k̸=iJjkσjσk
)
Taking expectation ofln P(σ\i|σi) under q(σi):
Eq[ln P(σ\i|σi)] = const+ ∑
j̸=ibjσj + S(bq) ∑
j̸=iJijσj + 1
2
∑
j,k̸=iJjkσjσk
Where S(bq) = Eq[σi] = coth bq −1/bq is the expected value of the CB, a sigmoid function of the
bias (#supplementary-information-4)).
1b. Complexity term
The complexity term in eq. is simply the KL-divergence term between two CB distributions. For
CBdistributions:
q(x) = bq
2 sinhbq
ebqx, p (x) = b
2 sinhbebx
·KL divergence definition:
DKL =
∫1
−1 q(x) ln q(x)
p(x) dx= Eq[ln q(x) −ln p(x)]
·Expand log terms:
ln q(x) = ln bq −ln(2 sinhbq) + bqx
ln p(x) = ln b−ln(2 sinhb) + bx
·Subtract log terms:
ln q(x)
p(x) = ln bq
b + ln sinh b
sinh bq
+ (bq −b)x
·Take expectation under q(x):
DKL = ln bq
b + ln sinh b
sinh bq
+ (bq −b)Eq[x]
·Compute expectationEq[x]:
Eq[x] =
∫1
−1 x bqebqx
2 sinhbq
dx= 1
2 sinhbq
[
ebqx
b2q
(bqx−1)
]1
−1
·Evaluate at bounds:
= 1
2 sinhbq
(
ebq(bq−1)−e−bq(−bq−1)
b2q
)
·Simplify using hyperbolic identities:
= (bqcosh bq−sinh bq)
b2qsinh bq
= coth bq − 1
bq
·Final substitution for the complexity term:
DKL = ln bqsinh b
bsinh bq
+ (bq −b)
(
coth bq − 1
bq
)
26
1c. Combining the two terms
Combining the two terms, we get the following expression for the free energy:
F = ln
(
bq
b
)
+ ln
(
sinh(b)
sinh(bq)
)
+ (bq −b)S(bq) −∑
j̸=i(bj + S(bq)Jij) σj −1
2
∑
j̸=i
∑
k̸=iJjkσjσk + C
where C denotes all constants in the equation that are independent ofσorbq.
2. Free Energy partial derivative calculation
·First, we differentiate the log terms:
∂
∂bq
[
ln bq
b + ln sinh b
sinh bq
]
= 1
bq
−coth bq
·Then, the KL core term:
∂
∂bq
[(bq −b)S(bq)] = S(bq) + (bq −b) dS
dbq
·The linear terms:
∂
∂bq
[
−∑
j̸=i(bj + S(bq)Jij)σj
]
= −∑
j̸=iJijσj dS
dbq
·The constants vanish.
·Now, combining all terms:
∂F
∂bq
=
(
1
bq
−coth bq
)
+
(
S(bq) + (bq −b) dS
dbq
)
−∑
j̸=iJijσj dS
dbq
·Substituting S(bq) = coth bq −1/bq:
=
(
1
bq
−coth bq
)
+
(
coth bq − 1
bq
+ (bq −b) dS
dbq
)
−∑
j̸=iJijσj dS
dbq
·Cancel terms:
1
bq−coth bq +coth bq−1
bq
+ (bq −b) dS
dbq
−∑
j̸=iJijσj dS
dbq
Gives us thefinal derivative:
∂F
∂bq
=
(
bq −b−∑
j̸=iJijσj
)
dS
dbq
Where dS
dbq
= −csch2bq + 1
b2q
.
Setting the derivative to zero and solving forbq, we get:bq = b+ ∑
j̸=iJijσj
Now we remember that the expected value of theCBis the Langevin function of its bias -,E(x) = coth(b)−1/b
(). For simplicity, we will denote it asL(x). Now we can write:
Eq[σi] = L(bq) = L
(
b+ ∑
j̸=iJijσj
)
Q.E.D.
27