RESEA RCH ARTICL E
Structure learning enhances concept
formation in synthetic Active Inference agents
Victorita Neacsu
ID
1
*, M. Berk Mirza
2,3
, Rick A. Adams
4,5‡
, Karl J. Friston
ID
1‡
1 Wellcome Centre for Human Neuroimag ing, Institute of Neurolo gy, University College London, London,
United Kingdom , 2 Department of Psychology , University of Camb ridge, Cambridge , United Kingdom,
3 Department of Neuroimag ing, Centre for Neuroimag ing Sciences, Institute of Psychiatry, Psycho logy and
Neurosc ience, King’s College London, London , United Kingdom , 4 Department of Computer Science, Centre
for Medical Image Computing, University College London , London, United Kingdom, 5 Max Planck Centre for
Comput ational Psychiatry and Ageing Research , University College London, London, United Kingdom
‡ RAA and KJF are joint senior author s on this work
* victorita.ne acsu.15 @ucl.ac.uk
Abstract
Humans display astonishing skill in learning about the environment in which they operate.
They assimilate a rich set of affordances and interrelations among different elements in par-
ticular contexts, and form flexible abstractions (i.e., concepts) that can be generalised and
leveraged with ease. To capture these abilities, we present a deep hierarchical Active Infer-
ence model of goal-directed behaviour, and the accompanyin g belief update schemes
implied by maximising model evidence. Using simulations, we elucidate the potential mech-
anisms that underlie and influence concept learning in a spatial foraging task. We show that
the representations formed–as a result of foraging–reflect environmental structure in a way
that is enhanced and nuanced by Bayesian model reduction, a special case of structure
learning that typifies learning in the absence of new evidence. Synthetic agents learn associ-
ations and form concepts about environmental context and configuration as a result of infer-
ential, parametric learning, and structure learning processes–thr ee processes that can
produce a diversity of beliefs and belief structures. Furthermore, the ensuing representa-
tions reflect symmetries for environmen ts with identical configurations.
1. Introduction
The main focus of this work is to illustrate the importance of structure learning as imple-
mented by Bayesian model reduction. We present a computational formulation of how agents
may come to form representations and concepts by foraging their environment, and how these
concepts may be shaped by structure learning (i.e., Bayesian model reduction). We attempt to
capture the computational mechanisms that underwrite concept formation and associated
relationships (i.e., relationships between elements within contexts and relationships between
contexts), as resulting from the action-perception cycles that govern agent-world interactions.
In this work, agents possess and update an internal model that entertains temporally and phys-
ically structured processes when interpreting these orderly interrelationships. Whereas
PLOS ONE
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 1 / 34
a1111111111
a1111111111
a1111111111
a1111111111
a1111111111
OPEN ACCESS
Citation: Neacsu V, Mirza MB, Adams RA, Friston
KJ (2022) Structure learning enhances concept
formation in synthetic Active Inference agents.
PLoS ONE 17(11): e0277199. https://do i.org/
10.1371/ journal.pone. 0277199
Editor: Stefan Kiebel, Technisc he Universita t
Dresden, GERMANY
Received: August 27, 2021
Accepted: October 24, 2022
Published: November 14, 2022
Copyright: © 2022 Neacsu et al. This is an open
access article distributed under the terms of the
Creative Commons Attribution License, which
permits unrestricte d use, distribu tion, and
reproduction in any medium, provided the original
author and source are credited.
Data Availabilit y Statement: Although the
generative model specified by the (A, B, C, and D)
matrices—ch anges from application to application,
the belief updates in Figs 1 and 3 are generic and
can be implemente d using standard routines
(spm_MDP _VB_X.m). These routines are available
as MATLAB code in the SPM academic software:
http://www .fil.ion.ucl.ac .uk/spm/. Code for
simulating the current implementa tion is available
at https:// github.com/vik yne/Concept_ Structure_
Learning. Please note that running these scripts
requires the SPM repository.
processes such as inference and parametric learning have been discussed extensively in the lit-
erature [1–17], little attention has been given to the type of off-line learning we define opera-
tionally as structure learning [18,19], with the implicit computational form within the Active
Inference framework. This is a nascent field of inquiry raising important questions about what
it means to process information off-line. In this paper, we take the first steps toward a compre-
hensive process theory of structure learning, grounded in a single objective function: that of
maximising model evidence. In the work by Smith et al, [19], the structure learning of concepts
proceeds passively (i.e., there is only an ‘observation model’). Here, we build on this work by
incorporating the ‘active’ part of the perception-action cycle, making this the first attempt to
connect action and perception in the context of structure learning, and thereby moving toward
a more ecologically valid account of model selection. Furthermore, this work is the first—to
our knowledge—to feature a comparison of information gain between online (i.e., inference,
parametric learning) and off-line (structure) learning.
The capacity to mine for similarities and detect dissimilarities across sets of experienced
(sensorial or autobiographical) events is a crucial facet of structured knowledge-building. This
type of relational thinking is known as concept learning, first proposed by Bruner, Goodnow
and Austin in 1956 in ‘A study of thinking’ [20]. More specifically, concepts are mental repre-
sentations that allow biological agents to compare and contrast collections or sets, and their
respective elements. Various researchers have since developed and expanded on concept learn-
ing. For instance, the prototype theory of concept learning suggests that biological agents pos-
sess a central example, a ‘common representation’ of a particular set, and then judge how
(semantically) far (or close) new experiences are in relation to the prototype [21]. Another
example is that of abstraction of rules, whereby concepts are characterised as a set of rules, and
agents assess new experiences (of objects, events, etc.) based solely on their respective proper-
ties and whether they fit the definitions or not [22,23]. A further instance is that of ad hoc cate-
gories in goal-directed behaviour [24], which describes a temporary and spontaneous type of
concept formation, such as ‘things to bring on a trip’. In this scenario, knowledge from differ-
ent domains is combined to form a novel temporary structure, specific to the context in play.
There is an intimate relationship between context and content: if I know the context (for
example, living room, beach, street, etc.) then I can call on a conditional probability distribu-
tion over the things I expect to see there (i.e., the content): sofa, TV, coffee table; sand, water,
floaties; buildings, cars, traffic signs. And if I know what I am seeing (i.e., the content), then I
can infer the contexts I may plausibly be in. This bidirectional relationship is implicit in our
computational model: during the inferential and learning processes, both these probability dis-
tributions are optimised simultaneously. That is, in order to find out which context is in play–
and to fulfil desired outcomes–agents use information acquired in previous time-steps to infer
the context in which they are operating. At the same time, this context places constraints on
outcomes in the future, given the actions they take (e.g., things I expect to see if I look over
there).
Concept learning spans several areas of inquiry relevant to both neuropsychology and
computational neuroscience: the way (biological) agents form concepts, how they interpret
context and content, what it means to represent concepts and relationships between different
elements within a context or between contexts, what similarity means, how humans categorise
environments, objects, and their elements into distinct entities, what role memory plays, what
counts as relevant information, and so on. A growing body of work in concept formation and
structure learning employs computational frameworks, such as non-parametric Bayesian mod-
els [25–29], where generative models are equipped with an extendable space. The focus in this
instance is on whether to incorporate additional components to the generative model, and at
what point. For example, in [29], concept learning is presented as inferring a hidden structure,
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 2 / 34
Funding: RAA is an MRC Skills Developm ent
Fellow (MR/S007 806/1) and is also supported by
the NIHR UCLH Biomedical Research Centre. KJF
was funded by a Wellcome Trust Principal
Research Fellows hip (088130/Z/0 9/Z). The funders
had no role in study design, data collecti on and
analysis, decision to publish, or prepara tion of the
manuscript.
Competing interests : The authors declare that the
research was conducted in the absence of any
commercial or financial relationships that could be
construed as a potentia l conflict of interes t.
and deciding whether the current structure should be reused, or a new structure should be cre-
ated. This representative example is relevant to our current model, where we disentangle three
processes: inference (about latent causes), parametric learning (learning associations), and
structure learning (deciding on the best generative model). Whereas non-parametric Bayesian
methods furnish one way of growing models in a principled way, our work considers Bayesian
model reduction, which starts with an over-complete, overly expressive model, and then
removes redundant components or model features, in order to minimise complexity. We use
Active Inference as the most generic formulation of Bayes-optimal behaviour that is necessary
to identify the best model or structure in terms of Bayesian model evidence.
Other related approaches to concept and structure learning from the machine learning lit-
erature involve model-based clustering algorithms such as Gaussian mixture models [30] or
hierarchical deep models [31]. Determining the optimal number of clusters in these
approaches ranges from fitting all the models in a family and selecting the best using a Bayes
information criterion [30], to augmenting deep Boltzmann machines with hierarchical Dirich-
let process priors [31]. Many of these approaches, however, require large amounts of training
data, unlike their human counterparts [32] and are generally difficult to evaluate in terms of
Bayesian model evidence [33,34].
In light of a common framework (i.e., Active Inference), most of these constructs can be
read as different manifestations (at various levels of description) of the same computational
mechanisms. In what follows, we hope to show that concept learning and formation can
emerge naturally from more parsimonious mechanisms: inference, parametric learning, and
structure learning, whose core imperative is that of maximising model evidence or minimising
(variational) free energy.
The neurobiological literature concerning concept learning is vast [35–45]. Many models
focus on hippocampal-neocortica l interactions, and more specifically on the interaction
between the hippocampus and the prefrontal cortex (PFC), given their wide involvement in
generalised knowledge-building [46–48]. In one relevant study Mack, Love, and Preston [41]
address the swiftness and flexibility of incorporating new knowledge and sensory information
into existing models of the world. In their study, they employ neuroimaging and a computa-
tional model called SUSTAIN [39] to ascertain the neural mechanisms underlying this aspect
of concept learning (i.e., integrating new information with pre-existing concepts). Subjects
viewed and categorised complex visual objects (insects) into groups by attending to either one
or two features (width of legs or antennae and pincers, respectively). Although the objects pre-
sented remained constant, they belonged to different categories based on the number of fea-
tures attended and their specific combinations. Subjects therefore had to integrate new and old
representations of the objects in line with this foundational structure. Neuroimaging results
confirmed the computational model prediction that objects encoded by similar representa-
tions should also evoke similar neural activity patterns. Further, in the hippocampus, these
conceptual representations were shown to evolve and reorganise as a result of assimilating new
information (in this case, new object features).
A complementary aspect of concept learning arises in the literature on problem solving and
insight. Generally speaking, there are two principal kinds of problem solving: a gradual, sys-
tematic approach towards a solution; and an instant, analytically based solution in the absence
of further (sensory) evidence or external information [49]. These two approaches have conve-
nient Active Inference counterparts: gradual updates to factorised beliefs about causes, sensa-
tions, and actions, (i.e., parametric learning) and the act of selection from some model or
hypothesis space (i.e., structure learning). In terms of the concept learning literature described
above, these two processes reflect the difference between forming representations (i.e., Bayes-
ian beliefs) by steady evidence accumulation and the belief updating it entails; and forming
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 3 / 34
new belief structures by associating the existing elements in novel ways (i.e., applying Bayesian
principles to the beliefs themselves). Finding a new or reduced model–that provides a better
explanation for the data encountered–provides a plausible formulation of insight and ‘Aha!’
moments [50–55].
Insight can be described as a sudden moment of understanding, and resolution of uncer-
tainty about a particular problem [49,56–59]. Four essential features define the experience of
insight: the initial impasse, the restructuring of existing associations and knowledge, the ‘Aha!’
moment, and a subjectively experienced feeling of certainty [49]. This resolution of uncer-
tainty has important consequences for how we solicit information and explore our environ-
ment: more certainty involves less exploration, or soliciting of new information. It is
noteworthy that neurobiological findings in problem solving parallel observations in category
learning and concept learning: as a reaction to the mental block, the PFC is thought to switch
its current approach of petitioning the hippocampus for information [59,60]. In addition to
this, the ventral tegmental area (VTA) is thought to encode the precision of the decision that
follows the reorganisation of associations [59]. Interestingly, empirical and theoretical work in
Active Inference suggests that the role of VTA is that of encoding the precision of policy selec-
tion [5,61]. Note that we associate insight with Bayesian model selection (i.e., Bayesian model
reduction), and therefore resolving uncertainty about causal structure, entailed by structure of
a generative model. This work focuses on two out of four features of insight: the initial trial-
and-error behaviour (associated with parametric learning in Active Inference), and the reorga-
nisation of existing current knowledge in the absence of new information (here, Bayesian
model reduction), rather than the subjective experience of insight. This work implies the use
of a generative model (i.e., beliefs encoding probability distributions over observed outcomes
and hidden causes). As we will see below, the reorganisation of knowledge entails restructuring
these beliefs as a result of Bayesian model reduction (BMR). Belief adaptation corresponds
here to parametric learning, where beliefs change gradually, based on observing data. Whereas
adaptation is in relation to the environment (i.e., agents ‘adapting to’ the environment as they
gather sensorial information), reorganisation is in relation to the generative model per se (i.e.,
agents minimising complexity, maximising model evidence). In other words, adaptation is due
to, and a result of moment-to-moment interactions with the environment, whereas reorgani-
sation entails off-line (model) optimisation in the absence of evidence. This reorganisation
may or may not be adaptive (to the environment), based on whether or how the environment
changes.
In Active Inference, the fastest timescale entails inferring hidden states of the environment
generating observed outcomes. At a slower timescale, it involves accumulating evidence about
contingencies, to optimise parameters of a generative model. At the slowest timescale, structure
learning proceeds by redistributing the products of learning to minimise model complexity,
thereby underwriting a generalisation to new experiences. Physiologically, these three levels
can be thought of in terms of neuronal dynamics responsible for belief updating about states
of affairs in the environment. Inference corresponds to moment-to-moment belief updating.
Experience-dependent learning can be associated with Hebbian plasticity as synapses accumu-
late contingencies [4,62]. Finally, model selection can be associated with a form of synaptic
homeostasis and the removal of redundant synaptic connections [63,64]. Computationally, all
these processes rest upon the imperative of minimising variational free energy (or maximising
model evidence), by changing the sufficient statistics that encode Bayesian beliefs about hidden
states (i.e., inference), parameters (i.e., parametric learning), and structure (i.e., model selec-
tion through reduction) of models. These optimisation processes have been formulated as vari-
ous aspects of Active Inference [3,4,55], whose biological plausibility is established to a certain
degree. Here, we put these three mechanisms in play together to see if we can reproduce the
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 4 / 34
cardinal aspects of concept learning, context learning and representation that emerge naturally
from self-evidencing [65].
Practically, we used simulations of agents situated in a novel environment. This environ-
ment comprised several rooms, two pairs of which had an identical form. Within each room, a
particular location afforded a reward. The agents had two hierarchical levels of action: they
could forage within each room at the lower level, or move between rooms at the higher level.
In ethology, this could be construed as a simple patch foraging paradigm [66]. To begin with,
agents had an imprecise representation of the possible types of rooms they would encounter,
but more importantly, they did not know a priori which context (i.e., room) they were in, or
the unique affordances of the different rooms. By simply optimising the evidence for their
model of the environment, we hypothesised that the agents would come to learn and remem-
ber the number of rooms and reward locations, thereby forming a representation of their
active engagement with the environment. Beliefs over hidden states, parameters, and the struc-
ture of their (actively explored) environment are encoded by–and underlie–these representa-
tions. The computational principles underlying Active Inference have been demonstrated in
other contexts such as saccadic eye movements [12,15], or at a more abstract level, prosocial
behaviours [67], and emotional constructs [68]. Here, we adopt a minimal model of spatial for-
aging and structure learning in order to clarify underlying processes and demonstrate key
ideas. This is the first paper, to our knowledge, to apply the principles of structure learning
(here Bayesian model selection) to spatial foraging during an active engagement with the
environment.
In what follows, structure learning will refer to Bayesian model selection, and in particular,
Bayesian model reduction, to find the best model of an active engagement with the environ-
ment. In virtue of the fact that these models are based upon discrete state-space models
(namely, Markov decision processes), different models are distinguished by the presence or
absence of a particular mapping among discrete states. In the context of the likelihood map-
ping, this will be between latent states of affairs in the world and observable outcomes. This
means that structure learning can be cast as exploring a space of mappings among discrete
states. We will demonstrate concept learning by applying parametric and structure learning to
the likelihood mappings, reading ‘concepts’ as the latent causes that generate observable out-
comes. Whereas parametric and structure learning are mechanistic processes, concept learning
is a teleological description of what these processes look like, from a psychological or construc-
tivist perspective.
The remainder of this paper comprises four sections. In the first, we briefly summarise
Active Inference as self-evidencing and its implicit minimisation of variational and expected
free energy, and we will introduce the notion of structure learning under Bayesian model
reduction. We then move on to a specific description of the generative model we use to unpack
these ideas and demonstrate the learning of likelihoods under structure learning and paramet-
ric learning. The third section presents a series of simulations (i.e., numerical analyses) show-
casing characteristic behaviours we hoped to elicit, and their associated belief updating. Our
key hypotheses were a) as agents forage their environment, they come to form representa-
tions–that is, precise (probabilistic) beliefs encoding the structure of the environment; b)
structure learning in the form of Bayesian Model Reduction (BMR) assists concept formation
and performance. With ongoing exposure to the environment, we hoped to see the emergence
of concept learning and improved performance both as a function of gradual (parametric)
learning and BMR. That is, agents will come to learn that there is a limited number of rooms,
with a particular topology, find the reward more often, and gather more reward overall. The
final section reviews the numerical experiments in light of existing empirical findings in
neurophysiology and ethology.
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 5 / 34
2. A brief account of Active Inference and structure learning
In this section we summarise Active Inference, and introduce the notion of structure learning
under Bayesian model reduction. The basic idea underlying Active Inference is that biological
agents are inference machines that minimise (variational) free energy or, equivalently, maxi-
mise model evidence. This can also be interpreted as self-evidencing [65]; namely, minimising
uncertainty about the environment [4]. Implicit in the Active Inference formulation is a (gen-
erative) model of the environment in the form of beliefs or probability distributions that
encode contingencies in the world. The environment (i.e., generative process) can be thought
of as being either extrapersonal [10] or the body [17], or both [9]. Briefly speaking, the agent
uses sensory data to update its beliefs about latent states and the most likely policies it should
pursue. This is known as inference. Since perception and action are optimised in tandem,
Active Inference agents hold and optimise beliefs about their own behaviour. They select
actions from the posterior beliefs about policies (i.e., plans), whereby a new observation is
solicited, in line with the goal of fulfilling prior preferences and resolving uncertainty. The
(variational) inference process in Active Inference can therefore be thought of as optimising
posterior beliefs about the causes of sensorial experience for past, present, and future (hidden)
states, based on observations, and depending upon the pursuit of specific policies [4]. The pro-
cess known as parametric learning involves the optimisation of beliefs about relationships
implicit in the interaction between different (latent) variables in the environment, where
actions are chosen to resolve uncertainty about the parameters of a generative model. These
parameters can encode beliefs (usually as concentration parameters) about likelihood (of out-
comes given hidden states), transitions (among states), preferences (for outcomes), initial
states and policies. In the Active Inference literature, these model parameters are usually called
A, B, C, D, and E matrices, respectively (please see Table 1 for a glossary of terms).
Further to minimising uncertainty about hidden states and parameters, agents also mini-
mise uncertainty about their generative models per se, also known as structure learning. Gener-
ative models are essentially alternative hypotheses about the potential causes that generate the
agent’s observations. With structure learning, one considers competing hypotheses about
these causes. Agents can therefore minimise uncertainty about their model based on model
comparison, where the winning model becomes the hypothesis under which observed out-
comes are the least surprising—i.e., the most likely hypothesis (having reduced all other types
of uncertainty). In order to optimise the other types of uncertainty (i.e., about latent states or
parameters), agents need sensorial (or factual) information, meaning that experience is
needed. However, Bayesian model selection (e.g., reduction) operates in the absence of further
sensory experience, since it proceeds by best explaining the experiences accumulated up until
that point in time.
Optimality—in the current Bayesian context—includes the joint principles of optimal
Bayesian decision-making under uncertainty, and the principles of optimal Bayesian design.
This is most clearly seen in terms of the two parts of the expected free energy objective func-
tion that we will describe in terms of intrinsic motivation (or value)—that scores the explor-
atory aspect of optimal behaviour—and the second part, which is the extrinsic motivation (or
value) that can be read as minimising the expected loss, or maximising expected reward. Opti-
mality entails both maximising expected (or extrinsic) reward, and minimising uncertainty (or
maximising information gain). By definition and construction, our agents are optimal in this
sense.
In Active Inference, extrinsic (pragmatic) value and intrinsic value (epistemic value, i.e.,
novelty and salience) are optimised simultaneously. This follows because policy selection is
based on expected free energy, which itself implies a dual pursuit: utility maximisation and
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 6 / 34
Table 1. Glossary of terms.
Notat ion/Term Meaning
o
t
2 f0; 1g
o
t
2 ½0; 1�
̑o
t
¼ ln o
t
Outcome s, their posterior expectation s and logarithms
~o ¼ ðo
1
; . . .o
t
Þ Sequences of outcomes until the current time point
s
t
2 f0; 1g
s
p
t
2 ½0; 1�
̑s
p
t
¼ ln s
p
t
Hidden states and their posterio r expect ations and logarithm s, conditioned on each policy
~s ¼ ðs
1
; . . .; s
T
Þ Sequences of hidden states until the end of the current trial
p ¼ ðp
1
; . . . ; p
K
Þ : p 2 f0; 1g
π ¼ ðπ
1
; . . . ; π
K
Þ : π 2 ½0; 1�
̑π ¼ ln π
Policies specifying action sequenc es, their posterior expectation s and logarithms
u = π(t) Action or control variables for each factor
γ, γ = 1/β The precision (inverse temperature ) of beliefs about policies and its posterior expectation
β Prior expectation of temperatu re (inverse precision) of beliefs about policies
A 2 ½0; 1�
̑A ¼ cðaÞ   cða
0
Þ
Likelihood matrix mapping from hidden states to outcomes and its expected logarithm
a
m
2 R 
a
m
2 R 
a
0
m 2 R 
a
0
m 2 R 
Prior concentratio n param eters of the likelihood
Posterior concent ration parameter s of likelihood
Reduced posterio r of the likelihood
Prior concentratio n param eters for the reduced model (of the likelihood)
B
p
t
¼ Bðu ¼ pðtÞÞ 2 ½0; 1�
̑B
p
t
¼ ln B
p
t
Transition probability for hidden states under each action prescribed by a policy at a particular time, and their logarithms
C :¼ ̑B
0
t
2 ½0; 1�
̑
C ¼ ln C
Transition probability for hidden states under a habit and their logarithm
U
τ
= ln P(o
τ
) Logarithm of prior preference or utility over outcomes
D 2 [0,1] Prior expectation of each state at the beginning of each trial
E 2 [0,1] Prior expectation of each policy at the beginning of each trial
Q Approxima te posterior distribut ion over the latent causes of the generative model– e.g. s, A, π
F : F
p
¼ F ðpÞ ¼
X
t
F ðp; tÞ 2 R 
Variational free energy for each policy
G : G
p
¼ GðpÞ ¼
X
t
Gðp; tÞ 2 R 
Expected free energy for each policy
H ¼   diag ð ̆A � ̑A Þ The vector encoding the entropy or ambiguit y over outcomes for each hidden state
s
t
¼
X
p
π
p
� s
p
t
Bayesian model average of hidden states over policies
Cat(A)
Dir(a)
Categori cal and Dirichlet distributio ns, defined in terms of their sufficient statistics (probabilities and concentr ation
paramete rs)
sð  GÞ
p
¼
expð  G
p
Þ
X
p
expð  G
p
Þ
Softmax function, returning a vector that can be treated as a proper probability distributio n
̑A ¼ E
Q
½ln A� ¼ cðaÞ   cða
0
Þ
̆A ¼ E
Q
½A
ij
� ¼ a � a
  1
0
a
0ij
¼
X
i
a
ij
Expected outcom e probabilitie s for each hidden states and their expect ed logarithm s
Bayesia n surprise A measure of salience based on the (Kullback -Leibler) divergence between the recognition and prior densities. It measures the
information in the data that can be recognis ed.
Conditi onal densit y/posterior
densit y
The probability distribution of causes or model param eters, given some data; i.e., a probabilist ic mapping from observed data
(consequence s) to causes.
(Kullbac k-Leibler ) Divergenc e Informati on divergence, information gain or relative entropy is a non-com mutative measure of the difference between two
probability distribut ions.
(Continued )
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 7 / 34
maximising information gain [55]. These complementary imperatives are combined into a sin-
gle objective function (expected free energy), such that the pragmatic and epistemic impera-
tives contextualise each other to provide the right balance of exploitative and exploratory
behaviour.
Typically, in a novel setting, the behaviour of Active Inference agents is dominated by
exploration until the epistemic values of available policies fall as uncertainty is resolved, at
which point extrinsic value dominates, manifesting as exploitative behaviour. The degree to
which agents explore therefore depends on the precision of their prior preferences that under-
write goals. Note that because these (extrinsic and intrinsic) values are log probabilities, their
combination in expected free energy corresponds to a multiplication of probabilities. This
means, a policy only has intrinsic value (i.e., information-seek ing value) provided it has a non-
trivial extrinsic value (i.e., goal-seeking value). For an extensive account of Active Inference
and associated tenets, please see: Da Costa, Parr [1], Friston, FitzGerald [3], Friston, FitzGerald
[4], Friston, Parr [7], Friston, Parr [18], Smith, Schwartenbeck [19].
In what follows, we briefly review inference, learning and model selection (a.k.a. Bayesian
model reduction) in terms of belief updating that minimises variational and expected free
energy.
2.1 Inference
The first equation describes the process of inference as the minimisation of variational free
energy–also known as an evidence bound [69]–with regards to the sufficient statistics of an
approximate posterior distribution over the hidden causes x (representing hidden states s, and
policies, π):
QðxÞ ¼ arg min
QðxÞ
F � P ðxj ~o Þ ð1Þ Var iat iona l free ener gy
F ¼ E
Q
½ln QðxÞ   ln P ð ~o jxÞ   ln P ðxÞ� ð1:1Þ
¼ E
Q
½ln QðxÞ   ln P ðxj ~o Þ   ln P ð ~o Þ� ð1:2Þ
Table 1. (Continued )
Notat ion/Term Meaning
Empiric al prior Priors that are induced by hierarchical models; they provide constraints on the recognition density is the usual way but depend
on the data.
Entro py The average surprise of outcom es sampled from a probability distribut ion or density. A density with low entropy means, on
average, the outcom e is relatively predictab le (certain).
Gener ative model A probabilist ic mapping from causes to observed consequen ces (data). It is usually specified in terms of the likelihood of
getting some data given their causes (parameters of a model) and priors on the parameters
Gradie nt descent An optimisation scheme that finds a minimum of a function by changing its argumen ts in proportion to the negative of the
gradient of the function at the current value.
Precis ion The inverse variance or dispersion of a random variable. The precision matrix of several variables is also called a concent ration
matrix. It quantifie s the degree of certainty about the variables
Prior The probability distribution or density on the causes of data that encode beliefs about those causes prior to observing the data.
Surpr ise Surprisal or self-informa tion is the negative log-probabil ity of an outcom e. An improbable outcome is therefore surprising.
Uncert ainty A measure of unpredictab ility or expected surprise (c.f., entropy) . The uncertainl y about a variable is often quantified with its
variance (inverse precision) .
https://do i.org/10.1371/j ournal.pone .0277199.t001
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 8 / 34
¼ D
KL
½QðxÞjjP ðxj ~o Þ�
|fflfflfflfflfflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflfflfflfflfflffl ffl}
rela tiv e ent rop y
  ln P ð ~o Þ
|fflffl ffl{zfflffl ffl}
log evi den ce
ð1:3Þ
¼ D
KL
½QðxÞjjP ðxÞ�
|fflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflffl}
comp lexi ty
  E
Q
½ln P ð ~o jxÞ�
|fflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflffl ffl}
accu rac y
ð1:4Þ
Where ~o ¼ ðo
1
; . . .; o
t
Þ designates observed outcomes to the current time. This equation can
be regarded as specifying the process of perception. It shows that minimising variational free
energy brings the Bayesian beliefs close to the true posterior beliefs by minimising the relative
entropy term (that is never less than zero). This is equivalent to forming beliefs about hidden
states of affairs that provide an accurate but parsimonious, complexity minimising, explana-
tion of observed outcomes. Complexity here is simply the difference between posterior and
prior beliefs; i.e., the degree to which one ‘changes one’s mind’ when updating prior to poste-
rior beliefs.
Action and planning are usually formulated as selecting the action from the most plausible
policy that has the least expected free energy:
p
�
¼ arg min
p
¼
X
t
Gðp; tÞ ð2Þ Expe cte d free ener gy
Gðp; tÞ ¼ E
~
Q
½ln QðA; s
t
jpÞ   ln P ðA; s
t
; o
t
j ~o ; pÞ� ð2:1Þ
¼ E
~
Q
½ln QðAÞ   ln QðAjs
t
; o
t
; pÞ�
|fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl ffl}
ðNega tiv eÞ nove lty
þE
~
Q
½ln Qðo
t
jpÞ   ln Qðo
t
js
t
; pÞ�
|fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl ffl}
ðNega tiveÞ sal ien ce
  E
~
Q
½ln P ðo
t
Þ�
|fflfflfflfflfflfflffl ffl{zfflfflfflfflfflfflffl ffl}
Extr ins ic val ue
ð2:2Þ
Where
~
Q ¼ Qðo
t
; s
t
jpÞ ¼ P ðo
t
js
t
ÞQðs
t
jpÞ. This equation identifies the best policy and accom-
panying action at the next time step. Notice that this kind of planning–based upon expected
free energy–involves averaging the free energy expected following a policy under the predicted
outcomes. This means the expected accuracy becomes extrinsic value; namely, the extent to
which outcomes conform to prior preferences. Similarly, the expected relative entropy
becomes an information gain pertaining to unknown model parameters (labelled novelty) and
unknown hidden states (labelled salience). These are sometimes referred to as intrinsic values.
2.2 Parametric learning
Parametric learning can be thought of as resolving uncertainty about (generative) model
parameters. Active Inference agents have implicit priors (e.g., A) and hyper-priors (e.g., a)
encoding beliefs about model parameters [3]. Since parametric beliefs (e.g., A) are represented
as categorical distributions, a suitable hyper-prior encoding the mapping between relevant
couplings (e.g., state-outcome) is specified in terms Dirichlet concentration parameters. Given
a state (s), the belief about the probability of an outcome is:
P ðojs; AÞ ¼ Cat ðAÞ ð3Þ
PðAjaÞ ¼ Dir ðaÞ )
E
PðAj aÞ
A
ij
h i
¼
a
ij
X
k
a
kj
E
PðAj aÞ
ln A
ij
h i
¼ cða
ij
Þ   c
X
k
a
kj
� �
8
>
>
<
>
>
:
ð4Þ
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 9 / 34
Where ψ represents the digamma (derivative of gamma) function. Agents accumulate Dirich-
let parameters as they are exposed to new observations, allowing them to learn. The updates
over these parameters involve accumulating the Dirichlet parameters that represent the map-
ping from hidden states to the observed outcome [1,3]. For example, updates to the concentra-
tion parameters of the likelihood mapping are defined as:
a ¼ a þ
X
t
s
t
� o
t
ð5Þ
Where a and a represent prior and posterior concentrations parameters, respectively and s
τ
corresponds to the posterior expectations about the hidden states.
Since accumulating (Dirichlet) concentration parameters (e.g., over the likelihood) is equiv-
alent to the type of change observed in synaptic (Hebbian) plasticity [4,62], this specific type of
update can be thought of as a synaptic strengthening, every time neurons encoding states and
observations (coupled by that synapse) are active simultaneously. This is a mathematical
description of Hebbian or associative plasticity. Note that in this particular example, noisy
mappings correspond to an imprecise likelihood mapping (e.g., making inferences under
observational uncertainty, such as being in a dimly lit room).
2.3 Structure learning and Bayesian model reduction
The previous two subsections summarised the computational processes entailed by online and
active engagement with the environment. We now turn to a different type of learning: learning
the structure of a model in the absence of new (sensorial) evidence. More specifically, this
work addresses the learning of the structure of the likelihood model, between periods of active
engagement with the environment. We operationally define this type of learning as structure
learning. In light of the Active Inference framework, structure learning has a specific computa-
tional form: Bayesian model (comparison and) selection [18,19]–which includes Bayesian
model reduction and expansion. Bayesian model selection entails the comparison and selec-
tion of models with the greatest (model) evidence (i.e., the least free energy) [70]. In other
words, structure learning can be thought of as a form of model selection, where agents compare
and assess alternative hypotheses defined by different (prior) configurations of their generative
model [18]. When synthetic agents engage in Bayesian model selection (i.e., reduction, expan-
sion), hypotheses about the structure of the environment are being compared against a single
objective function, and specific model features or mappings are removed (or retained). Since
in the Active Inference framework these (e.g. likelihood) mappings implicitly encode connec-
tion strengths [4], the reorganisation may entail the removal of existent (‘synaptic’) connec-
tions, or coupling of otherwise non-existent (‘synaptic’) connections. Although the ‘capacity’
for such connections exists in the generative model itself, the connections themselves are not
‘hard-coded’. Synthetic agents may perform an exhaustive search over the hypothesis space, by
considering all the associations found in the realm of possible combinatorics (of the specific
elements or features involved). To illustrate, consider the following example. An agent is look-
ing at two distinct faces for several trials, and there are two possible emotions being conveyed
(happiness, sadness). The agent starts with uniform beliefs. After a few trials, the agent
‘believes’ that face 1 is ‘happy’ and face 2 is ‘sad’. If it engages Bayesian model selection with
the current posterior beliefs, it can compare the current hypothesis against the hypothesis that
face 1 is ‘sad’ and face 2 is ‘happy’. If (retrospectively) there is more evidence for the second
hypothesis, then its associated ‘connectivity’ changes and the trials resume with this hypothesis
(i.e., model) instead. This means that although the ‘capacity’ for this specific belief structure
was there, there was no ‘connectivity’ between face 1 and ‘sad’ just before Bayesian model
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 10 / 34
selection. In this sense, Bayesian model selection (i.e., reduction, expansion) involves
reorganisation.
In this work, the focus is on a specific form of Bayesian model selection: Bayesian Model
Reduction (BMR). This involves applying Bayes’ rule to full and reduced (i.e., alternative)
models and evaluating the change in free energy (i.e., log Bayes factor or model evidence) for
each.
Bayesian model reduction is a post-hoc optimisation, and is applied to posterior beliefs (i.e.,
after all the data have been ‘seen’). Essentially, BMR refines the agents’ current beliefs based on
comparing alternative models (here, likelihood models) defined in terms of their priors. This
comparison entails evaluating the difference in free energy between the full model (i.e., model
with the original priors) and the model defined in terms of alternative priors. Applying BMR
reduces model complexity by eliminating redundant parameters.
The (relative) evidence for a full and an alternative (i.e., reduced) model with priors a
0
can
be derived by applying Bayes’ rule to both models:
P ðAj ~o ; m
alt
Þ
P ðAj ~o ; m
full
Þ
¼
P ðAjm
alt
ÞP ð ~o jm
fu ll
Þ
P ðAjm
full
ÞP ð ~o jm
alt
Þ
; ð6:1Þ
P ð ~o jm
full
Þ
P ð ~o jm
alt
Þ
¼
Z
d A P ðAj ~o ; m
full
Þ
P ðAjm
alt
Þ
P ðAjm
fu ll
Þ
�
Z
d A QðAÞ
P ðAja
0
Þ
P ðAjaÞ
ð6:2Þ
¼
BðaÞBða þ a
0
  aÞ
BðaÞBða
0
Þ
ð6:3Þ
P ðAjaÞ ¼ Di r ðaÞ ¼ BðaÞ
Y
i
A
a  1
i
ð6:4Þ
For a full derivation, please see [18]. In Eqs 6.3 and 6.4, B(�) denotes the multivariate beta func-
tion. The evidence ratio in Eq 6.2 may now be expressed as the change (i.e., increase or
decrease) in free energy as following:
DF ¼ ln P ð ~o jm
ful l
Þ   ln P ð ~o jm
alt
Þ ð7:1Þ
¼ ln BðaÞ þ ln Bða
0
Þ   ln BðaÞ   ln Bða
0
Þ ð7:2Þ
And
a
0
¼ a þ a
0
  a ð7:3Þ
Where a
0
is the reduced posterior, a represents the posterior concentration parameters, a
0
rep-
resents the prior concentration parameters defining a reduced (i.e., alternative) model, and a
represents the prior concentration parameters defining the full model. Note the simplicity of
these (local) update rules and their implicit biological plausibility [18]. The equalities in Eqs
7.2 and 7.3 allow ΔF to be computed in a biologically plausible way that underwrites synaptic
regression or pruning. In other words, the change in free energy that would have been
observed under alternative hypotheses (i.e., alternative/reduced models) can be used to remove
or retain certain connections depending upon whether the free energy bound on model evi-
dence increases or decreases. This measure is therefore used to either accept or reject alterna-
tive hypotheses (as defined by their concentration parameters). Usually, redundant parameters
are removed when ΔF � −3, corresponding to a Bayes factor approximately equivalent to 0.05,
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 11 / 34
meaning that the selected (reduced or alternative) model is 20 times more likely than the full
model [18].
The reduced (i.e., alternative) posteriors that emerge from the equations above—if the
reduced (i.e., alternative) model is accepted—are defined as following:
QðAjm
alt
Þ ¼ Bða
0
Þ
  1
Y
i
A
a
0
  1
i
¼ Di r ða
0
Þ ð8Þ
Bayesian model reduction is an efficient (and analytic) off-the-shelf procedure that scores
reduced (i.e., alternative) models—in terms of model evidence and accompanying posterior–
given the priors and the posterior under a parent model. For more technical details please see
[18]. The above equations (for the likelihood matrices) describe Bayesian model reduction for
Dirichlet processes that are apt for this particular model. Please see Table 1 in [18] for equa-
tions corresponding to other kinds of distributions.
3. The generative model
This section provides a specific description of the generative model we use to unpack and dem-
onstrate the learning of likelihoods under structure learning and parametric learning. A gener-
ative model is a joint probability distribution over observed outcomes and hidden causes.
Inference corresponds to inverting a generative model (generating consequences from causes)
using observed outcomes and forming posterior expectations about the hidden states (recover-
ing causes from consequences). Here, we describe the particular generative model used to sim-
ulate behaviour in terms of moment-to-moment belief updating, slower accumulation of
evidence–in the form of associative plasticity–and structure learning, in the form of Bayesian
model reduction. These distinct processes are emergent aspects of minimising the variational
bound on (negative log) model evidence described above. These processes have a fair degree of
biological plausibility, which enables us to simulate neuronal responses and changes in synap-
tic efficacy during inference and learning, respectively [4,7].
The challenge when using Active Inference schemes does not lie in devising a scheme that
underwrites Bayes optimal behaviour, but rather in specifying an appropriate generative
model that captures the behaviour and cognition induced by the task or problem at hand.
Once the generative model has been specified, model inversion (i.e., inference, learning and
selection) can proceed using standard belief updating schemes (e.g., spm_MDP_VB_X.m,
available in SPM12: www.fil.ion.ucl.ac.uk/spm/softwa re/download).
The generative model we use in the following simulations is a deep or hierarchical temporal
model [8,71] based on discrete states in a partially observable Markov decision process
(POMDP). It comprises two Markov decision processes, where the outputs of the higher level
generate the initial (hidden) states of the lower level. These types of models have been used
previously to model reading and language processing (57). Here, we use it to model spatial for-
aging within and between different contexts.
Each level of the generative model is parametrised by a set of matrices and vectors (more
generally, arrays): a likelihood matrix encoding probabilistic mappings from states to out-
comes (A), transition probabilities among the different hidden states (B), prior preferences
over outcomes (C), and finally, priors over initial states (D). As described above, these matrices
are parametrised with Dirichlet (concentration) parameters that are accumulated with experi-
ence: the combination of a given hidden state and outcome effectively adds a concentration
parameter (i.e., a count) to the appropriate element of the likelihood mapping.
The model used in this work generates three outcome modalities (at the lower level): the
location within a room, a reward outcome, and a room or context specific cue (i.e., the room
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 12 / 34
colour). The location modality has 16 levels corresponding to locations in a 4 x 4 grid. The
reward modality has two levels: present or absent. The context modality has 16 levels corre-
sponding to 16 possible rooms. The hidden states generating these outcomes comprise two fac-
tors: location (inside a specific room) and context (room identity). The location factor has 16
levels corresponding to sensed locations (i.e., 4 x 4 grid), while the context factor has 16 levels
corresponding to the room identity (i.e., contextual cue). In other words, we have two hidden
state factors (location and context) generating three outcome modalities (location, reward, and
context) at the lower level. The link to the higher level is via the context hidden state factor. The
content of the higher level therefore becomes the context for the lower level via this hidden
state factor—please see Fig 1 for a factor graph depiction of the generative model. As an anal-
ogy, being in a specific building (i.e. higher level) entails a specific set of available rooms (lower
level). For example, a school has laboratories and classrooms, each with their own configura-
tions, types of furniture, etc. The generative model acts as a simplification of the contingencies
entailed by a specific set of buildings, rooms, and their properties. Note that we deliberately
reproduce the (4 x 4) structure of the lower level at the higher level (i.e., 16 rooms with 16 loca-
tions). The implication here is that the generative model can be extended hierarchically to
Fig 1. Graphica l depiction of the generative model. This deep (tempor al) generative model has two hierarchi cal levels. At the lower level there are two
hidden state factors: Location and context. These generate outcomes in three outcome modalities: Location, reward, and context (i.e., room cue). At the
higher level, there is one hidden state factor and outcome modality : Context (room identity); the link between the higher and lower level is via the
context factor. Latent states at the higher level generate initial states for the lower level, which themselves unfold to generate a sequence of outcomes .
Lower levels cycle for a sequence of 5 time-steps for each transition of the higher level, and there are 5 epochs in the higher level for every iteration . This
scheduling endows the generative model with a deep temporal structure. The likelihood A is a matrix whose elements are the probabil ity of an outcome
under every combinatio n of hidden states. B represents probabilistic transiti ons between hidden states, which depend on actions determine d by policies
π. C specifies prior preferenc es and D specifies priors over initial states. Cat denotes a categorical probability distributi on. Dir denotes a Dirichlet
distribution (the conjugate prior of the Cat distribution). Please see Table 1 for a glossary of terms.
https://doi.o rg/10.1371/j ournal.pone .0277199.g001
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 13 / 34
furnish very deep inference and learning in a multiscale environment, with an implicit coarse
graining over successive scales (i.e., 16 buildings with 16 rooms with 16 locations).
Policies entailed four moves, where each move could be in one of four directions (up,
down, left, right). This means that there are 4
4
= 256 policies (i.e., plans) that could change the
location state (but not the context state). Agents could reach any of the locations in a room
from the starting location within this specified number of steps given the appropriate policy
(or policies). In each room, one location provided a preferred outcome or reward: present and
there was a null outcome or reward: absent everywhere else. The outcomes reward: present and
reward: absent were assigned a relative log probability (or utility) of 3 and 0, respectively. With
these utilities, the agent would expect (or ‘prefer’) a reward: present outcome � 20 times more
than the reward: absent outcome. At the lower level, the starting location was always the same,
namely location 7 (i.e., to the left and below the centre of the room). Please see Fig 2a for three
example illustrations of simulated behaviour in one of the 16 rooms.
The structure of the second level process is similar, however, the second level room identity
states generate the initial state of the context (i.e., room) factor at the lower level. More specifi-
cally, the room identity (i.e., the content from the point of view of the second level) becomes
the context from the point of view of the first level.
A crucial aspect of this (deep) generative model is that the higher level generates the initial
states at the lower level. This means that for every transition at the higher level, there is a suc-
cession of transitions at the lower level. For every room the agent visits, there are five time-
points at the lower level. The agent had control over only the location state through actions at
the lower level, while changes in the context (i.e., room) depend on the state transitions at the
higher level. This diachronic construction means that the context state cannot change in the
course of a trial at the lower level. The ensuing state transitions relax the Markovian con-
straints on belief updating–and accompanying behaviour–given the implicit separation of
temporal scales [8]. Note that the current model features similar contexts (i.e., rooms). This is
important: although agents can only forage within a specific room, they can generalise the con-
cept of that room to other similar rooms: ‘This is a living room’ (as opposed to a bedroom). Or
‘This is an apartment’, as opposed to ‘This is an office space’.
At the lower level, agents initially have uniform beliefs about the context they find them-
selves in (i.e., the room identities), and they are not equipped with any preferred trajectory or
sequential passage through the rooms. Furthermore, the agents have an imprecise mapping or
knowledge of reward locations. This means that upon entering one of the 16 possible rooms,
the agents were initially unaware of the identity of the context in which they were foraging,
and believed they could be in any of the 16 rooms. Conceptually, this can be thought of as hav-
ing an imprecise set of beliefs about what a room can contain: ‘I know that I am entering
Room 2, but I do not know what colour or reward location it entails, nor the relationship
between the room colour and reward location’.
The Dirichlet parameters encoding the confidence or precision about these various beliefs
(i.e., the likelihood mapping) were set to low values, such that accumulated experience would
have a substantive effect on the corresponding posterior expectations about probabilistic con-
tingencies. Importantly, although the process generating outcomes comprises 16 rooms, there
were only 14 unique rooms, as described by the contextual cue (colour) and reward location:
rooms 2 and 15 are identical both in terms of the colour and reward location, and so are
rooms 3 and 16 (please see Fig 2b for an illustrative sample of rooms). This means that the
agents have to learn there are only 14 unique context-specific reward locations. This presents a
learning problem for the agents at multiple levels. First, they have to explore each room opti-
mally, to resolve their uncertainty about whether there is a reward or not at each location. Fur-
thermore, they have to explore all the rooms to resolve uncertainty about which colours, and
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 14 / 34
reward locations would be elicited in the different rooms they explore. The agents can there-
fore leverage the information they know about the rooms (i.e., configuration) in order to pur-
sue the reward. Notice that, by construction, this hierarchical model can be extended to
arbitrary depth. That is, we could have rooms of rooms of rooms, navigated at progressively
slower timescales. For example, one could forage within rooms, apartments, buildings, bor-
oughs, cities and so on, whereby agents take several steps within a room during which the
apartment, building, etc., does not change.
The generative model generates outcomes by first evaluating the expected free energy for
each policy (at the higher level), and selecting the most likely policy (Figs 1 & 3). Latent states
are generated based on the transition probabilities specified for this policy. Latent states then
Fig 2. Example paths and room types. a) Examples of simulate d paths or policies that agents could choose in one of the 16 possible rooms. The agents
are allowed to make 4 moves, regardless of whether they find the reward or not. b) Sample rooms (out of 16 possible rooms). Rooms 2 and 15, as well as
rooms 3 and 16 share the same contextual cue (colour) and reward location (locatio ns 1 and 9 respectively). Every higher-leve l block involve s foraging
five rooms (whose identity is unknown) and exploring each of them in order for four time-steps in the lower level.
https://doi.o rg/10.1371/j ournal.pone .0277199.g002
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 15 / 34
generate outcomes in one modality (for this model, context), and the process repeats for the
lower level, whereby the outcomes are generated in three modalities: location, reward, and con-
text. Perception (i.e., inference about latent states) is equivalent to inversion of this generative
model (given a sequence of outcomes). Learning corresponds to parametric updates. Fig 3
summarises the associated belief updates about hidden states, policies and ensuing action
selection using the free energy minimising solutions of the previous section.
This model entails online planning–in the sense that, at each point in time, the agent evalu-
ates future trajectories in terms of expected free energy, and action is sampled from beliefs
about those policies. Briefly speaking, agents form expectations about future states by project-
ing their posterior beliefs to the future epochs, under each policy [3]. Policies are then evalu-
ated under these beliefs in terms of their expected free energy, which involves goal-fulfilling
and uncertainty-resolving components: c.f., expected value and information gain, respectively.
This renders policy selection (implicitly) contingent upon expectations of future states under
each policy. It is this aspect that lends synthetic agents the ability to plan (and explore). The
sampled action is more likely to originate from the policy with a lower expected free energy.
The selected action generates a new observation, and the perception-action cycle continues.
Please note that for the lower level in these simulations, the entire set of policies (4
4
= 256)
is in play. The set comprises a combination of every possible move (i.e., up, down, left, right)
over 4 time-points (also called deep policies). However, in a given trial, agents can eliminate
unlikely policies based on their evidence using an Occam’s window (i.e., if the difference in log
probability between a policy and the most likely policy is smaller than -3). This means that the
agent computes combinatorics over actions up until the very last time point. The policies at
the higher level refer to the potential rooms the agent can visit (i.e., 1–16); here, agents con-
sider policies over just one time step into the future.
Fig 3. Schemat ic overview of belief updating . Left panel: Belief updates defining Active Inference: State-estimati on, policy evaluation and action
selection. These belief updates are expressed in terms of expectations , which play the role of sufficient statistics for these categorical variables. Right
panel: Here, the expectations that are updated are assigned to various brain areas. This depicti on is purely schemati c, and its purpose is to illustrate a
rudimentary functiona l anatomy implied by the functiona l form of the belief updating. Here, we have assigned observed outcomes to the occipital
cortex, given its involvement in visual processing of spatial location [72,73], whereas reward outcomes are assigned to the inferotempo ral cortex given
its contributio ns to forming stimulus-re ward associations [74]. Hidden states encoding the context have been associate d with the hippocamp al
formation [75,76], and the remaining states encoding sampling location have been assigned to the parietal cortex, given its role in the encoding of
multiple action-based spatial representations [77–79]. The evaluatio n of policies, in terms of their expected free energy, has been placed in the ventral
prefrontal cortex. Expectations about policies per se and the precisio n of these beliefs have been associate d with striatal and ventral tegmen tal areas,
respectively, to indicate a putative role for dopamine in encoding precision [4]. The arrows denote message passing among the sufficient statistics of
each factor or marginal . First and second digits in the superscript (e.g., o
(1),1
) indica te the hierarchi cal level and modality, respectively. Please see
glossary in Table 1 and [4] for a detailed explanation of the equations and notation.
https://doi.o rg/10.1371/j ournal.pone .0277199.g003
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 16 / 34
4. Simulations and results
The main focus of this work is to illustrate the importance of structure learning as imple-
mented by Bayesian model reduction. We will demonstrate this in the context of Active Infer-
ence by showing how it enables agents to form concepts and improve their performance, as
scored by information gain, and the total reward gathered.
The kind of behaviour we hoped to elicit with this generative model can be described as fol-
lows: on repeated exposure to the rooms, the agents would explore optimally, defined by a tra-
jectory that avoids previously visited (i.e., uninformative) locations, thereby enabling the
agents to learn efficiently and remember which locations are rewarding and which locations
are not. Note that this learning is context-specific, in virtue of including a context factor in the
generative model. That is, each room has a location with reward and contextual cue (i.e., col-
our) to which the agent has access, regardless of the sampled location. Preferred outcomes are
time sensitive, in that the agents are only permitted to explore for up to 4 steps in each room
they forage. If the reward is not found within those 4 steps, foraging in that particular room
ceases, and they move to a different room. This process repeats for a given number of blocks at
the higher level, namely 2, 10, 20, 30, 40, and 50 blocks (of 5 rooms each). For the lower-level
process, this means that rooms were respectively sampled for 10, 50, 100, 150, 200, and 250 tri-
als, with five time-steps each.
In one condition, Bayesian model reduction was employed to reassign Dirichlet parameters
after each set of training blocks. This can be thought of as optimising the model structure in
the absence of any further sensory information. In this instance, Bayesian model reduction is
used to assess the evidence for reduced models that describe the structure of the environment.
For example, one hypothesis describes an environment where each possible room has its own
unique identity (despite the contextual cues and reward locations–see Fig 4a). A second exam-
ple hypothesis depicts the identical pairs of rooms as having a 50% probability of mapping on
to identical contextual cues (Fig 4b). Another hypothesis expresses the rooms with similar
reward locations and contextual cues as sharing a representation, therefore specifying the exis-
tence of only 14 rooms (Fig 4c). In a fourth example, rooms 15 and 16 have a uniform distribu-
tion over all the potential rooms, that is, these rooms are equally likely to have any of the other
possible identities (Fig 4d). These exemplar hypotheses describe potential model spaces depict-
ing likelihood mappings for the context factor. Values along a column must add to 1 since they
represent a probability distribution across the different possible rooms–that is, each column
represents the context states (i.e., room identity), and each row represents context outcomes
(i.e., room colour). Since the combinatorics of potential model spaces are extremely high, we
restrained the number of potential alternative hypotheses such that they reflect concentration
parameters for the likelihood context matrix that were observed as a result of the training trials
(i.e., such that they reflect the learnt state-outcome associations).
In the first set of simulations, the model accumulated concentration parameters to learn the
mappings from context and location states to the reward and context outcomes. If the free
energy (i.e., negative model evidence) is lower for any of the potential hypotheses, the mixture
of Dirichlet parameters is accepted and redundant synaptic connections are effectively pruned.
Conversely, if the free energy is higher, the original structure is left in play. For the testing
phase, we ran simulations in both conditions (i.e., models that did and did not undergo Bayes-
ian model reduction) for a further 20 blocks at the higher level (i.e., 100 more trials involving
the 16 potential rooms–please see Fig 4e for a graphical illustration of the simulation setup).
During these 20 test blocks, agents in both conditions (i.e., BMR versus No BMR) were pre-
cluded from accruing further concentration parameters, such that the performance with those
specific posterior distributions accumulated up to that point could be assessed (Fig 4e).
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 17 / 34
In what follows, we first show how agents learn associations and form concepts about the
identities and configurations of the rooms as a result of inference, learning, and model selec-
tion (i.e., Bayesian model reduction). Next, we show the performance benefits of Bayesian
model reduction, in terms of information gain, and the amount of reward accumulated. The
next set of simulation results addresses the capacity of agents to learn and infer that certain
rooms are identical, as defined by reward location and contextual cue (i.e., colour). Finally, we
show one source of individual differences in concept formation, namely how a stronger prefer-
ence for obtaining reward impacts concept acquisition and performance.
4.1 Agents form concepts by inferring and learning the structure of their
environment
For the training section of these simulations, agents started with uniform beliefs about which
context (i.e., room) they find themselves in. As far as they were concerned, upon entering a
(randomly) chosen room, they could be in any of the 16 possible room types. This is specified
in the structural prior of the Dirichlet concentration parameters of the context likelihood
matrix, initialised as a uniform 10
−1
. The reward concentration parameters were initialised as
10
−1
(i.e., imprecise priors) for the most plausible associations, and 0 otherwise. This means
that agents had a set of initial beliefs about the configuration of rooms (in terms of potential
reward locations), but were unable to make use of them without being able to discern the iden-
tity of the rooms. Initial concentration parameters over these modalities can be thought of as
an a priori set of associations (i.e., synaptic connectivity) about contingencies in the world that
Fig 4. Example alternati ve models and flow chart depicting simulations. a)-d) Example alternative models (i.e., hypotheses ). The genera tive process
(also Alternativ e model 1) and alternative hypothe ses were subject to Bayesian Model Reduction, focusing on the likelihood mappings encoding the
context modali ty. Matrices represent a mapping from context states (column s) to the context outcomes (rows)–this can be thought of as room identity
(s) to room colour (o). a) Note the identity matrix defined in the generative process is also used as an alternative hypothesis for model comparison . b)
The second hypothesis depicts the identical pairs of rooms as having a 50% probability. c) The third hypothe sis represents rooms 2 & 15 as being Room
15 and rooms 3 & 16 as Room 16. d) In the fourth hypothesis, rooms 15 and 16 do not exist, having a uniform distributi on over all the other potential
rooms–tha t is, rooms 15 and 16 are equally likely to have any other possible identities. e) Flow chart depicting the core simulatio ns for all 120 agents–
60 undergoing the ‘BMR’ condition, and 60 in the ‘No BMR’ condition.
https://doi.o rg/10.1371/j ournal.pone .0277199.g004
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 18 / 34
sets the scene for subsequent inference and learning. Initialising the context likelihood map-
ping with uniform concentration parameters—and the reward likelihood mapping with small
concentration parameters for the most plausible associations—is based on the following con-
siderations: initialising with uniform concentration parameters would cause the agent to attri-
bute any context and reward outcomes equally to all room identities (i.e., a uniform posterior
distribution over the room identity states), essentially preventing the agent from learning dis-
tinct associations between states and outcomes. We could have initialised likelihood mappings
with random concentration parameters, but this would have destroyed the relationship
between true and learnt room labels (e.g., room 1 is learned as room 5). For a straightforward
interpretation of the reduced models (used in the BMR analysis), we therefore used the reward
modality to resolve ambiguity about room identity.
As trials progress, agents update their beliefs about room identity (i.e., context) reflected in
both the configuration of their associations (a matrix), and the increased probability of finding
the reward. Fig 5a shows the averaged performance for agents foraging 50, 100, 150, 200, and
250 times (i.e., 10, 20, 30, 40, and 50 blocks respectively at the higher level), in terms of reward
accumulated. Performance per block increases for all the agents, with an initial concavity, sug-
gesting a preference for exploratory behaviour. In Fig 5b we show how the beliefs about con-
text change through time for one agent, and accordingly becoming increasingly precise.
Updates to the likelihood concentration parameters proceed as described above. A simple
interpretation of this (parametric) learning is a change in connectivity between observations
Fig 5. Average performance with learning . a) Progressive increase in performan ce scored by the amount of reward gained per block. For each higher-
level block, five rooms at the lower level are explored. The performance is averaged over 20 simulate d agents for each of the training settings : 10, 20, 30,
40, and 50 blocks (i.e., 50, 100, 150, 200, and 250 rooms respective ly). Please note that the dashed blue line illustrates a cap in performance , represente d
by total reward gathered per block, averaged over 20 fully knowled geable agents foraging for N = 50 blocks (i.e., agents that start with fully precise
likelihood matrices). As agents progress through the simulations, they accumulate more reward per trial. The concavi ty at the beginning of training
reflects explorato ry behaviour; i.e., intrinsic value predomina ted over the extrinsic value of rewards. b) Learning: The progressive updates to the
concentratio n parameters over state-outcom e associati ons from a uniform distribution to a more precise one, representing concept formation. The
agent forage s for N = 50 blocks at the higher level (i.e., 250 lower-lev el trials). Middle trial represents the end of block number 25 (at the higher level).
https://doi.o rg/10.1371/j ournal.pone .0277199.g005
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 19 / 34
and the specific context, quantified by the number of times they are inferred to co-occur [4].
For this model, this corresponds to the number of times the reward location and contextual
cue are associated with a particular room. In Fig 5b, rows represent the context outcome (i.e.,
room colour) and, and columns represent context state (i.e., room identity). This exemplifies
the notion of concept acquisition: agents start with uniform beliefs about state-outcome associ-
ations and as a result of inference and learning, they acquire an explicit (and reasonably pre-
cise) representation of environmental contingencies
The purpose of this section was not only to show that synthetic agents are able to form con-
cepts (resulting in better performance), but also to validate the generative model in its current
implementation and set the scene for illustrating benefits of BMR in later sections. Although
Fig 5b depicts one particular agent’s learning trajectory as it forages its environment—there
are 6 training conditions: we stop the training after 2, 10, 20, 30, 40, or 50 blocks. For each of
the 6 conditions, there are 20 agents. This numerical experiment is used later to illustrate how
state-outcome contingencies (and therefore performance) change with or without BMR, and
whether these effects vary with the amount of training.
4.2 Performance benefits of employing BMR
Along with a gradual learning of contingencies about the external world, concept formation
can also be a result of, and enhanced by, Bayesian Model Reduction–a faster and saltatory type
of learning. In Fig 6 we show results for three of the (120) simulated agents. We can see in Fig
6a (left panel) the generative process generating the data; that is, the ‘environment’ that agents
forage. It is useful to consider here the distinction between the generative process (i.e., envi-
ronment) and the generative model (i.e., the agent). Active Inference agents do not have direct
access to knowledge about (hidden) states of the environment and must infer them based on
observable outcomes. Although the structure of the environment that generates data is an
identity mapping (mapping from hidden states to observable outcomes) in the current work,
this does not preclude the agents from acquiring a different mapping; so long as it helps the
agents recognise the environment in a useful way that allows them to minimise uncertainty
and gather rewards. In other words, the representations (i.e., concepts) that agents form, need
not be identical to the actual form of the environment (and seldom are).
As previously mentioned, all agents start their foraging with an imprecise uniform distribu-
tion over their representations (of room identities). This means that they are not aware which
rooms they are foraging in (Fig 6a, right panel). Fig 6b shows the posterior concentration
parameters for three different agents after training for 2 (6b, left), 20 (6b, centre), and 50 (6b,
right) blocks (at the higher-level)—i.e., after 10, 100, and 250 training trials respectively).
Training blocks consist of inferential and parametric learning processes described above. Dur-
ing this training period, therefore, agents learn gradually as they accumulate evidence about
contingencies in the environment. After these training blocks, agents undergo BMR. As a
result of BMR, redundant parameters are pruned, and agents form more precise representa-
tions about contingencies in the environment (encoded as state-outcome associations–Fig 6c).
Because BMR maximises the evidence for a particular model or hypothesis, it does not just
find the simplest possible model, but discovers the best balance between accuracy and com-
plexity, thereby precluding over-pruning. Fig 6d quantifies the associated information gain
using the Kullback-Leibler (KL) divergence between the posteriors and priors over likelihood
parameters. The KL divergence is measured in nats: units of information based on natural log-
arithms. It can be seen in Fig 6d and 6e that Bayesian model reduction greatly enhances infor-
mation gain. Furthermore, engaging BMR after 2 training trials provides a very marked
change in concentration parameters relative to the two other conditions (Fig 6b and 6c). We
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 20 / 34
Fig 6. Likelihoo d mappings from hidden states to context outcomes , before and after BMR, and how these learned
mappin gs affect concept formation for three different agents. Matrices represent the likelihood mapping from context
states (i.e., columns ) to context outcomes (i.e., rows). a) The process generating the actual state-ou tcome mapping s (left)
and the uniform concent ration parame ters that agents start with (right). b) Likelihood matrices for the three agents
(averag ed over all locations ) at the end of 2, 20, and 50 training blocks at the higher level of foraging (from left to right). c)
Likeliho od matrice s after BMR, showing the reduced set of state-outcom e associations (i.e., likelihood) for the context
factor. d) Information gain for the context modality before and after BMR for each of the three agents. e) Compar ison of
inform ation gain before and after BMR, averaged over agents for each condition; light blue bars denote inform ation gain
after BMR whereas light green bars denote information gain before BMR (i.e., after N training blocks).
https:// doi.org/10.1371 /journal.pone. 0277199.g006
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 21 / 34
can also see in Fig 6d that agents that experienced fewer blocks ended up selecting different
alternative hypotheses as compared to the agents training for more blocks after undergoing
BMR. We expand on these results in the next section.
Fig 6e shows a comparison of information gain (in nats) before and after BMR. This is aver-
aged for all agents in each of the 6 conditions (i.e., N = 2, 10, 20, 30, 40, 50). As expected, infor-
mation gain shows an upward trend with the amount of training blocks both before and after
BMR. The trends observed in Fig 6d for the three different agents hold for the entire set of
agents: there is a marked difference in information gain when comparing between before
(light green bars) and after (light blue bars) undergoing BMR.
Next, we turn to the benefits of Bayesian Model Reduction for goal-directed behaviour in
terms of performance, defined as the time spent with reward–and how often the reward was
found. As described in the introduction, we can regard BMR as off-line hypothesis testing in
the absence of further information. Fig 7 shows the comparison between the two conditions in
question: BMR versus No BMR. In a training phase, agents foraged the environment for either
10, 50, 100, 150, 200, or 250 trials (i.e., 2, 10, 20, 30, 40, and 50 blocks at the higher level). In
one condition, the agents were subject to Bayesian model reduction (i.e., ‘BMR’ condition), as
described above, and thereafter continued foraging the rooms for a further 20 blocks (at the
higher level). In the other condition, agents continued foraging the rooms with the posteriors
accumulated during the training trials, without BMR (i.e., ‘No BMR’ condition).
Choosing to stop training after different numbers of blocks (i.e., 2, 10, 20, 30, 40, 50 blocks)
allows for a comparison between different stages in the learning trajectory and its effects on
performance: we can see in Fig 7 that in the ‘No BMR’ condition there is a sharp jump in per-
formance from 2 to 10 to 20 training blocks, which levels out with an increased number of
training blocks, but remains below the performance for agents in the ‘BMR’ condition.
Agents in the BMR condition appear to perform almost at peak even before foraging in all
of the 16 room types. This is because although the agents are not exposed to new sensory infor-
mation, the beliefs encoding the context factor become very precise, precluding further
Fig 7. Performan ce comparison between agents undergoin g BMR versus continuing with the posteriors accumulated after a specified number of
training trials (at the lower level). Each asterisk represents an agent; circles represent performance averaged over agents at a specified number of
training trials and their respective condition (BMR vs No BMR). Agents with n training trials are assigne d to one of the two conditions , and then
continue to forage for another 100 (lower level) trials. a) Total reward gained–agen ts undergoing BMR perform almost at peak, even before foragin g
through all of the 16 rooms . b) The number of times reward was (not) found–agent s in the ‘no BMR’ condition spend more time foraging without
finding the reward. Performan ce improves for agents in both conditions as they undergo more training trials.
https://doi.o rg/10.1371/j ournal.pone .0277199.g007
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 22 / 34
epistemic foraging, and therefore emphasising extrinsic value (i.e., agents become relatively
more exploitative). The performance for the agents in both conditions improves gradually,
levelling off after training for approximately 50 higher level blocks (i.e., 250 lower-level trials).
Furthermore, the agents in the ‘No BMR’ condition spend more time foraging the rooms with-
out finding any reward, a performance characteristic that does improve with more training.
4.3 Agents can learn that rooms with similar configurations are identical
One aspect of concept formation concerns the ability to represent invariance and symmetries.
In this section, we show that agents learn to associate the rooms with identical configurations
(i.e., colour and reward location) to form associations that encode similarity, defined as the
state-outcome connectivity of the context factor. This is a result of learning; however, this
aspect is evinced more clearly following Bayesian Model Reduction. Most importantly, none
of the simulated (sixty) agents undergoing BMR settled on the hypothesis specifying an iden-
tity mapping for the context factor. This means that despite having a process generating
observed outcomes (i.e., generative process) with an identity mapping (i.e., each room has an
individual identity), none of the agents judged this mapping (of state-outcome associations) as
being the most parsimonious (i.e., explaining the observations accurately, in as simple a way as
possible). The representations (i.e., concepts) formed by synthetic agents can–but do not have
to—reflect the actual form of the environment, as long as they aid synthetic agents in interpret-
ing the environment in a useful way (i.e., allows them to minimise uncertainty and gather
rewards).
The agents come to recognise the rooms as being different only when their configurations
could be disambiguated (i.e., as a result learning, rooms with different reward locations and
contextual cue were not confused with each other). Fig 8a shows the final encoding of environ-
mental structure (i.e., context mappings) for three different agents as examples. The agent on
the left in Fig 8a believes that rooms 2&15 are room 15, and rooms 3&16 are room 16. The
middle agent believes that rooms 2&15 are room 2, and rooms 3&16 are room 16. The agent
on the right, however, believes that there is a 50% probability of being in either of the rooms
with identical configurations. For example, when this agent is in room 15, it believes that it
could be in either room 15 or room 2, with equal probability.
There is diversity in terms of these learnt mappings, based on variations in foraging the
rooms as a result of pursuing different intrinsic and extrinsic affordances. As noted above,
when implementing BMR for Dirichlet hyperparameters (in this case the context likelihood
mappings), agents compute a relative log evidence (i.e., free energy) for each model, and com-
pare this score to the evidence of the parent model. Subsequently, agents select the model with
the greatest evidence (Fig 8b & 8c). The most frequently chosen alternative hypothesis (i.e.,
alternative model) is the one whereby there is a 50% probability of being in either of the rooms
represented by identical configurations (i.e., model/hypothesis 7). Fig 8b also shows the per-
centage of time the alternative hypothesis (i.e., alternative model) with a 50–50 probability for
the rooms with identical configurations was chosen when applying BMR for the entire set of
agents (i.e., when applying BMR to all the agents after various training blocks), consisting of
120 synthetic agents.
Interestingly, in addition to learning associations that encode similarity between rooms, in
some cases agents also showed a similarity in simulated neural activity as characterised by
(simulated) local field potentials, firing rates and dopaminergic responses. We illustrate the
electrophysiological responses, associated with belief updating, for one agent foraging two
rooms with identical configurations (rooms 2 and 15), during the training blocks (N = 50) (Fig
9). The agent follows a similar trajectory in these two rooms, gathering reward for the last two
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 23 / 34
time-steps. The top-left panel of Fig 9 shows average local field potentials over all the units
encoding the context factor before (dotted line) and after (solid line) bandpass filtering at 4Hz,
juxtaposed with its time frequency decomposition. The lower-left panel illustrates evidence
accumulation for these units. The top-right panel shows the rate of change of neuronal firing.
Finally, the lower-right panel illustrates simulated dopaminergic responses defined as an amal-
gamation of precision and its rate of change. This is reminiscent of demonstrations (using
Representational Similarity Analysis) that neural activity patterns to repeated presentations of
identical or related stimuli are likewise very similar [41]. Please see the S1 Appendix for a con-
trast in electrophysiological activity that ensues as a result of the same agent foraging the same
room, with the same trajectory (i.e., Room 15) at two different trials, and of foraging different
rooms (i.e., Rooms 4 and 12) with a similar trajectory.
Posterior beliefs about policies are obtained by applying a softmax function to precision
weighted (negative) expected free energy of each policy. The precision parameter is estimated
as new observations become available, and it plays the role of an inverse temperature, meaning
that the policy with the least expected free energy becomes more likely to be selected if the
Fig 8. Possible representa tions of similarity between the rooms for three different agents after BMR and the most frequen tly chosen hypothesis
during BMR. a) Likelihood matrices representing the reduced posterior concentration parame ters. The matrices represent the context state-ou tcome
mappings with rows representi ng the context state, and the columns representi ng the contex t outcome. The likelihood mapping for the first agent
shows that rooms 2&15 as having the identity of context 15, and rooms 3&16 as being context 16. The second agent’s beliefs show that rooms 3&16 have
the identity of contex t (room) 16, and rooms 2&15 as having the identity of context 2. The third agent believes that there is an equal probability for the
rooms that are identical in terms of their configurati on: 2&15 can be either context 2 or 15 and rooms 3&16 are equally likely to be either contex t
(room) 3 or 16. b) The percentag e of time the hypothesis with an equal (‘50–50’) probability for the rooms with identical configurati ons was chosen by
the agents, for different numbers of training blocks. At N = 50 (i.e., after 50 higher level training blocks) this hypothesis is chosen 100% of time–that is,
all 20 agents training for 50 higher level blocks, selected this hypothe sis as being the most parsimonio us, explaining the observat ions with the least
model complexity . c) The negative log evidence for the twelve alternative hypothe ses/models (x axis) for the entire set of agents (y axis, 120 agents).
Model 7 appears to consistentl y have the greatest eviden ce (i.e., least free energy).
https://doi.o rg/10.1371/j ournal.pone .0277199.g008
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 24 / 34
precision parameter is high. In other words, this precision encodes the confidence that the
inferred policies will lead to preferred outcomes or resolves uncertainty about the hidden
states. Previous work [16] suggests that the dopaminergic activity in the mid-brain might
encode this kind of precision. In our paradigm, the phasic bursts we see in simulated dopami-
nergic responses indicate that at step 2 (i.e., the 32
nd
iteration in terms of updates–Fig 9a and
9b, bottom right panels) the agent becomes more confident (i.e., resolves uncertainty) about
which policies to pursue, having eliminated the possibility that the room it is foraging is room
14, given that it did not discover a reward at location 6, which is the rewarding location for
room 14. During the second spike at step 4 (i.e., the 64
th
iteration) the agent eliminates further
possible policies, having become more confident that the room it is foraging is neither room 3
nor 16 (with reward at location 9), since it found a reward at location 1. This example illus-
trates how one can unpack belief updating and decision-making, while encoding uncertainty
and precision.
4.4 The strength of prior preferences impacts concept formation
One source of individual differences in concept formation reflects the preference for some out-
comes over others. We asked whether prior preferences (i.e., regarding reward) influence
learning and subsequent performance, by simulating three agents who experienced the same
number of training blocks at the higher level (N = 20). For one agent, we reduced the precision
of prior preference over outcomes (reward) to 0.5 and 0 elsewhere (as compared to the default
used in all other simulations of 3 and 0 elsewhere). This means that the agent has a weaker
reward preference, compared to its conspecifics. Fig 10a shows the learned context likelihood
matrices for the agents that have different degrees of preferences for reward. The third agent
(Fig 10a, right) starts with a fully precise set of likelihood matrices. We use this agent as a base-
line to help illustrate the performance comparison between agents with higher and lower pre-
cision in prior preferences, providing a cap on the total amount of rewards that agents can
Fig 9. Neural activity for a synthetic agent in two rooms with identical configuration s. In these epochs, the agent forages the two rooms in the same
manner–that is, it follows the same trajectory of locations. During the last two steps, the agent encount ers the reward and stays with the reward for one
more step. Please see main text above for more details. a) Room 2 b) Room 15.
https://doi.o rg/10.1371/j ournal.pone .0277199.g009
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 25 / 34
gather. The agent with weaker preferences does not accumulate as much reward (Fig 10b), but
learns more (Fig 10c). Here, the degree of learning was assessed with the information gain or
KL divergence between posterior and prior Dirichlet concentration parameters for the context
likelihood matrix. This quantifies how much the agent has learned about the state-outcome
associations from the start of the simulations. This example shows that the agent with a weaker
preference for rewards is more sensitive to epistemic incentives, and subsequently, learns
more efficiently.
5. Discussion
This work focused on the importance of structure learning as implemented by Bayesian model
reduction. We presented a series of simulations: we first established that agents form concepts
and gather rewards as a result of interacting with their environment. Consequently we demon-
strated the benefits of Bayesian model reduction, defined as an improvement in performance.
Synthetic agents foraged a novel environment defined as a set of (16) rooms (contexts), each
having 16 available locations. Agents started with an imprecise set of representations about the
potential types of room (contexts)–in terms of possible reward locations–and a uniform distri-
bution over the set of contexts in terms of their identity. That is, agents were unaware of which
specific context they are in when they started foraging, but they had some representation
about the possible configurations they could encounter. Without knowing the context they
find themselves in, agents can only find the reward by chance. Agents came to learn the identi-
ties of the rooms (via updates to concentration parameters), and form precise beliefs about the
Fig 10. Performance comparis on between an agent with a strong preferenc e for rewar d (C = 3) versus an agent with a weaker preferenc e (C = 0.5).
a) Likelihood mapping s after 20 training blocks, including a fully knowledge able agent (right). b) Total reward accumu lated over 20 (higher level)
blocks. c) Comparison between the two agents, in terms of the information gain associated with the context modality.
https://doi.o rg/10.1371/j ournal.pone .0277199.g010
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 26 / 34
structure of the environment they were foraging i.e., they formed concepts. By learning their
environment, agents gradually start performing better–they develop the ability to find the
reward more often, and gather more reward overall.
Bayesian model reduction enhanced the implicit concept acquisition and formation,
endowing (synthetic) agents with representations that are more precise. The BMR process
does not prune associations unless they combine high complexity with an inability to explain
observations accurately. In sections 4.1 and 4.2 we saw that the parametric beliefs encoding the
likelihood mappings of the context factor change (i.e., diverge from the initial uniform beliefs)
depending upon the amount of training and whether or not BMR has been used. There was a
marked difference in information gain after BMR relative to before BMR. Undergoing BMR
entails higher information gain with regards to the parameters encoding the (state-outcome)
representations. To our knowledge, this is the first attempt to compare information gain in
Active Inference agents using BMR–by comparing between information gain following BMR,
and information gain as a result of parametric learning. Future work could use this metric to
ascertain how and when it would be most useful for both machines and humans to engage in
structure learning.
Interestingly, during the learning (i.e., training) process, agents occasionally ‘mislabel’
rooms; i.e., appear to be confusing room identities. This is most likely due to the way these
agents forage the environment: for example, if we have one agent foraging in a room, and the
foraged locations do not contain the reward, the agent is likely to label this room as an other
room whose corresponding locations do not contain a reward, everything else being equal.
Further to the effect of increased precision in concept formation, we have illustrated the
performance benefits of BMR (section 4.2) in the context of goal-directed behaviour. In these
numerical experiments, alternative hypotheses about the structure of the likelihood matrix
(i.e., the agent’s beliefs about the set of rooms and their identities) were entertained. Selecting
the most parsimonious hypothesis (i.e., the one with the greatest evidence) allowed the agents
to minimise their uncertainty about their environment, and to use this knowledge (i.e., room
identity as defined by its cue) to secure rewards. For example, after BMR, agents become more
confident that the room they are foraging is pastel orange (i.e., room 6), and they can use this
information to head to the reward (left and up on the first move), rather than responding to
epistemic affordances or novelty. Agents undergoing BMR performed consistently better than
agents not undergoing BMR. Most importantly, agents with BMR performed well, even before
foraging all the rooms. This is important because it speaks to the ability of generative models
with higher evidence to be generalised to new data and contexts [80]. This reflects the implicit
idea behind [38,40,41] whereby the reorganisation and restructuring of information changes
the form of concepts, regardless of whether more external information is assimilated or not.
We have also seen this aspect in the problem-solving literature, where no further sensorial (or
factual) information is necessary for insight [49,56–59]. Furthermore, Bayesian model reduc-
tion has been associated with physiological processes such as the regression of synaptic con-
nections or pruning, observed during periods of sleep [63]. In this setting, the structure of
generative models is learned by minimising model complexity in the absence of sensory data,
when accuracy does not contribute to log evidence [81,82]. For example, in sleep, endogenous
activity–that resembles neural message passing in wakefulness–has been interpreted as the
generation of fictive data to evaluate model evidence: c.f., [83]. That is, fictive episodes are
‘replayed’, in the absence of (precise) sensory information, in order to optimise generative
models (with the implication that this kind of model reduction facilitates generalisation).
Concept learning and formation were defined as a basic cognitive function, whereby (bio-
logical or artificial) agents identify and update beliefs about conditional dependencies and
independencies in the environment. In problem solving, there is a slow, systematic process,
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 27 / 34
and a fast process; namely, insight (defined as a sudden recognition of task-dependent contin-
gencies). Although at first sight these processes appear to be disparate cognitive tools, they both
appear to be underlined by the same computational mechanisms: Active Inference, parameter
learning, and model selection. One might suggest that they have a similar teleology: to compare
and organise how things relate to one another, thereby systematising relational knowledge to
form parsimonious representations. Both these strategies imply a resolution of uncertainty
about the causes of sensorial experience, behaviour, parameters of their model and the model
per se. Biological agents can form and update representations in a gradual, systematic fashion,
their learning punctuated with sudden realisations (i.e., resolution of uncertainty about the best
model), regardless of what the representations themselves are about (e.g., the concept of a ‘liv-
ing room’ or how to best arrange tiles in a bathroom). As we have seen in our simulations,
agents can either learn the identities of the rooms gradually, or invoke BMR, and settle on a
reduced model in order to continue foraging familiar rooms and collect familiar rewards.
Drawing parallels to the literature on insight [49,56–59], one can associate the number of
training trials before invoking BMR as a ‘forced’ time-step threshold at which agents ‘reach’ an
impasse. This moment precedes reorganisation of accumulated associations (i.e., model com-
parison), where different hypotheses about the configuration of rooms come into play. The
‘Aha’ moment can be thought of as the point at which the agent has ‘decided’ upon a particular
hypothesis (i.e., model selection). Furthermore, we have seen in section 4.3 that agents can
exhibit a pronounced inter-agent variability: a characteristic that pertains to the outcomes
sampled, rather than the agents themselves. This has further implications in the realm of indi-
vidual differences, because it can potentially elucidate how different individuals sampling dif-
ferent (sensorial) observations can reach the same conclusion (i.e., alternative hypothesis
defining contingencies in the outside world), as well as how similar individuals sampling simi-
lar observations can reach different conclusions, as observed for example in [84], where indi-
viduals exposed to similar sensory information diverged in terms of whether they perceived a
(bistable) stimulus as a vase or face. A possible limitation of this current work is the small
number of alternative hypotheses (i.e., models). There is a combinatorial explosion for a 16x16
grid of possible room types given the contextual cue, resulting in a limited set of feasible alter-
native hypotheses (i.e., 12 in number). These arguments call for a better understanding of how
the abstract alternative hypotheses compared for model selection are themselves generated,
represented, and used both within and between agents. In other words, what are the rules that
govern which hypotheses to select among–and are there models of models?
We have shown that as agents forage and learn about their environment, they also come to
ascribe the same identity to rooms with similar configurations (i.e., colour and reward loca-
tion). Furthermore, the similarity in representation was accompanied by very similar neuro-
physiological responses, as seen empirically in the concept learning literature by Love and
Gureckis [38], Love, Medin [39]. It remains to be seen when this is or is not the case across the
board. For example, whether similar representations at time-points far apart evoke this same
effect, or whether there is a relationship describing discrepancies between items of the same
class. Interestingly, in sections 4.1 and 4.3, we saw that in the first instance (i.e., N = 2, N = 10),
Bayesian model reduction appears to promote generalisation, with agents perceiving the rooms
with identical configurations as being one room. After more experience however, the agents’
beliefs seem to diverge again, ‘perceiving’ the rooms with similar configurations as having a 50–
50% probability of being each of the two possible rooms. That is to say that agents are retaining
both representations in an attempt to maintain a ‘flexible’ set of beliefs, regardless of having evi-
dence to the contrary (i.e., that they do not need two separate concepts). A future research
direction could identify this potential computational benefit of developing and retaining a ‘flex-
ible’ set of beliefs about contingencies in the lived world, in light of the Active Inference
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 28 / 34
account. That is, what are the useful measures when deciding whether to retain a more flexible
but less precise set of beliefs, versus a more rigid but also more precise set of beliefs?
Finally, we considered one source of individual differences, namely the strength of prior
preferences for reward. Preferences affected concept acquisition and therefore the way agents
formed representations. An agent with an imprecise preference for reward explored its envi-
ronment more, and diverged more from its prior beliefs encoding state-outcome associations.
Making more exploratory choices in this case hindered performance, in terms of reward
gained, as well as the number of times the reward was found. These results are reminiscent of
work by [85]: here, the authors demonstrate that in Active Inference, uncertainties pertaining
to the agents’ goals and preferences are prioritised over other types of uncertainty. Active
inference thus provides a Bayes optimal and principled approach to balancing epistemic (i.e.,
exploratory) and instrumental (i.e., exploitative) actions. As predicted by [85], this balance
depends on the shape of agents’ beliefs; in our case underwritten by prior preferences. In light
of these (and our results), agents minimise uncertainty insofar as it is required for fulfilling
their goals, whatever they may be defined as. When the imperative for satisfying prior prefer-
ences is diminished in relation to epistemic imperatives, the balance between exploration and
exploitation shifts towards explorative behaviour, and vice versa. Future computational and
empirical work may involve assessing agents with different levels of reward preferences, to see
whether agents with a strong preference end up forfeiting exploratory behaviour (and, with it,
predictive power) in an attempt to obtain rewards.
Structure learning as implemented by Bayesian model selection (here, Bayesian model
reduction) is an emerging topic in the computational neurosciences. The literature on this bur-
geoning topic is currently limited. Part of the motivation for this work was to expand on this
literature and foreground the importance of this type of learning (i.e., learning in the absence
of new evidence), and how it can influence belief formation and updating. In summary, we
have presented a deep hierarchical Active Inference model of goal directed behaviour, and the
associated optimisation schemes implied by maximising model evidence. We have used
numerical experiments to showcase a series of potential mechanisms that underwrite concept
learning in a spatial foraging task. Synthetic agents formed concepts about the identities and
configurations of the ‘rooms’ in a synthetic environment as a result of free energy minimising
inference, learning, and model selection processes–three processes that contextualise each
other and can produce a diversity of beliefs and belief structures. Structure learning as imple-
mented by Bayesian model reduction enhanced concept formation, showing an improvement
in performance as scored by information gain and the amount of reward gathered. Further-
more, the representations formed as a result of these three processes reflected symmetries for
‘rooms’ with identical configurations (i.e., reward location and contextual cue). Finally, we
have shown that the shape of prior beliefs (in this case, the strength of prior preferences for
reward) affects the balance between exploration and exploitation.
Software note
Although the generative model specified by the (A, B, C, and D) matrices—changes from
application to application, the belief updates in Figs 1 and 3 are generic and can be imple-
mented using standard routines (spm_MDP_VB_X.m). These routines are available as
MATLAB code in the SPM academic software: http://www.fil.ion.ucl.ac. uk/spm/.
Supporting information
S1 Appendix. Neural activity comparison for four further rooms. Panels a) and b) in the fig-
ure show Room 15 with the agent adopting the same trajectory (locations 7, 11, 10, 14, 14) at 2
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 29 / 34
different instances: a) block 28 and b) block 30. Neural activity appears to be similar, as antici-
pated. Panels c) and d) compare different rooms with the same trajectory (locations 7, 11, 10,
14, and 14), who’s neurophysiological activity also differs in spite of having a similar trajectory.
Panel c) depicts simulated electrophysiological activity for Room 4 and panel d) shows activity
for Room 12.
(TIF)
Acknowledgmen ts
Special thanks to Thomas Parr for our virtual conversations about Active Inference.
Author Contributions
Conceptualization: Victorita Neacsu, M. Berk Mirza, Rick A. Adams, Karl J. Friston.
Data curation: Victorita Neacsu.
Formal analysis: Victorita Neacsu.
Investigation: Victorita Neacsu.
Methodology: M. Berk Mirza, Rick A. Adams, Karl J. Friston.
Supervision: Rick A. Adams, Karl J. Friston.
Visualization: Victorita Neacsu.
Writing – original draft: Victorita Neacsu.
Writing – review & editing: Victorita Neacsu, M. Berk Mirza, Rick A. Adams, Karl J. Friston.
References
1. Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V, Friston K. Active inference on discrete state-space s:
A synthesis . Journal of Mathemat ical Psychology. 2020; 99:102447. https://d oi.org/10.101 6/j.jmp.202 0.
102447 PMID: 333430 39
2. Friston K, Buzsa ´ ki G. The Functiona l Anatomy of Time: What and When in the Brain. Trends Cogn Sci.
2016; 20(7):500– 11. https://doi.or g/10.1016/ j.tics.2016.05 .001 PMID: 272610 57
3. Friston K, FitzGerald T, Rigoli F, Schwarten beck P, O’Doherty J, Pezzulo G. Active inference and learn-
ing. Neuroscienc e & Biobehavior al Reviews. 2016; 68:862– 79.
4. Friston K, FitzGerald T, Rigoli F, Schwarten beck P, Pezzulo G. Active Inferen ce: A Process Theory.
Neural Comp utation. 2017; 29(1):1–49 . https://doi.or g/10.1162/ NECO_a_0 0912 PMID: 27870614
5. Friston K, Schwarten beck P, Fitzgeral d T, Moutous sis M, Behrens T, Dolan R. The anatomy of choice:
active inference and agency. Front Hum Neurosci. 2013; 7(598). https://d oi.org/10.338 9/fnhum.20 13.
00598 PMID: 240930 15
6. Friston K, Schwarten beck P, FitzGerald T, Moutouss is M, Behrens T, Dolan RJ. The anatomy of choice:
dopamin e and decision-m aking. Philoso phical Transaction s of the Royal Society B: Biological Sci-
ences. 2014; 369(1655) :201304 81. https://doi.or g/10.109 8/rstb.2013. 0481 PMID: 25267823
7. Friston KJ, Parr T, de Vries B. The graphic al brain: Belief propagation and active inference. Netw Neu-
rosci. 2017; 1(4):381–4 14. https://doi. org/10.1162/N ETN_a_00 018 PMID: 29417960
8. Friston KJ, Rosch R, Parr T, Price C, Bowman H. Deep temporal models and active inference. Neuro-
science & Biobehavior al Reviews. 2017; 77:388–402. https://doi. org/10.1016/j .neubiorev .2017.04.009
PMID: 284164 14
9. Hesp C, Smith R, Parr T, Allen M, Friston KJ, Ramstea d MJD. Deeply Felt Affect: The Emergenc e of
Valence in Deep Active Inference. Neural Comput. 2021; 33(2):398– 446. https://doi.or g/10.116 2/neco_
a_01341 PMID: 33253028
10. Kaplan R, Friston KJ. Planning and navigation as active inference. Biological Cybernetic s. 2018; 112
(4):323–43 . https://doi. org/10.1007/s 00422-018- 0753-2 PMID: 29572721
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 30 / 34
11. Mirza MB, Adams RA, Friston K, Parr T. Introduci ng a Bayesian model of selective attention based on
active inference. Scientifi c Reports. 2019; 9(1):13915 . https://doi.or g/10.103 8/s41598-019- 50138-8
PMID: 315587 46
12. Mirza MB, Adams RA, Mathys CD, Friston KJ. Scene Construction, Visual Foraging, and Active Infer-
ence. Frontiers in computation al neuroscie nce. 2016; 10:56-. https://doi.or g/10.338 9/fncom.2016 .
00056 PMID: 273788 99
13. Neacsu V, Conve rtino L, Friston KJ. Synthetic Spatial Foraging With Active Inference in a Geocaching
Task. Frontiers in Neuroscience. 2022;16 .
14. Parr T, Friston KJ. Uncertainty, epistemics and active inferenc e. Journal of the Royal Society Interface.
2017; 14(136):20 170376 . https://doi.or g/10.109 8/rsif.2017.03 76 PMID: 29167370
15. Parr T, Friston KJ. Active inference and the anatomy of oculomotio n. Neuropsyc hologia. 2018;
111:334–43 . https://doi.or g/10.1016/ j.neuropsyc hologia.2018 .01.041 PMID: 29407941
16. Schwarten beck P, FitzGerald TH, Mathys C, Dolan R, Friston K. The Dopaminer gic Midbrain Encode s
the Expected Certainty about Desired Outcomes . Cereb Cortex. 2015; 25(10):343 4–45. https://doi.or g/
10.1093/ cercor/bhu159 PMID: 25056572
17. Seth AK, Friston KJ. Active interoceptiv e inference and the emotiona l brain. Philosop hical Transaction s
of the Royal Society B: Biological Sciences. 2016; 371(1708) :20160007. https:// doi.org/10.10 98/rstb.
2016.0007 PMID: 28080966
18. Friston KJ, Parr T, Zeidman P. Bayesian model reduction. arXiv: Methodol ogy. 2018.
19. Smith R, Schwarten beck P, Parr T, Friston KJ. An Active Inference Approac h to Modeling Structur e
Learning: Concept Learning as an Example Case. Frontiers in Computa tional Neuroscience. 2020;
14:41. https:// doi.org/10.33 89/fncom. 2020.00041 PMID: 325086 11
20. Bruner JS, Goodnow JJ, Austin GA. A study of thinking. Oxford , England: John Wiley and Sons; 1956.
xi, 330–xi, p.
21. Geeraerts D. Prototype theory. Cognitive linguistics: Basic readings. 2006; 34:141–65.
22. Goodman ND, Tenenbau m JB, Feldman J, Griffiths TL. A rational analysis of rule-based concept
learning. Cogn Sci. 2008; 32(1):108– 54. https:// doi.org/10 .1080/03640 2107018020 71 PMID:
2163533 3
23. Rouder JN, Ratcliff R. Comparing categorizat ion models. J Exp Psychol Gen. 2004; 133(1):63– 82.
https://doi.or g/10.103 7/0096-3445 .133.1.63 PMID: 14979752
24. Barsalou LW. Ad hoc categorie s. Memory & Cognition . 1983; 11(3):211– 27. https://doi.or g/10.3758/
bf03196968 PMID: 6621337
25. Blei DM, Griffiths TL, Jordan MI, Tenenbaum JB, editors. Hierarchic al topic models and the nested Chi-
nese restaur ant process. NIPS; 2003.
26. Gershm an SJ, Blei DM. A tutorial on Bayesian nonparamet ric models. Journal of Mathematic al Psy-
chology. 2012; 56(1):1–12 .
27. Griffiths TL, Sanborn AN, Canini KR, Navarro DJ, Tenenb aum JB. Nonparame tric Bayesian models of
categoriza tion. Formal approaches in categori zation. 2011:173–98.
28. Stoianov I, Genovesio A, Pezzulo G. Prefrontal goal codes emerge as latent states in probabilistic value
learning. Journal of Cognitiv e Neuroscience. 2016; 28(1):140– 57. https://doi.or g/10.116 2/jocn_a_
00886 PMID: 264392 67
29. Collins AG, Frank MJ. Cognitive control over learning: creating, clusterin g, and generalizi ng task-set
structure . Psychological review. 2013; 120(1): 190. https://doi.or g/10.1037/a 0030852 PMID:
2335678 0
30. McNichol as PD. Model-bas ed clustering. Journal of Classifica tion. 2016; 33(3):331– 73.
31. Salakhut dinov R, Tenenbaum JB, Torralba A. Learning with hierarchical- deep models. IEEE transac-
tions on pattern analysis and machine intelligence. 2012; 35(8):1958 –71.
32. Mnih V, Kavukcuogl u K, Silver D, Rusu AA, Veness J, Bellemare MG, et al. Human-lev el control throug h
deep reinforcem ent learning. nature. 2015; 518(7540) :529–33. https://doi.or g/10.103 8/nature1423 6
PMID: 257196 70
33. Fourmen t M, Magee AF, Whidde n C, Bilge A, Matsen FA IV, Minin VN. 19 dubious ways to compute the
marginal likeliho od of a phylogenet ic tree topology. Systematic biology. 2020; 69(2):209– 20. https:/ /doi.
org/10.1093/ sysbio/syz04 6 PMID: 315049 98
34. Penny WD. Comp aring dynamic causal models using AIC, BIC and free energy. Neuroimag e. 2012; 59
(1):319–30 . https://doi. org/10.1016/j .neuroim age.2011.07.0 39 PMID: 21864690
35. Barron HC, Auksztulewic z R, Friston K. Prediction and memory: A predictive coding account. Prog Neu-
robiol. 2020; 192:101 821. https://doi.or g/10.101 6/j.pneurobi o.2020.10 1821 PMID: 32446883
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 31 / 34
36. Bowman CR, Zeithamov a D. Abstract Memory Representa tions in the Ventrom edial Prefrontal Cortex
and Hippocam pus Support Concept Generaliza tion. The Journal of Neurosc ience. 2018; 38(10):260 5–
14. https://doi. org/10.1523/J NEUROSC I.2811-17.201 8 PMID: 29437891
37. Hutter SA, Wilson AI. A Novel Role for the Hippocam pus in Category Learning. The Journal of neurosci-
ence: the official journal of the Society for Neurosc ience. 2018; 38(31):680 3–5. https://doi.or g/10.1523 /
JNEUROS CI.1085-1 8.2018 PMID: 309956 10
38. Love BC, Gureckis TM. Models in search of a brain. Cogn Affect Behav Neurosci. 2007; 7(2):90–10 8.
https://doi.or g/10.375 8/cabn.7.2.9 0 PMID: 176723 81
39. Love BC, Medin DL, Gurecki s TM. SUSTAIN: a network model of category learning. Psychol Rev.
2004; 111(2):309 –32. https://doi.or g/10.1037/ 0033-295X .111.2.309 PMID: 15065912
40. Mack ML, Love BC, Preston AR. Building concepts one episode at a time: The hippoca mpus and con-
cept formation. Neurosc i Lett. 2018; 680:31– 8. https://doi.or g/10.101 6/j.neulet.2017 .07.061 PMID:
28801273
41. Mack ML, Love BC, Preston AR. Dynamic updating of hippoca mpal object representatio ns reflects new
conceptual knowledge. Procee dings of the National Academy of Sciences . 2016; 113(46):13 203–8.
https://doi.or g/10.107 3/pnas.16 14048113 PMID: 278033 20
42. Mack ML, Preston AR, Love BC. Ventromed ial prefront al cortex compress ion during concept learning.
Nature Communic ations. 2020; 11(1):46. https://doi.or g/10.103 8/s41467-019- 13930-8 PMID:
31911628
43. McClelland JL, McNaugh ton BL, O’Reilly RC. Why there are complem entary learning systems in the
hippoca mpus and neocortex: insights from the successes and failures of connection ist models of learn-
ing and memory. Psych ol Rev. 1995; 102(3):419 –57. https://doi. org/10.1037/0 033-295X.1 02.3.419
PMID: 762445 5
44. Mok RM, Love BC. A non-spat ial account of place and grid cells based on clustering models of concept
learning. Nature Communic ations. 2019; 10(1):5685 . https://doi.or g/10.103 8/s41467-019- 13760-8
PMID: 318317 49
45. Zeithamova D, Mack ML, Braunlich K, Davis T, Seger CA, van Kesteren MTR, et al. Brain Mechanis ms
of Concept Learning. J Neurosci. 2019; 39(42):82 59–66. https://doi.or g/10.152 3/JNEURO SCI.1166-
19.2019 PMID: 31619495
46. Eichenbaum H. Prefrontal– hippocamp al interactions in episodic memory. Nature Reviews Neurosci-
ence. 2017; 18(9):547– 58. https://doi. org/10.1038/n rn.2017. 74 PMID: 28655882
47. Gruber MJ, Hsieh L-T, Staresina BP, Elger CE, Fell J, Axmacher N, et al. Theta phase synchronizat ion
between the human hippocamp us and prefrontal cortex increases during encoding of unexpected infor-
mation: a case study. Journal of Cognitive Neuroscien ce. 2018; 30(11):164 6–56. https://doi.or g/10.
1162/jocn _a_01302 PMID: 299527 00
48. Rubin RD, Schwarb H, Lucas HD, Dulas MR, Cohen NJ. Dynamic hippoca mpal and prefront al contribu -
tions to memory processes and repres entations blur the bounda ries of traditional cognitive domains.
Brain sciences. 2017; 7(7):82. https://doi.or g/10.3390 /brainsci707008 2 PMID: 28704928
49. Weisberg RW. On the “Demystifi cation” of Insight: A Critique of Neuroimag ing Studie s of Insight. Crea-
tivity Research Journal. 2013; 25(1):1–14 .
50. Knoblich G, Ohlsson S, Raney GE. An eye movem ent study of insight problem solving. Mem Cognit.
2001; 29(7):1000 –9. https://doi.or g/10.3758/ bf03195762 PMID: 118207 44
51. MacGre gor JN, Ormerod TC, Chronicle EP. Information processing and insight: a process model of per-
formance on the nine-dot and related problem s. Journal of experimen tal psychology Learning, memory ,
and cognition . 2001; 27(1):176– 201. PMID: 11204097
52. Jones G. Testing two cognitive theories of insight. Journal of experime ntal psychology Learning, mem-
ory, and cognition . 2003; 29(5):1017 –27. https://doi.or g/10.103 7/0278-7393 .29.5.101 7 PMID:
14516232
53. Mai XQ, Luo J, Wu JH, Luo YJ. "Aha!" effects in a guessing riddle task: an event-related potentia l study.
Hum Brain Mapp. 2004; 22(4):261– 70. https://doi.o rg/10.1002/hb m.20030 PMID: 152021 04
54. Bowden EM, Jung-Bee man M, Fleck J, Kounios J. New approac hes to demystif ying insight. Trends
Cogn Sci. 2005; 9(7):322–8 . https://doi.or g/10.1016/j. tics.2005.05 .012 PMID: 159537 56
55. Friston KJ, Lin M, Frith CD, Pezzulo G, Hobson JA, Ondobak a S. Active Inferen ce, Curiosity and
Insight. Neural Comput ation. 2017; 29(10):263 3–83. https://doi.or g/10.1162/ neco_a_00999 PMID:
28777724
56. Kounios J, Fleck JI, Green DL, Payne L, Stevenson JL, Bowden EM, et al. The origins of insight in rest-
ing-state brain activity . Neuropsyc hologia. 2008; 46(1):281– 91. https://d oi.org/10.101 6/j.
neuropsy chologia.200 7.07.013 PMID: 177652 73
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 32 / 34
57. Kounios J, Frymiare JL, Bowden EM, Fleck JI, Subramaniam K, Parrish TB, et al. The Prepared Mind:
Neural Activity Prior to Problem Presentation Predicts Subsequent Solution by Sudden Insight. Psycho-
logical Science. 2006; 17(10):882 –90. https://doi.or g/10.1111 /j.1467-9280 .2006.017 98.x PMID:
17100789
58. Shen W, Tong Y, Li F, Yuan Y, Hommel B, Liu C, et al. Tracking the neurodynam ics of insight: A meta-
analysis of neuroim aging studies. Biological Psychology. 2018; 138:189–98. https://doi.or g/10.1016/j.
biopsycho.20 18.08.01 8 PMID: 301650 82
59. Tik M, Sladky R, Luft CDB, Willinger D, Hoffmann A, Banissy MJ, et al. Ultra-high-fie ld fMRI insights on
insight: Neural correlates of the Aha!-mo ment. Huma n Brain Mapping . 2018; 39(8):3241 –52. https://doi.
org/10.1002/ hbm.24073 PMID: 29665228
60. Luo J, Niki K. Function of hippoca mpus in “insight” of problem solving. Hippocam pus. 2003; 13(3):316–
23. https://doi. org/10.1002/h ipo.1006 9 PMID: 12722972
61. Adams RA, Moutous sis M, Nour MM, Dahoun T, Lewis D, Illingwor th B, et al. Variability in Action Selec-
tion Relates to Striatal Dopam ine 2/3 Receptor Availability in Huma ns: A PET Neuroimag ing Study
Using Reinforceme nt Learning and Active Inferen ce Models. Cerebra l Cortex . 2020; 30(6):3573 –89.
https://doi.or g/10.109 3/cercor/b hz327 PMID: 320832 97
62. Brown TH, Zhao Y, Leung V. Hebbian Plasticity. In: Squire LR, editor. Encycloped ia of Neuroscien ce.
Oxford: Academic Press; 2009. p. 1049–56.
63. Tononi G, Cirelli C. Sleep function and synaptic homeostas is. Sleep Med Rev. 2006; 10(1):49–6 2.
https://doi.or g/10.101 6/j.smrv.2005 .05.002 PMID: 16376591
64. Toutounj i H, Pipa G. Spatiotem poral computation s of an excitable and plastic brain: neuronal plasticity
leads to noise-robust and noise-con structive computat ions. PLoS Comput Biol. 2014; 10(3):e100 3512.
https://doi.or g/10.137 1/journal.pcbi .100351 2 PMID: 24651447
65. Hohwy J. The Self-Ev idencing Brain. Nou ˆ s. 2016; 50(2):259– 85.
66. Constantino SM, Daw ND. Learning the opportunity cost of time in a patch-foragin g task. Cogn Affect
Behav Neurosc i. 2015; 15(4):837– 53. https://d oi.org/10.375 8/s13415 -015-0350 -y PMID: 2591700 0
67. Constant A, Ramstea d MJD, Veissière SPL, Friston K. Regimes of Expectation s: An Active Inference
Model of Social Conformity and Human Decision Making. Frontiers in Psych ology. 2019; 10(679).
https://doi.or g/10.338 9/fpsyg.2019 .00679 PMID: 30988668
68. Smith R, Parr T, Friston KJ. Simulati ng emotions: An active inference model of emotiona l state infer-
ence and emotion concept learning. Frontie rs in psychology. 2019; 10:2844. https://doi. org/10.3389/
fpsyg.2019.0 2844 PMID: 31920873
69. Winn J, Bishop CM. Variationa l messag e passing. Journal of Machine Learning Research. 2005;
6:661–94.
70. Friston K, Penny W. Post hoc Bayesian model selection. Neuroimag e. 2011; 56(4):2089 –99. https:// doi.
org/10.1016/ j.neuroimage.2 011.03.0 62 PMID: 214591 50
71. George D, Hawkins J. Towards a mathematic al theory of cortical micro-circu its. PLoS Comp ut Biol.
2009; 5(10):e100 0532. https://doi.or g/10.1371/ journal.pcbi.1 000532 PMID: 19816557
72. Haxby JV, Grady CL, Horwitz B, Ungerle ider LG, Mishki n M, Carson RE, et al. Dissociati on of object
and spatial visual processing pathway s in human extrastria te cortex. Proceed ings of the National Acad-
emy of Sciences. 1991; 88(5):162 1–5. https://doi.or g/10.107 3/pnas.88.5 .1621 PMID: 2000370
73. Haxby JV, Horwitz B, Ungerle ider LG, Maisog JM, Pietrini P, Grady CL. The functional organizat ion of
human extrastriate cortex: a PET-rC BF study of selective attention to faces and locations. Journal of
neuroscie nce. 1994; 14(11):633 6–53. https://doi.or g/10.152 3/JNEURO SCI.14-11 -06336.1994 PMID:
7965040
74. Spiegler BJ, Mishkin M. Evidence for the sequentia l particip ation of inferior temporal cortex and amyg-
dala in the acquisition of stimulus -reward associations. Behavio ural Brain Resea rch. 1981; 3(3):303–
17. https://doi. org/10.1016/0 166-4328( 81)90002- 4 PMID: 730638 5
75. Miller JF, Neufang M, Solway A, Brandt A, Trippel M, Mader I, et al. Neural activity in human hippoca m-
pal formation reveals the spatial context of retrieved memories . Science. 2013; 342(6162) :1111–4.
https://doi.or g/10.112 6/science.124 4056 PMID: 24288336
76. Rudy JW, Barrientos RM, O’Reilly RC. Hippocam pal formation supports condition ing to memory of a
context. Behavio ral Neurosc ience. 2002; 116(4):530 –8. https://doi.or g/10.103 7//0735-7044 .116.4.53 0
PMID: 121489 21
77. Andersen RA. Encoding of intention and spatial location in the posterior pariet al cortex. Cerebral Cortex.
1995; 5(5):457–6 9. https://doi.or g/10.1093/c ercor/5.5.4 57 PMID: 8547792
78. Colby CL, Duhamel J-R. Spatial representatio ns for action in parietal cortex. Cognitive Brain Research.
1996; 5(1–2):105 –15. https://doi.or g/10.1016/ s0926-6410( 96)00046- 8 PMID: 9049076
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 33 / 34
79. Silver MA, Kastner S. Topographic maps in human frontal and parietal cortex. Trends in cognitive sci-
ences. 2009; 13(11):488 –95. https://doi.or g/10.1016/ j.tics.2009.08 .005 PMID: 19758835
80. MacKay DJC. Informatio n Theory, Inferen ce and Learning Algorithm s. Cambridge : Cambridge Univer-
sity Press; 2003.
81. Hobson JA, Friston KJ. Waking and dreamin g consciousnes s: neurobiolo gical and functional consider-
ations. Prog Neurobi ol. 2012; 98(1):82–9 8. https://doi.or g/10.1016/ j.pneurobio .2012.05.00 3 PMID:
22609044
82. Pezzulo G, Zorzi M, Corbetta M. The secret life of predicti ve brains: what’s spontane ous activity for?
Trends in Cognitiv e Sciences. 2021. https://doi.or g/10.101 6/j.tics.2021.0 5.007 PMID: 34144895
83. Hinton GE, Dayan P, Frey BJ, Neal RM. The" wake-sleep" algorithm for unsuper vised neural networks .
Science. 1995; 268(5214) :1158–61 . https://doi.or g/10.1126/ science.7761 831 PMID: 776183 1
84. Finlayson NJ, Neacsu V, Schwarzkopf DS. Spatial heterogenei ty in bistable figure-gro und perception . i-
Perception. 2020; 11(5):2041 66952096112 0. https://doi.or g/10.1177/2 04166952096 1120 PMID:
33194167
85. Tschantz A, Seth AK, Buckley CL. Learning action-o riented models through active inference. PLoS
computation al biology. 2020; 16(4):e100 7805. https:/ /doi.org/10.13 71/journal.p cbi.10078 05 PMID:
32324758
PLOS ONE
Structur e learning enhances concept formation in synthetic Active Inferen ce agents
PLOS ONE | https://doi.or g/10.137 1/journal.po ne.02771 99 November 14, 2022 34 / 34