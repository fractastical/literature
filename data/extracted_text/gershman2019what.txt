NB}
DT
ORIGINAL ARTICLE
Theory
What does the free energy principle tell us about
the brain?
SamuelJ.Gershman1∗
1DepartmentofPsychologyandCenterfor
Thefreeenergyprinciplehasbeenproposedasaunifying
BrainScience,HarvardUniversity,
Cambridge,MA,02138,USA accountofbrainfunction.Itiscloselyrelated,andinsome
casessubsumes,earlierunifyingideassuchasBayesianin-
Correspondence
NorthwestLaboratories,52OxfordSt., ference,predictivecoding,andactivelearning. Thisarti-
Room295.05,Cambridge,MA,02138,USA
cleclarifiestheseconnections,teasingapartdistinctiveand
Email:gershman@fas.harvard.edu
sharedpredictions.
Fundinginformation
Thisworkwassupportedbyaresearch
KEYWORDS
fellowshipfromtheAlfredP.Sloan
Foundation. Bayesianbrain,decisiontheory,variationalinference,predictive
coding
1 | INTRODUCTION
Thefreeenergyprinciple(FEP)states,inanutshell,thatthebrainseekstominimizesurprise[1].Itisarguablythemost
ambitioustheoryofthebrainavailabletoday,claimingtosubsumemanyotherimportantideas,suchaspredictivecoding,
efficientcoding,Bayesianinference,andoptimalcontroltheory.However,itispreciselythisgeneralitythatraisesa
concern:whatexactlydoesFEPpredict,andwhatdoesitnotpredict?Addressingthisconcernisnoteasy,becausethe
assumptionsunderlyingapplicationsofFEParemalleable(e.g.,differentapplicationsusedifferentgenerativemodels,
differentalgorithmicapproximations,anddifferentneuralimplementations).Moreover,someoftheseassumptionsare
sharedwithothertheories,andsomeareidiosyncratic;someassumptionsarecentraltothetheory,andothersaread
hocormadeforanalyticalconvenience.
ThisarticlesystematicallydeconstructstheassumptionsunderlyingFEP,withthegoalofidentifyingwhatits
distinctivetheoreticalclaimsare.Aswillbecomeclear,FEPdoesnothaveafixedsetofdistinctiveclaims.Rather,it
makesdifferentclaimsunderdifferentsetsofassumptions.Thisisnotnecessarilyabadthing,providedwecanverify
theseassumptionsinanyparticularapplicationandthusrenderthetheoreticalassumptionsfalsifiable.
1
9102
tcO
2
]CN.oib-q[
5v54970.1091:viXra
2 SAMUELJ.GERSHMAN
Beforeproceeding,wemustaddresstwoqualmswiththisdeconstructiveapproach.SomeproponentsofFEP
mightreasonablyarguethatidentifyingdistinctivetheoreticalclaimsispointless;thewholepointofaunifyingtheory
istounifyclaims,notdistinguishthem. However,thefundamentalissuehereisnotwhetheronetheoryisbetter
thananother,buthowtoassigncreditandblametodifferenttheoreticalassumptions.IfFEPfailstoaccountforthe
data,isthatattributabletotheassumptionthatthebrainisBayesian,oraparticularalgorithmicimplementationof
Bayesianinference,orparticularassumptionsabouttheprobabilisticmodel?Onlybyansweringsuchquestionscanwe
understandthesuccessesandfailuresofaunifyingtheory,devisesuitabletestsofitsassumptions,andidentifywaysto
improvethetheory.
AnotherqualmwiththisapproachisbasedontheargumentthatFEPisnotatheoryatall,inthesensethatatheory
constitutesasetoffalsifiableclaimsaboutempiricalphenomena.Whatmakesitaprinciple,ratherthanatheory,isthat
itconstitutesasetofself-consistentstatementsinaformalmathematicalsystem.Inthissense,aprinciplecannotbe
falsifiedthroughthestudyofempiricalphenomena.Atheorydesignatescorrespondencesbetweenformalstatements
andempiricalphenomena,andthuscanbefalsifiedifthetheorymakesincorrectpredictionsonthebasisofthese
correspondences.Viewedinthisway,FEPisunobjectionable:itsmathematicalsoundnessissufficientdemonstration
ofitscredentialsasaprinciple.Herewewillbeconcernedwithitscredentialsasatheory,andthereforewewillpay
particularattentiontospecificimplementations(processmodels).
2 | THE BAYESIAN BRAIN HYPOTHESIS
AsapreludetoFEP,itwillbehelpfultobrieflydescribetheBayesianbrainhypothesis[2,3,4],whichcanbeexpressedin
termsthataremorefamiliartoneuroscientists,andisinfactequivalenttoFEPundercertainconditions(aselaborated
inthenextsection).ThefirstclaimoftheBayesianbrainhypothesisisthatthebrainisequippedwithaninternal(or
“generative”)modeloftheenvironment,whichspecifiesa“recipe”forgeneratingsensoryobservations(denotedbyo)
fromhiddenstates(denotedbys).Thisinternalmodelmaynotberepresentedexplicitlyanywhereinthebrain;the
claimisthatthebraincomputes“asif”ithadaninternalmodel.InorderfortheBayesianbrainhypothesistohaveany
predictivepower,itisnecessarytomakespecificassumptionsaboutthestructureoftheinternalmodel.
Therearetwocomponentsoftheinternalmodelthatneedtobespecified. First,hiddenvariablesaredrawn
fromapriordistribution,p(s).Forexample,thehiddenstatemightbetheorientationofalinesegmentonthesurface
ofanobject,andthepriormightbeadistributionthatfavorscardinaloverobliqueorientations[5]. Second,the
sensoryobservationsaredrawnfromanobservationdistributionconditionalonthehiddenstate,p(o|s).Forexample,
thehiddenlineorientationisprojectedontotheretinaandthenencodedbythefiringofretinalganglioncells.This
encodingprocessmightbenoisy(duetostochasticityofneuralfiring)orambiguous(duetotheopticalprojection
ofthreedimensionsontothetwo-dimensionalretinalimage),suchthatdifferentsettingsofthehiddenstatecould
plausibly“explain”theobservationstovaryingdegrees.Thesedegreesofplausibilityarequantifiedbythelikelihood,the
probabilityoftheobservationsundertheobservationdistributiongivenahypotheticalsettingofthehiddenstate.
ThesecondclaimoftheBayesianbrainhypothesisisthatthethepriorandthelikelihoodarecombinedtoinferthe
hiddenstategiventheobservations,asstipulatedbyBayes’rule:
p(o|s)p(s)
p(s|o)= , (1)
p(o)
wherep(s|o)istheposteriordistributionandp(o)=(cid:205) p(o|s)p(s)isthemarginallikelihood(forcontinuousstates,the
s
summationisreplacedwithintegration).WecanthinkofBayes’ruleas“inverting”theinternalmodeltocomputea
SAMUELJ.GERSHMAN 3
beliefaboutthehiddenstateoftheenvironmentgiventheobservations.
TheBayesianbrainhypothesiscanbenaturallyextendedtosettingswhereanagentcaninfluenceitsobservations
bytakingactionsaccordingtoapolicyπ,whichisamappingfromobservationstoadistributionoveractions.Inthe
simplestvariant,anagentchoosesapolicythatmaximizesinformationgain:
(cid:213)
I(π)= p(o|π)D[p(s|o,π)||p(s|π)], (2)
o
whereonowdenotesafutureobservation,andDdenotestheKullback-Leibler(KL)divergence(alsoknownasrelative
entropy):
(cid:213) p(s|o,π)
D[p(s|o,π)||p(s|o)]= p(s|o,π)log . (3)
p(s|π)
s
TheexpressionforI(π)isequivalentto“Bayesiansurprise”[6],andtothemutualinformationbetweens ando
conditionalonπ[7,8].Informationmaximizationhasbeenstudiedextensivelyinthecognitivepsychologyliterature
[9,10,11]. Moregenerally,informationmaximizationcanbeunderstoodasaformofactivelearningthathasbeen
studiedextensivelyinthemachinelearningandstatisticsliterature[12].
InformationgainmaximizationisaspecialcaseofBayesiandecisiontheory,wheretheutilityu(o)=D[p(s|o,π)||p(s|π)]
ofanobservationcorrespondstoinformationgain.Iftheobservationsarevalenced(rewardsorpunishments),then
utilitiesmayreflecttheirgoodnesstotheagent,whoseekstomaximizetheexpectedutility:
(cid:213)
(cid:197)[u(o)|π]= p(o|π)u(o). (4)
o
Thisanalysiscanbegeneralizedtosequentialdecisionproblems[see13],whereanagent’sactionsandobservations
unfoldovertime.Typically,thegoalinsequentialdecisionproblemsistomaximizediscountedcumulativeutility(return):
R(o)=u(o1)+γu(o2)+γ 2 u(o3)+··· (5)
wherewehaveintroducedasubscriptdenotingtime-stepandtheboldnotationo=[o1,o2,...]denotesthetime-series
ofobservations.Thediscountfactorγdown-weightsfutureutilityexponentiallyasafunctionoftemporaldistance.The
expectedreturnundertheposterioristhendefinedanalogouslytoexpectedutility:
(cid:213)
(cid:197)[R(o)|π]= p(o|π)R(o). (6)
o
Insequentialdecisionproblems,anagentneedstotradeoffgatheringinformationtoreduceuncertainty(exploration)
andtakingactionsthatyieldimmediatereward(exploitation).Thismeansthatpreferencesforinformationwillarise
instrumentallyinthesequentialdecisionsetting;theyneednotbebuiltexplicitlyintotheutilityfunction.
Thereareseveralpointsworthnotingherebeforemovingon:
• AlthoughtheBayesianbrainhypothesishasreceivedconsiderablesupport,therearenumerousempiricaldeviations
fromitsclaims(e.g.,[14,15],)someofwhichmayberationalizedbyconsideringapproximateinferencealgorithms
[16].Thevariationalalgorithmsweconsiderbelowareexamplesofsuchapproximations.Wewillnotevaluate
theempiricalvalidityofthe(approximate)Bayesianbrainhypothesis,focusinginsteadonmoreconceptualissues
4 SAMUELJ.GERSHMAN
relatedtothefreeenergyprinciple.
• TheBayesianbrainhypothesisdoesnotmakeanyspecificclaimsaboutthepriorsandlikelihoodsofanindividual.
Rather,thecentralclaimconcernsconsistencyofbeliefs:aBayesianagentwillconvertpriorbeliefsintoposterior
beliefsinaccordancewithBayes’rule.
• TheBayesianbrainhypothesisabstractsawayfromanyparticularalgorithmicorneuralclaims: itispurelya
“computational-level”hypothesis.Allalgorithmsthatcomputetheposteriorexactlygiveequivalentpredictions
withregardtothecentralclaimsoftheBayesianbrainhypothesis,andlikewiseanyneuralimplementationwill
giveequivalentpredictions.Theseequivalencesdonothold,however,whenweconsiderapproximateinference
schemes,whichmaysystematicallydeviatefromtheBayesianideal.Wewillreturntothispointbelow.
3 | THE UNRESTRICTED FREE ENERGY PRINCIPLE IS BAYESIAN INFERENCE
ThebasicideaoftheFEPistoconvertBayesianinferenceintoanoptimizationproblem(see[17]foratutorialintro-
duction).Thisideawasfirstdevelopedinphysics,andlaterinmachinelearning,tohandlecomputationallyintractable
inferenceproblems.Thekeyalgorithmictrick,aswewillsee,istorestricttheoptimizationprobleminsuchawaythatit
isnotsearchingoverallpossibleposteriordistributions.
AssumewehaveavailableafamilyofdistributionsQ(discussedfurtherinthenextsection),andwecanchooseone
distributionq ∈ Qtoapproximatep(s|o).Thisleadstothefollowing“variational”optimizationproblem:
q∗(s)=argminD[q(s)||p(s|o)]. (7)
q(s)
TheKLdivergenceis0whenq(s)=p(s|o).Thus,ifp(s|o)iscontainedinthevariationalfamilyQ,thenthesolution
oftheoptimizationproblemyieldstheexactposterior:q∗(s)=p(s|o).Thisholdstruewhenthevariationalfamilyis
unrestricted(i.e.,containsallpossibledistributionswithsupportonthehypothesisspace).
Algorithmically,thisoptimizationproblemisnotverypracticalbecausetocomputetheKLdivergenceweneed
accesstoq(s)—preciselytheproblemwearetryingtosolve!However,itturnsoutthatonecanreformulatethisproblem
inawaythatismorepractical,basedonthefollowingidentity:
logp(o)=D[q(s)||p(s|o)]−F[q(s)], (8)
whereF[q(s)]isthevariationalfreeenergy:
(cid:213) q(s)
F[q(s)]= q(s)log . (9)
p(o,s)
s
Thefreeenergyisequivalenttothenegativeoftheevidencelowerbound,themorecommonterminthemachinelearning
literature[18].
Notethatthefreeenergyonlyrequiresknowledgeofp(s|o)uptoanormalizingconstant,sincep(s|o)∝p(o|s)p(s).
Thisistypicallyunproblematic,sincewecanoftencomputethepriorp(s)andlikelihoodp(o|s)ofanyparticularstates.
Critically,theidentityaboveimpliesthatminimizingthefreeenergyisequivalenttominimizingKLdivergence,sincethe
twomustbalanceeachotherouttomatchthemarginallikelihood,whichisfixedasafunctionofq.Thus,minimizing
freeenergywhenthevariationalfamilyisunrestrictedisequivalenttoexactBayesianinference.
IfFEP=Bayes,thenwecannotdistinguishitspredictionsfromotherasymptoticallycorrectinferencealgorithms,
SAMUELJ.GERSHMAN 5
suchasMonteCarlosampling,exceptwhenthesealgorithmsarerestrictedinsomeway.MonteCarlomethodsmay,for
example,berestrictedintermsofthenumberofsamplestheygenerateorhowtheygeneratethesamples(e.g.,[19]).
Optimizationoffreeenergyistypicallyrestrictedbyplacingconstraintsonthevariationalfamily,aswediscussnext.
4 | RESTRICTING THE VARIATIONAL FAMILY
Ifthehypothesisspaceisvast,thensumming(orintegrating)overallpossiblehypothesestocomputethefreeenergy
willbeintractable.Thus,essentiallyallpracticalapplicationsoffreeenergyoptimizationmakeuseofarestrictiononQ
thatrenderstheoptimizationtractable(aswillbediscussedbelow).Theimportantpointforpresentpurposesisthat
aslongasthetrueposteriorisinQ,theoptimalq∗willbeequaltotheposterior.Thus,FEPinitsmostgeneralformis
indistinguishablefromBayesianinference.
PracticalapplicationsoffreeenergyoptimizationrestrictQinsomewaytomaketheproblemtractable.These
restrictionstypicallymeanthattheposteriorisnolongercontainedinQ,andthusthedistributionthatminimizesfree
energywilldeviatefromBayes-optimality:q∗(s)(cid:44)p(s|o).
Thewidelyused“mean-field”approximationassumesthattheposteriorfactorizesacrosscomponentsofs(i.e.,
dimensionsofthestatespace):
(cid:214)
q(s)= qi (si ). (10)
i
Forexample,ifI’mtryingtoinfertheposteriorovertheheightandweightofanindividualgiventheirgender,Icould
assumethattheposteriorfactorizesintoq(height|gender)andq(weight|gender).Becausethetrueposteriorrarely
factorizes,themean-fieldapproximationwillproducesystematicerrors.Forexample,ifthefactorizationisacrossa
sequenceofstates,theposteriormaybebiasedbytheorderofthedata.Intriguingly,theseerrorscanbediscernedin
humanbehavior[20,21].Ontheotherhand,themean-fieldapproximationmayworkwellinmanycases,whichiswhyit
iswidelyadoptedinmachinelearning.Thiseffectivenesscanrenderitdifficulttotestasaprocessmodel,becauseit
oftenmakessimilarpredictionstoexactBayesianinference.
Whensiscontinuous,anothercommonrestrictionistoassumethattheposteriorisGaussian[22],parametrized
byameanµandcovariancematrixΣ:
q(s)=N(s;µ,Σ). (11)
Theseparametersarethenchosentominimizethefreeenergy,typicallybygradientdescent.TheGaussianapproxima-
tioncanbemotivatedbythe“Bayesiancentrallimittheorem,”whichstatesthattheposteriorisapproximatelyGaussian
aroundthemodewhentheamountofdataislargerelativetothedimensionalityofs.Itcanalsobegeneralizedto
mixturesofGaussianstoapproximatemultimodalposteriors[23].
OnechallengefacingapplicationsoftheGaussianapproximationisthatthefreeenergyisnot,ingeneral,tractable
(exceptinthecasewheretheexactposteriorisGaussian). Todealwiththisissue,acommontechnique,knownas
theLaplaceapproximation,istouseasecond-orderTaylorseriesexpansionaroundtheposteriormode.Thisreplaces
thenonlinearfreeenergywithaquadraticfunction,renderingthefreeenergytractable.Thepricewepayforthis
approximationisthatwearenolongeroptimizingthefreeenergy,andwehavenoguaranteethatthiswillproduce
sensibleanswers,orevenconverge.Itturnsout,however,thattheLaplaceapproximationhasintriguingimplicationsfor
theneurobiologicalimplementationofBayesianinference.
6 SAMUELJ.GERSHMAN
5 | PREDICTIVE CODING
TheLaplaceapproximationcanbeusedtoderivearguablythemostinfluentialanddistinctiveaspectofFEP—predictive
coding,accordingtowhichfeedbackpathwaysconveypredictions,andfeedforwardpathwaysinthebrainconvey
predictionerrors(discrepanciesbetweendataandpredictions).Theideaofpredictivecodinghasalonghistoryin
signalprocessing[24],andwaspreviouslyproposedasatheoryofredundancyreduction(efficientcoding)inneural
signals[25].Fristonandcolleaguesshowedhowpredictivecodingcouldbederivedwithintheframeworkoffreeenergy
minimization[22,26,27],howitcouldbemappedontothestructureofbiologicallyrealisticmicrocircuits[28],andhow
itcouldbeappliedtomotorcontrol[29]andactionselectionmoregenerally(atopicwevisitinthenextsection).
Fristonandcolleaguesstartedfromthefollowingassumptions:
• Theinternal(generative)modelishierarchicallystructured,suchthathiddenstatesathigherlevelsgeneratehidden
statesatlowerlevels.
• Theapproximateposteriorfactorizesacrosshiddenstatedimensionswithinandbetweenlevelsoftheinternal
model(i.e.,themean-fieldapproximation).
• EachcomponentofthefactorizedposteriorismodeledasaGaussian.
TheythenusedtheLaplaceapproximationtoapproximatethefreeenergyandderiveupdaterulesforoptimization
basedongradientdescent.Theyshowedthatthisoptimizationschemecorrespondstoaformofpredictivecoding,
whichisfoundubiquitouslyintheengineeringliterature(e.g.,Kalmanfiltering).
ItisimportanttoemphasizethatpredictivecodingisnotagenericconsequenceofFEP,orevenofFEPwitha
specificapproximationfamily.Itisderivedfromacombinationofassumptionsabouttheinternalmodel(hierarchical
organization),theapproximationfamily(factorizedandGaussian),theapproximationofthefreeenergy(quadratic
aroundthemode),andtheoptimizationscheme(gradientdescent). Withalloftheseassumptionsinplace,FEP
doesmakeclaimsthatgobeyondthegeneralBayesianbrainhypothesis,andhavereceivedampleempiricalsupport
[30,31,32,33,34].Alternatively,someauthorshaveexploredvariantsofFEPthatdonotinvokepredictivecoding,or
combineitwithotherneuralmessagepassingschemes(e.g.,[35]).
6 | ACTIVE INFERENCE
Letusreturnnowtothesettinginwhichanagentcantakeactions(accordingtopolicyπ)toinfluenceitsobservations.
Inthissetting,FEPpositsthattheagentseekstominimizeexpectedfreeenergyunderfutureobservationsoandfuture
states[36]:
(cid:213) (cid:213) q(s|π) (cid:213) (cid:213)
p(o|s,π) q(s|π)log =− q(o|π)D[q(s|o,π)||q(s|π)]− q(o|π)logp(o|π), (12)
p(o,s|π)
o s o o
whereq(s|π)istheapproximatebeliefaboutfuturestatespriortoobservingo.Fristonandcolleaguesrefertothe
minimizationofexpectedfreeenergywithrespecttoactionsasactiveinference. Notethatherethelikelihoodis
stipulatedtobep(o|s,π)=q(o|s,π),andwehaveassumedthatthepredictiveposteriorq(s|o,π)≈p(s|o,π).
Whentheapproximateposteriorisexact,thefirsttermintheexpressionisthenegativeinformationgainandthe
SAMUELJ.GERSHMAN 7
secondtermistheentropyH[p(o|π)]ofthefutureobservationsconditionalonthepolicy:
(cid:213) (cid:213) p(s|π)
p(o|s,π) p(s|π)log =−I(π)+H[p(o|π)]. (13)
p(o,s|π)
o s
Ifinadditionobservationsaredeterministicfunctionsofthepolicy,thentheentropytermis0,andminimizingexpected
freeenergyisequivalenttomaximizinginformationgain.Thus,undercertainconditionsactiveinferenceisequivalent
totheinformationgainpolicystudiedinstandardBayesiantreatmentsofinformationacquisition[10]. Whenthe
observationsarestochasticandcanbeinterpretedasrewardoutcomes(seenextsection),activeinferenceinstantiatesa
formofrisk-sensitivecontrol,sinceactionsthatreduceoutcomevariabilitywillbefavored(see[36]formorediscussion).
Anotherwayofthinkingabouttheentropytermisthatitreflectsthe“codingcost”ofunpredictabledata,sinceentropy
isalowerboundontheaveragenumberofbitsneededtocommunicateobservationsviaasensorychannelwithoutloss
ofinformation[37].Thus,activeinferenceprefersactionsthatproduceobservationswhicharebothinformativeand
predictable.
Asintheprevioussections,wecanaskwhichaspectsofthisanalysisaregenericimplicationsoftheBayesian
brainhypothesis(withaninformationgainpolicy),andwhicharespecifictoFEP.WeshowedthatFEPisequivalentto
Bayesianinformationgainonlyunderthespecialcaseofanexactposterioranddeterministicoutcomesinthefuture.
Whenthedeterminismconstraintisrelaxed,informationgainandexpectedfreeenergywillbesubstantivelydifferent.
7 | PLANNING AS INFERENCE
Anumberofpapersonactiveinferencemakeanadditionalconceptualmove(e.g.,[38,39,36]),reinterpretingthe
entropytermasaformofextrinsicvalue,contrastingitwiththeepistemicvalueoftheinformationgainterm.Centralto
thisreinterpretationisthepostulatethattheutilityofanoutcomeisequaltoitslogpriorprobability,u(o)=logp(o|π),
usuallyreferredtoasitspriorpreference.(Notethatweareconditioningonthepolicyheretoemphasizethatthefree
energyisbeingcomputedforafixedpolicy.)Thisleadstoaformofplanningasinference[40,41],wherebyminimizing
freeenergyoptimizesacombinationofexpectedutility(extrinsicvalue)andinformationgain(epistemicvalue).
Atfirstglance,thisseemsratherodd;whyshouldutilitybeproportionaltoprobability?Undoubtedlytherearehigh
probabilityeventsthathavelowutility(e.g.,ifyouarebornintopovertythenlackingaccesstobasicgoodsmaybehighly
probable).However,notethatthisispotentiallyjustBayesiandecisiontheoryindisguise:aslongasI’mallowedto
chooseprobabilitiesthatareproportionaltoutilities,FEPwillcoincidewithBayesiandecisiontheory.Thecriticalstep
inthislogicistheassumptionthatevolutionhasequippeduswiththebeliefthatlowutilitystatesarelowprobability,
duetothefactthatifourancestorsspentalotoftimeinthosestatestheywouldbelesslikelytoreproduce.Whetheror
notthisisareasonableassumption,thetechnicalpointisthatplanningasinferencecanbeunderstoodasanotational
variantofBayesiandecisiontheory,providedtheutilitiesandprobabilitiescoincide(freeenergytheoriststypically
stipulatethattheycoincide).FEPcanmakedistinctivepredictionswhentheydon’tcoincide,orwhentheplanning
asinferencetransformationleadstodifferentalgorithmicapproximationsorneuralimplementations,providedthe
utilitiesandpriorpreferencescoincide(i.e.,effectively,replacingutilitywithpriorpreferences).
8 | CONCLUSIONS
Thereareseveraltake-homemessagesfromthisarticle:
8 SAMUELJ.GERSHMAN
• Forpassiveobservations(noactions),thepredictionsofFEPareindistinguishablefromthepredictionsofthe
Bayesianbrainhypothesiswhenthevariationalfamilyisunrestricted(i.e.,thewhentheexactposteriorisinthe
variationalfamily,andhenceminimizingfreeenergyisequivalenttoexactinference).
• PredictivecodingisnotagenericconsequenceofFEP;itarisesonlyundercertainrestrictionsofthevariational
familyandaspecificchoiceofoptimizationscheme.
• Intheactivesetting(observationscanbeinfluencedbyactions),activeinferenceisequivalenttoaninformation
gainpolicywhentheapproximateposteriorisexactandtheobservationsaredeterministicfunctionsofactions.
Whenobservationsarestochastic,activeinferenceinducesaformofrisk-aversionnotfoundintheinformation
gainpolicy.
• Whenutilitiesareinterpretedaslogprobabilities,FEPcorrespondstoaformofplanningasinference,aclass
ofalgorithmsforutilitymaximization.ThepredictionsofFEParedistinguishedfromutilitymaximizationwhen
utilitiesdon’tcorrespondtologprobabilities.1
• Whenutilitiesareinterpretedaspriorpreferences,FEPplacesvalueoninformationgain.Thisalsoarisesnaturally
inBayesiandecisiontheoryappliedtosequentialdecisionproblemsandhenceisnotadistinctiveprediction.
Thesetake-homemessagesdonotexhaustthesetofideasthathavebeenintroducedunderthebannerofFEP.For
example,FEPhasbeenofferedasafirst-principleaccountofself-organization[43]andecologicalnicheconstruction
[44].Wehavefocusedhereonissuesthataremorecentraltoneuroscience.
ThebroaderpointofthisarticleisthataunifyingtheorylikeFEPneedstobedeconstructedinordertobeproperly
evaluatedandcomparedtoalternativetheories. Byundertakingpartofthisdeconstruction,wehopetomakethe
elegantsynthesisofferedbyFEPmoreaccessibletothebroaderneurosciencecommunity.
| Acknowledgments
IamgratefultoBenVincent,MomchilTomov,ChrisSummerfield,GiovanniPezzulo,PeterBattaglia,JanDrugowitsch,
RaniMoran,YuqingHou,JaschaAchterberg,RobertRosenbaum,SabyaShivkumar,andNathanielDawforcomments
onanearlierdraftofthepaper.
REFERENCES
[1] FristonK.Thefree-energyprinciple:aunifiedbraintheory?NatureReviewsNeuroscience2010;11:127.
[2] LeeTS,MumfordD.HierarchicalBayesianinferenceinthevisualcortex.JOSAA2003;20:1434–1448.
[3] KnillDC,PougetA. TheBayesianbrain:theroleofuncertaintyinneuralcodingandcomputation. TRENDSinNeuro-
sciences2004;27:712–719.
[4] DoyaK,IshiiS,PougetA,RaoRP.BayesianBrain:ProbabilisticApproachestoNeuralCoding.MITPress;2007.
[5] GirshickAR,LandyMS,SimoncelliEP.Cardinalrules:visualorientationperceptionreflectsknowledgeofenvironmental
statistics.NatureNeuroscience2011;14:926.
[6] IttiL,BaldiP.Bayesiansurpriseattractshumanattention.VisionResearch2009;49:1295–1306.
[7] CoverTM,ThomasJA.ElementsofInformationTheory.JohnWiley&Sons;1991.
1Technically,utilitiescanalwaysbeformulatedaslogprobabilities.Butitisanempiricalquestionwhethersubjectivebeliefsandpreferencesdisclosedby
behaviorcoincide[42].
SAMUELJ.GERSHMAN 9
[8] LindleyDV. Onameasureoftheinformationprovidedbyanexperiment. TheAnnalsofMathematicalStatistics
1956;27:986–1005.
[9] OaksfordM,ChaterN. Arationalanalysisoftheselectiontaskasoptimaldataselection. PsychologicalReview
1994;101:608–631.
[10] NelsonJD.Findingusefulquestions:onBayesiandiagnosticity,probability,impact,andinformationgain.Psychological
Review2005;112:979–999.
[11] TsividisP,GershmanS,TenenbaumJ,SchulzL.Informationselectioninnoisyenvironmentswithlargeactionspaces.In:
ProceedingsoftheAnnualMeetingoftheCognitiveScienceSociety,vol.36;2014..
[12] SettlesB.Activelearning.SynthesisLecturesonArtificialIntelligenceandMachineLearning2012;6:1–114.
[13] DayanP,DawND. Decisiontheory,reinforcementlearning,andthebrain. Cognitive,Affective,&BehavioralNeuro-
science2008;8:429–453.
[14] SoltaniA,KhorsandP,GuoC,FarashahiS,LiuJ. Neuralsubstratesofcognitivebiasesduringprobabilisticinference.
NatureCommunications2016;7:11393.
[15] RahnevD,DenisonRN.SuboptimalityinPerceptualDecisionMaking.BehavioralandBrainSciences2018;p.1–107.
[16] GershmanSJ,HorvitzEJ,TenenbaumJB. Computationalrationality:Aconvergingparadigmforintelligenceinbrains,
minds,andmachines.Science2015;349:273–278.
[17] BogaczR. Atutorialonthefree-energyframeworkformodellingperceptionandlearning. JournalofMathematical
Psychology2017;76:198–211.
[18] BleiDM,KucukelbirA,McAuliffeJD.Variationalinference:Areviewforstatisticians.JournaloftheAmericanStatistical
Association2017;112:859–877.
[19] DasguptaI,SchulzE,GershmanSJ.Wheredohypothesescomefrom?CognitivePsychology2017;96:1–25.
[20] DawND,CourvilleAC,DayanP.Semi-rationalmodelsofconditioning:Thecaseoftrialorder2008;p.431–452.
[21] SanbornAN,SilvaR.Constrainingbridgesbetweenlevelsofanalysis:AcomputationaljustificationforlocallyBayesian
learning.JournalofMathematicalPsychology2013;57:94–106.
[22] FristonK,MattoutJ,Trujillo-BarretoN,AshburnerJ,PennyW. VariationalfreeenergyandtheLaplaceapproximation.
Neuroimage2007;34:220–234.
[23] GershmanSJ,HoffmanMD,BleiDM. Nonparametricvariationalinference. In:Proceedingsofthe29thInternational
ConferenceonInternationalConferenceonMachineLearningOmnipress;2012.p.235–242.
[24] EliasP.Predictivecoding–I.IRETransactionsonInformationTheory1955;1:16–24.
[25] RaoRP,BallardDH.Predictivecodinginthevisualcortex:afunctionalinterpretationofsomeextra-classicalreceptive-
fieldeffects.NatureNeuroscience1999;2:79.
[26] FristonK.Hierarchicalmodelsinthebrain.PLoSComputationalBiology2008;4:e1000211.
[27] FristonK,KiebelS.Predictivecodingunderthefree-energyprinciple.PhilosophicalTransactionsoftheRoyalSocietyof
LondonB:BiologicalSciences2009;364:1211–1221.
[28] BastosAM,UsreyWM,AdamsRA,MangunGR,FriesP,FristonKJ.Canonicalmicrocircuitsforpredictivecoding.Neuron
2012;76:695–711.
10 SAMUELJ.GERSHMAN
[29] FristonK.Whatisoptimalaboutmotorcontrol?Neuron2011;72:488–498.
[30] AitchisonL,LengyelM.Withorwithoutyou:predictivecodingandBayesianinferenceinthebrain.CurrentOpinionin
Neurobiology2017;46:219–227.
[31] MurraySO,KerstenD,OlshausenBA,SchraterP,WoodsDL.Shapeperceptionreducesactivityinhumanprimaryvisual
cortex.ProceedingsoftheNationalAcademyofSciences2002;99:15164–15169.
[32] SummerfieldC,TrittschuhEH,MontiJM,MesulamMM,EgnerT.Neuralrepetitionsuppressionreflectsfulfilledpercep-
tualexpectations.NatureNeuroscience2008;11:1004.
[33] EgnerT,MontiJM,SummerfieldC.Expectationandsurprisedetermineneuralpopulationresponsesintheventralvisual
stream.JournalofNeuroscience2010;30:16601–16608.
[34] KokP,deLangeFP. Predictivecodinginsensorycortex. In:AnIntroductiontoModel-basedCognitiveNeuroscience
Springer;2015.p.221–244.
[35] FristonKJ,ParrT,deVriesB. Thegraphicalbrain: beliefpropagationandactiveinference. NetworkNeuroscience
2017;1:381–414.
[36] FristonK,RigoliF,OgnibeneD,MathysC,FitzgeraldT,PezzuloG. Activeinferenceandepistemicvalue. Cognitive
Neuroscience2015;6:187–214.
[37] ShannonCE.Amathematicaltheoryofcommunication.BellSystemTechnicalJournal1948;27:379–423.
[38] FristonKJ,DaunizeauJ,KiebelSJ.Reinforcementlearningoractiveinference?PloSone2009;4:e6421.
[39] FristonK,SamothrakisS,MontagueR. Activeinferenceandagency:optimalcontrolwithoutcostfunctions. Biological
Cybernetics2012;106:523–541.
[40] BotvinickM,ToussaintM.Planningasinference.TrendsinCognitiveSciences2012;16:485–488.
[41] Kappen HJ, Gómez V, Opper M. Optimal control as a graphical model inference problem. Machine Learning
2012;87:159–182.
[42] GershmanSJ,DawND,MRabinovichPVKFriston,editor,Perception,actionandutility:Thetangledskein. MITPress;
2012.
[43] FristonK.Lifeasweknowit.JournaloftheRoyalSocietyInterface2013;10:20130475.
[44] ConstantA,RamsteadMJ,VeissiereSP,CampbellJO,FristonKJ.Avariationalapproachtonicheconstruction.Journal
ofTheRoyalSocietyInterface2018;15:20170685.