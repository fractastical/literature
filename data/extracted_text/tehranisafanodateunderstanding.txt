Bayesian observational learning of other’s risk attitude:
Investigating the role of inferential uncertainty in the
approximate delta rule social inferences
AmirHossein Tehrani-Safa1, Reza Ghaderi1, Atiye Sarabi-Jamab2*,
1 Institute for Cognitive and Brain Sciences, Shahid Beheshti University, Tehran, P.O.
Box 19839-63113, Iran
2 School of Cognitive Sciences, Institute for Research in Fundamental Sciences
(IPM),Tehran, P.O. Box 19395-5746, Iran
* atiye.sarabi@gmail.com
Abstract
Trait, attitude and preference learning encompasses the encoding of stable
characteristics from observed behaviors, which are then used to make predictions and
influence how one interacts with an individual in various contexts. Being able to
understand the risk-taking tendencies of others is a complex example of such social
inference job. In this research, we used a Bayesian framework to explore how humans
can gauge another person’s risk-taking tendency. We used a sequential scenario where
an observer watched the other person’s choices between a high-risk gamble and a
guaranteed smaller reward. We proposed an approximate Bayesian observer to assess
an agent’s risk attitude. This learner utilizes a probabilistic generative model to model
the decision making process of others and then employs a variational Bayesian method
to invert the generative model. Our research adds to the accumulating evidence that
inverting generative models are an essential computing tool for understanding social
behavior. The learner updates the posterior estimation of the other’s risk attitude on
a trial-by-trial basis, with the discrepancy between the model predictions and the
choices observed followed the widely accepted prediction error framework namely delta
rule. By combining the algorithmic advantages of delta rule with the computational
advantage of Bayesian framework, we fashioned a more effective and comprehensible
learner. We showed that the accumulated uncertainty the observer builds up while
predicting the agent’s choices is the main factor that determines inferential
uncertainty, i.e., the uncertainty the learner has in estimation of the agent’s hidden
attitude, which is the impetus for learning and discovery. The model begins with a
high learning rate and gradually reduces it as more trials take place. This reflects the
way humans learn from examples sequentially, exploring and then making use of the
information as they progress. As more data is accumulated from someone who follows
a consistent decision-making process, the observer’s faith in his own judgments should
grow, and he should be less affected by each new observation.
Author summary
Our brains have an innate ability to assess the preferences and traits of others from
their observable behavior. For example, if a person is frequently willing to take
financial risks, we may deduce that they have a preference for high-risk, high-reward
August 24, 2023 1/26
outcomes. Despite this intuitive understanding, the exact computational framework
our brains use to make this inference has yet to be determined. Current theories draw
from the behavioral economics proposes a mathematical model for risk evaluation, and
a probabilistic decision rule to model how agents make choices in the face of
uncertainty. However, the precise process by which our brains simulate this complex
model of decision-making remains a mystery. We presented a Bayesian observer that
can gauge an agent’s risk-taking behavior. Our theoretical and experimental analyses,
using both synthesized and real data, indicated that this model can effectively and
accurately replicate the process of social learning, where one infers the minds of other
people. For instance, the belief updating follows the widely accepted prediction error
framework as a structure for human brain learning. Furthermore, learning rate
decreases with more trials, similar to the way humans ascertain the character of others
from the evidence of a few interactions, initially exploring widely and then gradually
applying the wisdom gained from their observations.
Introduction 1
As social beings, humans can accumulate knowledge throughobservational learning, 2
which refers to the cognitive process of acquiring attitudes, values, beliefs, and 3
associated behaviors by observing patterns displayed by others [1]. For example, 4
youngsters observing their parents take risks and make calculated decisions can 5
acquire their risk attitudes, and in turn, come to understand the importance of taking6
calculated risks in life. Comparatively to experiential learning [2], where the individual 7
experiences the outcomes directly, observational learning is perceived as less 8
hazardous [3].Three distinct forms of observational learning have been distinguished by9
contemporary research [3,4], namely, (1) vicarious (indirect) reinforcement-learning [5], 10
(2) action imitation, and (3) inference about other people’s beliefs and intentions. 11
Vicarious reinforcement-learning is aptly understood within the 12
reinforcement-learning framework and involves computing reward prediction errors 13
from observing other agents’ actions. By comparing the predicted rewards of another 14
agent with the actual outcomes, these prediction errors are calculated. Action 15
imitation learning, however, involves calculating the difference between what another 16
person does and what we thought they would do, known as action prediction error [4]. 17
Unlike imitation learning, which does not involve learning about internal variables,18
the final type involves learning about others’ intentions, attitudes, and preferences. By19
doing so, we can better understand those around us. Observing how a colleague 20
interacts with his/her manager can provide insight into their relationship; asking a 21
friend what his/her favorite restaurant is can reveal his/her food preferences; observing22
nonverbal cues, like facial expressions and body language, can reveal another’s 23
emotional state. Lastly, frequent visits to a website may indicate intention to purchase.24
Social neuroscience studies addressed the question of how we can infer individuals’25
preferences and beliefs from their observable behavior: See [6] for learning about other 26
person reputation and trust, see [7] for evaluating the expertise of others, see [8] for 27
learning about the abilities of other people in cooperative and competitive social 28
situations, see [9] for the validity of partner’s advice, see [10] for inferring another 29
person’s discounting rates, and see [11] for learning about others’ prudence, 30
impatience and laziness. 31
Learning about others often takes place in a context where there are no right or 32
wrong answers. One kind of such mental state is the risk attitude. In behavioral 33
economics, a risk situation is a situation where all the possible events are known, but34
their occurrence depends on probability [12]. Taking risks can yield substantial 35
benefits, but may also result in negative outcomes or none at all [12]. As a general rule, 36
August 24, 2023 2/26
taking risks involves choosing the option with the most uncertainty [13]. Essentially, a 37
risk attitude is an implicit belief about how to evaluate risks most effectively [11]. 38
Observational learning can provide insight into others’ risk preferences. For 39
instance, a person who chooses to take a high-risk gamble with a large reward would40
appear to be more risk tolerant than someone who chooses to take a low-risk gamble41
with a small reward. Furthermore, observing someone place large bets on a game may42
suggest that the individual is taking a high risk and may take on other risky bets in43
the future. 44
As internal variables, such as attitudes and subjective values, cannot be directly 45
measured from task design, they need to be inferred from participants’ 46
behavior [4,9,11]. Inferring internal variables from computational (generative) models, 47
is based on the assumption that humans perceive sensory input as a statistical 48
inference machine [14]. Additionally, some cognitive theories describe cognition as 49
probabilistic Bayesian inference in an uncertain world [15,16], leading to a Bayesian 50
brain hypothesis [14,17–20].The process involves combining prior beliefs and evidence 51
from observations of others’ behavior to update one’s beliefs about other people’s 52
intentions and attitudes [4].The approach at hand is similar to the well-known inverse53
reinforcement learning strategy [21] whereby the observer acquires knowledge of the 54
world that is ultimately abstracted from observations [22]. 55
Bayesian learning assumes a generative model, which includes explicit probabilistic56
assumptions about how decisions are made [23]. Inversion of the generative model 57
using Bayes’ rule provides optimal estimates of posterior densities of internal variables.58
In complex situations, exact inversion is generally unattainable, so algorithmic 59
inference requires additional approximations [24]. In technical terms, Bayesian 60
integrals are intractable and require approximate inference, which is achieved by using61
variational Bayesian inference [25]. Traditional sampling methods can also 62
approximate the posterior without addressing the integral, but doing so can be 63
computationally costly [26]. 64
To gain an understanding of people’s unobservable characteristics and beliefs, we 65
explored how their visible behavior can be interpreted. We used probabilistic models 66
to quantify the risk attitude of an observable agent based on how they make decisions.67
We utilized a Bayesian inference approach and used a variational Bayesian method to68
simplify the task. This led to the development of the Bayesian observer which is able69
to estimate an individual’s risk attitude by observing how they act in various 70
decision-making scenarios. 71
Results 72
Theoretical Results 73
Model inversion 74
Generally, generative models consist of a joint probability density of all states. Generic75
models are created by combining likelihood functions and state priors. 76
P

C(k), ρ(k)
β, p(k), r(k)

= P

C(k)
ρ(k), β, p(k), r(k)

× P

ρ(k)
β, p(k), r(k)

(1)
In Eq (1), the termP

C(k)
ρ(k), β, p(k), r(k)

is the likelihood function which is 77
defined by Eqs. (24), (25), (26) and (27) (seeComputational model of risky behavior). 78
A model inversion problem involves finding the termP

ρ(k)
C(k), β, p(k), r(k)

79
which describes the posterior probability of the stateρ(k) that is of interest. As 80
August 24, 2023 3/26
opposed to likelihood, which predicts choice from hidden parameters and states, 81
posterior density of the state predicts the desired state from what choices have been 82
observed. However, an analytical solution to the exact Bayesian model inversion is not83
feasible for our problem, so we should rely on some fast, but suﬀiciently accurate 84
approximation. 85
V ariational energies 86
The first step in this process is to incorporate what is known as the mean-field 87
approximation, which transforms the joint distribution into the product of marginal 88
posterior distributions [27]: 89
P

C(k), ρ(k)
β, p(k), r(k)

= ˆq(C(k)) × ˆq(ρ(k)) (2)
With the help of mean-field approximation, marginal posterior distributions can be 90
expressed as follows in terms of variational energyI(·) [27]: 91
ˆq(C(k)) = 1
ZC
exp
h
I

C(k)
i
, I(C(k)) =
D
ln P

C(k), ρ(k)
E
q(ρ(k))
ˆq(ρ(k)) = 1
Zρ
exp
h
I

ρ(k)
i
, I(ρ(k)) =
D
ln P

C(k), ρ(k)
E
q(Ck)
(3)
where, 92
⟨f(x)⟩q(x)
def
=
Z
q(x)f(x)dx (4)
In Eq (3), the normalization factorsZC and Zρ are used to ensure that the total 93
probability equals one. 94
As a next step, the maximum entropy principle is used to create approximate 95
posteriors with minimal assumptions. This principle states that the least arbitrary 96
distribution is the one that maximizes entropy [28]. The Bernoulli distribution is the 97
most entropy-rich distribution forC(k) due to its binary nature: 98
ˆq(C(k)) ∼ Bernoulli

C(k); µ(k)
C

(5)
Additionally, we choose the Gaussian distribution forρ(k) because it provides the most 99
randomness among continuous probability distributions: 100
ˆq(ρ(k)) ∼ N

ρ(k); µ(k)
ρ , σ(k)
ρ

(6)
Having determined the form of the distributions, the inverse problem is relegated to101
identifying moments in the distributions that have been represented byµ(k)
C (Eq(5)), 102
µ(k)
ρ , andσ(k)
ρ (Eq(6)). Identifying these moments involves two recursive steps: 103
I) During theprediction step , a decisionCk is predicted based on previously 104
updated posterior beliefs regarding the agent’s attitude toward risk. This is 105
accomplished by estimating the moment of choice distribution (µ(k)
C ). 106
II) After observing the value ofCk represented byCk
o , theupdate step is taken. 107
In this step, given the current choice data, the observer computes an updated posterior108
belief forρ(k), which includes the updated meanµ(k)
ρ and the updated varianceσ(k)
ρ . 109
August 24, 2023 4/26
I) Prediction Step 110
Starting from equation Eq(3), the variational energyI(C(k)) can be expanded as 111
follows: 112
I(C(k)) =
D
ln P

C(k), ρ(k)
E
q(ρ(k))
=
D
ln P

C(k)
ρ(k)
E
q(ρ(k))
+
D
ln P

ρ(k)
E
q(ρk)
(7)
Due to the fact that

ln P
 
ρ(k)
q(ρk) does not depend onCk, the Eq (7) can be 113
simplified as follows: 114
I(C(k)) =
D
ln P

C(k)
ρ(k)
E
q(ρ(k))
(8)
If we insertP

C(k)
ρ(k)

(Eq(27)) into Eq (8), we get: 115
I(C(k)) =
D
Ck ln S

F (ρ(k))

+ (1 − Ck) ln

1 − S

F (ρ(k))
E
q(ρ(k))
= Ck
D
ln S

F (ρ(k))
E
q(ρ(k))
+ (1 − Ck)
D
ln

1 − S

F (ρ(k))
E
q(ρ(k))
(9)
There is no closed form analytical solution for integrals in Eq (9). Additionally, 116
remember thatρ is updated in accordance with theC(k), at the time when we wish to117
estimate theC(k), the last updated moments ofρ would be those values obtained from 118
the previous trial (µ(k−1)
ρ , σ(k−1)
ρ ). So it seems reasonable to approximate the integrals119
using µ(k−1)
ρ in place ofρ(k). This results in the subsequent structure forˆq(C(k)): 120
ˆq(C(k)) = 1
ZC
exp
h
I

C(k)
i
=

S

F (µ(k−1)
ρ )
C(k)
×

1 − S

F (µ(k−1)
ρ )
1−C(k) (10)
As a result, the Eq(10) naturally has the Bernoulli distribution form, where: 121
µ(k)
C = S

F (µ(k−1)
ρ )

σ(k)
C =
h
S

F (µ(k−1)
ρ )
i
×
h
1 − S

F (µ(k−1)
ρ )
i (11)
The parameterµ(k)
C = S

F (µ(k−1)
ρ )

determines the shape ofˆq(C(k)) and provides the 122
most accurate estimate regarding the agent’s choice at trialk, C(k). 123
II) Update Step 124
Having observed the value ofC(k) = C(k)
o , it is now time to modify the model 125
parameter ρ(k) in accordance with the new observed value. We begin by rewriting the126
definition of variational energy forρ(k) as follows: 127
I(ρ(k)) =
D
ln P

C(k), ρ(k)
E
q(Ck)
=
D
ln P

C(k)
ρ(k)
E
q(Ck)
+
D
ln P

ρ(k)
E
q(Ck)
(12)
August 24, 2023 5/26
C(k) is an observable variable, and when the observer wishes to update the estimate of128
ρ, the value ofC(k) = C(k)
o is readily available. Therefore, it is acceptable to substitute129
C(k) with C(k)
o in the Eq (12). As a result, the expectation operator is simply 130
cancelled: 131
I(ρ(k)) = ln P

C(k)
o
ρ(k)

+ ln P

ρ(k)

(13)
According to Eq (27), P

C(k)
o
ρ(k)

represents the likelihood function, andP
 
ρ(k)
132
represents the prior knowledge ofρ’s distribution. Once we have placed the prior 133
distribution and likelihood into Eq (13), we have the following equation: 134
I(ρ(k)) = Ck
o × ln
h
S

F (ρ(k))
i
+ (1 − C(k)
o ) × ln
h
1 − S

F (ρ(k))
i
−

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
(14)
If we simplify the expression (seeAppendix S1:), we will arrive at the following result:135
I(ρ(k)) = ln
h
S

F (ρ(k))
i
+ β × (C(k)
o − 1) × F (ρ(k)) −

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
(15)
Considering that the obtained equation (Eq (15)) doesn’t have a quadratic form with 136
respect toρ(k), it cannot be used as an exponent of a Gaussian distribution. Applying137
a quadratic approximation to the variational energyI(ρ(k)) (see Appendix S2:), we 138
arrive at the following equations for updatingµ(k)
ρ and σ(k)
ρ : 139
µ(k)
ρ = µ(k−1)
ρ + β ×
0
@∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
1
A× σ(k)
ρ ×

C(k)
o − S

F (µ(k−1)
ρ )

(16)
140
1
σ(k)
ρ
= 1
σ(k−1)
ρ
+β2×
0
@∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
1
A
2
×
h
S

F (µ(k−1)
ρ )
i
×
h
1 − S

F (µ(k−1)
ρ )
i
(17)
Update Equations: structural interpretation 141
It is evident from Eqs. (16) and (17) that inverting the model by variational Bayes 142
approximation provides straightforward rules for updatingµ(k)
ρ and σ(k)
ρ in a 143
trial-by-trial basis that are computationally feasible. Although the updating equations144
are derived from Bayesian optimality, without consideration of reinforcement learning145
(RL) structure, they are well captured by the Rescorla-Wagner reinforcement learning146
model [29]. RL models are generally structured as follows: 147
µ(k)
ρ|{z}
posterior(k)
= µ(k−1)
ρ| {z }
posterior(k−1)
+ α(k)
|{z}
learning rate (k)
× δ(k)
|{z}
prediction error (k)
(18)
As per our model, the prediction errorδ(k) refers to the difference between the actual 148
choice C(k)
o and the predictionµ(k)
C = S

F (µ(k−1)
ρ )

made prior to the actual choice 149
C(k)
o being observed: 150
δ(k) = C(k)
o − µ(k)
C (19)
August 24, 2023 6/26
By introducingδ(k) into Eq (16) and then comparing it to Eq (18), we can derive the 151
expression for the time-varying learning rateα(k): 152
α(k) = β × γ(k) × σ(k)
ρ (20)
In Eq (20), γ(k) is the derivative of functionF (ρ(k)) (Eq (25)) relative toρ(k) at point 153
ρ(k) = µ(k−1)
ρ . γ(k) is influenced by both the probabilityp(k) and rewardr(k) of the 154
gamble proposed at trialk. At usual probabilities and rewards,γ(k) remains positive 155
(please refer to Eq (S2.13) for more details aboutγ(k) and Experimental Taskfor more 156
information about gambles’ settings). 157
γ(k) = ∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
(21)
In addition,β is also a non-negative term (seeComputational model of risky 158
behavior). Note that sinceβ is not provided to the observer, it will be estimated by 159
fitting to the entire set of choice data. 160
Additionally, the learning rateα(k) varies depending on the level of the positive 161
measure of varianceσ(k)
ρ . Considering thatσ(k)
ρ represents the variance of posterior 162
distribution, it is plausible that the learning rateα(k) will be proportional toσ(k)
ρ . It 163
is expected that upcoming observations will be more impactful on an observer who is164
less certain of what he or she knows. 165
Eq (17) gives usσ(k)
ρ , which can be rewritten in the following manner: 166
1
σ(k)
ρ
= 1
σ(k−1)
ρ
+ β2 × (γ(k))
2
× σ(k)
C (22)
σ(k)
C (see Eq (11)) represents the degree of uncertainty in the choice prediction. We167
can see that the amount ofσ(k)
ρ is inversely proportional to the amount ofσ(k)
C . As 168
long as the observer is confident about his prediction (σ(k)
C = 0 ), σρ(k) remains the 169
same, unlessσρ(k) decreases trial to trial. It should be noted that as a variance of 170
Bernoulli distribution,σ(k)
C is non-negative and has a peak value of 0.25. 171
Due to the recursive nature of Eq (22), σ(k−1)
ρ can be replaced by the value 172
specified in the equation. This process can be repeated forσ(k−2)
ρ and onward: 173
1
σ(k)
ρ
=
 
1
σ(k−2)
ρ
+ β2 × (γ(k−1))
2
× σ(k−1)
C
!
+ β2 × (γ(k))
2
× σ(k)
C
= 1
σ(0)
ρ
+ β2 ×

(γ(1))
2
× σ(1)
C + (γ(2))
2
× σ(2)
C + · · · + (γ(k))
2
× σ(k)
C

= 1
σ(0)
ρ
+ β2 ×
kX
i=1
(γ(i))
2
× σ(i)
C
(23)
It is clear from Eq (23) thatσ(k)
ρ is inversely proportional to the sum ofσ(i)
C weighted 174
by (γ(i))2. 175
Taking into consideration all of the above theoretical results, it seems that 176
predictive uncertaintyσ(k)
C keeps the learning rate lower than otherwise. This is 177
understandable since prediction error should be less magnified when the predictions 178
are less certain. 179
August 24, 2023 7/26
Simulation results 180
Each run of simulation contains a couple of blocks used in [30] (seeExperimental
Task). Fig 1 illustrates the computational trajectories of learning risk attitudeρ for
five simulated agents. The agents were designed to emulate a range of risk preferences
and behaviors. Simulated agents are characterized by their risk attitudesρ, the values
of which are outlined below:
ρ = {0.85, 0.95, 1, 1.05, 1.15}
In order to ensure that the task is at an appropriate level of diﬀiculty for the 181
learners, β was set to 1, so that the agent’s actions were neither too predictable nor182
too unpredictable. This balance allowed for the learner to be able to learn successfully,183
while still facing a challenging task. In the upcoming sections of the simulation part,184
we will examine quantitatively howβ value affects learning (see Fig4). 185
Fig 1 depicts the Bayesian learning process in the structure of reinforcement 186
learning model, with each panel corresponding to one element of Eq (18). The bottom 187
panel illustrates the observed choicesC(k)
o and associated choice prediction errorsδ(k), 188
the middle panel depicts the dynamic learning rateα(k), and the top panel presents 189
the posterior expectation of the parameter of interestµ(k)
ρ . 190
As shown in the bottom panel of Fig1, the simulated agent with the least 191
willingness to take risks (ρ = 0 .85) consistently turned down the offers. As the risk 192
attitude was increased from its minimum to its highest (ρ = 1 .15), the simulated 193
agents gradually became more likely to take risks. This trend reaﬀirms the 194
presumption that agents who are more disposed to risk are more likely to gamble. 195
Remarkably, the agent with the highest risk attitude accepted the gambles in the 196
majority of trials, with only rare exceptions. Through these simulations, we can 197
deeply examine the capacity of the learning model to adapt to a variety of situations198
in numerous ways. 199
When the learner’s predictionsµ(k)
C match the decision-maker’s choiceC(k)
o , the 200
posterior expectation of risk attitudeµ(k)
ρ remains the same. On the other hand, when201
the learner’s predictions are at odds with the decision-maker’s choice, the posterior 202
expectation is impacted accordingly. If the learner underestimates the 203
decision-maker’s risk attitude (δ(k) > 0), the expected risk attitude increases; 204
contrarily, if the learner overestimates the decision-maker’s risk attitude(δ(k) < 0), the 205
expected risk attitude decreases. 206
The Bayesian learner initially posits a prior mean and prior variance for the 207
parameter of interest:µ(0)
ρ = 1 , σ(0)
ρ = 0 .01. The Bayesian learner employs a dynamic 208
learning rate technique that starts with a broad exploration of the parameter space 209
and gradually transits to a more focused exploration of the parameter space as trials210
progress. As expressed through Eq (18), the learning rateα(k) governs the impact of 211
prediction errorsδ(k) on the model expectations. Through the course of the trials, the212
learning rateα(k) is reduced, allowing the learner to build on the knowledge gained 213
from earlier trials and become less responsive to new data. 214
We depicted the y-axis of middle panel in log scale since the learning rateα(k) in 215
starting trials is much larger than that in last trials. In the final trials, a low learning216
rate does not allow the model to alter its expectations in a significant way, even with217
high prediction error signals. This is because the model is confident that it has almost218
found the exact value of risk attitude, and that any discrepancies in the later trials are219
likely just noise. This approach is advantageous for our risk attitude learning problem220
as it is assumed that the underlying parameter of interestρ is unchanging. 221
The top panel in Fig2 illustrates that there is a close link between learning rates 222
α(k) and the degree of uncertainty in posterior expectationsσ(k)
ρ . This is evident from 223
August 24, 2023 8/26
Fig 1. Computational trajectories of learning risk attitudes ρ for 5
simulated agents. Risk attitude values:ρ = {0.85, 0.95, 1, 1.05, 1.15} and β = 1 .
Each column corresponds to one agent. Two blocks (56 trials) make up each
simulation run for each agent.T op:-blue line : Posterior expectationµ(k)
ρ of risk
attitude ρ. The regions within one standard deviation of the mean
q
σ(k)
ρ were shaded
in blue. The mild green area is indicative of a risk-taking attitude (ρ > 1), while the
mild red area is indicative of a risk-averse attitude(ρ < 1). The black dot line indicates
neutrality (ρ = 1 ). Middle: -orange line : Graph representing the Baysian dynamic
learning rateα(k) modulating the impact of choice prediction errorsδ(k) on µ(k)
ρ .The
y-axis is shown in log scale.Bottom: gray dot : simulated agent choicesC(k)
o . red
bar: Prediction errorδ(k), the difference between the actual choiceC(k)
o and the
prediction µ(k)
C = S

F (µ(k−1)
ρ )

.
Eq (20), which demonstrates thatα(k) and σ(k)
ρ have a linear relationship. 224
Consequently, a reduction in learning rate is coupled with a decrease inσ(k)
ρ . As the 225
trials advanced, the model’s risk attitude estimates became increasingly certain, as the226
variance of the posterior expectation of risk attitude decreased. In addition, the 227
learning rate trajectory showed variations that could not be explained by the variance228
of the risk attitude, but rather by another factor,γ(k) (as depicted in FigFigure S1: 229
and described by Eq (20)). 230
When the model felt sure of its prediction (σ(k)
C = 0 ), theσ(k)
ρ was kept unchanged 231
to maintain a high learning rate. This was because if the prediction was incorrect but232
the model was still highly confident, it had to be adjusted according to the error signal233
that was received. As the model’s certainty in prediction decreased, theσ(k)
ρ decreased 234
in relation to the prediction uncertainty, and experienced the most drastic decrease 235
when the uncertainty was at its highest (σCmax = 0 .25) .The result of the simulation 236
was in agreement with Eq (22), showing that the theoretical and simulated results 237
were consistent 238
Fig 3 illustrates the trajectory ofσ(k)
ρ and α(k) across thousands of trials. This 239
graph, with both the x- and y-axes in log scale, shows that there is a negative linear240
relationship betweenσ(k)
ρ and trial numberk in log space. In other words, the 241
logarithm ofσ(k)
ρ decreases linearly with the increase of logarithm of trial numberk. 242
As the trials continue, theσ(k)
ρ decreases until it converges to zero, but at a slower and243
slower pace. Consequently, the learning rateα(k) follows the same path asσ(k)
ρ , 244
August 24, 2023 9/26
Fig 2. The relationship between the rate of learning and the model’s
uncertainties. T op: -orange line (right axis): Learning rateα(k). -purple line
(left axis): Variance of the posterior expectationσ(k)
ρ . The y-axis is scaled in
logarithmic units on both sides.Middle: -purple line (right axis): Variance of the
posterior expectationσ(k)
ρ . The y-axis is shown in log scale.black bar (left axis):
The bar chart illustrates the collective uncertainty of the learning model in predicting
decisions made by the agent.Bottom: The chart displays the model’s uncertainty in
predicting agent’s choicesσ(k)
C . The value is confined to a positive number that is less
than one-fourth(0 ≤ σ(k)
C ≤ 0.25), as it reflects the variance of a Bernoulli distribution
employed to model predictions. The upper boundσmax is represented by a black
dotted line.
meaning that the learning process persists even after many trials, albeit at a very slow245
rate. The fitted line for the log-transformation of theσ(k)
ρ data points intersects the 246
y-axis at -2, indicating that the initial valueσ(0)
ρ was 0.01, as set initially. 247
In Fig4, β is a parameter that controls how random an agent’s decisions are (see 248
Computational model of risky behavior). Near-zero values ofβ producing completely 249
unpredictable choices (σ(k)
C = 0 .25). As the level ofβ rises, the amount of uncertainty 250
August 24, 2023 10/26
Fig 3. The figure demonstrates the convergence ofσ(k)
ρ and α(k) trajectories towards
a stable equilibrium throughout numerous simulation trials. The lineY=-X-2 serves
as an asymptotic line forσ(k)
ρ , depicting the pattern of variation ofσ(k)
ρ over the
course of numerous trials. both x and y axis depicted in log scale.
in the prediction lessens, which indicates that less random choices are more likely to251
be accurately predicted by the learner. In the most extreme case,σ(k)
C tends to 252
approach zero at the highest values ofβ, where the agent chooses in a predictable 253
fashion, demonstrating that deterministic choices are the easiest to comprehend. 254
Subsequently, we ran a simulation to explore the impact ofβ on σ(k)
ρ and α(k). The 255
upper panel of Fig4 illustrates the outcome of the simulation.σ(k)
ρ and β have an 256
inversely proportional relationship according to Eq (22), which shows that whenβ 257
increases, σ(k)
ρ decreases. Furthermore,β’s influence onσ(k)
C (bottom panel) also 258
impacts σ(k)
ρ in a complex way: asβ rises, σ(k)
C decreases, which in turn causesσ(k)
ρ to 259
increase (Eq22). Simulations indicate that when these two opposing forces are in 260
contention, σ(k)
ρ ultimately diminishes whenβ increases. Turning next toα(k), Eq (20) 261
elucidates its direct relationship to bothσ(k)
ρ and β. So too, we see an opposition 262
between two forces here. However, the simulations indicate that asβ increases, the 263
opposing forces betweenσ(k)
ρ and β remain in equilibrium, so the effect onα(k) is 264
minimal. This indicates that whileβ has a significant impact onσ(k)
ρ , it does not have 265
a significant effect on learning rateα(k). 266
Model comparison 267
We compared the Bayesian learner to the Rescorla-Wagner (R-W) model [29], a widely 268
used reinforcement learning model. The R-W model adjusts estimations through a 269
prediction error weighted by a preset learning rate, whereas the Bayesian model 270
adjusts the learning rate at each trial, eliminating the need for a predetermined 271
learning rate. As each participant’s data is unique, the R-W learner’s accuracy is 272
reduced if the optimal learning rate is not found for each individual. Therefore, the 273
Bayesian model only requires one adjustable parameter to fitβ to each set of data, 274
whereas the R-W model requires two. 275
August 24, 2023 11/26
Fig 4. effect of β on σ(k)
C , σ(k)
ρ and α(k). For each level ofβ, four blocks of
twenty-eight trials were run, with the average of each variable in the last block
represented in the figure. Risk attitudeρ was 1 in all simulations.T op:-orange
line: Learning rateα(k). -purple line : Variance of the posterior expectationσ(k)
ρ .
The y-axis is scaled in logarithmic units.Bottom: The model’s uncertainty in
predicting agent’s choicesσ(k)
C .
To compare the two learning models, we utilized the data collected by Suzuki et276
al [30]. We utilized the data from session 1, 3 and 5, in which the risk attitudes of the277
participants were assessed. Fig5 displays the learning trajectories of one of the 278
participants involved in the experiment. 279
We employed the BFGS optimization algorithm from the TAPAS 280
(https://tnu.ethz.ch/tapas) [31] to determine the best parameters for our models, 281
which were then used to calculate the quality measures of the models. This 282
optimization technique maximizes the log-joint posterior density across all parameters,283
based on the models’ trial-wise predictions. We used the Log Model Evidence (LME)284
and the Bayesian Information Criterion (BIC) to measure the performance of the 285
models. The Model Evidence is essentially the probability of the data given the model,286
which we approximated with the Free Energy [32]. When selecting the best model, the 287
Free Energy performs better than the Akaike Information Criterion (AIC) or the BIC,288
as the latter two focus solely on the number of parameters, rather than taking into 289
account their covariance [26,33]. 290
Fig 5 reveals that the Bayesian learner’s average learning rate¯αBL is comparable 291
to the fixed learning rate of the R-W reinforcement learning modelαRL, despite a 292
slight downward trend. This accomplishment is attributed to BL’s ability to 293
implement a vague prior and adjust its learning rate according to its uncertainty. This294
systematic tuning of the learning rate enables BL to achieve a similar accuracy score295
to R-W, while only needing one fitting parameter instead of two. Furthermore, the 296
August 24, 2023 12/26
Bayesian Information Criterion (BIC) and the Log Model Evidence (LME) scores 297
confirm that BL with one free parameter is superior to R-W with two free parameters,298
even if the accuracy likelihood score is marginally lower. Table1 provides a summary 299
of the results of the models quality assessments. 300
We implemented a Random-Effects Bayesian Model Selection (RFX-BMS) [32,34] 301
to compare the alternative models and determine their relative strength. The posterior302
probability of each model was then used to calculate the Exceedance Probability (EP)303
and Protected Exceedance Probability (PEP) [32,34]. We leveraged the open-source 304
VBA toolbox [35] to conduct the analysis and the results indicated that the Bayesian305
learner was the superior model compared to the R-W model in explaining the 306
participants’ behavior (EF= 0.99, EP=1, PEP=1). Please refer to Fig6. 307
Fig 5. Figure displays the learning trajectories of both R-W reinforcement learning
and Bayesian learning (BL) models. It is based on the choice behavior of one subject
from the study conducted by Suzuki et al. [30]. The R-W trajectories represents by
-light blue line and the BL trajectories represents by-dark blue line . On the top
panel, -dark green line illustrates the maximum likelihood estimation (MLE) of the
subject’s risk attitude, which has been calculated separately for each session. Here, we
present the fitted parameters for each model in each sessions. Additionally, we
calculated the average dynamic learning rate of the Bayesian Learner to compare with
the fixed rate of the Reinforcement Learner. Session 1:¯αBL = 0 .08 ; αR−W = 0 .08 ;
βBL = 1 .45 ; βR−W = 1 .44 / Session 3:¯αBL = 0 .03 ; αR−W = 0 .04 ; βBL = 0 .54 ;
βR−W = 0 .55 / Session 5:¯αBL = 0 .06 ; αR−W = 0 .07 ; βBL = 0 .77 ; βR−W = 1 .10
Fig 6. Bayesian Model Selection R-W vs BL
August 24, 2023 13/26
T able 1. Comparison of the goodness-of-fit between the Bayesian learner and the
Rescorla-Wagner learner. For the Bayesian Information Criterion (BIC), the smaller
the value, the better the fit. The Log-likelihood (LL) and Log Model Evidence (LME)
both indicate better fit with bigger values; # prms, number of free parameters. Each
session’s best model is bolded.
session Model LL BIC # prms LME
session 1 Bayesian Learner −245.96 571 .89 1 -277.79
Rescorla-Wagner Learner −236.02 631 .98 2 −313.44
session 3 Bayesian Learner −173.69 427 .35 1 -200.82
Rescorla-Wagner Learner −180.71 521 .36 2 −259.29
session 5 Bayesian Learner −182.54 445 .05 1 -210.40
Rescorla-Wagner Learner −168.12 496 .19 2 −234.89
Discussion 308
In this research, we applied a Bayesian framework to explore how humans may judge309
another person’s risk-taking tendency based on a few examples presented in succession.310
Our findings offer insights into the intricate social psychology of risk-assessment. We311
designed a scenario where an observer, online, could watch another person’s decisions312
and form an opinion about him. The observer was presented with a choice between a313
high-risk gamble with a larger potential reward or a risk-free option with a guaranteed314
smaller reward for the person being observed. The observer would infer the other 315
person’s attitude sequentially based on these observed choices. 316
We proposed a Bayesian approach to address this problem, which we then 317
simplified into a more accessible and computationally eﬀicient algorithm, namely delta318
rule [36,37]. The delta rule expresses that belief updates depend on the previous belief319
and the error made in predicting the most recent observation [38] and has biological 320
foundations examined in [39]. Delta rule has been used to analyze social behavior in 321
the field of social psychology [40]. 322
Early investigations showed that social learning in the lab can be comprehended 323
through the same delta rule learning principles employed in non-social learning [41–43]. 324
The discrepancies between what we anticipate and what we experience (the prediction325
error) can reveal how we acquire social value, such as how generous another can 326
be [44], or if someone’s willingness to punish impacts our own [45]. In our problem, 327
the model updates the posterior estimation of the other’s risk attitude on a 328
trial-by-trial basis, with the discrepancy between the model predictions and the 329
choices observed. By combining the algorithmic advantages of delta rule with the 330
Bayesian framework, we fashioned a more effective and comprehensible learner. 331
Bayesian learning posits that estimation uncertainty, also called inferential 332
uncertainty [46], is the impetus for exploration [46,47]. Research on human learning 333
demonstrated that such uncertainty plays a role in how humans learn and attain 334
knowledge [38]. The feeling-of-knowing, or subjective confidence [48], is believed to be 335
associated with the inferential uncertainty that arises during learning process [46], 336
leading to the idea of defining confidence as a Bayesian probability [48]. In this 337
framework, the precision of the distribution (its inverse variance) is seen as the 338
manifestation of confidence [49]. Furthermore, from a biological perspective, it appears339
that the brain not only estimates a value, but also the level of certainty of that 340
estimate, along with its entire probability distribution [19]. In our Bayesian risk 341
attitude learner,σρ characterizes the inferential uncertainty of the model in estimating342
other’s risk attitude. Mathematically, the confidence level is formulated as a negative343
logarithm of variance in the distribution [46]. In Fig2, Fig3 and Fig4, we showed the 344
trajectories ofσρ using the log scale, which is the natural space for illustrating 345
August 24, 2023 14/26
variance [50], making the transition from variance to confidence much simpler. 346
The Bayesian computational framework gives us the chance to investigate further347
how confident humans are when learning others’ preferences, which can build upon 348
prior research into human confidence in areas such as memorization [51], perceptual 349
decision-making [52], and probabilistic learning task [46]. In contrast, the 350
Rescorla-Wagner model cannot express inferential uncertainty; it provides only a 351
single point estimate, without giving any indication of the reliability of the 352
estimation [29,46,47]. Although progress has been made in understanding how the 353
brain uses prediction errors to learn, there is still limited knowledge about the learning354
rate [38]. 355
In our theoretical analysis, we connected the learning rate to the uncertainty the356
model experiences when predicting an agent’s choice. Additionally, we showed that the357
accumulated uncertainty the observer experiences when predicting the agent’s 358
decisions is what determines the width of the posterior distribution. To validate our 359
theoretical findings, we simulated data to assess the observer’s capability to precisely360
guess the decision maker’s risk attitudeρ. Furthermore, since simulation allows us to 361
pick the number of trials, we conducted the simulations with a larger than usual 362
number of trials to thoroughly explore the properties we discovered in the theoretical363
section, particularly concerning the connection between the learning rate and the three364
types of uncertainty: variance of posterior belief (σ(k)
ρ ), uncertainty in prediction 365
(σ(k)
C ) and decision maker randomness (β). The simulation results corroborated the 366
hypothesis that the learning rateα(k) was contingent onσ(k)
ρ and σ(k)
C , as indicated by 367
the theoretical analysis. Furthermore, the simulation results demonstrated that an 368
increase inβ has an adverse effect on both other types of uncertainty (σ(k)
ρ and σ(k)
C ), 369
while having no impact on the learning rateα(k). 370
Research on human social learning and cognition has shown that the greatest rate371
of learning takes place in the beginning of an experiment when there is the most 372
uncertainty about what others will do [42,53–55]. In this line, our Bayesian observer 373
begins with a high learning rate to explore all potential values, holding this rate at a374
high level as long as the model is confident in its predictions. 375
However, when the observer is uncertain (e.g., assuming a 50-50 chance for each 376
option), the model decreases the learning rate in order to prevent unhelpful 377
information from affecting its current estimation. The Bayesian learner is able to 378
adjust its learning speed according to the uncertainty of its beliefs. The width of the379
posterior belief distribution reflects the range in which the true value is likely to lie,380
and the learning rate is therefore proportional to this width, aiding the learner in 381
finding the most probable true value. 382
We compared our Bayesian learner with an uncertainty-based learning rate to the383
Rescorla-Wagner reinforcement learning model, which uses a constant learning rate. 384
We discovered that the Bayesian learner had greater model quality scores. This is due385
to the Bayesian learner having an intrinsic mechanism for determining the learning 386
rate, which the Rescorla-Wagner model lacks. The Rescorla-Wagner model must use a387
fitting algorithm to determine an optimal learning rate, which requires an additional388
fitting parameter. In contrast, the Bayesian observer eliminates the need for this extra389
parameter through its inherent strategy for determining the learning rate, which is 390
derived from Bayesian optimality principles. 391
The Rescorla-Wagner model’s best-fit learning rate for the exemplar participant’s 392
dataset was equivalent to the Bayesian observer’s average learning rate. Although this393
delta-rule algorithm with a fixed learning rate (i.e., Rescorla-Wagner model), similar 394
to the Bayesian approach, had good accuracy in predicting others’ attitudes, it does 395
not accurately replicate the human exploration-exploitation heuristic. For this reason,396
a fixed learning rate model is not a precise representation of behavior for this task. 397
August 24, 2023 15/26
Our Bayesian learner, by contrast, started with a high learning rate and decreased398
it as trials progress. The process reflects the way humans learn from examples in a 399
sequential manner, exploring and then exploiting the information as they go. It is 400
expected that confidence would increase with the number of observations made during401
stable periods [46]. As more data is collected from someone following a consistent 402
decision-making process, the observer’s confidence in his own judgments should 403
increase and he should be less influenced by each new observation. 404
Inferential social learning posits that people interpret information from others 405
based on an intuitive understanding of how their minds work. In the RL paradigm, 406
inverse reinforcement-learning [21,56] attempts to use the agent’s behavior to infer the407
underlying reward structure of the environment which is not directly observable [22]. 408
Inverse RL is also applied to understand how humans learn about others [7,44]. 409
However, these models lack a generative element that would explain how those actions410
came to be. By incorporating generative models into the process, learners can gain a411
richer understanding of the information they receive [57]. Bayesian learning, with its 412
probability-based generative models, has the potential to be more powerful than 413
reinforcement learning on its own. 414
Generative models are essential for understanding others’ risk preferences. 415
Predicting a decision-maker’s behavior involves understanding his attitude towards 416
risk. In a gamble, agents make decisions based on rewards and probabilities, and these417
parameters vary from trial to trial, making past decisions in isolation no longer a 418
reliable indicator of current behavior. For this reason, a generative model that takes419
into account gamble variables and a person’s attitude is required in order to assess the420
risk preference of the decision-maker and predict whether they will accept the new 421
gamble (Computational model of risky behavior). Uncovering the rationale behind a 422
decision entails tracing it back to the desires that prompted it, by turning the 423
generative model of choice on its head [58]. We used Variational Bayesian technique as424
a way of approximate reasoning to avoid complex and intractable integrals usually 425
encountered in Bayesian inference. This method, developed by Karl Friston [18] and 426
utilized in computational neuroscience, is a powerful tool for approximate reasoning, 427
which has been widely applied in neuroscience [26]. 428
Our research contributes to the ever-growing body of evidence which suggests that429
inverting generative models is a vital computational tool for social inference [58]. 430
Studies examining this phenomenon have implemented a range of formal assumptions,431
such as those conducted by [59–63]. While each of these studies have their own 432
distinct formal assumptions, they all lead to the same conclusion that inverting 433
generative models is a powerful psychological tool for social inference. 434
Methods 435
Experimental T ask 436
The experimental task that has been employed in this study is based on the work of437
Suzuki and his colleagues [30]. In each simulation run, there are two blocks, each with438
28 trials; therefore , there are 56 trials in total. The order in which trials are presented439
is random. In each trial, artificial agents are required to decide whether to accept or440
reject a risky gamble. The gamble has two possible outcomes: either receiving the 441
certain reward or not receiving it. The result of rejecting a gamble is a fixed 442
guaranteed payment. As a general rule, the reward of a gamble is greater than the 443
payment that is guaranteed. 444
Essentially, gambles are characterized by two parameters:p and r, withp being the 445
probability andr being the amount of the reward. Each of the 28 trials had its own446
August 24, 2023 16/26
combination of reward probabilityp and reward magnituder. There were three 447
possibilities for reward probabilities: 0.3, 0.4, and 0.5. We modified the actual reward448
magnitude in each trial by adding a small integer noise, ranging from -1 to 1. 449
Guaranteed payments are fixed at 10 dollars (Fig.7). 450
(a)
 (b)
Fig 7. (a) Schematic illustration of a gambling game. The reward probability
p and magnitude of the rewardr are presented in the form of a pie chart. In the pie
chart, there is a blue area indicating the likelihood of receiving a reward, and a gray
area indicating the likelihood of receiving nothing. The amount of the reward is
displayed within the blue area, which in this example is 25$.(b) A set of gambles
employed in simulation. Points correspond to gambles, which are described by
reward probabilityp and reward magnituder. The red color code is indicative of
gambles that risk-neutral individuals prefer to certain payments, and the blue color
code is indicative of gambles that risk-neutral individuals do not prefer to certain
payments. The yellow graph illustrates a curve of indifference when risk-neutral
attitudes are adopted, in which a gamble is just as valuable as a certain payment of
ten dollars. As can be seen on the right side of the plot, there are two distinct points.
These points correspond to two risk-free gambles that were used in this experiment.
Computational model of risky behavior 451
Artificial agents choices were generated by a mathematical algorithm. We assumed 452
that the simulated agents would make decisions based on the computational model 453
described here. The model is widely used to study decision making under 454
risk [11,12,64–67]. 455
Let us imagine that the decision-maker is faced with a choice between gambling or456
receiving a guaranteed monetary payment. Choosing between options requires the 457
decision maker to calculate subjective expected values. Depending on the decision 458
maker’s attitude toward risk, this evaluation may differ. According to one of the most459
common approaches based on a power utility function [68–70], the utility of the risky 460
option is denoted byUR as follows: 461
UR(r, p) = p × rρ (24)
In Eq(24), p is the probability of receiving a reward andr is the magnitude of the 462
reward. We do not use small probabilities (p < 0.3), so the subjective probability 463
distortion [69] is not relevant to our analysis.ρ refers to the participant’s risk attitude,464
August 24, 2023 17/26
which is the curvature of the utility function. Depending on whether the individual is465
risk-averse or risk-taking,ρ is lower or greater than 1. 466
Based on the principles of classical decision theory and behavioral economics, 467
people’s choices may reflect their relative preferences for different outcomes [71]. To 468
decide between the risky option and the sure option, one must compare their values469
using the functionF (·) , whereUS represents the certain payoff. 470
F = UR − US (25)
Generally, the greater the value ofF (·), the more likely it is that the decision 471
maker will prefer the risky option to the sure option. Next, a probabilistic rule called472
Softmax S(·) calculates the probability of choosing a risky option over a sure option.473
S(F (ρ)) = 1
1 + exp (−β × F (ρ)) (26)
In Eq (26), β is a non-negative free parameter that measures how much the choice474
probability relates to utility differences or how random are the subject’s decisions. 475
The probability of choosing a risky option will always be half ifβ is zero. Therefore, 476
the decision is made at random. Conversely, a largeβ suggests that the person 477
frequently makes non-random decisions. TheS(·) value tends to 1 whenF (·) is large 478
and positive. As a result, the decision-maker is certainly willing to take the risky 479
option. In contrast, whenF (·) is large and negative,S(·) tends to 0, and the 480
decision-maker rejects the risky options. 481
Lastly, a Bernoulli distribution is used to describe the choice of decision-maker,C. 482
The value ofC can be either 0 or 1. One means that the decision-maker chooses the483
risky option, and zero means that he or she rejects it. 484
P (C|ρ) = [ S (F (ρ))]C × [1 − S (F (ρ))]1−C (27)
Supporting information 485
Figure S1: γ(k) = ∂F (ρ(k))
∂ρ(k) 486
Appendix S1: Simplifying the variational energy . It is possible to simplify 487
the expressionln

1 − S
 
F (ρ(k)
in Eq(14) in the following manner: 488
ln
h
1 − S

F (ρ(k))
i
= ln
"
1 − 1
1 + exp
 
−β × F (ρ(k))

#
= ln
"
exp
 
−β × F (ρ(k))

1 + exp
 
−β × F (ρ(k))

#
= ln
h
exp

−β × F (ρ(k))
i
+ ln
"
1
1 + exp
 
−β × F (ρ(k))

#
= −β × F (ρ(k)) + ln
h
S

F (ρ(k))
i
(S1.1)
Substituting the value ofln[1 − S
 
F (ρ(k))

] in Eq(14) and carrying out the necessary 489
August 24, 2023 18/26
Fig Supp 1. γ(k) = ∂F (ρ(k))
∂ρ(k)
operations yields the following expression: 490
I(ρ(k)) = C(k) × ln
h
S

F (ρ(k))
i
+ (1 − C(k)) ×

−β × F (ρ(k)) + ln
h
S

F (ρ(k))
i
−

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
= ln
h
S

F (ρ(k))
i
+ β × (C(k) − 1) × F (ρ(k)) −

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
(S1.2)
491
Appendix S2: Obtaining a quadratic approximation of the variational 492
energy I(ρ(k)). 493
Here one can observe the second order approximation of the variational energy,
resulting in an update equation. The equation obtained forI(ρ(k)) contains
non-quadratic terms:
I(ρ(k)) = ln
h
S

F (ρ(k))
i
+ β × (C(k)
0 − 1) × F (ρ(k)) −

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
In order to approximate a Gaussian distribution, it is necessary to find a quadratic494
polynomial ˜I(ρ(k)) that approximatesI(ρ(k)). By doing so, the Gaussian distribution 495
can be accurately represented. 496
˜I(ρ(k)) = −

ρ(k) − µ(k)
ρ
2
2σ(k)
ρ
(S2.1)
When the second order Taylor series of functionI(ρ(k)) is expanded around a 497
particular expansion point, an approximate quadratic function˜I(ρ(k)) can be 498
August 24, 2023 19/26
obtained. This technique is referred to as Laplace approximation when the Taylor 499
expansion is carried out at the peak ofI(ρ(k)) [72]. An alternative site for the 500
expansion point would be the mean value ofρ(k) that has been previously updated, 501
symbolized byµ(k−1)
ρ [73]. The second suggestion is what we are going to use here. 502
Considering ˜I(ρ(k)) to be the second order Taylor expansion ofI(ρ(k)) about 503
µ(k−1)
ρ , the first and second derivatives of both˜I(ρ(k)) and I(ρ(k)) are equivalent at 504
this point: 505
 ∂I
∂ρ(k)

ρ(k)=µ(k−1)
ρ
=
 
∂ ˜I
∂ρ(k)
!
ρ(k)=µ(k−1)
ρ
 ∂2I
∂(ρ(k))2

ρ(k)=µ(k−1)
ρ
=
 
∂2 ˜I
∂(ρ(k))2
!
ρ(k)=µ(k−1)
ρ
(S2.2)
The derivatives of˜I(ρ(k)) at the pointµ(k−1)
ρ can be determined from Eq (S2.1). 506
Substituting these values into Eq(S2.2) leads to the following results: 507
 ∂I
∂ρ(k)

ρ(k)=µ(k−1)
ρ
= µ(k)
ρ − µ(k−1)
ρ
σ(k)
ρ
 ∂2I
∂(ρ(k))2

ρ(k)=µ(k−1)
ρ
= − 1
σ(k)
ρ
(S2.3)
Rearranging Eq(S2.3) yields into an updating equation forµ(k)
ρ and σ(k)
ρ : 508
µ(k)
ρ = µ(k−1)
ρ + σ(k)
ρ ×
 ∂I
∂ρ(k)

ρ(k)=µ(k−1)
ρ
1
σ(k)
ρ
= −
 ∂2I
∂(ρ(k))2

ρ(k)=µ(k−1)
ρ
(S2.4)
The final step to obtain the explicit update equations forµ(k)
ρ and σ(k)
ρ is to 509
calculate the first and second derivatives ofI(ρ(k)) with respect toµ(k−1)
ρ . We begin 510
by computing the first derivative. 511
∂I
∂ρ(k) = ∂
∂ρ(k)
8
><
>:
ln S

F (ρ(k))

+ β × (C(k)
o − 1) × F (ρ(k)) −

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
9
>=
>;
= ∂
∂ρ(k)
n
ln S

F (ρ(k))

+ β × (C(k)
o − 1) × F (ρ(k))
o
− ∂
∂ρ(k)
8
><
>:

ρ(k) − µ(k−1)
ρ
2
2σ(k−1)
ρ
9
>=
>;
= ∂
∂ρ(k)
n
ln S

F (ρ(k))
o
+ β × (C(k)
o − 1) × ∂F (ρ(k))
∂ρ(k) − ρ(k) − µ(k−1)
ρ
σ(k−1)
ρ
(S2.5)
Using a chain rule, it is possible to derive∂
∂ρ(k)

ln

S
 
F (ρ(k))
	
as follows: 512
∂
∂ρ(k)
n
ln
h
S

F (ρ(k))
io
= 1
S
 
F (ρ(k))
× ∂S
∂F (ρ(k)) × ∂F (ρ(k))
∂ρ(k) (S2.6)
August 24, 2023 20/26
where ∂S
∂F (ρ(k)) is: 513
∂S
∂F (ρ(k)) = β ×
h
S

F (ρ(k))
i
×
h
1 − S

F (ρ(k))
i
(S2.7)
Finally we have: 514
∂
∂ρ(k)
n
ln
h
S

F (ρ(k))
io
= β × ∂F (ρ(k))
∂ρ(k) ×
h
1 − S

F (ρ(k))
i
(S2.8)
The following equation for ∂I
∂ρ(k) can be calculated by substituting the value 515
achieved for ∂
∂ρ(k)

ln

S
 
F (ρ(k))
	
in the Eq (S2.5). 516
∂I
∂ρ(k) = β × ∂F (ρ(k))
∂ρ(k)
h
1 − S

F (ρ(k))
i
+ β × ∂F (ρ(k))
∂ρ(k) × (Ck
o − 1) − ρ(k) − µ(k−1)
ρ
σ(k−1)
ρ
= β × ∂F (ρ(k))
∂ρ(k) ×
h
Ck
o − S

F (ρ(k))
i
− ρ(k) − µ(k−1)
ρ
σ(k−1)
ρ
(S2.9)
By differentiating Eq (S2.9), we can obtain the expression for∂2I
∂(ρ(k))2 : 517
∂2I
∂(ρ(k))2 = ∂
∂ρ(k)
 ∂I
∂ρ(k)

= ∂
∂ρ(k)
 
β × ∂F (ρ(k))
∂ρ(k) ×
h
Ck
o − S

F (ρ(k))
i
− ρ(k) − µ(k−1)
ρ
σ(k−1)
ρ
!
= ∂
∂ρ(k)

β × ∂F (ρ(k))
∂ρ(k) ×
h
Ck
o − S

F (ρ(k))
i
− ∂
∂ρ(k)
 
ρ(k) − µ(k−1)
ρ
σ(k−1)
ρ
!
= β × ∂2F (ρ(k))
∂(ρ(k))2 ×
h
Ck
o − S

F (ρ(k))
i
− β ×
∂F (ρ(k))
∂ρ(k)
2
× ∂S
∂F (ρ(k)) − 1
σ(k−1)
ρ
(S2.10)
Omitting the initial term which incorporates the second order derivative∂2F(ρ(k))
∂(ρ(k))2 518
yields a more straightforward formula: 519
∂2I
∂(ρ(k))2 ≊ −β2 ×
∂F (ρ(k))
∂ρ(k)
2
×
h
S

F (ρ(k))
i
×
h
1 − S

F (ρ(k))
i
− 1
σ(k−1)
ρ
(S2.11)
The value of ∂2I
∂(ρ(k))2 is always kept at a negative level due to the presence of negative520
components. This ensures thatσ(k)
ρ is maintained at a positive value, which is a 521
necessary requirement based on its underlying characteristics. 522
Lastly, by substituting the values of∂I
∂ρ(k) and ∂2I
∂(ρ(k))2 at the pointρ(k) = µ(k−1)
ρ 523
into Equation Eq(S2.4), the update equation forµ(k)
ρ and σ(k)
ρ can be obtained: 524
µ(k)
ρ = µ(k−1)
ρ + β ×
0
@∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
1
A× σ(k)
ρ ×

C(k)
0 − S

F (µ(k−1)
ρ )

1
σ(k)
ρ
= 1
σ(k−1)
ρ
+ β2 ×
0
@∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
1
A
2
×
h
S

F (µ(k−1)
ρ )
i
×
h
1 − S

F (µ(k−1)
ρ )
i
(S2.12)
August 24, 2023 21/26
Where the value of∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
is: 525
∂F (ρ(k))
∂ρ(k)

ρ(k)=µ(k−1)
ρ
= ∂
∂ρ(k)
n
pk × (rk)ρk
− Us
o
ρ(k)=µ(k−1)
ρ
= pk × ln rk × (rk)µ(k−1)
ρ
(S2.13)
See Eqs. (24),(25) for the definition of functionF (·). 526
Acknowledgments 527
We are thankful to Shinsuke Suzuki for providing us access to the experimental 528
dataset utilized in this study. 529
References
1. Bandura A. Observational learning. The international encyclopedia of
communication. 2008;.
2. Kolb DA. Toward an applied theory of experiential learning. Theories of group
processes. 1975;.
3. Dunne S, O’Doherty JP. Insights from the application of computational
neuroimaging to social neuroscience. Current opinion in neurobiology.
2013;23(3):387–392.
4. Charpentier CJ, O’Doherty JP. The application of computational models to
social neuroscience: promises and pitfalls. Social neuroscience.
2018;13(6):637–647.
5. Bandura A, Ross D, Ross SA. Vicarious reinforcement and imitative learning.
The Journal of abnormal and social psychology. 1963;67(6):601.
6. King-Casas B, Tomlin D, Anen C, Camerer CF, Quartz SR, Montague PR.
Getting to know you: reputation and trust in a two-person economic exchange.
Science. 2005;308(5718):78–83.
7. Boorman ED, O’Doherty JP, Adolphs R, Rangel A. The behavioral and neural
mechanisms underlying the tracking of expertise. Neuron. 2013;80(6):1558–1571.
8. Wittmann MK, Kolling N, Faber NS, Scholl J, Nelissen N, Rushworth MF.
Self-other mergence in the frontal cortex during cooperation and competition.
Neuron. 2016;91(2):482–493.
9. Diaconescu AO, Mathys C, Weber LA, Daunizeau J, Kasper L, Lomakina EI,
et al. Inferring on the intentions of others by hierarchical Bayesian learning.
PLoS computational biology. 2014;10(9):e1003810.
10. Garvert MM, Moutoussis M, Kurth-Nelson Z, Behrens TE, Dolan RJ.
Learning-induced plasticity in medial prefrontal cortex predicts preference
malleability. Neuron. 2015;85(2):418–428.
August 24, 2023 22/26
11. Devaine M, Daunizeau J. Learning about and from others’ prudence,
impatience or laziness: The computational bases of attitude alignment. PLoS
computational biology. 2017;13(3):e1005422.
12. Blankenstein NE, Crone EA, van den Bos W, van Duijvenvoorde AC. Dealing
with uncertainty: Testing risk-and ambiguity-attitude across adolescence.
Developmental neuropsychology. 2016;41(1-2):77–92.
13. Defoe IN, Dubas JS, Figner B, Van Aken MA. A meta-analysis on age
differences in risky decision making: adolescents versus children and adults.
Psychological bulletin. 2015;141(1):48.
14. Dayan P, Hinton GE, Neal RM, Zemel RS. The helmholtz machine. Neural
computation. 1995;7(5):889–904.
15. Schwartenbeck P, FitzGerald TH, Mathys C, Dolan R, Wurst F, Kronbichler M,
et al. Optimal inference with suboptimal models: addiction and active Bayesian
inference. Medical hypotheses. 2015;84(2):109–117.
16. Geisler WS, Diehl RL. Bayesian natural selection and the evolution of
perceptual systems. Philosophical Transactions of the Royal Society of London
Series B: Biological Sciences. 2002;357(1420):419–448.
17. Friston K, Kilner J, Harrison L. A free energy principle for the brain. Journal of
physiology-Paris. 2006;100(1-3):70–87.
18. Friston K. The free-energy principle: a unified brain theory? Nature reviews
neuroscience. 2010;11(2):127–138.
19. Knill DC, Pouget A. The Bayesian brain: the role of uncertainty in neural
coding and computation. TRENDS in Neurosciences. 2004;27(12):712–719.
20. Pouget A, Beck JM, Ma WJ, Latham PE. Probabilistic brains: knowns and
unknowns. Nature neuroscience. 2013;16(9):1170–1178.
21. Ng AY, Russell S, et al. Algorithms for inverse reinforcement learning. In: Icml.
vol. 1; 2000. p. 2.
22. Collette S, Pauli WM, Bossaerts P, O’Doherty J. Neural computations
underlying inverse reinforcement learning in the human brain. Elife.
2017;6:e29718.
23. Behrens TE, Woolrich MW, Walton ME, Rushworth MF. Learning the value of
information in an uncertain world. Nature neuroscience. 2007;10(9):1214–1221.
24. Piray P, Daw ND. A simple model for learning in volatile environments. PLoS
computational biology. 2020;16(7):e1007963.
25. Daunizeau J, Den Ouden HE, Pessiglione M, Kiebel SJ, Stephan KE, Friston
KJ. Observing the observer (I): meta-bayesian models of learning and
decision-making. PloS one. 2010;5(12):e15554.
26. Zeidman P, Friston K, Parr T. A Primer on Variational Laplace (VL).
NeuroImage. 2023; p. 120310.
27. Friston KJ, Stephan KE. Free-energy and the brain. Synthese.
2007;159:417–458.
August 24, 2023 23/26
28. Jaynes ET. Information theory and statistical mechanics. Physical review.
1957;106(4):620.
29. Rescorla RA, Wagner AR. A theory of Pavlovian conditioning: Variations in
the effectiveness of reinforcement and non-reinforcement. Classical conditioning,
Current research and theory. 1972;2:64–69.
30. Suzuki S, Jensen EL, Bossaerts P, O’Doherty JP. Behavioral contagion during
learning about another agent’s risk-preferences acts on the neural representation
of decision-risk. Proceedings of the National Academy of Sciences.
2016;113(14):3755–3760.
31. Frässle S, Aponte EA, Bollmann S, Brodersen KH, Do CT, Harrison OK, et al.
TAPAS: an open-source software package for translational neuromodeling and
computational psychiatry. Frontiers in psychiatry. 2021;12:680811.
32. Stephan KE, Penny WD, Daunizeau J, Moran RJ, Friston KJ. Bayesian model
selection for group studies. Neuroimage. 2009;46(4):1004–1017.
33. Penny WD. Comparing dynamic causal models using AIC, BIC and free energy.
Neuroimage. 2012;59(1):319–330.
34. Rigoux L, Stephan KE, Friston KJ, Daunizeau J. Bayesian model selection for
group studies—revisited. Neuroimage. 2014;84:971–985.
35. Daunizeau J, Adam V, Rigoux L. VBA: a probabilistic treatment of nonlinear
models for neurobiological and behavioural data. PLoS computational biology.
2014;10(1):e1003441.
36. Williams RJ. Simple statistical gradient-following algorithms for connectionist
reinforcement learning. Machine learning. 1992;8:229–256.
37. Sutton R, Barto A. Reinforcement Learning: An Introduction. MIT Press; 1998.
38. Nassar MR, Wilson RC, Heasly B, Gold JI. An approximately Bayesian
delta-rule model explains the dynamics of belief updating in a changing
environment. Journal of Neuroscience. 2010;30(37):12366–12378.
39. Niv Y. Reinforcement learning in the brain. Journal of Mathematical
Psychology. 2009;53(3):139–154.
40. FeldmanHall O, Nassar MR. The computational challenge of social learning.
Trends in Cognitive Sciences. 2021;25(12):1045–1057.
41. Burke CJ, Tobler PN, Baddeley M, Schultz W. Neural mechanisms of
observational learning. Proceedings of the National Academy of Sciences.
2010;107(32):14431–14436.
42. Behrens TE, Hunt LT, Woolrich MW, Rushworth MF. Associative learning of
social value. Nature. 2008;456(7219):245–249.
43. van den Bos W, Talwar A, McClure SM. Neural correlates of reinforcement
learning and social preferences in competitive bidding. Journal of Neuroscience.
2013;33(5):2137–2146.
44. Hackel LM, Doll BB, Amodio DM. Instrumental learning of traits versus
rewards: dissociable neural correlates and effects on choice. Nature
Neuroscience. 2015;18(9):1233–1235.
August 24, 2023 24/26
45. FeldmanHall O, Otto AR, Phelps EA. Learning moral values: Another’s desire
to punish enhances one’s own punitive behavior. Journal of Experimental
Psychology: General. 2018;147(8):1211.
46. Meyniel F, Schlunegger D, Dehaene S. The sense of confidence during
probabilistic learning: A normative account. PLoS computational biology.
2015;11(6):e1004305.
47. Payzan-LeNestour E, Bossaerts P. Risk, unexpected uncertainty, and estimation
uncertainty: Bayesian learning in unstable settings. PLoS computational
biology. 2011;7(1):e1001048.
48. Meyniel F, Sigman M, Mainen ZF. Confidence as Bayesian probability: From
neural origins to behavior. Neuron. 2015;88(1):78–92.
49. Yeung N, Summerfield C. Metacognition in human decision-making: confidence
and error monitoring. Philosophical Transactions of the Royal Society B:
Biological Sciences. 2012;367(1594):1310–1321.
50. Gelman A, Carlin JB, Stern HS, Dunson D, Vehtari A, Rubin D. Bayesian data
analysis; 2013.
51. Koriat A, Sheffer L, Ma’ayan H. Comparing objective and subjective learning
curves: judgments of learning exhibit increased underconfidence with practice.
Journal of Experimental Psychology: General. 2002;131(2):147.
52. Barthelmé S, Mamassian P. Evaluation of objective uncertainty in the visual
system. PLoS computational biology. 2009;5(9):e1000504.
53. Apps MA, Rushworth MF, Chang SW. The anterior cingulate gyrus and social
cognition: tracking the motivation of others. Neuron. 2016;90(4):692–707.
54. Apps MA, Sallet J. Social learning in the medial prefrontal cortex. Trends in
Cognitive Sciences. 2017;21(3):151–152.
55. FeldmanHall O, Shenhav A. Resolving uncertainty in a social world. Nature
human behaviour. 2019;3(5):426–435.
56. Abbeel P, Ng AY. Apprenticeship learning via inverse reinforcement learning.
In: Proceedings of the twenty-first international conference on Machine learning;
2004. p. 1.
57. Vélez N, Gweon H. Learning from other minds: An optimistic critique of
reinforcement learning models of social learning. Current opinion in behavioral
sciences. 2021;38:110–115.
58. Jern A, Lucas CG, Kemp C. People learn other people’s preferences through
inverse decision-making. Cognition. 2017;168:46–64.
59. Baker CL, Jara-Ettinger J, Saxe R, Tenenbaum JB. Rational quantitative
attribution of beliefs, desires and percepts in human mentalizing. Nature
Human Behaviour. 2017;1(4):0064.
60. Baker CL, Saxe R, Tenenbaum JB. Action understanding as inverse planning.
Cognition. 2009;113(3):329–349.
61. Ullman T, Baker C, Macindoe O, Evans O, Goodman N, Tenenbaum J. Help or
hinder: Bayesian models of social goal inference. Advances in neural
information processing systems. 2009;22.
August 24, 2023 25/26
62. Tauber S, Steyvers M. Using inverse planning and theory of mind for social goal
inference. In: Proceedings of the annual meeting of the cognitive science society.
vol. 33; 2011.
63. Jern A, Kemp C. A decision network account of reasoning about other people’s
choices. Cognition. 2015;142:12–38.
64. Braams BR, Davidow JY, Somerville LH. Information about others’ choices
selectively alters risk tolerance and medial prefrontal cortex activation across
adolescence and young adulthood. Developmental Cognitive Neuroscience.
2021;52:101039.
65. Tymula A, Rosenberg Belmaker LA, Roy AK, Ruderman L, Manson K,
Glimcher PW, et al. Adolescents’ risk-taking behavior is driven by tolerance to
ambiguity. Proceedings of the National Academy of Sciences.
2012;109(42):17135–17140.
66. Levy I, Snell J, Nelson AJ, Rustichini A, Glimcher PW. Neural representation
of subjective value under risk and ambiguity. Journal of neurophysiology.
2010;103(2):1036–1047.
67. Hsu M, Bhatt M, Adolphs R, Tranel D, Camerer CF. Neural systems
responding to degrees of uncertainty in human decision-making. Science.
2005;310(5754):1680–1683.
68. Bernoulli D. Exposition of a New Theory on the Measurement of Risk.
Econometrica. 1954;22(1):23–36.
69. Tversky A, Kahneman D. Advances in prospect theory: Cumulative
representation of uncertainty. Journal of Risk and uncertainty. 1992;5:297–323.
70. Kahneman D, Tversky A. Prospect Theory: An Analysis of Decision under
Risk. Econometrica. 1979;47(2):263–292.
71. Samuelson PA. Consumption theory in terms of revealed preference.
Economica. 1948;15(60):243–253.
72. Friston K, Mattout J, Trujillo-Barreto N, Ashburner J, Penny W. Variational
free energy and the Laplace approximation. Neuroimage. 2007;34(1):220–234.
73. Mathys C, Daunizeau J, Friston KJ, Stephan KE. A Bayesian foundation for
individual learning under uncertainty. Frontiers in human neuroscience.
2011;5:39.
August 24, 2023 26/26