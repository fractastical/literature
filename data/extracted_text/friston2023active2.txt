Active Inference and Intentional Behaviour
Karl J. Friston1,2, Tommaso Salvatori2, Takuya Isomura3, Alexander
Tschantz2, Alex Kiefer2,4, Tim Verbelen2, Magnus Koudahl2, Aswin Paul2,6,9,
Thomas Parr4, Adeel Razi6,7,8, Brett Kagan10, Christopher L. Buckley2, and
Maxwell J. D. Ramstead1,2
1Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, UK.
2VERSES AI Research Lab, Los Angeles, California, 90016, USA
3Brain Intelligence Theory Unit, RIKEN Center for Brain Science, Wako, Saitama, Japan
4Nuffield Department of Clinical Neurosciences, University of Oxford, UK
5Monash Centre for Consciousness and Contemplative Studies, Melbourne, Australia
6Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash
University, Clayton, Australia
7Monash Biomedical Imaging, Monash University, Clayton, Australia
8CIFAR Azrieli Global Scholars Program, Toronto, Canada
9IITB-Monash Research Academy, Mumbai-76, India
10Cortical Labs, Melbourne, Australia
Abstract
Recent advances in theoretical biology suggest that basal cognition and sentient
behaviour are emergent properties ofin vitro cell cultures and neuronal networks, re-
spectively. Such neuronal networks spontaneously learn structured behaviours in the
absence of reward or reinforcement. In this paper, we characterise this kind of self-
organisation through the lens of the free energy principle, i.e., as self-evidencing. We
do this by first discussing the definitions of reactive and sentient behaviour in the
setting of active inference, which describes the behaviour of agents that model the con-
sequences of their actions. We then introduce a formal account ofintentional behaviour,
that describes agents as driven by a preferred endpoint or goal in latent state-spaces.
We then investigate these forms of (reactive, sentient, and intentional) behaviour us-
ing simulations. First, we simulate the aforementionedin vitro experiments, in which
neuronal cultures spontaneously learn to play Pong, by implementing nested, free en-
ergy minimising processes. The simulations are then used to deconstruct the ensuing
predictive behaviour—leading to the distinction between merely reactive, sentient, and
intentional behaviour, with the latter formalised in terms of inductive planning. This
distinction is further studied using simple machine learning benchmarks (navigation in
a grid world and the Tower of Hanoi problem), that show how quickly and efficiently
adaptive behaviour emerges under an inductive form of active inference.
Keywords: active inference; active learning; backwards induction;planning as inference;
free energy principle.
1
arXiv:2312.07547v2  [q-bio.NC]  16 Dec 2023
1 Introduction
In 2022, a paper was published that claimed to demonstrate sentient behaviour in a neuronal
culture grown in a dish (an in vitro neuronal network) [1]. The behaviour in question
was the spontaneous emergence of controlled movements of a paddle to hit a ball—and
thereby play Pong. This study has several sources of inspiration that speak to the notion
of basal cognition; see, e.g., [2–4] (and related work, e.g. [5]). In particular, the hypothesis
that adaptive and predictive behaviour would emerge spontaneously was based on earlier
work showing thatin vitro neuronal cultures could be described as minimising variational
free energy [6] and thereby evince active inference and learning. This application of the
free energy principle (FEP) to neuronal cultures was subsequently validated empirically
[7]: in the sense that changes in neuronal activity and synaptic efficacy—that underwrite
learning—could be predicted quantitatively, as a variational free energy minimising process.
So, are these findings remarkable, or were they predictable?
In one sense, these results were entirely predictable. Indeed, they were predictable from
the FEP, which states that any two networks—that are coupled in a certain sparse fash-
ion—will come to manifest a generalised synchrony [8, 9]. More formally, the FEP states
that if the probability density that underwrites the dynamics of coupled random dynamical
systems contains a Markov blanket—which shields internal states from external states, given
blanket (sensory and active) states—then internal states will look as if they track the statis-
tics of external states—or more precisely, as if they encode the parameters of a variational
density (or best guess about) external states beyond the blanket. Empirically, this synchro-
nisation was observed when the neuronal cultures learned to play Pong. However, the FEP
goes further and says that the internal and active states (together, autonomous states) of
either network can be described as minimising a variational free energy functional. This
functional is exactly the same used to optimise generative models in statistics and machine
learning [10]. On this reading, one can interpret the autonomous states—of a network, parti-
cle or person—as minimising variational free energy or surprise (a.k.a., self-information) or,
equivalently, maximising Bayesian model evidence (a.k.a., the marginal likelihood of sensory
states). Thisleadstoanimplicitteleology, inthesensethatonecandescribeself-organisation
in terms of self-evidencing [11] that entails active inference and learning, planning, purpose,
intentions and, perhaps, sentience. The underlying free energy minimising processes—and
their teleological interpretation—are the focus of this paper.
The results reported in [1] were considered by some to be unremarkable for a different
reason: learning to play (Atari) games like Pong was something that had been accomplished
with machine learning systems years earlier using neural networks and (deep) reinforcement
learning [12, 13]. So, what is remarkable about a neuronal network reproducing the same
kind of behaviour? It is remarkable because one cannot use the reinforcement learning (RL)
paradigm to explain the emergence of self-evidencing behaviour seenin vitro. This follows
from the fact that one cannot reward a neuronal network—because no one knows what any
given in vitroneuronal network finds rewarding. However, the FEP theorist knows exactly
what a self-evidencing network finds aversive; namely, surprise and unpredictability. This
was a rationale for delivering unpredictable noise to the sensory electrodes of the cell culture
2
(or restarting the game in an unpredictable way), whenever the neuronal network failed to
hit the ball [1].
Some found the results reported in [1] remarkable, but not in a good way: they disagreed
withtheclaimthatthebehaviourcouldbedescribedas‘sentient’[14]. Here, wehopetomake
sense of the notion of sentient behaviour in terms of Bayesian belief updating; where ‘sentient
behaviour’ denotes the capacity to generate appropriate responses to sensory perturbations
(as opposed to merely reactive behaviour). We pursue the narrative established by the cell
culture experiments above to illustrate why Pong-playing behaviour was considered sentient,
as opposed to reactive. In brief, we consider a bright line between actions based upon the
predictions of a generative model that does, and does not, entail the consequences of action.
Specifically, this paper differentiates between three kinds of behaviour:reactive, sentient,
and intentional. The first two have formulations that have been extensively studied in
the literature, under the frameworks of model-free reinforcement learning (RL) and active
inference, respectively. In model-free RL, the system selects actions using either a lookup
table (Q-learning), or a neural network (deep Q-learning). In standard active inference,
the action selection depends on the expected free energy of policies (Equation 2), where the
expectationisover observations in the futurethatbecomerandomvariables. Thismeansthat
preferredoutcomes—thatsubtendexpectedcostandrisk—arepriorbeliefsthatconstrainthe
implicit planning as inference [15–17]. Things that evince this kind of behaviour can hence
be described as planning their actions, based upon a generative model of the consequences
of those actions [15, 16, 18]. It was this sense in which the behaviour of the cell cultures was
considered sentient.
This form of sentient behaviour —described in terms of Bayesian mechanics [19–21]—can
be augmented with intended endpoints or goals. This leads to a novel kind of sentient
behaviour that not only predicts the consequences of its actions, but is also able to select
them to reach a goal state that may be many steps in the future. This kind of behaviour, that
we callintentional behaviour, generally requires some form of backwards induction [22, 23]
of the kind found in dynamic programming [24–27]: this is, starting from the intended goal
state, and working backwards, inductively, to the current state of affairs, in order to plan
moves to that goal state. Backwards induction was applied to the partially observable setting
and explored in the context of active inference in [27]. In that work, dynamic programming
was shown to be more efficient than traditional planning methods in active inference.
The focus of this work is to formally define a framework for intentional behaviour, where
the agent minimises a constrained form of expected free energy—and to demonstrate this
framework in silico. These constraints are defined on a subset of latent states that represent
the intended goals of the agent, and propagated to the agent via a form of backward induc-
tion. As a result, states that do not allow the agent to make any ‘progress’ towards one of the
intended goals are penalised, and so are actions that lead to such disfavoured states. This
leads to a distinction between sentient and intentional behaviour, were intentional behaviour
is equipped with inductive constraints.
In this treatment, the wordinductive is used in several senses. First, to distinguish in-
ductive planning from the abductive kind of inference that usually figures in applications
3
of Bayesian mechanics; i.e., to distinguish between mere inference to the best explanation
(abductive inference) and genuinely goal-directed inference (inductive planning) [28, 29].
Second, it is used with a nod to backwards induction in dynamic programming, where one
starts from an intended endpoint and works backwards in time to the present, to decide what
to do next [24, 25, 27, 30]. Under this naturalisation of behaviours, a thermostat would not
exhibit sentient behaviour, but insects might (i.e., thermostats exhibit merely reactive be-
haviour). Similarly, insects would not exhibit intentional behaviour, but mammals might
(i.e., insects exhibit merely sentient behaviour). The numerical analyses presented below
suggest thatin vitroneuronal cultures may exhibit sentient behaviour, but not intentional
behaviour. Crucially, we show that neither sentient nor intentional behaviour can be ex-
plained by reinforcement learning. In the experimental sections of this work, we study and
compare the performance of active inference agents with and without intended goal states.
For ease of reference, we will call active inference agents without goal statesabductive agents,
and agents with intended goalsinductive agents.
This paper comprises four sections. The first briefly rehearses active inference and learn-
ing—as a set of nested free energy minimising processes—applied to a generic generative
model of exchange with some world or environment. This model is a partially observed
Markov decision process that is conciliatory with canonical neural networks in machine
learning and apt to describe the self-evidencing of in vitro neuronal networks [6, 7]. This
section has a special focus on inductive planning and its relationship to expected free energy.
The subsequent sections use numerical studies to make a series of key points. The second
section reproduces the empirical behaviour ofin vitroneuronal networks playing Pong. Cru-
cially, this behaviour emerges purely in terms of free energy minimising processes, starting
with a naïve neuronal network. This section illustrates the failure of a (simulated) abductive
agent when the game is made more difficult. This failure is used to illustrate the role of
inductive planning, which restores performance and underwrites a fluent engagement with
the sensorium. The final two sections illustrate inductive planning using navigation in a
maze and the Tower of Hanoi problem, respectively. These numerical studies illustrate how
the simple application of inductive constraints to active inference allows tasks—that would
be otherwise intractable in discrete state spaces—to be solved efficiently. This efficiency
rests on the fact that distal goals can be reached by only planning a few steps in the future,
thanks to constraints furnished by inductive planning. Effectively, inductive planning takes
the pressure off deep tree searches by identifying ’blind alleys’ or ’dead ends’.
1.1 Glossary of definitions
Before introducing the inductive planning algorithm, we frame our treatment by clarifying
our use of some key terms. This framing is important, given that the goal of the present work
is not simply to describe a useful heuristic for efficient inference (i.e., inductive planning), but
to provide an account of how a new form of decision-making, characteristic of more complex
forms of agency, may be combined with, and folded into, a generic Bayesian (active) inference
scheme.
Figure 1 describes increasingly complex forms of behaviour—from reactive (merely re-
4
sponding to stimuli), to sentient (planning based on thesensory consequencesof actions),
to intentional (planning in order to bring aboutintended states)—and corresponding forms
of decision-making that may underwrite such behaviour.
Reactive behaviourcharacterises simple sensorimotor reflex arcs and the mere reali-
sation of set points or trajectories (e.g., simple cases of homeostasis and homeorhesis). This
form of behaviour can be accounted for acting in a way that realises predicted sensations,
with no anticipation of the future sensory consequences of action.
Sentient behaviourcharacterises the paradigmatic case of active inference, in which the
influence of perception on action is mediated by the results of planning, with a distribution
over policies derived from a model endowed with counterfactual depth (i.e., beliefs about the
future sensory consequences of action pursuant to a policy). In this case, we may characterise
the form of inference over actions or policies asabductive—i.e., as an inference to the policy
that best explains current and future observations under a generative model (see below).
Intentional behaviour is driven not simply by the generic imperative to minimise
sensory prediction error, present and future, but toward the attainment of a particular future
endpoint or goal state. This form of behaviour can be subserved by backward induction or
inductive planning, as defined below, which supplies a specific form of constraint on the
Bayesian (abductive) inference characteristic of (mere) sentient behaviour. In particular, it
implies not merely beliefs about sensory consequences of actions but rather beliefs about the
inferred or latent causes of sensory input.
Note that words like ‘sentient behaviour’ and ‘intentional behaviour’ are deliberately
defined here such that they can be operationalized within the framework of generative mod-
elling, in which terms like ‘state’, ‘belief’, and ‘confidence’ have precise, if narrow, interpreta-
tions in terms of belief structures of a mathematical sort [31]. Whether the phenomenology
of (propositional or subjective) beliefs—or sentience—could yield to the same naturalisa-
tion remains to be seen: see [32–34] for treatments in this direction. Note further that a
key distinction between sentient and intentional behaviour rests upon the consequences of
behaviour in (observable) outcome and (unobservable) latent spaces, respectively.
2 Active inference
Here, we introduce the generative model used in the following sections, which can be seen
as a generalisation of a partially observed Markov decision process (POMDP). The gener-
alisation in question covers trajectories, narratives or syntax—which may or may not be
controllable—by equipping a POMDP with random variables calledpaths. Paths effectively
pick out transitions among latent states. These models are designed to be composed hierar-
chically, in a way that speaks to a separation of temporal scales in deep generative models.
In other words, the number of transitions among latent states at any given level is greater
than the number of transitions at the level above. This furnishes a unique specification of
a hierarchy, in which the parents of any latentfactor (associated with unique states and
paths) contextualise the dynamics of their children.
The variational inference scheme [35] used to invert these models inherits from their ap-
5
Ambiguity
Expected Free Energy for POMDPs11 1 1( ) [ ( | ) || ( | )] [ln ( | , )]uKL QGu D Qo u Po c Qo s utt t t++ + +=-E!"""" "#""""" $!""" "#"""" $Risk
Sentient BehaviourAction selection based on the inferred consequences of action
Planning as inference under objective constraints or preferences over outcomes
() ( )Pus=-GIntentional BehaviourAction selection constrained by intended endpoint or goal
Inductive Planning under subjective constraints or preferences over latent states
() ( )Pus=--GHReactive BehaviourActions are selected in response to an observed state
Examples:Q-learning
() ( | )Pu sts=Q
Equivalent to active inference for MDPs
11( ) [ ( | ) || ( | )]KLQu D Qs u Ps ctt++=!""""#""""$Risk
KL (risk sensitive) control as inference
Figure 1: Glossary.In this figure, we provide illustrative definitions of the three kinds of
behaviour considered in this work, In terms of examples, and mathematical differences. Ex-
amples of agents with reactive behaviours are (1) Model-free reinforcement learning schemes,
such as Q-learning, where the agent makes use of a lookup table to select actions (more gen-
erally, a state-action policy). In this table, rows correspond to states, actions to columns,
and every entry encodes the value of taking a specific action (in this case: go up, right, down,
left) when in statesτ . There is no inference over policies, as for every state the agent auto-
matically selects the action with the highest value; and (2) KL control (a.k.a., risk-sensitive
control) methods, that automatically select actions that minimise a KL divergence between
anticipated and preferred states (where there is no uncertainty about the current state).
Sentient agents, on the other hand, plan by taking into account future outcomes and their
uncertainty, as they act by minimising an expected free energyG, that includes risk and
ambiguity terms. More details on this can be found in Equation 5. Finally, inductive agents
add constraints (H in the figure) in the action selection, by penalising actions that preclude
an intended goal. For a formal derivation ofH, we refer to Section 3.
plication to online decision-making tasks. This means that action selection rests primarily
on current beliefs about latent states and structures, and expectations about future obser-
vations. In that sense, the beliefs are updated sequentially—and in an online fashion—with
each newaction-outcome pair. This calls forBayesian filtering (i.e., forward messagepassing)
during the active sampling of observations, followed by Bayesian smoothing (i.e., forward
and backward message passing) to revise posterior beliefs about past states at the end of
6
an epoch. The implicit Bayesian smoothing ensures that the beliefs about latent states at
any moment in the past are informed by all available observations when updating model
parameters (and latent states of parents in deep models).
In neurobiology, this combination of Bayesian filtering and smoothing would correspond
to evidence accumulation during active engagement with the environment, followed by a
‘replay’ before the next epoch [36–39]. From a machine learning perspective, this can be
regarded as a forward pass (c.f., belief propagation) for online active inference, followed
by a backwards pass (implemented with variational message passing) for active learning.
The implicit belief updates, pertaining to states, parameters and structure, foreground the
conditional dependencies between active inference, learning, and selection, respectively.
Generative modelling
Active inference rests upon agenerative modelof observable outcomes (observations). This
model is used to infer the most likely causes of outcomes in terms of expected states of
the world. These states (and paths) are latent orhidden because they can only be inferred
through observations. Some paths are controllable in the sense they can be realised through
action. Therefore, certain observations depend upon action (e.g., where one is looking),
whichrequires thegenerativemodel toentertainexpectations aboutoutcomes underdifferent
combinations of actions (i.e., policies)1.
These expectations are optimised by minimising thevariational free energy, defined in
Equation(1). Variationalfreeenergyscoresthediscrepancybetweenthedataexpectedunder
the generative model and the actual data. Crucially, the prior probability of a policy depends
upon itsexpected free energy. Expected free energy, described in more detail in Equation (2),
is a universal objective function that can be read as augmenting mutual information with a
expected costs or constraints that need to be satisfied. Heuristically, it scores the free energy
expected under each course of action. Having evaluated the expected free energy of each
policy, the most likely action can be selected and the perception-action cycle continues [40].
The generative model
Figure 2 provides a schematic overview of the generative model used for the simulations
considered in this paper. Outcomes at any particular time depend upon hiddenstates, while
transitions among hidden states depend uponpaths. Note that paths are random variables,
in the sense that a particle can have both a position (i.e., a state) and momentum (i.e., a
path). Paths may or may not depend upon action. The resulting POMDP is specified by a
set of tensors. The first set of parameters, denotedA, maps from hidden states to outcome
modalities; for example, exteroceptive (e.g., visual) or proprioceptive (e.g., eye position)
modalities. These parameters encode the likelihood of an outcome given their hidden causes.
1Note that in this setting, a policy is not a sequence of actions, but simply a combination of paths, where
each hidden factor has an associated state and path. This means there are, potentially, as many policies as
there are combinations of paths.
7
()()()()()1100|, ( )|, , ( )|, ( )|( )|( )() ( )() ( )P o s a CatP s s u b CatP u u c CatP s d CatP u e CatPD i r aPD i r btttt ttt--=======ABCDEAB! C
A A AB B
D
B0u0s0o1o2o1s2s1u2uE C
Generative model
Figure 2: Generative models as agents.A generative model specifies the joint proba-
bility of observable consequences and their hidden causes. Usually, the model is expressed
in terms of alikelihood (the probability of consequences given their causes) andpriors (over
causes). When a prior depends upon a random variable it is called anempirical prior. Here,
the likelihood is specified by a tensorA, encoding the probability of an outcome under every
combination ofstates (s). The empirical priors pertain to transitions among hidden states,
B, that depend uponpaths (u), whose transition probabilities are encoded inC. E specifies
the empirical prior probability of each path. The subscripts in this graphic pertain to time.
The second setB prescribes transitions among the hidden states of afactor, under a partic-
ular path. Factors correspond to different kinds of causes; e.g., the location versus the class
of an object. The remaining tensors encode prior beliefs about pathsC, and initial states
D. The tensors—encoding probabilistic mappings or contingencies—are generally param-
eterised as Dirichlet distributions, whose sufficient statistics are concentration parameters
or Dirichlet counts. These count the number of times a particular combination of states or
outcomes has been inferred. We will focus on learning the likelihood model, encoded by
Dirichlet counts,a.
The generative model in Figure 2 means that outcomes are generated as follows: first,
a policy is selected using a softmax function of expected free energy. Sequences of hidden
states are generated using the probability transitions specified by the selected combination of
paths (i.e., policy). Finally, these hidden states generate outcomes in one or more modalities.
Perception or inference about hidden states (i.e., state estimation) corresponds to inverting
a generative model, given a sequence of outcomes, while learning corresponds to updating
model parameters. Perception therefore corresponds to updating beliefs about hidden states
and paths, while learning corresponds to accumulating knowledge in the form of Dirichlet
counts. The requisite expectations constitute the sufficient statistics(s, u, a) of posterior be-
liefs Q(s, u, a) =Qs(s)Qu(u)Qa(a). The implicit factorisation of this approximate posterior
effectively partitions model inversion into inference, planning, and learning.
8
Variational free energy and inference
In variational Bayesian inference (a form of approximate Bayesian inference), model inversion
entails the minimisation of variational free energy with respect to the sufficient statistics of
approximate posterior beliefs. This can be expressed as follows, where, for clarity, we will
deal with a single factor, such that the policy (i.e., combination of paths) becomes the path,
π = u. Omitting dependencies on previous states, we have for modelm:
Q (sτ , uτ , a) = arg min
Q
F
F = EQ[ln Q (sτ , uτ , a)| {z }
posterior
−ln P (oτ | sτ , uτ , a)| {z }
likelihood
−ln P (sτ , uτ , a)| {z }
prior
]
= DKL [Q (sτ , uτ , a) ∥P (sτ , uτ , a| oτ )]| {z }
divergence
−ln P (oτ | m)| {z }
log evidence
= DKL [Q (sτ , uτ , a) ∥P (sτ , uτ , a)]| {z }
complexity
−EQ [ln P (oτ | sτ , uτ , a)]| {z }
accuracy
(1)
Because the (KL) divergences cannot be less than zero, the penultimate equality means
that free energy is minimised when the (approximate) posterior is equal to the true posterior.
At this point, the free energy is equal to the negative log evidence for the generative model
[35]. This means minimising free energy is mathematically equivalent to maximising model
evidence, which is, in turn, equivalent to minimising the complexity of accurate explanations
for observed outcomes.
Planning emerges under active inference by placing priors over (controllable) paths to
minimise expected free energy [41]:
G(u) =EQu [ln Q (sτ+1, a| u) − ln Q (sτ+1, a| oτ+1, u) − ln P (oτ+1 | c)]
(2)
= −EQu [ln Q (a | sτ+1, oτ+1, u) − ln Q(a | sτ+1, u)]| {z }
expected information gain (learning)
−
EQu [ln Q (sτ+1 | oτ+1, u) − ln Q (sτ+1 | u)]| {z }
expected information gain (inference)
−EQu [ln P (oτ+1 | c)]| {z }
expected cost
(3)
(4)
= −EQu [DKL [Q (a | sτ+1, oτ+1, u) ∥Q(a | sτ+1, u)]]| {z }
novelty
+
DKL [Q (oτ+1 | u) ∥P (oτ+1 | c)]| {z }
risk
−EQu [ln Q (oτ+1 | sτ+1, u)]| {z }
ambiguity
Here, the posterior predictive distribution over parameters, hidden states and outcomes
9
at the next time step, under a particular path, is defined as follows:
Qu = Q (oτ+1, sτ+1, a| u)
= P (oτ+1, sτ+1, a| u, o0, . . . , oτ )
= P (oτ+1 | sτ+1, a) Q (sτ+1, a| u) .
One can also express the prior over the parameters in terms of an expected free energy,
where, marginalising over paths:
P(a) =σ(−G)
G(a) =EQa [ln P(s | a) − ln P(s | o, a) − ln P(o | c)]
= −EQa [ln P(s | o, a) − ln P(s | a)]| {z }
expected information gain
−EQa [ln P(o | c)]| {z }
expected cost
= −EQa [DKL[P(o, s| a)∥P(o | a)P(s | a)]| {z }
mutual information
−EQa [ln P(o | c)]| {z }
expected cost
(5)
whereQa = P(o|s, a)P(s|a) =P(o, s|a) isthejointdistributionoveroutcomesandhidden
states, encoded by the Dirichlet parameters,a, andσ(·) is the softmax function. Note that
the Dirichlet parameters encode the mutual information, in the sense that they implicitly
encode the joint distribution over outcomes and their hidden causes. When normalising each
column of thea tensor, we recover the likelihood distribution (as in Figure 2); however, we
could normalise over every element, to recover a joint distribution.
As discussed above, expected free energy can be regarded as a universal objective func-
tion that augments mutual information with expected costs or constraints. Constraints —
parameterised byc — reflect the fact that we are dealing with open systems with charac-
teristic outcomes. This allows an optimal trade-off between exploration and exploitation,
that can be read as an expression of the constrained maximum entropy principle that is dual
to the free energy principle [19]. Alternatively, it can be read as a constrained principle of
maximum mutual information or minimum redundancy [42–45]. In machine learning, this
kind of objective function underwrites disentanglement [46, 47], and generally leads to sparse
representations [45, 48–50].
When comparing the expressions for expected free energy in Equation 2 with variational
free energy in Equation 1, the expected divergence becomes expected information gain. Ex-
pected information gain about the parameters and states are sometimes associated with
distinct epistemic affordances; namely, novelty and salience, respectively [51]. Similarly,
expected log evidence becomes expected value, where value is the logarithm of prior prefer-
ences. The last equality in Equation 2 provides a complementary interpretation; in which
the expected complexity becomes risk, while expected inaccuracy becomes ambiguity.
There are many special cases of minimising expected free energy. For example, maximis-
ing expected information gain maximises (expected) Bayesian surprise [52], in accord with
10
the principles of optimal (Bayesian) experimental design [53]. This resolution of uncertainty
is related to artificial curiosity [54, 55] and speaks to the value of information [56].
Expected complexity or risk is the same quantity minimised in risk sensitive or KL control
[57, 58], and underpins (free energy) formulations of bounded rationality based on complexity
costs [59, 60] and related schemes in machine learning; e.g., Bayesian reinforcement learning
[61]. More generally, minimising expected cost subsumes Bayesian decision theory [62].
11
102030405060708090100
 5 1015202530
102030405060708090100
5 1015202530
01[,, , ]NII I I=!nfuture       past
nnpIt=s!mts011:arg max supln() ( )TnnnnnnmIIIupImp pIPuttees++===>$==<=×=--hBBBsHsGH!"!""Inductive planning
h
 20 40 60 80 100
102030405060708090100
:ue=>$BB!mI
Figure 3: Inductive Planning.This figure provides an overview of inductive planning
used in this paper. The left panel provides the expressions used to induce which subsequent
states do and do not contain paths to some intended end state, encoded by a one hot vectorh.
The central panel illustrates this induction graphically, where vectors and matrices are shown
in image format (black equals zero or false and white equals one or true). Working down
the equalities in the left panel, we first initialise a logical vector of states,I, to the intended
state h. Recursively, we evaluate all the states from which the previous state can be accessed
(a state can be accessed if the probability of transitioning from an adjacent state is larger
than ε). Because this recursive induction works backwards in time, the allowable transition
matrix is transposed. Having induced the reverse history of states—that contain paths to the
intended state—one can then evaluate the length of the shortest path to the intended state.
This depends upon posterior beliefs about the current state. In the example shown on the
left, we are currently in state20, which means that the shortest path to the intended state
(state 64) is12 time steps. This tells us that if we are pursuing the shortest path then there
are certain states we need to avoid—from which the intended state cannot be reached. These
states are encoded by the logical vector I at the next time step; namely, the last time before
the probabilityp of being on a path to the intended state reaches its supremum. Because the
eligible states can only increase—as we move backwards in time—this probability can only
increase, until all states are eligible (or there are no further eligible states). The first time
that the probability reaches its supremum tells us where we are on the path to our intended
state and, crucially, the ineligible states at the next time step. We now know the states to
avoid at the next time step. If ineligible states are precluded, the next state must be on
the path to the intended state. Ineligible states can be assigned a high cost (here, the log
of a small value) to evaluate the expected cost incurred by each policy, using its predictive
posterior over states (see Figure 2). Finally, we can supplement the expected free energy,G
of each policy with the ensuing inductive cost,H. In principle, this guarantees the selection
of paths or policies that lead to the intended state, provided that state can be reached. The
example shown on the right is taken from the maze navigation task described later. For
clarity, this example only considers a single factor. The mathematical expressions use the
notation of Figure 2: The dotted red line indicates the logical vector encoding which of the
100 states will lead to the intended state at the next time point; here,11 time steps from
the intended state (indicated with a small red arrow).
12
3 Inductive Planning
What we call inductive planning—in this setting—recalls the notion of backwards induc-
tion in dynamic programming and related schemes [23, 25–27, 30, 63, 64]. In this form of
inference, precise beliefs about state transitions are leveraged to rule out actions that are
inconsistent with the attainment of future goals, defined in belief or state space as a final
(or intended) state. This is a limiting case of inductive (Bayesian) inference [65–67] in which
the very high precision of beliefs about final or intended states allows one to use logical
operators in place of tensor operations; thereby vastly simplifying computations. In brief,
we will use this simplification to furnish constraints on action selection that inherit from
priors over intended states in the future.
Active inference rests on priors that place constraints on paths or trajectories through
state space. For example, a sparse prior preference with knowledge only about the final state
warrants deep planning to demonstrate intentional behaviour [27]. One can either specify
these constraints in terms of states that are unlikely to traversed, or in terms of the final
state. In other words, the agent may,a priori, believe it will navigate state space in a way
that avoids unlikely or surprising outcomes, or that it will reach some final destination (in
state space, not outcome space), irrespective of the path taken. These are distinct kinds of
constraints. The first is implemented byc, in terms of the cost or constraints that apply
during the entire path. We now introduce another prior or constrainth, over the final state.
The priors, d and h play reciprocal roles; in the sense they specify prior beliefs about the
initial and final states, respectively. Backwards induction now follows simply from this prior;
provided it is specified sufficiently precisely. We will refer to these final states as intended
states2.
Thebasicideaisthatalthoughwemaybeuncertainaboutthenextlatentstate, wecanbe
certain about which states cannot be accessed from the current state. This means we can use
induction to identify subsequent states that cannot be on a path to an intended state; thereby
rendering actions—(i.e., state transitions) to those ineligible, ’dead-end’ states—highly un-
likely (assuming that we are on a path to an intended state). The requisite induction goes
as follows:
Imagine that we know our current state and that we will be in a certain (intended) state in
the future. Imagine further that we know all possible transitions, afforded by action, among
states. This means we can identify all the states from which the intended state is accessible.
We can now repeat this and identify all the states from which the eligible states at the
penultimate time point can be accessed, and so on. We now repeat this recursively—moving
backwards in time—until our current state becomes eligible. At this point, we select an
action that precludes ineligible states at the preceding point in backwards time (or next
point in forwards time), bringing us one step closer to the intended state. We now repeat
the backwards induction, until we arrive at the intended state, via the shortest path. This
backwards induction is computationally cheap because it entails logical operations on a
2While c, d, andh are usually hard coded, they can be learnt very efficiently, for example using Z-learning
for certain classes of MDPs [27, 68]
13
sparse logical tensor, encoding allowable state transitions.
Figure 3 provides a pseudocode and graphical abstraction based upon the MATLAB
scripts implementing this inductive logic. For clarity, we have assumed a single factor and
that there are no constraints on the paths, other than those specified by a one hot vectorh,
specifying the agent’s intended states3.
Note that this is not vanilla backwards induction. It is simply a way of placing precise
priors on paths that render certain paths—that cannot access an intended state—highly
unlikely. The requisite priors complement expected free energy in the following sense (see
Figure 3): inductive priors over policies,H are derived from priors over intended statesh,
while the priors over policies scored by expected free energy,G inherit from priors over
preferred outcomes c. This distinction is important because it means that this kind of
reasoning—and intentional behaviour—can only manifest under precise beliefs about latent
states. For example, a baby (or unexplainable neural network) could not, by definition,
act intentionally because it does not have a precise generative model of latent states (or any
mechanism to specify intended states). We will return to prerequisites for inductive planning
in the discussion.
In summary, inductive planning propagates constraints backwards in time to provide
empirical priors for planning as inference in the usual way. This means that—within the
constraints afforded by such planning—actions will still be chosen that maximise expected
information gain and any constraints encoded byc. In this sense, the inductive part of this
inference scheme can be regarded as providing a constrained expected free energy, which win-
nows trajectories through state space to paths of least action. An equivalent and alternative
perspective is that inductive planning furnishes an empirical prior over policies.
When intended states are conditioned on some context—inferred by a supraordinate
(hierarchical) level—one has the opportunity to learn intended states and, effectively, make
planning habitual. In this setting, the implicit Dirichlet counts inh, could be regarded as
accumulating habitual courses of action that are learned as empirical priors in hierarchical
models. We will pursue this elsewhere. In what follows, we focus on the distinction between
sentient behaviour—based upon expected free energy—and intentional behaviour—based
upon inductive priors.
4 Pong Revisited
In this section, we first simulate ’mere’ sentient Behaviour and then examine the qualitative
differences in behaviour when adding inductive constraints. Specifically, we simulate thein
vitro experiments reported in [1], using both an abductive and an inductive agent. The first
3In our MATLAB implementation of inductive planning, constraints due to prior preferences in outcome
space are accommodated by precluding transitions to costly states during construction of the logical matrix
encoding possible or true transitions. Furthermore, the implementation deals with multiple factors using
appropriate tensor products. Finally, when multiple intended states are supplied, the nearest state is chosen
for induction; where nearest is defined in terms of the number of timesteps required to access an intended
state.
14
has no intended goals, and stands in for a naïve neuronal culture; the second has as set of
intended states: the ones where the paddle hits upcoming balls. As environments, we use
Pong of two different sizes, that reflect two different difficulties:5 × 6 (easy), and 8 × 4
(hard). The results show that while the simulatedin vitroagent is able to fluently play in
the easy environment, it struggles in the harder one. The inductive agent, on the other hand,
can master the harder environment in less than three minutes of (simulated) game time.
In thein vitroexperiments, certain cells were stimulated depending upon the configura-
tion of a virtual game of Pong, constituted by the position of a paddle and a ball bouncing
around a bounded box. Other recording electrodes were used to drive the paddle, thereby
closing the sparse coupling between the neuronal network and the computer network simu-
lating the game of Pong (see Figure 4). Typically, in these experiments, after a few minutes
of exposure to the game, short rallies of ball returns emerge. To emulate this setup, we
created a generative process (i.e., a hard-coded representation of the dynamics of external
states) in which a ball bounced around a box at45 degrees. The lower boundary contained
a paddle that could be moved to the right or left. The size of the box was5 ×6 units, where
the ball moved one unit up or down (and right or left) at every time point. The (one unit
wide) paddle could be moved left or right by one unit at every time point. In the in vitro
experiments, whenever the agent missed the ball, either white noise or no stimulation was
applied to the sensory electrodes; otherwise, the game remained in the play. We simulated
this by supplying random input to all sensory channels whenever the ball failed to contact
the paddle on the lower boundary.
The (sensory) outcomes of the POMDP comprised30 sensory channels that could be on
or off. These can be thought of as pixels in a simple Atari-like game. The latent states were
modelled as one long orbit, by equipping the generative model with a transition matrix that
moved from one state to the next (with circular boundary conditions) for a suitably long
sequence of state transitions (here,40). The generative model was equipped with a second
factor with three controllable paths. This factor moved the paddle one unit to the right or
left (or no movement). However, the (implicit) agent knew nothing more about its world and,
in particular, had no notion that the second factor endowed it with control over the paddle.
This was because the likelihood tensors mapping from the two latent factors to the outcomes
were populated with small and uniform Dirichlet counts (i.e., concentration parameters of
1/32). In other words, our naïve generative model could, in principle, model any given
world (providing this world has a limited number of states that are revisited systematically).
Figure 4 shows the setup of this paradigm and the parameters of the generative model learned
after 512 time steps.
15
Pong
A
B C D
Figure 4: Learning the world of Pong.Panel A: Setup used in the simulations. In
brief, the generative process modelled a ball bouncing around inside a bounding box, with a
movable paddle on the lower boundary. The(5×6 =)30locations or pixels provided outputs
with two states (black or white) that were subsequently learned via a likelihood mapping
to 40 latent states. The agent was equipped with a precise transition prior where40 latent
states followed each other, with circular boundary conditions. In addition, the agent was
equipped with a second factor that controlled the panel, moving it to the right, staying
still and moving it to the left. PanelB: graphical abstract (reproduced with permission
from the authors) describing thein vitroempirical study in which a closed loop system was
used to record from—and stimulate—a network of cultured neurons. The set up enabled
the neurons to control a virtual paddle in a simulated game of Pong. Sensory feedback
reported the location of the ball and paddle; enabling the neuronal preparation to learn how
to play a rudimentary form of ping-pong. PanelC shows the transitions of the generative
model, while PanelD shows the results of active learning—i.e., accumulation of Dirichlet
counts in the likelihood tensor—after512 time steps. Note that this is a precise likelihood
mapping due to the fact that the synthetic agent has precise, if generic, transition priors.
The likelihood mapping in panel D is shown in image format, with each of the30 likelihood
tensors stacked on top of each other. Of note here are certain latent states that produce
ambiguous (i.e., unpredictable) outcomes. The first three are labelled with small arrows
over the likelihood matrix. These ambiguous likelihood mappings appear as grey columns.
This reflects the fact that the agent has learned that states corresponding to ‘missing the
ball’ lead to unpredictable and ambiguous stimulation. The implicit surprise and ambiguity
means thattheagent plans toavoid thesestates and look as ifit is playingPong—by choosing
paths or policies that are more likely to hit the ball. The emergence of this behaviour is
described in the next figure.
16
To simulate thein vitrostudy, we exposed the synthetic neural network to512 observa-
tions—about two minutes of simulated time (i.e., a few seconds of computer time). Figure 5
shows the results of this simulation. The ensuing behaviour reproduced that observed em-
pirically; namely, the emergence of short rallies after a minute or so of exposure. The
question is now: can we understand this in terms of free energy minimising processes and
their teleological concomitants?
As time progresses, Dirichlet counts are accumulated in the likelihood tensor to establish
a precise mapping between each successive hidden state and the outcomes observed in each
modality. This accumulation is precise because the agent has precise beliefs about state
transitions. As the likelihood mapping is learned, it becomes apparent to the agent that
certain states produce ambiguous outputs. These are the states in which it fails to hit the
ball with the paddle. Because these ambiguous states have a high expected free energy—see
Equation 2—the agent considers that actions that bring about these states are unlikely and
therefore tries to avoid missing the ball. This is sufficient to support rallies of up to7 returns:
see Figure 5.
However, because this agent does not look deep into the future, it can only elude ambigu-
ous states when they are imminent. In other words, although this kind of behaviour can be
regarded as sentient—in the sense that it rests upon an acquired model of the consequences
of its own action—it is not equipped with intended states.
Note what has been simulated here does not rely on any notion of reinforcement learn-
ing: at no point was the agent rewarded for any behaviour or outcome. This kind of self-
organisation—to a synchronous exchange with the world—is an emergent property of the
system that simply rests onavoiding ambiguity or uncertainty of a particular kind. The
subtle distinction between a behaviourist (reinforcement learning) account and this kind of
self-evidencing rests upon the imperatives for self-organised behaviour. In thisin silicore-
production of in vitro experiments, behaviour is a consequence of (planning as) inference,
where inference is based upon what has been learned. What has been learned are just statis-
tical regularities (or unpredictable irregularities) in the environment: in this case, there are
certain states that lead to unpredictable outcomes. This gives the agent a precise grip on the
world and enables it to infer its most likely actions. Its most likely actions are those that are
characteristic of the thing it is; namely, something that minimises surprise, ambiguity, and
free energy. This is distinct from learning a behaviour in the sense of reinforcement learning
(e.g., a state-action mapping). The difference lies in the fact that behaviour—of the sort
demonstrated above—rests on inference, under a learned model.
In the next section, we turn to a different kind of behaviour that rests upon inductive
planning, equipping the agent with foresight and eliciting anticipatory behaviour.
17
Easy setup
1 2 3 4 5123456 Difficult setup
2 4 6 81234
50 100150200250300350400450500time-40-30-20-100nats
Negative variational free energy (ELBO)
50 100150200250300350400450500time
-1-0.8-0.6-0.4-0.20nats
Precision (confidence)
Bayesian beliefs about policies
50 100150200250300350400450500time
0.511.522.533.5policy
50 100150200250300350400450500time
-40-30-20-100nats
Negative variational free energy (ELBO)
50 100150200250300350400450500time
-1-0.8-0.6-0.4-0.20nats
Precision (confidence)
Bayesian beliefs about policies
50 100150200250300350400450500time
0.511.522.533.5policy
A B
Figure 5: The emergence of play.Panels A and B show the results of two simulations
of 512 time steps (i.e., about two minutes of simulated time) under two configurations of
the Pong set up: an easy setup in panel A and a slightly more difficult setup in panel
B, in which the width of the bounding box was increased, and its height decreased (from
5 × 6 to 8 × 4). In both panels, the configuration of the game is shown above three plots
reporting fluctuations in various measures of belief updating, and accompanying behaviour.
The first graph plots the (negative) variational free energy as a function of time (where each
time step corresponds roughly to250 ms). The black dots mark time points when the ball
was hit. It can be seen that during accumulation of the likelihood Dirichlet counts, the
ball was missed until time step150. After about a minute, the synthetic agent then starts
to emit short rallies of between one and seven consecutive hits. The emergence of game
play is accompanied by saltatory increases in negative variational free energy (or evidence
lower bound). These increases disappear whenever the agent misses the ball, terminating
little rallies. The second graph plots the average of the expected free energy under posterior
beliefs about policies. This can be read as the precision of policy beliefs or, more colloquially,
the confidence placed in policy selection. This illustrates that confident behaviour emerges
during the first minute and is subsequently restricted to moments prior to hitting the ball.
Heuristically, this can be read as the agent realising that it can avoid ambiguity by move
moving in such a way as to catch the ball. The accompanying posterior (Bayesian) beliefs
about policies are shown in image format in the lower plot. This illustrates that precise or
confident behaviour entails precise beliefs about what to do next. PanelB shows exactly
the same results but for a slightly more difficult game. Here, the ball has more latitude to
move horizontally and is returned more quickly, due to the reduced height of the bounding
box. In consequence, learning a precise likelihood mapping takes about twice the amount
of time. And, even when learned, the rallies are shorter, ranging from one to four, at most.
We will use this more difficult set up to look at the effect of inductive planning in the next
figure.
18
4.1 Inductive Planning
In this section, we repeat the simulations above, but making the game more difficult by
increasing the width of the box. This means that to catch the ball, the agent has to anticipate
outcomes in the distal future in order to respond with pre-emptive movement of the paddle.
Notethatthiskindofbehaviourgoesbeyondthesortofbehaviourpredictedunderperceptual
control theory and related accounts of ball catching [69, 70]. For example, one way to model
behaviour in this paradigm would be to move the paddle so that it was always underneath
the ball. However, this is not the behaviour that emerges under self-evidencing. In what
follows, we will see that avoiding ambiguity is not sufficient for skilled performance of a more
difficult game of Pong. However, if we equip the agent with intentions to hit the ball (i.e.,
as an intended state), it can use inductive planning to pursue a never ending rally, and play
the game skilfully.
Figure 5(B) reports performance over about two minutes of simulated time of an abduc-
tive agent when increasing the width of the Pong box to8 units (and decreasing its height
to 4 units). This simple change precludes sustained rallies; largely because the depth of
planning is not sufficient to support pre-emptive moves of the paddle.
The equivalent results under inductive planning are shown in Figure 6. Here, active
inference under inductive constraints produces intermittent rallies within about a minute of
simulated time—and skilled, and fluent play after three minutes.
19
Difficult setup
2 4 6 81234
50 100150200250300350400450500time
-40-30-20-100nats
Negative variational free energy (ELBO)
50 100150200250300350400450500time
-1-0.8-0.6-0.4-0.20nats
Precision (confidence)
Bayesian beliefs about policies
50 100150200250300350400450500time
0.511.522.533.5policy
50 100150200250300350400450500time
-40-30-20-100nats
Negative variational free energy (ELBO)
50 100150200250300350400450500time
-1-0.8-0.6-0.4-0.20nats
Precision (confidence)
Bayesian beliefs about policies
50 100150200250300350400450500time
0.511.522.533.5policy
A B
Figure 6: Inductive planning.This figure follows the same format as Figure 5, reporting
the emergence of pong-playing behaviour under the more difficult set up described in the
previous figure. However, here, we included inductive planning in the belief updating by
specifying the agent’s intentions in terms of priors over particular latent states; namely,
states in which the agent hit the ball. In realising these intentions, the agent quickly learns a
sufficiently precise likelihood mapping, evincing rallies of between four and six. after about
a minute (of simulated time). This is shown in panel A. Panel B, shows the performance
during the subsequent two minutes. By about three minutes, the agent has a precise grip
on its world and realises its intentions fluently. From a dynamical systems perspective, this
can be read as the emergence of generalised synchrony—or synchronisation of chaos—as the
joint system converges onto a synchronisation manifold: a manifold that contains the states
the agent intends to visit.
In this example, we simply specified the intended states as those states corresponding
to ball hits. This would be like instructing a child by telling her what is (i.e., which states
are) expected of her. She can then work out how to realise those states by using inductive
planning and selecting the most likely actions at each moment. Notice that there is no
sense in which this could be construed as reinforcement learning: no reward or cost is being
optimised, rather the behaviour is driven purely by the minimisation of uncertainty. A better
metaphor would be instantiating some intentional set by instilling intentions or prior beliefs
about characteristic states that should be realised.
From the perspective of the free energy principle, priors over intended states can be
cast as specifying a non-equilibrium steady-state with a (pullback) attractor that contains
intended or characteristic states. From a dynamical systems perspective, this is equivalent
to specifying unstable fixed points that characterise stable heteroclinic orbits [71, 72], which
have been discussed in terms of sequential behaviour [73]. Intuitively, this means the agent
has found a free energy minimum that is characterised by generalised synchrony between the
neuronal network and the process generating sensory inputs.
Given that this synchronisation was never seen in thein vitro experiments, one might
20
argue that thein vitrobehaviour was sentient but not intentional. In the remaining sections,
we briefly showcase inductive planning in two other paradigms to illustrate the interaction
between constraints—encoded by prior preferences over outcomes—and prior intentions, en-
coded by priors over latent states.
5 Navigation as Inductive Planning
In this section, we revisit a simple navigation problem addressed many times in the literature;
e.g., [74, 75] and in demonstrations of active inference: e.g., [8, 76]. Here, the problem is
to learn the structure of a two-dimensional maze and then navigate to a target location
based upon what has been learned. This features the dual problem of learning a world or
generative model and then using what has been learned for deep planning and navigation.
In detail, we constructed a simple maze—shown in Figure 7—for an agent who has a
myopic view of the world; namely, one output modality that reported whether the agent was
sitting on an allowable location (white square) in the maze or a disallowed location (black
square), which, a priori, it found surprising (e.g., experiencing a foot shock). A simple
generative model was supplied to the agent in the form of a single factor encoding each
location or way-point, equipped with five paths. These were controllable paths that moved
the agent up or down, or right or left (or staying still). The likelihood mapping was, as in the
previous simulation, initialised to small uniform Dirichlet counts. This means the agent has
no idea about the structure of its world but simply knew that a latent state could change in
one of five ways. Learning this kind of environment is straightforward under active inference,
due to the novelty or expected information gain about parameters (see Equation 2).
This means the agent chooses actions that resolve the greatest amount of uncertainty in
the likelihood mapping from each latent state to outcomes. This ensures a Bayes optimal
exploration of state space. Figure 7A shows that the agent pursues a path which covers all
locations in an efficient fashion: i.e., not revisiting experienced states or locations until it has
explored every location. The trajectory shown in Figure 7A corresponds to256 time steps.
After this exposure, the agent has learned a likelihood model that is sufficient to support
inductive planning. Figures 7B and C shows the results of this inductive navigation, reaching
a distal target (red dot) from a starting location, while avoiding surprises or black squares
in the maze. The two routes chosen are under imprecise and precise prior preferences for
avoiding black squares (i.e., a log odds ratio—encoded byc—of one and four, respectively).
Note that the path under precise preferences is about20 steps, speaking to the depth of
induction (here,32 time steps, as in Figure 3).
This example highlights an interesting aspect of inductive planning as defined here:
namely, the learned constraints on foraging act as constraints on intentional behaviour.
These constraints enter the allowable transitions, so that the paths that are induced respect
the constraints due to prior preferences that can be inferred after—and only after—learning
the likelihood mapping. In short, this example shows how it is possible to reach intended
endpoints, under constraints on the way one gets there. In the final section, we use the same
scheme to illustrate the efficiency of inductive planning in high dimensional problem spaces.
21
A CBExplorationWeak constraintsExploitationWeak constraintsExploitationPrecise constraints
Figure 7: Navigation by induction.A: this panel reports the exploration of an agent
that is building its likelihood mapping by exploring all the novel locations in a maze. Initially,
the agent does not know where it can go; in the sense that it can only see its current location,
which can be black or white. Therefore, every unvisited location furnishes some novelty; i.e.,
expected information gain (about likelihood parameters). This compels the agent to explore
all locations efficiently and uniformly with an effective inhibition of return, until it has
become familiar with this particular maze layout. After learning, the agent was given some
intentions in terms of a specific location it believed,a priori, it would visit. Panels B and C
show the results of planning under mild and precise preferences for being on white squares.
In panel B, the agent takes a short cut to the target location (red dot), which involves a
transgression of one black square. This means that the cost of being on black squares is not
sufficiently precise to have constrained the transitions used in inductive planning. However,
because the agent is still trying to minimise expected cost (encoded by preferences for white
squares) it navigates fairly gracefully until it encounters a barrier. In contrast, panel C
shows the same agent with precise costs, which preclude transitions to black squares during
inductive planning. This agent can swiftly induce the requisite path to the target location,
without transgressing constraints on outcomes.
6 Inductive Problem Solving
This section considers a canonical problem solving task; namely, the Tower of Hanoi [77].
In this problem, one has to rearrange a number of balls over a number of towers to reach
a target arrangement from any given initial arrangement: see Figure 8. The problem can
be made easier or more difficult by manipulating the number of intervening rearrangements
between the initial and target (intended) configurations. We have previously shown that
this problem can be learned from scratch using structure learning [78]. Here, we consider
problem-solving with and without inductive planning, after learning the likelihood model
and allowable state transitions.
As above, implementing inductive planning simply means equipping the agent with prior
beliefs about a final (intended) state and then letting it rearrange the balls until those in-
22
tended states are realised. To solve this problem using active inference, one usually supplies
constraints in terms of prior preferences that are mildly aversive for all but the target ar-
rangement. This means the agent will rearrange the balls, in a state of mild surprise until
the preferred arrangement is found—and the agent rests in a low free energy state. Because
constraints are only in outcome space, there are certain arrangements that are less surpris-
ing because they are similar to the target configuration (as defined in outcome space). This
enables the agent to solve fairly deep problems, even with a limited depth of planning (here,
one-step-ahead planning). However, problems requiring more than four or five moves usually
confound this kind of planning as inference. In contrast, if the intended target is specified in
state space, then it will invoke inductive planning and, in principle, solve difficult problems,
even with a limited depth of planning.
23
A B Likelihood
50100150200250300350latent states
10
20
30
40
50
60 outcomes
Allowable transitions
50100150200250300350latent states
50
100
150
200
250
300
350 latentstates
Figure 8: Inductive planning and the Tower of Hanoi. Panel A illustrates the
particular game used to illustrate inductive planning. Here, there are four balls on three
towers. The problem is to rearrange the initial configuration (on the upper left) to match
the target configuration (lowest arrangement). In this example, it takes five moves. Actions
correspond to moving a ball from one pillar to another. The generative model that supports
this kind of problem solving is shown in terms of the requisite likelihood and transition
mappings in panel B. The likelihood tensors have been stacked on top of each other (and
unfolded) to illustrate the mapping between the360 latent states and the (4 × 3 × 5 =)
60 outcomes. The accompanying transition parameters are shown in terms of allowable
transitions among latent states (as in Figure 3). This generative model can be learned
from scratch by presenting each arrangement—and then each rearrangement—of the balls to
accumulate the appropriate Dirichlet parameters. Of interest here, is the use of the ensuing
parameters or knowledge to solve problems that require deep planning. This problem is
straightforwardtosolveusinginductiveplanning; namely, workingbackwardsfromthetarget
state using the protocol described in Figure 3. The ensuing performance is shown in the
next figure.
Figure 9 shows the performance of two agents on 100 problems, given 12 moves for
each problem. The first (abductive) agent was equipped only with constraints in outcome
space; i.e., prior preferences that led to the target solution, provided that solution was
reasonably close in outcome space. This agent failed to solve problems with five or more
moves. In contrast, when specifying intentions in the form of the intended (target) state
24
or arrangement, the second (inductive) agent was able to solve problems of eight moves or
more almost instantaneously, without fail.
In these examples, the output space was a collection of(4 × 3 =)12 outcome modali-
ties—one for each location or pixel—with five levels (four coloured balls or an empty out-
come). The state space encompassed 360 arrangements, producing large (360 × 360 × 5)
transition tensors. However, reducing these to logical matrices—used in inductive plan-
ning—means one can effectively plan deep into the future (here,64 moves) within millisec-
onds, using a one-step-ahead, active inference scheme.
Performance
Abductive agentInductive agent0102030405060708090100success(%) 1 moves2 moves3 moves4 moves5 moves6 moves7 moves8 moves9 moves
Number of trials (100/100)
123456789number of moves0
10
20
30incidence
Figure 9: Tower of Hanoi Performance. This figure reports the performance of a
generative model that has learned the Tower of Hanoi problem in terms of transitions among
different arrangements of balls. We presented the agent with100 trials with different targets
of greater and lesser difficulty (i.e., with varying numbers of moves from the initial and
target arrangements). We presented exactly the same problems to agents with and without
inductive planning. The right panel shows the incidence of trials in terms of the numbers of
moves required until completion. The agent with inductive planning was able to solve100%
of trials successfully. In contrast, the agent that did not use inductive planning was only
able to complete problems of four moves or less. This is still impressive because both the
abductive and inductive agents only looked one step ahead. In other words, even though
the abductive agent could only evaluate the quality of its next move, it was still able to
work towards the final solution. This is possible because the prior preferences for the target
outcomes mean that certain outcomes are closer to the preferred outcomes than others. The
100 trials reported in this figure take less than 10 seconds to simulate.
25
7 Discussion
This paper has introduced a particular instance of backwards induction to active inference,
as well as a more formal characterisation of sentient and intentional behaviour. Induction
in this setting appeals to a simple kind of backwards induction via logical operators, which
is used to furnish constraints on the expected free energy, and hence, actions. Actions are
then selected in the usual way; namely, actions that maximise expected information gain and
value—where value is scored by log prior preferences over outcomes. The use of inductive
priors lends planning a deep reach into the future that rests upon specifying final or intended
endpoints. In turn, this differentiates sentient from intentional behaviour. To the extent that
one can describe Bayesian beliefs—about the ultimate consequences of plans—as intentions,
one could describe the behaviour illustrated above as intentional with a well-defined purpose
or goal.
Inductive planning, as described here, can also be read as importing logical or sym-
bolic (i.e. deductive) reasoning into a probabilistic (i.e., inductive, in the sense of inductive
programming) framework. This speaks to symbolic approaches to problem solving and plan-
ning—e.g., [79–81]—and a move towards the network tensor computations found in quantum
computing: e.g., [82, 83]. However, in so doing, one has to assume precise priors over state
transitions and intended states. In other words, this kind of inductive planning is only apt
when one has precisely stated goals and knowledge about state transitions. Is this a reason-
able assumption for active inference? It could be argued that it is reasonable in the sense
that: (i) goal-states or intended states are stipulatively precise (one cannot formulate an
intention to act without specifying the intended outcome with a certain degree of precision)
and (ii) the objective functions that underwrite self-evidencing lead to precise likelihood and
transition mappings. In other words, to minimise expected free energy—via learning—just is
to maximise the mutual information between latent states and their outcomes, and between
successive latent states.
To conclude, inductive planning differs from previous approaches proposed in both the
reinforcement learning and active inference literature, due to the presence of intended goals
defined in latent state space. In both model free and model based reinforcement learning,
goals are defined via a reward function. In alternative but similar approaches, such as
active inference, rewards are passed to the agent as privileged (usually precise but sparse)
observations [41, 84]. This influences the behaviour of the agent, which learns to design and
select policies that maximise expected future reward either via model-free approaches, which
assign values to state-action pairs, or via model-based approaches, which select actions after
simulating possible futures. Defining preferences directly in the state space, however, induces
a different kind of behaviour: the fast and frugal computation involved in inductive planning
is now apt to capture the efficiency of human-like decision-making, where indefinitely many
possible paths, inconsistent with intended states, are ruled outa priori—hence combining
the ability of agents to seek long-term goals, with the efficiency of short-term planning.
26
8 Conclusion
The aim of this paper was to characterise the self-organisation of adaptive behaviour through
the lens of the free energy principle, i.e., as self-evidencing. We did this by first discussing the
definitions of reactive and sentient behaviour in active inference, where the latter describes
the behaviour of agents that are aware of the consequences of their actions. We then intro-
duced a formal account ofintentional behaviour, Specified by intended endpoints or goals,
defined in state space rather than outcome space, as in abductive forms of active inference.
We then investigate these forms of (reactive, sentient, and intentional) behaviour using sim-
ulations. First, we simulate the aforementioned in vitro experiments, in which neuronal
cultures spontaneously learn to play Pong, by implementing nested, free energy minimis-
ing processes. We used these simulations to Illustrate the ensuing behaviour—leveraging
the distinction between merely reactive, sentient, and intentional behaviour. The requisite
inductive planning was then further illustrated using simple machine learning benchmarks
(navigation in a grid world and the Tower of Hanoi problem), that showed how quickly and
efficiently adaptive behaviour emerges under inductive constraints on active inference.
Disclosure statement
The authors have no disclosures or conflict of interest.
Acknowledgements
KFissupportedbyfundingfortheWellcomeCentreforHumanNeuroimaging(Ref: 205103/Z/16/Z),
a Canada-UK Artificial Intelligence Initiative (Ref: ES/T01279X/1) and the European
Union’s Horizon 2020 Framework Programme for Research and Innovation under the Spe-
cific Grant Agreement No. 945539 (Human Brain Project SGA3). AR is funded by the
Australian Research Council (Ref: DP200100757) and the Australian National Health and
Medical Research Council (Investigator Grant Ref: 1194910).
27
References
[1] B.J. Kagan, A.C. Kitchen, N.T. Tran, F. Habibollahi, M. Khajehnejad, B.J. Parker, A.
Bhat, B. Rollo, A. Razi, and K.J. Friston. “In vitro neurons learn and exhibit sentience
when embodied in a simulated game-world”. In:Neuron (2022).
[2] Chris Fields, James F Glazebrook, and Michael Levin. “Minimal physicalism as a scale-
free substrate for cognition and consciousness”. In:Neuroscience of Consciousness2021
(2021), niab013.
[3] M. Levin. “The Computational Boundary of a "Self": Developmental Bioelectric-
ity Drives Multicellularity and Scale-Free Cognition”. In:Frontiers in Psychology10
(2019), p. 2688.
[4] S. Manicka and M. Levin. Modeling somatic computation with non-neural bioelectric
networks. Scientific Reports 9, 18612. 2019.
[5] Atsushi Masumori, Norihiro Maruyama, Lana Sinapayen, Takeshi Mita, Urs Frey, Dou-
glas Bakkum, Hirokazu Takahashi, and Takashi Ikegami. “Emergence of sense-making
behavior by the Stimulus Avoidance Principle: Experiments on a robot behavior con-
trolled by cultured neuronal cells”. In: Artificial Life Conference Proceedings. MIT
Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info ... 2015,
pp. 373–380.
[6] T. Isomura and K. Friston. “In vitro neural networks minimise variational free energy”.
In: Scientific Reports8 (2018), p. 16926.
[7] T. Isomura, K. Kotani, Y. Jimbo, and K.J. Friston. “Experimental validation of the
free-energy principle with in vitro neural networks”. In:Nature Communications 14
(2023), p. 4547.
[8] K. Friston, L. Da Costa, D. Hafner, C. Hesp, and T. Parr. “Sophisticated inference”.
In: Neural Computation33 (2021), pp. 713–763.
[9] E.R. Palacios, T. Isomura, T. Parr, and K. Friston. “The emergence of synchrony in
networks of mutually inferring neurons”. In:Scientific Reports9 (2019), p. 6412.
[10] J. Winn and C.M. Bishop. “Variational message passing”. In: Journal of Machine
Learning Research6 (2005), pp. 661–694.
[11] Jakob Hohwy. “The self-evidencing brain”. In:Noûs 50.2 (2016), pp. 259–285.
[12] V. Mnih, K. Kavukcuoglu, D. Silver, A.A. Rusu, J. Veness, M.G. Bellemare, A. Graves,
M. Riedmiller, A.K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I.
Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. “Human-
level control through deep reinforcement learning”. In:Nature 518 (2015), pp. 529–
533.
[13] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez,
E. Lockhart, D. Hassabis, T. Graepel, T.P. Lillicrap, and D. Silver.Mastering Atari,
Go, Chess and Shogi by Planning with a Learned Model. arXiv:1911.08265. 2019.
28
[14] Fuat Balci et al. “A response to claims of emergent intelligence and sentience in a dish”.
In: Neuron 111 (2023), pp. 604–605.
[15] Hagai Attias. “Planning by probabilistic inference”. In:International workshop on ar-
tificial intelligence and statistics. PMLR. 2003, pp. 9–16.
[16] MatthewBotvinickandMarcToussaint.“Planningasinference”.In: Trends in cognitive
sciences 16.10 (2012), pp. 485–488.
[17] Sander G Van Dijk and Daniel Polani. “Informational constraints-driven organiza-
tion in goal-directed behavior”. In: Advances in Complex Systems 16.02n03 (2013),
p. 1350016.
[18] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sanja Veselic, Victor Neacsu, and Karl
Friston. “Active inference on discrete state-spaces: A synthesis”. In:Journal of Math-
ematical Psychology99 (2020), p. 102447.
[19] Maxwell JD Ramstead, Dalton AR Sakthivadivel, Conor Heins, Magnus Koudahl,
Beren Millidge, Lancelot Da Costa, Brennan Klein, and Karl J Friston. “On Bayesian
mechanics: a physics of and by beliefs”. In:Interface Focus13.3 (2023), p. 20220029.
[20] K. Friston, L. Da Costa, N. Sajid, C. Heins, K. Ueltzhöffer, G.A. Pavliotis, and T.
Parr. “The free energy principle made simpler but not too simple”. In:Physics Reports
1024 (2023), pp. 1–29.
[21] K. Friston, L. Da Costa, D.A.R. Sakthivadivel, C. Heins, G.A. Pavliotis, M. Ramstead,
and T. Parr. Path integrals, particular kinds, and strange things. arXiv:2210.12761.
2022.
[22] Colin F Camerer. “Progress in behavioral game theory”. In:Journal of Economic Per-
spectives 11 (1997), pp. 167–188.
[23] C. Hure, H. Pham, and X. Warin. “Deep Backward Schemes for High-Dimensional
Nonlinear Pdes”. In:Mathematics of Computation89 (2020), pp. 1547–1579.
[24] Richard Bellman. “On the Theory of Dynamic Programming”. In:Proc Natl Acad Sci
U S A38 (1952), pp. 716–719.
[25] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. “The
relationship between dynamic programming and active inference: the discrete, finite-
horizon case”. In:arXiv preprint arXiv:2009.08111(2020).
[26] R.S. Sutton, D. Precup, and S. Singh. “Between MDPs and semi-MDPs: A frame-
work for temporal abstraction in reinforcement learning”. In:Artificial Intelligence112
(1999), pp. 181–211.
[27] Aswin Paul, Noor Sajid, Lancelot Da Costa, and Adeel Razi. “On efficient computation
in active inference”. In:arXiv preprint arXiv:2307.00504(2023).
[28] G.H. Harman. “The inference to the best explanation”. In: Philosophical Review 74
(1965), pp. 88–95.
29
[29] A.K. Seth. “Inference to the Best Prediction”. In:Open MIND. Ed. by T.K. Metzinger
and J.M. Windt. Frankfurt am Main: MIND Group, 2015.
[30] R.A. Howard. Dynamic Programming and Markov Processes. Cambridge, MA: MIT
Press, 1960.
[31] M.J.D. Ramstead, D.A.R. Sakthivadivel, C. Heins, M. Koudahl, B. Millidge, L. Da
Costa, B. Klein, and K.J. Friston.On Bayesian Mechanics: A Physics of and by Beliefs.
arXiv:2205.11543. 2022.
[32] Andy Clark, Karl Friston, and Sam Wilkinson. “Bayesing Qualia: Consciousness as
Inference, Not Raw Datum”. In:Journal of Consciousness Studies26 (2019), pp. 19–
33.
[33] L. Sandved-Smith, C. Hesp, J. Mattout, K. Friston, A. Lutz, and M.J.D. Ramstead.
“Towards a computational phenomenology of mental action: modelling meta-awareness
and attentional control with deep parametric active inference”. In:Neuroscience of
Consciousness 2021 (2021), niab018.
[34] Ryan Smith, Maxwell J. D. Ramstead, and Alex Kiefer. “Active Inference Models Do
Not Contradict Folk Psychology”. In:Synthese 200.2 (2022), pp. 1–37.doi: 10.1007/
s11229-022-03480-w.
[35] Matthew J Beal. “Variational Algorithms for Approximate Bayesian Inference”. PhD
thesis. University College London, 2003.
[36] Randy L Buckner. “The role of the hippocampus in prediction and imagination”. In:
Annual review of psychology61 (2010), pp. 27–48.
[37] Kenway Louie and Matthew A Wilson. “Temporally structured replay of awake hip-
pocampal ensemble activity during rapid eye movement sleep”. In:Neuron 29.1 (2001),
pp. 145–156.
[38] Will D Penny, Peter Zeidman, and Neil Burgess. “Forward and backward inference in
spatial cognition”. In:PLoS computational biology9.12 (2013), e1003383.
[39] Giovanni Pezzulo, Matthijs AA Van der Meer, Carien S Lansink, and Cyriel MA Pen-
nartz. “Internally generated sequences in learning and executing goal-directed behav-
ior”. In:Trends in cognitive sciences18.12 (2014), pp. 647–657.
[40] Thomas Parr, Giovanni Pezzulo, and Karl J Friston.Active inference: the free energy
principle in mind, brain, and behavior. MIT Press, 2022.
[41] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzger-
ald, and Giovanni Pezzulo. “Active inference and epistemic value”. In:Cognitive neu-
roscience 6.4 (2015), pp. 187–214.
[42] Nihat Ay, Nils Bertschinger, Ralf Der, Frank Guttler, and Eckehard Olbrich. “Predic-
tiveinformationandexplorativebehaviorofautonomousrobots”.In: European Physical
Journal B 63 (2008), pp. 329–339.
30
[43] Horace B Barlow et al. “Possible principles underlying the transformation of sensory
messages”. In:Sensory communication1.01 (1961), pp. 217–233.
[44] R. Linsker. “Perceptual Neural Organization - Some Approaches Based on Net-
work Models and Information-Theory”. In:Annual Review of Neuroscience13 (1990),
pp. 257–281.
[45] B.A. Olshausen and D.J. Field. “Emergence of simple-cell receptive field properties by
learning a sparse code for natural images”. In:Nature 381 (1996), pp. 607–609.
[46] Irina Higgins, Le Chang, Victoria Langston, Demis Hassabis, Christopher Summerfield,
Doris Tsao, and Matthew Botvinick. “Unsupervised deep learning identifies semantic
disentanglement in single inferotemporal face patch neurons”. In:Nature communica-
tions 12.1 (2021), p. 6456.
[47] Eduardo Hugo Sanchez, Mathieu Serrurier, and Mathias Ortner. “Learning disentan-
gled representations via mutual information estimation”. In:Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XXII 16. Springer. 2020, pp. 205–221.
[48] C.Gros.“Cognitivecomputationwithautonomouslyactiveneuralnetworks:Anemerg-
ing field”. In:Cognitive Computation1 (2009), pp. 77–90.
[49] Dalton AR Sakthivadivel. “Weak Markov blankets in high-dimensional, sparsely-
coupled random dynamical systems”. In:arXiv preprint arXiv:2207.07620(2022).
[50] Michael E Tipping. “Sparse Bayesian learning and the relevance vector machine”. In:
Journal of machine learning research1.Jun (2001), pp. 211–244.
[51] Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald,
Martin Kronbichler, and Karl J Friston. “Computational mechanisms of curiosity and
goal-directed exploration”. In:elife 8 (2019), e41703.
[52] L.IttiandP.Baldi.“BayesianSurpriseAttractsHumanAttention”.In: Vision Research
49 (2009), pp. 1295–1306.
[53] D.V. Lindley. “On a Measure of the Information Provided by an Experiment”. In:
Annals of Mathematical Statistics27 (1956), pp. 986–1005.
[54] J. Schmidhuber. “Curious model-building control systems”. In: International Joint
Conference on Neural Networks. Vol. 2. IEEE. 1991, pp. 1458–1463.
[55] S. Still and D. Precup. “An information-theoretic approach to curiosity-driven rein-
forcement learning”. In:Theory in Biosciences131 (2012), pp. 139–148.
[56] Ronald A Howard. “Information value theory”. In:IEEE Transactions on systems sci-
ence and cybernetics2.1 (1966), pp. 22–26.
[57] Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. “Empowerment:
A universal agent-centric measure of control”. In:2005 ieee congress on evolutionary
computation. Vol. 1. IEEE. 2005, pp. 128–135.
31
[58] Bart van den Broek, Wim Wiegerinck, and Hilbert Kappen. “Risk sensitive path inte-
gral control”. In:arXiv preprint arXiv:1203.3523(2012).
[59] Daniel A Braun, Pedro A Ortega, Evangelos Theodorou, and Stefan Schaal. “Path inte-
gral control and bounded rationality”. In:2011 IEEE symposium on adaptive dynamic
programming and reinforcement learning (ADPRL). IEEE. 2011, pp. 202–209.
[60] Pedro A Ortega and Daniel A Braun. “Thermodynamics as a theory of decision-making
with information-processing costs”. In:Proceedings of the Royal Society A: Mathemat-
ical, Physical and Engineering Sciences469.2153 (2013), p. 20120683.
[61] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. “Bayesian
reinforcement learning: A survey”. In:Foundations and Trends® in Machine Learning
8.5-6 (2015), pp. 359–483.
[62] James O Berger. Statistical decision theory and Bayesian analysis. Springer Science &
Business Media, 2013.
[63] Colin F Camerer, Teck-Hua Ho, and Juin-Kuan Chong. “A cognitive hierarchy model
of games”. In:Quarterly Journal of Economics119 (2004), pp. 861–898.
[64] D Gowanlock R Tervo, Joshua B Tenenbaum, and Samuel J Gershman. “Toward the
neural implementation of structure learning”. In:Current opinion in neurobiology37
(2016), pp. 99–105.
[65] Horace B Barlow. “Inductive inference, coding, perception, and language”. In:Percep-
tion 3 (1974), pp. 123–134.
[66] J. Hawthorne. Inductive Logic. Stanford Encyclopedia of Philosophy. 2021.
[67] Alex Kiefer. “Literal Perceptual Inference”. In: Philosophy and predictive processing.
Ed. by Thomas Metzinger and Wanja Wiese. 2017.
[68] Emanuel Todorov. “Linearly-solvable Markov decision problems”. In:Advances in neu-
ral information processing systems19 (2006).
[69] G. Gigerenzer and H. Brighton. “Homo heuristicus: why biased minds make better
inferences”. In:Topics in Cognitive Science1 (2009), pp. 107–143.
[70] W. Mansell. “Control of perception should be operationalized as a fundamental prop-
erty of the nervous system”. In:Topics in Cognitive Science3 (2011), pp. 257–261.
[71] Valentin Afraimovich, Irma Tristan, Ramón Huerta, and Mikhail I Rabinovich. “Win-
nerless competition principle and prediction of the transient dynamics in a Lotka-
Volterra model”. In:Chaos 18.043103 (2008).
[72] M. Rabinovich, R. Huerta, and G. Laurent. “Transient dynamics for neural processing”.
In: Science 321 (2008), pp. 48–50.
[73] José Fonollosa, Emre Neftci, and Mikhail Rabinovich. “Learning of chunking sequences
in cognition and behavior”. In:PLoS Computational Biology11 (2015), e1004592.
[74] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. “Action understanding as
inverse planning”. In:Cognition 113 (2009), pp. 329–349.
32
[75] Peter Dayan, Yael Niv, Ben Seymour, and Nathaniel D Daw. “The misbehavior of
value and the discipline of the will”. In:Neural Networks19 (2006), pp. 1153–1160.
[76] R. Kaplan and K.J. Friston. “Planning and navigation as active inference”. In:Biolog-
ical Cybernetics112 (2018), pp. 323–343.
[77] Francesco Donnarumma, Domenico Maisto, and Giovanni Pezzulo. “Problem solving
as probabilistic inference with subgoaling: explaining human successes and pitfalls in
the tower of hanoi”. In:PLoS computational biology12.4 (2016), e1004864.
[78] Karl Friston, Lancelot Da Costa, Alexander Tschantz, Alex Kiefer, Tommaso Salvatori,
Victorita Neacsu, Magnus Koudahl, Conor Heins, Noor Sajid, Dimitrije Markovic,
ThomasParr,TimVerbelen,andChristopherBuckley.“Supervisedstructurelearning”.
In: arXiv preprint arXiv:2311.10300(2023).
[79] Frédéric Colas, Julien Diard, and Pierre Bessière. “Common Bayesian models for com-
mon cognitive issues”. In:Acta Biotheoretica58 (2010), pp. 191–216.
[80] Maria Fox and Derek Long. “PDDL2.1: An extension to PDDL for expressing temporal
planning domains”. In:Journal of Artificial Intelligence Research20 (2003), pp. 61–
124.
[81] M. Gilead, Y. Trope, and N. Liberman. “Above and beyond the concrete: The diverse
representational substrates of the predictive brain”. In:Behavioral and Brain Sciences
43 (2019), e121.
[82] Chris Fields et al. “Control flow in active inference systems—part II: tensor networks
as general models of control flow”. In:IEEE Transactions on Molecular, Biological and
Multi-Scale Communications9 (2023), pp. 246–256.
[83] E. Knill and R. Laflamme. “Theory of quantum error-correcting codes”. In:Physical
Review A55 (1997), pp. 900–911.
[84] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. “Reward
MaximizationThroughDiscreteActiveInference”.In: Neural Computation35.5(2023),
pp. 807–852.
33