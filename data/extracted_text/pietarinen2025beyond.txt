Received: 30 May 2025 / Accepted: 20 August 2025
© The Author(s) 2025
  Ahti-Veikko Pietarinen
pietarinen@hkbu.edu.hk
Majid D. Beni
mbeni@metu.edu.tr
1 Department of Religion and Philosophy, Hong Kong Baptist University, Hong Kong, Hong 
Kong
2 Department of Philosophy, Middle East Technical University, Ankara, Türkiye
Beyond Bayesian Accuracy: Skill, Abduction, and the Free 
Energy Principle in Normative Rationality
Original Research
Ahti-Veikko Pietarinen1  · Majid D. Beni2
Foundations of Science
https://doi.org/10.1007/s10699-025-10013-4
Abstract
This paper challenges traditional accuracy-centric accounts of rationality by synthesising 
the Free Energy Principle (FEP) with Charles Peirce’s pragmatist epistemology. Whereas 
the FEP frames cognition as a biological imperative to minimise surprise through predic -
tive models, we argue that its normative force emerges when integrated with Peircean 
abduction and skill-based metrics. By reinterpreting rationality through skill scores—
Peirce’s 1884 method for evaluating rare-event predictions—we demonstrate that surviv -
al-driven inference prioritises context-sensitive skill over abstract accuracy. The FEP’s 
variational free-energy minimisation aligns with abduction’s dynamic conjecture-making, 
revealing rationality as a pragmatic negotiation between organismic survival and envi -
ronmental complexity. Critically, we show that Bayesian accuracy measures (e.g., Kull -
back–Leibler divergence) fail to capture the adequacy conditions for skillful forecasting, 
whereas Peirce’s skill score satisfies constraints such as error weighting and directionality. 
This fusion of FEP and pragmatism advances a naturalistic-normative framework in which 
rationality is grounded in adaptive, enactive inference rather than idealised coherence, 
bridging computational neuroscience and theoretical biology with philosophical accounts 
of inquiry.
Keywords Normative rationality · Free energy principle · Pragmatism · Abduction · 
Accuracy · Skill scores
1 3

A.-V. Pietarinen, M. D. Beni
1 Introduction
1.1 The Limits of Accuracy-Centric Rationality
Traditional views of rationality often emphasise logical consistency, optimisation, and 
accuracy as hallmarks of a rational agent (Hájek, 2008; Hartmann, 2020; Leitgeb & Petti -
grew, 2010a, b). However, recent advancements in computational neuroscience, particularly 
through the Free Energy Principle (FEP), suggest that rationality emerges from biological 
imperatives. This paper elucidates the sense of rationality that arises in the context of these 
developments, examining the role of FEP in the process of probabilistic inference and its 
significance for maximising an agent’s survival in a dynamically uncertain world (Beni & 
Pietarinen, 2021; Parr & Pezzulo, 2021; Pietarinen & Beni, 2021; Seddon, 2022). Accord-
ing to these proposals, to justify the norms of rationality, one must draw on the ability and 
skill of self-organising systems to maximise their survival by minimising the discrepancy 
between their generative models and the environment.
The Free Energy Principle (FEP), developed by Karl Friston and colleagues (Friston, 
1994, 2010; Palm et al., 2015), has sparked lively philosophical and scientific debates. It has 
been applied to problems involving complex systems, the brain, cognition, life, and mean -
ing and has even been applied in simple physical systems in the universe (Aguilera et al., 
2022; Baltieri et al., 2020; Klein, 2018; Ramstead et al., 2020; van Es & Hipolito, 2020). 
Against this backdrop, a key contribution of this paper is to expose the reasons why FEP is 
operationalised in rational agents’ probabilistic inferences and decision-making.
1.2 FEP and Pragmatism: A New Synthesis
Our proposal builds on prior engagements with accounts of the cognitive and neurophysi -
ological mechanisms underlying FEP. We also consider the broader implications of this 
framework for understanding rational action (Beni & Pietarinen, 2021; Pietarinen & Beni, 
2021; Seddon, 2022). We argue that FEP, when viewed through the lens of scientific dis -
covery and prediction, aligns with Charles Peirce’s distinction between skill and accuracy. 
Peirce’s original work on skill scores in 1880 provides a framework for evaluating accuracy 
in successful predictions. Inspired by Peirce’s work, this paper unearths an alternative accu-
racy measure to accompany norms of rationality, which is based on the score matrix that 
estimates the true skill of the forecasting agent—the guesser at the scientific hypotheses or 
the bet-maker in rational decisions.
Moreover, Peirce famously defended and justified the third mode of reasoning alongside 
deduction and induction, which he termed abduction. Abduction has since become a bench-
mark mode of inference, attesting that Bayesian explanations of rationality are insufficient 
and do not go into the roots of the matter. An interesting tension is thus seen to arise between 
the justification of probabilism, the origins of accuracy measures from their historical per -
spective, and the justification of abductive inferences. We argue that this tension only seem-
ingly points to conflicting conclusions.
The paper’s pragmatist perspective resonates with our application of the FEP, expanding 
the formal depiction of cognitive processes beyond the rigid boundaries of pure Bayesian -
ism. This expansion is achieved through the adoption of active inference under the FEP for 
the analysis of rationality, allowing us a broad insight into the intricate biological mecha -
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
nisms enabling survival within fluctuating environments. We cultivate this insight by ten -
dering a yet more general perspective to rationality, namely one that sprouts from Peirce’s 
pragmatist insights into the reasons why accuracy alone is not sufficient and why accuracy 
in fact fails to capture the full breadth of rational action. Rather, the ability to make skill -
ful predictions, particularly in the context of uncertain and rare events, is of paramount 
importance.
The paper is structured as follows: Sect. 2 addresses the central question of how ratio -
nality can be justified. Section 3 introduces our pragmatist proposal. Section 4 explains the 
FEP, while Sect. 5 links it to abduction. Section 6 integrates Peirce’s skill score metrics, 
highlighting how they complement the FEP. The conclusion in Sect. 7 ties these elements 
together, advancing a unified account of rationality.
2 The Justification of Rationality Question
2.1 Bayesianism’s Survival Imperative
Bayes’ theorem specifies the conditional probability of a hypothesis given the evidence on 
the basis of the unconditional probability of the hypothesis. Where H stands for the hypoth-
esis and E represents the evidence, the rule is as follows:
 p(H | E) = p(H)
The fundamental question posed by this simple formula in the context of rational behaviour 
is as follows: why is it that the degrees of belief of a rational agent must conform to prob-
ability functions modelled by Bayes’ theorem?
Although the classification is not exhaustive, there are generally two kinds of arguments 
that may be invoked to justify Bayesianism in formal epistemology: the Dutch Book argu -
ment and the Expected (epistemic) Utility argument. The Dutch Book theorem, as stated 
by Bruno de Finetti ([1937] 1992), indicates that unless an agent conforms to the norms of 
probability calculus, there will be a Dutch Book against the agent. The Dutch Book consists 
of several bets, which, although those bets may individually be fair, taken collectively guar-
antee the agent’s loss.
2.2 Epistemic Utility Vs. Biological Pragmatism
In the present paper, we focus on justifications of Bayesianism in terms of the epistemic 
utility argument. Framing our argument in terms of expected utilities instead of the Dutch 
Book argument has no impact on the discussion, given Hajek’s ( 2008) conclusions that all 
justifications for probability have the same form: if the agents’ credences do not conform to 
norms of probability, then their rationality will be impugned in some way or another.  The 
epistemic utility argument accounts for norms of rationality by representing functions of 
credence stated in terms of utility functions. Utility functions contribute to maximising the 
expected utility of the agent by assuming that any violation of norms of probability calculus 
would be irrational for an agent with a utility function that aims at maximising the expected 
1 3
A.-V. Pietarinen, M. D. Beni
utility (Pettigrew, 2016, p. 19). Specifically, according to Leitgeb & Pettigrew’s (2010a, b) 
statement, the utility argument may be grounded upon norms of accuracy.1
Given Hajek’s (2008) demonstration that all justifications for probability have the same 
form and assuming that any sound defence of norms of rationality must be indifferent to 
using expected utility or the Dutch book, we argue that Bayesianism is justifiable on the 
basis of the survival value of natural processes that ground the theory. Arguing that Bayes-
ianism is justifiable on the basis of the survival value of natural processes is indifferent to 
the use of either the Dutch book argument or the expected utility argument because the 
survival value argument does not rely on any specific measure of epistemic utility but sim -
ply states that Bayesian belief updating is a natural process that has evolved because it has 
adaptive virtues. The proposal is more general than both the expected utility argument and 
the Dutch book argument, which both rely on specific measures of epistemic utility.
The general insight here is that the Dutch Book argument can be meaningfully integrated 
into our pragmatist framework by recasting its central demand for coherence through the 
lens of free-energy minimisation; the demand—that an agent’s credences must be coherent 
to avoid a sure loss—is precisely what free-energy minimisation enforces. Any incoherent 
set of credences would result in unbounded variational free energy, a state an organism 
cannot afford metabolically. In this view, a Dutch Book scenario corresponds to a failure 
to minimise surprise, which would trigger a runaway energy expenditure. For example, 
consider an organism—for example, a predator—with an internal generative model of its 
environment. Its model incorrectly assigns a low probability to a common, energy-rich prey 
species being present in its hunting ground. This is an incoherent credence, as it contradicts 
actual environmental statistics. When a predator encounters this prey, its sensory data are 
highly surprising because its internal model does not predict it. This shows that incoher -
ent credences are not merely a formal error but an immediate metabolic liability. This fur -
ther demonstrates how bounded agents can adopt fast, frugal heuristics that approximate 
Bayesian coherence without incurring prohibitive computational costs, thereby sidestepping 
sure-loss gambits under real-world constraints, given that cognition is not about passively 
representing a pregiven world but about an organism’s active, sensorimotor engagement 
with its environment. Therefore, within an embodied, ecologically embedded FEP, the 
Dutch Book criterion emerges as an energetic necessity and transcends a purely mental or 
logical inconsistency; it manifests as a failure of an embodied agent to successfully regu -
late its exchanges with its environment, where incoherent beliefs cause a breakdown in the 
agent’s capacity for autopoiesis—its ability to actively maintain itself. Thus, the Dutch Book 
scenario becomes a dramatisation of a system being driven into unstable states from which it 
cannot recover, a fate that is fundamentally contrary to the FEP’s core premise of a system 
striving to maintain its nonequilibrium steady state. Minimising free energy and maintain -
ing coherent beliefs are two sides of the same survival-driven coin: when belief updating is 
cast as free-energy minimisation within such a system, the Dutch Book argument becomes 
an integral principle ensuring both coherence and resource-efficient inference.
1  The Epistemic Utility argument is centred on the notion of accuracy. Epistemic agents desire accurate 
beliefs, because accurate beliefs are more likely to lead to success in achieving their epistemic goals.The 
argument assumes that “an epistemic agent ought to approximate the truth. In other words, she ought to 
minimize her inaccuracy” (Leitgeb & Pettigrew, 2010b, p. 209). To address normative aspects of Bayesian-
ism, Leitgeb and Pettigrew speak about local and global expected accuracy, with synchronic and diachronic 
bifurcations.
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
In sum, we at this point gesture that Bayesianism is a finessed form of underpinning 
inference that organic systems invoke to minimise the divergence between states that they 
can actually occupy and those that they believe (or anticipate) that they must occupy to 
maximise their survival (Beni & Pietarinen, 2021; Pietarinen & Beni, 2021; Seddon, 2022). 
Let us turn to a more specific, abductive charge.
3 Naturalism, Pragmatism, and Scientific Reasoning
3.1 Abduction as a Cognitive Dynamic
To explicate norms of probabilism in a naturalistic framework is to remain broadly loyal to the 
spirit of Peirce’s pragmatistic epistemology, which, from our perspective, scores well in answer-
ing the worries voiced on the presumed rationality of Bayesian models (Colombo et al., 2021).
Naturalism and pragmatism meet at the level of scientific reasoning. In the late 19th 
century, Peirce developed a theory of the logic of science, including nondeductive reason -
ing, one of which he termed abduction. Although his views on this topic matured over the 
decades, the core idea is the one that rests on the agent’s faculties of conjecture-making that 
revises the system of preexisting conjectures in a given situation or model of its environ -
ment. Peirce’s mature formulation of abduction’s inferential schema captures that core idea: 
abduction is “reasoning from surprise to inquiry” (Ma & Pietarinen, 2017). As such, infer-
ence portrays retroduction: “Given a (surprising) fact C, if A (subjunctively) implies C, then 
it is to be inquired whether  A plausibly holds.” Like abduction elsewhere in Peirce’s writ -
ings, this late schema also begins with an observation of a surprising fact or event (or, as in 
experimental research, is usually the case, an ‘event class’), and through a conditional major 
premise imparts, in a cohortative mood that Peirce termed “the investigand”, the conclusion 
that something ought to be done. Initially, aroused by curiosity, which leads to the formula-
tion of the first premise of inquiry, the abductive schema encapsulates the dynamic process 
of reason, striving toward assertions of anticipated conjectures. Loyal to Peircean insights, 
the philosophical account of abduction can indeed be anchored to a scientific theory of 
cognitive inference.
3.2 FEP’s Role in Inquiry
One such framework is FEP, which provides a comprehensive perspective on how living 
systems, including cognitive processes, operate. This suggests that living systems, includ -
ing the human mind, seek to minimise free energy or surprises in their interactions with the 
environment. This principle posits that organisms aim to reduce the discrepancy between 
their internal models of the world and the external sensory data they receive. In the context 
of abduction, this principle implies that the human mind, when faced with uncertain or 
novel situations, strives to generate hypotheses or conjectures that minimise the discrepancy 
between its current understanding and the observed data.
The issue addressed in this paper is that while the FEP offers a compelling framework for 
understanding cognitive processes and the minimisation of uncertainty, it does not, in and of 
itself, suffice as a justification for rationality. First, while the FEP provides a powerful math-
ematical framework—grounded in concepts such as nonequilibrium steady states and Mar-
1 3
A.-V. Pietarinen, M. D. Beni
kov blankets—critics often highlight the apparent inapplicability of these physics-derived 
formalisms to the disordered, context-sensitive reality of biological systems (Colombo & 
Palacios, 2021; Colombo & Wright, 2018). They argue that FEP’s assumptions, such as 
ergodicity, may oversimplify the nongeneric properties of living organisms, creating a sig -
nificant tension between its mathematical elegance and its biological realism.
This criticism, however, arises from a misconception of the FEP’s general role in theo -
retical biology. It is not an empirical hypothesis about biology but a formal modelling lan -
guage. Its scientific utility is not determined by the intrinsic valences of ‘truth’ or ‘falsity’ 
but by the way the language is applied to a specific domain (Andrews, 2021). The challenge, 
therefore, is not whether mathematical tools are applicable to cognition at all but how to cor-
rectly use them. As such, the FEP’s explanatory power depends on careful calibration of its 
models to balance the trade-off between generality, realism, and precision. This approach, 
which Beni (2022) describes as achieving the ‘right dosage,’ ensures that FEP models are 
neither overly abstract nor so detailed that they lose their unifying explanatory force. In this 
view, the quantitative approach is not abandoned but refined, with model builders explicitly 
defining the level of abstraction and the empirical constraints needed for a plausible expla -
nation (Beni, 2022).
The claim that an organism acts to minimise free energy does not necessarily account for 
why this should be considered a rational process in any normative or pragmatic sense. Mini-
mising surprise or error might explain how organisms, including human agents, manage 
to stay within viable biological bounds and maintain homeostasis, but it does not explain 
why this tendency should be equated with rational decision-making. Thus, FEP should be 
considered a genuine foundation for rationality, particularly when the cognitive strategies 
involved in abduction and inference are considered. We address this issue in the next sec -
tions by drawing on Peirce’s concept of “the investigand,” which essentially communicates 
a scientific urge never to stop questioning and exploring: continue active inquiry, and one is 
guaranteed to reduce unwanted uncertainty.
Against this backdrop, in the remainder of this paper, we draw on theoretical and histori-
cal argumentation to flesh out the following insight: the ability to choose suitable informa -
tion measures (including the prior probabilities) involves the capacity to make informed 
(intellectual, scientific, insightful, and skilful) guesses at unknown features of reality. These 
guesses are, in turn, informed by the guessing agent’s ability to reduce uncertainty. Cur -
rently, active inference is a natural counterpart to what noninductive ampliative inferences 
do, especially those that can formally be characterised as abduction. Integral to our argu -
mentation is that the notion of surprise (such as the strength of surprise in terms of novelty, 
unexpectedness, doubt and irritation, among other context-dependent influences and their 
varying degrees of intensity) plays a role in determining the pathways through which new 
information contributes to reasoners’ probabilities conditioned on evidence.
4 Free Energy Minimisation as Biological Rationality
4.1 Predictive Processing and State Management
The brain hypothesises on the expectations of the likely causes of the data it receives. When 
applied to the brain, predictive processing (PP) theory indicates that the brain is in the 
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
business of using approximate Bayesian mechanisms to optimise its posterior beliefs on 
the basis of sensory information. The brain’s prior beliefs about the world are registered in 
the brain’s generative models. Generative models register the likelihood of data given their 
causes, as well as prior beliefs about the causes. The brain predicts the states of the world 
and modifies and updates its generative models on the basis of the received sensory inputs. 
PP specifies perceptions in terms of inverted models of the likelihood of causes of sen -
sations—assuming that they are retroductive (i.e., formally abductive; Pietarinen & Beni, 
2021) inferences or mappings from sensations to causes.
Similarly, FEP promises profound insights into the unified core of life, perception, cog -
nition, and action. In summary, FEP provides a mathematical formulation of how adaptive 
systems can stay in nonequilibrium steady states. Under normal circumstances, the envi -
ronment is constantly changing, and organic systems must resist the dispersing forces of 
the changing environment by keeping the amount of their sensory entropy low. Entropy is 
the average surprise, which is defined as the negative log probability of an outcome. Free 
energy is an information-theoretic measure that bounds the surprise given some generative 
model (Friston, 2010, p. 127). The agent must keep its internal entropy low relative to the 
states that it occupies in spacetime to maximise its survival.
The bearing on this paper’s topic is that the FEP, as a guiding theory in computational 
neuroscience, posits that biological systems, including human cognitive agents, act in ways 
that minimise free energy or uncertainty about the world around them. By doing so, these 
systems maintain a form of internal coherence while actively engaging with their environ -
ments. Through the lens of FEP, rationality is reconceived not only as a sublime mathemati-
cal notion but also as a process deeply intertwined with survival. Agents are understood 
to form probabilistic inferences about the causal structure of their environments, enabling 
them to act adaptively in a manner that secures their continued existence. This process, 
although grounded in basic biological mechanisms, scales up to encompass sophisticated 
intellectual enterprises that require the agent to make predictions and decisions in complex 
and ambiguous circumstances.
The FEP perspective introduces a novel way to integrate these two senses of rational -
ity: the sublime and the mundane. The sublime mathematical formulation of free energy 
minimisation closely parallels probabilistic inference mechanisms. In contrast, the mundane 
aspect concerns the practical logic of optimising survival. These two forms of rationality 
are mutually reinforcing. The capacity for forming accurate probabilistic inferences about 
the environment provides a fundamental cognitive grip on causality. However, this grip is 
not an end in itself but serves the larger goal of survival optimisation, a form of rationality 
rooted in the practical demands of existence. We term the reinforcing situation “agential 
corationality.”
Although the FEP depends on a variational Bayesian framework to express its theoreti -
cal content, the theoretical content itself cannot be reduced to mere Bayesian formalism. 
The content designates the contribution of minimising uncertainty to maximising survival. 
To integrate the concept of enhancing cognitive models with biological realities, the FEP 
suggests that organisms that have survived (and are fit) have succeeded in reducing the 
inaccuracy of their models of the environment. When considered from a normative stand -
point, we propose that, to survive, organisms must develop accurate representations of the 
spatiotemporal structure by minimising predictive errors under the FEP. Some agents can 
estimate the precision of their expectations about the world as well as their inferences about 
1 3
A.-V. Pietarinen, M. D. Beni
the hidden states of the world. For such agents, relative entropy is decomposed into the 
divergence2 between entropy and expected utility. This speaks to the distinction between 
intrinsic and extrinsic rewards in embodied cognition (Friston et al., 2013). The idea of 
the divergence or uncertainty that must be minimised is at the very heart of PP-FEP. The 
immaculate variational Bayesian framework developed by Friston and colleagues aims at 
capturing an organism’s tendency to minimise its uncertainty.
4.2 Markov Blankets and Embodied Boundaries
The discrepancy between the active and sensory states of an organism increases the uncer -
tainty in the cognitive system. This is not a purely formal notion but a theoretical one that, 
under a realist interpretation, may refer to the states of affairs in the real world. However, 
modern science usually employs mathematical tools to represent its theoretical content, and 
PP-FEP is no exception. As such, distinction or conditional independence is represented by 
Markovian models in the context of FEP (Hipólito et al., 2021; Kirchhoff, 2018; Ramstead 
et al., 2020). Markovian blankets are Bayesian network models: the Markov blanket of a 
node consists of the node’s parents, children, and coparents of its children. In the context 
of FEP, Markov blankets are introduced to show how the internal states of all kinds of 
self-organising systems—from macromolecules to organisms and societies—can be distin-
guished from their external states. Markov blankets provide models of biophysical boundar-
ies in embodied systems. These boundaries consist of sensory states and active states whose 
patterns of dependencies are identifiable based on their spatial locations, provided that Mar-
kov blankets are embodied in biophysical systems (Palacios et al., 2020). Because the inside 
and outside of self-organising structures are statistically independent, they can regulate the 
amount of their sensory entropy through manipulation of their internal states.
We observe that the conditional independence between internal and external states is 
regimented into a Bayesian account of the maximisation of model evidence or marginal 
likelihood. However, the theoretical content is not Markov blankets or any mathematical 
Bayesian fact. In realist reading, theoretical content refers to natural facts (Beni, 2021; Bru-
inberg et al., 2022). The formalism of Markov blankets is used to regiment representations 
of the way that mechanisms of perception/cognition are related to fundamental biological 
mechanisms of self-reorganising complex systems.
The core idea of constructors is similar to the models arising from the agential perspec -
tive to active inferences, as both accommodate conditional expectations (i.e., likelihoods 
of the hypothesis on evidence) of what the future states would or could be like, given the 
evidence accumulated in agential generative models. Not only are there expectations about 
future states; organisms can actually find improved ways of choosing policies that satisfy 
the fulfilment of their expectations in the future. At any event, the conclusion of active 
inference is a conjecture about the feasibility of adopting certain strategy profiles that the 
organism entertains over time integrals. Expressed in the form of our natural language, the 
anticipatory conditional states that, “If I were to exist in the next state, then I would display 
certain classes of properties…” Such statements involve reference to real possibilities of 
what would, could, or might be the case in the states of affairs that are conceivable for the 
2  This is called Kullback–Leibler (KL) divergence, which is a measure of the difference between the approxi-
mate posterior distribution and the true posterior distribution.
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
agent. This, in other words, is a paraphrase of the kernel of Peirce’s pragmatist perspective 
to rationality, as we argue in the final section of this paper.
First, let us recap. The mechanisms of Bayesian inference that an organism applies to 
represent the structure of the world to itself are grounded in the dynamics of the organism’s 
informational interaction with the environment through its statistically definitive borders. In 
this fashion, via PP-FEP, the mechanisms of bounded rationality can be grounded in exis -
tential facts about the survival of the organism. Bounded rationality is defined in terms of 
variational Bayesian inferences that optimise free energy based on model evidence. Theory 
can be represented formally, but the content needs not be confused with the formalism that 
is used for representational purposes.
In the next section, we argue that the expected utility defence of norms of rationality 
can be effectively grounded in the FEP. This is because the FEP suggests that living sys -
tems, including cognitive processes, aim to minimise surprise or free energy, which can be 
seen as a way to minimise uncertainty or inaccuracy in their internal models of the world. 
In the context of rational decision-making, this aligns with the notion that rational agents 
should strive to make choices that maximise their expected epistemic utility. In this context, 
epistemic utility can be viewed as a measure of how beneficial a belief or action is for the 
acquisition of knowledge.
5 Abduction and the Pragmatist Turn
The normative aspects of probabilism can be explained in terms of the organism’s natural 
tendency to frequently occupy the same state. The capacity to stay in nonequilibrium steady 
states underlies the coherence of the organism’s beliefs and behaviours. The theoretical 
content of PP-FEP indicates that by believing and behaving coherently, agents whose rela -
tive entropy levels are constrained can maximise their survival. (This assumes that relative 
entropy is decomposable into the divergence between entropy and expected utility; see Fris-
ton et al., 2013.)
5.1 From Surprise To Hypothesis
In our pragmatist approach, we deconstruct the elevated conception of rationality’s norms 
into the fundamental aspects of an organism’s state occupancy. This mildly reductionist 
perspective centers on the assumption that the organism must occupy some states more 
than others to maximise its survival. This means that the organism must decrease the diver-
gence between the set of distributions of probabilities in states that it should occupy (the 
sense of ‘should’ is here evolutionary, cashed out in terms of a descent in adaptive behav -
iour, see Beni, 2020). Therefore, conforming to probabilistic calculus is just a manifestation 
of the natural tendency to minimise uncertainty by fixing coherent conjectures (Peirce’s 
‘investigands’), resulting in generalizable habits of acting in certain ways in certain kinds 
of circumstances (Peirce’s pragmaticism). The main insight here is the grounding of norms 
of rationality in the cognitive mechanisms delineated by the FEP, which underscores that 
living systems, including cognitive entities, are inherently driven to minimise free energy or 
surprise, which is synonymous with reducing uncertainty.
1 3
A.-V. Pietarinen, M. D. Beni
In the context of rationality norms, the imperative of minimising uncertainty is para -
mount. This is because high levels of uncertainty can lead to inaccurate judgments and 
decisions, which, in turn, can have adverse consequences for an organism’s ability to navi-
gate its environment and attain its goals, which explains how, by forming coherent beliefs 
and utilising them in decision-making, individuals reduce uncertainty and, consequently, 
enhance their capacity for rational and effective action. To support this general claim, we 
build upon recent attempts at grounding inferential thinking in the FEP framework. For 
example, Seddon ( 2022) proposed psychology and neuroscience to reinforce Lipton’s 
(2004) account of inference to the best explanation (IBE). An outline of Seddon’s argument 
is as follows (Seddon, 2022, p. 3):
P1. Best-guess theory aims to explain how the brain interprets information. The brain 
uses a “process very similar to abduction to make best-guess interpretations of sensory 
information.”
P2. “There is considerable empirical support from both psychology and neuroscience 
for Best-guess theory.”
P3. “[H]uman sensory systems are the result of hundreds of millions of years of evolu -
tion, they are very good at interpreting sensory information.
From P1, P2, and P3, it follows that:
C1. Without conscious effort, our brains use a process remarkably similar to abduction 
as a powerful way of interpreting sensory information.
P4. “Lipton’s (2004) IBE theory argues that scientists use abduction when creating theo-
ries to explain phenomena in the real world.”
From C1 and P4, it follows that
C2. Lipton is right to claim that scientists use abduction and explanatory inferences.
Overall, the argument purports to indicate that “IBE theory is likely to be valid because it 
proposes that theory building relies on abduction, and brain research shows that a process 
very similar to abduction is a powerful way of interpreting sensory information.” Seddon’s 
application of scientific evidence from best-guess theory (drawn from the context of predic-
tive coding theory, see Friston, 2012; Rao & Ballard, 1999) meshes with Lipton’s ( 2004) 
Bayesian account of explanatory inferences. The fact that versions of best-guess theory have 
recently been proven useful in shedding light on similar philosophical discussions may rein-
force this optimism. (For a similar scientifically informed defence of Lipton’s account, see 
Beni, 2019, p. 135 ff., and for an application of Bayesian accounts of cognition to Peirce’s 
account of abduction, see Beni & Pietarinen, 2021).
5.2 Skill Vs. Accuracy in Inference
Similarly, it is worth exploring how the FEP’s account of living systems, including cogni -
tive entities, which are inherently driven to minimise free energy or surprise, aligns with 
traditional justifications of rationality in terms of accuracy. The FEP posits that organisms 
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
aim to reduce the divergence between their internal models and the external world, thereby 
enhancing their cognitive grip on the environment. This is achieved through the minimisa -
tion of relative entropy, a measure of the discrepancy between desired states and attainable 
states over time (Friston et al., 2013). This approach closely resembles theories of bounded 
rationality, where an organism, utilising Bayesian inference, can optimise the free energy 
bound on model evidence. By minimising this bound, the organism effectively enhances the 
accuracy of its predictions, ensuring that its cognitive models better align with environmen-
tal contingencies.
However, while this framework provides a sophisticated account of how living sys -
tems increase accuracy and reduce uncertainty, it does not fully answer the primary ques -
tion posed by this paper: why should rationality, in a normative sense, be justified by the 
processes through which self-organising systems increase the accuracy of their models? 
Although the FEP explains how organisms act to optimise their survival by refining their 
internal models, this account remains semi-mechanistic and does not, by itself, offer a com-
pelling rationale for why these processes should be considered rational in a human sense. In 
the following section, we strive to develop a pragmatist response to this question, drawing 
on Peirce’s insights into abduction, skill, and the nature of the inquiry to suggest that ratio-
nality, when understood pragmatically, involves more than mere accuracy—it requires skill-
ful and context-sensitive engagement with the environment, aimed at achieving successful 
outcomes in a complex and uncertain world.
6 Skill Scores Over Accuracy: Adaptive Norms of Rationality
We are now ready to address the main question of the paper: What compelling reasons do 
we have that agents actually use FEP in the process of forming probabilistic inferences that 
maximise their survival in the world? In the pursuit of an answer, we draw on the works of 
Peirce, whose contributions to the philosophy of abduction and scientific reasoning offer 
crucial insights into how we might ground FEP’s account of minimising free energy in 
a broader, pragmatic theory of rationality. Our general insight is that Peirce’s concept of 
abductive reasoning provides a practical complement to the FEP. Abduction is the process 
of generating explanatory hypotheses in the face of novel or unexpected phenomena. We 
argue that rationality emerges through the generation of cognitive models that not only 
strive for accuracy but also reflect deeper engagement with uncertainty and error correction.
6.1 From Accuracy to Skill
Abduction does not guarantee truth or accuracy in any immediate or pressing senses. The 
abductive mode of inference serves as a guiding principle for further inquiry. Previous work 
has developed this insight into a pragmatic defence of abduction reinforced by theoretical 
findings from the FEP literature. Here, we argue that Peirce’s approach offers a critical 
refinement: rationality involves not only forming hypotheses that reduce discrepancy but 
also developing the skill to do so in ways that are sensitive to the complexity of the environ-
ment and the consequences of error (Beni & Pietarinen, 2021). This idea speaks directly 
to the scientific idea that the minimisation of free energy, as posited by FEP, can be under -
stood as a biological imperative that drives the organism to maintain stable conditions for 
1 3
A.-V. Pietarinen, M. D. Beni
survival. However, from a rational standpoint, it is not merely the reduction in free energy 
that justifies an agent’s actions; rather, it is the ability to adapt those actions to the complex-
ity and unpredictability of the world. This, too, can be explained in terms of the pragmatic 
assessment of which hypotheses are worth pursuing, which errors can be tolerated, and how 
the trade-offs between accuracy and other cognitive goals (such as efficiency and adaptabil-
ity) are negotiated (Pietarinen & Beni, 2021).
This insight provides leverage for arguing that the FEP’s measure of accuracy is supple-
mented by a deeper account of how organisms achieve rationality through their capacity 
for skilled inference. It is not enough for an agent to minimise free energy in a mechanistic 
sense; the agent must also demonstrate the capacity to navigate the uncertainties of the envi-
ronment in a way that reflects a deeper cognitive engagement with the world. This engage-
ment is precisely what Peirce’s account of abduction and skill elucidates: rationality is not 
about reducing surprise but about doing so in a way that is sufficiently context sensitive, 
error correct, and pragmatically justified in wider and strategic senses of justification. This 
justification of rationality within the FEP framework demands recognition of the role that 
skill plays in managing uncertainty and guiding logical inferences.
Our proposal brings together a sublime mathematical (probabilistic) notion of rationality 
and a mundane notion that concerns the logic of optimising survival. From a pragmaticist 
point of view, these two senses of rationality are interdependent and noncircular. On the one 
hand, the mechanism of minimising free energy is similar to processes of forming proba -
bilistic inferences that allow the organism to obtain a cognitive grip on the causal structure 
of its environment. As rudimentary as this capacity for cognitive grip on causes and effects 
may be, it also provides the basis for a sophisticated form of probabilistic inference applied 
in complicated forms of intellectual enterprises.
6.2 Peirce Skill Score: Metrics and Adequacy
Here follows our plot twist. Accuracy measures that were adopted much later to justify the 
rationality of Bayesian reasoning originated from Peirce’s paper published in Science in 
1884. The Kullback–Leibler (KL) divergence measure is actually among many alternative 
accuracy measures from the late 19th century, including one that Peirce proposed in his 
1884 note, which is now known as the Peirce skill score (PSS), and variously also known 
as the true skill statistic, Hanssen–Kuiper discriminant, or, in medical diagnostics, Youden’s 
index. The threshold that maximises relevant statistical indices is in the PSS calculated as 
the coincidence of the event’s posterior probability with its prior probability. The highest 
PSS point can be used as a measure of the classifier’s skill as the point that maximises the 
value of the prediction:
 PSS = Hits
Total events− False alarms
Total nulls = Hit rate− False alarm rate (1)
As a 2 × 2 matrix, PSS is calculated with four values: (ad – bc)/[(a + c) (b + d)], where a is the 
number of positive forecasts and positive occurrences of an event (hits), b positive forecasts 
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
and negative occurrences (false positives), c negative forecasts and positive occurrences 
(false negatives), and d negative forecasts and negative occurrences of the event (hits).3
The PSS is still considered to be the best scoring table method for measuring, among 
other factors, the forecast accuracies of significant weather events (Manzato, 2007). Ebert 
& Milne ( 2021) argue that “the Peirce Skill Score is the only score that meets all three 
adequacy constraints” for rare events. These adequacy constraints include the following 
three requirements.
1. Skilled predictions are better than chance; that is, the set of predictions is an improve-
ment in the levels of predictions obtained from the source of random chance. Conse -
quently, the quality of skill leads to a reduction in both Type 1 (false positive) and Type 
2 (false negative) statistical error classes.4
2. Skilled predictions have the right direction of fit , namely, predictions are evaluated 
against the standard of events that actually occurred, not by having events match 
predictions.
3. Skilled prediction tables are sensitive to error weighting . Hasty prediction has con -
sequences and, occasionally, even severe consequences. The scoring method that 
mitigates Type 2 errors compared with the method that makes equally proportionate 
mitigation of Type 1 errors can be argued to be more valuable and have less repair cost 
associated with it.
These constraints, one needs to emphasise, also hold for rare and severe events. This is 
not generally the case for the other accuracy measures. We submit that the essential episte-
mological element in significant scientific discoveries is t ocome up with hypotheses that 
are both hard to come by and impactful. The purpose of scientific inquiry is not to extol 
3  As we argue, justifications of rationality should revoke inadequate accuracy measures and look for scores 
that represent the skill of the forecasting agent, not merely the accuracy of various types of expectations. This 
desideratum is missing from Leitgeb & Pettigrew’s ( 2010a, b) account but is imperative to block unwanted 
results that follow from adopting an ultraconservative strategy for achieving any proportion-correct score – 
as Peirce famously noticed a long time ago, “an ignoramus in tornado studies can predict no tornadoes for 
a whole season, and obtain an average of fully ninety-five per cent” (G, 1884, p. 126). The converse can be 
noticed to obtain for any such strategy to minimise inaccuracy (see fn 4 below). No risk, no suprisal, no FEP 
to be applied.
4  Therefore, PSS is immune to the paradoxes of maximising accuracy and minimising inaccuracy. Charitably 
assuming that synchronic and diachronic expected local forms of accuracy, respectively, suffice to justify 
probabilism and conditionalization, accuracy would indicate that the agent is compelled to update its beliefs 
by valid rules of inference. Leitgeb and Pettigrew ( 2010a, b) did not validate the main assumption of the 
detailed justification of Bayesianism. Their justification draws on an assumption about the agent’s tendency 
to approximate the truth. Approximating truth has been operationalised in terms of minimising inaccuracy. 
Why does the agent need to do that? This question was not addressed by Leitgeb & Pettigrew (2010b). Nor do 
they justify their basic assumption (Leitgeb & Pettigrew, 2010a, p. 215). However, to paraphrase the previous 
quip from Peirce, the opposite of the paradox of accuracy also arises: pending further constraints on minimis-
ing inaccuracy; “an ignoramus in tornado studies can predict tornadoes for a whole season, and obtain an 
average of meagre five per cent” inaccuracy, as one now minimises the inaccuracy of one’s predictions well 
below the level of an expert who correctly predicts rare events more often than not. The ability to justify 
assumptions about agents’ tendencies to approximate the truth is assessed outside the domain of Leitgeb and 
Pettigrew’s particular piece of formal reasoning. This means that the particular piece of formalism adopted 
is too narrow to offer a full story about norms of rationality. We argue that this proposal indeed leaves much 
to be desired since the Brier score, which Leitgeb and Pettigrew adopted as the preferred accuracy measure, 
turns out to be an inadequate measure for rare events owing to its low resolution for small differences in 
predicting rare and unexpected occurrences.
1 3
A.-V. Pietarinen, M. D. Beni
competent but largely random guesses; the purpose is to match conjectures to the way the 
world may be while catching errors by systematic probing. Good statistical inference sub -
jects propositions to severe testing: a properly conducted method of efficient and organised 
experimentation follows a protocol that would find a claim false if it indeed were false, with 
high probability. Such methods can be trusted to catch false claims: they attempt to probe 
enough of the ways in which we could be wrong  about the proposition put forth. Good 
protocols and research agendas are those that expedite discovery and thus economise the 
process by following such a method of error correction.
6.3 Foundations of Skill: from History to Neurophysiology
On a historical note, Peirce did not explicitly connect his 1884 skill score metrics with the 
qualities of abductive reasoning. However, the mere fact that both have stood the test of the 
development of modern scientific methods so well is evidence that the PSS is the right tool 
for evaluating the quality of guessing in scientific abduction. After all, abduction strives to 
conclude that there is reason for further action to be taken regarding what is asserted in its 
conclusion. This is provoked whenever scientists encounter surprising phenomena, anoma-
lies, unforeseen events, or other cerebral frictions and cognitive irritations (aided by non -
epistemic principles such as the economy of research, Chiffi et al., 2020; Chiffi & Pietarinen, 
2019; Mukhopadhyay, 2022).
From this pragmatist perspective, the norms that justify rationality when rare and cog -
nitively weighty ideas and hypotheses are being guessed at the bottom cannot be the kinds 
of norms derived from measures of (expected) accuracy. They have to be norms derived 
from the predictor’s skill and must be allied with adequacy conditions such as those framed 
above by (1)–(3). Accuracy alone achieves better results than chance does; it has no pre -
ferred direction of fit, and it assigns no weights to its error classes, which manifestly do have 
nonsymmetric reward structures.
In contrast, among the measures that are well calibrated to compute rare-event accura -
cies, PSS is not only the original measurement metric provided for such cases but also 
represents the only calculation that satisfies all three adequacy constraints.
This insight is particularly compatible with an enactivist interpretation of the FEP and 
its measure of accuracy. Enactivism posits that cognition is not merely a matter of internal 
representation but is rooted in the organism’s dynamic interaction with its environment 
Kirchhoff & Robertson, 2018; Korbak, 2021). When applied to the context of the present 
discussion, enactivism is congenial to the idea that rationality cannot be justified solely by 
abstract accuracy metrics. Rather than treating accuracy as an isolated criterion, enactivism 
emphasises how organisms skillfully engage with their surroundings, using embodied prac-
tices that allow them to actively generate meaning and adapt to novel situations. This active, 
embodied engagement mirrors Peirce’s emphasis on predictor skill, where rationality is not 
simply a matter of matching predictions to outcomes but of skilfully navigating uncertainty 
in a way that reflects the organism’s ongoing involvement with its environment.
To return to the context of Peirce’s work, a common limitation of measures of accuracy 
and skill is that they are typically binary: either the prediction matches the event, or it does 
not: the hypothesis turns out to be true in the guise of the theory or it is rejected by sys -
tematic probing of falsification. In reality, conjectures are asserted in their promissory and 
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
indefinite dressing, where ‘strictly true’ and ‘strictly false’ are the extremal values assigned 
to a future state of those conjectured assertions.
Three remarks on this charge are in order. First, hypotheses that have made it through 
an abductive reasoning filter do sufficiently attract the inquirer’s mind such that scientists 
are typically invited to give them a try: even though their probabtive conjectures may well 
be wrong, something might remain from the process of deriving them for later uses and 
the progress of inquiry. Such hypotheses possess, in Peirce’s terms, “young truth,” cryptic 
qualities that ought to attract our careful attention beyond their status as possibilities or mere 
may-bes (Pietarinen, 2021).
Second, multicategory extensions of scoring methods are available for nonbinary events 
(Rodwell, 2011). This is good news: their fundamental qualities tend to remain the same 
as those of binary classifiers. PSS, for example, is among those metrics that can indeed be 
expanded to cover more than two categories, the fact that Peirce stated in his 1884 paper and 
while he did not prove that assertion in the published paper, a consistent nonbinary extension 
was developed in Rodwell ( 2011). On the other hand --- and here is another crucial point 
of our criticism of accuracy --- those other measures of accuracy, such as the Brier score or 
Kullback–Leibler (KL) divergence, do not have this property and hence remain inadequate 
measures for assessing rare-event occurrences when generalised to three or more categories. 
For such limitative reasons, accuracy measures, despite having prominently been proposed 
in the recent literature to justify norms of rationality, ought to be approached with caution 
as proper measures of accuracy when evaluating the inquirer’s true skill as a discoverer.
Third, the generalisation of the PSS to three-category cases is sufficient to cover all mul-
ticategory cases since the third category can be taken to tally all those guesses that have an 
‘undefined’ or ‘undetermined’ value.5 In general, extensions to multicategory cases apply 
equitability constraints in skill scoring (Gandin & Murphy, 1992; Joyce, 1998). Equitablity 
constraints guarantee fair differentiation between the scoring of two unskilled (random, 
static, noisy) systems on the one hand and the cases in which one of the unskilled systems 
receives a certain amount of skill and thus should fairly and equitably score above the other, 
on the other hand. In brief, Peirce’s original result both justifies and is justified using equi -
tability constraints in numerical estimations of skilful predictions.
Recalling the preceding section, whatever qualities and constraints a good notion of skill 
we ultimately take the predictors and guessers to possess, the faculties required for genu -
inely skilful performance have much to do with the development and performance of agents’ 
neurophysiological mechanisms. As explained, such mechanisms are the omphalos of the 
project of naturalising probabilistic reasoning (Seddon, 2022, p. 3) (On recent work specifi-
cally on abduction, predictions, and neuroscience, see e.g. Cevolani, 2023; Coraci et al., 
2022; Costa et al., 2022; Vallverdú & Sans Pinillos, 2022). Moreover, neurophysiological 
facts are at play in meeting the three adequacy constraints outlined above. For example, 
fine-grained synaptic plasticity addresses uncertainty by forecasting the error bars in addi -
tion to weights, which is conducive to succeeding with predictions above chance levels. 
Second, these mechanisms serve to fit predictions to the world rather than the other way 
around, which other proposed measures do not accomplish. Third, evolution has ensured 
that the brain errs on the side of false negatives. Above all, fine-graining guarantees sen -
sitivity to small differences in noisy data when predicting weak perceptual phenomena, as 
5  The technique is the same as in partial models in logic where the third category is not strictly speaking a 
separate value but the residual that the binary model leaves undetermined.
1 3
A.-V. Pietarinen, M. D. Beni
supported by theories of neuronal group voting, the ‘thousand-brains’ hypothesis, and other 
similar and recent findings (Hawkins et al., 2019).
The moral from Peirce’s 1884 schema is that distinguishing successful forecasts, accu -
rate predictions, and correct conjectures that may result from statistical chance, ill directions 
of fit, or insensitivity to error-weighing, from genuinely skilful performances that produce 
such results, is of perennial importance to the long-term success of anticipations, predic -
tions, and optimisations. Accuracy measures alone meet none of the natural adequacy con -
straints for truly skilful scoring. Among the many skill scores proposed during the century, 
only the PSS has been shown to meet these constraints. To repeat the lesson in the context 
of the present paper once more, it is not the hit-rate success, accuracy, or correctness but 
skill, coupled with the relevant adequacy constraints (1)–(3), that justify the adoption of 
probabilities in rational decision making.
One last point. A verification of the goodness of the classifier is the business of sig -
nal-detection theory (SDT), which dates back to Swets et al. ( 1961) and Green and Swets 
(1966). SDT aims at verifying the goodness of a classifier, a problem with no shortage of 
applications ranging from deep learning, image processing, medical diagnostics and psy -
chological testing to forecasting and forensic sciences. SDT has proven to be a promising 
methodology for computing optimal inferential thresholds (McNicol, 1972). The early goal 
was to statistically understand the mechanisms by which the human perceptual system is 
able to decide and report upon the reception of signals amidst insignificant noise. Interest -
ingly, the origins of SDT also date back to Peirce’s early experimental work on perception 
with Jastrow (Peirce & Jastrow, 1884) and their famous rebuttal of the Weber–Fechner dis-
crete threshold principle in psychophysics. They argued that the threshold principle relies 
on unviable Laplacian principles of probability, notably the principle of indifference. In its 
place, proper procedures of statistical inference must be adopted to evaluate claims of non-
continuous threshold functions that aim at detecting real signals from background noise. In 
SDT, the procedure involves the development of the notion of an ideal observer. It is this 
insight that SDT shares with the Peirce skill score (PSS): a skilful inquirer aims at matching 
predictions to the world; a task in which an ideal or omniscient being, free from all bias and 
noise, would succeed without failure. This involves setting up counterfactuals, as in abduc-
tion. In the PSS, the perfect predictor has the expected value of 1, and the score is rewritten 
as 1 – [(c/a + c)] - [b/b + d]]. (To recap, a is the number of predicted and observed events; b is 
predicted but not observed; c is not predicted but observed; d is the number of not predicted 
and not observed events; a generalisation of this formalisation to nonbinary category cases 
is straightforward.) The conceptual connection between the two methods should come as no 
surprise, given that Peirce wrote and published his skill score paper in the same year (1884), 
no less than a month apart when his pioneering psychophysical experiments with his student 
Joseph Jastrow on small differences in sensations were reported for the first time.
Accuracy measures fail to justify probabilism. Whether we can do so, we are first drawn 
to the late 19th century fountain of ideas that had tapped into the relevant insights early on. 
Moreover, the FEP itself has known antecedents in Hermann von Helmholtz’s work (Helm-
holtz, 1866), which inspired Peirce’s critical appraisal of discovery through the develop -
ment of statistical and logical methods. The theory of abduction matured in Peirce’s work 
throughout the late 19th century. What abductive reasoning does, and what the PSS-type 
accuracy matrices that track the forecaster’s skills add, to the question of the justification 
of prior probabilities in probabilistic foundations for rationality is that Bayes’s compressed 
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
formula actually signifies multiple convergent historical and systematic storylines that 
strive to make explicit the dynamics of how the priors arise and how they undergo change 
into posterity.
7 Rationality Recast: Pragmatist-Naturalist Synthesis
This paper was concerned with the justification of probabilism. We developed a general 
pragmatist-naturalist perspective on the question of norms of rationality. To reinforce our 
pragmatist-naturalist justification of rationality, we consolidated the justification of rational-
ity on the basis of the survival value of concrete evolutionary mechanisms captured by FEP 
formalism. The shift in view now becoming non-Bayesian, naturalist justification, from 
coupling priors and their update with abductive reasoning, we are led to a skill measure 
originally defined as the Peirce skill score to supplant faulty measures of accuracy previ -
ously proposed to justify the rationality of probabilism. The result is a historically informed, 
pragmatist–naturalist justification of norms of rationality, in which skill and not accuracy 
takes priority.
The proposed synthesis of the FEP and Peircean pragmatism reorients rationality from 
static coherence to dynamic, adaptive skill. By grounding norms in survival-driven infer -
ence and contextualised skill metrics, we resolve the tension between biological impera -
tives and normative epistemology. The FEP’s variational mechanisms, when coupled with 
abduction’s conjecture-making and the Peirce skill score, reveal rationality as an enactive 
negotiation between environmental complexity and organismic constraints. This framework 
not only challenges Bayesian accuracy-centric models but also invites interdisciplinary dia-
logue: cognitive science gains a biologically grounded account of inference, whereas phi -
losophy reclaims rationality as a pragmatic, evolutionary achievement. Future work could 
operationalise this approach in AI ethics, decision neuroscience, or cross-cultural episte -
mology—domains where skill, uncertainty, and adaptive forecaster success converge. Ulti-
mately, rationality emerges not as a rulebook but as the lived capacity to thrive in a world 
of surprises.
Author Contributions Authorship statement
•All authors whose names appear on the submission made substantial contributions to the conception or 
design of the work; the acquisition, analysis, and interpretation of data; drafted the work and revised it criti-
cally for important intellectual content; approved the version to be published; and agree to be accountable for 
all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work 
are appropriately investigated and resolved.
Funding Open access funding provided by Hong Kong Baptist University Library. This work was supported 
by the institutional academic grant RC-FNRA-IG/22–23/ARTS/01.
Data Availability No datasets were generated or analysed during the current study.
Declarations
Ethics Approval This is a theoretical study, and no ethical approval is needed.
Competing Interests The authors declare no competing interests.
1 3
A.-V. Pietarinen, M. D. Beni
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, 
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as 
you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons 
licence, and indicate if changes were made. The images or other third party material in this article are 
included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. 
If material is not included in the article’s Creative Commons licence and your intended use is not permitted 
by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the 
copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
References
Aguilera, M., Millidge, B., Tschantz, A., & Buckley, C. L. (2022). How particular is the physics of the free 
energy principle? Physics of Life Reviews, 40, 24–50. https://doi.org/10.1016/j.plrev.2021.11.001
Andrews, M. (2021). The math is not the territory: Navigating the free energy principle. Biology and Philoso-
phy, 36(3). https://doi.org/10.1007/S10539-021-09807-0
Baltieri, M., Buckley, C. L., & Bruineberg, J. (2020). Predictions in the eye of the beholder: an active infer-
ence account of Watt governors. http://arxiv.org/abs/2006.11495
Beni, M. D. (2018). The reward of unification: A realist reading of the predictive processing theory. New 
Ideas in Psychology, 48, 21–26.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / j .  n e w i d  e a p s y c  h . 2 0  1 7 . 1 0 . 0 0 1
Beni, M. D. (2019). Conjuring cognitive structures: Towards a unified model of cognition. In A. Nepomu -
ceno-Fernández, L. Magnani, F. Salguero-Lamillar, C. Barés-Gómez, & M. Fontaine (Eds.), Model-
Based Reasoning in Science and Technology. MBR 2018 (pp. 153–172). Springer.  h t t p s : / / d o i . o r g / 1 0 . 1 0 
0 7 / 9 7 8 - 3 - 0 3 0 - 3 2 7 2 2 - 4 _ 1 0       
Beni, M. D. (2020). An integrative explanation of action. Biosystems, 198, 104266.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / 
j .  b i o s y  s t e m s .  2 0 2 0  . 1 0 4 2 6 6
Beni, M. D. (2021). A critical analysis of Markovian monism. Synthese, 199, 6407–6427.  h t t p s : / / d o i . o r g / 1 0 
. 1 0 0 7 / S 1 1 2 2 9 - 0 2 1 - 0 3 0 7 5 - X       
Beni, M. D. (2022). Dosis Sola facit venenum: Reconceptualising biological realism. Biology & Philosophy, 
37(6), 1–18. https://doi.org/10.1007/S10539-022-09884-9
Beni, M. D., & Pietarinen, A. V . (2021). Aligning the free-energy principle with Peirce’s logic of science and 
economy of research. European Journal for Philosophy of Science,  11, 94.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / S 1 
3 1 9 4 - 0 2 1 - 0 0 4 0 8 - Y       
Bruinberg, J., Dołęga, K., Dewhurst, J., & Baltieri, M. (2022). The emperor’s new Markov blankets. Behav-
ioral and Brain Sciences, 45, Article E183. https://doi.org/10.1017/S0140525X21002351
Cevolani, G. (2023). Introduction to abduction and cognitive neuroscience. In L. Magnani (Ed.), Handbook 
of abductive cognition. Springer.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  0 7 / 9 7  8 - 3 - 0  3 0 - 6 8 4  3 6 - 5  _ 9 2 - 1
Chiffi, D., & Pietarinen, A. V . (2017). Fundamental uncertainty and values. Philosophia, 45, 1027–1037.
Chiffi, D., & Pietarinen, A. V . (2019). Risk and values in science: A Peircean view. Axiomathes, 29, 329–346.
Chiffi, D., Pietarinen, A. V ., & Proover, M. (2020). Anticipation, abduction and the economy of research: The 
normative stance. Futures, 115, 102471.
Clark, A. (2016). Surfing uncertainty. Oxford University Press.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  9 3 / a c  p r o f :  o s o / 9 7  8 0 1 9  0 
2 1 7 0 1 3 . 0 0 1 . 0 0 0 1
Colombo, M., & Palacios, P. (2021). Non-equilibrium thermodynamics and the free energy principle in biol-
ogy. Biology & Philosophy, 36(5), 1–26. https://doi.org/10.1007/S10539-021-09818-X
Colombo, M., & Wright, C. (2018). First principles in the life sciences: the free-energy principle, organicism, 
and mechanism. Synthese, 198(14), 3463–3488. https://doi.org/10.1007/S11229-018-01932-W
Colombo, M., Elkin, L., & Hartmann, S. (2021). Being realist about Bayes, and the predictive processing 
theory of mind. The British Journal for the Philosophy of Science, 72(1), 185–220.  h t t p s : / / d o i . o r g / 1 0 . 1 
0 9 3 / b j p s / a x y 0 5 9       
Coraci, D., Calzavarini, F., & Cevolani, G. (2022). Reverse inference, abduction, and probability in cognitive 
neuroscience. In L. Magnani (Ed.), Handbook of abductive cognition . Springer.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  0 
7 / 9 7  8 - 3 - 0  3 0 - 6 8 4  3 6 - 5  _ 6 0 - 1
Costa, T., Liloia, D., Ferraro, M., & Manuello, J. (2022). Plausible reasoning in neuroscience. In L. Magnani 
(Ed.), Handbook of abductive cognition. Springer.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  0 7 / 9 7  8 - 3 - 0  3 0 - 6 8 4  3 6 - 5  _ 7 4 - 1
de Finetti, B. (1992). Foresight: its logical laws, its subjective sources. Springer.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / 9 7 
8 - 1 - 4 6 1 2 - 0 9 1 9 - 5 _ 1 0       
1 3
Beyond Bayesian Accuracy: Skill, Abduction, and the Free Energy…
Ebert, P. A., & Milne, P. (2021). Methodological and conceptual challenges in rare and severe event forecast-
verification. Natural Hazards and Earth System Sciences,  22(2), 539–557.  h t t p s : / / d o i . o r g / 1 0 . 5 1 9 4 / n h e 
s s - 2 0 2 1 - 2 1 5       
Friston, K. J. (1994). Functional and effective connectivity in neuroimaging: A synthesis. Human Brain Map-
ping, 2(1–2), 56–78. https://doi.org/10.1002/hbm.460020107
Friston, K. J. (2010). The free-energy principle: A unified brain theory? Nature Reviews Neuroscience,11(2), 
127–138. https://doi.org/10.1038/nrn2787
Friston, K. J. (2012). Predictive coding, precision and synchrony. Cognitive Neuroscience,3(3–4), 238–239. 
https://doi.org/10.1080/17588928.2012.691277
Friston, K. J., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., & Dolan, R. J. (2013). The 
anatomy of choice: Active inference and agency. Frontiers in Human Neuroscience, 7, 598.  h t t p s : / / d o i 
. o r g / 1 0 . 3 3 8 9 / f n h u m . 2 0 1 3 . 0 0 5 9 8       
G. (1884). Letter to the editor: Tornado predictions. Science 4(80), 126–127.  h t t p s : / / d o i . o r g / 1 0 . 1 1 2 6 / s c i e n c 
e . n s - 4 . 8 0 . 1 2 6       
Gandin, L. S., & Murphy, A. H. (1992). Equitable skill scores for categorical forecasts. Monthly Weather 
Review, 120, 361–370.
Green, D. M., & Swets, J. A. (1966). Signal detection theory and psychophysics. Wiley.
Hájek, A. (2008). Arguments for—or against—probabilism? British Journal for the Philosophy of Science , 
59(4), 793–819. https://doi.org/10.1093/bjps/axn045
Hartmann, S. (2020). Bayes Nets and rationality. In M. Knauff, & W. Spohn (Eds.), Handbook of rationality. 
MIT Press. https://philpapers.org/rec/HARBNA-5
Hawkins, J., Lewis, M., Klukas, M., Purdy, S., & Ahmar, S. (2019). A framework for intelligence and corti-
cal function based on grid cells in the neocortex. Neural Circuits, 12.  h t t p s : / / d o i . o r g / 1 0 . 3 3 8 9 / f n c i r . 2 0 
1 8 . 0 0 1 2 1       
Helmholtz, H. V . (1866). Concerning the perceptions in general. In J. P. C. Southall (Ed.), Treatise on physi-
ological optics (pp. 1–37). Dover.
Hipólito, I., Ramstead, M. J. D., Convertino, L., Bhat, A., Friston, K., & Parr, T. (2021). Markov blankets in 
the brain. Neuroscience and Biobehavioral Reviews, 125, 88–97.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / j .  n e u b i  o r e v . 2  
0 2 1 .  0 2 . 0 0 3
Hohwy, J. (2013). The predictive mind. Oxford University Press.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  9 3 / a c  p r o f :  o s o / 9 7  8 0 1 9  
9 6 8 2 7 3 7 . 0 0 1 . 0 0 0 1
Joyce, J. M. (1998). A nonpragmatic vindication of probabilism. Philosophy of Science,  65(4), 575–603. 
https://doi.org/10.1086/392661
Kirchhoff, M. (2018). Hierarchical Markov blankets and adaptive active inference: Comment on “Answer -
ing Schrödinger’s question: A free-energy formulation” by Maxwell James Désormeau Ramstead et al. 
Physics of Life Reviews, 24, 27–28. https://doi.org/10.1016/J.PLREV .2017.12.009
Kirchhoff, M. D., & Robertson, I. (2018). Enactivism and predictive processing: A non-representational view. 
Philosophical Explorations, 21(2), 264–281.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  8 0 / 1 3  8 6 9 7 9  5 . 2 0 1 8  . 1 4 7  7 9 8 3
Klein, C. (2018). What do predictive coders want? Synthese, 195(6), 2541–2557.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / s 1 
1 2 2 9 - 0 1 6 - 1 2 5 0 - 6       
Korbak, T. (2021). Computational enactivism under the free energy principle. Synthese,198(3), 2743–2763. 
https://doi.org/10.1007/S11229-019-02243-4
Leitgeb, H., & Pettigrew, R. (2010a). An objective justification of bayesianism i: Measuring inaccuracy. 
Philosophy of Science,77(2), 201–235. https://doi.org/10.1086/651317
Leitgeb, H., & Pettigrew, R. (2010b). An objective justification of bayesianism II: The consequences of mini-
mizing inaccuracy*. Philosophy of Science,77(2), 236–272. https://doi.org/10.1086/651318
Lipton, P. (2004). Inference to the best explanation. Routledge/Taylor and Francis Group.  h t t p s : / / d o i . o r g / 1 0 
. 4 3 2 4 / 9 7 8 0 2 0 3 4 7 0 8 5 5       
Ma, M., & Pietarinen, A. V . (2017). Let us investigate! Dynamic conjecture-making as the formal logic of 
abduction. Journal of Philosophical Logic,47(6), 913–945. https://doi.org/10.1007/s10992-017-9454-x
Manzato, A. (2007). A note on the maximum Peirce skill score. Weather and Forecasting, 22(7), 1148–1154. 
https://doi.org/10.1175/WAF1041.1
McNicol, D. (1972). A primer of signal detection theory. George Allen & Unwin.
Mukhopadhyay, S. (2022). Abductive inference and C. S. Peirce: 150 years later. Journal of Quantitative 
Economics. https://doi.org/10.1007/s40953-022-00332-9
Palacios, E. R., Razi, A., Parr, T., Kirchhoff, M., & Friston, K. (2020). On markov blankets and hierarchical 
self-organisation. Journal of Theoretical Biology, 486, 110089.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . j t b i . 2 0 1 9 . 1 1 0 
0 8 9       
Palm, G., Wennekers, T., Kogo, N., & Trengove, C. (2015). Is predictive coding theory articulated enough to be 
testable? Frontiers in Computational Neuroscience, 9, 111. https://doi.org/10.3389/fncom.2015.00111
1 3
A.-V. Pietarinen, M. D. Beni
Parr, T., & Pezzulo, G. (2021). Understanding, explanation, and active inference. Frontiers in Systems Neu-
roscience, 15, 127. https://doi.org/10.3389/FNSYS.2021.772641
Peirce, C. S. (1884). The numerical measure of the success of predictions. Science, 4(93), 453–454.  h t t p s :  / / d 
o i  . o r g / 1  0 . 1 1  2 6 / s c  i e n c e  . n s - 4 .  9 3 . 4  5 3 . b
Peirce, C. S., & Jastrow, J. (1884). On small differences in sensation. Memoirs of the National Academy of 
Science, 3, 75–83.
Pettigrew, R. (2016). Accuracy and the laws of credence. Accuracy and the laws of credence. Oxford Univer-
sity Press.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  9 3 / a c  p r o f :  o s o / 9 7  8 0 1 9  8 7 3 2 7 1 6 . 0 0 1 . 0 0 0 1
Pietarinen, A. V . (2021). Abduction and diagrams. Logic Journal of the IGPL, 29(4), 447–468.  h t t p s : / / d o i . o 
r g / 1 0 . 1 0 9 3 / j i g p a l / j z z 0 3 4       
Pietarinen, A. V ., & Beni, M. D. (2021). Active inference and abduction. Biosemiotics, 14, 499–517.  h t t p s : / / 
d o i . o r g / 1 0 . 1 0 0 7 / s 1 2 3 0 4 - 0 2 1 - 0 9 4 3 2 - 0       
Ramstead, M. J. D., Friston, K. J., & Hipólito, I. (2020). Is the free-energy principle a formal theory of 
semantics? From variational density dynamics to neural and phenotypic representations. Entropy,22(8), 
889. https://doi.org/10.3390/e22080889
Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some 
extra-classical receptive-field effects. Nature Neuroscience, 2(1), 79–87. https://doi.org/10.1038/4580
Rodwell, M. J. (2011). On Peirce’s motivation for equitability in forecast verification. Monthly Weather 
Review, 139, 3667–3669. https://doi.org/10.1175/MWR-D-11-00167.1
Seddon, P. B. (2022). Nature chose abduction: Support from brain research for Lipton’s theory of inference 
to the best explanation. Foundations of Science,27, 1489–1505.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / S 1 0 6 9 9 - 0 2 1 - 0 
9 8 1 1 - 3       
Swets, J. A., Tanner, W. P., & Birdsall, T. G. (1961). Decision processes in perception. Psychological Review, 
68, 301–340. https://doi.org/10.1037/h0040547
Vallverdú, J., & Sans Pinillos, A. (2022). The foundations of creativity: Human inquiry explained through the 
neuro-multimodality of abduction. In L. Magnani (Ed.), Handbook of abductive cognition. Springer.  h t 
t p s :   /  / d o  i . o r  g /  1 0 .  1 0  0 7 /   9 7  8 -  3  - 0 3 0  - 6  8   4 3 6  - 5 _ 7 1 - 1
van Es, T., & Hipolito, I. (2020). Free-Energy Principle, Computationalism and Realism: a Tragedy . Pre-
print. http://philsci-archive.pitt.edu/18497/
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.
Ahti-Veikko Pietarinen is Professor at Hong Kong Baptist University. Research Associate of the Centre for 
Applied Ethics and member of the Transdisciplinary Theoretical and Ethical Artificial Intelligence Lab, 
Pietarinen’s research on the human and artificial minds includes emerging human-machine reasoning compe-
tences, capabilities, and defects; diverse and creative aspects of cognitions; signs, meanings, and notations; 
history and philosophy of logic, pragmatism, and Charles Peirce’s manuscripts.
Majid D. Beni  is Associate Professor of Philosophy at the Middle East Technical University, Ankara. His 
research spans philosophy of science, philosophy of cognitive science, and philosophy of mind, with par -
ticular focus on the autonomy of biology, the metaphysics of agency, and the conceptual foundations of 
consciousness studies.
1 3