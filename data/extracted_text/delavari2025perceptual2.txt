Perceptual Motor Learning with Active Inference Framework for
Robust Lateral Control
Elahe Delavari1, John Moore 2, Junho Hong 3, and Jaerock Kwon 4
This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
Abstract— This paper presents a novel Perceptual Motor
Learning (PML) framework integrated with Active Inference
(AIF) to enhance lateral control in Highly Automated Vehicles
(HA Vs). PML, inspired by human motor learning, emphasizes
the seamless integration of perception and action, enabling
efficient decision-making in dynamic environments. Traditional
autonomous driving approaches—including modular pipelines,
imitation learning, and reinforcement learning—struggle with
adaptability, generalization, and computational efficiency. In
contrast, PML with AIF leverages a generative model to
minimize prediction error (“surprise”) and actively shape
vehicle control based on learned perceptual-motor represen-
tations. Our approach unifies deep learning with active in-
ference principles, allowing HA Vs to perform lane-keeping
maneuvers with minimal data and without extensive retraining
across different environments. Extensive experiments in the
CARLA simulator demonstrate that PML with AIF enhances
adaptability without increasing computational overhead while
achieving performance comparable to conventional methods.
These findings highlight the potential of PML-driven active
inference as a robust alternative for real-world autonomous
driving applications.
I. INTRODUCTION
The rapid evolution toward fully Highly Automated Ve-
hicles (HA Vs) demands decision-making frameworks that
not only emulate human cognition but also robustly gen-
eralize across diverse and unforeseen environments without
extensive retraining. Traditional approaches in autonomous
driving—including Modular Pipelines (MP) [1], Imitation
Learning (IL) [2][3], and Reinforcement Learning (RL)
[4]—have paved the way for significant advances. However,
each of these methods faces inherent limitations. MPs, while
modular and interpretable, suffer from error propagation and
inflexibility; IL is capable of mimicking expert behavior yet
often fails when confronted with novel scenarios; and RL,
despite its ability to learn through interaction, is heavily
reliant on finely tuned reward-shaping, which is a process
that is both labor-intensive and highly task-specific.
To address the aforementioned limitations, we propose
Perceptual Motor Learning (PML) [5][6] with Active Infer-
ence Framework (AIF) [7]. PML refers to the process by
which sensory inputs (perception) and motor outputs (action)
become integrated to enable effective interaction with the
environment. In early human development, this integration
*This work was supported in part by the Ford/University of Michigan
Research Alliance Program (10.13039/100007270) and in part by the
National Science Foundation (NSF) under Grant MRI 2214830.
1E. Delavari, 3J. Hong, and 4J. Kwon are with the University of
Michigan-Dearborn. {elahed, jhwr, jrkwon}@umich.edu
Corresponding author: J. Kwon.
2J. Moore is with Ford Motor Company. jmoor422@ford.com
is crucial: while fundamental movement skills (e.g., hop-
ping, jumping, running, and balance) lay the groundwork,
perceptual motor development connects these skills with
sensory processing, enabling children to develop body aware-
ness, spatial awareness, directional awareness, and temporal
awareness [8]. To realize a PML, we propose to use AIF,
which has emerged as a promising alternative to conventional
control theories, grounded in theories of human brain func-
tion that posit the continuous prediction of sensory inputs
and the minimization of discrepancies between expected and
actual observations [9]. By unifying perception and action
within a single generative framework, active inference allows
an autonomous agent to actively anticipate environmental
changes rather than merely reacting to them. Nevertheless,
our work extends this paradigm by incorporating principles
from perceptual motor development—a concept tradition-
ally rooted in developmental psychology. Fig. 1 shows an
overview of our method.
Environment Agent
Sensing
Learning
Algorithm
Policy
Observation
Action
Perceived 
Environment
World
Fig. 1. Overview of Perceptual Motor Learning (PML) with Active
Inference Frameworks (AIF). The agent senses the environment through
its sensors and makes observations. These observations shape the agent’s
beliefs and perception of the world. Based on its perception, the agent takes
actions that, in turn, affect the real environment. (Icon made by Freepik from
www.flaticon.com)
In our framework for autonomous driving, we draw a
parallel to this human developmental process. Our model
leverages a deep learning architecture to fuse perceptual
and motor information, constructing an internal world model
that captures the causal dynamics of the environment. This
model enables the system to predict the consequences of
its actions and adapt to new road conditions without retrain-
ing—addressing the core issue of generalization that plagues
traditional IL and RL methods. Rather than blindly imitating
expert behavior or relying on externally defined rewards, our
approach infers how the world works to support robust and
flexible decision-making.
To evaluate the effectiveness of our approach, we applied
it to the lateral control of self-driving vehicles in a simulated
arXiv:2503.01676v2  [cs.RO]  5 Mar 2025
urban environment. The model was trained on data from
a simulated large urban area with multiple-lane highways
and tested in distinctively different settings: a town with
mountainous roads and a small town characterized by multi-
ple bridges. This diverse testing environment highlights the
model’s ability to generalize across varied road structures.
In summary, the key contributions of our work are:
• To our best knowledge, this study is the first integration
of PML with AIF in HA Vs. By leveraging an internal
world model to learn the causal relationships between
actions and environmental changes, our system adapts
to new scenarios without requiring retraining, ensuring
robust performance across diverse environments.
• PML with AIF allows greater resilience and adaptabil-
ity in novel driving conditions, whereas traditional IL
methods suffer from long tail problems.
• PML with AIF streamlines training, making it both
computationally efficient and easily scalable across dif-
ferent driving tasks by eliminating the need for complex
reward engineering.
II. R ELATED WORK
There are different methods for controlling an HA V in
an environment. One of the most popular methods is MP
in autonomous driving systems, which generally consists
of dedicated modules for perception, prediction, planning,
and control. While this approach offers clear advantages in
terms of interpretability and modularity, it also has several
limitations. Errors in upstream modules (e.g., perception)
can propagate downstream, leading to compounding errors
in prediction, planning, and control. MPs can be inflexible,
making it difficult to adapt the system to new tasks or
environments without significant re-engineering. This rigid-
ity can limit the scalability and versatility of the system.
Joint optimization across modules is challenging due to the
different nature of tasks each module performs [10][11].
Another method that is vastly used is Imitation Learn-
ing (IL) which has been a popular method for developing
autonomous driving systems due to its ability to mimic
expert behavior from demonstrations and a lot of work
has been done in this domain [2][3][10][12]. Traditional IL
methods, however, often struggle with generalizing to unseen
states not covered by the expert’s demonstrations. When the
agent encounters scenarios outside the training data, it may
fail to perform adequately, leading to safety and reliability
issues [10][13]. Additionally, in complex scenarios such as
urban driving, high-level decisions (e.g. choosing to turn left
or right at an intersection) cannot be easily inferred from
perceptual input alone, leading to ambiguity and oscillations
in the agent’s behavior. To overcome this, Codevilla et al.[10]
introduced Conditional Imitation Learning (CIL), where the
driving policy is conditioned on high-level commands.
Direct perception methods are also the other category
of methods that are used [14]. Sauer et al. [15] propose
Conditional Affordance Learning (CAL), a direct perception
approach for autonomous urban driving. This method maps
video inputs to intermediate representations (affordances)
that are then used by a control algorithm to maneuver the
vehicle. CAL aims to combine the advantages of MP and
IL by using a Deep Neural Network (DNN) to learn low-
dimensional, yet informative, affordances from high-level
directional inputs. The approach handles complex urban
driving scenarios, including navigating intersections, obeying
traffic lights, and following speed limits. However, choosing
the best set of affordances is challenging. Furthermore, the
effectiveness of this method relies heavily on precise affor-
dance predictions. Any inaccuracies or errors in predicting
affordances may result in less than optimal or even hazardous
driving decisions.
Use of RL has shown promising results in training au-
tonomous agents through trial and error[4][16][17]. However,
it also has its own set of limitations. RL methods typi-
cally necessitate extensive sample data to develop effective
policies, which can be especially challenging in real-world
settings where data acquisition is costly and time-intensive.
Achieving a balance between exploration (testing new ac-
tions) and exploitation (leveraging known actions) presents
a substantial hurdle in RL. Designing appropriate reward
functions is critical for RL but can be difficult and problem-
specific. Poorly designed rewards can lead to unintended
behaviors or sub-optimal performance. Ensuring the stability
and convergence of RL algorithms is challenging, especially
in complex and dynamic environments. Instabilities during
training can lead to unreliable and unpredictable agent be-
havior [10][11].
Liang et al.[17] introduce Controllable Imitative Rein-
forcement Learning (CIRL), which combines IL and RL
to address the challenges of autonomous urban driving
in complex, dynamic environments. CIRL utilizes a Deep
Deterministic Policy Gradient (DDPG) approach to guide
exploration in a constrained action space informed by human
demonstrations. This method allows the driving agent to
learn adaptive policies tailored to different control commands
(e.g., follow, turn left, turn right, go straight). Despite using
IL to guide initial exploration, CIRL still faces challenges
associated with exploration in RL. The agent may struggle
to discover optimal policies in highly dynamic and unpre-
dictable environments without effective exploration strate-
gies.
Active inference has been gaining interest in HA V re-
search. Friston et al. [18] proposed it as a process theory
of the brain that minimizes prediction error ( surprise) by
updating its generative model or inferring actions that reduce
uncertainty. While applied to simple simulated agents, its
use in realistic scenarios has been limited. Catal et al. [19]
integrated deep learning with active inference for real-world
robot navigation, using high-dimensional sensory data to
construct generative models without predefined state spaces.
Their work demonstrated successful navigation in a ware-
house setting, though in a controlled environment. de Tinguy
et al. [20] introduced a scalable hierarchical model for
autonomous navigation, combining curiosity-driven explo-
ration with goal-directed planning using visual and motion
perception. Their multi-layered approach (context, place,
motion) facilitated efficient navigation with fewer steps than
other models, though validation was limited to simulations
in a mini-grid environment. Nozari et al. [13] combined
active inference with IL to overcome IL’s limitations. Their
framework employs a Dynamic Bayesian Network (DBN) to
encode expert demonstrations and includes an active learning
phase for policy refinement. This approach dynamically bal-
ances exploration and exploitation, enhancing adaptability.
However, DBNs impose high computational overhead, mak-
ing them impractical for resource-constrained autonomous
driving platforms.
To the best of our knowledge, most prior studies on action-
based future scene prediction focus on standard driving
scenarios [21][22] or simplified environments like Atari
games [23][24], where predicted images closely resemble
the input. Even in these cases, models often struggle to gen-
erate high-quality predictions, leading to blurriness. Unlike
previous studies that focus on predicting future scenes in
simple or highly constrained environments, we apply this
idea within a PML with AIF, integrating an encoder-decoder
DNN architecture [25] as the forward transition model. Our
approach explicitly models the effect of steering actions on
future observations. By leveraging a generative model within
AIF, we improve scene prediction, leading to more reliable
action selection and decision-making under uncertainty.
III. M ETHOD
Our approach integrates PML with AIF to develop a com-
putationally efficient and explainable autonomous driving
system. PML allows the agent to learn from sensory experi-
ences, dynamically linking perception with motor execution,
while active inference ensures that decisions minimize pre-
diction errors based on the Free Energy Principle (FEP)[26].
In this work, decision-making is guided by the vehicle’s
current state, sensory observations from a single front-facing
camera, an internal world model, and predictions of how the
environment will change based on potential actions. Notably,
our model operates solely on visual perception, without
relying on privileged information such as LiDAR, GPS, or
HD maps. This design choice ensures that the model learns
and generalizes from vision alone, making it more adaptable
and comparable to human driving capabilities.
A. Perceptual Motor Learning with Active Inference
In autonomous systems, PML enables agents to build
internal representations of the environment and adapt their
actions dynamically based on sensory feedback. By lever-
aging PML principles, an autonomous vehicle can enhance
its control strategies through learned associations between
environmental perception and motor execution. This forms
the foundation for our approach, where we integrate PML
within an AIF to optimize decision-making under uncer-
tainty. According to FEP, agents minimize Free Energy (FE)
[26], a measure of surprise or prediction error. However,
since FE is intractable to compute directly, an approximation
called Variational Free Energy (VFE) [7] is used. VFE
enables Bayesian inference, allowing an agent to infer hidden
states from sensory observations. By minimizing VFE, the
agent continuously updates its beliefs, ensuring its internal
model remains consistent with sensory input. In our case,
input images serve as observations, while the states remain
hidden. Consequently, we model the environment as a Par-
tially Observable Markov Decision Process (POMDP), where
the agent must infer hidden states from sensory data. The
key variables and functions in our framework are defined as
follows:
• Variables:
– Hidden state: st ∈ RdS
– Action: at ∈ RdA
– Observation: ot ∈ RdO
– Data: D = {ot , at , ot+1}T
t=0
• Functions:
– Observation model: ˆot = fo(st )
– Forward transition model: ˆot+1 = fs(ot , at )
– Inverse transition model: ˆat = f −1
s (ot , ot+1)
– Distance model: dt = fd(ot , ˆot )
– Policy: π : st → at
The VFE serves as an upper bound on surprise and is used
to infer hidden states:
F = Eq(st+1)
h
−ln p(ot+1|st+1)
i
+ DKL
 
q(st+1) ∥ p(st+1|ot )

,
(1)
where p(ot+1|st+1) is the likelihood of the next observa-
tion given the inferred hidden state, p(st+1|ot ) is the prior
belief about the next state given past observations, and
DKL
 
q(st+1) ∥ p(st+1|ot )

measures the divergence between
the approximate posterior and the prior belief, ensuring the
update remains close to the true distribution.
To implement PML with AIF for autonomous driving,
we employ a two-stage process consisting of offline task-
agnostic training and online action selection. In the offline
phase, we pretrain a forward transition model fs [27] to
learn transition dynamics, enabling the agent to predict how
future observations evolve under different actions. Once
trained, this model remains fixed and does not require
further updates. In the online task-specific phase, the agent
predicts future states for all possible actions at each timestep
and selects the action that minimizes the EFE to optimize
decision-making under uncertainty.
In the context of autonomous driving, AIF involves the
vehicle continuously predicting its sensory inputs, which
consist of visual observations of the road, and minimizing
prediction error through either perception, by updating its
internal model, or action, by making steering adjustments.
To model perception, We employ a forward transition model
to predict future observations given the current state and
action. Meanwhile, action selection is performed through
sensory-motor simulation, which effectively serves as an
inverse transition model f −1
s by inferring the action that
would lead to a desired future state. In this process, the
agent evaluates covert actions before execution, selecting the
one that minimizes the EFE. Given that our task focuses
on lane-keeping, we incorporate preference-based [28] action
selection, encouraging actions that maintain lane alignment.
B. Perception Model
For perception, a forward transition model fs is trained to
contain the necessary information for the agent to understand
the environment in the context of the driving task. This fs
is used to generate future images influenced by the steering
angle. Training process of the forward transition model fs
is depicted in Fig. 2. For learning a forward transition
model fs, this paper uses a method inspired by action-based
representation learning [29].
As human beings, it is simple to predict how the environ-
ment will change based on the change of steering action,
as humans are exceptionally good at generating missing
information. However, it is not straightforward for a feed-
forward DNN to predict the details of a scene when provided
only one frame and the corresponding action—especially
when the future scene is significantly different from the
current scene. The primary challenge lies in the network’s
difficulty in learning a strong correlation between the current
and future scene, even with the help of action.
We used a U-Net with an Xception-style architecture [25],
incorporating task-specific modifications to better capture the
action and observation dependency. U-Net [30] is well-suited
for tasks with limited training data, as it efficiently extracts
hierarchical features using convolutional layers in both the
encoding and decoding paths. Its skip connections help
preserve spatial information from early layers, enhancing
feature retention. These properties make it an ideal choice
for the forward transition model.
C. Action Selection Model
Minimizing VFE ensures that the agent’s internal beliefs
align with observations, allowing it to infer states accurately.
While VFE is crucial for state estimation, it does not dictate
which actions the agent should take. For decision-making,
the agent minimizes Expected Free Energy (EFE) [7], which
predicts the impact of different actions on future states. To
make goal-directed decisions, the agent minimizes the EFE,
which considers both epistemic (information-seeking) and
pragmatic (goal-aligned) components:
G = Eq(ot+1,st+1|at )
h
DKL (q(st+1|at ) ∥ ppref(st+1))
−H(q(ot+1|st+1, at ))
i
,
(2)
where DKL (q(st+1|at ) ∥ ppref(st+1)) encourages the selection
of actions that lead to desired states, and H(q(ot+1|st+1, at ))
represents uncertainty reduction, guiding exploration.
The agent balances goal-directed behavior with learning
from its environment by seeking smaller G. In our work,
we focused on minimizing the first part of EFE with action
selection, as the exploration is not beneficial for our case due
to offline training of the forward transition model fs. This
approach allows the vehicle to adapt its driving behavior
in real-time, making steering adjustments that minimize
prediction errors. To find an action that minimizes G, the
agent generates covert actions to imagine future states caused
by the actions. To align with goal-directed behavior, we
compare predicted observations ˆot+1 with a preferred future
sensory state opref using Structural Similarity Index Measure
(SSIM) [31]. The SSIM index is between -1 and 1, where 1
is perfect similarity, and -1 is maximum dissimilarity. Thus,
our distance model ( fd) can be defined as 1 −SSIM. The
action can be selected by (3). Fig 3 illustrates the action
selection process.
argmin
i
(1 −SSIM( ˆot+1, opref)). (3)
The preferred future sensory state is represented as prefer-
ence. For instance, in a lane-keeping scenario, it corresponds
to a straight road within the same lane. Fig. 4 illustrates
the predicted semantic segmented images alongside their
ground truth, highlighting the SSIM difference. Road only
grayscaled version of the semantic segmented image used
to simplify the problem and reduce computational overhead
while maintaining the core driving information.
In this paper, we define steering as the action, while
speed is kept constant to eliminate extra complexity. The
future scene is predicted based on the current scene and the
selected action. The forward transition model fs provides
prior knowledge of how different steering actions influence
the future scene for the HA V . To guide decision-making, the
model also requires an expectation of the future—referred
to as preference. Using the current state and the transition
model fs, the HA V selects an action by comparing predicted
future scenes with the expected preference. The action asso-
ciated with the highest similarity to this expected outcome
is chosen.
IV. E XPERIMENTAL SETUP
A. DNN Architecture and Training
1) PML: We used a U-Net Xception-style model in
Keras 3 [32]. The input image was encoded with the
encoding part of the U-Net and then the action was added by
concatenating the action through Dense layers. For this task,
the upsampling layers are changed to the conv2d transpose
layer as they can make the reconstructed image less blurry.
In addition, as only the road part of the image is considered,
grayscale images are used. So, one channel in both the
input layer and output layer is used. The input and output
images have the same size of 160 × 160 × 1. The RELU
activation function is used for encoding and decoding layers
and the ELU activation function is used for the part where the
action was added. For the output layer, the sigmoid activation
function was used. A learning rate of 1e-4 and batch size of
128 were used to train this model.
We tested the trained model in the CARLA (Car Learning
to Act) [33] simulator using a covert action approach to
find an optimal steering command. The model predicted the
future image for twenty different steering values between -1
and 1, with a step size of 0.1, and the predicted images were
compared with a preference for the task. The comparison was
s*
t
fo
ot
enc
st
at
fd
Environment
(Hidden State)
Agent
Perceived 
Environment
fa
Action 
Generator 
dec
ôt
s*
t+1
fo
ot+1
enc
st+1
fs ôt+1
at+1
fd
dec
ôt+1
s*
t+2
fo
ot+2
enc
st+2
fs ôt+2
…
dec
ôt+2
…
Fig. 2. Task-agnostic offline perception model training. A hidden state s∗
t is perceived through the observation model fo. An observation ot is encoded
and kept inside the agent as a perceived state. When an action at is applied to the environment by the agent, the causal changes in the environment should
be observed as ˆot+1. The forward transition model fs is being trained using the distance model fd with ot+1 and ˆot+1. The encoder and decoder can be
the inside of fs in practice.
ot fs
atatatatai
t
ôi
t+1
oc
t
fd
Fig. 3. Action selection. The forward transition model fs generates causal
observations based on covert actions.oc
t is preference for a task. The distance
model fd calculates the dissimilarity between the observations and the
preference and finds which action will generate the most similar observations
to the preference.
Fig. 4. Predicted and the ground truth image for semantic segmented
images with the SSIM difference between them.
conducted using SSIM, selecting the action corresponding
to the predicted image with the highest SSIM score. This
process ensured that for each input image, the optimal
steering action was chosen, enabling the car to drive in the
simulator. The fourth future image prediction at a constant
speed was used for driving in the experiments.
2) Imitation Learning: Imitation learning encompasses
various approaches, including Behavioral Cloning (BC) and
Inverse Reinforcement Learning (IRL). To validate the PML
agent’s adaptability to new scenarios, we used BC for com-
parison, training a model to replicate expert demonstrations
through supervised learning and evaluating their online driv-
ing performance. The DNN architecture of the BC agent is
shown in Table I. A learning rate of 10 −3 and batch size of
64 were used to train this DNN.
TABLE I
DNN ARCHITECTURE USED FOR THE BC AGENT
Layer Type Description Kernel Size
/ Units
Activation
Input Input Image (160, 160, 3) -
Conv2D + BN 32 filters, stride 2 5x5 ReLU
Conv2D + BN 64 filters, stride 2 3x3 ReLU
Conv2D + BN 128 filters, stride 2 3x3 ReLU
Dropout 20% rate - -
Conv2D + BN 256 filters, stride 2 3x3 ReLU
Flatten Flatten - -
Dense Fully connected 512 ReLU
Dropout 20% rate - -
Dense Fully connected 128 ReLU
Dropout 20% rate - -
Dense Fully connected 64 ReLU
Dense Fully connected 1 -
B. Simulator and Dataset
CARLA is a high-fidelity simulator that provides a re-
alistic environment for developing, training, and validating
autonomous driving technologies in urban settings. CARLA
has multiple versions, each supporting different towns and
features. We used CARLA 0.9.14 to test and validate the
proposed PML with AIF in terms of online driving perfor-
mance. Then, we also used CARLA 0.8.4 to compare other
previously published methods in the original CARLA bench-
mark (referred to as CoRL2017) because the CoRL2017
benchmark is only compatible with the 0.8.4 version.
We trained our PML agent in Town06, which has long,
many-lane highways with many highway entrances and exits.
To conduct fair comparisons with a BC method, the BC
agent was also trained in the same town. Then, these two
agents were tested in Town01 (a small town with a river
and several bridges) and Town04 (a small town embedded
in the mountains with a loop highway). To assess their
robustness, we conducted tests across four different lanes
within each town. The test tracks used in these towns were
selected randomly and are illustrated in Fig. 5. We selected
three different driving scenarios in each town: straight, one-
turn, and two-turn. They are labeled as [A], [B], and [C],
respectively.
1) PML: For a task-agnostic exploration of the environ-
ment, 36,000 images in Town06, which enables rigorous
testing across various lane configurations. To expand the
dataset, images were augmented by flipping them along
with their corresponding steering angles, resulting in a total
of 72,000 image-steering pairs. A deliberate zigzag driving
pattern was used during data collection to ensure a diverse
and evenly distributed dataset covering the full range of
steering angles from -1 (full left) to 1 (full right). This
approach ensures the model learns the causal relationship
between steering inputs and visual observations, improving
its ability to generalize across different driving conditions.
Images used as preferences for the PML agent are pre-
sented in Fig. 6. Since the road structure varies between
towns, different preferences were used to adapt the model
accordingly. For benchmark experiments in Town01 and
Town02, a single preference image (the first image in Fig. 6)
was used for both towns, as their road structures were nearly
identical.
To evaluate the effectiveness of our approach, we con-
ducted two sets of experiments. First, we compared our
PML-based method against a BC agent across different
tracks in Town01, Town04, and Town06. Second, we tested
our method using the CoRL2017 benchmark, which is widely
used for evaluating autonomous driving policies. The stan-
dard CoRL2017 evaluation considers only RGB images, but
since our method relies on road-only segmented images, we
added a semantic segmentation camera instead of RGB to
extract relevant information.
2) Imitation Learning: The BC agent was trained on
a dataset collected from Town06, which is the same en-
vironment where the PML agent was trained and tested
across all towns. Training BC agents in Town06 was par-
ticularly challenging due to the presence of multiple lanes,
requiring a diverse dataset covering different lane positions.
Additionally, the vehicle often veered too close to the curb
when navigating turns. To ensure a fair comparison, the BC
agent was trained exclusively for lateral control. To capture
high-quality driving behavior, we collected 61,000 samples,
ensuring an approximately even distribution across different
lanes in the simulator. Despite applying data augmenta-
tion techniques such as brightness adjustments and image
flipping, the results remained unsatisfactory. To improve
performance, we explicitly included flipped versions of the
images in the dataset and applied a normalization strategy.
The collected data was normalized to balance the dataset
across different steering angles. Specifically, we ensured that
each steering bin contained a comparable number of images,
preventing the model from being biased toward more fre-
quently occurring steering values. Although this approach led
to better driving behavior, the agent still exhibited difficulties
in turns and struggled to recover when veering off the track.
To address this, we collected an additional 32,000 sam-
ples, specifically demonstrating recovery maneuvers from
different lanes. After applying the normalization process,
the final dataset consisted of 47,313 images paired with
steering angles, ensuring a well-distributed representation of
different driving scenarios. All models are implemented on
the TensorFlow framework. The lambda Workstation having
two NVIDIA RTX 6000 was used for training the images
of semantic segmentation and RGB images, and NVIDIA
GeForce RTX 4060 was used for training grayscale images
and doing the tests in the CARLA simulator.
V. R ESULTS
A. Performance Comparison
PML proved to be a robust and adaptable agent for
autonomous driving, as evidenced by the comprehensive
analysis presented in Table II. In the table, the deviation
is the average value of the Euclidean distance between the
vehicle’s positions and the corresponding waypoints, and
the success rate shows, across different tracks, how many
tracks were successfully completed. For each track, four
different experiments were conducted, and the success rate
was computed based on the success of these experiments.
The deviations should be considered bearing in mind that
the whole lane in Town01 has a width of 4 meters, and
Town04 and Town06 have a width of 3.5 meters. Table II
presents the performance metrics of PML and BC agents
across various tasks in different urban environments. The
performance is evaluated using two key metrics: average
deviation and success rate. The results indicate that the
PML agent consistently outperforms the BC agent across
various tasks and environments, achieving higher success
rates and lower deviations in most cases. In Town01, PML
demonstrated perfect performance with a 100% success rate
in all tasks, whereas BC struggled. In Town04, PML main-
tained strong performance across tasks, with BC showing
comparable results only in the Straight and Two Turns task.
In Town06, both agents performed similarly in terms of
success rate, but PML exhibited lower deviation, indicating
more precise trajectory tracking. Overall, PML proved to
be a more robust and adaptive approach, particularly in
challenging scenarios.
B. CoRL2017 Benchmark
We also compared our results with various methods,
including Modular Pipeline (MP), Imitation Learning (IL),
and Reinforcement Learning (RL), as implemented in the
CARLA simulator [33]. Additionally, we included bench-
mark results from CIRL [17], CAL [15], and LBC [11], as
shown in Table III. For these studies, Town01 was used for
training, while Town02 served as the testing environment.
Note that both towns were not seen to our PML agent in the
training stage.
A
B
C
A
B
C
A
B
CC
A
B
A
B
C
B
A
C
Fig. 5. Tracks used for testing in Town01, Town04, and Town06 from left to right. [A] shows the straight, [B] shows the one-turn, and [C] shows the
two-turn track.
Fig. 6. The examples of the preferences. The first is for Town01 and
Town02. The center two are for lanes 1 and 2 in Town04. The right two
are lanes 3 and 4 in Town06.
TABLE II
PERFORMANCE COMPARISON
Town Agent Task Avg. Dev. ( ↓) Success (% ↑)
Town01
PML
Straight 0.4080 100.00
One-Turn 0.5400 100.00
Two Turns 0.4860 100.00
Overall 0.4780 100.00
BC
Straight 0.9780 0.00
One-Turn 1.2170 0.00
Two Turns 3.4180 25.00
Overall 1.8710 8.33
Town04
PML
Straight 0.8120 100.00
One-Turn 0.9440 100.00
Two Turns 0.8230 100.00
Overall 0.8597 100.00
BC
Straight 0.6880 100.00
One-Turn 1.7140 0.00
Two Turns 0.8750 100.00
Overall 1.0923 66.67
Town06
PML
Straight 0.7010 75.00
One-Turn 0.5730 100.00
Two Turns 0.6460 100.00
Overall 0.6400 91.67
BC
Straight 1.1630 75.00
One-Turn 1.7840 100.00
Two Turns 1.4790 100.00
Overall 1.4753 91.67
TABLE III
SUCCESS RATE RESULT IN CORL2017 BENCHMARK .
MP IL RL CAL CIRL LBC PML
[33] [33] [33] [15] [17] [11] (Ours)
Town01-Train 98 95 89 100 98 100 96 (Test)
Town02-Test 92 97 74 93 100 100 92 (Test)
Our PML with AIF method outperforms IL and RL in the
straight driving task in Town01, though it is slightly outper-
formed by the other approaches (See Table III). However,
it is important to note that, unlike other methods, we did
not use any training data from Town01, which gives our
approach a strong generalization advantage. In Town02, our
agent demonstrates driving performance comparable to CAL
and MP and surpasses RL in the straight task.
Although CIRL and LBC achieve better results, our
method offers a significantly simpler training pipeline. LBC
employs a two-stage training process, making it more compu-
tationally demanding and resource-intensive. Similarly, CIRL
integrates a dual-stage learning process that adds complexity
in both implementation and hyperparameter tuning. The
need to balance IL and RL in CIRL introduces additional
challenges, such as overfitting to demonstration data or
inadequate generalization from RL. Overall, our PML with
AIF-based approach delivers competitive results, highlight-
ing its potential as an alternative to more complex learning
frameworks.
VI. C ONCLUSION
In this paper, we introduced a novel PML-based method
for controlling an autonomous vehicle, leveraging the FEP
to dynamically adapt to different road types, including two-
lane and multi-lane environments. Our method significantly
enhances the adaptability and performance of HA Vs, achiev-
ing comparable results to more complex baseline methods
in the CARLA benchmark. Our results demonstrate that the
PML agent, despite its simplicity, performs robustly across
various urban environments and driving tasks. It consistently
outperforms BC in terms of lower average deviations and
higher success rates in simulated environments.
Future work will aim to enhance the capabilities of
our PML-based agent by incorporating longitudinal control
(throttle and brake) and addressing more complex tasks such
as lane changing, handling dynamic objects in the driving
scene, and adhering to traffic rules. Additionally, we aim to
validate our approach using real-world data and explore its
application to actual autonomous driving scenarios. While
this study focused on the lane-keeping task, our approach
can be extended to more complex maneuvers, such as lane-
changing.
REFERENCES
[1] C. Badue, R. Guidolini, R. V . Carneiro, P. Azevedo, V . B. Cardoso,
A. Forechi, L. Jesus, R. Berriel, T. M. Paix ˜ao, F. Mutz, L. de Paula
Veronese, T. Oliveira-Santos, and A. F. De Souza, “Self-driving cars:
A survey,”Expert Systems with Applications, vol. 165, p. 113816, Mar.
2021.
[2] D. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a
Neural Network,” in Advances in Neural Information Processing
Systems, vol. 1. Morgan-Kaufmann, 1988.
[3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang,
J. Zhao, and K. Zieba, “End to End Learning for Self-Driving Cars,”
Apr. 2016.
[4] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V .-D.
Lam, A. Bewley, and A. Shah, “Learning to Drive in a Day,” in 2019
International Conference on Robotics and Automation (ICRA) , May
2019, pp. 8248–8254.
[5] J. E. Clark, “On the Problem of Motor Skill Development,” Journal
of Physical Education, Recreation & Dance , May 2007.
[6] “Perceptual and Motor Development Domain - Child Develop-
ment (CA Dept of Education),” https://www.cde.ca.gov/sp/cd/re/
itf09percmotdev.asp.
[7] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference: The Free
Energy Principle in Mind, Brain, and Behavior . MIT Press, Mar.
2022.
[8] D. F. Stodden, J. D. Goodway, S. J. Langendorfer, M. A. Roberton,
M. E. Rudisill, C. Garcia, and L. E. Garcia, “A Developmental
Perspective on the Role of Motor Skill Competence in Physical
Activity: An Emergent Relationship,” Quest, vol. 60, no. 2, pp. 290–
306, May 2008.
[9] K. J. Friston and K. E. Stephan, “Free-energy and the brain,” Synthese,
vol. 159, no. 3, pp. 417–458, Dec. 2007.
[10] “End-to-End Driving Via Conditional Imitation Learning | IEEE
Conference Publication | IEEE Xplore,” https://ieeexplore.ieee.org/
abstract/document/8460487.
[11] D. Chen, B. Zhou, V . Koltun, and P. Kr ¨ahenb¨uhl, “Learning by
Cheating,” in Proceedings of the Conference on Robot Learning .
PMLR, May 2020, pp. 66–75.
[12] H. Xu, Y . Gao, F. Yu, and T. Darrell, “End-To-End Learning of Driving
Models From Large-Scale Video Datasets,” inProceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2017, pp.
2174–2182.
[13] S. Nozari, A. Krayani, P. Marin-Plaza, L. Marcenaro, D. M. G ´omez,
and C. Regazzoni, “Active Inference Integrated With Imitation Learn-
ing for Autonomous Driving,” IEEE Access , vol. 10, pp. 49 738–
49 756, 2022.
[14] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning
affordance for direct perception in autonomous driving,” in Pro-
ceedings of the IEEE International Conference on Computer Vision
(ICCV), December 2015.
[15] A. Sauer, N. Savinov, and A. Geiger, “Conditional Affordance Learn-
ing for Driving in Urban Environments,” in Proceedings of The 2nd
Conference on Robot Learning . PMLR, Oct. 2018, pp. 237–252.
[16] M. Toromanoff, E. Wirbel, and F. Moutarde, “End-to-End Model-
Free Reinforcement Learning for Urban Driving Using Implicit Af-
fordances,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2020, pp. 7153–7162.
[17] X. Liang, T. Wang, L. Yang, and E. Xing, “CIRL: Controllable
Imitative Reinforcement Learning for Vision-based Self-driving,” in
Proceedings of the European Conference on Computer Vision (ECCV),
2018, pp. 584–599.
[18] K. Friston, P. Schwartenbeck, T. Fitzgerald, M. Moutoussis,
T. Behrens, and R. J. Dolan, “The anatomy of choice: Active inference
and agency,” Frontiers in Human Neuroscience , vol. 7, Sep. 2013.
[19] O. C ¸ atal, S. Wauthier, T. Verbelen, C. De Boom, and B. Dhoedt,
“Deep Active Inference for Autonomous Robot Navigation,” Mar.
2020, comment: workshop paper at BAICS at ICLR 2020.
[20] D. de Tinguy, T. Van de Maele, T. Verbelen, and B. Dhoedt, “Spatial
and Temporal Hierarchy for Autonomous Navigation Using Active
Inference in Minigrid Environment,” Entropy, vol. 26, no. 1, p. 83,
Jan. 2024.
[21] Y .-H. Kwon and M.-G. Park, “Predicting Future Frames Using Ret-
rospective Cycle GAN,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2019, pp. 1811–1820.
[22] D. Zhu, H. Chern, H. Yao, M. S. Nosrati, P. Yadmellat, and Y . Zhang,
“Practical Issues of Action-Conditioned Next Image Prediction,” in
2018 21st International Conference on Intelligent Transportation
Systems (ITSC), Nov. 2018, pp. 3150–3155.
[23] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh, “Action-Conditional
Video Prediction using Deep Networks in Atari Games,” in Advances
in Neural Information Processing Systems, vol. 28. Curran Associates,
Inc., 2015.
[24] E. Wang, A. Kosson, and T. Mu, “Deep action conditional neural
network for frame prediction in Atari games,” Technical report, 2017.
[25] F. Chollet, “Xception: Deep Learning With Depthwise Separable
Convolutions,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2017, pp. 1251–1258.
[26] K. Friston, “The free-energy principle: A unified brain theory?” Nature
Reviews Neuroscience, vol. 11, no. 2, pp. 127–138, Feb. 2010.
[27] D. M. Wolpert, Z. Ghahramani, and M. I. Jordan, “An Internal Model
for Sensorimotor Integration,” Science, vol. 269, no. 5232, pp. 1880–
1882, Sep. 1995.
[28] P. Mazzaglia, T. Verbelen, and B. Dhoedt, “Contrastive Active Infer-
ence,” in Advances in Neural Information Processing Systems, vol. 34.
Curran Associates, Inc., 2021, pp. 13 870–13 882.
[29] Y . Xiao, F. Codevilla, C. Pal, and A. Lopez, “Action-based Represen-
tation Learning for Autonomous Driving,” in Proceedings of the 2020
Conference on Robot Learning . PMLR, Oct. 2021, pp. 232–246.
[30] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional
Networks for Biomedical Image Segmentation,” in Medical Image
Computing and Computer-Assisted Intervention – MICCAI 2015 ,
N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, Eds. Cham:
Springer International Publishing, 2015, pp. 234–241.
[31] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality
assessment: From error visibility to structural similarity,” IEEE Trans-
actions on Image Processing , vol. 13, no. 4, pp. 600–612, Apr. 2004.
[32] “Keras documentation: Image segmentation with a U-Net-like
architecture,” https://keras.io/examples/vision/oxford pets image
segmentation/.
[33] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V . Koltun,
“CARLA: An Open Urban Driving Simulator,” in Proceedings of the
1st Annual Conference on Robot Learning . PMLR, Oct. 2017, pp.
1–16.