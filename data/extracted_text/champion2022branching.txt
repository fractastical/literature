arXiv:2111.11276v2  [cs.AI]  24 May 2022
Branching Time Active Inference
empirical study and complexity class analysis
Th´ eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´ s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor:
Abstract
Active inference is a state-of-the-art framework for model ling the brain that explains a wide range of
mechanisms such as habit formation, dopaminergic discharg e and curiosity. However, recent implemen-
tations suﬀer from an exponential (space and time) complexit y class when computing the prior over all
the possible policies up to the time horizon. Fountas et al (2 020) used Monte Carlo tree search to address
this problem, leading to very good results in two diﬀerent tas ks. Additionally, Champion et al (2021a)
proposed a tree search approach based on (temporal) structu re learning. This was enabled by the develop-
ment of a variational message passing approach to active inf erence (Champion et al, 2021b), which enables
compositional construction of Bayesian networks for activ e inference. However, this message passing tree
search approach, which we call branching-time active infer ence (BTAI), has never been tested empirically.
In this paper, we present an experimental study of the approa ch (Champion et al, 2021a) in the context
of a maze solving agent. In this context, we show that both imp roved prior preferences and deeper search
help mitigate the vulnerability to local minima. Then, we co mpare BTAI to standard active inference
(AcI) on a graph navigation task. We show that for small graph s, both BTAI and AcI successfully solve
the task. For larger graphs, AcI exhibits an exponential (sp ace) complexity class, making the approach
Submitted to Neural Networks
Champion et al.
intractable. However, BTAI explores the space of policies m ore eﬃciently, successfully scaling to larger
graphs. Then, BTAI was compared to the POMCP algorithm (Silv er and Veness, 2010) on the frozen
lake environment. The experiments suggest that BTAI and the POMCP algorithm accumulate a similar
amount of reward. Also, we describe when BTAI receives more r ewards than the POMCP agent, and
when the opposite is true. Finally, we compared BTAI to the ap proach of Fountas et al (2020) on the
dSprites dataset, and we discussed the pros and cons of each a pproach.
Keywords: Active Inference, Variational Message Passing, Tree Searc h, Planning, Free Energy Principle
1. Introduction
Active inference extends the free energy principle (Fristo n, 2010; Pitti et al, 2020) to generative models with
actions (Friston et al, 2016a; Da Costa et al, 2020; Champion et al, 2021b) and can be regarded as a form of
planning as inference (Botvinick and Toussaint, 2012). Thi s framework has successfully explained a wide range of
brain phenomena, such as habit formation (Friston et al, 201 6a), Bayesian surprise (Itti and Baldi, 2009), curiosity
(Schwartenbeck et al, 2018), and dopaminergic discharge (F itzGerald et al, 2015). It has also been applied to a va-
riety of tasks such as navigation in the Animal AI environmen t (Fountas et al, 2020), robotic control (Pezzato et al,
2020; Sancaktar et al, 2020; Wirkuttis and Tani, 2021), mult i-vehicle control (Butz et al, 2019), the mountain car
problem (Catal, Ozan and Verbelen, Tim and Nauta, Johannes a nd De Boom, Cedric and Dhoedt, Bart, 2020), the
game DOOM (Cullen et al, 2018) and the cart-pole problem (Mil lidge, 2019).
Active inference builds on a subﬁeld of Bayesian statistics called variational inference (Fox and Roberts, 2012),
in which the true posterior is approximated with a variation al distribution. This method provides a way to
balance the computational cost and accuracy of the posterio r distribution. Indeed, the variational approach is
only tractable because some statistical dependencies are i gnored during the inference process, i.e., the variational
distribution is generally assumed to fully factorise, lead ing to the well known mean-ﬁeld approximation:
Q(X) =
∏
i
Q(Xi)
where X is the set of all hidden variables of the model and Xi represents the i-th hidden variable. Winn and Bishop
(2005) presented a message-based implementation of variat ional inference, naturally called variational message
passing. And more recently, Champion et al (2021b) rigorous ly framed active inference as a variational mes-
sage passing procedure. By combining the Forney factor grap h formalism (Forney, 2001) with the method of
Winn and Bishop (2005), it becomes possible to create modula r implementations of active inference (van de Laar and de Vri es,
2019; Cox et al, 2019) that allows the users to deﬁne their own generative models without the burden of deriving the
2
Branching Time Active Inference
update equations. This paper uses a new software package cal led Homing Pigeon that implements such a modular
approach and the relevant code has been made publicly availa ble on GitHub: https://github.com/ChampiB/Homing-Pigeon.
Arguably, the major bottleneck for scaling up the active inf erence framework was the exponential growth of the
number of policies. In the reinforcement learning literatu re, this explosion is frequently handled using Monte Carlo
tree search (MCTS) (Silver et al, 2016; Browne et al, 2012; Sc hrittwieser et al, 2019). MCTS is based on the upper
conﬁdence bound for trees (UCT), which originally comes fro m the multi-armed bandit problem, and trades-oﬀ
exploration and exploitation during the tree search. In the reinforcement learning litterature, the selection of the
node to expand is carried out using the UCT criterion 1, which is deﬁned as:
UCT (s, a ) = q(s, a ) + Cexplore
P (s, a )
1 + N(s, a ), (1)
where q(s, a ) is the value of taking action a in state s (i.e. q here is not the variational posterior), Cexplore is the
exploration constant that modulates the amount of explorat ion, N(s, a ) is the visit count, and P (s, a ) is the prior
probability of selecting action a in state s. This approach has been applied to active inference in sever al papers
(Fountas et al, 2020; Maisto et al, 2021). Fountas et al (2020 ) chose to modify the original criterion used during
the node selection step that returns the node to be expanded. From equation (9) of (Fountas et al, 2020), one can
see that the UCT formula has been replaced by:
U(s, a ) = − ˜G(s, a ) + Cexplore
Q(a|s)
1 + N(s, a ) (2)
where U(s, a ) indicates the utility of selecting action a in state s; N(s, a ) is the number of times that action a
was explored in state s; Cexplore is an exploration constant equivalent to Cp in the UCT criterion; Q(a|s) is a
neural network modelling the posterior distribution over a ctions, which is trained by minimizing the variational
free energy and ˜G(s, a ) is the best estimation of the expected free energy (EFE) com puted from the following
equation:
G(π, τ ) = − EQ(θ|π )Q(sτ |θ,π )Q(oτ |sτ ,θ,π )
[
ln P (oτ |π)
]
+ EQ(θ|π )
[
EQ(oτ |θ,π )H(sτ |oτ , π ) − H(sτ |π)
]
+ EQ(θ|π )Q(sτ |θ,π )H(oτ |sτ , θ, π ) − EQ(sτ |π )H(oτ |sτ , π ),
1. This version of UCT comes from Silver et al (2016)
3
Champion et al.
using sampling of 3 (out of 4) neural networks 2 used by the system. Note that Q(a|s) in equation (2) specializes
P (s, a ) in equation (1), by providing the probability of selecting action a in state s. One can see that U(s, a ) in
equation (2) has been obtained from UCT in equation (1), by replacing the average reward by the negat ive EFE.
More recently, Champion et al (2021a) proposed an online met hod that frames planning as a form of (tempo-
ral) structure learning guided by the expected free energy. This method, called branching-time active inference
(BTAI), generalises active inference (Friston et al, 2016a ; Champion et al, 2021b; Da Costa et al, 2020) and re-
lates to another recently introduced framework for inferen ce and decision making, called sophisticated inference
(Friston et al, 2021). Importantly, the generative model of BTAI enables the agent to trade oﬀ risk and ambiguity,
instead of only seeking for certainty as was the case in (Cham pion et al, 2021b). In this paper, we provide an
empirical study of BTAI, enabling us to explicitly demonstr ate that BTAI provides a more scalable realization of
planning as inference than active inference.
Section 2 reviews the BTAI theory, with full details present ed in (Champion et al, 2021a). Then, Section
3 compares BTAI to standard active inference in the context o f a graph navigation task both empirically and
theoretically. We show that active inference is able to solv e small graphs but suﬀers from an exponential (space
and time) complexity class that makes the approach intracta ble for bigger graphs. In contrast, BTAI is able to
search the space of policies eﬃciently and scale to bigger gr aphs. Next, Section 4.2 presents the challenge of local
minima in the context of a maze solving task, and shows how bet ter prior preferences and deeper tree search
help to overcome this challenge. Lastly, Section 4.3 compar es two cost functions, gclassic and gpcost, in two new
mazes. In Section 5, BTAI was compared to the POMCP algorithm (Silver and Veness, 2010) on the frozen lake
environment; and the experiments suggest that BTAI and the P OMCP algorithm accumulate a similar amount
of reward. Also, we describe when BTAI receives more rewards than the POMCP agent, and when the opposite
is true. In Section 6, BTAI was compared to the approach of Fou ntas et al (2020) on the dSprites dataset, and
we discussed the pros and cons of each approach. Finally, Sec tion 7 concludes this paper, and provides ideas for
future research.
2. Branching Time Active Inference (BTAI)
In this section, we provide a short review of BTAI, and the rea der is referred to (Champion et al, 2021a) for details.
BTAI frames planning as a form of (temporal) structure learn ing guided by the expected free energy. This form of
structure learning should not be confused with representat ional or parametric structure learning that is currently
developped in the literature (Smith et al, 2020; Friston et a l, 2016b; Friston et al, 2018). The idea is to deﬁne a
generative model that can be expanded dynamically as shown i n Figure 1.
2. Fountas et al (2020) used neural networks to model the like lihood mapping P (oτ |sτ ), the transition mapping P (sτ +1|sτ , aτ ), the
posterior over states Q(sτ ), and the posterior over actions Q(aτ |sτ )
4
Branching Time Active Inference
The past and present is modelled using a partially observabl e Markov decision process (POMDP) in which
each observation ( Oτ ) only depends on the state at time τ, and this state ( Sτ ) only depends on the previous state
(Sτ− 1) and previous action ( Uτ− 1). In addition to the POMDP which models the past and present, the future
is modelled using a tree-like generative model whose branch es are dynamically expanded. Each branch of the
tree corresponds to a trajectory of states reached under a sp eciﬁc policy. The branches are expanded following a
logic similar to the Monte Carlo tree search algorithm (see b elow), and the state estimation is performed using
variational message passing (Winn and Bishop, 2005; Champi on et al, 2021b; Friston et al, 2017).
S0
PS0
PS...
S...
PSt
St
U0PU0
U...PU...
PO0 O0
PO... O...
POt Ot
PS(1)
S(1)
PS(2)
S(2)PO(1)O(1) PO(2) O(2)
PS(22)
S(22)
PS(11)
S(11)
PS(12)
S(12)PO(11)O(11)
Figure 1: This ﬁgure illustrates the expandable generative model allowing planning under active inference. The
current time point (the present) is denoted by t. All times before t are the past, and after t are the future. States
in the future are indexed by multi-index (action sequences) , with each digit indicating an action, e.g. S(11). The
future is a tree-like generative model whose branches corre spond to the policies considered by the agent. The
branches can be dynamically expanded during planning and th e nodes in light gray represent possible expansions
of the current generative model.
At the start of a trial, the model contains only the initial hi dden state S0 and the initial observation O0.
Then, the agent starts expanding the generative model using an approach inspired by Monte Carlo tree search
(Browne et al, 2012), where the selection of a node is based on expected free energy. More precisely, the node
selection is performed recursively from the root until reac hing a leaf node. At each level in the recursion the
selected node maximises the UCT criterion:
UCT J = −¯gJ
exploitation
+ Cp
√
ln n
nJ  
exploration
,
5
Champion et al.
where J is a multi-index representing a sequence of actions, SJ is the hidden state reached after performing the
actions sequence described by the multi-index J, n is the number of times the parent of SJ has been visited, nJ
is the number of times the child ( SJ ) was selected, and ¯ gJ is the average cost received after selecting SJ . In what
follows, we denote by J :: U the the multi-index obtained by adding the action U at the end of the sequence of
actions described by the multi-index J. Once a leaf node ( SJ ) is selected for expansion, all its children states
(i.e., all SJ::U ) are added to the generative model. The future observations (i.e., OJ::U ) associated to those hidden
states (i.e., all SJ::U ) are also added to the generative model. Next, the evaluatio n step estimates the cost of each
state-observation pair ( SJ::U , O J::U ). In this paper, we consider two kinds of cost. First, the sta ndard expected
free energy that trades oﬀ risk (over observations) and ambi guity:
gclassic
J =∆ DKL[Q(OJ )||V (OJ )] + EQ(SJ )[H[P (OJ |SJ )]],
where J = I :: U for an arbitrary action U, and V (OJ ) is a distribution encoding the prior preferences over obse r-
vations of the agent, which is generally parameterized by a v ector C or learnt using a Dirichlet prior (Sajid et al,
2021). Second, we also experiment with the following quanti ty:
gpcost
J =∆ DKL [ Q(SJ )|| V (SJ )] + DKL [ Q(OJ )|| V (OJ )] ,
where V (SJ ) is a distribution encoding the prior preferences of the age nt over the environment’s states. Note that
gpcost
J depends on both the risk over observations and the risk over s tates. The reader is referred to Appendix B
for a derivation of gpcost
J from the Free Energy of the Expected Future (FEEF) introduce d by Millidge et al (2021).
Lastly, the cost of the best action (i.e., the action that pro duces the smallest cost) is propagated towards the root
and used to update the aggregated cost of the ancestors of SJ .
Finally, during the planning procedure, the agent needs to p erform inference of the future hidden states
and observations. This is performed using variational mess age passing (VMP) on the set of newly expanded
nodes, i.e.
{
SI::U , O I::U | U ∈ { 1, ..., |U|}
}
, until convergence to a minimum in the free energy landscape . We
refer the interested reader to (Champion et al, 2021b) for ad ditional information about the derivation of the
update equations. Also, since this paper only considers inf erence and not learning (i.e. the model does not have
Dirichlet priors over the tensors deﬁning the world’s conti ngencies), the generative model is diﬀerent from the one
presented in the theoretical paper (Champion et al, 2021a). Therefore, we provide a mathematical description
6
Branching Time Active Inference
of the generative model, the variational distribution and t he belief updates in Appendix A. We summarise our
method using the pseudo-code in Algorithm 1.
Algorithm 1: Branching Time Active Inference
while end of trial not reacheddo
sample an observation ( Ot) from the environment;
perform inference using VMP and the newly acquired observat ion ( Ot);
while maximum planning iteration not reacheddo
select a node to be expanded using the UCT criterion;
perform the expansion of the generative model from the selec ted node;
perform inference on the newly expanded nodes using VMP;
evaluate the cost of the newly expanded nodes using gclassic
J or gpcost
J ;
back-propagate the cost of the nodes through the tree;
end
select an action to be performed;
execute the action in the environment;
end
3. BTAI vs active inference
In this section, we benchmark BTAI against standard active i nference as implemented in Statistical Parametric
Mapping (SPM), c.f. Friston (2007) for additional details a bout SPM. First, we do this in terms of complexity
class and then empirically through experiments of increasi ng diﬃculty.
3.1 BTAI vs active inference: Space and Time complexity
In this section, we compare our model to the standard model of active inference (Friston et al, 2016a; Da Costa et al,
2020). In the standard formulation, the implementation nee ds to store the parameter of the posterior over states
sπ
τ for each policy and each time step. Therefore, assuming |U| possible actions, T time steps, |π| = |U|T policies,
and |S| possible hidden state values, the space complexity class fo r storing the parameters of the posterior over
hidden states is O(|π| × T × |S|). This corresponds to the number of parameters that needs to be stored, and it is
a problem because |π| grows exponentially with the number of time steps. Addition ally, performing inference on
an exponential number of parameters will lead to an exponent ial time complexity class.
BTAI solves this problem by allowing only K expansions of the tree. In BTAI, we need to store |S| parameters
for each time step in the past and present, and for each expans ion, we only need to compute and store the
parameters of the posterior over the hidden states correspo nding to this expansion. Therefore, the time and space
7
Champion et al.
complexity class is O([K + t] × |S|), where t is the current time point. This is linear in the number of expa nsions.
Now, the question is how many expansions are required to solv e the task? Even if the task requires the tree to
be fully expanded, then the complexity class of BTAI would be O
(
[|U|T − t + t] × | S|
)
. Figure 2 illustrates the
diﬀerence between AcI and BTAI in terms of the space complexit y class, when BTAI performs a full expansion of
the tree.
|U|T − t |U|T
AI extra costs
BTAI nodes
τ = 0
τ = 1 =t
τ = 2
τ = 3 =T
Figure 2: This ﬁgure illustrates the diﬀerence between AcI an d BTAI in terms of space complexity class. The
time goes from top to bottom, we assume two actions at each tim e step, t denotes the current time point, and
each circle represents the storage of the |S| parameters required to store a categorical distribution of a hidden
state. Black nodes represent the nodes that must be stored in BTAI (under a full expansion of the tree), while
the red nodes represent AcI’s extra costs of storage. This ex tra cost comes from the fact that in AcI, one needs
to store posterior beliefs for each time step and for each pol icy, while in BTAI, the tree allows us to compress the
representation.
Additionally to the gain aﬀorded by the structure of the tree, most practical applications can be solved by
expanding only a small number of nodes (Silver et al, 2016; Sc hrittwieser et al, 2019), which means that MCTS
and BTAI approaches will be even more optimised than in Figur e 2 because most branches will not be expanded.
One could argue that there is a trade oﬀ in the nature and exten t of the information inferred by classic active
inference and branching-time active inference. Speciﬁcal ly, classic active inference exhaustively represents and
updates all possible policies, while branching-time activ e inference will typically only represent a small subset of
the possible trajectories. These will typically be the more advantageous paths for the agent to pursue, with the
less beneﬁcial paths not represented at all. Indeed, the tre e search is based on the expected free energy that favors
policies that maximize information gain while realizing th e prior preferences of the agent.
Additionally, the inference process can update the system’ s understanding of past contingencies on the basis of
new observations. As a result, the system can obtain more reﬁ ned information about previous decisions, perhaps
re-evaluating the optimality of these past decisions. Beca use classic active inference represents a larger space of
policies, this re-evaluation could apply to more policies.
We also know that humans engage in counterfactual reasoning (Rafetseder et al, 2013), which, in our planning
context, could involve the entertainment and evaluation of alternative (non-selected) sequences of decisions. It
may be that, because of the more exhaustive representation o f possible trajectories, classic active inference can
more eﬃciently engage in counterfactual reasoning. In cont rast, branching-time active inference would require
these alternative pasts to be generated “a fresh” for each co unterfactual deliberation. In this sense, one might
8
Branching Time Active Inference
argue that there is a trade oﬀ: branching-time active inferen ce provides considerably more eﬃcient planning to
attain current goals, classic active inference provides a m ore exhaustive assessment of paths not taken.
3.2 The deep reward environment
In this section, we introduce a canonical example of the kind of environment in which BTAI outperforms standard
active inference. This environment is called the deep rewar d environment because the agent needs to navigate a
tree like graph, where the graph’s nodes correspond to the st ates of the system, and the agent needs to look deep
into the future to diferentiate the favourable path from the traps.
At the beginning of each trial, the agent is placed at the root of the tree that corresponds to the initial state
(S0) of the system. From the initial state, the agent can select m actions leading immendiately to an undesirable
state, and n actions leading to seemingly pleasant states, for a total of n + m actions. If one of the m undesirable
actions is selected, then the agent will enter a bad path, in w hich (at each time step) n + m actions are available,
but all of them produce unpleasant observations. While thes e m undesirable actions that lead directly to terrible
states should be straightforward to avoid for any reasonabl e agent, the n seemingly favourable actions present an
additional challenge. Indeed, only one of those n actions will be beneﬁcial to the agent in the long run, and all
the others are long-term traps.
We let Lk with k ∈ { 1, ..., n } be the length of the k-th seemingly good path. Once the agent is engaged on
the k-th path, there are still n + m actions available, but only one of them keeps the agent on the right track.
All the other actions will produce unpleasant observations , i.e., the agent will enter a bad path. This process will
continue until the agent reaches the end of the k-th path, which is determined by the path’s length Lk. If the
k-th path was the longest of the n seemingly good paths, then the agent will from now on only rec eive pleasant
observations independently of the action performed. If the k-th path was not the longest path, then independently
of the action performed, the agent will receive painful obse rvations, i.e., the trap is revealed.
To summarize, at the beginning of each trial, the agent is pro mpted with n seemingly good paths and m
obviously bad paths. Only the longest of the seemingly pleas ant paths will be beneﬁcial in the long term, the
other are traps, which will ultimately lead the agent to an un desirable state. Figure 3 illustrates this environment.
Also in theory, this task does not have any terminal states, a nd the agent will keep taking actions forever. However,
in practice, each trial is stopped after a ﬁxed number of acti on-perception cycles.
9
Champion et al.
S0
Sb ... Sb
m bad paths
S1
1 S...
1 Sn
1
S1
2Sb...Sb
m + n − 1 bad paths
.
.
....Sg Sg
m + n good paths
...Sb Sb
m + n bad paths
Figure 3: This ﬁgure illustrates a type of environment in whi ch BTAI will outperform standard active inference.
Typically, this corresponds to environments in which there are only a small number of good actions. In such
environments, BTAI can safely discard a large part of the tre e, and speed up the search without impacting
performance. Note, S0 represents the initial state, Sb represents a bad state, Sg represents a good state, and Si
j
is the j-th state of the i-th seemingly good path. The above picture assumes that the l ongest path (which is
beneﬁcial in the long-term) is the path starting with the sta te S1
1 . Its length ( L1) is equal to two because after
performing two actions (i.e., the one leading to S1
1 and the one leading to S1
2 ), the agent is certain to receive
pleasant observations. Importantly, any other (seemingly ) good path starting with a state Si
1 with i ∈ { 2, ..., n }
will turn out to be a trap. A trap is simply a state from which al l actions lead to an undesirable state ( Sb), e.g.,
Sn
1 is a trap. Note, at each time point, the agent must pick from th e m + n possible actions, e.g, when reaching
S1
1 there is only one action keeping the agent on the right track, but all the other actions (i.e., m + n − 1 actions)
lead to a bad state.
3.2.1 The easy, medium and hard deep reward environment
In this section, we present three instances of the deep rewar d environment in increasing order of complexity (i.e.,
easy, medium, and hard). These instances will then be used to compare BTAI and (standard) active inference.
To specify an instance completely, it is suﬃcient to provide the number of obviously detrimental actions ( m), the
number of seemingly good actions ( n), and the length of the paths that follow from the seemingly g ood actions,
i.e., Lk for k ∈ { 1, ..., n }.
All three instances have ﬁve obviously detrimental actions (m = 5) and two seemingly good actions ( n = 2).
However, the lengths of the two good paths (i.e., L1 and L2) change from one instance to the other, and the reader
is referred to Table 1 for a summary. In all the environments c onsidered, L2 > L 1, therefore the ﬁrst path is a
trap that will lead to an undesirable state, and the second pa th is the one that should be taken. Also, to identify
that the ﬁrst path is a trap, the agent must be able to plan at le ast L1 + 1 steps ahead, since before that the
two seemingly good paths are identical. Importantly, an age nt trying to evaluate all possible policies L1 + 1 steps
into the future, will have to store and process: 343 policies for the easy instance, 16,807 policies for the medium
instance, and 5,764,801 policies for the hard instance. We c onclude this section with Figure 4 that illustrates the
easy instance of the deep reward environment.
10
Branching Time Active Inference
Environment L1 L2
easy 2 3
medium 4 5
hard 7 9
Table 1: This table presents the three deep reward environme nts on which experiments will be run.
S0
S2
1S1
1
S1
2 S2
2
S2
3
...Sg Sg
m + n good paths
...Sb Sb
m + n bad paths
Figure 4: This ﬁgure illustrates the easy instance of the dee p reward environment used to compare BTAI and AcI.
It contains two seemingly good paths ( n = 2): the ﬁrst of length two ( L1 = 2) and the second of length three
(L2 = 3). Upon reaching the end of the ﬁrst (and shortest) path, th e agent can only reach undesirable states,
i.e., the ﬁrst path is a trap. In contrast, when reaching the e nd of the second (and longest) path, the agent can
only reach pleasant states, i.e., the second path is beneﬁci al in the long term. Importantly, the entire graph of the
easy version contains more than 300 nodes, and is only partia lly represented. The exhaustive graph is obtained
by adding undesirable states ( Sb) until each node has n + m children, e.g., S0 has m = 5 unrepresented children
and S1
1 has six of them. Finally, the medium and hard versions of the d eep reward environment can be obtained
from the easy version by lengthening the two seemingly good p aths.
3.3 BTAI vs active inference: Simulations
In this section, we compare BTAI and active inference on the t hree instances of the deep reward environment
presented in Section 3.2.1. The Matlab code running an activ e inference agent was implemented by mod-
ifying the SPM demo called: DEMO_MDP_maze.m, and is publicly available on GitHub at the following URL:
https://github.com/ChampiB/Experiments_AI_TS, in the ﬁle: matlab/graph_navigation.m.
Table 2 shows the result of our simulation in which a standard active inference agent is run on the three deep
reward environments presented in Section 3.2.1. Since the b ehaviour of the simulation is deterministic, only one
run was executed. If the agent successfully selects the long est path, we report P (goal) = 1, otherwise, we report
P (trap) = 1. Lastly, the simulation was run on a standard laptop with 16GB of RAM, if the agent ran out of
memory, then we simply report a “crash” in the table. As expec ted, the agent successfully solved the easy and
medium environments, for which it was required to plan three and ﬁve steps ahead. However, for the hardest
version, the agent was supposed to store and process more tha n ﬁve millions policies and the associated beliefs
11
Champion et al.
over both: policies and hidden states. This is intractable u sing only 16GB of RAM and standard active inference
runs out of memory because of the exponential (space) comple xity class.
Environment Policy size P(goal) P(trap) Time (sec)
easy 3 1 0 14.79
medium 5 1 0 1177.05
hard 8 crash crash crash
Table 2: This table shows that the active inference agent was able to plan three and ﬁve time steps ahead to solve
the easy and medium deep reward environments. However, beca use of the exponential space complexity, SPM
runs out of memory when trying to plan eight time steps ahead t o solve the hardest deep reward environment.
The last column reports the time (in seconds) required for ru nning one simulation of the graph environment using
SPM.
The C++ code emulating BTAI can be found in the ﬁle experiments/main.cpp of the GitHub repository
previously discussed ( ChampiB/Experiments_AI_TS). The hyper-parameters used in the code are described in
Appendix D. Since action selection in BTAI is stochastic, we ran 100 simulations. We report the probability of
the agent selecting the longest path as: P (goal) = number of successes
100 . Simulations where the agent failed to select
the proper path are reported as: P (trap) = number of failures
100 . We experimented with various numbers of planning
iterations, starting with ten iterations and increasing th is number by ﬁve until the agent was able to solve the
task.
Table 3 shows the results obtained by BTAI on the three deep re ward environments presented in Table 1,
and the hyper-parameter values used in these simulations ar e reported in Appendix D. As expected, the agent
successfully solved the three deep reward environments. Te n planning iterations were required for the easy and
medium environments, and twenty for the hardest one. The abi lity of BTAI to ﬁnd the best policy among more
than ﬁve millions policies with only twenty planning iterat ions is explained by the sparcity of the deep reward
environment, i.e., the vast majority of the policies are cle arly detrimental to the agent. Note that this sparcity is
characteristic of many complex tasks such as chess. For exam ple, a chess player is frequently faced with (chess)
positions where twenty to forty legal moves are available, b ut one move is almost forced, i.e., if not played, the
player will almost surely lose the game.
Environment Planning iterations P(goal) P(trap) Time (sec)
easy 10 1 0 0.112 ± 0.008
medium 10 1 0 0.193 ± 0.007
hard 10 0.5 0.5 0.356 ± 0.020
15 0.49 0.51 0.536 ± 0.052
20 1 0 0.836 ± 0.075
Table 3: This table shows that BTAI was able to solve the three deep reward environments with at most 20
planning iterations. The reported time corresponds to the a verage runtime of one simulation, and the standard
deviation is reported after the symbol ±.
12
Branching Time Active Inference
4. BTAI Empirical Intuition
In this section, we study the BTAI agent’s behaviour through experiments highlighting its vulnerability to local
minimum and ways to mitigate this issue. The goal is to gain so me intuition about how the model behaves
when: enabling deeper searches, providing better preferen ces, and using diﬀerent kind of cost functions to guide
the Monte Carlo tree search. The code of those experiments is available on GitHub at the following URL:
https://github.com/ChampiB/Experiments_AI_TS, in the ﬁle: experiments/main.cpp.
4.1 The maze environment
This section presents the environment in which various simu lations will be run. In this environment, the agent can
be understood as a rat navigating a maze. Figure 5 illustrate s the three mazes studied in the following sections.
The agent can perform ﬁve actions, i.e., UP, DOWN, LEFT, RIGH T and IDLE. The goal is to reach the maze exit
from the starting position of the agent. To do so, the agent mu st move from empty cells to empty cells avoiding
walls. If the agent tries to move through a wall, the action be comes equivalent to IDLE. Finally, the observations
made by the agent correspond to the Manhattan distance (with the ability to traverse walls) between its current
position and the maze exit, i.e.,
M(x, y ) =
N∑
i=1
|xi − yi|,
where M(x, y ) is the Manhattan distance between x ∈ RN and y ∈ RN , x is the position of the agent, y the
position of the exit, and in a 2d maze N = 2. Figure 5 (left) illustrates the Manhattan distance rece ived on each
cell of a simple maze. Taking maze (A) from Figure 5 as an examp le, if the agent stands on the exit (green square),
the observation will be zero or equivalently using one-hot e ncoding3 [1 0 0 0 0 0 0 0 0 0], and if the agent stands
at the initial position (red square), the observation will b e nine or equivalently [0 0 0 0 0 0 0 0 0 1].
012345
6
7
8
6 5 4
5
1
2
3
456789
(A)
(B)
(C)
Figure 5: This ﬁgure illustrates the three mazes used to perf orm the experiments in the next sections. Black
squares correspond to walls, green squares correspond to th e maze exit and red squares correspond to the agent
starting position. Finally, the numbers displayed on each c ell of maze (A) correspond to the Manhattan distance
between this cell and the exit.
3. A one-hot encoding of a number n ∈ {0, ..., N} means representing n as a vector of size N + 1, where the n-th element is equal to
one and all the other are set to zeros. In this paper, we assume a zero based indexing, i.e., the ﬁrst element is at index zero .
13
Champion et al.
4.2 Overcoming the challenge of local minima
In this section, we investigate the challenge of local minim a and provide two ways of mitigating the issue: improving
the prior preferences and using a deeper tree.
4.2.1 Prior preferences and local minimum
In this ﬁrst experiment, the agent was asked to solve maze (B) from Figure 5, which has the property that
the start location (red square) is a local minimum. Remember from Section 4.1 that the agent observes the
Manhattan distance between its location and the maze exit. T he Manhattan distance naturally creates local
minima throughout the mazes, i.e., cells of the maze (apart f rom the exit) for which no adjacent cell has a lower
distance to the exit. An example of such a local minimum is sho wn as a blue square in Figure 6. The presence
of such a local minimum implies that a well behaved agent (i.e ., an agent trying to get as close as possible to the
exit) might get trapped in those cells for which no adjacent c ell has a lower distance to the exit and thus fail to
solve the task.
Figure 6: This ﬁgure illustrates the notion of local minimum (i.e., the blue cell) in the context of maze (A). Local
minima correspond to cells (apart from the exit) for which no adjacent cell has a lower distance to the exit.
Next, we need to deﬁne the prior preferences of the agent. Our framework allows the modeller to deﬁne prior
preferences over both future observations and future state s. However, we start by assuming no preferences over
the hidden states, i.e., V (SI ) is uniform. We deﬁne the prior preferences over future obse rvations as:
CO = σ
(
γv
)
with v =
[
|O| ... 2 1
] T
where |O| is the number of possible observations (10 in maze (A) from Fi gure 5), γ is the precision of the prior
preferences, and σ(·) is the softmax function. The above prior preferences will g ive high probability to cells close
to the exit and will exhibit the local minimum behaviours pre viously mentioned.
Using these prior preferences, we ran 100 simulations in maz e (B) from Figure 5. Each simulation was composed
of a maximum of 20 action-perception cycles, and was interru pted when the agent reached the maze exit. Note, the
results might vary from simulation to simulation, because t he actions performed in the environment are sampled
from σ(−ω g
N ), where σ(•) is a softmax function, ω is the precision of action selection, g is a vector whose elements
14
Branching Time Active Inference
correspond to the cost of the root’s children (i.e. the child ren of St) and N is a vector whose elements correspond
to the number of visits of the root’s children.
Table 4 reports the frequency at which the agent reaches the e xit. The hyper-parameters values are reported in
Appendix D. First, note that with 10 and 15 planning iteratio ns, the agent was unable to leave the initial position
(i.e., it is trapped in the local minimum). But as the number o f planning iterations is increased, the agent becomes
able to foresee the beneﬁts of leaving the local minimum.
Planning iterations P(exit) P(local) Time (sec)
10 0 1 0.701 ± 0.022
15 0 1 1.030 ± 0.070
20 1 0 0.233 ± 0.018
Table 4: This table presents the probability that the agent s olves maze (B), and the probability of the agent
being stuck into the local minimum. The reported time corres ponds to the average runtime of one simulation,
and the standard deviation is reported after the symbol ±. Importantly, when the agent reaches the exit of
the maze the simulation is interrupted, i.e., the simulatio n contains less than 20 action-perception cycles. This
explains why performing 20 planning iterations is faster (0 .233 seconds), than performing 15 planning iterations
(1.030 seconds), i.e., the simulations with 15 planning ite rations (that fail to solve the maze) contain 20 action-
perception cycles while the simulations with 20 planning it erations (that successfully solve the maze) contain less
than 20 action-perception cycles.
4.2.2 Improving prior preference to avoid local minimum
In this second experiment, we modiﬁed the prior preferences of the agent to enable it to avoid local minima. We
ﬁrst change the cost function from the expected free energy gclassic
I to the pure cost gpcost
I , which allows us to
set nontrivial preferences over states (in the previous sec tion, these were set to uniform). Speciﬁcally, the prior
preferences over hidden states will be of the form:
CS = σ
(
γw
)
,
where γ is the precision over prior preferences, and w is set according to Figure 7. Finally, the prior preferences
over future observations remain the same as in the previous s ection, and once again the hyper-parameters values
are reported in Appendix D.
15
Champion et al.
Walls
High prior (0.45)
Medium prior (0.15)
Low prior (0.04 <<)
Figure 7: This ﬁgure illustrates the new prior preferences o f the agent over the future states. Black squares
correspond to walls, the darkest red corresponds to high pri or preferences (really enjoyable states), the brightest
red corresponds to low prior preferences (annoying states) and the last kind of red corresponds to medium prior
preferences (boring states).
Tables 5 and 6 summarize the results of the experiments with a nd without the use of prior preferences over
hidden states, respectively. As expected better prior pref erences lead to better performance when less planning
iterations are performed. Specifying prior preferences ov er hidden states requires the modeller to bring additional
knowledge to the agent, and might not always be possible. How ever, when such knowledge is available it can
improve the agent’s performance. This illustrates the valu e of the BTAI approach, which enables preferences to
be speciﬁed for observations, as does active inference, as w ell as for states.
Planning iterations P(global) P(local) Time (sec)
10 0 1 0.683 ± 0.024
15 0 1 0.983 ± 0.030
20 1 0 0.217 ± 0.002
Table 5: This table presents the probability that the agent s olves maze (B), and the probability of the agent being
stuck in the local minimum. In this table, the agent was not eq uipped with prior preferences over hidden states.
The last column reports the (average) execution time requir ed for running one simulation and the associated
standard deviation.
Planning iterations P(global) P(local) Time (sec)
10 0 1 0.749 ± 0.045
15 1 0 0.181 ± 0.018
20 1 0 0.288 ± 0.092
Table 6: This table presents the probability that the agent s olves maze (B), and the probability of the agent being
stuck in the local minimum. In this table, the agent was equip ped with prior preferences over hidden states. The
last column reports the (average) execution time required f or running one simulation and the associated standard
deviation.
4.3 Solving more mazes
Up to now, we focused on maze (B) from Figure 5 to demonstrate t hat both improving prior preferences and
deepening the tree can help to mitigate the problem of local m inima. In this section, we extend our analysis to
mazes (A) and (C). Table 7 shows the performance of the BTAI ag ent in maze (A) when using gclassic
I and gpcost
I
as cost function. When gpcost
I was used as a cost function, the agent was only equipped with p rior preferences over
16
Branching Time Active Inference
observations (i.e., uniform preferences over hidden state s). Table 8 shows the results of the same experiments but
on maze (C). As usual the hyper-parameters values used for th ose simulations are given in Appendix D.
Tables 7 and 8 seem to indicate that both gclassic
I and gpcost
I perform similiarly on the maze environment, and
require approximatly the same amount of time to be computed. The similiar performance of gclassic
I and gpcost
I
may be surprising to the reader. Indeed, gclassic
I contains an ambiguity terms, i.e., EQ(SJ )[H[P (OJ |SJ )]], which
should be helping the agent. In contrast, gpcost
I contains the risk over states with uniform prior preference s over
states, i.e., DKL [ Q(SJ )|| V (SJ )], which should not be helpful (because of the uniformity of the prior preferences).
However, in the maze envionment the ambiguity of the likelih ood mapping P (Oτ |Sτ ) is identical for each
possible hidden state Sτ . Indeed, each state corresponds to a cell, and each cell is at a ﬁx Manhattan distance
from the exit. Thus, each state generates with high probabil ity the observation corresponding to the Manhattan
distance between the state’s cell and the exit; and generate s with small probability any other observations. For
example, the likelihood mapping of an imaginary maze could b e deﬁned as follow:
P (Oτ |Sτ ) = A =






0. 05 0 . 05 0 . 9
0. 05 0 . 9 0 . 05
0. 9 0 . 05 0 . 05






,
where P (Oτ = i|Sτ = j) = Aij. Importantly, each column of A has the same entropy, therefore the agent does
not care about which observation is made, i.e., they are all a s ambiguous. This is why the ambiguity term is in
fact not helpful in the maze environment, and why gclassic
I and gpcost
I produce similar performances.
Planning iterations P(global) P(local) Time (sec) for gclassic
I Time (sec) for gpcost
I
10 1 0 0.310 ± 0.032 0.287 ± 0.022
15 1 0 0.423 ± 0.008 0.432 ± 0.011
20 1 0 0.567 ± 0.026 0.579 ± 0.023
Table 7: This table presents the probability that the agent s olves maze (A) from Figure 5, and the probability of
the agent falling into the local minimum. Both cost function s gclassic
I and gpcost
I lead to the above results in maze
(A). The last two columns report the (average) execution tim e and the associated standard deviation of running
one simulation with gclassic
I and gpcost
I , respectively.
Planning iterations P(global) P(local) Time (sec) for gclassic
I Time (sec) for gpcost
I
10 1 0 0.498 ± 0.053 0.460 ± 0.019
15 1 0 0.696 ± 0.063 0,664 ± 0.075
20 1 0 0.920 ± 0.091 0.833 ± 0.038
Table 8: This table presents the probability that the agent s olves maze (C), and the probability of the agent falling
into the local minimum. Both cost functions gclassic
I and gpcost
I lead to the above results in maze (C). The last
two columns report the (average) execution time and the asso ciated standard deviation of running one simulation
with gclassic
I and gpcost
I , respectively.
17
Champion et al.
5. The frozen lake environment
In this section, we evaluate our agent on the frozen lake envi ronmnent introduced by OpenAI (Brockman et al,
2016). The frozen lake environment can be represented as a 2D grid with r rows and c columns. Each cell in
the grid is either a frozen surface that can support the agent ’s weight or a hole on which the agent cannot step
without receiving a heavy penalty. One of the cells with a fro zen surface contains a frisbee that the agent needs
to recover, i.e., this cell is the goal state. For our purpose , each cell is associated with a number describing its
location, and the agent observes only its location in the lak e. The agent can perform four actions (i.e., UP, DOWN,
LEFT, RIGHT) at any point in time. Actions that would lead the agent to leave the lake (through the external
boundary), are equivalent to doing nothing and the agent doe s not move.
(a)
(b)
Cumulative reward
Time steps
(c)
Cumulative reward
Time steps
(d)
Figure 8: (a) and (b) illustrate the lakes used to perform the experiments of the present section. The black
squares correspond to the external boundary of the lake, the green square corresponds to the frisbee location,
the red squares correspond to the agent starting position, t he orange squares correspond to local minima of the
lake (not all local minima are represented), and the dark blu e squares correspond to the holes in which the
agent can fall if not careful. Note, these environments cont ain over 100 states, i.e., one for each cell within the
external boundary. Finally, in (b) the green path correspon ds to the path taken by the BTAI agent, the red
path corresponds to the path selected by the POMCP agent (see the results in the main text), and the blue
path corresponds to the shortest path connecting the starti ng position to the frisbee location. By the “shortest
path”, we mean the path that is passing through the smallest n umber of frozen surfaces without passing through
a hole. (c) shows the cumulative reward (CR) received by the a gent when following the green, red or blue path.
The x-axis corresponds to the number of time steps, i.e., num ber of action-perception cycles, for which the agent
follows the green, red or blue path. We see that all three path s have almost identical values. (d) shows the CR
obtained along the green, red and blue paths minus the minimu m cumulative reward (MCR) at each time step,
where: MCR = min(CR green, CRred, CRblue).
18
Branching Time Active Inference
In terms of the reward funtion, the agent receives a penalty o f minus one each time it steps on a hole. Otherwise,
the agent receives a reward between zero and one. This reward increases linearly as the agent gets closer to the
frisbee location, where the distance between the agent and t he frisbee is measured using the Manhattan distance
as for the maze environment. Note, the reward received by the agent is maximum when the agent stands at the
frisbee location, for which it receives a reward of one. Also in theory, this task does not have any terminal states,
and the agent will keep taking actions forever. However, in p ractice, each trial is stopped after a ﬁxed number of
action-perception cycles. Figures 8(a) and 8(b) present th e lakes in which the upcoming simulations have been
ran. For reproducibility, we provide the values of the hyper -parameters used throughout this section in Appendix
D.
5.1 BTAI on the frozen lake environment
Table 9 shows the results obtained by the BTAI agent on the lak e of Figure 8(a). In short, the BTAI agent required
twenty planning iterations before it was able to solve this t ask. Each simulation takes an average of 7.870 seconds
of computational time, which correspond to approximatly 7 . 870/ 30 ≈ 0. 262 seconds of thinking (i.e., inference,
planning and action selection) per action-perception cycl e.
Planning iterations P(global) P(local) Time (sec)
10 0 1 6.991 ± 0.459
15 0 1 7.820 ± 0.577
20 1 0 7.870 ± 0.707
Table 9: This table presents the probability that the BTAI ag ent solves the lake of Figure 8(a), and the probability
of the agent falling into a local minimum of the EFE. Where by “ falling into a local minimum”, we mean that
the agent gets stuck into cells of the lake (apart from the exi t) for which no adjacent cell represents a frozen
surface that has a lower distance to the exit. The last column reports the execution time required for running one
simulation and the associated standard deviation.
Table 10 shows the results obtained by the BTAI agent on the la ke of Figure 8(b). In short, the BTAI agent
requires ﬁfty planning iterations to be able to solve this ta sk. Each simulation takes an average of 19.187 seconds
of computational time, which correspond to approximatly 19 . 187/ 30 ≈ 0. 639 seconds of thinking (i.e., inference,
planning and action selection) per action-perception cycl e.
Planning iterations P(global) P(local) Time (sec)
30 0 1 12.810 ± 1.071
40 0 1 15.589 ± 0.766
50 1 0 19.187 ± 1.317
Table 10: This table presents the probability that the BTAI a gent solves the lake of Figure 8(b), and the probability
of the agent falling into a local minimum of the EFE. Where by “ falling into a local minimum”, we mean that the
agent gets stuck into cells of the lake (apart from the exit) f or which no adjacent cell represents a frozen surface
that has a lower distance to the exit. The last column reports the (average) execution time required for running
one simulation, as well as the associated standard deviatio n.
19
Champion et al.
5.2 POMCP on the frozen lake environment
In this section, we compare BTAI to the partially observable Monte Carlo planning (POMCP) algorithm intro-
duced by Silver and Veness (2010). The code implementing the POMCP algorithm is available at the following
URL: https://github.com/ChampiB/POMCP. Brieﬂy, the POMCP agent performs MCTS (Silver et al, 2016;
Browne et al, 2012; Schrittwieser et al, 2019) to select an ac tion at each time step, and carries out inference using
a particle ﬁlter (Doucet et al, 2009). Table 11 shows the resu lts obtained by the POMCP agent on the lake of
Figure 8(a). In short, the POMCP agent was able to reach the fr isbee 97 % of the time when using one thousand
planning iterations. At which point, each simulation takes an average of 40.444 seconds of computational time,
which correspond to approximatly 40 . 444/ 30 ≈ 1. 348 seconds of thinking (i.e., inference, planning and acti on
selection) per action-perception cycle. This seems to indi cate that BTAI is able to solve this ﬁrst lake four times
faster than the POMCP algorithm.
Planning iterations P(global) P(local) Time (sec)
100 0.52 0.48 3.852 ± 0.227
500 0.89 0.11 20.550 ± 3.054
1000 0.97 0.03 40.444 ± 3.232
2000 0.93 0.07 83.156 ± 8.844
Table 11: This table presents the probability that the POMCP agent solves the lake of Figure 8(a), and the
probability of the agent falling into a local maximum of the r eward function. The last column of the above table
reports the execution time required for running one simulat ion and the associated standard deviation. Importantly,
this table can be compared with Table 9 that presents the perf ormance of the BTAI agent on the same lake.
On the lake of Figure 8(b), the POMCP agent picks the red path, while the BTAI agent chooses the green
path. As shown by Figure 8(c), even if BTAI reaches the goal st ate while POMCP does not, the cumulative reward
obtained by both agents is almost identical. This means that both agents collect a similar amount of reward.
Interestingly, the approach receiving the largest amount o f reward depends on the number of time steps in each
simulation, i.e., the length of each episode. Figure 8(d) il lustrates when BTAI is receiving more rewards than the
POMCP algorithm, and when the opposite is true. To sum up, if a simulation is composed of between one and
ﬁfteen time step(s), both approaches are equivalent. If a si mulation contains between sixteen and twenty-three
action-perception cycles, BTAI will accumulate more rewar ds than the POMCP algorithm. If the simulation has
between twenty-four and thirty-two time steps, then the POM CP agent will accumulate more rewards than BTAI.
Lastly, if the simulation contains more than twenty-three a ction-perception cycles, BTAI will accumulate more
rewards than the POMCP agent. Thus, in the long run, the POMCP algorithm selects a reasonable but slightly
suboptimal path. This might be due to the small diﬀerence of cu mulated reward obtained along the optimal path
and the path taken by the POMCP algorithm. Also, this may be wo rsened both by the large number of time
20
Branching Time Active Inference
steps required before to see any diﬀerence in accumulated rew ard between those two paths, and the variance of
the MCTS algorithm (Veness et al, 2011).
Note, the blue path in Figure 8(b) is the shortest path connec ting the starting position to the goal state, but
is never optimal in terms of cumulative reward. This is becau se the blue path makes a detour through an area
of the lake with low reward, while the green path makes a longe r detour but passes through an area with higher
rewards. Finally, if the reward received by the agent upon re aching the frisbee (i.e., green square) is increased
suﬃciently, then the POMCP agent gains incentive to cross th e hole separating it from the frisbee, i.e., POMCP
will accept a large penalty for an even greater reward.
6. The dSprites environment
The dSprites environment is based on the dSprites dataset (M atthey et al, 2017) initially designed for analysing
the latent representation learned by variational auto-enc oders (Doersch, 2016). The dSprites dataset is composed
of images of squares, ellipses and hearts. Each image contai ns one shape (square, ellipse or heart) with its own
size, orientation, and ( X, Y ) position. In the dSprites environment, the agent is able to move those shapes around
by performing four actions (i.e., UP, DOWN, LEFT, RIGHT). To make planning tractable, the action selected by
the agent is executed eight times in the environment before t he beginning of the next action-perception cycle, i.e.,
the X or Y position is increased or decreased by eight between time ste p t and t + 1. The goal of the agent is to
move all squares towards the bottom-left corner of the image and all ellipses and hearts towards the bottom-right
corner of the image, c.f. Figure 9.
Since, BTAI is a tabular model whose likelihood P (Oτ |Sτ ) and transition P (Sτ+1|Sτ , U τ ) mappings are repre-
sented using matrices, the agent does not directly take imag es as inputs. Instead, the metadata of the dSprites
dataset is used to specify the state space. In particular, th e agent observes the type of shape (i.e., square, ellipse,
or heart), as well as a coarse-grained version of the shape’s true position. Importantly, the original images are
composed of 32 possible values for both the X and Y positions of the shapes. A coarse-grained representation
with a granularity of two means that the agent is only able to p erceive 16 × 16 images, and thus, the positions at
coordinate (0 , 0), (0 , 1), (1 , 0) and (1 , 1) are indistinguishable. Figure 10 illustrates the coarse grained representa-
tion with a granularity of eight and the corresponding indic es observed by the agent. Note that this modiﬁcation
of the observation space can be seen as a form of state aggrega tion (Ren and Krogh, 2002). Finally, as shown in
Figure 10, the prior preferences of the agent are speciﬁed ov er an imaginary row below the dSprites image. This
imaginary row ensures that the agent selects the action “dow n” when standing in the “appropriate corner”, i.e.,
bottom-left corner for squares and bottom-right coner for e llipses and hearts.
21
Champion et al.
Figure 9: This ﬁgure illustrates the dSprites environment, in which the agent must move all squares towards the
bottom-left corner of the image and all ellipses and hearts t owards the bottom-right corner of the image. The red
arrows show the behaviour expected from the agent.
□ ♥
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
Figure 10: This ﬁgure illustrates the observations made by t he agent when using a coarse-grained representation
with a granularity of eight on the input image. On the left, on e can see an image from the dSprites dataset and
a grid containing red squares of 8 × 8 pixels. Any positions in those 8 × 8 squares are indistinguishable from the
perspective of the agent. Also, the bottom most row is an imag inary row used to specify the prior preferences of
the agent, i.e. the green square is the goal state and the oran ge squares correspond to undesirable states. Finally,
the three tables on the right contain the indices observed by the agent for each type of shape at each possible
position.
The evaluation of the agent’s performance is based on the rew ard obtained by the agent. Brieﬂy, the agent
receives a reward of −1, if it never enters the imaginary row or if it does so at the an tipode of the appropriate
corner. As the agent enters the imaginary row closer and clos er to the appropriate corner, its reward increases
until reaching a maximum of 1. The percentage of the task solv ed (i.e., the evaluation metric) is calculated as
follows:
P (solved) = total rewards + number of runs
2. 0 × number of runs .
Intuitively, the numerator shifts the rewards so that they a re bounded between zero and two, and the denominator
renormalises the reward to give a score between zero and one. A score of zero therefore corresponds to an agent
always failing to enter the imaginary row or doing so at the an tipode of the appropriate corner. In contrast, a
score of one corresponds to an agent always entering the imag inary row through the appropriate corner.
6.1 BTAI on the dSprites environment
In this section, we evaluate BTAI on the dSprites environmen t. The hyper-parameters used in this section are
presented in Appendix D. Brieﬂy, the agent is able to solve 88 .5% of the task when using a granularity of eight,
22
Branching Time Active Inference
c.f. Table 12. To understand why BTAI was not able to solve the task with 100% accuracy, let us consider the
example of an ellipse at position (24 , 31). With a granularity of eight, the agent perceives that th e ellipse is in the
bottom-right corner of the image, i.e., in the red square jus t above the goal state in Figure 10. From the agent’s
perspective, it is thus optimal to pick the action “down” to r each the goal state. However, in reality, the agent
will not reveive the maximum reward because its true X position is 24 instead of the optimal X position of 31.
As shown in Table 13, we can improve the agent’s perfomance, b y using a granularity of four. This allows
the agent to diﬀerentiate between a larger number of ( X, Y ) positions, i.e., it reduces the size of the red square
in Figure 10. With this setting, the agent is able to solve 96. 9% of the task. However, when decreasing the
granularity, the number of states goes up, and so does the wid th and height of the A and B matrices. As an
eﬀect, more memory and computational time is required for the inference and planning process. This highlights
a trade-oﬀ between the agent’s performance and the amount of memory and time required. Indeed, a smaller
granularity leads to better performance, but requires more time and memory.
Planning iterations P(solved) Time (sec)
10 0.813 0.859 ± 0.868
25 0.846 0.862 ± 0.958
50 0.885 1.286 ± 1.261
Table 12: This table presents the percentage of the dSprites environment solved by the BTAI agent when using a
granularity of eight, c.f. Figure 10. The last column report s the average execution time required for one simulation
and the associated standard deviation.
Planning iterations P(solved) Time (sec)
10 0.859 3.957 ± 4.027
25 0.933 3.711 ± 4.625
50 0.969 5.107 ± 5.337
Table 13: This table presents the percentage of the dSprites environment solved by the BTAI agent when using a
granularity of four. In this setting, there are 9 × 8 × 3 = 216 states. The last column reports the average execution
time required for one simulation and the associated standar d deviation.
6.2 Fountas et al approach on the dSprites environment
In this section, we experiment with the approach of Fountas e t al (2020). The code used in this section is available
on Github at the following URL: https://github.com/ChampiB/deep-active-inference-mc . First, we trained
the agent for around two days on a Nvidia Tesla P100 GPU. After the training process, we ran 100 simulations on
the original dSprites environment with both CPU and GPU. Tab le 14 reports the percentage of the task solved
and the average time required for running a trial. Running th e CPU simulations took on average 17.811 seconds
per simulation. This is around three times longer than the GP U counterpart, which required an average of 5.467
seconds per simulation. Fountas’ agent was able to solve up t o 84.1 % of the task, which is less than the 96.9 %
achieved by the BTAI agent in the previous section.
23
Champion et al.
However, it is important to acknowledge the diﬀerences betwe en the present paper and Fountas et al (2020),
as well as the diﬀerences between the two environments on whic h those approaches have been evaluated. First,
our approach is not equipped with deep neural networks, and i s therefore unable to deal with images as input.
Additionally, our agent was not asked to learn the environme nt’s dynamics, instead, our agent was provided with
a model of the environment since we are focusing on planning. In contrast, the agent of Fountas et al (2020) was
able to successfully learn the environment’s dynamics dire ctly from images and then do the planning.
To conclude, our approach was able to solve 96.9 % of a simpliﬁ ed version of the dSprites environment, and
the agent of Fountas et al (2020) was able to solve 84.1 % of the original dSprites environment. Additionally, our
approach was provided with the environment’s dynamics, whi le the agent of Fountas et al (2020) had to learn it,
which took around two days on a Nvidia Tesla P100 GPU. Another , important trade-oﬀ is between interpretability
and scalability. Indeed, the tabular representation of the likelihood and transition mappings makes the BTAI agent
very intuitive and easy to understand. However, this tabula r representation is also the main bottleneck blocking
BTAI from solving image based environments. Similarly, the deep neural networks used by Fountas et al (2020)
make their approach highly scalable, but also reduce the int erpretability of the approach.
Computation type P(solved) Time (sec)
CPU 0.798 17.811 ± 19.143
GPU 0.841 5.467 ± 5.706
Table 14: This table presents the percentage of the original dSprites environment solved by the approach of
Fountas et al (2020). The last column reports the average exe cution time required for one simulation and the as-
sociated standard deviation. Importantly, this table can b e compared with Table 13 that presents the performance
of the BTAI agent on a simpliﬁed version of the dSprites envir onment.
7. Conclusion and future works
In this paper, we provided an empirical study of branching ti me active inference (BTAI), where the name takes
inspiration from branching-time theories of concurrent an d distributed systems in computer science (Glabbeek,
1990; van Glabbeek, 1993; Bowman, 2005), and planning was ca st as (temporal) structure learning. Simply
put, the generative model is dynamically expanded and each e xpansion leads to the exploration of new policy
fragments. The expansions are guided by the expected free en ergy, which provides a trade oﬀ between exploration
and exploitation. Importantly, this approach is composed o f not two, but three major distributions. The ﬁrst is
the prior distribution (or generative model) that encodes t he agent’s beliefs before performing any observations.
The second is the posterior (or variational) distribution e ncoding the updated beliefs of the agent after performing
some observations. And the third is a target distribution ov er future states and observations that encodes the
prior preferences of the agent, i.e., a generalization of th e C matrix in the standard formulation of active inference
24
Branching Time Active Inference
proposed by Friston et al (2016a). An important advantage of this generalization is that it allows the speciﬁcation
of prior preferences over both future observations and futu re states at the same time.
We compared BTAI and standard active inference theoretical ly by studying its space and time complexity class.
This study highlights that our method should perform better than the standard model used in active inference
when the task can be solved by expanding the tree only a small n umber of times with respect to an exhaustive
search. Second, we compared BTAI to active inference empiri cally within the deep reward environment. Those
simulations suggest that BTAI is able to solve problems for w hich a standard active inference agent would run out
of memory. Interestingly, active inference oﬀers an Occam’s window (Da Costa et al, 2020) for policy pruning, i.e.,
a policy is prunned if its posterior probability is very low w .r.t. the current best policy. This approach provides
a way to reduce the amount of space used by active inference, s ince the policies with low probability and their
associated beliefs over states can be discarded. However, a direct application of Occam’s window will not solve the
exponential time complexity class because the posterior pr obability of all policies still needs to be evaluated. It
seems that a new AcI-based algorithm would be required to use the potential of Occam’s window. As elaborated
upon in Section 3.1, one might argue that there is a trade-oﬀ b etween banching-time active inference, which
provides considerably more eﬃcient planning to attain curr ent goals, and classic active inference which provides
a more exhaustive assessment of paths not taken. This might e nable active inference to more exhaustively reﬂect
counter-factuals and reasoning based upon them.
Also, BTAI was studied (experimentally) in the context of a m aze solving task and we showed that when the
heuristic used to create the prior preferences is not perfec t, the agent becomes vulnerable to local minima. In
other words, the agent might be attracted by a part of the maze that has low cost but does not allow it to solve
the task. Then, we demonstrated empirically that improving the prior preferences of the agent by specifying a
good prior over future hidden states and deepening the tree s earch, helped to mitigate this issue.
Moreover, BTAI was compared to the POMCP algorithm (Silver a nd Veness, 2010) on the frozen lake environ-
ment. This comparison was based upon two lakes each having th eir own topology. In terms of performance, both
approaches successfully solved the simplest lake. On the ha rdest lake, BTAI and the POMCP algorithm received
a similar amount of reward. Also, we described when BTAI rece ives more rewards than the POMCP agent, and
when the opposite is true.
Additionally, BTAI was compared to the approach of Fountas e t al (2020) on the dSprites dataset. The
experiments show that our approach was able to solve 96.9 % of a simpliﬁed version of the dSprites environment,
and the agent of Fountas et al (2020) was able to solve 84.1 % of the original dSprites environment. However, our
approach was provided with the environment’s dynamics, whi le the agent of Fountas et al (2020) had to learn it,
which took around two days on a Nvidia Tesla P100 GPU. Another , important trade-oﬀ is between interpretability
and scalability. Indeed, the tabular representation of the likelihood and transition mappings makes the BTAI
25
Champion et al.
agent very intuitive and easy to understand. Unfortunatly, this tabular representation is also the main bottleneck
blocking BTAI from solving image based environments. Simil arly, the deep neural networks used by Fountas et al
(2020) make their approach highly scalable, but reduce the i nterpretability of this approach.
The present paper could lead to a large number of future resea rch directions. One could for example add the
ability of the agent to learn the transition matrices B as well as the likelihood matrix A and the vector of initial
states D. This can be done in at least two ways. The ﬁrst is to add Dirich let priors over those matrices/vectors
and the second would be to use neural networks as function app roximators. The second option will lead to a
deep active inference agent (Sancaktar and Lanillos, 2020; Millidge, 2020) equiped with tree search that could be
directly compared to the method of Fountas et al (2020). Incl uding deep neural networks in the framework will
also open the door to direct comparison with the deep reinfor cement learning literature (Haarnoja et al, 2018;
Mnih et al, 2013; van Hasselt et al, 2015; Lample and Chaplot, 2017; Silver et al, 2016). Those comparisons will
enable the study of the impact of the epistemic terms when the agent is composed of deep neural networks.
Another, important direction of research would be to learn t he prior preferences of the agent (Sajid et al,
2021). Those preferences are encoded by the vector C, and could be learned by incorporating a Dirichlet prior
over C. Also, the incorporation of this Dirichlet prior leads to an augmented EFE that could be compared with
the standard formulation of the EFE.
Moreover, while the present paper is based on standard activ e inference that advocates that actions maximize
both reward and information gain, it would be interresting t o design a version of BTAI based on meta-control
(Markovi´ c et al, 2021). Meta-control is a hierarchical mod el where higher-level hidden states constrain decision
making at lower levels. Interestingly, Markovi´ c et al (202 1) argue that it may be beneﬁcial for the agent to switch
on and oﬀ its exploration tendency based on the current conte xt.
Another direction of research will be to set up behavioural e xperiments to try to determine which kind of
planning is used by the brain. This could simply be done by loo king at the time required by a human to solve
various mazes and compare it with both the classic model and t he tree search alternative. Finally, one could also
set up a hierarchical model of action and compare it to the tre e search algorithm presented here. One could also
evaluate the plausibility of a hierarchical model of action by running behavioural experiments on humans.
Finally, a completely diﬀerent direction will be to focus on t he integration of memory. At the moment, when a
new action is performed in the environment and a new observat ion is recieved from it, all the branches in the tree
are prunned and a new temporal slice (i.e. a new state, action and observation triple) is added to the POMDP. In
other words, the integration function simply records the pa st. This exact recording of the past is very unlikely to
really happen in the brain. Therefore, one might simply ask w hat to do with this currently ever growing record
of the past. This would certainly lead to the notion of an acti ve inference agent equipped with episodic memory
(Botvinick et al, 2019).
26
Branching Time Active Inference
Acknowledgments
We would like to thank the reviewers for their valuable feedb ack, which greatly improved the quality of the present
paper.
Appendix A: The theoretical approach of this paper.
This appendix describes the generative model, the variatio nal distribution and the update equations used through-
out this paper. For full details of vocabulary and notation t he reader is referred to Champion et al (2021a).
The generative model can be understood as a ﬁxed part modelli ng the past and present, and an expandable
part modelling the future. The past and present is represent ed as a sequence of hidden states, where the transition
between any two consecutive states depends on the action per formed and is modelled using the 3-tensor B. The
generation of an observation is modelled by the matrix A, and the prior over the initial hidden state as well as
the prior over the various actions are modelled using vector s, i.e., D and Θ τ , respectively.
Concerning the second part of the model (i.e., the one modell ing the future), the transition between consecutive
states in the future is deﬁned using the 2-sub-tensor B(•, •, I last), which is the matrix corresponding to the last
action performed to reach the node SI . The generation of future observations from future hidden s tates is identical
to the one used for the past and present.
For the sake of simplicity, we assume that the tensors A, B, D and Θ τ are given to the agent, which means
that the agent knows the dynamics of the environment (c.f., T able 15 for additional information about those
tensors). Practically, this means that the generative mode l does not have Dirichlet priors over those tensors.
Furthermore, we follow Parr and Friston (2018), by viewing f uture observations as latent random variables. The
formal deﬁnition of the generative model, which encodes our prior knowledge of the task, is given by:
P (O0:t, S 0:t, U 0:t− 1, O I, S I) = P (S0)
t− 1∏
τ=0
P (Uτ )
t∏
τ=0
P (Oτ |Sτ )
t∏
τ=1
P (Sτ |Sτ− 1, U τ− 1)
∏
I∈ I
P (OI |SI )P (SI |SI\ last)
where I is the set of all non-empty multi-indexes already expanded, and SI\ last is the parent of SI . Additionally,
we need to deﬁne the individual factors:
P (S0) = Cat( D) P (Uτ ) = Cat( Θ τ )
P (Oτ |Sτ ) = Cat( A) P (OI |SI ) = Cat( A)
P (Sτ |Sτ− 1, U τ− 1) = Cat( B) P (SI |SI\ last) = Cat( B[Ilast]).
27
Champion et al.
where Ilast is the last index of the multi-index I, i.e., the last action that led to SI , and B[Ilast] = B(•, •, I last) is
the matrix corresponding to Ilast. We now turn to the deﬁnition of the variational posterior. U nder the mean-ﬁeld
approximation:
Q(S0:t, U 0:t− 1, O I, S I) =
t− 1∏
τ=0
Q(Uτ )
t∏
τ=0
Q(Sτ )
∏
I∈ I
Q(OI )Q(SI )
where the individual factors are deﬁned as:
Q(Sτ ) = Cat( ˆDτ ) Q(Uτ ) = Cat( ˆΘ τ )
Q(OI ) = Cat( ˆEI ) Q(SI ) = Cat( ˆDI )
Lastly, we follow Millidge et al (2021) in assuming that the a gent aims to minimise the KL divergence between
the variational posterior and a desired (target) distribut ion. Therefore, our framework allows for the speciﬁcation
of prior preferences over both future hidden states and futu re observations:
V (OI, S I) =
∏
I∈ I
V (OI )V (SI )
where the individual factors are deﬁned as:
V (OI ) = Cat( CO), V (SI ) = Cat( CS).
Importantly, CO and CS play the role of the vector C in the active inference model (Friston et al, 2016a), i.e.,
they specify which observations and hidden states are rewar ding. To sum up, this framework is deﬁned using
three distributions: the prior deﬁnes the agent’s beliefs b efore performing any observation, the posterior is an
updated version of the prior that takes into account the obse rvation made by the agent, and the target (desired)
distribution encodes the agent’s prior preferences in term s of future observations and hidden states.
28
Branching Time Active Inference
Notation Meaning
T , t The time horizon and the current time step
Oi:j, Si:j, Ui:j The set of observations, states and actions between time ste p i and j (inclusive)
A The matrix deﬁning the mapping from states to observations
B/ ˆDτ
The 3-tensor deﬁning the mappings (a priori) between any two
consecutive hidden states and the parameters of the posteri or over Sτ
D/ ˆD0 The parameters of the prior/posterior over the initial hidd en states
ˆDI / ˆEI The parameters of the posterior over future states/observa tions
CS/CO The parameters of the prior preferences over future states/ observations
Θ τ / ˆΘ τ The parameters of the prior/posterior over actions at time s tep τ
σ(•) The softmax function
Cat(•) and Dir( •) Categorical and Dirichlet distributions
Table 15: Branching time active inference notation
Finally, the update equations used in this paper rely on vari ational message passing as presented in (Champion et al,
2021b; Winn and Bishop, 2005) and are given by:
Q∗(Sτ ) = σ
(
[τ = 0] ln D + [ τ ̸= 0] ln B ⊙ [ ˆDτ− 1, ˆΘ τ− 1]
t∑
+ ln A ⊙ oτ
t∑
τ
+[τ = t]
∑
J∈ cht
ln B[Jlast] ⊙ ˆDJ + [ τ ̸= t] ln B ⊙ [ ˆDτ+1, ˆΘ τ ]
)
Q∗(Uτ ) = σ
(
ln Θ τ + ln B ⊙ [ ˆDτ , ˆDτ+1]
) t∑
τ
Q∗(OI ) = σ
(
ln A ⊙ ˆDI
) t∑
τ
Q∗(SI ) = σ
(
ln A ⊙ ˆEI + ln B[Ilast] ⊙ ˆDI\ last +
∑
SK ∈ chI
ln B[Klast] ⊙ ˆDK
) t∑
τ
where oτ is the observation made at time step τ, Ilast is the last action of the sequence I, ch t is the set of multi-
indices corresponding to the children of the root node, and c hI is the set of multi-indices corresponding to the
children of SI . For additional information about ⊙, the reader is referred to Appendix C.
29
Champion et al.
Appendix B: Derivation ofgpcost
J .
In this appendix, we provide a derivation of gpcost
J from the Free Energy of the Expected Future (FEEF) introduce d
by Millidge et al (2021):
gfeef
I = DKL [ Q(OI , S I )|| V (OI , S I )] ,
by assuming the following factorizations for the variation al posterior:
Q(OI , S I ) = Q(OI )Q(SI ),
and target distribution:
V (OI , S I ) = V (OI )V (SI ).
Starting from gfeef
I , we use the deﬁnition of the KL divergence, the linearity of t he expectation, the log property
ln(ab) = ln( a) + ln( b), and the two assumptions described above to get:
gfeef
I = DKL [ Q(OI , S I )|| V (OI , S I )]
= DKL [ Q(OI )Q(SI )|| V (OI )V (SI )] (factorization assumptions)
= EQ(OI )Q(SI )
[
ln Q(OI )Q(SI ) − ln V (OI )V (SI )
]
(KL divergence deﬁnition)
= EQ(OI )Q(SI )
[
ln Q(OI ) − ln V (OI ) + ln Q(SI ) − ln V (SI )
]
(log property)
= EQ(OI )
[
ln Q(OI ) − ln V (OI )
]
+ EQ(SI )
[
ln Q(SI ) − ln V (SI )
]
(linearity of expectation)
= DKL [ Q(OI )|| V (OI )] + DKL [ Q(SI )|| V (SI )] (KL divergence deﬁnition)
= gpcost
J .
Appendix C: Generalized inner product
Generalized inner products: Given an N dimensional tensor W and M = N − 1 vectors V i, the generalized
inner product returns a vector Z obtained by performing a weighted average (with weighting c oming from the
vectors) over all but one dimension. In other words:
Z = W ⊙
[
V 1, ..., V M
]
⇔ Z(xj ) =
∑
x1∈{ 1,..., |V 1|}
}... }
xM ∈{ 1,..., |V M |}
V 1
x1 × ... × W (x1, ...,x j , ..., x M ) × ... × V M
xM
∀xj ∈ { 1, ..., |Z|},
30
Branching Time Active Inference
where |Z| denotes the number of elements in Z, and the large summand is over all xr for r ∈ { 1, ..., M } \ { j}, i.e.,
excluding j. Also, note that if |W |V i ∀i ∈ { 1, ..., M } is the number of elements in the dimension corresponding to
V i, then for W ⊙
[
V 1, ..., V M ]
to be properly deﬁned, we must have |W |V i = |V i| ∀ i ∈ { 1, ..., M } where |V i| is
the number of elements in V i. Figure 11 illustrates the generalized inner product for N = 3.
dim of V 1
dim of V 2dim of V 3
W
V 2 V 3
Z = W ⊙
[
V 2, V3]
dim of V 1
Z(i) =∑
j,kV 2(j)V 3(k)W (i, j, k)
Z
Figure 11: This ﬁgure illustrates the generalized inner pro duct Z = W ⊙
[
V 2, V 3]
, where W is a cube of values
illustrated in red with typical element W (i, j, k ). Also, the vectors Z and V i ∀i ∈ { 2, 3} are drawn in blue along
the dimension of the cube they correspond to.
Naming of the dimensions: Importantly, we should imagine that each side of W has a name, e.g., if W is
a 3x2 matrix, then the i-th dimension of W could be named: “the dimension of Vi”. This enables us to write:
Z1 = W ⊙ V 1 and Z2 = W ⊙ V 2, where Z1 is a 1x2 matrix (i.e., a vector with two elements) and Z2 is a 3x1
matrix (i.e., a vector with three elements). The operator ⊙ knows (thanks to the dimension name) that W ⊙ V 1
takes the weighted average w.r.t “the dimension of V1”, while W ⊙ V 2 must take the weighted average over “the
dimension of V2”.
In the context of active inference, the matrix A has two dimensions that we could call “the observation
dimension” (i.e., row-wise) and “the state dimension” (i.e ., column-wise). Trivially, A ⊙ oτ will then correspond
to the average of A along the observation dimension and A ⊙ ˆDτ will correspond to the average of A along the
state dimension.
Appendix D: Hyper-parameters used during the simulations
Lists of hyper-parameters: Table 16 describes the role of the hyper-parameters of the BT AI simulation.
31
Champion et al.
Name Description
NB_SIMULATIONS The number of simulations run during the experiment.
NB_ACTION_PERCEPTION_CYCLES The maximum number of actions in each simulation, after whic h the
simulation is terminated.
NB_PLANNING_STEPS The number of planning iterations performed by the agent.
EXPLORATION_CONSTANT The exploration constant of the UCT criterion.
PRECISION_PRIOR_PREFERENCES The precision of the prior preferences, i.e., γ in CO = σ(γv), where v is
a vector quantifying the preferences of the agent.
PRECISION_ACTION_SELECTION The precision of the distribution used for action selection , i.e., ω in
σ(−ω g
N ) where g is a vector whose elements correspond to the cost of
the root’s children (i.e. the children of St) and N is a vector whose
elements correspond to the number of visits of the root’s chi ldren.
EVALUATION_TYPE The type of cost used to evaluate the node during the tree sear ch, i.e.,
Gclassic
I reported as EFE or Gpcost
I reported as DOUBLE
KL.
Table 16: This table describes the hyper-parameters of the B TAI simulation.
Table 17 describes the role of the hyper-parameters of the PO MCP simulation.
Name Description
NB_SIMULATIONS The number of simulations run during the experiment.
NB_ACTION_PERCEPTION_CYCLES The maximum number of actions in each simulation, after whic h the
simulation is terminated.
TIMEOUT The number of planning iterations performed by the agent.
EXP_CONST The exploration constant of the UCT criterion.
GAMMA The value of the discount factor.
NO_PARTICLES The number of particles in the ﬁlter.
Table 17: This table describes the hyper-parameters of the P OMCP simulation.
Hyper-parameters used by BTAI in section 3.3: Table 18 provides the value of each hyper-parameter used
by BTAI in section 3.3.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 20
NB_PLANNING_STEPS 10 or 15 or 20
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 3
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE
Table 18: This table presents the value of each hyper-parame ter used by BTAI in section 3.3.
Hyper-parameters used by BTAI in section 4.2.1: Table 19 provides the value of each hyper-parameter
used by BTAI in section 4.2.1.
32
Branching Time Active Inference
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 20
NB_PLANNING_STEPS 10 or 15 or 20
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 2
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE
Table 19: This table presents the value of each hyper-parame ter used by BTAI in section 4.2.1.
Hyper-parameters used by BTAI in section 4.2.2: Table 20 provides the value of each hyper-parameter
used by BTAI in section 4.2.2.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 20
NB_PLANNING_STEPS 10 or 15 or 20
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 2
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE DOUBLE KL
Table 20: This table presents the value of each hyper-parame ter used by BTAI in section 4.2.2.
Hyper-parameters used by BTAI in section 4.3: Table 21 provides the value of each hyper-parameter used
by BTAI in section 4.3.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 20
NB_PLANNING_STEPS 10 or 15 or 20
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 2
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE or DOUBLE KL
Table 21: This table presents the value of each hyper-parame ter used by BTAI in section 4.3.
Hyper-parameters used by BTAI in section 5: Table 22 provides the value of each hyper-parameter used
by BTAI in section 5.
33
Champion et al.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 30
NB_PLANNING_STEPS 10, 15, 20, 30, 40 or 50
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 2
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE
Table 22: This table presents the value of each hyper-parame ter used by BTAI in section 5. Note, the number of
action-perception cycles has been increased from 20 to 30, b ecause the agent cannot possibly solve the task with
20 actions (the lake is too large).
Hyper-parameters used by the POMCP algorithm in section 5: Table 23 provides the value of each
hyper-parameter used by the POMCP algorithm in section 5.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 30
TIMEOUT 100, 500, 1000 or 2000
EXP_CONST 3
GAMMA 0.9
NO_PARTICLES 100
Table 23: This table presents the value of each hyper-parame ter used by the POMCP algorithm in section 5.
Hyper-parameters used by BTAI in section 6.1: Table 24 provides the value of each hyper-parameter used
by BTAI in section 6.1. Also, note that the granularity of the coarse-grained representation was set to four or
eight.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 30
NB_PLANNING_STEPS 10, 25 or 50
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 2
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE
Table 24: This table presents the value of each hyper-parame ter used by BTAI in section 6.1.
References
Botvinick M, Toussaint M (2012) Planning as inference. Tren ds in Cognitive Sciences 16(10):485 – 488, DOI
https://doi.org/10.1016/j.tics.2012.08.006
34
Branching Time Active Inference
Botvinick M, Ritter S, Wang JX, Kurth-Nelson Z, Blundell C, H assabis D (2019) Reinforcement learning, fast
and slow. Trends in Cognitive Sciences 23(5):408 – 422, DOI h ttps://doi.org/10.1016/j.tics.2019.02.006, URL
http://www.sciencedirect.com/science/article/pii/S1364661319300610
Bowman H (2005) Concurrency Theory: Calculi an Automata for Modelling Untimed and Timed Concurrent
Systems. Springer, Dordrecht, URL https://cds.cern.ch/record/1250124
Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, Zaremba W (2016) Openai gym.
arXiv:1606.01540
Browne CB, Powley E, Whitehouse D, Lucas SM, Cowling PI, Rohl fshagen P, Tavener S, Perez D, Samothrakis S,
Colton S (2012) A survey of monte carlo tree search methods. I EEE Transactions on Computational Intelligence
and AI in Games 4(1):1–43
Butz MV, Bilkey D, Humaidan D, Knott A, Otte S (2019) Learning , planning, and control in a monolithic neural
event inference architecture. Neural Networks 117:135–14 4, DOI https://doi.org/10.1016/j.neunet.2019.05.001,
URL https://www.sciencedirect.com/science/article/pii/S0893608019301339
Catal, Ozan and Verbelen, Tim and Nauta, Johannes and De Boom , Cedric and Dhoedt, Bart (2020)
Learning perception and planning with deep active inferenc e. In: ICASSP 2020 - 2020 IEEE Inter-
national Conference on Acoustics, Speech and Signal Proces sing (ICASSP), IEEE, pp 3952–3956, URL
http://dx.doi.org/10.1109/ICASSP40776.2020.9054364
Champion T, Bowman H, Grze´ s M (2021a) Branching time active inference: the theory and its generality
Champion T, Grze´ s M, Bowman H (2021b) Realizing Active Infe rence in Varia-
tional Message Passing: The Outcome-Blind Certainty Seeke r. Neural Computation
pp 1–65, DOI 10.1162/neco a 01422, URL https://doi.org/10.1162/neco_a_01422,
https://direct.mit.edu/neco/article-pdf/doi/10.1162/neco_a_01422/1930278/neco_a_01422.pdf
Cox M, van de Laar T, de Vries B (2019) A factor graph approach t o automated design of Bayesian
signal processing algorithms. Int J Approx Reason 104:185– 204, DOI 10.1016/j.ijar.2018.11.002, URL
https://doi.org/10.1016/j.ijar.2018.11.002
Cullen M, Davey B, Friston KJ, Moran RJ (2018) Active Inferen ce in OpenAI Gym: A Paradigm
for Computational Investigations Into Psychiatric Illnes s. Biological Psychiatry: Cognitive Neu-
roscience and Neuroimaging 3(9):809 – 818, DOI https://doi .org/10.1016/j.bpsc.2018.06.010, URL
http://www.sciencedirect.com/science/article/pii/S2451902218301617, computational Methods and
Modeling in Psychiatry
35
Champion et al.
Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V, Friston K (202 0) Active inference on discrete state-spaces:
A synthesis. Journal of Mathematical Psychology 99:102,44 7, DOI https://doi.org/10.1016/j.jmp.2020.102447,
URL https://www.sciencedirect.com/science/article/pii/S0022249620300857
Doersch C (2016) Tutorial on variational autoencoders. 1606.05908
Doucet A, Johansen AM, et al (2009) A tutorial on particle ﬁlt ering and smoothing: Fifteen years later. Handbook
of nonlinear ﬁltering 12(656-704):3
FitzGerald THB, Dolan RJ, Friston K (2015) Dopamine, reward learning, and active infer-
ence. Frontiers in Computational Neuroscience 9:136, DOI 1 0.3389/fncom.2015.00136, URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136
Forney GD (2001) Codes on graphs: normal realizations. IEEE Transactions on Information Theory 47(2):520–548
Fountas Z, Sajid N, Mediano PAM, Friston K (2020) Deep active inference agents using Monte-Carlo methods.
arXiv e-prints arXiv:2006.04176, 2006.04176
Fox CW, Roberts SJ (2012) A tutorial on variational Bayesian inference. Artiﬁcial Intelligence Review 38(2):85–95,
DOI 10.1007/s10462-011-9236-8, URL https://doi.org/10.1007/s10462-011-9236-8
Friston K (2010) The free-energy principle: a uniﬁed brain t heory? Nature Reviews Neuroscience 11(2):127–138,
DOI 10.1038/nrn2787, URL https://doi.org/10.1038/nrn2787
Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Doherty JO, Pezzulo G (2016a) Active inference and learning.
Neuroscience & Biobehavioral Reviews 68:862 – 879, DOI http s://doi.org/10.1016/j.neubiorev.2016.06.022
Friston K, Parr T, Zeidman P (2018) Bayesian model reduction . arXiv e-prints arXiv:1805.07092, 1805.07092
Friston K, Da Costa L, Hafner D, Hesp C, Parr T (2021) Sophisti cated Inference. Neural Com-
putation 33(3):713–763, DOI 10.1162/neco a 01351, URL https://doi.org/10.1162/neco_a_01351,
https://direct.mit.edu/neco/article-pdf/33/3/713/1889421/neco_a_01351.pdf
Friston KJ (2007) Statistical parametric mapping: the anal ysis of functional brain images. Elsevier
Friston KJ, Litvak V, Oswal A, Razi A, Stephan KE, van Wijk BC, Ziegler G, Zeidman P (2016b) Bayesian model
reduction and empirical bayes for group (dcm) studies. Neur oImage 128:413–431, DOI https://doi.org/10.1016/j.
neuroimage.2015.11.015, URL https://www.sciencedirect.com/science/article/pii/S105381191501037X
Friston KJ, Parr T, de Vries B (2017) The graphical brain: Bel ief propagation and active inference. Network
Neuroscience 1(4):381–414, DOI 10.1162/NETN \ a\ 00018, URL https://doi.org/10.1162/NETN_a_00018,
https://doi.org/10.1162/NETN_a_00018
36
Branching Time Active Inference
van Glabbeek RJ (1993) The linear time — branching time spect rum II. In: Best E (ed) CONCUR’93, Springer
Berlin Heidelberg, Berlin, Heidelberg, pp 66–81
Glabbeek RJv (1990) The linear time-branching time spectru m (extended abstract). In: Proceedings of the The-
ories of Concurrency: Uniﬁcation and Extension, Springer- Verlag, Berlin, Heidelberg, CONCUR ’90, p 278–297
Haarnoja T, Zhou A, Abbeel P, Levine S (2018) Soft actor-crit ic: Oﬀ-policy maximum entropy deep reinforcement
learning with a stochastic actor. CoRR abs/1801.01290, URL http://arxiv.org/abs/1801.01290, 1801.01290
van Hasselt H, Guez A, Silver D (2015) Deep reinforcement lea rning with double q-learning. CoRR abs/1509.06461,
URL http://arxiv.org/abs/1509.06461, 1509.06461
Itti L, Baldi P (2009) Bayesian surprise attracts human atte ntion. Vision Re-
search 49(10):1295 – 1306, DOI https://doi.org/10.1016/j .visres.2008.09.007, URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380, visual Attention: Psy-
chophysics, electrophysiology and neuroimaging
van de Laar T, de Vries B (2019) Simulating active inference p rocesses by message passing. Front Robotics and
AI 2019, DOI 10.3389/frobt.2019.00020, URL https://doi.org/10.3389/frobt.2019.00020
Lample G, Chaplot DS (2017) Playing FPS games with deep reinf orcement learning. In: Singh
SP, Markovitch S (eds) Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelli-
gence, February 4-9, 2017, San Francisco, California, USA, AAAI Press, pp 2140–2146, URL
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14456
Maisto D, Gregoretti F, Friston KJ, Pezzulo G (2021) Active t ree search in large pomdps. CoRR abs/2103.13860,
URL https://arxiv.org/abs/2103.13860, 2103.13860
Markovi´ c D, Goschke T, Kiebel SJ (2021) Meta-control of the exploration-exploitation dilemma emerges from
probabilistic inference over a hierarchy of time scales. Co gnitive, Aﬀective, & Behavioral Neuroscience 21(3):509–
533, DOI 10.3758/s13415-020-00837-x, URL https://doi.org/10.3758/s13415-020-00837-x
Matthey L, Higgins I, Hassabis D, Lerchner A (2017) dsprites : Disentanglement testing sprites dataset.
https://github.com/deepmind/dsprites-dataset/
Millidge B (2019) Combining active inference and hierarchi cal predictive coding: A tutorial introduction and case
study. DOI 10.31234/osf.io/kf6wc, URL https://doi.org/10.31234/osf.io/kf6wc
37
Champion et al.
Millidge B (2020) Deep active inference as variational poli cy gradients. Journal of Math-
ematical Psychology 96:102,348, DOI https://doi.org/10. 1016/j.jmp.2020.102348, URL
http://www.sciencedirect.com/science/article/pii/S0022249620300298
Millidge B, Tschantz A, Buckley CL (2021) Whence the expecte d free energy? Neural Comput 33(2):447–482,
DOI 10.1162/neco \ a\ 01354, URL https://doi.org/10.1162/neco_a_01354
Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wier stra D, Riedmiller MA (2013) Playing Atari with
Deep Reinforcement Learning. CoRR abs/1312.5602, URL http://arxiv.org/abs/1312.5602, 1312.5602
Parr T, Friston KJ (2018) Generalised free energy and active inference: can the future cause the
past? bioRxiv DOI 10.1101/304782, URL https://www.biorxiv.org/content/early/2018/04/23/304782,
https://www.biorxiv.org/content/early/2018/04/23/304782.full.pdf
Pezzato C, Corbato CH, Wisse M (2020) Active inference and be havior trees for reactive action planning and
execution in robotics. CoRR abs/2011.09756, URL https://arxiv.org/abs/2011.09756, 2011.09756
Pitti A, Quoy M, Lavandier C, Boucenna S (2020) Gated spiking neural network using itera-
tive free-energy optimization and rank-order coding for st ructure learning in memory sequences
(inferno gate). Neural Networks 121:242–258, DOI https:// doi.org/10.1016/j.neunet.2019.09.023, URL
https://www.sciencedirect.com/science/article/pii/S089360801930303X
Rafetseder E, Schwitalla M, Perner J (2013) Counterfactual reasoning: From childhood to adulthood. Journal of
experimental child psychology 114(3):389–404
Ren Z, Krogh B (2002) State aggregation in markov decision pr ocesses. In: Proceedings of the 41st IEEE Conference
on Decision and Control, 2002., vol 4, pp 3819–3824 vol.4, DO I 10.1109/CDC.2002.1184960
Sajid N, Tigas P, Zakharov A, Fountas Z, Friston K (2021) Expl oration and preference satisfaction trade-oﬀ in
reward-free learning. arXiv 2106.04316
Sancaktar C, Lanillos P (2020) End-to-end pixel-based deep active inference for body perception and action. ArXiv
abs/2001.05847
Sancaktar C, van Gerven MAJ, Lanillos P (2020) End-to-end pi xel-based deep active inference for body per-
ception and action. In: Joint IEEE 10th International Confe rence on Development and Learning and Epi-
genetic Robotics, ICDL-EpiRob 2020, Valparaiso, Chile, Oc tober 26-30, 2020, IEEE, pp 1–8, DOI 10.1109/
ICDL-EpiRob48136.2020.9278105, URL https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105
38
Branching Time Active Inference
Schrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L , Schmitt S, Guez A, Lockhart E, Hassabis D,
Graepel T, Lillicrap TP, Silver D (2019) Mastering Atari, Go , Chess and Shogi by Planning with a Learned
Model. ArXiv abs/1911.08265
Schwartenbeck P, Passecker J, Hauser TU, FitzGerald THB, Kr onbichler M, Friston K
(2018) Computational mechanisms of curiosity and goal-dir ected exploration. bioRxiv
DOI 10.1101/411272, URL https://www.biorxiv.org/content/early/2018/09/07/411272,
https://www.biorxiv.org/content/early/2018/09/07/411272.full.pdf
Silver D, Veness J (2010) Monte-carlo planning in large pomd ps. Advances in neural information processing systems
23
Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driess che G, Schrittwieser J, Antonoglou I, Panneer-
shelvam V, Lanctot M, Dieleman S, Grewe D, Nham J, Kalchbrenn er N, Sutskever I, Lillicrap TP, Leach M,
Kavukcuoglu K, Graepel T, Hassabis D (2016) Mastering the ga me of go with deep neural networks and tree
search. Nature 529(7587):484–489, DOI 10.1038/nature169 61, URL https://doi.org/10.1038/nature16961
Smith R, Schwartenbeck P, Parr T, Friston KJ (2020) An active inference approach to modeling structure learning:
Concept learning as an example case. Frontiers in Computati onal Neuroscience 14:41, DOI 10.3389/fncom.2020.
00041, URL https://www.frontiersin.org/article/10.3389/fncom.2020.00041
Veness J, Lanctot M, Bowling M (2011) Variance reduction in m onte-carlo tree search. Advances in Neural Infor-
mation Processing Systems 24
Winn J, Bishop C (2005) Variational message passing. Journa l of Machine Learning Research 6:661–694
Wirkuttis N, Tani J (2021) Leading or following? dyadic robo t imitative interaction using the active inference
framework. IEEE Robotics and Automation Letters 6(3):6024 –6031, DOI 10.1109/LRA.2021.3090015
39