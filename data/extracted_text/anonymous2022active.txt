Every thing flows, nothing stands still.
— Heraclitus, 501 BC
8.1 Introduction
This chapter complements chapter 7 by continuing our discussion of how to 
build a generative model. Our focus  here is on continuous state- space mod-
els, which are well suited for modeling the physical fluctuations impinging 
on sensory receptors and for the continuous motion of the effectors (e.g., 
muscles) we use to change the world around us.  There are many applica-
tions of  these models. In this chapter, we set out the princi ples  behind 
their use. We highlight the kinds of model used in motor control and the 
dynamical systems that play a role in such models, and we touch on the 
concept of generalized synchrony. Fi nally, we discuss the reconciliation of 
discrete and continuous generative models.
8.2 Movement Control
As we saw in chapter 4, the generative model that underwrites active infer-
ence in continuous time may be written as a pair of stochastic equations 
that determine how states ( x ) generate data ( y) and how states evolve over 
time depending on some static variable (v):
y = g(x) + ωy
x. = f (x,v) + ωx
 (8.1)
 These equations and the precision associated with the fluctuations (ω ) deter-
mine the model used to draw inferences about the  causes of sensations. 
8 Active Inference in Continuous Time
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
154 Chapter 8
Note that action is absent from equation 8.1. This is  because (as outlined in 
chapter 6), action is part of the generative pro cess, not the generative model. 
The generative model only deals with  those variables that are directly influ-
enced by states external to a Markov blanket. If we  were to write down the 
dynamics of the real world (i.e., the generative pro cess), we would have to 
include action (u):
y = g(x) + ωy
x. = f(x, u) + ωx
 (8.2)
Note that the functions g and f (and the precisions of ω) used to define the 
generative model (equation 8.1) are not necessarily the same as  those used 
to define the generative pro cess (equation 8.2). As we saw in chapters 2–4, 
actions change sensory data such that  free energy is minimized. This means 
we do not need to explic itly write down the dynamics of action in the gen-
erative model— they emerge from the choices made for the terms in equa-
tion 8.1. To gain some intuition for this, we start with a very  simple sort of 
generative model:
g( x )  =  x
f ( x, v)  =  v − x (8.3)
Equation 8.3 says that the hidden state represents the expected value for 
the data and that it has dynamics consistent with a  simple (i.e., point) 
attractor. By attractor, we mean that when x is less than v, the expected rate 
of change of x is positive, and vice versa. This means that x  will always flow 
 toward v (i.e., v is an attracting or fixed point). To generate data, we define a 
 simple generative pro cess:
g( x )  =  x
f ( x, u)  =  u (8.4)
On minimizing  free energy, this means that action  will change to fulfill 
the predictions of equation 8.3. If μ is the expected value of x , this means 
the action that minimizes the difference between the predicted data ( g( μ)) 
and the observed data ( y) is to set u equal to v − μ. This is an expression of 
the “equilibrium point hypothesis” (Feldman and Levin 2009), which treats 
motor control as enacted by reflex arcs that simply draw limbs  toward equi-
librium points set by descending motor signals.  Under Active Inference, 
 these signals are predictions— specifically, proprioceptive predictions about, 
for example, the expected position of limbs or eyes (Adams, Shipp, and 
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 155
Friston 2013). Therefore, movement control results from the fulfillment of 
(proprioceptive) predictions by action, as schematically illustrated in fig-
ure 8.1. Note that this scheme does not require specification of “inverse 
models” (i.e., mappings from desired consequences to the motor com-
mands to reach them) that are widely used in other formulations of motor 
control (Wolpert and Kawato 1998).
The expression in equation 8.3 is the simplest sort of attractor system 
we might employ in a generative model. However, it is too  simple in many 
settings, where more realistic Newtonian dynamics apply. A more sophis-
ticated model recognizes that forces— generated by muscles— change the 
velocity (i.e., induce an acceleration), not the position. Equation 8.5 sets 
this out explic itly with x1 as the position and x2 as the velocity:
f (x,v) =
x2
κ
m (v − x1 )
⎡
⎣
⎢
⎤
⎦
⎥ (8.5)
This expression is equivalent to the dynamics of a spring obeying Hooke’s 
law. The rate of change of the position (first ele ment) is simply the velocity. 
The rate of change of the velocity (second ele ment) is proportional to the 
distance between the current position and the point v, with the constant 
of proportionality: a ratio between the mass of the object (m) and a (spring) 
constant (κ ). Multiplying both sides by the mass, we have the force1 gener-
ated by a spring (κ (v − x1)) attached to the points v and x1 equal to the mass 
multiplied by the rate of change of the velocity. This is just Newton’s second 
law. In other words, we can write down a generative model that predicts the 
dynamics that would unfold if  there  were a spring drawing a limb to a desired 
location. By predicting the (proprioceptive) data consequent on this Newto-
nian mechanics, we can enact the movement that fulfills  these predictions.
8.3 Dynamical Systems
As outlined in section 8.2, continuous- time formulations of Active Infer-
ence are well suited to characterization of movements. More generally, 
they are appropriate in specifying generative models of nonlinear dynami-
cal systems wherein discretization of time and space is inefficient. The 
simplest form of dynamical system is the attractor of equation 8.3, but 
much richer be hav ior can be developed from more complex systems. In 
the  limited space of this book, we cannot do justice to the large body of 
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
156 Chapter 8
v
x
f (x,v)
y
x
f (x,u)
Generative model
Generative process
µx = Dµx + ∇µx f /glyph257 Πxεx + …/glyph257 
u = –∇u y(u)/glyph257 Πy εy/glyph257 
y = g(x) + ω y
εy = y – g(µx)
εy
µx
Figure 8.1
Spinal reflexes, illustrating the distinction between a generative pro cess (out  there 
in the world) and a generative model in the setting of action generation. The model 
assumes that the position ( x ) of a limb (or hand or other body part) is drawn  toward 
some point (v). The dashed arrow in the upper plot shows this belief. Beliefs about 
x( μx ) may be substituted in place of x  and used to update beliefs about its rate of 
change. The resulting μx is then used to predict sensory data ( y) via the g function in 
the generative model. Sensory data are actually generated by the generative pro cess 
via the function g , which takes the “real” value of x  as its argument. The error ( εy) 
then drives changes in action (u) such that the error is resolved. This resolution hap-
pens through the generative model, as the action determines the rate of change of x 
via f. This  causes x to move to the location in space that generates data y consistent 
with the prediction (  g(  μx )), setting εy and therefore the rate of change of a to zero.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 157
Box 8.1
Precision, attention, and sensory attenuation
We addressed the importance of precision in chapter 7, but it is worth recap-
ping its role in continuous- time systems. In many ways, this concept is more 
naturally addressed in this setting, as the Π  variable appears as a direct con-
sequence of the Laplace approximation. This acts directly as a multiplicative 
gain in the inferential dynamics (see figure  8.1), with dif fer ent precisions 
weighting alternative influences over belief updating.
The interpretation of precision as a synaptic gain connects it to several 
impor tant aspects of neurobiology. From an empirical point of view, higher 
precision implies more vigorous belief updating of the sort that might be mea-
sured in electrophysiological research as a large amplitude- evoked response 
with an early peak or in single- cell recordings as a multiplicative effect on 
neuronal firing rates in response to a stimulus placed in that cell’s receptive 
field.  These findings are often associated with attentional pro cessing, where 
one sensory channel (or subset of channels) is favored above  others. From 
the perspective of active inference, precision and attention are synonyms. The 
former has been used to reproduce a range of attentional phenomena in silico, 
including the Posner paradigm (Feldman and Friston 2010). Specifically, using 
a cue to predict the precision of sensory input from one of two locations 
reproduces the empirical finding that responses to stimuli in the cued loca-
tion are faster than  those appearing in the alternative location.
A second impor tant aspect of precision control is its role in movement 
generation. To understand this, it is worth thinking about what happens in 
the absence of this control. Imagine, first, that sensory data are predicted with 
high precision. The messages from  these data are therefore afforded high syn-
aptic gain and lead to veracious inferences about the position of some body 
part. The prob lem with this is the equivalence between motor commands and 
predictions  under Active Inference. An accurate belief that “I am not mov-
ing” cannot be used to predict the sensory consequences of movement, vital 
for the initiation of that movement. With high precision sensory input, the 
belief that “I am moving” is immediately corrected in the face of evidence to 
the contrary; hence no movement is executed. This tells us something impor-
tant: In order to generate movement, we must be able to ignore the sensory 
consequences of that movement to form the (initially false) belief that “I am 
moving.” Once this belief is established, the proprioceptive (and other sen-
sory) consequences of that movement may be predicted and enacted through 
the mechanisms outlined in figure 8.1. This pro cess of ignoring evidence to 
the contrary is known as “sensory attenuation” and represents the decrease in 
precision required for a movement to take place (Brown, Adams et al. 2013; 
Pezzulo 2013; Seth 2013; Pezzulo, Rigoli, and Friston 2015; Seth and Friston 
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
158 Chapter 8
2016; Allen et al. 2019). Clearly it is useful between movements to restore this 
precision, to draw the appropriate inferences from sensory input. This implies 
a cyclical pro cess of attenuating and moving (e.g., the cyclical suppression 
of visual input during saccades, then a suppression of saccades). Predicating 
movement on the suspension of attention has close relationships with an ideo-
motor theory that originated in the nineteenth  century to explain movements 
induced  under hypnosis.
Box 8.1 (continued)
work developing models with more complex dynamical systems (but see 
 table 8.1 for some of the key advances). Instead, we focus on a few of the 
princi ples needed to understand  these systems. In this section, we briefly 
overview two dynamical systems used in formulating generative models 
of this sort: Lotka- Volterra dynamics and Lorenz systems. The former may 
be used in characterizations of systems with a sequential aspect to their 
dynamics, while the latter represent chaotic systems.
Lotka- Volterra dynamics inherit from characterizations of predator- prey 
dynamics in ecol ogy. While they have since found application in numerous 
disciplines, predator- prey systems remain a useful example to provide some 
intuition about their workings. When the predator population is small, the 
prey may increase their numbers to become a relatively large population. 
This provides additional food for the predators, whose population size then 
grows. Increased predation  causes a decrease in the number of prey species 
and therefore a decrease in the number of predators. From  here, the cycle 
continues. This gives an oscillatory pattern whereby the prey population 
size peaks, then the predators’, then the preys’ again, and so on. By gener-
alizing this to more than two populations (e.g., carnivore, herbivore, and 
plant populations), we can generate a sequence of peaks. Figure 8.2 illus-
trates generalized Lotka- Volterra dynamics with three populations, which 
obey dynamics of the following form:
f  ( x, v)  =  x ° (v + Ax ) (8.6)
 Here, x is a vector as before. The ° symbol means an elementwise product. 
Intrinsic birth and death rates are given by the vector v , and A is a matrix 
whose ele ments are positive if the species indexed by the column prey on 
 those indexed by the row and negative if the relationship is inverted.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 159
Generalized Lotka-Volterra dynamics
6 c
c
c
c
c
c c c
p
hh h h h h h h
p p p p p p p4
2
0
4
3
2
1
0
02 4
Plant
02 4
Herbivore
6
05 0
Population size (a.u.)
Herbivore
0
2
4
6
Carnivore
100 150
Time
Plant
Herbivore
Carnivore
200 250 300
Figure 8.2
Generalized sequential dynamics emerging from Lotka- Volterra systems provide an 
impor tant point of connection with the discrete sequential dynamics assumed in 
chapter 7.  These dynamics can be applied to a range of systems but are framed  here 
in terms of predator- prey relationships for ease of interpretation. Top: Population 
changes over time. The population size is expressed in terms of arbitrary units (a.u.). 
The peaks are labeled on the basis of which species has the greatest population at 
that point. The repeated pattern of p , h, c can be seen as a sequence of three (not 
necessarily evenly spaced) discrete time steps. Bottom: Trajectories emphasizing the 
(approximately) periodic pattern that each follows.
Figure 8.2 makes clear that having a generative model that incorporates 
Lotka- Volterra dynamics allows for temporal sequencing (Huerta and Ra bino-
vich 2004)— depending on the current highest peak. Each line can be thought 
of as representing a hidden state, in place of a species.  Figure 8.3 highlights 
two impor tant examples in which  these dynamics have been exploited to 
generate be hav ior One is a hierarchical model that uses the sequential 
dynamics afforded by a Lotka- Volterra system to time the response of an 
eye blink relative to a conditioned stimulus (Friston and Herreros 2016). The 
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Expectations (µx
(2)) Expectations (µx)
Expectations (µx
(1))
Expectations (µv
(1))
–0.4
–0.2
0
0.2
0.4
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
20 40 60 80 1001 20
Time
20 40 60 80 100 120
Time
Action (u)
Action (u)
Blink
Writing
US
CS
20 40 60 80 100 120
Time
Time
50 100 150 200 250
5
0
–5
–8
–8
–6
–4
–2
0
2
4
20 40 60 80
Time
Time
100 120
50 100 150 200 250
Figure 8.3
Two applications of generalized sequential Lotka- Volterra dynamics in Active Infer-
ence. Left: Eyeblink conditioning used to empirically investigate cerebellar function 
(Friston and Herreros 2016). Starting at the highest level of the column, the expected 
states show the same kind of sequential pattern as in figure 8.2. This passes down 
to the next level to predict sequential hidden  causes; the vari ous peaks  here predict 
states at the next level down, where the first peak is the conditioned stimulus (CS) 
and the second is the unconditioned stimulus (US). Fi nally, the predicted US induces 
action— a blink. Right: Sequential peaks using an attracting point as in equation 8.5 but 
selecting the specific attractor on the basis of which population of a Lotka- Volterra 
system is currently highest; this leads to a sequential visiting of each point, giving 
rise to a form of handwriting (Friston, Mattout, and Kilner 2011).
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 161
paradigm is based on  those used in the investigation of cerebellar function. 
An unconditioned stimulus (a puff of air directed  toward an animal’s eye) 
elicits a response (blinking). A conditioned stimulus (an auditory tone) may 
be played prior to the unconditioned stimulus on multiple occasions. By 
learning (see box 8.2) the number of peaks in the Lotka- Volterra dynamics 
that separate the conditioned stimulus from the unconditioned stimulus, 
the animal learns to preempt the air puff and time the appropriate blink. 
This is a form of temporal learning, since the number of peaks provides an 
implicit estimate of the length of the temporal interval from the condi -
tioned to the unconditioned stimuli. In the second example in figure 8.3, 
each sequential peak is associated with an alternative attracting point that 
drives movements to a series of attracting points arranged to suggest hand-
writing (Friston, Mattout, and Kilner 2011). As the two examples illustrate, 
Box 8.2
Learning in continuous models
As discussed in chapter 7, learning is the pro cess of optimizing beliefs about 
the par ameters (θ ) of a generative model. In the continuous- time domain, 
this means accumulating evidence over time. This works as if we treat data in 
a series of infinitesimally small time- intervals as obeying i.i.d. (in de pen dent 
and identically distributed) assumptions and formulate a generative model 
that generates observations from (time- invariant) par ameters:
ln p( !y,θ) = ln p(θ) + ln p(y(t )∫ | θ)dt
≈ln p(θ) − F[ y(t )∫ | θ]dt
This may be used to formulate a functional (S ) that plays the role of a  free 
energy for par ameters using the time integral of the  free energy conditioned 
on par ameters. Using a Laplace approximation, we get the following, wherein 
α acts to accumulate  free energy gradients (i.e., evidence gradients):
S(θ) = Eq(θ)[ ln q(θ) + F[ y(t) | θ]∫ dt − ln p(θ)]
≈ F[ y(t) | µθ ]∫ dt − ln p(µθ )
µ.
θ = ∂µθ S(µθ )
= ∂µθ ln p(µθ ) − ∂µθ F[ y(t) | µθ ]∫ dt
= ∂µθ ln p(µθ ) −α
α. = ∂µθ F[ y(t) | µθ ]
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
162 Chapter 8
generalized Lotka- Volterra systems afford useful models of sequential dynam-
ics using a continuous dynamical system.
The POMDP formulation of chapter 7 has largely superseded the use of 
generalized Lotka- Volterra systems in Active Inference applications. How-
ever, it is useful to bear this kind of dynamic in mind as a plausible con-
tinuous system that might underwrite the discrete sequential dynamics of 
chapter 7. In addition, Lotka- Volterra systems make explicit the distinction 
between repre sen ta tions of sequences involved in temporally deep planning 
and repre sen ta tions of rates of change in generalized coordinates of motion 
(see chapter 4). Each has its place but deals with dif fer ent sorts of prob lems.
The second sort of dynamical system that has found widespread applica-
tion in active inference research is the Lorenz system:
x. =
σ(x2 − x1 )
x1(ρ − x3 ) − x2
x1x2 − βx3
⎡
⎣
⎢
⎢
⎢
⎤
⎦
⎥
⎥
⎥  
(8.7)
The par ameters are known as the Prandtl number (σ ), the Rayleigh num-
ber ( ρ), and a constant ( β ) that relates to the physics of the system. Depend-
ing on the values  these take, the system may behave in very dif fer ent ways. 
Lorenz attractors  were initially formulated to account for atmospheric 
convection dynamics. Their itinerant (wandering) be hav ior has prompted 
their use in generative models to simulate challenging inference prob lems. 
An impor tant example of this is in the simulation of birdsong, which we 
unpack in the next section.  These systems have also been used to simu-
late  simple physical systems and to investigate the conditions  under which 
their be hav ior starts to appear sentient. Figure 8.4 shows how the Lorenz 
system behaves  under example pa ram e ter settings.
8.4 Generalized Synchrony
As mentioned above, a key example application of continuous state- space 
models is in a series of studies based on synthetic birdsong (Friston and 
Frith 2015b). An impor tant aspect of  these studies looks at communication 
and multi- agent inference prob lems. The idea  here rests on the capacity 
of a creature to synchronize its internal states with something out  there 
in the world (i.e., inference). When what is out  there is another creature 
with a similar model, this synchronization means the internal states of one 
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 163
creature should come to resemble the internal states of the other: a primi-
tive kind of theory of mind.
Figure 8.5 shows the kind of generative model used to simulate song-
birds. In this hierarchical model, high- level states (level 2) evolve accord-
ing to a slow Lorenz system. One dimension of this system is then used to 
pa ram e terize the Rayleigh number of a faster Lorenz system at the lower 
level (level 1). The lower- level variables then map to sensory (sonographic) 
data. Analogous to figure 8.1, the generative pro cess additionally includes 
action;  here, instead of moving a limb, actions influence the larynx, such 
that the sonographic data may be influenced by the bird. As before, action 
is generated to resolve prediction error. This means that if a bird hears the 
song it is predicting,  there is no need to generate it itself. However, if it 
predicts a song that is not heard, it must start singing to resolve any error.
0 500
–20
0
20
40
Lorenz system
x
–20
–10
0
10
20
y
1000 1500 2000 2500
–10 01 02 0 –20 02 0
y
0
10
20
30
40z
Figure 8.4
Be hav ior of a Lorenz system attractor (using the same format as figure 8.2), showing 
how this 3- dimensional system evolves. Characteristically, it appears chaotic and 
unpredictable, spending some of its time orbiting one part of space before switch-
ing to another orbit. This itinerancy and apparent autonomy make this in ter est ing 
system well suited to inclusion in models of biological phenomena.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
164 Chapter 8
This dynamic becomes more in ter est ing when  there are two birds in play, 
with similarly structured generative models. As long as one bird is singing, 
the other does not need to, as  there is no error to resolve. However, if one 
bird stops singing, the other needs to continue the same song. This leads to 
a form of turn taking, sometimes phrased as “singing from the same hymn 
sheet,” with each bird contributing sections of the same song. What leads 
to this turn taking? Why  doesn’t one bird continue singing the  whole song 
to its conspecific? The answer relates to the issue of sensory attenuation (see 
box 8.1), as acting to generate birdsong requires a reduction in the preci-
sion of predictions about the consequences of action. Just as in saccadic eye 
movements, this implies alternation between attention to sensory (visual or 
auditory) data and attenuation during (saccadic or vocal) action designed 
to change  those data. When  there are two agents involved, this leads to an 
Generative model
g(2) = x1
(2)
Level 2
Level 1
Sensory data y
Synchronization manifold
Before learning
Expectations
(second bird)
Expectations
(second bird)
60
40
20
–20
–20
–50 05 0 100
02 0
Second-level
expectations (first bird)
Second-level
expectations (first bird)
40–40
–40
–20
0
20
40
60
80
0
After learning
10(x2
(2) – x1
(2))
(32 – x3
(2))x1
(2) – x2
(2)f (2) = 1
128–—
x1
(2) x2
(2) –   x3
(2)8
3–
10(x2
(1) – x1
(1))
θ((v1
(1) – 8) – x3
(1))x1
(1) – x2
(1)f (1) = 1
16––
x1
(1) x2
(1) –   x3
(1)8
3–
g(1) =
x2
(1)
x3
(1)
Figure 8.5
Synchronization and communication. Left: Generative model underwriting the bird-
song simulations described in the main text. This is a hierarchical model, with Lorenz 
attractors at each level. Right: Synchronization manifolds of expectations at the sec-
ond level for two birds before and  after they have learned about one another.  After 
learning the par ameters of each other’s generative models, the two bird’s joint trajec-
tory is confined to an (almost) 1- dimensional subspace, indicating synchronization.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 165
alternation between listening to the other and singing— a  simple form of 
conversation.
For this synthetic conversation to work, it is essential that the two birds 
synchronize with one another and know where they are in the song (or 
conversational trajectory). This implies that the inferences about hidden 
states in the generative model should be aligned between the birds. On 
the upper right of figure 8.5, we show a synchronization manifold of two 
birds who have not yet optimized their generative models in relation to one 
another; this plots a trajectory of the beliefs each bird has about the higher- 
level hidden states. Synchronization implies that when one bird infers a 
specific hidden state value, the other bird should infer the same; there-
fore, we would expect the trajectory to stay fixed to the x   =  y line (tech-
nically called identical synchronization of chaos). Fluctuations around this 
line imply imperfect synchronization, as this plot shows.  After exposure to 
one another and learning the par ameters of each other’s generative models 
(see box 8.2), the synchronization is nearly perfect (lower- right plot). The 
implication is that each bird has learned about the other and is able to infer 
what is  going on in the other’s head. In short, they have learned to share 
the same narrative and “sing from the same hymn sheet.”
A more general form of synchronization does not require synchronization 
along the x  =  y line. In generalized synchronization, the joint be hav ior occu-
pies a lower, 1- dimensional space than the higher, 2- dimensional space that 
could be occupied by this be hav ior. However, this low- dimensional space 
(the synchronization manifold) may be curved or have some other shape; 
this is analogous to the 2- dimensional space we occupy on the surface of 
the planet, despite the surface being curved into a 3- dimensional sphere. In 
addition to its central role in social be hav ior, generalized synchronization— 
occupancy of a low- dimensional region of a high- dimensional joint space—
is very impor tant in characterizations of biological systems as engaging in 
inference (generalized synchrony between internal and external states). 
While we do not have the space to unpack this extensive subject  here, the 
inferential perspective speaks to the failure of generalized synchrony associ-
ated with neuropsychiatric syndromes like autism. This kind of synchrony 
is impor tant not only in continuous- time models but also in POMDP 
models of linguistic communication between multiple agents (Friston, Parr 
et al. 2020).
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
166 Chapter 8
8.5 Hybrid (Discrete and Continuous) Models
As we have seen in this and the previous chapter, discrete and continuous 
models both have impor tant applications in Active Inference. While many 
settings call for one or the other, a more holistic perspective acknowledges 
that both are likely in play. This means we need a way to combine  these 
generative models so that a single model includes both continuous and 
discrete variables (Friston, Parr, and de Vries 2017). Such hybrid or mixed 
models allow inferences about sequential action plans and translations of 
 these decisions into movements through a continuous model. Figure 8.6 
shows the form of  these models, with a POMDP at the higher level, behav-
ing as described in chapter 7, which generates a continuous model of the 
D
η
f
gg g
f f
gg g
f f
gg g
f
ηη
B B
A A
G
A
sτ–1 sτ+1sτ
oτ–1
v
x xʹ xʺ
y yʹ yʺ
v
ϖ
x xʹ xʺ
y yʹ yʺ
v
x xʹ xʺ
y yʹ yʺ
oτ+1oτ
Figure 8.6
Mixed generative models in the form of a hierarchical model much like that in fig-
ure 7.12. However,  there is an impor tant difference between the form of the model 
at the higher level and that at the lower level. While the lower- level model (one 
example is highlighted by the dashed box) is the same form as the other models 
considered in this chapter— i.e., it is framed in terms of continuous states and con-
tinuous time and uses generalized coordinates of motion— the higher- level model is 
a POMDP model of the sort we saw throughout chapter 7— i.e., it is framed in terms 
of discrete states and times. Effectively, this means we can select (at regular time 
intervals) between alternative segments of a continuous trajectory.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 167
sort addressed in this chapter at each discrete time step. This decomposes 
continuous time into a discrete sequence of short continuous trajectories.
To translate from the outcomes of the discrete level to the continuous 
level, we need to associate each alternative outcome with a point in some 
continuous space. To develop intuition for this idea, we consider the exam-
ple of a delay- period oculomotor task— often used in primate research; see 
figure 8.7. This task involves three stages. First, a target appears in one of (for 
example) four pos si ble locations, while a monkey maintains fixation on a 
central fixation cross. Next, the target dis appears and must be remembered 
during a delay period. Fi nally, a signal is given for the monkey to make 
a saccade, at which point it must look at the location where the original 
target appeared. To complete this task, the monkey must be able to draw 
inferences about sequences (which stage of the task is currently in play) and 
to infer which of four locations to aim for.  These are categorical inference 
prob lems suited to a POMDP formulation. However, once the appropriate 
location has been selected, the monkey must perform the eye movement 
that brings its fovea to the (continuous) coordinates of the target location.
Figure 8.7 illustrates the functioning of a mixed generative model that 
solves this prob lem. In the top panel, the model’s higher level makes cate-
gorical decisions: it computes the posterior beliefs about four discrete target 
locations at four time periods. In the  middle and bottom plots, the model’s 
lower level computes continuous behavioral trajectories (eye movements) 
resulting from discrete inferences at the higher level.
Transforming decisions about discrete target locations into continuous 
eye movements requires each discrete target location (o ) to be associated 
with a distribution over continuous hidden  causes (v), which identifies the 
target coordinates. The prior over target coordinates can then be computed 
by taking the Bayesian model average over  these locations, weighted by the 
inferences at the POMDP level:
P !v | oτ( ) = N( !ηoτ, !Πv )
P( !v) ≈ N( !η, !Πv )
!η= EQ( oτ ) !η[ ] = oτ i !η  
(8.8)
To infer which discrete target best explains continuous data, we need to 
be able to compute the evidence associated with each hy po thet i cal target— 
which is a function of observed continuous data. This exemplifies the fact 
that mixed models require reciprocal interactions between higher and lower 
hierarchical levels. As we see in figure  8.7, this facilitates the formation of 
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
168 Chapter 8
Fixate center (τ = 1)
1
0.8
0.6
0.4
0.2
0
Fixate center (τ = 2)
Cue
25 0.6
0.5
0.4
0.3
0.2
0.1
0
20
15
10
5
0
0 200 400 600
T ime
ms
cm
Distance
Speed
m/s
800
Delay Saccade Fixate
Probability
Fixate up (τ = 4)
Firing rates (discrete )
Behavior (continuous ) Speed
Distance
Saccade up (τ = 3)
Figure 8.7
Transforming decisions into movement using mixed or hybrid models (Parr and Fris-
ton 2019b). This  simple example uses the oculomotor delay- period paradigm out-
lined in the main text. Top: Neuronal firing rates representing posterior beliefs about 
the target location. The target may be in four dif fer ent locations, and  there are four 
time steps in this synthetic experiment, so  there are 16 neural populations represent-
ing each of  these combinations. The lines corresponding to the final inferred state of 
affairs are annotated explic itly. Note the belief updating at the first time step (from 0 
to 250 ms), when the target initially appears, and at the third time step (from 500 to 
750 ms), when the agent observes itself performing a saccade to that location.  Middle: 
Be hav ior (i.e., a saccade from the center location to the target location). During the 
first quarter of the experiment, the target is vis i ble in the upper location. Next,  there 
is a delay period, during which fixation is maintained. Then a saccade is performed 
to the correct location. Fi nally,  there is a period during which fixation is maintained 
at the target location. Bottom: Continuous behavioral trajectories that result from the 
discrete inferences of the top plot.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 169
beliefs about where to perform saccades and the execution of  those saccades 
at the appropriate times. At the first discrete time step (up to 250 ms on the 
continuous scale), the monkey is able to infer with some confidence that 
its eyes are centered on the fixation cross during the first time step, that it 
 will maintain this fixation at the second time step (up to 500 ms), and that 
the most likely course of action  after this  will result in foveating the upper 
location. This can be seen in the discrete firing rates (top plot). This trans-
lates into the continuous be hav ior which, when implemented, increases the 
confidence in beliefs about the discrete states (note the increase in probabil-
ity for an upward saccade at the third time step once the continuous data 
become available between 500 and 750 ms). This  simple example, based on 
experimental cognitive research, illustrates the basic princi ples of translation 
between discrete action plans and their continuous implementation.
8.6 Summary
In this chapter, we have overviewed the applications of continuous- time 
generative models  under Active Inference. This is a huge topic, and much 
has been left out (see  table  8.1 for further reading). However, the broad 
Box 8.3
Mixture models and clustering
The issue of combining categorical and continuous generative models outside 
of Active Inference has primarily been framed through the lens of clustering. 
 Here, the aim is to assign each (continuous) data point to a (discrete) cluster. 
A range of algorithms have been employed to solve this prob lem, but most of 
them implicitly rely on a generative model similar to that used  here. This is a 
mixture of Gaussians (aka a Gaussian mixture model):
P( !y, !s, D, η, Π) = P(D)P(η)P(Π) P(si | D)P(yi | si ,η,Π)
i
∏
P(D) = Dir(d)
P(si |D) = Cat(D)
P(yi | si,η, Π) = N(ηsi , Πsi )
The prob lem in clustering approaches is to infer the mean and precision 
(η and Π, respectively) of each cluster and the posterior probability that each 
data- point ( yi) belongs to a given cluster P(si | yi). For our purposes (as described 
in section 8.5), we assume precise (delta- function) priors for η and Π and cal-
culate Q(si) ≈ P(si | yi) via Bayesian model reduction (see box 7.3).
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
170 Chapter 8
 Table 8.1
Key advances in continuous- time models
Application Sources Notes
Synthetic 
birdsong
Friston and Frith 2015a
Friston and Frith 2015b
Isomura, Parr, and 
Friston 2019
This series of papers deals with 
communication and the interaction 
between synthetic agents, a simu-
lated pair (or group) of songbirds 
singing to one another. The studies 
unpack phenomena from general-
ized synchrony to perceptual infer-
ence to sensory attenuation.
Oculomotor 
delays
Perrinet, Adams, and 
Friston 2014
By taking advantage of beliefs 
about the near past and  future 
implicit in models formulated in 
generalized coordinates of motion, 
it is pos si ble to account for sensori-
motor delays through projections  
a short way into the  future or past.
Conditioned 
reflexes
Friston and Herreros 
2016
Using a model based on a Lotka- 
Volterra system, the temporal 
relationship between a conditioned 
and unconditioned stimulus is 
learned and used to generate an 
anticipatory blink.
Smooth pursuit 
eye movements
Adams, Perrinet, and 
Friston 2012
This work looks at the role of 
smooth pursuit eye movements, 
following a visual target. It aims 
to reproduce differences between 
neurotypical and schizophrenic 
individuals in response to pursuit 
with and without visual occlusion.
Psychosis Adams, Stephan et al. 
2013
Building on the songbird and 
smooth pursuit models, this 
research looks at how false, psy-
chotic inference may arise from 
suboptimal prior beliefs.
Illusions Brown and Friston 2012
Brown, Adams et al. 2013
Illusions offer a useful tool to reveal 
the prior beliefs our brains appeal 
to in the presence of uncertain or 
ambiguous sensory input.  These 
papers take several examples of 
common illusions and demonstrate 
the optimality of illusory inferences 
 under certain prior beliefs.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
Active Inference in Continuous Time 171
 Table 8.1
(continued)
Application Sources Notes
Saccades Friston, Adams et al. 
2012
Donnarumma et al. 2017
Parr and Friston 2018a
Like the smooth pursuit simula-
tions,  these papers consider eye 
movement control. However,  here 
the eyes do not simply follow a 
target but must move to one of 
several pos si ble target locations. 
They deal with the generative 
models we need to be able to do 
this and (once the models have 
been specified) the emergent archi-
tectures and physiology.
Action 
observation
Friston, Mattout, and 
Kilner 2011
This work considers the role of the 
mirror neuron system and formal-
izes the idea that generative models 
of our own actions can also be put 
to use in modeling, and replicating, 
be hav ior observed in  others.
Attention Feldman and Friston 
2010
Kanai et al. 2015
Through predicting precision, we 
implicitly select the data that we 
believe is most informative. This 
work highlights how implementa-
tions of this idea reproduce classical 
psychophysical findings in the 
Posner paradigm and figure- ground 
discrimination tasks.
Hybrid models Friston, Parr, and de 
Vries 2017
Parr and Friston 2018c
Parr and Friston 2019b
 These models make use of discrete 
POMDP models in combination 
with predictive coding schemes. 
Most current examples of this 
modeling are framed in terms of 
visual search be hav ior or oculo-
motor control.  These require 
selecting where to look and then 
implementing the pro cess of 
looking  there.
Self- organization Friston 2013
Friston, Levin et al. 2015
Palacios et al. 2020
This line of research is based on 
the idea that groups of cells can 
or ga nize into a predefined struc-
ture when each cell has the same 
implicit generative model of that 
structure. Specifically, they must 
know what sort of sensory input 
they would predict if they  were a 
par tic u lar kind of cell.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
172 Chapter 8
concepts outlined  here provide a foundation from which  these models may 
be further explored. Specifically, we have considered movement generation 
in terms of the fulfillment of predictions. This greatly simplifies the treat-
ment of motor control prob lems, as we do not need any additional machin-
ery or inverse models— just spinal or brain stem reflex arcs. We highlighted 
the role of precision and sensory attenuation in motor control of this sort. 
Given that a key advantage of continuous schemes is to articulate generative 
models in terms of dynamical systems, we outlined two ubiquitous types of 
dynamical system that have found widespread application in Active Infer-
ence research. Generalized Lotka- Volterra systems act to provide temporal 
sequencing in a continuous context, while Lorenz attractors may be used 
to generate rich simulations, including synthetic birdsong. Next we consid-
ered the concept of generalized synchrony. Synchronization of the internal 
states of a system to external states forms the basis of inferential treatments 
of brain function and is crucial in accounts of social systems— where exter-
nal states largely comprise conspecifics (i.e., creatures like me). Fi nally, we 
set out the unification of the discrete and continuous models of chapters 7 
and 8, bringing together the expected  free energy minimizing (exploitative 
and explorative) dynamics of POMDP formulations, the enaction of the 
be hav iors  these mandate through continuous pro cesses, and the reciprocal 
message passing that mediates this interaction. In short, this takes us from 
decisions to movements— and back again.
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
This is a section of doi:10.7551/mitpress/12441.001.0001
Active Inference
The Free Energy Principle in Mind, Brain, and
Behavior
By: Thomas Parr, Giovanni Pezzulo, Karl J.
Friston
Citation:
ActiveInference:TheFreeEnergyPrincipleinMind,Brain,and
Behavior
By:
DOI:
ISBN (electronic):
Publisher:
Published:
Thomas Parr, Giovanni Pezzulo, Karl J. Friston
The MIT Press
2022
10.7551/mitpress/12441.001.0001
9780262369978
The open access edition of this book was made possible by
generous funding and support from MIT Press Direct to Open
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025
© 2022 Massachusetts Institute of Technology
This work is subject to a Creative Commons CC BY-NC-ND license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided 
comments on drafts of this book. The generous work of academic experts is essential 
for establishing the authority and quality of our publications. We acknowledge with 
gratitude the contributions of these otherwise uncredited readers.
This book was set in Stone Serif and Stone Sans by Westchester Publishing Services. 
Library of Congress Cataloging-in-Publication Data is available.
Names: Parr, Thomas, 1993– author. | Pezzulo, Giovanni, author. | Friston, K. J. 
(Karl J.), author.
Title: Active inference : the free energy principle in mind, brain, and behavior / 
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
Description: Cambridge, Massachusetts : The MIT Press, [2022] | Includes 
bibliographical references and index.
Identifiers: LCCN 2021023032 | ISBN 9780262045353 (hardcover)
Subjects: LCSH: Perception. | Inference. | Neurobiology. | Human behavior models. | 
Knowledge, Theory of. | Bayesian statistical decision theory.
Classification: LCC BF311 .P31366 2022 | DDC 153—dc23
LC record available at https://lccn.loc.gov/2021023032
MIT Press Direct
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246585/c006700_9780262369978.pdf by guest on 11 December 2025