The sum of log-normal variates in geometric
Brownian motion
Ole Peters and Alexander Adamou
London Mathematical Laboratory
8 Margravine Gardens, London W6 8RH, UK
February 9, 2018
Abstract
Geometric Brownian motion (GBM) is a key model for represent-
ing self-reproducing entities. Self-reproduction may be considered the
deﬁnition of life [7], and the dynamics it induces are of interest to those
concerned with living systems from biology to economics. Trajecto-
ries of GBM are distributed according to the well-known log-normal
density, broadening with time. However, in many applications, what’s
of interest is not a single trajectory but the sum, or average, of several
trajectories. The distribution of these objects is more complicated.
Here we show two diﬀerent ways of ﬁnding their typical trajectories.
We make use of an intriguing connection to spin glasses: the expected
free energy of the random energy model is an average of log-normal
variates. We make the mapping to GBM explicit and ﬁnd that the free
energy result gives qualitatively correct behavior for GBM trajecto-
ries. We then also compute the typical sum of lognormal variates using
Itˆ o calculus. This alternative route is in close quantitative agreement
with numerical work.
1 Introduction – why study GBM?
The basic phenomenon of self-reproduction occurs from the smallest chemical
compound capable of building copies of itself to arguably the most complex
known social system, namely the global economy. Not only is the economy
made by humans who self-reproduce and whose cells self-reproduce but capi-
talism itself taps into the power of self-reproduction: capital means resources
which can be deployed in order to generate resources, which can be deployed
1
arXiv:1802.02939v1  [cond-mat.stat-mech]  8 Feb 2018
in order. . . . Realism usually requires models that include noise, to account
for the multitude of eﬀects not explicitly modelled. GBM is a simple, intu-
itive, and analytically tractable model of noisy multiplicative growth. A deep
understanding of this basic model of self-reproduction is crucial and only in
the last few decades have we achieved this.
A quantity, x, of self-reproducing resources (such as biomass or capital)
follows GBM if it evolves over time, t, according to the Itˆ o stochastic diﬀer-
ential equation,
dx= x(µdt+ σdWt). (1)
dt denotes the inﬁnitesimal time increment and dWt the inﬁnitesimal incre-
ment in a Wiener process, which is a normal variate with ⟨dWt⟩= 0 and
⟨dWtdWs⟩= δ(t−s)dt. µ and σ are constant parameters called the drift
and volatility. Put simply, relative changes in resources, dx/x, are assumed
to be drawn independently from a stationary normal distribution at each
time step. Eq. (1) represents the continuous-time limit of this process. Its
solution,
x(t) = x(0) exp
[(
µ−σ2
2
)
t+ σW(t)
]
, (2)
yields exponential growth at a noisy rate. Hereafter, we shall assumex(0) = 1
and neglect it, except where retaining it is illustrative. The distribution of
x(t) is a time-dependent log-normal,
ln(x(t)) ∼N
((
µ−σ2
2
)
t, σ2t
)
, (3)
whose mean, median, and variance grow (or decay) exponentially in time:
⟨x(t)⟩ = exp( µt); (4)
median(x(t)) = exp[( µ−σ2/2)t]; (5)
var(x(t)) = exp(2 µt)[exp(σ2t) −1]. (6)
GBM is a standard model in ﬁnance for self-reproducing quantities such
as stock prices. Since relative changes are modelled as normal variates, the
central limit theorem means that GBM is an attractor process for a large
class of multiplicative dynamics. Any quantity whose relative changes are
random variables with ﬁnite mean and variance will behave like a GBM
after a suﬃciently long time. We work with GBM because it is standard
and general. It exempliﬁes important qualitative and universal features of
multiplicative growth.
Speciﬁcally, we are interested in GBM as a model of economic resources,
owned by some entity. We will think of x as measured in currency units,
such as dollars.
2
1.1 Non-ergodicity of GBM
The non-ergodicity of this growth process manifests itself in an intriguing
way as a diﬀerence between the growth of the expectation value of x and
the growth of x over time. Imagine a world where people’s wealth follows
Eq. (1). In such a world each person’s wealth grows exponentially at rate
gt = µ−σ2/2 (7)
with probability 1 if we observe the person’s wealth for a long time. The
expectation value of each person’s wealth grows exponentially at
g⟨⟩ = µ. (8)
The expectation value, by deﬁnition, is the average over an ensemble of
N realizations of x in the limit N →∞. Important insights follow: the
aggregate wealth in our model economy does not grow at the same rate as
an individual’s wealth [1] (meaning that GDP may be a ﬂawed reﬂection
of national economic well-being); inequality grows indeﬁnitely [4, 2] (even
without interactions between individuals); and pooling and sharing resources
accelerates growth [8, 3].
1.2 PEAs in GBM – a sketch
The following question often emerges in applications of these results. Equa-
tion (7) and Eq. (8) are limiting cases ( t →∞ and N →∞, respectively):
what happens when time and population size are ﬁnite? In [10] we stud-
ied the “partial ensemble average” (PEA) of GBM, details of which are in
Sec. (1.3). The PEA is the sample mean of N independent realisations of
GBM:
⟨x(t)⟩N ≡ 1
N
N∑
i=1
xi(t). (9)
Here we sketch out some simple arguments about how this object depends
on N and t.
Considering Eq. (7) and Eq. (8) together, we expect the following tension:
A) for large N, the PEA should resemble the expectation value, exp( µt);
B) for long t, all trajectories should grow like exp[( µ−σ2/2)t].
Situation A – when a sample mean resembles the corresponding expectation
value – is known in statistical physics as “self-averaging.” A simple strategy
3
for estimating when this occurs is to look at the relative variance of the PEA,
R≡var(⟨x(t)⟩N)
⟨⟨x(t)⟩N⟩2 . (10)
To be explicit, here the ⟨·⟩and var(·) operators, without N as a subscript,
refer to the mean and variance over all possible PEAs. The PEAs themselves,
taken over ﬁnite samples of size N, are denoted ⟨·⟩N. Using standard results
for the mean and variance of sums of independent random variables and
inserting the results in Eq. (4) and Eq. (6), we get
R(N) = eσ2t −1
N . (11)
If R ≪1, then the PEA will likely be close to its own expectation value,
which is equal to the expectation value of the GBM. Thus, in terms of N
and t, ⟨x(t)⟩N ≈⟨x(t)⟩when
t< ln N
σ2 . (12)
This hand-waving tells us roughly when the large-sample (or, as we see from
Eq. (12), short-time) self-averaging regime holds. A more careful estimate
of the cross-over time in Eq. (28) is a factor of 2 larger, but the scaling is
identical.
For t >ln N/σ2, the growth rate of the PEA transitions from µ to its
t →∞ limit of µ−σ2/2 (Situation B). Another way of viewing this is to
think about what dominates the average. For early times in the process, all
trajectories are close together, but as time goes by the distribution broadens
exponentially. Because each trajectory contributes with the same weight to
the PEA, after some time the PEA will be dominated by the maximum in
the sample,
⟨x(t)⟩N ≈ 1
N
N
max
i=1
{xi(t)}, (13)
as illustrated in Fig. 1. Self-averaging stops when even the “luckiest” trajec-
tory is no longer close to the expectation value exp(µt). This is guaranteed to
happen eventually because the probability for a trajectory to reach exp( µt)
decreases towards zero as t grows. Of course, this takes longer for larger
samples, which have more chances to contain a lucky trajectory.
4
0 100 200 300 400 500
t
10 6
10 4
10 2
100
102
104
106
exp(g t)
exp(gtt)
x(t) N
N
maxi {xi(t)}/N
Figure 1: PEA and maximum in a ﬁnite ensemble of sizeN = 256. Red line:
expectation value ⟨x(t)⟩. Green line: exponential growth at the time-
average growth rate. In the T → ∞limit all trajectories grow at this
rate. Yellow line:contribution of the maximum value of any trajectory
at time t to the PEA. Blue line:PEA ⟨x(t)⟩N. Vertical line:Crossover –
for t>t c = 2 lnN
σ2 the maximum begins to dominate the PEA (the yellow line
approaches the blue line). Grey lines:randomly chosen trajectories – any
typical trajectory soon grows at the time-average growth rate. Parameters:
N = 256, µ= 0.05, σ=
√
0.2.
5
1.3 Our previous work on PEAs
In [10] we analysed PEAs of GBM analytically and numerically. Using Eq. (2)
the PEA can be written as
⟨x⟩N = 1
N
N∑
i=1
exp
[(
µ−σ2
2
)
t+ σW(i)(t)
]
, (14)
where
{
W(i)(t)
}
i=1...N are N independent realisations of the Wiener process.
Taking the deterministic part out of the sum we re-write Eq. (14) as
⟨x⟩N = exp
[(
µ−σ2
2
)
t
] 1
N
N∑
i=1
exp
(
t1/2σξi
)
, (15)
where {ξi}i=1...N are N independent standard normal variates.
We found that typical trajectories of PEAs grow at g⟨⟩ up to a time
tc that is logarithmic in N, meaning tc ∝ ln N. This is consistent with
our sketch in Sec. (1.2). After this time, typical PEA-trajectories begin to
deviate from expectation-value behavior, and eventually their growth rate
converges to gt. While the two limiting behaviours N →∞ and t →∞
can be computed exactly, what happens in between is less straight-forward.
The PEA is a random object outside these limits. In [10] we dealt with this
issue numerically by creating a super-sample of S samples, each consisting
of N trajectories. In this way we were able to study the median of ⟨x⟩N (t),
representing the behavior of typical trajectories.
A quantity of crucial interest to us is the exponential growth rate expe-
rienced by the PEA,
gest(t,N) ≡ln(⟨x(t)⟩N) −ln(x(0))
t−0 = 1
t ln(⟨x(t)⟩N). (16)
In [10] we proved that the t→∞ limit for any (ﬁnite) N is the same as for
the case N = 1,
lim
t→∞
gest(t,N) = µ−σ2
2 (17)
for all N ≥1. Substituting Eq. (15) in Eq. (16) produces
gest(t,N) = µ−σ2
2 + 1
t ln
(
1
N
N∑
i=1
exp(t1/2σξi)
)
(18)
= µ−σ2
2 −ln N
t + 1
t ln
( N∑
i=1
exp(t1/2σξi)
)
. (19)
6
We didn’t look in [10] at the expectation value of gest(t,N) for ﬁnite time
and ﬁnite samples, but it’s an interesting object that depends on N and tbut
is not stochastic. Note that this is not gest of the expectation value, which
would be the N →∞ limit of Eq. (16). Instead it is the S →∞ limit,
⟨gest(t,N)⟩= 1
t ⟨ln(⟨x(t)⟩N)⟩= f(N,t), (20)
where, as in Sec. (1.2), ⟨·⟩without subscript refers to the average over all
possible samples, i.e. limS→∞ ⟨·⟩S. The last two terms in Eq. (19) suggest an
exponential relationship between ensemble size and time. The ﬁnal term is a
tricky stochastic object on which the properties of the expectation value in
Eq. (20) will hinge. This term will be the focus of our attention: the sum of
exponentials of normal random variates or, equivalently, log-normal variates.
2 Mapping to the random energy model
Since the publication of [10] we have learned, thanks to discussions with J.-
P. Bouchaud, that the key object in Eq. (19) – the sum of exponentials of
normal random variates – has been of interest to the mathematical physics
community since the 1980s.
The reason for this is Derrida’s random energy model [5, 6]. It is deﬁned as
follows. Imagine a system whose energy levels are 2 K = N random numbers
ξi (corresponding to K = ln N/ln 2 spins). This is a very simple model of a
disordered system, such as a spin glass, the idea being that the system is so
complicated that we “give up” and just model its energy levels as realizations
of a random variable. (We denote the number of spins by K and the number
of resulting energy levels by N, whereas Derrida uses N for the number of
spins). The partition function is then
Z =
N∑
i=1
exp
(
βJ
√
K
2 ξi
)
, (21)
where the inverse temperature, β, is measured in appropriate units, and the
scaling in K is chosen so as to ensure an extensive thermodynamic limit [5,
p. 79]. J is a constant that will be determined below. The logarithm of the
partition function gives the Helmholtz free energy,
F = −ln Z
β (22)
= −1
β ln
[ N∑
i=1
exp
(
βJ
√
K
2 ξi
)]
. (23)
7
Like the growth rate estimator in Eq. (16), this involves a sum of log-
normal variates and, indeed, we can rewrite Eq. (19) as
gest = µ−σ2
2 −ln N
t −βF
t , (24)
which is valid provided that
βJ
√
K
2 = σt1/2. (25)
Equation (25) does not give a unique mapping between the parameters of
our GBM, ( σ,t), and the parameters of the REM, ( β,K,J ). Equating (up
to multiplication) the constant parameters, σ and J, in each model gives us
a speciﬁc mapping:
σ= J√
2 and t1/2 = β
√
K. (26)
The expectation value of gest is interesting. The only random object in
Eq. (24) is F. Knowing ⟨F⟩thus amounts to knowing⟨gest⟩. In the statistical
mechanics of the random energy model ⟨F⟩is of key interest and so much
about it is known. We can use this knowledge thanks to the mapping between
the two problems.
Derrida identiﬁes a critical temperature,
1
βc
≡ J
2
√
ln 2
, (27)
above and below which the expected free energy scales diﬀerently with K
and β. This maps to a critical time scale in GBM,
tc = 2 lnN
σ2 , (28)
with high temperature (1 /β >1/βc) corresponding to short time ( t < tc)
and low temperature (1/β <1/βc) corresponding to long time (t>t c). Note
that tc in Eq. (28) scales identically with N and σ as the transition time,
Eq. (12), in our sketch.
In [5], ⟨F⟩is computed in the high-temperature (short-time) regime as
⟨F⟩ = E−S/β (29)
= −K
β ln 2−βKJ2
4 , (30)
8
and in the low-temperatures (long-time) regime as
⟨F⟩= −KJ
√
ln 2. (31)
Short time
We look at the short-time behavior ﬁrst (high 1 /β, Eq. (30)). The relevant
computation of the entropy S in [5] involves replacing the number of energy
levels n(E) by its expectation value ⟨n(E)⟩. This is justiﬁed because the
standard deviation of this number is √nand relatively small when ⟨n(E)⟩>
1, which is the interesting regime in Derrida’s case.
For spin glasses, the expectation value of F is interesting, supposedly,
because the system may be self-averaging and can be thought of as an en-
semble of many smaller sub-systems that are essentially independent. The
macroscopic behavior is then given by the expectation value.
Taking expectation values and substituting from Eq. (30) in Eq. (24) we
ﬁnd
⟨gest⟩short = µ−σ2
2 + 1
t
KJ2
4T2 . (32)
From Eq. (25) we know that t= KJ2
2σ2T2 , which we substitute, to ﬁnd
⟨gest⟩short = µ, (33)
which is the correct behavior in the short-time regime.
Long time
Next, we turn to the expression for the long-time regime (low temperature,
Eq. (31)). Again taking expectation values and substituting, this time from
Eq. (31) in Eq. (24), we ﬁnd for long times
⟨gest⟩long = µ−σ2
2 −ln N
t +
√
2 lnN
t σ, (34)
which has the correct long-time asymptotic behavior. The form of the cor-
rection to the time-average growth rate in Eq. (34) is consistent with [10]
and [11], where it was found that approximately N = exp( t) systems are
required for ensemble-average behavior to be observed for a time t, so that
the parameter ln N/t controls which regime dominates – if the parameter is
small, then Eq. (34) indicates that the long-time regime is relevant.
Figure 2 is a direct comparison between the results derived here, based
on [5], and numerical results using the same parameter values as in [10],
namely µ= 0.05,σ =
√
0.2,N = 256 and S = 105.
Notice that ⟨gest⟩is not the (local) time derivative ∂
∂t ⟨ln(⟨x⟩N)⟩, but a
time-average growth rate,
⣨
1
t ln
(
⟨x(t)⟩N
⟨x(0)⟩N
)⟩
. In [10] we used a notation that
9
we’ve stopped using since then because it caused confusion –⟨g⟩there denotes
the growth rate of the expectation value, which is not the expectation value
of the growth rate.
It is remarkable that the expectation value ⟨gest(N,t)⟩so closely reﬂects
the median, q0.5, of ⟨x⟩N, in the sense that
q0.5(⟨x(t)⟩N) ≈exp (⟨gest(N,t)⟩t) . (35)
In [9] it was discussed in detail that gest(1,t) is an ergodic observable for
Eq. (1), in the sense that ⟨gest(1,t)⟩ = lim t→∞ gest. The relationship in
Eq. (35) is far more subtle. The typical behavior of GBM PEAs is com-
plicated outside the limits N →∞ or t→∞, in the sense that growth rates
are time dependent here. This complicated behavior is well represented by
an approximation that uses physical insights into spin glasses. Beautiful!
10
0 100 200 300 400 500
Time
10 1
100
101
102
Typical PEA
gest 256 10, 000
Ito gest 256
median gest 256
Derrida short time
Derrida long time
Figure 2: Lines are obtained by exponentiating the various exponential
growth rates. Blue line:⟨⟨gest⟩256⟩10,000 is the numerical mean (approxima-
tion of the expectation value) over a super-ensemble of S = 10,000 samples
of gest estimated in sub-ensembles of N = 256 GBMs each. Green line:me-
dian in a super-ensemble ofSsamples of gest, each estimated in sub-ensembles
of size N. Yellow line:Eq. (45) is an exact expression for d⟨ln ⟨x⟩N⟩,
derived using Itˆ o calculus. We evaluate the expression by Monte Carlo,
and integrate, ⟨ln ⟨x⟩N⟩=
∫t
0 d⟨ln ⟨x⟩N⟩. Exponentiation yields the yellow
line. Red line: short-time behavior, based on the random energy model,
Eq. (33). Purple line: long-time behavior, based on the random energy
model, Eq. (34). Vertical line:Crossover between the regimes at tc = 2 lnN
σ2 ,
corresponding to βc = 2(ln 2)1/2
J . Parameters: N = 256, S = 10 ,000,
µ= 0.05, σ=
√
0.2.
11
3 Another route via Itˆ o calculus
Another way to ﬁnd the expectation value of PEA growth rates, Eq. (20), is
to compute ⟨dln ⟨x⟩N⟩using Itˆ o calculus. We compute this directly, without
invoking the random energy model. To apply Itˆ o calculus, we will need the
ﬁrst two partial derivatives of dln ⟨x⟩N with respect to xi.
∂ln ⟨x⟩N
∂xi
= 1
N⟨x⟩N
(36)
and
∂2 ln ⟨x⟩N
∂x2
i
= − 1
N2 ⟨x⟩2
N
(37)
Now, Taylor-expanding dln ⟨x⟩N we ﬁnd
dln ⟨x⟩N =
∑
i
∂ln ⟨x⟩N
∂xi
dxi + 1
2
∑
i
∑
j
∂2 ln ⟨x⟩N
∂xi∂xj
dxidxj + ... (38)
≈ 1
N⟨x⟩N
∑
i
dxi − 1
2N2 ⟨x⟩2
N
∑
i
∑
j
dxidxj. (39)
The double-sum can be split into ‘diagonal’ ( i= j) terms and cross-terms as
1
N⟨x⟩N
∑
i
xi(µdt+σdWi)− 1
2N2 ⟨x⟩2
N
(∑
i
dx2
i +
∑
j
∑
i̸=j
dxidxj
)
(40)
Parts of the cross-terms are negligible because they are of order dt2 and the
rest vanishes when taking the expectation value, as we see by writing out
one cross term
dxidxj = xixj(µ2dt2 + µσdtdWi + µσdtdWj + σ2dWidWj). (41)
We therefore drop these terms now, as we take the expectation value, using
the Wiener identity ⟨dW2
i ⟩= dt for the ﬁnal term
⟨dln ⟨x⟩N⟩ =
⟨
µdt− 1
2N2 ⟨x⟩2
N
∑
i
x2
i(µ2dt2 + σ2dt)
⟩
(42)
= µdt−
⟨
1
2σ2 ⟨x2⟩N
N⟨x⟩2
N
⟩
dt+ O(dt2). (43)
12
Discarding terms of higher-than-ﬁrst order in dtand re-writing the last term
as a fraction of sums, we thus have
⟨dln ⟨x⟩N⟩
dt = µ−1
2σ2
⟨
1
N
∑
ix2
i
N
(1
N
∑
ixi
)2
⟩
(44)
= µ−1
2σ2
⟨ ∑
ix2
i
(∑
ixi)2
⟩
(45)
This expression has the correct behaviour: for short times, all xi are essen-
tially identical, and the second term is −1
2 σ2 1
N, which is negligible if N is
large. So, for short times we see expectation-value behaviour. For long times,
the largest xi will dominate both the numerator and the denominator, and
we have −1
2 σ2: the full Itˆ o correction is felt for long times.
4 Discussion
The Itˆ o result is exact. A Monte-Carlo estimate of Eq. (45) (which is easy
to obtain) is shown in Fig. 2 (yellow line). This agrees well with numerical
observations. The approximations from the random energy model have the
right shape and asymptotic behavior, though they’re not on the same scale
as the median PEA. This is, of course, not surprising because these estimates
are not designed to coincide with the median PEA. Quantitatively they are
closer to a higher quantile of the distribution of PEAs. An intriguing question
is this: is our computation using Itˆ o calculus helpful to compute the expected
free energy of the random energy model?
References
[1] A. Adamou and O. Peters. Dynamics of inequality. Signiﬁcance,
13(3):32–35, 2016.
[2] Y. Berman, O. Peters, and A. Adamou. An empirical test of the ergodic
hypothesis: Wealth distributions in the United States. January 2017.
Available at SSRN: https://ssrn.com/abstract=2794830.
[3] J.-P. Bouchaud. On growth-optimal tax rates and the issue of wealth
inequalities. http://arXiv.org/abs/1508.00275, August 2015.
[4] J.-P. Bouchaud and M. M´ ezard. Wealth condensation in a simple model
of economy. Physica A, 282(4):536–545, 2000.
13
[5] B. Derrida. Random-energy model: Limit of a family of disordered
models. Phys. Rev. Lett., 45(2):79–82, July 1980.
[6] B. Derrida. Random-energy model: An exactly solvable model of disor-
dered systems. Phys. Rev. B , 24:2613–2626, September 1981.
[7] H. Morowitz. Beginnings of cellular life . Yale University Press, 1992.
[8] O. Peters and A. Adamou. The evolutionary advantage of cooperation.
arXiv:1506.03414, June 2015.
[9] O. Peters and M. Gell-Mann. Evaluating gambles using dynamics.
Chaos, 26:23103, February 2016.
[10] O. Peters and W. Klein. Ergodicity breaking in geometric Brownian
motion. Phys. Rev. Lett., 110(10):100603, March 2013.
[11] S. Redner. Random multiplicative processes: An elementary tutorial.
Am. J. Phys. , 58(3):267–273, March 1990.
14