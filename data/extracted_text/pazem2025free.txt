ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 1 ‚Äî #1
PLOS ONE
OPEN ACCESS
Citation: PazemJ,KrummM,ViningAQ,
FidererLJ,BriegelHJ(2025)FreeEnergy
ProjectiveSimulation(FEPS):Activeinference
withinterpretability.PLoSOne20(9):
e0331047.https://doi.org/10.1371/journal.
pone.0331047
Editor: DennisSalahub,UniversityofCalgary,
CANADA
Received: January23,2025
Accepted: August8,2025
Published: September4,2025
Copyright: ¬© 2025 Pazemetal.Thisisanopen
accessarticledistributedunderthetermsofthe
CreativeCommonsAttributionLicense ,which
permitsunrestricteduse,distribution,and
reproductioninanymedium,providedthe
originalauthorandsourcearecredited.
Data availability statement: Thecodeanddata
supportingthisstudyareaccessibleat
https://doi.org/10.5281/zenodo.15526876.
Funding: Thisresearchwasfundedinwhole,or
inpart,bytheAustrianScienceFund(FWF)DOI
10.55776/F71and10.55776/WIT9503323.For
thepurposeofopenaccess,theauthorhas
appliedaCCBYpubliccopyrightlicensetoany
AuthorAcceptedManuscriptversionarising
fromthissubmission.Wegratefully
acknowledgesupportfromtheEuropeanUnion
(ERCAdvancedGrant,QuantAI,
RESEARCHARTICLE
Free Energy Projective Simulation (FEPS):
Active inference with interpretability
Jos√©phine Pazem
 
 
1‚àó, Marius Krumm1, Alexander Q. Vining1,2,
Lukas J. Fiderer
 
 
1, Hans J. Briegel1,3
1 Institutf√ºr Theoretische Physik, Universit√§t Innsbruck, Innsbruck,Austria,2 Departmentfor the Ecology
ofAnimal Societies, Max Planck Institute ofAnimal Behavior,Konstanz, Germany,3 Departmentof
Philosophy,Universityof Konstanz, Konstanz, Germany
‚àó josephine.pazem@uibk.ac.at
Abstract
Inthe last decade, the free energyprinciple (FEP) and active inference (AIF)have
achievedmany successes connecting conceptual models oflearning and cognition to
mathematicalmodels of perception and action. Thiseffortisdriven by a multidisciplinary
interestin understanding aspects of self-organizing complexadaptive systems, including
elementsof agency.Variousreinforcement learning (RL) models performing activeinfer-
encehave been proposed and trainedonstandard RL tasks using deepneuralnetworks.
Recentwork has focused on improving suchagents‚Äô performance in complex environ-
mentsby incorporating the latest machine learningtechniques. In this paper,we build
uponthese techniques. Within the constraints imposedby the FEP and AIF,we attempt
tomodel agents in an interpretable waywithout deep neural networks by introducing
FreeEnergy Projective Simulation (FEPS). Using internalrewards only,FEPS agents
builda representation of their partially observableenvironments with which they inter-
act.Following AIF,the policy to achieve agiven task is derived from thisworld model by
minimizingthe expected free energy.Leveraging the interpretabilityof the model, tech-
niquesare introduced to deal with long-termgoals and reduce prediction errors caused
byerroneous hidden state estimation. Wetest the FEPSmodel on two RL environments
inspiredfrom behavioral biology: a timed responsetask and a navigation task ina par-
tiallyobservable grid. Our results show thatFEPS agents fully resolve the ambiguityof
bothenvironments by appropriately contextualizing their observationsbased on predic-
tionaccuracy only.In addition, they infer optimalpolicies flexibly for any target observa-
tionin the environment.
Introduction
A key challenge in both cognitive science and artificial intelligence is understanding cognitive
processes in biological systems and applying this knowledge to the development of artificial
agents. In this work, we develop an interpretable artificial agent which integrates key aspects
of both reinforcement learning (RL) [1] and active inference [2‚Äì6]. In RL, an external reward
signal is typically used to guide an agent‚Äôs behavior while in active inference no such reward
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 1/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 2 ‚Äî #2
PLOS One FreeEnergy Projective Simulation (FEPS)
signal exists. Instead, agents follow an intrinsic motivation which is rooted in the free energyNo.101055129).Theviewsandopinions
expressedinthisarticlearehoweverthoseof
theauthor(s)onlyanddonotnecessarilyreflect
thoseoftheEuropeanUnionortheEuropean
ResearchCouncil-neithertheEuropeanUnion
northegrantingauthoritycanbeheld
responsibleforthem.Thefundershadnorolein
studydesign,datacollectionandanalysis,
decisiontopublish,orpreparationofthe
manuscript.
Competing interests: Theauthorshave
declaredthatnocompetinginterestsexist.
principle (FEP) [7,8].
Central to the FEP is the idea that adaptive systems can be modeled as performing an
approximate form of Bayesian inference. The outcome of this process minimizes a quantity,
called variational free energy (VFE), which lends its name to the FEP . The FEP encompasses
various paradigms that align with the goal of Bayesian inference, such as predictive processing
[9] and the Bayesian brain hypothesis [10].
One way to implement the FEP is through active inference which involves a planning
method for action selection based on an internal model of the environment, referred to as the
world model, along with a preference distribution, which encodes the agent‚Äôs desired states
(e.g., maintaining a certain body temperature). According to the FEP , it is‚Äîin principle‚Äî
always possible to explain the observed behavior of living systems through active inference
[11]. The Free Energy Principle has been applied to a wide range of domains, including neu-
roscience [11‚Äì13], psychology [14,15], behavior biology [4,16,17], and machine learning
[7,18‚Äì21].
Active inference has recently gained traction in theoretical discussions [5,22‚Äì25] leading
to several successful algorithmic implementations [6,7,14,18‚Äì21,26,27]. Existing implementa-
tions are based on methods developed in the context of model-based reinforcement learning
and rely mostly on neural networks to represent the world model [6,7,14,18,21,26,28,29]. As
an alternative to neural networks, this work proposes implementing active inference using
Projective Simulation (PS) [30].
In PS, memory is organized as a directed graph, which‚Äîunlike neural networks‚Äîis
designed to be interpretable such that individual vertices carry semantic information [31,32].
The agent‚Äôs cognitive processes are modelled as (1) a random walk through the graph (deliber-
ation), and (2) the updating of transition probabilities along the graph‚Äôs edges (learning). The
primary motivation for using PS over neural networks is its inherent interpretability.
While so far PS has mostly been employed in the model-free RL framework, this work
extends PS to the active inference framework by proposing and testing an agent model called
Free Energy Projective Simulation (FEPS). FEPS is an active inference model which uses PS
for both the world model and the action policy, combining existing and novel methods for
training these components. Key features of FEPS include an internal reward signal derived
from the prediction accuracy, and the use of two distinct preference distributions‚Äîone for
learning the world model and another for achieving a specific goal. For the latter purpose, we
propose new heuristics to estimate the long-term value of belief states in sight of the goal.
In the following sections, we first introduce active inference (Sect 2) and PS (Sect 3), before
presenting the architecture (Sect 4) and algorithmic features (Sect 5) of the FEPS model. We
then test FEPS through numerical simulations in biologically inspired environments, focusing
on timed responses and navigation (Sect 6). Finally, we conclude with a discussion of future
research directions (Sect 7).
1 Related work
FEPS is a learning model that performs an approximate form of Bayesian inference, guided
by internal rewards generated by the model itself. It thus falls within the broad category of
reinforcement learning models: agents that interact with their environment to acquire data
and select actions aimed at maximizing a utility function. More specifically, FEPS agents
build uponmodel-based reinforcement learning (MBRL) since they are equipped with a world
model that they use to plan actions. Using an active inference approach to interact with and
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 2/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 3 ‚Äî #3
PLOS One FreeEnergy Projective Simulation (FEPS)
learn about the environment, FEPS is also intended to model some cognitive processes involv-
ing cognitive maps to support adaptive behavior, drawing on existing techniques in this field.
In the remainder of this section, we elaborate on how our model relates to the aforementioned
methods and what it contributes beyond them.
Model-based Reinforcement learning.agents use a model of the environment to make
predictions about outcomes of actions and the resulting expected utility. This not only helps
agents to learn from sparse rewards [33], but also makes the training more sample efficient
by using the world model as a generative model [34‚Äì38]. In particular, some successful
approaches such as Dreamer encode sensory inputs into latent space, while a recurrent neu-
ral network captures the dynamics in latent space [37,39,40]. In spite of their almost human-
like performance in some complex environments [39,40], the world models they learn remain
inaccessible to users.Interpretabilityis therefore a key challenge in order to use MBRL mod-
els to reason about cognitive processes underlying and using cognitive maps. While FEPS also
encodes observations into latent space and simulates dynamics on the latter, it remains inter-
pretable in three ways: 1) the world model, encoded on a graph-structured memory, is read-
able by users, 2) the agent‚Äôs decision-making process, modeled as a random walk on a graph,
is traceable, and 3) the learning process amounts to strengthening associations between nodes
on the memory.
RL with internal rewards:Finding efficient exploration strategies constitutes an essential
problem in RL [41]. Curiosity mechanisms have been proposed [42], which often rely on a
world model. In particular, the policy can be chosen to maximize both internal reward signals
emitted by the agent itself, and external goal-oriented rewards distributed by the environment
[43,44]. Popular formulations of internal rewards include information-theoretical quantities,
such as prediction errors [45], epistemic uncertainty [38], empowerment [46] or equivalently
[47], mutual information [48,49]. FEPS agents require no external rewards from the environ-
ment: thanks to active inference, their actions are internally motivated by information gain,
ensuring exploration, and rewards are generated internally from the accuracy of predictions to
reinforce relevant transitions in the world model.
Learning with active inference:Leveraging the free energy as a learning signal‚Äìsuch as
a loss function [50] or a reward [51]‚Äì for RL agents with active inference is advantageous in
that it establishes a trade-off between exploration and exploitation, and allows for two types
of exploration: goal-directed and epistemic [7,52]. Internal motivation in active inference
is rooted in a preference prior that describes the desirability of states, akin to a utility func-
tion in RL, and that can be either given or learned from experts [51], by predicting rewards
[53], or calculated iteratively from experience [18,20,54]. Most RL applications of active infer-
ence model the agents‚Äô distributions with deep neural networks [7,19,20,50,55‚Äì61], thereby
inheriting their lack of interpretability. The contribution of FEPS is threefold. 1) We provide
an iterative heuristics to calculate the preference prior from the world model, allowing the
agent to adapt rapidly to target changes. 2) FEPS is interpretable. 3) Free energy is not used
as a learning signal but it instead integrates world model and preferences to plan for actions,
such that the agent‚Äôs representation of the environment directly influences its behavior.
Cognitive maps: Cognitive maps [62] are mental representations of spatial and abstract
spaces [63,64], learned from experience and supported by episodic memory [65‚Äì67]. Clone-
structured cognitive graphs (CSCG) [68‚Äì72] have recently emerged as biologically-plausible
candidates to model cognitive maps: clones of sensory signals, organized on a graph, are
allocated to specific paths or contexts in the partially observable environment, thanks to the
sequences of observations collected. FEPS makes use of CSCG to structure the memory of the
agent and performs Bayesian inference to formulate beliefs about hidden states, as in [72,73]
for example. Relying on CSCG, FEPS uses active inference to add a model of behavior based
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 3/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 4 ‚Äî #4
PLOS One FreeEnergy Projective Simulation (FEPS)
on the cognitive map of the agent. Instead of the expectation-maximization algorithm, the
reinforcement of associations between successive states update the cognitive map of the agent.
Note that there are also parallels between energy-based techniques, such as Boltzmann
machines and simulated annealing, and aspects of FEPS. Both use physics-inspired loss func-
tions like the variational free energy to approximate complex probability distributions on
graphs. However, the Free Energy Principle has a broader aim: to describe complex adaptive
systems. In this framework, the minimization of the VFE is a tool to measure adaptation in
terms of uncertainty reduction, and it remains agnostic of any method to implement it. In
FEPS, the FEP further constrains the agent‚Äôs architecture and its interactions with external
degrees of freedom. A final conceptual distinction is that, unlike energy-based graph tech-
niques, the vertices of the graph are treated as fragments of the agent‚Äôs experiences in FEPS.
The graph thus represents the agent‚Äôs memory, which is used to guide decisions.
2 Active inference
The following presentation of active inference is divided into two parts. First, we outline how
the world model is constructed and updated in response to sensations (perceptual inference).
Next, we explain how planning and action selection are modeled (active inference).
During perceptual inference, through interactions with the environment, the agent
develops the world model [37,38], sometimes referred to as a generative model. The world
model [6,11] is described as a partially observable Markov decision process (POMDP)
(B, S, A, T, L, R), with belief statesB, observationsS, and actionsA. Thetransition function T
assigns probabilities to transitions between belief states given some action, whileL, sometimes
referred to as theemission function, defines the probability that some belief stateb ‚àà B emits a
sensory signals ‚àà S. While sensory states are observed and shared between the world model
and the environment, belief states are part of the world model only and support the encoding
of generally different unobservable hidden states in the environment.
In what follows, the time-indexed random variablesBt, St, andAt represent beliefs, obser-
vations, and actions at a given timet, with their possible values denoted by the corresponding
lower-case letters. The transition functionT between timest‚Äì1 andt corresponds to the con-
ditional distributionp(Bt|Bt‚Äì1, At‚Äì1), while the emission functionL at timet is the likelihood
p(St|Bt). Finally, rewardsR are used to learn the transition and emission functions. In this
work, rewards are internal (generated by the agent itself) [43] but in general they could also
be external (given by the environment). It is further assumed that actionAt is selected based
on the current belief stateBt, as represented by the conditional distributionùúã(At|Bt), which
defines the policy. Combining these elements, the joint distribution of the world model up to
time t can be decomposed as follows:
p(B0‚à∂t, A0‚à∂t‚Äì1, S0‚à∂t) = p(B0, S0)
t
‚àè
t‚Ä≤=1
ùúã(At‚Ä≤‚Äì1|Bt‚Ä≤‚Äì1) p(St‚Ä≤ |Bt‚Ä≤ ) p(Bt‚Ä≤ |Bt‚Ä≤‚Äì1, At‚Ä≤‚Äì1), (1)
where the index notationB0‚à∂t = (B0, B1, ‚Ä¶, Bt) is used for sequences of random variables, and
p(B0, S0) denotes the initial distribution over beliefs and observations.Eq (1)defines the
relations between random variables: while the distribution over observation and an action
is specified by the current belief state, both the previous belief state and the previous action
determine the current belief state.
The transition functionp(Bt|Bt‚Äì1, At‚Äì1) plays a crucial role in active inference because
it has to be learned by the agent. Following the idea of active inference, learning involves
updating p(Bt|Bt‚Äì1, At‚Äì1) through variational inference, an approximate version of Bayesian
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 4/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 5 ‚Äî #5
PLOS One FreeEnergy Projective Simulation (FEPS)
inference. Variational inference simplifies Bayesian inference by restricting the set of possible
posterior transition functions to a family{Qùúô(Bt|Bt‚Äì1, At‚Äì1)}ùúô, parametrized by some variable
ùúô, thus reducing computational complexity. When a new observationsenv
t is received from the
environment, the approximate posterior is obtained as the solution to an optimization prob-
lem which minimizes the VFE, here conditioned on the specific valuesbt‚Äì1 and at‚Äì1 for the
random variablesBt‚Äì1 and At‚Äì1 in the previous step:
F = ùîªKL [qùúô(Bt|bt‚Äì1, at‚Äì1)||p(Bt|bt‚Äì1, at‚Äì1)] + ùîºbt‚àºqùúô(Bt|bt‚Äì1,at‚Äì1) [‚Äì log(p(senv
t |bt)] , (2)
where ùîºbt‚àºqùúô(Bt|bt‚Äì1,at‚Äì1)[f(bt)] = ‚àëbt qùúô(bt|bt‚Äì1, at‚Äì1) f(bt) represents the expectation value
over belief states distributed according to the posterior distribution of some functionf of bt.
Minimizing the VFE balances two effects: while the first term penalizes drastic changes in
the distribution, the second promotes accuracy in the model to predict observations com-
ing from the environment. The Kullback-Leibler distanceùîªKL(‚óè) represents the dissimilar-
ity between the posterior and prior distributions for the transition function,qùúô(Bt|bt‚Äì1, at‚Äì1)
and p(Bt|bt‚Äì1, at‚Äì1) respectively. The second term is the surprise of the current observation.
In order to calculate this surprise, an expectation value is calculated from the posterior distri-
bution qùúô(Bt|bt‚Äì1, at‚Äì1) over belief states. Realizing that the first term is either larger or equal
to zero, the VFE is an upper bound on the surprise raised by the current input from the envi-
ronment. By updating its model and minimizing the variational free energy, the agent there-
fore minimizes its surprise. Typically, the prior is replaced by the posterior after collecting a
number of observations and implementing the corresponding updates on the posterior.
So far, the model acquisition has been described in terms of perceptual inference: the sys-
tem has no control over its actions yet, and constructs the internal model based solely on its
observations. The reduction of the free energy describes the adaptation of the system to its
environment. The distribution over actions, or policyùúã(At|Bt), can take any form and is not
yet optimized over.
A FEPS agent performs active inference [2] by exploiting the world model that has been
learned through perceptual inference in order to plan sequences of future actions. Using the
current world model, the agent estimates the future free energy for each of its actions based
on the current transition function and implements the one action that minimizes it. This esti-
mate of the free energy for future states does not have a unique form [74] and different formu-
lations can lead to different trade-offs between exploration and exploitation. Here, we use the
most common one, denoted in the literature as theExpected Free Energy(EFE):
Gbt‚Äì1 [a] = ùîºbt,st‚àºp(Bt,St|bt‚Äì1,a)[log p(bt|bt‚Äì1, a) ‚Äì log pref(st, bt|bt‚Äì1, a)], (3)
= ‚ÄìH[Bt|bt‚Äì1, a] + ùîºst,bt‚àºp(St,Bt|bt‚Äì1,a)[Spref(st, bt|bt‚Äì1, a)] (4)
where p(Bt, St|bt‚Äì1, a) refers to the world model over a single time step, and the surprise of
getting outcomesst and bt is denotedSpref(st, bt|bt‚Äì1, a) = ‚Äì log pref(st, bt|bt‚Äì1, a). H(Y|x) =
‚Äì ‚àëy p(Y = y|X = x) log p(Y = y|X = x) stands for the conditional entropy of the random vari-
able Y conditioned on a specific valueX = x. It corresponds to the expected surprise over
belief states and is therefore always positive. pref(St, Bt|bt‚Äì1, a) is a distribution over belief
states and observations.
The EFE relies on two measures to determine actions: how much uncertainty is expected,
and how useful is the action in order to fulfill a goal. In an active inference setting, an action‚Äôs
utility refers to its expected capacity to meet some preferences. The first term inEq (4)is the
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 5/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 6 ‚Äî #6
PLOS One FreeEnergy Projective Simulation (FEPS)
negative entropy over belief states, related to the expected information gain about the transi-
tion function: the larger the entropy, the larger the gain. Maximizing this entropy minimizes
the expected free energy and leads to explorative behaviors [74]. Finite entropy can have two
causes: (1) the transition to the next state is still uncertain in the world model, or (2) the tran-
sition to the next state in the environment is stochastic. In the second case, if entropy were
used alone, agents could fall in the so-called curiosity trap [45] by always choosing actions
that lead to random outcomes in the environment and inherently result in high entropy val-
ues. The second term reflects the utility gained from taking the action under consideration.
When minimized, the transitions are expected to match the preferences to a greater extent.
Therefore, minimizing the EFE maximizes the utility an agent expects from its action. For this
purpose, a new biased distribution, the preference distribution pref(St, Bt|bt‚Äì1, a), encodes the
desirability of some states and observations. The larger the preference for a state, the larger the
probability in the preference distribution, and the smaller the associated surprise. This second
term encourages exploitation of the world model in order to fulfill the preferences. In a bio-
logical agent, these preferences could be of genetic origin (a preference for homeostatic states
for example), socially learned (a preference for some type of songs for birds of certain areas,
that would be different for the same species in a different place), acquired or externally given.
As a result, they can encode a set of states and observations that are favorable to the survival
of the agent. As a modeling choice, preferences cover both belief states and observations, and
are conditioned on the previous state and action.
For the purpose of learning with an artificial agent, the preferences can naturally encode a
task to fulfill as a preferred target state or observation. Choosing the action that minimizes the
EFE, the agent selects the transition in its world model that will bring it as close as possible to
its preferred states according to its world model. Furthermore, by using the full world model
over long periods of time, say from 0 tot, as inEq (1), the long-term EFE can be calculated to
project t steps ahead in the world model and plan for sequences oft actions. Finding the opti-
mal sequence of actions can rely on tree search with pruning [75], or calculating a long-term,
discounted, expected free energy quantity [28,54,76] for example.
To summarize, according to the free energy principle, an agent that adapts to its sur-
roundings can be modeled as learning a world model of its environment. In this process, the
variational free energy is minimized, reflecting the reduction in the surprise the agent experi-
ences about new sensory events. Active inference prescribes a method to plan actions in order
to reduce this surprise. In other words, actions are chosen to consolidate the world model.
An action is chosen if it minimizes the expected free energy, or equivalently, if it is expected
to lead to transitions associated with high uncertainty and high utility. As noticed already
in [74], this can be counter-intuitive at first glance, since minimizing the VFE and the EFE
respectively minimizes and maximizes uncertainty. This paradox can be resolved by realiz-
ing that in active inference, actions target areas in the world model whose outcomes are least
predictable in order learn about them and to receive less surprising outcomes in the future,
thereby minimizing the VFE in the long run.
3 Projective simulation
Projective Simulation (PS) [30] is a model for embodied reinforcement learning and agency
inspired from physics that performs associative learning on a memory encoded as a graph.
It is composed of a network of vertices, denoted clips, with a defined structure that gives
each clip a semantic meaning, that can be assigned from the start or acquired progressively
through past experiences. For example, a clip may represent a sensory state the agent‚Äôs sen-
sors are capable of receiving, or it can inherit supplementary semantics from past experiences
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 6/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 7 ‚Äî #7
PLOS One FreeEnergy Projective Simulation (FEPS)
and reinforcements, through associations to other clips in the graph memory. Directed edges
between clips are weighted and can be modified dynamically to learn and adapt to the envi-
ronment. In particular, PS allows the simulation of percept-action loops to represent previous
interactions with the environment and the associated decision process. The resulting graph
is the Episodic and Compositional Memory (ECM). When a clip is visited, either because it
is currently experienced, or because it is used for deliberation, it is excited. After a first clip
was excited, deliberation takes place as a traceable random walk in the ECM originating from
this initial clip. It ends when a decoupling criterion is met, which leads to an action on the
environment.
For the purpose of solving different environments while retaining interpretability, the
ECM can adopt different structures. In order to imitate a percept-action loop, the ECMs
are often structured as bipartite graphs, where a first layer contains percepts and the second
is composed of actions [77,78]. In this case, the trained ECM is directly relatable to a pol-
icy. For more complex environments, or in order to extract abstract concepts from the per-
cepts, an intermediate layer can be added between the percept and action layers [31,79]. The
ECM can either contain a fixed number of clips, or it can dynamically add and erase some of
them when needed [79]. To consider composite percepts and actions, the ECM graph can be
replaced by a hypergraph, where each hyperedge connects a set of clips to another set [80].
Each directed edge is equipped with at least one attribute to track learning and allow the
agent to react adaptively to the environment.h-values increase as the edges are rewarded.
They encode the strength of associations between clips that are useful to fulfill some task and
record the learning of the agent. The probability of a transition associated with an edge with
h-value hij connecting two clips,ci ‚Üí cj, is inferred from theh-values:
p(cj|ci) = hij
‚àëk hik
. (5)
Alternatively, a softmax function can also be implemented to enhance the differences
between probabilities, especially in large ECMs. Upon receiving a new percept, the agent
deliberates by taking a random walk through the ECM, using the probabilities defined from
the h-values to sample a new edge.
As the agent modeled with PS interacts with its environment, it receives rewards that are
distributed over the different edges, which changes the correspondingh-values. Specifically,
the h-value of the edgeci ‚Üí cj is updated as follows:
ht+1
ij = ht
ij ‚Äì ùõæ(ht
ij ‚Äì h0
ij) + R (6)
where ùõæ is the forgetting parameter,h0
ij is the initialh-value forci ‚Üí cj and R is the reward.
When the reward is positive, theh-value of the corresponding edge is increased accordingly.
If an edge is not visited or does not receive a positive reward, the correspondingh-value
decreases back to their initial value thanks to the forgetting mechanism in the second term.
In order to accept continuous percepts and to unlock generalization on some task, neural net-
works have been used to update theh-values in some cases. Training was then implemented
by minimizing a loss function [81].
Projective simulation has been tested in multiple tasks, ranging from the standard RL toy
environments [77,78], robotics [82], to animal behavior simulations [83,84] and quantum
computations [85,86]. Extension of the model include modeling the ECM with a quantum
photonic circuit [32] and considering composite concepts in the form of multiple joint excited
clips using hypergraphs [80].
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 7/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 8 ‚Äî #8
PLOS One FreeEnergy Projective Simulation (FEPS)
4 The free energy projective simulation agent
We combine Projective Simulation, with Active Inference, following the free energy prin-
ciple‚Äôs framework. A FEPS agent is a model-based Projective Simulation agent, where the
world model is an ECM with a clone-structured architecture [68,70]. Consequently, clone
clips inherit the semantics of the unique sensory state they relate to, and context creates dis-
tinctions between hidden states that emit the same observations. As in the FEP , the agent does
not need external rewards. Instead, prediction accuracy, weighted with confidence, is used
as a reinforcement signal. The world model is directly exploited by the agent to set the edges‚Äô
h-values in the policy with active inference.
4.1 Architecture of the agent
To mimic a system described by the FEP , the agent is composed of two structures: the world
model and the policy, each represented by separate graphs with vertices corresponding to ran-
dom variables, and edges that can be sampled to perform a random walk, as inFig 1. Con-
sider the agent can perceiveNS sensory states, hasNB possible belief states and a repertoire
of NA actions. Each state is supported by one vertex in a graph. The world model‚Äôs vertices
can either support belief states or sensory states. The world model is the representation of
the environment (seeEq 1) required by active inference. For reasons that will become clear
shortly, the ECM of a FEPS agent is made of all vertices, that we call clips, that support belief
states, and edges that represent transitions between such clips. A belief state is then formally
defined by the excitation configuration of clips at one step of the deliberation. We limit the
number of excitations in the ECM at any given time to one. In this case, the excitation con-
figuration on the clips is analogous, from a Bayesian perspective, to a belief‚Äì a probability
distribution‚Äì over clips, where a single clip in the distribution is associated with a probability
of 1. Such a vertex deserves the name ‚Äù clip‚Äù because it receives an excitation when the corre-
sponding representation of an event is revisited in order to make a decision. The policy covers
all possible conditioned responses coming from any belief state, given the repertoire of actions
of the agent.
In the world model, two sets of edges can be traversed at different times during the deliber-
ation: we denote them emission and transition edges respectively. They aim at predicting and
explaining sensory signals received from the environment.
In the first set, emission edgesb ‚Üí s relate belief states and sensory states, modeling the lat-
ter as parts of larger, possibly contextualized, hidden states by using clone-structured Hidden
Markov Models (HMM) [68‚Äì70]. Each clip is bound to a single observation and is denoted
a clone clip. A single edge in the emission set carries a non-zero probability for each clip, as
shown inFig 1a. Consequently, this set of edges defines a deterministic likelihoodp(st|bt) =
ùõøst,s(bt) in the world model inEq (1)and the agent remains initially agnostic of the dynam-
ical structure of the hidden states{eùúè}t
ùúè=0 in the environment. Meanwhile, a clone clip can
readily be interpreted as an augmented description of a specific observation. In particular,
the additional information can relate to the cause or the context of the observation, such as
the previous belief state and previous action, for example. Sampling some sensory stateÃÇst+1
amounts to predicting the next observation: if it turns out to coincide with the actual obser-
vation perceived from the environment, a reward will be distributed to the edges contributing
to the random walk that led to this prediction. For clarity, we denote predicted states with hat-
ted low case letters. We choose to associate each observation to a fixed numberNclones of clone
clips, such thatNB = NS √óNclones. This approach works remarkably well for navigation prob-
lems [69,70]. Transferring the dynamics learned on some set of sensory states to another set
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 8/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 9 ‚Äî #9
PLOS One FreeEnergy Projective Simulation (FEPS)
Fig 1.Architecture and training of an FEPS agent.a) Architecture of a FEPS agent, with four sensory states (squares) and
two possible actions (diamonds). The agent has two main components: the world model and the policy. The world model
is composed of vertices representing observations (squares) while clone clips represent all values a belief state can take
(circles). As in a clone-structured graph, each clone clipb relates to exactly one observations and the emission function
p(s|b) is deterministic. The clone clips, together with the set of edges between them, form an ECM. A belief state, circled in
purple, is designated by an excited clone clip. The weighted edges in the ECM encode the transition function and are train-
able with reinforcement: there is one set of edges per action (light and dark turquoise arrows). The belief state in the ECM
is an input to the policy, where the probability of sampling an action is a function of the EFE. In turn, the action that was
selected determines the edge set to sample from in the world model in order to make a prediction for the next belief state
and observation. b) Training of the world model of a FEPS agent. The agent interacts with the environment by receiving
observations and implementing actions. When an actionat is chosen, a corresponding edgebt
at/leftrightline‚Üí bt+1 is sampled in the
world model, from the current to the next belief state, conditioned on the action. The observationst+1 associated with the
next belief state is the prediction for the next sensory state. Simultaneously, the action is applied to the environment and
creates a transition in the hidden states of the environment,et
at/leftrightline‚Üí et+1 (bottom, green rectangle). This transition is perceived
by the agent through the observationsenv
t+1. Finally, the weights of the edges are updated. The reinforcement of an edge is
proportional to the number of correct predictions it enabled in a row, as depicted with the thickness of the arrows in the
world model. When the agent makes an incorrect prediction (the purple arrow), the reinforcements are applied to the edges
that contributed to the trajectory. The last, incorrect, edge is not reinforced.
https://doi.org/10.1371/journal.pone.0331047.g001
is also possible by keeping the transition function unchanged, but redistributing clone clips to
the sensory states in the new set [70].
The purpose of the set of transition edgesbt
at
/leftrightline‚Üí bt+1 in the world model is to encode the
presumed dynamics in the environment as transitions between belief states, conditioned on
actions. They are represented as edges between clone clips inFig 1a. In contrast to other sets
of edges, transition edges are endorsed with attributes such ash-values that enable learning
with reinforcements (as inEq 6for example). Therefore, clone clips together with transition
edges constitute an ECM for the FEPS agent. From a given clone clip, for each action, a set of
edges points to the next possible estimated belief states. Theh-values of those edges indicate
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 9/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 10 ‚Äî #10
PLOS One FreeEnergy Projective Simulation (FEPS)
how certain the agent is that taking a particular action from the current belief state will lead to
any of the clone clips in the future. There can be at mostNB edges in each such group of tran-
sition edges. The full set of transition edges ‚ÄìNA ‚ãÖ(NS ‚ãÖNclones)2 edges in total ‚Äì defines the
transition functionp(Bt+1|Bt, At) in the world model inEq (1)and corresponds to the trained
part of the model. Before reinforcement, this distribution is referred to as the prior. The poste-
rior is the updated version of the transition function, after the agent distributed rewards to the
relevant transition edges. The posterior is labeledq(Bt+1|Bt, At).
The final component of the agent guides its behavior. The policyùúã(At|Bt) is modeled as
a separate graph with two layers ofNB and NA clips respectively. Given the clone clip corre-
sponding to belief statebt is excited in the world model, an actionat is sampled to be applied
to the environment, based on how much surprise it expects from this decision. Each edge
bt ‚Üí at is weighted with the expected free energyGbt [at], that defines the policy.
4.2 Reinforcement with prediction accuracy
Each interaction step with the environment involves a deliberation over three states: (1) the
next belief state is proposed, (2) the next sensory state is predicted and (3) an action is chosen.
The agent excites a belief statebt+1 it believes it will transition to, given its current actionat, by
sampling a transition edgebt
at
/leftrightline‚Üí bt+1. From there, the agent makes a predictionÃÇst+1 about the
next sensory state. Meanwhile, an actionat+1 is selected in the policy and it is applied to the
hidden state in the environment, that emits a new observation. The interaction step ends by
comparing the predicted and perceived sensory states,ÃÇst+1 and senv
t+1.
The world model is trained without external rewards, and reinforcement is instead based
on matching predictions and observations. We calltrajectory a sequence of transitions that
led to correct predictions about sensory states. To record the trajectory, transition edges are
equipped with a new attribute: theconfidence, f. Initialized at zero, it increases for all transi-
tions in the trajectory every time the predictionÃÇst+1 and the actual sensory statesenv
t+1 coincide.
The more subsequent predictions an edge enabled, the higher the confidence for that edge:
it reflects the number of correct predictions the edge enabled until the end of the trajectory.
Formally, a trajectoryùúè is a sequence of transitions whose predictions were confirmed by the
observations from the environment. If at stepn in thet-th trajectoryùúèt the sensory prediction
was accurate, confidence is enhanced for all edgesi ‚Üí j in ùúèt:
f(ùúèt),n
ij =
‚éß‚é™‚é™‚é®‚é™‚é™‚é©
f(ùúèt),n‚Äì1
ij + 1 if bi ‚Üí bj ‚àà ùúèt
0 otherwise.
(7)
When the prediction and observation do not match, the trajectoryùúèt is interrupted, and
the rewards are distributed to the transition edges‚Äôh-values proportionally to the correspond-
ing confidence:
ht
ij = ht‚Äì1
ij ‚Äì ùõæ(ht‚Äì1
ij ‚Äì h0
ij) + f(ùúèt)
ij R (8)
where ht‚Äì1
ij is theh-value at the end of the previous trajectory,h0
ij the initialh-value of the
edge, andR scales the reinforcement of the edges. Confidence values are reinitialized at
zero to start the next trajectory. This mechanism provides a built-in learning schedule such
that the scale of the reinforcement signals grows progressively: rewards are initially small
when trajectories are short, and they become larger when transitions are accurately cap-
tured in the model. For the world model to yield accurate predictions, at most each of the
NA ‚ãÖ(NS ‚ãÖNclones)2 transition edges must have been visited and reinforced appropriately. As a
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 10/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 11 ‚Äî #11
PLOS One FreeEnergy Projective Simulation (FEPS)
result, the computational cost of learning scales at least quadratically in the number of obser-
vations and clones per observation. During the deliberations, states are sampled from the
prior ECM that did not receive the rewards yet, while the posterior ECM is updated with con-
fidence and rewards at the end of the trajectory. It is equivalent to sampling states from the
prior ECM, but updating the posterior ECM with the rewardsR for all edges in the trajectory
every time a prediction was verified by the observation in the environment. Metaphorically
speaking, this mechanism is analogous to layers of snow accumulating in time on salient fea-
tures. At the end of a trajectory, the snow is cleared away, bringing all salient points back at an
equal level.
To complete the update of the FEPS agent, the policy is modified according to the EFE
inferred from the new world model and can be adjusted to make behaviors more or less
explorative. In particular,h-values of an edgebi ‚Üí aj are set to the expected free energy in
Eq (4)with a world model conditioned onbi and aj. Eachh-value directly carries the sur-
prise expected from traversing the corresponding edge. As in [6], the policy is defined using
a softmax function:
ùúã(aj|bi) = softmax(ùúÅ Gbi [aj]). (9)
where ùúÅ is a (real-valued) scaling parameter andGbi [aj] is the value of the EFE for actionaj
coming from statebi. In active inference,ùúÅ is typically negative. When it becomes more neg-
ative, actions associated with small EFE receive a large probability. More specifically, look-
ing at the decomposition of the EFE inEq (4), actions associated with larger entropies, that
is lower certainty, together with higher chances of landing on preferable states or observa-
tions, become more attractive during the deliberation. In contrast, if the scaling parameter is
positive, large EFE yield large probabilities in the policy, and actions with high certainty but
also less chances of meeting preferences are more likely to be sampled. In this case, the agent
implements a non-explorative policy that is confined to a known region of the environment at
the expense of reaching the preferred states. WhenùúÅ = 0, the policy is uniform, and the agent
has no bias towards certainty nor utility.
5 Algorithmic features of the FEPS
The FEPS can be augmented with a number of techniques that take advantage of the world
model and its interpretability. During learning, the internal model can be leveraged to iden-
tify transitions that are instrumental to gain information or to get closer to a preferred state.
Furthermore, the performance of an agent in completing a task can be enhanced by evaluat-
ing the correct belief state accurately and quickly. Since the policy depends on the EFE, the
preference distribution can be tuned according to the task: to seek information to complete
the model, or to complete a given goal. Therefore, we propose to separate training into two
phases, depending on how the preference distribution is constructed. We introduce a belief
state estimation scheme that distributes belief states over multiple clone clips and eliminates
those that are incompatible with new observations.
5.1 Leveraging preferences as a learning tool
So far, the preference distribution entering the EFE was not defined. One can optionally lever-
age it to define a goal in the environment, be it for the purpose of gaining information, or
to solve an actual task. Therefore, we propose to separate learning into two tasks: model the
environment and attain a goal in it. During the first phase, which we denote the exploration
phase, the agent explores the environment without a prescribed goal. Instead, actions whose
outcomes are expected to reduce prediction errors should be favorized. This phase spreads
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 11/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 12 ‚Äî #12
PLOS One FreeEnergy Projective Simulation (FEPS)
over multiple episodes and relies on interacting with the environment. In contrast to the
exploration phase, the second phase is dedicated to learning to complete a given task. For this
purpose, we designed an algorithm to infer a goal-oriented policy from the world model in a
single step and without further information.
5.1.1 Seek information gain about the world model.Before designating any task bound
to the environment as a preference, we investigate whether the preference distribution can be
used to incentivize actions that minimize prediction errors, according to the current world
model. This is directly related to the minimization of the VFE. Specifically, preferences should
encourage the agent to seek transitions the world model associates with certainty ‚Äì or equiv-
alently with high probabilities, irrespective of actions. Sequences of interaction steps with the
environment guided by this preference distribution belong to theexploration phase. To reflect
the preference for highly probable transitions in the world model regardless of the action
chosen, the preference distribution is constructed as the marginal of the world model over
actions:
pref(Bt+1, St+1|Bt) = ‚àë
a
ùúã(At = a|Bt) p(Bt+1|Bt, At = a) p(St+1|Bt+1) (10)
= p(Bt+1, St+1|Bt). (11)
Plugging this distribution into the expected free energy evaluated for an actiona in Eq (4)
results in the following:
Gbt [at] = ‚àë
bt+1,st+1
p(bt+1, st+1|bt, at) log p(bt+1|bt, at)
‚àëa ùúã(a|bt)p(bt+1|bt, a)p(st+1|bt+1) (12)
= ùîªKL [p(Bt+1|bt, at)||p(Bt+1|bt)] (13)
= IG(Bt+1, At = at), (14)
where IG(X, Y = y) = ùîªKL[p(X|Y = y)||p(X)] is the information gain about the random vari-
able X when the valuey for the second random variableY is known. The dependency on the
observations dropped from the first to second line thanks to the constraints the clone struc-
ture imposes on the emission function. A complete derivation of this formula is provided in
S2 Appendix.
If we follow the conventional formulation of active inference as inSect 2, the agent should
increase the probability of sampling an action that minimizes this EFE. Doing so during the
exploration phase, the agent would therefore seek actions it estimates will yield the lowest
information gain about belief states. As a result, the agent would stay in a region of the envi-
ronment where it predicts it will receive the least surprise, according to its world model. This
situation is sometimes referred to as the Dark Room problem [87]: an agent that adapts by
minimizing its surprise about observations would act to stay in a dark room instead of using
actions to explore other places that may be more surprising, but also more favorable to its
survival, because all observations there would be predictable.
There is, however, an easy solution to this problem for FEPS. In order to avoid the dark
room problem and to select actions that are expected to improve the model of the environ-
ment, the scaling parameterùúÅ in Eq (9)can be set to a positive value. In this case, the larger
the EFE associated to an action ‚Äì and the estimated information gain about the next belief
state, the larger the probability of this action in the policy. The scaling parameterùúÅ can be
understood as a way to determine how greedy an agent is in its exploration, or how strongly
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 12/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 13 ‚Äî #13
PLOS One FreeEnergy Projective Simulation (FEPS)
the information gain associated with each action influences its behavior. We callexploration
phase the interaction steps in which the agent samples its actions from such a policy.
5.1.2 Task-oriented behavior by inferring preferences on belief states.At the end of the
exploration phase, a task is designated by encoding the associated targets with a high proba-
bility in the preference distribution. From there, the agent can plan, that is sample a sequence
of actions to follow to achieve the goal. While the target is identified as a sensory state for the
FEPS, transitions that are useful to reach it are deduced from the world model. In our frame-
work, updating the policy takes a single step and does not require further interaction with the
environment.
Though active inference commonly determines the behavior by planning sequences of
actions, it becomes expensive for distant horizonsTh. A sequence ofTh actions must be cho-
sen out ofNTh
actions possible combinations, by evaluating the EFE in a space of(NB √óNS)Th pos-
sible outcomes for each sequence. Methods such as habitual tree search or sophisticated infer-
ence have been developed to mitigate this scaling issue [19,76]. Alternative approaches are
presented in [6].
Instead of planning by evaluating the generative distribution over all possible future
sequences of outcomes, we propose to encode the long-term value of a state directly into the
preference distribution. Our scheme is reminiscent of iterative value estimation [88,89] and
successor representation [35,90,91], in that it estimates a value function provided some expec-
tations about occupancies of states in the future, either acquired by experience with reinforce-
ment for example, or by inverting a learned transition function. In contrast to searching a
tree of future sequences of actions, this method does not rely on mental time traveling [92],
to the extent that agents do not simulate possible future scenarios. Instead, they are ‚Äústuck in
time‚Äù , and infer preferences in one go from stochastic quantities stored in the world model,
in contrast to [20]. Our method also departs from sophisticated inference [28,76]: instead of
bootstrapping expected free energies in time, we bootstrap preferences over belief states and
calculate the EFE only once.
We model the preference distribution to factorize over sensory and belief states [93], and
we condition it on the current belief statebt:
pref(St+1 = s, Bt+1 = b|bt) = pref(s) pref(b|bt). (15)
The first part, pref(s), is an absolute preference distribution over sensory states, that is
independent of where the agent believes it is in the world model. More specifically, ifs‚àó is
the target state for an observation, a probability pref(St+1 = s‚àó) = p‚àó is associated to it. All
other observations are given a uniform distribution pref(St+1 ‚â† s‚àó) = (1 ‚Äìp‚àó)/(|S| ‚Äì 1). Since
the target observation is given by design in the absolute preference distribution, it plays a
role analogous to that of a reward function in reinforcement learning. In addition, the pol-
icy derived fromEq 9aims at maximizing the probability of receiving the target observation:
the more probable a transition to the target observation, the lower the EFE and the larger the
probability of taking the corresponding action, thereby maximizing rewards. The second part
pref(b|bt) then reflects look-ahead preferences over belief states and how useful an agent esti-
mates a transition to be in order to satisfy its absolute preferences. In other words, the utility
of belief states over longer horizons is inferred from the value associated with the observations
they can transition to. This way, even if the goal might be far away in the world model, the
preference for the target observation propagates to intermediate belief states that contribute
to reaching it. Metaphorically speaking, the preference for a target observation propagates to
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 13/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 14 ‚Äî #14
PLOS One FreeEnergy Projective Simulation (FEPS)
the belief states that are useful to get to that target observation. For example, consider an ani-
mal in a maze. The target is manifested with a high preference towards observing food. How-
ever, the preference distribution over the locations does not indicate how to reach the food.
To remedy this, the agent infers the value of belief states in order to reach the target: if a tran-
sition to some location brings the animal closer to the food, it is assigned a higher probabil-
ity in the preference distribution. This way, the preference distribution highlights the path of
relevant actions to the target.
We propose a heuristic algorithm to estimate the look-ahead preference distribution
pref(Bt+1|bt) over transitions in the world model. The algorithm can update the policyKpref
times if needed, to refine the preference distribution. The initial policyùúã(0)(At|Bt) is uniform.
During each updatek, two quantities are calculated: (1) the look-ahead preference distribu-
tion results from the value of each transition, that estimates how useful a transition is to reach
a target within a prediction horizonTh, and (2) the policyùúã(k+1) is calculated withEq (9)and
the latest preference distribution.
To initialize each update stepk, the policyùúã(k) is used together with the world model to
evaluate how easy a belief state can be reached from the current state, in a distribution we
denote reachability:
r(k)(bt+1|bt) = ‚àë
a
p(bt+1|bt, a)ùúã(k)(a|bt). (16)
The reachability ofbt+1 coming frombt is large if there exists transitionsbt
a
/leftrightline‚Üí bt+1 asso-
ciated with high probabilities in the world model and the corresponding actions have high
chances to be sampled in the policy.
In addition, the initial value of a belief state equals the value of the observation it corre-
sponds to:
v0(bt) = ‚àë
s
pref(s) p(s|bt). (17)
For the clone-structured model, this sum reduces to a single term. At this stage, the only
belief states associated with a high value are those that represent a target observation in the
absolute preference distribution.
Next, for each iterationn ‚àà [1, Th], whereTh is a prediction horizon, the value of a belief
state is increased if it can lead to transitions that are useful to reach the target withinn steps in
the environment:
vn(bt+1) = max {vn‚Äì1(bt+1), max
b+
{ùõΩn‚Äì1 r(k)(b+|bt+1) vn‚Äì1(b+)}} , (18)
for 0‚â§ ùõΩ ‚â§1 some discount factor that makes the value of the state decrease with the number
of steps between this state and the target. At eachn, the value of a belief statebt+1 can either
keep its previous valuevn‚Äì1(bt+1), or it can take the discounted value of the best stateb+ it can
transition to. As a result, the value of a state can only increase. Ifn = 1, using this value func-
tion can point at the right decision while being one step away froms‚àó. However, it does not
incentivize the correct action when starting further away from the goal. To mitigate this effect,
a state can inherit the value of the states it can reach over larger time scalesn > 1, thereby
propagating the preference for the target to more distant but useful belief state states.
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 14/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 15 ‚Äî #15
PLOS One FreeEnergy Projective Simulation (FEPS)
When the prediction horizon is reached, a transitionbt ‚Üí bt+1 is associated with the fol-
lowing probability in the look-ahead preference distribution:
pref(k)(bt+1|bt) = ùõøbt+1‚ààch(bt)
vn=Th (bt+1)
‚àëb‚Ä≤
t+1‚ààch(bt) vn=Th (b‚Ä≤
t+1) (19)
where the setch(bt) = {b+ | r(k)(b+|bt) > r(k)(Bt+1|bt)} contains the children ofbt, or the
states that are easily reachable frombt. r(k)(Bt+1|bt) is the mean reachability of belief states
coming from statebt.
Finally, to conclude thek-th update step in the algorithm, the policyùúã(k+1) is calculated by
using the preference distribution pref(St+1) pref(k)(Bt+1|Bt) in the expected free energy, as in
Eq (9). We show a possible look-ahead preference distribution in the world model inFig 2.
The computational time cost of this planning procedure scales linearly with the product of
the prediction horizonTh and the number of update iterationsKpref. Keeping the number of
clones per observation constant, it also scales asO(N3
B). This cost can easily become constant
if the procedure is parallelized.
5.2 Delineate belief states for the same observation
In spite of prescribing a method to sample actions, active inference does not include tech-
niques to efficiently choose belief states when multiple of them could explain the current
observation. In particular, models involving neural networks lack the interpretability to
design suitable belief state selection methods. Therefore, letting the agent learn a world model
with a clone-structured HMM has advantages beyond planning. We propose a technique
to evaluate belief statesin superposition, depicted inFig 2. When placed in an environment
and receiving its first observation, the agent makes an initial hypothesis about its belief state
by distributing an excitation to any clone clip compatible with the observation, as shown
in Fig 1c. At each step, an action is sampled from the policy for each excited clone clip. The
resulting frequencies define a new distribution, from which an action is sampled before it is
applied to the environment. For each compatible clone clip, the agent samples a new clip to
represent the belief state it anticipates it would transition to if the clone clip under consider-
ation stands for the correct belief state. The excitation jumps onto the new clip. After apply-
ing the action to the environment and receiving the resulting sensory signal, the agent takes
away the excitation on any clip that does not match the current observation. Depending on
the structure of the environment and the number of clones for each observation, the agent
progressively narrows down its candidate belief states to a single clone clip, in spite of the ini-
tial uncertainty. This elimination process can be thought of as successively applying Bayes‚Äô
rule to the clone clips while keeping the posterior uniform as the agent receives more obser-
vations: thanks to the deterministic likelihood imposed by the clone structure of the ECM, the
probability of any incompatible candidate clone clip becomes zero. In the event that the world
model is imperfect and the agent has eliminated excitations on all clips, it starts its hypoth-
esis over, and considers all the clone clips of its current, unpredicted observation as candi-
date belief states. This mechanism allows the agent to evolve in environments with ambiguous
observations in the absence of more contextual information about its initial conditions. The
computational cost of this method depends on the structure of the environment. More specif-
ically, the maximum number of steps required to disambiguate hidden states is the length of
the longest sequence of actions that would produce the same observation sequences when
starting from those hidden states.
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 15/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 16 ‚Äî #16
PLOS One FreeEnergy Projective Simulation (FEPS)
Fig 2.Estimation of belief states in superposition, after the world model has been trained.To minimize its pre-
diction error due to faulty belief state estimation, an agent considers multiple clone clips as candidate belief states
simultaneously. For the initial observation,senv
1 (on the left), the agent includes all corresponding clone clips{bi
t}3
i=1
to its hypothesis, as depicted on the right. Conditioned on the chosen action, a clone clip is sampled for each candi-
date belief state to represent the next one. Finally, all clone clips that are incompatible with the observation from the
environment are eliminated from the hypothesis. The clips that remain become the current candidate belief states. In
the world model, the thickness of the arrows represents the look-ahead preferences: the larger the arrow, the more
advantageous is the transition in order to reach the target observation,s4 in this case.
https://doi.org/10.1371/journal.pone.0331047.g002
6 Numerical results
In this section, we present a numerical analysis of the model on environments inspired from
behavioral psychology tasks, namely a timed response experiment in a Skinner box and a nav-
igation task to forage for food. The parameters used for the simulations are given inS1 Table
in the Appendix.
6.1 The timed response task
6.1.1 Learn short-term associations.The timed response task is a minimal environment
for an agent to learn to contextualize its observations with past states and actions when the
sensory signals emitted by the environment are ambiguous. This environment simulates an
animal standing in front of a door that can be opened with a lever. The goal is for the agent
to learn a conditioned response and to press a lever at the right time in order to access food.
The environment‚Äôs MDP is depicted inFig 3. For this task, the observations combine two sen-
sory inputs:S = { (light off, hungry), (light off, satiated), (light on, hungry)}. Since food can be
consumed only when the light is off, the observation (light on, satiated) in excluded from the
set. The actions areA = {wait, press the lever}. The environment is initialized in the (hidden)
state E0, that emits observation (light off, hungry). From there, the light turns on, regardless of
the action taken by the agent. Once the light has turned on, the agent must learn to wait one
step before pressing the lever. If it does so too early, it gets back to the initial state. If the agent
activates the lever on time, the environment transitions to stateE‚Ä≤
0, where the light is off, but
the agent is satiated.
For the simulations, we give each observation two clones. It makes it possible to accommo-
date enough candidate belief states to model up to two ambiguous hidden states that emit the
same sensory signal, enabling the agent to adapt its policy to a one-step waiting time between
the light turning on and the food being accessible for example. When observations are not
ambiguous and can be emitted only by a single hidden state in the environment, some belief
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 16/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 17 ‚Äî #17
PLOS One FreeEnergy Projective Simulation (FEPS)
Fig 3.MDP for the timed response environment.This environment has four hidden states. The observations are
compositional and contain information that are both external (light on or off) and internal (hungry or satiated) to
the agent. Arrows correspond to the transitions the actions the agent can result in. In this environment, the agent
can either wait or press a lever. In order to complete the task, the agent must reachE‚Ä≤
0 and feel satiated. The only way
to do so is to follow the actions marked with thicker arrows. The observation (light on, hungry) is called ambiguous
because it can be emitted by two hidden statesE1 and E2 that can only be distinguished with context.
https://doi.org/10.1371/journal.pone.0331047.g003
states become redundant. This redundancy can make the training more challenging to the
extent that the agent has to find a convention and adapt its model accordingly before it can
account for all transitions in the environment faithfully. The larger the waiting timen, the
more clones might be necessary to learn. We train 100 agents for 4000 episodes of 80 steps in
this environment and we test two scenarios. In the first, the agent is directly given the pref-
erence for the target observation (light off, satiated). The EFE is then scaled with a scaling
parameter ofùúÅ = ‚Äì1 in the policy inEq (9). In the second scenario, we test whether the agent
can learn more efficiently if it explores aimlessly the environment for the same number of
episodes without preference for the target before adapting its policy to the task, with a scaling
parameter of 0.
6.1.2 Simulation results. The timed response environment is a minimal testbed for
the FEPS, where some hidden states are not uniquely defined by the observation they emit,
but also by the recent past. The agent must learn two types of belief states. While clones for
(light off, hungry) and (light off, satiated) only support transitions that are independent of
the actions the agent takes, the clones for (light on, hungry) must appropriately use informa-
tion about the previous observation and action to be contextualized and distinguished. Some
results are reported inFig 4.
During training, regardless of the strategy that the agent uses to resolve the environment,
all agents follow a similar learning pattern. The acquisition of the world model happens in
stages, where the transition from one to the next is manifested in a steep increase in the length
of the trajectories of correct predictions, or equivalently, as a steep decrease in the free energy,
as inFig 4a. First, agents quickly eliminate transitions that are impossible in the environ-
ment, leading to an initial drop in the VFE. For example, as inFig 3, a direct transition from
(light off, hungry) to (light off, satiated) is incompatible with the timed response task. Dur-
ing the second phase, the number of rewards the agent can collect is limited by the absence of
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 17/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 18 ‚Äî #18
PLOS One FreeEnergy Projective Simulation (FEPS)
Fig 4.Training FEPS agents for the timed response task.a) Evolution of the variational free energy (top) defined inEq (2)and expected free energy as inEq 4(bottom)
during the training, averaged over 100 agents and a time window of 100 episodes. At each step, the VFE depends on the specific belief states and actions that were sam-
pled. Two types of training are compared: a first set, ‚Äútask‚Äù in dark purple, learned the model with a preference to fulfill a task in the environment, while the second set,
‚Äúexplore‚Äù , in green, experimented aimlessly in the environment with a uniform policy before switching to the task. Both were trained for 4000 steps before being tested
on the task. The best and worst agents are represented with dashed and dotted lines respectively, and examples of individual agents were traced with transparent lines,
full for task-oriented agents, and dashed for the exploring ones. When the VFE converges to its minimal value, the world model is precise enough for most belief states
to make planning possible. As expected from the values chosen for the scaling parameterùúÅ, agents select actions that minimize and maximize the EFE for task-oriented
and exploring agents respectively. The EFE of exploring agents plateaus quickly at the limit derived forS3 Appendix. b) World model learned by one of the agents trained
on the task, where each circle is a belief state, whose observation is denoted by its colors and label. The numbers at the center of the circles are the clone indices for each
clone clip. Arrows indicate the transitions learned in the world model, red for action ‚Äúpress the lever‚Äù , and blue for ‚Äúwaiting‚Äù . Dashed lines indicate that both actions lead
to the same transition. The weight on the arrow indicates its probability in the world model. Stars mark transitions that were identified as useful to achieve the goal with
a probability of 1 in the preference distribution. The policy is indicated by the thickness of the arrows, where a thick arrow corresponds to probabilities close to 1, and
thinner close to 0.5.
https://doi.org/10.1371/journal.pone.0331047.g004
convention on the context-sensitive belief states. Therefore, a plateau is observed on the VFE.
A final sharp decrease in the VFE signals the adoption of a convention between clones to
accurately disentangle ambiguous hidden states that cannot be told apart with observations
alone. The EFE evolves as expected during the training: it decreases for the task-oriented
agents, while it rapidly plateaus to its limit (seeS3 Appendix) in with an exploration phase.
However, in contrast to the VFE, the convergence of the EFE to its asymptote value does not
reflect the fact that the model is good enough to make accurate predictions.
For most agents, training with or without a preference for the target in order to construct
the world model does not influence the final model. As inFig 4a, the two best agents, that is,
those that converge to free energy values below 1 the earliest, share similar learning curves.
The averaged behaviors remain fairly identical, except for the length of the second stage of
the learning phase, where the agents have yet to adopt a successful convention. In particular,
in the absence of aimless experimentation with the environment, this second phase can last
longer, such that convergence is not attained for a fraction of the agents trained in this way.
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 18/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 19 ‚Äî #19
PLOS One FreeEnergy Projective Simulation (FEPS)
After convergence of the free energy, or equivalently of the number of successful pre-
dictions, the model has collapsed on a single representation of the environment, where the
ambiguous observation is contextualized by the previous belief state and action. For example,
the observation (light on, hungry) is divided into two belief states with different uses, reflect-
ing the two hidden states in the environment. The corresponding clone clips bear more infor-
mation than the observation to which they are linked. InFig 4b, clone 2 for (light on, hungry)
is the first hidden state encountered when the light turns on, and clone 1 can only be accessed
by waiting from clone 2. When more clones are provided than necessary, two possibilities can
arise: either the agent uses all the clones as duplicates of the same hidden state in the environ-
ment (as for clones of (light off, hungry)), or a single clone is reachable (clone 2 of (light off,
satiated)) and the other is never trained because it was not visited. When multiple clone clips
participate equally in the representation of the observation, the actions that lead to it also split
with equal probability (as derived inS3 Appendix).
Finally, for the agents that converged to an appropriate representation of the environment,
the look-ahead preferences inferred from the world model result in an optimal policy when
using the EFE. A visualization of the preferences and policy is provided inFig 4. We chose a
prediction horizon of 2 steps, and a single iteration was required to calculate appropriate look-
ahead preferences for the transitions. The transitions with maximal preference were kept so
that, coming from a belief statebt, a single belief stateb‚àó is more preferable than the others.
The agents were tested for 1000 rounds, each starting inE0 in the environment. For a predic-
tion horizon of a single step, the edge between clone 1 of (light on, hungry) and clone 2 of
(light off, satiated) would be the only edge to carry a probability larger than other transitions
in the preference distribution. Thanks to the look-ahead preferences (and the adoption of a
convention between belief states), waiting between clones 1 and 2 of (light on, hungry) is also
preferred. The policy that results from this preference distribution is optimal to solve the task,
in spite of initially having no hints about the target prior to the last transition.
6.2 Navigation task in a partially observable grid
6.2.1 Long-term planning in an ambiguous environment with symmetry.The FEPS is
further challenged in a larger navigation task, where observations are shared among hidden
states, and multiple sequences of actions can emit the same observations, due to the symmetry
of the environment. In order to disentangle the hidden states, the agent must use long-term
information about its past observations and actions to contextualize its current state in a way
that is consistent across actions. In this environment, food is hidden in one position in a grid.
Locations in a 3 by 3 grid world are associated with smell intensities, denoted by their inte-
ger intensities from 0 to 3 from the lower left to the upper right corners, according to their
closeness to the food.
The world model is provided with 3 clones for each of the four observations, and the
behavior repertoire comprises directional actionsA = {go right, go left, go up, go down}.
30 agents were trained on different hyperparameters and preferences, for a total of 40000
episodes of 80 steps. The hyperparameters are provided inS1 Tablein the Appendix. Since
this environment is larger and more complex, we test different training techniques, by chang-
ing the preference distributions and varying the scaling parameter. In particular, we test two
scenarios. In the first the agent is trained with preferences pointing at a target in the environ-
ment, while in the second, the preferences are identified with the marginal of the world model
over actions, which incentivizes dissociating the effect of actions on the environment. While
in the first case, the agent tries to directly solve the task, in the second, the agent explores first
before it learns the task. For each scenario, different scaling parameters were implemented,
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 19/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 20 ‚Äî #20
PLOS One FreeEnergy Projective Simulation (FEPS)
ranging from -3 to 3 for the task-oriented training, and from 0 to 3 with an exploration phase.
Note that choosing a scaling parameter of 0 while training either directly on the task or with a
preceding exploration phase results in the same uniform policy.
6.3 Simulation results.
Training task-oriented and exploring agents.Compared to agents trained with a uniform
policy, agents using policies inferred from the EFE can achieve longer trajectories. This is
apparent inFig 5a), where with a scaling parameter of 0, the trajectories of agents reach a
length of 60 on average by the end of the training, whereas given a proper tuning, agents
using the EFE to define the policy can predict up to 69 transitions, regardless of the training
method. Adapting the scaling parameter to the preference distribution is crucial for the agents
to learn the model and query actions that result in longer trajectories. For task-oriented
agents, as expected fromEq (4), a negative scaling parameter at -3 is optimal, whereas +1
yields the best results for the exploring agents. For both training methods, setting it at +3
results in the shortest trajectories, that are 28 and 50 steps long, respectively. In the case of
task-oriented agents, a possible explanation is that in this regime and when the preference
distribution designates a target in the environment, the policy that is derived from the EFE
in Eq (4)minimizes the drive of the agent to explore and to fulfill any preferences in the envi-
ronment. Therefore, the agent has no incentive to go outside of a region of the environment
where it can predict its observations, and does not try to learn the rest of it. In contrast, a
scaling parameterùúÅ = +3 makes exploring agents very greedy. It is then possible that previ-
ously explored regions are not visited often enough to reinforce the correct associations in
the world model in the long-run. Finally, our simulation shows that for the best parameter
settings, the task-oriented agents converge on average faster than the exploring ones: while
17500 episodes are needed to predict 65 steps for the former, the latter crosses this milestone
at 28000 episodes.
Looking at the evolution of the free energy during the training inFig 5b), one sees that in
both cases, the VFE decreases as the length of trajectories increases. It looks as if agents min-
imize their free energy by maintaining a world model and improving their predictions about
future sensory states. This matches the free energy principle. Comparing individual learn-
ing curves, exploring agents have more diverse behaviors than those trained on the task. We
define the best and worst agents as those converging the earliest and the latest to a free energy
minimum, respectively. The best exploring agents can converge faster than the task-oriented
ones, whereas the worst exploring agent lands on a higher free energy by the end of the train-
ing. However, the free energy of the worst exploring agent either goes down or plateaus, but it
does not rise again, as for the worst agent directly trained on the task. When the training sce-
narios influence the scale of the EFE, no significant variations around the average behavior is
observed in the EFE to distinguish successful agents from those that did not converge. In par-
ticular, the EFE exploring agents converge very quickly to its asymptotic value, regardless of
the accuracy of the world model.
In order to fairly compare the models obtained from each training method, note that
whenever the length of trajectories or the free energy are optimized, two contributions are at
play. On the one hand, training the world model to capture the environment more accurately
decreases the uncertainty about the transitions following actions, and therefore improves the
length of the trajectories and the free energy. But on the other hand, as long as the policy
deviates from the uniform distribution, it influences how much uncertainty the agents seek.
Therefore, in order to evaluate how much the world model helps making accurate predictions,
we test the length of trajectories with a uniform policy for the agents trained in the two best
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 20/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 21 ‚Äî #21
PLOS One FreeEnergy Projective Simulation (FEPS)
Fig 5.Training results for the grid world environment.a) Evolution of the length of the trajectories during the training, for different scaling parameters ranging from
‚Äì3 to 3, and different preference distributions: the agent can either learn to complete the task from the start (‚Äútask‚Äù), or first explore the grid (‚Äúexplore‚Äù). We represent the
running averages over a time window of 1500 steps of the lengths of trajectories, averaged over 30 agents. These lengths depend on the specific belief states and actions
sampled by the agents. b) Evolution of the variational and expected free energies during the training for the two best settings in a): the task-oriented preferences are
paired with a scaling parameter of ‚Äì3, and the exploration preferences with a parameter of +1. The thick lines represent the running average of energies over a time
window of 1500 steps, averaged over all 30 agents, while the dotted and dashed lines stand for the best and worst agents, that is the agents whose VFE converge first
and last, respectively, to a minimum. The transparent lines indicate the behavior of agents selected randomly: these lines are full for task-oriented agents and dashed for
exploring agents. c) Comparison of the accuracy of the model to make predictions between the two best parameter settings in a). Blue and red colors indicate explorative
and task-oriented agents respectively. Darker and lighter shades distinguish the belief state estimation methods, ‚Äúsup. ‚Äù designating superposition. The policies of the
agents are uniform, such that the actions yielding most certain outcomes cannot be relied upon to validate predictions. Two belief state estimation techniques are tested.
Belief state estimation samples a single clone clip at a time, whereas the evaluation of belief states in superposition allows multiple clone clips to represent candidate
belief states simultaneously, as long as they produce predictions that are compatible with the next observation. Each individual agent is tested over 1000 trajectories of at
most 80 steps.
https://doi.org/10.1371/journal.pone.0331047.g005
parameter and preference settings. The trajectory lengths averaged over all agents trained with
the respective parameters are provided inFig 5c).
With the belief state estimation protocol where a single clone clip is excited at each step,
trajectories achieved by FEPS agents are analogous in length for both types of model-learning
strategies, either exploration- or task-oriented. More precisely, half of the agents are able to
predict half of the maximum length for a trajectory. Similarly, extreme cases are comparable.
Estimating belief states in superposition, i.e. considering all clone clips that are compati-
ble with the current trajectory at each step, doubles the length of trajectories for both training
strategies. Indeed, agents of both types reach near optimal length. For explorative agents, the
top 50% of the agents achieves the same performance, showing a slight improvement com-
pared to task-oriented agents. We expect that this improvement will become greater when the
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 21/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 22 ‚Äî #22
PLOS One FreeEnergy Projective Simulation (FEPS)
environments reach larger scales, and when the target given to task-oriented agents become
more ambiguous. In addition, the ability of the agents to adapt to different goals in the envi-
ronment would also be affected.
The agent learns an interpretable map of the environment.After the free energy con-
verges, the belief states in the world model no longer stand only for the observation they relate
to, but also convey information about the path they can appear in. As an example, the world
model of an agent trained directly on the task is represented inFig 6a). Because the environ-
ment the agent was trained in is deterministic, actions tend to map onto a single belief state
when the number of clones for an observation matches the number of hidden states in the
environment.
In order to understand how the world model can be interpreted, let us look at observation
0 (blue inFig 6a) and b)), that can be emitted by three different hidden states in the lower left
corner of the grid. After the training, the role of the three clone clips splits based on what has
been observed, including sequences of actions and observations that led to it, and those that
were predicted from it. For example inFig 6a) , going left or down from clonec3(0) maps it
onto itself, suggesting that it represents the lower left corner of the grid. Going right or up
Fig 6.Interpretability of the world model and robustness to reward reevaluation for the grid world environment.a) Example of a
world model learned by an agent directly trained on the task, with the target positioned in the top right corner of the grid. The circles
represent the belief states as inFig 1, numbered with clone indices, and colored according to the observation they relate to in the grid. The
arrows stand for the transition probabilities: the thicker the arrow, the more the agent believes taking an action from a belief state will lead
to the state the arrow points at. b) Mapping of the world model in a) onto the grid: each clone is associated with exactly one cell, and can
be interpreted as a single, specific hidden state in the environment. Stars stand for the preferred transitions, and the arrows for the policy
resulting from these preferences. c) Median number of steps from each initial position in the grid in order to reach the target, where belief
states are estimated in superposition. Two targets are given to the agent and symbolized with a gray triangle: reach observation of 3 (top
right, red) and 0 (bottom left, blue), each requiring opposite policies. When targets were swapped, no interaction with the environment was
necessary to re-evaluate the value of the transitions and the resulting policy. The median time to target is compared to that of a random
agent with uniform policy and tested with the same procedure.
https://doi.org/10.1371/journal.pone.0331047.g006
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 22/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 23 ‚Äî #23
PLOS One FreeEnergy Projective Simulation (FEPS)
from c3(0) predicts transitions to clonesc2(0) and c1(0) respectively, making them candi-
dates for the second positions on the first row and first column respectively. In this way, each
clone clip can be associated to a single location in the grid, as shown inFig 6b). As a result,
training the world model in this environment amounts to assigning a single hidden state to
each clone clip in a manner that is consistent across actions.
In particular, in spite of being agnostic of the spatial nature of the environment, the world
model can be interpreted as a topological map of the grid, as pictured inFig 6b). Each clone
maps onto a single location of the grid, and this mapping is consistent for all actions. When
too many clones were provided compared to the degeneracy of an observation, multiple
clones are associated with a single location, as for observation 3 in the upper right corner of
the grid. This consistent mapping suggests that the agent is able to contextualize the observa-
tions with a sequence of previous states and actions. Thanks to this mapping of clones onto
single locations, the look-ahead preferences over belief states encode paths ending at the
target. They steer the policy toward an optimal one.
Exploit the world model to complete tasks flexibly.Provided the target observation is
signaled in the preference distribution, the agents can adapt their policy to reach it following
near optimal trajectories. InFig 6c), agents whose model successfully converged are tested
to reach targets with observations 3 (top, red) and 0 (bottom, blue), requiring opposite poli-
cies to be attained. For both tasks, irrespective of the initial location of the agent, the perfor-
mance is drastically improved compared to a random agent with uniform policy. In order to
move the target, no other interaction with the environment was required than changing the
preference distribution over sensory states, and no new observation was exchanged.
While the paths towards observation 0 are mostly optimal, there is an overhead by at most
one step to reach observation 3. Since the policy is an optimal one, as shown inFig 6b), we
suggest that this is due to the estimation of the belief states. In particular, the small size of
the grid prevents agents from narrowing down their hypothesis to a single belief state early
enough to opt for the right policy at the boundary of the grid. This could explain why on aver-
age, agents take 1.5 steps to reach the target when being right under it: they have 0.5 probabil-
ity of finding the wrong belief state, and therefore choose the wrong action 50% of the time.
This eliminates the last erroneous state from the hypothesis and the agent behaves optimally
in the next step.
7 Discussion and outlook
In this work, we propose an interpretable, minimal model for cognitive processes that com-
bines associative learning and active inference to adapt to any environment, without any prior
knowledge about it and independently of any goal tied to it. We develop the Free Energy
Projective Simulation model, a physics-inspired model that seeks inspiration in current
paradigms in cognitive sciences, namely active inference, that encompasses the Bayesian
brain hypothesis (BBH) and fits more broadly into predictive processing. A FEPS agent is
equipped with a memory structured as a graph, the ECM, where the agent encodes associ-
ations between events represented as clone clips. As in models of associative learning in the
cerebellum [94‚Äì96], internal reinforcement signals based on the prediction accuracy of the
agent reinforce associations between observations. Clones clips are the support for belief
states and acquire contextual meaning as the agent collects longer sequences of correct predic-
tions about its observations and improves its world model. The behavior of FEPS agents does
not depend on any reinforcement and is fully determined with active inference, such that the
policy optimizes the expected free energy estimated for each action, given the current world
model. The resulting model is interpretable in three ways: 1) The world model is readable, 2)
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 23/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 24 ‚Äî #24
PLOS One FreeEnergy Projective Simulation (FEPS)
Deliberation is traceable and 3) Credit assignment during training is explainable. We perform
numerical analysis of the agents in two environments inspired from behavioral psychology: a
timed response environment, as well as a navigation task in a partially observable grid world.
Leveraging the interpretability of the model, we identified three capacities a FEPS agent
requires in order to interact with an environment and to reach a goal while controlling it, that
is, minimizing its surprise about future events. (1) The representation of the environment in
the world model should be accurate enough in its predictions, i.e. it should be capable of pre-
dicting future sensory signals. (2) The agent should be able to choose the appropriate belief
state for aliased observations. (3) For a given time horizon, the agent should be equipped with
an efficient mechanism to plan its course of actions.
First, in order to build a complete and accurate representation of the environment, we
propose to start learning with an exploration phase. It focuses on exploring the environment
strategically rather than completing a task. For this, we provide a model-dependent preference
distribution that evolves as the agent learns and we show that the corresponding expected
free energy of an action is equal to the information gain about belief states resulting from this
action. As a result, the behavior of the agent is motivated by the acquisition of information
in its world model rather than by a target bound to the environment. Our numerical simula-
tions showed that a model learned in such a way is able to predict longer sequences of obser-
vations when actions are selected at random, compared to the model trained directly on a
task. Previous works usually involve either adopting a different definition for the expected
free energy [74], or more commonly, adding terms to it to encourage explorative behaviors,
[20], leveraging existing literature on curiosity mechanisms, for example [29,43,45,97]. Alter-
natively, a count-based boredom mechanism [98] could be well-suited for FEPS, as it could be
directly implemented on the edge attributes, such as the confidence, to deviate the agent from
transitions it has already resolved.
Second, we design a simple procedure to progressively disambiguate belief states with
sequences of observations. It can be used to interact with the environment after the model has
been trained. Leveraging the clone structure imposed to the clips in the memory of the agent,
belief states can be estimated in superposition. As predictions for each candidate belief state
are validated or not by the observation collected from the environment, the agent can elimi-
nate candidate belief states that are incompatible with the context provided by its current sen-
sory state. We show in our numerical analysis that this simple mechanism allows to double
the number of correct, consecutive predictions, regardless of the technique used to learn the
world model.
Thirdly, we introduce a planning method based on the expected free energy. Instead of
relying on a tree search or a simulation of future scenarios, we propose to encode the utility
of a transition between two belief states in the preference distribution. For this, we factor-
ize the preference distribution into an absolute preference distribution that designates a tar-
get among the possible sensory states, and a look-ahead preference distribution. The latter
assigns a probability to transitions between belief states that is commensurate with its esti-
mated utility in reaching the target within a given number of actions. Using the world model,
the value for each transition is determined in an iterative manner. We tested this scheme in
two numerical experiments and achieved optimal policies in both cases, as long as the world
model was predictive enough. The closest model to our knowledge is Successor Representa-
tion [35,90,91], that has been hypothesized to account for so-called model-free learning in
cognitive systems [90]. A major difference is that in Successor Representation, the value of
a transition depends on the prediction error over the expected reward in the environment,
whereas we assign value to a belief state via the probability of the associated sensory state in
the absolute preference distribution. A limitation of our scheme, however, is that a target can
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 24/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 25 ‚Äî #25
PLOS One FreeEnergy Projective Simulation (FEPS)
only be encoded as a possibly ambiguous observation. Using reinforcement and a few interac-
tions with the environment, the preference for a particular hidden state could be encoded in
the look-ahead preference distribution in a hybrid scheme.
The numerical analysis of FEPS in larger environments has revealed that the scaling of the
method requires modifications of the model in some cases, inheriting in this regard scalabil-
ity issues from tabular RL methods. More specifically, the main obstacle in efficiently learn-
ing a world model is the number of edges to reinforce, because it scales quadratically with
the number of clones, and therefore, with the number of distinguishable hidden states in the
environment. Consequently, FEPS can learn a representation of a large environment with lit-
tle to no ambiguity in the observations, but can fail to model a smaller one with few but very
ambiguous observations. In addition, clone differentiation can become harder if many dif-
ferent paths in the environment can lead to the same observation sequence using the same
actions. Including a mechanism to add and delete clones, such that the world model approx-
imates anùúñ-transducer for the environment [99] on finite degrees of freedom, could help mit-
igate this scaling issue. Modifications to the decision-making and reinforcement algorithms
could also include multiple edges at each step in order to speed up the learning process.
Conceptually, the FEPS framework fits into the field of NeuroAI [100], at the intersection
of behavioral sciences, engineering and neurosciences. While Projective Simulation can be
used to learn in an artificial environment, its vocation is to understand agency and the behav-
ior of agents in the world. Furthermore, the FEPS attempts to give a biologically plausible
account of learning and adaptive behavior, grounding internal computations in the active
inference framework and the predictive processing paradigm. ECMs are potentially imple-
mentable on physical platforms and can be considered embodied structures underlying the
memory of the agents. For example, a parallel can be drawn between the role the network
of belief states plays for FEPS agents, and that of place cells and grid cells in the hippocam-
pus [69,70,72]. Both integrate stimuli to create a contextualized representation of an event in
order to make predictions about future stimuli. For the FEPS to provide a modeling platform
of interest for cognitive and behavioral sciences, the next challenge is to implement learning
on real-world tasks and in a fully embodied way, including the calculation of coincidences
between predictions and observations, and the update of the associations between states.
Supporting information
S1 Appendix.Table of hyperparameters for the numerical analysis.
(PDF)
S2 Appendix.Derivation of the expected free energy in the exploration phase.
(PDF)
S3 Appendix.Analytical estimation of the limits of the expected free energy.
(PDF)
Acknowledgments
The authors express their gratitude to Philip A. Lemaitre for fruitful discussions.
Author contributions
Conceptualization: Jos√©phine Pazem, Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer,
Hans J. Briegel.
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 25/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 26 ‚Äî #26
PLOS One FreeEnergy Projective Simulation (FEPS)
Data curation:Jos√©phine Pazem.
Formal analysis:Jos√©phine Pazem, Marius Krumm, Lukas J. Fiderer.
Funding acquisition:Lukas J. Fiderer, Hans J. Briegel.
Investigation: Jos√©phine Pazem, Marius Krumm, Alexander Q. Vining, Hans J. Briegel.
Methodology: Jos√©phine Pazem, Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer.
Software: Jos√©phine Pazem.
Supervision: Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer, Hans J. Briegel.
Visualization: Jos√©phine Pazem.
Writing ‚Äì original draft:Jos√©phine Pazem.
Writing ‚Äì review & editing:Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer, Hans J.
Briegel.
References
1. SuttonRS, Barto AG. Reinforcement learning: anintroduction. Cambridge, MA: MIT Press; 1998.
2. KirchhoffM, ParrT,Palacios E, Friston K, Kiverstein J.The Markov blankets of life: autonomy,
activeinference and the free energy principle.J R Soc Interface. 2018;15(138):20170792.
https://doi.org/10.1098/rsif.2017.0792PMID:29343629
3. LinsonA, Clark A, Ramamoorthy S, FristonK. The active inference approach toecological
perception:general information dynamics for natural andartificial embodied cognition. Front Robot
AI.2018;5:21. https://doi.org/10.3389/frobt.2018.00021PMID:33500908
4. PezzuloG, Rigoli F,Friston K. Active Inference,homeostatic regulation and adaptive behavioural
control.Prog Neurobiol. 2015;134:17‚Äì35.https://doi.org/10.1016/j.pneurobio.2015.09.001PMID:
26365173
5. RajaV,ValluriD,Baggs E, Chemero A, Anderson ML.The Markov blanket trick: On thescope of
thefree energy principle and active inference.Phys Life Rev.2021;39:49‚Äì72.
https://doi.org/10.1016/j.plrev.2021.09.001PMID:34563472
6. MazzagliaP,VerbelenT,√áatal O, Dhoedt B. The freeenergy principle for perception and action:a
deeplearning perspective. Entropy (Basel). 2022;24(2):301.https://doi.org/10.3390/e24020301
PMID:35205595
7. TschantzA,Millidge B, Seth AK, Buckley CL.Reinforcement learning through active inference.
arXivpreprint 2020.https://arxiv.org/abs/2002.12636
8. FristonK, FitzGerald T,Rigoli F,Schwartenbeck P,ODoherty J, Pezzulo G. Active inferenceand
learning.Neurosci Biobehav Rev.2016;68:862‚Äì79.
https://doi.org/10.1016/j.neubiorev.2016.06.022PMID:27375276
9. BubicA, von Cramon DY,Schubotz RI.Prediction, cognition and the brain. Front HumNeurosci.
2010;4:25.https://doi.org/10.3389/fnhum.2010.00025PMID:20631856
10. VilaresI, KordingK. Bayesian models: the structure ofthe world, uncertainty,behavior,and the
brain.Ann N Y Acad Sci. 2011;1224(1):22‚Äì39.https://doi.org/10.1111/j.1749-6632.2011.05965.x
PMID:21486294
11. FristonK. The free-energy principle: a unifiedbrain theory?. Nat Rev Neurosci. 2010;11(2):127‚Äì38.
https://doi.org/10.1038/nrn2787PMID:20068583
12. FristonK, Kiebel S. Predictive coding underthe free-energy principle. Philos TransR Soc LondB
BiolSci. 2009;364(1521):1211‚Äì21.https://doi.org/10.1098/rstb.2008.0300PMID:19528002
13. PezzuloG, D‚ÄôAmato L, Mannella F,Priorelli M,Vande Maele T,Stoianov IP,et al. Neural
representationin active inference: using generative modelsto interact with-and understand-the
livedworld. Ann N Y Acad Sci.2024;1534(1):45‚Äì68.https://doi.org/10.1111/nyas.15118PMID:
38528782
14. CullenM, Davey B, Friston KJ, MoranRJ. Active inference in OpenAI Gym:a paradigm for
computationalinvestigations into psychiatric illness. Biological psychiatry:Cognitive Neuroscience
andNeuroimaging. 2018;3:809‚Äì18.https://doi.org/10.1016/j.bpsc.2018.06.010
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 26/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 27 ‚Äî #27
PLOS One FreeEnergy Projective Simulation (FEPS)
15. McGovernHT,De Foe A, Biddell H, LeptourgosP,Corlett P,Bandara K, et al.Learned
uncertainty:the free energy principle in anxiety.Front Psychol.2022;13:943785.
https://doi.org/10.3389/fpsyg.2022.943785PMID:36248528
16. RamsteadMJD, Badcock PB, Friston KJ. AnsweringSchr√∂dinger‚Äôsquestion: afree-energy
formulation.Phys Life Rev.2018;24:1‚Äì16.https://doi.org/10.1016/j.plrev.2017.09.001PMID:
29029962
17. HeinsC, Millidge B, Da Costa L,Mann RP,Friston KJ, Couzin ID. Collectivebehavior from
surpriseminimization. Proc Natl Acad Sci US A. 2024;121(17):e2320239121.
https://doi.org/10.1073/pnas.2320239121PMID:38630721
18. MazzagliaP,VerbelenT,Dhoedt B. Contrastive active inference. In:Advances in Neural
InformationProcessing Systems. vol. 34. Curran Associates,Inc.; 2021. p. 13870‚Äì82.https:
//proceedings.neurips.cc/paper/2021/hash/73c730319cf839f143bf40954448ce39-Abstract.html
19. FountasZ, Sajid N, Mediano P,FristonK. Deep active inference agents using Monte-Carlo
methods.In: Advances in Neural Information ProcessingSystems. vol. 33. Curran Associates,
Inc.;2020. p. 11662‚Äì75.https:
//proceedings.neurips.cc/paper/2020/hash/865dfbde8a344b44095495f3591f7407-Abstract.html
20. NguyenVD, YangZ, Buckley CL, Ororbia A.R-AIF:solving sparse-reward robotic tasks from
pixelswith active inference and world models.arXiv preprint 2024.http://arxiv.org/abs/2409.14216
21. deTinguyD,VandeMaele T,VerbelenT,Dhoedt B. Spatial and temporal hierarchyfor
autonomousnavigation using active inference in minigridenvironment. Entropy (Basel).
2024;26(1):0.https://doi.org/10.3390/e26010083PMID:38248208
22. ClarkA. Whatever next? Predictive brains, situatedagents, and the future of cognitivescience.
BehavBrain Sci. 2013;36(3):181‚Äì204.https://doi.org/10.1017/S0140525X12000477PMID:
23663408
23. ClarkA. How to knit your ownMarkov blanket: resisting the second lawwith metamorphic minds.
Philosophyand Predictive Processing. 2017.https://doi.org/10.15502/9783958573031
24. KaganBJ, Kitchen AC, TranNT,Habibollahi F,Khajehnejad M, ParkerBJ, et al. In vitro neurons
learnand exhibit sentience when embodied ina simulated game-world. Neuron.
2022;110(23):3952‚Äì69.e8.https://doi.org/10.1016/j.neuron.2022.09.001
25. SmithR, Badcock P,Friston KJ. Recentadvances in the application of predictive codingand
activeinference models within clinical neuroscience. PsychiatryClin Neurosci. 2021;75(1):3‚Äì13.
https://doi.org/10.1111/pcn.13138PMID:32860285
26. LanillosP,Meo C, Pezzato C, MeeraAA, Baioumy M, Ohata W.Active inference inrobotics and
artificialagents: survey and challenges. arXiv preprint2021.http://arxiv.org/abs/2112.01871
27. PiriyakulkijT,Kuleshov V,Ellis K. Active preference inferenceusing language models and
probabilisticreasoning. 2023.http://arxiv.org/abs/2312.12009
28. KawaharaD, Ozeki S, Mizuuchi I. Acuriosity algorithm for robots based onthe free energy
principle.In: 2022 IEEE/SICE International Symposium onSystem Integration (SII). 2022. p. 53‚Äì9.
https://doi.org/10.1109/sii52469.2022.9708819
29. TinkerTJ, DoyaK, TaniJ. Intrinsic rewards for explorationwithout harm from observational noise:
asimulation study based on the freeenergy principle. arXiv preprint 2024.
http://arxiv.org/abs/2405.07473
30. BriegelHJ, De las Cuevas G. Projectivesimulation for artificial intelligence. Sci Rep.2012;2:400.
https://doi.org/10.1038/srep00400PMID:22590690
31. EvaB, Ried K, M√ºller T,Briegel HJ.How a minimal learning agent caninfer the existence of
unobservedvariables in a complex environment. MindsMach (Dordr). 2023;33(1):185‚Äì219.
https://doi.org/10.1007/s11023-022-09619-5 PMID:37041982
32. FlaminiF,Krumm M, Fiderer LJ, M√ºller T,BriegelHJ. Towardsinterpretable quantum machine
learningvia single-photon quantum walks. arXiv preprint2023.http://arxiv.org/abs/2301.13669
33. DawND, Dayan P.The algorithmic anatomyof model-based evaluation. Philos TransR Soc Lond
BBiol Sci. 2014;369(1655):20130478.https://doi.org/10.1098/rstb.2013.0478PMID:25267820
34. SuttonRS. Dyna, an integrated architecture forlearning, planning, and reacting. SIGARTBull.
1991;2(4):160‚Äì3.https://doi.org/10.1145/122344.122377
35. DayanP.Improving generalization for temporal differencelearning: the successorrepresentation.
NeuralComputation. 1993;5(4):613‚Äì24.https://doi.org/10.1162/neco.1993.5.4.613
36. GuS, Lillicrap T,Sutskever I, Levine S.Continuous deep Q-learning with model-based
acceleration.In: Proceedings of the 33rd InternationalConference on Machine Learning. 2016. p.
2829‚Äì38.https://proceedings.mlr.press/v48/gu16.html
37. HaD, Schmidhuber J. Worldmodels. arXiv preprint 2018.https://doi.org/abs/1803.10122
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 27/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 28 ‚Äî #28
PLOS One FreeEnergy Projective Simulation (FEPS)
38. MendoncaR, Rybkin O, Daniilidis K, HafnerD, Pathak D. Discovering and achievinggoals via
worldmodels. In: Advances in Neural InformationProcessing Systems. 2021. p. 24379‚Äì91.
https://proceedings.neurips.cc/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-
Abstract.html
39. HafnerD, Lillicrap T,Ba J, Norouzi M.Dream to control: learning behaviors bylatent imagination.
arXivpreprint 2020.http://arxiv.org/abs/1912.01603
40. KaiserL, Babaeizadeh M, Milos P,OsinskiB, Campbell RH, Czechowski K. Model-based
reinforcementlearning for Atari. arXiv preprint 2024.http://arxiv.org/abs/1903.00374
41. LadoszP,WengL,Kim M, Oh H. Exploration indeep reinforcement learning: a survey.Information
Fusion.2022;85:1‚Äì22. https://doi.org/10.1016/j.inffus.2022.03.003
42. SchmidhuberJ. Curious model-building control systems. In:Proceedings of the 1991 IEEE
InternationalJoint Conference on Neural Networks. 1991.p. 1458‚Äì63.
https://ieeexplore.ieee.org/document/170605/
43. OudeyerP-Y,Kaplan F.What is intrinsic motivation?a typology of computational approaches.
FrontNeurorobot. 2007;1:6.https://doi.org/10.3389/neuro.12.006.2007PMID:18958277
44. AubretA, Matignon L, Hassas S. Asurvey on intrinsic motivation in reinforcementlearning. arXiv
preprint2019. http://arxiv.org/abs/1908.06976
45. PathakD, Agrawal P,Efros AA, DarrellT.Curiosity-driven exploration by self-supervised
prediction.In: 2017 IEEE Conference on ComputerVisionandPattern Recognition Workshops
(CVPRW).Honolulu, HI,USA: IEEE; 2017. p. 488‚Äì9.
http://ieeexplore.ieee.org/document/8014804/
46. KlyubinAS, Polani D, Nehaniv CL. Empowerment:a universal agent-centric measure of control.In:
2005IEEE Congress on Evolutionary Computation. p.128‚Äì35.
https://doi.org/10.1109/cec.2005.1554676
47. MohamedS, Rezende DJ. Variationalinformation maximisation for intrinsicallymotivated
reinforcementlearning. arXiv preprint 2015.http://arxiv.org/abs/1509.08731
48. GregorK, Rezende DJ, Wierstra D. Variationalintrinsic control.arXiv preprint 2016.
http://arxiv.org/abs/1611.07507
49. KimH, Kim J, Jeong Y,LevineS, Song HO. EMI: exploration withmutualinformation. arXiv
preprint2019. http://arxiv.org/abs/1810.01176
50. Ueltzh√∂fferK. Deepactive inference. Biol Cybern. 2018;112(6):547‚Äì73.
https://doi.org/10.1007/s00422-018-0785-7 PMID:30350226
51. ShinJY,Kim C, Hwang HJ. Priorpreference learning from experts: designing a rewardwith active
inference.arXiv preprint 2021.http://arxiv.org/abs/2101.08937
52. SchwartenbeckP,Passecker J, Hauser TU, FitzGeraldTH, Kronbichler M, Friston KJ.
Computationalmechanisms of curiosity and goal-directed exploration.Elife. 2019;8:e41703.
https://doi.org/10.7554/eLife.41703PMID:31074743
53. SajidN, TigasP,Friston K. Active inference, preference learningand adaptive behaviour.IOP
ConfSer: Mater Sci Eng. 2022;1261(1):012020.https://doi.org/10.1088/1757-899x/1261/1/012020
54. PaulA, Sajid N, Da Costa L,Razi A. On efficient computation inactive inference. Expert Systems
withApplications. 2024;253:124315.https://doi.org/10.1016/j.eswa.2024.124315
55. MillidgeB. Deep active inference as variationalpolicy gradients. arXiv preprint 2019.
http://arxiv.org/abs/1907.03876
56. HeinsRC, Mirza MB, Parr T,Friston K,Kagan I, Pooresmaeili A. Deep activeinference and scene
construction.Front Artif Intell. 2020;3:509354.https://doi.org/10.3389/frai.2020.509354PMID:
33733195
57. SancaktarC, van Gerven MAJ, Lanillos P.End-to-end pixel-based deep active inference for body
perceptionand action. In: 2020 Joint IEEE10th International Conference on Development and
Learningand Epigenetic Robotics (ICDL-EpiRob); 2020. p.1‚Äì8.
https://ieeexplore.ieee.org/document/9278105/?arnumber=9278105
58. TschantzA,Baltieri M, Seth AK, Buckley CL.Scaling active inference. arXiv preprint 2019.
http://arxiv.org/abs/1911.10601
59. HimstO v d, Lanillos P.Deepactive inference for partially observable mdps. arXivpreprint 2020.
http://arxiv.org/abs/2009.03622
60. CatalO, VerbelenT,Nauta J, Boom CD, Dhoedt B.Learning perception and planning with deep
activeinference. In: ICASSP 2020 - 2020IEEE International Conference on Acoustics, Speech
andSignal Processing (ICASSP). 2020.https://doi.org/10.1109/icassp40776.2020.9054364
61. MillidgeB, Buckley CL. Successor representation activeinference. arXiv preprint 2022.
http://arxiv.org/abs/2207.09897
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 28/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 29 ‚Äî #29
PLOS One FreeEnergy Projective Simulation (FEPS)
62. TolmanEC.Cognitive maps in rats and men.Psychol Rev.1948;55(4):189‚Äì208.
https://doi.org/10.1037/h0061626PMID:18870876
63. StachenfeldKL, Botvinick MM, Gershman SJ. Thehippocampus as a predictive map. Nat
Neurosci.2017;20(11):1643‚Äì53. https://doi.org/10.1038/nn.4650PMID:28967910
64. EkstromAD, Ranganath C. Space, time, andepisodic memory: the hippocampus is allover the
cognitivemap. Hippocampus. 2018;28(9):680‚Äì7.https://doi.org/10.1002/hipo.22750PMID:
28609014
65. BehrensTEJ, Muller TH, Whittington JCR, MarkS, Baram AB, Stachenfeld KL, etal. What is a
cognitivemap? organizing knowledge for flexible behavior.Neuron. 2018;100(2):490‚Äì509.
https://doi.org/10.1016/j.neuron.2018.10.002PMID:30359611
66. RueckemannJW,SosaM, Giocomo LM, BuffaloEA. The grid codefor ordered experience. Nat
RevNeurosci. 2021;22(10):637‚Äì49.https://doi.org/10.1038/s41583-021-00499-9 PMID:34453151
67. MomennejadI. Memory,space, and planning: multiscale predictive representations.arXiv preprint
2024.http://arxiv.org/abs/2401.09491
68. DedieuA, Gothoskar N, Swingle S, LehrachW,L√°zaro-GredillaM, George D. Learning
higher-ordersequential structure with cloned HMMs. arXivpreprint 2019.
http://arxiv.org/abs/1905.00507
69. GeorgeD, Rikhye RV,GothoskarN, Guntupalli JS, Dedieu A, L√°zaro-GredillaM. Clone-structured
graphrepresentations enable flexible learning and vicariousevaluation of cognitive maps. Nat
Commun.2021;12(1):2392. https://doi.org/10.1038/s41467-021-22559-5 PMID:33888694
70. GuntupalliJS, Raju RV,KushagraS, WendelkenC, Sawyer D, Deshpande I. Graphschemas as
abstractionsfor transfer learning, inference, and planning.arXiv preprint 2023.
http://arxiv.org/abs/2302.07350
71. deTinguyD,VerbelenT,DhoedtB. Exploring and learning structure: activeinference approach in
navigationalagents. In: Buckley CL, Cialfi D,Lanillos P,Pitliya RJ, Sajid N, ShimazakiH, editors.
ActiveInference. Cham: Springer; 2025. p. 105‚Äì18.
72. WhittingtonJCR, McCaffaryD, Bakermans JJW,Behrens TEJ. How to builda cognitive map. Nat
Neurosci.2022;25(10):1257‚Äì72. https://doi.org/10.1038/s41593-022-01153-y PMID:36163284
73. WhittingtonJCR, Muller TH, Mark S, ChenG, Barry C, Burgess N, etal. The Tolman-Eichenbaum
machine:unifying space and relational memory throughgeneralization in the hippocampal
formation.Cell. 2020;183(5):1249-1263.e23.https://doi.org/10.1016/j.cell.2020.10.024PMID:
33181068
74. MillidgeB, TschantzA, Buckley CL. Whence theexpected free energy? arXiv preprint 2020.
http://arxiv.org/abs/2004.08128
75. DaCosta L, Parr T,Sajid N, VeselicS, NeacsuV,Friston K. Active inference on discrete
state-spaces:a synthesis. J Math Psychol. 2020;99:102447.
https://doi.org/10.1016/j.jmp.2020.102447PMID:33343039
76. FristonK, Da Costa L, Hafner D,Hesp C, Parr T.Sophisticated inference. NeuralComputation.
2021;33(3):713‚Äì63.
77. MelnikovAA, Makmal A, Briegel HJ. Projectivesimulation applied to the grid-world andthe
mountain-carproblem. arXiv preprint 2014.http://arxiv.org/abs/1405.5459
78. MautnerJ, Makmal A, Manzano D, TierschM, BriegelHJ. Projective simulation for classical
learningagents: a comprehensive investigation. New GenerComput. 2015;33(1):69‚Äì114.
https://doi.org/10.1007/s00354-015-0102-0
79. MelnikovAA, Makmal A, Dunjko V,Briegel HJ.Projective simulation with generalization. Sci Rep.
2017;7(1):14430.https://doi.org/10.1038/s41598-017-14740-y PMID:29089575
80. LeMaitrePA,Krumm M, Briegel HJ. Multi-excitation projective simulationwith a many-body
physicsinspired inductive bias. arXiv preprint 2024.http://arxiv.org/abs/2402.10192
81. JerbiS, TrenkwalderLM, Poulsen Nautrup H, Briegel HJ,Dunjko V.Quantum enhancements for
deepreinforcement learning in large spaces. PRXQuantum.
2021;2(1):010328.https://doi.org/10.1103/prxquantum.2.010328
82. HanglS, Dunjko V,Briegel HJ, Piater J.Skill learning by autonomous robotic playingusing active
learningand exploratory behavior composition. Front RobotAI. 2020;7:42.
https://doi.org/10.3389/frobt.2020.00042PMID:33501210
83. L√≥pez-InceraA, Ried K, M√ºller T,Briegel HJ.Development of swarm behavior in artificiallearning
agentsthat adapt to differentforaging environments. PLoS One.2020;15(12):e0243628.
https://doi.org/10.1371/journal.pone.0243628PMID:33338066
84. L√≥pez-InceraA, Nouvian M, Ried K, M√ºllerT,Briegel HJ. Honeybee communication during
collectivedefence is shaped by predation. BMCBiol. 2021;19(1):106.
https://doi.org/10.1186/s12915-021-01028-x PMID:34030690
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 29/30
ID: pone.0331047 ‚Äî 2025/8/29 ‚Äî page 30 ‚Äî #30
PLOS One FreeEnergy Projective Simulation (FEPS)
85. TierschM, GanahlEJ, Briegel HJ. Adaptive quantum computationin changing environments using
projectivesimulation. Sci Rep. 2015;5:12874.https://doi.org/10.1038/srep12874PMID:26260263
86. TrenkwalderLM, L√≥pez-InceraA, Nautrup HP,Flamini F,Briegel HJ. Automatedgadget discovery
inscience. arXiv preprint 2022.http://arxiv.org/abs/2212.12743
87. FristonK, Thornton C, Clark A. Free-energyminimization and the dark-room problem. Front
Psychol.2012;3:130. https://doi.org/10.3389/fpsyg.2012.00130PMID:22586414
88. √Östr√∂mKJ. Optimal control of Markov processeswith incomplete state information. Journal of
MathematicalAnalysis and Applications. 1965;10(1):174‚Äì205.
https://doi.org/10.1016/0022-247x(65)90154-x
89. SilverD, VenessJ. Monte-Carlo planning in large POMDPs.In: Advances in Neural Information
ProcessingSystems. 2010.https:
//papers.nips.cc/paper_files/paper/2010/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html
90. RussekEM, Momennejad I, Botvinick MM, GershmanSJ, Daw ND. Predictive representations can
linkmodel-based reinforcement learning to model-free mechanisms.PLoS Comput Biol.
2017;13(9):e1005768.https://doi.org/10.1371/journal.pcbi.1005768PMID:28945743
91. MomennejadI, Russek EM, Cheong JH, BotvinickMM, Daw ND, Gershman SJ. Thesuccessor
representationin human reinforcement learning. Nat HumBehav.2017;1(9):680‚Äì92.
https://doi.org/10.1038/s41562-017-0180-8 PMID:31024137
92. RobertsWA.Areanimals stuck in time? Psychol Bull.2002;128(3):473‚Äì89.
https://doi.org/10.1037/0033-2909.128.3.473 PMID:12002698
93. WeiR. Valueof informationand reward specification in active inferenceand POMDPs; 2024.
http://arxiv.org/abs/2408.06542
94. ThompsonRF,Thompson JK, Kim JJ, Krupa DJ,Shinkman PG. The nature of reinforcementin
cerebellarlearning. Neurobiology of Learning and Memory.1998;70(1‚Äì2):150‚Äì76.
https://doi.org/10.1006/nlme.1998.3845
95. WagnerMJ, LuoL. Neocortex-cerebellum circuits for cognitive processing.TrendsNeurosci.
2020;43(1):42‚Äì54.https://doi.org/10.1016/j.tins.2019.11.002PMID:31787351
96. RaoRP,Ballard DH. Predictive coding inthe visual cortex: a functional interpretation ofsome
extra-classicalreceptive-field effects.Nat Neurosci. 1999;2(1):79‚Äì87.https://doi.org/10.1038/4580
PMID:10195184
97. KimY,Nam W,Kim H, Kim JH, KimG. Curiosity-bottleneck: exploration by distilling task-specific
novelty.In:Proceedings of the 36th International Conference onMachine Learning. 2019. p.
3379‚Äì88.https://proceedings.mlr.press/v97/kim19c.html
98. BellemareM, Srinivasan S, Ostrovski G, SchaulT,Saxton D, Munos R. Unifying count-based
explorationand intrinsic motivation. In: Advances inNeural Information Processing Systems. 2016.
https://proceedings.neurips.cc/paper_files/paper/2016/hash/
afda332245e2af431fb7b672a68b659d-Abstract.html
99. BarnettN, Crutchfield JP.Computational mechanics ofinput‚Äìoutputprocesses: structured
transformationsand theùúñ-transducer.JStat Phys. 2015;161(2):404‚Äì51.
https://doi.org/10.1007/s10955-015-1327-5
100. MomennejadI. A rubric for human-like agentsand NeuroAI. Philos TransR Soc Lond BBiol Sci.
2023;378(1869):20210446.https://doi.org/10.1098/rstb.2021.0446PMID:36511409
PLOSOne https://doi.org/10.1371/journal.pone.0331047 September4, 2025 30/30