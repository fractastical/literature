1
Active Privacy-Utility Trade-off Against
Inference in Time-Series Data Sharing
Ecenaz Erdemir, Student Member, IEEE, Pier Luigi Dragotti, Fellow, IEEE,
and Deniz G ¨und¨uz, Fellow, IEEE
Abstract— Internet of things (IoT) devices, such as smart
meters, smart speakers and activity monitors, have become highly
popular thanks to the services they offer. However, in addition
to their many beneﬁts, they raise privacy concerns since they
share ﬁne-grained time-series user data with untrusted third
parties. In this work, we consider a user releasing her data
containing personal information in return of a service from
an honest-but-curious service provider (SP). We model user’s
personal information as two correlated random variables (r.v.’s),
one of them, called the secret variable, is to be kept private,
while the other, called the useful variable, is to be disclosed for
utility. We consider active sequential data release, where at each
time step the user chooses from among a ﬁnite set of release
mechanisms, each revealing some information about the user’s
personal information, i.e., the true values of the r.v.’s, albeit with
different statistics. The user manages data release in an online
fashion such that the maximum amount of information is revealed
about the latent useful variable as quickly as possible, while the
conﬁdence for the sensitive variable is kept below a predeﬁned
level. For privacy measure, we consider both the probability of
correctly detecting the true value of the secret and the mutual
information (MI) between the secret and the released data.
We formulate both problems as partially observable Markov
decision processes (POMDPs), and numerically solve them by
advantage actor-critic (A2C) deep reinforcement learning (DRL).
We evaluate the privacy-utility trade-off (PUT) of the proposed
policies on both the synthetic data and smoking activity dataset,
and show their validity by testing the activity detection accuracy
of the SP modeled by a long short-term memory (LSTM) neural
network.
Index Terms—Inference privacy, time-series privacy, privacy
funnel, active learning, actor-critic deep reinforcement learning,
human activity recognition.
I. I NTRODUCTION
R
ECENT advances in Internet of things (IoT) devices and
services have increased their usage in a wide range of
areas, such as health and activity monitoring, location-based
services, smart speakers and smart metering. Moreover, most
service providers encourage the users to share their personal
data in return for better user experience. For instance, the users
can beneﬁt from personalized dietary tips as a result of sharing
their Fitbit activity, while they can receive hotel, restaurant or
bar recommendations if they share their location. However,
in most of these applications, data collected by IoT devices
contain sensitive personal information about the users. The
concerning fact is that as soon as the user’s raw data is sent
The authors are with the Department of Electrical and Electronic En-
gineering, Imperial College London, London SW7 2AZ, U.K., (e-mail:
{e.erdemir17, p.dragotti, d.gunduz }@imperial.ac.uk).
This work was partially supported by the the European Research
Council through project BEACON (grant number 677854) and UK EP-
SRC through CHIST-ERA project CONNECT (CHISTERA-18-SDCDN-001,
EPSRC-EP/T023600/1).
to the service provider’s cloud, the sensitive information can
be inferred, misused or leaked through security vulnerabilities
even if the service provider and/or the communication link
are trusted third parties. For example, chronic illnesses, dis-
abilities, daily habits and psychological state can be revealed
by health monitoring systems [1], [2], while presence at
home and states of home appliances can be inferred from
collected smart meter (SM) data [3]. Hence, privacy is an
important concern for the adoption of many IoT services,
and there is a growing demand from consumers to keep their
personal information private against malicious attackers and/or
untrusted service providers (SPs), while preserving the utility
obtained from these IoT services. Privacy has been widely
studied in the literature [4]–[15], and a vast number of privacy
measures have been introduced, including differential privacy
[4], mutual information (MI) [6]–[12], total variation distance
[16], maximal leakage [17], [18], and guessing leakage [19],
to count a few. In this paper, we study the privacy-utility trade-
off (PUT) of time-series data sharing in the presence of a third
party which tries to infer the user’s sensitive information from
the released data.
A. Related Work
Privacy for time-series data sharing and its applications
to various domains have been extensively studied [8]–[11],
[15], [20]–[31]. Most of these works focus on protecting
the privacy of a single data point, e.g., the current mea-
surement [23]–[28]. However, causal relations in time-series
data require taking into account more than single data point
privacy. Individual measurements taken at each time instance,
such as electrocardiogram (ECG), body temperature, physical
activity, location, weather forecast, account balance and SM
readings, are temporally correlated and the strategies focusing
on the privacy of a single data point might reveal sensitive
information about the past or future measurements.
Among those that consider temporal correlations, most
existing works focus on the privacy of the time-series mea-
surements rather than hiding latent sensitive attributes [9],
[10], [26]–[30]. In the location sharing scenario, sensitive
information is the time-series data itself and the utility loss
can be measured by data distortion, whereas in many appli-
cations, the user might be interested in hiding an underlying
sensitive hypothesis. For instance, the user’s presence at home
or favorite TV channel can be inferred from SM readings,
while her sensitive daily habits can be revealed to the SP
through the sensors of a wearable device. Inference privacy
protects user’s data from an adversary’s attempt to deduce
sensitive information from an underlying distribution [12],
arXiv:2202.05833v1  [cs.IT]  11 Feb 2022
2
[15], [19], [32]–[36]. These techniques perform well against
inference attacks, in which the adversary aims at detecting
the user’s underlying private information with high conﬁdence
[30]. PUT between two correlated sensitive and useful r.v.’s
has also been studied under privacy funnel [32], which is
closely related to information bottleneck introduced in [37].
In privacy funnel approaches [12], [19], [32]–[36], the goal is
to conceal the sensitive information from SP’s inference while
gaining enough utility from the useful information, where
both the utility and the privacy leakage are measured by MI.
However, [12], [32]–[34] consider independent data without
temporal correlations, hence, these approaches are not suitable
for temporally correlated time-series data.
Differential privacy (DP), k-anonymity, information theo-
retic metrics and the SP’s error probability are commonly
used as privacy measures [8], [9], [11], [15], [20]–[31], [38].
By deﬁnition, DP prevents the SP from inferring the current
data of the user, even if the SP has the knowledge of all
the remaining data points. K-anonymity ensures that sensitive
data is indistinguishable from at least k−1 other data points.
However, DP and k-anonymity are meant to ensure the privacy
of a single point in a time-series and do not consider temporal
correlations. As an intermediate framework between complete
independence and complete correlation, pufferﬁsh privacy con-
siders low temporal correlations in time-series [36]. However,
the mechanism focuses on the privacy around the possible
current data, which might ignore the inference from future
and past values.
In the literature, several papers on DP consider temporal
correlations [4]. However, these usually follow myopic corre-
lations with the current data due to the utility loss concerns
raising from high noise. For example, in [22], physiological
measurements are obfuscated before reporting to an SP for
PUT. Instead of the entire time-series history, a selected
temporal section of the sensor data is considered, and solved
by using dynamic programming (DyP) and greedy algorithm.
In [27], DP in a SM with a rechargeable battery is achieved by
adding noise to the meter readings before reporting to an SP.
In order to guarantee DP, the perturbation must be independent
of the battery state of charge. However, for a ﬁnite capacity
battery, the energy management system cannot provide the
amount of noise required for preserving privacy.
Information-theoretic privacy (ITP) considers the statistics
of the entire time-series in terms of temporal correlations,
and study privacy mechanisms that allow arbitrary stochastic
transformations of data samples. However, DP considers only a
single data point privacy. This disadvantage of DP is partially
eliminated by group DP and pufferﬁsh privacy, which take
some degree of temporal correlations into account by con-
sidering DP of multiple neighboring data points and adding
a ﬁxed type of i.i.d. random noise to the samples. However,
these approaches lead to signiﬁcant utility loss, since each
speciﬁc privacy mechanism in DP limits the type of noise
added, e.g., Gaussian, Laplacian, and etc. On the other hand,
ITP has no such restriction in the stochastic transformations
that are applied to the data samples. This is one of the biggest
advantage of ITP over DP and pufferﬁsh privacy where only
some degree of temporal correlations are taken into account,
and a ﬁxed type of i.i.d. random noise is added for privacy.
In [39], an SM system is considered assuming Markovian
energy demands. Privacy is achieved by ﬁltering the energy
demand with the help of a rechargeable battery. ITP problem is
formulated as an MDP, and the minimum leakage is obtained
numerically through DyP, while a single-letter expression is
obtained for an i.i.d. demand. This approach is extended to the
scenario with a renewable energy source in [31]. In [40], PUT
is examined with a rechargeable battery. Due to Markovian
demand and price processes, the problem is formulated as a
partially observable MDP with belief-dependent rewards ( ρ-
POMDP), and solved by DyP for inﬁnite-horizon. In [11],
PUT is characterized numerically by DyP for a special energy
generation process.
In [35], PUT of time-series data is considered in both
online and ofﬂine settings. A user continuously releases data
samples which are correlated with its private information, and
in return obtains utility from an SP. The proposed schemes
are cast as convex optimization problems and solved under
hidden Markov model assumption. The simulation results are
provided for binary time-series data for a ﬁnite time horizon.
However, the dimensions of the optimization problems in both
schemes grow exponentially with time and the number of
sample states. Therefore, in a setting when ﬁne-grained sensor
data is considered for a long time horizon, computational
complexity of the proposed schemes is very high.
ITP and utility for location sharing is studied in [9], and
extended to generic time-series data release in [10]. The user
follows a history-dependent online data release policy by
minimizing the MI between the real and modiﬁed location
trajectories subject to a distortion constraint. Effectiveness
of the proposed approach against myopic policies and its
application to GeoLife GPS trajectory dataset are presented
through numerical simulations.
Privacy metrics based on the SP’s error probability focus
on concealing the true realization of the sensitive information.
In [13], the goal is to increase the ﬁdelity of the shared
data quantiﬁed through an additive distortion measure, while
guaranteeing privacy in an online manner. Privacy leakage is
measured by the error probability of the SP in detecting the
distribution of the underlying data samples. In [15], the user
shares her time-series data, which intrinsically contains corre-
lated sensitive and useful information, with an untrusted SP in
an online fashion. The goal is to maximize the conﬁdence in
the true useful variable for utility, while keeping the conﬁdence
in the sensitive r.v. below a pre-deﬁned level.
B. Contributions
In this paper, we consider an active learning scenario for
PUT against an honest-but-curious SP. We assume that a user
wants to share the “useful” part of her data with the SP.
However, the SP might also deduce user’s “secret” information
from the shared time-series data (e.g., location, heartbeat,
temperature or energy consumption). We model the user’s
secret and useful data as correlated discrete r.v.’s. The user’s
goal is to prevent the secret from being accurately detected by
the SP while revealing the useful data accurately for utility.
Differently from the existing works [6], [7], [16], [17],
[19], [32], [34], which typically consider a time-independent
3
data release problem, we consider a discrete time system,
and assume that the user can actively choose from among
a ﬁnite number of data release mechanisms (DRMs) at each
time. While each measurement reveals some information about
user’s latent states, we assume that each DRM has different
measurement characteristics, i.e., conditional probability dis-
tributions. User’s objective is to choose a DRM at each time
in an online fashion to reveal the value of the useful r.v. as
quickly as possible to maximize her utility while keeping the
leakage of the sensitive information below a prescribed value.
As privacy measure, we consider both the SP’s conﬁdence
in the secret and the MI between the secret r.v. and the
observations.
Our problem is similar to time-series data privacy in the
literature [8]–[11], [28], [29], [31], where the objective is
to minimize privacy leakage by modifying the original time-
series data while constraining the utility loss. However, in
this work, the user selects from among multiple DRMs in an
online fashion rather than modifying the non-causally available
time-series data. Similar time-series data release problems
are also considered in [13], [15] and [41]. However, [13]
considers the PUT of a binary secret r.v. in an asymptotic
regime, while [41] considers binary as well as M-ary r.v.’s for
an ofﬂine scenario using semi-deﬁnite programming, which
has high computational complexity when ﬁne-grained data is
considered. Data release history is taken into account for M-
ary r.v.’s in [15]; however, time aspect is not considered in the
PUT objective.
We introduce a sequential data release policy that minimizes
the SP’s error probability on the useful r.v. subject to a
constraint on the SP’s conﬁdence in the true secret. Besides
conﬁdence-based privacy, we also consider MI-based privacy
which keeps the total MI between the secret r.v. and the shared
data below a certain level. Note that MI-based privacy does
not necessarily prevent the detection of the true secret value;
instead, it limits the information leakage in an average sense.
We consider data release policies which take the entire
release history into account, and recast the problem under both
privacy measures as a POMDP. POMDPs can be represented as
continuous state belief-MDPs; however, ﬁnding optimal poli-
cies for continuous state and action spaces is a PSPACE-hard
problem [42]. Solving these MDPs with ﬁne discretization
causes an increase in the state space; hence, the complexity.
Therefore, after identifying the structure of the optimal policy,
we use a method called advantage actor-critic (A2C) deep
reinforcement learning (DRL) to evaluate our continuous state
and action probability space MDP numerically. We also use
variational representations for MI estimation through neural
networks. Finally, we examine the performances of the pro-
posed policies in human activity privacy scenario, in which we
use both synthetic data and smartwatch sensor readings from
smoking activity dataset [43]. We compare the privacy levels
achieved by the proposed policies using an SP that predicts
the true values for useful data and secret from the shared
observation history. The SP is represented by a long short-
term memory (LSTM) neural network.
Our contributions are summarized as follows:
• We propose an active learning framework for PUT in
Secret, 
Utility, 
DRM 1 
DRM 2 
DRM 3 
Service
Provider 
Cloud 
User's Goal 
Low 
Confidence 
on Secret,  
High 
Confidence 
on Utility,  
Fig. 1: System model for active PUT against the SP.
online sharing of time-series data.
• We propose a data sharing policy for optimal PUT against
an SP performing sequential Bayesian inference.
• We propose another policy based on privacy measured
by MI between the sensitive information and the released
data history against average-case adversaries.
• We recast the active time-series data release problem for
PUT as a POMDP, and evaluate both policies numerically
using A2C-DRL for human activity privacy application.
The remainder of the paper is organized as follows. We
present the problem formulation and POMDP approach in Sec-
tions II and III, respectively. MI-based privacy is introduced
in Section IV, and data-driven evaluation for human activity
privacy is presented in Section V-C. Finally, we conclude our
work in Section VI.
II. P ROBLEM STATEMENT
We consider a user that wants to share her data with an
honest-but-curious SP in return of utility. The data reveals in-
formation about two underlying latent variables; one represents
the user’s sensitive information, called the secret, while the
other is non-sensitive useful part, and is intentionally disclosed
for utility. The user’s goal is to release her data such that
the SP can quickly detect the non-sensitive information with
minimum error, while keeping his conﬁdence in the secret r.v.
below a predeﬁned level.
Fig. 1 shows an illustration of the system model with three
DRMs. Let S= {0,1,...,N −1}and U= {0,1,...,M −1}
be the ﬁnite sets of the hypotheses represented by the r.v.’s
S ∈ Sfor the secret and U ∈ Ufor the non-sensitive
useful information, respectively. Consider a ﬁnite set Aof
different DRMs available to the user, each modeled with a
different statistical relation with the underlying hypotheses.
For example, in the case of a user sharing activity data, e.g.,
Fitbit records, set A may correspond to different types of
sensor measurements the user may share. Useful information
the user wants to share may be the exercise type, while the
sensitive information can be various daily habits. Similarly,
in the case of smart meter readings, the useful information
might be ON/OFF state of home appliances for smart power
scheduling whereas the sensitive information might be the
types of TV channels the user watches. We assume that the
data revealed at time t, Zt, is generated by an independent re-
alization of a conditional probability distribution that depends
on the true hypotheses and the chosen DRM At ∈A, denoted
by q(Zt|At,S,U ).
The user’s goal is to disclose U through the released data
Zt, while keeping the SP’s conﬁdence in S below a certain
threshold. Let τ be the time that the SP is conﬁdent enough
4
about the true useful variable and makes a declaration. This
is also the time at which the user stops releasing data, since
U is already disclosed to the SP at the desired conﬁdence
level. The objective of the problem is to ﬁnd a sequence of
actions {A0,...,A τ−1}, a stochastic stopping time τ and a
declaration rule d: Aτ−1 ×Zτ−1 →U that collectively solve
the following optimization problem:
minimize
A0,...,A τ−1,d
E[τ] + λPerr(u)
subject to Ct(s) <LB,∀t≤τ,∀s∈S
(1)
where Perr(u) = P(d(Aτ−1,Zτ−1) ̸= u) is the error
probability of making wrong declaration for the true value
u∈U; Ct(s) is the SP’s instantaneous conﬁdence in the true
sensitive value s∈S, which is a probability distribution on s
given the observation history, i.e., P(S = s|Aτ−1,Zτ−1); LB
is a scalar of user’s choice; and the expectation is taken over
the action and observation distributions as well as the initial
distributions of the r.v.’s. Here, by adjusting λ, we can trade-
off between the speed of declaration and the SP’s accuracy.
For our theoretical results, we assume that the observation
statistics q(Zt|At,S,U ),∀At ∈A, and the employed DRM
At are known by both the user and the SP. Later, we will also
consider real datasets with unknown data distributions in our
simulations. To maximally confuse the SP, the user selects
action At with a probability distribution π(At|Zt−1,At−1)
conditioned on the SP’s observation history up to that time,
{Zt−1,At−1}. In this work, we assume that the true values s
and u are unknown to all the parties involved.
III. POMDP F ORMULATION
The above PUT can be recast as a POMDP with partially
observable static states {S,U}∈S×U , actions At ∈A∪{d},
and noisy observations Zt ∈Z. POMDPs can be reformulated
as belief-MDP with compact yet uncountable belief state, and
solved using classical MDP solution methods. We will follow
this approach, and introduce SP’s belief to determine the state
variable in three steps. Firstly, we deﬁne the belief of the SP
on S and U after he observes {Zt−1,At−1}by
βt(s,u) = P(S= s,U= u|Zt−1 = zt−1,At−1 = at−1) (2)
over the belief space P(B) := {βt ∈ [0,1]M×N :∑
s∈S,u∈Uβt(s,u) = 1 }, where the marginal beliefs are
represented by βt(u) := ∑
s∈Sβt(s,u) and βt(s) :=∑
u∈Uβt(s,u), respectively. The SP’s conﬁdence that S = s
at time t is represented by Ct(s) := βt(s). The user’s action
probabilities become conditioned on the belief distribution,
i.e., π(At = at|βt), while the observation probabilities are
the same as before. Secondly, we introduce a new state
FB := {max
s∈S
βt(s) ≥LB : βt ∈P(B)}for FB ⊆P(B), called
the forbidden-state, which represents the condition where the
constraint in (1) is violated. With slight abuse of notation, we
will use FB to denote both the forbidden state of the system
and the set of belief states that fall into this state. FB is ideally
an inﬁnite cost state; however, in practice, we assume it has
a large-cost. As the third step of deﬁning the state space, we
include a terminal state to fully characterize the state in which
the user stops sharing her data with the SP. We assume that
after the user makes the stopping decision, the system goes
to a terminal state, denoted by FT, and remains there forever.
This makes the problem an episodic MDP. Consequently, the
state space becomes X= P(B) ∪{FT}.
We always refer the time independent expression of belief,
i.e., β, as the current belief state. The optimal expected total
cost of our problem is deﬁned as follows:
Deﬁnition 1. For all β ∈P(B), let the optimal value function
V∗(β) represent the optimal expected cost of problem (1),
given the initial belief β. That is,
V∗(β) := min{E[τ] + λPerr(u)}, (3)
where the minimization is with respect to τ, DRMs, and the
declaration rule d.
Optimal expected total cost for active PUT against an SP
can be obtained by evaluating V∗at the initial belief. This can
be done by solving a DyP problem. After a single observation
{zt,at}, the SP updates its belief by Bayes’ rule as follows:
Φ(βt,zt,at) = q(zt|at,s,u )βt(s,u)∑
˜s,˜u
q(zt|at,˜s,˜u)βt(˜s,˜u), (4)
where the function Φ(βt,zt,at) represents the next belief state
βt+1(s,u) in terms of the current belief, the action and the
observation. We deﬁne a Markov operator Ta for action a,
such that for any measurable function V : P(B) →R,
(TaV)(β) :=
∫
V(Φ(β,z,a ))
∑
s,u
q(z|a,s,u )β(s,u)dz. (5)
For any state β ∈P(B), the user’s data release action a∈A
under the optimal policy results in an expected total cost of
1 + (TaV∗)(β), where time spent by the user for data release
is represented by cost 1, and (TaV∗)(β) is the expected future
value of V∗. On the other hand, the user’s stopping decision
d results in error probability of the declaration of true useful
value uwith penalty λ, i.e., λPerr(u) := λ(1−β(u)). Solution
for the optimal V∗ is formalized by the following theorem.
Theorem 1. [44] The optimal V∗for β ∈P(B) satisﬁes the
ﬁxed point equation:
V∗(β) = min{1 + min
a∈A
(TaV∗)(β),min
u∈U
λ(1 −β(u))}. (6)
Deﬁnition 2. Let a Markov stationary policy πbe a stochastic
kernel from the state space to the action space, including the
stopping action, which determines the stopping time τ, i.e.,
Π := P(B) →A∪{ d}. That is, the probability of choosing
DRM a under policy π at state β is denoted by π(a|β).
Following from Corollary 9.12.1 in [44], DyP equation (6)
characterizes the optimal deterministic stationary policy π∗for
β ∈P(B). The intuition behind Theorem 1 is that the user’s
data release action a∗ = arg mina∈ATa(V∗)(β) is the least
costly action with cost 1 + mina∈ATa(V∗)(β), unless choos-
ing the stopping action d and letting the SP make a decision
for u is less costly, i.e., λ(1 −β(u)). We also ensure that for
any two hypotheses u,u′∈U, u̸= u′, there exists an action
a ∈A, such that D(q(z|a,s,u )||q(z|a,s,u ′)) > 0,∀s ∈S,
5
where D (·||·) denotes the Kullback-Leibler (KL) divergence.
That is, hypotheses u and u′ are distinguishable all the time,
such that (1) has a meaningful solution.
Theorem 2. Suppose there exists a parameter CT > 0, e.g.,
time cost, and a functional V : P(B) →R+ such that for all
belief states β ∈P(B),
V(β) ≤min{CT+min
a∈A
(TaV∗)(β),min
u∈U
λCT(1−β(u))}. (7)
Then V∗(β) ≥ 1
CT
V(β) for all β ∈P(B).
See Appendix A for the proof of Theorem 2.
Theorem 2 provides a lower bound for a ﬁxed-point ex-
pression of V∗. However, it is difﬁcult to calculate the real
value of V∗ and solve the DyP equation over a continuous
belief space. Hence, we solve (1) numerically using an RL
approach to obtain a good approximation. Due to the belief-
based privacy constraint, we call our policy belief-privacy data
release policy (belief-PDRP), πB. We deﬁne an instantaneous
cost function for current state x and action a∈A∪{ d}as
cπB (x,a)=



1, if x= β ∈P(B)\FB,a ∈A
min
u∈U
(1 −β(u))λ, if x= β ∈P(B)\FB,a = d
CB, if x= FB,a ∈A
0, if x= FT.
The optimal policy π∗
B is induced as a result of the minimiza-
tion of cπB (x,a). The constraint on the SP’s conﬁdence in s
is enforced with a large instantaneous cost CB for reaching
state FB, which is ideally inﬁnite. Assuming that the system
follows the optimal policy, data release actions resulting in a
transition to FB with a large-cost CB would not be selected
by the minimization problem. See the proof of Theorem 2
in Appendix A. The overall strategy for belief update is
represented by the Bayes’ operator as follows:
ΦπB (x,z,a ) =



ΦπB (β,z,a ), if x= β ∈P(B),a ∈A
FT, if x= β ∈P(B),a = d
FT, if x= FT.
Since the user has access to all the information that the SP
has, it can perfectly track his beliefs. Hence, the user decides
her own policy facilitating the SP’s detection strategy, episodic
behavior and belief.
According to her strategy, the user checks whether the
selected optimal action is the stopping action d. If so, she
receives a cost determined by the current error probability of
u with penalty λ, then transitions to the terminal state and
ends the episode. If not, she checks whether the SP’s belief
on any secret exceeds LB. If the user is in the forbidden-state
she receives a large-cost CB; otherwise, either she receives a
time cost 1 or terminal state cost 0 depending on her state. If
the terminal state has not already been reached and stopping
action has not been taken at the moment, the user updates the
SP’s belief as in (4); otherwise she updates the state to the
ﬁnal state x= FT. Using the condition (7) in Theorem 2, we
write the Bellman equation induced by the optimal policy π∗
B
as [45],
V(x) = min
a∈A∪{d}
{cπB (x,a) + E[V(ΦπB (x,z,a ))]},∀x∈P(B).
(8)The objective is to ﬁnd a policy π∗
B that optimizes the cost
function. The proposed POMDP has a continuous state space
due to belief state and continuous action probabilities. Finding
optimal policies for continuous state and action is PSPACE-
hard [42]. In practice, to solve them by classical ﬁnite-
state MDP methods, e.g., value iteration, policy iteration and
gradient-based methods, belief discretization is required [46].
While a ﬁner discretization gets closer to the optimal solution,
it expands the state space; hence, the problem complexity.
Hence, we use A2C-DRL to numerically solve the continuous
state and action space MDP in Section V-C.
In addition to the conﬁdence-based privacy, we also consider
a MI privacy policy in Section IV.
IV. MI AS PRIVACY CONSTRAINT
In this section, we consider a scenario, in which the user
is interested in limiting the information leakage about the
sensitive information in an average sense, rather than hiding
its true value. For instance, the SP might be confused about
the true secret; however, he might still have an idea about
which secret values are unlikely. More concretely, consider a
secret r.v. with alphabet size of three, e.g., U= {1,2,3}. From
the perspective of conﬁdence, the belief of β(U = 1) = 1/2,
β(U = 2) = 1 /4, β(U = 3) = 1 /4 would be the same
as β(U = 1) = 1 /2, β(U = 2) = 1 /2, β(U = 3) = 0 .
While the latter clearly has additional information about the
secret resulting in reduced uncertainty. We tackle this issue by
measuring the privacy by the MI between the secret variable
S and the observation history {Zt,At}for t≤τ. According
to her policy, the user wants to minimize the error on useful
information as quickly as possible while keeping the total MI
between the secret and the observations below a prescribed
level, i.e., ∀Z ∈Z and ∀A∈A,
minimize E[τ] + λPerr(u) (9)
s.t. I (S; Zt,At) <LMI, ∀t≤τ,∀S ∈S
where LMI is a scalar of the user’s choice.
MI is commonly used both as a privacy and a utility measure
in the literature [8], [10], [32]. Here, it is used as a privacy
measure to control PUT between the useful variable and the
secret. Due to the MI-based privacy constraint in (9), we call
this policy MI-privacy data release policy (MI-PDRP), πMI.
MI between S and (ZT,AT) over time T is given by
I(S; ZT,AT) =
T∑
t=1
I(S; Zt,At|Zt−1,At−1). (10)
Theorem 3. The instantaneous MI cost between the secret
and the observations induced by policy πMI at time t can be
written as:
IπMI (S; Zt,At|β) = −
∑
s,u,zt,at
q(zt|at,s,u )π(at|β)β(s,u)
×log
∑
˜u
q(zt|at,s, ˜u)π(at|β)β(s,˜u)
β(s) ∑
¯s,¯u
q(zt|at,¯s,¯u)π(at|β)β(¯s,¯u). (11)
6
See Appendix B for the proof.
As before, we deﬁne the state in three stages, i.e., the belief,
the forbidden-MI-state as FMI := {βt(s) : IπMI (S; Zt,At) ≥
LMI,∀t ≤ τ} for FMI ⊆ P(B), where the constraint in
(9) is violated, and the ﬁnal state FT in which the episode
terminates. As before, we will use FMI to denote both the
forbidden state and the set of forbidden states for convenience.
We deﬁne an instantaneous cost function, cπMI (x,a), for
current state x∈X = P(B) ∪{FT}and action a∈A∪{ d},
which induces the optimal MI-PDRP π∗
MI when minimized:
cπMI (x,a)=



1, if x=β ∈P(B)\FMI,a ∈A
min
u∈U
(1 −β(u))λ,if x=β ∈P(B)\FMI,a = d
CMI, if x=FMI,a ∈A
0, if x=FT.
The constraint on the total MI leakage from Sis enforced with
a large-cost CMI for state FMI. Assuming that the system
follows the optimal MI-PDRP π∗
MI, FMI would not be visited
at all. The overall strategy for belief update is represented by
the Bayes’ operator as follows:
ΦπMI (x,z,a )=



ΦπMI (β,z,a ),if x= β ∈P(B),a ∈A,
FT, if x= β ∈P(B),a = d,
FMI, if x= β(s,u) ∈P(B)
t∑
i=1
Iπ(S; Zi,Ai|βi) ≥LMI,
FT, if x= FT.
Theorem 2 holds for (9) when we replace {cπB ,ΦπB ,FB}
with {cπMI ,ΦπMI ,FMI}, and provides a lower bound for the
value function V∗for all β ∈P(B). Hence, to ﬁnd the policy
π∗
MI, we solve the Bellman equation (8) using RL for cπB and
ΦπB . This policy minimizes the SP’s error on the true value
of u in the quickest way while constraining the MI leakage
from not only true secret s but all possible values for S.
A. Estimating MI
Exact computation of MI is possible when the data dis-
tribution is known. However, in most practical scenarios, the
user’s data distribution is not known or it is inaccurate. Hence,
we approximate I(S; ZT,AT) via a variational representation
which is inspired by Barber-Agakov MI estimation for single
letter MI [47]. Since (10) is history-dependent, we modify this
variational bound to a history dependent expression as follows:
I(S; Zt,At|Zt−1,At−1)
= H(S|Zt−1,At−1) −H(S|Zt,At) (12)
= H(S|Zt−1,At−1) + D(P(S|Zt,At)||Q(S|Zt,At))
+ E[log Q(S|Zt,At)] (13)
= H(S|Zt−1,At−1) + max
Q(S|Zt,At)
E[log Q(S|Zt,At)]
(14)
where (12) follows from the deﬁnition of MI, (13) holds
for any distribution Q(S|Zt,At) over S given the values
in Zt ×At, which represents what the belief would be
Fig. 2: Activity recognition with wearable IoT devices does
not only infer physical exercise but also sensitive daily habits.
after observing (At,Zt), and (14) follows from the fact that
maximum is attained when Q(S|Zt,At) = P(S|Zt,At).
Given (Zt−1,At−1) = ( zt−1,at−1), we can rewrite the
variational representation for the MI conditioned on the neural
estimation of the current belief ˆβ(S) = Q(S|Zt−1,At−1) as
I(S; Zt,At|ˆβ)=H( ˆβ(S)) + max
Q(S|Zt,At,ˆβ)
E[log Q(S|Zt,At,ˆβ)], (15)
where H( ˆβ(S)) = −∑
s∈S
ˆβ(s) log ˆβ(s), and the expectation is
with respect to (S,Zt,At) ∼ ˆβ(S),π(At|ˆβ),q(Zt|At,S,U ).
Since the current belief realization is known to both the user
and the SP, H( ˆβ(S)) is a constant. Numerical estimation of
the MI via neural networks is explained in Section V-C2.
V. N UMERICAL RESULTS
In this section, we present our results for both synthetic data
and human activity privacy use-cases. In the former we assume
that all the distributions of the DRM are known by both the
user and the SP, while in the latter, these distributions are
learnt from a real dataset. In human activity privacy use-case,
we focus on the sensors in wearable devices as an example
of DRMs, and their measurements as time-series data. In
this scenario, the user shares sensor readings of her wearable
device with the SP, while performing physical activities, with
the goal of tracking the type and duration of her activities.
However, as in Fig. 2, not only useful activities, such as
exercise type, but also sensitive activities, such as smoking,
drinking or eating habits, can be inferred from these readings,
which the user may not want to share with the SP as the
SP can exploit such information for commercial beneﬁt at the
detriment of the user. Hence, the user shares a single sensor
reading from among multiple sensors at a time such that the
useful activity is revealed to the SP while his conﬁdence in
the sensitive activity is kept hidden at a pre-deﬁned level.
The POMDP formulation in Section III enable us to nu-
merically approximate the proposed policies using RL. In RL,
an agent discovers the best action to take in a particular
state by receiving instant rewards or costs from the envi-
ronment [48]. POMDPs with continuous belief and action
spaces are difﬁcult to solve numerically by using classical
MDP solution methods. Actor-critic RL algorithms combine
the advantages of value-based (critic-only) and policy-based
(actor-only) methods, such as low variance and continuous
action probability producing capability. Therefore, we use
A2C-DRL for the numerical evaluation of our problem.
7
(a)
 (b)
(c)
 (d)
Fig. 3: Belief-PDRP’s, πB, (a) stopping time τ and β(u), and (b) SP’s accuracy for the secret and the useful information
with respect to LB, and MI-PDRP’s, πMI, (c) stopping time τ and β(u), and (d) SP’s accuracy for the secret and the useful
information with respect to LMI.
A. A2C-DRL
In the A2C-DRL algorithm, the actor represents the policy
structure and the critic estimates the value function [48].
In our setting, we parameterize the value function by the
parameter vector θ ∈Θ as Vθ(x), and the stochastic policy
by ξ ∈Ξ as πξ. The error between the critic’s estimate and
the target differing by one-step in time is called temporal
difference (TD) error [49]. The TD error for the experience
tuple (xt,π(at|xt),zt,xt+1,ct(xt,at)) is estimated as
δt = ct(xt) + γVθt(xt+1) −Vθt(xt), (16)
where ct(xt) +γVθt(xt+1) is called the TD target, and γ is a
discount factor chosen close to 1 to approximate the Bellman
equation for our episodic MDP. Instead of using the value
functions in actor and critic updates, we use the advantage
function to reduce the variance from the policy gradient. The
advantage is approximated by TD error. Hence, the critic is
updated by gradient ascent as:
θt+1 = θt + ηc
t∇θℓc(θt), (17)
where ℓc(θt) = δ2
t is the critic loss, and ηc
t is the learning rate
of the critic at time t. The actor is updated similarly as,
ξt+1 = ξt −ηa
t∇ξℓa(ξt), (18)
where ℓa(ξt) = −ln(π(at|xt,ξt))δt is the actor loss and ηa
t
is the actor’s learning rate. In implementation, we represent
the actor and critic by fully connected deep neural networks
(DNNs) with two hidden layers of 256 nodes and Leaky-ReLU
activation. The critic DNN takes the current state x of size
N×M as input, and outputs the corresponding state value for
the current action probabilities Vξ
θ (x). The actor takes the state
as input, and outputs the corresponding action probabilities
{ξ0,...,ξ |A|}from a softmax layer for a∈A∪ d.
B. Synthetic Data Use-Case
The synthetic data scenario represents the situations where
the probability distributions of DRMs and belief update rules
are known by both the user and the SP, while only the actions
are learned by the privacy mechanism.
We create a dataset for |A ∪ {d}|=4, |S|=3, |U|=3,
|Z|=50 and uniformly distributed S and U, and LB ∈
8
TABLE I
SELECTED ACTIVITIES AND SMARTWATCH SENSORS
FROM SMOKING ACTIVITY DATASET .
Sensors: A Activities: (S,U)
Accelerometer 0 Sitting (0,0)
Gyroscope 1 Standing (0,1)
Magnetometer 2 Walking (0,2)
Linear-accelerometer 3 Sitting while smoking (1,0)
Standing while smoking (1,1)
Walking while smoking (1,2)
Sitting while drinking (2,0)
Standing while drinking (2,1)
{0.6,0.7,0.8,0.9,0.99}. Observation probabilities are se-
lected such that each action distinguishes a different pair
of hypotheses well for both S and U. For example, we
created a matrix with each row representing the con-
ditional distribution of z for different (a,s,u ) realiza-
tions. For sensor a = 0, we used N(0,σj) for (s,u) =
{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)}, N(1,σj) for (s,u)
= (2,0), N(2,σj) for (s,u) = (2,1), and N(3,σj) for (s,u) =
(2,2), and we normalized through the columns representing z.
Here, σj’s are chosen randomly from the interval [0.5,1.5] for
each (a,s,u ) with index j={1,..,N ×M×|A|}. This sensor
discloses s=2 case more than the other secrets. Moreover,
a=1 and a=2 reveal more information for s=1 and s=0 cases,
respectively.
Fig. 3a shows the average stopping timeτ and the maximum
belief on u, β(u), with respect to LB for the belief-PDRP,
πB. As the constraint on β(s) is relaxed, the stopping time
increases as well as the maximum β(u). In Fig. 3b, on the
other hand, we present the prediction accuracy of the true-
useful activity u from the belief calculation. Red lines in
Fig. 3b represent accuracy on u, and blue lines show the
accuracy on s. The gap between the accuracy shows the
effectiveness of the proposed policy πB in minimizing the
SP’s error probability of u in the quickest way while keeping
his conﬁdence in sbelow the threshold for the synthetic data.
Fig. 3c shows the average stopping timeτ and the maximum
conﬁdence in u, ˆβ(u), with respect to LMI for the MI-PDRP,
πMI. As before, when the constraint on MI is relaxed, the
stopping time increases as well as the maximum ˆβ(u). In Fig.
3d, red lines represent accuracy on u, and blue lines show
the accuracy on s. Although πMI shows similar results with
πB, πB is more effective in hiding the true realization of S.
This is because MI-PDRP provides PUT by constraining the
statistics of all the realizations of S rather than only the true
realization.
C. Human Activity Privacy Use-Case
In human activity privacy scenario, We use smoking activity
dataset [43] which contains more than 40 hours of sensor
measurements for activities, such as smoking while walking,
drinking while standing, sitting etc. We use measurements
from four selected sensors of a smartwatch, i.e., |A∪{d}|= 5.
Table I shows these sensors and sensitive-useful activity pairs
from the dataset. We learn the probability distributions to-
gether with the actions from the real-world measurements.
1) Numerical Results for Belief-PDRP , πB: In this section,
we evaluate the PUT of the proposed optimal policy πB for
Actor 
Critic LSTM 
? 
No 
Y es 
Fig. 4: A2C-DRL process for belief-PDRP, πB.
smoking activity dataset. We model the SP by a long short-
term memory (LSTM) recurrent neural network with parame-
ters φ, which predicts the true useful variable u and secret s.
The LSTM-based predictor has 2 layers with 128 nodes and
2 look-backs, and inputs the past observations {zt−1,at−1}.
The output is a probability distribution representing the belief
vector ˆβφ(S,U) obtained by minimizing a cross-entropy loss
between ˆβφ(S,U) and true values of {S,U}. This is equivalent
to maximizing the log-likelihood of ˆβφ(S,U), i.e.,
H(β, ˆβ) = −
∑
s,u
β(s,u) log(ˆβ(s,u)) = −Es,u[log( ˆβ(s,u))].
To train the LSTM SP beforehand, we split the training data
into 3 portions. One is for pre-training the LSTM SP, which
will be used during A2C-DRL, one is for online A2C-DRL
training, and the last portion is to train an SP, i.e., LSTM
predictor, for testing the performance of PUT with A2C-DRL.
Let πR be a random policy with uniform action probabilities.
We create observation pairs {Zt,At}for LSTM training by
randomly sampling actions At from πR, and obtaining time-
series Zt from the corresponding portion of the dataset. We
also used CT = 0.5 for the time cost, and λ= 50.
Fig. 4 shows A2C-DRL process in which LSTM is used
as an online state predictor from the past observations. The
user checks if the termination action, i.e., at−1 = d, has been
taken, then she accordingly terminates the process. Otherwise
she predicts the current belief with the LSTM network, and
selects an action at via the actor. The actor-critic network
updates its parameters with the state value V(β) and action
probability π(at|β) accordingly. Sensor reading zt is observed
as per the selected action, and the observation pair zt,at is
shared with the SP.
Fig. 5a shows the average stopping time τ and the predicted
maximum belief on u, ˆβ(u), with respect to LB for the belief-
PDRP, πB. As the constraint on ˆβ(s) is relaxed, the stopping
time increases as well as the maximum ˆβ(u). In Fig. 5b, on
the other hand, no-PUT and PUT cases are compared in terms
of prediction accuracy of the test SP on true-useful activity u
and the secret s, where accuracy of the SP for the randomly
generated At and corresponding Zt represents no-PUT case,
while its accuracy for the A2C-DRL generated actions At
and Zt represents the PUT case. Red lines in Fig. represent
accuracy on u, and blue lines show the accuracy on s. The
ﬂat lines show the no-PUT case which does not depend on
9
(a)
 (b)
Fig. 5: (a) Stopping time τ and ˆβ(u), and (b) SP’s accuracy for the secret and the useful information with respect to LB.
Actor 
Critic 
LSTM 
FFNN 
? No 
Y es 
Fig. 6: A2C-DRL process for MI-PDRP, πMI.
LB, and the curved lines represent the PUT case. While the
gap between the accuracy of u and s is very low for random
policy (no-PUT case), it is very large for πB (PUT case).
This shows the effectiveness of the proposed policy πB in
minimizing the SP’s error probability of uin the quickest way
while keeping his conﬁdence in sbelow the threshold. On the
other hand, generating random actions from a random policy
does not yield a sophisticated strategy to reveal u and hide s.
The largest gap, i.e., the best performance of πB, occurs at
LB = 0.65 for πB. See Appendix C for a detailed breakdown
of the accuracy in U and S realizations.
2) Numerical Results for MI-PDRP , πMI: In this section,
we model the SP using two components; one is an LSTM-
based belief predictor with 2 layers of 128 nodes and 2 look-
backs, and the other one is a feed-forward neural network
(FFNN)-based observation generator with 3 layers of 256
nodes, where the output determines the mean µ and standard
deviation σ of a Gaussian distribution. As before, we use
CT = 0.5 for the time cost, and λ= 50.
As in Section V-C1, we train the LSTM network with
parameters φ by minimizing a cross-entropy loss between the
observations {Zt−1,At−1}and {S,U}, which is equivalent
to maximizing the log-likelihood of ˆβφ(S,U). As a result,
KL divergence between the real belief distribution β and the
predicted distribution ˆβφ goes to zero when the log-likelihood
is maximized [47]. In addition, we estimate q(Zt|At,S,U ),
which is represented by a Gaussian distribution,
ˆq(Zt|At,S,U ) = N(Zt|(µ,Σ)) = fψ(At,S,U ), (19)
where (µ,σ) are determined by a FFNN fψ by maximizing
its log-likelihood. During A2C-DRL, we sample observations
Zt and At to calculate the variational bound for MI using
the pre-trained FFNN and LSTM networks which satisfy the
maximization in (15). We approximate the MI by sampling k
observations {zi
t,ai
t}k
i ∼ˆq(zt|at,ˆs,ˆu),πMI(at|ˆβ), and using
the predictions for the next k belief states {Q(s|zi
t,ai
t,ˆβ)}k
i
as follows:
ˆI(S; Zt,At|φ,ψ)
= H( ˆβφ) + 1
n
n∑
j=1
[1
k
k∑
i=1
log[Qψ((ˆsj|zi
t,ai
t,ˆβφ))]
]
, (20)
where ˆsj is a realization of ssampled from the predicted belief
vector ˆβφ(s). Fig. 6 illustrates A2C-DRL process with belief
and MI calculation using pre-trained LSTM and FFNN. The
user checks if the termination action, i.e., at−1 = d, has been
taken. If so, she accordingly terminates the process. Otherwise,
she predicts the current belief from the previous observations
using the LSTM network, and takes action at. The actor-critic
network updates its parameters with the state value V(β) and
action probability π(at|β) accordingly. Sensor measurement is
observed as per the selected action, and the observation pair
zt,at is shared with the SP. ˆI( ˆS|At,Zt|βt) is calculated by
the SP using previous action at−1 and (ˆs,ˆu) according to (20).
Fig. 7a shows the average stopping timeτ and the maximum
conﬁdence in u, ˆβ(u), with respect to LMI for the MI-PDRP,
πMI. As the constraint on MI is relaxed, the stopping time
increases as well as the maximum ˆβ(u). In Fig. 7b, activity
prediction accuracy of the test SP for observations (Zt,At)
generated by random policy πR and πMI are compared. Red
lines in Fig. 7 represent accuracy on u, and blue lines show
the accuracy on s. Similarly to Section V-C1, the gap between
the accuracy of u and s is very low for random policy, while
10
(a)
 (b)
Fig. 7: (a) Stopping time τ and ˆβ(u) and (c) SP’s accuracy for the secret and the useful information with respect to LMI.
it is large for πMI. This shows that the proposed policy
πMI minimizes the SP’s error probability of u in a speedy
manner while keeping the information leakage from s below
the threshold. Although πMI shows similar results with πB,
πB is more effective in hiding the true realization of S. This is
because MI-PDRP provides PUT by constraining the statistics
of all the realizations of S rather than only the true realization.
The largest gap in Fig 7, i.e., the best performance of πMI,
occurs at LMI = 1.2 for πMI. See Appendix C for a detailed
breakdown of the accuracy in U and S realizations.
VI. C ONCLUSION
We studied the PUT in time-series data release to an SP.
The goal of the user is to reveal the true value of a latent
utility variable, while keeping the secret variable private from
the SP. In a sense, the SP is the legitimate receiver for
the utility variable, while acting as the adversary for the
sensitive variable. In particular, we measured the utility by
the conﬁdence of the SP in the latent useful information.
For privacy, we considered both the conﬁdence of the SP on
the sensitive information and the MI between the sensitive
variable and the revealed measurements. We proposed active
sequential data release policies to minimize the error proba-
bility on the true useful variable in a speedy manner, while
constraining the conﬁdence of the SP or the MI leakage for
the secret variable. We provided a POMDP formulation of
the problem, and used A2C-DRL for numerical evaluations.
Utilizing DNNs, we numerically evaluated the PUT curve
of the proposed policies for smoking activity dataset , where
useful and sensitive activities are revealed to the adversary
through smartwatch sensors selected by the user. We examined
the effectiveness of the optimal belief-PDRP and MI-PDRP
using an LSTM-based adversary network. According to the
numerical results, we have seen that the proposed data release
policies provide signiﬁcant privacy advantage compared to
random sensor selection. We have also seen that constraining
the MI does not necessarily hide the true value of the secret
at the same level as the belief-PDRP. However, this approach
may be more useful when the objective is not necessarily to
hide the true value of the secret, but limit the knowledge of
the SP in an average sense. We have also shown that decision
time gets longer when the constraint on the secret is relaxed.
APPENDIX A
PROOF OF THEOREM 2
We assume that after the user takes the stopping action for
data release, the system goes to a recursive termination state,
denoted by FT, and remains there forever. Hence, the state
space is X= P(B) ∪{FT}. Let instantaneous cost of taking
action a∈A∪{ d}
cπB (x,a)=



1, if x= β ∈P(B)\FB,a ∈A
min
u∈U
(1 −β(u))λ, if x= β ∈P(B)\FB,a = d
CB, if x= FB,a ∈A
0, if x= FT.
The constraint on the adversary’s conﬁdence insis enforced
with an instantaneous cost CB for state FB, which is ideally
inﬁnity but can be applied as a very large scalar in practice.
Assuming that the system follows the optimal policy, transition
to FB with a very large cost CB would not be chosen by the
minimization problem. The overall strategy for belief update
is represented by the Bayes’ operator as follows:
ΦπB (x,z,a ) =



ΦπB (β,z,a ), if x= β ∈P(B),a ∈A
FT, if x= β ∈P(B),a = d
FT, if x= FT.
Using the instantaneous cost and state update, the condition
V(β) ≤min{CT + min
a∈A
(TaV∗)(β),min
u∈U
λCT(1 −β(u))}is
rewritten as
V(FT) = 0,
V(x) ≤ min
a∈A∪{d}
{Tcc(x,a) + E[V(Φ(x,z,a ))]},∀x∈P(B),
as well as the state sequence at t= 0,1,2,... is denoted by
X0 = x,
Xn = Φ(Xn−1,Z,A (n)),∀n,n> 0.
When the condition is written in terms of the state sequence
of duration N for the optimal policy π∗, we obtain
11
TABLE II
ADVERSARY ACCURACY FOR ALL ACTIVITIES UNDER BELIEF -PRIVACY AND MI-PRIVACY POLICIES .
Policy Constraint τ/ ˆβ(U) Acc. U Acc. U=0 / U=1 / U=2 Acc. S Acc. S=0 / S=1 / S=2
πB
0.5 3.12 / %43 %44.4 %60.67 / %53.44 / %5.65 %41.2 %24.58 / %23.6 / %4.58
0.65 5.6 / %74 %77.4 %78.9 / %69.15 / %88.21 %46.2 %55.32 / %48.05 / %5.15
0.8 7.15 / %77 %78.4 %85.07 / %61.99 / %92.52 %54.3 %68.77 / %48.95 / %34.5
0.95 8.64 / %81 %85.3 %91.58 / %71.23 / %96.3 %65.3 %79.48 / %73.07 / %42.8
πMI
0.5 3 / %38 %37.5 %47.01 / %51.1 / %0 %36.4 %46.53 / %52.01 / %0.74
0.75 4.34 / %40 %42.7 %63.29 / %49.17 / %0 %38.3 %47.26 / %53.74 / %1.2
1 6.25 / %62 %65 %87.26 / %55.35 / %46.7 %56.5 %47.48 / %57.42 / %13.83
1.25 6.7 / %88 %87.1 %94 / %80.8 / %80.86 %68.6 %89.55 / %65.63 / %28.74
1.5 7.06 / %91 %91.2 %97.62 / %84 / %92.16 %79 %93.02 / %72.22 / %34.67
1.75 7.8 / %93 %93.7 %98.42 / %88.8 / %96.25 %80.8 %96.9 / %96.38 / %87.3
2 8.5 / %94 %95.7 %99.45 / %96.19 / %97.78 %81.9 %98,3 / %98.12 / %92.64
V(x) ≤CTEπ∗ [
N−1∑
n=0
c(Xn,An)] + Eπ∗ [V(XN)]. (21)
Taking the limit as N →∞, we get
V(x)
≤CTEπ∗ [
∞∑
n=0
c(Xn,An)] + lim
N→∞
Eπ∗ [V(XN)] (22)
= CTV∗(x) + lim
N→∞
Eπ∗ [XN] (23)
= CTV∗(x) +lim
N→∞
Eπ∗
[
V(FT)1{XN=FT } (24)
+ V(FB)1{XN=FB}+ V(XN)1{XN̸=FT ,XN̸=FB}
]
= CTV∗(x) + lim
N→∞
Eπ∗
[
V(XN)1{XN̸=FT } (25)
+ V(FB)1{XN=FB}
]
≤CTV∗(x) +λ lim
N→∞
(
Pπ∗ [XN ̸= FT] + Pπ∗ [XN = FB]
)
(26)
= CTV∗(x), (27)
where (22) holds due to the monotone convergence theorem;
(23) follows from the deﬁnition of V∗; (26) is due to the
fact that for any β ∈P(B), V(β) ≤min
u∈U
λ(1 −β(u)) ≤λ;
and (27) holds since λ≥V∗(x) ≥Eπ∗ [τ] = ∑∞
n=0 Pπ∗ (τ >
n) = ∑∞
n=0 Pπ∗ (Xn ̸= FT), and the probability of the system
following the optimal policy π∗ to transition to highest-cost
state FB at N is zero, i.e, lim
N→∞
Pπ∗ [XN = FB] = 0.
APPENDIX B
PROOF OF THEOREM 3
Consider a POMDP with the state space X= P(B)∪{FT}.
At time t, a decision maker observes Zt−1,At−1 and chooses
an action At ∈A∪{ d}as follows:
At = ft(Zt−1,At−1), (28)
where f = (f1,f2,... ) is called the policy. Based on the con-
ditional probability π(At|At−1,Zt−1) of taking this action,
Zt ∈Z is observed and revealed by the sensor distribution
q(Zt|At,S,U ), and the state evolves to the next belief state.
At each step, the system incurs a per-step cost
c(s,u,z t,at; f) :=
log Pf(Zt = zt,At = at|S = s,Zt−1= zt−1,At−1= at−1)
Pf(Zt = zt,At = at|Zt−1 = zt−1,At−1 = at−1) .
The objective is to ﬁnd a policy f = (f1,...,f T) that min-
imizes the total cost given by 1
TEf
[ T∑
t=1
c(S,U,Z t,At; f)
]
,
where the expectation is taken with respect to the distributions
induced by the policy f.
Let f = (f1,...,f T) be ft(zt−1,at−1) = π(·|zt−1,at−1).
Then the following holds:
IπMI (S; Zt,At|Zt−1,At−1) =
∑
s,u,zt,at
PπMI (S,U,Z t,At)
×log PπMI (Zt=zt,At=at|S=s,Zt−1= zt−1,At−1= at−1)
PπMI (Zt = zt,At = at|Zt−1 = zt−1,At−1 = at−1)
= Ef
[ T∑
t=1
c(S,U,Z t,At; f)
]
(29)
The probability distribution on (S,U,Z T,AT) induced by
the decision policy f is given by
Pf(S = s,U = u,ZT = zT,AT = aT)
= P(s,u)q(z1|a1,s,u )π(a1)
×
T∏
t=2
[
q(zt|at,s,u )π(at|zt−1,at−1)
]
, (30)
where π(·|zt−1,at−1) = f(zt−1,at−1). Under the transfor-
mations described above, Pf and PπMI are identical proba-
bility distributions. As a result, Ef
[
T∑
t=1
c(S,U,Z t,At; f)
]
=
IπMI (S; Zt,At|Zt−1,At−1). Hence, Theorem 3 holds.
APPENDIX C
ADDITIONAL SIMULATION RESULTS
In Table II, there is detailed performance of πB and πMI
policies, where ”Acc.” represents accuracy. Individual accura-
cies for U and S show that all activities are revealed as the
constraint is relaxed. On the other hand, U = 2 and S = 2 are
almost completely hidden for low constraint level, but they are
revealed faster then the other hypotheses. Moreover, πB and
πMI policies reveal or hide different activities better due to the
12
different characteristics of the activities. We also see the same
results that Fig. 5 and Fig. 7 show, i.e., πB outperforms πMI
in minimizing the error probability of U in a speedy manner
while keeping the secret below the pre-deﬁned level.
REFERENCES
[1] S. R and V . V, “Ecg-based secure healthcare monitoring system in body
area networks,” in 2018 Fourth Int’l Conf. on Biosignals, Images and
Instrum. (ICBSII), March 2018, pp. 206–212.
[2] T. Wearing and N. Dragoni, “Security and privacy issues in health
monitoring systems: ecare@home case study,” in Proc. of the Int’l Conf.
on IoT Technol. for HealthCare , 10 2016, pp. 165–170.
[3] G. Giaconi, D. G ¨und¨uz, and H. V . Poor, “Privacy-aware smart metering:
Progress and challenges,” IEEE Signal Processing Magazine , vol. 35,
no. 6, pp. 59–78, Nov 2018.
[4] Y . Cao, M. Yoshikawa, Y . Xiao, and L. Xiong, “Quantifying differential
privacy under temporal correlations,” in2017 IEEE 33rd Int’l Conf. Data
Eng. (ICDE), April 2017, pp. 821–832.
[5] E. Erdemir, P. L. Dragotti, and D. G ¨und¨uz, “Privacy-aware communi-
cation over a wiretap channel with generative networks,” ArXiv, vol.
abs/2110.04094, 2021.
[6] F. du Pin Calmon and N. Fawaz, “Privacy against statistical inference,”
in 2012 50th Annual Allerton Conference on Communication, Control,
and Computing (Allerton) , 2012, pp. 1401–1408.
[7] A. Zamani, T. Oechtering, and M. Skoglund, “A design framework for
epsilon-private data disclosure,” ArXiv, vol. abs/2009.01704, 2020.
[8] E. Erdemir, D. G ¨und¨uz, and P. L. Dragotti, “Smart meter privacy,”
in Privacy in Dynamical Systems , 1st ed., F. Farokhi, Ed. Springer
Singapore, 2020.
[9] E. Erdemir, P. L. Dragotti, and D. G ¨und¨uz, “Privacy-aware location shar-
ing with deep reinforcement learning,” inIEEE Workshop on Information
Forensics and Security (WIFS) , Delft, The Netherlands, Dec 2019.
[10] ——, “Privacy-aware time-series data sharing with deep reinforcement
learning,” IEEE Transactions on Information Forensics and Security ,
vol. 16, pp. 389–401, 2021.
[11] ——, “Privacy-cost trade-off in a smart meter system with a renewable
energy source and a rechargeable battery,” inIEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP) , Brighton,
UK, May 2019, pp. 2687–2691.
[12] B. Rassouli and D. G ¨und¨uz, “On perfect privacy,” IEEE Journal on
Selected Areas in Information Theory , pp. 1–1, 2021.
[13] Z. Li, T. J. Oechtering, and D. G ¨und¨uz, “Privacy against a hypothesis
testing adversary,” IEEE Trans. Inf. Forensics Security , vol. 14, no. 6,
pp. 1567–1581, June 2019.
[14] Y .-X. Wang, J. Lei, and S. E. Fienberg, “On-average kl-privacy and its
equivalence to generalization for max-entropy mechanisms,” Int.l Conf.
on Privacy in Statistical Databases , 2016.
[15] E. Erdemir, P. L. Dragotti, and D. G ¨und¨uz, “Active privacy-utility
trade-off against a hypothesis testing adversary,” in IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021,
pp. 2660–2664.
[16] B. Rassouli and D. G ¨und¨uz, “Optimal utility-privacy trade-off with total
variation distance as a privacy measure,” IEEE Trans. Inf. Forensics
Security, vol. 15, pp. 594–603, 2020.
[17] J. Liao, O. Kosut, L. Sankar, and F. P. Calmon, “A tunable measure for
information leakage,” in 2018 IEEE Int’l Symp. Inf. Theory (ISIT) , June
2018, pp. 701–705.
[18] I. Issa, S. Kamath, and A. B. Wagner, “An operational measure of in-
formation leakage,” in 2016 Annual Conference on Information Science
and Systems (CISS) , 2016, pp. 234–239.
[19] S. A. Osia, B. Rassouli, H. Haddadi, H. R. Rabiee, and D. G ¨und¨uz,
“Privacy against brute-force inference attacks,” in2019 IEEE Int’l Symp.
Inf. Theory (ISIT) , July 2019, pp. 637–641.
[20] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” inTheory of Cryptography, S. Halevi
and T. Rabin, Eds. Springer Berlin Heidelberg, 2006.
[21] S. Yoo, M. Shin, and D. Lee, “An approach to reducing information loss
and achieving diversity of sensitive attributes in k-anonymity methods,”
Interact J Med Res , vol. 1, no. 2, Nov 2012.
[22] N. Saleheen, S. Chakraborty, N. Ali, M. M. Rahman, S. M. Hossain,
R. Bari, E. Buder, M. Srivastava, and S. Kumar, “msieve: Differential
behavioral privacy in time series of mobile sensor data,” ACM Int’l Joint
Conf. on Pervasive and Ubiquitous Comput. , pp. 706–717, 2016.
[23] R. Shokri, C. Troncoso, C. Diaz, J. Freudiger, and J.-P. Hubaux,
“Unraveling an old cloak: k-anonymity for location privacy,” in ACM
Conference on Computer and Communications Security , Sep. 2010.
[24] R. Shokri, G. Theodorakopoulos, C. Troncoso, J.-P. Hubaux, and J.-Y .
Le Boudec, “Protecting location privacy: Optimal strategy against local-
ization attacks,” in ACM Conference on Computer and Communications
Security, Oct. 2012, pp. 617–627.
[25] J. C. Duchi, M. I. Jordan, and M. J. Wainwright, “Local privacy
and statistical minimax rates,” in IEEE Symposium on Foundations of
Computer Science, Oct 2013, pp. 429–438.
[26] Z. Zhang, Z. Qin, L. Zhu, J. Weng, and K. Ren, “Cost-friendly
differential privacy for smart meters: Exploiting the dual roles of the
noise,” IEEE Trans. Smart Grid , vol. 8, no. 2, pp. 619–626, Mar. 2017.
[27] J. Zhao, T. Jung, Y . Wang, and X. Li, “Achieving differential privacy
of data disclosure in the smart grid,” in IEEE INFOCOM 2014 - IEEE
Conference on Computer Communications , April 2014, pp. 504–512.
[28] G. Giaconi, D. G ¨und¨uz, and H. V . Poor, “Smart meter privacy with
renewable energy and an energy storage device,” IEEE Trans. Inf.
Forensics Security, vol. 13, no. 1, pp. 129–142, Jan 2018.
[29] W. Zhang, M. Li, R. Tandon, and H. Li, “Online location trace privacy:
An information theoretic approach,” IEEE Trans. Inf. Forensics Security,
vol. 14, no. 1, pp. 235–250, Jan 2019.
[30] V . Bindschaedler and R. Shokri, “Synthesizing plausible privacy-
preserving location traces,” in IEEE Symposium on Security and Privacy
(SP), May 2016, pp. 546–563.
[31] G. Giaconi and D. G ¨und¨uz, “Smart meter privacy with renewable energy
and a ﬁnite capacity battery,” in IEEE Int. Workshop on Sig. Proc.
Advances in Wireless Communications (SPAWC), July 2016, pp. 1–5.
[32] A. Makhdoumi, S. Salamatian, N. Fawaz, and M. M ´edard, “From the
information bottleneck to the privacy funnel,” in 2014 IEEE Information
Theory Workshop (ITW 2014) , 2014, pp. 501–505.
[33] M. Sun and W. P. Tay, “Inference and data privacy in IoT networks,” in
2017 IEEE 18th International Workshop on Signal Processing Advances
in Wireless Communications (SPAWC), 2017, pp. 1–5.
[34] A. Tripathy, Y . Wang, and P. Ishwar, “Privacy-preserving adversarial
networks,” in2019 57th Annual Allerton Conference on Communication,
Control, and Computing (Allerton) , 2019, pp. 495–505.
[35] M. A. Erdogdu and N. Fawaz, “Privacy-utility trade-off under continual
observation,” IEEE Int’l Symp. Inf. Theory , pp. 1801–1805, June 2015.
[36] S. Song and K. Chaudhuri, “Composition properties of inferential
privacy for time-series data,” in Allerton Conference on Communication,
Control, and Computing (Allerton) , 2017, pp. 814–821.
[37] N. Tishby, F. Pereira, and W. Bialek, “The information bottleneck
method,” Proceedings of the 37th Allerton Conference on Communi-
cation, Control and Computation , vol. 49, 07 2001.
[38] R. Shokri, G. Theodorakopoulos, J. Le Boudec, and J. Hubaux, “Quan-
tifying location privacy,” in IEEE Symposium on Security and Privacy ,
May 2011, pp. 247–262.
[39] S. Li, A. Khisti, and A. Mahajan, “Information-theoretic privacy for
smart metering systems with a rechargeable battery,” IEEE Trans. Inf.
Theory, vol. 64, no. 5, pp. 3679–3695, May 2018.
[40] P. Venkitasubramaniam, “Privacy in stochastic control: a Markov deci-
sion process perspective,” in 2013 51st Annual Allerton Conf. Commun.,
Control, and Comput. (Allerton) , Oct 2013, pp. 381–388.
[41] J. Liao, L. Sankar, V . Y . F. Tan, and F. du Pin Calmon, “Hypothesis
testing under mutual information privacy constraints in the high privacy
regime,” IEEE Transactions on Information Forensics and Security ,
vol. 13, no. 4, pp. 1058–1071, 2018.
[42] C. H. Papadimitriou and J. N. Tsitsiklis, “The complexity of markov
decision processes,” Mathematics of Operations Research, vol. 12, no. 3,
pp. 441–450, 1987.
[43] M. Shoaib, H. Scholten, P. J. M. Havinga, and O. D. Incel, “A hier-
archical lazy smoking detection algorithm using smartwatch sensors,”
in 2016 IEEE 18th International Conference on e-Health Networking,
Applications and Services (Healthcom) , 2016, pp. 1–6.
[44] D. P. Bertsekas and S. E. Shreve, Stochastic Optimal Control: The
Discrete-Time Case. Belmont, CA: Athena Scientiﬁc, 2007.
[45] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dy-
namic Programming, 1st ed. USA: John Wiley & Sons, Inc., 1994.
[46] N. Saldi, T. Linder, and S. Y ¨uksel, Approximations for Partially Ob-
served Markov Decision Processes . Cham: Springer Int’l Publishing,
2018, pp. 99–123.
[47] D. Barber and F. Agakov, “The im algorithm: a variational approach to
information maximization,” in NIPS 2003, 2003.
[48] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction,
2nd ed. The MIT Press, 2018.
[49] I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska, “A
survey of actor-critic reinforcement learning: Standard and natural policy
gradients,” IEEE Trans. Syst., Man, Cybern., Part C (Applications and
Reviews), vol. 42, no. 6, pp. 1291–1307, Nov 2012.