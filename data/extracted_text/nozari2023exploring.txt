Exploring Action-Oriented Models via Active
Inference for Autonomous Vehicles
Sheida NozariÂ 
University of Genoa: Universita degli Studi di Genova https://orcid.org/0000-0002-7767-6178
Ali KrayaniÂ 
University of Genoa: Universita degli Studi di Genova
Pablo MarinÂ 
UC3M: Universidad Carlos III de Madrid
Lucio MarcenaroÂ 
University of Genoa: Universita degli Studi di Genova
David Martin GomezÂ 
UC3M: Universidad Carlos III de Madrid
Carlo RegazzoniÂ 
University of Genoa: Universita degli Studi di Genova
Research Article
Keywords: Active Inference, Imitation learning, Action-oriented model, Bayesian Filtering, Autonomous
Driving
Posted Date: July 12th, 2023
DOI: https://doi.org/10.21203/rs.3.rs-3057679/v1
License: ï‰ ï“§ This work is licensed under a Creative Commons Attribution 4.0 International License. Â 
Read Full License
Version of Record: A version of this preprint was published at EURASIP Journal on Advances in Signal
Processing on October 29th, 2024. See the published version at https://doi.org/10.1186/s13634-024-
01173-9.
Exploring Action-Oriented Models via Active
Inference for Autonomous Vehicles
Sheida Nozari1,2*, Ali Krayani1, Pablo Marin2, Lucio Marcenaro1,
David Martin Gomez2, Carlo Regazzoni1
1Department of Electrical, Electronic, Telecommunications Enginee ring
and Naval Architecture, University of Genoa, Genoa, 16145, Italy.
2*Intelligent Systems Laboratory, Carlos III University of Madrid,
LeganÂ´ es, 28911, Spain.
*Corresponding author(s). E-mail(s): sheida.nozari@edu.unige.it;
Contributing authors: ali.krayani@edu.unige.it; pamarinp@ing.uc3m.es;
lucio.marcenaro@unige.it; dmgomez@ing.uc3m.es;
carlo.regazzoni@unige.it;
Abstract
Being able to robustly interact with and navigate a dynamic env ironment has
been a long-standing challenge in intelligent transportatio n systems. Autonomous
agents can use models that mimic the human brain to learn how to re spond to
other participantsâ€™ actions in the environment and proactivel y coordinate with
the dynamics. Modeling brain learning procedures is challengin g for multiple
reasons, such as stochasticity, multi-modality, and unobse rvant intents. Active
inference may be deï¬ned as the Bayesian modeling of a brain with a biolog-
ically plausible model of the agent. Its primary idea relies on t he free energy
principle and the prior preference of the agent. It enables the age nt to choose
an action that leads to its preferred future observations. An explori ng action-
oriented model is introduced to address the inference complexity and solve the
exploration-exploitation dilemma in unobserved environment s. It is conducted
by adapting active inference to an imitation learning approach a nd ï¬nding a
theoretical connection between them. We present a multi-moda l self-awareness
architecture for autonomous driving systems where the proposed te chniques are
evaluated on their ability to model proper driving behavior. Ex perimental results
provide the basis for the intelligent driving system to make mo re human-like
decisions and improve agent performance to avoid a collision.
1
Keywords: Active Inference, Imitation learning, Action-oriented mo del, Bayesian
Filtering, Autonomous Driving
1 Introduction
Autonomous Vehicles (AVs) have received much attention from both indu stry and
academia due to their potential to improve the safety and eï¬ƒciency of driving experi-
ences. Autonomous driving systems (ADS) are generally partitioned int o a hierarchical
structure, including perception, decision-making, action planni ng, and vehicle control
[
1]. Perception and navigation in a dynamic environment have been a long- standing
challenge in AVs. In addition to the complexity of the decision-making systems that
might provoke errors causing performance degradation and lead to severe s ituations
(e.g., collisions) [ 2]. Performing suitable actions according to the dynamic environ-
mental changes around the AV signiï¬cantly impacts error minimization. T hus, action
planning is still a challenging task responsible for safety and eï¬ƒcie ncy. It should con-
sider the feasibility constraints in a kinematic and dynamic manner based on the
information about the perceived environment and the reasonable predic tion of the
other contributorâ€™s behaviors. Moreover, it should be able to generat e optimal or
semi-optimal maneuvers that provide suitable driving quality, su ch as exactitude and
consistency.
Satisfying the earlier requirements mandates an eï¬ƒcient theory cap able of rep-
resenting causal relationships in the world and providing optimal be havior in highly
uncertain environments. In addition, for an AV to reach a high level of au tonomy, it
must be equipped with self-awareness to understand its own sit uation, performance
limits and the eï¬€ects of decisions on its surroundings [ 3]. Self-awareness refers to the
systemâ€™s capability to know not only its external context (i.e., oth er objects) but also
its state based on its sensations and internal models [ 4]. Computational self-awareness
methods are a promising ï¬eld that allows an autonomous agent to identify d ynamic
situations, create internal models of its world, and modify its beha vior autonomously
to contextual conditions [ 5].
Recent progress in signal processing and machine learning allows an i ntelligent
learning agent to achieve a self-awareness model by observing mult i-sensorial data
from an accomplished task by an expert agent. Observational learning is an on line
learning approach that occurs as a process of observing an expert agent pe rforming a
speciï¬c task that another agent retains [ 6]. It is imperative that autonomous agents
are capable of learning from and adapting to a variety of safe and desired beh aviors.
Imitation learning (IL) is an essential paradigm for learning preferenc es strategies via
imitating the expert demonstrations [ 7]. IL might utilize diverse and large-scale driving
data to handle huge state-action space and complicated driving scenarios . However,
its performance never exceeds the demonstratorâ€™s skills. Moreove r, it is vulnerable to
distributional shifts between demonstration and execution [ 8].
A self-aware autonomous system constantly deals with continuous and pote ntially
overwhelming signals from the agentâ€™s sensors and their interaction w ith the dynamic
surrounding. For learning and adaptation, the intelligent agent must tran sform the
2
sensory inputs into a reliable perception of the world. One of the ul timate goals of
artiï¬cial intelligence is to construct autonomous agents capable of human- level perfor-
mance. Motivated by this, cognitive science has debated how exactly t he brain carries
out the learning activities. While previous researches propose per ception primarily
as a bottom-up readout of sensory signals, emerging Bayesian models sugges t that
perception is cognitively modulated instead. It might be best view ed as a process of
prediction based on an integration of sensory inputs, prior experience , and contextual
cues [ 9].
The brain executes the Bayes rule to perceive the world by contin uously generating
a top-down cascade of encoded hypotheses about environmental states and processing
bottom-top projections of sensory inputs compared with the prior hypot hesisâ€™s top-
down ï¬‚ow [ 10]. Any mismatch between top-down predictions and bottom-up sensory
responses results in prediction errors, prompting the system to reï¬ne its hypotheses.
Thus, there is a strong link between bottom-up perception and top-d own prediction,
allowing to continuously update the priors to better predict the s ubsequent incoming
sensory inputs and minimize errors. In this view, experiences ar e necessary because
they assess how good the model is and give a hint to correct future pr edictions through
the computation of prediction errors. As a result, ascending project ions do not capture
the characteristics of a stimulus but rather how surprised the br ain is by it, given the
strong link between surprise and model uncertainty [ 11].
Consequently, Active Inference has emerged as a novel theory explai ning the idea
that the brain is essentially a prediction and inference machine th at actively attempts
to predict, experiment with, and comprehend its surroundings [ 12]. Perception and
action are strongly linked in active inference, both coming from the br ainâ€™s beliefs
about the world and being constrained by sensory inputs from the envir onment [ 13].
Active inference argues that action inï¬‚uences perception in order to minimize the Free-
Energy (FE) [ 14]. The FE principle has emerged as a uniï¬ed brain theory and a new
perspective on life and self-organization in complex far-from-equili brium systems [ 15].
In this paper, motivated by the above discussion and previous work [ 16], we
introduce a self-awareness framework empowered by Active Infer ence to improve
Autonomous Driving (AD). The proposed framework consists of three main mo dules:
a multi-modal perception module , a global learning module (world model)
and an active learning module . Thus, an AV (learning agent) equipped with self-
awareness is capable of learning how to self-drive in a dynamic envi ronment while
interacting with another moving object. The multi-modal percept ion module allows
the AV to perceive the external world as a bundle of exteroceptive and proprioceptive
sensations from multiple sensory modalities (e.g., positional inform ation from GPS
sensors, images from cameras, point clouds from Lidar, etc.) and to be able t o inte-
grate information from diï¬€erent sensory inputs and match them appropriat ely. In this
work, the AV integrates proprioceptive stimuli (i.e., AVâ€™s positi ons) with exterocep-
tive stimuli (i.e., the relative distance between AV and another ob ject), describing
the integration process using Bayesian inference. The AV relies on the global world
module to encode the dynamics of the surrounding environment that is structured in
a hierarchical representation. The idea is to use hierarchical rep resentations underly-
ing multisensory integration to explain best how sensory data are caus ed in multiple
3
modalities. The global learning module consists of: i) a situation model represent-
ing the dynamic driving behaviour of an expert agent interacting wit h another agent
in the environment that is learned from demonstrations and ii) a First-person (FP)
model enabling the third-agent (i.e., AV) in ï¬rst person, so that AV can experience a
certain driving task from the expertâ€™s real perspective. The sit uation and the First-
person models are represented in Coupled Generalized Dynamic Bay esian Networks
(C-GDBNs). The former is composed of two GDBNs representing the two agents
interacting in the environment where their hidden variables are s tochastically cou-
pled (variables are uncorrelated but have coupled means), and each GDB N has its
own private observation. Likewise, the First-person model repres ents the stochastic
coupling of the interaction between AV and another agent and the AVâ€™s beha viour
using a C-GDBN. The active learning module connects the internal m odels that the
AV holds with the decision-making process by enriching the FP mo del with active
variables representing the set of actions that the AV can perform and so creating the
Active FP model. This endows the AV with the capability to predic t what will happen
next in the surroundings and evaluate the environmental situation to u nderstand how
it should behave in ï¬rst person. Hence, the AV can either follow an oï¬„i ne planned
task by executing expert-like manoeuvres during normal situation s (i.e., situations
experienced by the expert) or by planning at run-time and learning incrementally to
resolve uncertainty during unexpected situations (i.e., situat ions not experienced by
the expert). To this purpose, we implement a hybrid mechanism b y pulling together
imitation learning and active inference, inspired by the brain lear ning procedure that
typically integrates the agentâ€™s prior knowledge and its actual observat ions. The AV
uses the mismatches between prediction and observations to jointly improve future
predictions and actions to minimize future FE (i.e., prediction e rrors).
The major contributions of this paper are as follows:
â€¢ It advances a probabilistic computational account of action, observation and i mita-
tion abilities grounded in the framework of active inference. While ou r proposal is
domain-general, in this paper, we illustrate it using driving tasks (i.e., lane chang-
ing) in a dynamic environment, where a naive learning agent infers and imitates the
actions executed by an expert agent.
â€¢ The proposed approach enables AV to follow an oï¬„ine planned task by execut -
ing expert-like overtaking manoeuvres in automated driving syst ems while still
taking autonomous decisions at run-time and learning incrementally to adapt to
unexpected situations.
â€¢ A probabilistic framework is developed to solve the exploration-expl oitation
dilemma by foreseeing actions that minimize the prediction errors and establish a
solid foundation for further research on the representation and learnin g of concepts
in a cognitive environment by an autonomous agent.
â€¢ An online evaluation of joint state predictions is applied to update the b elief during
the back-projection of detected errors at continuous and discrete le vels. We employ
a Bayesian sequential decision-making model (i.e., Particle ï¬lt er, Kalman ï¬lter)
to distinguish exploration and exploitation processes, which train A V to generate
the preferred performance or explore a new course of actions based on its sensory
observations and new information provided by the perception of the sur rounding.
4
â€¢ Extensive simulations on various overtaking tasks illustrate that th e performance
of the proposed approach outperforms that of RL. Furthermore, we discuss h ow
clustering of new experiences might aï¬€ect the performance of the AV in generalizing
what has been learned so far to unseen situations.
2 Related works
AD requires the resolution of perception and motion planning issues in the presence
of dynamic objects interacting in the environment. The complex int eractions between
multiple agents are signiï¬cant challenges due to the diï¬ƒculty of pred icting their future
motions. Most model-based AD approaches necessitate manually designing the driving
policy model [
17, 18] or employing safety assessments [ 19, 20]. While designing a
decision and planning system for AVs is complex, an alternative is to le arn the driving
policy from an expert agent using IL. The existing IL methods can handl e simple
driving tasks such as lane following [ 21, 22]. However, if the agent is dealing with a
new environment or a more complicated task (such as line-changing), it i s required
that the human has to take control, or the system fails ultimately [ 23, 24]. More
particular, a typical IL procedure is direct learning, where the m ain goal is to learn
a mapping from states to actions that mimic the demonstrator explicit ly [ 25, 26].
Direct learning methods are categorized into classiï¬cation methods when the learnerâ€™s
actions can be classiï¬ed into discrete classes [ 27, 28], and regression methods which are
used to learn actions in a continuous space [ 29]. Direct learning often is not adequate
to reproduce proper behavior due to insuï¬ƒcient demonstrations and environmental
changes. Besides, indirect learning can complement direct approach es by reï¬ning the
policies based on sub-optimal expert demonstrations [ 30].
The critical drawbacks of IL are that the policy never exceeds the su boptimal
expert performance, and the learning policy is vulnerable to distr ibutional shift [ 31].
Therefore, IL frequently involves another step that requires the learning agent reï¬ne-
ment of the estimated policy based on its current situation. This sel f-improvement
can be achieved by a quantiï¬able reward or learned from instances. Many of these
approaches come under the RL methods. RL allows encoding desired b ehavior â€”
such as reaching the target and avoiding collisions â€” and relies not only on perfect
expert demonstrations. In addition, RL maximizes the overall expec ted return on an
entire trajectory, while IL treats every observation independen tly [ 32], which concep-
tually makes RL superior to IL. RL does not have prior knowledge from an e xpert.
Therefore the learning agent has no clue to realize desired behavior s in sparse-reward
settings [ 33]. Even when RL succeeds in reward maximization, the policy does not nec-
essarily achieve behaviors that the reward designer has expected. I n addition, learning
through trial and error requires reward functions designed speciï¬cal ly for each task.
Deï¬ning rewards for such problems is complex and still unknown in m any cases. Thus,
behavior learning, such as IL and RL, would be complex without represen tation or
model learning from the environment. To overcome the mentioned li mitations, AS can
employ a GM of the world and computes the FE to explain perception, act ion, and
model learning in a Bayesian probabilistic way [ 34, 35], allowing to handle behavior
learning and model teaching at the same time in a dynamic environment . To highlight
5
Table 1 Comparison with existing methods from literature.
functionalities ours [ 23] [ 22] [ 26] [ 30] [ 32] [ 21] [ 19] [ 24] [ 33] [ 25] [ 20]
Indirect learning âœ“ âœ— âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ— âœ— âœ— âœ“
No expert intervention âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ— âœ— âœ“ âœ“ âœ— âœ—
Self-improvement âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
Adapt to dynamic environment/behavior âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
Incremental learning âœ“ âœ“ âœ— âœ“ âœ— âœ“ âœ— âœ— âœ— âœ— âœ— âœ“
Calculate future EFE/reward âœ“ âœ— âœ— âœ— âœ— âœ— âœ— âœ— âœ— âœ— âœ— âœ—
the discreteness of the presented study, a brief comparison of thi s work with other
existing works in the literature is provided in Table 1.
3 Proposed Framework
The proposed self-awareness architecture depicted in Fig.
1 is composed of several
modules forming the perception-action cycle that links an AV to its environment.
When facing a new situation, an AV makes sense of the external world by c reating and
testing hypotheses about how the world evolves. It makes prediction s based on prior
knowledge acquired from past experiences, takes actions based on those hypotheses,
perceives the consequences, and adjusts the hypotheses. The diï¬€ erent modules in the
experienced interaction
Situation 
model
exteroceptive prediction proprioceptive prediction
exteroceptive observation proprioceptive observation
prediction error
learning policies
action decisionActive First-Person model
First-Person 
model
world model
multi-modal perception
exteroceptive perception
proprioceptive perception
dynamic interaction
information
export knowledge to Learner view
extend 
knowledge
short term
memory
abnormality
cost calculation
dynamic world
exteroceptive sensors
proprioceptive sensors
exteroceptive perception
proprioceptive perception
Expert agent
dynamic agent
Fig. 1 A general schematic of the proposed Self-Awareness archite cture.
6
architecture can be seen as diï¬€erent areas of the biological brain, each one handling
particular functionalities. Some parts handle sensory perception, s uch as seeing, while
others handle planning and decision-making. All parts of the brain work toge ther,
with messages following between them. As shown in Fig. 1, the hierarchical message
passing through the levels is not regarded as a straightforward action-fee dback map-
ping. It is represented by inferences and perceptions across diï¬€e rent modalities of
proprioceptive and exteroceptive sensory signals. Learning this as sociation allows the
model to predict the perceptual consequences of acting. Additionall y, the model must
use these representations to reduce prediction errors and predic t how sensory signals
change under speciï¬c actions. The following sections present a det ailed description of
the diï¬€erent modules involved in the architecture.
3.1 Dynamic world
We approach self-awareness from a multi-sensory signal processing p erspective in a
non-stationary environment. The environment is considered dynamic due to the chang-
ing through the agent transitions and simultaneous other processes oper ating on it.
The agent is equipped with exteroceptive sensors to observe the e nvironment and
proprioceptive sensors to measure the internal parameters. Accordi ngly, the agent
continuously collects multisensorial data by observing itself and i ts surroundings and
processes the collected data to learn a contextual dynamic represe ntation.
3.2 Multi-modal perception
A perception system is employed for learning the interaction betw een an agent and
another dynamic object based on multimodal perception using multi-s ensorial infor-
mation. Multimodality enables the model to leverage the presented sensors to identify
causalities between multisensory data perceived by the agent. Leve raging multiple sen-
sors to perceive information about the environment is thus crucial wh en building a
model to perform predictions about the agentâ€™s dynamics to do motion pl anning. The
perception of multimodal stimuli is an important capability that prov ides multimodal
information in various conditions to enrich the scene library of AD mode ls.
We combine exteroceptive and proprioceptive perception to mode l a contextual
viewpoint for making inferences about future perceived information . Consequently,
the context comprises the internal and external perceptions of the age nt at each time
instant. The main idea is to use such information to predict the foll owing internal or
external states. Therefore, the movement of both agent and dynamic obje ct is simu-
lated at each instant by interacting rules that depend on their positi ons and motions
to generate coupled trajectory data. The purpose of analyzing such mult isensory data
is to encode the coupled agentsâ€™ dynamic interaction as probabilities into a C-GDBN
model. The obtained dynamic interaction model is self-aware due to its ability to
measure the abnormalities and incrementally learn newly interactin g behavior derived
from an initial one that aï¬€ects the agentâ€™s decision-making.
7
3.3 World model
The world model (WM) plays a simulator role in the brain, and such cons ideration
leads us to take inspiration from the mechanism whereby the brain lear ns to per-
form sensorimotor behaviours [
36]. In the presented architecture, we obtain the WM
using generative models through the interacting experiences from multimodal sensory
information. The WM consists of two models, the Situation model (SM) and the First-
Person model (FPM). The SM is an input module that demonstrates the collected
sub-optimal information of an expert AV (E) and its interaction with a mo ving vehi-
cle (O) in a continuous environment where E change-lane frequently and overtake O
without an accident. E motion features, O and their interaction are incor porated in
a graphical model (i.e., C-GDBN), and the intention of the vehicles c an be estimated
through probabilistic reasoning. The second model (FPM) is a transfer red generative
model. Our focus is attempting to transfer Eâ€™s knowledge across th e First-Person point
of view, where an intelligent vehicle (L) learns by interacting wit h its surroundings
via observing the expert behaviour and collecting prior knowledge to incorporate into
the environment.
3.3.1 Situation model (SM)
The SM is an interactive dynamic model encoding the interactions b etween two vehi-
cles, namely, E and O (refer to Fig.
2). The proposed model assumes synchronized
sensory data from both agentsâ€™ locations. Accordingly, the movement of bot h agents is
simulated at each time instant by interacting rules that depend on th eir positions and
motions. From the Eâ€™s perspective, it is possible to consider its l ocation measurements
ğ’
ğ’Œâˆ’ğŸ
ğ‘¶
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¶
ğ’
ğ’Œ
ğ‘¶
à·©
ğ‘¿
ğ’Œ
ğ‘¶
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘¶
ğ‘º
ğ’Œ
ğ‘¶
ğ‘«
ğ’Œâˆ’ğŸ
ğ‘«
ğ’Œ
âˆ = ğ‘·(ğ‘«
ğ’Œ
|ğ‘«
ğ’Œâˆ’ğŸ
)
ğ‘·(ğ’
ğ’Œâˆ’ğŸ
ğ‘¬
|
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¬
)
ğ‘·
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¬
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘¬
ğ‘· ğ‘º
ğ’Œ
ğ‘¬
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘¬
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘¬
ğ‘º
ğ’Œ
ğ‘¬
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¬
à·©
ğ‘¿
ğ’Œ
ğ‘¬
ğ’
ğ’Œâˆ’ğŸ
ğ‘¬
ğ’
ğ’Œ
ğ‘¬
ğ‘· ğ‘º
ğ’Œ
ğ‘¶
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘¶
ğ‘·
à·©
ğ‘¿
ğ’Œ
ğ‘¬
ğ‘º
ğ’Œ
ğ‘¬
ğ‘·(ğ’
ğ’Œ
ğ‘¬
|
à·©
ğ‘¿
ğ’Œ
ğ‘¬
)
ğ‘·
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¶
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘¶
ğ‘·
à·©
ğ‘¿
ğ’Œ
ğ‘¶
ğ‘º
ğ’Œ
ğ‘¶
ğ‘·(ğ’
ğ’Œâˆ’ğŸ
ğ‘¶
|
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¶
)
ğ‘·(ğ’
ğ’Œ
ğ‘¶
|
à·©
ğ‘¿
ğ’Œ
ğ‘¶
)
ğ‘·
à·©
ğ‘¿
ğ’Œ
ğ‘¬
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¬
ğ‘·
à·©
ğ‘¿
ğ’Œ
ğ‘¶
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘¶
Fig. 2 C-GDBN composed of two GDBNs representing the dynamic inter action between two AVs.
8
as proprioceptive data, whereas the relative position of O represents the exteroceptive
information.
The dynamic behaviour of how the two vehicles interact in the envi ronment is
described by a generalized hierarchical state-space model in disc rete-time comprised
of the following equations:
Dk = f(Dkâˆ’ 1) + w k, (1a)
ËœXk = g( ËœXkâˆ’ 1, Dk) = F ËœXkâˆ’ 1 + BUDk + wk, (1b)
Zk = h( ËœXk) + Î½k = H ËœXk + Î½k. (1c)
In ( 1a), Dk is a latent discrete state evolving from the previous state Dkâˆ’ 1 by a non-
linear state evolution function f(Â·) representing the transition dynamic model and by
a Gaussian process noise w k âˆ¼ N (0, Q). The discrete state variables Dk = [S E
k , SO
k ]
represent jointly the discrete states of E and O where S E
k âˆˆ SE, S O
k âˆˆ SO, Dk âˆˆ D,
and SE and SO are learned according to the approach discussed in [
37], while D =
{D1, D2, . . . , Dm} is the set that represents the dictionary consisting of all the possi ble
joint discrete states (i.e., conï¬gurations) and m is the total number of conï¬gurations.
Observing the conï¬gurationâ€™s evolution (i.e., joint activated clust ers of E and O) over
time makes it possible to estimate the transition matrix encoding t he probability of
switching from one conï¬guration to another, which is deï¬ned as:
Î  =
ï£®
ï£¯
ï£°
P(D1|D1), . . . , P(D1|Dm)
.
.
. ... .
.
.
P(Dm|D1), . . . , P(Dm|Dm)
ï£¹
ï£º
ï£» (2)
where Î  âˆˆ Rm,m , P( Di|Dj ) represents the transition probability from conï¬guration i
to conï¬guration j and âˆ‘ m
k=1 P(Di|Dk) = 1 âˆ€i.
In (
1b), the continuous latent state ËœXk = [ ËœXE
k , ËœXO
k ] âˆˆ Rnx represent a joint belief
state where ËœXE
k and ËœXO
k denote the hidden generalized states (GSs) of E and O, respec-
tively. The GSs consist of the vehiclesâ€™ position and velocity whe re ËœXi
k = [x i
k, yi
k, Ë™ xi
k, Ë™ yi
k]
and i âˆˆ { E, O}. The continuous variables ËœXk evolve from the previous state ËœXkâˆ’ 1 by
the linear state function g(Â·) and by a Gaussian noise w k. F âˆˆ Rnx,n x in (
1b) is the
state evolution matrix and U Dk = Ë™ÂµDk is the control unit vector. In ( 1c), Zk âˆˆ Rnz is
the generalized observation, which is generated from the latent contin uous states by
a linear function h(Â·) corrupted by Gaussian noise Î½k âˆ¼ N (0, R). Since the observa-
tion transformation is linear, there exists the observation matrix H âˆˆ Rnz ,n z mapping
hidden continues states to observations.
3.3.2 First-Person Model (FP)
The FP model organizes a descriptive dynamic model that enables the third-person
(i.e., the learner L) in ï¬rst-person. So L can experience a drivin g task from Eâ€™s real
perspective, which facilitates more precise imitative behaviou r and allows L to respond
quickly and appropriately during the driving task while interacti ng with another mov-
ing vehicle V. The FP model is initialized by mapping the hierarc hical levels of the SM
into FP (refer to Fig.
3). The top level of the hierarchy (discrete level) in FP represen ts
previously learned conï¬gurations ( D). So, L through the FP model can regenerate
9
â‹¯
ğ’
ğ’Œâˆ’ğŸ
ğ‘³
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘³
â‹¯ ğ’
ğ’Œ
ğ‘³
à·©
ğ‘¿
ğ’Œ
ğ‘³
â‹¯
â‹¯
ğ‘º
ğ’Œâˆ’ğŸ
ğ‘³
ğ‘º
ğ’Œ
ğ‘³
â‹¯â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯â‹¯
à·©
ğ‘¿
ğ’Œ
ğ’
ğ’Œ
ğ’
ğ’Œ+ğŸ
à·©
ğ‘¿
ğ’Œ+ğŸ
ğ‘«
ğ’Œ
ğ‘«
ğ’Œ+ğŸ
âˆ
ğ‘·(
à·©
ğ‘¿
ğ’Œ
|ğ‘«
ğ’Œ
)
ğ‘·(
à·©
ğ‘¿
ğ’Œ+ğŸ
|ğ‘«
ğ’Œ+ğŸ
)
ğ‘·(
à·©
ğ‘¿
ğ’Œ
ğ‘³
|ğ‘º
ğ’Œ
ğ‘³
)
ğ‘·(
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘³
|ğ‘º
ğ’Œâˆ’ğŸ
ğ‘³
)
ğ‘·(ğ’
ğ’Œ
ğ‘³
|
à·©
ğ‘¿
ğ’Œ
ğ‘³
)
ğ‘·(ğ’
ğ’Œâˆ’ğŸ
ğ‘³
|
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘³
)
ğ‘·(ğ’
ğ’Œ
|
à·©
ğ‘¿
ğ’Œ
)
ğ‘·(ğ’
ğ’Œ+ğŸ
|
à·©
ğ‘¿
ğ’Œ+ğŸ
)
ğ‘·(ğ‘º
ğ’Œ
ğ‘³
|ğ‘º
ğ’Œâˆ’ğŸ
ğ‘³
)
ğ‘·(
à·©
ğ‘¿
ğ’Œ
ğ‘³
|
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
ğ‘³
)
ğ‘·(
à·©
ğ‘¿
ğ’Œ+ğŸ
|
à·©
ğ‘¿
ğ’Œ
)
Fig. 3 First-Person model. It is composed of an uncoupled proprioc eptive model (right side) and
the learned joint conï¬guration (left side).
expected interactive manoeuvres that can be used as a reference to evaluate its own
interactions with V and infer how the interaction with the external world should be
performed. The hidden continuous states in the FP model represen t the dynamic
interaction in terms of generalized relative distance consisting of r elative distance and
relative velocity, which is deï¬ned as:
Xk = [ ËœXE
k âˆ’ ËœXO
k ] = [(x E
k âˆ’ xO
k ), ( Ë™ xE
k âˆ’ Ë™ xO
k )]. (3)
Likewise, the observations in the FP model depict the measured rel ative distance
between the two vehicles deï¬ned as Zk = [Z E
k âˆ’ ZO
k ].
3.4 Active FP model (AFP)
AFP model connects the world model that L holds with the decision-mak ing block
by enriching the FP model with active states representing the Lâ€™ s actions. Thus,
AFP represents a generative model P( ËœZ, ËœX, ËœD, a) of the environment (represented
graphically in Fig.
4) which is modeled as a partially observed Markov decision pro-
cess (POMDP). AFP encompasses joint probability distributions ove r observations,
environmental hidden states at multiple levels and actions performe d by L, which is
factorized as:
P(ËœZ, ËœX, ËœD, a) = P( ËœD0)P( ËœX0)
Kâˆ
k=2
P(ËœZk|ËœXk)
P( ËœXk|ËœXkâˆ’ 1, ËœDk)P( ËœDk|ËœDkâˆ’ 1, akâˆ’ 1)P(akâˆ’ 1|ËœDkâˆ’ 1). (4)
In a POMPD, 1) L does not always have access to the true environmental states
but might instead receive observations which are generated according t o P( ËœZk|ËœXk)
10
ğ‘«
ğ’Œâˆ’ğŸ
à·©
ğ‘¿
ğ’Œâˆ’ğŸ
à·©
ğ’
ğ’Œâˆ’ğŸ
ğ’‚
ğ’Œâˆ’ğŸ
ğ‘«
ğ’Œ
à·©
ğ‘¿
ğ’Œ
à·©
ğ’
ğ’Œ
ğ’‚
ğ’Œ
à·¤ğœº
à·©
ğ‘¿
ğ’Œ
à·¤ğœº
ğ‘«
ğ’Œ
à·¤ğœº
ğ’‚
ğ’Œâˆ’ğŸ
Î³
ğ‘«
ğ’Œ
Î³
à·©
ğ‘¿
ğ’Œ
âˆ(ğ‘«
ğ’Œâˆ’ğŸ
, ğ‘«
ğ’Œ
)
ğ›Œ(ğ‘«
ğ’Œ
)
ğ›Œ(
à·©
ğ‘¿
ğ’Œ
)
ğ›Œ(ğ’‚
ğ’Œâˆ’ğŸ
)
(ğ’‚
ğ’Œâˆ’ğŸ
)Ğ³
ğ›‘(ğ‘«
ğ’Œ
)
ğ›‘(
à·©
ğ‘¿
ğ’Œ
)
ğ›‘(ğ’‚
ğ’Œâˆ’ğŸ
)
Bayesian Filtering
predictive messages
diagnostic messages
generalized errors
FE measurements
Fig. 4 Graphical representation of the Active First-Person model .
to infer the real states of the environment. 2) L operates on beliefs about the hid-
den environmental states ( ËœDk, ËœXk) that evolve according to P( ËœXk|ËœXkâˆ’ 1, ËœDk) and
P( ËœDk|ËœDkâˆ’ 1, akâˆ’ 1). 3) L interacts with the external world by seeking to take actions
that minimise abnormalities and prediction errors.
Joint prediction and perception
Initially (at k = 1), L relies on prior probability distributions (P( ËœD0), P( ËœX0)) to
predict the environmental states according to ËœD0 âˆ¼ P( ËœD0) and ËœX0 âˆ¼ P( ËœX0), respec-
tively, using a hybrid Bayesian ï¬lter called the modiï¬ed Markov jump particle ï¬lter
(M-MJPF) [
38] consisting of particle ï¬lter (PF) and kalman ï¬lter (KF). In the suc -
cessive time instants ( k > 1), L relies on the a priori acquired knowledge of the
conï¬gurationsâ€™ evolution given by P( ËœDk|ËœDkâˆ’ 1) which is encoded in ( 2). PF propa-
gates N equally weighted particles drawn from the importance density dist ribution
Ï€( ËœDk) = P( ËœDk|ËœDkâˆ’ 1, akâˆ’ 1) forming the so-called set of particles
{ ËœD(i)
k , w(i)
k
}N
i=1. A
bank of KFs is employed for the set of particles to predict the corres ponding continu-
ous GSs
{ ËœX(i)
k
}N
i=1 where the prediction of GSs is guided by the upper level as pointed
out in (
1b) that can be expressed in probabilistic form as P( ËœX(i)
k |ËœX(i)
kâˆ’ 1, ËœD(i)
k ). The
posterior distribution associated with the predicted GSs is given by:
Ï€( ËœX(i)
k ) = P( ËœX(i)
k , ËœD(i)
k |ËœZkâˆ’ 1) =
âˆ«
P( ËœX(i)
k |ËœX(i)
kâˆ’ 1, ËœD(i)
k )Î»( ËœX(i)
kâˆ’ 1)d ËœX(i)
kâˆ’ 1, (5)
11
where Î»( ËœX(i)
kâˆ’ 1) = P( ËœZkâˆ’ 1|ËœX(i)
kâˆ’ 1) is the diagnostic message propagated previously
after observing ËœZkâˆ’ 1 at time k âˆ’ 1. Consequently, once a new observation ËœZk is
received, multiple diagnostic messages propagate in a bottom-up manner t o update
Lâ€™s belief in hidden environmental states. Thus, updated belief i n GSs is given by:
P( ËœX(i)
k , ËœD(i)
k |ËœZk) = Ï€( ËœX(i)
k ) Ã— Î»( ËœX(i)
k ). Whereas belief in discrete hidden states can be
updated by updating the particlesâ€™ weights according to w(i)
k = w(i)
k Ã— Î»( ËœDk) where
Î»( ËœDk) is a discrete probability distribution deï¬ned as:
Î»( ËœDk) =
[ 1
Î» ( ËœD(1)
k )
1âˆ‘ m
i=1 Î» ( ËœD(i)
k )
,
1
Î» ( ËœD(2)
k )
1âˆ‘ m
i=1 Î» ( ËœD(i)
k )
, . . . ,
1
Î» ( ËœD(m)
k )
1âˆ‘ m
i=1 Î» ( ËœD(i)
k )
]
, (6)
such that,
Î»( ËœD(i)
k ) = Î»( ËœX(i)
k )P( ËœX(i)
k |ËœD(i)
k ) = DB
(
Î»( ËœX(i)
k ), P( ËœX(i)
k |ËœD(i)
k )
)
=
âˆ’ ln
âˆ« âˆš Î»( ËœX(i)
k ), P( ËœX(i)
k |ËœD(i)
k )d ËœX(i)
k , (7)
where DB is the Battacharyya distance and P( ËœXk|ËœDk) âˆ¼ N (Âµ ËœDk
, Î£ ËœDk
).
Action selection
Lâ€™s choice of whether to explore or exploit is guided by its awareness of the interaction
with the surrounding environment, which is conditioned directl y onto particle beliefs.
L uses the updated particlesâ€™ weights to evaluate the encountered s ituation among
familiar with (i.e., already seen by E) or not familiar with (i.e., a no vel situation not
seen by E) as illustrated in Fig.
5 and Fig. 6. Thus, L selects an action ak according to:
ak =
ï£±
ï£²
ï£³
argmax
akâˆˆA
P(ak|D(Î² )
k ), if Ïµk < Ï (exploitation),
q
(
akâˆ’ 1, ËœE ËœX(Î² )
k
)
, if Ïµk â‰¥ Ï (exploration).
(8)
In (
8), if Ïµk < Ï , this means that L is facing similar situation encountered by E and
so it will imitate Eâ€™s action selected from the active inference tab le Î“ deï¬ned as:
Î“ =
ï£®
ï£¯
ï£°
P(a1|D1), P(a2|D1), . . . , P(am|D1)
.
.
.
.
.
. ... .
.
.
P(a1|Dm), P(a2|Dm), . . . , P(am|Dm)
ï£¹
ï£º
ï£» (9)
where âˆ‘ m
i=1 P(ai|Dk) = 1 âˆ€k, P( ai|Dj ) = 1
m is the probability of selecting action
ai âˆˆ A conditioned to be in conï¬guration Dj âˆˆ D, A = { Ë™Âµ D1 , Ë™Âµ D2 , . . . , Ë™Âµ Dm } is the
set of available actions, Ïµk is the exploration rate given by Ïµk = 1 âˆ’ Î±k where Î±k is
12
exteroceptive information 
noisy observation
scenario memory
prediction
observation
x
y
Ñ²
â‹¯
the most similar prior experience
ğ·
1
ğ·
2
ğ·
3
ğ·
4
ğ·
5
ğ·
6
ğ·
7
ğ·
8
ğ·
9
ğ·
10
learning agent
dynamic object
Fig. 5 Observing a familiar conï¬guration. The learning agent has p roper knowledge about its current
interaction with the other dynamic object.
exteroceptive information 
noisy observation
scenario
prediction
observation
x
y
Ñ²
memory
â‹¯
the most similar prior experience
ğ·
1
ğ·
2
ğ·
3
ğ·
4
ğ·
5
ğ·
6
ğ·
7
ğ·
8
ğ·
9
ğ·
10
learning agent
dynamic object
Fig. 6 Observing a novel conï¬guration. The learning agent experie nces a new interaction with the
dynamic object than the learned conï¬gurations.
the weight of the winning particle computed as:
Î±k = max
i
{w(i)
k }N
i=1, (10)
such that 0 â‰¤ Î±k â‰¤ 1. In addition, Î² denotes the index of the particle with the
maximum weight given by:
Î² = argmax
i
{w(i)
k }N
i=1. (11)
In (
8), if Ïµk â‰¥ Ï, this means that L is facing a novel situation not seen before by E and
so L will explore new actions by using the GEs as explained in the comi ng sections.
Abnormality Indicators and GEs
The predictive messages (i.e., Ï€( ËœDk), Ï€( ËœX(i)
k )) propagated top-down the hierarchy are
compared against sensory responses signaled via diagnostic messages (i.e. , Î»( ËœX(i)
k ),
Î»( ËœDk)) passing from bottom to up the hierarchy, resulting in multiple ab normal-
ity indicators and generalized errors. Evaluating the abnormality measu rement at a
certain node allows evaluating to what extent the current observati ons support the
modelâ€™s predictions, while the GEs allow understanding of how we can suppress those
abnormalities in the future. The multi-level abnormality indicator s are deï¬ned as:
Î¥ ËœDk
= DKL
(
Ï€( ËœDk), Î» ( ËœDk)
)
+ DKL
(
Î»( ËœDk), Ï€ ( ËœDk)
)
, (12)
13
Î¥ ËœX(i)
k
= âˆ’ ln
(
BC(Ï€( ËœX(i)
k ), Î» ( ËœX(i)
k ))
)
, (13)
where DKL is the Kullbackâ€“Leibler divergence and BC is the Bhattacharyya coeï¬ƒcient.
The GE associated with ( 12) and conditioned on transiting from ËœDkâˆ’ 1 is deï¬ned as:
ËœE ËœDk
= [ ËœDk, P( Ë™E ËœDk
)] = [ ËœDk, Î»( ËœDk) âˆ’ Ï€( ËœDk)], (14)
where Ë™E ËœDk
is an aleatory variable described by a discrete probability density f unction
(pdf) P( Ë™E ËœDk
). While the GE projected on the GS space and associated with ( 13) can
be expressed as:
ËœE ËœX(i)
k
= [ ËœX(i)
k , P( Ë™E ËœX(i)
k
)] = [ ËœX(i)
k , Hâˆ’ 1 ËœE ËœZk
], (15)
where Ë™E ËœX(i)
k
is an aleatory variable described by a continuous pdf P( Ë™E ËœX(i)
k
) and ËœE ËœZk
âˆ¼
N (ËœÂµ ËœE ËœZk
, Î£ ËœE ËœZk
) characterized by the following statistical properties: ËœÂµ ËœE ËœZk
= ËœZk âˆ’
H ËœXk, Î£ ËœE ËœZk
= HÎ£ ËœE ËœZk
HâŠº + R, where ËœÂµ ËœE ËœZk
is the Kalman innovation computed in the
measurement space and Î£ ËœE ËœZk
is the innovation covariance.
Incremental active learning and inference
Active learning and active inference aim to reduce surprises (or abn ormalities), either
by developing a reliable world model or actively engaging with the env ironment [
39].
Active learning allows an agent to build a predictive model capturin g the novel worldâ€™s
regularities through model parameter exploration. In contrast, active inference allows
using the world model to infer the current context and consequent ly to infer what
to do through the active states exploration. These two types of exploration provide a
balanced trade-oï¬€ between adaptive behaviour that aims to minimize abn ormalities
by fulï¬lling the learnerâ€™s preferences on the one hand and acquirin g information about
the world on the other hand.
Active states exploration: When encountering surprising conditions, L can dis-
cover new actions to avoid future abnormal situations. While L is expl oring, its new
actions evolve from the previous actions and current GEs by a linear fun ction q(Â·) as
pointed out in ( 8), which is calculated with the ï¬rst-order Euler integration as follo ws:
q
(
akâˆ’ 1, ËœE ËœX(Î² )
k
)
= akâˆ’ 1 + âˆ† kP( Ë™E ËœX(Î² )
k
), (16)
where âˆ† k is the step size, akâˆ’ 1 is the previous performed action and P( Ë™E ËœX(Î² )
k
) is the
GEâ€™s pdf deï¬ned in ( 15).
Model parameter exploration: Under abnormal conditions and during explo-
ration, L can cluster the novel situations and encode them incremen tally in the WM
by updating the transition matrix and the active inference matrix, r espectively. It is
to note that during abnormal situations, new conï¬gurations might appear rep resent-
ing novel relative distances between L and the other dynamic object not experienced
by E. Thus, clustering the observed relative distance along with t he new actions
will lead to discovering new conï¬gurations and learning how to behav e by facing
14
them in the future (refer to Fig. 7). Consequently, a set C consisting of the rela-
tive distance-action pair can be performed during the abnormal period T (i.e., during
exploration) as C = {ËœZk, ak}T
t which can be used as input to the Growing Neural
Gas (GNG) for unsupervised clustering. GNG outputs a set of new conï¬gur ations
deï¬ned as D
â€²
= {Dm+1, Dm+2, . . . , Dm+n} = {D
â€²
1, D
â€²
2, . . . , D
â€²
n}, where n is the
total number of the newly acquired conï¬gurations and D
â€²
l âˆ¼ N (Âµ D
â€²
l
, Î£ D
â€²
l
) such that
D
â€²
l âˆˆ D
â€²
. Analysing the dynamic evolution of the new conï¬gurations allows estimat ing
the transition probability P( ËœDt|ËœDtâˆ’ 1) encoded in Î 
â€²
, which is deï¬ned as:
Î 
â€²
=
ï£®
ï£¯
ï£¯
ï£°
P(D
â€²
1|D
â€²
1), . . . , P(D
â€²
1|D
â€²
n)
.
.
. ... .
.
.
P(D
â€²
n|D
â€²
1), . . . , P(D
â€²
n|D
â€²
n)
ï£¹
ï£º
ï£º
ï£» , (17)
where âˆ‘ m
k=1 P(D
â€²
i|D
â€²
k) = 1 âˆ€i. Consequently, the updated global transition matrix
Î 
â€²â€²
âˆˆ R(m+n), (m+n) is expressed as:
Î 
â€²â€²
=
[ Î  0 m,n
0n,m Î 
â€²
]
, (18)
where Î  is the original transition matrix and Î 
â€²
is the newly acquired one.
Likewise, the newly discovered action-conï¬guration pairs characteri zed by
P(a
â€²
k|D
â€²
1) are encoded in Î“
â€²â€²
according to:
Î“
â€²
=
ï£®
ï£¯
ï£¯
ï£°
P(a
â€²
1|D
â€²
1), P(a
â€²
2|D
â€²
1), . . . , P(a
â€²
n|D
â€²
1)
.
.
.
.
.
. ... .
.
.
P(a
â€²
1|D
â€²
n), P(a
â€²
2|D
â€²
n), . . . , P(a
â€²
n|D
â€²
n)
ï£¹
ï£º
ï£º
ï£» , (19)
and the active inference table can be adjusted as follows:
Î“
â€²â€²
=
[ Î“ 1
n (JmÃ— n)
1
n (JnÃ— m) Î“
â€²
]
=
ï£®
ï£°
Î³ 11 . . . Î³ 1n
.
.
. ... .
.
.
Î³ n1 . . . Î³ nn
ï£¹
ï£» (20)
where JmÃ— n = [ aij ]mÃ— n and JnÃ— m = [ bji ]nÃ— m are the unit matrices, such that, aij =
bji = 1 âˆ€i, j. It is to note that Î“
â€²â€²
â€™s row do not summing 1 due to the addition of the
unit matrices and Î“
â€²
. Thus, normalization is needed, and it can be performed as:
Ë†Î“
â€²â€²
=
ï£®
ï£°
Ë†Î³ 11 . . . Ë†Î³ 1n
.
.
. ... .
.
.
Ë†Î³ n1 . . . Ë†Î³ nn
ï£¹
ï£» , (21)
where Ë†Î³ ij =
Î³ ij
âˆ‘ n
j=1 Î³ ij
âˆ€i.
15
explored trajectory exploited trajectory
start
predicted trajectory
(a)
k
ğœº
ğ’Œ+ğŸ
k+1 k+2 k+3
â‹¯
ğ·
1
ğ·
2
ğ·
3
ğ·
ğ‘€
ğ·
1
â€ 
ğ·
2
â€ 
GNG
ğœº
ğ’Œ+ğŸ
(b)
Fig. 7 A schematic view of an overtaking situation example used in o ur study: (a) exploratory
behaviour minimizing the divergence with the predicted tra jectory. (b) associating exploratory clus-
ters to the learning model.
Action Update
L relies on the abnormality indicators calculated at time k and deï¬ned in (
12) and
(13) to evaluate the performed actions at time k âˆ’ 1. Under abnormal conditions, L
learns how to avoid those abnormalities in the future by seeking inf ormation about
the surrounding environment and how to engage inside it based on the t wo types of
exploration discussed previously.
In contrast, during exploitation and under abnormal conditions, L update s the
existing active inference table and transition matrix using the di agnostic messages
(Î»( ËœDk), Î»(akâˆ’ 1)). The existing transition matrix can be updated using the GE deï¬n ed
in ( 14) as follows:
Ï€âˆ— ( ËœDk) = Ï€( ËœDk) + P( Ë™E ËœDk
). (22)
The active inference table Î“ can be adjusted according to:
Ï€âˆ— (ak) = Ï€(ak) + P( Ë™Eak ), (23)
where Ï€(ak) = P( Â·|ËœDk) is a speciï¬c row in Î“ and P( Ë™Eak ) is the GEâ€™s pdf related to
the active states that can be calculated as [ 40]:
Eakâˆ’1 = [ akâˆ’ 1, P( Ë™Eakâˆ’1 )] = [ akâˆ’ 1, Î»(akâˆ’ 1) âˆ’ Ï€(akâˆ’ 1)], (24)
where Î»(akâˆ’ 1) = Î»( ËœDk) Ã— P( ËœDk|akâˆ’ 1).
4 Experiments and Results Analysis
In this section, we evaluate the proposed framework in diï¬€erent sett ings. In the fol-
lowing, the experimental dataset is introduced ï¬rst. Then, the l earning process is
presented, consisting of oï¬„ine and online phases. Finally, the action -oriented model
and the learning cost are discussed and the performance of the proposed ap proach is
compared with two benchmark schemes, AIL [
16] and Q-learning [ 41].
16
(a)
 (b)
-1 -0.5 0 0.5 1 1.5
-1
-0.5
0
0.5
1
1.5
cluster
mean action
(c)
 (d)
Fig. 8 a) Autonomous vehicles: iCab1 and iCab2. b) An example of ove rtaking scenario. c) the
generated clusters, and d) the reference transition matrix based on AVs movements.
4.1 Experimental Dataset
The employed dataset was obtained from real experiments carried out on a univer-
sity campus for diï¬€erent manoeuvring situations, including two I ntelligent Campus
Automobile (iCab) vehicles [
42]. The expert demonstrations are collected during the
experiments by considering two AVs, iCab1 and iCab2, interacting in the environment
to perform a speciï¬c driving manoeuvre, i.e., iCab2 overtakes fr om the left side iCab1
where iCab2 is playing the role of an expert (E) and iCab1 represents a dynamic object
(O). Each AV ( i) is equipped with exteroceptive and proprioceptive sensors fr om which
odometry trajectories and control parameters are collected to analyze t he interactions
between AVs. The sensory modules provide 4-dimensional information , including the
positions of AV in ( x, y) coordinates, and the control parameters in terms of the AVâ€™s
velocity (i.e., Ë™x, Ë™y).
4.2 Oï¬„ine Learning Phase
The expert demonstrations collected from the interactions between the two AVs (i.e.,
iCab1 and iCab2) are used to learn the SM in an oï¬„ine manner as explained in S ection
3.3.1. The acquired SM consists of 24 joint clusters encoding the dynamic interaction
between the two AVs and the corresponding transition matrix is shown in Fig. 8.
Consequently, FP is initialized using 24 learned conï¬gurations, enc oding the position
data and control parameters, as explained in Section 3.3.2.
17
Table 2 The transition matrix evolution during
active learning.
Original Î  Explored Î  Merged Î  Updated Î 
24 clusters 6 clusters 30 clusters 28 clusters
4.3 Online Learning Phase
The FP model acquired oï¬„ine is enriched with active states to buil d the AFP model
as discussed in Section
3.4. AFP model enables the learner agent (L) in ï¬rst-person
during the online phase. While the L is experiencing a speciï¬c d riving task, it starts
evaluating the situation based on its belief about the environmental st ateâ€™s evolu-
tion and actual observations. During normal situations (i.e., when obser vations match
predictions), L decides to execute expert-like manoeuvres by imitating the expertâ€™s
actions (i.e., exploitation). In contrast, when an abnormal condition occ urs, i.e., an
unexpected situation, L starts exploring a new course of actions base d on observa-
tions and generalized errors, allowing to avoid abnormal situations in th e future and
to reach the goal successfully (e.g., overtaking a dynamic object).
4.3.1 Action-Oriented Model
Fig.
9-(a) depicts an example of how L agent switches between exploration and
exploitation while performing a driving task where the initial posi tion is diï¬€erent from
that seen in the demonstrations (i.e., from where the expert agent st arted). The learn-
ing agent recognizes an abnormal situation after comparing its current conï¬ guration
(which will lead to an unexpected trajectory) with the expected one. Therefore, L
adopts an exploratory policy to minimise the divergence between the ob served con-
ï¬gurations and the expected ones. It can be seen from the ï¬gure how the n ewly
explored actions allowed L to approach the reference model (i.e., go b ack to the nor-
mal situation) and switch back to exploitative behaviour, demonstrati ng that the
learned action-oriented model adapts eï¬ƒciently to changing variabili ty in the envi-
ronment. Moreover, Fig. 9-(b) illustrates the newly generated conï¬gurations and their
corresponding mean actions obtained from the new experience. The nov el learned
conï¬gurations obtained by clustering both the performed actions and the generalized
errors allow the L later to infer the best actions when facing similar situation.
The evolution of the transition matrix (Î ) is shown in Fig. 10 starting from the
original one (learned oï¬„ine from demonstrations) depicted in Fig. 8-(d). Fig. 10-(a)
shows the updated matrix during exploitation. While, Fig. 10-(b) displays the new
matrix estimated after clustering the trajectory that was followed during exploration,
encoding the novel conï¬gurations. Fig. 10-(c) shows the updated transition matrix
obtained after combining Fig. 10-(a) and Fig. 10-(b) and so by appending the explored
conï¬gurations to the original transition matrix. To avoid memorizing sim ilar conï¬gu-
rations, we used a predeï¬ned threshold to evaluate how similar the n ew conï¬gurations
are to the existing ones before deciding whether to append them. T his process will
lead to a reï¬ned transition matrix with fewer elements, as shown in Fig. 10-(d) and
Table 2.
18
Fig. 11 illustrates the exploratory behavioural responses during subsequ ent actions
performed in the experiments shown in Fig. 9-(a). The maximum particle weight after
each performed action is displayed in Fig. 11-(a). It can be shown that at the beginning
of the experiment, the distinction between the Lâ€™s performance and the expectation
is high, resulting in low particlesâ€™ weights and so leading the L to explore new actions
allowing to resolve uncertainties about the environment. Fig. 11-(b) demonstrates that
L makes exploratory and novelty-seeking choices at the beginning of th e experiment
and tends to minimize the exploration probability after updating its belief about the
best way to interact with the environment (after 15 trails). There fore L is conï¬dent
to behave imitatively, compels it to choose exploitative actions. T he panel presented
in Fig. 11-(b) illustrates whether L performs exploratory or exploitative acti ons as
-10 -5 0 5 10 15
X
-10
-8
-6
-4
-2
0
2
4
6
8
Y
Ground truth
Predicted trajectory
Observed trajectory
Explored trajectory
(a)
-10 -5 0 5 10
X
-10
-8
-6
-4
-2
0
2
4
6
8
Y
Original cluster
Explored cluster
Ground truth
Explored trajectory
Mean action
(b)
Fig. 9 A driving task example. In (a) L explores new actions based on generalized errors and (b)
shows the Original and explored clusters of expected and new trajectories followed by L.
19
(a)
 (b)
 (c)
 (d)
Fig. 10 Transition matrix evolution during the active learning pro cess.
indicated by the blue dots. Darker background implies higher certain ty about select-
ing an exploitative action. Moreover, Fig. 11-(d) shows that imitative behavior causes
decreasing mean error that represents the divergence between the agentâ€™s observation
and prediction.
(a)
5 10 15 20 25 30 35
Performed actions
0
0.2
0.4
0.6
0.8
1
Max. weight of particle
(b)
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38
Performed actions
exploration
exploitation
(c)
5 10 15 20 25 30 35
Performed actions
0
0.2
0.4
0.6
0.8
1
Probability
(d)
5 10 15 20 25 30 35
Performed actions
0
5
10
15Error
Fig. 11 The exploratory behavioural responses during subsequent a ctions in terms of (a) maximum
particles weights, (b) exploitation-exploration choices , (c) probability of exploration, and (d) mean
error quantifying the divergence between Lâ€™s observations and its prediction.
20
200 400 600 800 1000 1200 1400 1600 1800 2000
Episode
0
10
20
30
40
50
60
70Cumulative FE measurement
GNG - Calculated actions
(a)
200 400 600 800 1000 1200 1400 1600 1800 2000
Episode
0
10
20
30
40
50
60
70Cumulative FE measurement
GNG - Defined actions
(b)
200 400 600 800 1000 1200 1400 1600 1800 2000
Episode
0
10
20
30
40
50
60
70Cumulative FE measurement
No GNG - Calculated actions
(a)
200 400 600 800 1000 1200 1400 1600 1800 2000
Episode
0
10
20
30
40
50
60
70
80
90
100Cumulative FE measurement
No GNG - Defined actions
(b)
Fig. 12 Cumulative free energy. Models A, B, C, and D, respectively.
4.3.2 Cost of Learning
Updating and correcting the beliefs about the agentâ€™s surroundings min imize the FE
measurement via hierarchical processing in which prior expectat ions generate top-
down predictions of likely observations and where discrepancies be tween predictions
and observations ascend to hierarchically higher levels as prediction errors. In this
section, the eï¬ƒciency of 4 action-oriented models is studied in te rms of cumulative
FE measurement during 2 k training episodes as follows: i) Model A applies GNG to
cluster novel experienced trajectories and employs GEs to calc ulate the exploratory
21
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Episode
0
10
20
30
40
50
60
70Cumulative Reward
Our method
Q-learning
AIL
Fig. 13 Cumulative reward during 2 k training episodes.
actions, ii) Model B applies GNG to cluster novel experienced trajectories an d uses
predeï¬ned actions during exploration, iii) Model C only employs GEs to calculate the
exploratory actions, and iv) Model D only uses predeï¬ned actions during exploration.
Fig. 12 displays the comparison among the four models deï¬ned previously in t erms
of FE measurement, showing the superiority of Model A (which repr esents the pro-
posed approach) compared to the other models. This validates that clust ering the novel
conï¬guration and calculating the associated actions using the GEs greatly impact
minimizing the FE measurement during the active learning phase. Moreover, Fig. 13
evaluates the performance of the proposed model in terms of reward compar ed to the
Q-learning algorithm and AIL method (which adopt Model B).
5 Conclusion
This paper introduced a hierarchical self-awareness ADS that advanc es a probabilistic
computational account of action, observation, and imitation abilities grounde d in an
active inference framework. For learning and adaptation, the agent must t ransform the
sensory inputs into a reliable perception of the world. The proposed model comprises
several modules forming the perception-action cycle that links th e autonomous vehicle
to its environment. With inspiration from the biological brain, the di ï¬€erent modules
are linked together via message passing, where each module handles spe ciï¬c function-
alities. The hierarchical message passing is represented by infer ences and perceptions
across diï¬€erent modalities of multisensorial information (i.e., ext eroceptive and pro-
prioceptive data). Learning this association allows the learning mod el to predict the
perceptual consequences of acting. These representations are emplo yed to minimize
prediction errors and accurately predict how sensory signals will ch ange in response
to speciï¬c actions. The experimental evaluations show that modifyi ng and updating
the belief during the online learning phase via the back-project ion of detected errors
at the multilevel hierarchy solves the exploration-exploitation dil emma. It also shows
that the generalized hierarchical model learns incrementally from n ew information
provided by the perception of the surroundings to adapt to unexpect ed situations.
Declarations
â€¢ Funding University of Genoa
â€¢ Conï¬‚ict of interest/Competing interests Not applicable
â€¢ Ethics approval Not applicable
22
â€¢ Consent to participate Not applicable
â€¢ Consent for publication Not applicable
â€¢ Availability of data and materials Not applicable - Data and material restric ted by
the University Carlos III of Madrid
â€¢ Code availability Not applicable
â€¢ Authorsâ€™ contributions Sheida Nozari analyzed the data, conducted the ex peri-
ments, wrote the manuscript, and Ali Krayani, Pablo Marin, Lucio Marce naro,
David Martin Gomez and Carlo Regazzoni provided critical revisions.
References
[1] Buehler, M., Iagnemma, K., Singh, S.: The DARPA Urban Challenge:
Autonomous Vehicles in City Traï¬ƒc, 1st edn., Springer Publishing Com pany,
Incorporated (2009)
[2] Garcia, J., et al.: A Comprehensive Study of Autonomous Vehicle Bugs. In: 2020
IEEE/ACM 42nd International Conference on Software Engineering (ICSE) , pp.
385â€“396 (2020)
[3] Schlatow, J., et al.: Self-awareness in autonomous automotive systems. In: Design,
Automation & Test in Europe Conference & Exhibition (DATE), 2017, pp. 1050â€“
1055 (2017).
https://doi.org/10.23919/DATE.2017.7927145
[4] Regazzoni, C.S., et al.: Multisensorial Generative and Descriptive Self-Awareness
Models for Autonomous Systems. Proceedings of the IEEE 108(7), 987â€“1010
(2020) https://doi.org/10.1109/JPROC.2020.2986602
[5] Ravanbakhsh, M., et al. : Learning multi-modal self-awareness models for
autonomous vehicles from human driving. In: 2018 21st International Confer-
ence on Information Fusion (FUSION), pp. 1866â€“1873 (2018).
https://doi.org/
10.23919/ICIF.2018.8455667
[6] Nozari, S., Marcenaro, L., Martin, D., Regazzoni, C.: Observational learn ing: Imi-
tation through an adaptive probabilistic approach. In: 2021 IEEE International
Conference on Autonomous Systems (ICAS), pp. 1â€“5 (2021). IEEE
[7] Ravichandar, H., Polydoros, A., Chernova, S., Billard, A.: Recent adv ances in
robot learning from demonstration. Annual review of control, robotics, and
autonomous systems 3(ARTICLE), 297â€“330 (2020)
[8] Brantley, K., Sun, W., Henaï¬€, M.: Disagreement-regularized imit ation learning.
In: International Conference on Learning Representations (2019)
[9] Ongaro, G., Kaptchuk, T.J.: Symptom perception, placebo eï¬€ects , and the
bayesian brain. Pain 160(1), 1 (2019)
[10] Hohwy, J.: The Predictive Mind, OUP Oxford (2013)
23
[11] Priorelli, M., Stoianov, I.P.: Flexible intentions in the post erior parietal cortex:
An active inference theory. bioRxiv (2022)
[12] Millidge, B.: Combining active inference and hierarchical pred ictive coding: A
tutorial introduction and case study (2019)
[13] Barrett, L.F., Simmons, W.K.: Interoceptive predictions in t he brain. Nature
reviews neuroscience 16(7), 419â€“429 (2015)
[14] Friston, K., et al.: Active inference and learning. Neuroscience & Biobehavioral
Reviews 68, 862â€“879 (2016)
[15] Friston, K.: The free-energy principle: a uniï¬ed brain theory ? Nature reviews
neuroscience 11(2), 127â€“138 (2010)
[16] Nozari, S., Krayani, A., Marin-Plaza, P., Marcenaro, L., GÂ´ omez, D.M., R egazzoni,
C.: Active inference integrated with imitation learning for autonomous driving.
IEEE Access 10, 49738â€“49756 (2022)
[17] Paden, B., et al.: A survey of motion planning and control techniques for self-
driving urban vehicles. IEEE Transactions on intelligent vehicle s 1(1), 33â€“55
(2016)
[18] GonzÂ´ alez, D., et al.: A review of motion planning techniques for automated vehi-
cles. IEEE Transactions on intelligent transportation systems 17(4), 1135â€“1145
(2015)
[19] Pakdamanian, E., et al.: Deeptake: Prediction of driver takeover behavior using
multimodal data. In: Proceedings of the 2021 CHI Conference on Human Factors
in Computing Systems, pp. 1â€“14 (2021)
[20] Wang, Y., et al.: Trajectory planning and safety assessment of autonomous vehi-
cles based on motion prediction and model predictive control. IEEE Tr ansactions
on Vehicular Technology 68(9), 8546â€“8556 (2019)
[21] Onishi, T., et al.: End-to-end Learning Method for Self-Driving Cars with Tra-
jectory Recovery Using a Path-following Function. In: 2019 Internati onal Joint
Conference on Neural Networks, pp. 1â€“8 (2019).
https://doi.org/10.1109/IJCNN.
2019.8852322
[22] Chen, Z., Huang, X.: End-to-end learning for lane keeping of self-dr iving cars. In:
2017 IEEE Intelligent Vehicles Symposium (IV), pp. 1856â€“1860 (2017). IEEE
[23] Bojarski, M., et al.: End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016)
[24] Sauer, A., Savinov, N., Geiger, A.: Conditional aï¬€ordance learning for d riving
in urban environments. In: Conference on Robot Learning, pp. 237â€“252 (2018).
24
PMLR
[25] Vogt, D., Ben Amor, H., Berger, E., Jung, B.: Learning two-person inter ac-
tion models for responsive synthetic humanoids. Journal of Virtual Real ity and
Broadcastings 11(1) (2014)
[26] Droniou, A., et al.: Learning a repertoire of actions with deep neural networks.
In: 4th International Conference on Development and Learning and on Epigen etic
Robotics, pp. 229â€“234 (2014). IEEE
[27] Liu, M., Buntine, W., Haï¬€ari, G.: Learning how to actively learn: A d eep imitation
learning approach. In: Proceedings of the 56th Annual Meeting of the Asso ciation
for Computational Linguistics (Volume 1: Long Papers), pp. 1874â€“1883 (2018)
[28] Argall, B.D., Chernova, S., Veloso, M., Browning, B.: A survey of rob ot learning
from demonstration. Robotics and autonomous systems 57(5), 469â€“483 (2009)
[29] Ross, S., Bagnell, D.: Eï¬ƒcient reductions for imitation learning. In: Proceedings of
the Thirteenth International Conference on Artiï¬cial Intelligence and Statistics,
pp. 661â€“668 (2010). JMLR Workshop and Conference Proceedings
[30] Gangwani, T., Peng, J.: State-only imitation with transition dynamic s mismatch.
arXiv preprint arXiv:2002.11879 (2020)
[31] Ogishima, R., Karino, I., Kuniyoshi, Y.: Combining imitation and rei nforcement
learning with free energy principle (2020)
[32] Kueï¬‚er, A., Morton, J., Wheeler, T., Kochenderfer, M.: Imitat ing driver behav-
ior with generative adversarial networks. In: 2017 IEEE Intelligent Veh icles
Symposium (IV), pp. 204â€“211 (2017). IEEE
[33] Schroecker, Y., Vecerik, M., Scholz, J.: Generative predece ssor models for sample-
eï¬ƒcient imitation learning. arXiv preprint arXiv:1904.01139 (2019)
[34] Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of physiology-Paris 100(1-3), 70â€“87 (2006)
[35] Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behavior: a free-
energy formulation. Biological cybernetics 102(3), 227â€“260 (2010)
[36] Kim, S., Laschi, C., Trimmer, B.: Soft robotics: a bioinspired ev olution in robotics.
Trends in biotechnology 31(5), 287â€“294 (2013)
[37] Nozari, S., et al.: Active Inference Integrated With Imitation Learning for
Autonomous Driving. IEEE Access 10, 49738â€“49756 (2022) https://doi.org/10.
1109/ACCESS.2022.3172712
25
[38] Krayani, A., Baydoun, M., Marcenaro, L., Alam, A.S., Regazzoni, C.: Self -
Learning Bayesian Generative Models for Jammer Detection in Cognitiv e-UAV-
Radios. In: GLOBECOM 2020 - 2020 IEEE Global Communications Conference,
pp. 1â€“7 (2020).
https://doi.org/10.1109/GLOBECOM42002.2020.9322583
[39] Ofner, A., Stober, S.: Balancing Active Inference and Active Learni ng with Deep
Variational Predictive Coding for EEG. In: 2020 IEEE International Confe rence
on Systems, Man, and Cybernetics (SMC), pp. 3839â€“3844 (2020). https://doi.
org/10.1109/SMC42975.2020.9283147
[40] Krayani, A., Alam, A.S., Marcenaro, L., Nallanathan, A., Regazzoni, C.: A Novel
Resource Allocation for Anti-Jamming in Cognitive-UAVs: An Active Infere nce
Approach. IEEE Communications Letters 26(10), 2272â€“2276 (2022) https://doi.
org/10.1109/LCOMM.2022.3190971
[41] Watkins, C.J., Dayan, P.: Q-learning. Machine learning 8(3), 279â€“292 (1992)
[42] MarÂ´ Ä±n-Plaza., P., et al.: Stereo Vision-based Local Occupancy Grid Map for
Autonomous Navigation in ROS. In: Proceedings of the 11th Joint Conference
on Computer Vision, Imaging and Computer Graphics Theory and Applications
- Volume 3: VISAPP, (VISIGRAPP 2016), pp. 701â€“706. SciTePress, ??? (2016).
https://doi.org/10.5220/0005787007010706 . INSTICC
26