Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 418
nature computational science
Article
https://doi.org/10.1038/s43588-023-00439-w
Decoding reward‚Äìcuriosity conflict in 
decision-making from irrational behaviors
Yuki Konaka‚Äâ  ‚Äâ1 & Honda Naoki‚Äâ  ‚Äâ1,2,3,4 
Humans and animals are not always rational. They not only rationally 
exploit rewards but also explore an environment owing to their curiosity. 
However, the mechanism of such curiosity-driven irrational behavior is 
largely unknown. Here, we developed a decision-making model for a two-
choice task based on the free energy principle, which is a theory integrating 
recognition and action selection. The model describes irrational behaviors 
depending on the curiosity level. We also proposed a machine learning 
method to decode temporal curiosity from behavioral data. By applying 
it to rat behavioral data, we found that the rat had negative curiosity, 
reflecting conservative selection sticking to more certain options and that 
the level of curiosity was upregulated by the expected future information 
obtained from an uncertain environment. Our decoding approach can be 
a fundamental tool for identifying the neural basis for reward‚Äìcuriosity 
conflicts. Furthermore, it could be effective in diagnosing mental disorders.
Animals and humans perceive the external world through their sensory 
systems and make decisions accordingly1,2. Generally, they cannot make 
optimal decisions because of the uncertainty of the environment as 
well as the limited computational capacity of the brain and time con-
straints associated with decision-making3. In fact, they perform irra-
tional actions. For example, people play lotteries and gamble despite 
low reward expectations. In this case, they face a dilemma between 
low expected reward and curiosity regarding whether a reward will 
be acquired. Thus, understanding how animals control the balance 
between reward and curiosity is important for clarifying the whole 
decision-making process. However, a method is yet to be established for 
quantifying the reward‚Äìcuriosity balance has yet been established. In 
this study, we developed a machine learning method to decode the time 
series of the reward‚Äìcuriosity balance from animal behavioral data.
Some irrational behaviors emerge because of the strength of 
curiosity4,5. For example, conservative individuals avoid uncertainty 
and prefer to select an action that leads to predictable outcomes. 
Conversely, inquisitive individuals strongly desire to know the envi -
ronment rather than rewards and prefer to select an action that leads 
to unpredictable outcomes. T oo conservative and inquisitive natures 
can be interpreted as autism spectrum disorder and attention deficit 
hyperactivity disorder, patients with which are known to substantially 
avoid and seek novel information, respectively6‚Äì14. Rational individuals 
fall midway between these two extremes. In an ambiguous environ -
ment, they select an action to efficiently understand the environ -
ment, and if the environment becomes clear, they select an action to 
efficiently exploit the rewards. Therefore, curiosity has a major impact 
on behavioral patterns, and it is believed that animals control the bal-
ance between reward and curiosity in a context-dependent manner.
Decision-making has been modeled primarily by reinforcement 
learning (RL), which is a theory for describing reward-seeking adaptive 
behavior in which animals not only exploit rewards but also explore 
the environment. In RL, explorative behavior was addressed by a  
passive, random choice of action15. However, animals actively explore 
the environment by selecting actions that minimize the uncertainty  
of the environment given their curiosity.
Recently, the free energy principle (FEP) was proposed by Karl Friston  
under the Bayesian brain hypothesis, in which the brain optimally 
recognizes the outside world according to Bayesian estimation 16‚Äì18. 
The FEP addresses not only the recognition of the external world but 
also the information-seeking action selection, which minimizes the 
uncertainty of the recognition of the external world, known as ‚Äúactive 
Received: 21 June 2022
Accepted: 29 March 2023
Published online: 15 May 2023
 Check for updates
1Laboratory of Data-Driven Biology, Graduate School of Integrated Sciences for Life, Hiroshima University, Hiroshima, Japan. 2Kansei-Brain Informatics 
Group, Center for Brain, Mind and Kansei Sciences Research, Hiroshima University, Hiroshima, Japan. 3Theoretical Biology Research Group, Exploratory 
Research Center on Life and Living Systems, National Institutes of Natural Sciences, Okazaki, Japan. 4Laboratory of Theoretical Biology, Graduate School 
of Biostudies, Kyoto University, Kyoto, Japan. ‚Äâe-mail: nhonda@hiroshima-u.ac.jp
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 419
Article https://doi.org/10.1038/s43588-023-00439-w
process, the agent selects an action based on the current recognition 
and curiosity (Fig. 1a, process 2). The agent repeats these two processes 
in the two-choice task.
We modeled the first process by sequential Bayesian estimation, 
under the assumption that reward probabilities latently fluctuate in 
time (Fig. 1b ). The agent updates the belief about the reward prob -
abilities, which are expressed as estimation distributions, in response 
to actions and consequence reward observations (Fig. 1c). We derived 
the equations of the belief update (Fig. 1d and Methods). We modeled 
the second process by action selection based on two kinds of motiva-
tions: the desires to maximize the reward and to gain the information 
from the environment (Fig. 1e). This sum, called ‚Äòexpected net utility‚Äô 
in this study, can be expressed by
Ut (at+1) = E[Rewardt+1]+ct ‚ãÖE[Infot+1], (1)
where a and t indicate the action and trial index, respectively, and E[x] 
denotes the expectation value of x based on current recognition. The 
first and second terms represent expectations of reward and informa-
tion derived from a new observation, respectively, for the next action 
at+1 based on current recognition. ct denotes a meta-parameter describ-
ing the intensity of curiosity, which weighs the expected information 
gain (see Methods for detail). We assumed an irrational mental conflict 
as ct varies over time (Fig. 1f). In decision-making, the agents prefer to 
select action at+1 with the higher expected net utility, in which the action 
is selected probabilistically following a sigmoidal function
P(at+1) = 1
1+exp(‚àíŒ≤ŒîUt)
, (2)
where ŒîUt = Ut(at‚Äâ+‚Äâ1)‚Äâ‚àí‚ÄâUt(at+1), and Œ≤ denotes the inverse temperature 
controlling the randomness of action selection22,29,30.
Recognition and decision-making in the simulation
T o validate our model, we performed simulations with constant curi-
osity ct‚Äâ=‚Äâ1 for two cases. In the first case, where reward probabilities 
were constant and different between the two options (Fig. 2a), the 
agent preferred to select the option with the higher reward probability 
(Fig. 2b). The recognized reward probabilities converged to ground 
truths, indicating that the agent accurately recognized the reward 
probabilities (Fig. 2c). The recognition confidence changed over time 
depending on the behavior in each trial; the confidence of the selected 
option increased with information from the observation, whereas that 
of the unselected option decreased because of the agent‚Äôs assumption 
regarding the fluctuation in reward probabilities (Fig. 2d). Similarly, 
the expected information gain of an option increased and decreased 
when that option was selected and unselected, respectively. Thus, 
the expected information gain was lower for the option with higher 
confidence (Fig. 2e). The expected reward followed the recognized 
reward probability (Fig. 2f). Initially, decreasing expected informa-
tion gain and increasing expected reward eventually cross at some 
number of trials, which correspond to switching between information 
exploration and reward acquisition (Supplementary Fig. 1a). These two 
factors are negatively correlated, indicating a trade-off relationship 
(Supplementary Fig. 1b). The expected net utility, which is the sum of 
the expected information gain and reward, represents the value of each 
selection (Fig. 2g), resulting in the agent‚Äôs preferentially selecting the 
option with the higher expected net utility.
In the second case, we assumed a dynamic environment with a 
time-dependent reward probability (Fig. 2h). In the simulation, the 
agent adaptively changed its recognition of the reward probability 
following the change in the true reward probability, and selected the 
option with the higher estimated reward probability (Fig. 2i,j). The 
confidence was affected by the uncertainty of the reward probability 
at each time; the confidence was high where the reward probabilities 
inference‚Äù19‚Äì21. Furthermore, FEP proposed a score of action, called 
expected free energy, which consists of the expected reward and curi-
osity with the same unit22‚Äì27. Thus, action selection can be formulated 
by maximizing both reward and curiosity. Note that curiosity can be 
regarded as information gain, that is, the extent to which we expect our 
recognition to be updated by the new observation through the action. 
However, FEP assumes that the weighting of rewards and curiosity is 
always even and constant. Although a previous FEP study modeled 
active inference in a two-choice task, it assumed a constant intensity of 
curiosity and thus could not treat actual animal behaviors in which the 
weights of rewards and curiosity are expected to change over time28. 
Hence, conventional theories such as RL and FEP are limited in describ-
ing the conflict between reward and curiosity.
Identifying the temporal variability of curiosity is important for 
future clarifying the neural mechanisms of the reward and curiosity 
conflicts in decision-making. Many FEP studies have been devoted to 
the construction of theory, assuming that the decision-making pro -
cesses of animals are Bayes optimal. Thus, there was not even the idea 
that animals irrationally make decisions depending on the reward and 
curiosity conflicts. For this reason, a method to decode the temporal 
balance between reward and curiosity from behavioral data is yet to 
be established. Such a method would enable us to analyze neural cor-
relates with the temporal variability of curiosity, and consequently, it 
would help us clarify how the brain controls the balance of reward and 
curiosity in a context-dependent manner.
In this study, we extended FEP by incorporating a meta-parameter 
that controls the conflict dynamics between reward and curiosity, 
called the reward‚Äìcuriosity decision-making (ReCU) model. The ReCU 
model can exhibit various behavioral patterns, such as greedy behavior 
toward reward, information-seeking behaviors with high curiosity and 
conservative behaviors avoiding uncertainty. Moreover, we developed 
a machine learning method called the inverse FEP (iFEP) method to esti-
mate the internal variables of decision-making information processing. 
Applying the iFEP method to a behavioral time series in a two-choice 
task, we successfully estimated the internal variables, such as varia -
tions in curiosity, recognition of reward availability and its confidence.
Results
Decision-making with the reward‚Äìcuriosity dilemma
Animals perceive the environment by inferring causes such as reward 
availability from observation, and then they make decisions based on 
their own inferences. In this study, we developed an ReCU model of a 
decision-making agent facing a dilemma between reward and curiosity 
in a two-choice task, wherein the agent selects either of two choices 
associated with the same rewards but with different reward probabili-
ties (Fig. 1a). If the agent aims to maximize cumulative rewards, the 
agent must select an option with a higher reward probability. However, 
in animal behavioral experiments, even after they learned which option 
was more associated with a reward, they did not exclusively select the 
best choice, but also often selected the option with a smaller reward 
probability, which seems unreasonable.
Here, we hypothesized the following: Animals assume that the 
reward probability for each option might fluctuate over time, and 
therefore, the continuous selection of one option decreases the 
 
confidence of the reward probability estimation for the other option. 
Thus, they become curious about the ambiguous option even with a 
smaller reward probability, and so selecting the ambiguous option is 
reasonable for increasing the confidence of the estimation for both 
options. Therefore, we considered that the agent should make deci -
sions driven by reward and curiosity in a situation-dependent manner.
ReCU model
In the ReCU model, we divided information processing in the brain into 
two processes. In the first process, the agent updates the recognition of 
the reward probability of each option (Fig. 1a, process 1). In the second 
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 420
Article https://doi.org/10.1038/s43588-023-00439-w
were near-deterministic, around 1 and 0, but low where the reward 
probabilities were uncertain, i.e., approximately 0.5 (Fig. 2k ). The 
expected information gain of an option was negatively correlated 
with the confidence of the option (Fig. 2l), suggesting that the agent 
was curious about the uncertain option. The expected reward just 
varied depending on the recognized reward probability (Fig. 2m).  
In this case, we also observed a switching between information explo-
ration and reward acquisition at the initial phase (Supplementary  
Fig. 1c). In contrast to the above case, the expected reward and expected 
information gain did not show clearly linear correlation because of the 
dynamic environment (Supplementary Fig. 1d). The expected net util-
ity changed similarly to the expected reward; however, the difference 
between the left and right options was less pronounced because of 
curiosity (Fig. 2n). These two demonstrations indicate that our model 
can represent the process of cognition and decision-making based on 
reward and curiosity.
Discrimination of passive and curiosity-dependent behaviors
The behavioral difference based on curiosity is interesting. The 
expected net utility can be rewritten as
ŒîUt = ŒîE[Rewardt+1]+ct ‚ãÖŒîE[Infot+1], (3)
where the first and second terms represent the differences between 
the expected reward and information gain of two options, respectively. 
Thus, the agent is decided based on the balance between ŒîE[Rewardt+1] 
Process 1:
recognition
f
a
Process 2:
action selection
or
b
c
d
Latent
variable
Observation
Action
w t‚Äì2 w t‚Äì1 w t w t+1
ot‚Äì2 ot‚Äì1 ot ot+1
at‚Äì2 at‚Äì1 at at+1
Previous
belief
New
belief
EstimateEstimate
Update
Recognition process
Latent variable
controlling
reward probability
Action selection process
e
R
I IR
R I
U t(a t+1 ) = E[Reward] + c tE[Info]
ct = ct‚Äì1  + Œ≥ noise
œÉt‚Äì1
¬µt‚Äì1
œÉt
¬µt
Time
Curiosity¬µt = ¬µ t‚Äì1  + Œ± K t {o t ‚Äì f(¬µt‚Äì1 )}
œÉ 2
t
1 1= + f(¬µ t){1 ‚Äì f(¬µ t)}
Kt
Fig. 1 | Decision-making model for the two-choice task with reward‚Äìcuriosity 
dilemma. a, Decision-making in the two-choice task. Reward is provided at 
different probabilities for each option. The agent does not know those 
probabilities. Through repeated trial and error, the agent recognizes the world by 
inferring the latent reward probability of each option, and decides to choose the 
next action, that is, option, based on its own inference. b, Sequential Bayesian 
estimation as a recognition process. The agent assumes that the reward 
probabilities change over time owing to the fluctuation in the latent variable 
controlling reward probability. c, Belief updating. The agent recognizes the 
latent variable as a probability distribution. d, The update rule of the mean and 
variance of the estimation distribution for each option. Œ±, Kt and f(Œºt) indicate the 
learning rate, Kalman gain, and the prediction of the reward probability, 
respectively. The second term in both equations disappears if the option is not 
selected. e, The action selection process by the agent. The agent evaluates the 
expected net utility 
Ut(at+1) of each action using the weighted sum of the 
expected reward and information gain, as shown in the equation. The agent 
compares the expected net utilities for both actions and prefers the option with 
the larger expected net utility. f, Time-dependent curiosity. The intensity of 
curiosity changes over time owing to the fluctuation of ct.
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 421
Article https://doi.org/10.1038/s43588-023-00439-w
and ŒîE[Infot+1]. Here, we created a diagram visualizing the selected 
actions in the space of ŒîE[Rewardt+1] and ŒîE[Infot+1]. Depending on 
the intensity of curiosity c t, the left and right actions can be sepa ra-
ted by a boundary of ŒîE[Rewardt+1]+ct ‚ãÖŒîE[Infot+1] = 0 (Fig. 2o‚Äìq).  
The agent with ct‚Äâ=‚Äâ0 selected an option based only on ŒîE[Rewardt+1]  
(Fig. 2p). In contrast, if ct was a nonzero value, the boundary leaned to 
a different direction depending on the positive and negative values of 
ct (Fig. 2o,q). These results indicate that passive choice (c t‚Äâ=‚Äâ0) and 
curiosity-dependent choice (ct ‚â† 0) can be discriminated based on the 
distributed pattern of selected actions in the space of ŒîE[Rewardt+1] 
and ŒîE[Infot+1].
Curiosity-dependent irrational behaviors
We examined how behavioral patterns are regulated by the intensity 
of curiosity and the degree of reward seeking (Fig. 3 ). In a scenario 
where the reward probabilities were zero for the left and 0.5 for the 
right (Fig. 3a), we simulated a model by varying the meta-parameters 
c and Po, which is a control parameter of the reward amount (Fig. 3b and 
Methods). When the agent strongly desired the reward (Po = 0.99)  
with no curiosity (c‚Äâ=‚Äâ0), the agent preferred the right option with a 
higher reward probability (Fig. 3c, point a). If the agent had no desire 
for a reward (Po = 0.5) with high curiosity (c‚Äâ=‚Äâ0), the agent preferred 
the option with a higher reward probability (Fig. 3c, point b). Although 
this behavior seems to be rational at first glance, the agent did not seek 
the reward, but rather sought the information (i.e., belief update) driven 
by curiosity, which resulted in a preference for the uncertain option. 
When the agent has negative curiosity (c‚Äâ=‚Äâ‚àí10), the agent continuously 
selected either of the two options depending on the first selection  
(Fig. 3c, point c). In this behavior, the agent conservatively selected 
the more certain option, as patients with autism spectrum disorder 
irrationally avoid new information and repeat the same choices6‚Äì11.
In addition, we obtained a nontrivial result in another scenario, 
where the reward probabilities were 0.5 for the left and 1 for the right 
(Fig. 3d‚Äìf). As in the previous scenario, the agents with a strong desire 
for the reward (Po = 0.99, nearly equal to 1) preferred the right option 
with a higher reward probability (Fig. 3f , point a). The agent with no 
desire for reward (P o‚Äâ=‚Äâ0.5) and high curiosity (c ‚Äâ=‚Äâ10) preferred the  
left option with a lower reward probability (Fig. 3f , point b). This  
seemingly irrational behavior was the outcome of focusing on satis -
fying curiosity and not seeking rewards, which recalls patients with 
attention deficit hyperactivity disorder irrationally exploring new 
information12‚Äì14. In addition, as seen in the previous scenario (Fig. 3c, 
point c), the agents with negative curiosity ( c‚Äâ=‚Äâ‚àí10), irrespective of  
the desire for the reward, exhibited conservative selection (Fig. 3f , 
point c). In combination, these results clearly indicate that behavioral 
patterns largely depend on the degree of conflict between reward  
and curiosity.
Inverse FEP: Bayesian estimation of the internal state
In the above cases, we assumed a constant balance between reward 
and curiosity. However, our feelings swing in a context-dependent 
manner. Although it is important to decipher the temporal swinging 
of the conflict between reward and curiosity in terms of neuroscience 
and psychology, it is difficult to quantify the conflict because of its 
temporal dynamics. Here, we addressed the inverse problem to esti-
mate the internal states of the agent from behavioral data, known as 
computational phenotyping or meta-Bayesian inference31‚Äì35. T o this 
end, we developed a machine learning method called iFEP to quantita-
tively decipher the temporal dynamics of the internal state including 
the curiosity meta-parameter from behavioral data.
For developing iFEP, we needed to switch the viewpoint from the 
agent to the observer of the agent, that is, from animals to us. In the 
state-space model (SSM) from the viewpoint of the agent, we described 
the sequential recognition of reward probabilities by the agent (Figs. 1b  
and 4a,b). Conversely, we developed a state-space model from the 
observer‚Äôs eye (the observer-SSM) to determine the internal state 
of the agent, for example, the intensity of curiosity c t, recognition  
Œºi,t and its confidence Pi,t (i.e., inverse of the variance of the estimation 
distribution) (Fig. 4c). In the observer-SSM, the intensity of curios -
ity was assumed to change continuously over time, and the agent‚Äôs 
recognition of the reward probability was updated by using the equa-
tions shown in Fig. 1c following the FEP; however, they were unknown 
to the observers. In addition, the agent‚Äôs actions were assumed to be 
generated depending on the intensity of its curiosity, recognition and 
confidence, as described in equation ( 2), but the observers can only 
monitor the agent‚Äôs action and the presence of a reward. In iFEP, based 
on the observer-SSM, we estimate the latent internal state of agent z 
from observation x in a Bayesian manner as
P(z1‚à∂T|x1‚à∂T) ‚àù P(x1‚à∂T|z1‚à∂T)P(z1‚à∂T), (4)
where z1‚à∂T = {Œºi,1‚à∂T,pi,1‚à∂T,c1‚à∂T},x1‚à∂T = {a1‚à∂T,o1‚à∂T}, and the subscript 1:T 
indicates steps 1 to T. In this Bayesian estimation, a posterior distribu-
tion P(z1‚à∂T|x1‚à∂T) represents the observer‚Äôs recognition of the  
estimated z1‚à∂T given observation x1:T with uncertainty. A prior distribu-
tion P(z1‚à∂T) represents our belief, which is expressed as the ReCU model 
with the random motion of the curiosity meta-parameter c as
ct = ct‚àí1 +œµŒ∂t, (5)
where Œ∂t indicates the white standard Gauss noise and œµ indicates its 
noise intensity. The likelihood P(x1:T|z1:T) represents the probability that 
x1:T was observed assuming z1:T, which also follows the ReCU model. 
This Bayesian estimation, namely iFEP, was conducted using a particle 
filter and Kalman backward algorithm (Methods).
Validation of iFEP with artificial data
We tested the validity of the iFEP method by applying it to the artificial 
data generated by the ReCU model. We simulated a model agent with 
nonconstant curiosity in the two-choice task, where reward probabil-
ities varied temporally. We then demonstrated that iFEP estimated the 
ground truth of the internal state of the simulated agent, that is, the 
agent‚Äôs intensity of curiosity, recognition and confidence (Supplemen-
tary Fig. 2). We also confirmed that the estimation performance is 
robust against the value of œµ (Supplementary Fig. 3). Therefore, iFEP 
is in a position to provide efficient estimators of belief updating to 
clarify decision-making processing and the accompanying temporal 
swing in the conflict between reward and curiosity.
Fig. 2 | Simulations of the decision-making model. a, The two-choice task with 
constant reward probabilities. b, The selection probabilities for the left and right 
options, plotted as a moving average with window width of 101. c, The recognized 
reward probabilities for the left and right options compared with the ground 
truths depicted by dashed lines. d, The confidences of reward probability 
recognitions for left the and right options. e‚Äìg, The expected brief updates (e), 
expected reward (f) and expected net utility (g) for the left and right options.  
h, The two-choice task with constant and temporally varying reward probabilities 
for the left and right options. i‚Äìn, The same as b‚Äìg with parameter values of  
c = 1, Po = 0.8, Œ± = 0.05, Œ≤ = 2 and œÉw = 0.63. o‚Äìq, The selected options in a 
space of left‚Äìright differences of the expected reward and information gain. The 
ReCU model was simulated with dynamically changing reward probabilities for 
different intensities of curiosity: 
c =‚àí 1 (o), c = 0 (p) and c = 3 (q). The reward 
probabilities were generated by the Ornstein‚ÄìUhlenbeck process of w for 1,000 
trials: wi,t = wi,t‚àí1 ‚àí0.01wi,t‚àí1 +0.15Œæt, where Œæt indicates the standard Gauss 
noise. The heatmap represents the probability of action selection in the space 
(equation (2)).
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 422
Article https://doi.org/10.1038/s43588-023-00439-w
20%80%
Selection probability
Information
gain
Expected reward Expected net utility
Estimated
reward probability
Confidence
Selection probability
Information
gain
Expected reward Expected net utility
Estimated
reward probability
Confidence
Trials
b
a
i
h
c d
o p q
e f g
j k
l m n
Reward probability
Reward probability
Trials
Trials
Reward probability Reward probability
Trials
Right
Left
Trials
0
0
0.25
0.50
0.75
1.00
0
0.25
0.50
0.75
1.00
0
11
22
33
44
500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Left
Left
Right
Left
Right
Left
Right
Right
Left
Right
Left
Right
Left
Right
Left
Right
‚àÜ reward
‚Äì1.0
‚Äì1.0
‚Äì0.5
‚Äì0.5
0
0
0.5
0.5
1.0
1.0
‚àÜ reward
‚Äì1.0 ‚Äì0.5 0 0.5 1.0
‚àÜ reward
‚Äì1.0 ‚Äì0.5 0 0.5 1.0
‚àÜ info gain
‚Äì1.0
‚Äì0.5
0
0.5
1.0
‚àÜ info gain
‚Äì1.0
‚Äì0.5
0
0.5
1.0
‚Äì0.5
0
0.5
1.0
1.5
‚àÜ info gain
Left
Right
Probability of selecting the right
Left Left
Right
Left Right
Left
Right
Right
Left
Left
Right
Right
Right Left Right
Left
0
0.15
0.30
0.45
0.60
0
0.3
0.6
0.9
1.2
0
0.35
0.70
1.05
1.40
0
0.25
0.50
0.75
1.00
0
0.25
0.50
0.75
1.00
0
25
50
75
100
0
0.2
0.4
0.6
0.8
0
0.3
0.6
0.9
1.2
0
0.35
0.70
1.05
1.40
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Trials
0 500 1,000
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 423
Article https://doi.org/10.1038/s43588-023-00439-w
iFEP-decoded internal state behind rat behaviors
We applied iFEP to actual rat behavioral data from the two-choice 
task experiment with temporally varying reward probabilities 36  
(Fig. 5a). In this experiment, once the reward probabilities were sud-
denly changed in a discrete manner, the rat slowly adapted to select 
the option with the higher reward probability (Fig. 5b), suggesting that 
the rat sequentially updated its recognition of the reward probability. 
Based on these behavioral data of the rat, iFEP estimated the internal 
state, that is, the intensity of curiosity, the recognized reward prob-
abilities and their confidence levels (Fig. 5c‚Äìe). We found that the rat 
was not perfectly aware of the true reward probabilities but was able 
to recognize increases and decreases in reward probability (Fig. 5d,e). 
We also found that confidence increased with choice and decreased 
with no choice (Fig. 5f,g).
With iFEP, we can examine whether the rat subjectively assumed 
fluctuating or constant environments, that is, reward probabilities.  
We estimated the degree of fluctuation of the reward probabilities the 
rat assumes pw = 1.785 (that is, œÉ2
w = 0.560) from the rat behavioral 
data. This estimated value implied that the latent variable controlling 
the reward probabilities showed a random walk with increasing s.d. 
with trials as ‚àöœÉ2
wt. Compared with a reward probability represented 
by the sigmoidal of the latent variable, the estimated reward probability 
can largely change from 0.5 to 0.5‚Äâ¬±‚Äâ0.4 during only ten trials (Supple-
mentary Fig. 4). Therefore, it was suggested that the rat assumed  
fluctuating environments and, thus, easily forgot its recognition and 
lost its confidence.
Negative curiosity and its dynamics decoded by iFEP
Interestingly, the curiosity held by the rat was estimated to be negative 
for almost all trials (Fig. 5h). In other words, the rat conservatively  
preferred certain choices but did not explore uncertain choices.  
Ratio
Point a
Point a
Point a
Point c
Point b
Point b Point a
Point c
Point b
Point c
Point b
Point c
Reward intensity, P o
0.50
1.0
0.5
0
Ratio
1.0
0.5
0
1.0
0.5
0
1.0
0.5
0
1.0
0.5
0
Ratio
1.0
0.5
0
1.0
0.5
0
1.0
0.5
0
10
‚Äì10
0
0.75 1.00
Reward intensity, P o
0.50 0.75 1.00
Left
Intensity of curiosity, C
t
10
‚Äì10
0
1
0
Intensity of  curiosity, C
t
0% 50% 50% 100%
a
b
d
e
c f
Fraction of selecting the right
Right Left Right
Left Right Left Right
Left Right Left Right
Left Right Left Right
Fig. 3 | Curiosity-dependent irrational behaviors. a, The two-choice task with 
different, constant reward probabilities: 0% for the left and 50% for the right.  
b, The heatmap of the selection probability of the right option as a function of the 
parameters of curiosity and reward intensity. The probability was obtained 
empirically by running 1,000 simulations for each set of curiosity and reward. 
Three representative conditions are indicated by black dots: reward-seeking 
(point a; 
c = 0,Po = 1), information-seeking (point b; c = 10,Po = 0.5) and 
information-avoiding (point c; c =‚àí 10,Po = 0.75). c, Box plots showing the 
selection ratios of the right option for the conditions at points a‚Äìc, where the 
central line indicates the median, the edges are the lower and upper quantiles and 
the upper and lower whiskers represent the highest and lowest value after 
excluding outliers, respectively. At point c, the model agent dominantly selected 
either the right or left option, where the left- and right-dominantly selected 
simulations occurred 505 and 495 times, respectively. The box plots for point c 
appear crushed because the data points are too densely packed. d‚Äìf, The same as 
a‚Äìc for the two-choice task with different, constant reward probabilities: 50% for 
the left and 100% for the right. At point c in f, the left- and right-dominantly 
selected simulations occurred 489 and 511 times, respectively.
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 424
Article https://doi.org/10.1038/s43588-023-00439-w
This negative curiosity is reasonable for the starved animals because 
animals desired to obtain more reward with higher confidence. T o 
validate the negative curiosity, we visualize the selected action in the 
space of ŒîE[Rewardt+1] and ŒîE[Infot+1], as shown in Fig. 2o‚Äìq   
(Fig. 6a‚Äìc). When the estimated ct was negative, the rat dominantly 
selected the left action in a region with positive ŒîE[Rewardt+1] and 
negative ŒîE[Infot+1] (Fig. 6a for ct‚Äâ<‚Äâ‚àí1.1; Fig. 6b for ‚àí1.1 ‚â§ ct <‚àí 0.7), 
clearly indicating that the rat has positive subjective reward and nega-
tive curiosity. When the estimated ct was close to 0, the rat selected 
both actions based on ŒîE[Rewardt+1], independent of ŒîE[Infot+1]  
(Fig. 6c for ‚àí0.7 ‚â§ ct). These results clearly supported the expected 
net utility with positive weight for the expected reward and time-
dependent weight for the expected information gain. In addition, we 
statistically tested negative curiosity (P‚Äâ<‚Äâ0.01 for Monte Carlo testing 
in Supplementary Fig. 5).
Further, we noticed an increase in the estimated level of curiosity at 
which the reward probabilities changed suddenly (Fig. 5h). This curios-
ity dynamics can be interpreted such that the rat recognized the rule 
change and adaptively controlled the extent to which the rat sought 
new information. We further examined how the curiosity is regulated 
by recognized environmental information. We did not detect the cor-
relation between the estimated curiosity and expected information 
gains (Fig. 6d,e). Moreover, we found that the temporal derivative of 
the estimated curiosity highly correlated with the sum of the expected 
information gains for both options (Fig. 6f,g). These results implied that 
the rat actively upregulated the curiosity level when the uncertainty  
in the recognized reward probabilities increases, such as,
dc
dt
‚àù ‚àë
i
E[Infoi]. (6)
Evaluations of alternative models from rat behaviors
Finally, to further confirm the validity of the ReCU model, we compared 
it with other decision-making models based on the rat behavioral data. 
As an alternative version of the expected net utility, we introduced the 
time-dependent desire for reward as
Ut (at+1) = dt ‚ãÖE[Rewardt+1]+E[Infot+1], (7)
where dt denotes a meta-parameter describing subjective reward (see 
Methods for details). With this alternative model, the time series of dt 
were estimated by iFEP from the rat behavioral data. We found that the 
estimation of the subjective reward meta-parameter dt changed dynam-
ically and sometimes became close to zero when the rat encountered 
drastic changes in the reward probabilities (Supplementary Fig. 6),  
which indicated that the rat suddenly no longer needed rewards. This 
Latent
variable
Observation
Action
zt‚Äì2 zt‚Äì1 zt zt+1
ot‚Äì2 ot‚Äì1 ot ot+1
at‚Äì2 at‚Äì1 at at+1
at‚Äì2 at‚Äì1
Observing
Observation
Action
a
c
b
I am wondering:
- How much is the reward probability?
- Which option should be selected?
I am wondering:
- How does the agent recognize the reward probability?
- How much confidence does the agent have in its recognition?
Latent
variable
Observation
Intensity of
curiosity
Action
¬µt‚Äì2
œÉ2
t‚Äì2
ot‚Äì2 ot‚Äì1 ot ot+1
ct‚Äì2 ct‚Äì1 ct ct+1
at at+1
Old
belief
New
belief
Update
R
I IR
¬µt‚Äì1
œÉ2
t‚Äì1
¬µt
œÉ2
t
¬µt+1
œÉ2
t+1
Fig. 4 | The scheme of iFEP by an observer of a decision-making agent.  
a, An agent performing a two-choice task from the observer‚Äôs perspective.  
b, The observer‚Äôs assumption about the agent‚Äôs decision-making. The agent is 
assumed to follow the decision-making model, as described in Fig. 1. c, The SSM 
of the observer‚Äôs eye. For the observer, the agent‚Äôs reward‚Äìcuriosity conflict, 
recognized reward probabilities and their uncertainties are temporally varying 
latent variables, whereas the agent‚Äôs action and the presence/absence of a reward 
are observable. The observer estimates the latent internal states of the agent.
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 425
Article https://doi.org/10.1038/s43588-023-00439-w
should be unnatural for animals that were starved before the experi-
mental task to motivate them to obtain food. Thus, the alternative 
model is not suitable for describing the rat behavior.
Another possible model is Q-learning in the framework of RL, 
which has been widely used to model the decision-making tasks. Fol-
lowing previous studies36‚Äì38, we introduced the time-dependent inverse 
temperature Bt, which controls the randomness of the action selec -
tion; however, it does not lead to information-seeking behavior in the 
ReCU model. With the Q-learning model, we estimated time series of Bt 
(Methods). Then, we determined that the Bt decreased when the reward 
probabilities suddenly changed (Supplementary Fig. 7), meaning that 
the rat tends to perform a random selection of actions in response 
to the environmental rule change. Although this dynamic behavior 
of the inverse temperature seems reasonable, it is unknown how it is 
regulated in the Q-learning model. Here, we hypothesized that Bt could 
be regulated by the uncertainty of our recognition. T o probe this, we 
compared Bt and the expected information gain, where the former 
was estimated based on Q-learning and the latter was estimated by 
iFEP in the ReCU model. We found that they are positively correlated 
(Supplementary Fig. 7) as
Bt ‚àù ‚àë
i
E[Infot,i]. (8)
Therefore, Q-learning requires a cue regarding the uncertainty of 
recognition, that is, the expected information gain, which supports the 
ReCU model for explaining the curiosity-driven behavior.
Rewarded
Non-rewarded
Left
Right
(0.9, 0.5)(L, R) = (0.5, 0.9) (0.5, 0.1) (0.1, 0.5) (0.5, 0.9) (0.9, 0.5) (0.1, 0.5) (0.5, 0.1)
Left
P(L)
Right
Trials
0 100 200 300 400
b
a
Reward probability
Trials
1.0
0.5
0
Reward probability
Trials
Selection probability
Estimated reward probability (right)Estimated reward probability (left)
c d e
Right
Left
Estimation
Estimation
Ground
truth
Ground
truth
Confidence
(left)
Intensity of curiosityConfidence
(right)
Trials
f
g
h
0
1.00
0.75
0.50
0.25
0
1.00
0.75
0.50
0.25
0
1.00
0.75
0.50
0.25
0
100 200 300 400
Trials
0 100 200 300 400
Trials
0 100 200 300 400
Trials
0 100 200 300 400
Trials
0 100 200 300 400
Trials
0 100 200 300 400
24
18
12
6
0
24
18
12
6
0
1
0
‚Äì1
‚Äì2
‚Äì3
Fig. 5 | Estimation of the rat‚Äôs internal state by iFEP . a, A rat in the two-choice 
task with temporally switching reward probabilities. b, Rat‚Äôs behaviors from a 
public dataset at https://groups.oist.jp/ja/ncu/data ref. 5. Vertical lines indicate 
the selections of left (L) and right (R) options, respectively. The time series 
indicates the moving average of the selection probability of the left option with 
25 window width backward. c, The moving average of the selection probabilities 
for the left and right options. d,e, The rat behavior data-driven estimations of 
agent-recognized reward probabilities for the left (d) and right (e) options.  
f‚Äìh, The rat behavior-driven estimations of agent‚Äôs confidence about recognized 
reward probabilities for the left (f) and right (g) options, and the agent‚Äôs curiosity 
(h). The estimated parameter values were Œ± = 0.058, Œ≤ = 6.991 and œÉ2
w = 0.560. 
The number of particles was 100,000 in the particle filter. Continuous shaded 
ranges represent the s.d. for all the particles in d‚Äìh.
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 426
Article https://doi.org/10.1038/s43588-023-00439-w
Discussion
The advancement realized in this study is the modeling and decod -
ing of mental conflict between reward and curiosity, which is yet to 
be quantified. The proposed approach can potentially improve our 
understanding of how mental conflict is regulated and highlight its 
neural mechanisms in the future by combining neural recording data.
Our decoding approach has some limitations. The iFEP requires 
long trial behavioral data in the two-choice task because the particle 
filter needs certain trials for converging the estimation. Thus, the 
iFEP is not applicable to short behavioral data. In addition, the iFEP 
assumed gradual change in curiosity in time and cannot follow the 
pathologically rapid dynamics of curiosity in the estimation process 
of the particle filter.
Comparing the ReCU model and other models including Q-learn-
ing is difficult. In all these models, the action selection is commonly 
formulated with the sigmoidal function. Thus, any models can be made 
to fit the observed behaviors, i.e., increase likelihood, by adjusting 
the time-dependent meta-parameters. Therefore, model selection 
based solely on likelihood is not very helpful and determining whether 
animals use the ReCU model or other models to make decisions seems 
inherently challenging. However, a potential advantage of the ReCU 
model is its ability to capture the dynamics of curiosity and the inter-
play between curiosity and reward-based learning.
There is a related theoretical model, which differs from RL and FEP. 
Ortega and Braun formulated FEP, which describes irrational decision 
making39,40. Interestingly, their formulation was based on microscopic 
thermodynamics and the temperature parameters controlled this 
irrationality. However, the thermodynamics-based FEP did not treat 
the sequential update of the recognition from the observations.
Finally, it is worth discussing future perspectives of our iFEP 
approach in medicine. In general, mental diagnosis relies on medical 
interviews and has not been evaluated quantitatively. Our iFEP method 
can quantitatively estimate the psychological state of patients based on 
their behavioral data. For example, patients with social withdrawal, also 
known as ‚ÄòHikikomori, ‚Äô have no interest in anything. In this case, social 
withdrawal would be characterized by a negative value of curiosity in  
‚àÜ reward
‚Äì0.4
‚Äì0.4
‚Äì0.6
‚Äì0.2
‚Äì0.2
0
0
0.2
0.2
0.4
0.4
0.6
‚àÜ reward
‚Äì0.4 ‚Äì0.2 0 0.2 0.4 0.6
‚àÜ reward
‚Äì0.4 ‚Äì0.2 0 0.2 0.4 0.6
‚àÜ info gain
‚Äì0.4
‚Äì0.6
‚Äì0.2
0
0.2
0.4
‚àÜ info gain
‚Äì0.4
‚Äì0.6
‚Äì0.2
0
0.2
0.4
0.498
0.499
0.500
0.501
0.502
‚àÜ info gain
Trials Time lag
Correlation coeÔ¨Äicient
a
d
f g
e
b c
Right
Left
Logistic
regression
iFEP
c (intensity of curiosity)
Intensity of curiosity, cInfo gain
Info gain
Probability of selecting the right
Logistic
regression
iFEP
Logistic
regression
iFEP
Right
Left
Right Left
dc/dt (temporal derivative of curiosity)
Time derivative of curiosity, dc/dt
Info gain
0
0.4
0.6
0.8
1.0
1.2
1.4
‚Äì3.0
‚Äì2.2
‚Äì1.4
‚Äì0.6
0.2
1.0
Info gain
0.4
0.6
0.8
1.0
1.2
1.4
‚Äì0.08
‚Äì0.01
0.06
0.13
‚Äì0.15
0.20
100 200 300 400 ‚Äì60 ‚Äì40 ‚Äì20 0
0
0.3
‚Äì0.3
‚Äì0.6
0.6
0.9
Correlation coeÔ¨Äicient
0
0.3
‚Äì0.3
‚Äì0.6
0.6
0.9
20 40 60
Time lag
‚Äì60 ‚Äì40 ‚Äì20 0 20 40 60
Trials
0 50 100 150 200 250 300 350 400
Fig. 6 | Negative curiosity and its dynamics. a‚Äìc, The selected options in a space 
of left‚Äìright differences of the expected reward and information gain. All actions 
were separated into three: when the estimated curiosity is negatively large 
(
c ‚â§‚àí 1.1, n = 110) (a), negatively medium (‚àí1.1 < c ‚â§‚àí 0.7, n‚Äâ=‚Äâ93) (b) and close 
to 0 (‚àí0.7 < c, n = 187) (c). The left and right options were discriminated by a 
logistic regression with P(at = 1) = f(wRŒîE[Rewardt+1]+wIŒîE[Infot+1]), where 
wR and wI indicate the weight parameters. Two linear lines indicate discrimination 
boundaries using the estimated wR and wI from this scattered data and using 
wR = 1 and wI = ‚àëct/N, which is an average of the estimated curiosity, 
respectively. the heatmap represents the probability of selecting the left option 
based on the estimated w
R and wI. d,e, Time series of the intensity of curiosity and 
the sum of the expected information gains for both options (d), and their 
cross-correlation (e). f,g, Time series of the temporal derivative of curiosity and 
the sum of the expected information gains for both options (f), and their 
cross-correlation (g). The temporal derivative was computed by linear regression 
within a time window of seven trials.
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 427
Article https://doi.org/10.1038/s43588-023-00439-w
our FEP model. Therefore, the iFEP method could be considered  
effective for diagnosing mental disorders.
Methods
Amount of reward
In the two-choice task, the reward is given as all-or-none; however, its 
intensity depends on the agent‚Äôs own feeling as
R = 0orln
Po
1‚àíPo
, (9)
where Po represents the desired probability of how much the agent 
wants the reward and controls the reward intensity felt by the agent 
(Supplementary Fig. 8). In Friston‚Äôs formulation, the presence and 
absence of rewards are given by log preference with natural unit as ln 
Po and In(1‚Äâ‚àí‚ÄâPo), which take negative values22,23. In this equation, the 
reward is the difference between log preferences to ensure that the 
reward intensity is positive.
State space model for reward probability recognition
The agent assumed that the reward is generated probabilistically from 
the latent cause w, and probabilities Œªi are represented by
Œªi = f(wi), (10)
where i indicates the indices of the options and f(x) = 1/(1+e‚àíx).  
In addition, the agent assumed an ambiguous environment  
in which the reward probabilities are fluctuated by a random  
walk as
wi,t = wi,t‚àí1 +œÉwŒæi,t, (11)
where t, Œæi,t, and œÉw denote the trial of the two-choice task, standard 
Gaussian noise and the noise intensity, respectively. Thus, the agent 
assumes an environment expressed by an SSM as
P(wt|wt‚àí1) =ùí©ùí© (wt|wt‚àí1,œÉ2
wI), (12)
P(ot|wt,at) = ‚àè
i
[f(wi,t)
ot
{1‚àíf(wi,t)}
1‚àíot
]
ai,t
, (13)
where wt and o t denote the latent variables controlling the reward 
probabilities of both options at step t (wt = (w1,t,w2,t)
T
) and the observa-
tion of the presence of the reward (ot ‚àà {0,1}), respectively. Meanwhile, 
at denotes the agent‚Äôs action at step t, which is represented by a one-hot 
vector (at ‚àà {(1,0)
T
,(0,1)
T
})‚Äâ, ùí©ùí©(x|Œº,Œ£) denotes the Gaussian distribu-
tion mean Œº and variance Œ£, œÉ2
w denotes the variance of the transition 
probability of w, I denotes an identity matrix, and f(wi,t)= 1/(1+e‚àíwi,t ),  
which represents the probability of the reward of option i at step t. The 
initial distribution of w1 is given by P(w1) =ùí©ùí© (w1|0,Œ∫I), where Œ∫ denotes 
variance.
FEP for reward probability recognition
We modeled the agent‚Äôs recognition process of reward probability 
using sequential Bayesian updating as
P(wt|o1‚à∂t,a1‚à∂t) ‚àù P(ot|wt,at)‚à´P(wt|wt‚àí1)P(wt‚àí1|o1‚à∂t‚àí1,a1‚à∂t‚àí1)dwt‚àí1. (14)
Because of the non-Gaussian P(ot|wt, at), the posterior distribution of 
wt, P(wt|o1‚à∂t,a1‚à∂t), becomes non-Gaussian and cannot be calculated 
analytically. T o avoid this problem, we introduced a simple posterior 
distribution approximated by a Gaussian distribution:
Q(wt|œÜt) =ùí©ùí© (wt|Œºt,Œõ‚àí1
t )‚âíP(wt|o1‚à∂t,a1‚à∂t), (15)
where œÜt = {Œº t, Œõt}, and Œº t and Œõ t denote the mean and precision,  
respectively (Œºt = (Œº1,t,Œº2,t)
T
; Œõt = diag(p1,t,p2,t)). Q(wt|œÜt) denotes the 
recog nition distribution. The model agent aims to update the recog-
nition distribution through œÜt at each time step by minimizing  
the surprise, which is defined by ‚àílnP(ot|o1‚à∂t‚àí1). The surprise can be 
decomposed as follows:
‚àílnP(ot|o1‚à∂t‚àí1) = ‚à´Q(wt|œÜt)ln
Q(wt|œÜt)
P(ot,wt|o1‚à∂t‚àí1,a1‚à∂t)
dwt
‚àíKL[Q(wt|œÜt)||P(wt|o1‚à∂t,a1‚à∂t)],
(16)
where KL[q(x)||p(x)] denotes the Kullback‚ÄìLeibler (KL) divergence 
between the probability distributions q (x) and p(x). Because of the 
nonnegativity of KL divergence, the first term is the upper bound of 
the surprise:
F(ot,œÜt) = ‚à´Q(wt|œÜt)lnQ(wt|œÜt)dwt +‚à´Q(wttt|œÜt)J(ot,wt)dwt, (17)
which is called the free energy, where J(ot,wt) =‚àí lnP(ot,wt|o1‚à∂t‚àí1,a1‚à∂t).  
The first term of the free energy corresponds to the negative entropy 
of a Gaussian distribution:
F1 = ‚à´Q(wt|œÜt)lnQ(wttt|œÜt)dwt. (18)
The second term is approximated as
F2 = ‚à´Q(wttt|œÜt)J(ot,wt)dwt
‚âÖ ‚à´Q(wt|œÜt){J(ot,Œºt)+
dJ
dw
(wt ‚àíŒºt)+
1
2
d2J
dw2 (wt ‚àíŒºt)
2
}dwt
= J(ot,wt)|wt=Œºt
+
1
2
d2J
dw2
||wt=Œºt
Œõ‚àí1
t .
(19)
Note that E(ot,wt) is expanded by a second-order Taylor series  
around Œºt. At each time step, the agent updates œÜt by minimizing 
F(ot,œÜt).
Calculation of free energy
The free energy is derived as follows: F1 simply becomes
F1 =
1
2
ln2œÄp‚àí1
1,t +
1
2
ln2œÄp‚àí1
2,t +const. (20)
For computing F2,
P(ot,wt|o1‚à∂t‚àí1,a1‚à∂t) = P(ot|wt,at)‚à´P(wt|wt‚àí1)P(wt‚àí1|o1‚à∂t‚àí1,a1‚à∂t‚àí1)dwt‚àí1
‚âÖ P(ot|wt,at)‚à´P(wt|wt‚àí1)N(wt‚àí1|Œºt‚àí1,Œõ‚àí1
t‚àí1)dwt‚àí1.
(21)
In the second line of this equation, we use the approximated recogni-
tion distribution as the previous posterior P(wt‚àí1|o1‚à∂t‚àí1,a1‚à∂t‚àí1)‚Äâ. This 
equation can be written as
P(ot,wt|o1‚à∂t‚àí1,a1‚à∂t) ‚âÖ ‚àè
i
[f(wi,t)
ot
{1‚àíf(wi,t)}
1‚àíot
]
ai,t
N(wi,t|Œºi,t‚àí1,p‚àí1
i,t +œÉ2
w).
(22)
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 428
Article https://doi.org/10.1038/s43588-023-00439-w
Then
E(ot,wt)|wt=Œºt
= J1 (ot,Œº1,t)+J2 (ot,Œº2,t)+const., (23)
where
Ji (ot, Œºi,t) = ai,t [ot lnf(Œºi,t)+(1‚àíot)ln{1‚àíf(Œºi,t)}]
‚àí
1
2
(Œºi,t‚àíŒºi,t‚àí1)
2
p‚àí1
i,t +œÉ2
w
‚àí
1
2
ln(p‚àí1
i,t +œÉ2
w).
(24)
Thus, F2 is calculated by substituting this equation into equation (19). 
Taken together,
F(ot,œÜt) = ‚àë
i
{Ji (ot,Œºi,t)+
1
2
d2Ji
dw2
i,t
|||wi,t=Œºi,t
p‚àí1
i,t +
1
2
ln2œÄp‚àí1
i,t }. (25)
Sequential updating of the agent‚Äôs recognition
The updating rule for œÜ t was derived by minimizing the free  
energy. The optimized p i,t can be computed by ‚àÇF/‚àÇp‚àí1
i,t
= 0, which  
leads to
pi,t =
d2Ji
dw2
i,t
|||wi,t=Œºi,t
. (26)
By substituting pi,t into equation (25), the second term in the summation 
becomes constant, irrespective of Œºi,t. Thus, Œºi,t is updated by minimiz-
ing only the first term as
Œºi,t = Œºi,t‚àí1 ‚àíŒ±Œ¥i,a
‚àÇJi
‚àÇŒºi,t
|||Œºi,t=Œºi,t‚àí1
, (27)
where Œ± is the learning rate. These two equations finally lead to
Œºi,t = Œºi,t‚àí1 +Œ±Ki,t (ot ‚àíf(Œºi,t‚àí1)), (28)
pi,t = K‚àí1
i,t +f(Œºi,t)(1‚àíf(Œºi,t)), (29)
where Ki,t = (pi,t‚àí1 +œÉ‚àí2
w )/(pi,t‚àí1œÉ‚àí2
w
), which is called the Kalman gain. If 
option i was not selected, the second terms in both equations will van-
ish, which results in belief Œºi,t staying the same, while its precision 
decreases (that is, pi,t+1 < pi,t). If it is selected, the belief is updated by 
the prediction error (that is, ot ‚àíf(Œºi,t)), and its precision is improved. 
The confidence of the recognized reward probability should be evalu-
ated not in wi,t space but in Œªi,t space; hence, the confidence is defined 
by Œ≥i,t = pi,t/f‚Ä≤ (Œºi,t)
2
.
Expected net utility
The expected net utility is described by
Ut (at+1) = ct ‚ãÖEP(ot+1|at+1) [KL[Q(wt+1|ot+1,at+1)||Q(wt+1|at+1)]]
+EP(ot+1|at+1) [R(ot+1)],
(30)
where R(ot+1) = ot+1 ln(Po/(1‚àíPo))‚Äâ; the first and second terms represent 
the expected information gain and expected reward, respectively; and 
ct denotes the intensity of curiosity at time t . The heuristic idea of 
introducing the curiosity meta-parameter ct was also proposed in the 
RL field30.
We briefly show how to derive it based on ref. 41. The free energy 
at current time t is described by
F(ot,œÜt) = EQ(wt|œÜt) [lnQ(wt|œÜt)‚àílnP(ot,wt|o1‚à∂t‚àí1,a1‚à∂t)], (31)
which is a rewriting of equation (17). Here, we attempt to express the 
future free energy at time t‚Äâ+‚Äâ1, conditioned on action at +1 as
F(ot+1,at+1) = EQ(wt+1|œÜt) [lnQ(wt+1|œÜt)‚àílnP(ot+1,wt+1|o1‚à∂t,a1‚à∂t+1)]. (32)
However, there is an apparent problem in this equation: ot+1 has not yet 
been observed because it is a future observation. T o resolve this issue, 
we take an expectation with respect to ot+1 as
F(at+1) = EQ(ottt+1|wt+1,at+1)Q(wttt+1|œÜt)
[lnQ(wt+1|œÜt)‚àílnP(ot+1,wt+1|o1‚à∂t,a1‚à∂t+1)].
(33)
This can be rewritten as
F(at+1) = EQ(ot+1,wt+1|o1‚à∂t,at+1) [lnQ(wt+1|œÜt)
‚àílnP(wt+1|o1‚à∂t+1,a1‚à∂t+1)‚àílnP(ot+1|o1‚à∂t,a1‚à∂t+1)]
(34)
Although P(ot+1|o1‚à∂t,a1‚à∂t+1) can be computed by the generative model 
as
P(ot+1|o1‚à∂t,a1‚à∂t+1) = ‚à´P(ot+1|wt+1,a1‚à∂t+1)P(wt+1|o1‚à∂t)dwt+1
‚âÖ ‚à´P(ot+1|wt+1,a1‚à∂t+1)Q(wt+1|œÜt)dwt+1,
(35)
it is assumed that P(ot+1|o1‚à∂t,a1‚à∂t+1)‚Äâwas heuristically replaced with  
P(ot+1) as the prior agent‚Äôs preference of observing ot+1, so that  
lnP(ot+1) can be addressed as an instrumental reward. The equation (34)  
was further transformed into
F(at+1) = EQ(ot+1|at+1)Q(wt+1|o1‚à∂t+1,at+1) [lnQ(wt+1|œÜt)
‚àílnQ(wt+1|o1‚à∂t+1,a1‚à∂t+1)‚àílnP(ot+1)] .
(36)
Finally, we obtained the so-called expected free energy as
F(at+1) = EQ(ot+1|at+1) [‚àíKL[Q(wt+1|ot+1,at+1)||Q(wt+1)]‚àílnP(ot+1)]. (37)
where the KL divergence is called a Bayesian surprise, representing the 
extent to which the agent‚Äôs beliefs are updated by observation, whereas 
the second term lnP(ot+1) can be addressed as the agent‚Äôs prior prefer-
ence of observing ot+1, which can be interpreted as an instrumental 
reward. Therefore, the expected free energy is derived in an ad hoc 
manner.
In the first term of equation (30), the posterior and prior distribu-
tions of wt+1 in the KL divergence are derived as
Q(wt+1) = ‚à´P(wt+1|wt)Q(wt)dwt, (38)
Q(wt+1|ot+1,at+1) =
P(ot+1|wt+1,at+1)Q(wttt+1|at+1)
P(ot+1|at+1)
, (39)
respectively. ot+1 is a future observation, and therefore, the first term 
was expected by P(ot+1|at+1), which can be calculated as
P(ot+1|at+1) = ‚à´P(ot+1|wt+1,at+1)Q(wt+1|at+1)dwt+1. (40)
In the second term, the reward is quantitatively interpreted as the 
desired probability of ot+1. For the two-choice task, we use
P(ot+1) = Pot+1
o (1‚àíPo)
(1‚àíot+1)
, (41)
where Po indicates the desired probability of the presence of a  
reward. According to the probabilistic interpretation of reward 22,23, 
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 429
Article https://doi.org/10.1038/s43588-023-00439-w
the presence and absence of a reward can be evaluated by ln P o and 
ln(1‚Äâ‚àí‚ÄâPo), respectively.
In this study, we changed the sign of the expected free energy as 
the expected net utility, because we modeled decision-making as the 
maximization of the expected free energy. We ventured to introduce 
the curiosity meta-parameter ct to express irrational decision-making. 
Because rewards are relative, in this study, we set ln{Po/(1‚àíPo)} and 0 
for the presence and absence of a reward, respectively. Thus, our 
expected net utility can be described by equation (30), in which there 
is a gap between the original expected free energy and our expected 
net utility.
Model for action selection
The agent probabilistically selects a choice with higher expected net 
utility as
P(at+1) =
exp(Œ≤U(at+1))
‚àëa exp(Œ≤U(a))
, (42)
where U(at+1) indicates the expected net utility of action a t+1.  
Equation (42) is equivalent to equation (2). T o derive equation (42), we 
considered the expectation of the expected net utility with respect to 
probabilistic action as
E[U] = EQ(at+1) [U(at+1)‚àíŒ≤‚àí1 lnQ(at+1)], (43)
where Œ≤ indicates an inverse temperature, and the entropic constraint 
of action probability is introduced in the second term. This equation 
can be rewritten as
E[U] =‚àí Œ≤‚àí1KL[Q(at+1)‚Äñexp(Œ≤U(at+1))/Z]+Œ≤‚àí1 lnZ, (44)
where Z indicates a normalization constant. Thus, its maximization 
respective to Q(at+1) leads to the optimal action probability as shown 
in equation (42).
Alternative expected net utility
For comparison, we consider an alternative expected net utility by intro-
ducing a time-dependent meta-parameter in the second term as follows:
U(at+1) = EP(ot+1|at+1) [KL[Q(wt+1|ot+1,at+1)||Q(wt+1|at+1)]]
+dt ‚ãÖEP(ot+1|at+1) [R(ot+1)],
(45)
where dt denotes the subjective intensity of reward at time t. In this case, 
the agent with high dt will show more exploitative behavior, whereas 
the agent with dt = 0 shows more explorative behavior driven by the 
expected information gain.
Calculation of expected net utility
Here, we present the calculation of the expected net utility. The KL 
divergence in the first term of equation (30) can be transformed into
EP(ot+1|at+1) [KL[Q(wt+1|ot+1,at+1)||Q(wt+1|at+1)]] = H(ot+1)‚àíH(ot+1|wt+1),
(46)
where the first and second terms represent the conditional and mar-
ginal entropies, respectively:
H(ot+1|wt+1) = EP(ot+1|wt+1,at+1)Q(wt+1|aaat+1) [‚àílnP(ot+1|wt+1,at+1)], (47)
H(ot+1) = EP(ot+1|at+1) [‚àílnP(ot+1|at+1)]. (48)
The conditional entropy H(ot+1|wt+1) can be calculated by substitut-
ing equation (13) into equation (47) as
H(ot+1|wt+1) =‚àí EQ(wt+1|at+1) [‚àëi ai,t+1g(wi,t+1)], (49)
where
g(w) = f(w)lnf(w)+(1‚àíf(w))ln(1‚àíf(w)). (50)
Here, we approximately calculate this equation by using the sec-
ond-order Taylor expansion as
H(ot+1|wt+1) ‚âÖ‚àí EQ(wt+1|at+1)
‚é°‚é¢‚é¢‚é¢
‚é£
‚àë
i
ai,t+1
‚éß‚é™
‚é®‚é™‚é©
g(Œºi,t+1)+
‚àÇg
‚àÇwi,t+1
(wi,t+1 ‚àíŒºi,t+1)
+
1
2
‚àÇ2g
‚àÇw2
i,t+1
(wi,t+1 ‚àíŒºi,t+1)
2
‚é´‚é™
‚é¨‚é™‚é≠
‚é§‚é•‚é•‚é•
‚é¶
,
(51)
which leads to
H(ot+1|wt+1) =
‚àí‚àë
i
ai,t+1
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
f(Œºi,t+1)lnf(Œºi,t+1)+(1‚àíf(Œºi,t+1))ln(1‚àíf(Œºi,t+1))
+
1
2
{f(Œºi,t+1)(1‚àíf(Œºi,t+1))(1+(1‚àí2f(Œºi,t+1))ln
f(Œºi,t+1)
1‚àíf(Œºi,t+1)
)}
(p‚àí1
i,t +p‚àí1
w )
‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
.
(52)
The marginal entropy H(ot+1) can be calculated as
H(ot+1) =‚àí ‚àë
i
ai,t+1 {P(ot+1 = 0|at+1)lnP(ot+1 = 0|at+1),
+P(ot+1 = 1|at+1)lnP(ot+1 = 1|at+1)}
(53)
where
P(ot+1|at+1) = ‚à´P(ot+1|wt+1,at+1)Q(wt+1|at+1)dwt+1
= ‚à´‚àè
i
{f(wi,t+1)
ot+1
(1‚àíf(wi,t+1))
1‚àíot+1
}
ai,t+1
Q(wt+1|at+1)dwt+1
= ‚àè
i
[
f(Œºi,t+1)
ot+1
(1‚àíf(Œºi,t+1))
1‚àíot+1
+1ot+1 (‚àí1)
1‚àíot+1 1
2
f(Œºi,t+1){1‚àíf(Œºi,t+1)}{1‚àí2f(Œºi,t+1)}(p‚àí1
i,t +p‚àí1
w )
]
ai,t+1
.
(54)
The second term of the expected net utility (equation ( 36)) is 
calculated as
EP(ot+1|at+1) [lnP(ot+1)] = EP(ot+1|at+1) [ot+1lnPo +(1‚àíot+1)ln(1‚àíPo)]
= P(ot+1 = 0|at+1)ln(1‚àíPo)+P(ot+1 = 1|at+1)ln(1‚àíPo).
(55)
Observer-SSM
We constructed the observer-SSM, which describes the temporal 
transitions of the latent internal state z of agent and the generation 
of action, from the viewpoint of the observer of the agent. This is 
depicted graphically in Fig. 4. As prior information, we assumed that 
the agent acts based on the internal state, that is, the intensity of 
curiosity, the recognized reward probabilities and their confidence 
levels. The intensity of curiosity was assumed to change temporally 
as a random walk:
ct = ct‚àí1 +œµŒ∂t, (56)
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 430
Article https://doi.org/10.1038/s43588-023-00439-w
where Œ∂t denotes white noise with zero mean and unit variance, and œµ 
denotes its noise intensity. Other internal states, that is, Œºi and pi, were 
assumed to update as equations ( 28) and (29). The transition of the 
internal state is expressed by the probability distribution
P(zt|zt‚àí1) =ùí©ùí© (zt|F(zt‚àí1,at‚àí1),Œì), (57)
F(zt‚àí1,at‚àí1) =
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
0
h(Œº1,t‚àí1,p1,t‚àí1,ot,a1)
h(Œº2,t‚àí1,p2,t‚àí1,ot,a2)
k(Œº1,t‚àí1,p1,t‚àí1,a1)
k(Œº2,t‚àí1,p2,t‚àí1,a2)
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, (58)
where zt = (ct,ŒºT
t ,pT
t )
T
 and Œì = œµ2diag(1,0,0,0,0)‚Äâ. h(Œºi,t‚àí1,pi,t‚àí1,ot,ai)  
and k(Œºi,t‚àí1,pi,t‚àí1,ai) represent the right-hand sides of equations ( 28) 
and (29), respectively; and Œì and diag(x) denote the variance‚Äìcovari-
ance matrix and square matrix whose diagonal component is x, respec-
tively. In addition, the agent was assumed to select an action at+1  
based on the expected net utilities, as follows:
P(at+1) =
exp(Œ≤U(at+1))
‚àëa exp(Œ≤U(a))
, (59)
and the reward was obtained by the following probability distribution:
P(ot|at) = ‚àè
i
{Œªot
i,t(1‚àíŒªi,t)1‚àíot }
ai,t
. (60)
Q-leaning in two-choice task and its observer-SSM
The decision-making in the two-choice task was also modeled by 
Q-learning. Reward prediction for the ith option Qi,t is updated as
Qi,t = Qi,t‚àí1 +Œ±t‚àí1 (rtai,t‚àí1 ‚àíQi,t‚àí1), (61)
where Œ±t indicates a learning rate at trail t. The agents selected action 
following a softmax function:
P(ai,t = 1) =
exp(BtQi,t)
‚àëi exp(BtQi,t)
, (62)
where Bt indicates the inverse temperature at trail t  controlling the 
randomness of the action selection.
The time-dependent parameters Œ±t and B t in Q-learning were  
estimated from behavioral data 37. These parameters were assumed  
to change temporally as a random walk:
Œ∏t = Œ∏t‚àí1 +œµŒ∏Œ∂Œ∏,t, (63)
where Œ∏ ‚àà {Œ±,B}, Œ∂Œ∏,t denotes white noise with zero mean and unit  
variance and œµŒ∏ denotes its noise intensity. Thus, the transition of  
the internal state is expressed by the probability distribution
P(zt|zt‚àí1) =ùí©ùí© (zt|F(zt‚àí1,at‚àí1),Œì), (64)
F(zt‚àí1,at‚àí1) =
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
0
0
h1 (Œ±t,Q1,t‚àí1,at‚àí1)
h2 (Œ±t,Q2,t‚àí1,at‚àí1)
‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
, (65)
where zt = (Œ±t,Bt,Q1,t,Q2,t)
T
, Œì = œµ2diag(1,1,0,0), hi (Œ±t,Qi,t‚àí1,at) repre -
sents the right-hand side of equation (61); and Œì and diag(x) denote the 
variance‚Äìcovariance matrix and square matrix whose diagonal com-
ponent is x, respectively.
iFEP by particle filter and Kalman backward algorithm
Based on the observer-SSM, we estimated the posterior distribution 
of the latent internal state of agent zt given all observations from 1 to 
T (x1‚à∂T) in a Bayesian manner, that is, P(zt|x1‚à∂T). This estimation was done 
by forward and backward algorithms, which are called filtering and 
smoothing, respectively.
In filtering, the posterior distribution of zt given observations until 
t ( x1‚à∂t) is sequentially updated in a forward direction as
P(zt|x1‚à∂t) ‚àù P(xt|zt,Œ∏)‚à´P(zt|zt‚àí1,Œ∏)P(zt‚àí1|x1‚à∂t‚àí1)dzt‚àí1, (66)
where xt = (aT
t ,ot)
T
 and Œ∏ = {œÉ2,Œ±,Po}. The prior distribution of z1 is
P(z1) = [‚àè
i
ùí©ùí©(Œºi,1|Œº0,œÉ2
Œº)Gam(pi,1|ag,bg)]Uni(c1|au,bu), (67)
where Œº0 and œÉ2
Œº denote means and variances, Gam(x|ag,bg) indicates 
the Gamma distribution with shape parameter ag and scale parameter  
bg, and Uni(x|au,bu) indicates uniform distribution from au to bu. We 
used a particle filter 42 to sequentially calculate the posterior  
P(zt|x1‚à∂t), which cannot be analytically derived because of the nonlinear 
transition probability.
After the particle filter, the posterior distribution of zt given all 
observations (x1‚à∂T) is sequentially updated in a backward direction as
P(zt|x1‚à∂T) = ‚à´P(zt+1|x1‚à∂T)P(zt|zt+1,x1‚à∂t,Œ∏)dzt+1
= ‚à´P(zt+1|x1‚à∂T)
P(zt+1|zt,Œ∏)P(zt|x1‚à∂t,Œ∏)
‚à´
P
(zt+1|zt,Œ∏)P(zt|x1‚à∂t,Œ∏)dzt
dzt+1.
(68)
However, this backward integration is intractable because of  
the non-Gaussian P(zT|x1‚à∂T), which was represented by the particle 
ensemble in the particle filter, and the nonlinear relationship between 
zt and zt+1 in P(zt+1|zt,Œ∏) (equation (57 )). Thus, we approximated 
P(zt|x1‚à∂t) as ùí©ùí©(zt|mt,Vt), where mt and Vt denote a sample mean and  
a sample variance of the particles at t , whereas we linearized  
P(zt|zt‚àí1,Œ∏) as
P(zt|zt‚àí1,Œ∏) ‚âÖùí©ùí© (zt|Azt‚àí1 +b,Œì), (69)
A =
‚àÇF(zt‚àí1,at‚àí1)
‚àÇzt‚àí1
|||mt
, (70)
b = F(mt,at‚àí1)‚àíAmt, (71)
where A denotes a Jacobian matrix. Because these approximations 
make the integration of equation (68) tractable, the posterior distribu-
tion P(zt|x1‚à∂T) can be computed by a Gaussian distribution as
P(zt|x1‚à∂T) =ùí©ùí© (zt|ÀÜmt,ÀÜVt) (72)
whose mean and variance were analytically updated by Kalman back-
ward algorithms as43
ÀÜmt = mt +Jt {ÀÜmt+1 ‚àí(Amt +b)}, (73)
ÀÜVt = Vt +Jt {ÀÜmt+1 ‚àí(AVtAT +Œì)}JT
t , (74)
where
Jt = VtAT (AVtAT +Œì)
‚àí1
. (75)
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432
 431
Article https://doi.org/10.1038/s43588-023-00439-w
Impossibility of model discrimination
In the ReCU and Q-learning models, the action selections were formu-
lated with the same softmax functions as
P(ai,t = 1) =
exp(Œ≤(E[Rewardi,t]+ctE[Infoi,t]))
‚àëj exp(Œ≤(E[Rewardj,t]+ctE[Infoj,t]))
. (76)
P(ai,t = 1) =
exp(Œ≤tQi,t)
‚àëi exp(Œ≤tQi,t)
, (77)
which correspond to that of the ReCU and Q-learning models, respec-
tively. These equations contain the time-dependent meta-parameters, 
that is, ct and Œ≤t. For both models, the goodness of fit (that is, likelihood) 
for the actual behavioral data can be freely improved by tuning the 
time-dependent meta-parameters. Thus, discrimination between the 
ReCU and Q-learning models must be essentially impossible.
Estimation of parameters in iFEP
The ReCU model has several parameters: œÉ2
w, Œ±, Œ≤, Po and œµ. In the estima-
tion, we set œµ to 1, which was the optimal value for estimation in the 
artificial data (Supplementary Fig. 3). We assumed the unit intensity 
of reward, that is, lnPo/(1‚àíPo)= 1, because it is impossible to estimate 
both Po and Œ≤ caused by multiplying Œ≤ and lnPo/(1‚àíPo) in the expected 
net utility (equations (30) and (42)). This treatment is suitable for rela-
tive comparison between the curiosity meta-parameter and the reward. 
In addition, we addressed Œ≤ct as a latent variable as ÃÇct = Œ≤ct because of 
the multiplication of Œ≤  in the expected net utility (equations ( 30)  
and (42)). Thus, the estimation of c t can be obtained by dividing the 
estimated ÃÇct by the estimated Œ≤. Therefore, the hyperparameters to be 
estimated were œÉ2
w, Œ± and Œ≤.
T o estimate these parameters Œ∏ = {œÉ2
w,Œ±,Œ≤}, we extended the 
observer-SSM to a self-organizing SSM44 in which Œ∏ was addressed as 
constant latent variables:
P(zt,Œ∏|x1‚à∂t) ‚àù P(xt|zt)‚à´P(zt|zt‚àí1,Œ∏)P(zt‚àí1,Œ∏|x1‚à∂t‚àí1)dzt‚àí1, (78)
where P(Œ∏) = Uni(œÉ2|aœÉ,bœÉ)Uni(Œ±|aŒ±,bŒ±)ùí©ùí©(Œ≤|mŒ≤,vŒ≤). T o sequentially cal-
culate the posterior P(zt,Œ∏|x1‚à∂t) using the particle filter, we used 
100,000 particles and augmented the state vector of all particles by 
adding the parameter Œ∏, which was not updated from randomly sam-
pled initial values.
The hyperparameter values used in this estimation were Œº 0 = 0, 
œÉ2
Œº = 0.012, ag = 10, bg = 0.001, au = ‚àí15, bu = 15, aœÉ = 0.2, bœÉ = 0.7, aŒ± = 0.04, 
bŒ± = 0.06, aŒ≤ = 0 and bŒ≤ = 50, which were heuristically given as parameters 
correctly estimated using the artificial data (Supplementary Fig. 2).
Statistical testing with Monte Carlo simulations
Supplementary Fig. 5 shows statistical testing of the negative curiosity 
estimated in Fig. 5. A null hypothesis is that an agent has no curiosity 
(that is, ct‚Äâ=‚Äâ0) decides on a choice only depending on its recognition of 
the reward probability. Under the null hypothesis, model simulations 
were repeated 1,000 times under the same experimental conditions as 
in Fig. 5 and the curiosity was estimated for each using iFEP. We adopted 
the temporal average of the estimated curiosity as a test statistic and 
plotted the null distribution of the test statistic. Compared with the 
estimated curiosity of the rat behavior, we computed the P value for a 
one-sided left-tailed test.
Reporting summary
Further information on research design is available in the Nature Port-
folio Reporting Summary linked to this article.
Data availability
Source data for Figs. 2, 3, 5 and 6 are available with this paper. Source 
data for Supplementary Figures are available in Supplementary Data. 
We used the rat behavioral data published in ref. 36, which is publicly 
available at https://groups.oist.jp/ja/ncu/data. These rat behavioral 
data are also included in the Source Data for Fig. 5 and on Zenodo45.
Code availability
The computer simulation and data analysis were performed using  
MATLAB (version R2020b) software. The code used for this work 
is available on GitHub at https://github.com/YukiKonaka/Konaka_
Honda_2023. The specific version used to produce the results in this 
manuscript is also available on Zenodo45.
References
1. Helmholtz, H. Handbuch der Physiologischen Optik (Andesite 
Press, 1867).
2. Yuille, A. & Kersten, D. Vision as Bayesian inference: analysis by 
synthesis? Trends Cogn. Sci. 10, 301‚Äì308 (2006).
3. Millett, J. D. & Simon, H. A. Administrative behavior: a study of 
decision-making processes in administrative organization.  
Polit. Sci. Q. 62, 621 (1947).
4. Dubey, R. & Griffiths, T. L. Understanding exploration in humans 
and machines by formalizing the function of curiosity. Curr. Opin. 
Behav. Sci. 35, 118‚Äì124 (2020).
5. Kidd, C. & Hayden, B. Y. The psychology and neuroscience of 
curiosity. Neuron 88, 449‚Äì460 (2015).
6. Klein, U. & Nowak, A. J. Characteristics of patients with autistic 
disorder (AD) presenting for dental treatment: a survey and chart 
review. Spec. Care Dentist. 19, 200‚Äì207 (1999).
7. Lockner, D. W., Crowe, T. K. & Skipper, B. J. Dietary intake and 
parents‚Äô perception of mealtime behaviors in preschool-
age children with autism spectrum disorder and in typically 
developing children. J. Am. Diet. Assoc. 108, 1360‚Äì1363 (2008).
8. Schreck, K. A. & Williams, K. Food preferences and factors 
influencing food selectivity for children with autism spectrum 
disorders. Res. Dev. Disabil. 27, 353‚Äì363 (2006).
9. Esposito, M. et al. Sensory processing, gastrointestinal symptoms 
and parental feeding practices in the explanation of food 
selectivity: clustering children with and without autism.  
Int. J. Autism Relat. Disabil. 2, 1‚Äì12 (2019).
10. Hobson, R. P. Autism and the development of mind. Essays Dev. 
Psychol. (Routledge, 1993).
11. Burke, R. Personalized recommendation of PoIs to people with 
autism. Commun. ACM 65, 100 (2022).
12. Ghanizadeh, A. Educating and counseling of parents of children 
with attention-deficit hyperactivity disorder. Patient Educ. Couns. 
68, 23‚Äì28 (2007).
13. Sedgwick, J. A., Merwood, A. & Asherson, P. The positive 
aspects of attention deficit hyperactivity disorder: a qualitative 
investigation of successful adults with ADHD. ADHD Atten. Deficit 
Hyperact. Disord. 11, 241‚Äì253 (2019).
14. Redshaw, R. & McCormack, L. ‚ÄòBeing ADHD‚Äô: a qualitative study. 
Adv. Neurodev. Disord. 6, 20‚Äì28 (2022).
15. Sutton, R. S. & Barto, A. G. Reinforcement Learning: an Introduction 
(MIT Press, 1998).
16. Friston, K. A theory of cortical responses. Philos. Trans. R. Soc. B 
360, 815‚Äì836 (2005).
17. Friston, K., Kilner, J. & Harrison, L. A free energy principle for the 
brain. J. Physiol. Paris 100, 70‚Äì87 (2006).
18. Friston, K. The free-energy principle: a unified brain theory?  
Nat. Rev. Neurosci. 11, 127‚Äì138 (2010).
19. Lindley, D. V. On a measure of the information provided by an 
experiment. Ann. Math. Stat. 27, 986‚Äì1005 (1956).
20. MacKay, D. J. C. Information-based objective functions for active 
data selection. Neural Comput. 4, 590‚Äì604 (1992).
21. Berger, J. O. Statistical Decision Theory and Bayesian Analysis, 
Springer Series in Statistics (Springer, 2011).
Nature Computational Science | Volume 3 | May 2023 | 418‚Äì432 432
Article https://doi.org/10.1038/s43588-023-00439-w
22. Friston, K. et al. Active inference and epistemic value. Cogn. 
Neurosci. 6, 187‚Äì214 (2015).
23. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P. & Pezzulo, G.  
Active inference: a process theory. Neural Comput. 29, 1‚Äì49 (2017).
24. Attias, H. Planning by probabilistic inference in Proc. 9th Int. Work. 
Artif. Intell. Stat. 4, 9‚Äì16 (2003).
25. Botvinick, M. & Toussaint, M. Planning as inference. Trends Cogn. 
Sci. 16, 485‚Äì488 (2012).
26. Kaplan, R. & Friston, K. J. Planning and navigation as active 
inference. Biol. Cybern. 112, 323‚Äì343 (2018).
27. Matsumoto, T. & Tani, J. Goal-directed planning for habituated 
agents by active inference using a variational recurrent neural 
network. Entropy 22, (2020).
28. Schwartenbeck, P. et al. Computational mechanisms of curiosity 
and goal-directed exploration. eLife 8, 1‚Äì45 (2019).
29. Millidge, B., Tschantz, A. & Buckley, C. L. Whence the expected 
free energy? Neural Comput. 33, 447‚Äì482 (2021).
30. Houthooft, R. et al. VIME: variational information maximizing 
exploration. Adv. Neural Inf. Process. Syst. 0, 1117‚Äì1125 (2016).
31. Smith, R. et al. Greater decision uncertainty characterizes a 
transdiagnostic patient sample during approach-avoidance 
conflict: a computational modelling approach. J. Psychiatry 
Neurosci. 46, E74‚ÄìE87 (2021).
32. Smith, R. et al. Long-term stability of computational parameters 
during approach-avoidance conflict in a transdiagnostic 
psychiatric patient sample. Sci Rep. 11, 1‚Äì13 (2021).
33. Schwartenbeck, P. & Friston, K. Computational phenotyping in 
psychiatry: a worked example. eNeuro 3, 1‚Äì18 (2016).
34. Daunizeau, J. et al. Observing the observer (I): meta-Bayesian models 
of learning and decision-making. PLoS ONE 5, e15554 (2010).
35. Patzelt, E. H., Hartley, C. A. & Gershman, S. J. Computational 
phenotyping: using models to understand individual differences 
in personality, development, and mental illness. Personal. 
Neurosci. 1, e18 (2018).
36. Ito, M. & Doya, K. Validation of decision-making models and 
analysis of decision variables in the rat basal ganglia. J. Neurosci. 
29, 9861‚Äì9874 (2009).
37. Samejima, K., Doya, K., Ueda, Y. & Kimura, M. Estimating internal 
variables and parameters of a learning agent by a particle filter. 
Adv. Neural Inf. Process. Syst. 16 (2003).
38. Samejima, K., Ueda, Y., Doya, K. & Kimura, M. Neuroscience: 
representation of action-specific reward values in the striatum. 
Science (80-.) 310, 1337‚Äì1340 (2005).
39. Ortega, P. A. & Braun, D. A. Thermodynamics as a theory of 
decision-making with information-processing costs. Proc. R. Soc. 
London. A 469, 20120683 (2013).
40. Gottwald, S. & Braun, D. A. The two kinds of free energy and the 
Bayesian revolution. PLoS Comput. Biol. 16, (2020).
41. Parr, T. & Friston, K. J. Generalised free energy and active 
inference. Biol. Cybern. 113, 495‚Äì513 (2019).
42. Kitagawa, G. Monte Carlo filter and smoother for non-Gaussian 
nonlinear state space models. J. Comput. Graph. Stat. 5, 1‚Äì25 (1996).
43. Bishop, C. M. Pattern Recognition and Machine Learning  
(Springer, 2006).
44. Kitagawa, G. A self-organizing state-space model. J. Am. Stat. 93, 
1203‚Äì1215 (1998).
45. Konaka, Y. & Naoki, H. Codes for Konaka and Honda 2023. Zenodo 
https://doi.org/10.5281/zenodo.7722905 (2023)
Acknowledgements
We are grateful to K. Doya and M. Ito for providing rat behavioral 
data. We thank the organizers of the tutorial on the free energy 
principle in 2019, which inspired this research, and I. Higashino  
and M. Fujiwara-Yada for carefully checking all the equations in  
the manuscript. This study was supported in part by a Grant-in-Aid 
for Transformative Research Areas (B) (no. 21H05170), AMED  
(grant no. JP21wm0425010), Moonshot R&D‚ÄìMILLENNIA program  
(grant no. JPMJMS2024-9) by JST, the Cooperative Study  
Program of Exploratory Research Center on Life and Living Systems 
(ExCELLS) (program no. 21-102) and the grant of Joint Research 
by the National Institutes of Natural Sciences (NINS program no. 
01112102).
Author contributions
H.N. conceived of the project. Y. K. and H.N. developed the method, 
and Y.K. implemented the model simulation. Y.K. and H.N. wrote  
the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version  
contains supplementary material available at  
https://doi.org/10.1038/s43588-023-00439-w.
Correspondence and requests for materials should be addressed to 
Honda Naoki.
Peer review information Nature Computational Science thanks 
Junichiro Yoshimoto and Karl Friston for their contribution to the  
peer review of this work. Primary Handling Editors: Ananya Rastogi  
and Jie Pan, in collaboration with the Nature Computational Science 
team.
Reprints and permissions information is available at  
www.nature.com/reprints.
Publisher‚Äôs note Springer Nature remains neutral with  
regard to jurisdictional claims in published maps and  
institutional affiliations.
Open Access This article is licensed under a Creative Commons 
Attribution 4.0 International License, which permits use, sharing, 
adaptation, distribution and reproduction in any medium or format, 
as long as you give appropriate credit to the original author(s) and the 
source, provide a link to the Creative Commons license, and indicate 
if changes were made. The images or other third party material in this 
article are included in the article‚Äôs Creative Commons license, unless 
indicated otherwise in a credit line to the material. If material is not 
included in the article‚Äôs Creative Commons license and your intended 
use is not permitted by statutory regulation or exceeds the permitted 
use, you will need to obtain permission directly from the copyright 
holder. To view a copy of this license, visit http://creativecommons.
org/licenses/by/4.0/.
¬© The Author(s) 2023