Perspective
Dimensionality and dynamics
for next-generation
artiﬁcial neural networks
Ge Wang1,* and Feng-Lei Fan2,*
1Department of Biomedical Engineering, Department of Electrical, Computer, and Systems Engineering, Department of Computer Science,
Center for Computational Innovations, Biomedical Imaging Center, Center for Biotechnology and Interdisciplinary Studies, Rensselaer
Polytechnic Institute, Troy, NY, USA
2Department of Data Science, City University of Hong Kong, Kowloon, Hong Kong
*Correspondence: wangg6@rpi.edu (G.W.), fenglfan@cityu.edu.hk (F.-L.F.)
https://doi.org/10.1016/j.patter.2025.101231
SUMMARY
The recent awarding of the Nobel Prize in Physics to Geoffrey E. Hinton and John J. Hopﬁeld highlights
their profound impact on artiﬁcial neural networks. In this perspective, we explore how their founda-
tional insights can drive the advancement of next-generation artiﬁcial intelligence (AI) models. We pro-
pose expanding beyond conventional architectures by introducing dimensionality through intra-layer
links and dynamics via feedback loops. Network height and additional dimensions, alongside traditional
width and depth, enhance learning capabilities, while entangled loops across scales induce emergent
behaviors akin to phase transitions in physics. We discuss how these principles extend beyond trans-
formers, fostering a new paradigm of intelligence inspired by physics-driven models and biological
cognition mechanisms.
Since 2006, deep learning—pioneered by Hinton and others
through innovations like the restricted Boltzmann machine—
has ﬂourished. 1 We propose further generalization by intro-
ducing network height and even higher, more abstract
dimensions through various links, such as Kolmogorov-Arnold
Network (KAN)-type lines.
2,3 Hopﬁeld’s 1982 network model
for human associative memory, 4 which employs loops for dy-
namic evolution toward ﬁxed points, serves as another founda-
tion. More recently, physics-inspired models, including state-
space models like Mamba, 5 have leveraged noising-denoising,
forward-backward, and compression-recovery loops to create
foundational models for Bayesian inference, particularly via
conditioned sampling for downstream tasks. These novel con-
structions of links and loops across space and time are poised
to drive the next generation of artiﬁcial neural networks, shaping
the future of artiﬁcial intelligence (AI).
The seminal work on the Transformer
6 has come to dominate
in research and applications of deep learning with phenomenal
successes of foundation models 7,8 such as ChatGPT, GPT-4o,
and o1.9 Recently, it has been widely discussed how to surpass
the current Transformer-based architecture toward next-gener-
ation AI models, especially foundation models. Although the
Transformer, as the standard framework, cannot currently be re-
placed by non-Transformer architectures, the limits of Trans-
formers are already well known. Transformers’ complexity
does not fully support scaling law. In the foreseeable future,
the most advanced GPUs will be based on 1 nm fabrication, ap-
proaching the atomic scale and meeting a physical ceiling to
accommodate much larger models. Also, Transformers strongly
rely on a large amounts of data, making them less effective in
many tasks without troublesome ﬁne-tuning. Therefore, sur-
passing the Transformer architecture represents one of the
THE BIGGER PICTUREWith Geoffrey E. Hinton and John J. Hopﬁeld having been awarded the Nobel Prize in
Physics, it is a good time to reﬂect on how to sustain the ongoing momentum in artiﬁcial neural network
research. Achieving artiﬁcial general intelligence remains the holy grail, presenting numerous challenges
that require novel insights to overcome. We underline that Hinton and Hopﬁeld’s work has not only made his-
torical contributions but will continue to have a profound impact on the development of artiﬁcial neural net-
works. From our perspective, their groundbreaking research suggests that new foundational architectures
can be strengthened by incorporating links and loops within networks, aligning with the growing interest
in surpassing Transformer architectures to enable next-generation foundational models that offer unprece-
dented feature representations and sophisticated emergent behaviors.
ll
OPEN ACCESS
Patterns 6, August 8, 2025 ª 2025 The Author(s). Published by Elsevier Inc. 1
This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/).
most promising directions. The recent works of this type include
Mamba,4 KAN-type lines, 2,3 the receptance weighted key
value,10 and Mega, 11 just to name a few.
The Nobel Prize in Physics was recently awarded to
Geoffrey E. Hinton and John J. Hopﬁeld ‘‘for foundational dis-
coveries and inventions that enable machine learning with
artiﬁcial neural networks.’’ We underline that their work is
not just important in the history of machine learning but will
also have a far-reaching impact in the future, particularly rele-
vant to the development of novel foundational architectures.
By drawing insights from their works credited by the 2024
Nobel Prize in Physics, we believe that the novel use of links
and loops in space and time would be the key to the future
of the AI ﬁeld at large, as illustrated in Figure 1
5 to provide
our conjectures into the future of deep learning and catalyze
next-generation neural networks. Speciﬁcally, links represent
dimensionality enhancement to enrich feature representations,
and loops induce sophisticated dynamics, such as phase
transitions, that enable the emergence of new AI model
capabilities.
We emphasize that our perspective, which focuses on
dimensionality and dynamics,
5 differs signiﬁcantly from other
well-known perspectives, such as the fusion of rule-based
and data-driven methods, 12 embodied AI, 13 quantum deep
learning,14 and autonomous machine intelligence. 15 Rule-based
systems rely on explicit rules, while data-driven approaches
leverage data to derive patterns and trends. Their fusion prom-
ises a balance between human expertise and machine intelli-
gence. Embodied AI emphasizes interactions with the physical
world to perceive the environment and take action. Quantum
deep learning may revolutionize the computing paradigm, but
its prime time is yet to come despite exciting technical develop-
ments over the past few years. In autonomous machine intelli-
gence, Yann LeCun champions the concept of a world model,
arguing that existing large models should enhance their capabil-
ities in the perception-planning-action cycle. In contrast to these
excellent and other perspectives on the future of deep learning,
our perspective is unique and at the fundamental level of links
and loops to form motifs, constructs, and architectures for supe-
rior intelligent performance.
INTRODUCING HEIGHT AND MORE DIMENSIONS
The formulation of backpropagation by Shun’ichi Amari
16 paved
the way for training multilayer neural networks. However,
Geoffrey Hinton’s and others’ contributions to deep learning
are also foundational and transformative, profoundly shaping
the trajectory of AI research and its applications. These
pioneers’ dedication to deep learning enabled learning intricate
representations at scale and with ﬂexibility, generating huge
impacts on real-world applications. Their revolutionary insights
initiated a dimensional outreach into the depth direction. In
retrospect, further enriching feature representations through
higher-dimensional augmentations promises to yield even
greater advancements.
Indeed, naively deepening models may not be beneﬁcial.
17 In
terms of expressivity, the width and depth of an artiﬁcial neural
network are basically equivalent in principle, meaning that a
wide network can be functionally transformed into a deep
network, and vice versa.
18 Practically, deepening a network
often yields better performance than widening it, but a decent
performance of a deep network requires a speciﬁc requirement
of layer width. It has been proven that to approximate a function
from R
m to Rn, the minimum width required is m + n.19,20 The
interplay between width and depth is intricate; for example, in
Transformers, widening becomes necessary when deepening.
If the width and depth are not balanced well, then increasing
the depth would be inefﬁcient or defective.
17 It was demon-
strated in Goyal et al. 21 that a wider network with only 12 layers
achieves performance comparable to that of a deep network
with 30 layers. Throughput-wise, while a deeper network implies
more sequential computations and higher latency, a wider
network allows for easier parallelization.
The human brain operates as a three-dimensional (3D)
network of adaptively interconnected neurons.
22 This intricate
network allows for complex processing of information and
AB
Figure 1. Future of artiﬁcial neural networks
A 3D neural network with (A) width, depth, height, and potentially more dimensions to enhance deep learning spatially and (B) dynamics through feedba ck
mechanisms to enhance deep learning temporally. A network empowered with links in higher dimensions and loops of more complicated topology would
facilitate more advanced intelligent behaviors, such as phase transitions.
ll
OPEN ACCESS
2 Patterns 6, August 8, 2025
Perspective
seamless integration of various cognitive functions. Given the
complexity of human intelligence, it seems plausible that artiﬁcial
networks should adopt a 3D structure to emulate the brain’s ef-
ﬁciency and versatility. First, the brain’s volumetric nature en-
ables it to process and store vast amounts of information in a
highly efﬁcient manner. Neurons are densely packed in three di-
mensions, forming synaptic connections that facilitate rapid
communication and parallel processing. Second, the brain’s or-
ganization is resilient and fault tolerant due to its redundant, en-
tangled, and distributed nature, enabling it to withstand damage
to individual components without catastrophic failure. Third, the
three-dimensionality of the brain fosters the emergence of
cognitive functions. The interplay between different regions
of the brain, facilitated by its 3D connectivity, underpins our ca-
pacity for creativity, abstract thinking, and nuanced decision-
making.
In addition to the width and depth of a deep artiﬁcial neural
network, the third dimension of a network is height, which is
largely overlooked in literature. By transitioning to 3D network
models, we aspire to develop artiﬁcial systems that induce
higher-order cognitive functions akin to those observed in the
human brain. The height can be introduced through intra-layer
links, as pointed out in recent studies.
23,24 Shortcuts, which
bypass layers, have worked well. Different from the commonly
used shortcuts that connect layers, intra-layer links incorporate
shortcuts within a layer, as shown in Figure 1 A. The concept of
height is a natural extension beyond the width and depth of neu-
ral networks. Simply wrapping a linear array of neurons into a 2D
array does not change the fact that these neurons can be easily
straightened out. On the other hand, intra-layer links implement
neural interconnections within a layer so that neurons in the
layer cannot be linearly unrolled, making the height direction
non-trivial.
Introducing height is different from increasing depth in the
following three senses. First, it increases neither the number of
afﬁne transformations nor the number of nonlinear activations,
while increasing either width or depth does. Second, the mech-
anism behind intra-layer links involves breaking symmetry and
reinforcing the mutual information among neurons within the
same layer,
24 thus reducing the hypothesis space of relevance.
In Zhang and Zhou, 24 intra-layer links are central (referred to as
self-connections) in proving that spiking networks with these
intra-layer links exhibit improved approximation capabilities
and computational efﬁciency, i.e., modeling discrete dynamical
systems with a polynomial number of parameters and a polyno-
mial time complexity, although the concept of network height is
not explicitly elaborated. Consequently, a narrower network with
intra-layer links can possess the same capability as an exponen-
tially wider network. For example, while the extended long short-
term memory (xLSTM) replaced the scalar with a matrix to deﬁne
cell states,
25 the long short-term memory (LSTM) can be equip-
ped with the height dimension to increase its memory capacity.
Furthermore, like ‘‘depth separation,’’ we should consider
‘‘height separation.’’ This emphasizes the importance of height
for network design, suggesting that a tall network can only be ex-
pressed by a short network with very large width and depth. It is
also important to note that the width, depth, and height of a
network could be ﬂexibly converted for universal approximation,
meaning that tall, wide, and deep networks can be transformed
from one type to another. Coupling the width, depth, and height
of a network empowers its performance without increasing the
number of network parameters much. Therefore, instead of
blindly increasing depth, we advocate for optimal balance
among the width, depth, and height of a network to maximize
its representation power and computational efﬁciency.
Since we exist in a 3D world, we can design neural networks
beyond 2D conﬁgurations. We should seek a ‘‘dimensionality
blessing’’ and avoid a ‘‘dimensionality curse.’’ For instance, in a
4D space, there are countless differential structures on
4-manifolds, whereas this is not the case in higher-dimensional
spaces. Solving the Poincare ´ conjecture becomes notably
simpler in spaces exceeding three dimensions, leading to a faster
resolution of the high-dimensional Poincare ´ conjecture than the
3D counterpart. In mean-ﬁeld theory, individual components,
such as spins, of a system are assumed to interact not directly
with their speciﬁc neighbors but with some averaged effect of
the system. This approach replaces the complexity of local inter-
actions with a mean ﬁeld that encapsulates the behaviors of
surrounding elements. As dimensionality increases, random ﬂuc-
tuations are mitigated through the averaging effect, which tends
to cancel them out. As a result, the more interactions are aggre-
gated, the smaller the impact of individual variations. Conse-
quently, mean-ﬁeld theory becomes more accurate when the
dimensionality goes higher.
26 Physicists are actively pursuing a
theory that uniﬁes standard models and general relativity, and it
seems such a theory demands an 11D space. Unfortunately, in
the machine learning ﬁeld, the advantages of dimensionality
often receive less attention than its drawbacks. In fact, increasing
dimensionality has proven to be an effective strategy for
improving feature separability, which is widely utilized in kernel
methods. Anderson et al.
27 demonstrate that a mixture of m
Gaussians with equal known variance can be efﬁciently learned
if m is lower bounded by the dimension d, implying that a sufﬁ-
ciently large dimension d is necessary to induce a favorable
learning behavior. The crux of these ﬁndings lies in tensor decom-
position. Distributing features across various axes can enhance
the ﬂexibility and decomposability of feature representation,
thereby boosting learning effectiveness and efﬁciency.
The height dimension can be viewed or adapted as a depen-
dency relationship between two networks: one is the main
network and the other is the hyper-network interacting with the
main network. In biology, the genotype encodes the phenotype,
as seen from the fact that a human genome consisting of a small
number of genes encodes the development of a human brain
with a vast number of connections.
28 Inspired by this idea, the
hyper-network (genotypes) can be developed to design,
construct, and optimize the main network (phenotypes). In other
words, we can focus on relationships in the spirit of category the-
ory.
29 According to Yoneda’s Lemma 29 in category theory, two
categories, X and Y, are isomorphic if and only if their corre-
sponding functors are isomorphic, where functors are struc-
ture-preserving maps between categories. Following this princi-
ple, we treat objects as indivisible atoms without an internal
structure, meaning that each object is characterized by its rela-
tions to other objects. In this view, the entire system can be
described by its associated functor.
Despite the merits of height or higher dimensions in general,
incorporating them does not come at zero cost. The augmented
ll
OPEN ACCESS
Patterns 6, August 8, 2025 3
Perspective
hierarchy of higher dimensions may generally entail increased
computational complexity and latency, leading to longer training
and inference time. On the other hand, there may exist a potential
"sweet spot" where the advantages of dimensional augmenta-
tion can render resultant networks more powerful and more efﬁ-
cient in certain tasks.
DYNAMICS THROUGH FEEDBACK MECHANISMS
The Hopﬁeld network is characterized by feedback loops, with
the dynamics of interconnected neurons forming associative
memory. As physics-inspired models, Hopﬁeld networks employ
an energy landscape to encode stored patterns at stable states
and have the capacity to recall patterns even from partial infor-
mation, like the human brain. This feedback dynamics can be
likened to phase transitions, where the system evolves toward
stable states, analogous to cognitive clarity in the brain. One of
the most intriguing aspects of the brain’s function is its ability
to undergo phase transitions, a phenomenon that has been
observed in a wide range of physical systems, from water turning
into ice to magnetization processes. Phase transitions occur
through a sudden characteristic change of a sufﬁciently complex
system with respect to a control variable, such as its tempera-
ture, pressure, or others. Such changes are often associated
with the emergence of new collective behaviors that are not pre-
sent in the system’s individual components. In the brain, phase
transitions are thought to play a crucial role in the emergence
of cognitive functions, such as learning, memory, and deci-
sion-making. Friston’s free energy principle in the cognitive sci-
ence ﬁeld
30 assumes that any self-organizing agent must
actively reduce disorder or uncertainty in the changing environ-
ment and constantly optimizes itself to include a built-in world
model so that a ‘‘surprise’’ measure can be minimized that re-
lates to the overall accuracy of the agent’s prediction or the ex-
pected outcome of the agent’s action. By this principle, intelli-
gence emerges when the state of a self-organizing agent
migrates from disorder to order, with its behavior seemingly pur-
poseful and comprehensive.
We believe that an advanced intelligent neural network should
also undergo phase transitions, which allow the mimicry of some
essential brain functions. For example, just like the cognition sta-
tus is transformed from confusion to clarity, a neural network
should have a phase transition from a state of uncertainty to a
state of conﬁdence. Such phase transitions suggest that neural
networks have learned in a way similar to how the brain works.
To harness the full potential of connectionism and engender
phase transition behaviors, as shown in Figure 1B, we should still
use feedforward links and, at the same time, embrace feedback
loops. The Hopﬁeld network and its variants
31 rely on a feedback
mechanism that fundamentally differs from a feedforward
network and produces intriguing outcomes, such as associative
memory. We should extend the Hopﬁeld network to combine the
feedforward and feedback mechanisms in innovative ways.
Such a combination of links and loops would bring signiﬁcant ad-
vantages. While the Hopﬁeld network is featured by simple
loops, the recently emerging diffusion-/ﬂow-based and other
physics-inspired generative models
32 use multi-step loops. For
example, the diffusion model allows for an original data distribu-
tion to be gradually noised into a featureless random ﬁeld, and
then incremental noise components can be step-by-step
removed from the random ﬁeld to sample the original data distri-
bution (the feedback loop closed by the forward and reverse pro-
cesses), which is also shown in Figure 1 B. The whole noising-
denoising loop can be done in either many (generic probability
ﬂow solutions) or single (consistency models) steps, leading to
powerful dynamics with the solution existence, uniqueness,
and stability established by the stochastic differential equation
theory. An increasing number of studies report that the diffu-
sion-/ﬂow-based models set the state-of-the-art performance
of generative AI, outperforming the famous generative adversa-
rial network (GAN) and variational autoencoder (VAE) networks.
In addition, while chain-of-thought
33 focuses on linearity,
tree-of-thought34 emphasizes branching and hierarchy, and
graph-of-thought35 showcases connectivity and relationships,
feedback loops enable the ‘‘loop-of-thought’’ process, a cyclical
process where ideas are revisited and reﬁned iteratively, allow-
ing for ongoing introspection and retrospection, which can be
seen as an advanced form of cognitive processing, allowing for
a more dynamic exploration of ideas. As the latest example,
the recently emerging structural state-space sequence models,
such as Mamba,
4 compress history to predict the future, and the
relevant information is then selectively recovered in the future.
The whole compression-recovering loop leads to dynamics for
long-range dependence modeling.
From this point of view, a network that incorporates links in
multiple dimensions and loops of complicated topology would
behave more like our brain, giving rise to critical behaviors that
have not yet been a focus of today’s AI research. Thus, future
research should capitalize on sophisticated phase transitions,
including Ising-type and topological phase transitions.
36 Inspired
by the Ising model, consisting of spin-up and spin-down polar-
ities, in our recent study we ﬂipped the gradient directions of
neurons during network training to enhance robustness. 37 Along
this direction, topological phase transitions in physics should
have counterparts in artiﬁcial neural networks. Topology de-
scribes properties of objects that remain unchanged under
stretching or twisting but not tearing. The topological phase tran-
sition differs from a conventional phase transition. Therein, small
vortices play a key role within a 2D or higher-dimensional space.
At low temperatures, vortex/antivortex pairs form. As the tem-
perature increases, such pairs disappear. Graphically, loops
can be viewed as ‘‘holes’’ of a network, which can be regarded
as a topological defect. The movement of vortices can be ex-
pressed as the transformation between links and loops. Through
this transformation, a variety of topological phases can be man-
ifested, and the rate of such transformation is analogous to tem-
perature.
The proposed integration of intra-links and topological loops is
fundamentally different from widely known baselines such as
graph neural networks (GNNs
38) or other advanced architec-
tures. GNNs focus on learning structural relationships over graph
data through message passing across edges. In contrast, our
approach suggests the embedding of Hopﬁeld-like dynamics
directly into the architecture, introducing mechanisms for asso-
ciative memory, energy minimization, state convergence, and
phase transition within layers, sub-networks, or the entire
network. Indeed, GNNs are primarily designed to propagate in-
formation between nodes along edges in a graph and generate
ll
OPEN ACCESS
4 Patterns 6, August 8, 2025
Perspective
node embeddings and edge predictions without inducing and
utilizing neither internal dynamics nor memory. Again, our focus
is on enhancing and empowering contemporary deep learning
architectures with recurrent intra-layer/network interactions
inspired by Hopﬁeld’s work. These proposed mechanisms are
not possessed by GNNs, which are designed for static graph-
based learning tasks.
The enhancement of a neural network in both dimensionality
and dynamics also facilitates AI for science. Demis Hassabis
and John Jumper’s AlphaFold
39 has exempliﬁed the immense
potential of AI for scientiﬁc discovery. However, a signiﬁcant
challenge remains; for example, how AI can deepen our under-
standing of highly complex nonlinear systems. In this context,
reservoir computing models,
40 such as echo-state networks 41
and liquid-state networks, 42 provide a promising framework for
predicting nonlinear dynamic behaviors, including chaotic pro-
cesses and extreme events. Reservoir computing takes advan-
tage of the "blessing of dimensionality" by mapping input signals
into high-dimensional feature spaces through a randomly initial-
ized, ﬁxed, highly nonlinear, and dynamic system. Then, a rela-
tively simple yet trainable mechanism makes predictions based
on the latent time-varying features extracted via these links and
associated loops. We further posit that neural networks can
serve as effective models for deciphering neural processes in
the human brain, thereby enhancing our understanding of how
neurons and neural circuits process information. In this context,
the proposed 3D network, characterized by phase transitions,
may become instrumental in elucidating biological intelligent be-
haviors, such as the critical dynamics of the brain, which is intrin-
sically 3D. This could also be crucial for the diagnosis, prognosis,
and treatment of various neurological disorders.
Given the rapid momentum of AI model evolution, our above
analysis suggests transcending the territory of the current
network architectures from spatial and temporal angles along
the directions revealed by Nobel Prize laureates Hopﬁeld and
Hinton, as well as other leading AI researchers. This perspective
represents a convergence of classic and contemporary sciences
with a central focus on the development of next-generation arti-
ﬁcial neural networks. We welcome further brainstorming and
collaboration.
DECLARATION OF INTERESTS
The authors declare no competing interests.
REFERENCES
1. Hinton, G.E., and Salakhutdinov, R.R. (2006). Reducing the dimensionality
of data with neural networks. Science 313, 504–507. https://doi.org/10.
1126/science.1127647.
2. Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja /C20ci/C19c, M., Hou,
T., and Tegmark, M. (2024). Kan: Kolmogorov-arnold networks. Preprint
at: arXiv. https://doi.org/10.48550/arXiv.2404.19756
3. Li, C., Liu, X., Li, W., Wang, C., Liu, H., Liu, Y., Chen, Z., and Yuan, Y.
(2024). U-kan makes strong backbone for medical image segmentation
and generation. Preprint at: arXiv. https://doi.org/10.48550/arXiv.
2406.02918
4. Hopﬁeld, J.J. (1982). Neural networks and physical systems with emer-
gent collective computational abilities. Proc. Natl. Acad. Sci. USA 79,
2554–2558. https://doi.org/10.1073/PNAS.79.8.2554.
5. Gu, A., and Dao, T. (2023). Mamba: Linear-time sequence modeling with
selective state spaces. Preprint at: arXiv. https://doi.org/10.48550/arxiv.
2312.00752
6. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
A.N., Kaiser, q., and Polosukhin, I. (2017). Attention is all you need. Adv.
Neural Inf. Process. Syst. 30, 5998-6008. https://doi.org/10.5555/
3721488.3721525.
7. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer,
J., Steiner, A.P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. (2023).
Scaling vision transformers to 22 billion parameters. In International Con-
ference on Machine Learning, pp. 7480–7512. https://doi.org/10.5555/
3618408.3618704.
8. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozie` re, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open
and efﬁcient foundation language models. Preprint at: arXiv. https://doi.
org/10.48550/arxiv.2302.13971
9. Jones, N. (2024). ‘In awe’: scientists impressed by latest ChatGPT model
o1. Nature 634, 275–276. https://doi.org/10.1038/d41586-024-03169-9.
10. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman,
S., Cao, H., Cheng, X., Chung, M., Grella, M., et al. (2023). RWKV: Rein-
venting RNNs for the Transformer Era. In In Findings of the Association
for Computational Linguistics: EMNLP, pp. 14048–14077. https://
aclanthology.org/2023.ﬁndings-emnlp.936/.
11. Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, G., and Zettle-
moyer, L. (2022). Mega: Moving Average Equipped Gated Attention. In In
The Eleventh International Conference on Learning Representations
https://openreview.net/forum?id=qNLe3iq2El.
12. Mendel, J.M. (1995). Fuzzy logic systems for engineering: a tutorial. Proc.
IEEE 83, 345–377. https://ieeexplore.ieee.org/document/364485/.
13. Chrisley, R. (2003). Embodied artiﬁcial intelligence. Artif. Intell. 149,
131–150. https://doi.org/10.1016/S0004-3702(03)00055-9.
14. Cerezo, M., Verdon, G., Huang, H.Y., Cincio, L., and Coles, P.J. (2022).
Challenges and opportunities in quantum machine learning. Nat. Comput.
Sci. 2, 567–576. https://www.nature.com/articles/s43588-022-00311-3.
15. LeCun, Y. (2022). A path towards autonomous machine intelligence
version 0.9. 2, 2022-06-27. Open Review 62, 1–62. https://doi.org/10.
48550/arXiv.2306.02572.
16. Amari, S. (1967). A theory of adaptive pattern classiﬁers. IEEE Trans. Elec-
tron. Comput. EC-16, 299–307. https://doi.org/10.1109/PGEC.1967.
264666.
17. Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A. (2020). Limits to
depth efﬁciencies of self-attention. Adv. Neural Inf. Process. Syst. 33,
22640–22651. https://doi.org/10.5555/3495724.3497622.
18. Fan, F., Lai, R., and Wang, G. (2023). Quasi-Equivalence between Width
and Depth of Neural Networks. J. Mach. Learn. Res. 24, 1–22. https://
doi.org/10.5555/3648699.3648882.
19. Park, S., Yun, C., Lee, J., and Shin, J. (2020). Minimum Width for Universal
Approximation. In International Conference on Learning Representations
https://openreview.net/forum?id=O-XJwyoIF-k.
20. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. (2020). The expressive power
of neural networks: A view from the width. Adv. Neural Inf. Process. Syst.
30, 6232–6240. https://doi.org/10.5555/3295222.3295371.
21. Goyal, A., Bochkovskiy, A., Deng, J., and Koltun, V. (2022). Non-deep net-
works. Adv. Neural Inf. Process. Syst. 35, 6789–6801. https://doi.org/10.
5555/3600270.3600762.
22. Gao, L., Liu, S., Wang, Y., Wu, Q., Gou, L., and Yan, J. (2023). Single-
neuron analysis of dendrites and axons reveals the network organization
in mouse prefrontal cortex. Nat. Neurosci. 26, 1111–1126. https://doi.
org/10.1038/s41593-023-01339-y.
23. Fan, F.L., Li, Z.Y., Xiong, H., and Zeng, T. (2023). Rethink Depth Separation
with Intra-layer Links. Preprint at: arXiv. https://doi.org/10.48550/arXiv.
2305.07037
ll
OPEN ACCESS
Patterns 6, August 8, 2025 5
Perspective
24. Zhang, S.Q., and Zhou, Z.H. (2022). Theoretically provable spiking neural
networks. Adv. Neural Inf. Process. Syst. 35, 19345–19356. https://doi.
org/10.5555/3600270.3601676.
25. Beck, M., Po ¨ ppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M.,
Klambauer, G., Brandstetter, J., and Hochreiter, S. (2024). xLSTM:
Extended Long Short-Term Memory. Preprint at arXiv. https://doi.org/
10.48550/arXiv.2405.04517.
26. Kadanoff, L.P. (2009). More is the same; phase transitions and mean ﬁeld
theories. J. Stat. Phys. 137, 777–797. https://doi.org/10.1007/s10955-
009-9814-1.
27. Anderson, J., Belkin, M., Goyal, N., Rademacher, L., and Voss, J. (2014).
The more, the merrier: the blessing of dimensionality for learning large
Gaussian mixtures. In Conference on Learning Theory, pp. 1135–1164.
https://proceedings.mlr.press/v35/anderson14.
28. Stanley, K.O., D’Ambrosio, D.B., and Gauci, J. (2009). A hypercube-based
encoding for evolving large-scale neural networks. Artif. Life 15, 185–212.
https://doi.org/10.1162/artl.2009.15.2.15202.
29. Leinster, T. (2014). Basic Category Theory (Cambridge Univer-
sity Press).
30. Friston, K. (2010). The free-energy principle: a uniﬁed brain theory? (2010).
Nat. Rev. Neurosci. 11, 127–138. https://doi.org/10.1038/nrn2787.
31. Krotov, D. (2023). A new frontier for hopﬁeld networks. Nat. Rev. Phys. 5,
366–367. https://doi.org/10.1038/s42254-023-00595-y.
32. Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic
models. Adv. Neural Inf. Process. Syst. 33, 6840–6851. https://doi.org/
10.5555/3495724.3496298.
33. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
and Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large
language models. Adv. Neural Inf. Process. Syst. 35, 24824–24837.
https://doi.org/10.5555/3600270.3602070.
34. Yao, S., Yu, D., Zhao, J., Shafran, I., Grifﬁths, T., Cao, Y., and Narasimhan,
K. (2024). Tree of thoughts: Deliberate problem solving with large language
models. Adv. Neural Inf. Process. Syst. 36, 11809–11822. https://doi.org/
10.5555/3666122.3666639.
35. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gia-
ninazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., and
Hoeﬂer, T. (2024). Graph of thoughts: Solving elaborate problems with
large language models. Proc. AAAI Conf. Artif. Intell. 38, 17682–17690.
https://doi.org/10.1609/aaai.v38i16.29720.
36. Kosterlitz, J.M. (2017). Nobel lecture: Topological defects and phase tran-
sitions. Rev. Mod. Phys. 89, 040501. https://doi.org/10.1103/RevMod-
Phys.89.040501.
37. Liang, Y., Niu, C., Yan, P., and Wang, G. (2024). Flipover outperforms
dropout in deep learning. Vis. Comput. Ind. Biomed. Art 7, 4–9. https://
doi.org/10.1186/s42492-024-00153-y.
38. Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and
Sun, M. (2020). Graph neural networks: A review of methods and applica-
tions. AI open 1, 57–81. https://doi.org/10.1016/J.AIOPEN.2021.01.001.
39. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O.,
Tunyasuvunakool, K., Bates, R., /C20
Zı´dek, A., Potapenko, A., et al. (2021).
Highly accurate protein structure prediction with AlphaFold. Nature 596,
583–589. https://doi.org/10.1038/s41586-021-03819-2.
40. Tanaka, G., Yamane, T., He ´ roux, J.B., Nakane, R., Kanazawa, N., Takeda,
S., Numata, H., Nakano, D., and Hirose, A. (2019). Recent advances in
physical reservoir computing: A review. Neural Netw. 115, 100–123.
https://doi.org/10.1016/j.neunet.2019.03.005.
41. Jaeger, H., and Haas, H. (2004). Harnessing nonlinearity: Predicting
chaotic systems and saving energy in wireless communication. Science
304, 78–80. https://doi.org/10.1126/SCIENCE.1091277.
42. Zhang, Y., Li, P., Jin, Y., and Choe, Y. (2015). A digital liquid state machine
with biologically inspired learning and its application to speech recogni-
tion. IEEE Transact. Neural Networks Learn. Syst. 26, 2635–2649.
https://doi.org/10.1109/TNNLS.2015.2388544.
Dr. Ge Wang (Fellow of IEEE, SPIE, AAPM, OSA, AIMBE, AAAS, and NAI) is the
Clark & Crossan Chair Professor and Director of Biomedical Imaging Center at
Rensselaer Polytechnic Institute (Troy, New York, USA). He pioneered the spi-
ral cone-beam CT method in 1991 and published the ﬁrst perspective on deep
learning-based tomographic imaging in 2016. His honors include the IEEE
EMBS Career Achievement Award, SPIE Meinel Technology Award, Sigma
Xi Chubb Award for Innovation, RPI Wiley Distinguished Faculty Award, IEEE
R1 Outstanding Teaching Award, and IEEE NPSS/NMISC Hoffman Medical
Imaging Scientist Award. He is the Editor-in-Chief Trans. Medical Imaging.
Dr. Fenglei Fan is an Assistant Professor with the Department of Data Science,
City University of Hong Kong. His interests are in NeuroAI and its applications
in model compression and medical imaging. He was the recipient of the IBM AI
Horizon Scholarship. His PhD dissertation was recognized by the 2021 Inter-
national Neural Network Society Doctoral Dissertation Award. His paper was
selected as one of ten 2024 CVPR Best Paper Award Candidates. He also
won the IEEE Nuclear and Plasma Society IEEE TRPMS Best Paper Award
in 2024.
ll
OPEN ACCESS
6 Patterns 6, August 8, 2025
Perspective