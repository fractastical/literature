royalsocietypublishing.org/journal/rsfs
Research
Cite this article: Kim CS. 2023 Free energy
and inference in living systems.Interface Focus
13: 20220041.
https://doi.org/10.1098/rsfs.2022.0041
Received: 23 June 2022
Accepted: 18 January 2023
One contribution of 15 to a theme issue
‘Making and breaking symmetries in mind and
life’.
Subject Areas:
systems biology
Keywords:
living system, homeostasis and allostasis,
Bayesian brain, free-energy principle,
Schrödinger’s machine, neural attractor
Author for correspondence:
Chang Sub Kim
e-mail: cskim@jnu.ac.kr
Free energy and inference in living
systems
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186, Republic of Korea
CSK, 0000-0002-1549-9305
Organisms are non-equilibrium, stationary systems self-organized via spon-
taneous symmetry breaking and undergoing metabolic cycles with broken
detailed balance in the environment. The thermodynamic free-energy (FE)
principle describes an organism’s homeostasis as the regulation of biochemi-
cal work constrained by the physical FE cost. By contrast, recent research in
neuroscience and theoretical biology explains a higher organism’s homeosta-
sis and allostasis as Bayesian inference facilitated by the informational FE. As
an integrated approach to living systems, this study presents an FE minimiz-
ation theory overarching the essential features of both the thermodynamic
and neuroscientific FE principles. Our results reveal that the perception
and action of animals result from active inference entailed by FE minimiz-
ation in the brain, and the brain operates as a Schrödinger ’s machine
conducting the neural mechanics of minimizing sensory uncertainty. A par-
simonious model suggests that the Bayesian brain develops the optimal
trajectories in neural manifolds and induces a dynamic bifurcation between
neural attractors in the process of active inference.
1. Introduction
Although there is no standard definition of life [1– 7], the literature often states
that a living system tends to reduce its entropy, defying the second law of ther-
modynamics to sustain its non-equilibrium (NEQ) existence. However,
conforming to the second law of thermodynamics, adjudication between the
entropy reduction and augmentation of an open system must depend on the
direction of irreversible heat flux at the system– reservoir interface. Organisms
are open systems in the environment; hence, they obey the second law by con-
tributing to the total-entropy increase in the universe. The above confusion,
perhaps, is rooted in Erwin Schrödinger’s annotation, which metaphorically
explains living organisms as feeding on negative entropy [8]. In the same mono-
graph, Schrödinger continues to explicate that a more appropriate discussion
for metabolism is to be addressed in terms of free energy (FE). He made this
clarification because, in contrast to the Clausius entropy to which he was refer-
ring, thermodynamic FE always decreases during irreversible processes in any
open system [9]. Many studies have been based on Schrödinger’s insight into
how biological systems can be explained by physical laws and principles. We
examine the definition of life in terms of FE minimization.
Organisms maintain biologically essential properties, such as body temp-
erature, blood pressure and glucose levels, which are distinct from ambient
states. Living systems continuously exchange heat and material fluxes with
the environment by performing metabolic work, which is subject to the
energy balance described by the first law of thermodynamics. The second
law posits that the entropy of an isolated macroscopic system increases mono-
tonically with any spontaneous changes. Organisms and the environment
together constitute the biosphere, which is isolated and macroscopic; thus,
metabolic processes in organisms increase thetotal entropy. The second law
affects organisms by limiting metabolic efficiency. The thermodynamic free-
© 2023 The Authors. Published by the Royal Society under the terms of the Creative Commons Attribution
License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original
author and source are credited.
energy principle (TFEP) encompasses thermodynamic laws
and provides qualitative and quantitative explanation of
how living systems biophysically sustain homeostasis by
minimizing FEs. Recent studies have addressed the modern
metabolism perspective as energy regulation of multisensory
integration across both interoceptive and exteroceptive pro-
cesses [10,11]. This explains metabolism not only at the
level of individual organisms, but also at the ecosystem and
planetary levels [12,13], and emphasizes the energetics and
power efficiency in brain performance [14– 16]. By contrast,
the ability of organisms to undergo allostasis, which predic-
tively regulates homeostasis [17,18], or, more generally , their
autopoietic properties [19], are unable to be explained by
the TFEP. Allostatic ability is the main driver of adaptive fit-
ness, the emergence of which cannot be solely attributed to a
(bio)physical self-organization from a myriad of emergent
possibilities in the primitive circuits of neuronal activities.
Organisms are under environmental constraints, and adap-
tive fitness, or natural selection, is the consequence of
survival optimization in specific environments during evol-
ution. Therefore, the FE minimization scheme requires a
top-down or high-level computational mechanism that
facilitates hardwiring of the allostatic capability.
The brain-inspired FE theory in neuroscience and theor-
etical biology suggests a universal biological principle in an
axiomatic manner, the free-energy principle (FEP). The FEP
provides the informational FE-minimization formalism, that
accounts for the perception, learning and behaviour of
living systems in the environment [20,21]. This principle
has been also applied to other cognitive systems, such as arti-
ficial intelligence and robots [22 – 28]; however, our study
primarily focuses on living systems and implications of the
FEP in a biological context, emphasizing the embodied
nature of inference [29]. According to the informational FEP
(IFEP), all life forms are evolutionarily self-organized to mini-
mize surprisal, which is an information-theoretic measure of
the improbability or unexpectedness of the environmental
niche of organisms. Informational FE (IFE) is a theoretical
construct, rather than a physical (thermodynamic) quantity
specified by the temperature, chemical potential, volume,
etc. Informational FE mathematically bounds the surprisal
from above; accordingly , the IFEP suggests that natural selec-
tion reflects minimization of IFE in an organism as a proxy
for surprisal at all biological time scales. The IFEP employs
Helmholtz’s early idea of perception as unconscious infer-
ence [30]: an organism’s brain possesses an internal model
of sensory generation and infers the external causes of sen-
sory data by matching them with prior knowledge. The
active-inference framework following from the IFEP encapsu-
lates motor control and planning beyond Helmholtzian
perception as an additional inferential scheme [31,32]. The
brain possesses the probabilistic internal model whose par-
ameters (sufficient statistics) are encoded by brain variables
in the NEQ stationary state; however, thus far, no physical
theory has been developed for determining NEQ probabil-
ities in the macroscopic brain. In practice, the IFEP assumes
open forms, or some fixed forms, for the NEQ densities
and implements IFE minimization. The Gaussian fixed-form
assumption can be used to convert the IFE to a sum of discre-
pancies between the predicted and actual signals [33], which
is known asprediction error in predictive coding theory [34].
Commonly, the transformed IFE objective is minimized by
employing the gradient-descent method widely used in
machine learning [35]. The resulting variational-filtering
equations compute the Bayesian inversion of sensory data
by inferring the external sources [36], known as recognition
dynamics (RD) [20]. Recently, the IFEP was generalized in a
manner that minimizessensory uncertainty, which is a long-
term surprisal over a temporal horizon of an organism’s
changing environmental niche [37]. Despite being a promising
universal biological principle, the IFEP has led to controversy
regarding its success as the universal principle and its distance
between biophysical reality and epistemological grounds
[38– 47].
In this study, the two FE approaches are jointly con-
sidered to develop a unified paradigm for living systems:
the TFEP does not describe the brain’s ability to infer and
act in the environment, whereas the brain-inspired IFEP is
mainly a purposive (hypothesis-driven) framework lacking
intimate connections to neuronal substrates and physical
laws. Our goal is to link the two FEPs and propose a biologi-
cal FEP that integrates the reductionistic base and top-down
teleology in the brain. In addition, we unveil the attractor
dynamics that computes allostatic regulation, perception
and motor inference, in the brain, based on our proposed
FE-minimization framework. A similar approach was
reported in [48], in which formalisms underwriting stochastic
thermodynamics and the IFEP were presented without
addressing the direct link between the thermodynamic and
informational FE. In addition, a unified Bayesian and thermo-
dynamic view attempted to explain the brain’s learning and
recognition as a neural engine and proposed the laws of neu-
rodynamics [49]. We also note another recent work that made
the neural manifold models from a symmetry-breaking
mechanism in brain-network synergetics, commensurate
with the maximum information principle [50].
In brain architecture, enormous degrees of freedom of
neuronal activities pose the classical negligence in a high-
dimensional problem; thus, the underlying neural dynamics
appears to be stochastic. However, we argue that perception,
learning and motor-inference in the brain is low-dimensional
at the functional level, obeying the law of large numbers;
accordingly , RD becomes deterministic, involving a limited
number of latent variables. For instance, a few joint angles
suffice for the brain to infer arm movement in motor control.
In contrast, the emergence of deterministic RD is more subtle
in perception and learning, which demands a systematic
coarse-graining of stochastic neuronal dynamics. Our investi-
gation facilitates the systematic derivation of Bayesian-brain
RD in terms of a few effective variables, which we term
Bayesian mechanics (BM); BM conducts the homeostasis
and allostasis (that is, adaptive fitness) of living systems,
conforming with the proposed biological FEP.
The concept of coarse-graining, or effective description, is
ubiquitous in computational neurosciences [51– 57]. Here, we
review the recent research relevant to our work, which motiv-
ated the development of BM. Many previous studies of
recorded neurons showed that population dynamics is con-
fined to a low-dimensional manifold in empirical neural
space, where trajectories are neural representations of the
population activity [58]. In mathematical terms, the neural
modes were defined as eigen-fields that span the neural
manifold. The latent variables, or collective coordinates,
were defined as projection of the population activity onto
the neural modes [59,60]. Other theoretical models support
the idea that long-term dynamics in recurrent neural
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
2
networks gives rise to the attractor manifold [61], which is a
continuous set of fixed points occupying a limited region of
neural space. Consequently, the attractor dynamics and
switching between different attractors were manifested [62],
indicating a contextual change in neuronal representations
[63,64]. Moreover, the manifold hypothesis is widely applied
in machine learning to approximate high-dimensional data
using a small number of parameters [65]. Experimental
studies showed that a dynamical collapse occurs in the
brain from incoherent baseline activity to low-dimensional
coherent activity across neural nodes [66– 68]. Synchronized
patterns emerged when the featured inputs and prediction
derived from prior or stored knowledge matched; in contrast,
when there was a mismatch, the high-dimensional multi-unit
activity increased. This observation also provided empirical
evidence that neural signals reduce prediction errors, thereby
minimizing the IFE.
Based on the results described above, we suggest that the
latent dynamics can be effectively described by a small
number of coarse-grained variables in the reduced dimen-
sion. In this study, we formulate the BM of inferential
regulation of homeostasis in living systems in terms of a
few latent variables. The latent variables are determined as
the brain activities and their conjugate momenta that rep-
resent the external, environmental and motor, states and
online prediction errors, respectively. The sensory error at
the peripheral level acts as a time-dependent driving source
in BM, providing the neural mechanism for sensory, as well
as motor, inferences. Our continuous-state formulation in
continuous time may be useful for studying situated-action
problems in which biological systems must make decisions
even during ongoing sensorimotor activity [69].
The remainder of this paper is organized as follows. In §2,
we describe the establishment of the TFEP from NEQ fluctu-
ation theorems (FTs) when applied to living systems. Section
3 explains how stochastic dynamics at the neuronal level can
be modelled and how a statistical approach can be used to
determine the NEQ densities of neural states in the physical
brain. In §4, we present the proposed biological IFEP mini-
mizing long-term surprisal and establish its continuous-
state implementation that yields BM in the neural phase
space. Next, in §5, we numerically integrate BM and manifest
the attractor dynamics that performs perception and motor
inference in the brain. Finally , we summarize important out-
comes of our investigation and provide the conclusions in §6.
In appendix A, we present the dual closed-loop circuitry of
active inference resulting from our model.
2. Non-equilibrium fluctuation theorems applied
to organisms
FTs concisely describe stochastic NEQ processes in terms of
mathematical equalities [70,71]. Although FTs were initially
established for small systems, where fluctuations are appreci-
able, they also apply to macroscopic deterministic dynamics
[72]. Here, we present FTs in an appropriate context of bio-
logical problems and propose that the FTs suggest a living
organism is an NEQ system that maintains thehousekeeping
temperature, T (average 36:5
/C14 C in humans) within its body
and employs metabolism isothermally to act against its
environment.
To this end, among the various expressions of FTs, we use
the NEQ work relation [71]:
he/C0 bðW/C0 DFÞi¼ 1, ð2:1Þ
where β = 1/(kBT), withkB being the Boltzmann constant and
T being the temperature as described below. The mathemat-
ical equality given in equation (2.1) is known as the
Jarzynski relation [73]. Here,W is the amount of experimental
work performed on a small system immersed in a thermal
reservoir andΔF is the induced change in the Helmholtz FE
of the system. Accordingly , W − ΔF is the excess energy
associated with each irreversible work process in the
system, which is unavailable for a useful conversion. The
bracket, 〈 ··· 〉, indicates the average over many work strokes,
that is, work distribution subject to a protocol. The average
must be considered because the experimental work perform-
ance on small systems fluctuates.
The Jarzynski relation can be converted to an expression for
entropy as follows. By applying〈e
−βW〉 ≥ e−β〈W〉 to equation
(2.1), which is known as the Jensen inequality [74], we obtain
the inequality ΔF ≤ 〈W〉. This inequality is an alternative
expression that can be used to apply the second law toisother-
mal irreversible processes of the system initially prepared in
equilibrium with a reservoir [72]. Using the inequality,
one can consider the change in the average total entropy:
〈ΔS
tot〉 = 〈ΔSsys〉 + 〈ΔSR〉, where ΔSsys is the change in the
system entropy andΔSR is the change in the reservoir entropy.
The average associated withΔSR, which is reversible by defi-
nition, can be further manipulated to obtain〈ΔSR〉 = −〈Qsys〉/
T =( 〈W〉 − ΔU)/T, where QR = −Qsys is used in the first step,
and then the thermodynamic first law is applied for〈Qsys〉; U
is the internal energy of the system. Therefore, T〈ΔStot〉 =
〈W〉 − (ΔU − T〈ΔSsys〉)= 〈W〉 − ΔF, which leads to the stochastic
second law for the combined system and reservoir:
hDStoti/C21 0: ð2:2Þ
The possibility of tightening the preceding inequality has been
investigated among researchers by revealing a non-zero, posi-
tive bound, leading to thermodynamic uncertainty relations
[75,76]. The unavailable energy associated with individual
work processes amounts to the total entropy change, namely,
bðW /C0 DFÞ¼ k/C0 1
B DStot under isothermal conditions. By apply-
ing the final identity to equation (2.1), the Jarzynski equality is
cast to the integral form of entropy fluctuation:
he/C0 k/C0 1
B DStot i¼ 1: ð2:3Þ
In the biological context, W is the amount of environ-
mental work involved in the metabolism of a living system,
such as the biological reactions of oxygenic photosynthesis
and aerobic respiration [77,78]. The biological work is not
controllable and thus, stochastic. The FTs describe the imbal-
ance between energy intake and expenditure in an organism
while maintaining the housekeeping temperature. The Helm-
holtz FE increment in the living system over a metabolic work
cycle is limited by the average environmental work done on
the organism. The resulting inequality from the Jarzynski
relation can be written in the organism-centric form as
hWi/C20 DF, ð2:4Þ
where we sethWi¼/C0 h Wi and DF ; /C0 DF, which now states
that the work performance, hWi, of a biological system
against the environment (e.g. via metabolism) is bounded
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
3
from above by the thermodynamic FE cost,DF. The preced-
ing inequality reflects the limited efficiency of metabolic work
in living systems. Rare individual processes that violate
equation (2.4) may occur in small systems; however, such stat-
istical deflection is not expected in a finite biological system
with macroscopic degrees of freedom. The equality in
equation (2.4) holds for reversible work cycles in inanimate
matter, attaining thermodynamic efficiency at its maximum,
but not in the metabolic processes of living organisms,
which are irreversible. Our consideration of metabolic work
may be generalized to the multi-level autocatalytic cycles
suggested as the chemical origins of life [79].
Note here that we considered the temperature appearing
in the Jarzynski relation as the body temperature of a specific
biological system, unlike the usual implication of FTs; in the
standard derivation of the Jarzynski relation [80], the
temperature, T, appearing in the NEQ equality is, by con-
struction, the reservoir temperature. The FT is generally
intended for an irreversible process during which the
system temperature may not be defined. However, the initial
and end states must be in equilibrium so that the FE is mean-
ingful. The subtlety lies in the fact that the end-state
temperature may or may not be the same as the reservoir
temperature for experiments performed in isolation after the
initial equilibrium preparation. Living organisms are in an
NEQ stationary state, maintaining a housekeeping tempera-
ture, T, that is distinct from the ambient temperature, to
which they equilibrate only when ceasing to exist. Thus,
organisms are viewed as isothermal systems, which are
open to heat and particle exchange with the environment.
The NEQ work relation expresses the second law of ther-
modynamics as the mathematical equality in equation (2.1).
The second law, in its biological context, renders the thermo-
dynamic constraint on living organisms given by the
inequality in equation (2.4), which reveals the inevitable (ther-
modynamic) FE waste produced during metabolic cycles.
However, this inequality accounts for neither self-adaptiveness
nor brain functions, such as perception, learning and behav-
iour. To address these essential features of life, researchers
currently employ a hybridizing scheme, which first proposes
how the system-level biological functions operate and then
attempts to make connections to biophysical substances.
Particularly , the Bayesian mechanism built into the IFEP
provides a crucial component in this promising hybrid
explanation of life, which is described in detail in §4.
3. Statistical–physical description of the non-
equilibrium brain
The brain comprises a myriad of complex neurons; accord-
ingly , its internal dynamics at the mesoscopic level must
obey some stochastic equations of motion on account of clas-
sical indeterminacy. The relevant coarse-grained neural
variables are local-scale population activities, or intra-area
brain rhythms. In the following, we consider that the brain
matter itself constitutes the thermal environment at body
temperature for the mesoscopic neural dynamics.
Below, we assume that the neural activity,μ, at the coarse-
grained population level obeys the stochastic dynamics [81]:
dm
dt ¼ f ðm; tÞþ wðtÞ, ð3:1Þ
where the inertial term in the Langevin equation was
dropped by taking the over-damping limit. Here,f may rep-
resent both conservative and time-dependent metabolic
forces, and w represents random fluctuation characterized
as a delta-correlated Gaussian noise satisfying the following
conditions:
hwðtÞi ¼ 0 and hwðtÞwðt0Þi ¼ Idðt /C0 t0Þ,
where I is the noise strength. In one dimension (1D), for
simplicity, the environmental perturbation and noise strength
are physically specified, respectively , as [82]
f ¼ 1
mg A and I ¼ 2 kBT
mg ,
where A is a conservative force acting on a neural unit
with mass, m, neglecting time-dependent driving, T is
the body temperature, and γ is the phenomenological
frictional coefficient whose inverse corresponds to
momentum relaxation time. The solutions to equation (3.1)
describe the individual trajectories of random dynamical
processes.
In general, coloured noises can be considered beyond the
delta-correlated white noise by generalizing equation (3.1) to
incorporate the non-Markovian memory effect:
m
ð
t
/C0 1
dt0gðt /C0 t0Þ _mðt0Þ¼ AðmÞþ z:
To ensure equilibrium at temperatureT, the coloured Lange-
vin equation must satisfy the fluctuation– dissipation theorem
that accounts for the non-singular noise correlation [83]:
hzðtÞzðt0Þi ¼ 2kBTgðjt /C0 t0jÞ:
A standard example of such coloured noise is the Orstein–
Uhlenbeck memory kernel given by γ(|t − t0|) =γτ−1exp
( − |t − t0|/τ), whereτ is the noise autocorrelation time.
As an alternative to the Langevin equation (equation
(3.1)), one may collectively consider an ensemble of identical
systems displaying various values of the state,μ, and ask how
the statistical distribution changes over time. After normali-
zation, the ensemble distribution is reduced to the
probability density, say p(μ, t), so that p(μ, t)d μ specifies
the probability that an individual Brownian particle is
found in the range (μ, μ +d μ) at time t. In the Markovian
approximation, the change in the probability density is
determined by the probability density at the current time,
which is generally described by the master equation given
in the continuous-state formulation as
@pðm, tÞ
@t ¼
ð n
wðm, m0Þpðm0, tÞ/C0 wðm0, mÞpðm, tÞ
o
dm0, ð3:2Þ
where w(μ0, μ) is the transition rate of the state change fromμ to
another μ0. We further assume that the transition occurs
between two infinitesimally close states,μ and μ0, whereμ0 −
μ = x ≪ 1, so that the transition rate sharply peaks at around
x = 0 to approximate the value asw(μ0, μ) ≈ w(μ; x). Then,
p(μ0) can be expanded aboutμ to the second-order inx and
all higher-order terms are neglected. Consequently, the
master equation can be converted into the Smoluchowski–
Fokker– Planck (S-F-P) equation [82]:
@pðm, tÞ
@t ¼ @
@m
n
/C0 D1ðmÞþ @
@m D2ðmÞ
o
pðm, tÞ, ð3:3Þ
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
4
where D1 and D2 correspond to the first two expansion
coefficients in the Kramers– Moyal formalism, which are deter-
mined in the present case to be
D1 ¼ f and D2 ¼ 1
2 I:
The S-F-P equation can be expressed in three dimensions (3D)
as
@pðm, tÞ
@t þr/C1
n
fðmÞ/C0 Dr
o
pðm, tÞ¼ 0, ð3:4Þ
where r is the gradient operation with respect to the three-
dimensional state, m. In equation (3.4), the drift term, pf,
accounts for conservative potential forces. In addition, the diffu-
sion term, /C0 Drp, appears, assuming spatial isotropy for
simplicity and notational convention, where D
2 has been
replaced withD.
The S-F-P equation describes local conservation of the
probability ,pðm, tÞ, in the state space spanned by the state
vector, m, which carries the probability flux,j, identified as
jðm, tÞ¼ pðm, tÞfðmÞ/C0 Drpðm, tÞ:
In steady state (SS),∂pst/∂t = 0, wherepst ≡ p(μ, ∞); accord-
ingly , the divergence of the SS flux, jst ; jðm, 1Þ, must
vanish in the S-F-P equation:
r/C1 jst ¼ 0: ð3:5Þ
If Brownian particles undergo motion in an isolated or
infinite medium, jst should disappear on the local
boundary because the total flux through the surface must
vanish to ensure probability conservation. 1 Because the
flux must be continuous over the entire space, the SS
condition in equation (3.5) imposes j
st ; 0 everywhere,
reflecting thedetailed balancebetween the drift flux and dissi-
pative flux. In this case, the system holds in equilibrium,
where life ceases to exist. The equilibrium probability
can be obtained from the conditionj
st = 0, giving canonical
Boltzmann probability as the result:
peqðmÞ / expf/C0 bVðmÞg,
where V(μ) is potential energy. The kinetic-energy term does
not appear in peq because the Langevin dynamics we con-
sider are in the over-damping limit.
However, for a finite open system, such as a living organ-
ism, the system ’s SS flux does not necessarily vanish
on the local boundary; instead, it must be compensated by
the environmental afferent or efferent fluxes to achieve
steady state. Thus, for a living system, the detailed balance
is not satisfied in the steady state [84,85]; that is, j
st = 0.
Instead, the vanishing condition of the divergence of the
probability flux entails a necessary balance. The mathemat-
ical expression in equation (3.5) admits a non-vanishing
vector fieldBð
mÞ via
jstðmÞ ; r/C2 BðmÞ, ð3:6Þ
which shows that the SS flux is divergenceless or, equiva-
lently , solenoidal [86– 88]. The life flux, jst, defined in this
manner is unchanged when B is transformed to
B0 ¼ B þr L, where L is a scalar function of the state,m.2
From equation (3.6), the following generalized balance
condition must hold locally on the boundary:
pstðmÞfstðmÞ¼ DrpstðmÞþr/C2 BðmÞ: ð3:7Þ
The abovemodified detailed-balancecondition supports the fre-
quent interpretation of the force field,fst, as thegradient flow
of the SS probability ,pst [3,91]:
fstðmÞ¼ð D /C0 QÞr ln pstðmÞ, ð3:8Þ
where we introduced the scalar field,Q(μ), via
Qrpst ; /C0r /C2 B;
for simplicity,Q was assumed to be isotropic as it was for the
diffusion constant,D. The gradient flow is driven by entropy
because the most likely equilibrium state of the combined
system and environment is achieved by maximizing the
total entropy; hence, it is an entropic force, conforming to
the second law. Note that equation (3.6) mimics the
Ampere law in magnetism [92]; the effective fieldB may be
construed as an induced field by the static current, j
st.
Accordingly , the vector field,BðmÞ, can be determined by
means of
BðmÞ¼ 1
4p
ð
jstðm0Þ/C2 ðm /C0 m0Þ
jm /C0 m0j3 dm0: ð3:9Þ
Note that the modified detailed-balance condition given in
equation (3.7) is only a formal description for determining
the NEQ density, p
st, given SS flux, jst, or, equivalen-
tly, the environmental magnetic field, B, in equation (3.9).
Precise determination of pst is an independent research
subject, which may be non-Gaussian with a coloured
autocorrelation.
In general, it is difficult to obtain an analytic expression
for the NEQ probability density for open systems, except in
low-density and/or linear-response regimes [93,94]. Because
of further morphological complexity , it is practically intract-
able to derive the NEQ densities specifying the physical
brain states. Accordingly , in the following, the neural states
under continual sensory perturbation are assumed to be stat-
istically described by time-dependent Gaussian densities,
predicted from Gaussian random noises imposed on the
Langevin description.
4. Latent dynamics of sensorimotor inference
in the brain
Here, we present the BM for conducting Bayesian inversion
of sensory observation in the brain under the proposed gen-
eralized IFEP. This idea was previously developed by
considering passive perception [37] and only implicitly
including active inference [95]. Here, we advance this formal-
ism by explicitly introducing motor inference and planning in
the generative models to fully conform to the active-inference
framework.
The environmental states,
q, cause sensory stimuli,w,a t
the organism ’s receptors through mechanical, optical or
chemical perturbations, which are transduced in the brain’s
functional hierarchy in the form of a nervous signal. The sen-
sory perturbations may be altered by the organism’s motor
manipulation, and we designateu to denote the motor vari-
ables responsible for such control over the effectors. A
crucial point here is that the brain has access only to the sen-
sory data and not their causes; accordingly , from the brain’s
perspective, both the environmental states,
q, and motor
variables, u, are external, that is, hidden. In terms of these
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
5
relevant variables, we define the variational IFE functional,
denoted as F:
F½qðq, uÞ, pðq, u; wÞ/C138 ;
ð
dq
ð
duqðq, uÞ ln qðq, uÞ
pðq, u; wÞ ,
ð4:1Þ
where qðq, uÞ and pðq, u; wÞ are the recognition density (R-
density) and generative density ( G-density), respectively.
The R-density is the brain ’s online estimate of posterior
beliefs about the external causes of the sensory perturbation
(it probabilistically represents the environmental states). The
G-density encapsulates the brain ’s likelihood in beliefs
about sensory-data generation and prior beliefs about the
hidden environmental as well as motor dynamics (it prob-
abilistically specifies the internal model of sensory-data
generation, environmental dynamics and motor feedback).
Note that whereas theR-density is the current estimate, the
G-density contains the stored knowledge in the brain,
which can be updated by learning. In this study, we general-
ize theR-density as a bi-modal probability of
q and u, andG-
density as a tri-modal probability ofq, u and w. Note that a
semicolon is used between the sensory perturbation,w, and
hidden variables q and u in the G-density rather than a
comma to emphasize their differential role in perception.
The explicit inclusion of the motor variable,u, in theR-den-
sity and G-density is a key advancement over the standard
definition of IFE [33].
Now, using the product rule, pð
q, u; wÞ¼ pðq, ujwÞpðwÞ
for the G-density in equation (4.1), we decompose the IFE
to a form applicable in the biological context:
F½qðq, uÞ, pðq, u; wÞ/C138 ¼ DKL(qðq, uÞkpðq, ujwÞ) /C0 ln pðwÞ,
where DKL is the Kullback– Leibler divergence [74]. Because
DKL is non-negative, the following inequality holds, which
underpins the IFEP described in §1:
/C0 ln pðwÞ/C20 F½qðq, uÞ, pðq, u; wÞ/C138 , ð4:2Þ
where −lnp(w) is the information-theoretic measure ofsurpri-
sal. Here, it is important to notice the resemblance between
the preceding inequality and that given in equation (2.4)
from the TFEP.
Under the IFEP , the organism’s cognitive goal is to infer
the hidden environmental causes of sensory inputs with feed-
back from the motor-behaviour inference. This goal is
achieved by minimizing F with respect to the R-density,
qð
q, uÞ, which corresponds to the online adaptation of the
sensory and motor modules in the brain. For instance, in
the classic reflex arc, the proprioceptive stimulus evokes the
activity of sensory neurons in the dorsal root, and the
motor variable is engaged by the effector’s active states of
the motor neurons in the ventral root. The double procedures
are involved in the minimization scheme to cope with the bi-
modal cognitive nature of sensory and motor inferences: (i)
the internal model is updated to better predict the sensory
perturbation and (ii) the sensory perturbation is modified
by the agent ’s motor engagement to further reduce the
residual discrepancy with the internal model. The former is
termed aspassive perceptionand the latter asactive perception.
However, the two inferential mechanisms do not separately
engage, but act as a whole in the sensorimotor closed loop
in the embodied brain, and are therefore jointly termed as
active inferenceunder the IFEP [31,32].
To draw a connection between the IFE minimization and
neural correlates, it is practically convenient to use the fixed
form for the unknownR-density [33], whose sufficient stat-
istics are assumed to be encoded neurophysiologically by
brain variables, that is, neuronal activities. Here, we write
the R-density asqð
q, uÞ¼ qðqÞqðuÞ by considering the exter-
nal variables q and u as conditionally independent.
Furthermore, it is assumed that the factorizing densities,
qðqÞ and q(u), are Gaussian; the means of the environmental
states, q, and motor states,u, are encoded by the neuronal
variables μ and a, respectively. Then, by performing technical
approximations similar to those used in [33], we convert the
IFE functional, F, of theR- andG- densities to the IFEfunction,
F, of the neural representationsμ and a, given sensory data,s.
The sensory data or inputs are a neural representation of
the evoked perturbation, w, at the receptors, observed by
the organism’s brain. Here, the homunculus hypothesis,
the brain as a neural observer, is implicit, which assumes
teleological homology between the environmental processes
and brain’s internal dynamics.
The result for the IFE function, up to an additive constant,
is given as
Fð
m, a; sÞ¼/C0 ln pðm, a; sÞ; ð4:3Þ
here, the dependence on the second-order sufficient statistics,
namely (co)variances of the R-density, was optimally
removed. Consequently, the brain need only update the
means in the R-density in conducting the latent RD. The
mathematical procedure involved in equation (4.3) extends
the Laplace approximation delineated in the review [33]. To
complete the Laplace-encoded IFE, one must specify the
inferential structure in the encodedG-density, p(μ, a; s). We
facilitate probabilistic implementation of the generative
model using the product rule:
pð
m, a; sÞ¼ pðsjm, aÞpðm, aÞ, ð4:4Þ
where the likelihood density,p(s|μ, a), is the brain’s concur-
rent prediction of the encoded sensory data, s, from the
neuronal response,μ, and motor manipulation,a. Assuming
conditional independence between μ and a, the joint prior
p(μ, a) can be further factorized as
pðm, aÞ¼ pðmÞpðaÞ,
where p(μ) andp(a) are the brain’s prior beliefs regarding the
environmental-state changes and motor dynamics, respect-
ively. Thus, the Laplace-encoded IFE has been specified
solely in terms of the neural variablesμ and a, given sensory
data s, which is suitable for biologically plausible implemen-
tation of active inference in the physical brain.
Sensory states are stimulated by environmental causes
encoding sensory data, which, in turn, neurophysically
drive the neuronal population dynamics in the brain. The
population dynamics is complex and high-dimensional; how-
ever, the RD of the perceptual and behavioural inferences
may be well described in lower-dimensional neural mani-
folds. Below, we set up the plausible dynamics of coarse-
grained neural variables from classical indeterminacy ,
which constitute our generative models. First, we assume
that sensory data, s, are measured by a neural observer
according to instant mapping:
s ¼ gð
m, a; ugÞþ z, ð4:5Þ
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
6
where g is the generative map of the sensory data, andz is the
observation noise. Note that the generative map encapsulates
both the perceptual states,μ, and motor states,a, which con-
jointly predict the sensory data,s. We consider the sensory
generative model as a continuous process of sensory predic-
tion, g1(μ), by μ and error prediction, g2(a), by a via the
effector alteration:
½s /C0 g1ðmÞ/C138 /C0 g2ðaÞ ; s /C0 gðm, aÞ,
where we set g(μ, a)= g1(μ)+ g2(a). Second, we assume that
the neural activity,μ, obeys neuronal dynamics as described
in §3:
dm
dt ¼ f ðm; uf Þþ w, ð4:6Þ
where f is the generative function of the neuronal change, and
w is the involved random noise. Third, we assume that the
motor state,a, bears the motor-neuronal dynamics:
da
dt ¼ pða; upÞþ h, ð4:7Þ
where π is the generative function of the motor-neuronal
change, andη is the noise in the process. The generative func-
tion, π, plays the role of thepolicy in machine learning [35]:
the policy function, pða; upÞ, encapsulates the internal
model of motor planning in continuous time (see appendix
A). The dependence of the generative models on the par-
ameters θg, θf and up enables incorporation of a longer-term
neural efficacy , such as synaptic plasticity; below, we omit
the parameter dependence for notational simplicity. For the
neuronal generative equations, the continuous Hodgkin –
Huxley model [37] or a more biophysically realistic model
can be employed; however, our simple model in §5 suffices
to unveil the emergence of BM.
Noises in the neural generative models (equations
(4.5)– (4.7)) indicate stochastic mismatches between the cogni-
tive objectives on the left-hand side (l.h.s.) and their
prediction through the generative functions/map. Accord-
ingly , we consider thatz, w and η neurophysically encode
the probabilistic generative modelsp(s|μ, a), p(μ), and p(a),
respectively, (equation (4.4)) in the neuronal dynamics. Fur-
thermore, we assume that the random noises are
continuously distributed according to the normalized NEQ
Gaussian. Therefore, the Laplace-encoded likelihood,p(s|μ,
a), and prior densities, p(μ) and p(a), in equation (4.4) take
the following forms:
pðsj
m, aÞ¼ Nðs /C0 g;0 ,szÞ,
pðmÞ¼ Nð _m /C0 f;0 ,swÞ
and pðaÞ¼ Nð_a /C0 p;0 ,shÞ;
9
=
; ð4:8Þ
here, Nðx /C0 h;0 ,sÞ ; expf/C0ð 1=2sÞðx /C0 hÞ2g=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ps
p
denotes
a Gaussian density of stochastic variablex − h with variance
σ about the zero mean3 , and _x denotes the time derivative
of x, that is, dx/dt. The generative likelihood and prior den-
sities in equation (4.8) are thought to be stationary solutions
to the S-F-P equation or a more general non-Markovian exten-
sion, the biophysical derivation of which is beyond the scope
of this work. Instead, we assume the time-dependent Gaus-
sian probabilities effectively at zero temperature as
physically admissible densities encoding internal models in
the brain. Removing the assumption by rigorously deriving
physical probabilities is a key theoretical demand in future
studies.
Next, by substituting the expressions in equation (4.8) into
equation (4.3) using the decompositions via equation (4.4),
we obtain an explicit expression for the IFE function at an
instant t:
Fð
m, a; sÞ¼ 1
2sz
ðs /C0 gðm, aÞÞ2 þ 1
2sw
ð _m /C0 f ðmÞÞ2
þ 1
2sh
ð_a /C0 pðaÞÞ2, ð4:9Þ
where we dismissed the term1
2 lnfszswshg [37]. Our specific
construct of the IFE encapsulates motor planning explicitly in
continuous time via the policy,π(a), in the generative models.
Based on the Laplace-encoded IFE, the mathematical
statement for the biological FEP is given as
ð
dt{ /C0 ln pðsÞ} /C20
ð
dtFð
m, a; sÞ, ð4:10Þ
where the l.h.s. is equivalent to the Shannon uncertainty,Ð
dsf/C0 ln pðsÞgpðsÞ, under the ergodic assumption, which is
assured by the NEQ stationarity of living systems. The
inequality (equation (4.10)) shows that the upper bound of
sensory uncertainty can be estimated by minimizing the
time integral of F over a temporal horizon. Accordingly , if
we regard the integrandF as a Lagrangian, the systematic fra-
mework of the Hamilton principle can be employed to
implement the minimization scheme [96]. Next, we cast
equation (4.9) to a weighted summation of the quadratic
terms: F ¼
1
2
P
i mi12
i ði ¼ w, z, aÞ, where we defined the
notations 1i as
1w ; _m /C0 f ðmÞ,
1h ; _a /C0 pðaÞ
and 1z ; s /C0 gðm, aÞ,
9
=
; ð4:11Þ
which represent theprediction errorsinvolved in state, motor
and sensory inferences, respectively. Additionally , the
weight factors, m
w, mh and mz, are defined through the
variances as
mw ; 1
sw
, mh ; 1
sh
and mz ; 1
sz
, ð4:12Þ
where mi may be considered as a metaphor for the neural
inertial masses. The neural masses correspond to the predic-
tive precisions in the standard terminology [33]; heavier
neural masses lead to more precise predictions. The IFE F
as a Lagrangian, conforming to classical dynamics, can be
considered as a function of theinstant trajectories of μ(t) and
a(t), subject to the time-dependent force,s = s(t).
To exercise the Hamilton principle, we define theclassical
Action, S, as the time integral of arbitrary trajectoriesμ(t) and
a(t) in the configurational state space:
S½
mðtÞ, aðtÞ; tÞ/C138 ¼
ðt
t0
dt0F(mðt0Þ, aðt0Þ; sðt0Þ), ð4:13Þ
where t0 is the initial time, andτ ≡ t − t0 is the temporal hor-
izon of the relevant biological process. The initial time can be
chosen either in the past, that is,t
0 →−∞, or at present, that
is, t0 = 0. In the former,t is the present time, whereas in the
latter, t is the future time. Hence, active inference of the
living systems mathematically corresponds to varyingS, sub-
ject to the sensory stream, to find an optimal trajectory in the
configurational state space spanned byμ and a.
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
7
Furthermore, it is advantageous to consider the brain’s
RD in phase space rather than configurational space; the
phase space is spanned by positions and momenta. This is
because the momentum variables are meaningfulprediction
errors in the brain’s message passing algorithms; they are
defined via the informational Lagrangian,F,a s
pm ; @F
@ _m ¼ mwð _m /C0 fÞð 4:14Þ
and
pa ; @F
@ _a ¼ mhð_a /C0 pÞ, ð4:15Þ
where pm and pa are the momentum conjugates correspond-
ing to μ and a, respectively. Equation (4.11) reveals that the
momenta, pm and pa, are indeed the prediction errors, 1m
and 1h, weighted by the neural masses,mw and mz, respect-
ively. The purposive Hamiltonian, H, can be obtained by
performing the Legendre transformationH ; pm _m þ pa _a /C0 F.
After straightforward manipulation, we obtain the Hamil-
tonian function:
Hðm, a, pm, pa; sÞ¼ 1
2mw
p2
m þ 1
2mh
p2
a þ pmf ðmÞ
þ papðaÞ/C0 1
2 mz12
z , ð4:16Þ
which is a generator of time evolution in neural phase space.
The function H is specified in the cognitive phase space
spanned by the four-component vector,C, in the single corti-
cal-column formulation, whose components are defined as
CT ¼ð C1, C2, C3, C4Þ ; ðm, a, pm, paÞ,
where CT is the transpose ofC. Having determined the Ham-
iltonian, the Bayesian mechanical equations of motion
(termed as BM) can be abstractly written in the symplectic
representation as
_
Ci ¼/C0 Jij
@H
@Cj
, ð4:17Þ
where the block matrixJ is defined as
J ; 0 /C0 1
1 0
/C18/C19
, where 1 ¼ 10
01
/C18/C19
:
Specifically, we unpack equation (4.17) and explicitly display
the outcome:
_m ¼ 1
mw
pm þ f ðmÞ, ð4:18Þ
_a ¼ 1
mh
pa þ pðaÞ, ð4:19Þ
_pm ¼/C0 pm
@f
@m /C0 mzðs /C0 gÞ @g
@m ð4:20Þ
and _pa ¼/C0 pa
@p
@a /C0 mzðs /C0 gÞ @g
@a , ð4:21Þ
which are a coupled set of differential equations that are non-
linear, in general.
The preceding equations (4.18)– (4.21) comprise the BM of
the brain variables, which execute the RD of the Bayesian per-
ception and motor inference in the brain. The BM was
attained by applying the Hamilton principle, for which we
adopted the Laplace-encoded IFE as an informational
Lagrangian and derived the Hamiltonian to generate the
equations of motion. Our latent variables are the neural
representations (μ, a) and their conjugate momenta (p
m, pa);
they span the reduced-dimensional neural manifold. The
momenta represent the prediction errors neurophysiologi-
cally encoded by the error units in the neuronal population.
Below, we describe two significant features of the latent
dynamics, governed by the derived BM, subjected to the
time-varying sensory input,s(t).
(i) Equations (4.18)– (4.21) suggest that the brain mechan-
istically executes the cognitive operation, which reflects
Schrödinger’s suggestion of the organism’s functional
operation as a mechanical work [8]. Our derived BM
addresses the continuous-state implementation of IFE
minimization in continuous time, which contrasts fre-
quent discrete-time approaches [97 – 100]. We
considered that biological phenomena are naturally
continuous and, thus, continuous representations
better suit perception and behaviour.
(ii) BM in symplectic form (equation (4.17)) represents the
gradient-descent (GD) on the Hamiltonian function.
However, under non-stationary sensory inputs, the
multi-dimensional energy landscape is not static,
but incurs time dependence. Accordingly , the pre-
sented BM naturally facilitates fast dynamics beyond
the quasi-static limit implied by the usual GD
methods. In addition, it does not invoke the concept
of higher-order motions in the conventional frame-
work [101]; accordingly , our theory is not limited by
the issue of average flows versus the rate of change of
the average[38].
5. Numerical study of Bayesian mechanics
In this section, we numerically illustrate the latent dynamics
of the brain’s sensorimotor system resulting from the Hamil-
ton principle-based FE minimization formulation. For
simplicity, we consider a homogeneous, but time-dependent,
sensory input, such as non-stationary light intensity or temp-
erature, at the receptors, which emits a motor output
innervating the effectors that alter the sensory observation.
There are approximately 150 000 cortical columns in the
mammalian neocortex, and each cortical column exhibiting
a six-laminae structure may be considered as an independent
sensorimotor system [102,103]. Our simple model features the
double closed-loop circuitry delineated in appendix A within
a single column, which constitutes the basic computational
unit of canonical circuits in an actual large-scale brain net-
work [104].
The generative map, g, and functions, f and π,a r e
unknown; they may be nonlinear or even undescribable
within ordinary mathematics. Here, we exploit the linear
models assuming the generic structures:
gð
m, a; ugÞ¼ uð0Þ
g þ uð1Þ
g m þ uð2Þ
g a, ð5:1Þ
f ðm; uf Þ¼ uð0Þ
f þ uð1Þ
f m ð5:2Þ
and pða; upÞ¼ uð0Þ
p þ uð2Þ
p a, ð5:3Þ
where uðiÞ
a (α = f, g, π) are the parameters that are to be learned
and encoded as long-term plasticity in the neural circuits.
We have included the term uð2Þ
g a in equation (5.1), which
facilitates the additive motor-inference mechanism of the
sensory data; additionally,uð1Þ
g and uð2Þ
g magnify or demagnify
sensory prediction and motor emission by the internal
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
8
state, μ, and motor state, a, respectively; uð0Þ
g denotes
the default belief in the prediction. The constant termsuð0Þ
f
and uð0Þ
p in equations (5.2) and (5.3) specify the prior
beliefs on the state and motor changes, respectively; the
coefficients
uð1Þ
f and uð2Þ
p modulate the relaxation times to
the targets. In addition to these seven parametersuðiÞ
a , there
appear three neural masses, ma, in the BM unpacked in
equations (4.18)– (4.21). Hence, the proposed parsimonious
BM still encloses 10 parameters, which define a multidimen-
sional parameter space to explore for learning. The learning
problem was not pursued in this study but should be
explored in future investigations. Here, we focus on the
active inference problem, assuming that the optimal par-
ameters were already learned or amortized over the
developmental and evolutionary timescales; these par-
ameters are assumed to be shared for generating present
and future sensory data.
By substituting the generative functions given in
equations (5.1)– (5.3) into equations (4.18)– (4.20), the BM of
the state vector,
C, can be concisely expressed as
_C þ RC ¼ I, ð5:4Þ
where the relaxation matrix,R,i s
R ¼
/C0 uð1Þ
f 0 /C0 m/C0 1
v 0
0 /C0 uð2Þ
p 0 /C0 m/C0 1
h
/C0 mzuð1Þ
g uð1Þ
g /C0 mzuð1Þ
g uð2Þ
g uð1Þ
f 0
/C0 mzuð1Þ
g uð2Þ
g /C0 mzuð2Þ
g uð2Þ
g 0 uð2Þ
p
0
BBB
B
B
@
1
CCC
C
C
A
ð5:5Þ
and the source term,I, on the right-hand side (r.h.s.) is
IðtÞ¼
uð0Þ
f
uð0Þ
p
/C0 mzuð1Þ
g sðtÞþ mzuð0Þ
g uð1Þ
g
/C0 mzuð2Þ
g sðtÞþ mzuð0Þ
g uð2Þ
g
0
BB
B
B
@
1
CC
C
C
A
: ð5:6Þ
Note that the time-dependence in the source termI occurs
through the sensory inputs, s. The general solution
for equation (5.4) can be formally expressed by direct
integration as
CðtÞ¼ e/C0 RtCð0Þþ
ðt
0
dt0 e/C0 Rt0
Iðt /C0 t0Þ: ð5:7Þ
The first term on the r.h.s. of equation (5.7) describes
the homogeneous solution for an initial condition ofCð0Þ,
and the second term is the inhomogeneous solution driven
by the source, IðtÞ, manifesting the history-dependent
feature. The solution represents the brain’s cognitive trajec-
tory in action while continuously perceiving the sensory
inputs, s(t).
In the long-time limit,t →∞, we mathematically predict
that the trajectory in the state manifold will fall onto either
a fixed point, spiral node or repeller, satisfying_
Cst ¼ 0o ra
limit cycle about a centre satisfying_Cst ¼/C0 ivCst, where ω
is an angular frequency characterizing stationarity4 . The
details of the solution’s approach to a steady state will be
determined from the eigenvalue spectrum of the matrixR
and time-varying feature ofs(t). We denote the eigenvalues
and eigenvectors by λ( ≡ iω) and ϕ, respectively, and set up
the eigenvalue problem:
Rfa ¼ lafa:
The trace and determinant are invariant under a similarity
transformation; accordingly , the ensuing eigenvalues
must satisfy:
X
a
la ¼ trðRÞ¼ 0, ð5:8Þ
Y
a
la ¼ detðRÞ
¼ uð1Þ
f uð1Þ
f uð2Þ
p uð2Þ
p þ mz
mw
uð1Þ
g uð1Þ
g uð2Þ
p uð2Þ
p
þ mz
mh
uð1Þ
f uð1Þ
f uð2Þ
g uð2Þ
g : ð5:9Þ
The eigenvalues form the Lyapunov exponents in the finite-
dimensional manifold and characterize the dynamical
behaviour of the state vector near an attractor. Because of
the multi-dimensionality of the parameter space, it is not
ideal to extract the eigenvalue properties analytically from
the trace and determinant conditions. Accordingly, informa-
tive constraints on the parameters must be determined on
the heuristic basis. In this study, we numerically searched
for parameters that led to pure-imaginary eigenvalues,
thereby entailing stationary attractors.
5.1. Numerical result I: spontaneous dynamics
We first consider the spontaneous dynamics of the brain
evolved from the particular solution in equation (5.7) with
null sensory inputs in our proposed BM. The formal rep-
resentation for the spontaneous trajectory ,
CspðtÞ, can be
obtained by direct integration as
CspðtÞ¼ Cc /C0 R/C0 1 e/C0 RtIsp, ð5:10Þ
where the constant vector,Cc, is specified asCc ¼ R/C0 1Isp,
where Isp is the inhomogeneous term solely from the internal
driving sources without the sensory inputs, that is,s = 0 (see
equation (5.6)).
In figure 1, we depict the trajectories generated assuming
a set of parameters in the neural generative models
(equations (5.1)– (5.3)) as
5
ðuð0Þ
g , uð0Þ
f , uð0Þ
p Þ¼ð 0, 10, 10Þ,
ðuð1Þ
g , uð1Þ
f Þ¼ð 2e ip=2, /C0 1Þ
and ðuð2Þ
g , uð2Þ
p Þ¼ð eip=2,e ip=2Þ:
In addition, the neural inertial masses were assumed to have
values of
ðmz, mw, mhÞ¼ð 1, 1, 1Þ:
The major numerical observations are as follows. The brain’s
spontaneous trajectory occupies a limited region in the state
space around acentre,
Cc, which describes adynamic attractor
forming the brain’s resting states before sensory influx occurs.
The centre is specified by the internal parameters, that is, the
generative parameters and neural masses. We numerically
checked that the position of
Cc varies with the values of
neural masses and the brain’s prior belief on the hidden
causes of the sensory input and motor state. We also con-
firmed that the size of attractors is affected by the
generative parameters and neural masses.
5.2. Numerical result II: passive recognition dynamics
To demonstrate passive perception, we exposed the resting
brain to astatic sensory signal; that is, we inserteds = constant
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
9
in equation (5.6). In this case, the formal solution equation
(5.7) can be reduced to
CðtÞ¼ e/C0 RtCð0Þþ Cc /C0 R/C0 1 e/C0 RtI, ð5:11Þ
where, on the r.h.s., the first term specifies the homogeneous
transience of the initial resting state,Cð0Þ, second term,Cc,
denotes the centre of attractors, and last term describes the
dynamic development from the inhomogeneous source,
IðsÞ. In contrast to the spontaneous attractors, the location
of the centre depends on the sensory input, s:
Cc ¼ CcðsÞ¼ R/C0 1IðsÞ.
We performed numerical integration and obtained the
stationary attractor in the presence of static sensory inputs.
Thus, we confirmed that the attractor behaved similarly as
in the spontaneous case, but with a shift of the centre because
of the non-zero sensory stimulus. The outcome is presented
in figure 2. Figure 2 a shows a typical attractor in the
two-dimensional state space, which is evolved from a spon-
taneous state shown in figure 1. In addition, in figure 2b,
we show the change in thecognitive intensity, j
CcðsÞj2, with
respect to sensory inputs,s, which is defined as
jCcðsÞj2 ; CcC/C3
c :
Given a sensory stimulus, we numerically observe that the
cognitive intensity is weaker for a larger inertial mass. The
neural inertial masses represent the inferential precision in
the internal models; accordingly , the result shows that less
cognitive intensity is required when the internal model is
more precise in perceptual inference. The cognitive intensity
may be used as a quantitative measure of awareness or atten-
tion in phycology. Our intensity measure is closely related to
neuroimaging analysis [105], where the neural response to
sensory inputs was analysed as the energy-level change
associated with information encoding.
5.3. Numerical result III: active recognition dynamics
To cope with active perception, we considered the non-
stationary sensory input, s(t), that renders the time-depen-
dent driving I (equation (5.6)) in the latent dynamics: the
sensory receptors are continuously elicited, and the brain
engages in online computation to integrate the BM. For
numerical purposes, we assumed the salient feature of
sensory signal,s(t), as a sigmoid temporal dependence:
sðtÞ¼
s1
1 þ e/C0 kðt/C0 tmÞ , ð5:12Þ
where tm indicates the time when the sensory intensity
reaches the midpoint andk adjusts the stiffness of transience
in approaching the limiting value,s(t) →s∞. The sigmoidal
sensory inputs are depicted as a function of time in figure 3a.
We numerically integrated equations (4.18)– (4.21) assum-
ing the same initial state selected for the data shown in figure
2a, subject to the sensory stream presented in figure 3a.I n
figure 3b, we illustrate the imaginary part of the motor
state, pa(t), in continuous time, which is the online outcome
of active inference of the sensory input. For illustrational pur-
poses, we adopted the sigmoid shape for the temporal
dependence with a saturated value ofs
∞ = 100, stiffness of
k = 0.2, and mid-time of tm = 250. The results suggest
that the motor state aligns with the sensory variation and suc-
cessfully infers the sharp change in the sensory input around
t = 250.
In addition, figure 4 presents the attractor dynamics at
several time steps exhibiting state transition,dynamic bifur-
cation, from a resting state, Cð0Þ, to a cognitive attractor,
CðtÞ, over time [106]. The numerical computation reveals
the initial development of the NEQ attractor with passage
of time shown in figure 4a and figure 4b, which corresponds
to the inferential outcome of the lower part of the sigmoid
influx depicted in figure 3a. The intermediate attractor in
figure 4b repeats the spontaneous attractor presented in
figure 1 because the sensory input is nearly null apart from
the negligible fluctuation in the present model. As time
elapses from figure 4 b to figure 4 c, the cognitive state
begins to escape from the first attractor and build the
second attractor. Eventually, with passage of time shown in
figure 4c and figure 4d, the dynamic transition between two
attractors completes over a relaxation time period, say ,τ.A t
time t > τ, the stationary attractor can be described by the
expansion
CðtÞ¼ /C22Cc þ
X
a
ca e/C0 ivatfa, ð5:13Þ
where iva ; la and fa are the eigenvalues and correspond-
ing eigenvectors of the relaxation matrix,R, respectively. The
expansion coefficients, ca, are specified by the initial con-
dition, Cð0Þ. The centre of mass of the attractor, /C22Cc,i s
specified by R/C0 1I1, where I1 is the source vectorI with
the saturated sensory input, s∞. The shift of the centre
between two stationary attractors is shown in figure 4d.
The concrete example presented above fully accommo-
dates the active inference of a living agent inferring the
sensory signal’s salient feature and performing feedback
motor-inference in the double closed-loop cognitive architec-
ture (see appendix A). Although the illustration accounts for
a single sensorimotor system, our formulation can also
handle multiple modalities of sensory inputs posing multi-
sensory perception problems. Notably, the time-dependent
–10–20
20
10
0
–20
0
–40
0a
μ
pμ
Figure 1. Spontaneous attractor: for illustrational purposes, we depict the
attractor in the 3D state space spanned by ðRe½m/C138 ,R e½a/C138 ,R e½pm/C138Þ with
instantaneous other variables; the attractor centre, Cc , is positioned at
(−10, 10, −20). The full attractor evolves in the hyper space spanned by
the eight components of complex vector, C; in our model, there are the
four types of neuronal units ðm, a, pm, paÞ in a single cortical-column,
each of which is allowed to be a complex variable. (Data are in arbitrary units.)
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
10
sensory influx, s(t), makes the BM non-conservative, which,
from a dynamical-systems perspective, serves as a bifurcation
parameter. Our numerical illustration of the dynamic tran-
sition from a resting state to a cognitive attractor is relevant
to recent studies of cognitive control of behaviour in psychia-
try [107,108] and stability of conscious states against external
perturbations in patients with brain injury [109,110].
6. Summary and conclusion
This study is based on the consensus that living systems are
self-organized into an NEQ stationary state that violates the
detailed balance while sustaining physiological and bodily
properties. In a biological context, the thermodynamic
second law implies that there is inevitably uncompensated
energy in an organism’s metabolic processes of maintaining
its homeostasis in the environment. More precisely, the
amount of metabolic work is bounded from above by the
thermodynamic FE expense. Efficiency is important in any
irreversible phenomena exhibiting the arrow of time, and
by extension, in brain work. We applied modern FTs to a bio-
logical agent as an open system and clarified why the concept
of the FE is more appropriate than entropy when discussing
the question ofWhat is life?The thermodynamic and neuros-
cientific FEPs were evaluated based on their respective
mathematical inequalities, implying the FE bounds as vari-
ational objective functions for minimization. Consequently ,
we revealed the disadvantages of both principles in account-
ing for cognitive biological systems and proposed an
integrated thermodynamic and Bayesian approach to the
biological FEP as a self-organizing principle of life.
100
350cognitive intensity
300
250
200
150
100
50
20 40 60 80 100
s
p
a
50
–50
–100 –80 –60
(a)
(b)
–40 –20 20
a
Figure 2. Latent dynamics under static sensory inputs: (a) attractor developed from a resting state,Cð0Þ, and driven by the static inputs = 100, using the same
parameter values as in figure 1; the initial state was chosen from the spontaneous states in figure 1, and for illustrational purposes, the attractor is depicted in the
two-dimensional state space spanned by ðRe½C2/C138 ,R e½C4/C138Þ .( b) Cognitive intensity, jCc j2, versus sensory input, s. The filled squares are the results from the
neural inertial masses ðmz , mw , mhÞ¼ð 10, 1, 10Þ and open circles are the results fromðmz , mw , mhÞ¼ð 1, 1, 1Þ; the numerical values for the other gen-
erative parameters are the same as those used in figure 1. (Data are in arbitrary units.)
120
100
80
60
40
20
0 100 200 300 400 500
t
100 200 300 400 500
t
60
(b)(a)
40
20
–20
s (t)
pa (t)
Figure 3. Active dynamics under time-dependent sensory inputs: (a) salient feature of streaming perturbation at the receptor state,s(t); we assume a sigmoid
shape for the temporal dependence with the saturated values∞ = 100, stiffnessk = 0.2, and mid-timetm = 250. (b) Motor inference of the sensory signals; the
BM was integrated using the same parameter values as in figure 2a for the generative parameters and neural masses. (All curves are in arbitrary units.)
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
11
The brain states of higher organisms can only be realisti-
cally described probabilistically because of the enormous
neuronal degrees of freedom and morphological complexity.
And at the core of the biological FEP are the likelihood and
prior densities, making up theG-density, which are thought
to be the NEQ probabilities of the physical brain variables.
The G-density was Laplace-encoded by the brain variables
that were assumed to be the sufficient statistics of the
R-density. This study argues that brain dynamics at
the constitutional level are stochastic because of classical
negligence, for which time-asymmetric Langevin equations
were employed. The broken time-reversal symmetry was
attributed to the fact that biological systems are open to the
environment. To statistically describe the brain states, we
further used the Markovian approximation in state tran-
sitions and adopted the S-F-P equation to determine the
probability densities of the continuous brain variables. We
viewed the S-F-P equation as a local balance equation for
probability and argued that its steady-state solutions furnish
the NEQ densities. The probability flux appearing in the S-F-
P equation does not vanish at the brain– environment inter-
face, which reflects that a detailed balance will not be
reached in the SS limit, and thus, no standard fluctuation–
dissipation theorem is available in the NEQ brain. Instead,
the SS flux resembles the Ampere law in magnetism,
resulting from the modified detailed-balance condition and
supporting the gradient flow of the NEQ probabilities.
We presented the brain as Schrödinger ’s mechanical
machine operating predictive regulation of physiology and
adaptive behaviour of the body. The BM at the system level
is deterministic, indicating that the brain, as a macroscopic
physical system, obeys the law of large numbers entailing
dimensionality reduction. In addition, thermal fluctuations
from body temperature do not have significant effects on
the brain’s low-dimensional functions; in other words, the
brain is cognitively in its ground state at effective zero temp-
erature. The IFE was specified in terms of the latent brain
variables that probabilistically encode the environmental
and motor states. As aforementioned, the encoded prob-
ability densities were assumed to be SS solutions to the S-F-
P equation or more realistic ones. Central to our study was
the idea that the encoded, online IFE in the brain is a Lagran-
gian, defining the informational action. Based on Hamilton’s
principle, we found that the brain deterministically conducts
allostatic regulation by completing the double closed-loop
dynamics of perception and motor behaviour. We employed
a simple model for non-stationary sensory influx and illus-
trated the development of optimal trajectories in the neural
phase space: we numerically observed that the brain under-
goes a dynamic transition from a resting state to the
20
–20
–20
–20 –10 0
pμ
μ
–20 –10 0μ
–20 –10 0μ
–20 –10 0μ
–40
0
–20pμ
–40
0
–20
pμ
–40
0
–20
pμ
–40
0
–40
0
20
–20
–40
0
a
a
20
–20
–40
0a
20
–20
–40
0a
(a)( b)
(c) (d)
Figure 4. Attractor dynamics inferring the non-stationary sensory influx depicted in figure 3a:( a) t =5 , (b) t = 100, (c) t = 260 and (d ) t = 500. The trajectory,
CðtÞ, results from the direct numerical integration of the BM described by equations (4.18)–(4.21); the initial state, Cð0Þ¼ð /C0 16:9, 21:1, /C0 13:3Þ,w a s
selected from the spontaneous attractor given in figure 1. For numerical purposes, the attractor evolution is depicted in the three-dimensional state space spanned
by ðRe½m/C138 ,R e½a/C138 ,R e½pm/C138Þ . The numerical values adopted for all parameters are the same as those in figure 3. (Data are in arbitrary units.)
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
12
stationary attractor, which corresponds to the online inference
of the environmental causes in continuous time. The pro-
posed BM may apply to any generic cognitive processes at
the interoceptive, exteroceptive and proprioceptive levels.
In conclusion, organisms’ adaptive sustentation cannot be
described within thermodynamic laws and the ensuing TFEP ,
for which the brain-inspired IFEP provides a promising
avenue. The IFEP , however, uses teleological information-
theoretic models and then considers the neural bases of
those models. To establish an integrated framework of the
operational principle of life, two rationales of FE minimiz-
ation and Bayesian inference were hybridized, and the BM
directing the brain’s latent dynamics of active inference was
derived. Consequently, the brain’s perception and motor
inference in higher organisms were revealed to operate effec-
tively as Schrödinger’s mechanical machine. In addition, we
numerically illustrated the attractor dynamics that develops
online during a sensory stream in the low-dimensional
neural space.
Data accessibility.
This article has no additional data.
Conflict of interest declaration. I declare I have no competing interests.
Funding. No funding has been received for this article.
Acknowledgements. The author is grateful to J. Kang for providing assist-
ance with the mathematica programming.
Endnotes
1In an isolated or infinite medium, the net flux through the entire
surface must vanish to ensure probability conservation, i.e.
jnet ;
Þ
jst /C1 da ¼ 0, where d a is the outward, infinitesimal area
element. Accordingly ,jst ¼ 0 at every point on the surface.
2The freedom to choose the vector field, B, without affecting the
physical quantity ,jst, is known asgauge symmetry. Recently , research-
ers attempted to determine the implication and utility of the gauge
transformation in neuronal dynamics in the brain and emergent func-
tions [89,90].
3Here, we useσ, notσ2, to denote the variances only to be consistent
with the notations in an earlier publication [33].
4In this study , we distinguish between the concept of a stationary
state and a steady state: a steady state is the general term indicating
the limiting state ast →∞, whereas a stationary state is the specific
steady state where an oscillatory time dependence remains.
5These parameter values were selected in an ad hoc manner through
numerical inspection to produce a dynamic attractor; therefore, the
latent dynamics of the cognitive vector,C, is evolved in the extended,
complex-valued phase space in the present manifestation.
Appendix A. Dual structure of perception and
motor inference
Here, we describe a significant feature of our derived BM
capturing the dual nature of the sensory and motor inference
in the neocortex [111], and briefly discuss its relevance to
other control theories.
Figure 5 depicts the double-loop architecture of the neural
circuitry emerging from the attained BM. The environmental
cause,
q, registers the sensory data,s, at the peripheral inter-
face (receptors or input layers), and the brain conducts the
variational Bayesian inference that conjointly integrates the
double closed-loop dynamics of sensory perception (A) and
motor control (B). Note that the neural units ð
m, pm, a, paÞ
are connected by arrows for excitatory driving and by lines
guided by filled dots for inhibitory driving. Loop (A): the
state unit, μ, in neuronal population predicts the input, s,
based on the internal model,g1ðmÞ¼ uð0Þ
g þ uð1Þ
g m. The error
signal, ξz(μ)= s − g1(μ), weighted by the accuracy ,mz, of the
model, innervates the state-error unit,pm, in the population.
The error unit assimilates the discrepancy and sends the feed-
back signal to the state unit. Then, the state unit updates its
expectation and predicts the sensory reality again, which
completes the passive perceptual loop. Loop (B): the motor
(effector) unit, a, alters the sensory input,s, according to the
generative protocol, g
2ðaÞ¼ uð2Þ
g a, to promote accurate sen-
sation of the data. The error signal, mzξz(a)= mz(s − g2(a)),
acts as a control command to call for an adjustment in the
motor-error unit, pa. Then, the adjusted motor-dynamics
transmits the feedback signal to the effector state to further
modify the sensory reality , which completes the active
perceptive loop. The double closed-loop dynamics concur-
rently continue until an optimal trajectory ,
CðtÞ, is fulfilled
in the neural hyper-phase space, which corresponds to opti-
mizing the informational classical action, S, defined in
equation (4.13).
Our Hamiltonian formulation renders the sensory-
driving term, s − g(μ, a), to appear explicitly in the BM (see
equations (4.20) and (4.21)). Its role is similar to the unsuper-
vised updating rule in the reinforcement-learning framework
[35]; specifically, it resembles the continuous control signal
in the optimal control theory described by the Hamilton–
Jacobi– Bellman equation [112]. The sensory-discrepancy
signal not only affects the prediction error,p
m, in the state
prediction (equation (4.20)), but also the prediction error,pa,
of the motor inference (equation (4.21)); this interrelation pro-
vides the neural mechanism for adaptive motor feedback via
equation (4.19). The momenta in our formulation are termed
the costates in the deterministic optimal control theory.
Also, the policy,π, in equation (4.19) accounts for the online
motor behaviour, which prescribes action planning and can
accommodate asituated decision[69]. In the discrete-state for-
mulations, the policy is defined as a sequence of actions or
decisions in discrete time [99,113], where the authors incorpor-
ate the necessary state transitions directly in the definition of
FE. On the contrary , our continuous-time theory defines the
policy as continuous planning, which we model as the genera-
tive function of motor inference. The time-dependence of
policy generates the history-dependent response of the
brain’s cognitive state; see equation (5.7), in which the time,
t, can be either at present or in the future. When the dynamic
perception is coupled to categorical decision making, the
a
ϑ
μ
pa
g2(a)
g1(μ)
mz ξz(a)
mz ξz(μ)
S
pμ
(A)
(B)
Figure 5. Schematic of the neural circuitry exhibiting the double closed-loop
architecture of perception and action, which emerges from the Bayesian
mechanics prescribed by equations (4.18)–(4.21).
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
13
mixed continuous-discrete approaches may shape the active
inference problems [114].
Finally , in this work, we employed a set of simple and
specific generative models (equations (5.1)– (5.3)) for a con-
crete numerical illustration. In practice, however, the
employed models can be readily generalized. For instance,
one may consider an action-dependent generative function,
f(μ, a; θf ), which will make the state dynamics (equation
(4.6)) subjected to action. Similarly , the policy function
pða; upÞ in equation (4.7) may also be considered contingent
on states μ. Further investigations using more realistic
models are required to learn the implication and utility
of our dual closed-loop dynamics, related to the standard
control theories.
References
1. Fang X, Kruse K, Lu T, Wang J. 2019 Nonequilibrium
physics in biology. Rev. Mod. Phys. 91, 045004.
(doi:10.1103/RevModPhys.91.045004)
2. Farnsworth KD. 2018 How organisms gained causal
independence and how it might be quantified.
Biology 7, 38. (doi:10.3390/biology7030038)
3. Friston K. 2013 Life as we know it. J. R. Soc.
Interface 10, 20130475. (doi:10.1098/rsif.2013.0475)
4. Goldenfeld N, Woese C. 2011 Life is physics:
evolution as a collective phenomenon far from
equilibrium. Annu. Rev. Condens. Matter Phys. 2,
375–399. (doi:10.1146/annurev-conmatphys-
062910-140509)
5. Ivanitskii GR. 2010 21st century: what is life from
the perspective of physics?Phys. Usp. 53, 327–356.
(doi:10.3367/UFNe.0180.201004a.0337)
6. Kauffman S. 2020 Answering Schrödinger ’s ‘what is
life?’. Entropy 22, 815. (doi:10.3390/e22080815)
7. Nurse P 2020 What is life? Five great ideas in
biology. New York: W W Norton & Company.
8. Schrödinger E, Penrose R. 1992 What is life?: with
mind and matter and autobiographical sketches.
Cambridge, UK: Cambridge University Press.
9. Landau LD, Lifshitz EM 1980 Statistical physics part
1, vol. 5. 3rd edn. Course of theoretical physics
series. Oxford, UK: Pergamon Press.
10. Corcoran AW, Hohwy J. 2018 Allostasis,
interoception, and the free energy principle: feeling
our way forward. In The interoceptive mind: from
homeostasis to awareness (eds M Tsakiris, H De
Preester). Oxford, UK: Oxford University Press.
(doi:10.1093/oso/9780198811930.003.0015)
11. Quigley KS, Kanoski S, Grill WM, Barrett LF, Tsakiris
M. 2021 Functions of interoception: from energy
regulation to experience of the self.Trends Neurosci.
44,2 9–38. (doi:10.1016/j.tins.2020.09.008)
12. Goldford JE, Segre D. 2018 Modern views of ancient
metabolic networks. Curr. Opin. Syst. Biol. 8,
117–124. (doi:10.1016/j.coisb.2018.01.004)
13. Jelen BI, Giovannelli D, Falkowski PG. 2016 The role of
microbial electron transfer in the coevolution of the
biosphere and geosphere.Annu. Rev. Microbiol.70,
45–62. (doi:10.1146/annurev-micro-102215-095521)
14. Balasubramanian V. 2015 Heterogeneity and
efficiency in the brain. Proc. IEEE 103, 1346–1358.
(doi:10.1109/JPROC.2015.2447016)
15. Levy WB, Calvert VG. 2021 Communication
consumes 35 times more energy than computation
in the human cortex, but both costs are needed to
predict synapse number. Proc. Natl Acad. Sci. USA
118, e2008173118. (doi:10.1073/pnas.2008173118)
16. Sengupta B, Stemmler MB, Friston KJ. 2013
Information and efficiency in the nervous system—
a synthesis. PLoS Comput. Biol. 9,1 –12. (doi:10.
1371/journal.pcbi.1003157)
17. Schulkin J, Sterling P. 2019 Allostasis: a brain-
centered, predictive mode of physiological
regulation. Trends Neurosci. 42, 740–752. (doi:10.
1016/j.tins.2019.07.010)
18. Sterling P. 2012 Allostasis: a model of predictive
regulation. Physiol. Behav. 106,5 –15. (doi:10.1016/
j.physbeh.2011.06.004)
19. Maturana HR, Varela FJ 1980 Autopoiesis and
cognition: the realization of the living. Boston, MA: D
Reidel Publishing Company.
20. Friston K. 2009 The free-energy principle: a rough
guide to the brain? Trends Cogn. Sci. 13, 293–301.
(doi:10.1016/j.tics.2009.04.005)
21. Friston K. 2010 The free-energy principle: a unified
brain theory? Nat. Rev. Neurosci. 11, 127–138.
(doi:10.1038/nrn2787)
22. Catal O, Nauta J, Verbelen T, Simoens P, Dhoedt B.
2019 Bayesian policy selection using active
inference. In Workshop on structure & priors in
reinforcement learning at ICLR 2019: proceedings.
New Orleans, USA.
23. Catal O, Verbelen T, Van de Maele T, Dhoedt B,
Safron A. 2021 Robot navigation as hierarchical
active inference. Neural Netw. 142, 192–204.
(doi:10.1016/j.neunet.2021.05.010)
24. Da Costa L, Lanillos P, Sajid N, Friston K,
Khan S. 2022 How active inference could help
revolutionise robotics. Entropy 24, 361.
(doi:10.3390/e24030361)
25. Matsumoto T, Tani J. 2020 Goal-directed planning
for habituated agents by active inference using a
variational recurrent neural network. Entropy 22,
564. (doi:10.3390/e22050564)
26. Mazzaglia P, Verbelen T, Catal O, Dhoedt B. 2022
The free energy principle for perception and action:
a deep learning perspective. Entropy 24, 301.
(doi:10.3390/e24020301)
27. Meo C, Lanillos P. 2021 Multimodal V AE active
inference controller. In 2021 IEEE/RSJ Int. Conf. on
Intelligent Robots and Systems (IROS), Prague, Czech
Republic, pp. 2693–2699. (doi:10.1109/IROS51168.
2021.9636394)
28. Sancaktar C, van Gerven MAJ, Lanillos P. 2020 End-
to-end pixel-based deep active inference for body
perception and action. In 2020 Joint IEEE 10th Int.
Conf. on Development and Learning and Epigenetic
Robotics (ICDL-EpiRob), Valparaiso, Chile, pp. 1–8.
(https://doi.org/10.1109/ICDL-EpiRob48136.2020.
9278105)
29. Pezzulo G, Rigoli F, Friston K. 2015 Active inference,
homeostatic regulation and adaptive behavioural
control. Prog. Neurobiol.134,1 7–35. (doi:10.1016/j.
pneurobio.2015.09.001)
30. von Helmholtz H, Southall JPC. 1962 Helmholtz’s
treatise on physiological optics, vol. 3. New York, NY:
Dover Publications.
31. Adams RA, Shipp S, Friston KJ. 2013 Predictions not
commands: active inference in the motor system.
Brain Struct. Funct. 218, 611–643. (doi:10.1007/
s00429-012-0475-5)
32. Friston K, Mattout J, Kilner J. 2011 Action
understanding and active inference. Biol. Cybern.
104, 137–160. (doi:10.1007/s00422-011-0424-z)
33. Buckley CL, Kim CS, McGregor S, Seth AK. 2017 The
free energy principle for action and perception: a
mathematical review. J. Math. Psychol. 81,5 5–79.
(doi:10.1016/j.jmp.2017.09.004)
34. Huang Y, Rao RPN. 2011 Predictive coding.
WIREs Cogn. Sci. 2, 580–593. (doi:10.1002/
wcs.142)
35. Sutton R, Barto A (eds). 1998 Reinforcement
learning. Cambridge, MA: MIT Press.
36. Balaji B, Friston K. 2011 Bayesian state estimation
using generalized coordinates. In Signal processing,
sensor fusion, and target recognition XX (ed. I
Kadar), vol. 8050, p. 80501Y. International Society
for Optics and Photonics, SPIE. (doi:10.1117/12.
883513)
37. Kim CS. 2018 Recognition dynamics in the brain
under the free energy principle.Neural Comput. 30,
2616–2659. (doi:10.1162/neco_a_01115)
38. Aguilera M, Millidge B, Tschantz A, Buckley CL.
2021 How particular is the physics of the free
energy principle? Phys. Life Rev.40,2 4–50. (doi:10.
1016/j.plrev.2021.11.001)
39. Biehl M, Pollock FA, Kanai R. 2021 A technical
critique of some parts of the free energy principle.
Entropy 23, 293. (doi:10.3390/e23030293)
40. Bruineberg J, Dolega K, Dewhurst J, Baltieri M. 2021
The emperor’s new Markov blankets. Behav. Brain
Sci. 45,1 –63. (doi:10.1017/S0140525X21002351)
41. Colombo M, Wright C. 2021 First principles in the
life sciences: the free-energy principle, organicism,
and mechanism. Synthese
198, 3463–3488. (doi:10.
1007/s11229-018-01932-w)
42. Fiorillo C. 2010 A neurocentric approach to Bayesian
inference. Nat. Rev. Neurosci.11, 605. (doi:10.1038/
nrn2787-c1)
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
14
43. Kirchhoff M, Parr T, Palacios E, Friston K, Kiverstein
J. 2018 The Markov blankets of life: autonomy,
active inference and the free energy principle.
J. R. Soc. Interface 15, 20170792. (doi:10.1098/rsif.
2017.0792)
44. Korbak T. 2021 Computational enactivism under the
free energy principle. Synthese 198, 2743–2763.
(doi:10.1007/s11229-019-02243-4)
45. Raja V, Valluri D, Baggs E, Chemero A, Anderson
ML. 2021 The Markov blanket trick: on the scope
of the free energy principle and active inference.
Phys. Life Rev.39,4 9–72. (doi:10.1016/j.plrev.2021.
09.001)
46. Ramstead MJD, Badcock PB, Friston KJ. 2018
Answering Schrödinger’s question: a free-energy
formulation. Phys. Life Rev.24,1 –16. (doi:10.1016/
j.plrev.2017.09.001)
47. Sánchez-Cañizares J. 2021 The free energy principle:
good science and questionable philosophy in a
grand unifying theory. Entropy 23, 238. (doi:10.
3390/e23020238)
48. Friston K. 2019 A free energy principle for a
particular physics. arXiv 1906.10184. (doi:10.48550/
arXiv.1906.10184)
49. Shimazaki H. 2020 The principles of adaptation in
organisms and machines ii: thermodynamics of the
Bayesian brain. arXiv 2006.13158. (doi:10.48550/
arXiv.2006.13158)
50. Jirsa V, Sheheitli H. 2022 Entropy, free energy,
symmetry and dynamics in the brain. J. Phys.:
Complex. 3, 015007. (doi:10.1088/2632-072X/
ac4bec)
51. Amari S. 1977 Dynamics of pattern formation in
lateral-inhibition type neural fields.Biol. Cybern. 27,
77–87. (doi:10.1007/BF00337259)
52. Cook BJ, Peterson ADH, Woldman W, Terry JR. 2022
Neural field models: a mathematical overview and
unifying framework. Math. Neurosci. Appl. 2,1 –67.
(doi:10.46298/mna.7284)
53. Deco G, Jirsa VK, Robinson PA, Breakspear M,
Friston K. 2008 The dynamic brain: from spiking
neurons to neural masses and cortical fields.PLoS
Comput. Biol. 4, e1000092. (doi:10.1371/journal.
pcbi.1000092)
54. Fung CCA, Wong KYM, Wu S. 2010 A moving bump
in a continuous manifold: a comprehensive study of
the tracking dynamics of continuous attractor neural
networks. Neural Comput. 22, 752–792. (doi:10.
1162/neco.2009.07-08-824)
55. Hopfield JJ. 1982 Neural networks and physical
systems with emergent collective computational
abilities. Proc. Natl Acad. Sci. USA 79, 2554–2558.
(doi:10.1073/pnas.79.8.2554)
56. Robinson PA, Rennie CJ, Wright JJ. 1997
Propagation and stability of waves of
electrical activity in the cerebral cortex.
Phys. Rev. E 56, 826–840. (doi:10.1103/
PhysRevE.56.826)
57. Transtrum MK, Machta BB, Brown KS, Daniels BC,
Myers CR, Sethna JP. 2015 Perspective: sloppiness
and emergent theories in physics, biology, and
beyond. J. Chem. Phys. 143, 010901. (doi:10.1063/
1.4923066)
58. Cunningham J, Yu B. 2014 Dimensionality reduction
for large-scale neural recordings. Nat. Neurosci. 17,
1500–1509. (doi:10.1038/nn.3776)
59. Gallego JA, Perich MG, Chowdhury RH, Solla SA,
Miller LE. 2020 Long-term stability of cortical
population dynamics underlying consistent
behavior. Nat. Neurosci. 23, 260–270. (doi:10.1038/
s41593-019-0555-4)
60. Gallego JA, Perich MG, Miller LE, Solla SA. 2017
Neural manifolds for the control of movement.
Neuron 94, 978–984. (doi:10.1016/j.neuron.2017.
05.025)
61. Monasson R, Rosay S. 2013 Crosstalk and transitions
between multiple spatial maps in an attractor
neural network model of the hippocampus: phase
diagram. Phys. Rev. E 87, 062813. (doi:10.1103/
PhysRevE.87.062813)
62. Monasson R, Rosay S. 2015 Transitions between
spatial attractors in place-cell models. Phys. Rev.
Lett. 115, 098101. (doi:10.1103/PhysRevLett.115.
098101)
63. Jezek K, Henriksen E, Treves A, Moser EI, Moser MB.
2011 Theta-paced flickering between place-cell
maps in the hippocampus. Nature 478, 246–249.
(doi:10.1038/nature10439)
64. Wills TJ, Lever C, Cacucci F, Burgess N, O’Keefe J.
2005 Attractor dynamics in the hippocampal
representation of the local environment. Science
308, 873–876. (doi:10.1126/science.1108905)
65. Costa JA, Hero AO. 2004 Geodesic entropic graphs
for dimension and entropy estimation in manifold
learning. IEEE Trans. Signal Process.52, 2210–2221.
(doi:10.1109/TSP.2004.831130)
66. Peter A et al. 2019 Surface color and predictability
determine contextual modulation of V1 firing and
gamma oscillations. eLife 8, e42101. (doi:10.7554/
eLife.42101)
67. Singer W. 2021 Recurrent dynamics in the cerebral
cortex: integration of sensory evidence with stored
knowledge. Proc. Natl Acad. Sci. USA 118,
e2101043118. (doi:10.1073/pnas.2101043118)
68. Uran C et al. 2021 Predictive coding of natural
images by V1 activity revealed by self-supervised
deep neural networks. bioRxiv 2020.08.10.242958.
(doi:10.1101/2020.08.10.242958)
69. Cos I, Pezzulo G, Cisek P. 2021 Changes of mind
after movement onset depend on the state of the
motor system. eNeuro 8, ENEURO.0174-21.2021.
(doi:10.1523/ENEURO.0174-21.2021)
70. Crooks GE. 1999 Entropy production fluctuation
theorem and the nonequilibrium work relation for
free energy differences. Phys. Rev. E 60,
2721–2726. (doi:10.1103/PhysRevE.60.2721)
71. Jarzynski C. 1997 Nonequilibrium equality for free
energy differences. Phys. Rev. Lett. 78, 2690–2693.
(doi:10.1103/PhysRevLett.78.2690)
72. Kim CS. 2015 Statistical work-energy theorems in
deterministic dynamics. J. Korean Phys. Soc. 67,
273–289. (doi:10.3938/jkps.67.273)
73. Seifert U. 2008 Stochastic thermodynamics:
principles and perspectives. Eur. Phys.
J. B 64, 423–431. (doi:10.1140/epjb/e2008-
00001-9)
74. Cover TM, Thomas JA. 1991 Elements of information
theory. New York, NY: Wiley-Interscience.
75. Horowitz JM, Gingrich TR. 2019 Thermodynamic
uncertainty relations constrain non-equilibrium
fluctuations. Nat. Phys. 16,1 5–20. (doi:10.1038/
s41567-019-0702-6)
76. Seifert U. 2019 From stochastic thermodynamics to
thermodynamic inference. Annu. Rev. Condens.
Matter Phys. 10, 171–192. (doi:10.1146/annurev-
conmatphys-031218-013554)
77. Albarran-Zavala E, Angulo-Brown F. 2007 A simple
thermodynamic analysis of photosynthesis. Entropy
9, 152–168. (doi:10.3390/e9040152)
78. Swedan N. 2020 Photosynthesis as a
thermodynamic cycle. Heat Mass Transfer 56,
1649–1658. (doi:10.1007/s00231-019-02768-x)
79. Lehman NE, Kauffman SA. 2021 Constraint closure
drove major transitions in the origins of life.Entropy
23, 105. (doi:10.3390/e23010105)
80. Jarzynski C. 2011 Equalities and inequalities:
irreversibility and the second law of
thermodynamics at the nanoscale. Annu. Rev.
Condens. Matter Phys. 2, 329–351. (doi:10.1146/
annurev-conmatphys-062910-140506)
81. Kubo R, Toda M, Hashitsume N. 1992 Statistical
physics II. Berlin, Germany: Springer.
82. Risken H. 1989 The Fokker– Planck equation. Berlin,
Germany: Springer-Verlag.
83. Zwanzig R. 2001 Nonequilibrium statistical
mechanics. New York, NY: Oxford University Press.
84. Gnesotto FS, Mura F, Gladrow J, Broedersz CP. 2018
Broken detailed balance and non-equilibrium
dynamics in living systems: a review.Rep.
Prog. Phys. 81, 066601. (doi:10.1088/1361-
6633/aab3ed)
85. Lynn CW, Cornblath EJ, Papadopoulos L, Bertolero
MA, Bassett DS. 2021 Broken detailed balance and
entropy production in the human brain. Proc. Natl
Acad. Sci. USA 118, e2109889118. (doi:10.1073/
pnas.2109889118)
86. Qian H. 2013 A decomposition of irreversible
diffusion processes without detailed balance.
J. Math. Phys. 54, 053302. (doi:10.1063/1.
4803847)
87. Perl YS, Bocaccio H, Pallavicini C, Pérez-Ipiña I,
Laureys S, Laufs H, Kringelbach M, Deco G,
Tagliazucchi E. 2021 Nonequilibrium brain
dynamics as a signature of consciousness.
Phys. Rev. E 104, 014411. (doi:10.1103/PhysRevE.
104.014411)
88. Wang J, Xu L, Wang E. 2008 Potential landscape
and flux framework of nonequilibrium networks:
robustness, dissipation, and coherence of
biochemical oscillations. Proc. Natl Acad. Sci. USA
105, 12 271–12 276. (doi:10.1073/pnas.
0800579105)
89. Sakthivadivel DAR. 2022 Towards a geometry and
analysis for bayesian mechanics. arXiv 2204.11900.
(doi:10.48550/arXiv.2204.11900)
90. Sengupta B, Tozzi A, Cooray GK, Douglas PK, Friston
KJ. 2016 Towards a neuronal gauge theory.PLoS
Biol. 14, e1002400. (doi:10.1371/journal.pbio.
1002400)
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
15
91. Parr T, Costa LD, Friston K. 2020 Markov blankets,
information geometry and stochastic
thermodynamics. Phil. Trans. R. Soc. A 378,
20190159. (doi:10.1098/rsta.2019.0159)
92. Griffiths DJ. 2017 Introduction to electrodynamics,4
edn. Cambridge, UK: Cambridge University Press.
93. Freitas N, Falasco G, Esposito M. 2021 Linear
response in large deviations theory: a method to
compute non-equilibrium distributions.New J. Phys.
23, 093003. (doi:10.1088/1367-2630/ac1bf5)
94. Kim CS, Morriss GP. 2009 Local entropy in quasi-
one-dimensional heat transport. Phys. Rev. E 80,
061137. (doi:10.1103/PhysRevE.80.061137)
95. Kim CS. 2021 Bayesian mechanics of perceptual
inference and motor control in the brain.Biol.
Cybern. 115,8 7–102. (doi:10.1007/s00422-021-
00859-9)
96. Landau LD, Lifshitz EM. 1976 Mechanics, vol. 1. 3rd
edn. Course of theoretical physics series.
Amsterdam, The Netherlands: Elsevier Ltd.
97. Da Costa L, Parr T, Sajid N, Veselic S, Neacsu V,
Friston K. 2020 Active inference on discrete state-
spaces: a synthesis. J. Math. Psychol. 99, 102447.
(doi:10.1016/j.jmp.2020.102447)
98. Friston K, FitzGerald T, Rigoli F, Schwartenbeck P,
Pezzulo G. 2017 Active inference: a process theory.
Neural Comput. 29,1 –49. (doi:10.1162/NECO_a_
00912)
99. Sajid N, Ball PJ, Parr T, Friston KJ. 2021
Active inference: demystified and compared.
Neural Comput. 33, 674–712. (doi:10.1162/neco_
a_01357)
100. Smith R, Friston KJ, Whyte CJ. 2022 A step-by-step
tutorial on active inference and its application to
empirical data. J. Math. Psychol. 107, 102632.
(doi:10.1016/j.jmp.2021.102632)
101. Friston K, Ao P. 2012 Free energy, value, and
attractors. Comput. Math. Methods Med. 2012,
937860. (doi:10.1155/2012/937860)
102. Hawkins J, Ahmad S, Cui Y. 2017 A theory of how
columns in the neocortex enable learning the
structure of the world.Front. Neural Circuits11, 81.
(doi:10.3389/fncir.2017.00081)
103. Mountcastle VB. 1997 The columnar organization of
the neocortex. Brain 120, 701–722. (doi:10.1093/
brain/120.4.701)
104. Bastos AM, Usrey WM, Adams RA, Mangun GR, Fries
P, Friston KJ. 2012 Canonical microcircuits for
predictive coding. Neuron 76, 695–711. (doi:10.
1016/j.neuron.2012.10.038)
105. Kuzma S. 2019 Energy-information coupling
during integrative cognitive processes.
J. Theor. Biol. 469, 180–186. (doi:10.1016/
j.jtbi.2019.03.005)
106. Izhikevich E. 2007 Dynamical systems in
neuroscience. Cambridge, MA: MIT Press.
107. Cui Z et al. 2020 Optimization of energy state
transition trajectory supports the development of
executive function during youth. Elife 27, e53060.
(doi:10.7554/eLife.53060)
108. Parkes L et al. 2021 Network controllability in
transmodal cortex predicts positive psychosis
spectrum symptoms. Biol. Psychiatry 90, 409–418.
(doi:10.1016/j.biopsych.2021.03.016)
109. Deco G, Cruzat J, Cabral J, Tagliazucchi E, Laufs H,
Logothetis N, Kringelbach M. 2019 Awakening:
predicting external stimulation to force transitions
between different brain states.Proc. Natl Acad.
Sci. USA116, 201905534. (doi:10.1073/pnas.
1905534116)
110. Perl YSet al.2021 Perturbations in dynamical models of
whole-brain activity dissociate between the level and
stability of consciousness.PLoS Comput. Biol.17,
e1009139. (doi:10.1371/journal.pcbi.1009139)
111. Doya K. 2021 Canonical cortical circuits and the
duality of Bayesian inference and optimal control.
Curr. Opin. Behav. Sci.41, 160–167. (doi:10.1016/j.
cobeha.2021.07.003)
112. Todorov E. 2006 Optimal control theory. InBayesian
brain: probabilistic approaches to neural coding
(eds K Doya, S Ishii, A Pouget, RPN Rao), pp. 269–
298. Cambridge, MA: The MIT Press.
113. Isomura T, Shimazaki H, Friston KJ. 2022
Canonical neural networks perform active inference.
Commun. Biol. 5, 55. (doi:10.1038/s42003-
021-02994-2)
114. Parr T, Friston KJ. 2018 The discrete and continuous
brain: from decisions to movement— and back
again. Neural Comput. 30, 2319–2347. (doi:10.
1162/neco_a_01102)
royalsocietypublishing.org/journal/rsfsInterface Focus13: 20220041
16