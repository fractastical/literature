Optimal Design for Adaptive In-Context Prompt
Tuning in Large Language Models
Subhojyoti Mukherjee∗
ECE Department
UW-Madison
Wisconsin, Madison
smukherjee27@wisc.edu
Anusha Lalitha
AWS AI Labs
Santa Clara
USA
Aniket Deshmukh
AWS AI Labs
Santa Clara
USA
Ge Liu
AWS AI Labs
Santa Clara
USA
Yifei Ma
AWS AI Labs
Santa Clara
USA
Branislav Kveton
AWS AI Labs
Santa Clara
USA
Abstract
One emergent ability of large language models (LLMs) is that query-specific
examples can be included in the prompt at inference time. In this work, we use
active learning for adaptive prompt design and call it Active In-context Prompt
Design (AIPD). We design the LLM prompt by adaptively choosing few-shot
examples from a training set to optimize performance on a test set. The training
examples are initially unlabeled and we obtain the label of the most informative
ones, which maximally reduces uncertainty in the LLM prediction. We propose two
algorithms, GO and SAL, which differ in how the few-shot examples are chosen.
We analyze these algorithms in linear models: first GO and then use its equivalence
with SAL. We experiment with many different tasks in small, medium-sized, and
large language models; and show that GO and SAL outperform other methods for
choosing few-shot examples in the LLM prompt at inference time.
1 Introduction
Large language models (LLMs), such as Vicuna [Chiang et al., 2023], Falcon-40B [Penedo et al.,
2023], and OpenLLaMA [Touvron et al., 2023] are applied in mainly two ways: fine-tuning and
prompt tuning. In fine-tuning, the LLM weights are adapted to a downstream task [Devlin et al.,
2018]. Fine-tuning can easily incorporate domain knowledge that a pre-trained model may not
possess and resembles classic inductive inference. Fine-tuned models often do not need carefully
designed prompts, which makes them easier to deploy. The main drawback of fine-tuning is that it
can be costly, because tens of thousands of training examples may be needed to fine-tune billions
of parameters of the LLM [Ding et al., 2023]. In prompt tuning, the LLM weights are fixed and the
LLM is given query-specific examples at inference time that affect its output [Lester et al., 2021].
This ability to conduct in-context inference is one of the emergent abilities of LLMs. Prompt tuning
does not require large training sets. It is also preferred when query-specific examples are private or
change over time, and thus can only be utilized at inference time.
Prior works on prompt tuning mainly focus on hard prompts, which are carefully handcrafted to
get the desired output. This can be time-consuming and fragile, as minor prompt modifications can
∗Work conducted during an internship at Amazon.
Preprint. Under review.
arXiv:2404.08846v2  [cs.LG]  31 May 2024
lead to a significant performance drop on the downstream task [Suzgun et al., 2022]. In contrast,
Zhang et al. [2022a,b] and Diao et al. [2023] explored adaptive prompt design using clustering-based
and uncertainty-reducing approaches. While these approaches offer some benefits, we argue that
optimal designs [Pukelsheim, 2006, Fedorov, 2013] can outperform them by effectively balancing
uncertainty and diversity. Similarly to Zhang et al. [2022a,b] and Diao et al. [2023], we propose a
framework for adaptive prompt design called active in-context prompt design (AIPD). The key idea is
to design the LLM prompt by adaptively choosing few-shot examples for a set of test examples at
inference time. The examples are initially unlabeled and we obtain labels for the most informative
ones, which maximally reduce the uncertainty in the LLM prediction for all test examples. We
assume that the observed labels are collected from experts (human-in-the-loop) or revealed by an
oracle [Dasgupta, 2005, Hanneke et al., 2014]. The focus on informativeness and diversity ensures
efficient label acquisition by selecting the best examples. This reduces reliance on limited and costly
resources such as expert labeling.
One motivating example for our work is theme recognition, where the goal is to identify a unifying
theme for a set of items (e.g., movies, grocery items, or books) provided by the user. For example, let
the test query be a triplet of movie titles “Lion King”, “Jungle Book”, and “Tarzan”, and the goal is
that the LLM should infer a plausible common theme such as “Disney animated movies”, “Children’s
movies”, or “Movies with deep connections with nature”. This task is challenging due to the inherent
ambiguity and many plausible themes. To address this, we can give the LLM a few informative
examples of triplets of movies and their common themes as training examples in context that can
guide it towards the correct theme for the test query. This inherently requires a human-in-the-loop
who can go over the set of triplets of movies and label their common theme for each example which
can be costly. Hence, it is critical to narrow down to a few informative examples from exponentially
many training examples possible for vast amounts of data like movies. Finally note that by exposing
the LLM to these training examples, we refine its understanding of the task, improve handling of
ambiguity, and thus improve its ability to identify the common theme for unseen test examples. To
address the above challenges, we propose a framework for adaptive prompt design called active
in-context prompt design (AIPD). Our framework is general and can be easily extended to any active
supervised-learning task, like active regression [Gao and Koller, 2011] and active classification [Gao
and Koller, 2011]. At a high level, we treat the LLM as a general inference machine [Brown et al.,
2020b, Mirchandani et al., 2023] that is shown adaptively-chosen examples with labels at inference
time. The LLM then utilizes them to answer any set of related test examples. The key idea is to
choose the next example to label such that we maximally reduce the estimated uncertainty of the
answer to the test examples. We focus on designing algorithms with the following two properties:
(1) Implementable in any LLM that can be queried efficiently. The parameters of the LLM do not
change or have to be observed. (2) Analyzable in simple models. In this work, we use linear models
to motivate and analyze our algorithms.
We now state the main contributions of our work:
(1) We propose a G-Optimal design algorithm (GO). The key idea in GO is to retrieve the examples
to label that are closest to the set of test examples in the inference task. Our main contribution is the
right notion of closeness, based on posterior covariance in a simpler model. GO is implementable
with any LLM that can be sampled from, and does not require access to model parameters, feature
embeddings of the LLM, or its gradients.
(2) We propose a Simulation-Based Active Learning algorithm (SAL). SAL uses simulation to
estimate the impact of labeling unlabeled examples on uncertainty of the example in the inference
task. SAL is also implementable with any LLM that can be sampled from.
(3) GO is motivated by optimal designs in linear models [Kiefer and Wolfowitz, 1960, Pukelsheim,
2006]. This allows us to analyze GO in linear models (Theorem 1). Our proof is a major departure
from similar analyses in fixed-budget best-arm identification in bandits [Azizi et al., 2022, Yang
and Tan, 2022], for instance because we directly analyze the discrete allocation problem and each
unlabeled example can be labeled at most once. We discuss this in detail right after Theorem 1. SAL
is more general than GO because it does not make any linear model assumption in its design. We
show that SAL and GO are equivalent in linear models in Theorem 2.
(4) We evaluate GO and SAL on UCI [Markelle Kelly, 1988] and OpenML [Vanschoren et al., 2013]
regression and classification tasks, custom NLP datasets, abstract reasoning corpus (ARC) tasks
[Alford, 2021, Mirchandani et al., 2023], and Probabilistic Context Free Grammar (PCFG) tasks
2
[Hupkes et al., 2020]. GO and SAL consistently outperform other active prompt tuning methods
[Zhang et al., 2022a,b, Diao et al., 2023] for choosing few-shot examples in majority of the tasks.
We advance the understanding of active in-context prompt design in LLMs and develop a practical
methodology for adaptive prompt design. To our knowledge, this is the first paper that analyzes
optimal design based prompting approaches that correctly balance uncertainty and diversity-based
sampling as opposed to other existing adaptive prompting-based approaches [Zhang et al., 2022a,b,
Diao et al., 2023].
This paper is organized as follows. Section 2 introduces the problem setting. Section 3 presents our
methods and discusses their properties. Section 4 is devoted to analyzing our methods. Section 5
validates our approach empirically. We review related work in detail in Appendix A. Finally, Section 6
summarizes our contributions and suggests avenues for future work.
2 Setting
We pose the problem of adaptive prompt design as active learning. We adopt the following standard
active learning terminology [Lewis, 1995, Tong and Koller, 2001, Dasgupta, 2005, Dasgupta et al.,
2007, Hanneke et al., 2014]. We have a d-dimensional feature space X ⊂Rd and a dy-dimensional
label space Y ⊆Rdy . A labeled example is a pair (x, Y) ∈ X × Y. The feature vectors and labels
are related as Y = f(x, θ∗) + ε, where f is an underlying model, θ∗ is its parameter, and ε is an
independent zero-mean noise vector. Our goal is to learn to estimate f on test examples by labeling
training examples. We have a budget T on the maximum number of training examples that can
be labeled. This constraint can arise due to multiple reasons. For instance, human labels may be
necessary and they are naturally costly. Another reason may be that the machine learning model has
a limited capacity for including labeled examples, such as the length of prompts in LLMs [Zhang
et al., 2022a,b, Diao et al., 2023].
Now we introduce our notation in detail. Denote [m] = {1, 2, . . . , m}. We have n training examples
Xexamples = {x1, . . . ,xn} and K test examples X∗ = {x∗,1, . . . ,x∗,K}. We assume that both sets are
related, such as being sampled from the same distribution. The label of the test example x∗,k is Y∗,k.
In our motivating theme recognition example the training examples xi and test example x∗,k is a
concatenation of triplets of movies, and the label Yi or Y∗,k is the common theme amongst the triplets
respectively. We want to infer Y∗,k for all k ∈ [K] without explicitly modeling the complex function
f. We model the function using an LLM which we treat as a general inference machine because of
its large representation capacity [Brown et al., 2020a, Mirchandani et al., 2023]. Specifically, let
Ht = {(Xℓ, Yℓ)}ℓ∈[t−1] be a set of t − 1 previously labeled examples, where Xℓ ∈ Xexamples is the
ℓ-th labeled example and Yℓ is its label. Then we denote by p(· |x, Ht) the distribution over labels
of an LLM for a queried example x when Ht is used as few-shot in-context examples. To implement
this in the LLM, we simply concatenate x and Ht in context [Zhang et al., 2022a,b, Diao et al., 2023].
We know that in-context examples affect the distribution of responses of an LLM [Xie et al., 2021,
Suzgun et al., 2022, Deng et al., 2023, Lee et al., 2023]. So, the problem of learning f under a budget
T can be viewed as selecting HT+1 such that p(Y∗,k | x∗,k, HT+1) is high for all test examples
k ∈ [K]. This problem is challenging, especially when the training examples need to be labeled.
To effectively reduce the uncertainty of Y∗,k | x∗,k, HT+1, we need to quantify it. One possibility
it to use the entropy −EY∗,k∼p(·|x∗,k,HT+1)[log p(Y∗,k | x∗,k, HT+1)]. This is problematic because
the entropy is hard to estimate for high-dimensional random variables [Vershynin, 2020], especially
without having access to p(· |x∗,k, HT+1) beyond sampling from it. This is a shortcoming of recent
adaptive prompting techniques [Zhang et al., 2022a,b, Diao et al., 2023]. Therefore, we propose
using the covariance of Y∗,k | x∗,k, HT+1 as the uncertainty measure. Specifically, we measure the
uncertainty of the k-th test example by tr(cov[Y∗,k | x∗,k, HT+1]) and the uncertainty over all test
examples by maxk∈[K] tr(cov[Y∗,k | x∗,k, HT+1]). Since the trace of the covariance is the sum of
the variances in individual dimensions, our objective can be interpreted as minimizing the maximum
variance over the predicted labels of all test examples. This is a natural measure of uncertainty in
linear models and corresponding optimal designs [Pukelsheim, 2006, Fedorov, 2013].
Before we present our algorithms, we wanted to outline their general design. Given a budget T, we
design sequential adaptive algorithms over T rounds, where the example Xt ∈ Xexamples in round
t ∈ [T] is chosen as a function of Ht = {(Xℓ, Yℓ)}ℓ∈[t−1] up to that round. Since Ht summarizes
past actions of the algorithm, we call it a history. The label of example Xt is Yt = f(Xt, θ∗) + εt,
3
Algorithm 1 G-optimal design (GO)
1: Input: Training set Xexamples = {xi}n
i=1, test set X∗ = {x∗,k}K
k=1, budget T
2: L1 ← ∅, U1 ← [n], H1 ← {}
3: for t = 1, . . . , Tdo
4: It = arg mini∈Ut maxk∈[K] x⊤
∗,k

bΣ−1
t + xix⊤
i
−1
x∗,k
5: Xt ← xIt ∈ Xexamples
6: Observe label Yt of example Xt
7: Lt+1 ← Lt ∪ {It}, Ut+1 ← Ut \ {It}
8: Ht+1 ← Ht ∪ {(Xt, Yt)}
9: end for
10: Output: Sample Y∗,k ∼ p(· |x∗,k, HT+1) for k ∈ [K]
where εt is an independent zero-mean noise vector in round t. Our objective is to minimize the
maximum uncertainty over all test examples, maxk∈[K] tr(cov[Y∗,k | x∗,k, HT+1]).
3 Algorithms
In this section, we introduce our active learning algorithms for selecting most informative training
examples from Xexamples. To simplify notation, we assume scalar labels and then discuss an extension
to vector labels at the end of the section. We also let Lt ⊆ [n] and Ut ⊆ [n] be the indices of all
labeled and unlabeled training examples up to round t, respectively. Note that Lt ∪ Ut = [n].
3.1 Optimal Design Algorithm
The key idea is to label examples in Xexamples that minimize the maximum uncertainty of predictions
over all test examples x∗,k. Our computation of uncertainty is borrowed from linear models. Specif-
ically, take a linear model Y = x⊤θ∗ + ε, where θ∗ ∈ Rd is its parameter and ε ∼ N(0, σ2) is
independent noise. Suppose that θ∗ ∼ N(θ0, Σ0). Then a well-known result in Bayesian statistics
[Bishop, 2006] is that the posterior variance of the model estimate at an example x∗,k given labeled
examples Ht is x⊤
∗,k bΣtx∗,k, where bΣt = (Σ−1
0 + σ−2 Pt−1
ℓ=1 XℓX⊤
ℓ )−1 is the posterior covariance
of θ∗ | Ht. Therefore, the maximum uncertainty over test examples is maxk∈[K] x⊤
∗,k bΣtx∗,k. The
key observation is that this quantity does not depend on labels. Therefore, it can be optimized greedily
by choosing the training example that minimizes it the most,
It = arg min
i∈Ut
max
k∈[K]
x⊤
∗,k

bΣ−1
t + xix⊤
i
−1
x∗,k , (1)
where Ut are indices of all unlabeled training examples up to round t. After the index It is chosen,
the example xIt and its label Yt are added to the history to get Ht+1 for the next iteration t + 1.
This algorithm is a greedy solution to the G-optimal design [Pukelsheim, 2006, Katz-Samuels et al.,
2021]. We call it G-Optimal design and abbreviate it as GO. The pseudocode of GO is in Algorithm 1.
Note that GO does not depend on observed Yt. Similar optimal designs have been effectively applied
in active learning [Chaudhuri et al., 2015, Mukherjee et al., 2022], bandits [Fontaine et al., 2021,
Mason et al., 2021], and reinforcement learning [Wagenmaker et al., 2022]. However, this is the first
paper that studies optimal design for adaptively designing prompts [Zhang et al., 2022a, Diao et al.,
2023]. (1) can be viewed as choosing that training example xi ∈ Ut that minimizes the maximum
eigenvalue of the posterior covariance bΣt. Therefore this leads to reducing the uncertainty over
the model parameter θ∗ as the confidence ellipsoid around θ⋆ shrinks [Lattimore and Szepesvári,
2020]. Note that maximum eigenvalue reduction also ensures diversity as it leads to choosing training
examples along all directions in Rd.
The time complexity of GO is O(Kd2nT). This is because, for T rounds, the algorithm searches
for the best training example out of at most n and evaluates it on all test examples x∗,k ∈ X∗. The
evaluation of each test example in round t, x⊤
∗,k(bΣ−1
t + xix⊤
i )−1x∗,k, takes O(d2) time, because
(bΣ−1
t + xix⊤
i )−1 can be computed in O(d2) time using the Sherman-Morrison formula. In the last
step, the LLM is queried K times to return {Y∗,k}K
k=1.
4
Algorithm 2 Simulation-based active learning (SAL)
1: Input: Training set Xexamples = {xi}n
i=1, test set X∗ = {x∗,k}K
k=1, budget T
2: L1 ← ∅, U1 ← [n], H1 ← {}
3: for t = 1, . . . , Tdo
4: for all i ∈ Ut do
5: for all x∗,k ∈ X∗ do
6: for j = 1, 2, . . . , mdo
7: Sample Y (j)
t,i ∼ p(· |xi, Ht)
8: Ht,i,j ← Ht ∪ {(xi, Y(j)
t,i )}
9: Sample ˜Y (j,1)
t,i,k , ˜Y (j,2)
t,i,k ∼ p(· |x∗,k, Ht,i,j)
10: end for
11: end for
12: end for
13: It ← arg min
i∈Ut
max
k∈[K]
1
m
mX
j=1

˜Y (j,1)
t,i,k − ˜Y (j,2)
t,i,k
2
14: Xt ← xIt ∈ Xexamples
15: Observe label Yt of example Xt
16: Lt+1 ← Lt ∪ {It}, Ut+1 ← Ut \ {It}
17: Ht+1 ← Ht ∪ {(Xt, Yt)}
18: end for
19: Output: Sample Y∗,k ∼ p(· |x∗,k, HT+1) for k ∈ [K]
3.2 Simulation-Based Algorithm
While GO reduces uncertainty in label predictions, it has a major limitation. The chosen example
Xt at round t is not affected by observed labels (Yℓ)ℓ∈[t−1]. This is because (1) does not depend on
(Yℓ)ℓ∈[t−1]. While this is a property of linear models, it is undesirable in non-linear models, such as
LLMs. To address this limitation, we propose a new algorithm that simulates the impact of labeling
examples on the uncertainty of predicted labels. We call it Simulation-Based Active Learning and
abbreviated it as SAL. The pseudocode of SAL is provided in Algorithm 2.
The key idea in SAL is to replace the closed-form formula in (1) by a simulation. We detail the
algorithm next. Fix round t, history Ht, and candidate example xi. To estimate the impact of labeling
xi, we simulate its labels m times. For each simulation j ∈ [m], we sample Y (j)
t,i from the conditional
distribution p(· |xi, Ht) using the LLM. Then we extend the history Ht by xi and its simulated label
Y (j)
t,i , Ht,i,j = Ht ∪ {(xi, Y(j)
t,i )}. This process results in m copies of augmented histories, each
reflecting a potential outcome of labeling of xi. Finally, we take two independent samples for each
j ∈ [m] as ˜Y (j,1)
t,i,k , ˜Y (j,2)
t,i,k ∼ p(· |x∗,k, Ht,i,j). The maximum uncertainty over test examples after
labeling xi is estimated as
max
k∈[K]
1
m
mX
j=1

˜Y (j,1)
t,i,k − ˜Y (j,2)
t,i,k
2
. (2)
The training example with the lowest value is chosen and we denote its index by It. Then xIt and its
observed label Yt are added to the history to get Ht+1 for the next iteration t + 1.
Next we justify SAL. Consider the same setting as in Section 3.1. Given a label Y (j)
t,i for example xi,
the posterior distribution of θ∗ | Ht,i,j is N(bθt,i,j, bΣt,i), where bΣt,i = (bΣ−1
t + σ−2xix⊤
i )−1 is the
simulated posterior covariance of θ∗ and
bθt,i,j = bΣt,i
 
bΣ−1
0 θ0 + σ−2
 t−1X
ℓ=1
XℓYℓ + xiY (j)
t,i
!!
is the posterior mean. By design, ˜Y (j,1)
t,i,k and ˜Y (j,2)
t,i,k are independent samples from N(x⊤
∗,kθ∗, σ2),
where θ∗ ∼ N(bθt,i,j, bΣt,i). Therefore, ˜Y (j,1)
t,i,k − ˜Y (j,2)
t,i,k ∼ N(0, 2(x⊤
∗,k bΣt,ix∗,k +σ2)). By definition,
5
( ˜Y (j,1)
t,i,k − ˜Y (j,2)
t,i,k )2 is a single sample estimate of 2(x⊤
∗,k bΣt,ix∗,k + σ2) and the sum in (2) estimates
this quantity from m samples. Note that this estimate is proportional to x⊤
∗,k bΣt,ix∗,k that appears
in the G-optimal design objective in (1). Therefore, in linear models, SAL can be viewed as an
inefficient implementation of GO. This inefficiency stems from the need to simulate the LLM.
The time complexity of SAL is O(nKmT ). This is because it searches for the best example out of at
most n in T rounds for each test example k ∈ [K]. The evaluation of impact on each test example
requires 2m LLM queries.
Vector labels: GO and SAL are easy to extend to vector labels, dy > 1. GO does not depend on
labels at all. The only modification in SAL is that (2) is replaced with maxk∈[K]
1
m
Pm
j=1 ∥˜Y (j,1)
t,i,k −
˜Y (j,2)
t,i,k )∥2
2. This is the sum of the posterior variances of the labels over all dimensions.
4 Analysis
In this section, we analyze GO and SAL. The analysis is under the assumption that the labels are
scalar and hence, our objective simplifies to minimizing maxk∈[K] var[Y∗,k | x∗,k, HT+1]. The
analysis is organized as follows. First, we prove that our objective is decreasing in history but not
supermodular, which precludes a straightforward analysis. This property of our objective function is
proved in Appendix B.1 and Appendix B.2. Second, we analyze GO using the closed form of the
posterior covariance bΣt. Finally, we prove the equivalence of GO and SAL, and thereby provide
guarantees for SAL. All analyses are under the assumption of a linear model with Gaussian noise.
These proofs are in Appendix B.3 and Appendix B.4.
4.1 Analysis of GO
To address challenge posed due to f not being a supermodular (Lemma 5), we leverage the properties
of the rank-1 updates in GO. The proof is under the assumption that at round t, the training examples
can be partitioned as X = Sk ∪ Sk. The set Sk represents examples that are close to x∗,k. The
set Sk is convex such that for a αk ≥ 0 we have x⊤y ≥ αk for all x, y ∈ Sk. In essence, αk
governs the minimum level of similarity required for examples within SK to be considered similar
to the test example x∗,k. This is achieved by setting a lower bound on the inner product between
any two examples in the set. The set Sk represents examples that are not close to x∗,k. It is defined
βk ≥ 0 such that x⊤y ≤ βk for all x ∈ Sk and y ∈ Sk. In contrast to αk, βk limits the maximum
similarity any example in Sk can have with examples outside this set. Define αmin = mink αk, and
βmax = βmax. Define the set S = ∩K
k=1Sk as the set of all examples that are close to all {x∗,k}K
k=1
and S = ∪K
k=1Sk as the set of all examples that are not close to all {x∗,k}K
k=1. Assume S ̸= {∅} and
|S| > T. With this in hand, we prove the following claim.
Theorem 1. Let αmin, βmax ≥ 0 be set such that βmax ≥ 1 − α2
min and T ≤ α2
min
(βmax+
√
2)βmaxd . Then
for any x∗,k we can show that x⊤
∗,k bΣT+1x∗,k ≤ 1
α2maxT+1 + (1 − α2
max) .
The proof is in Appendix B.3. It is a major departure from similar proofs in active learning with a
fixed budget [Tong and Koller, 2001, Hanneke et al., 2014, Azizi et al., 2022, Yang and Tan, 2022]
in three aspects. First, we analyze the discrete allocation problem in (3) instead of its continuous
optimal-design relaxation [Pukelsheim, 2006]. Second, any unlabeled example inX is labeled at most
once. Finally, (3) is asymmetric in the sense that we optimize the uncertainty of a single example
x∗ over a larger set. To make the analysis manageable, we impose structure on X. The claim in
Theorem 1 holds for any T if βmax = 1/(4dn) and αmin = √1 − βmax. In this case, αmin is close
to 1, and we get a near-optimal O(1/T) decrease in posterior variance.
4.2 Analysis of SAL
For a sufficiently large sample size m in SAL, we can establish the following equivalence of SAL
and GO.
Theorem 2. Fix a failure probability δ ∈ (0, 1). Define σ2
t,i,k=E[ 1
m
Pm
j=1(eY (j,1)
t,i,k − eY (j,2)
t,i,k )2] =
2x⊤
∗,k bΣt,ix∗,k + σ2, and define σ2
t,i,max = maxk∈[K] σ2
t,i,k. Then for any t ∈ [T] and i ∈ Ut, we
6
have that
σ2
t,i,max
"
1 − 2
r
log(1/δ)
m
#
≤ max
k∈[K]
1
m
mX
j=1

eY (j,1)
t,i,k − eY (j,2)
t,i,k
2
≤ σ2
t,i,max
"
1 + 2
r
log(1/δ)
m + 2 log(1/δ)
m
#
.
Moreover, form ≥ 8 log(1/δ) we have that
2 max
k
x⊤
∗,k bΣt,ix∗,k + σ2
2 ≤ max
k
1
m
mX
j=1

eY (j,1)
t,i,k − eY (j,2)
t,i,k
2
≤ 5 max
k
x⊤
∗,k bΣt,ix∗,k + 5σ2
2 .
These claims hold with probability at least 1 − δ.
The claim is proved in Appendix B.4. The key idea in the proof is that (2) multiplied by
m/[2(x⊤
∗,k bΣt,ix∗,k + σ2)] is a chi-squared random variable with m degrees of freedom. Then
we use concentration inequalities of Laurent and Massart [2000] to get a high-probability confidence
interval on distance to the mean m, which in turn allows us to relate the actual variance to its
empirical estimate. Theorem 2 shows that SAL is equivalent to GO for a sufficiently large sample
size m. Theorem 1 can be then adapted to SAL as follows. The only change is in condition on T,
which changes to T ≤ α2

β +
√
2O

1−
√
1/m
1+
√
1/m

βd + O(
p
1/m) . Therefore, SAL attains a
near-optimal O(1/T) decrease in posterior variance as m → ∞.
5 Experiments
We evaluate GO and SAL on a variety of prediction tasks. These tasks cover both classification and
regression, including natural language features, and help us to evaluate the capabilities of GO and
SAL to choose few-shot examples for active in-context prompt design. We also demonstrate that
GO and SAL can be used for general pattern recognition. Detailed descriptions of all datasets are in
Appendix C.1. We describe the prompts in detail in Appendix D.
5.1 Experimental Setup
We use Mistral-7B [Jiang et al., 2023], Vicuna-13B [Chiang et al., 2023], and Falcon-40B [Penedo
et al., 2023] as the LLMs and design prompts following Dinh et al. [2022] and Suzgun et al. [2022].
To investigate the impact of LLM model size on performance, we experiment with these three models
of varying sizes: 7B, 13B, and 40B. Interestingly, we observe that the smaller models (Mistral-7B and
Vicuna-13B) perform very poorly on certain tasks. Examples of the prompts are given in Appendix D.
Each experiment is averaged over 10 trials. At the beginning of each trial, we randomly select
K = 20 test examples. We describe in detail how the training set and n are chosen for each dataset
in Appendix C.
Each run is a simulation that proceeds as follows. In round t, each method selects a training example
to label Xt and then observes the true label Yt. All past interactions Ht = {(Xℓ, Yℓ)}ℓ∈[t−1] along
with the test examples x∗,k are used to craft a prompt for the LLM. The performance at round t is
evaluated by the error L(t) = 1
K
PK
k=1 L(Y∗,k, ˜Y∗,k,t), where Y∗,k is the true label of test example
x∗,k, ˜Y∗,k,t is its LLM predicted label in round t, and L(y∗, y) is a task-specific error function. For
classification tasks, we choose L(y∗, y) = I{y∗ = y} and call L(t) a misclassification error. For
regression tasks, we choose L(y∗, y) = (y∗ − y)2 and call L(t) the MSE. For pattern recognition
tasks, where Y∗,k and ˜Y∗,k,t are either vectors or matrices, we choose let L(y∗, y) = I{y∗ = y} and
L(t) represents 0-1 error.
We posit that GO and SAL perform well because they both reduce the uncertainty of test examples
based on the right notion of similarity. To show this, we compare to baselines that reduce uncertainty
uniformly (like Uniform), or reduce uncertainty informatively (Least or Max-Entropy), or only select
examples with similar features to test examples (Greedy-NN). As shown in our extensive experiments,
these baselines fail to match the capabilities of GO and SAL to select informative examples in the
majority of the tasks. The following methods are compared in our experiments:
(1) Uniform: The example Xt in round t is sampled uniformly at random from the unlabeled set Ut.
Uniform is a pure exploration algorithm that does not take into account the similarity to test examples
and variance reduction. We chose it as a baseline because it tends to work well in practice. Therefore,
7
Datasets Uniform Greedy-NN Least Max-EntropyGO (ours) SAL (ours)iris 0.41±0.11 0.60±0.13 0.64±0.15 0.72±0.17 0.38±0.14 0.34±0.14M banknote 0.75±0.10 0.58±0.04 0.59±0.02 0.73±0.16 0.77±0.07 0.75±0.15balance-scale0.61±0.13 0.69±0.22 0.55±0.25 0.57±0.14 0.48±0.09 0.72±0.04thyroid-new0.44±0.12 0.70±0.08 0.74±0.12 0.57±0.08 0.55±0.06 0.63±0.14iris 0.22±0.24 0.60±0.37 0.60±0.49 0.40±0.20 0.20±0.24 0.20±0.24V banknote 0.40±0.37 0.80±0.24 0.50±0.32 0.50±0.32 0.50±0.32 0.10±0.20balance-scale0.60±0.20 0.60±0.37 0.50±0.32 0.80±0.24 0.30±0.24 0.50±0.00thyroid-new0.52±0.45 1.00±0.00 0.70±0.24 0.50±0.00 0.60±0.20 0.50±0.32iris 0.20±0.06 0.62±0.14 0.70±0.20 0.65±0.18 0.42±0.10 0.33±0.23F banknote 0.45±0.23 0.53±0.25 0.60±0.12 0.42±0.17 0.45±0.06 0.45±0.1balance-scale0.70±0.28 0.68±0.13 0.85±0.12 0.62±0.08 0.47±0.24 0.45±0.13thyroid-new0.55±0.29 0.57±0.20 0.75±0.19 0.65±0.15 0.55±0.23 0.53±0.12
Table 1: Misclassification error in classification datasets using Mistral-7B (M), Vicuna-13B (V), and
Falcon-40B (F) on K = 20 test examples at the end of budget T = 5.
Datasets Uniform Greedy-NN Least Max-Entropy GO (ours) SAL (ours)machine(e+04)11.4±3.34 10.5±2.44 14.3±3.39 11.0±1.93 10.5±3.74 10.6±3.84M fifa(e-04) 1.40±.216 3.72±1.12 1.18±.53 4.15±1.11 .999±.404 .68±.26machine(e+04)5.59±1.35 5.04±.851 7.95±1.69 4.98±1.06 5.66±1.54 4.80±1.46V fifa(e+03) 5.90±1.59 4.72±.931 5.11±1.12 6.76±1.69 1.44±.258 2.59±.742machine(e+03)1.16±1.22 4.28±2.30 2.15±1.08 3.50±2.45e+ 03 .32±.209 2.96±1.56F fifa(e+01) 7.78±3.85 6.95±2.93 12.4±12.3 26.3±37.6 7.90±4.64 4.61±4.35
Table 2: MSE in regression datasets using Mistral-7B (M), Vicuna-13B (V), and Falcon-40B (F) on
K = 20 test examples at the end of budget T = 5.
it is used frequently in active learning and prompt tuning papers [Zhang et al., 2022b, Diao et al.,
2023].
(2) Greedy-NN: The example Xt in round t is chosen to align the most with all test examples x∗,k
such that It ← arg maxi∈Ut maxk∈[K] x⊤
∗,kxi. This baseline shows that our information gathering
rule in (1) goes beyond pure feature similarity. This baseline is similar to the automatic exemplar
construction method by clustering by Zhang et al. [2022b].
(3) Least: This is similar to the disagreement-based method of Diao et al. [2023]. The disagreement
score of the example i ∈ Ut is calculated as si = PK
k=1 Ytik where Ytik ∼ p (· |x∗,k, xi) is the
number of unique answers by for test example x∗,k using only xi as the in-context example by the
LLM. Then the example selected at round t is It ← arg maxi∈Ut si. This is the unlabeled example
where the LLM, disagrees the most for all test examples and is least confident. We compare against
this baseline to show that our information gathering rule in (1) goes beyond just uncertainty sampling
but also takes into account the diversity of training examples when choosing to label the next example.
(4) Max-Entropy: This is the uncertainty-based maximum entropy method of Zhang et al.
[2022a], Diao et al. [2023]. At round t the example with the highest entropy is selected as
It ← arg maxi∈Ut −PK
k=1 ¯Ytik ln ¯Ytik where ¯Ytik ∼ p (· |x∗,k, xi) is the frequency of a pre-
dicted answer among all predictions for the test example x∗,k using xi as the in-context example by
the LLM. A larger entropy denotes greater uncertainty and therefore, an unlabeled example with the
largest entropy will be selected. Again we compare against this uncertainty-based baseline to show
that our information gathering rule in (1) goes beyond just uncertainty sampling but also considers
the diversity of training examples when choosing to label the next example.
(5) GO (ours): This is Algorithm 1 where X are the original feature vectors.
(6) GO-Inst (ours): This is Algorithm 1 where the original feature vectors are used in the prompt
but X are their 768-dimensional Instructor embeddings [Su et al., 2022]. We use this for natural
language classification tasks.
(7) SAL (ours): This is Algorithm 2 where X are the original feature vectors. To implement SAL
efficiently, we combine it with GO as a preprocessing step. Specifically, in round t, GO first chooses
5 most informative examples from Ut and then we apply SAL. We use m = 1 in all experiments.
We use these approximations because SAL is computationally expensive (Section 3.2). Similarly to
GO-Inst, we use Instructor embeddings for natural language classification tasks.
All used datasets and experimental setups are described in Appendix C. This section only summarizes
the main results.
8
Task Uniform Greedy-NN Least Max-EntropyGO (ours) SAL (ours)
Arc-1 0.45±0.50 0.45±0.50 0.90±0.30 0.60±0.49 0.30±0.46 0.15±0.36
Arc-2 0.80±0.40 1.00±0.00 0.80±0.40 0.80±0.40 0.80±0.40 0.01±0.01
PCFG-1 0.60±0.49 1.00±0.00 1.00±0.00 1.00±0.01 0.20±0.40 0.02±0.01
PCFG-2 0.20±0.40 1.00±0.00 0.20±0.40 1.00±0.00 0.20±0.40 0.14±0.40
Table 3: 0-1 error using Falcon-40B on K = 20 test examples at the end of budget T = 5. ARC-1
is the expansion-contraction task, ARC-2 is the rotation task, PCFG-1 is the add-subtract task, and
PCFG-2 is the repeat experiment task. Mistral-7B and Vicuna-13B perform very poorly on these
tasks and thus are omitted.Datasets Uniform Greedy-NN Least Max-EntropyGO (ours) SAL (ours)
movie 0.32±0.17 0.90±0.06 0.87±0.09 0.55±0.18 0.27±0.10 0.49±0.11
M entity 0.69±0.19 0.86±0.06 0.87±0.09 0.59±0.15 0.65±0.18 0.39±0.19
theme 0.74±0.05 0.74±0.09 0.82±0.16 0.68±0.09 0.80±0.09 0.81±0.08
movie 0.10±0.20 0.70±0.24 0.90±0.20 0.30±0.24 0.02±0.01 0.10±0.20
V entity 0.20±0.24 0.90±0.20 0.70±0.24 0.60±0.20 0.10±0.20 0.10±0.20
theme 0.90±0.20 0.70±0.40 1.00±0.00 0.70±0.24 0.60±0.37 0.80±0.40
movie 0.55±0.06 0.62±0.18 0.78±0.17 0.53±0.22 0.38±0.18 0.47±0.17
F entity 0.55±0.23 0.62±0.08 0.75±0.14 0.65±0.24 0.47±0.23 0.42±0.24
theme 0.68±0.20 0.70±0.13 0.85±0.05 0.85±0.09 0.53±0.18 0.55±0.19
Table 4: Misclassification error in natural language classification tasks using Mistral-7B (M), Vicuna-
13B (V), and Falcon-40B (F) on K = 20 test examples at the end of budget T = 5.
Standard classification and regression tasks. We use 4 classification and 2 regression datasets
from UCI and OpenML (Appendix C.1). We set T = 5 to simulate the realistic scenario when the
test queries provided by the user need to be inferred quickly. For classification tasks, K = 20 test
examples are chosen among the different classes of the dataset. We describe in detail how the training
set and n are chosen for each dataset in Appendix C. For regression tasks, K = 20 random test
examples are chosen. Our results on classification tasks are reported in Table 1 and on regression
tasks in Table 2. We observe that GO and SAL are the best-performing methods in the majority of
the datasets. Note that there is no single baseline that consistently outperforms them.
General pattern recognition. We experiment with 4 tasks: ARC expansion and contraction, ARC
rotation, PCFG Add-Subtract, and PCFG Repeat. Both inputs and outputs in these tasks are vectors
or matrices. We describe examples of ARC and PCFG tasks in detail in Appendix C.1. Each dataset
comprises examples of two patterns: expansion and contraction, clockwise and counter-clockwise
rotation, add and subtract, repeat first and second digits. In each trial, we choose K = 20 different
test examples equally from two patterns and set T = 5. Our results are reported in Table 3. In all
tasks, GO and SAL are the best-performing methods. SAL outperforms GO in ARC [Alford, 2021,
Mirchandani et al., 2023] and PCFG [Hupkes et al., 2020] consistently.
Natural language classification tasks (NLC). We show that GO and SAL work well on general
NLP tasks where no explicit numerical features are available. We create three synthetic datasets
based on the following tasks: (i) movie-names: predicting a genre from a movie name (e.g.,
romance, horror), (ii) movie-theme: predicting a common theme for a pair of movie names (e.g.,
coming-of-age, sci-fi), and (iii) entity-names: predicting an entity’s type from its name (e.g.,
celebrity, mountain, river). Each dataset comprises 5 classes. Further details regarding the additional
datasets are provided in Appendix C. In each trial, K = 20 test examples were randomly chosen
across the five classes. The feature vectors in GO and SAL are Instructor embeddings of the original
text features. Our results are reported in Table 4. We observe again that GO and SAL are the
best-performing methods in the majority of the datasets. There is no single baseline that consistently
outperforms them. This shows that the optimal design-based approach of GO and SAL correctly
balances uncertainty and diversity-based sampling.
6 Conclusions
In this paper, we studied the framework of active in-context prompt design (AIPD) that uses optimal
design to systematically choose the most informative unlabeled examples to label for a set of test
examples. These informative labeled examples are then used to minimize the prediction error of
the LLM for all the test examples. To our knowledge, this is the first paper that studies optimal
design for adaptive prompt design. Inspired by the linear model, we proposed an algorithm GO
that strategically chooses the most informative examples that minimize the variance of the posterior
covariance for any test example from the test set. We proposed a second algorithm SAL that uses
simulations to estimate the impact of how unlabeled examples reduce LLMuncertainty for all test
examples. It then chooses to label examples that maximally reduce the uncertainty of the LLM for all
9
test examples from the test set. We theoretically analyze GO and SAL and show their equivalence in
linear models. We show that both algorithms guarantee information gain at each iteration. Finally,
we show empirically that, when used with LLMs like Mistral-7B, Vicuna-13B, and Falcon-40B, both
GO and SAL result in better prediction accuracy than other baselines [Zhang et al., 2022a,b, Diao
et al., 2023] on tasks like classification, regression, ARC, PCFG, and natural language generation.
Our research opens up exciting new directions for future work such as extending AIPD framework
beyond text to enable informative example selection for tasks involving images, videos, or other
modalities using multi-modal LLMs [Yin et al., 2023]. Additionally, the integration of active learning
with diffusion models, a powerful class of generative models, presents promising directions for future
research [Ho et al., 2020].
References
Simon Alford. A Neurosymbolic Approach to Abstraction and Reasoning. PhD thesis, Massachusetts
Institute of Technology, 2021.
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical
report, Stanford, 2006.
Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone fishing: Neural active
learning with fisher embeddings. Advances in Neural Information Processing Systems, 34, 2021.
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds.arXiv preprint arXiv:1906.03671,
2019.
Mohammad Javad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh. Fixed-budget best-arm
identification in structured bandits. In Proceedings of the 31st International Joint Conference on
Artificial Intelligence, 2022.
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637,
2023.
Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. Journal of
Computer and System Sciences, 75(1):78–89, 2009.
Gantavya Bhatt, Yifang Chen, Arnav M Das, Jifan Zhang, Sang T Truong, Stephen Mussmann,
Yinglun Zhu, Jeffrey Bilmes, Simon S Du, Kevin Jamieson, et al. An experimental design
framework for label-efficient supervised finetuning of large language models. arXiv preprint
arXiv:2401.06692, 2024.
Christopher Bishop. Pattern Recognition and Machine Learning. Springer, New York, NY , 2006.
Benjamin Bowman, Alessandro Achille, Luca Zancato, Matthew Trager, Pramuditha Perera, Gio-
vanni Paolini, and Stefano Soatto. a-la-carte prompt tuning (apt): Combining distinct data via
composable prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 14984–14993, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020a.
Tom Brown et al. Language models are few-shot learners. In Advances in Neural Information
Processing Systems 33, 2020b.
BuoyHealth. BuoyHealth. https://www.buoyhealth.com/.
Loredana Caruccio, Stefano Cirillo, Giuseppe Polese, Giandomenico Solimando, Shanmugam Sun-
daramurthy, and Genoveffa Tortora. Can chatgpt provide intelligent diagnoses? a comparative
study between predictive models and chatgpt to define a new medical diagnostic bot. Expert
Systems with Applications, 235:121186, 2024. ISSN 0957-4174. doi: https://doi.org/10.1016/j.
eswa.2023.121186. URL https://www.sciencedirect.com/science/article/pii/S0957417423016883.
10
Kamalika Chaudhuri, Sham M Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence
rates of active learning for maximum likelihood estimation. In Advances in Neural Information
Processing Systems, pages 1090–1098, 2015.
Yifang Chen, Yingbing Huang, Simon S Du, Kevin Jamieson, and Guanya Shi. Active representation
learning for general task space with applications in robotics. arXiv preprint arXiv:2306.08942,
2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin
Rostamizadeh, and Sanjiv Kumar. Batch active learning at scale. Advances in Neural Information
Processing Systems, 34, 2021.
Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural Information
Processing Systems 17, pages 337–344, 2005.
Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm.
Advances in neural information processing systems, 20, 2007.
Zhijie Deng, Hongcheng Gao, Yibo Miao, and Hao Zhang. Efficient detection of llm-generated texts
with a bayesian surrogate model. arXiv preprint arXiv:2305.16617, 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-of-thought
for large language models. arXiv preprint arXiv:2302.12246, 2023.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained
language models. Nature Machine Intelligence, 5(3):220–235, 2023.
Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong
Sohn, Dimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for
non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:
11763–11784, 2022.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.
Yonatan Dukler, Alessandro Achille, Giovanni Paolini, Avinash Ravichandran, Marzia Polito, and
Stefano Soatto. Diva: Dataset derivative of a learning task. arXiv preprint arXiv:2111.09785,
2021.
Valerii Vadimovich Fedorov.Theory of optimal experiments. Elsevier, 2013.
Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying
task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34:
27503–27516, 2021.
Xavier Fontaine, Pierre Perrault, Michal Valko, and Vianney Perchet. Online a-optimal design and
active linear regression. In International Conference on Machine Learning, pages 3374–3383.
PMLR, 2021.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data.
In Doina Precup and Yee Whye Teh, editors,Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1183–1192.
PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/gal17a.html.
Tianshi Gao and Daphne Koller. Active classification based on value of classifier.Advances in neural
information processing systems, 24, 2011.
11
Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint
arXiv:1711.00941, 2017.
Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in
Machine Learning, 7(2-3):131–309, 2014.
Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fa-
had Shahbaz Khan, and Salman Khan. Align your prompts: Test-time prompting with distribution
alignment for zero-shot generalization. arXiv preprint arXiv:2311.01459, 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840–6851, 2020.
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How
do neural networks generalise? Journal of Artificial Intelligence Research, 67:757–795, 2020.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic
pool-based active classification. In International Conference on Machine Learning, pages 5334–
5344. PMLR, 2021.
Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian Journal of
Mathematics, 12:363–366, 1960.
Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch
acquisition for deep bayesian active learning. Advances in neural information processing systems,
32, 2019.
Jan Kremer, Kim Steenstrup Pedersen, and Christian Igel. Active learning with support vector
machines. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(4):313–326,
2014.
Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun Peng. Active instruction tun-
ing: Improving cross-task generalization by training on prompt sensitive tasks. arXiv preprint
arXiv:2311.00288, 2023.
Tomoyuki Kuroiwa, Aida Sarcon, Takuya Ibara, Eriku Yamada, Akiko Yamamoto, Kazuya
Tsukamoto, and Koji Fujita. The potential of chatgpt as a self-diagnostic tool in common orthope-
dic diseases: Exploratory study. J Med Internet Res, 25:e47621, Sep 2023. ISSN 1438-8871. doi:
10.2196/47621. URL https://www.jmir.org/2023/1/e47621.
Anusha Lalitha Lalitha, Kousha Kalantari, Yifei Ma, Anoop Deoras, and Branislav Kveton. Fixed-
budget best-arm identification with heterogeneous reward variances. In Uncertainty in Artificial
Intelligence, pages 1164–1173. PMLR, 2023.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection.
Annals of statistics, pages 1302–1338, 2000.
Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma
Brunskill. Supervised pretraining can learn in-context reinforcement learning. arXiv preprint
arXiv:2306.14892, 2023.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021.
David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data.
In Acm Sigir Forum, volume 29, pages 13–19. ACM New York, NY , USA, 1995.
12
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. ACM Computing Surveys, 55(9):1–35, 2023a.
Yajing Liu, Yuning Lu, Hao Liu, Yaozu An, Zhuoran Xu, Zhuokun Yao, Baofeng Zhang, Zhiwei
Xiong, and Chenguang Gui. Hierarchical prompt learning for multi-task learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10888–10898,
2023b.
livehealthily. livehealthily. https://www.livehealthily.com/.
Katerina Margatina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. Active learning principles
for in-context learning with large language models. arXiv preprint arXiv:2305.14264, 2023.
Kolby Nottingham Markelle Kelly, Rachel Longjohn. The uci machine learning repository, 1988.
URL https://archive.ics.uci.edu.
Blake Mason, Romain Camilleri, Subhojyoti Mukherjee, Kevin Jamieson, Robert Nowak, and Lalit
Jain. Nearly optimal algorithms for level set estimation. arXiv preprint arXiv:2111.01768, 2021.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke
Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv
preprint arXiv:2202.12837, 2022.
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas,
Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines.
arXiv preprint arXiv:2307.04721, 2023.
Subhojyoti Mukherjee, Ardhendu S Tripathy, and Robert Nowak. Chernoff sampling for active
testing and extension to active regression. In International Conference on Artificial Intelligence
and Statistics, pages 7384–7432. PMLR, 2022.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing
submodular set functions - I. Mathematical Programming, 14(1):265–294, 1978.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb
dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv
preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.
Pramuditha Perera, Matthew Trager, Luca Zancato, Alessandro Achille, and Stefano Soatto. Prompt
algebra for task composition. arXiv preprint arXiv:2306.00310, 2023.
Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, and Liat
Ein-Dor. Active learning for natural language generation. arXiv preprint arXiv:2305.15040, 2023.
Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen,
and Xin Wang. A survey of deep active learning. ACM Computing Surveys (CSUR), 54(9):1–40,
2021.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017.
Burr Settles. Active learning literature survey. 2009.
Burr Settles. From theories to queries: Active learning in practice. InActive learning and experimental
design workshop in conjunction with AISTATS 2010, pages 1–18. JMLR Workshop and Conference
Proceedings, 2011.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615, 2022.
13
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih,
Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned
text embeddings. 2022. URL https://arxiv.org/abs/2212.09741.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks
and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.
Simon Tong and Daphne Koller. Support vector machine active learning with applications to text
classification. Journal of machine learning research, 2(Nov):45–66, 2001.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked science in
machine learning. SIGKDD Explorations, 15(2):49–60, 2013. doi: 10.1145/2641190.2641198.
URL http://doi.acm.org/10.1145/2641190.2641198.
Roman Vershynin. High-dimensional probability. University of California, Irvine, 2020.
Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. Reward-
free rl is no harder than reward-aware rl in linear markov decision processes. In International
Conference on Machine Learning, pages 22430–22456. PMLR, 2022.
Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International
joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014.
Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic
models: Explaining and finding good demonstrations for in-context learning. arXiv preprint
arXiv:2301.11916, 2023a.
Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt
tuning. arXiv preprint arXiv:2305.18787, 2023b.
Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang
Wang, and Mingyuan Zhou. In-context learning unlocked for diffusion models. arXiv preprint
arXiv:2305.01115, 2023c.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652, 2021.
Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.
arXiv preprint arXiv:2302.03668, 2023.
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning.
arXiv preprint arXiv:2212.10375, 2022.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context
learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
Junwen Yang and Vincent Tan. Minimax optimal fixed-budget best arm identification in linear bandits.
In Advances in Neural Information Processing Systems 35, 2022.
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on
multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.
Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, JaeSung Park, Ximing Lu, Prithviraj
Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, et al. Multimodal knowledge
alignment with reinforcement learning. arXiv preprint arXiv:2205.12630, 2022.
14
Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers,
Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, et al. Fusing pre-trained language models
with multimodal prompts through reinforcement learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 10845–10856, 2023.
Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. Cold-start active learning through self-
supervised language modeling. arXiv preprint arXiv:2010.09535, 2020.
Xueying Zhan, Qingzhong Wang, Kuan-hao Huang, Haoyi Xiong, Dejing Dou, and Antoni B Chan.
A comparative survey of deep active learning. arXiv preprint arXiv:2203.13450, 2022.
Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv
preprint arXiv:2211.04486, 2022a.
Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context
learning? arXiv preprint arXiv:2301.13670, 2023.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in
large language models. arXiv preprint arXiv:2210.03493, 2022b.
Broader Impact: We propose a framework for adaptive prompt design for Large Language Models
(LLMs). Our method does not address the fundamental ethical and societal challenges inherent to
these models. Specifically, it does not correct the biases or discriminatory outputs that may exist
in the LLMs. Additionally, our approach does not solve the issue of LLMs generating reasonable
but inaccurate information, known as hallucinations. Therefore, while our method improves the
functionality of LLMs, it is essential for users and developers to use these models responsibly and
remain aware of their limitations, particularly in terms of content accuracy and fairness.
15
A Related Work
We study the problem of choosing human demonstrations adaptively to get the desired output from
the LLM as quickly as statistically possible. We use active learning to choose them and then ask
a human to label them. Finally, the human demonstrations are fed as in-context input to the LLM
together with the main user query, to obtain the desired output. Thus the name is active transductive
inference. Prior works on prompt-tuning and transductive inference [Lester et al., 2021, Dong et al.,
2022, Zhang et al., 2022a, Min et al., 2022, Wu et al., 2022, Yu et al., 2022, Suzgun et al., 2022,
Liu et al., 2023a, Yu et al., 2023, Liu et al., 2023b] focus on hard prompt-tuning where the user
must carefully handcraft the prompt to get the desired output for tasks like movie recommendation,
disambiguation QA, navigation, etc. Such examples of carefully handcrafting hard prompts with
demonstrations can be found in Suzgun et al. [2022], Srivastava et al. [2022]. These papers also
show how failing to design such prompts with demonstrations can lead the LLM to predict wrong
outputs. Note that we adaptively design the prompt through carefully chosen demonstrations. In our
experiments, we show significant improvement over randomly chosen demonstrations.
The problem of active learning is also related to dataset augmentation [Dukler et al., 2021]. In this
work, the most informative unlabeled examples are chosen by optimizing the error of the model on
the validation set. The gradient of the validation set error with respect to the weights of unlabeled
examples has a closed form when the original model is linearized. The main difference in active
learning, including in our work, is that labeling all unlabeled examples would be costly. Therefore,
the labels are not available in advance and are queried adaptively. We also learn in context and do
not assume that the gradient information of the LLM is available. Prompt composition has also been
an active area of research. Bowman et al. [2023] proposed a-la-carte prompt tuning, where prompts
are tuned on individual datasets and composed at inference time to mimic the performance of the
model that would have been trained on the union of the corresponding datasets. This idea has been
further extended by Perera et al. [2023], where prompts for previously unseen tasks are obtained
by linearly combining prompts from known tasks. To do this, they use spectral decomposition and
project prompts from known tasks to a lower dimensional space. In our work, we do not tune prompts
or compose simpler models. We actively probe an LLM, treated as a black box without any extra side
information, to answer a test example as accurately as possible, with as little variance in the answer
as possible.
Recently there has been a lot of progress in prompt tuning (or aligning). Hassan et al. [2023]
studies prompt aligning for a single test sample to adapt to multi-modal test prompts at test time
by minimizing the feature distribution shift to the test domain. In contrast in this paper, we study
adapting prompts for many test samples without the feature distribution shift assumption. The Wang
et al. [2023a] trains a smaller LLMto select demonstrations for a larger LLM. However, we rely
on active learning to select the smallest number of informative prompts to be labeled by human
labelers. This avoids finetuning a smaller LLM for individual tasks. Wang et al. [2023c] studies
transductive inference for diffusion models for a different setting where given a pair of task-specific
example images, such as depth from/to image and scribble from/to image, and text guidance, the
model automatically understands the underlying task and performs the same task on a new query
image following the text guidance. However, in our setting, we do not explicitly encode any guidance
text. The Wen et al. [2023] proposes to mix the nearest neighbor method with gradient optimization to
select prompts during test time. Similarly, Zhang et al. [2023] proposes a nearest neighbor approach to
select in-context examples for computer vision tasks. We compare our approach against such nearest-
neighbor selection algorithms. Finally, Bai et al. [2023], Wang et al. [2023b] analyze transductive
inference theoretically to understand its universality, generalization capability, and limitations. In
contrast, we only do a theoretical analysis of AIPD to show it maximally reduces the estimated
variance of the answer to the user’s query. The Zhang et al. [2022b] study the chain-of-thought
prompting where they automatically select the prompt using a clustering-based approach.
There are some related works in the area of medical diagnosis chatbot examples that we shared in
the introduction. One notable study Caruccio et al. [2024], although not utilizing machine learning
techniques, provides valuable insights with its implementation of three hardcoded prompt designs
for accurate diagnosis. These designs, however, do not engage in active learning, as they lack the
capability to adapt based on user input. Similarly, Kuroiwa et al. [2023] gives more insights into
self-diagnostics of orthopedic diseases using ChatGPT. In contrast, web applications such as Buoy
Health BuoyHealth and Live Healthily livehealthily employ a more dynamic approach, actively
16
tailoring subsequent questions based on users’ responses. This aligns with active learning principles
but it is not clear what techniques they apply and is notably underexplored in academic literature,
indicating a potential area for further research. Our setting also goes beyond the single shot active
prompt tuning studied in Margatina et al. [2023]. Note that this work studies prompt tuning only
for one iteration, and does not take into account the historical context. So it has limited ability for
complex tasks like ARC [Alford, 2021] and PCFG [Hupkes et al., 2020] as well as handling vector
labels like GO and SAL.
Active Learning (AL): Recently, there has been a lot of focus on using deep AL to finetune LLMs.
All AL algorithms tend to balance uncertainty and diversity in the selection of unlabeled examples.
We briefly discuss them below and also highlight the main difference of these approaches with prompt
aligning with GO and SAL.
(1) Coreset: This is a pure diversity-based approach using a coreset selection. In every iteration, first,
the embedding of each unlabeled example is computed from the network’s penultimate layer, and
then unlabeled examples are selected using a greedy furthest-first traversal conditioned on all labeled
examples [Sener and Savarese, 2017, Geifman and El-Yaniv, 2017, Citovsky et al., 2021]. Observe
that in our setting we do not have access to the penultimate layer of the LLM.
(2) Least: This is an uncertainty-based active learning algorithm. Here, the uncertainty score of an
unlabeled example is its predicted class probability. At every iteration, this algorithm then samples
unlabeled examples with the smallest uncertainty scores [Settles, 2009, 2011, Wang and Shang,
2014].
(3) Margin: This is also an uncertainty-based active learning algorithm [Tong and Koller, 2001,
Balcan et al., 2009, Settles, 2009]. At every iteration t it selects unlabeled examples that are sorted
according to their multiclass margin score and then selects unlabeled examples that are the hardest to
discriminate and can be thought of as examples closest to their class margin. However, in our setting,
we do not have any information on the hypothesis space of the LLM and hence cannot implement
such a baseline.
(4) Entropy: This is also an uncertainty-based active learning algorithm Wang and Shang [2014],
Kremer et al. [2014], Diao et al. [2023]. At every iteration t it selects unlabeled examples according
to the entropy of the example’s predictive class probability distribution. We show that Greedy-NN-
max-mean is outperformed significantly by GO and SAL in the prediction, pcfg, or arc tasks.
(5) Badge: This is an algorithm that combines both uncertainty and diversity sampling [Ash et al.,
2019, 2021]. For each unlabeled example x its gradient embedding gx is computed with respect to
the parameters of the model’s penultimate layer. Finally, Badge chooses a batch of samples to sample
by applying k-Means++ [Arthur and Vassilvitskii, 2006] on the gradient embeddings. Again recall
that we cannot implement such a baseline as we do not have access to the LLMs last layer.
(6) Badge-KM: This algorithm is similar to Badge but in the final step instead of k-Means++ it uses
k-Means on the gradient embeddings. In Yuan et al. [2020] it is observed that applying k-Means on
the embeddings results in an increase in accuracy over baselines in some datasets. Further Yuan et al.
[2020] observed from the t-SNE plots that k-Means select centers that are further apart compared to
the ones chosen by k-Means++ which leads to more diverse sampling in batches.
(7) Bald: Bayesian Active Learning by Disagreements [Kirsch et al., 2019, Gal et al., 2017] chooses
unlabeled examples that are expected to maximize the information gained from the model parameters
θt, i.e. the mutual information between predictions and model posterior.
A more comprehensive survey on how AL is used for finetuning deep models can be found in Ren
et al. [2021], Zhan et al. [2022]. The Bhatt et al. [2024] study how experimental design can be used to
select prompts for finetuning a pre-trained LLM. Some recent works have also focused on selecting
unlabeled examples only within a task Wei et al. [2021], Chen et al. [2023], Fifty et al. [2021].
However, these works are geared towards selecting prompts within a task for finetuning, whereas
we focus on adaptive prompt design using experimental design. The work of Perlitz et al. [2023]
also uses AL for finetuning prompts for LLMsto improve label efficiency. The Kung et al. [2023]
proposes an AL framework for instruction tuning. However, their approach again focuses on selecting
unlabeled examples inside each task and discriminating one task from another. However, they make
the simplifying assumption that all unlabeled examples inside the tasks are equally informative which
may inhibit the quality of the selected subset.
17
B Proofs
This section contains the properties of our objective and proofs of our main claims.
B.1 Properties of our objective
By the total variance decomposition for a linear model with observation variance σ2, we get
max
k∈[K]
var[Y∗,k | x∗,k, HT+1] = max
k∈[K]
var[E[Y∗,k | x∗,k, θ∗, HT+1] | x∗,k, HT+1]
+ max
k∈[K]
E[var[Y∗,k | x∗,k, θ∗, HT+1] | x∗,k, HT+1]
= max
k∈[K]
σ2x⊤
∗,k bΣ−1
t x∗,k + σ2 .
Therefore, the variance minimization of maxk∈[K] Y∗,k | x∗,k, HT+1 is a combinatorial optimization
problem. It can be formulated as follows
S∗ = arg minS⊆[n],|S|=T f(S) , (3)
where the function f(S) = max k∈[K] x⊤
∗,k
 
Σ−1
0 + σ−2 P
i∈S xix⊤
i
−1
x∗,k represents the un-
certainty associated with any subset S, and S∗ denotes the optimal subset of size T. If f(S) was
monotone and supermodular inS, we would have(1−1/e)-optimality guarantees for GO [Nemhauser
et al., 1978]. We start with proving the monotonicity of f(S).
Lemma 3. Function f(S) is monotonically decreasing.
The claim is proved in Appendix B.2. Now we state the definition of a supermodular function and
show that f(S) is not supermodular.
Definition 4 (Supermodular function). Let [n] be a set of n elements. A function f : 2[n] → R is
supermodular if it satisfies the property of diminishing marginal returns. For any S1 ⊆ S2 ⊆ [n] and
x /∈ S2, we have f(S1 ∪ {x}) − f(S1) ≤ f(S2 ∪ {x}) − f(S2).
Lemma 5. The function f(S) is not supermodular.
Proof. Take Σ0 = Id, fix K = 1 , x1 = (1 /
√
2, 1/
√
2), x2 = (0 , 1), and x∗ = (1 , 0). Then
f({2}) − f(∅) = 0 < 0.035714 ≈ f({1, 2}) − f({1}).
B.2 Proof of Lemma 3
Recall that the objective function isf(S) = maxk∈[K] x⊤
∗,k
 
Σ−1
0 + σ−2 P
i∈S xix⊤
i
−1
x∗,k. With-
out loss of generality, let σ2 = 1. Fix any j ∈ [n] \ S and let S+ = S ∪ {j}. Then the objective
value at S+ can be written as
f(S+) = max
k∈[K]
x⊤
∗,k

Σ−1
0 +
X
i∈S+
xix⊤
i


−1
x∗,k = max
k∈[K]
x⊤
∗,k
 
A + xjx⊤
j
−1
x∗,k ,
where A = Σ−1
0 + P
i∈S xix⊤
i . By the Sherman–Morrison formula, we have
 
A + xjx⊤
j
−1
= A−1 − A−1xjx⊤
j A−1
1 + x⊤
j A−1xj
.
Now note that A−1xjx⊤
j A−1
1 + x⊤
j A−1xj
is a positive semi-definite matrix for any xj and any positive semi-
definite matrix A. As a result, x⊤
∗,k
A−1xjx⊤
j A−1
1 + x⊤
j A−1xj
x∗,k ≥ 0 holds for any x∗,k and thus
f(S+) = max
k∈[K]
x⊤
∗,kA−1x⊤
∗,k − x⊤
∗,k
A−1xjx⊤
j A−1
1 + x⊤
j A−1xj
x∗,k ≤ max
k∈[K]
x⊤
∗,kA−1x⊤
∗,k = f(S) .
This proves our claim.
18
B.3 Proof of Theorem 1
The proof is under the assumption that at round t, the training examples can be partitioned as
Xexamples = Sk ∪ Sk. The set Sk represents examples that are close to x∗,k. The set Sk is convex
and define αk ≥ 0 such that x⊤y ≥ αk for all x, y ∈ Sk. The set Sk represents examples that are
not close to x∗,k. It is defined βk ≥ 0 such that x⊤y ≤ βk for all x ∈ Sk and y ∈ Sk. Define
αmin = mink αk, and βmax = βmax.
Define the set S = ∩K
k=1Sk as the set of all examples that are close to all{x∗,k}K
k=1 and S = ∪K
k=1Sk
as the set of all examples that are not close to all {x∗,k}K
k=1. Assume S ̸= {∅} and |S| > T.
The inverse of the covariance matrix after t − 1 observations is
bΣ−1
t = Λt = Id +
t−1X
ℓ=1
XℓX⊤
ℓ =
dX
i=1
λt,ivt,iv⊤
t,i .
The latter is the eigendecompositon of Λt, where λt,i is the i-th largest eigenvalue and vt,i is the
corresponding eigenvector. Note that Λ−1
t = Pd
i=1 λ−1
t,i vt,iv⊤
t,i. To simplify exposition, we assume
that all examples have unit length. We analyze the eigenvalues ofΛt first.
Lemma 6. For all i ∈ [d], 1 ≤ λt,i ≤ t. Moreover, let Xℓ ∈ S hold for all ℓ ∈ [t − 1]. Then
λt,1 ≥ α2
min(t − 1) + 1.
Proof. The first claim follows directly from the definition of Λt and that ∥Xℓ∥2 ≤ 1. The second
claim is proved using the definition of the maximum eigenvalue,
λt,1 = v⊤
t,1Λtvt,1 ≥ x⊤
∗,k
 
Id +
t−1X
ℓ=1
XℓX⊤
ℓ
!
x∗,k ≥ α2
min(t − 1) + 1.
The last inequality follows from x⊤
∗,kXℓ ≥ αmin. This completes the proof.
We continue with claims about the eigenvectors ofΛt.
Lemma 7. Let Xℓ ∈ S hold for all ℓ ∈ [t − 1]. Then vt,1 ∈ S. Moreover, let βmax ≥ 1 − α2
min.
Then vt,i ∈ S for all i ≥ 2.
Proof. Since all Xℓ ∈ S and S is a convex set, vt,1 ∈ S. Now take any x ∈ S and i ≥ 2, and note
that
(x⊤vt,i)2 ≤
dX
i=2
(x⊤vt,i)2 = 1 − (x⊤vt,1)2 ≤ 1 − α2
min .
We use that Pd
i=1(x⊤vt,i)2 = 1 and x⊤vt,1 ≥ αmin. Therefore, when βmax ≥ 1 − α2
min, we have
vt,i ∈ S for all i ≥ 2.
Our analysis has two parts. First, we bound the approximation error under the assumption that
Xℓ ∈ S holds for all ℓ ∈ [T]. Second, we show how to choose αmin and βmax to guarantee this. We
start with the approximation error.
Lemma 8. Let Xℓ ∈ S hold for all ℓ ∈ [T]. Then for any x∗,k
x⊤
∗,kΛ−1
T+1x∗,k ≤ 1
α2
minT + 1 + (1 − α2
min) .
Proof. We start with
x⊤
∗,kΛ−1
T+1x∗,k =
dX
i=1
λ−1
T+1,ix⊤
∗,kvT+1,iv⊤
T+1,ix∗,k ≤
(x⊤
∗,kvT+1,1)2
α2
minT + 1 +
dX
i=2
(x⊤
∗,kvT+1,i)2 .
The inequality uses lower bounds in Lemma 6. Then we apply x⊤
∗,kvT+1,1 ≤ 1 andPd
i=2(x⊤
∗,kvT+1,i)2 ≤ 1 − α2
min.
19
Now we prove by induction that Xℓ ∈ S holds for all ℓ ∈ [T].
Lemma 9. Let Xℓ ∈ S hold for all ℓ ∈ [t − 1]. Suppose that t ≤ α2
min
(β +
√
2)βd . Then xt ∈ S.
Proof. Our algorithm chooses a example xt ∈ S when for all x∗,k
x⊤
∗,kΛ−1
t xx⊤Λ−1
t x∗,k
1 + x⊤Λ−1
t x ≥
x⊤
∗,kΛ−1
t yy⊤Λ−1
t x∗,k
1 + y⊤Λ−1
t y
holds for any x ∈ S and y ∈ S. Since 0 ≤ v⊤Λ−1
t v ≤ 1 for any ∥v∥2 ≤ 1, the above event occurs
when
min
k
(x⊤
∗,kΛ−1
t x)2 = min
k
x⊤
∗,kΛ−1
t xx⊤Λ−1
t x∗,k ≥ 2 max
k∈[K]
x⊤
∗,kΛ−1
t yy⊤Λ−1
t x∗,k = 2 max
k∈[K]
(x⊤
∗,kΛ−1
t y)2 .
We start with an upper bound on the right-hand side,
max
k∈[K]
|x⊤
∗,kΛ−1
t y| ≤max
k∈[K]
dX
i=1
λ−1
t,i |x⊤
∗,kvt,iv⊤
t,iy| ≤βmaxd .
Here we use λt,i ≥ 1 (Lemma 6), and that v⊤
t,1y ≤ βmax and x⊤
∗ vt,i ≤ βmax when i ≥ 2.
Now we bound the left-hand side as
min
k
|x⊤
∗,kΛ−1
t x| ≥min
k
λ−1
t,1 |x⊤
∗,kvt,1v⊤
t,1x| −
dX
i=2
λ−1
t,i |x⊤
∗,kvt,iv⊤
t,ix| ≥α2
min
t − β2
maxd .
To bound the first term, we use λt,1 ≤ t, and that x⊤
∗ vt,1 ≥ αmin and v⊤
t,1x ≥ αmin. To bound the
second term, we use λt,i ≥ 1, and that x⊤
∗ vt,i ≤ βmax and v⊤
t,ix ≤ βmax.
Now we chain all inequalities and get that our algorithm chooses a example xt ∈ S when
t ≤ α2
min
(βmax +
√
2)βmaxd .
This completes the proof.
B.4 Proof of Theorem 2
Consider the test examples {x∗}K
k=1. Recall that Ht = (Xℓ, Yℓ)ℓ∈[t−1] is the history till round t.
Then the posterior variance is
bΣt =
 
Σ−1
0 + σ−2
t−1X
ℓ=1
XℓX⊤
ℓ
!−1
and bθt = bΣt

Σ−1
0 θ0 + σ−2 Pt−1
ℓ=1 XℓYℓ

. Now fix a Xt at round t. Then Yt = X⊤
t θ∗ + ϵt where
θ∗ | Ht ∼ N

bθt, bΣt

and ϵt ∼ N
 
0, σ2
. Then θ∗|Ht+1 ∼ N

bθt+1, bΣt+1

holds for any Yt.
Now fix an xi ∈ Xexamples and add it to bΣt such that
bΣt,i =
 
Σ−1
0 + σ−2
 t−1X
ℓ=1
XℓX⊤
ℓ + xix⊤
i
!!−1
.
Define ˆσ2
t,i,k = 1
m
Pm
j=1( ˜Y (j,1)
t,i,k − ˜Y (j,2)
t,i,k )2 and the E[ˆσ2
t,i,k] = σ2
t,i,k. Then define σ2
t,i,max =
maxk∈[K] E[ˆσ2
t,i,k]. Then using Lemma 10 we can show that with probability (1 − δ)
σ2
t,i,max
"
1 − 2
r
log(1/δ)
m
#
≤ max
k∈[K]
ˆσ2
t,i,k ≤ σ2
t,i,max
"
1 + 2
r
log(1/δ)
m + 2 log(1/δ)
m
#
(4)
20
Set m ≥ 8 log(1/δ) in (4). It follows then that
1
2σ2
t,i,max ≤ max
k∈[K]
1
m
mX
j=1
(eY (j,1)
t,i,k − eY (j,2)
t,i,k )2 ≤ 5
2σ2
t,i,max.
Hence, to minimize the quantity maxk∈[K] eY (j,1)
t,i,k − eY (j,2)
t,i,k we should be minimizing the vari-
ance 2 maxk∈[K] x⊤
∗,k bΣt,ix∗,k + σ2. Observe that minimizing the variance in SAL leads to
minimizing the quantity 2 maxk∈[K] x⊤
∗,k bΣt,ix∗,k + σ2 which is same as minimizing the score
maxk∈[K] x⊤
∗,k bΣt,ix∗,k for GO. The claim of the theorem follows.
Lemma 10. Fix round t ∈ [T], a sample example xi, and a test example x∗,k, and failure probability
δ ∈ (0, 1). Suppose that m > 4 log(1/δ). Define ˆσ2
t,i,k = 1
m
Pm
j=1( ˜Y (j,1)
t,i,k − ˜Y (j,2)
t,i,k )2 and the
E[ˆσ2
t,i,k] = σ2
i,k. Then
P
 
ˆσ2
t,i,k ≤ σ2
i,k
"
1 − 2
r
log(1/δ)
m
#!
≤ δ
holds with probability at least 1 − δ. Analogously,
P
 
ˆσ2
t,i,k ≥ σ2
i,k
"
1 + 2
r
log(1/δ)
m + 2 log(1/δ)
m
#!
≤ δ
holds with probability at least 1 − δ.
Proof. Fix an xi ∈ Xexamples and add it to bΣt. Denote this new co-variance matrix as bΣt,i such that
bΣt,i =
 
Σ−1
0 + σ−2
 t−1X
ℓ=1
XℓX⊤
ℓ + xix⊤
i
!!−1
.
Let eY (1)
t,i,j = x⊤
∗,kθ∗ + ϵt,i,j,1, where θ∗ | Ht ∼ N

bθt, bΣt+1

and ϵt,i,j,1 ∼ N
 
0, σ2
. This
yields that eY (1)
t,i,j ∼ N

x⊤
∗,k bθt+1, x⊤
∗,k bΣt,ix∗,k + σ2

. Similarly eY (2)
t,i,j = x⊤
∗,kθ∗ + ϵt,i,j,2, where
θ∗ | Ht ∼ N

bθt+1, bΣt,i

and ϵt,i,j,2 ∼ N
 
0, σ2
. Therefore, we get that
eY (j,1)
t,i,k − eY (j,2)
t,i,k ∼ N(0, 2x⊤
∗,k bΣt,ix∗,k + σ2).
Now we proceed the same way as in Lemma 2 of Lalitha et al. [2023]. Using Cochran’s theorem,
we have that ˆσ2
t,i,km/σ2
i,k is a χ2 random variable with m degrees of freedom. Then using (4.4) and
Lemma 1 of Laurent and Massart [2000] we can show that
P
 
m −
ˆσ2
t,i,km
σ2
i,k
≥ 2
p
m log(1/δ)
!
≤ δ (5)
Dividing both sides of (5) in the probability by m, and multiplying by σ2
i,k, we can get the following
P

σ2
i,k(1 − 2
p
log(1/δ)/m) ≥ ˆσ2
t,i,k

≤ δ.
Observe that 1 − 2
p
log(1/δ)/m >0, we can divide both sides by it and get the first claim of the
lemma. The second claim is proved by (4.3) in Laurent and Massart [2000], an immediate corollary
of their Lemma 1, we have
P
 
ˆσ2
t,i,km
σ2
i,k
− m ≥ 2
p
m log(1/δ) + 2 log(1/δ)
!
≤ δ
The claim of the lemma follows.
21
C Additional Experiments and Results
We use NVIDIA GeForce RTX 3090 GPU with 24GB RAM to load the Large Language Models for
inference. The Mistral-7B model requires less than 16GB RAM, and Vinuna-13B model requires less
than 22 GB RAM during execution. To run Falcon-40B model we use AWS ml-g5.12xlarge machine.
To run the full set of experiments it takes 24-27 hours of compute job. We now briefly discuss the
various datasets used in this work.
C.1 Datasets
We now briefly describe the datasets used for our experiments. All the real-life datasets are from UCI
[Markelle Kelly, 1988] and OpenML [Vanschoren et al., 2013] repositories. We use4 classification
and 3 regression datasets from UCI and OpenML. Additionally, we use 2 custom datasets for movie
names and entity names for classification task in our experiments. These are as follows:
(1) Iris: We use this UCI dataset for classification task. This dataset consists of four features of
flowers and three classes of flowers. We use all four features in the prompts as well as estimating
the score for selecting the next action. The dataset consists of 150 instances. We randomly choose
K = 20 as test examples and the remaining instances as training examples.
(2) Banknote-authentication: We use this OpenML dataset for classification task. This dataset consists
of five features of banknotes and two classes for identifying fake or original banknote. Out of these
five features, we use four features in the prompts as well as estimating the score for selecting the next
action. The dataset consists of 150 instances. We randomly choose K = 20 as test examples and the
remaining instances as training examples.
(3) Balance-scale: We use this OpenML dataset for classification task. This dataset consists of five
features of a scale and three classes of whether the scale tips left/right or is balanced. Out of these
five features, we use four features in the prompts as well as estimating the score for selecting the next
action. The dataset consists of 625 instances. We randomly choose K = 20 as test examples and the
remaining instances as training examples.
(4) Thyroid-new: We use this OpenML dataset for classification task. This dataset consists of six
features for thyroids and three classes. Out of these six features, we use five features in the prompts
as well as estimating the score for selecting the next action. The dataset consists of 215 instances.
We randomly choose K = 20 as test examples and the remaining instances as training examples.
(5) Movie-name: We use this custom dataset for classification task. This dataset consists of movie
names across five genres (classes) romance, horror, thriller, sport, and action. We convert the movie
names into 768 dimensional feature embeddings using Instructor embeddings. Note that in the prompt
to the LLM we only pass the movie names and the goal is to identify the common genre. The dataset
consists of 100 instances. We randomly choose K = 20 as test examples and the remaining instances
as training examples.
(6) Movie-theme: We use this custom dataset for classification tasks in identifying a common theme
between pairs of movies. This dataset consists of movie names across five themes (classes) good-vs-
evil, man-vs-nature, redemption, Love conquers all, and coming-of-age. We convert the movie names
into 768 dimensional feature embeddings using Instructor embeddings. Note that in the prompt to
the LLM we only pass the pair of movie names and the goal is to identify the common theme. The
dataset consists of 100 instances. We randomly choose K = 20 as test examples and the remaining
instances as training examples.
(7) Entity-name: We use this custom dataset for classification task. This dataset consists of entity
names across five entity types (classes) like mountains, seas, rivers, vehicles, and celebrities. Again,
we convert the entity names into 768 dimensional feature embeddings using Instructor embeddings.
Note that in the prompt to the LLM we only pass the entity names and the goal is to identify the
entity type. The dataset consists of 100 pairs of instances. We randomly choose K = 20 pairs as test
examples and the remaining instances as training examples.
(8) Fifa: We use this OpenML dataset for the regression task. This dataset consists of six features
of players and the clubs they joined as targets. Out of these six features, we use five features in
the prompts as well as estimating the score for selecting the next action. The dataset consists of
22
18063 instances. We randomly choose K = 20 test examples and another 200 examples as training
examples.
(9) Machine-cpu: We use this OpenML dataset for regression tasks. This dataset consists of seven
features of machine cpu and the target variable is the performance of the cpu. Out of these seven
features, we use five features in the prompts as well as estimating the score for selecting the next
action. The dataset consists of 209 instances. We randomly choose K = 20 test examples and the
remaining examples as training examples.
C.1.1 ARC Experiment
In the recent works of Mirchandani et al. [2023], Srivastava et al. [2022] they showed that LLMs
behave as general pattern-matching machine. In fact they showed that LLMs can be used to solve
tasks from Abstract Reasoning Corpus (ARC) tasks. In the following experiments, we choose two
such tasks: (1) ARC expansion and contraction experiment and (2) ARC rotation experiment.
(1) ARC expansion and contraction experiment: In the expansion and contraction experiment,
there are two sets of matrices of dimension 4 × 4 which constitute half the examples of the feature
space X. The first set of input matrices have integer values in their center 2 × 2 cells while all the
other cells are 0. The label space Y of this 4 × 4 matrix is also a 4 × 4 matrix where the 4 inner cells
have moved to the 4 corners. These matrices are termed as expansion matrices.
Similarly, the other set of4 ×4 matrices have the 4 non-zeros values in their corners. These constitute
the remaining examples in X. Then the label space Y is given by 4 × 4 matrix where the four
non-zeros cells come to the center and all the other cell values are 0. These matrices are termed as
contraction matrices. This is shown in Figure 1(a).
Therefore, the feature space X consists of both the expansion and contraction matrices. At every
trial, and n training examples and K test examples are chosen randomly from X. Then we run all
baselines for T iterations where the classification accuracy is calculated if the LLM is able to predict
the exact matching. This experiment is shown in Table 3.
(2) ARC rotation experiment: In the rotation experiment, there are again two sets of matrices of
dimension 4 × 4 which constitute half the examples of the feature space X. The first set of matrices
are have integer values in their four corner cells while all the other cells are 0. The label space Y
of this 4 × 4 matrix is also a 4 × 4 matrix where the 4 corner cell values have moved 90◦ in the
clockwise direction. These matrices are termed as clockwise matrices.
Similarly, the other set of4 ×4 matrices have the 4 non-zeros values in their corners. These constitute
the remaining examples in the feature space X. Then the label space Y is given by 4 × 4 matrix
where the four non-zeros cells have moved 15◦ in the anti-clockwise direction and all the other cell
values are 0. These matrices are termed as anti-clockwise matrices. This is shown in Figure 1(b).
Therefore, the feature space X consists of both the clockwise and anti-clockwise matrices. At every
trial, n training examples and K test examples are chosen randomly. Then we run all the baselines
for T iterations where the classification accuracy is calculated if the LLM is able to predict the exact
matching. This experiment is shown in Table 3.
C.1.2 PCFG Experiment
In this experiment the goal is to predict the next output of a sequence. In the following experiments
we choose two such tasks: (1) PCFG add-subtract experiment and (2) PCFG repeat experiment.
(1) PCFG add-subtract experiment: In the add-subtract experiment, there are two sets of sequence
of 4 integers. The first set of sequence of 4 integers consists of odd integer values which constitute
half the examples in the feature space X. The label space Y of this sequence of 4 odd examples is
sequences of 5 integers where the last integer is padded to the original sequence by adding one to the
last odd integer. These sequences are termed as add examples.
Similarly, the other set of examples consists of a sequence of 4 even integer values which constitute
the remaining examples in the feature space X. The label space Y of this sequence of 4 even integer
examples is a sequence of 5 integers where the last integer is padded to the original sequence by
subtracting one from the last even integer. These examples are termed as even examples. This is
shown in Figure 2(a).
23
  1    2 
   3   4 
 
 
   1   2 
  3  4 
 
 1   2
 
 
 3  4
 1   2
   
    
 3  4
 
  15  28 
  37  48 
 
(a) Classification on ARC Expansion and Contraction
Experiment
  2  
      4
 1   
  3  
  2   
 1 
  4
  3  
  3   1
   
    
 4  2
 1   2
   
    
 3  4
 24   
19 
 47
 39  
(b) Classification on ARC Rotation Exper-
iment
Figure 1: Explanation of ARC tasks
Therefore, the feature space X consists of both the odd and even sequence of4 integer value examples.
At every trial, n training and K test examples are chosen randomly. Then we run all the baselines for
T iterations where the classification accuracy is calculated if the LLM is able to predict the exact
matching. This experiment is shown in Table 3.
(2) PCFG repeat experiment: In the repeat experiment, there are two sets of sequence of 4 integers.
The first set of sequence of 4 odd integer values constitute half the examples of the feature space X.
The label space Y of this sequence of 4 integers examples is a sequence of 5 integers where the last
integer is padded to the original sequence by repeating the first odd integer. These sequences are
termed as odd-repeat examples.
Similarly, the other set of examples consists of sequence of 4 even integer values which constitute
the remaining examples in the feature space X. The label space Y of these sequence of 4 integer
value examples is a sequence of 5 integers where the last integer is padded to the original sequence
by repeating the second even integer. These examples are termed as even-repeat examples. This is
shown in Figure 2(b).
Therefore, the feature space X consist of both the odd-repeat and even-repeat examples. At every
trial, n training examples and K test examples are chosen randomly. Then we run all the baselines
for T iterations where the classification accuracy is calculated if the LLM is able to predict the exact
matching. This experiment is shown in Table 3.
Add 1 to 
odd 
sequence
Subtract 1 
from even 
sequence
  1  3  5   1    3   5   6
  2  4  6   2   4   6   5
+1
-1
 51  37  25
(a) Classification on PCFG add-
subtract Experiment
  1  3  5   1    3   5   1
  2  4  6   2   4   6   4
 42  30  88
(b) Classification on PCFG re-
peat Experiment
Figure 2: Explanation of PCFG task.
24
D Prompt Examples
Classification Dataset Prompts : Below we give an example of how we use the prompts to be
used in the LLM for the Iris misclassification task. Similar types of prompts can be found in Dinh
et al. [2022], Suzgun et al. [2022]. This is shown in Figure 3(a). Note that since we have the feature
representation of the training and test examples from the dataset, we directly use them as xi and x∗,k.
Regression Dataset Prompts : In Figure 3(b) we give an example of a prompt for regression task in
Fifa dataset. Note that since we have the feature representation of the training and test examples from
the dataset, we directly use them as xi and x∗,k.
(a) Classification Prompt
 (b) Regression Prompt
(c) Movie Prompt
 (d) Entity Prompt
Figure 3: Prompt examples for Classification, Regression, Movie, and Prompt
Movie Theme Experiment: We use a similar technique as in Iris dataset for this setting. The
labels of the pairs of movies belong to 5 classes as follows: good-vs-evil, man-vs-nature, re-
demption, Love conquers all, and coming-of-age. At every iteration, we pass K pairs of movie
test examples where each x∗,k is a pair of movies. In the example below we have x∗,1 =
[’Swallows and Amazon (2016), Grizzly (1976)’]. Note that we feed the natural language text to
the LLM as prompts as shown in Figure 4(a). However, to run GO, SAL, and other baselines we
require a featurization of these natural language prompts. We obtain a 768 dimensional featurized
representation of the pairs of movies ’Monsters Inc, Frozen (2013)’ using Instructor embedding [Su
et al., 2022]. This constitutes xi ∈ R768 and x∗,k ∈ R768.
Movie Name Experiment: We use a similar technique as in Iris dataset for this setting. The labels
of the movie genres belong to 5 classes as follows: romance, horror, thriller, sport, and action. At
every iteration we pass a set of test movie name examples where each x∗,k is now movie name. Note
that we feed the natural language text to the LLM as prompts as shown in Figure 3(c). However, to
run GO, SAL, and other baselines we require a featurization of these natural language prompts. We
25
(a) Theme Prompt
 (b) PCFG Prompt
Figure 4: Prompt examples for Theme and PCFG tasks
(a) ARC Prompt
Figure 5: Prompt examples for ARC task
obtain a 768 dimensional featurized representation of the movie names using Instructor embedding
[Su et al., 2022]. This constitutes xi ∈ R768 and x∗,k ∈ R768.
Entity Name Experiment: The labels of the entity genres belong to 5 classes as follows: mountains,
seas, rivers, vehicles, and celebrities. At every iteration, we pass a set of test entity name examples
where each x∗,k is now an entity name. Note that we feed the natural language text to the LLM
as prompts as shown in Figure 3(d). However, to run GO, SAL, and other baselines we require a
featurization of these natural language prompts. Again, we obtain a 768 dimensional featurized
representation of the entity names using Instructor embedding [Su et al., 2022]. This constitutes
xi ∈ R768 and x∗,k ∈ R768.
PCFG Experiment: We show an example of this prompt in Figure 4(b). Here we concatenate the
sequence to obtain training examples xi and test examples x∗,k. So a sequence of 4 integers of length
4 will be represented by xi, x∗,k ∈ R16. Similarly the label Yi and Y∗,k consist of sequence of 5
integers of length 4 which we concatentate to get a vector of length R20.
26
ARC Experiment: We show an example of this prompt in Figure 5(a). Here we vectorized the 4 × 4
matrix to obtain training examples xi ∈ R16 and test examples x∗,k ∈ R16. Similarly the label Yi
and Y∗,k consist of vectorized matrices of length R16.
E Table of Notations
Notations Definition
n Total unlabeled examples
d Dimension of the feature
X Feature set
Y Label space
θ∗ Unknown model parameter
xi Feature of sample example i
x∗,k k-th test example
f(x, θ∗) Model
Y∗ Label
Ht = (Xℓ, Yℓ)ℓ∈[t−1] History of t − 1 previously labeled examples
p(· |x, Ht) Distribution of the label of example x conditioned on
Ht
θ0 Prior mean of the unknown model parameter θ∗
Σ0 Prior mean of the unknown model parameter θ∗
bΣt =

Σ−1
0 + σ−2 Pt−1
ℓ=1 XℓX⊤
ℓ
−1
Posterior covariance
Lt Set of labeled examples
Ut Set of unlabeled examples
bθt,i,j Posterior mean
Table 5: Table of Notations
27