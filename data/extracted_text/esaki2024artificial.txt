Artificial Minimal Self on Free Energy Principle
for Autonomous Cooperative Behavior
Kanako Esaki1, Tadayuki Matsumura1, Takeshi Kato2,
Shunsuke Minusa1, Yang Shao1, and Hiroyuki Mizuno 1
1Research & Development Group, Hitachi, Ltd., Tokyo, Japan
2Hitachi Kyoto University Laboratory, Kyoto University, Kyoto, Japan
kanako.esaki.oa@hitachi.com
Abstract
To achieve autonomous cooperation among heterogeneous
agents, we propose the artificial minimal self. The artificial
minimal self is implemented based on the free energy princi-
ple (FEP), which all living organisms are supposed to follow.
In a standard FEP, the generative model configured for each
physical agent consists only of its own observations and ac-
tions. The key to autonomous cooperation among heteroge-
neous agents is to extend the sense of self beyond to others to
include interaction with others to self. In this study, focusing
on Gallagherâ€™s minimal self where the self is extended to oth-
ers, the generative model integrates its own observations and
actions and those of other heterogeneous agents. Multiple
heterogeneous physical agents have different embodiments,
resulting in different types of observations and actions. To
integrate different types of observations and actions, we de-
fine observations and actions based on environmental infor-
mation independent of the agentâ€™s embodiment. Integrating
the observations and actions of multiple physical agents into
a generative model allows extending the sense of self beyond
to others, leading to autonomous cooperation between hetero-
geneous agents. We demonstrated cooperative object trans-
port by two heterogeneous robot arms implementing the ar-
tificial minimal self. The results confirmed that each robot
arm can autonomously share tasks and transport objects by
simply providing a common goal of placing the object into
the goal. Furthermore, the process of self-extension to oth-
ers reproduced the behavior of observing the characteristics
of others that is seen in human cooperation.
Introduction
Creating social agents that cooperate with others like hu-
mans is one of the major challenges in artificial life re-
search. Cooperation means that multiple agents share work
to achieve a common goal (Roth, 2017). In an automobile
factory, for example, one robot supports one part and an-
other robot attaches another part to achieve assembling a car.
Cooperation among multiple agents enables them to achieve
challenging goals that cannot be achieved by a single agent.
Cooperation-like behavior of agents has been studied. Co-
ordination that simply assigns tasks to multiple robots with-
out sharing a common goal is known as the multi-robot task
assignment (MRTA) problem. This problem has been solved
by optimizing the task completion time, given the content of
each task and the task performance of each robot (Agrawal
et al., 2021; Park et al., 2022; Paul et al., 2022; Gautier et al.,
2023). As shown in Figure 1(a), each robot (agent) acquired
observation o as necessary information for its assigned task
and determined action a based on the observation. These so-
lutions, however, simply have each robot perform â€œindivid-
ual behaviorsâ€ determined by the task assignment algorithm,
and do not provide autonomy to the robot. Algorithms for
â€œsocial behaviorâ€ have also been studied, as shown in Fig-
ure 1(b), where each agent acquires observation o as infor-
mation necessary to determine the action of the correspond-
ing agent toward a common goal, and determines action a
based on the observation (Farivarnejad et al., 2016; Alkil-
abi et al., 2017; Wan et al., 2020; Zhu et al., 2020; Mat-
sumura et al., 2022). In these algorithms, the model for au-
tonomously determining actions such as pushing force and
direction of movement has the same configuration regard-
less of the agent, assuming that each agent has homoge-
neous abilities such as moving and grasping. Accordingly,
they do not support agents with heterogeneous abilities. Het-
erogeneity is not limited to differences in hardware config-
uration, such as between an arm robot and a mobile robot,
but also includes differences due to environmental factors,
such as differences in reach associated with the relative po-
sitions of arm robots. Heterogeneity arises from the inter-
action of agents. In order for heterogeneous agents to co-
operate autonomously, â€œfirst-person pluralistic behaviorâ€ is
required, which incorporates interactions among agents into
each agentâ€™s internal model (Gallotti and Frith, 2013).
To achieve cooperation with â€œfirst-person pluralistic be-
havior,â€ we focus on Gallagherâ€™s minimal self(Gallagher,
2000), which states that a personâ€™s sense of self extends
beyond his or her own body to others. Considering self-
extension to others, one can take optimal actions to achieve
cooperation, not only oneâ€™s own actions, but also those of
others. For example, when a person rides a bicycle, he or
she treats not only his or her own feet, but also the pedals
of the bicycle, and thus the bicycle itself, as if they were his
or her own body. The person and the bicycle perform as a
single entity to accomplish the work of cycling. In this way,
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

Model
ğ‘ğ´,ğ‘ğµ
= ğ‘”(ğ‘œğµ)
Model
ğ‘ğ´ = ğ‘“ğ´(ğ‘œğ´)
Model
ğ‘ğµ = ğ‘“B(ğ‘œğµ)
Model
ğ‘ğ´ = ğ‘“A(ğ‘œğ´)
Model
ğ‘ğµ = ğ‘“ğµ(ğ‘œğµ)
Model
ğ‘ğ´ = ğ‘“ ğ‘œğ´
ğ‘ğ´
ğ‘ğ´,ğ‘ğµ
Model
ğ‘ğµ = ğ‘” ğ‘œğµ
ğ‘ğµ
Model
ğ‘ğ´,ğ‘ğµ
= ğ‘“(ğ‘œğ´)
ğ‘ğ´,ğ‘ğµ
ğ‘ğ´ğ‘ğ´
ğ‘ğµğ‘ğµ
(c)
(b)
(a)
Agent A Agent B
Agent A Agent B
Agent A Agent B
ğ‘œğ´ ğ‘œğµ
ğ‘œğ´ ğ‘œğ´
ğ‘œğµ ğ‘œğµ
ğ‘œğ´ ğ‘œğµ
homogeneous
heterogeneous
Figure 1: Cooperation-like behaviors are (a) Individual be-
havior in coordination and (b)social behavior. Human coop-
eration is (c)first-person pluralistic behavior.
an artificial agent that follows the minimal self is expected
to achieve autonomous cooperation among multiple agents.
We propose an artificial minimal self that implements
a minimal self in an artificial agent. The artificial mini-
mal self is implemented based on the free energy principle
(FEP), which all organisms, including humans, are said to
follow(Friston et al., 2006; Friston, 2010; McGregor et al.,
2015; Friston et al., 2017; Parr et al., 2022). In the stan-
dard FEP, a self consistent with the embodiment of a phys-
ical agent is defined by a generative model consisting of
the agentâ€™s observations and actions.(Pearl, 1988; Kirchhoff
et al., 2018; Friston, 2019; Palacios et al., 2020). In this
study, according to a minimal self that can be extended to
others, the self is defined by a generative model that in-
tegrates the observations and actions of multiple heteroge-
neous physical agents as shown in Figure 1(c). Multiple
heterogeneous physical agents have different embodiments,
resulting in different types of observations and actions. Con-
sidering not only the observations of other agents, but also
their actions, allows to select appropriate actions that in-
clude the actions of other agents. To integrate the differ-
ent types of observations and actions, the observations and
actions are defined based on environmental information in-
dependent of the agentâ€™s embodiment. Integrating the obser-
vations and actions of multiple physical agents into a gener-
ative model allows the self to be extended to others, leading
to autonomous cooperation among heterogeneous agents.
Method
Free Energy Principle
Living organisms, including humans, have been proposed
to transmit information in the brain according to the free
energy principle (FEP). According to FEP, a perceptual-
action loop is formed between the agent and the environment
through observation and action. The agent is unable to di-
rectly know the hidden state of the environment, but predicts
the hidden state sÏ„ based on the observation oÏ„ of the envi-
ronment. Furthermore, the agent is unable to directly change
the hidden state of the environment, but indirectly changes
it by its action Ï€ toward the environment. The agentâ€™s gen-
erative model p(sÏ„ , oÏ„ ) is used to predict the hidden statesÏ„
and to choose the action Ï€ to take against the environment.
The generative modelp(sÏ„ , oÏ„ ) is decomposed into likeli-
hood p(oÏ„ | sÏ„ ), transition probability p(sÏ„+1 | sÏ„ , Ï€), pref-
erence distribution p(oÏ„ | C) and prior distribution p(s1).
When the observation oÏ„ , hidden state sÏ„ , and action Ï€ are
restricted to discrete space, they are represented as matrices
A, B, C, and D, respectively (Parr et al., 2022):
p(oÏ„ | sÏ„ ) = Cat(A)
p(sÏ„+1 | sÏ„ , Ï€) = Cat(BÏ€Ï„ )
p(oÏ„ | C) = Cat(C)
p(s1) = Cat(D)
(1)
where Cat is the categorical distribution. In addition, the
action Ï€ is stochastically chosen to minimize the expected
free energy G:
p(Ï€) = Cat(Ï€0)
Ï€0 = Ïƒ(âˆ’G)
GÏ€ = G(Ï€)
= âˆ’E ËœQ[DKL[Q(Ëœs | Ëœo, Ï€) âˆ¥ Q(Ëœs | Ï€)]]
âˆ’ E ËœQ ln P(Ëœo | C)
(2)
where DKLis the Kullback-Leibler divergence.
Artificial Minimal Self Based on Free Energy
Principle
The artificial minimal self is defined by a generative model
that integrates the observations and actions of multiple phys-
ical agents. In the standard FEP, as shown in Figure 2(a), the
variables of observation oÏ„ and action Ï€ in the generative
model included only the observations and actions of a single
physical agent, respectively. In this study, the observation
oÏ„ and action Ï€ variables also include the observations and
actions of multiple physical agents, as shown in Figure 2(b).
When an agent uses this integrated generative model, the
observations and actions of other agents must be inferred.
In general, the variables of observation Xability
o and action
Xability
Ï€ have been defined as mappings to the space whose
basis is the agentâ€™s ability (o, Ï€) âˆˆ V ability:
Xability
o : â„¦o â†’ {o | o âˆˆ V ability}
Xability
Ï€ : â„¦Ï€ â†’ {Ï€ | Ï€ âˆˆ V ability}
(3)
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

ğ‘ ğœ
ğ‘ ğœ+1
Actions
of others
ğ‘ ğœ
ğ‘ ğœ+1
Actions
of selfğ‘œğœ
ğ‘ ğœ
Observations
of self
ğ‘œğœ
ğ‘ ğœObservations
of self
Observations
of others
Actions
of self
ğ‘¨:ğ‘ ğ‘œğœ|ğ‘ ğœ ğ‘©:ğ‘ ğ‘ ğœ+1|ğ‘ ğœ,ğœ‹
ğ‘¨:ğ‘ ğ‘œğœ|ğ‘ ğœ ğ‘©:ğ‘ ğ‘ ğœ+1|ğ‘ ğœ,ğœ‹
(b)
(a)
ğœ‹
ğœ‹
Figure 2: Integration of observations and actions of differ-
ent agents on generative model.
where â„¦o and â„¦Ï€ are sample spaces of observation and ac-
tion, respectively. For example, these variables include â€œthe
robot armâ€™s paw is at coordinate X in the cameraâ€™s field of
viewâ€ or â€œmove the robot armâ€™s paw one meter.â€ These vari-
ables can be defined because the agent has the ability to ob-
tain the X coordinate in the cameraâ€™s field of view and the
ability to move the robot armâ€™s paw one meter. Under these
definitions, in the case of homogeneous agents, the observa-
tions and actions of the other agent can be inferred, since the
other agentâ€™s observation and action variables are the same
as those of oneâ€™s own agent. In the case of heterogeneous
agents, however, since the other agentâ€™s observation and ac-
tion variables are different from oneâ€™s own, the other agentâ€™s
observations and actions cannot be inferred. To solve this
problem, the variables of observationXenv
o and action Xenv
Ï€
are defined as mappings to the space whose basis is the en-
vironmental information (o, Ï€) âˆˆ V env:
Xenv
o : â„¦o â†’ {o | o âˆˆ V env}
Xenv
Ï€ : â„¦Ï€ â†’ {Ï€ | Ï€ âˆˆ V env}. (4)
For example, these variables include â€œthe robot armâ€™s paw is
at point Pâ€ and â€œmove the robot armâ€™s paw to point Pâ€. Un-
der these definitions, observation and action variables can be
directly associated with the environment, allowing the ob-
servation and action of the other agent to be inferred based
on observations of the environment, independent of the abil-
ities of the other agent. This environment-based definition
of observation and action variables enables an agent to han-
dle the observations and actions of multiple heterogeneous
agents without distinguishing between self and others, and
to implement a minimal self that is extended to the others.
A generative model that defines an artificial minimal self
provides the sense of self of an artificial agent. The minimal
self consists of a sense of ownership and a sense of agency.
The sense of ownership is the feeling that a region in the
environment is oneâ€™s own body. The more predictable the
change in the region associated with an action, the greater
the sense of ownership. The sense of agency, on the other
hand, is the feeling of moving a region in the environment
on oneâ€™s own initiative. The more confident the agent is
that its actions will change the region, the greater its sense
of agency. Both senses are related to the predictability of
changes in the environment. In an artificial agent follow-
ing the FEP, it is the predictability for the transition of the
hidden state, i.e. the entropy of a matrix B of transition
probability. The lower the entropy of matrix B (minimum
is 0), the more predictable, i.e., the stronger the sense of self.
The entropy of B thus expresses the strength of the artificial
agentâ€™s sense of self.
Cooperation among Agents with Artificial Minimal
Selves
All agents have a common goal and perform actions accord-
ing to the FEP to approach that goal. Algorithm 1 shows
the behavior algorithm of an agent with artificial minimal
self. Each agent has a generative model A, B, C, and D
in which the observations and actions of all agents are inte-
grated. To make all agents have a common goal, a matrix
C of common preference distributions for the observation
oÏ„ of the environment including all agents is set for each
agent (lines 1-3). After the first observation oÏ„ is acquired
(line 4), the process of lines 5-24 is repeated for the number
of episodes specified in Episodes. In each episode, the pro-
cess of lines 6 to 22 is repeated for the number of timesteps
specified in Timesteps, and then the environment is reset. In
each timestep, first, all agents infer the hidden statesÏ„ using
generative models. The policy Ï€ is then inferred based on
the expected free energy G computed using the generative
models (line 9). After that, an action Ï€Ï„ is chosen based on
the policy Ï€ (line 10).The action that minimizes the expected
free energy G is chosen with high probability. Then, if the
chosen action Ï€Ï„ is the agentâ€™s own action, the action Ï€Ï„ is
executed (lines 12-17). Then, the generative models A, B,
C, and D are updated for all agents (lines 18-20). After
that, the observation oÏ„ is acquired (line 21).
Results and Discussion
Experimental Setup
To validate cooperation among heterogeneous agents with
artificial minimal selves, a cooperative task was performed
by two types of robot arms. Figure 3 shows the robot arms
used for the validation. One is a Universal Robots UR5e
6-axis robot arm, and the other is a Denso COBOTTA 6-
axis robot arm. A ROBOTIQ 2F-140 adaptive gripper is
attached to the tip of UR5e, and a Denso electric gripper
for COBOTTA is attached to the tip of COBOTTA. These
robot arms and grippers are controlled by ROS Melodic on
Ubuntu 18.04. In addition, pymdp (Heins et al., 2022), an
OSS library (Python 3 based) for the free energy principle
and active inference, is used to implement the artificial min-
imal self for each robot arm.
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

Algorithm 1 Algorithm for behavior of agent with extended
self
1: for all agent do
2: Initialize generative models A, B, C, and D
3: end for
4: Acquire observation oÏ„
5: while e < Episodes do
6: while t < Timesteps do
7: for all agent do
8: Infer state sÏ„
9: Infer policy Ï€ with expected free energy G
10: Select action Ï€Ï„ based on policy Ï€
11: end for
12: for all agent do
13: if action Ï€Ï„ is agentâ€™s own action then
14: Execute action Ï€Ï„
15: Break
16: end if
17: end for
18: for all agent do
19: Update generative models A, B, C, and D
20: end for
21: Acquire observation oÏ„
22: end while
23: Reset environment
24: end while
The cooperative task used for validation was to transport
an object with these two robot arms. Figure 4 shows the
task scenario. Both robot arms UR5e and COBOTTA are
initially at the â€œOriginâ€ position. The object is placed in
front of robot arm UR5e, which is the â€œStartâ€ position. The
storage box is placed in front of the robot arm COBOTTA,
which is the â€œGoalâ€ position. The common goal of the coop-
erative task is to place the object into the storage box. In this
task, the robot hand of robot arm UR5e is able to reach the
object placed at the â€œStartâ€ position, but is unable to place
the object grasped by the robot hand into the storage box
because the size of the robot hand is larger than that of the
storage box. The robot hand of the robot arm COBOTTA, on
the other hand, is unable to reach objects placed at the out-
of-reach â€œStartâ€ position, but is able to place objects into
the storage box because the size of the robot hand is smaller
than that of the storage box. The heterogeneity in this vali-
dation is due to the environmental factors of the differences
in reach caused by the relative robot position and differences
in the size of the robot hand relative to the storage box. Un-
der the heterogeneity, robot arm UR5e carries the object to
the â€œIntermediateâ€ position between robot arms UR5e and
COBOTTA with the expectation that robot arm COBOTTA
will carry the object to the â€œGoalâ€ position. The robot arm
COBOTTA then carries the object placed at the â€œIntermedi-
ateâ€ position to the â€œGoalâ€ position.
UR5e Cobotta
Figure 3: Robot system for validation.
UR5e Cobotta
Reaching range
of Cobotta
Start Intermediate
Goal
Storage 
boxTarget 
object
1) UR5e move object
to Intermediate 2) COBOTTA move 
object to Goal
Origin Origin
Figure 4: Cooperative task scenario with two robot arms
moving object.
The variables for observation oÏ„ , hidden state sÏ„ , and
action Ï€ were set on environment basis. Figure 5 shows
configuration of generative models A and B in this val-
idation. The observation oÏ„ , the rows of matrix A, is
set for robot arm UR5e, robot arm COBOTTA, and ob-
ject, associated with their position in the environment:
{UR5e/COBOTTA/Object}{position}. For example, the
observation that the robot hand of the robot arm UR5e is
at the â€œStartâ€ position is â€œUR5eStart (oUS)â€. The hidden
state sÏ„ , which is the columns of matrix A and the rows
and columns of matrix B, is a possible state among the
combinations of observations oÏ„ : {observation of UR5e }-
{observation of COBOTTA}-{observation of object }. For
example, the combination of the observations â€œUR5eOrigin
(UO)â€, â€œCOBOTTAOrigin (CO)â€ and â€œObjectOrigin (OO)â€
can be observed in this validation, then it is included in
the hidden state variable as â€œUO-CO-OO (sO)â€. The com-
bination of â€œUR5eOrigin (UO)â€, â€œCOBOTTAOrigin (CO)â€,
and â€œObjectGoal (OG)â€ of the observation cannot be ob-
served in this validation, then it is not included in the hid-
den state variables. The action Ï€, the pages of matrix B,
is set associated with the positional information in the envi-
ronment, considering the reach of both robot arms UR5e and
COBOTTA: {UR5e/COBOTTA}{Move/Stop}{(position)}.
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

1.0
0.8
0.6
0.4
0.2
0.0
Element value
ğ‘ ğœ
UO-CO-OO (sO)
US-CO-OS (sS)
UI-CO-OI (sIU)
UI-CI-OI (sIC)
UI-CG-OG (sG)
ğ‘œğœ
UR5eOrigin (UO)
UR5eStart (US)
UR5eIntermediate (UI)
COBOTTAOrigin (CO)
COBOTTAIntermediate (CI)
COBOTTAGoal (CG)
ObjectOrigin (OO)
ObjectStart (OS)
ObjectIntermediate (OI)
ObjectGoal (OG)
ğ‘ ğœ
sO sS sIU sIC sG
ğ‘ ğœ+1
sO
sS
sIU
sIC
sG
UR5eMoveStart (UStart)
UR5eMoveIntermediate (UInterm)
UR5eStop (UStop)
COBOTTAMoveIntermediate (CInterm)
COBOTTAMoveGoal (CGoal)
COBOTTAStop (CStop)
ğ‘¨:ğ‘ ğ‘œğœ|ğ‘ ğœ
ğ‘©:ğ‘ ğ‘ ğœ+1|ğ‘ ğœ,ğœ‹
ğœ‹
Figure 5: Configuration of generative models A and B for
validation.
For example, UR5e can reach â€œStartâ€ position, then
â€œUR5eMoveStartâ€ is included in the action variable Ï€. On
the other hand, COBOTTA cannot reach â€œStartâ€ position,
then â€œCOBOTTAMoveStartâ€ is not included in the action
variable Ï€. Each action was assumed to include not only
moving to the corresponding position, but also grasping the
object when it is in that position, and releasing the object
when it is grasped. For each of the robot arms UR5e and
COBOTTA, an artificial minimal self is set and a generative
model shown in Figure 5 is implemented.
Cooperative Task after Self-Extension
When the self is extended to the others, the sense of self
is maximized. Figure 6 shows the matrix B of transition
probabilities p(sÏ„+1 | sÏ„ , Ï€) after self-extension. It shows
the probability distribution of the hidden state sÏ„+1 shown
in the rows when the corresponding action Ï€ is performed
in the hidden state sÏ„ shown in the columns. For example,
in the matrix of the action â€œUIntermâ€, the transition to the
next hidden state â€œsIUâ€ is achieved only if the current hid-
den state is â€œsSâ€. In other current hidden states the next hid-
den state remains unchanged. The entropy of the matrix B
of transition probability is minimal, 0. This means that the
predictability of the environment is maximum, i.e. the sense
of self is maximum.
After self-extension, robot arm UR5e and robot arm
COBOTTA chose their respective actions to minimize the
expected free energy G, resulting in a hidden state transi-
tion with the shortest timestep. Figure 7 shows the transi-
tion of the hidden states and actions of robot arm UR5e and
robot arm COBOTTA for 10 timesteps. First, the robot arm
UR5e and robot arm COBOTTA start from the hidden state
Transition probability of UR5e
Transition probability of COBOTTA
UStart UInterm UStop CInterm CStopCGoal
UStart UInterm UStop CInterm CStopCGoal
Figure 6: Expected transition probability matrix B after
self-extension. Rows, columns, and pages of matrix B fol-
low Figure 5.
1 2 3 4 5 6 7 8 9 10
Timestep
State
sO
sS
sIU
sIC
sG
Action
Ustart
Uinterm
Ustop
Cinterm
CGoal
CStop Action
Ustart
Uinterm
Ustop
Cinterm
CGoal
CStop
State
sO
sS
sIU
sIC
sG
1.0
0.8
0.6
0.4
0.2
0.0
Value
COBO
TTA
UR5e
Figure 7: Transition of inferred hidden states and actions.
â€œsOâ€. The action â€œUStartâ€, and the action â€œUIntermâ€ were
selected in sequence until the hidden state â€œsIUâ€. After that,
the action â€œCIntermâ€, and the action â€œCGoalâ€ were selected
in sequence. It took the shortest 5 timestep, to reach the final
hidden state â€œsGâ€. Figure 8 shows the motion sequences of
the two robot arms. Robot arm UR5e grasped the object at
the â€œStartâ€ position and then placed the object at the â€œInter-
mediateâ€ position. The robot arm COBOTTA then moved to
the â€œIntermediateâ€ position, grasped the object, and placed
the object in the storage box at the â€œGoalâ€ position. This se-
ries of processes was executed without stopping for a single
timestep. It was confirmed that the agents whose self was
extended to the other agent cooperated with each other only
by being given a common goal in the form of a matrix C of
preference distributions.
The cooperative behavior of robots without stagnation
can be considered to reproduce the way humans cooperate
(Roth, 2017). Humans who have collaborated with each
other before may have an understanding of each otherâ€™s be-
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

All at 
Origin
UR5e 
move to 
Start
UR5e at 
Start
UR5e 
move to 
Interm.
Timestep 1 Timestep 2
Object at 
Interm.
COBO
move to 
Interm.
COBO at 
Interm.
COBO
move to 
Goal
Object at 
Goal
Timestep 3 Timestep 4 Timestep 
5
Interm.=IntermediateCOBO=COBOTTA
Figure 8: Motion sequences from timestep 1 to 5.
havior patterns in cooperation. Given a common goal, these
humans will decide what they need to do and what they need
to delegate to others, and act efficiently. They can cooper-
ate autonomously without detailed assignment of tasks. It
was confirmed that agents with extended selves cooperate
autonomously with each other in the same way.
State and Action during Self-Extension
To analyze the process of self-extension in the cooperative
task, the self prior to self-extension was assumed to be iden-
tical to the physical body. Figure 9 shows the matrix B of
the transition distribution when the self is assumed to be
identical to the physical body. In the matrix B that robot
arm UR5e has, the matrix elements corresponding to the ac-
tions of robot arm UR5e are 0 or 1. This means that the next
hidden state sÏ„+1 is calculated deterministically when the
corresponding action Ï€ is performed in the current hidden
state sÏ„ . The matrix elements corresponding to the actions
of the robot arm COBOTTA, on the other hand, all have the
same value. This means that the next hidden statesÏ„+1 when
the corresponding action Ï€ is performed in the current hid-
den state sÏ„ is equally probable for all hidden state variables.
Likewise for the matrix B of the robot arm COBOTTA. The
transitions of the hidden states associated with the actions of
its own agent can be completely inferred, while the transi-
tions of the hidden states associated with the actions of the
other agent cannot be inferred at all. The cooperation was
repeated with 5 episodes and 20 timesteps in Algorithm 1.
During self-extension, the agents gradually performed co-
operative tasks smoothly. Figure 10 shows the transitions of
the inferred hidden states during the self-extension of UR5e.
In episode 1 (timesteps 1 to 20), there was a long stagnation
time in each hidden state, and the hidden state did not reach
â€œsGâ€. Then, in episode 2 (timesteps 21 to 40), the hidden
state reached â€œsGâ€ in 9 timesteps, although it stagnated for a
few timesteps in â€œsOâ€ and â€œsIUâ€. From episode 3 (timesteps
41 to 60) on, the hidden state reached â€œsGâ€ in the shortest 5
Transition probability of UR5e
Transition probability of COBOTTA
UStart UInterm UStop CInterm CStopCGoal
UStart UInterm UStop CInterm CStopCGoal
Figure 9: Expected transition probability B matrix when
self is identical to physical body. Rows, columns, and pages
of matrix B follow Figure 5.
timesteps without stagnating in any of the hidden states. Fig-
ure 11 shows the transition of each robotâ€™s ratio of selecting
its own actions to those of others during self-extension. For
all robots, the actions of others were selected more often in
the first half of the episode. This is due to the high entropy of
the probability distribution of state transitions regarding the
actions of others. The high entropy means that the state tran-
sitions are unpredictable, resulting in a larger search term in
the expected free energy G. As a result, more actions of
others are selected to obtain information about state transi-
tions. In the later episodes, oneâ€™s own actions were selected
more often. This is due to an overall decrease in entropy.
Low entropy means that the state transitions are predictable,
resulting in a larger exploitation term for the expected free
energy G. As a result, more self actions are selected that are
optimal for the current hidden state.
Cooperative performance during self-extension can be
thought of as reproducing a situation in which people who
have never met work together. The fact that more of the
other personâ€™s actions were selected in the first half of the
episode is reminiscent of the situation of observing the other
personâ€™s behavior patterns in cooperative work. When peo-
ple meet for the first time, the other personâ€™s behavior pat-
tern is initially unknown, so both sides observe each other in
an attempt to understand the other personâ€™s behavior pattern.
The expectation of the other person in a certain hidden state
also cannot be read, and the state stagnates. As a result, it be-
comes a time-consuming cooperative task that repeats state
stagnation. In addition, the fact that more of oneâ€™s own ac-
tions were selected in the later episodes reminds us of a situ-
ation in which one selects oneâ€™s own actions based on oneâ€™s
understanding of the otherâ€™s behavior pattern. Observing the
otherâ€™s behavior pattern allows for understanding their be-
havior pattern. Then, they are able to read the expectations
of the other agent in a certain hidden state and immediately
decide their own actions. This results in smooth cooperation
without stagnation of the hidden state. In this way, agents
who had never met before gradually became familiar with
each other, and the cooperative task became smooth.
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

1.00.80.60.40.20.0
Value
State
sO
sS
sIU
sIC
sG
1 10 20
21 30 40
81 90 100
Timestep
41 50 60
61 70 80
State
sO
sS
sIU
sIC
sGState
sO
sS
sIU
sIC
sGState
sO
sS
sIU
sIC
sGState
sO
sS
sIU
sIC
sG
Episode 1
Episode 2
Episode 3
Episode 4
Episode 5
Figure 10: Transition of inferred hidden states during self
extention.
0%
100%
1 2 3 4 5
Episode
Action of others Action of self
UR5e
0%
100%
1 2 3 4 5
Episode
Action of others Action of self
COBOTTA
Figure 11: Ratio of choices between actions of others and
actions of self.
Sense of Self in Extended Self
The generative model obtained by self-extension showed
multiple training patterns. Figure 12 shows the training
results for the matrix B of UR5e and the matrix B of
COBOTTA at timestep 100 for some trials. In Figure 12(b),
the columns of matrix B with the current hidden states of
â€œsOâ€, â€œsSâ€, and â€œsIUâ€ of COBOTTAâ€™s matrix B are closer
to the matrix B of UR5eâ€™s matrix B shown in Figure 12(a).
This indicates that the state transition model associated with
the actions of the other (UR5e) has been trained as the
model that the other (UR5e) has. On the other hand, in Fig-
ure 12(c), columns â€œsOâ€, â€œsSâ€, and â€œsIUâ€ of the matrix B
of COBOTTAâ€™s current hidden state are different from those
of UR5eâ€™s matrix B. Specifically, when the current hid-
den state was â€œsOâ€, selecting â€œUIntermâ€ increased the prob-
ability that the next hidden state would be â€œsSâ€. When the
UStart UInterm UStop
(c)
(b)
(a)
1.0
0.8
0.6
0.4
0.2
0.0
Value
Figure 12: Trained matrix B of (a) UR5e, (b) COBOTTA at
timestep 100 in one case and (c) COBOTTA at timestep 100
in another case. Rows, columns, and pages of matrix B fol-
low Figure 5. Since the matrix B for its own (COBOTTAâ€™s)
actions has already been trained as shown in Figure 9, only
the matrix B for othersâ€™ (UR5eâ€™s) actions is shown.
current hidden state was â€œsSâ€, selecting â€œUStartâ€ increased
the probability that the next hidden state would be â€œsIUâ€.
The actions â€œUIntermâ€ and â€œUStartâ€ were trained inversely.
This is due to the fact that they were trained as state tran-
sitions for the otherâ€™s inappropriate action, because at the
timestep when the otherâ€™s inappropriate action was selected,
the other itself performed the correct action and transitioned
to the correct state. It is remarkable that the hidden state
transitions correctly even though the actions were trained in-
correctly. The hidden state â€œsIUâ€ is reached even though the
actions are selected in the order of action â€œUIntermâ€ and ac-
tion â€œUStartâ€, when they should be selected in the order of
action â€œUStartâ€ and action â€œUIntermâ€. Although the matri-
ces B of UR5e and COBOTTA are trained on each other and
do not necessarily converge (Fruchart et al., 2021), the inter-
esting result is that even if they do not converge, cooperation
is still achieved.
The gap between COBOTTAâ€™s internal inference and
UR5eâ€™s own internal choice about UR5eâ€™s actions corre-
sponding to the same hidden state transition due to the in-
consistency of the generative model indicates a philosophi-
cal problem called inverted qualia (Byrne, 2020). Inverted
qualia refers to the possibility that multiple agents have dif-
ferent internal senses of the same event. When the gap in
the otherâ€™s action appears in some way, for example, in fail-
ure to cooperate, exploratory actions are generated to fill the
gap, leading to further cooperation. The gap associated with
inverted qualia motivates autonomous cooperation. In the
experiment, the hidden state of both robot arms transitioned
as expected, and the gap regarding the otherâ€™s action did not
appear, however, if the gap appears due to a change in the
cooperation goal or other reasons, further autonomous co-
operation is expected.
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

As the cooperative task was repeated, the equivalent of
a sense of self gradually increased. Figure 13 shows the
negative entropy of the next hidden state and its sum in
the matrix B of transition probabilities from before to af-
ter self-extension of the robot arm UR5e. The negative en-
tropy of the next hidden state is specifically the negative en-
tropy of the probability distribution of the next hidden state
âˆ’H{p(sÏ„+1 | scur, Ï€cur)} for the hidden state inferred at
a given timestep scur and the action chosen at that timestep
Ï€cur. Figure 13(a) shows the negative entropy of the next
hidden state. Episode 1 (timesteps 1 to 20) started with
small values and gradually increased at the time of transi-
tion to the hidden state â€œsSâ€ (timestep 9) and to the hidden
state â€œsIUâ€ (timestep 17). After episode 4 (timestep 61), the
first 2 timesteps when the robot arm UR5e performs its own
action, the values were maximum at zero, but decreased in
the next 2 timesteps when the other robot arm COBOTTA
performs its own action. Figure 13(b) shows the negative
entropy of the next hidden state summed over all hidden
states P
s[âˆ’H{p(sÏ„+1 | s, Ï€cur)}]. This value represents
the predictability of the next hidden state when focusing on
a particular action. From episode 1 to 3 (timesteps 1 to 60),
the values tended to increase gradually, despite the phases in
which various actions were chosen exploratively. Timesteps
with zero values, such as timestep 8, are when an appro-
priate action is selected and the hidden state is transitioned.
After episode 4 (timestep 61), as in Figure 13(a), the first 2
timesteps in which the robot arm UR5e performs its own ac-
tion have a maximum value of zero, but the next 2 timesteps
in which the other robot arm COBOTTA performs an ac-
tion have decreased values. Figure 13(c) shows the nega-
tive entropy of the next hidden state summed over all ac-
tions P
Ï€[âˆ’H{p(sÏ„+1 | scur, Ï€)}]. This value refers to the
predictability of the next hidden state when focusing on a
particular hidden state. From episode 1 to 3 (timesteps 1 to
60), as in Figure 13(a), the values started small and gradually
increased as the hidden states changed. The difference from
Figure 13(a) and (b) is that after episode 3 (timestep 41),
the value was the smallest in the timesteps where the hidden
state is â€œsGâ€ (timesteps 51 to 60, 65 to 80, and 85 to 100).
This means that predictability is high for actions selected at
the time when the hidden state transitions, while predictabil-
ity does not increase for actions selected at the time when
the hidden state reaches â€œsGâ€ and does not transition there-
after, no matter what action is taken. Figure 13(d) shows the
negative entropy of the next hidden state, summed over all
hidden states and actions P
s,Ï€[âˆ’H{p(sÏ„+1 | s, Ï€)}]. This
value represents the predictability of all hidden states and
actions. At the first timestep, the entropy for UR5eâ€™s own
actions is zero, and thus the value of this timestep can be
said to be the default value of cooperation with COBOTTA.
Focusing on each hidden state or action, as in Figure 13(a)
to (c), the sense of self varied greatly depending on whether
the selected action was the selfâ€™s or the otherâ€™s, but viewing
(c)
0.0
-2.0
0 20 40 60 80 100
Timestep
-1.0
-7.0
0 20 40 60 80 100
Timestep
à·
ğœ‹
âˆ’ğ» ğ‘ ğ‘ ğœ+1|ğ‘ ğ‘ğ‘¢ğ‘Ÿ,ğœ‹ âˆ’ğ» ğ‘ ğ‘ ğœ+1|ğ‘ ğ‘ğ‘¢ğ‘Ÿ,ğœ‹ğ‘ğ‘¢ğ‘Ÿ
(a) (b)
0.0
-12.0
à·
ğ‘ 
âˆ’ğ» ğ‘ ğ‘ ğœ+1|ğ‘ ,ğœ‹ğ‘ğ‘¢ğ‘Ÿ
0 20 40 60 80 100
Timestep
-15.0
-35.0
à·
ğ‘ ,ğœ‹
âˆ’ğ» ğ‘ ğ‘ ğœ+1|ğ‘ ,ğœ‹
0 20 40 60 80 100
Timestep
(d)
Figure 13: Transitions in sense of self. (a) Negative en-
tropy of next hidden state (b) Negative entropy summed over
hidden states (c) Negative entropy summed over actions (d)
Negative entropy summed over hidden states and actions
them as a single entity increased the sense of self.
Conclusion
An artificial minimal self was proposed to achieve au-
tonomous cooperation among heterogeneous agents. The
artificial minimal self is implemented based on the free en-
ergy principle (FEP) that all organisms are supposed to fol-
low. Focusing on Gallagherâ€™s minimal self, the generative
model of the FEP configured for each agent integrates not
only its own observations and actions but also those of other
heterogeneous agents. The environment-based definition of
observation and action variables allows to integrate observa-
tions and actions with different embodiments, leading to au-
tonomous cooperation between heterogeneous agents. Co-
operative object transport was demonstrated by two robot
arms implemented with artificial minimal selves. The results
confirmed that each robot arm was able to autonomously
share the task and transport the object by simply provid-
ing a common goal of placing the object at the target po-
sition. Furthermore, the self-extension to others reproduced
the behavior of observing the characteristics of others ob-
served in human cooperation. Future work includes expand-
ing the state space and validating on tasks that involve more
switching between self and other actions.
References
Agrawal, A., Won, S. J., Sharma, T., Deshpande, M., and McComb,
C. (2021). A multi-agent reinforcement learning frame-
work for intelligent manufacturing with autonomous mobile
robots. Proceedings of the Design Society, 1:161â€“170.
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025

Alkilabi, M. H. M., Narayan, A., and Tuci, E. (2017). Cooperative
object transport with a swarm of e-puck robots: robustness
and scalability of evolved collective strategies. Swarm Intel-
ligence, 11(3):185â€“209.
Byrne, A. (2020). Inverted Qualia. In Zalta, E. N., editor, The
Stanford Encyclopedia of Philosophy. Metaphysics Research
Lab, Stanford University, Fall 2020 edition.
Farivarnejad, H., Wilson, S., and Berman, S. (2016). Decentralized
sliding mode control for autonomous collective transport by
multi-robot systems. In 2016 IEEE 55th conference on deci-
sion and control (CDC), pages 1826â€“1833. IEEE.
Friston, K. (2010). The free-energy principle: a unified brain the-
ory? Nature reviews neuroscience, 11(2):127â€“138.
Friston, K. (2019). A free energy principle for a particular physics.
arXiv preprint arXiv:1906.10184.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pez-
zulo, G. (2017). Active Inference: A Process Theory. Neural
Computation, 29(1):1â€“49.
Friston, K., Kilner, J., and Harrison, L. (2006). A free energy prin-
ciple for the brain. Journal of Physiology-Paris, 100(1):70â€“
87. Theoretical and Computational Neuroscience: Under-
standing Brain Functions.
Fruchart, M., Hanai, R., Littlewood, P. B., and Vitelli, V . (2021).
Non-reciprocal phase transitions. Nature, 592(7854):363â€“
369.
Gallagher, S. (2000). Philosophical conceptions of the self: impli-
cations for cognitive science. Trends in cognitive sciences ,
4(1):14â€“21.
Gallotti, M. and Frith, C. D. (2013). Social cognition in the we-
mode. Trends in Cognitive Sciences, 17(4):160â€“165.
Gautier, P., Laurent, J., and Diguet, J.-P. (2023). Deep q-
learning-based dynamic management of a robotic cluster.
IEEE Transactions on Automation Science and Engineering,
20(4):2503â€“2515.
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K.,
Couzin, I. D., and Tschantz, A. (2022). pymdp: A python
library for active inference in discrete state spaces. Journal
of Open Source Software, 7(73):4098.
Kirchhoff, M., Parr, T., Palacios, E., Friston, K., and Kiverstein,
J. (2018). The markov blankets of life: autonomy, active in-
ference and the free energy principle. Journal of The Royal
Society Interface, 15(138):20170792.
Matsumura, T., Esaki, K., and Mizuno, H. (2022). Empathic Ac-
tive Inference: Active Inference with Empathy Mechanism
for Socially Behaved Artificial Agent. In ALIFE 2022: The
2022 Conference on Artificial Life, page 18.
McGregor, S., Baltieri, M., and Buckley, C. L. (2015). A minimal
active inference agent. arXiv preprint arXiv:1503.04187.
Palacios, E. R., Razi, A., Parr, T., Kirchhoff, M., and Fris-
ton, K. (2020). On markov blankets and hierarchical self-
organisation. Journal of Theoretical Biology, 486:110089.
Park, B., Kang, C., and Choi, J. (2022). Cooperative multi-robot
task allocation with reinforcement learning. Applied Sci-
ences, 12(1).
Parr, T., Pezzulo, G., and Friston, K. J. (2022). Active inference:
the free energy principle in mind, brain, and behavior . MIT
Press.
Paul, S., Ghassemi, P., and Chowdhury, S. (2022). Learning scal-
able policies over graphs for multi-robot task allocation using
capsule attention networks. In2022 International Conference
on Robotics and Automation (ICRA), pages 8815â€“8822.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: net-
works of plausible inference. Morgan kaufmann.
Roth, A. S. (2017). Shared Agency. In Zalta, E. N., editor, The
Stanford Encyclopedia of Philosophy. Metaphysics Research
Lab, Stanford University, Summer 2017 edition.
Wan, W., Shi, B., Wang, Z., and Fukui, R. (2020). Multirobot ob-
ject transport via robust caging. IEEE Transactions on Sys-
tems, Man, and Cybernetics: Systems, 50(1):270â€“280.
Zhu, P., Dai, W., Yao, W., Ma, J., Zeng, Z., and Lu, H. (2020).
Multi-robot flocking control based on deep reinforcement
learning. IEEE Access, 8:150397â€“150406.
Downloaded from http://direct.mit.edu/isal/proceedings-pdf/isal2024/36/9/2461085/isal_a_00720.pdf by guest on 13 December 2025
