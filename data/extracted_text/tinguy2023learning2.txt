Learning Spatial and Temporal Hierarchies:
Hierarchical Active Inference for navigation
in Multi-Room Maze Environments
Daria de Tinguy 1, Toon Van de Maele 1, Tim Verbelen2 and Bart Dhoedt 1
Abstract— Cognitive maps play a crucial role in facilitating
flexible behaviour by representing spatial and conceptual re-
lationships within an environment. The ability to learn and
infer the underlying structure of the environment is crucial for
effective exploration and navigation. This paper introduces a
hierarchical active inference model addressing the challenge of
inferring structure in the world from pixel-based observations.
We propose a three-layer hierarchical model consisting of a
cognitive map, an allocentric, and an egocentric world model,
combining curiosity-driven exploration with goal-oriented be-
haviour at the different levels of reasoning from context to place
to motion. This allows for efficient exploration and goal-directed
search in room-structured mini-grid environments.
I. I NTRODUCTION
The development of autonomous systems able to navigate
in their environment is a crucial step towards building intelli-
gent agents that can interact with the real world. Developing
navigation skills in artificial agents to mirror the natural nav-
igational abilities observed in animals, enabling these agents
to adeptly move autonomously through their surroundings,
has been a topic of great interest in the field of robotics and
artificial intelligence. Understanding complex and potentially
aliased environments and effectively navigating them require
both spatial hierarchy, i.e. capturing spatial structures and
relationships [1], and temporal hierarchy [2] as they are
essential for devising long-term navigation strategies. This
has led to the exploration of various approaches, including
cognitive mapping inspired by animal navigation strategies.
In the animal kingdom, cognitive mapping plays a crucial
role in navigation. Cognitive maps allow animals to under-
stand the spatial layout of their surroundings [3], [4], [5],
remember key locations, solve ambiguities thanks to context
[6] and plan efficient routes [6], [7]. By leveraging cognitive
mapping strategies, animals can successfully navigate com-
plex environments, adapt to changes, and return to previously
visited places.
Several approaches have been proposed to learn the struc-
ture of the world in the context of navigation. [8] proposes
a clone structured graph representation of the environment
to disambiguate aliased observations. [9] presents a deep
hierarchical model based on active inference and casts
structure learning as a Bayesian model reduction problem.
[10] introduces a hierarchical generative model learning and
recognising maze structures based on specific localisation in
1Department of Information Technology, University of Ghent, Ghent,
Belgium,2Verses AI, Vancouver, Canada {Correspondence to:
Daria de Tinguy <Daria.detinguy at ugent.be>
a global prefixed frame. While all these generative models
aim to capture the underlying structure and dynamics of the
world, these are typically limited to small simulations with
discrete state and observation spaces.
Addressing this aspect, recent approaches like G-SLAM
[11] and Dreamer [12] use deep neural networks to learn
generative world models from high-dimensional observations
such as pixels. However, as these capture the world in a
flat latent state space, these models struggle with long-term
planning, especially in aliased environments.
In this paper, in order to enhance the agent’s ability to
navigate autonomously and intelligently we propose a pixel-
based hierarchical active inference model exhibiting both
spatial and temporal hierarchies. The model is geared to-
wards learning the structure of maze mini-grid environments
[13]. A maze consists of interconnected, visually similar
rooms with variations in shape, size, and colour as depicted
in Fig 1. Within the maze, there is a single white goal tile.
Our model consists of amortised inference models at the
lower levels, which are trained on pixel data, for representing
movement and pose in egocentric and allocentric reference
frames respectively, combined with a graph-structured model
at the top to capture the maze structure. The model navi-
gation is based on a principled active inference approach,
which balances goal-directed behaviour and epistemic for-
aging through information gain [14]. Moreover, the planning
happens at different temporal time scales at each level in the
hierarchy, allowing for long-term decision-making.
Our contributions can be summarised as follows:
• We introduce a system leveraging hierarchical active in-
ference and world modelling for achieving autonomous
navigation without the necessity for task-specific train-
ing. This approach allows agents to navigate intelli-
gently in familiar environments.
• Our system is built upon visual observations, which
holds promise for real-world applications.
• The proposed system not only learns the spatial struc-
ture of the environment but also adapts to its dynamic
constraints enhancing autonomous navigation.
• It creates an internal map of the entire environment,
exhibiting scalability by not demanding increased com-
putational resources with larger environments.
• We conduct comprehensive evaluations in a mini-grid
room maze environment [13]. Our approach demon-
strates its efficiency in tasks related to exploration and
goal attainment, outperforming other Reinforcement
arXiv:2309.09864v1  [cs.AI]  18 Sep 2023
Fig. 1: Example of a 3 × 3 rooms mini-grid environment and
our model navigation in it during an exploration and goal-
reaching task, where the starting position is the red triangle.
Noise on the visualised path was added in post-processing
for observing superposed visits on a single tile.
Learning (RL) models.
• Through quantitative and qualitative analyses, we
demonstrate the effectiveness of our hierarchical active
inference world model in accomplishing various tasks.
Our approach exhibits resilience to aliasing and show-
cases its ability to learn the underlying structure of the
environment.
II. M ETHOD
Symbols Associated Terms
l location, experience
z place, room, allocentric state
p pose, position
s egocentric state
a action
o observation
c collision
πx policy, sequence of x
TABLE I: Description of the variables used in our model
The active inference framework [15] is built on the
premise that intelligent agents minimise their surprise. An
active inference agent entails an internal generative model
aiming to best explain the causes of external observation
and the consequences of its actions through the minimisation
of surprise or prediction error, which is also known as free
energy (FE). Agents minimise this quantity with respect to
model parameters in learning and with respect to action in
planning [14], [16].
We propose a hierarchical generative model consisting of
three layers functioning at nested timescales (see Fig 2).
From top to bottom: the cognitive map, creating a coherent
topological map, the allocentric model, representing space,
and the egocentric model, managing motions. The system
infers the environment’s structure, over time, by agglomer-
ating visual observations, creating representations of distinct
places such as rooms, and progressively revealing the maze’s
connectivity as a graph.
The cognitive map: The top layer in the generative model,
illustrated in Fig 2a) functions at the coarsest time scale
(T), each tick at this time scale corresponds to a distinct
location ( lT ) integrating the initial positions ( pT
0 ) of the
place ( zT ). These locations are depicted as nodes in a
topological graph, as shown in Fig 2d). Edges between nodes
are added as the agent moves from one location to another,
effectively learning the maze structure. In order to maintain
the spatial structure between locations, the agent keeps track
of its relative rotation and translation using a continuous
attractor network (CAN) as in [17]. Hence the cognitive map
forms a comprehensive representation of the environment,
enabling the agent to navigate by formulating believes over
its surroundings.
The allocentric model : The middle layer, illustrated in
Fig 2b), plays a crucial role in constructing a coherent
understanding of the environment, denoted as zT . This model
functions at a finer time scale ( t), forming a belief over the
place by integrating a sequence of observations ( sT
t ) and
poses (pT
t ) to generate this representation [18], [19]. Fig 2e)
and Fig 5 showcase the resulting place defining the environ-
ment given accumulated observations. As the agent moves
from one place to another, once the current observations do
not align with the previously formed prediction about the
place, the allocentric model resets its place description and
gathers new evidence to construct a representation of the
newly discovered room ( zT+1), advancing by one tick on
the coarser time scale and resetting the mid-level time scale
t to 0.
The egocentric model : The lowest layer, illustrated in
Fig 2c), has the finest time scale ( τ). To evolve in time
this model requires the prior state ( st
τ ) and current action
(at
τ+1) to infer the current observation (ot
τ+1) [20]. Based
on its current position, the model generates possible future
trajectories while considering the constraints imposed by the
environment, such as the inability to pass through walls
(achieved by discerning the cause-effect relationship between
actions and observations). Fig 2f) illustrates the current ob-
servation in the middle o) and shows the imagined potential
observations if the agent were to turn left i), right iii), or
move forward ii).
Planning: The model operates within a hierarchical active
inference scheme, planning at different time scales. The
cognitive map plays a vital role in long-term navigation by
handling place connectivity, allowing the model to plan the
locations to visit ( π) at a high level, the poses to visit within
each place (πl) at a mid-level, and determining the best action
policy (πp) at the low level while considering obstacles such
as walls. To infer the best navigation strategy to reach a
desired objective, the agent employs active inference and
utilises the concept of Expected Free Energy (EFE). EFE
is a measure of the agent’s projected uncertainty or surprise
about future states. By minimising EFE, the agent aims to re-
duce uncertainty and make accurate predictions about future
outcomes, thus determining an optimal path to the objective
[16], [21]. This hierarchical active inference process, coupled
with the integration of EFE, allows the agent to effectively
Fig. 2: Our generative model unrolled in time and levels as defined in Eq.1. The left shows the graphical model of the
3-layer hierarchical Active inference model consisting of a) the cognitive map, b) the allocentric model, and c) the egocentric
model, each operating at a different time scale. The orange circles represent latent states that have to be inferred, the blue
circles denote observable outcomes and the white circles are random variables to be inferred by planning. The right part
visualises the representation at each layer. The cognitive map is represented as d) a topological graph composed of all the
locations (l) and their connections, in which each location is stored in a distinct node. The allocentric model e) infers place
representations ( z) by integrating sequences of states ( s) and poses ( p), from which the room structure can be generated.
The egocentric model f) imagines future observations given the current position, state ( s), and possible actions ( a) from this
position. Here o) depicts an actual observation ( o) and the predicted observations of the possible actions left i), forward ii),
and right iii).
explore new rooms at the highest level, navigate within the
rooms at the mid-level, and execute actions seamlessly at the
low level.
The overall system can be represented as the formula
equation 1.
P(˜o, ˜z, ˜s, ˜l, π,˜πl, ˜πp) =P(π)
Y
T
P(zT , p0
T |lT )P(lT |π)P(πl)
Y
t
P(st
0|zT , pT
t )P(pT
t |πl, pT
0 )P(πp)
Y
τ
P(st
τ+1|st
τ , at
T )P(at
τ |πp)P(ot
τ , ct
τ |st
τ )
(1)
Our model hyper-parameters are defined Appendix C.
It is trained on a dataset of pixel observations collected
by sampling random actions in a 3 by 3 rooms mini-grid
environment as depicted in Fig1. For more details on the
models and training procedure we also refer to Appendix E.
III. R ESULTS
A. Tasks oriented navigation
Our testing primarily centre around assessing the model’s
capacity to execute specific functional tasks, including ac-
quiring spatial maps through exploration in the presence
of aliased and disjoint sensory input, transferring structural
knowledge, and facilitating hierarchical planning. The agent
is assigned two tasks: exploration and goal-reaching, both
achieved without requiring re-training beyond learning fa-
miliar room structures.
Baseline. To establish a baseline for the navigation tasks,
we compare our method against:
• C-BET [22], an RL algorithm combining model-based
planning with uncertainty estimation for efficient explo-
ration decision-making.
• Random Network Distillation (RND) [23], integrates
intrinsic curiosity-driven exploration to incentivise the
agent’s visitation of novel states, meant to foster a
deeper understanding of the environment.
• Curiosity [24], leverages information gain as an intrinsic
reward signal, encouraging the agent to explore areas of
uncertainty and novelty.
• Count-based exploration [25] uses a counting mecha-
nism to track state visitations, guiding the agent toward
less explored regions.
• Dreamerv3 [26] represents an advanced iteration of
world models for RL, offering the potential to enhance
navigation by predicting and simulating future trajecto-
ries for improved decision-making.
• A-star Algorithm (Oracle) [27], is a path planning
algorithm to which the full layout of the environment
and its starting position is given to plan the ideal path
to take between two points.
Each of these models propose various RL based explo-
ration strategies for robotics navigation. All baselines were
trained and tested on the exact same environments. For each
model training details, please refer to Appendix A to D.
The testing environments consist of connected rooms of
increasing scale, ranging from 9 up to 20 rooms, each room
with a width of 4 tiles.
1) Exploration: We assess to what extent the hierarchi-
cal active inference model enables our agent to efficiently
explore the environment. Without a preferred state leading
the model toward an objective, the agent is purely driven
by epistemic foraging, i.e. maximising information gain,
effectively driving exploration [14].
Our evaluation involves comparing the performance of
various models, including our proposed hierarchical active
inference model, C-BET, Count, Curiosity, RND models, and
an Oracle. These models are tasked with exploring fully
new environments with configurations ranging from 9 to
20 rooms. While the oracle possesses complete knowledge
of the environment and its initial position, other models
are equipped only with their top down view observations
(and, in the case of some RL models, extrinsic rewards)
-see AppendixD for more information about each model
observation type-. The RL models are encouraged to explore
until they locate a predefined goal (white tile); however, the
reward associated with the white tile is muted to encourage
continued exploration. Notably, the DreamerV3 model faces
challenges in effective exploration due to its reliance on
visual observations of the white tile for reward extraction.
Consequently, an adapted environment without the white
tile or a specific training would be necessary to employ
DreamerV3 as an exploration-oriented agent in this study.
Across more than 30 runs by environment scale, our model
demonstrates efficient exploration comparable to C-BET and
notably outperforming other RL models in all tested environ-
ments, as depicted in Fig.3. Moreover, the agent successfully
achieves the desired level of exploration more frequently than
any other model across all configurations, as demonstrated
in Table II. For an exploration attempt to be considered
successful, the agents must observe a minimum of 90% of the
observable environment. This criterion ensures that all rooms
are observed at least once, without imposing a penalty on the
models for not capturing every single corner. Since the agents
cannot see through walls (see Appendix D), entering a room
may result in missing the adjacent wall corners, but these
corners hold limited importance for the agent’s objective.
As an unlikely example, missing all the corner tiles of each
room results in 9% of the environment not being observed
(thus no matter the scale of the environment).
success
rate (%) models
environment
configuration ours C-BET RND curiosity count
3x3 rooms 93 81 16 32 13
3x4 rooms 94 87 16 19 0
4x4 rooms 91 81 26 16 0
4x5 rooms 81 74 7 23 3
TABLE II: The success rate of each model across all runs in
each environment is defined as the percentage of runs where
the exploration covers at least 90% of the environment.
2) Exploitative Behaviour: To evaluate the exploitative
behaviour of the models, we configure all the models men-
tioned in the baseline to navigate to the single white tile
within the environment. This is conducted across environ-
ments of escalating size, ranging from 9 to 20 rooms. Goal-
directed behaviour is induced in our model by setting a
preferred observation (i.e. the white tile) as typically done
in active inference [14], [21]. In the other RL models, an
extrinsic and intrinsic reward is associated with this white
tile, motivating the agents to explore until they reach this
tile. The task is considered successful when the agent steps
on the single white tile of the maze. All the models, except
the oracle, start without knowing their and the goal position
in the environment, they therefore need to explore until
they find the objective. Fig.4 displays the results of all the
model in the goal seeking task in the diverse environments.
Our model requires, in average, more steps than the Count
model to reach the white tile in the 3 by 3 and 3 by 4
rooms configurations, however we can observe that count
also has the lowest success rate. The Count model often
fails when reaching the goal requires to cross several rooms.
Overall our model reaches the white tile 89% of the time
over all environments (see Tab.III), Dreamerv3 is showing
a poor performance because of over-fitting, not adapting
well to new rooms configurations and white tile placement
it has never seen during training. All models underwent
training using the identical dataset detailed in Appendix B,
and the optimal models are selected for testing purposes.
This observation suggests that Dreamerv3 might require
a comparatively higher degree of human intervention to
effectively operate within our environment compared to other
models.
B. Qualitative results
Figure 5 illustrates the inference process of place descrip-
tions. Within approximately three steps, the main features
of the environment are captured and form a reasonably
accurate picture of the seen observations. When encountering
a new aisle for the first time at step 11, the model is able
(a) coverage over steps of
all models in 3 by 3 rooms
environments
(b) coverage over steps of
all models in 3 by 4 rooms
environments
(c) coverage over steps of
all models in 4 by 4 rooms
environments
(d) coverage over steps of
all models in 4 by 5 rooms
environments
Fig. 3: The average exploration coverage across all test instances ( >30runs) for each model computed within a certain scale
of environment. Our model and C-Bet demonstrate similar performances in terms of speed and overall coverage, with our
model exhibiting a narrower error margin in 3 by 4 rooms configuration, indicating consistent achievement of the specified
coverage in most runs.
Fig. 4: All models are assigned a goal seeking task in more
than 30 environments varying in maze scale from 3 by 3 to 4
by 5 rooms presented in order from the 1st column to the last.
The first row presents the mean number of steps each model
take to successfully achieve the objective, with the black
bars indicating a deviation of 15% around the mean. The
second row representing the success rate of each model in
completing the task as a percentage. Across all environments,
both C-BET and our model outperform the other RL models
in terms of the mean number of steps required to reach the
goal. Each model has its own colour: Grey for the Oracle,
red for Count, blue for Ours, green for curiosity, orange for
C-BET, pink for DreamerV3 and purple for RND.
Models success
rate (%)
Oracle 100%
Ours 89%
C-BET 86%
RND 81%
Curiosity 79%
DreamerV3 72%
Count 31%
TABLE III: The success rate of each model across all
environments and runs.
to adapt and generate a well-imagined representation. Each
observation corresponds to the red agent’s clear field of view,
as depicted in the agent position row of the figure (more
details about the observations AppendixD).
Figure 6 provides a direct comparison between the ac-
curacy of the cognitive map’s room reconstruction and the
corresponding physical environment. This comparison re-
veals that the estimated map closely aligns with the actual
map, with only minor discrepancies observed in some blurry
passageways and a slight misplacement of the aisle in
the bottom right room. This shows how important global
position estimation is as the cognitive map uses believed
location to distinguish between two identical rooms (purple
rooms in the second column). This alignment between real
and imagined map underscores the fidelity of our model’s
internal representation in capturing the structural layout of
the environment. A supplementary demonstration showing
the model’s ability for place recognition can be found in
AppendixF Fig.10.
Additional quantitative and qualitative findings are avail-
able in Appendix F, illustrating the agent’s capacity to gen-
eralise and adeptly reconstruct more expansive environments
beyond its training exposure. The appendix also presents
an instance of navigation within a maze characterised by
aliasing, showcasing the model’s ability in constructing and
selecting pertinent place representations. Furthermore, we
demonstrate the proficiency of our hierarchical model in
making precise predictions over extended time-frames, en-
compassing transitions across various rooms where, in con-
trast, recurrent state space models often encounter challenges
Fig. 5: Evolution of the place representation in a room as new
observations are provided by the moving agent (red triangle).
The model is able to correctly reconstruct the structure of the
room as observations are collected.
(a) A 3 by 4 room-maze seen from above.
(b) A map of a 3 by 4 environment and rooms representation given
by the cognitive map. We can see that the estimated map is loyal to
the real map aside from some non precisely defined aisles and the
bottom right room having a slightly wrong aisle position.
Fig. 6: a) displays the real map while b) is a composition of
a cognitive map room’s representations. Overall, the rooms
are recognisable when compared to the real map.
when tasked with predicting across room boundaries. Lastly,
we highlight the computational efficiency of our system in
comparison to the conventional RL models employed in this
study (Appendix A and F Tab.VII ).
IV. D ISCUSSION
The discussion section of this paper aims to provide a
comprehensive analysis of the proposed hierarchical active
inference model for autonomous navigation, considering its
strengths and limitations. We outline the key contributions
of our work and discuss the potential future works.
Hierarchical Active Inference Model . Our proposal in-
troduces a three-layered hierarchical active inference model
:
• The cognitive map unifies spatial representation and
memorises location characteristics.
• The allocentric model creates chunked spatial represen-
tations.
• The egocentric model assesses policy plausibility, con-
sidering dynamic limitations.
These layers collaborate at different time scales: the high
level oversees the whole environment through locations, the
allocentric model refines place representations as it change
position, and the egocentric model imagines action conse-
quences.
Low Computational Demands . Our hierarchical active
inference model has a low computational demands regardless
of the environment’s scale. This efficiency is particularly
valuable as environments scale up, making our approach a
potential solution for real-world applications.
Scalability. Our model efficiently learn spatial layouts and
their connectivity. There exists the potential for our approach
to adapt to novel scenarios by incorporating diverse environ-
ments into its learning process, thus expanding allocentric
representations. Furthermore, the possibility of introducing
additional higher layers could facilitate greater abstraction,
transitioning from room-level learning to broader structural
insights.
Task Agnostic. The system doesn’t require task-specific
training, promoting adaptability to diverse navigational sce-
narios. It learns environment structures and generalises to
new scenarios, demonstrating applicability to various objec-
tives.
Visual based navigation . Leveraging visual cues should
enhance our model’s real-world applicability.
Aliasing Resistant. We show resistance to aliases, distin-
guishing between identical places and thus ideally supporting
robust navigation.
While our approach offers several advantages, it is also
important to acknowledge its limitations:
Environment Adaptation Our model requires adaptation
to fully new environments for optimal performance. Training
the allocentric model on room-specific data restricts navi-
gation to familiar settings. To mitigate this, and generalise
to arbitrary environments, we could consider splitting the
data by unsupervised clustering [1], or by using the model’s
prediction error to chunk the data into separate spaces [28].
Recognition of Changed Environments Our proposal
might struggle to detect environmental changes like altered
tile colors, although this may not significantly impact navi-
gation performance as a new place will replace or be added
with the previous one in the cognitive map, it remains an
area for improvement.
Our comprehensive assessment, both quantitative and
qualitative, underscores the adaptability and resilience of our
approach, which fares well in explorative and exploitative
tasks compared to C-BET [22], Random Network Distillation
(RND) [23], Curiosity [24] or Count [25], Dreamerv3 [26].
In conclusion, while our navigation demonstrates promis-
ing exploration and goal-reaching abilities in a minigrid [13]
environment by learning structures and leveraging visual
cues, future research could focus on enhancing adaptation
to new environments, handling changes in familiar ones,
and adding comprehension to the cognitive map. Scalability
and extension to complex dynamic scenarios are also poten-
tial avenues for exploration. While our model demonstrates
promising performance in a mini-grid environment, its ap-
plication to more realistic scenarios, such as the Memory
maze [29] or Habitat [30], could lead to more practical
implementations. Further improvements could also involve
learning a prior over topological map structures, promoting
informed exploration and imagination of shortcuts [31].
ACKNOWLEDGMENT
This research received funding from the Flemish Govern-
ment under the “Onder- zoeksprogramma Artifici ¨ele Intelli-
gentie (AI) Vlaanderen” programme.
REFERENCES
[1] Y . Asano, C. Rupprecht, and A. Vedaldi, “Self-labelling via
simultaneous clustering and representation learning,” in International
Conference on Learning Representations , 2020. [Online]. Available:
https://openreview.net/forum?id=Hyx-jyBFPr
[2] A. Zakharov, M. Crosby, and Z. Fountas, “Episodic memory for
learning subjective-timescale models,” 2020.
[3] R. Epstein, E. Z. Patai, J. Julian, and H. Spiers, “The cognitive map in
humans: Spatial navigation and beyond,”Nature Neuroscience, vol. 20,
pp. 1504–1513, 10 2017.
[4] P. Foo, W. Warren, A. Duchon, and M. Tarr, “Do humans integrate
routes into a cognitive map? map- versus landmark-based navigation
of novel shortcuts.” Journal of experimental psychology. Learning,
memory, and cognition , vol. 31, pp. 195–215, 04 2005.
[5] M. Peer, I. K. Brunec, N. S. Newcombe, and R. A. Epstein,
“Structuring knowledge with cognitive maps and cognitive graphs,”
Trends in Cognitive Sciences , vol. 25, no. 1, pp. 37–54, 2021.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S1364661320302503
[6] J. Balaguer, H. Spiers, D. Hassabis, and C. Summerfield, “Neural
mechanisms of hierarchical planning in a virtual subway network,”
Neuron, vol. 90, pp. 893–903, 05 2016.
[7] M. S. Tomov, S. Yagati, A. Kumar, W. Yang, and S. J. Gershman,
“Discovery of hierarchical representations for efficient planning,”
bioRxiv, 2018. [Online]. Available: https://www.biorxiv.org/content/
early/2018/12/17/499418
[8] D. George, R. Rikhye, N. Gothoskar, J. S. Guntupalli, A. Dedieu, and
M. L ´azaro-Gredilla, “Clone-structured graph representations enable
flexible learning and vicarious evaluation of cognitive maps,” Nature
Communications, vol. 12, 04 2021.
[9] V . Neacsu, M. B. Mirza, R. A. Adams, and K. J. Friston, “Structure
learning enhances concept formation in synthetic active inference
agents,” PLOS ONE , vol. 17, no. 11, pp. 1–34, 11 2022. [Online].
Available: https://doi.org/10.1371/journal.pone.0277199
[10] I. Stoianov, D. Maisto, and G. Pezzulo, “The hippocampal formation
as a hierarchical generative model supporting generative replay
and continual learning,” Progress in Neurobiology , vol. 217, p.
102329, 2022. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0301008222001150
[11] A. Safron, O. C ¸ atal, and T. Verbelen, “Generalized simultaneous
localization and mapping (g-SLAM) as unification framework for
natural and artificial intelligences: towards reverse engineering the
hippocampal/entorhinal system and principles of high-level cognition,”
Oct. 2021. [Online]. Available: https://doi.org/10.31234/osf.io/tdw82
[12] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, “Mastering
atari with discrete world models,” CoRR, vol. abs/2010.02193, 2020.
[Online]. Available: https://arxiv.org/abs/2010.02193
[13] M. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic grid-
world environment for openai gym,” https://github.com/maximecb/
gym-minigrid, 2018.
[14] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, J. O.
Doherty, and G. Pezzulo, “Active inference and learning,”
Neuroscience & Biobehavioral Reviews , vol. 68, pp. 862–879,
2016. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S0149763416301336
[15] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference: The Free
Energy Principle in Mind, Brain, and Behavior . The MIT Press,
2022. [Online]. Available: https://direct.mit.edu/books/oa-monograph/
5299/Active-InferenceThe-Free-Energy-Principle-in-Mind
[16] R. Kaplan and K. Friston, “Planning and navigation as active infer-
ence,” 12 2017.
[17] M. Milford, G. Wyeth, and D. Prasser, “Ratslam: a hippocampal model
for simultaneous localization and mapping,” vol. 1, pp. 403–408 V ol.1,
2004.
[18] S. M. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos,
M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka, K. Gregor,
D. P. Reichert, L. Buesing, T. Weber, O. Vinyals, D. Rosenbaum,
N. Rabinowitz, H. King, C. Hillier, M. Botvinick, D. Wierstra,
K. Kavukcuoglu, and D. Hassabis, “Neural scene representation
and rendering,” Science, vol. 360, no. 6394, pp. 1204–1210, 2018.
[Online]. Available: https://www.science.org/doi/abs/10.1126/science.
aar6170
[19] T. Van de Maele, T. Verbelen, O. C ¸ atal, C. De Boom, and B. Dhoedt,
“Active vision for robot manipulators using the free energy principle,”
Frontiers in Neurorobotics , vol. 15, 2021. [Online]. Available:
https://www.frontiersin.org/articles/10.3389/fnbot.2021.642780
[20] O. C ¸ atal, S. Wauthier, C. De Boom, T. Verbelen, and
B. Dhoedt, “Learning generative state space models for active
inference,” Frontiers in Computational Neuroscience , vol. 14,
2020. [Online]. Available: https://www.frontiersin.org/article/10.3389/
fncom.2020.574372
[21] P. Schwartenbeck, J. Passecker, T. U. Hauser, T. H. FitzGerald,
M. Kronbichler, and K. J. Friston, “Computational mechanisms of
curiosity and goal-directed exploration,” eLife, vol. 8, p. e41703, may
2019. [Online]. Available: https://doi.org/10.7554/eLife.41703
[22] S. Parisi, V . Dean, D. Pathak, and A. Gupta, “Interesting object,
curious agent: Learning task-agnostic exploration,” CoRR, vol.
abs/2111.13119, 2021. [Online]. Available: https://arxiv.org/abs/2111.
13119
[23] Y . Burda, H. Edwards, A. J. Storkey, and O. Klimov, “Exploration
by random network distillation,” CoRR, vol. abs/1810.12894, 2018.
[Online]. Available: http://arxiv.org/abs/1810.12894
[24] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven
exploration by self-supervised prediction,”CoRR, vol. abs/1705.05363,
2017. [Online]. Available: http://arxiv.org/abs/1705.05363
[25] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and
R. Munos, “Unifying count-based exploration and intrinsic motiva-
tion,” in Advances in Neural Information Processing Systems , D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29.
Curran Associates, Inc., 2016.
[26] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, “Mastering diverse
domains through world models,” 2023.
[27] A. Candra, M. A. Budiman, and K. Hartanto, “Dijkstra’s and a-
star in finding the shortest path: a tutorial,” in 2020 International
Conference on Data Science, Artificial Intelligence, and Business
Analytics (DATABIA), 2020, pp. 28–32.
[28] T. Verbelen, D. de Tinguy, P. Mazzaglia, O. Catal, and A. Safron,
“Chunking space and time with information geometry,” in NeurIPS
2022 Workshop on Information-Theoretic Principles in Cognitive
Systems, 2022.
[29] J. Pasukonis, T. Lillicrap, and D. Hafner, “Evaluating long-term
memory in 3d mazes,” 2022.
[30] M. Savva, A. Kadian, O. Maksymets, Y . Zhao, E. Wijmans, B. Jain,
J. Straub, J. Liu, V . Koltun, J. Malik, D. Parikh, and D. Batra, “Habitat:
A platform for embodied ai research,” inProceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , October 2019.
[31] D. d. Tinguy, P. Mazzaglia, T. Verbelen, and B. Dhoedt, “Home run:
Finding your way home by imagining trajectories,” in Active Inference.
Cham: Springer Nature Switzerland, 2023, pp. 210–221.
[32] O. C ¸ atal, T. Verbelen, T. Van de Maele, B. Dhoedt, and
A. Safron, “Robot navigation as hierarchical active inference,” Neural
Networks, vol. 142, pp. 192–204, 2021. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S0893608021002021
[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic
optimization.” [Online]. Available: https://arxiv.org/abs/1412.6980
APPENDIX
The appendixes are composed of two parts, the training specifics detailed from the models required computational
consideration Appendix A, then describing the dataset App.B then describing for each model the hyperparameters used
if any different from their source paper and the observations used for their model App.C and each model observation type
App.D. Moreover Appendix E details our training more in depth with the loss functions used for optimisation. Finally
App.F shows all the additional tests results from quantitative assessment of the generalisation ability to represent places to
the models testing computational requirements.
A. Training system requirements
model Training
time (h) n◦ CPU n◦ GPU used
RAM (G)
used
Memory (G) GPU type
Ours
egocentric 32 4 1 12 GTX 980
Ours
allocentric 95 2 1 2.5 20 GTX 1080
Dreamerv3 411 5 2 10 30 GTX 1080 Ti
C-BET 232 10 1 2.6 32 GTX 980
RND 117 6 1 2.7 10 GTX 980
Curiosity 90 6 1 3 10 GTX 980
Count 141 6 1 2.7 11 GTX 980
TABLE IV: Comprehensive insights into the training specifics of all models are provided, encompassing their respective
training duration until reaching their finalised versions. Additionally, details regarding the computational resources employed
are presented including the maximum RAM and memory allocation required for each model. Notably, our model underwent
training for both egocentric and allocentric components, executed in parallel using the same dataset, their division into
distinct sets realised for practical reasons. Unfortunately, the information pertaining to the RAM utilisation by the egocentric
model is unavailable.
B. Training Dataset
Uniformity in training conditions was achieved by conducting training sessions for all models within identical environ-
ments, facilitated by the consistent application of a shared seed to generate these environments. The model is trained on a
mini-grid environment consisting of 3 by 3 squared rooms of 4 to 7 tiles wide connected by aisles of fixed length randomly
placed, separated by a closed door in the middle. Each room is assigned a colour at random from a set of four: red, green,
blue, and purple. In addition, white tiles may be present at random positions in the map. The agent has a top view of the
environment covering a windows of 7 by 7 tiles, including it’s own occupied tile. It cannot see behind itself, nor through
walls or closed doors.
C. Hyper-Parameters
All the adversarial models were trained upon pre-set hyper-parameters, with C-BET, Count, Curiosity and RND upon [22]
described parameters. And DreamerV3 upon [26] proposed work, however the behaviour was modified from the original,
setting an Exploring task behaviour and a Greedy exploration behaviour as the original configuration was over-fitting in our
scenarios.
Our model was trained using the hyper-parameters in Tab.V for the allocentric model and Tab.VI for the egocentric model
Layer Neurons/Filters Stride
PositionalEncoder Linear 9
Posterior Convolutional 16 1 // (kernel:1)Convolutional 32 2Convolutional 64 2Convolutional 128 2Linear 2*32
Likelihood
ConcatenationLinear 256*4*4UpsampleConvolutional 128 1UpsampleConvolutional 64 1UpsampleConvolutional 32 1UpsampleConvolutional 3 1
TABLE V: allocentric model parameters
Layer Neurons/Filters Stride
Prior ConcatenationLSTM 256Linear 2*32
Posterior
Convolutional 8 2Convolutional 16 2Convolutional 32 2ConcatenationLinear 256Linear 64
ImageLikelihood
Linear 256Linear 32*7*7UpsampleConvolutional 16 1UpsampleConvolutional 8 1UpsampleConvolutional 3 1
CollisionLikelihoodLinear 16Linear 8Linear 1
TABLE VI: egocentric model parameters
D. Models observations
All models use the agent’s top down vision of the agent, consisting in 7 by 7 tiles with the agent placed at the bottom centre
of the image, as shown in 7. Our model and DreamerV3 use an RGB pixel rendering of shape 3x56x56. The observation
the agent interprets is a . while C-BET, Count, Curiosity and RND use a flat hot-encoded view of the environment as well
as an extrinsic reward when passing over the single white tile in the environment. We can point out that the agent can’t see
through walls in RGB image, we can see in Fig.7 a) the environment and the agent’s field of view represented by lighter
colours. Fig.7 b) shows the actual observation seen by the agent.
The number of actions cbet could take is greatly reduced compared to the original work, limiting to actions such as
forward, left, right and stand-by.
Fig. 7: a) cropped top down view of the environment, b) the RGB view of the agent, each tile of the environment are
composed of 8 by 8 pixels, generating a 56 by 56 total image. c) the equivalent hot-encoded view as a matrix, the numbers
and colours are only relevant for the example.
All the RL models have a sparse reward system, with an extrinsic reward generated only when passing on the white tile
disposed in the environment. Our model doesn’t require rewards, as such the goal we desire to set during the testing could
be any kind of observation.
E. Our Model training
In order to effectively train this hierarchical model, the two lower level models are considered independent and trained
in parallel To optimise the two ego-allocentric neural network models we first obtain a dataset of sequences of action-
observation pairs by interacting with the environment. This can for example be obtained using a random policy or by human
demonstrations.
The egocentric and allocentric models are considered independent to optimise training. While they are trained over the
same data, the allocentric model has sequences chunked up to maximum the transition between two rooms, so that each
sequence encompass only a room each time. Thus the allocentric model received as training data of 1000 sequences of
40steps per room size (4 to 7 tiles width) while the egocentric received 100 sequences of 400 steps per room size, each full
sequence then cut in sub-sequences of 20 steps. All the training took place in 3x3 rooms maze where the agent could start
the sequence randomly from any door (or near door) position. The training was realised on 100 environments per room size.
Both neural networks can be trained in parallel end-to-end on this dataset using stochastic gradient descent by minimising
the free energy loss function.
For the egocentric model:
L =
TX
t=1
DKL[pϕ(st|st−1, at−1, ot)||pθ(st|st−1, at−1)] − log[pξ(ot|st)] (2)
The egocentric model was trained by minimising, in one part, the difference between the expected belief state given the
couple action, previous history and the estimated posterior obtained given the action, observation and updated history. And
in a second part minimising the difference between the reconstructed observation and the input observation [32].
While for the allocentric model:
L =
TX
t=0
DKL[Qϕ(z|ot, pt)||N(0, 1)] +||ˆot − ot||2 (3)
The approximate posterior Q is being modelled by the factorisation of the posteriors after each observation. The belief
over z can then be acquired by multiplying the posterior beliefs over z for every observation. We learn an encoder neural
network with parameters ϕ to learn the posterior state z given a single observation and pose pair (ok, sk).
And the Likelihood being optimised by an MSE given the real observation ok and the predicted observation ˆok [19], for
each room the posterior is built on random sequence length varying from 15 observations up to the whole sequence of 40
steps. In order to obtain a position, the action of the agent is integrated into the next position. Both models use an Adam
optimisation [33].
While the cognitive map has been adapted for navigation in this type of mini-grid world [13], it is thought to be re-scaled
or adapted to other environments.
F . Supplementary results
The following appendix contributes to the work by shedding additional lights on the ability of the model to solve rooms
aliasing and its ability to generalise.
Fig3 shows the agent consistently achieving a stable place description within about three observations in room sizes that
were part of its training. Interestingly, the agent also demonstrates the ability to accurately reconstruct larger rooms, even
though it did not encounter such sizes during training. In particular, stable place descriptions for 8-tile wide rooms are
achieved in approximately five steps. This showcases the agent’s allocentric model generalisation abilities beyond the limits
of its training. The experiment was conducted over 125 runs in 25 environments with the agent tasked to predict observations
from unvisited poses after each new motion. Fig4 demonstrate the significance of the Mean Squared Error (MSE) value by
displaying examples of predicted observations and their corresponding MSE values. It can be observed that under an MSE
of 0.5, the predictions of the observations are visually quite accurate.
Fig. 8: Prediction error of unvisited positions over
min 5 tests per 5 environments by room size
starting from step 0 where the models has no
observation.
Fig. 9: Observation queries, sampled prediction over unvisited
position and mean MSE over 5 samples of predictions
In order to navigate autonomously, an agent must be capable of self-localisation and position correction based on visual
information and its internal beliefs about the environment. We perform navigation in a highly aliased mini-grid maze
consisting of four interconnected rooms. These rooms shared similarities in colour, configuration, or both colour and
configuration, differing only by a single white tile. These four rooms are depicted in Figure 10 A. The complete Figure 10
demonstrates the agent’s exploration of the rooms and its ability to differentiate between them without becoming confused,
even when retracing its path from the starting room by entering rooms through different aisles than before.
effectively, when the agent identifies a new place, it creates a fresh experience by incorporating its location. Figure 10 B.
displays each newly generated experience with a distinct ID and colour. has entered a new place or returned to a familiar
one, the agent considers the probability of each place to explain the current observations, as depicted in Figure 10 C. The
bars illustrate the number of hypotheses considered at each step, and the lines represent the probability of the place being
either a new or a previously visited one. The colours of the lines correspond to the colours attributed to the experiences
in Figure 10 B, with blue lines representing new unidentified places. Figure 10 D. displays the internal representation of
the places the agent uses. It is evident that the rooms are accurately imagined, and even a doubt in an aisle position in
experience 1 is not sufficient to confuse the agent.
In this context, the agent demonstrates the capability to navigate effectively and distinguish between rooms in a novel,
highly aliased environment. The agent’s aptitude to identify previously visited rooms even upon entering from a new doorway
underscores its capacity to retain a spatial memory of the environment.
Fig. 10: Navigation samples of the agent looping clockwise and anti-clockwise (resulting in entry through different doors) in
a new 2 by 2 room environment. Clockwise navigation corresponds to entirely new exploration, generating new places (as
seen with the blue lines in C. corresponding to a new room generation), while the anti-clockwise loop traverses previously
explored places. A. shows a new world comprising four visually similar rooms (identical in colour, shape, or both). B.
illustrates the model’s association of each room with a distinct experience ID C. C depicts the probability of a new place
being created (in blue, representing the most probable place among all possibilities) or an existing place being deemed the
most likely to explain the environment. The grey bars indicate the number of new places considered concurrently, with the
count of simultaneous hypotheses displayed on the right side of the plot. D. showcases the imagined place generated for
each experience ID. Experience 1 is not entirely accurate, yet it suffices to distinguish it from other rooms given actual
observations of it.
Furthermore, our hierarchical model also facilitates accurate predictions over extended timescales that span different rooms.
In contrast, recurrent state space models commonly struggle when tasked with predicting across room boundaries. Fig11
illustrates the prediction capabilities of each layer over a prolonged imagined trajectory within a familiar environment. The
figure showcases the predictions that each layer of the model would make as we project the imagination into the future, up
to the point of transitioning to a new room and beyond. The first row demonstrates how the egocentric model gradually loses
the spatial layout information over time, making it more suitable for short-term planning. The second row highlights the
model’s limitation to a single place in the environment, failing to recognise the subsequent room as the same place. Lastly,
in the third row, the cognitive map’s imagined trajectory accounts for the agent’s location and is capable of summoning
the appropriate place representation while estimating the agent’s motion across space and time. The final row displays the
ground truth trajectory, which aligns quite closely with the expectations of the cognitive map.
Finally, among all the reinforcement learning (RL) models employed in this study, an increase in the number of steps
directly correlates with higher memory usage, frequently leading to failure if memory capacity falls short. In contrast, our
approach provides a notably more efficient solution, requiring a maximum of 1G of memory space and mitigating scalability
concerns related to the size of the environment. Refer to Table VII for a summary of the most demanding requirements for
an exploration/goal task involving a maximum of 1500 steps.
Fig. 11: A trajectory leading toward a previously visited room is imagined by each model’s layer. The egocentric model,
characterised by its short-term memory, gradually loses information as time progresses. This is evident from step 2 onward,
where the front aisle is no longer represented after the agent makes a few turns without visual input. In contrast, the
allocentric model maintains the place description over time but encounters difficulty once it moves beyond the current place
it occupies. The cognitive map, possessing knowledge of the connections between locations, accurately deduces the expected
place behind the door, resulting in a prediction remarkably similar to the ground truth.
model n◦ CPU n◦ GPU used
Memory (G)
Ours 2 0 1
Dreamerv3 2 1 28
C-BET 2 0 12
RND 2 0 9
Curiosity 2 0 11
Count 2 0 8
TABLE VII: Every model has specific system requirements, this table outlines the most demanding criteria needed to achieve
successful exploration or goal seeking in the 4 by 5 rooms environment configuration.