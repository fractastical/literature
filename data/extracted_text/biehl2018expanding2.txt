METHODS
published: 30 August 2018
doi: 10.3389/fnbot.2018.00045
Frontiers in Neurorobotics | www.frontiersin.org 1 August 2018 | Volume 12 | Article 45
Edited by:
Antonio Chella,
Universit degli Studi di Palermo, Italy
Reviewed by:
Karl Friston,
University College London,
United Kingdom
Alessandro Di Nuovo,
Shefﬁeld Hallam University,
United Kingdom
*Correspondence:
Martin Biehl
martin@araya.org
Received: 18 April 2018
Accepted: 02 July 2018
Published: 30 August 2018
Citation:
Biehl M, Guckelsberger C, Salge C,
Smith SC and Polani D (2018)
Expanding the Active Inference
Landscape: More Intrinsic Motivations
in the Perception-Action Loop.
Front. Neurorobot. 12:45.
doi: 10.3389/fnbot.2018.00045
Expanding the Active Inference
Landscape: More Intrinsic
Motivations in the Perception-Action
Loop
Martin Biehl1*, Christian Guckelsberger2, Christoph Salge3,4 , Simón C. Smith4,5 and
Daniel Polani4
1 Araya Inc., Tokyo, Japan, 2 Computational Creativity Group, Department of Computing, Goldsmiths, University of London,
London, United Kingdom, 3 Game Innovation Lab, Department of Computer Science and Engi neering, New York University,
New York, NY, United States, 4 Sepia Lab, Adaptive Systems Research Group, Department of C omputer Science, University
of Hertfordshire, Hatﬁeld, United Kingdom, 5 Institute of Perception, Action and Behaviour, School of Info rmatics, The
University of Edinburgh, Edinburgh, United Kingdom
Active inference is an ambitious theory that treats percept ion, inference, and
action selection of autonomous agents under the heading of a single principle. It
suggests biologically plausible explanations for many cog nitive phenomena, including
consciousness. In active inference, action selection is dr iven by an objective function
that evaluates possible future actions with respect to curr ent, inferred beliefs about the
world. Active inference at its core is independent from extr insic rewards, resulting in a
high level of robustness across e.g., different environmen ts or agent morphologies. In
the literature, paradigms that share this independence hav e been summarized under the
notion of intrinsic motivations. In general and in contrast to active inference, these models
of motivation come without a commitment to particular infer ence and action selection
mechanisms. In this article, we study if the inference and ac tion selection machinery
of active inference can also be used by alternatives to the or iginally included intrinsic
motivation. The perception-action loop explicitly relate s inference and action selection
to the environment and agent memory, and is consequently use d as foundation for our
analysis. We reconstruct the active inference approach, lo cate the original formulation
within, and show how alternative intrinsic motivations can be used while keeping many
of the original features intact. Furthermore, we illustrat e the connection to universal
reinforcement learning by means of our formalism. Active in ference research may proﬁt
from comparisons of the dynamics induced by alternative int rinsic motivations. Research
on intrinsic motivations may proﬁt from an additional way to implement intrinsically
motivated agents that also share the biological plausibili ty of active inference.
Keywords: intrinsic motivation, free energy principle, active inference, predictive information, empowerment,
perception-action loop, universal reinforcement learning,variational inference
Biehl et al. Expanding the Active Inference Landscape
1. INTRODUCTION
Active inference (
Friston et al., 2012 ), and a range of other
formalisms usually referred to as intrinsic motivations ( Storck
et al., 1995; Klyubin et al., 2005; Ay et al., 2008 ), all aim to
answer a similar question: “Under minimal assumptions, how
should an agent act?” More practically, they relate to what wou ld
be a universal way to generate behaviour for an agent or robot
that appropriately deals with its environment, i.e., acquires the
information needed to act and acts toward an intrinsic goal. To
this end, both the free energy principle and intrinsic motivat ions
aim to bridge the gap between giving a biologically plausible
explanation for how real organism deal with the problem and
providing a formalism that can be implemented in artiﬁcial
agents. Additionally, they share a range of properties, such a s
an independence of a priori semantics and being deﬁned purely
on the dynamics of the agent environment interaction, i.e., the
agent’s perception-action loop.
Despite these numerous similarities, as far as we know, there
has not been any uniﬁed or comparative treatment of those
approaches. We believe this is in part due to a lack of an
appropriate unifying mathematical framework. To alleviate th is,
we present a technically complete and comprehensive treatment
of active inference, including a decomposition of its percepti on
and action selection modes. Such a decomposition allows us to
relate active inference and the inherent motivational princ iple
to other intrinsic motivation paradigms such as empowerment
(
Klyubin et al., 2005 ), predictive information ( Ay et al., 2008 ),
and knowledge seeking ( Storck et al., 1995; Orseau et al.,
2013). Furthermore, we are able to clarify the relation to
universal reinforcement learning ( Hutter, 2005 ). Our treatment
is deliberately comprehensive and complete, aiming to be a
reference for readers interested in the mathematical funda ment.
A considerable number of articles have been published on
active inference (e.g.,
Friston et al., 2012, 2015, 2016a,b, 2017a,b;
Linson et al., 2018 ). Active inference deﬁnes a procedure for
both perception and action of an agent interacting with a
partially observable environment. The deﬁnition of the meth od,
in contrast to other existing approaches (e.g.,
Hutter, 2005;
Doshi-Velez et al., 2015; Leike, 2016 ), does not maintain a
clear separation between the inference and the action select ion
mechanisms, and the objective function. Most approaches for
perception and action selection are generally formed of three
steps: The ﬁrst step involves a learning or inference mechani sm
to update the agent’s knowledge about the consequences of its
actions. In a second step, these consequences are evaluated
with respect to an agent-internal objective function. Final ly, the
action selection mechanism chooses an action depending on th e
preceding evaluation.
In active inference, these three elements are entangled.
On one hand, there is the main feature of active inference:
the combination of knowledge updating and action selection
into a single mechanism. This single mechanism is the
minimization of a “variational free energy” (
Friston et al.,
2015, p. 188). The “inference” part of the name is justiﬁed
by the formal resemblance of the method to the variational
free energy minimization (also known as evidence lower
bound maximization) used in variational inference. Variat ional
inference is a way to turn Bayesian inference into an optimiza tion
problem which gives rise to an approximate Bayesian inference
method (
Wainwright and Jordan, 2007 ). The “active” part is
justiﬁed by the fact that the output of this minimization is a
probability distribution over actions from which the action s of
the agent are then sampled. Behaviour in active inference is th us
the result of a variational inference-like process. On the ot her
hand, the function (i.e., expected free energy) that induces the
objective function in active inference is said to be “of the s ame
form” as the variational free energy (
Friston et al., 2017a, p. 2673)
or even to “follow” from it ( Friston et al., 2016b, p. 10). This
suggests that expected free energy is the only objective func tion
compatible with active inference.
In summary, perception and action in active inference
intertwines four elements: variational approximation, infe rence,
action selection, and an objective function. Besides these
formal features, active inference is of particular interest for
its claims on biological plausibility and its relationship to t he
thermodynamics of dissipative systems. According to
Friston
et al. (2012, Section 3) active inference is a “corollary” to the free
energy principle. Therefore, it is claimed, actions must minim ize
variational free energy to resist the dispersion of states of se lf-
organizing systems (see also Friston, 2013b; Allen and Friston,
2016). Active inference has also been used to reproduce a range
of neural phenomena in the human brain ( Friston et al., 2016b ),
and the overarching free energy principle has been proposed as a
“uniﬁed brain theory”
Friston (2010). Furthermore, the principle
has been used in a hierarchical formulation as theoretical
underpinning of the predictive processing framework (
Clark,
2015, p. 305–306), successfully explaining a wide range of
cognitive phenomena. Of particular interest for the present
special issue, the representation of probabilities in the acti ve
inference framework is conjectured to be related to aspects o f
consciousness (
Friston, 2013a; Linson et al., 2018 ).
These strong connections between active inference and
biology, statistical physics, and consciousness research mak e the
method particularly interesting for the design of artiﬁcial agents
that can interact with- and learn about unknown environment s.
However, it is currently not clear to which extent active infe rence
allows for modiﬁcations. We ask: how far do we have to commit
to the precise combination of elements used in the literature, and
what becomes interchangeable?
One target for modiﬁcations is the objective
function. In situations where the environment does not
provide a speciﬁc reward signal and the goal of the agent is
not directly speciﬁed, researchers often choose the objectiv e
function from a range of intrinsic motivations . The concept of
intrinsic motivation was introduced as a psychological conc ept
by
Ryan and Deci (2000) , and is deﬁned as “the doing of
an activity for its inherent satisfactions rather than for s ome
separable consequence.” The concept helps us to understand
one important aspect of consciousness: the assignment of aﬀect
to certain experiences, e.g., the experience of fun (
Dennett,
1991) when playing a game. Computational approaches to
intrinsic motivations ( Oudeyer and Kaplan, 2009; Schmidhuber,
2010; Santucci et al., 2013 ) can be categorized roughly by the
Frontiers in Neurorobotics | www.frontiersin.org 2 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
psychological motivations they are imitating, e.g., drives to
manipulate and explore, the reduction of cognitive dissonanc e,
the achievement of optimal incongruity, and ﬁnally motivati ons
for eﬀectance, personal causation, competence and self-
determination. Intrinsic motivations have been used to enh ance
behaviour aimed at extrinsic rewards (
Sutton and Barto, 1998 ),
but their deﬁning characteristic is that they can serve as a
goal-independent motivational core for autonomous behavio ur
generation. This characteristic makes them good candidate s for
the role of value functions for the design of intelligent sys tems
(
Pfeifer et al., 2005 ). We attempt to clarify how to modify
active inference to accommodate objective functions based on
diﬀerent intrinsic motivations. This may allow future studi es
to investigate whether and how altering the objective funct ion
aﬀects the biological plausibility of active inference.
Another target for modiﬁcation, originating more from a
theoretical standpoint, is the variational formulation of a ctive
inference. As mentioned above, variational inference form ulates
Bayesian inference as an optimization problem; a family of
probability distributions is optimized to approximate the dire ct,
non-variational Bayesian solution. Active inference is fo rmulated
as an optimization problem as well. We consequently ask: is
active inference the variational formulation of a direct (n on-
variational) Bayesian solution? Such a direct solution wou ld
allow a formally simple formulation of active inference with out
recourse to optimization or approximation methods, at the cost
of sacriﬁcing tractability in most scenarios.
To explore these questions, we take a step back from
the established formalism, gradually extend the active
inference framework, and comprehensively reconstruct the
version presented in
Friston et al. (2015) . We disentangle
the four components of approximation, inference, action
selection, and objective functions that are interwoven in a ctive
inference.
One of our ﬁndings, from a formal point of view, is
that expected free energy can be replaced by other intrinsic
motivations. Our reconstruction of active inference then y ields
a uniﬁed formal framework that can accommodate:
• Direct, non-variational Bayesian inference in combinatio n
with standard action selection schemes known from
reinforcement learning as well as objective functions
induced by intrinsic motivations.
• Universal reinforcement learning through a special choice of
the environment model and a small modiﬁcation of the action
selection scheme.
• Variational inference in place of the direct Bayesian approach .
• Active inference in combination with objective functions
induced by intrinsic motivations.
We believe that our framework can beneﬁt active inference
research as a means to compare the dynamics induced by
alternative action selection principles. Furthermore, it eq uips
researchers on intrinsic motivations with additional ways for
designing agents that share the biological plausibility of ac tive
inference.
Finally, this article contributes to the research topic:
Consciousness in Humanoid Robots, in several ways. First, th ere
have been numerous claims on how active inference relates to
consciousness or related qualities, which we outlined earl ier in
the introduction. The most recent work by
Linson et al. (2018) ,
also part of this research topic, speciﬁcally discusses this re lation,
particularly in regards to assigning salience. Furthermore ,
intrinsic motivations (including the free energy principle f or this
argument) have a range of properties that relate to or are useful to
a range of classical approaches recently summarized as as Good
Old-Fashioned Artiﬁcial Consciousness (GOFAC,
Manzotti and
Chella, 2018 ). For example, embodied approaches still need
some form of value-function or motivation ( Pfeifer et al., 2005 ),
and beneﬁt from the fact that intrinsic motivations are usuall y
universal yet sensitive in regards to an agent’s embodiment .
The enactive AI framework ( Froese and Ziemke, 2009 ), another
candidate for GOFAC, proposes further requirements on how
value underlying motivation should be grounded in constitut ive
autonomy and adaptivity.
Guckelsberger and Salge (2016)
present tentative claims on how empowerment maximization
relates to these requirements in biological systems, and how it
could contribute to realizing them in artiﬁcial ones. Final ly, the
idea of using computational approaches for intrinsic motivati on
goes back to developmental robotics (
Oudeyer et al., 2007 ),
where it is suggested as way to produce a learning and adapting
robot, which could oﬀer another road to robot consciousness.
Whether these Good Old-Fashioned approaches will ultimately
be successful is an open question, and
Manzotti and Chella (2018)
asses them rather critically. However, extending active in ference
to alternative intrinsic motivations in a uniﬁed framework allows
to combine features of these two approaches. For example it may
bring together the neurobiological plausibility of active in ference
and the constitutive autonomy aﬀorded by empowerment.
2. RELATED WORK
Our work is largely based on
Friston et al. (2015) and we
adopt the setup and models from it. This means many of our
assumptions are due to the original paper. Recently,
Buckley
et al. (2017) have provided an overview of continuous-variable
active inference with a focus on the mathematical aspects, ra ther
than the relationship to thermodynamic free energy, biolog ical
interpretations or neural correlates. Our work here is in as
similar spirit but focuses on the discrete formulation of act ive
inference and how it can be decomposed. As we point out in
the text, the case of direct Bayesian inference with separate
action selection is strongly related to general reinforcem ent
learning (
Hutter, 2005; Leike, 2016; Aslanides et al., 2017 ).
This approach also tackles unknown environments with- and
in later versions also without externally speciﬁed reward in a
Bayesian way. Other work focusing on unknown environments
with rewards are e.g., (
Ross and Pineau, 2008; Doshi-Velez et al.,
2015). We would like to stress that we do not propose agents
using Bayesian or variational inference as competitors to an y
of the existing methods. Instead, our goal is to provide an
unbiased investigation of active inference with a particula r focus
on extending the inference methods, objective functions an d
action-selection mechanisms. Furthermore, these agents f ollow
Frontiers in Neurorobotics | www.frontiersin.org 3 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
almost completely in a straightforward (if quite involved) w ay
from the model in Friston et al. (2015) . A small diﬀerence is
the extension to parameterizations of environment and senso r
dynamics. These parameterizations can be found in Friston et al.
(2016b).
We note that work on planning as inference ( Attias, 2003;
Toussaint, 2009; Botvinick and Toussaint, 2012 ) is generally
related to active inference. In this line of work the probabil ity
distribution over actions or action sequences that lead to a given
goal speciﬁed as a sensor value is inferred. Since active infe rence
also tries to obtain a probability distribution over actions the
approaches are related. The formalization of the goal however
diﬀers, at least at ﬁrst sight. How exactly the two approaches
relate is beyond the scope of this publication.
3. STRUCTURE OF THIS ARTICLE
Going forward, we will ﬁrst outline our mathematical notati on
in Section 4. We then introduce the perception-action loop,
which contains both agent and environment in Section 5. In
Section 6 we introduce the model used by
Friston et al. (2015) .
We then show how to obtain beliefs about the consequences of
actions via both (direct) Bayesian inference (Section 6.2) and
(approximate) variational inference (Section 6.4). These be liefs
are represented in the form of a set of complete posteriors. Such
a set is a common object but usually does not play a prominent
role in Bayesian inference. Here, it turns out to be a conveni ent
structure for capturing the agent’ knowledge and describing
intrinsic motivations. Under certain assumptions that we dis cuss
in Section 6.3 the direct Bayesian case specializes to the bel ief
updating of the Bayesian universal reinforcement learning a gent
of
Aslanides et al. (2017) . We then discuss in Section 7 how those
beliefs (i.e., the set of complete posteriors) can induce acti on-
value functions (playing the role of objective functions) vi a a
given intrinsic motivation function. We present standard (i .e.,
non-active inference) ways to select actions based on such a ction-
value functions. Then we look at diﬀerent instances of intrin sic
motivation functions. The ﬁrst is the “expected free energy” of
active inference. For this we explicitly show how our formalis m
produces the original expression in Friston et al. (2015) . Looking
at the formulations of other intrinsic motivations it becom es clear
that the expected free energy relies on expressions quite simi lar
or identical to those that occur in other intrinsic motivati ons.
This suggests that, at least in principle, there is no reason wh y
active inference should only work with expected free energy a s
an intrinsic motivation. Finally, in Section 8 formulate ac tive
inference for arbitrary action-value functions which incl ude
those induced by intrinsic motivations. Modifying the gener ative
model of Section 6.1 and looking at the variational approximat ion
of its posterior comes close but does not correspond to the
original active inference of
Friston et al. (2015) . We explain the
additional trick that is needed.
In the Appendix we provide some more detailed calculations
as well as notation translation tables ( Appendix C) from our own
to those of Friston et al. (2015) and Friston et al. (2016b) .
4. NOTATION
We will explain our notation in more detail in the text,
but for readers that mostly look at equations we give a
short summary. Note that, Appendix C comprises a translation
between
Friston et al. (2015, 2016b) and the present notation.
Mostly, we will denote random variables by upper case letters e.g .,
X, Y, A, E, M, S, ... their state spaces by calligraphic upper case
letters X , Y, A, E, M, S..., speciﬁc values of random variables
which are elements of the state spaces by lower case letters
x, y, a, e, m, s, .... An exception to this are random variables that
act as parameters of probability distributions. For those, we u se
upper case Greek letters /Xi1, /Phi1, /Theta1, ..., for their usually continuous
state spaces we use /Delta1/Xi1, /Delta1/Theta1, /Delta1/Phi1, ... and for speciﬁc values the
lower case Greek letters ξ , φ, θ , .... In cases where a random
variable plays the role of an estimate of another variable X, we
write the estimate as ˆX, its state space as ˆX and its values as ˆx.
We distinguish diﬀerent types of probability distributions
with letters p, q, r, and d. Here, p corresponds to probability
distributions describing properties of the physical world
including the agent and its environment, q identiﬁes
model probabilities used by the agent internally, r denotes
approximations of such model probabilities which are also
internal to the agent, and d denotes a probability distributio n
that can be replaced by a q or a r distribution. We write
conditional probabilities in the usual way, e.g., p( y|x). For a
model of this conditional probability parameterized by θ , we
write q( ˆy|ˆx, θ ).
5. PERCEPTION-ACTION LOOP
In this section we introduce an agent’s perception-action loo p
(PA-loop) as a causal Bayesian network. This formalism forms th e
basis for our treatment of active inference. The PA-loop sho uld be
seen as specifying the (true) dynamics of the underlying physic al
system that contains agent and environment as well as their
interactions. In Friston’s formulation, the environment dynamics
of the PA-loop are referred to as the generative process. In general
these dynamics are inaccessible to the agent itself. Nonethe less,
parts of these (true) dynamics are often assumed to be known to
the agent in order to simplify computation (see e.g.,
Friston et al.,
2015). We ﬁrst formally introduce the PA-loop as causal Bayesian
network, and then state speciﬁc assumptions for the rest of thi s
article.
5.1. PA-loop Bayesian Network
Figure 1 shows an agent’s PA-loop, formalized as causal
Bayesian network. The network describes the following caus al
dependencies over time: At t = 0 an initial environment state
e0 ∈ E leads to an initial sensor value s0 ∈ S. This sensor value
inﬂuences the memory state m1 ∈ M of the agent at time t = 1.
Depending on this memory state, action a1 ∈ A is performed
which inﬂuences the transition of the environment state fro m
e0 to e1 ∈ E. The new environment state leads to a new sensor
value s1 which, together with the performed action a1 and the
memory state m1, inﬂuence the next memory state m2. The loop
then continues in this way until a ﬁnal time step T.
Frontiers in Neurorobotics | www.frontiersin.org 4 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
FIGURE 1 |First two time steps of the Bayesian network representing th e
perception-action loop (PA-loop). All subsequent time ste ps are identical to the
one from time t = 1 to t = 2.
We assume that all variables are ﬁnite and that the PA-loop is
time-homogeneous1. We exclude the ﬁrst transition from t = 0
to t = 1 from the assumption of time-homogeneity in order
to avoid having to pick an arbitrary action which precedes the
investigated time-frame. The ﬁrst transition is thus simpli ﬁed
to p( m1|s0, a0) : = p(m1|s0). Under the assumption of time-
homogeneity and the causal dependencies expressed in Figure 1,
the joint probability distribution over the entire PA-loop is
deﬁned by:
p(e0:T, s0:T, a1:T, m1:T) =
( T∏
t=1
p(at|mt) p(mt|st−1, at−1) p(st|et)
× p(et|at, et−1)
)
p(s0|e0) p(e0) (1)
where e0:T is shorthand for states ( e0, e1, . . . , eT). In order to
completely determine this distribution we therefore have to
specify the state spaces E, S, A, and M as well as the following
probabilities and mechanisms for all e0, et, et+1 ∈ E; s0, st ∈
S; at, at+1 ∈ A; m1, mt, mt+1 ∈ M for t > 0:
• Initial environment distribution: p( e0),
• Environment dynamics: p( et+1|at+1, et),
• Sensor dynamics: p( st|et),
• Action generation: p( at|mt),
• Initial memory step p( m1|s0),
• Memory dynamics: p( mt+1|st, at, mt).
In the following we will refer to a combination of initial
environment distribution, environment dynamics, and sens or
dynamics simply as an environment. Similarly, an agent is
a particular combination of initial memory step, memory
dynamics, and action generation. The indexing convention w e
use here is identical to the one used for the generative model (see
Section 6.1) in
Friston et al. (2015) .
Also, note the dependence of Mt on St−1, Mt−1, and
additionally At−1 in Figure 1. In the literature, the dependence
1This means that all state spaces and transition probabilities are ind ependent of
the time step, e.g., Mt = Mt−1 and p(st|et) = p(st−1|et−1).
on At−1 is frequently not allowed ( Ay et al., 2012; Ay and
Löhr, 2015 ). However, we assume an eﬀerence-like update
of the memory. Note that this dependence in addition to
the dependence on mt−1 is only relevant if the actions are
not deterministic functions of the memory state 2. If action
selection is probabilistic, knowing the outcome at−1 of the
action generation mechanism p( at−1|mt−1) will convey more
information than only knowing the past memory state mt−1.
This additional information can be used in inference about
the environment state and fundamentally change the intrins ic
perspective of an agent. We do not discuss these changes
in more detail here but the reader should be aware of the
assumption.
In a realistic robot scenario, the action at, if it is to be known
by the agent, can only refer to the “action signal” or “action value”
that is sent to the robot’s physical actuators. These actuator s will
usually be noisy and the robot will not have access to the ﬁnal
eﬀect of the signal it sends. The (noisy) conversion of an acti on
signal to a physical conﬁguration change of the actuator is he re
seen as part of the environment dynamics p( et|at, et−1). Similarly,
the sensor value is the signal that the physical sensor of the rob ot
produces as a result of a usually noisy measurement, so just like
the actuator, the conversion of a physical sensor conﬁguration to
a sensor value is part of the sensor dynamics p( st|et) which in
turn belongs to the environment. As we will see later, the act ions
and sensor values must have well-deﬁned state spaces A and S for
inference on an internal model to work. This further justiﬁes this
perspective.
5.2. Assumptions
For the rest of this article we assume that the environment st ate
space E, sensor state space S as well as environment dynamics
p(et+1|at+1, et) and sensor dynamics p( st|et) are arbitrarily ﬁxed
and that some initial environmental state e0 is given. Since we
are interested in intrinsic motivations, our focus is not on speciﬁc
environment or sensor dynamics but almost exclusively on act ion
generation mechanisms of agents that rely minimally on the
speciﬁcs of these dynamics.
In order to focus on action generation, we assume that all
the agents we deal with here have the same memory dynamics.
For this, we choose a memory that stores all past sensor values
s≺t = (s0, s1, ..., st−1) and actions a≺t = (a1, a2, ..., at−1) in the
memory state mt. This type of memory is also used in
Friston
et al. (2015, 2016b) and provides the agent with all existing data
about its interactions with the environment. In this respect , it
could be called a perfect memory. At the same time, whatever th e
agent learned from s≺t and a≺t that remains true based on the
next time step’s s⪯ t+1 and a⪯ t+1 must be relearned from scratch
by the agent. A more eﬃcient memory use might store only a
suﬃcient statistic of the past data and keep reusable results of
computations in memory. Such improvements are not part of this
article (see e.g.,
Fox and Tishby, 2016, for discussion).
Formally, the state space M of the memory is the set of all
sequences of sensor values and actions that can occur. Since there
2In the deterministic case there is a function f : M → A such that
p(mt|st−1, at−1, mt−1) = p(mt|st−1, f (mt−1), mt−1) = p(mt|st−1, mt−1).
Frontiers in Neurorobotics | www.frontiersin.org 5 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
is only a sensor value and no action at t = 0, these sequences
always begin with a sensor value followed by pairs of sensor
values and actions. Furthermore, the sensor value and actio n
at t = T are never recorded. Since we have assumed a time-
homogeneous memory state space M we must deﬁne it so that it
contains all these possible sequences from the start. Formal ly, we
therefore choose the union of the spaces of sequences of a ﬁxed
length (similar to a Kleene-closure):
M = S ∪
(T−1⋃
t = 1
S × (S × A)t
)
. (2)
With this we can deﬁne the dynamics of the memory as:
p(m1|s0) :=
{
1 if m1 = s0
0 else. (3)
p(mt|st−1, at−1, mt−1) :=
{
1 if mt = mt−1st−1at−1
0 else. (4)
This perfect memory may seem unrealistic and can cause
problems if the sensor state space is large (e.g., high resolut ion
images). However, we are not concerned with this type of
problem here. Usually, the computation of actions based on past
actions and sensor values becomes a challenge of eﬃciency lo ng
before storage limitations kick in: the necessary storage s pace
for perfect memory only increases linearly with time, while, as
we show later, the number of operations for Bayesian inferenc e
increases exponentially.
For completeness we also note how the memory dynamics
look if actions are a deterministic function f : M → A of the
memory state. Recall that in this case we can drop the edge from
At−1 to Mt in the PA-loop in Figure 1 and have at = f (mt) so
that we can deﬁne:
p(m1|s0) :=
{
1 if m1 = s0
0 else. (5)
p(mt|st−1, mt−1) :=
{
1 if mt = mt−1st−1f (mt−1)
0 else. (6)
Given a ﬁxed environment and the memory dynamics, we only
have to deﬁne the action generation mechanism p( at|mt) to fully
specify the perception-action loop. This is the subject of the ne xt
two sections.
In order to stay as close to
Friston et al. (2015) as possible,
we ﬁrst explain the individual building blocks that can be
extracted from Friston’s active inference as described in
Friston
et al. (2015) . These are the variational inference and the action
selection. We then show how these two building blocks are
combined in the original formulation. We eventually levera ge
our separation of components to show how the action selection
component can be modiﬁed, and thus extend the active inferenc e
framework.
6. INFERENCE AND COMPLETE
POSTERIORS
Ultimately, an agent needs to select actions. Inference based o n
past sensor values and actions is only needed if it is relevant to
the action selection. Friston’s active inference approach pro mises
to perform action selection within the same inference step th at
is used to update the agent’s model of the environment. In this
section, we look at the inference component only and show how
an agent can update a generative model in response to observed
sensor values and performed actions.
The natural way of updating such a model is Bayesian
inference via Bayes’ rule. This type of inference leads to wha t we
call the complete posterior . The complete posterior represents all
knowledge that the agent can obtain about the consequences of
its actions from its past sensor values and actions. In Section 7 we
discuss how the agent can use the complete posterior to decide
what is the best action to take.
Bayesian inference as straightforward recipe is usually not
practical due to computational costs. The memory requirements
of the complete posterior update increases exponentially with
time and so does the number of operations needed to select
actions. To keep the computational tractable, we have to
limit ourselves to only use parts of the complete posterior.
Furthermore, since the direct expressions (even of parts) of
complete posteriors are usually intractable, approximations are
needed. Friston’s active inference is committed to variati onal
inference as an approximation technique. Therefore, we explai n
how variational inference can be used as an approximation
technique. Our setup for variational inference (generativ e model
and approximate posterior) is identical to the one in
Friston et al.
(2015), but in this section we ignore the inference of actions
included there. We will look at the extension to action infere nce
in Section 7.
In the perception-action loop in Figure 1, action selection
(and any inference mechanism used in the course of it) depends
exclusively on the memory state mt. As mentioned in Section 5,
we assume that this memory state contains all past sensor values
s≺t and all past actions a≺t. To save space, we write sa≺t : =
(s≺t, a≺t) to refer to both sensor values and actions. We then have:
mt = sa≺t. (7)
However, since it is more intuitive to understand inference
with respect to past sensor values and actions than in terms of
memory, we use sa≺t explicitly here in place of mt.
6.1. Generative Model
The inference mechanism, internal to the action selection
mechanism p( a|m), takes place on a hierarchical generative
model (or density, in the continuous case). “Hierarchical”
means that the model has parameters and hyperparameters, and
“generative” indicates that the model relates parameters and
latent variables, i.e., the environment state, as “generative” causes
to sensor values and actions as data in a joint distribution. The
generative model we investigate here is a part of the generati ve
model used in
Friston et al. (2015) . For now, we omit the
Frontiers in Neurorobotics | www.frontiersin.org 6 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
probability distribution over future actions and the “precis ion”,
which are only needed for active inference and are discussed later.
The generative models in Friston et al. (2016a,b, 2017a) are all
closely related.
Note that we are not inferring the causal structure of the
Bayesian network or state space cardinalities, but deﬁne the
generative model as a ﬁxed Bayesian network with the graph
shown in Figure 2. It is possible to infer the causal structure (see
e.g.,
Ellis and Wong, 2008 ), but in that case, it becomes impossible
to represent the whole generative model as a single Bayesian
network (
Ortega, 2011).
The variables in the Bayesian network in Figure 2 that
model variables occurring outside of p( a|m) in the perception-
action loop ( Figure 1), are denoted as hatted versions of their
counterparts. More precisely:
• ˆs ∈ ˆS = S are modelled sensor values,
• ˆa ∈ ˆA = A are modelled actions,
• ˆe ∈ ˆE are modelled environment states.
To clearly distinguish the probabilities deﬁned by the gener ative
model from the true dynamics, we use the symbol q instead
of p. In accordance with Figure 2, and also assuming time-
homogeneity, the joint probability distribution over all va riables
in the model until some ﬁnal modelled time ˆT is given by:
q(ˆe0:T, ˆs0:T, ˆa1:T, θ 1, θ 2, θ 3, ξ 1, ξ 2, ξ 3)
:=
( T∏
t = 1
q(ˆst|ˆet, θ 1) q(ˆet|ˆat, ˆet−1, θ 2) q(ˆat)
)
× q(ˆs0|ˆe0, θ 1) q(ˆe0|θ 3)
( 3∏
i = 1
q(θ i|ξ i) q(ξ i)
)
(8)
Here, θ 1, θ 2, θ 3 are the parameters of the hierarchical model, and
ξ 1, ξ 2, ξ 3 are the hyperparameters. To save space, we combine the
FIGURE 2 |Bayesian network of the generative model with parameters
/Theta1= (/Theta11, /Theta12, /Theta13) and hyperparameters /Xi1 = (/Xi11, /Xi12, /Xi13). Hatted variables
are models / estimates of non-hatted counterparts in the per ception-action
loop in Figure 1. An edge that splits up connecting one node to n nodes (e.g.,
/Theta12 to ˆE1, ˆE2, ...) corresponds to n edges from that node to all the targets
under the usual Bayesian network convention. Note that in co ntrast to the
perception-action loop in Figure 1, imagined actions ˆAt have no parents. They
are either set to past values or, for those in the future, a pro bability distribution
over them must be assumed.
parameters and hyperparameters by writing
θ := (θ 1, θ 2, θ 3) (9)
ξ := (ξ 1, ξ 2, ξ 3). (10)
To fully specify the generative model, or equivalently a
probability distribution over Figure 2, we have to specify the state
spaces ˆE, ˆS, ˆA and:
• q(ˆs|ˆe, θ 1) the sensor dynamics model,
• q(ˆe′|ˆa′, ˆe, θ 2) the environment dynamics model,
• q(ˆe0|θ 3) the initial environment state model,
• q(θ 1|ξ 1) the sensor dynamics prior,
• q(θ 2|ξ 2) the environment dynamics prior,
• q(θ 3|ξ 3) the initial environment state prior,
• q(ξ 1) sensor dynamics hyperprior,
• q(ξ 2) environment dynamics hyperprior,
• q(ξ 3) initial environment state hyperprior,
• ˆT last modelled time step,
• q(ˆat) for all t ∈ { 1, , ..., ˆT} the probability distribution over the
actions at time t.
The state spaces of the parameters and hyperparameters are
determined by the choice of ˆE, ˆS, ˆA. We will see in Section 6.2
that ˆS = S and ˆA = A should be chosen in order to use this
model for inference on past sensor values and actions. For ˆE it
is not necessary to set it equal to E for the methods described to
work. We note that if we set ˆE equal to the memory state space
of Equation (2) the model and its updates become equivalent
to those used by the Bayesian universal reinforcement learn ing
agent
Hutter (2005) in a ﬁnite (environment and time-interval)
setting (see Section 6.3).
The last modelled time step ˆT can be chosen as ˆT = T, but
it is also possible to always set it to ˆT = t + n, in which case
n speciﬁes a future time horizon from current time step t. Such
an agent would model a future that goes beyond the externally
speciﬁed last time step T. The dependence of ˆT on t (which we do
not denote explicitly) within p( a|m) is possible since the current
time step t is accessible from inspection of the memory state mt
which contains a sensor sequence of length t.
The generative model assumes that the actions are not
inﬂuenced by any other variables, hence we have to specify act ion
probabilities. This means that the agent does not model how
its actions come about, i.e., it does not model its own decisio n
process. Instead, the agent is interested in the (parameters o f) the
environment and sensor dynamics. It actively sets the probab ility
distributions over past and future actions according to its
needs. In practice, it either ﬁxes the probability distributio ns to
particular values (by using Dirac delta distributions) or to v alues
that optimize some measure. We look into the optimization
options in more detail later.
Note that the parameters and hyperparameters are standard
random variables in the Bayesian network of the model. Also, t he
rules for calculating probabilities according to this model are just
the rules for calculating probabilities in this Bayesian net work.
In what follows, we assume that the hyperparameters are
ﬁxed as /Xi11 = ξ 1, /Xi12 = ξ 2, /Xi13 = ξ 3. The following
Frontiers in Neurorobotics | www.frontiersin.org 7 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
procedures (including both Bayesian and variational infere nce)
can be generalized to also infer hyperparameters. However, ou r
main reference ( Friston et al., 2015 ) and most publications on
active inference also ﬁx the hyperparameters.
6.2. Bayesian Complete Posteriors
During action generation [i.e., within p( a|m)] at time t, the agent
has retained all its previously perceived sensor states and it s
previously performed actions in memory. The “experience” or
data contained in its memory is thus mt = sa≺t. This data can be
plugged into the generative model to obtain posterior probabil ity
distributions over all non-observed random variables. Als o, the
model can estimate the not yet observed sensor values ˆst: ˆT, past
and future unobservable environment states ˆe0: ˆT, parameters θ
and hyperparameters ξ . These estimations are done by setting:
ˆAτ = aτ , for τ < t (11)
and
ˆSτ = sτ , for τ < t. (12)
as shown in Figure 3 for t = 2. For these assignments to be
generally possible, we need to choose ˆA and ˆS equal to A and S
respectively. The resulting posterior probability distribut ion over
all non-observed random variables is then, according to sta ndard
rules of calculating probabilities in a Bayesian network:
q(ˆst: ˆT, ˆe0: ˆT, ˆat: ˆT, θ |sa≺t, ξ )
:= q(s≺t, ˆst: ˆT, ˆe0: ˆT, a≺t, ˆat: ˆT, θ , ξ )
∫ ∑
ˆst: ˆT ,ˆe0: ˆT ,ˆat: ˆT
q(s≺t, ˆst: ˆT, ˆe0: ˆT, a≺t, ˆat: ˆT, θ , ξ ) dθ . (13)
Eventually, the agent needs to evaluate the consequences of its
future actions. Just as it can update the model with respect to pas t
actions and sensor values, the agent can update its evaluatio ns
with “contemplated” future action sequences ˆat: ˆT. For each such
FIGURE 3 |Internal generative model with plugged in data up to t = 2 with
ˆS0 = s0, ˆS1 = s1 and ˆA1 = a1 as well as from now on ﬁxed hyperparameters
ξ = (ξ 1, ξ 2, ξ 3). Conditioning on the plugged in data leads to the posterior
distribution q(ˆst: ˆT , ˆe0: ˆT , ˆat: ˆT , θ |sa≺t, ξ ). Predictions for future sensor values
can be obtained by marginalising out other random variables e.g., to predict
ˆS2 we would like to get q(ˆs2|s0, s1, a1, ξ ). Note however that this requires an
assumption for the probability distribution over ˆA2.
future action sequence ˆat: ˆT, the agent obtains a distribution over
the remaining random variables in the model:
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ )
:= q(s≺t, ˆst: ˆT, ˆe0: ˆT, a≺t, ˆat: ˆT, θ , ξ )∫ ∑
ˆst: ˆT ,ˆe0: ˆT
q(s≺t, ˆst: ˆT, ˆe0: ˆT, a≺t, ˆat: ˆT, θ , ξ ) dθ . (14)
We call each such distribution a Bayesian complete posterior . We
choose the term complete posterior since the “posterior” by itself
usually refers to the posterior distribution over the paramete rs
and latent variables q( θ , ˆet−1|sa≺t, ξ ) [we here call this a
posterior factor , see Equation (16)] and the posterior predictive
distributions marginalize out the parameters and latent var iables
to get q( ˆst: ˆT|ˆat: ˆT, sa≺t, ξ ). The complete posteriors are probability
distributions over all random variables in the generative m odel
including parameters, latent variables, and future variabl es. In
this sense the set of all (Bayesian) complete posteriors represe nts
the complete knowledge state of the agent at time t about
consequences of future actions after updating the model with
past actions and observed sensor values sa≺t. At each time step
the sequence of past actions and sensor values is extended from
sa≺t to sa≺t+1 (i.e., mt goes to mt+1) and a new set of complete
posteriors is obtained.
All intrinsic motivations discussed in this article evalua te
future actions based on quantities that can be derived from t he
corresponding complete posterior.
It is important to note that the complete posterior can be
factorized into a term containing the inﬂuence of past sensor
values and actions (data). This factorization can be made on the
parameters θ and ξ , the environment states ˆe≺t, predicted future
environment states ˆet: ˆT and sensor values ˆst: ˆT depending on the
future actions ˆat: ˆT, and the estimated environment state ˆet−1 and
θ . Using the conditional independence
SA≺t ⊥ ⊥ˆSt: ˆT, ˆEt: ˆT | ˆAt: ˆT, ˆEt−1, /Theta1, /Xi1, (15)
which can be identiﬁed (via d-separation;
Pearl, 2000 ) from the
Bayesian network in Figure 3, we can rewrite this as:
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ )
= q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ). (16)
This equation represents the desired factorization. This
formulation separates complete posteriors into a predictive and
a posterior factor. The predictive factor is given as part of the
generative model (Equation 8)
q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) =
ˆT∏
r=t
q(ˆsr|ˆer, θ 1) q(ˆer|ˆar, ˆer−1, θ 2) (17)
and does not need to be updated through calculations at diﬀeren t
time steps. This factor contains the dependence of the complete
posterior on future actions. This dependency reﬂects that, un der
the given generative model, the consequences of actions for ea ch
combination of /Theta1 and ˆEt−1 remain the same irrespective of
experience. What changes when a new action and sensor value
Frontiers in Neurorobotics | www.frontiersin.org 8 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
pair comes in is the distribution over the values of /Theta1 and ˆEt−1
and with them the expectations over consequences of actions.
On the other hand, the posterior factor must be updated
at every time step. In Appendix A, we sketch the computation
which shows that it involves a sum over |E|t elements. This
calculation is intractable as time goes on and one of the reas ons
to use approximate inference methods like variational infere nce.
Due to the above factorization, we may only need to
approximate the posterior factor q( ˆe≺t, θ |sa≺t, ξ ) and use the
exact predictive factor if probabilities involving future se nsor
values or environment states are needed.
This is the approach taken e.g., in Friston et al. (2015) .
However, it is also possible to directly approximate parts of
the complete posterior involving random variables in both
factors, e.g., by approximating q( ˆe0: ˆT, θ 1|ˆat: ˆT, sa≺t, ξ ). This latter
approach is taken in
Friston et al. (2016b) and we see it again in
Equation (43) but in this publication the focus is on the forme r
approach.
In the next section, we look at the special case of universal
reinforcement learning before we go on to variational infer ence
to approximate the posterior factor of the (Bayesian) complete
posteriors.
6.3. Connection to Universal
Reinforcement Learning
In this section, we relate the generative model of Equation ( 8) and
its posterior predictive distribution to those used by the Bay esian
universal reinforcement learning agent. Originally, this a gent is
deﬁned by
Hutter (2005). More recent work includes Leike (2016)
and (for the current purpose suﬃcient and particularly relevan t)
Aslanides et al. (2017) .
Let us set ˆE = M with M as in Equation (2) and let the agent
identify each past sa≺t with a state of the environment, i.e.,
ˆet−1 = sa≺t. (18)
Under this deﬁnition the next environment state ˆet is just the
concatenation of the last environment state sa≺t with the next
next action selected by the agent ˆat and the next sensor value ˆst:
ˆet = ˆsˆa⪯ t = sa≺tˆsˆat. (19)
So given a next contemplated action ¯
ˆat the next environment
state ˆet is already partially determined. What remains to be
predicted is only the next sensor value ˆst. Formally, this is
reﬂected in the following derivation:
q(ˆet|¯
ˆat, ˆet−1, θ 2) := q(ˆst, ˆat, ˆsˆa≺t|¯
ˆat, sa≺t, θ 2) (20)
= q(ˆst|ˆat, ˆsˆa≺t, ¯
ˆat, sa≺t, θ 2) q(ˆat, ˆsˆa≺t|¯
ˆat, sa≺t, θ 2)
(21)
= q(ˆst|ˆat, ˆsˆa≺t, ¯
ˆat, sa≺t, θ 2)δ¯
ˆat (ˆat)δsa≺t (ˆsˆa≺t) (22)
= q(ˆst|¯
ˆat, sa≺t, θ 2)δ¯
ˆat (ˆat)δsa≺t (ˆsˆa≺t). (23)
This shows that in this case the model of the next environment
state (the left hand side) is determined by the model of the ne xt
sensor value q( ˆst|¯
ˆat, sa≺t, θ 2).
So instead of carrying a distribution over possible models of
the next environment state such an agent only needs to carry a
distribution over models of the next sensor value. Furthermor e,
an additional model q( ˆs|ˆe, θ 1) of the dependence of the sensor
values on environment states parameterized by θ 1 is superﬂuous.
The next predicted sensor value is already predicted by the mod el
q(ˆst|ˆat, sa≺t, θ 2). It is therefore possible to drop the parameter θ 1.
The parameter θ 3, for the initial environment state
distribution, becomes a distribution over the initial sensor
value since ˆe0 = ˆs0:
q(ˆe0|θ 3) = q(ˆs0|θ 3). (24)
We can then derive the posterior predictive distribution and
show that it coincides with the one given in
Aslanides et al.
(2017). For the complete posterior of Equation (16) we ﬁnd:
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ )
= q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) (16 revisited)
= q(ˆet: ˆT|ˆst: ˆT, ˆat: ˆT, ˆet−1, θ ) q(ˆst: ˆT|ˆat: ˆT, ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ )
(25)
= q(ˆst: ˆT|ˆat: ˆT, sa≺t, θ ) q(θ |sa≺t, ξ )
×
t∏
τ =0
δsa≺τ (ˆeτ )
ˆT∏
τ =t+1
δsa≺tˆsˆat : τ (ˆeτ ). (26)
To translate this formulation into the notation of Aslanides et al.
(2017) ﬁrst drop the representation of the environment state
which is determined by the sensor values and actions anyway.
This means that the complete posterior only needs to predict
future sensor values and parameters. Formally, this means th e
complete posterior can be replaced without loss of generality:
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) → q(ˆst: ˆT|ˆat: ˆT, sa≺t, θ ) q(θ |sa≺t, ξ ).
(27)
To translate notations let θ → ν; ˆa, a → a; ˆs, s → e. Also, set
ˆT → t because only one step futures are considered in universal
reinforcement learning (this is due to the use of policies inst ead
of future action sequences). Then, the equation for the poster ior
predictive distribution
q(ˆst|ˆat, sa≺t, ξ ) =
∫
q(ˆst|ˆat, sa≺t, θ ) q(θ |sa≺t, ξ ) dθ , (28)
is equivalent to
Aslanides et al. (2017, Equation 5) (the sum
replaces the integral for a countable /Delta1/Theta1):
ξ (e|ae≺t, a) =
∑
ν
p(e|ν, ae≺t, a)p(ν|ae≺t) (29)
⇔ ξ (e) =
∑
ν
p(e|ν)p(ν), (30)
where we dropped the conditioning on ae≺t, a from the notation
in the second line as done in the original (where this is claime d
to improve clarity). Also note that ξ (e) would be written q( e|ξ ) in
Frontiers in Neurorobotics | www.frontiersin.org 9 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
our notation. In the universal reinforcement learning lite rature
parameters like θ (or ν) and ξ are sometimes directly used to
denote the probability distribution that they parameterize.
Updating of the posterior q( θ |sa≺t, ξ ) in response to new data
also coincides with updating of the weights p(ν):
q(θ |sa⪯ t, ξ ) = q(θ , st|at, sa≺t, ξ )
q(st|at, sa≺t, ξ ) (31)
= q(st|at, sa≺t, θ , ξ ) q(θ |at, sa≺t, ξ )
q(st|at, sa≺t, ξ ) (32)
= q(st|at, sa≺t, θ ) q(θ |sa≺t, ξ )
q(st|at, sa≺t, ξ ) (33)
= q(st|at, sa≺t, θ )
q(st|at, sa≺t, ξ ) q(θ |sa≺t, ξ ). (34)
The ﬁrst two lines are general. From the second to third we use d
St ⊥ ⊥/Xi1|At, SA≺t, /Theta1 (35)
and
/Theta1 ⊥ ⊥At|SA≺t, /Xi1 (36)
which follow from the Bayesian network structure Figure 2. In
the notation of Aslanides et al. (2017) Equation (34) becomes
p(ν|e) = p(e|ν)
p(e) p(ν). (37)
This shows that assuming the same model class /Delta1/Theta1 the
predictions and belief updates of an agent using the Bayesian
complete posterior of Section 6.2 are the same as those of
the Bayesian universal reinforcement learning agent. Acti on
selection can then be performed just as in
Aslanides et al.
(2017) as well. This is done by selecting policies. In the present
publication we instead select action sequences directly. Ho wever,
in both cases the choice maximizes the value predicted by the
model. More on this in Section 7.2.
6.4. Approximate Complete Posteriors
As mentioned in the last section, the complete posterior can
be approximated via variational inference (see
Attias, 1999;
Winn and Bishop, 2005; Bishop, 2011; Blei et al., 2017 ). There
are alternative methods such as belief propagation, expectati on
propagation ( Minka, 2001; Vehtari et al., 2014 ), and sampling-
based methods ( Lunn et al., 2000; Bishop, 2011 ), but active
inference commits to variational inference by framing infe rence
as variational free energy minimization ( Friston et al., 2015 ).
Variational free energy (Equation 45) is just the negative e vidence
lower bound (ELBO) of standard variational inference (e.g. ,
Blei et al., 2017 ). In the following, we show how the complete
posterior can be approximated via variational inference.
The idea behind variational inference is to use a simple famil y
of probability distributions and identify the member of that
family which approximates the true complete posterior best.
This turns inference into an optimization problem. According
to
Wainwright and Jordan (2007) this reformulation as an
optimization problem is the essence of variational methods.
If the family of distributions is chosen such that it include s
the complete posterior then the optimization will eventually
lead to the same result as Bayesian inference. However, one
advantage of the formulation as an optimization is that it can
also be performed over a family of probability distributions
that is simpler than the family that includes the actual
complete posterior. This is what turns variational inference
into an approximate inference procedure. Usually, the (simpler )
families of probability distributions are chosen as products of
independent distributions.
Recalling Equation (16), the complete posterior as a product
of a predictive and a posterior factor is:
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ )
= q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ). (16 revisited)
This product is the main object of interest. We want to
approximate the formula with a probability distribution that le ts
us (tractably) calculate the posteriors required by a given in trinsic
motivation, which can consequently be used for action select ion.
As mentioned before, to approximate the complete posterior
we here approximate only the posterior factor and use the given
generative model’s predictive factor as is done in
Friston et al.
(2015)3 The approximate posterior factor is then combined with
the exact predictive factor to get the approximate complete
posterior. Let us write r( ˆe≺t, θ |φ) for the approximate posterior
factor (Figure 4), deﬁned as:
r(ˆe≺t, θ |φ) := r(ˆe≺t|φE≺t ) r(θ |φ) (38)
:=
t−1∏
τ = 0
r(ˆeτ |φEτ )
3∏
i = 1
r(θ i|φi). (39)
As we can see it models each of the random variables that the
posterior factor ranges over as independent of all others. Thi s
is called a mean ﬁeld approximation. Then, the approximate
complete posterior ( Figure 5) is:
r(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, φ) := q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) r(ˆe≺t, θ |φ).
(40)
Note that the variational parameter absorbs the hyperparamete r
ξ as well as the past sensor values and actions sa≺t. The parameter
does not absorb future actions which are part of the predictive
factor. The dependence on future actions needs to be kept if we
want to select actions using the approximate complete posterior.
3A close inspection of Friston et al. (2015, Equation 9) shows that the approximate
complete posterior that ends up being evaluated by the action-value function
is the one we discuss in Equation (40). It uses the predictive fa ctor to get
the probabilities r( ˆet: ˆT |ˆat: ˆT , ˆet−1, φ) of future environment states. However, the
approximate posterior in Friston et al. (2015, Equation 10) uses a factorization
of all future environment states like the one we give in Equation (4 3). The
probabilities of future environment states in that posterior are not u sed anywhere
in Friston et al. (2015) . In principle, they could be used as is done in Friston et al.
(2016b, Equation 2.6) where the complete posterior of Equation (43) is use d in the
action-value function. Both approaches are possible.
Frontiers in Neurorobotics | www.frontiersin.org 10 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
FIGURE 4 |Bayesian network of the approximate posterior factor at t = 2.
The variational parameters /Phi11, /Phi12, /Phi13, and /Phi1E≺t = (/Phi1E0 , /Phi1E1 ) are positioned
so as to indicate what dependencies and nodes they replace in the generative
model in Figure 2.
FIGURE 5 |Bayesian network of the approximate complete posterior of
Equation (40) at t = 2 for the future actions ˆat: ˆT . Only ˆEt−1, /Theta11, /Theta12 and the
future action ˆat: ˆT appear in the predictive factor and inﬂuence future variable s.
In general there is one approximate complete posterior for ea ch possible
sequence ˆat: ˆT of future actions.
We have:
r(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, φ) ≈ q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) (41)
if
r(ˆe≺t, θ |φ) ≈ q(ˆe≺t, θ |sa≺t, ξ ). (42)
This approximation can be achieved by standard variational
inference methods.
For those interested more in the approximation of the
complete posterior as in
Friston et al. (2016b) , we provide the
used family of factorized distributions. It must be noted th at
the agent in this case carries a separate approximate posterior
for each possible complete action sequence ˆa0:T. For predictions
of environment states, it does not use the predictive factor, but
instead looks at the set of generative models compatible with t he
past. For each of those, the agent considers all environment s tates
at diﬀerent times as independent. The approximate posteriors,
compatible with a past sequence of actions a≺t, are of the
form:
r(ˆst: ˆT, ˆe0: ˆT, θ 1|ˆat: ˆT, a≺t, φ1)
= q(ˆst: ˆT|ˆet: ˆT, θ 1)
ˆT∏
τ = 0
r(ˆeτ |ˆat: ˆT, a≺t, φEτ ) r(θ 1|φ1). (43)
Note also that the relation between sensor values and
environment states is still provided by the generative model s’
sensor dynamics q( ˆst: ˆT|ˆet: ˆT, θ 1). In this article however, we focus
on the approach in
Friston et al. (2015) which requires only one
approximate posterior at time t since future actions only occur
in the predictive factors which we do not approximate.
We deﬁne the relative entropy (or KL-divergence) between the
approximate and the true posterior factor:
KL[r(ˆE≺t, /Theta1|φ)|| q(ˆE≺t, /Theta1|sa≺t, ξ )]
:=
∑
ˆe≺t
∫
r(ˆe≺t, θ |φ) log r(ˆe≺t, θ |φ)
q(ˆe≺t, θ |sa≺t, ξ ) dθ . (44)
Note that, we indicate the variables that are summed over by
capitalizing them. The KL-divergence quantiﬁes the diﬀerenc e
between the two distributions. It is non-negative, and only zero
if the approximate and the true posterior factor are equal (see
e.g.,
Cover and Thomas, 2006 ).
The variational free energy, also known as the (negative)
evidence lower bound (ELBO) in variational inference liter ature,
is deﬁned as:
F[ξ , φ, sa≺t] :=
∑
ˆe≺t
∫
r(ˆe≺t, θ |φ) log r(ˆe≺t, θ |φ)
q(s⪯ t, ˆe≺t, θ |a≺t, ξ ) dθ
(45)
= − log q(s≺t|a≺t, ξ )
+ KL[r(ˆE≺t, /Theta1|φ)|| q(ˆE≺t, /Theta1|sa≺t, ξ )] (46)
The ﬁrst term in Equation (46) is the surprise of negative
log evidence. For a ﬁxed hyperparameter ξ it is a constant.
Minimizing the variational free energy therefore directly
minimizes the KL-divergence between the true and the
approximate posterior factor given sa≺t and ξ .
In our case, variational inference amounts to solve the
optimization problem:
φ∗
sa≺t,ξ := arg min
φ
F[φ, sa≺t, ξ ]. (47)
This optimization is a standard problem. See
Bishop (2011) and
Blei et al. (2017) for ways to solve it.
The resulting variational parameters φ∗
sa≺t,ξ =
(φE0
sa≺t,ξ , ..., φEt−1
sa≺t,ξ , φ1sa≺t,ξ , φ2sa≺t,ξ , φ3sa≺t,ξ ) deﬁne the
approximate posterior factor. The variational parameters,
together with the exact predictive factors, allow us to comput e
the approximate complete posteriors for each sequence of future
actions ˆat: ˆT:
r(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, φ∗
sa≺t,ξ )
= q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) r(ˆe≺t, θ |φ∗
sa≺t,ξ ) (48)
Frontiers in Neurorobotics | www.frontiersin.org 11 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
≈ q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ). (49)
In the next section, we look at action selection as the second
component of action generation. To this end, we show how to
evaluate sequences of future actions ˆat: ˆT by evaluating either
Bayesian complete posteriors or the approximate complete
posteriors.
7. ACTION SELECTION BASED ON
INTRINSIC MOTIVATIONS
7.1. Intrinsic Motivation and Action-Value
Functions
The previous section resulted in sets of Bayesian or approximat e
complete posteriors. Independently of whether a complete
posterior is the approximate or the Bayesian version, it represe nts
the entire knowledge of the agent about the consequences of t he
sequence of future actions ˆat: ˆT that is associated with it. In order
to evaluate sequences of future actions the agent can only re ly
on its knowledge which suggests that all such evaluations sh ould
depend solely on complete posteriors. One could argue that
the motivation might also depend directly on the memory state
containing sa≺t. We here take a position somewhat similar to the
one proposed by
Schmidhuber (2010) that intrinsic motivations
concerns the “learning of a better world model.” We consider
the complete posterior as the current world model and assume
that intrinsic motivations depend only on this model and not
on the exact values of past sensor values and actions. As we will
see this assumption is also enough to capture the three intrins ic
motivations that we discuss here. This level of generality is
suﬃcient for our purpose of extending the free energy principle.
Whether it suﬃcient for a ﬁnal and general intrinsic motivat ion
deﬁnition is beyond the scope of this publication.
Complete posteriors are essentially conditional probability
distributions over ˆS ˆT−t+1 × ˆE ˆT+1 × /Delta1/Theta1 given elements
of ˆA ˆT−t+1. A necessary (but not suﬃcient) requirement
for intrinsic motivations in our context (agents with
generative models) is then that they are functions on
the space of such conditional probability distributions.
Let /Delta1 ˆS ˆT−t+1× ˆE ˆT+1×/Delta1/Theta1 | ˆA ˆT−t+1 be the space of conditional
probability distributions over ˆS ˆT−t+1 × ˆE ˆT+1 × /Delta1/Theta1 given
elements of ˆA ˆT−t+1. Then an intrinsic motivation is a function
M : /Delta1 ˆS ˆT−t+1× ˆE ˆT+1×/Delta1/Theta1 | ˆA ˆT−t+1 × ˆA ˆT−t+1 → R taking a
probability distribution d(., ., . |.) ∈ /Delta1 ˆS ˆT−t+1× ˆE ˆT+1×/Delta1/Theta1 | ˆA ˆT−t+1
and a given future actions sequence ˆat: ˆT ∈ ˆA ˆT−t+1 to a real
value M(d(., ., .|.), ˆat: ˆT) ∈ R. We can then see that the Bayesian
complete posterior q( ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) for a ﬁxed past
sa≺t written as q(., ., . |., sa≺t, ξ ) provides such conditional
probability distribution. Similarly, every member of the fa mily
of distributions used to approximate the Bayesian complete
posterior via variational inference r( ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, φ) written
as r(., ., . |., φ) also provides such a conditional probability
distribution. It will become important when discussing acti ve
inference that the optimized value φ∗
sa≺t,ξ of the variational
parameters as well as any other value of the variational
parameters φ deﬁne an element with the right structure to be
evaluated together with a set of future actions by an intrins ic
motivation function.
Using intrinsic motivation functions we then deﬁne two kind s
of induced action-value functions. These are similar to valu e
functions in reinforcement learning 4 The ﬁrst is the Bayesian
action-value function (or functional):
ˆQ(ˆat: ˆT, sa≺t, ξ ) := M(q(., ., .|., sa≺t, ξ ), ˆat: ˆT). (50)
In words the Bayesian action-value function ˆQ(ˆat: ˆT, sa≺t, ξ )
infers the set of Bayesian complete posteriors of past experience
sa≺t and then evaluates the sequence of future actions ˆat: ˆT
according to the intrinsic motivation function M.
The variational action-value function is deﬁned as 5:
ˆQ(ˆat: ˆT, φ) := M(r(., ., .|., φ), ˆat: ˆT). (51)
So the variational action-value function ˆQ(ˆat: ˆT, φ) directly takes
the conditional probability distribution deﬁned by variatio nal
parameter φ and evaluates the sequence of future actions ˆat: ˆT
according to M. Unlike in the Bayesian case no inference takes
place during the evaluation of ˆQ(ˆat: ˆT, φ).
At the same time, after variational inference, if we plug in
φ∗
sa≺t,ξ for φ we have:
ˆQ(ˆat: ˆTa
, φ∗
sa≺t,ξ ) ≈ ˆQ(ˆat: ˆTa
, sa≺t, ξ ). (52)
Note that the reason we have placed a hat on ˆQ is that, even in the
Bayesian case, it is usually not the optimal action-value fun ction
but instead is an estimate based on the current knowledge sta te
represented by the complete posteriors of the agent.
Also note that some intrinsic motivations (e.g.,
empowerment) evaluate e.g., the next n actions by using
predictions reaching n + m steps into the future. This means
that they need all complete posteriors for ˆat : t+n+m−1 but only
evaluate the actions ˆat : t+n−1. In other words they cannot
evaluate actions up to their generative model’s time-horiz on ˆT
but only until a shorter time-horizon ˆTa = ˆT − m for some
natural number m. When necessary we indicate such a situation
by only passing shorter future action sequences ˆat: ˆTa
to the
action-value function, in turn, the intrinsic motivation f unction.
The respective posteriors keep the original time horizon ˆT > ˆTa.
7.2. Deterministic and Stochastic Action
Selection
We can then select actions simply by picking the ﬁrst action in t he
sequence ˆat: ˆT that maximizes the Bayesian action-value function:
ˆa∗
t: ˆT(mt) := ˆa∗
t: ˆT(sa≺t) := arg max
ˆat: ˆT
ˆQ(ˆat: ˆT, sa≺t, ξ ) (53)
4The main diﬀerence is that the action-value functions here evalua te sequences
of future actions as opposed to policies. This is the prevalent practice in active
inference literature including Friston et al. (2015) and we therefore follow it here.
5We abuse notation here by reusing the same symbol ˆQ for the variational
action-value function as for the Bayesian action-value funct ion. However, in this
publication the argument ( sa≺t, ξ or φ) always indicates which one is meant.
Frontiers in Neurorobotics | www.frontiersin.org 12 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
and set
ˆa∗(mt) := ˆa∗
t (mt). (54)
or for the variational action value function:
ˆa∗
t: ˆT(mt) := ˆa∗
t: ˆT(φ∗
sa≺t,ξ ) := arg max
ˆat: ˆT
ˆQ(ˆat: ˆT, φ∗
sa≺t,ξ ). (55)
and set
ˆa∗(mt) := ˆa∗
t (mt). (56)
This then results in a deterministic action generation p( a|m):
p(at|mt) := δˆa∗(mt)(at).
We note here that in the case of universal reinforcement lear ning
the role of ˆQ(ˆat: ˆT, sa≺t, ξ ) is played by Vπ
ξ (sa≺t). There π is
a policy that selects actions in dependence on the entire past
sa≺t and ξ parameterizes the posterior just like in the present
publication. The arg max in Equation (53) selects a policy inst ead
of an action sequence and that policy is used for the action
generation.
A possible stochastic action selection that is important for
active inference is choosing the action according to a so cal led
softmax policy (
Sutton and Barto, 1998 ):
p(at|mt) :=
∑
ˆat+1 : ˆT
1
Z(γ , sa≺t, ξ ) eγ ˆQ(ˆat: ˆT ,sa≺t,ξ ) (57)
where:
Z(γ , sa≺t, ξ ) :=
∑
ˆat: ˆT
eγ ˆQ(ˆat: ˆT ,sa≺t,ξ ) (58)
is a normalization factor. Note that we are marginalizing ou t later
actions in the sequence ˆat: ˆT to get a distribution only over the
action ˆat. For the variational action-value function this becomes:
p(at|mt) :=
∑
ˆat+1 : ˆT
1
Z(γ , φ∗
sa≺t,ξ ) eγ ˆQ(ˆat: ˆT ,φ∗
sa≺t,ξ ) (59)
where:
Z(γ , φ∗
sa≺t,ξ ) :=
∑
ˆat: ˆT
eγ ˆQ(ˆat: ˆT ,φ∗
sa≺t,ξ ). (60)
Since it is relevant for active inference (see Section 8), no te that
the softmax distribution over future actions can also be deﬁ ned
for arbitrary φ and not only for the optimized φ∗
sa≺t,ξ . At the
same time, the softmax distribution for the optimized φsa≺t,ξ
clearly also approximates the softmax distribution of the Bay esian
action-value function.
Softmax policies assign action sequences with higher values of
ˆQ higher probabilities. They are often used as a replacement for
the deterministic action selection to introduce some explor ation.
Here, lower γ leads to higher exploration; conversely, in the
limit where γ → ∞ the softmax turns into the deterministic
action selection. From an intrinsic motivation point of view such
additional exploration should be superﬂuous in many cases sin ce
many intrinsic motivations try to directly drive exploratio n by
themselves. Another interpretation of such a choice is to see γ
as a trade-oﬀ factor between the processing cost of choosing an
action precisely and achieving a high action-value. The lower γ ,
the higher the cost of precision. This leads to the agent more o ften
taking actions that do not attain maximum action-value.
We note that the softmax policy is not the only possible
stochastic action selection mechanism. Another option disc ussed
in the literature is Thompson sampling (
Ortega and Braun, 2010,
2014; Aslanides et al., 2017 ). In our framework this corresponds
to a two step action selection procedure where we ﬁrst sample an
environment and parameter pair ( ¯
ˆet−1, ¯θ ) from a posterior factor
(Bayesian or variational)
(¯
ˆet−1, ¯θ ) ∼ d(ˆEt−1, /Theta1|sa≺t, ξ ) (61)
then plug the according predictive factor q( ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ¯
ˆet−1, ¯θ )
into the action value function
ˆQ(ˆat: ˆT, sa≺t, ξ ) := M(q(., .|., ¯
ˆet−1, ¯θ ), ˆat: ˆT). (62)
This allows intrinsic motivations that only evaluate the
probability distribution over future sensor values ˆSt: ˆT and
environment states ˆEt: ˆT. However, it rules out those that evaluate
the posterior probability of environment parameters /Theta1 because
we sample a speciﬁc ¯θ .
7.3. Intrinsic Motivations
Now, we look at some intrinsic motivations including the
intrinsic motivation part underlying Friston’s active infer ence.
In the deﬁnitions, we use d(., ., . |.) ∈ /Delta1 ˆS ˆT−t+1× ˆE ˆT+1×/Delta1/Theta1 | ˆA ˆT−t+1
as a generic conditional probability distribution. The gener ic
symbol d is used since it represents both Bayesian complete
posteriors and approximate complete posteriors. In fact, the
deﬁnitions of the intrinsic motivations are agnostic with re spect
to the method used to obtain a complete posterior. In the present
context, it is important that these deﬁnitions are general en ough
to induce both Bayesian and variational action-value funct ions.
We usually state the deﬁnition of the motivation function
using general expressions (e.g., marginalizations) derived from
d(, ., .|.). Also, we look at how they can be obtained from
Bayesian complete posteriors to give to the reader an intuition
for the computations involved in applications. The approximate
complete posterior usually makes these calculations easier an d we
will present an example of this.
7.3.1. Free Energy Principle
Here, we present the non-variational Bayesian inference ver sions
for the expressions that occur in the “expected free energy”
in
Friston et al. (2015, 2017a) . These papers only include
approximate expressions after variational inference. Most of t he
expressions we give here can be found in Friston et al. (2017b) .
The exception is Equation (74), which can be obtained from
Frontiers in Neurorobotics | www.frontiersin.org 13 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
an approximate term in Friston et al. (2017a) in the same way
that the non-variational Bayesian inference terms in Friston et al.
(2017b) are obtained from the approximate ones in Friston et al.
(2015).
In the following, we can set ˆTa = ˆT, since actions are only
evaluated with respect to their immediate eﬀects.
According to Friston et al. (2017b, Equation (A2) Appendix),
the “expected free energy” is just the future conditional ent ropy
of sensor values 6 given environment states. Formally, this is
(with a negative sign to make minimizing expected free energy
equivalent to maximizing the action-value function):
M(d(., ., .|.), ˆat: ˆT ) :=
∑
ˆet: ˆT
d(ˆet: ˆT |ˆat: ˆT )
∑
ˆst: ˆT
d(ˆst: ˆT |ˆet: ˆT ) log d(ˆst: ˆT |ˆet: ˆT )
(63)
= −
∑
ˆet: ˆT
d(ˆet: ˆT |ˆat: ˆT ) Hd(ˆSt: ˆT |ˆet: ˆT ) (64)
= − Hd(ˆSt: ˆT |ˆEt: ˆT , ˆat: ˆT ). (65)
Note that, we indicate the probability distribution d used to
calculate entropies H d(X) or mutual informations I d(X : Y) in
the subscript. Furthermore,we indicate the variables that a re
summed over with capital letters and those that are ﬁxed (e.g. ,
ˆat: ˆT above) with small capital letters.
In the case where d(., ., . |.) is the Bayesian complete
posterior q(., ., . |., sa≺t, ξ ), it uses the predictive distribution of
environment states q( ˆet: ˆT|ˆat: ˆT, sa≺t, ξ ) and the posterior of the
conditional distribution of sensor values given environme nt
states q( ˆst: ˆT|ˆet: ˆT, sa≺t, ξ ). As we see next, both distributions can
be obtained from the Bayesian complete posterior.
The former distribution is a familiar expression in hierarch ical
Bayesian models and corresponds to a posterior predictive
distribution or predictive density [cmp. e.g.,
Bishop, 2011 ,
Equation (3.74)] that can be calculated via:
q(ˆet: ˆT|ˆat: ˆT, sa≺t, ξ )
=
∫ ∑
ˆst: ˆT ,ˆe≺t
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) dθ (66)
=
∫ ∑
ˆst: ˆT ,ˆe≺t
q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) dθ (67)
=
∫ ∑
ˆet−1
q(ˆet: ˆT|ˆat: ˆT, ˆet−1, θ ) q(ˆet−1, θ |sa≺t, ξ ) dθ , (68)
where we split the complete posterior into the predictive and
posterior factor and then marginalized out environment stat es
ˆe≺t−1 since the predictive factor does not depend on them.
Note that in practice, this marginalization corresponds to a s um
over |E|t−1 terms and therefore has a computational cost that
grows exponential in time. However, if we use the approximate
complete posterior such that d(., ., . |.) = r(., ., .|., φ), we see from
6The original text refers to this as the “expected entropy of outcome s, ” not
the expected conditional entropy of outcomes. Nonetheless, the a ssociated
Equation (A2) in the original is identical to ours.
Equation (40), that q( ˆe≺t, θ |sa≺t, ξ ) is replaced by r( ˆe≺t, θ |φ)
which is deﬁned as (Equation 38):
r(ˆe≺t, θ |φ) :=
t−1∏
τ =0
r(ˆeτ |φEτ )
3∏
i = 1
r(θ i|φi). (69)
This means that r( ˆet−1, θ |φ) is just r( ˆet−1|φEt−1 ) r(θ |φ), which
we obtain directly from the variational inference without a ny
marginalization. If Bayesian inference increases in comput ational
cost exponentially in time, this simpliﬁcation leads to a sign iﬁcant
advantage. This formulation leaves an integral over θ or,
more precisely, a triple integral over the three θ 1, θ 2, θ 3.
However, if the q( θ i|ξ i) are chosen as conjugate priors to
q(ˆs|ˆe, θ 1), q(ˆe′|ˆa′, ˆe, θ 2), q(ˆe0|θ 3) respectively, then these integrals
can be calculated analytically [compare the similar calculat ion of
q(ˆe≺t, θ |sa≺t, ξ ) in Appendix A]. The remaining computational
problem is only the sum over all ˆet−1.
The latter term (the posterior conditional distribution over
sensor values given environment states) can be obtained via
q(ˆst: ˆT|ˆet: ˆT, sa≺t, ξ ) = q(ˆst: ˆT|ˆet: ˆT, ˆat: ˆT, sa≺t, ξ ) (70)
= q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, sa≺t, ξ )
q(ˆet: ˆT|ˆat: ˆT, sa≺t, ξ ) . (71)
Here, the ﬁrst equation holds since
ˆSt: ˆT ⊥ ⊥ˆAt: ˆT | ˆEt: ˆT, SA≺t. (72)
Both numerator and denominator can be obtained from the
complete posterior via marginalization as for the former term.
This marginalization also shows that the intrinsic motivat ion
function, Equation (63), is a functional of the complete poster iors
or d(., ., . |.).
In most publications on active inference the expected free
energy in Equation (63) is only part of what is referred to as th e
expected free energy. Usually, there is a second term measuri ng
the relative entropy to an externally speciﬁed prior over future
outcomes (also called “predictive distribution encoding goals”
Friston et al. 2015 ), i.e., a desired probability distribution p d(ˆst: ˆT).
The relative entropy term is formally given by:
KL[d(ˆSt: ˆT|ˆat: ˆT)|| pd(ˆSd
t: ˆT)] =
∑
ˆst: ˆT
d(ˆst: ˆT|ˆat: ˆT) log d(ˆst: ˆT|ˆat: ˆT)
pd(ˆst: ˆT) .
(73)
Clearly, this term will lead the agent to act such that the
future distribution over sensor values is similar to the desi red
distribution. Since this term is used to encode extrinsic val ue
for the agent, we mostly ignore it in this publication. It could
included into any of the following intrinsic motivations.
In
Friston et al. (2017a) yet another term, called “negative
novelty” or “ignorance”, occurs in the expected free energy.
This term concerns the posterior distribution over parameter
θ 1. It can be slightly generalized to refer to any subset of the
parameters θ = (θ 1, θ 2, θ 3). We can write it as a conditional
Frontiers in Neurorobotics | www.frontiersin.org 14 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
mutual information between future sensor values and paramet ers
(the “ignorance” is the negative of this):
Id(ˆSt: ˆT : /Theta1|ˆat: ˆT) =
∑
ˆst: ˆT
d(ˆst: ˆT|ˆat: ˆT)
∫
d(θ |ˆst: ˆT, ˆat: ˆT)
× log d(θ |ˆst: ˆT, ˆat: ˆT)
d(θ ) dθ . (74)
This is identical to the information gain used in knowledge
seeking agents. The necessary posteriors in the Bayesian cas e are
q(ˆst: ˆT|ˆat: ˆT, sa≺t, ξ ), q(θ |ˆst: ˆT, ˆat: ˆT, sa≺t, ξ ) and q( θ |sa≺t, ξ ) with
q(ˆst : ˆT |ˆat : ˆT , sa≺t, ξ ) =
∫ ∑
ˆe≺t
q(ˆst: ˆT |ˆat: ˆT , ˆet−1, θ ) q(ˆe≺t, θ |sa≺t, ξ ) dθ
(75)
a straightforward (if costly) marginalization of the complet e
posterior. Just like previously for q( ˆet: ˆT|ˆat: ˆT, sa≺t, ξ ), the
marginalization is greatly simpliﬁed in the variational cas e (see
Appendix B for a more explicit calculation). The integrals can be
computed if using conjugate priors. The other two posteriors ca n
be obtained via
q(θ |ˆst: ˆT, ˆat: ˆT, sa≺t, ξ )
= 1
q(ˆst: ˆT|ˆat: ˆT, sa≺t, ξ )
∑
ˆe0: ˆT
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ). (76)
and
q(θ |sa≺t, ξ ) = q(θ |ˆat: ˆT, sa≺t, ξ ) (77)
=
∑
ˆst: ˆT ,ˆe0: ˆT
q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ). (78)
In the latter equation we used
ˆAt: ˆT ⊥ ⊥/Theta1|SA≺t. (79)
The marginalizations grow exponentially in computational co st
with ˆT. In this case, the variational approximation only reduces
the necessary marginalization over ˆe≺t−1 to one over ˆet−1, but the
marginalization over future environment states ˆet: ˆT and sensor
values ˆst: ˆT remains the same since we use the exact predictive
factor. In practice the time horizon into the future ˆT − t must
then be chosen suﬃciently short, so that marginalizing out ˆet: ˆT
and ˆSt: ˆT is feasible. Together with the variational approximation
the required marginalizations over past and future are then
constant over time which makes the implementation of agents
with extended lifetimes possible.
The combination of the conditional entropy term and the
information gain deﬁnes the (intrinsic part) of the action-v alue
function of Friston’s active inference (or free energy princ iple):
MFEP(d(., ., .|.), ˆat: ˆT) = − Hd(ˆSt: ˆT|ˆEt: ˆT) + Id(ˆSt: ˆT : θ |ˆat: ˆT) (80)
In the active inference literature this is usually approximated by
a sum over the values at individual timesteps:
MFEP(d(., ., .|.), ˆat: ˆT) =
ˆT∑
τ =t
− Hd(ˆSτ |ˆEτ ) + Id(ˆSτ : /Theta1|ˆat: ˆT).
(81)
7.3.2. Free Energy Principle Specialized to
Friston
et al. (2015)
Using Appendix C, we show how to get the action-value function
of Friston et al. (2015, Equation 9) in our framework. In Friston
et al. (2015) , the extrinsic value term of Equation (73) is included,
but not the information gain term of Equation (74). Furtherm ore,
the sum over timesteps in Equation (81) is used. This leads to th e
following expression:
MFEP(d(., ., .|.), ˆat: ˆT) =
ˆT∑
τ =t
− Hd(ˆSτ |ˆEτ )
− KL[d(ˆSτ |ˆat: ˆT)|| pd(ˆSτ )]. (82)
If we plug in an approximate complete posterior, we get:
MFEP(r(., ., .|.), ˆat: ˆT) =
ˆT∑
τ =t
− Hr(ˆSτ |ˆEτ )
− KL[r(ˆSτ |ˆat: ˆT)|| pd(ˆSτ )]. (83)
with
− Hr(ˆSτ |ˆEτ ) =
∑
ˆeτ
r(ˆeτ |ˆat: ˆT , ˆet−1, φ)
∑
ˆsτ
r(ˆsτ |ˆeτ , φ) log r(ˆsτ |ˆeτ , φ),
(84)
and
KL[r(ˆSτ |ˆat: ˆT)|| pd(ˆSτ )] =
∑
ˆsτ
r(ˆsτ |ˆat: ˆT, φ) log r(ˆsτ |ˆat: ˆT, φ)
pd(ˆsτ ) .
(85)
For the particular approximate posterior of Equation (40), with
its factorization into exact predictive and approximate posteri or
factor, the individual terms can be further rewritten.
r(ˆeτ |ˆat: ˆT , ˆet−1, φ) =
∑
ˆst: ˆT ,ˆeτ +1 : ˆT ˆet : τ −1 ˆe0:T−2
∫
r(ˆst: ˆT , ˆe0: ˆT , θ |ˆat: ˆT , φ) dθ
(86)
=
∑
ˆst: ˆT ,ˆeτ +1 : ˆT ˆet : τ −1 ˆe0:T−2
∫
q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ )
× r(ˆe≺t, θ |φ) dθ (87)
=
∑
ˆst: ˆT ,ˆeτ +1 : ˆT ˆet : τ −1 ˆe0:T−2
∫
q(ˆst: ˆT , ˆet: ˆT |ˆat: ˆT , ˆet−1, θ )
×
t−1∏
r = 0
r(ˆer|φEr )
3∏
i = 1
r(θ i|φi) dθ (88)
Frontiers in Neurorobotics | www.frontiersin.org 15 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
=
∑
ˆet : τ −1
∫
q(ˆet : τ −1|ˆat: ˆT , ˆet−1, θ 2)
× r(ˆet−1|φEt−1 ) r(θ 2|φ2) dθ 2 (89)
=

∑
ˆet : τ −1
∫ τ∏
r = t
q(ˆer|ˆar, ˆer−1, θ 2) r(θ 2|φ2) dθ 2


× r(ˆet−1|φEt−1 ). (90)
In
Friston et al. (2015) , the environment dynamics
q(ˆer|ˆar, ˆer−1, θ 2) are not inferred and are therefore not
parameterized:
q(ˆer|ˆar, ˆer−1, θ 2) = q(ˆer|ˆar, ˆer−1) (91)
and are set to the physical environment dynamics:
q(ˆer|ˆar, ˆer−1) = p(ˆer|ˆar, ˆer−1). (92)
This means the integral over θ 2 above is trivial and we get:
r(ˆeτ |ˆat: ˆT, ˆet−1, φ) =
∑
ˆet : τ −1
τ∏
r = t
q(ˆer|ˆar, ˆer−1) r(ˆet−1|φEt−1 ) (93)
In the notation of Friston et al. (2015) (see Appendix C for a
translation table), we have
q(ˆer|ˆar, ˆer−1) = B(ˆar)ˆer ˆer−1 (94)
where B(ˆar) is a matrix, and
r(ˆet−1|φEt−1 ) = (ˆst−1)ˆet−1 (95)
where (ˆst−1) is a vector, so that
r(ˆeτ |ˆat: ˆT, ˆet−1, φ) = (B(ˆaτ ) · · ·B(ˆat) · ˆst−1)ˆeτ (96)
=: (ˆsτ (ˆat: ˆT))ˆeτ (97)
Similarly, since the sensor dynamics in Friston et al. (2015) are
also not inferred, we ﬁnd
r(ˆsτ |ˆeτ , φ) = q(ˆsτ |ˆeτ ) = p(ˆsτ |ˆeτ ). (98)
Friston et al. writes:
q(ˆsτ |ˆeτ ) =: Aˆsτ ˆeτ (99)
with A a matrix. So that,
r(ˆsτ |ˆat: ˆT, φEt−1 ) = A · ˆsτ (ˆat: ˆT) (100)
=: ˆoτ (ˆat: ˆT). (101)
Then
Hr(ˆSτ |ˆEτ ) = − 1 ·(A × log A) · ˆsτ (ˆat: ˆT) (102)
where × is a Hadamard product and 1 is a vector of ones. Also,
KL[r(ˆSτ |ˆat: ˆT)|| pd(ˆSτ )] = ˆoτ (ˆat: ˆT)·(log ˆoτ (ˆat: ˆT)−log Cτ ) (103)
where ( Cτ )ˆsτ = pd(ˆsτ ). Plugging these expressions into
Equation (83), substituting ˆat: ˆT → π , and comparing this to
Friston et al. (2015, Equation 9) shows that 7:
MFEP(r(., ., .|.), π ) = 1 ·(A × log A) · ˆsτ (ˆat: ˆT) (104)
−ˆoτ (ˆat: ˆT) ·(log ˆoτ (ˆat: ˆT) − log Cτ )
= Q(π ). (105)
This veriﬁes that our formulation of the action-value funct ion
specializes to the “expected (negative) free energy” Q(π ).
7.3.3. Empowerment Maximization
Empowerment maximization (
Klyubin et al., 2005 ) is an intrinsic
motivation that seeks to maximize the channel capacity from
sequences of the agent’s actions into the subsequent sensor
value. The agent, equipped with complete knowledge of the
environment dynamics, can directly observe the environmen t
state. If the environment is deterministic, an empowerment
maximization policy leads the agent to a state from which it ca n
reach the highest number of future states within a preset numb er
of actions.
Salge et al. (2014) provide a good overview of existing research
on empowerment maximization. A more recent study relates
the intrinsic motivation to the essential dynamics of livin g
systems, based on assumptions from autopoietic enactivism
Guckelsberger and Salge (2016) . Several approximations have
been proposed, along with experimental evaluations in complex
state / action spaces.
Salge et al. (2018) show how deterministic
empowerment maximization in a three-dimensional grid-world
can be made more eﬃcient by diﬀerent modiﬁcations of
UCT tree search. Three recent studies approximate stochastic
empowerment and its maximization via variational inference
and deep neural networks, leveraging a variational bound
on the mutual information proposed by
Barber and Agakov
(2003). Mohamed and Rezende (2015) focus on a model-
free approximation of open-loop empowerment, and Gregor
et al. (2016) propose two means to approximate closed-
loop empowerment. While these two approaches consider
both applications in discrete and continuous state / action
spaces,
Karl et al. (2017) develop an open-loop, model-
based approximation for the continuous domain speciﬁcally.
The latter study also demonstrates how empowerment can
yield good performance in established reinforcement learni ng
benchmarks such as bipedal balancing in the absence of
extrinsic rewards. In recent years, research on empowerment
has particularly focused on applications in multi-agent system s.
Coupled empowerment maximization as a speciﬁc multi-
agent policy has been proposed as intrinsic drive for either
supportive or antagonistic behaviour in open-ended scenarios
with sparse reward landscapes
Guckelsberger et al. (2016b) .
This theoretical investigation has then been backed up with
empirical evaluations on supportive and adversarial video
game characters
Guckelsberger et al. (2016a, 2018) . Beyond
virtual agents, the same policy has been proposed as a
7There is a small typo in Friston et al. (2015, Equation 9) where the time index of
ˆst−1 in (ˆsτ (ˆat: ˆT )) = (B(ˆaτ ) · · ·B(ˆat) · ˆst−1) is given as t instead of t − 1.
Frontiers in Neurorobotics | www.frontiersin.org 16 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
good heuristic to facilitate critical aspects of human-robo t
interaction, such as self-preservation, protection of the hu man
partner, and response to human actions Salge and Polani
(2017).
For empowerment, we select ˆTa = t + n and ˆT = t + n + m,
with n ≥ 0 and m ≥ 1. This means the agent chooses n+1 actions
which it expects to maximize the resulting m-step empowerment.
The according action-value function is:
MEM(d(., ., .|.), ˆat: ˆTa
) := max
d(ˆa ˆTa+1 : ˆT )
Id( ˆAˆTa+1 : ˆT : ˆSˆT|ˆat: ˆTa
) (106)
= max
d(ˆa ˆTa+1 : ˆT )
∑
ˆa ˆTa+1 : ˆT ,ˆs ˆT
d(ˆaˆTa+1 : ˆT)
× d(ˆsˆT|ˆat: ˆT) log d(ˆsˆT|ˆat: ˆT)
d(ˆsˆT|ˆat: ˆTa
) . (107)
Note that in the denominator of the fraction, the action sequ ence
only runs to t: ˆTa and not to t: ˆT as in the numerator.
In the Bayesian case, the required posteriors are
q(ˆsˆT|ˆat: ˆT, sa≺t, ξ ) (for each ˆaˆTa+1 : ˆT) and q( ˆsˆT|ˆat: ˆTa
, sa≺t, ξ ). The
former distribution is a further marginalization over ˆst+1 : ˆT−1
of q( ˆst: ˆT|ˆat: ˆT, sa≺t, ξ ). The variational approximation only helps
getting q( ˆst: ˆT|ˆat: ˆT, sa≺t, ξ ), not the further marginalization. The
latter distribution is obtained for a given q( ˆaˆTa+1 : ˆT) from the
former one via
q(ˆsˆT |ˆat: ˆTa
, sa≺t, ξ ) =
∑
ˆa ˆTa+1 : ˆT
q(ˆsˆT , ˆaˆTa+1 : ˆT |ˆat: ˆTa
, sa≺t, ξ ) (108)
=
∑
ˆa ˆTa+1 : ˆT
q(ˆsˆT |ˆaˆTa+1 : ˆT , ˆat: ˆTa
, sa≺t, ξ ) q(ˆaˆTa+1 : ˆT )
(109)
since the empowerment calculation imposes
q(ˆaˆTa+1 : ˆT|ˆat: ˆTa
, sa≺t, ξ ) = q(ˆaˆTa+1 : ˆT). (110)
7.3.4. Predictive Information Maximization
Predictive information maximization, (
Ay et al., 2008 ),
is an intrinsic motivation that seeks to maximize the
predictive information of the sensor process. Predictive
information is the mutual information between past and
future sensory signal, and has been proposed as a general
measure of complexity of stochastic processes (
Bialek
and Tishby, 1999 ). For applications in the literature see
Ay et al. (2012); Martius et al. (2013, 2014) . Also, see
Little and Sommer (2013) for a comparison to entropy
minimization.
For predictive information, we select a half time horizon k =
⌊(t: ˆT − t + 1)/2⌋ where k > 0 for predictive information
to be deﬁned (i.e., t: ˆT − t > 0). Then, we can deﬁne the
expected mutual information between the next m sensor values
and the subsequent m sensor values as the action-value function
of predictive information maximization. This is similar to t he
time-local predictive information in Martius et al. (2013) :
MPI(d(., ., .|.), ˆat: ˆT) := Id(ˆSt : t+k−1 : ˆSt+k : t+2k−1|ˆat: ˆT). (111)
We omit writing out the conditional mutual information
since it is deﬁned in the usual way. Note that it is possible
that t + 2k − 1 < t: ˆT so that the action sequence ˆat: ˆT
might go beyond the evaluated sensor probabilities. This
displacement leads to no problem since the sensor values
do not depend on future actions. The posteriors needed are:
q(ˆst : t+k−1|ˆat: ˆT, sa≺t, ξ ), q( ˆst+k : t+2k−1|ˆst : t+k−1, ˆat: ˆT, sa≺t, ξ ),
and q( ˆst+k : t+2k−1|ˆat: ˆT, sa≺t, ξ ). The ﬁrst and the last are again
marginalizations of q( ˆst: ˆT|ˆat: ˆT, sa≺t, ξ ) seen in Equation (75).
The second posterior is a fraction of such marginalizations.
7.3.5. Knowledge Seeking
Knowledge seeking agents (
Storck et al., 1995; Orseau et al., 2013 )
maximize the information gain with respect to a probability
distribution over environments. The information gain we use
here is the relative entropy between the belief over environm ents
after actions and subsequent sensor values and the belief ove r
environments (this is the KL-KSA of
Orseau et al. 2013 , “KL ”
for Kullback-Leibler divergence). In our case the belief ove r
environments can be identiﬁed with the posterior q( θ |sa≺t, ξ )
since every θ = (θ 1, θ 2, θ 3) deﬁnes an environment. In principle,
this can be extended to the posterior q( ξ |sa≺t, ξ ) over the
hyperprior ξ , but we focus on θ here. This deﬁnition is more
similar to the original one. Then, we deﬁne the knowledge
seeking action-value function using the information gain of
Equation (74):
MKSA(d(., ., .|.), ˆat: ˆT) := Id(ˆSt: ˆT : /Theta1|ˆat: ˆT). (112)
We have discussed the necessary posteriors following
Equation (74).
After this overview of some intrinsic motivations, we look
at active inference. However, what should be clear is, that, i n
principle, both the posteriors needed for the intrinsic motiva tion
function of the original active inference (
Friston et al., 2015 )
and the posteriors needed for alternative inferences overla p. This
overlap shows that the other intrinsic motivations mentione d
here also proﬁt from variational inference approximations. The re
is also no indication that these intrinsic motivations cann ot be
used together with the next discussed active inference.
8. ACTIVE INFERENCE
Now, we look at active inference. Note that this section is
independent of the intrinsic motivation function underlyin g the
action-value function ˆQ.
In the following we ﬁrst look at and try to explain a slightly
simpliﬁed version of the active inference in
Friston et al. (2015) .
Afterwards we also state the full version.
As mentioned in the introduction, current active inference
versions are formulated as an optimization procedure that, at
least at ﬁrst sight, looks similar to the optimization of a vari ational
free energy familiar from variational inference. Recall tha t, in
variational inference the parameters of a family of distribut ions
are optimized to approximate an exact (Bayesian) posterior of
a generative model. In the case we discussed in Section 6.4
the sought after exact posterior is the posterior factor of the
Frontiers in Neurorobotics | www.frontiersin.org 17 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
generative model of Section 6.1. One of our questions about
active inference is whether it is a straightforward applicati on of
variational inference to a posterior of some generative mode l.
This would imply the existence of a generative model whose
standard updating with past actions and sensor values leads to an
optimal posterior distribution over future actions. Note tha t, this
does not work with the generative model in of Section 6.1 sinc e
the future actions there are independent of the past sensor val ues
and actions. Given the appropriate generative model, it would
then be natural to introduce it ﬁrst and then apply a variationa l
approximation similar to our procedure in Section 6.
We were not able to ﬁnd in the literature or construct
ourselves a generative model such that variational inferen ce
leads directly to the active inference as given in
Friston et al.
(2015). Instead we present a generative model that contains a
posterior whose variational approximation optimization is ver y
similar to the optimization procedure of active inference. It is
also closely related to the two-step action generation of ﬁr st
inferring the posterior and then selecting the optimal action s.
This background provides some intuition for the particularit ies
of active inference.
One diﬀerence of the generative model used here is that
its structure depends on the current time step in a systematic
way. The previous generative model of Section 6.1 had a time-
invariant structure.
In Section 6, we showed how the generative model, together
with either Bayesian or variational inference, can provide a n
agent with a set of complete posteriors. Each complete posterior
is a conditional probability distribution over all currentl y
unobserved variables ( ˆSt: ˆT, ˆE0:T) and parameters ( /Theta1 and more
generally also /Xi1) given past sensor values and actions sa≺t and
a particular sequence of future actions ˆat: ˆT. Inference means
updating the set of posteriors in response to observations sa≺t.
Active inference should then update the distribution over fu ture
actions in response to observations. This means the accordin g
posterior cannot be conditional on future action sequences l ike
the complete posterior in Equation (16). Since active inferen ce
promises belief or knowledge updating and action selection in
one mechanism the posterior should also range over unobserve d
relevant variables like future sensor values, environment states,
and parameters. This leads to the posterior of Equation (13):
q(ˆst: ˆT, ˆe0: ˆT, ˆat: ˆT, θ |sa≺t, ξ ). (13 revisited)
If this posterior has the right structure, then we can derive a
future action distribution by marginalizing:
q(ˆat: ˆT|sa≺t, ξ ) =
∑
ˆst: ˆT ,ˆe0: ˆT
∫
q(ˆst: ˆT, ˆe0: ˆT, ˆat: ˆT, θ |sa≺t, ξ ) dθ . (113)
Actions can then be sampled from the distribution obtained by
marginalizing further to the next action only:
p(at|mt) :=
∑
ˆat+1 : ˆT
q(ˆat: ˆT|sa≺t, ξ ). (114)
This scheme could justiﬁably be called (non-variational) a ctive
inference since the future action distribution is directly o btained
by updating the generative model.
However, as we mentioned above, according to the generative
model of Figure 2, the distribution over future actions is
independent of the past sensor values and actions:
q(ˆst: ˆT, ˆe0: ˆT, ˆat: ˆT, θ |sa≺t, ξ ) = q(ˆst: ˆT, ˆe0: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) q(ˆat: ˆT)
(115)
since
q(ˆat: ˆT|sa≺t, ξ ) = q(ˆat: ˆT). (116)
Therefore, we can never learn anything about future actions
from past sensor values and actions using this model. In other
words, if we intend to select the actions based on the past, we
cannot uphold this independent model. The inferred actions
must become dependent on the history and the generative model
has to be changed for a scheme like the one sketched above to be
successful.
In Section 7.2, we have mentioned that the softmax policy
based on a given action-value function ˆQ could be a desirable
outcome of an active inference scheme such as the above. Thus ,
if we ended up with
q(ˆat: ˆT|sa≺t, ξ ) = 1
Z(γ , sa≺t, ξ ) eγ ˆQ(ˆat: ˆT ,sa≺t,ξ ) (117)
as a result of some active inference process, that would be a
viable solution. We can force this by building this conditio nal
distribution directly into a new generative model. Note tha t
this conditional distribution determines all future actio ns ˆat: ˆT
starting at time t and not just the next action ˆat. In the
end however only the next action will be taken according
to Equation (114) and at time t + 1 the action generation
mechanism starts again, now with ˆat+1 : ˆT inﬂuenced by the new
data sat in addition to sa≺t. So the model structure changes
over time in this case with the dependency of actions on pasts
sa≺t shifting together with each time-step. Keeping the rest
of the previous Bayesian network structure intact we deﬁne
that at each time t the next action ˆAt depends on past sensor
values and actions sa≺t as well as on the hyperparameter ξ (see
Figure 6):
q(ˆst: ˆT, ˆe0: ˆT, ˆat: ˆT, θ |sa≺t, ξ ) := q(ˆst: ˆT, ˆet: ˆT|ˆat: ˆT, ˆet−1, θ )
× q(ˆat: ˆT|sa≺t, ξ ) q(θ , ˆe≺t|sa≺t, ξ ).
(118)
On the right hand side we have the predictive and posterior
factors left and right of the distribution over future action s.
We deﬁne this conditional future action distribution to be th e
softmax of Equation (117). This means that the mechanism-
generating future actions uses the Bayesian action-value f unction
ˆQ(ˆat: ˆT, sa≺t, ξ ). The Bayesian action-value function depends
on the complete posterior q( ˆst: ˆT, ˆet: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) calculated
using the old generative model of Figure 2 where actions do
Frontiers in Neurorobotics | www.frontiersin.org 18 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
FIGURE 6 |Generative model including q(ˆat: ˆT |sa≺t, ξ ) at t = 2 with ˆS ˆA≺2 inﬂuencing future actions ˆA2 : ˆT . Note that, only future actions are dependent on past
sensor values and actions, e.g., action ˆA1 has no incoming edges. The increased gap between time step t = 1 and t = 2 is to indicate that this time step is special in
the model. For each time step t there is an according model with the particular relation bet ween past ˆS ˆA≺t and ˆAt: ˆT shifted accordingly.
not not depend on past sensor values and actions. This is a
complex construction with what amounts to Bayesian inferenc e
essentially happening within an edge (i.e., ˆS ˆA≺t → ˆAt: ˆT) of a
Bayesian network. However, logically there is no problem sin ce
the posterior q( ˆst: ˆT, ˆet: ˆT, θ |ˆat: ˆT, sa≺t, ξ ) for each ˆat: ˆT to be well
deﬁned really only needs sa≺t, ξ , and the model structure. Here
we see the model structure as “hard wired” into the mechanism ,
since it is ﬁxed for each time step t from the beginning.
We now approximate the posterior of Equation (117) using
variational inference. Like in Section 6.4 we do not approxima te
the predictive factor. Instead we only approximate the product
of posterior factor q( θ , ˆe≺t|sa≺t, ξ ) and future action distribution
q(ˆat: ˆT|sa≺t, ξ ). By construction these are two independent factors
but with an eye to active inference which treats belief or
knowledge updating and action generation together we also
treat them together. For the approximation we again use the
approximate posterio factor of Equation (38) and combine it wit h
a distribution over future actions r( ˆat: ˆT|π ) parameterized by π :
r(ˆat: ˆT, ˆe≺t, θ |π , φ) := r(ˆat: ˆT|π ) r(ˆe≺t, θ |φ) (119)
:= r(ˆat: ˆT|π ) r(ˆe≺t|φE≺t ) r(θ |φ). (120)
The variational free energy is then:
F[π , φ, sa≺t, ξ ] :=
∑
ˆat: ˆT ,ˆe≺t
∫
r(ˆat: ˆT |π ) r(ˆe≺t, θ |φ)
× log r(ˆat: ˆT |π ) r(ˆe≺t, θ |φ)
q(s≺t, ˆat: ˆT , ˆe≺t, θ |a≺t, ξ ) dθ (121)
=
∑
ˆat: ˆT ,ˆe≺t
∫
r(ˆat: ˆT |π ) r(ˆe≺t, θ |φ)
× log r(ˆat: ˆT |π ) r(ˆe≺t, θ |φ)
q(ˆat: ˆT |sa≺t, ξ ) q(ˆe≺t, θ |sa≺t, ξ ) q(s≺t|a≺t, ξ ) dθ
(122)
= F[φ, sa≺t, ξ ] + KL[r( ˆAt: ˆT |π )|| q( ˆAt: ˆT |sa≺t, ξ )].
(123)
Where F[φ, sa≺t, ξ ] is the variational free energy of the (non-
active) variational inference (see Equation 45). Variatio nal
inference then minimizes the above expression with respect to
parameters φ and π :
φ∗
sa≺t,ξ , π ∗
sa≺t,ξ := arg min
φ,π
F[π , φ, sa≺t, ξ ]
= arg min
φ
F[φ, sa≺t, ξ ] (124)
+ arg min
π
KL[r( ˆAt: ˆT|π )|| q( ˆAt: ˆT|sa≺t, ξ )].
(125)
We see that the minimization in this case separates into two
minimization problems. The ﬁrst is just the variational infer ence
of Section 6.4 and the second minimizes the KL-divergence
between the parameterized action distribution r( ˆat: ˆT|π ) and the
softmax q( ˆat: ˆT|sa≺t, ξ ) of the Bayesian action-value function. It
is instructive to look at this KL-divergence term closer:
KL[r( ˆAt: ˆT |π )|| q( ˆAt: ˆT |sa≺t, ξ )] = − Hr( ˆAt: ˆT |π ) (126)
−
∑
ˆat: ˆT
r(ˆat: ˆT |π ) log q( ˆat: ˆT |sa≺t, ξ )
= − Hr( ˆAt: ˆT |π )
−
∑
ˆat: ˆT
r(ˆat: ˆT |π ) ˆQ(ˆat: ˆT , sa≺t, ξ )
+ log Z(γ , sa≺t, ξ ). (127)
We see that the optimization of π leads toward high entropy
distributions for which the expectation value of the action- value
function ˆQ(ˆat: ˆT, φ) is large. Action selection could then happen
according to
p(at|mt) :=
∑
ˆat+1:T
r(ˆat: ˆT|π ∗
sa≺t,ξ ). (128)
So the described variational inference procedure, at least
formally, leads to a useful result. However, this is not the ac tive
Frontiers in Neurorobotics | www.frontiersin.org 19 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
inference procedure of Friston et al. (2015) . As noted above the
minimization actually splits into two completely independent
minimizations here. The result of the minimization with res pect
to φ in Equation (125) is actually not used for action selection a nd
since action selection is all that matters here is mere ornam ent.
However, there is a way to make use of it. Recall that plugging
φ∗
sa≺t,ξ into the variational action-value function ˆQ(ˆat: ˆT, φ) means
that it approximates the Bayesian action value function (see
Equation 52). This means that if we deﬁne a softmax distribut ion
r(ˆat: ˆT|φ) of the variational action-value function parameterized
by φ as:
r(ˆat: ˆT|φ) = 1
Z(γ , φ) eγ ˆQ(ˆat: ˆT ,φ). (129)
Then this approximates the softmax of the Bayesian action-val ue
function:
r(ˆat: ˆT|φ∗
sa≺t,ξ ) ≈ q(ˆat: ˆT|sa≺t, ξ ). (130)
Consequently, once we have obtained φ∗
sa≺t,ξ from the ﬁrst
minimization problem in Equation (125) we can plug it into
r(ˆat: ˆT|φ) and then minimize the KL-divergence of r( ˆat: ˆT|π ) to
this distribution instead of the one to q( ˆat: ˆT|sa≺t, ξ ). In this way
the result of the ﬁrst could be reused for the second minimiza tion.
This remains a two part action generation mechanism however.
Active inference combines these two steps into one minimizat ion
by replacing q( ˆat: ˆT|sa≺t, ξ ) in the variational free energy of
Equation (121) with r( ˆat: ˆT|φ). Since r( ˆat: ˆT|φ) thereby becomes
part of the denominator it is also given the same symbol (in our
case q) as the generative model. So we deﬁne:
q(ˆat: ˆT|φ) := r(ˆat: ˆT|φ). (131)
In this form the softmax q( ˆat: ˆT|φ) is a cornerstone of active
inference. In brief, it can be regarded as a prior over action
sequences. To obtain purposeful behaviour it speciﬁes prior
assumptions about what sorts of actions an agent should take
when its belief parameter takes value φ. Strictly speaking the
expression resulting from the replacement q( ˆAt: ˆT|sa≺t, ξ ) →
q(ˆat: ˆT|φ) in Equation (121) is then not a variational free energy
anymore since the variational parameters φ occur in both
the numerator and the denominator. Nonetheless, this is the
functional that is minimized in active inference as describ ed
in
Friston et al. (2015) . So active inference is deﬁned as the
optimization problem (cmp. Friston et al., 2015, Equation 1):
φ∗
sa≺t,ξ , π ∗
sa≺t,ξ = arg min
φ,π
∑
ˆat: ˆT ,ˆe≺t
∫
r(ˆat: ˆT|π ) r(ˆe≺t, θ |φ)
log r(ˆat: ˆT|π ) r(ˆe≺t, θ |φ)
q(s≺t, ˆat: ˆT, ˆe≺t, θ |φ, a≺t, ξ ) dθ (132)
= arg min
φ,π
(F[φ, sa≺t, ξ ]
+ KL[r( ˆAt: ˆT|π )|| q( ˆAt: ˆT|φ)]
)
. (133)
This minimization does not split into the two independent parts
anymore since both the future action distribution q( ˆAt: ˆT|φ) of
the generative model and the approximate posterior factor in the
variational free energy F[φ, sa≺t, ξ ] are parameterized by φ. This
justiﬁes the claim that active inference obtains both belie f update
and action selection through a single principle or optimizatio n.
Compared to
Friston et al. (2015) , we have introduced a
simpliﬁcation of active inference. In the original text, add itional
distributions over γ (with according random variable Ŵ) are
introduced to the generative model as q( γ |ξ Ŵ ) (which is a ﬁxed
prior) and to the approximate posterior as r( γ |φŴ ). For the sake
of completeness, we show the full equations as well. Since γ is
now part of the model, we write q( ˆat: ˆT|γ , φ) instead of q( ˆat: ˆT|φ).
The basic procedure above stays the same. The active inferenc e
optimization becomes:
φ∗
sa≺t,ξ , φŴ∗
sa≺t,ξ , π ∗
sa≺t,ξ
= arg min
φ,φŴ ,π
∑
ˆat: ˆT ,ˆe≺t
∫∫
r(ˆat: ˆT|π ) r(γ |φŴ ) r(ˆe≺t, θ |φ)
× log r(ˆat: ˆT|π ) r(γ |φŴ ) r(ˆe≺t, θ |φ)
q(s≺t, ˆat: ˆT, γ , ˆe≺t, θ |φ, a≺t, ξ ) dθ dγ . (134)
Note that here, by construction, the denominator can be writ ten
as:
q(s≺t, ˆat: ˆT, γ , ˆe≺t, θ |φ, a≺t, ξ )
= q(ˆat: ˆT|γ , φ) q(γ |φŴ ) q(ˆe≺t, θ |sa≺t, ξ ) q(s≺t|a≺t, ξ ). (135)
Which allows us to write Equation (134) with the original
variational free energy again:
φ∗
sa≺t ,ξ , φŴ∗
sa≺t ,ξ , π ∗
sa≺t ,ξ = arg min
φ,φŴ ,π
(F[φ, sa≺t, ξ ]
+ KL[r( ˆAt: ˆT , Ŵ|π , φŴ )|| q( ˆAt: ˆT , Ŵ|φ, ξ Ŵ )]
)
.
(136)
9. APPLICATIONS AND LIMITATIONS
An application of the active inference described here to a
simple maze task can be found in
Friston et al. (2015) . Active
inference using diﬀerent forms of approximate posteriors can be
found in
Friston et al. (2016b) . Here, Friston et al. (2017a) also
includes a knowledge seeking term in addition to the conditi onal
entropy term. In the universal reinforcement learning frame work
Aslanides et al. (2017) also implement a knowledge seeking agent.
These works can be quite directly translated into our framew ork.
For applications of intrinsic motivations that are not so
directly related to our framework see also the references in t he
according Sections 7.3.3 to 7.3.5.
A quantitative analysis of the limitations of the diﬀerent
approaches we discussed is beyond the scope of this publication.
However, we can make a few observations that may help
researchers interested in applying the discussed approaches.
Concerning the computation of the complete posterior by
direct Bayesian methods is not feasible beyond the simplest o f
systems and even then only for very short time durations. As
mentioned in the text it contains a sum over | ˆE|t elements. If the
Frontiers in Neurorobotics | www.frontiersin.org 20 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
time horizon into the future is ˆT − t then the predictive factor
consists of ˆS ˆT−t × ˆE ˆT−t × ˆA ˆT−t entries. This means predicting far
into the future is also not feasible. Therefore ˆT−t will usually have
to be ﬁxed to a small number. Methods that also approximate the
predictive factor (e.g.,
Friston et al., 2016b, 2017a ) may be useful
here. However, to our knowledge, their scalability has not b een
addressed yet. Since in these approaches the predictive factor is
approximated in a similar way as the posterior factor here, we
would expect that it is similar to the scalability of approximat ing
the posterior factor.
Employing variational inference reduces the computational
burden for obtaining a posterior factor considerably. The su m
over all possible past environment histories (the | ˆE|t elements)
is approximated within the optimization. Clearly, by employing
variational inference we inherit all shortcomings of this
method. As mentioned also in
Friston et al. (2016b) variational
inference approximations are known to become overconﬁdent
i.e., the approximate posterior tends to ignore values with low
probabilities (see e.g.,
Bishop, 2011). In practice this can of course
lead to poor decision making. Furthermore, the convergence o f
the optimization to obtain the approximate posterior can also
become slow. As time t increases the necessary computations
for each optimization step in the widely used coordinate asce nt
variational inference algorithm (
Blei et al., 2017 ) grow with t2.
Experiments suggest that the number of necessary optimizatio n
steps also grows over time. At the moment, we do not know how
fast but this may also lead to problems. A possible solution woul d
be to introduce some form of forgetting such that the conside red
past does not grow forever.
Ignoring the problem of obtaining a complete posterior,
we still have to evaluate and select actions. Computing the
information theoretic quantities needed for the mentioned
intrinsic motivations and their induced action-value func tions is
also computationally expensive. In this case ﬁxing the future time
horizon ˆT − t can lead to constant computational requirements.
These grow exponentially with the time horizon which makes
large time horizons impossible without further approximation s.
Note that the action selection mechanisms discussed here al so
require the computation of the action-value functions for ea ch
of the future action sequences.
Active inference is not a standard variational inference
problem and therefore standard algorithms like the coordina te
ascent variational inference may fail in this case. Other
optimization procedures like gradient descent may still work . As
far as we know there have been no studies of the scalability of the
active inference scheme up to now.
10. CONCLUSION
We have reconstructed the active inference approach of
Friston
et al. (2015) in in a formally consistent way. We started by
disentangling the components of inference and action select ion.
This disentanglement has allowed us to also remove the
variational inference completely and formulate the pure Baye sian
knowledge updating for the generative model of
Friston et al.
(2015). We have shown in Section 6.3 that a special case of this
model is equivalent to a ﬁnite version of the model used by the
Bayesian universal reinforcement agent (
Hutter, 2005 ). We then
pointed out how to approximate the pure Bayesian knowledge
updating with variational inference. To formalize the notio n of
intrinsic motivations within this framework, we have intro duced
intrinsic motivation functions that take complete posteriors and
future actions as inputs. These induce action-value functio ns
similar to those used in reinforcement learning. The action -
value functions can then be used for both, the Bayesian and the
variational agent, in standard deterministic or softmax ac tion
selection schemes.
Our analysis of the intrinsic motivations Expected Free
Energy Maximization , Empowerment Maximization , Predictive
Information Maximization, and Knowledge Seeking indicates that
there is signiﬁcant common structure between the diﬀerent
approaches and it may be possible to combine them. At the
time of writing, we have already made ﬁrst steps toward
using the present framework for a systematic quantitative
analysis and comparison of the diﬀerent intrinsic motivations .
Eventually, such studies will shed more conclusive light on the
computational requirements and emergent dynamics of diﬀeren t
motivations. An investigation of the biological plausibilit y of
diﬀerent motivations might lead to diﬀerent results and this i s
of equal interest.
Beyond the comparison of diﬀerent intrinsic motivations
within an active inference framework, the present work can th us
contribute to investigations on the role of intrinsic motiv ations in
living organisms. If biological plausibility of active infere nce can
be upheld, and maintained for alternative intrinsic motivat ions,
then experimental studies might be derived to test diﬀerentia ting
predictions. If active inference was key to cognitive phenome na
such as consciousness, it would be interesting to see how the
cognitive dynamics would be aﬀected by alternative intrinsi c
motivations.
AUTHOR CONTRIBUTIONS
MB, CG, CS, SS, and DP conceived of this study, discussed the
concepts, revised the formal analysis, and wrote the article . MB
contributed the initial formal analysis.
FUNDING
CG is funded by EPSRC grant [EP/L015846/1] (IGGI). CS is
funded by the EU Horizon 2020 programme under the Marie
Sklodowska-Curie grant 705643. DP is funded in part by EC
H2020-641321 socSMCs FET Proactive project.
ACKNOWLEDGMENTS
MB would like to thank Yen Yu for valuable discussions on activ e
inference.
Frontiers in Neurorobotics | www.frontiersin.org 21 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
REFERENCES
Allen, M., and Friston, K. J. (2016). From cognitivism to autopoie sis: towards a
computational framework for the embodied mind. Synthese 195, 2459–2482.
doi: 10.1007/s11229-016-1288-5
Aslanides, J., Leike, J., and Hutter, M. (2017). “Universal rein forcement
learning algorithms: survey and experiments, ” in Proceedings of the 26th
International Joint Conference on Artiﬁcial Intelligence (Melbourne, VIC),
1403–1410.
Attias, H. (1999). “A variational Bayesian framework for graphical models, ” in
Proceedings Advances in Neural Information Processing Syste ms 12, eds S. Solla,
T. Leen, and K. Müller (Cambridge, MA: MIT Press), 209–215.
Attias, H. (2003). “Planning by probabilistic inference, ” in Proceedings 9th
International Workshop on Artiﬁcial Intelligence and Stat istics (Key West, FL).
Ay, N., Bernigau, H., Der, R., and Prokopenko, M. (2012). Informati on-driven self-
organization: the dynamical system approach to autonomous robot b ehavior.
Theor. Biosci. 131, 161–179. doi: 10.1007/s12064-011-0137-9
Ay, N., Bertschinger, N., Der, R., Güttler, F., and Olbrich, E. (2 008). Predictive
information and explorative behavior of autonomous robots. Eur. Phys. J. B
Cond. Matter Complex Syst. 63, 329–339. doi: 10.1140/epjb/e2008-00175-0
Ay, N. and Löhr, W. (2015). The umwelt of an embodied agent–
a measure-theoretic deﬁnition. Theor. Biosci. 134, 105–116.
doi: 10.1007/s12064-015-0217-3
Barber, D., and Agakov, F. (2003). “The IM algorithm: a variation al approach
to information maximization, ” in Proceedings Advances in Neural Information
Processing Systems 16 , eds S. Thrun, L. K. Saul, and B. Schölkopf (Vancouver,
BC: MIT Press), 201–208.
Bialek, W., and Tishby, N. (1999). Predictive information. arXiv:cond-
mat/9902341.
Bishop, C. M. (2011). Pattern Recognition and Machine Learning. Information
Science and Statistics . New York, NY: Springer.
Blei, D. M., Kucukelbir, A., and McAuliﬀe, J. D. (2017). Variatio nal
inference: a review for statisticians. J. Am. Stat. Assoc. 112, 859–877.
doi: 10.1080/01621459.2017.1285773
Botvinick, M., and Toussaint, M. (2012). Planning as inferenc e. Trends Cogn. Sci.
16, 485–488. doi: 10.1016/j.tics.2012.08.006
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017). T he free energy
principle for action and perception: a mathematical review. J. Math. Psychol.
81, 55–79. doi: 10.1016/j.jmp.2017.09.004
Clark, A. (2015). Surﬁng Uncertainty: Prediction, Action, and the Embodied Min d.
Oxford University Press.
Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory . Hoboken,
N.J: Wiley-Interscience.
Dennett, D. C. (1991). Consciousness Explained. London: Penguin Books.
Doshi-Velez, F., Pfau, D., Wood, F., and Roy, N. (2015). Bayes ian nonparametric
methods for partially-Observable reinforcement learning. IEEE Trans. Patt.
Anal. Mach. Intell. 37, 394–407. doi: 10.1109/TPAMI.2013.191
Ellis, B., and Wong, W. H. (2008). Learning causal Bayesian netwo rk
structures from experimental data. J. Am. Stat. Assoc. 103, 778–789.
doi: 10.1198/016214508000000193
Fox, R., and Tishby, N. (2016). “Minimum-information LGQ control part II:
retentive controllers, ” in 2016 IEEE 55th Conference on Decision and Control
(CDC) (Las Vegas), 5603–5609.
Friston, K. (2010). The free-energy principle: a uniﬁed brain theory ? Nat. Rev.
Neurosci. 11, 127–138. doi: 10.1038/nrn2787
Friston, K. (2013a). Consciousness and hierarchical inferenc e.
Neuropsychoanalysis 15, 38–42. doi: 10.1080/15294145.2013.10773716
Friston, K. (2013b). Life as we know it. J. R. Soc. Interface 10, 1–12.
doi: 10.1098/rsif.2013.0475
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O’Do herty, J., and Pezzulo,
G. (2016a). Active inference and learning. Neurosci. Biobehav. Rev. 68(Suppl.
C), 862–879. doi: 10.1016/j.neubiorev.2016.06.022
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and P ezzulo, G.
(2016b). Active inference: a process theory. Neural Comput. 29, 1–49.
doi: 10.1162/NECO_a_00912
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T ., and Pezzulo,
G. (2015). Active inference and epistemic value. Cogn. Neurosci. 6, 187–214.
doi: 10.1080/17588928.2015.1020053
Friston, K., Samothrakis, S., and Montague, R. (2012). Active i nference and
agency: optimal control without cost functions. Biol. Cybernet. 106, 523–541.
doi: 10.1007/s00422-012-0512-8
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., a nd Ondobaka, S.
(2017a). Active inference, curiosity and insight. Neural Comput. 29, 2633–2683.
doi: 10.1162/neco_a_00999
Friston, K. J., Parr, T., and de Vries, B. (2017b). The graphical bra in:
belief propagation and active inference. Netw. Neurosci. 1, 381–414.
doi: 10.1162/NETN_a_00018
Froese, T., and Ziemke, T. (2009). Enactive artiﬁcial intelligen ce: investigating
the systemic organization of life and mind. Artif. Intell. 173, 466–500.
doi: 10.1016/j.artint.2008.12.001
Gregor, K., Rezende, D. J., and Wierstra, D. (2016). Variationa l intrinsic control.
arXiv [Preprint]. arXiv:1611.07507 .
Guckelsberger, C., and Salge, C. (2016). “Does empowerment maximisa tion allow
for enactive artiﬁcial agents?” in Proceedings of the Fifteenth International
Conference on the Synthesis and Simulation of Living Systems (A life 2016)
(Cancun: MIT Press), 8.
Guckelsberger, C., Salge, C., and Colton, S. (2016a). “Intrinsi cally motivated
general companion NPCs via coupled empowerment maximisation, ” in
Proceedings Conference on Computational Intelligence in Ga mes (Fira).
Guckelsberger, C., Salge, C., Saunders, R., and Colton, S. (201 6b). “Supportive
and antagonistic behaviour in distributed computational creat ivity via coupled
empowerment maximisation, ” in Proceedings 7th International Conference on
Computational Creativity (Paris).
Guckelsberger, C., Salge, C., and Togelius, J. (2018). “New and surprising ways
to be mean: adversarial NPCs with coupled empowerment minimisation, ” i n
Proceedings Conference on Computational Intelligence in Ga mes (Maastricht).
Hutter, M. (2005). “Universal artiﬁcial intelligence: sequenti al decisions based on
algorithmic probability, ” in Texts in Theoretical Computer Science. An EATCS
Series, eds W. Bauer, G. Rozenberg, and A. Salomaa (Berlin; Heidelberg:
Springer-Verlag).
Karl, M., Soelch, M., Becker-Ehmck, P., Benbouzid, D., van der Sma gt, P.,
and Bayer, J. (2017). Unsupervised real-time control through variat ional
empowerment. arXiv [Preprint]. arXiv:1710.05101 .
Klyubin, A., Polani, D., and Nehaniv, C. (2005). “Empowerment: a un iversal
agent-centric measure of control, ” in The 2005 IEEE Congress on Evolutionary
Computation, 2005, Vol. 1 (Edinburgh), 128–135.
Leike, J. (2016). Nonparametric general reinforcement learning. arXiv [Preprint].
arXiv:1611.08944.
Linson, A., Clark, A., Ramamoorthy, S., and Friston, K. (2018). The active
inference approach to ecological perception: general information dyn amics
for natural and artiﬁcial embodied cognition. Front. Robot. AI 5:21.
doi: 10.3389/frobt.2018.00021
Little, D. Y.-J., and Sommer, F. T. (2013). Maximal mutual informati on, not
minimal entropy, for escaping the Dark Room. Behav. Brain Sci. 36, 220–221.
doi: 10.1017/S0140525X12002415
Lunn, D. J., Thomas, A., Best, N., and Spiegelhalter, D. (2000). W inBUGS - A
Bayesian modelling framework: concepts, structure, and extensibilit y. Stat.
Comput. 10, 325–337. doi: 10.1023/A:1008929526011
Manzotti, R., and Chella, A. (2018). Good old-fashioned artiﬁci al
consciousness and the intermediate level fallacy. Front. Robot. AI 5:39.
doi: 10.3389/frobt.2018.00039
Martius, G., Der, R., and Ay, N. (2013). Information driven self-
organization of complex robotic behaviors. PLoS ONE 8:e63400.
doi: 10.1371/journal.pone.0063400
Martius, G., Jahn, L., Hauser, H., and Hafner, V. V. (2014). “Se lf-exploration
of the stumpy robot with predictive information maximization, ” in From
Animals to Animats 13: 13th International Conference on Simulati on of Adaptive
Behavior, SAB 2014, Castellón, Spain , eds A. P. del Pobil, E. Chinellato, E.
Martinez-Martin, J. Hallam, E. Cervera, and A. Morales (Springer), 32–4 2.
Minka, T. P. (2001). “Expectation propagation for approximate Bayes ian
inference, ” in Proceedings of the Seventeenth Conference on Uncertainty
in Artiﬁcial Intelligence , UAI’01 (San Francisco, CA: Morgan Kaufmann
Publishers Inc.), 362–369.
Mohamed, S., and Rezende, D. J. (2015). “Variational informatio n maximisation
for intrinsically motivated reinforcement learning, ” in Proceedings Advances in
Neural Information Processing Systems 28 , eds C. Cortes, N.D. Lawrence, D.
Frontiers in Neurorobotics | www.frontiersin.org 22 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
D. Lee, M. Sugiyama, and R. Garnett (Montréal, BC: Curran Associate s, Inc.),
2125–2133.
Orseau, L., Lattimore, T., and Hutter, M. (2013). “Universal kno wledge-seeking
agents for stochastic environments, ” in Algorithmic Learning Theory , Number
8139 in Lecture Notes in Computer Science, eds S. Jain, R. Munos, F. Stephan,
and T. Zeugmann (Berlin; Heidelberg: Springer)158–172.
Ortega, P. A. (2011). Bayesian causal induction. arXiv [Preprint]. arXiv:1111.0708 .
Ortega, P. A., and Braun, D. A. (2010). A minimum relative entropy princi ple for
learning and acting. J. Artif. Intell. Res. 38, 475–511. doi: 10.1613/jair.3062
Ortega, P. A., and Braun, D. A. (2014). Generalized Thompson sampling f or
sequential decision-making and causal inference. Complex Adapt. Syst. Model.
2:2. doi: 10.1186/2194-3206-2-2
Oudeyer, P.-Y., and Kaplan, F. (2009). What is intrinsic motivat ion?
A typology of computational approaches. Front. Neurorobot. 1:6.
doi: 10.3389/neuro.12.006.2007
Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. (2007). Intrinsic motivation systems
for autonomous mental development. IEEE Trans. Evol. Comput. 11, 265–286.
doi: 10.1109/TEVC.2006.890271
Pearl, J. (2000). Causality: Models, Reasoning, and Inference . Cambridge:
Cambridge University Press.
Pfeifer, R., Iida, F., and Bongard, J. (2005). New robotics: de sign principles for
intelligent systems. Artif. Life 11, 99–120. doi: 10.1162/1064546053279017
Ross, S. and Pineau, J. (2008). “Model-based Bayesian reinfor cement learning in
large structured domains, ” in Proceedings 24th Conference on Uncertainty in
Artiﬁcial Intelligence (Helsinki), 476–483.
Ryan, R. M., and Deci, E. L. (2000). Intrinsic and extrinsic moti vations:
classic deﬁnitions and new directions. Contemp. Educ. Psychol. 25, 54–67.
doi: 10.1006/ceps.1999.1020
Salge, C., Glackin, C., and Polani, D. (2014). “Empowerment–an intro duction, ”
in Guided Self-Organization: Inception , ed M. Prokopenko (Berlin; Heidelberg:
Springer), 67–114.
Salge, C., Guckelsberger, C., Canaan, R., and Mahlmann, T. (2018 ). “Accelerating
empowerment computation with UCT tree search, ” in Proceedings Conference
on Computational Intelligence in Games (Maastricht: IEEE).
Salge, C., and Polani, D. (2017). Empowerment as replacement for the thre e laws
of robotics. Front. Robot. AI 4:25. doi: 10.3389/frobt.2017.00025
Santucci, V. G., Baldassarre, G., and Mirolli, M. (2013). Which is t he best
intrinsic motivation signal for learning multiple skills? Front. Neurorobot. 7:22.
doi: 10.3389/fnbot.2013.00022
Schmidhuber, J. (2010). Formal theory of creativity, fun,
and intrinsic motivation (1990-2010). IEEE Trans. Auton.
Mental Dev. 2, 230–247. doi: 10.1109/TAMD.2010.20
56368
Storck, J., Hochreiter, S., and Schmidhuber, J. (1995). Reinfo rcement driven
information acquisition in non-deterministic environments. in Proceedings of
the International Conference on Artiﬁcial Neural Networks , Vol. 2 (Perth, WA),
159–164.
Sutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An Introduction .
Cambridge, MA; London: MIT Press.
Toussaint, M. (2009). Probabilistic inference as a model of planne d behavior.
Künstliche Intelligenz 3, 23–29.
Vehtari, A., Gelman, A., Sivula, T., Jylänki, P., Tran, D., Sahai, S ., et al.
(2014). Expectation propagation as a way of life: a framework for
Bayesian inference on partitioned data. arXiv [Preprint]. arXiv:14
12.4869.
Wainwright, M. J., and Jordan, M. I. (2007). Graphical models, expon ential
families, and variational inference. Foundations Trends Mach. Learn. 1, 1–305.
doi: 10.1561/2200000001
Winn, J. and Bishop, C. M. (2005). Variational message passing. J. Mach. Learn.
Res. 6, 661–694.
Conﬂict of Interest Statement: CG, CS, SS, and DP declare no competing
interests. In accordance with Frontiers policy MB declares that he i s employed by
company Araya Incorporated, Tokyo, Japan.
Copyright © 2018 Biehl, Guckelsberger, Salge, Smith and Polani. This is an open-
access article distributed under the terms of the Creative C ommons Attribution
License (CC BY). The use, distribution or reproduction in ot her forums is permitted,
provided the original author(s) and the copyright owner(s) a re credited and that the
original publication in this journal is cited, in accordanc e with accepted academic
practice. No use, distribution or reproduction is permitte d which does not comply
with these terms.
Frontiers in Neurorobotics | www.frontiersin.org 23 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
APPENDIX
A. POSTERIOR FACTOR
Here we want to calculate the posterior factor q( ˆe≺t, θ |sa≺t, ξ )
of the complete posterior in Equation (16) without an
approximation (i.e., as in direct, non-variational Bayesian
inference).
q(ˆe≺t, θ |sa≺t, ξ )
= 1
q(s≺t|a≺t, ξ ) q(s≺t, ˆe≺t, θ |a≺t, ξ ) (A1)
= 1
q(s≺t|a≺t, ξ ) q(s≺t|ˆe≺t, θ 1) q(ˆe≺t|a≺t, θ 2, θ 3) q(θ |ξ ) (A2)
= 1
q(s≺t|a≺t, ξ )
t∏
τ =0
q(sτ |ˆeτ , θ 1)
t∏
r=1
q(ˆer|ar, ˆer−1, θ 2) q(ˆe0|θ 3)
3∏
i=1
q(θ i|ξ i). (A3)
We see that the numerator is given by the generative model. The
denominator can be calulated according to:
q(s≺t|a≺t, ξ )
=
∫
/Delta1/Theta1
q(s≺t|a≺t, θ ) q(θ |ξ ) dθ (A4)
=
∫
/Delta1/Theta1

∑
ˆe≺t
q(ˆe0|θ 3)
t∏
τ =0
q(sτ |ˆeτ , θ 1)
t∏
r=1
q(ˆer|ar, ˆer−1, θ 2)


3∏
i=1
q(θ i|ξ i) dθ (A5)
=
∑
ˆe≺t
∫
/Delta1/Theta1
q(ˆe0|θ 3)
t∏
τ =0
q(sτ |ˆeτ , θ 1)
t∏
r=1
q(ˆer|ar, ˆer−1, θ 2)
3∏
i=1
q(θ i|ξ i) dθ (A6)
=
∑
ˆe≺t
(∫
q(ˆe0|θ 3) q(θ 3|ξ 3) dθ 3
∫ t∏
τ =0
q(sτ |ˆeτ , θ 1) q(θ 1|ξ 1) dθ 1
×
∫ t∏
r=1
q(ˆer|ar, ˆer−1, θ 2) q(θ 2|ξ 2) dθ 2
)
(A7)
The three integrals can be solved analytically if q( θ i|ξ i) are
chosen as conjugate priors to q( sτ |ˆeτ , θ 1), q( ˆer|ar, ˆer−1, θ 2), and
q(ˆe0|θ 3) respectively. However, the sum is over |E|t terms and
therefore untractable as time increases.
B. APPROXIMATE POSTERIOR
PREDICTIVE DISTRIBUTION
Here, we calculate the (variational) approximate predictive
posterior distribution of q( ˆst: ˆT|ˆat: ˆT, sa≺t, ξ ) from a given
approximate complete posterior. This expression plays a role
in multiple intrinsic motivation functions like empowerment
maximization, predictive information maximization, and
knowledge seeking. For an arbitrary φ we have:
r(ˆst: ˆT|ˆat: ˆT, φ)
: =
∑
ˆe≺t
∫
q(ˆst: ˆT|ˆat: ˆT, ˆet−1, θ ) r(ˆe≺t, θ |φ) dθ (A8)
=
∑
ˆet−1
∫
q(ˆst: ˆT|ˆat: ˆT, ˆet−1, θ ) r(ˆet−1, θ |φ) dθ (A9)
=
∑
ˆet−1
(∫
q(ˆst: ˆT|ˆat: ˆT, ˆet−1, θ )
3∏
i=1
r(θ i|φi) dθ
)
r(ˆet−1|φEt−1 )
(A10)
=
∑
ˆet−1



∑
ˆet: ˆT
∫
q(ˆst: ˆT|ˆet: ˆT, θ 1) r(θ 1|φ1) dθ 1×
×
∫
q(ˆet: ˆT|ˆat: ˆT, ˆet−1, θ 2) r(θ 2|φ2) dθ 2 r(ˆet−1|φEt−1 ) (A11)
=
∑
ˆet−1



∑
ˆet: ˆT
∫ ˆT∏
τ =t
q(ˆsτ |ˆeτ , θ 1) r(θ 1|φ1) dθ 1×
×
∫ ˆT∏
τ =t
q(ˆeτ |ˆaτ , ˆer−1, θ 2) r(θ 2|φ2) dθ 2 r(ˆet−1|φEt−1 ) (A12)
=
∑
ˆet−1
∑
ˆet: ˆT
r(ˆst: ˆT|ˆet: ˆT, φ1) r(ˆet: ˆT|ˆat: ˆT, ˆet−1, φ2) r(ˆet−1|φEt−1 )
(A13)
From ﬁrst to second line we usually have to marginalize
q(ˆe≺t, θ |sa≺t, ξ ) to q( ˆet−1, θ |sa≺t, ξ ) with a sum over all |E|t−1
possible environment histories ˆe≺t−1. Using the approximate
posterior, we can use r( ˆet−1|φEt−1 ) directly without dealing with
the intractable sum. From third to fourth line, r( θ 3|φ3) drops
out since it can be integrated out (and its integral is equal t o
one). Note that during the optimization Equation (47) r( θ 3|φ3)
does play a role so it is not superﬂuous.From ﬁfth to last line,
we perform the integration over the parameters θ 1 and θ 2.
These integrals can be calculated analytically if we choose t he
models r( θ 1|φ1) and r( θ 2|φ2) as conjugate priors to q( s|e, θ 1)
and q( e′|a′, e, θ 2). Variational inference prediction of the next
n = ˆT − t − 1 sensor values requires the sum and calculation
of | ˆE|n terms for | ˆS|n possible futures.
C. NOTATION TRANSLATION TABLES
A table to translate between our notation and the one used in
Friston et al. (2015) . The translation is also valid in many cases
for Friston et al. (2016a,b, 2017a) . Some of the parameters shown
here only show up in the latter publications.
Frontiers in Neurorobotics | www.frontiersin.org 24 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
This article Friston et al. (2015) Note
et ∈ E Actual environment states
ˆet ∈ ˆE st ∈ S Estimated/modeled environment states
st ∈ S ot ∈ /Omega1 Actual/observed sensor or outcome values
ˆst ∈ ˆS = S ot ∈ /Omega1 Estimated/modeled (usually future) sensor or outcome value s. Note that the index τ instead of t often indicates an
estimated future sensor value in Friston et al. (2015) .
at ∈ A at ∈ A Actions
ˆat ∈ ˆA = A ut ∈ U Contemplated (usually future) actions
mt ∈ M Agent memory state
ˆat: ˆT π , ˜u π and ˜u both uniquely specify future action sequences
θ θ Generative model parameters
q(ˆs|ˆe, θ 1) = q(ˆs|ˆe) P(o|s) = Aos Model sensor dynamics, not parameterised in Friston et al. (2015) , A is a matrix representation
q(ˆe′|ˆa′, ˆe, θ 2) = q(ˆe′|ˆa′, ˆe) P(s′|s, u) = B(u)s′s Model environment dynamics, not parameterised in Friston et al. (2015) , B(u) is a matrix representation for each
possible action u
q(ˆe0|θ 3) P(s0|m) = Ds0 Modeled initial environment state, not parameterised in Friston et al. (2015) , D is a vector representation. Note, the
parameter m is a ﬁxed hyperparameter
ξ = (ξ 1, ξ 2, ξ 3) m Generative model hyperparam. or model parameter that subsu mes all hyperparameters
ξ 1 sensor dynamics hyperparam.
ξ 2 Environment dynamics hyperparam.
ξ 3 Initial environment state hyperparam.
ξ Ŵ (α, β) Precision hyperparam.
(φ, φŴ ) µ Variational param.
φE0: ˆT
(
s Environment states variational param.,
φEτ
(
s τ for each timestep τ
φ1 Sensor dynamics variational param.
φ2 Environment dynamics variational param.
φ3 Initial environment state variational param.
π
(
π Future action sequence variational param.
φŴ
(
γ Precision variational param.
ˆQ(ˆat: ˆT , φ) Q(π ) = Q(˜u|π ) Variational action-value function. The dependence of Q(˜u|π ) on
(
s t is omitted
p(s⪯ t, e⪯ t, a≺t) R(˜o, ˜s, ˜a) Our physical environment corresponds to the generative pr ocess
q(ˆs⪯ t, ˆe⪯ t, ˆat: ˆT , γ |a≺t, ξ ) P(˜o, ˜s, ˜u, γ |˜a, m) The generative model for active inference including γ (which we mostly omit)
r(ˆe0: ˆT , ˆat: ˆT , γ |π , φ, φŴ ) Q(˜s, ˜u, γ |µ ) Approximate complete posterior for active inference
pd(ˆsτ ) P(oτ |m) Prior over future outcomes.
Since our treatment is more general than that of Friston et al. (2015) and quite similar (though not identical) to the treatment in
Friston et al. (2016a,b, 2017a) we also give the relations to variables in those publications . We hope this will help interested readers to
understand the latter publications even if some aspects of tho se are diﬀerent. A discussion of those diﬀerences is beyond the scope of
the present article.
Frontiers in Neurorobotics | www.frontiersin.org 25 August 2018 | Volume 12 | Article 45
Biehl et al. Expanding the Active Inference Landscape
This article Friston et al. (2016b) Note
et ∈ E Actual environment states
ˆet ∈ ˆE st ∈ S Estimated/modeled environment states
st ∈ S ot ∈ /Omega1 Actual/observed sensor or outcome values
ˆst ∈ ˆS = S ot ∈ /Omega1 Estimated/modeled (usually future) sensor or outcome value s. Note that the index τ instead of t
often indicates an estimated future sensor value in Friston et al. (2015) .
at ∈ A ut ∈ A Actions
ˆat ∈ ˆA = A ut ∈ Υ Contemplated (usually future) actions
mt ∈ M Agent memory state
ˆa0: ˆT π , action sequences
θ θ Generative model parameters
θ 1 A Sensor dynamics param.
θ 2 B Environment dynamics param.
θ 3 D Initial environment state param.
ξ η Generative model hyperparam. or model parameter that subsu mes all hyperparameters
ξ 1 a sensor dynamics hyperparam.
ξ 2 b Environment dynamics hyperparam.
ξ 3 d Initial environment state hyperparam.
ξ Ŵ β Precision hyperparam.
(φ, φŴ ) η Variational param.
φE0: ˆT s0:T Environment states variational param.
q(ˆeτ |ˆat: ˆT , a0 : t−1, φEτ ) ( sπ
τ )ˆeτ For each sequence of actions and for each timestep there is a p arameter sπ
τ . Since a
categorical distribution is used, the parameter is a vector of probabilities whose entry ˆeτ is
equal to the probability of ˆeτ if we set ˆE = { 1, ..., | ˆE|}
φ1 a Sensor dynamics variational param.
φ2 b Environment dynamics variational param.
φ3 d Initial environment state variational param.
π π Future action sequence variational param.
φŴ β Precision variational param.
ˆQ(ˆat: ˆT , φ) −G(π ) Variational action-value function. The dependence of G(π ) on sπ
0:T is omitted
p(s⪯ t, e⪯ t, a≺t) R(˜o, ˜s, ˜a) Our physical environment corresponds to the generative pr ocess
q(ˆs⪯ t, ˆe0: ˆT , ˆa0: ˆT , γ , θ , ξ ) P(˜o, ˜s, π , γ , A, B, D|a, b, d, β) The generative model for active inference
r(ˆe0: ˆT , ˆa0: ˆT , γ , θ |π , φŴ , φ) Q(˜s, π , A, B, D, γ |sπ
0: ˆT , π , a, b, d, β) Approximate complete posterior for active inference
pd(ˆsτ ) P(oτ ) = σ (Uτ ) Prior over future outcomes.
Frontiers in Neurorobotics | www.frontiersin.org 26 August 2018 | Volume 12 | Article 45