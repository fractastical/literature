1 
 
Experimental validation of the free-energy principle with in vitro 1 
neural networks 2 
 3 
Takuya Isomura1*, Kiyoshi Kotani2, Yasuhiko Jimbo3, Karl J. Friston4 4 
1 Brain Intelligence Theory Unit, RIKEN Center for Brain Science, 2-1 Hirosawa, Wako, Saitama 351-5 
0198, Japan 6 
2 Research Center for Advanced Science and Technology, The University of Tokyo, 4-6-1 Komaba, 7 
Meguro-ku, Tokyo 153-8904, Japan 8 
3 Department of Precision Engineering, School of Engineering, The University of Tokyo, 7-3-1 9 
Hongo, Bunkyo-ku, Tokyo 113-8656, Japan 10 
4 Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, 12 11 
Queen Square, London, WC1N 3AR, UK 12 
* Corresponding author email: takuya.isomura@riken.jp 13 
  14 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
2 
 
Abstract 15 
Empirical applications of the free-energy principle are not straightforward because they entail a 16 
commitment to a particular process theory, especially at the cellular and synaptic levels. Using a 17 
recently established reverse engineering technique, we confirm the quantitative predictions of the 18 
free-energy principle using in vitro networks of rat cortical neurons that perform causal inference. 19 
Upon receiving electrical stimuli‚Äîgenerated by mixing two hidden sources‚Äîneurons self-20 
organised to selectively encode the two sources. Pharmacological up- and downregulation of 21 
network excitability disrupted the ensuing inference, consistent with changes in prior beliefs about 22 
hidden sources. As predicted, changes in effective synaptic connectivity reduced variational free 23 
energy, where the connection strengths encoded parameters of the generative model. In short, we 24 
show that variational free energy minimisation can quantitatively predict the self-organisation of 25 
neuronal networks, in terms of their responses and plasticity. These results demonstrate the 26 
applicability of the free-energy principle to in vitro neural networks and establish its predictive 27 
validity in this setting. 28 
 29 
INTRODUCTION 30 
Elucidating the self-organising principles of biological neural networks is one of the most 31 
challenging questions in the natural sciences, and should prove useful for characterising impaired 32 
brain function and developing biologically inspired (i.e., biomimetic) artificial intelligence. 33 
According to the free-energy principle, perception, learning, and action‚Äîof all biological 34 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
3 
 
organisms‚Äîcan be described as minimising variational free energy, as a tractable proxy for 35 
minimising the surprise (i.e., improbability) of sensory inputs [1,2]. By doing so, neuronal (and 36 
neural) networks are considered to perform variational Bayesian inference [3]. This inference 37 
follows from treating neuronal dynamics as a gradient flow on variational free energy, which can 38 
be read as a form of belief updating about the network‚Äôs external milieu. The free energy in 39 
question is a functional of a generative model that expresses a hypothesis about how sensory data 40 
are generated from latent or hidden states. However, to apply the free-energy principle at the 41 
cellular and synaptic levels, it is necessary to identify the requisite generative model that explains 42 
neuronal dynamics (i.e., inference) and changes in synaptic efficacy (i.e., learning). 43 
The activity of neurons has also been modelled with realistic spiking neuron models [4‚Äì6] or 44 
reduced rate coding models [7]. Moreover, synaptic plasticity‚Äîthat depends on the firing of pre- 45 
and postsynaptic neurons [8‚Äì12]‚Äîhas been modelled as Hebbian-type plasticity rules [13‚Äì15]. 46 
Although a precise link between the equations that underwrite these models‚Äîderived from 47 
physiological phenomena‚Äîand the corresponding equations from the free-energy principle has 48 
not been fully established, we recently identified a formal equivalence between neural network 49 
dynamics and variational Bayesian inference [16‚Äì18]. Specifically, we reverse-engineered a class of 50 
biologically plausible cost functions‚Äîfor canonical neural networks‚Äîand showed that the cost 51 
function can be cast as variational free energy, under a class of well-known partially observed 52 
Markov decision process (POMDP) models. This suggests that any (canonical) neural network, 53 
whose activity and plasticity minimise a common cost function, implicitly performs variational 54 
Bayesian inference and learning about external states. This ‚Äòreverse engineering‚Äô approach‚Äî55 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
4 
 
guaranteed by formal equivalence‚Äîallows us, for the first time, to identify the implicit generative 56 
model from empirical neuronal activity. Further, it can precisely link quantities in biological 57 
neuronal networks with those in variational Bayesian inference. This enables an experimental 58 
validation of the free-energy principle, when applied to these kinds of canonical networks. 59 
Having said this, the free-energy principle is sometimes considered to be experimentally 60 
irrefutable in the sense that it can describe any observed biological data [19]. However, when 61 
applying the free-energy principle to a particular system, one can examine its predictive validity by 62 
asking whether it can predict systemic responses [18]. This offers a formal avenue for validation of 63 
(applications of) the free-energy principle. To establish predictive validity, one needs to monitor 64 
the long-term self-organisation of neuronal networks and compare their dynamics and 65 
architecture with theoretical predictions. 66 
To pursue this kind of validation, we used a previously established microelectrode array (MEA) 67 
cell culture system for the long-term monitoring of the self-organisation of in vitro neural networks 68 
[20,21]. We have used this setup to investigate causal inference in cortical cells obtained from rat 69 
embryos [22,23]. Causal inference is a simple form of Bayesian inference; namely, inferring and 70 
disentangling multiple causes of sensory inputs in the sense of blind source separation [24‚Äì26]. 71 
Although blind source separation is essential to explain the cocktail party effect‚Äîthe ability of 72 
partygoers to distinguish the speech of one speaker from others in a noisy room [27,28]‚Äîits 73 
precise neuronal mechanisms have yet to be elucidated. We previously demonstrated that, upon 74 
receiving sensory stimuli, some population of neurons in in vitro neural networks self-organised (or 75 
learned) to infer hidden sources by responding specifically to distinct causes [22]. Subsequently, 76 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
5 
 
we showed that this sensory learning is consistent with variational free energy minimisation under 77 
a POMDP generative model [23]. These results‚Äîand related in vitro work [29‚Äì34]‚Äîspeak to the 78 
tractability and stability of this neuronal system, making it an ideal tool for examining theoretical 79 
predictions in a precise and quantitative fashion. 80 
In the present work, we attempted an experimental validation of the free-energy principle by 81 
showing that it predicts the quantitative self-organisation of in vitro neural networks using an 82 
established in vitro causal inference paradigm. Henceforth, we will refer to in vitro neural networks 83 
as neuronal networks and reserve the term neural network for an in silico model. We reverse-84 
engineered an implicit generative model (including prior beliefs), under which a neuronal network 85 
operates. We subsequently demonstrated that the free-energy principle can predict the trajectory 86 
of synaptic plasticity (i.e., learning curve) as well as neuronal responses after learning, based 87 
exclusively on empirical neuronal responses before training. 88 
Using pharmacological manipulations, we further examined whether the change in baseline 89 
excitability of in vitro networks was consistent with the change in prior beliefs about hidden states 90 
(i.e., the state prior), confirming that priors over hidden states are encoded by firing thresholds. 91 
These results demonstrate that the self-organisation of neuronal networks can be cast as Bayesian 92 
belief updating. This endorses the plausibility of the free-energy principle as an account of self-93 
organisation in neural and neuronal networks. We conclude by discussing possible extensions of 94 
our reverse-engineering approach to in vivo data. 95 
 96 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
6 
 
RESULTS 97 
Equivalence between canonical neural networks and variational Bayes 98 
First, we summarise the mathematical (or natural) equivalence between canonical neural 99 
networks and variational Bayesian inference, which enables one to apply the free-energy principle 100 
to predict empirical data. In this work, we adopted an experimental setup that could be 101 
formulated as a simple POMDP generative process that does not exhibit state transitions (Fig. 1a). 102 
Here, two-dimensional binary hidden sources ùë† = (ùë†!, ùë†") were sampled at random from a prior 103 
distribution ùê∑ in a mutually independent manner, and 32-dimensional sensory inputs ùëú =104 
(ùëú!, ‚Ä¶ , ùëú#") were generated from ùë† with a categorical distribution characterised by a mixing 105 
matrix ùê¥. Each element of ùë† and ùëú took either a 1 (ON) or a 0 (OFF) state. The left stimuli group 106 
(ùëú!, ‚Ä¶ , ùëú!$) in Fig. 1a (left) took the value of source 1 with a 75% probability, or the value of source 107 
2 with a 25% probability. In contrast, the right group (ùëú!%, ‚Ä¶ , ùëú#") took the value of source 1 or 2 108 
with a 25% or 75% probability, respectively. Analogous to the cocktail party effect [27,28], this 109 
setup is formally homologous to the task of distinguishing the voices of speakers 1 (ùë†!) and 2 (ùë†") 110 
based exclusively on mixed auditory inputs (ùëú), and in the absence of supervision. Here, the mixing 111 
(a.k.a., likelihood) matrix (ùê¥) determines the mixing of the two voices, and the prior (ùê∑) 112 
corresponds to the frequency or probability of each speaker generating speech. Hence, neurons 113 
must unmix sensory inputs into hidden sources to perceive the underlying causes. Please refer to 114 
the Methods section ‚ÄòGenerative process‚Äô for the formal expression in terms of probability 115 
distributions. 116 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
7 
 
In this work, we considered that in vitro neurons can be modelled as a canonical neural network 117 
comprising a single feed-forward layer of rate coding models (Fig. 1b, top left) [16]. We considered 118 
two distinct ensembles of neurons. Upon receiving sensory inputs ùëú, these neurons computed the 119 
weighted sum of sensory inputs weighted by a synaptic strength matrix W to generate a response 120 
(firing intensity) ùë• = (ùë•!, ùë•"). This canonical neural network has a certain biological plausibility 121 
because it derives from realistic neuron models [4‚Äì6] through some approximations [17]; further, 122 
its fixed point equips the rate coding model [7] with the widely used sigmoid activation function, 123 
also known as a neurometric function [35]. We will show below that this canonical neural network 124 
is a plausible computational architecture for neuronal networks that receive sensory stimuli. 125 
Previous work has identified a class of biologically plausible cost functions for canonical neural 126 
networks that underlie both neuronal responses and synaptic plasticity [16,17]. This cost function 127 
can be obtained by simply calculating the integral of the neural activity equation (Fig. 1b, middle 128 
left; see the Methods section ‚ÄòCanonical neural networks‚Äô for details). The reconstructed neural 129 
network cost function L is biologically plausible because both neuronal responses and synaptic 130 
plasticity equations can be derived as a gradient descent on L. The ensuing synaptic plasticity rule 131 
has a biologically plausible form, comprising Hebbian plasticity [13], accompanied by an activity-132 
dependent homeostatic plasticity [36] (Fig. 1b, bottom left). 133 
Variational Bayesian inference casts belief updating as revising a prior belief to the 134 
corresponding (approximate) posterior belief based on a sequence of observations. The 135 
experimental setup considered here is expressed as a POMDP generative model [37,38]. The 136 
inversion of this model‚Äîvia a gradient descent on variational free energy‚Äîcorresponds to 137 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
8 
 
inference. In other words, the generative model generates sensory consequences from hidden 138 
causes (i.e., two sources), while model inversion (i.e., inference) maps from sensory consequences 139 
to hidden causes (Fig. 1a, right). Variational free energy F is specified by the sensory input and 140 
probabilistic beliefs about hidden states under a generative model. Minimisation of variational free 141 
energy, with respect to these beliefs, yields the posterior over hidden states ùê¨& (Fig. 1b, top right) 142 
and parameters ùêÄ (Fig. 1b, bottom right), realising Bayesian inference and learning, respectively. 143 
The explicit forms of posterior beliefs are described in the Methods section ‚ÄòVariational Bayesian 144 
inference‚Äô. 145 
Crucially, previous work has shown that the neural network cost function L can be read as 146 
variational free energy F [16,17]. This equivalence allows us to identify the physiological 147 
implementation of variational Bayesian inference by establishing a one-to-one mapping between 148 
neural network quantities and the quantities in Bayesian inference, as summarised in Table 1. 149 
Namely, neural activity (ùë•) of the canonical neural networks corresponds to the posterior 150 
expectation about the hidden states (ùê¨), synaptic strengths (ùëä) correspond to the posterior 151 
expectation about the parameters (ùêÄ), and firing threshold factors (ùúô) correspond to the initial 152 
state prior (ùê∑). These mappings establish a formal relationship between a neural network 153 
formulation (Fig. 1b, left) and a variational Bayesian formulation (Fig. 1b, right). In summary, the 154 
neural activity and plasticity of canonical networks that minimise a common cost function perform 155 
variational Bayesian inference and learning, respectively. 156 
This notion is essential because, by observing neuronal responses, we can reverse-engineer the 157 
implicit generative model‚Äîunder which the neuronal network operates‚Äîfrom empirical neuronal 158 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
9 
 
responses, to characterise the neuronal network in terms of Bayesian inference (Fig. 1c) [18]. 159 
Perhaps surprisingly, using the reverse engineering technique, if one can derive the neural activity 160 
equation from experimental data (Fig. 1c, step 1,2), it is possible to identify the generative model 161 
that the biological system effectively employs (step 3,4). This allows one to link empirical data to 162 
quantities in variational Bayesian inference. Subsequently, by computing the derivative of 163 
variational free energy under the generative model, one can derive the synaptic plasticity 164 
predicted theoretically (step 5). In short, if one has neuronal response data, one can predict how 165 
synaptic plasticity will unfold over time. This means that if the free-energy principle applies, it will 166 
predict the self-organisation of neuronal networks (step 6). 167 
The virtue of the free-energy principle is that it lends an explainability to neuronal network 168 
dynamics and architectures, in terms of variational Bayesian inference. Given this generative 169 
model, the free-energy principle provides qualitative predictions of the dynamics and self-170 
organisation of neuronal networks, under the given experimental environment. In other words, 171 
because neuronal responses and synaptic plasticity are expected to minimise variational free 172 
energy by exploiting the shortest path (i.e., a geodesic or path of least action) on the free energy 173 
landscape, this property in turn enables us to theoretically predict a plausible synaptic trajectory 174 
(i.e., activity-dependent plasticity). 175 
In the remainder of this paper, we examine the plausibility of variational free energy 176 
minimisation as the mechanism underlying the self-organisation of neuronal networks. We will 177 
compare the empirical encoding of the sources of sensory inputs with a synthetic simulation of 178 
ideal Bayesian encoding, and investigate whether variational free energy minimisation can predict 179 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
10 
 
the neuronal responses and plasticity of in vitro networks. 180 
 181 
 182 
Fig. 1. Reverse engineering of the generative model from empirical data. In (a)‚Äì(c), panels on the 183 
left-hand side depict neural (and neuronal) network formation, while panels on the right-hand side 184 
depict variational Bayes formation. a Schematics of experimental setup (left) and corresponding 185 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
11 
 
POMDP generative model (right). Two sequences of independent binary hidden sources generate 186 
32 sensory stimuli through a mixing matrix A, which were applied into cultured neurons on an MEA 187 
as electrical pulses. Waveforms at the bottom represent the spiking responses to a sensory 188 
stimulus. The diagram on the right-hand side depicts the POMDP scheme expressed as a Forney 189 
factor graph [60‚Äì62]. The variables in bold (e.g., ùê¨&) denote the posterior beliefs about the 190 
corresponding variables in non-bold italics (e.g., ùë†&). b Equivalence between canonical neural 191 
networks and variational Bayesian inference. See the main text and Methods for details. c 192 
Procedure for reverse-engineering the implicit generative model and predicting subsequent data. 193 
(1) The neuronal responses are recorded, and (2) the canonical neural network (rate coding model) 194 
is used to explain the empirical responses. (3) The dynamics of the canonical neural network can 195 
be cast as the gradient descent on a cost function. Thus, the original cost function L can be 196 
reconstructed by taking the integral of the network‚Äôs neural activity equation. Free parameters ùúô 197 
are estimated from the mean response to characterise L. (4) Identification of an implicit generative 198 
model and the ensuing variational free energy F using the equivalence of functional forms in Table 199 
1. (5) The synaptic plasticity rule is derived as a gradient descent on variational free energy. (6) The 200 
obtained plasticity scheme is used to predict self-organisation of neuronal networks. The details 201 
are provided in Methods and have been described previously [16‚Äì18]. 202 
 203 
Consistency between in vitro neural networks and variational Bayes 204 
In this section, we verify some qualitative predictions of the free-energy principle when applied 205 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
12 
 
to our in vitro neural networks in terms of response selectivity (i.e., inference), plasticity (i.e., 206 
learning), and effects of pharmacological manipulations on inference and subsequent learning. 207 
Using our in vitro experimental setup [20,21], cortical cells obtained from rat embryos were 208 
cultured on an MEA dish with 64 microelectrodes on its floor (Fig. 1a, left). Each electrode was 209 
used to deliver electrical stimuli and record the spiking response. After approximately 10 days in 210 
culture, the neurons self-organised into a network and exhibited spontaneous activity, with clear 211 
evoked responses to electrical stimuli. Neurons were stimulated with carefully constructed 212 
patterns of sensory input, comprising 32 binary sensory inputs (o) that were generated from two 213 
sequences of independent binary hidden sources (s) in the manner of the POMDP generative 214 
model above (Fig. 1a, right). When a sensory input took the value of 1, an electrical pulse was 215 
delivered to the cultured neurons. Evoked extracellular activity (i.e., the early neuronal response) 216 
was recorded from 64 MEA electrodes. Each session lasted 256 s, in which a 256-time-step 217 
sequence of random stimulations was delivered every second, followed by a 244-s resting period. 218 
The training comprised 100 sessions. 219 
Upon electrical simulation‚Äîgenerated by the mixture of the two hidden sources‚Äîour previous 220 
work showed the emergence of selective neuronal responses to either of the two sources [22,23]. 221 
Response intensity was defined as the number of spikes 10‚Äì30 ms after a stimulation (Fig. 2a). The 222 
recorded neurons were categorised into source 1- and source 2-preferring and no-preference 223 
groups, depending on the average response intensity, conditioned upon the hidden source (Fig. 224 
2b). Learning was quantified as the emergence of functional specialisation for recognising 225 
particular sources. The response intensity of the source 1-preferring neurons changed during the 226 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
13 
 
training period to exhibit a strong response selectivity to source 1 (Fig. 2c, left). These neurons 227 
self-organised to fire at a high level when source 1 was ON, but had a low response rate when 228 
source 1 was OFF. Similarly, source 2-preferring neurons self-organised to respond selectively to 229 
source 2 during training (Fig. 2c, right). These changes were inhibited by an N-methyl-D-aspartate 230 
(NMDA) receptor antagonist, 2-amino-5-phosphonopentanoic acid (APV) (Fig. 2d), indicating that 231 
the observed self-organisation depends on NMDA-receptor-dependent plasticity. These results 232 
indicate the occurrence of blind source separation at a single-neuron level‚Äîthrough activity-233 
dependent synaptic plasticity‚Äîsupporting the theoretical notion that neural activity encodes the 234 
posterior belief (i.e., expectation) about hidden sources or states [1,2]. 235 
Given the consistency between source-preferring neuronal responses and state posterior, one 236 
can then ask about the neuronal substrates for other quantities in variational Bayesian inference. 237 
In light of the above, we modelled neuronal networks using a canonical neural network, comprising 238 
a single feedforward layer (Fig. 1b, top left). As noted above, this neural network acts as an ideal 239 
Bayesian observer, exhibiting Bayesian belief updating under a POMDP generative model (Fig. 1b, 240 
top right), where the firing threshold encodes a prior over initial states (Table 1) [16,17]. Thus, this 241 
in silico model can learn to detect hidden sources successfully when the implicit state prior 242 
matches that of the true generative process (in this case, ùê∑! = 0.5; Fig. 2e, middle). Conversely, 243 
both upregulation (Fig. 2e, right) and downregulation (Fig. 2e, left) of the state prior significantly 244 
disrupted this sensory learning (Fig. 2f). These simulations used the same empirical stimuli applied 245 
to neuronal networks. Hence, if this canonical neural network is an apt model for neuronal 246 
networks, the firing threshold (i.e., baseline excitability) of the neuronal network should encode 247 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
14 
 
the state prior, and changes in baseline excitability should disrupt the inference and ensuing 248 
sensory learning. 249 
To examine this hypothesis, we asked whether pharmacological modulations of the baseline 250 
excitability of in vitro networks induce the same disruptions of inference as the alterations in the 251 
state prior in the in silico network. Pharmacological upregulation of gamma-aminobutyric acid 252 
(GABA)-ergic inputs (using a GABAA-receptor antagonist, bicuculline) or its downregulation (using a 253 
benzodiazepine receptor agonist, diazepam) altered the baseline excitability of neuronal networks. 254 
Average response levels in bicuculline-treated cultures were larger than those of control cultures, 255 
while diazepam-treated cultures exhibited lower response levels. Crucially, alterations in neuronal 256 
responses‚Äîand subsequent learning‚Äîwere observed when we pharmacologically modulated the 257 
GABAergic input level (Fig. 2g). We observed that both hyper-excitability (Fig. 2g, right) and hypo-258 
excitability (Fig. 2g, left) significantly suppressed the emergence of response specificity at the 259 
single-neuron level (Fig. 2h). This disruption of learning was observed both for source 1- and 2-260 
preferring neurons. 261 
Remarkably, our in silico model‚Äîunder ideal Bayesian assumptions‚Äîcould predict the effects of 262 
this GABAergic modulation on learning using a simple manipulation of the prior belief about 263 
hidden states (please compare Fig. 2e,f with Fig. 2g,h). This involved setting the prior expectations 264 
so that sensory causes were generally present (analogous to the GABAergic antagonist effect) or 265 
generally absent (analogous to the agonist effect). Physiologically, this corresponds to increasing 266 
and reducing the response intensity, respectively, which is consistent with the effects of these 267 
pharmacological manipulations on baseline activity. In terms of inference, this manipulation 268 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
15 
 
essentially prepares the network to expect the presence or absence of an object (i.e., a hidden 269 
source) prior to receiving sensory evidence. The key finding here is that this simple manipulation 270 
was sufficient to account for the failure of inference and subsequent learning, as evidenced by the 271 
absence of functional specialisation. Thus, changes in the prior (neuronal) representations of 272 
states provide a sufficient explanation for aberrant learning. 273 
In summary, the emergence of response specificity observed under normal network excitability 274 
was disrupted by pharmacologically induced hyper- or hypo-excitability of the network. The 275 
canonical neural network (i.e., ideal Bayesian observer) predicted these empirical effects‚Äîof the 276 
agonist and antagonist‚Äîby reproducing the hypo-excitability (diazepam) condition, analogous to 277 
the prior belief that sources are OFF (‚Äònothing there‚Äô), or by the hyper-excitability (bicuculline) 278 
condition, analogous to the prior belief that sources are present (ON). In either case, in vitro and in 279 
silico networks failed to perform causal inference, suggesting that the failure can be attributed to a 280 
biased state prior, under which they operated. These results corroborate the theoretical prediction 281 
that the firing threshold is the neuronal substrate of the state prior [16,17], validating the 282 
proposed equivalence at the cellular level. This further licences an interpretation of neuronal 283 
network dynamics in terms of Bayesian inference and learning. 284 
 285 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
16 
 
 286 
Fig. 2 Neuronal networks perform blind source separation, consistent with Bayesian belief 287 
updating. a Early evoked responses of in vitro neurons recorded at a single electrode, showing a 288 
source 1-preferring neuronal response. Raster plot of spiking responses (left) and peristimulus 289 
time histogram (PSTH, right) before and after training are shown. The two sources provide four 290 
hidden state patterns, ùë†& = (1,1), (1,0), (0,1), (0,0), and responses in these four conditions are 291 
plotted in green, red, blue, and black, respectively. Responses in shaded areas (10‚Äì30 ms after a 292 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
17 
 
stimulus) were used for analyses. b Neurons were categorised into source 1-preferring (red), 293 
source 2-preferring (blue), and no-preference (grey) groups. The Pie-chart indicates numbers 294 
(electrodes) in each group, obtained from 30 cultures. c Changes in evoked responses of source 1- 295 
(left) and source 2- (right) preferring neurons, respectively. Response change from session 1 is 296 
shown. Shaded areas represent standard errors. d Comparison with neuronal responses in APV-297 
treated culture group (n = 296 electrodes from 9 cultures). e Simulations of ideal Bayesian 298 
observers. Bayesian belief updating of the posterior belief about source 1 with varying hidden 299 
state priors is shown. Red and blue lines represent how much the posterior expectation changes 300 
from session 1, when the preferred source takes values of 1 or 0, respectively. Shaded areas 301 
represent standard deviations (n = 100 for each condition). f Difference in posterior expectation 302 
changes with s1 = 1 and s1 = 0, at session 100. g Transitions of selective neuronal responses of 303 
source 1-preferring neurons under control (middle), hypo- (left), and hyper-excitability (right) 304 
conditions. GABAergic inputs were up- or downregulated by adding diazepam (left) or bicuculline 305 
(right), respectively, to the culture medium. Red and blue lines represent the averaged evoked 306 
response of source 1-preferring neurons, when source 1 is ON or OFF, respectively. Here, changes 307 
in response from session 1 were computed and then the averaged response (trend) in each session 308 
was subtracted to focus on response specificity to the preferred source. Shaded areas represent 309 
standard deviations (n = 514, 127, 148 electrodes for control, diazepam, and bicuculline 310 
conditions, respectively). h Same as (f), but for empirical responses. 311 
 312 
The free-energy principle predicts learning in neuronal networks 313 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
18 
 
In this section, we examine the predictive validity of the free-energy principle by asking whether 314 
its application to neuronal networks can predict their self-organisation. We considered that the 315 
neuronal responses of source 1- and source-2 encoding ensembles are represented by their 316 
averaged response intensity and refer to them as x1 and x2, where the offset was subtracted, and 317 
the value was normalised in the range between 0 and 1. Here, we modelled the neuronal 318 
responses of in vitro networks in the form of a canonical neural network and estimated the 319 
requisite synaptic strengths W (i.e., effective synaptic connectivity) by fitting empirical neuronal 320 
responses to the model (Fig. 3a; see the Methods section ‚ÄòReverse engineering of generative 321 
models‚Äô for details). Using these estimates, we predicted the trajectories (i.e., learning curves) 322 
evinced by subsequent neuronal responses. 323 
First, we computed the synaptic strengths W that minimised the neural network cost function L 324 
using neuronal responses x. This corresponds to a conventional (model-based) connection strength 325 
estimation, where the W of the canonical neural network model was optimised to fit the empirical 326 
data (see Methods). We then plotted the trajectory of the estimated synaptic strengths on the 327 
landscape of variational free energy F (Fig. 3b, left). This landscape was characterised by the state 328 
prior (encoded by the firing threshold) estimated using empirical data from only the initial 10 329 
sessions. According to the free-energy principle, synaptic plasticity occurs in a manner that 330 
descends on free energy gradients. As predicted, we found that the trajectory of the empirically 331 
computed synaptic connectivity descended the free energy landscape (Fig. 3b, left; see also 332 
Supplementary Video 1). This observation suggests that variational free energy minimisation is a 333 
plausible description of self-organisation or learning in neuronal networks [1,2]. 334 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
19 
 
Interestingly, the reverse engineering enables us to map empirically estimated synaptic 335 
strengths (W) to the posterior expectation (A) about parameter matrix A, to identify the generative 336 
model that the neuronal network employs (Table 1). The reconstructed posterior A precisely 337 
captured the characteristics of the true A in the external milieu, such that source 1 has a greater 338 
contribution to (ùëú!, ‚Ä¶ , ùëú!$), while source 2 to (ùëú!%, ‚Ä¶ , ùëú#") (Fig. 3c). An error between empirical 339 
and ideal (Bayesian) posteriors significantly decreased with sessions (Fig. 3d, left). The error was 340 
larger in bicuculline- or diazepam-treated condition, owing to biased inference and subsequent 341 
learning in these neuronal networks (Fig. 3d, right). These results support the theoretical notion 342 
that synaptic strengths encode the posterior expectation of the parameter [1,2]. As predicted, 343 
synaptic plasticity following the free energy gradient entailed a recapitulation of the generative 344 
process within the neuronal network architecture. 345 
The observation that the empirical synaptic trajectory pursues a gradient descent on variational 346 
free energy implies that one can predict the subsequent learning in the absence of empirical 347 
constraints. Once the initial values of synaptic strengths are identified, the subsequent learning 348 
process can in principle be predicted using the free-energy principle, under the canonical neural 349 
network (i.e., generative) model. 350 
To test this hypothesis, we predicted the neuronal responses (x) and synaptic plasticity (W) in 351 
sessions 11‚Äì100 using the neural network cost function L reconstructed based exclusively on the 352 
empirical responses in the initial 10 sessions (see the Methods section ‚ÄòData prediction using the 353 
free-energy principle‚Äô for details). As established above, this cost function is formally identical to 354 
variational free energy F under a class of POMDP generative models [16,17]. Thus, evaluating the 355 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
20 
 
responses and plasticity that minimise this cost function ùêø (‚â° ùêπ)‚Äîin the absence of data‚Äî356 
furnishes a prediction of neuronal responses and plasticity under the free-energy principle. 357 
We found that the predicted changes in connectivity matched the changes in empirically 358 
estimated effective synaptic connectivity (Fig. 3b, right). Specifically, we observed a strong 359 
correlation between the synaptic strengths estimated using neuronal data and the strengths 360 
predicted in the absence of data (Fig. 3e). The prediction error was less than 4%, up to the final 361 
session (Fig. 3f; n = 30 independent experiments). These results indicate that, based on initial 362 
conditions, the free-energy principle can predict the self-organisation of neuronal networks. 363 
In addition to the synaptic trajectory, we confirmed that a minimisation of free energy can 364 
predict the underlying changes in neuronal responses (Fig. 3g). The predictions based only on 365 
initial conditions were consistent with observed responses. Specifically, the predicted responses 366 
were consistent with the observed responses at each time step (Fig. 3h,i). Quantitatively, we could 367 
predict more than 80% of the neuronal responses in session 100, based only on data from sessions 368 
1‚Äì10 (Fig. 3j). These results suggest that the free-energy principle can predict both changes in 369 
synaptic efficacy and the time evolution of neuronal responses based only on initial data. Note 370 
that this is a highly nontrivial prediction, because synaptic efficacy shows activity-dependent 371 
changes and neuronal responses depend upon synaptic efficacy. 372 
Another interesting observation was that when we varied the free energy landscape by 373 
manipulating the mixing matrix A in the stimulus generating process, empirical synaptic plasticity 374 
kept pursuing a gradient descent on the new variational free energy (Fig. 3k). This speaks to a 375 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
21 
 
generality of this physiological property. Here, we experimentally varied the mixing balance (A) of 376 
two sources between 0 and 50%, to train neuronal networks with the generated sensory stimuli 377 
(o), where 0% indicates an unmixed (i.e., easily separable) condition, while 50% indicates a 378 
uniformly mixed (i.e., inseparable) condition. Irrespective of various conditions (i.e., forms of 379 
generative process and prior beliefs), the reverse engineering could reconstruct generative models 380 
and predict subsequent self-organisations of neuronal networks (Extended Fig. 1). 381 
Finally, we observed that during the process of assimilating sensory information, neuronal 382 
networks significantly reduced their variational free energy (Fig. 3l). Here, variational free energy F 383 
for each session was calculated empirically by substituting the observed neuronal responses into 384 
the cost function L. As expected, an easier task (i.e., 0% mix condition) entailed a faster (i.e., 385 
greater) reduction of variational free energy. These results provide explicit empirical evidence that 386 
neuronal networks self-organise to minimise variational free energy. 387 
In summary, we found that the trajectory of the empirically estimated effective synaptic 388 
connectivity is consistent with a slow gradient descent on variational free energy. Furthermore, we 389 
demonstrated that the free-energy principle can quantitatively predict sensory learning in 390 
neuronal networks in terms of both neuronal responses and plasticity. These results suggest that 391 
the self-organisation of the neuronal networks‚Äîin response to structured sensory input‚Äîis 392 
consistent with Bayesian belief updating and the minimisation of variational free energy. This 393 
endorses the plausibility of variational free energy minimisation as a rule underlying the dynamics 394 
and self-organisation of neuronal networks. 395 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
22 
 
 396 
 397 
Fig. 3. Predictive validity of the free-energy principle. a Schematic of the system architecture 398 
comprising the generative process and the in vitro neuronal network modelled as a canonical in 399 
silico neural network. Two neural ensembles receive 32 inputs generated from two sources. b Left: 400 
Trajectory of empirically estimated synaptic connectivity (W) depicted on the landscape of 401 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
23 
 
variational free energy (F). The slope indicates a theoretically predicted free energy landscape. 402 
Darker green represents lower free energy. Whereas, synaptic strengths (i.e., effective synaptic 403 
connectivity or efficacy) are calculated using empirical data in sessions 1‚Äì100. Right: Predictions of 404 
synaptic plasticity during training. The initial conditions (i.e., parameters of a generative model) 405 
were identified using neuronal responses from the first 10 sessions. A brighter colour indicates the 406 
predicted synaptic trajectory in the absence of experimental data. c Empirically estimated 407 
posterior belief (A) about parameter A. d Error in neuronal networks estimating each column of A 408 
matrix, defined as the squared error between empirical and ideal A matrices, divided by the 409 
squared amplitude of A (n = 28, 120, 24 columns for diazepam, control, and bicuculline conditions, 410 
respectively). e Correlation between theoretically predicted strengths and strengths estimated 411 
from data, at session 100. f Error in predicting synaptic strengths, defined as the squared error 412 
between estimated and predicted 6ùëä7!, ùëä7'8, divided by the squared Frobenius norm of 6ùëä7!, ùëä7'8 413 
(see Methods for the definition). g Trajectory of observed (left) and predicted (right) neuronal 414 
responses of source 1-coding ensembles, during training. Red and blue lines indicate the responses 415 
when source 1 is ON and OFF, respectively. h Comparison of observed (black) and predicted (red) 416 
responses in session 100. i Correlation between observed and predicted responses during session 417 
91‚Äì100. j Error in predicting neuronal responses, defined as the mean squared error: ùëíùëüùëü =418 
E[|ùë•& ‚àí ùë•&
(|"]/2. k Synaptic trajectories on free energy landscape under 0, 25, and 50% mix 419 
conditions. l Trajectory of variational free energy. Changes from session 1 are plotted (n = 4, 30, 4 420 
for 0, 25, and 50% mix conditions, respectively). Data from n = 30 independent experiments were 421 
used in (d,left)(f)(i)(j)(l,left). Shaded areas and bars in (d)(f)(g)(i)(j)(l) represent standard deviations. 422 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
24 
 
Grey areas in (f)(g)(j) indicate the first 10 sessions, from which data were used. See Methods for 423 
further details. 424 
 425 
DISCUSSION 426 
The present work examined the predictive validity of the free-energy principle at the cellular 427 
and synaptic levels, by delineating the functional specialisation and segregation in neuronal 428 
networks via free energy minimisation. Previous work has established that ensembles of neurons 429 
encode posterior expectations [39] and prediction errors [40]; however, other quantities in 430 
Bayesian inference‚Äîsuch as the state prior and parameter posterior‚Äîhave yet to be fully 431 
investigated, owing to a lack of analytical techniques. The reverse engineering approach enables us 432 
to identify the structures, variables, and parameters of generative models from experimental data, 433 
which is essential for empirical applications of the free-energy principle. This is a notion referred to 434 
as computational phenotyping [41]; namely inferring the generative model‚Äîand in particular, the 435 
priors‚Äîthat best explain empirical responses under ideal Bayesian assumptions. The reverse 436 
engineering naturally maps empirical (biological) quantities to quantities in variational Bayesian 437 
inference. Our empirical results suggest that neuronal responses encode the hidden state posterior 438 
(Fig. 2c), baseline excitability encodes the state prior (Fig. 2g), and synaptic efficacies encode the 439 
parameter posterior (Fig. 3c), as predicted theoretically (Table 1). 440 
Having said this, because the free-energy principle can arguably describe any observed 441 
biological data by its construction [19], showing the existence of such a mapping alone is 442 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
25 
 
insufficient as an empirical validation. Conversely, one can examine the predictive validity, which is 443 
a more delicate problem, by asking whether the free-energy principle can predict subsequent self-444 
organisation without reference to empirical data. Such a generalisability on previously unseen 445 
(test) data comprises an essential aspect for empirical applications of the free-energy principle. 446 
We demonstrated that, equipped with the initial conditions (i.e., generative model and implicit 447 
prior beliefs of the network) characterised by the experimental data, variational free energy 448 
minimisation can predict the subsequent self-organisation of in vitro neural networks, in terms of 449 
quantitative neuronal responses and plasticity. It further predicted their performance when 450 
spontaneously solving source separation problems, including their speed and accuracy. These 451 
results not only validate this application of the free-energy principle; they also speak to the 452 
neurophysiological plausibility of related theories of the brain [42,43] and spiking neural network 453 
models that perform Bayesian inference [44‚Äì46]. 454 
In essence, the free-energy principle constrains the relationship between neural activity and 455 
plasticity because both activity and plasticity follow a gradient descent on a common variational 456 
free energy, under ideal Bayesian assumptions. This property in turn enables precise 457 
characterisation of plausible self-organisation rules and quantitative prediction of subsequent 458 
neuronal activity and plasticity, under a canonical neural network (generative) model. 459 
Our combined in vitro‚Äìin silico system showed that variation of the state prior (in in silico 460 
model) is sufficient to reproduce the changes in neural excitability and inhibition of sensory 461 
learning and inference observed in vitro. These results suggest that a neuronal networks‚Äô 462 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
26 
 
excitability is normally tuned so that the ensemble behaviour is close to that of a Bayesian optimal 463 
encoder under biological constraints. This is reminiscent of previous experimental observation that 464 
suggests that the activity of sensory areas encodes prior beliefs [47]. 465 
These empirical data and complementary modelling results also explain the strong influence of 466 
prior belief on perception and causal inference‚Äîand the disruptive effects of drugs on perception 467 
in neuronal networks. Both synaptic plasticity and inference depend on convergent neuronal 468 
activity; therefore, disruption of inference will disrupt learning. Conversely, inference is not 469 
possible without the knowledge accumulated through experience (i.e., learning). Thus, inference is 470 
strongly linked to learning about contingencies that generate false inferences. Our findings 471 
demonstrate this association both mechanistically and mathematically, in terms of one simple rule 472 
that allows prior beliefs to underwrite inferences about hidden states. 473 
The notion that manipulating the state prior (encoded by neuronal excitability) disrupts 474 
inference and learning may explain the perceptual deficits produced by drugs that alter neuronal 475 
excitability, such as anxiolytics and psychedelics [48]. This may have profound implications for our 476 
understanding of how anxiolytics and psychedelics mediate their effects; namely, a direct effect on 477 
baseline activity can alter subsequent perceptual learning. Reproduction of this phenomenon in in 478 
vitro (and in vivo) networks provides the opportunity to elucidate the precise pharmacological, 479 
electrophysiological, and statistical mechanisms underlying Bayesian inference. 480 
Importantly, although this paper focused on a comparison of in vitro data and theoretical 481 
prediction, the reverse engineering approach is applicable to characterising in vivo neuronal 482 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
27 
 
networks, in terms of their implicit generative model with prior beliefs. It can in principle be 483 
combined with electrophysiological, functional imaging, and behavioural data‚Äîand give 484 
predictions, if the learning process is continuously measured. 485 
The generic mechanism of acquiring the generative model can be used to construct a 486 
neuromorphic hardware for universal applications [49,50]. Such a biomimetic artificial intelligence 487 
that implements the self-organising mechanisms of neuronal networks is expected to be an 488 
alternative to conventional learning algorithms relying on back-propagation [51,52], and to have 489 
the high data, computational, and energy efficiency of biological organisms [53,54]. This makes it 490 
promising as a next-generation artificial intelligence. In addition, the creation of biomimetic 491 
artificial intelligence will further deepen our understanding of the brain. 492 
In summary, complementary in vitro neural network recordings and in silico modelling suggest 493 
that variational free energy minimisation is an apt explanation for dynamics and self-organisation 494 
of neuronal networks that assimilate sensory data. The reverse engineering approach provides a 495 
powerful tool for mechanistic investigation of inference and learning, enabling identification of 496 
generative models and application of the free-energy principle. The observed sensory learning was 497 
consistent with Bayesian belief updating and the minimisation of variational free energy. Thus, 498 
variational free energy minimisation could qualitatively predict neuronal responses and plasticity 499 
of in vitro neural networks. These results highlight the validity of the free-energy principle as a rule 500 
underlying the self-organisation and learning of neuronal networks. 501 
 502 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
28 
 
Acknowledgements 503 
T.I. is funded by RIKEN Center for Brain Science. KJF is supported by funding for the Wellcome 504 
Centre for Human Neuroimaging (Ref: 205103/Z/16/Z), a Canada-UK Artificial Intelligence Initiative 505 
(Ref: ES/T01279X/1) and the European Union‚Äôs Horizon 2020 Framework Programme for Research 506 
and Innovation under the Specific Grant Agreement No. 945539 (Human Brain Project SGA3). The 507 
funders had no role in study design, data collection and analysis, decision to publish, or 508 
preparation of the manuscript. 509 
 510 
Competing Interests 511 
The authors have no competing interests to declare. 512 
 513 
METHODS 514 
Generative process 515 
The experimental paradigm established in previous work [22] was employed. Two sequences of 516 
mutually independent hidden sources or states ùë†& = Bùë†&
(!), ùë†&
(")C
+
 generated 32 sensory stimuli 517 
ùëú& = Bùëú&
(!), ‚Ä¶ , ùëú&
(#")C
+
 through a stochastic mixture characterised by matrix A. Each source and 518 
observation took values of 1 (ON) or 0 (OFF) for each trial (or time) t. These stimuli were applied to 519 
in vitro neural networks as electrical pulses from 32 electrodes (Fig 1a, left). In terms of the 520 
POMDP scheme [37,38], this corresponds to the likelihood mapping A from two sources ùë†& to 32 521 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
29 
 
observations ùëú& (Fig. 1a, right). The hidden sources ùë†& were sampled from a categorical 522 
distribution ùëÉ Bùë†&
(,)C = Cat6ùê∑(,)8. The state priors varied between 0 and 1, in keeping with 523 
ùê∑!
(,) + ùê∑'
(,) = 1. The likelihood of ùëú&
(-) is given in the form of a categorical distribution, 524 
ùëÉBùëú&
(-)|ùë†&, ùê¥C = Cat6ùê¥(-)8, each element of which represents ùëÉBùëú&
(-) = ùëó|ùë†&
(!) = ùëò, ùë†&
(") = ùëô, ùê¥C =525 
ùê¥,./
(-) . Half of the electrodes (1 ‚â§ ùëñ ‚â§ 16) conveyed the source 1 signal with a 75% probability or 526 
the source 2 signal with a 25% probability, which is expressed as ùê¥!‚àô‚àô
(-) = (1,0.75,0.25,0). The 527 
remaining electrodes (17 ‚â§ ùëñ ‚â§ 32) conveyed the source 1 or 2 signal with a 25% or 75% 528 
probability, respectively, ùê¥!‚àô‚àô
(-) = (1,0.25,0.75,0). The remaining elements of A were given by 529 
ùê¥'‚àô‚àô
(-) = 1 ‚àí ùê¥!‚àô‚àô
(-). The prior distribution of A was given by the Dirichlet distribution ùëÉ6ùê¥(-)8 =530 
Dir6ùëé(-)8 with sufficient statistics a. Hence, the generative model was given as follows: 531 
ùëÉ(ùëú!:&, ùë†!:&, ùê¥) = ùëÉ(ùê¥) U ùëÉ(ùë†2)ùëÉ(ùëú2|ùë†2, ùê¥)
&
23!
(1) 532 
Here, ùëú!:& ‚âî {ùëú!, ‚Ä¶ , ùëú&} represents a sequence of observations, ùëÉ(ùê¥) = ‚àè ùëÉ6ùê¥(-)8#"
-3! , ùëÉ(ùë†2) =533 
ùëÉBùë†2
(!)CùëÉBùë†2
(")C, and ùëÉ(ùëú2|ùë†2, ùê¥) = ‚àè ùëÉBùëú2
(-)|ùë†2, ùê¥C#"
-3!  are prior distributions and likelihood that 534 
factorise [16]. 535 
 536 
Variational Bayesian inference 537 
We considered a Bayesian observer under the generative model in the form of the above 538 
POMPD and implemented variational message passing to derive the Bayes optimal encoding of 539 
hidden sources or states [37,38]. Under the mean-field approximation, the posterior beliefs about 540 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
30 
 
states and parameters were provided as follows: 541 
ùëÑ(ùë†!:&, ùê¥) = ùëÑ(ùê¥) U ùëÑ(ùë†2)
&
23!
(2) 542 
Here, the posterior distributions of ùë†2 and ùê¥ are given by categorical ùëÑ(ùë†2) = Cat(ùê¨2) and 543 
Dirichlet ùëÑ(ùê¥) = Dir(ùêö) distributions, respectively. The bold case variables (e.g., ùê¨&) denote the 544 
posterior beliefs about the corresponding italic case variables (e.g., ùë†&), and ùêö indicates the 545 
Dirichlet concentration parameter. 546 
Variational free energy‚Äîor equivalently, the negative of evidence lower bound (ELBO) [3]‚Äîis 547 
defined as an upper bound of sensory surprise ùêπ6ùëú!:&, ùëÑ(ùë†!:&, ùê¥)8 ‚âî548 
E4(5!:#)4(6)[‚àí ln ùëÉ(ùëú!:&, ùë†!:&, ùê¥) + ln ùëÑ(ùë†!:&, ùê¥)]. Given the above-defined generative model and 549 
posterior beliefs, ensuing variational free energy of this system is given by: 550 
ùêπ = ^ ùê¨2 ‚àô (ln ùê¨2 ‚àí ln ùêÄ ‚àô ùëú2 ‚àí ln ùê∑)
&
23!
+ ùí™(ln ùë°) (3) 551 
up to a negligible ùí™(ln ùë°) term. This ùí™(ln ùë°) corresponds to the parameter complexity expressed 552 
using the Kullback‚ÄìLeibler divergence ùíü78[ùëÑ(ùê¥)||ùëÉ(ùê¥)] = ‚àë d6ùêö(-) ‚àí ùëé(-)8 ‚àô ln ùêÄ(-) ‚àí#"
-3!553 
ln ‚Ñ¨6ùêö(-)8f and is negligible when t is sufficiently large. Note that ‚àô expresses the inner product 554 
operator, ln ùêÄ(-) indicates the posterior expectation of ln ùê¥(-), and ‚Ñ¨(‚àô) is the beta function. 555 
Inference and learning entail updating posterior expectations about hidden states and parameters, 556 
respectively, to minimise variational free energy. Solving the fixed point ùúïùêπ/ùúïùê¨& = 0 and 557 
ùúïùêπ/ùúïùêö = ùëÇ yields the following analytic expression: 558 
ùê¨& = ùúé(ln ùêÄ ‚àô ùëú&) (4) 559 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
31 
 
ùêö = ùëé + ^ ùëú2‚®Çùê¨2
&
23!
(5) 560 
where ùúé(‚àô) is a softmax function, which corresponds to the sigmoid activation function, and ‚®Ç 561 
expresses the outer product operator. From equation (5), the parameter posterior is given as 562 
ln ùêÄ = ùúì(ùêö) ‚àí ùúì(ùêö!‚àô + ùêö'‚àô) using the digamma function ùúì(‚àô) = ùõ§9(‚àô)/ùõ§(‚àô). 563 
 564 
Canonical neural networks 565 
The complete class theorem [55‚Äì57] suggests that any neural network whose internal states 566 
minimise a common cost function can be read as performing Bayesian inference. However, the 567 
implicit Bayesian model that corresponds to any given cost function is a more complicated issue. 568 
Thus, we reverse-engineered cost functions for canonical neural networks to identify the 569 
corresponding generative model [16,17]. 570 
The neural response ùë•& = (ùë•&!, ùë•&")+ at time t upon receiving sensory inputs ùëú& is modelled 571 
as the canonical neural network, which is expressed as the following ordinary differential equation: 572 
ùë•Ãá& ‚àù ‚àísig:!(ùë•&) + ùëäùëú& + ‚Ñé (6) 573 
where sig:!(ùë•&) indicates the leak current characterised by the inverse of sigmoid function (or 574 
equivalently, logit function), ùëä denotes a 2 √ó 32 matrix of synaptic strengths, ùëäùëú& is the 575 
synaptic input, and ‚Ñé is a vector of the adaptive firing thresholds. We considered that ùëä ‚âî576 
ùëä! ‚àí ùëä' is the sum of excitatory (ùëä!) and inhibitory (ùëä') synaptic strengths. The firing threshold 577 
is expressed as ‚Ñé ‚âî ‚Ñé! ‚àí ‚Ñé' using ‚Ñé! and ‚Ñé' that are functions of ùëä! and ùëä', respectively. 578 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
32 
 
This model derives from realistic neuron models [4‚Äì6] through approximations [17]. 579 
Without loss of generality, equation (6) can be derived as the gradient descent on a cost 580 
function L. Following previous work [16,17], this cost function can be identified by taking the 581 
integral of the right-hand side of equation (6) with respect to ùë•& (referred to as reverse 582 
engineering): 583 
ùêø = ^ Bùë•2
ùë•2
C
+
sln Bùë•2
ùë•2
C ‚àí tùëä!
ùëä'
u ùëú2 ‚àí t‚Ñé!
‚Ñé'
uv
&
23!
+ ùí™(1) (7) 584 
up to a negligible ùí™(1) term. The overline variable indicates one minus the variable, ùë•& ‚âî 1w‚Éó ‚àí585 
ùë•&, where 1w‚Éó ‚âî (1, ‚Ä¶ ,1)+ is a vector of ones. Equation (7) ensures that the gradient descent on L 586 
with respect to ùë•&, ùë•Ãá& ‚àù ‚àíùúïùêø/ùúïùë•&, provides equation (6). The firing thresholds can be 587 
decomposed as ‚Ñé! = ln ùëä7! 1w‚Éó + ùúô! and ‚Ñé' = ln ùëä7' 1w‚Éó + ùúô', respectively, where ùúô! and ùúô' are 588 
constants representing the threshold factors, ùëä7! ‚âî sig(ùëä!) is the sigmoid function of ùëä! in the 589 
elementwise sense, and ùëä7! ‚âî 1w‚Éó1w‚Éó+ ‚àí ùëä7! indicates one minus ùëä7! in the elementwise sense. 590 
Subsequently, equation (7) can be transformed as follows: 591 
ùêø = ^ Bùë•2
ùë•2
C
+
yln Bùë•2
ùë•2
C ‚àí ln zùëä7! ùëä7!
ùëä7' ùëä7'
{ Bùëú2
ùëú2
C ‚àí tùúô!
ùúô'
u|
&
23!
+ ùí™(1) (8) 592 
We showed that this cost function L can be cast as variational free energy F under a class of 593 
POMPD generative models [16,17]. Equation (8) is equivalent to variational free energy (equation 594 
(3)) under the generative model defined in equation (1), up to a negligible ùí™(ln ùë°) term. One-to-595 
one correspondences between components of L and F can be observed. Specifically, the neural 596 
response ùë•& encodes the state posterior ùê¨&, B
ùë•2
ùë•2
C = ùê¨&; synaptic strengths W encodes the 597 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
33 
 
parameter posterior ùêÄ, ln zùëä7! ùëä7!
ùëä7' ùëä7'
{ = ln ùêÄ; and the threshold factor ùúô encodes the state 598 
prior ùê∑, ùúô = tùúô!
ùúô'
u = ln ùê∑, as summarised in Table 1. Hence, the neural network cost function is 599 
asymptotically equivalent to variational free energy for sufficiently large t. 600 
The virtue of this equivalence is that it links quantities in the neural network with those in the 601 
variational Bayes formation. Moreover, this suggests that a physiologically plausible synaptic 602 
plasticity (derived from L) enables the network to learn the parameter posterior in a self-organising 603 
or unsupervised manner. Further details are provided in previous work [16,17]. 604 
 605 
Simulations 606 
In Fig. 2, simulations continued over ùëá = 25600 time steps and used the empirical stimuli 607 
applied to in vitro neural networks. Synaptic strengths were initialised as values close to 0. Here, 608 
ùê∑! = 0.5 (Fig. 2e, centre) matched the true process that generates sensory stimuli. Either the 609 
upregulation (right, ùê∑! = 0.8) or downregulation (left, ùê∑! = 0.2) of the state prior from the 610 
optimal value disrupted inference and ensuing learning. 611 
 612 
Cell culture 613 
The dataset used for this work comprised data obtained by newly conducted experiments, and 614 
those originally used in the previous work [22]. All animal experiments were performed with the 615 
approval of the animal experiment ethics committee at the University of Tokyo (approval number 616 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
34 
 
C-12-02, KA-14-2) and according to the University of Tokyo guidelines for the care and use of 617 
laboratory animals. The procedure for preparing dissociated cultures of cortical neurons followed 618 
the procedures described in previous work [22]. Pregnant Wistar rats (Charles River Laboratories, 619 
Yokohama, Japan) were anaesthetised with isoflurane and immediately sacrificed. The cerebral 620 
cortex was removed from 19-day-old embryos (E19) and dissociated into single cells by treatment 621 
with 2.5% trypsin (Life Technologies, Carlsbad, CA, USA) at 37¬∞C for 20 min, followed by mechanical 622 
pipetting. Half million dissociated cortical cells (a mixture of neurons and glial cells) were seeded 623 
on the centre of MEA dishes. These cells were cultured in the CO2 incubator. Culture medium 624 
comprised Neurobasal Medium (Life Technologies) containing 2% B27 Supplement (Life 625 
Technologies), 2 mM GlutaMAX (Life Technologies), and 5‚Äì40 U/mL penicillin/streptomycin (Life 626 
Technologies). Half of the culture medium was changed once every second or third day. These 627 
cultures were recorded during the age of 18‚Äì83 days in vitro. During this stage, the spontaneous 628 
firing patterns of the neurons had reached a developmentally stable period [58,59]. 629 
We used 30 cultures for the control condition, 6 were treated with bicuculline, 7 with diazepam, 630 
9 with APV, 4 were trained under the 0% mix condition, and 4 under the 50% mix condition. 631 
Among them, 7 cultures used as the control, 6 treated with bicuculline, and 7 with diazepam were 632 
obtained by newly conducted experiments, where their response intensities were 3.0 ¬± 1.1, 3.7 ¬± 633 
1.9, and 2.3 ¬± 0.86 spike/trial, respectively (mean ¬± standard deviation). Other cultures were 634 
originally recorded for previous work [22]. The cell-culturing and experimental conditions in the 635 
previous work were essentially the same as ones recorded for the present work. Note that the 636 
same cultures were used more than once for experiments with other stimulation pattern 637 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
35 
 
conditions. This is justified because the different stimulation patterns were independent of each 638 
other, and thus, learning history with other stimulation patterns did not affect the subsequent 639 
experiments [22]. 640 
 641 
Pharmacological treatment 642 
The excitability of cultured neurons was pharmacologically controlled. To block GABAA-receptor 643 
activity, bicuculline, a GABAA-receptor antagonist (Sigma‚ÄíAldrich, St. Louis, MO, USA) was used. 644 
Bicuculline was adjusted to 10 mM using phosphate-buffered saline (PBS), and 10 ¬µL was added to 645 
the culture medium to a final concentration of 50 ¬µM. To upregulate GABAA-receptor activity, 646 
diazepam, a benzodiazepine receptor agonist (Sigma‚ÄíAldrich) was used. Diazepam was adjusted to 647 
100 ¬µM using N,N-dimethylformamide (DMF), and 20 ¬µL was added to the culture medium to a 648 
final concentration of 1 ¬µM. After adding the solution to the medium, cultured neurons were 649 
placed in a CO2 incubator for 30 min, and stable activity of the neurons was confirmed before 650 
recording. 651 
 652 
Electrophysiological experiments 653 
Electrophysiological experiments were conducted using an MEA system (NF Corporation, 654 
Yokohama, Japan). This enabled extracellular recordings of evoked spikes from multiple sites 655 
immediately after electrical stimulation [20,21]. An MEA dish comprises 8√ó8 microelectrodes (50-656 
¬µm square each) embedded on its centre, deployed on a grid with 250-¬µm microelectrodes 657 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
36 
 
separation. Recordings were conducted with a 25-kHz sampling frequency and band-pass filter of 658 
100‚Äì2000 Hz. The spike sorting analysis suggested that an electrode was expected to record the 659 
activities from up to four neurons. Three-phasic extracellular potentials, as described in previous 660 
work [20,21], were recorded from a large part of electrodes. 661 
The 32 stimulation electrodes were randomly distributed over 8√ó8 MEAs in advance and fixed 662 
over training. A biphasic pulse with a 1-V amplitude and 0.2-ms duration, which efficiently induces 663 
activity-dependent synaptic plasticity [22], was used as a sensory stimulus. A session of training 664 
comprised a 256-time-step sequence of stimuli with 1-s intervals, followed by a 244-s resting 665 
period. We repeated this training for 100 sessions (approximately 14 h in total). All recordings and 666 
stimulation were conducted in a CO2 incubator. 667 
 668 
Data preprocessing 669 
For spike detection, the acquired signals were passed through a digital band-pass filter of 500‚Äí670 
2000 Hz after removal of the saturated ranges and noises that were caused by electric potential 671 
variations associated with the switch from the stimulation circuit to the recording circuit. 672 
Subsequently, waveform valleys that fell below 4 times the standard deviation of the signal 673 
sequence of each electrode were detected as spikes. Note that for data obtained in the previous 674 
work [22], waveform valleys that fell below 5 times the standard deviation were detected as spikes 675 
because of the difference in the noise level. 676 
Irrespective of the presence or absence of bicuculline or diazepam, the peak of evoked response 677 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
37 
 
fell at 10‚Äì20 ms after each stimulus on average. Accordingly, we defined the intensity of the 678 
evoked response to the stimulus by the number of spikes generated until 10‚Äì30 ms after each 679 
stimulus. We refer to the evoked response at electrode i as ùëü&- (spike/trial), using discrete time 680 
step (or trial) t. Only electrodes at which the all-session average of ùëü&- was larger than 1 spike/trial 681 
were used for subsequent analyses. 682 
The conditional expectation of evoked response ùëü&-‚Äîwhen a certain source state (ùë†!, ùë†") =683 
(1,1), (1,0), (0,1), (0,0) is provided‚Äîis given as E[ùëü-&|ùë†!, ùë†"] ‚âî E[ùëü&-|ùë†& = (ùë†!, ùë†"), 1 ‚â§ ùë° ‚â§ 256] 684 
(spike/trial). This E[ùëü-&|ùë†!, ùë†"] was computed for each session. Recorded neurons were 685 
categorised into three groups based on their preference to sources. We referred to a neuron as s1-686 
preferring when the all-session average of E[ùëü-&|1,0] ‚àí E[ùëü-&|0,1] was larger than 0.5 spike/trial, 687 
as s2-preferring when the all-session average of E[ùëü-&|1,0] ‚àí E[ùëü-&|0,1] was smaller than ‚Äì0.5 688 
spike/trial, or no preference when otherwise. 689 
Our hypothesis [23] was that the stimulus (o) obligatorily excites a subset of neurons in the 690 
network, while repeated exposure makes other neurons with appropriate connectivity learn that 691 
the patterns of responses are caused by ON or OFF of hidden sources (s). Thus, the recorded 692 
neuronal responses comprise the activity of neurons directly receiving the input and that of 693 
neurons encoding the sources. To identify functionally specialised neurons, we modelled recorded 694 
activity as a mixture of the response directly triggered by the stimulus and functionally specialised 695 
response to the sources. Because the former remains invariant over time, while the latter emerges 696 
during training, this distinction enables the decomposition of the responses into stimulus- and 697 
source-specific components. 698 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
38 
 
The empirical responses were summarised in terms of the averaged responses in each group. 699 
For subsequent analyses, we defined ùë•&! as the ensemble average over source 1-preferring 700 
neurons, and ùë•&" as that over source 2-preferring neurons. For analytical tractability, we 701 
normalised the recorded neural response to ensure that it was within the range of 0 ‚â§ ùë•&!, ùë•&" ‚â§702 
1. 703 
 704 
Statistical tests 705 
The Wilcoxon signed-rank test was used for paired comparisons. The Mann‚ÄíWhitney U test was 706 
used for unpaired comparisons. 707 
 708 
Reverse engineering of generative models 709 
In this section, we elaborate the procedure for estimating the threshold factor (ùúô) and effective 710 
synaptic connectivity (ùëä) from empirical data, to characterise the landscape of the neural network 711 
cost function ùêø (‚â° ùêπ). 712 
Assuming that the change in threshold factors was sufficiently slow relative to a short 713 
experimental period, the threshold factor ùúô was estimated based on the mean response intensity 714 
of empirical data. Following the treatment established in previous work [16,17], the constants are 715 
estimated for each culture using the empirical data as follows: 716 
ùúô = tùúô!
ùúô'
u = t‚ü®ùë•&‚ü©
‚ü®ùë•&‚ü©u (9) 717 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
39 
 
where ‚ü®‚àô‚ü© ‚âî
!
; ‚àë ‚àô;
&3!  indicates the average over time. Subsequently, the state prior ùê∑ was 718 
reconstructed from the relationship ln ùê∑ = ùúô (Table 1). This ùê∑ expresses the implicit perceptual 719 
bias of an in vitro network about hidden sources. 720 
Synaptic plasticity rules conjugate to equation (6) are derived as the gradient descent on L 721 
[16,17], given as ùëäÃá! ‚àù ‚àíùúïùêø/ùúïùëä! = ¬Çùë•&ùëú&
+¬É ‚àí ¬Çùë•&1w‚Éó+¬É ‚äô ùëä7! and ùëäÃá ' ‚àù ‚àíùúïùêø/ùúïùëä' = ¬Çùë•&ùëú&
+¬É ‚àí722 
¬Çùë•&1w‚Éó+¬É ‚äô ùëä7', where ‚äô denotes the Hadamard product. These rules comprise Hebbian plasticity 723 
accompanied with an activity-dependent homeostatic term, endorsing the biological plausibility of 724 
this class of cost functions. Solving the fixed point of these equations provides the following 725 
synaptic strengths: 726 
¬Öùëä! = sig:!6¬Çùë•&ùëú&
+¬É ‚äò ¬Çùë•&1w‚Éó+¬É8
ùëä' = sig:!6¬Çùë•&ùëú&
+¬É ‚äò ¬Çùë•&1w‚Éó+¬É8
(10) 727 
where ‚äò denotes the elementwise division operator. In this work, we refer to equation (10) as 728 
the empirically estimated effective synaptic connectivity. This was estimated for each session, 729 
using empirical neuronal response data ùë•&. These synaptic strengths encode the posterior belief 730 
about the mixing matrix A (Table 1). Further details are provided in previous work [16,17]. 731 
These empirically estimated parameters are sufficient to characterise the generative model that 732 
an in vitro neural network employs. Owing to the equivalence, the empirical variational free 733 
energy F for the in vitro network was computed by substituting empirical neuronal responses x and 734 
empirically estimated parameters ùëä (equation (10)) and ùúô (equation (9)) into the neural 735 
network cost function L (equation (8)): see Fig. 3l for its trajectory. 736 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
40 
 
 737 
Data prediction using the free-energy principle 738 
The virtues of the free-energy principle are that it offers the quantitative prediction of 739 
transitions (i.e., plasticity) of the neural response and synaptic strengths in future, in the absence 740 
of empirical response data. We denote the predicted neural responses and synaptic strengths as 741 
ùë•&
( and ùëä(, respectively, to distinguish them from the observed neural responses ùë•& and 742 
empirically estimated synaptic strengths ùëä defined above. 743 
The predicted neural response is given as the fixed-point solution of equation (6): 744 
ùë•&
( = sig(ùëä(ùëú& + ‚Ñé() (11) 745 
where ‚Ñé( = ln ùëä7!
( 1w‚Éó ‚àí ln ùëä7'
( 1w‚Éó + ùúô! ‚àí ùúô' denotes the adaptive firing threshold. Empirical ùúô 746 
(equation (9)) estimated from data in the initial 10 sessions was used to characterise ‚Ñé(. Here, 747 
predicted synaptic strength matrix ùëä( was used instead of the empirically estimated ùëä. The 748 
predicted synaptic strengths are given as follows: 749 
¬á
ùëä!
( = sig:!6¬Çùë•&
(ùëú&
+¬É ‚äò ¬Çùë•&
(1w‚Éó+¬É8
ùëä'
( = sig:! B¬àùë•&
(ùëú&
+¬â ‚äò ¬àùë•&
(1w‚Éó+¬âC
(12) 750 
where ùëä( ‚âî ùëä!
( ‚àí ùëä'
(. Here, the predicted neural responses ùë•&
( were employed to compute 751 
the outer products. The initial value of ùëä( was computed using empirical response data in the 752 
first 10 sessions. By computing equations (11) and (12), one can predict the subsequent self-753 
organisation of neuronal networks in sessions 11‚Äì100, without reference to the observed neuronal 754 
responses. 755 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
41 
 
 756 
Data Availability 757 
The neuronal response data are available at 758 
https://github.com/takuyaisomura/reverse_engineering after publication. 759 
 760 
Code Availability 761 
The simulations and analyses were conducted using MATLAB. The scripts are available at 762 
https://github.com/takuyaisomura/reverse_engineering after publication. 763 
 764 
References 765 
1. Friston, K. J., Kilner, J. & Harrison, L. A free energy principle for the brain. J. Physiol. Paris 100, 766 
70‚Äì87 (2006). 10.1016/j.jphysparis.2006.10.001, Pubmed:17097864. 767 
2. Friston, K. J. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127‚Äì138 768 
(2010). 10.1038/nrn2787, Pubmed:20068583. 769 
3. Blei, D. M., Kucukelbir, A. & McAuliffe, J. D. Variational inference: A review for statisticians. J. 770 
Am. Stat. Assoc. 112, 859‚Äì877 (2017). 10.1080/01621459.2017.1285773. 771 
4. Hodgkin, A. L. & Huxley, A. F. A quantitative description of membrane current and its 772 
application to conduction and excitation in nerve. J. Physiol. 117, 500‚Äì544 (1952). 773 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
42 
 
10.1113/jphysiol.1952.sp004764, Pubmed:12991237. 774 
5. FitzHugh, R. Impulses and physiological states in theoretical models of nerve membrane. 775 
Biophys. J. 1, 445‚Äì466 (1961). 10.1016/s0006-3495(61)86902-6, Pubmed:19431309. 776 
6. Nagumo, J., Arimoto, S. & Yoshizawa, S. An active pulse transmission line simulating nerve 777 
axon. Proc. IRE 50, 2061‚Äì2070 (1962). 10.1109/JRPROC.1962.288235. 778 
7. Rosenblatt, F. The perceptron: a probabilistic model for information storage and organization 779 
in the brain. Psychol. Rev. 65, 386‚Äì408 (1958). 10.1037/h0042519, Pubmed:13602029. 780 
8. Bliss, T. V. & L√∏mo, T. Long-lasting potentiation of synaptic transmission in the dentate area of 781 
the anaesthetized rabbit following stimulation of the perforant path. J. Physiol. 232, 331‚Äì356 782 
(1973). 10.1113/jphysiol.1973.sp010273, Pubmed:4727084. 783 
9. Malenka, R. C. & Bear, M. F. LTP and LTD: an embarrassment of riches. Neuron 44, 5‚Äì21 784 
(2004). 10.1016/j.neuron.2004.09.012, Pubmed:15450156. 785 
10. Markram, H., L√ºbke, J., Frotscher, M. & Sakmann, B. Regulation of synaptic efficacy by 786 
coincidence of postsynaptic APs and EPSPs. Science 275, 213‚Äì215 (1997). 787 
10.1126/science.275.5297.213, Pubmed:8985014. 788 
11. Bi, G. Q. & Poo, M. M. Synaptic modifications in cultured hippocampal neurons: dependence 789 
on spike timing, synaptic strength, and postsynaptic cell type. J. Neurosci. 18, 10464‚Äì10472 790 
(1998). 10.1523/JNEUROSCI.18-24-10464.1998, Pubmed:9852584. 791 
12. Butts, D. A., Kanold, P . O. & Shatz, C. J. A burst-based ‚ÄúHebbian‚Äù learning rule at 792 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
43 
 
retinogeniculate synapses links retinal waves to activity-dependent refinement. PLoS Biol. 5, 793 
e61 (2007). 10.1371/journal.pbio.0050061, Pubmed:17341130. 794 
13. Hebb, D. O. The Organization of Behavior: A Neuropsychological Theory (Wiley, New York, 795 
1949). 796 
14. Song, S., Miller, K. D. & Abbott, L. F. Competitive Hebbian learning through spike-timing-797 
dependent synaptic plasticity. Nat. Neurosci. 3, 919‚Äì926 (2000). 10.1038/78829, 798 
Pubmed:10966623. 799 
15. Clopath, C., B√ºsing, L., Vasilaki, E. & Gerstner, W. Connectivity reflects coding: a model of 800 
voltage-based STDP with homeostasis. Nat. Neurosci. 13, 344‚Äì352 (2010). 10.1038/nn.2479, 801 
Pubmed:20098420. 802 
16. Isomura, T. & Friston, K. J. Reverse-engineering neural networks to characterize their cost 803 
functions. Neural Comput. 32, 2085‚Äì2121 (2020). 10.1162/neco_a_01315, 804 
Pubmed:32946704. 805 
17. Isomura, T., Shimazaki, H. & Friston, K. J. Canonical neural networks perform active inference. 806 
Commun. Biol. 5, 55, (2022). 10.1038/s42003-021-02994-2, Pubmed:35031656. 807 
18. Isomura, T. Active inference leads to Bayesian neurophysiology. Neurosci. Res. 175, 38‚Äì45 808 
(2022). 10.1016/j.neures.2021.12.003, Pubmed:34968557. 809 
19. Daunizeau, J., Den Ouden, H. E., Pessiglione, M., Kiebel, S. J., Stephan, K. E. & Friston, K. J. 810 
Observing the observer (I): Meta-Bayesian models of learning and decision-making. PLoS One 811 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
44 
 
5, e15554 (2010). 10.1371/journal.pone.0015554, Pubmed:21179480. 812 
20. Jimbo, Y ., Tateno, T. & Robinson, H. P . C. Simultaneous induction of pathway-specific 813 
potentiation and depression in networks of cortical neurons. Biophys. J. 76, 670‚Äì678 (1999). 814 
10.1016/S0006-3495(99)77234-6, Pubmed:9929472. 815 
21. Jimbo, Y ., Kasai, N., Torimitsu, K., Tateno, T. & Robinson, H. P . C. A system for MEA-based 816 
multisite stimulation. IEEE Trans. Biomed. Eng. 50, 241‚Äì248 (2003). 817 
10.1109/TBME.2002.805470, Pubmed:12665038. 818 
22. Isomura, T., Kotani, K. & Jimbo, Y . Cultured cortical neurons can perform blind source 819 
separation according to the free-energy principle. PLoS Comput. Biol. 11, e1004643 (2015). 820 
10.1371/journal.pcbi.1004643, Pubmed:26690814. 821 
23. Isomura, T. & Friston, K. J. In vitro neural networks minimise variational free energy. Sci. Rep. 822 
8, 16926 (2018). 10.1038/s41598-018-35221-w, Pubmed:30446766. 823 
24. Belouchrani, A., Abed-Meraim, K., Cardoso, J. -F. & Moulines, E. A blind source separation 824 
technique using second-order statistics. IEEE Trans. Signal Process. 45, 434‚Äì444 (1997). 825 
10.1109/78.554307. 826 
25. Cichocki, A., Zdunek, R., Phan, A. H. & Amari, S. I. Nonnegative Matrix and Tensor 827 
Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source 828 
Separation (John Wiley & Sons, 2009). 829 
26. Comon, P . & Jutten, C. Handbook of Blind Source Separation: Independent Component 830 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
45 
 
Analysis and Applications (Academic Press, 2010). 831 
27. Brown, G. D., Yamada, S. & Sejnowski, T. J. Independent component analysis at the neural 832 
cocktail party. Trends Neurosci. 24, 54‚Äì63 (2001). 10.1016/s0166-2236(00)01683-0, 833 
Pubmed:11163888. 834 
28. Mesgarani, N. & Chang, E. F. Selective cortical representation of attended speaker in multi-835 
talker speech perception. Nature 485, 233‚Äì236 (2012). 10.1038/nature11020, 836 
Pubmed:22522927. 837 
29. Ruaro, M. E., Bonifazi, P . & Torre, V. Toward the neurocomputer: image processing and pattern 838 
recognition with neuronal cultures. IEEE Trans. Biomed. Eng. 52, 371‚Äì383 (2005). 839 
10.1109/TBME.2004.842975, Pubmed:15759567. 840 
30. Chao, Z. C., Bakkum, D. J. & Potter, S. M. Shaping embodied neural networks for adaptive goal-841 
directed behavior. PLoS Comput. Biol. 4, e1000042 (2008). 10.1371/journal.pcbi.1000042, 842 
Pubmed:18369432. 843 
31. Feinerman, O., Rotem, A. & Moses, E. Reliable neuronal logic devices from patterned 844 
hippocampal cultures. Nat. Phys. 4, 967‚Äì973 (2008). 10.1038/nphys1099. 845 
32. Johnson, H. A., Goel, A. & Buonomano, D. V. Neural dynamics of in vitro cortical networks 846 
reflects experienced temporal patterns. Nat. Neurosci. 13, 917‚Äì919 (2010). 10.1038/nn.2579, 847 
Pubmed:20543842. 848 
33. Yada, Y ., Yasuda, S. & Takahashi, H. Physical reservoir computing with FORCE learning in a 849 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
46 
 
living neuronal culture. Appl. Phys. Lett. 119, 173701 (2021). 10.1063/5.0064771. 850 
34. Kagan, B. J. et al. In vitro neurons learn and exhibit sentience when embodied in a simulated 851 
game-world. bioRxiv (2021). 10.1101/2021.12.02.471005. 852 
35. Newsome, W. T., Britten, K. H. & Movshon, J. A. Neuronal correlates of a perceptual decision. 853 
Nature 341, 52‚Äì54 (1989). 10.1038/341052a0, Pubmed:2770878. 854 
36. Turrigiano, G. G. & Nelson, S. B. Homeostatic plasticity in the developing nervous system. Nat. 855 
Rev. Neurosci. 5, 97‚Äì107 (2004). 10.1038/nrn1327, Pubmed:14735113. 856 
37. Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P . & Pezzulo, G. Active inference and 857 
learning. Neurosci. Biobehav. Rev. 68, 862‚Äì879 (2016). 10.1016/j.neubiorev.2016.06.022, 858 
Pubmed:27375276. 859 
38. Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P . & Pezzulo, G. Active inference: A 860 
process theory. Neural Comput. 29, 1‚Äì49 (2017). 10.1162/NECO_a_00912, 861 
Pubmed:27870614. 862 
39. Funamizu, A., Kuhn, B. & Doya, K. Neural substrate of dynamic Bayesian inference in the 863 
cerebral cortex. Nat. Neurosci. 19, 1682‚Äì1689 (2016). 10.1038/nn.4390, Pubmed:27643432. 864 
40. Torigoe, M. et al. Zebrafish capable of generating future state prediction error show improved 865 
active avoidance behavior in virtual reality. Nat. Commun. 12, 5712 (2021). 10.1038/s41467-866 
021-26010-7, Pubmed:34588436. 867 
41. Schwartenbeck, P . & Friston, K. J. Computational phenotyping in psychiatry: a worked 868 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
47 
 
example. eNeuro 3, ENEURO.0049-16.2016 (2016). 10.1523/ENEURO.0049-16.2016, 869 
Pubmed:27517087. 870 
42. George, D. & Hawkins, J. Towards a mathematical theory of cortical micro-circuits. PLoS 871 
Comput. Biol. 5, e1000532 (2009). 10.1371/journal.pcbi.1000532, Pubmed:19816557. 872 
43. Doya, K. Canonical cortical circuits and the duality of Bayesian inference and optimal control. 873 
Curr. Opin. Behav. Sci. 41, 160‚Äì167 (2021). 10.1016/j.cobeha.2021.07.003. 874 
44. Deneve, S. Bayesian spiking neurons II: learning. Neural Comput. 20, 118‚Äì145 (2008). 875 
10.1162/neco.2008.20.1.118, Pubmed:18045003. 876 
45. Kappel, D., Nessler, B. & Maass, W. STDP installs in winner-take-all circuits an online 877 
approximation to hidden Markov model learning. PLoS Comput. Biol. 10, e1003511 (2014). 878 
10.1371/journal.pcbi.1003511, Pubmed:24675787. 879 
46. Jimenez Rezende, D. & Gerstner, W. Stochastic variational learning in recurrent spiking 880 
networks. Front. Comput. Neurosci. 8, 38 (2014). 10.3389/fncom.2014.00038, 881 
Pubmed:24772078. 882 
47. Berkes, P ., Orb√°n, G., Lengyel, M. & Fiser, J. Spontaneous cortical activity reveals hallmarks of 883 
an optimal internal model of the environment. Science 331, 83‚Äì87 (2011). 884 
10.1126/science.1195870, Pubmed:21212356. 885 
48. Nour, M. M. & Carhart-Harris, R. L. Psychedelics and the science of self-experience. Br. J. 886 
Psychiatry 210, 177‚Äì179 (2017). 10.1192/bjp.bp.116.194738, Pubmed:28249943. 887 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
48 
 
49. Merolla, P . A. et al. A million spiking-neuron integrated circuit with a scalable communication 888 
network and interface. Science 345, 668‚Äì673 (2014). 10.1126/science.1254642, 889 
Pubmed:25104385. 890 
50. Roy, K., Jaiswal, A. & Panda, P . Towards spike-based machine intelligence with neuromorphic 891 
computing. Nature 575, 607‚Äì617 (2019). 10.1038/s41586-019-1677-2, Pubmed:31776490. 892 
51. Whittington, J. C. & Bogacz, R. An approximation of the error backpropagation algorithm in a 893 
predictive coding network with local Hebbian synaptic plasticity. Neural Comput. 29, 1229‚Äì894 
1262 (2017). 10.1162/NECO_a_00949, Pubmed:28333583. 895 
52. Whittington, J. C. & Bogacz, R. Theories of error back-propagation in the brain. Trends Cogn. 896 
Sci. 23, 235‚Äì250 (2019). 10.1016/j.tics.2018.12.005, Pubmed:30704969. 897 
53. Sengupta, B. & Friston, K. J. How robust are deep neural networks?. arXiv arXiv:1804.11313 898 
(2018). https://arxiv.org/abs/1804.11313. 899 
54. Sengupta, B., Stemmler, M. B. & Friston, K. J. Information and efficiency in the nervous 900 
system‚Äîa synthesis. PLoS Comput. Biol. 9, e1003157 (2013). 10.1371/journal.pcbi.1003157, 901 
Pubmed:23935475. 902 
55. Wald, A. An essentially complete class of admissible decision functions. Ann. Math. Stat. 18, 903 
549‚Äì555 (1947). 10.1214/aoms/1177730345. 904 
56. Brown, L. D. A complete class theorem for statistical problems with finite-sample spaces. Ann. 905 
Stat. 9, 1289‚Äì1300 (1981). 10.1214/aos/1176345645. 906 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
49 
 
57. Berger, J. O. Statistical Decision Theory and Bayesian Analysis (Springer Science & Business 907 
Media, 2013). 908 
58. Kamioka, H., Maeda, E., Jimbo, Y ., Robinson, H. P . C. & Kawana, A. Spontaneous periodic 909 
synchronized bursting during formation of mature patterns of connections in cortical cultures. 910 
Neurosci. Lett. 206, 109‚Äì112 (1996). 10.1016/s0304-3940(96)12448-4, Pubmed:8710163. 911 
59. Tetzlaff, C., Okujeni, S., Egert, U., W√∂rg√∂tter, F. & Butz, M. Self-organized criticality in 912 
developing neuronal networks. PLoS Comput. Biol. 6, e1001013 (2010). 913 
10.1371/journal.pcbi.1001013, Pubmed:21152008. 914 
60. Forney, G. D. Codes on graphs: normal realizations. IEEE Trans. Info. Theory 47, 520‚Äì548 915 
(2001). 10.1109/18.910573. 916 
61. Dauwels, J. On variational message passing on factor graphs. In 2007 IEEE International 917 
Symposium on Information Theory (IEEE, 2007). 918 
62. Friston, K. J., Parr, T. & de Vries, B. D. The graphical brain: belief propagation and active 919 
inference. Netw. Neurosci. 1, 381‚Äì414 (2017). 10.1162/NETN_a_00018, Pubmed:29417960. 920 
  921 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
50 
 
Table 1. Correspondence of variables and functions 922 
Neural network formation  Variational Bayes formation 
Neural network cost function   ùêø ‚ü∫ ùêπ Variational free energy 
Sensory stimuli ùëú& ‚ü∫ ùëú& Observations 
Neural response B
ùë•&
ùë•&
C ‚ü∫ ùê¨& State posterior 
Synaptic strengths ùëä/ ‚ü∫ sig:!(ùêÄ!/) Parameter posterior 
Threshold factor ùúô ‚âî tùúô!
ùúô'
u ‚ü∫ ln ùê∑ State prior 
Firing threshold ‚Ñé/ = ln ùëä7/ ‚ãÖ 1w‚Éó + ùúô/ ‚ü∫ ln ùêÄ'/ ‚ãÖ 1w‚Éó + ln ùê∑/ 
Initial synaptic strengths ùúÜ/
< ‚äô ùëä7/
=>=? ‚ü∫ ùëé!/ Parameter prior 
Bold case variables (e.g., ùê¨2) denote the posterior expectations of the corresponding italic case 923 
random variables (e.g., ùë†2). Note that ùëä/
=>=? is the initial value of ùëä/ (for ùëô = 0,1) and ùúÜ/
< is 924 
the inverse learning rate factor that expresses the insensitivity of synaptic strengths to plasticity. 925 
Please refer to previous work [16,17] for details. 926 
 927 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 
51 
 
 928 
Extended Fig. 1. Reverse engineering of generative models and predictions of self-organisations 929 
under various conditions. Mixing matrix A in the external milieu and state prior employed by 930 
neuronal networks were varied. Top: Estimation of synaptic trajectories. Second line: Prediction of 931 
subsequent self-organisation. Third line: Reconstruction of posterior expectation about mixing 932 
matrix A. Averages among each group are shown. Fourth line: Error in predicting synaptic 933 
strengths. Bottom: Error in predicting neuronal responses. Prediction errors in bicuculline- and 934 
diazepam-treated groups were slightly larger than those in the control group. This may be partially 935 
because the formers involve a relatively larger error in estimating the initial conditions. 936 
(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
The copyright holder for this preprintthis version posted October 7, 2022. ; https://doi.org/10.1101/2022.10.03.510742doi: bioRxiv preprint 