Exploration and preference satisfaction
trade-oﬀ in reward-free learning
Noor Sajid∗
WCHN, University College London, UK
noor.sajid.18@ucl.ac.uk
Panagiotis Tigas
OATML, Oxford University, UK
ptigas@robots.ox.ac.uk
Alexey Zakharov
Imperial College London, UK
alexey.zakharov19@imperial.ac.uk
Zafeirios Fountas
WCHN, University College London, UK
zafeirios.fountas@huawei.com
Karl Friston
WCHN, University College London, UK
k.friston@ucl.ac.uk
Abstract
Biologicalagentshavemeaningfulinteractionswiththeirenvironmentdespite
the absence of immediate reward signals. In such instances, the agent can
learn preferred modes of behaviour that lead to predictable states – necessary
for survival. In this paper, we pursue the notion that this learnt behaviour
can be a consequence of reward-free preference learning that ensures an
appropriate trade-oﬀ between exploration and preference satisfaction. For
this, we introduce a model-based Bayesian agent equipped with a preference
learning mechanism (pepper) using conjugate priors. These conjugate
priors are used to augment the expected free energy planner for learning
preferences over states (or outcomes) across time. Importantly, Pepper
enables the agent to learn preferences that encourage adaptive behaviour at
test time. We illustrate this in the OpenAI Gym FrozenLake and the 3D
mini-world environments – with and without volatility. Given a constant
environment, these agents learn conﬁdent (i.e., precise) preferences and
act to satisfy them. Conversely, in a volatile setting, perpetual preference
uncertainty maintains exploratory behaviour. Our experiments suggest that
learnable (reward-free) preferences entail a trade-oﬀ between exploration and
preference satisfaction. Pepper oﬀers a straightforward framework suitable
for designing adaptive agents, when reward functions cannot be predeﬁned
as in real environments.
1 Introduction
Extrinsic rewards are not necessary to characterise an agent’s interaction with its environment.
For example, humans have been shown to rely on intrinsic motivation [47, 43, 6, 72], that
can adequately regulate behaviour2. Consequently, in the absence of immediate rewards,
∗Corresponding author
2Explicitly, this prescribes Bayes-optimal behaviour in the sense of Bayesian design and active
learning.
Proceedings of the Unsupervised Reinforcement Learning Workshop at the38 th International Con-
ference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).
arXiv:2106.04316v2  [cs.AI]  18 Jul 2021
there is a preferred exchange with the environment [2, 53, 15], that can be updated under
changing circumstances. Interestingly, this can result in accruing preferences – and habits –
that may be at odds with objective goals, e.g., kleptomania. In this paper, we demonstrate
that this kind of behaviour can be a consequence of reward-free preference learning that
encourages self-evidencing[29] and maintains an appropriate arbitration between exploration
and preference satisfaction. In brief, we will see that agents learn to explore or exploit,
depending upon the predictability of environmental contingencies.
Preference satisfaction subsumes homeostatic (extrinsic) motivations that encourage individ-
uals to maintain some ’preferred’ behaviour [43] and resist eﬀects of perturbations (external
or otherwise). Generally, these refer to base needs that can be satisﬁed, e.g., going to sleep or
eating food. Conversely, exploration involves heterostatic (intrinsic) motivations that distract
the agent from its homeostatic imperatives, e.g., novelty-seeking behaviour. Exploratory
behaviours would include trying a new hobby or taking a diﬀerent route to work. This
kind of exploration is distinct from random behaviour because it depends upon what the
agent does not know. Interestingly, over time, exploration can become the primary mode
of behaviour if exploration satisﬁes the agent’s (learnt) preferences when dealing with an
uncertain environment.
Our work is based on the notion that an adaptive agent learns the preferences that best
reﬂect its environment. To this end, we presentpepper; a preference learning mechanism
that can accumulate preferences over states (or outcomes) using conjugate priors – given
a model-based Bayesian agent. Here, we instantiate Pepper as a deep active inference
agent [17, 69, 10, 68, 14], maximising the evidence lower bound (or minimising the free
energy) during training, and optimising the expected free energy (or free energy of future
trajectories) for planning [44, 40]. However, it would be straightforward to add Pepper to
other Bayesian reinforcement learning (RL) agents instead e.g., MaxEnt[20], Max[56] or
Dreamer[25], etc. Active inference was chosen deliberately to leverage the expected free
energy (EFE) as a planning objective that captures the imperative to maximise: a) intrinsic
value – from interactions with the environment – about latent states, and b) extrinsic value,
namely, realising prior preferences over outcomes. Moreover, active inference’s Bayesian
formulation provides a natural way to introduce conjugate priors necessary for amortised
learning of preferences over states (or outcomes) – as previously shown in a simpliﬁed setting
for outcome preference learning[48].
Brieﬂy, Pepper comprises a two-step procedure which occurs after the (generative) model
of the agent is optimised for the environment (i.e., training time – see Fig. 1). The ﬁrst
step consists of short episodes of direct exchange with the environment, where a history of
observations and latent state representations are retained. Once each episode ﬁnishes, the
second step involves updating prior preferences based on the history using simple update
rules (see Section 4). Importantly, this means that agent can learn (diﬀerent) preferences
that encourage adaptive behaviour at test.
The key contributions of this work are:
• We present a simple, and ﬂexible, preference learning mechanism (pepper) to augment
the planning objective (i.e., EFE) for learning (state or outcome) preferences using
deep learning.
• Pepper is reward-free at train and test time. This is achieved by casting rewards as
a random variable in our generative model; equivalent to any other observation.
• Adaptive behaviour is conceptualised as a trade-oﬀ between exploration and prefer-
ence satisfaction.
In what follows, we review the related literature. Next, we introduce the problem setting
and pepper (the preference learning mechanism). We then evaluate the diﬀerent types of
preferences learnt during test time, and how they engender an appropriate trade-oﬀ between
exploration and preference satisfaction. Finally, we discuss the potential implications of this
work.
2 Related work
RL is regarded as a suitable framework for building artiﬁcial agents. However, by deﬁnition,
it relies on a reward signal to reinforce agent behaviour [66]. In reality, agents do not
2
operate in a problem-solving setting, where a “critic” may not be readily available to provide
immediate rewards [5, 59, 60]. Without task-speciﬁc reward signal (also calledextrinsic
reward), the agent is driven by intrinsic motivations that promote exploration, play and
curiosity [47, 58, 59]. Over the years, a variety of intrinsic motivation methods have been
proposed, largely focusing on exploration, based on information gain [30, 64], prediction
error [1, 45, 61], novelty search [39, 57], curiosity [50, 51], entropy [21, 38], or empowerment
[36, 42].
Lately, through the popularisation of self-supervised learning (SSL) methods [22, 41], the deep
RL community has turned its attention to self-supervised reinforcement learning. Auxiliary
tasks or rewards [31] are used – in the absence of any extrinsic rewards during train time – to
train intrinsically motivated agents for representation learning [70, 19, 35, 65] or generative
model learning [56, 55, 3]. Recent work [32, 71] has focused on theoretical properties of
reward-free Reinforcement Learning. However, the ultimate goal of such methods is to yield
easily transferable representations to be exploited upon introduction of a task.
Our approach diﬀers in several ways. First, we formulate intrinsic motivation using the
Expected Free Energy [17, 33] and focus on investigating the behaviour of intrinsically
motivated agents. Now exploration is simply an emergent behaviour of the planning objective
and not a mechanism for improving future task performance. Second, Pepper can be used
to learn preferences over both states and outcomes in a deep learning setting – extending
previous formulations to high-dimensional spaces [48]. Conceptually, open-ended learning
[52, 62, 63], where agents are responsible for never-ending learning opportunities, is closest
to our formulation. However, Pepper extends this scenario to show how agents can trade-oﬀ
between actively looking for opportunities to learn but also enjoy moments of preference
satisfaction.
3 Problem setting
We consider a world that can be represented as a discrete-time Markov decision process
(MDP), formally deﬁned as a tuple of ﬁnite sets(S,Π ,Ω ,P,R), such that: s ∈S is a
particular latent state,o∈Ω is a particular image observation,r∈R is a particular reward
observation, andPis a set of transition probabilities. Notice, we cast the reward function as
another random variable i.e., no diﬀerent to an image observation. Further,π∈Π where
π= {a1,a2,...,aT}is a policy (i.e., action trajectory) andΠ a ﬁnite set of all possible policies
up to a given time horizonT ∈N+ and T = {0,..,t,..,τ,T}a ﬁnite set which stands for
discrete time;t the current time andτsome future time. In short, we do not assume an
optimal state-action policy but consider sequential policy optimisation inherent in active
inference. Accordingly, the agent’s generative model is deﬁned as a probability density,
Pθ(o,r,s,π ), parameterised byθ(Fig. 1).
The generative model is instantiated as a Recurrent State-Space Model (RSSM) [24, 23, 25]3
where a history of observations (o0,o1,..,ot) and actions (a0,a1,..,at) are mapped to a
sequence of deterministic statesht. Using these, distributions over the latent states – both
prior and posterior – can be attained. Formally, this consists of the following:
• GRU [12] based deterministic recurrent model,ht = fθ(h<t,s<t,π<t);
• Latent state posterior,Qφ(st|ht,ot) ∼Cat, and prior,P(s) ∼Cat(D)
• Transition model,Pθ(st|ht) ∼Cat ;
• Image predictor (or emission model),Qφ(ot|ht,st) ∼Bernoulli ;
• Reward model,Qφ(rt|ht,st), and prior,P(r) ∼Cat(C);
where Qφ(·) denotes the approximate distribution, parameterised byφ.
3.1 Learning the generative model
To learn the generative model the evidence lower bound ( ELBO) of the likelihood
p(o1:T,r1:T |π) [24] or equivalently, the variational free energy [16, 14, 48] was optimised:
3https://github.com/danijar/dreamerv2 (MIT License)
3
Figure 1: Model architecture and2-step training procedure. Circles and squares denote
random and deterministic variables respectively. Coloured lines denote connections where
learning is employed. Shaded circles represent outcomes that have already been observed by
the agent. The ﬁrst ﬁgure shows the generative model used during learning. The second
panel is for Pepper (the preference learning phase) – comprising two steps in each episode:
1) interaction with the environment, &2) accumulation of preferences once interaction ends.
There is a bi-directional ﬂow between the2 steps: step 1 inﬂuences preference learning and
step 2 in turn inﬂuences environment interaction in the next episode.
L(θ) =
T∑
t=1

−EQφ [ln Pθ(ot |st,π) + lnPθ(rt |st,π)]  
reconstruction
+ EQφ [DKL(Qφ ∥Pθ(st |st−1,π)]  
dynamics

(1)
where, DKL denotes the Kullback–Leibler divergence. Practically, this entails using tra-
jectories generated under a random policy. See Appendix A forELBO implementation.
4 Pepper: preference learning mechanism
After learning the generative model, we substituted the planning objective with the expected
free energy. At time-stept and for a time horizon up to timeT, the expected free energy
(EFE) is [17, 14]:
G(π) =
T∑
τ=t
G(π,τ) =
T∑
τ=t
E˜Q

log Qφ(sτ|π) −log ˜Pθ(oτ,sτ|π)

, (2)
where ˜Q = Qφ(oτ,sτ,θ|π) = Q(θ|π)Q(sτ|,π)Qφ(oτ|sτ,π) and ˜Pθ(oτ,sτ|π) =
P(oτ)Q(sτ|oτ)P(θ|sτ,oτ,π). This is an appropriate planning objective because it:1) is
analogous to the expectation of theELBO (Eq.1) under the predictive posteriorPθ(ot|st,π)
and 2) can be decomposed into extrinsic and intrinsic value without any additional terms
[13, 40]. Accordingly, in the absence of learnt preferences – or whilst learning them – intrinsic
motivation contextualises agent’s interactions with the environment environment in a way
that depends upon its posterior beliefs about latent environmental states [4, 47]. Actions are
selected by sampling from distributionP(π) = arg max(−G(π)). We Pepper this planning
objective with conjugate priors to allow for preference learning over time.
4.1 Learning preferences using conjugate priors
A natural way to learn preferences is to extend the agent’s generative model with conjugate
priors over prior beliefs (i.e., hyper-priors) for each appropriate probability distributions,
that are learnt over time [17, 48, 13]. Generally, for closed-form updates any exponential
family would be appropriate [46]. Since our distributions of interest, latent state and rewards
are Categorical, we used the Dirichlet distribution as the conjugate prior. For the latent
state, P(s) ∼Cat(D), this is deﬁned as:
4
P(Di|di) = Dir(di) ⇒



EP(Di|di)

Di
ij

=
di
ij∑
kdi
kj
EP(Di|di)

log(Di
ij)

= Ϝ(di
ij) −Ϝ(∑
kdi
kj)
(3)
where, Ϝ is the digamma function,d ∼R+ and same parameterisation holds forP(r) ∼
Cat(C). The posteriors for the Dirichlet hyper–parameters are evaluated by updating the
prior using the following ruledi,j+ α∗si,j where si,j are the observations for that particular
category andα the learning rate. These estimates can be treated as pseudo–counts, and the
ensuing learning procedure is reminiscent of Hebbian plasticity – see [17, 49] for discussion.
In eﬀect, this allows the agent to accumulate contingencies and learn about what it prefers –
either via outcomes or states.
4.1.1 Augmented expected free energy
To incorporate preference over both states and reward outcomes, we use two distinct EFE
decompositions. The ﬁrst one instantiates preference learning over reward outcomes, as
presented below for a single time instanceτ[54, 14]:
G(π,τ) = −E˜Q

log P(r|C)

(4a)
+ E˜Q

log Q(sτ|π) −log P(sτ|oτ,π)

(4b)
+ E˜Q

log Q(θ|sτ,π) −log P(θ|sτ,oτ,π)

. (4c)
where, P(r|C) istheprobabilityofaparticularrewardoutcomegiven(learnt)priorpreferences
(C). The second decomposition incorporates preferences over latent states:
G(π,τ) = −E˜Q

log P(oτ|sτ,π)

(5a)
+ E˜Q

log Q(sτ|π) −log P(s|D)

(5b)
+ E˜Q

log Q(θ|sτ,π) −log P(θ|sτ,oτ,π)

. (5c)
where, P(s|D) is the probability of a particular state given (learnt) prior preferences (D).
These prior distributions are read as ‘preferences’ in active inference [48] – in the sense they
are the outcomes the agent expects its plans to secure. For both formulations, we drop the
conditioning on policy when learning of preferences and the requisite probability is calculated
using Thompson sampling. This entails sampling from the prior Dirichlet distribution
and estimating the likelihood. Additionally, two of the three terms that constitute the
expected free energy cannot be easily computed as written in Eq.4 & Eq.5. To ﬁnesse their
computation, we re-arrange these expressions and use deep ensembles [37] to render these
expressions tractable. See Appendix A for implementation details.
4.1.2 Pepper
Pepper comprises a double loop during preference learning. The ﬁrst loop is across time-steps
when the agent interacts with the environment. This loop stores information about what
happened (including observations, rewards, posterior, prior, etc). The second loop, evolving
at a slower timescale, is across episodes and entails preference learning. Speciﬁcally, once
the interaction with the environment ends, the agent updates the prior Dirichlet distribution
(over preferences) using the data gathered during the episode. In the subsequent time-steps
the updated preferences are used to select the next action (via their inﬂuence on expected free
energy). The data gathered during this episode are used to update preferences. In turn, these
preferences are used to select actions during the next episode, and so on. This demonstrates
a bi-directional ﬂow between the two loops: information from environment interactions
inﬂuences preference learning and the learnt preferences inﬂuence environment interaction
5
in the subsequent episode. The pepper preference learning procedure is summarised in
Algorithm.1.
Algorithm 1:Pepper
Input:
ht := fθ(h<t,s<t,π<t) Recurrent model
Qφ(st|ht,ot) Posterior model
Qφ(st|ht) Prior model
Pθ(ot|ht,st) Observation model
Pθ(rt|ht,st) Reward model
Initialise
uniform Dirichlet prior overP(r) or P(s) /* prior preference being learnt */
learning rateα
for each episodee do
reset environment and collect initial observations (o0 or r0)
for each time stept do
compute spo ∼Qφ(st|ht,ot) or spr ∼Qφ(st|ht)
compute G (Eq.4 or Eq.5) using (learnt) priors, observed and predicted posteriors
at ← arg max(−G(π))
execute at and receiveo or r
ot+1 ← o and rt+1 ← r
if reward preference learningthen
ci,t ← ci,t−1 + α∗ri,t /* Update rule for dir(c) */
else ifstate preference learningthen
dij,t ← dij,t−1 + α∗sprij,t /* Update rule for dir(d) */
5 Results
Here, we present two sets of numerical experiments that underwrite the face validity of
pepper in two and three-dimensional environments, respectively.
5.1 FrozenLake
We used a variation of the OpenAI Gym [8]4 FrozenLake environment to:1) evaluate diﬀerent
behaviours acquired (at test time) when eitherP(s) or P(r) was learnt, 2) qualify how
preferences can evolve as a result of environmental volatility, and3) quantify the trade-oﬀ
between exploration and preference satisfaction. The agent in the original FrozenLake
formulation is tasked with navigating a grid world comprised of frozen, hole and goal tiles,
using 4 actions (left, right, down or up). The agent receives a reward of10 upon reaching
the goal and a penalty−0.25 upon moving to the hole. To test preference learning, we
included a sub-goal tile and removed the reward signal (Fig.2A). In other words, although
the preference learning agent can diﬀerentiate between tile categories – given its generative
model – it receives no extrinsic signal from the environment. Here, we simulated a volatile
environment by switching the FrozenLake tile conﬁguration everyK steps and initialising
the agent in a diﬀerent location at the start of each episode. This furnished an appropriate
test-bed to assess how much volatility was necessary to induce exploratory behaviour and
shifts in learnt prior preference.
5.2 TileWorld
We extended the FrozenLake environment in the miniworld framework [11]5 to three dimen-
sions to test the generalisation and scalability of preference learning, when operating in a 3D
visual world (Fig.2C). In this task, the agent moves around in a small room with grey walls,
frozen and goal tiles on the ﬂoor. The agent spawns in a random location and receives pixel
observations (32x32 pixels with RGB channels) and a scalar value (reward) containing some
4https://github.com/openai/gym/ (MIT license)
5https://github.com/maximecb/gym-miniworld/ (Apache 2.0 License)
6
Goal
Hole
Sub-goal
Frozen
Reward
States
Episode
Marginal likelihood
A B
C
State learning (volatile)
State learning (static)
D
E Reward learning (volatile)
Reward learning (static)
Figure 2: A: The graphics show examples of the diﬀerent OpenAI Gym FrozenLake 16x16
environments used. B: The line plots shows the marginal likelihood (y-axis), across50
preference learning episodes (x-axis), forP(s) (i.e., states) andP(r) (i.e., reward) during
test time. Here, the dark lines represent the mean (across10 seeds), and shaded area the
95% conﬁdence interval. Diﬀerent shades of green denote levels of environment volatility.
C: The graphics show a particular agent trajectory across the3D−TileWorld environment
– with a10−step interval between each imageD-E: Visualisation of the posterior latent
states (estimated usingQφ(st|ht,ot)) during preference learning of (P(s) D and P(r) E)
episodes. The states have been projected onto the ﬁrst two principle components, and the
black circles represent their k-mean centroid. Here, the accompanying graphics present a
representative agent trajectory (D with visited tiles highlighted) and reward proﬁle (E) from
that particular cluster.
information regarding the tile its currently on (1 for red tiles,2 for green tiles). Additionally,
we introduce environmental volatility by changing the ﬂoor tiles to a random map and back
to the original map everyK steps. Alternating between the original and a random map
every K step is important to promote exploratory, novelty-seeking behaviour.
In the experiments that follow, we test agent behaviour in the two environments, with and
without volatility. The Dirichlet distribution for either prior preference distribution,P(s) or
P(r), was initialised as 1 (i.e., uniform preferences). Trained network weights, optimised Eq.1
using ADAM [34], were frozen during these experiments. Therefore, behavioural diﬀerences
are a direct consequence of pepper that induces diﬀerences in estimation of the EFE. See
Appendix B for architecture and training details for each environment.
5.3 Learnt preferences
State preferences Unsurprisingly, preference learning over latent states in the FrozenLake
environment revealed two types of behaviours: exploration and preference satisfaction (Fig.2D
& 3A). Here, under a static setting, preference satisfaction entailed restricted movement
within a small section of the FrozenLake with gradual accumulation of prior preferences (see
example trajectories in Appendix C.1). This speaks to the self-evidencing nature of Pepper.
That is as the Pepper agent sees similar observations across episodes it grows increasingly
conﬁdent (via increased precision over the prior preference) that these are the outcomes it
prefers. Conversely, exploratory behaviour is evident in a volatile setting (Fig.2D& 3A), as
gradual preference accumulation entails encoding of previously unseen states (see example
trajectories in Appendix C.1).
7
Pref. satisfy & explore trade-off
Environment volatility (%)
 Episode
Entropy
A B
EFE
Reward learning
State learning
Env. volatility (%)
Random
Figure 3: A: The violin plot presents the preference satisfaction and exploration trade-oﬀ
measured using Hausdorﬀ distance [7] at diﬀerent levels of volatility in the environment.
The x-axis denotes environment volatility at constant map (0%), change in map every40
steps (25%), 20 steps (50%), 10 steps (75%) and every step (100%). The y-axis denotes the
Hausdorﬀ distance. Here, red is for the agent optimising the standard expected free energy
(EFE) Eq.2, blue for reward preference learning Eq.4 and green for state preference learning
Eq.5. B: The line plot depicts the entropy overP(s) across varying levels of volatility in
the environment. The x-axis represents the episodes, and the y-axis entropy (in natural
units). Here, the dark lines represent the mean (across10 seeds), and shaded area the95%
conﬁdence interval. The pink line is for0%, blue for25%, green for50%, black for75% and
red for100% volatility in the environment.
Reward preferences Reward preference learning revealed subtle diﬀerences in preferences
(Fig.2E & 3A), where certain agents preferred sub-goal tiles more than neutral tiles. All
Pepper agents were able to immediately maximise their marginal likelihood6 over the
reward (Fig.2B). However we did not observe clear diﬀerences in preference learning as the
environment context shifted from non-volatile (i.e., map change) to increasingly volatile (map
changed every time step). We consider this to be consequence of the sparse categories over
the reward distribution, and the large percentage of map being taken up by the neutral tiles.
This meant preference accumulation was biased in favour of the neutral tile (see examples in
Appendix C.1).
5.4 Exploration and preference satisfaction trade-oﬀ
We evaluated the exploration and preference satisfaction trade-oﬀ using Hausdorﬀ distance
(Fig.3) [7]. This is an appropriate metric, which calculates the maximum distance of the
agents position in a particular trajectory to the nearest position taken in another trajectory.
Accordingly, a high Hausdorﬀ distance denotes increased exploration, since trajectories
observed across episodes diﬀer from one other. Whereas, a low distance entails prior
preference satisfaction as agents repeat trajectories across episodes. Using this metric, an
inverted u-shaped association (Fig.3A), between volatility in the environment and preference
satisfaction, is observed for preference learning over the states. Here,50% volatility in the
environment, shifts the pepper agents behaviour from satisfying preferences to becoming
exploratory, when faced with an uncertain environment (and inability to predict the future).
Agents in this setting (with the highest Hausdorﬀ distance) tend to pursue long paths from
the initial location. (Fig.2D).
Interestingly, at100% environmental volatility, the Pepper agents behaviour shifts back to
satisfying its preferences. These agents learn bi-modal preferences over the latent states.
6Marginal likelihood is simply the likelihood function of the parameter of interest (here state or
reward) where some parameter variables have been marginalised out.
8
Preference for grey wallsA B
T=0 T=7 T=15
C
Figure 4: A: The bar chart plots the percentage of time the agents spent observing the
3 colours in the TileWorld. The x-axis presents the colours: red (ﬂoor tile), green (ﬂoor
tile) and grey (wall colour), and y-axis the percentage of observations calculated using the
32x32x3 pixel image the agent received at each time step. Red is for the agent optimising
the standard expected free energy (EFE) Eq.2, blue for reward preference learning Eq.4
and green for state preference learning Eq.5.B: Sample trajectories for a single agent are
presented for agents acquiring a preference for observing grey walls during state preference
learning. Here, an orange circle denotes starting position, a blue triangle represents the
agents location until the ﬁnal position, and a green cross is the ﬁnal position.C: The line
plot depicts the entropy overP(s) (in orange) andP(r) (in blue) across varying levels of
volatility in the environment. The x-axis represents the episodes, and the y-axis entropy (in
natural units). Here, the dark lines represent the mean (across10 seeds), and shaded area
the 95% conﬁdence interval.
Regardless of how the map changes they move directly to either location given the initial
position (see trajectories in Appendix C.2). This ability to disregard random, noisy infor-
mation with continuous map changes (analogous to the noisy-TV setting introduced in [9])
highlights a motivation beyond random exploration under state preference learning. For
reward learning, exploration is also instantiated with increased volatility in the environment.
Yet, complete volatility does not trigger a deﬁnitive shift back to preference satisfaction.
Additionally, the expected free energy agent, without preference learning capacity, also
exhibits a shift in behaviour as the environment becomes volatile. The exploratory behaviour
here is driven exclusively by an imperative to resolve state uncertainty, (Eq.4b): i.e., the
mutual information between the agent’s beliefs about its latent state representation of the
world, before and after making a new observation.
5.5 Preference learning in the volatile TileWorld
Preference learning agents evinced a strong preference for looking at grey walls in the volatile
TileWorld environment (Fig.4A). This was consistently observed for both preference learning
over latent states and rewards. Importantly, when spawned in a location right next to the
wall, these agents were happy to satisfy their preferences and not move (Fig.4B). This is
driven by three factors: fast preference accumulation over grey walls, a small number of state
conﬁgurations and a generative model that is able to appropriately predict future trajectories
(see example reconstructions and imagined roll-outs in the Appendix B.2). However, volatility
in the environment does inﬂuence the encoding of prior preferences – as evident from the
observed state entropy ﬂuctuations across the episodes (Fig.4).
9
6 Concluding remarks
Summary: Pepper – the reward-free preference learning mechanism presented – provides
a simple way to inﬂuence agent behaviour. Although, unlike the RL formulation, we cast
what is preferredto the agent instead of the environment ’designer’. That is, the agent is
responsible for interacting with the environment and over time developing preferences that
it acts to satisfy without an extrinsic signal. Our experiments revealed that rich category
spaces are necessary for learning preferences that can establish distinct behavioural strategies
– speciﬁcally, within a volatile setting. Thus, future experiments looking to leveragePepper
should provide a suitable category space to learn over.
Conjugate priors and Hebbian plasticity:We employed a simple learning strategy for
accruing preferences, using conjugate priors. This type of learning usually calls on associative
or Hebbian plasticity [26], where synaptic eﬃcacy is reinforced by the simultaneous ﬁring
of pre-and post-synaptic neurons. For example, as more neutral tiles are observed, more
evidence is accumulated in the synaptic connection to support the hypothesis that neutral
tiles are preferentially observed. Implicitly, Pepper rests upon experience-dependent plasticity,
i.e., strengthening of synaptic connections during inference. These updates can have diﬀerent
times scales, and experiential levels (see Appendix D).
Additionally, our learning process is driven by synaptic plasticity that allows certain random
variables (s,r) to expand in light of new experiences [18]. They are only updated after
an exchange with the environment. This separation of ’experiencing’ the world, and then
’updating’ model parameters is reminiscent of sleep; in which synaptic homeostasis resets
model parameterisation, by encoding new synapses or removing redundant ones [27, 67, 28].
Removal of redundant model parameters is evaluated in experiments presented in Appendix
D, where removal of accumulated Dirichlet parameters reveals consistently exploratory
agents.
Limitations: Notably, havingareward-freeformulationisbothanadvantageandlimitation
of our approach. By not specifying an extrinsic reward function we forego control over the
agent’s behaviour. In other words, by removing the ability to manipulate agent preferences
regarding what is considered rewarding leads to the removal of the only clear communication
channel that can be used by a designer to control agent behaviour and/or deﬁne task goals.
Therefore, it would not be appropriate to use pepper in a setting where control of the agent’s
behaviour is required e.g., for autonomous vehicles. And, if it were, precise preferences would
have to be established under supervision.
Lastly, Pepper is contingent upon having a suitable Bayesian generative model that has all
the necessary components for evaluating the EFE. Without this, Pepper may not work or
the accumulated preferences might be misaligned with the observations. That is an inability
to optimise the marginal likelihood over the random variables of interest. Therefore, great
care should be placed to ensure that an appropriate model has been learnt (see Appendix
B.2 for implementation details).
Acknowledgements We thank Fatima Sajid for reviewing the manuscript. NS acknowl-
edges funding from the Medical Research Council, UK (MR/S502522/1). PT is supported
by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems (grant refer-
ence EP/L015897/1). KJF is funded by the Wellcome Trust (Ref: 203147/Z/16/Z and
205103/Z/16/Z).
References
[1] Achiam, J. and Sastry, S. (2017). Surprise-based intrinsic motivation for deep reinforce-
ment learning.ArXiv, abs/1703.01732.
[2] Ashby, W. R. (1961).An introduction to cybernetics. Chapman & Hall Ltd.
[3] Ball, P., Parker-Holder, J., Pacchiano, A., Choromanski, K., and Roberts, S. (2020).
Ready policy one: World building through active learning. InInternational Conference
on Machine Learning, pages 591–601. PMLR.
10
[4] Barto, A. G. (2013). Intrinsic motivation and reinforcement learning. InIntrinsically
motivated learning in natural and artiﬁcial systems, pages 17–47. Springer.
[5] Barto, A. G., Singh, S., and Chentanez, N. (2004). Intrinsically motivated learning of
hierarchical collections of skills. InProceedings of the 3rd International Conference on
Development and Learning, pages 112–19. Piscataway, NJ.
[6] Berlyne, D. E. (1960). Conﬂict, arousal, and curiosity.
[7] Blumberg, H. (1920). Hausdorﬀ’s grundzüge der mengenlehre.Bulletin of the American
Mathematical Society, 27(3):116–129.
[8] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and
Zaremba, W. (2016). Openai gym.arXiv preprint arXiv:1606.01540.
[9] Burda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018). Exploration by random
network distillation.arXiv preprint arXiv:1810.12894.
[10] Çatal, O., Verbelen, T., Nauta, J., De Boom, C., and Dhoedt, B. (2020). Learning
perception and planning with deep active inference. InICASSP 2020-2020 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3952–3956.
IEEE.
[11] Chevalier-Boisvert, M. (2018). gym-miniworld environment for openai gym.https:
//github.com/maximecb/gym-miniworld.
[12] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H.,
and Bengio, Y. (2014). Learning phrase representations using rnn encoder-decoder for
statistical machine translation.arXiv preprint arXiv:1406.1078.
[13] Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., and Friston, K. (2020).
Active inference on discrete state-spaces: a synthesis.Journal of Mathematical Psychology,
99:102447.
[14] Fountas, Z., Sajid, N., Mediano, P. A., and Friston, K. (2020). Deep active inference
agents using monte-carlo methods.arXiv preprint arXiv:2006.04176.
[15] Friston, K. (2013). Life as we know it. Journal of the Royal Society Interface,
10(86):20130475.
[16] Friston, K. J. (2010). The free-energy principle: A uniﬁed brain theory?Nature Reviews
Neuroscience, 11(2):127–138.
[17] Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo, G. (2017).
Active inference: A process theory.Neural Computation, 29(1):1–49.
[18] Fu, M. and Zuo, Y. (2011). Experience-dependent structural plasticity in the cortex.
Trends in neurosciences, 34(4):177–187.
[19] Guo, Z. D., Azar, M. G., Piot, B., Pires, B. A., Pohlen, T., and Munos, R. (2018).
Neural predictive belief representations.ArXiv, abs/1811.06407.
[20] Haarnoja, T., Ha, S., Zhou, A., Tan, J., Tucker, G., and Levine, S. (2018a). Learning
to walk via deep reinforcement learning.arXiv preprint arXiv:1812.11103.
[21] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018b). Soft actor-critic: Oﬀ-policy
maximum entropy deep reinforcement learning with a stochastic actor. InICML.
[22] Hadsell, R., Chopra, S., and LeCun, Y. (2006). Dimensionality reduction by learning
an invariant mapping. In2006 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’06), volume 2, pages 1735–1742. IEEE.
[23] Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2019a). Dream to control: Learning
behaviors by latent imagination.arXiv preprint arXiv:1912.01603.
11
[24] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J.
(2019b). Learning latent dynamics for planning from pixels. InInternational Conference
on Machine Learning, pages 2555–2565. PMLR.
[25] Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. (2020). Mastering atari with discrete
world models. arXiv preprint arXiv:2010.02193.
[26] Hebb, D. O. (1949). The organization of behavior; a neuropsycholocigal theory.A Wiley
Book in Clinical Psychology, 62:78.
[27] Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The" wake-sleep" algorithm
for unsupervised neural networks.Science, 268(5214):1158–1161.
[28] Hobson, J. A. and Friston, K. J. (2012). Waking and dreaming consciousness: neurobio-
logical and functional considerations.Progress in neurobiology, 98(1):82–98.
[29] Hohwy, J. (2016). The self-evidencing brain.Noûs, 50(2):259–285.
[30] Houthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F. D., and Abbeel, P. (2016).
Vime: Variational information maximizing exploration. InNIPS.
[31] Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., and
Kavukcuoglu, K. (2016). Reinforcement learning with unsupervised auxiliary tasks.arXiv
preprint arXiv:1611.05397.
[32] Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020). Reward-free exploration
for reinforcement learning. In International Conference on Machine Learning, pages
4870–4879. PMLR.
[33] Kaplan, R. and Friston, K. J. (2018). Planning and navigation as active inference.
Biological cybernetics, 112(4):323–343.
[34] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization.arXiv
preprint arXiv:1412.6980.
[35] Kipf, T., van der Pol, E., and Welling, M. (2020). Contrastive learning of structured
world models. ArXiv, abs/1911.12247.
[36] Klyubin, A. S., Polani, D., and Nehaniv, C. L. (2005). Empowerment: A universal
agent-centric measure of control. In2005 IEEE Congress on Evolutionary Computation,
volume 1, pages 128–135. IEEE.
[37] Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2016). Simple and scalable
predictive uncertainty estimation using deep ensembles.arXiv preprint arXiv:1612.01474.
[38] Lee, L., Eysenbach, B., Parisotto, E., Xing, E. P., Levine, S., and Salakhutdinov, R.
(2019). Eﬃcient exploration via state marginal matching.ArXiv, abs/1906.05274.
[39] Lehman, J. and Stanley, K. (2008). Exploiting open-endedness to solve problems through
the search for novelty. InALIFE.
[40] Millidge, B., Tschantz, A., and Buckley, C. L. (2021). Whence the Expected Free
Energy? Neural Computation, 33(2):447–482.
[41] Misra, I. and Maaten, L. v. d. (2020). Self-supervised learning of pretext-invariant
representations. InProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 6707–6717.
[42] Mohamed, S. and Rezende, D. J. (2015). Variational information maximisation for
intrinsically motivated reinforcement learning. InNIPS.
[43] Oudeyer, P.-Y. and Kaplan, F. (2009). What is intrinsic motivation? a typology of
computational approaches.Frontiers in Neurorobotics, 1:6.
12
[44] Parr, T. and Friston, K. J. (2019). Generalised free energy and active inference.Biological
Cybernetics, 113(5-6):495–513.
[45] Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven explo-
ration by self-supervised prediction. 2017 IEEE Conference on Computer Vision and
Pattern Recognition Workshops (CVPRW), pages 488–489.
[46] Raiﬀa, H. and Schlaifer, R. (1961).Applied Statistical Decision Theory. Studies in
managerial economics. Division of Research, Graduate School of Business Administration,
Harvard University.
[47] Ryan, R. M. and Deci, E. L. (2000). Intrinsic and extrinsic motivations: Classic
deﬁnitions and new directions.Contemporary educational psychology, 25(1):54–67.
[48] Sajid, N., Ball, P. J., Parr, T., and Friston, K. J. (2021). Active inference: demystiﬁed
and compared. Neural Computation, 33(3):674–712.
[49] Sajid, N., Parr, T., Hope, T. M., Price, C. J., and Friston, K. J. (2020). Degeneracy
and redundancy in active inference.Cerebral Cortex, 30(11):5750–5766.
[50] Schmidhuber, J. (1991). A possibility for implementing curiosity and boredom in
model-building neural controllers.
[51] Schmidhuber, J. (2007). Simple algorithmic principles of discovery, subjective beauty,
selective attention, curiosity & creativity. InDiscovery Science.
[52] Schmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation
(1990–2010). IEEE Transactions on Autonomous Mental Development, 2(3):230–247.
[53] Schrodinger, R., Schrödinger, E., and Dinger, E. S. (1992).What is life?: With mind
and matter and autobiographical sketches. Cambridge University Press.
[54] Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M.,
and Friston, K. J. (2019). Computational mechanisms of curiosity and goal-directed
exploration. eLife, 8:e41703.
[55] Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D. (2020).
Planning to explore via self-supervised world models. InInternational Conference on
Machine Learning, pages 8583–8592. PMLR.
[56] Shyam, P., Jaśkowski, W., and Gomez, F. (2019a). Model-based active exploration. In
International conference on machine learning, pages 5779–5788. PMLR.
[57] Shyam, P., Jaskowski, W., and Gomez, F. (2019b). Model-based active exploration. In
ICML.
[58] Singh, S., Barto, A., and Chentanez, N. (2004). Intrinsically motivated reinforcement
learning. In NIPS.
[59] Singh, S., Barto, A. G., and Chentanez, N. (2005). Intrinsically motivated reinforce-
ment learning. Technical report, MASSACHUSETTS UNIV AMHERST DEPT OF
COMPUTER SCIENCE.
[60] Singh, S., Lewis, R. L., Barto, A. G., and Sorg, J. (2010). Intrinsically motivated
reinforcement learning: An evolutionary perspective.IEEE Transactions on Autonomous
Mental Development, 2(2):70–82.
[61] Stadie, B. C., Levine, S., and Abbeel, P. (2015). Incentivizing exploration in reinforce-
ment learning with deep predictive models.NIPS.
[62] Standish, R. K. (2003). Open-ended artiﬁcial evolution. International Journal of
Computational Intelligence and Applications, 3(02):167–175.
13
[63] Stanley, K. O., Lehman, J., and Soros, L. (2017). Open-endedness: The last grand
challenge you’ve never heard of.While open-endedness could be a force for discovering
intelligence, it could also be a component of AI itself.
[64] Still, S. and Precup, D. (2011). An information-theoretic approach to curiosity-driven
reinforcement learning.Theory in Biosciences, 131:139–148.
[65] Stooke, A., Lee, K., Abbeel, P., and Laskin, M. (2020). Decoupling representation
learning from reinforcement learning.ArXiv, abs/2009.08319.
[66] Sutton, R. S. and Barto, A. G. (2018).Reinforcement Learning: An Introduction. MIT
press.
[67] Tononi, G. and Cirelli, C. (2006). Sleep function and synaptic homeostasis.Sleep
medicine reviews, 10(1):49–62.
[68] Tschantz, A., Seth, A. K., and Buckley, C. L. (2020). Learning action-oriented models
through active inference.PLoS computational biology, 16(4):e1007805.
[69] Ueltzhöﬀer, K. (2018). Deep active inference.Biological Cybernetics, 112(6):547–573.
[70] van den Oord, A., Li, Y., and Vinyals, O. (2018). Representation learning with
contrastive predictive coding.ArXiv, abs/1807.03748.
[71] Wang, R., Du, S. S., Yang, L. F., and Salakhutdinov, R. (2020). On reward-free rein-
forcement learning with linear function approximation.arXiv preprint arXiv:2006.11274.
[72] Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., and Cohen, J. D. (2014). Humans
use directed and random exploration to solve the explore–exploit dilemma.Journal of
Experimental Psychology: General, 143(6):2074.
14
A Pepper implementation
Pepper was implemented as an extension to Dreamer V2 [25] public implementation7.
Speciﬁcally, Dreamer’s generative model training loop was used, alongside a model predictive
control (MPC) planner. Therefore, the actor learning part of Dreamer was not incorporated,
and the generative model was trained using Plan2Explore [55]. Like Plan2Explore, an
ensemble of image encoders were learnt and the “disagreement” of the encoders was used as
an intrinsic reward during training. This guides the agent to explore areas of the map that
have high novelty and potentially high information gain, when acquiring a generative (i.e.,
world) model. Unfortunately, replacing the amortised policy with this planner made the
environment interaction (i.e., the acting loop) relatively slow. We implemented the planner
as described in Algorithm 2.
Algorithm 2:Planner
Input:
st current state
N Number of random action sequences to evaluate
Initialise
for i=1...N do
πi ∼U /* Random action */
scorei ← 0
for τ= t...H do
scorei ← scorei −G(πi,τ) /* Updated according Eq.4 or Eq.5 */
k← arg maxscore
Return πk
Upon training completion, we froze the generative model learnt weights and only allowed
learning of prior preferences. These (state or reward) preferences were updated after each
episode, as described in Algorithm 1.
A.1 Evidence lower bound
The generative model was optimised using the ELBO formulation introduced in [23]:
L(θ) =
T∑
t=1

−E[log Pθ(ot |st,π)]
Qφ(st|o≤t,a≤t)
−E[log Pθ(rt |st,π)]
Qφ(st|o≤t,a≤t)
  
reconstruction
(6)
+ E[DKL(Qφ(st |ot,st−1,π)) ∥Pθ(st |st−1,π)]
Qφ(st|o≤t,a≤t)
  
dynamics

.
A.2 Expected free energy for pepper
We implemented EFE using the parameterisations introduced in [14] and adapted for [25]:
• Term 4awas modelled as a categorical likelihood model (using normalised Dirichlet
counts).
• Term 4bwas computed as theKL divergence between the prior (s∼Q(sτ|π)) and
the posterior (s∼P(sτ|oτ,π)) states. This could be computed analytically because
the prior and posterior state distributions were modelled as Categorical distributions.
Here, the dependency on the policyπ was accounted for by using the RNN hidden
state ht summarising past actions and roll-outs.
• Term5a wascomputedastheentropyoftheobservationmodel P(oτ|sτ,π). Happily,
the factorisation of the observation model – as independent Gaussian distributions –
allowed us to calculate the entropy term in closed form.
7https://github.com/danijar/dreamerv2
15
• Term 5bwas computed as the diﬀerence betweenlog Q(sτ|π) and log P(sτ|D),
where log Q(sτ|π) was approximated using a single sample from the prior model
Q(sτ|θ,π). Again, the dependency onπ was substituted byht.
• Terms 4c and 5cwere more challenging to compute. Like [14], we rearranged
the expression toH(oτ|sτ,θ,π) −H(oτ|sτ,π). This translates toI(oτ; θ|sτ,π), and
can be approximated using Deep Ensembles [37, 55] and calculating their variance
Varθ[EQ(oτ|sτ,θ,π)]. Here, each ensemble component can be seen as a sample from
the posterior Q(θ|sτ,π). Our experiments showed that using 5 components was
suﬃcient.
B Experiments
FrozenLake For these experiments, we simulated the agent in ﬁve distinct situations
ranging from a non-volatile, static environment to a highly volatile one i.e., a diﬀerent
FrozenLake map every step. For all episodes in the static setting, the agent was initialised at
a ﬁxed location with no changes to the FrozenLake map throughout that particular episode.
Conversely, agents operating in the volatile setting were initialised at a diﬀerent location each
time. Moreover, the FrozenLake map was also changed everyN steps – given the desired
volatility level. For100% volatility the map changed every step,75% volatility corresponded
to map changes every10 steps, 50% volatility corresponded to map changes every20 steps
and 25% volatility corresponded to map changes every40 steps. Additionally, the generative
model used for these experiments was trained in a volatile setting where the map changed
every 5 steps (Table 1).
TileWorld For these experiments, we simulated the agent under two conditions (non-
volatile and volatile). For the non-volatile setting, the agent was initialised at a ﬁxed location
with no changes to the TileWorld map throughout training and testing. In the volatile
setting, for everyK step, we toggle alternate between a randomly sampled map and the
original map. This allowed us to simulate uncertain states that trigger exploratory behaviour.
The generative model used for these experiments was trained in a volatile setting where the
map changed every10 steps (Table 1).
Table 1: Training parameters
Parameter FrozenLake TileWorld
Planning Horizon 15 steps 15 steps
Episode Length 50 steps 200 steps
Reset Every 5 steps 10 steps
No. Episodes 50 episodes 50 episodes
No. State Categories 64 categories 32 categories
No. State Dimensions 50 dimensions 50 dimensions
No. Reward Categories 4 categories 3 categories
Table 2: Preference learning parameters for long-term learning
Parameter FrozenLake TileWorld
Planning Horizon 15 steps 15 steps
Episode Length 50 steps 200 steps
No. Episodes 50 episodes 50 episodes
Reset Map Every 1,10,20,40,50 steps 10 steps
No. Agents 10 agents 3 agents
B.1 Computational requirements
Overall, our experiments required1344 GPU hours. Each GPU was a GeForce RTX3090.
16
B.2 Image reconstruction and imagined roll-outs
For apt learning of preferences, the agent’s generative model must be able to accurately infer
the current (and future) states of aﬀairs. To evaluate this for our learnt generative models,
we illustrate representative examples of reconstructions encoded by the pepper agents for a
particular episode. Fig.5 shows the reconstructions for FrozenLake, and Fig.6 for TileWorld.
The imagined roll-outs for the TileWorld environment are shown in Fig.7.
Figure 5: An example of the FrozenLake reconstruction for the ﬁrst10 steps of an episode,
with map changes atT = 4 & 9.
Figure 6: An example of TileWorld reconstruction for the ﬁrst10 steps of an episode.
C Behaviour under long-term preference learning
C.1 Learnt preferences
We expected diﬀerences in learnt preferences to induce shifts in agent behaviour. For example,
agents who repeatedly accrued Dirichlet pseudo-counts for the same category would exhibit
preference satisfying behaviour. This would be due to high precision (or conﬁdence) over
that particular category. In contrast, an agent who accrued Dirichlet pseudo-counts for
diﬀerent categories would exhibit exploratory behaviour given an imprecise (or low conﬁdence)
distribution over the categories. To illustrate how diﬀerent environment settings shaped
preference learning we looked at the static and volatile setting where the FrozenLake map
changed every step. Fig.8 shows a representative example of state preference learning under
these conditions, and Fig.9 an example of reward preference learning. We observed that
state preferences learnt under a static setting were precise – denoted by the repeated pseudo-
count accumulation over category25. Conversely, for the volatile setting an imprecise state
preference distribution was learnt (Fig.8). Separately, we observed that the learnt reward
preferences were precise – regardless of the setting. We posit that this is a consequence of
17
Figure 7: An example the observations from an imagined roll-out in the latent state space.
These roll-outs are constructed using a random action trajectory that is propagated forward
to get a latent state sequence. For each of the latent states the observations were sampled
from the observation modelP(ot|st).
diﬀerences in the reward and state category space. In other words, having a large number of
state categories allowed distinct preferences to be learnt under static and volatile settings.
This is reﬂected in the qualitative diﬀerences seen between the two (Fig.8).
C.2 Agent trajectories
Next, we evaluated how disparate the agent trajectories were given the observed diﬀerences
in preference accumulation (Figure 8 and 9). For the static setting, we observed agents
satisfying their preferences by restricting movement to a small patch in the FrozenLake.
This behaviour was observed consistently across all agents (i.e., diﬀerent seeds) and episodes.
We present a representative example in Fig.10. Separately, agents simulated in the volatile
setting (where the map changed every step) learnt a bi-modal preference set (i.e., preferred
to go to one of two locations in the FrozenLake). Here, the location preference depended
on the initial location i.e., if the agent was initialised in a tile close to the ﬁrst preferred
location then it choose to go there. However, the second location was preferred if the agent
was initialised close to it. We present a representative example in Fig.11. Interestingly, this
18
Figure 8: An example of learnt state preferences for a single agent in a static and highly
volatile setting. Here,64 state categories are presented on the x-axis and episodes on the
y-axis. The ﬁrst panel is for preferences learnt under a static setting, and the second for
preferences learnt under a volatile setting. The scale goes from white (i.e., high Dirichlet
concentration) to black (i.e., low Dirichlet concentration), and grey indicates gradations
between these.
Figure 9: An example of learnt reward preferences for a single agent in a static and highly
volatile setting. The ﬁrst row is for preferences learnt under a static setting, and the second
row for preferences learnt in a volatile setting. Each ﬁgure illustrates the Dirichlet distribution
in a 3-dimensional coordinate space, i.e., 2-simplex – for a particular episode (T). Here, the
concentration of dots in one corner reﬂect precise beliefs; and scattered dots denote imprecise
beliefs. Each dot represents a single sample from the Dirichlet distribution (determined
by the alpha parameters denoted at the bottom of each ﬁgure), and each plot displays500
samples. For clarity, we collapsed Goal and Sub-goals into one category. Preferences for
both static and volatile setting are initialised as uniform (i.e.,(1,1,1,1) = (1,1,2)) denoted
by the dots scattered across the simplex.
behaviour was observed in agents where the environment was100% volatile, whereas agents
operating in slightly less volatile settings continued exploring (Fig.3A).
Importantly, these agents were able to disregard, noisy information about the states from
the environment. To qualify this, we looked at how the variance between the posterior
19
Figure 10: Representative example of the agent trajectories observed during state preference
learning under a static setting.
Figure 11: Representative example of the agent trajectories observed during state preference
learning under the volatile setting. Each ﬁgure is an illustration of the agents trajectory for
a particular episode. Here, purple is the agents starting position, pink the trajectory, cyan
square denotes the ﬁrst learnt preference and dark blue denotes the second learnt preference.
(s ∼Qφ(st|ht,ot)) and prior (s ∼Qφ(st|ht)) estimates diﬀered across the50 episodes for
these agents (Fig.12). We observed that the posterior estimates had a greater variance across
the 50 dimensions relative to the prior variance. Given how these estimates are calculated,
we postulate that diﬀerences in the variance were due to the change in the FrozenLake map
that the agent ﬁnds itself in after it moved one step. These high variances in the posterior
estimate, under a highly volatile setting, induced a change in behaviour from exploratory to
preference satisfaction.
D Behaviour under short-term preference learning
We expected reduced preference learning timescales to inﬂuence the agent’s preferred be-
haviour. To evaluate this, we consider a setting where the agent was equipped with a sliding
preference window i.e., afterk steps the previous preferences were removed in favour of new
ones. To evaluate short-term preference learning, we considered state preferences with a
sliding window of5 episodes (Table 3).
For this, we looked at the preferences learnt in the static and volatile (map changes every10
steps) setting (Fig.13). Predictably, we observed diﬀerences in the preference accumulation
when the agents learnt short-term preferences regardless of the setting. Explicitly, in the static
20
0 10 20 30 40 50 60
categories
0
10
20
30
40 episodes
Prior
0 10 20 30 40 50 60
categories
0
10
20
30
40
Posterior
Figure12: Varianceovertheestimatedposterior( s∼Qφ(st|ht,ot))andpriors( s∼Qφ(st|ht))
under the volatile setting. The scale goes from brown (low variance) to turquoise (high
variance), and light shades indicate gradations between these.
Table 3: Preference learning parameters for short-term learning
Parameter FrozenLake
Planning Horizon 15 steps
Episode Length 50 steps
No. Episodes 50 episodes
Reset Map Every 1,10,20,40,50 steps
Reset Preference Every 5 episodes
No. Agents 5 agents
setting the accrued preferences were ﬂexibly learnt and unlearnt over time e.g., category24
was slowly unlearnt in favour of11 category. Volatile conditions fostered perpetual preference
uncertainty as accumulated Dirichlet pseudo-counts were repeatedly updated. Therefore, we
would expect these agents to exhibit exploratory behaviour compared to agents equipped
with long-term preferences due to imprecise preference learning. To quantify this behaviour,
we projected the latent states onto the ﬁrst two components (ﬁtted using long-term state
preferences simulation data). The short-term simulation data only mapped onto a small
space in Fig.2 latent space. Furthermore, there was no clear separation between projected
latent states across the the volatile and static settings. This is reﬂected in the increased state
entropy (∼0.5 nats), under both settings, as previously learnt preferences were removed
(Fig.15).
Next, we considered how the exploration and preference satisfaction trade-oﬀ might vary
when preferences were learnt over a short time horizon. Using the Hausdorﬀ distance, we
evaluated how the volatility in the environment changed the agent’s behaviour to either
exploratory or satisfying preferences (Fig.15). In contrast to the long-term state preference
learning setting, we see a (slight) linear association between environment volatility and
preference satisfaction. This is consistent with our expectation that a slow removal of
accumulated Dirichlet parameters engenders consistently exploratory agents.
21
Figure 13: An example of short term state preference learning for an agent in a static and
highly volatile (map change every step) environment. Here,64 state categories are presented
on the x-axis and episodes on the y-axis. The ﬁrst panel is for preferences learnt under a
static setting, and the second for preferences learnt in a volatile setting. The scale goes
from white (high Dirichlet concentration) to black (low Dirichlet concentration), and grey
indicates gradations between these.
Figure 14: Visualisation of the posterior latent states (estimated usingQφ(st|ht,ot)) during
state preference learning. The states have been projected onto the ﬁrst two principle
components (ﬁtted using long-term state preferences simulation data), and the black circles
represent their k-mean centroid. Here, the accompanying graphics present a representative
agent trajectory with visited tiles highlighted from that particular cluster.
22
Figure 15: A: The violin plot illustrates the preference satisfaction and exploration trade-oﬀ
measured using the Hausdorﬀ distance [7] at diﬀerent levels of volatility in the environment,
when the agent had short term preferences. The x-axis denotes environment volatility: with
a constant map (0%), change in map every40 steps (25%), 20 steps (50%), 10 steps (75%)
and every step (100%). The y-axis denotes the Hausdorﬀ distance. Here, red is for the agent
optimising state preference learning Eq.5.B: The line plot depicts the entropy overP(s)
across varying levels of volatility in the environment. The x-axis represents the episodes,
and the y-axis entropy (in natural units). Here, the dark lines represent the mean (across5
seeds), and shaded area the95% conﬁdence interval. The pink line is for0%, blue for25%,
green for50%, black for75% and red for100% volatility in the environment. The spikes in
entropy correspond to overwriting of learnt preferences.
23