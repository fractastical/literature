The Role of Valence and Meta-awareness in Mirror
Self-recognition Using Hierarchical Active Inference
Jonathan Bauermeister1 and Pablo Lanillos1
Radboud University, Houtlaan 4, 6525 XZ Nijmegen, NL
Abstract. The underlying processes that enable self-perception are crucial for under-
standing multisensory integration, body perception and action, and the development of
the self. Previous computational models have overlooked an essential aspect: aﬀective or
emotional components cannot be uncoupled from the self-recognition process. Hence, here
we propose a computational approach to study self-recognition that incorporatesaﬀect
using state-of-the-art hierarchical active inference. We evaluated our model in a synthetic
experiment inspired by the mirror self-recognition test, a benchmark for evaluating self-
recognition in animals and humans alike. Results show thati) negative valence arises when
the agent recognizes itself and learns something unexpected about its internal states. Fur-
thermore, ii) the agent in the presence of strong prior expectations of a negative aﬀective
state will avoid the mirror altogether in anticipation of an undesired learning process. Both
results are in line with current literature on human self-recognition.
Keywords: Active Inference· Aﬀect · Self-mirror recognition
1 Introduction
The ability of self-recognition has been typically attributed only to humans and few other
species [2] and hides several essential brain processes related to multisensory perception, em-
bodiment and decision making [14,15]. To evaluate this ability, Gallup, in 1970, developed a test
for chimpanzees named the mirror self-recognition (MSR) [12]. This test, which was also adapted
for infants [1], consists of placing a mark, unbeknownst to the subject, on her face. The subject
is then placed in front of a mirror. The agent passes the test if there are reaching or exploratory
behaviours to remove the mark or inspect it.
While most studies postulate mark directed behaviour (inspect or remove) as a necessary
condition for self-recognition [3], more recent cross-cultural studies have shown that children
from cultures with higher parental authority are not inclined to remove the mark [4]. Similarly
if one creates a social context during the mirror test, where several other subjects around the
infant also have marks on their face, the infant despite having passed the mirror test before is less
motivated to engage in mark directed behaviour [19]. Both results, by enriching the complexity
of such behaviour by environmental and social factors, cast doubts on interpreting the necessity
or suﬃciency of mark directed behaviour for self-recognition.
On the other hand, it has been evidenced in humans a strong emotional component, i.e., to
express negative aﬀect when seeing their own reﬂection. When infants pass the test around the
age of two, they universally express negative aﬀect toward their mirror image, which has been
interpreted as embarrassment, shyness or puzzlement [1,18]. Ultimately, we do not know what
the phenomenology of a two-year-old seeing herself in the mirror is like. Anyhow,the negative
aﬀective part of the experience seems to be uncontested.
While there is previous research on computational models of self-recognition (e.g., generative
modelling focusing on visual-kinesthetic matching or appearance cues [16,14]), none of the works
arXiv:2208.13213v1  [q-bio.NC]  28 Aug 2022
2 J. Bauermeister and P. Lanillos
has attended to the emotional component. Here, we studied self-recognition by (i) developing of a
computational model that incorporates the aﬀective component into the self-recognition process
and (ii) evaluating it on a new synthetic experiment based on the MSR.
To model the aﬀective component we use the notion of valence [13]. The working hypothesis
is that the negative or positive quality of an aﬀective experience can arise as a consequence of
obtaining new information about oneself through mirror self-recognition. Importantly, this new
information might favour diﬀerent action selection policies (action dependent valence), leading to
a change in valence. This iterative process coherently connects emotions with self-recognition and
decision-making. To incorporate valence into the perception-action loop we used the hierarchical
active inference construct [11], where the agent perceives, learns and acts to obtain the expected
outcomes by minimizing the expected free energy.
We further developed an experimental benchmark to evaluate the eﬀect of valence in the pro-
cess of decision-making and self-recognition. In the experiment, the agent can decide to look at a
mirror, look at a wall or look at a video of another agent. Furthermore, thanks to the hierarchical
nature of the model, we further studied the importance of meta-cognition (‘higher’ layers), in
combination with aﬀect, for (anticipated) self-recognition. For instance, adults in full possession
of a self-concept can also anticipate a confrontation with their mirror image. If self-evaluation
is negative, or one’s body image has radically shifted due to surgery, patients are motivated to
actively avoid the mirror [10].
Related Work. There were several tries to build a computational model of self-recognition—
See [14] for a review. Relevant to this work, in [16] the robot inferred itself by answering the
question ’did I generate those sensory outcomes?’. For example, if the robot has an intention
to move its arm and can predict its interoceptive and exteroceptive sensory outcomes with
low prediction error, then it will infer that the likeliest cause of this action was the system
itself. While this approach may be promising to give insights into self-recognition, it does not
yet explain how aﬀect arises during the human MSR test. It has been theorized that aﬀective
and action based self-modelling naturally arises for a system engaged in deep temporal active
inference [6,7]. Nevertheless, to the authors knowledge, there is no computational model as of
now, that explicitly assesses aﬀect within self-recognition. Fortunately, recently within the active
inference research, there has been an eﬀort to introduce internal drivers that modulate the
generativemodelparametersandtheactionselectionprocess.Forinstance,valence–pleasantness
or unpleasantness of an emotional stimulus – was introduced in [13] to model the conﬁdence
of the model estimates. Importantly, valence encodes how well the agent is performing in the
environment, thus, aiding action selection. The mathematical formalization of valence and the
hierarchical structure of the generative model allows the building of a complex agent that has
beliefs over beliefs, which are modulated by the increase or decrease of valence, thus aﬀecting
the whole decision making process. In this work, we adapt this model to study self-recognition
and decision making.
2 Methods
First, we describe the general framework of our approach and how to introduce valence based on
the work of [13]. Second, we detail the aﬀective self-recognition computational model and ﬁnally,
we describe the experimental setup for self-recognition.
Title Suppressed Due to Excessive Length 3
2.1 Discrete hierarchical active inference
We model the problem under the discrete state-space formulation of active inference [17]1. The
agent computes both the posterior state estimation (perception) and action selection by minimiz-
ing, through marginal message passing, a single quantity: the expected free energy. This quantity
measures the divergence from the current expectations to the real world state. In order to be
able to compute it the agent needs a generative model of the world, thus, allowing predictions of
the future outcomes. See Appendix 5 for detailed explanation.
Temporal DepthTo achieve temporal depth we use a hierarchical generative model as depicted
in Fig. 1. Here, the hidden states on the second layer change slower than the hidden states on the
ﬁrst layer. Thus the beliefs about states in the ﬁrst layer (bottom) can ﬂuctuate several times
within one trial, where the beliefs on the second layer (top) only change at the end of each trial
(the length of a trial is deﬁned by the modeller). For example, the agent can have the abstract
belief that it is in a happy mood. This belief will set the priors on the ﬁrst level at the beginning
of a trial accordingly. So now the agent expects certain observations (facial muscles expressing a
smile, heart rate going up etc.), even if within the trial the facial muscles will most likely change
several times (depending on the granularity of the model) and not only stay in one position, say
smiling, the agent could still infer that overall it is happy (which is an abstract second-order
belief that integrates information over time). Only if in the course of the trial it consistently
observes unexpected observations (prediction errors) it will update its belief on the second level
at the end of a trial accordingly.
Mathematically, the second layer has likelihood mappings and state transitions like the ﬁrst layer.
They diﬀer in that the likelihood mapping A2 does not map from observation to hidden state
but from the hidden state at layer one (facial expression i.e smile, frown, neutral) to the hidden
state on layer two (mood i.e happy or sad). The transition matrix B2 then encodes how likely
the context, for example, the agent’s mood, changes over trials. As a consequence, the prior D1
is replaced by a more dynamically changing higher-order state. At the beginning of each trial,
the higher-order state acts as prior for the agent. By the end of the trial the agent updates the
higher-order state based on the information collected in this trial. For a mathematical expression
of ascending and descending messages—See [13] for further description.
Meta-cognition and Valence.An agent equipped with such a deep temporal model can learn
context and perform some tasks very well but underperforms in a volatile changing environment.
Here we describe, based on the work of [13], how aﬀect can be included in a discrete hierarchical
active inference network. Aﬀect can be formalized through valence (negative or positive). Valence
can be explained as the expression of conﬁdence in the model estimates. If the agent’s actions
continuously lead to the outcomes that it expects and prefers, it grows more conﬁdent in its
action model and weighs it stronger as acquired habits. Whereas if the environment is very
volatile and it cannot rely on its learned action model yielding to an ’anxious’ state. The agent
equipped with aﬀective states ﬁnds better and biologically more plausible action plans (policies)
than one without [13]. Partly because it takes time to construct a reliable action model that
tells the agent which policies to take under which circumstances. Hence, when the environment
changes fast and unexpected the action model (learn by experience) might become completely
useless. An aﬀective agent that reacts with negative valence towards the unexpected change
in the environment will able to quickly adapt by lowering the precision of its action model to
1 For a thorough tutorial on discrete active inference formulation see [21] and for a concise mathematical
overview see [5].
4 J. Bauermeister and P. Lanillos
Fig. 1.A temporally deep generative model with two hierarchical layers. The blue boxes in the ﬁrst layer
correspond to trials (here simpliﬁed). This is the architecture the agent uses to perceive and act in the
world. The second layer only communicates with the ﬁrst at the beginning of a trial through descending
Messages and at the end of the trial is informed by ascending messages. Hence, it can already be seen
that states on the second level change slower (only once every trial) than states on the ﬁrst layer. Here
the agent has two state factors on the second level. It has a contextual belief which replaces the prior at
the beginning of each trial. Additionally, it has an aﬀective state which sets the precision on G at the
beginning of the trial. The impact of G onπ can now be regulated by the agent through its aﬀective
state. Figure adapted from [13].
reevaluate the new situation. Conversely, an agent without aﬀective states cannot quickly adapt
and will execute the same actions despite an environment that has changed.
Thus, valence acts as a second-order state. Implementation-wise the agent has a categorical
distribution over it being either in the state ’positive valence’ or ’negative valence’, and it is
updated at the end of a trial via ascending messages. If in a trial the agent could rely on its
action model then it increases its valence for the next trial. At the beginning of the next trial,
instead of using a static prior the second-order aﬀective state informs theprecision of the action
model. Such a top-down estimation of the reliability of its model can also be understood as a
form of meta-cognition as it is monitoring the conﬁdence of the cognitive process.
Meta-awareness and attention.The meta-cognition architecture can be extended to model
meta-awareness by adding a third layer, and allowing the agent to dynamically change the infor-
mation ﬂow between second and ﬁrst layer. This has been implemented by [20] as a precision on
the likelihood mapping. Generally, precision on likelihood matrices in active inference is linked
to attentional processes. The states on the third level then represent if the agent is aware of
Title Suppressed Due to Excessive Length 5
her cognitive processes on the second level. The beneﬁts and biological plausibility of this meta-
awareness capacity have been shown by [20]. Here we use a simpliﬁed setup where we change the
precision of the 2nd layer likelihood by hand to see how the agent’s behaviour changes if it is
very aware of its emotional states and hence, can use the information for cognition deeper down
the hierarchy. The precision modulates how strong the connection between the two layers is and
therefore how informative descending and ascending messages are. For instance, low precision
will lead the agent to rely more on its ‘direct experience’ (ﬁrst layer of its generative model).
2.2 Aﬀective self-recognition model
Based on the previously described framework, we propose a computational model to investi-
gate self-recognition with the emotional component. We focused on the following two research
questions: (i) How does valence inﬂuence behaviour during mirror self-recognition? and (ii) How
might negative valence arise in mirror self-recognition?
To evaluate the model and further be able to answer the questions, we designed an exper-
iment where an agent can decide to either see its emotional expression in the mirror, look at
a wall and see no face or look at a video of an emotional expression of another person. Note
that our computer-simulated agent can not look into an actual physical mirror, so we need to
formalize the function of the mirror, which possibly leads to an aﬀective reaction. We interpret
making an observation in the mirror as acquiring information about oneself (here: about the
agent’s emotional state via its facial expression) by dragging the internal attention of the agent
onto this aspect of itself. Thus, the mirror is a self-exploration tool that allows this information
to be available for decision making and introspection from layers higher up the hierarchy.
Generative Model. We formalize a two-layered deep generative model to capture the self-
recognition experiment, as described in Fig. 2. The agent can obtain exteroceptive observations,
where it either sees a face that is happy, neutral or sad, or nothing when it looks at the wall.
This information can be used to infer the emotional state of the other person. And it can obtain
interoceptive observations about its facial expression (for example sensing its facial muscles) to
infer its emotional state (happy, neutral, sad). The agent also has a state (attention in the ﬁgure)
that captures if it is paying attention to its interoceptive observation with values Yes or No. This
is an attention state, in the sense that it modulates the precision on the likelihood A, but it
cannot be actively controlled by the agent. Instead, it captures what happens internally when
the agent sees itself in the mirror. By recognizing itself, it is forced to pay attention to its internal
observation (the precisionωon A will become very precise). Hence, formalizing the notion of the
mirror dragging attention onto oneself and making certain observations informative.
On the second layer, the agent has two state factors. The valence can be positive or negative
(implemented as a categorical distribution). Its value depends on the expected precision of the
action model G. Additionally, the agent has a second-order belief about which mood it is in
(happy, neutral, sad). This state sets the priors of what facial expression the agent expects when
observing itself. For all the details of how this generative model is implemented please see the
Appendix 5.
Agent operational speciﬁcation.First, inspired by the idea that if self-evaluation is positive
one seeks out a mirror, we assume that the agent prefers to seeitself but only if it is ’happy’ or
‘neutral’ instead of ‘sad’. This is encoded in the preference matrix C. The agent’s actions are to
go to one of the three locations: mirror, wall or tv. Because the agent knows from its generative
model that it will pay attention to itself if it goes to the mirror its behaviour will depend on its
self-knowledge, i.e., what state it believes to be in and how aware it is of that state. If it thinks it
6 J. Bauermeister and P. Lanillos
Fig. 2. Generative model description. Two layered model of an agent inferring its own mood and
deciding weather to look at the mirror or not. At any given time step the 1st layer includes an action
model G, preferences C and four state factors: Location, Other-facial-expression, My-facial-expression
and Attention. The Attention state modulates the likelihood A via the precisionω. For each state factor,
the agent has a categorical distribution of which state it believes itself to be in. The 2nd layer tracks the
agent’s valence and belief about its mood. Valence interacts with the action model G and the mood or
context state interacts with the agents belief of its facial expression on the ﬁrst level via A2.
is happy one would expect that the agent will act to admire itself in the mirror. Second, the true
emotional state of the agent may change. Here we have coupled the dynamics of the true state
to the current valence of the agent. If its positive valence goes above 70 % its true state shifts
to happy, below 30% to sad and otherwise neutral. To be robust against small ﬂuctuations, we
imposed that the agent’s belief about its valence has to shift at least by 15% to make a switch.
Although, these decisions are arbitrary, they are designed to show how the agent adapts to a
change in its true state.
3 Results
we analyzed our model behaviour in our synthetic experiment to study which actions the agent
chooses and when and how the valence of the agent changes under diﬀerent conditions, such as
changing the true emotional state, the prior knowledge the agent has about its emotional state
or the introspective availability of its emotional states (precision on A2). Particularly, we focused
on two diﬀerent initial conditions. In the ﬁrst experiment (Sec. 3.1), we study how valence might
naturally evolve in a mirror self-recognition scenario. To this end, the agents true state was set
to sad, but it had low meta-awareness. For the second experiment (Sec. 3.2), we studied mirror
avoidance behaviour. Thus, the agent had high meta-awareness. The code to replicate the results
can be found in this this link.
Each agent was evaluated for 8 consecutive trials in each condition, where each trial lasts for
three time steps (three observations). After the ﬁrst observation, the agent will have to decide
where to go (mirror, wall or tv). Its action plan horizon is two steps ahead, thus, it can predict
outcomes until the end of a trial by using its generative model.
3.1 Experiment 1: I am sad and I know it, but I am not very aware of it
Wesetthetruestateoftheagentto‘sad’andtheprecisiononA2low.Also,theagent‘knows’that
it is sad on the second level of the hierarchy. However, due to the low precision on A2 this will not
Title Suppressed Due to Excessive Length 7
Fig. 3. Experiment 1. The belief distribution of My- facial-expression (SFace1 and SFace2) at the
beginning of each trial are shown. Below that the agent with its true state is shown as smiley. It is
indicated at what location it currently is. The green box indicates if its attention is on the exteroceptive
or interoceptive observation. The graph next to it shows how the agent’s valence evolves throughout
the trials. The value is a probability, where a high value means high conﬁdence in being in the state of
‘positive valence’. At each trial, the agent has to choose where to go and hence at which location it will
start the next trial. The valence graph shows how negative aﬀect is elicited when the agent makes an
unexpected observation in the mirror and therefore learns something new about its internal states.
inform the ﬁrst level, thus, the agent will be more informed by the actual perceptual information
in a given trial. The experiment is described in Fig. 3. At trial 0 the agent is convinced enough
that it is sad and calculates that its best action will be to go to the video. Paying attention to
the other face, loosens its priors making them less informative about its emotional state. This
is reﬂected in the categorical distribution at trial 1. It is more entropic or less precise as in trial
0. In trial 1 it decides to go to the mirror. Hence in trial 2, it makes an unexpected observation
(seeing itself frown in the mirror) which also results in a drop in valence, indicating that this
particular mirror encounter is negatively experienced. Having reaﬃrmed its belief that it is sad,
it ﬁnds it best to go to the video. With this decision, the agent regains a bit of conﬁdence in its
action model. The valence goes up between trials 2 and 3. And due to the in build dynamics,
its true state shifts to neutral. Finally, the agent notices the change in its true state to neutral
and decides it’s time to go to the mirror again, which even further improves its conﬁdence in its
action model and for the rest of the trials it will be happily smiling at itself in the mirror.
3.2 Experiment 2: I am sad and I know it and I am aware of it
We explored how the behaviour of the agent changes if its ﬁrst-order states are introspectively
available to it. The experiment is described in Fig. 4. We set the same initial state as in the
previous study: true state is ‘sad’ and ﬁrst location is the ‘wall’. Diﬀerently, this time the mapping
A2 is very precise.
8 J. Bauermeister and P. Lanillos
Fig. 4. Experiment 2.The agent, now being more aware of its own internal state, is anticipating an
uncanny encounter with the mirror. Hence it is avoiding the mirror longer. However, it is also less able
to pick up on a change in the true state of its emotion due to it expecting, much stronger, that it is
actually sad. Its valence only shifts between sad and neutral. The results show how having strong priors
about its emotional state and being able to attend to it discourages exploratory behaviour such as going
to the mirror and learning about itself.
At the ﬁrst time point of each trial, the beliefs on both levels of the hierarchy are the same due
to the almost one-to-one mapping of A2. The agent’s behaviour diﬀers from previous experiment
in that it decides to stay at the video until trial 3. Only after a much longer time—when its
priors have loosened enough—it tries out the mirror again. When this happens (in trial 4) we
observe again a reconﬁrmation of its belief of being sad. This is accompanied by the same drop
in valence once it realizes it wasn’t the best action to go to the mirror. When going back to the
video the agent has the chance to pick up on the change of its true state. However, it misses it
because it keeps its prior belief of being sad extended through time.
4 Discussion
The experiments showed a possible self-recognition process where the agent gets insight into its
own generative model due to observations about itself made available through the mirror. The
valence of the agent was coupled to this observation being surprising or not. The results show,
how the valence changes and how the agents favoured actions change as a result of a change in
valence. Thus, modelling mirror self-recognition as an internal shift of attention shows how nega-
tive and positive valence plausibly arises. The mirror self-recognition provides the agent with new
self-knowledge, which can be used by deeper levels in the hierarchy to perform further inference.
For example, changing precision estimates, thereby possibly favouring diﬀerent actions, which
in turn results in a change in action-based valence. We do not model how self-knowledge ﬁrst
arises, but what can be shown here is that negative valence arises in self-recognition processes
Title Suppressed Due to Excessive Length 9
that yield insights about oneself, which change the best available action. The negative valence
is not directly dependent on the agent feeling sad or happy (its emotional states that the agents
tries to infer), but rather about the (accurate) knowledge the agent has about these states. It
is important to highlight that emotion is a more complex phenomenon that is likely constituted
by many more dimensions than just valence [8]. Therefore, this computational model only oﬀers
the ﬁrst steps, namely trying to account for the valenced part of mirror self-recognition. Besides,
only using a categorical attention internal state is a strong simpliﬁcation of the reﬂection of one’s
physical appearance for visual-kinesthetic matching as shown by [16,14]. Mirror self-recognition
in humans may additionally involve further internal attentional dynamics.
Meta-awareness.The capacity of meta-awareness allows an agent to change the strength with
which one is aware of oneself. From dreaming to being awake, from being lost in thought to paying
attention, humans in full possession of a self-concept do it all the time. The model behaviour in
experiment 2 (Fig. 4) shows how meta-awareness is important to explain mirror avoidance and
engagement behaviour. Being highly aware of a negative state of self an agent can anticipate an
unsettling mirror encounter and prefers to avoid the mirror. Although at the cost of potentially
missing a change in its true state. Given the limitations of the model, these statements are
speculative.Byexpandingthemodelinfutureresearchonecanpotentiallyaddressopenquestions
such as mirror avoidance and modelling mirrors in therapy [9]. Airing on the side of caution, even
if the proposed computational model here does not simulate self-awareness, it can be used to pose
interesting questions about action dependent aﬀect in mirror self-recognition for future work. For
example what are the actions available to an infant recognizing itself in the mirror? Is its negative
aﬀect resulting from suddenly being suspect of its usual policy of playful engagement with the
other in the mirror? Or is it a feeling of alienation? If one prefers to interpret the negative aﬀect
as a feeling of alienation one could argue to expand the model to include mental actions. Planning
on the second level (mental actions) could have its own conﬁdence and valence associated with
them. Actions on this (or even higher levels) could answer more existential questions such as
what kind of person should I be? How do others see me? Tracking the expected conﬁdence in
one’s mental actions might be an interesting choice to model more complex emotions such as the
feeling of alienation. It could be interesting to design clever mirror tests, that involve diﬀerent
action aﬀordances to test diﬀerent stages of self-awareness more speciﬁcally.
5 Conclusion
This thesis proposes an aﬀective self-recognition model based on the formalization of action
dependent valence, using hierarchical active inference. As a proof of concept, we have shown
how a synthetic aﬀective response towards one’s mirror image might arise. The results show that
mirror self-recognition provides the agent with new information, which changes the favoured
strategy and hence leads to negative valence. Secondly, the results show how an active inference
agent with high meta-awareness of a negative evaluated state of self displays mirror avoidance
behaviour. Therefore emphasizing the importance of deeper hierarchical layers, regarded as meta-
cognition and meta-awareness, to explain more complex behaviours seen when facing the MSR
test.
References
1. Amsterdam, B.: Mirror self-image reactions before age two5(4), 297–305 (1972).https://doi.org/
10.1002/dev.420050403
10 J. Bauermeister and P. Lanillos
2. Anderson, J.R., Gallup, G.G.: Which primates recognize themselves in mirrors? PLoS Biology9(3),
e1001024 (mar 2011).https://doi.org/10.1371/journal.pbio.1001024
3. Bard, K.A., Todd, B.K., Bernier, C., Love, J., Leavens, D.A.: Self-awareness in human and chim-
panzee infants: What is measured and what is meant by the mark and mirror test?9(2), 191–219
(mar 2006).https://doi.org/10.1207/s15327078in0902_6
4. Broesch, T., Callaghan, T., Henrich, J., Murphy, C., Rochat, P.: Cultural variations in children’s
mirror self-recognition42(6), 1018–1029 (sep 2010).https://doi.org/10.1177/0022022110381114
5. Costa, L.D., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active inference on discrete
state-spaces: A synthesis. Journal of Mathematical Psychology99, 102447 (dec 2020). https://
doi.org/10.1016/j.jmp.2020.102447
6. Deane, G.: Dissolving the self. Philosophy and the Mind Sciences1(I), 1–27 (mar 2020).https:
//doi.org/10.33735/phimisci.2020.i.39
7. Deane, G.: Consciousness in active inference: Deep self-models, other minds, and the challenge of
psychedelic-induced ego-dissolution. Neuroscience of Consciousness2021(2) (2021). https://doi.
org/10.1093/nc/niab024
8. Fontaine, J.R., Scherer, K.R., Roesch, E.B., Ellsworth, P.C.: The world of emotions is not two-
dimensional. Psychological Science 18(12), 1050–1057 (dec 2007). https://doi.org/10.1111/j.
1467-9280.2007.02024.x
9. Freysteinson, W.M.: Demystifying the mirror taboo: A neurocognitive model of viewing self in the
mirror. Nursing Inquiry27(4) (mar 2020).https://doi.org/10.1111/nin.12351
10. Freysteinson, W.M., Deutsch, A.S., Lewis, C., Sisk, A., Wuest, L., Cesario, S.K.: The experience
of viewing oneself in the mirror after a mastectomy. Oncology Nursing Forum39(4), 361–369 (jun
2012). https://doi.org/10.1188/12.onf.361-369
11. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active inference: A process
theory. Neural Computation29(1), 1–49 (jan 2017).https://doi.org/10.1162/neco_a_00912
12. Gallup, G.G.: Chimpanzees: self-recognition. Science167(3914), 86–87 (1970)
13. Hesp, C., Smith, R., Parr, T., Allen, M., Friston, K.J., Ramstead, M.J.D.: Deeply felt aﬀect: The
emergence ofvalence in deep active inference. Neural Computation33(2), 398–446(feb 2021).https:
//doi.org/10.1162/neco_a_01341
14. Hoﬀmann, M., Wang, S., Outrata, V., Alzueta, E., Lanillos, P.: Robot in the mirror: Toward an
embodied computational model of mirror self-recognition. KI - Künstliche Intelligenz35(1), 37–51
(2021). https://doi.org/10.1007/s13218-020-00701-7
15. Lanillos,P.,Dean-Leon,E.,Cheng,G.:Enactiveself:astudyofengineeringperspectivestoobtainthe
sensorimotor self through enaction. In: 2017 Joint IEEE International Conference on Development
and Learning and Epigenetic Robotics (ICDL-EpiRob). pp. 72–78. IEEE (2017)
16. Lanillos,P.,Pages,J.,Cheng,G.:Robotself/otherdistinction:activeinferencemeetsneuralnetworks
learning in a mirror (Apr 2020)
17. Parr, T., Markovic, D., Kiebel, S.J., Friston, K.J.: Neuronal message passing using mean-ﬁeld,
bethe, and marginal approximations. Scientiﬁc Reports9(1) (feb 2019).https://doi.org/10.1038/
s41598-018-38246-3
18. Rochat, P.: Five levels of self-awareness as they unfold early in life. Consciousness and Cognition
12(4), 717–731 (dec 2003).https://doi.org/10.1016/s1053-8100(03)00081-3
19. Rochat, P., Broesch, T., Jayne, K.: Social awareness and early self-recognition21(3), 1491–1497 (sep
2012). https://doi.org/10.1016/j.concog.2012.04.007
20. Sandved-Smith, L., Hesp, C., Mattout, J., Friston, K., Lutz, A., Ramstead, M.J.D.: Towards a
computational phenomenology of mental action: modelling meta-awareness and attentional control
with deep parametric active inference. Neuroscience of Consciousness2021(1) (aug 2021).https:
//doi.org/10.1093/nc/niab018
21. Smith, R., Friston, K., Whyte, C.: A step-by-step tutorial on active inference and its application to
empirical data (jan 2021).https://doi.org/10.31234/osf.io/b4jm6
Title Suppressed Due to Excessive Length 11
Appendix
Discrete Hierarchical Active Inference
Fig. 5.A generative model of one trial with three time steps. Rectangles correspond to categorical
distributions and circles to the random variables the agent wants to learn (here hidden states and
policies). A is the likelihood that deﬁnes how likely observations are given the stateP(oτ |sτ ). B encodes
the probability of moving from one state into the next oneP(sτ+1|sτ ,π) given the policy π. C is a
vector or matrix that encodes which observations the agent prefers. D gives the prior at the ﬁrst time
step in the trial to perform Bayesian inference. G is the expected free energy. The best policy is the one
that minimizes G (future reward + information gain) and F (current perceptual evidence or prediction
error). F is also calculated for each policy meaning that the agent has a posterior state estimate for all
possible policies. Lastly, E sets a ‘habitual’ prior for policies in case G is uninformative. Note that the
past messageln Bπτ−1sτ−1 at the ﬁrst time point becomes the priorln D and at the last time point the
future message becomes ones (hence uninformative). Finally, the actual observation is marked with a
bar o in contrast to the predictive posterior over observationso. Figure adapted from [13].
Figure 5 shows a generative model used for discrete hierarchical active inference. The agent’s
hidden state issτ, where τ indexes the time step. At each step, the agent gets an observation
from the environmentoτ. Following active inference simpliﬁed notation [5] we will use capital
letters to deﬁne the probabilistic functions. The agent has a prior belief D about hidden states
sτ, a likelihood mapping A between states and observations (P(oτ|sτ)), and a transitions matrix
B that encodes how states evolve over time depending on the policyπ(P(sτ+1|sτ,π)). The agent
12 J. Bauermeister and P. Lanillos
can invert the generative model to perform Bayesian inference and get from an observation to a
posterior over hidden states (in active inference this inference process is equated with perception).
To encode the intention or goal, the agent has preferred observations deﬁned by the matrix
C. By minimizing the expected free energy the agent chooses a policy that changes the hidden
states such that they are likely to produce preferred observations (and minimize overall percep-
tual ambiguity). The action model G uses those preferences to track how well each policyπ is
expected to achieve this goal.
To understand the computations we will describe an agent that performs a trial with three
time steps, meaning it has three observationsτ = {1,2,3}, as described in Fig. 5. Here an ac-
tual observation is denoted with a baro in contrast to the probability distribution of expected
observations o. From the ﬁrst observation the agent infers the posterior hidden statesτ at time
instant τ = 0through Bayesian inference, via the likelihood matrix A2 and the prior D:
sτ = lnA·oτ + lnD
Note that we almost get classical Bayesian inference (likelihood multiplied by the prior), but
without normalizing by the evidence term (the multiplication turns into addition due to working
in logarithmic space). The evidence term is mostly intractable in larger models. So to compute
the full posterior, we can alternatively minimize the variational free energy bound instead [11].
This free energy formulation in discrete state space boils down to the diﬀerence in the belief the
agent has about the world before (priorsτ) and after (posteriorsτ) an observation.
F = sτ −sτ
In other words, the free energy encodes the prediction error. If the prior belief matches or is
supported by the observation the free energy is low. In contrast to a surprising observation that
renders the prior belief less likely and therefore increases F. The agent can predict observations
with the generative model and the belief about hidden states. It evaluates these predictions by
how well they compare to the actual evidence, by calculating F. Then the agent can iteratively
make predictions that will decrease F and hence lead to more accurate estimates of hidden states.
We have described how the agent updates its beliefsτ about the world by trying to minimize
prediction errors, therefore getting good at expecting what is really out there. Next, the agent
also computes the expected observations to optimize not only for the current time point but for
the whole trial. Under each policy or plan of actionsπ the agent can evaluate, how likely certain
observations are in the future. Additionally, it can consider how ambiguous possible future ob-
servations are. Both, information gain and preferred observations, are described in the expected
free energy G:
G= ∑
τ(oτ ·(ln oτ −C) −diag(A·ln A) ·sτ)
The ﬁrst part of the equation is the average diﬀerence in expected observationsoτ and preferred
outcomes C over all time points. The second part relates to the model entropy or how precise the
distribution is from which the expected observations are sampled. For each state at timeτ there
is a likelihood A that can give the agent more or less certainty about what outcome to expect.
To sum up, the agent minimizesF to optimize the posterior belief about states (estimation)
and minimizes G to compute which policy to choose (action). Lastly, marginal message passing
2 Using the discrete space formulation of active inference in matrix form this is computed by selecting
the right column of the matrix A, i.e., through a one-hot observational vector.
Title Suppressed Due to Excessive Length 13
is just a mathematical way of sending information across time. For example, if the agent already
knows which policy it is likely to take after seeing the ﬁrst observation then that knowledge
can, through marginal message passing, already inform its prior at the next time point in the
trial. Vice versa the agent can update past beliefs, based on new observations which later can
be helpful for learning. This leaves us with the ﬁnal equations for posterior state estimation
including future and past messages (using the transition matrix B) and the average free energy
over timepoints in one trial:
sτ = σ(ln Bτ−1sτ−1 + lnA·oτ + lnBτsτ+1) (1)
F =
∑
τ
sτ ·(sτ −sτ) (2)
Where σ is a softmax function that normalizes the input vector such that it sums to 1 and forms
a proper probability distribution. The G and these two equations deﬁned as shown in Fig. 5,
describe the agents’ basis to act in the world within a given trial. A limitation of this scheme
is the static nature of the prior D at the beginning of each trial. It would be preferable that
the agent can update/learn its prior based on the information it gathered in a trial. To make
the context in which the agent navigates learnable one can expand the generative model with
a deep temporal layer [11]. This allows the agent to form abstract and contextual beliefs that
carry across trials—as described in the next subsection.
Aﬀective self-recognition model implementation details
This section provides details about the generative model. Simulations were run by extending
the pymdp infer-actively framework on github. The inference process of state estimation and
policy selection on the ﬁrst layer has been calculated using the pymdp framework. Inference
on the second level, via ascending and descending messages was programmed for this setup. A
commented code is available on github via this link.
First Layer
The priors on the state factors are speciﬁed in the D matrix. For the state factor ’Location’ (Mir-
ror, Wall, Video) the prior is uniform. The state factor ’Other emotional state’ (Happy, Neutral,
Sad, Null), which can be inferred via the observations (Smile, Neutral, Frown, None) also has
a uniform prior. The prior on ’Self emotional state’ depends on the starting condition and the
second layer. Lastly, the state ’Mirror-controlled attention’ (don’t attend, attend) is set on don’t
attend:
P(SMC−Attention
τ0 ) = [0.99, 0.01]
For each observation, there is a likelihood tensorA1−3. The ﬁrst observation is exteroceptive
(Smile, Neutral, Frown, None), the second interoceptive (Smile, Neutral, Frown) and the third
an observation about the location (which ensures the agent always knows where she is). The di-
mensionsofthelikelihoodsaretheobservationandallthehiddenstatefactors,i.e:A[Observation,
Location, Other, Self, Attention] orA1[4,3,3,3,2]. For example, if I want to index the likelihood
of my exteroceptive observation given that I am looking at the wall:
14 J. Bauermeister and P. Lanillos
P(Oex|SLocation = Wall,S Self,SOther,SMC−Attention) =
for i,j in 0:2, k in 0:1
A1[:,1,i,j,k ] =


0.01 Smile
0.01 Neutral
0.01 Frown
0.97 None


Basically saying the agent knows her probability of seeing ’None’ if she is at the wall is 0.97,
independent of all the other states she is in. If the agent is in the ’attend’ state she is attending to
herself and therefore can only relate the information of the exteroceptive observation to herself.
This has to be deﬁned for all states, but eﬀectively the agent only makes use of this attention
when she is in front of the mirror and the exteroceptive observation in fact relates to her:
P(Oex|SMC−Attention = attend,SSelf,SLocation) =
for l,i in 0:3 :
A1[:,l,i, :,0] =


0.97 0.01 0.01Smile
0.01 0.97 0.01Neutral
0.01 0.01 0.97Frown
0.01 0.01 0.01None


Here the columns stand for the diﬀerent states in the state factor ’Self emotional state’ (Happy,
Neutral, Sad). If the agent is not paying attention we get the same matrix, but this time relating
to the state of the other.
P(Oex|SMC−Attention = don’t attend,SLocation,SOther) =
for l,j in 0:3 :
A1[:,l, :,j, 1] =


0.97 0.01 0.01Smile
0.01 0.97 0.01Neutral
0.01 0.01 0.97Frown
0.01 0.01 0.01None


Now for the interoceptive observation, the precision on A will depend on the state of at-
tention the agent is in. Therefore one can push A through a softmax with a precision (inverse
temperature) parameter c.
P(Oin|SMC−Attention,SLocation,SOther) =
for l,i in 0:3 and k in 0:1 :
A2[:,l,i, :,0] =


0.97 0.01 0.01, cSmile
0.01 0.97 0.01, cNeutral
0.01 0.01 0.97, cFrown


Where paying attention hasc= 5and not paying attentionc= 0.001. Finally, the location
observation is a 1 to 1 mapping:
P(Oloc|SMC−Attention,SSelf,SOther) =
A3[:,l,i, :,0] =


1 0 0Mirror
0 1 0Wall
0 0 1Video


Next the transition matrices B need to be deﬁned. The rows correspond to the state in the
Title Suppressed Due to Excessive Length 15
next time step and columns the state in the current time step. The transition for the location
depends on the action chosen and the agent knows with certainty where she will be next. The
agent also knows that her attention state will shift to focused when she goes to the mirror and
unfocused going to the video. The agent has a bit of uncertainty around how her own emotional
state is changing in time and a bit more uncertainty about how the state of the other is changing.
P(SSelf
τ+1 |SSelf
τ ) =
B1[:,:,0] =


0.95 0.05 0.05
0.05 0.95 0.05
0.05 0.05 0.95


P(SOther
τ+1 |SOther
τ ) =
B2[:,:,0] =


0.8 0.1 0.1
0.1 0.8 0.1
0.1 0.1 0.8


The preference are set with the C matrix. For all observation modalities C will be initiated
with zeros. Then the preference to see self happy or neutral can be encoded as:
C1[0] =3.0
C1[1] =3.0
The description of the ﬁrst layer concludes with the policies available to the agent. They are
any combination of going to a location that is possible within a trial. The trials consist of three
observations and 2 actions. The agent starts by sampling an observation then decides where to
go, and repeats this step. After the ﬁnal observation, the agent doesn’t need to go anywhere
because the trial is over and will start again from the beginning.
Second Layer
The A and B matrix for the Valence state are the same as in [13]:
A2valence[:,:] =
(
0.97 0.3
0.3 0.97
)
B2valence[:,:] =
(0.8 0.3
0.2 0.7
)
For the state S2Face or ’Mood’, the A2 matrix can again be changed with a precision param-
eter c. This one is set manually to simulate meta-awareness. In my simulation high means c = 5
and low c = 1.
A2Face[:,:] =
(1 0, c
0 1, c
)
16 J. Bauermeister and P. Lanillos
B2Face[:,:] =


1 0 0
0 1 0
0 0 1


This concludes the description of the two-layered generative model.