 
 
Sensorimotor Visual Perception on Embodied System Using Free Energy Principle 
Kanako Esaki1, Tadayuki Matsumura1, Kiyoto Ito1 and Hiroyuki Mizuno1 
1Center for Exploratory Research, Research & Development Group, Hitachi, Ltd., Tokyo, Japan 
kanako.esaki.oa@hitachi.com 
 
 
Abstract 
We propose  an embodied system based on  the free energy 
principle (FEP)  for sensorimotor visual  perception. We 
evaluated it in  a character -recognition task using the MNIST 
dataset. Although the FEP has successfully described a rule that 
living things obey  mathematically and claims that a biological 
system continues to change its internal models and behaviors to 
minimize the difference in predicting sensory input, it is not 
enough to model sensorimotor visual perception. An 
embodiment of the system is the key to achieving sensorimotor 
visual perception. The proposed embodied system is configured 
by a body and mem ory. The body has an ocular motor system 
controlling the direction of eye gaze, which means that the eye 
can only observe a small focused area of the environment. The 
memory is not photographic, but is a generative model 
implemented with a variational auto encoder that contains prior 
knowledge about the environment , and that knowledge is 
classified. By limiting body and memory abilities and operating 
according to the FEP, the embodied system repeatedly takes 
action to obtain the next sensory input based on various 
potentials of future sensory inputs.  In the evaluation, the 
inference of the environment was represented as an approximate 
posterior distribution of characters (0 â€“9). As the number of 
repetitions increase d, the attention area move d continuously, 
gradually reducing the uncertain ty of  characters. F inally, the 
probability of the correct character bec ame the highest among 
the characters. Changing the initial attention position provides a 
different final distribution, suggesting that the proposed system 
has a confirmation bias. 
Introduction 
The human visual field seems to cover a wide area  of the 
surrounding environment , but the range with high enough 
resolution to identify the shape and color of the target object is 
limited to only the central visual fie ld of about 5 deg rees 
(Mandelbaum, J.  and Sloan, L. L. , 1947). Since the human 
visual field has this spatial limitation, it is necessary to move 
the gazing position to obtain environmental information (see) 
in a wide area. This leads to a phenomenon called sensorimotor 
contingency (hereafter called SMC; Oâ€™Regan, J. K. and NoÃ«, 
A., 2001; Seth, A.K., 2015 ), which changes the interpretation 
of â€œseeingâ€ (Land, M. F., 2006 ; Friston, K. and Kiebel, S., 
2009; Seth, A.K. et al., 2012; Adams, R. A. et al., 2013; 
Bogacz, R., 2017; Lotter, W., 2017 ). Human sensorimotor 
visual perception makes â€œseeingâ€ knowing about things to do 
rather than making an internal representation.  In other words, 
experience is not something we feel but something we do  
(Oâ€™Regan, J. K., 2001). This is because human sensorimotor 
visual perception infers the environmental state, not the sensory 
input itself. Therefore, the spatial limitation of the human visual 
field is not a â€œlimitationâ€ but is actually a â€œtriggerâ€ for an action 
that moves the gazing position. 
 In machine learning, many methods have been proposed that 
incorporate various characteristics and phenomena found in 
humans. The spatial limitations of the human body, such as that 
described above , are treated as constraints called partial 
observation in the context of reinforcement learning (Pineau, J. 
et al., 2003; Ji, S. et al., 2007; Silver, D. and Veness, J., 2010). 
These papers evaluated the degree of performance degradation 
by partial observa tion in classification and regression 
problems, assuming that not all necessary information can be 
obtained in most practical cases.  In terms of classification and 
regression performance, the spatial  limitations are certainly 
constraints and not treated as  â€œtriggersâ€ for actions. Various 
models and algorithms  similar to SMC have also been 
proposed. A uto regressive  models (Gregor, K.  et al., 2015 ; 
Oord, A. et al., 2016; Salimans, T. et al., 2017) predict the entire 
image by repeating the action of obtaining a partial image . 
Algorithms that generate exploring action s for reinforcement 
learning (Oh, J. et al., 2015 ; Houthooft, R. et al., 2016)  
contribute to finding optimum solutions by covering a wide 
range of the action space without bias. These models and 
algorithms generate the actions not on a basis of inference of 
environmental states but on that of sensory inputs such as 
partial image s and observations in reinforcement learning. 
None of the above studies  have achieved the inference of 
environmental states, which is the essence of human 
sensorimotor visual perception. 
 The purpose of this study is to achieve human sensorimotor 
visual perception using the free energy principle ( hereafter 
called FEP; Friston, K.  et al. , 2006; Friston, K. J., 2010; 
McGregor, S. et al., 2015; Friston, K. et al., 2017; Buckley, C. 
L. et al., 2017). The FEP mathematically describes the principle 
that living things obey . The free energy in the FEP measures 
the difference between the probability distributio n of 
environmental states that act on a biological system and an 
approximate posterior distribution of environmental states 
encoded by the configuration of that system. The biological 
system minimizes the free energy by changing its configuration 
to affect the way it samples the environment or by changing the 
approximate posterior distribution it encodes. These two 
changes correspond to action and perception, respectively, and 
lead to â€œactive inferenceâ€ and â€œperceptual inference.â€ The 
biological system thus  encodes an implicit and probabilistic 
model of the environment. 
 
 
Although the FEP has successfully describe d a biological 
system of performing active and perceptual  inferences 
mathematically, it is not enough to model human sensorimotor 
visual perception. An embodiment of this system is the key to 
achieving human sensorimotor visual perception. The 
embodiment provides an interaction between th e biological 
system and environment, resulting in sensory in puts and 
actions, and the causal relationship between these is stored in 
the biological system (Fitzpatrick, P. et al., 2003; Cheng, G. et 
al., 2006; Friston, K., 2011; Gallagher, S. and Allen, M., 2018). 
 In this paper, we propose an embodied system based on FEP 
to achieve sensorimotor visual perception.  The proposed 
embodied system is configured by a body, which partially 
observes the environment, and memory, which has classified 
prior knowledge about the environment as a generative model. 
Evaluation using the MNIST dataset  (LeCun, Y. and Cortes, 
C., 1998) shows that the proposed system triggers an action that 
moves a gazing position and repeatedly performs active and 
perceptual inferences by following the FEP. Moreover, the 
intentionality is reproduced on the proposed system, producing 
an equivalent of human confirmation bias.  We discuss how 
important this bias is for taking the action in an unknown 
environment. 
Sensorimotor Visual Perception 
To list the components necessary for achieving sensorimotor 
visual perception, the following problem settings are 
considered. Figure 1 shows an overview of our problem setting. 
Let us consider a situation in which  there is an object to be 
perceived, e.g., the number 5, in the environment and the vision 
sensor is used to perceive it.  The situation is called an 
environmental state ğ‘¥ğ‘¡. The vision sensor replaces the role of 
the human eye and has a spatial limitation of the visual field. 
The spatial limitation leads to a change in direction of the vision 
sensor to obtain images in the w ide area of the environment.  
When the direction of the vision sensor is determined in a 
certain direction , an image of a specific region of the 
environment is obtained. A representative position of the region 
(for example, the position of the upper left corner of the region) 
is defined as an attention position, which equals the gazing 
position of the human eye . The image of the region is defined 
as an attention image. The attention image at each time point 
( ğ‘‡ = ğ‘¡ âˆ’ 2, t âˆ’ 1, t ) is obtained by the previous actions 
ğ‘ğ‘¡âˆ’3, ğ‘ğ‘¡âˆ’2, ğ‘ğ‘¡âˆ’1  that move the attention position every time  
step. A composition image of the attention images obtained up 
to the current time ğ‘‡ = ğ‘¡ is used as a sensory input (hereafter 
called sensory input image ğ‘ ğ‘¡ ). As described above, 
sensorimotor visual perception is to perceive the object (the 
number 5) in the environment by repeating the actions to obtain 
ğ‘ ğ‘¡ . Our goal of this study is to achi eve sensorimotor visual 
perception based on FEP. 
 Free Energy Principle 
The FEP mathematically describes the perceptual and active 
inferences. In our problem setting, t he sensory input image ğ‘ ğ‘¡ 
is determined by the previous action ğ‘ğ‘¡âˆ’1  and the current 
environmental state ğ‘¥ğ‘¡. The ğ‘¥ğ‘¡ does not change (ğ‘¥ğ‘¡âˆ’1 = ğ‘¥ğ‘¡ =
ğ‘¥ğ‘¡+1). Under the above assumptions, the variational free energy 
and expected free energy , which are necessary components to 
describe the perceptual and active inferences , are expressed as 
follows. The variational free energy ğ¹(ğœ™ğ‘¥ğ‘¡ , ğ‘ğ‘¡âˆ’1) described in 
the FEP is expressed as 
 ğ¹(ğœ™ğ‘¥ğ‘¡ , ğ‘ğ‘¡âˆ’1)  
 = ğ¸ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡)[ln ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) âˆ’ ln ğ‘ğ‘ğ‘¡âˆ’1(ğ‘¥ğ‘¡, ğ‘ ğ‘¡ )]  
 = ğ·ğ¾ğ¿[ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ )||ğ‘ğ‘ğ‘¡âˆ’1 (ğ‘¥ğ‘¡|ğ‘ ğ‘¡)] âˆ’ ln ğ‘ğ‘ğ‘¡âˆ’1 (ğ‘ ğ‘¡), (1) 
where ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) is the approximate posterior distribution of 
ğ‘¥ğ‘¡ , and ğ‘ğ‘ğ‘¡âˆ’1 (ğ‘¥ğ‘¡, ğ‘ ğ‘¡)  is a generative model that stores the 
causal relationship of ğ‘¥ğ‘¡ and ğ‘ ğ‘¡ under ğ‘ğ‘¡âˆ’1. The ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) 
is made to minimize the variational free energy.  The expected 
free energy ğº(ğœ™ğ‘¥ğ‘¡+1, ğ‘ğ‘¡) described in the FEP is expressed as 
 ğº(ğœ™ğ‘¥ğ‘¡+1 , ğ‘ğ‘¡)  
 = ğ¸ğ‘(ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1,ğœ™ğ‘¥ğ‘¡+1)ğ¹(ğœ™ğ‘¥ğ‘¡+1, ğ‘ğ‘¡)  
 = ğ¸ğ‘(ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1,ğœ™ğ‘¥ğ‘¡+1)ğ¸ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1)  
   [ln ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1) âˆ’ ln ğ‘ğ‘ğ‘¡ (ğ‘¥ğ‘¡+1, ğ‘ ğ‘¡+1)]  
 = ğ¸ğ‘(ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1,ğœ™ğ‘¥ğ‘¡+1) [ğ¸ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1)  
   [ln ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1) âˆ’ ln ğ‘(ğ‘¥ğ‘¡+1)]  
   âˆ’ğ¸ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1) ln ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1)]  
 = ğ¸ğ‘(ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1,ğœ™ğ‘¥ğ‘¡+1)ğ·ğ¾ğ¿[ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1 )||ğ‘(ğ‘¥ğ‘¡+1)]  
 +ğ¸ğ‘(ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1,ğœ™ğ‘¥ğ‘¡+1) [âˆ’ğ¸ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1) ln ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1)]. (2) 
ğ‘ğ‘ğ‘¡ (ğ‘¥ğ‘¡+1, ğ‘ ğ‘¡+1)  is factorized into ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1)ğ‘(ğ‘¥ğ‘¡+1) 
since ğ‘¥ğ‘¡+1  and ğ‘ğ‘¡  are independent. The next action is 
selected to minimize the expected free energy. 
 The purpose of perceptual inference is to find ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) 
that minimizes ğ¹(ğœ™ğ‘¥ğ‘¡ , ğ‘ğ‘¡âˆ’1) . Since the second term of 
Equation (1) is composed of ğ‘ğ‘¡âˆ’1 and the current ğ‘ ğ‘¡, which 
have already been determined, the purpose is achieved by 
changing ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) with ğœ™ğ‘¥ğ‘¡  to make the first term close to 
zero. When t he first term  is made close to  zero, 
ğ‘ğ‘ğ‘¡âˆ’1 (ğ‘¥ğ‘¡|ğ‘ ğ‘¡ )~ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). Active inference aims to find  action 
Vision sensor
Environmental state ğ‘¥ğ‘¡
T=t-2T=t-1T=t
Attention image
Sensory input image ğ‘ ğ‘¡
Attention position moves ( ğ‘ğ‘¡âˆ’1)
every time step
Figure 1 Overview of problem setting to consider sensorimotor 
visual perception. 
 
 
ğ‘ğ‘¡ that minimizes ğº(ğœ™ğ‘¥ğ‘¡+1 , ğ‘ğ‘¡). Since ğ‘ğ‘¡ is included only in 
the second term  (hereafter called uncertainty), the purpose is 
achieved by minimizing the uncertainty.  
Proposed Embodied System 
for Sensorimotor Visual Perception 
An embodiment  is the key to achieving sensorimotor visual 
perception based on the FEP. The proposed embodied system 
is configured by a body and memory. The body has an ocular 
motor system for controlling the attention position. I n our 
problem setting shown in Figure 1 , the vision sensor is the 
body. The body has thus limited spatial observation ability, 
which means that it can only observe a small area of the 
environment that is focused upon . The memory is not 
photographic but is a generative model that contains prior 
knowledge about the environment , and that knowledge is 
classified. By limiting body and memory abilities and operating 
according to the FEP, t he proposed embodied system 
repeatedly performs perceptual and active inference . Figure 2 
shows the processing flow of sensorimotor visual perception 
with t he proposed system. In perceptual inference, a current 
sensory input image ğ‘ ğ‘¡ is generated using an attention image 
obtained from the  environment and the past attention images 
obtained up to the current time point. Then, ğ‘ ğ‘¡ is input to the 
generative model ğ‘ğ‘ğ‘¡âˆ’1(ğ‘¥ğ‘¡, ğ‘ ğ‘¡)  to calculate an approximate 
posterior distribution ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) and an approximate posterior 
image ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) which is a conversion of ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) into 
an image format . In active inference, the uncertainty of the 
expected sensory input image s ğ‘ ğ‘¡+1  is calculated using 
ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) . After that , a n attention  position having the 
minimum value is selected from an uncertainty map in which 
uncertainty is mapped for each ğ‘ ğ‘¡+1 . In generating ğ‘ ğ‘¡  (the 
first process of perceptual inference) , the attention image 
obtained from the environment and the past attention images 
are composed while maintaining their relative attention 
positions to generate ğ‘ ğ‘¡ . In calculating ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ )  and 
ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ )  (the second process of perceptual inference) , 
ğ‘ğ‘ğ‘¡âˆ’1 (ğ‘¥ğ‘¡, ğ‘ ğ‘¡)  is implemented with  a combination of a 
variational autoencoder  (VAE) and a fully  connected neural 
network (FNN), which is inspired by the idea of auxiliary loss 
in GoogleNet ( Szegedy, C.  et al.,  2015). The VAE contains 
prior knowledge about the environment, and the FNN classifies 
the prior knowledge in the latent space generated in the VAE. 
The combination of the VAE and FNN is pre-trained, which 
corresponds to human â€œexperience.â€ The pre -training is 
equivalent to changing ğœ™ğ‘¥ğ‘¡  in Equation (1) to make the first 
term of that equation closer to zero . Although ğœ™ğ‘¥ğ‘¡  is updated 
in the long term, it is fixed in the short term from ğ‘¡ âˆ’ 1 to ğ‘¡. 
The ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) and ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) are calculated by inputting 
the ğ‘ ğ‘¡ to the combination of the VAE and FNN.  Calculation 
of uncertainty (the first process of active inference)  uses 
ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). Uncertainty is the expected information amount 
of ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1), which is the probability distribution of ğ‘ ğ‘¡+1 
with action ğ‘ğ‘¡ as a parameter, which is conditioned by ğ‘¥ğ‘¡+1. 
Conditioning by ğ‘¥ğ‘¡+1(= ğ‘¥ğ‘¡) is interpreted as extracting ğ‘ ğ‘¡+1 
from ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). The ğ‘ ğ‘¡+1 is composed of the current ğ‘ ğ‘¡ 
and the candid ate attention images surrounding  it. 
Parameterizing ğ‘ğ‘¡  is interpreted as assuming the candidate 
attention images. Under these interpretations, the information 
amount of ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡+1|ğ‘¥ğ‘¡+1) is calculated as that of ğ‘ ğ‘¡+1. The 
information amount of ğ‘ ğ‘¡+1 is the entropy of the approximate 
posterior distribution calculated by inputting ğ‘ ğ‘¡+1  to 
ğ‘ğ‘ğ‘¡âˆ’1 (ğ‘¥ğ‘¡, ğ‘ ğ‘¡). Since ğ‘ ğ‘¡+1 is deterministically calculated from 
one ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) , the expected-value calculation is not  
Past attention
images
5
Sensory input
image ğ‘ ğ‘¡
V AE
Approximate posterior
distribution ğ‘ ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
0123456789
Approximate posterior
image ğ‘ğ‘–ğ‘šğ‘” ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
5
5 â€¦
ğ‘ ğ‘¡
Candidate
attention
image
Uncertainty =
Set of approximate posterior
entropies of each expected 
sensory input image
Attention position that
minimizes entropy is selected
Environmental
state ğ‘¥ğ‘¡
Attention image Attention position
Uncertainty map
<Perceptual inference> <Active inference>
FNN
Sensory-input-
image generation
Attention-position selectionApproximate posterior
calculation
Uncertainty calculation
One of
expected
sensory
input images
ğ‘ ğ‘¡+1
Generated for each candidate
Approximate posterior
image ğ‘ğ‘–ğ‘šğ‘” ğ‘¥ğ‘¡|ğœ™ğ‘¡
System
Body Memory
Environment
Generative
model
ğ‘ğ‘ğ‘¡âˆ’1 ğ‘¥ğ‘¡,ğ‘ ğ‘¡
Figure 2 Processing flow of sensorimotor visual perception with proposed embodied system. 
 
 
required. Uncertainty is thus the set of  the entropies of each 
ğ‘ ğ‘¡+1. In selecting the attention position (the second process of 
active inference) , an uncertainty map is generated where the 
entropy of each ğ‘ ğ‘¡+1corresponds to each attention position. An 
attention position that minimi zes the entropy is selected 
according to the uncertainty map. 
 Algorithm 1 shows the pseudo code of the processing flow. 
The ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡, ğ‘¥ğ‘¡) is pre-trained using training data of (ğ‘ ğ‘¡, ğ‘¥ğ‘¡). 
All the training data of ğ‘ ğ‘¡ are pre-processed so that the center 
of gravity of an image is shifted to the center position. During 
the operation of the proposed embodied system, the process 
from the 2nd line to the 12th line is repeated. First, an attention 
image ğ‘ â€²ğ‘¡  is obtained from the vision sensor. The past sensory 
input image s are composed with the obtained ğ‘ â€²ğ‘¡  while 
maintaining each relative attention position. The center of 
gravity of the composed image is calculated, and the composed 
image is shifted so that the center of gravity is located at the 
center position of the image. The shifted composed image is an 
ğ‘ ğ‘¡ . Then, ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ )  is calculated by inputting ğ‘ ğ‘¡  to 
ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡, ğ‘¥ğ‘¡). After that, the sub function starting from the 14th 
line is called to generate expected sensory input images ğ‘ ğ‘¡+1. 
In the sub function, an ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) is calculated by inputting 
ğ‘ ğ‘¡ to ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡, ğ‘¥ğ‘¡). A template image is generated by detecting a 
bounding rectangle area of non -zero pixels in ğ‘ ğ‘¡  and 
extracting the area from ğ‘ ğ‘¡. Template matching is carried out 
in the ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ), and the representative position  of the 
current ğ‘ ğ‘¡, ğ‘¢ğ‘ğ‘¢ğ‘Ÿ , is obtained. To calculate the next candidate 
attention positions ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡ , a candidate region of ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡ is set. 
The candidate region is a  region obtained by add ing a fixed 
margin pixel to a region of ğ‘ ğ‘¡ in ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). The region of 
ğ‘ ğ‘¡ is defined by ğ‘¢ğ‘ğ‘¢ğ‘Ÿ  and the size of the template image. The 
ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡  are calculated by sliding the window with the fixed stride 
pixel in the candidate region. The window is the size of ğ‘ â€²ğ‘¡ . 
The representative positions of all the window positions during 
sliding are ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡ . The ğ‘ ğ‘¡+1  are generated by extracting the 
region of the ğ‘ ğ‘¡ and the region of the next candidate attention 
images ğ‘ â€²ğ‘¡+1  from  ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). The region of the ğ‘ ğ‘¡  is 
defined by ğ‘¢ğ‘ğ‘¢ğ‘Ÿ  and the size of the template image, as 
mentioned above . The region of the ğ‘ â€²ğ‘¡+1  are defined by 
ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡  and the size of ğ‘ â€²ğ‘¡+1. The extracted images are  clipped 
or applied with zero-padding to have the same size as ğ‘ ğ‘¡. Each 
approximate posterior distribution ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1)  is 
calculated by inputting each image included in ğ‘ ğ‘¡+1  to 
ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡, ğ‘¥ğ‘¡). Note that ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) is calculated using the current 
ğ‘ ğ‘¡, while ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1) is calculated using ğ‘ ğ‘¡+1. The entropy 
of each ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1)  is calculated and added to the 
uncertainty map ğ‘€. Finally, the attention position having the 
minimum value in ğ‘€ is defined as the next attention position. 
 
Algorithm 1 Pseudocode of processing flow 
1: while system is operating do: 
2:   obtain attention image ğ‘ â€²ğ‘¡  
3:   compose past sensory input images with ğ‘ â€²ğ‘¡  
4:   generate sensory input image ğ‘ ğ‘¡ by shifting 
gravity point to center of composed image 
5:   calculate approximate posterior distribution 
ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) using ğ‘ ğ‘¡ and generative model 
ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡, ğ‘¥ğ‘¡)  
6:   call generate_expected_sensory_input_image 
7: for expected sensory input images ğ‘ ğ‘¡+1: 
8:     calculate approximate posterior distribution 
ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1) using ğ‘ ğ‘¡+1[ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥] and 
ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡ , ğ‘¥ğ‘¡)  
9:     calculate entropy of ğ‘(ğ‘¥ğ‘¡+1|ğœ™ğ‘¥ğ‘¡+1 ) 
10:     add entropy to uncertainty map ğ‘€ 
11:   end for 
12:   set attention position using ğ‘€ 
13: end while 
  
14: generate_expected_sensory_input_image 
15:   generate approximate posterior image 
ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) using ğ‘ ğ‘¡ and ğ‘ğ‘ğ‘¡ (ğ‘ ğ‘¡, ğ‘¥ğ‘¡) 
16:   generate template from ğ‘ ğ‘¡ 
17:   obtain current sensory input position ğ‘¢ğ‘ğ‘¢ğ‘Ÿ  
by template matching in ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) 
18:   calculate next candidate attention positions 
ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡  using ğ‘¢ğ‘ğ‘¢ğ‘Ÿ  
19:   generate ğ‘ ğ‘¡+1 using ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ), ğ‘¢ğ‘ğ‘¢ğ‘Ÿ , 
and ğ‘¢ğ‘›ğ‘’ğ‘¥ğ‘¡  
20 return with ğ‘ ğ‘¡+1 
 
Evaluation 
We evaluated t he proposed embodied system for achieving 
sensorimotor visual perception in a character -recognition task 
with partial observation constraint. We used the MNIST dataset 
as the character data set. The generative model was 
implemented with a convolutional VAE ( Kingma, D.P. and 
Welling, M., 2014; Rezende, D. J. et al., 2014) that was robust 
against displacement and a three -layer FNN (30 units in the 
hidden layer, 10 units in the output layer, and the activation 
function of the output layer was the Softmax function).  
Training of  the generative model involved 60,000 MNIST 
images (28 Ã— 28 pixels) . The stochastic variables ğ‘¥ğ‘¡  of the 
approximate posterior distribution ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) were labels 0â€“9 
of the MNIST dataset. The size of both the sensory input image 
and approximate posterior image was 28 Ã— 28 pixels, while the 
size of the attention image was 6 Ã— 6 pixels.  The margin pixel 
and s tride pixel for generating the next candidate attention  
images were set to 3 and 1, respectively.  The number of 
attention repetitions was set to 20. 
 Perceptual inference was performed during the attention 
repetitions. Figure 3 shows the transition of ğ‘(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) when 
making an attention at characters â€œ0â€, â€œ2â€, â€œ9â€, and â€œ3â€. 
Symbols A, B, C, and D in Figure 3 (a) are described later. Each 
graph plots the probability distribution for the stochastic 
variable (0â€“9). In each subfigure ((a)â€“(d)), the graph on the left 
shows the distribution for the 1st to 10th attentions in order 
from the top, and the grap h on the right shows the distribution 
for the 11th to 20th attentions in order from the top. The image 
on the right side of each graph is the c orresponding 
approximate posterior image ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). Regardless of the 
characters, the probability of â€œ1â€ was maximum from the 1st to 
the 2nd attention, and the ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) at that time contained 
a shape that was difficult to recognize as a character. After the 
3rd attention, the probability of the characters different from 
the correct characters â€œ0â€, â€œ2â€, â€œ9â€, and â€œ3â€ changed to the 
maximum, and the ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) at that time contained shapes 
similar to numbers â€œ5â€ and â€œ7â€ . When making an attention  at 
characters â€œ0â€, â€œ2â€, and â€œ9â€, the probability of correct 
 
 
characters was maximum at the 20th attention, and â€œ0â€, â€œ2â€, 
and â€œ9â€ appeared in the ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ). On the other hand, 
when making an attention  at character â€œ3â€, the probability of 
â€œ7â€ was maximum from the 4th to 13th and from the 15th to 
19th attentions, and the probability of â€œ2â€ was maximum even 
at the 20th attention. The ğ‘ğ‘–ğ‘šğ‘”(ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡ ) contained a part of 
â€œ3â€, not the entire â€œ3â€. 
 Active inference was performed during attention repetitions. 
Figure 4 shows the uncertainty map at the 1st, 4th, 13th, and 
20th attentions (corresponding to A, B, C, and D  in Figure 3 
A
B
D
C
ğ‘ ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
ğ‘¥ğ‘¡ ğ‘¥ğ‘¡
(a)
1st to 10th attentions 11th to 20th attentions
(b)
ğ‘ ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
ğ‘¥ğ‘¡ ğ‘¥ğ‘¡
1st to 10th attentions 11th to 20th attentions
ğ‘ ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
ğ‘¥ğ‘¡ ğ‘¥ğ‘¡
(c)
1st to 10th attentions 11th to 20th attentions
ğ‘ ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
ğ‘¥ğ‘¡ ğ‘¥ğ‘¡
(d)
1st to 10th attentions 11th to 20th attentions
Figure 3 Transition of approximate posterior distribution along with attention repetition. Each 
subfigure (a), (b), (c), and (d) corresponds to making attention at the characters â€œ0â€, â€œ2â€, â€œ9â€, 
and â€œ3â€, respectively. 
 
 
(a), respectively) when making an attention at â€œ0â€. The vertical 
(x) and horizontal (y) axes of the maps indicate the position 
coordinates of the upper left corner of the candidate attention 
image. The values of the uncertainty map of the 1st and 20th 
attentions were biased toward the minimum value, whereas the 
values of the uncertainty map of the 4th and 13 th attentions 
were dispersed. 
 Sensorimotor visual perception was performed by repeating 
perceptual and active inferences. Figure 5 shows attention  
images and sensory input images for each number of attentions 
when making attentions from â€œ0â€ to â€œ9â€. Each subset of two 
rows corresponds t o making attentions from â€œ0â€ to â€œ9â€.  The 
first row shows the attention images (The rectangle area of each 
image is the attention image ). The second row shows the 
sensory input images . As the number of attention repetition 
increased, the visible area of a character became larger, and at 
the 20th attention , except â€œ3 â€, most of the characters were 
visible enough to be recognized by humans as numbers. At the 
20th attention of â€œ3â€, however, a part of â€3â€ was missing. 
 To analyze the case of making an attention at â€œ3â€, the initial 
attention position was changed. Figure 6 shows transition of 
approximate posterior distribution , attention image, and 
sensory input image  when the initial attention position was 
changed to a position different from those in Figures 3 and 5 in 
the case of making an attention at â€œ3â€. Similar to Figure 3 (d), 
the probability of â€œ1â€ was maximum from the 1st to 4th 
attentions, a nd the approximate posterior image at that time 
contained shapes that were difficult to recognize as characters. 
Different from Figure 3 (d), the probabilities of â€œ1â€, â€œ3â€, and 
â€œ5â€ changed to maximum after the 5 th attention, and the 
probability of correct character â€œ3â€ reached maximum at the 
20th attention. The probability image contained â€œ3â€. 
Discussion 
In the transition of the approximate posterior distribution , the 
probability of â€œ1â€ was maximun at the 1st attention for all 
characters from â€œ0â€ to â€œ9â€ that were verified. This is because, 
at the 1st attention, the sensory input image only includes the 
1st attention image capturing only points or short lines, and this 
is perceived as a part of â€œ1â€. In fact, the approximate posterior 
images also looked like â€œ1.â€ This is similar to the situation in 
which humans temporarily labeled information (in this case, 
â€œ1â€) so that they could identify the environment based on the 
information they had collected and their experience thus far and 
moved on to the next action. 
 In the middle stage of attention repetition for most of the 
characters, a form that could not be identified as any shape was 
shown in the approximate posterior image; accordingly, the 
approximate po sterior distribution fluctuated among various 
shapes during attention repetition. This situation is a nalogous 
to a human making a decision without a specific classification 
label, considering the various possibilities when information is 
uncertain. 
 While most characters fluctuated among various shapes in 
the middle stage of the attention, the period in w hich â€œ7â€ was 
higher was maintained in the middle stage of the attention 
repetition when making the attention of â€œ2â€. This is because the 
shape of â€œ2â€ includes the shape of â€œ7â€, and â€œ7â€ appears first in 
this attention-position order. In fact, the approximate posterior 
image in the middle stage of the attention repetition was â€œ7â€. 
 In the early stages of attention repetition, the probability of 
a particular number in the approximate posterior distribution  
will not increase wherever the next attention is made.  The 
Current sensory input
image
Candidate attention
image at next time
(x, y)
Approximate posterior image
y
x
O
x
y
A
x
y
B
x
y
C
x
y
D
Figure 4 Uncertainty map when making attention at â€œ0â€. Each subfigure A, B, C, and D corresponds to 1st, 4th, 13th, and 
20th attentions, respectively. 
 
 
distribution of the uncertainty map is thus biased toward the 
minimum value of the uncertainty. In this case, the difference 
in the value included in the uncertainty map is mainly due to 
the random number included in the VAE calculation and 
considered to be in the situation equivalent to random selection 
of the next attention position. This is similar to a situation in 
which a human takes a random attention action to obtain clues 
in an unknown environment. 
 In the middle stage of the attention repetition, the number 
can be specified depending on the next  sensory input image 
determined with  attention position, so the distribution of the 
uncertainty map is more dispersed than that in the early stage 
of attention repetition. This is equivalent to a human taking 
actions to increase the certainty of a certain number when the 
likelihood of that number increases. 
 In the final stage  of attention repetition, most regions of 
numbers have already become visible in t he approximate 
posterior image; thus, the distribution of the uncertainty map is 
again biased toward the minimum-value side. This is a situation 
in which  the probability of a particular number in the 
approximate posterior dist ribution has already been high, and 
the entropy of this distribution is low no matter where the next 
attention position is. For this reason, the range was smaller than 
that of the uncertainty map in the early stage of attention 
repetition in which the distribution was similarly biased. 
 In the transition of attention images, the attention-position 
movement was along the line  of the character , but not a 
complete â€œone-strokeâ€. This is because the approximate 
posterior image is not necessarily the image with the correct 
number. A number different from the correct number is shown 
in the approximate posterior image from the early to  middle 
stages of attention repetition, and the uncertainty is calculated 
using the approximate posterior image. Even at the final stage 
of the attention repetition, due to the randomness of the VAE, 
a random movement can also be seen along the character line.  
 When making an attention  at â€œ3â€, different initial attention 
positions confirmed both cases in which  the probability of 
correct characters became the highest and that of different 
characters became the highest after a fixed number of attention 
repetitions. This result suggests that the proposed system has a 
confirmation bias similar to humans that depends on what is 
obtained from the environment and prior knowledge about the 
environment. The human confirmation bias has a negative 
impact on decision -making in various fields, from politics to 
science and education (Kappes, A., 2020). Whereas, the 
confirmation bias has the advantage of adaptability to unknown 
environments. General optimization problems require complete 
models of the environment, but lack of environmental 
information and time constraints cannot provide complete 
models in most practical cases. In these cases, humans take the 
next action  on the basis of the confirmation bias made of 
* Attention image is rectangular area (        )
extracted from MNIST dataset image.
Number of attention repetitions
111 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20
Figure 5 Transition of attention  images and sensory input 
images. Each subset of two rows corresponds to making 
attentions from â€œ0â€ to â€œ9â€. First row shows attention images* 
and second row shows sensory input images along with number 
of attention repetitions. 
ğ‘ ğ‘¥ğ‘¡|ğœ™ğ‘¥ğ‘¡
ğ‘¥ğ‘¡ ğ‘¥ğ‘¡
1st to 10th attentions 11th to 20th attentions
Number of attention repetitions
111 2 3 4 5 6 7 8 9 10 12 13 14 15 16 17 18 19 20
Figure 6 Attention images, sensory input images, and transition 
of approximate posterior distribution. Changes in initial 
attention position when making attention at â€œ3â€ provides 
different transition patterns. 
 
 
previous experiences. Taking the next action (exploring action) 
enables making decisions in practical time and obtaining the 
environmental information. We believe that our results will 
help solve the previously difficult problem of triggering action 
in an unknown environment. 
Conclusion 
We proposed  an embodied system based on  the free energy 
principle (FEP)  for sensorimotor visual  perception. We 
evaluated the proposed system  in a character-recognition task 
using the MNIST dataset. The proposed embodied system is 
configured by a body and memory. By limiting body and 
memory abilities and operating according to the FEP, the  
proposed system  triggers an action that moves an attention 
position and repeatedly performs perceptual and active 
inferences. In the evaluation, a s the number of repetitions 
increases, the attention area moves continuously, gradually 
reducing the uncertainty of the characters . Finally, th e 
probability of the correct character becomes the highest among 
the characters. It was thus confirmed that the embodied system 
greatly contributes to achiev ing sensorimotor visual 
perception. Moreover, c hanging the initial attention position 
provides a different final distribution, suggesting that the 
proposed system has a confirmation bias similar to humans. We 
believe that these results will help solve the difficult problem 
of triggering action in an unknown environment. 
 
References 
 
Adams, R. A., Shipp, S., and Friston, K. J. (2013). Predictions not 
commands: active inference in the motor system. Brain Structure and 
Function, 218(3):611â€“643. 
Bogacz, R. (2017). A tutorial on the free-energy framework for modelling 
perception and learning. Journal of Mathematical Psychology , 
76:198â€“211. 
Buckley, C. L., Kim, C. S., McGregor, S., and Seth, A. K. (2017). The free 
energy principle for action and perception: A mathematical review. 
Journal of Mathematical Psychology, 14:55â€“79. 
Cheng, G., Hyon, S., Morimoto, J., Ude, A., Colvin, G., Scroggin, W., and 
Jacobsen, S. C. (2006). CB: A Humanoid Research Platform for 
Exploring NeuroScience. 2006 6th IEEE -RAS International 
Conference on Humanoid Robots, pages 182â€“187. 
Fitzpatrick, P., Metta, G., Natale, L., Rao, S., and Sandini, G. (2003). 
Learning about objects through action - initial steps towards artificial 
cognition. 2003 IEEE International Conference on Robotics and 
Automation, pages 3140â€“3145. 
Friston, K. J. (2010). The free-energy principle: a unified brain theory? 
Nature Reviews Neuroscience, 11:127â€“138. 
Friston, K. (2011). Embodied inference: or "I think therefore I am, if I am 
what I think". The implications of embodiment: Cognition and 
communication, 89â€“125. 
Friston, K., FitzGerald, T., Rigoli, F., Schwar tenbeck, P., and Pezzulo, G. 
(2017). Active Inference: A Process Theory. Neural Computation, 
29(1):1â€“49.  
Friston, K. and Kiebel, S. (2009). Predictive coding under the free -energy 
principle Philosophical Transactions of the Royal Society B: 
Biological Sciences, 364(1521):1211â€“1221. 
Friston, K., Kilner, J., and Harrison, L. (2006). A free energy principle for 
the brain. Journal of Physiology-Paris, 100:70â€“87. 
Gallagher, S. and Allen, M. (2018). Active inference, enactivism and the 
hermeneutics of social cognition. Synthese, 195: 2627â€“2648. 
Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. 
(2015). DRAW: A Recurrent Neural Network For Image Generation. 
Proceedings of the 32nd International Conference on Machine 
Learning, pages 1462â€“1471. 
Houthooft, R., Chen, X., Chen, X., Duan, Y., Schulman, J., De Turck, F., 
and Abbeel, P. (2016). VIME: Variational Information Maximizing 
Exploration., Advances in neural information processing systems 29, 
pages 1109â€“1117. 
Ji, S., Parr, R., Li, H., Liao, X. , and Carin, L. (2007). Point -based policy 
iteration., Proceedings of the 22nd National Conference on Artificial 
Intelligence (AAAI), pages 1243â€“1249. 
Kappes, A., Harvey, A.H., Lohrenz, T., Montague, P.R., and Sharot, T. 
(2020). Confirmation bias in the ut ilization of othersâ€™ opinion 
strength. Nature Neuroscience, 23: 130â€“137. 
Kingma, D.P. and Welling, M. (2014). Auto-Encoding Variational Bayes., 
2nd International Conference on Learning Representations 
(ICLR2014.) 
Land, M. F. (2006). Eye movements and the control of actions in everyday 
life. Progress in Retinal and Eye Research, 25(3): 296â€“324. 
LeCun, Y. and Cortes, C.  (1998). MNIST handwritten digit database.  
http://yann.lecun.com/exdb/mnist/. Last accessed on 23/04/2020. 
Lotter, W., Kreiman , G., and Cox, D. (2017). Deep predictive coding 
networks for video prediction and unsupervised learning., 5th 
International Conference on Learning Representations (ICLR2017.) 
Mandelbaum, J. and Sloan, L. L. (1947). Peripheral Visual Acuity*: With 
Special Reference to Scotopic Illumination. American Journal of 
Ophthalmology, 30(5): 581â€“ 588. 
McGregor, S., Baltieri, M., and Buckley, C. L. (2015). A minimal active 
inference agent., arXiv preprint arXiv:1503.04187. 
Oh, J., Guo, X., Lee, H., Lewis, R., and Sing h, S. (2015). Action -
Conditional Video Prediction Using Deep Networks in Atari Games., 
Advances in neural information processing systems 28, pages 2863â€“
2871. 
Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. (2016). Pixel Recurrent 
Neural Networks., Proceedings of the 33rd International Conference 
on Machine Learning, pages 1747â€“1756. 
Oâ€™Regan, J. K. (2001). Experience is not something we feel but something 
we do: a principled way of explaining sensory phenomenology, with 
Change Blindness and other emp irical consequences.  
http://nivea.psycho.univ-paris5.fr/ASSChtml/Pacherie4.html. Last 
accessed on 23/04/2020. 
Oâ€™Regan, J. K. and NoÃ«, A. (2001). A sensorimotor account of vision and 
visual consciousness. The Behavioral and Brain Sciences , 24(5):939â€“73; 
discussion 973â€“1031. 
Pineau, J., Gordon, G., and Thrun, S. (2003). Point -based value iteration: 
An anytime algorithm for POMDPs., International Joint Conference 
on Artificial Intelligence (IJCAI) , pages 1025â€“1032. 
Rezende, D. J., Mohamed, S., and Wierstra , D. (2014). Stochastic 
Backpropagation and Approximate Inference in Deep Generative 
Models., Proceedings of the 31st International Conference on 
International Conference on Machine Learning (ICML2014) , pages 
1278â€“1286. 
Salimans, T., Karpathy, A., Chen, X. , and Kingma, D. P. (2017). 
PixelCNN++: Improving the PixelCNN with Discretized Logistic 
Mixture Likelihood and Other Modifications., 5th International 
Conference on Learning Representations (ICLR2017.) 
Seth, A. K. (2015). The Cybernetic Bayesian Brain: Fr om Interoceptive 
Inference to Sensorimotor Contingencies. Open MIND, 35. 
Seth, A. K., Suzuki, K., and Critchley, H. D. (2012). An interoceptive 
predictive coding model of conscious presence. Frontiers in 
psychology, 2:395. 
Silver, D. and Veness, J. (2010). Monte-Carlo planning in large POMDPs., 
Advances in neural information processing systems, pages 2164 â€“
2172. 
Szegedy, C., Liu, W., Jia, Y, Sermanet P., Reed, S., Anguelov, D, Erhan, 
D., Vanhoucke V., and Rabinovich, A. (2015). Going Deeper With 
Convolutions., The IEEE Conference on Computer Vision and 
Pattern Recognition (CVPR), pages 1â€“9. 
 