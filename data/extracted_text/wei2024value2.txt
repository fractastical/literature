arXiv:2408.06542v1  [cs.AI]  13 Aug 2024
VA L U E O F IN F O R M AT IO N A N D RE WA R D SP E C I FI CAT IO N I N
AC T I V E IN F E R E N C E A N D POMDP S
Ran W ei
VERSES Research Lab
ran.wei@verses.ai
ABSTRACT
Expected free energy (EFE) is a central quantity in active in ference which has recently gained pop-
ularity due to its intuitive decomposition of the expected v alue of control into a pragmatic and an
epistemic component. While numerous conjectures have been made to justify EFE as a decision
making objective function, the most widely accepted is stil l its intuitiveness and resemblance to
variational free energy in approximate Bayesian inference . In this work, we take a bottom up ap-
proach and ask: taking EFE as given, what’s the resulting agent’s optimalit y gap compared with a
reward-driven reinforcement learning (RL) agent, which is well understood? By casting EFE under
a particular class of belief MDP and using analysis tools fro m RL theory, we show that EFE approxi-
mates the Bayes optimal RL policy via information value. W e d iscuss the implications for objective
speciﬁcation of active inference agents.
1 Introduction
Active inference (Parr et al., 2022) is an agent modeling fra mework derived from the free energy principle, which
roughly states that all cognitive behavior of an agent can be described as minimizing free energy, an information
theoretic measure of the "ﬁt" between the environment and th e agent’s internal model thereof (Friston, 2010). In
recent years, active inference has seen increased populari ty in various ﬁelds including but not limited to cognitive
and neural science, machine learning, and robotics (Smith e t al., 2021; Mazzaglia et al., 2022; Lanillos et al., 2021).
One common application of active inference across these ﬁel ds is in modeling decision making behavior, often taking
place in partially observable Markov decision processes (P OMDP). This offers active inference as complementary, a
potential alternative to, or a possible generalization of o ptimal control and reinforcement learning (RL).
The central difference between active inference and RL is th at instead of choosing actions that maximize expected
reward or utility, active inference agents are mandated to m inimize expected free energy (EFE), which in its most
common form is written as (Da Costa et al., 2020):
EF E (a) = − EQ(o|a)[log ˜P (o)]  
Pragmatic value
− EQ(o|a)[KL[Q(s|o, a)||Q(s|a)]]  
Epistemic value
(1)
Here, a is a sequence of actions to be evaluated, Q(s|a) and Q(o|a) are the agent’s prediction of future states s and
observations o, Q(s|o, a) is the future updated beliefs about states given future obse rvations, ˜P (o) is a distribution
encoding the agent’s preferred observations and KL denotes Kullback-Leibler (KL) divergence, a measure of dis tance
between two distributions.
One can obtain an intuitive understanding of the EFE objecti ve by analyzing the two terms separately. The ﬁrst term is
the negative expected log likelihood of predicted future ob servations under the preference or target distribution, wh ich
is equivalent to the cross entropy between the predicted and preferred observation distributions. Minimizing this ter m
encourages the agent to take actions that lead to preferred o bservations. It is thus usually referred to as the "pragmati c
value" or "expected value". The second term is the expected K L divergence between the predicted future states and
updated beliefs about future states given future observati ons, which quantiﬁes the belief update amount. This term is
usually referred to as "epistemic value" or "expected infor mation gain" because it encourages the agent to take actions
that lead to a higher amount of belief update – an implicit res olution of uncertainty.
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
The intuitive addition of pragmatic and epistemic values ha s been taken as one of the major appeals of EFE. In some
sense, it puts both values under the "same currency" when eva luating the total value of actions (Friston et al., 2015).
This perspective has motivated prior work to interpret epis temic value as the "value of information" (Da Costa et al.,
2020), a term which has a similar connotation in economics (H oward, 1966). Indeed, experimental evaluations of
active inference agents have shown that the epistemic value term in EFE contributes to structured exploratory behavior ,
resolving uncertainty before attempting to obtain reward, often leading to higher coverage of the state space and
enhanced task performance (Millidge, 2020; Tschantz et al. , 2020; Engström et al., 2024). Such a behavior primitive
is especially important in challenging partially observab le task environments.
It appears, at a ﬁrst glance, that RL and optimal control miss the epistemic value term. However, it is widely known
that the Bayes optimal policy in POMDPs already trades off ex ploration and exploitation (Roy et al., 2005). This
makes intuitive sense because resolving uncertainty often leads to more downstream rewards, essentially by "opening
up" opportunities. Speciﬁcally, the Bayes optimal policy l everages the equivalence between POMDPs and a special
class of MDPs deﬁned on the reward and transition of beliefs c alled belief MDPs to characterize the expected value
(i.e., cumulative reward) following an action given the cur rent belief, from which an optimal policy can be constructed
as a mapping from beliefs to actions (Kaelbling et al., 1998) . These policies, as demonstrated by Bayes adaptive RL
and meta RL, also exhibit structured exploratory behavior ( Zintgraf et al., 2019; Duan et al., 2016). It thus begs the
question:
What is the relationship between the Bayes optimal RL policy and the active inference policy based on optimizing
EFE?
The main contribution of this paper is providing one answer t o the above question:
EFE approximates the Bayes optimal RL policy via epistemic v alue.
W e achieve this by ﬁrst establishing the equivalence betwee n the EFE objective and a different class of belief MDPs,
which allows us to deﬁne EFE-optimal policies rather than ac tion sequences (i.e., plans) to form direct comparisons
with RL policies. W e then examine the source of epistemic beh avior in POMDPs using a deﬁnition of the value of
information for POMDPs based on Howard’s information value theory (1966). In brief, the value of information is
the difference in the expected values between the Bayes opti mal policy and another "naive" policy which plans as if it
would not be able to update beliefs based on observations in t he future. When casting the latter policy also using belief
MDPs, we observe that it uses the same belief transition dyna mics as the EFE policy but it uses the same belief reward
as the Bayes optimal policy. Our key result is a regret bound s howing that the EFE objective closes the performance
gap between the naive policy and the Bayes optimal policy by a ugmenting the reward function of the former with
epistemic value. W e discuss the implications of our results for specifying active inference agents in practice.
Our work is complementary to prior work examining the relati onship between active inference and RL (Millidge et al.,
2020; W atson et al., 2020; Da Costa et al., 2023) and the effec t of epistemic value on agent behavior (Schwöbel et al.,
2018; Koudahl et al., 2021). However, instead of trying to de rive the EFE objective from ﬁrst principles, we take a
"bottom up" approach and analyze it against the well-known B ayes optimal policy. T o our knowledge, this is the ﬁrst
regret bound of active inference agents in reward seeking ta sks.
2 Background
In this section, we introduce notations for Markov decision process, partially observable Markov decision process, an d
the belief MDP view of POMDPs. W e then introduce active infer ence and the EFE objective.
2.1 Markov Decision Process
A discrete time inﬁnite-horizon discounted Markov decisio n process (MDP; Sutton and Barto, 2018) is deﬁned by a
tuple M = ( S, A, P, R, µ, γ ), where S is a set of states, A a set of actions, P : S × A → ∆( S) a state transition
probability distribution (also called transition dynamic s), R : S × A → R a reward function, µ : ∆( S) the initial state
distribution, and γ ∈ (0, 1) a discount factor. In this work, we consider planning as oppo sed to learning, where the
MDP tuple M is known to the agent rather than having to be estimated from s amples obtained by interacting with the
environment deﬁned by M. W e use π : S → ∆( A) to denote a time-homogeneous Markovian policy which maps a
state to a distribution over actions. Rolling out a policy in the environment for a ﬁnite number of time steps T induces
a sequence of states and actions τ = ( s0:T , a0:T ) (also known as a trajectory) which is distributed according to:
P (τ) =
T∏
t=0
P (st|st−1, at−1)π(at|st) , (2)
2
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
where P (so|s−1, a−1) = µ(s0). W e use ρπ
P (s, a) = E[∑ ∞
t=0 γt Pr(st = s, at = a)] to denote the state-action
occupancy measure of policy π in environment with dynamics P , where the expectation is taken w .r.t. the interaction
process (2) for T → ∞ . W e denote the normalized occupancy measure, also called th e marginal state distribution or
state marginal, as dπ
P (s, a) = (1 − γ)ρπ
P (s, a).
Solving a MDP refers to ﬁnding a policy π which maximizes the expected cumulative discounted reward in the envi-
ronment J(π) deﬁned as:
J(π) = E
[ ∞∑
t=0
γtR(st, at)
]
. (3)
The process of ﬁnding an optimal policy is sometimes referre d to as reinforcement learning and it is a well-known
result that there exists at least one time-homogeneous Mark ovian policy which is optimal w .r.t. (3) (Sutton and Barto,
2018). This signiﬁcantly simpliﬁes our analysis later comp ared to ﬁnite horizon un-discounted MDPs for which the
optimal policy is time-dependent. The quantity 1
1−γ has a similar notion to planning horizon, because it represe nts the
time step at which discounting is effectively zero. The opti mal policy π∗ is characterized by the Bellman optimality
equation:
Q(s, a) = R(s, a) + γEs′∼P (·|s,a )[V (s′)], V (s) = max
a
Q(s, a) , (4)
from which it can be obtained by taking the action which maxim izes the action value function Q for each state as
π∗(a|s) = δ(a − arg max˜a Q(s, ˜a)), where δ(a − b) is the dirac delta distribution which has probability 1 if a = b
and probability 0 elsewhere. The advantage function A(s, a) = Q(s, a) − V (s) ≤ 0 quantiﬁes the suboptimality of
an action. W e will omit the ∗ notation in most cases. When needed, we denote the value and a dvantage functions
associated with policy π and MDP M as Qπ
M , V π
M , Aπ
M .
2.2 Partially Observable Markov Decision Process
A discrete time inﬁnite-horizon discounted partially obse rvable MDP (POMDP; Kaelbling et al., 1998) is character-
ized by a tuple M = ( S, A, O, P, R, µ, γ ), where the newly introduced symbol O is a set of observations, and the
new transition dynamics P consists of the state transition probability distribution P (st+1|st, at) and an observation
emission distribution P (ot|st). In a POMDP environment, the agent only has access to observa tions emitted from the
environment state but not the state itself. It is thus genera lly not sufﬁcient to consider Markovian policies but polici es
that depend on the history of observation-action sequences , i.e., π(at|ht) where ht = ( o0:t, a0:t−1).
It is a well-known result that the Bayesian belief distribut ion bt = P (st|ht) is a sufﬁcient statistic for the interaction
history (Kaelbling et al., 1998). The history dependent val ue functions and policy in POMDP can thus be written in
terms of beliefs:
Q(b, a) =
∑
s
b(s)R(s, a) + γ
∑
o′
P (o′|b, a)V (b′(o′, a, b)), V (b) = max
a
Q(b, a) , (5)
where P (o′|b, a) = ∑
s,s ′ P (o′|s′)P (s′|s, a)b(s) and b′(o′, a, b) denotes the belief update function from prior b(s) to
the posterior:
b′(o′, a, b) := b′(s′|o′, a, b) = P (o′|s′) ∑
s P (s′|s, a)b(s)
∑
s′ P (o′|s′) ∑
s P (s′|s, a)b(s) . (6)
The optimal policy derived from the above value functions is sometimes referred to as the Bayes optimal policy (Duff,
2002).
The belief value functions in (5) imply a special class of MDP s known as belief MDPs (Kaelbling et al., 1998) where
the reward and dynamics are deﬁned on the belief state as:
R(b, a) =
∑
s
b(s)R(s, a), P (b′|b, a) = P (o′|b, a)δ(b′ − ˜b′(o′, a, b)) . (7)
The stochasticity in the belief dynamics is entirely due to t he stochasticity of the next observation; the belief updati ng
process itself is deterministic.
In this work, we generalize the notion of belief MDP to refer t o any MDP deﬁned on the space of beliefs. However,
not all belief MDPs could yield the optimal policies for some POMDPs.
3
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
2.3 Active Inference
Active inference is an application of the variational princ iple to perception and action, where intractable Bayesian
belief updates (i.e., (6)) are approximated by variational inference (Da Costa et al., 2020). At every time step t, vari-
ational inference searches for an approximate posterior Q(st) which maximizes the evidence lower bound of data
marginal log likelihood, or equivalently minimizes the var iational free energy F:
F(Q) = EQ(st)[log Q(st) − log P (ot, st)] , (8)
where P (ot, st) = P (ot|st)P (st). In the context of POMDPs, the prior is given by P (st) =∑
st−1 P (st|st−1, at−1)Q(st−1). It is well-known that the optimal variational approximati on under appropriately
chosen family of posterior distributions equals to the exac t posterior in (6) (Blei et al., 2017). W e will thus assume
appropriate choices of variational family and omit subopti mal belief updating in subsequent analyses.
Central to the current discussion is the policy selection ob jective functions used in active inference, which is its mai n
difference from classic POMDPs. In particular, active infe rence introduces an objective function called expected fre e
energy (EFE) which, given an initial belief Q0(s0) and a ﬁnite sequence of actions a0:T −1, is deﬁned as (Friston et al.,
2017):
EF E (a0:T −1, Q0) = EQ(o1:T ,s 1:T |a0:T −1)[log Q(s1:T |a0:T −1) − log ˜P (o1:T , s1:T )] , (9)
where Q(s1:T |a0:T −1) is deﬁned as the product of the marginal state distributions along the action sequence (we show
how this can be approximately obtained as a result of variati onal inference and discuss the implication of deﬁning this
instead as the joint distribution in the appendix, which also contains all deriv ations and proofs):
Q(s1:T |a1:T −1) =
T∏
t=1
Q(st|Qt−1, at−1) ,
Q(st|Qt−1, at−1) :=
∑
st−1
P (st|st−1, at−1)Q(st−1|Qt−2, at−2) ,
(10)
and Q(o1:T , s1:T |a1:T −1) = ∏ T
t=1 P (ot|st)Q(st|Qt−1, at−1). These distributions represent the agent’s posterior
predictive beliefs about states and observations in the fut ure. Notice (9) is different from (1), but it is used here
because it is more general (Champion et al., 2024).
The distribution ˜P (o1:T , s1:T ) is interpreted as a "preference" distribution under which p referred observations and
states have higher probabilities. While there are multiple ways to specify ˜P in the literature, we will focus on the most
popular speciﬁcation:
˜P (o0:T , s0:T ) =
T∏
t=0
˜P (ot) ˜P (st|ot) , (11)
where ˜P (st|ot) is an arbitrary distribution. This speciﬁcation allows us t o factorize EFE over time and construct the
following approximation:
EF E (a0:T −1, Q0) ≈
T∑
t=1
− EQ(ot|a0:T −1)[log ˜P (ot)]
 
Pragmatic value
− EQ(ot|a0:T −1)[KL[Q(st|ot, a0:T −1)||Q(st|a0:T −1)]]  
Epistemic value
, (12)
where KL denotes Kullback-Leiblier divergence, Q(ot|a0:T −1) = ∑
st P (ot|st)Q(st|Qt−1, at−1) is the posterior
predictive over observations, and Q(st|ot, a0:T −1) ∝ P (ot|st)Q(st|Qt−1, at−1) is the future posterior given posterior
predictive of future states as prior and future observation s. W e discuss this approximation and optimal choice of ˜P (s|o)
further in the appendix.
As preempted in the introduction, in (12), the ﬁrst term "pra gmatic value" scores the quality of predicted observa-
tions under the preferred distribution. The second term "ep istemic value" measures the distance between future prior
Q(st|a0:T −1) and posterior beliefs Q(st|ot, a0:T −1), which corresponds to the amount of expected "information g ain"
from future observations. The epistemic value term is an esp ecially salient difference between active inference and
classic POMDPs.
4
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
3 Unifying Active Inference and RL Under Belief MDPs
The use of EFE vs. reward and the search for action sequences ( i.e., plans) vs. policies are the main contentions
between active inference and RL. In this section, we show tha t active inference can be equally represented using
reward and policy in a special class of belief MDPs. The key is to show that the EFE objective can be characterized
using a recursive equation akin to the Bellman equation. Thi s can be achieved immediately by expressing the predictive
distribution at each step using the predictive distributio n at the previous step:
EF E (a0:T −1, Q0)
≈
T∑
t=1
−EQ(ot|a0:T −1)[log ˜P (ot)] − EQ(ot|a0:T −1)[KL[Q(st|ot, a0:T −1)||Q(st|a0:T −1)]]
=
T −1∑
t=0
−EQ(ot+1|Qt,a t)[log ˜P (ot+1)] − EQ(ot+1|Qt,a t)[KL[Q(st+1|ot+1, Qt, at)||Q(st+1|Qt, at)]]
= −EQ(o1|Q0,a 0)[log ˜P (o1)] − EQ(o1|Q0,a 0)[KL[Q(s1|o1, Q0, a0)||Q(s1|Q0, a0)]] + EF E (a1:T −1, Q1) .
(13)
The recursive equation implies a transition dynamics over t he state marginal Qt which only depends on the previous
state marginal Qt−1, i.e., the transition is Markovian. The per-time step EFE on ly depends on the current state marginal.
Using the equivalence between the optimal Qt and bt, we can write the reward and transition dynamics of the belie f
MDP implied by EFE as follows:
REF E (b, a) = EP (o′|b,a )[log ˜P (o′)] + EP (o′|b,a )[KL[b(s′|o′, b, a)||b(s′|b, a)]] (14a)
:= ˜R(b, a) + IG(b, a) , (14b)
P open(b′|b, a) = δ(b′ − b′(a, b)), where b′(a, b) := b′(s′|b, a) =
∑
s
P (s′|s, a)b(s) . (14c)
By constructing the above belief MDP , the search for optimal action sequences can be equally represented as the search
for optimal belief-action policies.
Proposition 3.1. (Active inference policy) The EFE achieved by the optimal ac tion sequence can be equivalently
achieved by a time-indexed belief-action policy π(at|bt).
Proof. The proof is due to the above belief MDP characterization. An alternative proof is given in the appendix.
These identities enable us to deﬁne inﬁnite-horizon discou nted belief MDPs using the EFE reward and dynamics in
(14) and restrict our search to time-homogeneous Markovian belief-action policies. A similar result was presented
recently by Malekzadeh and Plataniotis (2022). However, ra ther than focusing on policy optimization algorithms, our
goal here is to clarify the belief MDP implied by EFE.
However, notice a few differences between the EFE belief MDP and the Bayes optimal belief MDP . First, the belief
dynamics in (14c) does not contain observation o; rather it is the marginal prediction of the next state given the previous
belief. Such a belief dynamics has been referred to as open-loop in the literature (Flaspohler et al., 2020) in the sense
that it does not take into account the possibility of updatin g beliefs based on future observations, akin to open-loop
controls. In contrast to the POMDP belief dynamics in (7), th e open-loop belief dynamics is deterministic given a.
Second, the EFE reward function contains an information gai n term which corresponds to epistemic value. The ﬁrst
term pragmatic value is deﬁned as the expected log likelihoo d of the next observation. This does not introduce much
difference from the POMDP reward function because we can deﬁ ne the active inference preference distribution as a
Boltzmann distribution parameterized by a reward function ˜P (o) ∝ exp( ˜R(o)) and assume that ˜R(o) self-normalizes
so that the partition function equals 1. The resulting rewar d can still be written as a linear combination of state-actio n
reward:
˜R(b, a) = EP (o′|b,a )[log ˜P (o′)]
=
∑
s
b(s)
∑
s′
P (s′|s, a)
∑
o′
P (o′|s′) ˜R(o′)
=
∑
s
b(s) ˜R(s, a) .
(15)
5
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
The linearity and thus convexity of the state-action reward is an important property of POMDPs, because it implies the
optimal value function is also convex in the beliefs, which m eans that lower entropy or more certain beliefs generally
correspond to higher values (Kaelbling et al., 1998). The ad dition of information gain, however, makes the EFE reward
no longer convex. In this case, the agent may be driven to coll ect more information and "distracted" from accruing
task rewards.
Proposition 3.2. The EFE reward function as deﬁned in (14b) is concave in the be lief.
Proof sketch. Information gain can be rearranged as follows:
IG(b, a) = H[P (o′|b, a)] − EP (s′ |b,a )[H[P (o′|s′)]] (16)
where H denotes Shannon entropy and the second term is linear in the b elief. Since entropy is concave, the combined
reward function is also concave.
In summary, the EFE objective and the classic POMDP can be und erstood as two different belief MDPs with different
reward functions and different dynamics.
4 Analyzing Policies in MDPs
The belief MDP characterizations of both the EFE policy and t he Bayes optimal policy enable us to use MDP analysis
tools for POMDPs. The main analysis tools we use in this paper are recent extensions of the performance difference
lemma (Kakade and Langford, 2002) and simulation lemma (Kea rns and Singh, 2002) which are well-known results
in RL theory that quantify the performance difference betwe en different policies or the same policy in different en-
vironments. T o compare active inference with RL, we are inte rested in the setting where two policies are optimal
w .r.t. both different rewards and different dynamics, howe ver, the evaluation reward and dynamics are equivalent to
only one of the policies (here referred to as the expert polic y). The following lemma, which extends lemma 4.1 in
(V emula et al., 2023) to the setting of different rewards, gi ves the performance gap (also known as the regret) between
the two policies:
Lemma 4.1. (P erformance difference in mismatched MDPs) Let π and π′ be two policies which are optimal w .r .t.
two MDPs M and M′. The two MDPs share the same initial state distribution and d iscount factor but have different
rewards R, R′ and dynamics P, P ′. Denote ∆ R(s, a) = R′(s, a) − R(s, a). The performance difference between π
and π′ when both are evaluated in M is given by:
JM (π) − JM (π′)
= 1
(1 − γ)E(s,a )∼dπ
P
[
Aπ ′
M′ (s, a)
]

 
P olicy advantage under expert distribution
+ 1
(1 − γ)E(s,a )∼dπ ′
P
[
∆ R(s, a) + γ
(
Es′∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′∼P (·|s,a )[V π ′
M′ (s′′)]
)]
  
Reward-model advantage under own distribution
+ 1
(1 − γ)E(s,a )∼dπ
P
[
−∆ R(s, a) + γ
(
Es′′∼P (·|s,a )[V π ′
M′ (s′′)] − Es′∼P ′(·|s,a )[V π ′
M′ (s′)]
)]

 
Reward-model disadvantage under expert distribution
.
(17)
Lemma 4.1 decomposes the performance gap in MDP M between policy π (the expert) and π′ into three terms. The
ﬁrst term is the advantage value of π′ under the expert’s state-action marginal distribution. Th e second term is the
difference in reward between MDP M′ and M and the difference in the value V π ′
M′ of π′ in M′ due to the difference
in dynamics expected under the state-action marginal distr ibution of π′. This term quantiﬁes the "advantage" of being
evaluated in one MDP vs another. The last term is the opposite of reward-model advantage, i.e., disadvantage, expected
under the expert policy π’s state-action marginal distribution.
One can obtain an intuitive understanding of (17) by attempt ing to minimize the performance gap via optimizing
R′, P ′, given we require π′ to be the optimal policy w .r.t. some R′, P ′. First, it holds that when R′, P ′ are respectively
equal to R, P , the reward and model advantages are zeros, and the policy ad vantage is zero as a result. This means one
can read policy, reward, and model advantage as a measure of e rror from the expert MDP and policy. When such error
is nonzero, R′, P ′ are optimized to increase reward-model advantage under the expert distribution and decrease reward-
model advantage under the policy’s own distribution. This e ncourages π′ to choose actions that lead to state-actions
6
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
achieved by the expert policy, eventually matching expert d istribution and thus expert performance. This property has
been used to learn value-aware dynamics models to robustly i mitate expert behavior in ofﬂine inverse reinforcement
learning (W ei et al., 2023).
Using (17), we can obtain an upper bound on the performance ga p in terms of the policy advantage and reward and
model difference:
Lemma 4.2. F or the setting considered in lemma 4.1, let ǫπ ′ = E(s,a )∼dπ
P
[|Aπ ′
M′ (s, a)|], ǫR′ = E(s,a )∼dπ
P
[|∆ R(s, a)|],
ǫP ′ = E(s,a )∼dπ
P
[KL[P (·|s, a)||P ′(·|s, a)]], and R′
max = max s,a |R′(s, a)|. Let the two policies have bounded state-
action marginal density ratio dπ ′
P (s,a )
dπ
P (s,a ) ≤ C. The performance gap is bounded as:
JM (π) − JM (π′) ≤ 11 − γ ǫπ ′ + C + 1
1 − γ ǫR′ + (C + 1)γR′
max
(1 − γ)2
√2ǫP ′ (18)
Lemma 4.2 shows that the performance gap is linear (w .r.t. pl anning horizon) in the expected policy advantage and
reward difference and quadratic in the model difference. Th us, model difference is a main contributor to performance
difference if it has a similar magnitude to policy and reward differences. However, it should be noted that this bound
can be overly conservative (sometimes known as the worst-ca se bound; Ross et al., 2011) since it doesn’t consider the
possibility of reward advantage being cancelled out by mode l advantage.
5 V alue of Information in POMDPs
Given the primary difference between active inference and R L is the deﬁnition of epistemic value and open-loop belief
dynamics, we ask whether it could be seen as an approximation to the Bayes optimal policy, speciﬁcally the epistemic
aspect thereof? T o this end, we ﬁrst analyze the "value of inf ormation" in the Bayes optimal policy. W e then show that
epistemic value closes the gap to the Bayes optimal policy by making up for the loss of information value.
5.1 V alue of Information in Bayes Optimal RL Policy
It’s colloquially accepted that the Bayes optimal policy ch aracterized by the value functions in (5) optimally trades o ff
exploration and exploitation. However, it’s not immediate ly obvious what is being traded off, the comparison is made
against which alternative action or policy, and how large is the performance gap. In this paper, we adopt the view that
what’s being traded off is the value of information, which we try to quantify in an action or policy. In (Howard, 1966),
the value of information for a single step decision making pr oblem is deﬁned as the reward a decision maker is willing
to give away if they could have their uncertainty resolved (e .g., by a clairvoyant). Formally, the expected value of
perfect information (EVPI) is deﬁned as the difference betw een the expected value given perfect information (EV |PI)
and the expected value without perfect information (EV).
In the POMDP setting, the agent cannot in general obtain perf ect information about the hidden state, but an observation
that is usually correlated with the state. It turns out that t his corresponds to an extension of Howard’s deﬁnition in
the single step decision making setting called the value of i mperfect information (Raiffa and Schlaifer, 2000). For
consistency in notation, we will label it as the expected val ue of perfect observation (EVPO) and deﬁne it as:
EV P O = EV |P O − EV ,
EV = max
a
∑
s
b(s)R(s, a) ,
EV |P O =
∑
o
∑
s
P (o|s)b(s) max
a
R(b(s|o), a) .
(19)
Similar to EVPI, EVPO is non-negative because an optimal dec ision maker cannot gain information and do worse
(Howard, 1966).
Extending this deﬁnition for the multi-stage sequential de cision making setting, we have the following corollary of EV
and EV |PO for POMDPs:
EV : Qopen(b, a) =
∑
s
b(s)R(s, a) + γV open(b′(a, b)) , (20a)
EV |P O : Q(b, a) =
∑
s
b(s)R(s, a) + γ
∑
o′
P (o′|b, a)V (b′(o, a, b)) . (20b)
7
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
The deﬁnition is the same as that of Flaspohler et al. (2020), except here we introduce additional motivation and
justiﬁcation based on the framework of Howard (1966) and Rai ffa and Schlaifer (2000). It is clear that EV |PO is the
same as the Bayes optimal value function. Interestingly, EV uses the open-loop belief dynamics that we saw earlier in
EFE but it uses the same reward as Bayes optimal policy. W e thu s label its value functions as Qopen and V open. The
following proposition shows that EVPO in the POMDP setting i s also non-negative:
Proposition 5.1. Let Qopen(b, a), V open(b) and Q(b, a), V (b) denote the open and closed-loop value functions as
deﬁned in (20), it holds that:
Q(b, a) ≥ Qopen(b, a) and V (b) ≥ V open(b) for all b ∈ ∆( S) and a ∈ A . (21)
Intuitively, the closed-loop Bayes optimal policy is bette r because it can take actions that lead to future observation s
which upon update lead to lower entropy beliefs. Given the cl osed-loop value function is convex in the beliefs, lower
entropy beliefs generally have higher value. These actions are referred to as epistemic actions.
However, simply comparing open and closed-loop value funct ions doesn’t give us an adequate measure of the value
of information since in most realistic settings, agents are allowed to observe the environment and update their beliefs
despite using potentially suboptimal open-loop policies. W e thus consider this setting by deploying both policies in
a POMDP for which the closed-loop policy is optimal, and the o nly difference between the two policies is that the
open-loop policy will choose actions according to (20a) as i f it would not be able to observe the environment in the
future. From lemma 4.1 we know that the primary contributor t o the performance gap between the two policies is the
difference in their transition dynamics and the resulting m odel advantage. The following proposition characterizes t he
advantage of the closed-loop dynamics:
Proposition 5.2. Let Rmax = max s,a |R(s, a)|. The closed-loop model advantage is bounded as follows:
0 ≤ EP (b′|b,a )[V open(b′)] − EP open(b′′|b,a )[V open(b′′)] ≤ Rmax
1 − γ
√
2IG(b, a) . (22)
It shows that the advantage of closed-loop dynamics is prima rily due to information gain which scales linearly w .r.t.
the planning horizon.
5.2 Main Result: EFE Approximates Bayes Optimal RL Policy
The main insight of this work is that EFE closes the optimalit y gap between open and closed-loop policies by aug-
menting the reward of the open-loop policy with the epistemi c value term. Given the pragmatic value is linear in the
belief (15), we will use it as the shared reward between activ e inference and RL agents, i.e., R(s, a) = ˜R(s, a).
Proposition 5.2 shows that the advantage of closed-loop bel ief transition is proportional to the information gain pro-
vided by the next observation. While the agent cannot change either belief transition distributions, it can change its
reward to alter the reward-model advantage and the marginal distribution under which it is evaluated. An obvious
choice for the reward advantage is to set it to the informatio n gain in order to cancel with the information disadvantage
of open-loop belief dynamics. T o ensure the agent does not ge t distracted by gaining information and still focus on
task relevant behavior, we make the following assumption on preference distribution speciﬁcation:
Assumption 5.3. (Preference speciﬁcation) The preference distribution or reward is speciﬁed such that the gain in
pragmatic value after receiving a new observation is higher than the loss in epistemic value in expectation under the
Bayes optimal policy π in closed-loop belief dynamics P :
E(b,a )∼dπ
P
[ ∑
s
(b(s|o) − b(s)) R(s, a)
]
≥ E(b,a )∼dπ
P
[IG(b(s), a) − IG(b(s|o), a)] . (23)
This assumption also ensures that the advantage of closed-l oop belief dynamics under the EFE value function is non-
negative. In practice, since the Bayes optimal policy behav ior can be difﬁcult to know a priori, we can approximate
the above by setting a reward function such that the reward di fference is sufﬁciently high. In the appendix, we prove
that the advantage upper bound given this assumption is the s ame as that evaluated under the open-loop belief MDP in
proposition 5.2. T o facilitate the comparison between open -loop and EFE policy, we introduce two more assumptions:
Assumption 5.4. (P olicy behavior) W e make the following assumptions on the b ehavior of the evaluated policies:
1. The absolute advantage of the EFE policy πEF E expected under the Bayes optimal policy’s marginal
distribution is no worse than that of the open-loop policy πopen: ǫ˜π = E(b,a )∼dπ
P
[|Aπ open
P (b, a)|] ≥
E(b,a )∼dπ
P
[|Aπ EF E
P (b, a)|].
8
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
2. F or both the open-loop policy πopen and EFE policy πEF E , it always holds that IG(b, a) ≥ 2 for any b, a
sampled from either their own or the expert policy’s margina l distribution.
Note that both assumptions are conservative but they will en able us to focus the comparison of both policies on their
information seeking behavior. Assumption 1 is reasonable b ecause we expect the EFE policy to be more similar to the
expert than the open-loop policy given the information gain reward encourages information seeking behavior. This
enables us to remove policy advantage from the comparison. A ssumption 2 is partly numerically motivated because it
allows us to further upper bound the closed-loop model advan tage in proposition 5.2 via
√
2KL ≤ KL so that the IG
reward bonus in EFE can be directly compared with closed-loo p model advantage and subtracted from it. In practice,
many POMDP environments are much more benign in that partial observability, and thus the value of information,
decreases to zero in a small number of time steps (Liu et al., 2 022). In that case, the difference between open-loop,
EFE, and Bayes optimal policies become very small. Thus, the setting we consider is harder or more pessimistic.
The following theorem, which is the main result, gives the pe rformance gap of both policies compared to the Bayes
optimal policy:
Theorem 5.5. Let all policies be deployed in POMDP M and all are allowed to update their beliefs according to
b′(o′, a, b). Let ǫIG = E(b,a )∼dπ
P
[IG(b, a)] denotes the expected information gain under the Bayes optim al policy’s
belief-action marginal distribution and let the belief-ac tion marginal induced by both open-loop and EFE policies
have bounded density ratio with the Bayes optimal policy



d˜π
P (b,a )
dπ
P (b,a )



∞
≤ C. Under assumptions 5.3 and 5.4, the
performance gap of the open-loop and EFE policies from the op timal policy are bounded as:
JM (π) − JM (πopen) ≤ 1
1 − γ ǫ˜π + (C + 1)γRmax
(1 − γ)2 ǫIG ,
JM (π) − JM (πEF E ) ≤ 1
1 − γ ǫ˜π + (C + 1)γRmax
(1 − γ)2 ǫIG − C + 1
1 − γ ǫIG .
(24)
Theorem 5.5 shows that the performance gap of both policies a re linear (w .r.t. planning horizon) in the policy advantage
and quadratic in the information gain. However, the EFE poli cy improves over the open-loop policy with a linear
increase in information gain. As mentioned before, these bo unds are conservative estimates since the information
seeking propriety of the EFE policy could further reduce pol icy disadvantage and the IG bonus could further reduce
closed-loop model advantage.
6 Discussions
Our results highlight the nuanced relationship between act ive inference and the classic approach to POMDPs. In this
section, we provide a few complementary perspectives on rel ated POMDP approximation and extensions from the RL
literature and discuss objective speciﬁcation in active in ference informed by our results.
6.1 POMDP Approximation and Extensions
In the POMDP planning literature, there is a suite of approxi mation techniques to overcome the intractability of exact
belief update and value function representation. The simpl est ones are the maximum likelihood heuristic and QMDP
heuristic which ﬁrst compute the underlying MDP value funct ion and then obtain the belief value function using either
the most likely state under the current belief or a belief-we ighted average (Littman et al., 1995). These approximation s
leverage the fact that MDP value functions (in discrete spac e) are easy to compute, but they can be overly optimistic
since they implicitly assume the state in the next time step w ill be fully observed (Hauskrecht, 2000). As a result, the
agent does not take information gathering actions.
T o address this shortcoming, there is a special set of heuris tics dedicated to inducing information gathering actions
(Roy et al., 2005). These information gathering heuristics typically operate in a "dual-mode" fashion where exploita-
tion and exploration are arbitrated by some criterion. For e xample, in Cassandra et al. (1996), the exploitation mode
chooses actions based on the underlying MDP whereas the expl oration mode chooses actions to minimize belief en-
tropy in the next time step. These two modes are arbitrated by the entropy of the current belief. Complementary
to dual-mode execution, Flaspohler et al. (2020) propose to interleave open-loop with closed-loop belief dynamics
when the value of information is low to speed up value functio n computation. In doing so, these methods alleviate
the expensive belief updating operation during planning. W hile EFE resembles these heuristics and thus amenable to
efﬁciency gain, it introduces an information gain term in th e reward, which could be an expensive operation in itself
(Belghazi et al., 2018).
9
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Recently, there is a family of methods called information di rected sampling (IDS) which also introduces informa-
tion objectives primarily to improve Thompson sampling-ba sed algorithms in the context of multi-arm bandits and
Bayesian RL (Russo and V an Roy, 2018; Lu et al., 2023; Hao and L attimore, 2022; Chakraborty et al., 2023). These
problems can be seen as subsets of POMDPs where the only hidde n state is the unknown environment model parame-
ters (Doshi-V elez and Konidaris, 2016). Similar to our work , their analyses are also based on characterization of the
relationship between information gain and regret, but inst ead via a quantity called "information ratio". Furthermore ,
we consider planning with open-loop belief dynamics rather than Thompson sampling and our focus is on analyzing
EFE.
Beyond information gathering heuristics, there is a family of POMDP extensions called active sensing or ρ-POMDP
(Araya et al., 2010), where the reward function is directly d eﬁned on beliefs. These POMDPs are typically used to
model settings where the reduction in belief entropy is the p rimary goal, such as in the exploration of an area, and
a goal-related reward can be optionally added. Without loss of generality, we can deﬁne this reward as the one-step
ahead belief entropy:
RAS (b, a) = −EP (o′|b,a )[H[b′(s′|o′, b, a)]]
∝ EP (o′|b,a )[Eb′(s′|o′,b,a )[log b′(s′|o′, b, a) − log 1
|S| ]]
= EP (o′|b,a )[KL[b′(s′|o′, b, a)||˜b(s′)]] .
(25)
where ˜b(s′) = 1
|S| is a uniform prior belief. This shows that the active sensing objective can be written as a special type
of information gain that is evaluated against a uniform prio r belief, thus resembling the EFE objective. An attractive
property of this objective is that it is convex in the belief, and thus is the value function, which makes the agent
potentially less distracted by information gain when task r ewards are introduced. A further difference is that it uses
closed-loop belief dynamics which enables better optimiza tion of the information objective.
6.2 Objective Speciﬁcation in Active Inference
The objective functions in active inference have been subje ct to various interpretations since its inception in the lat e
2000’s and have only slowed down relatively recently (Gottw ald and Braun, 2020). The EFE objective, which ﬁrst
appeared in the literature as early as 2015 in (Friston et al. , 2015), was initially motivated by an intuitive argument th at
"free energy minimizing agents should choose actions to min imize (expected) free energy". However, far from being
heuristic, the EFE objective is rooted in the free energy pri nciple which adopts a physics and information geometric
perspective, rather than a decision theoretic perspective , on agent behavior (Friston et al., 2023b,a; Barp et al., 202 2),
in which case open-loop belief dynamics is the natural outco me. It should be mentioned, however, that the information
geometric derivation of EFE relies on a "precise agent" assu mption on the environment in which future actions and
observations are assumed to have matching entropy (Barp et a l., 2022; Da Costa et al., 2024). It remains open whether
this assumption is satisﬁed in real environments.
Recently, Friston et al. (2021) introduced a "sophisticate d" version of EFE as an improved planning objective for activ e
inference agents, where instead of evaluating EFE based on f uture state marginals, EFE is evaluated based on future
posterior beliefs Q(st|ot, a0:t−1). This means that the belief MDP underlying the sophisticate d EFE uses the closed-
loop belief dynamics rather than the open-loop belief dynam ics in the vanilla EFE, however, the information gain term
is still used in the reward function. This means that we can no longer view sophisticated EFE as an approximation to
the Bayes optimal policy. Rather, the combination of pragma tic value and closed-loop belief dynamics renders parts
of sophisticated EFE exactly equal to the Bayes optimal beli ef MDP , until the equivalence is "broken" again by the
additional information gain term. Does this mean the agent m ay be motivated to acquire too much information while
compromising task performance? A simple manipulation show s that if we deﬁne the preference distribution as the
exponentiated reward multiplied by a negative temperature parameter λ ˜P (o) ∝ exp(λ ˜R(o)), then the EFE reward
becomes proportional to a weighted combination of reward an d information gain:
˜R(s, a) ∝
∑
s′
P (s′|s, a)
∑
o′
P (o′|s′)λ ˜R(o′)
= λ ˜R(s, a) ,
REF E (b, a) ∝
∑
s
b(s) ˜R(s, a) + 1
λ IG(b, a) ,
(26)
where choosing a high λ → ∞ corresponds to purely optimizing reward. However, this doe s mean that when λ is not
sufﬁciently high, in which case the objective highly resemb les active sensing, the agent may be distracted. But whether
10
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
this will be the case depends on the actual environment. Thus , similar to assumption 5.3, achieving Bayes optimal
behavior requires setting the preference in such a way that t he cumulative reward outweighs cumulative information.
Another perspective on the EFE objective is that the agent pe rforms distribution matching as opposed to reward max-
imization (Da Costa et al., 2023) where the agent additional ly seeks out diverse states or observations. This can be
seen from a rearrangement of the pragmatic-epistemic decom position of the EFE objective:
REF E (b, a) = EP (o′|b,a )[log ˜P (o′)] + EP (o′|b,a )[KL[b′(s′|o′, b, a)||b′(s′|b, a)]]
= − KL[P (o′|b, a)|| ˜P (o′)]  
Risk
− EP (s′|b,a )[H[P (o′|s′)]]  
Ambiguity
. (27)
This is the well-known risk-ambiguity decomposition of EFE (Sajid et al., 2021), where the ﬁrst term "risk" measures
the KL divergence of the predicted observation distributio n from the preferred observation distribution and the secon d
term "ambiguity" measures the entropy of observations expe cted under predicted future states.
In the MDP setting, with closed-loop belief updating, the ob jective reduces to the following due to no ambiguity,
which is precisely the well-known distribution matching ob jective (Hafner et al., 2020):
REF E (s, a) = −KL[P (s′|s, a)|| ˜P (s′)] , (28)
This objective has been shown to enhance exploration and tes t-time adaptation in an RL setting (Lee et al., 2019).
Again, as shown in (Da Costa et al., 2023), distribution matc hing and reward maximization can be interpolated using
a temperature parameter on the state preference ˜P (s) ∝ exp(λ ˜R(s)):
REF E (s, a) = EP (s′|s,a )[log ˜P (s′)] + H[P (s′|s, a)]
∝ EP (s′|s,a )[ ˜R(s′)] + 1
λH[P (s′|s, a)] .
(29)
In this setting, the temperature parameter λ represents the allowed dispersion around the optimal behav ior (or path of
least action) speciﬁed by the ﬁrst expected reward term in (2 9). Alternatively, it can be interpreted as the tightness of
the (soft) constraint to abide by optimal behavior, followi ng the constrained maximum entropy view of the free energy
principle (Friston et al., 2023b).
Putting together these perspectives, it appears that the no tion of "Bayes optimal" in the spirit of active inference (in
closed-loop), as well as extensions of POMDPs, may not be res tricted to the usual sense of Bayesian decision theory
(i.e., maximizing utility; Howard 1966; Raiffa and Schlaif er 2000; Berger 2013); it may also apply to that of Bayesian
optimal design (i.e., maximizing information gain; Lindle y 1956; MacKay 1992) and principle of maximum caliber
(i.e., maximizing coverage; Jaynes 1980).
7 Conclusion
In this paper, we study the theoretical connection between a ctive inference and reinforcement learning and show that
the epistemic value in the EFE objective of active inference can be seen as an approximation to the Bayes optimal
RL policy in POMDPs, achieving a linear improvement in regre t compared to a naive policy which doesn’t take
into account the value of information. The results also sugg est that, from the perspective of RL, the speciﬁcation of
EFE needs to balance reward with information gain in the envi ronment, via an appropriate temperature parameter ( λ).
Conversely, from the perspective of active inference, an EF E minimizing agent will pursue a Bayes optimal RL policy,
under a suitable temperature parameter. This conclusion mi ght have been anticipated by one reading of the complete
class theorem (W ald, 1947; Brown, 1981); namely, for any pai r of reward function and choices, there exists some prior
beliefs that render the choices Bayes optimal, in a decision theoretic sense (Berger, 2013).
Acknowledgement
The author would like to thank Alex Kiefer, Axel Constant, Da vid Hyland, Karl Friston, Lance Da Costa, Peter W aade,
Ryan Singh, Sanjeev Namjoshi, and Shohei W akayama for helpf ul feedback.
References
A. Agarwal, N. Jiang, S. M. Kakade, and W . Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW
Seattle, Seattle, W A, USA, T ech. Rep , 32:96, 2019.
11
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
M. Araya, O. Buffet, V . Thomas, and F . Charpillet. A pomdp ext ension with belief-dependent rewards. Advances in
neural information processing systems , 23, 2010.
A. Barp, L. Da Costa, G. França, K. Friston, M. Girolami, M. I. Jordan, and G. A. Pavliotis. Geometric methods
for sampling, optimization, inference, and adaptive agent s. In Handbook of Statistics , volume 46, pages 21–78.
Elsevier, 2022.
M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y . Bengio , A. Courville, and D. Hjelm. Mutual information neural
estimation. In International conference on machine learning , pages 531–540. PMLR, 2018.
J. O. Berger. Statistical decision theory and Bayesian analysis . Springer Science & Business Media, 2013.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. V ariational i nference: A review for statisticians. Journal of the
American statistical Association , 112(518):859–877, 2017.
L. D. Brown. A complete class theorem for statistical proble ms with ﬁnite sample spaces. The Annals of Statistics ,
pages 1289–1300, 1981.
A. R. Cassandra, L. P . Kaelbling, and J. A. Kurien. Acting und er uncertainty: Discrete bayesian models for mobile-
robot navigation. In Proceedings of IEEE/RSJ International Conference on Intel ligent Robots and Systems. IROS’96 ,
volume 2, pages 963–972. IEEE, 1996.
S. Chakraborty, A. S. Bedi, A. Koppel, M. W ang, F . Huang, and D . Manocha. Steering: Stein information directed
exploration for model-based reinforcement learning. arXiv preprint arXiv:2301.12038 , 2023.
T . Champion, H. Bowman, D. Markovi ´c, and M. Grze ´s. Reframing the expected free energy: Four formulations an d a
uniﬁcation. arXiv preprint arXiv:2402.14460 , 2024.
L. Da Costa, T . Parr, N. Sajid, S. V eselic, V . Neacsu, and K. Fr iston. Active inference on discrete state-spaces: A
synthesis. Journal of Mathematical Psychology , 99:102447, 2020.
L. Da Costa, N. Sajid, T . Parr, K. Friston, and R. Smith. Rewar d maximization through discrete active inference.
Neural Computation , 35(5):807–852, 2023.
L. Da Costa, S. T enka, D. Zhao, and N. Sajid. Active inference as a model of agency. arXiv preprint arXiv:2401.12917 ,
2024.
F . Doshi-V elez and G. Konidaris. Hidden parameter markov de cision processes: A semiparametric regression approach
for discovering latent task parametrizations. In IJCAI: proceedings of the conference , volume 2016, page 1432. NIH
Public Access, 2016.
Y . Duan, J. Schulman, X. Chen, P . L. Bartlett, I. Sutskever, a nd P . Abbeel. rl2: Fast reinforcement learning via slow
reinforcement learning. arXiv preprint arXiv:1611.02779 , 2016.
M. O. Duff. Optimal Learning: Computational procedures for Bayes-ada ptive Markov decision processes . University
of Massachusetts Amherst, 2002.
J. Engström, R. W ei, A. D. McDonald, A. Garcia, M. O’Kelly, an d L. Johnson. Resolving uncertainty on the ﬂy:
modeling adaptive driving behavior as active inference. Frontiers in neurorobotics , 18:1341750, 2024.
G. Flaspohler, N. A. Roy, and J. W . Fisher III. Belief-depend ent macro-action discovery in pomdps using the value of
information. Advances in Neural Information Processing Systems , 33:11108–11118, 2020.
K. Friston. The free-energy principle: a uniﬁed brain theor y? Nature reviews neuroscience , 11(2):127–138, 2010.
K. Friston, F . Rigoli, D. Ognibene, C. Mathys, T . Fitzgerald , and G. Pezzulo. Active inference and epistemic value.
Cognitive neuroscience , 6(4):187–214, 2015.
K. Friston, T . FitzGerald, F . Rigoli, P . Schwartenbeck, and G. Pezzulo. Active inference: a process theory. Neural
computation, 29(1):1–49, 2017.
K. Friston, L. Da Costa, D. Hafner, C. Hesp, and T . Parr. Sophi sticated inference. Neural Computation , 33(3):713–763,
2021.
K. Friston, L. Da Costa, N. Sajid, C. Heins, K. Ueltzhöffer, G . A. Pavliotis, and T . Parr. The free energy principle
made simpler but not too simple. Physics Reports , 1024:1–29, 2023a.
K. Friston, L. Da Costa, D. A. Sakthivadivel, C. Heins, G. A. P avliotis, M. Ramstead, and T . Parr. Path integrals,
particular kinds, and strange things. Physics of Life Reviews , 2023b.
S. Gottwald and D. A. Braun. The two kinds of free energy and th e bayesian revolution. PLoS computational biology ,
16(12):e1008420, 2020.
D. Hafner, P . A. Ortega, J. Ba, T . Parr, K. Friston, and N. Hees s. Action and perception as divergence minimization.
arXiv preprint arXiv:2009.01791 , 2020.
12
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
B. Hao and T . Lattimore. Regret bounds for information-dire cted reinforcement learning. Advances in neural infor-
mation processing systems , 35:28575–28587, 2022.
M. Hauskrecht. V alue-function approximations for partial ly observable markov decision processes. Journal of artiﬁ-
cial intelligence research , 13:33–94, 2000.
R. A. Howard. Information value theory. IEEE T ransactions on systems science and cybernetics , 2(1):22–26, 1966.
E. T . Jaynes. The minimum entropy production principle. Annual Review of Physical Chemistry , 31(1):579–601,
1980.
L. P . Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains.
Artiﬁcial intelligence , 101(1-2):99–134, 1998.
S. Kakade and J. Langford. Approximately optimal approxima te reinforcement learning. In Proceedings of the
Nineteenth International Conference on Machine Learning , pages 267–274, 2002.
M. Kearns and S. Singh. Near-optimal reinforcement learnin g in polynomial time. Machine learning , 49:209–232,
2002.
M. T . Koudahl, W . M. Kouw , and B. de Vries. On epistemics in exp ected free energy for linear gaussian state space
models. Entropy, 23(12):1565, 2021.
P . Lanillos, C. Meo, C. Pezzato, A. A. Meera, M. Baioumy, W . Oh ata, A. Tschantz, B. Millidge, M. Wisse, C. L. Buck-
ley, et al. Active inference in robotics and artiﬁcial agent s: Survey and challenges. arXiv preprint arXiv:2112.01871 ,
2021.
L. Lee, B. Eysenbach, E. Parisotto, E. Xing, S. Levine, and R. Salakhutdinov. Efﬁcient exploration via state marginal
matching. arXiv preprint arXiv:1906.05274 , 2019.
D. V . Lindley. On a measure of the information provided by an e xperiment. The Annals of Mathematical Statistics , 27
(4):986–1005, 1956.
M. L. Littman, A. R. Cassandra, and L. P . Kaelbling. Learning policies for partially observable environments: Scaling
up. In Machine Learning Proceedings 1995 , pages 362–370. Elsevier, 1995.
Q. Liu, A. Chung, C. Szepesvári, and C. Jin. When is partially observable reinforcement learning not scary? In
Conference on Learning Theory , pages 5175–5220. PMLR, 2022.
X. Lu, B. V an Roy, V . Dwaracherla, M. Ibrahimi, I. Osband, Z. W en, et al. Reinforcement learning, bit by bit.
F oundations and T rends® in Machine Learning , 16(6):733–865, 2023.
D. J. MacKay. Information-based objective functions for ac tive data selection. Neural computation , 4(4):590–604,
1992.
P . Malekzadeh and K. N. Plataniotis. Active inference and re inforcement learning: A uniﬁed infer-ence on continuous
state and action spaces under partially observ-ability. arXiv preprint arXiv:2212.07946 , 2022.
P . Mazzaglia, T . V erbelen, O. Catal, and B. Dhoedt. The free e nergy principle for perception and action: A deep
learning perspective. Entropy, 24(2):301, 2022.
B. Millidge. Deep active inference as variational policy gr adients. Journal of Mathematical Psychology , 96:102348,
2020.
B. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley. On the relationship between active inference and control
as inference. In Active Inference: First International W orkshop, IW AI 2020 , Co-located with ECML/PKDD 2020,
Ghent, Belgium, September 14, 2020, Proceedings 1 , pages 3–11. Springer, 2020.
T . Parr, G. Pezzulo, and K. J. Friston. Active inference: the free energy principle in mind, brain, and behavior . MIT
Press, 2022.
H. Raiffa and R. Schlaifer. Applied statistical decision theory , volume 78. John Wiley & Sons, 2000.
S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation l earning and structured prediction to no-regret online
learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics , pages
627–635. JMLR W orkshop and Conference Proceedings, 2011.
N. Roy, G. Gordon, and S. Thrun. Finding approximate pomdp so lutions through belief compression. Journal of
artiﬁcial intelligence research , 23:1–40, 2005.
D. Russo and B. V an Roy. Learning to optimize via information -directed sampling. Operations Research , 66(1):
230–252, 2018.
N. Sajid, P . J. Ball, T . Parr, and K. J. Friston. Active infere nce: demystiﬁed and compared. Neural computation , 33
(3):674–712, 2021.
13
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
S. Schwöbel, S. Kiebel, and D. Markovi ´c. Active inference, belief propagation, and the bethe appr oximation. Neural
computation, 30(9):2530–2567, 2018.
R. Smith, P . Badcock, and K. J. Friston. Recent advances in th e application of predictive coding and active inference
models within clinical neuroscience. Psychiatry and Clinical Neurosciences , 75(1):3–13, 2021.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . MIT press, 2018.
J. T omczak and M. W elling. V ae with a vampprior. In International conference on artiﬁcial intelligence and st atistics,
pages 1214–1223. PMLR, 2018.
A. Tschantz, M. Baltieri, A. K. Seth, and C. L. Buckley. Scali ng active inference. In 2020 international joint confer-
ence on neural networks (ijcnn) , pages 1–8. IEEE, 2020.
A. V emula, Y . Song, A. Singh, D. Bagnell, and S. Choudhury. Th e virtues of laziness in model-based rl: A uniﬁed
objective and algorithms. In International Conference on Machine Learning , pages 34978–35005. PMLR, 2023.
A. W ald. An essentially complete class of admissible decisi on functions. The Annals of Mathematical Statistics , pages
549–555, 1947.
J. W atson, A. Imohiosen, and J. Peters. Active inference or c ontrol as inference? a unifying view . arXiv preprint
arXiv:2010.00262 , 2020.
R. W ei, S. Zeng, C. Li, A. Garcia, A. D. McDonald, and M. Hong. A bayesian approach to robust inverse reinforcement
learning. In Conference on Robot Learning , pages 2304–2322. PMLR, 2023.
J. Winn, C. M. Bishop, and T . Jaakkola. V ariational message p assing. Journal of Machine Learning Research , 6(4),
2005.
L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y . Gal, K. Hofma nn, and S. Whiteson. V aribad: A very good method for
bayes-adaptive deep rl via meta-learning. arXiv preprint arXiv:1910.08348 , 2019.
14
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
A Appendix
A.1 Proofs for Section 2.3
Derivation of Q(s1:T |a0:T −1) in (10) from variational inference W e aim to obtain a predictive distribution over
future states s1:T given an action sequence a0:T −1 using variational inference. T ypically, active inference assumes
a mean-ﬁeld factorization of the variational distribution Q(s1:T |a0:T −1) = ∏ T
t=1 Q(st|a0:T −1). Since there is no
observation and thus no likelihood term, the variational fr ee energy F can be written as:
F(Q) = EQ(s1:T |a0:T −1)[log Q(s1:T |a0:T −1) − log P (s1:T |a0:T −1)]
= EQ(s1:T |a0:T −1)
[ T∑
t=1
(log Q(st|a0:T −1) − log P (st|st−1, at−1))
]
=
T∑
t=1
EQ(st−1:t |a0:T −1)[log Q(st|a0:T −1) − log P (st|st−1, at−1)] .
(30)
From (Winn et al., 2005), we know the optimal variational dis tribution has the form:
Q(st|a0:T −1) ∝ exp(EQ(st−1 |a0:T −1)[log P (st|st−1, at−1)])
≈ exp(log EQ(st−1 |a0:T −1)[P (st|st−1, at−1)])
=
∑
st−1
P (st|st−1, at−1)Q(st−1|a0:T −1)
:= Q(st|Qt−1, at−1) .
(31)
which recovers the deﬁnition in (10). The approximation in t he second line is due to Jensen’s inequality and does
not signiﬁcantly affect our results, because we know from th e variational inference literature that the optimal vari-
ational distribution must be equal to that of exact inferenc e, which is given by the last line. This also matches the
implementation in Pymdp 1, which is one of the main software repositories for active in ference.
Active inference and QMDP It is crucial to have a precise deﬁnition of the distribution s Q(s0:T |a0:T −1) and
Q(o0:T , s0:T |a0:T −1). In the main text, we have speciﬁed these as the product of mar ginal distributions over states and
observations. Here, we brieﬂy study the consequences of deﬁ ning these as the joint distributions:
Q(s0:T |a0:T −1) = b(s0)
T∏
t=1
P (st|st−1, at−1) ,
Q(o0:T , s0:T |a0:T −1) = b(s0)P (o0|s0)
T∏
t=1
P (st|st−1, at−1)P (ot|st) .
(32)
W e start by factorizing the full EFE objective in (9) as:
EF E (a0:T −1)
= EQ(o1:T ,s 1:T |a0:T −1)[log Q(s1:T |a0:T −1) − log ˜P (o1:T , s1:T )]
= EQ(o1:T ,s 1:T |a0:T −1)
[ T∑
t=1
(
log P (st|st−1, at−1) − log ˜P (ot, st)
)]
= Eb(s0)P (s1|s0,a 0)P (o1|s1)
[
log P (s1|s0, a0) − log ˜P (o1, s1)
+ EQ(o2:T ,s 2:T |s0:1,a 1:T −1)
[ T∑
t=2
(
log P (st|st−1, at−1) − log ˜P (ot, st)
)] ]
= Eb(s0)P (s1|s0,a 0)P (o1|s1)
[
log P (s1|s0, a0) − log ˜P (o1, s1) + EF E (a1:T −1)
]
= Eb(s0)
[
EP (s1|s0,a 0)P (o1|s1)[log P (s1|s0, a0) − log ˜P (o1, s1)] + EP (s1|s0,a 0)[EF E (a1:T −1)]
]
.
(33)
1 https://github.com/infer-actively/pymdp
15
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
This allows us to write down a recursive equation:
Q(st, at) = EP (st+1 |st,a t)P (ot+1|st+1)[log P (st+1|st, at) − log ˜P (ot+1, st+1)]  
R(st,a t)
+EP (st+1|st,a t)[V (st+1)] ,
V (st) = max
a
Q(st, at) ,
(34)
and
EF E (a0:T −1) = Eb(s0)[Q(s0, a0)] . (35)
This corresponds to what’s known as the QMDP approximation i n the POMDP literature (Littman et al., 1995), which
is known to overestimate the value of a belief by planning und er the implicit assumption that future states are observed
(Hauskrecht, 2000).
EFE bound and choice of preference Despite being the most popular choice of EFE, the pragmatic- epistemic value
decomposition (12) is actually a bound on the full EFE deﬁned in (9). T o show this, let’s consider a single time step
since both formulations can be decomposed across time steps . Recall that the pragmatic-epistemic decomposition
assumes the following factorization of ˜P (o, s) = ˜P (o) ˜P (s|o). The full EFE can be written as:
EF Et(a0:T −1) = EQ(ot,s t|a0:T −1)[log Q(st|a0:T −1) − log ˜P (ot, st)]
= −EQ(ot|a0:T −1)[log ˜P (ot)] − EQ(ot,s t|a0:T −1)[log ˜P (st|ot)] + EQ(st |a0:T −1)[Q(st|a0:T −1)]
= −EQ(ot|a0:T −1)[log ˜P (ot)] + EQ(ot,s t|a0:T −1)[Q(st|ot, a0:T −1)] − EQ(ot,s t|a0:T −1)[log ˜P (st|ot)]
+ EQ(st|a0:T −1)[Q(st|a0:T −1)] − EQ(ot,s t|a0:T −1)[Q(st|ot, a0:T −1)]
= −EQ(ot|a0:T −1)[log ˜P (ot)] + EQ(ot|a0:T −1)KL[Q(st|ot, a0:T −1)|| ˜P (st|ot)]
− EQ(ot|a0:T −1)[KL[Q(st|ot, a0:T −1)||Q(st|a0:T −1)]]
≥ − EQ(ot|a0:T −1)[log ˜P (ot)] − EQ(ot|a0:T −1)[KL[Q(st|ot, a0:T −1)||Q(st|a0:T −1)]] .
(36)
Thus, to keep the bound tight, we could set ˜P (s|o) as:
˜P ∗(s|o) = arg min
˜P (s|o)
EQ(ot|a0:T −1)KL[Q(st|ot, a0:T −1)|| ˜P (st|ot)]
≈ arg min
˜P (s|o)
EQ(ot|a0:T −1)KL[ ˜P (st|ot)||Q(st|ot, a0:T −1)]
∝ exp
(
EQ(ot|a0:T −1)[log Q(st|ot, a0:T −1)]
)
,
(37)
where the approximation in the second line assumes the forwa rd and reverse KL divergences have similar solutions.
The result on the last line is sometimes referred to as the agg regate posterior (T omczak and W elling, 2018). However,
since the aggregate posterior depends on the action sequenc e evaluated, the tightest bound is achieved by an aggregate
posterior that updates during each EFE optimization step to ensure that the ﬁnal aggregate posterior is evaluated under
the optimal action sequence.
Proposition A.1. (Active inference policy; restate of proposition 3.1) The E FE achieved by the optimal action se-
quence can be equivalently achieved by a time-indexed belie f-action policy π(at|Qt).
Proof. While the proof in the main text is given by characterizing th e EFE objective as a belief MDP , we give an
alternative proof here based on Bellman optimality for the f ull EFE objective in (9) starting with the base case:
EF E (aT −1, QT −1) = EQ(oT ,s T |QT −1,a T −1)[log Q(sT |QT −1, aT −1) − log ˜P (oT , sT )] . (38)
It is easy to see that
min
aT −1
EF E (aT −1, QT −1) = max
π T −1
∑
aT −1
π(aT −1|QT −1)EF E (aT −1, QT −1) , (39)
where the optimal policy is π∗
T −1(aT −1|QT −1) = δ(aT −1 − arg min˜aT −1 EF E (˜aT −1, QT −1)).
Applying the identity recursively, we have:
min
π t
Eπ (at|Qt)[EF E (at, Qt)] = min
π t
Eπ (at|Qt)
{
EQ(ot+1,s t+1|Qt,a t)[log Q(st+1|Qt, at) − log ˜P (ot+1, st+1)] + Eπ ∗(at|Qt)[EF E (at+1, Qt+1)]
}
.
(40)
The optimal policy at each step can be obtained by π(at|Qt) = δ(at − arg min˜at EF E (˜at, Qt)).
16
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Proposition A.2. (Restate of proposition 3.2) The EFE reward function as deﬁn ed in (14b) is concave in the belief.
Proof. Recall the EFE reward is deﬁned as:
R(b, a) = EP (o′|b,a )[log ˜P (o′)] + EP (o′|b,a )[KL[b′(s′|o′, b, a)||b′(s′|b, a)]] . (41)
From (15) we know the ﬁrst term is linear in the belief b.
The second term can be written as:
EP (o′|b,a )[KL[b′(s′|o′, b, a)||b′(s′|b, a)]]
= EP (o′,s ′|b,a )[log b′(s′|o′, b, a) − log b′(s′|b, a)]
= EP (o′,s ′|b,a )[log b′(s′|b, a) + log P (o′|s′) − log P (o′|b, a) − log b′(s′|b, a)]
= EP (o′,s ′|b,a )[log P (o′|s′) − log P (o′|b, a)]
= H[P (o′|b, a)] − EP (s′|b,a )[H[P (o′|s′)]]
= −
∑
o′
P (o′|b, a) log P (o′|b, a) −
∑
s
b(s)
∑
s′
P (s′|s, a)H[P (o′|s′)] .
(42)
The second term above is a linear function of the belief.
Applying the deﬁnition of convexity to the negative of the ﬁr st term:
∑
o′
P (o′|λb + (1 − λ)b′, a) log P (o′|λb + (1 − λ)b′, a)
=
∑
o′
∑
s
P (o′|s, a) [λb(s) + (1 − λ)b′(s)] log
[ ∑
s
(λb(s)P (o′|s, a) + (1 − λ)b′(s)P (o′|s, a)))
]
=
∑
o′
[λP (o′|b, a) + (1 − λ)P (o′|b, a)] log λP (o′|b, a) + (1 − λ)P (o′|b′, a)
λ + (1 − λ)
≤
∑
o′
λP (o′|b, a) log P (o′|b, a) +
∑
o′
(1 − λ)P (o′|b′, a) log P (o′|b′, a) ,
(43)
where the last line uses the log sum inequality and shows the e quation is convex. Thus, the ﬁrst term is concave and
the EFE reward is concave in the belief.
A.2 Proofs for Section 4
Lemma A.3. (P erformance difference in mismatched MDPs; restate of lem ma 4.1) Let π and π′ be two policies which
are optimal w .r .t. two MDPs M and M′. The two MDPs share the same initial state distribution and d iscount factor
but have different rewards R, R′ and dynamics P, P ′. Denote ∆ R(s, a) = R′(s, a) − R(s, a). The performance
difference between π and π′ when both are evaluated in M is given by:
JM (π) − JM (π′)
= 1
(1 − γ)E(s,a )∼dπ
P
[
Aπ ′
M′ (s, a)
]

 
Advantage under expert distribution
+ 1
(1 − γ)E(s,a )∼dπ ′
P
[
∆ R(s, a) + γ
(
Es′∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′∼P (·|s,a )[V π ′
M′ (s′′)]
)]
  
Reward-model advantage under own distribution
+ 1
(1 − γ)E(s,a )∼dπ
P
[
−∆ R(s, a) + γ
(
Es′′∼P (·|s,a )[V π ′
M′ (s′′)] − Es′∼P ′(·|s,a )[V π ′
M′ (s′)]
)]

 
Reward-model disadvantage under expert distribution
.
(44)
Proof. Following (V emula et al., 2023), we expand the performance d ifference as:
JM (π) − JM (π′) = Eµ (s0)[V π
M (s0) − V π ′
M (s0)]
= Eµ (s0)[V π
M (s0) − V π ′
M′ (s0)] + Eµ (s0 )[V π ′
M′ (s0) − V π ′
M (s0)] .
(45)
17
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
The second term can be expanded as:
Eµ (s0)[V π ′
M′ (s0) − V π ′
M (s0)]
= Es0∼µ (·),a 0∼π ′(·|s0)[R′(s0, a0) + γEs1∼P ′(·|s0,a 0)[V π ′
M′ (s1)] − R(s0, a0) − γEs1∼P (·|s0,a 0)[V π ′
M (s1)]]
= Es0∼µ (·),a 0∼π ′(·|s0)[∆ R(s0, a0) + γEs1∼P ′(·|s0,a 0)[V π ′
M′ (s1)] − γEs1∼P (·|s0,a 0)[V π ′
M′ (s1)]
+ γEs1∼P (·|s0,a 0)[V π ′
M′ (s1)] − γEs1∼P (·|s0,a 0)[V π ′
M (s1)]]
= Es0∼µ (·),a 0∼π ′(·|s0)[∆ R(s0, a0) + γEs1∼P ′(·|s0,a 0)[V π ′
M′ (s1)] − γEs1∼P (·|s0,a 0)[V π ′
M′ (s1)]]
+ γEs0∼µ (·),a 0∼π ′(·|s0),s 1∼P (·|s0,a 0)[V π ′
M′ (s1) − V π ′
M (s1)  
term a
] ,
(46)
where ∆ R(s, a) = R′(s, a) − R(s, a).
Expanding term a, we arrive at a similar structure to the abov e:
term a = Ea1∼π ′(·|s1)[R′(s1, a1) + γEs2∼P ′(·|s1,a 1)[V π ′
M′ (s2)] − R(s1, a1) − γEs2∼P (·|s1,a 1)[V π ′
M (s2)]]
= Ea1∼π ′(·|s1)[∆ R(s1, a1) + γEs2∼P ′(·|s1,a 1)[V π ′
M′ (s2)] − γEs2∼P (·|s1,a 1)[V π ′
M′ (s2)]]
+ γEa1∼π ′(·|s1),s 2∼P (·|s1,a 1) [V π ′
M′ (s2) − V π ′
M (s2)]  
term a’
.
(47)
W e can thus unroll the last term iteratively and obtain:
Eµ (s0 )[V π ′
M′ (s0) − V π ′
M (s0)]
= E
[ ∞∑
t=0
γt
(
∆ R(st, at) + γEs′∼P ′(·|st,a t)[V π ′
M′ (s′)] − γEs′′∼P (·|st,a t)[V π ′
M′ (s′′)]
)]
= 1
(1 − γ)E(s,a )∼dπ ′
P
[
∆ R(s, a) + γ
(
Es′∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′ ∼P (·|s,a )[V π ′
M′ (s′′)]
)]
,
(48)
where the expectation in the second line is taken w .r.t. the s tochastic process induced by π′, P .
W e now expand the ﬁrst term in the performance difference:
Es0 ∼µ (·)[V π
M (s0) − V π ′
M′ (s0)]
=
(
Es0∼µ (·)[V π
M (s0) − Ea0∼π (·|s0)[Qπ ′
M′ (s0, a0)]]
)
+
(
Es0∼µ (·)[Ea0∼π (·|s0)[Qπ ′
M′ (s0, a0)] − V π ′
M′ (s0)]
)
=
(
Es0∼µ (·),a 0∼π (·|s0)[Qπ ′
M′ (s0, a0)] − V π ′
M′ (s0)]
)
+ Es0∼µ (·),a 0∼π (·|s0)[R(s0, a0) + γEs1∼P (·|s0,a 0)[V π
M (s1)]]
− Es0∼µ (·),a 0∼π (·|s0)[R′(s0, a0) + γEs1∼P ′(·|s0,a 0)[V π ′
M′ (s1)]]
= Es0∼µ (·),a 0∼π (·|s0)[Aπ ′
M′ (s0, a0)]
+ Es0∼µ (·),a 0∼π (·|s0)
[
− ∆ R(s0, a0) + γEs1∼P (·|s0,a 0)[V π
M (s1)] − γEs1∼P (·|s0,a 0)[V π ′
M′ (s1)]
+ γEs1∼P (·|s0,a 0)[V π ′
M′ (s1)] − γEs1∼P ′(·|s0,a 0)[V π ′
M′ (s1)]
]
= Es0∼µ (·),a 0∼π (·|s0)[Aπ ′
M′ (s0, a0)]
+ Es0∼µ (·),a 0∼π (·|s0)[−∆ R(s0, a0) + γEs1∼P (·|s0,a 0)[V π ′
M′ (s1)] − γEs1∼P ′(·|s0,a 0)[V π ′
M′ (s1)]]
+ γEs0∼µ (·),a 0∼π (·|s0),s 1∼P (·|s0,a 0)[V π
M (s1) − V π ′
M′ (s1)]  
term b
.
(49)
18
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Apply the same unrolling method to term b, we have:
Es0∼µ (·)[V π
M (s0) − V π ′
M′ (s0)]
= E
[ ∞∑
t=0
γtAπ ′
M′ (st, at)
]
+ E
[ ∞∑
t=0
γt
(
−∆ R(st, at) + γEs′′∼P (·|st,a t)[V π ′
M′ (s′′)] − γEs′∼P ′(·|st,a t)[V π ′
M′ (s′)]
)]
= 1
(1 − γ)E(s,a )∼dπ
P
[
Aπ ′
M′ (s, a)
]
+ 1(1 − γ)E(s,a )∼dπ
P
[
−∆ R(s, a) + γ
(
Es′′∼P (·|s,a )[V π ′
M′ (s′′)] − Es′∼P ′(·|s,a )[V π ′
M′ (s′)]
)]
,
(50)
where the expectations in the ﬁrst equality is again taken w . r.t. the stochastic process induced by π, P .
Putting together, we have:
JM (π) − JM (π′)
= 1(1 − γ)E(s,a )∼dπ
P
[
Aπ ′
M′ (s, a)
]
+ 1(1 − γ)E(s,a )∼dπ ′
P
[
∆ R(s, a) + γ
(
Es′∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′∼P (·|s,a )[V π ′
M′ (s′′)]
)]
+ 1
(1 − γ)E(s,a )∼dπ
P
[
−∆ R(s, a) + γ
(
Es′′∼P (·|s,a )[V π ′
M′ (s′′)] − Es′∼P ′(·|s,a )[V π ′
M′ (s′)]
)]
.
(51)
Proposition A.4. (Model advantage bound) Let V π
P (s) be the value function of policy π in dynamics P with reward
R(s, a) and let Rmax = max s,a |R(s, a)|. The absolute value of the advantage of dynamics P over P ′ is bounded by:
⏐
⏐EP (s′ |s,a )[V π
P (s′)] − EP ′(s′′|s,a )[V π
P (s′′)]
⏐
⏐ ≤ Rmax
1 − γ
√
2KL[P (·|s, a)||P ′(·|s, a)] . (52)
Proof. The proof is the same as lemma B.2. in (W ei et al., 2023).
⏐
⏐EP (s′|s,a )[V π
P (s′)] − EP ′(s′′|s,a )[V π
P (s′′)]
⏐
⏐
=
⏐
⏐
⏐
⏐
⏐
∑
s′
V π
P (s′) (P (s′|s, a) − P ′(s′|s, a))
⏐
⏐
⏐
⏐
⏐
(1)
≤
∑
s′
|V π
P (s′)| |P (s′|s, a) − P ′(s′|s, a)|
(2)
≤ ∥ V π
P (·)∥∞∥P (·|s, a) − P ′(·|s, a)∥1
(3)
≤ ∥ V π
P (·)∥∞
√
2KL[P (·|s, a)||P ′(·|s, a)] ,
(53)
where (1) uses Jensen’s inequality since the inner sum is a co nvex combination, (2) uses Holder’s inequality and (3)
uses Pinsker’s inequality. The coefﬁcient ∥V π
P (·)∥∞ ≤ Eπ,P [∑ ∞
t=0 γt maxs,a |R(s, a)|] = Rmax
(1−γ ) .
Putting together, we have:
⏐
⏐EP (s′ |s,a )[V π
P (s′)] − EP ′(s′′|s,a )[V π
P (s′′)]
⏐
⏐ ≤ Rmax
1 − γ
√
2KL[P (·|s, a)||P ′(·|s, a)] . (54)
Lemma A.5. (Restate of lemma 4.2) F or the setting considered in lemma 4. 1, let ǫπ ′ = E(s,a )∼dπ
P
[|Aπ ′
M′ (s, a)|],
ǫR′ = E(s,a )∼dπ
P
[|∆ R(s, a)|], ǫP ′ = E(s,a )∼dπ
P
[KL[P (·|s, a)||P ′(·|s, a)]], and R′
max = max s,a |R′(s, a)|. Let the
19
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
two policies have bounded state-action marginal density ra tio dπ ′
P (s,a )
dπ
P (s,a ) ≤ C. The performance gap is bounded as:
JM (π) − JM (π′) ≤ 11 − γ ǫπ ′ + C + 1
1 − γ ǫR′ + (C + 1)γR′
max
(1 − γ)2
√
2ǫP ′ . (55)
Proof. The absolute value of the performance gap can be written as:
|JM (π) − JM (π′)|
≤ 1
(1 − γ)E(s,a )∼dπ
P
[
|Aπ ′
M′ (s, a)|
]
+ 1(1 − γ)
⏐
⏐
⏐E(s,a )∼dπ
P
[−∆ R(s, a)]
⏐
⏐
⏐+ 1
(1 − γ)
⏐
⏐
⏐E(s,a )∼dπ ′
P
[∆ R(s, a)]
⏐
⏐
⏐
+ γ
(1 − γ)
⏐
⏐
⏐E(s,a )∼dπ
P
[
Es′′∼P (·|s,a )[V π ′
M′ (s′′)] − Es′∼P ′(·|s,a )[V π ′
M′ (s′)]
] ⏐
⏐
⏐
+ γ
(1 − γ)
⏐
⏐
⏐E(s,a )∼dπ ′
P
[
Es′ ∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′∼P (·|s,a )[V π ′
M′ (s′′)]
] ⏐
⏐
⏐
(56)
due to Jensen’s inequality.
Expanding the third term reward advantage on the right hand s ide:
⏐
⏐
⏐E(s,a )∼dπ ′
P
[∆ R(s, a)]
⏐
⏐
⏐=
⏐
⏐
⏐
⏐
⏐E(s,a )∼dπ
P
[
dπ ′
P (s, a)
dπ
P (s, a) ∆ R(s, a)
] ⏐
⏐
⏐
⏐
⏐
≤ E(s,a )∼dπ
P
[ ⏐
⏐
⏐
⏐
⏐
dπ ′
P (s, a)
dπ
P (s, a) ∆ R(s, a)
⏐
⏐
⏐
⏐
⏐
]
≤





dπ ′
P (s, a)
dπ
P (s, a)





∞
E(s,a )∼dπ
P
[|∆ R(s, a)|]
= CǫR′ .
(57)
For the second term we drop C from the above due to no distribution mismatch.
Applying proposition A.4 to the last term:
⏐
⏐
⏐E(s,a )∼dπ ′
P
[
Es′ ∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′∼P (·|s,a )[V π ′
M′ (s′′)]
] ⏐
⏐
⏐
=
⏐
⏐
⏐
⏐
⏐E(s,a )∼dπ
P
[
dπ ′
P (s, a)
dπ
P (s, a)
(
Es′∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′ ∼P (·|s,a )[V π ′
M′ (s′′)]
)] ⏐
⏐
⏐
⏐
⏐
≤ E(s,a )∼dπ
P
[ ⏐
⏐
⏐
⏐
⏐
dπ ′
P (s, a)
dπ
P (s, a)
(
Es′∼P ′(·|s,a )[V π ′
M′ (s′)] − Es′′ ∼P (·|s,a )[V π ′
M′ (s′′)]
)⏐
⏐
⏐
⏐
⏐
]
≤





dπ ′
P (s, a)
dπ
P (s, a)





∞
∥V π ′
M′ (·)∥∞E(s,a )∼dπ
P
[∥P (·|s, a) − P ′(·|s, a)∥1]
≤





dπ ′
P ′ (s, a)
dπ
P (s, a)





∞
∥V π ′
M′ (·)∥∞
√
2E(s,a )∼dπ
P
[KL[P (·|s, a)||P ′(·|s, a)]]
= CR′
max
1 − γ
√2ǫP ′ .
(58)
Again for the fourth term we drop C from the above due to no distribution mismatch.
Putting together and apply the fact that JM (π) ≥ JM (π′), we have:
JM (π) − JM (π′) ≤ 1
1 − γ ǫπ ′ + C + 1
1 − γ ǫR′ + (C + 1)γR′
max
(1 − γ)2
√2ǫP ′ . (59)
20
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
A.3 Proofs for Section 5.1
A.3.1 Helpful Identities
Proposition A.6. (Open-loop value function convexity) The open-loop value f unction as deﬁned in (20a) is piece-wise
linear and convex in the beliefs.
Proof. Recall the deﬁnition of the open-loop value function is:
Qopen(b, a) =
∑
s
b(s)R(s, a) + γV open(b′(a, b)) . (60)
Furthermore, it is a valid belief MDP given the deterministi c transition of the belief state deﬁned in (14c).
Although this is an inﬁnite horizon value function, due to th e contraction mapping property of Bellman equation
(Agarwal et al., 2019), it can be approximated arbitrarily c lose using a ﬁnite number of K iterations starting from the
base case Qopen
k=0 (b, a) = ∑
s b(s)R(s, a). It is clear the base case value function V open
k=0 (b) = max ˜a Qopen
k=0 (b, ˜a) is
piecewise linear and convex in b.
For iteration k ∈ { 1, ..., ∞}, we have:
Qopen
k+1 (b, a) =
∑
s
b(s)R(s, a) + γ max
a′
Qopen
k (b′(a, b), a′) . (61)
The belief update b′(a, b) = ∑
s P (s′|s, a)b(s) is linear and convex in b, making the second term piecewise linear and
convex. The ﬁrst term is also linear and convex. The combinat ion is thus piecewise linear and convex.
Proposition A.7. (EVPO non-negativity) Let the expected value of perfect obs ervation for a single stage decision
making problem with reward R(s, a), prior belief b(s) and marginal observation distribution P (o) = ∑
s P (o|s)b(s)
be deﬁned as:
EV P O = EV |P O − EV ,
EV = max
a
∑
s
b(s)R(s, a) ,
EV |P O =
∑
p
P (o) max
a
∑
s
b(s|o)R(s, a) .
(62)
It holds that EV P O ≥ 0.
Proof. W e wish to show:
∑
o
P (o) max
a
∑
s
b(s|o)R(s, a) ≥ max
a′
∑
s
b(s)R(s, a′) . (63)
Let use deﬁne a∗(o) = arg max a
∑
s b(s|o)R(s, a), and a∗ = arg max a
∑
s b(s)R(s, a) so that we can write the LHS
as ∑
o P (o) ∑
s b(s|o)R(s, a∗(o)) and the RHS as ∑
s b(s)R(s, a∗).
By deﬁnition, we have:
∑
s
b(s|o)R(s, a∗(o)) ≥
∑
s
b(s|o)R(s, a∗) , (64)
since a∗(o) is the optimal action taking into consideration of o.
Applying expectation over P (o) to the above inequality, we have:
∑
o
P (o)
∑
s
b(s|o)R(s, a∗(o)) ≥
∑
o
P (o)
∑
s
b(s|o)R(s, a∗)
=
∑
s
b(s)R(s, a∗) ,
(65)
which completes the proof.
21
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Proposition A.8. (EVPO upper bound) Let Rmax = max s,a |R(s, a)|. The expected value of perfect observation as
deﬁned in (62) is upper bounded as follows:
EV P O ≤ Rmax
√
2EP (o)[KL[b(s|o)||b(s)]] . (66)
Proof. Recall the deﬁnition of EVPO is:
EV P O = EP (o)[V (b(s|o))] − V (b(s))
= EP (o)
[
max
a(o)
∑
s
b(s|o)R(s, a(o))
]
− max
a
∑
s
b(s)R(s, a)
≤ EP (o)
[ ∑
s
b(s|o)R(s, a∗(o))
]
−
∑
s
b(s)R(s, a∗(o))
= EP (o)
[ ∑
s
R(s, a∗(o)) (b(s|o) − b(s))
]
,
(67)
where we have used a∗(o) = arg max a(o)
∑
s b(s|o)R(s, a(o)) and the inequality is due to a∗(o) being suboptimal
for the second term.
T aking the absolute value of the above EVPO bound, we have:
|EV P O | =
⏐
⏐
⏐
⏐
⏐EP (o)
[ ∑
s
R(s, a∗(o)) (b(s|o) − b(s))
] ⏐
⏐
⏐
⏐
⏐
(1)
≤ EP (o)
[ ⏐
⏐
⏐
⏐
⏐
∑
s
R(s, a∗(o)) (b(s|o) − b(s))
⏐
⏐
⏐
⏐
⏐
]
(2)
≤ EP (o)
[ ∑
s
|R(s, a∗(o))| |b(s|o) − b(s)|
]
(3)
≤ ∥ R(·, ·)∥∞EP (o) [∥b(s|o) − b(s)∥1]
(4)
≤ Rmax
√
2EP (o)[KL[b(s|o)||b(s)]]
(68)
where (1) and (2) are due to Jensen’s inequality, (3) is due to Holder’s inequality, and (4) is due to Pinsker’s inequality .
A.3.2 Main Results of Section 5.1
Proposition A.9. (EVPO-POMDP non-negativity; restate of proposition 5.1) L et Qopen(b, a), V open(b) and
Q(b, a), V (b) denote the open and closed-loop value functions as deﬁned in (20), it holds that:
Q(b, a) ≥ Qopen(b, a) and V (b) ≥ V open(b) for all b ∈ ∆( S) and a ∈ A . (69)
Proof. Recall the open and closed-loop value functions are deﬁned a s:
Qopen(b, a) =
∑
s
b(s)R(s, a) + γV open(b′(a, b)), V open(b) = max
a
Qopen(b, a) ,
Q(b, a) =
∑
s
b(s)R(s, a) + γ
∑
o′
P (o′|b, a)V (b′(o′, a, b)), V (b) = max
a
Q(b, a) .
(70)
Although these are inﬁnite horizon value functions, again d ue to their contraction mapping property (Agarwal et al.,
2019), they can be approximated arbitrarily close using a ﬁn ite number of K iterations starting from the base case
Qk=0(b, a) = ∑
s b(s)R(s, a).
Starting with k = 1 , we have:
Qopen
1 (b, a) =
∑
s
b(s)R(s, a) + γV open
0 (b′(a, b)), V open
0 (b) = max
a
∑
s
b(s)R(s, a) ,
Q1(b, a) =
∑
s
b(s)R(s, a) + γ
∑
o′
P (o′|b, a)V0(b′(o′, a, b)), V 0(b) = max
a
∑
s
b(s)R(s, a) .
(71)
22
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
T aking the difference between the two value functions and mu ltiply by 1
γ , we have:
1
γ [Q1(b, a) − Qopen
1 (b, a)]
=
∑
o′
P (o′|b, a)V0(b′(o′, a, b)) − V open
0 (b′(a, b))
=
∑
o′
P (o′|b, a) max
aclose
∑
s
b′(s′|o′, a, b)R(s′, aclose) − max
aopen
∑
s
b′(s′|a, b)R(s′, aopen)
= EV P O ≥ 0 ,
(72)
where the second to last line equals EVPO in proposition A.7 u nder prior belief b′(s′|b, a) for all b ∈ ∆( s), a ∈ A .
Thus it must be non-negative.
Applying the above to the value functions at k = 1 , we have:
V1(b) − V open
1 (b) = max
aclose
Q1(b, aclose) − max
aopen Qopen
1 (b, aopen)
≥ Q1(b, aopen∗) − Qopen
1 (b, aopen∗)
≥ 0 ,
(73)
where we have deﬁned aopen∗ = arg max aopen Qopen
1 (b, aopen).
Now consider k = 2 , where
Qopen
2 (b, a) =
∑
s
b(s)R(s, a) + γV open
1 (b′(a, b)), V open
1 (b) = max
a
Qopen
1 (s, a) ,
Q1(b, a) =
∑
s
b(s)R(s, a) + γ
∑
o′
P (o′|b, a)V1(b′(o′, a, b)), V 1(b) = max
a
Q1(s, a) .
(74)
T aking the difference between the two value functions again , we have:
1
γ [Q2(b, a) − Qopen
2 (b, a)]
=
∑
o′
P (o′|b, a)V1(b′(o, a, b)) − V open
1 (b′(a, b))
=
∑
o′
P (o′|b, a) max
a′close
{ ∑
s
b′(s|o′, a, b)R(s, a
′close) +
∑
o′′
P (o′′|b′, a
′close)V0(b′′(o′′, a
′close, b′))
}
− max
a′open
{ ∑
s
b′(s|a, b)R(s, a
′open) + V open
0 (b′′(a
′open, b′))
}
.
(75)
23
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Let a
′open∗ = arg max a′open
{ ∑
s b′(s|a, b)R(s, a
′open) + V open
0 (b′′(a
′open, b′))
}
and a
′close∗ =
arg maxa′close
{ ∑
s b′(s|o′, a, b)R(s, a
′close) + ∑
o′′ P (o′′|b′, a
′close)V0(b′′(o′′, a
′close, b′))
}
, we have:
∑
o′
P (o′|b, a) max
a′close
{ ∑
s
b′(s|o′, a, b)R(s, a
′close) +
∑
o′′
P (o′′|b′, a
′close)V0(b′′(o′′, a
′close, b′))
}
− max
a′open
{ ∑
s
b′(s|a, b)R(s, a
′open) + V open
0 (b′′(a
′open, b′))
}
≥
∑
o′
P (o′|b, a) max
a′close
{ ∑
s
b′(s|o′, a, b)R(s, a
′close) +
∑
o′′
P (o′′|b′, a
′open∗)V0(b′′(o′′, a
′open∗, b′))
}
−
{ ∑
s
b′(s|a, b)R(s, a
′open∗) + V open
0 (b′′(a
′open∗, b′))
}
=
∑
o′
P (o′|b, a)
{
max
a′close
∑
s
b′(s|o′, a, b)R(s, a
′close) −
∑
s
b′(s|a, b)R(s, a
′open∗)
}
  
EV P O ≥0
+
∑
o′
P (o′|b, a)
{ ∑
o′′
P (o′′|b′, a
′open∗)V0(b′′(o′′, a
′open∗, b′)) − V open
0 (b′′(a
′open∗, b′))
}
  
≥0 due to (72)
≥ 0 .
(76)
Applying the above to k ∈ { 1, ..., ∞} recursively, we have:
Q(b, a) ≥ Qopen(b, a) and V (b) ≥ V open(b) . (77)
Proposition A.10. (Closed-loop model advantage upper bound) Let Rmax = max s,a |R(s, a)|. The closed-loop
model advantage is upper bounded as follows:
EP (b′|b,a )[V open(b′)] − EP open(b′′|b,a )[V open(b′′)] ≤ Rmax
1 − γ
√
2IG(b, a) . (78)
Proof. Recall the closed-loop model advantage is deﬁned as:
EP (b′|b,a )[V (b′)] − EP open(b′′|b,a )[V (b′′)] = EP (o′|b,a )[V (b′(s′|o′, b, a))] − V (b′(s′)) (79)
T o simplify notation, we will drop the conditioning on b, a in the expectation. This also enables us to remove the " ′"
notation.
W e will use a similar method as before where we leverage the co ntraction mapping property of the value function and
start from the base case. It is clear for the base case k = 0 where V (b) = max a
∑
s b(s)R(s, a), the model advantage
is EVPO and thus the upper bound from proposition A.8 applies . T o simplify notation, let’s denote the upper bound
as C(b) since b(s|o) can be calculated from b(s)
24
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
W e now consider k = 1 :
EP (o)[V1(b(s|o))] − V1(b(s))
= EP (o)
[
max
aclose
∑
s
b(s|o)R(s, aclose) + γV0(b′(aclose, b(s|o)))
]
−
[
max
aopen
∑
s
b(s)R(s, aopen) + γV0(b′(aopen, b(s)))
]
≤ EP (o)
[ ∑
s
b(s|o)R(s, aclose∗) + γV0(b′(aclose∗, b(s|o)))
]
−
[ ∑
s
b(s)R(s, aclose∗) + γV0(b′(aclose∗, b(s)))
]
= EP (o)
[ ∑
s
b(s|o)R(s, aclose∗) −
∑
s
b(s)R(s, aclose∗)
]
  
term a
+ γ EP (o)
[
V0(b′(aclose∗, b(s|o))) − V0(b′(aclose∗, b(s)))
]
  
term b
.
(80)
T erm a is the same as the one in EVPO, thus the upper bound C(b) applies again. In term b, recall the open-loop belief
updates are deﬁned as:
b′(a, b(s|o)) =
∑
s
P (s′|s, a)b(s|o) := b′(s′|o) ,
b′(a, b(s)) =
∑
s
P (s′|s, a)b(s) := b′(s′) .
(81)
Due to the convexity of the value functions, we have term b ≥ 0. Furthermore, term b corresponds to EVPO for stage
0 with modiﬁed belief updates as deﬁned above. Thus C(b′) applies again.
Combining both, we have:
EP (o)[V1(b(s|o))] − V1(b(s))
≤ Rmax
√
2EP (o)[KL[b(s|o)||b(s)]] + γRmax
√
2EP (o)[KL[b′(s′|o, aclose∗)||b′(s′)]]
≤ Rmax
√
2EP (o)[KL[b(s|o)||b(s)]] + γRmax
√
2EP (o)[KL[b(s|o)||b(s)]] ,
(82)
where the second inequality is due to data processing inequa lity.
Applying the above to k ∈ { 2, ..., ∞} recursively, we have:
EP (o′|b,a )[V (b′(s′|o′))] − V (b′(s′)) ≤ Rmax
∞∑
t=0
γt
√
2EP (o′|b,a )[KL[b′(s′|o′)||b′(s′)]]
= Rmax
1 − γ
√
2EP (o′|b,a )[KL[b′(s′|o′)||b′(s′)]] .
(83)
A.4 Proofs for Section 5.2
Proposition A.11. (EFE EVPO upper bound) Let Rmax = max s,a |R(s, a)|. The expected value of perfect observa-
tion as deﬁned in (62) is upper bounded as follows:
EV P O EF E ≤ ˜Rmax
√
2EP (o)[KL[b(s|o)||b(s)]] . (84)
25
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Proof. Recall the one-step EFE belief reward is:
R(b, a) =
∑
s
b(s)R(s, a) + IG(b, a) , (85)
where the reward is deﬁned as R(s, a) := ˜R(s, a) in (14b) and IG(b, a) is the information gain.
W e can thus write EVPO as:
EV P O
= EP (o)
[
max
a(o)
∑
s
b(s|o)R(s, a(o)) + IG(b(s|o), a(o))
]
− max
a
[ ∑
s
b(s)R(s, a) + IG(b(s), a)
]
≤ EP (o)
[ ∑
s
b(s|o)R(s, a∗(o)) + IG(b(s|o), a∗(o))
]
−
[ ∑
s
b(s)R(s, a∗(o)) + IG(b(s), a∗(o))
]
= EP (o)
[ ∑
s
R(s, a∗(o)) (b(s|o) − b(s))
]
+ EP (o)[IG(b(s|o), a∗(o)) − IG(b(s), a∗(o))]  
≤0
≤ EP (o)
[ ∑
s
R(s, a∗(o)) (b(s|o) − b(s))
]
,
(86)
where we have used a∗(o) = arg max a(o)
∑
s b(s|o)R(s, a(o)) and the last inequality is due to IG being a concave
function of beliefs. The remaining term is the same as the one in proposition A.8. Thus, applying the result from
proposition A.8 we complete the proof.
Proposition A.12. (EFE closed-loop model advantage upper bound) Let Rmax = max s,a |R(s, a)|. The closed-loop
model advantage under the EFE value function is upper bounde d as follows:
EP (b′|b,a )[V EF E (b′)] − EP open(b′′|b,a )[V EF E (b′′)] ≤ Rmax
1 − γ
√
2IG(b, a) (87)
Proof. Similar to the proof to proposition A.10, we start with the ba se case which is covered by proposition A.11. T o
simplify notation, we drop the EFE superscript with the unde rstanding that V EF E is the value function under the EFE
belief MDP .
26
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Starting with k = 1 , we have:
EP (o)[V1(b(s|o))] − V1(b(s))
= EP (o)
[
max
aclose
∑
s
b(s|o)R(s, aclose) + IG(b(s|o), aclose) + γV0(b′(aclose, b(s|o)))
]
−
[
max
aopen
∑
s
b(s)R(s, aopen) + IG(b(s), aopen) + γV0(b′(aopen, b(s)))
]
≤ EP (o)
[ ∑
s
b(s|o)R(s, aclose∗) + IG(b(s|o), aclose∗) + γV0(b′(aclose∗, b(s|o)))
]
−
[ ∑
s
b(s)R(s, aclose∗) + IG(b(s), aclose∗) + γV0(b′(aclose∗, b(s)))
]
= EP (o)
[ ∑
s
b(s|o)R(s, aclose∗) −
∑
s
b(s)R(s, aclose∗)
]
+ EP (o)[IG(b(s|o), aclose∗) − IG(b(s), aclose∗)]  
≤0
+ γEP (o)
[
V0(b′(aclose∗, b(s|o))) − V0(b′(aclose∗, b(s)))
]
≤ EP (o)
[ ∑
s
b(s|o)R(s, aclose∗) −
∑
s
b(s)R(s, aclose∗)
]
  
term a
+ γ EP (o)
[
V0(b′(aclose∗, b(s|o))) − V0(b′(aclose∗, b(s)))
]
  
term b
(88)
W e arrive at the same form as proposition A.10. While we canno t guarantee term b > 0, the same upper bound holds.
The next remark ensures the expected closed-loop model adva ntage under the EFE reward is non-negative, which
provides the motivation for assumption 5.3.
Finally, applying the above recursively to k ∈ { 2, ..., ∞}, we complete the proof.
Remark A.13. (Motivation for assumption 5.3) T o ensure the EFE model adva ntage expected under the Bayes optimal
policy π is non-negative, we need to set the reward such that:
E(b,a )∼dπ
P
[ ∑
s
(b(s|o) − b(s)) R(s, a)
]
≥ E(b,a )∼dπ
P
[IG(b(s), a) − IG(b(s|o), a)] , (89)
where dπ
P is the marginal distribution induced by the Bayes optimal po licy in the closed-loop belief dynamics.
Theorem A.14. (Open-loop and EFE policy performance gaps; restate of theo rem 5.5) Let all policies be deployed
in POMDP M and all are allowed to update their beliefs according to b′(o′, a, b). Let ǫIG = E(b,a )∼dπ
P
[IG(b, a)]
denotes the expected information gain under the Bayes optim al policy’s belief-action marginal distribution and let th e
belief-action marginal induced by both open-loop and EFE po licies have bounded density ratio with the Bayes optimal
policy



d˜π
P (b,a )
dπ
P (b,a )



∞
≤ C. Under assumptions 5.3 and 5.4, the performance gap of the op en-loop and EFE policies
from the optimal policy are bounded as:
JM (π) − JM (πopen) ≤ 1
1 − γ ǫ˜π + (C + 1)γRmax
(1 − γ)2 ǫIG ,
JM (π) − JM (πEF E ) ≤ 1
1 − γ ǫ˜π + (C + 1)γRmax
(1 − γ)2 ǫIG − C + 1
1 − γ ǫIG .
(90)
27
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
Proof. Let us start by bounding the absolute value of the EFE policy’ s performance gap:
|JM (π) − JM (πEF E )|
≤
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[Aπ EF E
MEF E (b, a)]
⏐
⏐
⏐
⏐
+
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[
−IG(b, a) + γ
(
Eb′′∼P (·|b,a )[V π EF E
MEF E (b′′)] − Eb′∼P open(·|b,a )[V π EF E
MEF E (b′)]
)] ⏐
⏐
⏐
⏐
+
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ EF E
P
[
IG(b, a) + γ
(
Eb′′∼P open(·|b,a )[V π EF E
MEF E (b′′)] − Eb′∼P (·|b,a )[V π EF E
MEF E (b′)]
)] ⏐
⏐
⏐
⏐ .
(91)
Examining the second term, we have:⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[
−IG(b, a) + γ
(
Eb′′ ∼P (·|b,a )[V π EF E
MEF E (b′′)] − Eb′∼P open(·|b,a )[V π EF E
MEF E (b′)]
)] ⏐
⏐
⏐
⏐
≤
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[
−IG(b, a) + γRmax
1 − γ
√
2IG(b, a)
] ⏐
⏐
⏐
⏐
≤
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[
−IG(b, a) + γRmax
1 − γ IG(b, a)
] ⏐
⏐
⏐
⏐
= γRmax + γ − 1
(1 − γ)2
⏐
⏐
⏐E(b,a )∼dπ
P
[IG(b, a)]
⏐
⏐
⏐
= γRmax + γ − 1
(1 − γ)2 E(b,a )∼dπ
P
[IG(b, a)] .
(92)
Plugging into the performance gap, we have:
|JM (π) − JM (πEF E )|
≤
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[Aπ EF E
MEF E (b, a)]
⏐
⏐
⏐
⏐
+ γRmax + γ − 1
(1 − γ)2 E(b,a )∼dπ
P
[IG(b, a)] + γRmax + γ − 1(1 − γ)2 E(b,a )∼dπ
P
[ ⏐
⏐
⏐
⏐
⏐
dπ EF E
P (b, a)
dπ
P (b, a) IG(b, a)
⏐
⏐
⏐
⏐
⏐
]
≤
⏐
⏐
⏐
⏐
1
1 − γ E(b,a )∼dπ
P
[Aπ EF E
MEF E (b, a)]
⏐
⏐
⏐
⏐
+ γRmax + γ − 1
(1 − γ)2 E(b,a )∼dπ
P
[IG(b, a)] + γRmax + γ − 1(1 − γ)2





dπ EF E
P (b, a)
dπ
P (b, a)





∞
E(b,a )∼dπ
P
[|IG(b, a)|]
= 1
1 − γ ǫπ EF E + (C + 1)(γRmax + γ − 1)
(1 − γ)2 ǫIG
≤ 1
1 − γ ǫπ open + (C + 1)γRmax
(1 − γ)2 ǫIG − C + 1
1 − γ ǫIG .
(93)
For the open-loop policy which does not have the IG term in the reward, it is easy to see that the performance gap i s:
|JM (π) − JM (πopen)| ≤ 1
1 − γ ǫπ open + (C + 1)γRmax
(1 − γ)2 ǫIG . (94)
A.5 Proofs for Section 6.2
Equivalence between state marginal matching (Lee et al., 20 19) and closed-loop EFE in MDP The marginal
state distribution at time step t following policy π is deﬁned as:
dπ
t (st) = bt(st|bt−1, π)
=
∑
st−1
∑
at−1
P (st|st−1, at−1)π(at−1|st−1)bt−1(st−1) , (95)
28
V alue of Information and Reward Speciﬁcation in Active Infe rence and POMDPs
where the state marginal at one time step depends on the state marginal at the previous time step.
W e can deﬁne the time-averaged state marginal matching prob lem as state marginal matching at all time steps:
arg min
π
KL[dπ (s)|| ˜P (s)] = arg min
π
1
T
T∑
t=0
KL[dπ
t (st)|| ˜P (st)]
= arg min
π
T∑
t=0
Ebt (st|bt−1,π )[log bt(st|bt−1, π) − log ˜P (st)] .
(96)
Since we consider closed-loop policies where the agent can o bserve the environment state rather than computing the
next state marginal from an imprecise current state margina l, it follows that:
bt(st|bt−1, π) = P (st|st−1, at−1) . (97)
Thus the objective is equivalent to:
min
π
EP (s0:T ,a 0:T −1)
[ T∑
t=0
(
log P (st|st−1, at−1) − log ˜P (st)
)]
. (98)
This is the same objective in (Lee et al., 2019) and also (Da Co sta et al., 2023).
29