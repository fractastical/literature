Deconstructing deep active inference.
Th´ eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´ s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Lisa Bonheme lb732@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
University College London, Wellcome Centre for Human Neuroimaging (honorary)
London WC1N 3AR, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a theory of perception, learning and decision making, which can be applied to
neuroscience, robotics, psychology, and machine learning. Recently, intensive reasearch has been
taking place to scale up this framework using Monte-Carlo tree search and deep learning. The
end-goal of this activity is to solve more complicated tasks using deep active inference. First,
we review the existing literature, then, we progresively build a deep active inference agent as
follows: (i) implement a variational auto-encoder (VAE), (ii) implement a deep hidden Markov
model (HMM), (iii) implement a deep critical hidden Markov model (CHMM), and (iv) imple-
ment a complete deep active inference agent (DAI). For the CHMM and DAI agents, we have
experimented with ﬁve deﬁnitions of the expected free energy and three diﬀerent action selection
strategies. According to our experiments, the models able to solve the dSprites environment are
the ones that maximise rewards. Finally, we compare the similarity of the representation learned
by the layers of various models (e.g., deep Q-network, CHMM, DAI) using centered kernel align-
ment. Importantly, the CHMM maximising reward and the CHMM minimising expected free
energy learn very similar representations except for the last layer of the critic network (reﬂect-
ing the diﬀerence in learning objective), and the variance layers of the transition and encoder
networks. While performing further inspection of those (variance) layers, we found that the tran-
sition network of the reward maximising CHMM is a lot more certain than the transition network
of the CHMM minimising expected free energy. More precisely, the CHMM minimising expected
free energy is only conﬁdent about the world transition when performing action down. This sug-
gests that the CHMM minimising expected free energy always picks the action down, and does
not gather enough data for the other actions. In contrast, the CHMM maximising reward, keeps
on selecting the actions left and right, enabling it to successfully solve the task. The only diﬀer-
ence between those two CHMMs is the epistemic value, which aims to make the outputs of the
transition and encoder networks as close as possible. Thus, the CHMM minimising expected free
arXiv:2303.01618v2  [cs.AI]  8 May 2023
Champion et al.
energy repeatedly picks a single action (down), and becomes an expert at predicting the future
when selecting this action. This eﬀectively makes the KL divergence between the output of the
transition and encoder networks small. Additionally, when selecting the action down the average
reward is zero, while for all the other actions, the expected reward will be negative. Therefore, if
the CHMM has to stick to a single action to keep the KL divergence small, then the action down
is the most rewarding. Thus, the appropriate formulation of the epistemic value in deep active
inference remains an open question.
Keywords: Deep Learning, Active Inference, Bayesian Statistics, Free Energy Principle, Rein-
forcement Learning
1. Introduction
Active inference is a uniﬁed framework for perception, learning, and planning that has emerged from theo-
retical neuroscience (Costa et al, 2020a; Champion et al, 2021, 2022b,a,c). This framework has successfully
explained a wide range of brain phenomena (Friston et al, 2016; Itti and Baldi, 2009; Schwartenbeck et al,
2018; FitzGerald et al, 2015), and has been applied to a large number of tasks in robotics and artiﬁcial
intelligence (Fountas et al, 2020; Pezzato et al, 2020; Sancaktar et al, 2020; C ¸ atal et al, 2020; Cullen et al,
2018; Millidge, 2019).
A promising area of research revolves around scaling up this theoretical framework to tackle in-
creasingly complex tasks. Research towards this goal is generally driven from recent advances in machine
learning. For example, variational auto-encoders (Doersch, 2016; Higgins et al, 2017; Kingma and Welling,
2014; Rezende et al, 2014) have been key to the integration of deep neural networks within active infer-
ence (Sancaktar et al, 2020; C ¸ atal et al, 2020; Millidge, 2020), and the Monte Carlo tree search algorithm
(Browne et al, 2012; Silver et al, 2016) has been used to improve planning eﬃcency (Fountas et al, 2020;
Champion et al, 2022b,a,c,d).
Another closely related ﬁeld is reinforcement learning (Mnih et al, 2013; van Hasselt et al, 2015;
Lample and Chaplot, 2016), which addresses the same kind of tasks, where an agent must interact with
its environment. A known challenge in this ﬁeld is the correlation between the consecutive samples,
which violates the standard i.i.d. assumption on which most of machine learning relies. To break this
correlation, researchers proposed to store past experiences of the agent inside a replay buﬀer (Mnih et al,
2013). Experiences can then be re-sampled randomly from the buﬀer to train the Q-network, which is
used to approximate Q-values. The Q-network is trained to minimize the mean squared error between its
output and a target value, which is deﬁned as:
y(ot,at) = Eot+1∼E(ot,at)
[
rt + γ max
at+1∈A
Qθa(ot+1,at+1)
]
,
where t is the present time step, Ais the set of available actions, y(ot,at) is the target Q-value to be
predicted, E is the expectation w.r.t the observations received from the environment, rt is the reward
obtained by the agent when performing action at in state1 ot, ot+1 is the state reached when performing
action at in state ot, E is the environment emulator from which ot+1 is sampled, γ is the discount factor
that discounts future rewards, and Qθa(ot+1,at+1) is the output of the Q-network, i.e., the estimated
Q-value of performing action at+1 in state ot+1.
Unfortunatly, using the above target to train the Q-network can make the training unstable. Generally,
the problem is addressed by introducing a target network ˆQˆθa
(ot+1,at+1), which is simply a copy of the
Q-network. The weights of the target network are then synchronized with the weights of the Q-network
1. Note, we are using the notation oτ for the (observable) state at (an arbitrary) time step τ, instead of the more standard
notation sτ. This is because we reserve the notation sτ for the (unobserved) states that arise in the context of active
inference.
2
Deconstructing deep active inference.
every K (learning) iterations (Mnih et al, 2013). The new target is obtained by replacing the Q-network
by the target network, i.e.,
y(ot,at) = Eot+1∼E(ot,at)
[
rt + γ max
at+1∈A
ˆQˆθa
(ot+1,at+1)
]
.
In Section 2, we review the existing literature and present: the Deep Q-network (DQN) agent (Mnih et al,
2013), the deep active inference with Monte-Carlo methods ( DAIMC) agent by Fountas et al (2020), the
deep active inference as variational policy gradients ( DAIVPG ) approach by Millidge (2020), the deep
active inference agent of rubber hand illusion ( DAIRHI) by Rood et al (2020), the deep active inference
agent for humanoid robot control ( DAIHR) by Sancaktar et al (2020); Lanillos et al (2020); Oliver
et al (2019), the deep active inference agent based on the free action objective ( DAIFA) by Ueltzh¨ oﬀer
(2018), a deep active inference agent for partially observable Markov decision processes ( DAIPOMDP )
by van der Himst and Lanillos (2020), as well as various methods for which the code is not available
online. We argue that while all these approaches illuminate important issues associated with realising a
deep active inference agent, a fully complete implementation has not yet been published. Consequently,
to systematically explore the construction of deep active inference agents, in Section 3, we incrementally
build such an agent. We start with a simple variational auto-encoder (VAE) composed of an encoder and
decoder network. Next, a transition network is added to create a deep hidden Markov model (HMM).
Then, a critic network is added to deﬁne a prior over actions, which leads to the critical HMM (CHMM).
Lastly, the policy network is added to approximate the posterior over actions leading to the full deep active
inference (DAI) agent. Then, in Section 4, we discuss our ﬁndings regarding the abilities and limitations
of each intermediate step. This section also presents an analysis and discussion of the representations
learned by each intermediate model. Finally, Section 6 puts our ﬁndings in context and concludes this
paper.
2. Review of existing research
In this section, we discuss the DQN agent from the reinforcement learning literature, six agents from the
active inference literature for which the code is available online ( DAIMC, DAIVPG , DAIRHI, DAIHR,
DAIFA, and DAIPOMDP ), and a few other deep active inference agents for which the code is unavailable.
Finally, we explain how the representations learned by the agents can be compared using centered kernel
alignment. Note, the notation used throughout this section is summarised in Appendix A.
2.1 DQN agent (Mnih et al, 2013)
Let us start with the DQN agent (Mnih et al, 2013), whose goal is to maximise the amount of reward
obtained over time. At each time step τ, the agent is observing an image oτ, and is allowed to perform one
action aτ ∈A. After performing aτ when observing oτ, the agent receives a reward rτ. The Q-learning
algorithm (Sutton et al, 1998) aims to maximise reward by computing the Q-values Q(oτ,aτ), for each
state-action pair ( oτ, aτ). The Q-values represent the expected amount of rewards obtained by taking
action aτ in state oτ. This approach is intractable for image based domains such as Atari games, since one
would need to store a vector of Q-values for each possible image. Instead, the DQN algorithm (illustrated
in Figure 1) has been developed, which uses a deep neural network Qθa to approximate the Q-values.
More formally, Qθa maps any observation to a vector of size # Acontaining the Q-values of each possible
action, and we denote by Qθa(oτ,aτ) the element at position aτ in the output vector predicted by Qθa
when provided with the image oτ. As we discussed in the introduction, the training stability of the Q-
network is improved by introducing a target network ˆQˆθa
, which is structurally identical to the Q-network
and whose weights are synchronised with the weights of the Q-network everyK (learning) iterations. The
Q-network’s weights are then optimised using gradient descent to minimise the mean square error between
3
Champion et al.
the output of the Q-network and a target value, i.e.,θ∗
a = arg minθa MSE[Qθa(ot,•),y(ot,•)], where y(ot,at)
is the target Q-value for each state-action pair, and, as highlighted earlier, is deﬁned as follows:
y(ot,at) = Eot+1∼E(ot,at)
[
rt + γ max
at+1∈A
ˆQˆθa
(ot+1,at+1)
]
.
ot Qθa Qθa(ot,•)
ot+1 ˆQˆθa
ˆQˆθa
(ot+1,•)
rt
γ
y(ot,•)
MSE
Figure 1: This ﬁgure illustrates the DQN agent. Brieﬂy, the image ot is fed into the Q-network, and
the image ot+1 is fed into the target network. The Q-network outputs the Q-values for each action at
time t, and the target network outputs the Q-values for each action at time t+ 1. Then, the reward, the
discount factor, and Q-values of each action at time t+ 1 are used to compute the target values y(ot,•).
Finally, the goal is to minimise the MSE between the prediction of the Q-network and the target values
by changing the weights of the Q-network.
2.2 DAIMC agent (Fountas et al, 2020)
In this section, we review the DAIMC agent proposed by Fountas et al (2020), which represents the most
ambitious and complete implementation of a deep active inference agent that accordingly adds important
new concepts to the ﬁeld. The relevant code is available at the following URL: https://github.com/
zfountas/deep-active-inference-mc . The DAIMC agent is composed of four deep neural networks, as
illustrated in Figures 2 and 3. The encoder Eφs takes images as input, and outputs the mean and variance
of the variational distribution over hidden states, i.e., Qφs(st) = N(st; µ,σ), where µ,σ = Eφs(ot). The
decoder Dθo takes a state as input, and outputs the parameters of a product of Bernoulli distributions,
which can be interpreted as the expected (reconstructed) image ˆot, i.e.,
Pθo(ot|st) = Bernoulli(ot; ˆot),
where ˆot = Dθo(st) are the values predicted by the decoder, and Bernoulli(ot; ˆot) is a product of Bernoulli
distributions deﬁned as:
Bernoulli(ot; ˆot) =
∏
x,y
Bernoulli(ot[x,y]; ˆot[x,y]),
4
Deconstructing deep active inference.
Policy ˆπ
ωt = a
1+exp(−b−Dt
c ) + d
Dt = DKL [ Qφa(at|st)||P(at)]
ot
Encoder
ln σµϵ
Decoder
ˆst
ˆot
at
Transition
ln˚σ
˚µ
ϵ
˚st+1
ot+1
Encoder
ln ˆσˆµϵ
Decoder
ˆst+1
ˆot+1
Figure 2: This ﬁgure illustrates the DAIMC agent, which is composed of an encoder, a decoder, a
transition network, and a policy network. The same VAE (encoder and decoder) is repeated in the ﬁgure
to reﬂect successive time-points.
where Bernoulli( •; •) is a Bernoulli distribution over the possible values of the pixelot[x,y], parameterized
by the parameter ˆot[x,y]. The transition network Tθs takes a state-action pair as input, and outputs the
mean and variance of a Gaussian distribution over hidden states, i.e., Pθs(sτ+1|sτ,aτ) = N(sτ+1;˚µ, ˚σ
ωt),
where ˚µ,˚σ = Tθs(sτ,aτ), and ωt is the top-down attention parameter modulating the precision of the
transition mapping (see below). The policy network Pφa takes a state as input, and outputs a distribution
over actions, i.e., Qφa(at|st) = Cat(at; ˆπ), where ˆπ= Pφa(st). Finally, the prior over actions is deﬁned as
follows:
P(at) =
∑
π∈Π
[πt = at]P(π), (1)
where Π is the set of all possible policies, πt is the action precribed by policy π at time t, the square
brackets represent an indicator function that equals one if the condition within the bracket is satisﬁed
and zero otherwise, and P(π) is the prior over policies deﬁned as:
P(π) = σ[−G(π)],
5
Champion et al.
where σ[ •] is the softmax function, and G(π) is the expected free energy (EFE) of policy π, which is
deﬁned as:
G(π) =
T∑
τ=t
Gτ(π) =
T∑
τ=t
E ˜Q
[
ln Q(sτ,θ|π) −ln ˜P(oτ,sτ,θ|π)
]
, (2)
where ˜Q= Q(oτ,sτ,θ|π) = Q(θ|π)Q(sτ|θ,π)Q(oτ|sτ,θ,π ) is the predictive posterior, and ˜P(oτ,sτ,θ|π) =
P(θ|sτ,oτ,π)P(sτ|oτ,π)P(oτ|π) is the target distribution. However, Equation 2 needs to be re-arranged
to be computed in practice 2, and Section 2.2.3 will present this derivation. Finally, as shown in Figure 2,
the top-down attention parameter is computed as follows:
ωt = a
1 + exp(−b−Dt
c ) + d,
where Dt = DKL [ Qφa(at|st)||P(at)], and {a,b,c,d }are ﬁxed hyperparameters. Intuitively, ωt is high
when the posterior over actions (from the policy network) is close to the prior over actions (from the
expected free energy), and low when the posterior is far away from the prior. This, in turn, means
that extra uncertainty is introduced into the transition mapping (see paragraph before Equation 1) when
posterior over actions and prior over actions are very diﬀerent. Finally, note that the number of terms
required to compute the prior over actions (deﬁned in Equation 1) grows exponentially with the time
horizon of planning. Because this is intractable in practice, Fountas et al (2020) implemented a Monte-
Carlo tree search (MCTS) algorithm to evaluate the expected free energy of each action (see below).
Finally, action selection is performed by sampling from the following distribution:
˜P(at) = N(ˆst,at)∑
ˆat N(ˆst,ˆat),
where ˆst is the current state of the environment, and N(st,at) is the number of times action at has been
visited from state st during MCTS.
2.2.1 The Monte-Carlo tree search
In this section, we describe the planning algorithm used byDAIMC, i.e., Monte-Carlo tree search (MCTS).
MCTS is used to enhance the planning ability of the agent by allowing it to look into the future. At the
beginning of an action-perception cycle, the agent is provided with an image ot. This image can be feed
into the encoder to get the mean vectorµof the posterior over the latent states, i.e.,Qφs(st) = N(st; µ,σ).
Since µis the mean of the Gaussian posterior, it can be interpreted as the maximum a posteriori (MAP)
estimate of the latent states at time step t. This MAP estimate will constitute the root node of the
Monte-Carlo tree search (MCTS).
The ﬁrst step of the MCTS is to use the Upper Conﬁdence bounds for Trees (UCT) criterion to
determine which node in the tree should be expanded. Let the tree’s root ˆ st be called the current node,
which is denoted ˆsτ. If the current node has no children (i.e., no previously selected actions from the
current node), then it is selected for expansion. Alternatively, the child with the highest UCT criterion
becomes the new current node and the process is iterated until we reach a leaf node (i.e. a node from
which no action has previously been selected). The UCT criterion (Browne et al, 2012) of the child of ˆ sτ
corresponding to action ˆaτ is given by:
UCT(ˆsτ,ˆaτ) = −¯G(ˆsτ,ˆaτ) + Cexplore ·Qφa(aτ = ˆaτ|sτ = ˆsτ)
1 + N(ˆsτ,ˆaτ) ,
2. By “in practice”, we mean “in the code” or equivalently “when implementing the approach”.
6
Deconstructing deep active inference.
Qφs(sτ)
input: 64 ×64×#C
size: 31 ×31 ×32
kernel: 3
strides: (2,2)
ReLU
size: 15 ×15 ×32
kernel: 3
strides: (2,2)
ReLU
size: 31 ×31 ×32
kernel: 3
strides: (2,2)
ReLU
size: 7 ×7 ×64
kernel: 3
strides: (2,2)
ReLU
size: 3 ×3 ×64
kernel: 3
strides: (2,2)
ReLUReLU
size: 256
rate: 0.5
ReLU
size: 256
rate: 0.5
ReLU
size: 256
rate: 0.5
size: #S + #S
Pθo(oτ|sτ)
input: #S
ReLU
size: 256
rate: 0.5
ReLU
size: 256
rate: 0.5
ReLU
size: 256
rate: 0.5
ReLU
size: 256
rate: 0.5
size: 16 ×16 ×64
kernel: 3
strides: (2,2)
ReLU
size: 32 ×32 ×64
kernel: 3
strides: (2,2)
ReLU
size: 64 ×64 ×32
kernel: 3
strides: (2,2)
ReLU
size: 64 ×64 ×#C
kernel: 3
strides: (2,2)
Pθs(sτ+1|sτ,aτ)
input: #S + #A
ReLU
size: 512
rate: 0.5
ReLU
size: 512
rate: 0.5
ReLU
size: 512
rate: 0.5
size: #S + #S
Qφa(aτ|sτ)
input: #S
ReLU
size: 128
ReLU
size: 128
size: #A
Figure 3: Neural network architectures of the DAIMC agent. Orange blocks correspond to convolutional
layers, green blocks correspond to fully connected layers, blue blocks correspond to dropout, and yellow
blocks correspond to up-convolutional layers. For the dSprites environment, there are four actions (i.e.,
#A = 4), ten states (i.e., #S = 10), and only one channel (i.e., #C = 1). For the Animal-AI environment,
there are three actions (i.e., #A = 3), ten states (i.e., #S = 10), and three channels (i.e., #C = 3). These
are all trained to minimize variational free energy.
7
Champion et al.
where ¯G(ˆsτ,ˆaτ) is the average expected free energy of taking action ˆ aτ in state ˆsτ, Cexplore is the explo-
ration constant that modulates the amount of exploration at the tree level, N(ˆsτ,ˆaτ) is the number of
times action ˆaτ was visited in state ˆsτ, and Qφa(aτ = ˆaτ|sτ = ˆsτ) is the posterior probability of action ˆaτ
in state ˆsτ as predicted by the policy network.
Let ˚sτ be the (leaf) node selected by the above selection procedure. The MCTS then expands one of
the children of ˚sτ. The expansion uses the transition network to compute the mean ˚ µ of Pθs(sτ+1|sτ =
˚sτ,aτ = ˚aτ), which is viewed as a MAP estimate of the states at time τ + 1. Then, we need to estimate
the cost of (virtually) taking action ˚aτ. By deﬁnition, the cost is the expected free energy given by (2),
and Monte-Carlo rollouts can be run to improve its estimation. The ﬁnal step of the planning iteration is
to back-propagate the cost of the newly expanded (virtual) action toward the root of the tree. Formally,
we write the update as follows:
∀s∈A˚sτ ∪{˚sτ}, Gs ←Gs + G˚sτ, (3)
where ˚sτ is the node that was selected for expansion, Gs is the expected free energy of s, and A˚sτ is the
set of all ancestors of ˚sτ in the tree. During the back propagation, we also update the number of visits as
follows:
∀s∈A˚sτ ∪{˚sτ}, Ns ←Ns + 1. (4)
If we let Gaggr
s be the aggregated cost of an arbitrary node sobtained by applying Equation 3 after each
expansion, then we are now able to express ¯Gs formally as:
¯Gs = Gaggr
s
Ns
.
Importantly, if the node s corresponds to the state reached from state ˆ sτ by performing action ˆ aτ,
then ¯G(ˆsτ,ˆaτ) = ¯Gs and N(ˆsτ,ˆaτ) = Ns. The planning procedure described above ends when the
maximum number of planning iterations is reached, or when a clear winner has been identiﬁed, i.e.,
if max at P(at) − 1
#A > Tdec where # A is the number of possible actions, and Tdec is a (threshold)
hyperparameter.
2.2.2 Derivation of the variational free energy
In this section, we provide a derivation for the variational free energy used by Fountas et al (2020).
This derivation was introduced in (Millidge, 2020), and can be adapted to derive the variational free
energy of the models presented in Section 3. Recall, the goal of the variational free energy as classically
presented is to make the approximate posterior Qφ(st,at) as close as possible to the true posterior 3
P(st,at|ot,st−1,at−1), i.e.,
Q∗
φ(st,at) = arg min
Qφ(st,at)
DKL [ Qφ(st,at)||P(st,at|ot,st−1,at−1)] .
Using Bayes theorem, the linearity of expectation, and the fact that Qφ(st,at) integrates to one:
Q∗
φ(st,at) = arg min
Qφ(st,at)
DKL [ Qφ(st,at)||P(st,at|ot,st−1,at−1)]
= arg min
Qφ(st,at)
DKL [ Qφ(st,at)||P(st,at,ot,st−1,at−1)] + ln P(ot,st−1,at−1)  
Constant w.r.t Qφ(st,at)
= arg min
Qφ(st,at)
DKL [ Qφ(st,at)||P(st,at,ot,st−1,at−1)] .
3. By posterior we mean a conditional distribution, where the given variables are those for which we observe a speciﬁc value.
Note, the value of st−1 is unknown but can be sampled from the posterior over st−1 (from the previous action-perception
cycle).
8
Deconstructing deep active inference.
Using the d-separation criteria (Koller and Friedman, 2009), it can be shown that:
P(st,at,ot,st−1,at−1) = Pθo(ot|st)P(at)Pθs(st|st−1,at−1)Qφ(st−1,at−1),
where Qφ(st−1,at−1) is the variational posterior obtained through the inference process at the previous
time step. In the above equation, Qφ(st−1,at−1) was used to replace P(st−1,at−1), i.e., Qφ(st−1,at−1)
was used as an empirical prior. Additionally, since Qφ(st−1,at−1) is a constant w.r.t Qφ(st,at), the above
minimization problem reduces to:
Q∗
φ(st,at) = arg min
Qφ(st,at)
DKL [ Qφ(st,at)||Pθo(ot|st)P(at)Pθs(st|st−1,at−1)]  
variational free energy
(5)
= arg min
Qφa(at|st)Qφs(st)
EQφs(st)
[
DKL [ Qφa(at|st)||P(at)]
]
+ DKL [ Qφs(st)||Pθs(st|st−1,at−1)]
−EQφs(st)
[
ln Pθo(ot|st)
]
.
By comparing the VFE in (5) and the EFE in (2), one can see an inconsistency. Namely, the parameters are
seen as latent variables in the EFE deﬁnition, c.f., θ in (2), but they are regarded as parameters of neural
networks in the VFE, c.f., θs and θo in 5. Note, θ cannot be both a parameter (i.e, parameter, vector, or
matrix) and a random variable, and if θ is a random variable, one must deﬁne its probability density, i.e.,
P(θ). Additionally, this inconsistency raises the question of whether the EFE is really the expectation of
the VFE. To sum up, the DAIMC agent is equipped with four deep neural networks modelling Qφa(at|st),
Qφs(st), Pθs(st|st−1,at−1), and Pθo(ot|st). The weights of those networks are optimised using back-
propagation to minimise the VFE given by (5). Note, (5) decomposes into two KL-divergence terms that
can be computed analytically, and the expectations can be approximated using a Monte-Carlo estimate.
Also, because Pθo(ot|st) is modelled as a product of Bernoulli distributions, the logarithm of Pθo(ot|st)
reduces to the binary cross entropy.
2.2.3 Independence assumptions and the expected free energy
The EFE as stated in Equation (2) needs to be re-arranged because it cannot be easily evaluated. We
therefore present the derivation proposed by Fountas et al (2020). Then, we highlight two independence
assumptions, i.e., sτ ⊥ ⊥θ |π and sτ ⊥ ⊥θ |π,oτ, used without explicitly presented proofs. Finally,
we propose an alternative derivation that does not require these two assumptions and produces a sim-
pler result. Using the product rule of probability, one can see that Q(sτ,θ|π) = Q(θ|sτ,π)Q(sτ|π) and
˜P(oτ,sτ,θ|π) = P(oτ|π)P(sτ|oτ,π)P(θ|sτ,oτ,π). Using those two factorisations, the EFE given in (2),
i.e.,
Gτ(π) = E ˜Q
[
ln Q(sτ,θ|π) −ln ˜P(oτ,sτ,θ|π)
]
,
where ˜Q= Q(oτ,sτ,θ|π), and can be re-arranged as follows:
Gτ(π) = −E ˜Q
[
ln ˜P(oτ|π)
]
+ E ˜Q
[
ln Q(sτ|π) −ln ˜P(sτ|oτ,π)
]
+ E ˜Q
[
ln Q(θ|sτ,π) −ln ˜P(θ|sτ,oτ,π)
]
. (6)
Note, the above derivation follows the work of Fountas et al (2020).
9
Champion et al.
Re-arranging the second term of Equation (6) according to Fountas et al (2020)
First, the second term of Equation (6) is re-arranged into entropy terms for which an analytical solution
exists. In the supplementary material of (Fountas et al, 2020), the derivation proceeds as follows:
E ˜Q
[
ln Q(sτ|π) −ln ˜P(sτ|oτ,π)
]
=∆ E ˜Q
[
ln Q(sτ|π) −ln Q(sτ|oτ,π)
]
(7)
= EQ(θ|π)Q(sτ|θ,π)Q(oτ|sτ,θ,π)
[
ln Q(sτ|π) −ln Q(sτ|oτ,π)
]
= EQ(θ|π)
[
EQ(sτ|θ,π)[ln Q(sτ|π)] −EQ(sτ|θ,π)Q(oτ|sτ,θ,π)[ln Q(sτ|oτ,π)]
]
= EQ(θ|π)
[
EQ(sτ|θ,π)[ln Q(sτ|π)] −EQ(oτ|θ,π)Q(sτ,|oτ,θ,π)[ln Q(sτ|oτ,π)]
]
,
where in the ﬁrst line a distribution was renamed, i.e., ˜P(sτ|oτ,π) =∆ Q(sτ|oτ,π). The next step in the
derivation (c.f. supplementals of Fountas et al (2020)) re-arranges this ﬁnal expression to the following:
E ˜Q
[
ln Q(sτ|π) −ln Q(sτ|oτ,π)
]
= EQ(θ|π)
[
EQ(oτ|θ,π)[H[Q(sτ|oτ,π)]] −H[Q(sτ|π)]
]
.
However, the above equation assumes that sτ ⊥ ⊥θ |π and sτ ⊥ ⊥θ |π,oτ. In other words, some of the
conditioning on θ has been dropped, i.e.,
EQ(θ|π)
[
EQ(sτ|θ,π)[ln Q(sτ|π)] −EQ(oτ|θ,π)Q(sτ,|oτ,θ,π)[ln Q(sτ|oτ,π)]
]
(last expression of derivation 7)
̸= EQ(θ|π)
[
EQ(sτ|π)[ln Q(sτ|π)] −EQ(oτ|θ,π)Q(sτ|oτ,π)[ln Q(sτ|oτ,π)]
]
= EQ(θ|π)
[
EQ(oτ|θ,π)
[
H[Q(sτ|oτ,π)]
]
−H[Q(sτ|π)]
]
.
Whether this conditioning can be dropped or not depends on the factorisation of the distribution. In
other words, the two assumptions (i.e., sτ ⊥ ⊥θ |πand sτ ⊥ ⊥θ |π,oτ) would have to be checked using the
d-separation criterion. However, this is diﬃcult to do, since as mentioned previously, the parameters θare
latent variables in (2) but are regarded as parameters in (5), which makes the graphical model unclear.
Instead of attempting to prove that sτ ⊥ ⊥θ |π and sτ ⊥ ⊥θ |π,oτ, we propose an alternative derivation
that does not require such independence assumptions.
Alternative derivation of the second term of Equation (6)
Restarting from the second term of Equation (6), we can re-arrange as follows:
E ˜Q
[
ln Q(sτ|π) −ln ˜P(sτ|oτ,π)
]
=∆ E ˜Q
[
ln Q(sτ|π) −ln Q(sτ|oτ,π)
]
= EQ(sτ|π)Q(θ,oτ|sτ,π)
[
ln Q(sτ|π)
]
−EQ(oτ|π)Q(sτ|oτ,π)Q(θ|sτ,oτ,π)
[
ln Q(sτ|oτ,π)
]
= EQ(sτ|π)
[
ln Q(sτ|π)
]
−EQ(oτ|π)Q(sτ|oτ,π)
[
ln Q(sτ|oτ,π)
]
= EQ(oτ|π)
[
H[Q(sτ|oτ,π)]
]
−H[Q(sτ|π)],
where in the ﬁrst line a distribution was renamed, i.e., ˜P(sτ|oτ,π) =∆ Q(sτ|oτ,π), two diﬀerent factoriza-
tions of ˜Qare used going from the ﬁrst to the second line, the linearity of expectations was used between
the ﬁrst and second line, and the expectation w.r.t θ was dropped (between lines two and three) because
the expectation of a constant is the constant itself. Importantly, the above derivation does not make any
assumption of independence, and leads to a simpler result. This alternative line of reasoning is beneﬁcial
as it produces a stronger derivation that relies upon fewer assumptions. The simpler result produced by
this derivation, also have a practical implication. Indeed, the expectation w.r.t. Q(θ|π) disappears and
the expectation w.r.t. Q(oτ|θ,π) is now w.r.t. Q(oτ|π). Those two changes sugguest that a diﬀerent
implementation of this term is required.
10
Deconstructing deep active inference.
Re-arranging the third term of Equation (6) from Fountas et al (2020)
For completeness, we now focus on the third term of (6), which can be re-arranged as follows:
E ˜Q
[
ln Q(θ|sτ,π) −ln ˜P(θ|sτ,oτ,π)
]
=∆ E ˜Q
[
ln Q(θ|sτ,π) −ln Q(θ|sτ,oτ,π)
]
= E ˜Q
[
ln Q(oτ|sτ,π) −ln Q(oτ|sτ,θ,π )
]
,
where ˜P(θ|sτ,oτ,π) was renamed as Q(θ|sτ,oτ,π), and Bayes theorem was used to get:
Q(θ|sτ,π) = Q(θ|sτ,oτ,π)Q(oτ|sτ,π)
Q(oτ|sτ,θ,π ) ⇔ Q(θ|sτ,π)
Q(θ|sτ,oτ,π) = Q(oτ|sτ,π)
Q(oτ|sτ,θ,π ).
Finally, by recalling that ˜Q = Q(oτ,sτ,θ|π) = Q(θ|π)Q(sτ|θ,π)Q(oτ|sτ,θ,π ), and using the linearity of
expectation, we get:
E ˜Q
[
ln Q(θ|sτ,π) −ln Q(θ|sτ,oτ,π)
]
= EQ(oτ,sτ|π)
[
ln Q(oτ|sτ,π)
]
+ EQ(θ|π)Q(sτ|θ,π)
[
H[Q(oτ|sτ,θ,π )]
]
= EQ(oτ|sτ,π)Q(sτ|π)
[
ln Q(oτ|sτ,π)
]
+ EQ(θ|π)Q(sτ|θ,π)
[
H[Q(oτ|sτ,θ,π )]
]
= −EQ(sτ|π)
[
H
[
Q(oτ|sτ,π)
]]
+ EQ(θ|π)Q(sτ|θ,π)
[
H[Q(oτ|sτ,θ,π )]
]
,
where because ln Q(oτ|sτ,π) is a constant w.r.t θ, we have been able to use:
EQ(oτ,θ,sτ|π)[ln Q(oτ|sτ,π)] = EQ(oτ,sτ|π)[ln Q(oτ|sτ,π)].
To sum up, this derivation provides an expression based on the following two entropy terms:H[Q(oτ|sτ,π)]
and H[Q(oτ|sτ,θ,π )], which the authors claim can be estimated (c.f. Appendix B for more details). Note
that our proposed alternative to the EFE (see below) uses this derivation for the third term of Equation
(6).
The EFE from Fountas et al (2020)
If one follows the derivation proposed by Fountas et al (2020), then the EFE is given by:
Gτ(π) = −E ˜Q
[
ln ˜P(oτ|π)
]
+ EQ(θ|π)
[
EQ(oτ|θ,π)
[
H[Q(sτ|oτ,π)]
]
−H[Q(sτ|π)]
]
+ EQ(θ|π)Q(sτ|θ,π)
[
H[Q(oτ|sτ,θ,π )]
]
−EQ(sτ|π)
[
H
[
Q(oτ|sτ,π)
]]
. (8)
Note, in Section 2.2, we focused on presenting the approach given in Fountas et al (2020), with some
adjustments for consistency. More details about the implementation of Equation 8 are presented in
Appendix B, along with some discrepancies between the paper and the code.
Our proposed alternative to the EFE
If one follows our alternative derivation, then the EFE is given by:
Gτ(π) = −E ˜Q
[
ln ˜P(oτ|π)
]
+ EQ(oτ|π)
[
H[Q(sτ|oτ,π)]
]
−H[Q(sτ|π)],
+ EQ(θ|π)Q(sτ|θ,π)
[
H[Q(oτ|sτ,θ,π )]
]
−EQ(sτ|π)
[
H
[
Q(oτ|sτ,π)
]]
.
Finally, DAIMC does solve the dSprites environment.
11
Champion et al.
2.3 DAIVPG agent (Millidge, 2020)
In this section, we explain and discuss the approach of Millidge (2020). The code is available at the
following URL: https://github.com/BerenMillidge/DeepActiveInference. Note that, though the
mathematics in the paper are based on the formalism of a partially observable Markov decision process
(POMDP), the code does not implement an encoder/decoder architecture, which means that the code
implements a fully observable decision process, i.e., MDP. Additionally, theDAIVPG is composed of three
neural networks, as illustrated in Figures 4 and 5. The ﬁrst is the transition network that predicts the
future observations based on the current observations and action, i.e., ˚oτ+1 = Tθo(oτ,aτ). The second is
the policy network that models the variational distribution over actionsQφa(aτ|oτ). The third is the critic
network that predicts the expected free energy of each action given the current observation. Moreover,
Millidge (2020) deﬁnes the prior over actions as follows:
P(aτ|oτ) = σ[−ζG(oτ,aτ)],
where ζ is the precision of the prior over actions, σ[ •] is a softmax function, and G(oτ,aτ) is the expected
free energy (EFE) of taking action aτ when observing oτ. In the paper, the mathematics are based on
the POMDP formalism. Therefore, G(oτ,aτ) is denoted G(sτ,aτ), and is deﬁned as follows:
G(sτ,aτ) = −rτ + DKL [ Q(sτ)||Q(sτ|oτ)]  
intrinsic value
+ ˆGˆθa
(aτ+1,sτ+1), (9)
where rτ is the reward gathered by the agent at time step τ, and ˆGˆθa
(aτ+1,sτ+1) is the target network
(i.e., a copy of the critic network whose weights are synchronised every K iterations of learning). Now,
remember that in the implementation, there is no encoder Q(sτ) and no decoder P(oτ|sτ). In other
words, there are no hidden states sτ, raising some uncertainty about how the intrinsic value is com-
puted. The code available on Github 4 at the following URL: https://github.com/BerenMillidge/
DeepActiveInference, in the ﬁle active_inference_with_Tmodel.jl (see line 51) suggests that the
following equation is used:
intrinsic value =
∑
i
[
oτ+1[i] −˚oτ+1[i]
]2
, (10)
where oτ+1[i] is the i-th observation received at time step τ + 1, and ˚oτ+1[i] is the (numerical) value of
the i-th observation (at time step τ + 1) predicted by the transition network. More formally, the above
formulation for the intrisic value corresponds to the KL-divergence between two Gaussian distributions
both having an identity covariance matrix, i.e.,
intrinsic value = DKL [ Q(oτ+1)||P(oτ+1|oτ,aτ)] =
∑
i
[
oτ+1[i] −˚oτ+1[i]
]2
,
where P(oτ+1|oτ,aτ) = N(oτ+1;˚oτ+1,I) and Q(oτ+1) is a Gaussian distribution with mean vector oτ+1
and an identity covariance matrix. However, note that (9) is the deﬁnition of the expected free energy in
the POMDP setting. As explained by Costa et al (2020b), the expected free energy in the MDP setting
is given by:
G(at:T−1,ot) ≈
T∑
τ=t+1
DKL [ P(oτ|at:T−1,ot)||P(oτ)] ,
4. We are referring to the version of the code that was available on github on the 6th of June 2022.
12
Deconstructing deep active inference.
where P(oτ) are the prior preferences of the agent (related to rewards in reinforcement learning), and
P(oτ|at:T−1,ot) is the transition mapping. Importantly, this deﬁnition for the expected free energy does
not decompose into extrinsic and intrinsic terms as in (9). Thus, (as it stands) the implementation of
the DAIVPG agent is a mixture between the POMDP and MDP setting, where the generative model
corresponds to an MDP, and the expected free energy is adapted from the POMDP setting.
We conclude this section by dicussing the training procedure of the transition, policy and critic net-
works. As explained in the paper, the transition network is trained to minimise the variational free enery.
Additionally, because of the Gaussian assumptions (with identity covariance matrices) mentioned above,
the KL-divergence reduces to the mean square error (MSE). Thus, the transition network is updated to
minimise the MSE between the observations made by the agent at time τ+1, and the observations (˚oτ+1)
predicted by the transition network, i.e.,
θ∗
o = arg min
θo
MSE
[
oτ+1,˚oτ+1
]
,
where ˚oτ+1 = Tθo(oτ,aτ). The policy network is trained to minimise the KL-divergence between the
variational posterior over actions Qφa(aτ|oτ) and the prior over actions P(aτ|oτ), i.e.,
φ∗
a = arg min
φa
DKL [ Qφa(aτ|oτ)||P(aτ|oτ)] ,
which minimises the variational free energy. Finally, the critic is trained by minimising the MSE between
the target EFE as deﬁned in (9) and the ouput of the critic Gθa(oτ,•):
θ∗
a = arg min
θa
MSE
[
G(oτ,•),Gθa(oτ,•)
]
.
DAIVPG is able to solve the CartPole environment.
There is no encoder
Q(sτ), and no
decoderP(oτ|sτ).
Tθo(oτ,aτ)
input:#O + #A
ReLU
size: 100
size: #O
Qφa(aτ|oτ)
input:#O
ReLU
size: 100
size: #A
Gθa(oτ,•)
input:#O
ReLU
size: 100
size: #A
Figure 4: Neural networks architecture of the DAIVPG agent. Green blocks correspond to fully connected
layers. The ﬁrst neural network is the transition network that takes as input the observation and action
at time step τ, and outputs the mean of a Gaussian distribution over observation at time step τ+ 1. The
second neural network is the policy network that models the variational posterior over actions. The third
neural network is the critic that takes as input an observation and outputs the expected free energy of
each action.
13
Champion et al.
Policy ˆπ
Critic Got
at
Transition ˚ot+1
Figure 5: This ﬁgure illustrates the DAIVPG agent. The only new part is the policy network, which takes
as input the hidden state at time tand ouputs the parameters ˆπ of the variational posterior over actions.
Importantly, the DAIVPG takes actions based on the EFE.
2.4 DAIRHI agent (Rood et al, 2020)
In this section, we explain and discuss the approach of Rood et al (2020). Put simply, this paper proposes
a variational auto-encoder (VAE), which is able to account for results that were observed in the context of
the rubber-hand illusion (RHI) experiment. In the experiment in Rood et al (2020), an agent (i.e., either
a human or a computer) is able to move an arm in a 3D space. However, the agent does not observe the
real position of the arm, instead, the agent sees an artiﬁcial hand placed in a diﬀerent location. This can
be implemented using virtual reality (for humans) or within a simulator (for computers). Since, Rood
et al (2020) restricted themself to the context of a VAE, this approach cannot be considered as a complete
implementation of deep active inference. More precisely, the transition and critic (or policy) networks are
missing.
2.5 DAIHR agent (Sancaktar et al, 2020; Lanillos et al, 2020; Oliver et al, 2019)
In this section, we explain and discuss the following approaches: Sancaktar et al (2020), Lanillos et al
(2020), and Oliver et al (2019). Brieﬂy, those papers propose a free energy minimisation scheme based
on a single decoder network, which is used to control Nao, TIAGo, and iCub robots, respectively. Since,
14
Deconstructing deep active inference.
Sancaktar et al (2020); Lanillos et al (2020); Oliver et al (2019) restricted themself to the context of a
single decoder, this approach can not be considered as a complete implementation of deep active inference.
More precisely, the encoder, transition and critic (or policy) networks are missing.
2.6 DAIFA agent (Ueltzh¨ oﬀer, 2018)
In this section, we review the approach proposed by Ueltzh¨ oﬀer (2018). The original code of this pa-
per is available on GitHub at the following URL: https://github.com/kaiu85/deepAI_paper. This
approach is composed of four deep neural networks. The encoder Eφs models the approximate pos-
terior over states Qφs(st|st−1,ot) as a Gaussian distribution, i.e., Qφs(st|st−1,ot) = N(st; µ,σ) where
µ,σ = Eφs(st−1,ot). The decoder Dθo models the likelihood mapping Pθo(oτ|sτ) as a Gaussian distribution,
i.e., Pθo(oτ|sτ) = N(oτ; µo,σo) where µo,σo = Dθo(sτ). The transition network Tθs models the transition
mapping Pθs(sτ|sτ−1) as a Gaussian distribution, i.e., Pθs(sτ|sτ−1) = N(sτ;˚µ,˚σ) where ˚µ,˚σ= Tθs(sτ−1).
Note that, the transition network is only conditioned on the previous state. This is because the action
is contained in the observations predicted by the decoder. More precisely, the experiments were run in
the MountainCar environment, which means that the agent is observing the x position of the car ox
τ.
Additionally, according to the idea of proprioception, the agent observes its own action, i.e., oa
τ−1 = aτ−1
where aτ−1 is the action performed by the agent at time τ−1. In what follows, we let oτ = (ox
τ,oa
τ−1) be
the concatenation of the x position of the car and the action taken by the agent. Importantly, because
oτ contains oa
τ−1, the latent space has to (implicitly) encode the action for the decoder to successfully
predict the observations. Finally, the policy network Pθa models the prior over actions Pθa(aτ|sτ) as a
Gaussian distribution, i.e., Pθa(aτ|sτ) = N(aτ; µa,σa) where µa,σa = Pθa(sτ). Figure 6 illustrates the
architectures of those deep neural networks. Then, Ueltzh¨ oﬀer (2018) deﬁnes the free action objective as
the cumulated variational free energy over time:
FA(o1:T,φ,θ ) =
T∑
τ=1
[ accuracy
  
−EQφs(sτ|sτ−1,oτ)
[
ln Pθo(oτ|sτ)
]
+
complexity
  
DKL [ Qφs(sτ|sτ−1,oτ)||Pθs(sτ|sτ−1)]
  
VFEτ
]
,
where s0 = (0 ,..., 0) is a vector of zeros representing the initial hidden state, T is the time horizon,
Pθo(oτ|sτ), Qφs(sτ|sτ−1,ot), Pθs(sτ|sτ−1) are modeled using Gaussian distributions whose parameters
are predicted by the decoder, encoder and transition network, respectively. Figure 7 illustrates the
computation of the free action objective, and the action-perception cycle of the agent. The ﬁrst action-
perception cycle is initiated when the intital hidden state s0 is being fed into the policy network, which
outputs the parameters of a Gaussian distribution over actions. Then, an action ˆ a0 is sampled from
this Gaussian, and executed in the environment leading to a new observation ox
1. Next, the action ˆa0 is
concatenated with ox
1 to form o1. The observation o1 and the state s0 are then fed into the encoder that
outputs the parameters of a Gaussian distribution over ˆs1. Lastly, a state is sampled from this Gaussian
distribution and is used as input to the next action-perception cycle. This process continues until reaching
the time horizon.
Within each action-perception cycle, the variational free energy of this time step is computed. To
compute VFE τ, the state sτ is fed into both the tansition network and the encoder. Both networks
output the parameters of a Gaussian distribution over sτ+1. A state is sampled from the distribution
predicted by the encoder, and is used as input to the decoder that outputs the parameters of a Gaussian
distribution over oτ+1. Finally, the parameters of the Gaussian distribution over oτ+1 is used to compute
the accuracy term, and the parameters of the two Gaussian distributions over sτ+1 are used to compute
the complexity term.
We now focus on the prior preferences of the agent. Usually, prior preferences are part of the expected
free energy. However, Ueltzh¨ oﬀer (2018) takes a diﬀerent approach. Recall, the latent variable sτ is
15
Champion et al.
modeled using a multivariate Gaussian. The DAIFA agent reserves the ﬁrst dimension of the latent space
to the encoding of the prior preferences. Speciﬁcally, the transition network predicts the mean vector and
the diagonal of the covariance matrix (i.e., another vector) of a multivariate Gaussian over latent states.
The ﬁrst element in the mean vector is clamped to the target x position, and the ﬁrst element of the
variance vector is set to a relatively small value. This eﬀectively propels the agent towards the target
location. Additionally, the encoder predicts another set of mean and variance vectors. The ﬁrst element
of the mean vector predicted by the encoder is clamped to the current x position observed by the agent,
and the ﬁrst element of the variance vector is set to a relatively small value. Note, clamping the value of
the ﬁrst element of the mean and variance vectors predicted by the transition is uncontentious, i.e., this is
simply how the generative model is deﬁned. However, clamping the value of the ﬁrst element of the mean
and variance vectors predicted by the encoder may be debated. Speciﬁcally, the encoder is supposed to
predict the variational distribution, which is an approximation of the true posterior. However, clamping
the value of the ﬁrst element of the mean and variance vectors predicted by the encoder is likely to push
the variational posterior further from the true posterior.
Qφs(st|ot,st−1)
input: #O + #S
ReLU
size: #S
ReLU
size: #S
THSsize: #S + #S
Pθo(oτ|sτ)
input: #S
ReLU
size: #S
ReLU
size: #S
ReLU
size: #S
LSsize: #O + #O
Pθs(sτ|sτ−1)
input: #S
THSsize: #S + #S
Pθa(aτ|sτ)
input: #S
size: #S
LSsize: #A + #A
Figure 6: Neural networks architecture of the DAIFA agent. Green blocks correspond to fully connected
layers. The ﬁrst neural network is the encoder that takes as input the state at time t−1 and the
observation at time t, and outputs the parameters of a distribution over the state at time t. The second
neural network is the decoder that takes as input the state at time τ, and outputs the parameters of a
distribution over the observation at time τ. The third is the transition network that takes as input the
state at time step τ−1, and outputs the parameters of a distribution over the state at time step τ. The
fourth neural network is the policy network that models the prior over actions, i.e., the policy takes as
input a state at time τ and outputs the parameters of a distribution over the actions at time step τ.
Finally, THS stands for tangent hyperbolic and softplus, i.e., the tangent hyperbolic activation is over
the ﬁrst half of the neurons and the softplus activation function is over the second half, and LS stands
for linear activation function and softplus, i.e., the linear activation is over the ﬁrst half of the neurons
and the softplus activation function is over the second half.
There are a number of important aspects of the DAIFA agent. First, there is no expected free energy,
instead the agent is trained to minimise the cumulated variational free energy over time. Second, this
approach unrolls the partially observable Markov decision process over time. In other words, the code
builds a huge computational graph containing the encoder, decoder, transition and policy networks for
each action-perception cycle. Therefore, the approach is computationally intensive and can become in-
16
Deconstructing deep active inference.
tractable for a large time horizon. Third, the DAIFA requires the modeller to encode the prior preferences
within the distributions predicted by the encoder and transition network. This can limit the applicability
of the approach. Indeed, as previously explained, one can encode the prior preferences of the agent for
the MountainCar problem within the ﬁrst dimension of the latent space.
However, manually encoding the prior preferences in the latent space has two major drawbacks. First,
the model needs to be modiﬁed from one environment to the next. This is because for each environment,
the prior preferences of the agent will be diﬀerent. Second, for some environments, it is unclear how the
prior preferences may be deﬁned. For example, when playing PacMan, the agent needs to eat all the dots,
while simultaneously avoiding the ghosts. How can this be encoded in the model’s latent space? This is
particularly challenging because the only observation made by the agent is an image of the game, i.e., the
agent does not directly have access to the positions of PacMan and the ghosts.
sτ
Pθa(sτ)
σaµaϵ
ˆaτ
env.execute(ˆaτ)
ox
τ+1
Eφs(oτ+1,sτ)
ϵ
ˆµ
ˆσ
ˆsτ+1
µo σo
Dθo(ˆsτ+1)
VFEτ
Tθs(sτ) ˚µ
˚σ
Figure 7: Action-perception cycles (in black) and estimation of the free action objective (in red). Note,
˚µ, ˚σ, ˆµ and ˆσ are used to compute the complexity terms of the variational free energy, while µo and σo
are used to compute the accuracy term of the variational free energy.
2.7 DAIPOMDP agent (van der Himst and Lanillos, 2020)
In this section, we review the approach proposed by van der Himst and Lanillos (2020). The code is avail-
able here: https://github.com/Grottoh/Deep-Active-Inference-for-Partially-Observable-MDPs .
The DAIPOMDP agent is composed of ﬁve deep neural networks.
The decoder Dθo models Pθo(oτ|sτ) as a product of Bernoulli distributions, therefore: Pθo(oτ|sτ) =
Bernoulli(oτ; ˆoτ) where ˆoτ = Dθo(sτ). The transition network Tθs models Pθs(sτ+1|sτ,aτ) as a Gaussian
distribution, i.e., Pθs(sτ+1|sτ,aτ) = N(sτ+1|˚µ,˚σ) where ˚µ,ln˚σ = Tθs(sτ,aτ). The critic Gθa outputs a
vector containing the predicted expected free energy of each action, which is used to deﬁne the prior over
action as Pθa(aτ|sτ) = σ[−ζGθa(sτ,•)], where σ[•] is a softmax function, ζ is the precision of the prior over
actions, and Gθa(sτ,•) is the expected free energy of each action as predicted by the critic network when
state sτ is provided as input. The variational posterior over states Qφs(st) is a Gaussian distribution
modelled by the encoder Eφs, i.e., Qφs(st) = N(st; µ,σ) where µ,ln σ= Eφs(ot). The variational posterior
over actions Qφa(at|st) is a categorical distribution modelled by the policy network Pφa, i.e., Qφa(at|st) =
Cat(at; ˆπ) where ˆπ= Pφa(st). Then, the agent is supposed to minimise the variational free energy deﬁned
17
Champion et al.
as follows:
Q∗
φ(st,at) = arg min
Qφ(st,at)
DKL [ Qφa(at|st)Qφs(st)||Pθo(ot|st)Pθs(st|st−1,at−1)Pθa(at|st)]
= arg min
Qφ(st,at)
DKL [ Qφs(st)||Pθs(st|st−1,at−1)] + DKL [ Qφa(at|st)||Pθa(at|st)] −EQφs(st)[ln Pθo(ot|st)].
However, as explained in the paper, the KL-divergence (over states) is replaced by the mean square error
(MSE) as follows:
Q∗
φ(st,at) = arg min
Qφ(st,at)
MSE(µ,˚µ) + DKL [ Qφa(at|st)||Pθa(at|st)] −EQφs(st)[ln Pθo(ot|st)],
where µ and ˚µ are the mean vectors predicted by the encoder and the transition network, respectively.
The paper justiﬁes this substitution by saying that the maximum a posteriori (MAP) estimate is used to
compute the state prediction error, instead of using the KL-divergence over the densities. However, the
state prediction error and the KL-divergence over states are two diﬀerent quantities, which are only equal
when the two densities over states are Gaussian distributions with identity covariance matrix. However,
the distribution predicted by the encoder network does not have an identity covariance matrix.
Put simply, in this context, the MSE and the KL-divergence between the densities over state are not
necessarily equivalent. As a result, the DAIPOMDP agent may not always follow the free energy principle.
2.8 DAISSM agent (van der Himst and Lanillos, 2020)
The deep active inference agent proposed by (C ¸ atal et al, 2020) is based on a state space model, and is
therefore called DAISSM. The code of this approach was not available online, but we were able to retrieve
it from the authors. Importantly, DAISSM is an oﬄine approach meaning that the model is trained ﬁrst
on a ﬁxed dataset gathered either by taking random actions in the environment or by manually controlling
the robot. Then, when the model is trained, the expected free energy of diﬀerent sequences of actions
can be estimated using imaginary rollouts, and the ﬁrst action of the policy with the lowest expected free
energy is executed in the environment.
2.9 Active exploration for robotic manipulation (Schneider et al, 2022)
The last paper that we review (Schneider et al, 2022) is motivated from a reinforcement learning per-
spective, where the agent aims to maximise reward. However, instead of greedily maximising reward, the
agent also maximises the information gain between the model parameters and the expected states and
rewards, i.e.,
max
π
EP(r|π)
[
f(r)
]
  
Expected reward
+ βEP(s,r|π,st)
[
DKL [ P(θ|s,r,π,s t)||P(θ)]
]
  
Information gain
, where: f(r) =
t+H∑
τ=t+1
rτ (11)
where t is the current time step, H is the time horizon of planning, π is the agent policy, θ are the
parameters of the model, r = rt+1:t+H is the sequence of rewards between time step t+ 1 and t+ H,
s= st+1:t+H is the sequence of states between time step t+ 1 and t+ H, and β is a hyper-parameter
modulating the impact of information gain. Note, the distribution over the sequence of rewardsrobtained
by the agent when behaving according to a policy π, i.e., P(r|π), is not deterministic. Indeed, the same
policy can produce diﬀerent trajectories of states as the transition mapping is stochastic, and those
diﬀerent states can produce diﬀerent rewards. Notice, the expectation w.r.t P(r|π) is iterating over all
possible sequences of rewards, and passing each sequence to f(r). Then, the authors demonstrate the
relationship between Equation 11 and the expected free energy. This relationship is explained in more
details in (Schneider, N.D.), and relies on the the following assumptions:
18
Deconstructing deep active inference.
1. the states sτ and rewards rτ are latent variables for all τ ∈{t+ 1,...,t + H}
2. the generative model contains observed variables for states os
τ and rewards or
τ
3. the likelihood mappings, i.e., P(or
τ|rτ) and P(os
τ|sτ), are delta distributions which eﬀectively make
the latent variables fully observable, except for the model parametersθthat are distributed according
to a delta distribution at time step zero and remain ﬁxed over time
4. the variance of the transition mapping P(sτ+1|sτ,aτ,θ) does not depend on the parameters θ,
e.g., P(sτ+1|sτ,aτ,θ) = N(sτ+1; µ,σ) where µ= fθ(sτ,aτ) is predicted by a neural network with
parameters θ, and σ= σI is a diagonal matrix whose diagonal elements are equal to σ.
Importantly, while the above assumptions allow the authors to derive the expected free energy from
Equation 11, these assumptions also impose a lot of constraints on the model. For example, the proof
does not hold if the likelihood mappings are not delta distributions. In this paper, we focus on such
models. To conclude, this approach is a great contribution to reinforcement learning applied to robotic
control, but cannot be considered as a complete deep active inference agent. The code of this approach
is available at the following URL: https://github.com/TimSchneider42/aerm.
2.10 Representational similarity with centered kernel alignment
The goal of representational similarity metrics is, as its name indicates, to measure the similarity between
two representations. In the context of deep learning, these representations correspond to Rn×p matrices
of activations, where n is the number of data examples and p the number of neurons in a layer. In this
paper, we aim to use such metrics to compare the representations learned by the deep learning models
described in Section 3 and the representations learned by a DQN.
For our analysis, we will use Centred Kernel Alignment (CKA) (Cortes et al, 2012; Cristianini et al,
2002), a normalised version of the Hillbert-Schmidt Independence Criterion (HSIC) (Gretton et al, 2005),
which measures the alignment between the n×n kernel matrices of two representations. Kornblith
et al (2019) have shown that for deep learning applications, linear kernels with centred layer activations
worked well. We thus focus on the linear CKA, also known as RV-coeﬃcient (Robert and Escouﬁer, 1976).
Moreover, it has been shown to provide results similar to other representational similarity metrics, while
being faster to compute (Bonheme and Grzes, 2022).
For conciseness, we will refer to linear CKA as CKA in the rest of this paper. We now deﬁne CKA
more formally. Given the centered layer activations x∈Rn×m and y∈Rn×p taken over ndata examples,
CKA is deﬁned as:
CKA(x,y) = ∥yTx∥2
F
∥xTx∥F∥yTy∥F
,
where ∥·∥F is the Frobenius norm, which is deﬁned as:
∥a∥F =
√
tr(aaT) =
√
k∑
i=1
l∑
j=1
|aij|2,
where a∈Rk×l is an arbitrary k×l matrix, and aT is the transpose of a.
Limitations of CKA While CKA leads to accurate results in practice, it can be overly sensitive to
diﬀerences in neural architectures (Maheswaranathan et al, 2019), and can thus underestimate the simi-
larity between activations coming from layers of diﬀerent type (e.g., convolutional and deconvolutional).
Thus, we will only discuss the variation of similarity when analysing such cases. For example, we will not
compare CKA(a,b) and CKA(a,c) if a and b are convolutional layers but c is linear. We will, however,
compare CKA(a,c) and CKA(b,c).
19
Champion et al.
3. Incrementally building a deep active inference agent
All of the deep active inference models we have presented make important contributions, illustrating a
range of possible implementations. However, we do not feel that any of these approaches is a complete
and deﬁnitive realisation of deep active inference. We have highlighted limitations of these published
approaches throughout our presentation. Accordingly, in the remainder of this paper, we step back
to ﬁrst principles and “build up” an agent component-by-component to determine which parts of a
“natural” deep active inference framework underlie its capacity to solve or fail to solve inference problems.
Additionally, throughout this component-by-component investigation, we compare the diﬀerent variants
of deep active inference that result with a standard (well-attested) approach: a deep Q-network (Mnih
et al, 2013). Thus, in this section, we progresively build a deep active inference agent. Section 3.1 presents
the dSprites environment in which all our simulations will be run. This environment was picked to test
whether an active inference agent is able to solved the dSprites problem, as explored in (Fountas et al,
2020). Section 3.2 describes how the agents introduced later in this paper interact with the environment.
Then, Section 3.3 introduces a variational auto-encoder (VAE) agent, Section 3.4 discusses a deep hidden
Markov model (HMM) agent, Section 3.5 presents a deep critical HMM (CHMM) agent, and ﬁnally,
Section 3.6 introduces a complete deep active inference agent. Note, the notation used throughout this
section are summarised in Appendix A.
3.1 dSprites environment
The dSprites environment is based on the dSprites dataset (Matthey et al, 2017), initially designed for
analysing the latent representation learned by variational auto-encoders (Doersch, 2016). The dSprites
dataset is composed of images of squares, ellipses and hearts. Each image contains one shape (square,
ellipse or heart) with its own scale, orientation, and ( X,Y ) position. In the dSprites environment, the
agent is able to move those shapes around by performing four actions (i.e., UP, DOWN, LEFT, RIGHT).
To make the task tractable, the action selected by the agent is executed eight times in the environment
before the beginning of the next action-perception cycle, i.e., the X or Y position is increased or decreased
by eight between time steptand t+1. The goal of the agent is to move all squares towards the bottom-left
corner of the image and all ellipses and hearts towards the bottom-right corner of the image, c.f. Figure
8.
Figure 8: This ﬁgure illustrates the dSprites environment, in which the agent must move all squares
towards the bottom-left corner of the image and all ellipses and hearts towards the bottom-right corner
of the image. The red arrows show the behaviour expected from the agent.
20
Deconstructing deep active inference.
3.2 Agent-environment interaction
In this section, we present how all the agents introduced in the next sections interact with the environment.
Each agent was trained for N = 500K iterations. At the begining of a trial, the environment is reset to
a random state and the agent receives an observation 5 ot. Using ot, the agent selects an action at, which
is then executed in the environment. This leads the agent to receive a new obervation ot+1, a reward
rt+1 and a boolean done describing whether the trial is over or not. Then, the new experience ( ot, at,
ot+1, rt+1, done) is added to the replay buﬀer, from which a batch is sampled to train the various neural
networks of the agent. Finally, if the trial has ended, then the environment is reset to a random state
leading to a new observation ot, otherwise ot+1 becomes the new ot closing the action-perception cycle.
Algorithm 1 summarises the agent-environment interaction.
Algorithm 1: The interaction between the agent and the environment.
Input: env the environment,
agent the agent,
buffer the replay buﬀer,
N the number of training iterations.
space
ot = env.reset() // Get the initial observation from environment
repeat N times
at ←select action(ot) // Select an action
ot+1,rt+1,done ←env.execute(at) // Execute the action in the environment
buﬀer.push new experience(ot, at, ot+1, rt+1, done) // Add the experience to the
replay buffer
agent.learn(buﬀer) // Perform one iteration of training
if done == Truethen
ot ←env.reset() // Reset the environment when a trial ends
else
ot ←ot+1
end
end
3.3 Variational auto-encoder
In this section, we present our ﬁrst agent based on a variational auto-encoder. The agent is composed of
two deep neural networks, i.e., an encoder and a decoder. The encoder Eφs takes as input an image ot
and outputs the parameters of the variational posterior Qφs(st) = N(st; µ,σ), where µis the mean vector
of the Gaussian distribution, and σ are the diagonal elements of the covariance matrix. The decoder Dθo
models the likelihood mapping Pθo(ot|st), which attributes a probability to each image ot given a state
st, and is deﬁned as:
Pθo(ot|st) = Bernoulli(ot; ˆot),
where ˆot = Dθo(st) are the values predicted by the decoder, and Bernoulli(ot; ˆot) is a product of Bernoulli
distributions deﬁned as:
Bernoulli(ot; ˆot) =
∏
x,y
Bernoulli(ot[x,y]; ˆot[x,y]),
where Bernoulli( •; •) is a Bernoulli distribution over the possible values of the pixelot[x,y], parameterized
by the parameter ˆot[x,y], which is predicted by the decoder network. The goal of the agent is to minimise
5. Each observation contains a sequence of three images, i.e., the image corresponding to the current state of the environment,
and the two images gathered during the previous two time steps.
21
Champion et al.
the variational free energy (VFE):
F = DKL [ Qφs(st)||Pθo(ot,st)] = DKL [ Qφs(st)||Pθo(ot|st)P(st)] ,
where P(st) = N(st; 0,I) is an isotropic (multivariate) Gaussian with variance one. The VFE can be
re-arranged as follows:
F = DKL [ Qφs(st)||P(st)] −EQφs(st)[ln Pθo(ot|st)],
where the KL-divergence between two Gaussian distributions can be computed using an analytical solu-
tion, and the expectation of the logarithm of Pθo(ot|st) is approximated by a Monte-Carlo estimate using
a single sample ˆst ∼Qφs(st). The sample ˆst is obtained using the reparameterisation trick as follows:
ˆst = µ+ σ⊙ˆϵ, where ⊙is an element-wise product between two vectors, and ˆϵ∼N(ϵ; 0,I).
To sum up, this agent takes random actions, and stores its experiences in a replay buﬀer (c.f. Section
3.2). Then, batches of experiences ( ot, at, ot+1, rt+1, done) are sampled from the replay buﬀer. The
observations at time step t are then fed into the encoder, which outputs the mean and log variance of
a Gaussian distribution Qφs(st) = N(st; µ,σ). A latent state is sampled from Qφs(st) using the re-
parameterisation trick, and is then provided as input to the decoder which outputs the parameters of
Bernoulli distributions ˆot. The KL-divergence between Qφs(st) and P(st) is computed analytically, and
the logarithm of Pθo(ot|st) reduces to the binary cross entropy (BCE) because Pθo(ot|st) is a product of
Bernoulli distributions. Next, the VFE is obtained by subtracting the BCE from the KL-divergence, and
back-propagation is used to update the weights of the encoder and decoder networks. Figure 9 illustrates
the VAE agent presented in this section. Note, this agent takes random actions.
ot Encoder
ln σ
µ
ϵ
Decoderˆst ˆot
Figure 9: This ﬁgure illustrates the VAE agent. From left to right, we have the input image ot, the
encoder network, the layer of mean µ and log variance ln σ, the epsilon random variable used for the
reparameterisation trick, the latent state ˆst, the decoder network, and ﬁnally, the reconstructed image
ˆot. Note, there are no actions in this agent’s generative model. Therefore, the VAE agent takes random
actions.
3.4 Deep hidden Markov model
In this section, we present our second agent based on a hidden Markov model. Similarly to the VAE
agent, the HMM agent is composed of an encoder network modelling Qφs(sτ), and a decoder network
modelling Pθo(oτ|sτ). However, the prior over the hidden states at time step t+ 1 depends on the hidden
states and action at time step t. This prior is modelled by the transition network Tθs that predicts
the parameters of the Gaussian distribution Pθs(st+1|st,at) = N(st+1;˚µ,˚σ), where ˚µ is the mean of the
Gaussian distribution, and ˚σ are the diagonal elements of the covariance matrix. Recall, that the goal of
the inference process is to ﬁt the approximate posterior Qφs(st) to the true posterior P(st|ot,st−1,at−1).
Formally, this optimisation can be written as the minimization of the Kullback-Leibler divergence between
22
Deconstructing deep active inference.
the approximate and the true posterior, i.e.,
Q∗(st) = arg min
Qφs(st)
DKL [ Qφs(st)||P(st|ot,st−1,at−1)] .
Using a derivation almost identical to the one presented in Section 2.2.2, the VFE can be proven to be:
Q∗
φs(st) = arg min
Qφs(st)
DKL [ Qφs(st)||Pθo(ot|st)Pθs(st|st−1,at−1)]  
variational free energy
(12)
= arg min
Qφs(st)
DKL [ Qφs(st)||Pθs(st|st−1,at−1)] −EQφs(st)[Pθo(ot|st)].
The VFE of Equation 12 can be computed in a similar way to the VAE agent. Put simply, this agent
takes random actions, and stores its experiences in a replay buﬀer (c.f. Section 3.2). Then, batches of
experiences (ot−1, at−1, ot, rt, done) are sampled from the replay buﬀer. The observations at time step
t−1 are feed into the encoder, which outputs the mean and log variance of a Gaussian distribution
Qφs(st−1) = N(st−1; µ,σ). A latent state is sampled from Qφs(st−1) using the re-parameterisation trick,
and is then provided as input to the transition network along with action at−1. The transition network
outputs the parameters of the Gaussian distributions Pθs(st|st−1,at−1) = N(st;˚µ,˚σ). Additionally, the
observations at time step t can be fed into the encoder to get the parameters of Qφs(st) = N(st; ˆµ,ˆσ).
Sampling from Qφs(st) using the reparameterisation trick gives a state ˆ st that when given as input to
the decoder produces the parameters of a product of Bernoulli distributions ˆ ot+1. The KL-divergence
between Qφs(st) and Pθs(st|st−1,at−1) is computed analytically, and the logarithm of Pθo(ot|st) reduces
to the binary cross entropy (BCE) because Pθo(ot|st) is a product of Bernoulli distributions. Next, the
VFE is obtained by subtracting the BCE from the KL-divergence, and back-propagation is used to update
the weights of the encoder, decoder and transition networks. Figure 10 illustrates the HMM agent.
3.5 Deep critical HMM
In this section, we present our third agent that incorporates a critic network to the deep HMM presented
in the previous section. The resulting model is called a deep CHMM and is illustrated in Figure 11.
The CHMM is equipped with an encoder Eφs modelling Qφs(sτ), a decoder Dθo modelling Pθo(oτ|sτ), a
transition network Tθs modelling Pθs(st|st−1,at−1), and a critic networkGθa that predicts the expected free
energy (see below) of each action. The critic is then used to deﬁne the prior over actions as: Pθa(at|st) =
σ[−ζGθa(st,•)], where ζ is the precision of the prior over actions, and Gθa(st,•) is the EFE of taking
each action in state st as predicted by the critic. The encoder, decoder and transition networks are all
trained in the same way as before to minimise the VFE. The critic however is trained to minimise the
smooth L1 norm between its output Gθa(st,•) and the target G-values y( •), i.e., the critic minimises
SL1[Gθa(st,•),y( •)]. Note, the SL1 was picked because it is less sensitive to outliers than the MSE, and
is deﬁned as:
SL1[x,y] =
{
0.5 ×(x−y)2
β if |x−y|<β
|x−y|−0.5 ×β otherwise
,
where β is an hyper-parameter such that as β goes to zero, the SL1 loss converges to the L1 loss, and
when β tends to + ∞, the SL1 loss converges to a constant zero loss. Intuitively, the SL1 loss uses a
squared term if the absolute element-wise error falls below β, and an L1 term otherwise. Addtionally, the
target G-values are deﬁned as:
y(at) = Gt+1(at) + γEQφs(st+1)
[
max
at+1∈A
ˆGˆθa
(st+1,at+1)
]
,
where Qφs(st+1) can be computed by feeding the image ot+1 sampled from the replay buﬀer as input to
the encoder, Gt+1(at) is the expected free energy at time t+ 1 after taking action at (see below), and γ is
23
Champion et al.
ot
Encoder
ln σµϵ
Decoder
ˆst
ˆot
at
Transition
ln˚σ
˚µ
ϵ
˚st+1
ot+1
Encoder
ln ˆσˆµϵ
Decoder
ˆst+1
ˆot+1
Figure 10: This ﬁgure illustrates the HMM agent. On the left and right, one can see two auto-encoders,
i.e., one at time step t and one at time step t+ 1. In the middle, the transition network takes as
input the state and action at time t, i.e., (ˆst,at), and outputs the mean ˚µ and log variance ln˚σ of a
Gaussian distribution. By sampling the latent variable ϵ, and using the reparameterisation trick, we
get the latent state outputed by the transition network: ˚ st+1 = ˚µ+ ˚σ⊙ˆϵ where ˆϵ is sampled from a
Gaussian distribution with mean zero and variance one. Importantly, the model seems to be composed
of two disconnected parts, however, the variational free energy will have a complexity term between the
Gaussian distributions outputed by the transition network and encoder at time t+ 1. Note, this agent
takes random actions.
a discount factor. Note, the above Equation is an application of Bellman’s equation (Bellman, 1952) to
the expected free energy. Also, as for the DQN agent, we improved the training stability by implementing
a target network ˆGˆθa
, which is structurally identical to the critic and whose weights are synchronised with
the weights of the critic every K (learning) iterations. The last question to answer before focusing on the
subject of the EFE is: how does the CHMM select the action to be performed in the environment?
There are at least four possibilities: (i) select a random action, (ii) select the action that maximises
EFE according to the critic, i.e., a∗
t = arg maxat Gθa(st,at), (iii) sample an action from a softmax function
of the output of the critic, i.e., a∗
t ∼σ[Gθa(st,•)] where σ[ •] is a softmax function, and (iv) use the ˚ϵ-greedy
24
Deconstructing deep active inference.
algorithm with exponential decay, i.e., select a random action with probabilty ˚ϵ or select the best action
with probability 1 −˚ϵ where ˚ϵ starts with a high value and decays exponentially fast.
Critic G
ot
Encoder
ln σµϵ
Decoder
ˆst
ˆot
at
Transition
ln˚σ
˚µ
ϵ
˚st+1
ot+1
Encoder
ln ˆσˆµϵ
Decoder
ˆst+1
ˆot+1
Figure 11: This ﬁgure illustrates the CHMM agent. The only new part is the critic network, which takes
as input the hidden state at time t and ouputs the expected free energy of each action G. Importantly,
the CHMM takes actions based on the EFE.
3.5.1 Expected free energy
In this section, we discuss the deﬁnition of the expected free energy (EFE) before investigating various
ways to implement it in the context of deep active inference. Recently in (Parr and Friston, 2019), the
expected free energy was deﬁned as:
G(π) =
T∑
τ=t+1
Gτ(π) =
T∑
τ=t+1
EP(oτ|sτ)Q(sτ|π)
[
ln Q(sτ|π) −ln P(oτ,sτ)
]
, (13)
where P(oτ|sτ) is the likelihood mapping, Q(sτ|π) is the variational distribution, and in the literature,
P(oτ,sτ) is called the generative model but is better understood as a target distribution encoding the
prior preferences of the agent. Indeed, assuming the standard generative model of active inference (i.e.,
25
Champion et al.
st Critic Gθa(st,•)
st+1 Target ˆGˆθa
(st+1,•)
rt
γ
y( •)
SL1
Figure 12: This ﬁgure illustrates the computation of the critic’s loss function when the critic is only
maximising reward, i.e., when Equation 19 is used for the expected free energy. Brieﬂy, the state st is
fed into the Critic, and the state st+1 is fed into the target network. The critic outputs the G-values for
each action at time t, and the target network outputs the G-values for each action at time t+ 1. Then,
the reward, the discount factor, and G-values of each action at time t+ 1 are used to compute the target
values y( •). Finally, the goal is to minimise the SL1 between the prediction of the critic and the target
values by changing the weights of the critic.
a partially observable Markov decision process), the hidden states sτ should depend on sτ−1 and aτ−1.
This point is important because it impacts the question of whether P(oτ,sτ) is indeed the generative
model, and therefore whether the expected free energy is the expectation of the variational free energy.
According to the free energy principle, an active inference agent must minimise its (variational) free
energy. However, if such a relationship cannot be established between the expected and variational free
energy, then one cannot claim that an agent minimising expected free energy also minimises its variational
free energy. Additionally, we need to re-arrange the deﬁnition of the EFE stated in (13) to allow rewards
to be incorporated:
Gτ(π) = EP(oτ|sτ)Q(sτ|π)
[
ln Q(sτ|π) −ln P(oτ,sτ)
]
= EP(oτ|sτ)Q(sτ|π)
[
ln Q(sτ|π) −ln P(sτ|oτ) −ln P(oτ)
]
≈EP(oτ|sτ)Q(sτ|π)
[
ln Q(sτ|π) −ln Q(sτ) −ln P(oτ)
]
= DKL [ Q(sτ|π)||Q(sτ)]  
epistemic value
−EP(oτ|sτ)Q(sτ|π)
[
ln P(oτ)
]
  
extrinsic value
, (14)
3.5.2 A principled estimate of the EFE at timet+ 1?
Now, the question is how to estimate (14), and we focus on the case where τ = t+ 1. Note, because
τ = t+1, the policy πcontains only one actionat, i.e., π= at. In the tabular version of active inference, the
variational distribution is composed of a factor Q(sτ|π). However, in the deep active inference literature,
26
Deconstructing deep active inference.
the variational distribution does not contain such a factor. Generally, a Monte-Carlo estimate is used as
follows:
Q(st+1|at) = EQφs(st)
[
Pθs(st+1|st,at)
]
≈ 1
N
N∑
i=1
Pθs(st+1|st = ˆsi
t,at), (15)
where ˆsi
t ∼Qφs(st). Importantly, for the expected free energy to be the expectation of the variational
free energy, Q(st+1|at) should be a factor of the variational distribution. However, (15) is estimated using
a factor of the generative model Pθs(st+1|st = ˆsi
t,at). This is a conceptual issue, associated with current
deep active inference approaches, such as Fountas et al (2020). In what follows, we use N = 1 leading to
a simpliﬁed version of the estimate:
Q(st+1|at) ≈Pθs(st+1|st = ˆst,at). (16)
Note, in the above equation, ˆsi
t is denoted ˆst because there is only one sample, i.e., N = 1. At this point,
we have an estimate for Q(st+1|at) and Qφs(st) is the variational distribution. The only missing piece is
an estimate of the extrinsic value. In the tabular version of active inference, the preferences of the agent
can be related to the rewards from the reinforcement learning literature. In this paper, we follow (Costa
et al, 2020b) and deﬁne the prior preferences as:
P(oτ) = exp(ψrτ[oτ])∑
oτ exp(ψrτ[oτ]),
where ψis the precision of the prior preferences, andrτ[oτ] is the reward obtained when making observation
oτ. Taking the logarithm of the above equation leads to:
ln P(oτ) = ψrτ[oτ] −ln
∑
oτ
exp(ψrτ[oτ])
= ψrτ[oτ] + C, (17)
where we used the fact that the summation over all oτ is a normalisation term, i.e., a constant. Using
(17), we can now create an estimate of the extrinsic value as follows:
EPθo(oτ|sτ)Q(sτ|at)
[
ln P(oτ)
]
≈ 1
M
M∑
i=1
ln P(oτ = ˆoτ) = 1
M
M∑
i=1
ψrτ[oτ] + C,
where ˆoτ ∼Pθo(oτ|sτ = ˆsτ) and ˆsτ ∼Q(sτ|at). In what follows, we use M = 1 and discard the constant6,
which leads to a simpliﬁed version of the estimate:
EPθo(oτ|sτ)Q(sτ|at)
[
ln P(oτ)
]
=∆ ψrτ[oτ] =∆ ψrτ,
where we simplied the notation by deno in which all our simulations will be ting rτ[oτ] as rτ. To conclude,
we have the following estimate for the EFE at time τ = t+ 1:
Gt+1(at) ≈DKL [ Q(st+1|at)||Qφs(st+1)] −EPθo(ot+1|st+1)Q(st+1|at)
[
ln P(ot+1)
]
≈DKL [ Pθs(st+1|st = ˆst,at)||Qφs(st+1)] −ψrt+1, (18)
where ˆst ∼Qφs(st), Pθs(st+1|st,at) is known from the generative model, Qφs(st+1) is known from the
variational distribution, the KL-divergence can be estimated using an analytical solution, ψ is a hyper-
parameter modulating the precision of the prior preferences, and rt+1 is the reward obtained at time step
t+ 1. As shown in Figure 12, the reward at time step t+ 1 is used to compute the target values that
must be predicted by the critic network.
6. Removing a constant does not inﬂuence which policy is the best. Indeed, π∗ = arg maxπG(π) = arg maxπG(π) − C.
27
Champion et al.
3.5.3 Other definitions of the EFE at timet+ 1
In the previous section, we have presented what may be a principled way to estimate the EFE. As will be
discussed later in this paper, this estimate of the EFE was not very fruitful empirically. To explore the
range of alternatives, we also experimented with the following deﬁnitions, which contain relatively minor
perturbations of the epistemic value term:
G1
t+1(at) = H[Qφs(st+1)] −H[Pθs(st+1|st = ˆst,at)] −ψrt+1,
G2
t+1(at) = H[Pθs(st+1|st = ˆst,at)] −H[Qφs(st+1)] −ψrt+1,
G3
t+1(at) = DKL [ Qφs(st+1)||Pθs(st+1|st = ˆst,at)] −ψrt+1,
where all the entropy terms and the KL-divergence were computed analytically. Also, we experimented
with simply predicting the (negative) expected future reward as follows:
G4
t+1(at) = −ψrt+1, (19)
which is eﬀectively making the job of the critic identical to the job of the Q-network in the DQN agent
(c.f. Section 2.1 for details). More precisely, they are identical, except for the fact that the Q-network is
taking observations as input, while the critic takes hidden states.
3.6 Deep active inference
In this section, we discuss the full deep active inference (DAI) agent illustrated in Figure 13. Put simply,
this agent is composed of ﬁve deep neural networks, i.e., the encoder, the decoder, the transition, the critic
and the policy network. The decoder Dθo models Pθo(oτ|sτ) as a product of Bernoulli distributions, i.e.,
Pθo(oτ|sτ) = Bernoulli(oτ; ˆoτ) where ˆoτ = Dθo(sτ). The transition network Tθs models Pθs(sτ+1|sτ,aτ) as
a Gaussian distribution, i.e., Pθs(sτ+1|sτ,aτ) = N(sτ+1|˚µ,˚σ) where ˚µ,ln˚σ = Tθs(sτ,aτ). The critic Gθa
outputs a vector containing the predicted expected free energy of each action, which is used to deﬁne the
prior over action as Pθa(aτ|sτ) = σ[−ζGθa(sτ,•)], where σ[•] is a softmax function and ζ is the precision
of the prior over actions. With this in mind, the full generative model of the agent is:
Pθ(oo:T,so:T,ao:T−1) = P(s0)
T∏
τ=0
Pθo(oτ|sτ)
T−1∏
τ=0
Pθs(sτ+1|sτ,aτ)Pθa(aτ|sτ),
where P(s0) = N(s0; µ0,σ0) is a Gaussian prior over initial hidden states. Let t be the present time
step. The DAI agent maintains posterior beliefs over the present states st and action at. The variational
posterior over states Qφs(st) is a Gaussian distribution modelled by the encoder Eφs, i.e., Qφs(st) =
N(st; µ,σ) where µ,ln σ = Eφs(ot). The variational posterior over actions Qφa(at|st) is a categorical
distribution modelled by the policy network Pφa, i.e., Qφa(at|st) = Cat( at; ˆπ) where ˆπ = Pφa(st). The
full variational distribution is therefore deﬁned as:
Qφ(at,st) = Qφa(at|st)Qφs(st).
The variational free energy of the DAI agent is derived in a similar way to the VFE of Section 2.2.2, and
is deﬁned as:
Q∗
φ(st,at) = arg min
Qφ(st,at)
DKL [ Qφ(st,at)||Pθo(ot|st)Pθs(st|st−1,at−1)Pθa(at|st)]  
variational free energy
(20)
= arg min
Qφ(st,at)
EQφs(st)
[
DKL [ Qφa(at|st)||Pθa(at|st)]
]
+ DKL [ Qφs(st)||Pθs(st|st−1,at−1)]
−EQφs(st)[ln Pθo(ot|st)].
28
Deconstructing deep active inference.
Policy ˆπ
Critic G
ot
Encoder
ln σµϵ
Decoder
ˆst
ˆot
at
Transition
ln˚σ
˚µ
ϵ
˚st+1
ot+1
Encoder
ln ˆσˆµϵ
Decoder
ˆst+1
ˆot+1
Figure 13: This ﬁgure illustrates the DAI agent. The only new part is the policy network, which takes
as input the hidden state at time tand ouputs the parameters ˆπ of the variational posterior over actions.
Importantly, the DAI takes actions based on the EFE.
The VFE is therefore a function of st−1, at−1, and ot. Both at−1, and ot can be obtained from the
replay buﬀer, and st−1 can be sampled from the variational distribution predicted by the encoder network
when observation ot−1 is provided as input. Also, the KL-divergences can be computed analytically,
the expectations w.r.t Qφs(st) can be approximated using a Monte-Carlo estimate, and the logarithm of
the likelihood mapping reduces to the binary cross entropy because Pθo(ot|st) is a product of Bernoulli
distributions. Thus, all the VFE terms can be estimated, and the encoder, decoder, transition and policy
29
Champion et al.
networks can be trained to minimise the VFE using gradient descent. The critic’s weights are optimised
as in Section 3.5 using gradient descent to minimise the smooth L1 norm between the critic’s output
Gθa(st,•) and the target G-values y( •), i.e., SL1[ Gθa(st,•),y( •)], where the target G-values are deﬁned
as:
y(at) = Gt+1(at) + γEQφs(st+1)
[
max
at+1∈A
ˆGˆθa
(st+1,at+1)
]
,
where Qφs(st+1) can be computed by feeding the image ot+1 sampled from the replay buﬀer as input to
the encoder, γ is a discount factor, and Gt+1(at) is the expected free energy received at time t+ 1 after
taking action at, i.e.,
Gt+1(at) ≈DKL [ Pθs(st+1|st = ˆst,at)||Qφs(st+1)] −ψrt+1, (21)
where ˆst ∼Qφs(st), ψ is a hyperparamter modulating the precision of the prior preferences, and rt+1 is
the reward obtained at time step t+ 1. Note, we also experimented with other deﬁnitions of the EFE
at time t+ 1 as presented in Section 3.5.3. Finally, with regard to the action selection performed by the
DAI agent, there are at least four possibilities: (i) select a random action, (ii) select the action with the
highest posterior probability according to the policy network, i.e., a∗
t = arg maxat Qφa(at|st), (iii) sample
an action from the posterior over actions, i.e., a∗
t ∼Qφa(at|st), and (iv) use the ˚ϵ-greedy algorithm with
exponential decay, i.e., random action with probabilty ˚ϵ or best action with probability 1 −˚ϵ.
4. Results
In this section, we discuss the results obtained by the DQN agent and each model presented in Section 3 at
solving the dSprites problem. The code that can be used to reproduce all the experiements can be found
on GitHub at the following URL:https://github.com/ChampiB/Challenges_Deep_Active_Inference.
Section 4.1 presents the results obtained by the DQN agent. Section 4.2 presents the VFE obtained by
the VAE agent, and shows the reconstructed images produced by the VAE. Section 4.3 shows the VFE of
the HMM agent as well as the generated sequences of images. Section 4.4 illustrates the VFE obtained
by the CHMM as well as the reward obtained by this model when using diﬀerent action selection schemes
and diﬀerent deﬁnitions for the EFE. Finally, Section 4.5 dicusses the VFE obtained by the DAI agent,
as well as the rewards obtained by this model. Note, each time CKA is used in the following sections, we
sampled 5K data examples, and we used them to compute all the CKA scores.
4.1 DQN agent
In this section, we report the results obtained from the DQN agent. As shown in Figure 14, the DQN was
able to accumulate a total amount of reward of around 50K. This result conﬁrms the correctness of our
implementation, and gives us a baseline which can be used to evaluate the performance of the CHMM and
DAI agents. To better understand the representations learned by a DQN, we compute the CKA scores
between the activations of its layers. We can see in Figure 15 that while the layers closer to the input
retain some similarity with each other, the last two layers learn highly speciﬁc representations.
30
Deconstructing deep active inference.
Total Reward
Training Iterations
Figure 14: This ﬁgure illustrates the cumulated rewards obtained by the DQN agent during the 500K
training iterations.
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6
Model=DQN
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
Figure 15: Value X labels the X-th layer of the value network, i.e., Value 1 is the closest to the input
and Value 6 is the output layer. We can see that the ﬁrst three layers of the DQN learn very similar
representations (CKA is close to 1). The representations learned by the fourth layer start to diverge
(CKA is lower), and the last two layers learn highly speciﬁc representations that are very diﬀerent from
the previous layers (CKA is close to 0), but slightly similar to each other.
4.2 VAE agent
In this section, we report the results obtained by the VAE agent. As shown in Figure 16, the VFE
decreases as training progresses. Also, at the end of the 500K training iterations, the VAE is able to
properly reconstruct the images, c.f., Figure 18. Additionally, since the VAE takes random actions in
the environment, the agent was unable to solve the task and accumulated a total amount of reward of
around -7K. Those results suggest our implementation is correct, and gives us a baseline for the amount
of rewards obtained under random play in the dSprites environment.
31
Champion et al.
VFE
Training Iterations
Figure 16: This ﬁgure illustrates the variational free energy of the VAE agent during the 500K iterations
of training.
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Model=VAE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_varianceModel=VAE
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Model=VAE
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6 Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Figure 17: Encoder X is to the X-th layer of the encoder network, i.e., Encoder1 is the closest to the input
and Encoder 5 is the one just before the mean and variance layers. Encoder mean and Encoder variance
are the mean and variance layers of the encoder network, respectively. (a) shows the similarity between
the representations learned by diﬀerent layers of the encoder of a VAE. (b) shows the similarity between
the representations learned by a DQN and a VAE. Note, both the VAE and DQN take images as input
and need to process them to either learn a compact representation and reconstruct the images or predict
the cumulated reward, respectively. Thus, both learn to represent edges and combination of edges in their
ﬁrst layers.
To observe the representations learned by VAEs, we compute the CKA scores between the activations of
the diﬀerent layers of the encoder. We can see in Figure 17a that the representations are strongly similar
between all layers, with the exception of the mean and variance representations at the output end (last
two layers), similarly to what was observed in Bonheme and Grzes (2022), and is therefore expected. As
illustrated in Figure 17b, these representations are generally similar to those learned by the DQN in early
layers but the representations of the last two layers strongly diﬀer, reﬂecting the diﬀerence of learning
objectives between the VAE and DQN.
32
Deconstructing deep active inference.
Ground Truth (GT):
Reconstruction (R):
Figure 18: This ﬁgure illustrates the reconstructed image produced by the VAE after 500K training
iterations. The columns alternate between the input images and the reconstructed images.
4.3 HMM agent
In this section, we report the results obtained by the HMM agent. As shown in Figure 19, the VFE
decreases as training progresses. By comparing Figure 16 and 19, one can see that the HMM has a
lower VFE than the VAE. This is because the agent has more ﬂexibility regarding the prior, i.e., the
log-likelihood is the same between the two models but the complexity term is smaller for the HMM than
for the VAE. Also, at the end of the 500K training iterations, the HMM is able to properly generate se-
quences of images, c.f., Figure 21. Additionally, since the HMM takes random actions in the environment,
the agent was unable to solve the task and accumulated a total amount of reward of around -7K. These
results suggest that our implementation is correct, and comﬁrm our baseline for the amount of rewards
obtained under random play in the dSprites environment.
Similarly to VAEs, we are interested in observing the representations learned by the encoder of the
HMM, and also by its transition network. The representations learned by the encoder of the HMM fol-
low the same trend as those learned by VAEs with an even more marked dissimilarity between the log
variance of the HMM and the other representations learned by this model, as illustrated in Figures 20a
and 20b. We can further observe in Figure 20a that the transition network learns representations similar
to the mean representation (Encoder mean of HMM) in its ﬁrst three layers, while the representations
learned by the last layer (Transition variance) are not similar to any other representations learned by the
transition or encoder networks. We can also see in Figure 20b that the mean and variance representa-
tions (Encoder mean and Encoder variance) learned by HMMs are diﬀerent from those learned by VAEs,
possibly indicating that the transition network inﬂuences these two representations. Similarly to VAEs,
one can observe in Figure 20c, that the representations learned by the variance layers of the encoder and
transition networks (Encoder variance and Transition variance) are very diﬀerent to the representation
learned by the DQN. In contrast, the ﬁrst four layers of the encoder (Encoder 1 to Encoder 4) are similar
33
Champion et al.
to the representation learned by the ﬁrst layers of the DQN (Value 1 to Value 4), but are very diﬀerent
from the last two layers (Value 5 and Value 6).
VFE
Training Iterations
Figure 19: This ﬁgure illustrates the variational free energy of the HMM agent during the 500K iterations
of training.
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Model=HMM
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_varianceModel=HMM
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Model=VAE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_varianceModel=HMM
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Model=HMM
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6 Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
 (c)
Figure 20: Transition X is to the X-th layer of the transition network, i.e., Transition 1 is the closest
to the input and Transition 2 is the one just before the mean and variance layers. Transition mean and
Transition variance are the mean and variance layers of the transition network, respectively. (a) shows the
similarity between the representations learned by diﬀerent layers of the encoder and transition network
of an HMM. (b) shows the similarity between the representations learned by an HMM and a VAE. (c)
shows the similarity between the representations learned by a DQN and an HMM.
34
Deconstructing deep active inference.
Ground Truth (GT):
Reconstruction (R):
Figure 21: This ﬁgure illustrates the sequences of reconstructed images generated by the HMM after
500K training iterations. The columns alternate between the ground truth images and the reconstructed
images. Time passes vertically (from top to bottom), and within each column, the same action is executed
repeatedly.
4.4 CHMM agent
In this section, we report the results obtained by the CHMM agent, when using diﬀerent action selection
strategies and diﬀerent deﬁnitions of the expected free energy. Figure 22 presents the cumulated rewards
obtained by the CHMM agents using an ˚ ϵ-greedy algorithm for action selection, as well as the total
rewards obtained by the DQN agent. The critic network of the CHMM agents were trained to predict the
ﬁve deﬁnitions of the expected free energy proposed in Section 3.5. Note, the DQN agent performs better
that any of the CHMM agents, and the only agent that solves the task is the CHMM maximising reward
only, i.e., without any information gain. Figure 23 presents the same experiement as Figure 22 except
that the CHMM agents were using softmax sampling for action selection. In this setting, none of the
CHMM agents were able to solve the task. Finally, Figure 24 presents yet again the same experiements
but this time the CHMM agents were selecting the best action according to the critic. In this setting,
only the CHMM maximising reward was able to solve the task. Also, by comparing Figures 22 and 24,
it becomes clear that the CHMM using an ˚ϵ-greedy algorithm performs better than the CHMM selecting
the best action according to the critic. Put simply, the latter suﬀers from a lack of exploration that slows
down its learning.
Additionally, Figure 25 represents the variational free energy of the CHMM agent using the ˚ϵ-greedy
algorithm. All the agents were able to minimise their variational free energy, except the one displayed in
orange whose VFE suddenly became equal to NaN (i.e., Not a Number); this agent was minimising the
expected free energy as deﬁned by G1. Note, G1 is neither the reward nor the “principled” expected free
energy, G1 is one of deﬁnitions that we experimented with to explore alternative deﬁnitions. Also, the
35
Champion et al.
variational free energy of the CHMM agents using softmax sampling and best action selection are not
presented, because their results are very similar to the results shown in Figure 25.
Finally, Figure 26 shows examples of predicted trajectories after a CHMM (maximising reward) was
trained. By comparing with Figure 21, we see that the CHMM does not understand the dynamics of the
environment as well as the HMM agent. This sugguests a conﬂict between the two goals of the agent,
i.e., maximising reward 7 (or expected free energy) and learning a model of the world. More precisely,
Figure 21 shows that an HMM agent taking random actions is able to gather a large diversity of training
examples and learns the dynamic of the environment beautifully, but does not solve the task. In contrast,
the CHMM maximising reward solves the task but learns a poor model of the environment, c.f., Figure
26.
Training iterations
Total Reward
DQN:
CHMM[G4]:
CHMM[G, G2, G3]:
CHMM[G1]:
Figure 22: This ﬁgure illustrates the total amount of reward gathered by the CHMM agents (with ˚ ϵ-
greedy action selection) during the 500K iterations of training. The only two models that were able to
solve the task are the ones maximising reward (without information gain), i.e., the DQN agent in red and
the CHMM whose critic network was predicting only reward in gray.
7. As shown in Figure 12, the reward is used to compute the target values that must be predicted by the critic network.
36
Deconstructing deep active inference.
Figure 23: This ﬁgure illustrates the total amount of reward gathered by the CHMM agents (with softmax
sampling) during the 500K iterations of training. The only model that was able to solve the task is the
DQN agent in red, and all CHMM agents failed.
Figure 24: This ﬁgure illustrates the total amount of reward gathered by the CHMM agents (with best
action selection) during the 500K iterations of training. The only two models that were able to solve
the task are the ones maximising reward (without information gain), i.e., the DQN agent in red and the
CHMM whose critic network was predicting only reward in pink.
37
Champion et al.
CHMM[G4]:
CHMM[G3]:
CHMM[G2]:
CHMM[G1]:
CHMM[G]:
Training iterations
VFE
Figure 25: This ﬁgure illustrates the variational free energy of the CHMM agents during the 500K
iterations of training. All the agents were able to minimise their variational free energy, except the one
displayed in orange which crashed; this agent was minimising the expected free energy as deﬁned by G1.
More precisely, the variational free energy took the value “Not a Number” (NaN), which is visible because
of the thick horizontal line between 270K and 500K training iterations.
Ground Truth (GT):
Reconstruction (R):
Figure 26: This ﬁgure illustrates the sequences of reconstructed images generated by a CHMM (max-
imising reward) after 500K training iterations. The columns alternate between the ground truth images
and the reconstructed images. Time passes vertically (from top to bottom), and within each column, the
same action is executed repeatedly.
38
Deconstructing deep active inference.
4.4.1 How do CHMMs learn?
CHMM with ˚ϵ-greedy action selection As illustrated in Figure 22, only the CHMM whose critic
maximises the reward was able to solve the task. We could thus expect the representations learned by
this CHMM to be closer to those learned by the DQN than those learned by the other CHMMs. We can
see in Figure 28 that the last two layers of the critic network of the CHMM maximising the reward are
indeed a bit more similar to the representations of the last two layers of the DQN than the representations
learned when the critic is minimising the EFE (see intersection of Value 5 and Value 6 with Critic 3 and
Critic 4, i.e., bottom right corner of the matrix). However, the representations learned by the critic of
both CHMMs are still quite diﬀerent from the last two layers of the DQN (CKA is lower than 0.4, bottom
right corner of the matrix, again). Interestingly, the ﬁrst four layers of the CHMM maximising the reward
retain a high similarity with the earlier layers of the DQN (4 ×4 region at upper left), suggesting some
common representations between models. We can further see in Figure 27a that the CKA score between
the encoder, transition and critic networks is higher or equal to 0.6 (except for the variance layer of the
transition and the last layer of the critic), indicating that the transition and critic networks of the CHMM
maximising the reward retain some information from the encoder. The information retained by the ﬁrst
three layers of the critic when the CHMM minimises the EFE is much lower, as illustrated in Figures 27b.
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=Reward
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=EFE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=EFE
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=EFE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
 (c)
Figure 27: (a) shows the similarity between the representations learned by diﬀerent layers of the encoder, transition
and critic networks of a CHMM whose critic maximises the reward (with˚ϵ-greedy selection). (b) shows the similarity
between the representations learned by diﬀerent layers of the encoder, transition and critic networks of a CHMM
whose critic minimises the EFE (with˚ϵ-greedy selection) (c) shows the similarity between the representations learned
by two CHMMs, one whose critic optimises EFE and the other optimises reward (both with ˚ϵ-greedy selection).
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=Reward
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6 Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action= -greedy,
 Gain=EFE
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6 Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Figure 28: (a) shows the similarity between the representations learned by a CHMM whose critic maximises the
reward (with ˚ϵ-greedy selection) and a DQN; (b) shows the similarity between the representations learned by a
CHMM whose critic minimises the EFE (with ˚ϵ-greedy selection) and a DQN
39
Champion et al.
CHMM with best action selection As illustrated in Figure 24, only the CHMM whose critic max-
imises the reward was able to solve the task. However, one can see in Figure 29 that the CHMM
whose critic minimises the EFE learns representations similar to those of the CHMM maximising the
reward in most layers, with the exception of the variance layer of the encoder and transition network
(Encoder variance and Transition variance). To better understand the diﬀerences between the represen-
tations learned by the variance layer of both models, we fed 5K state-action pairs through the transition
network, and displayed the distribution of the variances outputed by the transition network. This analysis
reveals that the variance (of the variance layer) of the transition network is very small and does not change
much when maximising the reward but is larger and varies more when minimising the EFE as illustrated
in Figure 30. This reﬂects a higher uncertainty of the transitions for the CHMM minimising EFE. More
speciﬁcally, the CHMM minimising EFE seems to be conﬁdent for the action down, but is very uncertain
for all the other actions, which suggests that the CHMM minimising EFE always picks the action down
and does not gather enough data for the other actions.
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
 Gain=Reward
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
 Gain=EFE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
 Gain=EFE
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
 Gain=EFE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Best Action,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
 (c)
Figure 29: (a) shows the similarity between the representations learned by diﬀerent layers of the encoder, transition
and critic networks of a CHMM whose critic maximises the reward (with best action selection). (b) shows the
similarity between the representations learned by diﬀerent layers of the encoder, transition and critic networks
of a CHMM whose critic minimises the EFE (with best action selection) (c) shows the similarity between the
representations learned by two CHMMs, one whose critic optimises EFE and the other that optimises reward (both
with best action selection).
Variance of the latent variable at index 9
(a)
 (b)
Figure 30: (a) shows one latent dimension of the variance layer of the transition network for the CHMM maximising
the reward. (b) shows one latent dimension of the variance layer of the transition network for the CHMM minimising
the EFE. Both ﬁgures are typical of the distributions of variance activations in the two models. Note, only the
action down has low variance for the CHMM minimising the EFE. This suggests that the CHMM minimising the
EFE always picks the action down, and does not gather enough data for the other actions.
40
Deconstructing deep active inference.
CHMM with softmax action selection As illustrated in Figure 23, none of the CHMMs with
softmax action selection were able to solve the task. Once again, the variance (of the variance layer) of the
transition network is very diﬀerent in the CHMM whose critic minimises the EFE compared to the CHMM
whose critic maximises the reward (see Figure 31c at the intersection of the two Transition variances).
We further observe the same trend regarding the uncertainty of the output of the transition network when
optimising the EFE, as shown in Figure 32. While this may explain why the model optimising the EFE
does not solve the task, this does not indicate why the model maximising the reward cannot solve the
task, and we can hypothesise that those results may be attributed to the softmax action selection. More
precisely, if the values predicted by the critic network are very close to each other, then an agent using
softmax sampling may perform random actions.
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=Reward
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=EFE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=EFE
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=EFE
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
 (c)
Figure 31: (a) shows the similarity between the representations learned by diﬀerent layers of the encoder,
transition and critic networks of a CHMM whose critic maximises the reward (with softmax action se-
lection). (b) shows the similarity between the representations learned by diﬀerent layers of the encoder,
transition and critic networks of a CHMM whose critic minimises the EFE (with softmax action selection).
(c) shows the similarity between the representations learned by two CHMMs, one whose critic optimises
EFE and the other that optimises reward (both with softmax action selection).
4.4.2 Degenerate behaviour with the expected free energy?
Up to now, we saw that the CHMM minimising expected free energy (EFE) was not able to solve the
task. Also, we discovered that the transition network is uncertain for the actions: up, left, and right,
which suggests that the CHMM minimising EFE always takes action down. Figure 33 corroborates this
story. Indeed, Figure 33a shows that the agent minimising EFE almost exclusively picked action down,
and Figure 33b shows that the entropy of the prior over actions very quickly converges to zero.
In contrast, the CHMM maximising reward, keeps on selecting the actions right and left, which enables
it to drag the shape towards the appropriate corner (see Figure 33c). Also, as shown in Figure 33d, the
entropy of the prior over actions remained a lot higher than zero. Note, the only diﬀerence between the
CHMM minimising EFE and the one maximising reward is the information gain, which is deﬁned as the
KL divergence between the output of the transition and encoder networks. Since the EFE is minimised,
the output of those two networks need to be as close as possible to each other.
41
Champion et al.
Variance of the latent variable at index 9
(a)
Variance of the latent variable at index 9 in which all our simulations will be
(b)
Figure 32: (a) shows one latent dimension of the variance layer of the transition network for the CHMM maximising
the reward. (b) shows one latent dimension of the variance layer of the transition network for the CHMM minimising
the EFE. Both ﬁgures are representative of the distributions of variance activations in the two models. Note, only
the action down has low variance for the CHMM minimising the EFE. This suggests that the CHMM minimising
the EFE always pick the action down, and does not gather enough data for the other actions.
(a)
0 100000 200000 300000 400000 500000
Training iterations
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4Entropy of prior over actions (b)
0 100000 200000 300000 400000 500000
Training iterations
Down
Right
Left
Up
(c)
0 100000 200000 300000 400000 500000
Training iterations
1.26
1.28
1.30
1.32
1.34
1.36
1.38Entropy of prior over actions (d)
Figure 33: (a) shows the action taken for each planning iteration when the CHMM is minimising expected free
energy. (b) shows the entropy of the prior over actions when the CHMM is minimising the EFE. (c) shows the
action taken for each planning iteration when the CHMM is maximising reward. (d) shows the entropy of the prior
over actions when the CHMM is maximising reward.
42
Deconstructing deep active inference.
This suggests that the CHMM minimising EFE is picking a single action (down), and becomes an
expert at predicting the future when selecting this action. This eﬀectively makes the KL divergence
between the output of the transition and encoder networks small. Additionally, when selecting the action
down, the average reward is zero, because (in the dSprites dataset) there are as many shapes on the left
of the image as on the right, and when crossing the bottom line, the agent receives a reward which is
linearly increasing (or decreasing) as a corner is approached and is zero at the center of the image. For all
the other actions, the expected reward will be negative because after 50 action-perception cycles without
crossing the bottom line, the trial is interrupted and the agent receive a reward of -1. Thus, if the CHMM
has to stick to a single action to keep the KL divergence small, then the best action it can choose is down,
i.e., action down has the highest expected reward.
Also, Figure 34 shows the impact of adding X% of the information gain into the objective function,
i.e., the agent starts by only maximising reward (c.f. Equation 19), and after 200K training iterations
minimises reward plus X% of the information gain. One can see that adding even 1% of the information
gain already dramatically decreases the amount of reward gathered.
To conclude, the same information gain that is intended to give an EFE minimising agent its explo-
ration behaviour, also prevents the agent from solving the dSprites environment. This is because the
agent is reduced to picking a single action, leading to a suboptimal policy.
CHMM[0%]:
CHMM[1%]:
CHMM[5%]:
CHMM[15%]:
CHMM[25%]:
CHMM[50%]:
Training iterations
Total Reward
Figure 34: This ﬁgure illustrates the total reward aggregated by CHMM agents during the 500K iterations
of training. All the agents start by only maximising reward, and after 200K training iterations, X% of
the information gain is added to the objective function. Note, even adding 1% of the information gain
is enough to drastically reduce the total reward aggregated by the agent. The diﬀerences in trajectories
before 200K are arbitrary, arising from diﬀerences in random initializations.
4.5 DAI agent
In this section, we report the results obtained by the DAI agent, when using diﬀerent action selection
strategies and diﬀerent deﬁnitions of the expected free energy. First, most of the ﬁfteen DAI agents
crashed because of numerical instability, i.e., the VFE suddenly became “Not a Number”. The only DAI
agent that survived (i.e., did not crash) was maximising rewards while performing softmax sampling for
action selection. Figure 38 shows that the DAI agent successfully minimises its variational free energy,
but as shown in Figure 37, the DAI agent does not solve the task and performs as well as a random agent.
Finally, Figure 35 shows sequences of images produced by the DAI agent after 500K training iterations.
43
Champion et al.
Note, while the agent does not solve that task, it understands the dynamics of the environment pretty
well. However, the agent struggles with images representing hearts.
By comparing Figures 21, 26 and 35, we see that the DAI agent with softmax sampling has a better
reconstruction than the CHMM agent with the ˚ ϵ-greedy algorithm (which is presented in Figure 26).
In contrast, the DAI agent does not reconstruct the sequences of images as well as the HMM agent
performing random actions (which is presented in Figure 21).
Ground Truth (GT):
Reconstruction (R):
Figure 35: This ﬁgure illustrates the sequences of reconstructed images generated by the DAI after
500K training iterations. The columns alternate between the ground truth images and the reconstructed
images. Time passes vertically (from top to bottom), and within each column, the same action is executed
repeatedly.
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=Reward
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6 Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Policy_1
Policy_2
Policy_3
Policy_4
Critic_1
Critic_2
Critic_3
Critic_4
Model=DAI, Action=Softmax,
 Gain=Reward
Value_1
Value_2
Value_3
Value_4
Value_5
Value_6 Model=DQN
0.0
0.2
0.4
0.6
0.8
1.0
 (b)
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Policy_1
Policy_2
Policy_3
Policy_4
Critic_1
Critic_2
Critic_3
Critic_4
Model=DAI, Action=Softmax,
 Gain=Reward
Encoder_1
Encoder_2
Encoder_3
Encoder_4
Encoder_5
Encoder_mean
Encoder_variance
Transition_1
Transition_2
Transition_mean
Transition_variance
Critic_1
Critic_2
Critic_3
Critic_4
Model=CHMM, Action=Softmax,
 Gain=Reward
0.0
0.2
0.4
0.6
0.8
1.0
 (c)
Figure 36: (a) shows the similarity between the representations learned by a DQN and a CHMM maximis-
ing the reward and using softmax action selection. (b) shows the similarity between the representations
learned by a DQN and a DAI maximising the reward and using softmax action selection. (c) shows the
similarity between the representations learned by a CHMM and a DAI. Both maximise the reward and
use softmax action selection.
44
Deconstructing deep active inference.
As previously mentioned, the only DAI that did not crash during training used softmax action selection
and had a critic maximising reward. We can see in Figures 36b and 36a that the representational similarity
between this DAI and DQN is very close to the representational similarity between a DQN and a CHMM
using the same action selection and maximising reward. This is further conﬁrmed by a comparison
between the CHMM and the DAI model in Figure 36c. Interestingly, we can see that the policy and critic
network learn similar representations, indicating that the policy network is learning correctly. However,
we previously inferred that the softmax action selection may be suboptimal and this seems to hold true
for the DAI as well, given that it is unable to solve the task.
Total rewards
Training Iterations
DQN
DAI[G4]
Figure 37: This ﬁgure illustrates the total amount of reward gathered by a DAI agent during the 500K iterations of
training. This agent was maximising rewards while sampling actions from a softmax function of the policy network
output. Put simply, the DAI agent does not solve the task and performs at the level of a random agent.
VFE
Training Iterations
Figure 38: This ﬁgure illustrates the variational free energy of the DAI agent during the 500K iterations of training.
This agent was maximising reward while sampling actions from a softmax function of the policy network output.
The agent was able to minimise its variational free energy.
45
Champion et al.
5. Discussion of epistemic value
In this section, we discuss the decomposition of the expected free energy into epistemic and extrinsic
value. More precisely, intrigued by the poor results of deep active inference agents (e.g., CHMM and
DAI), we seek to understand the damaging impact of the epistemic value on performance. Assuming the
following deﬁnition for the epistemic value:
EV = EP(oτ|sτ)Q(sτ|π)
[
ln P(sτ|oτ) −ln Q(sτ|π)
]
,
we set up two experiments (c.f., Appendix C for details) in which all the disributions are stored in tables.
In other words, the following categorical distributions are represented using matrices: P(oτ|sτ), Q(sτ|π),
and P(sτ|oτ).
In the ﬁrst experiement, the prior over states P(sτ) is ﬁxed, and the likelihood P(oτ|sτ) is becoming
more and more uniform. While this is happening, the true posterior P(sτ|oτ) becomes more similar to
the approximate posterior Q(sτ|π), and the epistemic value decreases; see left panel of Figure 39. This is
the expected behaviour, and in this setting, the epistemic value encourages exploration.
In the second experiment, the likelihood P(oτ|sτ) is ﬁxed and has rather high entropy, while the
prior over states P(sτ) is shifted in one direction on the state axis. As a result of this, the true posterior
P(sτ|oτ) becomes more diﬀerent to the approximate posterior Q(sτ|π), but this causes the epistemic value
to decrease; see Figure 39, right panel. This is a degenerate behaviour, as in this setting, the epistemic
value discourages exploration.
To sum up, the expected free energy decomposition into epistemic and extrinsic value — as presented
in Equation (10) of Parr and Friston (2019) — seems to exhibit two very diﬀerent behaviours depending
on how the distributions are deﬁned, c.f., Figure 39. This is particularly important in the deep active
inference literature, which builds on this equation. For example, the graphs presented in Section 4.4.2
indicate that the CHMM agent minimising expected free energy is focusing almost exclusively on the
action “down”, i.e., it is not exploring, which leads to poor performance.
6. Conclusion
In this paper, we challenged the common assumption that deep active inference is a solved problem, by
highlighting the challenges that need to be resolved for the ﬁeld to move forward. We reviewed eight
approaches implementing deep active inference: (a) DAIMC by Fountas et al (2020), (b) DAIVPG by
Millidge (2020), (c) DAIRHI by Rood et al (2020), (d) DAIHR by Sancaktar et al (2020), (e) DAIFA by
Ueltzh¨ oﬀer (2018), (f)DAIPOMDP by van der Himst and Lanillos (2020), (g) DAISSM by C ¸ atal et al
(2020), and (h) the approach by Schneider et al (2022) for which the code was not available online.
Overall, those approaches brought interesting ideas such as: using deep neural networks to predict the
parameters of the distributions of interest, using Monte-Carlo tree search for planning in active inference,
and using a bootstrap estimate of the expected free energy to train a critic network. Yet, we struggled
to replicate some of the results claimed, e.g. training DAIMC on the animal AI environment, and we
were unable to access code in some instances. Ideally, future research should draw inspiration from the
open science framework, by making the code that produced the claimed results open source. Also, the
deﬁnition of the expected free energy varied between papers. This suggests that additional research is
required to clarify the deﬁnition and justiﬁcation of the expected free energy both in tabular and deep
active inference (c.f., Section 5 and Appendix C). To sum up, recent research on deep active inference
(Fountas et al, 2020; Millidge, 2020; Rood et al, 2020; Sancaktar et al, 2020; Ueltzh¨ oﬀer, 2018; van der
Himst and Lanillos, 2020; C ¸ atal et al, 2020) has made an important ﬁrst step towards a complete deep
active inference agent, but a number of details still need to be honed, e.g., the deﬁnition and derivation
of the expected free energy, and reproducibilty of research. For more details, the reader is referred to
Section 2.
46
Deconstructing deep active inference.
Epistemic Value
Similarity of posteriors: P(sτ|oτ) and Q(sτ|π)
 Epistemic Value
Similarity of posteriors: P(sτ|oτ) and Q(sτ|π)
Figure 39: The left-most ﬁgure shows the result of the ﬁrst experiment where the likelihood was becoming
more and more uniform. In this setting, the epistemic value encourages exploration, as maximising the
epistemic value makes the true posterior P(sτ|oτ) as diﬀerent as possible to the approximate posterior
Q(sτ|π), leftward on x-axis. However, the right-most ﬁgure illustrates the result of the second experiment
where the prior over states was changing. In this case, the epistemic value does not promote exploration, as
maximising the epistemic value makes the true posteriorP(sτ|oτ) as similar as possible to the approximate
posterior Q(sτ|π), rightward on x-axis. Indeed, in this case, maximising the epistemic value causes
information to be lost.
After reviewing existing research, we tried to progressively implement a deep active inference agent.
First, we produced a variational auto-encoder agent that takes random actions. This agent was able to
learn a useful (latent) representation of the task. However, since the agent takes random actions, it was
unable to solve the task. Then, we added the transition network to create the hidden Markov model
(HMM) agent, which also takes random actions. The agent was able to learn a good represenation of the
task and of its dynamics, but was not able to solve the task.
Next, we tried to incorporate a critic network into the approach leading to the critical hidden Markov
model (CHMM). In this context, we experimented with several possible implementations of the expected
free energy. We also tried to remove the information gain and simply predict the reward. Additionally,
we implemented three types of action selection strategies, namely: best action according to the expected
free energy, softmax sampling, and epsilon-greedy.
When the epsilon-greedy algorithm was used, only the agent maximising reward was able to solve the
task. However, the agent requires more training iterations than a simple deep Q-network to learn the right
behaviour. This may be explained by the fact that the CHMM not only has to learn to solve the task,
but also learn the dynamics of the environment. When softmax sampling was used, all the agents failed
to solve the task. One of them perfoming even worse than an agent selecting random actions. Lastly,
when selecting the best action, only the reward maximising agent was able to solve the task. Importantly,
according to our experiments, the agent using the epsilon-greedy algorithm received the highest amount of
cumulated rewards and learned to solve the task the fastest. Additionally, the reward maximising agents
properly solve the task, but the quality of their latent representation is not as good as that of an HMM
agent. This may be due to the fact that when performing reward maximising actions, the data available
to learn the model of the environment lacks diversity, i.e., not enough exploration.
Next, we tried to incorporate a policy network into the approach leading to a complete deep active
inference agent (DAI). As for the CHMM agent, we experiemented with several possible implementations
47
Champion et al.
of the expected free energy, tried to remove the information gain and simply predict the reward, and
implemented three types of action selection strategies, namely: best action according to the expected free
energy, softmax sampling, and epsilon-greedy algorithm. When the epsilon-greedy algorithm was used or
the best action was selected, all agents failed to solve the task. When using softmax sampling, most of
the agents were numerically unstable and crashed, and the remaining agents failed to solve the task.
Finally, we compared the similarity of the representation learned by the layers of various models (e.g.,
deep Q-network, CHMM, DAI, etc...) using centered kernel alignment. This reveals that the DQN learns
general features in its ﬁrst few layers, and very specialised features in its last two layers. The VAE learns
similar features to the DQN in the ﬁrst layers, but diﬀers from the DQN in the last two layers, reﬂecting
the diﬀerence in learning objectives. Similarly, the HMM learns similar features to the DQN in the ﬁrst
layers, but diﬀers from the representation learned by the DQN in the last two layers. Also, the mean and
variance representations learned by the HMM are diﬀerent from their VAE counterparts, which suggests
that the transition network inﬂuences the latent representation of the model.
Additionally, when using the ˚ϵ-greedy algorithm for action selection, the representations learned by
the CHMM maximising reward is closer to the DQN than the CHMM minimising expected free energy is
to the DQN. Importantly, the critic network of the reward maximising CHMM retains more information
from the encoder than the CHMM minimising expected free energy. When the best action (according
to the critic network) is selected, the CHMM maximising reward and the CHMM minimising expected
free energy learn very similar representations except for the variance layers of the transition and encoder
network. While performing further inspection of those (variance) layers, we found that the transition
network of the reward maximising CHMM is a lot more certain than the transition network of the
CHMM minimising expected free energy. More precisely, the CHMM minimising expected free energy is
only conﬁdent about the world transition when performing action down. This suggests that the CHMM
minimising expected free energy always picks the action down, and does not gather enough data for the
other actions. Visualising the distribution of actions selected as training progresses corroborates this
story by showing that the agent minimising EFE almost exclusively picks action down. In contrast, the
CHMM maximising reward, keeps on selecting the actions left and right, which enables it to successfully
solve the task. The only diﬀerence between those two CHMMs is the epistemic value, which aims to make
the outputs of the transition and encoder network as close as possible. Thus, the CHMM minimising
expected free energy is picking a single action (down), and becomes an expert at predicting the future
when selecting this action. This eﬀectively makes the KL divergence between the output of the transition
and encoder networks small. Additionally, when selecting the action down, the average reward is zero,
while for all the other actions, the expected reward will be negative. Therefore, if the CHMM has to stick
to a single action to keep the KL divergence small, then the action down is the most rewarding.
The same observations about the variance layers also applies to CHMMs using softmax sampling for
action selection. While this may explain why the model optimising the expected free energy does not
solve the task, it does not explain why the model maximising the reward cannot solve the task, and we
can hypothesise that those results may be due to the softmax action selection. More precisely, if the
values predicted by the critic network are very close to each other, then an agent using softmax sampling
may perform random actions. Note, increasing the gain parameter may help the agent to diﬀerentiate
between values close to each other.
In addition, the representational similarity between the DAI (maximising reward using softmax sam-
pling) and DQN is very close to the representational similarity between a DQN with a CHMM (maximising
reward using softmax sampling). Also, the DAI’s policy and critic network learn similar representations,
which indicates that the policy network is learning correctly. Thus, the fact that the DAI (maximising
reward using softmax sampling) fails in the dSprites environment is likely due to the softmax action
selection and not to the representation learned by the model.
Lastly, our investigations of the expected free energy often used in deep active inference (Fountas et al,
2020), suggests that degenerate behaviour can arise from it, in certain situations. This could explain why
48
Deconstructing deep active inference.
adding epistemic value to our planning objective seems to have such a damaging impact on the agent’s
capacity to explore its environment and gain information. The use of this deﬁnition of the expected free
energy in the deep learning context may be at the core of our diﬃculty getting deep active inference to
work and may also explain some of the presentational uncertainties (e.g., additions of minus signs) found
in the deep active inference literature.
To conclude, the ﬁeld of deep active inference has beneﬁted from a large variety of ideas from the
reinforcement and deep learning literature. In the future, it would be valuable to provide an approach
that satisﬁes the following ﬁve desirata: (i) the approach is complete, i.e., it is composed of an encoder,
a decoder, a transition network, a policy network and (optionally) a critic network, (ii) the mathematics
underlying the approach is errorless and consistent with the free energy principle, (iii) the implementation
is consistent with the mathematics, (iv) the code is publicly available so that the correctness of the
implementation can be veriﬁed and the results reproduced, and (v) the approach is able to solve tasks
with a large input space, e.g. image-based tasks. We believe that such an approach will beneﬁt the ﬁeld
of deep active inference by providing a strong and reproducible baseline against which future research
could benchmark.
Acknowledgments
We thank Karl J. Friston, Thomas Parr, Lancelot Da Costa, and Zafeirios Fountas for useful discussions
and feedback surronding this paper, as well as for pointing us towards important resources such as their
new book and several papers.
References
Bellman R (1952) On the theory of dynamic programming. Proc Natl Acad Sci U S A 38(8):716–719
Bonheme L, Grzes M (2022) How do Variational Autoencoders Learn? Insights from Representational
Similarity. arXiv e-prints arXiv:2205.08399, 2205.08399
Browne CB, Powley E, Whitehouse D, Lucas SM, Cowling PI, Rohlfshagen P, Tavener S, Perez D,
Samothrakis S, Colton S (2012) A survey of monte carlo tree search methods. IEEE Transactions on
Computational Intelligence and AI in Games 4(1):1–43
C ¸ atal O, Wauthier S, De Boom C, Verbelen T, Dhoedt B (2020) Learning generative state space models
for active inference. Frontiers in Computational Neuroscience 14:574,372
Champion T, Grze´ s M, Bowman H (2021) Realizing Active Inference in Variational Message Passing: The
Outcome-Blind Certainty Seeker. Neural Computation 33(10):2762–2826, DOI 10.1162/neco a 01422,
URL https://doi.org/10.1162/neco_a_01422, https://direct.mit.edu/neco/article-pdf/33/
10/2762/1963368/neco_a_01422.pdf
Champion T, Bowman H, Grze´ s M (2022a) Branching time active inference: Empirical study and com-
plexity class analysis. Neural Networks 152:450–466, DOI https://doi.org/10.1016/j.neunet.2022.05.010,
URL https://www.sciencedirect.com/science/article/pii/S0893608022001824
Champion T, Da Costa L, Bowman H, Grze´ s M (2022b) Branching time active inference: The theory and
its generality. Neural Networks 151:295–316, DOI https://doi.org/10.1016/j.neunet.2022.03.036, URL
https://www.sciencedirect.com/science/article/pii/S0893608022001149
Champion T, Grze´ s M, Bowman H (2022c) Branching Time Active Inference with Bayesian Filtering.
Neural Computation 34(10):2132–2144, DOI 10.1162/neco a 01529, URL https://doi.org/10.1162/
49
Champion et al.
neco_a_01529, https://direct.mit.edu/neco/article-pdf/34/10/2132/2042425/neco_a_01529.
pdf
Champion T, Grze´ s M, Bowman H (2022d) Multi-modal and multi-factor branching time active inference.
DOI 10.48550/ARXIV.2206.12503, URL https://arxiv.org/abs/2206.12503
Cortes C, Mohri M, Rostamizadeh A (2012) Algorithms for Learning Kernels Based on Centered Align-
ment. J Mach Learn Res 13(1):795–828
Costa LD, Parr T, Sajid N, Veselic S, Neacsu V, Friston K (2020a) Active inference on discrete state-
spaces: a synthesis. 2001.07203
Costa LD, Sajid N, Parr T, Friston K, Smith R (2020b) The relationship between dynamic programming
and active inference: the discrete, ﬁnite-horizon case. 2009.08111
Cristianini N, Shawe-Taylor J, Elisseeﬀ A, Kandola JS (2002) On Kernel-Target Alignment. In:
Dietterich TG, Becker S, Ghahramani Z (eds) Advances in Neural Information Processing Sys-
tems, vol 14, MIT Press, Vancouver, Canada, pp 367–373, URL http://papers.nips.cc/paper/
1946-on-kernel-target-alignment.pdf
Cullen M, Davey B, Friston KJ, Moran RJ (2018) Active inference in openai gym: A paradigm for
computational investigations into psychiatric illness. Biological Psychiatry: Cognitive Neuroscience
and Neuroimaging 3(9):809 – 818, DOI https://doi.org/10.1016/j.bpsc.2018.06.010, URL http://www.
sciencedirect.com/science/article/pii/S2451902218301617, computational Methods and Mod-
eling in Psychiatry
Doersch C (2016) Tutorial on variational autoencoders. 1606.05908
FitzGerald THB, Dolan RJ, Friston K (2015) Dopamine, reward learning, and active inference. Frontiers in
Computational Neuroscience 9:136, DOI 10.3389/fncom.2015.00136, URL https://www.frontiersin.
org/article/10.3389/fncom.2015.00136
Fountas Z, Sajid N, Mediano PAM, Friston K (2020) Deep active inference agents using Monte-Carlo
methods. 2006.04176
Friston K, FitzGerald T, Rigoli F, Schwartenbeck P, Doherty JO, Pezzulo G (2016) Active inference and
learning. Neuroscience & Biobehavioral Reviews 68:862 – 879, DOI https://doi.org/10.1016/j.neubiorev.
2016.06.022
Gretton A, Bousquet O, Smola A, Sch¨ olkopf B (2005) Measuring Statistical Dependence with Hilbert-
Schmidt Norms. In: Jain S, Simon HU, Tomita E (eds) Algorithmic Learning Theory, Springer Berlin
Heidelberg, pp 63–77
van Hasselt H, Guez A, Silver D (2015) Deep reinforcement learning with double Q-learning. 1509.06461
Higgins I, Matthey L, Pal A, Burgess C, Glorot X, Botvinick M, Mohamed S, Lerchner A (2017) beta-
VAE: Learning basic visual concepts with a constrained variational framework. In: 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings, OpenReview.net, URL https://openreview.net/forum?id=Sy2fzU9gl
van der Himst O, Lanillos P (2020) Deep active inference for partially observable mdps. CoRR
abs/2009.03622, URL https://arxiv.org/abs/2009.03622, 2009.03622
50
Deconstructing deep active inference.
Itti L, Baldi P (2009) Bayesian surprise attracts human attention. Vision Research 49(10):1295 – 1306,
DOI https://doi.org/10.1016/j.visres.2008.09.007, URL http://www.sciencedirect.com/science/
article/pii/S0042698908004380, visual Attention: Psychophysics, electrophysiology and neuroimag-
ing
Kingma DP, Welling M (2014) Auto-Encoding Variational Bayes. In: International Conference on Learn-
ing Representations, Banﬀ, Canada, vol 2, URL http://arxiv.org/abs/1312.6114
Koller D, Friedman N (2009) Probabilistic graphical models: principles and techniques. MIT press
Kornblith S, Norouzi M, Lee H, Hinton G (2019) Similarity of Neural Network Representations Revisited.
In: Chaudhuri K, Salakhutdinov R (eds) Proceedings of the 36th International Conference on Machine
Learning, PMLR, Long Beach, USA, Proceedings of Machine Learning Research, vol 97, pp 3519–3529,
URL http://proceedings.mlr.press/v97/kornblith19a.html
Lample G, Chaplot DS (2016) Playing fps games with deep reinforcement learning. 1609.05521
Lanillos P, Cheng G, et al (2020) Robot self/other distinction: active inference meets neural networks
learning in a mirror. arXiv preprint arXiv:200405473
Maheswaranathan N, Williams A, Golub M, Ganguli S, Sussillo D (2019) Universality and individu-
ality in neural dynamics across large populations of recurrent networks. In: Wallach H, Larochelle
H, Beygelzimer A, d 'Alch´ e-Buc F, Fox E, Garnett R (eds) Advances in Neural Information Process-
ing Systems, Curran Associates, Inc., vol 32, URL https://proceedings.neurips.cc/paper/2019/
file/5f5d472067f77b5c88f69f1bcfda1e08-Paper.pdf
Matthey L, Higgins I, Hassabis D, Lerchner A (2017) dsprites: Disentanglement testing sprites dataset.
https://github.com/deepmind/dsprites-dataset/
Millidge B (2019) Combining active inference and hierarchical predictive coding: A tutorial introduction
and case study. URL https://doi.org/10.31234/osf.io/kf6wc
Millidge B (2020) Deep active inference as variational policy gradients. Journal of Mathematical Psychol-
ogy 96:102,348, DOI https://doi.org/10.1016/j.jmp.2020.102348, URL http://www.sciencedirect.
com/science/article/pii/S0022249620300298
Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D, Riedmiller M (2013) Playing
atari with deep reinforcement learning. 1312.5602
Oliver G, Lanillos P, Cheng G (2019) Active inference body perception and action for humanoid robots.
arXiv preprint arXiv:190603022
Parr T, Friston KJ (2019) Generalised free energy and active inference. Biological cybernetics 113(5-
6):495–513
Pezzato C, Hernandez C, Wisse M (2020) Active inference and behavior trees for reactive action planning
and execution in robotics. 2011.09756
Rezende DJ, Mohamed S, Wierstra D (2014) Stochastic Backpropagation and Approximate Inference in
Deep Generative Models. In: Xing EP, Jebara T (eds) Proceedings of the 31st International Conference
on Machine Learning, PMLR, Bejing, China, Proceedings of Machine Learning Research, vol 32, pp
1278–1286, URL http://proceedings.mlr.press/v32/rezende14.html
51
Champion et al.
Robert P, Escouﬁer Y (1976) A Unifying Tool for Linear Multivariate Statistical Methods: The RV-
Coeﬃcient. Journal of the Royal Statistical Society Series C (Applied Statistics) 25(3):257–265, URL
http://www.jstor.org/stable/2347233
Rood T, van Gerven M, Lanillos P (2020) A deep active inference model of the rubber-hand illusion.
In: Verbelen T, Lanillos P, Buckley CL, De Boom C (eds) Active Inference, Springer International
Publishing, Cham, pp 84–91
Sancaktar C, van Gerven M, Lanillos P (2020) End-to-end pixel-based deep active inference for body
perception and action. 2001.05847
Schneider T (N.D.) Active inference for robotic manipulation, unpublished
Schneider T, Belousov B, Abdulsamad H, Peters J (2022) Active inference for robotic manipulation. arXiv
preprint arXiv:220610313
Schwartenbeck P, Passecker J, Hauser TU, FitzGerald THB, Kronbichler M, Friston K (2018) Com-
putational mechanisms of curiosity and goal-directed exploration. bioRxiv DOI 10.1101/411272,
URL https://www.biorxiv.org/content/early/2018/09/07/411272, https://www.biorxiv.org/
content/early/2018/09/07/411272.full.pdf
Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, Schrittwieser J, Antonoglou I,
Panneershelvam V, Lanctot M, Dieleman S, Grewe D, Nham J, Kalchbrenner N, Sutskever I, Lillicrap
TP, Leach M, Kavukcuoglu K, Graepel T, Hassabis D (2016) Mastering the game of go with deep
neural networks and tree search. Nature 529(7587):484–489, DOI 10.1038/nature16961, URL https:
//doi.org/10.1038/nature16961
Sutton RS, Barto AG, et al (1998) Introduction to reinforcement learning. MIT press Cambridge
Ueltzh¨ oﬀer K (2018) Deep active inference. Biol Cybern 112(6):547–573, DOI 10.1007/s00422-018-0785-7,
URL https://doi.org/10.1007/s00422-018-0785-7
52
Appendix A: Notation
Symbol Meaning
sτ, oτ, rτ, aτ State, observation, reward and action at time step τ, respectively.
or
τ, ˆsr
τ, ˚or
τ
An observation in which a reward has been encoded as explained in Figure 40, the hidden
state sampled from the encoder when feeding or
τ as input, and the observation reconstructed
by the decoder from the output of the transition network, respectively.
ˆsτ, ˆoτ
A state sampled from the encoder at time step τ when oτ is provided as input, and the image
reconstructed by the decoder at time τ when using ˆsτ as input, respectively.
˚oτ The observations at time step τ predicted by the transition network.
si:j, oi:j, ai:j Respectively, the set of states, observations, and actions between time step i and j (included).
π, πτ, π′ A policy, i.e. a sequence of actions, the action prescribed by the policy at time step τ, and
another policy whose size is smaller or equal than the size of π, respectively.
A, Π The set of possible actions, and the set of possible policies, respectively.
As The set of all anscestors of a node s.
#A,#C,#S,#O The number of actions, channels, states and observations, respectively.
Qθa, ˆQˆθa
The Q-network parameterised by θa, and the target network parameterised by ˆθa
Gθa, ˆGˆθa
The critic network parameterised by θa, and the target network parameterised by ˆθa.
Eφs, Dθo The encoder network parameterised by φs, and the decoder network parameterised by θo.
Pφa The policy network parameterised by φa.
Tθs, Tθo The transition network parameterised by θs or θo, respectively.
θ, φ All the parameters of the generative model, and the variational distribution, respectively.
a, b, c, d Four hyperparameters involved in the computation of ωt.
N(sτ,aτ) The number of times action aτ was explored in state sτ.
t, γ The present time step, and the discount factor, respectively.
ζ, ψ The precision of the prior over actions, and the precision of the prior preferences.
ϵ, ˆϵ The random variable used in the re-parameterisation trick, and a sample of epsilon.
˚ϵ The probability of selecting a random actions when using the ˚ϵ-greedy algorithm.
Tdec A hyperparameter deﬁning the threshold value corresponding to a clear winner during MCTS.
G(π), Gτ(π) The expected free energy (EFE) of policy π, and the EFE received at time
step τ when following policy π, respectively.
¯Gs, Gaggr
s , Gs, Ns
The average EFE, the aggregated EFE, the EFE, and the number of visits
of a node s, respectively.
µo,σo,µa,σa The mean and variance vectors predicted by the encoder and policy networks of the DAIFA.
µ, σ The mean and variance of the Gaussian distribution over st predicted by the encoder.
˚µ, ˚σ The mean and variance of the Gaussian distribution over st+1 predicted by the transition.
ˆµ, ˆσ The mean and variance of the Gaussian distribution over st+1 predicted by the encoder.
ˆπ The parameters of the categorical distribution over at predicted by the policy network.
ωt The top-down attention parameter modulating the precision of the transition mapping.
[ condition ] An indicator function that equals one if the condition is satisﬁed and zero otherwise.
σ[ •] The softmax function.
Cat(x; φx) A categorical distribution over x parameterised by φx.
Bernoulli(x; φx) A Bernoulli distribution over x parameterised by φx.
Bernoulli(x; φx) A product of Bernoulli distributions over x parameterised by φx.
N(x; µx,σx) A multivariate Gaussian over x parameterised by a mean vector µx, and
a diagonal covariance matrix whose diagonal elements are σx.
X i→Y, X m→Y X is fed as input to Y, and the mean of the distribution predicted by X is fed as input to Y.
X s→Y, X →Y a sample from the distribution predicted by X is fed as input to Y, and X outputs Y.
Table 1: Notation of Sections 2 and 3.
Champion et al.
Appendix B: DAIMC discrepancies between the paper and the code
In this section, we focus on the authors’ implementation of DAIMC available on GitHub: https://
github.com/zfountas/deep-active-inference-mc/ . First, according to a personal communication
with one of the authors, the code available on GitHub (on the 6th of June 2022) is not the same as the
one used to run the experiments of the paper. Below, we describe the discrepancies between the paper
and the code. For example, the computation of ωt in the paper is as follows:
ωt = a
1 + exp(−b−Dt
c ) + d,
while the code uses the following formula:
ωt = a×
(
1 − 1
1 + exp
(
−Dt−b
c
)
)
+ d.
Also, the paper states that MCTS is perfomed to compute the prior over policies during training. How-
ever, in the code, MCTS is only used when testing the model, i.e., no MCTS when training the agent.
Additionally, the paper states that actions are selected by sampling from:
˜P(at) = N(ˆst,at)∑
ˆat N(ˆst,ˆat).
However, the code selects an entire sequence of actions π = (˚at,˚at+1,...,˚at+n) recursively from the root
node in the tree. At each step in the recursion, the node with the highest number of visits ˚aτ is selected.
Then, actions cancelling each other are removed from the sequence, e.g., if aτ = LEFT and aτ+1 =
RIGHT then both actions are removed from the sequence. This procedure generates a new sequence
of actions π′ of equal or smaller length. Finally, the entire sequence of actions π′ is performed in the
environment. This avoids the repetition of the planning process for each action-perception cycle (saving
computational time), however, this also requires domain knowledge (to remove actions that cancel each
other out).
Additionally, in the paper, experiments are run on both the dSprites environment and the animal AI
environment. However, the code does not allow the replication of the results on the animal AI environment,
i.e., the code handling the animal AI environment has been removed. In addition, the evaluation of the
expected free energy is non trivial (see below) and the details are not discussed in the paper. Before
explaining how the terms of the EFE are computed, we introduce notation that allows us to express those
computational steps concisely. For example, we note:
or
t
i→Encoder s→Transition m→˚sr
t+1,
meaning that or
t is used as input ( i→) for the encoder, then a state is sampled ( s→) from the distribution
predicted by the encoder and used as input for the transition network, ﬁnally, the mean ( m→) of the
distribution predicted by the transition network is used as a maximum aposteriori estimate of ˚sr
t+1. Note,
the transition network takes two inputs (i.e., a state and an action), when using our concise notation we
implicitly assume that the actions prescribed by the policy 8 π are provided as input to the transition
network. Also, for each time step τ, the reward rτ collected by the agent is encoded in the pixels of the
image oτ as explained in Figure 40, leading to a new image or
τ. As illustrated on the right of Figure 41,
the encoder/decoder networks are trained to predict the resulting images or
τ. The computation of the
ﬁrst term in equation (8) is illustrated on the left of Figure 41. Concisely, we have:
or
t
i→Encoder s→Transition s→Decoder m→˚or
t+1.
8. π is the policy for which the expected free energy is being computed.
54
Deconstructing deep active inference.
Figure 40: This ﬁgure illustrates how the reward rτ ∈[−1,1] is encoded in image oτ. On the left, the
plus and minus signs shows where the reward will be encoded in the image if the reward is positive or
negative, respectively. In the middle, a positive reward is being encoded on the left side of the image. On
the right, a negative reward is being encoded on the right of the image.
Next, a matrix (˚rt+1) encoding the maximum reward that the agent can gather is used as parameter of
Bernoulli distributions to compute the logarithm of the probability (i.e., L) of the three ﬁrst rows of
the reconstructed image ˚or
t+1. Note, as explained in Figure 40, the ﬁrst three rows contain the predicted
reward obtained at time t+ 1. Finally, the mean of Lis then computed and is multiplied by ten to get
E ˜Q[ln ˜P(oτ|π)]. Similarly, the computation of H[Q(sτ|π)] proceeds as follows:
or
t
i→Encoder s→Transition →˚µ,ln˚σ,
where Q(sτ|π) is equated with N(sτ;˚µ,˚σ), and an analytical solution is used to compute the entropy of
Q(sτ|π). Next, the computation of H[Q(oτ|sτ,π)] proceeds as follows:
or
t
i→Encoder s→Transition s→Decoder m→˚or
t+1,
where observation ˚or
t+1 is equated to the parameters of the Bernoulli distribution Q(oτ|sτ,π), and an
analytical solution is used to compute H[Q(oτ|sτ,π)]. Surprisingly, another observation ˚or
t+1 sampled
exactly as before is equated to the parameters of the Bernoulli distribution Q(oτ|sτ,θ,π ), and the same
analytical solution is used to compute H[Q(oτ|sτ,θ,π )]. Finally, H[Q(sτ|oτ,π)] is computed by feeding
˚or
t+1 back into the encoder to obtain the mean and log-variance of the Gaussian distribution Q(sτ|oτ,π),
and the analytical solution for the entropy of a Gaussian is used to compute H[Q(sτ|oτ,π)].
In summary, two samples of ˚ or
t+1 (sampled as described in Figure 40) have been equated to the
parameters of two diferent distributions, i.e., Q(oτ|sτ,θ,π ), and Q(oτ|sτ,π). Additionally, a third sample
of ˚or
t+1 (sampled in the same way) has also been used as input to the distribution ˜P(oτ|π). Lastly, while
Fountas et al (2020) deﬁnes the EFE as in (8), the code turns a plus into a minus, leading to the following
deﬁnition of the EFE:
Gτ(π) = −E ˜Q
[
ln ˜P(oτ|π)
]
−EQ(θ|π)
[
EQ(oτ|θ,π)
[
H[Q(sτ|oτ,π)]
]
−H[Q(sτ|π)]
]
+ EQ(θ|π)Q(sτ|θ,π)
[
H[Q(oτ|sτ,θ,π )]
]
−EQ(sτ|π)
[
H
[
Q(oτ|sτ,π)
]]
,
where the red minus was a plus.
55
Champion et al.
envt envt+1
perform action ˆat
rt ∼U([−1,1])
ot
rt+1 = envt+1.computereward()
ot+1
encode reward in image
ort+1
Encoder
ln ˆσ ˆµ ϵ
Decoder
ˆsrt+1
ˆort+1
The variational
auto-encoder
at timet+ 1
is used to train
the encoder
and decoder
networks.
ort
Encoder
lnσ µ ϵ
Decoder
ˆsrt
at
˚ort+1
Transition
˚µ
ln˚σ
ϵ
˚srt+1
˚rt+1 = 01 R(˚ort+1)
L= lnBernoulli(R(˚ort+1);˚rt+1)
E˜Q[ln ˜P(oτ|π)] = mean(L) ×10
def envt+1.computereward():
rt+1 = rt×0.95;
if agent.y≥32:
if shape == “square”:
rt+1 = 15.5−agent.x
16
else:
rt+1 = agent.x−15.5
16
Figure 41: This ﬁgure illustrates the computation of E ˜Q[ln ˜P(oτ|π)] in (the code of) Fountas et al (2020).
The environment at time t provides the agent with an image ot and a reward rt randomly sampled from
the interval [−1; 1]. Then, action ˆat is performed in the environment and the agent observes an imageot+1
and a reward rt+1, where rt+1 is computed according to the function presented in the center of the image.
Next, the reward at time tand t+ 1 are encoded in the images received at time tand t+ 1, respectively,
c.f., Figure 40 for details about the encoding. The encoded image at time t+ 1 (i.e., or
t+1) is then fed into
the encoder, the re-parameterisation trick is then used to sample a state from the variational posterior.
This state is fed into the decoder which tries to reconstruct the image inputed into the encoder. Once
ˆor
t+1 has been computed, the weights of the encoder and decoder are learned using back-propagation. On
the other hand, the encoded image at time t (i.e., or
t) is used to compute E ˜Q[ln ˜P(oτ|π)]. More precisely,
the or
t is fed into the encoder, and a state is sampled from the variational posterior Qφs(st). This state
is then fed as input into the transition network along with the action prescribed by π at time t, i.e., at.
A state at time t+ 1 can then be sampled from the distribution predicted by the transition network.
This state is then inputed into the decoder, which outputs ˚or
t+1. Next, a matrix (i.e., ˚rt+1) encoding the
maximum reward that the agent can gather is used as a parameter of a Bernoulli distribution to compute
the logarithm of the probability (i.e., L) of the ﬁrst three rows of ˆor
t+1, i.e., R(˚or
t+1). The mean of Lis
then computed and is multiplied by ten to obtain E ˜Q[ln ˜P(oτ|π)].
56
Deconstructing deep active inference.
Appendix C: Analysis of the epistemic value
In this appendix, we study the expected free energy decomposition into epistemic and extrinsic value
as given in Equation (10) of Parr and Friston (2019). A particular reason for being interrested in this
formulation is that it is the version of the expected free energy that has the most inﬂuenced the deep
active inference approaches, such as Fountas et al (2020). Starting with the deﬁnition of the expected
free energy, see Equation (14) in the main-body:
Gτ(π) = EP(oτ|sτ)Q(sτ|π)
[
ln Q(sτ|π) −ln P(oτ,sτ)
]
= EP(oτ|sτ)Q(sτ|π)
[
ln Q(sτ|π) −ln P(sτ|oτ) −ln P(oτ)
]
= −EP(oτ|sτ)Q(sτ|π)
[
ln P(sτ|oτ) −ln Q(sτ|π)
]
  
Epistemic value
−EP(oτ|sτ)Q(sτ|π)
[
ln P(oτ)
]
  
Extrinsic value
In the rest of this appendix, we will focus on the epistemic value and report two experiments. In the ﬁrst
experiment, the prior over states (c.f., left-most graph in Figure 45) is equal to the approximate posterior
(c.f., Figure 42), and the likelihood (c.f., Figure 43) is becoming more and more uniform (c.f., Figure 44).
Note, if the likelihood becomes uniform, then the true posterior P(sτ|oτ) becomes equal to the prior over
states P(sτ), i.e.,
P(sτ|oτ) ∝P(oτ|sτ)P(sτ) = 1
|oτ|P(sτ),
where |oτ|is the number of observations at time step τ, and after renormalisation P(sτ|oτ) = P(sτ). The
left-most graph of Figure 39 shows that the epistemic value decreases as the likelihood becomes more
uniform, i.e., the epistemic value decreases as the true posterior P(sτ|oτ) becomes more similar to the
approximate posterior Q(sτ|π). This behaviour is to be expected, as the epistemic value is bigger when
the true posterior P(sτ|oτ) and the approximate posterior Q(sτ|π) are more diﬀerent. Thus, maximising
epistemic value will promote exploration and information gain.
In the second experiment, the likelihood has high entropy (c.f., right-most graph in Figure 44), and the
prior over states is shifting from left to right (c.f., Figure 45). When the prior over states P(sτ) and the
approximate posterior Q(sτ|π) are diﬀerent, the joint distribution P(oτ|sτ)Q(sτ|π) will be more similar
to the approximate posterior Q(sτ|π) than it is to the true posterior P(sτ|oτ). Indeed, as the likelihood
is almost uniform, the true posterior P(sτ|oτ) will almost be equal to the prior over states P(sτ). At the
same time, as the likelihood is almost uniform, it will not have much impact on the joint distribution
P(oτ|sτ)Q(sτ|π) and the approximate posterior Q(sτ|π) will dominate. To sum up, when the prior over
states P(sτ) and the approximate posterior Q(sτ|π) are diﬀerent:
• the true posterior will almost be equal to the prior over states P(sτ|oτ) ≈P(sτ)
• the joint distribution P(oτ|sτ)Q(sτ|π) will be more similar to Q(sτ|π) than it is to P(sτ|oτ)
Therefore, the joint distribution P(oτ|sτ)Q(sτ|π) will tend to be large when the diﬀerence within the
expectation is negative (c.f., Figure 46). Indeed, as the joint is similar to the approximate posterior, it
means that the joint distribution is large when the approximate posterior is large and the true posterior is
smaller. This implies that the logarithm of true posterior will be very negative, while the logarithm of the
approximate posterior will be less negative, i.e., closer to zero. Then, the logarithm of the approximate
posterior will be substrated from the logarithm of the true posterior, i.e., a small positive number will be
added to a very negative number. Therefore, the result will be negative.
The right-most graph of Figure 39 shows that the epistemic value increases as the prior P(sτ) and
therefore the true posterior P(oτ|sτ) becomes more similar to the approximate posterior Q(sτ|π). This
behaviour should not be observed, as the epistemic value is bigger when the true posterior P(sτ|oτ) and
the approximate posterior Q(sτ|π) are more similar. Thus, maximising epistemic value will not promote
exploration. In fact, it will recommend a strong focus on a single action as was observed in Figure 33.
57
Champion et al.
Figure 42: This ﬁgure illustrates the approximate posterior Q(sτ|π), which is distributed according to a
binomial distribution corresponding to 6 trials with a probability of success of 0.5.
Figure 43: This ﬁgure provides two views of the same likelihood mapping P(oτ|sτ), i.e., one from the
front and one from behind. The likelihood was created by sliding a binomial distribution (corresponding
to 6 trials with a probability of success of 0.5) across the observation axis, each time the state increases
by one. Finally, when the binomial reached its most extreme position, it stays the same for the remaining
states.
Figure 44: During the ﬁrst experiement, we changed the likelihood mapping P(oτ|sτ) by making it more
and more uniform. This change is shown in the ﬁgure from left to right.
58
Deconstructing deep active inference.
Figure 45: During the second experiement, we changed the prior over states P(sτ) by making it more
and more diﬀerent to the approximate posterior Q(sτ|π). This change is shown in the ﬁgure from left to
right.
Figure 46: To compute the epistemic value, we ﬁrst computed the diﬀerence between the logarithm of the
true posterior lnP(sτ|oτ) and the logarithm of the approximate posterior lnQ(sτ|π) for all the values taken
by the observation oτ (c.f., left-most ﬁgure). Then, we computed the joint distribution P(oτ|sτ)Q(sτ|π)
used in the expectation (c.f., middle ﬁgure). Next, we compute the element-wise product between the
matrices illustrated in the left-most and middle ﬁgures (c.f., right-most ﬁgure). Finally, the epistemic
value is obtained by summing up all the elememts of the element-wise product. In this instance, the
epistemic value will be strongly negative.
59