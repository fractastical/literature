Exploring and Learning Structure:
Active Inference Approach in Navigational Agents
Daria de Tinguy1[0000−0003−1112−049X], Tim Verbelen2, and bart Dhoedt1
1 Ghent University, Ghent, Belgiumfirst_name.family_name@ugent.be
2 Verses AItim.verbelen@verses.ai
Abstract. Drawing inspiration from animal navigation strategies, we
introduce a novel computational model for navigation and mapping,
rooted in biologically inspired principles. Animals exhibit remarkable
navigation abilities by efficiently using memory, imagination, and strate-
gicdecision-makingtonavigatecomplexandaliasedenvironments.Build-
ing on these insights, we integrate traditional cognitive mapping ap-
proaches with an Active Inference Framework (AIF) to learn an environ-
ment structure in a few steps. Through the incorporation of topological
mapping for long-term memory and AIF for navigation planning and
structure learning, our model can dynamically apprehend environmen-
tal structures and expand its internal map with predicted beliefs during
exploration. Comparative experiments with the Clone-Structured Graph
(CSCG) model highlight our model’s ability to rapidly learn environmen-
tal structures in a single episode, with minimal navigation overlap. this is
achieved without prior knowledge of the dimensions of the environment
or the type of observations, showcasing its robustness and effectiveness
in navigating ambiguous environments.
Keywords: exploration · active inference· topological graph· structure
learning.
1 Introduction
A functional navigation system must seamlessly fulfil three key functions: self-
localisation, mapping, and path planning. This requires both a sensing compo-
nent for spatial perception and a storage capability to extend these perceptions
temporally and spatially [33]. Animals exhibit a remarkable capacity for rapidly
learning the structure of their environment, often in just one or a few visits,
relying on memory, imagination, and strategic decision-making [32,26].
The hippocampus and neocortex play crucial roles in episodic memory, spa-
tial representation, and relational inference. Mammals rely on mental representa-
tions of spatial structures, traditionally viewed as either cognitive maps or cogni-
tivegraphs,conceptualisingenvironmentalspaceasanetworkofnodes[33,25,7,1].
Recent research suggests an integrated approach combining these concepts is
more effective [23].
arXiv:2408.05982v2  [cs.AI]  2 Sep 2024
2 D. de Tinguy & al
Our approach adopts this viewpoint, proposing a topological map incor-
porating internal motion (Euclidean parameters) to delineate spatial experi-
ences. The neural positioning system, found in rodents and primates, supports
self-localisation and provides a metric for distance and direction between lo-
cations [33]. This system includes place cells, heading direction cells [15], grid
cells [2], speed cells [14], and border cells [30], working together to enable rapid
learning, disambiguation of aliases, and a comprehensive understanding of spa-
tial navigation [5].
Building on these concepts, we introduce a novel model that dynamically
learns environmental structure and expands its cognitive map. Integrating visual
information and proprioception (inferred body motion), our model constructs lo-
cations and connections within its cognitive map. Starting with uncertainty, the
model envisions action outcomes, expanding its map by incorporating hypothe-
ses into its generative model, analogous to Bayesian model reduction [9] that
grows its model upon receiving new observation, we extend ours upon predicted
beliefs. This process allows our model to efficiently navigate and comprehend
environmental structures with minimal steps, using an active inference naviga-
tion scheme [17]. Compared to the Clone-Structured Graph (CSCG) [24], our
model rapidly learns environmental layouts more efficiently. An exploration run
is shown in Figure 1, displaying from left to right the full environment, the ex-
tracted observation and exploration path and the resulting internal map of the
agent.
Fig. 1.From the mini-grid environment [3,31] with different rooms annotated by colour
to the path our agent took -from black to white- to form a successful exploration
correctly linking all the rooms up to the agent’s internal topological graph with the
state associated to each room.
2 Related work
While navigating their environment, animals often encounter ambiguous sensory
inputs due to aliasing, resulting in repetitive observations, such as encountering
Exploring and Learning Structure 3
two overly similar corridors. They must rapidly disambiguate the structure of
their environment to navigate successfully.
Models like the Clone-Structured Graph (CSCG) [12] or Transformers rep-
resentations [4] have been proposed to form cognitive maps that disambiguate
aliased environments through partial observations. However, these models re-
quire substantial training time using random or hard-coded policies. In contrast,
animals adapt their actions based on subtle cues and incentives, learning to
navigate with minimal instances [32].
Animals exhibit decision-making abilities powered by imagination (estimat-
ing actions’ consequences) and a holistic understanding of the environment, nat-
urally imagining un-visited areas and guiding their next steps [2]. This intuitive
decision-making process,consideringincentives like food or safety,rapidly directs
them toward their objectives [26].
Integrating observations with proprioception [33] helps animals circumvent
aliasing, using a process similar to active inference for judgement [17]. Active in-
ference involves continuously updating internal models based on sensory inputs,
enabling adaptive and efficient decision-making. This normative framework ex-
plains cognitive processing and brain dynamics by positing that actions and per-
ceptions aim to minimise free energy, encapsulating causal relationships among
observable outcomes, actions, and hidden states [22,8].
At the core of adaptive behaviour is the balance between exploitation (select-
ing the most valuable option based on existing beliefs) and exploration (choosing
options that facilitate learning) [27]. Recent behavioural evidence suggests hu-
mans mix random and goal-directed exploration [11]. Our model adopts this bal-
ance through free energy minimisation, choosing stochastic policies to enhance
environmental understanding. This enables active learning, rapidly reducing un-
certainty over model parameters (i.e. reducing uncertainty over our beliefs) [27].
By balancing curiosity and goal-directed behaviour through free energy, our
systemguidesanagentmeaningfullyandlearnsinabiologicallyplausibleway[22].
It achieves few-shot or one-shot learning, similar to mice in a labyrinth [26]. By
projecting the consequences of actions into its internal map, the agent extends
its imagination beyond known territories, improving its navigation ability to
explore environments of any dimension.
3 Method
Inourstudy,ouragentinitiatesexplorationoftheenvironmentwithoutanyprior
knowledge regarding the observations and dimensions of the map it is about to
navigate. Subsequently, we will clarify how, at each step, the agent engages in
inferring the current state, a process that integrates both the notion of obser-
vation and proprioception (position perception given a motion). This inference
task involves updating past beliefs based on the latest observation and motion,
following the principles of a Partially Observable Markov Model (POMDP).
Henceforth, the agent strategically envisions sequences of actions to explore,
termed policies, while concurrently expanding its internal map to accommodate
4 D. de Tinguy & al
potential unexplored areas with uncertain priors. Although the agent may know
the relative positions of these areas, it does not foresee observations. This iter-
ative and multi-step process serves as the cornerstone for the agent’s adaptive
learning and navigation strategies within the environment.
3.1 Inference and spatial abstraction
In the context of Active Inference (AIF), the process of inferring the agent’s cur-
rent state involves integrating sensory inputs and prior beliefs within a Partially
Observable Markov Decision Process (POMDP). We consider that our inference
mechanism operates at the highest level of abstraction within a hierarchical spa-
tial framework [31], where lower layers handle observation transformation and
the concept of blocked paths, akin to how visual observations are processed
in the visual cortex and motion limitations are perceived by border cells [30].
Figure 1 illustrates the agent navigating through an environment, where doors
signify transitions to different states while walls correspond to obstacles. We
give our agent the notion that doors lead to another location, while walls lead
to the same observation and the pose stays static. At the centre of this Figure,
we see a path taken by the agent depicted along with the observations perceived
by our model, demonstrating how observations are simplified and generalised at
the highest abstraction level into a single colour per room (floor colour). The
internal topological map generated by the agent based on its exploration path is
presented in the final frame of Figure 1. The underlying POMDP model guiding
this inference process is depicted in Figure 2, where the current statest (defin-
ing a room) and positionpt (the location of that room) are inferred based on
the previous state st−1, pt−1 and action at−1 leading to the current observa-
tion ot (the colour of that room). The generative model capturing this process
is described by Equation 1, where the joint probability distribution over time
sequences of states, observations, and actions is formulated. Tildes are used to
denote sequences over time.
P(˜o, ˜s, ˜p, ˜a) =P(o0|s0)P(s0)P(p0)P(a0)
τY
t=1
P(ot|st)P(st, pt|st−1, pt−1, at−1)
(1)
Due to the posterior distribution over a state becoming intractable in large
state spaces, we use variational inference instead. This approach introduces an
approximate posterior denoted asQ(˜s, ˜p|˜o, ˜a) and is presented in equation 2 [28].
Q(˜s, ˜p|˜o, ˜a) =Q(s0, p0|o0)
τY
t=1
Q(st, pt|st−1, pt−1, at−1, ot) (2)
The classical inference scheme heavily relies on past and current experiences
to localise the agent within its environment, using observation alone, the agent
would be weak to aliased observations at different locations. By combining ob-
servation with the agent’s proprioception the model is much more robust in dif-
ferentiating ambiguous environments. The internal positioningp0 is initialised
Exploring and Learning Structure 5
Fig. 2.factor graph POMDP of our generative model transitioning from past and
present (up to time-stept) to future (time-stept + 1). The posept is inferred from the
previous posept−1 and the action from policyπ, while the statest determined by the
corresponding observation ot and influenced by the previous statest−1, pose pt and
action at−1. Past actions and observations are assumed observable, indicated by a blue
colour. In the future, the actions are defined by a policyπ influencing the new states
and position in orange and new predictions in grey.
at the start of exploration in the absence of prior information, and is updated
as the agent transitions between rooms (e.i., by passing through a door), thus
as long as the agent is confident in its current state. the POMDP factor graph
showing the association between poses, states and observations is illustrated in
Figure 2
If the agent were to be kidnapped and re-localised elsewhere, the observation
o and inferred positionp would not match expectations and the confidence in the
state would decrease. If the confidence in the state goes below a given threshold,
the agent stops updating its internal model given new information and focuses
on re-gaining confidence over its state/location.
However, inferring the positionp has much more to offer than localisation
robustness, it is key to extending the internal map over unexplored areas yet to
be integrated into the model through parameter learning.
3.2 Parameter Learning
Learning within the Active Inference framework encompasses the adaptation of
beliefs concerning model parameters, such as transition probabilitiesP(st|st−1)
(e.g. how rooms are connected) and likelihood probabilitiesP(ot|st) (e.g. what
a room looks like). These parameters reflect the structural connectivity of the
environment and the expected sensory outcomes given particular states.
Generative models in Active Inference rely on prior beliefs regarding param-
eter distributions, with updates driven by the active inference framework [22].
6 D. de Tinguy & al
Unlike traditional discrete-time POMDP, where either transitions or likelihoods
probabilities are fixed and updating parameters implies reasoning over a fixed
spatial dimension [21,13,19], our model learns the probabilities of all its Markov
matrices and extends their dimensions dynamically. The state transitionsBs =
P(st|st−1, at−1) and the observation Ao = P(ot|st), position likelihoodAp =
P(pt|st) probabilities are optimised over transitions. The position transition
Bp = P(pt|pt−1, at−1), however, is not a Markov matrix and entails an incremen-
tal process based on consecutive motions (experimented or predicted), without
any parameters to be learned by belief optimisation.
The optimisation of beliefs of the generative model parametersθ occur after
state inference and involves minimising the free energy Fθ while considering
prior beliefs and uncertainties associated with both parameters and policies, as
defined in [22]:
θ =(Ao, Ap, Bs)
Fθ =EQ(π,θ)[F(π, θ)] +DKL[Q(θ)||P(θ)] +DKL[Q(π)||P(π)] (3)
With P and Q being respectively the joint distribution and the approximate
posteriorofthemodel.Themodelupdatesitsparametersbasedonobserveddata
and transitions, expanding the observation dimension of A upon encountering
new information as can be seen in [29]. At initialisation, high certainty is assigned
to the likelihood probabilities integrating the first observation. After realising the
parameter update based on priors, the model edits its internal map dimensions
and parameters based on predicted transitions, expanding all parameters in their
state dimensions, thus improving exploration in unexplored environments of any
unknown size.
3.3 Incorporating spatial dynamics in model parameters
To extend the internal map (our state space), we propose a novel approach where
the agent predicts one-step policy outcomes in all directions considering detected
obstacles. Bp can expand its position dimension given a motion andAp considers
the probability of being at a given state given the position. When we predict a
new position given no obstacle in a direction,Bp expands. If the expected mo-
tion leads to an un-visited location,st+1 does not exist in the model. The state
is undefined while the positionpt+1 is certain, therefore all Markov matrices are
expected to grow in their state dimension to match this new prediction. This
process enables the dynamic expansion of the dimensions of both the observa-
tion and position likelihoods (Ao, Ap) and state transition (Bs) to consider the
novel statest+1 in a process equivalent to [9]. Subsequently, the state transition
probability (Bs) and position likelihood (Ap) can be updated through the same
equation 4, here shown with a transition matrix.
Bπ = Q(st+1|st, π)Q(st) ∗ Bπ ∗ learning_rate (4)
With st+1 being a new state if there are no obstacles detected and a new position
is predicted or the same statest otherwise. The learning rate is set higher for
Exploring and Learning Structure 7
experimented transitions than imagined transitions such that we form new con-
nections weaker toward expected places compared to visited places as we would
expect from animal synaptic learning [6].Ao has grown in its state dimension,
however, it lacks information regarding the specific observationot+1 expected
in that location, resulting in a uniform distribution for that state. Such areas
exhibit high uncertainty in their observation likelihood model. Using those prior,
the agent can leverage the Active Inference scheme to determine where to di-
rect itself to maximise its objective (e.g. forming a comprehensive map of the
environment). While previous models such as [9,29] adjust their internal model
growth to accommodate new patterns of observations, we extend the concept to
predict, un-visited, areas and generate new states holding no observation. Those
unknown states are therefore highly attractive when seeking information gain
and largely improve exploration strategy.
3.4 Policy Selection in Active Inference
Policy selection plays a crucial role in exploring those expected states generated
by the model. The AIF guides the agent’s decision-making process based on the
minimisation of expected surprise and uncertainty. Policy selection, informed
by the AIF, determines the agent’s actions and map extension in response to
sensory inputs and internal beliefs.
Typically, agents are assumed to desire to minimise their variational free
energy (F), which can serve as a metric to quantify the discrepancy between the
joint distributionP and the approximate posteriorQ as presented in Equation 5.
F = EQ(˜s,˜p|˜a,˜o)[log[Q(˜s, ˜p|˜a, ˜o)] − log[P(˜s, ˜p, ˜a, ˜o)]
= DKL[Q(˜s, ˜p|˜a, ˜o))||P(˜s, ˜p|˜a, ˜o)]| {z }
posterior approximation
− log[P(˜o)]| {z }
log evidence
= DKL[Q(˜s, ˜p|˜a, ˜o))||P(˜s, ˜p, ˜a)]| {z }
complexity
−EQ(˜s,˜p|˜a,˜o)[log[P(˜o|˜s)]| {z }
accuracy
(5)
Active inference agents aim to minimise their free energy by engaging in three
main processes: learning, perception, and planning. Learning involves optimis-
ing the model parameters, perception entails estimating the most likely state,
and planning involves selecting the policy or action sequence that leads to the
lowest expected free energy. Essentially, this means that the process involves
forming beliefs about hidden states that offer a precise and concise explanation
of observed outcomes while minimising complexity.
While planning, however, we use the expected free energy (G), indicating
the agent’s anticipated variational free energy following the implementation of
a policyπ. Unlike the variational free energy, which focuses on current and past
observations, the expected free energy incorporates future expected observations
generated by the selected policy.
8 D. de Tinguy & al
G(π, τ) =EQ(oτ ,sτ |π)[log(Q(sτ |π) − log(Q(sτ |oτ , π))]| {z }
information gain term
− EQ(oτ ,sτ |π))[log(P(oτ ))]| {z }
utility term
(6)
The expected information gain quantifies the anticipated shift in the agent’s
belief over the state from the priorQ(sτ |π) to the posteriorQ(sτ |oτ , π) when
pursuing a particular policy. On the other hand, the utility term assesses the
expected log probability of observing the preferred outcome under the chosen
policy. This value intuitively measures the likelihood that the policy will guide
the agent toward its prior preferences. In this study, we give no prior preference
to the agent, as it does not know the environment (unknown observations and
map size).
To calculate this expected free energyG(π) over each stepτ of a policy we
sum the expected free energy of each time-step.
G(π) =
X
τ
G(π, τ) (7)
To consider the best policy, we recall that active inference achieves goal-
directed behaviour by selecting policies minimising this expected free energy,
thereby aiming to produce observations closer to preferred outcomes or prior
preferences. This is achieved by setting the approximate posterior over policies
as in Equation 8 [19]:
P(π) =σ(−γG(π)) (8)
Where σ, the softmax function is tempered with a temperature parameterγ,
given as a hyper-parameter, converting the expected free energy of policies into
a categorical distribution over policies. Actions are then sampled based on this
posterior distribution, with lower temperatures resulting in more deterministic
behaviour.
By navigating without a clear preference, we desire the highest information
gain, effectively pushing the agent toward states it anticipates but doesn’t know
what to expect from.
4 Results
We explore experimental scenarios where an agent navigates within a grid en-
vironment with cardinal motions and still motion. The agents have no direct
access to a map of the environment and visual observations are considered to
undergo hierarchical processing, transforming them from a vector to a single
descriptor corresponding to one colour per room. They receive localised sensory
inputs, corresponding to the current room they are in. Sensory inputs which are
possibly repeated at different locations (aliased observations). Given a series of
Exploring and Learning Structure 9
discretised egocentric observations and actions, the agent must deduce the la-
tent topology of its environment to assess various navigation options. Learning
this latent graph from aliased observations presents a challenge for most artifi-
cial agents [18]. We contrast our model with CSCG [10], a specialised variant
of Hidden Markov Models (HMM). CSCG employs a probabilistic approach, us-
ing sequences of action-observation pairs without assuming Euclidean geometry.
Each observation corresponds to a subset of hidden states known as clones. Al-
though these states share the same observation likelihood, they differ in their
implied dynamics encoded in the transition model. By analysing the sequence of
action-observation pairs, specific clones with higher likelihoods can disambiguate
the aliased observations. Initially, CSCG gathers a dataset through maze explo-
ration to learn the spatial structure [10].
To make the two models more similar for a fair comparison, we include our
model’s current state estimation mechanism in the CSCG approach [19]. More-
over, we decided to see how CSCG would behave if we included the position as
an observation. This effectively removes aliasing and is believed equivalent to the
proprioception of our model when starting without prior, we call that specific
case "CSCG pose ob". We compared the performance of our model (receiving
only observation as input) to the CSCG receiving only visual observations or
visual observation-position pairs with or without random exploration policies.
The CSCG internal path estimator is based on the Viterbi method [16,10] and is
updated every 5 steps with the sequence of pairs going from the first observation
to the current time-step.
Our environments are composed of several rooms connected in diverse ways
(fully connected 3 by 3 and 4 by 4 rooms environments, T-shaped, donuts-shaped
mazes with and without aliased floor colours). All models receive the room floor
colour as observation. An example of a 3 by 3 rooms environment and extracted
observations per room are presented in Figure 1 first and second panel. The
agents can move in the four cardinal directions or choose to stay at the present
location.
In our exploration runs, across all environments, agents are initially placed
at random starting positions and tasked with learning the environment’s topol-
ogy. Results represent the mean over a minimum of ten successful runs in each
environment. Notably, our agent always achieves successful exploration, while
CSCG occasionally fails due to insufficient steps allotted to learn the topology.
The Oracle model, analogous to an A-star path planning, demonstrates the ideal
scenario where the agent seizes the full topology of the environment by visiting
each position only once. In the case of the T-maze, results are averaged across
all runs considering the starting positions of the models.
Our exploration results can be seen in Figure 3a. Exploration is deemed com-
plete when the internal belief over the transition between observations aligns
with the ground-truth transition matrix with a minimum certainty of 60% over-
all correct transitions, the threshold was set arbitrarily based on the resulting
successful transition representation as the one that can be seen in a 3 by 3 obser-
vations map depicted in figure 4. The figure shows how well-defined are possible
10 D. de Tinguy & al
(a) average steps to explore the environments
(b) average steps to discover the environments
Fig. 3.The average steps are depicted on a logarithmic scale. Remarkably, our agent
achieves all tasks in significantly fewer steps compared to the CSCG model. The oracle
sets the benchmark, representing the minimum steps necessary to visit all rooms once.
Additionally, an aliased room signifies the recurrence of identical observations across
various locations, posing a challenge as it could mislead the agent regarding its current
position.
transitions compared to impossible transitions (due to walls). We also see that
givinguniqueobservations(visualobservation-positionpair)informationreduces
the CSCG training time of about 100 steps in aliased squared environments and
200 in T-shaped mazes, most probably due to its structure, the agent stays stuck
in an aisle. However using random policy or the Viterbi algorithm for navigation
does not improve exploration, because the agent can not extrapolate on un-
seen observations, thus finally leading to almost random action selection. This
demonstrates the benefit of map extension over un-visited areas.
Exploring and Learning Structure 11
Fig. 4.Example of a successful transition representation between positions in a 3x3
grid map. Each state in the plot is paired with its corresponding ground-truth pose for
clarity (pose, state). The intensity of colour in the figure indicates the level of certainty
the agent has about the transition.
If we compare the number of steps required to learn the structure of the envi-
ronment and the number of steps the agent takes to visit all unknown positions,
we can deduce a few things. Firstly, not having an imagination over possible tra-
jectories disadvantages the CSCG, as it repeatedly visits known rooms, randomly
or not, instead of being attracted to novelty as ours is. Ours has prior over non-
visited states, rendering unknown rooms highly uncertain, and thus attractive to
diminish the agent’s internal model’s parameters uncertainty. Secondly, we see
our agent exploring all rooms with steps closely matching the oracle, this implies
that it could have the potential to learn transitions faster, in a one-shot learning
if we were to increase confidence in imagined beliefs. However, this could also
consolidate misbeliefs about transitions, in those experiments we let the agent
confirm its priors instead of over-trusting them by setting the learning rate of
the model low on predicted transitions. The given exploration seems to follow
biological evidence on mouse behaviour in a maze [26].
We give a qualitative example of the agent behaviour in the T-maze Figure 5.
Figure 5a shows the full path taken by a line varying from black to yellow, the
agent starts at the bottom of the T-maze. Figure 5b shows the imagined trajec-
tories of the agent (represented by X in the figure) at various steps to read from
left to right and top to bottom. Imagined trajectories are associated with their
expected free energy, the darker, the more desirable the path to the agent. The
agent is purely driven by information gain in those experiments. Our model has
low interest in paths leading into the current room walls and is highly attracted
to unexplored areas. Upon reaching the end of the right aisle (1st image, 2nd
row of Fig 5b), the unexplored aisle is notably more attractive than the previ-
ously visited one, highlighting the agent’s preference for uncertain observations
over confirming existing beliefs. While returning to the starting point, the agent
shows interest in paths going through walls. This is because these transitions
become more intriguing as the agent has gained a better understanding of the
12 D. de Tinguy & al
environment’s connectivity. A consolidation of its belief can be realised through a
new observation of the walls. Those observations confirm that the agent exhibits
a coherent and effective exploration behaviour akin to how we would explore an
environment, first discovering all areas before delving into specific details.
(a)
 (b)
Fig. 5.Exploration of a T-maze starting at the base of the T. a) depicts the full path
as a line transitioning from black to white. b) showcases, from top to bottom columns
one to two, the agent -represented as an X- imagined optimal policies. Darker colours
indicate higher expected free energy.
5 Discussion
Thisstudyproposesanovelhigh-levelabstractionmodelinformedbybiologically
plausible principles mimicking key points of animal navigation strategies [33,1].
ByintegratingadynamiccognitivegraphwithinternalpositioningandanActive
InferenceFramework,ourmodelsuccessfullyexplorestheenvironmentandlearns
itsstructureinafewsteps,asexpectedfromanimals[32,26],facilitatingadaptive
learning and efficient exploration. Moreover, allowing the internal map to grow
with expected beliefs not only creates a map adapted to any environment di-
mension, shape or observations but also enhances exploration by creating highly
uncertain states where the whereabouts are predictable but the corresponding
observations aren’t. Comparative experiments with the Clone-Structured Graph
(CSCG) model [10] underscore the effectiveness of our approach in learning en-
vironment structures with minimal data and without prior knowledge of specific
observation dimensions. This is mainly due to our agent’s capacity to imagine
actions’ consequences and integrate them into its beliefs. Moving forward, it
would be interesting to increase the prediction range of new states to integrate
into the model and determine the impact on navigation. Moreover, studying the
impact of a perfect memory on future policies and exploration efficiency, as well
as seeing how the agent fares when trying to reach a defined objective it has
prior upon in a familiar or novel environment would enhance the research. Fi-
nally deploying this model in real-world scenarios such as StreetLearn [20], based
on Google map observations, would approach further this mechanism to animal
behaviour and provide more conclusive evidence.
Exploring and Learning Structure 13
Acknowledgement
This research received funding from the Flemish Government under the “Onder-
zoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” programme.
References
1. Balaguer, J., Spiers, H., Hassabis, D., Summerfield, C.: Neural mechanisms of
hierarchical planning in a virtual subway network. Neuron90, 893–903 (05 2016).
https://doi.org/10.1016/j.neuron.2016.03.037
2. Bush, D., Barry, C., Manson, D., Burgess, N.: Using grid cells for navigation. Neu-
ron 87, 507 – 520 (2015),https://api.semanticscholar.org/CorpusID:7275119
3. Chevalier-Boisvert, M., Willems, L., Pal, S.: Minimalistic gridworld environment
for openai gym.https://github.com/maximecb/gym-minigrid (2018)
4. Dedieu, A., Lehrach, W., Zhou, G., George, D., Lázaro-Gredilla, M.: Learning
cognitive maps from transformer representations for efficient planning in partially
observed environments (2024)
5. Edvardsen, V., Bicanski, A., Burgess, N.: Navigating with grid and place cells in
cluttered environments. Hippocampus30 (08 2019). https://doi.org/10.1002/
hipo.23147
6. Eichenbaum, H.: The hippocampus as a cognitive map ... of social space.
Neuron 87(1), 9–11 (2015). https://doi.org/https://doi.org/10.1016/j.
neuron.2015.06.013, https://www.sciencedirect.com/science/article/pii/
S0896627315005267
7. Epstein, R., Patai, E.Z., Julian, J., Spiers, H.: The cognitive map in humans:
Spatial navigation and beyond. Nature Neuroscience 20, 1504–1513 (10 2017).
https://doi.org/10.1038/nn.4656
8. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Doherty, J.O.,
Pezzulo, G.: Active inference and learning. Neuroscience & Biobehavioral Re-
views 68, 862–879 (2016). https://doi.org/https://doi.org/10.1016/j.
neubiorev.2016.06.022, https://www.sciencedirect.com/science/article/
pii/S0149763416301336
9. Friston, K., Parr, T., Zeidman, P.: Bayesian model reduction (2019)
10. George, D., Rikhye, R., Gothoskar, N., Guntupalli, J.S., Dedieu, A., Lázaro-
Gredilla, M.: Clone-structured graph representations enable flexible learning and
vicarious evaluation of cognitive maps. Nature Communications 12 (04 2021).
https://doi.org/10.1038/s41467-021-22559-5
11. Gershman, S.: Deconstructing the human algorithms for exploration. Cognition
173, 34–42 (12 2017).https://doi.org/10.1016/j.cognition.2017.12.014
12. Guntupalli, J.S., Raju, R., Kushagra, S., Wendelken, C., Sawyer, D., Deshpande,
I., Zhou, G., Lázaro-Gredilla, M., George, D.: Graph schemas as abstractions for
transfer learning, inference, and planning (02 2023).https://doi.org/10.48550/
arXiv.2302.07350
13. Heins, R.C., Mirza, M.B., Parr, T., Friston, K., Kagan, I., Pooresmaeili, A.: Deep
active inference and scene construction. Frontiers in Artificial Intelligence3 (2020).
https://doi.org/10.3389/frai.2020.509354, https://www.frontiersin.org/
articles/10.3389/frai.2020.509354
14. Hinman, J., Brandon, M., Climer, J., Chapman, W., Hasselmo, M.: Multiple run-
ning speed signals in medial entorhinal cortex. Neuron 91 (07 2016). https:
//doi.org/10.1016/j.neuron.2016.06.027
14 D. de Tinguy & al
15. Jacobs, J., Kahana, M., Ekstrom, A., Mollison, M., Fried, I.: A sense of direction
in human entorhinal cortex. Proceedings of the National Academy of Sciences of
the United States of America107, 6487–92 (03 2010).https://doi.org/10.1073/
pnas.0911213107
16. Jelinek, F.: Continuous speech recognition by statistical methods. Proceedings of
the IEEE64(4), 532–556 (1976).https://doi.org/10.1109/PROC.1976.10159
17. Kaplan, R., Friston, K.: Planning and navigation as active inference (12 2017).
https://doi.org/10.1101/230599
18. Lajoie,P.,Hu,S.,Beltrame,G.,Carlone,L.:ModelingperceptualaliasinginSLAM
via discrete-continuous graphical models. CoRRabs/1810.11692 (2018), http:
//arxiv.org/abs/1810.11692
19. de Maele, T.V., Dhoedt, B., Verbelen, T., Pezzulo, G.: Bridging cognitive
maps: a hierarchical active inference model of spatial alternation tasks and the
hippocampal-prefrontal circuit (2023)
20. Mirowski, P., Banki-Horvath, A., Anderson, K., Teplyashin, D., Hermann, K.M.,
Malinowski, M., Grimes, M.K., Simonyan, K., Kavukcuoglu, K., Zisserman, A.,
Hadsell, R.: The streetlearn environment and dataset. CoRRabs/1903.01292
(2019), http://arxiv.org/abs/1903.01292
21. Neacsu, V., Mirza, M.B., Adams, R.A., Friston, K.J.: Structure learning enhances
concept formation in synthetic active inference agents. PLOS ONE17(11), 1–34
(11 2022).https://doi.org/10.1371/journal.pone.0277199,https://doi.org/
10.1371/journal.pone.0277199
22. Parr, T., Pezzulo, G., Friston, K.: Active Inference: The Free Energy Principle
in Mind, Brain, and Behavior (03 2022).https://doi.org/10.7551/mitpress/
12441.001.0001
23. Peer, M., Brunec, I.K., Newcombe, N.S., Epstein, R.A.: Structuring knowledge
with cognitive maps and cognitive graphs. Trends in Cognitive Sciences25(1), 37–
54 (2021). https://doi.org/https://doi.org/10.1016/j.tics.2020.10.004,
https://www.sciencedirect.com/science/article/pii/S1364661320302503
24. Peer, M., Brunec, I.K., Newcombe, N.S., Epstein, R.A.: Structuring knowledge
with cognitive maps and cognitive graphs. Trends in Cognitive Sciences25(1), 37–
54 (2021). https://doi.org/https://doi.org/10.1016/j.tics.2020.10.004,
https://www.sciencedirect.com/science/article/pii/S1364661320302503
25. Raju, R.V., Guntupalli, J.S., Zhou, G., Lázaro-Gredilla, M., George, D.: Space is a
latent sequence: Structured sequence learning as a unified theory of representation
in the hippocampus (2022)
26. Rosenberg, M., Zhang, T., Perona, P., Meister, M.: Mice in a labyrinth show
rapid learning, sudden insight, and efficient exploration. eLife 10, e66175
(jul 2021).https://doi.org/10.7554/eLife.66175, https://doi.org/10.7554/
eLife.66175
27. Schwartenbeck, P., Passecker, J., Hauser, T.U., FitzGerald, T.H., Kronbichler,
M., Friston, K.J.: Computational mechanisms of curiosity and goal-directed ex-
ploration. eLife 8, e41703 (may 2019).https://doi.org/10.7554/eLife.41703,
https://doi.org/10.7554/eLife.41703
28. Smith, R., Friston, K.J., Whyte, C.J.: A step-by-step tutorial on ac-
tive inference and its application to empirical data. Journal of Mathemat-
ical Psychology 107, 102632 (2022). https://doi.org/https://doi.org/10.
1016/j.jmp.2021.102632, https://www.sciencedirect.com/science/article/
pii/S0022249621000973
Exploring and Learning Structure 15
29. Smith, R., Schwartenbeck, P., Parr, T., Friston, K.J.: An active inference approach
to modeling structure learning: Concept learning as an example case. Frontiers in
Computational Neuroscience14 (2020). https://doi.org/10.3389/fncom.2020.
00041, https://www.frontiersin.org/articles/10.3389/fncom.2020.00041
30. Solstad, T., Boccara, C.N., Kropff, E., Moser, M.B., Moser, E.I.: Repre-
sentation of geometric borders in the entorhinal cortex. Science 322(5909),
1865–1868 (2008). https://doi.org/10.1126/science.1166466, https://www.
science.org/doi/abs/10.1126/science.1166466
31. de Tinguy, D., Van de Maele, T., Verbelen, T., Dhoedt, B.: Spatial and tempo-
ral hierarchy for autonomous navigation using active inference in minigrid envi-
ronment. Entropy26(1), 83 (Jan 2024). https://doi.org/10.3390/e26010083,
http://dx.doi.org/10.3390/e26010083
32. Tyukin, I.Y., Gorban, A.N., Alkhudaydi, M.H., Zhou, Q.: Demystification of few-
shot and one-shot learning. CoRRabs/2104.12174 (2021), https://arxiv.org/
abs/2104.12174
33. Zhao, M.: Human spatial representation: What we cannot learn from the studies of
rodent navigation. Journal of Neurophysiology120 (08 2018).https://doi.org/
10.1152/jn.00781.2017