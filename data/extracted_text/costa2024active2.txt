Active Inference as a Model of Agency
Lancelot Da Costa
Department of Mathematics
Imperial College London
l.da-costa@imperial.ac.uk
Samuel Tenka
CSAIL
Massachusetts Institute of Technology
c o l i@m i t.edu
Dominic Zhao
Common Sense Machines
dominic.zhao@csm.ai
Noor Sajid
Wellcome Centre for Human Neuroimaging
University College London
noor.sajid.18@ucl.ac.uk
Abstract
Is there a canonical way to think of agency beyond reward maximisation? In this paper, we show that any type of
behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world
canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about states of the world.
This description, known as active inference, refines the free energy principle, a popular descriptive framework for action
and perception originating in neuroscience. Active inference provides a normative Bayesian framework to simulate and
model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics. The usefulness
of active inference for RL is three-fold.a) Active inference provides a principled solution to the exploration-exploitation
dilemma that usefully simulates biological agency.b) It provides an explainable recipe to simulate behaviour, whence
behaviour follows as an explainable mixture of exploration and exploitation under a generative world model, and all
differences in behaviour are explicit in differences in world model.c) This framework is universal in the sense that it is
theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an
active inference algorithm. Thus, active inference can be used as a tool to uncover and compare the commitments and
assumptions of more specific models of agency.
Keywords: exploration, exploitation, expected free energy, generative world model, Bayesian inference.
Acknowledgements
The authors are indebted to Alessandro Barp, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan and
Grigorios A. Pavliotis for helpful input on a preliminary version to this manuscript. The authors thank Joshua B.
Tenenbaum and MIT’s Computational Cognitive Science group for interesting discussions that lead to some of the points
discussed in this paper. LD is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875)
and a G-Research grant. This publication is based on work partially supported by the EPSRC Centre for Doctoral Training
in Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1). NS is funded by the Medical
Research Council (MR/S502522/1) and 2021–2022 Microsoft PhD Fellowship.
arXiv:2401.12917v1  [cs.AI]  23 Jan 2024
1 Introduction
Reinforcement learning (RL) is a collection of methods that describe and simulate agency—how to map situations to
actions—traditionally framed as optimising a numerical reward signal [1]. The idea of maximising reward as underpinning
agency is ubiquitous: with roots in utilitarianism [2] and expected utility theory [3], it also underwrites game theory [3],
statistical decision-theory [4], optimal control theory [5,6], and much of modern economics. From its inception, RL practi-
tioners have supplemented reward seeking algorithms with various heuristics or biases geared towards simulating intelligent
behaviour. Especially effective are intrinsic motivation or curiosity-driven rewards that encourage exploration [7–9]. Thus,
we ask:is there a canonical way to think of agency beyond reward maximisation?
In this paper, we show that any behaviour complying with physically sound assumptions about how macroscopic biological
agents interact with the world canonically integrates exploration and exploitation by minimising risk and ambiguity about
external states of the world. This description, known as active inference, refines the free energy principle, a popular
descriptive framework for action and perception birthed in neuroscience [10–12].
Active inference provides a generic framework to simulate and model agency that is widely used in neuroscience [13–17],
RL [18–21] and robotics [22–25]. The usefulness of active inference for RL is three-fold.a) Active inference provides
an effective solution to the exploration-exploration dilemma that englobes the principles of expected utility theory [3]
and Bayesian experimental design [26] and finesses the need for ad-hoc exploration bonuses in the reward function or
decision-making objective. b) Active inference provides a transparent recipe to simulate behaviour by minimising risk
and ambiguity with respect to an explicit generative world model. This enables safe and explainable decision-making by
specifically encoding the commitments and goals of the agent in the world model [23].c) Active inference is universal in
the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active
inference as an active inference algorithm, e.g., [27]. Thus, active inference can be used as a tool to uncover and compare
the commitments and assumptions of more specific models of agency.
2 Deriving agency from physics
Wedescribesystemsthatcompriseanagentinteractingwithitsenvironment. Weassumethatanagentanditsenvironment
evolve together according to a stochastic processx. This definition entails a notion of timeT , which may be discrete or
continuous, and a state spaceX, which should be a measure space (e.g., discrete space, manifold, etc.). Recall that a
stochastic process x is a time-indexed collection of random variablesxt on state spaceX. Equivalently, x is a random
variable over trajectories on the state spaceT → X. We denote by P the probability density of x on the space of
trajectories T → X(with respect to an implicit base measure).
In more detail, we factorise the state spaceX into states that belong to the agentH and statesexternal to the agentS
that belong to the environment. Furthermore, we factorise agent’s states intoautonomous states A and observable states
O, respectively defined as the states which the agent does and does not have agency over. In summary, the systemx
is formed ofexternal s and agent processes h, the latter which is formed ofobservable o, and autonomous a processes
X ≡ S × H ≡ S × O × A=⇒ x ≡ (s, h) ≡ (s, o, a).
The description adopted so far could aptly describe particles interacting with a heat bath [28–30] as well as humans
interacting with their environment (Figure 1.A). We would like a description of macroscopic biological systems; so what
distinguishes people from small particles? A clear distinction is that human behaviour occurs at the macroscopic level
and, thus, is subject to classical mechanics. In other words, people areprecise agents:
Definition 2.1. An agent is precise when it responds deterministically to its environment, that is when h | s is a
deterministic process.
Remark 2.2. It is important not to conflate an agent’s mental representations of external reality with external states; the
former are usually an abstraction or coarse-grained representation of the latter. We do not consider agent’s representations
in this paper.We simply posit that there exists a detailed enough description of the environment that determines the agent’s
trajectory (e.g., observations and actions). This will always be true for agents evolving according to classical mechanics.
Agency means being in control one’s actions and using them to influence the environment [31]. At timet, the information
available to the agent is its past trajectoryh≤t = (o≤t, a≤t), i.e. its history. We definedecision-making as a choice of
autonomous trajectory in the futurea>t given available knowledgeh≤t. Furthermore, we defineagency as the process
through which an agent makes and executes decisions. We interpretP(s, o| h≤t) as expressing the agent’spreferencesover
environmental and observable trajectories given available data, andP(s, o| a>t, h≤t) as expressing the agent’spredictions
over environmental and observable paths given a decision (e.g., Figure 1.B). Crucially, agency is governed by anexpected
free energyfunctional of the agent’s predictions and preferences (Appendix D)
−log P(a>t | h≤t) =EP(s,o|a>t,h≤t)[log P(s | a>t, h≤t) − log P(s, o| h≤t)]. (EFE)
We have formulated agency as optimising an objective: the lower the expected free energy, the more likely a course of
action, and vice-versa.
2
Figure 1: (A) This figure illustrates a human (agent processh) interacting with its environment (external processs), and the
resulting partition into externals, observableo, and autonomousa processes. The agent does not have direct access to the external
process, but samples it through the observable process. The observable process constitutes the sensory epithelia (e.g., eyes and
skin), which influences the environment through touch. The autonomous process constitutes the muscles and nervous system,
which influences the sensory epithelia, e.g., by moving a limb, and the environment, e.g., through speech by activating vocal cords.
Autonomous responses at timet + δt may depend upon all the information available to the agent at timet, that ish≤t. Thus,
the systems we are describing are typicallynon-Markovian. (B) An active inference agent is completely described by its prediction
model P(s, o| a) and its preference modelP(s, o). When the prediction model is a POMDP the preference model is a hidden
Markov model. In this setting, the colour scheme illustrates the problem of agency att = 1: the agent must execute an action (in
red) based on previous actions and observations (in grey), which are informative about external states and future observations (in
white). When specifying an active inference agent, it is important that prediction and preference models coincide on those parts
they have in common; in this example the likelihood mapP(o | s). Note that these models need not be Markovian.
3 Characterising agency
This description of agency combines many accounts of behaviour that predominate in cognitive science and engineering.
Indeed, decomposing the expected free energy (EFE) reveals fundamental imperatives that underwrite agency, such as
minimising risk and ambiguity (Appendix D):
−log P(a>t | h≤t) = DKL

predicted paths
z }| {
P(s | a>t, h≤t) |
preferred paths
z }| {
P(s | h≤t)

| {z }
risk
+ EP(s|a>t,h≤t)

H[P(o | s, h≤t)]

| {z }
ambiguity
. (1)
Risk refers to the KL divergence between the predicted and preferred external course of events. As minimising a reverse
KL divergence leads to mode matching behaviour [32], minimising risk leads torisk-averse decisions that avoid disfavoured
courses of events (c.f., Appendix B). In turn, risk-aversion is a hallmark of prospect theory, which describes human choices
under discrete alternatives with no ambiguity [33]. Additionally, risk is the main decision-making objective in modern
approaches to control as inference [34–37], variously known as Kalman duality [38,39], KL control [40] and maximum
entropy RL [41].
Ambiguityrefers to the expected entropy of future observations, given future external trajectories. An external trajectory
thatmayleadtovariousdistinctobservationtrajectoriesishighlyambiguous—andvice-versa. Thus, minimisingambiguity
leads to sampling observations that enable to recognise the external course of events. This leads to a type of observational
bias commonly known as thestreetlight effector drunkard’s searchin behavioural psychology [42]: when a person loses
their keys at night, they initially search for them under the streetlight because the resulting observations ("I see my keys
under the streetlight" or "I do not see my keys under the streetlight") accurately disambiguate external states of affairs.
Additionally, agency maximises extrinsic and intrinsic value (Appendix D):
−log P(a>t | h≤t) ≥ −EP(o|a>t,h≤t)

log
preferred paths
z }| {
P (o | h≤t)

| {z }
extrinsic value
−EP(o|a>t,h≤t)

DKL [P (s | o, a>t, h≤t) | P (s | a>t, h≤t)]

| {z }
intrinsic value
. (2)
Extrinsic valuerefers to the (log) likelihood of observations under the model of preferences. Maximising extrinsic value
leads to sampling observations that are likely under the model of preferences. Through the correspondence between log
probabilities and utility functions [38,43,44] this is equivalent to maximising expected utility or expected reward. This
3
underwrites expected utility theory [3], game theory [3], optimal control [5,6] and RL [1]. Bayesian formulations of
maximising expected utility under uncertainty are also known as Bayesian decision theory [4].
Intrinsic valuerefers to the amount of information gained about external courses of events under a decision. Favouring
decisions to maximise information gain leads to a goal-directed form of exploration [15], driven to answer "what would
happen if I did that?" [45]. Interestingly, this decision-making procedure underwrites Bayesian experimental design [26]
and active learning in statistics [46], intrinsic motivation and artificial curiosity in machine learning and robotics [45,47–50].
This is mathematically equivalent to optimising expected Bayesian surprise and mutual information, which underwrites
visual search [51,52] and the organisation of our visual apparatus [53–55].
We have formulated agent’s behaviour as minimising an objective that canonically weighs an exploitative term with an
explorative term—risk plus ambiguity—which provides a principled solution to the exploration-exploitation dilemma [56].
4 Simulating agency
Active inference specifies an agent by aprediction model P(s, o| a), expressing the distribution over external and ob-
servable paths under autonomous paths, and apreference modelP(s, o), expressing the preferred external and observable
trajectories (e.g., Figure 1.B). In discrete time, agency proceeds by approximating the expected free energy given past
observations and actionsh≤t = (o≤t, a≤t) and use it to govern agency (details and simulations in Appendices A and B):
1. Preferential inference:infer preferences about external and observable trajectories, i.e., approximateP(s, o| h≤t)
with Q(s, o| h≤t) (e.g., Figure 2).
2. For each possible sequence of future actionsa>t:
(a) Perceptual inference: infer external and observable paths under the action sequence, i.e., approximate
P(s, o| a>t, h≤t) with Q(s, o| a>t, h≤t).
(b) Planning as inference:assess the action sequence by evaluating its expected free energy (EFE), i.e.,
−log Q(a>t | h≤t) ≡ EQ(s,o|a>t,h≤t)

log Q(s | a>t, h≤t) − log Q(s, o| h≤t)

.
3. Agency: execute the most likely decisionat+1, or useQ(at+1 | h≤t) as a model for behavioural data, where
at+1 = arg maxQ(at+1 | h≤t), Q (at+1 | h≤t) =
X
a>t
Q(at+1 | a>t)Q(a>t | h≤t).
5 Concluding remarks
Active inference is a description of macroscopic biological agents derived from physics, which provides a generic framework
to model biological and artificial behaviour. It provides a transparent description of agency in terms of minimising risk
and ambiguity (i.e., an expected free energy functional) under some prediction and preference models about the world.
This formulation is universal in the sense that any RL agent satisfying the descriptive assumptions of active inference
(Appendix D) is behaviourally equivalent to an active inference agent under some implicit prediction and preference
models.
Active inference provides an effective solution to the exploration-exploitation dilemma [56] that finesses the need for ad-hoc
exploration bonuses in the reward function or decision-making objective (Appendices B and C) [57–59] . In particular,
the expected free energy englobes various objectives used to describe or simulate behaviour across cognitive science and
engineering, endowing it with various useful properties such as information-sensitivity (Appendix C). Perhaps the closest
RL formulations are action and perception as divergence minimisation [60], which considers a similar decision-making
objective; control as inference, which can be seen as minimising risk but not ambiguity [34,37,41]; and Hyper [61], which
proposes reward maximisation alongside minimising uncertainty over both external states and model parameters.
Active inference provides a recipe for safe algorithmic decision-making. All behaviour under active inference is explainable
as a mixture of exploration and exploitation under a generative world model, and all differences in behaviour are explicit
in differences in world model. In particular, the agent’s commitments and goals can be explicitly encoded in the world
model by the user. In addition, the expected free energy leads to risk-averse decisions (Appendix C), a crucial feature for
engineering applications where catastrophic consequences are possible, e.g., robot-assisted surgery [23].
While active inference clarifies how agency unfolds given a generative world model representing how the environment causes
observations (Section 4), it does not specify which representations underwrite highly intelligent behaviour. Promising steps
in this direction include employing hierarchical probabilistic generative models with deep neural networks [13,24,62–64].
Beyond this, we conclude by asking: what kinds of generative models do humans use to represent their environment? And:
what are the computational mechanisms under which a child’s mind develops into an adult mind by gradually learning
its world model [65–68]?
4
References
[1] A. Barto and R. Sutton.Reinforcement Learning: An Introduction. A Bradford Book, 1992.
[2] Jeremy Bentham. An Introduction to the Principles of Morals and Legislation. Dover Publications Inc., Mineola,
N.Y, June 2007.
[3] J. Von Neumann and O. Morgenstern.Theory of Games and Economic Behavior. Princeton University Press, 1944.
[4] J. O. Berger.Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics. Springer-Verlag, New
York, second edition, 1985.
[5] Richard E. Bellman.Dynamic Programming. Princeton University Press, Princeton, NJ, US, 1957.
[6] K. J Åström. Optimal control of Markov processes with incomplete state information. Journal of Mathematical
Analysis and Applications, 10(1):174–205, February 1965.
[7] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In
Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222–227,
1991.
[8] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised
prediction. In International conference on machine learning, pages 2778–2787. PMLR, 2017.
[9] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-
based exploration and intrinsic motivation.Advances in neural information processing systems, 29, 2016.
[10] Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A. Pavliotis, and Thomas Parr.
The free energy principle made simpler but not too simple.arXiv:2201.06387 [cond-mat, physics:nlin, physics:physics,
q-bio], January 2022.
[11] K. Friston. The free-energy principle: A unified brain theory?Nature Reviews Neuroscience, 11(2):127–138, 2010.
[12] K. Friston, J. Kilner, and L. Harrison. A free energy principle for the brain.J. Physiology-Paris, 100(1-3):70–87,
2006.
[13] Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston. The computational neurology of movement under
active inference.Brain, March 2021.
[14] G. Pezzulo, F. Rigoli, and K. J. Friston. Hierarchical Active Inference: A Theory of Motivated Control.Trends in
Cognitive Sciences, 22(4):294–306, April 2018.
[15] Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald, Martin Kronbichler, and
Karl J Friston. Computational mechanisms of curiosity and goal-directed exploration.eLife, page 45, 2019.
[16] Ryan Smith, Rayus Kuplicki, Justin Feinstein, Katherine L. Forthman, Jennifer L. Stewart, Martin P. Paulus,
Tulsa1000Investigators, andSahibS.Khalsa. ABayesiancomputationalmodelrevealsafailuretoadaptinteroceptive
precision estimates across depression, anxiety, eating, and substance use disorders.PLOS Computational Biology,
16(12):e1008484, December 2020.
[17] Takuya Isomura, Hideaki Shimazaki, and Karl J. Friston. Canonical neural networks perform active inference.
Communications Biology, 5(1):1–15, January 2022.
[18] Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive Active Inference. InAdvances in Neural Information
Processing Systems, May 2021.
[19] ZafeiriosFountas, NoorSajid, PedroA.M.Mediano, andKarlFriston. DeepactiveinferenceagentsusingMonte-Carlo
methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.
[20] Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, and Stefan J. Kiebel. An empirical evaluation of active inference
in multi-armed bandits.Neural Networks, 144:229–246, December 2021.
[21] Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas, and Karl Friston. Exploration and preference
satisfaction trade-off in reward-free learning.arXiv preprint arXiv:2106.04316, 2021.
[22] Pablo Lanillos, Cristian Meo, Corrado Pezzato, Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata, Alexander
Tschantz, Beren Millidge, Martijn Wisse, Christopher L. Buckley, and Jun Tani. Active Inference in Robotics and
Artificial Agents: Survey and Challenges.arXiv:2112.01871 [cs], December 2021.
[23] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How Active Inference Could Help
Revolutionise Robotics. Entropy, 24(3):361, March 2022.
[24] Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation as hierarchical
active inference.Neural Networks, 142:192–204, October 2021.
[25] Hendry F. Chame and Jun Tani. Cognitive and motor compliance in intentional human-robot interaction. In2020
IEEE International Conference on Robotics and Automation (ICRA), pages 11291–11297, May 2020.
[26] D. V. Lindley. On a Measure of the Information Provided by an Experiment.The Annals of Mathematical Statistics,
27(4):986–1005, 1956.
[27] Manuel Baltieri and Christopher L. Buckley. PID Control as a Process of Active Inference with Linear Generative
Models. Entropy, 21(3):257, March 2019.
[28] Grigorios A. Pavliotis.Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin
Equations. Number volume 60 in Texts in Applied Mathematics. Springer, New York, 2014.
[29] Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov blankets, information geometry and stochastic ther-
modynamics. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
378(2164):20190159, February 2020.
5
[30] Luc Rey-Bellet. Open Classical Systems. In Stéphane Attal, Alain Joye, and Claude-Alain Pillet, editors,Open Quan-
tum Systems II: The Markovian Approach, Lecture Notes in Mathematics, pages 41–78. Springer, Berlin, Heidelberg,
2006.
[31] Patrick Haggard and Manos Tsakiris. The experience of agency: Feelings, judgments, and responsibility.Current
Directions in Psychological Science, 18(4):242–246, 2009.
[32] Thomas Minka. Divergence measures and message passing. Technical report, 2005.
[33] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk. Econometrica,
47(2):263–291, 1979.
[34] Sergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.
arXiv:1805.00909 [cs, stat], May 2018.
[35] KonradRawlik, MarcToussaint, andSethuVijayakumar. OnStochasticOptimalControlandReinforcementLearning
by Approximate Inference. InTwenty-Third International Joint Conference on Artificial Intelligence, June 2013.
[36] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th Annual
International Conference on Machine Learning, ICML ’09, pages 1049–1056, Montreal, Quebec, Canada, June 2009.
Association for Computing Machinery.
[37] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. On the Relationship Between Active
Inference and Control as Inference. In Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom,
editors, Active Inference, Communications in Computer and Information Science, pages 3–11, Cham, 2020. Springer
International Publishing.
[38] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering,
82(1):35–45, March 1960.
[39] Emanuel Todorov. General duality between optimal control and estimation. In 2008 47th IEEE Conference on
Decision and Control, pages 4286–4292, December 2008.
[40] Hilbert J. Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine Learning, 87(2):159–182, May 2012.
[41] B. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.PhD thesis,
Carnegie Mellon University, Pittsburgh, 2010.
[42] Abraham Kaplan. The Conduct of Inquiry. Transaction Publishers, 1973.
[43] John S. Bridle. Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to
Statistical Pattern Recognition. In Françoise Fogelman Soulié and Jeanny Hérault, editors,Neurocomputing, NATO
ASI Series, pages 227–236, Berlin, Heidelberg, 1990. Springer.
[44] R. Duncan Luce.Individual Choice Behavior. Individual Choice Behavior. John Wiley, Oxford, England, 1959.
[45] Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010).IEEE Transactions
on Autonomous Mental Development, 2(3):230–247, September 2010.
[46] David J. C. MacKay. Information-Based Objective Functions for Active Data Selection. Neural Computation,
4(4):590–604, July 1992.
[47] Pierre-Yves Oudeyer and Frederic Kaplan. What is Intrinsic Motivation? A Typology of Computational Approaches.
Frontiers in Neurorobotics, 1:6, November 2007.
[48] A. Barto, M. Mirolli, and G. Baldassarre. Novelty or Surprise?Frontiers in Psychology, 4, 2013.
[49] Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian Exploration in
Dynamic Environments.arXiv:1103.5708 [cs, stat], March 2011.
[50] Edward Deci and Richard M. Ryan.Intrinsic Motivation and Self-Determination in Human Behavior. Perspectives
in Social Psychology. Springer US, New York, 1985.
[51] Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention.Vision research, 49(10):1295–1306, May
2009.
[52] Thomas Parr, Noor Sajid, Lancelot Da Costa, M. Berk Mirza, and Karl J. Friston. Generative Models for Active
Vision. Frontiers in Neurorobotics, 15, 2021.
[53] H. B. Barlow.Possible Principles Underlying the Transformations of Sensory Messages. The MIT Press, 1961.
[54] R Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and Information Theory.
Annual Review of Neuroscience, 13(1):257–281, 1990.
[55] L. M. Optican and B. J. Richmond. Temporal encoding of two-dimensional patterns by single units in primate inferior
temporal cortex. III. Information theoretic analysis.Journal of Neurophysiology, 57(1):162–178, January 1987.
[56] Oded Berger-Tal, Jonathan Nathan, Ehud Meron, and David Saltz. The Exploration-Exploitation Dilemma: A
Multidisciplinary Framework.PLOS ONE, 9(4):e95693, April 2014.
[57] Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Ready policy one:
World building through active learning. InInternational Conference on Machine Learning, pages 591–601. PMLR,
2020.
[58] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon White-
son. Varibad: A very good method for bayes-adaptive deep rl via meta-learning.arXiv preprint arXiv:1910.08348,
2019.
6
[59] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by
latent imagination.arXiv preprint arXiv:1912.01603, 2019.
[60] Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action and Perception
as Divergence Minimization.arXiv:2009.01791 [cs, math, stat], October 2020.
[61] Luisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and Shimon White-
son. Exploration in approximate hyper-state space for meta reinforcement learning. InInternational Conference on
Machine Learning, pages 12991–13001. PMLR, 2021.
[62] AA. Soltani, H. Huang, J. Wu, T. Kulkarni, and J. Tenenbaum. Synthesizing 3d shapes via modeling multi-view
depth maps and silhouettes with deep generative networks.CVPR, 2017.
[63] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and active
inference. Neuroscience & Biobehavioral Reviews, 90:486–501, July 2018.
[64] N. Gothoskar, M. Cusumano-Towner, B. Zinberg, M. Ghavamizadeh, F. Pollok, A. Garrett, J. Tenenbaum, D. Gut-
freund, and V. Mansinghka. 3dp3: 3d scene perception via probabilistic programming.NeurIPS, 2021.
[65] D. Gowanlock R. Tervo, Joshua B. Tenenbaum, and Samuel J. Gershman. Toward the neural implementation of
structure learning. Current Opinion in Neurobiology, 37:99–105, April 2016.
[66] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building Machines That
Learn and Think Like People.arXiv:1604.00289 [cs, stat], April 2016.
[67] Tomer D. Ullman and Joshua B. Tenenbaum. Bayesian Models of Conceptual Development: Learning as Building
Models of the World.Annual Review of Developmental Psychology, 2(1):533–558, 2020.
[68] Noah D. Goodman, Tomer D. Ullman, and Joshua B. Tenenbaum. Learning a theory of causality.Psychological
Review, 118(1):110–119, 2011.
[69] Alessandro Barp, Lancelot Da Costa, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan, and Grigo-
rios A. Pavliotis. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. InGeometry and
Statistics, number 46 in Handbook of Statistics. Academic Press, 2022.
[70] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston. Active inference on discrete state-spaces: A
synthesis. J. Math. Psychology, 99:102447, 2020.
[71] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl Friston, Iain Couzin, and Alexander Tschantz.
Pymdp: A Python library for active inference in discrete state spaces.arXiv:2201.03904 [cs, q-bio], January 2022.
[72] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its application
to empirical data.Journal of Mathematical Psychology, 107:102632, April 2022.
[73] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.Active Inference: The Free Energy Principle in Mind, Brain,
and Behavior. MIT Press, Cambridge, MA, USA, March 2022.
[74] Sarah Schwöbel, Stefan Kiebel, and Dimitrije Marković. Active Inference, Belief Propagation, and the Bethe Approx-
imation. Neural Computation, 30(9):2530–2567, September 2018.
[75] Théophile Champion, Howard Bowman, and Marek Grześ. Branching Time Active Inference: Empirical study and
complexity class analysis.arXiv:2111.11276 [cs], November 2021.
[76] Théophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grześ. Branching Time Active Inference:
The theory and its generality.arXiv:2111.11107 [cs], November 2021.
[77] Domenico Maisto, Francesco Gregoretti, Karl Friston, and Giovanni Pezzulo. Active Tree Search in Large POMDPs.
arXiv:2103.13860 [cs, math, q-bio], March 2021.
[78] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active Inference:
A Process Theory.Neural Computation, 29(1):1–49, January 2017.
[79] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystified and Compared.Neural
Computation, 33(3):674–712, January 2021.
[80] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka. Active
Inference, Curiosity and Insight.Neural Computation, 29(10):2633–2683, October 2017.
[81] Thomas Parr.The Computational Neurology of Active Vision. PhD thesis, University College London, London, 2019.
[82] B. Millidge. Deep active inference as variational policy gradients.J. Math. Psychology, 96:102348, 2020.
[83] K. Friston, Lancelot Da Costa, Dalton A. R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis, and Thomas Parr.
Path integrals, particular kinds and strange things. 2022.
[84] Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, Bayesian optimal design, and
expected utility.arXiv:2110.04074 [cs, math, stat], September 2021.
A Details on the active inference algorithm
One specifies an active inference agent by aprediction model P(s, o| a) and a preference model P(s, o). Taking the
example of Figure 1.B, the prediction model may be apartially observable Markov decision process(POMDP). A POMDP
is a discrete time model of how actions influence external and observable trajectories. In a POMDP, 1) each external
state depends only on the current action and previous external stateP(st | st−1, at), and 2) each observation depends
only on the current external stateP(ot | st). If one additionally specifies 3) a distribution of preferences over external
trajectories P(s), one obtains a (hidden Markov) preference model by combining 2) & 3). In general, the preference model
7
may be specified independently from the prediction model, however, since these are the same distribution conditioned on
different variables it is important that they coincide on the parts they have in common; in this example, the likelihood
map P(ot | st). Importantly, the generative models specify the temporal horizon of the agent (i.e., how much time ahead
it plans and represents the world).
Under this model specification, agency is governed by the expected free energy (EFE)
−log P(a>t | h≤t) =EP(s,o|a>t,h≤t)[log P(s | a>t, h≤t) − log P(s, o| h≤t)].
Figure 2: Preferential inference on POMDPs. This figure
illustrates how one may infer preferences efficiently when the pre-
diction model is a POMDP and the preference model is a hidden
Markov model (c.f., Figure 1.B). The problem is illustrated at
time t = 1. We may approximate the posterior preferences over
states and observations given available dataP(s, o| h≤t) by the
product of the predictions of past states and observations given
available data P(s≤t, o≤t | h≤t) and the preferences over future
states and observations given the current stateP(s>t, o>t | st),
c.f., (3). These two distributions can, in turn, be obtained from
the prediction and preference models via approximate inference.
More simply, when preferencesP(st, ot) are i.i.d. for all t (c.f.,
Figure 3), the latter distribution simply equals future preferences
P(s>t, o>t).
The standard active inference algorithm (Section 4) first
approximates the expected free energy by approximating
the usually intractable posterior distributions within. For
instance, preferential inference involves inferring prefer-
ences given previous actions and observationsP(s, o| h≤t).
Observe that this distribution factorises
P(s, o| h≤t) =P(s≤t, o≤t | h≤t)P(s>t, o>t | x≤t) (3)
leading to efficient approximation in a variety of cases (e.g.,
Figure 2). More simply, perceptual inference simply in-
volves conditioning the prediction model on past observa-
tions
P(s, o| a>t, h≤t) =P(s, o| a, o≤t).
Once one has obtained an approximate posterior distribu-
tion over the next action Q(at+1 | h≤t), one can either
execute the most likely action as shown in Section 4, or
simulate generic behaviour by sampling actions from the
posterior distribution
at+1 ∼ Q(at+1 | h≤t)
(notshowninSection4). Thelatteristobeemployedwhen
using active inference as a generative model for behavioural
data.
There are many ways to scale the active inference algo-
rithm for practical applications [69]. These generally in-
volve employing hierarchical generative models with deep
neural networks [24, 63], structured mean-field approxi-
mations [70–74], amortisation [19], and Monte-Carlo tree
search [19,75–77]. Furthermore, this algorithm can also be
used to learn the agent’s prediction and preference mod-
els [69].
B Simulations of behaviour
Here is a simple simulation of behaviour in a T-Maze environment (Figure 3). The environment has four spatial locations
(top left, top right, middle and bottom). One of the top arms yields a reward of1000$ while the other yields a loss of
1000$—per time-step spent in each respective location. The two remaining locations have no reward (0$). Suppose that
you start in the middle arm while being unaware of the reward’s location. The task has two time-steps, so you may visit
two (not necessarily distinct) locations of the Maze in the order you wish. At first, you may:a) stay where you are,b)
go to the bottom arm to collect a cue that discloses the reward’s location,c) go to one of the top arms to determine the
reward’s location by elimination, even though this risks receiving the punishment. Optionsb) andc) both enable you to
collect 1000$ upon visiting the reward’s location at the second step of the task. In choosing policyb) the payoff is1000$
with certainty. In choosing policyc) the payoff is variable:2000$, 1000$ or 0$ with 1/3 probability each. In choosing
policy a), the payoff is strictly worse.Which of these options a), b) or c) will you choose?
Inhumans, themostcommonchoiceisoption( b)—visitingthebottomarmtocollectthecue, therebyinferringthereward’s
location—and subsequently collecting the reward [33], while always avoiding the punishment. We now compare the
behaviours of active inference agents (Section 4), reward maximising agents, and reward plus information gain maximising
agents:
1. Active inference agent.o0: The agent is in the middle of the Maze and is unaware of the context.a1: Visiting the
bottom or top arms have a lower ambiguity than staying, as they yield observations that disclose the context. However,
8
Figure 3:T-Maze environment.This Figure illustrates a simple sequential decision-making task with a temporal horizon of2. In
other words, the agent must choosea1 given h0 = o0 and, subsequently,a2 given h≤1 = (a1, o≤1). In more detail,st: The T-Maze
has four possible spatiallocations: middle, top-left, top-right, bottom. One of the top locations yields a reward of 1000$ (money
bag), while the other yields a punishment of -1000$ (flying money bag)—per time-step spent in respective locations. The reward’s
location determines the context. The bottom arm contains a cue whose colour (blue or green) discloses the context. Together,
location and context determine the external state.ot: The agent always observes its spatial location. In addition, when it is at the
top of the Maze, it receives the reward or the punishment; when it is at the bottom, it observes the colour of the cue.at: Each action
corresponds to visiting one of the four spatial locations.P(st): The agent prefers being at the reward’s location (−log P(st) = 1000)
and avoid the punishment’s location (−log P(st) =−1000). All other external states have a neutral preference (−log P(st) = 0).
s0: the agent starts in the middle location and the context is initialised at random. Together these datum determine prediction
P(s, o| a) and preferenceP(s, o) models, which in turn, determine agency by minimisation of expected free energy (Section 4).
staying or visiting the bottom arm are safer options, as visiting a top arm risks receiving the punishment. By acting to
minimise both risk and ambiguity (1) the agent goes to the bottom.o1: The agent observes the cue and hence determines
the context. a2: All actions have equal ambiguity as the context is known. Collecting the reward has a lower risk than
staying or visiting the middle, which themselves have a lower risk than collecting the punishment. Thus, the agent visits
the arm with the reward. See [15,78] for more details. In summary, the active inference agent ends the task with1000$
with certain probability.
2. Expected reward maximising agent.o0: idem. a1: all locations have the same expected reward of0$, so it is
unclear where to go. Suppose then that the agent chooses an action at random according to an uniform distribution.
o1: In the event that it has chosen to leave the middle location (i.e.,75% of the time), this enables it to unambiguously
determine the reward’s location.a2: the agent collects the reward. In summary,75% of the time, the agent ends the task
with 2000$, 1000$ or 0$ with 1/3 probability, respectively.25% of the time, it’s performance will be worse:1000$, −1000$
with 1/4 probability, respectively, and0$ with probability1/2 probability.
3. Expected reward plus information gain maximising agent.o0: idem. a1: all locations have the same expected
reward of0$ and the bottom and top locations give information as to the reward’s location. Therefore, in maximising the
sum of expected utility and expected information gain the agent leaves the middle location.a2: the agent collects the
reward. In summary, the agent ends the task with2000$, 1000$ or 0$ with 1/3 probability, respectively.
We analyse these differences in behaviour in Appendix C. For more complex simulations of sequential decision-making
using active inference, please see [19,24,63,79–82].
9
C Comparing expected free energy, expected reward, and expected reward plus
information gain
So what distinguishes these objectives? Following the simulations of Appendix B we distinguish two main features:
information-sensitivity, i.e., encoding the value of information, andrisk-aversion (see Table 1). Briefly,
Objective Information-sensitive Risk-averse
Expected free energy ✓ ✓
Expected reward ✗ ✗
Expected reward plus information gain ✓ ✗
Table 1: Comparing objectives.
1. Information-sensitivity. Optimising expected reward does not intrinsically value the information gained following
an action. In contrast, the expected free energy, or an expected information gain add-on to expected reward, both select
actions that value the information afforded by an action.
2. Risk-aversion. Expected reward and expected information-gain maximising agents cannot distinguish between two
alternatives with the same expected reward and information gain. In that sense, they are perfectly rational agents. In
contrast, active inference agents are risk-averse to the extent that they will avoid choices that can yield to a large loss,
even when the expected payoff is neutral. Being risk-averse in this sense has two advantages:a) in modelling human
behaviour, as this is a defining feature of human decision-making [33].b) In engineering applications where the actions of
artificial agents can potentially have catastrophic consequences—such as in algorithmic trading, human-robot interaction
and robot-assisted surgery—risk-aversion enables safe decision-making by actively avoiding negative outcomes [23].
Inducing information-sensitivity and risk-aversion by tuning the reward function.Note that we can endow
expected reward (ER) and expected reward plus information gain (ERIG) agents with information-sensitivity and risk-
aversion by simply tuning the reward function. For instance, in the simulations of Appendix B, we can penalise the middle
location of the T-Maze by−1$ as it affords no information. In this case, the ER and ERIG maximising agents will behave
the same as the initial ERIG maximising agent (Simulation 3). Furthermore, in remarking that losing1000$ hurts more
than the gain afforded by receiving1000$ we may update the (negative) reward associated with the loss to−1001. With
this the ER and ERIG maximising agents will behave exactly as the active inference agent with the initial reward function
(Simulation 1).
Yet, tuning the reward function to achieve desired behaviour has two main drawbacks. For example, should we score the
punishment by−1001 or −1005? There is no definite answer. Though these will not affect behaviour in the T-Maze task,
this will not be the case in complex environments where different reward functions will lead to (sometimes unexpectedly)
distinct behaviour. Secondly, it is well-known that manually tuning the reward to produce desired behaviour is a difficult
and impractical task, especially in complex or changing environments.
We conclude that it is best to work with the ground truth reward whenever this is unambiguously defined (e.g., monetary
reward or payoff by reaching a goal) and for information-sensitivity and risk-aversion to be incorporated in the agent’s
decision-making objective. Though many approaches to RL integrate information-sensitivity [59,61] and risk-aversion [34,
41] with reward maximisation, active inference allows to canonically and transparently integrate these imperatives in the
objective function.
D Assumptions and mathematical derivations
This Appendix lists the different assumptions and mathematical arguments that underwrite the derivation of active
inference presented in Sections 2 and 3.
Assumptions:
1. Stochastic process: Agent and environment evolve according to a stochastic processx ≡ (s, h) ≡ (s, o, a).
2. Precise agent: The description of external state space is sufficiently detailed such thath | s is a deterministic
process.
3. Countability of the path space: The path spaceT → Xis countable (e.g., timeT is countable and the state
space X is finite, or vice-versa). Presumably, our results can be extended to uncountable path spaces by a limiting
argument. This is left for future work.
Under the countability assumption we may takeP to be the probability density of the process w.r.t. the uniform measure.
We have [83]:
10
−log P(a>t | h≤t) =EP(s,o|a>t,h≤t)[−log P(a>t | h≤t)]
= EP(s,o|a>t,h≤t)[log P(s, o| a>t, h≤t) − log P(s, o, a>t | h≤t)]
= EP(s,o|a>t,h≤t)[log P(o | s, a>t, h≤t) + logP(s | a>t, h≤t) − log P(a>t | s, o, h≤t) − log P(s, o| h≤t)].
Lemma D.1. Under the countability and precise agent assumptions, we have for any value ofa>t, h≤t
EP(s,o|a>t,h≤t)[log P(o | s, a>t, h≤t) − log P(a>t | s, o, h≤t)] = 0.
Proof. By the precise agent assumption,s determines(o, a>t). In other words, there are functionsf : (T → S) → (T → O)
and g : (T → S) → (T>t → A) such that(o, a>t) = (f, g) ◦ s. In particular,
P(o | s, a>t, h≤t) =P(o | s, o≤t) ∝ P(o | s) =δf(s)(o), P (a>t | s, o, h≤t) =P(a>t | s) =δg(s)(a>t),
P(s, o| a>t, h≤t) =P(o | s, a>t, h≤t)P(s | a>t, h≤t) ∝ δf(s)(o)P(a>t | s, h≤t)P(s | h≤t) =δf(s)(o)δg(s)(a>t)P(s | h≤t).
Therefore we can compute the expectation, for any value ofa>t:
EP(s,o|a>t,h≤t) [log P(o | s, a>t, h≤t) − log P(a>t | s, o, h≤t)] =Eδf(s)(o)δg(s)(a>t)P(s|h≤t)

log δf(s)(o) − log δg(s)(a>t)

= EP(s|h≤t)

log δf(s)(f(s)) − log δg(s)(g(s))

= EP(s|h≤t) [log 1− log 1] = 0.
By Lemma D.1, we conclude that agency can be expressed as a functional of the agent’s predictions and preferences,
known as theexpected free energy[10,69]
−log P(a>t | h≤t) =EP(s,o|a>t,h≤t)[log P(s | a>t, h≤t) − log P(s, o| h≤t)] (EFE)
Furthermore, we can rewrite the expected free energy (EFE) in the following ways [69,84]
−log P(a>t | h≤t) = DKL

P(s | a>t, h≤t) | P(s | h≤t)

+ EP(s,o|a>t,h≤t)

− log P(o | s, h≤t)

= −EP(o|a,h≤t)

log P (o | h≤t)

− EP(o|a>t,h≤t)

DKL [P (s | o, a>t, h≤t) | P (s | a>t, h≤t)]

+ EP(o|a>t,h≤t)

DKL [P (s | o, a>t, h≤t) | P (s | o, h≤t)]

≥ −EP(o|a,h≤t)

log P (o | h≤t)

− EP(o|a>t,h≤t)

DKL [P (s | o, a>t, h≤t) | P (s | a>t, h≤t)]

.
11