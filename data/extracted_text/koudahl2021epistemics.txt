entropy
Article
On Epistemics in Expected Free Energy for Linear Gaussian
State Space Models
Magnus T. Koudahl 1,*, Wouter M. Kouw 1
 and Bert de Vries 1,2
/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001
/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046
Citation: Koudahl, M.T.; Kouw,
W.M.; de Vries, B. On Epistemics in
Expected Free Energy for Linear
Gaussian State Space Models. Entropy
2021, 23, 1565. https://doi.org/
10.3390/e23121565
Academic Editors: Thomas Parr,
Manuel Baltieri, Thijs van de Laar,
Kai Ueltzhöffer, Daniela Cialﬁ and
Karl Friston
Received: 28 September 2021
Accepted: 23 November 2021
Published: 24 November 2021
Publisher’s Note:MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright: © 2021 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
1 Department of Electrical Engineering, Eindhoven University of Technology,
5612 AZ Eindhoven, The Netherlands; w.m.kouw@tue.nl (W.M.K.); Bert.de.Vries@tue.nl (B.d.V .)
2 GN Hearing, JF Kennedylaan 2, 5612 AB Eindhoven, The Netherlands
* Correspondence: m.t.koudahl@tue.nl
Abstract: Active Inference (AIF) is a framework that can be used both to describe information pro-
cessing in naturally intelligent systems, such as the human brain, and to design synthetic intelligent
systems (agents). In this paper we show that Expected Free Energy (EFE) minimisation, a core feature
of the framework, does not lead to purposeful explorative behaviour in linear Gaussian dynamical
systems. We provide a simple proof that, due to the speciﬁc construction used for the EFE, the terms
responsible for the exploratory (epistemic) drive become constant in the case of linear Gaussian
systems. This renders AIF equivalent to KL control. From a theoretical point of view this is an inter-
esting result since it is generally assumed that EFE minimisation will always introduce an exploratory
drive in AIF agents. While the full EFE objective does not lead to exploration in linear Gaussian
dynamical systems, the principles of its construction can still be used to design objectives that include
an epistemic drive. We provide an in-depth analysis of the mechanics behind the epistemic drive of
AIF agents and show how to design objectives for linear Gaussian dynamical systems that do include
an epistemic drive. Concretely, we show that focusing solely on epistemics and dispensing with
goal-directed terms leads to a form of maximum entropy exploration that is heavily dependent on the
type of control signals driving the system. Additive controls do not permit such exploration. From
a practical point of view this is an important result since linear Gaussian dynamical systems with
additive controls are an extensively used model class, encompassing for instance Linear Quadratic
Gaussian controllers. On the other hand, linear Gaussian dynamical systems driven by multiplicative
controls such as switching transition matrices do permit an exploratory drive.
Keywords: active inference; epistemics; expected free energy; free energy principle; linear Gaussian
dynamical system
1. Introduction
Active Inference (AIF) is a mathematical description of information processing in
intelligent systems. In brief it states that agents, originally biological but in later years
also synthetic, act to minimise their surprise by seeking out stimuli and states that are
compatible with their model of the world. AIF is an attractive framework for designing
synthetic agents since AIF agents possess a well-balanced drive towards both explorative
(epistemic) and exploitative (pragmatic, goal-driven) behaviour. These characteristics
follow from choosing the Expected Free Energy (EFE) as the objective function for planning.
In this paper we explicitly derive the equations for applying AIF in linear Gaussian
dynamical systems (LGDS) with the EFE objective. In doing so we uncover a novel result
showing that, in the case of linear models, the epistemic term of the EFE objective becomes
constant. This means that any application of EFE in LGDS will not lead to exploration and
the resulting agents will engage in purely goal-driven behaviour. The proof is given in
Section 5.5. The remainder of the paper is structured as an in-depth analysis of the AIF
framework and the mechanisms driving its claims to epistemic behaviour. We isolate the
Entropy 2021, 23, 1565. https://doi.org/10.3390/e23121565 https://www.mdpi.com/journal/entropy
Entropy 2021, 23, 1565 2 of 23
epistemic term of the EFE and identify it as a (bound on) mutual information (MI). We
then show that, when considering epistemics in isolation instead of the full EFE construct,
it is still possible to generate an epistemic drive using the machinery of AIF. Isolating
epistemics corresponds to a special case of EFE where priors on future observations are left
unspeciﬁed [1,2]. We analyze the behaviour of the resulting epistemic drive and show that,
for the case of additive controls, the epistemic drive is independent of state transitions and
only depends on the prior variance associated with the belief over the control signal. On the
other hand, LGDS driven by multiplicative control signals do exhibit a dependence between
state transitions and the epistemic drive. Prior work on AIF in LGDS such as [3–7], have
focused mostly on the goal-directed components of the AIF framework. The results, while
impressive, largely do not address questions of epistemics and exploration. This means
that in cases where AIF is applied to LGDS, EFE and the resulting desirable exploratory
drive have so far not been thoroughly investigated. Our results show that, provided the
model in question can be cast as a LGDS, incorporating EFE does not lead to meaningful
exploration. The present paper makes the following contributions:
• We derive the ﬁltering and planning equations for AIF using EFE in LGDS,
Sections 4 and 5.
• We consider the epistemic term of EFE in isolation and show that in the case of
additive controls actions become decoupled from state transitions when computing the
epistemic term of EFE, Section 5.3. Therefore, we do not ﬁnd meaningful exploration
in this case.
• We show that in the case of multiplicative controls, meaningful exploratory behaviour
re-emerges when isolating the epistemic term of EFE, Section 5.4.
• We prove that when considering the full EFE construct, parts of the instrumental and
epistemic value terms cancel each other out. This renders the epistemic value constant.
In turn, the EFE functional becomes equivalent to KL control plus an additive constant,
Section 5.5.
• Finally, we provide simulations that corroborate our claims. We ﬁrst demonstrate the
differences in exploration when considering purely epistemic agents using both addi-
tive and multiplicative control signals. Finally we show that LGDS agents optimising
the full EFE do not exhibit epistemic drives under any circumstances, Section 6.
The core message is thus that translating AIF to the linear Gaussian case presents
unique challenges, speciﬁcally because the exploration/exploitation trade-off that follows
from EFE minimisation does not manifest. Code to reproduce our experiments is available
at github.com/biaslab/efe_lgds (accessed on 19 September 2021).
2. Exploration and Exploitation
In this section we aim to introduce the concepts of exploration and exploitation on
intuitive grounds before commencing with our formal analysis. Exploitation refers to goal
directed behaviour. An agent that engages in exploitation performs actions that are aimed
at optimising some measure of preferences which we will refer to as “Instrumental value”.
As an example, consider minimisation of mean squared error, cross entropy or a similar
cost function. Exploration on the other hand, refers to behaviour directed at collecting
information about the environment in which the agent is embedded. An agent that engages
in exploration performs actions that are aimed at acquiring further information about its
environment. We will refer to any metric that quantiﬁes the value of gathering information
as “Epistemic value”. Optimising epistemic value biases the agent towards actions that
gather information. We will refer to this bias in action selection as an “Epistemic drive”.
There are many candidates for the epistemic value term. We will brieﬂy consider two that
are particularly relevant for the present analysis. This will not be a formal comparison but
an intuitive introduction to the qualitative differences in behaviour that can be expected
from agents that employ different epistemic value terms. First, we can consider agents that
aim to maximise entropy (uncertainty). For such an agent, the epistemic drive biases it
towards seeking out areas of state space where uncertainty is high. By repeatedly visiting
Entropy 2021, 23, 1565 3 of 23
uncertain areas of state space, the agent collects observations in said areas which in turn
reduces uncertainty. As an example, we can consider an agent trying to navigate an arena.
The agent is equipped with a sensor and the arena is subject to strong winds that induce
sensor noise by pushing the agent around. In this case maximising entropy drives the agent
to seek out parts of the arena where the winds (and corresponding sensor noise) are high.
This means the agent collects information primarily in areas where more observations are
needed, due to increased sensor noise. Second, we can consider an agent that aims to
maximise MI, also known as Information Gain. We provide a formal treatment of MI in
Appendix A.4. Intuitively, MI scores the reduction in uncertainty that the agent expects
given a particular observation. In the present example, an agent that optimises MI might
correctly identify that although windy areas are noisy, collecting information in those areas
is unlikely to reduce uncertainty because the winds will remain high. Instead the agent
will prefer to move towards areas that have less wind, in order to obtain more accurate
measurements. This is the approach taken by AIF agents when optimising EFE [ 8,9].
Optimising both instrumental and epistemic value terms by selecting actions necessarily
entail a trade-off between short term gains (exploitation) and gathering information in
order to perform better in the future (exploration). Having agents that are able to optimally
balance this trade-off is therefore desirable because it allows for autonomous systems that
are able to learn to navigate novel environments in order to achieve desired goals. A
core feature of the EFE is that it presents a single objective functional that encompasses
both instrumental and epistemic value terms [ 8,9]. In order to formally unpack how
AIF manifests both instrumental and epistemic value terms, we now need to detail the
LGDS model that speciﬁes our agent before deriving the equations for computing the
EFE objective.
3. Generative Model
AIF is fundamentally a model-based approach [8,10]. As such, the core part of an agent
is given by a generative model. Given a generative model, the agent engages in a perception-
action loop with its environment. In practice this means the agent will, at any time step,
absorb a new observation and emit a new action. The ﬁrst step is always perception,
followed by action selection and emission. Letting x ∈Rd denote observations, z ∈Rn a
latent state vector and u actions (we will use “actions” and “controls” interchangeably to
refer to u throughout), the generative model for an agent at a single time step, indicated by
subscripts t, has the form
p(xt, zt|ut, zt−1) = p(xt|zt)  
Likelihood
p(zt|zt−1, ut)  
State transition
. (1)
This form can be extended, for example by including parameters θ. If applied recur-
sively, this model corresponds to a discrete-time state space model. A common approach
when designing AIF agents is to work directly with a policy deﬁned as a particular se-
quence of actions ut+1:T [8,11–13] where the subscript denotes discrete time steps ranging
from the next time step t + 1 to some known planning horizon T. In (1) we indicate
this by explicitly conditioning on ut. This sequence of actions is then considered ei-
ther as an explicit vector of control signals [8,13–15] or amortised for instance by neural
networks [16–19]. Proceeding in this way leads to a particular scheme for action selection
which we will detail in Section 5.
In this paper we consider the case of LGDS with multiplicative or additive controls.
To clarify the distinction between additive and multiplicative controls, we deﬁne “multi-
plicative controls” as state transitions of the form
p(zt|zt−1, ut) =N(zt|B(ut)zt−1, Σz) , (2)
Entropy 2021, 23, 1565 4 of 23
where zt ∈ Rn is a latent state vector, ut is a discrete control signal and B(ut) is the
transition matrix. We consider the case where the control signal functions as a selector
variable. Formally we deﬁne a vector of candidate transition matrices[B1, B2, . . .BS] and let
B(ut) =
S
∏
s=1
Buts
s . (3)
Here ut is a one-hot encoded vector ut = [ut1, . . ., utS] that takes values in uts ∈{0, 1}and
where ∑S
s=1 uts = 1. Each Bs is raised to the power given by uts, which means that only
the selected Bs will be active. The control signal therefore inﬂuences state transitions by
selecting the transition matrix B directly.
We can visualise this model using the Forney-style Factor Graph (FFG) formalism [20].
In an FFG, each edge represents a variable and each node a factor. An edge is connected to
a node if and only if the corresponding variable is an argument of that factor. Each edge
connects at most two nodes. When a variable is an argument of more than two factors, we
can circumvent the two node per edge limit by linking edges together through equality
factors. This effectively creates an auxiliary variable (a new edge) for which the posterior
beliefs are constrained to be equal to the beliefs for the original variable. The new edge
can also be attached to two factors, so by adding equality factors we can use the same
variable as an argument of multiple factors. Observed variables and clamped parameters
are denoted by a small black square and selected actions by a small black diamond. The
selection mechanism described by (3) is denoted by the multiplexer (MUX) node. Instead of
cluttering the graph by drawing the full set of [B1, B2, ··· BS] candidate transition matrices
as separate nodes, we denote them by a shaded circle. The circle contains S nodes and their
corresponding outgoing edges all connect to the MUX node. For a further introduction to
FFGs, see [21,22]. The FFG of the multiplicative model can be seen in Figure 1.
×
MUX
+ =
A
+
N
N
··· ···
zt−1
B1
BS
ut
Bs
zt
xt
Σz
Σx
Figure 1. Factor graph of the generative model of an agent with multiplicative control signals.
For comparison, we now consider the case of additive controls. For the additive case
the generative model is again given by (1). However exact model speciﬁcation is a little
more involved. We consider transition models of the form
p(zt|zt−1, ut) =N(zt|Bzt−1 + b(ut), Σz) , (4)
where B ∈Rn×n is a known transition matrix and b(ut) ∈Rn is a vector function that adds
to the latent state. To rigorously compare the multiplicative and additive control cases, ut
must remain a categorical selector variable. To that end, we introduce an auxiliary variable
bt. The purpose of bt is to allow ut to function as a categorical selector variable. Instead of
Entropy 2021, 23, 1565 5 of 23
selecting between transition matrices Bs, ut now selects the parameters Θs = {µs, Σs}of a
Gaussian input signal. Formally, we will write the generative model as
p(zt|zt−1, ut) =
∫
p(zt|zt−1, bt)p(bt|ut)dbt (5a)
=
∫
N(zt|Bzt−1 + bt, Σz)
S
∏
s=1
N(bt|Θs)uts dbt . (5b)
where we recognise a similar selection mechanism of (3) in the second term of (5b). Se-
lecting an action means ﬁxing ut = ˆut which leads to selecting one of the candidate
Gaussian distributions. With only a single Gaussian surviving, integration over bt becomes
straightforward and yields
p(zt|zt−1, ˆut) =N(zt|Bzt−1 + ˆµut , Σz + ˆΣut ) , (6)
where ˆΘs = {ˆµut , ˆΣut }represent the parameters of the Gaussian distribution selected by ˆut.
The factor graph of the additive model is shown in Figure 2 where the MUX node
now selects between Θ1:s. In both the multiplicative and additive settings, we employ a
likelihood term of the form:
p(xt|zt) =N(xt|Azt, Σx) , (7)
where A ∈Rd×n is a known emission matrix and Σx represents measurement or observa-
tion noise.
B + + =
N
A
+
N
MUX
··· ···
N
N
Θ1
ΘS
zt−1
Θs
bt
ut
zt
xt
Σz
Σx
Figure 2. Factor graph of the generative model of an agent with additive controls.
Having established the relevant model structures, we now examine the percep-
tion/action loop starting with perception.
4. Perception as Bayesian Filtering
The perception part of the action/perception loop involves making inference about
observed data and can be cast as a Bayesian ﬁltering problem. This part of the process
describes the agent inferring the hidden state of its environment based on the sequence
of actions taken so far, the resulting sequence of states visited and the accompanying
Entropy 2021, 23, 1565 6 of 23
observations. We can write the resulting inference problem in an intuitive way as a
prediction-correction process:
p(zt|x1:t)  
posterior
= p(xt|zt)
p(xt|x1:t−1)  
correction
based on xt
×p(zt|x1:t−1)  
prediction of zt
based on x1:t−1
. (8)
The above shows how the inference over states can be accomplished recursively (due to the
model obeying the Markov property) by ﬁrst computing a prediction for the next hidden
state zt to generate a prior belief which is then updated in a correction step based on the
observed data point xt.
The Bayesian ﬁltering problem is generic. To see how it translates to our case, we can
expand the prior predictive in terms of our generative model
p(zt|x1:t)  
state posterior
=
likelihood  
p(xt|zt)
p(xt|x1:t−1)  
evidence
prior predictive p(zt|x1:t−1)
  ∫∫
p(zt|zt−1, ut)  
state transition
δ(ut −ˆut)  
control
signal
p(zt−1|x1:t−1)  
state prior
dzt−1dut , (9)
We use δ(ut −ˆut) where δ is the Dirac- δ to ﬁx the value of ut to the selected action ˆut
for that particular time step. The particular value chosen for ˆut is the result of the action
selection procedure described in Section 5. The evidence term is given by
p(xt|x1:t−1) =
∫
p(xt|zt)
(∫∫
p(zt|zt−1, ut)δ(ut −ˆut)p(zt−1|x1:t−1)dzt−1dut
)
dzt . (10)
For the LGDS models considered in this paper, ﬁltering can be performed using the Kalman
ﬁltering equations. We will ﬁrst work this out explicitly in the multiplicative case and then
in the additive. To show how to perform ﬁltering in the multiplicative model, we start
by assuming that the agent has selected an action ut = ˆut by the procedure described in
Section 5. We can then calculate the prior predictive distribution of (9) according to our
model speciﬁcation (1) as
p(zt|x1:t−1) =
∫∫
p(zt|zt−1, ut)δ(ut −ˆut)p(zt−1|x1:t−1)dzt−1dut (11a)
=
∫∫
N(zt|B(ut)zt−1, Σz)  
State transition
δ(ut −ˆut)  
Selected
control signal
N(zt−1|µzt−1 , Σzt−1 )  
State prior
dzt−1dut (11b)
=
∫
N(zt|B( ˆut)zt−1, Σz)N(zt−1|µzt−1 , Σzt−1 )dzt−1 (11c)
= N(zt|ˆBtµzt−1  
µ−zt
, ˆBtΣzt−1
ˆBT
t + Σz
  
Σ−zt
) , (11d)
which we recognise as the prediction step of a Kalman ﬁlter [23]. We use the superscript −
notation to indicate that the variable in question is not based on the full data set x1:t but
instead on a smaller data set x1:t−1. In moving from (11b) to (11c) we rely on the sifting
property of the Dirac-δ to substitute the selected value for ut in (3). Since B(ut) is a function
of ut and ut is now ﬁxed to ˆut, we can directly substitute the selected parameterisation by
setting B(ut) = ˆBt where ˆBt denotes the parameterisation given by the selected Bs. This
takes us from (11b) to (11c). Finally we can rely on standard results for linearly related and
jointly Gaussian variables to go from (11c) to (11d), see for example [23] [Appendix A.1]
for details of this move in the context of Gaussian state space models or Appendix A.2 for
an abbreviated version. For the additive control case, we can calculate the prior predictive
distribution in a similar fashion. Starting from the model deﬁnition (1), we can write
Entropy 2021, 23, 1565 7 of 23
p(zt|x1:t−1) =
∫∫∫
p(zt|zt−1, bt)p(bt|ut)δ(ut −ˆut)p(zt−1|x1:t−1)dutdbtdzt−1 (12a)
=
∫∫∫
N(zt|Bzt−1+bt, Σz)
S
∏
s=1
N(bt|µs, Σs)uts
  
State transition
δ(ut−ˆut)  
Selected
control signal
N(zt−1|µzt−1 , Σzt−1 )  
State prior
dutdbtdzt−1 (12b)
=
∫∫
N(zt|Bzt−1 + bt, Σz)
S
∏
s=1
N(bt|µs, Σs) ˆuts N(zt−1|µzt−1 , Σzt−1 )dbtdzt−1 (12c)
=
∫∫
N(zt|Bzt−1 + bt, Σz) N(bt|ˆµut , ˆΣut  
Selected
parameters, ˆΘs
)N(zt−1|µzt−1 , Σzt−1 )dbtdzt−1 (12d)
=
∫
N(zt|Bzt−1 + ˆµut , Σz + ˆΣut )N(zt−1|µzt−1 , Σzt−1 )dzt−1 (12e)
= N(zt|Bµzt−1 + ˆµut
  
µ−zt
, Σz + ˆΣut + BΣzt−1 BT
  
Σ−zt
) . (12f)
We again denote the selected parameters for the additive control at the k-th time step as
ˆµuk and ˆΣuk . To proceed from (12b) to (12c) we rely on the sifting property of the Dirac-δ
and substitute the selected value for ut. The move from (12c) to (12d) acknowledges that
only the selected parameterisation is active once we substitute ut = ˆut as covered in
Section 3. The ﬁnal steps from (12d) to (12e) and from (12e) to (12f) uses standard results
for multiplication and marginalisation of jointly Gaussian variables for which we again
refer to [23] [Appendix A.1] and Appendix A.2. In summary, we see that when ut = ˆut the
model speciﬁcation given in (5b) reduces to a standard LGDS and can be updated using
the prediction step of the Kalman ﬁltering equations.
Both the additive and multiplicative models use similar likelihood models, meaning
they can be updated using the same Kalman correction step. To perform the Kalman
correction step, we need to apply Bayes rule
p(zt|x1:t)  
posterior
p(xt|x1:t−1)  
evidence
= p(xt|zt)  
likelihood
p(zt|x1:t−1)  
prior
, (13)
where the factor on the right-hand site (RHS) are given and the terms on the left-hand side
are the desired factors. This equation can be solved analytically. First, we evaluate the
RHS as
p(xt|zt)p(zt|x1:t−1) =N(xt|Azt, Σx)N(zt|µ−
zt , Σ−
zt ) (14a)
= N
([zt
xt
]⏐⏐⏐⏐⏐
[ µ−
zt
Aµ−
zt
]
,
[ Σ−
zt Σ−
zt AT
AΣ−
zt AΣ−
zt AT + Σx
])
. (14b)
Then if, for notational convenience, we rewrite the covariance matrix as
[Σ11 Σ12
Σ21 Σ22
]
≜
[ Σ−
zt Σ−
zt AT
AΣ−
zt AΣ−
zt AT + Σx
]
, (15)
(14b) can be written as the product of the state posterior
p(zt|x1:t) =N(zt|µ−
zt + Σ12Σ−1
22 (xt −Aµ−
zt ), Σ11 −Σ12Σ−1
22 Σ21) , (16)
and evidence
p(xt|x1:t−1) =N(xt|Aµ−
zt , Σ22) . (17)
Entropy 2021, 23, 1565 8 of 23
due to the theorem for decomposing a multi-variate Gaussian into the product of a conditional
distribution [23] [Appendix A.1].
Finally , we can also calculate the conditional distributionp(xt|zt, x1:t−1) [23] [Appendix A.1].
While this is not required for solving the Bayesian ﬁltering problem, it will prove useful
for deriving the epistemic value term (as derived in Appendix A.4) used for the action
selection procedure described in Section 5. We can ﬁnd it as
p(xt|zt, x1:t−1) =N(xt|Aµ−
zt + Σ21Σ−1
11 (zt −µ−
zt ), Σ22 −Σ21Σ−1
11 Σ12) . (18)
Having described perception as Bayesian ﬁltering, we now turn our attention to action
selection under AIF.
5. Action Selection under Active Inference
When we are interested in constructing AIF agents, arguably the core task is that of
action selection. Under AIF we solve this task by ﬁrst computing a prior over future control
signals. Technically, we seek to compute
p(ut+1:T) ∝ exp(−G(ut+1:T)) , (19)
i.e., the prior on controls is a softmax function of G(ut+1:T) [8] [Equation (7)]. Here,
G(ut+1:T) denotes the expected free energy (EFE) for a policy that extends into the future
until a known horizon T. We further discuss the computation of G(ut+1:T) in Section 5.1.
To obtain the control prior at time t + 1, we can marginalise this distribution as
p(ut+1) =
∫
···
∫
p(ut+1:T)dut+2 ··· duT (20a)
∝
∫
···
∫
exp(−G(ut+1:T))dut+2 ··· duT . (20b)
If we assume independent control priors for each time step, that is, if we assume
p(ut+1:T) =∏T
k=t+1 p(uk), or equivalently,
exp(−G(ut+1:T)) =
T
∏
k=t+1
exp(−G(uk)) , (21)
then the marginalisation (20) evaluates to
p(ut+1) ∝ exp(−G(ut+1)) . (22)
Since the marginalisation procedure is identical for any other time step, we can deduce
that the total EFE for a policy is equal to the exponentiated sum of EFEs at individual time
steps. That is
p(ut+1:T) ∝ exp(−G(ut+1:T)) =
T
∏
k=t+1
exp(−G(uk)) =exp
(
−
T
∑
k=t+1
G(uk)
)
. (23)
This suggests a recursive scheme over time steps for computing policy priors, similar to the
proposal by [13]. In the AIF literature the executed action is then commonly sampled from
p(ut+1:T) and emitted to the environment [8,13]. Other action selection approaches such as
selecting the MAP estimate (the arg max or mode of the distribution) are also possible. We
now turn our attention to how the expected free energy is computed.
5.1. Computing G—Expected Free Energy
The EFE is an AIF speciﬁc construct that attempts to model what the variational free
energy would be at a future time step, conditioned on a particular sequence of actions.
Of special interest is the decomposition of EFE into an epistemic (explorative) term and
Entropy 2021, 23, 1565 9 of 23
an instrumental (exploitative) term. It is due to this decomposition that AIF claims an
adaptive trade-off between exploration and exploitation [8,13]. Note that we provide the
derivation here only for the simplest case. There are extensions to the EFE such as [24] that
include additional terms which induce changes in the agent’s behaviour. Including these
additional terms are not necessary for our core argument so we omit them here and refer
interested readers to [24,25]. To show how we arrive at our formulation for EFE, we ﬁrst
need to introduce variational inference. While the ﬁltering equations in Section 4 permit
analytical solutions by applying Bayes rule directly, this is often not the case as solving the
required integrals can become intractable. In those cases, we can instead approximate the
exact solution p(zt|x1:t) by a recognition density q(zt). Formally we accomplish this by
minimising the KL divergence between the exact solution and our recognition density
KL[q(zt)||p(zt|x1:t)] =
∫
q(zt) log q(zt)
p(zt|x1:t)dzt (24)
If we now multiply and divide by p(xt|x1:t−1) inside the log-operator,
KL[q(zt)||p(zt|x1:t)] =
∫
q(zt)
[
log q(zt)
p(xt, zt|x1:t−1) + log p(xt|x1:t−1)
]
dzt (25)
=
∫
q(zt) log q(zt)
p(xt, zt|x1:t−1)dzt
  
VFE F[q]
+ log p(xt|x1:t−1)  
log-evidence
. (26)
We obtain the variational free energy (VFE) F[q] by noting that log p(xt|x1:t−1) is not
dependent on zt. Since the term p(xt, zt|x1:t−1) in the denominator of F[q] is given by our
generative model, we can choose constraints on q(zt) to make optimization of (26) tractable.
Minimizing F[q] then constitutes an upper bound on −log p(xt|x1:t−1), meaning we can
optimise (26) to obtain an approximate solution to our original inference problem. We
deﬁne the optimal recognition density q∗as the one that minimises F[q]:
q∗= arg min
q
F[q] (27)
For further background on variational inference we refer interested readers to the seminal
works by [26,27]. Now we are ready to introduce the EFE. To do so, we start by obtaining
our best estimate of the time step k in question by integrating out contributions from past
time steps as
p(xk, zk|uk) =
∫
p(xk, zk|zk−1, uk)p(zk−1|x1:t)dzk−1 , (28)
where we write k instead of t to indicate that we are referring to an arbitrary time step
within the planning horizon t < k ≤T. p(zk−1|x1:t) denotes the posterior state estimate
at the previous time step given all available observations. For notational brevity we
suppress the dependency on x1:t on the LHS. Unless otherwise noted, all distributions are
conditioned on prior observations moving forward. For LGDS p(zk−1|x1:t) is available
from recursive application of the ﬁltering equations described in Section 4. For k = t + 1 it
is given by (16) and for k > t + 1, p(zk−1|x1:t) is given by (11) in the case of multiplicative
controls and (12) in the case of additive controls. When p(zk−1|x1:t) can not be obtained
through application of Bayes rule (providing the exact solution), one can employ variational
inference (providing an approximate solution). In that case derivations must instead
proceed in terms of the approximate posterior q(zk−1|x1:t). Now we can write out the
variational free energy conditioned on a particular action uk = ˆuk and recognition density
as F[q; uk]. Note that while F[q] is a functional (a function of a function) of q, we also
explicitly include conditioning on action given by the parameter uk. To differentiate, we
separate them with a semicolon when writing F[q; uk]. Given the factorisation in (23), it
Entropy 2021, 23, 1565 10 of 23
is sufﬁcient to consider a single time step k since we can substitute any value for k. This
gives us
F[q; uk] =
∫
q(zk|uk) log q(zk|uk)
p(xk, zk|uk)dzk . (29)
However this expression includes observations xk which are not available, since we are
working with time steps in the future ( t < k ≤T) and the future is by deﬁnition not
observed yet. To alleviate this issue, we can take the expectation of this expression with
respect to the data generating distribution over observations. When the data generating
distribution is available from the generative model, we can equivalently write p(xk|zk)
instead of q(xk|zk). This gives the expression for the expected free energy at the k’th
time step:
G[q; uk] =
∫∫
q(xk|zk)
F[q; uk] if xk was observed
  [
q(zk|uk) log q(zk|uk)
p(xk, zk|uk)dzk
]
dxk
  
Expected F[q; uk] since xk is not yet observed
. (30)
As with the VFE in (26), we are interested in the minimum of (30) which once again entails
ﬁnding q∗. For clarity of notation we deﬁne the solution as
G(uk) =G[q∗; uk] =arg min
q
G[q; uk] , (31)
where G(uk) is used to compute the policy prior by plugging into (23). Note that G(uk) is
a scalar value that denotes the expectation of F[q∗; uk] under a particular set of constraints
on q and given a speciﬁc action uk. To get an intuition for G(uk) it can be useful to think
of the computation as a two-step procedure consisting of an inner and an outer loop. The
inner loop performs variational inference and ﬁnds q∗conditioned on an action uk. The
outer loop then computes the resulting EFE by taking the expectation of F[q∗; uk] under
the matching data generating distribution. A core property of EFE is that it introduces an
epistemic value term into the optimisation. This leads agents that optimise EFE to seek out
areas of state space that have high information gain under the current model, allowing for
a principled trade-off between exploration and exploitation [9,28]. To show how this comes
about, we can decompose the EFE into a cross-entropy loss and a mutual information (MI)
term where the latter quantiﬁes the information gain (in nats or bits) about hidden states
zk from observing outcomes xk. For the following derivation we will need a bound, the
details of which can be found in Appendix A.3. Starting from (30), we can factorise the
denominator as p(xk, zk|uk) =p(zk|xk, uk)p(xk), leading to
G(uk) =
∫∫
q(xk|zk)
[
q(zk|uk) log q(zk|uk)
p(zk|xk, uk)p(xk)dzk
]
dxk (32a)
=
∫∫
q(xk|zk)
[
q(zk|uk)
[
log q(zk|uk)
p(zk|xk, uk) −log p(xk)
]
dzk
]
dxk . (32b)
Now we apply the bound from Appendix A.3 to swap q for p in the denominator. Making
use of this inequality is a standard move across the active inference literature [8,11,13,25,29].
The bound becomes exact when we perform exact inference which is the case in the
models we consider here and the discrete models often employed in AIF research, see
for instance [8,25]). Instead of applying the bound, another option is to utilise (30) as is,
see [30] for an example. We proceed as
Entropy 2021, 23, 1565 11 of 23
∫∫
q(xk|zk)
[
q(zk|uk)
[
log q(zk|uk)
p(zk|xk, uk) −log p(xk)
]
dzk
]
dxk (33)
≥
∫∫
q(xk|zk)
[
q(zk|uk)
[
log q(zk|uk)
q(zk|xk, uk) −log p(xk)
]
dzk
]
dxk .
Finally we split the integral and integrate over zk to obtain
G(uk) ≥
∫∫
q(xk, zk|uk) log 1
p(xk)dzkdxk −
∫∫
q(xk, zk|uk) log q(zk|xk, uk)
q(zk|uk) dzkdxk (34a)
=
∫
q(xk|uk) log 1
p(xk)dxk −
∫∫
q(xk, zk|uk) log q(zk|xk, uk)
q(zk|uk) dzkdxk (34b)
=
∫
q(xk|uk) log 1
p(xk)dxk
  
cross-entropy
−
∫∫
q(xk, zk|uk) log q(zk, xk|uk)
q(zk|uk)q(xk|uk)dzkdxk
  
Mutual Information
. (34c)
In the last line we multiply and divide by q(xk|uk) to make the MI term explicit. Readers
familiar with the broader AIF literature such as [8,9,12] might not immediately recognise
the form of (34c) as a common decomposition of the EFE. The equivalence between (34c)
and the EFE was originally noted in [ 8] where the move from (34b) to (34c) is done to
show the relation between AIF and InfoMax methods. An advantage of writing the EFE
as (34c) is that it cleanly shows how the EFE can be viewed as a combination of two
well-known and widely established objectives. From (34c), we see that G(uk) decomposes
into a (bound on a) cross-entropy term minus an MI term. Maximising MI is a known
way to induce exploration (i.e., information gain about hidden states from observations) in
agents and has been employed in multiple settings both within the control theory [31,32]
and reinforcement learning literature [17,30,33]. The cross-entropy loss is between a prior
p(xk) and the posterior distribution q(xk|uk) over future observations. This allows for
interpreting p(xk) as a target/goal prior [25,34]. It endows the agent with an instrumental
value term that elicits goal-directed behaviour from inferred policies.
Taking this view,G(uk) can be adequately viewed as scoring the behavior resulting
from the action uk as a balancing act between MI-based explorative and cross-entropy-
based exploitative terms. We now examine each of these terms separately to understand
how they work in the linear Gaussian case before considering them jointly. We begin by
focusing on the MI and how it may drive exploration when considered in isolation.
5.2. Mutual Information Computation
Epistemic behaviour in AIF agents can be considered to be driven by minimising
negative MI, as shown in (34c). MI is in general deﬁned as
I[x, z] =
∫∫
p(x, z) log p(x, z)
p(x)p(z)dxdz = H[z] −H[z|x] =H[x] −H[x|z] . (35)
Note that we can write (35) in terms of entropies of either x or z. We can do this since MI
is symmetric in its arguments. In the LGDS models we consider, we can evaluate the MI
component of G(uk) as
I[xk, zk] =1
2 log |I + Σ−1
x AΣ−
zk AT|. (36)
The detailed derivation of (36) can be found in Appendix A.4. To facilitate purely epistemic
behaviour, AIF agents can optimise this quantity by selecting appropriate control signals.
We will therefore use optimisation of MI as the basis from which to investigate purely
epistemic behaviour.
Entropy 2021, 23, 1565 12 of 23
5.3. Pure Exploration as a Function of Additive Control Signals
To show the relation between exploration and controls in the additive case, we now
need to show how MI depends on the control signal uk. For clarity of notation we will
do the derivation for the case k > t + 1, i.e. we will write in terms of the prior predic-
tive p(zk−1|x1:k−2) =N(zk−1|µ−
zk−1 , Σ−
zk−1 ) obtained from (12) instead of p(zk−1|x1:k−1) =
N(zk−1|µzk−1 , Σzk−1 ) from (16). We do so since observations are not available for k > t,
meaning we can not perform a full ﬁltering step for the prior time step k −1 unless
k = t + 1. For the case k = t + 1, we can perform ﬁltering for the state prior and can
therefore substitute in parameters of p(zk−1|x1:k−1) where appropriate. We start the deriva-
tion by generating a prediction from our model using (12). We can ﬁnd the relevant joint
distribution at the k’th time step by plugging the result into (14b) to obtain
p
([zk
xk
])
=N
([zk
xk
]⏐⏐⏐⏐⏐
[
Bµ−
zk−1 + ˆµuk
A[Bµ−
zk−1 + ˆµuk ]
]
,
[ ˆΣuk + Σz + BΣ−
zk−1 BT [ ˆΣuk + Σz + BΣ−
zk−1 BT]AT
A[ ˆΣuk +Σz+BΣ−
zk−1 BT] A[ ˆΣuk +Σz+BΣ−
zk−1 BT]AT +Σx
])
, (37)
where we see that the control signal contributes an additive term ˆΣuk which is the variance
associated with the selected action. Interestingly, this means that if we let ˆΣuk go to 0, the
covariance matrix becomes identical to the multiplicative case detailed in Section 5.4. We
plug the marginal over states zk into (36) to get
I[xk, zk] =1
2 log |I + Σ−1
x A
[
BΣ−
zk−1 BT + Σz + ˆΣuk
]
  
Σzk
AT| (38a)
= 1
2 log |I + Σ−1
x ABΣ−
zk−1 BT AT
  
Dynamics dependent
+ Σ−1
x AΣz AT
  
Policy independent
+ Σ−1
x A ˆΣuk AT
  
Policy dependent
|. (38b)
We notice that the MI decomposes into three terms. We label the ﬁrst “Dynamics dependent”
since it depends only on the transition matrix B, observation noise Σx and the prior state
variance Σ−
zk−1 . The second term is labeled “Policy independent” since it only depends on
the observation noise Σx and transition noise Σz. Note that neither of the ﬁrst two terms are
inﬂuenced by the control signal. The last term is the only one to include ˆΣuk and is therefore
“Policy dependent”. Crucially, the policy dependent term only depends on the variance of
the selected control signal ˆΣuk and the observation noiseΣx. In other words, it isindependent
of the latent state zk. Since both ˆΣuk and Σx are available a priori, we can precompute the
effect of a policy on the epistemic value term before receiving any observations. Further
the result is also independent of the trajectory taken by the agent. Therefore in the case of
additive controls, maximising MI does not produce targeted exploration. This necessitates
the use of a different model structure when epistemic behaviour is desired. A similar result
to ours was obtained by [35] for the case of linear dynamics with additive controls.
5.4. Pure Exploration as a Function of Multiplicative Control Signals
To show how epistemic behaviour re-emerges as a function of multiplicative control
signals, we now need to show how MI depends on the choice of transition matrix ˆBk.
We again proceed by generating a prediction from our model using (11). Plugging this
into (14b) gives us the joint distribution as
p
([zk
xk
])
= N
([zk
xk
]⏐⏐⏐⏐⏐
[ ˆBkµ−
zk−1
A ˆBkµ−
zk−1
]
,
[
Σz + ˆBkΣ−
zk−1
ˆBT
k [Σz + ˆBkΣ−
zk−1
ˆBT
k ]AT
A[Σz + ˆBkΣ−
zk−1
ˆBT
k ] A[Σz + ˆBkΣ−
zk−1
ˆBT
k ]AT + Σx
])
. (39)
Entropy 2021, 23, 1565 13 of 23
Plugging the above into (36) we ﬁnd that
I[xk, zk] =1
2 log |I + Σ−1
x A
[
ˆBkΣ−
zk−1
ˆBT
k + Σz
]
  
Σzk
AT| (40a)
= 1
2 log |I + Σ−1
x A ˆBkΣ−
zk−1
ˆBT
k AT
  
Policy dependent
+ Σ−1
x AΣz AT
  
Policy independent
|. (40b)
We see that MI now decomposes into two terms. The ﬁrst term depends on ˆBk and can
be controlled by selecting appropriate transition matrices. The second is independent of
policy as it only involves process Σz and observation noise Σx. Note that similar terms also
appear in the additive case. The difference between the additive and multiplicative cases is
that the choice of transition matrix ˆBk is now under the control of the agent. To maximise
MI, the agent must therefore select the ˆBk that maximises the entropy of its latent states zk.
Taking this view offers a nice intuitive explanation for the resulting exploratory drive: To
gain the most information, we must perform the actions that lead to the most uncertain
outcomes as described in Section 2. To learn the most, we must sample where we know
the least.
5.5. Instrumental Value and Expected Free Energy
We now turn our attention to the instrumental value term of G(uk) after which we
analyse the full EFE construct. Recall from (34c) that the instrumental value term is a
cross-entropy of the form
∫
q(xk|uk) log 1
p(xk)dxk =
∫
q(xk|uk) log q(xk|uk)
p(xk)q(xk|uk)dxk = KL[q(xk|uk)||p(xk)] +H[xk|uk] . (41)
In many cases it is not trivial to obtain q(xk|uk) due to intractable integrals. However
in the LGDS we are considering, which only involve linear Gaussian relations, it has a
tractable expression given by (17). The KL divergence between two Gaussian distributions
is given by
KL[q(xk|uk)||p(xk)] =1
2
(
log |Σp|
|Σq|+ n + (µq −µp)TΣ−1
p (µq −µp) +tr[Σ−1
p Σq]
)
. (42)
We use subscripts {p, q}to denote whether a term comes from p(xk) or q(xk|uk) and use
{µ, Σ}for the parameters of the corresponding distribution. We can now consider both
terms of (34c) jointly in the case of LGDS. Taking (35) and (41) together and making the
conditioning on uk in (35) explicit, we see the full objective comes out as
G(uk) ≥KL[q(xk|uk)||p(xk)] +H[xk|uk]  
Instrumental value
−H[xk|uk] +H[xk|zk, uk]  
Negative MI
(43a)
= KL[q(xk|uk)||p(xk)]  
Risk
+ H[xk|zk, uk]  
Ambiguity
, (43b)
where we recover the familiar risk and ambiguity terms. In the speciﬁc case of LGDS the
inequality becomes an equality when we perform exact inference following the equations
laid out in Section 4. However note that when combining the instrumental and epistemic
terms instead of considering them in isolation, we perform a seemingly innocuous cancel-
lation and remove the entropy H[xk|uk] from the equation. Previously H[xk|uk] appeared
twice since we considered the epistemic and instrumental terms separately. However when
considering the full EFE construct, this is no longer necessary and we are left with just the
Entropy 2021, 23, 1565 14 of 23
ambiguity H[xk|zk, uk]. Using the entropy expression for a Gaussian distribution, we can
write the ambiguity as
H[xk|zk, uk] =1
2
(
n log 2π + log |Σ22 −Σ21Σ−1
11 Σ12|+ n
)
(44)
Recalling the form of the joint given in (14b), we can write each block of the covariance
matrix out and ﬁnd
H[xk|zk, uk] =1
2
(
n log 2π + log |AΣzk AT + Σx  
Σ22
−AΣzk
Σ21
Σ−1
zk
Σ−1
11
Σzk AT
  
Σ12
|+ n
)
(45a)
= 1
2
(
n log 2π + log |AΣzk AT + Σx −AΣzk AT|+ n
)
(45b)
= 1
2 (n log 2π + log |Σx|+ n) (45c)
The cancellation that follows from using a cross-entropy term to drive goal-directed be-
haviour means that we are left with only the conditional entropy to drive exploration. The
above derivation shows that this term is constant and only depends on the observation
noise variance Σx. This proves that EFE minimisation in LGDS does not lead to exploration.
In fact, minimising a KL divergence between a predicted and desired state (the risk term) is
the objective of KL control [36] or message passing based simulations of AIF that minimise
variational free energy [6,37]. We conclude that in the case of LGDS, the EFE objective is
equivalent to the objective of KL control plus an additive constant that depends only on
the observation noise variance.
6. Experiments
We investigate the proposed agents in three different settings. First, we investigate
pure epistemics in the additive case and show that they do not manifest. Second, we inves-
tigate pure epistemics in the multiplicative case and conﬁrm that the agent does indeed
perform maximum entropy exploration. Finally we provide comparable experiments for
full EFE and show that it indeed reduces to a KL divergence plus a constant.
6.1. Pure Epistemics for Additive Controls
In this section we investigate how the epistemic component of EFE behaves in the
additive case. In particular we investigate the effects of different transitions on the epistemic
value assigned to a policy. For this experiment the transition model is given by (4). We
deﬁne the state prior as
p(zt−1|z1:t−1) =N
(
zt−1
⏐⏐⏐⏐⏐
[1
1
]
,
[1 0
0 1
])
, (46)
and set both transition and observation noise to identity matrices. We allow the agent a
single action by setting T = t + 1, which will be the case for all experiments. Further we
deﬁne the transition matrix B, emission matrix A and observation noise Σx as
A =
[1 0
0 1
]
, B =
[1 0
0 1
]
, Σx =
[1 0
0 1
]
. (47)
Entropy 2021, 23, 1565 15 of 23
Note that in the additive case, neither matrix has to be time-varying and so we remove
the subscripts. We will also use the same parameterization of Σx for all experiments. We
compare 4 different candidate parameterisations of the control signal
Θ1 =
{
µ1 =
[1
1
]
, Σ1 =
[1 0
0 1
]}
, Θ2 =
{
µ2 =
[10
10
]
, Σ2 =
[1 0
0 1
]}
Θ3 =
{
µ3 =
[1
1
]
, Σ3 =
[3 0
0 3
]}
, Θ4 =
{
µ4 =
[10
10
]
, Σ4 =
[3 0
0 3
]}
. (48)
We choose Θ1 to function as a baseline. For comparison Θ2 shares the same covariance
matrix but offers a higher displacement of the mean. Θ3 shares the mean parameter with
Θ1 but has higher variance. Finally Θ4 increases both the mean and variance over Θ1.
According to (38) varying the mean should not affect the epistemic value since it does not
enter into the MI computation. On the other hand, we expect higher variance to affect
the policy independent term and lead to increased epistemic value. Consequently we
hypothesise that Θ1 and Θ2 will lead to identical results in terms of epistemics even though
they result in very different posterior states. Following the same line of reasoning, we
hypothesise that Θ3 and Θ4 will lead to identical results. This in turn implies that Θ1 and
Θ3 will lead to different values even though the displacement is the same and that a similar
pattern will hold for Θ2 and Θ4. Results are shown in Table 1, rounded to 3 digits.
Table 1. Epistemic value for additive control signals given state transitions.
Transition −MI
Θ1 −1.386
Θ2 −1.386
Θ3 −1.609
Θ4 −1.609
We observe that as hypothesised, MI is not affected by the state transition ( Θ1 and
Θ2 show identical values). We do ﬁnd an effect of changing the variance which is again
independent of the mean ( Θ3 and Θ4 show identical values). This simple experiment
conﬁrms our hypotheses given by (38b): Changing the mean of the control signal does
not affect the epistemic term. Changing the variance of the control signal does affect the
epistemic term. We conclude that when considering purely epistemic value and additive
controls, state transitions and exploration are decoupled. Any effect of the control signal
on epistemics is only proportional to the variance of the control, can be pre-computed and
does not depend on the agent’s trajectory.
6.2. Pure Epistemics for Multiplicative Controls
For comparison, we now perform an analogous experiment for the case of multiplica-
tive controls. We deﬁne all quantities in the same way as the additive case. The only change
we introduce is deﬁning four transition matrices B1:4 to replace Θ1:4. The four candidate
transitions we consider are
B1 =
[0.1 0
0 0.1
]
, B2 =
[1 0
0 1
]
B3 =
[10 0
0 10
]
B4 =
[100 0
0 100
]
. (49)
Following (40), we hypothesise that larger transitions should lead to lower negative MI by
virtue of increasing the value of the policy dependent term. We test this hypothesis across
four orders of magnitude and show the results in Table 2.
Entropy 2021, 23, 1565 16 of 23
Table 2. Epistemic value for multiplicative control signals given state transitions.
Transition −MI
B1 −0.698
B2 −1.099
B3 −4.625
B4 −9.211
We observe that, as hypothesised, negative MI decreases as a function of the size of the
state transition. Larger transitions lead to lower negative MI though the exact relationship
is nonlinear in the size of the transition.
6.3. Lack of Epistemics for Expected Free Energy
To investigate the behaviour of AIF agents optimising the full EFE construct, we now
repeat both the additive and multiplicative experiments but introduce a goal prior p(xt).
We deﬁne the state prior and the goal as
p(zt−1|x1:t−1) =N
(
zt−1
⏐⏐⏐⏐⏐
[1
1
]
,
[1 0
0 1
])
, p(xt) =N
(
xt
⏐⏐⏐⏐⏐
[3
3
]
,
[3 0
0 3
])
. (50)
Both the multiplicative and the additive agent employ the same emission matrix A. For the
multiplicative agent we further deﬁne the set of candidate transition matrices B1:4
A =
[1 0
0 1
]
, B1 =
[1 0
0 1
]
, B2 =
[2 0
0 2
]
, B3 =
[3 0
0 3
]
, B4 =
[4 0
0 4
]
. (51)
Here we choose B1 as the identity matrix to serve as a baseline. B2 moves the agent towards
the goal but stops short while B4 overshoots by the same amount. This means that either
transition puts the agent at the same distance from the goal but with different variances
and hence different values of the policy dependent term. Finally we allow B3 to move the
agent directly to the goal. For the additive case we set the transition matrix B = B1 and
consider the set of candidate parameterisations Θ1:4
Θ1 =
{
µ1 =
[0
0
]
, Σ1 =
[1 0
0 1
]}
, Θ2 =
{
µ2 =
[2
2
]
, Σ2 =
[1 0
0 1
]}
Θ3 =
{
µ3 =
[0
0
]
, Σ3 =
[2 0
0 2
]}
, Θ4 =
{
µ4 =
[2
2
]
, Σ4 =
[2 0
0 2
]}
. (52)
where we again vary the mean and variance parameters following a similar logic as for the
additive experiments. Notably, both Θ2 and Θ4 take the agent directly to the goal but with
different variances. We ﬁrst examine results in the multiplicative case, shown in Table 3.
Table 3. EFE for multiplicative controls.
Transition KL Ambiguity G Instrumental Epistemic
B1 1.33 2.84 4.17 5.27 −1.10
B2 0.64 2.84 3.48 5.27 −1.79
B3 1.37 2.84 4.21 6.60 −2.40
B4 3.54 2.84 6.38 9.27 −2.89
Here the ﬁrst column shows the KL divergence between the posterior predictive
distribution over observations q(xt|ut) and the goal prior p(xt) after the corresponding
transition. The second column show the additive constant that corresponds to the ambi-
guity term. The full EFE is displayed in the third column marked G. Finally the last two
Entropy 2021, 23, 1565 17 of 23
columns display the cross-entropy and negative MI terms as Instrumental and Epistemic
value respectively. From Table 3 we see that the lowest KL, and consequently lowest
G, is obtained when selecting the B2 transition matrix. Recall that B2 stopped short of
the goal while B3 placed the agent directly on top of it. However, because controls are
multiplicative, B3 also results in substantially larger variance which is penalised in the KL.
To show that KL is indeed the only driving factor, we can examine the second column,
containing the Ambiguity term. We see that it is constant since the observation noise is
constant. In turn, we ﬁnd that the EFE (third column, G) can be written as the sum of the
KL and Ambiguity columns. For completeness we have also calculated the cross-entropy
(Instrumental value) and MI (Epistemic value). Here we observe similar patterns as in the
purely exploratory case; larger transitions lead to lower negative MI. This is accurately
balanced by the instrumental term though, highlighting an important point: Our result
that EFE does not lead to epistemics is only revealed when we consider a particular way of
writing the EFE. If we had instead proceeded from the cross-entropy/MI decomposition,
the ambiguity constant would not have materialised. We can create a similar table for the
additive case, shown in Table 4.
Table 4. EFE for additive controls.
Transition KL Ambiguity G Instrumental Epistemic
Θ1 3.05 2.84 5.88 7.27 −1.39
Θ2 0.38 2.84 3.22 4.60 −1.39
Θ3 3.16 2.84 5.99 7.60 −1.61
Θ4 0.49 2.84 3.33 4.94 −1.61
We observe that the lowest KL and G corresponds to the transition parameterised
by Θ2 as it takes the agent directly to the goal with small variance. What is interesting
about Table 4 is the ambiguity column. We obtain the same additive constant as in the
multiplicative case which corroborates our results. Even though the dynamics are different
and there are substantial differences in both the instrumental and epistemic value terms,
the EFE can still be decomposed as a KL and an additive constant that only depends on the
observation noise.
7. Discussion
Viewing EFE from the point of view of mutual information and cross entropy allows
for isolating the epistemic and instrumental value terms so they can be investigated sepa-
rately. This angle was originally taken in [8] and used as a method of relating AIF to other
frameworks. Recent work [1,9] investigates a similar decomposition in the discrete case to
highlight how pure exploration and exploitation manifest. Our results as well as [1,8,9]
all explore how the EFE operates in speciﬁc model architectures. Additionally [1,2,8] also
note the equivalence between the mutual information term and the objective of optimal
Bayesian design. While work such as [29,30,38] have investigated this link in the general
case, deriving the speciﬁc equations for a wider class of model architectures promises to be
a fruitful area for further research. In those cases, the approach followed in our analysis
presents a straightforward way to derive the form of the EFE objective by ﬁrst decomposing
it into a pair of known objective functions and then deriving the expressions separately.
Because EFE can be written in terms of marginal/conditional distributions over the
latent states z, the analysis presented here applies to any model that utilises a linear
likelihood. The results do not depend on the transition model, as demonstrated by our
experiments showing similar behaviour for EFE minimization using two different transition
models. Our results are consequently equally applicable for a large class of transition
models such as auto-regressive models, Gaussian process state space models or deep
neural networks without additional adaptation provided the observation model remains
linear and Gaussian.
Entropy 2021, 23, 1565 18 of 23
On a similar note, a clear limitation of the present work is the strong reliance on
linear observation models. We chose to focus on this case since it allows for an analytical
expression of the MI term. However, in general MI is a difﬁcult quantity to compute and
one often has to rely on approximations. When approximations are involved, the present
analysis is not necessarily applicable, since the decoupling of control signals and epistemics
is only demonstrated for the linear case.
In special cases, one can also approximate the joint covariance matrix instead of the
mutual information - this is the case for extended Kalman ﬁlters for example. In these
cases, the present analysis can still apply. Investigating different methods for handling
non-linearities is an interesting area for future work on AIF in Gaussian state space models
(both linear and non-linear), that can prove useful for neuroscientists and engineers alike.
8. Conclusions
In this paper we have shown how to apply AIF in linear Gaussian state space models.
We have derived the expressions for EFE in the linear Gaussian case and investigated how
the epistemic value terms function. In particular we have shown that in the case of LGDS,
EFE reduces to a KL divergence and an additive constant that only depends on observation
noise. We therefore conclude that, in the linear Gaussian case, EFE minimization does not
lead to epistemic behaviour.
Additionally we have provided an analysis of the epistemic value term considered in
isolation, since the cancellation that leads to an absence of epistemic drive for the full EFE
is not present when the instrumental term is not included. Our analysis showed that using
additive control signals renders the epistemic value term independent of state transitions.
This in turn means that any contribution to the epistemic value term is only dependent
on the variance associated with the control signal. In other words, it is independent of
any observations the agent might receive and any states it may visit, as was previously
demonstrated by [35].
Finally we have shown that utilising multiplicative controls, i.e. selecting from a set
of candidate transition matrices, circumvents this problem in the purely epistemic case
and provides a meaningful interpretation of controls as inducing epistemic behaviour. The
resulting setup is reminiscent of the classical Hidden Markov Model that is commonly seen
in AIF. Future work can investigate this link by applying recent advances for the discrete
case such as [13] to continuous state spaces with multiplicative control signals.
Author Contributions: Conceptualization, M.T.K.; Formal analysis, M.T.K.; Methodology, M.T.K.;
Writing—original draft, M.T.K.; Writing—review & editing, M.T.K., W.M.K. and B.d.V . All authors
have read and agreed to the published version of the manuscript.
Funding: This research was funded by Dutch Research Council grant number NWO perspective
program P16-25.
Acknowledgments: This work is part of the research programme Efﬁcient Deep Learning with
project number P16-25 project 5, which is (partly) ﬁnanced by the Netherlands Organisation for
Scientiﬁc Research (NWO). The authors also wish to thank the rest of the BIASLab team for helpful
discussions throughout the writing of this manuscript and the reviewers for providing helpful and
thorough feedback, in particular reviewer 3 who suggested the example used in Section 2.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
Appendix A. Derivations
Appendix A.1. Perception as Bayesian Filtering
To derive the ﬁltering equations in Section 4, we need to infer p(zt|x1:t). We consider
a model of the form
p(xt, zt, ut|zt−1) =p(xt|zt)p(zt|zt−1, ut)p(ut) . (A1)
Entropy 2021, 23, 1565 19 of 23
We can write the inference task as
p(zt|x1:t) =p(zt|xt, x1:t−1) (A2a)
= 1
p(xt|x1:t−1) p(xt, zt|x1:t−1) (A2b)
= 1
p(xt|x1:t−1) p(xt|zt)p(zt|x1:t−1) (A2c)
= 1
p(xt|x1:t−1) p(xt|zt)
∫∫
p(zt, zt−1, ut|x1:t−1)dzt−1dut (A2d)
= 1
p(xt|x1:t−1)  
Evidence
p(xt|zt)  
Likelihood
∫∫
p(zt|zt−1, ut)  
State transition
p(ut)
Control
prior
p(zt−1|x1:t−1)  
State prior
dzt−1dut . (A2e)
Now assuming that observations x1:t = ˆx1:t are available and the state prior is available
from the last time step, we can select a control by setting
p(ut) =δ(ut −ˆut) . (A3)
The inference problem becomes
p(zt|ˆx1:t) = 1
p( ˆxt|ˆx1:t−1) p( ˆxt|zt)
∫∫
p(zt|zt−1, ut)δ(ut −ˆut)p(zt−1|ˆx1:t−1)dzt−1dut (A4a)
= 1
p( ˆxt|ˆx1:t−1) p( ˆxt|zt)
∫
p(zt|zt−1, ˆut)p(zt−1|ˆx1:t−1)dzt−1 . (A4b)
The likelihood is then available from the generative model and the state prior is available
from the previous time step. We can ﬁnd the evidence by marginalising out zt as
p( ˆxt|ˆx1:t−1) =
∫
p( ˆxt|zt)
∫
p(zt|zt−1, ˆut)p(zt−1|ˆx1:t−1)dzt−1dzt . (A5)
Solving these equations amount to performing Bayesian ﬁltering, synonymous
with perception.
Appendix A.2. Linearly Related Gaussian Variables
In this section we show how to obtain a posterior marginal, given linearly related and
jointly Gaussian variables. We use this result throughout our derivations in Section 4, for
instance when moving from (11c) to (11d). Using x and z for generic variables, the goal is
to obtain the posterior
p(x) =
∫
p(x|z)p(z)dz (A6)
where
p(z) =N(z|µz, Σz) (A7a)
p(x|z) =N(x|Az + b, Σx) (A7b)
We can view the problem of obtaining p(x) as ﬁrst applying a linear transform ( Az + b)
and then adding Gaussian noise with mean 0 and variance Σx. We will deal with each of
these steps in turn, starting with the linear transform. The posterior mean µ is given by
µ = E
[
Az + b
]
= AE[z] +b = Aµz + b (A8)
where E denotes the expectation operation. Here we ﬁrst factor out the terms that do no
depend on z out of the expectation and then identify E[z] =µz as z is Gaussian. To ﬁnd the
Entropy 2021, 23, 1565 20 of 23
covariance matrix Σ we can proceed by plugging the terms we know into the deﬁnition of
covariance
Σ = E
[
(x −µ)(x −µ)T
]
(A9a)
= E


(
Az + b  
x
−Aµz −b  
µ
)(
Az + b  
x
−Aµz −b  
µ
)T

 (A9b)
= E
[
(Az −Aµz)(Az −Aµz)T
]
(A9c)
Now we can factor A out of the expectation
= E
[
A(z −µz)(z −µz)T AT
]
(A10a)
= A E
[
(z −µz)(z −µz)T
]
  
Σz
AT (A10b)
= AΣz AT . (A10c)
In the last line we recognise the deﬁnition of the prior covariance matrix Σz. To obtain our
ﬁnal result we now need to add Gaussian noise with mean 0 and variance Σx. We know
that if two Gaussian variables are independent, the variance of their sum is the sum of their
variances. We can use this to write the ﬁnal covariance matrix as
Σ = AΣz AT + Σx (A11)
Which gives the result utilised in the main text as
p(x) =N(x|Aµz + b, AΣz AT + Σx) (A12)
Appendix A.3. Mutual Information Bound
In this section, we examine the result of substituting q for p in (32). We see that the
substitution turns (32) into a bound on the expected free energy that becomes exact when
we can do exact inference. We can show this by writing
∫∫
q(xk, zk|uk) log q(zk|uk)
q(zk|xk, uk)dxkdzk
=
∫∫
q(xk, zk|uk) log q(zk|uk)
q(zk|xk, uk)
p(zk|xk, uk)
p(zk|xk, uk)dxkdzk (A13a)
=
∫∫
q(xk, zk|uk) log q(zk|uk)
p(zk|xk, uk)
p(zk|xk, uk)
q(zk|xk, uk) dxkdzk (A13b)
=
∫∫
q(xk, zk|uk) log q(zk|uk)
p(zk|xk, uk)dxkdzk −
∫∫
q(xk, zk|uk) log q(zk|xk, uk)
p(zk|xk, uk)dxkdzk
  
Eq(xk|uk)[KL[q(zk|xk,uk)||p(zk|xk,uk)]]
(A13c)
≤
∫∫
q(xk, zk|uk) log q(zk|uk)
p(zk|xk, uk)dxkdzk, (A13d)
where we recognise the upper bound since the expected KL divergence is non-negative.
When the expected KL divergence goes to 0, which is the case when we do exact inference,
the bound becomes exact. That is the case for all models we consider in this paper since all
relations are linear and all distributions Gaussian.
Entropy 2021, 23, 1565 21 of 23
Appendix A.4. Mutual Information Derivation
In this section, we derive the expression for mutual information in detail. We restate
the deﬁnition given in (35) here as a starting point
I[x, z] =
∫∫
p(x, z) log p(x, z)
p(x)p(z)dxdz = H[z] −H[z|x] =H[x] −H[x|z] . (A14)
Note that we can write (35) in terms of entropies of either x or z since mutual information
is symmetric in its arguments.
Recall that (34c) optimises mutual information between expected observations xk
and latent states zk. This means that we need the marginal and conditional distributions
of xk in order to calculate the requisite entropies. The marginal is given by (17) and the
conditional by (18). Since both distributions are Gaussian, their entropies are available in
closed form as
H[xk] =1
2 (n log 2π + log |Σ22|+ n) (A15a)
H[xk|zk] =1
2
(
n log 2π + log |Σ22 −Σ21Σ−1
11 Σ12|+ n
)
, (A15b)
where n denotes the dimensionality. Before we proceed, we need the generalised matrix
determinant lemma which can be written as
|A + UVT|= |I + VT A−1U||A|, (A16)
where I denotes the identity matrix and A is invertible. By setting V = U = I we obtain a
form which will be convenient moving forwards
|A + UVT|= |A + I|= |I + IT A−1 I||A| (A17a)
= |I + A−1||A|, (A17b)
which also implies
|I −A−1|= |I −A|
−|A| . (A18)
Now we can write the mutual information as
I[xk, zk] =H[xk] −H[xk|zk] (A19a)
= 1
2 log |Σ22|− 1
2 log |Σ22 −Σ21Σ−1
11 Σ12| (A19b)
= −1
2 log |Σ22|−1|Σ22 −Σ21Σ−1
11 Σ12| (A19c)
= −1
2 log |I −Σ−1
22 Σ21Σ−1
11 Σ12| (A19d)
= −1
2 log |I −[AΣzk AT + Σx]−1
  
Σ−1
22
AΣzk
Σ21
Σ−1
zk
Σ−1
11
Σzk AT
  
Σ12
| (A19e)
= −1
2 log |I −[AΣzk AT + Σx]−1 AΣzk AT| (A19f)
= −1
2 log |I −[[AΣzk AT]−1 AΣzk AT
  
Evaluates to I
+[AΣzk AT]−1Σx]−1| (A19g)
= −1
2 log |I −[I + [AΣzk AT]−1Σx]−1||. (A19h)
Entropy 2021, 23, 1565 22 of 23
Using (A18) we can rewrite (A19h) as
−1
2 log |I −[I + [AΣzk AT]−1Σx]−1|= −1
2 log
(
|I −
[
I + (AΣzk AT)−1Σx
]
|
−|I + (AΣzk AT)−1Σx|
)
(A20a)
= −1
2 log
(
−|(AΣzk AT)−1Σx|
−|I + (AΣzk AT)−1Σx|
)
(A20b)
= −1
2 log
(
|(AΣzk AT)−1Σx|
|I + (AΣzk AT)−1Σx|
)
. (A20c)
Applying (A17a) in the denominator, we now rewrite (A20c) as
−1
2 log
(
|(AΣzk AT)−1Σx|
|I + (AΣzk AT)−1Σx|
)
= −1
2 log
(
|(AΣzk AT)−1Σx|
|I + [(AΣz AT)−1Σx]−1||(AΣzk AT)−1Σx|
)
(A21a)
= −1
2 log
(
((((((((|(AΣzk AT)−1Σx|
|I + [(AΣz AT)−1Σx]−1|((((((((|(AΣzk AT)−1Σx|
)
(A21b)
= −1
2 log
(
1
|I + [(AΣz AT)−1Σx]−1|
)
(A21c)
= 1
2 log
(⏐⏐⏐⏐I +
[
(AΣz AT)−1Σx
]−1⏐⏐⏐⏐
)
(A21d)
= 1
2 log |I + Σ−1
x AΣzk AT|, (A21e)
which gives the expression found in (36).
References
1. Sajid, N.; Costa, L.D.; Parr, T.; Friston, K. Active inference, Bayesian optimal design, and expected utility. arXiv 2021,
arXiv:2110.04074.
2. Friston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P .; Doherty, J.; Pezzulo, G. Active inference and learning.Neurosci. Biobehav.
Rev. 2016, 68, 862–879. [CrossRef] [PubMed]
3. Baltieri, M.; Buckley, C. PID Control as a Process of Active Inference with Linear Generative Models. Entropy 2019, 21, 257.
[CrossRef]
4. Friston, K.; Ao, P . Free Energy, Value, and Attractors. Comput. Math. Methods Med.2012, 2012, 937860. [CrossRef] [PubMed]
5. Buckley, C.L.; Kim, C.S.; McGregor, S.; Seth, A.K. The free energy principle for action and perception: A mathematical review.
J. Math. Psychol.2017, 81, 55–79. [CrossRef]
6. van de Laar, T.W.; de Vries, B. Simulating Active Inference Processes by Message Passing. Front. Robot. AI2019, 6. [CrossRef]
7. Baltieri, M.; Buckley, C.L. An active inference implementation of phototaxis. arXiv 2017, arXiv:1707.01806.
8. Friston, K.; Rigoli, F.; Ognibene, D.; Mathys, C.; Fitzgerald, T.; Pezzulo, G. Active inference and epistemic value. Cogn. Neurosci.
2015, 6, 187–214. [CrossRef]
9. Sajid, N.; Ball, P .J.; Friston, K.J. Active inference: Demystiﬁed and compared. arXiv 2020, arXiv:1909.10863.
10. Ghavamzadeh, M.; Mannor, S.; Pineau, J.; Tamar, A. Bayesian Reinforcement Learning: A Survey. arXiv 2016, arXiv:1609.04436.
doi:10.1561/2200000049.
11. Cullen, M.; Davey, B.; Friston, K.J.; Moran, R.J. Active Inference in OpenAI Gym: A Paradigm for Computational Investigations
Into Psychiatric Illness. Biol. Psychiatry Cogn. Neurosci. Neuroimaging2018, 3, 809–818. [CrossRef]
12. Parr, T.; Friston, K.J. Generalised free energy and active inference. Biol. Cybern.2019, 113, 495–513. [CrossRef] [PubMed]
13. Friston, K.; Da Costa, L.; Hafner, D.; Hesp, C.; Parr, T. Sophisticated Inference. arXiv 2020, arXiv:2006.04120.
14. Fountas, Z.; Sajid, N.; Mediano, P .A.M.; Friston, K. Deep active inference agents using Monte-Carlo methods. arXiv 2020,
arXiv:2006.04176.
15. Tschantz, A.; Seth, A.K.; Buckley, C.L. Learning action-oriented models through active inference. PLoS Comput. Biol.2020, 16,
e1007805. [CrossRef]
16. Tschantz, A.; Millidge, B.; Seth, A.K.; Buckley, C.L. Reinforcement Learning through Active Inference. arXiv 2020,
arXiv:2002.12636.
17. Millidge, B. Deep Active Inference as Variational Policy Gradients. arXiv 2019, arXiv:1907.03876.
18. Tschantz, A.; Baltieri, M.; Seth, A.K.; Buckley, C.L. Scaling active inference. arXiv 2019, arXiv:1911.10601.
19. Ueltzhöffer, K. Deep Active Inference. Biol. Cybern.2018, 112, 547–573. [CrossRef]
Entropy 2021, 23, 1565 23 of 23
20. Forney, G.D. Codes on graphs: Normal realizations. IEEE Trans. Inf. Theory2001, 47, 520–548. [CrossRef]
21. Loeliger, H.A.; Dauwels, J.; Hu, J.; Korl, S.; Ping, L.; Kschischang, F.R. The Factor Graph Approach to Model-Based Signal
Processing. Proc. IEEE2007, 95, 1295–1322. [CrossRef]
22. Loeliger, H.A. An introduction to factor graphs. Signal Process. Mag. IEEE2004, 21, 28–41. [CrossRef]
23. Sarkka, S. Bayesian Filtering and Smoothing; Cambridge University Press: Cambridge, UK, 2013. [CrossRef]
24. Schwartenbeck, P .; FitzGerald, T.; Dolan, R.J.; Friston, K. Exploration, novelty, surprise, and free energy minimization.Front.
Psychol. 2013, 4. [CrossRef] [PubMed]
25. Da Costa, L.; Parr, T.; Sajid, N.; Veselic, S.; Neacsu, V .; Friston, K. Active inference on discrete state-spaces: A synthesis.arXiv
2020, arXiv:2001.07203.
26. ¸ Senöz,˙I.; van de Laar, T.; Bagaev, D.; de Vries, B. Variational Message Passing and Local Constraint Manipulation in Factor
Graphs. Entropy 2021, 23, 807. [CrossRef]
27. Blei, D.M.; Kucukelbir, A.; McAuliffe, J.D. Variational Inference: A Review for Statisticians. J. Am. Stat. Assoc.2017, 112, 859–877.
[CrossRef]
28. Schwartenbeck, P .; Passecker, J.; Hauser, T.U.; FitzGerald, T.H.B.; Kronbichler, M.; Friston, K. Computational mechanisms of
curiosity and goal-directed exploration. Neuroscience 2018. [CrossRef]
29. Millidge, B.; Tschantz, A.; Buckley, C.L. Whence the Expected Free Energy? arXiv 2020, arXiv:2004.08128.
30. Hafner, D.; Ortega, P .A.; Ba, J.; Parr, T.; Friston, K.; Heess, N. Action and Perception as Divergence Minimization.arXiv 2020,
arXiv:2009.01791.
31. Buisson-Fenet, M.; Solowjow, F.; Trimpe, S. Actively learning gaussian process dynamics. In Proceedings of the 2nd Conference
on Learning for Dynamics and Control, Online, 11–12 June 2020; pp. 5–15.
32. Bai, S.; Wang, J.; Chen, F.; Englot, B. Information-theoretic exploration with Bayesian optimization. In Proceedings of the 2016
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea, 9–14 October 2016; pp. 1816–1822.
ISSN: 2153-0866. [CrossRef]
33. Berseth, G.; Geng, D.; Devin, C.; Finn, C.; Jayaraman, D.; Levine, S. SMiRL: Surprise Minimizing RL in Dynamic Environments.
arXiv 2019, arXiv:1912.05510.
34. Friston, K. A free energy principle for a particular physics. arXiv 2019, arXiv:1906.10184.
35. Solopchuk, O. Information Theoretic Approach to Decision Making in Continuous Domains. Ph.D. Thesis, UCL-Université
Catholique de Louvain, Ottigny, Belgium, 2021.
36. Kappen, H.J.; Gómez, V .; Opper, M. Optimal control as a graphical model inference problem. Mach. Learn.2012, 87, 159–182.
[CrossRef]
37. Schwoebel, S.; Kiebel, S.; Markovic, D. Active Inference, Belief Propagation, and the Bethe Approximation. Neural Comput.2018,
30, 2530–2567. [CrossRef] [PubMed]
38. Millidge, B.; Tschantz, A.; Seth, A.; Buckley, C. Understanding the Origin of Information-Seeking Exploration in Probabilistic
Objectives for Control. arXiv 2021, arXiv:2103.06859.