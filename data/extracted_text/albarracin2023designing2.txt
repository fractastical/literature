Designing explainable artificial intelligence with active
inference: A framework for transparent introspection
and decision-making
Mahault Albarracin1,2, InÃªs HipÃ³lito1,3,4, Safae Essafi Tremblay1,5,
Jason G. Fox1, Gabriel RenÃ©1, Karl Friston1,6, and Maxwell J. D. Ramstead1,6
1VERSES Research Lab, Los Angeles, CA 90016, USA
2DÃ©partement dâ€™informatique, UniversitÃ© du QuÃ©bec Ã  MontrÃ©al, 201, Avenue du
PrÃ©sident-Kennedy, MontrÃ©al, H2X 3Y7
3Berlin School of Mind & Brain, Humboldt-UniversitÃ¤t zu Berlin, Berlin, Germany
4Department of Philosophy, Macquarie University, Sydney, New South Wales, Australia
5DÃ©partement de philosophie, UniversitÃ© du QuÃ©bec Ã  MontrÃ©al, 455, Boulevard RenÃ©-LÃ©vesque
Est, MontrÃ©al, H2L 4Y2
6Wellcome Centre for Human Neuroimaging, University College London,
London WC1N 3AR, UK
June 8, 2023
Abstract
This paper investigates the prospect of developing human-interpretable, explainable
artificial intelligence (AI) systems based on active inference and the free energy prin-
ciple. We first provide a brief overview of active inference, and in particular, of how
it applies to the modeling of decision-making, introspection, as well as the generation
of overt and covert actions. We then discuss how active inference can be leveraged
to design explainable AI systems, namely, by allowing us to model core features of
â€œintrospectiveâ€ processes and by generating useful, human-interpretable models of the
processes involved in decision-making. We propose an architecture for explainable AI
systems using active inference. This architecture foregrounds the role of an explicit hi-
erarchical generative model, the operation of which enables the AI system to track and
explain the factors that contribute to its own decisions, and whose structure is designed
to be interpretable and auditable by human users. We outline how this architecture
can integrate diverse sources of information to make informed decisions in an auditable
manner, mimicking or reproducing aspects of human-like consciousness and introspec-
tion. Finally, we discuss the implications of our findings for future research in AI, and
the potential ethical considerations of developing AI systems with (the appearance of)
introspective capabilities.
1
arXiv:2306.04025v1  [cs.AI]  6 Jun 2023
Contents
1 Introduction: Explainable AI and active inference 3
2 Active inference and introspection 6
2.1 A brief introduction to active inference . . . . . . . . . . . . . . . . . . . . . 6
2.2 Active inference, introspection, and self-modeling . . . . . . . . . . . . . . . 7
3 Using active inference to design self-explaining AI 11
4 Discussion 13
4.1 Directions for future research . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.2 Ethical considerations of introspective AI systems . . . . . . . . . . . . . . . 14
5 Conclusion 14
Acknowledgements
The authors are grateful to VERSES for supporting the open access publication of this pa-
per. SET is supported in part by funding from the Social Sciences and Humanities Research
Council of Canada (Ref: 767-2020-2276). KF is supported by funding for the Wellcome Cen-
tre for Human Neuroimaging (Ref: 205103/Z/16/Z) and a Canada-UK Artificial Intelligence
Initiative (Ref: ES/T01279X/1). The authors are grateful to Brennan Klein for assistance
with typesetting.
Conflict of interest statement
The authors disclose that they are contributors to the Institute of Electrical and Electronics
Engineers (IEEE) P2874 Spatial Web Working Group.
2
1 Introduction: Explainable AI and active inference
Artificial intelligence (AI) systems continue to proliferate and, at the time of writing, have
become an integral part of various intellectual and industrial domains, including healthcare,
finance, and transportation [1, 2]. Traditional AI models, such as deep learning neural
networks, have been widely recognized for their ability to achieve high performance and
accuracy across various tasks [3, 4]. However, it is well known that these models almost
invariably function as â€œblack boxes,â€ with limited transparency and interpretability of their
decision-making processes [5, 6]. This lack of explainability can lead to skepticism and
reluctance to adopt AI systemsâ€”and indeed, to harm, particularly in high-stakes situations,
where the consequences of a wrong decision can be severe and harmful [7â€“10]. Indeed, a lack
of explainability precludes applications in certain domains, such as fintech.
The problem of explainable AI (sometimes referred to as the â€œblack boxâ€ problem) is
the problem of understanding and interpreting how these models arrive at their decisions or
predictions [11, 12]. While researchers and users may have knowledge of the inputs provided
to the model and the corresponding outputs that it produces, comprehending the internal
workings and decision-making processes of AI systems can be complex and challenging. This
is in no small part because their intricate architectures and numerous interconnected layers
learn to make predictions by analyzing vast amounts of training data and adjusting their
internal parameters, without explicit instruction from a programmer [13]. The method by
which these systems are trained thus, by design, limits their explainability. Moreover, the
internal computations that are performed by these modelsâ€”when they engage in decision-
makingâ€”can be highly complex and nonlinear, making it difficult to extract meaningful
explanations of their behavior, or insights into their decision-making process [14]. This
problem is compounded by the fact that most machine learning implementations of AI fail
to represent or quantify their uncertainty; especially, uncertainty about the parameters and
weights that underwrite their accurate performance. This means that AI, in general, cannot
evaluate (or report) the confidence in its decisions, choices or recommendations.
The lack of interpretability poses several challenges. Firstly, it hampers transparency and
makes audits by third parties next to impossible, as the designers, users, and stakeholders
of these systems may struggle to understand why a particular decision or prediction was
made. This becomes problematic in critical domains such as healthcare or finance, where
the ability to explain the reasoning behind a decision is essential for trust, accountability, and
compliance with regulations [15, 16]. Secondly, the black box nature of machine learning
models can hinder the identification and mitigation of biases or discriminatory patterns.
Without visibility into the underlying decision-making process, it becomes challenging to
detect and address biases that may exist within the modelâ€™s training data or architecture.
This opacity can lead to unfair or biased outcomes, perpetuating social inequalities or
discriminatory practices [17â€“19]. Additionally, the lack of interpretability of the model limits
its ability to provide meaningful explanations to end-users. Individuals interacting with
machine learning systems often seek explanations for the decisions made by these systems
[20, 21]. For instance, in medical diagnosis, patients and healthcare professionals may want
to understand why a particular diagnosis or treatment recommendation was given [22, 23]; or
3
consider automated suggestions in practical industrial settings [24]. Without explainability,
users may be hesitant to trust the systemâ€™s recommendations or may feel apprehensive (not
without good reason) about relying on the outputs of such models.
Accordingly, the need for explainable AI has become increasingly important [25]. â€œEx-
plainableAIâ€ referstothedevelopmentofAIsystemsthatcanprovidehuman-understandable
explanations for their decisions and actions [26]. This level of transparency is crucial for fos-
tering trust [27], ensuring accountability [28], and facilitating inclusive collaboration between
humans and AI systems [29â€“31]. Recent efforts to regulate AI may turn explainability into
a requirement for the deployment of any AI system at scale. For instance, in the United
States, the National Institute of Standards and Technology (NIST) released its Artificial
Intelligence Risk Management Framework (RMF) in 2023, which includes explainability and
interpretability as crucial characteristics of a trustworthy AI system. The RMF is envisioned
as a guide for tech companies to manage the risks of AI and could eventually be adopted as
an industry standard. In a similar vein, US Senator Chuck Schumer has led a congressional
effort to establish US regulations on AI, with one of the key aspects being the availability of
explanations for how AI arrives at its responses [32].
In the European Union, a proposed Regulation Laying Down Harmonized Rules on Arti-
ficial Intelligence (better known as the â€œAI Actâ€) is set to increase the transparency required
for the use of so-called â€œhigh-riskâ€ AI systems. For instance, groups that deploy automated
emotion recognition systems may be obligated to inform those on whom the system is being
deployed that they are being exposed to such a system. The AI Act is expected to be final-
ized and adopted in 2023, with its obligations likely to apply within three yearsâ€™ time. The
Council of Europe is also in the process of developing a draft convention on artificial intelli-
gence, human rights, democracy, and the rule of law, which will be the first legally binding
international instrument on AI. This convention seeks to ensure that research, development,
and deployment of AI systems are consistent with the values and interests of the EU, and
that they remain compatible with the AI Act and the proposed AI Liability Directive, which
includes a risk-based approach to AI. In addition, the US-EU Trade and Technology Coun-
cil published a joint Roadmap for Trustworthy AI and Risk Management in 2022, which
aims to advance collaborative approaches in international standards bodies related to AI,
among other objectives [33]. Therefore, explainability is clearly a major issue in research,
development, and deployment of AI systems, and will remain so for the foreseeable future.
Explainable AI aims to bridge the gap between the complexity and lack of auditability
of contemporary AI systems and the need for human interpretability and auditability [25,
26, 34]. It seeks to provide insights into the factors that influence AI decision-making,
enabling users to understand the explicit reasoning and other factors driving the output of
AI systems. Understanding the performance and potential biases of AI systems is crucial
for their ethical and responsible deployment [35, 36]. This understanding, however, must
extend beyond the performance of AI systems on academic benchmarks and tasks to include
a deep understanding of what the models represent or learn, as well as the algorithms that
they instantiate [37].
Transparency considerations are embedded in the design, development, and deployment
4
of AI systems, from the societal problems that arise worth developing a solution, to the data
collection stage, and still at the point where the AI system is deployed in the real world and
iteratively improved [30, 38]. This transparency may enable the implementation of other
ethical AI dimensions like interpretability, accountability, and safety [39].
Researchers have been exploring various approaches to develop more explainable AI sys-
tems [7, 40]. However, these efforts have yet to yield a principled and widely accepted path
method for, or path to, explainability. One promising direction is to draw inspiration from
research into human introspection and decision-making processes. Furthermore, a two-stage
decision-making process, which includes a reflection stage where the network reflects on its
feed-forward decision, can enhance the robustness and calibration of AI systems [41]. It has
been suggested that explainability in AI systems can be further enhanced through techniques
such as layer-wise relevance propagation [42] and saliency maps [43], which aid in visualizing
the modelâ€™s reasoning process. By translating the internal models of AI systems into human-
understandable explanations, we can foster trust and collaboration between AI systems and
their human users [44]. However, as [37] argue, we must also consider the metatheoreti-
cal calculus that underpins our understanding and use of these models. This involves not
only considering the performance of the model on a task, but also the implications of the
performance of the model for our understanding of the mind and brain.
In this paper, we investigate the potential of active inference, and the free energy principle
(FEP) upon which is based [45, 46], to enhance explainability in AI systems, notably by
capturing core aspects of introspective processes, hierarchical decision-making processes, and
(cover and overt) forms of action in human beings [47â€“49]. The FEP is a variational principle
of information physics that can be used to model the dynamics of self-organizing systems
like the brain. Active inference is an application of the FEP to model the perception-action
loops of cognitive systems: it provides us with the basis of a unified theory of the structure
and function of the brain (and indeed, of living and self-organizing systems more generally;
[50, 51]. Active inference allows us to model self-organizing systems like brains as being
driven by the imperative to minimize surprising encounters with the environment; where
this surprise scores how far a thing or system deviates from its characteristic states (e.g., a
fish out of water). By doing so, the brain continually updates and refines its world model,
allowing the agent to act adaptively and in situationally appropriate ways.
The relevance of using active inference is that the models of cognitive dynamicsâ€”and in
particular, introspectionâ€”that have been developed using its tools can be adapted to enable
the design of human interpretable and auditable (and indeed, self-auditable) AI systems.
The ethical and epistemological or epistemic gains that this enables are notable. The pro-
posed active inference based AI system architecture would enable artificial agents to access
and analyze their own internal states and decision-making processes, leading to a better
understanding of their decision-making processes, and the ability to report on themselves.
Proof of concept for this kind of â€œself reportâ€ is already at hand [52] and, in principle, is
supported in any application of active inference. At one level, committing to a generative
modelâ€”implicit in any active inference schemeâ€”dissolves the explainability problem. This
is because one has direct access to the beliefs and belief-updating of the agent in question.
5
Indeed, this is why active inference has been so useful in neuroscience to model and
explain behavioral and neuronal responses in terms of underlying belief states: e.g., [53â€“
57]. As demonstrated in [52] it is a relatively straightforward matter to augment generative
models to self-report their belief states. In this paper, we address a slightly more subtle
aspect of explainability that rests upon â€œself-accessâ€; namely, when an agent infers its own
â€œstates of mindâ€â€”states of mind that underwrite its sense-making and choices. Crucially,
this kind of meta-inference [58â€“61] may rest on exactly the representations of uncertainty
(a.k.a., precision) that are absent in conventional AI.
This paper is organized as follows. We first introduce essential aspects of active infer-
ence. We then discuss how active inference can be used to design explainable AI systems.
In particular, we propose that active inference can be used as the basis for a novel AI
architectureâ€”based on explicit generative modelsâ€”that both endows AI systems with a
greater degree of explainability and audibility from the perspective of users and stakehold-
ers, and allows AI systems to track and explain their own decision-making processes in a
manner understandable to users and stakeholders. Finally, we discuss the implications of
our findings for future research in auditable, human-interpretable AI, as well as the poten-
tial ethical considerations of developing AI systems with the appearance of introspective
capabilities.
2 Active inference and introspection
2.1 A brief introduction to active inference
Active inference offers a comprehensive framework for naturalizing, explaining, simulating,
and understanding the mechanisms that underwrite decision-making, perception, and action
[62, 63]. The free energy principle (FEP) is a variational principle of information physics [45].
It has gained considerable attention and traction since it was first introduced in the context
of computational neuroscience and biology [64, 65]. Active inference denotes a family of
models premised on the FEP, which are used to understand and predict the behavior of self-
organizing systems. The tools of active inference allow us to model self-organizing systems as
driven by the imperative to minimize surprise, which quantifies the degree to which a given
path or trajectory deviates from its inertial or characteristic pathâ€”or its upper bound,
variational free energy, which scores the difference between its predictions and the actual
sensory inputs it receives [50].
Active inference modeling work suggests that decision-making, perception, and action
involve the optimization of a world model that represents the causal structure of the system
generating outcomes of observations [45]. In particular, active inference models the way that
latent states or factors in the world cause sensory inputs, and how those factors cause each
other, thereby capturing the essential causal structure of the measured or sensed world [66].
Minimizing surprise or free energy on average and over time allows the brain to maintain a
consistent and coherent internal model of the worldâ€”one that maximizes predictive accuracy
while minimizing model complexityâ€”which, in turn, enables agents to adapt and survive in
6
their environments [64, 67]. (Strictly speaking, this is the other way around. In other words,
agents who â€œsurviveâ€ can always be read as minimizing variational free energy or maximizing
their marginal likelihood (a.k.a., model evidence). This is often called self-evidencing [68].)
Active inference has instrumental value in allowing us to model, and thereby hopefully
help to understand, core aspects of human consciousness (for a review, see [64]Friston, 2010).
Of particular interest to us here, it enables us to model the processes involved in introspective
self-access (see [48, 49]. Active inference modeling deploys the construct of generative models
to make sense of the dynamics of self-organizing systems. In this context, a generative model
is a joint probability density over the hidden or latent causes of observable outcomes; see
[45] for a discussion of how to interpret these models philosophically and [61] for a gentle
introduction to the technical implementation of these models.
We depict a simple generative model, apt for perceptual inference, in Figure 1, and a
more complex generative model, apt for the selection of actions (a.k.a. policy selection) in
Figure 2. These models specify the way in which observable outcomes are generated by
(typically non-observable) states or factors in the world.
The main advantage of using generative models over current state of the art black box
approaches is interpretability and auditability. Indeed, the factors that figure in the gen-
erative model are explicitly labeled, such that their contributions to the operations of the
model can be read directly off its structure. This lends the generative model a degree of
auditability that other approaches do not have.
2.2 Active inference, introspection, and self-modeling
Active inference modeling has been deployed in the context of the scientific study of intro-
spection, self-modeling, and self-access, which has led to the development of several leading
theories of consciousness (for a review, see [49, 69]). Introspection, which is defined as the
ability to access and evaluate oneâ€™s own mental states, thoughts, and experiences, plays a
pivotal role in self-awareness, learning, and decision-making and is a pillar of human con-
sciousness [70]. Self-modeling and self-access can be defined as interconnected processes
that contribute to the development of self-awareness and to the capacity for introspection.
Self-modeling involves the creation of internal representations of oneself, while self-access
refers to the ability to access and engage with these representations for self-improvement
and learning [71, 72]. These processes, in conjunction with introspection, form a complex
dynamic system that enriches our understanding of consciousness and the selfâ€”and indeed,
may arguably form the causal basis of our capacity to understand ourselves and others.
Introspective self-access has been modeled using active inference by deploying a hierar-
chically structured generative model [73]. The basic idea is that for a system to report or
evaluate its own inferences, it must be able to enact some form of self-access, where some
parts of the system can take the output of other parts as their own input, for further process-
ing. This has been discussed in computational neuroscience under the rubric of â€œopacityâ€ and
â€œtransparencyâ€ [61, 74â€“76]. The idea is that some cognitive processes are â€œtransparentâ€: like
a (clean, transparent) window, they enable us to access some other thing (say, a tree outside)
while not themselves being perceivable. Other cognitive processes are â€œopaqueâ€: they can
7
ğ‘ 
ğ‘œ
ğ´
DPrior over hidden states
Sensory outcome!ğ’=	ğ›¿(o)
Posterior over hidden states, given observation o)ğ’”=ğœ(lnğ‘«+ln!ğ‘¨Â·!ğ’)
Generative model of precision weighted perception
ğ›¾A
Prior over likelihood precision
Aij=AijÎ³AAkjÎ³Akâˆ‘Precision weighted likelihood mapping
P(Î³A)=Î“(1,Î²A)Expected precisionÎ³A=1Î²AÎ³A=1Î²APosterior precisionÎ²A=Î²Aâˆ’lnAâ‹…(oâˆ’As)
Figure 1: A basic generative model for precision-weighted perceptual inference.
This figure depicts an elementary generative model that is capable of performing precision-
weighted perceptual inference. States are depicted as circles and denoted in lowercase: ob-
servable states or outcomes are denotedo and latent states (which need to be inferred) are
denoted s. Parameters are depicted as squares and denoted as uppercase. The likelihood
mapping A relates outcomes to the states that cause them, whereasD harnesses our prior
beliefs about states, independent of how they are sampled. The precision termÎ³ controls
the precision or weighting assigned to elements of the likelihood, and implements attention
as precision-weighting. Figure from [61].
be assessed per se, as in introspective self-awareness (i.e., aware that you are looking at a
tree as opposed to seeing a tree). The idea, then, is that introspective processes make other
cognitive processes accessible to the system as such, rendering them opaque.
In the context of self-access, the transparency and opacity of introspective processes has
been modeled using a three-level generative model [61]. The model is depicted in Figure
3. This model provides a framework for understanding how we access and interpret our
internal states and experiences. The first level of the model (in blue), which implements
the selection of overt actions, can be seen as a transparent process. The second, hierar-
chically superordinate level (in orange), which implements attention and covert action [48,
76], represents more opaque processes, which make processes in the first layer accessible to
the system. This layer models mental actions and shifts in attention that we may not be
consciously aware of, or able to report. The second level takes as its input the inferences
8
ğ‘ !
ğ‘œ!
ğ´
D
Sensory outcomes!ğ’ğ‰=	ğ›¿(oğœ)
Posterior over hidden states, given observation oğœ
Generative model of action and perception
ğ›¾A
ğ‘ "
ğ‘œ"
ğ´ğ›¾A
ğ‘ #
ğ‘œ#
ğ´ğ›¾A
ğµ! ğµ"
ğœ‹
GC EPreference over outcomesExpected Free EnergyBaseline prior over policies
sÏ„+1=BÏ„sÏ„State transitions
Prior over policiesÏ€=Ïƒ(âˆ’Î•Ï€âˆ’GÏ€) Ï€=Ïƒ(âˆ’Î•Ï€âˆ’GÏ€âˆ’FÏ€)Posterior over policies
sÏ€Ï„=Ïƒ(lnBÏ€Ï„âˆ’1sÏ„âˆ’1past!"#$#+lnAâ‹…oÏ„present!"#$#+lnBÏ€Ï„sÏ„+1future!"#$#)Free Energy (Perceptual Evidence)
Î²A=Î²Aâˆ’lnAâ‹…(oÏ„sÏ„âˆ’AsÏ„)Ï„âˆ‘Expected precisionÎ³A=1Î²A Posterior precision
ğ‘­"=##[%ğ’”"#'(ln%ğ’”"#âˆ’lnğ‘¨'ğ‘œ#âˆ’0.5lnğ‘©#$%%ğ’”"#$%âˆ’0.5lnğ‘©#&%%ğ’”"#&%)]
Expected Free Energyğ‘®"=##[ğ’"#'lnğ’"#âˆ’ğ‘ªâˆ’ğ‘‘ğ‘–ğ‘ğ‘”(ğ‘¨'lnğ‘¨)'%ğ’”"#]phenotypic riskperceptual ambiguity+
Figure 2: A generative model for policy selection.This figure depicts a more sophis-
ticated generative model that is apt for planning and the selection of actions in the future.
The basic model depicted in Figure 1 has now been expanded to include beliefs about the
current course of action or policy (denotedÂ¯Ï€), as well asB, C, E, F and G parameters. This
kind of model generates a time series of states (s1, s2, etc.) and outcomes (o1, o2, etc.). The
state transition (B) parameter encodes the transition probabilities between states over time,
independently of the way they are sampled.B, C, E, F and G enter into the selection of
beliefs about courses of action, a.k.a. policies. TheC vector specifies preferred or expected
outcomes and enters into the calculation of variational (F) and expected (G) free energies.
The E vector specifies a prior preference for specific courses of action. Figure from [61].
(posterior state estimations) ongoing at the first level, as data for further inferenceâ€”about
the systemâ€™s inferences. Attentional processes are of this sort: they are about cognitive
processes and action, and they modulate the activity of the first level. The third, final level
(in green) implements the awareness of where oneâ€™s attention is deployed. In other words,
it both recognizes and instantiates a particular attentional set via bottom-up and top-down
messages between levels, respectively. On the whole, this three-level architecture models our
self-access and introspective abilities in terms of the processes regulating transparency and
opacity at a phenomenal level of description, or attentional selection at a psychological level.
Ramstead, Albarracin et al. (2023) [48] recently discussed how active inference enables
us to model both overt and covert action (also see [58, 59, 70, 73, 76]). Overt actionsâ€”
observable behaviors such as physical movements or verbal responsesâ€”are directly influenced
by the brainâ€™s hierarchical organization and can be modeled using active inference [77â€“79]. In
9
!!
(#)
. #!
(#)#%
(#)
!%
(#)
!#
(%) !%
(%)
!!
(#)
&#
(%)
'(%)
((%))(%) *(%)
!!
(#)
&%
(%)
!#
(#) 
##
(#)
!!
(%)
'(#)
!!
(%)
!!
(#)
!!
(#)
. #!
(#)#%
(#)
!%
(#)!#
(#) 
##
(#)
!!
(%)
'(#)
!!
(#)
. #!
(#)#%
(#)
!%
(#)!#
(#) 
##
(#)
!!
(%)
'(#)
!#
(!)
%(!)
&#
(!) !%
(!)
Deep generative model of meta-awareness and mental action
What am I paying 
attention to?
What am I trying to 
pay attention to?
What am I 
perceiving?
sÏ€(2)t
sÏ€(1)Ï„
What am I 
trying to do?
Ï€ (1)
Ï€ (2)
Hidden states
Hidden mental 
states
How aware am I of 
where my attention is?
Hidden meta-
awareness states
sÏ€(3)T
!!
(%)!%
(%)!#
(%) 
!!
(#)
'(%)
$(%) $(%) $(%)
$(!) $(!)
'(!)
((!))(!) *(!)
Am I trying to maintain 
awareness of my attentional 
state?
!(")
##
(%) #%
(%) #!
(%)
##
(!) #%
(!)
Figure 3: A hierarchical generative model capable of self-access.Here, the gen-
erative model depicted in Figure 2 (in blue) has been augmented with two superordinate
hierarchical layers. In this architecture, posterior state estimates at one level are passed
onto the next level as data for further inference. Note that this induces an architecture
where the system is able to make inferences about its own inferences. Figure from [61].
contrast, covert actions refer to internal mental processes, such as attention and imagination,
which involve the manipulation and processing of internal representations in the absence of
observable behaviors [80â€“89]â€”of the sort discussed as â€œmental actionâ€ [61, 70, 76, 90]. These
actions are essential for higher cognitive functions, which rely on the brainâ€™s capacity to
explore and manipulate abstract concepts and relationships.
In Smith et al. (2019) [91], a hierarchical architecture of this type was deployed that
was augmented with the capacity to report on its emotional states. Thus, it is possible to
use active inference to design systems that can not only access their own states and perform
inferencesontheirbasis, butalsotoreportontheirintrospectiveprocessesinamannerthatis
readilyunderstandablebyhumanusersandstakeholders. Withthisformulationofhowactive
inference enables agents to model their overt and covert action, in the following sections, we
argue that we can and ought to research, design, and develop AI systems that mimic these
introspective processes, ultimately leading to more human-like artificial intelligence.
10
3 Using active inference to design self-explaining AI
We argue that incorporating the design principles of active inference into AI systems can
lead to better explainability. This is for two key reasons. The first is that, by deploying an
explicit generative model, AI systems premised on active inference are designed explicitly
such that their operations can be interpreted and audited by a user or stakeholder that
is fluent in the operation of such models. We believe that the inherent explainability of
active inference AI might be scaled up, by deploying the kind of explicit, standardized world
modelling techniques that are being developed as open standards within the Institute of
Electrical and Electronics Engineers (IEEE) P2874 Spatial Web Working Group [92], to
formalize contextual relationships between entities and processes and to create digital twins
of environments that are able to update in real time.
The second is that, by implementing an architecture inspired by active inference models
of introspection, we can build systems that are able to accessâ€”and report onâ€”the reasons
for their decisions, and their state of mind when reaching these decisions.
AI systems designed using active inference can incorporate the kind of hierarchical self-
access described by [61] and by [91], to enhance their introspection during decision-making.
As discussed, in the active inference tradition, introspection can be understood in the context
of the (covert and overt) actions that AI systems perform. Covert actions, which are internal
computations and decision-making processes that are not directly observable to users and
stakeholders, can be recorded or explained to make the system more explainable. Overt
actions, which are actions that an AI system takes based on its internal computations, such
as making a recommendation or decision, can be explained to help users understand why the
AI system acted as it did. This kind of deep inference promotes introspection, adaptability,
and responses to environmental changes [93, 94].
The proposed AI architecture includes components that continuously update and main-
tain an internal model of its own states, beliefs, and goals. This capacity for self-access (and
implicitly self-report) enables the AI system to optimize (and report on) its decision-making
processes, fostering introspection (and enhanced explainability). It incorporates metacogni-
tive processing capabilities, which involve the ability to monitor, control, and evaluate its
own cognitive processes. The AI system can thereby better explain the factors that con-
tribute to its decisions, as well as identify potential biases or errors, ultimately leading to
improved decision-making and explainability.
The proposed AI architecture would include introspection and a self-report interface,
which translates the AI systemâ€™s internal models and decision-making processes into human-
understandable (natural) language (using, e.g., large language models). In effect, the agent
would be talking to itself, describing its current state of mind and beliefs. This interface
bridges the gap between the AI systemâ€™s internal workings and human users, promoting
epistemic trust and collaboration. In this way, the system can effectively mimic human-
like consciousness and transparent introspection, leading to a deeper understanding of its
decision-making processes and explainability. This advancement may be essential in fostering
trust and collaboration between AI systems and their human users, paving the way for more
effective and responsible AI applications.
11
Augmenting a generative model with black box systemsâ€”like large language modelsâ€”
may be a useful strategy to help AI systems articulate their â€œunderstandingâ€ of the world.
Using large language models to furnish an introspective interface may be relatively straight-
forward, leveraging their powerful natural language processing capabilities to create expla-
nations of belief updating. This architectureâ€”with a hierarchical generative model at its
coreâ€”may contribute to the overall performance and explainability of hybrid AI systems.
Attention mechanisms also achieve this purpose by enhancing the explainability of the AI
systemâ€™s decision making, emphasizing important factors in the hierarchical generative model
that contribute to its decisions and actions.
These ideas are not new. Attentional mechanisms, particularly those at the word-level,
have been identified as crucial components in AI architecture, specifically in the context of
hierarchical generative modelsâ€”and in generative AI, in the form of transformers. They
function by focusing on relevant aspects during decision-making processes, thereby allowing
the system to effectively process and prioritize information [95]. In fact, the performance of
hierarchical models, which are a type of AI architecture, can be significantly improved by
integrating word-level attention mechanisms. These mechanisms are powerful because they
can leverage context information more effectively, especially fine-grained information.
The AI architecture that we propose employs a soft attention mechanism, which uses
a weighted combination of hierarchical generative model components to focus on relevant
information. The attention weights are dynamically computed based on the input data
and the AI systemâ€™s internal state, allowing the system to adaptively focus on different
aspects of the hierarchical generative model [96]. This approach is similar to the use of
deep learning models for global coordinate transformations that linearize partial differential
equations, where the model is trained to learn a transformation from the physical domain
to a computational domain where the governing partial differential equations are simpler, or
even linear [97].
TheAIarchitecturethatwedescribehereeffectivelyintegratesdiverseinformationsources
for decision-making, mirroring the complex information processing capabilities observed in
the human brain. The hierarchical structure of the generative model facilitates the exchange
of information between different levels of abstraction. This exchange allows the AI system
to refine and update its internal models based on both high-level abstract knowledge and
low-level detailed information.
In conclusion, the integration of introspective processes in AI systems may represent a
significant step towards achieving more explainable AI. By leveraging explicit generative
models, as well as attention and introspection mechanisms, we can design AI systems that
are not only more efficient and robust, but also more understandable and trustworthy. This
approach allows us to bridge the gap between the complex internal computations of AI
systems and the human users who interact with them. Ultimately, the goal is to create
AI systems that can effectively communicate the reasons that drive their decision-making
processes, adapt to environmental changes, and collaborate seamlessly with human users. As
we continue to advance in this field, the importance of introspection in AI will only become
more apparent, paving the way for more sophisticated and ethically sound AI systems.
12
4 Discussion
4.1 Directions for future research
The problem of explainable AI is the problem of understanding how AI models arrive at their
decisions or predictions. This problem is especially relevant to avoid biases and harm in the
design, implementation, and use of AI systems. By incorporating explicit generative models
and introspective processing into the proposed AI architecture, we can create a system that is
or seems capable of introspection and, thereby, that displays greatly enhanced explainability
and auditability. This approach to AI design paves the way for more effective AI deployment
across various real-world applications, by shedding light upon the problem of explainability,
thereby offering opportunities for fostering trust, fairness, and inclusivity.
The development of the AI architecture based on active inference opens several potential
avenues for future research. One possible direction is to further investigate the role of
attention and introspection mechanisms in both AI systems and human cognition, as well
as the development of more efficient attentional models to improve the AI systemâ€™s ability
to focus on salient information during decision-making. The approach that we propose
bridges the gap between AI and cognitive neuroscience by incorporating biologically-inspired
mechanisms into the design of AI systems. As a result, the proposed architecture promotes
a deeper understanding of the nature of cognition and its potential applications in artificial
intelligence, thus paving the way for more human-like AI systems capable of introspection
and enhanced collaboration with human users.
Future work could explore more advanced data fusion techniques, such as deep learning-
based fusion or probabilistic fusion, to improve the AI systemâ€™s ability to combine and
process multimodal data effectively. Evaluating the effectiveness of these techniques in di-
verse application domains will also be a valuable avenue for research [98, 99]. Furthermore,
the explanation dimension of these AI systems has been a significant topic in recent years,
particularly in decision-making scenarios. These systems provide more awareness of how AI
works and its outcomes, building a relationship with the system and fostering trust between
AI and humans [100].
In addition to the aforementioned avenues for future research, another promising direc-
tion lies in the realm of computational phenomenology (for a review and discussion, see
[101]. Beckmann, KÃ¶stner, & HipÃ³lito (2023) [102] have proposed a framework that deploys
phenomenologyâ€”the rigorous descriptive study of first-person experienceâ€”for the purposes
of machine learning training. This approach conceptualizes the mechanisms of artificial neu-
ral networks in terms of their capacity to capture the statistical structure of some kinds of
lived experience, offering a unique perspective on deep learning, consciousness, and their
relation. By grounding AI training in socioculturally situated experience, we can create
systems that are more aware of sociocultural biases and capable of mitigating their impact.
Ramstead et al. (2022) [101] propose a similar methodology based on explicit generative
models as they figure in the active inference tradition. This connection to first-person expe-
rience, of course, does not guarantee unbiased AI. But by moving away from traditional black
box AI systems, we shift towards human-interpretable models that enable the identification
13
and correction of biases in the AI system. This approach aligns with our goal of creating
AI systems that are not only efficient and effective, but also ethically sound and socially
responsible.
The incorporation of computational phenomenology into our proposed AI architecture
could further enhance its introspective capabilities and its ability to understand and navigate
the complexities of human sociocultural contexts. This could lead to AI systems that are
moreadaptable, moretrustworthy, andmorecapableofmeaningfulcollaborationwithhuman
users. As we continue to explore and integrate such innovative approaches, we move closer
to our goal of creating AI systems that truly mirror the richness and complexity of human
cognition and consciousness.
4.2 Ethical considerations of introspective AI systems
Ethical AI starts with the development of AI systems that are ethically designed; AI sys-
tems must be designed in such a way as to be transparent, auditable, and explainable, and
to minimize harm. But as these systems become increasingly integrated into our daily lives,
research on the ethical implications of introspective AI systems, as well as the development
of regulatory frameworks and guidelines for responsible AI use, become crucial. The develop-
ment of introspective AI systems raises several ethical considerations. Even if these systems
provide more human-like decision-making capabilities and enhanced explainability, it is and
will remain crucial to ensure that their decisions are transparent, fair, and unbiased, and
that their designers and users can be held accountable for harm that their use may cause.
To address these concerns, future research should focus on developing methods to audit
and evaluate the AI systemâ€™s decision-making processes, as well as identify and mitigate
potential biases within the system. Additionally, the development of ethical guidelines and
regulatory frameworks for the use of introspective AI systems will be essential to ensure that
they are deployed responsibly and transparently. Moreover, as introspective AI systems be-
come more prevalent, issues related to agency, privacy, and data security may arise. Ensuring
that these systems protect sensitive information by abiding by data protection regulations,
thereby safeguarding agency, will be of paramount importance.
In conclusion, the development of AI systems based on active inference has broad impli-
cations for both the fields of AI and consciousness studies. As future research explores the
potential of this novel approach, ethical considerations and responsible use of introspective
AI systems must remain at the forefront of these advancements, ultimately leading to more
transparent, effective, and user-friendly AI applications.
5 Conclusion
We have argued that active inference has demonstrated significant potential in advancing
the field of explainable AI. By incorporating design principles from active inference, the AI
system can better tackle complex real-world problems with improved auditability of decision-
making, thereby increasing safety and user trust.
14
Throughout our discussions and analysis, we have highlighted the importance of active
inference models as a foundation for designing more human-like AI systems, seemingly capa-
ble of introspection and finessed (epistemic) collaboration with human users. This novel ap-
proach bridges the gap between AI and cognitive neuroscience by incorporating biologically-
inspired mechanisms into the design of AI systems, thus promoting a deeper understanding
of the nature of consciousness and its potential applications in artificial intelligence.
As we move forward in the development of AI systems, the importance of advancing
explainable AI becomes increasingly apparent. By designing AI systems that can not only
make accurate and efficient decisions, but also provide understandable explanations for their
decisions, we foster (epistemic) trust and collaboration between AI systems and human
users. This advancement ultimately leads to more transparent, effective, and user-friendly
AI applications that can be tailored to a wide range of real-world scenarios.
References
[1] Wullianallur Raghupathi and Viju Raghupathi. â€œBig data analytics in healthcare:
Promise and potentialâ€. In:Health Information Science and Systems2 (2014), pp. 1â€“
10. doi: 10.1186/2047-2501-2-3.
[2] Miguel Mascarenhas, JoÃ£o Afonso, Tiago Ribeiro, PatrÃ­cia Andrade, HÃ©lder Cardoso,
andGuilhermeMacedo.â€œThePromiseofArtificialIntelligenceinDigestiveHealthcare
and the Bioethics Challenges It Presentsâ€. In: Medicina 59.4 (2023), p. 790. doi:
10.3390/medicina59040790.
[3] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT press,
2016.
[4] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. â€œDeep learningâ€. In: Nature
521.7553 (2015), pp. 436â€“444.doi: 10.1038/nature14539.
[5] Davide Castelvecchi. â€œCan we open the black box of AI?â€ In:Nature News538.7623
(2016), p. 20.doi: 10.1038/538020a.
[6] David Gunning. â€œExplainable Artificial Intelligence (XAI)â€. In:Defense Science Re-
search Projects Agency2.2 (2017), p. 1.doi: 10.1609/aimag.v40i2.2850.
[7] Finale Doshi-Velez and Been Kim. â€œTowards a rigorous science of interpretable ma-
chine learningâ€. In:arXiv (2017). doi: 10.48550/arXiv.1702.08608.
[8] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. â€œâ€œWhy should I trust
you?â€ Explaining the predictions of any classifierâ€. In:Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016,
pp. 1135â€“1144.doi: 10.1145/2939672.2939778.
[9] Abeba Birhane. â€œThe impossibility of automating ambiguityâ€. In:Artificial Life27.1
(2021), pp. 44â€“61.doi: 10.1162/artl_a_00336.
15
[10] Abeba Birhane, William Samuel Isaac, Vinodkumar Prabhakaran, Mark DÃ­az,
Madeleine Clare Elish, Iason Gabriel, and Shakir Mohamed. â€œFrameworks and Chal-
lenges to Participatory AIâ€. In:Proceeding of the Second Conference on Equity and
Access in Algorithms, Mechanisms, and Optimization (EAAMO â€™22). 2022.doi: 10.
48550/arXiv.2209.07572.
[11] Jean-Christophe BÃ©lisle-Pipon, Erica Monteferrante, Marie-Christine Roy, and Vin-
cent Couture. â€œArtificial intelligence ethics has a black box problemâ€. In:AI & SO-
CIETY (2022), pp. 1â€“16.doi: 10.1007/s00146-021-01380-0.
[12] KevinBauer,MoritzvonZahn,andOliverHinz.â€œExpl(AI)ned:Theimpactofexplain-
able artificial intelligence on cognitive processesâ€. In:Information Systems Research
(2021). doi: 10.1287/isre.2023.1199.
[13] Sajid Ali, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, Jose M Alonso-
Moral, Roberto Confalonieri, Riccardo Guidotti, Javier Del Ser, Natalia DÃ­az-
RodrÃ­guez, and Francisco Herrera. â€œExplainable Artificial Intelligence (XAI): What
we know and what is left to attain Trustworthy Artificial Intelligenceâ€. In:Information
Fusion (2023), p. 101805.doi: 10.1016/j.inffus.2023.101805.
[14] Jacques A Esterhuizen, Bryan R Goldsmith, and Suljo Linic. â€œInterpretable machine
learning for knowledge generation in heterogeneous catalysisâ€. In:Nature Catalysis
5.3 (2022), pp. 175â€“184.doi: 10.1038/s41929-022-00744-z.
[15] Abhishek Mishra. â€œTransparent AI: Reliabilist and proudâ€. In: Journal of Medical
Ethics 47.5 (2021), pp. 341â€“342.doi: 10.1136/medethics-2021-107352.
[16] Warren J von Eschenbach. â€œTransparency and the black box problem: Why we do not
trust AIâ€. In:Philosophy & Technology34.4 (2021), pp. 1607â€“1622.doi: 10.1007/
s13347-021-00477-0.
[17] Michael Veale and Reuben Binns. â€œFairer machine learning in the real world: Miti-
gating discrimination without collecting sensitive dataâ€. In:Big Data & Society4.2
(2017). doi: 10.1177/2053951717743530.
[18] Benjamin van Giffen, Dennis Herhausen, and Tobias Fahse. â€œOvercoming the pitfalls
and perils of algorithms: A classification of machine learning biases and mitigation
methodsâ€. In: Journal of Business Research144 (2022), pp. 93â€“106.doi: 10.1016/
j.jbusres.2022.01.076.
[19] Nathalia Nascimento, Paulo Alencar, and Donald Cowan. â€œComparing Software De-
velopers with ChatGPT: An Empirical Investigationâ€. In:arXiv (2023). doi: 10 .
48550/arXiv.2305.11837.
[20] Gregor Stiglic, Primoz Kocbek, Nino Fijacko, Marinka Zitnik, Katrien Verbert, and
Leona Cilar. â€œInterpretability of machine learning-based prediction models in health-
careâ€. In: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery
10.5 (2020), e1379.doi: 10.1002/widm.1379.
16
[21] Samuli Laato, Miika Tiainen, AKM Najmul Islam, and Matti MÃ¤ntymÃ¤ki. â€œHow to
explain AI systems to end users: A systematic literature review and research agendaâ€.
In: Internet Research32.7 (2022), pp. 1â€“31.doi: 10.1108/INTR-08-2021-0600.
[22] Emanuele Neri, Gayane Aghakhanyan, Marta Zerunian, Nicoletta Gandolfo, Roberto
Grassi, Vittorio Miele, Andrea Giovagnoni, Andrea Laghi, and SIRM expert group
on Artificial Intelligence. â€œExplainable AI in radiology: A white paper of the Italian
Society of Medical and Interventional Radiologyâ€. In:La Radiologia Medica(2023),
pp. 1â€“10.doi: 10.1007/s11547-023-01634-5.
[23] Luis Oberste, Florian RÃ¼ffer, Okan AydingÃ¼l, Johann Rink, and Armin Heinzl. â€œDe-
signingUser-CentricExplanationsforMedicalImagingwithInformedMachineLearn-
ingâ€. In:Design Science Research for a New Society: Society 5.0: 18th International
Conference on Design Science Research in Information Systems and Technology,
DESRIST 2023, Pretoria, South Africa, May 31â€“June 2, 2023, Proceedings. 2023,
pp. 470â€“484.doi: 10.1007/978-3-031-32808-4_29 .
[24] Thi-Thu-Huong Le, Aji Teguh Prihatno, Yustus Eko Oktian, Hyoeun Kang, and
Howon Kim. â€œExploring Local Explanation of Practical Industrial AI Applications:
A Systematic Literature Reviewâ€. In: Applied Sciences 13.9 (2023), p. 5809. doi:
10.3390/app13095809.
[25] Amina Adadi and Mohammed Berrada. â€œPeeking inside the black-box: A survey on
explainable artificial intelligence (XAI)â€. In:IEEE Access6 (2018), pp. 52138â€“52160.
doi: 10.1109/ACCESS.2018.2870052.
[26] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Gian-
notti, and Dino Pedreschi. â€œA survey of methods for explaining black box modelsâ€.
In: ACM Computing Surveys51.5 (2018), pp. 1â€“42.doi: 10.1145/3236009.
[27] Jenna Burrell. â€œHow the machine â€˜thinksâ€™: Understanding opacity in machine learning
algorithmsâ€. In:Big Data & Society3.1 (2016), p. 2053951715622512.doi: 10.1177/
2053951715622512.
[28] Beatriz San Miguel, Aisha Naseer, and Hiroya Inakoshi. â€œPutting accountability of AI
systems into practiceâ€. In:Proceedings of the Twenty-Ninth International Conference
on International Joint Conferences on Artificial Intelligence. 2021, pp. 5276â€“5278.
doi: 10.24963/ijcai.2020/768.
[29] Nadin Kokciyan, Biplav Srivastava, Michael N Huhns, and Munindar P Singh. â€œSo-
ciotechnical perspectives on AI ethics and accountabilityâ€. In:IEEE Internet Com-
puting 25.6 (2021), pp. 5â€“6.doi: 10.1109/MIC.2021.3117611.
[30] InÃªs HipÃ³lito, Katie Winkle, and Merete Lie. â€œEnactive Artificial Intelligence: Sub-
verting Gender Norms in Robot-Human Interactionâ€. In:Frontiers in Neurorobotics
17 (2023), p. 77.doi: 10.48550/arXiv.2301.08741.
17
[31] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S Brown, Johnathan Flow-
ers, Anthony Ventresque, and Christopher L Dancy. â€œThe forgotten margins of AI
ethicsâ€. In: 2022 ACM Conference on Fairness, Accountability, and Transparency.
2022, pp. 948â€“958.doi: 10.1145/3531146.3533157.
[32] Marianna Drake, Jiayen Ong, Marty Hansen, and Lisa Peets. â€œEU AI Policy and
Regulation: What to look out for in 2023â€. In:Inside Privacy (2023). url: https:
/ / www . insideprivacy . com / artificial - intelligence / eu - ai - policy - and -
regulation-what-to-look-out-for-in-2023/ .
[33] Caleb Skeath, Lindsey Tonsager, and Jenna Zhang. â€œFTC Announces COPPA Settle-
ment Against Ed Tech Provider Including Strict Data Minimization and Data Reten-
tion Requirementsâ€. In:Inside Privacy (2023). url: https://www.insideprivacy.
com / childrens - privacy / ftc - announces - coppa - settlement - against - ed -
tech-provider-including-strict-data-minimization-and-data-retention-
requirements.
[34] Andrea Brennen. â€œWhat Do People Really Want When They Say They Want â€œEx-
plainable AI?â€ We Asked 60 Stakeholdersâ€. In:Extended Abstracts of the 2020 CHI
Conference on Human Factors in Computing Systems. 2020, pp. 1â€“7.doi: 10.1145/
3334480.3383047.
[35] Emanuele Ratti and Mark Graves. â€œExplainable machine learning practices: Opening
another black box for reliable medical AIâ€. In:AI and Ethics2.4 (2022), pp. 801â€“814.
doi: 10.1007/s43681-022-00141-z.
[36] Michael Ridley. â€œExplainable Artificial Intelligence (XAI)â€. In:Information Technol-
ogy and Libraries41.2 (2022).doi: 10.6017/ital.v41i2.14683.
[37] Olivia Guest and Andrea E Martin. â€œOn logical inference over brains, behaviour, and
artificial neural networksâ€. In: Computational Brain & Behavior (2023), pp. 1â€“15.
doi: 10.1007/s42113-022-00166-x.
[38] InÃªs HipÃ³lito. â€œThe Human Roots of Artificial Intelligenceâ€. In: (2023). doi: 10 .
31234/osf.io/cseqt.
[39] Muhammad Ali Chaudhry, Mutlu Cukurova, and Rose Luckin. â€œA transparency index
framework for AI in educationâ€. In:Artificial Intelligence in Education. Posters and
Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks,
Practitionersâ€™ and Doctoral Consortium: 23rd International Conference, AIED 2022,
Durham, UK, July 27â€“31, 2022, Proceedings, Part II. 2022, pp. 195â€“198.doi: 10.
1007/978-3-031-11647-6_33 .
[40] Alejandro Barredo Arrieta, Natalia DÃ­az-RodrÃ­guez, Javier Del Ser, Adrien Bennetot,
Siham Tabik, Alberto Barbado, Salvador GarcÃ­a, Sergio Gil-LÃ³pez, Daniel Molina,
Richard Benjamins, Raja Chatila, and Francisco Herrera. â€œExplainable Artificial In-
telligence (XAI): Concepts, taxonomies, opportunities and challenges toward respon-
sible AIâ€. In:Information Fusion 58 (2020), pp. 82â€“115.doi: 10.1016/j.inffus.
2019.12.012.
18
[41] Mohit Prabhushankar and Ghassan AlRegib. â€œIntrospective learning: A two-stage
approach for inference in neural networksâ€. In: arXiv (2022). url: https : / /
openreview.net/forum?id=in1ynkrXyMH.
[42] Sebastian Bach, Alexander Binder, GrÃ©goire Montavon, Frederick Klauschen, Klaus-
Robert MÃ¼ller, and Wojciech Samek. â€œOn pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagationâ€. In:PloS One 10.7 (2015),
e0130140. doi: 10.1371/journal.pone.0130140.
[43] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. â€œInterpretable convolutional
neural networksâ€. In:Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 2018, pp. 8827â€“8836.doi: 10.1109/CVPR.2018.00920.
[44] William Franz Lamberti. â€œAn overview of explainable and interpretable AIâ€. In:AI
Assurance (2023), pp. 55â€“123.doi: 10.1016/B978-0-32-391919-7.00015-9 .
[45] Maxwell J D Ramstead, Dalton A R Sakthivadivel, Conor Heins, Magnus Koudahl,
Beren Millidge, Lancelot Da Costa, Brennan Klein, and Karl J Friston. â€œOn Bayesian
mechanics: A physics of and by beliefsâ€. In:Interface Focus13 (2023), p. 20220029.
doi: 10.1098/rsfs.2022.0029.
[46] Karl J Friston, Maxwell J D Ramstead, Alex B Kiefer, Alexander Tschantz, Christo-
pher L Buckley, Mahault Albarracin, Riddhi J Pitliya, Conor Heins, Brennan Klein,
Beren Millidge, Dalton A R Sakthivadivel, Toby St Clere Smithe, Magnus Koudahl,
Safae Essafi Tremblay, Capm Petersen, Kaiser Fung, Jason G Fox, Steven Swanson,
Dan Mapes, and Gabriel RenÃ©. â€œDesigning Ecosystems of Intelligence from First Prin-
ciplesâ€. In:arXiv (2022). doi: 10.48550/arXiv.2212.01354.
[47] Jakob Hohwy. The Predictive Mind. Oxford University Press, 2013.doi: 10.1093/
acprof:oso/9780199682737.001.0001.
[48] Maxwell J D Ramstead, Mahault Albarracin, Alex Kiefer, Brennan Klein, Chris
Fields, Karl J Friston, and Adam Safron. â€œThe inner screen model of consciousness:
Applying the free energy principle directly to the study of conscious experienceâ€. In:
PsyArXiv (2023). doi: 10.31234/osf.io/6afs3.
[49] Maxwell J D Ramstead, Mahault Albarracin, Alex Kiefer, Brennan Klein, Ken Willi-
ford, Adam Safron, Chris Fields, Mark Solms, and Karl J Friston. â€œSteps towards a
minimal unifying model of consciousness: An integration of models of consciousness
based on the free energy principleâ€. In: (2023).
[50] Maxwell J D Ramstead, Paul Benjamin Badcock, and Karl John Friston. â€œAnswering
SchrÃ¶dingerâ€™s question: A free-energy formulationâ€. In:Physics of Life Reviews 24
(2018), pp. 1â€“16.doi: 10.1016/j.plrev.2017.09.001.
[51] Maxwell J D Ramstead, Axel Constant, Paul B Badcock, and Karl J Friston. â€œVari-
ational ecology and the physics of sentient systemsâ€. In:Physics of Life Reviews31
(2019), pp. 188â€“205.doi: 10.1016/j.plrev.2018.12.002.
19
[52] Thomas Parr and Giovanni Pezzulo. â€œUnderstanding, explanation, and active infer-
enceâ€. In: Frontiers in Systems Neuroscience15 (2021), p. 772641.doi: 10.3389/
fnsys.2021.772641.
[53] Ryan Smith, Sahib S Khalsa, and Martin P Paulus. â€œAn active inference approach
to dissecting reasons for nonadherence to antidepressantsâ€. In:Biological Psychiatry:
Cognitive Neuroscience and Neuroimaging6.9 (2021), pp. 919â€“934.doi: 10.1016/j.
bpsc.2019.11.012.
[54] Ryan Smith, Samuel Taylor, and Edda Bilek. â€œComputational mechanisms of addic-
tion: Recent evidence and its relevance to addiction medicineâ€. In:Current Addiction
Reports (2021), pp. 1â€“11.doi: 10.1007/s40429-021-00399-z.
[55] Rick A Adams, Stewart Shipp, and Karl J Friston. â€œPredictions not commands: Ac-
tive inference in the motor systemâ€. In:Brain Structure and Function218.3 (2013),
pp. 611â€“643.doi: 10.1007/s00429-012-0475-5.
[56] Rick A Adams, Peter Vincent, David Benrimoh, Karl J Friston, and Thomas Parr.
â€œEverything is connected: Inference and attractors in delusionsâ€. In:Schizophrenia
Research 245 (2022), pp. 5â€“22.doi: 10.1016/j.schres.2021.07.032.
[57] Philipp Sterzer, Rick A Adams, Paul Fletcher, Chris Frith, Stephen M Lawrie, Lars
Muckli, Predrag Petrovic, Peter Uhlhaas, Martin Voss, and Philip R Corlett. â€œThe
predictive coding account of psychosisâ€. In:Biological Psychiatry84.9 (2018), pp. 634â€“
643. doi: 10.1016/j.biopsych.2018.05.015.
[58] Stephen M Fleming. â€œAwareness as inference in a higher-order state spaceâ€. In:Neu-
roscience of Consciousness2020.1 (2020), niz020.doi: 10.1093/nc/niz020.
[59] Daniel Yon and Chris D Frith. â€œPrecision and the Bayesian brainâ€. In:Current Biology
31.17 (2021), R1026â€“R1032.doi: 10.1016/j.cub.2021.07.044.
[60] Chris D Frith. â€œConsciousness, (meta) cognition, and cultureâ€. In: Quarterly Jour-
nal of Experimental Psychology (2023), p. 17470218231164502. doi: 10 . 1177 /
17470218231164502.
[61] Lars Sandved-Smith, Casper Hesp, JÃ©rÃ©mie Mattout, Karl J Friston, Antoine Lutz,
and Maxwell J D Ramstead. â€œTowards a computational phenomenology of mental
action: Modelling meta-awareness and attentional control with deep parametric ac-
tive inferenceâ€. In:Neuroscience of Consciousness2021.1 (2021).doi: 10.1093/nc/
niab018.
[62] Axel Constant, Maxwell J D Ramstead, Samuel P L VeissiÃ¨re, and Karl J Friston.
â€œRegimes of expectations: An active inference model of social conformity and human
decision makingâ€. In:Frontiers in Psychology10 (2019), p. 679.doi: 10.3389/fpsyg.
2019.00679.
[63] Lancelot Da Costa, Karl J Friston, Conor Heins, and Grigorios A Pavliotis. â€œBayesian
mechanics for stationary processesâ€. In:Proceedings of the Royal Society A477.2256
(2021). doi: 10.1098/rspa.2021.0518.
20
[64] Karl J Friston. â€œIs the free-energy principle neurocentric?â€ In:Nature Reviews Neu-
roscience 11.8 (2010), pp. 605â€“605.doi: 10.1038/nrn2787-c2.
[65] Karl J Friston. â€œA theory of cortical responsesâ€. In:Philosophical Transactions of the
Royal Society B: Biological Sciences360.1456 (2005), pp. 815â€“836.doi: 10.1098/
rstb.2005.1622.
[66] Yuki Konaka and Honda Naoki. â€œDecoding rewardâ€“curiosity conflict in decision-
making from irrational behaviorsâ€. In: Nature Computational Science 3.5 (2023),
pp. 418â€“432.doi: 10.1038/s43588-023-00439-w.
[67] Karl J Friston. â€œLife as we know itâ€. In:Journal of the Royal Society Interface10.86
(2013), p. 20130475.doi: 10.1098/rsif.2013.0475.
[68] Jakob Hohwy. â€œThe self-evidencing brainâ€. In: Nous 50.2 (2016), pp. 259â€“285.doi:
10.1111/nous.12062.
[69] Anil K Seth and Tim Bayne. â€œTheories of consciousnessâ€. In:Nature Reviews Neuro-
science 23.7 (2022), pp. 439â€“452.doi: 10.1038/s41583-022-00587-4.
[70] Jakub Limanowski and Karl J Friston. â€œâ€˜Seeing the darkâ€™: Grounding phenomenal
transparency and opacity in precision estimation for active inferenceâ€. In:Frontiers
in Psychology9 (2018), p. 643.doi: 10.3389/fpsyg.2018.00643.
[71] Garold Murray. â€œSelf-Access Environments as Self-Enriching Complex Dynamic
Ecosocial Systemsâ€. In: Studies in Self-Access Learning Journal 9.2 (2018). doi:
10.37237/090204.
[72] John R Baker. â€œGoing beyond brick and mortar self-access centers: Establishing a
satellite activity self-access programâ€. In:Studies in Self-Access Learning Journal13.1
(2022), pp. 129â€“141.doi: 10.37237/130107.
[73] Jakub Limanowski and Karl J Friston. â€œAttenuating oneself: An active inference per-
spective on â€œselflessâ€ experiencesâ€. In:Philosophy and the Mind Sciences1.I (2020),
pp. 1â€“16.doi: 10.33735/phimisci.2020.I.35.
[74] Thomas Metzinger. â€œPhenomenal transparency and cognitive self-referenceâ€. In:Phe-
nomenology and the Cognitive Sciences2 (2003), pp. 353â€“393. doi: 10 . 1023 / b :
phen.0000007366.42918.eb.
[75] ThomasMetzinger.â€œEmpiricalperspectivesfromtheself-modeltheoryofsubjectivity:
A brief summary with examplesâ€. In:Progress in Brain Research168 (2007), pp. 215â€“
278. doi: 10.1016/S0079-6123(07)68018-2.
[76] Thomas Metzinger. â€œThe problem of mental actionâ€. In: Philosophy and Predictive
Processing. Ed. by Thomas Metzinger and Wanja Wiese. Frankfurt am Main: MIND
Group, 2017.doi: 10.15502/9783958573208.
[77] Karl J Friston, JÃ©rÃ©mie Mattout, and James Kilner. â€œAction understanding and active
inferenceâ€. In:Biological Cybernetics104 (2011), pp. 137â€“160.doi: 10.1007/s00422-
011-0424-z.
21
[78] Karl J Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman.
â€œDeep temporal models and active inferenceâ€. In:Neuroscience & Biobehavioral Re-
views 77 (2017), pp. 388â€“402.doi: 10.1016/j.neubiorev.2017.04.009.
[79] Karl J Friston, Thomas Parr, and Bert de Vries. â€œThe graphical brain: Belief propa-
gation and active inferenceâ€. In:Network Neuroscience1.4 (2017), pp. 381â€“414.doi:
10.1162/NETN_a_00018.
[80] Giovanni Pezzulo. â€œAn Active Inference view of cognitive controlâ€. In:Frontiers in
Psychology 3 (2012), p. 478.doi: 10.3389/fpsyg.2012.00478.
[81] Harriet Feldman and Karl J Friston. â€œAttention, uncertainty, and free-energyâ€. In:
Frontiers in Human Neuroscience4 (2010).doi: 10.3389/fnhum.2010.00215.
[82] Mark J Edwards, Rick A Adams, Harriet Brown, Isabel Parees, and Karl J Friston.
â€œA Bayesian account of â€˜hysteriaâ€™â€. In:Brain 135.11 (2012), pp. 3495â€“3512.doi: 10.
1093/brain/aws129.
[83] Jakob Hohwy. â€œAttention and conscious perception in the hypothesis testing brainâ€.
In: Frontiers in Psychology3 (2012), p. 96.doi: 10.3389/fpsyg.2012.00096.
[84] Harriet Brown, Rick A Adams, Isabel Parees, Mark Edwards, and Karl J Friston. â€œAc-
tive inference, sensory attenuation and illusionsâ€. In:Cognitive Processing14 (2013),
pp. 411â€“427.doi: 10.1007/s10339-013-0571-3.
[85] Ryota Kanai, Yutaka Komura, Stewart Shipp, and Karl J Friston. â€œCerebral hierar-
chies: predictive processing, precision and the pulvinarâ€. In:Philosophical Transac-
tions of the Royal Society B: Biological Sciences370.1668 (2015), p. 20140169.doi:
10.1098/rstb.2014.0169.
[86] Simone Vossel, Christoph Mathys, Klaas E Stephan, and Karl J Friston. â€œCortical
coupling reflects Bayesian belief updating in the deployment of spatial attentionâ€. In:
Journal of Neuroscience35.33 (2015), pp. 11532â€“11542.doi: 10.1523/JNEUROSCI.
1382-15.2015.
[87] Vivien Ainley, Matthew A J Apps, Aikaterini Fotopoulou, and Manos Tsakiris. â€œâ€˜Bod-
ily precisionâ€™: a predictive coding account of individual differences in interoceptive
accuracyâ€. In:Philosophical Transactions of the Royal Society B: Biological Sciences
371.1708 (2016), p. 20160003.doi: 10.1098/rstb.2016.0003.
[88] Jakub Limanowski. â€œ(Dis-)Attending to the Body â€” Action and Self-Experience in
the Active Inference Frameworkâ€. In:Philosophy and Predictive Processing. Ed. by
Thomas Metzinger and Wanja Wiese. Frankfurt am Main: MIND Group, 2017.doi:
10.15502/9783958573192.
[89] Thomas Parr and Karl J Friston. â€œAttention or salience?â€ In: Current Opinion in
Psychology 29 (2019), pp. 1â€“5.doi: 10.1016/j.copsyc.2018.10.006.
[90] Jakub Limanowski. â€œPrecision control for a flexible body representationâ€. In: Neu-
roscience and Biobehavioral Reviews 134 (2022), p. 104401. doi: 10 . 1016 / j .
neubiorev.2021.10.023.
22
[91] Ryan Smith, Richard D Lane, Thomas Parr, and Karl J Friston. â€œNeurocomputational
mechanisms underlying emotional awareness: Insights afforded by deep active infer-
ence and their potential clinical relevanceâ€. In:Neuroscience & Biobehavioral Reviews
107 (2019), pp. 473â€“491.doi: 10.1016/j.neubiorev.2019.09.002.
[92] Standard for Spatial Web Protocol, Architecture and Governance. 2020.url: https:
//standards.ieee.org/ieee/2874/10375/.
[93] Somayajulu L N Dhulipala and Ryan C Hruska. â€œEfficient interdependent systems
recovery modeling with DeepONetsâ€. In: arXiv (2022), pp. 1â€“6. doi: 10 . 48550 /
arXiv.2206.10829.
[94] Jakob Schoeffer, Johannes Jakubik, Michael Voessing, Niklas Kuehl, and Gerhard
Satzger. â€œOn the Interdependence of Reliance Behavior and Accuracy in AI-Assisted
Decision-Makingâ€. In:arXiv (2023). doi: 10.48550/arXiv.2304.08804.
[95] Tian Lan, Xian-Ling Mao, Wei Wei, and Heyan Huang. â€œWhich Kind Is Better in
Open-domain Multi-turn Dialog, Hierarchical or Non-hierarchical Models? An Em-
pirical Studyâ€. In:arXiv (2020). doi: 10.48550/arXiv.2008.02964.
[96] Mandar Kulkarni and Aria Abubakar. â€œSoft Attention Convolutional Neural Networks
for Rare Event Detection in Sequencesâ€. In: 2020.doi: 10.48550/arXiv.2011.02338.
[97] Craig Gin, Bethany Lusch, Steven L Brunton, and J Nathan Kutz. â€œDeep learn-
ing models for global coordinate transformations that linearise PDEsâ€. In: Euro-
pean Journal of Applied Mathematics 32.3 (2021), pp. 515â€“539. doi: 10 . 1017 /
S0956792520000327.
[98] Dana Lahat, TÃ¼lay Adali, and Christian Jutten. â€œMultimodal data fusion: An
overview of methods, challenges, and prospectsâ€. In:Proceedings of the IEEE103.9
(2015), pp. 1449â€“1477.doi: 10.1109/JPROC.2015.2460697.
[99] Microsoft Defender Security Research Team. â€œSeeing the big picture: Deep learning-
based fusion of behavior signals for threat detectionâ€. In: (2020).url: https : / /
tinyurl.com/3kpzvk9d.
[100] Juliana Jansen Ferreira and Mateus Monteiro. â€œThe human-AI relationship in
decision-making: AI explanation to support people on justifying their decisionsâ€. In:
arXiv (2021). doi: 10.48550/arXiv.2102.05460.
[101] Maxwell J D Ramstead, Anil K Seth, Casper Hesp, Lars Sandved-Smith, Jonas Mago,
Michael Lifshitz, Giuseppe Pagnoni, Ryan Smith, Guillaume Dumas, Antoine Lutz,
Karl J Friston, and Axel Constant. â€œFrom generative models to generative passages:
A computational approach to (Neuro) Phenomenologyâ€. In:Review of Philosophy and
Psychology 13.4 (2022).doi: 10.1007/s13164-021-00604-y.
[102] Pierre Beckmann, Guillaume KÃ¶stner, and InÃªs HipÃ³lito. â€œRejecting Cognitivism:
Computational Phenomenology for Deep Learningâ€. In:arXiv (2023).doi:10.48550/
arXiv.2302.09071.
23