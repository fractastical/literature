ಈ
Deep Free Energy Principle for Adaptive Behavior
˓ Ԯౡ ྒ໵(ֶ)ޗ(ֶ)෉(ֶ)
Ryoya OGISHIMA, The University of Tokyo, ogishima@isi.imi.i.u-tokyo.ac.jp
Shogo YONEKURA, The University of Tokyo, yonekura@isi.imi.i.u-tokyo.ac.jp
Yasuo KUNIYOSHI, The University of Tokyo, kuniyosh@isi.imi.i.u-tokyo.ac.jp
Free Energy Principle enables agents to understand the generative models of the environ-
ment, to have beliefs about their current states by perceptual inference, and to behave adaptively
to environmental changes by minimizing their prediction errors. This work combines Free Energy
Principle from computational cognitive neuroscience and Deep Learning from computer science,
suggesting its potentials to be applied to the understanding of agents’ adaptive behaviors in
complex environments. As an example, this paper shows that an agent can behave adaptively
when it is given an expert’s goal-directed belief.
Key Words:
Free Energy Principle, Bayesian, Prediction Error, Adaptive Behavior
1ܠ
࢝
ಈ͢Δ஌ೳͷཧղ͓ΑͼɼͦͷΑ͏
Λਪ͠ਐΊΔ্ͰॏཁͰ͋Δɽ
σʔλΛઆ໌Ͱ͖Δ࿮૊Έͱ͠
ͨ͜͠
ΘΕΔ [1]ɽ͜ͷ֊૚ੑ
श͢Δ͜ͱ͕Ͱ
श͢ΔͨΊʹॏ
༝Τωϧ
ࣜ
ཧʹै
͏γεςϜͷ૑ൃతৼΔ෣͍ͷҰଆ໘ͱͯ͠ଊ͑ΒΕΔ [2]ɽ͜
ͱ
ங͢ΔաఔͰ͋Δͱओு͢Δ Bayesian
Brain Ծઆ[3] Λ಺แ͢Δ΋ͷͰ͋Δ͕ɼͦΕ͚ͩʹͱͲ·Βͣɼ
খԽ͢ΔΑ͏ʹੜ੒͞Ε
͕ agentೖྗΛੜͤ͡͞ɼ
agent͑Δͱɼ
ೖྗ΋มԽ͢Δͨ
͜͢ঢ়ଶʹ
͑ΒΕΔɽ
2ཧ(Free Energy Principle)
ཧ [2] ͱ͸ Bayesian Brain ʹ୺Λൃͯ͠
Fristonೳͱͯ͠͸ೝ
Λ͢Δ Perceptual InferenceಈΛ͢Δ Active Inferenceɼ
श͢ΔPerceptual Learningͷ3 ͭʹ෼͚
ಈ·ͰͷҰ࿈ͷϧʔϓΛ౷Ұతʹ
ѻ͏͜ͱ͕Ͱ͖Δɽ
2.1 Perceptual Inference
Perceptual Inferenceͱ͸ɼagentͬ
ݪ
ೖྗ
(sensory observation)
Λ oҼͷਪఆͰ͋Δ৴೦(belief)͹Ε
ΔӅΕঢ়ଶ(hidden state)Λsલ෼෍p(s)ɼ໬
౓ p(o∣s) ͔ΒͳΔੜ੒Ϟσϧ p(sɼo) = p(s)p(o∣s) ͸͢Ͱʹ෼
͔͍ͬͯΔͱͯ͠ɼϕΠζͷఆཧΑΓ
p(s∣o) = p(s)p(o∣s)
p(o) (1)
ͱͯ͠ɼ͋Δ oҼͷਪఆ s෼෍
p(s∣o)ʹ͸p(o) = ∫ p(s)p(o∣s)ds ͷ
ख๏ͱͯ͠ม෼
཰෼෍
q(s)
෼෍p(s∣o)͢Δɽ
2཰෼෍p ͱ q͞Λද͢ྔͱͯ͠KL divergence (Χ
ϧόοΫϥΠϒϥʔ৘ใྔ)཰෼෍q(s)ࣄ
෼෍ p(s∣o)͚͍ͮͯ͘͜ͱ͸ DKL(q(s)∣∣p(s∣o))খ
߲p(o) Λ෼཭͢Δͱɼ
DKL(q(s)∣∣p(s∣o)) = ln p(o) + F (2)
Ͱ͖ΔF = DKL(q(s)∣∣p(sɼo))Εɼ͜ΕΛ
(ม෼)খԽʹΑΓq(s) ͕ p(s∣o)
෼෍ q(s)ೖ
ྗ oҼͷ೴಺Ͱͷਪఆ s཰෼෍ͱ΋
Ͱ͋Δ q(s)ʹ͢
খԽ͢ΔΑ͏ʹมԽͤ͞Δաఔ͕
Perceptual InferenceͰ͋Δɽ
2.2 Active Inference
Active Inferenceͱ͸ɼagent֮ײ
ೖྗoಈa਺ͱͯ͠o(a) ͱॻ͚Δinverse modelͬ
͍ͯΔͱ͍͏Ծఆͷ΋ͱɼagent͢ΔΑ
ड़͢Δ͜ͱͰ͋
ΔɽF͑ΒΕΔɽ
F = Eq(s)[−ln p(o∣s)] + DKL(q(s)∣∣p(s)) (3)
Active Inferenceଌ஋o ͷΈͳ
খԽΛ͢Δ͜ͱʹͳΔɽ͜Ε͸৴೦ʹͭ
͢ΔΑ͏໬
খԽ͢Δ͜
ͱͱಉ஋Ͱ͋Δɽ
2.3 Perceptual Learning
Perceptual LearningهPerceptual Inference͓Αͼ
Active InferenceͰagentಘ͍ͯ͠ΔͱԾఆ͍ͯͨ͠
ମ pθ(sɼo) = pθ(s)pθ(o∣s) (θ Ͱύϥϝτϥ
Πζ͢Δ)Ͱ͖ͳ͍Ώ͑ʹ (2)
Ͱ෼཭ͨ͠P (o) ͸ Bayesian Model Evidenceɼ
཰Λද͠
ଌ
शͨ͜͠ͱʹͳΔɽ
1A1-F16
No. 18-2 Proceedings of the 2018 JSME Conference on Robotics and Mechatronics, Kitakyushu, Japan, June 2-5, 2018
1A1-F16(1)
(2)ͰKL divergenceΑΓ0 Ҏ্ͷ஋ΛͱΔ͜ͱΛར༻
͢Δͱɼ
0 ≦ −ln p(o) ≦ F (4)
ͱͳΔɽ−ln p(o) ͸ surprisalೖྗo ͷΊͣΒ͠
͞Λද͢ɽBayesian Model EvidenceେԽ͢Δ͜ͱ͸ͦͷ
ෛͷର਺໬౓Ͱ͋Δ surprisalখԽ͢Δ͜ͱͱಉ஋Ͱ͋Γɼ
͘
ঢ়ଶΛආ͚Δͱ͍͏ homeostasis ͱͷΞφϩδʔͰཧղͰ͖Δɽ
surprisalখԽ͢Δʹ͸ɼ(4)༝ΤωϧΪʔF
Λੜ੒ϞσϧͷύϥϝʔλθখԽ͢Ε͹ྑ͍͜ͱ͕Θ
͔Δɽ
Ҏ্ΑΓPerceptual InferenceɼActive InferenceɼPerceptual
Learning ͸ͦΕͧΕq(s)஋µsಈaɼύϥϝʔλθ ʹ
༝ΤωϧΪʔ FదԽ໰୊ͱͳ͍ͬͯΔ
͜ͱ͕ཧղͰ͖ͨɽ
(µsɼaɼθ) = arg min
µsɼaɼθ
F (µsɼo(a)ɼθ) (5)
2.4ࢉܭ
༝ΤωϧΪʔ͸ɼ
F = Eqθ(s∣o)[−ln pθ(o∣s)] + DKL(qθ(s∣o)∣∣pθ(s)) (6)
Ͱ͖ͳ͍ͷͰɼ
s ∼ qθ(s∣o) (s = µq
θ + ϵσq
θ) (7)
ͱαϯϓϦϯά͢Δͱɼ
F ≈ 1
nF
∑
nF
(−ln pθ(o∣s) + DKL(qθ(s∣o)∣∣pθ(s))) (8)
= 1
nF
∑
nF
T
∑
t=1
[−ln pθ(ot∣st) + DKL(qθ(st∣ot)∣∣pθ(st))] (9)
Ͱ͖Δɽ
2.5খԽ
ಈ aଌ஋ o؀
ʹґଘ͠ɼagent༝
ΤωϧΪʔ Fಈ aΊΒΕͳ͍ɽͦ͜Ͱ͜
దԽ໰୊Λ Evolution Strategies(ਐԽઓ
ུ) ͷ PEPG(Parameter-Exploring Policy Gradients)[4]Ͱղ
͘ɽ·ͣɼύϥϝʔλθ཰෼෍pψ(θ) = N(θ; µψɼσψ)
͢Δ F଴஋ η(ψ) = Epψ(θ)[F (θ)] Λ θ
ྔͰ͋Δ ψ = [µψ; σψ]഑Λ
͑Δͱɼlog derivative trick[5]Λͯ͠ɼ
∇ψη(ψ) = ∫
pψ(θ)
pψ(θ)
∂pψ(θ)
∂ψ F (θ)dψ (10)
= Epψ(θ)[F (θ)∇ψ ln pψ(θ)] (11)
Ͱ͖Δ͜ͱ͔Βpψ(θ)਺Ͱ͋Δ͜ͱʹ஫ҙ͠
଴஋Λθ ∼ pψ(θ)
ͯ͠ɼҎԼΛಘΔɽ
∇µψ η(ψ) ≈ 1
npop
npop
∑
i=1
F (θ) 1
(σψ)2 (θ − µψ) (12)
∇σψ η(ψ) ≈ 1
npop
npop
∑
i=1
F (θ) (θ − µψ)2 − (σψ)2
(σψ)3 (13)
3 Deep Free Energy Principle
3.1ͱ໰୊఺
Ͱ͋ΔDeep Active Inference[6]Ͱ͸ɼagent༝
ཧͷੜ੒Ϟσϧ(લ෼෍ͱ໬౓)෼෍Λ
ड़ͨ͠ਂ૚ੜ੒ϞσϧΛ΋
·Ͱagentಈaଌ
஋o܎ؔinverse modelo(a)शͷΑ͏
ࡦπ(a∣s) Λಋ
ೖͯ͠ɼ͜Ε΋agentशͤ͞ΔɽagentΛಈ͖ճΓͳ
લ෼෍ɼ
ͷ
4ࣜ12, 13)ͷ
཰෼෍χϡʔϥϧωοτϫʔΫͷύϥϝʔλθ
Λ F਺ F
༝ΤωϧΪʔ (༝ΤωϧΪʔ/λΠϜεςοϓ਺)
Լ๏ͱͯ͠ ADAM optimizer͏ɽ
લ෼෍ʹΑ
ΘΕ͍ͯͳ͍ͷͰagent໨త
લ
͛ΒΕΔɽ
3.2ͷ໨త
ͷμΠφϛΫεΛ
͕ଟ͘ɼͦ΋ͦ΋
ʹ͓͍ͯͲ͏͢Δ͔͸આ໌͕ͳ͞Εͯ͜ͳ͔ͬͨɽͦ
ΘͤΔ͜ͱͰagent ͕μΠφϛ
؍
લ෼෍ʹΑΔ༧ଌϞ
͢Δख๏ΛఏҊ͠ɼ
ಓ
ྻͰagentͰ(1) ΤΩεύʔτʹԊ͏
Α͏֎ྗͰagentಘͰ͖Δ͜ͱ
(2)Ͱ͋Ε͹ɼ֎ྗʹΑΔॿ͚͕ͳ͘ͳͬͨͱ͠
ߦ
ಈΛग़ྗͰ͖ΔΑ͏ʹͳΔ͜ͱ(3)͔ΒมԽ
ͨ͠ͱͯ͠΋agentಈΛదԠతʹੜ੒͢
ޙ࠷
ʹ͍ͭͯ΋
(4)લ෼෍
͏ɽ
3.3 ఏҊख๏
Fig.1 Proposed System Architecture
੒ͱͯ͠͸ਤ1 ͷΑ͏ʹͳ͍ͬͯΔɽstࠁ࣌t
ࡦPolicy π(at∣st)ಈat
ڥ؀Envࠁ࣌t + 1ଌ஋ot+1
ଌ஋ot+1લ෼෍Prior p(st+1∣st) ʹΑΔ
ಈ͢Δલͷ৴೦ stࠁ࣌t + 1 ͷ৴೦ͷ༧ଌ st+1 pred
෼෍ q(st+1∣st+1 pred,o t+1)ࠁ࣌t + 1
ʹ͓͚Δ৴೦ st+1ࡦ
Γฦ͢͜ͱͰagentߋ
ӡಈϧʔϓ
ͱ͸ผʹ໬౓Likelihood p(ot+1∣st+1)ଌ஋Λ༧ଌ͠
Ͱද͢ͱɼ
F = DKL(qθ(s∣o)∣∣pθ(s)) + Eqθ(s∣o)[−ln pθ(o∣s)] (14)
= PE s + PE o (15)
No. 18-2 Proceedings of the 2018 JSME Conference on Robotics and Mechatronics, Kitakyushu, Japan, June 2-5, 2018
1A1-F16(2)
લ෼෍ p(s)Ռ
෼෍q(s∣o)ࠩޡPE s Ͱ
ଌ o ͕༧ଌ௨Γͩͱ p(o∣s) ͕େ͖͘ͳΔΑ͏
਺ʹͳ͍ͬͯΔͷͰ Eq[−ln p(o∣s)] ͸খ͘͞ͳΔ͜ͱ
ࠩޡPE o͑Δ (ॱ൪ʹਤ 1 ͷԼɼӈ্ͷ
Prediction Error ʹରԠ͢Δ)࠷
খԽ͢ΔΑ͏ʹγεςϜΛ
৽͍ͯ͘͜͠ͱʹଞͳΒͳ͍ɽAlgorithm 1ͷ PE s͸ɼ
Algorithm 1ࢉܭ( ఏҊख๏)
Initialize the free energy estimate withF = 0
Initialize nF agents with ˆs0 = (0,...,0)
for each agentdo
for t = 1,...,T do
Sample an action ˆat from πθ(at∣ˆst−1)
Propagate the environment and get observations
ˆot = Env(ˆat)
Draw a single sample ˆst,pred from
the priorp(st∣ˆst−1)
Draw a single sample ˆst from
the variational posteriorqθ(st∣ˆst,pred, ˆot)
Calculate PE o = −ln pθ(ˆot∣ˆst)
Calculate PE s
= DKL(qθ(st∣ˆst,pred, ˆot)∣∣pθ(st∣ˆst−1))
Increment free energyF = F + (PE o + PE s)/nF
Carry ˆst over to the next time step
end for
end for
return F
Diagonal Gaussianͷ KL divergence͕ɼ
PE s = DKL(N(st; µq,σ q)∣∣N(st; µt,σ t)) (16)
=
n
∑
i=1
1
2 (2l nσt
i
σq
i
+ (σq
i )2 + (µq
i − µt
i)2
(σt
i )2 − 1) (17)
લ෼෍3
૚ɼ໬౓ 1෼෍4ࡦ3਺ tanh
ۉ
ఆͨ͠ɽ
ͯ͠͸σ = 10−3ɼ
ͯ͠͸ σ = 10−4Λ༻͍
ͯ͠͸σ = 10−2͠
ͯ͸ σ = 10−3͕
ΑΓେ͖͘ઃఆͨ͠ͷ͸ɼagentଌ஋Α
৴Λ΋ͪɼΑΓྑ͘ΤΩ
ಘͰ͖ΔΑ͏ʹ͢ΔͨΊͰ͋Δɽ͜Ε͸༧ଌ
఻
͑ΒΕ͓ͯΓɼͨͱ
৴౓͕མͪͯɼ
ΕΔ͜ͱΛઆ໌ग़
དྷΔ
[7]ɽ
4Ռ
शͷ෼໺ͰϕʔεϥΠϯͱͯ͠ѻΘΕ͍ͯΔ OpenAI
Gym[8] ͷ Mountain Car Problemͬ
ͨɽਤ 2ʹғ·Εͨ୩͔Βελʔτͯ͠ӈ
Λ௚઀ొΔ΄
ʹ
ʹొΒͳ͍ͱొΓ͖Εͳ
͔Β
agent ΁
ͷೖྗ͸ंͷ xඪͱ xɼagent΁
Ͱ͋ΔɽελʔτҐஔͰ͋Δ୩ఈ͸
x = −0.5ɼΰʔϧ͸ x = 0.45 Ͱ͋Δɽ
Fig.2 Mountain Car Problem Environment
(Left: Original, Right: Changed)
4.1ݧ࣮1:ݧ࣮
ྻΛ
ͬͯɼagent෼
Ռ͸ 1 Τϐ
ͷมԽ͸ਤ3ͷΑ
͏ʹͳͬͨɽ·ͨɼਤ3ଌ஋
(Observation)෼෍ʹΑΔ৴೦(Belief)લ෼෍(Prior)
෼෍͕
ͷ஋ʹͳ͍ͬͯΔ͜ͱ͕Θ͔
Ͱਖ਼͘͠ agentಘͰ͖ͨ͜ͱ
͍
͕খ͞
͘ɼagentͨͨ͠ΊͰ͋Δͱઆ໌ग़དྷΔɽ
Fig.3 Free Energy and Result Dynamics
(Exp1: Perceptual Inference)
4.2ݧ࣮2:ݧ࣮
ݧ࣮1લ෼෍ͷ
͢Δ
ಈΛagentಘͰ͖Δ͔(Active Inference)
Ռɼ
ਤ4 ӈԼͷΑ͏ʹΰʔϧʹ౸ୡͰ͖ΔμΠφϛΫεʹͳΔΑ͏ͳ
ਤ 4ݧ࣮1Ռͱಉ༷
ʹɼਤ4෼෍ͷBeliefଌ஋ͷ
ͯͱΕΔɽ
ͷมԽ͸ਤ4༝Τ
ωϧΪʔ͸ਤ4෼෍
খ஋
Λͱ͍ͬͯΔ͜ͱ͕Θ͔Δɽ
4.3ݧ࣮3:ݧ࣮
ݧ࣮1෼෍ͱΤΩεύʔ
ʹ͓͍ͯͰ΋
ݧ࣮2Ͱ
ͨ͠ɽ͔͠͠ɼActive
Inference͢Δͨ
૊ΈͰ͋Δɽͦ͜Ͱɼ͜͜Ͱ͸Mountain
No. 18-2 Proceedings of the 2018 JSME Conference on Robotics and Mechatronics, Kitakyushu, Japan, June 2-5, 2018
1A1-F16(3)
Fig.4 Velocity, Free Energy, Action over timesteps and
Result Dynamics (Exp2: Active Inference)
Car ProblemΛਤ 2ʹ͓͍ͯ
ূ͠
Ռ͸ਤ 5͠
ݧ࣮2ͱൺ΂Δͱਤ 5ͱͳ͍ͬͯͨɽ
͖͸ҧ͍ͬͯ
ݧ࣮
Ͱ agentײ
͍ͯ͡ΔͨΊɼ୭͔ʹҠಈΛअຐ͞Εͯ֎ྗ͕͸ͨΒ͍͍ͯΔ৔
Γ΍͘͢ͳͬͯ
ͷมԽͰ΋దԠͰ͖Δ͜ͱ͕༧૝͞
ΕɼActive Inferenceӈ͞Εͣʹ໨ඪୡ੒Ͱ͖
͑Δɽ
Fig.5 Action and Result Dynamics
(Exp3: Changed Environment)
4.4ݧ࣮4:ݧ࣮
ݧ࣮1,2,3ಓɼ͢ͳΘͪ
ྻͷ
৘ใΛagentʹґ
͸͜ͷঢ়ଶͰ͋Δͱ͍͏Α͏ͳ
ಘ͢Δ͜
͑Δɽ
ಓͦͷ΋ͷ͔ΒભҠμΠφϛΫε
श͞
ಓ͔Β st ͱ st+1Γɼ
st ͔Β st+1 Λ༧ଌ͢ΔΑ͏ʹωοτϫʔΫDNN (st+1∣st)ֶ
Ռ͸ਤ6ͷΑ͏ʹҐஔ͕-0.5ɼ଎౓͕0 ͷ৴೦͔
Βελʔτͯ͠ɼΰʔϧͰ͋ΔҐஔ 0.45͔͏μΠφϛΫε
Ͱ͋Δɽ͜ͷωοτϫʔΫDNN (st+1∣st)
ݧ࣮2ݧ࣮1෼෍
ಘͤ͞Δ(Active
Inference)Ռ͸ਤ6ଌ஋(੨ઢ) ͷΑ͏ʹ
Ґஔ͕-0.5ɼ଎౓͕0 ͔Βελʔτͯ͠ɼΰʔϧͰ͋ΔҐஔ0.45
͢ΔΑ͏
͍ͯ͠Δͱ
͑Δɽ
Fig.6 Prior Dynamics and Result Dynamics
(Exp4: Active Inference with Expert DNN)
5࿦ͱల๬
ͷੜ੒Ϟσϧ
ͬ
ಈ͢ΔϞσϧΛσΟʔϓχϡʔϥϧωοτϫʔΫ
ங͠ɼagent؀
ͬͨɽ
લ෼෍ͱͯ͠
agent ʹ༩͑ɼͦΕʹԊ͏Α͏ʹ֎ྗͰagentʹ
෼෍ΛPerceptual Inferenceֶ
͢Δ֎ྗ͕ͳ
෼෍Λ༻͍Ε͹ΤΩεύʔτͷ
ಈΛग़ྗͰ͖ΔΑ͏ʹ
शΛ
Active InferenceʹΑΓୡ੒Ͱ͖
ΑΓมΘͬ
؀
ಈग़ྗΛม͑ΒΕΔ͜ͱ͕෼͔ͬͨɽ͜ΕΒ
ྻͰ༩͍͕͑ͯͨɼͦͷμΠφ
લ෼෍Λ
Ͱ͸ɼ
शͨ͠μΠφϛΫ
ମΛPerceptual LearningʹΑΔ
ͷ՝୊ͱͳΔɽ
ࣙ
։
ՌಘΒΕͨ΋ͷͰ͢ɽ
ݙ
[1] R. P. Rao and D. H. Ballard, ʠPredictive coding in the
visual cortex: a functional interpretation of some extra-
classical receptive-ﬁeld eﬀects, ʡ Nature neuroscience, vol.2,
no.1, pp.79–87, 1999.
[2] K. Friston, J. Kilner, and L. Harrison,ʠAf r e ee n e r g yp r i n c i -
ple for the brain,ʡJournal of Physiology-Paris, vol.100, no.1,
pp.70–87, 2006.
[3] D. C. Knill and A. Pouget,ʠThe bayesian brain: the role of
uncertainty in neural coding and computation,ʡTRENDS in
Neurosciences, vol.27, no.12, pp.712–719, 2004.
[4] F. Sehnke, C. Osendorfer, T. Ruckstieb, et al.,ʠParameter-
exploring policy gradients,ʡ Neural Networks, vol.23, no.4,
pp.551–559, 2010.
[5] R. J.Williams, ʠSimple statistical gradient-following algo-
rithms for connectionist reinforcement learning,ʡin Reinforce-
ment Learning. Springer, 1992, pp.5–32.
[6] K. Ueltzhoﬀ er, ʠDeep active inference, ʡ arXiv preprint
arXiv:1709.02341, 2017.
[7] K. J. Friston, J. Daunizeau, J. Kilner, et al.,ʠAction and
behavior: a free-energy formulation,ʡBiological cybernetics,
vol.102, no.3, pp.227–260, 2010.
[8] G. Brockman, V. Cheung, L. Pettersson, et al.,ʠOpenai gym,ʡ
2016.
No. 18-2 Proceedings of the 2018 JSME Conference on Robotics and Mechatronics, Kitakyushu, Japan, June 2-5, 2018
1A1-F16(4)