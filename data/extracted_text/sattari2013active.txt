arXiv:1007.3336v2  [cs.NI]  6 Feb 2013
Active Topology Inference using Network Coding
Pegah Sattari a, Christina Fragouli b, Athina Markopoulou a
aCalit2 Building, Suite 4100, UC Irvine, Irvine, CA 92697-28 00, United States
bEPFL IC ARNI, Building BC, Station 14, CH - 1015 Lausanne, Swi tzerland
Abstract
Our goal is to infer the topology of a network when (i) we can se nd probes between sources and
receivers at the edge of the network and (ii) intermediate no des can perform simple network coding
operations, i.e., additions. Our key intuition is that network coding introdu ces topology-dependent
correlation in the observations at the receivers, which can be exploited to infer the topology. For
undirected tree topologies, we design hierarchical cluste ring algorithms, building on our prior work
in [24]. For directed acyclic graphs (DAGs), ﬁrst we decompo se the topology into a number of
two-source, two-receiver (2-by-2) subnetwork components and then we merge these components to
reconstruct the topology. Our approach for DAGs builds on pr ior work on tomography [36], and
improves upon it by employing network coding to accurately d istinguish among all diﬀerent 2-by-2
components. We evaluate our algorithms through simulation of a number of realistic topologies
and compare them to active tomographic techniques without n etwork coding. We also make
connections between our approach and alternatives, includ ing passive inference, traceroute, and
packet marking.
Keywords: Network Tomography, Network Coding, Topology Inference
1. Introduction
Knowledge of network topology is important for network mana gement, diagnosis, operation,
security and performance optimization. Depending on the co ntext, one may be interested in
the topology at diﬀerent layers, such as the Internet’s route r-level topology, an overlay network
topology, the topology of an ad-hoc wireless network, etc.
There is a large body of prior work on measurements and infere nce of network topology. One
family of techniques assumes the cooperation of nodes in the middle of the network, and uses
traceroute measurements to collect the ids of nodes along paths and use t hem to reconstruct the
topology. Another family of techniques, referred to as network tomography , assumes no coopera-
tion from internal nodes and relies on end-to-end probes to i nfer internal network characteristics,
including topology. More speciﬁcally, multicast or unicas t probes are sent/received between sets
of sources/receivers at the edge of the network, and the topo logy is inferred based on the number
and order of received probes.
In this paper, we revisit the problem of topology inference u sing end-to-end probes, in networks
where intermediate nodes are equipped with simple network c oding capabilities. We show how to
exploit these capabilities in order to perform active topol ogy inference in a more accurate and
eﬃcient way than existing tomographic techniques.
Email addresses: psattari@uci.edu (Pegah Sattari), christina.fragouli@epfl.ch (Christina Fragouli),
athina@uci.edu (Athina Markopoulou)
1
Our key intuition is that network coding introduces topolog y-dependent correlation in the
content of packets observed at the receivers, which can then be exploited to reverse-engineer the
topology. For example, a coding point (that combines multip le incoming packets into one or more
outgoing packets) introduces correlation between packets coming from diﬀerent sources, in a similar
way that multicast introduces correlation in the packets se nt by the same source and observed by
several receivers. In fact, the correlation introduced by m ulticast has been the starting point and
the main idea underlying tomographic topology inference. S ubsequent schemes made this idea
more practical, by emulating multicast with back-to-back u nicast probes [10, 37]. In contrast,
relating probes from diﬀerent sources to reveal intermediat e nodes, also referred to as multiple-
source tomography, has been a challenge [3, 36, 37]. Using si mple network coding operations at
coding points solves this problem and allows accurate and fa st topology inference.
Our approach is general and can be applied to infer the topolo gy in a range of scenarios, in-
cluding but not limited to wireless multi-hop networks. Wir eless multi-hop networks are able to
support simple network coding operations (additions are su ﬃcient for our schemes), as demon-
strated in [32], and can therefore beneﬁt from our technique s. Furthermore, there is a good match
between some properties and constraints of such networks an d our schemes. First, there is natural
variability in the delay of wireless links, which (if approp riately used - as explained in later sec-
tions) can expedite inference. Second, our schemes keep int ernal nodes simple (moving processing
for inference to dedicated nodes at the edge) and anonymous ( revealing the logical topology but
not the identities of nodes). Finally, improving the speed o f inference may prove important to keep
up with changes, e.g., due to mobility.
Our contributions are as follows. First, we consider undire cted trees, where leaves can act as
sources or receivers of probes, and we design hierarchical c lustering algorithms that infer the topol-
ogy, building on our prior work in [24]. Then, we consider dir ected acyclic graphs (DAGs) with a
ﬁxed set of M sources and N receivers and a pre-determined rou ting scheme. We ﬁrst decompose
the topology into a number of two-source, two-receiver subn etwork components and then we merge
these components to reconstruct the topology. Our approach for DAGs builds on prior work on
tomography [36], and improves upon it by employing simple ne twork coding operations at inter-
mediate nodes to deterministically distinguish among all p ossible 2-by-2 subnetwork components,
which was impossible without network coding [36, 37]. We eva luate our algorithms through simula-
tion over a number of topologies and we show that they can infe r the topology accurately and faster
than tomographic approaches without network coding. We pre sent our schemes as active probing:
special probes are sent by the sources, speciﬁcally for the p urpose of inference, and are treated
in special ways by intermediate nodes and eventually receiv ed by the receivers and processed at
a fusion center. We believe that our active probing approach with network coding provides one
more building block, in the already large space of topology i nference techniques, with core strength
and ability to identify joining points. We also compare and m ake connections between our active
probing approach and alternatives, such as passive inferen ce, traceroute, and packet marking.
The rest of the paper is organized as follows. Section 2 discu sses related work. Section 3
presents our assumptions, notation, and problem statement . Section 4 summarizes the main re-
sults of the paper. Section 5 presents algorithms for inferr ing tree topologies. Binary trees are
discussed in Section 5.1, in the absence (Section 5.1.1) or p resence (Section 5.1.2) of packet loss.
General trees are discussed in Sections 5.2.1 and 5.2.2. Sec tion 6 presents algorithms for inferring
directed acyclic graphs (DAGs). Section 6.2 presents algor ithms for inferring 2-by-2 subnetwork
components, in the absence (6.2.1) or presence (6.2.2) of pa cket loss. Section 6.3 explains how to
merge these components to reconstruct the topology. Sectio n 7 provides simulation results for some
realistic topologies. Section 8 discusses two possible dep loyment scenarios (one as an active probing
scheme and another one using packet marking), and makes conn ections between our approach and
2
alternative topology inference approaches. Section 9 conc ludes the paper. Appendices A and B
analyze the probability of error of our inference algorithm s in trees and DAGs, respectively.
2. Related Work
One body of related work is network tomography in general, an d topology inference in par-
ticular. A good survey of network tomography can be found in [ 6]. An early work on topology
inference using end-to-end measurements is [38], where the correlation between end-to-end multi-
cast packet loss patterns was used to infer the topology of bi nary trees. The correctness of this
idea was rigorously established in [19], and was extended to general trees and to measurements
other than loss, such as delay variance [20], or more general ly any metric that grows monotonically
with the number of traversed links. The idea has also been ext ended to unicast probes [10, 37]. In
summary, tomographic schemes for topology inference use en d-to-end active probing and feed the
number, order, or a monotonic property of received probes as input to statistical signal-processing
techniques. Inference of link characteristics [5] can also be combined with topology inference [37].
In a diﬀerent context, similar problems have been studied in t he context of phylogenetic trees [22].
The work in [1] uses such algorithms [22], for topology infer ence in sparse random graphs.
In addition, inference of congested links has been studied f rom the angles of compressive sensing
[11, 12, 48] and group testing [7, 16, 35]. The work in [7] form ulates the problem as a graph-
constrained group testing, where the items correspond to edges, some of t hem being defective,
and the goal is to identify the defective edges given that the test matrix conforms to constraints
imposed by the graph, e.g., the path connectivities. The work in [48] recovers sparse ve ctors,
representing certain parameters of the links over the graph , through l1 minimization. It improves
the number of required measurements over [7], as compressiv e sensing allows real numbers for the
link characteristics and measurements instead of true/fal se binary values in group testing problems.
Most tomographic approaches rely on probes sent from a singl e source in a tree topology
[2, 4, 10, 17–21, 23, 38, 44]. Rabbat et al. [13, 36, 37] introduced the multiple-source multiple-
destination (M-by-N) tomography problem, by sending probe s between M sources and N receivers.
In [36, 37], it was shown that an M-by-N network can be decompo sed into a collection of 2-by-2
components. Then, coordinated transmission of multi-pack et probes from two sources and packet
arrival order measurements at the two receivers were used to infer some information about the
2-by-2 topology. Assuming knowledge of M 1-by-N topologies and all 2-by-2 components, it was
also shown how to merge a second source’s 1-by-N tree topolog y with the ﬁrst one. The resulting
M-by-N topology is not exact, but bounds are provided on the l ocations of joining points with
respect to the branching points. This approach also require s a large number of probes, as do all
approaches that need to collect enough probes for statistic al signiﬁcance [10, 17, 20, 21, 46]. Our
work on DAGs builds on and extends the multiple-source multi ple-destination work in [36, 37], but
uses network coding to achieve exact and fast topology infer ence.
A second body of related work is from the network coding liter ature. It is well known that linear
network coding makes a network behave as a linear system, who se transfer function depends on
the topology. Based on the source packets and the observatio ns at the receivers, one can then try
to passively infer the topology. The following papers consi der that random linear network coding
is employed for the purpose of information transfer, and the y perform passive inference on the
side. In [28], passive techniques are used to distinguish am ong failure patterns. In [30, 31, 42, 49],
subspace properties at various nodes are used for topology i nference and error localization. In [42],
each node passively infers its upstream topology at no cost t o throughput, but at high complexity.
In contrast, we propose active probing and a simple coding scheme at intermediate nodes, to
achieve low-complexity topology inference at the end nodes . In Section 8, we provide a detailed
comparison and make connections between active and passive topology inference. In [25, 34], we
3
revisited link-loss (but not topology) tomography using ac tive probing and network coding. In the
ﬁrst part of this paper, we extend our preliminary work in [24 ], where we showed that active probes
from two sources and XOR at intermediate nodes are suﬃcient to infer binary tree topo logies. This
approach generalizes to trees, but not to general graphs. In [40], we used a diﬀerent approach for
general graphs, which builds on [36, 37]: we identify 2-by-2 components and merge them together
in an M-by-N topology. This journal paper combines and exten ds our preliminary work in [24, 40].
A practical approach for inferring the network topology is b ased on traceroute [8, 9, 14, 15, 26,
29, 43, 45, 47, 50]. Multiple traceroute’s are sent among monitoring hosts, they record node ids
along paths, and this information is put together to reconst ruct the graph. The traceroute-based
approach is discussed in detail in Section 8.5.
Wireless sensor networks and information fusion are consid ered in [27, 33]. Information is
collected at sensor nodes and is forwarded towards a fusion c enter, following a known reverse tree
topology. Information is aggregated [27] or network coded [ 33] at intermediate nodes, and the
loss rates of links are inferred based on the observations at the fusion center. In contrast, we are
interested in inferring the topology of DAGs.
3. Problem Statement
3.1. Model
Assumptions about the Network. We are interested in inferring static topologies1. We are
also interested in inferring logical topologies, which are deﬁned by the branching and joining po ints
where the measured end-to-end paths meet 2.
In the ﬁrst part of the paper, we consider undirected trees wi th |V | = n vertices, |E| = n − 1
edges that can be used in both directions, and exactly one pat h between any two vertices. We
denote by L = {1, 2, ..., L } the leaf-vertices of the tree, which correspond to end-host s that can act
as sources or receivers of probe packets.
In the second part, we consider directed acyclic graphs (DAG s) with M sources and N receiver
nodes, which we refer to as M-by-N topology , following the terminology of [36, 37]. Without loss
of generality (W.l.o.g.), we present most of our discussion in terms of M = 2, i.e., inferring a
2-by-N topology; an M-by-N topology can be constructed by me rging smaller structures. Similarly
to [36, 37], we also assume that a predetermined routing poli cy maps each source-destination pair
to a unique route from the source to the destination. 3 This implies the following three properties,
ﬁrst stated in [36].
A1 There is a unique path from each source to each receiver.
A2 Two paths from the same source to diﬀerent receivers take th e same route until they branch,
so that all 1-by-2 components have the “inverted Y” structur e; the node where the paths to
the two receivers split is called a branching point, B.
1Our algorithms assume that the topology remains static duri ng the inference. However, the topology may change
over longer time scales.
2Intermediate nodes in a logical topology have degree at leas t 3, and in-degree and out-degree at least 1. Since it
is necessary for identiﬁability, focusing on logical topol ogies is a standard assumption in topology inference proble ms.
3Our assumption for single path routing is based on the follow ing reasons: (1) single path routing is the reality
in most networks today: routers pick the unique next hop towa rds the destination; (2) this was also the assumption
made by Rabbat et al. [36, 37], which is the starting point on which we build in this paper, by adding simple network
coding operations; (3) topology inference when multi-path routing is used is an open problem; the state of the art is
[1], which proposes a heuristic approach; and (4) network co ding is only used on special probes for the purpose of
inference here, and not for throughput, which could be impro ved by using multi-path routing.
4
A3 Two paths from diﬀerent sources to the same receiver use exa ctly the same set of links after
they join, so that all 2-by-1 components have the “Y” structu re; the node where the paths
from the two sources merge is called a joining point , J.
These properties are consistent with destination-based ro uting: the next hop taken by a packet
is determined by a routing table lookup on the destination ad dress. Each subnetwork from one
source to N receivers is a 1-by-N tree; the general graph is called a “mul tiple-tree” network [36].
Loss and Delay. We consider scenarios with and without packet loss. Each lin k has a delay
with a ﬁxed part, e.g., the propagation and transmission delay, and a variable part , e.g., the
queueing delay. Path delay is the sum of delays across the lin ks in the path. We have no control
over the delays of the links but we have control over the timin g of operations at sources and
intermediate nodes. We can make sources and intermediate no des operate in time slots of duration
T and W , respectively, which can be chosen to be quite longer than li nk delays as explained later. 4
Goal. Our goal, in this paper, is to design active probing schemes, i.e., the operation of
sources, intermediate nodes and receivers, that will allow us to infer the logical topology from the
observations at the receivers. We restrict the space of poss ible operations to the simple options
described in the rest of this section. In later sections, we d esign schemes based on these simple
operations and we show that they are suﬃcient for topology in ference. We will revisit the problem
statement and make it more precise in the sections for trees a nd DAGs.
Operation of Sources. An experiment consists of a pair of sources S1 and S2 sending,
at the same time, a multicast probe packet each ( x1 = [1 , 0] and x2 = [0 , 1], or more generally
symbols from a ﬁnite ﬁeld) to all N receivers. These are special probes sent solely for the purpose of
inference, not for regular data transfer, and treated in a sp ecial way, speciﬁed next, by intermediate
nodes. We perform up to countMax experiments. Consecutive experiments are spaced apart by a
large time interval T , to ensure that only probes in the same experiment are combin ed together.
Operation of Intermediate Nodes. Intermediate nodes are assumed to support unicast,
multicast and the simplest possible network coding operati on, i.e., addition over a ﬁnite ﬁeld Fq.
They operate in time slots of pre-determined duration or win dow W : a node waits for W to receive
probe packets from its incoming links; if it receives more th an one packet, it codes them together
and forwards (unicast or multicast) the resulting packet do wnstream. The choice of W aﬀects
where the packets from the two sources meet. Essentially, an intermediate node can act either as a
joining point (J), in which case it adds all incoming packets and forwards t he output to all outgoing
links;5 or as a branching point (B), in which case it sends (multicasts) the single incoming packet
downstream. This operation will be speciﬁed more precisely in the sections for trees and DAGs.
Operation of Receivers. Each receiver i receives probes Ri, which are the source packets
x1, x2, or a linear combination of x1 and x2, as the result of network coding operations at in-
termediate nodes. Inference of topology is based only on the observations Ri’s. We assume that
these observations are sent to a fusion center for central pr ocessing and inference; consistently to
all tomography literature, the communication of the receiv ers and the fusion center is out of the
scope of this paper.
Intuition. Multicast as well as network coding (which is limited to simp le addition in this
paper, thus can be thought of as reverse multicast) introduc e topology-dependent correlation in the
4This can be achieved, for example, by assuming a coarse synch ronization across source and network nodes (on
the order of 5-10ms using NTP), and by making nodes wait for a w indow before sending or coding/forwarding probe
packets respectively.
5In our schemes, all joining points perform network coding. T herefore, in the rest of the paper, we use the terms
joining point , which comes from the tomography literature [13, 36, 37], an d coding point , which comes from the
network coding literature, interchangeably.
5
content of packets, which can be used at the receivers to infe r the underlying topology. In particular,
multicast helps reveal the branching points while network c oding helps reveal the joining points.
3.2. Scope and Discussion
Possible deployment scenarios are described in Section 8.1 . The ﬁrst scenario (sending special
probes for the sole purpose of topology inference) is used to describe the schemes throughout the
paper. Furthermore, and beyond the speciﬁc details of the de ployment, we believe that our work
provides a fundamental building block for exploiting corre lation in the content of network coded
packets for inference of joining points. Similarly, multic ast tomography showed how to exploit
correlation in multicast packets for inference of branchin g points; it was then followed by a series
of papers that used unicast traﬃc to “mimic” multicast probe s and the whole functionality while
being more practical.
We would like to emphasize that, in this paper, we apply netwo rk coding on special probes
solely for the purpose of topology inference, and not for improving data transfer (decoding the
source messages at the receivers). In data transfer, throug hput and delay are indeed important
metrics. In our problem, the important metrics are: identiﬁ ability (for which, we show that network
coding is necessary); and eﬃciency, i.e., the number of probes used and the amount of network
resources consumed for a certain level of estimation accura cy (which we show that it is improved
by network coding). Therefore, the delay of the algorithms i s not of primary concern in this paper:
inferring the topology in the order of seconds as opposed to m illiseconds is acceptable in our setup.
Multi-path routing, which could increase throughput with n etwork coding, is not considered either.
4. Main Results
The main results obtained in this paper are the following:
• For tree networks:
– When there is no packet loss in the links, we design the determ inistic Alg. 1, which
infers the topology in O(n) iterations, where n is the number of edges in the tree.
– When there is packet loss in the links, we design Alg. 2, which infers the topology in
O(nM) iterations, where M ≤ 1
Pmin
, and Pmin is the minimum probability of success
across all paths between a source and a destination.
• For DAGs, we decompose the topology into 2-by-2 subnetwork c omponents, and then we
merge these components to reconstruct the topology. We desi gn inference algorithms to infer
the 2-by-2’s, and we design merging algorithms to merge them back to the original topology:
– When there is no packet loss in the links, we design Alg. 5, whi ch identiﬁes any 2-by-2
topology with probability of error analyzed in Eq.(2), in countMax experiments.
– When there is packet loss in the links, we design Alg. 6, which identiﬁes any 2-by-2
topology with probability of error analyzed in Lemma 6.1, in countMax experiments.
– Assuming knowledge of all the 2-by-2’s and one source’s 1-by -N tree topology, we design
Alg. 7 that merges a second source’s topology with the ﬁrst on e by identifying all the
joining points in O(N log N) steps.
– Assuming knowledge of all the 2-by-2’s, but not the 1-by-N tr ee topology, we can identify
all the joining points if and only if there are no branching po ints in a row. We merge
the two topologies in
( N
2
)
steps, as we describe in Section 6.3.2.
6
– We also provide a lower bound on the number of 2-by-2’s requir ed by any merging
algorithm to uniquely localize all the joining points in a 2- by-N topology, given one
source’s 1-by-N topology. In Lemma 6.2, we show that it is N
2 .
• We also make connections between our approach and alternati ve topology inference ap-
proaches in Section 8.
Note that Alg. 5, Alg. 6, and Alg. 7 build on and extend the corr esponding algorithms by
Rabbat et al. [13, 36, 37], in the presence of network coding.
5. Inferring Trees
Overview. We design algorithms for inferring undirected tree topolog ies, based only on probes
sent between leaf nodes. We follow a hierarchical, top-down approach, by iteratively dividing the
tree topology into smaller clusters and revealing how the gr oups are connected to each other.
Operation of Sources and Receivers. In each iteration (timeslot T >> W ) a set of leaves
(diﬀerent across timeslots) are chosen to act as sources and t he remaining leaves act as receivers.
Each source sends a distinct packet. The receiver stores the ﬁrst packet it receives, and discards
any subsequent packets (in the same iteration).
Operation of Intermediate Nodes. Every intermediate node operates in intervals of du-
ration W . If, within W , the node receives a single probe from one of its neighbors, i t multicasts
the probe to all other neighbors. If, within W , it receives more than one packet from diﬀerent
neighbors, it adds them and forwards the result to all remain ing neighbors. In binary trees, this
linear combination is simply XOR. In general trees, we need operations over higher ﬁelds. 6
Summary of Results. In the rest of this section, we ﬁrst consider binary trees, wi th or
without packet loss. Then we extend our algorithms to m-ary trees. For trees without loss, we
design deterministic algorithms that infer the topology in O(n) iterations. For trees with loss, just
one successfully received probe per network path is suﬃcien t, without the need to collect packet
loss statistics, a property that enables rapid discovery of the underlying topology.
5.1. Binary Trees
5.1.1. Lossless Binary Tree
Let us ﬁrst consider the simplest case: an undirected binary tree without packet loss. The
following example illustrates the main idea.
Example 1. Consider the tree shown in Fig. 1(a), with 7 leaves (1,2, ...7) and 5 intermediate nodes
(A,B,C,D,E). Assume that nodes 1, 7 act as sources S1, S 2 and send probes x1 = [1 , 0], x 2 = [0 , 1],
respectively. All other leaves act as receivers. Intermedia te node A receives x1 and forwards it to
leaf 2 and to node C. Similarly, node D receives x2 and forwards it to node E (which in turn
6Note that other mappings at the joining point, from ( x1, x 2) to f(x1, x 2), would achieve the same goal. Linear
network coding, f(x1, x 2) = x1 + x2 , is only one such mapping. Concatenation ( x1, x 2), for example, is another
mapping. This approach is, for example, used as data aggrega tion in [27], where a node waits to receive data from all
its children in the reverse multicast tree (or until a speciﬁ ed period of time has elapsed). The node then aggregates
all the data and forwards it to the sink via the reverse tree. H owever, the advantages of using network coding,
in particular, compared to these other mapping functions, i nclude simple linear operations and ﬁxed packet size.
Indeed, when we have more than two source packets meeting at a joining point, network coding provides an output
packet of ﬁxed size at the output, while concatenation provi des an output packet with output linear in the number
of incoming packets. The same advantage applies when we have the same probe packet meeting multiple times with
the other probe at a J (as it may be the case in DAGs), e.g., network coding results in 2 x2. In summary, although
it is possible to use other approaches for the same purpose, n etwork coding is the most eﬃcient way to do the task.
7
E
1 42 3
5 6
7
A B
D
C
(a) Undirected binary tree we
want to infer.
D
1 42 3
5
C
6 7
A B
(b) Structure revealed after 1
iteration. Leaves 1 and 7 act
as sources. Probes meet at C.
7
1 42 3
CA B
D
5 6
(c) Structure revealed after two
iterations. Leaves 5 and 6 act
as sources. Probes meet at E.
Figure 1: Example 1: inferring the topology of an undirected binary tree with 7 leaves and 5 intermediate nodes.
forwards it to leaves 5, 6) and to node C. Probe packets x1 and x2 arrive at node C, which adds
them, creates the packet x3 = x1 ⊕ x2 = [1 , 1], and forwards x3 to node B, which in turn forwards
it to leaves 3, 4.7
At the end, leaf 2 receives x1, leaves 5, 6 receive x2 and leaves 3, 4 receive x3 = x1 ⊕ x2. Thus,
the leaves of the tree can be partitioned into three sets: L1 containing S1 and the leaves that
received x1, i.e., L1 = {1, 2}; L2 = {5, 6, 7} containing S2 and the leaves that received x2; and
L3 = {3, 4} containing the leaves that received x1 ⊕ x2. From this information observed at the edge
of the network, we can deduce that the binary tree has the stru cture depicted in Fig. 1(b): three
components, each seeing a diﬀerent probe ( x1, x 2, x 1 ⊕x2) ﬂowing through it, and connected through
three links to the middle node C. This concludes the ﬁrst experiment/iteration.
To infer the structure that connects leaves {5, 6, 7} to node C, we need a second experiment. We
randomly choose two of these leaves, e.g., 5 , 6, to act as sources S1, S 2. Any probe packet leaving
node D will be multicast to all remaining leaves of the tree, i.e., nodes {1, 2, 3, 4} observe the same
packet. One can think of node D as a single “aggregate-receiver” that observes the common p acket
received at nodes {1, 2, 3, 4}. Following the same procedure as before, assuming that x1, x 2 meet at
node E, nodes 7 and {1, 2, 3, 4} receive x3 = x1 ⊕ x2. Using this additional information and the
fact that the topology is a binary tree, we reﬁne the inferred structure from Fig. 1(b) to Fig. 1(c).
Algorithm 1 generalizes the previous example and can infer a ny binary tree topology. It starts
by considering all the leaves L. It calls SendTwoProbes and partitions L into smaller sets L1, L2,
L3. It proceeds by recursively calling SendTwoProbes within each set, until all edges are revealed.
Lemma 5.1. Algorithm 1 terminates in at most n iterations and exactly infers the topology of an
undirected binary tree.
Proof. Consider a particular iteration (call of SendTwoProbes): sources S1 and S2 send exactly one
probe packet each to all other leaves. Now consider the inter mediate nodes on the path P between
the two sources. Depending on the link delays, there are two p ossibilities.
The ﬁrst possibility is that x1 and x2 meet (arrive within the same W ) at one of the intermediate
nodes on P, e.g., node A. Node A forwards their XOR to its third link, and the iteration reveals the
7We have chosen the directionality of the edges depending on w hich source reaches the intermediate node ﬁrst.
In this example, we assume that all links have the same delay. For diﬀerent delays, x1, x 2 could meet at diﬀerent
nodes, but the algorithm will still work, as discussed after Lemma 5.1.
8
Algorithm 1 Topology Inference for Lossless Binary Tree
1: E = ∅; /*Initially, we can only observe the leaves ( L); our goal is to reveal all the edges in the tree, i.e., set E.*/
2: InferBinaryTree(L):
3: ( L1, L2, L3, A 1, A 2, A 3)=SendTwoProbes(L);
4: for i ∈ { 1, 2, 3} do
5: if |Li| == 0 then
6: Continue;
7: else if |Li| == 1 || |L i| == 2 then
8: for v ∈ L i do
9: E = E ∪ {(v, A i)}; /*Connect the leaf nodes v in Li through node Ai to the rest of the network.*/
10: end for
11: else
12: /* |Li| > 2, i.e., Li contains three or more leaves.*/
13: InferBinaryTree(Li ∪ Ai); /*Node Ai that connects Li to the network acts as an aggregate receiver 8.*/
14: end if
15: end for
16: return
17: Replace vertices of degree two with a single node.
18: SendTwoProbes(L):
19: Randomly choose two leaves in L to act as sources S1, S2 and send probe packets x1, x2 respectively. All other
leaves L − { S1, S 2} act as receivers. Intermediate nodes act as branching or joi ning points.
20: When all receivers receive a probe, partition L into L1, L2, L3 as follows.
21: Set L1 contains S1 and all receivers that observe x1. Set L2 contains S2 and all receivers that observe x2. Set
L3 contains all receivers that observe x3 = x1 ⊕ x2.
22: if |L3| ̸= 0 then
23: Create new nodes A1, A 2, A 3, A ∗.
24: E = E ∪ {(A1, A ∗), (A2, A ∗), (A3, A ∗)};
25: /*This case is depicted in Fig. 2(a): L is divided into three components L1, L2, L3, connected through three
edges to nodes A1, A 2, A 3.*/
26: else
27: /* L3 = ∅*/
28: Create new nodes A1, A 2. Also, A3 = null.
29: E = E ∪ {(A1, A 2)};
30: /*This case is depicted in Fig. 2(b): L is divided into 2 components L1, L2, connected through a single edge.*/
31: end if
32: return the components L1, L2, L3 and the nodes A1, A 2, A 3.
neighboring edges and nodes to A as depicted in Fig. 2(a). Another possibility is that x1 and x2
cross each other while traversing the same link of P in opposite directions, i.e., they do not meet
at a node. Even if a leaf node receives more than one probe, we d esign their operation so that they
only keep the ﬁrst one. In this case, we infer the conﬁguratio n in Fig. 2(b) that reveals one edge.
In summary, the algorithm iteratively divides the binary tr ee into smaller components until one
component has two or less leaves, in which case we know its str ucture. In each iteration, we reveal
three edges or one edge. At the end, we have revealed all n − 1 edges. Therefore, the algorithm
requires between n−1
3 and n − 1 iterations.
Notes. In each iteration, every link is traversed exactly once by a p robe. Link delays aﬀect
where the probes meet and thus what components are revealed i n each iteration. However, they
do not aﬀect the correctness of the algorithm.
8Although we cannot directly observe Ai, whatever is received by Ai will be received by the leaves that are in L
but not in Li; thus acting as an “aggregate” receiver on their behalf.
9
2
A
L L
L
A 1
2
A 3
1 2
3
S
S
x
x
x
1
2
1
2
3 =x + x1
(a) Dividing L into 3 components.
A
L L
A 1
1 2
S
S
x
x
1
2
1
2
2
(b) Dividing L into 2 components
Figure 2: Edges and vertices of the graph, as revealed by a sin gle iteration (call of SendTwoProbes) in Algorithm 1.
The leaves L are partitioned into two or three groups, based on their obse rvations, x1, x 2, x 1 ⊕ x2.
5.1.2. Lossy Binary Tree
Packet loss may cause confusion when dividing the receivers into components. One solution is
to send multiple probes from the same two sources in each iter ation as we discuss next. However,
given packet loss and delay variability, this may result in p robes meeting at diﬀerent nodes in the
same iteration 9. This problem exists because we deal with undirected graphs , where a link may be
traversed in opposite directions by probes sent in the same i teration. It can be avoided by ﬁxing
the directionality of edges in each iteration. This can be ac hieved in a distributed way by the ﬁrst
packet arriving at each intermediate node. We modify the int ermediate node operations as follows.
Intermediate Node Operation: Each intermediate node keeps a table of its neighbors. In
each iteration, it marks these neighbors as source or sink ne ighbors. Once this marking is done,
it does not change for the duration of the iteration. The ﬁrst time during an iteration that an
intermediate node receives a probe, it waits for a window W to receive probes from other neighbors.
After this time W passes, the node marks all neighbors from which it received p ackets as sources
and all other neighbors as sinks. For the remaining duration of the iteration, the node accepts
packets only if they originate from its source neighbors. If the node receives a packet from one of
its source neighbors, it forwards it to all its sink neighbor s. If it receives more than one packet from
diﬀerent source neighbors, it linearly combines them, and fo rwards the result to its sink neighbors.
The node rejects probes coming from sinks, and does not forwa rd packets towards sources.
Alg. 2 presents the modiﬁcations required for Alg. 1 to be abl e to infer binary trees with lossy
links. The main diﬀerence is that in each iteration, each sour ce sends M instead of one probes.
Performance: Alg. 2 has an associated probability of error, since a leaf mi ght not receive the
“correct” probe packet 10. For our algorithm to operate correctly, it suﬃces that each receiver
receives at least one packet from each of the sources it is connected to. Nodes in L1 or L2 are
connected to one source ( S1 or S2), while nodes in L3 to two sources. In general, the number
of probes M required per iteration in order to have one “success” is a ran dom variable that
depends on the topology. For general trees, the distributio n of this random variable is diﬃcult
to characterize precisely, but upper bounds can be provided . In particular, we need every path
(from each source to each receiver) to work at least once. Diﬀe rent paths have diﬀerent probability
of success depending on their length ( lP ) and the probability of loss on every link across the
9This was not a problem in the lossless case. In a given iterati on, since only one probe packet is generated by
each source, the packets at most meet at one intermediate nod e.
10In a given iteration, an error may occur either because a leaf does not receive any packet (which can be made
arbitrarily small by increasing the number of probes M) or, because it belongs to L3 but happens to receive only x1
or only x2 packets. This probability decreases very fast as M increases, as observed in the simulations of Section 7.
10
Algorithm 2 Topology Inference for Lossy Binary Tree. We only describe t he SendTwoProbes
function below since the ﬁrst part (which contained the InferBinaryTree procedure) is similar
to Algorithm 1. SendTwoProbes is diﬀerent from Algorithm 1 in that: (i) each source sends M,
instead of one, probe packets; and (ii) the rules to divide L into three components change as follows.
1: SendTwoProbes(L):
2: Randomly choose two leaves in L to act as S1, S2. The sources transmit, for M times, probe packets x1, x2,
respectively. All other leaves L − {S1, S 2} act as receivers. Intermediate nodes act as branching or joi ning points.
3: /*Partition L into L1, L2, L3 as follows.*/
4: for each receiver j do
5: Let Oj be the set of all observations of receiver j. /*We consider the union of observations for each receiver. */
6: /*For aggregate receiver Ai, we apply the same rule using the union of the aggregate recei ver observations.*/
7: if Oj contains only x1 then
8: Assign receiver j to the set L1.
9: else if Oj contains only x2 then
10: Assign receiver j to the set L2.
11: else if Oj contains both x1 and x2, or it contains an x1 ⊕ x2 packet then
12: Assign receiver j to the set L3.
13: else
14: /* Oj = ∅*/
15: Randomly assign receiver j to one of the components.
16: end if
17: end for
18: if |L3| ̸= 0 then
19: Create new nodes A1, A 2, A 3, A ∗.
20: E = E ∪ {(A1, A ∗), (A2, A ∗), (A3, A ∗)};
21: /*This case is depicted in Fig. 2(a): L is divided into three components L1, L2, L3, connected through three
edges to nodes A1, A 2, A 3.*/
22: else
23: /* L3 = ∅*/
24: Create new nodes A1, A 2. Also, A3 = null.
25: E = E ∪ {(A1, A 2)};
26: /*This case is depicted in Fig. 2(b): L is divided into 2 components L1, L2, connected through a single edge.*/
27: end if
28: return the components L1, L2, L3 and the nodes A1, A 2, A 3.
path ( pi for link i): P = (1 − p1) · · · (1 − plP ). Let Pmin be the minimum probability of success
across all paths. Then M is a geometric random variable with success probability Pmin. Therefore,
E[M] = 1 /P min, var[M] = (1 −Pmin)/P 2
min, and P r(|M −E[M]| ≥ m) ≤ var[M]/m 2. An example
computation for the exact probability of error in a speciﬁc t opology (tree of Fig. 1(a)) is provided
in Appendix A. Note that in general, M is much smaller compared to the methods that collect a
statistically signiﬁcant number of packets and perform est imation.
5.2. M-ary Trees
5.2.1. Full M-ary Trees
We ﬁrst consider full m-ary trees, where all intermediate nodes have degree m + 1, m ≥ 3,
without packet loss. Alg. 1 can still accurately infer the to pology in less than n iterations. However,
we can modify the algorithm to infer the topology even faster . The idea is to keep the hierarchical
clustering approach but increase the number of components r evealed in each iteration, either (i)
by changing the intermediate nodes so that they forward diﬀer ent linear combinations of incoming
probes to diﬀerent outgoing links; or (ii) by increasing the n umber of sources in each iteration.
Modiﬁcation I: (two sources per iteration, coding points send diﬀerent comb inations to diﬀer-
ent links). When an intermediate node receives two incoming packets from two diﬀerent neighbors,
11
it deterministically generates diﬀerent linear combinatio ns, e.g., x1 + x2, x 1 + 2x2, · · · and forwards
each resulting packet to a diﬀerent neighbor. Therefore, whe n x1 and x2 meet at any intermediate
node, the leaves of the network will be divided into m + 1 components, depending on which probe
packet they receive. If the probe packets do not meet at a node but cross each other, the leaves of
the network will be divided into two components. Once a compo nent has m or less leaves, since
we have a full m-ary tree, we know its structure. Therefore, in each iterati on, we reveal m + 1
edges or one edge, and the total number of iterations is reduc ed to at least n−1
m+1 and at most n − 1
iterations. Note that the operations are performed over Fm2 in this case.
Modiﬁcation II: (more than two sources per iteration, coding points send the same com-
bination to all outgoing links). Alternatively, we can use u p to m sources (as per Lemma 5.2)
per iteration. The sources send x1 = [1 , 0, 0, · · · , 0], x 2 = [0 , 1, 0, · · · , 0], · · · , x m = [0 , 0, 0, · · · , 1],
respectively. When an intermediate node receives k packets from diﬀerent neighbors within W ,
2 ≤ k ≤ m, it simply adds them up (over F2m ) and forwards the result to all remaining neighbors.
Depending on whether the node receives k packets or only a single packet, the leaves of the network
will be divided into m + 1 or m more components; i.e., in each iteration, we reveal m + 1 or m
edges. Therefore, the algorithm requires at least n−1
m+1 and at most n−1
m iterations.
Lemma 5.2. The maximum number of sources that can be used to uniquely infe r the topology of
a full m-ary tree is m.
Proof. We show that if we use m + 1 sources to infer the topology of a full m-ary tree, it cannot be
uniquely identiﬁed. For example, consider a binary tree wit h three sources sending x1 = [1 , 0, 0],
x2 = [0 , 1, 0], and x3 = [0 , 0, 1] respectively, to all other leaves in the tree. Assume that the three
probe packets meet at one intermediate node; thus, we divide the leaves into four components,
which observe x1, x2, x3, and x1 + x2 + x3 = [1 , 1, 1] respectively. Since the degree of intermediate
nodes is three, we conclude that two of the three sources must have joined at one intermediate node
ﬁrst, and then their result must have joined with the third so urce in another intermediate node, so
that they result in x1 + x2 + x3 in the last component. The ﬁrst two sources can be either x1, x 2
or x2, x 3. Therefore, we cannot uniquely infer the underlying binary tree topology by observing
these four components. The same discussion applies to large r full m-ary trees ( m > 2).
Note. In the presence of loss, the same argument as in Section 5.1.2 applies, i.e., we can assign
directions to edges in each iteration, so that our algorithm s are applicable to the lossy case as well.
5.2.2. General M-ary Trees
In general m-ary trees, the degree of intermediate nodes varies from thr ee up to a maximum of
m + 1. We can still apply Alg. 1 and infer the tree topology in O(n) iterations. We can also apply
Modiﬁcation I described in Section 5.2.1; the operations ar e still performed over Fm2 since probe
packets may meet at an intermediate node of degree m+1. However, we cannot apply Modiﬁcation
II here: since probe packets may meet at an intermediate node of degree three, we cannot use more
than two sources according to Lemma 5.2, although there exis t larger degree nodes in the tree.
6. Inferring Directed Acyclic Graphs (DAGs)
6.1. From a Single-Tree to Multiple-Tree Topologies
So far, we considered undirected trees. Let us now consider d irected trees, which are a special
case of DAGs.
12
S1 S2 
J 
B 
R2 R1 
J1=J 2= 
B1=B 2= 
(a) type 1: shared
S1 
B2 
J1 
J2 
B1 
R2 R1 
S2 
(b) type 2: non-shared
S1 
J2 
B1 
J1 
B2 
R2 R1 
S2 
(c) type 3: non-shared
S1 
J2 
B2 
J1 
B1 
R2 R1 
S2 
(d) type 4: non-shared
Figure 3: The four possible types of a 2-by-2 subnetwork comp onent, as deﬁned in [36]. There are two sources
(S1, S 2) multicasting packets x1, x 2 to two receivers ( R1, R 2). (The 1-by-2 topology of S1 is a tree composed of
S1, B 1, R 1, R 2. Similarly, The 1-by-2 tree rooted at S2 is S2, B 2, R 1, R 2. J1 and J2 are the joining points, where the
paths from S2 to R1 and R2, join/merge with S1’s topology.)
Example 2. Assume that we assign directions to the links of the binary tr ee in Fig. 1(a), all
from the top to the bottom. Clearly, we can no longer send probe packets in arbitrary directions
in each iteration. However, we can still infer some informat ion about the topology. Assume that
we send probes from the source nodes 1 and 2, and we observe x1 ⊕ x2 at the receiver nodes 5, 6,
and 7. Therefore, we identify three components L1 = {1}, L2 = {2}, and L3 = {5, 6, 7}, together
with the intermediate nodes A and D, and three edges 1A, 2A, and AD, which connect the three
components together. However, we cannot obtain more inform ation about the internal structure of
the component L3 or any other part of the tree network.
Next, consider a 2-by-2 network as deﬁned in Section 3, i.e., a directed acyclic graph with two
sources, two receivers and predetermined routing. Note tha t directed trees are only one type among
all four possible types of the basic 2-by-2 components of any multiple-tree network, as deﬁned in
Section 3. There exist four 2-by-2 topologies, as shown in Fi g. 3, which were ﬁrst deﬁned in [36, 37].
Following the same terminology as in [36, 37], we refer to Fig . 3(a), (b), (c) and (d) as type 1, 2,
3 and 4, respectively. Type 1 is called shared [36, 37] since the joining points for both receivers
coincide ( J1 = J2) and the branching points for both sources coincide ( B1 = B2). The other three
types (types 2, 3 and 4) are called non-shared since they have two distinct joining points and two
distinct branching points.
In a directed tree, all 2-by-2 components are of type 1. Howev er, in a general M-by-N topology,
several diﬀerent 2-by-2 types may co-exist. The algorithms d escribed so far can identify type 1
2-by-2 topologies, and thus, trees (either completely or pa rtially, as described above). However,
they cannot distinguish between type 1 and type 4 2-by-2’s, a s described in the following example.
Example 3. Consider Fig. 3 (a) and (d). Assume that in both cases, we send x1, x 2 from S1, S 2
to R1, R 2 and that x1, x 2 meet (arrive within the same W ) at any joining point. Therefore, in both
type 1 and type 4, both receivers observe x1 + x2, and we cannot distinguish between the two types.
In general, unlike single-tree networks, the observations do not uniquely characterize the un-
derlying topology in multiple-tree networks. The reason is that once two sources in a tree network
transmit their probe packets, they at most meet at one coding point for all the receivers, as we saw
in Section 5. On the other hand, in a multiple-tree network, p robe packets may meet at diﬀerent
coding points for diﬀerent receivers, as depicted in Fig. 4. T herefore, we need a diﬀerent approach.
Problem Statement. Our goal in this section is to infer a multiple-tree topology , or an “M-
by-N” topology according to the terminology of Section 3. Si milarly to [36], we take two steps. In
13
S1 
x1 x2 
S2 
x1+x 2 x1+x 2 
R1 R2 
2 sources meet only once  
(for all the receivers) 
Figure 4: Single-tree vs. multiple-tree topologies. Consi der a single iteration. In a multiple-tree topology, unlike the
single-tree topology, the observations at the receivers no longer uniquely identify the topology.
the ﬁrst step (Section 6.2), we use several experiments and w e exactly identify the type of every
2-by-2 component. In the second step (Section 6.3), we merge these 2-by-2 subnetwork components
to reconstruct the M-by-N network.
Operation of Sources. Pairs of sources are selected and send up to countMax coordinated
multicast packets to all receivers. As in the general setup, probes are spaced apart by intervals of
length T . In addition, we introduce a diﬀerence in the sending time of t he two sources, which we
call the oﬀset u. W.l.o.g., let S1 send ﬁrst and S2 second.
The timing parameters T, u, W are coarsely tuned so as to create observations that can dist in-
guish among diﬀerent 2-by-2 types. In particular, (i) T >> W ensures that only probes within
the same experiment are coded together. To be more precise, w e choose T ≥ g · W , where g is the
maximum number of joining points on any ( Si, R j ) path in the topology. In the worst case, there
can be N joining points in a row and thus, g ≤ N. However, in practice, g is usually a lot smaller.
(ii) W >> path delay (between the sources and the joining points) ensu res that source packets
meet at the joining points despite link delays. (iii) u is selected randomly in each iteration, so that
it forces probes to meet at diﬀerent points, or not meet at all, in diﬀerent iterations. Finally, coarse
selection of T, W with rough estimates of upper bounds on link and path delays i s suﬃcient.
Operation of Receivers. For a given 2-by-2 subnetwork, let the observations at the tw o
receivers be R1 = c11x1 + c12x2, R2 = c21x1 + c22x2. Based on these observations, we design
Inference algorithms that identify the 2-by-2 type (in Section 6.2) an d Merging algorithms that
build the M-by-N from the 2-by-2’s (in Section 6.3).
Operation of Intermediate Nodes. In DAGs, the operation of an intermediate node,
depending on whether it acts as a joining point or a branching point, is summarized in Alg. 3
and Alg. 4, respectively. A joining point (J) adds and forwar ds packets, while a branching point
(B) forwards the single received packet to all “interested” links downstream. A link is “interested”
in the routing sense if it is the next hop for at least one sourc e packet in the network coded packet.
6.2. Identifying 2-by-2 Components
In this section, we propose an approach to exactly identify a 2-by-2 component, using the same
intuition as in trees, i.e., coding operations result in observations that can uniquely characterize
the underlying 2-by-2. Our approach builds on [36] and impro ves over it by uniquely distinguishing
among all four 2-by-2 types, while [36] could only distingui sh between shared and non-shared types.
6.2.1. Lossless 2-by-2
First, we provide an algorithm to identify the type of a 2-by- 2 component without packet loss.
In the ﬁrst experiment, sources S1, S 2 multicast probe packets x1, x 2 to R1, R 2. We begin with
the assumption that S1, S 2 act simultaneously, or in practice within the synchronizat ion oﬀset. A
14
Algorithm 3 Operation at Joining Point J, in DAGs. When two sources multicast to N receivers, J has
two incoming links and one outgoing link. Additions are perf ormed over Fq.
1: for every time window W do
2: if (J receives two packets within W from its incoming links) then
3: as soon as the last packet arrives, it adds them up, and forw ards the resulting packet downstream.
4: else if (J receives only one packet within W ) then
5: it forwards the packet downstream.
6: else if (J does not receive any packet within W ) then
7: /*nothing to do.*/
8: end if
9: end for
Algorithm 4 Operation at Branching Point B, in DAGs. While two sources multicast to N receivers, B
has one incoming packet and multiple outgoing links.
1: for each incoming packet do
2: if the incoming packet is x1 (or x2) then
3: forward it only on the outgoing links that are next hops for S1 (S2).
4: else
5: /* The incoming packet is of the form ax1 + bx2. */
6: forward the packet to all outgoing links.
7: end if
8: end for
choice of large W guarantees that x1, x 2 meet at both joining points J1, J 2, which add the incoming
probes over F3. Depending on the underlying 2-by-2 type, R1, R 2 observe one of the following pairs:
• type 1: R1: x1 + x2 , R2: x1 + x2
• type 2: R1: x1 + x2 , R2: x1 + 2x2
• type 3: R1: x1 + 2x2 , R2: x1 + x2
• type 4: R1: x1 + x2 , R2: x1 + x2
Types 2 and 3 result in unique observations that make them dis tinguishable from any other
type; i.e., one such observation suﬃces to identify type 2 or type 3. Howe ver, types 1 and 4
result in the same pair of observations; therefore, we need t o design diﬀerent experiments to get
observations that can uniquely characterize type 1 or type 4 .
In the next experiment, we exploit the observation, ﬁrst mad e in [36], that type 1 is the only
2-by-2 where the two joining points coincide ( J1 = J2 = J). Therefore, the observations at the two
receivers are always the same: either x1 + x2 when the two packets meet at J; or a single packet
(x1 or x2) when the two packets do not meet at J. In contrast, type 4 has two diﬀerent joining
points J1 ̸= J2. If we force packets to meet only at one of the joining points b ut not at the other
one, the receivers will have diﬀerent observations. These ar e observations #3 and #4 in Table 1
and they can uniquely characterize type 4.
These observations can be achieved by appropriately select ing the oﬀset u in the sources’ sending
times. u needs to be large enough so that after addition to the link del ays, it can aﬀect W : if D1, D 2
represent the delays on the paths from S2 to J1, J 2, respectively, u must be in [ W −D1, W −D2]11.12
Alg. 5 summarizes the experiments we perform in order to infe r the type of a 2-by-2 network.
Types 2 and 3 are identiﬁed in the ﬁrst observation. Type 4 is i dentiﬁed the ﬁrst time that the two
11 In 2-by-2 components, this interval is close to W , since D1 and D2 are small compared to W . In more general
2-by-N networks that we consider for our simulations, there exist multiple links between the sources and the joining
points, link delays are on the order of tens of ms, and W is on the order of hundreds of ms. Therefore, we can
safely choose u ∈ [f · W, W ] in the general case, where 0 < f < 1 is a tunable parameter. We choose f = 0 . 7 in our
15
Table 1: Lossless Case. Possible observations for types 1 and 4 2-by-2 topologies. ( Observation #1 occurs when
the sources are synchronized. Observations #2-4 occur when S2 sends after S1, by an oﬀset u ∈ [f · W, W ].)
Observation Type 1 Type 4
Number R1 R2 R1 R2
1 x1 + x2 x1 + x2 x1 + x2 x1 + x2
2 x1 x1 x1 x1
3 x1 + x2 x1
4 x1 x1 + x2
Algorithm 5 Lossless Case - Inferring a 2-by-2 component. Sources S1 and S2 multicast x1 and x2.
Receivers observe R1 = c11x1 + c12x2 and R2 = c21x1 + c22x2.
1: n=1; /*ﬁrst experiment*/
2: if c22 > c 12 then
3: Output type 2;
4: else if c22 < c 12 then
5: Output type 3;
6: else
7: /*It is R1 = R2*/
8: while n < countMax & R1 == R2 do
9: Draw oﬀset u uniformly at random out of [ f · W, W ].
10: Send probes; S2 transmits u time later than S1.
11: if R1 ̸= R2 then
12: Output type 4;
13: Exit;
14: end if
15: n++;
16: end while
17: Output type 1; /* It is n == countMax*/
18: end if
receivers see diﬀerent observations. If after countMax trials, we still have not seen any diﬀerent
observations at the two receivers, then we declare the 2-by- 2 to be of type 1.
Choosing countMax. countMax should be large enough to ensure small probability of error.
The probability of error of Alg. 5 can be computed as follows. Let X = I{R1 = R2} indicate
whether the two observations are the same or not; it is a Berno ulli random variable with success
probability P r{R1 = R2}. The number of required experiments is a geometric random va riable.
The only possible error is to mistakenly declare type 4 as typ e 1, which happens with probability:
P r(error) = 1 −P r(type = 1 |X1 = 1 , · · · , X countMax = 1) = 1 − 1
1 + ( P r(X = 1 |type = 4)) countMax
(1)
In type 4, X = 1 occurs when u / ∈ [W − D1, W − D2], i.e., with probability 1 − |D1−D2|
(1−f)·W . Thus,
Alg. 5 identiﬁes any 2-by-2 topology in countMax experiments with the following error probability:
P r(error) = 1 − 1
1 + (1 − |D1−D2|
(1−f)·W )countMax
(2)
We can then ﬁnd countMax by replacing the appropriate values [39]. One can calculate that in
order to ensure an accuracy of 99% in distinguishing between types 1 and 4 2-by-2’s, countMax
simulations, to force diﬀerent observations at the two rece ivers.
12In fact, we can obtain similar observations without using th e oﬀset u, but instead, by changing W in a range of
values, from the maximum path delay (as it is now), down to 0. O ne can check that Alg. 5 (without u) can still be
applied in this case. Therefore, using the oﬀset is not reall y crucial in our scheme.
16
Table 2: Lossy Case. Possible observations for all four types of 2-by-2 topolog ies. (Sources send synchronized and
W is large. Observation #13 for types 2, 3 occurs only when S2 sends with oﬀset u ∈ [f · W, W ] after S1.) We divide
the observations into 3 groups: (i) at least one receiver doe s not receive any packet, (ii) R1 = R2 and (iii) R1 ̸= R2.
Type 1 Type 2 Type 3 Type 4
# grp R1 R2 grp R1 R2 grp R1 R2 grp R1 R2
1 i - - i - - i - - i - -
2 - x1 + x2 - x1 + 2x2 x1 + 2x2 - - x1 + x2
3 - x1 - x1 + x2 x1 + x2 - - x1
4 - x2 - x1 x1 - - x2
5 x1 + x2 - - x2 x2 - x1 + x2 -
6 x1 - x1 + x2 - - x1 + x2 x1 -
7 x2 - x1 - - x1 x2 -
8 ii x1 + x2 x1 + x2 x2 - - x2 ii x1 + x2 x1 + x2
9 x1 x1 ii x1 + x2 x1 + x2 ii x1 + x2 x1 + x2 x1 x1
10 x2 x2 x1 x1 x1 x1 x2 x2
11 x2 x2 x2 x2 iii x1 x1 + x2
12 iii x1 + x2 x1 + 2x2 iii x1 + 2x2 x1 + x2 x1 + x2 x1
13 x1 x1 + x2 x1 + x2 x1 x1 x2
14 x1 x2 x2 x1 x2 x1
15 x1 + x2 x2 x2 x1 + x2 x1 + x2 x2
16 x2 x1 + x2
needs to be ∼ 450. However, this is a pessimistic upper bound: simulation results in Section 7
show that a much smaller countMax is suﬃcient in practice.
6.2.2. Lossy 2-by-2
Let us now consider a 2-by-2 network where packets may be lost on some links. In this case, we
can no longer guarantee meetings of x1 and x2 at the joining points and predictable observations
at the receivers. There are two diﬀerences from the lossless c ase. First, because of random packet
loss, each experiment might result in diﬀerent outcomes, sho wn in Table 2. Second, there are
common observations across all four types, as opposed to jus t between types 1 and 4. We divide
the observations in Table 2 into three groups: (i) at least on e of the receivers does not receive any
packet (“-”) due to loss, (ii) both receivers have the same ob servation R1 = R2, and (iii) the two
receivers have diﬀerent observations R1 ̸= R2.
We choose to ignore the observations of group (i) because they can occur in any of the four
2-by-2 types and thus, they do not help to distinguish among 2 -by-2’s in the deterministic way
adopted in this paper. Observations of group (ii) can also be the result of any 2-by-2 type: unlike
the lossless case, where R1 = R2 is unique to type 1 or 4 topologies, any of the four topologies
may result in such observations if some packets are lost. We o bserve that group (ii) are the only
possibility for type 1 topology, apart from the group (i) tha t we ignore, while all other three 2-by-2
types may result in either R1 = R2 or R1 ̸= R2. Therefore, if after countMax trials, we only have
observations of group (ii), we declare the topology to be typ e 1.
In observations of group (iii) , it is R1 ̸= R2, which means that c12 ̸= c22 and/or c11 ̸= c21. An
important observation is that the diﬀerence of the coeﬃcient s between the two receivers contains
topology-related information. W.l.o.g., we focus on the co eﬃcient of x2 and look at the diﬀerence
c12 − c22. Table 2 shows that c12 − c22 < 0 can only occur in type 2 or type 4 topologies; while
c12 − c22 > 0 can only occur in a type 3 or 4 topology. Note that the coeﬃcie nt is larger on
one side ( e.g., c12 > c 22) when the probe ( x2) goes through two joining points on its way to one
receiver (in this case, R1) and through one joining point on its way to the other receive r ( R2). By
performing several independent experiments and collectin g several observations of group (iii), we
17
Algorithm 6 Lossy Case - Inferring a 2-by-2 component.Sources S1 and S2 multicast x1 and x2, respec-
tively. Receivers observe R1 = c11x1 + c12x2 and R2 = c21x1 + c22x2. The variable type stores our estimate of the
type of the 2-by-2 component and it gets updated during the ex periments.
1: n = 1; /*ﬁrst experiment*/
2: type=0; /*initialization*/
3: while n ≤ countMax do
4: if R1 ̸= [0 , 0] & R2 ̸= [0 , 0] then
5: if c22 > c 12 then
6: if type ̸= 3 then
7: type=2;
8: else
9: type=4; Break;
10: end if
11: else if c22 < c 12 then
12: if type ̸= 2 then
13: type=3;
14: else
15: type=4; Break;
16: end if
17: else if type == 0 & R1 == R2 then
18: type=1;
19: end if
20: end if
21: n++;
22: Draw oﬀset u uniformly at random out of [ f · W, W ].
23: Send probes; S2 transmits u time later than S1.
24: end while
25: Output type.
can distinguish among the candidate topologies. If after countMax experiments, there are only
observations of group (ii) or (iii) with c12 − c22 ≤ 0, we declare the topology as type 2. If there
are only observations of group (ii) or (iii) with c12 − c22 ≥ 0, we declare it as type 3. If there are
observations of group (ii) or (iii) with both c12 − c22 < 0 and c12 − c22 > 0, we declare it as type 4.
In our experiments, we try to create those observations that reveal the topology. These can
occur either naturally, as the result of packet loss, or arti ﬁcially, by us introducing an oﬀset u in
S2’s sending time with respect to S1. To help these observations occur, especially for small los s
rates, and similarly to the lossless case, we use a random oﬀse t u ∈ [f · W, W ]. To make these
experiments independent, we space apart successive sets of probes by roughly selecting T ≥ 3W ,
which is suﬃcient since there are at most two joining points o n any ( Si, R j ) path in a 2-by-2.
Alg. 6 summarizes the 2-by-2 inference for lossy networks. T he algorithm is simple and fol-
lows a deterministic approach: one observation, or a set of o bservations, is suﬃcient to uniquely
distinguish among types. For example, at least one observat ion of group (iii) rules out the type 1
topology; a pair of group (iii) observations with both c12 −c22 > 0 and c12 −c22 < 0 indicates type 4;
etc. As a result, we require less experiments compared to tho usands of arrival order measurements
required by [36, 37] for statistical signiﬁcance. In additi on and more importantly, we identify the
exact 2-by-2 type, while [36] was only able to distinguish be tween shared and non-shared types.
The following Lemma describes the probability of error of Al g. 6 with respect to the number of
experiments ( countMax) more precisely.
Lemma 6.1. Alg. 6 identiﬁes any 2-by-2 topology with P r(error) ≤ ∑ 2
i=1(1 − ρ′
i)countMax in
countMax experiments, where ρ′
i = (1 − p)6(p + (1 − p)γi)(p + (1 − p)
γi′ ), i, i ′ = 1 , 2, i ̸= i′, p is
the link loss rate (same for all links), and γi is the probability that probe packet x2 arrives within
W at Ji in a type 4 topology, i.e., γi = P r(u + Di ≤ W ), i = 1 , 2.
18
The proof is provided in Appendix B.
6.2.3. Inferring all 2-by-2’s in a 2-by-N Network
Algorithms 5 and 6 can be directly applied to a 2-by-N network , where two sources multicast
to N receivers. A diﬀerence is that intermediate nodes need to per form addition over a larger ﬁnite
ﬁeld, of order larger than the maximum number of joining poin ts on a path (g), since a packet may
meet itself at all the joining points on the path. In the worst case, there can be N joining points
in a row and thus, the maximum required ﬁeld size is the ﬁrst pr ime greater than N. Algorithm
5 and Algorithm 6 can be performed on any pair of receivers amo ng all
( N
2
)
possible pairs. The
same set of 2-by-N probes can be used to infer, in parallel and independently, the type of all 2-by-2
topologies. This reduces the number of probes, as we re-use t hem, instead of sending
( N
2
)
diﬀerent
sets of probes. The 2-by-N structure is important for the mer ging algorithm in Section 6.3.
6.2.4. 2-by-2’s vs. other Subnetwork Components
We now discuss why we choose to decompose an M-by-N network in to 2-by-2 subnetwork
components, as opposed to any other subnetwork structures m − by − n, 1 ≤ m ≤ M, 1 ≤ n ≤ N:
• 1-by-1: This is the smallest component and corresponds to me asuring a single end-to-end
path. However, it reveals neither joining nor branching poi nts.
• 1-by-2 and 2-by-1: These correspond to a 2-leaf multicast or a reverse-multicast tree, respec-
tively. The 2-by-1 consists of 2 sources, one coding point, a nd 1 receiver. The 2-by-1 cannot
identify the branching points while the 1-by-2 cannot ident ify the joining points. Similar
comments apply to M-by-1 and 1-by-N.
• 2-by-2: This is the smallest structure that gives informati on about the relative locations of
joining and branching points.
• m-by-n, with 2 < m < M, 2 < n < N : If we consider larger structures, there is an exponen-
tially larger number of possible types, which requires more complicated inference algorithms.
For example, there exist 19 possible types for a 2-by-3 struc ture.
• M-by-N: In the extreme case, we need to enumerate all possibl e M-by-N topologies as in [42].
The larger the subnetwork component we use as a building bloc k, the less components we need to
infer and the simpler the merging algorithm. However, as the size of the basic component grows,
the number of possible types increases exponentially and th e inference step becomes increasingly
complex. In this paper, we choose to decompose an M-by-N into 2-by-2 components, inspired by
the approach in [36]. We note that 2-by-2 is the minimum size b uilding block required to infer
both joining and branching points and strikes a good tradeoﬀ of inference vs. merging complexity.
6.3. Merging Algorithm
Assuming knowledge of all 2-by-2 subnetwork components, fr om Section 6.2, we now merge
them together to reconstruct the M-by-N network. We study me rging in two diﬀerent scenarios:
(i) when a 1-by-N tree topology is known, which is the same pro blem studied in [36]; and (ii)
without knowledge of any 1-by-N, which is new to our work. Exp loiting the accurately identiﬁed
2-by-2’s, we can solve (i) exactly, which was previously onl y approximately solved; and also solve
(ii), which was previously not known how to address.
More precisely, our merging algorithm can identify every jo ining point, in the sense that it can
localize it between two branching points. However, note tha t when there are several joining points
in a row, without any branching point in between, it is not pos sible to identify the relative locations
of these joining points with respect to each other. In fact, t his is the case in a tree topology.
19
Algorithm 7 Merging Algorithm: Given the two sources S1, S 2, a set of receivers R1, R 2, · · · , R N , the 1-by-N
S1 tree topology, and the 2-by-2 results from Alg. 6 for any pair of receivers Ri, R j , this algorithm identiﬁes a single
link for the location of every Ji (the joining point for Ri), on S1 topology.
1: for each receiver Ri do
2: if ∃ k < i such that the S1, S 2, R k, R i 2-by-2 is shared then
3: Ji = Jk;
4: else
5: Let B be the closest branching point to Ri.
6: while Ji is not localized to a single link do
7: Let Rj be any child of B (j ̸= i).
8: Based on the type of the 2-by-2 component S1, S 2, R i, R j , locate Ji above/below B.
9: if (Ji is below B) || ((Ji is above B) && ( ∄ other branching point above B on S1’s 1-by-N)) then
10: Ji is localized to a single link.
11: Output this link; Break;
12: else
13: B = the next upstream branching point.
14: end if
15: end while
16: end if
17: end for
6.3.1. Merging a 1-by-N and 2-by-2’s into a 2-by-N
In this section, we assume that the 1-by-N from S1 to N receivers is known using either the
classic methods for single-tree topology inference [6] or o ur algorithms in Section 5 for tree networks.
This 1-by-N is a tree rooted at S1 and contains only branching points. We also assume that the
2-by-2’s between S1, a new source S2, and any pair of receivers are known, using the algorithms of
Section 6.2. Our goal is to locate the joining points where pa ths from S2 to the same N receivers
join S1’s topology. We use the assumptions of Section 3 for routing.
This problem was posed in [13, 36] and was solved there in an ap proximate way. Bounds on the
joining point locations in the S1 topology were provided within a sequence of consecutive log ical
links. This was because the 2-by-2’s are only identiﬁed as sh ared or non-shared types in [36, 37].
In contrast, we design Algorithm 7, which localizes each joi ning point for each receiver to a single
logical link, between two branching points in the S1 topology. Our algorithm is simpler, faster,
and more accurate: it can identify all joining points for any topology and with lower complexity,
thanks to our complete knowledge of the 2-by-2 types.
Example 4. Fig. 7(a) depicts a 2-by-9 topology constructed based on the Abilene network [51].
Consider R1: it forms a type 1 2-by-2 with R2. Therefore, J1 must lie above B1,2, so that there
exists a unique path from each source to R1. We then need to localize J1 with respect to B1,3: R1,R3
form a 2-by-2 of type 4; thus, J1 must lie below B1,3. J1 is now localized to one link (between B1,2
and B1,3), and the algorithm ends here for R1. Other receivers are considered similarly. Note that
a joining point can be placed on any link from the receiver to S1. Therefore, the number of steps
required to localize a joining point is at most equal to the he ight of the S1 tree. Also, when there is a
group of receivers within which all pairs are of type 1, the al gorithm is run only once and it assigns
the same joining point to all of them. For this example, the al gorithm in [36] cannot completely
resolve all joining points, and provides bounds within a seq uence of several logical links instead.
6.3.2. Merging 2-by-2’s into a 2-by-N
In this section, we infer a 2-by-N without prior knowledge of any 1-by-N. Inference under this
relaxed assumption is enabled by our exact knowledge of 2-by -2’s and was not possible before
[13, 36]. We ﬁrst send probes over the 2-by-N and then merge al l
( N
2
)
2-by-2’s, as described next.
20
Example 5. We ﬁrst consider all shared (type 1) 2-by-2 components and as sign them the minimum
number of branching and joining points required. For exampl e in Fig. 7(a), B1,2, B 3,4 and J1 =
J2, J 3 = J4 = J5 = J6 = J7 = J8 = J9 are identiﬁed in this step. Second, we consider all non-shar ed
2-by-2 topologies (of type 2, 3, or 4). We use the information about the locations of the branching
and joining points in each type to: (i) add the minimum number of branching points required to the
ones already identiﬁed from the shared pairs; and (ii) assig n joining points to those receivers that
have not been already assigned one. In the example of Fig. 7(a) , an additional branching point B1,3
is required, which is connected to both joining points J1 = J2 and J3 = J4 = J5 = J6 = J7 = J8 =
J9, to satisfy the 2-by-2’s of type 4 between the two shared grou ps. No additional joining point is
required in this example.
This approach identiﬁes the locations of all joining points , between the S1 and S2 1-by-N
topologies, but it does not identify all the branching point s in the S1 tree topology. Only the
“minimum” S1 topology is identiﬁed, i.e., the tree made by the “necessary” branching points. We
deﬁne as “necessary” branching points the ones located belo w a joining point of S1 and S2 in the
2-by-N. An “unnecessary” branching point is the child of ano ther branching point with no joining
point in between. For example in Fig. 7(a), this approach doe s not identify B4,5, B 6,7, B 6,9, and
directly connects their children ( R4, R 5, R 6, R 7, R 8, R 9) to the upstream branching point ( B3,4).
Note that the worst case input for this approach is a tree netw ork. Since all 2-by-2’s are of
type 1, and the algorithm cannot reconstruct branching poin ts in a row, it can only identify the
top-most branching point of the entire tree structure.
6.3.3. From 2-by-N to M-by-N
We can directly extend the 2-by-N inference techniques to th e M-by-N case [13]. We start from
a 2-by-N topology, and add one source at a time, to connect the 1-by-N’s of the remaining M − 2
sources. Assume that we have constructed a k-by-N topology, 2 ≤ k < M . To add the ( k + 1)th
source, we perform k experiments, where at each experiment one diﬀerent of the k sources and the
(k +1)th source send x1 and x2. We then glue these topologies together by following the topological
rules of Section 6.3.1 (with single-source trees given) or S ection 6.3.2 (without that assumption).
6.3.4. Complexity of Merging
Lemma 6.2. If one source’s 1-by-N tree topology is given, the minimum num ber of 2-by-2’s required
by any merging algorithm to uniquely localize all the joinin g points (between two branching points)
in the 2-by-N topology is N
2 .
Proof. One can think of checking the types of the 2-by-2 components i n the following sense: we
divide the N receivers in the network into two sets of vertices, in a bipar tite graph, and we draw
an edge between any two vertices for which we check the 2-by-2 type. The minimum number of
required 2-by-2’s is then given by a perfect matching in this bipartite graph; therefore, it is N
2 .
Example 6. Fig. 5 shows two 2-by-N topologies that require exactly N
2 2-by-2’s for their joining
points to be uniquely identiﬁed by any merging algorithm. In F ig. 5(a), checking the types of
(R1, R 2) and (R3, R 4) is suﬃcient for localizing all four joining points. In Fig. 5( b), where all the
joining points are the same as J, checking the types of (R1, R 3) and (R2, R 4) would be suﬃcient.
Note on Lemma 6.2: If the 2-by-2’s are properly selected, N
2 of them can be suﬃcient in some
topologies, as we see in the examples of Fig. 5. Unfortunatel y, we do not know in advance (without
knowledge of the 2-by-N topology) which 2-by-2’s to choose o ut of all
( N
2
)
possible 2-by-2’s, so as to
uniquely localize the joining points between branching poi nts. Nevertheless, from the given S1 1-
by-N topology, we can give an upper bound on the number of 2-by -2’s required. Since every receiver
21
R1 R2 
B1,3 
B1,2 
J1 
S2 S1 
B3,4 
R3 R4 
J2 J3 J4 
B2 
(a) Example topology 1
R1 R2 
B1,3 
B1,2 
J 
S2 
S1 
B3,4 
R3 R4 
(b) Example topology 2
Figure 5: Two example 2-by-N topologies that require exactl y N
2 2-by-2’s for their joining points to be uniquely
identiﬁed by any merging algorithm.
is checked with other receivers that are children of its uppe r branching points, up to the location
of its joining point, we need to check for O(N log N) 2-by-2’s. This is less than identifying all
( N
2
)
2-by-2’s. Note that we still need to multicast x1, x 2 to all receivers and monitor all observations,
but we can use only the observations of the selected 2-by-2’s for inference, and ignore the rest.
Lemma 6.3. Algorithm 7 takes at most O(N log N) steps.
Proof. As mentioned in the note above, Algorithm 7 considers every s ingle receiver and checks the
2-by-2 type of that receiver with other receivers that are ch ildren of its upper branching points, up
to the location of its joining point. Therefore, it takes at m ost O(N log N) steps.
This is an improvement over O(N3) in [36]. Note that the second merging algorithm requires
all
( N
2
)
2-by-2’s; therefore, it takes
( N
2
)
steps.
7. Simulation Results
We now simulate our Inference 13 algorithms in some representative topologies that exempli fy
diﬀerent characteristics.
7.1. Trees
7.1.1. Simulation Setup
Consider the binary tree example of Fig. 1(a). Assume that al l links have the same loss
probability p ∈ [0, 10%]. We simulate Algorithm 2 and we send up to M = 10 probes per iteration.
We conservatively consider an error to be any divergence from the true topology. The results are
averaged over 10 , 000 realizations of the loss process.
7.1.2. Simulation Results
Fig. 6 shows the percentage of inference errors in each of the ﬁrst two iterations (shown in
Fig. 1(b) and Fig. 1(c)) as a function of p and M. As expected, the probability of error is increasing
with p, since packet losses may lead to the misclassiﬁcation of a le af to the incorrect component. For
a ﬁxed number of probes per iteration and ﬁxed loss rate p, the probability of error decreases with
13We note that, in both our approach and in past work [36, 37], th e error in identifying the 2-by-2’s, in the ﬁrst
step, may propagate to the Merging algorithm, in the next ste p. However, there is no additional error introduced by
the Merging algorithm itself, and thus no need to simulate it .
22
0 1 2 3 4 5 6 7 8 9 10 
0
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
% loss (same on every link) 
% wrong inference 
 
 
M=1 
M=2 
M=3 
M=5 
(a) Iteration 1 infers the topology in Fig. 1(b).
0 1 2 3 4 5 6 7 8 9 10 
0
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
% loss (same on every link) 
% wrong inference 
 
 
M=1 
M=2 
M=3 
M=5 
(b) Iteration 2 infers the topology in Fig. 1(c).
Figure 6: Probability of incorrect inference for the binary tree of Fig. 1(a), as a function of the loss probability p
(same for all links) and of the number of probes M per iteration. The results are averaged over 10 , 000 realizations.
the iterations. It also decreases rapidly with the number of probes M per iteration: it decreases
signiﬁcantly even with M = 2 , 3, even for large p, and becomes practically zero for M ≥ 5.14
7.2. Multiple-Tree Topologies (DAGs)
We simulate our algorithms in example multiple-tree networ ks. In summary, we show that
(i) our approach signiﬁcantly improves over [13, 36, 37], in terms of the number of experiments
required to identify the type of all 2-by-2’s as well as of the associated probability of error; (ii) the
probability of error in identifying the 2-by-2’s depends on the underlying topology. In particular,
it is smaller for preferential attachment graphs as compare d to ER random graphs.
7.2.1. Simulation Setup
To demonstrate (i), we consider Fig. 7(a), which shows the Ab ilene topology [51], with two
sources located at the Chicago and Indiana nodes, and nine re ceivers, each located at one of the
other core network nodes. This is the same topology consider ed in [36]. To investigate (ii), we
consider Fig. 7(b) and Fig. 7(c). Fig. 7(b) shows a random top ology with 2 sources and 7 receivers
generated by LEDA [53], which can be used to model wireless mu lti-hop topologies. Fig. 7(c) shows
a preferential attachment topology generated by Brite [52] . We pick 2 sources and 8 receivers and
we select the route for every source-destination pair, acco rding to our assumptions in Section 3.
We run Alg. 5 and Alg. 6 in the absence and presence of packet lo ss, respectively, and we
compute the error. In the lossless case, we identify the 2-by -2 types and we report the error as a
function of the number of experiments countMax. The only possible error in this case is to falsely
declare type 4 as type 1. In the lossy case, we also report the e rror assuming that there is packet loss
in the network (with probability p independently on every link), and after applying Alg. 6 to ea ch
topology. An error in this case can result either from declar ing type 2 or 3 or 4 as type 1; or from
declaring type 4 as type 2 or 3. We consider values of p ∈ [0, 10%] and countMax = 100 , 200, 250.
14This second observation is due to the fact that one correctly received packet is suﬃcient for the correct opera tion
of Alg. 2. E.g., if a node receives a mixture of x1 and x2 probes, it will be correctly assigned to component L3 even if
some probes are lost. In contrast, methods that require each receiver to receive enough packets to infer the loss rate
associated with the network links with a certain accuracy, n eed a larger number of probes for statistical signiﬁcance.
23
B1,3 
S1 
B2 
J3 
B3,4 
B6,7 B4,5 
R9 R8 R7 R6 R5 R4 R3 R2 R1 
B1,2 
B6,9 
J1 
S2 
(a) The Abilene topology [51].
S1 
B5,6 
B1,2 
J1 
R7 R6 R5 R4 R3 R2 R1 
J5 
J4 B1,3 
B1,4 
B2 
S2 
(b) An Erdos-Renyi random graph.
S1 
B2 
S2 
B1,2 
J1 
R2 
J2 
R4 R3 
B2,3 
R8 R7 R6 R5 R1 
(c) A preferential attachment graph.
Figure 7: Three diﬀerent topologies used to test our inferen ce algorithms in simulation.
0 10 20 30 40 50 60 70 80 90 100
0
10
20
30
40
50
60
70
80
number of experiments (countMax)
% error
 
 
Random (ER)
Abilene
Preferential
(a) Lossless case. Probability of error vs. the number
of experiments for the three topologies in Fig. 7. The
results are averaged over 1000 realizations.
0 1 2 3 4 5 6 7 8 9 10
0
5
10
15
20
25
30
% loss (same on every link)
% error
The Abilene topology
 
 
countMax=100
countMax=200
countMax=250
(b) Lossy case. The probability of error vs. the loss
rate for diﬀerent countMax values for the Abilene
topology in Fig. 7(a); 5000 realizations.
Figure 8: Simulation results both in the absence and in the pr esence of loss for the topologies in Fig. 7.
We assume that individual link delays have a ﬁxed part of 5-10 ms (propagation delay) and
a variable part, which is exponential with mean 2ms (queuein g delay). We choose a large time
window W = 100 ms. The oﬀset u is drawn uniformly at random from [70 , 100]ms, i.e., f = 0 . 7.
7.2.2. Simulation Results
Fig. 8(a) reports the results for the lossless case for all th ree topologies in Fig. 7. We also
report the results for the lossy case: only one topology is sh own in Fig. 8(b) due to the lack of
space; additional ﬁgures can be found in the technical repor t [39].
We ﬁrst discuss the Abilene topology in Fig. 7(a). In the lossless case, the Abilene curve in
Fig. 8(a) shows that the error probability decreases very ra pidly with countmax and reaches 0 at
countMax ≃ 50. In the lossy case shown in Fig. 8(b), the error probabilit y also decreases rapidly
with countMax; it is negligible with 200 − 250 experiments. This is a signiﬁcant improvement over
[36] for the same example topology: they use 1000 measuremen ts to distinguish only between type
1 and the other types, for very small loss rates up to 1 . 5%, and they achieve error probability 10%.
In contrast, with an order of magnitude less probes, we disti nguish among all four types, and we
have a very small error probability for larger loss rates (up to 10%). Note that the error probability
is decreasing with the loss rate because loss actually helps to create observations of group (iii) [39].
24
We now consider random graphs, in particular Erdos-Renyi (ER) vs. Preferential Attachme nt,
as shown in Fig. 7(b), 7(c). In the lossless case, Fig. 8(a) sh ows that the error probability decreases
rapidly with countMax and reaches 0 at countMax ≃ 50. We also observe that for the same num-
ber of experiments, the error is generally smaller in the top ology generated using the preferential
attachment rather than the ER model. This is true in both loss less and lossy cases [39]. 15
8. Our Work in Perspective
First, we discuss possible deployment scenarios in Section 8.1. Then, in Sections 8.2-8.6, we
revisit the related work, brieﬂy outlined in Section 2, but n ow focusing on the most closely re-
lated parts. We discuss the trade-oﬀs involved in each approa ch and we identify connections and
diﬀerences between our approach and each of the alternative a pproaches.
8.1. Deployment Scenarios
In practice, our scheme can be deployed in two ways:
• Active probing scenario: special probes are sent by the sources, recognized and coded /multicast
at intermediate nodes, received by the receivers, sent and p rocessed at a fusion center.
• Packet marking scenario: a special header is reserved on regular packets, marking and coding
occurs only on this header, without coding the data in the pac ket. The rest of the ideas in
this paper apply in the same way, but now considering the cont ent of the headers as opposed
to the content of the payload.
The ﬁrst scenario is the one already described throughout th e paper. The sources send special
probe packets, whose sole purpose is to be received and proce ssed for inference purposes. The
intermediate nodes perform simple network coding operatio ns on these packets, to allow for more
accurate and eﬃcient inference. We are going to compare the ﬁ rst scenario to alternative topology
inference approaches in Sections 8.2, 8.3, 8.4 and 8.5. In th e second scenario, our scheme could also
be implemented as passive on top of regular traﬃc. A 2-by-2 co mponent consists of four regular
unicast ﬂows between the two sources and the two receivers, i ntermediate nodes mark a special
ﬁeld in the headers of these regular packets. The special ﬁel d in the headers can be coded (by
constructing a mark that is the sum of the marks from two ﬂows g oing through the intermediate
node), or multicast (by marking several outgoing ﬂows with t he mark found on the header of one
incoming ﬂow). However, all the “action” is now on the header s, while the actual data can remain
uncoded. In this respect, this deployment scenario is simil ar to packet marking techniques, which
we describe in detail in Section 8.6.
Beyond the deployment scenario and practical applicabilit y, we believe that our work provides
a fundamental building block for exploiting correlation in the content of network coded packets
for inference. It is already applicable to networks with net work coding (which can be applied to
dedicated probes sent over the network, as in the ﬁrst scenar io above) or packet marking capa-
bilities (which can be applied only on the headers of packets from regular unicast ﬂows, as in the
second scenario above), or can be used as the basis for design ing more “practical” schemes that
approximate its functionality. For example, a large portio n of the tomography literature is based
15The reason is that in the preferential attachment topologie s, we have a few nodes with a very high degree and
many nodes with a low degree. As a result, we have a large numbe r of receivers with a shared joining point and
some other receivers with distinct joining points. In contr ast, in ER graphs, we have several roughly equally-sized
groups of shared receivers, where each group forms a non-sha red type with any other group. Therefore, we have
more 2-by-2’s of type 1 in preferential attachment graphs, w hich results in a smaller error in Alg. 5 and Alg. 6.
25
on multicast, which has not been deployed in the Internet eit her, thus one can say that it was not
“practical”. Nevertheless, multicast tomography showed h ow to exploit fundamental properties
of topology-dependent correlation in multicast traﬃc for i nference. Later on, unicast tomography
used unicast probes to develop more “practical” schemes tha t mimicked and approximated the
multicast tomography. Similarly, we believe that our work p rovides a fundamental building block
for exploiting correlation in the content of network coded p ackets for inference. In our context,
network coding can be thought of as “reverse multicast” that can be exploited for inference.
8.2. Comparison to Traditional Tomography w/o Network Coding
Within the large literature on network tomography, the most closely related work is the Multiple
Source Network Tomography in [13, 36, 37], which formally de ﬁnes M-by-N tomography problem.
Our work on DAGs builds on [13, 36]: we follow their approach f or decomposing the M-by-N into a
number of 2-by-2 components, inferring the type of each 2-by -2 and then merging them together to
reconstruct the M-by-N. Using simple network coding operat ions at intermediate nodes provides a
graceful way to reveal coding points, which has been typical ly a challenge in traditional tomography.
Our work improves upon [13, 36] in that: (i) it can exactly identify the 2-by-2 type, as opposed to
just distinguish between shared and non-shared types; and ( ii) the merging algorithms can precisely
locate the joining points with respect to the branching poin ts, as opposed to provide bounds.
Simulation results in Section 7 on the same topology used in [ 36], showed that our approach is
more accurate, with less experiments. In essence, our appro ach is deterministic (one observation
suﬃces to distinguish among types) as opposed to probabilis tic (which needs to collect a large
number of probes for statistical signiﬁcance). This beneﬁt comes at the cost of having intermediate
nodes do some operations. However, these operations are so s imple (just additions), that can be
simply thought of as inverse multicast. This cost can be remo ved, if our approach is implemented
as passive on top of random network coding, as outlined in Sec tion 8.4.
8.3. Comparison to Passive Tomography with Network Coding
Recently, a passive approach for topology inference on top o f random network coding has been
proposed in [42]. Probes are sent once, and intermediate nod es pick coding coeﬃcients β uniformly
at random out of a large ﬁeld Fq. The key idea is that, under assumptions of strong connectiv ity and
large enough ﬁnite ﬁeld, Fq, the transfer matrix M, from the sender to the receiver, is distinct for
diﬀerent networks, w.h.p. Then, using the observations Y at the receiver and the source messages
X, exhaustive enumeration of all possible topologies is used to ﬁnd an M that matches Y = MX .
An extended version of this work to erroneous networks is pro vided in [49], where diﬀerent (ergodic
or adversarial) failures lead to diﬀerent transfer function s. Our approach is diﬀerent in that it is
active and uses several probes but simple coding operations over a small ﬁeld.
Example 7. To better illustrate the diﬀerences, we consider a 2-by-2 to pology, and we try to infer
its type using the two approaches. The transfer matrices corr esponding to the four 2-by-2 types of
Fig. 3 are provided in Fig. 9. The approach in [42, 49] tries to d istinguish among these four M’s in
a single experiment. In contrast, we send probe packets in mul tiple rounds. In each experiment, β’s
are either 0 or 1 (since we do additions only 16). We exclude some of the possible topologies in each
experiment, until we are left with only one unique topology, in at most countMax experiments.
Our countMax experiments can be thought of as collecting observations Y1 = M1X, Y 2 =
M2X, · · · , Y countMax = McountMax X, where M1, M 2, · · · , M countMax are diﬀerent representations
16In a joining point J, the β from an incoming link to the outgoing link is 1 if the packet ar rives at J within W ,
and 0 otherwise. In a branching point B, all β ’s are 1 unless there is loss, which makes β ’s 0 in both J’s and B’s.
26
M1 =
( βS1J,JB · βJB,BR1 βS1J,JB · βJB,BR2
βS2J,JB · βJB,BR1 βS2J,JB · βJB,BR2
)
M2 =
( βS1J1,J1B1 · βJ1B1,B1R1 βS1J1,J1B1 · βJ1B1,B1J2 · βB1J2,J2R2
βS2B2,B2J1 · βB2J1,J1B1 · βJ1B1,B1R1 βS2B2,B2J2 · βB2J2,J2R2 + βS2B2,B2J1 · βB2J1,J1B1 · βJ1B1,B1J2 · βB1J2,J2R2
)
M3 =
( βS1J2,J2B1 · βJ2B1,B1J1 · βB1J1,J1R1 βS1J2,J2B1 · βJ2B1,B1R2
βS2B2,B2J1 · βB2J1,J1R1 + βS2B2,B2J2 · βB2J2,J2B1 · βJ2B1,B1J1 · βB1J1,J1R1 βS2B2,B2J2 · βB2J2,J2B1 · βJ2B1,B1R2
)
M4 =
( βS1B1,B1J1 · βB1J1,J1R1 βS1B1,B1J2 · βB1J2,J2R2
βS2B2,B2J1 · βB2J1,J1R1 βS2B2,B2J2 · βB2J2,J2R2
)
Figure 9: Comparison of our approach to [42, 49] in the exampl e of a 2-by-2 topology. M1, M2, M3, and M4 are the
transfer matrices resulting from the four diﬀerent types (t ypes 1, 2, 3, and 4, respectively, in Fig. 3) of a 2-by-2, if
intermediate nodes use coding coeﬃcients β . The approach in [42, 49] tries to distinguish among these fo ur M’s in
a single experiment. In contrast, we use β ’s either 0 or 1 and multiple experiments to choose an M.
of the unique M. Note that, although M is unique in terms of β’s for each topology, it can be
shown to be non-unique when these β’s are replaced by 0 / 1 values. For example in Fig. 9, the
transfer matrices of types 1 and 4 would look similar if all β’s are equal to 1, but only type 4 can
potentially result in M = [1 , 1; 0, 1], while type 1 cannot. We send probes in multiple experimen ts
to create those representations of M that help us uniquely identify the underlying topology.
In terms of the ﬁeld size, [42, 49] require a larger ﬁeld than u s, to obtain distinct transfer
matrices for diﬀerent topologies. 17 In terms of bandwidth, we use small packets in each experimen t,
since we perform operations over a small ﬁeld, and a few exper iments are required. On the other
hand in [42], the coeﬃcients, sent anyway along with packets through the network, are used to
reveal the topology from the transfer matrix at the receiver , and can be thought of as the equivalent
of probes. The distinction between active and passive appro aches becomes even less pronounced, if
we consider that [42, 49] require the receiver to have a-prio ri knowledge of the size of the network,
and of the code-book used at each node (referred to as common randomness), which depends on the
node id [49]. We do not require such knowledge and we infer the topology with smaller complexity.
Further comparison of our approach to [49] for larger M-by-N topologies can be found in [39].
8.4. Extension to Passive Tomography with RNC
We now discuss how our approach can potentially be extended t o be implemented as passive,
when random network coding (RNC) is used in the middle. Intui tively, the same topology inference
algorithms should apply if we ensure that RNC coeﬃcients sat isfy necessary conditions for inference.
Assume that in each experiment, the intermediate nodes perf orm random linear network coding
operations instead of the simple additions assumed so far. I n this case, Alg. 6 will still work if
we assign coding coeﬃcients to the joining points in a partia l order, so as to ensure that the
minimum coding coeﬃcient of a joining point is always greate r than or equal to the maximum
coding coeﬃcient of its ancestor joining point. Under this c ondition, we can prove that the same
rationale as in Section 6.2.2 still holds, i.e., type 1 results in similar observations; type 2 results in
17[42] shows that if local coding variables are i.i.d uniform r andom variables over Fq, then the probability that all
diﬀerent unicast networks with at most |V | nodes and |E| edges have distinct transfer matrices is ≥ 1 − |V |4|E|(1 −
(1− 1
q )|V |). This shows that: (i) the success probability → 1 iﬀ q → ∞ ; (ii) q needs to increase rapidly as the network
size grows. For example, our approach described in Section 6 .2 requires a small ﬁeld F3 to distinguish among diﬀerent
2-by-2’s. While one can calculate that if β ’s are chosen uniformly at random out of F3, then P r(M4 = M1) ∼
= 0. 04.
27
c12 − c22 ≤ 0; type 3 results in c12 − c22 ≥ 0; and type 4 results in c12 − c22 ≤ 0 or c12 − c22 ≥ 0.
The proof can be found in the technical report [39]. Code desi gn to jointly meet both random
network coding goals (large enough ﬁeld for independent lin ear equations) and tomographic goals
(the aforementioned condition) is part of future work. If su ch a code design is possible, Algorithm
6 can be directly applied to the case of random network coding . An example can be found in [39].
8.5. Comparison to traceroute-like Approaches
In practice, the dominant approach to Internet mapping is ba sed on traceroute [8, 9, 14, 15,
26, 29, 43, 45, 47, 50]. It uses traceroute’s sent between selected nodes and collects the ids of
the nodes along the paths traversed. It faces the challenges of (i) resolving anonymous routers and
router aliases and (ii) causing congestion close to the moni toring points [15].
Similarly to traceroute, we also use active end-to-end probes and we require some min imal
co-operation from internal nodes (simple additions in our c ase vs. traceroute-speciﬁc responses).
However, unlike traceroute, we do not ask intermediate nodes to reveal their node id, whi ch has
the advantage of preserving the anonymity of intermediate n odes. A design diﬀerence was also
noted in Section 6.2.4: we infer 2-by-2 components, instead of 1-by-1’s (paths) for traceroute.
In terms of measurement bandwidth, our approach uses exactl y one probe per link per exper-
iment, which is the minimum possible. This is thanks to netwo rk coding that combines multiple
incoming packets into one, and thanks to multicast that repl icates a single incoming packet into
many outgoings, thus eliminating overlap. On the other hand , standard traceroute implemen-
tations, e.g., skitter [29], send three probe packets for each hop count in every sin gle path. 18 For
example, our approach reduces the number of measured paths in a 2-by-N topology by a factor of
two, compared to traceroute; i.e., we require O(N) instead of O(2N) measurements, since each
coded packet observes two paths.
In terms of the amount of information per probe, we can think o f our approach as traceroute
probes that only report the joining points in the topology; w hile traceroute reports all the node
ids, some of them being duplicate. Therefore, the amount of i nformation per probe is signiﬁcantly
decreased in our approach. In essence, we only report the min imum required information, which
we prove to be suﬃcient for reconstructing the topology.
8.6. Comparison to Packet Marking Approaches
As we mentioned in the previous section, traceroute sends many rounds of probes, one for
each hop towards the destination [8, 9, 14, 15, 26, 29, 43, 45, 47, 50]. Therefore, our approach is not
directly comparable to traceroute. On the other hand, as we described in Section 8.1, our scheme
can also be implemented as a packet marking scheme on top of re gular unicast ﬂows. Indeed, the
most comparable approaches to our work are the packet markin g schemes, e.g., traceback schemes
[41], which aim at identifying the source(s) of a sequence of packets and the nodes these packets
have traversed. This is useful for tracing the source(s) of h igh volume traﬃc, e.g., in Distributed
Denial-of-Service attacks [41]. In packet marking schemes , intermediate nodes (probabilistically)
mark packets with information about their identity, and the receiver uses the information from
several packets to reconstruct the path(s) they have traver sed. Packet marking schemes have
been mostly used in a single-path or a reverse tree topology, rooted on the victim of the attack.
Therefore, they identify an M-by-1 topology, while we ident ify a more general M-by-N topology.
18As an example, the average number of packets/link required b y both standard traceroute [8] and our approach
to identify the 2-by-2’s is as follows: in type 2 or 3, it is 12 f or traceroute and 1 for our approach. In type 1, it is
14.4, and in type 4, it is 9, for traceroute, while both are countMax for our approach. The beneﬁt of our approach
in terms of the number of required packets depends on the topo logy. Also, we can trade-oﬀ accuracy for the load by
adjusting countMax. On the other hand, Doubletree [15] can reduce the traceroute overhead to some extent.
28
On the other hand, traceback needs to reconstruct not only th e node ids, but also the order in
which they are traversed by the packets, i.e., the attack paths and graph. In single-path scenarios,
this can be done by adding a distance value to the marking ﬁeld . However, in multi-path scenarios,
this has been a challenge since there exist multiple nodes at the same distance from the receiver.
Many approaches have been proposed to overcome this limitat ion, and most of them suﬀer from
practical issues in terms of the amount of space required in t he packet header for marking. In
contrast, multi-path scenarios are the strength of our appr oach, which identiﬁes the joining points
using network coding.
9. Conclusion
In this paper, we designed active probing schemes that explo it simple operations at intermediate
nodes to accurately infer the topology, based on end-to-end observations. We designed algorithms
for trees and general topologies, and we simulated them in re presentative examples. Our schemes
build on the work by Rabbat et al. [13, 36, 37] and extend it when joining points perform networ k
coding operations. Furthermore, we make connections with s everal alternative approaches, includ-
ing passive inference, traceroute, and packet marking. Our main contribution is that we show
how to exploit the fundamental connection between network c oding and topology, and thus adding
one new building block in the, already large, space of availa ble options for topology inference. We
expect the techniques developed in this paper to be most usef ul in networks that can perform
simple network coding operations, including but not limite d to wireless multi-hop networks.
Acknowledgment
We would like to thank Prof. Animashree Anandkumar at UC Irvi ne for useful discussion on
the performance analysis of Algorithm 6 and on the complexit y of merging algorithms.
Appendix A. Probability of Error of Alg. 2 (Topology Inference in Trees with Loss)
In this Appendix, we consider the topology inference algori thm for tree networks in the presence
of packet loss ( i.e., Alg. 2), and we derive the exact probability of error of its ﬁr st iteration in
inferring the binary tree topology of Fig. 1(a). Assume that all links have the same loss rate p.
Let Xi, i = 2 , 3, 4, 5, 6, be the number of experiments required for receiver i to observe one correct
packet in the ﬁrst iteration, as described in Example 1; e.g., X2 is the number of experiments
required for receiver 2 to observe x1, X3 is the number of experiments required for receiver 3 to
observe x1 + x2, etc. Xi is a geometric random variable with probability of success ρi, as follows:
ρ2 = (1 − p)2 , ρ 5 = ρ6 = (1 − p)3 , ρ 3 = ρ4 = (1 − p)6 (A.1)
If we denote the total number of required experiments for the ﬁrst iteration of Alg. 2 by X,
then we have that: X = maxi∈{2,3,4,5,6}Xi. Therefore, we can compute the Chebyshev bound on
X. On the other hand, in M experiments, we can also compute the probability of error as follows:
P r(error) = 1 − P r(X2 ≤ M & X3 ≤ M & X4 ≤ M & X5 ≤ M & X6 ≤ M) (A.2)
For general M, Eq.(A.2) becomes complicated to compute. Here we only comp ute it for M = 1:
29
P r(error) = 1 − [P r(X2 = 1) P r(X5 = 1 |X2 = 1) P r(X6 = 1 |X2 = 1 , X 5 = 1)
P r(X3 = 1 |X2 = 1 , X 5 = 1 , X 6 = 1) P r(X4 = 1 |X2 = 1 , X 5 = 1 , X 6 = 1 , X 3 = 1)]
= 1 − (1 − p)2(1 − p)3(1 − p)(1 − p)4(1 − p)
(A.3)
For example, for M = 1 and p = 10%, Eq.(A.3) results in P r(error) = 0 . 69, which agrees with
the simulation results in Fig. 6(a). We can also provide an up per bound on P r(error) using the
union bound, as follows:
P r(error) ≤
∑
i∈{2,3,4,5,6}
P r(Xi > M ) = (1 −(1 − p)2)M +2(1−(1 − p)3)M +2(1−(1 − p)6)M (A.4)
For example, for M = 5 and p = 10%, Eq.(A.4) results in P r(error) ≤ 0. 05, which is an upper
bound on the probability of error that we observe in Fig. 6(a) . Similar analysis can be used for the
second iteration and for any other tree topology.
Appendix B. Prob. of Error of Alg. 6 (Inference of 2-by-2’s in DAGs with loss)
In this Appendix, we consider the topology inference algori thm for DAGs in the presence of
packet loss ( i.e., Alg. 6), and we derive the probability of error of Alg. 6, whic h we stated in
Lemma 6.1. Assume that all links have the same loss rate p. As we mentioned in Section 7.2.1,
an error in Alg. 6 can result either from declaring type 4 as ty pe 2 or 3, or from declaring type
2 or 3 or 4 as type 1. In countMax experiments, the probability of error is maximized when the
underlying topology is of type 4. In fact, a type 4 topology ca n give an upper bound on countMax,
as it requires the maximum number of experiments to be identi ﬁed correctly: it requires both a
c12 − c22 > 0 observation and a c12 − c22 < 0 observation.
Let Y1 be the number of experiments required to obtain a c12 − c22 > 0 observation and let Y2
be the number of experiments required to obtain a c12 − c22 < 0 observation. Both Y1 and Y2 are
geometric random variables with success probabilities ρ′
1 and ρ′
2, respectively. In a type 4 topology,
from Table 2, we have that:
ρ′
1 = P r(x1 + x2, x 1) + P r(x2, x 1) (B.1)
One can compute each of these probabilities by considering t he loss events and the late arrival
events (at joining points) that can cause such observations in type 4. If we denote the probability
that packet x2 arrives at joining point Ji within W by P r(u + Di ≤ W ) = γi, i = 1 , 2, and we also
use the notation P r(u + Di > W ) =
γi, then we have that:
P r(x1 + x2, x 1) = p(1 − p)7γ1 + (1 − p)8γ2γ1 = (1 − p)7γ1(p + (1 − p)γ2) (B.2)
Also:
P r(x2, x 1) = p2(1 − p)6 + (1 − p)7γ2p = p(1 − p)6(p + (1 − p)γ2) (B.3)
Therefore, we have that:
ρ′
1 = (1 − p)6(p + (1 − p)γ1)(p + (1 − p)
γ2) (B.4)
30
Similarly, one can compute that:
ρ′
2 = (1 − p)6(p + (1 − p)γ2)(p + (1 − p)
γ1) (B.5)
If we denote the total number of required experiments for Alg . 6 by Y , then we can estimate Y
by Y ≤ Y1 +Y2. If the link delay distribution and the link loss rates are gi ven, we can use the above
relationships to ﬁnd the Chebyshev bound on Y . On the other hand, in countMax experiments,
we can also compute the probability of error as follows:
P r(error) = 1 − P r(Y1 ≤ countMax & Y2 ≤ countMax) (B.6)
We can assume that the events Y1 ≤ countMax and Y2 ≤ countMax are independent. Thus:
P r(error) = 1 −
countMax−1∑
i=0
(1 − ρ′
1)iρ′
1
countMax−1∑
j=0
(1 − ρ′
2)j ρ′
2 (B.7)
We can also provide an upper bound on P r(error) using the union bound, as follows:
P r(error) ≤ P r(Y1 > countMax ) + P r(Y2 > countMax ) = (1 − ρ′
1)countMax + (1 − ρ′
2)countMax
(B.8)
Therefore, the proof of Lemma 6.1 is complete. □
References
[1] A. Anandkumar, A. Hassidim, and J. Kelner, “Topology Dis covery of Sparse Random Graphs with Few Partic-
ipants,” in Proc. of ACM Sigmetrics ’11 , San Jose, CA, Jun. 2011.
[2] A. Bestavros, J. Byers, and K. Harfoush, “Inference and l abeling of metric-induced network topologies,” in
IEEE Transactions on Parallel and Distributed Systems , vol. 16, no. 11, pp. 1053–1065, Nov. 2005.
[3] T. Bu, N. Duﬃeld, F. Lo Presti, and D. Towsley, “Network to mography on general topologies,” in Proc. of
ACM Sigmetrics , Marina Del Rey, CA, Jun. 2002.
[4] R. Caceres, N. Duﬃeld, J. Horowitz, F. L. Presti, and D. To wsley, “Loss based inference of multicast network
topology,” in Proc. of the 38th IEEE conference on Decision and Control , Phoenix, AZ, Dec. 1999.
[5] R. Caceres, N. G. Duﬃeld, J. Horowitz, and D. Towsley, “Mu lticast-based inference of network-internal loss
characteristics”, in IEEE Transactions on Information Theory , vol. 45, pp. 2462–2480, Nov. 1999.
[6] R. Castro, M. Coates, G. Liang, R. Nowak, and B. Yu, “Netwo rk tomography: Recent developments,” Statistical
Science, vol. 19, no. 3, pp. 499–517, 2004.
[7] M. Cheraghchi, A. Karbasi, S. Mohajer, and V. Saligrama, “Graph-Constrained Group Testing,” in Computing
Research Repository (CoRR) , 2010.
[8] B. Cheswick, H. Burch, S. Branigan, “Mapping and visuali zing the Internet,” in Proc. of USENIX ATC , 2000.
[9] A. Clauset and C. Moore, “Why mapping the Internet is hard ,” arXiv:cond-mat/0407339, 2004.
[10] M. Coates, R. Castro, M. Gadhiok, R. King, Y. Tsang, R. No wak, “Maximum likelihood network topology
identiﬁcation from edge-based unicast measurements,” in Proc. of ACM Sigmetrics , Marina Del Rey, CA, 2002.
[11] M. Coates, Y. Pointurier, and M. Rabbat, “Compressed ne twork monitoring for IP and all-optical networks,”
in Proc. of ACM IMC , San Diego, CA, Oct. 2007.
[12] M. Coates, Y. Pointurier, and M. Rabbat, “Compressed ne twork monitoring,” in Proc. of IEEE Statistical
Signal Processing Workshop , Madison, Wisconsin, Aug. 2007.
[13] M. Coates, M. Rabbat, and R. Nowak, “Merging Logical Top ologies Using End-to-end Measurements,” in Proc.
of ACM IMC , Miami, FL, Oct. 2003.
[14] L. Dall’Asta, I. Alvarez-Hamelin, A. Barrat, A. Vazque z, A. Vespignani, “A statistical approach to the
traceroute-like exploration of networks: Theory and simul ations,” Lecture Notes in CS , vol. 3405, p. 140, 2005.
[15] B. Donnet, P. Raoult, T. Friedman, and M. Crovella, “Dep loyment of an Algorithm for Large-Scale Topology
Discovery,” IEEE JSAC , vol. 24, issue 12, pp. 2210–2220, Dec. 2006.
[16] D-Z. Du, F. Hwang, “Combinatorial group testing and its applications,” Series on Applied Mathematics , 2000.
[17] N. Duﬃeld, J. Horowitz, F. Presti, “Adaptive multicast topology inference,” in Proc. of IEEE INFOCOM , 2001.
31
[18] N. Duﬃeld, J. Horowitz, F. L. Presti, and D. Towsley, “Mu lticast topology inference from end-to-end measure-
ments,” in Proc. of ITC Seminar on IP Traﬃc, Measurement, and Modelin g, Monterey, CA, Sep. 2000.
[19] N. G. Duﬃeld, J. Horowitz, F. L. Presti, and D. Towsley, “ Multicast topology inference from measured end-to-
end loss,” in IEEE Trans. on Inf. Theory , vol. 48, no. 1, pp. 26–45, Jan. 2002.
[20] N. G. Duﬃeld and F. L. Presti, “Network tomography from m easured end-to-end delay covariance,” in
IEEE/ACM Transactions on Networking , vol. 12, no. 6, pp. 978–992, Dec. 2004.
[21] N. Duﬃeld, F. L. Presti, V. Paxson, and D. Towsley, “Infe rring link loss using striped unicast probes,” in Proc.
of IEEE INFOCOM , Anchorage, AK, Apr. 2001.
[22] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison, “Biolo gical Sequence Analysis: Probabilistic Models of
Proteins and Nucleic Acids,” Cambridge University Press , 1999.
[23] B. Eriksson, G. Dasarathy, P. Barford, and R. Nowak, “To ward the Practical Use of Network Tomography for
Internet Topology Discovery”, in Proc. of IEEE infocom , San Diego, CA, Mar. 2010.
[24] C. Fragouli, A. Markopoulou, and S. Diggavi, “Topology Inference using Network Coding,” in Proc. of the
Allerton Conference , Monticello, IL, Sept. 2006.
[25] M. Gjoka, C. Fragouli, P. Sattari, and A. Markopoulou, “ Loss Tomography in General Topologies with Network
Coding,” in Proc. of IEEE Globecom , Washington DC, Nov. 2007.
[26] R. Govindan, H. Tangmunarunkit, “Heuristics for Inter net map discovery,” in Proc. of IEEE Infocom , 2000.
[27] G. Hartl, B. Li, “Loss inference in wireless sensor netw orks based on data aggregation,” in Proc. of IPSN , 2004.
[28] T. Ho, B. Leong, Y. Chang, Y. Wen, and R. Koetter, “Networ k Monitoring in Multicast Networks Using Network
Coding,” in Proc. of IEEE ISIT , Adelaide, Australia, Sep. 2005.
[29] B. Huﬀaker, D. Plummer, D. Moore, k. claﬀy, “Topology di scovery by active probing,” in Proc. of SAINT , 2002.
[30] M. Jafarisiavoshani, C. Fragouli, and S. Diggavi, “Sub space properties of randomized network coding,” in Proc.
of ITW , Bergen, Norway, Jul. 2007.
[31] M. Jafarisiavoshani, C. Fragouli, S. Diggavi, C. Gkant sidis, “Bottleneck discovery and overlay management in
network coded peer-to-peer systems,” in Proc. of ACM SIGCOMM INM Workshop , Kyoto, Japan, Aug. 2007.
[32] S. Katti, H. Rahul, W. Hu, D. Katabi, M. M´ edard, and J. Cr owcroft, “XORs in the Air: Practical Wireless
Network Coding,” in IEEE Trans. on Networking , vol. 16, no. 3, Jun. 2008.
[33] Y. Lin, B. Liang, and B. Li, “Passive Loss Inference in Wi reless Sensor Networks Based on Network Coding,”
in Proc. of IEEE Infocom , Rio De Janeiro, Brazil, Apr. 2009.
[34] A. Markopoulou, C. Fragouli, M. Gjoka, “A network codin g approach to loss tomography,” arXiv:1005.4769.
[35] H. Nguyen and P. Thiran, “The Boolean solution to the con gested IP link location problem: theory and practice,”
in Proc. of IEEE Infocom , May 2007.
[36] M. Rabbat, M. Coates, and R. Nowak, “Multiple Source Int ernet Tomography,” in IEEE JSAC , vol. 24, no. 12,
pp. 2221–2234, Dec. 2006.
[37] M. Rabbat, R. Nowak, and M. Coates, “Multiple Source Mul tiple Destination Network Tomography,” in Proc.
of IEEE INFOCOM , Hong Kong, Mar. 2004.
[38] S. Ratnasamy and S. McCanne, “Inference of multicast ro uting trees and bottleneck bandwidths using end-to-
end measurements”, in Proc. of IEEE Infocom , New York, NY, Mar. 1999.
[39] P. Sattari, C. Fragouli, A. Markopoulou, “Active Topol ogy Inference using Network Coding,” arXiv:1007.3336.
[40] P. Sattari, A. Markopoulou, and C. Fragouli, “Multiple Source Multiple Destination Topology Inference using
Network Coding,” in Proc. of NetCod Workshop , EPFL, Switzerland, Jun. 2009.
[41] S. Savage, D. Wetherall, A. Karlin, and T. Anderson, “Ne twork support for IP traceback,” in IEEE/ACM ToN ,
vol. 9, issue 3, pp. 226–237, 2001.
[42] G. Sharma, S. Jaggi, B. K. Dey, “Network tomography via n etwork coding,” in Proc. of ITA Workshop , 2008.
[43] Y. Shavitt, E. Shir, “DIMES: Let the Internet Measure It self,” in ACM SIGCOMM CCR , vol. 35, issue 5, 2005.
[44] M. Shih, A. Hero, “Network topology discovery using ﬁni te mixture models,” in Proc. of IEEE ICASSP , 2004.
[45] N. Spring, R. Mahajan, and D. Wetherall, “Measuring ISP topologies with Rocketfuel,” in Proc. of ACM
Sigcomm, Pittsburgh, PA, Aug. 2002.
[46] Y. Tsang, M. Yildiz, P. Barford, and R. Nowak, “Network r adar: tomography from round trip time measure-
ments,” in Proc. of ACM IMC , Taormina, Sicily, Italy, Oct. 2004.
[47] F. Viger, B. Augustin, X. Cuvellier, C. Magnien, M. Lata py, T. Friedman, R. Teixeira, “Detection, understanding
and prevention of traceroute measurement artifacts,” Computer Networks , vol. 52, no. 5, pp. 998–1018, 2008.
[48] W. Xu, E. Mallada, and A. K. Tang, “Compressive Sensing o ver Graphs,” in Proc. of IEEE Infocom , 2011.
[49] H. Yao, S. Jaggi, and M. Chen, “Passive network tomograp hy for erroneous networks: A network coding
approach,” arXiv:0908.0711, 2010.
[50] B. Yao, R. Viswanathan, F. Chang, and D. Waddington, “To pology inference in the presence of anonymous
routers,” in Proc. of IEEE INFOCOM , San Francisco, CA, Mar. 2003.
[51] The Abilene Network, http://noc.net.internet2.edu/.
[52] BRITE Topology Generator. Available at: http://www.cs.bu.edu/brite/.
[53] LEDA software, http://www.algorithmic-solutions.com/.
32
Pegah Sattari (SM’08) received the B.S. degree in Electrical Engineering from
Sharif University of Technology, Tehran, Iran, in 2006, and the M.S. degree in
Electrical and Computer Engineering from the University of California, Irvine, in
2007. She is currently a Ph.D. candidate in the EECS Departme nt at the Univer-
sity of California, Irvine. Her research interests include network measurements,
network coding, and its applications to inference problems .
Christina Fragouli is an assistant professor in the School of Computer and
Communication Sciences, EPFL, Switzerland. She received t he B.S. degree in
Electrical Engineering from the National Technical Univer sity of Athens, Greece,
in 1996, and the M.Sc. and Ph.D. degrees in Electrical Engine ering from the Uni-
versity of California, Los Angeles, in 1998 and 2000, respec tively. She has worked
at the Information Sciences Center, AT&T Labs, and the Natio nal University of
Athens. She has also visited Bell Labs and DIMACS, Rutgers Un iversity. Her
research interests include network coding, network inform ation ﬂow theory and algorithms, and
connections between communications and computer science. She received the ERC Starting Grant
from the European Research Council in 2009.
Athina Markopoulou (SM’98, M’02) is an assistant professor in the EECS De-
partment at the University of California, Irvine. She recei ved the Diploma degree
in Electrical and Computer Engineering from the National Te chnical University
of Athens, Greece, in 1996, and the M.S. and Ph.D. degrees in E lectrical Engi-
neering from Stanford University in 1998 and 2003, respecti vely. She has been a
postdoctoral fellow at Sprint Labs and at Stanford Universi ty, and a member of
the technical staﬀ at Arastra Inc.. Her research interests i nclude network coding, network measure-
ments and security, media streaming and online social netwo rks. She received the NSF CAREER
award in 2008.
33