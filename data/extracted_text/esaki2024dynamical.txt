Deep active inference agents using Monte-Carlo
methods
Zafeirios Fountas‚àó
Emotech Labs &
WCHN, University College London
f@emotech.co
Noor Sajid
WCHN, University College London
noor.sajid.18@ucl.ac.uk
Pedro A.M. Mediano
University of Cambridge
pam83@cam.ac.uk
Karl Friston
WCHN, University College London
k.friston@ucl.ac.uk
Abstract
Active inference is a Bayesian framework for understanding biological intelligence.
The underlying theory brings together perception and action under one single
imperative: minimizing free energy. However, despite its theoretical utility in
explaining intelligence, computational implementations have been restricted to low-
dimensional and idealized situations. In this paper, we present a neural architecture
for building deep active inference agents operating in complex, continuous state-
spaces using multiple forms of Monte-Carlo (MC) sampling. For this, we introduce
a number of techniques, novel to active inference. These include: i) selecting free-
energy-optimal policies via MC tree search, ii) approximating this optimal policy
distribution via a feed-forward ‚Äòhabitual‚Äô network,iii) predicting future parameter
belief updates using MC dropouts and, Ô¨Ånally, iv) optimizing state transition
precision (a high-end form of attention). Our approach enables agents to learn
environmental dynamics efÔ¨Åciently, while maintaining task performance, in relation
to reward-based counterparts. We illustrate this in a new toy environment, based
on the dSprites data-set, and demonstrate that active inference agents automatically
create disentangled representations that are apt for modeling state transitions.
In a more complex Animal-AI environment, our agents (using the same neural
architecture) are able to simulate future state transitions and actions (i.e., plan),
to evince reward-directed navigation - despite temporary suspension of visual
input. These results show that deep active inference ‚Äì equipped with MC methods ‚Äì
provides a Ô¨Çexible framework to develop biologically-inspired intelligent agents,
with applications in both machine learning and cognitive science.
1 Introduction
A common goal in cognitive science and artiÔ¨Åcial intelligence is to emulate biological intelligence, to
gain new insights into the brain and build more capable machines. A widely-studied neuroscience
proposition for this is the free-energy principle, which views the brain as a device performing varia-
tional (Bayesian) inference [1, 2]. SpeciÔ¨Åcally, this principle provides a framework for understanding
biological intelligence, termed active inference, by bringing together perception and action under
a single objective: minimizing free energy across time [ 3‚Äì7]. However, despite the potential of
active inference for modeling intelligent behavior, computational implementations have been largely
restricted to low-dimensional, discrete state-space tasks [8‚Äì11].
‚àóCorresponding author
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Recent advances have seen deep active inference agents solve more complex, continuous state-space
tasks, including Doom [ 12], the mountain car problem [ 13‚Äì15], and several tasks based on the
MuJoCo environment [16], many of which use amortization to scale-up active inference [13‚Äì15, 17].
A common limitation of these applications is a deviation from vanilla active inference in their ability
to plan. For instance, Millidge [17] introduced an approximation of the agent‚Äôsexpected free energy
(EFE), the quantity that drives action selection, based on bootstrap samples, while Tschantzet al. [16]
employed a reduced version of EFE. Additionally, since all current approaches tackle low-dimensional
problems, it is unclear how they would scale up to more complex domains. Here, we propose an
extension of previous formulations that is closely aligned with active inference [4, 9] by estimating
all EFE summands using a single deep neural architecture.
Our implementation of deep active inference focuses on ensuring both scalability and biological
plausibility. We accomplish this by introducing Monte-Carlo (MC) sampling ‚Äì at several levels ‚Äì
into active inference. For planning, we propose the use of MC tree search (MCTS) for selecting
a free-energy-optimal policy. This is consistent with planning strategies employed by biological
agents and provides an efÔ¨Åcient way to select actions (see Sec. 5). Next, we approximate the optimal
policy distribution using a feed-forward ‚Äòhabitual‚Äô network. This is inspired by biological habit
formation, when acting in familiar environments that relieves the computational burden of planning
in commonly-encountered situations. Additionally, for both biological consistency and reducing
computational burden, we predict model parameter belief updates using MC-dropouts, a problem
previously tackled with networks ensembles [16]. Lastly, inspired by neuromodulatory mechanisms in
biological agents, we introduce a top-down mechanism that modulates precision over state transitions,
which enhances learning of latent representations.
In what follows, we brieÔ¨Çy review active inference. This is followed by a description of our deep
active inference agent. We then evaluate the performance of this agent. Finally, we discuss the
potential implications of this work.
2 Active Inference
Agents deÔ¨Åned under active inference: A) sample their environment and calibrate their internal
generative model to best explain sensory observations (i.e., reduce surprise) and B) perform actions
under the objective of reducing their uncertainty about the environment. A more formal deÔ¨Ånition
requires a set of random variables: s1:t to represent the sequence of hidden states of the world till time
t, o1:t as the corresponding observations, œÄ = {a1,a2,...,a T}as a sequence of actions (typically
referred to as ‚Äòpolicy‚Äô in the active inference literature) up to a given time horizon T ‚ààN+, and
PŒ∏(o1:t,s1:t,a1:t‚àí1) as the agent‚Äôs generative model parameterized byŒ∏till time t. From this, the
agent‚Äôs surprise at timetcan be deÔ¨Åned as the negative log-likelihood ‚àílog PŒ∏(ot). Through slight
abuse of notation, PŒ∏(.) denotes distribution parameterisation by Œ∏ and P(Œ∏) denotes use of that
particular distribution as a random variable. See supplementary materials for deÔ¨Ånitions (Table 1).
To address objective A) under this formulation, the surprise of current observations can be indirectly
minimized by optimizing the parameters, Œ∏, using as a loss function the tractable expression:
‚àílog PŒ∏(ot) ‚â§EQœÜ(st,at)
[
log QœÜ(st,at) ‚àílog PŒ∏(ot,st,at)
]
, (1)
where QœÜ(st,at) is an arbitrary distribution of st and at parameterized by œÜ. The RHS expression
of this inequality is the variational free energy at time t. This quantity is commonly referred to as
negative evidence lower bound [18] in variational inference. Furthermore, to realize objective B),
the expected surprise of future observations ‚àílog P(oœÑ|Œ∏,œÄ) where œÑ ‚â•t‚àícan be minimized by
selecting the policy that is associated with the lowest EFE, G[19]:
G(œÄ,œÑ) =EP(oœÑ|sœÑ,Œ∏)EQœÜ(sœÑ,Œ∏|œÄ)
[
log QœÜ(sœÑ,Œ∏|œÄ) ‚àílog P(oœÑ,sœÑ,Œ∏|œÄ)
]
, (2)
Finally, the process of action selection in active inference is realized as sampling from the distribution
P(œÄ) =œÉ
(
‚àíG(œÄ)
)
= œÉ
(
‚àí
‚àë
œÑ>t
G(œÄ,œÑ)
)
, (3)
where œÉ(¬∑) is the softmax function.
2
ùëúùë°
QœÜs
ùë†ùë°ùë†ùë°-1
Œë
QœÜ
PŒ∏o
PŒ∏s
Generative model
Inference model
Top-down precision
H(ot+1|s, Œ∏, œÄ)
H(st+1|ot+1, œÄ)
log P(ot+1|œÄ)
Œí
ùë†ùë°+1
ùëúùë°+1
ùë†ùë°
H(st+1|œÄ)
s ~ PŒ∏s(st+1|st , œÄ)
s ~ PŒ∏s(st+1|st , œÄ)
Œ∏
Œ∏ ~ Q(Œ∏|œÄ)
Used in Eq. (8a)
Used in Eq. (8b)
Used in Eq. (8c)
Used in Eq. (8)
H(ot+1|s, œÄ)
C
ùë†ùë°+1
ùë†ùë°
ùë†ùë°+1 ùë†ùë°+1 ùë†ùë°+1
ùë†ùë°+2 ùë†ùë°+2 ùë†ùë°+2 ùë†ùë°+2
1
2 3 4
1
2 3 4
habitual prior
QœÜ
Figure 1: A: Schematic of model architecture and networks used during the learning process. Black
arrows represent the generative model (P), orange arrows the recognition model (Q), and the green
arrow the top-down attention (œât). B: Relevant quantities for the calculation of EFE G, computed
by simulating the future using the generative model and ancestral sampling. Where appropriate,
expectations are taken with a single MC sample. C: MCTS scheme used for planning and acting,
using the habitual network to selectively explore new tree branches.
3 Deep Active Inference Architecture
In this section, we introduce a deep active inference model using neural networks, based on amortiza-
tion and MC sampling.
Throughout this section, we denote the parameters of the generative and recognition densities with Œ∏
and œÜ, respectively. The parameters are partitioned as follows: Œ∏= {Œ∏o,Œ∏s}, where Œ∏o parameterizes
the observation function PŒ∏o(ot|st), and Œ∏s parameterizes the transition function PŒ∏s(sœÑ|st,at). For
the recognition density, œÜ= {œÜs,œÜa}, where œÜs is the amortization parameters of the approximate
posterior QœÜs(st) (i.e., the state encoder), and œÜa the amortization parameters of the approximate
posterior QœÜa(at) (i.e., our habitual network).
3.1 Calculating variational and expected free energy
First, we extend the probabilistic graphical model (as deÔ¨Åned in Sec. 2) to include the action sequences
œÄand factorize the model based on Fig. 1A. We then exploit standard variational inference machinery
to calculate the free energy for each time-step tas:
Ft = ‚àíEQœÜs(st)
[
log PŒ∏o(ot|st)
]
+ DKL
[
QœÜs(st) ‚à•PŒ∏s(st|st‚àí1,at‚àí1)
]
+ EQœÜs(st)
[
DKL
[
QœÜa(at) ‚à•P(at)
]]
,
(4)
where
P(a) =
‚àë
œÄ:a1=a
P(œÄ) (5)
is the summed probability of all policies that begin with action a. We assume that st is normally dis-
tributed and ot is Bernoulli distributed, with all parameters given by a neural network, parameterized
by Œ∏o, Œ∏s, and œÜs for the observation, transition, and encoder models, respectively (see Sec. 3.2 for
details about QœÜa). With this assumption, all the terms here are standard log-likelihood and KL terms
easy to compute for Gaussian and Bernoulli distributions. The expectations over QœÜs(st) are taken
via MC sampling, using a single sample from the encoder.
Next, we consider EFE. At time-step tand for a time horizon up to time T, EFE is deÔ¨Åned as [4]:
G(œÄ) =
T‚àë
œÑ=t
G(œÄ,œÑ) =
T‚àë
œÑ=t
E ÀúQ
[
log Q(sœÑ,Œ∏|œÄ) ‚àílog ÀúP(oœÑ,sœÑ,Œ∏|œÄ)
]
, (6)
where ÀúQ = Q(oœÑ,sœÑ,Œ∏|œÄ) = Q(Œ∏|œÄ)Q(sœÑ|Œ∏,œÄ)Q(oœÑ|sœÑ,Œ∏,œÄ ) and ÀúP(oœÑ,sœÑ,Œ∏|œÄ) =
P(oœÑ|œÄ)Q(sœÑ|oœÑ)P(Œ∏|sœÑ,oœÑ). Following Schwartenbeck et al. [20], the EFE of a single time instance
3
œÑ can be further decomposed as
G(œÄ,œÑ) =‚àíE ÀúQ
[
log P(oœÑ|œÄ)
]
(7a)
+ E ÀúQ
[
log Q(sœÑ|œÄ) ‚àílog P(sœÑ|oœÑ,œÄ)
]
(7b)
+ E ÀúQ
[
log Q(Œ∏|sœÑ,œÄ) ‚àílog P(Œ∏|sœÑ,oœÑ,œÄ)
]
. (7c)
Interestingly, each term constitutes a conceptually meaningful expression. The term (7a) corresponds
to the likelihood assigned to the desired observations oœÑ, and plays an analogous role to the notion of
reward in the reinforcement learning (RL) literature [21]. The term (7b) corresponds to the mutual
information between the agent‚Äôs beliefs about its latent representation of the world, before and after
making a new observation, and hence, it reÔ¨Çects a motivation to explore areas of the environment that
resolve state uncertainty. Similarly, the term (7c) describes the tendency of active inference agents to
reduce their uncertainty about model parameters via new observations and is usually referred to in
the literature as active learning [3], novelty, or curiosity [20].
However, two of the three terms that constitute EFE cannot be easily computed as written in Eq. (7).
To make computation practical, we will re-arrange these expressions and make further use of MC
sampling to render these expressions tractable and re-write Eq. (7) as
G(œÄ,œÑ) =‚àíEQ(Œ∏|œÄ)Q(sœÑ|Œ∏,œÄ)Q(oœÑ|sœÑ,Œ∏,œÄ)
[
log P(oœÑ|œÄ)
]
(8a)
+ EQ(Œ∏|œÄ)
[
EQ(oœÑ|Œ∏,œÄ) H(sœÑ|oœÑ,œÄ) ‚àíH(sœÑ|œÄ)
]
(8b)
+ EQ(Œ∏|œÄ)Q(sœÑ|Œ∏,œÄ) H(oœÑ|sœÑ,Œ∏,œÄ ) ‚àíEQ(sœÑ|œÄ) H(oœÑ|sœÑ,œÄ) , (8c)
where these expressions can be calculated from the deep neural network illustrated in Fig. 1B. The
derivation of Eq. (8) can be found in the supplementary material. To calculate the terms (8a) and
(8b), we sample Œ∏, sœÑ and oœÑ sequentially (through ancestral sampling) and then oœÑ is compared with
the prior distribution log P(oœÑ|œÄ) The parameters of the neural network Œ∏are sampled from Q(Œ∏)
using the MC dropout technique [ 22]. Similarly, to calculate the expectation of H(oœÑ|sœÑ,œÄ), the
same drawn Œ∏is used again and sœÑ is re-sampled for N times while, for H(oœÑ|sœÑ,Œ∏,œÄ ), the set of
parameters Œ∏is also re-sampled N times. Finally, all entropies can be computed using the standard
formulas for multivariate Gaussian and Bernoulli distributions.
3.2 Action selection and the habitual network
In active inference, agents choose an action given by their EFE. In particular, any given action
is selected with a probability proportional to the accumulated negative EFE of the corresponding
policies G(œÄ) (see Eq. (3) and Ref. [19]). However, computing Gacross all policies is costly since
it involves making an exponentially-increasing number of predictions for T-steps into the future,
and computing all the terms in Eq. (8). To solve this problem, we employ two methods operating in
tandem. First, we employ standard MCTS [23‚Äì25], a search algorithm in which different potential
future trajectories of states are explored in the form of a search tree (Fig. 1C), giving emphasis to
the most likely future trajectories. This algorithm is used to calculate the distribution over actions
P(at), deÔ¨Åned in Eq. (5), and control the agent‚Äôs Ô¨Ånal decisions. Second, we make use of amortized
inference through a habitual neural network that directly approximates the distribution over actions,
which we parameterize by œÜa and denote QœÜa(at) ‚Äì similarly to Refs. [26‚Äì28]. In essence, QœÜa(at)
acts as a variational posterior that approximates P(at|st), with a prior P(at), calculated by MCTS
(see Fig. 1A). During learning, this network is trained to reproduce the last executed action at‚àí1
(selected by sampling P(at)) using the last state st‚àí1. Since both tasks used in this paper (Sec. 4)
have discrete action spaces A, we deÔ¨Åne QœÜa(at) as a neural network with parameters œÜa and |A|
softmax output units.
During the MCTS process, the agent generates a weighted search tree iteratively that is later sam-
pled during action selection. In each single MCTS loop, one plausible state-action trajectory
(st,at,st+1,at+1,...,s œÑ,aœÑ) ‚Äì starting from the present time-step t ‚Äì is calculated. For states
that are explored for the Ô¨Årst time, the distribution PŒ∏s(st+1|st,at) is used. States that have been
explored are stored in the buffer search tree and accessed during later loops of the same planning
process. The weights of the search tree ÀúG(st,at) represent the agent‚Äôs best estimation for EFE after
taking action at from state st. An upper conÔ¨Ådence bound for G(st,at) is deÔ¨Åned as
U(st,at) = ÀúG(st,at) +cexplore ¬∑QœÜa(at|st) ¬∑ 1
1 +N(at,st) , (9)
4
where N(at,st) is the number of times that at was explored from state st, and cexplore a hyper-
parameter that controls exploration. In each round, the EFE of the newly-explored parts of the
trajectory is calculated and back-propagated to all visited nodes of the search tree. Additionally,
actions are sampled in two ways. Actions from states that have been explored are sampled from
œÉ(U(at,st)) while actions from new states are sampled from QœÜa(at).
Finally, the actions that assemble the selected policy are drawn fromP(at) = N(at,st)‚àë
jN(aj,t,st) . In our
implementation, the planning loop stops if either the process has identiÔ¨Åed a clear option (i.e. if
max P(at) ‚àí1/|A|>Tdec) or the maximum number of allowed loops has been reached.
Through the combination of the approximation QœÜa(at) and the MCTS, our agent has at its disposal
two methods of action selection. We refer to QœÜa(at) as the habitual network, as it corresponds to a
form of fast decision-making, quickly evaluating and selecting a action; in contrast with the more
deliberative system that includes future imagination via MC tree traversals [29].
3.3 State precision and top-down attention
One of the key elements of our framework is the state transition model PŒ∏s(st|st‚àí1,at‚àí1), that
belongs to the agent‚Äôs generative model. In our implementation, we takest ‚àºN(¬µ,œÉ2/œât), where
the multidimensional ¬µand œÉ come from the linear and softplus units (respectively) of a neural
network with parameters Œ∏s applied to st‚àí1, and, importantly, œât is a scalar precision factor (c.f.
Fig. 1A) modulating the uncertainty on the agent‚Äôs estimate of the hidden state of the environment [8].
We model the precision factor as a simple logistic function of the belief update about the agent‚Äôs
current policy,
œât = Œ±
1 +e‚àí
b‚àíDt‚àí1
c
+ d, (10)
where Dt = DKL
[
QœÜa(at) ‚à•P(at)
]
and {Œ±,b,c,d }are Ô¨Åxed hyper-parameters. Note that œât is a
monotonically decreasing function of Dt‚àí1, such that when the posterior belief about the current
policy is similar to the prior, precision is high.
In cognitive terms, œât can be thought of as a means of top-down attention [30], that regulates
which transitions should be learnt in detail and which can be learnt less precisely. This attention
mechanism acts as a form of resource allocation: if DKL
[
QœÜa(at) ‚à•P(at)
]
is high, then a habit
has not yet been formed, reÔ¨Çecting a generic lack of knowledge. Therefore, the precision of the
prior PŒ∏s(st|st‚àí1,at‚àí1) (i.e., the belief about the current state before a new observation ot has been
received) is low, and less effort is spent learningQœÜs(st).
In practice, the effect of œât is to incentivize disentanglement in the latent state representation st ‚Äì the
precision factor œât is somewhat analogous to the Œ≤parameter in Œ≤-V AE [31], effectively pushing
the state encoder QœÜs(st) to have independent dimensions (since PŒ∏s(st|st‚àí1,at‚àí1) has a diagonal
covariance matrix).2 As training progresses and the habitual network becomes a better approximation
of P(at), œât is gradually increased, implementing a natural form of precision annealing.
4 Results
First, we present the two environments that were used to validate our agent‚Äôs performance.
Dynamic dSprites We deÔ¨Åned a simple 2D environment based on the dSprites dataset [ 32, 31].
This was used to i) quantify the agent‚Äôs behavior against ground truth state-spaces andii) evaluate the
agent‚Äôs ability to disentangle state representations. This is feasible as the dSprites data is designed for
characterizing disentanglement, using a set of interpretable, independent ground-truth latent factors.
In this task, which we call object sorting, the agent controls the position of the object via 4 different
actions (right, left, up or down) and is required to sort single objects based on their shape (a latent
factor). The agent receives reward when it moves the object across the bottom border, and the reward
value depends on the shape and location as depicted in Fig. 2A. For the results presented in Section 4,
the agent was trained in an on-policy fashion, with a batch size of 100.
2In essence, for the parameter ranges of interest œât induces a near-linear monotonic increase in DKL, akin to
the linear increase induced by Œ≤in Œ≤-V AE.
5
Bi
Bii
t=1 2 3 4 5 6
t=1 3 6 9 12 15 18 21 24 27 30
Input
Prediction
Visible Blackout: Visual input is not provided
Input
Prediction
Input
Prediction
reward
1
-1
1
-1
1
-1
reward
A
wall
wall
wall
Figure 2: A: The proposed object sorting task based on the dSprites dataset. The agent can perform 4
actions; changing the position of the object in both axis. Reward is received if an object crosses the
bottom boarder and differs for the 3 object shapes. B: Prediction of the visual observations under
motion if input is hidden in both (i) AnimalAI and (ii) dynamic dSprites environments.
Animal-AI We used a variation of ‚Äòpreferences‚Äôtask from the Animal-AI environment [33]. The
complexity of this, partially observable, 3D environment is the ideal test-bed for showcasing the
agent‚Äôs reward-directed exploration of the environment, whilst avoiding negative reward or getting
stuck in corners. In addition, to test the agent‚Äôs ability to rely on its internal model, we used a
‚Äòlights-off ‚Äô variant of this task, with temporary suspension of visual input at any given time-step with
probability R. For the results presented in Section 4, the agent was trained in an off-policy fashion
due to computational constraints. The training data for this was created using a simple rule: move in
the direction of the greenest pixels.
In the experiments that follow, we encode the actual reward from both environments as the prior
distribution of future expected observations log P(oœÑ|œÄ) or, in active inference terms, the expected
outcomes. This is appropriate because the active inference formulation does not differentiate reward
from other types of observations, but it rather deÔ¨Ånes certain (future) observations (e.g. green color
in Animal-AI) as more desirable given a task. Therefore, in practice, rewards can be encoded as
observations with higher prior probability using log P(oœÑ|œÄ).
We optimized the networks using ADAM [ 34], with loss given in Eq. (4) and an extra regular-
ization term DKL
[
QœÜs(st) ‚à•N(0,1)
]
. The explicit training procedure is detailed in the supple-
mentary material. The complete source-code, data, and pre-trained agents, is available on GitHub
(https://github.com/zfountas/deep-active-inference-mc).
4.1 Learning environment dynamics and task performance
We initially show ‚Äì through a simple visual demonstration (Fig. 2B) ‚Äì that agents learn the environ-
ment dynamics with or without consistent visual input for both dynamic dSprites and AnimalAI. This
is further investigated, for the dynamic dSprites, by evaluating task performance (Fig. 3A-C), as well
as reconstruction loss for both predicted visual input and reward (Fig. 3D-E) during training.
To explore the effect of using different EFE functionals on behavior, we trained and compared active
inference agents under three different formulations, all of which used the implicit reward function
log P(oœÑ), against a baseline reward-maximizing agent. These include i) beliefs about the latent
states (i.e., terms a,b from Eq. 7), ii) beliefs about both the latent states and model parameters (i.e.,
complete Eq. 7) and iii) beliefs about the latent states, with a down-weighted reward signal. We found
that, although all agents exhibit similar performance in collecting rewards (Fig. 3B), active inference
agents have a clear tendency to explore the environment (Fig. 3C). Interestingly, our results also
demonstrate that all three formulations are better at reconstructing the expected reward, in comparison
to a reward-maximizing baseline (Fig. 3D). Additionally, our agents are capable of reconstructing the
6
Reward/round
Learning iteration
Reward reconstruction (MSE)
pixel reconstruction (MSE)
Learning iteration
After 5 transitions
Immediate
Learning iteration
Positions visited/round
a a+b a+b+c
A B C
D E
Learning iteration
Reward/round term 7a
terms 7a+b
terms 7a+b+c
term 7a
terms 7a+b
terms 7a+b+c
terms 7a+b
F
Total correlation of Q(s)
Learning iteration
MCTS
VAE
Agent with œâ fixed
Figure 3: Agent‚Äôs performance during on-policy training in theobject sorting task. A: Comparison
of different action selection strategies for the agent driven by the full Eq. (8). B-C: Comparison of
agents driven by different functionals, limited to state estimations of a single step into the future. In
C, the violin plots represent behavior driven by P(at) (the planner) and the gray box plots driven by
the habitual network QœÜa(at). D-F: Reconstruction loss and total correlation during learning for 4
different functionals. In A, B and D-F, the shaded areas represent the standard deviation.
current observation, as well as predicting 5 time-steps into the future, for all formulations of EFE,
with similar loss with the baseline (Fig. 3E).
4.2 Disentanglement and transition learning
Disentanglement of latent spaces leads to lower dimensional temporal dynamics that are easier
to predict [35]. Thus, generating a disentangled latent space scan be beneÔ¨Åcial for learning the
parameters of the transition function PŒ∏s(st+1|st,at). Due to the similarity between the precision
term œât and the hyper-parameter Œ≤in Œ≤-variational autoencoders (V AEs) [31] discussed in Sec. 3.3,
we hypothesized that œât could play an important role in regulating transition learning. To explore this
hypothesis, we compared the total correlation (as a metric for disentanglement [36]) of latent state
beliefs between i) agents that have been trained with the different EFE functionals, ii) the baseline
(reward-maximizing) agent, iii) an agent trained without top-down attention (although the average
value of œât was maintained), as well as iv) a simple V AE that received the same visual inputs. As
seen in Fig. 3F, all active inference agents using œât generated structures with signiÔ¨Åcantly more
disentanglement (see traversals in supp. material). Indeed, the performance ranking here is the same
as in Fig. 3D, pointing to disentanglement as a possible reason for the performance difference in
predicting rewards.
4.3 Navigation and planning in reward-based tasks
The training process in the dynamic dSprites environment revealed two types of behavior. Initially,
we see epistemic exploration (i.e., curiosity), that is overtaken by reward seeking (i.e., goal-directed
behavior) once the agent is reasonably conÔ¨Ådent about the environment. An example of this can be
seen in the left trajectory plot in Fig. 4Ai, where the untrained agent ‚Äì with no concept of reward
‚Äì deliberates between multiple options and chooses the path that enables it to quickly move to the
next round. The same agent, after 700 learning iterations, can now optimally plan where to move
the current object, in order to maximize potential reward, log P(oœÑ|œÄ). We next investigated the
sensitivity when deciding, by changing the threshold Tdec. We see that changing the threshold has
clear implications for the distribution of explored alternative trajectories i.e., number of simulated
states (Fig. 4Aii). This plays an important role in the performance, with maximum performance
found at Tdec ‚âà0.8 (Fig. 4Aiii).
7
Figure 4: Agent‚Äôs planning performance. A: Dynamic dSprites. i) Example planned trajectory plots
with number of visits per state (blue-pink color map) and the selected policy (black lines). ii) The
effect of decision threshold Tdec on the number of simulated states andiii) the agent‚Äôs performance.B:
Animal-AI. i) Same as in A. ii) System performance over hyper-parameters and iii) in the lights-off
task. Error bars in Aiii denote standard deviation and in B standard error of the mean.
Agents trained in the Animal-AI environment also exhibit interesting (and intelligent) behavior. Here,
the agent is able to make complex plans, by avoiding obstacles with negative reward and approaching
expected outcomes (red and green objects respectively, Fig. 4Bi). Maximum performance can be
found for 16 MCTS loops and Tdec ‚âà0.1 (Fig. 4Bii; details in the supplementary material). When
deployed in lights-off experiments, the agent can successfully maintain an accurate representation
of the world state and simulate future plans despite temporary suspension of visual input (Fig. 2B).
This is particularly interesting because PŒ∏s(st+1|st,at) is deÔ¨Åned as a feed-forward network, without
the ability to maintain memory of states before t. As expected, the agent‚Äôs ability to operate in this
set-up becomes progressively worse the longer the visual input is removed, while shorter decision
thresholds are found to preserve performance longer (Fig. 4Biii).
4.4 Comparison with model-free reinforcement learning agents
To assess the performance of the active inference agent with respect to baseline (model-free) RL
agents, we employed OpenAI‚Äôs baselines [37] repository to train DQN [ 38], A2C [ 39] and
PPO2 [40] agents on the Animal-AI environment. The resulting comparison is shown in Fig. 5. Our
experiments highlight that, given the same number of training episodes (2M learning iterations), the
active inference agent performs considerably better than DQN and A2C, and is comparable to PPO2
(all baselines were trained with default settings). In addition, note that of the two best-performing
agents (DAIMC and PPO2), DAIMC has substantially less variance across training runs, indicating a
more stable learning process. Nonetheless, these comparisons should be treated as a way of illustrat-
ing the applicability of the active inference agent operating in complex environments, and not as a
thorough benchmark of performance gains against state-of-the-art RL agents.
5 Concluding Remarks
The attractiveness of active inference inherits from the biological plausibility of the framework [4,
41, 42]. Accordingly, we focused on scaling-up active inference inspired by neurobiological structure
and function that supports intelligence. This is reÔ¨Çected in the hierarchical generative model, where
the higher-level policy network contextualizes lower-level state representations. This speaks to a
separation of temporal scales afforded by cortical hierarchies in the brain and provides a Ô¨Çexible
framework to develop biologically-inspired intelligent agents.
8
Figure 5: Comparison of the performance of our agent (DAIMC) with DQN, A2C and PPO2. The
bold line represents the mean, and shaded areas the standard deviation over multiple training runs.
We introduced MCTS for tackling planning problems with vast search spaces [23, 43, 24, 44, 45].
This approach builds upon √áatal et al.‚Äôs [46] deep active inference proposal, to use tree search to
recursively re-evaluate EFE for each policy, but is computationally more efÔ¨Åcient. Additionally, using
MCTS offers an Occam‚Äôs window for policy pruning; that is, we stop evaluating a policy path if
its EFE becomes much higher than a particular upper conÔ¨Ådence bound. This pruning drastically
reduces the number of paths one has to evaluate. It is also consistent with biological planning, where
agents adopt brute force exploration of possible paths in a decision tree, up to a resource-limited
Ô¨Ånite depth [47]. This could be due to imprecise evidence about different future trajectories [ 48]
where environmental constraints subvert evaluation accuracy [ 49, 50] or alleviate computational
load [51]. Previous work addressing the depth of possible future trajectories in human subjects under
changing conditions shows that both increased cognitive load [50] and time constraints [52, 53, 49]
reduce search depth. Huys et al. [51] highlighted that in tasks involving alleviated computational
load, subjects might evaluate only subsets of decision trees. This is consistent with our experiments
as the agent selects to evaluate only particular trajectories based on their prior probability to occur.
We have shown that the precision factor, œât, can be used to incorporate uncertainty over the prior
and enhances disentanglement by encouraging statistical independence between features [ 54‚Äì57].
This is precisely why it has been associated with attention [58]; a signal that shapes uncertainty [59].
Attention enables Ô¨Çexible modulation of neural activity that allows behaviorally relevant sensory data
to be processed more efÔ¨Åciently [60, 61, 30]. The neural realizations of this have been linked with
neuromodulatory systems, e.g., cholinergic and noradrenergic [62‚Äì66]. In active inference, they have
been associated speciÔ¨Åcally with noradrenaline for modulating uncertainty about state transitions [8],
noradrenergic modulation of visual attention [67] and dopamine for policy selection [4, 67].
An important piece of future work is to more thoroughly compare the performance of DAIMC agent
to reward-maximizing agents. That is, if the speciÔ¨Åc goal is to maximize reward, then it is not clear
whether deep active inference (i.e., full speciÔ¨Åcation of EFE) has signiÔ¨Åcant performance beneÔ¨Åts
over simpler reward-seeking agents (i.e., using only Eq. 7a) or other model-based RL agents [68, 69]
(c.f. Sec. 4.4). We emphasize, however, that the primary purpose of the active inference framework
is to serve as a model for biological cognition, and not as an optimal solution for reward-based
tasks. Therefore, we have deliberately not focused on benchmarking performance gains against
state-of-the-art RL agents, although we hypothesize that insights from active inference could prove
useful in complex environments where either reward maximization isn‚Äôt the objective, or in instances
where direct reward maximization leads to sub-optimal performance.
There are several extensions that can be explored, such as testing whether performance would increase
with more complex, larger neural networks, e.g., using LSTMs to model state transitions. One could
also assess if including episodic memory would Ô¨Ånesse EFE evaluation over a longer time horizon,
without increasing computational complexity. Future work should also test how performance shifts if
the objective of the task changes. Lastly, it might be neurobiologically interesting to see whether the
generated disentangled latent structures are apt for understanding functional segregation in the brain.
9
6 Broader impact
Our deep active inference agent ‚Äì equipped with MC methods ‚Äì provides a Ô¨Çexible framework that
may help gain new insights into the brain by simulating realistic, biologically-inspired intelligent
agents. General contributions of this framework include helping bridge the gap between cognitive
science and deep learning and providing an architecture that would allow psychologists to run
more realistic experiments probing human behavior. SpeciÔ¨Åcally, we hope that simulating this
agent will allow us use the neural network gradients to make predictions about the underlying
physiology associated with behaviors of interest and formulate appropriate hypothesis. We believe
this architecture may also help elucidate complex structure-function relationships in cognitive systems
through manipulation of priors (under the complete class theorem). This would make it a viable
(scaled-up) framework for understanding how brain damage (introduced in the generative model by
changing the priors) can affect cognitive function, previously explored in discrete-state formulations
of active inference [67, 70].
A potential (future) drawback is that this model could be used to exploit people‚Äôs inherent cognitive
biases, and as such could potentially be used by bad actors trying to model (and then proÔ¨Åt from)
human behavior.
7 Acknowledgements
The authors would like to thank Sultan Kenjeyev for his valuable contributions and comments on
early versions of the model presented in the current manuscript and Emotech team for the great
support throughout the project. NS was funded by the Medical Research Council (MR/S502522/1).
PM and KJF were funded by the Wellcome Trust (Ref: 210920/Z/18/Z - PM; Ref: 088130/Z/09/Z -
KJF).
References
[1] Karl J Friston. The free-energy principle: A uniÔ¨Åed brain theory? Nature Reviews Neuroscience,
11(2):127‚Äì138, 2010.
[2] Karl J Friston. A free energy principle for a particular physics.arXiv preprint arXiv:1906.10184,
2019.
[3] Karl J Friston, Thomas. FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O‚ÄôDoherty,
and Giovanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews,
68:862‚Äì79, 2016.
[4] Karl J Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Active inference: A process theory. Neural Computation, 29(1):1‚Äì49, 2017.
[5] Karl J Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and
active inference. Network Neuroscience, 1(4):381‚Äì414, 2017.
[6] Giovanni Pezzulo, Francesco Rigoli, and Karl J Friston. Hierarchical active inference: A theory
of motivated control. Trends in Cognitive Sciences, 22(4):294‚Äì306, 2018.
[7] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl
Friston. Active inference on discrete state-spaces: A synthesis.arXiv preprint arXiv:2001.07203,
2020.
[8] Thomas Parr and Karl J Friston. Uncertainty, epistemics and active inference. Journal of The
Royal Society Interface, 14(136):20170376, 2017.
[9] Karl J Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal
models and active inference. Neuroscience and Biobehavioral Reviews, 90:486‚Äî501, 2018.
[10] Noor Sajid, Philip J Ball, and Karl J Friston. Active inference: DemystiÔ¨Åed and compared.
arXiv preprint arXiv:1909.10863, 2019.
10
[11] Casper Hesp, Ryan Smith, Micah Allen, Karl J Friston, and Maxwell Ramstead. Deeply felt
affect: The emergence of valence in deep active inference. PsyArXiv, 2019.
[12] Maell Cullen, Ben Davey, Karl J Friston, and Rosalyn J. Moran. Active inference in Ope-
nAI Gym: A paradigm for computational investigations into psychiatric illness. Biological
Psychiatry: Cognitive Neuroscience and Neuroimaging, 3(9):809‚Äì818, 2018.
[13] Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference?
PLoS ONE, 4(7):e6421, 2009.
[14] Kai Ueltzh√∂ffer. Deep active inference. Biological Cybernetics, 112(6):547‚Äì573, 2018.
[15] Ozan √áatal, Johannes Nauta, Tim Verbelen, Pieter Simoens, and Bart Dhoedt. Bayesian policy
selection using active inference. arXiv preprint arXiv:1904.08149, 2019.
[16] Alexander Tschantz, Manuel Baltieri, Anil Seth, Christopher L Buckley, et al. Scaling active
inference. arXiv preprint arXiv:1911.10601, 2019.
[17] Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical
Psychology, 96:102348, 2020.
[18] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for
statisticians. Journal of the American Statistical Association, 112(518):859‚Äì877, 2017.
[19] Thomas Parr and Karl J Friston. Generalised free energy and active inference. Biological
Cybernetics, 113(5-6):495‚Äì513, 2019.
[20] Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald, Martin
Kronbichler, and Karl J Friston. Computational mechanisms of curiosity and goal-directed
exploration. eLife, 8:e41703, 2019.
[21] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press,
2018.
[22] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In International Conference on Machine Learning, pages
1050‚Äì1059, 2016.
[23] R√©mi Coulom. EfÔ¨Åcient selectivity and backup operators in monte-carlo tree search. In
International Conference on Computers and Games, pages 72‚Äì83. Springer, 2006.
[24] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling,
Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton.
A survey of Monte Carlo tree search methods.IEEE Transactions on Computational Intelligence
and AI in Games, 4(1):1‚Äì43, 2012.
[25] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of
go without human knowledge. Nature, 550(7676):354‚Äì359, 2017.
[26] Alexandre Pich√©, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, and Chris Pal. Probabilistic
planning with sequential monte carlo methods. In International Conference on Learning
Representations, 2018.
[27] Alexander Tschantz, Beren Millidge, Anil K Seth, and Christopher L Buckley. Control as
hybrid inference. arXiv preprint arXiv:2007.05838, 2020.
[28] Joseph Marino and Yisong Yue. An inference perspective on model-based reinforcement
learning. ICML Workshop on Generative Modeling and Model-Based Reasoning for Robotics
and AI, 2019.
[29] Matthijs Van Der Meer, Zeb Kurth-Nelson, and A David Redish. Information processing in
decision-making systems. The Neuroscientist, 18(4):342‚Äì359, 2012.
11
[30] Anna Byers and John T. Serences. Exploring the relationship between perceptual learning and
top-down attentional control. Vision Research, 74:30 ‚Äì 39, 2012.
[31] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. International Conference on Learning Representations, 2
(5):6, 2017.
[32] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentangle-
ment testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
[33] Matthew Crosby, Benjamin Beyret, and Marta Halina. The Animal-AI olympics. Nature
Machine Intelligence, 1(5):257‚Äì257, 2019.
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[35] Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei, and Juan Carlos Niebles. Learning
to decompose and disentangle representations for video prediction. In Advances in Neural
Information Processing Systems, pages 517‚Äì526, 2018.
[36] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018.
[37] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec
Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines.
https://github.com/openai/baselines, 2017.
[38] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529‚Äì533, 2015.
[39] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-
crap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In International conference on machine learning, pages 1928‚Äì1937,
2016.
[40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[41] Takuya Isomura and Karl J Friston. In vitro neural networks minimise variational free energy.
Nature ScientiÔ¨Åc Reports, 8(1):1‚Äì14, 2018.
[42] Rick A Adams, Stewart Shipp, and Karl J Friston. Predictions not commands: Active inference
in the motor system. Brain Structure and Function, 218(3):611‚Äì643, 2013.
[43] Levente Kocsis and Csaba Szepesv√°ri. Bandit based Monte-Carlo planning. In European
Conference on Machine Learning, pages 282‚Äì293. Springer, 2006.
[44] Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep
learning for real-time Atari game play using ofÔ¨Çine Monte-Carlo tree search planning. In
Advances in Neural Information Processing Systems, pages 3338‚Äì3346, 2014.
[45] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.
Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484,
2016.
[46] Ozan √áatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning
perception and planning with deep active inference. In IEEE International Conference on
Acoustics, Speech and Signal Processing, pages 3952‚Äì3956, 2020.
[47] Joseph Snider, Dongpyo Lee, Howard Poizner, and Sergei Gepshtein. Prospective optimization
with limited resources. PLoS Computational Biology, 11(9), 2015.
12
[48] Alec Solway and Matthew M Botvinick. Evidence integration in model-based tree search.
Proceedings of the National Academy of Sciences, 112(37):11708‚Äì11713, 2015.
[49] Bas van Opheusden, Gianni Galbiati, Zahy Bnaya, Yunqi Li, and Wei Ji Ma. A computational
model for decision tree search. In CogSci., 2017.
[50] Dennis H Holding. Counting backward during chess move choice. Bulletin of the Psychonomic
Society, 27(5):421‚Äì424, 1989.
[51] Quentin JM Huys, Neir Eshel, Elizabeth O‚ÄôNions, Luke Sheridan, Peter Dayan, and Jonathan P
Roiser. Bonsai trees in your head: How the Pavlovian system sculpts goal-directed choices by
pruning decision trees. PLoS Computational Biology, 8(3), 2012.
[52] Bruce D Burns. The effects of speed on skilled chess performance. Psychological Science, 15
(7):442‚Äì447, 2004.
[53] Frenk Van Harreveld, Eric-Jan Wagenmakers, and Han LJ Van Der Maas. The effects of
time pressure on chess skill: An investigation into fast and slow processes underlying expert
performance. Psychological Research, 71(5):591‚Äì597, 2007.
[54] Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement
in variational autoencoders. arXiv preprint arXiv:1812.02833, 2018.
[55] Minyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir Pavlovic. Bayes-Factor-V AE: Hierar-
chical Bayesian deep auto-encoder models for factor disentanglement. In IEEE International
Conference on Computer Vision, pages 2979‚Äì2987, 2019.
[56] Hadi Fatemi Shariatpanahi and Majid Nili Ahmadabadi. Biologically inspired framework for
learning and abstract representation of attention control. In Attention in Cognitive Systems.
Theories and Systems from an Interdisciplinary Viewpoint, pages 307‚Äì324, 2007.
[57] Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo J Rezende.
Towards interpretable reinforcement learning using attention augmented agents. In Advances in
Neural Information Processing Systems, pages 12329‚Äì12338, 2019.
[58] Thomas Parr, David A. Benrimoh, Peter Vincent, and Karl J Friston. Precision and false
perceptual inference. Frontiers in Integrative Neuroscience, 12:39, 2018.
[59] Peter Dayan, Sham Kakade, and Read P Montague. Learning and selective attention. Nature
Neuroscience, 3(11):1218‚Äì1223, 2000.
[60] Farhan Baluch and Laurent Itti. Mechanisms of top-down attention. Trends in Neurosciences,
34(4):210‚Äì224, 2011.
[61] Yuka Sasaki, Jose E Nanez, and Takeo Watanabe. Advances in visual perceptual learning and
plasticity. Nature Reviews Neuroscience, 11(1):53‚Äì60, 2010.
[62] Michael I Posner and Steven E Petersen. The attention system of the human brain. Annual
Review of Neuroscience, 13(1):25‚Äì42, 1990.
[63] Peter Dayan and Angela J Yu. ACh, uncertainty, and cortical inference. In Advances in Neural
Information Processing Systems, pages 189‚Äì196, 2002.
[64] Q Gu. Neuromodulatory transmitter systems in the cortex and their role in cortical plasticity.
Neuroscience, 111(4):815‚Äì835, 2002.
[65] Angela J Yu and Peter Dayan. Uncertainty, neuromodulation, and attention. Neuron, 46(4):
681‚Äì692, 2005.
[66] Rosalyn J Moran, Pablo Campo, Mkael Symmonds, Klaas E Stephan, Raymond J Dolan, and
Karl J Friston. Free energy, precision and learning: The role of cholinergic neuromodulation.
Journal of Neuroscience, 33(19):8227‚Äì8236, 2013.
[67] Thomas Parr. The Computational Neurology of Active Vision. PhD thesis, University College
London, 2019.
13
[68] Matthew Fellows, Anuj Mahajan, Tim GJ Rudner, and Shimon Whiteson. Virel: A variational
inference framework for reinforcement learning. In Advances in Neural Information Processing
Systems, pages 7122‚Äì7136, 2019.
[69] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and
review. arXiv preprint arXiv:1805.00909, 2018.
[70] Thomas Parr and Karl J Friston. The computational anatomy of visual neglect. Cerebral Cortex,
28(2):777‚Äì790, 2018.
14