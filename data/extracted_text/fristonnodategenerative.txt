Modelling ‘Twenty Questions’ 
 
 1 
Generative models, language and active inference 1 
 2 
Karl J Friston a, Thomas Parr a, Yan Yufik b, Noor Sajid a, Catherine J Price a, Emma Holmes a* 3 
 4 
a The Wellcome Centre for Human Neuroimaging, UCL Queen Square Institute of Neurology, 12 5 
Queen Square, London, WC1N 3AR, U.K. 6 
b Virtual Structures Research, Inc., 12204 Saint James Rd, Potomac, MD 20854, U.S.A. 7 
 8 
Emails: k.friston@ucl.ac.uk, thomas.parr.12@ucl.ac.uk, imc.yufik@att.net, noor.sajid.18@ucl.ac.uk, 9 
c.j.price@ucl.ac.uk, emma.holmes@ucl.ac.uk 10 
 11 
 12 
Declarations of interest: none 13 
 14 
 15 
 16 
 17 
__________________ 18 
* Correspondence: Emma Holmes; The Wellcome Centre for Human Neuroimaging ; UCL Queen 19 
Square Institute of Neurology, UCL, 12 Queen Square, London, UK WC1N 3AR; +44 (0)20 3448 4362; 20 
emma.holmes@ucl.ac.uk 21 
 22 
Modelling ‘Twenty Questions’ 
 
 2 
Abstract 23 
This paper presents a biologically plausible generative model and inference scheme that is capable of 24 
simulating the generation and comprehension of language, when synthetic subjects talk to each other. 25 
Building on active inference formulations of dyadic interactions, we simulate linguistic exchange to  26 
explore generative models that support dialogues. These models employ high-order interactions among 27 
abstract (discrete) states in deep  (hierarchical) models. The sequential nature of language processing 28 
mandates generative models with a particular factorial structure—necessary to accommodate the rich 29 
combinatorics of language.  We illustrate this by simulating a synthetic subject who can play the 30 
‘Twenty Questions’ game. In this game, synthetic subjects take the role of the questioner or answerer, 31 
using the same generative model. This simulation setup is used to illustrate some key architectural 32 
points and demonstrate that many behavioural and neurophysiological correlates of language processing 33 
emerge under variational (marginal) message passing , given the right kind of generative model.  For 34 
example, we show that theta-gamma coupling is an emergent property of belief updating, when listening 35 
to another. 36 
 37 
 38 
Keywords: language; hierarchical; inference; Bayesian; neuronal; connectivity; free energy; message 39 
passing 40 
 41 
Modelling ‘Twenty Questions’ 
 
 3 
1. Introduction 42 
In April, 2018, an international group of experts assembled in Frankfurt for an Ernst Strüngmann forum 43 
addressing complexity and computation in the cortex  (Singer et al., 2019) . One group was briefed to 44 
discuss human cognition and , reflecting the  interests of that group, chose to focus on language 45 
processing—a challenging area for computational neuroscience, linguistics and theoretical 46 
neurobiology (Hauser et al., 2002). What follows is a  formal analysis that speaks to a key conclusion 47 
of that group ; namely, that the neuronal correlates of language processing and functional brain 48 
architectures should emerge naturally, given the right kind of generative model. In brief, this paper uses 49 
simulations of language processing to show that many behavioural and neurophysiological correlates 50 
of language processing emerge under deep diachronic models. 51 
A generative model refers to a probabilistic  mapping from causes (e.g., semantics) to consequences 52 
(e.g., auditory signal). Perception, recognition or inference then becomes the (Bayesian) inversion of 53 
the generative model to infer causes  from consequences. The notion of a generative model rests on a 54 
commitment to the brain as a constructive organ, generating explanations for its sensations. We will use 55 
an active inference (a.k.a., predictive processing) formulation of this basic idea that inherits from a long 56 
tradition of psychological ideas about how the brain works; from Kant through Helmholtz (Helmholtz, 57 
1878 (1971)), from analysis by synthesis (Yuille and Kersten, 2006) to perception as hypothesis testing 58 
(Gregory, 1980), from the Helmholtz machine (Dayan et al., 1995) to the free energy principle (Friston, 59 
2010). Specifically, we w ill use a corollary of the free energy principle; namely, active inference  60 
(Friston et al., 2017a) . The basic idea behind active inference is that any  neuronal processing can be 61 
formulated, in a normative sense, as a minimisation of the same quantity used in approximate Bayesian 62 
inference; i.e., a variational free energy or evidence bound  (Mattys et al., 2005; Winn and Bishop, 63 
2005). 64 
Minimizing variational free energy is equivalent to maximizing the sensory evidence for an internal 65 
model of how unobserved ( i.e., hidden) states o f the world  generate observed (i.e., sensory) 66 
Modelling ‘Twenty Questions’ 
 
 4 
consequences. Technically, this can be formulated in terms of maximising the marginal likelihood for  67 
models of the lived world—that is neatly summarized as self-evidencing (Clark, 2016; Hohwy, 2016); 68 
in other words, gathering sensory evidence for our generative models. Having specified the generative 69 
model one can then use standard, ‘off-the-shelf’ belief updating schemes (Friston et al., 2017c) to create 70 
synthetic agents, who perceive and act in a self-evidencing fashion. These simulations can also be used 71 
to predict empirical behavioural and physiological responses . Here, we use simulations to test 72 
hypotheses about language; such as generation and understanding, in relation to conceptual knowledge 73 
(Barsalou, 2003; Yufik, 1998; Yufik, 2019) , the use of a shared narrative  (Mar, 2011; Mathewson et 74 
al., 2019), and the linearization of language (Bornkessel et al., 2005).  75 
This paper extends a long line of existing work  in the domain of natural language processing (and 76 
response generation). Previously, the focus has been on treating natural language processing as a 77 
learning problem (Elman, 1990) , where the use of deep learning has sp earheaded algorithmic 78 
developments (Young et al., 2018) : e.g., word embeddings derived from learning predictive 79 
relationships (Collobert et al., 2011; Mikolov, 2010; Mikolov et al., 2013; Pennington et al., 2014) and 80 
fully contextualized word representations (Devlin et al., 2018; Radford et al., 2019; Vaswani et al., 81 
2017). These approaches to natural language processing  limit themselves to learning associations —82 
between an input and output —via the training of particular neural networks. In contrast, response 83 
generation—including conversational dialogue agents—have been framed as either deep reinforcement 84 
learning (Li et al., 2016; Zhao et al., 2017) or inference problems (Liu et al., 2018). These approaches, 85 
whilst closely aligned with our work, are optimising objective functions that do not account for the 86 
future, including their ability to have forward-looking conversations, due to word-repetitions or closed-87 
form replies (Li et al., 2016) . In contrast, by framing language as an active (Bayesian) inference 88 
problem, with an underlying generative model, our approach infers causal relationships between inputs 89 
and outputs—and provides a structural understanding of the sequences of words being pres ented and 90 
their context sensitivity. This results in uncertainty resolving actions that lead to forward -looking 91 
Modelling ‘Twenty Questions’ 
 
 5 
conversations: as demonstrated in the simulations that follow, an agent does not need to revisit  issues 92 
that have already been resolved.  93 
The resulting approach also differs from previous cognitive theories of language processing. Although 94 
the idea of ‘surprisal’ has become increasingly prevalent in the literature (Hale, 2001; Levy, 2008), this 95 
usually refers to the magnitude of ‘surprise’ conveyed by individual words, such that expected 96 
semantics are simply an amalgamation of the semantics conveyed by all preceding words. In contrast, 97 
in the current formulation, belief updating occurs at a higher level and relies on beliefs about an acoustic 98 
scene, about which the agent has prior beliefs. Note that the mathematical formulation  used here—99 
which is described in detail in the sections that follow —differs from previous approaches in this 100 
literature. There are two key points to note here. First, the current formulation considers the uncertainty 101 
of the agent’s beliefs about the scene at hand. Second, we introduce an active component —which 102 
generates predictions about the information that an agent will seek to resolve their uncertainty. In other 103 
words: What questions should I ask next , to resolve my uncertainty about the subject of our 104 
conversation? 105 
This paper comprises four sections.  The first (Generative models of language) describes a top-down 106 
approach to understanding functional brain architectures in terms of generative models, with a special 107 
focus on models that are apt for language processing. This section considers the requisite computational 108 
architecture and the second section (Active inference) describes the  accompanying message passing. 109 
The third section (“Twenty Questions” simulations) uses the generative model to illustrate behavioural 110 
and neurophysiological correlates of speaking and listening (Edwards and Chang, 2013; Kayser, 2019; 111 
Lizarazu et al., 2019; Pefkou et al., 2017). This section concludes with a reproduction of some simple 112 
(violation) paradigms, in terms of synthetic event related potentials and difference waveforms—of the 113 
sort seen in mismatch negativity, P300, and N400 studies (Coulson et al., 1998; Van Petten and Luka, 114 
2012). The final section  (Synthetic communication ) turns to communication per se , using dyadic 115 
interactions between two synthetic subjects  to illustrate that certain kinds of belief updating can be 116 
Modelling ‘Twenty Questions’ 
 
 6 
instantiated linguistically. We conclude with a discussion of what has not been addressed; for example, 117 
a sense of agency and the acquisition of language through learning deep models. 118 
 119 
2. Generative models of language 120 
The advantage of focusing on generative models for language—as opposed to recognition models—is 121 
that the same generative model can be used to generate an auditory signal given a narrative (i.e., for 122 
language production) and to infer the narrative given auditory input (i.e., for language understanding). 123 
Here, we focus on simulating a simple agent, who can ask questions and answer them . In this 124 
formulation, the agent does not know whether its beliefs are its own or are generated by some external 125 
narrator. We will return to this issue in the discussion. 126 
So, what are the special requirements of a generative model for language? Here, we take a common -127 
sense approach and list the necessary properties such a model must possess. Starting with the generative 128 
model somewhat simplifies things, in the sense that one only has to specify what would be sufficient to 129 
generate meaningful language . One can then use established inversion schemes for language 130 
understanding. First, we will assume that language is for communication, which immediately implies a 131 
shared forward-looking narrative (Allwood et al., 1992; Brown and Brune, 2012; Friston and Frith, 132 
2015a; Mar, 2011; Schegloff and Sacks, 1973; Specht, 2014). In turn, this implies shared (and evolving) 133 
beliefs about the subject of communication (Mathewson et al., 2019). This simple observation has some 134 
fundamental implications. The first may be slightly counterintuitive and borrows from earlier work on 135 
neuronal hermeneutics (Friston and Frith, 2015a). This work—using generalised synchrony to simulate 136 
communication between songbirds—suggests that it is sufficient to share the same generative model to 137 
infer the meaning of sensory  exchanges between interlocutors. The issue of who is talking and 138 
attribution of agency then becomes a somewhat secondary issue, which is only necessary for turn-taking 139 
(Ghazanfar and Takahashi, 2014; Wilson and Wilson, 2005) . In short, a narrative cannot be uniquely 140 
attributed to you or me—it is our narrative. 141 
Modelling ‘Twenty Questions’ 
 
 7 
The notion of a shared narrative is central to  our formulation of language. Usually, in realising or 142 
simulating active inference (in real artefacts or in silico), outcomes are generated by external states of 143 
the world that agents navigate. These sensory outcomes are then used to update beliefs about  external 144 
states, which are used to plan actions. Policies—which are sequences of actions—change external states 145 
and generate new outcomes. And so, the perception-action cycle continues. However, in the context of 146 
dyadic exchange, outcomes are generated by another person or agent, without any necessary reference 147 
to external states. In this setting, when  an artificial agent speaks, it generates outcomes that are most 148 
consistent with its beliefs which, in turn, update the beliefs of its correspondent. The upshot of this 149 
exchange is a synchronisation or alignment of belief states that—in pure communication—circumvent 150 
any reference to external states of the world.  151 
This alignment follows naturally from generating outcomes that are consistent with beliefs (technically, 152 
outcomes that have the greatest marginal likelihood or model evidence ). Actions and outcomes are 153 
assumed to be isomorphic. Subsequent belief updating based on those outcomes  makes the beliefs of 154 
both subjects consistent with the outcomes  they share. In short, outcomes and beliefs are selected in 155 
concert to maximise model evidence  and, implicitly, the predictability of sensory samples . The  156 
inevitable endpoint of this reciprocal exchange is convergence to the same belief states (Isomura et al., 157 
2019), which ensure the outcomes are generated by one agent are easily predictable, in virtue of the fact 158 
that these are the same outcomes the agent would have produced itself. This kind of generalised 159 
synchronisation has been explored in numerical analyses of communication by birdsong and 160 
intracellular communicati on (Friston et al., 2015 ; Friston and Frith, 2015b; Isomura et al., 2019; 161 
Kuchling et al., 2019). In this paper, we focus on pure communication; in the sense that  all outcomes 162 
are generated by one or another agent. This means there are no  other states of the world to consider . 163 
See Figure 1  for a graphical depiction of the  special conditional dependencies  implied by pure 164 
communication.  165 
Modelling ‘Twenty Questions’ 
 
 8 
 166 
FIGURE 1 167 
Active inference and Markov blankets. This figure illustrates the conditional dependencies among various states 168 
that constitute (active) inference about external states of affairs in the world. Active inference rests upon a four -169 
way partition of states into external states (s) and internal states (s, π) that are separated by Markov blanket states 170 
(o, u). Technically, the Markov blanket of internal states comprises their parents, their children and the parents of 171 
the children. In this figure, blanket states correspond to the pale blue circles. Blanket states comprise observations 172 
or outcomes (o) and action ( u). The upper panel illustrates the standard way in which conditional dependencies 173 
are mediated: internal states are treated as encoding representations of external states. These representations 174 
prescribe action on external states, which generates outcomes. In this construction, internal states play the role of 175 
sufficient statistics or parameters of a posterior belief ( Q) about external states and plans or policies that are 176 
realised by action. These beliefs are optimised by mi nimising a free energy functional of posterior beliefs, given 177 
outcomes. Posterior beliefs about the policies provide a probability distribution from which the next action is 178 
sampled. This action changes external states, which generate outcomes – and so the  (perception-action) cycle 179 
continues. The lower panel shows the simplified scheme used in this paper, labelled ‘Diachronic inference’. In 180 
this setting, actions ( u) and outcomes ( o) are assumed to be isomorphic. In other words, I act by generating an 181 
outcome that minimises free energy. This is equivalent to generating or selecting outcomes that are the most likely 182 
under my beliefs about the causes of that outcome. Because these outcomes are shared between two (or more) 183 
agents, they constitute the Markov blan ket that separates the internal states of every agent in the exchange. This 184 
means the internal states of one agent now constitute the external states of another (and vice versa). Crucially, this 185 
rests upon a diachronic switching, in which only one agent generates outcomes at any one time. Heuristically, this 186 

Modelling ‘Twenty Questions’ 
 
 9 
means that I can either listen or speak but not both at once. With this particular constraint on conditional 187 
dependencies, the shared outcome is (e.g., spoken words) constitute the blanket states that are shared by all agents. 188 
The superscripts in the lower panel denote two agents ( i and j). The equations express the sampling of various 189 
states, or their minimisation with respect to variational free energy. An interesting aspect of the diachronic setup 190 
is that everything minimises a free energy; effectively resolving uncertainty; such that the beliefs of one agent are 191 
installed in another, via an exchange of outcomes. 192 
 193 
 194 
In what follows, we try to show how the belief states of two or more agents become align ed through 195 
pure communication, where this alignment is  an emergent property of selecting beliefs that are 196 
consistent with what is heard while, at the same time, generating outputs that are consistent with those 197 
beliefs. If two or more agents comply with th ese imperatives, their beliefs align, thereby evincing a 198 
minimal form of communication. It is interesting to consider how external states might get into the 199 
game; for example, providing visual cues that affect the beliefs of one agent: i.e., how does one person 200 
convey her beliefs about what she is seeing to another, or how do they reach consensus when they can 201 
see different parts of the same scene? However, in this work, we will just consider pure communication 202 
without external states and focus on how beliefs about a scene are installed by a shared narrative. 203 
So, what is a narrative? On the active inference view, everything we do  can be regarded as pursuing a 204 
narrative that resolves uncertainty (Friston et al., 2017a; Mirza et al., 2016) . This means that the only 205 
sort of narrative that matters is one that has epistemic affordance ; namely, th e opportunity to reduce 206 
uncertainty under a particular belief structure about the world. In this sense, the formal imperatives for 207 
language become exactly the same as any active inference; for example, active vision (Ferro et al., 2010; 208 
Ognibene and Baldassarre, 2014). Indeed, the same principles underlie experimental design in scientific 209 
enquiry, where one solicits data that disambiguate among competing hypotheses (Lindley, 1956). Much 210 
of the motivation for the generative model below inherits from the formally identical problem of 211 
querying the world with saccadic eye movements  during scene construction (Mirza et al., 2 016; 212 
Ognibene and Baldassarre, 2014; Purpura et al., 2003) . In short, we will regard language as acting on 213 
Modelling ‘Twenty Questions’ 
 
 10 
the world to resolve uncertainty, where epistemic foraging has been elevated from visual palpation (i.e., 214 
visual search in active vision) to an interrogation of the world via semantics and semiotics, acquired by 215 
encultured learning (Constant et al., 2019; Creanza and Feldman, 2014; Penn et al., 2008; Rizzolatti 216 
and Craighero, 2004) . Following this analogy to its conclusion, a minimal but sufficient model of 217 
language can then be framed in terms of a series of ‘questions’ and ‘answers’ (i.e., propositions and 218 
responses), in the same way that sampling the world with our sensory epithelia constitutes a ‘question 219 
about what is out there’  (Gregory, 1980). And the subsequent sensory samples provide some salient, 220 
uncertainty reducing, evidence for our beliefs about the world. 221 
With this in mind, we set ourselves the task of formulating a generative model that could play a game 222 
of “Twenty Questions”. In other words, a model that could generate a sequence of questions and closed 223 
“yes/no” answers, which progressively reduce uncertainty about the subject of conversation (i.e., 224 
contextual knowledge). These sorts of sequential language games have extensively been tackled in the 225 
literature: including one round of question-answer ‘whisky pricing’ interaction (Hawkins et al., 2015),  226 
playing restricted ‘cards corpus’ with one -off communication (Potts, 2012), sequential ‘info -jigsaw’ 227 
game (Khani et al., 2018) , ‘hat game’ where agents learn to communicate via observing actions  228 
(Foerster et al., 2016) and conversations about visual stimulus (Das et al., 2017). Having specified the 229 
generative model for our “Twenty Questions” paradigm , we made no further assumptions —we used 230 
off-the-shelf (marginal) message passing to simulate  neuronal processing (Dauwels, 2007; Friston et 231 
al., 2017c; Parr and Friston, 2018; Winn and Bishop, 2005). Exactly the same belief updating scheme, 232 
for partially observed Markov decision processes, has been used in many contexts; ranging from 233 
exploration of mazes and economic game theory, through to abstract rule solving and scene construction 234 
(Friston et al., 2017a) . We anticipated that these simulations would reproduce  key behavioural and 235 
neuronal responses seen in empirical language studies.  236 
 237 
Modelling ‘Twenty Questions’ 
 
 11 
2.1. A deep diachronic model for language 238 
In brief, our generative model has to generate a sequence of questions and answers, under the constraint 239 
that they are articulated as a discrete sequence of continuous outcomes; here, spoken words. This means 240 
that narratives emerge at  several (i.e., discrete and continuous)  levels, which speaks to the deep or 241 
hierarchical aspects of the requisite model. This way of hierarchically framing the conversational 242 
dialogue problem, has previously been explored through the inclusion of two separate (fast and slow) 243 
levels using artificial neural networks (Serban et al., 2016; Sordoni et al., 2015). To illustrate this deep 244 
structure and implicit separation of temporal scales , we considered the problem of  generating a 245 
succession of question and answers that depend upon beliefs about the world. States of the world come 246 
in many flavours. We will refer to these states as hidden factors, where each factor (e.g., ‘colour’) has 247 
a number of discrete states (e.g., ‘red’, ‘green’, ‘blue’ …). The use of factors is known as a mean field 248 
approximation in the variational machine learning literature (Jaakkola and Jordan, 1998; Kschischang 249 
et al., 2001; Sallans and Hinton, 2004; Zhang et al., 2018) and is important for simplifying the form of 250 
the generative model and ensuing inference. In fact, the notion of approximate Bayesian inference using 251 
variational Bayes, is defined operationally in terms of this sort of factorisation.  252 
The problem of specifying a generative model now reduces to specifying the factors that are sufficient 253 
to generate a particular question or answer. These include the form of the question, its content, and the 254 
beliefs about the world that determine the correct answer. By inducing a factorisation between the form 255 
of the sentence and its content, one can finesse the combinatorics of representing all possible questions 256 
with all possible content . In other words, we will a ssume that the brain represents —at some suitably 257 
high level —the form of a question and its content separately, where the two only interact  when 258 
generating an outcome or context for the hierarchical level below.  259 
In this paper, we  consider two hierarchical levels; namely, a conceptual level generating syntax and 260 
semantics, and a lower level generating lexical sequences of words or phrases. One could consider 261 
further levels, all the way down to phonemes and articulation; however, this level of modelling has 262 
Modelling ‘Twenty Questions’ 
 
 12 
already been considered in the context of active listening (Friston et al., under review). We will therefore 263 
restrict the current analyses to the generation and understanding of fully formed words (noting that the 264 
Matlab simulations that accompany this pap er include a full three -level demonstration that supports 265 
spoken answers and questions: please see software note). 266 
So, what does one need to know to generate a sentence? Basically, we need the temporal structure or 267 
syntax of the question, the semantic content—that fills in content words like nouns and verbs—and the 268 
answer (e.g., ‘yes’ or ‘no’). However, to generate syntax and semantics, we need the narrative (e.g., is 269 
this a question or answer?) and the form of the question (e.g., is this a question about where something 270 
is—i.e., location—or what something is—i.e., shape?). We also need to know the states of the world 271 
being described (e.g., contextual or scenic knowledge) and  which particular attributes are being 272 
discussed. These conceptual constructs constitute the highest level of the generative model; namely, 273 
everything one would need to know to specify the syntax and semantics of a lower-level.  274 
In deep models of this sort,  deeper hierarchical levels are constituted by fact ors that change over 275 
progressively longer timescales (Friston et al., 2017d; George and Hawkins, 2009; Kiebel et al., 2009). 276 
This means higher level factors are attributes of a sentence or phrase , while lower level factors may 277 
change from word to word (Davis and Johnsrude, 2003; Specht, 2014). Here, high-level factors include 278 
the narrative structure; namely, is this sentence a prompt, question or answer? If this  sentence is a 279 
question, then what is  the question about; e.g., the location or colour of an object? If the narrative 280 
requires an answer, then there  have to be scenic factors encoding states of the world that render the 281 
answer right or wrong; e.g., the object is ‘red’. Finally, and possibly most importantly, there have to be 282 
factors that support a shared narrative; namely, the shared subject of discussion. We will refer to these 283 
as semiotic factors to emphasise that this kind of factor underwrites communication (Roy, 2005; Steels, 284 
2011). In other words,  semiotic factors entail latent states that exist only in the context of discourse; 285 
e.g., ‘we are discussing the colour of something’. 286 
These four kinds of factors (narrative, question, scenic and semiotic) are sufficient to specify a question 287 
about something, or an answer , generated under beliefs about something. Crucially, some of these 288 
Modelling ‘Twenty Questions’ 
 
 13 
factors depend upon choices or policies and the others do not.  For example, the agent can choose the 289 
form of a question and its semiotic content but cannot change scenic states (i.e., the scene or concept 290 
being discussed). In addition, we will assume that the narrative cannot be changed, in the sense that a 291 
question is always invited by a prompt and is followed by an answer. With this particular construction, 292 
agents can update their beliefs about scenic states on the basis of their beliefs about the current semiotic 293 
state and responses to questions. In other words, hidden states of the world  can be communicated via 294 
shared semiotics that rest upon lawful answers to questions under a  shared generative model. One can 295 
intuit that this generative model requires high-order interactions among the factors in play to generate 296 
sentences. In other words, the contingencies that generate a sequence of veridical questions and answers 297 
necessarily entail the interactions or conjunctions among several factors.  Much of what follows is an 298 
attempt to illustrate these interactions using worked examples. 299 
Having specified the  form and semiotic content of a sentence, one can now  generate a sequence of  300 
words in a subordinate level of a generative model that is equipped with probabilistic transitions among 301 
lexical states. The implicit transitions from word to word are prescribed by the narrative and question 302 
factors of the higher level to generate a syntax, while semantic content is specified by the semiotics. 303 
These two attributes (syntactic and semantic) constitute, in the example below, the hidden factors of the 304 
lower level of the model. Finally, given these two factors one can generate the appropriate lexical 305 
sequence of words; again, via an interaction of (syntactic and semantic) factors. 306 
 307 
2.2. A generative model for “Twenty Questions” 308 
Figure 2 provides a schematic illustration of this kind of generative model and fills in some of the details 309 
(please see figure legend). This example will be used later to illustrate a simplified version of ‘Twenty 310 
Questions’, where a subject has to determine the configuration of two hidden objects by asking a series 311 
of closed questions in response to a prompt. The two objects are placed on top of each other and each 312 
object can either be a square or a triangle , which can be either red or green. This means that an id eal 313 
Modelling ‘Twenty Questions’ 
 
 14 
(active) Bayesian observer should be able to disclose the configuration with four questions: two 314 
questions to establish the colour and shape of the lower object and two questions do the same for the 315 
upper object. However, this depends upon asking the right questions, in relation to updated beliefs based 316 
upon previous answers. It is this epistemic, uncertainty reducing aspect of communicative exchange we 317 
hoped to demonstrate and characterise.  318 
 319 
 320 
 321 
FIGURE 2 322 
A generative model for Twenty Questions: This figure provides a schematic illustration of the generative model. 323 
This schematic displays the architecture involved in generating a sequence of words that could constitute a 324 
language-like narrative. In brief, this is a hierarchical (i.e., deep) generative model formulated in terms of discrete 325 
hidden states and outcomes (here, outcomes from the lower level are single words). The architecture is deep 326 

Modelling ‘Twenty Questions’ 
 
 15 
because there are two levels of hidden states, where the higher (deeper) level unfo lds slowly in time—furnishing 327 
contextual constraints on the lower level that generates a sequence of words. The higher level contains hidden 328 
factors that generate the syntax and semantic content of a sentence, which are passed to the lower -level. Each 329 
panel uses a coloured shape to describe the different states of each factor. At the higher level, transitions among 330 
narrative states ( B(2)) generate a sequence of phrases that cycle in a particular order through “Prompts”, 331 
“Questions” and “Answers”, where their form depends upon interactions with other hidden states in the generative 332 
model. The form of questions has been factorised into the type of question (“Shape”, “Location”, or “Colour”) 333 
and its semiotic content. The semiotic content has three factors (noun, objective and adverb), each with two states 334 
(noun: “square” or “triangle”; adjective: “green” or “red” and adverb: “below” or “above”). Similarly, the four 335 
scenic factors correspond to beliefs about the attributes of upper and lower objects in the wo rld; namely, their 336 
colours (green or red) and shapes (square or triangle). In this generative model, choices about the type of question 337 
and its semiotic content are policy-dependent—as illustrated by the red arrows. In other words, policies determine 338 
transitions (encoded by the B(2) matrices) among controllable states, so that question and semiotic states are 339 
selected intentionally. For example, the question generated by the thick red arrows in the figure would be: “Is a 340 
red triangle above?” The combination of these states completely determines the syntax and semantic content of a 341 
sentence at the lower level (which is encoded in the matrix D(1)). The hidden syntax states at the lower level 342 
comprise specific words, such as “Ready” and “Is”, grammar, such as “ ?” or “!”, and abstract representations, 343 
such as noun, adverb, and adjective. The words denoted by the abstract representations are determined by the 344 
semantic factor,  which is isomorphic with the semiotic factor of the higher level. The first word of the phrase 345 
corresponds to the initial syntactic state at the lower level—which is determined by the interactions among states 346 
at the higher level, encoded by the mapping D. For example, if the narrative state is a Question, then the initial 347 
syntax state is the word “Is”, no matter which of the three question states are selected at the higher level. The B(1) 348 
matrices then determine subsequent words (illustrated by the black arrows), by specifying transitions among 349 
syntax states that do depend upon the question states at the higher level. However, if the narrative state is Answer, 350 
then the initial syntax state can be “Yes” or “No”, depending upon high order interactions among the remaining 351 
high-level states: a “Yes” will be generated when, and only when, scenic and semiotic states are congruent (e.g., 352 
if the question “Is a red triangle above?” admitted a positive response, because a red triangle is in the upper 353 
location). For clarity, some syntaxes have been omitted; for example, a “Not sure” answer. In addition, this figure 354 
omits embellishments that generate synonymous phrases (e.g., “not sure”, “can't say”, and so on). The final stage 355 
is to map states at the lower level to outcomes at each time step of the generative process. This is denoted by the 356 
likelihood mapping A(1). In the example highlighted here, the articulated word “triangle” depends upon the current 357 
syntax state being a noun and the associated content being a “triangle”. States without arrows are absorbing states; 358 
in other words, the state only transitions to itself. 359 
 360 
 361 
Modelling ‘Twenty Questions’ 
 
 16 
The particular levels of the factors in the generative model of Figure 2 have been constructed to create 362 
a minimal model of how one agent can communicate beliefs about scenic states (i.e., configurations of 363 
hidden objects) to another. At the higher level, the model incorporates beliefs about the part of the 364 
narrative that is enacted (prompt, question, or answer), the type of question (shape, location, or colour), 365 
the putative scenic state of the world, and a semiotic factor indicating the topic of discussion. These  366 
factors generate expectations for the lower level: namely, the syntax (i.e., the ordering and content of 367 
words) and the semantics (i.e., which shapes, colours, and locations the agent is being questioned on ). 368 
The lower level thus generates sequences of words, which  are concatenated to  form phrases—and 369 
sequences of phrases (i.e., exchanges) occur as the higher level cycles through the lower level. 370 
Note that the ‘syntaxes’ included here would not all be consid ered as syntax under traditional 371 
definitions. In the current implementation, syntaxes are just sequences of states (words), with grammar 372 
used as terminating states to indicate that the conversational turn has ended. 373 
The repertoire of syntactic structures within this model is limited to three sorts of questions and two 374 
sorts of answers; however, even with this limited repertoire , the combinatorics of what could  said is 375 
non-trivial. To ask a particular question, the subject has to choose the form of the question and the levels 376 
of the three semiotic factors by selecting the appropriate policy. To make sense of any answer, it also 377 
has to remember these choices. This memory is endowed by a higher level that maintains beliefs about 378 
controllable (i.e., question and semiotic) factors, after committing to a particular policy. The selected 379 
policy minimises uncertainty and will therefore change with beliefs about the hidden scene, over 380 
successive questions and answers , ensuring forward -looking exchanges (Allwood et al., 1992) . Note 381 
that this kind of working memory —and epistemic behaviour —emerges naturally from maximising 382 
model evidence (i.e., minimising variational free energy), given a generative model of successive states 383 
of the world that generate outcomes. 384 
We included a prompt state simply to demonstrate the cycling between prompts, questions, and answers. 385 
In this formulation, the prompt conveys no interesting information: it is merely part of the structured 386 
Modelling ‘Twenty Questions’ 
 
 17 
dialogue. In the following simulations, we simply use it to convey the type of turn -taking that is often 387 
seen in realistic settings. 388 
Clearly, there are many ways in which we could carve up the factors or causes that underwrite linguistic 389 
exchanges, and we have ignored many interesting aspects; however, the basic message is that one needs 390 
to consider the factorisation and the deep (hierarchical) nature of generative models before dissecting 391 
the computational architecture of language. In what follows, we consider more broadly how generative 392 
models of this sort can be represented in graphical form, and how variational message passing generates 393 
predictions for neuronal dynamics.  394 
 395 
3. Active inference 396 
The previous section considered the form of a generative model. We can now use active inference to 397 
simulate action and perception under that model. The procedures  used here  assume that the brain  398 
restricts itself to a limited number of  characteristic states (Friston, 2013)—a property that all sentient 399 
systems must possess . Mathematically, these procedures minimise surprise (in information theoretic 400 
terms), which is equivalent to maximising Bayesian model evidence; in other words, they maximise the 401 
probability of sensory exchanges with the environment , under a  generative model of how those 402 
sensations were caused. This is the essence of active inference , and implicit self-evidencing (Hohwy, 403 
2016). 404 
Intuitively, self-evidencing means the brain can be described as inferring the causes of sensory samples 405 
while, at the same time, soliciting sensations that are the least surprising (e.g., not looking at the sun 406 
directly or maintaining thermoreceptor firing within a physiological range). Technic ally, this take on 407 
action and perception can be cast as minimising a proxy for surprise, known as variational free energy. 408 
From a statistical perspective, variational free energy can be decomposed into complexity and accuracy, 409 
such that minimising variational free energy provides an accurate account of some data in the simplest 410 
Modelling ‘Twenty Questions’ 
 
 18 
way possible (Maisto et al., 2015). Crucially, active inference generalises Bayesian inference, such that 411 
the objective is not just to infer the latent or hidden states that cause sensations but to act in a way that 412 
minimises expected surprise. In information theory, expected surprise is known as entropy or 413 
uncertainty. This means, one can define optimal behaviour as acting to resolve uncertainty : e.g., 414 
saccading to salient, or information rich, regimes of visual space or avoiding outcomes that are, a priori, 415 
costly or unattractive. In the same way that direct action and perception minimise free energy, action 416 
can be specified in terms of plans or policies that minimise the free energy expected on pursuing that 417 
policy. 418 
This section briefly reviews parts of active inference that are relevant to the current paper. We begin by 419 
explaining expected free energy. We then consider how active inference is applied to discrete generative 420 
models, such as the model described in the previous section. Finally, we consider how belief updating 421 
can be implemented as a neuronally plausible message passing scheme. 422 
 423 
3.1. Expected free energy 424 
Expected free energy has a relatively simple form (see Appendix 1), which can be decomposed into an 425 
epistemic, information seeking , uncertainty reducing  part (intrinsic value ) and a pragmatic,  goal 426 
seeking, cost aversive part (extrinsic value). Formally, the expected free energy for a particular policy 427 

 can be expressed in terms of posterior beliefs 
( , ) ( | ) ( | )Q o s P o s Q s     =  about outcomes and 428 
states of the world at time 
  in the future: 429 
intrinsic value extrinsic value
( , ) [ln ( | , ) ln ( | )] [ln ( )]QQG E Q s o Q s E P o       − − −
    (1) 430 
Extrinsic (i.e., pragmatic) value is simply the expected value of a policy defined in terms of outcomes 431 
that are preferred a priori; where the equivalent cost corresponds to Bayesian risk or prior surprise (see 432 
Table 1 and Appendix 2) . The more interesting part is the unce rtainty resolving or intrinsic ( i.e., 433 
Modelling ‘Twenty Questions’ 
 
 19 
epistemic) value, variously referred to as relative entropy, mutual information, information gain, 434 
Bayesian surprise or the value of information expected under a particular policy (Barlow, 1961; 435 
Howard, 1966; Itti and Baldi, 2009; Linsker, 1990; Optican and Richmond, 1987) . An alternative 436 
formulation of expected free energy can be found in Appendix 1 : this formulation rearranges the 437 
equation for expected free energy , so that it is cast as the expected uncertainty about outcomes (i.e. 438 
ambiguity or expected inaccuracy) plus the Kullback -Leibler divergence between predicted and 439 
preferred outcomes (i.e., risk or expected complexity). This formulation shows that minimising expected 440 
free energy is guaranteed to realise preferred outcomes, while resolving uncertainty about the states of 441 
the world generating those outcomes. 442 
Here, we  are less concerned with the pragmatic aspect of expected free energy  and focus on the 443 
epistemic drive to reduce uncertainty. We have previously addressed this epistemic affordance in terms 444 
of saccadic eye movements —to provide a constructivist explanation for visual searches: c.f., scene 445 
construction (Hassabis and Maguire, 2007; Mirza et al., 2016) . In this paper , we use a more 446 
sophisticated generative model to illustrate the same sort of epistemic foraging, mediated by linguistic 447 
exchange. It is worth bearing in mind that the purposeful, inquisitive and abductive behaviours we will 448 
see later are all emergent properties of minimi sing (expected) free energy. In other words, there is no 449 
need to handcraft any rules or grammar, or provide any reinforcement or feedback. All of the behaviours 450 
shown in this paper result from the structure of the generative model. Subsequent sections will illustrate 451 
the belief updating under this model—and so, first, we consider how belief updating relates to neuronal 452 
processes and action. 453 
 454 
3.2. Belief updating and neuronal dynamics in discrete generative models 455 
This section focuses on generative models of discrete outcomes caused by discrete states that cannot be 456 
observed directly (i.e., hidden states). This summary is based on (Friston et al., 2017c), which contains 457 
more detail for readers who are interested in the approach . In brief, the unknown variables in these 458 
Modelling ‘Twenty Questions’ 
 
 20 
models correspond to  states of the world —that generate outcomes —and the policies that generate 459 
successive states. For simplicity, we introduce belief updating in terms of a generic discrete generative 460 
model, which has a single level; we then extend this  description to discrete hierarchical models 461 
containing two levels. 462 
Figure 3 describes the basic form of these generative models in two complementary formats, and the 463 
implicit belief updating following the observation of new (sensory) outcomes. The equations on the left 464 
specify the generative model in terms of a probability distribution over outcomes, states and policies 465 
that can be expressed in terms of  marginal densities or factors. These factors are condi tional 466 
distributions that entail conditional dependencies —encoded by the edges in the Bayesian network on 467 
the upper right. The model in Figure 3 generates outcomes in the following way. First, a policy (i.e., a 468 
plan or controlled action sequence) is selected at the highest level using a softmax function of the free 469 
energy expected under plausible policies . Sequences of hidden states are then generated using the 470 
probability transitions specified by the selected policy (encoded in B matrices). As the policy unfolds, 471 
the states generate probabilistic outcomes at each point in time (encoded in A matrices). 472 
The equivalent representation of this graphical model is shown as a Forney factor graph on the lower 473 
right. Here, the factors of the generative model (numbers in square boxes) now constitute the nodes and 474 
the (probability distribution over the) unknown  states are associated with edges. The rules used to 475 
construct a factor graph are simple : the edge associated with each variable is connected to the facto rs 476 
in which it participates. 477 
Given a generative model of the form shown in Figure 3 , one can construct a neuronally plausible 478 
message passing scheme, whose solution minimises free energy. This involves associating the logarithm 479 
of an expected hidden state, under a particular policy, with  the average transmembrane potential of a 480 
neuronal population 
,, ln   =νs . By introducing an auxiliary  variable (i.e., state prediction error) , 481 
one obtains the following update scheme , whose solution satisfies the belief  update equations in the 482 
lower left panel of Figure 3. 483 
Modelling ‘Twenty Questions’ 
 
 21 
, , 1 , 1 , , 1 ,
,,
,,
ln ln ln ln
()
o            
   
   
− − += +  +  −
=
=
ε B s B s A s
v ε
sv
     (2) 484 
 485 
 486 
 487 
FIGURE 3 488 
Generative models for discrete states and outcomes. Upper left panel: These equations specify the generative 489 
model. A generative model is the joint probability of outcomes and their (latent or hidden) causes, see first 490 
equation. Usually, the model is expressed in terms of a likelihood (the probability of consequences given causes) 491 
and priors over causes. When a prior depends upon a random variable it is called an empirical prior. Here, the 492 
likelihood is specified by a matrix A, whose elements are the probability of an outcome under every combination 493 
of hidden states. The empir ical priors pertain to probabilistic transitions (in the B matrix) among hidden states 494 
that can depend upon action, which is determined probabilistically by policies (sequences of actions encoded by 495 

Modelling ‘Twenty Questions’ 
 
 22 
π). The key aspect of this generative model is that polic ies are more probable a priori if they minimise expected 496 
free energy G, which depends upon prior preferences about outcomes or costs encoded by C. Finally, the vector 497 
D specifies the initial state. This completes the specification of the model in terms of  its parameters; namely, A, 498 
B, C and D. Bayesian model inversion refers to the inverse mapping from outcomes to causes; i.e., estimating the 499 
hidden states that cause outcomes. In approximate Bayesian inference, one specifies the form of an approximate 500 
posterior distribution. This particular form in this paper uses a mean field approximation, in which posterior beliefs 501 
are approximated by the product of marginal distributions over time points. Subscripts index time (or policy). See 502 
Section 2 and Table 1 for a  detailed explanation of the variables (italic variables represent hidden states, while 503 
bold variables indicate expectations about those states). Upper right panel: This Bayesian network represents the 504 
conditional dependencies among hidden states and how t hey cause outcomes. Open circles are random variables 505 
(hidden states and policies) while filled circles denote observable outcomes. Squares indicate fixed or known 506 
quantities, such as the model parameters. Lower left panel : these equalities are the belief updates mediating 507 
approximate Bayesian inference and outcome selection. When the agent is responsible for generating outcomes 508 
(e.g., speaking), they are selected to minimise free energy or, in other words, maximise accuracy under posterior 509 
beliefs about the next state of the world. Lower right panel: this is an equivalent representation of the Bayesian 510 
network in terms of a Forney or normal style factor graph. Here the nodes (square boxes) correspond to factors 511 
and the edges are associated with unknown variables. Filled squares denote observable outc omes. The edges are 512 
labelled in terms of the sufficient statistics of their marginal posterior. Factors have been labelled in terms of the 513 
parameters encoding the associated probability distributions (on the upper left). The circled numbers correspond 514 
to the messages that are passed from nodes to edges (the labels are placed on the edge that carries the message 515 
from each node). The key aspect of this graph is that it discloses the messages that contribute to the posterior 516 
marginal over hidden states; here, conditioned on each policy. These constitute [ forward: ] messages from 517 
representations of the past, [ backward: ] messages from the future and [ likelihood: ] messages from the 518 
outcome. Crucially, the past and future are represented at all times so that a s new outcomes become available, 519 
with the passage of time, more likelihood messages participate in the message passing; thereby providing more 520 
informed (approximate) posteriors. This effectively performs online data assimilation (mediated by forwarding 521 
messages) that is informed by prior beliefs concerning future outcomes (mediated by backward messages). Please 522 
see Table 1 for a definition of the variables in this figure.  Adapted with permission from (Friston et al., 2017c). 523 
 524 
 525 
Modelling ‘Twenty Questions’ 
 
 23 
Although we employ a marginal message passing sche me in the simulations presented later, the  526 
derivations presented here use a mean-field approximation to simplify the expressions. While we could 527 
have used a mean field approximation and the ensuing variational message passing, this tends to lead 528 
to overconfident inferences. Practically, there is li ttle difference between the two:  both rely upon the 529 
synthesis of local messages from the Markov blankets of variables in the factor graph. 530 
These differential equations correspond to a gradient descent on variational free energy as described in 531 
(Friston et al., 2017a) and Appendix 2: 532 
1
,,
,
( | ) 1
accuracycomplexity
( , )
( , ) [ [ ( | ) || ( | , )]] [ln ( | )]Qs
F
F E D Q s P s s E P o s


   

 
     

   
− −
= =− 
=
=−

Fv ε s
F
    (3) 533 
Crucially, they also furnish a process theory for neuronal dynamics, in which the sigmoid function can 534 
be thought of as a sigmoid (firing rate) activation function of transmembrane potential. This means log 535 
expectations about hidden states can be associated with depolarisation of neuronal populations encoding 536 
expectations. This has some construct validity in relation to theoretical proposals and empirical work 537 
on evidence accumulation (de Lafuente et al., 2015; Kira et al., 2015)  and the neuronal encoding of 538 
probabilities (Deneve, 2008) . Equivalent update s can be derived for beliefs about policies and the 539 
precision of those  beliefs. Although omitted from Figure 3 for simplicity, the expected precision of 540 
beliefs about policies is interesting because it has all the hallmarks o f phasic dopamine dynamics. We 541 
will look briefly at simulated dopaminergic firing later. Interested readers are referred to (Friston et al., 542 
2017a; Friston et al., 2014) for details. 543 
As noted above, in this ( pure communication) setting, outcomes are generated by the agent who is 544 
currently speaking. These outcomes are those that minimise variational free energy. These are simply 545 
the outcomes that maximise accuracy: 546 
Modelling ‘Twenty Questions’ 
 
 24 
1
11
11
min
[ln ( | )]
(ln )
oo
oQ
o
E P o s
o



+
++
++
=
=−
=− 
G
G
As
        (4) 547 
This follows from the fact that the complexity part of free energy does not depend upon outcomes. This 548 
sort of out come is formally related to motor output under active inference; namely, the fulfilment of 549 
proprioceptive predictions by classical reflexes (Adams et al., 2013; Shipp et al., 2013). In the current 550 
simulations, words or  phrases are generated, which play the equivalent role of fulfilling predictions 551 
based upon beliefs about hidden states at each point in time. 552 
The final step is to create deep generative models by stacking generative models on top of each other; 553 
such that the outcomes generated by one level provide (empirical) priors on the initial states of the level 554 
below. By linking hierarchical levels in this fashion, states at the higher level change slowly over time, 555 
because states higher level remain the same throughout a sequence of state transitions at the lower level. 556 
In the current setting, this means that beliefs about successive words at the lower level are updated on 557 
a faster timescale than beliefs about a phrase at the higher level—obliging a phrase to consist of multiple 558 
words. Top-down (empirical) priors from the higher level provide a context for inference about the next 559 
word, which is informed by all the preceding words in a senten ce. This is an important aspect of deep 560 
temporal models that lends inference a hierarchical nature; known technically as a semi-Markovian 561 
process. Figure 4 illustrates the hierarchical form of the generative model (upper panels) and the 562 
accompanying message passing scheme (lower panels) in the form of a factor graph.  Note that Figure 563 
4 is simply an extension of Figure 3. At the higher level, the likelihood mapping from hidden states to 564 
outcomes (A) from Figure 3 is replaced by a mapping from hidden states in the higher level to the initial 565 
states of a lower-level (denoted by D). These mappings allow interactions between states at the higher 566 
level to influence states in the lower level. 567 
In the spirit of Figure 4, Figure 5 shows the particular Forney style factor graph for the generative model 568 
of Twenty Questions in Figure 2. Here, the hidden states have been unpacked into their factors, where 569 
controllable states are labelled in red. As noted above, controllable states have  transition probabilities 570 
Modelling ‘Twenty Questions’ 
 
 25 
that are prescribed by a policy. For example, for the semiotic factor noun, the available policies could 571 
move the semiotic state ‘square’ to ‘triangle’. Crucially, it can be seen that the policy remains in play 572 
for a succession of  state transitions. In other words, once a policy is inferred or selected, it remains 573 
operational in terms of predicting successive outcomes. For example, if I commit to the semiotic state 574 
‘square’, then it remains the subject of the next question and subsequent answer. 575 
Equipped with this model and variational message passing scheme, we are now in a position to simulate 576 
conversations; both in terms of belief updating and associated neuronal message passing. When the 577 
agent is listening, the outcomes can be g enerated by another agent to simulate  dyadic exchange. 578 
Conversely, when the agent is talking, outcomes are selected to minimise the free energy  under the 579 
agent’s beliefs. In other words, when the agent is talking, it selects the least surprising words, given its 580 
beliefs about the current syntax and semantics. Notice that the agent does not ‘know’ who is talking—581 
it just expects to hear things that are consistent wi th its beliefs. If it hears something that is surprising 582 
or unexpected, the agent will update its beliefs about the scene and semiotics currently in play. More 583 
importantly, the agent’s  beliefs about what is being said depend upon the policies inferred. The se 584 
policies minimise expected free energy, which means the agent expects to encounter salient, informative 585 
answers and, crucially, questions. In other words, it expects to hear questions and answers that resolve 586 
uncertainty, which will be the same as the questions it would ask and the answers it would supply. 587 
When considering hierarchical generative models of language processing , we are confronted with the 588 
linearization problem (Bornkessel et al., 2005): namely, how are outcomes supplied to higher levels of 589 
the generative model and, how  do higher levels provide constraints on evidence gathering at lower 590 
levels? In other words, how can one accumulate evidence from sequential stimuli to form beliefs about 591 
things that do not change with time ? Happily, t his problem  that has already been  solved by deep 592 
temporal models of the sort above. We demonstrate the implicit message passing and belief updating 593 
that underwrites this form of ( linearised) evidence accumulation in the next section, by simulating an 594 
agent playing the “Twenty Questions” game. 595 
Modelling ‘Twenty Questions’ 
 
 26 
 596 
FIGURE 4 597 
Deep temporal models. Left panel : This figure provides the Bayesian network and associated Forney factor 598 
graph for deep temporal models, described in terms of factors and belief updates on the left. The graphs adopt the 599 
same format as Figure 3 ; however , here the model has been extended hierarchically, where (bracketed) 600 
superscripts index the hierarchical level. The key aspect of this model is its hierarchical structure that represents 601 
sequences of hidden states over time or epochs. In this model, hidden states at higher levels generate the initial 602 
states for lower levels , which unfold to generate a sequence of outcomes: c.f., associati ve chaining (Page and 603 
Norris, 1998). Crucially, lower levels cycle over a sequence for each transition of the level above. This is indicated 604 
by the subgraphs enclosed in dashed boxes, which are ‘reus ed’ as higher levels unfold. It is this scheduling that 605 
endows the model with deep temporal structure. The probability distribution over initial states is now conditioned 606 
on the state (at the current time) of the level above. Practically, this means that D now becomes a tensor, as 607 
opposed to a vector. The messages passed from the corresponding factor node rest on Bayesian model averages 608 
that require the expected policies [message ] and expected states under each policy. The resulting averages are 609 
then used to compose descending [message ] and ascending messages [message ] that mediate the exchange 610 
of empirical priors and posteriors between levels, respectively. Adapted with permission from (Friston et al., 611 
2017c). 612 

Modelling ‘Twenty Questions’ 
 
 27 
 613 
FIGURE 5 614 
Factor graph for 20 questions: this schematic illustrates the message passing using a Forney style factor graph 615 
for the generative model in Figure 2, using the format of Figure 4. In this schematic, we have unpacked the hidden 616 
state factors, labelling those with multiple (policy -dependent) probability transition matrices in red. This graphic 617 
was produced automatically using the SPM software (please see software note).  618 
 619 
 620 
4. “Twenty Questions” simulations 621 
To illustrate belief updating —and its neuronal correlates —we use a simplified version of “ Twenty 622 
Questions”. Specifically, we simulated conversations comprising six exchanges, where each exchange 623 
comprises three phrases or sentences. The phrases always followed the same sequence : a prompt, a 624 
question, then an answer. This order was fixed by specifying very precise priors about transitions among 625 

Modelling ‘Twenty Questions’ 
 
 28 
narrative states. Each phrase comprised up to six words, and each word was processed with belief 626 
updates described by Equation 3. These updates were evaluated in 16 time-steps of 16 ms (of simulated 627 
and approximately real time). This meant that words were generated every 256 ms, such that a sentence 628 
of four words takes about a second to articulate. In these simulations, the artificial agent could take the 629 
role of the questioner or the answer er: the agent either listened for the  prompt, asked a question, and 630 
then listened to the answer, or issued the prompt, listened for a question, then supplied the answer. In 631 
all cases, the agent (slightly) preferred affirmative answers  (“Yes”) over negative answers (“No”). 632 
These preferences were specified by setting prior costs of C = –¼ for “Yes” and C = ¼ for “No” (see 633 
Table 1). This means that the agent  will ask questions that  it believes will elicit a “Yes” answer , 634 
everything else being equal. 635 
In these simulations, the agent started out with uniform prior beliefs about which colour and shape was 636 
present at the two locations (above and below).  It played the role of the questioner for the first four 637 
exchanges, after which it identified the colours and shapes of both objects with high confidence. Having 638 
updated its beliefs, it then switched roles to answer two questions. To allow the agent to play the roles 639 
of the questioner and answerer for these simulations, we separated the agent’s generative model from 640 
the generative process; effectively, this means that the agent was ‘in conversation’ with the generative 641 
process. The generative process  had exactly the same for m as  the generative model, except the 642 
generative process had more precise beliefs about the scene. Here, as the agent was not equipped with 643 
beliefs about whether it was speaking or listening, we simulated t urn-taking by sampling the output 644 
from the agent or generative process at the appropriate stages of the exchange.  645 
Figure 6 displays the results of belief updating. Each panel shows posterior expectations about the two 646 
hidden objects after each of the six  questions were answered. The questions are shown in black text  647 
towards the top of the panel, while the answer is shown at the bottom of the panel. All answers in this 648 
example were correct (i.e., correspond to the true scene), so are displayed in green. Within each panel, 649 
the agent’s beliefs about  shape (square versus triangle ) and colour (green versus red) at the two 650 
locations are depicted with large icons. The true (narrative) scene is shown with small icons to the right.  651 
Modelling ‘Twenty Questions’ 
 
 29 
 652 
FIGURE 6 653 
Behavioural responses: Each panel shows the posterior expectations of a synthetic subject after its question had 654 
been answered. The  agent’s beliefs about  shape (square versus triangle) and colour (green versus red) for the 655 
upper and lower locations are depicted with large icons. Where the agent has no particular (i.e., uniform) beliefs, 656 
the two shapes are displayed overlaid and/or in grey (e.g., upper locations in panels A and B); where the agent’s 657 
beliefs tend toward a particular colour, the shape is shaded slightly red or green.  The true scene (with veridical 658 
colours and shapes) is shown with small icons to the right. The question is shown in black text (above each set of 659 
expectations), while the answer is shown below. All of the answers in this simulation are correct, so they are 660 
displayed in green text. The human icons and purple callouts are positioned next to the agent’s vocalisations, to 661 
illustrate whether the subject was asking questions (first four exchanges) or answering them (last two exchanges). 662 
 663 
 664 

Modelling ‘Twenty Questions’ 
 
 30 
During the first four questions, the agent accumulates evidence and builds veridical beliefs about the 665 
scene at hand. At the beginning, it has no particular (i.e. uniform) beliefs about the shapes and colours 666 
at the two locations. First, it chooses to ask a question about the shape, because it is more likely to get 667 
an affirmative (preferred) response than if  it were to ask a question about shape and colour together. 668 
After the first answer, it knows there is a square below (see first panel) and subsequently asks a question 669 
about the combination of shape and colour.  After the second answer (see second panel), it knows that 670 
the square is not green and must therefore be red. It then goes on to ask similar questions for the upper 671 
location, after which time it holds precise beliefs about the shapes and colours at the two locations. By 672 
the time it answers the fifth and sixth questions,  the agent can provide veridical answers to questions 673 
about specific scene components (the fact these responses are in green text indicates that the answers 674 
are correct).  675 
Notice that the expectations of the colour (red) of the lower panel become less precise after first inferring 676 
there is a red square  below (compare second and third panels) . This arises because we have slowed 677 
down belief updating, so that its time constants correspond roughly to those observed empirically (see 678 
below). This precludes complete convergence to free energy minima during belief updating. The 679 
ensuing uncertainty is then carried over to the next exchange. Further , notice that after responding 680 
correctly to the question about the colour of the lower square  (fifth question; lower middle panel), the 681 
agent’s beliefs are refreshed as the answer provides confirmatory evidence about what was believed. 682 
As anticipated, the artificial agent resolved uncertainty about the hidden scene after only four questions, 683 
suggesting that appropriate questions were asked. For example, the first question establishes that there 684 
is a square below, while the second discloses the fact that it is red. It could have opted to ask only What 685 
questions, but then it could have needed as many as 8 questions to infer the correct scene. Notice also 686 
that the second question is not redundant: it is asked in the context of knowledge that the lower object 687 
is square. A possible second question would have been to ask: “Is there a circle below?”, but given (i) 688 
the agent already knows the lower object is a square and (ii) in this scenario only one object is present 689 
at each location, this question would not reduce uncertainty about the contents of the scene. Ultimately, 690 
Modelling ‘Twenty Questions’ 
 
 31 
the behaviour demonstrated in these simulations  emerges because the agent selected  policies that  691 
reduced uncertainty about the scene . This can only happen because the  generative model entertains 692 
future states, which enable the agent to evaluate expected outcomes in the future. For example, any 693 
answer to the second question (“Is a red square below” ) completely resolves uncertainty about its 694 
colour. The agent knows this question will resolve uncertainty before the question is even asked. Thus, 695 
this type of question has epistemic value.  696 
Note the subtle nature of this epistemic behaviour: the agent is using semiotics states (noun, adjective, 697 
and adverb), over which it (believes it) has control, to resolve uncertainty about scenic states, over 698 
which it (believes it) has no control. In this scenario, the agent exerts c ontrol by generating outcomes 699 
(e.g., questions); it will generate outcomes  that are the least surprising, under uncertainty resolving 700 
policies. This vicarious belief updating is central to the current formulation when considering how we 701 
might install beliefs in others through linguistic communication. 702 
 703 
4.1. Message passing and neurophysiology 704 
Having illustrated belief updating behaviour, we now take a closer look at the predictions of this type 705 
of inference, or sequential evidence accumulation,  for neurophysiology. Figure 7  illustrates 706 
electrophysiological and dopaminergic responses to the six questions from the simulation above. These 707 
responses are shown in various formats: Figure 7A shows posterior expectations about the colour of the 708 
lower object at various times during the sequence of six narratives, displayed in raster plot format. On 709 
this plot, each prompt, question, and answer, is assumed to last for 250 ms.  For simplicity, this figure 710 
displays only the colour states. Different units are labelled on the Y -axis; namely, green or red at 711 
successive epochs (1, 2, and 3) within each exchange.  712 
During the time window corresponding to the first question and answer (0–0.75 seconds), Figure 7A is 713 
shaded in grey, indicating that the agent has uniform beliefs about the colo ur of the object during the 714 
first question, which queries the shape and not colour. The second question asks about the colour of the 715 
Modelling ‘Twenty Questions’ 
 
 32 
lower object. This question Figure 6B has the answer “No” (i.e., not green), indicating that the colour 716 
of the square at the lower position must be red. The plot shows that this answer induces profound belief 717 
updating; the  belief that the lower square is red is very precise and this  belief is  maintained (i.e., 718 
‘remembered’) throughout the exchange (i.e., for the remainder of the time plotted) . During this time, 719 
the shading on this plot allows us to visualise the reduction in precision for the belief that the object is 720 
red, and subsequent reinstatement of precision  after the fifth question —as discussed in the  previous 721 
section. 722 
Notice in Figure 7A that the latency of this belief updating for expectations at the beginning of the trial 723 
is greater than at the end—this is due to message-passing backwards in time (message  in Figure 4). 724 
As noted above, these posterior beliefs decay a little over subsequent trials, until the agent reaffirms its 725 
conviction that the lower colour is indeed red.  726 
Figure 7B shows the same data in a different format. Here, pooled expectations (after filtering between 727 
4 and 32 Hz ) are shown as a white line. This is superimposed upon a time -frequency heat map to 728 
illustrate bursts of frequency-specific energy during periods of belief updating. We will examine this 729 
characterisation in more detail later, with reference to the next figure.  730 
Figure 7C illustrates the simulated fluctuations in neuronal activity, after bandpass filtering. These can 731 
be regarded as simulated local field potentials or event related potentials (Leonard et al.), corresponding 732 
roughly to the voltage fluctuations in Equation 3. Later in the paper, we will revisit these synthetic ERPs 733 
to characterise responses to surprising outcomes. The current simulation  simply shows that the 734 
amplitude of simulated ERPs is related to the amount of information conveyed by an outcome—it shows 735 
greater responses to more informative parts of the narrative.  736 
Modelling ‘Twenty Questions’ 
 
 33 
 737 
FIGURE 7 738 
Electrophysiological responses: This figure shows the simulated electrophysiological responses associated with 739 
the belief updating reported in Figure 6. In this figure, we focus on beliefs about the colour of the lower object, 740 
which is at the higher level of the generative model —thus, these plots show simulated responses following each 741 
phrase (i.e., prompt, question, and answer) rather than following each word. The horizontal axes show time over 742 
the entire exchange, assuming each phrase lasts for 250 ms . Expectations about the hidden state encoding the 743 
colour of the lower object are presented in raster format in the panel  A, where black corresponds to maximum 744 
firing rates. Panel B shows the same data but in a different format: here, pooled expectations (filtered between 4 745 
and 32 Hz) are shown as a white line. This simulated local field potential is superimposed upon a time-frequency 746 
heat map to illustrate bursts of frequency -specific energy  (white), during pe riods of belief updating. The 747 
underlying fluctuations in simulated neuronal activity, after bandpass filtering between 4 Hz and 32 Hz, are shown 748 
in panel C. Each of the coloured lines on this plot represent belief updating for a given unit (i.e., the rows of the 749 
upper panel). Panel D shows simulated dopamine responses after each answer : these attenuate as uncertainty is 750 
resolved.  751 

Modelling ‘Twenty Questions’ 
 
 34 
Finally, Figure 7B  shows simulated dopamine responses  (i.e., expected precision of beliefs about 752 
policies), as described in (Friston et al., 2014) . Interestingly, the peaks of these phasic responses 753 
coincide with times that answers are given. The key point to  take from these phasic responses is that 754 
the implicit changes in confidence—about the policies being pursued—depends on the extent to which 755 
answers resolve uncertainty and fulfil prior preferences. Every time the agent receives (and to a lesser 756 
extent delivers) an answer, it becomes more confident about what it is doing. However, becoming more 757 
confident about the hidden scene attenuates the ‘confidence boosts’ (i.e., phasic dopamine responses). 758 
Anecdotally, this seems consistent with the subjective experience of “Twenty Questions”, where each 759 
confirmatory answer is rewarding, especially at the beginning of the game. 760 
Figure 8 presents the simulated electrophysiological responses from  Figure 7 in terms of what one 761 
would predict when analysing spectral responses from the higher order area during belief updating. The 762 
lower panels show the spectral responses. The lower left panel reports the log spectral density of the six 763 
units (i.e., neuronal populations), whose event related responses are shown in Figure 7C. This shows 764 
that spectral r esponses show a degree of scale -free broadband activity, reflecting the fact that the 765 
simulated neuronal dynamics have multiple nested timescales.  766 
The ensuing nonlinear coupling between fluctuations at different frequencies is summarised in terms of 767 
cross frequency coupling on the lower right panel of Figure 8 . This simple characterisation is the 768 
correlation between the response magnitudes, over frequencies ranging from 4 to 32 Hz (based on the 769 
time frequency response in the lower panel). The key thing to note from this panel is the off -diagonal 770 
structure, suggesting that there are profound correlations among different frequenci es. This would be 771 
interpreted as some form of nonlinear or phase -amplitude coupling, if subject to standard procedures 772 
such as bi -coherence analysis or phase -synchronisation measures: for example, see (Giraud and 773 
Poeppel, 2012; Lizarazu et al., 2019; Pefkou et al., 2017) . In general, this nonlinear coupling is 774 
consistent with “evidence that the temporal modulation transfer function (TMTF) of human auditory 775 
perception is not simply low-pass in nature” (from (Edwards and Chang, 2013) p113).776 
Modelling ‘Twenty Questions’ 
 
 35 
 777 
FIGURE 8 778 
Spectral responses and nested oscillations . This figure shows the spectral responses associated with the 779 
simulated electrophysiological responses in Figure 7. Panel A is a reproduction of Figure 7B. Panel B reports the 780 
spectral density of the six units  (i.e., ‘red’ or ‘green’ for epochs 1, 2, and 3) . Only three lines are visible because 781 
pairs of responses overlap perfectly. Note that the scale is expressed in terms of log power. The matrix in panel C 782 
shows the correlation between the magnitudes of responses over frequencies ranging from 4 to 32 Hz. These 783 
correlations are based on the time frequency response in panel A. 784 
 785 

Modelling ‘Twenty Questions’ 
 
 36 
The implicit nonlinear coupling arises, in this setting, from the message passing within and between 786 
hierarchical levels of deep temporal models. Neuronal dynamics perform a gradient descent on 787 
variational free energy, as each new outcome becomes available (for the higher level, when each phrase 788 
is spoken). By virtue of this temporal scheduling, there are necessarily nested oscillations in the sense 789 
that fast ( e.g., gamma) fluctuations unfold at a slow ( e.g., theta) rhythm (Friston et al., 2017 a): a 790 
succession of transients containing high-frequency components is induced by hearing each word, and 791 
these transients recur at the (necessarily lower) frequency of word presentation.  792 
In short, these types of responses would be expected, based on belief updating at different hierarchical 793 
levels of a generative model, which necessarily occur at  different temporal scales . This nesting is 794 
illustrated in Figure 9, which shows the simulated neuronal firing and associated local field potentials 795 
for neuronal populations at the higher and lower levels. The key thing to observe here is that lower level 796 
transients (cyan lines) are faster than the accompanying higher-level transients (red lines). This means 797 
that fluctuations in the amplitude of frequency -specific responses to each word or phrase will 798 
necessarily produce cross frequency coupling. Phenomenologically, this means that one would not be 799 
surprised to see bursts of beta activity at the higher level coincide with bursts of gamma activity in the 800 
lower-level. See (Arnal and Giraud, 2012; Giraud and Poeppel, 2012)  for a discussion of related 801 
phenomena.802 
Modelling ‘Twenty Questions’ 
 
 37 
 803 
FIGURE 9 804 
Hierarchical message passing and nested oscillations. The upper panel illustrates responses at the 805 
second level using the format of the upper panel of Figure 7. Here, we focus on representations of the 806 
colour of the upper object—following each phrase—for the last three exchanges. At this point, the 807 
agent is fairly sure the upper object is green (as indicated by the darker shading for the ‘green’ unit in 808 
the upper panel. The middle panel shows the equivalent results for representations in the lower level, 809 
encoding the semantic adjective factor, which switches between green and red for the last three 810 
questions. The lower panel shows the band-pass filtered responses (between 4 and 32 Hz) to illustrate 811 
the nesting of simulated electrophysiological responses (solid lines: higher-level scenic responses. 812 
Broken lines: lower-level semantic responses). Two responses have been highlighted for illustration in 813 
red (high level) and cyan (lower level). The nesting of (simulated) neuronal fluctuations is evident at a 814 
number of levels. First, bursts of activity are organised around periods of belief updating, when 815 
sensory evidence becomes available. Periods of activity are evoked by auditory outcomes (words) at 816 
the lower level and—at the higher level—evidence that speaks to the posterior expectations or 817 
representations. Second, it demonstrates transients at the onset of each word, which recur at a theta 818 
frequency. Each transient carries fast (e.g., gamma) frequency components. This means there is a 819 
theta-gamma coupling in the sense that the amplitude of gamma responses fluctuates at a theta 820 
frequency. Finally, note that the transients at the lower level (cyan line) are ‘sharper’ than the 821 
transients at the higher level (red line). 822 

Modelling ‘Twenty Questions’ 
 
 38 
4.2. Deep violation responses 823 
Thus far, we have focused on the sort of message passing—and its neurophysiological correlates—that 824 
would be measured using time frequency analyses of induced responses. Here, we consider predictions 825 
for evoked responses. In particular, we simulate the differential waveforms evoked by stimuli that 826 
violate expectations, which have been the focus of many empirical studies; e.g., (Coulson et al., 1998; 827 
Friederici, 2011; Pulvermuller et al., 1995; Van Petten and Luka, 2012; Ylinen et al., 2016). 828 
Figure 10 illustrates the neurophysiological simulation of a violation response; for example, P300 or 829 
N400 responses to a semantic violation or unexpected sentence closure. Here, we reproduce a 830 
violation paradigm by rerunning the fifth exchange from the previous simulations with the wrong 831 
answer at the end. Recall that at the beginning of fifth exchange, the agent is confident that the colour 832 
of the lower shape is red: it obtained this information from the answer to question 2. The left panels 833 
show the standard responses, using a similar format to Figure 7. 834 
Simulated event related potentials (i.e., band pass filtered expectations of the lower colour at the three 835 
epochs) are shown on the upper right. The underlying unit activities producing these fluctuations are 836 
shown in terms of a simulated raster of unit firing—for the six units in question—on the lower left. Of 837 
note, simulated event related potentials in the right panel (i.e., when the wrong answer is provided at 838 
0.5 seconds) have longer latencies (as illustrated by the blue arrow). These long latency responses 839 
have a remarkably similar morphology to P300 (Donchin and Coles, 1988; Van Petten and Luka, 840 
2012) and N400 (Kutas and Hillyard, 1984; Van Petten et al., 1999; Van Petten and Kutas, 1990) 841 
waveform components in empirical violation paradigms. Here, they simply reflect the fact that the 842 
artificial agent has to change its mind and undo the conviction that the lower square was red, given the 843 
evidence was in favour of green; in other words, update its beliefs. As can be seen from the lower 844 
right panel, the agent becomes less certain about the colour, and leans slightly towards the belief that 845 
the colour of the lower object is green. This posterior belief is entirely congruent with hearing a 846 
negative answer to the question “is there a red square below?”. 847 
Modelling ‘Twenty Questions’ 
 
 39 
 848 
FIGURE 10 849 
Violation responses: This figure illustrates the neurophysiological simulation of a violation response, 850 
of the sort seen in response to a semantic violation or unexpected sentence closure. We reproduced 851 
this paradigm by rerunning the fifth narrative but supplying the wrong answer at the end (see panel 852 
H). The left box (A–D) shows the standard responses when the correct answer is supplied (see panel 853 
D) using a similar format to Figure 7. Here, the simulated unit firing of neurons that respond to the 854 
colour of the lower object (i.e., the scenic representation at the higher level) are shown in raster 855 
format (panel C). The population average or expected firing rate is used to simulate unit activity by 856 
sampling from a binomial distribution at each 16 ms time window. The average response magnitude 857 
and time frequency response are shown in panel A for the three epochs (prompt, question, answer) of 858 
the fifth exchange. The simulated event-related potentials (i.e., expectations about the colour of the 859 
lower object—red or green—at the three epochs, band pass filtered at 4–32 Hz) are shown in panel B. 860 
The right box (E–H) reproduces the same results after supplying the wrong answer (i.e., “No” versus 861 
“Yes”), which induces protracted belief updating over a longer latency, as indicated by the blue 862 
arrow. 863 
 864 
 865 

Modelling ‘Twenty Questions’ 
 
 40 
5. Synthetic communication 866 
In the simulations above, external states of affairs were used to supply veridical answers to the first four 867 
questions by sampling from the generative process. In other words, the external states were standing in 868 
for the beliefs of someone answering or asking questions. In what follows, we make an important move 869 
and replace external states with another synthetic subject. This has the interesting consequence of taking 870 
external states off the table: outcomes are generated or sampled from the (posterior) predictions of one 871 
or other subject, so at no point do we need to refer to (external) states of the world (see Figure 1). This 872 
is a straightforward consequence of allowing agents to generate outcomes that are shared between them. 873 
Heuristically, the imperative to resolve uncertainty (i.e., minimise expected free energy) is now 874 
reflected in a synchronisation of belief states; namely, a ‘meeting of minds’ and mutual understanding. 875 
 876 
5.1. Questions and answers 877 
The simulations reported in Figure 11 use a similar format to Figure 6 however, here there are two 878 
synthetic subjects. The second subject has precise (i.e., confident) beliefs about the scene at hand 879 
(namely, a green square above and a red square below). In contrast, the first subject is less confident 880 
before the exchange begins and effectively inherits the belief of the confident subject by listening to 881 
the answers to the questions it asks. Analogous to Figure 6,  after the fourth answer, the first subject 882 
has a precise understanding of what the confident subject believes and is able to answer correctly the 883 
when quizzed with two final questions. 884 
In this example, the first agent chose its questions carefully to resolve as much uncertainty as possible. 885 
In the lower panel, we reverse the roles so that the second (confident) agent asks questions and the less 886 
confident agent provides answers. The results of the simulation are shown in the lower panel ( entitled 887 
role reversal). In comparison to the upper panel, the first agent accumulates  evidence for the beliefs of 888 
the second agent much more slowly, and the two agents do not share the same beliefs after the fourth 889 
answer. The questions asked by the second agent are insensitive to the particular uncertainty that 890 
Modelling ‘Twenty Questions’ 
 
 41 
confounds the first, and so all the first agent can do is say that it is “not sure” in response to the first 891 
two questions, when its beliefs are uniform across the red and green states. For the third question, it 892 
responds “no”. Unlike the first two questions, the third question asks about the combination of colour 893 
and shape  attributes in the upper location , which has four possible options, and so the balance of 894 
probabilities means that the most likely answer is “no”. After hearing its own negative answer, the 895 
subject then convinces itself that the upper shape cannot be a red triangle and is more likely to be a  896 
green square, which is further endorsed by its subsequent response to the (same) fourth question. Only 897 
when observing definitive and veridical  answers can it then start to accumulate proper beliefs about 898 
what the other subject believes. 899 
 900 
5.2. Storytelling 901 
We can use exactly the same scheme above to simulate instruction or storytelling: the same underlying 902 
joint belief updating characterises all forms of exchange in this active inference formulation. We reran 903 
the simulations from the previous section, but this time the second agent answered its own questions, 904 
while the first simply listened for the first four exchanges and supplied answers for the last two 905 
exchanges. As above, the first agent inherits scenic beliefs from the second agent, but here this is simply 906 
by listening to the second agent’s soliloquy. After the four questions and answers, the f irst agent is 907 
sufficiently confident about the scene to answer correctly; even though it is unsure whether the lower 908 
object is a red square or a red triangle. This ambiguity reflects the fact that the preceding questions and 909 
answers were not selected to re duce the first agent’s  uncertainty—they were selected by the second 910 
subject, who had very precise beliefs. 911 
Modelling ‘Twenty Questions’ 
 
 42 
 912 
FIGURE 11 913 
Playing ‘Twenty Questions’ with a partner: These simulations use a similar format to Figure 6; 914 
however, here there are two synthetic subjects. Their beliefs are displayed in separate columns within 915 
each panel, and the text is placed next to the agent who spoke the phrase. The second subject (purple 916 
icon, right column) has precise (i.e., confident) beliefs about the scene at hand: it believes there is a 917 
green square above a red square). In contrast, the first agent (green icon, left column) begins with 918 
imprecise beliefs and effectively inherits the beliefs of the confident subject, by listening to the 919 
answers to the questions it asks. It is then able to answer the two questions asked by the other agent in 920 
the fifth and sixth narratives. The lower panels replicate the simulation but here the less confident 921 
agent answers questions. 922 

Modelling ‘Twenty Questions’ 
 
 43 
 923 
FIGURE 12 924 
Storytelling: The result of an exchange between two  synthetic agents, when the second agent (purple icon, right 925 
panel) answered its own questions for the first four exchanges (panels A –D). For the fourth and fifth exchanges 926 
(panels E–F), the second agent a sked the questions and the first agent (green icon, left column) answered. Here, 927 
the first agent had to rely upon the question selected by the second agent to update its beliefs about the scene. This 928 
resulted in some residual ambiguity about the lower object (i.e., it is most likely to be a red triangle, it could be a 929 
red square, but it is probably not a green square). Nevertheless, the first subject was still able to answer the 930 
questions correctly. 931 

Modelling ‘Twenty Questions’ 
 
 44 
5.3. Making your mind up  932 
Things get int eresting if we reduce the precision of prior beliefs , so that both subjects  are uncertain 933 
about the scene. Recall that the synthetic agents are given three possible responses: “Yes!”, “No!”, and 934 
“I’m not sure”. When allowed to question each other in this setting, they simply respond truthfully that 935 
they are unsure about the answer (see the upper panel of Figure 13). However, when we reduce the prior 936 
probability of the ‘not sure’ response, both subjects effectively tell each other about what they believe, 937 
until they come to hold the same beliefs (see the lower panel of Figure 13). At this point, uncertainty is 938 
precluded because each can predict the other and their shared understanding. This  is an example of 939 
neural hermeneutics (Frith and Wentzer, 2013) in the absence of ‘truth pointing’. As noted above, this 940 
is a form of generalised synchronisation (Friston and Frith, 2015a), where the orbits of belief states that 941 
underlie linguistic exchange become mutually predictable as (expected) free energy is minimised.  942 
Anthropomorphically speaking, the two synthetic subjects have simply reached a consensus about how 943 
to describe some shared construct. Crucially, the construct (i.e., scene) does not exist and, from our 944 
perspective, therefore, could be described as a Folie à deux (Arnone et al., 2006). On a more positive 945 
note, it could also be construed as a joint exercise in creative thinking. Although not pursued here, one 946 
can think about extensions of this sort of simulation that could be framed in terms of artistic 947 
communication and creativity , bringing us back to the resolution of uncertainty through epistemic 948 
foraging, novelty and fun (Schmidhuber, 2006). 949 
Returning to the upper panel of Figure 13, this example illustrates the joint maintenance of uncertain 950 
beliefs. This is interesting because these rudimentary agents have no formal metacognitive capacity (see 951 
discussion). In other words, their uncertainty is implicit in neuronal states encoding uncertain belief 952 
distributions, rather than possessing neuronal states that encode posterior beliefs about the precision of 953 
their beliefs. Having beliefs about the preci sion of beliefs may sound rather complicated; however, 954 
statistical models with a deep structure very often encode uncertainty explicitly. For example, when we 955 
report the degrees of freedom of a statistical test, we are effectively reporting the confidence in our 956 
estimate of uncertainty; e.g., the standard error on some parameter estimates  (Friston et al., 2007). In 957 
Modelling ‘Twenty Questions’ 
 
 45 
the current simulations, there is no such metacognitive inference—and yet the two agents continue to 958 
answer that they are uncertain about the hidden states they are being questioned on, as is Bayes optimal.  959 
 960 
 961 
 962 
FIGURE 13 963 
Folie à deux: The result of an exchange between two interlocutors (green and purple), who are both unsure about 964 
the scene they are discussing. The format of this figure follows that of previous figures. The upper panels  (A–F) 965 
show the questions and answers that confess a lack of knowledge or certainty. Each agent’s posterior expectations 966 
about the scene are indicated by the coloured shapes.  In this simulation, neither agent informs the other  agent 967 
about the objects present in the scene,  and so they both remain in a state of mutually cons istent ignorance. The 968 
lower panels (G–L) show the same simulation when the likelihood of an “I’m not sure” response was set to zero. 969 
This produces a folie à deux described in the main text. In brief, the ensuing belief updating starts from an unstable 970 
fixed point of uncertainty that converges onto a shared fantasy about what both agents (are confident they) believe.971 

Modelling ‘Twenty Questions’ 
 
 46 
The mechanism that underwrites this apparent  confirmation of ‘known unknowns’ is straightforward. 972 
It rests upon a nontrivial likelihood of saying “I'm not sure”, irrespective of one's beliefs. Consider the 973 
following: I am thinking about numbers between one and one hundred and I can either report a number 974 
or select a “not sure” option. If the likelihood of reporting a number is 90% and I am sure about the 975 
number, then I am nine times more likely to report the (exact) number I have in mind than to say “not 976 
sure”. Conversely, if I have no idea about the number, then the likelihood of reporting any number is 977 
equal to the probability of selecting any other number ; the probability of reporting any individual 978 
number therefore falls to less than 1% , because the probabilities are dispersed or diluted over 100 979 
number options. In this case, I am therefore more than 10 times more likely to report “not sure”  than 980 
any individual number. In Bayesian model selection, this phenomena is known as evidence dilution 981 
(Hoeting et al., 1999). The example in the upper panel of Figure 13 highlights this emergent but simple 982 
consequence of entertaining declarations of uncertainty. Note that this kind of uncertainty rests upon a 983 
shared generative model , in which uninformative  responses can be selected, even in the absence of 984 
uncertainty. When we remove the opportunity to generate such agnostic responses, a different pattern 985 
of mutual understanding emerges (see the lower panel of Figure 13). 986 
 987 
6. Discussion 988 
In summary, we have illustrated a number of plausible  correlates of communication that emerge from 989 
active inference under a particular sort of generative model. This generative model was motivated by 990 
the nature of language and communication. The key attributes of this model speak to the notion of a 991 
shared narrative that  reduces uncertainty. In the Section 4  (“Twenty Questions” simulations) , we 992 
simulated an agent that was speaking to itself (i.e., in ‘conversation’ with a generative process), and in 993 
the Section 5 (Synthetic communication) we used exactly the same generative model to simulate two 994 
subjects who were asking and  answering questions. In each of these cases, the inference and senso ry 995 
evidence were identical: the only difference was agency (i.e., who was talking).  996 
Modelling ‘Twenty Questions’ 
 
 47 
In our simulations, hierarchical inference led to belief updating  that resembled theta-gamma coupling 997 
(Figure 8), which has often been observed empirically ; for example, see (Giraud and Poeppel, 2012; 998 
Lizarazu et al., 2019; Pefkou et al., 2017). Under this framework, theta-gamma coupling arises because 999 
hearing each word induces a succession of transients containing high-frequency (gamma) components, 1000 
and these transients recur at the frequency of word presentation , which is in the theta ra nge. A multi-1001 
timescale nesting process has been proposed by others as a plausible explanation (Arnal and Giraud, 1002 
2012; Giraud and Poeppel, 2012), as it has been noted that the timing of these rhythms corresponds to 1003 
important timescales in language. Previous approaches  to modelling this phenomenon (Hovsepyan et 1004 
al., 2018; Hyafil and Cernak, 2015)  have been data -driven—incorporating explicit theta and gamma 1005 
‘units’. Here, we took a theoretical approach and show that theta-gamma coupling can arise from belief 1006 
updating, given an agent’s goal to understand the contents of a scene from a dialogue. 1007 
Our simulations also predict electrophysiological violation responses, of the sorts observed in P300 and 1008 
N400 studies. The P300 has been observed in oddball paradigms, in which repeating stimuli are 1009 
interspersed with unexpected deviants. In this setting, the P300 has been interpreted as reflecting 1010 
violations of high-level context (Donchin and Coles, 1988). The N400 is commonly observed in studies 1011 
of language. It has been elicited when participants  hear words that have low frequency (Kutas and 1012 
Hillyard, 1984; Van Petten et al., 1999; Van Petten and Kutas, 1990) , or words that are semantically 1013 
related to words that have high probability (Kutas and Hillyard, 1984). 1014 
The generative model we have introduced represents a different way to think about semantic or 1015 
contextual aspects of language, in relation to previous accounts. Surveying the empirical and theoretical 1016 
antecedents of the current formulation of language —and understanding —would be an enormous 1017 
undertaking, given the vast amount of psychological, philosophical and computational literature in this 1018 
area. In this context, three observations are relevant. First, in the current framework, belief updating is 1019 
hierarchical: beliefs about the content of a scene are maintained at the higher level. Second, an agent’s 1020 
uncertainty in their beliefs about the current state of the world affect the magnitude of belief updating. 1021 
Modelling ‘Twenty Questions’ 
 
 48 
Finally, here, we cast language understanding as an active processes—allowing an agent to ask 1022 
questions that maximally resolve their uncertainty about states of affairs. 1023 
Before commenting upon some salient points of contact with related work, we will qualify this 1024 
discussion with the following observations: if one commits to active inference (and implicitly, the free 1025 
energy principle), there is little latitude for hypothesising about the nature and form of linguistic 1026 
processing. This is because everything of interest is defined operationally by the generative model and 1027 
the generative model is, in turn, defined by what we want to explain; namely language and 1028 
understanding. In other words, simply defining the inference problem dictates the form of the requisite 1029 
generative model , in terms of what how outcomes are caused by states of the world (or others). 1030 
Furthermore, once the generative model has been specified, the belief updating is prescribed by standard 1031 
belief updating schemes ; here, variational  or marginal message passing (Dauwels, 2007; Parr et al., 1032 
2019; Winn and Bishop, 2005). 1033 
This means that there is no latitude to accommodate alternative hypotheses or constructs, if they are not 1034 
consistent with the sort of formulation above—or the basic architecture of belief updating. In short, in 1035 
active inference, the only questions are: what kind of generative model could explain these responses? 1036 
Strictly speaking, this precludes questions about the implementation and the neurophysiological 1037 
correlates of language processing  (Braiman et al., 2018; D owty et al., 1985; Lizarazu et al., 2019; 1038 
Pefkou et al., 2017; Wilson et al., 2017; Ylinen et al., 2016) . While many of these may be especially 1039 
useful within their own remit, unless neurophysiological correlates can be linked to belief updating (i.e., 1040 
understanding through communication), they cannot be used to simulate—and therefore understand—1041 
communication. In a similar vein, any exciting advances in computational neurolinguistics (Barlow, 1042 
1974; Lightfoot, 1979; MacKay and Peto, 1995; Norris et al., 2016; Rosenfeld, 2000)  that do not deal 1043 
explicitly with belief states updating cannot be used to create artefacts that communicate. For example, 1044 
the use of deep learning in speech recognition may provide compelling insights into the computational 1045 
architecture of linguistic processing at an auditory level; however, speech recognition does not  1046 
constitute understanding. In other words, simply mapping from auditory input to a list of words does 1047 
Modelling ‘Twenty Questions’ 
 
 49 
not constitute the inversion of a generative model. Some research, within machine learning, has looked 1048 
at schemes similar to active inference , within partial observability frameworks . For example , the 1049 
Bayesian Action Decoder (Foerster et al., 2018) uses approximate Bayesian update to obtain a ‘public’ 1050 
belief that is conditioned on  the actions of all agents in the environment, leading to efficient 1051 
communication when playing multi-agent games. 1052 
There are impor tant developments in computational linguistics that could inform active inference 1053 
schemes in a useful way. For example, the use of hierarchical Dirichlet processes to solve the structure 1054 
learning problem in generative models of language (MacKay and Peto, 1995; Salakhutdinov et al., 1055 
2013) could be the right approach to grow generative models —and subsequently  prune them with 1056 
Bayesian model reduction (Friston and Penny, 2011)—in the context of language acquisition. We have 1057 
not touched upon this  issue in the current paper; however, having established the basic form of a 1058 
generative model for language and understanding, the next challenge would be to study learning through 1059 
optimisation of the model parameters; e.g., the likelihood mapping is entailed by the A matrices between 1060 
hierarchical levels. After this (learning) has been addressed, the next level of optimisation concerns the 1061 
form and structure of the model itself. For example, how many hidden factors should be included—and 1062 
how many levels or mutually exclusive states occupy each factor? This is the problem of structure 1063 
learning (Catal et al., 2019; Gershman, 2017; Tenenbaum et al., 2011; Tervo et al., 201 6) that is 1064 
elegantly addressed using nonparametric Bayesian methods (Collins and Frank, 2013; Goldwater, 2006; 1065 
Teh et al., 2006), such as those found in computational linguistics (please see below). 1066 
In this paper, we ignored the attribution of  (i.e., inference about ) agency; namely , metacognitive 1067 
capability (Fleming et al., 2012; Shea et al., 2014). This means that each synthetic subject had no notion 1068 
of who was talking. Nevertheless, our synthetic subject could still use the information  provided to 1069 
resolve uncertainty about states the world ( e.g., the configuration of objects in a scene). More 1070 
sophisticated generative models would include hidden factors that include agency per se. This was not 1071 
necessary for the current examples, but  would be necessary for simulating turn taking in linguistic 1072 
exchange (Garrod and Pickering, 2009; Ghazanfar and Takahashi, 2014; Wilson and Wilson, 2005) . 1073 
Modelling ‘Twenty Questions’ 
 
 50 
This was a focus of our earlier work using simulated songbirds (Friston and Frith, 2015a). In the current 1074 
work, we simply replaced internally generated speech with the external speech of a conversant to 1075 
simulate asking questions and answering, respectively. However, the agents were not aware of this.  1076 
An important aspect of  metacognition is knowing when one is uncertain. In the simulations above,  1077 
agents were able to maintain their uncertainty by providing each other with uninformative (“not sure”) 1078 
answers. However, they were not aware of being uncertain (i.e., their generative models did not have a 1079 
hidden ‘state of uncertainty’). A more sophisticated generative model would realise that something was 1080 
not known with confidence and respond with "I really don't know".  This apparently simple capacity 1081 
rests upon a generative model of confidence that is quintessentially metacognitive; in the sense that 1082 
inverting this kind of deep generative model produces (posterior) beliefs about beliefs. 1083 
It is an interesting challenge to formulate metacognitive depth using dis crete state space models (i.e., 1084 
hidden Markov models or Markov decision processes). In one sense, the encoding of precision or 1085 
confidence in beliefs about policies is a metacognitive representation  (see th e simulated dopamine 1086 
responses in  Figure 7 ); howeve r, it is quite elemental . F urthermore, this sort of representation is a 1087 
continuous (real valued) variable, of the sort that has been used to explain dopaminergic fluctuations in 1088 
reinforcement learning paradigms (Schwartenbeck et al., 2015). It would be nice to have the categorical 1089 
step state of “I am uncertain” or “I am very confused ”. This speaks to the use of higher hierarchical 1090 
levels that prescribe uniform (empirical) priors over the initial states of a level below. In other words, 1091 
one can generate belief distributions about the context of a lower level, based upon a discretisation into 1092 
confident beliefs about particular states of affairs and  complete uncertainty (with uniform priors). In 1093 
principle, this should equip agents with a metacognitive sense of the ir beliefs —and a way of 1094 
communicating these beliefs via language. 1095 
An important aspect of language that we ignored is its computational richness (e.g., discrete infinity) 1096 
afforded by the combinatorics of narratives and sentences  (Chomsky, 2017) . In addition , we  have 1097 
ignored the parsing and transpositions that characterise real language processing—that themselves have 1098 
a deep hierarchical form. This issue presents some interesting challenges, in terms of articulating the  1099 
Modelling ‘Twenty Questions’ 
 
 51 
structure of the generative model , which may involve separately generating the ordinal aspects of 1100 
spoken language from its content. Technically, this would involve an interaction between—or coupling 1101 
of—separate ordinal and content factors  (Dehaene et al., 2015; Friston and Buzsaki, 2016) . In other 1102 
words, we would have to replace the probability transition matrices ( B) above with high dimensional 1103 
arrays, so that the probability transitions among the levels of one factor depend upon the level of 1104 
another. Note that learning the factorial structure of natural language is the focus of much work:  e.g., 1105 
neural language modelling using recurrent neural networks (Bengio et al., 2003; Mikolov, 2010; 1106 
Mikolov et al., 2013; Shang et al., 2015) , or sequence-to-sequence modelling (Bahdanau et al., 2014; 1107 
Ghazvininejad et al., 2018; Sutskever et al., 2014; Vinyals and Le, 2015). 1108 
We have not considered language acquisition; e.g. via the learning of the A, B and D parameters above 1109 
(Al-Muhaideb and Menai, 2011; Bengio et al., 2009; Fr iston et al., 2016). In principle, by listening to 1110 
an authoritative sequence of questions and answers, it should be possible to simulate language 1111 
acquisition at various levels, via structure learning and Bayesian model reduction (Tervo et al., 2016). 1112 
This has been pursued in the context of abstract rule learning  (Friston et al., 2017b), but has not been 1113 
applied in the present context. At this point, we get close to the problems addressed in computational 1114 
linguistics, via the use of hierarchal Dirichlet processes (MacKay and Peto, 1995; Salakhutdinov et al., 1115 
2013; Teh et al., 2006). In this setting, the key problem is to optimise the structure and hierarchal form 1116 
of the model—and to know when to add an extra factor or level. It is possible that this structure learning 1117 
problem may be usefully addressed with existing work on hierarchal Dirichle t process models and 1118 
nonparametric Bayes (Goldwater, 2006); combined with the more top-down approach promoted in this 1119 
work. 1120 
Finally, our syntax factor is over -simplistic, encompassing only a handful of possibilities. This was 1121 
sufficient for the simulations we presented, but will become important in applications of this  kind of 1122 
generative model. There is a substantial literature on cognitive models of syntax processing (for a recent 1123 
review, see (Demberg and Keller, 2019)) and how listeners deal with semantic ambiguity (Altmann and 1124 
Steedman, 1988; Bever, 1970; Gibson, 1998, 2000) . Generally speaking, e vidence from visual 1125 
Modelling ‘Twenty Questions’ 
 
 52 
paradigms (Kamide et al., 2003) points to a predictive process, which is broadly consistent with active 1126 
inference. It has also been proposed that syntax may itself be hierarchical (Van Schijndel et al., 2013). 1127 
In summary, we have presented a generative model and inference scheme that is capable of simulating 1128 
exchanges between synthetic subjects. This generative model is deep and hierarchical: inferences at the 1129 
higher level inform words that are selected at the lower level —and these levels are nested, such that 1130 
phrase-level inferences generate the words contained within the phrase, and then the lower level ‘resets’ 1131 
for the next phrase. Our simulations of the “Twenty Questions” game show that agents can select the  1132 
best questions—to ask of another—to reduce their uncertainty (in a Bayes optimal fashion) about the 1133 
subject of conversation. We have also shown that, i f the agent has precise beliefs about the nature of 1134 
the scene, it can correctly answer another agent’s questions. These types of exchanges demonstrate a 1135 
convergence of beliefs, reflecting a successful linguistic exchange. We have also simulated situations 1136 
where, if the agent has very imprecise beliefs, it will acknowledge its own uncertainty. If two agents 1137 
both start with imprecise beliefs, then their generative models will converge, even though neither agent 1138 
knows the veridical state of the scene. This type of setting could be considered as a folie à deux or an 1139 
example of joint creative thinking. Finally, this formulation of communication makes predictions for 1140 
neurophysiological responses, based on belief updating. It predicts violation responses, like P300 and 1141 
N400 responses, when an answer is inconsistent with the agent’s beliefs, and shows theta -gamma 1142 
coupling as an emergent property of belief updating. Overall, we envisage that this model will be a 1143 
useful starting point for simulating more complex linguistic exchanges—that include metacognition, or 1144 
which simulate language acquisition.  1145 
 1146 
Software note 1147 
Although the generative model changes  from application to application, the belief updates —and 1148 
simulated neuronal responses —described in this paper are generic and can be implemented using 1149 
standard routines (here spm_MDP_VB_X.m). These routines are available as Matlab code in the SPM 1150 
Modelling ‘Twenty Questions’ 
 
 53 
academic software: http://www.fil.ion.ucl.ac.uk/spm/. The simulations in this paper can be reproduced 1151 
(and customised) via a graphical user interface by typing DEM and selecting the 20 questions demo. 1152 
 1153 
Acknowledgements 1154 
The Wellcome Trust funded K.J.F. (Ref: 088130/Z/09/Z), E.H. (Ref: WT091681MA), and the 1155 
Wellcome Centre for Human Neuroimaging (Ref: 203147/Z/16/Z), where this work was conducted. 1156 
N.S. was funded by the Medical Research Council ( MR/S502522/1). T.P. was supported by the 1157 
Rosetrees Trust (Award number: 173346). 1158 
 1159 
Disclosure statement 1160 
The authors have no disclosures or conflict of interest. 1161 
 1162 
Appendices 1163 
Appendix 1  – Expected free energy : the variational free energy  of a policy  is a functional of an 1164 
approximate posterior distribution over states 
( | )Qs   given observed outcomes  o, under a 1165 
probabilistic generative model 
( , | )P o s  , given a policy 
 : 1166 
( | )
( | )
complexity accuracy
[ln ( | ) ln ( , | )]
[ ( | ) || ( | )] [ln ( | )]
Qs
KL Q s
F E Q s P o s
D Q s P s E P o s




=−
=−
      A.1 1167 
The second equality expresses free energy as the difference between a Kullback -Leibler divergence 1168 
(i.e., complexity) and the expected log likelihood (i.e., accuracy), given (observed) outcomes.  1169 
Modelling ‘Twenty Questions’ 
 
 54 
In contrast, the expected free energy  of a policy  is an average over (unobserved) outcomes that 1170 
determines the distribution over (future) states: 1171 
( , | )
( , | )
[ln ( | ) ln ( , | )]
[ln ( | ) ln ( | , ) ln ( | )]
Q o s
Q o s
G E Q s P o s
E Q s P s o P o



  
=−
= − −
     A.2 1172 
Replacing the true predictive posterior with the approximate predictive posterior gives: 1173 
( , | )
intrinsic value extrinsic value
ambiguityrisk
[ln ( | ) ln ( | , ) ln ( | )]
[ ( | ) || ( | )] [ ( | )]
Q o s
KL
G E Q s Q s o P o
D Q o P o H Q o s
   

= − −
=+
     A.3 1174 
The first equality expresses expected free energy in terms of  intrinsic and extrinsic value, while the 1175 
second expression is an equivalent formulation in terms of the divergence between predicted and prior 1176 
preferences over outcomes (i.e., risk) and expected uncertainty about outcomes, given their causes (i.e., 1177 
ambiguity). By comparing equation A.3 with A.1, it can be seen that risk is expected complexity and 1178 
ambiguity is expected inaccuracy, under a particular policy. 1179 
Note that, for completeness and clarity, in the above equations (and the expressions in Table 1) we have 1180 
conditioned prior preferences on policies. In practice, however, we assume that prior preferences do not 1181 
depend upon policies. 1182 
 1183 
Appendix 2 – Belief updating: approximate Bayesian inference corresponds to minimising variational 1184 
free energy, with re spect to  the sufficient statistics that constitute posterior beliefs. For generative 1185 
models of discrete states, the free energy of hidden states and policies can be expressed as the (time -1186 
dependent) free energy under each policy plus the complexity incurred by posterior beliefs about (time-1187 
invariant) policies, where (with some simplifications): 1188 
Modelling ‘Twenty Questions’ 
 
 55 
[ ] [ ( , ) || ( , )] E [ln ( | )]
E [ ( , )] [ ( ) || ( )]
(ln )
KL Q
Q KL
F Q D Q s P s P o x
F D Q P

   
=−
=+
=  + +

π π F G
 1189 
The free energy of hidden states under each policy is then given by: 1190 
1( | ) 1
accuracycomplexity
, , , 1 , 1
( , )
( , ) [ [ ( | ) || ( | , )]] [ln ( | )]
(ln ln ln )
Qs
F
F E D Q s P s s E P o s
o

 
     
        

   
− −
−−
=
=−
=  − − 
F
s s B s A
   A.4 1191 
The expected free energy of a  policy has a similar form, but the expectation is over hidden states and  1192 
outcomes that have yet to be observed; namely, 
( , ) ( | ) ( | )Q o s P o s Q s     = . 1193 
intrinsic value extrinsic value
risk ambiguity
, , ,
( , )
( , ) [ln ( | , ) ln ( | )] [ln ( | )]
[ ( | ) || ( | )] [ ( | )]
(ln )
QQ
KL
G
G E Q s o Q s E P o
D Q o P o H Q o s
 
   
   
      

    

=
=− − −
=+
=  + + 
=
G
o o C H s
H
 ( ln )
ln ( | )
diag
Po 
−
=−
AA
C
   A.5 1194 
Please see Table 1 for a definition of the variables in these equations. 1195 
Modelling ‘Twenty Questions’ 
 
 56 
Table 1 – Expressions pertaining to models of discrete states: the shaded rows describe hidden states and 1196 
auxiliary variables, while the remaining rows describe model parameters and functionals.  1197 
Expression Description 
,
{0,1 }
[0,1]
o
    

=  o πo
 Outcomes and their posterior expectations 
,
{0,1 }
[0,1]
s
    

=  s πs
 Hidden states and their posterior expectations 
,,   =o As
 Expected outcome, under a particular policy 
1
{ 1, , }
( , , ) [0,1]K
K
=π π π
 
Policies specifying state transitions and their 
posterior expectations 
, , , ,
,,
ln :
()
       
   
==
=
ν s ε v
s ν
 
Auxiliary variable representing depolarisation 
and expected state, under a particular policy 
, , 1 , 1 , , 1
,
ln ln
ln ln o
         
  
− − += + 
+  −
ε B s B s
As
 
Auxiliary variables representing state 
prediction error 
A
 
The likelihood of an outcome under each 
hidden state 
,B
 
Time dependent probability transition matrices 
specified by the policy 
Modelling ‘Twenty Questions’ 
 
 57 
ln ( | )Po =−C
 
Prior surprise about outcomes; i.e. prior cost or 
inverse preference 
D
 
(Empirical) Prior expectations about initial 
hidden states 
( , )F  =F
 Variational free energy for each policy 
( , )G  =G
 Expected free energy for each policy 
()o Go=G
 Expected free energy for next outcome 
exp( )() exp( )



 −−= −
GG G
 
Softmax function, returning a vector that 
constitutes a proper probability distribution. 
 1198 
 1199 
  1200 
Modelling ‘Twenty Questions’ 
 
 58 
References 1201 
Adams, R.A., Shipp, S., Friston, K.J., 2013. Predictions not commands: active inference in the 1202 
motor system. Brain Struct Funct. 218, 611-643. 1203 
Al-Muhaideb, S., Menai, M.E., 2011. Evolutionary computation approaches to the Curriculum 1204 
Sequencing problem. Natural Computing 10, 891-920. 1205 
Allwood, J., Nivre, J., Ahlsén, E., 1992. On the semantics and pragmatics of linguistic 1206 
feedback. Journal of semantics 9, 1-26. 1207 
Altmann, G., Steedman, M., 1988. Interaction with context during human sentence processing. 1208 
Arnal, L.H., Giraud, A.L., 2012. Cortical oscillations and sensory predictions. Trends Cogn 1209 
Sci 16, 390-398. 1210 
Arnone, D., Patel, A., Tan, G.M. -Y., 2006. The nosological significance of Folie à Deux: a 1211 
review of the literature. Ann Gen Psychiatry 5, 11-11. 1212 
Bahdanau, D., Cho, K., Bengio, Y., 2014. Neural machine translation by jointly learning to 1213 
align and translate. arXiv preprint arXiv:1409.0473. 1214 
Barlow, H., 1961. Possible principles underlying the transformations of sensory messages, in: 1215 
Rosenblith, W. (Ed.), Sensory Communication. MIT Press, Cambridge, MA, pp. 217-234. 1216 
Barlow, H.B., 1974. Inductive inference, coding, perception, and language. Perception 3, 123-1217 
134. 1218 
Barsalou, L.W., 2003. Situated simulation in the human conceptual system. Language and 1219 
Cognitive Processes 18, 513-562. 1220 
Bengio, Y., #233, #244, Louradour, m., Collobert, R., Weston, J., 2009. Curriculum learning, 1221 
Proceedings of the 26t h Annual International Conference on Machine Learning. ACM, 1222 
Montreal, Quebec, Canada, pp. 41-48. 1223 
Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C., 2003. A neural probabilistic language 1224 
model. Journal of machine learning research 3, 1137-1155. 1225 
Modelling ‘Twenty Questions’ 
 
 59 
Bever, T.G., 1970. The cognitive basis for linguistic structures. Cognition and the development 1226 
of language 279, 1-61. 1227 
Bornkessel, I., Zysset, S., Friederici, A.D., von Cramon, D.Y., Schlesewsky, M., 2005. Who 1228 
did what to whom? The neural basis of argument hierarchies during language comprehension. 1229 
Neuroimage 26, 221-233. 1230 
Braiman, C., Fridman, E.A., Conte, M.M., Voss, H.U., Reichenbach, C.S., Reichenbach, T., 1231 
Schiff, N.D., 2018. Cortical Response to the Natural Speech Envelope Correlates with 1232 
Neuroimaging Evidence of Cognition in Severe Brain Injury. Current biology : CB 28, 3833 -1233 
3839.e3833. 1234 
Brown, E.C., Brune, M., 2012. The role of prediction in social neuroscience. Front Hum 1235 
Neurosci 6, 147. 1236 
Catal, O., Nauta, J., Verbelen, T., Simoens, P., Dhoedt, B., 2019. Bayesian P olicy Selection 1237 
Using Active Inference. Workshop on “Structure & Priors in Reinforcement Learning" at ICLR 1238 
2019 : Proceedings. 2019. 1239 
Chomsky, N., 2017. The language capacity: architecture and evolution. Psychon Bull Rev 24, 1240 
200-203. 1241 
Clark, A., 2016. Surfin g Uncertainty: Prediction, Action, and the Embodied Mind. Oxford 1242 
University Press. 1243 
Collins, A.G.E., Frank, M.J., 2013. Cognitive control over learning: Creating, clustering and 1244 
generalizing task-set structure. Psychological review 120, 190-229. 1245 
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P., 2011. Natural 1246 
language processing (almost) from scratch. Journal of machine learning research 12, 2493 -1247 
2537. 1248 
Modelling ‘Twenty Questions’ 
 
 60 
Constant, A., Ramstead, M.J.D., Veissiere, S.P.L., Friston, K., 2019. Regimes of Expectations: 1249 
An Active Inference Model of Social Conformity and Human Decision Making. Front Psychol 1250 
10, 679. 1251 
Coulson, S., King, J.W., Kutas, M., 1998. Expect the unexpected: Event-related brain response 1252 
to morphosyntactic violations. Language and Cognitive Processes 13, 21-58. 1253 
Creanza, N., Feldman, M.W., 2014. Complexity in models of cultural niche construction with 1254 
selection and homophily. Proceedings of the National Academy of Sciences of the United 1255 
States of America 111, 10830-10837. 1256 
Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M., Parikh, D., Batra, D., 2017. 1257 
Visual dialog, Proceedings of the IEEE Conference on Computer Vision and Pattern 1258 
Recognition, pp. 326-335. 1259 
Dauwels, J., 2007. On Variational Message Passing on Factor Graphs, 2007 IEEE International 1260 
Symposium on Information Theory, pp. 2546-2550. 1261 
Davis, M.H., Johnsrude, I.S., 2003. Hierarchical processing in spoken language 1262 
comprehension. Journal of Neuroscience 23, 3423-3431. 1263 
Dayan, P., Hinton, G.E., Neal, R.M., Zemel, R.S., 1995. The Helmholtz machine. Neural 1264 
Comput 7, 889-904. 1265 
de Lafuente, V., Jazayeri, M., Shadlen, M.N., 2015. Representation of accumulating evidence 1266 
for a decision in two parietal areas. J Neurosci 35, 4306-4318. 1267 
Dehaene, S., Meyniel, F., Wacongne, C.,  Wang, L., Pallier, C., 2015. The Neural 1268 
Representation of Sequences: From Transition Probabilities to Algebraic Patterns and 1269 
Linguistic Trees. Neuron 88, 2-19. 1270 
Demberg, V., Keller, F., 2019. Cognitive Models of Syntax and Sentence Processing. Human 1271 
Language: From Genes and Brains to Behavior. Cambridge, MA: MIT Press. 1272 
Deneve, S., 2008. Bayesian spiking neurons I: inference. Neural Comput 20, 91-117. 1273 
Modelling ‘Twenty Questions’ 
 
 61 
Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of deep bidirectional 1274 
transformers for language understanding. arXiv preprint arXiv:1810.04805. 1275 
Donchin, E., Coles, M.G.H., 1988. Is the P300 component a manifestation of context updating? 1276 
Behavioral and Brain Sciences 11, 357. 1277 
Dowty, D.R., Karttunen, L., Zwicky, A.M., 1985. Natural langu age parsing: Psychological, 1278 
computational, and theoretical perspectives, in: Joshi, A.K. (Ed.), Tree adjoining grammars: 1279 
How much context -sensitivity is required to provide reasonable structural descriptions? 1280 
Cambridge Univ. Press, Cambridge, UK, pp. 206 - 250. 1281 
Edwards, E., Chang, E.F., 2013. Syllabic ( approximately 2 -5 Hz) and fluctuation ( 1282 
approximately 1-10 Hz) ranges in speech and auditory processing. Hearing research 305, 113-1283 
134. 1284 
Elman, J.L., 1990. Finding structure in time. Cognitive science 14, 179-211. 1285 
Ferro, M., Ognibene, D., Pezzulo, G., Pirrelli, V., 2010. Reading as active sensing: a 1286 
computational model of gaze planning during word recognition. Frontiers in Neurorobotics 4, 1287 
1. 1288 
Fleming, S.M., Dolan, R.J., Frith, C.D., 2012. Metacognition: comput ation, biology and 1289 
function. Philos Trans R Soc Lond B Biol Sci. 367, 1280-1286. 1290 
Foerster, J.N., Assael, Y.M., de Freitas, N., Whiteson, S., 2016. Learning to communicate to 1291 
solve riddles with deep distributed recurrent q-networks. arXiv preprint arXiv:1602.02672. 1292 
Foerster, J.N., Song, F., Hughes, E., Burch, N., Dunning, I., Whiteson, S., Botvinick, M., 1293 
Bowling, M., 2018. Bayesian action decoder for deep multi -agent reinforcement learning. 1294 
arXiv preprint arXiv:1811.01458. 1295 
Friederici, A.D., 2011. The Brain Basis of Language Processing: From Structure to Function. 1296 
Physiological Reviews 91, 1357-1392. 1297 
Modelling ‘Twenty Questions’ 
 
 62 
Friston, K., 2010. The free -energy principle: a unified brain theory? Nat Rev Neurosci. 11, 1298 
127-138. 1299 
Friston, K., 2013. Life as we know it. J R Soc Interface 10, 20130475. 1300 
Friston, K., Buzsaki, G., 2016. The Functional Anatomy of Time: What and When in the Brain. 1301 
Trends Cogn Sci. 1302 
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O'Doherty, J., Pezzulo, G., 2016. 1303 
Active inference and learning. Neuroscience and biobehavioral reviews 68, 862-879. 1304 
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G., 2017a. Active Inference: 1305 
A Process Theory. Neural Comput 29, 1-49. 1306 
Friston, K., Frith, C., 2015a. A Duet for one. Conscious Cogn 36, 390-405. 1307 
Friston, K., Levin, M., Sengupta, B., Pezzulo, G., 2015. Knowing one's place: a free -energy 1308 
approach to pattern regulation. J R Soc Interface 12. 1309 
Friston, K., Mattout, J., Trujillo-Barreto, N., Ashburner, J., Penny, W., 2007. Variational free 1310 
energy and the Laplace approximation. NeuroImage 34, 220-234. 1311 
Friston, K., Penny, W., 2011. Post hoc Bayesian model selection. Neuroimage 56, 2089-2099. 1312 
Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., Dolan, R.J., 2014. 1313 
The anatomy of choice : dopamine and decision -making. Philosophical transactions of the 1314 
Royal Society of London. Series B, Biological sciences 369. 1315 
Friston, K.J., Frith, C.D., 2015b. Active inference, communication and hermeneutics(). Cortex; 1316 
a journal devoted to the study of the nervous system and behavior 68, 129-143. 1317 
Friston, K.J., Lin, M., Frith, C.D., Pezzulo, G., Hobson, J.A., Ondobaka, S., 2017b. Active 1318 
Inference, Curiosity and Insight. Neural Comput 29, 2633-2683. 1319 
Friston, K.J., Parr, T., de Vries, B., 2017c. The graphic al brain: Belief propagation and active 1320 
inference. Network neuroscience (Cambridge, Mass.) 1, 381-414. 1321 
Modelling ‘Twenty Questions’ 
 
 63 
Friston, K.J., Rosch, R., Parr, T., Price, C., Bowman, H., 2017d. Deep temporal models and 1322 
active inference. Neuroscience and biobehavioral reviews 77, 388-402. 1323 
Frith, C., Wentzer, T., 2013. Neural Hermeneutics, in: Kaldis, B. (Ed.), Encyclopedia of 1324 
Philosophy and the Social Sciences. SAGE publications, Inc, Thousand Oaks. 1325 
Garrod, S., Pickering, M.J., 2009. Joint Action, Interactive Alignment,  and Dialog. Topics in 1326 
Cognitive Science 1, 292-304. 1327 
George, D., Hawkins, J., 2009. Towards a mathematical theory of cortical micro-circuits. PLoS 1328 
Comput Biol 5, e1000532. 1329 
Gershman, S.J., 2017. Predicting the Past, Remembering the Future. Curr Opin Behav S ci 17, 1330 
7-13. 1331 
Ghazanfar, A.A., Takahashi, D.Y., 2014. The evolution of speech: vision, rhythm, cooperation. 1332 
Trends Cogn Sci 18, 543-553. 1333 
Ghazvininejad, M., Brockett, C., Chang, M. -W., Dolan, B., Gao, J., Yih, W. -t., Galley, M., 1334 
2018. A knowledge-grounded neural conversation model, Thirty-Second AAAI Conference on 1335 
Artificial Intelligence. 1336 
Gibson, E., 1998. Linguistic complexity: Locality of syntactic dependencies. Cognition 68, 1-1337 
76. 1338 
Gibson, E., 2000. The dependency locality theory: A distance -based theory of  linguistic 1339 
complexity. Image, language, brain 2000, 95-126. 1340 
Giraud, A.L., Poeppel, D., 2012. Cortical oscillations and speech processing: emerging 1341 
computational principles and operations. Nat Neurosci 15, 511-517. 1342 
Goldwater, S., 2006. Nonparametric Bayesi an Models of Lexical Acquisition. Brown 1343 
University. 1344 
Gregory, R.L., 1980. Perceptions as hypotheses. Phil Trans R Soc Lond B. 290, 181-197. 1345 
Modelling ‘Twenty Questions’ 
 
 64 
Hale, J., 2001. A probabilistic Earley parser as a psycholinguistic model, Proceedings of the 1346 
second meeting of the N orth American Chapter of the Association for Computational 1347 
Linguistics on Language technologies. Association for Computational Linguistics, pp. 1-8. 1348 
Hassabis, D., Maguire, E.A., 2007. Deconstructing episodic memory with construction. Trends 1349 
Cogn Sci 11, 299-306. 1350 
Hauser, M.D., Chomsky, N., Fitch, W.T., 2002. The faculty of language: What is it, who has 1351 
it, and how did it evolve? Science 298, 1569-1579. 1352 
Hawkins, R.X., Stuhlmüller, A., Degen, J., Goodman, N.D., 2015. Why do you ask? Good 1353 
questions provoke informative answers. Citeseer. 1354 
Helmholtz, H., 1878 (1971). The Facts of Perception, in: Middletown, R.K. (Ed.), The Selected 1355 
Writings of Hermann von Helmholtz. Wesleyan University Press, Connecticut, p. 384. 1356 
Hoeting, J.A., Madigan, D., Raftery, A.E., Volinsky, C.T., 1999. Bayesian Model Averaging: 1357 
A Tutorial. Statistical Science 14, 382-401. 1358 
Hohwy, J., 2016. The Self-Evidencing Brain. Noûs 50, 259-285. 1359 
Hovsepyan, S., Olasagasti, I., Giraud, A. -L., 2018. Combining predictive coding with neural 1360 
oscillations optimizes on-line speech processing. bioRxiv, 477588. 1361 
Howard, R., 1966. Information Value Theory. IEEE Transactions on Systems, Science and 1362 
Cybernetics SSC-2, 22-26. 1363 
Hyafil, A., Cernak, M., 2015. Neuromorphic based oscillatory device for incremental syllable 1364 
boundary detection. Idiap. 1365 
Isomura, T., Parr, T., Friston, K., 2019. Bayesian Filtering with Multiple Internal Models: 1366 
Toward a Theory of Social Intelligence. Neural Comput, 1-42. 1367 
Itti, L., Baldi, P., 2009. Bayesian Surprise Attracts Human Attention. Vision Res. 49, 1295 -1368 
1306. 1369 
Modelling ‘Twenty Questions’ 
 
 65 
Jaakkola, T., Jordan, M., 1998. Improving the Mean Field Approximation Via the Use of 1370 
Mixture Distributions, in: Jordan, M. (Ed.), Learning in Graphical Models, vol. 89. Springer 1371 
Netherlands, pp. 163-173. 1372 
Kamide, Y., Scheepers, C., Altm ann, G.T., 2003. Integration of syntactic and semantic 1373 
information in predictive processing: Cross -linguistic evidence from German and English. 1374 
Journal of psycholinguistic research 32, 37-55. 1375 
Kayser, C., 2019. Evidence for the Rhythmic Perceptual Sampling of Auditory Scenes. Front 1376 
Hum Neurosci 13, 249. 1377 
Khani, F., Goodman, N.D., Liang, P., 2018. Planning, Inference and Pragmatics in Sequential 1378 
Language Games. Transactions of the Association for Computational Linguistics 6, 543-555. 1379 
Kiebel, S.J., Daunizeau, J., Friston, K.J., 2009. Perception and hierarchical dynamics. Front 1380 
Neuroinform 3, 20. 1381 
Kira, S., Yang, T., Shadlen, M.N., 2015. A neural implementation of Wald's sequential 1382 
probability ratio test. Neuron 85, 861-873. 1383 
Kschischang, F.R., Frey, B. J., Loeliger, H.A., 2001. Factor graphs and the sum -product 1384 
algorithm. Ieee T Inform Theory 47, 498-519. 1385 
Kuchling, F., Friston, K., Georgiev, G., Levin, M., 2019. Morphogenesis as Bayesian 1386 
inference: A variational approach to pattern formation and control in complex biological 1387 
systems. Phys Life Rev. 1388 
Kutas, M., Hillyard, S.A., 1984. Brain potentials during reading reflect word expectancy and 1389 
semantic association. Nature 307, 161. 1390 
Leonard, M.K., Baud, M.O., Sjerps, M.J., Chang, E.F., 2016. Perceptual restoration of masked 1391 
speech in human cortex. Nat Commun 7, 13619. 1392 
Levy, R., 2008. Expectation-based syntactic comprehension. Cognition 106, 1126-1177. 1393 
Modelling ‘Twenty Questions’ 
 
 66 
Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., Jurafsky, D., 2016. Deep reinforcement 1394 
learning for dialogue generation. arXiv preprint arXiv:1606.01541. 1395 
Lightfoot, D.W., 1979. Principles of diachronic syntax. Cambridge Univ. Press, Cambridge, 1396 
UK, and New York. 1397 
Lindley, D.V., 1956. On a Measure of the Information Provided by an Experiment. Ann. Math. 1398 
Statist. 27, 986-1005. 1399 
Linsker, R., 1990. Perceptual neural organization: some approaches based on network models 1400 
and information theory. Annu Rev Neurosci. 13, 257-281. 1401 
Liu, X., Duh, K., Gao, J., 2018. Stochastic answer networks for natural language inference. 1402 
arXiv preprint arXiv:1804.07888. 1403 
Lizarazu, M., Lallier, M., Molinaro, N., 2019. Phase -amplitude coupling between theta and 1404 
gamma oscillations adapts to speech rate. Ann N Y Acad Sci. 1405 
MacKay, D.J.C., Peto, L.C.B., 1995. A hierarchical Dirichlet language model.  Natural 1406 
Language Engineering 1, 289-308. 1407 
Maisto, D., Donnarumma, F., Pezzulo, G., 2015. Divide et impera: subgoaling reduces the 1408 
complexity of probabilistic inference and problem solving.  12, 20141335. 1409 
Mar, R.A., 2011. The Neural Bases of Social Cognitio n and Story Comprehension, in: Fiske, 1410 
S.T., Schacter, D.L., Taylor, S.E. (Eds.), Annual Review of Psychology, Vol 62, vol. 62, pp. 1411 
103-134. 1412 
Mathewson, K.W., Castro, P.S., Cherry, C., Foster, G., Bellemare, M.G., 2019. Shaping the 1413 
Narrative Arc: An Information-Theoretic Approach to Collaborative Dialogue. arXiv preprint 1414 
arXiv:1901.11528. 1415 
Mattys, S.L., White, L., Melhorn, J.F., 2005. Integration of multiple speech segmentation cues: 1416 
A hierarchical framework. Journal of Experimental Psychology-General 134, 477-500. 1417 
Mikolov, T., 2010. Statistical language models based on neural networks. 1418 
Modelling ‘Twenty Questions’ 
 
 67 
Mikolov, T., Yih, W. -t., Zweig, G., 2013. Linguistic regularities in continuous space word 1419 
representations, Proceedings of the 2013 Conference of the North American Chapter of t he 1420 
Association for Computational Linguistics: Human Language Technologies, pp. 746-751. 1421 
Mirza, M.B., Adams, R.A., Mathys, C.D., Friston, K.J., 2016. Scene Construction, Visual 1422 
Foraging, and Active Inference. Frontiers in computational neuroscience 10, 56. 1423 
Norris, D., McQueen, J.M., Cutler, A., 2016. Prediction, Bayesian inference and feedback in 1424 
speech recognition. Language, cognition and neuroscience 31, 4-18. 1425 
Ognibene, D., Baldassarre, G., 2014. Ecological Active Vision: Four Bio -Inspired Principles 1426 
to Integrate Bottom-Up and Adaptive Top-Down Attention Tested With a Simple Camera-Arm 1427 
Robot, IEEE Transactions onAutonomous Mental Development. IEEE, p. 99. 1428 
Optican, L., Richmond, B.J., 1987. Temporal encoding of two -dimensional patterns by single 1429 
units in primate inferior cortex. II Information theoretic analysis. J Neurophysiol. 57, 132-146. 1430 
Page, M.P., Norris, D., 1998. The primacy model: a new model of immediate serial recall. 1431 
Psychol Rev 105, 761-781. 1432 
Parr, T., Friston, K.J., 2018. The Anatomy of Inference : Generative Models and Brain 1433 
Structure. Frontiers in computational neuroscience 12. 1434 
Parr, T., Markovic, D., Kiebel, S.J., Friston, K.J., 2019. Neuronal message passing using Mean-1435 
field, Bethe, and Marginal approximations. Scientific reports 9, 1889. 1436 
Pefkou, M., Arnal, L.H., Fontolan, L., Giraud, A.L., 2017. theta -Band and beta-Band Neural 1437 
Activity Reflects Independent Syllable Tracking and Comprehension of Time -Compressed 1438 
Speech. J Neurosci 37, 7930-7938. 1439 
Penn, D.C., Holyoak, K.J., Povinelli, D.J., 2008. Darwin's mistake: Explaining the 1440 
discontinuity between human and nonhuman minds. Behavioral and Brain Sciences 31, 109-+. 1441 
Modelling ‘Twenty Questions’ 
 
 68 
Pennington, J., Socher, R., Manning, C., 2014. Glove: Global vectors for word representation, 1442 
Proceedings of the 2014 conference  on empirical methods in natural language processing 1443 
(EMNLP), pp. 1532-1543. 1444 
Potts, C., 2012. Goal-driven answers in the Cards dialogue corpus. 1445 
Pulvermuller, F., Lutzenberger, W., Birbaumer, N., 1995. Electrocortical distinction of 1446 
vocabulary types. Electroencephalogr Clin Neurophysiol 94, 357-370. 1447 
Purpura, K.P., Kalik, S.F., Schiff, N.D., 2003. Analysis of perisaccadic field potentials in the 1448 
occipitotemporal pathway during active vision. J Neurophysiol 90, 3455-3478. 1449 
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., 2019. Language models 1450 
are unsupervised multitask learners. 1451 
Rizzolatti, G., Craighero, L., 2004. The mirror-neuron system. Annual Review of Neuroscience 1452 
27, 169-192. 1453 
Rosenfeld, R., 2000. Two decades of statistical language modeling: Where do we go from here? 1454 
Proceedings of the Ieee 88, 1270-1278. 1455 
Roy, D., 2005. Semiotic schemas: A framework for grounding language in action and 1456 
perception. Artificial Intelligence 167, 170-205. 1457 
Salakhutdinov, R., Tenenbaum, J.B., Torralba, A., 20 13. Learning with hierarchical -deep 1458 
models. IEEE transactions on pattern analysis and machine intelligence 35, 1958-1971. 1459 
Sallans, B., Hinton, G.E., 2004. Reinforcement Learning with Factored States and Actions. 1460 
Journal of Machine Learning Research 5, 1063–1088. 1461 
Schegloff, E.A., Sacks, H., 1973. Opening up closings. Semiotica 8, 289-327. 1462 
Schmidhuber, J., 2006. Developmental robotics, optimal artificial curiosity, creativity, music, 1463 
and the fine arts. Connection Science 18, 173-187. 1464 
Modelling ‘Twenty Questions’ 
 
 69 
Schwartenbeck, P., FitzGe rald, T.H., Mathys, C., Dolan, R., Friston, K., 2015. The 1465 
Dopaminergic Midbrain Encodes the Expected Certainty about Desired Outcomes. Cereb 1466 
Cortex 25, 3434-3445. 1467 
Serban, I.V., Sordoni, A., Bengio, Y., Courville, A., Pineau, J., 2016. Building end -to-end 1468 
dialogue systems using generative hierarchical neural network models, Thirtieth AAAI 1469 
Conference on Artificial Intelligence. 1470 
Shang, L., Lu, Z., Li, H., 2015. Neural responding machine for short -text conversation. arXiv 1471 
preprint arXiv:1503.02364. 1472 
Shea, N., Bo ldt, A., Bang, D., Yeung, N., Heyes, C., Frith, C.D., 2014. Supra -personal 1473 
cognitive control and metacognition. Trends Cogn Sci 18, 186-193. 1474 
Shipp, S., Adams, R.A., Friston, K.J., 2013. Reflections on agranular architecture: predictive 1475 
coding in the motor cortex. Trends Neurosci. 36, 706-716. 1476 
Singer, W., Sejnowski, T.J., Rakic, P., 2019. The neocortex. 1477 
Sordoni, A., Bengio, Y., Vahabi, H., Lioma, C., Grue Simonsen, J., Nie, J. -Y., 2015. A 1478 
hierarchical recurrent encoder -decoder for generative context -aware qu ery suggestion, 1479 
Proceedings of the 24th ACM International on Conference on Information and Knowledge 1480 
Management. ACM, pp. 553-562. 1481 
Specht, K., 2014. Neuronal basis of speech comprehension. Hearing research 307, 121-135. 1482 
Steels, L., 2011. Modeling the cultural evolution of language. Physics of Life Reviews 8, 339-1483 
356. 1484 
Sutskever, I., Vinyals, O., Le, Q., 2014. Sequence to sequence learning with neural networks. 1485 
Advances in NIPS. 1486 
Teh, Y.W., Jordan, M.I., Beal, M.J., Blei, D.M., 2006. Hierarchical Dirichlet Processes. Journal 1487 
of the American Statistical Association 101, 1566-1581. 1488 
Modelling ‘Twenty Questions’ 
 
 70 
Tenenbaum, J.B., Kemp, C., Griffiths, T.L., Goodman, N.D., 2011. How to grow a mind: 1489 
statistics, structure, and abstraction. Science 331, 1279-1285. 1490 
Tervo, D.G., Tenenbaum, J.B., Gersh man, S.J., 2016. Toward the neural implementation of 1491 
structure learning. Curr Opin Neurobiol 37, 99-105. 1492 
Van Petten, C., Coulson, S., Rubin, S., Plante, E., Parks, M., 1999. Time course of word 1493 
identification and semantic integration in spoken language. Jo urnal of Experimental 1494 
Psychology: Learning, Memory, and Cognition 25, 394. 1495 
Van Petten, C., Kutas, M., 1990. Interactions between sentence context and word 1496 
frequencyinevent-related brainpotentials. Memory & cognition 18, 380-393. 1497 
Van Petten, C., Luka, B.J., 2012. Prediction during language comprehension: Benefits, costs, 1498 
and ERP components. International Journal of Psychophysiology 83, 176-190. 1499 
Van Schijndel, M., Exley, A., Schuler, W., 2013. A model of language processing as hierarchic 1500 
sequential prediction. Topics in Cognitive Science 5, 522-540. 1501 
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., 1502 
Polosukhin, I., 2017. Attention is all you need, Advances in neural information processing 1503 
systems, pp. 5998-6008. 1504 
Vinyals, O., Le, Q., 2015. A neural conversational model. arXiv preprint arXiv:1506.05869. 1505 
Wilson, B., Marslen -Wilson, W.D., Petkov, C.I., 2017. Conserved Sequence Processing in 1506 
Primate Frontal Cortex. Trends Neurosci 40, 72-82. 1507 
Wilson, M., Wilson, T., 2005. An oscillator model of the timing of turn -taking. Psychonomic 1508 
Bulletin & Review 12, 957-968. 1509 
Winn, J., Bishop, C.M., 2005. Variational message passing. Journal of Machine Learning 1510 
Research 6, 661-694. 1511 
Modelling ‘Twenty Questions’ 
 
 71 
Ylinen, S., Huuskonen, M., Mikkola, K., Saure, E., Sinkko nen, T., Paavilainen, P., 2016. 1512 
Predictive coding of phonological rules in auditory cortex: A mismatch negativity study. Brain 1513 
and language 162, 72-80. 1514 
Young, T., Hazarika, D., Poria, S., Cambria, E., 2018. Recent trends in deep learning based 1515 
natural language processing. ieee Computational intelligenCe magazine 13, 55-75. 1516 
Yufik, Y.M., 1998. Virtual Associative Networks: A framework for cognitive modeling, in: K. 1517 
Pribram (ed.), Brain and Values, LEA, New Jersey, 109-177. 1518 
Yufik, Y.M., 2019. The Understanding Capacity and Information Dynamics in the Human 1519 
Brain. Entropy 21, 308, 1-38. 1520 
Yuille, A., Kersten, D., 2006. Vision as Bayesian inference: analysis by synthesis? Trends 1521 
Cogn Sci. 10, 301-308. 1522 
Zhang, C., Butepage, J., Kjellstrom, H., Mandt, S., 2018. Advances in Variational Inference. 1523 
IEEE transactions on pattern analysis and machine intelligence. 1524 
Zhao, T., Zhao, R., Eskenazi, M., 2017. Learning discourse -level diversity for neural dialog 1525 
models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960. 1526 