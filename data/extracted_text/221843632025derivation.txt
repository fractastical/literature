S2 Appendix: Derivation of the expected free energy in the exploration
phase
During the exploration phase, the preference distribution is designed to encourage the agent to seek the relevant
information to complete its model. The preferences are equal to the world model marginalized over actions:
pref(St+1, Bt+1|bt) =
X
a
π(a|bt) p(Bt+1|bt, a) p(St+1|Bt+1). (1)
Plugging this result in the expected free energy in Eq. (4), one obtains:
Gbt [at] =Ebt+1,st+1∼p(Bt+1,St+1|bt,at)[log p(bt+1|bt, at) − log pref(st+1, bt+1|bt, at)] (2)
=
X
bt+1,st+1
p(bt+1|bt, at) p(st+1|bt+1) (3)
"
log p(bt+1|bt, at) − log
X
a
π(a|bt) p(bt+1|bt, a) p(st+1|bt+1)
#
(4)
=
X
bt+1,st+1
p(bt+1|bt, at) p(st+1|bt+1) (5)
"
log p(bt+1|bt, at) − log
X
a
π(a|bt) p(bt+1|bt, a) − log p(st+1|bt+1)
#
(6)
=
X
bt+1
p(bt+1|bt, at)
"
−
X
s
p(st+1|bt+1) logp(st+1|bt+1)
#
(7)
+
X
bt+1
p(bt+1|bt, at)
"
log p(bt+1|bt, at) − log
X
a
π(a|bt) p(bt+1|bt, a)
#
.
In the second line, we replace the preference distribution by its definition with respect to the world model,
and the additivity rule of the logarithm is used to move to the third. In the fourth fourth line, we separated the
expectation values over Bt+1 and St+1 by noticing that for two random variables X, Y,Ex∼p(X)[f(Y = y)] = f(y)
by the normalization constraint on probability distributions. As a result, terms are grouped by dependency, where
in particular, the second term has become independent of sensory states. The first term is the conditional entropy of
sensory states, conditioned on belief states. For some belief state bt+1, and when the world model is clone-structured,
the likelihood is a delta function: p(st+1|bt+1) = δst+1,s(bt+1) where s(bt+1) designates the observation bt+1 is a clone
of. As a result, when the sensory states match, the logarithm vanishes, and otherwise, it is multiplied by zero.
Therefore, the entropy over observations cancels by design. The remaining term reduces to the Kullback-Leibler
divergence between the transition function and the transition function marginalized over actions:
Gbt [at] = DKL
"
p(Bt+1|bt, at)||
X
a
π(a|bt) p(Bt+1|bt, a)
#
(8)
= DKL [p(Bt+1|bt, at)||p(Bt+1|bt)] (9)
= IG(Bt+1, At = at), (10)
where p(Bt|bt) is the marginal of the transition function over actions, and in the last line, IG(X|Y = y) is the
information gain about X from knowing the value of Y = y.
1