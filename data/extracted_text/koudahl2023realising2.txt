arXiv:2306.08014v2  [cs.AI]  16 Oct 2023
Realising Synthetic Active Inference Agents,
Part I: Epistemic Objectives and Graphical
Speciﬁcation Language
Magnus Koudahl1,2, Thijs van de Laar1, and Bert de V ries1,3
1BIASLab, Department of Electrical Engineering, Eindhoven
University of T echnology , The Netherlands
2VERSES AI Research Lab, Los Angeles, CA, USA, 90016
3GN Hearing, JF Kennedylaan 2, 5612 AB Eindhoven, The
Netherlands
October 17, 2023
Abstract
The F ree Energy Principle (FEP) is a theoretical framework f or de-
scribing how (intelligent) systems self-organise into coh erent, stable struc-
tures by minimising a free energy functional. Active Infere nce (AIF) is a
corollary of the FEP that speciﬁcally details how systems th at are able to
plan for the future (agents) function by minimising particu lar free energy
functionals that incorporate information seeking compone nts. This paper
is the ﬁrst in a series of two where we derive a synthetic versi on of AIF
on free form factor graphs. The present paper focuses on deri ving a local
version of the free energy functionals used for AIF. This ena bles us to
construct a version of AIF which applies to arbitrary graphi cal models
and interfaces with prior work on message passing (MP) algor ithms. The
resulting messages are derived in our companion paper. We al so identify a
gap in the graphical notation used for factor graphs. While f actor graphs
are great at expressing a generative model, they have so far b een unable
to specify the full optimisation problem including constra ints. T o solve
this problem we develop constrained F orney-style factor gr aph (CFFG)
notation which permits a fully graphical description of var iational infer-
ence objectives. We then proceed to show how CFFGs can be used to
reconstruct prior algorithms for AIF as well as derive new on es. The
latter is demonstrated by deriving an algorithm that permit s direct pol-
icy inference for AIF agents, circumventing a long standing scaling issue
that has so far hindered the application of AIF in industrial settings. We
demonstrate our algorithm on the classic T-maze task and sho w that it
reproduces the information seeking behaviour that is a hall mark feature
of AIF.
1
1 Introduction
Active Inference (AIF) is an emerging framework for modelli ng intelligent agents
interacting with an environment. Originating in the ﬁeld of computational neu-
roscience, it has since been spread to numerous other ﬁelds s uch as modern
machine learning. At its core, AIF relies on variational inf erence techniques
to minimise a free energy functional. A key diﬀerentiator of AIF compared to
other approaches is the use of custom free energy functional s such as Expected
and Generalised free energies (expected free energy (EFE) a nd generalised free
energy (GFE), respectively). These functionals are speciﬁ cally constructed so
as to elicit epistemic, information-seeking behaviour whe n used to infer actions.
Optimisation of these functionals have so far relied on cust om algorithms
that require evaluating very large search trees which has re ndered upscaling of
AIF diﬃcult. Many recent works, such as Branching Time AIF [3 ] and sophis-
ticated inference [7] have investigated algorithmic ways t o prune the search tree
in order to solve this problem.
This paper is the ﬁrst in a series of two parts where we take a di ﬀerent
approach and formulate a variation of the GFE optimisation p roblem using
a custom Lagrangian derived from constrained Bethe free ene rgy (CBFE) on
factor graphs. Using variational calculus we then derive a c ustom MP algorithm
that can directly solve for ﬁxed points of local GFE terms, re moving the need for
a search tree. This allows us to construct a purely optimisat ion-based approach
to AIF which we name Lagrangian Active Inference (LAIF).
LAIF applies to arbitrary graph topologies and interfaces w ith generic MP
algorithms which allow for scaling up of AIF using oﬀ-the-sh elf tools. W e ac-
complish this by constructing a node-local GFE on generic fa ctor graphs.
The present paper is structured as follows: In section 2, we r eview relevant
background material concerning F orney-style factor graph s (FFGs) and Bethe
free energy (BFE). In section 3 we formalise what we mean by "e pistemics" and
construct an objective that is local to a single node on an FFG and possesses
an epistemic, information-seeking drive. This objective t urns out to be a local
version of the GFE which we review in section 3.2.
Once we start modifying the free energy functional, we recog nise a prob-
lem with current FFG notation. FFGs visualise generative mo dels but fail to
display a signiﬁcant part of the optimisation problem, name ly , the variational
distribution and the functional to be optimised.
T o remedy this, we develop the CFFG graphical notation in sec tion 5 as a
method for visualising both the variational distribution a nd any adaptations to
the free energy functional that are needed for AIF. These too ls form the basis
of the update rules derived in our companion paper [15].
With the CFFG notation in hand, in section 6 we then proceed to demon-
strate how to recover prior algorithms for AIF as MP on a CFFG. AIF is often
described as MP on a probabilistic graphical model, see [4, 6 , 13, 26] for exam-
ples. However, this relationship has not been properly form alised before, in part
because adequate notation has been lacking. Using CFFGs it i s straightforward
to accurately write down this relation. F urther, due to the m odular nature of
2
CFFGs it becomes easy to devise extensions to prior AIF algor ithms that can
be implemented using oﬀ-the-shelf MP tools.
Finally , section 7 demonstrates a new algorithm for policy i nference using
LAIF that scales linearly in the planning horizon, providin g a solution to a long-
standing barrier for scaling AIF to larger models and more co mplex tasks.
2 The Lagrangian approach to message passing
In this section, we review the Bethe free energy along with FF Gs. These con-
cepts form the foundation from which we will build towards lo cal epistemic,
objectives.
As a free energy functional, the BFE is unique because statio nary points of
the BFE correspond to solutions of the belief propagation al gorithm [20, 25, 28]
which provides exact inference on tree-structured graphs.
F urthermore, by adding constraints to the BFE using Lagrang e multipliers,
one can form a custom Lagrangian for an inference problem. T a king the ﬁrst
variation of this Lagrangian, one can then solve for station ary points and obtain
MP algorithms that solve the desired problem. Prior work [25 , 28] have shown
that adding additional constraints to the BFE allows for der iving a host of dif-
ferent message passing algorithms including variational m essage passing (VMP)
[27] and expectation propagation (EP) [18] among others. W e refer interested
readers to [25] for a comprehensive overview of this techniq ue and how diﬀerent
choices of constraints can lead to diﬀerent algorithms.
Constraints are speciﬁed at the level of nodes and edges, mea ning this pro-
cedure can produce hybrid MP algorithms, foreshadowing the approach we are
going to take for deriving a local, epistemic objective for A IF.
2.1 Bethe free energy and F orney-style factor graphs
Throughout the remainder of the paper we will use FFGs to visu alise probabilis-
tic models models. In Section 5 we extend this notation to add itionally allow
for specifying constraints on the variational optimisatio n objective.
F ollowing [25] we deﬁne an FFG as a graph G= (V, E) with nodes Vand
edges E⊆V×V . F or a node a ∈V we denote the connected edges by E(a).
Similarly for an edge i ∈E , we denote the connected nodes by V(i).
An FFG can be used to represent a factorised function (model) over variables
s, as
f(s) =
∏
a∈V
fa(sa) , (1)
where sa collects the argument variables of the factor fa. Throughout this
paper we will use cursive bold font to denote collections of variables. In the
corresponding FFG, the factor fa is denoted by a square node and connected
edges E(a) represent the argument variables sa.
3
fb fcfa
fd fe
s1 s2
s3 s4
Figure 1: Example of an FFG
As an example, we can consider the FFG shown in Fig. 1 which cor responds
to the model
f(s1, s 2, s 3, s 4) =fa(s1)fb(s1, s 2, s 3)fc(s2, s 4)fd(s3)fe(s4) (2)
In Fig. 1, the vertex set is V= {a, . . . , e }and the edge set is E= {1, . . . , 4}.
As an example, the neighbouring edges of the node c are given by E(c) ={2, 4}.
In this way , FFGs allow for a simple visualisation of the fact orisation properties
of a high-dimensional function.
The problem we will focus on concerns minimisation of a free e nergy func-
tional over a generative model. More formally , given a model (1) and a "varia-
tional" distribution q(s), the variational free energy (VFE) is deﬁned as
F [q] ≜
∫
q(s) logq(s)
f(s) ds . (3)
V ariational inference concerns minimising this functiona l, leading to the so-
lution
q∗ = arg min
q∈Q
F [q] , (4)
with Qdenoting the admissible family of functions q. The optimised VFE
upper-bounds the negative log-evidence (surprisal), as
F [q∗ ] =
∫
q∗(s) logq∗(s)
p(s) ds
  
Posterior divergence
−log Z  
Surprisal
, (5)
with Z =
∫
f(s)ds is the model evidence and the exact posterior is given by
p(s) =f(s)/Z .
The BFE applies the Bethe assumption to the factorisation of q which yields
an objective that decomposes into a sum of local free energy t erms, each local
to a node on the corresponding FFG. Each node local free energ y will include
entropy terms from all connected edges. Since an edge is conn ected to (at most)
4
two nodes, that means the corresponding entropy term would b e counted twice.
T o prevent overcounting of the edge entropies, the BFE inclu des additional
terms entropy terms that cancel out overcounted terms.
Under the Bethe approximation q(s) is given by
q(s) =
∏
a∈V
qa(sa)
∏
i∈E
qi(si)1− di (6)
with di the degree of edge i. As an example, on the FFG shown in Fig. 1
this would correspond to a variational distribution of the f orm
q(s1, . . . , s 4) = qa(s1)qb(s1, s 2, s 3)qc(s2, s 4)qd(s3)qe(s4)
q1(s1)q2(s2)q3(s3)q4(s4) (7)
where we see terms for the edges in the denominator and for the nodes in
the numerator. With this deﬁnition, the free energy factori ses over the FFG as
F [q] =
∑
a∈V
∫
qa(sa) logqa(sa)
fa(sa) dsa
  
F [qa]
+
∑
i∈E
(1 −di)
∫
qi(si) log 1
qi(si) dsi
  
H[qi ]
. (8)
Eq. (8) deﬁnes the BFE. Note that F deﬁnes a free energy functional which
can be either local or global depending on its arguments. Mor e speciﬁcally , F [q]
deﬁnes the free energy for the entire model, while F [qa] deﬁnes a node-local (to
the node a) BFE contribution of the same functional form.
Under optimisation of the BFE- solving Eq. (4) - the admissib le set of func-
tions Qenforces consistent normalisation and marginalisation of the node and
edge local distributions, such that
∫
qi(si) dsi = 1for all i ∈E (9a)
∫
qa(sa) dsa = 1for all a ∈V (9b)
∫
qa(sa) dsa\ i = qi(si) for all a ∈V , i ∈E (a) . (9c)
Because we always assume the constraints given by Eq. (9) to b e in eﬀect,
we will omit the subscript on individual q’s moving forwards and instead let the
arguments determine which marginal we are referring to, for example writing
q(sa) instead of qa(sa).
A core aspect of FFGs is that they allow for easy visualisatio n of MP al-
gorithms. MP algorithms are a family of distributed inferen ce algorithms with
the common trait that they can be viewed as messages ﬂowing on an FFG.
5
As a general rule, to infer a marginal for a variable, message s are passed on
the graph toward the associated edge for that variable. Mult iplication of col-
liding (forward and backward) messages on an edge yields the desired posterior
marginal. W e denote a message on an FFG by arrows pointing in t he direction
that the message ﬂows.
fb fcfa
fd fe
s1
→
s2
→←
s3↑ s4↑
Figure 2: Example of messages ﬂowing on an FFG
W e show an example in Fig. 2 of inferring a posterior marginal for the
variable s2. In this example, messages are ﬂowing on the FFG of Fig. (1)
towards the variable s2 where we see two arrows colliding. The exact form of
the individual messages depend on the algorithm being used.
3 Deﬁning epistemic objectives
Now we move on to the central topic of AIF, namely agents that i nteract with
the world they inhabit. Under the heading of AIF, an agent ent ails a generative
model of its environment and is engaged in the process of achi eving future goals
(speciﬁed by a target distribution or goal prior) through ac tions. This task can
be cast as a process of free energy minimisation [5, 7, 8]. A na tural question to
ask is then, what should this free energy functional look lik e and why? Here we
wish to highlight a core feature of AIF that sets it apart from other approaches:
Systematic information gathering by targeted exploration of an environment.
Whatever the form of our free energy functional, it should le ad to agents that
possess an exploratory drive consistent with AIF.
While it is tempting to default to a standard VFE, prior work [ 24] has
shown that directly optimising BFE or VFE when inferring a se quence of actions
(which we will refer to as a policy) does not lead to agents that systematically
explore their environment. Instead, directly inferring a p olicy by minimising a
VFE/BFE leads to Kullback-Leibler divergence (KL)-contro l [12] 1. This means
that VFE/BFE is not the correct choice when we desire an agent that actively
samples its environment with the explicit purpose of gather ing information.
Instead a hallmark feature of AIF is the use of alternative fu nctionals in
place of either the VFE or the BFE, speciﬁcally for inferring policies. The
goal of these alternative functionals is often speciﬁcally to induce an epistemic,
explorative term that drives AIF agents to seek out informat ion.
1Other works will sometimes use π as a symbol to denote a policy
6
Epistemics, epistemic behaviour or "foraging for informat ion" are commonly
used terms in the AIF literature and related ﬁelds. While we h ave used the term
colloquially until this point, we now clarify formally how w e use the term in the
present paper and how it relates to the objective functional s we consider. A
core problem is that epistemics is most often deﬁned in terms of the behaviour
of agents ("what does my agent do?") rather than from a mathematical point
of view. Prior work on this point includes [10, 17].
W e take the view that epistemics arise from the optimisation of either an mu-
tual information (MI) term or a bound thereon. The MI (betwee n two variables
x and z) is deﬁned as [16]
I[ x, z ] =
/dispiint
p(x, z ) log p(x, z )
p(x)p(z)dxdz . (10)
T o gain an intuition for why maximising MI leads to agents tha t seek out
information, we can rewrite MI as
I[ x, z ] =
/dispiint
p(x, z ) logp(z |x)
p(z) dxdz
= H[z] −H[z |x] = H[x] −H[x |z]
(11)
Since MI is symmetric in its arguments, Eq. (11) can equally w ell be written
in terms of x rather than z. Eq. (11) shows that MI decomposes as the diﬀerence
between the marginal entropy z and the expected entropy of z conditional on
x.
If we let z denote an internal state of an agent and x an observation and
allow our agent to choose x - for instance through acting on an environment
- we can see why maximising Eq. (10) biases the agent towards s eeking our
observations that reduce entropy in z. Maximising Eq. (10) means the agent
will prefer observations that provide useful (in the sense o f reducing uncertainty)
information about its internal states z. F or this reason MI is also known as
Information Gain.
Actually computing Eq. (10) is often intractable and in prac tice, a bound is
often optimised instead.
3.1 Constructing a local epistemic objective
At this point, we have seen that the BFE is deﬁned over arbitra ry FFGs, yet
does not lead to epistemic behaviour. On the other hand, maxi mising MI leads
to the types of epistemic behaviour we desire, yet is not dist ributed like the BFE.
The question becomes whether there is a way to merge the two an d obtain a
distributed functional - like the BFE- that includes an epis temic term?
W e will now show how to construct such a functional. Our start ing point
will be the BFE given by Eq. (8). W e will focus on a single node a and its
7
associated local energy term and partition the incoming edg es into two sets, x
and z. This gives the node local free energy
F [qa] =
/dispiint
q(x, z) logq(x, z)
f(x, z)dxdz (12)
Now we need to add on an MI term to induce epistemics. Since we a re
minimising our free energy functional and want to maximise M I, we augment
the free energy with a negative MI term as
G[ qa ] =
Variational free energy
  /dispiint
q(x, z) logq(x, z)
f(x, z) dxdz +
Negative mutual information
  /dispiint
q(x, z) logq(x)q(z)
q(x, z) dxdz (13a)
=
/dispiint
q(x, z) log✘✘✘✘q(x, z)
f(x, z) dxdz +
/dispiint
q(x, z) logq(x)q(z)
✘✘✘✘q(x, z) dxdz (13b)
=
/dispiint
q(x, z) logq(x)q(z)
f(x, z) dxdz (13c)
which we can recognise as a node-local GFE [19]. In section 3. 2 we review
the results of [19] to expand upon this statement. Eq. (13a) p rovides a straight-
forward explanation of the kinds of behaviour that we can exp ect out of agents
optimising a GFE. Namely , minimising BFE with a goal prior co rresponds to
performing KL-control [12] while the MI term adds an epistem ic, information-
seeking component. Viewed in this way , there is nothing myst erious about the
kind of objective optimised by AIF agents: It is simply the su m of two well-
known and established objectives that are each widely used w ithin the control
and reinforcement learning communities.
3.2 Generalised free energy
The objective derived in Eq. (13) is a node local version of th e GFE originally
introduced by [19]. In this section, we review the GFE as cons tructed by [19] in
order to relate our construction to prior work on designing A IF functionals. In
section 6, we show how to reconstruct the exact method of [19] using the tools
we develop in this paper. Prior to [19], the functional of cho ice was the EFE [4,
6]. [19] identiﬁed some issues with the EFE and proposed the G FE as a possible
solution.
W e show how to reconstruct the original EFE-based algorithm of [6] using
our local objective in section 6. W e also provide a detailed d escription of the
EFE in Appendix. A since it is still a popular choice for desig ning AIF agents.
A core issue with the EFE is that it is strictly limited to plan ning over future
timesteps. This means that AIF agents that utilise the EFE fu nctional need
to maintain two separate models: One for inferring policies (using EFE) and
one for state inference (using VFE/BFE) that updates as obse rvations become
8
available. The key advantage of EFE is that it induces the epi stemic drive that
we desire from AIF agents.
The goal of [19] was to extend upon the EFE by introducing a fun ctional
that could induce similar epistemic behaviour when used to i nfer policies while
at the same time reducing to a VFE when dealing with past data p oints. In
this way , an agent would no longer have to maintain two separa te models and
could instead utilise only one.
The GFE as introduced by [19] is tied to a speciﬁc choice of gen erative model.
The model introduced by [19] is given by
p(x, z |ˆ u) ∝ p(z0)
T∏
k=1
p(xk|zk)p(zk|ˆ uk, zk− 1)˜p(xk) (14)
where x denotes observations, z denotes latent states and ˆ udenotes a ﬁxed
policy . Throughout this paper we will use t to refer to the current time step in
a given model and denote ﬁxed values by a hat, here exempliﬁed by ˆ u. F urther,
we also use z to denote vectors.
Given a current time step t, we have that for future time steps k > t , ˜p(xk)
deﬁnes a goal prior over desired future observations. F or pa st time steps k ≤ t,
we instead have ˜p(xk) = 1which makes it uninformative 2. W riting the model
in this way allows for a single model for both perception (int egrating past data
points) and action (inferring policies) since both past and future time steps are
included. GFE is deﬁned as [19]
G[ q; ˆu ] =
T∑
k=1
/dispiint
q(xk|zk)q(zk|ˆ uk) logq(xk|ˆ uk)q(zk|ˆ uk)
˜p(xk)p(xk, zk|ˆ uk)dxkdzk (15)
where
q(xk|zk) =
{
δ(xk − ˆ xk) if k ≤ t
p(xk|zk) if k > t . (16)
and ˆ xk denotes the observed data point at time step k. p(xk, zk |ˆ uk) can
be found recursively by Bayesian smoothing, see [14, 23] for details. T o see how
the GFE reduces to a VFE when data is available, we refer to App endix B.
The GFE introduced by [19] improves upon prior work utilisin g EFE but
still has some issues. The ﬁrst lies in tying it to the model de ﬁnition in Eq. (14).
Being committed to a model speciﬁcation apriori severely li mits what the GFE
can be applied to since not all problems are going to ﬁt the mod el speciﬁcation.
Additionally , a more subtle issue lies in the commitment to a hard time horizon.
If t denotes the current timestep, at some point time will advanc e to the point
t > T . When this happens Eq. (14) loses the capacity to plan and bec omes
2[19] writes this as the prior being ﬂat to achieve a similar eﬀ ect
9
static since all observations are clamped by Eq. (16). A furt her complication
arises from ˆu being a ﬁxed parameter and not a random variable. Being a
ﬁxed parameter means it is not possible to perform inference for ˆu which in
turn makes scaling diﬃcult. Moving to a fully local version o f the GFE instead
means we can construct a new synthetic approach to AIF that ad dresses these
issues.
4 LAIF - Lagrangian Active Inference
Armed with the node local GFE derived in Eq. (13), we can const ruct a La-
grangian for AIF. The goal is to adapt the Lagrangian approac h to MP sketched
in section 2 to derive a MP algorithm that optimises local GFE in order to have
a distributed inference procedure that incorporates epist emic terms. Naively
applying the method of [25] to Eq. (13) does not yield useful r esults because the
numerator diﬀers from the term we take the expectation with r espect to. T o
obtain a useful solution we need that
q(x, z) =q(x |z)q(z) (17)
and add the original assumption in [19], given by Eq. (16)
q(x |z) ≜ p(x |z) . (18)
With these additional assumptions, we can obtain meaningfu l solutions and
derive a message passing algorithm that optimises a local GF E and induces
epistemic behaviour which we demonstrate in section7. The d etailed derivation
of these results can be found in our companion paper [15]. F or practical AIF
modelling, we provide the relevant messages in Fig. 21.
At this point, we will instead show a diﬀerent way to arrive at the local
GFE. This approach is more "mechanical" but has the advantag e that it can
more easily be written in terms of constraints on the BFE. Our starting point
will once again be a free energy term local to the node a
F [qa] =
∫
q(sa) logq(sa)
f(sa) dsa (19)
T o turn Eq. (19) into a local GFE we need to perform two steps. T he ﬁrst
is to enforce a mean ﬁeld factorisation
q(sa) =
∏
i∈E (a)
q(si) (20)
The second is to change the expectation to obtain
10
∫
q(sa) logq(sa)
f(sa)dsa =⇒
∫
p(si|sa\ i)q(sa\ i) logq(sa)
f(sa)dsa (21)
where i ∈E (a). This partitions the connected variables into two sets: A se t
of variables where the expectation is modiﬁed and any remain der that is not
modiﬁed. p(si |sa\ i) denotes a conditional probability distribution. W e write
p here instead of f to emphasise that the conditional needs to be normalised in
order for us to be able to take the expectation.
W e refer to this move as a P-substitution. Once the mean ﬁeld factorisation
is enforced, we can recognise Eq. (21) as a node-local GFE G[qa].
As an example of constructing a node-local GFE with this appr oach, we can
consider a node with two connected variables, {x, z }.
The local free energy becomes
F [qa] =
/dispiint
q(x, z ) logq(x, z )
p(x, z )dxdz (22)
Now we apply a mean ﬁeld factorisation
F [qa] =
/dispiint
q(x)q(z) logq(x)q(z)
p(x, z ) dxdz (23)
And ﬁnally , perform P-substitution to obtain a node-local G FE
/dispiint
q(x)q(z) logq(x)q(z)
p(x, z ) dxdz =⇒
/dispiint
p(x |z)q(z) logq(x)q(z)
p(x, z ) dxdz (24)
W e denote the set of nodes for which we want to perform P-subst itution with
P⊆V . When performing P-substitution, we are replacing a local variational
free energy F [qa] with a local generalised free energy G[ qa ]. Armed with P-
substitution, we can now write the simplest instance of a Lag rangian for Active
Inference
L[q] =
∑
a∈P
G[ qa ]
  
P-substituted subgraph
w / naive mean ﬁeld
+
∑
b∈V\P
F [qb]
  
Node local
free energies
+
∑
i∈E
(1 −di) H[ q(si)]  
Edge entropy
+
∑
a∈V
∑
i∈E
∫
λia(si)
[
q(si) −
∫
q(sa)dsa\ i
]
dsi
  
Marginalisation
+
∑
a∈V
λa
[ ∫
q(sa)dsa −1
]
  
Normalisation of node marginals
+
∑
i∈E
λi
[ ∫
q(si)dsi −1
]
  
Normalisation of edge marginals
.
(25)
11
With the Active Inference Lagrangian in hand, we can now solv e for station-
ary points using variational calculus and obtain MP algorit hms for LAIF. The
key insight is that messages ﬂowing out of Pderived from stationary points of
Eq. (25) will correspond to stationary points of the local GF E rather than BFE,
meaning the result will include an epistemic component. Thi s paves the way for
a localised version of AIF that applies to arbitrary graph st ructures, does not
suﬀer from scaling issues as the planning horizon increases , and can be solved
eﬃciently and asynchronously using MP.
5 Constrained F orney-style factor graphs
While FFGs are a useful tool for writing down generative mode ls, we have by
now established the importance of knowing the exact functio nal to be minimised.
This requires specifying not just the model f but also the family Qthrough
constraints and any potential P-substitutions. This is imp ortant if we want to
be able to succinctly specify not just the model but also the e xact inference
problem we aim to solve.
W e will now develop just such a new notation for writing const raints directly
as part of the FFG. W e refer to FFGs with added constraints spe ciﬁcation as
CFFGs.
Figure 3: An example FFG.
Fig 3 shows a model comprised of ﬁve edges and six nodes. FFGs t radition-
ally represent the model f using squares connected by lines as shown in Fig. 3.
The squares represent factors and the connections between t hem represent vari-
ables. Connecting an edge to a square node indicates that the variable on that
edge is an argument of the factor it is connected to.
The notation for CFFGs adheres to similar principles when sp ecifying f.
However, we augment the FFG with circular beads to indicate t he constraints
that deﬁne our family Q. Each factor of the variational distribution in q will
correspond to a bead and the position of a bead indicates to wh ich marginal
it refers - a bead on an edge denotes an edge marginal q(si) and a bead inside
a node denotes a node marginal q(sa). An empty bead will denote the default
normalisation constraints while a connection between bead s indicates marginal-
isation constraints following Eq. (9). These beads form the basic building blocks
of our notation.
T o write the objective corresponding to the model in Fig. 3 gi ven the default
constraints of Eq. (9), we add beads for every term and extend edges through the
12
node boundary to connect variables that are under marginali sation constraints
as shown in Fig. 4.
Figure 4: Example CFFG with normalisation and marginalisat ion constraints.
5.1 F actorisation constraints
W e will now extend our notation with the most common types of c onstraints
used for deﬁning Q. A common choice is factorisation of the variational distri -
bution with the most well-known example being the naive mean ﬁeld approx-
imation. Under a naive mean-ﬁeld factorisation, all margin als are considered
independent. F ormally this means we enforce
q(sa) =
∏
i∈E (a)
q(si) . (26)
T o write Eq. (26) on a CFFG we need to replace the joint node mar ginal
with the product of adjacent edge marginals.
fa
fa
Figure 5: Changing a local joint factorisation to a naive mea n ﬁeld assumption
on a CFFG.
T o do this we can replace the bead indicating the joint margin al with a bead
for each edge marginal in Eq. (26) as shown in Fig. 5.
The naive mean ﬁeld is the strongest factorisation possible . It is possible
to utilise less aggressive factorisations by appealing to a structured mean ﬁeld
approximation instead. The structured mean ﬁeld constrain t takes the form
q(sa) =
∏
n∈ l(a)
qn(sn
a ) (27)
13
where l(a) denotes a set of one or more edges connected to the node a such
that each element in E(a) can only appear in l(a) once [25]. F or example if
E(a) = {i, j, k }corresponding to variables {x, y, z }, we can factorise q(x, y, z )
as q(x)q(y, z ) or q(z)q(x, y ) but not as q(x, y )q(y, z ) since y appears twice. The
naive mean ﬁeld is a special case of the structured mean ﬁeld w here every
variable appears only by itself.
T o write a structured mean ﬁeld factorisation on a CFFG we can apply a
similar logic and replace the single bead denoting the joint with beads that
match the structure of l(a). Each set of variables that are factorised together
corresponds to a single bead connected to the edges in the set that factor to-
gether.
fa
fa
fa
Figure 6: Changing a local joint factorisation to structure d mean ﬁeld on a
CFFG.
Fig. 6 shows two example factorisations. The ﬁrst option fac torises the four
incoming edges into two sets of two while the second partitio ns the incoming
edges into two sets of one and a single set of two. Using these p rinciples it is
possible to specify complex factorisation constraints as p art of the CFFG by
augmenting each node on the original FFG.
The ﬁnal situation we need to consider is the case when a singl e node has
a variable or very high number of incoming edges. An example c ould be a
Gaussian Mixture Model with a variable number of mixture com ponents. On a
CFFG we indicate variable or large numbers of identical edge s by drawing two
of the relevant edges and separating them with dots ( ···) as shown in Fig. 7.
T o indicate factorisation constraints we can write either a joint or a naive mean
ﬁeld factorisation between the two edges, letting the dots d enote that a similar
factorisation applies to the remaining edges.
14
fa fa
Figure 7: Mean ﬁeld (left) and joint (right) factorisation c onstraints for variable
numbers of edges on a CFFG.
5.2 F orm constraints
W e will now extend CFFG notation with constraints on the func tional form of
nodes and edges. F orm constraints are used to enforce a parti cular form for a
local marginal on either an edge or a node. F or an edge si they enforce
∫
q(sa)dsa\ i = q(si) =g(si) (28)
where g(si) denotes the functional form we are constraining the edge mar ginal
si to take. F orm constraints on node marginals take the form
q(sa) =g(sa) (29)
Conventionally FFGs denote the form of a factor by a symbol in side the node.
W e adopt a similar convention to denote form constraints on q by adding symbols
within the corresponding beads. F or instance, we can indica te a Gaussian form
constraint on an edge as shown in Fig 8
N
Figure 8: Notation for enforcing a Gaussian form constraint on an edge.
Note that this is not dependent on the form of the neighbourin g factors. This
is a subtle point as it allows us to write approximations into the speciﬁcation
of Q. As an example, the unconstrained marginal in Fig 8 might be b imodal
or highly skewed but by adding a form constraint, we are enfor cing a Gaussian
approximation. Outside of a few special cases, enforcing fo rm constraints on
edges is rarely done in practice since the functional form of q most often follows
from optimisation [25].
A special case of form constraints is the case of dangling edg es (edges that
are not terminated by a factor node). T echnically these woul d not warrant a
bead since they would not appear explicitly in the BFE due to h aving degree
1. Intuitively this means that the edge marginal is only count ed once and we
therefore do not need to correct for overcounting. However, without a bead,
there is nowhere to annotate a form constraint which is probl ematic.
15
The solution for CFFG notation is to simply draw the bead anyw ay , in case
a form constraint is needed. This is formally equivalent to t erminating the
dangling edge by a factor node with the node function fa(sa) = 1. T erminating
the edge in this way means the edge in question now has degree 2 and therefore
warrants a bead. This is always a valid move since multiplica tion by 1 does not
change the underlying function [25].
W e can denote form constraints on node marginals in the same m anner as
edge marginals. W e show an example in Fig 9 where we enforce a G aussian
form constraint on one node marginal and a Wishart on the othe r. Again it is
important to note that these are constraints on q and not part of the underlying
model speciﬁcation f.
N W
Figure 9: Notation for enforcing form constraints on nodes.
T wo kinds of form constraints warrant extra attention: δ-constraints and
moment matching. W e will now deal with these in turn.
5.3 δ-constraints and data points
δ-constraints are the most commonly used form constraints be cause they allow
us to incorporate data points into a model. A δ-constraint on an edge deﬁnes
the function g(si) in Eq. (28) to be
g(si) =δ(si − ˆsi) (30)
What makes the δ-constraint special is that ˆsi can either be a known value
or a parameter to optimise [25]. In the case where ˆsi is known, it commonly
corresponds to a data point. W e will refer to this case as a dat a constraint and
denote it with a ﬁlled circle as shown in Fig 10
δ δ
Figure 10: T erminating and non-terminating notation for da ta constraints.
Data constraints are special because they denote observati ons. They also
block any information ﬂow across the edge in question [25]. B ecause they block
information ﬂow, CFFG notation optionally allows data cons traints to terminate
edges.
Here we wish to raise a subtle point about prior FFG notation. Previous
work has used small black squares to denote data constraints following [21].
In keeping with our convention, a small black square on a CFFG denotes a
16
δ-distributed variable in the model f rather than the variational distribution
q. Being able to diﬀerentiate data constrained variables in q and apriori ﬁxed
parameter of the model f allows us to be explicit about what actually constitutes
a data point for the inference problem at hand [ caticha_entropic_2012].
In the case where ˆsj is not known, it can be treated as a parameter to be
optimised. W e refer to this case as a δ-constraint or a pointmass constraint and
notate it with an unﬁlled circle as shown in Fig 11
δ
Figure 11: Notation for δ-constraints.
Unlike data constraints, the δ-constraint allows messages to pass and is there-
fore not allowed to terminate an edge. Optimising the value o f ˆsi under a
δ-constraint leads to EM as message passing [25].
5.4 Moment matching constraints
Moment matching constraints are special in that they replac e the hard marginal-
isation constraints of Eq. (8) with constraints of the form
q(si) =
∫
q(sa)dsa\ i =⇒
∫
q(sa)Ti(si)dsa =
∫
q(si)Ti(si)dsi (31)
where Ti(si) are the suﬃcient statistics of an exponential family distri bution.
This move loosens the marginalisation constraint by instea d only requiring that
the moments in question align. When taking the ﬁrst variatio n and solving, one
obtains the expectation propagation (EP) algorithm [18].
F or notational purposes, moment matching constraints are u nique in that
they involve both an edge- and a node-marginal. That means th e eﬀects are not
localised to a single bead. T o indicate which beads are invol ved, we replace the
solid lines between them with dashed lines instead.
W e denote moment matching constraints by an E inside the corresponding
edge-bead as shown in Fig 12. Choosing the edge-bead over the node-bead is
an arbitrary decision made mainly for convenience.
E E
Figure 12: Notation for moment matching with a single-sided (left) and double-
sided (right) node/edge pairs.
The left side of Fig. 12 shows notation for constraining a sin gle node/edge
pair by moment matching. If both nodes connected to an edge ar e under moment
matching constraints, the double-sided notation on the rig ht of Fig. 12 applies.
17
Given the modular nature of CFFG notation it is easy to compos e diﬀer-
ent local constraints to accurately specify a Lagrangian an d by extension an
inference problem. Adding custom marginal constraints to a CFFG is also
straightforward as it simply requires deﬁning the meaning o f a symbol inside a
bead.
5.5 P-substitution on CFFGs
The ﬁnal piece needed to represent the Active Inference Lagr angian on a CFFG
is P-substitution. Being able to represent LAIF on a CFFG is t he reason for
constructing the local GFE using a mean-ﬁeld factorisation and P-substitution.
This construction is much more amenable to the tools we have d eveloped so far
as we will now demonstrate.
Recall that P-substitution involves substituting part of t he model p for q in
the expectation only . T o write P-substitution on a CFFG, the logical notation
is therefore to replace a circle with a square. Fig. 13 shows a n example of adding
a P-substitution to a mean-ﬁeld factorised node marginal
fa
x
y
z fa
x
y
z
Figure 13: P-substitution on a CFFG with naive mean ﬁeld fact orisation.
The square notation for P-substitution on CFFGs implies a co nditioning
of the P-substituted variable on all other connected variab les that are not P-
substituted. F or example, in Fig. 13 the P-substitution cha nges local VFE to a
GFE by
/dispiiint
q(y)q(x)q(z) logq(y)q(x)q(z)
p(y, x, z ) dydxdz
=⇒
/dispiiint
p(y |x, z )q(x)q(z) logq(y)q(x)q(z)
p(y, x, z ) dydxdz
(32)
Here, the P-substituted variable is y, and the remainder are {x, z }.
5.6 CFFG Compression
The value of CFFG notation is measured by how much it aids othe r researchers
and practitioners in expressing their ideas accurately and succinctly . W e envi-
18
sion two main groups for whom CFFGs might be of particular int erest. The
ﬁrst group is comprised of mathematical researchers workin g on constrained free
energy optimisation on FFGs F or this group we expect that the notation de-
veloped so far will be both useful and practically applicabl e since work is often
focused on the intricacies of performing local optimisatio n. Commonly an FFG
in this tradition is small but a very high level of accuracy is desired in order to
be mathematically rigorous.
However there is a second group composed of applied research ers for whom
the challenge is to accurately specify a larger inference pr oblem and its solu-
tion to solve an auxiliary goal - for instance controlling a d rone, transmitting
a coded message or simulating some phenomenon using AIF. F or this group
of researchers in particular, CFFG notation as described so far might be too
verbose and the overhead of using it may not outweigh the bene ﬁts gained. T o
this end, we now complete CFFG notation by a mandatory compre ssion step.
The compression step is designed to remove redundant inform ation by en-
forcing an emphasis on deviations from a default BFE . By default, we mean
no constraints other than normalisation and marginalisati on and with a joint
factorisation around every node. Recall that default norma lisation constraints
are denoted by empty , round beads and marginalisation by con nected lines. A
joint factorisation means all incoming edges are connected and the node only
has a single, internal bead.
T o provide a recipe for compressing a CFFG, we will need the co ncept of a
bead chain . A bead chain is simply a series of beads connected by edges. I n the
following recipe, a bead will only be summarised as part of a c hain if it contains
no additional information, meaning if it is round and empty . T o compress a
CFFG, we follow a series of four steps:
1. Summarise every bead chain by their terminating beads.
2. F or nodes with no factorisation constraints, remove empt y , internal beads.
3. F or nodes with no factorisation constraints, remove all i nternal edges.
4. F or all factor nodes, push remaining internal beads to the border of the
corresponding factor node.
After performing these steps, we are left with a compressed v ersion of the
original CFFG where each node can be much smaller and more con cise. T o
exemplify , we apply the recipe to the CFFG in Fig. 14.
19
fa fbfc
fd fe
δ
=
ff fg
Eδ
Figure 14: Initial CFFG before compression.
Fig. 14 is a complicated graph with loops, dangling edges, an d multiple
diﬀerent factorisations in play . As a result, Fig. 14 covers a lot of special cases
that one might encounter on a CFFG. W e will now apply the steps in sequence,
starting by removing empty beads on chains. This removes mos t of the beads
in the inner loop as shown in Fig. 15
20
fa fbfc
fd fe
δ
=
ff fg
Eδ
fa fbfc
fd fe
δ
=
ff fg
Eδ
Figure 15: Step 1: Removing empty beads from chains. Aﬀected beads are
highlighted in red.
Note that the dangling edge extending from fa is treated as terminated by a
bead and the bead on the edge is removed. Next, we remove any in ternal beads
for nodes with no factorisation constraints. W e show this st ep in Fig. 16
21
fa fbfc
fd fe
δ
=
ff fg
Eδ
fa fbfc
fd fe
δ
=
ff fg
Eδ
Figure 16: Step 2. Removing beads on nodes with default facto risation. Aﬀected
nodes are highlighted in red.
The next step is to remove any internal edge extensions for no des with no
factorisation constraints. W e demonstrate this step in Fig . 17. Note that the
internal edge of the node fd does not get cancelled since one of the connected
edges is a pointmass in the model and therefore not present in q, meaning fd
does not have a default factorisation.
22
fa fbfc
fd fe
δ
=
ff fg
Eδ
fa fbfc
fd fe
δ
=
ff fg
Eδ
Figure 17: Step 3. Removing internal edges. Aﬀected edges ar e highlighted in
red.
Finally , we can push any remaining internal beads to their no de border, illus-
trated in Fig. 18. This step allows for writing the CFFG much m ore compactly ,
as we can see in Fig. 19.
23
fa fbfc
fd fe
δ
=
ff fg
Eδ
fa fbfc
fd fe
δ
=
ff fg
Eδ
Figure 18: Step 4. Pushing beads to node borders. Aﬀected bea ds are high-
lighted in red.
At this point, we have removed most of the beads and edges and s till retain
most of the relevant information around all nodes. What is le ft is only what
deviates from a default BFE speciﬁcation. The goal here is, a s stated initially , to
make it easier to work with CFFG’s for larger models which nec essitate working
with smaller nodes.
24
fa fbfc
fd fe
δ
=
ff fg
Eδ
fa fbfc
fd fe
δ
=
ff fg
Eδ
Figure 19: Compression allows for much more compact CFFGs wh ich in turn
are better suited for larger inference problems.
F rom Fig. 19 it is clear that compression allows for much more compact
CFFG’s which in turn are more amenable to larger CFFG’s. This is especially
true in the absence of any factorisation or form constraints and P-substitutions
in which case the compressed CFFG and underlying FFG are iden tical. W e see
this with nodes fc, ff , fg and = in Fig. 19.
6 Classical AIF and the original GFE algorithm
as special cases of LAIF
An integral part of working with MP algorithms is the choice o f schedule -
the order in which messages are passed. Many MP algorithms ar e iterative in
nature and can therefore be sensitive to scheduling. LAIF is also an iterative
MP algorithm and might consequently be sensitive to the choi ce of schedule.
A particularly interesting observation is that we can recov er both the clas-
sical AIF planning algorithm of [6] and the scheme proposed b y [19] as special
cases of LAIF by carefully choosing the schedule and perform ing model compar-
ison. This result extends upon prior work by [13] who reinter preted the classical
algorithm through the lens of MP on an FFG.
25
6.1 Example: CFFG and message updates for a discrete
observation model with goals
T
Cat
x
z
A
c 1↓
2↑
3
←
Figure 20: CFFG of composite node for LAIF on discrete state s paces, repro-
duced from [15].
In order to both demonstrate LAIF and recover prior work, we n eed to derive
the required messages. W e refer to our companion paper [15] f or the detailed
derivations and summarise the results here. F or the remaini ng paragraphs, we
will use x to denote that x is vector-valued and A to denote that A is either
a matrix or a tensor. F or the discrete case, we work with a comp osite node
corresponding to the factor
p(x |A, z) =Cat(x |Az) (33a)
˜p(x |c) =Cat(x |c) . (33b)
W e show the corresponding CFFG in Fig. 20 where we indicate th e neces-
sary factorisation and P-substitution. In the parlance of A IF Eq. 33a deﬁnes
a model composed of a discrete state transition ( T ) with transition matrix A
and a categorical goal prior ( Cat) with parameter vector c. Before stating the
messages, we deﬁne the vector
h(A) =−diag(AT log A) (34)
This term is often denoted H in other works [4, 19]. W e instead opt for
using lowercase since the term is a vector and for making the d ependence on A
explicit. In the following section, we will sometimes use an
overbar to denote
expectations, such that Eq(x)[g(x)] =g(x). Additionally , we deﬁne
26
ξ(A) =AT(
log c −log
(
Az
))
−h(A) (35a)
ρ = A
T(
log c −log
(
Az
))
−h(A) . (35b)
in order to make the expressions more concise. Where expecta tions cannot
be computed in closed form, we instead resort to Monte Carlo e stimates. With
this notation in place, we can write the required messages as
µ 1 (c) ∝D ir(c|Az + 1)
µ 2 (z) ∝C at(z|σ(ρ))
Solve: z
!
= σ(ρ(z) + logd)
µ 2 (z) ∝C at(z|σ(log z∗ −log d))
log µ 3 (A) = zTξ(A)
Ux = −zTρ
Figure 21: Message updates for the discrete composite node, reproduced from
[15].
Fig. 21 shows the message updates towards all connected vari ables as well as
the average energy term ( Ux) for the composite node. d denotes the parameters
of the incoming message from the rest of the CFFG on the edge z. Interestingly ,
the energy term Ux corresponds exactly to the EFE as used in standard AIF [4,
6].
Special attention needs to be given to the messages µ 2 (z) and µ 3 (A). While
µ 2 (z) can be solved for in closed form, in practice applying this re sult directly
can lead to unstable solutions that ﬂuctuate between multip le extrema. F or this
reason, we opt for parameterising the message by z and solving for parameters
of the marginal directly using Newtons method. Having found the optimum z∗
we can then substitute the result into the message expressio n to obtain a stable
solution.
The message µ 3 (A) does not follow a nice exponential family distribution.
T o circumvent this problem, we can pass on the logpdf directl y . When we need
to compute the marginal q(A), we can then use sampling procedures to estimate
the necessary expectations [1]. F urther details and full de rivations can be found
in our companion paper [15].
6.2 Reconstructing classical AIF
With our new messages in hand and equipped with CFFG notation , we can now
restate prior work unambiguously , starting with the classi cal algorithm of [6].
T o reconstruct the algorithm of [6], we start by deﬁning the g enerative model.
27
The generative model is a discrete partially observed Marko v decision process
(POMDP) over future time steps given by
p(xt+1:T , zt:T  
Future
|ˆ ut+1:T  
Policy
, x1:t, u1:t  
Past
)
∝ p(zt |x1:t, u1:t)  
State Prior
T∏
k=t+1
p(xk |zk)  
Likelihood
p(zk |zk− 1, ˆ uk)  
State T ransition
˜p(xk)
Goal prior
(36)
where
p(zt |x1:t, u1:t) =Cat(zt |d) (37)
p(zk |zk− 1, ˆ uk) =Cat(zk |Bˆuk zk− 1) (38)
p(xk |zk) =Cat(xk |Azk) (39)
˜p(xk) =Cat(xk |ck) . (40)
Here we let xk denote observations, zk latent states, and ˆ uk a ﬁxed control,
with the subscript k indicating the future time step in question. The initial sta te
zt represents the ﬁltering solution given the agents trajecto ry so far which we
summarise in the parameter vector d. Control signals correspond to particular
transition matrices Bˆ uk where we use the subscript to emphasize that each B
matches a particular control ˆ uk. The observation model is given by the known
matrix A. Note that Eq. (36) is not normalised as it includes goal prio rs over
x. The CFFG of (36) is shown in Fig 22.
TCat =
T
T =
T
Cat Cat
···ztd
Bˆut+1
xt+1
A
ct+1
zt+1
Bˆut+2
xt+2
A
ct+2
zt+2
Figure 22: CFFG of discrete POMDP as used for planning in stan dard AIF
models.
where T nodes denote a discrete state transition (multiplication o f a cate-
gorical variable by a transition matrix). Given a generativ e model with a ﬁxed
28
set of controls, the next step is to compute the EFE [4, 6, 13, 1 4]. W e provide
a brief description here and refer to Appendix A and [4, 13, 14 , 22] for more
detailed descriptions. The EFE is given by
G(ˆut+1:T ) =
T∑
k=t
/dispiint
p(xk |zk)q(zk |ˆ uk) log q(zk |ˆ uk)
p(xk, zk |ˆ uk)dxkdzk (41)
With a slight abuse of notation, we can compute EFE by ﬁrst app lying
transition matrices to the latent state and generating pred icted observations as
zk = Bˆuk zk− 1
xk = Azk . (42)
In Eq. (42) we slightly abuse notation by having zk (resp. xk) refer to the
prediction after applying the transition matrix Bˆuk (resp. A) instead of the
random variable as we have done elsewhere.
W e can recognise these operations as performing a forwards M P sweep using
belief propagation messages. With this choice of generativ e model, the EFE of
a policy ˆ ut+1:T is found by [4, Eq. D.2-3].
G(ˆut+1:T ) =
T∑
k=t+1
−diag
(
AT log A
) T
zk + xT
k (log xk −log ck) (43)
where ck denotes the parameter vector of a goal prior at the k’th time step.
T o select a policy we simply pick the sequence ˆ ut+1:T that results in the lowest
numerical value when solving Eq. (43).
T o write this method on the CFFG in Fig. 22, we can simply add me ssages to
the CFFG and note that the sum of energy terms of the P-substit uted composite
nodes in Fig. 21 matches Eq. (43). W e show this result in Fig. 2 3.
TCat =
T
T =
T
Cat Cat
···→→
↓
→
↓
→
→
→
↓
→
↓
→
→
→
Figure 23: Classical EFE computation on the corresponding C FFG.
29
Comparing diﬀerent policies (choices of ˆ ut+1:T ) and computing the energy
terms of the P-substituted composite nodes is then exactly e qual to the EFE-
computation detailed in [6].
6.3 Reconstructing the original GFE method
W e can further exemplify the capabilities of CFFG notation b y recapitulating
the update rules given in the original GFE paper [19]. T o reco ver the procedure
of [19], we need to extend the generative model to encompass p ast observations
as
p(x1:T , z0:T |ˆ u1:T )
∝ p(z0)
t∏
l=1
p(xl |zl)p(zl |zl− 1, ˆ ul)

 
Past
T∏
k=t+1
p(xk |zk)p(zk |zk− 1, ˆ uk)
Goal prior

˜p(xk)
  
Future
(44)
where
p(z0) =Cat(z0 |d)
p(zt |zt− 1, ˆ ut) =Cat(zt |Bˆ ut zt− 1)
p(xt |zt) =Cat(xt |Azt)
˜p(xk) =Cat(xk |ck) .
(45)
F or past time steps, we add data constraints and for future ti me steps we
perform P-substitution. W e also apply a naive mean ﬁeld fact orisation for every
node. The ﬁnal ingredient we need is a schedule that includes both forwards
and backwards passes as hinted at in [13]. F or t = 1 the corresponding CFFG
is shown in Fig. 24 with messages out of the P-substituted nod es highlighted in
red. These messages are given by µ 2 (z) in Fig. 21.
30
TCat =
T
δ
T =
T
Cat
T =
T
Cat
···→
←
↑
↓
→
←
↑
↑
→
→
←
↓
→
←
↓ ↑
→
→
zt
→
←
↓
→
←
↓ ↑
→
→
→
←
Figure 24: GFE computation on the CFFG.
With these choices, the update equations become identical t o those of [19].
Indeed, careful inspection of the update rules given in [19] reveals the com-
ponent parts of the P-substituted message. However, by usin g CFFGs and
P-substitution, we can cast their results as MP on generic gr aphs which imme-
diately generalises their results to free-form graphical m odels.
Once inference has converged, we note that [19] shows that GF E evaluates
identically to the EFE, meaning we can use the same model comp arison pro-
cedure to select between policies as we used for reconstruct ing the classical
algorithm. This shows how we can obtain the algorithm of [19] as a special case
of LAIF.
7 LAIF for policy inference
The tools presented in this chapter are not limited to restat ing prior work. In-
deed, LAIF oﬀers several advantages over prior methods, one of which is the
ability to directly infer a policy instead of relying on a pos t hoc comparison of
energy terms. T o demonstrate, we solve two variations of the classic T-maze
task [6]. This is a well-studied setting within the AIF liter ature and therefore
constitutes a good minimal benchmark. In the T-maze experim ent, the agent
lives in a maze with four locations as depicted in Fig. 25. The agent ( /smiley) starts
in position 1 and knows that a reward is present at either posi tion 2 or 3, but
not which one. At position 4 is a cue that informs the agent whi ch arm con-
tains the reward. The optimal action to take is therefore to ﬁ rst visit 4 and
learn which arm contains the reward before going to the rewar ded arm. Be-
cause this course of action requires delaying the reward, an agent following a
greedy policy behaves sub-optimally . The T-maze is therefo re considered a rea-
sonable minimal example of the epistemic, information-see king behaviour that
is a hallmark of AIF agents. W e implemented our experiments i n the reactive
MP toolbox RxInfer [2]. The source code for our simulations is available at
31
https://github.com/biaslab/LAIF.
1
32
4
/smiley
Figure 25: The T-maze environment
7.1 Model speciﬁcation
The generative model for the T-maze is an adaptation of the di screte POMDP
used by [6, 19]. W e assume the agent starts at some current tim estep t and
want to infer a policy up to a known time horizon T . The generative model is
then
p(x, z, u) ∝ p(zt)

Initial
state
T∏
k=t+1
p(xk |zk)  
Observation
model
p(zk |zk− 1, uk)  
T ransition
model
p(uk)
Control
prior
˜p(xk)
Goal
prior
(46)
where
p(zt) =Cat(zt|d) (47a)
p(xk|zk) =Cat(xk|Azk) (47b)
p(zk|zk− 1, uk) =
∏
n
Cat(zk|Bnzk− 1)unk (47c)
p(uk) =Cat(uk|ek) (47d)
˜p(xk) =Cat(xk|ck) . (47e)
uk is a one-hot encoded vector of length n. The notation unk picks the n’th
entry of uk. W e show the corresponding CFFG in Fig 26.
32
Cat TM = TM
B1 B4··· B1 B4···
T T
Cat Cat
Cat Cat
d zt zt+1
ut ut+1
et et+1
At At+1
ct ct+1
Figure 26: CFFG for the T-maze experiment
Eq. (47c) deﬁnes a mixture model over candidate transition m atrices indexed
by uk. W e give the details of this node function and the required me ssages in
Appendix C. The MP schedule is shown in Fig. 27 with GFE-based messages
highlighted in red
33
Cat TM = TM
↑ ↑··· ↑ ↑···
T T
Cat Cat
Cat Cat
→ →
←
→
←
→
←
→
↑ ↓ ↑ ↓
→ →
↑ ↓ ↑
→ →
→ →
Figure 27: Message passing schedule for the T-maze experime nt
F ollowing [6] we deﬁne the initial state and control prior as
d = (1, 0, 0, 0)T ⊗(0. 5, 0. 5)T (48a)
ek = (0. 25, 0. 25, 0. 25, 0. 25)∀k (48b)
with ⊗ denoting the Kronecker product. The transition mixture nod e re-
quires a set of candidate transition matrices. The T-maze ut ilises four possible
transitions, given below
B1 =




1 1 1 1
0 0 0 0
0 0 0 0
0 0 0 0



⊗I2 , B2 =




0 1 1 0
1 0 0 1
0 0 0 0
0 0 0 0



⊗I2
B3 =




0 1 1 0
0 0 0 0
1 0 0 1
0 0 0 0



⊗I2 , B4 =




0 1 1 0
0 0 0 0
0 0 0 0
1 0 0 1



⊗I2
(49)
where I2 denotes a 2 ×2 identity matrix. Note that these diﬀer slightly from
the original implementation of [6]. In [6] invalid transiti ons are represented
by an identity mapping where we instead model invalid transi tions by sending
the agent back to position 1. The likelihood matrix A is given by four blocks,
corresponding to the observation likelihood in each positi on.
34
A =




A1
A2
A3
A4



, (50)
with everything outside the blocks being set to 0. The blocks are
A1 =




0. 5 0 . 5
0. 5 0 . 5
0 0
0 0



, A2 =




0 0
0 0
α 1 −α
1 −α α




A3 =




0 0
0 0
1 −α α
α 1 −α



, A4 =




1 0
0 1
0 0
0 0



, (51)
with α being the probability of observing a reward. The goal prior i s given
by
ck = σ
(
(0, 0, c, −c)T ⊗(1, 1, 1, 1)T )
∀k (52)
with c being the utility ascribed to a reward and σ(·) the softmax function.
Inference for the parts of the model not in Pcan be accomplished using belief
propagation. W e follow the experimental setup of [6] and let c = 2, α = 0. 9. F or
inference, we perform two iterations of our MP procedure and use 20 Newton
steps to obtain the parameters of the outgoing message from t he P-substituted
nodes.
W e show the results in Fig. 28. The number in each cell is the po sterior
probability mass assigned to the corresponding action, wit h the most likely
actions highlighted in
red.
35
0. 25
0. 200. 20
0. 35
Controls at time step 1
0. 13
0. 300. 30
0. 26
Controls at time step 2
Figure 28: Posterior controls for the T-maze experiment
Fig. 28 shows an agent that initially prefers the epistemic a ction (move to
state 4) at time t + 1 and subsequently exhibits a preference for either of the
potentially rewarding arms (indiﬀerent between states 2 an d 3). This shows that
LAIF is able to infer the optimal policy and that our approach can reproduce
prior results on the T-maze.
Since CFFGs are inherently modular, they allow us to modify t he inference
task without changing the model. T o demonstrate, we now add δ-constraints
to the control variables. This corresponds to selecting the MAP estimate of the
control posterior [25] and results in the CFFG shown in Fig. 2 9. The schedule
and all messages remain the same as our previous experiment - however now the
agent will select the most likely course of action instead of providing us with a
posterior distribution.
36
Cat TM = TM
B1 B4··· B1 B4···
T T
Cat Cat
Cat Cat
δ δ
Figure 29: CFFG for the T-maze model with additional δ-constraints
Performing this experiment yields the policy shown in Fig. 3 0
0. 0
0. 00. 0
1. 0
Controls at time step 1
0. 0
0. 01. 0
0. 0
Controls at time step 2
Figure 30: Posterior controls for the T-maze experiment wit h δ-constraints
Fig. 30 once again shows that the agent is able to infer the opt imal policy for
solving the task. F or repeated runs, the agent will randomly select to move to
either position 2 or 3 at the second step due to minute diﬀeren ces in the Monte
Carlo estimates used for computing the messages. The pointm ass constraint
obscures this since it forces the marginal to put all mass on t he MAP estimate.
37
The point of repeating the experiments with δ-constraints is not to show
that the behaviour of the agent changes dramatically . Inste ad, the idea is to
demonstrate that CFFGs allow for modular speciﬁcation of AI F agents which
allows for adapting parts of the model without having to touc h the rest. In
this case, the only parts of inference that change are those i nvolving the control
marginal. This means all messages out of the P-substituted c omposite nodes
are unaﬀected since the δ-constraints are only applied to the control marginals.
8 Conclusions
In this paper we have proposed a novel approach to AIF based on lagrangian
optimisation which we have named Lagrangian Active Inferen ce (LAIF). W e
demonstrated LAIF on a classic benchmark problem from the AI F literature
and found that it inherits the epistemic drive that is a hallm ark feature of AIF.
LAIF presents three main advantages over previous algorith ms for AIF.
Firstly , an advantage of LAIF is the computational eﬃciency aﬀorded by
being able to pass backward messages instead of needing to pe rform forwards
rollouts for every policy . While LAIF is still an iterative p rocedure, the compu-
tational complexity of each iteration scales linearly in th e size of the planning
horizon T instead of exponentially .
A second advantage is that LAIF allows for directly inferrin g posteriors
over control signals instead of relying on a model compariso n step based on
EFE/GFE. This means that LAIF uniﬁes inference for percepti on, learning,
and actions into a single procedure without any overhead - it all becomes part
of the same inference procedure. See our companion paper [15 ] for more details.
Thirdly , LAIF is inherently modular and consequently works for freely deﬁn-
able CFFGs, while prior work has focused mostly on speciﬁc ge nerative models.
Extensions to hierarchical or heterarchical models are str aightforward and only
require writing out the corresponding CFFG.
W e have also introduced CFFG notation for writing down const raints and
P-subtitutions on an FFG. CFFGs are useful not just for AIF bu t for specifying
free energy functionals in general. CFFGs accomplish this t hrough a simple
and intuitive graphical syntax. Our hope is that CFFGs can be come a standard
tool similar to FFGs when it is desirable to write not just a mo del f but also
a family of approximating distributions Q. Speciﬁcally in the context of AIF
we have also introduced P-substitution as a way to modify the underlying free
energy functional. In doing so we have formalised the relati on between AIF and
message passing on a CFFG, paving the way for future developm ents.
In future work, we plan to extend LAIF to more node constructi ons to further
open up the scope of available problems that can be attacked u sing AIF.
38
Acknowledgements
This research was made possible by funding from GN Hearing A/ S. This work
is part of the research programme Eﬃcient Deep Learning with project number
P16-25 project 5, which is (partly) ﬁnanced by the Netherlan ds Organisation
for Scientiﬁc Research (NWO).
W e gratefully acknowledge stimulating discussions with th e members of the
BIASlab research group at the Eindhoven University of T echn ology and mem-
bers of VERSES Research Lab, in particular Ismail Senoz, Bar t van Erp, Dmitry
Bagaev, Karl F riston, Chris Buckley , Conor Heins and Tim V er belen.
References
[1] Semih Akbayrak, Ivan Bocharov, and Bert de V ries. “ Exten ded variational
message passing for automated approximate Bayesian infere nce”. In: En-
tropy 23.7 (2021), p. 815.
[2] Dmitry Bagaev and Bert de V ries. “ Reactive Message Passi ng for Scalable
Bayesian Inference”. en. In: (2022). Submitted to the Journ al of Machine
Learning Research.
[3] Théophile Champion et al. “ Branching Time Active Infere nce: The the-
ory and its generality”. In: Neural Networks 151 (2022), pp. 295–316. issn:
0893-6080. doi: https://doi.org/10.1016/j.neunet.2022.03.036. url:
https://www.sciencedirect.com/science/article/pii/S0893608022001149.
[4] Lancelot Da Costa et al. “ Active inference on discrete st ate-spaces: a syn-
thesis”. en. In: arXiv:2001.07203 [q-bio] (Jan. 2020). arXiv: 2001.07203.
url: http://arxiv.org/abs/2001.07203 (visited on 01/22/2020).
[5] Karl F riston. “ A free energy principle for a particular p hysics”. In: arXiv:1906.10184
[q-bio] (June 2019). arXiv: 1906.10184. url: http://arxiv.org/abs/1906.10184
(visited on 06/12/2020).
[6] Karl F riston et al. “ Active inference and epistemic valu e”. In: Cogni-
tive Neuroscience 6.4 (Oct. 2015), pp. 187–214. issn: 1758-8928. doi:
10.1080/17588928.2015.1020053. url: https://doi.org/10.1080/17588928.2015.1020053
(visited on 09/09/2019).
[7] Karl F riston et al. “ Sophisticated Inference”. In: Neural Computation 33.3
(Mar. 2021), pp. 713–763. issn: 0899-7667. doi: 10.1162/neco_a_01351.
url: https://doi.org/10.1162/neco_a_01351 (visited on 12/22/2021).
[8] Karl J. F riston et al. “ Action and behavior: a free-energ y formulation”. en.
In: Biological Cybernetics 102.3 (Mar. 2010), pp. 227–260. issn: 0340-1200,
1432-0770. doi: 10.1007/s00422-010-0364-z . url: http://link.springer.com/10.1007/s00422-010-0364-z
(visited on 04/13/2020).
[9] Karl J. F riston et al. SPM12 toolbox, http://www.ﬁl.ion.ucl.ac.uk/spm/softwa re/.
2014.
39
[10] Danijar Hafner et al. “ Action and Perception as Diverge nce Minimization”.
In: arXiv:2009.01791 [cs, math, stat] (Oct. 2020). arXiv: 2009.01791. url:
http://arxiv.org/abs/2009.01791 (visited on 01/07/2021).
[11] Conor Heins et al. “ pymdp: A Python library for active in ference in dis-
crete state spaces”. In: arXiv preprint arXiv:2201.03904 (2022).
[12] Hilbert J. Kappen, Vicenç Gómez, and Manfred Opper. “ Op timal control
as a graphical model inference problem”. en. In: Machine Learning 87.2
(May 2012), pp. 159–182. issn: 0885-6125, 1573-0565. doi: 10.1007/s10994-012-5278-7 .
url: http://link.springer.com/10.1007/s10994-012-5278-7 (visited
on 02/07/2020).
[13] Magnus Koudahl, Christopher L Buckley , and Bert de V rie s. “ A Mes-
sage Passing Perspective on Planning Under Active Inferenc e”. In: Active
Inference: Third International Workshop, IW AI 2022, Grenoble, F rance,
September 19, 2022, Revised Selected Papers . Springer. 2023, pp. 319–327.
[14] Magnus T. Koudahl, W outer M. Kouw, and Bert de V ries. “ On Epistemics
in Expected F ree Energy for Linear Gaussian State Space Mode ls”. In: En-
tropy 23.12 (Nov. 2021), p. 1565. issn: 1099-4300. doi: 10.3390/e23121565.
url: http://dx.doi.org/10.3390/e23121565.
[15] Thijs van de Laar, Magnus Koudahl, and Bert de V ries. “ Re alising Syn-
thetic Active Inference Agents, Part II: V ariational Messa ge Updates”. In:
(2023). arXiv: 2306.02733 [stat.ML].
[16] David MacKay. Information theory, inference and learning algorithms .
Cambridge university press, 2003.
[17] Beren Millidge et al. “ Understanding the Origin of Info rmation-Seeking
Exploration in Probabilistic Objectives for Control”. In: arXiv:2103.06859
[cs] (June 2021). arXiv: 2103.06859. url: http://arxiv.org/abs/2103.06859
(visited on 07/02/2021).
[18] Thomas P . Minka. “ Expectation Propagation for Approxi mate Bayesian
Inference”. In: Proceedings of the Seventeenth Conference on Uncertainty
in Artiﬁcial Intelligence . Uai’01. San F rancisco, CA, USA: Morgan Kauf-
mann Publishers Inc., 2001, pp. 362–369. isbn: 978-1-55860-800-9. url:
http://dl.acm.org/citation.cfm?id=2074022.2074067 (visited on 04/04/2018).
[19] Thomas Parr and Karl J. F riston. “ Generalised free ener gy and active in-
ference”. en. In: Biological Cybernetics 113.5-6 (Dec. 2019), pp. 495–513.
issn: 0340-1200, 1432-0770. doi: 10.1007/s00422-019-00805-w . url:
http://link.springer.com/10.1007/s00422-019-00805-w (visited on
08/18/2020).
[20] Judea Pearl. “ Reverend Bayes on Inference Engines: A Di stributed Hierar-
chical Approach”. In: Proceedings of the Second AAAI Conference on Ar-
tiﬁcial Intelligence . Aaai’82. Pittsburgh, Pennsylvania: AAAI Press, 1982,
pp. 133–136. url: http://www.aaai.org/Papers/AAAI/1982/AAAI82-032.pdf
(visited on 07/28/2017).
40
[21] Christoph Reller. State-space methods in statistical signal processing: New
ideas and applications . V ol. 23. ETH Zurich, 2013.
[22] Noor Sajid, Philip J. Ball, and Karl J. F riston. “ Active inference: demys-
tiﬁed and compared”. en. In: arXiv:1909.10863 [cs, q-bio] (Jan. 2020).
arXiv: 1909.10863. url: http://arxiv.org/abs/1909.10863 (visited on
02/10/2020).
[23] Simo Sarkka. Bayesian Filtering and Smoothing . en. Cambridge: Cam-
bridge University Press, 2013. isbn: 978-1-139-34420-3. doi: 10.1017/cbo9781139344203.
url: http://ebooks.cambridge.org/ref/id/CBO9781139344203 (visited
on 04/04/2019).
[24] Sarah Schwöbel, Stefan Kiebel, and Dimitrije Marković . “ Active Inference,
Belief Propagation, and the Bethe Approximation”. en. In: Neural Compu-
tation 30.9 (Sept. 2018), pp. 2530–2567. issn: 0899-7667, 1530-888x. doi:
10.1162/neco_a_01108. url: https://direct.mit.edu/neco/article/30/9/2530-2567/8396
(visited on 05/13/2021).
[25] İsmail Şenöz et al. “ V ariational Message Passing and Lo cal Constraint Ma-
nipulation in F actor Graphs”. en. In: Entropy 23.7 (July 2021). Number:
7 Publisher: Multidisciplinary Digital Publishing Instit ute, p. 807. doi:
10.3390/e23070807. url: https://www.mdpi.com/1099-4300/23/7/807
(visited on 10/04/2021).
[26] Ryan Smith, Karl F riston, and Christopher Whyte. A Step-by-Step Tuto-
rial on Active Inference and its Application to Empirical Data . Jan. 2021.
doi: 10.31234/osf.io/b4jm6.
[27] John Winn and Christopher M. Bishop. “ V ariational mess age passing”.
In: Journal of Machine Learning Research 6.4 (2005), pp. 661–694. url:
http://www.jmlr.org/papers/volume6/winn05a/winn05a.pdf.
[28] Dan Zhang et al. “ Unifying message passing algorithms u nder the frame-
work of constrained Bethe free energy minimization”. In: IEEE Transac-
tions on Wireless Communications 20.7 (2021), pp. 4144–4158.
A Expected free energy
This section will focus on the most common alternative funct ional used for AIF,
the expected free energy (EFE).
EFE is the standard choice for AIF models and is what is found i n most AIF
focused software such as SPM [9] and PyMDP [11]. In this section, we will cover
the details of how EFE is commonly treated.
EFE is deﬁned speciﬁcally over future time steps and on a part icular choice
of generative model. The model in question is a state space mo del (SSM) of the
form
41
p(x, z |ˆu) =p(zt)
State
prior
T∏
k=t+1
p(xk|zk)  
Observation
model
p(zk|zk− 1, ˆuk)  
T ransition
model
(53)
where ˆuk denotes a particular control parameter that is known aprior i and
ﬁxed. Note that (53) does not include a prior over actions u and is instead
conditional on a ﬁxed policy ˆu = ˆ ut+1:T . W e use the ˆnotation on u to indicate
that it is treated as a known value instead of a random variabl e.
The EFE is evaluated as a function of a particular policy . The EFE is deﬁned
as [6]
G[q; ˆu] =
T∑
k=t+1
/dispiint
p(xk|zk) q(zk|ˆuk) log q(zk|ˆuk)
p(xk, z k|ˆuk)  
VFE conditioned on ˆuk
dxkdzk (54a)
=
T∑
k=t+1
/dispiint
p(xk|zk)q(zk|ˆuk) log q(zk|ˆuk)
p(zk|xk, ˆuk) −log p(xk)dxkdzk (54b)
where p(xk) denotes a goal prior over preferred observations. Note that
p(xk) is not part of the generative model in Eq. (53). T o compute Eq. (54) we
also need
p(xk, z k |ˆuk) =
∫
p(xk |zk)p(zk |zk− 1, ˆuk)q(zk− 1)dzk− 1 (55)
meaning we use the forward prediction from the previous time step to com-
pute p(xk, z k |ˆuk). If we further we assume that
q(xk, z k |ˆuk) =p(xk |zk)q(zk |ˆuk) (56)
It can be shown [4, 6, 14, 22] that (54) can be decomposed into a bound on
a mutual information term and a cross-entropy loss between p redicted observa-
tions and the goal prior.
B VFE and GFE
In this section, we walk through how the formulation of GFE gi ven by [19]
reduces to the VFE when observations are available. T o show h ow this comes
about, we note that [19] assumes that
q(xk|zk) =
{
δ(xk − ˆ xk) if k ≤ t
p(xk|zk) if k > t . (57)
42
where t is the current time step and ˆ xk denotes the observed data point at
time step k. This is a problematic move since q(xk |zk) is no longer a function
of zk for k ≤ t. However if we do take Eq. (16) as valid and further assume
q(xk, zk |ˆ uk) =q(xk |zk)q(zk |ˆ uk) (58a)
˜p(xk) = 1if k ≤ t (58b)
and note that
q(xk|ˆ uk) =
∫
q(xk|zk)q(zk|ˆ uk)dzk . (59)
W e can plug this result into (15) and obtain for k ≤ t
t∑
k=1
/dispiint
q(xk|zk)q(zk |ˆ uk) logq(xk |ˆ uk)q(zk |ˆ uk)
p(xk, zk |ˆ uk)˜p(xk) dxkdzk
=
t∑
k=1
/dispiint
q(xk|zk)q(zk |ˆuk) log
∫
[q(xk|zk)q(zk |ˆ uk)] dzkq(zk |ˆ uk)
p(xk, zk |ˆ uk) dxkdzk
(60a)
where the last line follows from Eq. (58b). Then by (16) we hav e
=
t∑
k=1
/dispiint
δ(xk − ˆ xk)q(zk |ˆ uk) log
∫[δ(xk − ˆ xk)q(zk |ˆ uk)] dzkq(zk |ˆ uk)
p(xk, zk |ˆ uk) dxkdzk
(61)
Now we pull δ(xk − ˆxk) out of the integral and solve to ﬁnd
=
t∑
k=1
/dispiint
δ(xk − ˆ xk)q(zk |ˆ uk) log
δ(xk − ˆ xk)
Integrates to 1
  [ ∫
q(zk |ˆ uk)dzk
]
q(zk |ˆ uk)
p(xk, zk |ˆ uk) dxkdzk
(62a)
=
t∑
k=1
∫
q(zk |ˆ uk) log q(zk |ˆ uk)
p(ˆ xk, zk |ˆ uk) dzk (62b)
which we can recognise as a VFE with data constraints.
43
C The transition mixture node
In this section, we derive the MP rules required for the T rans ition Mixture node
used in our experiments on policy inference. Before doing so , we ﬁrst establish
some preliminary results for the categorical distribution and the standard tran-
sition node. In this section we will also on occasion use an
overbar to indicate
an expectation, Eq(x)[g(x)] =g(x) The categorical distribution is given by
Cat(x |z) =
I∏
i=1
zxi
i (63a)
=
I∑
i=1
xizi (63b)
where x and z are both one-hot encoded vectors. The move Eq. (63a) to
Eq. (63b) is possible only because x is one-hot encoded. Similarly , we have for
the standard T ransition node that
Cat(x |Az) =
I∏
i=1
I∏
j=1
[
Aij
] xizj
(64a)
=
I∑
i=1
I∑
j=1
xizj Aij (64b)
and again, since x and z are one-hot encoded
log Cat(x |Az) = log
[ I∏
i=1
I∏
j=1
[
Aij
] xizj
]
(65a)
=
I∑
i=1
I∑
j=1
log
[
Axizj
ij
]
(65b)
=
I∑
i=1
I∑
j=1
xizj log Aij (65c)
Now, we are ready to start derivations for the transition mix ture node. The
transition mixture node has the node function
44
f(x, y, z, A) =
∏
k
Cat
(
x |Akz
) yk
(66a)
=
∏
k
[ ∏
i
∏
j
[
Ak
] zixj
ij
] yk
(66b)
=
∏
i,j,k
Azixj yk
ijk (66c)
Where we use i to index over columns, j to index over rows and k to index
over factors. A is a 3-tensor that is normalised over columns. In other words ,
each slice of A corresponds to a valid transition matrix and slices are inde xed
by k. W e assume a structured mean-ﬁeld factorisation such that
q(x, y, z, A) =q(x, y, z)
∏
k
q(Ak) (67)
The CFFG of the transition mixture node is shown in Fig. 31
TM
z
x
y
A1
Ak
Figure 31: The transition mixture node
W e now deﬁne
log fA(x, y, z) =Eq(A)
[
log f(x, y, z, A)
]
(68a)
= Eq(A)
[ ∑
ijk
zixj yk log Aijk
]
(68b)
=
∑
k
ykEq(Ak)
[ ∑
ij
zixj log Aijk
]
(68c)
=
∑
k
yk
∑
ij
zixj
[
log Ak
]
ij
(68d)
45
which implies that
fA(x, y, z) = exp
(∑
k
yk
∑
ijk
zixj
[
log Ak
]
ij
)
(69a)
=
∏
ijk
exp
(
log Ak
) zixj yk
(69b)
=
∑
ijk
zixj yk exp
(
log Ak
)
ij  
˜Aijk
(69c)
Now, we are ready to derive the desired messages. The ﬁrst mes sage ν(·)
will be towards x. If we use µ(y) to denote the incoming message on the edge
y (similar for µz), then the message towards x is given by
ν(x) =
∑
z
∑
y
µ(z)µ(y) exp
(
Eq(A) log f(x, y, z, A)
)
(70a)
=
∑
z
∑
y
µ(z)µ(y)fA(x, y, z)
)
(70b)
=
∑
z
∑
y
µ(z)µ(y)
∑
ijk
zixj yk ˜Aijk (70c)
Assuming the incoming messages are Categorically distribu ted, we can con-
tinue as
= ECat(z|π z )ECat(y|π y )
∑
ijk
zixj yk ˜Aijk (71a)
=
∑
ijk
πziπyk xj ˜Aijk (71b)
=
∏
j
[ ∑
ik
πziπyk ˜Aijk
] xj
(71c)
where the last line follows from x being one-hot encoded. At this point, we
can recognise the message
ν(x) ∝C at(x |ρ) where ρj =
∑
ik πziπyk ˜Aijk
∑
ijk πziπyk ˜Aijk
(72)
By symmetry , similar results hold for messages ν(z) and ν(y). ν(z) is given
by
46
ν(z) =
∑
x
∑
y
µ(x)µ(y)fA(x, y, z) (73a)
∝C at(z |ρ) where ρi =
∑
jk πxj πyk ˜Aijk
∑
ijk πxj πyk ˜Aijk
. (73b)
Similarly , ν(y) evaluates to
ν(y) =
∑
x
∑
z
µ(x)µ(z)fA(x, y, z) (74a)
∝C at(y |ρ) where ρk =
∑
ij πx,j πz,i ˜Aijk
∑
ijk πx,j πz,i ˜Aijk
. (74b)
T o compute the message towards the n’th candidate transition matrix An,
we will need to take an expectation with respect to q(x, y, z). It is given by
q(x, y, z) =µ(y)µ(x)µ(z)fA(x, y, z) (75a)
=
∏
ijk
πxj
xj πzi
zi πyk
yk
˜A
xj ziyk
ijk (75b)
=
∏
ijk
[
πxj πziπyk ˜Aijk
] xj ziyk
(75c)
∝C at(x, y, z |B) (75d)
where B is a three-dimensional contingency tensor with entries
Bijk = πxj πziπyk ˜Aijk
∑
ijk πxj πziπyk ˜Aijk
(76)
Now we can compute the message towards a transition matrix An as
log ν(An) =Eq(Ak\ n)q(x, y, z)
[
log f(x, y, z, A)
]
(77a)
= Eq(Ak\ n)q(x, y, z)
[ ∑
ijk
xj ziyk log Aijk
]
(77b)
= Eq(Ak\ n)
[ ∑
ijk
Bijk log Aijk
]
(77c)
=
∑
k\ n
[ ∑
ijk
Bijk log Aijk
]
  
Constant w.r.t An
+
∑
ij
Bijn log Aijn (77d)
∝ tr
(
Bijn log Aijn
)
(77e)
47
which implies that
ν(An) ∝D ir(An |Bn + 1). (78)
W e see that messages towards each component in A are distributed according
to a Dirichlet distribution with parameters Bn +1. The ﬁnal expression we need
to derive is the average energy term for the transition mixtu re node. It is given
by
Ux[q] =−Eq(A)q(x, y, z)
(
log f(x, y, z, A)
)
(79a)
= −Eq(x, y, z)
(
fA(x, y, z)
)
(79b)
= −Eq(x, y, z)
[ ∑
ijk
xj ziyk
[
log Ak
]
ij
)
(79c)
= −
∑
ijk
Bijk
[
log Ak
]
ij (79d)
= −
∑
k
tr
(
BT
k
log Ak
)
(79e)
48