Active Inference AI Systems for Scientific Discovery
Karthik Duraisamy
Michigan Institute for Computational Discovery & Engineering,
University of Michigan, Ann Arbor.
Abstract
The rapid evolution of artificial intelligence has led to expectations of transformative im-
pact on science, yet current systems remain fundamentally limited in enabling genuine scien-
tific discovery. This perspective contends that progress turns on closing three mutually rein-
forcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these
gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis gen-
eration—exploring counterfactual spaces where physical laws can be temporarily violated to
discover new patterns—and reasoning as fast, deterministic validation, traversing established
knowledge graphs to test consistency with known principles. Abstractions in this loop should
bemanipulablemodelsthatenablecounterfactualprediction, causalattribution, andrefinement.
Design principles—rather than a monolithic recipe— are proposed for systems that reason in
imaginary spaces and learn from the world: causal, multimodal models for internal simulation;
persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established
claims; formal verification pathways coupled to computations and experiments. It is also ar-
gued that the inherent ambiguity in feedback from simulations and experiments, and underlying
uncertainties make human judgment indispensable, not as a temporary scaffold but as a per-
manent architectural component. Evaluations must assess the system’s ability to identify novel
phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward
genuine discoveries.
Overthepastdecade, theevolutionofAIfoundationmodelresearchhasfollowedaclearsequence
of discrete jumps in capability. The advent of the Transformer[62] marked a phase dominated by ar-
chitectural innovations, which was rapidly succeeded by scaling demonstrations such as GPT-2[55].
The maturation of large-language-model pre-training then gave way to the usability turn: chat-
oriented models fine-tuned for alignment and safety that enabled direct human interaction[47]. The
current frontier is characterised by reasoning-emulation systems that incorporate tool use, scratch-
pad planning, or program-synthesis objectives[45]. A fifth, still-incipient phase points toward au-
tonomous agents that can decompose tasks, invoke external software or laboratories, and learn from
the resulting feedback. Scientific applications of AI have echoed each of these transitions at a com-
pressed cadence. As examples, SchNet translated architectural advances to quantum chemistry[59];
AlphaFold leveraged domain knowledge infused scaling to solve protein-fold prediction[30]; Chem-
BERTa [14] and FourCastNet [48] adapted language and vision innovations to molecular and climate
domains; and AlphaGeometry applied reasoning-centric objectives to symbolic mathematics[61].
Collectively, recent works [24, 7, 9] chart a shift from single, specialized pre-trained model to work-
flow orchestration, suggesting that future breakthroughs may hinge on integrating heterogeneous,
domain-aware agents capable of planning experiments, steering simulations, and iteratively refining
hypotheses across scales.
This highlights a deeper challenge for scientific discovery, which must reason across stacked layers
of abstraction: the emergence of unexpected phenomena at higher scales, just as local atmospheric
equations do not directly predict large-scale El Niño patterns. To address this challenge, it may
1
arXiv:2506.21329v3  [cs.AI]  2 Aug 2025
be required to deliberately architect systems with built-in mechanisms for hierarchical inference,
equipping them with specialized components that can navigate between reductionist details and
emergent phenomena. A compelling counter-argument posits that such abstract reasoning is not
a feature to be explicitly engineered, but an emergent property that will arise from sufficient scale
and diverse data. Proponents of this view might point to tools such as AlphaGeometry [61], where
complex, formal reasoning appears to emerge from a foundation model trained on vast synthetic
data. However, we contend that while scaling can master any pattern present in the training distri-
bution—even highly complex ones—it is fundamentally limited to learning correlational structures.
Scientific discovery, in contrast, hinges on understanding interventional and counterfactual logic:
what happens when the system is deliberately perturbed?This knowledge cannot be passively ob-
served in static data; it must be actively acquired through interaction with the world or a reliable
causal model thereof. The ’reality gap’ thus remains a significant barrier that pure scaling may not
cross.
It is also pertinent to examine the nature of present-day scientific discovery before speculat-
ing the role of AI. Modern science has moved beyond the romanticized vision of solitary geniuses
grappling with nature’s mysteries. It may be difficult to generalize or even define the nature of
discovery, but it is safe to assume that many of today’s discoveries emerge from vast collaborations
parsing petabytes of data from instruments such as the Large Hadron Collider or from distributed
sensor networks or large-scale computations and most importantly, refining hypothesis in concur-
rence with experiments and simulations. In fields such as high-energy physics, the bottleneck has
shifted toward complexity management, whereas in data-constrained arenas such as fusion-plasma
diagnostics, insight scarcity remains dominant; any general framework must therefore account for
both regimes. Even if one possesses the raw data to answer profound questions, we often lack
the cognitive architecture to navigate the combinatorial explosion of hypotheses, interactions, and
emergent phenomena. This creates an opportunity for AI systems—to excel precisely where human
cognition fails, in maintaining consistency across very high-dimensional parameter spaces, identify-
ing and reasoning about subtle patterns in noisy data. At this juncture, it has to be emphasized
that generating novel hypotheses might be the easy part [25]: the challenge is in rapidly assessing
the impact of a hypothesis or action in an imaginary space. Thus AI systems have to be equipped
with rich world models that can rapidly explore vast hypothesis spaces, and integrated with efficient
computations and experiments to provide valuable feedback.
Against this backdrop, this perspective piece is organized around three interlocking hurdles
(i) the abstraction gap, which separates low-level statistical regularities from the mechanistic con-
cepts on which scientists actually reason; (ii) the reasoning gap, which limits today’s models to
correlation-driven pattern completion rather than causal, counterfactual inference; and (iii) the re-
ality gap, which isolates computation from the empirical feedback loops that ultimately arbitrate
truth. The central thesis is that scientific discovery demands a reimagining of AI architectures. Fu-
ture AI systems must integrate active inference principles, maintaining persistent scientific memories
while engaging in closed-loop interaction with both simulated and physical worlds. The following
sections examine each gap in detail before presenting an integrated architecture that addresses these
challenges holistically.
1 Fundamental Gaps in AI Models
The path from current AI capabilities to genuine scientific discovery is obstructed by interconnected
barriers that reflect deep architectural limitations rather than scaling or engineering challenges.
These gaps are not independent failures but symptoms of a unified problem: current AI systems
lack the cognitive architecture necessary for scientific thinking. Understanding these gaps requires
recognizing that they form a mutually reinforcing system of constraints: without rich abstractions
2
there is little substrate for reasoning, and without tight coupling to reality even the most elegant
abstractions may drift toward irrelevance.
1.1 The Abstraction Gap
While early models largely manipulated tokens and pixels, recent advances in concept-bottleneck
networks[35], symmetry-equivariant graph models[60], and neuro-symbolic hybrids[40] show prelim-
inary evidence that contemporary AI can already represent and reason over higher-order scientific
concepts and principles. Yet a physicist reasons in conservation laws and symmetry breaking,
whereas language models still operate on surface statistics. Closing this abstraction gap requires
addressing several intertwined weaknesses.
Inset 1: Thinking and reasoning
Understanding how AI systems can enable scientific discovery requires examining the cogni-
tive processes that these systems attempt to emulate. In this context, a critical distinction
emerges between thinking and reasoning: Thinking can be operationalized as an iterative, ex-
ploratory process—searching for partial solutions in the form of patterns without guaranteed
convergence. It is the slow, generative phase where new connections form and novel patterns
emerge from a number of possibilities. Reasoning, by contrast, represents the fast, determinis-
tic traversal of established knowledge structures—building the most expressive path through
a graph of already-discovered patterns. This dichotomy [29] may explain why current AI
systems excel at certain tasks while failing at others. Large language models can reason im-
pressively when the requisite patterns exist in their training data—they rapidly traverse their
learned knowledge graphs to construct seemingly intelligent responses. Yet they struggle with
genuine thinking: the patient, iterative discovery of patterns that do not yet exist in their
representational space. Scientific discovery demands both capabilities in careful balance.
Thinking generates hypotheses by discovering new patterns through mental simulation and
exploration; reasoning then rapidly tests these patterns against existing knowledge and em-
pirical constraints. The purpose of thinking therefore is not to solve problems directly but
to expand the pattern vocabulary available for subsequent reasoning. Each thinking cy-
cle potentially adds new nodes and edges to the knowledge graph, creating shortcuts and
abstractions that make previously intractable reasoning paths suddenly accessible. This is
perhaps why breakthrough discoveries often seem obvious in retrospect—the thinking phase
has restructured the problem space so thoroughly that the reasoning path becomes trivial.
It is also possible to formalise the intuitive split betweenslower counterfactual thinkingand
faster deductive reasoning by mapping them onto the well-studied System-1/System-2 di-
chotomy in cognitive science [31, 21]. Recent neuro-symbolic RL agents already hint at this
synergy: the survey of Acharya et al.[1] chronicles agents that fuse neural perception (System
1) with first-order symbolic planners (System 2) while Mao et al.[40] demonstrate composi-
tional question-answering by training a neural concept learner that hands off logic programs
to a symbolic executor.
Modern transformer variants assemble chain-of-thought proofs[65] by replaying patterns ob-
served during pre-training; they do not build explicit causal graphs or exploit formal logic engines
except in narrow plug-in pipelines. As a result they fail at problems that demand deep composi-
tionality. Several other shortcomings have also been pointed out [42].
The gap between correlation and causation represents perhaps the most fundamental challenge
in automated scientific discovery. While current models excel at finding statistical regularities,
scientific understanding requires the ability to reason about interventions—to ask not just “what
3
correlates with what?" but “what happens when we change this?". Pearl’s causal hierarchy [49]
distinguishes three levels of cognitive ability: association (seeing), intervention (doing), and coun-
terfactuals (imagining). Current AI systems operate primarily at the associative level, occasionally
reaching intervention through experimental design. True scientific reasoning requires all three,
particularly the counterfactual ability to imagine alternative scenarios that violate observed corre-
lations. This connects directly to ethologist Konrad Lorenz’s insight –first tied to learning systems
by Scholkopf [58]– that thinking is fundamentally about acting in imaginary spaces where we can
violate the constraints of observed data. Thismental experimentation—impossible in physical re-
ality but accessible in the imagination—forms the basis of scientific law formation, as explained in
Inset 1.
Critically, this mental experimentation must integrate diverse modalities of scientific data and
representation. Scientific phenomena manifest as continuous vector/tensor fields over space-time
(e.g., velocity-pressure fields, concentration gradients, electromagnetic fields) interleaved with dis-
crete events (chemical reactions, phase transitions) and symbolic constructs (reaction mechanisms,
theoretical frameworks). Effective AI systems for discovery must therefore maintain multi-modal
embeddings that seamlessly translate between these representational levels—from raw sensor data
to mathematical abstractions to causal hypotheses—enabling the system to reason simultaneously
across observational patterns, physical mechanisms, and theoretical principles.
Recent empirical work by Buehler [11, 10] demonstrates that graph-based knowledge represen-
tations can bridge the abstraction gap. Specifically, recursive graph expansion experiments show
that autonomous systems naturally develop hierarchical, scale-free networks mirroring human sci-
entific knowledge structures. Without predefined ontologies, these systems spontaneously form
conceptual hubs and persistent bridge nodes, maintaining both local coherence and global integra-
tion—addressing precisely the limitations that prevent current AI from connecting low-level patterns
to high-level scientific concepts. Indeed, success in one class of problems does not guarantee trans-
lation to other problems, domains and disciplines, but these works show that with appropriate
graph-based representations, AI systems can discover novel conceptual relationships.
1.2 The Reasoning Gap
Future systems must balance the complementary modes of thinking and reasoning as first-class ar-
chitectural principles. Thinking—or slow, iterative discovery of new patterns—demands (i) world-
model agents that can explore counterfactual spaces through mental simulation [26]; (ii) curiosity-
driven mechanisms that reward pattern novelty over immediate task performance; and (iii) patience
parameters that prevent premature convergence. Reasoning—the fast, deterministic traversal of
pattern graphs—demands (i) efficient knowledge graph architectures with learned traversal poli-
cies; (ii) neuro-symbolic stacks that maintain both continuous representations and discrete logical
structures[40]; and (iii) caching mechanisms that transform expensive thinking outcomes into rapid
reasoning primitives. The interplay between these modes mirrors how scientists alternate between
exploratory experimentation (thinking) and theoretical derivation (reasoning) as referenced in Inset
1.
Thenotionthat“ thinking is acting in an imaginary space"—asKonradLorenzobserved—provides
a foundational principle for understanding how world models enable scientific discovery. Just as bi-
ological organisms evolved the capacity to simulate actions internally before committing physical
resources, AI systems with rich world models can explore vast hypothesis spaces through mental
simulation. This capability transcends mere pattern matching: it enables counterfactual reasoning,
experimental design optimization, and the anticipation of empirical surprises before they manifest
in costly real-world experiments. World models can serve as the substrate for this imaginary action
space, encoding not just correlations but causal structures that permit intervention and manipula-
4
tion. The fidelity of these mental simulations—their alignment with physical reality—determines
whether the system’s thoughts translate into valid discoveries.
Scientific progress thrives on disciplined risk: venturing beyond received wisdom while remaining
falsifiable. Current alignment protocols deliberately dampen exploratory behaviour, biasing models
toward safe completion of well-trodden trajectories. Controlled speculation frameworks—for ex-
ample, curiosity-driven reinforcement learning [46] combined with Bayesian epistemic uncertainty
quantification—could allow systems to seek novel hypotheses, flag them with calibrated uncertainty,
and propose targeted experiments for arbitration. Mechanisms such as self-consistency voting [64],
adversarial peer review, and tool-augmented chain-of-thought audits offer additional scaffolding to
keep high-variance reasoning connected to empirical reality.
A key aspect of closing the abstraction and reasoning gaps, therefore, is in developing architec-
tures that can construct and manipulate explicit symbolic representations as dynamic objects rather
than static patterns—essentially giving models the ability to retain abstract concepts like conser-
vation laws or causal structures in working memory and actively transform them through mental
experimentation [37]. This requires moving beyond current approaches that merely associate pat-
terns to instead build compositional graph-based [5] reasoning systems where abstract principles
can be instantiated, violated, and reconstructed in imaginary spaces. Future systems can thus tar-
get manipulable conceptual building blocks that can be assembled into novel configurations that
have not been explicitly seen in training—enabling the kind of counterfactual reasoning that can
ultimately bridge the gap between correlation and causation that defines scientific thinking.
1.3 The Reality Gap
While the abstraction and reasoning gaps constrain what AI systems can represent and manipulate
internally, the reality gap addresses a more fundamental limitation: the disconnect between com-
putational models and the real world they aim to describe. As detailed in Inset 2, the necessity of
empirical feedback for scientific discovery emerges from theoretical constraints—Gödel’s incomplete-
ness theorems and Wolfram’s computational irreducibility guarantee that no purely computational
system can discover all truths about nature. Scientific progress depends not on escaping these con-
straints but on navigating them through continuous dialogue with reality. In practice, this dialogue
is inherently multi-modal [18] and spatio-temporal [48, 27]. AI systems must maintain a physically
grounded latent state, with dynamics constrained by known operators and invariances (conservation
laws; symmetries such as Galilean/rotational). This structured, multi-modal view reduces sample
complexity and yields representations that extrapolate across space, time, and interventions.
Empirical feedback complements formal reasoning by supplying information inaccessible to
purelydeductivesystems, therebyexpanding—ratherthanmechanicallyescaping—thesetoftestable
scientific propositions. The interplay between formal systems and empirical validation creates a
bootstrap mechanism that circumvents incompleteness and irreducibility constraints. This suggests
that AI systems for discovery must be fundamentally open—not just to new data, but to surprise
from reality itself. Scientific history abounds with internally coherent theories that later failed em-
pirical tests, underscoring the indispensability of continuous validation against data. Current AI
systems excel at interpolation within their training distributions but struggle with the extrapolation
that defines discovery. This is exacerbated by the fact that many scientific domains are character-
ized by sparse, expensive data and imperfect simulators. Unlike language modeling where data is
abundant, a single protein crystallography experiment might take months and cost thousands of
dollars. Simulations help but introduce their own biases, further contributing to the reality gap.
The synthesis presented in Inset 2 directly informs our architecture. Thinking explores for
new pockets and tests boundaries; reasoning exploits discovered regularities. World models encode
provisional maps of known pockets, subject to Popper’s falsification and Kuhn’s paradigm shifts.
5
Human steering proves essential. Humans provide non-computational insight for recognizing gen-
uine understanding, value judgments for directing exploration, and navigation through paradigm
shifts where evaluation criteria themselves transform. Humans can shape the search process by en-
coding domain knowledge, identifying significant anomalies, and recognizing connections that form
larger frameworks. When Faraday discovered electromagnetic induction, he did not deduce it from
Maxwell’s equations (which did not yet exist)—he found it through experiment. Thus, productive
collaborations can implement the complete scientific method: AI generates and tests hypotheses at
scale; humans provide insight and judgment and empirical feedback provides critical steering. Our
architecture must therefore implement a hybrid loop: physics priors guide ML surrogates, which
direct active experiments, which update our understanding in continuous iteration.
Inset 2: Necessity of Empirical Feedback for Scientific Discovery
The quest for scientific discovery via computation confronts a fundamental paradox.
Gödel [23] proved that formal systems are incomplete, and cannot self-consistently prove
all truths contained within the system, while Wolfram [66] demonstrates that computational
irreducibility pervades nature. Additionally, Penrose [50, 51] contends that human insight
transcends algorithms. Yet scientific theories and computations are found to be highly ef-
fective in many cases. Insight can be gained from Wolfram’s recent comment [67]:The very
presence of computational irreducibility necessarily implies that there must be pockets of com-
putational reducibility, where at least certain things are regular and predictable. It is within
these pockets of reducibility that science fundamentally lives.
In most cases, these pockets cannot be deduced a priori—they require empirical discovery.
This connects to Popper’s [52] falsificationism: we cannot prove we have found true reducibil-
ity, but we can discover boundaries through experiments that challenge assumptions. Empir-
ical feedback escapes Gödel’s constraints while delineating where nature permits shortcuts.
Kuhn’s analysis [36] adds temporal dynamics: Science alternates between prevailing theories
within established pockets and paradigm shifts that restructure understanding. AI systems
must balance exploiting known regularities with flexibility to reconceptualize when evidence
demands. In other words, even if equipped with highly effective abstractions and reasoning
mechanisms, AI systems may not be able to reason their way to scientific truth through pure
computation; they must actively probe reality through experiments and simulations, using
empirical surprises to update their world models.
Physics priors While generative models can create visually compelling outputs, they lack physi-
cal consistency—objects appear and disappear, gravity works intermittently, and causality is merely
suggested rather than enforced. Mitchell [43] states that without biases to prefer some generaliza-
tions over others, a learning system cannot make the inductive leap necessary to classify instances
beyond those it has already seen. Such inductive biases or physics priors—can be built-in to ensure
generated realizations obey conservation laws, maintain object permanence, and support counter-
factual reasoning about physical interactions.
Recent implementations demonstrate that world models can also discover physical laws through
interaction. The joint embedding predictive architecture [3, 4] learns to predict object movements
without labeled data, suggesting that the feedback loop between mental simulation and empirical
observation can be implemented through self-supervised learning objectives that reward accurate
forward prediction. Current world models and coceptualizations thereof, however, remain limited
to relatively simple physical scenarios. While they excel at rigid body dynamics and basic occlusion
6
reasoning, they are generally insufficient to describe complex phenomena like fluid dynamics or
emergent collective behaviors. This gap between toy demonstrations and the full complexity of
scientific phenomena represents the next frontier.
Causal models The current paradigm of domain-specific foundation models—from protein lan-
guage models to molecular transformers—represents significant progress in encoding domain knowl-
edge. However, these models fundamentally learn correlational patterns rather than causal mech-
anisms. ChemBERTa [14] can predict molecular properties through pattern matching but cannot
simulate how modifying a functional group alters reaction pathways. AlphaFold [30] predicts protein
structures through evolutionary patterns but does not model the physical folding process.
Scientific discovery requires models that transcend pattern recognition to capture causal dynam-
ics. A causal molecular model would not just recognize that certain molecular structures correlate
with properties—it would explain how electron density distributions cause reactivity, and how ther-
modynamic gradients drive reactions. This causal understanding enables the counterfactual rea-
soning essential to science: predicting outcomes of novel interventions never seen in training data.
This architectural choice has profound implications: foundation models scale with data and com-
pute, but causal models scale with understanding. With accumulaion of structural data, AI models
can improve at interpolation. As we refine causal mechanisms, foundation models can improve at
extrapolation—the essence of scientific discovery.
2 Additional Improvements
A certain level of consensus appears to be forming in the community that incremental scaling
of present architectures may not deliver the qualitative leap that scientific discovery demands.
Progress hinges on removing the design constraints through concurrent advances in algorithms
and architectures (such as those described above) but also by improving efficiencies via hardware-
software co-design and better evaluation benchmarks.
Computational Efficiency Scaling laws show that models get predictably better with more
data, parameter count and test time compute, yet every small gain might come at a great expense
in time and/or energy. Such brute-force optimization contrasts sharply with biological economies
in which sparse, event-based spikes[20] and structural plasticity[32] deliver continual learning at
milliwatt scales. Bridging the gap will demand both algorithmic frugality—latent-variable models,
active-learning curricula, reversible training—and hardware co-design. State-of-the-art foundation
models require months of GPU time and> 1025 FLOPs to reach acceptable performance on long-
horizon benchmarks. Memory-reversible Transformers [39, 70] and curriculum training [63] have
recently reduced end-to-endtraining costs by 30–45%, without loss of final accuracy. Similar level
of cost reductions have been reported [16] leveraging energy and power draw scheduling.
The von Neumann bottleneck—shuttling tensors between distant memory and compute—now
dominates energy budgets [41]. Processing-in-memory fabrics [34], spiking neuromorphic cores that
exploit event sparsity, analog photonic accelerators for low-latency matrix products, quantum sam-
plers for combinatorial sub-routines [2] could open new algorithmic spaces. Realising their potential
outside of niche applications, however, will require co-design of hardware, software and algorithms
and extensive community effort.
Evaluations Current leaderboards—e.g. MathBench[38], ARC[15], GSM8K[17]—scarcely probe
the generative and self-corrective behaviours central to science. A rigorous suite should test whether
a model can (i) identify when empirical data violate its latent assumptions, (ii) propose falsifiable
hypotheses with quantified uncertainty, and (iii) adapt its internal representation after a failed
7
prediction. Concretely, this may involve closed-loop benchmarks[33] in which the system picks ex-
periments from a simulated materials lab, updates a dynamical model, and is scored on discovery
efficiency; or theorem-proving arenas where credit is given only for proofs accompanied by inter-
pretable lemmas. Without such stress-tests, superficial gains risk being mistaken for conceptual
breakthroughs. Future evaluations can also assess the human-AI-reality-discovery feedback loop
itself. Early exemplars such as DiscoveryWorld [28], PARTNR [12] and SciHorizon [54] represent
steps towards this direction.
3 Architecture of Active Inference AI Systems
The fundamental gaps analyzed in the preceding sections are not independent failures but symptoms
of a deeper architectural mismatch between current AI systems and the requirements of scientific
discovery. These insights, combined with the inherently multi-scale and multi-modal nature of
scientific phenomena—from molecular interactions to emergent spatio-temporal dynamics—dictate
specific architectural requirements. A system capable of genuine discovery must therefore integrate:
internal models that support mental experimentation, knowledge structures that grow through
thinking-reasoning cycles, and verification mechanisms that ground speculation in empirical real-
ity. No monolithic approach can address these diverse demands; instead, we propose a modular
active inference architecture where specialized components work in concert. Figure 1 illustrates this
architecture, and the key components include:
1. Base reasoning model suite with inference-tunable capabilities:This top-layer component com-
prises large reasoning models that can dynamically adjust their inference strategies based on
the problem context. In contrast to being optimized for next-token prediction, these models
support extended thinking times, systematic exploration of solution paths, and explicit rea-
soning chains. The suite has the ability to recognize which mode of reasoning is appropriate.
Value specifications from humans guide the reasoning process, ensuring that resources are
allocated to scientifically meaningful directions rather than arbitrary pattern completion.
2. Multi-modal domain foundation models with shared representations:Theseareeffectivelyworld
models that maintain causal representations of scientific domains. These models allow the
system to mentally simulate interventions, test counterfactuals, and explore hypothesis spaces
before committing to physical experiments. These function as oracles or world models, serving
as the substrate for both pattern discovery (thinking) and rapid inference (reasoning). These
domain-specific models must share embeddings that enable cross-pollination of insights.
3. Dynamic knowledge graphs as evolving scientific memory:Unlike static knowledge bases, these
graphs function as cognitive architectures that grow through the interplay of thinking, reason-
ing, and experimentation. Nodes represent concepts ranging from raw observations to abstract
principles, while weighted edges encode causal relationships with associated uncertainty. The
graphs expand as thinking discovers new patterns (adding nodes), reasoning establishes log-
ical connections (adding edges), and experiments validate or falsify relationships (adjusting
weights). Version-controlled evolution allows the system to maintain competing hypotheses,
track conceptual development, and recognize when anomalies demand fundamental restruc-
turing rather than incremental updates. This persistent, growing memory enables genuine
scientific progress rather than mere information retrieval.
4. Reality tethering through verification layers:The verification layer partitions scientific claims
into formally provable statements and empirically testable hypotheses. Mathematical deriva-
tions, algorithmic properties, and logical arguments can be decomposed into proof obligations
8
Figure 1: Exemplar architecture of an Active Inference AI system for scientific discovery.
for interactive theorem provers (Lean [44], Coq [6]), creating a growing corpus of machine-
verified knowledge that future reasoning can build upon. For claims beyond formal correct-
ness—predictions about physical phenomena, chemical reactions, or biological behaviors—the
system generates targeted computational simulations and experimental protocols. This dual
approach acknowledges that scientific knowledge spans from mathematical certainty to empir-
ical contingency. Crucially, failed verifications become learning opportunities, updating the
system’s confidence bounds and identifying gaps between its world model and reality.
5. Human-steerable orchestration: Humans excel at recognizing meaningful patterns and mak-
ing creative leaps; AI can perform exhaustive search and maintaining consistency across vast
knowledge spaces; Well-understood computational science tools (e.g. optimal experimental
design) can execute efficient agentic actions in a reliable manner. This symbiotic relation-
ship ensures that the system’s powerful reasoning capabilities remain tethered to meaningful
scientific questions, and existing algorithms are efficiently leveraged.
6. Proactive exploration engines:Rather than passively responding to queries (the primary mode
in which language models are used currently), these systems work persistently in the back-
grond to generate hypotheses, identify gaps in knowledge, and propose experiments. Driven
by uncertainty quantification and novelty detection algorithms, these engines can maintain a
priority queue of open questions ranked by their potential to achieve specified goals versus
resource requirements. This layer enables the system to operate across multiple time hori-
zons—pursuing rapid experiments vs long-term research campaigns that systematically map
uncharted territories in the knowledge space.
The architectural principles outlined above find grounding in recent work on transformational
scientific creativity. For instance, Schapiro et al. [56] formalize scientific conceptual spaces as di-
rected acyclic graphs, where vertices represent generative rules and edges capture logical dependen-
cies. This offers a concrete implementation pathway for the proposed dynamic knowledge graphs.
Their distinction between modifying existing constraints versus fundamentally restructuring the
space itself maps directly onto our architecture’s dual modes of reasoning (traversing established
9
knowledge) and thinking (discovering new patterns that may violate existing assumptions). This
convergence suggests that achieving transformational scientific discovery through AI systems re-
quires systems capable of identifying and modifying the foundational axioms that constrain current
scientific understanding—a capability the active inference framework aims to provide through its
stacked architecture and integration of models, empirical feedback, and human guidance.
ItisacknowledgedthatwhiletheAIsystemcan–inprinciple–beoperatedautonomouslythrough
well-definedinterfacesbetweencomponents, humaninteractionanddecisionscanbeexpectedtoplay
a key role. The architectural principles outlined above find partial instantiation in contemporary
systems, thoughnonefullyrealizethecompletevisionofscientificintelligence. AppendixAexamines
some current implementations through the lens of our three-gap framework, and discusses both
substantial progress and persistent limitations that illuminate the path forward.
4 Limitations
While the aforementioned architecture presents a compelling vision of AI systems that learn from
real-world interaction, incorporating feedback into iterative training poses fundamental challenges
that cannot be overlooked. Scientific experiments produce sparse, noisy, and often contradictory
signals. A single failed synthesis might stem from equipment miscalibration, modeling errors, or
genuine chemical impossibility—yet the system must learn appropriately from each case. The
tensionbetweengeneralizationandspecificitybecomesacute: overfittingtoparticularconfigurations
may yield brittle models that fail to transfer across laboratories, while excessive generalization may
miss critical context-dependent phenomena.
This inherent ambiguity in processing experimental feedback into actionable model refinements
makes human judgment indispensable, not as a temporary scaffold but as a permanent architec-
tural component. Thus, the challenge lies not merely in designing systems that can incorporate
feedback, but in creating architectures that handle the full spectrum of empirical reality, including
clear confirmations, ambiguous results, systematic biases and truly novel results. Effective human-
AI collaboration must therefore go beyond simple oversight. This partnership becomes especially
critical when experiments and computations challenge fundamental assumptions.
Finally, it has to be emphasized that modern AI systems are already useful in their present
form, and are being utilized effectively by scientific research groups across the world. However,
even with future improvements, these tools bring many systemic hazards [42]:a) false positives
and false negatives:spurious correlations can be mistaken for laws, while cautious priors may hide
real effects, and thus rigorous uncertainty metrics and adversarial falsification must be built in;b)
epistemic overconfidence:large models could shrink error bars off-distribution, demanding ensemble
disagreement; c) erosion of insight and rigor: over time, there is signficiant risk of researchers
losing key scientific skills;d) Cost: simulation-driven exploration can consume resources long after
marginal information saturates, so schedulers must weigh value against resources;e) concept drift:
equipments and sensors evolve, and thus without continual residual checks and rapid retraining,
predictions may silently bias. These issues have to be continually acknowledged, recognizing and
safeguards should be embeded into the scientific process.
5 Summary and Outlook
This work has argued that the path to genuine scientific discovery through AI requires more than
improvements in data, compute and scaling—it demands a reimagining of the underlying architec-
ture. Current systems lack (i) abstractions that support mechanism-level reasoning, (ii) reasoning
that operates counterfactually rather than correlationally, and (iii) reality coupling– particularly
across multimodal, spatio-temporal measurements— that continuously calibrates beliefs to exper-
iments and high-fidelity simulations. Although fully resolving these challenges is a long-horizon
research program, immediate progress is possible by adopting an active-inference stack that (a)
10
learns causal, multi-modal world models for internal simulation; (b) maintains persistent, versioned
knowledge graphs with uncertainty; (c) routes claims either to formal proof engines or to empirical
verification; and (d) operates under human steering where ambiguity and value trade-offs dominate.
The aim is a shift from pattern completion to principle discovery.
This perspective calls upon the community to build on substantial progress in causal machine
learning, active learning, and automated scientific discovery to address these critical gaps. The
causal machine learning community has made significant strides in developing methods for causal
inference from observational data, with frameworks such as Pearl’s causal hierarchy and recent
advances in causal representation learning providing mathematical foundations for understanding
interventions and counterfactuals. Similarly, active learning has evolved sophisticated strategies for
optimal experimental design, while automated discovery systems have demonstrated success in spe-
cific domains such as materials science and drug discovery. However, these communities have largely
operated in isolation, with causal methods focusing primarily on statistical inference rather than
physical mechanism discovery, active learning optimizing for narrow uncertainty reduction rather
than conceptual breakthroughs, and automated discovery systems excelling at interpolation within
known spaces rather than extrapolation to genuinely novel phenomena. Realizing such a capability
requires a diverse consortium to co-develop models, agentic infrastructure, formal verification stacks,
simulators, robotic labs, and evaluations. Success cannot merely be measured by benchmarks alone,
but by the moment when these systems become truly useful –as judged by domain experts– and
make genuinely novel scientific discoveries.
Acknowledgment
This piece has benefitted directly or indirectly from many discussions with Jason Pruet (OpenAI),
Venkat Raman, Venkat Viswanathan, Alex Gorodetsky (U. Michigan), Rick Stevens (Argonne Na-
tional Laboratory/U. Chicago), Earl Lawrence (Los Alamos National Laboratory) and Brian Spears
(Lawrence Livermore National Laboratory). This work was partly supported by Los Alamos Na-
tional Laboratory under the grant #AWD026741 at the University of Michigan.
References
[1] Kamal Acharya, Waleed Raza, Carlos Dourado, Alvaro Velasquez, and Houbing Herbert Song. Neu-
rosymbolic reinforcement learning and planning: A survey.IEEE Transactions on Artificial Intelligence,
5(5), 2023.
[2] Frank Arute, Kunal Arya, Ryan Babbush, et al. Quantum supremacy using a programmable supercon-
ducting processor. Nature, 574(7779), 2019.
[3] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat,
Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive
architecture. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 15619–15629, 2023.
[4] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar
Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models
enable understanding, prediction and planning.arXiv preprint arXiv:2506.09985, 2025.
[5] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational
inductive biases, deep learning, and graph networks.arXiv preprint arXiv:1806.01261, 2018.
[6] Yves Bertot and Pierre Castéran.Interactive theorem proving and program development: Coq’Art: the
calculus of inductive constructions. Springer Science & Business Media, 2013.
[7] Celeste Biever. Ai scientist ‘team’joins the search for extraterrestrial life.Nature, 641(8063):568–569,
2025.
11
[8] Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with
large language models.Nature, 624(7992):570–578, 2023.
[9] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller.
Augmenting large language models with chemistry tools.Nature Machine Intelligence, pages 1–11, 2024.
[10] Markus J Buehler. Agentic deep graph reasoning yields self-organizing knowledge networks. arXiv
preprint arXiv:2502.13025, 2025.
[11] Markus J Buehler. In situ graph reasoning and knowledge expansion using graph-preflexor.Advanced
Intelligent Discovery, 2025.
[12] Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac,
Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, et al. Partnr: A benchmark
for planning and reasoning in embodied multi-agent tasks.arXiv preprint arXiv:2411.00081, 2024.
[13] Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, and Janosh Riebesell. Llamp: Large language
model made powerful for high-fidelity materials knowledge retrieval and distillation.arXiv preprint
arXiv:2401.17244, 2024.
[14] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: Large-scale self-supervised
pretraining for molecular property prediction.CoRR, 2020.
[15] François Chollet. On the measure of intelligence.arXiv preprint arXiv:1911.01547, 2019.
[16] Jae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury. Reducing
energy bloat in large model training. InProceedings of the ACM SIGOPS 30th Symposium on Operating
Systems Principles, pages 144–159, 2024.
[17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word
problems. arXiv preprint arXiv:2110.14168, 2021.
[18] Haotian Cui, Alejandro Tejada-Lapuerta, Maria Brbić, Julio Saez-Rodriguez, Simona Cristea, Hani
Goodarzi, Mohammad Lotfollahi, Fabian J Theis, and Bo Wang. Towards multimodal foundation
models in molecular cell biology.Nature, 640(8059):623–633, 2025.
[19] Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic,
Yang Cao, Han Hao, Haoping Xu, Alan Aspuru-Guzik, et al. Organa: A robotic assistant for automated
chemistry experimentation and characterization.arXiv preprint arXiv:2401.06949, 2024.
[20] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday,
Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore
processor with on-chip learning.Ieee Micro, 38(1):82–99, 2018.
[21] Jonathan St B T Evans and Keith E Stanovich. Dual-process theories of higher cognition.Perspectives
on Psychological Science, 8(3):223–241, 2013.
[22] Alireza Ghafarollahi and Markus J Buehler. Protagents: protein discovery via large language model
multi-agent collaborations combining physics and machine learning.Digital Discovery, 2024.
[23] Kurt Gödel. Über formal unentscheidbare sätze der principia mathematica und verwandter systeme i.
Monatshefte für Mathematik und Physik, 38(1):173–198, 1931.
[24] Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, and Christina Mack. Agen-
tic ai for scientific discovery: A survey of progress, challenges, and future directions. International
Conference on Learning Representations, 2025.
[25] Xuemei Gu and Mario Krenn. Interesting scientific idea generation using knowledge graphs and llms:
Evaluations with 100 research group leaders.arXiv preprint arXiv:2405.17044, 2024.
[26] David Ha and Jürgen Schmidhuber. World models.arXiv preprint arXiv:1803.10122, 2018.
12
[27] Christian Jacobsen, Yilin Zhuang, and Karthik Duraisamy. Cocogen: Physically consistent and con-
ditioned score-based generative models for forward and inverse problems.SIAM Journal on Scientific
Computing, 47(2):C399–C425, 2025.
[28] Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bod-
hisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: A virtual environment
for developing and evaluating automated scientific discovery agents.Advances in Neural Information
Processing Systems, 37:10088–10116, 2024.
[29] Philip Nicholas Johnson-Laird.Mental models: Towards a cognitive science of language, inference, and
consciousness. Number 6. Harvard University Press, 1983.
[30] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein
structure prediction with alphafold.Nature, 596(7873):583–589, 2021.
[31] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011.
[32] Narayanan Kasthuri, Kenneth Jeffrey Hayworth, Daniel Raimund Berger, Richard Lee Schalek,
José Angel Conchello, Seymour Knowles-Barley, Dongil Lee, Amelio Vázquez-Reina, Verena Kaynig,
Thouis Raymond Jones, et al. Saturated reconstruction of a volume of neocortex.Cell, 162(3):648–661,
2015.
[33] Lance Kavalsky, Vinay I Hegde, Eric Muckley, Matthew S Johnson, Bryce Meredig, and Venkatasub-
ramanian Viswanathan. By how much can closed-loop frameworks accelerate computational materials
discovery? Digital Discovery, 2(4):1112–1125, 2023.
[34] Joo-Young Kim, Bongjin Kim, and Tony Tae-Hyoung Kim. Processing-in-memory for ai. 2022.
[35] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and
Percy Liang. Concept bottleneck models. In International conference on machine learning, pages
5338–5348. PMLR, 2020.
[36] Thomas S. Kuhn.The Structure of Scientific Revolutions. University of Chicago Press, 1962.
[37] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines
that learn and think like people.Behavioral and Brain Sciences, 40:e253, 2017.
[38] Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang,
Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the theory and application profi-
ciency of llms with a hierarchical mathematics benchmark.arXiv preprint arXiv:2405.12209, 2024.
[39] Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer,
and Jitendra Malik. Reversible vision transformers. InProceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2022.
[40] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-symbolic
concept learner: Interpreting scenes, words, and sentences from natural supervision. InInternational
Conference on Learning Representations, 2023.
[41] Kim Martineau. How the von neumann bottleneck is impeding ai computing, 2024. IBM Research Blog,
accessed 30 June 2025.
[42] Lisa Messeri and MJ Crockett. Artificial intelligence and illusions of understanding in scientific research.
Nature, 627(8002):49–58, 2024.
[43] Tom M Mitchell. The need for biases in learning generalizations.CS Tech Report CBM-TR-117, Rutgers
University, 1980.
[44] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language.
In Automated Deduction–CADE 28: 28th International Conference on Automated Deduction, Virtual
Event, July 12–15, 2021, Proceedings 28, pages 625–635. Springer, 2021.
13
[45] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratch-
pads for intermediate computation with language models. 2021.
[46] Pierre-Yves Oudeyer, Frederic Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-
tonomous mental development.IEEE Transactions on Evolutionary Computation, 11(2), 2007.
[47] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instruc-
tions with human feedback.arXiv preprint arXiv:2203.02155, 2022.
[48] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcast-
net: A global data-driven high-resolution weather model using adaptive fourier neural operators.arXiv
preprint arXiv:2202.11214, 2022.
[49] Judea Pearl. Causality. Cambridge university press, 2009.
[50] Roger Penrose. The Emperor’s New Mind: Concerning Computers, Minds, and the Laws of Physics.
Oxford University Press, 1989.
[51] Roger Penrose. Shadows of the Mind: A Search for the Missing Science of Consciousness. Oxford
University Press, 1994.
[52] Karl Popper. The Logic of Scientific Discovery. Hutchinson & Co., 1959.
[53] Michael H Prince, Henry Chan, Aikaterini Vriza, Tao Zhou, Varuni K Sastry, Yanqi Luo, Matthew T
Dearing, Ross J Harder, Rama K Vasudevan, and Mathew J Cherukara. Opportunities for retrieval and
tool augmented large language models in scientific facilities.npj Computational Materials, 10(1):251,
2024.
[54] Chuan Qin, Xin Chen, Chengrui Wang, Pengmin Wu, Xi Chen, Yihang Cheng, Jingyi Zhao, Meng
Xiao, Xiangchao Dong, Qingqing Long, et al. Scihorizon: Benchmarking ai-for-science readiness from
scientific data to large language models.arXiv preprint arXiv:2503.13503, 2025.
[55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners.OpenAI Blog, 1(8), 2019.
[56] Samuel Schapiro, Jonah Black, and Lav R Varshney. Transformational creativity in science: A graphical
theory. arXiv preprint arXiv:2504.18687, 2025.
[57] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng
Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants.arXiv preprint
arXiv:2501.04227, 2025.
[58] Bernhard Schölkopf. Causality for machine learning. InProbabilistic and causal inference: The works
of Judea Pearl, pages 765–804. 2022.
[59] Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-filter convolutional neural network for
modeling quantum interactions.Advances in neural information processing systems, 30, 2017.
[60] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley.
Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.arXiv
preprint arXiv:1802.08219, 2018.
[61] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without
human demonstrations.Nature, 625(7995):476–482, 2024.
[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information processing
systems, 2017.
14
[63] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio
Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for
efficient transformer training. InThe Eleventh International Conference on Learning Representations.
[64] Xuezhi Wang, Dale Wei, Yizhong Dong, Nan Bao, Michelle Yang, Denny Yu, Zijian Guo, Quoc V Le,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. InAdvances
in Neural Information Processing Systems, 2022.
[65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in neural
information processing systems, 35:24824–24837, 2022.
[66] Stephen Wolfram.A New Kind of Science. Wolfram Media, 2002.
[67] Stephen Wolfram. Can ai solve science?, March 2024.https://writings.stephenwolfram.com/2024/
03/can-ai-solve-science/ .
[68] Yihang Xiao, Jinyi Liu, Yan Zheng, Xiaohan Xie, Jianye Hao, Mingzhi Li, Ruitao Wang, Fei Ni, Yuxiao
Li, Jintian Luo, et al. Cellagent: An llm-driven multi-agent framework for automated single-cell data
analysis. bioRxiv, pages 2024–05, 2024.
[69] Qi Xin, Quyu Kong, Hongyi Ji, Yue Shen, Yuqi Liu, Yan Sun, Zhilin Zhang, Zhaorong Li, Xunlong
Xia, Bing Deng, et al. Bioinformatics agent (bia): Unleashing the power of large language models to
reshape bioinformatics workflow.bioRxiv, pages 2024–05, 2024.
[70] Guoqiang Zhang, JP Lewis, and W Bastiaan Kleijn. On exact bit-level reversible transformers without
changing architectures.arXiv preprint arXiv:2407.09093, 2024.
Appendix A: Current Implementations of Agentic Systems
A comprehensive review of agentic systems for scientific discovery can be found in Ref. [24]. Below,
a few references that are related to abstraction, reasoning and reality gaps are provided.
Recent systems demonstrate varying degrees of success in elevating from statistical patterns to
scientific abstractions. ChemCrow [9] integrates eighteen expert-designed tools to bridge token-
level operations with chemical reasoning, enabling tasks such as reaction prediction and molecular
property analysis. ProtAgents [22] employs reinforcement learning to navigate the conceptual space
of protein design, moving beyond sequence statistics to optimize for biochemical properties. Agent
Laboratory’s [57] achieves high success rates in data preparation and experimentation phases while
exhibiting notable failures during literature review.
The reasoning gap manifests most clearly in limited capacity for genuine causal inference. Co-
scientist [8] represents the current frontier, successfully designing and optimizing cross-coupling
reactions through iterative experimentation, though its reasoning remains fundamentally correla-
tional. LLaMP [13] attempts to address this limitation by grounding material property predictions
in atomistic simulations, effectively implementing a preliminary form of mental experimentation.
These systems, while promising, cannot yet perform the counterfactual reasoning that distinguishes
scientific understanding from mere pattern matching.
The reality gap presents both tangible progress and stark limitations. Systems such as Or-
gana [19] demonstrate sophisticated integration with laboratory robotics, automating complex ex-
perimental protocols in electrochemistry and materials characterization. CALMS [53] extends this
integration by providing context-aware assistance during experimental execution. However, these
implementations reveal brittleness: when experimental outcomes deviate from expected patterns,
current systems lack the adaptive capacity to reformulate hypotheses or recognize when their fun-
damental assumptions require revision.
Multi-agent architectures such as BioInformatics Agent [69] and CellAgent [68] represent at-
tempts to address these limitations through specialized collaboration, with distinct agents handling
15
data retrieval, analysis, and validation. While these systems demonstrate improved performance
on well-structured tasks, they do not yet perform the open-ended exploration that characterizes
genuine discovery. The coordination overhead and brittleness of inter-agent communication often
negate the benefits of specialization when confronting novel phenomena.
These implementations and others are already accelerating science, but also collectively reveal
a critical insight: current systems excel at automating well-defined scientific workflows but falter
when required to navigate the uncertain terrain of genuine discovery. They can execute sophisticated
experimental protocols, analyze complex datasets, and even generate plausible hypotheses, yet they
lack the metacognitive capabilities to recognize when they are operating beyond their training
domains.
16