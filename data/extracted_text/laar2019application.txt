Application of the Free Energy Principle to
Estimation and Control
Thijs van de Laar∗, Ay¸ca O ¨ z¸celikkale†, and Henk Wymeersch‡
∗Dept. of Electrical Engineering, Eindhoven University of
Technology, The Netherlands
†Dept. of Electrical Engineering, Uppsala University, Sweden
‡Dept. of Electrical Engineering, Chalmers University of
Technology, Sweden
October 21, 2020
Abstract
Basedonagenerativemodel(GM)andbeliefsoverhiddenstates,the
freeenergyprinciple(FEP)enablesanagenttosenseandactbyminimiz-
ing a free energy bound on Bayesian surprise. Inclusion of prior beliefs
intheGMaboutdesiredstatesleadstoactiveinference(ActInf). Inthis
work,weaimtorevealconnectionsbetweenActInfandstochasticoptimal
control. Werevealthat,incontrasttostandardcostandconstraint-based
solutions, ActInf gives rise to a minimization problem that includes both
aninformation-theoreticsurprisetermandamodel-predictivecontrolcost
term. We further show under which conditions both methodologies yield
thesamesolutionforestimationandcontrol. ForacasewithlinearGaus-
siandynamicsandaquadraticcost,weillustratetheperformanceofAct-
Inf under varying system parameters and compare to classical solutions
for estimation and control.
Index terms — Active Inference, Stochastic Optimal Control, Message Pass-
ing, Factor Graphs
∗TvdLacknowledgesthesupportfromGNHearingA/SandtheNetherlandsOrganization
forScientificResearch,projectnumber13925.
†AO¨ acknowledgesthesupportfromtheSwedishResearchCouncil,Grant2015-04011.
‡HW acknowledges the support from the Swedish Research Council, Grant 2018-03701.
TheauthorsthankThemistoklisCharalambousforvaluablecommentsandMagnusKoudahl
forthestimulatingdiscussions.
1
0202
tcO
02
]YS.ssee[
3v32890.0191:viXra
1 Introduction
Bayesian graphical models (BGMs) constitute an important family of tools in
signalprocessing,astheyallowlearningofmodelsaswellasinferenceofhidden
states in a unified way, often with low complexity. BGMs have been widely
applied for a wide variety of estimation and detection problems in signal pro-
cessing[1,2],withapplicationsthatincludesensornetworks[3],surveillance[4],
and information-seeking control [5]. They also naturally unify several stan-
dard methods from statistical estimation theory, such as the forward-backward
algorithm [6], the Kalman filter [7], the particle filter [8], and the Viterbi algo-
rithm [7].
Beyond learning, estimation and detection, BGMs have also found applica-
tions in stochastic control problems, which involve not only estimation of the
stateofasystem, butalsodeterminationofsuitablecontrolactionsinthepres-
enceofuncertainty. Applicationsincludecontrolofvehicles,robots,factories,or
teams of agents. Often, the control problem and inference/estimation problem
areconsideredseparate,wherebythecontrollerassumesanestimateofthestate
andtheinferenceoccurswithoutknowledgeoffuturecontrol. Suchaseparation
principle is only valid in certain cases, such as the celebrated linear quadratic
Gaussian (LQG) control [9]. Over the past decades, several approaches have
been proposed to unify inference and control [10–16], largely based on BGMs.
The core ideas of these approaches can already be found in the early work [10],
which posed the role of a controller as follows: “A controller of a stochastic
system ‘shapes’ the joint pdf describing the closed-loop behaviour. The ‘optimal’
controller should make this joint pdf as close as possible to a desired pdf.” With
thisinmind,[10]posesanidealstatedistributionandcontroldistribution,after
which an optimized controller can be found be minimizing a Kullback-Leibler
divergence(KLD).Intermsofmathematicaltractability,animportantimprove-
ment was the use of Bayesian graphical models [2], which led to the methods
in [12,13,16]. In [12], a similar idea as [10] was proposed, which allowed the
formulation of control cost as a KLD, which could be solved by approximate
inference on the corresponding graphical model. In [13], a linear transforma-
tion of the control cost function is replaced by a log-likelihood function and an
optimized controller is found by an expectation-maximization procedure over
the corresponding factor graph. A similar idea was introduced in [14,15] where
an artificial observation and associated likelihood was introduced so that state
trajectories with highest posterior probability also have lowest associated con-
trol cost. In [13,15] controllers similar to LQG were found. In [16] the LQG
control problem was targeted specifically, and under perfect knowledge of the
current state, the exact LQG controller was recovered by an inference-based
controller. Itshouldbenotedthatsomeoftheaboveworksaimtofindapolicy
(i.e. a mapping from state estimate to control) while others aim to determine
an optimal control sequence.
More generally, stochastic optimal control problems have been solved using
a diverse range of approaches, where model-predictive control (MPC) [17] and
reinforcement learning (RL) [18] are arguably the most prominent approaches.
2
Whenamodelofthesystemisavailable,thecontrolproblembecomesaMarkov
decision process, which can, in principle, be solved through dynamic program-
ming [19]. If no model is available, RL can provide model-free solutions that
learn state-action mappings from interactions with the system [20]. Recently,
there has been work combining these two approaches, originating either from
the control theory community [21] or the computer science community [22].
In addition to MPC and RL, a third and more recent path is the free en-
ergy principle (FEP), which originates from cognitive neuroscience as a way to
explain biological behavior [23,24]. The main hypothesis is that agents (i) in-
ternalize a generative model (GM) of the system, and (ii) perceive and act in
such a way as to minimize a free-energy bound on surprise relative to the GM.
Interestingly, free-energy minimization is a concept that is also used in RL to
encourageexplorationandmodelbuilding[25]. Objectivefunctionsforanykind
of system or application can be included in the GM in the form of a goal prior,
which results in formulations of active inference (ActInf) [26]. Despite a large
numberofpublicationsintheActInffieldincludingapplicationsinrobotics[27]
andsynthesiswithreinforcementlearning[28],therehavebeenonlyfewefforts,
e.g., [16,29–33], to apply it to more traditional stochastic control settings, such
as linear quadratic Gaussian (LQG) control.
In this paper, our main aim is to reveal the connections between inference
and control over BGM from an ActInf perspective. Specifically, we have the
following contributions:
1. We propose an ActInf joint inference and control formulation that casts
the control problem as an inference problem and explicitly encodes the
control cost in the FEP framework;
2. WeshowunderwhichconditionstheActInf-basedjointinferenceandcon-
trol method yields the solution to the original stochastic optimal control
problem;
3. We prove that LQG can be expressed as a special case of the proposed
ActInf joint inference and control method.
The article is structured as follows: In the remainder of this section, we
provide an overview of the notation. Sec. 2 introduces the model and optimal
control objective. Sec. 3 introduces the ActInf objective and further notation
relatedtoprobabilisticmodelformulations. Sec.4formallyrelatestheobjective
function of ActInf with stochastic optimal control. Sec. 5 then applies these
formulations to a LQG control problem, which is illustrated by the numerical
results in Sec. 6. We conclude with Sec. 7.
Notation
We write a sequence of variables as s ={s ,...,s }. At any current time
t1:t2 t1 t2
t, we consider a sequence of states, observations and controls as x = x ,
0:t+T
y = y
1:t+T
, u = u
0:t+T−1
, with x
k
∈ Rnx, controls u
k
∈ Rnu, and observations
3
Table 1: Common notations for distributions and functions.
Short Description Normalized
p joint distribution yes
p generative model at time t yes
t
p˜ goal priors yes
f goal-constrained generative model no
t
p posterior distribution of hidden variables yes
p
q belief / variational posterior yes
π stochastic policy mapping yes
t
y
k
∈Rny respectively,withatimehorizonofT time-stepsintothefuture. Note
that the start and the end points between the state, observation and control
sequence differ slightly. When explicitly required, we denote the realizations of
the random variables, such as (past) observed values, estimates and performed
actions, by a bold script.
In order to easily distinguish between the past and future variables, we
adoptthefollowingconvention: wedividetheobservationsyintopast(including
present) variables y = y and future variables y = y . Similarly, the
t 1:t t t+1:t+T
state sequence x consists of x =x and x =x . The control sequence
t 0:t t t+1:t+T
u consists of u = u and u = u (with present control included).
t 0:t−1 t t:t+T−1
For notational convenience, we drop the dependence on the current time t. For
instance, weusexinsteadofx . Weuses todenotethesequenceobtainedby
t \t
removing s from s.
t
Similar to the notation for the sequences, some of the probability density
functions (pdfs) are expressed using the notation p(.) and p (.) to emphasize
t
functions of past and future variables, respectively. As usual, marginal and
conditional pdfs associated with a given joint pdf are denoted using the same
letter/subscript. Forinstance,themarginalobtainedbymarginalizing(i.e.,inte-
(cid:82)
grating)p (s ,s )overs isdenotedbyp (s )= p (s ,s )ds . Toavoidclut-
a 1 2 2 a 1 a 1 2 2
ter, we drop the distribution arguments (i.e., we write p instead of p (s ,s ))
a a 1 2
whenever these dependencies are clear from the context.
2 System Model
2.1 Dynamical System Model
Weconsiderthefollowingdynamicalsystemwiththestate-spacemodel(SSM):
x ∼p(x |x ,u ), t≥0, (1a)
t+1 t+1 t t
y ∼p(y |x ), t≥1, (1b)
t t t
where x ∼ p(x ) and u = 0. Using the system definition in (1), the prob-
0 0 0
abilistic system model for the state and outcome sequence for a given control
4
sequence over a time window of k ∈[0,t+T] can be expressed as follows
t+T−1
(cid:89)
p(y,x|u)=p(x ) p(y |x )p(x |x ,u ). (2)
0 k+1 k+1 k+1 k k
k=0
At time t, we have the probabilistic system model
p(y =y ,y ,x|u =u ,u )
p t (y,x|u)= (cid:82) t t t t t t , (3)
p(y =y ,y ,x|u =u ,u )dy dx
t t t t t t t
wherey andu aresettotheirrealizations(y andu ). Wewillgenerallyomit
t t t t
the explicit dependence on y and u and instead rely on the sub-script t to
t t
indicatethatpastcontrolsandobservationsarefixedinp . Sincep isobtained
t t
by plugging in the values of the realizations, we re-normalize. An overview of
the common distributions used in this paper together with their normalization
status is provided in Table 1.
2.2 Control Objective
We consider stochastic policy mappings in the form of π (u |y ) from the set
k k 1:t
of measurements (up to the current time t) to the control at time k where
k ∈ [t,t + T]. The objective is to find the mappings π that minimize the
k
expected cost J over current and future states x and controls u , defined as:
t k k
t+T
(cid:88)
J = E [(cid:96) (x ,u )] , (4)
t pt,πk k k k
k=t
wherep istheprobabilisticsystemmodelasexpressedin(3),and(cid:96) (x ,u )≥0
t k k k
is the cost function at time-step k that encodes the cost of being in state x
k
and applying the control u . The realization for the current control (action) is
k
then determined using the stochastic policy mapping π .
t
In particular, the control is u =u∗ , where
t t,π
u∗ =g(π∗) (5)
t,π t
with
π∗ =argminJ (6)
t t
πt
and where g(·) represents the mapping from the probability distribution to a
single action u , which can be chosen, for instance, as the mean or the mode of
t
π∗ orasasample(i.e. realization)fromπ∗ [18,30,34,35]. Thisarticleconsiders
t t
a sliding horizon, i.e., after taking the action at the current time instant t and
obtaining the next observation, the stochastic policies are again determined by
looking T steps ahead.
Example: A typical cost function is the quadratic cost:
(cid:96) (x ,u )=(cid:96)(x ,u )= 1xTQx + 1uTRu , (7)
k k k k k 2 k k 2 k k
for Q∈Rnx×nx,Q(cid:23)0 and R∈Rnu×nu,R(cid:31)0.
5
3 Active Inference
In this section, we describe the ActInf approach and the FEP. The concepts
and approaches in this section have similarities to the control as inference lit-
erature [12,13,16], but are here presented from the ActInf perspective. The
main idea of ActInf is that at each time t, the controller minimizes the free
energy functional F [q], defined as [23] F [q] = D[q||f ], where D[q||f ] is the
t t t t
Kullback-Leibler divergence, q represents a variational distribution (the opti-
mization variable) and f represents the known generative model p with sub-
t t
stitutedobservationsorwithmodificationswithmoregeneralconstraints. Each
of these concepts will now be explained in detail. Minimization of the free en-
ergy, and the well-established framework of minimization of Bayesian surprise
are closely connected. We further discuss this relationship in Remark 1.
3.1 Generative Model and Goal Priors
3.1.1 The Generative Model
The notation introduced earlier allows us to concisely write the system model
at time t (3) in terms of a past and a future contribution:
p (y,x|u)=p (y,y,x,x|u,u), (8a)
t t
=p (y,x|u,u)p (y,x|y,x,u,u), (8b)
t t
=p (y,x|u)p (y,x|x ,u). (8c)
t t t
We note that the first factor depends on past controls and the second on the
future controls. Both factors condition on the controls, and p does not incor-
t
porate the control cost J .
t
3.1.2 Goal Priors
Thedesigneroftheagentshouldgovernthesystembehaviourtowardsdesirable
systemstates,e.g. theexitofamaze. Inordertoachievethis,ActInfintroduces
the concept of a prior belief on the future outcomes [36–38] (or equivalently
referred as a goal prior) which constrains the system model (2). This goal
prior is set by the designer of the agent and encodes the future states that are
desirable, in other words the states that we want to be unsurprising for the
agent. Actions selected as a result of free energy minimization will then move
theagentascloseaspossibletotheseunsurprising(desired)states. Agoalprior
isaddedasanadditionalfactortothesystemmodeldescription[37,38],leading
to the goal-constrained (unnormalized) GM:
f (y,x,u|u)=p (y,x|u) p˜(y,x ,x,u) . (9)
t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
generative goalprior
model
6
In order to relate the goal prior to the traditional control cost, a natural
choice is:
(cid:32) t+T (cid:33)
1 (cid:88)
p˜(y,x ,x,u)= exp −λ (cid:96) (x ,u ) , (10)
t Γ k k k
k=t
where Γ the the normalization constant and λ ≥ 0 is the scaling factor. For
thequadraticcostof (7)thegoalpriorfactorsintoindependentGaussians(i.e.,
consists of factors in the form ∝exp(−λ1xTQx )exp(−λ1uTRu )), where the
2 k k 2 k k
weighting matrices Q and R take the role of (scaled) precisions. A related
probabilistic approach is described in [15], where a binary reward is defined by
using a cost function.
3.2 Free Energy Objective
Consider the latent (hidden) variables at time t: y,x,u. Note that the state
sequencexisunknownforboththefutureandthepast,whereasfortheobserva-
tionsandthecontrols,onlythefuturevariablesareunknown. Letusconsidera
variationalposteriordistributionq(y,x,u|y,u)definedoverthelatentvariables.
Here, the label variational refers to the fact that the objective function (11)
is optimized by variations in the conditional [39]. Note that q(y,x,u|y,u) is a
posteriorconditionedonthepastobservationsandcontrols. Toavoidnotational
clutter, we adopt a mainstream notational convention in probabilistic inference
where conditioning is dropped from the variational posterior distribution, and
represent q(y,x,u|y,u) as q(y,x,u) or simply as q.
The free energy F [q] is defined as follows [23]:
t
F [q]=D[q||f ], (11)
t t
where D[q||f ] (cid:44) (cid:82) q(s)log(q(s)/f (s))ds is the Kullback-Leibler (KL) diver-
t t
gence (i.e., relative entropy). The KL divergence is an information-theoretic
concept that quantifies how much one probability distribution differs from an-
other distribution [40]. By straightforward manipulation, the free energy can
be decomposed as follows:
(cid:20) (cid:21)
q(y,x,u)
F [q]=−logZ+E log , (12)
t q p (y,x,u|y,u)
(cid:124) (cid:123)(cid:122) (cid:125) p
surprise (cid:124) (cid:123)(cid:122) (cid:125)
posteriordivergence
=−logZ+D[q||p ], (13)
p
(cid:82)
wherep denotedtheexact(Bayesian)posterior,andwhereZ = f (y,x,u|u)dydxdu,
p t
with substituted past observations y and controls u; and p (y,x,u|y,u) =
p
1f (y,x,u|u). Since the posterior (KL) divergence term is always positive,
Z t
thefreeenergyprovidesanupperboundontheexact(Bayesian)surprise. This
decomposition is often employed to justify the use of free energy as a tool for
(approximate) inference and model selection [41].
7
p˜(y,xt,x,u)
ut−1 ut ut+T−1 xt+T
...
xt−1
=
xt
= =
xt+1
...
xt+T−1
= =
p(y,x|xt,u)
yt yt+1 yt+T
p(y,x|u)
Figure 1: Forney-style factor graph representation of the goal-constrained gen-
erative model (9) with indicated factorizations. Observations are indicated by
small solid nodes.
↓ ut−1 A ↓ E ↓↑ ut D ↓ ↓ ut+T−1↓ xt+T
... → → = → = → = xt+1 ... xt+T−1 = =
xt−1 xt ← ← ← ← ←
↑ ↑ C B ↑
↑ yt ↑ yt+1 ↑ yt+T
Figure 2: Forney-style factor graph specification of the inference algorithm on
thegoal-constrainedgenerativemodel. Here, message A representsastateesti-
mate that summarizes past control and observations. The product of messages
D and E yields a posterior belief over the current control, the mode of which
is taken as the present action.
8
Remark 1 (Relation between FEP and surprise). Generally, the generative
model p is a pdf over hidden states (say x) and observations (say y), while q
is a pdf only over hidden states. Hence, p(x,y) = p(y|x)p(x), in which p(x)
represents a prior. Substituting an observation y = y in the model, yields
f (x) = p(y = y|x)p(x), which represents the product of a likelihood and the
t
prior. We are interested in obtaining a posterior belief p (x) = f (x)/Z. How-
p t
(cid:82)
ever, the normalizing constant (Bayesian evidence) Z = f (x)dx is often
t
intractable to compute, because it involves an integral over all hidden state con-
figurations. As a result, it is often prohibitively expensive (in terms of computa-
tional power) to obtain an exact solution for the posterior p . Instead, posterior
p
inference is often cast as a free energy optimization problem, where the free en-
ergy factorizes as F [q] = −logZ +D[q||p ] as in (12)–(13). Minimization of
t p
F thus maximizes Bayesian evidence, while minimizing posterior divergence,
t
making q a close approximation to the (usually intractable) posterior p .
p
3.3 Control
At time t, the objective is to find the q that minimizes the free energy, i.e.,
q∗ =argmin F [q]. (14)
t t
q
Looking at (9) and (11), we observe that after optimization, the variational
distribution q∗ simultaneously accounts for the constraints enforced by the sys-
t
tem model (2) and the goal prior (10). The posterior for the current control is
obtained by marginalization, i.e., q∗(u ) = (cid:82) ··· (cid:82) q∗(y,x,u)dy,dx,du , where
t t t \t
u denotes the sequence obtained by removing u from u. The current action
\t t
is then chosen as
u∗ =g(q∗), (15)
t,q t
where g(.) is the same as in (5).
3.4 Free Energy Minimization by Message Passing on a
Forney-style factor graph
It is instructive to separate inference relating to the past/present from the in-
ferencerelatingtothefuture. Tothisend,wesubstitutetheGMf (y,x,u|u)of
t
(9)into(11)anduse(8c)tofactorizethefreeenergyinthefollowingformwith
separate contributions from the present (V [q]) and (expected) future (G [q]):
t t
(cid:34) (cid:35) (cid:20) (cid:21)
q(x) q(y,x,u|x)
F [q]=E log +E log .
t q p (y =y,x|u=u) q p (y,x|x ,u)p˜(y,x ,x,u)
t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Vt[q] Gt[q]
(16)
In practice, the optimization of F is often intractable and a specific choice
t
forthefactorizationofqismadetoaidcomputation[42]. Inourcase,duetothe
9
factorization of the GM, we can optimize F exactly by belief propagation over
t
thefactorgraphoftheGM[43,44]. TominimizeV andG ,aForney-stylefactor
t t
graph (FFG) offers a convenient visual representation of a factorized function
[45],andisespeciallywell-suitedforrepresentingprobabilisticmodels[46]. Inan
FFG, edges represent variables and nodes (factors) represent relations between
variables. TheFFGrepresentationoftheGM(9)withsubstitutedfactorizations
(2)andincludedgoalpriors(10)isdrawninFig.1. Thefreeenergy(11)isthen
minimized by message passing [38,43,44,47] on the FFG representation of the
goal-constrained GM. Message passing can be interpreted as first minimizing
V and then minimizing a modified version of G , based on the outcome of
t t
minimizing V [2].
t
Minimizing V [q]
t
Message passing yields a message A (Fig. 2) that represents the current state
estimate, given past observations and controls. This message summarizes the
information contained within the dashed box:
(cid:90)
µ (x )(cid:44) p (y =y,x|u=u)dx . (17)
A t t 0:t−1
From the perspective of stochastic optimal control, message A connects with
the estimator. Moreover, for a linear Gaussian state-space model, the recursive
message updates for computing A constitute a Kalman filter [48].
Minimizing G [q]
t
InordertominimizeV [q]+G [q],were-normalizethemessageµ (x )toobtain
t t A t
a prior p (x ), i.e.,
e t
1
p (x )(cid:44) µ (x ) (18)
e t C A t
e
where
(cid:90)
C = p (y =y,x|u=u)dx . (19)
e t 0:t
Here, the subscript e emphasizes the fact that p represents the pdf of an esti-
e
mate(ofthecurrentstate). Wethendefineamodifiedobjectivefortheexpected
future
(cid:20) (cid:21)
q(y,x,u,x )
G˜[q]=E log t , (20)
t q p (x )p (y,x|x ,u)p˜(y,x ,x,u)
e t t t t
which can again be minimized by message passing. This yields messages B
– E (Fig. 2) by a backward recursion (smoothing pass) over the GM of future
variables. Theproductof D and E thenleadstoamarginalbeliefq t ∗(u t ). Then,
the current control action is obtained using (15).
10
4 From Active Inference To Stochastic Optimal
Control
Themainquestionwe’reinterestedinisthefollowing: “When does (15) provide
the same control actions as (5)?”. In other words, when can the ActInf frame-
work be used to solve the traditional stochastic control problem? Below, we
investigate this question. Since the goal priors only appear in G˜ and V can be
t t
minimized independently, we focus exclusively on the minimization of G˜. We
t
formulate two conditions under which minimizing G˜ reduces to minimizing the
t
stochastic optimal control objective (4).
Note that G˜[q] can be written as
t
(cid:20) (cid:21)
q(y,x,u,x )
G˜[q]=E log t −E [logp˜(y,x ,x,u)] .
t q p (x )p (y,x|x ,u) q t
e t t t
Then, minimizing G˜[q] is equivalent to minimizing G†[q]:
t t
(cid:20) (cid:21) (cid:34)t+T (cid:35)
G†[q]=E log q(y,x,u,x t ) +λE (cid:88) (cid:96)(x ,u ) , (21)
t q p (x )p (y,x|x ,u) q k k
e t t t k=t
wherewesubstitutedthegoalpriorfrom(10)andomittedtheadditiveconstants
that do not depend on the optimization variables.
A striking difference between the optimal control and ActInf objective is
that the optimal control objective (4) involves an expectation w.r.t. the system
model p and policy mapping π, while the free energy involves an expectation
t
w.r.t. thevariationaldistributionq. Inordertocomparethesolutions, weneed
to create an equal footing.
4.1 Rewriting G†
t
Westartbynotingthatallargumentsofq(y,x,u)thatarenotwithintheexpec-
tation brackets in (21) are marginalized, i.e., x is marginalized. Therefore, G˜
\t t
is effectively only optimized with respect to q(y,x,u,x ). We then rewrite the
t
variational distribution in terms of the policy by making use of a region-based
approximation [43,49]. Note that, for a model that is a tree (which is the case
for f ), the region-based approximation is exact. Without loss of generality, we
t
write:
(cid:81)t+T−1q(y
,x ,x ,u )
q(y,x,u,x )= k=t k+1 k k+1 k (22)
t (cid:81)t+T−1q(x
)
k=t+1 k
= (cid:34)t+ (cid:89) T−1 q(u ) (cid:35) (cid:34)(cid:81)t k + = T t −1q(y k+1 ,x k ,x k+1 |u k ) (cid:35) q(x ),
k (cid:81)t+T−1q(x
)
t
k=t k=t k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
π(u) φ(y,x|xt,u)
11
wherewesimplyappliedtheBethefactorizationtowritethevariationaldistribu-
tionintermsofacontrolposteriorπ,asystemposteriorφ,andthecurrent-state
posterior q(x ).
t
We now use (22) to rewrite the second term of (21), as:
(cid:34)t+T (cid:35) (cid:34) t+T (cid:35)
λE (cid:88) (cid:96)(x ,u ) =λE p e (x t )p t (y,x|x t ,u) (cid:88) (cid:96)(x ,u ) (23a)
q k k q p (x )p (y,x|x ,u) k k
k=t e t t t k=t
(cid:34) (cid:34) t+T (cid:35)(cid:35)
=λE E p e (x t )p t (y,x|x t ,u) (cid:88) (cid:96)(x ,u ) , (23b)
π q(xt),φ p (x )p (y,x|x ,u) k k
e t t t k=t
(cid:34) t+T (cid:35)
=λE q(x t ) φ(y,x|x t ,u) (cid:88) (cid:96)(x ,u ) , (23c)
pe,p t ,π p (x )p (y,x|x ,u) k k
e t t t k=t
wherein(23b)wemadetheexpectationsduetodifferenttermsofq(y,x,u,x )=
t
π(u)φ(y,x|x )q(x ) from (22) explicit and in the last step we interchanged dis-
t t
tributions in the expectation subscript with distributions in the numerators,
(cid:104) (cid:105) (cid:104) (cid:105)
i.e., we used E p(s)f(s) = E q(s)f(s) for a function f and probability dis-
q p(s) p p(s)
tributions q and p.
Wenowturntothefirsttermof(21). Againusingthefactorizationq(y,x,u,x )=
t
π(u)φ(y,x|x )q(x ) from (22), we rewrite this first term as the sum of the neg-
t t
ative policy entropy and a posterior divergence:
(cid:20) (cid:21)
q(y,x,u,x )
E log t = (24)
q p (x )p (y,x|x ,u)
e t t t
(cid:20) (cid:21)
φ(y,x|x ,u)
E [logπ(u)]+D(q(x )(cid:107)p (x ))+E log t .
q t e t q p (y,x|x ,u)
t t
Substituting (23c) and (24) in (21) then reveals the full expression for the
ActInf controller objective:
(cid:20) (cid:21)
φ(y,x|x ,u)
G†[q]=E [logπ(u)]+D(q(x )(cid:107)p (x ))+E log t +
t q t e t q p (y,x|x ,u)
t t
(cid:34) t+T (cid:35)
λE q(x t ) φ(y,x|x t ,u) (cid:88) (cid:96)(x ,u ) . (25)
pe,p,π p (x )p (y,x|x ,u) k k
e t t t k=t
Remark 2 (Interpretation of the FEP objective). The FEP objective can be
seen as a trade-off between two terms: one pulls q towards p (under unin-
formative future values for the controls) and another that minimizes the cost
(cid:80)t+T
(cid:96)(x ,u ). Minimization of only the first three terms in (25) (i.e. the case
k=t k k
with λ = 0) leads to an undetermined variational distribution q∗. Namely, be-
cause the first three terms in (25) directly stem from the first term of Eqn. (21),
we have q∗(y,x,u,x ) = p (x )p (y,x|x ,u). Then, we have G†[q∗] = 0 (with
t e t t t t
λ=0).
Conversely, minimization of only the last term in (25) (with λ > 0) leads
to a degenerate variational distribution q∗ with mass only at a global minimizer
12
(cid:104) (cid:105)
of (cid:80)t+T (cid:96)(x ,u ), because this last term is equal to E (cid:80)t+T (cid:96)(x ,u ) (recall
k=t k k q k=t k k
that the last term in (25) is only a rewritten version of the last term in (21)).
(cid:104) (cid:105)
In particular, while minimizing E (cid:80)t+T (cid:96)(x ,u ) over q, since there are no
q k=t k k
other constraints on q, and q can be directly chosen as a distribution with point
mass at a global minimizer of
(cid:80)t+T
(cid:96)(x ,u ). For instance, with (cid:96)(.) defined as
k=t k k
(7), q with point mass at x =0, u =0 ∀k is a minimizer.
k k
We now present two sets of conditions under which the actions chosen by
the ActInf agent by (15) are the same as the stochastic control actions chosen
using (5).
4.2 FirstCondition: DeterministicModelwithPoint-Estimate
Let mode(·) represent the mode of a distribution, where ties between multiple
modes (if any) are resolved by uniform random selection.
Theorem 1. Let (i) λ>0, (ii) φ=p , (iii) q =p , and (iv) g(·)=mode(·).
t e e
Then, u∗ =u∗ .
t,q t,π
Proof. See Appendix A.
The below result shows that Theorem 1 implies that the optimal solution is
recovered for deterministic systems with a point-estimate.
Corollary 1. The conditions (ii) φ=p , and (iii) q =p of Theorem 1 occur
t e e
in the case of a deterministic model p in conjunction with a point estimate for
t
the current state.
Proof. In the case of a deterministic model, p of (1) is constrained to delta
t
functions; i.e., p(x |x ,u ) = δ(x −f (x ,u )) and p(y |x ) = δ(y −
k+1 k k k+1 x k k k k k
f (x )), for some deterministic functions f (·) and f (·). A point estimate for
y k x y
thecurrentstate(aftertheminimizationofV )ischosenasp (x )(cid:44)δ(x −xˆ ).
t e t t t
Then, any condition other than (ii) φ = p , (iii) q = p will lead to infinite
t e e
divergence in (24), and hence in (25). By contradiction, (ii) φ = p and (iii)
t
q =p are the only viable solutions to the minimization of G˜ under the choice
e e t
of a deterministic model with a point estimate for the current state.
4.3 Second Condition: Vanishing State and Control Cost
We now consider minimizers of (25) as a function of λ, and define
q∗(x )φ∗(y,x|x ,u) q∗(y,x,x |u)
r (y,x,x ,u)(cid:44) t t = t . (26)
λ t p (x )p (y,x|x ,u) p (y,x,x |u)
e t t t t t
Note that the distribution q is an argument of (25). Hence, the optimal q
depends on λ. In light of Remark 2, we see that for λ = 0, r (y,x,x ,u) = 1,
λ t
while for λ>0, r (y,x,x ,u)(cid:54)=1.
λ t
13
Figure 3: Results comparing LQG with ActInf control, where the time-axis is
log-scaled. ThemoreaggressiveLQGcontrol(bottomleft),leadstofasterstate
adjustments (top left). ActInf control for small but nonzero λ reduces to LQG
control. Notably, although ActInf control with λ = 1 accumulates higher cost
in terms of (cid:96)(x ,u ) in (7) (bottom right), it achieves lower free energy than
k k
ActInf control with small λ (top right).
Theorem2. Let(i)lim 1E[logr (y,x,x ,u)]=0,(ii)lim r (y,x,x ,u)=
λ→0+ λ q λ t λ→0+ λ t
1, ∀y,x,x ,u, and (iii) u =modeπ . Then, u∗ =u∗ .
t t t t,q t,π
Proof. See Appendix B.
Condition (ii) requires that, under a vanishing λ, the second term of (25)
growstozerofasterthanλitself. Hence, under(ii), thelasttermwilldominate
over the second term, retaining the dependence of G˜ on (cid:96) (see the proof for de-
t
tails). Notethatifweoutrightrequireλ=0,alldependenceon(cid:96)isimmediately
lost. Instead, the limit ensures that the influence of the cost (cid:96) is retained.
It is not straightforward to see when the conditions (i) – (ii) of Theorem 2
apply. In the subsequent sections, we further discuss the implications of Theo-
rem 1 and Theorem 2 for the special case of a linear Gaussian SSM.
5 Relationship Between LQG Control And Ac-
tive Inference For A Linear Gaussian SSM
WenowinvestigatethebehavioroftheActInfcontrollerunderalinearGaussian
state-space model. We assume a linear Gaussian system with the respective
transition and observation precisions W and W , as follows:
w v
p(x |x ,u )=N(x |Ax +Bu ,W−1) (27a)
t+1 t t t+1 t t w
p(y |x )=N(y |Cx ,W−1). (27b)
t t t t v
14
wherethenotationN(z|m,V)representsaGaussiandistributionwiththemean
m and the covariance (inverse precision) matrix V = W−1 for the variable z.
We consider the quadratic cost in (7), leading to independent Gaussian goal
priors (10).
5.1 Algebraic Results for the Active Inference Controller
Aclosed-form expressionfortheresulting ActInfregulatorisobtainedbyprop-
agating the messages of Fig. 2 algebraically as follows:
Theorem 3. The ActInf solution for the system in (27) is given by
u∗ =−K xˆ (28a)
t,q t t
(cid:20) (cid:16) (cid:17)−1 (cid:21)−1
K = BT AVˆ(cid:48)AT+P−1 +W−1 B+λR ×
t t t+1 w
(cid:16) (cid:17)−1
BT AVˆ(cid:48)AT+P−1 +W−1 AVˆ(cid:48)Wˆ (28b)
t t+1 w t t
(cid:16) (cid:17)−1
Vˆ(cid:48) = Wˆ +λQ , (28c)
t t
where xˆ and Wˆ are the respective mean and precision of p (the normalized
t t e
message A). Here, P
k
, i.e. the precision of the backward message over state
x , is given by
k
P =λQ (29a)
t+T
P =−ATP B (cid:0) R(cid:48)+BTP B (cid:1)−1 BTP A+
k−1 k k k
ATP A+λQ (29b)
k
R(cid:48) = (cid:16) [λR]−1+ (cid:2) BTW B (cid:3)−1 (cid:17)−1 . (29c)
w
Proof. See Appendix C.
Note that this result provides an iterative procedure for finding the ActInf
solution. In particular, we initialize P with (29a). Then, P ’s can be cal-
t+T k
culated iteratively and offline, i.e., without obtaining the measurements. Then,
the action is found using (28). Here, calculation of K requires calculation of
t
Wˆ . In the LQG case, p is a Gaussian pdf with a mean and covariance that
t e
are given by the standard Kalman filtering equations, see for instance [48,50].
We now investigate the conditions implied by the two theorems and the
corollary from Sec. 3.1.2. Theorem 1 assumes a deterministic model and a
point estimate for the current state (Cor. 1), which corresponds to Wˆ =
t
W = (cid:15)I ,(cid:15) → ∞. Theorem 2 investigates the dependence on λ, and lets
w 2
λ → 0+. In both cases (29c) reduces to R(cid:48) = λR, and (28b) reduces to
K = (cid:2) BTP B+λR (cid:3)−1 BTP A,thusrecoveringtheclassicallyoptimalLQG
t t+1 t+1
solution in the form of the discrete-time finite horizon Ricatti equations [34].
Note that compared to the standard LQG solution, both Q and R appear to be
15
scaled with λ in the above equations, which has no effect on the optimal solu-
tion. This can be seen for instance by recognizing that this scaling corresponds
to the scaling of both matrices with λ in (7), which corresponds to a simple
scalar scaling of the objective function.
6 Numerical Results
6.1 Scenario
In this section, we illustrate the performance of the ActInf controller for vary-
ing positive values of λ and compare the results with the standard LQG sce-
nario. The ActInf simulations are performed with the ForneyLab probabilistic
programming toolbox [51], and follow the experimental protocol in [38]. The
protocol at each time t consists of three main steps: (i) find A and the current
state estimate p by minimizing V (16), (ii) from the estimate, find a control
e t
posterior q∗(u ) by minimizing G˜ (20), and (iii) pass a selected action to the
t t t
system (1) to obtain a new observation.
For the system, we use (27), with C = R = Q = W = W = I , A =
v w 2
(cid:18) (cid:19) (cid:18) (cid:19)
1 0.1 0.1 0.5
,B = . TheGMfollowsthesystemassumptionsanduses
0 1 0.05 0.5
a lookahead of T =10. We initialize the system relatively far from equilibrium,
at x =(25,25)T and choose a vague prior for the initial state x .
0 0
6.2 Discussion
The results are presented in Fig. 3, which leads to several interesting observa-
tions. Firstly,forsmallbutnonzeroλ,theresults(controls,statetrajectory,the
accumulated cost for (cid:96)(x ,u ) and also G ) of the ActInf controller approaches
k k t
theresultsoftheLQGcontrollerasexpected; seeSec.3.1.2andalsothediscus-
sions at the end of Sec. 5. We note that, for the current system, λ=0.01 (not
plotted) already renders the results of the ActInf and LQG controller nearly
visually indistinguishable.
Secondly, the LQG controller is more aggressive than the ActInf regulator
in terms of the controls, i.e., the magnitude of the LQG controls are relatively
large compared to those of the ActInf regulator. The explicit inclusion of the
processnoiseinActInfisincontrasttotheLQGscenariowheretheprocessand
estimation noise only affect the state estimation directly but not the regulator
[16]. Inparticular,(29a–28c)dependexplicitlyuponW andWˆ ,whereasthese
w t
terms are not present in the original Ricatti equations. These terms make the
ActInf controller more conservative.
Thirdly, the accumulated cost in terms of (cid:96)(x ,u ) for the ActInf controller
k k
approaches the optimal cost of the LQG controller under decreasing λ. This
observationisconsistentwithTheorem2,whichformulatessufficientconditions
for making the ActInf solution coincide with the LQG solution. Interestingly,
and perhaps counter intuitively, the terminal free energy for λ=1 is improved
16
(lower) compared to the λ=0.1 case. This effect can be interpreted in light of
the good regulator theorem, which states that “every good regulator of a system
mustbeamodelofthatsystem”[52]. Namely,wheretheLQGcostfunction(7)
measures a quadratic cost, the free energy (25) offers an approximate measure
of model fitness (12). This then implies that the ActInf regulator with λ = 1
better models the system properties than the ActInf regulator with λ = 0.1,
leading to lower free energy.
7 Conclusions
ActInf and the free energy principle provide a flexible and general framework
for stochastic optimal control problems. By including the control cost as goal
priors, the control cost appears as an additive term in the free energy. The
resulting free energy minimization problem can be solved by belief propagation
over the associated factor graph, leading to an elegant and tractable approach
to solve stochastic optimal control problems. In general, the ActInf controller
does not solve the underlying stochastic optimal control problem. To address
this, we provide sufficient conditions for which ActInf reduces to traditional
stochastic optimal control. In other words, under certain conditions, stochastic
optimal control is a subset of ActInf control. Finally, while it is not known for
whichclassesofproblemthesufficientconditionshold,weproveandnumerically
demonstrate that the ActInf controller is a generalization of the important case
of the LQG controller.
At the heart of these methods lies the fact that ActInf allows us to directly
control the modeling assumptions. Therefore, we can explicitly include the
anticipated effect of the costs and the noise in the control policy. Controlling
these assumptions allows us to reproduce traditional stochastic optimal control
solutions, such as the LQG controller.
References
[1] H.-A. Loeliger, J. Dauwels, V. M. Koch, and S. Korl, “Signal processing
with factor graphs: examples,” in First International Symposium on Con-
trol, Communications and Signal Processing, 2004., pp. 571–574, IEEE,
2004.
[2] H.-A.Loeliger,J.Dauwels,J.Hu,S.Korl,L.Ping,andF.R.Kschischang,
“Thefactorgraphapproachtomodel-basedsignalprocessing,”Proceedings
of the IEEE, vol. 95, no. 6, pp. 1295–1322, 2007.
[3] A. Swami, Q. Zhao, Y. Hong, and L. Tong, Graphical Models and Fusion
in Sensor Networks, pp. 215–249. John Wiley & Sons, 2007.
[4] G. Ferri, A. Munaf`o, A. Tesei, P. Braca, F. Meyer, K. Pelekanakis,
R. Petroccia, J. Alves, C. Strode, and K. LePage, “Cooperative robotic
17
networks for underwater surveillance: an overview,” IET Radar, Sonar &
Navigation, vol. 11, no. 12, pp. 1740–1761, 2017.
[5] F. Meyer, H. Wymeersch, M. Fr¨ohle, and F. Hlawatsch, “Distributed esti-
mationwithinformation-seekingcontrolinagentnetworks,”IEEE Journal
on Selected Areas in Communications,vol.33,no.11,pp.2439–2456,2015.
[6] L. R. Rabiner, “A tutorial on hidden markov models and selected appli-
cations in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2,
pp. 257–286, 1989.
[7] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger, “Factor graphs and
the sum-product algorithm,” IEEE Transactions on information theory,
vol. 47, no. 2, pp. 498–519, 2001.
[8] A. T. Ihler, J. W. Fisher, R. L. Moses, and A. S. Willsky, “Nonparametric
belief propagation for self-localization of sensor networks,” IEEE Journal
on Selected Areas in Communications, vol. 23, no. 4, pp. 809–819, 2005.
[9] P. E. Caines, Linear stochastic systems, vol. 77. SIAM, 2018.
[10] M.K´arny`,“Towardsfullyprobabilisticcontroldesign,”Automatica,vol.32,
no. 12, pp. 1719–1722, 1996.
[11] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum entropy
inverse reinforcement learning,” AAAI, 2008.
[12] H. J. Kappen, V. Gomez, and M. Opper, “Optimal control as a graphical
model inference problem,” Machine Learning, vol. 87, pp. 159–182, May
2012.
[13] J. Watson, H. Abdulsamad, and J. Peters, “Stochastic Optimal Control as
Approximate Input Inference,” Conf. on Robot Learning, 2019.
[14] S.Levine,“ReinforcementLearningandControlasProbabilisticInference:
Tutorial and Review,” arXiv:1805.00909, 2018.
[15] M. Toussaint, “Robot trajectory optimization using approximate infer-
ence,” in Int. Conf. on Machine Learning (ICML), pp. 1–8, 2009.
[16] C.HoffmannandP.Rostalski,“LinearOptimalControlonFactorGraphs-
a Message Passing Perspective,” in 20th IFAC World Congress, (Toulouse,
France), July 2017.
[17] J.H. Lee, “Model predictivecontrol: Reviewof thethreedecades ofdevel-
opment,” IJCAS, vol. 9, no. 3, p. 415, 2011.
[18] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
MIT press, 2018.
18
[19] M.L.Puterman,Markov Decision Processes: Discrete Stochastic Dynamic
Programming. John Wiley & Sons, 2014.
[20] T. Degris, P. M. Pilarski, and R. S. Sutton, “Model-free reinforcement
learning with continuous action in practice,” in 2012 American Control
Conference (ACC), pp. 2177–2182, 2012.
[21] G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots,
and E. A. Theodorou, “Information theoretic MPC for model-based rein-
forcement learning,” in 2017 IEEE ICRA, pp. 1714–1721, May 2017.
[22] S. Kamthe and M. P. Deisenroth, “Data-Efficient Reinforcement Learn-
ing with Probabilistic Model Predictive Control,” arXiv:1706.06491, June
2017.
[23] K. J. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
brain,” Journal of Physiology, Paris, vol. 100, pp. 70–87, Sept. 2006.
[24] M. J. D. Ramstead, P. B. Badcock, and K. J. Friston, “Answering
Schr¨odinger’s question: A free-energy formulation,” Physics of Life Re-
views, 2018.
[25] B. Sallans and G. E. Hinton, “Using free energies to represent Q-values in
a multiagent reinforcement learning task,” in Adv. in neural information
process. systems, pp. 1075–1081, 2001.
[26] K. J. Friston, “The free-energy principle: a unified brain theory?,” Nature
Reviews Neuroscience, vol. 11, pp. 127–138, Feb. 2010.
[27] G. Oliver, P. Lanillos, and G. Cheng, “Active inference body perception
and action for humanoid robots,” Arxiv:1906.03022, 2019.
[28] O. C¸atal, J. Nauta, T. Verbelen, P. Simoens, and B. Dhoedt, “Bayesian
policy selection using active inference,” arXiv:1904.08149, 2019.
[29] K. Ueltzh¨offer, “Deep Active Inference,” Biological Cybernetics, vol. 112,
pp. 547–573, Dec. 2018.
[30] S. Schw¨obel, S. Kiebel, and D. Markovic, “Active Inference, Belief Prop-
agation, and the Bethe Approximation,” Neural Computation, vol. 30,
pp. 2530–2567, Sept. 2018.
[31] M. Baltieri and C. L. Buckley, “Active Inference: Computational Models
of Motor Control without Efference Copy,” in 2019 Conf. on Cognitive
Computational Neuroscience, 2019.
[32] B. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley, “On the relation-
shipbetweenactiveinferenceandcontrolasinference,”in1stInternational
Workshop on Active Inference, 2020.
19
[33] A. Imohiosen, J. Watson, and J. Peters, “Active inference or control as
inference? a unifying view,” in 1st International Workshop on Active In-
ference, 2020.
[34] T.GladandL.Ljung, Control Theory: Multivariable and Nonlinear Meth-
ods. Taylor-Francis, 2000.
[35] H. Attias, “Planning by Probabilistic Inference,” in Inter. Conf. on Artifi-
cial Intelligence and Statistics (AISTATS), 2003.
[36] B. de Vries and K. J. Friston, “A Factor Graph Description of Deep Tem-
poral Active Inference,” Frontiers in Computational Neuroscience, vol. 11,
Oct. 2017.
[37] T. Parr and K. J. Friston, “Generalised free energy and active inference:
can the future cause the past?,” bioRxiv, Apr. 2018.
[38] T.W.vandeLaarandB.deVries,“SimulatingActiveInferenceProcesses
by Message Passing,” Frontiers in Robotics and AI, vol. 6, p. 20, 2019.
[39] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational Inference: A
Review for Statisticians,” Journal of the American Statistical Association,
vol. 112, pp. 859–877, Apr. 2017.
[40] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley,
1991.
[41] H. Attias, “A variational Bayesian framework for graphical models,” in
Adv. in neural information process. systems, pp. 209–215, 2000.
[42] C. M. Bishop, Pattern recognition and machine learning. Springer, 2006.
[43] J.S.Yedidia,W.T.Freeman,andY.Weiss,“Constructingfree-energyap-
proximations and generalized belief propagation algorithms,” IEEE Trans.
Inf. Theory, vol. 51, pp. 2282–2312, July 2005.
[44] T.Heskes,“Stablefixedpointsofloopybeliefpropagationarelocalminima
of the Bethe free energy,” in Adv. in neural information process. systems,
pp. 359–366, 2003.
[45] G. D. Forney, “Codes on graphs: normal realizations,” IEEE Trans. on
Information Theory, vol. 47, pp. 520–548, Feb. 2001.
[46] H.-A.Loeliger,“Anintroductiontofactorgraphs,”Signal Processing Mag-
azine, IEEE, vol. 21, no. 1, pp. 28–41, 2004.
[47] J. Dauwels, “On Variational Message Passing on Factor Graphs,” in IEEE
Inter. Symp. on Information Theory, pp. 2546–2550, June 2007.
[48] S. Korl, A factor graph approach to signal modelling, system identification
and filtering. ETH Zurich, 2005.
20
[49] R. Cowell, “Introduction to inference for Bayesian networks,” in Learning
in graphical models, pp. 9–26, Springer, 1998.
[50] S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation The-
ory. Prentice-Hall, 1993.
[51] M. Cox, T. W. van de Laar, and B. de Vries, “A factor graph approach
to automated design of Bayesian signal processing algorithms,” IJAR,
vol. 104, pp. 185–204, Jan. 2019.
[52] R. C. Conant and W. R. Ashby, “Every good regulator of a system must
be a model of that system,” Intl. J. Systems Science, pp. 89–97, 1970.
Appendix
A Proof of Theorem 1
First, we substitute (ii), (iii) in (25) which removes the second and third term
and the factors within the expectation of the last term resulting in
(cid:34)t+T (cid:35)
(cid:88)
G†[q]=E [logπ(u)]+λE (cid:96)(x ,u ) . (30)
t q pe,p t ,π k k
k=t
Let LT(x,x ,u)(cid:44)(cid:80)t+T (cid:96)(x ,u ). We now focus on the second term in (30)
t t k=t k k
(cid:90)
λ p (x )p(y,x|x ,u)π(u)LT(x,x ,u)dudxdydx (31a)
e t t t t t
(cid:90)
=C λ p (y,x|u)p (y,x|x ,u)π(u)LT(x,x ,u)dydxdu (31b)
e t t t t t
(cid:90)
=C λ p (y,x|u)π(u)LT(x,x ,u)dydxdu (31c)
e t t t
=C λJ , (31d)
e t
wherein(31b),wehaveused(19)and(18); andin(31c)wehaveused(8c); and
in (31d) we have used (4), i.e. the definition of J .
t
Since the mode of the policy is used for the current action by (iv), the
negativepolicyentropytermE [logπ(u)]in(30)doesnotaffectactionselection.
q
We therefore absorb the policy entropy in a constant C. Hence, (25) reduces
to a function of the form G†[q] = λC J [q]+C. Scaling of the scalar optimal
t e t
control objective J does not affect regulator behavior. Hence, the standard
t
stochastic control solution u∗ is the same as the ActInf solution u∗ .
t,π t,q
B Proof of Theorem 2
RecallfromTheorem1thatundercondition(iii),thefirsttermin(25)doesnot
affect the optimal solution. Furthermore, (ii) removes the ratio r (y,x,x ,u)
λ t
21
from the last term in (25). Hence, substituting in these modifications and
multiplying the objective with 1/λ (note that multiplications with 1/λ > 0 do
not change optimal solutions), we have following objective function:
1 D(q∗(x )(cid:107)p (x ))+ 1 E (cid:20) log φ∗(y,x|x t ,u) (cid:21) +E (cid:34)t (cid:88) +T (cid:96)(x ,u ) (cid:35)
λ t e t λ q∗ p (y,x|x ,u) p∗ e ,p∗,π∗ k k
t t k=t
(32a)
= 1 E (cid:20) log q∗(x t )φ∗(y,x|x t ,u) (cid:21) +E (cid:34)t (cid:88) +T (cid:96)(x ,u ) (cid:35) , (32b)
λ q∗ p (x )p (y,x|x ,u) p∗ e ,p∗,π∗ k k
e t t t k=t
where in (32b) we have used
(cid:20) q∗(x ) (cid:21) (cid:20) q∗(x ) (cid:21)
D(q∗(x )(cid:107)p (x ))=E log t =E log t .
t e t q∗(xt) p (x ) q∗ p (x )
e t e t
(cid:104) (cid:105)
Takingthelimitwithλ→0+andsubstituting(ii),weobtainE (cid:80)t+T (cid:96)(x ,u )
p∗ e ,p∗,π∗ k=t k k
as desired. Then, the optimal control objective (4) is again (proportionally) re-
covered, and hence u∗ =u∗ .
t,q t,π
C Proof of Theorem 3
ThealgebraicresultfortheActInfregulator, (29)and(28), isobtainedbymes-
sage passing (Fig. 2). We derive this result by following the standard belief
propagation update rules as summarized by [48, Table 4.1]. For notational con-
venience, we write mean-variance and mean-precision parameterized Gaussian
distributionsasN andN respectively,wheredistributionvariablearguments
V W
are left implicit.
Backward Recursion
The backward recursion (29) follows from message passing in a section of the
model as visualized in Fig. 4. Note that our specific choice of goal prior (10) is
independent of y k . As a result, messages 2 and 3 are uninformative, and do
not contribute to the end result.
The messages of Fig. 4 are computed as follows:
1 ∝N W (0,P k )
2 ∝1
3 ∝1
4 ∝N W (0,P k )
5 ∝N V (cid:0) 0,P k −1+W w −1(cid:1)
6 ∝N W (0,λR)
7 ∝N V
(cid:0) 0,B(λR)−1BT(cid:1)
22
0 λR
N
→ W ←
6 ↓ u k−1
0 λQ
N B
→ W ←
↓ W
10 ↓ 7 ↓ w
1
... x k−1 = A + N = ← ...
← ← ← ← W ← x
k
11 9 8 5 4
↑ 3
C
↑ 2
W
v
N
→ W
y k ↑
Figure 4: Message passing schedule for recursive backward propagation in a
single (future) section of a linear Gaussian state-space model (27).
8 ∝N V (cid:0) 0,P k −1+B(λR)−1BT+W w −1(cid:1)
9 ∝N W (cid:0) 0,AT(P k −1+B(λR)−1BT+W w −1)A (cid:1)
10 ∝N W (0,λQ)
11 ∝N W (cid:0) 0,AT(P k −1+B(λR)−1BT+W w −1)A+λQ (cid:1) ,
(cid:124) (cid:123)(cid:122) (cid:125)
Pk−1
whereweidentifyarecursionoverP . TherecursionisinitializedwithP =
k−1 t+T
λQ, and terminates when P is computed. The result simplifies further:
t+1
[R(cid:48)]−1
(cid:122) (cid:125)(cid:124) (cid:123)
P =AT(P−1+B[(λR)−1+(BTW B)−1]BT)A
k−1 k w
+λQ (using BB−1 =I)
=ATP A−ATP B[R(cid:48)+BTP B]−1BTP A
k k k k
+λQ (using Woodbury identity),
with
R(cid:48) =[(λR)−1+(BTW B)−1]−1.
w
This concludes the derivation of (29). The computation of R(cid:48) can be simplified
by making use of Searle’s identity.
23
Control Law
The control law (28) follows from message passing in a section of the model as
visualized in Fig. 5.
0 λR
N
→ W ←
6 ↓ ↑ 12 u t
0 λQ
N B
→ W ←
↓ W
8 ↓ ↑ 11 w
1
... x t = A + N = ← ...
→ → → ← W ← x
t+1
7 9 10 5 4
↑ 3
C
↑ 2
W
v
N
→ W
y t+1 ↑
Figure 5: Message passing schedule for the control law in a single (present)
section of a linear Gaussian state-space model (27).
The messages of Fig. 5 are computed as follows:
1 ∝N W (0,P t+1 )
2 ∝1
3 ∝1
4 ∝N W (0,P t+1 )
5 ∝N V (cid:0) 0,P t − + 1 1 +W w −1(cid:1)
6 ∝N W (0,λR)
(cid:16) (cid:17)
7 ∝N W xˆ t ,Wˆ t
8 ∝N W (0,λQ)
(cid:16) (cid:17)
9 ∝N V [Wˆ t +λQ]−1Wˆ t xˆ t ,[Wˆ t +λQ]−1
(cid:16) (cid:17)
10 ∝N V A[Wˆ t +λQ]−1Wˆ t xˆ t ,A[Wˆ t +λQ]−1AT
(cid:16)
11 ∝N V −A[Wˆ t +λQ]−1Wˆ t xˆ t ,
(cid:17)
A[Wˆ +λQ]−1AT+P−1 +W−1
t t+1 w
24
(cid:16)
12 ∝N W −B−1A[Wˆ t +λQ]−1Wˆ t xˆ t ,
(cid:17)
BT[A[Wˆ +λQ]−1AT+P−1 +W−1]−1B .
t t+1 w
The current control then follows from
q t ∗(u t )∝ 6 × 12
u =modeq∗(u )
t t t
=−K xˆ ,
t t
where (using the Gaussian equality rule)
K =[BT(AVˆ(cid:48)AT+P−1 +W−1)−1B+λR]−1×
t t t+1 w
BT(AVˆ(cid:48)AT+P−1 +W−1)−1AVˆ(cid:48)Wˆ ,
t t+1 w t t
with
Vˆ(cid:48) =(Wˆ +λQ)−1.
t t
This concludes the derivation of (28).
25