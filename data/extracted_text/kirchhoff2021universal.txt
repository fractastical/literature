arXiv:2004.08128v5  [cs.AI]  28 Sep 2020
WH E N C E T H E EX P E C T E D FR E E EN E R G Y ?
Beren Millidge
School of Informatics
University of Edinburgh
beren@millidge.name
Alexander Tschantz
Sackler Center for Consciousness Science
School of Engineering and Informatics
University Sussex
tschantz.alec@gmail.com
Christopher L Buckley
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
C.L.Buckley@sussex.ac.uk
September 30, 2020
ABSTRACT
The Expected Free Energy (EFE) is a central quantity in the th eory of active inference. It is the
quantity that all active inference agents are mandated to mi nimize through action, and its decompo-
sition into extrinsic and intrinsic value terms is key to the balance of exploration and exploitation
that active inference agents evince. Despite its importanc e, the mathematical origins of this quantity
and its relation to the V ariational Free Energy (VFE) remain unclear. In this paper, we investigate
the origins of the EFE in detail and show that it is not simply " the free energy in the future". W e
present a functional that we argue is the natural extension o f the VFE, but which actively discourages
exploratory behaviour, thus demonstrating that explorati on does not directly follow from free energy
minimization into the future. W e then develop a novel object ive, the Free-Energy of the Expected
Future (FEEF), which possesses both the epistemic componen t of the EFE as well as an intuitive
mathematical grounding as the divergence between predicte d and desired futures.
1 Introduction
The Free-Energy Principle (FEP) (K. Friston, 2010; K. Frist on & Ao, 2012a; K. Friston, Kilner, & Harrison, 2006)
is an emerging theory from theoretical neuroscience which o ffers a unifying explanation of the dynamics of self-
organising systems (K. Friston, 2019; Parr, Da Costa, & Fris ton, 2020). It proposes that such systems can be inter-
preted as embodying a process of variational inference whic h minimizes a single information-theoretic objective – the
V ariational Free-Energy (VFE). In theoretical neuroscien ce, the FEP translates into an elegant account of brain func-
tion (K. Friston, 2003, 2005, 2008; K. J. Friston, 2008; K. J. Friston, Trujillo-Barreto, & Daunizeau, 2008), extending
the Bayesian Brain hypothesis (Deneve, 2005; Doya, Ishii, P ouget, & Rao, 2007; Knill & Pouget, 2004) by postulating
that the neural dynamics of the brain perform variational in ference. Under certain assumptions about the forms of the
densities embodied by the agent, this theory can even be tran slated down to the level of neural circuits in the form of a
A PREPRINT - S EPTEMBE R 30, 2020
biologically plausible neuronal process theory (Bastos et al., 2012; K. Friston, 2008; Kanai, Komura, Shipp, & Friston ,
2015; Shipp, 2016; Spratling, 2008).
Action is then subsumed into this formulation, under the nam e of active inference (K. Friston, 2011; K. Friston & Ao,
2012a; K. J. Friston, Daunizeau, & Kiebel, 2009) by mandatin g that agents act so as to minimize the VFE with re-
spect to action (Buckley, Kim, McGregor, & Seth, 2017; K. Fri ston et al., 2006). This casts action and perception
as two aspects of the same imperative of free-energy minimiz ation, resulting in a theoretical framework for con-
trol which applies to a variety of continuous-time tasks (Ba ltieri & Buckley, 2017, 2018; Calvo & Friston, 2017;
K. Friston, Mattout, & Kilner, 2011; Millidge, 2019c).
Recent work has extended these ideas to account for inferenc e over temporally extended action sequences.
(K. Friston & Ao, 2012a; K. Friston, FitzGerald, Rigoli, Sch wartenbeck, & Pezzulo, 2017a; K. Friston et al.,
2016, 2015; Tschantz, Seth, & Buckley, 2019). Here it is assu med that rather than action minimis-
ing the instantaneous VFE, sequences of actions (or policie s) minimise the cumulative sum over time
of a quantity called the Expected Free Energy (EFE) (K. Friston et al., 2015). Active inference using
the EFE has been applied to a wide variety of tasks and applica tions, from modelling human and an-
imal choice behaviour (FitzGerald, Schwartenbeck, Moutou ssis, Dolan, & Friston, 2015; K. Friston et al.,
2015; Pezzulo, Cartoni, Rigoli, Pio-Lopez, & Friston, 2016 ), simulating visual saccades and other ‘epis-
temic foraging behaviour’ (K. J. Friston, Lin, et al., 2017; K. J. Friston, Rosch, Parr, Price, & Bowman, 2018;
Mirza, Adams, Mathys, & Friston, 2016; Parr & Friston, 2017a , 2018a), solving reinforcement learning benchmarks
(Çatal, V erbelen, Nauta, De Boom, & Dhoedt, 2020; Millidge, 2019a, 2019b; Tschantz, Baltieri, Seth, Buckley, et al.,
2019; Ueltzhöffer, 2018; van de Laar & de Vries, 2019), to mod elling psychiatric disorders as cases of aber-
rant inference (Cullen, Davey, Friston, & Moran, 2018; Mirz a, Adams, Parr, & Friston, 2019; Parr & Friston,
2018b). Like the continuous-time formulation, active infe rence also comes equipped with a biologically plausi-
ble process theory with variational update equations which have been argued to be homologous with observed
neural ﬁring patterns (K. Friston et al., 2017a; K. Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo, 2017b;
K. J. Friston, Parr, & de Vries, 2017; Parr, Markovic, Kiebel , & Friston, 2019).
A key property of the EFE is that it decomposes into both an ext rinsic, value-seeking, and an intrinsic (epistemic),
information-seeking term (K. Friston et al., 2015). The lat ter mandates active inference agents to resolve uncertaint y
by encouraging the exploration of unknown regions of the env ironment, a property which has been extensively in-
vestigated (K. Friston et al., 2017a, 2015; Schwartenbeck, FitzGerald, Dolan, & Friston, 2013; Schwartenbeck et al.,
2019). The fact that intrinsic drives naturally emerge from this formulation is argued as an advantage over other for-
mulations that typically encourage exploration by adding a d-hoc exploratory terms to their loss function (Burda et al. ,
2018; Mohamed & Rezende, 2015; Oudeyer & Kaplan, 2009; Patha k, Agrawal, Efros, & Darrell, 2017). While the
EFE is often described as a straightforward extension to the free energy principle that can account for prospective
policies and is typically expressed in similar mathematica l form (Da Costa et al., 2020; K. Friston et al., 2017a, 2015;
Parr & Friston, 2017b, 2019), its origin remains obscure. Mi nimization of the EFE is sometimes motivated by a re-
ductio ad absurdum argument following from the FEP (K. Friston et al., 2015; Par r & Friston, 2019) in that agents are
driven to minimize the VFE, and therefore the only way they ca n act is to minimize their free-energy into the future.
Since the future is uncertain, however, instead they must mi nimize the expected free energy. Central to this logic is the
formal identiﬁcation of the VFE with the EFE.
In this paper, we set out to investigate the origin of the EFE a nd its relations with the VFE. W e provide a broader
perspective on this question, showing that the EFE is not the only way to extend the VFE to account for action-
conditioned futures. W e derive an objective which we believ e to be a more natural analogue of the VFE, which we call
the Free Energy of the Future (FEF), and make a detailed side-by-side comparison of the tw o functionals.Crucially,
we show that the FEF actively discourages information-seek ing behaviour, thus demonstrating that epistemic terms do
not necessarily arise simply from extending the VFE into the future. W e then investigate the origin of the epistemic
term of the EFE, and show that the EFE is just the FEF minus the n egative of the epistemic term in the EFE, which
2
A PREPRINT - S EPTEMBE R 30, 2020
thus provides a straightforward perspective on the relatio n between the two functionals. W e then propose our own
mathematically principled starting point for action-sele ction under active inference – the divergence between desir ed
and expected futures, from which we obtain a novel functiona l the Free-Energy of the Expected Future (FEEF), which
has close relations to the generalized free energy (Parr & Fr iston, 2019). This functional has a natural interpretation
in terms of the divergence between a veridicial and a biased g enerative model; it allows use of the same functional
for both inference and policy selection, and it naturally de composes into an extrinsic value term and an epistemic
action term, thus maintaining the attractive exploratory p roperties of EFE-based active inference while also possess ing
a mathematically principled starting point with an intuiti ve interpretation.
2 The V ariational Free Energy
The V ariational Free Energy (VFE) is a core quantity in varia tional inference and constitutes a tractable
bound on both the log model evidence and the KL divergence bet ween prior and posterior (Beal et al., 2003;
Blei, Kucukelbir, & McAuliffe, 2017; Fox & Roberts, 2012; W a inwright, Jordan, et al., 2008). For an in-depth mo-
tivation of the VFE and its use in variational inference, see Appendix 9.
The VFE, deﬁned at time t, denoted by Ft, is given by,
Ft = DKL [Q(xt|ot; φ)||p(ot, x t)]
= EQ(xt|ot;φ )
[
ln Q(xt|ot; φ)
p(ot, x t)
]
(1)
The agent receives observations ot and must infer the values of hidden states xt. The agent assumes that the environ-
ment evolves according to a Markov process so that the distri bution over states at the current time-step only depends
on the state at the previous time-step, and that the observat ion generated at the current time-step depends only on
the state at the current time-step. Given a distribution ove r a trajectory of states and observations, and under Markov
assumptions it can be factorised as follows: p(o0:T , x 0:T ) = p(s0) ∏ T
t=0 p(ot|st)p(st+1|st). In this paper, we also
consider inference over future states and observations whi ch have yet to be observed. Such future variables are de-
noted oτ or xτ where τ > t . T o avoid dealing with inﬁnite sums, agents only consider fu tures up to some ﬁnite
time horizon, denoted T . Q(xt|ot; φ) denotes an approximate posterior density parametrised by φ which, during the
course of variational inference, is ﬁt as closely as possibl e to the true posterior. Note: there is a slight difference in
notation here compared to that usually used in variational i nference. Normally the approximate posterior is written as
Q(xt; φ) without the dependence on o made explicit. This is because the variational posterior is not a direct function
of observations, but rather the result of an optimization pr ocess which depends on the observations.Here, we make the
dependence on o explicit to keep a clear distinction between the variationa l posterior Q(xt|ot; φ), obtained through
optimization of the variational parameters φ, and the variational prior Q(xt) = Ep(st|st− 1)[Q(st− 1|ot− 1; φ)], obtained
by mapping the previous posterior through the transition dy namics. Throughout this paper, we assume that inference
is occurring in a discrete-time Partially-Observed Markov Decision Process (POMDP). This is to ensure compatibility
with the EFE formulation later on, which is also situated wit hin discrete-time POMDPs. 1
The utility of the VFE for inference comes from the fact that t he VFE is equal to the divergence between true and
approximate posteriors up to a constant: Ft ≥ DKL [Q(xt|ot; φ)||p(xt|ot)]. Thus, minimizing Ft with respect to the
parameters of the variational distribution makes Q(xt; φ) a good approximation of the true posterior.
1 It is important to note that the original FEP was formulated i n continuous time with generalised coordinates (K. Friston , 2008;
K. Friston et al., 2006) (where the hidden states are augment ed with their temporal derivatives up to theoretically inﬁn ite order). The
generalised coordinates mean that the agent is effectively performing variational inference over a T aylor-expanded f uture trajectory
instead of a temporally-instant hidden state (K. J. Friston , 2008; K. J. Friston et al., 2008). Action is derived by minim izing the
gradients of the instantaneous VFE with respect to action, w hich requires the use of a forward model. More recent work on a ctive
inference and the FEP returns to the continuous-time formul ation (K. Friston, 2019; Parr et al., 2020) and the conclusio ns drawn in
this paper may look different in the continuous-time domain .
3
A PREPRINT - S EPTEMBE R 30, 2020
One can also motivate the VFE as a technique to estimate model evidence. Log model evidence is a key quantity in
Bayesian inference but is often intractable, meaning it can not be computed directly. Intuitively, the log model eviden ce
scores the likelihood of the data under a model, and thus prov ides a direct measure of the quality of a model. Under the
free energy principle, minimizing the negative log model ev idence (or surprisal) is the ultimate goal of self-organising
systems (K. Friston & Ao, 2012a, 2012b; K. Friston et al., 200 6). The VFE provides an upper bound on the log model
evidence. This can be shown by importance sampling the model evidence with respect to the approximate posterior,
and applying Jensen’s inequality:
− ln p(ot) = − ln
∫
dxt p(ot, x t)
= − ln
∫
dxt p(ot, x t)Q(xt|ot; φ)
Q(xt|ot; φ)
≤ −
∫
dxt Q(xt|ot; φ) ln p(ot, x t)
Q(xt|ot; φ)
≤ DKL [Q(xt|ot; φ)||p(ot, x t)]
≤ Ft
Since the VFE is an upper bound on the log model evidence (or su rprisal), as the VFE is minimized, it becomes an
increasingly accurate estimate of the surprisal. T o get a fe el for the properties of the VFE, we showcase the following
decomposition:
F = DKL [Q(xt|ot; φ)||p(ot, x t)]
= EQ(xt|ot;φ )
[
ln Q(xt|ot; φ)
p(ot, x t)
]
= −EQ(xt|ot;φ )[ln p(ot|xt)]  
Accuracy
+ DKL [Q(xt|ot; φ)||p(xt)]  
Complexity
(2)
This decomposition is the one typically used to compute the V FE in practice and has a straightforward interpretation.
Speciﬁcally, minimizing the negative accuracy (and thus ma ximizing accuracy) ensures that the observations are as
likely as possible under the states, xt, predicted by the variational posterior while simultaneou sly minimizing the
complexity term, which is a KL divergence between the variat ional posterior and the prior. Thus the goal is to keep
the posterior as close to the prior as possible while still ma ximizing accuracy. Effectively, the complexity term acts a s
an implicit regulariser, reducing the risk of overﬁtting to any speciﬁc observation.
3 The Expected Free Energy
While variational inference as presented above only allows us to perform inference at the current time given observa-
tions, it is possible to extend the formalism to allow for inf erence over actions or policies in the future.
T o achieve this extension, a variational objective is requi red which can be minimized contingent upon future states
and policies, which will allow the problem of adaptive actio n selection to be reformulated as a process of variational
inference. T o do this, the formalism must be extended in two w ays. First, the generative model is augmented to
include actions aτ , and policies, which are sequences of actions π = [ a1, a 2...a T ]. The action taken at the current
time can affect future states, and thus future observations . In order to transform action selection into an inference
problem, policies are treated as an inferred distribution Q(π) which is optimised to meet the agents goals. The second
extension required is to translate the notion of an agent’s g oals into this probabilistic framework. Active inference
encodes an agent’s goals as a desired distribution over obse rvations ˜p(oτ :T ). W e denote the biased distribution using a
tilde over the probability density ˜p rather than the random variable to make clear that the random variables themselves
4
A PREPRINT - S EPTEMBE R 30, 2020
are unchanged, it is only the agent’s subjective distributi on over the variables that is biased. 2 This distribution
is then incorporated into a biased generative model of the wo rld ˜p(oτ , x τ ) ≈ ˜p(oτ )Q(xτ |oτ ) 3 , where we have
additionally made the assumption that the true posterior ca n be well approximated with the variational posterior:
p(xτ |oτ ) ≈ Q(xτ |oτ ) which simply states that the variational inference procedu re was successful 4 . Active inference
proceeds by inferring a variational policy distribution Q(π) that maximizes the evidence for this biased generative
model. Intuitively, this approach turns the action selecti on problem on its head. Instead of saying: I have some goal,
what do I have to do to achieve it? the active inference agent a sks: Given that my goals were achieved, what would
have been the most probable actions that I took?
A further complication of extending VFE into the future come s from the future observations. While agents have access
to current observations (or data) for planning problems, th ey must also reason about unknown future observations.
This is dealt with by taking the expectation of the objective with respect to predicted observations oτ drawn from the
generative model.
In the active inference framework, the goal is to infer a vari ational distribution over both hidden states and policies
that maximally ﬁt to a biased generative model of the future. The framework deﬁnes the variational objective function
to be minimized, the Expected Free Energy , from time τ until the time horizon T , which is denoted G:
G = EQ(oτ :T ,x τ :T ,π )[ln Q(xτ :T , π ) − ln ˜p(oτ :T , x τ :T )]
A temporal mean-ﬁeld factorisation of the approximate post erior and of the generative model is assumed such that
Q(xτ :T , π ) ≈ Q(π) ∏ T
τ Q(xτ ) and ˜p(oτ :T , x τ :T ) ≈ ∏ T
t ˜p(oτ )Q(xτ |oτ ). This factorisation neatly severs the temporal
dependencies between time-steps. Given these assumptions , inferring the optimal Q(π), turns out to be relatively
straightforward.
G = EQ(oτ :T ,x τ :T ,π )
[
ln Q(xτ :T , π ) − ln ˜p(oτ :T , x τ :T )
]
= EQ(oτ :T ,x τ :T |π )Q(π )
[
ln Q(xτ :T |π) + ln Q(π) − ln ˜p(oτ :T , x τ :T )
]
= EQ(π )[ln Q(π) − EQ(oτ :T ,x τ :T |π )
[ T∑
t
[ln Q(xτ ) − ln ˜p(oτ , x τ )]
]
= DKL
[
Q(π)∥e− ∑ T
t Gτ (π )]
Where Gτ (π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )] is deﬁned to be the EFE for a single time–step τ. From the
KL-divergence above, it follows that the optimal variation al policy distribution Q∗ (π) is simply the path integral into
the future of the expected free energies for each individual time-step:
Q∗ (π) = σ(
T∑
t
Gτ (π)),
where σ(x) is a softmax function. This implies that to infer the optimal policy distribution it sufﬁces to minimize the
sum of expected free energies for each time step into the futu re. Inference proceeds by using the generative model
2 It is important to note that this encoding of preferences thr ough a biased generative model is unique to active in-
ference. Other variational control schemes (Levine, 2018; K. Rawlik, T oussaint, & Vijayakumar, 2013; K. C. Rawlik, 201 3;
E. Theodorou, Buchli, & Schaal, 2010; E. A. Theodorou & T odor ov, 2012) instead encode desires through binary optimality vari-
ables and optimize the posterior given that the optimal path was taken. The relation between these frameworks is explore d further
in ?.
3 Some more recent work (Da Costa et al., 2020; K. Friston, 2019 ) prefers an alternative factorisation of the biased genera tive
model in terms of an unbiased likelihood and a biased prior st ate distribution ˜p(oτ , xτ ) = p(oτ |xτ )˜p(xτ ). This leads to a dif-
ferent decomposition of the EFE in terms of risk and ambiguit y (see Appendix 10) but which is mathematically equivalent t o the
factorisation described here.
4 For additional information on the effect of this assumption , see appendix 12.
5
A PREPRINT - S EPTEMBE R 30, 2020
to rollout predicted futures, computing the EFE of those fut ures, and then selecting policies which minimize the sum
of the expected free energies. Since under temporal mean ﬁel d assumptions, trajectories decompose into a sum of
time-steps, it is sufﬁcient for the rest of the paper to only c onsider a single time-step τ.
T o gain an intuition for the EFE, we showcase the following de composition:
Gτ (π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
≈ EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ ) − ln Q(xτ |oτ )]
≈ − EQ(oτ ,x τ |π )
[
ln ˜p(oτ )
]
  
Extrinsic V alue
− EQ(oτ )DKL [Q(xτ |oτ )||Q(xτ |π)]  
Epistemic V alue
(3)
While the EFE admits many decompositions, see Appendix 10 fo r a comprehensive overview , the one presented
in Equation 3 is perhaps the the most important because it sep arates the EFE into an extrinsic, goal-directed term
(sometimes also called ‘instrumental value’ in the literat ure) and an intrinsic, information-seeking term 5 . The ﬁrst
term requires agents to maximize the likelihood of the desir ed observations ˜p(oτ ) under beliefs about the future. It
thus directs an agent to act to maximize the probability of it s desires occurring in the future. It is called the extrinsic
value term since it is the term in the EFE which accounts for th e agent’s preferences.
The second term in equation 3 is the expected information gai n, which is often termed the ‘epistemic value’ since
it quantiﬁes the amount of information gained by visiting a s peciﬁc state. Since the information gain is negative,
minimizing the EFE as a whole mandates maximizing the expected information gain. This drives the agent to maxi -
mize the divergence between its posterior and prior beliefs , thus inducing the agent to take actions which maximally
inform their beliefs and reduce uncertainty. It is the combi nation of extrinsic and intrinsic value terms which belies
active inference’s claim to have a principled approach to th e exploration-exploitation dilemma (K. Friston et al., 201 7a,
2015).
The idea of maximizing expected information gain or "Bayesi an surprise" (Itti & Baldi, 2009) to drive exploratory
behaviour has been argued for in neuroscience (Baldi & Itti, 2010; Ostwald et al., 2012), and has been regularly pro-
posed in reinforcement learning (Houthooft et al., 2016; St ill & Precup, 2012; Sun, Gomez, & Schmidhuber, 2011;
Tschantz, Millidge, Seth, & Buckley, 2020). It is important to note however that in these prior works, information
gain has often been proposed as an ad-hoc addition to an alrea dy existing objective function with only the intuitive
justiﬁcation of boosting exploration. In contrast, expect ed information gain falls naturally out of the EFE formalism ,
arguably lending the formalism a degree of theoretical eleg ance.
4 Origins of the EFE
Given the centrality of the EFE to the active inference frame work, it is important to explore the origin and nature
of this quantity. The EFE is typically motivated through a reductio ad absurdum argument (K. Friston et al., 2015;
Parr & Friston, 2019) 6 . The logic is as follows. Agents have prior beliefs over poli cies that drive action selection. By
the FEP , all states of an organism, including those determin ing policies, must change so as to minimize free energy.
Thus, the only self-consistent prior belief over policies i s that the agent will minimize free-energy into the future
through its policy selection process. If the agent did not ha ve such a prior belief then it would select policies which
5 The approximation in the ﬁnal line of equation (3) is that we a ssume that the true and approximate posteriors are the same
Q(xτ |oτ ) ≈ p(xτ |oτ ). Without this assumption, you obtain an additional KL diver gence between the true and approximate
posterior, which exactly quantiﬁes the discrepancy betwee n them (see appendix section 10 and 12 for more detail).
6 An alternative motivation exists which situates the expect ed free energy in terms of a non-equilibrium steady state dis tribution
(Da Costa et al., 2020; K. Friston, 2019; Parr, 2019). This ar gument reframes everything in terms of a Gibbs free-energy , from
which the EFE can be derived as a special case. The problem bec omes, then, one of the motivation of the Gibbs free-energy as an
objective function.
6
A PREPRINT - S EPTEMBE R 30, 2020
did not minimize the free-energy into the future and would th us not be a free-energy minimizing agent. This logic
requires a well-deﬁned notion of the free-energy of future s tates and observations given a speciﬁc policy. The active
inference literature implicitly assumes that the EFE is the natural functional which ﬁts this notion (K. Friston et al.,
2017b, 2015). In the following section, we argue that the EFE is not in fact the only functional which can quantify the
notion of the free energy of policy-conditioned futures, an d indeed we propose a different functional The Free Energy
of the Future , which we argue is a more natural extension of the VFE to accou nt for future states.
4.1 The Free Energy of the Future
W e argue that the natural extension of the free energy into th e future must possess direct analogs to the two crucial
properties of the VFE: it must be expressible as a KL-diverge nce between a posterior and a generative model, such that
minimizing it causes the variational density to better appr oximate the true posterior. Secondly, it must also bound the
log model evidence of future observations. Bounding the log model evidence (or surprisal) is vital since the surprisal
is the core quantity which, under the FEP , all systems are dri ven to minimize. If the VFE extended into the future
failed to bound the surprisal, then minimizing this extensi on would not necessarily minimize surprisal, and thus any
agent which minimized such an extension would be in violatio n of the FEP . Here, we present a functional which we
claim satisﬁes these desiderata – the Free Energy of the Future (FEF).
W e wish to derive an expression for variational free energy a t some future time τ that is conditioned on some policy π.
In other words, we wish to quantify the free energy that will o ccur at some future time point, given some sequence of
actions. Here, we derive a form of the ‘variational free ener gy of the future’, denoted FEFτ (π), by keeping the same
terms as the VFE (Equation 1), but conditioning the variatio nal distributions on our policy of interest and rewriting fo r
the future time-point τ. Additionally, since observations in the future are unknow n, we must evaluate our free energy
under the expectation of our beliefs about future observati ons, as in the EFE. W e thus deﬁne:
FEFτ (π) = EQ(oτ ,x τ |π )[ln Q(xτ |oτ ) − ln ˜p(oτ , x τ )]
Since this equation is simply the KL-divergence between the variational posterior and the generative model, it satisﬁe s
the ﬁrst desideratum. W e next investigate the properties of the FEF by showcasing one key decomposition. As with
the the VFE, we can then split the FEF into an energy and an entr opy or an accuracy and complexity term, which
correspond to the extrinsic and epistemic action terms in th e EFE:
FEFτ (π) = EQ(oτ |π )DKL [Q(xτ |oτ )||˜p(oτ , x τ )]
≈ − EQ(oτ ,x τ |π )
[
ln ˜p(oτ |xτ )
]
  
Accuracy
+ EQ(oτ |π )DKL [Q(xτ |oτ )||Q(xτ |π)]  
Complexity
Unlike the EFE however, the expected information gain (comp lexity) term is positive while in the EFE term it is
negative. Since the objective function, whether EFE or FEF , is to be minimized, we see that using the FEF mandates us
to minimize the information gain while the EFE requires us to maximize it (or minimize the negative information gain).
An FEF agent thus tries to maximize its reward while trying to explore as little as possible. While this sounds surprising ,
it is in fact directly analogous to the complexity term in the VFE, which mandates maximizing the likelihood of an
observation, while also keeping the posterior as close as po ssible to the prior 7 .
4.2 Bounds on the Expected Model Evidence
W e next show how the FEF can be derived as a bound on the expecte d model evidence satisfying the second desidara-
tum. W e deﬁne the expected model evidence to be a straightfor ward extension of the model-evidence to unknown
7 An objective functional equivalent to the FEF – the "Predict ed Free Energy" – has also been proposed in
(Schwöbel, Kiebel, & Markovi ´c, 2018). See section 14 of the appendix for more details.
7
A PREPRINT - S EPTEMBE R 30, 2020
future states. The expected negative log model evidence for a trajectory from the current time-step t to some time
horizon T is:
−EQ(ot:T |π )
[
ln ˜p(ot:T )
]
This objective states that we wish to maximize the probabili ty (minimize the negative probability) of being in a desired
trajectory ˜p(ot:T ), expected under the distribution of our beliefs about our li kely future trajectories Q(ot:T |π) under a
speciﬁc policy π. Given a Markov generative model p(o1:T , x 1:T |π) = ∏ T
t p(ot|xt)p(xt|xt− 1|π), and assuming that
the approximate posterior factorises Q(x1:T |o1:T ) = ∏ T
t Q(xt|ot), the expected model evidence factorises across
time-steps, it sufﬁces to show the derivation for a single ti me-step τ > t (see Appendix 11 for a full trajectory
derivation). W e further deﬁne Q(oτ , x τ |π) = Q(oτ |π)Q(xτ |oτ ) = p(oτ |xτ )Q(xτ |π). W e therefore take the expected
model evidence for a single time-step, and show that the FEF i s a bound on this quantity.
−EQ(oτ |π )
[
ln ˜p(oτ )
]
= −EQ(oτ |π )
[
ln
∫
dxτ ˜p(oτ , x τ )
]
(4)
= −EQ(oτ |π )
[
ln
∫
dxτ ˜p(oτ , x τ )Q(xτ |oτ )
Q(xτ |oτ )
]
≤ − EQ(oτ |π )
∫
dxτ Q(xτ |oτ )
[
ln ˜p(oτ , x τ )
Q(xτ |oτ )
]
≤ − EQ(oτ ,x τ |π )
[
ln ˜p(oτ , x τ )
Q(xτ |oτ )
]
≤ EQ(oτ ,x τ |π )
[
ln Q(xτ |oτ )
˜p(oτ , x τ )
]
≤ EQ(oτ |π )DKL [Q(x|oτ )||˜p(oτ , x τ |π)] = FEF(π)
Crucially, this is an upper bound on expected model evidence which can be tightened by minimiz ing the FEF . By
contrast, returning to the EFE, we see below that since KL div ergences are always ≥ 0, the expected information gain
is always positive, and so the EFE is a lower bound on the expec ted model evidence:
Gτ (π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
≈ − EQ(oτ ,x τ |π )
[
ln ˜p(oτ )
]
  
Negative Expected Log Model Evidence
− EQ(oτ |π )DKL [Q(xτ |oτ )||Q(xτ |π)]  
Expected Information Gain
Since the expected information gain is an expected KL diverg ence, it must be ≥ 0, and thus the negative expected
information gain must be ≤ 0. Since the EFE aims to minimize negative information gain (t hus maximizing positive
information gain), we can see minimizing the EFE actually dr ives it further from the expected model evidence. 8
W e further investigate the EFE and its properties as a bound i n Appendix 12. Additionally, in Appendix 13 we
review other attempts in the literature to derive the EFE as a bound on the expected model evidence and discuss their
shortcomings.
4.3 The EFE and the FEF
T o get a stronger intuition for the subtle differences betwe en the EFE and the FEF , we present a detailed side-by-side
comparison of the two functionals.
FEF = EQ(oτ ,x τ |π )[ln Q(xτ |oτ ) − ln ˜p(oτ , x τ )]
EFE = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
8 There is a slight additional subtlety here involving the fac t that there is also a posterior approximation error term whi ch is
positive. In general the EFE functions as an upper bound when the posterior error is greater than the information gain and a lower
bound when the posterior error is smaller. Since the goal of v ariational inference is to minimize posterior error, and EF E agents are
driven to maximize expected information gain, we expect thi s latter condition to occur rarely . For more detail see Appen dix 12.
8
A PREPRINT - S EPTEMBE R 30, 2020
While the two formulations might initially look very simila r, the key difference is the variational term. The FEF ,
analogously to the VFE, measures the difference between a va riational posterior Q(xτ |oτ ) and the generative model
Q(xτ |π). The EFE, on the other hand, measures the difference between a variational prior and the generative model.
It is this difference which makes the EFE not a straightforwa rd extension to the VFE for future time-steps, and under-
writes its unique epistemic value term.
W e now demonstrate that both the EFE and the FEF can be decompo sed into an expected likelihood, associated with
extrinsic value, and an expected KL-divergence between a va riational posterior and a variational prior, associated wi th
epistemic value. W e factorise the generative model in the FE F into the (biased) likelihood and a variational prior, and
factorise the generative model in the EFE into an approximat e posterior, and a (biased) marginal:
FEF = EQ(oτ ,x τ |π )[ln Q(xτ |oτ ) − ln ˜p(oτ |xτ ) − ln Q(xτ |π)]
EFE = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ ) − ln Q(xτ |oτ )]
The variational prior and variational posterior can then be combined in both the FEF and the EFE to form epistemic
terms. Crucially, the epistemic value term is positive in th e FEF and negative in the EFE, meaning that the FEF
penalizes epistemic behavior whereas the EFE promotes it:
FEF = − EQ(oτ ,x τ |π )
[
ln ˜p(oτ |xτ )
]
  
Extrinsic V alue
+ EQ(oτ |π )DKL [Q(xτ |oτ )||Q(xτ |π)]  
Epistemic V alue
(5)
EFE = −EQ(oτ ,x τ |π )
[
ln ˜p(oτ )
]
  
Extrinsic V alue
− EQ(oτ |π )DKL [Q(xτ |oτ )||Q(xτ |π)]  
Epistemic V alue
Equation 5. demonstrates that the FEF and EFE can be decompos ed in similar fashion. W e note that the extrinsic
value term for the FEF is a likelihood and a marginal for the EF E. The most important difference, however, lies in the
sign of the epistemic value term. Since optimizing either th e FEF or the EFE requires their minimization, minimizing
the FEF mandates us to minimize information gain while the EF E requires us to maximize it. An FEF agent thus tries
to maximize its extrinsic value while trying to explore as li ttle as possible. A key question then arises: where does the
negative information gain in the EFE come from?
While this difference in the sign of the expected informatio n gain term may speak to some deep connection between
the two quantities, here we offer a pragmatic perspective on the matter. W e show that a possible route to the EFE is
simply that it is the FEF minus the expected information-gai n. This implies that the epistemic value term of the EFE
arises not from some connection to variational inference bu t is present by construction:
FEFτ (π) − IGτ = EQ(oτ ,x τ |π ) ln(Q(xτ |oτ )
˜p(oτ , x τ ) ) − EQ(oτ ,x τ |π ) ln(Q(xτ |oτ )
Q(xτ |π) )
= EQ(oτ ,x τ |π ) ln( Q(xτ |oτ )Q(xτ |π)
˜p(oτ , x τ )Q(xτ |oτ ))
= EQ(oτ ,x τ |π ) ln( Q(xτ |π)
˜p(oτ , x τ ))
= EFE(π)τ
While this proof illustrates the relation between the EFE an d the FEF , it is theoretically unsatisfying as an account of
the origin of the EFE. A large part of the appeal of the EFE is th at it purports to show that epistemic value arises ‘natu-
rally’ out of minimizing free-energy into the future. In con trast, here we have shown that minimizing free-energy into
the future requires no commitment to exploratory behaviour . While this does not question the usefulness of using an
9
A PREPRINT - S EPTEMBE R 30, 2020
information gain term for exploration, or the use of the EFE a s a loss function, it does raise questions about the math-
ematically principled nature of the objective. It is thus no t straightforward to see why agents are directly mandated by
the FEP to minimize the EFE speciﬁcally, as opposed to some ot her free-energy functional. While this fact may at ﬁrst
appear concerning, we believe it ultimately enhances the po wer of the formalism by licensing the extension of active in-
ference to encompass other objective functions in a princip led manner (Biehl, Guckelsberger, Salge, Smith, & Polani,
2018). In the following section, we propose an alternative o bjective to the EFE, which results in the same information-
seeking epistemic value term, but derives it in a mathematic ally principled and intuitive way as a bound on the diver-
gence between expected and desired futures.
5 Free Energy of the Expected Future
In this section, we propose a novel objective functional whi ch we call The Free-Energy of The Expected Future
(FEEF) which possesses the same epistemic value term as the E FE, while additionally possessing a more naturalistic
and intuitive grounding. W e begin with the intuition that, t o act adaptively, agents should act so as to minimize the
difference between what they predict will happen, and what t hey desire to happen. Put another way, adaptive action
for an agent consists of forcing reality to unfold according to its’ preferences. W e can mathematically formulate this
objective as the KL divergence between the agent’s veridici al generative model of what is likely to happen, and a
biased generative model of what it desires to happen.
π∗ = argmin
π
DKL [Q(ot:T , x t:T |π)||˜p(ot:T , x t:T )]
The FEEF can be interpreted as the divergence between a verid icial and a biased generative model, and thus furnishes
a direct intuition of the goals of a FEEF-minimizing agent. T he divergence objective compels the agent to bring the
biased and the veridicial generative model into alignment. Since the predictions of the biased generative model are
heavily biased towards the agent’s a-priori preferences, t he only way to achieve this alignment is to act so as to make
the veridicial generative model predict desired outcomes i n line with the biased generative model. The FEEF objective
encompasses the standard active inference intuition of an a gent acting through biased inference to maximize accuracy
of a biased model. However, the maintenance of two separate g enerative models (one biased and one veridicial) also
helps ﬁnesse the conceptual difﬁculty of how the agent manag es to make accurate posterior inferences and future
predictions about complex dynamics if all it has access to is a biased generative model. It seems straightforward
that the biased model would also bias these crucial parts of i nference which need to be unimpaired for the scheme
to function at all. However, by keeping both a veridicial gen erative model (the same one used at the present time
and learnt through environmental interactions), and a bias ed generative model (created by systematically biasing a
temporary copy of the veridicial model), we elegantly separ ate the need for both veridicial and biased inferential
components for future prediction 9 .
Similarly to the EFE, the FEEF objective can be decomposed in to an extrinsic and an intrinsic term. W e compare this
directly to the EFE decomposition:
FEEF(π)τ = EQ(oτ ,x τ |π ) ln
[ Q(oτ , x τ |π)
˜p(oτ , x τ )
]
= EQ(xτ |π )DKL
[
Q(oτ |xτ )∥˜p(oτ )
]
  
Extrinsic V alue
− EQ(oτ |π )DKL
[
Q(xτ |oτ )∥Q(xτ |π)
]
  
Intrinsic V alue
EFE = −EQ(oτ ,x τ |π )
[
ln ˜p(oτ )
]
  
Extrinsic V alue
− EQ(oτ |π )DKL [Q(xτ |oτ )||Q(xτ |π)]  
Intrinsic V alue
9 This approach bears a resemblance to that taken in (K. Fristo n, 2019) which separates the evolving dynamical policy-dep endent
density of the agent, and a desired steady state density whic h is policy-invariant. This approach arises from deep therm odynamic
considerations in continuous time, while ours is applicabl e to discrete time reinforcement learning frameworks.
10
A PREPRINT - S EPTEMBE R 30, 2020
The ﬁrst thing to note is that the intrinsic value terms of the FEEF and the EFE are identical, under the assumption
that the variational posterior is approximately correct Q(xτ |oτ ) ≈ p(xτ |oτ ) such that FEEF-minimizing agents will
necessarily show identical epistemic behaviour to EFE-min imizing agents. Unlike the EFE, however, the FEEF also
possesses a strong naturalistic grounding as a bound on a the oretically relevant quantity. The FEEF can maintain both
its information-maximizing imperative and its theoretica l grounding since it is derived from the minimization of a KL
divergence rather than the maximization of a log model evide nce.
The key difference with the EFE lies in the likelihood term. W hile the EFE simply tries to maximize the expected
evidence of the desired observations, the FEEF minimizes th e KL divergence between the likelihood of observations
predicted under the veridicial generative model 10 and the marginal likelihood of observations under the biase d gener-
ative model. This difference is effectively equivalent to a n additional veridicial generative model expected likelih ood
entropy term H[Q(oτ |xτ )] subtracted from the EFE. The extrinsic value term thus encou rages the agent to choose its
actions such that its predictions over states lead to observ ations which are close to its preferred observations, while
also trying move to states whereby the entropy over observat ions is maximized, thus leading the agent to move towards
states where the generative model is not as certain about the likely outcome. In effect, the FEEF possesses another
exploratory term, in addition to the information gain, whic h the EFE lacks.
Another important advantage of the FEEF is that it is mathema tically equivalent to the VFE (with a biased generative
model) in the present time with a current observation. This i s because when we have a real observation, the distribution
over the possible veridicial observations collapses to a de lta distribution, so that the outer expectation has no effec t
as EQ(oτ ,x τ |π ) =
∫
Q(xτ |oτ )Q(oτ |π) =
∫
Q(xτ |oτ )δ(o − ¯o) =
∫
Q(xτ |¯oτ ) when a real observation ¯o is available.
Similarly, the veridicial model can be factorised as Q(oτ , x τ ) = Q(xτ |oτ )Q(oτ ) and when the observation is known
the entropy of the observation marginal Q(oτ |π) is 0, thus resulting in the VFE. Simultaneously, biased like lihood
is equivalent to the veridicial likelihood ˜p(¯oτ |xτ ) = Q(¯oτ |xτ ), assuming that (barring counterfactual reasoning ca-
pability), one cannot usefully desire things to be other tha n how they are at the present moment. This means that
theoretically we can consider an agent to be both inferring a nd planning using the same objective, which is not true of
the EFE. The EFE does not reduce to the VFE when observations a re known, and thus requires a separate objective
function to be minimized for planning compared to inference . Because of this, it is actually possible to argue that FEEF
is mandated by the free-energy principle. On this view there is no distinction between present and future inference and
both follow from minimizing the same objective but under dif ferent informational constraints.
Since the FEEF and the EFE are identical in their intrinsic va lue term, and share deep similarities in their extrinsic
term, we believe that the FEEF can serve as a relatively strai ghtforward "plug-in replacement" for the EFE for many
active inference agents. Moreover, it has a much more straig htforward intuitive basis than the EFE, is arguably a better
continuation of the VFE into the future, and possesses a stro ng naturalistic grounding as a bound on the divergence
between predicted and desired futures.
6 Discussion
W e believe it is valuable at this point to step back from the mo rass of various free-energies and take stock of what
has been achieved. Firstly, we have shown that it is not possi ble to directly derive epistemic value from varia-
tional inference objectives which serve as a bound on model e vidence. However, it is possible to derive epistemic
value terms from divergences between the biased and veridic ial generative models. A deep intuitive understanding
of why this is the case is an interesting avenue for future wor k. The intuition behind the FEEF as a divergence
between desired and expected future observations is also si milar to probabilistic formulations of the reinforcement
learning problem (Attias, 2003; Kappen, 2005; Levine, 2018 ; T oussaint, 2009), which typically try to minimize the
10 The term ‘veridicial’ needs some contextualising. W e simpl y mean that the model is not biased towards the agent’s desire s.
The veridicial generative model is not required to be a perfe ctly accurate map of the agent’s entire world, only of action -relevant
sub-manifolds of the total space (Tschantz, Seth, & Buckley , 2019).
11
A PREPRINT - S EPTEMBE R 30, 2020
divergence between a controlled trajectory, and an optimal trajectory (Kappen, 2007; E. A. Theodorou & T odorov,
2012; Williams, Aldrich, & Theodorou, 2017). These schemes also obtain some degree of (undirected) exploratory
behaviour through their objective functionals which conta in entropy terms and the FEEF can be seen as a way of
extending these schemes to partially-observed environmen ts. Understanding precisely how active inference and the
free-energy principle relate mathematically to such schem es is another fruitful avenue for future work.
It seems intuitive that a Bayes-optimal solution to the expl oration-exploitation dilemma should arise directly out
of the formulation of reward maximization as inference, giv en that sources of uncertainty are correctly quantiﬁed.
However, in this paper, we have shown that merely quantifyin g uncertainty in states and observations through mean-
ﬁeld-factorised time-steps is insufﬁcient to derive such a principled solution to the dilemma, as seen by the explorati on-
discouraging behaviour of the FEF . W e therefore believe tha t to derive Bayes optimal exploration policies in the contex t
of active-learning – such that we have to select actions that give us the most information now to use in the future to
maximize rewards – it is likely to require both modelling mul tiple interconnected time-steps, as well as the mechanics
of learning with parameters and update rules, and correctly quantifying the uncertainties therein. This is beyond the
scope of this paper, but is a very interesting avenue for futu re work.
The comparison of the FEEF and the EFE also raises an interest ing philosophical point about the number and types
of generative models employed in the active-inference form alism. One interpretation of the FEEF is in terms of two
generative models, but other interpretations are possible such as between a single unbiased generative model and a
simple density of desired states and observations. It is als o important to note that due to requiring different objectiv e
functions for inference and planning, the EFE also formulat ion appears to implicitly require two generative models –
the generative model of future states, and the generative mo del of states in the future (K. Friston et al., 2015). While th e
mathematical formalism is relatively straightforward, th e philosophical question of how to translate the mathematic al
objects into ontological objects called ‘generative model s’ is unclear and progress on this front would be useful in
determining the philosophical status, and perhaps even neu ral implementation of active inference.
The implications of our results for studies of active infere nce are varied. Nothing in what we have shown argues
directly against the use of the EFE as a objective for an activ e inference agent. However, we believe we have shown
that the EFE is not the necessarily the only, or even the natur al, objective function to use. W e thus follow (Biehl et al.,
2018) in encouraging experimentation with different objec tive functions for active inference. W e especially believe
that our objective, the FEEF future has promise due its intui tive interpretation, largely equivalent terms to the EFE,
its straightforward use of two generative models rather tha n just a single biased one, and its close connections to
similar probabilistic objectives used in variational rein forcement learning, while also maintaining the crucial epi stemic
properties of the EFE. Moreover, while in this paper we have a rgued for the FEF instead of the EFE as a direct extension
of the VFE into the future, the logical requirements of exact ly which functional (if any) is, in fact, mandated by the
free-energy principle remains open. W e believe that elucid ating the exact constraints which the free-energy principl e
places upon a theory of variational action, and understandi ng more deeply the relations between the various free-
energies, could shed light on deep questions regarding noti ons of Bayes-optimal epistemic action in self-organising
systems.
Finally, it is important to note that although in this paper w e have solely been concerned with the EFE and active
inference in discrete-time POMDPs, the original intuition s and mathematical framework of the free-energy principle
arose out of a continuous time formulation, deeply interwov en with concerns from information theory and statistical
physics (K. Friston, 2019; K. Friston & Ao, 2012b; K. Friston et al., 2006; Parr et al., 2020). As such there may be
deep connections between the EFE, FEF , and log model evidenc e which exist only in the continuous time limit, and
which furnish a mathematically principled origin of episte mic action.
12
A PREPRINT - S EPTEMBE R 30, 2020
7 Conclusion
In this paper, we have examined in detail the nature and origi n of the EFE. W e have shown that it is not a direct analog
of the VFE extended into the future. W e then derived a novel ob jective, the FEF , which we claimed is a more natural
extension and shown that it lacks the beneﬁcial epistemic va lue term of the EFE. W e then proved that this term arises
in the EFE directly as a result of its non-standard deﬁnition since the EFE can be expressed as just the FEF minus
the expected information gain. T aking this into account, we then proposed another objective, the Free Energy of the
Expected Future (FEEF) which attempts to get the best of both worlds by preser ving the desirable information-seeking
properties of the EFE, while also maintaining a mathematica lly principled origin.
8 Acknowledgements
BM is supported by an EPSRC funded PhD Studentship. A T is fund ed by a PhD studentship from the Dr. Mor-
timer and Theresa Sackler Foundation and the School of Engin eering and Informatics at the University of Sussex.
CLB is supported by BBRSC grant number BB/P022197/1. A T is gr ateful to the Dr. Mortimer and Theresa Sackler
Foundation, which supports the Sackler Centre for Consciou sness Science.
References
Attias, H. (2003). Planning by probabilistic inference. In Aistats.
Baldi, P ., & Itti, L. (2010). Of bits and wows: A bayesian theo ry of surprise with applications to attention. Neural
Networks, 23(5), 649–666.
Baltieri, M., & Buckley, C. L. (2017). An active inference im plementation of phototaxis. In Artiﬁcial life conference
proceedings 14 (pp. 36–43).
Baltieri, M., & Buckley, C. L. (2018). A probabilistic inter pretation of pid controllers using active inference. In
International conference on simulation of adaptive behavi or (pp. 15–26).
Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries , P ., & Friston, K. J. (2012). Canonical microcircuits
for predictive coding. Neuron, 76(4), 695–711.
Beal, M. J., et al. (2003). V ariational algorithms for approximate bayesian inferenc e. university of London London.
Biehl, M., Guckelsberger, C., Salge, C., Smith, S. C., & Pola ni, D. (2018). Expanding the active inference landscape:
more intrinsic motivations in the perception-action loop. Frontiers in neurorobotics , 12, 45.
Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). V aria tional inference: A review for statisticians. Journal of
the American statistical Association , 112(518), 859–877.
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and perception:
A mathematical review . Journal of Mathematical Psychology , 81, 55–79.
Burda, Y ., Edwards, H., Pathak, D., Storkey, A., Darrell, T . , & Efros, A. A. (2018). Large-scale study of curiosity-
driven learning. arXiv preprint arXiv:1808.04355 .
Calvo, P ., & Friston, K. (2017). Predicting green: really ra dical (plant) predictive processing. Journal of The Royal
Society Interface , 14(131), 20170096.
Çatal, O., V erbelen, T ., Nauta, J., De Boom, C., & Dhoedt, B. ( 2020). Learning perception and planning with deep
active inference. arXiv preprint arXiv:2001.11841 .
Cullen, M., Davey, B., Friston, K. J., & Moran, R. J. (2018). A ctive inference in openai gym: A paradigm for compu-
tational investigations into psychiatric illness. Biological psychiatry: cognitive neuroscience and neuroi maging,
3(9), 809–818.
Da Costa, L., Parr, T ., Sajid, N., V eselic, S., Neacsu, V ., & F riston, K. (2020). Active inference on discrete state-spac es:
a synthesis. arXiv preprint arXiv:2001.07203 .
Deneve, S. (2005). Bayesian inference in spiking neurons. I n Advances in neural information processing systems (pp.
353–360).
13
A PREPRINT - S EPTEMBE R 30, 2020
Doya, K., Ishii, S., Pouget, A., & Rao, R. P . (2007). Bayesian brain: Probabilistic approaches to neural coding . MIT
press.
FitzGerald, T. H., Schwartenbeck, P ., Moutoussis, M., Dola n, R. J., & Friston, K. (2015). Active inference, evidence
accumulation, and the urn task. Neural computation , 27(2), 306–328.
Fox, C. W ., & Roberts, S. J. (2012). A tutorial on variational bayesian inference. Artiﬁcial intelligence review , 38(2),
85–95.
Friston, K. (2003). Learning and inference in the brain. Neural Networks , 16(9), 1325–1352.
Friston, K. (2005). A theory of cortical responses. Philosophical transactions of the Royal Society B: Biologi cal
sciences, 360(1456), 815–836.
Friston, K. (2008). Hierarchical models in the brain. PLoS computational biology , 4(11).
Friston, K. (2010). The free-energy principle: a uniﬁed bra in theory? Nature reviews neuroscience , 11(2), 127–138.
Friston, K. (2011). What is optimal about motor control? Neuron, 72(3), 488–498.
Friston, K. (2019). A free energy principle for a particular physics. arXiv preprint arXiv:1906.10184 .
Friston, K., & Ao, P . (2012a). Free energy, value, and attrac tors. Computational and mathematical methods in
medicine, 2012.
Friston, K., & Ao, P . (2012b). Free energy, value, and attrac tors. Computational and mathematical methods in
medicine, 2012.
Friston, K., FitzGerald, T ., Rigoli, F ., Schwartenbeck, P . , & Pezzulo, G. (2017a). Active inference: a process theory.
Neural computation , 29(1), 1–49.
Friston, K., FitzGerald, T ., Rigoli, F ., Schwartenbeck, P . , & Pezzulo, G. (2017b). Active inference: a process theory.
Neural computation , 29(1), 1–49.
Friston, K., FitzGerald, T ., Rigoli, F ., Schwartenbeck, P . , Pezzulo, G., et al. (2016). Active inference and learning.
Neuroscience & Biobehavioral Reviews , 68, 862–879.
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-P aris ,
100(1-3), 70–87.
Friston, K., Mattout, J., & Kilner, J. (2011). Action unders tanding and active inference. Biological cybernetics ,
104(1-2), 137–160.
Friston, K., Rigoli, F ., Ognibene, D., Mathys, C., Fitzgera ld, T ., & Pezzulo, G. (2015). Active inference and epistemic
value. Cognitive neuroscience , 6(4), 187–214.
Friston, K. J. (2008). V ariational ﬁltering. NeuroImage, 41(3), 747–766.
Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinfo rcement learning or active inference? PloS one , 4(7).
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active inference, curiosity
and insight. Neural computation , 29(10), 2633–2683.
Friston, K. J., Parr, T ., & de Vries, B. (2017). The graphical brain: belief propagation and active inference. Network
Neuroscience, 1(4), 381–414.
Friston, K. J., Rosch, R., Parr, T ., Price, C., & Bowman, H. (2 018). Deep temporal models and active inference.
Neuroscience & Biobehavioral Reviews , 90, 486–501.
Friston, K. J., Trujillo-Barreto, N., & Daunizeau, J. (2008 ). Dem: a variational treatment of dynamic systems.
Neuroimage, 41(3), 849–885.
Houthooft, R., Chen, X., Duan, Y ., Schulman, J., De Turck, F . , & Abbeel, P . (2016). V ariational information maximiz-
ing exploration. Advances in Neural Information Processing Systems (NIPS) .
Itti, L., & Baldi, P . (2009). Bayesian surprise attracts hum an attention. V ision research, 49(10), 1295–1306.
Kanai, R., Komura, Y ., Shipp, S., & Friston, K. (2015). Cereb ral hierarchies: predictive processing, precision and the
pulvinar. Philosophical T ransactions of the Royal Society B: Biologi cal Sciences , 370(1668), 20140169.
Kappen, H. J. (2005). Path integrals and symmetry breaking f or optimal control theory. Journal of statistical mechan-
ics: theory and experiment , 2005(11), P11011.
Kappen, H. J. (2007). An introduction to stochastic control theory, path integrals and reinforcement learning. In Aip
14
A PREPRINT - S EPTEMBE R 30, 2020
conference proceedings (V ol. 887, pp. 149–181).
Knill, D. C., & Pouget, A. (2004). The bayesian brain: the rol e of uncertainty in neural coding and computation.
TRENDS in Neurosciences , 27(12), 712–719.
Levine, S. (2018). Reinforcement learning and control as pr obabilistic inference: Tutorial and review . arXiv preprint
arXiv:1805.00909 .
Millidge, B. (2019a). Combining active inference and hiera rchical predictive coding: A tutorial introduction and cas e
study.
Millidge, B. (2019b). Deep active inference as variational policy gradients. arXiv preprint arXiv:1907.03876 .
Millidge, B. (2019c). Implementing predictive processing and active inference: Preliminary steps and results.
Mirza, M. B., Adams, R. A., Mathys, C. D., & Friston, K. J. (201 6). Scene construction, visual foraging, and active
inference. Frontiers in computational neuroscience , 10, 56.
Mirza, M. B., Adams, R. A., Parr, T ., & Friston, K. (2019). Imp ulsivity and active inference. Journal of cognitive
neuroscience, 31(2), 202–220.
Mohamed, S., & Rezende, D. J. (2015). V ariational informati on maximisation for intrinsically motivated reinforce-
ment learning. In Advances in neural information processing systems (pp. 2125–2133).
Ostwald, D., Spitzer, B., Guggenmos, M., Schmidt, T. T ., Kie bel, S. J., & Blankenburg, F . (2012). Evidence for neural
encoding of bayesian surprise in human somatosensation. NeuroImage, 62(1), 177–188.
Oudeyer, P.-Y ., & Kaplan, F . (2009). What is intrinsic motiv ation? a typology of computational approaches. Frontiers
in neurorobotics , 1, 6.
Parr, T . (2019). The computational neurology of active vision (Unpublished doctoral dissertation). UCL (University
College London).
Parr, T ., Da Costa, L., & Friston, K. (2020). Markov blankets , information geometry and stochastic thermodynamics.
Philosophical T ransactions of the Royal Society A , 378(2164), 20190159.
Parr, T ., & Friston, K. J. (2017a). The active construction o f the visual world. Neuropsychologia, 104, 92–101.
Parr, T ., & Friston, K. J. (2017b). Uncertainty, epistemics and active inference. Journal of The Royal Society Interface ,
14(136), 20170376.
Parr, T ., & Friston, K. J. (2018a). Active inference and the a natomy of oculomotion. Neuropsychologia, 111, 334–343.
Parr, T ., & Friston, K. J. (2018b). The computational anatom y of visual neglect. Cerebral Cortex , 28(2), 777–790.
Parr, T ., & Friston, K. J. (2019). Generalised free energy an d active inference. Biological cybernetics , 113(5-6),
495–513.
Parr, T ., Markovic, D., Kiebel, S. J., & Friston, K. J. (2019) . Neuronal message passing using mean-ﬁeld, bethe, and
marginal approximations. Scientiﬁc reports , 9(1), 1–18.
Pathak, D., Agrawal, P ., Efros, A. A., & Darrell, T . (2017). C uriosity-driven exploration by self-supervised predicti on.
In Proceedings of the ieee conference on computer vision and pa ttern recognition workshops (pp. 16–17).
Pezzulo, G., Cartoni, E., Rigoli, F ., Pio-Lopez, L., & Frist on, K. (2016). Active inference, epistemic value, and
vicarious trial and error. Learning & Memory , 23(7), 322–338.
Rawlik, K., T oussaint, M., & V ijayakumar, S. (2013). On stoc hastic optimal control and reinforcement learning by
approximate inference. In T wenty-third international joint conference on artiﬁcial intelligence.
Rawlik, K. C. (2013). On probabilistic inference approache s to stochastic optimal control.
Schwartenbeck, P ., FitzGerald, T ., Dolan, R., & Friston, K. (2013). Exploration, novelty, surprise, and free energy
minimization. Frontiers in psychology , 4, 710.
Schwartenbeck, P ., Passecker, J., Hauser, T. U., FitzGeral d, T. H., Kronbichler, M., & Friston, K. J. (2019). Compu-
tational mechanisms of curiosity and goal-directed explor ation. Elife, 8, e41703.
Schwöbel, S., Kiebel, S., & Markovi ´c, D. (2018). Active inference, belief propagation, and the bethe approximation.
Neural computation , 30(9), 2530–2567.
Shipp, S. (2016). Neural elements for predictive coding. Frontiers in psychology , 7, 1792.
Spratling, M. W . (2008). Reconciling predictive coding and biased competition models of cortical function. Frontiers
15
A PREPRINT - S EPTEMBE R 30, 2020
in computational neuroscience , 2, 4.
Still, S., & Precup, D. (2012). An information-theoretic ap proach to curiosity-driven reinforcement learning. Theory
in Biosciences , 131(3), 139–148.
Sun, Y ., Gomez, F ., & Schmidhuber, J. (2011). Planning to be s urprised: Optimal bayesian exploration in dynamic
environments. In International conference on artiﬁcial general intelligen ce (pp. 41–51).
Theodorou, E., Buchli, J., & Schaal, S. (2010). A generalize d path integral control approach to reinforcement learning .
journal of machine learning research , 11(Nov), 3137–3181.
Theodorou, E. A., & T odorov, E. (2012). Relative entropy and free energy dualities: Connections to path integral and
kl control. In 2012 ieee 51st ieee conference on decision and control (cdc) (pp. 1466–1473).
T oussaint, M. (2009). Probabilistic inference as a model of planned behavior. KI, 23(3), 23–29.
Tschantz, A., Baltieri, M., Seth, A., Buckley, C. L., et al. ( 2019). Scaling active inference. arXiv preprint
arXiv:1911.10601 .
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (202 0). Reinforcement learning through active inference.
arXiv preprint arXiv:2002.12636 .
Tschantz, A., Seth, A. K., & Buckley, C. L. (2019). Learning a ction-oriented models through active inference. bioRxiv,
764969.
Ueltzhöffer, K. (2018). Deep active inference. Biological cybernetics , 112(6), 547–573.
van de Laar, T. W ., & de Vries, B. (2019). Simulating active in ference processes by message passing. Frontiers in
Robotics and AI , 6(20).
W ainwright, M. J., Jordan, M. I., et al. (2008). Graphical mo dels, exponential families, and variational inference.
F oundations and T rends R⃝ in Machine Learning , 1(1–2), 1–305.
Williams, G., Aldrich, A., & Theodorou, E. A. (2017). Model p redictive path integral control: From theory to parallel
computation. Journal of Guidance, Control, and Dynamics , 40(2), 344–357.
Y edidia, J. S., Freeman, W. T ., & W eiss, Y . (2001). Generaliz ed belief propagation. In Advances in neural information
processing systems (pp. 689–695).
Y edidia, J. S., Freeman, W. T ., & W eiss, Y . (2005). Construct ing free-energy approximations and generalized belief
propagation algorithms. IEEE T ransactions on information theory , 51(7), 2282–2312.
9 V ariational Inference
T o motivate the variational free-energy, and variational i nference more generally, we setup a standard inference prob -
lem. Let us say we are an agent that exists in a partially obser ved world. W e have some observation ot, and from this
we wish to infer the hidden state of the world xt. That is, we want to compute the posterior p(xt|ot). While we do not
know this posterior directly, we do possess a generative mod el of the world. This is a model that maps from hidden
states to observations. Mathematically, we possess p(ot, x t) = p(ot|xt)p(xt). Since computing the true posterior ex-
actly is likely intractable, the strategy in variational in ference is to try to approximate this density with a tractabl e one
Q(xt|ot; φ) which we postulate, and thus have full control over. While th e true posterior might be arbitrarily complex,
we might deﬁne Q(xt|ot; φ) to be a Gaussian distribution: Q(xt|ot; φ) = N (x; µφ , σ φ ), for instance. Given that we
have this variational density q, parametrised by some parameters φ, the goal is to adjust the parameters to make q as
close as possible to the true posterior p(xt|ot). Mathematically speaking, this means we want to minimize:
argminφ DKL [Q(xt|ot; φ)||p(xt|ot)]
Where DKL [Q∥P ] is the Kullback-Leibler divergence. This initially doesn’ t seem to have bought us much. W e wish
to minimize the divergence between the variational density q and the true posterior p(xt|ot). However, by assumption,
we do not know the true posterior. So how can we possibly minim ize this divergence if we do not know one of the parts?
This is where we use the key trick of variational inference. B y Bayes theorem we know that: p(xt|ot) = p(ot|xt)p(xt)
p(ot)
16
A PREPRINT - S EPTEMBE R 30, 2020
we we can thus substitute this into the KL divergence term.
argminφ DKL [Q(xt|ot; φ)||p(xt|ot)] = argmin φ DKL [Q(xt|ot; φ)||p(ot|xt)p(xt)
p(ot) ]
= EQ(xt|ot;φ ) ln(Q(xt|ot; φ)p(ot)
p(ot|xt)p(xt) )
= EQ(xt|ot;φ ) ln( Q(xt|ot; φ)
p(ot|xt)p(xt)) + EQ(xt|ot;φ ) ln p(ot)
= DKL [Q(xt|ot; φ)||p(ot|xt)p(xt)] + ln p(ot) (6)
In step 2 we have applied Bayes theorem the the posterior. In s tep 3 we have simply utilized the deﬁnition of the KL-
divergence DKL [Q||P ] = EQ ln(Q
P ). In step 4 we have then applied the property of logs that ln(a∗b) = ln( a)+ln( b).
In step 5 we then recognise that the remaining ﬁrst term is now a KL divergence between the variational posterior and
the generative model. W e also recognise that since the ln p(ot) term has no dependence on x or φ, the expectation
EQ(xt|ot;φ ) ln p(ot) vanishes leaving just the ln p(ot) term alone. It is important to note that the KL term in equatio n 6
is now between two things we can actually compute – the variat ional posterior, which we control, and the generative
model, which we assume that we know . The remaining ln p(ot) term is called the log model evidence and it is
incomputable in general. However, since it is not affected b y the parameters φ of the variational density, then it does
not affect the minimization and so for the purposes of the min imization process can be ignored. W e can thus write out
what we have deﬁned as
DKL [Q(xt|ot; φ)||p(xt|ot)] = DKL [Q(xt|ot; φ)||p(ot|xt)p(xt)] + ln p(ot)
⇒ DKL [Q(xt|ot; φ)||p(ot|xt)p(xt)] ≥ DKL [Q(xt|ot; φ)||p(xt|ot)]
This implies that the KL divergence between the variational density and the generative model is always greater than or
equal to the KL divergence between the true and variational p osteriors. Since we can compute the ﬁrst KL divergence,
we call it the variational free-energy F . Since it is an upper bound on the divergence between the true posterior
and the variational posterior, which is what we really want t o minimize, then if we minimize F , we are constantly
pushing that bound lower and thus largely minimizing the div ergence between the true and variational posterior. As an
additional bonus, when the true and variational posteriors are approximately equal: DKL [Q(xt|ot; φ)||p(xt|ot)] ≈ 0
then DKL [Q(xt|ot; φ)||p(ot|xt)p(xt)] ≈ − ln p(ot), which means that the ﬁnal value of the variational-free-en ergy is
thus equal to the negative log model evidence. Since the log m odel evidence is a very useful quantity to compute for
Bayesian model selection, it effectively means that once we have ﬁnished ﬁtting our model, we are automatically left
with a measure of how good our model is.
In effect the variational free energy is useful because it ha s two properties. The ﬁrst is that it is an upper bound
on the divergence between the true and approximate posterio r. By adjusting our approximate posterior to minimize
this bound, we drive it closer to the true posterior, thus ach ieving more accurate inference. Secondly, the variational
free-energy is a bound on the log model evidence. This is an im portant term which scores the likelihood of the data
observed given your model and so can be used in Bayesian model selection.
The log model evidence takes on an additional importance in t erms of the free-energy principle, since the negative
log model evidence − ln p(ot) is surprisal, which all agents, it is propsed are driven to mi nimize (K. Friston et al.,
2006). This is because the expected log model evidence is the entropy of observations, the minimisation of which
is postulated as a necessary condition for any self-sustain ing organism to maintain itself as a unique system. The
free-enregy minimization comes about since the VFE is, as we have seen a tractable bound on the log model evidence,
or surprisal.
The VFE can be decomposed in three principle ways, which each showcases a different facet of the objective.
17
A PREPRINT - S EPTEMBE R 30, 2020
F = DKL [Q(xt|ot; φ)||p(ot, x t)]
= EQ(xt|ot;φ )
[
ln Q(xt|ot; φ)
p(ot, x t)
]
= EQ(xt|ot;φ )[ln Q(xt|ot; φ)]  
Entropy
− EQ(xt|ot;φ )[ln p(ot, x t)]  
Energy
= −EQ(xt|ot;φ )[ln p(ot|xt)]  
Accuracy
+ DKL [Q(xt|ot; φ)||p(xt)]  
Complexity
= − ln p(ot)  
Negative Log Model Evidence
+ DKL [Q(xt|ot; φ)||p(xt|ot)]  
Posterior Divergence
In the ﬁrst entropy-energy decomposition, we simply split t he KL divergence using the properties of logarithms so
that the numerator of the fraction becomes the entropy term a nd the denominator becomes the energy term. If
we are seeking to minimize the variational-free-energy the n this means we need to both minimize the negative en-
tropy (since entropy is deﬁned as −EQ(x)
[
ln Q(x)
]
and also minimize the negative energy (or maximize the energ y)
EQ(xt|ot;φ )[ln p(ot, x t)]. This can be interpreted as saying we require that the variat ional posterior be as entropic as
possible while also maximizing the likelihood that the xs proposed as probable by the variational posterior also be
judged as probable under the generative model.
The second decomposition into accuracy and complexity perh aps has a more straightforward interpretation. W e wish
to minimize the negative accuracy (and thus maximize the acc uracy), which means we want the actually observed
observation to be as likely as possible under the xs predicted by the variational posterior. However, we also wa nt to
minimize the complexity term which is a KL divergence betwee n the variational posterior and the prior. That is, we
wish to keep your posterior as close to our prior as possible w hile still maximizing accuracy. The complexity term
then functions as a kind of implicit regulariser, making sur e we do not overﬁt to any speciﬁc observation.
The ﬁnal decomposition speaks the the inferential function s of the VFE. It serves as an upper bound on the log model
evidence, since the posterior divergence term, as a KL diver gence, is always positive. Moreover, we see that by
minimizing the free-energy, we must also be minimizing the p osterior divergence, which is the difference between the
approximate and true posterior, and we are thus improving ou r variational approximation.
10 Decompositions of the EFE
In this section we provide a comprehensive overview of the ma ny decompositions of the EFE. The EFE is deﬁned as:
G(π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
The standard decomposition is into the extrinsic term (expe cted log likelihood of the desired observations) and an
epistemic term (the information gain, or KL divergence betw een variational prior and posterior from the generative
model.
EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )] = EQ(oτ ,x τ |π )[− ln ˜p(oτ ) − ln p(xτ |oτ ) + ln Q(xτ |π)]
= −EQ(oτ ,x τ |π )
[
ln ˜p(oτ )
]

 
Extrinsic V alue
− EQ(oτ |π )
[
DKL [Q(xτ |oτ )||Q(xτ |π)]
]
  
Epistemic V alue
Similar to the VFE, it is also possible to split it into an ener gy and an entropy term. While the energy term is similar
to the VFE as the expectation of the generative model (albeit an expectation over the joint instead of the posterior), the
18
A PREPRINT - S EPTEMBE R 30, 2020
entropy term is different as it is the entropy of the variatio nal prior, not the approximate posterior, which results.
G(π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
= EQ(oτ ,x τ |π )[ln Q(xτ |π)] − EQ(oτ ,x τ |π )[ln ˜p(oτ , x τ )]
= − EQ(oτ |xτ )
[
H
[
Q(xτ |π)
]]
  
Entropy
− EQ(oτ ,x τ |π )[ln ˜p(oτ , x τ )]  
Energy
It is also possible to decompose the biased generative model the other way around, thus in line with that of the VFE to
derive:
G(π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
= EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ |xτ ) − ln p(xτ )]
= −EQ(oτ ,x τ |π )
[
ln ˜p(oτ |xτ )
]
  
Accuracy
+ EQ(oτ |xτ )
[
DKL
[
Q(xτ |π)∥p(xτ )
]]
  
Complexity
Unlike the VFE, however the divergence is between the variat ional prior and the generative prior, rather than between
the variational posterior and the generative prior. Finall y, the EFE can also be represented in observation space by
using Bayes rule to ﬂip the likelihoods and priors.
G(π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
= EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ ) − ln Q(xτ |oτ )]
= EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ ) − ln Q(oτ |xτ ) − ln Q(xτ |π) + ln Q(oτ )]
= EQ(oτ ,x τ |π )[− ln ˜p(oτ ) − ln Q(oτ |xτ ) + ln Q(oτ )]
= EQ(xτ |π )
[
H
[
Q(oτ |xτ )
]]
  
Predicted Uncertainty
− EQ(xτ |oτ )
[
DKL
[
Q(oτ )∥˜p(oτ )
]]
  
Predicted Divergence
= − EQ(xτ |π )
[
ln ˜p(oτ )
]
  
Extrinsic V alue
− EQ(xτ |π )
[
DKL
[
Q(oτ |xτ )∥Q(oτ )
]]
  
(Observation) Information Gain
It is also possible to factorise the biased generative model the other way around in terms of an unbiased likelihood and
biased states: ˜p(oτ , x τ ) = p(oτ |xτ )˜p(xτ ). This different factorisation leads to a new decomposition in terms of risk
and ambiguity, as well as potentially different behaviour d ue to the change from desired observations to desired states
11 .
G(π) = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
= EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln p(oτ |xτ ) − ln ˜p(xτ )]
= EQ(xτ |π )
[
H[p(oτ |xτ )]
]
  
Ambiguity
+ DKL
[
Q(xτ |π)∥˜p(xτ |π)
]
  
Risk
Here the agent is driven to minimize the divergence between d esired and prior expected states, while also trying
to minimize the entropy of the observations it receives. Thi s drives the agent to try to sample observations with a
minimally ambiguous (or maximally precise) mapping back to states.
This formulation is mathematically equivalent to the previ ous decompositions despite deﬁning desired states instead
of desired observations, as can be seen with the following ma nipulations:
G(π) = EQ(xτ |π )
[
H[p(oτ |xτ )]
]
  
Ambiguity
+ DKL
[
Q(xτ )∥˜p(xτ )
]
  
Risk
= EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln p(oτ |xτ ) − ln Q(xτ |oτ ) − ln ˜p(oτ ) + ln p(oτ |xτ )]
= −EQ(oτ ,x τ |π )
[
ln ˜p(oτ )
]
  
Extrinsic V alue
− EQ(oτ |π )
[
DKL [Q(xτ |oτ )||Q(xτ |π)]
]
  
Epistemic V alue
= G(π)
11 For further detail on this factorization see Da Costa et al. ( 2020).
19
A PREPRINT - S EPTEMBE R 30, 2020
The risk-ambiguity formulation has very close relations to KL control (K. Rawlik et al., 2013), in that it encompasses
KL control with an additional "epistemic" ambiguity term.
G(π) = EQ(xτ |π )H[p(oτ |xτ )] + DKL
[
Q(xτ |π)∥˜p(xτ )
]
  
KL Control  
Active Inference
11 T rajectory Derivation of the Expected Model Evidence
Here we present the derivation of the free energy of the futur e (FEF) from the expected model evidence for the full
trajectory distribution rather than a single time-step. Im portantly, we show that with a temporal mean-ﬁeld approxima -
tion on the approximate posterior: p(x1:T |o1:T ) ≈ ∏ T
t p(xt|ot), the assumption that desired rewards are independent
in time: p(ˆr1:T ) ≈ ∏ T
t p(ˆrt), and given a Markovian generative model, then the trajector y distribution factorises into
a sum of individual time-steps 12 , only dependent on the past through the prior term p(xt) = EQ(xt− 1|ot− 1 p(xt|xt− 1).
W e name this ﬁnal approximation the factorization approxim ation, and it simply states that your prior at the current
time-step is based on the posterior of the previous time-ste p mapped through the transition dynamics p(xt|xt− 1).
argminp(π ) − EQ(o1:T |π ) ln ˜p(o1:T )
= −EQ(o1:T |π ) ln
∫
dx1:T ˜p(o1:T , x 1:T )
= −EQ(o1:T |π ) ln
∫
dx1:T
˜p(o1:T , x 1:T )Q(x1:T |o1:T )
Q(x1:T |o1:T )
= −EQ(o1:T |π ) ln
∫
dx1:T
T∏
t
˜p(ot, x t)Q(xt|ot)
Q(xt|ot)
= −EQ(o1:T |π ) ln
∫
dx1:T
T∏
t
˜p(ot|xt)EQ(xt− 1|ot− 1)p(xt|xt− 1)Q(xt|ot)
Q(xt|ot)
= EQ(o1:T |π )
t∑
t
ln
∫
dxt
˜p(ot|xt) − EQ(xt− 1|ot− 1)p(xt|xt− 1)Q(xt|ot)
Q(xt|ot)
≥ −
t∑
t
EQ(o1:T |π )
∫
dxt Q(xt|ot) ln ˜p(ot|xt)EQ(xt− 1|rt− 1)p(xt|xt− 1)
Q(xt|ot)
≥ −
t∑
t
∫
dxt
∫
do1:T Q(o1:T , x t|π) ln ˜p(ot|xt)EQ(xt− 1 |ot− 1)p(xt|xt− 1)
Q(xt|ot)
≥ −
t∑
t
EQ(ot,x t|π ) ln ˜p(ot|xt)EQ(xt− 1|ot− 1)p(xt|xt− 1)
Q(xt|ot)
≥ −
t∑
t
EQ(ot,x t|π ) ln ˜p(ot|xt) −
t∑
t
Ep(ot)DKL [p(xt|ot)||EQ(xt− 1 |ot− 1)p(xt|xt− 1)]
≥ −
t∑
t
FEFt
12 W e assume discrete time so there is a sum over timesteps. W e al so assume continuous states so there is an integral over stat es
x. However, the derivation is identical in the case of discret e states where the integral is simply replaced with a sum.
20
A PREPRINT - S EPTEMBE R 30, 2020
The trajectory derivation of the FEEF follows an almost iden tical scheme to that of the FEF . The only difference is that
now the term inside the log also contains an additional − ln ˜p(o), which is then combined with the likelihood from the
generative model to form the extrinsic-value KL divergence .
12 EFE Bound on the Negative Log Model Evidence
It is important to note that the EFE is also a bound on the negat ive log model evidence, but a lower bound, not an upper
bound. This means that in theory, one should want to maximize the EFE, instead of minimize it, to make the bound as
tight as possible.
It is straightforward to show the bound, since the extrinsic value term of the EFE simply is the log model evidence.
EFE = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
≈ EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln Q(xτ |oτ ) − ln ˜p(oτ )]
≈ − EQ(oτ |π )[ln ˜p(oτ )]
 
Negative Expected Log Model Evidence
− EQ(oτ |π )DKL [Q(xτ |oτ )∥Q(xτ |π)]|  
Information Gain
This derivation assumes that the true and approximate poste riors are approximately equal p(xτ |oτ ) ≈ Q(xτ |oτ ) such
that this is true only after a variational inference procedu re is completed.
W e wish to minimize both log model evidence, and minimize the EFE. Since the information gain term is a KL
divergence, which is always ≥ 0, and we have a negative information gain term, this means tha t the EFE is always less
than the log model evidence and so is a lower bound. However, t his bound becomes tight when the information gain
is 0, so to maximally tighten the bound we wish to reduce the in formation gain, while the EFE demands we maximize
it. In effect, this means that the EFE bound is the wrong way ar ound.
W e can see this more clearly when we retrace the logic for the F EF . From equation 4, we have that the FEF is an upper
bound on the negative log model evidence. This means that minimizi ng the FEF necessarily tightens the bound, while
this is not true of the EFE lower bound , where minimizing the EFE can actually cause it to diverge fr om the log model
evidence. W e can see this even more clearly by doing an analog ous decomposition of the FEF .
FEF = EQ(oτ ,x τ |π )[ln Q(xτ |oτ ) − ln ˜p(oτ , x τ )]
= EQ(oτ ,x τ |π )[ln Q(xτ |oτ ) − ln p(xτ |oτ ) − ln ˜p(oτ )]
= −EQ(oτ |π )[ln ˜p(oτ )]  
Negative Expected Log Model Evidence
+ EQ(oτ |π )DKL [Q(xτ |oτ )∥p(xτ |oτ )]|  
Posterior Approximation Error
Here, since the KL is between the generative model and the app roximate posterior, and then decompose the generative
model into a true posterior and marginal, we can no longer mak e the assumption, made in the EFE derivation, that
the true and approximate posterior are approximately equal , since that would leave us with only the model evidence.
Therefore, instead we get a posterior approximation error t erm which is the KL divergence between the approximate
and true posteriors. When the true and approximate posterio r are equal, we are just left with the log model evidence.
Since, the posterior approximation error is always ≥ 0, then the FEF is an upper bound on the negative log model
evidence, and thus by minimizing the FEF , we make the bound ti ghter. This logic is essentially a reprise of the standard
variational inference logic from a slightly different pers pective.
21
A PREPRINT - S EPTEMBE R 30, 2020
If we do not make the assumption in the EFE that the approximat e and true posterior are the same, we can derive a
similar expression to the EFE which will shed more light on th e relation.
EFE = EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
≈ EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln p(xτ |oτ ) − ln ˜p(oτ )]
≈ EQ(oτ ,x τ |π )[ln Q(xτ |π) − ln p(xτ |oτ ) − ln ˜p(oτ ) + ln Q(xτ |oτ ) − ln Q(xτ |oτ )]
≈ − EQ(oτ |π )[ln ˜p(oτ )]  
Negative Expected Log Model Evidence
+ EQ(oτ |π )DKL [Q(xτ |oτ )∥p(xτ |oτ )]|  
Posterior Approximation Error
  
FEF
− EQ(oτ |π )DKL [Q(xτ |oτ )∥Q(xτ |π)]|  
Information Gain
Without the true posterior assumption, we thus ﬁnd that the E FE could be both an upper or a lower bound on the log
model evidence, since the two additional KL divergence term s have opposite signs. If the posterior approximation error
is larger than the information gain, then the EFE functions c orrectly as an upper bound. However, if the information
gain is larger, then the EFE will become a lower bound and coul d diverge from the log model evidence. Moreover, this
latter situation is more likely, since the goal of variation al inference is to reduce the approximation error, while EFE
agents seek to maximize information gain. This means that th e EFE only functions correctly as an upper bound on log
model evidence during the early stages of optimization wher e the posterior approximation is poor. Further optimizatio n
steps likely drive the EFE further away from the model eviden ce. The bound is tight when the information gain equals
the posterior approximation error. W e can also see that the ﬁ rst two terms of the EFE is simply the FEF , we have thus
rederived by a rather roundabout route, the fact that the EFE is simply the FEF minus the information gain.
W e thus see that the EFE as a bound on the log model evidence is s haky, since it depends on the information gain
always being larger or smaller than the posterior approxima tion error. Moreover, the bounding behaviour seems to
emerge directly from the relation of the EFE to the FEF rather than the intrinsic qualities of the EFE, and it is primarily
the information-seeking properties of the EFE which serve t o damage the clean bounding behaviour of the FEF .
It can be argued that although the mathematical justiﬁcatio n of the EFE as a bound may be shaky, that the additional
information gain term may be beneﬁcial, and the bound may be r ecovered in the long run, since that as a result of
short-term actions to maximize the EFE, the epistemic value itself goes to 0, and thus the EFE exactly approximates
the bound, while also potentially increasing the ultimate e xpected reward achieved. This argument is valid heuristica lly
and is identical to the standard justiﬁcations for ad-hoc in trinsic measures terms in the literature (Oudeyer & Kaplan,
2009) namely that exploration hurts in the short run but help s in the long run. W e do not dispute that argument in this
paper, instead we simply show that the EFE cannot straightfo rwardly be justiﬁed mathematically as being a result of
variational inference into the future, or as a bound on model -evidence. W e do not argue at all against its heuristic use
to encourage exploration of the environment and thus (hopef ully) better performance overall.
13 Attempts at Naturalising the EFE
In this appendix, we review several attempts to derive the EF E directly from the expected model evidence.
Since we have derived the FEF by importance sampling the expe cted model evidence with the approximate posterior,
one obvious avenue would be to importance sample on the varia tional prior instead. Following this line of thought
22
A PREPRINT - S EPTEMBE R 30, 2020
gives us:
−EQ(oτ |π )
[
ln ˜p(oτ )
]
= −EQ(oτ |π )
[
ln
∫
dx˜p(oτ , x τ )
]
= −EQ(oτ |π )[
[
ln
∫
dxτ ˜p(oτ , x τ )Q(xτ |π)
Q(xτ |π)
]
≤ − EQ(oτ |π )
[ ∫
dxτ Q(xτ |π) ln ˜p(oτ , x τ )
Q(xτ |π)
]
≤ − EQ(oτ |π )Q(xτ |π )
[
ln ˜p(oτ , x τ )
Q(xτ |π)
]
≤ − EQ(oτ |π )Q(xτ |π )[ln Q(xτ |π) − ln ˜p(oτ , x τ )]
While this approach gets the correct form of the EFE inside th e expectation, the expectation itself is the product of the
two marginals rather than the joint required for the full EFE . While this may seem minor, this difference must underpin
all the other differences and relations we have explored thr oughout this paper.
T o get to the full EFE we must make some assumption to allow us t o combine the expectation under two marginals
into an expectation under the joint. The ﬁrst and simplest as sumption is that they simply are the same such that the
joint factorises into the two marginals – Q(oτ , x τ |π) ≈ Q(oτ |π)Q(xτ |π). This assumption is equivalent to assuming
independence of observations and latent states, which rath er defeats the point of a latent variable model.
A second approach is to assume that the variational prior equ als the variational posterior Q(xτ |π) ≈ Q(xτ |oτ ). This
allows you then to combine the marginal and posterior into a j oint, giving the EFE as desired. However this assumption
has several unfortunate consequences. Firstly, it elimina tes the entire idea of inference, since the prior and posteri or
are assumed to be the same, thus no real inference can have tak en place. This is not necessarily an issue if we separate
the inference and planning stages of the algorithm, such tha t they optimize different objective functions, however it i s
more elegant, as the FEEF does, is that it enables the optimiz ation of the same objective function for both inference
and planning, thus casting them as simply different facets o f the same underlying process. Moreover, a more serious
issue is that this assumption also eliminates the informati on gain term in active inference – since the prior and posteri or
are the same, the divergence between them (which is the infor mation gain), must be zero.
A slightly different approach is taken in a proof in (Parr, 20 19), which begins with the KL divergence between two
distributions, one encoding beliefs about future states an d observations, and the other being the biased generative
model. By deﬁnition, this KL divergence is always ≥ 0, which allows us to write.
DKL [p(oτ , x τ |π)∥˜p(oτ , x τ )] ≥ 0
= Ep(oτ |π )DKL [p(xτ |oτ )∥˜p(oτ , x τ )] − Ep(oτ |π )[ln p(oτ |π)] ≥ 0
⇒ − Ep(oτ |π )DKL [p(xτ |oτ )∥˜p(oτ , x τ )] ≥ − Ep(oτ |π )[ln p(oτ |π)]
⇒ FEF ≥ − Ep(oτ |π )[ln p(oτ |π)]
Under the assumption that p(x|o) ≈ Q(x|π), this becomes:
− Ep(oτ |π )DKL [p(xτ |oτ )∥˜p(oτ , x τ )] ≥ − Ep(oτ |π )[ln p(oτ |π)]
≈ Ep(oτ |π )DKL [Q(xτ |π)∥˜p(oτ , x τ )] ≥ − Ep(oτ |π )[ln p(oτ |π)]
≈ EFE ≥ − Ep(oτ |π )[ln p(oτ |π)]
This proof derives the FEF as a bound on, not the expected mode l evidence by our deﬁnition, but on the entropy of
expected observations given a policy. The EFE is then derive d from the FEF by assuming that the prior and posterior
are the same, which comes with all the drawbacks explained ab ove. This proof is primarily unworkable because
of the assumption that the prior and the posterior are identi cal. While this may be arguable in the continuous time
23
A PREPRINT - S EPTEMBE R 30, 2020
limit, where it is equivalent to the assumption that that dQ(x|o)
dt ≈ 0, which is when the continuous-time inference has
reached an equilibrium, it is deﬁnitely not true in discrete time, where although there is a relation between the prior
in the current time-step and the posterior in the previous on e, it must be mapped through the transition dynamics –
Q(xt|π) = EQ(xt− 1|π )[p(xt|xt− 1, π )].
One can also attempt a related proof by splitting the KL diver gence the other way. This gives you:
DKL [p(oτ , x τ |π)∥˜p(oτ , x τ )] ≥ 0
= DKL [p(oτ , x τ |π)∥˜p(xτ |oτ )] − Ep(oτ |π )[ln ˜p(oτ )] ≥ 0
⇒ − DKL [p(oτ , x τ |π)∥˜p(xτ |oτ )] ≥ − Ep(oτ |π )[ln ˜p(oτ )]
⇒ Ep(xτ |π )[ln p(oτ |xτ )] + Ep(oτ |π )DKL [p(xτ |oτ )∥˜p(xτ |π)] ≥ − Ep(oτ |π )[ln ˜p(oτ )]
⇒ FEF ≥ − Ep(oτ |π )[ln ˜p(oτ )]
Which is just another way of showing that the FEF is a bound on t he expected model evidence.
14 Related Quantites
Recently a new free-energy, the generalised free energy (GF E) (Parr & Friston, 2019), has been proposed in the lit-
erature as an alternative or an extension to the EFE. The GFE s hares some close similarities with the FEEF . Both
fundamentally extend the EFE by proposing a uniﬁed objectiv e function which is valid for both inferencce at the cur-
rent time and planning into the future, whereas the EFE can on ly be used for planning. Moreover, both GFE and FEEF
encode future observations as latent unobserved variables , over which posterior beliefs can be formed. Moreover
agents maintain prior beliefs over these variables which en code its preferences or desires 13 .
The generalised free energy is deﬁned as
GFE = EQ(oτ ,x τ )[ln Q(oτ ) + ln Q(xτ ) − ln ˜p(oτ , x τ )]
Whereas the FEEF is deﬁned as
FEEF = EQ(oτ ,x τ )[ln Q(oτ , x τ ) − ln ˜p(oτ , x τ )]
There are two key differences mathematically and intuitive ly between the GFE and the FEEF . The ﬁrst is that the GFE
maintains a factorised posterior over beliefs and observat ions, where the posterior beliefs of the two are separated by a
mean ﬁeld approximation and assumed to be separate. By contr ast the FEEF maintains a joint approximate belief over
both observations and states simultaneously. This joint in the case of the FEEF effectively functions as a veridicial
generative model since Q(o|x) = p(o|x) and Q(x) = EQ(xt− 1|π )p(xt|xt− 1). This is means that posterior beliefs of
the future are computed simply by rolling forward the genera tive model given the beliefs about the current time.
A second and more important differences lies in the generati ve models. The GFE assumes that the agent is only
equipped with a single generative model with both veridicia l and biased components. The preferences of an EFE
agent are encoded as a separate factorisable marginal over o bservations. This means that the generative model of the
GFE agent factorises as ˜p(o, x )GF E ∝ p(o|x)p(x)˜p(o). This means that for the GFE the likelihood and the prior are
unbiased and there is simply an additional prior preference s term in the free-energy expression. By contrast, the FEEF
eschews this unusual factorisation of the generative model and instead presupposes a separate warped generative model
for use in the future which is intrinsically biased. The FEEF generative model thus decomposes as ˜p(o, x )F EEF =
˜p(o|x)˜p(x), which is the standard factorisation of the joint distribut ion in a generative model, but where both the
likelihood and prior distributions are biased towards gene rating more favourable states of affairs for the agent. This
inherent optimism bias then drives action.
13 T o help make clear the similarity between the GFE and the FEEF , we have deﬁned the veridicial generative model as Q(oτ , xτ )
24
A PREPRINT - S EPTEMBE R 30, 2020
A further free-energy proposed in the literature has been th e Bethe free-energy and the Bethe approximation
(Schwöbel et al., 2018). This approach eschews the standard mean ﬁeld assumption on the approximate posterior
in favour of a Bethe approximation from statistical physics (Y edidia, Freeman, & W eiss, 2001, 2005) which instead
represents the approximate posterior as the product of pair wise marginals, thus preserving a constraint of pairwise te m-
poral consistency which the mean-ﬁeld assumption lacks. Du e to this greater representation of temporal constraints
(the approximate posteriors at each time-step being no long er assumed to be independent), the Bethe free-energy has
the potential to be signiﬁcantly more accurate than the stan dard mean-ﬁeld variational free energy (and is, in fact,
exact for factor graphs without cycles such as the standard n on-hierarchical POMDP model). In this paper, we focus
entirely on the standard mean-ﬁeld variational free-energ y used in the vast majority of active inference publications ,
and thus the Bethe free-energy is out of scope for this paper. However, exploring the nature of any intrinsic terms
which might arise from the Bethe free-energy is an interesti ng avenue for future work. Although primarily focused on
the Bethe free-energy, (Schwöbel et al., 2018) also introdu ced a ‘predicted free energy’ functional. This functional i s
equivalent to the FEF as we have deﬁned it here, and so has a com plexity instead of an information gain term, leading
to minimizing the prior-posterior divergence.
Finally, (Biehl et al., 2018) suggested that if the EFE is not mandated by the free-energy principle, which we have
argued for in this paper, then in theory any standard intrins ic measure, such as empowerment, could be used as an
objective. W e believe that exploring the effect of these oth er potential loss functions could be a area of great interest
for future work.
25