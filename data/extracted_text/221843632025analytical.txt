S3 Appendix. Limits of the EFE
In this section, we derive, under certain assumptions that we justify with our numerical simulations, a limit for the
expected free energy for environments with deterministic transitions, which include the timed response and navigation
tasks. Formally, this means that the world model has been fully trained and the transition function yields perfect
prediction accuracy over sensory states given the correct belief state is known. Equivalently, the variational free energy
is minimized, and the posterior and prior distributions for the transition functions stay essentially identical after
each update, i.e. ∃ϵ ∈ R, such that DKL[qϕ(Bt|bt−1, at−1)||p(Bt|bt−1, at−1)] < ϵ, where ϵ >0 can come arbitrarily
close to 0. Then, considering the expression for the VFE in Eq. (2), we see that the first term can be neglected and
the second term simplifies to −P
bt p(bt|bt−1, at−1) logp(senv
t |bt), where we used the fact that the Kullback-Leibler
divergence cancels out if and only if the two distributions are equal, that is, qϕ(Bt|bt−1, at−1) = p(Bt|bt−1, at−1).
In this limit, a good approximation of the VFE is: F = −P
bt p(bt|bt−1, at−1) logp(senv
t |bt) = 0 for any observation
received from the environment. Furthermore, our assumption of a perfect world model implies that each belief state
b is associated to exactly one hidden state e in the (deterministic) environment. That means that when the agent is
in belief state b, the environment is in hidden state e. In these conditions, we say that b represents e. As a result,
the transition function has the following form:
p(bt|bt−1, at−1) =
(
xb if s(bt) = senv
t and bt represents a hidden state et that emitted senv
t
0 otherwise. (1)
such that xb ∈ [0, 1] and the sum of probabilities for all belief states that represent the same hidden state et sum up
to 1: P
b xb = 1. In principle, the values for the xb can be arbitrary, and depend on the individual models the agents
collapse onto after training. We call children of bt and at the set of all belief states that can be reached with non-zero
probability from bt with action at: ch(bt, at) = {bt+1|p(bt+1|bt, at) > 0}. Similarly, we call e(bt, at) the hidden state
in the environment that results from applying at from bt. e(bt, at) = et+1 designates the fact that starting from belief
state bt and under action at, the environment will transition to et+1 in the next step.
Two asymptotic configurations stood out during the numerical simulations: (1) the agent adopts a distributed
representation of the hidden state, and any belief state participating in it can be sampled with close to uniform
probability (that we idealize to be exactly uniform in the derivation), or (2) reinforcements of individual edges in the
models led to the adoption of a single belief state to fully represent the hidden state while ignoring the other clones
associated with zero probability. A belief state represents at most one hidden state. In order to accommodate the
first possibility, we define the set:
Dbt,et+1 = {bt+1|∃a ∈ A, p(bt+1|bt, a) > 0 and bt+1 represents et+1 only} (2)
the set of all belief states that can be transitioned to and that uniquely represent the hidden state e(bt, at). Further-
more, we require that belief states bt+1, b′
t+1 ∈ Dbt,et+1 are degenerate, that is, ∀a ∈ A, p(bt+1|bt, a) = p(b′
t+1|bt, a).
The size |Dbt,e| of the set a belief state belongs to is called its degeneracy and ∀b ∈ Dbt,e(bt,at), p(b|bt, at) =
1/|Dbt,e(bt,at)|. As a result, the world model becomes:
p(bt+1, st+1|bt, at) = δbt+1∈ch(bt,at)
|Dbt,e(bt,at)| δst+1,s(bt+1), (3)
where as in the main text, δbt+1∈ch(bt,at) equals 1 if bt+1 is a child of bt and at and 0 otherwise, and s(bt+1) is
the sensory state that bt+1 is a clone of, in contrast to st+1, the value that the sensory state can take. Another
important consequence is that if two actions a, a′ ∈ Alead to the same hidden state in the environment, that is,
et+1 = e(bt, a) = e(bt, a′), then the children ofbt and a, and bt and a′, respectively, are the same: ch(bt, a) = ch(bt, a′).
Conversely, ife(bt, a) ̸= e(bt, a′), then the two sets have no belief state in common:ch(bt, a)∩ch(bt, a′) = ∅. Therefore,
the following holds for any two actions a, a′ ∈ A:
∃b ∈ Bsuch that b ∈ ch(bt, a) and b ∈ ch(bt, a′) ⇐⇒ ch(bt, a) = ch(bt, a′) (4)
Using Eq. (3) for the world model, the new expression for the expected free energy (Eq. (4)in the main text) is:
Gbt[at] =
X
bt+1,st+1
δbt+1∈ch(bt,at)
|Dbt,e(bt,at)| δst+1,s(bt+1) log
δst+1,s(bt+1)/|Dbt,e(bt,at)|
pref(bt+1, st+1|bt)

(5)
=
X
bt+1∈ch(bt,at),st+1
1
|Dbt,e(bt,at)|δst+1,s(bt+1) log
 1/|Dbt,e(bt,at)|
pref(bt+1, st+1|bt)

. (6)
1
Exploration phase:During the exploration phase, the preferences of the agents are set as the marginal of their
world model over actions:
pref(bt+1, st+1|bt) =
X
a
p(st+1|bt+1) p(bt+1|bt, a) π(a|bt) (7)
=
X
a
δst+1,s(bt+1)
δbt+1∈ch(bt,a)
|Dbt,e(bt,a)| π(a|bt) (8)
and the EFE is determined by the final policy:
Gbt[at] =
X
bt+1∈ch(bt,at)
1
|Dbt,e(bt,a)| log

 X
a|e(bt,a)=e(bt,at)
1/|Dbt,e(bt,at)|
π(a|bt) · 1/|Dbt,e(bt,a)|

, (9)
where by Eq. (4), the sum over actions in the logarithm is restricted to those whose children include bt+1, or
equivalently, represent the same hidden state and belong to the same set of degenerate states. Therefore, the
degeneracies inside the logarithm cancel each other out, and each term in the sum is the same for each degenerate
belief state:
Gbt[at] = −log

 X
a|e(bt,a)=e(bt,at)
π(a|bt)

. (10)
In order to find the final value the EFE converges to, we must find the fixed point for the policy. This means
that the policy π(n+1) after the n-th update using the expected free energy is equal to the original policy at step n,
π(n) = π:
π(at|bt) = π(n)(at|bt) (11)
= π(n+1)(at|bt) (12)
= softmax(ζGbt[at]) (13)
=
exp

−ζ log
P
a|e(bt,a)=e(bt,at) π(a|bt)

P
a′ exp

−ζ log
P
a′′|e(bt,a′′)=e(bt,a′) π(a′′|bt)
 (14)
=
P
a|e(bt,a)=e(bt,at) π(a|bt)
−ζ
P
a′
P
a′′|e(bt,a′′)=e(bt,a′) π(a′′|bt)
−ζ (15)
where we have used exp( y log x) = (exp(log x))y = xy to get the last line and b′
t+1 in the denominator is a belief
state that can be reached from bt with action a′ ∈ A.
To proceed further, we introduce the following notations:
• Ebt := {ek
t+1 | ∃a ∈ Asuch that e(bt, a) = ek
t+1}, the set of hidden states that can be reached from the belief
state bt;
• Abt,k := {a |e(bt, a) = ek
t+1, a∈ A}, the set of actions that induce a transition to the k-th hidden state ek
t+1 in
Ebt. |Abt,k| is the number of elements in Abt,k. It has at least one element;
• ∀ek
t+1 ∈ Ebt, ∀a ∈ Abt,k, πbt,k = π(a|bt) denotes the probability of taking any action a that will result in the
k-th hidden state ek
t+1 in Ebt. All actions that result in the same hidden state ek
t+1 have the same probability
in the policy, as can be seen in Eq. (15).
We rewrite Eq. (15) with this notations, assuming at ∈ Abt,k:
π(at|bt) = πbt,k (16)
= (|Abt,k|πbt,k)−ζ
P
l |Abt,l| (|Abt,l|πbt,l)−ζ (17)
= |Abt,k|−ζ/(1+ζ)
P
l |Abt,l| (|Abt,l|πbt,l)−ζ
1/(1+ζ) (18)
= N × |Abt,k|−ζ/(1+ζ) (19)
2
where from the second to third line, we used that for x ̸= 0, x= yxα ⇐⇒ x1−α = y , and N is the normalization
factor in the second to last line. N does not depend on the action considered and it normalizes the policy to 1:
X
k
|Abt,k|πbt,k = N ×
X
k
|Abt,k|1/(1+ζ) = 1. (20)
Therefore, we find N = 1/ P
k |Abt,k|1/(1+ζ). At the end of a successful training in a deterministic environment, the
policy converges to:
πbt,k = |Abt,k|−ζ/(1+ζ)
P
l |Abt,l|1/(1+ζ) . (21)
Finally, during the exploration phase, once the world model perfectly represents the environment, the expected free
energy in Eq. (10) related to taking an action at ∈ Abt,k converges to:
Gbt[at] = −log

 X
a′∈Abt,k
|Abt,k|−ζ/(1+ζ)
P
l |Abt,l|1/(1+ζ)

 (22)
= −log
 |Abt,k|1/(1+ζ)
P
l |Abt,l|1/(1+ζ)

. (23)
In Figs 4 and 5, we average this value over actions and previous belief states, since it would otherwise depend on
individual deliberations of the agents.
3