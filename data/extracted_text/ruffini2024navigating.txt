Navigating Complexity: How Resource-Limited
Agents Derive Probability and Generate Emergence
Giulio Ruffini∗
Neuroelectrics
Sept 16, 2024 (v2)
Abstract
In the Kolmogorov Theory of consciousness (KT), an algorithmic agent is an
information-processing system that compresses sensory data into simpler represen-
tations to plan actions that optimize its objective function. Algorithmic agents
operate under limited data access, finite computational resources, and fundamental
limits from the algorithmic information theory (AIT). In this paper, we demon-
strate how these limitations naturally give rise to the principles of probability,
Bayesian inference, and the concept of emergence within this framework. Using
a toy example of an agent compressing data from a large library, we demonstrate
how limitations in data access naturally lead to a multi-model strategy, where
probabilistic reasoning and Occam’s razor emerge as the agent navigates between
different models. This process is naturally extended by allowing it to accommodate
objective functions beyond compression. Next, we propose a formal definition of
emergence from the notions of coarse-graining and Kolmogorov complexity. Due
to its limited resources, the agent must employ coarse-graining to transform data
that initially appears incompressible into a compressible, aggregate form while re-
taining a non-trivial structure. The agent must find patterns and operate at some
coarse-graining level to ensure survival. Coarse-graining may include various forms
of data reorganization, such as spatiotemporal averaging, compressive sensing, and
dimensionality reduction techniques. We discuss the connections to other theo-
retical approaches, such as Jaynes’ robot, the Free Energy Principle, and Active
Inference. By addressing how an ideal agent copes with the inherent limitations
of data access and computational capacity, we provide a unified framework for
understanding both probabilistic reasoning and emergence in algorithmic agents.
∗giulio.ruffini@neuroelectrics.com
1
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
Contents
1 Introduction 3
2 Bayesian inference 4
2.1 Multi-Model Strategy for Compression in Agents . . . . . . . . . . . . . 4
2.2 Occam’s razor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Active Inference and the Free Energy Principle . . . . . . . . . . . . . . . 7
3 Emergence 8
3.1 Formal Definition of Emergence in the agent framework . . . . . . . . . . 9
3.2 Emergence in Cellular Automata . . . . . . . . . . . . . . . . . . . . . . 10
3.3 Emergence and the Renormalization Group . . . . . . . . . . . . . . . . . 11
3.4 Relation to emergence through partial models . . . . . . . . . . . . . . . 11
4 Discussion 13
5 Conclusion 15
2
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
1 Introduction
Understanding how complex behaviors and probabilistic reasoning emerge from simple
rules is a central challenge in both neuroscience and artificial intelligence. Traditional
approaches often rely on probabilistic modeling, but an alternative view is provided by the
Kolmogorov Theory (KT) of consciousness [25, 26, 28]. In this framework, thealgorithmic
agent interacts with its environment by compressing sensory data into simpler forms,
i.e., by constructing models, to guide its actions for increased valence [30, 29]. However,
real agents operate under inherent constraints, including limited data access and finite
computational resources.
Moreover, the uncomputability of Kolmogorov complexity means that agents cannot de-
duce the actual program (model) generating the data they receive. Specifically, a foun-
dational result in algorithmic information theory (AIT) — independently derived by Ray
Solomonoff, Andrey Kolmogorov, and Gregory Chaitin — states that there is no general
algorithm that, given a string x, can compute its exact Kolmogorov complexity K(x).
In other words, there is no computable function that can take x as input and output
K(x) [19]. Thus, the shortest program or minimal model that generates the string x
cannot be found.
This paper aims to demonstrate how these limitations give rise to probability theory and
Bayesian inference, and the phenomenon of emergence [1, 13, 35].
We build on foundational results. When the agent’s computational repertoire is re-
stricted to counting the frequency of string appearances, compression naturally aligns
with Shannon information theory. In this framework, strategies such as Huffman coding
and Lempel-Ziv (LZ) rely on — or arguably give rise to — probability theory [7]. Both
LZ and Huffman can be viewed as computational frameworks with a limited set of opera-
tions, primarily focused on counting symbol frequencies and performing string matching.
While highly effective for specific tasks like data compression, these algorithms lack the
general-purpose computational capabilities of Turing-complete systems.
To generalize this framework, we extend the agent’s capabilities to discover more so-
phisticated coding schemes — specifically, short programs as described by Kolmogorov
complexity. In this broader context, Bayesian inference can also emerge as a natural con-
sequence of searching for the simplest and most efficient models (or programs) to compress
data, especially when agents are constrained by limited computational resources.
In summary, we aim to show that Bayesian inference naturally arises as agents strive
for efficient compression, prediction, and planning, linking with Karl Friston’s Active
Inference [23]. We assume that Agents seek to predict the world efficiently by finding
short programs for data (under the Solomonoff prior) but are limited by
• Computational constraints (limited resources, uncomputability of Kolmogorov com-
plexity).
• Partial access to data ( coarse-grained observations).
• Noisy and incomplete data.
First, we show that an ideal agent, driven by its homeostatic goals, is naturally led to
develop a weighted multi-model strategy with priors to interpret and predict its envi-
3
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
ronment. This, which is essentially Bayesian inference, is important to be able to relate
theories of agenthood based on AIT with others based on Shannon’s information theory
and probability theory, such as the Free Energy Principle and Active Inference [11, 24].
Second, we show that coarse-graining of the world for emergent modeling arises as the
agent navigates the complexity of its sensory inputs with limited computational resources.
Given the compositional nature of natural data, this perspective suggests that agents
process information through hierarchical coarse-graining, in line with current research
findings in the visual [9, 14] and auditory primate systems [3].
2 Bayesian inference
Consider an agent tasked with modeling the world, which, as discussed in Kolmogorov’s
theory of consciousness (KT), is equivalent to being able to compress world data [27,
30]. The agent needs to build a good predictive model of the world for survival, and
compression performance indicates predictive power in the Solomonoff prior [7, 32].
The agent may use simple compression algorithms such as gzip or its more sophisti-
cated descendants, such as Large Language models (LLMs), which can also be used for
compression [8, 33].
The agent consists of three interacting modules [28, 30, 29]: a Modeling Engine, an
Objective Function, and a Planning Engine. The agent’s Modeling Engine’s primary
goal is to find simple models of the world to predict the future and plan accordingly.
The goal of the agent itself is to survive, to achieve “stasis” (a term that refers to both
homeostasis and telehomeostasis, i.e., preservation of self and kind).
Our primary contribution is to show first that a multi-model strategy (a “meta”-model),
where an agent maintains multiple models to balance compression efficiency with robust-
ness to noise and data diversity, leads to better compression as the agent interacts with
the World. Second, this fact, combined with the notion of Solomonoff’s priors, naturally
leads to Bayesian inference and the emergence of probability complemented by Occam’s
razor.
In what follows, we will use a toy example of an agent being fed pages from a large library,
such as the Library of Congress. The agent’s task is to compress newly arriving pages as
efficiently as possible using the (meta)model it can build using past data. As new data
arrives, the agent adjusts its world model to optimize the compression of future data.
As we shall see, this leads naturally to the emergence of probability theory and Bayesian
notions.
Using this example, we will show how the agent’s strategy leads to the notion of prob-
ability and Bayesian inference, examine the role of Solomonoff Induction and Universal
Priors, and generalize these ideas to objective functions other than compression. Finally,
we will use these insights to connect the algorithmic agent in KT with Karl Friston’s
Active Inference [23].
2.1 Multi-Model Strategy for Compression in Agents
In this section, we explore how an agent designed to harvest data and construct a world
model for compression might naturally adopt a multi-model strategy, implicitly or explic-
4
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
itly. The agent’s goal is to compress both past and future data as efficiently as possible.
To be clear, the task we are discussing is not just to compress all the past data. To do that,
it should simply find the shortest model possible. The idea is to choose a compression
strategy that will allow it to compress also future data well.
For concretion, consider the specific scenario where an agent is sequentially fed pages
from a large library, such as the Library of Congress in the United States (which includes
more than 32 million books and other print materials in 470 languages). We can imagine
the agent being fed pages randomly by a demigod. Or perhaps the agent is autonomous,
roaming from place to place, picking up a random book and page each time.
The agent aims to compress each of the newly arriving pages as efficiently as possible.
Critically, it has to do with only partial access to world data, and a model must be chosen
after the first few lines in each pattern.
We can evaluate how well it is doing by seeing at any given point how much it can
compress newly arriving data using the model it has built. That is, the reward is directly
tied to the degree of compression it achieves over time for each newly arriving page with
the model is has built using past data.
The agent begins by receiving a few pages, perhaps all from a novel. It builds a sim-
ple model that captures the patterns present in this initial data (e.g., typical sentence
structure, vocabulary). The agent uses this model to compress newly incoming pages,
assuming future pages will be similar while monitoring compression performance.
As the agent receives more pages, it encounters a research paper, which differs significantly
from the novels it has modeled so far. The existing model, optimized for novels, performs
poorly when compressing the newly arrived data. The agent faces a decision: should
it modify the current model to handle both types of content, or should it maintain a
separate model for research papers? What should it do when ingesting a page in which
English and Latin are used?
The agent faces several key challenges: a) partial access to coarse-grained data
from the world, and sequential data arrival : the agent receives one page at a time
and must update its model as it encounters new content; b) diversity of content :
the library contains various types of documents in different languages, such as novels,
research papers, historical documents, and poetry, each with different structures and
linguistic patterns; c) noisy data : some pages may contain errors or inconsistencies,
such as OCR mistakes or corrupted scans, which introduce noise into the data.
If the agent had immediate access to the entire corpus in the library, it could build a
single unified compressor, perhaps also considering where the agent is in the library and
the library map. But it does not have this information, only partially arriving pages as
it moves from place to place, picks a book, and selects a random page. A single, holistic
compression model is unlikely to be rapidly built by the agent to optimize compression in
the presence of diverse content and noise. In fact, this model may be impossible to create
because it will have to include the agent or demigod actions, for example (which book
and page will be picked next?), together with a cognitive map of the library’s content.
An effective strategy for the compression of future data is to maintain a set of simpler
multiple submodels, each optimized for a different type of content. For instance, one
model could specialize in English fiction, while another handles academic writing in Ital-
5
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
Figure 1: Our robot explores a grand library, analyzing and processing books. The robot
represents the agent in our toy example, dynamically building a world model to compress
diverse and noisy data as it collects more pages.
ian. The agent then selects the most appropriate model for each incoming page based
on a notion of “probability”, i.e., given a new page, which models are more likely to
apply? More commonly, models seen — a posteriori — in past pages can exploit more
compressive schemes at the expense of rare models.
For compression of future data and minimizing its expected description length, the agent
compressor must consider a tradeoff between the observed frequency (probability) of mod-
els and their lengths (compression performance). This approach is reminiscent of Huffman
coding [7], where more frequent symbols are encoded with shorter bit sequences, thereby
achieving more efficient compression for common elements. Compression is achieved by
combining the models probabilistically, effectively leveraging the strengths of multiple
models to reduce the future expected total description length.
This is essentially a Bayesian approach, assigning probabilities to each model based on
how well it compresses the data and how often it has been used in the past. To update
its world model, when new data arrives, the agent evaluates the likelihood of all the data
seen so far under each model and updates the model probabilities accordingly.
Thus, an agent aiming to compress both past and future data in a sequential fashion is
naturally led to employ a multi-model strategy with a notion of probability to achieve
optimal compression. This strategy not only improves data compression but also enhances
the agent’s adaptability and generalization to new types of data.
Finally, while we have discussed the agent’s goal of compressing data, the framework
can be generalized to any objective/utility function that requires a model of the world.
Whether the agent’s goal is to achieve a state of stasis or predict future events, the need
for a model to plan actions naturally leads to a probabilistic multi-model strategy.
2.2 Occam’s razor
An important feature of Bayesian inference is the use of priors. We have seen how priors
arise naturally from building multiple models and assessing how frequently they are used
with past data. But why are simple models better?
6
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
Solomonoff Induction provides a formal foundation for the emergence of simplicity priors
from Algorithmic Information Theory (AIT) principles. In this framework, the probabil-
ity of an observation is computed as the weighted sum of all possible computable models
that could generate the data, with each model weighted by its simplicity, measured by its
Kolmogorov complexity. The agent considers all possible programs that could explain the
data and assigns probabilities based on their simplicity, with simpler models (i.e., those
with shorter descriptions) being more likely. This reflects the assumption that the world
follows algorithmic rules, where simpler models are more likely to generate the observed
data.
The probability assigned to a model Mi is given by:
P(Mi) ∝ 2−K(Mi),
where K(Mi) is the Kolmogorov complexity of the model, representing the length of the
shortest program capable of generating the data.
The preference for shorter programs originates from the hypothesis of a computational
universe, where programs are generated randomly, akin to the ’monkey typing’ analogy.
In this scenario, shorter self-terminating programs are statistically more probable than
longer ones, due to the exponential growth in the number of possible programs as length
increases. This gives rise to the Solomonoff prior, which penalizes longer, more complex
models and formalizes Occam’s Razor: among competing hypotheses that explain the
data equally well, the simplest one should be preferred.
Thus, Solomonoff Induction introduces the concept of a “universal prior” over all possible
models, further tying algorithmic complexity to probability and model simplicity. This
framework provides a mathematically rigorous basis for rational inference within AIT,
where simplicity is statistically more likely to correspond to the true data-generating
process.
However, we may consider other potential answers by invoking an algorithmic form of
evolution and natural selection in a computational soup. First, natural selection may
select agents that find simple models. These will be easier to construct, store, and use.
Second, it may favor agents that operate at a coarse-grained level of the world in a way
that can be modeled simply. This motivates a definition of Emergence in the next
section.
2.3 Active Inference and the Free Energy Principle
In previous work [30], we highlighted the conceptual compatibility between the Free
Energy Principle (FEP) and Active Inference [12, 23], and the Kolmogorov Theory (KT)
of consciousness. FEP provides a causal statistical framework that accounts for action,
perception, and learning by positing that an adaptive agent minimizes free energy (or
surprise) to maintain stasis and resist disorder. This minimization involves maximizing
the evidence for the agent’s internal generative model of sensory data.
While earlier discussions hinted at the relationship between FEP and KT, our work here
clarifies this link by deriving the principles of probability and Bayesian inference from
AIT within the framework of the algorithmic agent. This derivation demonstrates that
probabilistic reasoning and Bayesian updating are natural consequences of the agent’s
7
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
drive to compress and model the world efficiently. It shows that FEP and Active Inference
are specific implementations of KT’s broader principles.
In FEP, the minimization of surprise is a dual process: it involves finding models that
not only match incoming data but also remain consistent with prior models, which en-
code the agent’s homeostatic goals and a notion of complexity as a departure from prior
models. This process balances statistical complexity (related to algorithmic complexity)
and accuracy. Surprise is represented as the negative log-likelihood of the data given the
model, J = −log P(y). The agent seeks to minimize surprise by adjusting its internal
models to better explain sensory inputs while aligning with its homeostatic priors. This
aligns partly with KT, where the agent aims to minimize the description length of its
internal models by balancing model complexity and data fit. However, in fact, in KT,
the primary goal is the optimization of the Objective Function, which may rely partly on
the simplicity of the found models.
Furthermore, FEP’s emphasis on prediction errors and their hierarchical use maps onto
the function of the Comparator in the algorithmic agent framework. In our model, the
Comparator assesses the errors between predicted and observed data, guiding real-time
model refinement. This process is crucial in both frameworks, as it allows the agent
to probabilistically select models that balance between fitting current data and being
adaptable to future data. In KT, however, the Comparator deals only with accuracy, not
homoeostatic priors.
By deriving probability and Bayesian inference directly from AIT, we clarify that the
Objective Function in KT corresponds to the priors encoded within the FEP framework.
In KT, the agent’s goal is to reduce the algorithmic complexity of its data representation
while maintaining adaptability. This naturally leads to a probabilistic framework where
the agent evaluates a distribution of models based on their compressive power and gener-
alizability. In FEP, the homeostatic priors represent the agent’s expected states, guiding
the minimization of surprise in a way that reflects the agent’s goals and expectations —
i.e., they are an implementation of the Objective Function.
This synthesis clarifies that Active Inference (and the Free Energy Principle) can be
viewed as a specific realization of KT in a Bayesian context, utilizing probabilistic rea-
soning as an emergent strategy for the agent to manage uncertainty and optimize its
interaction with the environment. Thus, AIF serves as a statistical implementation of
the algorithmic processes in KT, where the agent seeks not only to compress sensory data
but to do so in a manner that aligns with its intrinsic goals encoded as priors.
3 Emergence
Emergence is central to understanding complex systems, where simple rules at a micro-
scopic level give rise to intricate, often unpredictable, behaviors at a macroscopic scale
[1, 13, 35]. In this paper, we adopt an AIT approach to emergence in the context of the
Kolmogorov theory of consciousness (KT), defining it in terms of Kolmogorov complex-
ity and coarse-graining by agents. Our goal is to formalize the conditions under which
emergent behavior can be recognized and understood through this framework.
As it turns out, our work is closely in spirit to the one in [18], where “Emergence is
the phenomenon characterized by observation data that display several minimal partial
8
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
Figure 2: The agent coarse-graining the world to make sense out of it. This process, if successful,
gives rise to emergent phenomena. Not all coarse-grainings are meaningful, however.
models”, but differs in the final definition of Emergence. We will clarify the relationship
in the discussion.
3.1 Formal Definition of Emergence in the agent framework
In this section, we define emergence formally in terms of coarse-graining and Kolmogorov
complexity. We recall that Kolmogorov complexity is a measure of the complexity of
a dataset in terms of the length of the shortest algorithm capable of reproducing it.
Specifically, we propose the following definition:
Definition: Emergence occurs when the coarse-graining of a complex system transforms
data that appears incompressible (with high apparent Kolmogorov complexity and high
entropy) at the microscopic level into a new system that retains non-trivial structure
(high entropy) but can be compressed by a simpler, lower-complexity model.
Let S be a complex system with a microscopic state space X and microscopic evolution
governed by a set of rules Rmicro : X → X. The system generates a data sequence
Dmicro ∈ Xbased on these rules.
We say that the Kolmogorov complexity of the data sequence Dmicro is low if the
system is governed by simple, underlying rules, Ktrue(Dmicro) ≪ |Dmicro|, where absolute
values indicated the length of the sequence. However, to an agent unaware of the simple
underlying rules, the data will appear highly complex and incompressible. We say that
the apparent Kolmogorov complexity is given by Kapparent(Dmicro) ≈ |Dmicro|. This
reflects the difficulty for the observer in discovering the simple rules that generate the
system’s behavior.
Let C : X → Ydenote a coarse-graining operator that maps the microscopic state spaceX
to a macroscopic state space Y. The coarse-grained system is governed by new, emergent
rules Rmacro : Y → Y, generating a macroscopic data sequence Dmacro = C(Dmicro).
Emergence occurs if the following conditions hold:
1. The apparent Kolmogorov complexity of the microscopic data Dmicro is high due
to the apparent randomness or incompressibility perceived by the observer. This
9
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
complexity reflects the observer’s inability to easily extract the simple rules at the
microscopic scale. Our definition does not require a specific level of simplicity of
the source data, however.
2. After applying the coarse-graining operator C, the macroscopic data sequenceDmacro
retains high entropy and apparent complexity but is describable by a simpler
algorithm:
Kapparent(Dmacro) ≪ Kapparent(Dmicro).
The system after coarse-graining must retain high entropy (i.e., it is still apparently
complex and non-trivial), but now its behavior can be described using a simpler
algorithm, reflecting a reduction in perceived complexity without loss of structure.
The entropy of the coarse-grained system remains significant, indicating the pres-
ence of non-trivial emergent phenomena,
H(Dmacro) ≫ 0.
3. The coarse-graining C must be non-trivial, ensuring that the coarse-grained sys-
tem retains enough structure to exhibit complex, emergent behavior, with mutual
algorithmic information with the original system. A trivial coarse-graining (e.g.,
mapping all states to a constant) would not result in meaningful emergence,
Kperceived(Dmacro) > 0.
The system’s high entropy and apparent complexity indicate that the coarse-grained
system is still rich in structure, despite being simpler to model.
In summary, emergence is the process by which the perceived Kolmogorov complexity of
a system is reduced through coarse-graining, allowing an observer to recognize simpler
patterns or rules at the macroscopic level, while the coarse-grained system retains mutual
algorithmic information with the original system, high entropy and apparent complexity,
ensuring that the coarse-graining process is non-trivial.
3.2 Emergence in Cellular Automata
Cellular automata (CA), as discussed by Wolfram [35], serve as compelling examples
of how simple underlying rules can produce patterns with high apparent Kolmogorov
complexity at the microscopic level. These patterns can often appear chaotic or random
but are, in fact, governed by intrinsically simple rules.
A key example is elementary cellular automaton Rule 110, which is capable of generating
outputs that are highly complex and even Turing complete [6]. However, upon coarse-
graining or analysis at a macroscopic level, emergent structures such as periodic patterns
or self-similar behavior can be observed. These macroscopic phenomena are often much
simpler to describe, suggesting that while the microscopic details seem complex, the
emergent behavior can be captured with a lower Kolmogorov complexity.
Israeli and Goldenfeld [15] demonstrated that coarse-graining could transform elementary
cellular automata into new effective rules that describe the system’s behavior at a larger
scale, often revealing emergent phenomena that were not obvious at the microscopic level.
The definition of emergence presented here is closely tied to this idea: while the micro-
scopic rules of the system may generate outputs that appear to have high Kolmogorov
10
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
complexity, the coarse-grained system exhibits patterns that can be described using sim-
pler, more compressible models. At the same time, the coarse-grained system retains
mutual algorithmic information with the original CA.
3.3 Emergence and the Renormalization Group
The concept of emergence, defined through coarse-graining and Kolmogorov complex-
ity, parallels the Renormalization Group (RG) approach in statistical physics and quan-
tum field theory. RG is a mathematical framework describing how a system’s behavior
changes from microscopic to macroscopic scales. This process effectively implements
coarse-graining by integrating out or averaging over microscopic degrees of freedom to
derive an effective macroscopic description.
Developed by Kadanoff, Wilson, and others [17, 34, 22], the RG approach captures how
systems exhibit different behaviors at varying length scales. By iteratively coarse-graining
the system—averaging over clusters or integrating out high-frequency modes—we gen-
erate a sequence of effective theories. This often leads to emergent, simplified behavior
at the macroscopic level, not directly inferable from microscopic interactions. Under RG
transformations, system parameters (e.g., coupling constants) may flow towards fixed
points, representing scale-independent, emergent behaviors. These fixed points corre-
spond to universal properties that are insensitive to microscopic details, akin to emergent
phenomena having lower Kolmogorov complexity than their underlying rules.
The RG process reveals non-trivial macroscopic structures through coarse-graining. At
critical points, systems show emergent scaling behavior, where the RG flow leads to scale-
invariant descriptions. The universality in RG reflects the idea that different microscopic
systems can exhibit the same emergent behavior after coarse-graining, formalizing how
diverse rules can lead to identical macroscopic phenomena.
From the algorithmic complexity perspective, RG reduces the apparent complexity of a
system by creating an effective theory with fewer degrees of freedom to describe large-
scale behavior. This mirrors our definition of emergence, where complex microscopic data
becomes compressible and simpler at the macroscopic level.
3.4 Relation to emergence through partial models
As mentioned in the introduction, B´ edard and Bergeron (2022) [4] propose a closely re-
lated definition of emergence using Kolmogorov’s structure-function. While our definition
can be connected with a standard view of coarse-graining, the two definitions are closely
related. What [4] defines as partial models using Kolmogorov’s Structure Function are
included, in fact, coarse-graining.
An intuitive way to understand data compression is to consider the idea of finding the
compressed size of a dataset that can be maximally achieved using a given number of
bits, a. Suppose we want to describe a dataset as efficiently as possible, but are limited
to using only a bits. The goal is to capture as much of the dataset’s structure and
regularities as possible within this constraint. As we allocate more bits to describe the
dataset, we can capture more details and encode more of its regularities. Initially, with
very few bits, we might only be able to capture the most significant patterns, leading to
a coarse, highly compressed representation. As a increases, the description can include
11
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
more nuanced details, reducing the overall size of the most compressed representation we
can achieve.
This concept is closely related to Kolmogorov’s structure function , which can be
expressed as [7]
hx(a) = min
S
{log |S| : x ∈ S, K(S) ≤ a},
where x is a binary string of length n, S is a contemplated model (a set of n-length
strings) containing x, and K(S) is the Kolmogorov complexity of S. The value a is a
nonnegative integer that bounds the complexity of the contemplated sets S.
In this formulation, the structure function hx(a) measures the minimal log-size of a set
S that contains x and has a complexity less than or equal to a. The function is non-
increasing and reaches log |{x}| = 0 when a = K(x) + c, where K(x) is the Kolmogorov
complexity of x and c represents the additional bits needed to transform x into the set
{x}.
This function provides insight into how the complexity of a set S changes as we vary the
allowable complexity a. As we increase a, allowing for more complex descriptions, we can
describe x using smaller sets, effectively compressing the data further by including more
of its regularities.
This notion relates to our initial idea of compressed size achievable with a bits. It illus-
trates the trade-off between the complexity of the model and the compressed representa-
tion of the data. The function hx(a) shows how, as we allocate more bits to describe a
dataset, we can capture more of its structure, achieving a more compact representation.
Both the intuitive concept of optimizing compression witha bits and the formal definition
of Kolmogorov’s structure function revolve around the idea of balancing model complexity
with data description. They explore how the size of the minimal description set varies
with the number of bits available, providing a unified perspective on data compression
and structure.
Kolmogorov’s structure function can also be as a measure of the minimal description
length of a model that fits the data x with a specified accuracy α [20]. Formally, it is
defined as
hx(α) = min {|d| : d describes a model M such that d(M) = x′ and ∆(x, x′) ≤ α},
where |d| is the length of the description d, x′ is the output of model M, and ∆ is a
measure of the error between x and x′. This structure function captures the trade-off
between model simplicity and accuracy.
The above definitions are also related to the concept of Effective Complexity [13]. Briefly,
one can conceptually split the Kolmogorov optimal program describing a data string into
two parts: a set of bits describing its regularities and another that captures the rest
(the part with no structure). The first term is effective complexity, which is the minimal
description of the regularities of the data. This concept brings to light the power of the
notion of Kolmogorov complexity, as it provides, single-handedly, the means to account
for and separate regularities in data from noise [28].
An agent limited to some program length or computational limitations will, as part of
the strategy, explore coarse-graining the data in different ways. In doing so, it will find
12
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
program lengths in which compression is suddenly more effective. These will correspond
to effective coarse-graining strategies — what the authors call regraining, which lead to
drops in apparent complexity.
Finally, in the algorithmic agent framework, coarse-graining may not just be employed for
compression. The agent’s primary driver is stasis as dictated by its Objective Function.
It may, therefore, choose coarse-grainings (or regrainings) that are suboptimal from the
point of view of compression.
Hence, the two definitions are closely related but not equivalent, and this is due to our
focus here on an appropriate definition for the algorithmic agent.
4 Discussion
A key feature of the framework we have outlined is the necessity for the agent to build
and refine its models dynamically, “on the go,” as new data arrives, and its limited
computational resources, which lead it into coarse-graining. The agent is not presented
with the entire dataset at once but rather must process data sequentially. This continuous
adaptation process is central to the emergence of probability and the adoption of a multi-
model strategy.
The agent’s primary goal is not only to compress past data effectively but also to remain
prepared for future data that may arrive in a different form. This foresight requires
the agent to maintain a flexible modeling approach, as future data may introduce new
patterns, noise, or entirely new types of content. By updating its models in real-time, the
agent assigns probabilities to various models based on how simple (compressive) they are
and on how well they explain past data, as this reflects how likely they are to generalize
to future data.
This probabilistic approach emerges naturally to allow the agent to hedge its bets, bal-
ancing between fitting the current data accurately and being adaptable enough to handle
potential variations in future data.
By maintaining a compressive model, which is, in fact, a distribution over multiple sub-
models, the agent can optimally compress future data it encounters. This dynamic mod-
eling process is what drives the natural emergence of probability as the agent seeks to
optimize its readiness for the unknown future. When optimizing a utility function in
the presence of uncertainty, it is generally more effective to utilize the full Probability
Density Function (PDF) of models rather than relying solely on a Maximum A Posteriori
(MAP) estimate. The PDF represents the entire distribution of possible model outcomes,
allowing for a more comprehensive assessment of expected utility by integrating across
all potential scenarios. This approach ensures that uncertainties are fully accounted for,
leading to more robust and reliable decision-making.
This aligns with decision theory, where the concept of expected utility plays a central role
in modeling rational decision-making under uncertainty (see [2] for a detailed discussion
of probabilistic modeling). The expected utility is defined as the product of the util-
ity associated with a model and the probability density function (PDF) of that model.
Essentially, the expected utility represents the average utility that an agent expects to
achieve, considering both the desirability of the model (utility) and the likelihood of that
13
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
model occurring (probability).
While modern large language models (LLMs) like GPT do not explicitly use a multi-model
strategy, they manage diverse tasks by relying on a single, highly complex model trained
on vast, varied data. This unified architecture allows the model to generalize across
multiple domains, though it does not explicitly maintain distinct models for different
types of content.
On the other hand, multi-model strategies are widely used in machine learning in general
to enhance performance and efficiency. One example of an explicit strategy is the Mixture
of Experts (MoE) framework [31], where multiple specialized models (or “experts”) are
trained for different tasks or data types. A gating mechanism selects the most relevant ex-
pert based on the input. This is closely aligned with the multi-model approach discussed
in this paper, where different models handle different kinds of data. Ensemble learning
[10] is another common multi-model strategy in machine learning. In this method, mul-
tiple models are trained independently, and their predictions are combined to improve
accuracy and robustness. Although ensemble methods are not typically used in LLMs
due to their computational demands, they are effective in many other machine learning
applications. Finally, context mixing models, like those used in the PAQ compressor,
combine predictions from multiple models based on different data contexts, effectively
applying a multi-model strategy to text compression [21].
These real-world applications highlight the effectiveness of multi-model strategies in
achieving efficient modeling (compression) by tailoring models to specific data features
and contexts.
In the process of coarse-graining, the transformation from a microscopic to a macroscopic
description is just one specific example of a more general principle: the reorganization
or lossy compression of data from the aggregation of complex information. The key
requirement for emergence is not necessarily the transition between scales but rather
any meaningful transformation that simplifies the representation of complex data while
retaining its essential structure. A common technique, such as taking a running aver-
age, smooths out local fluctuations to reveal larger patterns, but other approaches are
equally valid. For instance, one might transform the data into Fourier space, where pe-
riodicities and global features become more apparent, allowing for a different type of
simplification. Compressive sensing also becomes highly relevant here, as it emphasizes
the reconstruction of signals using fewer measurements by exploiting the sparsity of the
data in a transformed domain. This parallels the idea of simplifying complex systems by
identifying the most informative features and discarding redundancies. More generally,
dimensionality reduction techniques like principal component analysis (PCA) or nonlin-
ear methods like t-SNE or autoencoders can be used to uncover emergent structures by
reducing the complexity of the system while preserving key relationships. These diverse
methods illustrate that coarse-graining can be applied in various domains, and emergence
is not tied to a specific spatial or temporal scale, but to the revelation of simpler patterns
from the aggregation and compression of complex data.
Emergence is in the eyes of the beholder (the agent)
In this sense, emergence is not merely a property of the data-generating system but also
of the modeling process carried out by the observer. The emergent behavior depends
14
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
on how the system is being observed and modeled and how effective the coarse-graining
and modeling processes carried out by the observing agent are at uncovering simplified
macroscopic patterns (compression) from seemingly complex microscopic interactions.
5 Conclusion
In an evolutionary setting (an algorithmic information “soup”) where programs compete,
how do agents and their computational strategies evolve? Given the inherent constraints
of finite agents and the fundamental limitation imposed by the uncomputability of Kol-
mogorov complexity, coarse-graining and probabilistic inference could be inevitable adap-
tations, allowing agents to operate effectively within the bounds of their computational
resources and the mathematics of algorithmic information.
In an evolutionary setting (an algorithmic information “soup”) where programs com-
pete, how do agents and their computational strategies evolve? Given the inherent con-
straints of finite agents and the fundamental limitation imposed by the uncomputability of
Kolmogorov complexity, coarse-graining and probabilistic inference could be inevitable
adaptations. These strategies allow agents to operate effectively within the bounds of
their computational resources and the mathematics of algorithmic information. This
perspective aligns with Chaitin’s exploration of the intersection between evolution and
algorithmic information theory [5].
In this paper, we have shown that probability theory and Bayesian inference naturally
emerge from AIT in the context of an agent tasked with data compression. By employing
a multi-model strategy, the agent balances competing models to optimize compression, re-
sulting in a probabilistic view of the world. This framework generalizes to other objective
functions and is grounded in Solomonoff Induction and Universal Priors.
These ideas resonate with foundational approaches like E.T. Jaynes’ robot formalism,
where probability serves as a tool for rational inference in the face of uncertainty [16].
Just as Jaynes’ robot uses probability to infer the most plausible explanation from incom-
plete data, our agent leverages probabilistic reasoning to manage competing compressive
models. This further supports the notion that probability emerges as an algorithmic
strategy for handling uncertainty, regardless of whether the agent aims to compress data
or optimize other objectives.
Similarly, the No Free Lunch theorem [36] reinforces the agent’s use of a multi-model
strategy. The theorem states that no single algorithm is universally superior across
all problems, underscoring the need for adaptability and flexibility in model selection.
By probabilistically selecting between multiple models, the agent aligns with the NFL
theorem’s insight into navigating diverse data distributions.
By framing probabilistic reasoning as a natural outcome of an agent’s drive to model
and compress the world, we offer a unified perspective on how rational agents manage
uncertainty and optimize performance. AIT inherently leads to probabilistic reason-
ing, bridging the gap between compression, prediction, and probability. This provides a
conceptual link between theories like the Kolmogorov Theory of consciousness and the
algorithmic agent and others based on Shannon probability, such as the Free Energy Prin-
ciple and Active Inference. Additionally, we have formalized the process of emergence
using coarse-graining and Kolmogorov complexity. A system exhibits emergence when
15
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
coarse-graining reduces complexity while retaining high entropy and relevant structure
for the agent. This framework unifies emergence with findings in cellular automata and
renormalization group theory, showing how simple microscopic rules lead to complex but
compressible macroscopic behavior. The observer’s perspective is crucial, as the detected
emergence depends on the chosen coarse-graining. Emergence is a process witnessed by
agents carrying out coarse-graining for stasis.
In conclusion, this work shows that both probabilistic reasoning and emergence are prac-
tical consequences of an agent’s need to navigate a world with limited resources and
incomplete information. Grounding these principles in AIT reveals that an agent’s drive
to compress and model the world leads to Bayesian inference and an understanding of
emergent phenomena. This unified framework offers deeper insights into the interplay
between information processing, inference, and the intrinsic complexity of the systems
agents inhabit.
References
[1] P W Anderson. More is different. Science (New York, N.Y.) , 177(4047):393–396,
August 1972. Publisher: American Association for the Advancement of Science
(AAAS).
[2] Christopher M. Bishop. Pattern recognition and machine learning . Information
science and statistics. Springer, New York, 2006.
[3] Jennifer K. Bizley and Yale E. Cohen. The what, where and how of auditory-object
perception. Nature Reviews Neuroscience, 14(10):693–707, October 2013. Publisher:
Nature Publishing Group.
[4] Charles Alexandre B´ edard and Geoffroy Bergeron. An Algorithmic Approach to
Emergence. Entropy (Basel, Switzerland) , 24(7):985, July 2022.
[5] Gregory J. Chaitin. Proving Darwin: making biology mathematical. Pantheon Books,
New York, 2012.
[6] Matthew Cook. Universality in elementary cellular automata. Complex Systems,
15 (2004) ; , 15:1–40, 2004. tex.date-added: 2016-05-24 15:46:35 +0000 tex.date-
modified: 2016-09-17 21:49:32 +0000.
[7] Thomas M. Cover and Joy A. Thomas. Elements of information theory . John
Wiley & sons, 2 edition, 2006. tex.date-added: 2016-10-20 21:00:29 +0000 tex.date-
modified: 2016-10-20 21:00:58 +0000.
[8] Gr´ egoire Del´ etang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Ge-
newein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew
Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. Language Modeling
Is Compression, March 2024. arXiv:2309.10668 [cs, math].
[9] James J. DiCarlo, Davide Zoccolan, and Nicole C. Rust. How does the brain solve
visual object recognition? Neuron, 73(3):415–434, February 2012.
[10] Thomas G. Dietterich. Ensemble Methods in Machine Learning. In Multiple Classi-
fier Systems, pages 1–15, Berlin, Heidelberg, 2000. Springer.
16
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
[11] K. J. Friston. The free-energy principle: A rough guide to the brain?. Trends in
Cognitive Science, 13(7):293–301, 2009. Publication title: Friston, K. J. (2009).
The free-energy principle: A rough guide to the brain?. Trends in Cognitive Science,
13(7), 293–301. tex.date-added: 2016-12-17 21:14:09 +0000 tex.date-modified: 2016-
12-17 21:15:36 +0000.
[12] Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neu-
roscience, 11(2):127–138, February 2010. Publisher: Springer Science and Business
Media LLC.
[13] M Gell-mann and Seth Lloyd. Effective complexity. SFI working paper, Santa
Fe Institute, 2003. Issue: 2003-12-068 tex.date-added: 2016-10-16 14:02:46 +0000
tex.date-modified: 2016-10-16 14:04:20 +0000.
[14] Kalanit Grill-Spector and Kevin S. Weiner. The functional architecture of the ven-
tral temporal cortex and its role in categorization. Nature Reviews Neuroscience ,
15(8):536–548, August 2014. Publisher: Nature Publishing Group.
[15] N. Israeli and N Goldenfeld. Coarse-graining of cellular automata, emergence, and
the predictability of complex systems. Physical Review, E(73), 2006. tex.date-added:
2016-05-27 02:05:15 +0000 tex.date-modified: 2016-05-27 02:06:21 +0000.
[16] E.T. Jaynes. Probability theory - the logic of science . Cambridge, 2003. tex.date-
added: 2009-04-07 20:03:42 +0200 tex.date-modified: 2009-04-07 20:04:24 +0200.
[17] Leo P. Kadanoff. Scaling laws for ising models near ${T} {c}$. Physics Physique
Fizika, 2(6):263–272, June 1966. Publisher: American Physical Society.
[18] Ali Khaleghi, Mohammad Reza Mohammadi, Kian Shahi, and Ali Motie Nasrabadi.
Computational Neuroscience Approach to Psychiatry: A Review on Theory-driven
Approaches. Clinical Psychopharmacology and Neuroscience: The Official Scientific
Journal of the Korean College of Neuropsychopharmacology , 20(1):26–36, February
2022.
[19] Ming Li and Paul Vitanyi. An introduction to Kolmogorov Complexity and its appli-
cations. Spriger Verlag, 2008. tex.date-added: 2009-03-01 22:14:38 +0100 tex.date-
modified: 2009-05-24 11:58:30 +0200.
[20] Ming Li and Paul M.B. Vitanyi. Applications of algorithmic information theory.
Scholarpedia, 2(5):2658, 2007. tex.date-added: 2016-10-15 14:12:39 +0000 tex.date-
modified: 2016-10-22 13:13:18 +0000.
[21] Matthew Mahoney. Adaptive Weighing of Context Models for Lossless Data Com-
pression. Electrical Engineering and Computer Science Faculty Publications, Decem-
ber 2005.
[22] Leenoy Meshulam and William Bialek. Statistical mechanics for networks of real
neurons, August 2024. arXiv:2409.00412 [cond-mat, q-bio].
[23] Thomas Parr. Active inference : the free energy principle in mind, brain, and be-
havior. The MIT Press, Cambridge, Massachusetts, 2022.
[24] Thomas Parr and Giovanni Pezzulo. Active inference. MIT Press, London, England,
March 2022.
17
How Resource-Limited Agents Derive Probability and Generate Emergence G. Ruffini
[25] Giulio Ruffini. Information, complexity, brains and reality (“Kolmogorov Mani-
festo”). arXiv, 2007. tex.date-added: 2009-03-01 11:09:48 +0100 tex.date-modified:
2009-06-06 18:41:44 +0200.
[26] Giulio Ruffini. Reality as simplicity. 2009. Publisher: arXiv.
[27] Giulio Ruffini. Models, networks and algorithmic complexity. 2016. Publisher: arXiv.
[28] Giulio Ruffini. An algorithmic information theory of consciousness. 2017(1):nix019,
October 2017.
[29] Giulio Ruffini, Francesca Castaldo, Edmundo Lopez-Sola, Roser Sanchez-Todo, and
Jakub Vohryzek. The algorithmic agent perspective and computational neuropsychi-
atry: from etiology to advanced therapy in major depressive disorder, March 2024.
[30] Giulio Ruffini and Edmundo Lopez-Sola. AIT foundations of structured experience.
9(2):153–191, September 2022. Publisher: World Scientific Pub Co Pte Ltd.
[31] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Ge-
offrey Hinton, and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-
Gated Mixture-of-Experts Layer. July 2022.
[32] R. J. Solomonoff. A formal theory of inductive inference. Part I. Information and
Control, 7(1):1–22, March 1964.
[33] Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-
Francois Chamberland, and Srinivas Shakkottai. LLMZip: Lossless Text Compres-
sion using Large Language Models, June 2023. arXiv:2306.04050 [cs, math].
[34] Kenneth G. Wilson. Renormalization Group and Critical Phenomena. I. Renormal-
ization Group and the Kadanoff Scaling Picture. Physical Review B, 4(9):3174–3183,
November 1971. Publisher: American Physical Society.
[35] Stephen Wolfram. A new kind of science . Wolfram Media, 2002. tex.date-added:
2016-05-06 23:17:17 +0000 tex.date-modified: 2016-05-06 23:21:31 +0000.
[36] David H. Wolpert and William G. Macready. No free lunch theorems for optimiza-
tion. IEEE Transactions on Evolutionary Computation , 1(1):67–82, 1997. tex.date-
added: 2009-05-11 14:42:14 +0200 tex.date-modified: 2009-05-11 19:08:37 +0200.
18