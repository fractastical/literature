Article https://doi.org/10.1038/s41467-024-49711-1
Complex behavior from intrinsic motivation
to occupy future action-state path space
Jorge Ramírez-Ruiz 1 , Dmytro Grytskyy1, Chiara Mastrogiuseppe1,
Yamen Habib 1 & Rubén Moreno-Bote1,2
Most theories of behavior posit thatagents tend to maximize some form of
reward or utility. However, animals very often move with curiosity and seem to
be motivated in a reward-free manner. Here we abandon the idea of reward
maximization and propose that the goal of behavior is maximizing occupancy
of future paths of actions and states.According to this maximum occupancy
principle, rewards are the means to occupy path space, not the goal per se;
goal-directedness simply emerges as rational ways of searching for resources
so that movement, understood amply, never ends. Weﬁnd that action-state
path entropy is the only measure consistent with additivity and other intuitive
properties of expected future action-state path occupancy. We provide ana-
lytical expressions that relate the optimal policy and state-value function
and prove convergence of our value iteration algorithm. Using discrete and
continuous state tasks, including a high-dimensional controller, we show that
complex behaviors such as“dancing”, hide-and-seek, and a basic form of
altruistic behavior naturally result fromthe intrinsic motivation to occupy path
space. All in all, we present a theory of behavior that generates both variability
and goal-directedness in the absence of reward maximization.
Natural agents are endowed with a tendency to move, explore, and
interact with their environment1,2. For instance, human newborns
unintentionally move their body parts3,a n d7–12-month-old infants
spontaneously babble vocally4 and with their hands5. Exploration and
curiosity are major drives for learning and discovery through
information-seeking6–8. These behaviors seem to elude a simple
explanation in terms of extrinsic reward maximization. However,
intrinsic motivations, such as curiosity, push agents to visit new states
by performing novel courses of action, which helps learning and the
discovery of even larger rewards in the long run
9,10. Therefore, it has
been argued that exploration and curiosity could arise as a con-
sequence of seeking extrinsic reward maximization by endowing
agents with the necessary inductive biases to learn in complex and
ever-changing natural environments
11,12.
While most theories of rational behavior do posit that agents
are reward maximizers13–16, very few of us would agree that the
sole goal of living agents is maximizing money gains or food intake.
Indeed, expressing excessive emphasis on these types of goals is
usually seen as a sign of psychological disorders17,18. Further, setting
a reward function by design as the goal of artiﬁcial agents is, more
often than not, arbitrary14,19–21, leading to the recurrent problem faced
by theories of reward maximization of deﬁning what rewards are22–26.
In some cases, like in artiﬁcial games, rewards can be unambiguously
deﬁned, such as number of collected points or wins27.H o w e v e r ,i n
most situations deﬁning rewards is task-dependent, non-trivial, and
problematic. For instance, a vacuum cleaner robot could be designed
to either maximize the weight or volume of dust collected, energy
efﬁciency, or a weighted combination of them
28. In more complex
cases, companies can aim at maximizing pro ﬁt, but without a
suitable innovation policy, proﬁt maximization can be self-defeating29.
Even when a task is well-deﬁned from the perspective of an external
observer, the actions and states of natural agents are not always
compatible with a deterministic maximization of rewards and
are consistently found to be highly variable
30–32. Natural agents are
Received: 10 March 2023
Accepted: 13 June 2024
Check for updates
1Center for Brain and Cognition, Departament d’Enginyeria i Escola d’Enginyeria, Universitat Pompeu Fabra, Barcelona, Spain.2Serra Húnter Fellow Pro-
gramme, Universitat Pompeu Fabra, Barcelona, Spain.e-mail: jorgeerrz@gmail.com
Nature Communications|         (2024) 15:6368 1
1234567890():,;
1234567890():,;
also sensitive to the uncertainty in or about the environment, behaving
in ways that are inconsistent with strict reward maximization in
familiar environments
33,34. While there are isolated attempts to
understand the function and mechanisms of behavioral variability35,
and risk sensitivity in uncertain environments36–38, general principles
for the importance of stochasticity in so-called goal-directed behavior
are lacking.
Here, we abandon the idea that the goal is maximizing extrinsic
rewards and that movement over space is a means to achieve this goal.
Instead, we adopt the opposite view, inspired by the nature of our
intrinsic drives: we propose that the objective is to maximally occupy
action-state path space, understood in a broad sense, in the long term.
We call this principle the maximum occupancy principle (MOP), which
posits that the goal of agents is to generate all sorts of variable beha-
viors and visit, on average, as much space (action-state paths)
as possible in the future. According to MOP, extrinsic rewards serve
to obtain the energy necessary to move in order to occupy action-state
space, they are not the goals per se. The usual exploration-exploitation
tradeoff
39, therefore, disappears: agents that seek to occupy
space“solve” this issue naturally because they care about rewards only
as a means to an end. Furthermore, in this sense, surviving is only
preferred because it is needed to keep visiting action-state path space.
Our theory provides a rational account of exploratory and curiosity-
driven behavior where the problem of deﬁning a reward function
vanishes, and captures the variability of behavior
40–45 by taking it as a
principle.
In this work, we model a MOP agent interacting with the envir-
onment as a Markov decision process (MDP) where the intrinsic,
immediate reward is the occupancy of the next action-state visited,
which is largest when performing an uncommon action and visiting a
rare state— there are no extrinsic rewards (i.e., no task is deﬁned) that
drive the agent. We show that (weighted) action-state path entropy is
the only measure of occupancy consistent with additivity per time
step, positivity, and smoothness. Due to the additivity property, the
value of being in a state, deﬁned as the expected future time-
discounted action-state path entropy, can be written in the form of a
Bellman equation, which has a unique solution that can be found with
an iterative map. Following this entropy objective leads to agents that
seek variability while being sensitive to the constraints imposed by the
agent-environment interaction on the future path availability. We
demonstrate in various simulated experiments with discrete and
continuous state and action spaces that MOP generates complex
behaviors that, to the human eye, look genuinely goal-directed and
playful, such as hide-and-seek in a prey-predator problem, dancing of a
cartpole, a basic form of altruism in an agent-and-pet example, and
rich behaviors in a high-dimensional quadruped.
MOP builds over an extensive literature on entropy-regularized
reinforcement learning (RL)
46–56 or pure entropic objectives57–62.T h i s
body of work emphasizes the regularization beneﬁts of entropy for
learning, but extrinsic rewards still serve as the major drive of beha-
vior, and arbitrary mixtures of action-state entropy are rarely
considered
56. Our work also relates to reward-free theories of behavior.
These typically minimize prediction errors63–68,s e e kn o v e l t y69–71,o r
maximize data compression72, and therefore the major behavioral
driver depends on the agent’se x p e r i e n c ew i t ht h ew o r l d .O nt h eo t h e r
hand, MOP agents ﬁnd the action-states that lead to high future
occupancy“interesting”, regardless of experience. There are two other
approaches that sit closer to this description, one maximizing mutual
information between actions and future states (empowerment,
MPOW)
20,73–75, and the other minimizing the distance between the
actual and a desired state distribution (free energy principle, FEP)76,77.
We show that both MPOW and FEP tend to collapse to deterministic
policies with little behavioral variability. In contrast, MOP results in
lively and seemingly goal-directed behavior by taking behavioral
variability and the constraints of embodied agents as principles.
Results
Maximum occupancy principle
We model an agent as aﬁnite action-state MDP in discrete time. The
policy π describes the probabilityπ(a∣s) of performing actiona, from
some setAðsÞ, given that the agent is in states at some time step, and
pðs0js,aÞ is the transition probability froms to a successor states0 in the
next time step given that actiona is performed. Starting att = 0 in state
s0, an agent performing a sequence of actions and experiencing state
transitionsτ ≡ (s0, a0, s1,...,at, st+1,...) gets a return deﬁned as
RðτÞ =
X1
t =0
γt Rðst ,at Þ = /C0
X1
t =0
γt ln παðat jst Þpβðst +1 jst ,at Þ
/C16/C17
, ð1Þ
with action and state weightsα > 0 andβ ≥ 0, respectively, and dis-
count factor 0 <γ < 1. A larger return is obtained when, fromst,al o w -
probability actionat is performed and followed by a low-probability
transition to a statest+1. Therefore, maximizing the return in Eq. (1)
favors “visiting” action-states (at, st+1) with a low transition probability.
From st+1, another low-probability action-state transition is preferred,
and so on, such that low-probability trajectoriesτ a r em o r er e w a r d i n g
than high-probability ones. Thus, the agent is pushed to visit action-
states that are rare or“unoccupied”, implementing the intuitive notion
of MOP. Due to the freedom to choose actiona
t given statest and the
uncertainty of the resulting next statest+1,a p p a r e n ti nE q .(1), the term
“action-states”used here is more natural than“state-actions”.W es t r e s s
that this return is purely intrinsic, namely, there is no extrinsic reward
that the agent seeks to maximize. We deﬁne intrinsic rewards as any
reward signal that depends on the policy or the state transition
probability, and therefore it can change with the course of learning as
the policy is improved, or the environment is learned. An extrinsic
reward is the complementary set of reward signals: any functionR(s, a)
that is both policy-independent and transition probability-indepen-
dent, and therefore it does not change with the course of improving
the policy or learning the state transition probability of the
environment.
The agent is assumed to optimize the policyπ to maximize the
state-valueV
π(s), deﬁned as the expected return
VπðsÞ/C17 Eat ∼ π,st +1 ∼ p½RðτÞjs0 = s/C138
= Eat ∼ π,st +1 ∼ p
X1
t =0
γt αHðAjst Þ + βHðS0jst ,at Þ
/C0/C1
js0 = s
"#
ð2Þ
given the initial conditions0 = s and following policyπ, that is, the
expectation is over theat ~ π(⋅∣st)a n dst+1 ~ p(⋅∣st, at), t ≥ 0. In the last
identity, we have rewritten the expectations of the terms in Eq. (1)
as a discounted and weighted sum of action and successor state
conditional entropies HðAjsÞ = /C0 P
aπðajsÞ ln πðajsÞ and HðS0js,aÞ =
/C0 P
s0 pðs0js,aÞ ln pðs0js,aÞ, respectively, averaged over previous states
and actions.
We deﬁne a MOP agent as the one that optimizes the policy to
maximize the state value in Eq. (2). The entropy representation in Eq.
(2) of MOP has several implications. First, agents prefer regions of state
space that lead to a large number of successor states (Fig.1a) or larger
number of actions (Fig.1b). Second, death (absorbing) states where
only one action-state (i.e.,“stay”) is available forever are naturally
avoided by a MOP agent, as they promise zero future action and state
entropy (Fig. 1c— hence absorbing states s
+ have zero state-value,
Vπ(s+) = 0). Therefore, our framework implicitly incorporates a survival
instinct. Finally, regions of state space where there are“rewarding”
states that increase the capacity of the agent to visit further action-
states (such asﬁlling an energy reservoir) are more frequently visited
than others (Fig.1d).
We found that maximizing the discounted action-state path
entropy in Eq. (2) is the only reasonable way of formalizing MOP, as it is
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 2
the only measure of action-state path occupancy in Markov chains
consistent with the following intuitive conditions (see Methods and
Supplemental Sec. A): if a pathτ has probabilityp, visiting it results in
an occupancy gainC(p) that (i) decreases withp and (ii) isﬁrst-order
differentiable. Condition (i) implies that visiting a low-probability path
increases occupancy more than visiting a high-probability path, and
our agents should tend to occupy“unoccupied” path space; condition
(ii) requires that the measure should be smooth. We also ask that (iii)
the occupancy of paths, deﬁned as the expectation of occupancy gains
over paths given a policy, is the sum of the expected occupancies of
their subpaths (additivity condition). This last condition implies that
agents can accumulate occupancy over time by keeping visiting low-
probability action-states, but the accumulation should be consistent
with the Markov property of the decision process. These conditions
are similar but not exactly the same as those used to derive Shannon’s
information measure
78 (see Methods and Supplemental Sec. A).
Optimal policy and state-value function
The state-valueVπ(s)i nE q .(2) can be recursively written using the
values of successor states through the standard Bellman equation
VπðsÞ = αHðAjsÞ + β
X
a
πðajsÞHðS0js,aÞ + γ
X
a,s0
πðajsÞpðs0js,aÞVπðs0Þ
=
X
a,s0
πðajsÞpðs0js,aÞ/C0 α ln πðajsÞ/C0 β ln pðs0js,aÞ + γVπðs0Þ
/C0/C1
,
ð3Þ
where the sum is over the available actionsa from states, AðsÞ,a n d
over the successor statess0 given the performed action at states.T h e
number of actions and state transitions available does not need to be
the same for all statess. In particular, absorbing statess+ have only the
“stay” action available, leading to zero action and state transition
entropies, and therefore they have zero state-valueVπ(s+)=0 r e g a r d -
less of the policy.
The optimal policyπ* that maximizes the state value is deﬁned as
π* = arg maxπVπ and the optimal state value is
V*ðsÞ =m a xπVπðsÞ, ð4Þ
where the maximization is with respect to the {π(⋅∣⋅)} for all actions and
states. To obtain the optimal policy, weﬁrst determine the critical
points of the expected returnVπ(s) in Eq. (3) using Lagrange multipliers
(see Methods and Supplemental Sec. B). The optimal state-valueV*(s)
is found to obey the non-linear self-consistency set of equations
V*ðsÞ = α ln ZðsÞ = α ln
X
a
exp α/C0 1βHðS0js,aÞ + α/C0 1γ
X
s0
pðs0js,aÞV*ðs0Þ
 !"#
,
ð5Þ
where Z(s) is the partition function, deﬁned by substitution, and the
critical policy satisﬁes
π*ðajsÞ = 1
ZðsÞ exp α/C0 1βHðS0js,aÞ + α/C0 1γ
X
s0
pðs0js,aÞV*ðs0Þ
 !
: ð6Þ
Note that the optimal policy in MOP only depends on a single para-
meter, the ratioβ/α and that the optimal state-value function is simply
scaled byα.W eﬁnd that the solution to the non-linear system of Eqs.
(5) is unique and, moreover, the unique solution is the absolute
maximum of the state-values over all policies (Supplemental Sec. C).
To determine the actual value function from such non-linear set of
equations, we derive an iterative map, a form of value iteration that
exactly incorporates the optimal policy at every step. De ﬁning
zi =e x pðα/C0 1γVðsiÞÞ, pijk = p(sj∣si, ak)a n dHik = α/C0 1βHðS0jsi,ak Þ,E q .(5)
Fig. 1 | MOP agents maximize action-state path occupancy. aA MOP agent (gray
triangle) in the middle of two rooms has the choice between going left or right.
When the number of actions (black arrows) in each room is the same, the agent
prefers going to the room with more state transitions (blue arrows indicate random
transitions after choosing moving right or moving left actions, and pink arrow
width indicates the probabilities of those actions).b When the state transitions are
the same in the two rooms, the MOP agent prefers the room with more available
actions. c If there are many absorbing states in the room where many actions are
available, the MOP agent avoids it.d Even if there are action and state-transition
incentives (in the left room), an MOP agent might prefer a region of state space
where it can reliably get food (right room), ensuring occupancy of future action-
state paths. See Supplemental Fig. D.1 for a more formal example.
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 3
can be turned into the iterative map
zðn +1 Þ
i =
X
k
wik eHik
Y
j zðnÞ
j
/C16/C17 pijk
 ! γ
ð7Þ
for n ≥ 0 and with initial conditionszð0Þ
i >0. Here, the matrix with
coefﬁcients wik ∈ {0, 1} indicates whether actionak is available at state
si (wik =1 ) o r n o t (wik = 0), andj extends over all states, with the
understanding that if a statesj is not a possible successor from statesi
after performing actionak then pijk =0 .W eﬁnd that the inﬁnite series
zðnÞ
i deﬁned in Eq. (7) converges to aﬁnite limitzðnÞ
i ! z1
i regardless of
the initial condition in the positive ﬁrst orthant, and that
V*ðsiÞ = αγ/C0 1 ln z1
i is the optimal state-value function, which solves Eq.
(5) (Supplemental Sec. C). Iterative maps similar to Eq. (7) have been
studied before46,79, subsequently shown to have uniqueness80 and
convergence guarantees54,81 in the absence of state entropy terms. A
summary of results and particular examples can be found in
Supplemental Sec. D.
We note that in the deﬁnition of return in Eq. (2)w ec o u l dr e p l a c e
the absolute action entropy termsHðAjsÞ by relative entropies of the
form /C0 D
KLðπðajsÞjjπ0ðajsÞÞ = P
aπðajsÞ lnðπ0ðajsÞ=πðajsÞÞ,a si nK L -
regularization46,50,55,79, but in the absence of any extrinsic rewards. In
this case, one obtains an equation identical to (7) where the coefﬁ-
cients wik are simply replaced byπ0(ak∣si), one-to-one. This apparently
minor variation uncovers a major qualitative difference between
absolute and relative action entropy objectives: as∑
kwik ≥ 1, absolute
entropy-seeking favors visiting states with a large action accessibility,
that is, where the sum∑kwik and thus the argument of Eq. (7)t e n d st o
be largest. In contrast, as∑kπ0(ak∣si) = 1, maximizing relative entropies
provides no preference for statess with a large number of accessible
actionsjAðsÞj. This happens even if the default policy is uniform in the
actions, as then the immediate intrinsic return becomes
/C0 D
KLðπðajsÞjjπ0ðajsÞÞ = HðAjsÞ/C0 ln jAðsÞj,i n s t e a do fHðAjsÞ.T h en e g a -
tive logarithm penalizes visiting states with large number of actions,
which is the opposite goal to occupying action-state path space (see
details in Supplemental Sec. F).
MOP agents quicklyﬁll physical space
In very simple environments with high symmetry and little constraints,
like open space, maximizing path occupancy amounts to performing a
random walk that chooses at every step any available action with equal
probability. However, in realistic environments where space is not
homogeneous, where there are energetic limitations for moving, or
where there are absorbing states, a random walk is no longer optimal.
To illustrate how interesting behaviors arise from MOP in these cases,
we ﬁrst tested how a MOP agent moving in a 4-room and 4-food-
sources environment (Fig.2a) compares in occupying physical space
to a random walker (RW) and to a reward-seeking agent (R agent) (see
Methods for deﬁnitions of the agents). The deﬁnitions of the three
agents are identical in most ways. They have nine possible movement
actions, including not moving; they all have an internal state corre-
sponding to the available energy, which reduces by one unit at every
time step and gets increased by aﬁxed amount (food gain) whenever a
food source is visited; and they can move as long as their energy is non-
zero. The total state space is the Cartesian product between physical
space and internal energy. The agents differ however in their objective
Fig. 2 | Maximizing future path occupancy leads to high occupancy of
physical space. aGrid-world arena. The agents have nine available actions (arrows,
and staying still) when alive (internal energyE larger than zero) and away from
walls. There are four rooms, each with a small food source in a corner (green
diamonds).b Probability of visited spatial states for an MOP agent, anϵ-greedy
reward (R) agent that survives as long as the MOP agent, and a random walker. Food
gain = 10 units, maximum reservoir energy = 100, episodes of 5 × 10
4 time steps,
and (α, β) = (1, 0) for the MOP agent. All agents are initialized in the middle of the
lower left room.c Optimal value functionV*(s) over locations when energy isE =5 .
Black arrows represent the optimal policy given by Eq. (6) ;t h e i rl e n g t hi s
proportional to the probability of each action. The size of red dots is proportional
to the probability of the“stay” action.d Fraction of locations of the arena visited at
least once per episode as a function of food gain. Error bars correspond to s.e.m
over 50 episodes.e Noisy room problem. The bottom right room of the arena was
noisy, such that agents in this room jumped randomly to neighboring locations
regardless of their actions. Food gain equals maximum reservoir energy = 100.
Histogram of visited locations for an episode as long as in (b) for a MOP agent with
β = 0.3 (left) and time fraction spent in the noisy room (right) show that MOP agents
with β > 0 can either be attracted to the room or repelled depending onγ.
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 4
function. The MOP agent has a reward-free objective and implements
MOP by maximizing path action entropy, Eq. (2). In contrast, the R
agent maximizes future discounted reward (in this case, food), and
displays stochastic behavior through anϵ-greedy action selection, with
ϵ matched to the survival of the MOP agent (Supplemental Sec. E and
Fig. E.2a). Finally, the RW is simply an agent that in each state takes a
uniformly random action from the available actions at that state.
We ﬁnd that the MOP agent generates behaviors that can be
dubbed goal-directed and curiosity-driven (Supplementary Movie 1).
First, by storing enough energy in its reservoir, the agent reaches far,
entering the four rooms in the long term (Fig.2b, left panel), and
visiting every location of the arena except when food gain is small
(Fig. 2d, blue line). In contrast, the R agent lingers over one of the food
sources for most of the time (Fig.2b, middle panel; Supplementary
Movie 1). Although its ϵ-greedy action selection allows for brief
exploration of other rooms, the R agent does not on average visit the
whole arena (Fig.2d, orange line). Finally, the RW dies before it has
time to visit a large fraction of the physical space (Fig.2b, right panel).
These differences hold for a large range of food gains (Fig.2d). The
MOP agent, while designed to generate variability, is also capable of
deterministic behavior: when its energy is low, it moves toward the
food sources with little to no variability, a distinct mark of goal-
directedness (Fig.2c, corner spots show that only one action is con-
sidered by optimal policy).
We next considered a slightly more complex environment where
actions in one of the rooms lead to uniformly stochastic transitions to
any of the neighboring locations (noisy room— a spatial version of the
noisy TV problem
66,82). A stochastic region in the environment can
reﬂect uncertainty about this region (e.g., due to agents with limited
resources) or “true” noise in the environment. Regardless of the
source, a MOP agent with β ≠ 0 will exhibit risk-sensitivity, i.e.,
preference or avoidance of this uncertainty15. In particular, MOP agents
withβ >0( s e eE q .(2)) have a preference for stochastic state transitions
and a priori they could get attracted and stuck in the noisy room,
where actions do not have any predictable effect. Indeed, we see that
for largerβ, which measures the strength of the state entropy con-
tribution to the agent’s objective, the attraction to the noisy room
increases (Fig.2e, right panel). However, MOP agents also care about
future states, and thus getting stuck in regions where energy cannot be
predictably obtained is avoided by sufﬁciently long-sighted agents, as
shown by the reduction of the time spent in the noisy room with
increasing γ (Fig. 2e; Supplemental Sec. E.3). This shows how MOP
agents can tradeoff immediate with future action-state occupancy.
Hide and seek in a prey-predator interaction
More interesting behaviors arise from MOP in increasingly complex
environments. To show this, we next considered a prey and a predator
i nag r i dw o r l dw i t has a f ea r e a( a“home”) and a single food source
(Fig. 3a). The prey (a“mouse”, gray up triangle) is the agent whose
behavior is optimized by maximizing future action path entropy, while
the predator (a“cat”, red down triangle) acts passively chasing the prey.
The state of the agent consists of its location and energy level, but it also
includes the predator’s location being accurately perceived. The prey
can move as in the previous 4-room grid world and it also has aﬁnite
energy reservoir. For simplicity, we only considered a food gain equal to
the size of the energy reservoir, such that the agent fully replenishes its
reservoir each time it visits the food source. The predator has the same
available actions as the agent and is attracted to it stochastically, i.e.,
actions that move the predator towards the agent are more probable
t h a nt h o s et h a tm o v ei ta w a yf r o mi t( S u p p l e m e n t a lS e c .E . 4 ) .
MOP generates complex behaviors, not limited to visiting
the food source to increase the energy buffer and hide at home. In
Fig. 3 | Complex hide-and-seek and escaping strategies in a prey-predator
example. aGrid-world arena. The agent has nine available actions when alive and
far from walls. There is a small food source in a corner (green diamond). A predator
(red, down triangle) is attracted to the agent (gray, up triangle), such that when they
are at the same location, the agent dies. The predator cannot enter the locations
surrounded by the brown border. Arrows show a clockwise trajectory.b Histogram
of visited spatial states across episodes for the MOP and R agents. The vectorﬁeld
at each location indicates probability of transition at each location. Green arrows on
R agent show major motion directions associated with its dominant clockwise
rotation.c Fraction of clockwise rotations (as in (a)) to total rotations as a function
of food gain, averaged over epochs of 500 timesteps. Error bars are s.e.m.
d Optimal value functions for different energy levels and same predator position;
black arrows indicate optimal policy, as in Fig.2c.
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 5
particular, the agent very oftenﬁrst teases the cat and then performs
a clockwise rotation around the obstacle, which forces the cat to chase
it around, leaving the food source free for harvest (Fig.3a, arrows
show an example; Supplementary Movie 2, MOP agent). Importantly,
this behavior is not restricted to clockwise rotations, as the agent
performs an almost equal number of counterclockwise rotations to
free the food area (Fig.3c, MOP agent, blue line). The variability of
these rotations in the MOP agent is manifest in the lack of virtually any
preferred directionality of movement in the arena at any single posi-
tion. Indeed, arrows pointing toward several directions indicate that
on average the mouse moves following different paths to get to the
food source (Fig.3b, MOP agent). Finally, the optimal value function
and optimal policy show that the MOP agent can display deterministic
behaviors as a function of internal state as well as distance to the cat
(Fig. 3d): for instance, it prefers running away from the cat when
energy is large (right), and it risks getting caught to avoid starvation if
energy is small (left), both behaviors starkly opposite to stochastic
actions.
The behavior of the MOP agent was compared with an R agent
that receives a reward of 1 each time it is alive and 0 otherwise. To
promote variable behavior in this agent as well, we implemented anϵ-
greedy action selection (Supplemental Sec. E.4), whereϵ was chosen to
match the average lifetime of the MOP agent (Supplemental Fig. E.2b).
The behavior of the R agent was strikingly less variable than that of the
MOP agent, spending more time close to the food source (Fig.3b, R
agent). Most importantly, while the MOP agent performs an almost
equal number of clock and counterclockwise rotations, the R agent
strongly prefers the clockwise rotations, reaching 90% of all observed
rotations (Supplementary Movie 3, R-agent; Fig.3c, orange line). This
shows that the R agent mostly exploits only one strategy to survive and
displays a smaller behavioral repertoire than the MOP agent.
Dancing in an entropy-seeking cartpole
In the previous examples, complex behaviors emerge as a con-
sequence of the presence of obstacles, predators, and limited food
sources, but the actual dynamics of the agents are very coarse-grained.
Here, we considered a system with physically realistic dynamics, the
balancing cartpole
83,84, composed of a moving cart with an attached
pole free to rotate (Fig.4a). The cartpole is assumed to reach an
absorbing state when either it hits a border, or when the pole angle
exceeds 36°. Thus, we consider a broad range of angles that makes the
agents reach a larger state space than in standard settings
85. We dis-
cretized the state space and used a linear interpolation to solve for the
optimal value function in Eq. (4), and to implement the optimal policy
in Eq. (6) (Supplemental Sec. E.5). The MOP agent widely occupies the
horizontal position, and more strikingly it produces a wide variety of
pole angles, constantly swinging sideways as if it were dancing (Sup-
plementary Movie 4, MOP agent; Fig.4b, c).
We compared the behavior of an MOP agent with that of an R
agent that receives a reward of 1 for being alive and 0 otherwise. The R
agent gets this reward regardless of the pole angle and cart position
within the allowed broad ranges, so that behaviors of the MOP and R
agents can be better compared without explicitly favoring in any of
them any speciﬁc behavior, such as the upright pole position. As
expected, the R agent maintains the pole close to the balanced posi-
tion throughout most of a long episode (Fig.4b, bottom), because it is
the furthest to the absorbing states and thus the safest. Therefore, the
R agent produces very little behavioral variability (Fig.4c, right panel)
and no movement that could be dubbed“dancing” (Supplementary
Movie 4, R agent). Although both MOP and R agents use a similar
strategy which keeps the pole pointing towards the center for sub-
stantial amounts of time (Fig.4c, positive angles correlate with positive
positions in both panels), the behavior of the R agent is qualitatively
Fig. 4 | Dancing of a MOP cartpole. aThe cart (brown rectangle) has a pole
attached. The cartpole reaches an absorbing state if the magnitude of the angleθ
exceeds 36° or its position reaches the borders. There areﬁve available actions
when alive: a big and a small force to either side (arrows on cartpole) and doing
nothing (full circle).b Time-shifted snapshots of the pole in the reference frame of
the cart as a function of time for the MOP (top) and R (bottom) agents.c Position
and angle occupation for a 2 × 105 time step episode.d Here, the right half of the
arena is stochastic, while the left remains deterministic. In the stochastic half, the
intended state transition due to an applied action (force) succeeds with probability
1 − η (and thus zero force is applied with probabilityη). e Fraction of time spent on
the right half of the arena increases as a function ofβ, regardless of the failure
probabilityη. f The fraction has a non-monotonic behavior as a function ofη when
state entropy is important for the agent (β = 1), highlighting a stochastic resonance
behavior. When the agents do not seek state entropy (β = 0) the fraction of time
spent by the agent on the right decreases with the failure probability, and thus they
avoid the stochastic right side.γ = 0.99 for (e, f).
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 6
different and is best described as a bang-bang sort of control for which
the angle is kept very close to zero while the cart is allowed to travel
and oscillate around the origin, which is more apparent in the actual
paths of the agent (see trajectories in phase space in Supplementary
Movie 5). We alsoﬁnd that the R agent does not display much varia-
bility in state space even after using anϵ-greedy action selection
(Supplemental Fig. E.3, Supplementary Movie 6), withϵ chosen to
match average lifetimes between agents (Supplemental Fig. E.2c). This
result showcases that the MOP agent exhibits the most appropriate
sort of variability for a given average lifetime.
We ﬁnally introduced a slight variation to the environment, where
the right half of the arena has stochastic state transitions, to showcase
the ability of MOP to model risk-sensitive agents. Here, when agents
choose an action (force) to be executed, a state transition in the
desired direction occurs with probability 1− η, and a transition corre-
sponding to zero force occurs with probabilityη (Fig. 4d). Therefore, a
MOP agent that seeks state entropy (β > 0) will show a preference for
the right side, where there is in principle higher state entropy resulting
from the stochastic transitions over more successor states than on the
left side. Indeed, weﬁnd that MOP agents spend more time on the right
side asβ increases, regardless of the probabilityη (Fig. 4e). Forﬁxed γ,
spending more time on the right side can bring the life expectancy to
decrease signiﬁcantly depending onβ and η (Supplemental Fig. E.2d,
e). Interestingly, forβ > 0 there is an optimal value of the noiseη that
maximizes the fraction of time spent on the right side (Fig.4f), which is
a form of stochastic resonance. Therefore, for differentβ,q u a l i t a t i v e l y
different behaviors emerge as a function of the noise levelη.
MOP agents can also seek entropy of others
Next, we considered an example where an agent seeks to occupy path
space, which includes another agent’sl o c a t i o na sw e l la si t so w n .T h e
agent can freely move (Fig.5a; gray triangle) and open or close a fence
by pressing a lever in a corner (blue triangle). The pet of the agent
(green triangle) can freely move if the fence is open, but when the
fence is closed the pet is conﬁned to move in the region where it is
currently located. The pet moves randomly at each step, but its
available actions are restricted by its available space (Supple-
mental Sec. E.6).
To maximize action-state path entropy, the agent ought to tra-
deoff the state entropy resulting from letting the pet free with the
action entropy resulting from using the open-close action when visit-
ing the lever location. The optimal tradeoff depends on the relative
strength of action and state entropies. In fact, when state entropy
weighs as much as action entropy (α = β = 1), the fraction of time that
the agent leaves the fence open is close to 1 (rightmost point in Fig.5b)
so that the pet is free to move (Fig.5c, right panel;β =1M O Pa g e n t ) .
However, when the state entropy has zero weight (α =1 ,β =0 ) , t h e
fraction of time that the fence remains open is close to 0.5 (leftmost
point in Fig.5b), and the pet remains conﬁned to the right side for most
o ft h et i m e( F i g .5c, left panel;β = 0 MOP agent), the region where it
was initially placed. As a function ofβ the fraction of time the fence is
open increases. Therefore, the agent gives more freedom to its pet, as
measured by the pet’s state entropy, by curtailing its own action
freedom, as measured by action entropy, thus becoming more
“altruistic”.
MOP compared to other reward-free approaches
One important question is how MOP compares to other reward-free,
motivation-driven theories of behavior. Here we focus on two
popular approaches: empowerment and the FEP. In empowerment
(MPOW)
20,73–75 agents maximize the mutual information betweenn-
step actions and the successor states resulting from them20,86,am e a -
sure of their capability to perform diverse courses of actions with
predictable consequences. MPOW formulates behavior as greedy
maximization of empowerment
20,73, such that agents move to acces-
sible states with the largest empowerment (maximal mutual informa-
tion), and stay there with high probability.
We applied MPOW to the grid world and cartpole environments
(Fig. 6). In the gridworld, MPOW agents (5-step MPOW, see Supple-
mental Sec. G.1) prefer states from where they can reach many distinct
states, such as the middle of a room. However, due to energetic con-
straints, they also gravitate towards the food source when energy is
low, and they alternate between these two locations ad nauseam
(Fig. 6a, middle; Supplementary Movie 7). In the cartpole, MPOW
agents (3-step MPOW
73, see Supplemental Sec. G.1) favor the upright
position because, being an unstableﬁxed point, it is the state with
highest empowerment, as previously reported73,87. Given the unstable
equilibrium, the MPOW agent gets close to it but needs to con-
tinuously adjust its actions when greedily maximizing empowerment
(Fig. 6b, middle; Supplementary Movie 8). The paths traversed by
MPOW agents in state space are highly predictable, and they are similar
to the ones of the R agent (see Fig.4c). The only source of stochasticity
comes from the algorithm, which approximately calculates empow-
erment, and thus a more precise estimation of empowerment leads to
even less variability.
In the FEP, agents seek to minimize the negative log probability,
called surprise, of a subset of desired states via the minimization of an
upper bound, called free energy. This minimization reduces behavioral
richness by making a set of desired (homeostatic) states highly
likely
76,77, rendering this approach almost opposite to MOP. In a recent
MDP formalization, FEP agents aim to minimize the (future) expected
free energy (EFE)
88, which equals the future cumulative KL divergence
between the probability of states and the desired (target) probability
of those states (see Supplemental Sec. G.2 for details). Even though this
Fig. 5 | Modeling altruism through an optimal tradeoff between own action
entropy and other’s state entropy. aAn agent (gray up triangle) has access to nine
movement actions (gray arrows and doing nothing) and opens or closes a fence
(dashed blue lines). This fence does not affect its movements. A pet (green, down
triangle) has access to the same actions and chooses one randomly at each
timestep, but it is constrained by the fence when closed. Pet location is part of the
state of the agent.b As β in Eq. (2) is increased, the agent tends to leave the fence
open for a larger fraction of time. This helps its pet reach other parts of the arena.
Error bars correspond to s.e.m.c Occupation heatmaps for 2000 timestep-
episodes forβ = 0 (left) andβ = 1 (right). In all casesα =1 .
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 7
objective contains the standard exploration entropy term on state
transitions77,89, we prove that the optimal policy is deterministic (see
Supplemental Sec. G.2).
As a consequence, weﬁnd that in both the gridworld and cartpole
environments, the behavior of the EFE agent (receding horizon
H = 200) is much less variable than the MOP agent in general (Fig.6a,
right panel for the gridworld, Supplementary Movie 7; and b, right
panel, for the cartpole, Supplementary Movie 8). The only source of
action variability in the EFE agent is due to the degeneracy of the EFE,
and thus behavior collapses to a deterministic policy as soon as the
target distribution is not perfectly uniform (see Supplemental
Sec. G.2.2 for details). Weﬁnally prove that under discounted inﬁnite
horizon and assuming a deterministic environment, the EFE agent is
equivalent to a classical reward maximizer agent with rewardR =1f o r
all non-absorbing states andR = 0 for the absorbing states (Supple-
mental Sec. G.2). In conclusion, MOP generates much more variable
behaviors than MPOW and FEP.
MOP in continuous and large action-state spaces
The examples so far can be solved exactly with numerical methods,
without relying on function approximation of the value function or the
policy, which could obscure the richness of the resulting behaviors.
However, one important question is whether our approach scales up to
large continuous action-state spaces where no exact solutions are
available. To show that MOP generates rich behaviors even in high-
dimensional agents, we simulated a quadruped via MuJoCo
90 from
Gymnasium91 without imposing any explicitﬁne-tuned reward func-
tion (Fig.7a). The only externally imposed conditions are the absorb-
ing states, which are reached when either the agent falls (given by the
torso touching the ground), or the torso reaches a maximum height
91.
We ﬁrst trained the MOP agent by approximating the state-value
function, Eq. (5), using the soft-actor critic (SAC) architecture49 with
zero rewards, which corresponds to the caseα =1a n dβ =0 .T h eM O P
agent learns to stabilize itself and walk around, sometimes jumping,
spinning, and moving up and down the legs, without any instructions
to do so (Supplementary Movie 9). The MOP agent exhibits variable
and long excursions over state space (Fig.7b, c blue) and displays a
broad distribution of speeds (Fig.7d, blue). We compared the MOP
agent with an R agent that obtains a reward ofR = 1 whenever it is alive
and R = 0 when it reaches an absorbing state. As before, we add
variability to the R agent with anϵ-greedy action selection, adjustingϵ
so that the average lifetime of the R agent matched that of the MOP
agent (Supplemental Fig. E.4a). In contrast to the MOP agent, the R
agents exhibit much shorter excursions (Fig.7b, c yellow) and a velo-
city distribution that peaks around zero, indicating prolonged periods
spent with no translational movement (Fig.7d, yellow). When visually
compared, the behavior for MOP and R agents shows stark differences
(Supplementary Movie 9).
While the MOP agent elicits variable behaviors, it is also capable of
generating deterministic, goal-directed behaviors when needed. To
show this, we added a food source in the arena and extended the state
of the agent with its internal energy. Now the agent can also die of
starvation when the internal energy hits zero (Fig.7e). As expected,
when the initial location of the MOP quadruped is far from the food
source, it directly moves to the food source to avoid dying from
starvation (Fig.7f). After the food source is reached for theﬁrst time,
the MOP quadruped generates random excursions away from the food
source. During these two phases, the agent displays very different
speed distributions (Fig.7g), showing also quantitative differences in
the way it moves (see a comparisonwith the R agent in Supplemental
Fig. E.4, and Supplementary Movie 10).
Finally, we modiﬁed the environment by adding state transition
noise of various magnitudes in one half of the arena (x > 0), while the
other half remained deterministic. Weﬁnd that the agent’s behavior is
modulated by β, which controls the preference of state transition
entropy (see details in Supplemental Sec. E.7). As expected, forﬁxed α
and positive noise magnitude, MOP agents show increasing preference
toward the noisy side asβ increases (Supplemental Fig. E.5). However,
as noise magnitude increases, and forﬁxed β, MOP agents tend to
avoid the noisy side to prevent them from falling. This shows that MOP
agents can exhibit approach and avoidance behaviors depending on
the environment’s stochasticity and theirβ hyperparameter.
Discussion
Often, the success of agents in nature is not measured by the amount
of reward obtained, but by their ability to expand in state space and
perform complex behaviors. Here we have proposed that a major goal
of intelligence is to“occupy path space”. Extrinsic rewards are thus the
means to move and occupy action-state path space, not the goal of
behavior. In an MDP setting, we have shown that the intuitive notion of
path occupancy is captured by future action-state path entropy, and
we have proposed that behavior is driven by the maximization of this
Fig. 6 | Empowerment (MPOW) and Free Energy Principle (EFE) lack robust
occupation of action-states. aIn the grid-world environment, MPOW and
expected free energy (EFE) only visit a restricted portion of the arena. Initial
position was the center of a room (x, y)=( 3 ,3 ) .b In the cartpole environment, both
MPOW and EFE shy away from large angles, producing a limited repertoire of
predictable behaviors.
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 8
intrinsic goal— the MOP. We have solved the associated Bellman
equation and provided a convergent iterative map to determine the
optimal policy. In several discrete and continuous state examples, we
have shown that MOP, along with the agent’s constraints and dynam-
ics, leads to complex behaviors that are not observed in other simple
reward-maximizing agents. Quickﬁlling of physical space by a moving
agent, hide-and-seek behavior and variable escaping routes in a
predator-prey example, dancing in a realistic cartpole dynamical sys-
tem, altruistic behavior in an agent-and-pet duo and successful, vig-
orous movement in a high-dimensional quadruped are all behaviors
that strike as being playful, curiosity-driven and energetic.
To the human eye, these behaviors look genuinely goal-directed,
like approaching the food source when the energy level is low or
escaping from the cat when it gets close to the mouse (see
Figs. 2ca n d3d). Although MOP agents do not have any extrinsically
designed goal, like eating or escaping, they generate these determi-
nistic, goal-directed behaviors whenever necessary so that they can
keep moving in the future and maximize future path action-state
entropy (see Supplemental Sec. H). These results show that the pre-
sence of internal states (e.g., energy) and absorbing states (e.g., having
zero energy or being eaten) are critical for generating interesting
behaviors, as getting close to different types of absorbing states trig-
gers qualitatively different behaviors. This capability of adapting
variability depending on internal states has been overlooked in the
literature and is essential to obtaining the goal-directed behaviors we
have shown here.
While all reward-maximizing principles of behavior predict
that learning reduces variability and it collapses to zero once learning
is deemed to have ﬁnished (i.e., there always exists an optimal
deterministic policy
92), MOP predicts that behavioral stochasticity
persists even after learning (i.e., optimal policies under MOP are
non-deterministic). This would imply that noise is promoted as long
as it does not compromise the functioning of the agent, even
after extensive experience with an environment. This prediction is
supported by observations across different living organisms and at
different levels of organization, including perception, behavior,
neural circuits, and individual neurons. At the perceptual level, a
paradigmatic example of the ubiquity of stochasticity is multistable
perception, observed in humans
93 and other animals94–97, a phenom-
enon consisting of a stochastic alternation between percepts that
never stops, even though stimuli are simple and familiar. At
the behavioral level, stochastic actions are observed in all kinds of
living organisms. For example, foraging behavior in bacteria,
plants, and animals is well-described by random walks and Lévy
ﬂights
30,98,99, which remain valid descriptions even in familiar envir-
onments. Further, mice are found to perform random choices from
time to time in simple binary tasks even after extended periods of
training
32. Neuroanatomically, emerging evidence indicates that spe-
ciﬁc neural circuits play a crucial role in generating behavioral varia-
bility. A striking example is the avian basal ganglia–forebrain circuit,
which modulates the variability of the birds’ songs100,101,e v e na f t e r
learning. Indeed, adult male birds with learned motor skills exhibit a
higher level of variability when they sing alone, compared to during
courtship
102,103. At the neural level, single-neuron responses across
sensory and motor areas are highly variable, and this variability per-
sists after long periods of stimulation and adaptation
104–106.A l lt h i s
evidence points out that, at least in part, behavior and its associated
neural circuits and components are stochastic, regardless of the state
of learning.
Not only do organisms seem to generate variability, but they are
also sensitive to the state-transition uncertainty in or about the
environment, a phenomenon known as risk sensitivity. Risk-seeking
Fig. 7 | MOP in high-dimensional states generates variable and goal-directed
behaviors. aThe quadruped environment, adapted from Gymnasium, serves as the
testing environment. Thex, y dimensions are unbounded.b Trajectories of the
center of mass of the torso of the MOP (left panel) and R (right) agents. MOP
occupies more space for approximately the same survival time (see Supplemental
Fig. E.4a). Distribution of the planar distanced from the origin (c) and planar speed
v
xy (d) for MOP (blue) and R (yellow) agents.e In a new environment, a food source
(green ball) is available so that the MOP agent can replenish its internal energy to
avoid starvation.f Trajectories of the MOP agent before (left) and after (right)
getting to the food source. Colormap deﬁned by the energy level of the agent.
g Distribution of the planar speed showcasing changes before (dark blue) and after
(light blue) the MOP agent reaches the food source for theﬁrst time. Distributions
computed only on the tests where the quadrupedﬁnds the food source.
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 9
and risk-aversive behaviors are both found in humans33,107, non-human
primates34,108, rodents, birds and other species38. While other works
have looked at risk-sensitivity in RL, it is commonly associated to
sensitivity to variations in extrinsic rewards
36,37.I nc o n t r a s t ,M O Pc a n
model risk-sensitivity by weighing the state-transition entropy withβ.
By adding stochastic regions to otherwise noiseless environments
(four-room gridworld, cartpole, and quadruped), we showed that MOP
can model risk-sensitive agents in the absence of extrinsic reward. For
example, for the right combination of state entropy weightβ,a n d
lookahead horizon, controlled byγ, MOP agents could get stuck in a
noisy TV, consistent with the observation that humans have a pre-
ference for noisy TVs under particular conditions
109.H o w e v e r ,i tc a n
also capture the avoidance of noisy TVs for sufﬁciently long sighted
agents (see Fig.2e). These observations and results show that MOP
could provide a uniﬁed account for the stochastic nature of behavior
and its risk sensitivity, offering the opportunity to model such see-
mingly disparate set of observations by modulating its action and state
entropy contributions.
A set of algorithms related to MOP, known as empowerment, has
also proposed using reward-free objectives as the goal of
behavior
20,73,75. In this approach, the mutual information between a
sequence of actions and theﬁnal state is maximized. This makes
empowerment agents prefer states where actions lead to large and
predictable changes, such as unstableﬁxed points
73.W eh a v es h o w n
that one drawback is that empowered agents tend to remain close to
those states without producing diverse behavioral repertoires (see
Fig. 6b and Supplementary Movie 8), as it also happens in causal
entropy approaches
110. Another difference is that empowerment is not
additive over paths because the mutual information of a path of
actions with the path of states is not the sum of the per-step mutual
information, and thus it cannot be formalized as a cumulative per-step
objective (Supplemental Sec. I)
73,75,81,111, in contrast to action-state path
entropy. We note, however, that an approximation to empowerment
having the desired additive property could be obtained from our fra-
mework by puttingβ <0 i n E q . (2), such that more predictable state
transitions are preferred. Similarly to empowerment, we have also
shown that agents following the FEP
76,88 collapse behavior to deter-
ministic policies in known environments (see Fig.6b and Supple-
mentary Movie 8). Other reward-free RL settings and pure exploration
o b j e c t i v e sh a v eb e e np r o p o s e di nt h ep a s t
57,59,61,67,112–115, but this body of
work typically investigates how to efﬁciently sample MDPs to con-
struct near-optimal policies when reward functions are introduced in
the exploitation phase. More importantly, this work differs from ours
in that the goal-directedness that MOP displays entails behavioral
variability at its core, even in known environments (see examples
above). Finally, other overlapping reward-free approaches focus on the
unsupervised discovery of skills, by encouraging diversity
26,116–118.W h i l e
the motivation is similar, they focus on skill-conditioned policies,
whereas our work demonstrates that complex sequences of behaviors
are possible working from the primitive actions of agents, although a
possible future avenue for MOP is to apply it to temporally extended
actions
119. In addition, these works deﬁne tasks based on extrinsic
rewards, whereas we have shown that internal state signals are sufﬁ-
cient to let agents deﬁne sub-tasks autonomously.
Our approach is conceptually different as well from hybrid
approaches that combine extrinsic rewards with action entropy or KL
regularization terms
46,47,50,52,120 for two main reasons. First, entropy-
seeking behavior does not pursue any form of extrinsic reward max-
imization. But most importantly, using KL-regularization using a
default policyπ
0(a∣s) in our framework would be self-defeating. This is
because the absolute action entropy termsHðAjsÞ in the expected
return in Eq. (2) favor visiting states where a large set of immediate and
future action-states are accessible. In contrast, using relative action
entropy (KL) precludes this effect by normalizing the number of
accessible actions, as we have shown above. Additionally, minimizing
the KL divergence with a uniform default policy and without extrinsic
rewards leads to an optimal policy that is uniform regardless of the
presence of absorbing states, equivalent to a random walk, which
shows that a pure KL objective does not lead to interesting behaviors
(Supplemental Sec. F, Supplemental Fig. F.6). The idea of having a
variable number of actions that depend on the state is consistent with
the concept of affordance
121. While we do not address the question of
how agents get the information about the available actions, an option
would be to use the notion of affordances as actions
122. Secondly, while
previous work has studied the performance beneﬁts of either action49,
state51,57 or equally weighted action-state53,123 steady-state entropies,
our work proposes mixing them arbitrarily through path entropy,
leading to a more general theory without any loss in mathematical
tractability
56.
We have also shown that MOP is scalable to high-dimensional
problems and when the state-transition matrix is unknown, using the
SAC architecture124 to approximate the optimal policy prescribed by
MOP. Nevertheless, several steps remain to have a more complete
MOP theory with learning. Previous related attempts have introduced
Z-learning
46,79 and G-learning125 using off-policy methods, so our results
could be extended to learning following similar lines. Other possibi-
lities are using transition estimators using counts or pseudo-counts
69,
or hashing70, for the learning of the transition matrices. One potential
advantage of our framework is that, as entropy-seeking behavior
obviates extrinsic rewards, those rewards do not need to be learned
and optimized, and thus the learning problem reduces to transition
matrices learning. In addition, modeling and injecting prior informa-
tion could be particularly simple in our setting in view that intrinsic
entropy rewards can be easily bounded before the learning process if
action space is known. Therefore, initializing the state-value function
to the lower or upper bounds of the action-state path entropy could
naturally model pessimism or optimism during learning, respectively.
All in all, we have introduced MOP as a novel theory of behavior,
which promises new ways of understanding goal-directedness without
reward maximization, and that can be applied to artiﬁcial agents to
discover by themselves ways of surviving and occupying action-
state space.
Methods
Entropy measures the occupancy of action-state paths
We consider a time-homogeneous MDP withﬁnite state setS and ﬁnite
action set AðsÞ for every state s 2 S. Henceforth, the action-state
xj =( aj, sj) is any joint pair of one available actionaj and one possible
successor statesj that results from making that action under policy
π ≡ {π(a∣s)} from the action-statexi =( ai, si). By assumption, the avail-
ability of actionaj depends on the previous statesi alone, not onai.
Thus, the transition probability fromxi to xj in one-time step is
pij = π(aj∣si)p(sj∣si, aj), wherep(sj∣si, ai) is the conditional probability of
transitioning from statesi to sj given that action aj is performed.
Although there is no dependence of the previous actionai on this
transition probability, it is notationally convenient to deﬁne transitions
between action-states.
We conceive of rational agents as maximizing future action-state
path occupancy. Any measure of occupancy should obey the intuitive
Conditions1–4 listed below.
Intuitive Conditions for a measure of action-state path occupancy:
1. Occupancy gain of action-statex
j from xi is a function of the
transition probabilitypij, C(pij)
2. Performing a low probability transition leads to a higher occu-
pancy gain than performing a high probability transition, that is,
C(pij)d e c r e a s e sw i t hpij
3. The ﬁrst order derivativeC0ðpijÞ is continuous forpij ∈ (0, 1)
4. (De ﬁnition: the action-state occupancy of a one-step path from
action-state xi is the expectation over occupancy gains of the
immediate successor action-states,Cð1Þ
i /C17P
jpijCðpijÞ). The action-
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 10
state occupancy of a two-step path is additive, Cð2Þ
i /C17P
jk pijpjk Cðpijpjk Þ = Cð1Þ
i + P
jpijCð1Þ
j for any choice of thepij and
initial xi
Condition 1 simply states that occupancy gain from an initial
action-state is deﬁned over the transition probabilities to successor
action-states in a sample space. Condition2 implies that performing a
low-probability transition leads to a higher occupancy of the successor
states than performing a high-probability transition. This is because
performing a rare transition allows the agent to occupy a space that
was left initially unoccupied. Condition3 imposes smoothness of the
measure.
In Condition4 we have deﬁned the occupancy of the successor
action-states (one-step paths) in the Markov chain as the expected
occupancy gain. Condition4 is the central property, and it imposes
that the occupancy of action-states paths with two steps can be broken
down into a sum of the occupancies of action-states at each time step.
Note that the action-state path occupancy can be written as
C
ð2Þ
i /C17
X
jk
pijpjk Cðpijpjk Þ =
X
j
pijCðpijÞ
+
X
jk
pijpjk Cðpjk Þ =
X
jk
pijpjk CðpijÞ + Cðpjk Þ
/C16/C17
,
which imposes a strong condition on the functionC(p). Note also that
the sum∑jkpijpjkC(pijpjk) extends the notion of action-state to a path of
two consecutive action-states, each path having probabilitypijpjk due
to the (time-homogeneous) Markov property. The last equality is an
identity. While here we consider paths of length equal to 2, in
the Supplementary Information we show that there is no difference in
imposing additivity to paths of anyﬁxed or random length (Corollary
2). The additivity of occupancy over paths is equivalent to time
homogeneity, that is, all times count the same.
Theorem 1. CðpÞ = /C0 k ln p with k > 0 is the only function that satisﬁes
Conditions1–4.
Corollary 1.T h ee n t r o p yC
ð1Þ
i = /C0 kP
jpij ln pij is the only measure of
action-state occupancy of successor action-statesxj from xi with
transition probabilitiespij consistent with Conditions1–4.
S e eS u p p l e m e n t a r yI n f o r m a t i o nf o rt h ep r o o f .W eh a v ef o u n dt h a t
entropy is the measure of occupancy. Shannon’s famous derivation of
entropy as a measure of information78 uses similar elements, but some
differences are worthy to be mentioned. First, our proof uses the
notion of additivity of occupancy on MDPs of length two (our Condi-
tion 4), while Shannon’s notion of additivity uses sequences of random
variables of arbitrary length (his Condition3), and therefore his con-
dition is in a sense stronger than ours. Second, our proof enforces
continuous derivative of the measure, while Shannon enforces con-
tinuity of the measure, rendering our Condition3 stronger. Finally, we
enforce a speciﬁc form of the measure as an average over occupancy
gains (our Condition4 again), because it intuitively captures the notion
of occupancy, while Shannon does not enforce this structure in his
information measure.
Critical values and policies
Theorem 2.T h ec r i t i c a lv a l u e sV c(s) of the expected returnsVπ(s)
in equation (3) with respect to the policy probabilitiesπ ={ π(a∣s): a ∈
A(s), s ∈ S} obey
VcðsÞ = α ln ZðsÞ = α ln
X
a2AðsÞ
exp α/C0 1βHðS0js,aÞ + α/C0 1γ
X
s0
pðs0js,aÞVcðs0Þ
 !"#
ð8Þ
where HðS0js,aÞ = /C0 P
s0 pðs0js,aÞ ln pðs0js,aÞ i st h ee n t r o p yo ft h es u c -
cessors ofs after performing actiona,a n dZ(s) is the partition function.
The critical points (critical policies) are
πcðajsÞ = 1
ZðsÞ exp α/C0 1βHðS0js,aÞ + α/C0 1γ
X
s0
pðs0js,aÞVcðs0Þ
 !
, ð9Þ
one per critical value, where the partition functionZ(s) is the normal-
ization constant.
Note that we simultaneously optimize∣S∣ expected returns, one
per states, each with respect to the set of probabilitiesπ ={ π(a∣s):
a ∈ A(s), s ∈ S}.
Proof.W eﬁrst note that the expected return in Eq. (2)i sc o n t i n u o u s
and has continuous derivatives with respect to the policy except at the
boundaries (i.e.,π(a∣s) = 0 for some action-state (a, s)). Choosing a
state s,w eﬁrst take partial derivatives with respect toπ(a∣s)f o re a c h
a 2 AðsÞ in both sides of Eq. (3) and then evaluate them at a critical
point π
c to obtain the condition
λðs,sÞ =
X
s0
pðs0js,aÞ/C0 lnðπcðajsÞαpβðs0js,aÞÞ + γVcðs0Þ
/C16/C17
/C0 α + γ
X
b,s0
πcðbjsÞpðs0js,bÞλðs0,sÞ
= /C0 α ln πcðajsÞ/C0 β
X
s0
pðs0js,aÞ ln pðs0js,aÞ/C0 α
+ γ
X
s0
pðs0js,aÞVcðs0Þ + γ
X
b,s0
πcðbjsÞpðs0js,bÞλðs0,sÞ,
ð10Þ
where we have deﬁned the partial derivative at the critical point
∂Vπ ðs0Þ
∂πðajsÞ jπc /C17λðs0,sÞ and used the fact that this partial derivative should be
action-independent. To understand this, note that the critical policy
should lie in the simplex∑
aπ(a∣s)=1 , π(a∣s) ≥ 0, and therefore the
gradient ofVπðs0Þ with respect to theπ(a∣s) at the critical policy should
be along the normal to the constraint surface, i.e., the diagonal
direction (hence, action-independent), or be zero. Indeed, the action-
independence of theλðs
0,sÞ also results from interpreting them as
Lagrange multipliers:λðs0,sÞ is the Lagrange multiplier corresponding
to the state-value function ats0, Vπðs0Þ, associated to the constraint
∑aπ(a∣s)=1 , π(a∣s) ≥ 0, deﬁning the simplex where the probabilities
{π(a∣s): a ∈ A(s)} lie.
Noticing that the last term of Eq. (10) does not depend ona,w e
can solve for the critical policyπc(a∣s) to obtain Eq. (9). Eq. (9) implicitly
relates the critical policy with the critical value of the expected returns
from each states. Inserting the critical policy, Eq. (9), into Eq. (3), we
get Eq. (8), which is an implicit non-linear system of equations exclu-
sively depending on the critical values.
It is easy to verify that the partial derivatives ofV
π(s)i nE q .(3)w i t h
respect toπða0js0Þ for s ≠ s0 are
λðs,s0Þ = γ
X
s00
pðs00jsÞλðs00,s0Þ,
and thus they provide no additional constraint on the critical policy.□
We further prove in the Supplementary Information (Theorems 3
and 4) that the critical point is unique, corresponds to the optimal
policy, and that the optimal state-value function can efﬁciently be
obtained from the iterative mapping in Eq. (7).
Deﬁnition of agents
MOP agent. In all the experiments presented, we introduce the MOP
agent. The objective function that this agent maximizes, in general, is
Eq. (2). As described in the manuscript, theα and β parameters control
the weights of action and next-state entropies to the objective func-
tion, respectively, but only their ratio β/α is important. Unless
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 11
indicated otherwise, we always useα =1 ,β = 0 for the experiments. It is
important to note that if the environment is deterministic, then the
next-state entropyHðS
0js,aÞ = /C0 P
s0 pðs0js,aÞ ln pðs0js,aÞ =0 ,a n dt h e r e -
fore β does not change the optimal policy, Eq. (6).
The number of actions and state transitions available can depend
on thes. In particular, we consider absorbing statess+, where only the
“stay” action available. This construction leads to states with zero
action and state transition entropies, and thus absorbing states have
zero state-valueV
π(s+) = 0. While a reward function (see next section)
tells an agent what needs to be accomplished, absorbing states tell a
MOP agent what not to do. Thus, to occupy action-state path space, a
MOP agent fabricates by itself a variety of goals and subgoals that are
solely constrained by the presence of absorbing states and the
environment-agent dynamics.
We have implemented the iterative map, Eq. (7), to solve for the
optimal value, usingz
ð0Þ
i =1 f o r a l li as initial condition. Theorem (3)
ensures that this iterative mapﬁn d sau n i q u eo p t i m a lv a l u er e g a r d l e s s
of the initial condition in theﬁrst orthant. To determine a degree of
convergence, we compute the supremum norm between iterations,
δ =m a xijVðn +1 Þ
i /C0 VðnÞ
i j,
where Vi = α
γ logðziÞ, and the iterative map stops whenδ <1 0−3.
Ra g e n t. We also introduce a reward-maximizing agent in the usual
RL sense. In this case, the reward isr =1f o rl i v i n ga n dr =0w h e nd y i n g .
In other words, this agent maximizes life expectancy. Additionally, to
emphasize the typical reward-seeking behavior and avoid degenerate
cases induced by the tasks, we introduced a small reward for the Four-
room grid world (see Supplementary Information for details of the
environments). In all other aspects, the modeling of the R agent is
identical to the MOP agent. To allow for reward-maximizing agents to
display some stochasticity, we used anϵ-greedy policy, the best in the
family of ϵ-soft policies
14. At any given state, a random admissible
action is chosen with probabilityϵ, and the action that maximizes the
value is chosen with probability 1− ϵ. Given that the world models
pðs0js,aÞ are known and the environments are static, thisϵ-greedy
policy does not serve the purpose of exploration (in the sense of
learning), but only to inject behavioral variability. Therefore, we con-
struct an agent with state-independent variability, whose value func-
tion satisﬁes the optimality Bellman equation for thisϵ-greedy policy,
VϵðsÞ = ð1 /C0 ϵÞmaxa
X
s0
pðs0js,aÞ r + γVϵðs0Þ
/C0/C1
+ ϵ
jAðsÞj
X
a,s0
pðs0js,aÞ r + γVϵðs0Þ
/C0/C1
,
ð11Þ
where jAðsÞj is the number of admissible actions at states.T os o l v ef o r
the optimal value in this Bellman equation, we perform value
iteration14.T h eϵ-greedy policy for the R agent is therefore given by
πðajsÞ =
1 /C0 ϵ + ϵ
jAðsÞj ,i f a = arg maxa0
P
s0 pðs0js,a0Þ r + γVϵðs0Þ
/C0/C1
ϵ
jAðsÞj ,o t h e r w i s e
(
where ties in arg max are broken randomly. Note that ifϵ = 0, we obtain
the usual greedy optimal policy that maximizes reward. The parameter
ϵ is typically chosen so that average lifetimes for both R and MOP
agents are matched.
Reporting summary
Further information on research design is available in the Nature
Portfolio Reporting Summary linked to this article.
Data availability
The data generated in this study have been generated through a cus-
tom code126. The speciﬁc quadruped data analyzed in this study are
available under request, although an equivalent dataset can be gen-
erated with the code provided.
Code availability
T h ec o d et og e n e r a t et h er e s u l t sa n dv a r i o u sﬁg u r e si sa v a i l a b l ea sP y t h o n
and Julia code along with guided notebooks to reproduce theﬁgures126.
References
1. Ryan, R. M. & Deci, E. L. Intrinsic and extrinsic motivations: classic
deﬁnitions and new directions.Contemp. Educ. Psychol.25,
54–67 (2000).
2 . O u d e y e r ,P . - Y . ,K a p l a n ,F .&H a f n e r ,V .V .I n t r i n s i cm o t i v a t i o ns y s -
tems for autonomous mental development.IEEE Trans. Evolut.
Comput. 11,2 6 5–286 (2007).
3 . A d o l p h ,K .E .&B e r g e r ,S .E .M o t o rd e v e l o p m e n t .i nHandbook of
Child PsychologyVol. 2. (Wiley Online Library, 2007).
4. MacNeilage, P. F. & Davis, B. L. On the origin of internal structure of
word forms.Science 288,5 2 7–531 (2000).
5 . P e t i t t o ,L .A .&M a r e n t e t t e ,P .F .B a b b l i n gi nt h em a n u a lm o d e :
evidence for the ontogeny of language.Science 251,1 4 9 3–1496
(1991).
6. Dietrich, A. The cognitive neuroscience of creativity.Psychon. Bull.
Rev. 11,1 0 1 1–1026 (2004).
7. Kidd, C. & Hayden, B. Y. The psychology and neuroscience of
curiosity.Neuron 88,4 4 9–460 (2015).
8. Gottlieb, J., Oudeyer, P.-Y., Lopes, M. & Baranes, A. Information-
seeking, curiosity, and attention: computational and neural
mechanisms.Trends Cogn. Sci.17,5 8 5–593 (2013).
9. Gittins, J., Glazebrook, K. & Weber, R.Multi-Armed Bandit Alloca-
tion Indices(John Wiley & Sons, 2011).
10. Averbeck, B. B. Theory of choice in bandit, information sampling
and foraging tasks.PLoS Comput. Biol.11, e1004164 (2015).
11. Doll, B. B., Simon, D. A. & Daw, N. D. The ubiquity of model-based
reinforcement learning.Curr. Opin. Neurobiol.22,1 0 7 5–1081
(2012).
12. Wang, M. Z. & Hayden, B. Y. Latent learning, cognitive maps, and
curiosity.Curr. Opin. Behav. Sci.38,1 –7 (2021).
13. Von Neumann, J. & Morgenstern, O.Theory of Games And Eco-
nomic Behavior(Princeton University Press, 2007).
14. Sutton, R. S. et al.Introduction to Reinforcement Learning(MIT
Press Cambridge, 1998).
15. Kahneman, D. & Tversky, A. Prospect theory: an analysis of deci-
sion under risk. inHandbook of the Fundamentals of Financial
Decision Making: Part I99–127 (World Scientiﬁc, 2013).
16. Silver, D., Singh, S., Precup, D. & Sutton, R. S. Reward is enough.
Artif. Intell.299,1 0 3 5 3 5( 2 0 2 1 ) .
17. Rash, C. J., Weinstock, J. & Van Patten, R. A review of gambling
disorder and substance use disorders.Subst. Abus. Rehabil.7,
3( 2 0 1 6 ) .
18. Ágh, T. et al. A systematic review of the health-related quality of
life and economic burdens of anorexia nervosa, bulimia nervosa,
and binge eating disorder.Eat. Weight Disord.-Stud. Anorex.,
Bulim. Obes.21,3 5 3–364 (2016).
19. McNamara, J. M. & Houston, A. I. The common currency for
behavioral decisions.Am. Nat.127,3 5 8–378 (1986).
2 0 . K l y u b i n ,A .S . ,P o l a n i ,D .&N e h a n i v ,C .L .E m p o w e r m e n t :a
universal agent-centric measure of control. InProc. 2005
IEEE Congress on Evolutionary ComputationVol. 1 128–135
(IEEE, 2005).
21. Lehman, J. & Stanley, K. O. Abandoning objectives: evolution
through the search for novelty alone.Evolut. Comput.19,
189–223 (2011).
2 2 . S i n g h ,S . ,L e w i s ,R .L .&B a r t o ,A .G .W h e r ed or e w a r d sc o m ef r o m ?
In Proc. of the Annual Conference of the Cognitive Science Society
2601–2606 (Cognitive Science Society, 2009).
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 12
23. Zhang, T., Rosenberg, M., Zeyu, J., Perona, P. & Meister, M.
Endotaxis: a universal algorithm for mapping, goal-learning,
and navigation.eLife 12, RP84141 (2024).
24. Schmidhuber, J. A possibility for implementing curiosity and
boredom in model-building neural controllers. InProc. of the
International Conference on Simulation of Adaptive Behavior: From
Animals to Animats( e d s .J .A .M e y e r&S .W .W i l s o n )2 2 2–227 (MIT
Press/Bradford Books, 1991).
25. Had ﬁeld-Menell, D., Milli, S., Abbeel, P., Russell, S. J. & Dragan, A.
Inverse reward design. inAdvances in Neural Information Proces-
sing SystemsVol. 30 (University of California, Berkeley, 2017).
26. Eysenbach, B., Gupta, A., Ibarz, J. & Levine, S. Diversity is all you
need: Learning skills without a reward function. In7th International
Conference on Learning Representations,(ICLR, 2019).
27. Schrittwieser, J. et al. Mastering atari, go, chess and shogi by
planning with a learned model.Nature 588,6 0 4–609 (2020).
28. Asafa, T., Afonja, T., Olaniyan, E. & Alade, H. Development of a
vacuum cleaner robot.A l e x .E n g .J .57,2 9 1 1–2920 (2018).
29. Kline, S. J. & Rosenberg, N. An overview of innovation. inStudies
on Science and the Innovation Process: Selected Works of Nathan
Rosenberg173–203 (World Scientiﬁc, 2010).
30. Campeau, W., Simons, A. M. & Stevens, B. The evolutionary
maintenance of Lévyﬂight foraging.PLOS Comput. Biol.18,
e1009490 (2022).
31. Beron, C. C., Neufeld, S. Q., Linderman, S. W. & Sabatini, B. L. Mice
exhibit stochastic and efﬁcient action switching during probabil-
istic decision making.P r o c .N a t lA c a d .S c i .119, e2113961119 (2022).
32. Pisupati, S., Chartarifsky-Lynn, L., Khanal, A. & Churchland, A. K.
Lapses in perceptual decisions reﬂect exploration.eLife 10,
e55490 (2021).
33. Braun, D. A., Nagengast, A. J. & Wolpert, D. Risk-sensitivity in
sensorimotor control. inFrontiers in Human NeuroscienceVol. 5
https://www.frontiersin.org/articles/10.3389/fnhum.2011.00001
(Frontiers, 2011).
34. Hayden, B. Y., Heilbronner, S. R., Nair, A. C. & Platt, M. L. Cognitive
inﬂuences on risk-seeking by rhesus macaques.Judgm. Decis.
Mak. 3,3 8 9–395 (2008).
35. Renart, A. & Machens, C. K. Variability in neural activity and
behavior.Curr. Opin. Neurobiol.25,2 1 1–220 (2014).
36. Fei, Y., Yang, Z., Chen, Y., Wang, Z. & Xie, Q. Risk-sensitive rein-
forcement learning: near-optimal risk-sample tradeoff in regret. in
Advances in Neural Information Processing SystemsVol. 33
22384–22395 https://proceedings.neurips.cc/paper/2020/hash/
fdc42b6b0ee16a2f866281508ef56730-Abstract.html(Curran
Associates, Inc., 2020).
37. Grau-Moya, J. et al. Beyond Bayes-optimality: meta-learning
what you know you don’tk n o whttp://arxiv.org/abs/2209.15618
(2022).
38. Kacelink, A. & Bateson, M. Risky theories— the effects of variance
on foraging decisions1.Am. Zool.36,4 0 2–434 (1996).
39. Wilson, R. C., Bonawitz, E., Costa, V. D. & Ebitz, R. B. Balancing
exploration and exploitation with information and randomization.
C u r r .O p i n .B e h a v .S c i .38,4 9–56 (2021).
40. Moreno-Bote, R., Knill, D. C. & Pouget, A. Bayesian sampling in
visual perception.Proc. Natl Acad. Sci.108,1 2 4 9 1–12496 (2011).
41. Recanatesi, S., Pereira-Obilinovic, U., Murakami, M., Mainen, Z. &
Mazzucato, L. Metastable attractors explain the variable timing
of stable behavioral action sequences.Neuron 110,1 3 9–153
(2022).
42. Corver, A., Wilkerson, N., Miller, J. & Gordus, A. Distinct movement
patterns generate stages of spider web building.Curr. Biol.31,
4983–4997 (2021).
4 3 . D a g e n a i s ,P . ,H e n s m a n ,S . ,H a e c h l e r ,V .&M i l i n k o v i t c h ,M .C .E l e -
phants evolved strategies reducing the biomechanical complexity
of their trunk.Curr. Biol.31,4 7 2 7–4737 (2021).
44. Mochol, G., Kiani, R. & Moreno-Bote, R. Prefrontal cortex repre-
sents heuristics that shape choice bias and its integration into
future behavior.Curr. Biol.31,1 2 3 4–1244 (2021).
45. Cazettes, F., Murakami, M., Renart, A. & Mainen, Z. F. Reservoir of
decision strategies in the mouse brain.bioRxiv. (Cold Spring
Harbor Laboratory, 2021).
46. Todorov, E. Efﬁcient computation of optimal actions.Proc. Natl
Acad. Sci.106, 11478–11483 (2009).
4 7 . Z i e b a r t ,B .D .Modeling purposeful adaptive behavior with the
principle of maximum causal entropy(Carnegie Mellon Uni-
versity, 2010).
48. Haarnoja, T., Tang, H., Abbeel, P. & Levine, S. Reinforcement
learning with deep energy-based policies. InProc. International
Conference on Machine Learning1352–1361 (PMLR, 2017).
49. Haarnoja, T., Zhou, A., Abbeel, P. & Levine, S. Soft actor-critic: off-
policy maximum entropy deep reinforcement learning with a
stochastic actor. InProc. International Conference on Machine
Learning1861–1870 (PMLR, 2018).
50. Schulman, J., Chen, X. & Abbeel,P .E q u i v a l e n c eb e t w e e np o l i c y
gradients and soft q-learning.arXiv preprint arXiv:1704.06440
(2017).
51. Neu, G., Jonsson, A. & Gómez, V. A uniﬁed view of entropy-
regularized Markov decision processes.arXiv preprint
arXiv:1705.07798(2017).
52. Hausman, K., Springenberg, J. T., Wang, Z., Heess, N. & Riedmiller,
M. Learning an embedding space for transferable robot skills.
In Proc. International Conferenceon Learning Representations
(ICLR, 2018).
53. Tishby, N. & Polani, D. Information theory of decisions and actions.
in Perception-action Cycle,6 0 1–636 (Springer, 2011).
54. Nachum, O., Norouzi, M., Xu, K. & Schuurmans, D. Bridging the
gap between value and policy based reinforcement learning. in
Advances in Neural Information Processing SystemsVol. 30 (Cur-
ran Associates Inc., 2017).
55. Galashov, A. et al. Information asymmetry in KL-regularized RL.In
International Conference on Learning Representations
(ICLR, 2019).
56. Grytskyy, D., Ramírez-Ruiz, J. & Moreno-Bote, R. A general Markov
decision process formalism for action-state entropy-regularized
reward maximizationhttp://arxiv.org/abs/2302.01098(2023).
57. Hazan, E., Kakade, S., Singh, K. & Van Soest, A. Provably efﬁcient
maximum entropy exploration. InProc. International Conference
on Machine Learning,2 6 8 1–2691 (PMLR, 2019).
5 8 . L i u ,H .&A b b e e l ,P .B e h a v i o rf r o mt h ev o i d :u n s u p e r v i s e da c t i v e
pre-training.Adv. Neural Inf. Process. Syst.34,1 8 4 5 9–18473
(2021).
5 9 . M u t t i ,M . ,P r a t i s s o l i ,L .&R e s t e l l i ,M .T a s k - a g n o s t i ce x p l o r a t i o nv i a
policy gradient of a non-parametric state entropy estimate. In
Proc. of the AAAI Conference on Artiﬁcial IntelligenceVol. 35,
9028–9036 (2021).
60. Seo, Y. et al. State entropy maximization with random encoders for
efﬁcient exploration. InProc. International Conference on Machine
Learning9443–9454 (PMLR, 2021).
61. Zhang, C., Cai, Y., Huang, L. & Li, J. Exploration by maximizing
Rényi entropy for reward-free RL framework. InProc. of the AAAI
Conference on Artiﬁcial Intelligence Vol.35 10859–10867
(2021).
62. Amin, S., Gomrokchi, M., Satija, H., van Hoof, H. & Precup, D. A
Survey of Exploration Methods in Reinforcement Learning.arXiv
preprint arXiv:2109.00157(2021).
6 3 . B u r d a ,Y . ,E d w a r d s ,H . ,S t o r k e y ,A .&K l i m o v ,O .E x p l o r a t i o nb y
random network distillation. InInternational Conference on
Learning Representations(ICLR, 2018).
64. Achiam, J. & Sastry, S. Surprise-based intrinsic motivation for deep
reinforcement learning.arXiv preprint arXiv:1703.01732
(2017).
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 13
65. Fountas, Z., Sajid, N., Mediano, P. & Friston, K. Deep active infer-
ence agents using Monte-Carlo methods.Adv. neural Inf. Process.
Syst. 33, 11662–11675 (2020).
66. Burda, Y. et al. Large-Scale Study of Curiosity-Driven Learning.
(ICLR, 2019).
67. Pathak, D., Agrawal, P., Efros, A.A. & Darrell, T. Curiosity-driven
exploration by self-supervised prediction. InProc. International
Conference on Machine Learning2778–2787 (PMLR, 2017).
68. Hafner, D. et al. Action and perception as divergence minimization
http://arxiv.org/abs/2009.01791(2022).
69. Bellemare, M. et al. Unifying count-based exploration and intrinsic
motivation.Advances in Neural Information Processing Systems
Vol. 29 (NIPS, 2016).
70. Tang, H. et al. #Exploration: a study of count-based exploration for
deep reinforcement learning. inAdvances in Neural Information
Processing SystemsVol. 30 (2017).
71. Aubret, A., Matignon, L. & Hassas, S. An information-theoretic
perspective on intrinsic motivation in reinforcement learning: a
survey. Entropy 25,3 2 7( 2 0 2 3 ) .
72. Schmidhuber, J. Driven by compression progress: a simple prin-
ciple explains essential aspectsof subjective beauty, novelty,
surprise, interestingness, attention, curiosity, creativity, art, sci-
e n c e ,m u s i c ,j o k e s .I nWorkshop on anticipatory behavior in
adaptive learningsystems, 48–76 (Springer Berlin Heidel-
berg, 2009).
73. Jung, T., Polani, D. & Stone, P. Empowerment for continuous agent
— environment systems.Adapt. Behav.19,1 6–39 (2011).
74. Still, S. & Precup, D. An information-theoretic approach to
curiosity-driven reinforcement learning.Theory Biosci.131,
139–148 (2012).
75. Mohamed, S. & Jimenez Rezende, D. Variational information
maximisation for intrinsically motivated reinforcement learning. in
Advances in Neural Information Processing SystemsVol. 28
(NIPS, 2015).
76. Friston, K., Kilner, J. & Harrison, L. A free energy principle for the
brain. J. Physiol.100,7 0–87 (2006).
77. Buckley, C. L., Kim, C. S., McGregor, S. & Seth, A. K. The free
energy principle for action and perception: a mathematical
review. J. Math. Psychol.81,5 5–79 (2017).
78. Shannon, C. E. A mathematical theory of communication.Bell
Syst. Tech. J.27,3 7 9–423 (1948).
79. Todorov, E. Linearly-solvable Markov decision problems. in
Advances in Neural Information Processing SystemsVol. 19
(NIPS, 2006).
8 0 . R u b i n ,J . ,S h a m i r ,O .&T i s h b y ,N .T r a d i n gv a l u ea n di n f o r m a t i o ni n
MDPs. inDecision Making with Imperfect Decision Makers
,5 7–74
(Springer, 2012).
81. Leibfried, F., Pascual-Díaz, S. & Grau-Moya, J. A Uniﬁed Bellman
Optimality principle combining reward maximization and
empowerment. inAdvances in Neural Information Processing
Systems Vol. 32https://proceedings.neurips.cc/paper_ﬁles/
paper/2019/hash/13384ffc9d8bdb21c53c6f72d46f7866-
Abstract.html(Curran Associates, Inc., 2019).
82. Schmidhuber, J. Curious model-building control systems. InProc.
International Joint Conference on Neural Networks1458–1463
(1991).
8 3 . B a r t o ,A .G . ,S u t t o n ,R .S .&A n d e r s o n ,C .W .N e u r o n l i k ea d a p t i v e
elements that can solve difﬁcult learning control problems. in
Proc. IEEE Transactions on Systems, Man, and Cybernetics
834–846 (IEEE, 1983).
84. Florian, R. V. Correct equations for the dynamics of the cart-pole
system. inCenter for Cognitive and Neural Studies (Coneural)
(Citeseer, Romania, 2007).
85. Brockman, G. et al. Openai gym.arXiv preprint arXiv:1606.01540
(2016).
86. Blahut, R. Computation of channel capacity and rate-distortion
functions.IEEE Trans. Inf. Theory18,4 6 0–473 (1972).
87. Klyubin, A. S., Polani, D. & Nehaniv, C. L. Keep your options open:
an information-based driving principle for sensorimotor systems.
PLOS ONE3,e 4 0 1 8( 2 0 0 8 ) .
8 8 . D aC o s t a ,L . ,S a j i d ,N . ,P a r r ,T . ,F r i s t o n ,K .&S m i t h ,R .R e w a r d
maximization through discrete active inference.Neural Comput.
35,8 0 7–852 (2023).
89. Tschantz, A., Millidge, B., Seth, A. K. & Buckley, C. L. Reinforce-
ment learning through active inference.arXiv preprint
arXiv:2002.12636(2020).
90. Todorov, E., Erez, T. & Tassa, Y. Mujoco: a physics engine for
model-based control. InProc. 2012 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems5026–5033 (IEEE, 2012).
91. Towers, M. et al. Gymnasiumhttps://zenodo.org/record/
8127025 (2023).
92. Puterman, M. L. inMarkov Decision Processes: Discrete Stochastic
Dynamic Programminghttps://books.google.ca/books?hl=en&lr=
&id=VvBjBAAAQBAJ&oi=fnd&pg=PT9&dq=markov+decision
+processes:+discrete+stochastic+dynamic+programming
+pdf&ots=rtgxtNPVQG&sig=ezUHyiSDwVSvQD0mVNk_LqRHTVE
(John Wiley & Sons, 2014).
93. Wheatstone, C. XVIII. Contributions to the physiology of vision.
Part theﬁrst. On some remarkable, and hitherto unobserved,
phenomena of binocular vision.P h i l o s .T r a n s .R .S o c .L o n d .128,
371–394 (1838).
94. Leopold, D., Maier, A. & Logothetis, N. K. Measuring subjective
visual perception in the nonhuman primate.J. Conscious. Stud.10,
115–130 (2003).
95. Pettigrew, J. D., Collin, S. P. & Ott, M. Convergence of specialised
behaviour, eye movements and visual optics in the sandlance
(Teleostei) and the chameleon (Reptilia).Curr. Biol.9,4 2 1–424
(1999).
96. Moreno-Bote, R., Shpiro, A., Rinzel, J. & Rubin, N. Alternation rate in
perceptual bistability is maximal at and symmetric around equi-
dominance.J. Vis.10,1 –1( 2 0 1 0 ) .
9 7 . C a r t e r ,O . ,v a nS w i n d e r e n ,B . ,L e o p o l d ,D .A . ,C o l l i n ,S .&M a i e r ,A .
Perceptual rivalry across animal species.J. Comp. Neurol.528,
3123–3133 (2020).
98. Ariel, G. et al. Swarming bacteria migrate by Lévy Walk.Nat.
Commun. 6,8 3 9 6( 2 0 1 5 ) .
9 9 . R e i j e r s ,V .C .e ta l .AL é v ye x p a n s i o ns t r a t e g yo p t i m i z e se a r l y
dune building by beach grasses.Nat. Commun. 10,2 6 5 6
(2019).
100. Doya, K. & Sejnowski, T. J. A novel reinforcement model of bird-
song vocalization learning. inAdvances in Neural Information
Processing SystemsVol. 7https://proceedings.neurips.cc/paper/
1994/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html
(MIT Press, 1994).
101. Aronov, D. & Fee, M. S. Natural changes in brain temperature
underlie variations in song tempo during a mating behavior.
PLOSOne 7,e 4 7 8 5 6( 2 0 1 2 ) .
102. Kao, M. H. & Brainard, M. S. Lesions of an avian basal ganglia circuit
prevent context-dependent changes to song variability.J. Neu-
rophysiol.96, 1441–1455 (2006).
103. Woolley, S. C. & Doupe, A. J. Social context-induced song variation
affects female behavior and gene expression.PLOS Biol.6,
e62 (2008).
1 0 4 . S o f t k y ,W .R .&K o c h ,C .T h eh i g h l yi r r e g u l a rﬁring of cortical cells
is inconsistent with temporal integration of random EPSPs.J.
Neurosci.13, 334–350 (1993).
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 14
105. Tolhurst, D. J., Movshon, J. A. & Dean, A. F. The statistical reliability
of signals in single neurons in cat and monkey visual cortex.Vis.
Res. 23,7 7 5–785 (1983).
106. Shadlen, M. N. & Newsome, W. T. The variable discharge of cor-
tical neurons: implications forconnectivity, computation, and
information coding.J. Neurosci.18,3 8 7 0–3896 (1998).
107. Fishburn, P. C. & Kochenberger, G. A. Two-piece von
Neumann–Morgenstern utility functions*.Decis. Sci.10,
503–518 (1979).
108. Eisenreich*, B. R., Hayden, B. Y. & Zimmermann, J. Macaques are
risk-averse in a freely moving foraging task.Sci. Rep.9,1 5 0 9 1
(2019).
1 0 9 . M o d i r s h a n e c h i ,A . ,L i n ,W . - H . ,X u ,H .A . ,H e r z o g ,M .H .&G e r s t n e r ,W .
The curse of optimism: a persistent distraction by noveltyhttps://
www.biorxiv.org/content/10.1101/2022.07.05.498835v2(2022).
110. Wissner-Gross, A. D. & Freer, C. E. Causal entropic forces.Phys.
Rev. Lett.110, 168702 (2013).
111. Volpi, N. C. & Polani, D. Goal-directed empowerment: combining
intrinsic motivation and task-oriented behaviour. inIEEE Transac-
tions on Cognitive and Developmental Systems(IEEE, 2023).
1 1 2 . L e e ,L .e ta l .E fﬁcient exploration via state marginal matching.arXiv
preprint arXiv:1906.05274(2019).
113. Jin, C., Krishnamurthy, A., Simchowitz, M. & Yu, T. Reward-free
exploration for reinforcement learning. inProc. International
Conference on Machine Learning,4 8 7 0–4879 (PMLR, 2020).
114. Mutti, M. & Restelli, M. An intrinsically-motivated approach for
learning highly exploring and fast mixing policies. InProc. of the
AAAI Conference on Artiﬁcial IntelligenceVol. 34, 5232–5239
(2020).
115. Eysenbach, B. & Levine, S. Maximum entropy RL (provably) solves
some robust RL problems. InInternational Conference on Learning
Representations(ICLR, 2022).
1 1 6 . G r e g o r ,K . ,R e z e n d e ,D .J .&W i e r s t r a ,D .V a r i a t i o n a li n t r i n s i cc o n t r o l
http://arxiv.org/abs/1611.07507(2016). ArXiv:1611.07507 [cs].
117. Sharma, A., Gu, S., Levine, S., Kumar, V. & Hausman, K. Dynamics-
aware unsupervised skill discovery. InProc. International Con-
ference on Learning Representations(2020). MAG ID: 2995736683
S2ID: ae3b2768b0a3c73410bce0d2ae03feaf01f6f864.
118. Park, S., Lee, K., Lee, Y. & Abbeel, P. Controllability-aware unsu-
pervised skill discoveryhttp://arxiv.org/abs/2302.05103(2023).
119. Sutton, R. S., Precup, D. & Singh, S. Between MDPs and semi-
MDPs: a framework for temporal abstraction in reinforcement
learning.Artif. Intell.112,1 8 1
–211 (1999).
120. Grau-Moya, J., Leibfried, F., Genewein, T. & Braun, D. A. Planning
with information-processing constraints and model uncertainty
in Markov decision processes. InMachine Learning and Knowl-
edge Discovery in Databases: European Conference, ECML
PKDD Proceedings, Part II 16475–491 (Springer International
Publishing, 2016).
121. Gibson, J. J.The Ecological Approach to Visual Perception: Classic
Edition (Psychology Press, 2014).
1 2 2 . K h e t a r p a l ,K . ,A h m e d ,Z . ,C o m a n i c i ,G . ,A b e l ,D .&P r e c u p ,D .W h a t
can I do here? A theory of affordances in reinforcement learning.
In Proc. International Conference on Machine Learning,5 2 4 3–5253
https://proceedings.mlr.press/v119/khetarpal20a.html
(PMLR, 2020).
1 2 3 . P e t e r s ,J . ,M u l l i n g ,K .&A l t u n ,Y .R e l a t i v ee n t r o p yp o l i c ys e a r c h .I n
Proc. of the AAAI Conference on Artiﬁcial IntelligenceVol. 24
1607–1612 https://ojs.aaai.org/index.php/AAAI/article/view/
7727 (2010).
124. Haarnoja, T. et al. Soft actor-critic algorithms and applications
http://arxiv.org/abs/1812.05905(2019).
125. Fox, R., Pakman, A. & Tishby, N. Taming the noise in reinforcement
learning via soft updates. In32nd Conference on Uncertainty in
Artiﬁcial Intelligence,2 0 2–211 (AUAI, 2016).
126. Ramírez-Ruiz, J., Grytskyy, D., Mastrogiuseppe, C., Habib, Y. &
Moreno-Bote, R. Complex behavior from intrinsic motivation to
occupy action-state path space.https://zenodo.org/records/
11401402.
Acknowledgements
This work is supported by the Howard Hughes Medical Institute (HHMI,
ref 55008742), ICREA Academia 2022 and MINECO (Spain; BFU2017-
85936-P) to R.M.-B., MINECO/ESF (Spain; PRE2018-084757) to J.R.-R,
and AGAUR-FI ajuts from Generalitat de Catalunya/ESF (2024 FI-B3
00020) to C.M and (2023 FI-1 00245) to Y.H.
Author contributions
J.R.-R. and R.M.-B. conceived the presented idea. J.R.-R., D.G., and R.M.B.
developed the theory. J.R.-R., D.G., C.M., and Y.H. performed the com-
putations and analyzed the data. All authors discussed the results and
contributed to theﬁnal manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary informationThe online version contains
supplementary material available at
https://doi.org/10.1038/s41467-024-49711-1.
Correspondenceand requests for materials should be addressed to
Jorge Ramírez-Ruiz.
Peer review informationNature Communicationsthanks the anon-
ymous, reviewer(s) for their contribution to the peer review of this work.
Ap e e rr e v i e wﬁle is available.
Reprints and permissions informationis available at
http://www.nature.com/reprints
Publisher’s noteSpringer Nature remains neutral with regard to jur-
isdictional claims in published maps and institutional afﬁliations.
Open AccessThis article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this
article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not
included in the article’s Creative Commons licence and your intended
use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright
holder. To view a copy of this licence, visithttp://creativecommons.org/
licenses/by/4.0/.
© The Author(s) 2024
Article https://doi.org/10.1038/s41467-024-49711-1
Nature Communications|         (2024) 15:6368 15