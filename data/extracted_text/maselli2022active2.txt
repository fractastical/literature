V ARIATIONAL ALGORITHMS FOR
APPROXIMATE BAYESIAN INFERENCE
by
Matthew J. Beal
M.A., M.Sci., Physics, University of Cambridge, UK (1998)
The Gatsby Computational Neuroscience Unit
University College London
17 Queen Square
London WC1N 3AR
A Thesis submitted for the degree of
Doctor of Philosophy of the University of London
May 2003
Abstract
The Bayesian framework for machine learning allows for the incorporation of prior knowledge
in a coherent way, avoids overﬁtting problems, and provides a principled basis for selecting
between alternative models. Unfortunately the computations required are usually intractable.
This thesis presents a uniﬁed variational Bayesian (VB) framework which approximates these
computations in models with latent variables using a lower bound on the marginal likelihood.
Chapter 1 presents background material on Bayesian inference, graphical models, and propaga-
tion algorithms. Chapter 2 forms the theoretical core of the thesis, generalising the expectation-
maximisation (EM) algorithm for learning maximum likelihood parameters to the VB EM al-
gorithm which integrates over model parameters. The algorithm is then specialised to the large
family of conjugate-exponential (CE) graphical models, and several theorems are presented to
pave the road for automated VB derivation procedures in both directed and undirected graphs
(Bayesian and Markov networks, respectively).
Chapters 3-5 derive and apply the VB EM algorithm to three commonly-used and important
models: mixtures of factor analysers, linear dynamical systems, and hidden Markov models.
It is shown how model selection tasks such as determining the dimensionality, cardinality, or
number of variables are possible using VB approximations. Also explored are methods for
combining sampling procedures with variational approximations, to estimate the tightness of
VB bounds and to obtain more effective sampling algorithms. Chapter 6 applies VB learning
to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares
the performance to annealed importance sampling amongst other methods. Throughout, the
VB approximation is compared to other methods including sampling, Cheeseman-Stutz, and
asymptotic approximations such as BIC. The thesis concludes with a discussion of evolving
directions for model selection including inﬁnite models and alternative approximations to the
marginal likelihood.
2
Acknowledgements
I am very grateful to my advisor Zoubin Ghahramani for his guidance in this work, bringing
energy and thoughtful insight into every one of our discussions. I would also like to thank other
senior Gatsby Unit members including Hagai Attias, Phil Dawid, Peter Dayan, Geoff Hinton,
Carl Rasmussen and Sam Roweis, for numerous discussions and inspirational comments.
My research has been punctuated by two internships at Microsoft Research in Cambridge and
in Redmond. Whilst this thesis does not contain research carried out in these labs, I would like
to thank colleagues there for interesting and often seductive discussion, including Christopher
Bishop, Andrew Blake, David Heckerman, Nebojsa Jojic and Neil Lawrence.
Amongst many others I would like to thank especially the following people for their support and
useful comments: Andrew Brown, Nando de Freitas, Oliver Downs, Alex Gray, Yoel Haitovsky,
Sham Kakade, Alex Korenberg, David MacKay, James Miskin, Quaid Morris, Iain Murray,
Radford Neal, Simon Osindero, Lawrence Saul, Matthias Seeger, Amos Storkey, Yee-Whye
Teh, Eric Tuttle, Naonori Ueda, John Winn, Chris Williams, and Angela Yu.
I should thank my friends, in particular Paola Atkinson, Tania Lillywhite, Amanda Parmar,
James Tinworth and Mark West for providing me with various combinations of shelter, com-
panionship and retreat during my time in London. Last, but by no means least I would like
to thank my family for their love and nurture in all my years, and especially my dear ﬁanc ´ee
Cassandre Creswell for her love, encouragement and endless patience with me.
The work in this thesis was carried out at the Gatsby Computational Neuroscience Unit which is
funded by the Gatsby Charitable Foundation. I am grateful to the Institute of Physics, the NIPS
foundation, the UCL graduate school and Microsoft Research for generous travel grants.
3
Contents
Abstract 2
Acknowledgements 3
Contents 4
List of ﬁgures 8
List of tables 11
List of algorithms 12
1 Introduction 13
1.1 Probabilistic inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.1.1 Probabilistic graphical models: directed and undirected networks . . . 17
1.1.2 Propagation algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 19
1.2 Bayesian model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
1.2.1 Marginal likelihood and Occam’s razor . . . . . . . . . . . . . . . . . 25
1.2.2 Choice of priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.3 Practical Bayesian approaches . . . . . . . . . . . . . . . . . . . . . . . . . . 32
1.3.1 Maximum a posteriori (MAP) parameter estimates . . . . . . . . . . . 33
1.3.2 Laplace’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
1.3.3 Identiﬁability: aliasing and degeneracy . . . . . . . . . . . . . . . . . 35
1.3.4 BIC and MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
1.3.5 Cheeseman & Stutz’s method . . . . . . . . . . . . . . . . . . . . . . 37
1.3.6 Monte Carlo methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
1.4 Summary of the remaining chapters . . . . . . . . . . . . . . . . . . . . . . . 42
2 Variational Bayesian Theory 44
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
2.2 Variational methods for ML / MAP learning . . . . . . . . . . . . . . . . . . . 46
2.2.1 The scenario for parameter learning . . . . . . . . . . . . . . . . . . . 46
2.2.2 EM for unconstrained (exact) optimisation . . . . . . . . . . . . . . . 48
4
Contents Contents
2.2.3 EM with constrained (approximate) optimisation . . . . . . . . . . . . 49
2.3 Variational methods for Bayesian learning . . . . . . . . . . . . . . . . . . . . 53
2.3.1 Deriving the learning rules . . . . . . . . . . . . . . . . . . . . . . . . 53
2.3.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
2.4 Conjugate-Exponential models . . . . . . . . . . . . . . . . . . . . . . . . . . 64
2.4.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
2.4.2 Variational Bayesian EM for CE models . . . . . . . . . . . . . . . . . 66
2.4.3 Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.5 Directed and undirected graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.5.1 Implications for directed networks . . . . . . . . . . . . . . . . . . . . 73
2.5.2 Implications for undirected networks . . . . . . . . . . . . . . . . . . 74
2.6 Comparisons of VB to other criteria . . . . . . . . . . . . . . . . . . . . . . . 75
2.6.1 BIC is recovered from VB in the limit of large data . . . . . . . . . . . 75
2.6.2 Comparison to Cheeseman-Stutz (CS) approximation . . . . . . . . . . 76
2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
3 Variational Bayesian Hidden Markov Models 82
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
3.2 Inference and learning for maximum likelihood HMMs . . . . . . . . . . . . . 83
3.3 Bayesian HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.4 Variational Bayesian formulation . . . . . . . . . . . . . . . . . . . . . . . . . 91
3.4.1 Derivation of the VBEM optimisation procedure . . . . . . . . . . . . 92
3.4.2 Predictive probability of the VB model . . . . . . . . . . . . . . . . . 97
3.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.5.1 Synthetic: discovering model structure . . . . . . . . . . . . . . . . . 98
3.5.2 Forwards-backwards English discrimination . . . . . . . . . . . . . . . 99
3.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4 Variational Bayesian Mixtures of Factor Analysers 106
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4.1.1 Dimensionality reduction using factor analysis . . . . . . . . . . . . . 107
4.1.2 Mixture models for manifold learning . . . . . . . . . . . . . . . . . . 109
4.2 Bayesian Mixture of Factor Analysers . . . . . . . . . . . . . . . . . . . . . . 110
4.2.1 Parameter priors for MFA . . . . . . . . . . . . . . . . . . . . . . . . 111
4.2.2 Inferring dimensionality using ARD . . . . . . . . . . . . . . . . . . . 114
4.2.3 Variational Bayesian derivation . . . . . . . . . . . . . . . . . . . . . 115
4.2.4 Optimising the lower bound . . . . . . . . . . . . . . . . . . . . . . . 119
4.2.5 Optimising the hyperparameters . . . . . . . . . . . . . . . . . . . . . 122
4.3 Model exploration: birth and death . . . . . . . . . . . . . . . . . . . . . . . . 124
4.3.1 Heuristics for component death . . . . . . . . . . . . . . . . . . . . . 126
4.3.2 Heuristics for component birth . . . . . . . . . . . . . . . . . . . . . . 127
5
Contents Contents
4.3.3 Heuristics for the optimisation endgame . . . . . . . . . . . . . . . . . 130
4.4 Handling the predictive density . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.5 Synthetic experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.5.1 Determining the number of components . . . . . . . . . . . . . . . . . 133
4.5.2 Embedded Gaussian clusters . . . . . . . . . . . . . . . . . . . . . . . 133
4.5.3 Spiral dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.6 Digit experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
4.6.1 Fully-unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . 138
4.6.2 Classiﬁcation performance of BIC and VB models . . . . . . . . . . . 141
4.7 Combining VB approximations with Monte Carlo . . . . . . . . . . . . . . . . 144
4.7.1 Importance sampling with the variational approximation . . . . . . . . 144
4.7.2 Example: Tightness of the lower bound for MFAs . . . . . . . . . . . . 148
4.7.3 Extending simple importance sampling . . . . . . . . . . . . . . . . . 151
4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5 Variational Bayesian Linear Dynamical Systems 159
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.2 The Linear Dynamical System model . . . . . . . . . . . . . . . . . . . . . . 160
5.2.1 Variables and topology . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.2.2 Speciﬁcation of parameter and hidden state priors . . . . . . . . . . . . 163
5.3 The variational treatment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5.3.1 VBM step: Parameter distributions . . . . . . . . . . . . . . . . . . . . 170
5.3.2 VBE step: The Variational Kalman Smoother . . . . . . . . . . . . . . 173
5.3.3 Filter (forward recursion) . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.3.4 Backward recursion: sequential and parallel . . . . . . . . . . . . . . . 177
5.3.5 Computing the single and joint marginals . . . . . . . . . . . . . . . . 181
5.3.6 Hyperparameter learning . . . . . . . . . . . . . . . . . . . . . . . . . 184
5.3.7 Calculation of F . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
5.3.8 Modiﬁcations when learning from multiple sequences . . . . . . . . . 186
5.3.9 Modiﬁcations for a fully hierarchical model . . . . . . . . . . . . . . . 189
5.4 Synthetic Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
5.4.1 Hidden state space dimensionality determination (no inputs) . . . . . . 189
5.4.2 Hidden state space dimensionality determination (input-driven) . . . . 191
5.5 Elucidating gene expression mechanisms . . . . . . . . . . . . . . . . . . . . 195
5.5.1 Generalisation errors . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
5.5.2 Recovering gene-gene interactions . . . . . . . . . . . . . . . . . . . . 200
5.6 Possible extensions and future research . . . . . . . . . . . . . . . . . . . . . 201
5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
6 Learning the structure of discrete-variable graphical models with hidden vari-
ables 206
6
Contents Contents
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
6.2 Calculating marginal likelihoods of DAGs . . . . . . . . . . . . . . . . . . . . 207
6.3 Estimating the marginal likelihood . . . . . . . . . . . . . . . . . . . . . . . . 210
6.3.1 ML and MAP parameter estimation . . . . . . . . . . . . . . . . . . . 210
6.3.2 BIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
6.3.3 Cheeseman-Stutz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
6.3.4 The VB lower bound . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.3.5 Annealed Importance Sampling (AIS) . . . . . . . . . . . . . . . . . . 218
6.3.6 Upper bounds on the marginal likelihood . . . . . . . . . . . . . . . . 222
6.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
6.4.1 Comparison of scores to AIS . . . . . . . . . . . . . . . . . . . . . . . 226
6.4.2 Performance averaged over the parameter prior . . . . . . . . . . . . . 232
6.5 Open questions and directions . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6.5.1 AIS analysis, limitations, and extensions . . . . . . . . . . . . . . . . 236
6.5.2 Estimating dimensionalities of the incomplete and complete-data models 245
6.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
7 Conclusion 250
7.1 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
7.2 Summary of contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
Appendix A Conjugate Exponential family examples 259
Appendix B Useful results from matrix theory 262
B.1 Schur complements and inverting partitioned matrices . . . . . . . . . . . . . . 262
B.2 The matrix inversion lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
Appendix C Miscellaneous results 265
C.1 Computing the digamma function . . . . . . . . . . . . . . . . . . . . . . . . 265
C.2 Multivariate gamma hyperparameter optimisation . . . . . . . . . . . . . . . . 266
C.3 Marginal KL divergence of gamma-Gaussian variables . . . . . . . . . . . . . 267
Bibliography 270
7
List of ﬁgures
1.1 The elimination algorithm on a simple Markov network . . . . . . . . . . . . . 20
1.2 Forming the junction tree for a simple Markov network . . . . . . . . . . . . . 22
1.3 The marginal likelihood embodies Occam’s razor . . . . . . . . . . . . . . . . 27
2.1 Variational interpretation of EM for ML learning . . . . . . . . . . . . . . . . 50
2.2 Variational interpretation of constrained EM for ML learning . . . . . . . . . . 51
2.3 Variational Bayesian EM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.4 Hidden-variable / parameter factorisation steps . . . . . . . . . . . . . . . . . 59
2.5 Hyperparameter learning for VB EM . . . . . . . . . . . . . . . . . . . . . . . 62
3.1 Graphical model representation of a hidden Markov model . . . . . . . . . . . 83
3.2 Evolution of the likelihood for ML hidden Markov models, and the subsequent
VB lower bound. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.3 Results of ML and VB HMM models trained on synthetic sequences. . . . . . . 101
3.4 Test data log predictive probabilities and discrimination rates for ML, MAP,
and VB HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.1 ML Mixtures of Factor Analysers . . . . . . . . . . . . . . . . . . . . . . . . 110
4.2 Bayesian Mixtures of Factor Analysers . . . . . . . . . . . . . . . . . . . . . . 114
4.3 Determination of number of components in synthetic data . . . . . . . . . . . . 134
4.4 Factor loading matrices for dimensionality determination . . . . . . . . . . . . 135
4.5 The Spiral data set of Ueda et. al . . . . . . . . . . . . . . . . . . . . . . . . . 136
4.6 Birth and death processes with VBMFA on the Spiral data set . . . . . . . . . . 137
4.7 Evolution of the lower bound Ffor the Spiral data set . . . . . . . . . . . . . . 137
4.8 Training examples of digits from the CEDARdatabase . . . . . . . . . . . . . . 138
4.9 A typical model of the digits learnt by VBMFA . . . . . . . . . . . . . . . . . 139
4.10 Confusion tables for the training and test digit classiﬁcations . . . . . . . . . . 140
4.11 Distribution of components to digits in BIC and VB models . . . . . . . . . . . 143
4.12 Logarithm of the marginal likelihood estimate and the VB lower bound during
learning of the digits {0,1,2}. . . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.13 Discrepancies between marginal likelihood and lower bounds during VBMFA
model search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8
List of ﬁgures List of ﬁgures
4.14 Importance sampling estimates of marginal likelihoods for learnt models of
data of differently spaced clusters . . . . . . . . . . . . . . . . . . . . . . . . 156
5.1 Graphical model representation of a state-space model . . . . . . . . . . . . . 161
5.2 Graphical model for a state-space model with inputs . . . . . . . . . . . . . . . 162
5.3 Graphical model representation of a Bayesian state-space model . . . . . . . . 164
5.4 Recovered LDS models for increasing data size . . . . . . . . . . . . . . . . . 190
5.5 Hyperparameter trajectories showing extinction of state-space dimensions . . . 191
5.6 Data for the input-driven LDS synthetic experiment . . . . . . . . . . . . . . . 193
5.7 Evolution of the lower bound and its gradient . . . . . . . . . . . . . . . . . . 194
5.8 Evolution of precision hyperparameters, recovering true model structure . . . . 196
5.9 Gene expression data for input-driven experiments on real data . . . . . . . . . 197
5.10 Graphical model of an LDS with feedback of observations into inputs . . . . . 199
5.11 Reconstruction errors of LDS models trained using MAP and VB algorithms
as a function of state-space dimensionality . . . . . . . . . . . . . . . . . . . . 200
5.12 Gene-gene interaction matrices learnt by MAP and VB algorithms, showing
signiﬁcant entries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
5.13 Illustration of the gene-gene interactions learnt by the feedback model on ex-
pression data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
6.1 The chosen structure for generating data for the experiments . . . . . . . . . . 225
6.2 Illustration of the trends in marginal likelihood estimates as reported by MAP,
BIC, BICp, CS, VB and AIS methods, as a function of data set size and number
of parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.3 Graph of rankings given to the true structure by BIC, BICp, CS, VB and AIS
methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
6.4 Differences in marginal likelihood estimate of the top-ranked and true struc-
tures, by BIC, BICp, CS, VB and AIS . . . . . . . . . . . . . . . . . . . . . . 232
6.5 The median ranking given to the true structure over repeated settings of its
parameters drawn from the prior, by BIC, BICp, CS and VB methods . . . . . 233
6.6 Median score difference between the true and top-ranked structures, under BIC,
BICp, CS and VB methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
6.7 The best ranking given to the true structure by BIC, BICp, CS and VB methods 235
6.8 The smallest score difference between true and top-ranked structures, by BIC,
BICp, CS and VB methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6.9 Overall success rates of BIC, BICp, CS and VB scores, in terms of ranking the
true structure top . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
6.10 Example of the variance of the AIS sampler estimates with annealing schedule
granularity, using various random initialisations, shown against the BIC and
VB estimates for comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
9
List of ﬁgures List of ﬁgures
6.11 Acceptance rates of the Metropolis-Hastings proposals as a function of size of
data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.12 Acceptance rates of the Metropolis-Hastings sampler in each of four quarters
of the annealing schedule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
6.13 Non-linear AIS annealing schedules . . . . . . . . . . . . . . . . . . . . . . . 244
10
List of tables
2.1 Comparison of EM for ML/MAP estimation against VB EM with CE models . 70
4.1 Simultaneous determination of number of components and their dimensionalities 135
4.2 Test classiﬁcation performance of BIC and VB models . . . . . . . . . . . . . 142
4.3 Speciﬁcations of six importance sampling distributions . . . . . . . . . . . . . 155
6.1 Rankings of the true structure amongst the alternative candidates, by MAP,
BIC, BICp, VB and AIS estimates, both corrected and uncorrected for posterior
aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
6.2 Comparison of performance of VB to BIC, BICp and CS methods, as measured
by the ranking given to the true model . . . . . . . . . . . . . . . . . . . . . . 233
6.3 Improving the AIS estimate by pooling the results of several separate sampling
runs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
6.4 Rate of AIS violations of the VB lower bound, alongside Metropolis-Hastings
rejection rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
6.5 Number of times the true structure is given the highest ranking by the BIC,
BICp, CS, CS†, and VB scores . . . . . . . . . . . . . . . . . . . . . . . . . . 247
11
List of Algorithms
5.1 Forward recursion for variational Bayesian state-space models . . . . . . . . . 178
5.2 Backward parallel recursion for variational Bayesian state-space models . . . . 181
5.3 Pseudocode for variational Bayesian state-space models . . . . . . . . . . . . . 187
6.1 AIS algorithm for computing all ratios to estimate the marginal likelihood . . . 221
6.2 Algorithm to estimate the complete- and incomplete-data dimensionalities of a
model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
12
Chapter 1
Introduction
Our everyday experiences can be summarised as a series of decisions to take actions which
manipulate our environment in some way or other. We base our decisions on the results of
predictions or inferences of quantities that have some bearing on our quality of life, and we
come to arrive at these inferences based on models of what we expect to observe. Models are
designed to capture salient trends or regularities in the observed data with a view to predicting
future events. Sometimes the models can be constructed with existing expertise, but for the
majority of real applications the data are far too complex or the underlying processes not nearly
well enough understood for the modeller to design a perfectly accurate model. If this is the case,
we can hope only to design models that are simplifying approximations of the true processes
that generated the data.
For example, the data might be a time series of the price of stock recorded every day for the last
six months, and we would like to know whether to buy or sell stock today. This decision, and
its particulars, depend on what the price of the stock is likely to be a week from now. There
are obviously a very large number of factors that inﬂuence the price and these do so to varying
degrees and in convoluted and complex ways. Even in the unlikely scenario that we knew
exactly how all these factors affected the price, we would still have to gather every piece of data
for each one and process it all in a short enough time to decide our course of action. Another
example is trying to predict the best location to drill for oil, knowing the positions of existing
drill sites in the region and their yields. Since we are unable to probe deep beneath the Earth’s
surface, we need to rely on a model of the geological processes that gave rise to the yields in
those sites for which we have data, in order to be able to predict the best location.
The machine learning approach to modelling data constructs models by beginning with a ﬂexi-
ble model speciﬁed by a set of parameters and then ﬁnds the setting of these model parameters
that explains or ﬁts the data best. The idea is that if we can explain our observations well, then
we should also be conﬁdent that we can predict future observations well. We might also hope
13
Introduction
that the particular setting of the best-ﬁt parameters provides us with some understanding of the
underlying processes. The procedure of ﬁtting model parameters to observed data is termed
learning a model.
Since our models are simpliﬁcations of reality there will inevitably be aspects of the data which
cannot be modelled exactly, and these are considered noise. Unfortunately it is often difﬁcult
to know which aspects of the data are relevant for our inference or prediction tasks, and which
aspects should be regarded as noise. With a sufﬁciently complex model, parameters can be
found to ﬁt the observed data exactly, but any predictions using this best-ﬁt model will be sub-
optimal as it has erroneously ﬁtted the noise instead of the trends. Conversely, too simple a
model will fail to capture the underlying regularities in the data and so will also produce sub-
optimal inferences and predictions. This trade-off between the complexity of the model and its
generalisation performance is well studied, and we return to it in section 1.2.
The above ideas can be formalised using the concept of probability and the rules of Bayesian
inference. Let us denote the data set by y, which may be made up of several variables indexed
by j: y = {y1,..., yj,..., yJ}. For example, y could be the data from an oil well for which
the variables might be measurements of the type of oil found, the geographical location of the
well, its average monthly yield, its operational age, and a host of other measurable quantities
regarding its local geological characteristics. Generally each variable can be real-valued or
discrete. Machine learning approaches deﬁne a generative model of the data through a set of
parameters θ = {θ1,...,θ K}which deﬁne a probability distribution over data, p(y |θ). One
approach to learning the model then involves ﬁnding the parameters θ∗ such that
θ∗ = arg max
θ
p(y |θ) . (1.1)
This process is often called maximum likelihood learning as the parameters θ∗ are set to max-
imise the likelihood of θ, which is probability of the observed data under the model. The
generative model may also include latent or hidden variables, which are unobserved yet inter-
act through the parameters to generate the data. We denote the hidden variables by x, and the
probability of the data can then be written by summing over the possible settings of the hidden
states:
p(y |θ) =
∑
x
p(x |θ)p(y |x,θ) , (1.2)
where the summation is replaced by an integral for those hidden variables that are real-valued.
The quantity ( 1.2) is often called the incomplete-data likelihood, and the summand in ( 1.2)
correspondingly called the complete-data likelihood. The interpretation is that with hidden vari-
ables in the model, the observed data is an incomplete account of all the players in the model.
14
Introduction
For a particular parameter setting, it is possible to infer the states of the hidden variables of the
model, having observed data, using Bayes’ rule:
p(x |y,θ) = p(x |θ)p(y |x,θ)
p(y |θ) . (1.3)
This quantity is known as the posterior distribution over the hidden variables. In the oil well
example we might have a hidden variable for the amount of oil remaining in the reserve, and this
can be inferred based on observed measurements such as the operational age, monthly yield and
geological characteristics, through the generative model with parameters θ. The term p(x |θ)
is a prior probability of the hidden variables, which could be set by the modeller to reﬂect the
distribution of amounts of oil in wells that he or she would expect. Note that the probability
of the data in ( 1.2) appears in the denominator of ( 1.3). Since the hidden variables are by
deﬁnition unknown, ﬁnding θ∗ becomes more difﬁcult, and the model is learnt by alternating
between estimating the posterior distribution over hidden variables for a particular setting of the
parameters and then re-estimating the best-ﬁt parameters given that distribution over the hidden
variables. This procedure is the well-known expectation-maximisation (EM) algorithm and is
discussed in more detail in section 2.2.
Given that the parameters themselves are unknown quantities we can treat them as random
variables. This is the Bayesian approach to uncertainty, which treats all uncertain quantities
as random variables and uses the laws of probability to manipulate those uncertain quantities.
The proper Bayesian approach attempts to integrate over the possible settings of all uncertain
quantities rather than optimise them as in ( 1.1). The quantity that results from integrating out
both the hidden variables and the parameters is termed the marginal likelihood:
p(y) =
∫
dθ p(θ)
∑
x
p(x |θ)p(y |x,θ) , (1.4)
where p(θ) is a prior over the parameters of the model. We will see in section 1.2 that the
marginal likelihood is a key quantity used to choose between different models in a Bayesian
model selection task. Model selection is a necessary step in understanding and representing the
data that we observe. The diversity of the data available to machine learners is ever increasing
thanks to the advent of large computational power, networking capabilities and the technolo-
gies available to the scientiﬁc research communities. Furthermore, expertise and techniques of
analysis are always improving, giving rise to ever more diverse and complicated models for
representing this data. In order to ‘understand’ the data with a view to making predictions based
on it, we need to whittle down our models to one (or a few) to which we can devote our limited
computational and conceptual resources. We can use the rules of Bayesian probability theory to
entertain several models and choose between them in the light of data. These steps necessarily
involve managing the marginal likelihood.
15
Introduction 1.1. Probabilistic inference
Unfortunately the marginal likelihood, p(y), is an intractable quantity to compute for almost all
models of interest (we will discuss why this is so in section1.2.1, and see several examples in the
course of this thesis). Traditionally, the marginal likelihood has been approximated either using
analytical methods, for example the Laplace approximation, or via sampling-based approaches
such as Markov chain Monte Carlo. These methods are reviewed in section 1.3. This thesis is
devoted to one particular method of approximation,variational Bayes, sometimes referred to as
ensemble learning. The variational Bayesian method constructs a lower bound on the marginal
likelihood, and attempts to optimise this bound using an iterative scheme that has intriguing
similarities to the standard expectation-maximisation algorithm. There are other variational
methods, for example those based on Bethe and Kikuchi free energies, which for the most part
are approximations rather than bounds; these are brieﬂy discussed in the ﬁnal chapter.
Throughout this thesis we assume that the reader is familiar with the basic concepts of probabil-
ity and integral and differential calculus. Included in the appendix are reference tables for some
of the more commonly used probability distributions.
The rest of this chapter reviews some key methods relevant to Bayesian model inference and
learning. Section 1.1 reviews the use of graphical models as a tool for visualising the prob-
abilistic relationships between the variables in a model and explains how efﬁcient algorithms
for computing the posterior distributions of hidden variables as in ( 1.3) can be designed which
exploit independence relationships amongst the variables. In section 1.2, we address the issue
of model selection in a Bayesian framework, and explain why the marginal likelihood is the
key quantity for this task, and how it is intractable to compute. Since all Bayesian reasoning
needs to begin with some prior beliefs, we examine different schools of thought for expressing
these priors in section 1.2.2, including conjugate, reference, and hierarchical priors. In section
1.3 we review several practical methods for approximating the marginal likelihood, which we
shall be comparing to variational Bayes in the following chapters. Finally, section 1.4 brieﬂy
summarises the remaining chapters of this thesis.
1.1 Probabilistic inference
Bayesian probability theory provides a language for representing beliefs and a calculus for ma-
nipulating these beliefs in a coherent manner. It is an extension of the formal theory of logic
which is based on axioms that involve propositions that are true or false. The rules of proba-
bility theory involve propositions which have plausibilities of being true or false, and can be
arrived at on the basis of just three desiderata: (1) degrees of plausibility should be represented
by real numbers; (2) plausibilities should have qualitative correspondence with common sense;
(3) different routes to a conclusion should yield the same result. It is quite astonishing that from
just these desiderata, the product and sum rules of probability can be mathematically derived
16
Introduction 1.1. Probabilistic inference
(Cox, 1946). Cox showed that plausibilities can be measured on any scale and it is possible to
transform them onto the canonical scale of probabilities that sum to one. For good introductions
to probability theory the reader is referred to Pearl (1988) and Jaynes (2003).
Statistical modelling problems often involve large numbers of interacting random variables and
it is often convenient to express the dependencies between these variables graphically. In par-
ticular such graphical models are an intuitive tool for visualising conditional independency re-
lationships between variables. A variable ais said to be conditionally independent of b, given c
if and only if p(a,b |c) can be written p(a|c)p(b|c). By exploiting conditional independence
relationships, graphical models provide a backbone upon which it has been possible to derive
efﬁcient message-propagating algorithms for conditioning and marginalising variables in the
model given observation data ( Pearl, 1988; Lauritzen and Spiegelhalter , 1988; Jensen, 1996;
Heckerman, 1996; Cowell et al., 1999; Jordan, 1999). Many standard statistical models, espe-
cially Bayesian models with hierarchical priors (see section 1.2.2), can be expressed naturally
using probabilistic graphical models. This representation can be helpful in developing both sam-
pling methods (section 1.3.6) and exact inference methods such as the junction tree algorithm
(section 1.1.2) for these models. All of the models used in this thesis have very simple graphi-
cal model descriptions, and the theoretical results derived in chapter 2 for variational Bayesian
approximate inference are phrased to be readily applicable to general graphical models.
1.1.1 Probabilistic graphical models: directed and undirected networks
A graphical model expresses a family of probability distributions on sets of variables in a model.
Here and for the rest of the thesis we use the variable z to denote all the variables in the model,
be they observed or unobserved (hidden). To differentiate between observed and unobserved
variables we partition z into z = {x,y}where x and y are the sets of unobserved and observed
variables, respectively. Alternatively, the variables are indexed by the subscript j, with j ∈H
the set of indices for unobserved (hidden) variables and j ∈V the set of indices for observed
variables. We will later introduce a further subscript, i, which will denote which data point out
of a data set of size n is being referred to, but for the purposes of the present exposition we
consider just a single data point and omit this further subscript.
Each arc between two nodes in the graphical model represents a probabilistic connection be-
tween two variables. We use the terms ‘node’ and ‘variable’ interchangeably. Depending on the
pattern of arcs in the graph and their type, different independence relations can be represented
between variables. The pattern of arcs is commonly referred to as the structure of the model.
The arcs between variables can be all directed or all undirected. There is a class of graphs in
which some arcs are directed and some are undirected, commonly calledchain graphs, but these
are not reviewed here. Undirected graphical models, also called Markov networks or Markov
17
Introduction 1.1. Probabilistic inference
random ﬁelds, express the probability distribution over variables as a product overclique poten-
tials:
p(z) = 1
Z
J∏
j=1
ψj(Cj(z)) , (1.5)
where z is the set of variables in the model, {Cj}J
j=1 are cliques of the graph, and {ψj}J
j=1
are a set of clique potential functions each of which returns a non-negative real value for every
possible conﬁguration of settings of the variables in the clique. Each clique is deﬁned to be a
fully connected subgraph (that is to say each cliqueCj selects a subset of the variables inz), and
is usually maximal in the sense that there are no other variables whose inclusion preserves its
fully connected property. The cliques can be overlapping, and between them cover all variables
such that {C1(z) ∪···∪ CJ(z)} = z. Here we have written a normalisation constant, Z,
into the expression (1.5) to ensure that the total probability of all possible conﬁgurations sums
to one. Alternatively, this normalisation can be absorbed into the deﬁnition of one or more
of the potential functions. Markov networks can express a very simple form of independence
relationship: two sets of nodes Aand Bare conditionally independent from each other given a
third set of nodes C, if all paths connecting any node in Ato any node in B via a sequence of
arcs are separated by any node (or group of nodes) in C. Then C is said to separate Afrom B.
The Markov blanket for the node (or set of nodes) Ais deﬁned as the smallest set of nodes C,
such that Ais conditionally independent of all other variables not in C, given C.
Directed graphical models, also called Directed Acyclic Graphs (DAGs), or Bayesian networks,
express the probability distribution over J variables, z = {zj}J
j=1, as a product of conditional
probability distributions on each variable:
p(z) =
J∏
j=1
p(zj |zpa(j)) , (1.6)
where zpa(j) is the set of variables that are parents of the node jin the graph. A node ais said
to be a parent of a node bif there is a directed arc from ato b, and in which case bis said to
be a child of a. In necessarily recursive deﬁnitions: the descendents of a node are deﬁned to
include its children and its childrens’ descendents; and the ancestors of a node are its parents
and those parents’ ancestors. Note that there is no need for a normalisation constant in ( 1.6)
because by the deﬁnition of the conditional probabilities it is equal to one. A directed path
between two nodes a and b is a sequence of variables such that every node is a parent of the
following node in the sequence. An undirected path from ato bis any sequence of nodes such
that every node is a parent or child of the following node. An acyclic graph is a graphical
model in which there exist no directed paths including the same variable more than once. The
semantics of a Bayesian network can be summarised as: each node is conditionally independent
from its non-descendents given its parents.
18
Introduction 1.1. Probabilistic inference
More generally, we have the following representation of independence in Bayesian networks:
two sets of nodes Aand B are conditionally independent given the set of nodes C if they are
d-separated by C(here the d- preﬁx stands for directed). The nodes Aand Bare d-separated by
Cif, along every undirected path fromAto B, there exists a node dwhich satisﬁes either of the
following conditions: either (i) dhas converging arrows (i.e. dis the child of the previous node
and the parent of the following node in the path) and neither dnor its descendents are in C; or
(ii) ddoes not have converging arrows and is in C. From the above deﬁnition of the Markov
blanket, we ﬁnd that for Bayesian networks the minimal Markov blanket for a node is given by
the union of its parents, its children, and the parents of its children. A more simple rule for
d-separation can be obtained using the idea of the ‘Bayes ball’ ( Shachter, 1998). Two sets of
nodes Aand B are conditionally dependent given C if there exists a path by which the Bayes
ball can reach a node in Bfrom a node in A(or vice-versa), where the ball can move according
to the following rules: it can pass through a node in the conditioning set C provided the entry
and exit arcs are a pair of arrows converging on that node; similarly, it can only pass through
every node in the remainder of the graph provided it does so on non-converging arrows. If there
exist no such linking paths, then the sets of nodes Aand Bare conditionally independent given
C.
Undirected models tend to be used in the physics and vision communities, where the systems
under study can often be simply expressed in terms of many localised potential functions. The
nature of the interactions often lack causal or direct probabilistic interpretations, and instead
express degrees of agreement, compatibility, constraint or frustration between nodes. In the
artiﬁcial intelligence and statistics communities directed graphs are more popular as they can
more easily express underlying causal generative processes that give rise to our observations.
For more detailed examinations of directed and undirected graphs see Pearl (1988).
1.1.2 Propagation algorithms
The conditional independence relationships discussed in the previous subsection can be ex-
ploited to design efﬁcient message-passing algorithms for obtaining the posterior distributions
over hidden variables given the observations of some other variables, which is called inference.
In this section we brieﬂy present an inference algorithm for Markov networks, called the junc-
tion tree algorithm. We will explain at the end of this subsection why it sufﬁces to present the
inference algorithm for the undirected network case, since the inference algorithm for a directed
network is just a special case.
For data in which every variable is observed there is no inference problem for hidden variables,
and learning for example the maximum likelihood (ML) parameters for the model using ( 1.1)
often consists of a straightforward optimisation procedure. However, as we will see in chapter
2, if some of the variables are hidden this complicates ﬁnding the ML parameters. The common
19
Introduction 1.1. Probabilistic inference
x1
x4 x5
x3
x2
(a) Original Markov network.
x1
x4 x5
x3
x1
x5
x3
x1
x3
x1
(b) One possible elimination ordering: (x2, x4, x5, x3)
x1
x4
x3
x2 x1
x4
x3
x1
x4
x1
(c) Another possible elimination ordering: (x5, x2, x3, x4).
Figure 1.1: (a) The original Markov network;(b) The sequence of intermediate graphs resulting
from eliminating (integrating out) nodes to obtain the marginal on x1 — see equations ( 1.9–
1.14); (c) Another sequence of graphs resulting from a different elimination ordering, which
results in a suboptimal inference algorithm.
practice in these cases is to utilise expectation-maximisation (EM) algorithms, which in their E
step require the computation of at least certain properties of the posterior distribution over the
hidden variables.
We illustrate the basics of inference using a simple example adapted from Jordan and Weiss
(2002). Figure 1.1(a) shows a Markov network for ﬁve variables x = {x1,...,x 5}, each of
which is discrete and takes on kpossible states. Using the Markov network factorisation given
by (1.5), the probability distribution over the variables can be written as a product of potentials
deﬁned over ﬁve cliques:
p(x) = p(x1,...,x 5) = 1
Zψ(x1,x2)ψ(x1,x3)ψ(x1,x4)ψ(x2,x5)ψ(x3,x5)ψ(x4,x5) , (1.7)
20
Introduction 1.1. Probabilistic inference
where we have included a normalisation constant Zto allow for arbitrary clique potentials.
Note that in this graph 1.1(a) the maximal cliques are all pairs of nodes connected by an arc,
and therefore the potential functions are deﬁned over these same pairs of nodes. Suppose we
wanted to obtain the marginal distribution p(x1), given by
p(x1) = 1
Z
∑
x2
∑
x3
∑
x4
∑
x5
ψ(x1,x2)ψ(x1,x3)ψ(x1,x4)ψ(x2,x5)ψ(x3,x5)ψ(x4,x5) .
(1.8)
At ﬁrst glance this requires k5 computations, since there are k4 summands to be computed for
each of the ksettings of the variable x1. However this complexity can be reduced by exploiting
the conditional independence structure in the graph. For example, we can rewrite (1.8) as
p(x1) = 1
Z
∑
x2
∑
x3
∑
x4
∑
x5
ψ(x1,x3)ψ(x3,x5)ψ(x1,x4)ψ(x4,x5)ψ(x1,x2)ψ(x2,x5) (1.9)
= 1
Z
∑
x3
ψ(x1,x3)
∑
x5
ψ(x3,x5)
∑
x4
ψ(x1,x4)ψ(x4,x5)
∑
x2
ψ(x1,x2)ψ(x2,x5)
(1.10)
= 1
Z
∑
x3
ψ(x1,x3)
∑
x5
ψ(x3,x5)
∑
x4
ψ(x1,x4)ψ(x4,x5)m2(x1,x5) (1.11)
= 1
Z
∑
x3
ψ(x1,x3)
∑
x5
ψ(x3,x5)m4(x1,x5)m2(x1,x5) (1.12)
= 1
Z
∑
x3
ψ(x1,x3)m5(x1,x3) (1.13)
= 1
Zm1(x1) (1.14)
where each ‘message’ mj(x·,... ) is a new potential obtained by eliminating the jth vari-
able, and is a function of all the variables linked to that variable. By choosing this ordering
(x2,x4,x5,x3) for summing over the variables, the most number of variables in any summand
is three, meaning that the complexity has been reduced toO(k3) for each possible setting of x1,
which results in an overall complexity of O(k4) .
This process can be described by the sequence of graphs resulting from the repeated application
of a triangulation algorithm (see ﬁgure 1.1(b)) following these four steps: (i) choose a node xj
to eliminate; (ii) ﬁnd all potentials ψ and any messages m that may reference this node; (iii)
deﬁne a new potential mj that is the sum with respect to xj of the product of these potentials;
(iv) remove the node xj and replace it with edges connecting each of its neighbours — these
represent the dependencies from the new potentials. This process is repeated until only the
variables of interest remain, as shown in the above example. In this way marginal probabilities
of single variables or joint probabilities over several variables can be obtained. Note that the
second elimination step in ﬁgure1.1(b)), that of marginalising outx4, introduces a new message
m4(x1,x5) but since there is already an arc connectingx1 and x5 we need not add a further one.
21
Introduction 1.1. Probabilistic inference
x1
x4 x5
x3
x2
(a)
x1
x1
x1
x1
x1
x4 x5
x5
x5
x5
x5
x3
x2
A
B
C
mAB(x1,x5)
mBC(x1,x5)
mBA(x1,x5)
mCB(x1,x5) (b)
Figure 1.2: (a) The triangulated graph corresponding to the elimination ordering in ﬁgure1.1(b);
(b) the corresponding junction tree including maximal cliques (ovals), separators (rectangles),
and the messages produced in belief propagation.
The ordering chosen for this example is optimal; different orderings of elimination may result in
suboptimal complexity. For example, ﬁgure1.1(c) shows the process of an elimination ordering
(x5,x2,x3,x4) which results in a complexity O(k5). In general though, it is an NP-hard prob-
lem to ﬁnd the optimal ordering of elimination that minimises the complexity. If all the nodes
have the same cardinality, the optimal elimination ordering is independent of the functional
forms on the nodes and is purely a graph-theoretic property.
We could use the above elimination algorithm repeatedly to ﬁnd marginal probabilities for each
and every node, but we would ﬁnd that we had needlessly computed certain messages several
times over. We can use the junction tree algorithm to compute all the messages we might need
just once. Consider the graph shown in ﬁgure 1.2(a) which results from retaining all edges that
were either initially present or added during the elimination algorithm (using the ordering in our
worked example). Alongside in ﬁgure1.2(b) is the junction tree for this graph, formed by linking
the maximal cliques of the graph, of which there are three, labelledA, Band C. In between the
clique nodes are separators for the junction tree, which contain nodes that are common to both
the cliques attached to the separator, that is to say SAB = CA ∩CB. Here we use calligraphic
Cto distinguish these cliques from the original maximal cliques in the network 1.1(a). For a
triangulated graph it is always possible to obtain such a singly-connected graph, or tree (to be
more speciﬁc, it is always then possible to obtain a tree that satisﬁes the running intersection
property, which states that if a variable appears in two different cliques, then it should also
appear in every clique in the path between the two cliques). The so-called ‘messages’ in the
elimination algorithm can now be considered as messages sent from one clique to another in the
junction tree. For example, the message m2(x1,x5) produced in equation (1.11) as a result of
summing over x2 can be identiﬁed with the message mAB(x1,x5) that clique Asends to clique
B. Similarly, the message m4(x1,x5) in (1.12) resulting from summing over x4 is identiﬁed
22
Introduction 1.1. Probabilistic inference
with the message mCB(x1,x5) that C passes on to B. To complete the marginalisation to
obtain p(x1), the clique Babsorbs the incoming messages to obtain a joint distribution over its
variables (x1,x3,x5), and then marginalises out x3 and x5 in either order. Included in ﬁgure
1.2(b) are two other messages, mBA(x1,x5) and mBC(x1,x5), which would be needed if we
wanted the marginal over x2 or x4, respectively.
For general junction trees it can be shown that the message that clique rsends to clique sis a
function of the variables in their separator, Srs(x), and is given by
mrs(Srs(x)) =
∑
Cr(x)\Srs(x)
ψr(Cr(x))
∏
t∈N(r)\s
mtr(Str(x)) , (1.15)
where N(r) are the set of neighbouring cliques of clique r. In words, the message from r to
sis formed by: taking the product of all messages rhas received from elsewhere other than s,
multiplying in the potential ψr, and then summing out all those variables in rwhich are not in
s.
The joint probability of the variables within clique r is obtained by combining messages into
clique rwith its potential:
p(Cr(x)) ∝ψr(Cr(x))
∏
t∈N(r)
mtr(Str(x)) . (1.16)
Note that from deﬁnition (1.15) a clique is unable to send a message until it has received mes-
sages from all other cliques except the receiving one. This means that the message-passing
protocol must begin at the leaves of the junction tree and move inwards, and then naturally the
message-passing moves back outwards to the leaves. In our example problem the junction tree
has a very trivial structure and happens to have both separators containing the same variables
(x1,x5).
Here we have explained how inference in a Markov network is possible: (i) through a process of
triangulation the junction tree is formed; (ii) messages (1.15) are then propagated between junc-
tion tree cliques until all cliques have received and sent all their messages; (iii) clique marginals
(1.16) can then be computed; (iv) individual variable marginals can be obtained by summing out
other variables in the clique. The algorithm used for inference in a Bayesian network (which is
directed) depends on whether it is singly- or multiply-connected (a graph is said to be singly-
connected if it includes no pairs of nodes with more than one path between them, and multiply-
connected otherwise). For singly-connected networks, an exactly analogous algorithm can be
used, and is called belief propagation. For multiply-connected networks, we ﬁrst require a pro-
cess to convert the Bayesian network into a Markov network, called moralisation. We can then
form the junction tree after a triangulation process and perform the same message-passing al-
gorithm. The process of moralisation involves adding an arc between any variables sharing the
same child (i.e. co-parents), and then dropping the directionality of all arcs.
23
Introduction 1.2. Bayesian model selection
Moralisation does not introduce any further conditional independence relationships into the
graph, and in this sense the resulting Markov network is able to represent a superset of the
probability distributions representable by the Bayesian network. Therefore, having derived the
inference procedure for the more general Markov network, we already have the result for the
Bayesian network as a special case.
1.2 Bayesian model selection
In this thesis we are primarily concerned with the task of model selection, or structure discovery.
We use the term ‘model’ and ‘model structure’ to denote a variety of things, some already
mentioned in the previous sections. A few particular examples of model selection tasks are
given below:
Structure learning In probabilistic graphical models, each graph implies a set of conditional
independence statements between the variables in the graph. The model structure learn-
ing problem is inferring the conditional independence relationships that hold given a set
of (complete or incomplete) observations of the variables. Another related problem is
learning the direction of the dependencies, i.e. the causal relationships between variables
(A→B, or B →A).
Input dependence A special case of this problem is input variable selection in regression. Se-
lecting which input (i.e. explanatory) variables are needed to predict the output (i.e. re-
sponse) variable in the regression can be equivalently cast as deciding whether each input
variable is a parent (or, more accurately, an ancestor) of the output variable in the corre-
sponding directed graph.
Cardinality Many statistical models contain discrete nominal latent variables. A model struc-
ture learning problem of interest is then choosing the cardinality of each discrete latent
variable. Examples of this problem include deciding how many mixture components are
required in a ﬁnite mixture model, or how many hidden states are needed in a hidden
Markov model.
Dimensionality Other statistical models contain real-valued vectors of latent variables. The
dimensionality of this latent vector is usually unknown and needs to be inferred. Exam-
ples include choosing the intrinsic dimensionality in a probabilistic principal components
analysis (PCA), or factor analysis (FA) model, or in a linear-Gaussian state-space model.
In the course of this thesis we tackle several of the above model selection problems using
Bayesian learning. The machinery and tools for Bayesian model selection are presented in
the following subsection.
24
Introduction 1.2. Bayesian model selection
1.2.1 Marginal likelihood and Occam’s razor
An obvious problem with using maximum likelihood methods ( 1.1) to learn the parameters of
models such as those described above is that the probability of the data will generally be greater
for more complex model structures, leading to overﬁtting. Such methods fail to take into account
model complexity. For example, inserting an arc between two variables in a graphical model can
only help the model give higher probability to the data. Common ways for avoiding overﬁtting
have included early stopping, regularisation, and cross-validation. Whilst it is possible to use
cross-validation for simple searches over model size and structures — for example, if the search
is limited to a single parameter that controls the model complexity — for more general searches
over many parameters cross-validation is computationally prohibitive.
A Bayesian approach to learning starts with some prior knowledge or assumptions about the
model structure — for example the set of arcs in the Bayesian network. This initial knowledge
is represented in the form of a prior probability distribution over model structures. Each model
structure has a set of parameters which have prior probability distributions. In the light of ob-
served data, these are updated to obtain a posterior distribution over models and parameters.
More formally, assuming a prior distribution over models structures p(m) and a prior distribu-
tion over the parameters for each model structure p(θ |m), observing the data set y induces a
posterior distribution over models given by Bayes’ rule:
p(m|y) = p(m)p(y |m)
p(y) . (1.17)
The most probable model or model structure is the one that maximises p(m|y). For a given
model structure, we can also compute the posterior distribution over the parameters:
p(θ |y,m) = p(y |θ,m)p(θ |m)
p(y |m) , (1.18)
which allows us to quantify our uncertainty about parameter values after observing the data.
We can also compute the density at a new data point y′, obtained by averaging over both the
uncertainty in the model structure and in the parameters,
p(y′ |y) =
∑
m
∫
dθ p(y′ |θ,m, y)p(θ |m,y)p(m|y) , (1.19)
which is known as the predictive distribution.
The second term in the numerator of ( 1.17) is called the marginal likelihood, and results from
integrating the likelihood of the data over all possible parameter settings under the prior:
p(y |m) =
∫
dθ p(y |θ,m)p(θ |m) . (1.20)
25
Introduction 1.2. Bayesian model selection
In the machine learning community this quantity is sometimes referred to as the evidence for
model m, as it constitutes the data-dependent factor in the posterior distribution over models
(1.17). In the absence of an informative prior p(m) over possible model structures, this term
alone will drive our model inference process. Note that this term also appears as the normal-
isation constant in the denominator of ( 1.18). We can think of the marginal likelihood as the
average probability of the data, where the average is taken with respect to the model parameters
drawn from the prior p(θ).
Integrating out the parameters penalises models with more degrees of freedom since these mod-
els can a priori model a larger range of data sets. This property of Bayesian integration has been
called Occam’s razor, since it favours simpler explanations (models) for the data over complex
ones (Jefferys and Berger, 1992; MacKay, 1995). Having more parameters may impart an ad-
vantage in terms of the ability to model the data, but this is offset by the cost of having to code
those extra parameters under the prior (Hinton and van Camp, 1993). The overﬁtting problem is
avoided simply because no parameter in the pure Bayesian approach is actuallyﬁt to the data. A
caricature of Occam’s razor is given in ﬁgure1.3, where the horizontal axis denotes all possible
data sets to be modelled, and the vertical axis is the marginal probability p(y |m) under each
of three models of increasing complexity. We can relate the complexity of a model to the range
of data sets it can capture. Thus for a simple model the probability is concentrated over a small
range of data sets, and conversely a complex model has the ability to model a wide range of data
sets.
Since the marginal likelihood as a function of the data y should integrate to one, the simple
model can give a higher marginal likelihood to those data sets it can model, whilst the complex
model gives only small marginal likelihoods to a wide range of data sets. Therefore, given a
data set, y, on the basis of the marginal likelihood it is possible to discard both models that are
too complex and those that are too simple. In these arguments it is tempting, but not correct, to
associate the complexity of a model with the number of parameters it has: it is easy to come up
with a model with many parameters that can model only a limited range of data sets, and also
to design a model capable of capturing a huge range of data sets with just a single parameter
(speciﬁed to high precision).
We have seen how the marginal likelihood is an important quantity in Bayesian learning, for
computing quantities such as Bayes factors (the ratio of two marginal likelihoods, Kass and
Raftery, 1995), or the normalising constant of a posterior distribution (known in statistical
physics as the ‘partition function’ and in machine learning as the ‘evidence’). Unfortunately
the marginal likelihood is a very difﬁcult quantity to compute because it involves integrating
over all parameters and latent variables, which is usually such a high dimensional and compli-
cated integral that most simple approximations fail catastrophically. We will see in section 1.3
some of the approximations to the marginal likelihood and will investigate variational Bayesian
approximations in the following chapter.
26
Introduction 1.2. Bayesian model selection
too simple
too complex
"just right"
space of all data sets
marginal likelihood  p(y|m)
Y
Figure 1.3: Caricature depicting Occam’s razor (adapted from MacKay, 1995). The horizon-
tal axis denotes all possible data sets of a particular size and the vertical axis is the marginal
likelihood for three different model structures of differing complexity. Simple model structures
can model certain data sets well but cannot model a wide range of data sets; complex model
structures can model many different data sets but, since the marginal likelihood has to integrate
to one, will necessarily not be able to model all simple data sets as well as the simple model
structure. Given a particular data set (labelled Y), model selection is possible because model
structures that are too simple are unlikely to generate the data set in question, while model
structures that are too complex can generate many possible data sets, but again, are unlikely to
generate that particular data set at random.
It is important to keep in mind that a realistic model of the data might need to be complex.
It is therefore often advisable to use the most ‘complex’ model for which it is possible to do
inference, ideally setting up priors that allow the limit of inﬁnitely many parameters to be taken,
rather than to artiﬁcially limit the number of parameters in the model ( Neal, 1996; Rasmussen
and Ghahramani, 2001). Although we do not examine any such inﬁnite models in this thesis,
we do return to them in the concluding comments of chapter 7.
Bayes’ theorem provides us with the posterior over different models ( 1.17), and we can com-
bine predictions by weighting them according to the posterior probabilities ( 1.19). Although
in theory we should average over all possible model structures, in practice computational or
representational constraints may make it necessary to select a single most probable structure
by maximising p(m|y). In most problems we may also have good reason to believe that the
marginal likelihood is strongly peaked, and so the task of model selection is then justiﬁed.
1.2.2 Choice of priors
Bayesian model inference relies on the marginal likelihood, which has at its core a set of prior
distributions over the parameters of each possible structure, p(θ |m). Speciﬁcation of param-
eter priors is obviously a key element of the Bayesian machinery, and there are several diverse
27
Introduction 1.2. Bayesian model selection
schools of thought when it comes to assigning priors; these can be loosely categorised into sub-
jective, objective, and empirical approaches. We should point out that all Bayesian approaches
are necessarily subjective in the sense that any Bayesian inference ﬁrst requires some expression
of prior knowledge p(θ). Here the emphasis is not on whether we use a prior or not, but rather
what knowledge (if any) is conveyed inp(θ). We expand on these three types of prior design in
the following paragraphs.
Subjective priors
The subjective Bayesian attempts to encapsulate prior knowledge as fully as possible, be it in
the form of previous experimental data or expert knowledge. It is often difﬁcult to articulate
qualitative experience or beliefs in mathematical form, but one very convenient and analytically
favourable class of subjective priors are conjugate priors in the exponential family. Generally
speaking, a prior is conjugate if the posterior distribution resulting from multiplying the likeli-
hood and prior terms is of the same form as the prior. Expressed mathematically:
f(θ |˜µ) = p(θ |y) ∝f(θ |µ)p(y |θ) , (1.21)
where f(θ |µ) is some probability distribution speciﬁed by a parameter (or set of parameters)
µ. Conjugate priors have at least three advantages: ﬁrst, they often lead to analytically tractable
Bayesian integrals; second, if computing the posterior in ( 1.21) is tractable, then the modeller
can be assured that subsequent inferences, based on using the posterior as prior, will also be
tractable; third, conjugate priors have an intuitive interpretation as expressing the results of pre-
vious (or indeed imaginary) observations under the model. The latter two advantages are some-
what related, and can be understood by observing that the only likelihood functions p(y |θ) for
which conjugate prior families exist are those belonging to general exponential family models.
The deﬁnition of an exponential family model is one that has a likelihood function of the form
p(yi|θ) = g(θ) f(yi) eφ(θ)⊤u(yi) , (1.22)
where g(θ) is a normalisation constant:
g(θ)−1 =
∫
dyi f(yi) eφ(θ)⊤u(yi) , (1.23)
and we have used the subscript notationyi to denote each data point (not each variable!). We as-
sume that ndata points arrive independent and identically distributed (i.i.d.) such that the prob-
ability of the data y = {y1,..., yn}under this model is given by p(y |θ) = ∏n
i=1 p(yi|θ).
28
Introduction 1.2. Bayesian model selection
Here φ(θ) is a vector of so-called natural parameters, and u(yi) and f(yi) are functions deﬁn-
ing the exponential family. Now consider the conjugate prior:
p(θ |η,ν) = h(η,ν) g(θ)ηeφ(θ)⊤ν , (1.24)
where ηand ν are parameters of the prior, andh(η,ν) is an appropriate normalisation constant.
The conjugate prior contains the same functions g(θ) and φ(θ) as in (1.22), and the result of
using a conjugate prior can then be seen by substituting ( 1.22) and (1.24) into (1.21), resulting
in:
p(θ |y) ∝p(θ |η,ν)p(y |θ) ∝p(θ |˜η,˜ν) , (1.25)
where ˜η= η+ nand ˜ν = ν + ∑n
i=1 u(yi) are the new parameters for the posterior distribution
which has the same functional form as the prior. We have omitted some of the details, as a
more general approach will be described in the following chapter (section 2.4). The important
point to note is that the parameters of the prior can be viewed as the number (or amount), η,
and the ‘value’, ν, of imaginary data observed prior to the experiment (by ‘value’ we in fact
refer to the vector of sufﬁcient statistics of the data). This correspondence is often apparent
in the expressions for predictive densities and other quantities which result from integrating
over the posterior distribution, where statistics gathered from the data are simply augmented
with prior quantities. Therefore the knowledge conveyed by the conjugate prior is speciﬁc and
clearly interpretable. On a more mathematical note, the attraction of the conjugate exponential
family of models is that they can represent probability densities with a ﬁnite number of sufﬁcient
statistics, and are closed under the operation of Bayesian inference. Unfortunately, a conjugate
analysis becomes difﬁcult, and for the majority of interesting problems impossible, for models
containing hidden variables xi.
Objective priors
The objective Bayesian’s goal is in stark contrast to a subjectivist’s approach. Instead of at-
tempting to encapsulate rich knowledge into the prior, the objective Bayesian tries to impart as
little information as possible in an attempt to allow the data to carry as much weight as possible
in the posterior distribution. This is often called ‘letting the data speak for themselves’ or ‘prior
ignorance’. There are several reasons why a modeller may want to resort to the use of objec-
tive priors (sometimes called non-informative priors): often the modeller has little expertise and
does not want to sway the inference process in any particular direction unknowingly; it may be
difﬁcult or impossible to elicit expert advice or translate expert opinions into a mathematical
form for the prior; also, the modeller may want the inference to be robust to misspeciﬁcations of
the prior. It turns out that expressing such vagueness or ignorance is in fact quite difﬁcult, partly
because the very concept of ‘vagueness’ is itself vague. Any prior expressed on the parameters
29
Introduction 1.2. Bayesian model selection
has to follow through and be manifest in the posterior distribution in some way or other, so this
quest for uninformativeness needs to be more precisely deﬁned.
One such class of noninformative priors are reference priors. These originate from an infor-
mation theoretic argument which asks the question: “which prior should I use such that I max-
imise the expected amount of information about a parameter that is provided by observing the
data?”. This expected information can be written as a function of p(θ) (we assume θ is one-
dimensional):
I(p(θ),n) =
∫
dy(n) p(y(n))
∫
dθp(θ|y(n)) ln p(θ|y(n))
p(θ) , (1.26)
where we use y(n) to make it obvious that the data set is of sizen. This quantity is strictly posi-
tive as it is an expected Kullback-Leibler (KL) divergence between the parameter posterior and
parameter prior, where the expectation is taken with respect to the underlying distribution of the
data y(n). Here we assume, as before, that the data arrive i.i.d. such that y(n) = {y1,..., yn}
and p(y(n) |θ) = ∏n
i=1 p(yi|θ). Then the n-reference prior is deﬁned as the prior that max-
imises this expected information from ndata points:
pn(θ) = arg max
p(θ)
I(p(θ),n) . (1.27)
Equation (1.26) can be rewritten directly as a KL divergence:
I(p(θ),y(n)) =
∫
dθp(θ) ln fn(θ)
p(θ) , (1.28)
where the function fn(θ) is given by
fn(θ) = exp
[∫
dy(n) p(y(n) |θ) ln p(θ|y(n))
]
, (1.29)
and nis the size of the data set y. A naive solution that maximises (1.28) is
pn(θ) ∝fn(θ) , (1.30)
but unfortunately this is only an implicit solution for the n-reference prior as fn(θ) (1.29) is a
function of the prior through the term p(θ|y(n)). Instead, we make the approximation for large
n that the posterior distribution p(θ|y(n)) ∝p(θ) ∏n
i=1 p(yi|θ) is given by p∗(θ|y(n)) ∝∏n
i=1 p(yi|θ), and write the reference prior as:
p(θ) ∝ lim
n→∞
f∗
n(θ)
f∗n(θ0) , (1.31)
where f∗
n(θ) is the expression ( 1.29) using the approximation to the posterior p∗(θ|y(n)) in
place of p(θ|y(n)), and θ0 is a ﬁxed parameter (or subset of parameters) used to normalise the
30
Introduction 1.2. Bayesian model selection
limiting expression. For discrete parameter spaces, it can be shown that the reference prior is
uniform. More interesting is the case of real-valued parameters that exhibit asymptotic normal-
ity in their posterior (see section 1.3.2), where it can be shown that the reference prior coincides
with Jeffreys’ prior (see Jeffreys, 1946),
p(θ) ∝h(θ)1/2 , (1.32)
where h(θ) is the Fisher information
h(θ) =
∫
dyi p(yi|θ)
[
−∂2
∂θ2 ln p(yi|θ)
]
. (1.33)
Jeffreys’ priors are motivated by requiring that the prior is invariant to one-to-one reparameteri-
sations, so this equivalence is intriguing. Unfortunately, the multivariate extensions of reference
and Jeffreys’ priors are fraught with complications. For example, the form of the reference prior
for one parameter can be different depending on the order in which the remaining parameters’
reference priors are calculated. Also multivariate Jeffreys’ priors are not consistent with their
univariate equivalents. As an example, consider the mean and standard deviation parameters of
a Gaussian, (µ,σ). If µis known, both Jeffreys’ and reference priors are given by p(σ) ∝σ−1.
If the standard deviation is known, again both Jeffreys’ and reference priors over the mean
are given by p(µ) ∝1. However, if neither the mean nor the standard deviation are known,
the Jeffreys’ prior is given by p(µ,σ) ∝σ−2, which does not agree with the reference prior
p(µ,σ) ∝σ−1 (here the reference prior happens not to depend on the ordering of the parame-
ters in the derivation). This type of ambiguity is often a problem in deﬁning priors over multiple
parameters, and it is often easier to consider other ways of specifying priors, such as hierarchi-
cally. A more in depth analysis of reference and Jeffreys’ priors can be found in Bernardo and
Smith (1994, section 5.4).
Empirical Bayes and hierarchical priors
When there are many common parameters in the vector θ = ( θ1,...,θ K), it often makes sense
to consider each parameter as being drawn from the same prior distribution. An example of this
would be the prior speciﬁcation of the means of each of the Gaussian components in a mixture
model — there is generally no a priori reason to expect any particular component to be different
from another. The parameter prior is then formed from integrating with respect to a hyperprior
with hyperparameter γ:
p(θ |γ) =
∫
dγ p(γ)
K∏
k=1
p(θk|γ) . (1.34)
Therefore, each parameter is independent given the hyperparameter, although they are depen-
dent marginally. Hierarchical priors are useful even when applied only to a single parameter,
31
Introduction 1.3. Practical Bayesian approaches
often offering a more intuitive interpretation for the parameter’s role. For example, the precision
parameter ν for a Gaussian variable is often given a (conjugate) gamma prior, which itself has
two hyperparameters (aγ,bγ) corresponding to the shape and scale of the prior. Interpreting the
marginal distribution of the variable in this generative sense is often more intuitively appealing
than simply enforcing a Student-t prior. Hierarchical priors are often designed using conjugate
forms (described above), both for analytical ease and also because previous knowledge can be
readily expressed.
Hierarchical priors can be easily visualised using directed graphical models, and there will be
many examples in the following chapters. The phrase empirical Bayes refers to the practice of
optimising the hyperparameters (e.g. γ) of the priors, so as to maximise the marginal likelihood
of a data setp(y |γ). In this way Bayesian learning can be seen as maximum marginal likelihood
learning, where there are always distributions over the parameters, but the hyperparameters are
optimised just as in maximum likelihood learning. This practice is somewhat suboptimal as it
ignores the uncertainty in the hyperparameter γ. Alternatively, a more coherent approach is to
deﬁne priors over the hyperparameters and priors on the parameters of those priors, etc., to the
point where at the top level the modeller is content to leave those parameters unoptimised. With
sufﬁciently vague priors at the top level, the posterior distributions over intermediate parameters
should be determined principally by the data. In this fashion, no parameters are actually ever ﬁt
to the data, and all predictions and inferences are based on the posterior distributions over the
parameters.
1.3 Practical Bayesian approaches
Bayes’ rule provides a means of updating the distribution over parameters from the prior to the
posterior distribution in light of observed data. In theory, the posterior distribution captures all
information inferred from the data about the parameters. This posterior is then used to make
optimal decisions or predictions, or to select between models. For almost all interesting appli-
cations these integrals are analytically intractable, and are inaccessible to numerical integration
techniques — not only do the computations involve very high dimensional integrals, but for
models with parameter symmetries (such as mixture models) the integrand can have exponen-
tially many modes.
There are various ways we can tackle this problem. At one extreme we can restrict ourselves
only to models and prior distributions that lead to tractable posterior distributions and inte-
grals for the marginal likelihoods and predictive densities. This is highly undesirable since it
inevitably leads us to lose prior knowledge and modelling power. More realistically, we can
approximate the exact answer.
32
Introduction 1.3. Practical Bayesian approaches
1.3.1 Maximum a posteriori (MAP) parameter estimates
The simplest approximation to the posterior distribution is to use a point estimate, such as the
maximum a posteriori (MAP) parameter estimate,
ˆθ = arg max
θ
p(θ)p(y |θ) , (1.35)
which chooses the model with highest posterior probability density (the mode). Whilst this esti-
mate does contain information from the prior, it is by no means completely Bayesian (although
it is often erroneously claimed to be so) since the mode of the posterior may not be represen-
tative of the posterior distribution at all. In particular, we are likely (in typical models) to be
over-conﬁdent of predictions made with the MAP model, since by deﬁnition all the posterior
probability mass is contained in models which give poorer likelihood to the data (modulo the
prior inﬂuence). In some cases it might be argued that instead of the MAP estimate it is sufﬁ-
cient to specify instead a set ofcredible regionsor ranges in which most of the probability mass
for the parameter lies (connected credible regions are called credible ranges). However, both
point estimates and credible regions (which are simply a collection of point estimates) have the
drawback that they are not unique: it is always possible to ﬁnd a one-to-one monotonic mapping
of the parameters such that any particular parameter setting is at the mode of the posterior prob-
ability density in that mapped space (provided of course that that value has non-zero probability
density under the prior). This means that two modellers with identical priors and likelihood
functions will in general ﬁnd different MAP estimates if their parameterisations of the model
differ.
The key ingredient in the Bayesian approach is then not just the use of a prior but the fact that
all variables that are unknown are averaged over, i.e. that uncertainty is handled in a coherent
way. In this way is it not important which parameterisation we adopt because the parameters
are integrated out.
In the rest of this section we review some of the existing methods for approximating marginal
likelihoods. The ﬁrst three methods are analytical approximations: the Laplace method ( Kass
and Raftery, 1995), the Bayesian Information Criterion (BIC; Schwarz, 1978), and the criterion
due to Cheeseman and Stutz (1996). All these methods make use of the MAP estimate ( 1.35),
and in some way or other try to account for the probability mass about the mode of the posterior
density. These methods are attractive because ﬁnding the MAP estimate is usually a straight-
forward procedure. To almost complete the toolbox of practical methods for Bayesian learning,
there follows a brief survey of sampling-based approximations, such as importance sampling
and Markov chain Monte Carlo methods. We leave the topic of variational Bayesian learning
until the next chapter, where we will look back to these approximations for comparison.
33
Introduction 1.3. Practical Bayesian approaches
1.3.2 Laplace’s method
By Bayes’ rule, the posterior over parameters θ of a model mis
p(θ |y,m) = p(θ |m) p(y |θ,m)
p(y |m) . (1.36)
Deﬁning the logarithm of the numerator as
t(θ) ≡ln [p(θ |m) p(y |θ,m)] = ln p(θ |m) +
n∑
i=1
ln p(yi|θ,m) , (1.37)
the Laplace approximation (Kass and Raftery , 1995; MacKay, 1995) makes a local Gaussian
approximation around a MAP parameter estimate ˆθ (1.35). The validity of this approximation
is based on the large data limit and some regularity conditions which are discussed below. We
expand t(θ) to second order as a Taylor series about this point:
t(θ) = t(ˆθ) + ( θ −ˆθ)⊤ ∂t(θ)
∂θ
⏐⏐⏐⏐
θ=ˆθ
+ 1
2!(θ −ˆθ)⊤ ∂2t(θ)
∂θ∂θ⊤
⏐⏐⏐⏐
θ=ˆθ
(θ −ˆθ) + ... (1.38)
≈t(ˆθ) + 1
2(θ −ˆθ)⊤H(ˆθ)(θ −ˆθ) , (1.39)
where H(ˆθ) is the Hessian of the log posterior (matrix of the second derivatives of ( 1.37)),
evaluated at ˆθ,
H(ˆθ) = ∂2 ln p(θ |y,m)
∂θ∂θ⊤
⏐⏐⏐⏐
θ=ˆθ
= ∂2t(θ)
∂θ∂θ⊤
⏐⏐⏐⏐
θ=ˆθ
, (1.40)
and the linear term has vanished as the gradient of the posterior ∂t(θ)
∂θ at ˆθ is zero as this is the
MAP setting (or a local maximum). Substituting ( 1.39) into the log marginal likelihood and
integrating yields
ln p(y |m) = ln
∫
dθ p(θ |m) p(y |θ,m) (1.41)
= ln
∫
dθ exp [t(θ)] , (1.42)
≈t(ˆθ) + 1
2 ln
⏐⏐2πH−1⏐⏐ (1.43)
= ln p(ˆθ |m) + ln p(y |ˆθ,m) + d
2 ln 2π−1
2 ln |H|, (1.44)
where dis the dimensionality of the parameter space. Equation (1.44) can be written
p(y |m)Laplace = p(ˆθ |m) p(y |ˆθ,m)
⏐⏐2πH−1⏐⏐1/2
. (1.45)
Thus the Laplace approximation to the marginal likelihood consists of a term for the data likeli-
hood at the MAP setting, a penalty term from the prior, and a volume term calculated from the
local curvature.
34
Introduction 1.3. Practical Bayesian approaches
Approximation (1.45) has several shortcomings. The Gaussian assumption is based on the large
data limit, and will represent the posterior poorly for small data sets for which, in principle, the
advantages of Bayesian integration over ML or MAP are largest. The Gaussian approximation
is also poorly suited to bounded, constrained, or positive parameters, such as mixing proportions
or precisions, since it assigns non-zero probability mass outside of the parameter domain. Of
course, this can often be alleviated by a change of parameter basis (see for example, MacKay,
1998); however there remains the undesirable fact that in the non-asymptotic regime the ap-
proximation is still not invariant to reparameterisation. Moreover, the posterior may not be log
quadratic for likelihoods with hidden variables, due to problems of identiﬁability discussed in
the next subsection. In these cases the regularity conditions required for convergence do not
hold. Even if the exact posterior is unimodal the resulting approximation may well be a poor
representation of the nearby probabilitymass, as the approximation is made about a locally max-
imum probability density. The volume term requires the calculation of |H|: this takes O(nd2)
operations to compute the derivatives in the Hessian, and then a further O(d3) operations to
calculate the determinant; this becomes burdensome for high dimensions, so approximations
to this calculation usually ignore off-diagonal elements or assume a block-diagonal structure
for the Hessian, which correspond to neglecting dependencies between parameters. Finally, the
second derivatives themselves may be intractable to compute.
1.3.3 Identiﬁability: aliasing and degeneracy
The convergence to Gaussian of the posterior holds only if the model is identiﬁable. Therefore
the Laplace approximation may be inaccurate if this is not the case. A model is not identiﬁable
if there is aliasing or degeneracy in the parameter posterior.
Aliasing arises in models with symmetries, where the assumption that there exists a single mode
in the posterior becomes incorrect. As an example of symmetry, take the model containing a
discrete hidden variable xi with k possible settings (e.g. the indicator variable in a mixture
model). Since the variable is hidden these settings can be arbitrarily labelled k! ways. If the
likelihood is invariant to these permutations, and if the prior over parameters is also invariant to
these permutations, then the landscape for the posterior parameter distribution will be made up
of k! identical aliases. For example the posterior for HMMs converges to a mixture of Gaussians,
not a single mode, corresponding to the possible permutations of the hidden states. If the aliases
are sufﬁciently distinct, corresponding to well deﬁned peaks in the posterior as a result of large
amounts of data, the error in the Laplace method can be corrected by multiplying the marginal
likelihood by a factor of k!. In practice it is difﬁcult to ascertain the degree of separation of
the aliases, and so a simple modiﬁcation of this sort is not possible. Although corrections
have been devised to account for this problem, for example estimating the permanent of the
model, they are complicated and computationally burdensome. The interested reader is referred
35
Introduction 1.3. Practical Bayesian approaches
to Barvinok (1999) for a description of a polynomial randomised approximation scheme for
estimating permanents, and to Jerrum et al. (2001) for a review of permanent calculations.
Parameter degeneracy arises when there is some redundancy in the choice of parameterisation
for the model. For example, consider a model that has two parameters θ = ( ν1,ν2), whose
difference speciﬁes the noise precision of an observed Gaussian variable yi with mean 0, say,
yi ∼ N(yi|0, ν1 −ν2). If the prior over parameters does not disambiguate ν1 from ν2, the
posterior over θ will contain an inﬁnity of distinct conﬁgurations of (ν1,ν2), all of which give
the same likelihood to the data; this degeneracy causes the volume element ∝
⏐⏐H−1⏐⏐to be
inﬁnite and renders the marginal likelihood estimate ( 1.45) useless. Parameter degeneracy can
be thought of as a continuous form of aliasing in parameter space, in which there are inﬁnitely
many aliases.
1.3.4 BIC and MDL
The Bayesian Information Criterion (BIC) ( Schwarz, 1978) can be obtained from the Laplace
approximation by retaining only those terms that grow with n. From (1.45), we have
ln p(y |m)Laplace = ln p(ˆθ |m)  
O(1)
+ ln p(y |ˆθ,m)  
O(n)
+ d
2 ln 2π
  
O(1)
−1
2 ln |H|
  
O(dln n)
, (1.46)
where each term’s dependence on n has been annotated. Retaining O(n) and O(ln n) terms
yields
ln p(y |m)Laplace = ln p(y |ˆθ,m) −1
2 ln |H|+ O(1) . (1.47)
Using the fact that the entries of the Hessian scale linearly with n(see (1.37) and (1.40)), we
can write
lim
n→∞
1
2 ln |H|= 1
2 ln |nH0|= d
2 ln n+ 1
2 ln |H0|
  
O(1)
, (1.48)
and then assuming that the prior is non-zero atˆθ, in the limit of largenequation (1.47) becomes
the BIC score:
ln p(y |m)BIC = ln p(y |ˆθ,m) −d
2 ln n. (1.49)
The BIC approximation is interesting for two reasons: ﬁrst, it does not depend on the prior
p(θ |m); second, it does not take into account the local geometry of the parameter space and
hence is invariant to reparameterisations of the model. A Bayesian would obviously baulk at
the ﬁrst of these features, but the second feature of reparameterisation invariance is appealing
because this should fall out of an exact Bayesian treatment in any case. In practice the dimension
of the model dthat is used is equal to the number of well-determined parameters, or the number
36
Introduction 1.3. Practical Bayesian approaches
of effective parameters, after any potential parameter degeneracies have been removed. In the
example mentioned above the reparameterisation ν∗ = ν1 −ν2 is sufﬁcient, yielding d =
|ν|. The BIC is in fact exactly minus the minimum description length (MDL) penalty used
in Rissanen (1987). However, the minimum message length (MML) framework of Wallace
and Freeman (1987) is closer in spirit to Bayesian integration over parameters. We will be
revisiting the BIC in the following chapters as a comparison to our variational Bayesian method
for approximating the marginal likelihood.
1.3.5 Cheeseman & Stutz’s method
If the complete-data marginal likelihood deﬁned as
p(x,y |m) =
∫
dθ p(θ |m)
n∏
i=1
p(xi,yi|θ,m) (1.50)
can be computed efﬁciently then the method proposed in Cheeseman and Stutz (1996) can be
used to approximate the marginal likelihood of incomplete data. For any completion of the data
ˆx, the following identity holds
p(y |m) = p(ˆx,y |m) p(y |m)
p(ˆx,y |m) (1.51)
= p(ˆx,y |m)
∫
dθ p(θ |m)p(y |θ,m)∫
dθ′ p(θ′ |m)p(ˆx,y |θ′,m) . (1.52)
If we now apply Laplace approximations (1.45) to both numerator and denominator we obtain
p(y |m) ≈p(ˆx,y |m) p(ˆθ |m)p(y |ˆθ,m)
⏐⏐2πH−1⏐⏐1/2
p(ˆθ
′
|m)p(ˆx,y |ˆθ
′
,m)
⏐⏐2πH′−1⏐⏐1/2 . (1.53)
If the approximations are made about the same point ˆθ
′
= ˆθ, then the hope is that errors in each
Laplace approximation will tend to cancel one another out. If the completion ˆx is set to be the
expected sufﬁcient statistics calculated from an E step of the EM algorithm (discussed in more
detail in chapter 2), then the ML/MAP setting ˆθ
′
will be at the same point as ˆθ. The ﬁnal part of
the Cheeseman-Stutz approximation is to form the BIC asymptotic limit of each of the Laplace
approximations (1.49). In the original Autoclass application (Cheeseman and Stutz, 1996) the
dimensionalities of the parameter spaces for the incomplete and complete-data integrals were
assumed equal so the terms scaling as ln ncancel. Since ˆθ
′
= ˆθ, the terms relating to the prior
probability of ˆθ and ˆθ
′
also cancel (although these are O(1) in any case), and we obtain:
p(y |m)CS = p(ˆx,y |m) p(y |ˆθ,m)
p(ˆx,y |ˆθ,m)
. (1.54)
37
Introduction 1.3. Practical Bayesian approaches
where ˆθ is the MAP estimate. In chapter 2 we see how the Cheeseman-Stutz approximation
is related to the variational Bayesian lower bound. In chapter 6 we compare its performance
empirically to variational Bayesian methods on a hard problem, and discuss the situation in
which the dimensionalities of the complete and incomplete-data parameters are different.
1.3.6 Monte Carlo methods
Unfortunately the large data limit approximations discussed in the previous section are limited
in their ability to trade-off computation time to improve their accuracy. For example, even if
the Hessian determinant were calculated exactly (costingO(nd2) operations to ﬁnd the Hessian
and then O(d3) to ﬁnd its determinant), the Laplace approximation may still be very inaccurate.
Numerical integration methods hold the answer to more accurate, but computationally intensive
solutions.
The Monte Carlo integration method estimates the expectation of a functionφ(x) under a prob-
ability distribution f(x), by taking samples {x(i)}N
i=1 : x(i) ∼ f(x). An unbiased estimate, ˆΦ,
of the expectation of φ(x) under f(x), using N samples is given by:
Φ =
∫
dx f(x)φ(x) ≃ˆΦ = 1
N
N∑
i=1
φ(x(i)) . (1.55)
Expectations such as the predictive density, the marginal likelihood, posterior distributions over
hidden variables etc. can be obtained using such estimates. Most importantly, the Monte Carlo
method returns more accurate and reliable estimates the more samples are taken, and scales well
with the dimensionality of x.
In situations where f(x) is hard to sample from, one can use samples from a different aux-
iliary distribution g(x) and then correct for this by weighting the samples accordingly. This
method is called importance sampling and it constructs the following estimator using N sam-
ples, {x(i)}N
i=1, generated such that each x(i) ∼ g(x):
Φ =
∫
dx g(x)f(x)
g(x) φ(x) ≃ˆΦ = 1
N
N∑
i=1
w(i)φ(x(i)) , (1.56)
where w(i) = f(x(i))
g(x(i)) (1.57)
are known as the importance weights. Note that the estimator in ( 1.56) is unbiased just as that
in (1.55). It is also possible to estimate Φ even if p(x) and g(x) can be computed only up to
38
Introduction 1.3. Practical Bayesian approaches
multiplicative constant factors, that is to say:f(x) = f∗(x)/Zf and g(x) = g∗(x)/Zg. In such
cases it is straightforward to show that an estimator for Φ is given by:
Φ =
∫
dx g(x)f(x)
g(x) φ(x) ≃ˆΦ =
∑N
i=1 w(i)φ(x(i))∑N
i=1 w(i) , (1.58)
where w(i) = f∗(x(i))
g∗(x(i)) (1.59)
are a slightly different set of importance weights. Unfortunately this estimate is now biased as
it is really the ratio of two estimates, and the ratio of two unbiased estimates is in general not
an unbiased estimate of the ratio. Although importance sampling is simple, ˆΦ can often have
very high variance. Indeed, even in some simple models it can be shown that the variance of the
weights w(i), and therefore of ˆΦ also, are unbounded. These and related problems are discussed
in section 4.7 of chapter 4 where importance sampling is used to estimate the exact marginal
likelihood of a mixture of factor analysers model trained with variational Bayesian EM. We use
this analysis to provide an assessment of the tightness of the variational lower bound, which
indicates how much we are conceding when using such an approximation (see section 4.7.2).
A method related to importance sampling is rejection sampling. It avoids the use of a set of
weights {w(i)}N
i=1 by stochastically deciding whether or not to include each sample from g(x).
The procedure requires the existence of a constant c such that cg(x) > f(x) for all x, that
is to say cg(x) envelopes the probability density f(x). Samples are obtained from f(x) by
drawing samples from g(x), and then accepting or rejecting each stochastically based on the
ratio of its densities under f(x) and g(x). That is to say, for each sample an auxiliary variable
u(i) ∼ U(0,1) is drawn, and the sample under g(x) accepted only if
f(x(i)) >u(i)cg(x(i)) . (1.60)
Unfortunately, this becomes impractical in high dimensions and with complex functions since
it is hard to ﬁnd a simple choice of g(x) such that cis small enough to allow the rejection rate
to remain reasonable across the whole space. Even in simple examples the acceptance rate falls
exponentially with the dimensionality of x.
To overcome the limitations of rejection sampling it is possible to adapt the density cg(x) so
that it envelopes f(x) more tightly, but only in cases where f(x) is log-concave. This method
is called adaptive rejection sampling (Gilks and Wild, 1992): the envelope function cg(x) is
piecewise exponential and is updated to more tightly ﬁt the density f(x) after each sample
is drawn. The result is that the probability of rejection monotonically decreases with each
sample evaluation. However it is only designed for log-concave f(x) and relies on gradient
information to construct tangents which upper bound the densityf(x). An interesting extension
(Gilks, 1992) to this constructs a lower bound bl(x) as well (where b is a constant) which is
updated in a similar fashion using chords between evaluations of f(x). The advantage of also
39
Introduction 1.3. Practical Bayesian approaches
using a piecewise exponential lower bound is that the method can become very computationally
efﬁcient by not having to evaluate densities under f(x) (which we presume is costly) for some
samples. To see how this is possible, consider drawing a sample x(i) which satisﬁes
bl(x(i)) >u(i)cg(x(i)) . (1.61)
This sample can be automatically accepted without evaluation of f(x(i)), since if inequality
(1.61) is satisﬁed then automatically inequality ( 1.60) is also satisﬁed. If the sample does not
satisfy (1.61), then of course f(x(i)) needs to be computed, but this can then be used to tighten
the bound further. Gilks and Wild (1992) report that the number of density evaluations required
to sample N points from f(x) increases as
3√
N, even for quite non-Gaussian densities. Their
example obtains 100 samples from the standard univariate Gaussian with approximately 15
evaluations, and a further 900 samples with only 15 further evaluations. Moreover, in cases
where the log density is close to but not log concave, the adaptive rejection sampling algorithm
can still be used with Metropolis methods (see below) to correct for this (Gilks et al., 1995).
Markov chain Monte Carlo (MCMC) methods (as reviewed in Neal, 1992) can be used to gen-
erate a chain of samples, starting from x(1), such that the next sample is a non-deterministic
function of the previous sample: x(i) P←x(i−1), where we deﬁne P(x′,x) as the probabil-
ity of transition from x′ to x. If Phas f(x) as its stationary (equilibrium) distribution, i.e.
f(x) =
∫
dx′ f(x′)P(x′,x), then the set {x(i)}N
i=1 can be used to obtain an unbiased estimate
of Φ as in (1.55) in the limit of a large number of samples. The set of samples have to drawn
from the equilibrium distribution, so it is advisable to discard all samples visited at the begin-
ning of the chain. In general Pis implemented using a proposal density x(i) ∼ g(x,x(i−1))
about the previous sample. In order to ensure reversibility of the Markov chain, the probability
of accepting the proposal needs to take into account the probability of a reverse transition. This
gives rise to the the Metropolis-Hastings ( Metropolis et al., 1953; Hastings, 1970) acceptance
function a(·,·):
a(x(i),x(i−1)) = f∗(x(i))g(x(i−1),x(i))
f∗(x(i−1))g(x(i),x(i−1)) . (1.62)
If a(x(i),x(i−1)) ≥1 the sample is accepted, otherwise it is accepted according to the prob-
ability a(x(i),x(i−1)). Several extensions to the MCMC method have been proposed includ-
ing over-relaxation ( Adler, 1981), hybrid MCMC ( Neal, 1993), and reversible-jump MCMC
(Green, 1995). These and many others can be found at the MCMC preprint service (Brooks).
Whilst MCMC sampling methods are guaranteed to yield exact estimates in the limit of a large
number of samples, even for well-designed procedures the number of samples required for ac-
curate estimates can be infeasibly large. There is a large amount of active research dedicated to
constructing measures to ascertain whether the Markov chain has reached equilibrium, whether
the samples it generates are independent, and analysing the reliability of the estimates. This
40
Introduction 1.3. Practical Bayesian approaches
thesis is concerned with fast, reliable, deterministic alternatives to MCMC. Long MCMC runs
can then be used to check the accuracy of these deterministic methods.
In contrast to MCMC methods, a new class of sampling methods has been recently devised
in which samples from exactly the equilibrium distribution are generated in a ﬁnite number of
steps of a Markov chain. These are termed exact sampling methods, and make use of trajectory
coupling and coalescence via pseudorandom transitions, and is sometimes referred to as cou-
pling from the past(Propp and Wilson, 1996). Variations on exact sampling include interruptible
algorithms (Fill, 1998) and continuous state-space versions ( Murdoch and Green, 1998). Such
methods have been applied to graphical models for machine learning problems in the contexts of
mixture modelling (Casella et al., 2000), and noisy-or belief networks (Harvey and Neal, 2000).
Finally, one important role of MCMC methods is to compute partition functions. One such pow-
erful method for computing normalisation constants, such as Zf used above, is called annealed
importance sampling (Neal, 2001). It is based on methods such as thermodynamic integration
for estimating the free energy of systems at different temperatures, and work on tempered tran-
sitions (Neal, 1996). It estimates the ratio of two normalisation constants Zt and Z0, which we
can think of for our purposes as the ratio of marginal likelihoods of two models, by collating the
results of a chain of intermediate likelihood ratios of ‘close’ models,
Zt
Z0
= Z1
Z0
... Zt−2
Zt−3
Zt−1
Zt−2
Zt
Zt−1
. (1.63)
Each of the ratios is estimated using samples from a Markov chain Monte Carlo method. We will
look at this method in much more detail in Chapter 6, where it will be used as a gold standard
against which we test the ability of the variational Bayesian EM algorithm to approximate the
marginal likelihoods of a large set of models.
To conclude this section we note that Monte Carlo is a purely frequentist procedure and in
the words of O’Hagan (1987) is ‘fundamentally unsound’. The objections raised therein can be
summarised as follows. First, the estimate ˆΦ depends on the sampling densityg(x), even though
g(x) itself is ancillary to the estimation. Put another way, the same set of samples {x(i)}n
i=1,
conveying exactly the same information aboutp(x), but generated under a differentg(x) would
produce a different estimate ˆΦ. Of course, the density g(x) is often tailored to the problem
at hand and so we would expect it to contain some of the essence of the estimate. Second,
the estimate does not depend on the location of the x(i)s, but only on function evaluations at
those points, e.g. f(x(i)). This is surely suboptimal, as the spatial distribution of the function
evaluations provides information on the integrand f(x)φ(x) as a whole. To summarise, clas-
sical Monte Carlo bases its estimate on irrelevant information, g(x), and also discards relevant
information from the location of the samples. Bayesian variants of Monte Carlo integration pro-
cedures have been devised to address these objections using Gaussian process models (O’Hagan,
1991; Rasmussen and Ghahramani, 2003), and there is much future work to do in this direction.
41
Introduction 1.4. Summary of the remaining chapters
1.4 Summary of the remaining chapters
Chapter 2 Forms the theoretical core of the thesis, and examines the use of variational meth-
ods for obtaining lower bounds on the likelihood (for point-parameter learning) and the
marginal likelihood (in the case of Bayesian learning). The implications of VB applied
to the large family of conjugate-exponential graphical models are investigated, for both
directed and undirected representations. In particular, a general algorithm for conjugate-
exponential models is derived and it is shown that existing propagation algorithms can be
employed for inference, with approximately the same complexity as for point-parameters.
In addition, the relations of VB to a number of other commonly used approximations are
covered. In particular, it is shown that the Cheeseman-Stutz (CS) score is in fact a looser
lower bound on the marginal likelihood than the VB score.
Chapter 3 Applies the results of chapter 2 to hidden Markov models (HMMs). It is shown that
it is possible to recover the number of hidden states required to model a synthetic data
set, and that the variational Bayesian algorithm can outperform maximum likelihood and
maximum a posteriori parameter learning algorithms on real data in terms of generalisa-
tion.
Chapter 4 Applies the variational Bayesian method to a mixtures of factor analysers (MFA)
problem, where it is shown that the procedure can automatically determine the optimal
number of components and the local dimensionality of each component (i.e. the number
of factors in each analyser). Through a stochastic procedure for adding components to
the model, it is possible to perform the variational optimisation incrementally and avoid
local maxima. The algorithm is shown to perform well on a variety of synthetic data sets,
and is compared to a BIC-penalised maximum likelihood algorithm on a real-world data
set of hand-written digits.
This chapter also investigates the generally applicable method of drawing importance
samples from the variational approximation to estimate the marginal likelihood and the
KL divergence between the approximate and exact posterior. Speciﬁc results applying
variants of this procedure to the MFA model are analysed.
Chapter 5 Presents an application of the theorems presented in chapter 2 to linear dynamical
systems (LDSs). The result is the derivation of a variational Bayesian input-dependent
Rauch-Tung-Striebel smoother, such that it is possible to infer the posterior hidden state
trajectory whilst integrating over all model parameters. Experiments on synthetic data
show that it is possible to infer the dimensionality of the hidden state space and determine
which dimensions of the inputs and the data are relevant. Also presented are prelimi-
nary experiments for elucidating gene-gene interactions in a well-studied human immune
response mechanism.
42
Introduction 1.4. Summary of the remaining chapters
Chapter 6 Investigates a novel application of the VB framework to approximating the marginal
likelihood of discrete-variable directed acyclic graphs (DAGs) that contain hidden vari-
ables. The VB lower bound is compared to MAP, BIC, CS, and annealed importance
sampling (AIS), on a simple (yet non-trivial) model selection task of determining which
of all possible structures within a class generated a data set.
The chapter also discusses extensions and improvements to the particular form of AIS
used, and suggests related approximations which may be of interest.
Chapter 7 Concludes the thesis with a discussion on some topics closely related to the ideas
already investigated. These include: Bethe and Kikuchi approximations, inﬁnite models,
inferring causality using the marginal likelihood, and automated algorithm derivation.
The chapter then concludes with a summary of the main contributions of the thesis.
43
Chapter 2
Variational Bayesian Theory
2.1 Introduction
This chapter covers the majority of the theory for variational Bayesian learning that will be used
in rest of this thesis. It is intended to give the reader a context for the use of variational methods
as well as a insight into their general applicability and usefulness.
In a model selection task the role of a Bayesian is to calculate the posterior distribution over a
set of models given some a priori knowledge and some new observations (data). The knowledge
is represented in the form of a prior over model structures p(m), and their parameters p(θ |m)
which deﬁne the probabilistic dependencies between the variables in the model. By Bayes’ rule,
the posterior over models mhaving seen data y is given by:
p(m|y) = p(m)p(y |m)
p(y) . (2.1)
The second term in the numerator is the marginal likelihood or evidence for a model m, and is
the key quantity for Bayesian model selection:
p(y |m) =
∫
dθ p(θ |m)p(y |θ,m) . (2.2)
For each model structure we can compute the posterior distribution over parameters:
p(θ |y,m) = p(θ |m)p(y |θ,m)
p(y |m) . (2.3)
44
VB Theory 2.1. Introduction
We might also be interested in calculating other related quantities, such as thepredictive density
of a new datum y′ given a data set y = {y1,..., yn}:
p(y′ |y,m) =
∫
dθ p(θ |y,m) p(y′ |θ,y,m) , (2.4)
which can be simpliﬁed into
p(y′ |y,m) =
∫
dθ p(θ |y,m) p(y′ |θ,m) (2.5)
if y′ is conditionally independent of y given θ. We also may be interested in calculating the
posterior distribution of a hidden variable, x′, associated with the new observation y′
p(x′ |y′,y,m) ∝
∫
dθ p(θ |y,m) p(x′,y′ |θ,m) . (2.6)
The simplest way to approximate the above integrals is to estimate the value of the integrand
at a single point estimate of θ, such as the maximum likelihood (ML) or the maximum a pos-
teriori (MAP) estimates, which aim to maximise respectively the second and both terms of the
integrand in (2.2),
θML = arg max
θ
p(y |θ,m) (2.7)
θMAP = arg max
θ
p(θ |m)p(y |θ,m) . (2.8)
ML and MAP examine only probability density, rather than mass, and so can neglect poten-
tially large contributions to the integral. A more principled approach is to estimate the integral
numerically by evaluating the integrand at many different θ via Monte Carlo methods. In the
limit of an inﬁnite number of samples of θ this produces an accurate result, but despite inge-
nious attempts to curb the curse of dimensionality in θ using methods such as Markov chain
Monte Carlo, these methods remain prohibitively computationally intensive in interesting mod-
els. These methods were reviewed in the last chapter, and the bulk of this chapter concentrates
on a third way of approximating the integral, using variational methods. The key to the varia-
tional method is to approximate the integral with a simpler form that is tractable, forming a lower
or upper bound. The integration then translates into the implementationally simpler problem of
bound optimisation: making the bound as tight as possible to the true value.
We begin in section 2.2 by describing how variational methods can be used to derive the well-
known expectation-maximisation (EM) algorithm for learning the maximum likelihood (ML)
parameters of a model. In section 2.3 we concentrate on the Bayesian methodology, in which
priors are placed on the parameters of the model, and their uncertainty integrated over to give the
marginal likelihood (2.2). We then generalise the variational procedure to yield the variational
Bayesian EM (VBEM) algorithm, which iteratively optimises a lower bound on this marginal
45
VB Theory 2.2. Variational methods for ML / MAP learning
likelihood. In analogy to the EM algorithm, the iterations consist of a variational Bayesian E
(VBE) step in which the hidden variables are inferred using anensemble of models according to
their posterior probability, and a variational Bayesian M (VBM) step in which a posteriordistri-
bution over model parameters is inferred. In section 2.4 we specialise this algorithm to a large
class of models which we call conjugate-exponential (CE): we present the variational Bayesian
EM algorithm for CE models and discuss the implications for both directed graphs (Bayesian
networks) and undirected graphs (Markov networks) in section 2.5. In particular we show that
we can incorporate existing propagation algorithms into the variational Bayesian framework
and that the complexity of inference for the variational Bayesian treatment is approximately the
same as for the ML scenario. In section 2.6 we compare VB to the BIC and Cheeseman-Stutz
criteria, and ﬁnally summarise in section 2.7.
2.2 Variational methods for ML / MAP learning
In this section we review the derivation of the EM algorithm for probabilistic models with hidden
variables. The algorithm is derived using a variational approach, and has exact and approximate
versions. We investigate themes on convexity, computational tractability, and the Kullback-
Leibler divergence to give a deeper understanding of the EM algorithm. The majority of the
section concentrates on maximum likelihood (ML) learning of the parameters; at the end we
present the simple extension to maximum a posteriori (MAP) learning. The hope is that this
section provides a good stepping-stone on to the variational Bayesian EM algorithm that is
presented in the subsequent sections and used throughout the rest of this thesis.
2.2.1 The scenario for parameter learning
Consider a model with hidden variables x and observed variables y. The parameters describ-
ing the (potentially) stochastic dependencies between variables are given by θ. In particular
consider the generative model that produces a dataset y = {y1,..., yn}consisting of n in-
dependent and identically distributed (i.i.d.) items, generated using a set of hidden variables
x = {x1,..., xn}such that the likelihood can be written as a function of θ in the following
way:
p(y |θ) =
n∏
i=1
p(yi|θ) =
n∏
i=1
∫
dxi p(xi,yi|θ) . (2.9)
The integration over hidden variables xi is required to form the likelihood of the parameters,
as a function of just the observed data yi. We have assumed that the hidden variables are
continuous as opposed to discrete (hence an integral rather than a summation), but we do so
without loss of generality. As a point of nomenclature, note that we use xi and yi to denote
collections of |xi|hidden and |yi|observed variables respectively: xi = {xi1,..., xi|xi|}, and
46
VB Theory 2.2. Variational methods for ML / MAP learning
yi = {yi1,..., yi|yi|}. We use |·|notation to denote the size of the collection of variables. ML
learning seeks to ﬁnd the parameter setting θML that maximises this likelihood, or equivalently
the logarithm of this likelihood,
L(θ) ≡ln p(y |θ) =
n∑
i=1
ln p(yi|θ) =
n∑
i=1
ln
∫
dxi p(xi,yi|θ) (2.10)
so deﬁning
θML ≡arg max
θ
L(θ) . (2.11)
To keep the derivations clear, we write Las a function of θ only; the dependence on y is im-
plicit. In Bayesian networks without hidden variables and with independent parameters, the
log-likelihood decomposes into local terms on each yij, and so ﬁnding the setting of each pa-
rameter of the model that maximises the likelihood is straightforward. Unfortunately, if some
of the variables are hidden this will in general induce dependencies between all the parameters
of the model and so make maximising (2.10) difﬁcult. Moreover, for models with many hidden
variables, the integral (or sum) over x can be intractable.
We simplify the problem of maximising L(θ) with respect to θ by introducing an auxiliary dis-
tribution over the hidden variables. Any probability distribution qx(x) over the hidden variables
gives rise to a lower bound on L. In fact, for each data point yi we use a distinct distribution
qxi(xi) over the hidden variables to obtain the lower bound:
L(θ) =
∑
i
ln
∫
dxi p(xi,yi|θ) (2.12)
=
∑
i
ln
∫
dxi qxi(xi)p(xi,yi|θ)
qxi(xi) (2.13)
≥
∑
i
∫
dxi qxi(xi) ln p(xi,yi|θ)
qxi(xi) (2.14)
=
∑
i
∫
dxi qxi(xi) ln p(xi,yi|θ) −
∫
dxi qxi(xi) ln qxi(xi) (2.15)
≡F(qx1 (x1),...,q xn(xn),θ) (2.16)
where we have made use of Jensen’s inequality (Jensen, 1906) which follows from the fact that
the log function is concave. F(qx(x),θ) is a lower bound onL(θ) and is a functional of the free
distributions qxi(xi) and of θ (the dependence on y is left implicit). Here we use qx(x) to mean
the set {qxi(xi)}n
i=1. Deﬁning the energy of a global conﬁguration (x,y) to be −ln p(x,y |θ),
the lower boundF(qx(x),θ) ≤L(θ) is the negative of a quantity known in statistical physics as
the free energy: the expected energy under qx(x) minus the entropy of qx(x) (Feynman, 1972;
Neal and Hinton, 1998).
47
VB Theory 2.2. Variational methods for ML / MAP learning
2.2.2 EM for unconstrained (exact) optimisation
The Expectation-Maximization (EM) algorithm ( Baum et al., 1970; Dempster et al., 1977) al-
ternates between an E step, which infers posterior distributions over hidden variables given a
current parameter setting, and an M step, which maximises L(θ) with respect to θ given the
statistics gathered from the E step. Such a set of updates can be derived using the lower bound:
at each iteration, the E step maximises F(qx(x),θ) with respect to each of the qxi(xi), and the
M step does so with respect to θ. Mathematically speaking, using a superscript (t) to denote
iteration number, starting from some initial parameters θ(0), the update equations would be:
E step: q(t+1)
xi ←arg max
qxi
F(qx(x),θ(t)) , ∀i∈{1,...,n }, (2.17)
M step: θ(t+1) ←arg max
θ
F(q(t+1)
x (x),θ) . (2.18)
For the E step, it turns out that the maximum over qxi(xi) of the bound ( 2.14) is obtained by
setting
q(t+1)
xi (xi) = p(xi|yi,θ(t)) , ∀i, (2.19)
at which point the bound becomes an equality. This can be proven by direct substitution of
(2.19) into (2.14):
F(q(t+1)
x (x),θ(t)) =
∑
i
∫
dxi q(t+1)
xi (xi) ln p(xi,yi|θ(t))
q(t+1)
xi (xi)
(2.20)
=
∑
i
∫
dxi p(xi|yi,θ(t)) ln p(xi,yi|θ(t))
p(xi|yi,θ(t))
(2.21)
=
∑
i
∫
dxi p(xi|yi,θ(t)) ln p(yi|θ(t)) p(xi|yi,θ(t))
p(xi|yi,θ(t))
(2.22)
=
∑
i
∫
dxi p(xi|yi,θ(t)) ln p(yi|θ(t)) (2.23)
=
∑
i
ln p(yi|θ(t)) = L(θ(t)) , (2.24)
where the last line follows as ln p(yi|θ) is not a function of xi. After this E step the bound is
tight. The same result can be obtained by functionally differentiating F(qx(x),θ) with respect
to qxi(xi), and setting to zero, subject to the normalisation constraints:
∫
dxi qxi(xi) = 1 , ∀i. (2.25)
48
VB Theory 2.2. Variational methods for ML / MAP learning
The constraints on each qxi(xi) can be implemented using Lagrange multipliers {λi}n
i=1, form-
ing the new functional:
˜F(qx(x),θ) = F(qx(x),θ) +
∑
i
λi
[∫
dxi qxi(xi) −1
]
. (2.26)
We then take the functional derivative of this expression with respect to eachqxi(xi) and equate
to zero, obtaining the following
∂
∂qxi(xi)
˜F(qx(x),θ(t)) = ln p(xi,yi|θ(t)) −ln qxi(xi) −1 + λi = 0 (2.27)
=⇒ q(t+1)
xi (xi) = exp ( −1 + λi) p(xi,yi|θ(t)) (2.28)
= p(xi|yi,θ(t)) , ∀i, (2.29)
where each λi is related to the normalisation constant:
λi = 1 −ln
∫
dxi p(xi,yi|θ(t)) , ∀i. (2.30)
In the remaining derivations in this thesis we always enforce normalisation constraints using
Lagrange multiplier terms, although they may not always be explicitly written.
The M step is achieved by simply setting derivatives of (2.14) with respect to θ to zero, which is
the same as optimising the expected energy term in (2.15) since the entropy of the hidden state
distribution qx(x) is not a function of θ:
M step: θ(t+1) ←arg max
θ
∑
i
∫
dxi p(xi|yi,θ(t)) ln p(xi,yi|θ) . (2.31)
Note that the optimisation is over the second θ in the integrand, whilst holding p(xi|yi,θ(t))
ﬁxed. Since F(q(t+1)
x (x),θ(t)) = L(θ(t)) at the beginning of each M step, and since the E
step does not change the parameters, the likelihood is guaranteed not to decrease after each
combined EM step. This is the well known lower bound interpretation of EM: F(qx(x),θ) is
an auxiliary function which lower bounds L(θ) for any qx(x), attaining equality after each E
step. These steps are shown schematically in ﬁgure 2.1. Here we have expressed the E step as
obtaining the full distribution over the hidden variables for each data point. However we note
that, in general, the M step may require only a few statistics of the hidden variables, so only
these need be computed in the E step.
2.2.3 EM with constrained (approximate) optimisation
Unfortunately, in many interesting models the data are explained by multiple interacting hid-
den variables which can result in intractable posterior distributions (Williams and Hinton, 1991;
49
VB Theory 2.2. Variational methods for ML / MAP learning
log likelihood
ln p(y | θ(t))
KL
h
q(t)
x ∥ p(x | y, θ(t))
i
F(q(t)
x , θ(t))
lower bound
E step
E step makes the
lower bound tight
ln p(y | θ(t))
= F(q(t+ 1)
x , θ(t))
KL
h
q(t+ 1)
x ∥ p(x | y, θ(t))
i
= 0
M step
new log likelihood
ln p(y | θ(t+ 1))
KL
h
q(t+ 1)
x ∥ p(x | y, θ(t+ 1))
i
F(q(t+ 1)
x , θ(t+ 1))
new lower bound
Figure 2.1: The variational interpretation of EM for maximum likelihood learning. In the E step
the hidden variable variational posterior is set to the exact posterior p(x |y,θ(t)), making the
bound tight. In the M step the parameters are set to maximise the lower bound F(q(t+1)
x ,θ)
while holding the distribution over hidden variables q(t+1)
x (x) ﬁxed.
Neal, 1992; Hinton and Zemel, 1994; Ghahramani and Jordan, 1997; Ghahramani and Hinton,
2000). In the variational approach we can constrain the posterior distributions to be of a partic-
ular tractable form, for example factorised over the variable xi = {xij}|xi|
j=1. Using calculus of
variations we can still optimiseF(qx(x),θ) as a functional of constrained distributionsqxi(xi).
The M step, which optimises θ, is conceptually identical to that described in the previous sub-
section, except that it is based on sufﬁcient statistics calculated with respect to the constrained
posterior qxi(xi) instead of the exact posterior.
We can write the lower bound F(qx(x),θ) as
F(qx(x),θ) =
∑
i
∫
dxi qxi(xi) ln p(xi,yi|θ)
qxi(xi) (2.32)
=
∑
i
∫
dxi qxi(xi) ln p(yi|θ) +
∑
i
∫
dxi qxi(xi) ln p(xi|yi,θ)
qxi(xi) (2.33)
=
∑
i
ln p(yi|θ) −
∑
i
∫
dxi qxi(xi) ln qxi(xi)
p(xi|yi,θ) . (2.34)
Thus in the E step, maximising F(qx(x),θ) with respect to qxi(xi) is equivalent to minimising
the following quantity
∫
dxi qxi(xi) ln qxi(xi)
p(xi|yi,θ) ≡KL [qxi(xi) ∥p(xi|yi,θ)] (2.35)
≥0 , (2.36)
which is the Kullback-Leibler divergence between the variational distribution qxi(xi) and the
exact hidden variable posterior p(xi|yi,θ). As is shown in ﬁgure 2.2, the E step does not
50
VB Theory 2.2. Variational methods for ML / MAP learning
log likelihood ln p(y | θ(t))
KL
h
q(t)
x ∥ p(x | y, θ(t))
i
F(q(t)
x , θ(t))lower bound
E step
constrained E step,
so lower bound
is no longer tight
ln p(y | θ(t))
KL
h
q(t+ 1)
x ∥ p(x | y, θ(t))
i
F(q(t+ 1)
x , θ(t))
M step
new log likelihood ln p(y | θ(t+ 1))
KL
h
q(t+ 1)
x ∥ p(x | y, θ(t+ 1))
i
F(q(t+ 1)
x , θ(t+ 1))new lower bound
Figure 2.2: The variational interpretation of constrained EM for maximum likelihood learn-
ing. In the E step the hidden variable variational posterior is set to that which minimises
KL
[
qx(x) ∥p(x |y,θ(t))
]
, subject to qx(x) lying in the family of constrained distributions.
In the M step the parameters are set to maximise the lower bound F(q(t+1 )
x ,θ) given the current
distribution over hidden variables.
generally result in the bound becoming an equality, unless of course the exact posterior lies in
the family of constrained posteriors qx(x).
The M step looks very similar to ( 2.31), but is based on the current variational posterior over
hidden variables:
M step: θ(t+1) ←arg max
θ
∑
i
∫
dxi q(t+1)
xi (xi) ln p(xi,yi|θ) . (2.37)
One can choose qxi(xi) to be in a particular parameterised family:
qxi(xi) = qxi(xi|λi) (2.38)
where λi = {λi1,..., λir}are rvariational parameters for each datum. If we constrain each
qxi(xi|λi) to have easily computable moments (e.g. a Gaussian), and especially ifln p(xi|yi,θ)
is polynomial in xi, then we can compute the KL divergence up to a constant and, more impor-
tantly, can take its derivatives with respect to the set of variational parametersλi of each qxi(xi)
distribution to perform the constrained E step. The E step of thevariational EM algorithm there-
fore consists of a sub-loop in which each of the qxi(xi|λi) is optimised by taking derivatives
with respect to each λis, for s= 1 ,...,r .
51
VB Theory 2.2. Variational methods for ML / MAP learning
The mean ﬁeld approximation
The mean ﬁeld approximation is the case in which each qxi(xi) is fully factorised over the
hidden variables:
qxi(xi) =
|xi|∏
j=1
qxij (xij) . (2.39)
In this case the expression for F(qx(x),θ) given by (2.32) becomes:
F(qx(x),θ) =
∑
i
∫
dxi


|xi|∏
j=1
qxij (xij) ln p(xi,yi|θ) −
|xi|∏
j=1
qxij (xij) ln
|xi|∏
j=1
qxij (xij)


(2.40)
=
∑
i
∫
dxi


|xi|∏
j=1
qxij (xij) ln p(xi,yi|θ) −
|xi|∑
j=1
qxij (xij) ln qxij (xij)

 .
(2.41)
Using a Lagrange multiplier to enforce normalisation of the each of the approximate posteriors,
we take the functional derivative of this form with respect to each qxij (xij) and equate to zero,
obtaining:
qxij (xij) = 1
Zij
exp


∫
dxi/j
|xi|∏
j′/j
qxij′ (xij′) ln p(xi,yi|θ)

 , (2.42)
for each data pointi∈{1,...,n }, and each variational factorised componentj ∈{1,..., |xi|}.
We use the notationdxi/j to denote the element of integration for all items inxi except xij, and
the notation ∏
j′/j to denote a product of all terms excludingj. For the ith datum, it is clear that
the update equation (2.42) applied to each hidden variable j in turn represents a set of coupled
equations for the approximate posterior over each hidden variable. These ﬁxed point equations
are called mean-ﬁeld equations by analogy to such methods in statistical physics. Examples of
these variational approximations can be found in the following: Ghahramani (1995); Saul et al.
(1996); Jaakkola (1997); Ghahramani and Jordan (1997).
EM for maximum a posteriori learning
In MAP learning the parameter optimisation includes prior information about the parameters
p(θ), and the M step seeks to ﬁnd
θMAP ≡arg max
θ
p(θ)p(y |θ) . (2.43)
52
VB Theory 2.3. Variational methods for Bayesian learning
In the case of an exact E step, the M step is simply augmented to:
M step: θ(t+1) ←arg max
θ
[
ln p(θ) +
∑
i
∫
dxi p(xi|yi,θ(t)) ln p(xi,yi|θ)
]
.
(2.44)
In the case of a constrained approximate E step, the M step is given by
M step: θ(t+1) ←arg max
θ
[
ln p(θ) +
∑
i
∫
dxi q(t+1)
xi (xi) ln p(xi,yi|θ)
]
. (2.45)
However, as mentioned in section 1.3.1, we reiterate that an undesirable feature of MAP esti-
mation is that it is inherently basis-dependent: it is always possible to ﬁnd a basis in which any
particular θ∗ is the MAP solution, provided θ∗ has non-zero prior probability.
2.3 Variational methods for Bayesian learning
In this section we show how to extend the above treatment to use variational methods to ap-
proximate the integrals required for Bayesian learning. By treating the parameters as unknown
quantities as well as the hidden variables, there are now correlations between the parameters
and hidden variables in the posterior. The basic idea in the VB framework is to approximate the
distribution over both hidden variables and parameters with a simpler distribution, usually one
which assumes that the hidden states and parameters are independent given the data.
There are two main goals in Bayesian learning. The ﬁrst is approximating the marginal likeli-
hood p(y |m) in order to perform model comparison. The second is approximating the posterior
distribution over the parameters of a model p(θ |y,m), which can then be used for prediction.
2.3.1 Deriving the learning rules
As before, let y denote the observed variables, x denote the hidden variables, and θ denote the
parameters. We assume a prior distribution over parameters p(θ |m) conditional on the model
m. The marginal likelihood of a model, p(y |m), can be lower bounded by introducing any
53
VB Theory 2.3. Variational methods for Bayesian learning
distribution over both latent variables and parameters which has support where p(x,θ |y,m)
does, by appealing to Jensen’s inequality once more:
ln p(y |m) = ln
∫
dθ dx p(x,y,θ |m) (2.46)
= ln
∫
dθ dx q(x,θ)p(x,y,θ |m)
q(x,θ) (2.47)
≥
∫
dθ dx q(x,θ) ln p(x,y,θ |m)
q(x,θ) . (2.48)
Maximising this lower bound with respect to the free distribution q(x,θ) results in q(x,θ) =
p(x,θ |y,m) which when substituted above turns the inequality into an equality (in exact
analogy with ( 2.19)). This does not simplify the problem since evaluating the exact poste-
rior distribution p(x,θ |y,m) requires knowing its normalising constant, the marginal likeli-
hood. Instead we constrain the posterior to be a simpler, factorised (separable) approximation
to q(x,θ) ≈qx(x)qθ(θ):
ln p(y |m) ≥
∫
dθ dx qx(x)qθ(θ) ln p(x,y,θ |m)
qx(x)qθ(θ) (2.49)
=
∫
dθ qθ(θ)
[∫
dx qx(x) ln p(x,y |θ,m)
qx(x) + ln p(θ |m)
qθ(θ)
]
(2.50)
= Fm(qx(x),qθ(θ)) (2.51)
= Fm(qx1 (x1),...,q xn(xn),qθ(θ)) , (2.52)
where the last equality is a consequence of the data y arriving i.i.d. (this is shown in theorem
2.1 below). The quantity Fm is a functional of the free distributions, qx(x) and qθ(θ).
The variational Bayesian algorithm iteratively maximises Fm in (2.51) with respect to the free
distributions, qx(x) and qθ(θ), which is essentially coordinate ascent in the function space of
variational distributions. The following very general theorem provides the update equations for
variational Bayesian learning.
Theorem 2.1: Variational Bayesian EM (VBEM).
Let m be a model with parameters θ giving rise to an i.i.d. data set y = {y1,... yn}with
corresponding hidden variables x = {x1,... xn}. A lower bound on the model log marginal
likelihood is
Fm(qx(x),qθ(θ)) =
∫
dθ dx qx(x)qθ(θ) ln p(x,y,θ |m)
qx(x)qθ(θ) (2.53)
and this can be iteratively optimised by performing the following updates, using superscript (t)
to denote iteration number:
VBE step: q(t+1)
xi (xi) = 1
Zxi
exp
[∫
dθ q(t)
θ (θ) ln p(xi,yi|θ,m)
]
∀i (2.54)
54
VB Theory 2.3. Variational methods for Bayesian learning
where
q(t+1)
x (x) =
n∏
i=1
q(t+1)
xi (xi) , (2.55)
and
VBM step: q(t+1)
θ (θ) = 1
Zθ
p(θ |m) exp
[∫
dx q(t+1)
x (x) ln p(x,y |θ,m)
]
. (2.56)
Moreover, the update rules converge to a local maximum ofFm(qx(x),qθ(θ)) .
Proof of qxi(xi) update: using variational calculus.
Take functional derivatives ofFm(qx(x),qθ(θ)) with respect to qx(x), and equate to zero:
∂
∂qx(x)Fm(qx(x),qθ(θ)) =
∫
dθ qθ(θ)
[ ∂
∂qx(x)
∫
dx qx(x) ln p(x,y |θ,m)
qx(x)
]
(2.57)
=
∫
dθ qθ(θ) [ln p(x,y |θ,m) −ln qx(x) −1] (2.58)
= 0 (2.59)
which implies
ln q(t+1)
x (x) =
∫
dθ q(t)
θ (θ) ln p(x,y |θ,m) −ln Z(t+1)
x , (2.60)
where Zx is a normalisation constant (from a Lagrange multiplier term enforcing normalisation
of qx(x), omitted for brevity). As a consequence of the i.i.d. assumption, this update can be
broken down across the ndata points
ln q(t+1)
x (x) =
∫
dθ q(t)
θ (θ)
n∑
i=1
ln p(xi,yi|θ,m) −ln Z(t+1)
x , (2.61)
which implies that the optimal q(t+1)
x (x) is factorised in the form q(t+1)
x (x) = ∏n
i=1 q(t+1)
xi (xi),
with
ln q(t+1)
xi (xi) =
∫
dθ q(t)
θ (θ) ln p(xi,yi|θ,m) −ln Z(t+1)
xi ∀i, (2.62)
with Zx =
n∏
i=1
Zxi . (2.63)
Thus for a given qθ(θ), there is a unique stationary point for each qxi(xi).
Proof of qθ(θ) update: using variational calculus.
55
VB Theory 2.3. Variational methods for Bayesian learning
log marginal likelihood
ln p(y | m)
KL
h
q(t)
x q(t)
θ ∥ p(x, θ | y)
i
F(q(t)
x (x), q(t)
θ (θ))
lower bound
VBE step
ln p(y | m)
KL
h
q(t+ 1)
x q(t)
θ ∥ p(x, θ | y)
i
F(q(t+ 1)
x (x), q(t)
θ (θ))
new lower bound
VBM step
ln p(y | m)
KL
h
q(t+ 1)
x q(t+ 1)
θ ∥ p(x, θ | y)
i
F(q(t+ 1)
x (x), q(t+ 1)
θ (θ))
newer lower bound
Figure 2.3: The variational Bayesian EM (VBEM) algorithm. In the VBE step, the variational
posterior over hidden variablesqx(x) is set according to (2.60). In the VBM step, the variational
posterior over parameters is set according to ( 2.56). Each step is guaranteed to increase (or
leave unchanged) the lower bound on the marginal likelihood. (Note that the exact log marginal
likelihood is a ﬁxed quantity, and does not change with VBE or VBM steps — it is only the
lower bound which increases.)
Proceeding as above, take functional derivatives ofFm(qx(x),qθ(θ)) with respect to qθ(θ) and
equate to zero yielding:
∂
∂qθ(θ)Fm(qx(x),qθ(θ)) = ∂
∂qθ(θ)
∫
dθ qθ(θ)
[∫
dx qx(x) ln p(x,y |θ,m) (2.64)
+ ln p(θ |m)
qθ(θ)
]
(2.65)
=
∫
dx qx(x) ln p(x,y |θ) + ln p(θ |m) −ln qθ(θ) + c′ (2.66)
= 0 , (2.67)
which upon rearrangement produces
ln q(t+1)
θ (θ) = ln p(θ |m) +
∫
dx q(t+1)
x (x) ln p(x,y |θ) −ln Z(t+1)
θ , (2.68)
where Zθ is the normalisation constant (related to the Lagrange multiplier which has again been
omitted for succinctness). Thus for a given qx(x), there is a unique stationary point for qθ(θ).
56
VB Theory 2.3. Variational methods for Bayesian learning
At this point it is well worth noting the symmetry between the hidden variables and the param-
eters. The individual VBE steps can be written as one batch VBE step:
q(t+1)
x (x) = 1
Zx
exp
[∫
dθ q(t)
θ (θ) ln p(x,y |θ,m)
]
(2.69)
with Zx =
n∏
i=1
Zxi . (2.70)
On the surface, it seems that the variational update rules ( 2.60) and ( 2.56) differ only in the
prior term p(θ |m) over the parameters. There actually also exists a prior term over the hidden
variables as part of p(x,y |θ,m), so this does not resolve the two. The distinguishing feature
between hidden variables and parameters is that the number of hidden variables increases with
data set size, whereas the number of parameters is assumed ﬁxed.
Re-writing (2.53), it is easy to see that maximising Fm(qx(x),qθ(θ) is simply equivalent to
minimising the KL divergence between qx(x) qθ(θ) and the joint posterior over hidden states
and parameters p(x,θ |y,m):
ln p(y |m) −Fm(qx(x),qθ(θ)) =
∫
dθ dx qx(x) qθ(θ) ln qx(x) qθ(θ)
p(x,θ |y,m) (2.71)
= KL [ qx(x) qθ(θ) ∥p(x,θ |y,m)] (2.72)
≥0 . (2.73)
Note the similarity between expressions (2.35) and (2.72): while we minimise the former with
respect to hidden variable distributions and the parameters, the latter we minimise with respect
to the hidden variable distribution and a distribution over parameters.
The variational Bayesian EM algorithm reduces to the ordinary EM algorithm for ML estimation
if we restrict the parameter distribution to a point estimate, i.e. a Dirac delta function, qθ(θ) =
δ(θ −θ∗), in which case the M step simply involves re-estimating θ∗. Note that the same
cannot be said in the case of MAP estimation, which is inherently basis dependent, unlike both
VB and ML algorithms. By construction, the VBEM algorithm is guaranteed to monotonically
increase an objective function F, as a function of a distribution over parameters and hidden
variables. Since we integrate over model parameters there is a naturally incorporated model
complexity penalty. It turns out that for a large class of models (see section 2.4) the VBE
step has approximately the same computational complexity as the standard E step in the ML
framework, which makes it viable as a Bayesian replacement for the EM algorithm.
57
VB Theory 2.3. Variational methods for Bayesian learning
2.3.2 Discussion
The impact of the q(x,θ) ≈qx(x)qθ(θ) factorisation
Unless we make the assumption that the posterior over parameters and hidden variables fac-
torises, we will not generally obtain the further hidden variable factorisation over n that we
have in equation (2.55). In that case, the distributions of xi and xj will be coupled for all cases
{i,j}in the data set, greatly increasing the overall computational complexity of inference. This
further factorisation is depicted in ﬁgure 2.4 for the case of n= 3 , where we see: (a) the origi-
nal directed graphical model, where θ is the collection of parameters governing prior distribu-
tions over the hidden variablesxi and the conditional probability p(yi|xi,θ); (b) the moralised
graph given the data {y1,y2,y3}, which shows that the hidden variables are now dependent
in the posterior through the uncertain parameters; (c) the effective graph after the factorisation
assumption, which not only removes arcs between the parameters and hidden variables, but also
removes the dependencies between the hidden variables. This latter independence falls out from
the optimisation as a result of the i.i.d. nature of the data, and is not a further approximation.
Whilst this factorisation of the posterior distribution over hidden variables and parameters may
seem drastic, one can think of it as replacing stochastic dependencies between x and θ with
deterministic dependencies between relevant moments of the two sets of variables. The ad-
vantage of ignoring how ﬂuctuations in x induce ﬂuctuations in θ (and vice-versa) is that we
can obtain analytical approximations to the log marginal likelihood. It is these same ideas that
underlie mean-ﬁeld approximations from statistical physics, from where these lower-bounding
variational approximations were inspired ( Feynman, 1972; Parisi, 1988). In later chapters the
consequences of the factorisation for particular models are studied in some detail; in particular
we will use sampling methods to estimate by how much the variational bound falls short of the
marginal likelihood.
What forms for qx(x) and qθ(θ) ?
One might need to approximate the posterior further than simply the hidden-variable / parameter
factorisation. A common reason for this is that the parameter posterior may still be intractable
despite the hidden-variable / parameter factorisation. The free-form extremisation of Fnor-
mally provides us with a functional form for qθ(θ), but this may be unwieldy; we therefore
need to assume some simpler space of parameter posteriors. The most commonly used distribu-
tions are those with just a few sufﬁcient statistics, such as the Gaussian or Dirichlet distributions.
Taking a Gaussian example, Fis then explicitly extremised with respect to a set of variational
parameters ζθ = ( µθ,νθ) which parameterise the Gaussian qθ(θ |ζθ). We will see examples
of this approach in later chapters. There may also exist intractabilities in the hidden variable
58
VB Theory 2.3. Variational methods for Bayesian learning
q
x1
y1
x2
y2
x3
y3

(a) The generative graphical
model.
q
x1
y1
x2
y2
x3
y3

(b) Graph representing the
exact posterior.
q
x1 x2 x3
(c) Posterior graph after the
variational approximation.
Figure 2.4: Graphical depiction of the hidden-variable / parameter factorisation. (a) The origi-
nal generative model for n = 3 . (b) The exact posterior graph given the data. Note that for all
case pairs {i,j}, xi and xj are not directly coupled, but interact through θ. That is to say all
the hidden variables are conditionally independent of one another, but only given the parame-
ters. (c) the posterior graph after the variational approximation between parameters and hidden
variables, which removes arcs between parameters and hidden variables. Note that, on assum-
ing this factorisation, as a consequence of the i.i.d. assumption the hidden variables become
independent.
59
VB Theory 2.3. Variational methods for Bayesian learning
posterior, for which further approximations need be made (some examples are mentioned be-
low).
There is something of a dark art in discovering a factorisation amongst the hidden variables and
parameters such that the approximation remains faithful at an ‘acceptable’ level. Of course it
does not make sense to use a posterior form which holds fewer conditional independencies than
those implied by the moral graph (see section 1.1). The key to a good variational approximation
is then to remove as few arcs as possible from the moral graph such that inference becomes
tractable. In many cases the goal is to ﬁnd tractable substructures ( structured approximations)
such as trees or mixtures of trees, which capture as many of the arcs as possible. Some arcs
may capture crucial dependencies between nodes and so need be kept, whereas other arcs might
induce a weak local correlation at the expense of a long-range correlation which to ﬁrst order
can be ignored; removing such an arc can have dramatic effects on the tractability.
The advantage of the variational Bayesian procedure is that any factorisation of the posterior
yields a lower bound on the marginal likelihood. Thus in practice it may pay to approximately
evaluate the computational cost of several candidate factorisations, and implement those which
can return a completed optimisation of Fwithin a certain amount of computer time. One would
expect the more complex factorisations to take more computer time but also yield progressively
tighter lower bounds on average, the consequence being that the marginal likelihood estimate
improves over time. An interesting avenue of research in this vein would be to use the vari-
ational posterior resulting from a simpler factorisation as the initialisation for a slightly more
complicated factorisation, and move in a chain from simple to complicated factorisations to help
avoid local free energy minima in the optimisation. Having proposed this, it remains to be seen
if it is possible to form a coherent closely-spaced chain of distributions that are of any use, as
compared to starting from the fullest posterior approximation from the start.
Using the lower bound for model selection and averaging
The log ratio of posterior probabilities of two competing models mand m′ is given by
ln p(m|y)
p(m′ |y) = + ln p(m) + p(y |m) −ln p(m′) −ln p(y |m′) (2.74)
= + ln p(m) + F(qx,θ) + KL [ q(x,θ) ∥p(x,θ |y,m)]
−ln p(m′) −F′(q′
x,θ) −KL
[
q′(x,θ) ∥p(x,θ |y,m′)
]
(2.75)
where we have used the form in (2.72), which is exact regardless of the quality of the bound used,
or how tightly that bound has been optimised. The lower bounds for the two models, Fand F′,
are calculated from VBEM optimisations, providing us for each model with an approximation
to the posterior over the hidden variables and parameters of that model,qx,θ and q′
x,θ; these may
in general be functionally very different (we leave aside for the moment local maxima problems
60
VB Theory 2.3. Variational methods for Bayesian learning
in the optimisation process which can be overcome to an extent by using several differently
initialised optimisations or in some models by employing heuristics tailored to exploit the model
structure). When we perform model selection by comparing the lower bounds, Fand F′, we
are assuming that the KL divergences in the two approximations are the same, so that we can
use just these lower bounds as guide. Unfortunately it is non-trivial to predict how tight in
theory any particular bound can be — if this were possible we could more accurately estimate
the marginal likelihood from the start.
Taking an example, we would like to know whether the bound for a model with S mixture
components is similar to that for S + 1 components, and if not then how badly this inconsis-
tency affects the posterior over this set of models. Roughly speaking, let us assume that every
component in our model contributes a (constant) KL divergence penalty of KLs. For clarity
we use the notation L(S) and F(S) to denote the exact log marginal likelihood and lower
bounds, respectively, for a model with S components. The difference in log marginal likeli-
hoods, L(S+ 1) −L(S), is the quantity we wish to estimate, but if we base this on the lower
bounds the difference becomes
L(S+ 1) −L(S) = [ F(S+ 1) + ( S+ 1) KL s] −[F(S) + SKLs] (2.76)
= F(S+ 1) −F(S) + KL s (2.77)
̸= F(S+ 1) −F(S) , (2.78)
where the last line is the result we would have basing the difference on lower bounds. Therefore
there exists a systematic error when comparing models if each component contributes indepen-
dently to the KL divergence term. Since the KL divergence is strictly positive, and we are basing
our model selection on (2.78) rather than (2.77), this analysis suggests that there is a systematic
bias towards simpler models. We will in fact see this in chapter 4, where we ﬁnd an importance
sampling estimate of the KL divergence showing this behaviour.
Optimising the prior distributions
Usually the parameter priors are functions of hyperparameters, a, so we can write p(θ |a,m).
In the variational Bayesian framework the lower bound can be made higher by maximising Fm
with respect to these hyperparameters:
a(t+1) = arg max
a
Fm(qx(x),qθ(θ),y,a) . (2.79)
A simple depiction of this optimisation is given in ﬁgure 2.5. Unlike earlier in section 2.3.1,
the marginal likelihood of model m can now be increased with hyperparameter optimisation.
As we will see in later chapters, there are examples where these hyperparameters themselves
have governing hyperpriors, such that they can be integrated over as well. The result being that
61
VB Theory 2.3. Variational methods for Bayesian learning
log marginal likelihoodln p(y | a(t), m)
KL
h
q(t)
x q(t)
θ ∥ p(x, θ | y, a(t))
i
F(q(t)
x (x), q(t)
θ (θ), a(t))lower bound
VBEM step
log marginal likelihood ln p(y | a(t), m)
KL
h
q(t+ 1)
x q(t+ 1)
θ ∥ p(x, θ | y, a(t))
i
F(q(t+ 1)
x (x),
q(t+ 1)
θ (θ), a(t))
new lower bound
hyperparameter optimisation
new optimised
log marginal likelihood ln p(y | a(t+ 1), m)
KL
h
q(t+ 1)
x q(t+ 1)
θ ∥ p(x, θ | y, a(t+ 1))
i
F(q(t+ 1)
x (x), q(t+ 1)
θ (θ), a(t+ 1))
new lower bound
Figure 2.5: The variational Bayesian EM algorithm with hyperparameter optimisation. The
VBEM step consists of VBE and VBM steps, as shown in ﬁgure 2.3. The hyperparameter
optimisation increases the lower bound and also improves the marginal likelihood.
we can infer distributions over these as well, just as for parameters. The reason for abstracting
from the parameters this far is that we would like to integrate out all variables whose cardinality
increases with model complexity; this standpoint will be made clearer in the following chapters.
Previous work, and general applicability of VBEM
The variational approach for lower bounding the marginal likelihood (and similar quantities)
has been explored by several researchers in the past decade, and has received a lot of attention
recently in the machine learning community. It was ﬁrst proposed for one-hidden layer neural
networks (which have no hidden variables) by Hinton and van Camp (1993) where qθ(θ) was
restricted to be Gaussian with diagonal covariance. This work was later extended to show that
tractable approximations were also possible with a full covariance Gaussian (Barber and Bishop,
1998) (which in general will have the mode of the posterior at a different location than in the
diagonal case). Neal and Hinton (1998) presented a generalisation of EM which made use
of Jensen’s inequality to allow partial E-steps; in this paper the term ensemble learning was
used to describe the method since it ﬁts an ensemble of models, each with its own parameters.
Jaakkola (1997) and Jordan et al. (1999) review variational methods in a general context (i.e.
non-Bayesian). Variational Bayesian methods have been applied to various models with hidden
variables and no restrictions onqθ(θ) and qxi(xi) other than the assumption that they factorise in
some way (Waterhouse et al., 1996; Bishop, 1999; Ghahramani and Beal, 2000; Attias, 2000).
Of particular note is the variational Bayesian HMM of MacKay (1997), in which free-form
optimisations are explicitly undertaken (see chapter 3); this work was the inspiration for the
examination of Conjugate-Exponential (CE) models, discussed in the next section. An example
62
VB Theory 2.3. Variational methods for Bayesian learning
of a constrained optimisation for a logistic regression model can be found inJaakkola and Jordan
(2000).
Several researchers have investigated using mixture distributions for the approximate posterior,
which allows for more ﬂexibility whilst maintaining a degree of tractability ( Lawrence et al. ,
1998; Bishop et al. , 1998; Lawrence and Azzouzi , 1999). The lower bound in these models
is a sum of a two terms: a ﬁrst term which is a convex combination of bounds from each
mixture component, and a second term which is the mutual information between the mixture
labels and the hidden variables of the model. The ﬁrst term offers no improvement over a naive
combination of bounds, but the second (which is non-negative) has to improve on the simple
bounds. Unfortunately this term contains an expectation over all conﬁgurations of the hidden
states and so has to be itself bounded with a further use of Jensen’s inequality in the form of
a convex bound on the log function ( ln(x) ≤λx−ln(λ) −1) (Jaakkola and Jordan , 1998).
Despite this approximation drawback, empirical results in a handful of models have shown that
the approximation does improve the simple mean ﬁeld bound and improves monotonically with
the number of mixture components.
A related method for approximating the integrand for Bayesian learning is based on an idea
known as assumed density ﬁltering (ADF) (Bernardo and Giron, 1988; Stephens, 1997; Boyen
and Koller, 1998; Barber and Sollich , 2000; Frey et al. , 2001), and is called the Expectation
Propagation (EP) algorithm ( Minka, 2001a). This algorithm approximates the integrand of
interest with a set of terms, and through a process of repeated deletion-inclusion of term ex-
pressions, the integrand is iteratively reﬁned to resemble the true integrand as closely as pos-
sible. Therefore the key to the method is to use terms which can be tractably integrated. This
has the same ﬂavour as the variational Bayesian method described here, where we iteratively
update the approximate posterior over a hidden state qxi(xi) or over the parameters qθ(θ).
The key difference between EP and VB is that in the update process (i.e. deletion-inclusion)
EP seeks to minimise the KL divergence which averages according to the true distribution,
KL [p(x,θ |y) ∥q(x,θ)] (which is simply a moment-matching operation for exponential fam-
ily models), whereas VB seeks to minimise the KL divergence according to the approximate
distribution, KL [q(x,θ) ∥p(x,θ |y)]. Therefore, EP is at least attempting to average according
to the correct distribution, whereas VB has the wrong cost function at heart. However, in gen-
eral the KL divergence in EP can only be minimised separately one term at a time, while the KL
divergence in VB is minimised globally over all terms in the approximation. The result is that
EP may still not result in representative posterior distributions (for example, see Minka, 2001a,
ﬁgure 3.6, p. 6). Having said that, it may be that more generalised deletion-inclusion steps can
be derived for EP, for example removing two or more terms at a time from the integrand, and
this may alleviate some of the ‘local’ restrictions of the EP algorithm. As in VB, EP is con-
strained to use particular parametric families with a small number of moments for tractability.
An example of EP used with an assumed Dirichlet density for the term expressions can be found
in Minka and Lafferty (2002).
63
VB Theory 2.4. Conjugate-Exponential models
In the next section we take a closer look at the variational Bayesian EM equations, ( 2.54) and
(2.56), and ask the following questions:
- To which models can we apply VBEM? i.e. which forms of data distributions p(y,x |θ)
and priors p(θ |m) result in tractable VBEM updates?
- How does this relate formally to conventional EM?
- When can we utilise existing belief propagation algorithms in the VB framework?
2.4 Conjugate-Exponential models
2.4.1 Deﬁnition
We consider a particular class of graphical models with latent variables, which we callconjugate-
exponential (CE) models. In this section we explicitly apply the variational Bayesian method to
these parametric families, deriving a simple general form of VBEM for the class.
Conjugate-exponential models satisfy two conditions:
Condition (1). The complete-data likelihood is in the exponential family:
p(xi,yi|θ) = g(θ) f(xi,yi) eφ(θ)⊤u(xi,yi) , (2.80)
where φ(θ) is the vector of natural parameters, u and f are the functions that deﬁne the expo-
nential family, and gis a normalisation constant:
g(θ)−1 =
∫
dxidyi f(xi,yi) eφ(θ)⊤u(xi,yi) . (2.81)
The natural parameters for an exponential family model φ are those that interact linearly with
the sufﬁcient statistics of the data u. For example, for a univariate Gaussian in xwith mean µ
and standard deviation σ, the necessary quantities are obtained from:
p(x|µ,σ) = exp
{
−x2
2σ2 + xµ
σ2 − µ2
2σ2 −1
2 ln(2πσ2)
}
(2.82)
θ =
(
σ2,µ
)
(2.83)
64
VB Theory 2.4. Conjugate-Exponential models
and are:
φ(θ) =
(1
σ2 , µ
σ2
)
(2.84)
u(x) =
(
−x2
2 ,x
)
(2.85)
f(x) = 1 (2.86)
g(θ) = exp
{
−µ2
2σ2 −1
2 ln(2πσ2)
}
. (2.87)
Note that whilst the parameterisation for θ is arbitrary, e.g. we could have let θ = ( σ,µ), the
natural parameters φ are unique up to a multiplicative constant.
Condition (2). The parameter prior is conjugate to the complete-data likelihood:
p(θ |η,ν) = h(η,ν) g(θ)ηeφ(θ)⊤ν , (2.88)
where ηand ν are hyperparameters of the prior, andhis a normalisation constant:
h(η,ν)−1 =
∫
dθ g(θ)ηeφ(θ)⊤ν . (2.89)
Condition 1 ( 2.80) in fact usually implies the existence of a conjugate prior which satisﬁes
condition 2 (2.88). The prior p(θ |η,ν) is said to be conjugate to the likelihood p(xi,yi|θ) if
and only if the posterior
p(θ |η′,ν′) ∝p(θ |η,ν)p(x,y |θ) (2.90)
is of the same parametric form as the prior. In general the exponential families are the only
classes of distributions that have natural conjugate prior distributions because they are the only
distributions with a ﬁxed number of sufﬁcient statistics apart from some irregular cases (see
Gelman et al., 1995, p. 38). From the deﬁnition of conjugacy, we see that the hyperparameters
of a conjugate prior can be interpreted as the number (η) and values (ν) of pseudo-observations
under the corresponding likelihood.
We call models that satisfy conditions 1 (2.80) and 2 (2.88) conjugate-exponential.
The list of latent-variable models of practical interest with complete-data likelihoods in the ex-
ponential family is very long, for example: Gaussian mixtures, factor analysis, principal compo-
nents analysis, hidden Markov models and extensions, switching state-space models, discrete-
variable belief networks. Of course there are also many as yet undreamt-of models combining
Gaussian, gamma, Poisson, Dirichlet, Wishart, multinomial, and other distributions in the expo-
nential family.
65
VB Theory 2.4. Conjugate-Exponential models
However there are some notable outcasts which do not satisfy the conditions for membership
of the CE family, namely: Boltzmann machines ( Ackley et al. , 1985), logistic regression and
sigmoid belief networks ( Bishop, 1995), and independent components analysis (ICA) (as pre-
sented in Comon, 1994; Bell and Sejnowski, 1995), all of which are widely used in the machine
learning community. As an example let us see why logistic regression is not in the conjugate-
exponential family: for yi ∈{−1,1}, the likelihood under a logistic regression model is
p(yi|xi,θ) = eyiθ⊤xi
eθ⊤xi + e−θ⊤xi
, (2.91)
where xi is the regressor for data point iand θ is a vector of weights, potentially including a
bias. This can be rewritten as
p(yi|xi,θ) = eyiθ⊤xi−f(θ,xi) , (2.92)
where f(θ,xi) is a normalisation constant. To belong in the exponential family the normalising
constant must split into functions of onlyθ and only (xi,yi). Expanding f(θ,xi) yields a series
of powers of θ⊤xi, which could be assimilated into the φ(θ)⊤u(xi,yi) term by augmenting
the natural parameter and sufﬁcient statistics vectors, if it were not for the fact that the series is
inﬁnite meaning that there would need to be an inﬁnity of natural parameters. This means we
cannot represent the likelihood with a ﬁnite number of sufﬁcient statistics.
Models whose complete-data likelihood is not in the exponential family can often be approxi-
mated by models which are in the exponential family and have been given additional hidden
variables. A very good example is the Independent Factor Analysis (IFA) model of Attias
(1999a). In conventional ICA, one can think of the model as using non-Gaussian sources, or
using Gaussian sources passed through a non-linearity to make them non-Gaussian. For most
non-linearities commonly used (such as the logistic), the complete-data likelihood becomes
non-CE. Attias recasts the model as a mixture of Gaussian sources being fed into a linear mix-
ing matrix. This model is in the CE family and so can be tackled with the VB treatment. It is
an open area of research to investigate how best to bring models into the CE family, such that
inferences in the modiﬁed model resemble the original as closely as possible.
2.4.2 Variational Bayesian EM for CE models
In Bayesian inference we want to determine the posterior over parameters and hidden variables
p(x,θ |y,η, ν). In general this posterior is neither conjugate nor in the exponential family. In
this subsection we see how the properties of the CE family make it especially amenable to the
VB approximation, and derive the VBEM algorithm for CE models.
66
VB Theory 2.4. Conjugate-Exponential models
Theorem 2.2: Variational Bayesian EM for Conjugate-Exponential Models.
Given an i.i.d. data set y = {y1,... yn}, if the model satisﬁes conditions (1) and (2), then the
following (a), (b) and (c) hold:
(a) the VBE step yields:
qx(x) =
n∏
i=1
qxi(xi) , (2.93)
and qxi(xi) is in the exponential family:
qxi(xi) ∝f(xi,yi) eφ
⊤
u(xi,yi) = p(xi|yi,φ) , (2.94)
with a natural parameter vector
φ =
∫
dθ qθ(θ)φ(θ) ≡⟨φ(θ)⟩qθ(θ) (2.95)
obtained by taking the expectation of φ(θ) under qθ(θ) (denoted using angle-brackets
⟨·⟩). For invertible φ, deﬁning ˜θ such that φ(˜θ) = φ, we can rewrite the approximate
posterior as
qxi(xi) = p(xi|yi,˜θ) . (2.96)
(b) the VBM step yields that qθ(θ) is conjugate and of the form:
qθ(θ) = h(˜η,˜ν) g(θ)˜ηeφ(θ)⊤˜ν , (2.97)
where
˜η= η+ n, (2.98)
˜ν = ν +
n∑
i=1
u(yi) , (2.99)
and
u(yi) = ⟨u(xi,yi)⟩qxi(xi) (2.100)
is the expectation of the sufﬁcient statisticu. We have used⟨·⟩qxi(xi) to denote expectation
under the variational posterior over the latent variable(s) associated with the ith datum.
(c) parts (a) and (b) hold for every iteration of variational Bayesian EM.
Proof of (a): by direct substitution.
67
VB Theory 2.4. Conjugate-Exponential models
Starting from the variational extrema solution (2.60) for the VBE step:
qx(x) = 1
Zx
e⟨ln p(x,y | θ,m)⟩qθ(θ) , (2.101)
substitute the parametric form for p(xi,yi|θ,m) in condition 1 (2.80), which yields (omitting
iteration superscripts):
qx(x) = 1
Zx
e
Pn
i=1⟨ln g(θ)+ln f(xi,yi)+φ(θ)⊤u(xi,yi)⟩qθ(θ) (2.102)
= 1
Zx
[ n∏
i=1
f(xi,yi)
]
e
Pn
i=1 φ
⊤
u(xi,yi) , (2.103)
where Zx has absorbed constants independent of x, and we have deﬁned without loss of gener-
ality:
φ = ⟨φ(θ)⟩qθ(θ) . (2.104)
If φ is invertible, then there exists a ˜θ such that φ = φ(˜θ), and we can rewrite (2.103) as:
qx(x) = 1
Zx
[ n∏
i=1
f(xi,yi)eφ(˜θ)⊤u(xi,yi)
]
(2.105)
∝
n∏
i=1
p(xi,yi|˜θ,m) (2.106)
=
n∏
i=1
qxi(xi) (2.107)
= p(x,y |˜θ,m) . (2.108)
Thus the result of the approximate VBE step, which averages over the ensemble of models
qθ(θ), is exactly the same as an exact E step, calculated at the variational Bayes point estimate
˜θ.
Proof of (b): by direct substitution.
Starting from the variational extrema solution (2.56) for the VBM step:
qθ(θ) = 1
Zθ
p(θ |m) e⟨ln p(x,y | θ,m)⟩qx(x) , (2.109)
68
VB Theory 2.4. Conjugate-Exponential models
substitute the parametric forms for p(θ |m) and p(xi,yi|θ,m) as speciﬁed in conditions 2
(2.88) and 1 (2.80) respectively, which yields (omitting iteration superscripts):
qθ(θ) = 1
Zθ
h(η,ν)g(θ)ηeφ(θ)⊤ν e⟨
Pn
i=1 ln g(θ)+ln f(xi,yi)+φ(θ)⊤u(xi,yi)⟩qx(x) (2.110)
= 1
Zθ
h(η,ν)g(θ)η+neφ(θ)⊤[ν+Pn
i=1 u(yi)] e
Pn
i=1⟨ln f(xi,yi)⟩qx(x)
  
has no θ dependence
(2.111)
= h(˜η,˜ν)g(θ)˜ηeφ(θ)⊤˜ν , (2.112)
where
h(˜η,˜ν) = 1
Zθ
e
Pn
i=1⟨ln f(xi,yi)⟩qx(x) . (2.113)
Therefore the variational posterior qθ(θ) in (2.112) is of conjugate form, according to condition
2 (2.88).
Proof of (c): by induction.
Assume conditions 1 (2.80) and 2 (2.88) are met (i.e. the model is in the CE family). From part
(a), the VBE step produces a posterior distribution qx(x) in the exponential family, preserving
condition 1 (2.80); the parameter distribution qθ(θ) remains unaltered, preserving condition 2
(2.88). From part (b), the VBM step produces a parameter posterior qθ(θ) that is of conjugate
form, preserving condition 2 ( 2.88); qx(x) remains unaltered from the VBE step, preserving
condition 1 (2.80). Thus under both the VBE and VBM steps, conjugate-exponentiality is pre-
served, which makes the theorem applicable at every iteration of VBEM.
As before, since qθ(θ) and qxi(xi) are coupled, ( 2.97) and ( 2.94) do not provide an analytic
solution to the minimisation problem, so the optimisation problem is solved numerically by
iterating between the ﬁxed point equations given by these equations. To summarise brieﬂy:
VBE Step: Compute the expected sufﬁcient statistics {u(yi)}n
i=1 under the hidden vari-
able distributions qxi(xi), for all i.
VBM Step: Compute the expected natural parameters φ = ⟨φ(θ)⟩under the parameter
distribution given by ˜ηand ˜ν.
2.4.3 Implications
In order to really understand what the conjugate-exponential formalism buys us, let us reiterate
the main points of theorem 2.2 above. The ﬁrst result is that in the VBM step the analytical
form of the variational posterior qθ(θ) does not change during iterations of VBEM — e.g.
if the posterior is Gaussian at iteration t = 1 , then only a Gaussian need be represented at
future iterations. If it were able to change, which is the case in general (theorem 2.1), the
69
VB Theory 2.4. Conjugate-Exponential models
EM for MAP estimation Variational Bayesian EM
Goal: maximise p(θ |y,m) w.r.t. θ Goal: lower bound p(y |m)
E Step: compute VBE Step: compute
q(t+1)
x (x) = p(x |y,θ(t)) q(t+1)
x (x) = p(x |y,φ
(t)
)
M Step: VBM Step:
θ(t+1) = arg max θ
∫
dx q(t+1)
x (x) ln p(x,y,θ) q(t+1)
θ (θ) ∝exp
∫
dx q(t+1)
x (x) ln p(x,y,θ)
Table 2.1: Comparison of EM for ML/MAP estimation against variational Bayesian EM for CE
models.
posterior could quickly become unmanageable, and (further) approximations would be required
to prevent the algorithm becoming too complicated. The second result is that the posterior over
hidden variables calculated in the VBE step is exactly the posterior that would be calculated had
we been performing an ML/MAP E step. That is, the inferences using an ensemble of models
qθ(θ) can be represented by the effect of a point parameter, ˜θ. The task of performing many
inferences, each of which corresponds to a different parameter setting, can be replaced with a
single inference step — it is possible to infer the hidden states in a conjugate exponential model
tractably while integrating over an ensemble of model parameters.
Comparison to EM for ML/MAP parameter estimation
We can draw a tight parallel between the EM algorithm for ML/MAP estimation, and our VBEM
algorithm applied speciﬁcally to conjugate-exponential models. These are summarised in table
2.1. This general result of VBEM for CE models was reported in Ghahramani and Beal (2001),
and generalises the well known EM algorithm for ML estimation ( Dempster et al. , 1977). It
is a special case of the variational Bayesian algorithm (theorem 2.1) used in Ghahramani and
Beal (2000) and in Attias (2000), yet encompasses many of the models that have been so far
subjected to the variational treatment. Its particular usefulness is as a guide for the design of
models, to make them amenable to efﬁcient approximate Bayesian inference.
The VBE step has about the same time complexity as the E step, and is in all ways identical
except that it is re-written in terms of the expected natural parameters. In particular, we can
make use of all relevant propagation algorithms such as junction tree, Kalman smoothing, or
belief propagation. The VBM step computes a distribution over parameters (in the conjugate
family) rather than a point estimate. Both ML/MAP EM and VBEM algorithms monotonically
increase an objective function, but the latter also incorporates a model complexity penalty by
70
VB Theory 2.4. Conjugate-Exponential models
integrating over parameters so embodying an Occam’s razor effect. Several examples will be
presented in the following chapters of this thesis.
Natural parameter inversions
Unfortunately, even though the algorithmic complexity is the same, the implementations may
be hampered since the propagation algorithms need to be re-derived in terms of the natural
parameters (this is essentially the difference between the forms in (2.94) and (2.96)). For some
models, such as HMMs (see chapter3, and MacKay, 1997), this is very straightforward, whereas
the LDS model (see chapter5) quickly becomes quite involved. Automated algorithm derivation
programs are currently being written to alleviate this complication, speciﬁcally for the case
of variational Bayesian EM operations ( Bishop et al. , 2003), and also for generic algorithm
derivation (Buntine, 2002; Gray et al., 2003); both these projects build on results inGhahramani
and Beal (2001).
The difﬁculty is quite subtle and lies in the natural parameter inversion problem, which we now
brieﬂy explain. In theorem 2.2 we conjectured the existence of a ˜θ such that φ = ⟨φ(θ)⟩qθ(θ)
?=
φ(˜θ), which was a point of convenience. But, the operation φ−1
[
⟨φ⟩qθ(θ)
]
may not be well
deﬁned if the dimensionality ofφ is greater than that ofθ. Whilst not undermining the theorem’s
result, this does mean that representationally speaking the resulting algorithm may look different
having had to be cast in terms of the natural parameters.
Online and continuous variants
The VBEM algorithm for CE models very readily lends itself to online learning scenarios in
which data arrives incrementally. I brieﬂy present here an online version of the VBEM algorithm
above (but see also Ghahramani and Attias, 2000; Sato, 2001). In the standard VBM step (2.97)
the variational posterior hyperparameter ˜η is updated according to the size of the dataset n
(2.98), and ˜ν is updated with a simple sum of contributions from each datum u(yi), (2.99).
For the online scenario, we can take the posterior over parameters described by ˜η and ˜ν to be
the prior for subsequent inferences. Let the data be split in to batches indexed byk, each of size
n(k), which are presented one by one to the model. Thus if the kth batch of data consists of the
71
VB Theory 2.4. Conjugate-Exponential models
n(k) i.i.d. points {yi}j(k)+n(k)−1
i=j(k) , then the online VBM step replaces equations (2.98) and (2.99)
with
˜η= η(k−1) + n(k) , (2.114)
˜ν = ν(k−1) +
j(k)+n(k)−1∑
i=j(k)
u(yi) . (2.115)
In the online VBE step only the hidden variables {xi}j(k)+n(k)−1
i=j(k) need be inferred to calculate
the required u statistics. The online VBM and VBE steps are then iterated until convergence,
which may be fast if the size of the batchn(k) is small compared to the amount of data previously
seen ∑k−1
k′=1 n(k′). After convergence, the prior for the next batch is set to the current posterior,
according to
η(k) ←˜η, (2.116)
ν(k) ←˜ν . (2.117)
The online VBEM algorithm has several beneﬁts. First and foremost, the update equations give
us a very transparent picture of how the algorithm incorporates evidence from a new batch of
data (or single data point). The way in which it does this makes it possible to discard data from
earlier batches: the hyperparameters ˜η and ˜ν represent all information gathered from previ-
ous batches, and the process of incorporating new information is not a function of the previous
batches’ statistics {u(yi)}j(k−1)+n(k−1)−1
i=j(1) , nor previous hyperparameter settings{η(l),ν(l)}k−2
l=1 ,
nor the previous batch sizes {n(l)}k−1
l=1 , nor the previous data {yi}j(k−1)+n(k−1)−1
i=j(1) . Implemen-
tationally this offers a large memory saving. Since we hold a distribution over the parameters
of the model, which is updated in a consistent way using Bayesian inference, we should hope
that the online model makes a ﬂexible and measured response to data as it arrives. However it
has been observed (personal communication, Z. Ghahramani) that serious underﬁtting occurs in
this type of online algorithm; this is due to excessive self-pruning of the parameters by the VB
algorithm.
From the VBM step (2.97) we can straightforwardly propose an annealing variant of the VBEM
algorithm. This would make use of an inverse temperature parameter β ∈[0,1] and adopt the
following updates for the VBM step:
˜η= η+ βn, (2.118)
˜ν = ν + β
n∑
i=1
u(yi) , (2.119)
which is similar to the online algorithm but “introduces” the data continuously with a schedule
of β from 0 →1. Whilst this is a tempting avenue for research, it is not clear that in this
72
VB Theory 2.5. Directed and undirected graphs
setting we should expect any better results than if we were to present the algorithm with all
the data (i.e. β = 1 ) from the start — after all, the procedure of Bayesian inference should
produce the same inferences whether presented with the data incrementally, continuously or all
at once. The advantage of an annealed model, however, is that we are giving the algorithm a
better chance of escaping the local minima in the free energy that plague EM-type algorithms,
so that the Bayesian inference procedure can be given a better chance of reaching the proper
conclusions, whilst at every iteration receiving information (albeit β-muted) about all the data
at every iteration.
2.5 Directed and undirected graphs
In this section we present several important results which build on theorems 2.1 and 2.2 by
specifying the form of the joint density p(x,y,θ). A convenient way to do this is to use the
formalism and expressive power of graphical models. We derive variational Bayesian learn-
ing algorithms for two important classes of these models: directed graphs (Bayesian networks)
and undirected graphs (Markov networks), and also give results pertaining to CE families for
these classes. The corollaries refer to propagation algorithms material which is covered in
section 1.1.2; for a tutorial on belief networks and Markov networks the reader is referred to
Pearl (1988). In the theorems and corollaries, VBEM and CE are abbreviations for variational
Bayesian Expectation-Maximisation and conjugate-exponential.
2.5.1 Implications for directed networks
Corollary 2.1: (theorem 2.1) VBEM for Directed Graphs (Bayesian Networks).
Let mbe a model with parametersθ and hidden and visible variablesz = {zi}n
i=1 = {xi,yi}n
i=1
that satisfy a belief network factorisation. That is, each variable zij has parents zipa(j) such
that the complete-data joint density can be written as a product of conditional distributions,
p(z |θ) =
∏
i
∏
j
p(zij |zipa(j),θ) . (2.120)
Then the approximating joint distribution formsatisﬁes the same belief network factorisation:
qz(z) =
∏
i
qzi(zi) , q zi(zi) =
∏
j
qj(zij |zipa(j)) , (2.121)
where
qj(zij |zipa(j)) = 1
Zqj
e⟨ln p(zij | zipa(j),θ)⟩qθ(θ) ∀{i,j} (2.122)
73
VB Theory 2.5. Directed and undirected graphs
are new conditional distributions obtained by averaging over qθ(θ), and Zqj are normalising
constants.
This corollary is interesting in that it states that a Bayesian network’s posterior distribution
can be factored into the same terms as the original belief network factorisation ( 2.120). This
means that the inference for a particular variable depends only on those other variables in its
Markov blanket; this result is trivial for the point parameter case, but deﬁnitely non-trivial in the
Bayesian framework in which all the parameters and hidden variables are potentially coupled.
Corollary 2.2: (theorem 2.2) VBEM for CE Directed Graphs (CE Bayesian Networks).
Furthermore, if m is a conjugate-exponential model, then the conditional distributions of the
approximate posterior joint have exactly the same form as those in the complete-data likelihood
in the original model:
qj(zij |zipa(j)) = p(zij |zipa(j),˜θ) , (2.123)
but with natural parameters φ(˜θ) = φ. Moreover, with the modiﬁed parameters ˜θ, the ex-
pectations under the approximating posterior qx(x) ∝qz(z) required for the VBE step can be
obtained by applying the belief propagation algorithm if the network is singly connected and
the junction tree algorithm if the network is multiply-connected.
This result generalises the derivation of variational learning for HMMs (MacKay, 1997), which
uses the forward-backward algorithm as a subroutine. We investigate the variational Bayesian
HMM in more detail in chapter 3. Another example is dynamic trees (Williams and Adams ,
1999; Storkey, 2000; Adams et al. , 2000) in which belief propagation is executed on a single
tree which represents an ensemble of singly-connected structures. Again there exists the natural
parameter inversion issue, but this is merely an implementational inconvenience.
2.5.2 Implications for undirected networks
Corollary 2.3: (theorem 2.1) VBEM for Undirected Graphs (Markov Networks).
Let mbe a model with hidden and visible variables z = {zi}n
i=1 = {xi,yi}n
i=1 that satisfy a
Markov network factorisation. That is, the joint density can be written as a product of clique-
potentials {ψj}J
j=1,
p(z |θ) = 1
Z
∏
i
∏
j
ψj(Cj(zi),θ) , (2.124)
where each cliqueCj is a (ﬁxed) subset of the variables inzi, such that{C1(zi)∪···∪CJ(zi)}=
zi. Then the approximating joint distribution for msatisﬁes the same Markov network factori-
sation:
qz(z) =
∏
i
qzi(zi) , q zi(zi) = 1
Zq
∏
j
ψj(Cj(zi)) , (2.125)
74
VB Theory 2.6. Comparisons of VB to other criteria
where
ψj(Cj(zi)) = e⟨ln ψj(Cj(zi),θ)⟩qθ(θ) ∀{i,j} (2.126)
are new clique potentialsobtained by averaging overqθ(θ), and Zq is a normalisation constant.
Corollary 2.4: (theorem 2.2) VBEM for CE Undirected Graphs (CE Markov Networks).
Furthermore, if mis a conjugate-exponential model, then the approximating clique potentials
have exactly the same form as those in the original model:
ψj(Cj(zi)) ∝ψj(Cj(zi),˜θ) , (2.127)
but with natural parameters φ(˜θ) = φ. Moreover, the expectations under the approximating
posterior qx(x) ∝qz(z) required for the VBE Step can be obtained by applying the junction
tree algorithm.
For conjugate-exponential models in which belief propagation and the junction tree algorithm
over hidden variables are intractable, further applications of Jensen’s inequality can yield tractable
factorisations (Jaakkola, 1997; Jordan et al., 1999).
2.6 Comparisons of VB to other criteria
2.6.1 BIC is recovered from VB in the limit of large data
We show here informally how the Bayesian Information Criterion (BIC, see section 1.3.4) is
recovered in the large data limit of the variational Bayesian lower bound (Attias, 1999b). Fcan
be written as a sum of two terms:
Fm(qx(x),qθ(θ)) = −KL [qθ(θ) ∥p(θ |m)]
  
Fm,pen
+
⟨
ln p(x,y |θ,m)
qx(x)
⟩
qx(x) qθ(θ)  
Dm
. (2.128)
Let us consider separately the limiting forms of these two terms, constraining ourselves to the
cases in which the model mis in the CE family. In such cases, theorem 2.2 states that qθ(θ) is
of conjugate form (2.97) with parameters given by (2.98) and (2.99). It can be shown that under
mild conditions exponential family distributions of this form exhibit asymptotic normality (see,
for example, the proof given in Bernardo and Smith, 1994, pp. 293–4). Therefore, the entropy
75
VB Theory 2.6. Comparisons of VB to other criteria
of qθ(θ) appearing in Fm,pen can be calculated assuming a Gaussian form (see appendix A),
and the limit becomes
lim
n→∞
Fm,pen = lim
n→∞
[
⟨ln p(θ |m)⟩qθ(θ) + d
2 ln 2π−1
2 ln |H|
]
(2.129)
= −d
2 ln n+ O(1) , (2.130)
where H is the Hessian (matrix of second derivatives of the parameter posterior evaluated at
the mode), and we have used similar arguments to those taken in the derivation of BIC (section
1.3.4). The second term, Dm, can be analysed by appealing to the fact that the term inside the
expectation is equal to ln p(y |θ,m) if and only if qx(x) = p(x |y,θ,m). Theorem 2.1 states
that the form of the variational posterior over hidden states qx(x) is given by
ln qx(x) =
∫
dθ qθ(θ) ln p(x,y |θ,m) −ln Zx (2.131)
(which does not depend on CE family membership conditions). Therefore as qθ(θ) becomes
concentrated about θMAP, this results in qx(x) = p(x |y,θMAP,m). Then Dm asymptotically
becomes ln p(y |θMAP,m). Combining this with the limiting form for Fm,pen given by (2.130)
results in:
lim
n→∞
Fm(qx(x),qθ(θ)) = −d
2 ln n+ ln p(y |θMAP,m) + O(1) , (2.132)
which is the BIC approximation given by ( 1.49). For the case of a non-CE model, we would
have to prove asymptotic normality for qθ(θ) outside of the exponential family, which may
become complicated or indeed impossible. We note that this derivation of the limiting form of
VB is heuristic in the sense that we have neglected concerns on precise regularity conditions
and identiﬁability.
2.6.2 Comparison to Cheeseman-Stutz (CS) approximation
In this section we present results regarding the approximation of Cheeseman and Stutz (1996),
covered in section1.3.5. We brieﬂy review the CS criterion, as used to approximate the marginal
likelihood of ﬁnite mixture models, and then show that it is in fact a strict lower bound on the
marginal likelihood. We conclude the section by presenting a construction that proves that VB
can be used to obtain a bound that is always tighter than CS.
Let m be a directed acyclic graph with parameters θ giving rise to an i.i.d. data set denoted
by y = {y1,..., yn}with corresponding discrete hidden variables s = {s1,..., sn}each of
cardinality k. Let ˆθ be a result of an EM algorithm which has converged to a local maximum
in the likelihood p(y |θ), and let ˆs = {ˆsi}n
i=1 be a completion of the hidden variables, chosen
76
VB Theory 2.6. Comparisons of VB to other criteria
according to the posterior distribution over hidden variables given the data and ˆθ, such that
ˆsij = p(sij = j|y,ˆθ) ∀i= 1 ,...,n .
Since we are completing the hidden variables with real, as opposed to discrete values, this
complete data set does not in general correspond to a realisable data set under the generative
model. This point raises the question of how its marginal probability p(ˆs,y |m) is deﬁned. We
will see in the following theorem and proof (theorem 2.3) that both the completion required of
the hidden variables and the completed data marginal probability are well-deﬁned, and follow
from equations 2.141 and 2.142 below.
The CS approximation is given by
p(y |m) ≈p(y |m)CS = p(ˆs,y |m) p(y |ˆθ)
p(ˆs,y |ˆθ)
. (2.133)
The CS approximation exploits the fact that, for many models of interest, the ﬁrst term on the
right-hand side, the complete-data marginal likelihood, is tractable to compute (this is the case
for discrete-variable directed acyclic graphs with Dirichlet priors, see chapter 6 for details).
The term in the numerator of the second term on the right-hand side is simply the likelihood
of the data, which is an output of the EM algorithm (as is the parameter estimate ˆθ), and the
denominator is a straightforward calculation that involves no summations over hidden variables
or integrations over parameters.
Theorem 2.3: Cheeseman-Stutz approximation is a lower bound on the marginal likeli-
hood.
Let ˆθ be the result of the M step of EM, and let{p(si|yi,ˆθ)}n
i=1 be the set of posterior distribu-
tions over the hidden variables obtained in the next E step of EM. Furthermore, letˆs = {ˆsi}n
i=1
be a completion of the hidden variables, such that ˆsij = p(sij = j|y,ˆθ) ∀i= 1 ,...,n . Then
the CS approximation is a lower bound on the marginal likelihood:
p(y |m)CS = p(ˆs,y |m) p(y |ˆθ)
p(ˆs,y |ˆθ)
≤p(y |m) . (2.134)
This observation should be attributed to Minka (2001b), where it was noted that (in the context
of mixture models with unknown mixing proportions and component parameters) whilst the CS
approximation has been reported to obtain good performance in the literature ( Cheeseman and
Stutz, 1996; Chickering and Heckerman, 1997), it was not known to be a bound on the marginal
likelihood. Here we provide a proof of this statement that is generally applicable to any model.
77
VB Theory 2.6. Comparisons of VB to other criteria
Proof of theorem 2.3: via marginal likelihood bounds using approximations over the posterior
distribution of only the hidden variables. The marginal likelihood can be lower bounded by
introducing a distribution over the settings of each data point’s hidden variablesqsi(si):
p(y |m) =
∫
dθ p(θ)
n∏
i=1
p(yi|θ) (2.135)
≥
∫
dθ p(θ)
n∏
i=1
exp
{∑
si
qsi(si) ln p(si,yi|θ)
qsi(si)
}
. (2.136)
We return to this quantity shortly, but presently place a similar lower bound over the likelihood
of the data:
p(y |ˆθ) =
n∏
i=1
p(yi|ˆθ) ≥
n∏
i=1
exp
{∑
si
qsi(si) ln p(si,yi|ˆθ)
qsi(si)
}
(2.137)
which can be made an equality if, for each data point, q(si) is set to the exact posterior distri-
bution given the parameter setting θ (for example see equation ( 2.19) and the proof following
it),
p(y |ˆθ) =
n∏
i=1
p(yi|ˆθ) =
n∏
i=1
exp
{∑
si
ˆqsi(si) ln p(si,yi|ˆθ)
ˆqsi(si)
}
, (2.138)
where
ˆqsi(si) ≡p(si|y,ˆθ) , (2.139)
which is the result obtained from an exact E step with the parameters set to ˆθ. Now rewrite the
marginal likelihood bound (2.136), using this same choice of ˆqsi(si), separate those terms that
depend on θ from those that do not, and substitute in the form from equation (2.138) to obtain:
p(y |m) ≥
n∏
i=1
exp
{∑
si
ˆqsi(si) ln 1
ˆqsi(si)
}
·
∫
dθ p(θ)
n∏
i=1
exp
{∑
si
ˆqsi(si) ln p(si,yi|θ)
}
(2.140)
= p(y |ˆθ)
∏n
i=1 exp
{∑
si ˆqsi(si) ln p(si,yi|ˆθ)
}
∫
dθ p(θ)
n∏
i=1
exp
{∑
si
ˆqsi(si) ln p(si,yi|θ)
}
(2.141)
= p(y |ˆθ)∏n
i=1 p(ˆsi,yi|ˆθ)
∫
dθ p(θ)
n∏
i=1
p(ˆsi,yi|θ) , (2.142)
78
VB Theory 2.6. Comparisons of VB to other criteria
where ˆsi are deﬁned such that they satisfy:
ˆsi deﬁned such that: ln p(ˆsi,y |ˆθ) =
∑
si
ˆqsi(si) ln p(si,yi|θ) (2.143)
=
∑
si
p(si|y,ˆθ) ln p(si,yi|θ) (2.144)
where the second line comes from the requirement of bound equality in ( 2.139). The existence
of such a completion follows from the fact that, in discrete-variable directed acyclic graphs
of the sort considered in Chickering and Heckerman (1997), the hidden variables appear only
linearly in logarithm of the joint probability p(s,y |θ). Equation ( 2.142) is the Cheeseman-
Stutz criterion, and is also a lower bound on the marginal likelihood.
It is possible to derive CS-like approximations for types of graphical model other than discrete-
variables DAGs. In the above proof no constraints were placed on the forms of the joint distribu-
tions over hidden and observed variables, other than in the simplifying step in equation (2.142).
So, similar results to corollaries 2.2 and 2.4 can be derived straightforwardly to extend theorem
2.3 to incorporate CE models.
The following corollary shows that variational Bayes can always obtain a tighter bound than the
Cheeseman-Stutz approximation.
Corollary 2.5: (theorem 2.3) VB is at least as tight as CS.
That is to say, it is always possible to ﬁnd distributions qs(s) and qθ(θ) such that
ln p(y |m)CS ≤Fm(qs(s),qθ(θ)) ≤ln p(y |m) . (2.145)
Proof of corollary 2.5. Consider the following forms for qs(s) and qθ(θ):
qs(s) =
n∏
i=1
qsi(si) , with qsi(si) = p(si|yi,ˆθ) , (2.146)
qθ(θ) ∝⟨ln p(θ)p(s,y |θ)⟩qs(s) . (2.147)
We write the form for qθ(θ) explicitly:
qθ(θ) = p(θ) ∏n
i=1 exp
{∑
si qsi(si) ln p(si,yi|θ)
}
∫
dθ′ p(θ′) ∏n
i=1 exp
{∑
si qsi(si) ln p(si,yi|θ′)
}, (2.148)
79
VB Theory 2.7. Summary
and note that this is exactly the result of a VBM step. We substitute this and the form for qs(s)
directly into the VB lower bound stated in equation (2.53) of theorem 2.1, obtaining:
F(qs(s),qθ(θ)) =
∫
dθ qθ(θ)
n∑
i=1
∑
si
qsi(si) ln p(si,yi|θ)
qsi(si) +
∫
dθ qθ(θ) ln p(θ)
qθ(θ)
(2.149)
=
∫
dθ qθ(θ)
n∑
i=1
∑
si
qsi(si) ln 1
qsi(si)
+
∫
dθ qθ(θ) ln
∫
dθ′ p(θ′)
n∏
i=1
exp
{∑
si
qsi(si) ln p(si,yi|θ′)
}
(2.150)
=
n∑
i=1
∑
si
qsi(si) ln 1
qsi(si) + ln
∫
dθ p(θ)
n∏
i=1
exp
{∑
si
qsi(si) ln p(si,yi|θ)
}
,
(2.151)
which is exactly the logarithm of equation (2.140). And so with this choice of qθ(θ) and qs(s)
we achieve equality between the CS and VB approximations in (2.145).
We complete the proof of corollary2.5 by noting that any further VB optimisation is guaranteed
to increase or leave unchanged the lower bound, and hence surpass the CS lower bound. We
would expect the VB lower bound starting from the CS solution to improve upon the CS lower
bound in all cases, except in the very special case when the MAP parameter ˆθ is exactly the
variational Bayes point , deﬁned as θBP ≡φ−1(⟨φ(θ)⟩qθ(θ)) (see proof of theorem 2.2(a)).
Therefore, since VB is a lower bound on the marginal likelihood, the entire statement of (2.145)
is proven.
2.7 Summary
In this chapter we have shown how a variational bound can be used to derive the EM algorithm
for ML/MAP parameter estimation, for both unconstrained and constrained representations of
the hidden variable posterior. We then moved to the Bayesian framework, and presented the
variational Bayesian EM algorithm which iteratively optimises a lower bound on the marginal
likelihood of the model. The marginal likelihood, which integrates over model parameters, is
the key component to Bayesian model selection. The VBE and VBM steps are obtained by
taking functional derivatives with respect to variational distributions over hidden variables and
parameters respectively.
We gained a deeper understanding of the VBEM algorithm by examining the speciﬁc case of
conjugate-exponential models and showed that, for this large class of models, the posterior dis-
tributions qx(x) and qθ(θ) have intuitive and analytically stable forms. We have also presented
80
VB Theory 2.7. Summary
VB learning algorithms for both directed and undirected graphs (Bayesian networks and Markov
networks).
We have explored the Cheeseman-Stutz model selection criterion as a lower bound of the
marginal likelihood of the data, and have explained how it is a very speciﬁc case of varia-
tional Bayes. Moreover, using this intuition, we have shown that any CS approximation can be
improved upon by building a VB approximation over it. It is tempting to derive conjugate-
exponential versions of the CS criterion, but in my opinion this is not necessary since any
implementations based on these results can be made only more accurate by using conjugate-
exponential VB instead, which is at least as general in every case. In chapter 6 we present a
comprehensive comparison of VB to a variety of approximation methods, including CS, for a
model selection task involving discrete-variable DAGs.
The rest of this thesis applies the VB lower bound to several commonly used statistical models,
with a view to performing model selection, learning from both real and synthetic data sets.
Throughout we compare the variational Bayesian framework to competitor approximations,
such as those reviewed in section 1.3, and also critically analyse the quality of the lower bound
using advanced sampling methods.
81
Chapter 3
Variational Bayesian Hidden Markov
Models
3.1 Introduction
Hidden Markov models (HMMs) are widely used in a variety of ﬁelds for modelling time se-
ries data, with applications including speech recognition, natural language processing, protein
sequence modelling and genetic alignment, general data compression, information retrieval,
motion video analysis and object/people tracking, and ﬁnancial time series prediction. The core
theory of HMMs was developed principally by Baum and colleagues ( Baum and Petrie, 1966;
Baum et al., 1970), with initial applications to elementary speech processing, integrating with
linguistic models, and making use of insertion and deletion states for variable length sequences
(Bahl and Jelinek, 1975). The popularity of HMMs soared the following decade, giving rise to
a variety of elaborations, reviewed in Juang and Rabiner (1991). More recently, the realisation
that HMMs can be expressed as Bayesian networks (Smyth et al., 1997) has given rise to more
complex and interesting models, for example, factorial HMMs (Ghahramani and Jordan, 1997),
tree-structured HMMs (Jordan et al., 1997), and switching state-space models (Ghahramani and
Hinton, 2000). An introduction to HMM modelling in terms of graphical models can be found
in Ghahramani (2001).
This chapter is arranged as follows. In section 3.2 we brieﬂy review the learning and infer-
ence algorithms for the standard HMM, including ML and MAP estimation. In section 3.3 we
show how an exact Bayesian treatment of HMMs is intractable, and then in section 3.4 follow
MacKay (1997) and derive an approximation to a Bayesian implementation using a variational
lower bound on the marginal likelihood of the observations. In section3.5 we present the results
of synthetic experiments in which VB is shown to avoid overﬁtting unlike ML. We also com-
pare ML, MAP and VB algorithms’ ability to learn HMMs on a simple benchmark problem of
82
VB Hidden Markov Models 3.2. Inference and learning for maximum likelihood HMMs
s1
y1 y2 y3 yT
s2 sTs3 ...A
C
Figure 3.1: Graphical model representation of a hidden Markov model. The hidden variables st
transition with probabilities speciﬁed in the rows ofA, and at each time step emit an observation
symbol yt according to the probabilities in the rows of C.
discriminating between forwards and backwards English sentences. We present conclusions in
section 3.6.
Whilst this chapter is not intended to be a novel contribution in terms of the variational Bayesian
HMM, which was originally derived in the unpublished technical report of MacKay (1997), it
has nevertheless been included for completeness to provide an immediate and straightforward
example of the theory presented in chapter2. Moreover, the wide applicability of HMMs makes
the derivations and experiments in this chapter of potential general interest.
3.2 Inference and learning for maximum likelihood HMMs
We brieﬂy review the learning and inference procedures for hidden Markov models (HMMs),
adopting a similar notation to Rabiner and Juang (1986). An HMM models a sequence of p-
valued discrete observations (symbols) y1:T = {y1,...,y T}by assuming that the observation
at time t, yt, was produced by a k-valued discrete hidden state st, and that the sequence of
hidden states s1:T = {s1,...,s T}was generated by a ﬁrst-order Markov process. That is to say
the complete-data likelihood of a sequence of length T is given by:
p(s1:T,y1:T) = p(s1)p(y1 |s1)
T∏
t=2
p(st|st−1)p(yt|st) . (3.1)
where p(s1) is the prior probability of the ﬁrst hidden state, p(st|st−1) denotes the probability
of transitioning from state st−1 to state st (out of a possible kstates), and p(yt|st) are the emis-
sion probabilities for each of psymbols at each state. In this simple HMM, all the parameters
are assumed stationary, and we assume a ﬁxed ﬁnite number of hidden states and number of
observation symbols. The joint probability ( 3.1) is depicted as a graphical model in ﬁgure 3.1.
For simplicity we ﬁrst examine just a single sequence of observations, and derive learning and
inference procedures for this case; it is straightforward to extend the results to multiple i.i.d.
sequences.
83
VB Hidden Markov Models 3.2. Inference and learning for maximum likelihood HMMs
The probability of the observations y1:T results from summing over all possible hidden state
sequences,
p(y1:T) =
∑
s1:T
p(s1:T,y1:T) . (3.2)
The set of parameters for the initial state prior, transition, and emission probabilities are repre-
sented by the parameter θ:
θ = ( A,C, π) (3.3)
A= {ajj′}: ajj′ = p(st = j′ |st−1 = j) state transition matrix (k ×k) (3.4)
C = {cjm}: cjm = p(yt = m|st = j) symbol emission matrix (k ×p) (3.5)
π = {πj}: πj = p(s1 = j) initial hidden state prior (k ×1) (3.6)
obeying the normalisation constraints:
A= {ajj′}:
k∑
j′=1
ajj′ = 1 ∀j (3.7)
C = {cjm}:
p∑
m=1
cjm = 1 ∀j (3.8)
π = {πj}:
k∑
j=1
πj = 1 . (3.9)
For mathematical convenience we represent the state of the hidden variables usingk-dimensional
binary column vectors. For example, if st is in state j, then st is a vector of zeros with ‘1’ in the
jth entry. We use a similar notation for the observations yt. The Kronecker- δfunction is used
to query the state, such that st,j = δ(st,j) returns 1 if st is in state j, and zero otherwise.
Using the vectorial form of the hidden and observed variables, the initial hidden state, transition,
and emission probabilities can be written as
p(s1 |π) =
k∏
j=1
πs1,j
j (3.10)
p(st|st−1,A) =
k∏
j=1
k∏
j′=1
a
st,j′st−1,j
jj′ (3.11)
p(yt|st,C) =
k∏
j=1
p∏
m=1
cst,jyt,m
jm (3.12)
84
VB Hidden Markov Models 3.2. Inference and learning for maximum likelihood HMMs
and the log complete-data likelihood from (3.1) becomes:
ln p(s1:T,y1:T |θ) =
k∑
j=1
s1,j ln πj +
T∑
t=2
k∑
j=1
k∑
j′=1
st−1,j ln ajj′st,j′
+
T∑
t=1
k∑
j=1
p∑
m=1
st,j ln cjmyt,m (3.13)
= s⊤
1 ln π +
T∑
t=2
s⊤
t−1 ln Ast +
T∑
t=1
s⊤
t ln C yt , (3.14)
where the logarithms of the vectorπ and matrices Aand Care taken element-wise. We are now
in a position to derive the EM algorithm for ML parameter estimation for HMMs.
M step
Learning the maximum likelihood parameters of the model entails ﬁnding those settings of A,
C and π which maximise the probability of the observed data ( 3.2). In chapter 2 we showed
that the M step, as given by equation (2.31), is
M step: θ(t+1) ←arg max
θ
∑
s1:T
p(s1:T |y1:T,θ(t)) ln p(s1:T,y1:T |θ) , (3.15)
where the superscript notation (t) denotes iteration number. Note in particular that the log likeli-
hood in equation (3.14) is a sum of separate contributions involving π, Aand C, and summing
over the hidden state sequences does not couple the parameters. Therefore we can individually
optimise each parameter of the HMM:
π : πj ←⟨s1,j⟩ (3.16)
A : ajj′ ←
∑T
t=2⟨st−1,jst,j′⟩
∑T
t=2⟨st−1,j⟩
(3.17)
C : cjm ←
∑T
t=1⟨st,jyt,m⟩∑T
t=1⟨st,j⟩
(3.18)
where the angled brackets ⟨·⟩denote expectation with respect to the posterior distribution over
the hidden state sequence, p(s1:T |y1:T,θ(t)), as calculated from the E step.
E step: forward-backward algorithm
The E step is carried out using a dynamic programming trick which utilises the conditional
independence of future hidden states from past hidden states given the setting of the current
85
VB Hidden Markov Models 3.2. Inference and learning for maximum likelihood HMMs
hidden state. We deﬁne αt(st) to be the posterior over the hidden state st given the observed
sequence up to and including time t:
αt(st) ≡p(st|y1:t) , (3.19)
and form the forward recursion from t= 1 ,...,T :
αt(st) = 1
p(yt|y1:t−1)
∑
st−1
p(st−1 |y1:t−1)p(st|st−1)p(yt|st) (3.20)
= 1
ζt(yt)

∑
st−1
αt−1(st−1)p(st|st−1)

p(yt|st) , (3.21)
where in the ﬁrst time step p(st|st−1) is replaced with the prior p(s1 |π), and for t = 1 we
require the convention α0(s0) = 1 . Here, ζt(yt) is a normalisation constant, a function of yt,
given by
ζt(yt) ≡p(yt|y1:t−1) . (3.22)
Note that as a by-product of computing these normalisation constants we can compute the prob-
ability of the sequence:
p(y1:T) = p(y1)p(y2 |y1) ...p (yT |y1:T−1) =
T∏
t=1
p(yt|y1:t−1) =
T∏
t=1
ζt(yt) = Z(y1:T) .
(3.23)
Obtaining these normalisation constants using a forward pass is simply equivalent to integrating
out the hidden states one after the other in the forward ordering, as can be seen by writing the
incomplete-data likelihood in the following way:
p(y1:T) =
∑
s1:T
p(s1:T,y1:T) (3.24)
=
∑
s1
···
∑
sT
p(s1)p(y1 |s1)
T∏
t=2
p(st|st−1)p(yt|st) (3.25)
=
∑
s1
p(s1)p(y1 |s1) ···
∑
sT
p(sT |sT−1)p(yT |sT) . (3.26)
Similarly to the forward recursion, the backward recursion is carried out from t= T,..., 1:
βt(st) ≡p(y(t+1):T |st) (3.27)
=
∑
st+1
p(yt+2:T |st+1)p(st+1 |st)p(yt+1 |st+1) (3.28)
=
∑
st+1
βt+1(st+1)p(st+1 |st)p(yt+1 |st+1) , (3.29)
with the end condition βT(sT) = 1 , as there is no future observed data beyond t= T.
86
VB Hidden Markov Models 3.2. Inference and learning for maximum likelihood HMMs
The forward and backward recursions can be executed in parallel as neither depends on the
results of the other. The quantities {αt}T
t=1 and {βt}T
t=1 are now combined to obtain the single
and pairwise state marginals:
p(st|y1:T) ∝p(st|y1:t)p(yt+1:T |st) (3.30)
= αt(st)βt(st) , t = 1 ,...,T (3.31)
and
p(st−1,st|y1:T) ∝p(st−1 |y1:t−1)p(st|st−1)p(yt|st)p(yt+1:T |st) (3.32)
= αt−1(st−1)p(st|st−1)p(yt|st)βt(st) , t = 2 ,...,T (3.33)
which give the expectations required for the M steps (3.16-3.18),
⟨st,j⟩= αt,jβt,j
∑k
j′=1 αt,j′βt,j′
(3.34)
⟨st−1,jst,j′⟩= αt−1,jajj′p(yt|st,j′)βt,j′
∑k
j=1
∑k
j′=1 αt−1,jajj′p(yt|st,j′)βt,j′
. (3.35)
The E and M steps described above form the iterations for the celebrated Baum-Welch algorithm
(Baum et al., 1970). From the analysis in chapter 2, we can prove that each iteration of EM is
guaranteed to increase, or leave unchanged, the log likelihood of the parameters, and converge
to a local maximum.
When learning an HMM from multiple i.i.d. sequences {yi,1:Ti}n
i=1 which are not necessarily
constrained to have the same lengths {Ti}n
i=1, the E and M steps remain largely the same.
The E step is performed for each sequence separately using the forward-backward algorithm,
and the M step then uses statistics pooled from all the sequences to estimate the mostly likely
parameters.
HMMs as described above can be generalised in many ways. Often observed data are recorded
as real-valued sequences and can be modelled by replacing the emission processp(yt|st) with a
Gaussian or mixture of Gaussians distribution: each sequence of the HMM can now be thought
of as deﬁning a sequence of data drawn from a mixture model whose hidden state labels for the
mixture components are no longer i.i.d., but evolve with Markov dynamics. Note that inference
in such models remains possible using the forward and backward recursions, with only a change
to the emission probabilities p(yt|st); furthermore, the M steps for learning the parameters π
and Afor the hidden state transitions remain identical.
Exactly analogous inference algorithms exist for the Linear Dynamical Systems (LDS) model,
except that both the hidden state transition and emission processes are continuous (referred to
87
VB Hidden Markov Models 3.3. Bayesian HMMs
as dynamics and output processes, respectively). In the rest of this chapter we will see how a
variational Bayesian treatment of HMMs results in a straightforwardly modiﬁed Baum-Welch
algorithm, and as such it is a useful pedagogical example of the VB theorems given in chapter
2. On the other hand, for the LDS models the modiﬁed VB algorithms become substantially
harder to derive and implement — these are the subject of chapter 5.
3.3 Bayesian HMMs
As has already been discussed in chapters1 and 2, the maximum likelihood approach to learning
models from data does not take into account model complexity, and so is susceptible to over-
ﬁtting the data. More complex models can usually give ever-increasing likelihoods to the data.
For a hidden Markov model, the complexity is related to several aspects: the number of hidden
states kin the model, the degree of connectivity in the hidden state transition matrix A, and the
distribution of probabilities to the symbols by each hidden state, as speciﬁed in the emission
matrix, C. More generally the complexity is related to the richness of possible data sets that the
model can produce. There are k(k−1) parameters in the transition matrix, and k(p−1) in the
emission matrix, and so if there are many different observed symbols or if we expect to require
more than a few hidden states then, aside from inference becoming very costly, the number of
parameters to be ﬁt may begin to overwhelm the amount of data available. Traditionally, in
order to avoid overﬁtting, researchers have limited the complexity of their models in line with
the amount of data they have available, and have also used sophisticated modiﬁcations to the
basic HMM to reduce the number of free parameters. Such modiﬁcations include: parameter-
tying, enforcing sparsity constraints (for example limiting the number of candidates a state can
transition to or symbols it can emit), or constraining the form of the hidden state transitions (for
example employing a strict left-to-right ordering of the hidden states).
A common technique for removing excessive parameters from a model is to regularise them
using a prior, and then to maximise the a posteriori probability of the parameters (MAP). We will
see below that it is possible to apply this type of regularisation to the multinomial parameters of
the transition and emission probabilities using certain Dirichlet priors. However we would still
expect the results of MAP optimisation to be susceptible to overﬁtting given that it searches for
the maximum of the posterior density as opposed to integrating over the posterior distribution.
Cross-validation is another method often employed to minimise the amount of overﬁtting, by
repeatedly training subsets of the available data and evaluating the error on the remaining data.
Whilst cross-validation is quite successful in practice, it has the drawback that it requires many
sessions of training and so is computationally expensive, and often needs large amounts of data
to obtain low-variance estimates of the expected test errors. Moreover, it is cumbersome to
cross-validate over the many different ways in which model complexity could vary.
88
VB Hidden Markov Models 3.3. Bayesian HMMs
The Bayesian approach to learning treats the model parameters as unknown quantities and,
prior to observing the data, assigns a set of beliefs over these quantities in the form of prior
distributions. In the light of data, Bayes’ rule can be used to infer the posterior distribution over
the parameters. In this way the parameters of the model are treated as hidden variables and are
integrated out to form the marginal likelihood:
p(y1:T) =
∫
dθ p(θ)p(y1:T |θ) where θ = ( π,A,C ) . (3.36)
This Bayesian integration embodies the principle of Occam’s razor since it automatically pe-
nalises those models with more parameters (see section 1.2.1; also see MacKay, 1992). A
natural choice for parameter priors over π, the rows of A, and the rows of C are Dirichlet dis-
tributions. Whilst there are many possible choices, Dirichlet distributions have the advantage
that they are conjugate to the complete-data likelihood terms given in equations (3.1) (and with
foresight we know that these forms will yield tractable variational Bayesian algorithms):
p(θ) = p(π)p(A)p(C) (3.37)
p(π) = Dir( {π1,...,π k}|u(π))) (3.38)
p(A) =
k∏
j=1
Dir({aj1,...,a jk}|u(A)) (3.39)
p(C) =
k∏
j=1
Dir({cj1,...,c jp}|u(C)) . (3.40)
Here, for each matrix the same single hyperparameter vector is used for every row. This hyper-
parameter sharing can be motivated because the hidden states are identical a priori. The form of
the Dirichlet prior, using p(π) as an example, is
p(π) = Γ(u(π)
0 )
∏k
j=1 Γ(u(π)
j )
k∏
j=1
π
u(π)
j −1
j , u (π)
j >0, ∀j, (3.41)
where u(π)
0 = ∑k
j=1 u(π)
j is the strength of the prior, and the positivity constraint on the hyperpa-
rameters is required for the prior to be proper. Conjugate priors have the intuitive interpretation
of providing hypothetical observations to augment those provided by the data (see section1.2.2).
If these priors are used in a maximum a posteriori (MAP) estimation algorithm for HMMs, the
priors add imaginary counts to the M steps. Taking the update for Aas an example, equation
(3.17) is modiﬁed to
A : ajj′ ←
(u(A)
j′ −1) + ∑T
t=2⟨st−1,jst,j′⟩
∑k
j′=1(u(A)
j′ −1) + ∑T
t=2⟨st−1,j⟩
. (3.42)
Researchers tend to limit themselves to hyperparameters uj ≥1 such that this MAP estimate is
guaranteed to yield positive probabilities. However there are compelling reasons for having hy-
89
VB Hidden Markov Models 3.3. Bayesian HMMs
perparameters uj ≤1 (as discussed in MacKay and Peto, 1995; MacKay, 1998), and these arise
naturally as described below. It should be emphasised that the MAP solution is not invariant to
reparameterisations, and so ( 3.42) is just one possible result. For example, reparameterisation
into the softmax basis yields a MAP estimate without the ‘-1’ terms, which also coincides with
the predictive distribution obtained from integrating over the posterior. The experiments carried
out in this chapter for MAP learning do so in this basis.
We choose to use symmetric Dirichlet priors, with a ﬁxed strength f, i.e.
u(A) =
[
f(A)
k ,..., f(A)
k
]⊤
, s.t.
k∑
j=1
u(A)
j = f(A) , (3.43)
and similarly so for u(C) and u(π). A ﬁxed strength is chosen because we do not want the
amount of imaginary data to increase with the complexity of the model. This relates to a key is-
sue in Bayesian prior speciﬁcation regarding the scaling of model priors. Imagine an un-scaled
prior over each row of Awith hyperparameter
[
f(A),...,f (A)]⊤
, where the division by khas
been omitted. With a ﬁxed strength prior, the contribution to the posterior distributions over the
parameters from the prior diminishes with increasing data, whereas with the un-scaled prior the
contribution increases linearly with the number of hidden states and can become greater than
the amount of observed data for sufﬁciently large k. This means that for sufﬁciently complex
models the modiﬁcation terms in (3.42) would obfuscate the data entirely. This is clearly unde-
sirable, and so the 1
k scaling of the hyperparameter entries is used. Note that this scaling will
result in hyperparameters ≤1 for sufﬁciently large k.
The marginal probability of a sequence of observations is given by
p(y1:T) =
∫
dπ p(π)
∫
dAp(A)
∫
dC p(C)
∑
s1:T
p(s1:T,y1:T |π,A,C ) , (3.44)
where the dependence on the hyperparameters is implicitly assumed as they are ﬁxed before-
hand. Unfortunately, we can no longer use the dynamic programming trick of the forward-
backward algorithm, as the hidden states are now coupled by the integration over the parameters.
Intuitively this means that, because the parameters of the model have become uncertain quan-
tities, the future hidden states s(t+1):T are no longer independent of past hidden states s1:(t−1)
given the current state st. The summation and integration operations in ( 3.44) can be inter-
changed, but there are still an intractable number of possible sequences to sum over, a number
exponential in the length of the sequence. This intractability becomes even worse with multiple
sequences, as hidden states of different sequences also become dependent in the posterior.
It is true that for any given setting of the parameters, the likelihood calculation is possible,
as is ﬁnding the distribution over possible hidden state sequences using the forward-backward
algorithm; but since the parameters are continuous this insight is not useful for calculating
90
VB Hidden Markov Models 3.4. Variational Bayesian formulation
(3.44). It is also true that for any given trajectory representing a single hidden state sequence,
we can treat the hidden variables as observed and analytically integrate out the parameters to
obtain the marginal likelihood; but since the number of such trajectories is exponential in the
sequence length (kT), this approach is also ruled out.
These considerations form the basis of a very simple and elegant algorithm due to Stolcke and
Omohundro (1993) for estimating the marginal likelihood of an HMM. In that work, the pos-
terior distribution over hidden state trajectories is approximated with the most likely sequence,
obtained using a Viterbi algorithm for discrete HMMs (Viterbi, 1967). This single sequence (let
us assume it is unique) is then treated as observed data, which causes the parameter posteriors
to be Dirichlet, which are then easily integrated over to form an estimate of the marginal likeli-
hood. The MAP parameter setting (the mode of the Dirichlet posterior) is then used to infer the
most probable hidden state trajectory to iterate the process. Whilst the reported results are im-
pressive, substituting MAP estimates for both parameters and hidden states seems safe only if:
there is plenty of data to determine the parameters (i.e. many long sequences); and the individual
sequences are long enough to reduce any ambiguity amongst the hidden state trajectories.
Markov chain Monte Carlo (MCMC) methods can be used to approximate the posterior distri-
bution over parameters (Robert et al., 1993), but in general it is hard to assess the convergence
and reliability of the estimates required for learning. An analytically-based approach is to ap-
proximate the posterior distribution over the parameters with a Gaussian, which usually allows
the integral to become tractable. Unfortunately the Laplace approximation is not well-suited to
bounded or constrained parameters (e.g. sum-to-one constraints), and computation of the likeli-
hood Hessian can be computationally expensive. In MacKay (1998) an argument for transform-
ing the Dirichlet prior into the softmax basis is presented, although to the best of our knowledge
this approach is not widely used for HMMs.
3.4 Variational Bayesian formulation
In this section we derive the variational Bayesian implementation of HMMs, ﬁrst presented in
MacKay (1997). We show that by making only the approximation that the posterior over hid-
den variables and parameters factorises, an approximate posterior distribution over hidden state
trajectories can be inferred under an ensemble of model parameters, and how an approximate
posterior distribution over parameters can be analytically obtained from the sufﬁcient statistics
of the hidden state.
91
VB Hidden Markov Models 3.4. Variational Bayesian formulation
3.4.1 Derivation of the VBEM optimisation procedure
Our choice of priors p(θ) and the complete-data likelihood p(s1:T,y1:T |θ) for HMMs satisfy
conditions (2.80) and ( 2.88) respectively, for membership of the conjugate-exponential (CE)
family. Therefore it is possible to apply the results of theorem 2.2 directly to obtain the VBM
and VBE steps. The derivation is given here step by step, and the ideas of chapter 2 brought in
gradually. We begin with the log marginal likelihood for an HMM ( 3.36), and lower bound it
by introducing any distribution over the parameters and hidden variables q(π,A,C, s1:T):
ln p(y1:T) = ln
∫
dπ
∫
dA
∫
dC
∑
s1:T
p(π,A,C )p(y1:T,s1:T |π,A,C ) (3.45)
≥
∫
dπ
∫
dA
∫
dC
∑
s1:T
q(π,A,C, s1:T) ln p(π,A,C )p(y1:T,s1:T |π,A,C )
q(π,A,C, s1:T) .
(3.46)
This inequality is tight when q(π,A,C, s1:T) is set to the exact posterior over hidden variables
and parameters p(π,A,C, s1:T |y1:T), but it is intractable to compute this distribution. We
make progress by assuming that the posterior is factorised:
p(π,A,C, s1:T |y1:T) ≈q(π,A,C )q(s1:T) (3.47)
which gives a lower bound of the form
ln p(y1:T) ≥
∫
dπ
∫
dA
∫
dC
∑
s1:T
q(π,A,C, s1:T) ln p(π,A,C )p(y1:T,s1:T |π,A,C )
q(π,A,C, s1:T)
(3.48)
=
∫
dπ
∫
dA
∫
dC q(π,A,C )
[
ln p(π,A,C )
q(π,A,C )
+
∑
s1:T
q(s1:T) ln p(y1:T,s1:T |π,A,C )
q(s1:T)
]
(3.49)
= F(q(π,A,C ),q(s1:T)) , (3.50)
where the dependence on y1:T is taken to be implicit. On taking functional derivatives of F
with respect to q(π,A,C ) we obtain
ln q(π,A,C ) = ln p(π,A,C )⟨ln p(y1:T,s1:T |π,A,C )⟩q(s1:T ) + c (3.51)
= ln p(π) + ln p(A) + ln p(C)
+ ⟨ln p(s1 |π)⟩q(s1) + ⟨ln p(s2:T |s1,A)⟩q(s1:T )
+ ⟨ln p(y1:T |s1:T,C)⟩q(s1:T ) + c, (3.52)
92
VB Hidden Markov Models 3.4. Variational Bayesian formulation
where cis a normalisation constant. Given that the prior over the parameters ( 3.37) factorises,
and the log complete-data likelihood (3.14) is a sum of terms involving each ofπ, A, and C, the
variational posterior over the parameters can be factorised without further approximation into:
q(π,A,C ) = q(π)q(A)q(C) . (3.53)
Note that sometimes this independence is assumed beforehand and believed to concede accu-
racy, whereas we have seen that it falls out from a free-form extremisation of the posterior with
respect to the entire variational posterior over the parameters q(π,A,C ), and is therefore exact
once the assumption of factorisation between hidden variables and parameters has been made.
The VBM step
The VBM step is obtained by taking functional derivatives of Fwith respect to each of these
distributions and equating them to zero, to yield Dirichlet distributions:
q(π) = Dir( {π1,...,π k}|{w(π)
1 ,...,w (π)
k }) (3.54)
with w(π)
j = u(π)
j + ⟨δ(s1,j)⟩q(s1:T ) (3.55)
q(A) =
k∏
j=1
Dir({aj1,...,a jk}|{w(A)
j1 ,...,w (A)
jk }) (3.56)
with w(A)
jj′ = u(A)
j′ +
T∑
t=2
⟨δ(st−1,j)δ(st,j′)⟩q(s1:T ) (3.57)
q(C) =
k∏
j=1
Dir({cj1,...,c jp}|{w(C)
j1 ,...,w (C)
jp }) (3.58)
with w(C)
jq = u(A)
q +
T∑
t=1
⟨δ(st,j)δ(yt,q)⟩q(s1:T ) . (3.59)
These are straightforward applications of the result in theorem 2.2(b), which states that the
variational posterior distributions have the same form as the priors with their hyperparameters
augmented by sufﬁcient statistics of the hidden state and observations.
The VBE step
Taking derivatives of F(3.49) with respect to the variational posterior over the hidden state
q(s1:T) yields:
ln q(s1:T) = ⟨ln p(s1:T,y1:T |π,A,C )⟩q(π)q(A)q(C) −ln ˜Z(y1:T) , (3.60)
93
VB Hidden Markov Models 3.4. Variational Bayesian formulation
where ˜Z(y1:T) is an important normalisation constant that we will return to shortly. Substituting
in the complete-data likelihood from (3.14) yields
ln q(s1:T) =
⣨
s⊤
1 ln π +
T∑
t=2
s⊤
t−1 ln Ast +
T∑
t=1
s⊤
t ln C yt
⟩
q(π)q(A)q(C)
−ln ˜Z(y1:T) (3.61)
= s⊤
1 ⟨ln π⟩q(π) +
T∑
t=2
s⊤
t−1⟨ln A⟩q(A) st +
T∑
t=1
s⊤
t ⟨ln C⟩q(C) yt −ln ˜Z(y1:T) .
(3.62)
Note that (3.62) appears identical to the complete-data likelihood of (3.14) except that expecta-
tions are now taken of the logarithm of the parameters. Relating this to the result in corollary
2.2, the natural parameter vector φ(θ) is given by
θ = ( π ,A,C ) (3.63)
φ(θ) = (ln π ,ln A, ln C) , (3.64)
and the expected natural parameter vector φ is given by
φ ≡⟨φ(θ)⟩q(θ) = ( ⟨ln π⟩q(π), ⟨ln A⟩q(A), ⟨ln C⟩q(C)) . (3.65)
Corollary 2.2 suggests that we can use a modiﬁed parameter, ˜θ, in the same inference algo-
rithm (forward-backward) in the VBE step. The modiﬁed parameter ˜θ satisﬁes φ = φ(˜θ) =
⟨φ(θ)⟩q(θ), and is obtained simply by using the inverse of the φ operator:
˜θ = φ−1(⟨φ(θ)⟩q(θ)) = (exp ⟨ln π⟩q(π) ,exp⟨ln A⟩q(A) ,exp⟨ln C⟩q(C)) (3.66)
= ( ˜π , ˜A, ˜C) . (3.67)
Note that the natural parameter mapping φ operates separately on each of the parameters in the
vector θ, which makes the inversion of the mappingφ−1 straightforward. This is a consequence
of these parameters being uncoupled in the complete-data likelihood. For other CE models,
the inversion of the natural parameter mapping may not be as simple, since having uncoupled
parameters is not necessarily a condition for CE family membership. In fact, in chapter 5 we
encounter such a scenario for Linear Dynamical Systems.
It remains for us to calculate the expectations of the logarithm of the parameters under the
Dirichlet distributions. We use the result that
∫
dπ Dir(π |u) ln πj = ψ(uj) −ψ(
k∑
j=1
uj) , (3.68)
94
VB Hidden Markov Models 3.4. Variational Bayesian formulation
where ψis the digamma function (see appendices A and C.1 for details). This yields
˜π = {˜πj}= exp

ψ(w(π)
j ) −ψ(
k∑
j=1
w(π)
j )

 :
k∑
j=1
˜πj ≤1 (3.69)
˜A= {˜ajj′}= exp

ψ(w(A)
jj′ ) −ψ(
k∑
j′=1
w(A)
jj′ )

 :
k∑
j′=1
˜ajj′ ≤1 ∀j (3.70)
˜C = {˜cjm}= exp
[
ψ(w(C)
jm ) −ψ(
p∑
m=1
w(C)
jm )
]
:
p∑
m=1
˜cjm ≤1 ∀j. (3.71)
Note that taking geometric averages has resulted in sub-normalised probabilities. We may still
use the forward-backward algorithm with these sub-normalised parameters, but should bear in
mind that the normalisation constants (scaling factors) change. The forward pass (3.21) becomes
αt(st) = 1
˜ζt(yt)

∑
st−1
αt−1(st−1)˜p(st|st−1)

˜p(yt|st) , (3.72)
where ˜p(st|st−1) and ˜p(yt|st) are new subnormalised probability distributions according to
the parameters ˜A, ˜C, respectively. Since αt(st) is the posterior probability of st given data y1:t,
it must sum to one. This implies that, for anyparticular time step, the normalisation ˜ζt(yt) must
be smaller than if we had used normalised parameters. Similarly the backward pass becomes
βt(st) =
∑
st+1
βt+1(st+1)˜p(st+1 |st)˜p(yt+1 |st+1) . (3.73)
Computation of the lower bound F
Recall from (3.22) that the product of the normalisation constants corresponds to the probability
of the sequence. Here the product of normalisation constants corresponds to a different quantity:
T∏
t=1
˜ζt(yt) = ˜Z(y1:T) (3.74)
which is the normalisation constant given in (3.60). Thus the modiﬁed forward-backward algo-
rithm recursively computes the normalisation constant by integrating out each st in q(s1:T), as
opposed to p(s1:T |y1:T). We now show how ˜Z(y1:T) is useful for computing the lower bound,
just as Z(y1:T) was useful for computing the likelihood in the ML system.
95
VB Hidden Markov Models 3.4. Variational Bayesian formulation
Using (3.49) the lower bound can be written as
F(q(π,A,C ),q(s1:T)) =
∫
dπ q(π) ln p(π)
q(π) +
∫
dAq(A) ln p(A)
q(A) +
∫
dC q(C) ln p(C)
q(C)
+ H(q(s1:T))
+ ⟨ln p(s1:T,y1:T |π,A,C )⟩q(π)q(A)q(C)q(s1:T ) , (3.75)
where H(q(s1:T)) is the entropy of the variational posterior distribution over hidden state se-
quences. Straight after a VBE step, the form of the hidden state posterior q(s1:T) is given by
(3.60), and the entropy can be written:
H(q(s1:T)) = −
∑
s1:T
q(s1:T) ln q(s1:T) (3.76)
= −
∑
s1:T
q(s1:T)
[
⟨ln p(s1:T,y1:T |π,A,C )⟩q(π)q(A)q(C) −ln ˜Z(y1:T)
]
(3.77)
= −
∑
s1:T
q(s1:T)⟨ln p(s1:T,y1:T |π,A,C )⟩q(π)q(A)q(C) + ln ˜Z(y1:T) . (3.78)
Substituting this into (3.75) cancels the expected log complete-data likelihood terms, giving
F(q(π,A,C ),q(s1:T)) =
∫
dπ q(π) ln p(π)
q(π) +
∫
dAq(A) ln p(A)
q(A) +
∫
dC q(C) ln p(C)
q(C)
+ ln ˜Z(y1:T) (3.79)
Therefore computing Ffor variational Bayesian HMMs consists of evaluating KL divergences
between variational posterior and prior Dirichlet distributions for each row of π, A, C (see
appendix A), and collecting the modiﬁed normalisation constants {˜ζt(yt)}T
t=1. In essence we
have by-passed the difﬁculty of trying to compute the entropy of the hidden state by recursively
computing it with the VBE step’s forward pass. Note that this calculation is then only valid
straight after the VBE step.
VB learning with multiple i.i.d. sequences is conceptually straightforward and very similar to
that described above for ML learning. For the sake of brevity the reader is referred to the chapter
on Linear Dynamical Systems, speciﬁcally section 5.3.8 and equation (5.152), from which the
implementational details for variational Bayesian HMMs can readily be inferred.
Optimising the hyperparameters of the model is straightforward. Since the hyperparameters
appear in Fonly in the KL divergence terms, maximising the marginal likelihood amounts to
minimising the KL divergence between each parameter’s variational posterior distribution and
its prior distribution. We did not optimise the hyperparameters in the experiments, but instead
examined several different settings.
96
VB Hidden Markov Models 3.4. Variational Bayesian formulation
3.4.2 Predictive probability of the VB model
In the Bayesian scheme, the predictive probability of a test sequence y′ = y′
1:T′, given a set
of training cases denoted by y = {yi,1:Ti}n
i=1, is obtained by averaging the predictions of the
HMM with respect to the posterior distributions over its parameters θ = {π,A,C }:
p(y′ |y) =
∫
dθ p(θ |y)p(y′ |θ) . (3.80)
Unfortunately, for the very same reasons that the marginal likelihood of equation ( 3.44) is in-
tractable, so is the predictive probability. There are several possible methods for approximating
the predictive probability. One such method is to sample parameters from the posterior distri-
bution and construct a Monte Carlo estimate. Should it not be possible to sample directly from
the posterior, then importance sampling or its variants can be used. This process can be made
more efﬁcient by employing Markov chain Monte Carlo and related methods. Alternatively, the
posterior distribution can be approximated with some form which when combined with the like-
lihood term becomes amenable to integration analytically; it is unclear which analytical forms
might yield good approximations.
An alternative is to approximate the posterior distribution with the variational posterior distri-
bution resulting from the VB optimisation:
p(y′ |y) ≈
∫
dθ q(θ)p(y′ |θ) . (3.81)
The variational posterior is a product of Dirichlet distributions, which is in the same form as
the prior, and so we have not gained a great deal because we know this integral is intractable.
However we can perform two lower bounds on this quantity to obtain:
p(y′ |y) ≈
∫
dθ q(θ)p(y′ |θ) (3.82)
≥exp
∫
dθ q(θ) ln
∑
s′
1:T′
p(s′
1:T,y′
1:T |θ) (3.83)
≥exp
∫
dθ q(θ)
∑
s′
1:T′
q(s′
1:T′) ln p(s′
1:T,y′
1:T |θ)
q(s′
1:T′) . (3.84)
Equation 3.84 is just the last term in the expression for the lower bound of the marginal likeli-
hood of a training sequence given by ( 3.49), but with the test sequence in place of the training
sequence. This insight provides us with the following method to evaluate the approximation.
One simply carries out a VBE step on the test sequence, starting from the result of the last VBM
step on the training set, and gathers the normalisation constants {˜Z′
t}T′
i
t=1 and takes the product
of these. Whilst this is a very straightforward method, it should be remembered that it is only a
bound on an approximation.
97
VB Hidden Markov Models 3.5. Experiments
A different way to obtain the predictive probability is to assume that the model at the mean (or
mode) of the variational posterior, with parameter θMVB, is representative of the distribution as
a whole. The likelihood of the test sequence is then computed under the single model with those
parameters, which is tractable:
p(y′ |y)MVB =
∑
s′
1:T
p(s′
1:T,y′
1:T |θMVB) . (3.85)
This approach is suggested as further work in MacKay (1997), and is discussed in the experi-
ments described below.
3.5 Experiments
In this section we perform two experiments, the ﬁrst on synthetic data to demonstrate the ability
of the variational Bayesian algorithm to avoid overﬁtting, and the second on a toy data set to
compare ML, MAP and VB algorithm performance at discriminating between forwards and
backwards English character sequences.
3.5.1 Synthetic: discovering model structure
For this experiment we trained ML and VB hidden Markov models on examples of three types
of sequences with a three-symbol alphabet{a,b,c }. Using standard regular expression notation,
the ﬁrst type of sequence was a substring of the regular grammar (abc)∗, the second a substring
of (acb)∗, and the third from (a∗b∗)∗ where a and b symbols are emitted stochastically with
probability 1
2 each. For example, the training sequences included the following:
y1,1:T1 = ( abcabcabcabcabcabcabcabcabcabcabcabc)
y2,1:T2 = ( bcabcabcabcabcabcabcabcabcabcabcabc)
...
y12,1:T12 = ( acbacbacbacbacbacbacbacb)
y13,1:T13 = ( acbacbacbacbacbacbacbacbacbacbacbacbac)
...
yn−1,1:Tn−1 = ( baabaabbabaaaabbabaaabbaabbbaa)
yn,1:Tn = ( abaaabbababaababbbbbaaabaaabba) .
In all, the training data consisted of 21 sequences of maximum length 39 symbols. Looking at
these sequences, we would expect an HMM to require 3 hidden states to model (abc)∗, a dif-
98
VB Hidden Markov Models 3.5. Experiments
ferent 3 hidden states to model (acb)∗, and a single self-transitioning hidden state stochastically
emitting a and b symbols to model (a∗b∗)∗. This gives a total of 7 hidden states required to
model the data perfectly. With this foresight we therefore chose HMMs with k = 12 hidden
states to allow for some redundancy and room for overﬁtting.
The parameters were initialised by drawing the components of the probability vectors from a
uniform distribution and normalising. First the ML algorithm was run to convergence, and
then the VB algorithm run from that point in parameter space to convergence. This was made
possible by initialising each parameter’s variational posterior distribution to be Dirichlet with
the ML parameter as mean and a strength arbitrarily set to 10. For the MAP and VB algorithms,
the prior over each parameter was a symmetric Dirichlet distribution of strength 4.
Figure 3.2 shows the proﬁle of the likelihood of the data under the ML algorithm and the subse-
quent proﬁle of the lower bound on the marginal likelihood under the VB algorithm. Note that
it takes ML about 200 iterations to converge to a local optimum, and from this point it takes
only roughly 25 iterations for the VB optimisation to converge — we might expect this as VB
is initialised with the ML parameters, and so has less work to do.
Figure 3.3 shows the recovered ML parameters and VB distributions over parameters for this
problem. As explained above, we require 7 hidden states to model the data perfectly. It is
clear from ﬁgure 3.3(a) that the ML model has used more hidden states than needed, that is
to say it has overﬁt the structure of the model. Figures 3.3(b) and 3.3(c) show that the VB
optimisation has removed excess transition and emission processes and, on close inspection, has
recovered exactly the model that was postulated above. For example: state (4) self-transitions,
and emits the symbols a and b in approximately equal proportions to generate the sequences
(a∗b∗)∗; states (9,10,8) form a strong repeating path in the hidden state space which (almost)
deterministically produce the sequences (acb)∗; and lastly the states (3,12,2) similarly interact
to produce the sequences (abc)∗. A consequence of the Bayesian scheme is that all the entries
of the transition and emission matrices are necessarily non-zero, and those states (1,5,6,7,11)
that are not involved in the dynamics have uniform probability of transitioning to all others, and
indeed of generating any symbol, in agreement with the symmetric prior. However these states
have small probability of being used at all, as both the distribution q(π) over the initial state
parameter π is strongly peaked around high probabilities for the remaining states, and they have
very low probability of being transitioned into by the active states.
3.5.2 Forwards-backwards English discrimination
In this experiment, models learnt by ML, MAP and VB are compared on their ability to dis-
criminate between forwards and backwards English text (this toy experiment is suggested in
MacKay, 1997). A sentence is classiﬁed according to the predictive log probability under each
99
VB Hidden Markov Models 3.5. Experiments
0 50 100 150 200 250−650
−600
−550
−500
−450
−400
−350
−300
−250
−200
−150
(a) ML: plot of the log likelihood of the data,
p(y1:T | θ).
300 305 310 315 320 325−800
−750
−700
−650
−600
−550
−500
−450
−400
−350
(b) VB: plot of the lower bound
F(q(s1:T ), q(θ)).
0 50 100 150 200 250 30010
−8
10
−6
10
−4
10
−2
10
0
10
2
(c) ML: plot of the derivative of the log likeli-
hood in (a).
300 305 310 315 320 32510
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
10
4
(d) VB: plot of the derivative of the lower
bound in (b).
Figure 3.2: Training ML and VB hidden Markov models on synthetic sequences drawn from
(abc)∗, (acb)∗ and (a∗b∗)∗ grammars (see text). Subplots (a) & (c) show the evolution of the
likelihood of the data in the maximum likelihood EM learning algorithm for the HMM with
k= 12 hidden states. As can be seen in subplot (c) the algorithm converges to a local maximum
after by about 296 iterations of EM. Subplots(b) & (d) plot the marginal likelihood lower bound
F(q(s1:T),q(θ)) and its derivative, as a continuation of learning from the point in parameter
space where ML converged (see text) using the variational Bayes algorithm. The VB algorithm
converges after about 29 iterations of VBEM.
100
VB Hidden Markov Models 3.5. Experiments
(a) ML state prior π, tran-
sition A and emission C
probabilities.
(b) VB variational posterior
parameters for q(π), q(A)
and q(C).
(c) Variational posterior
mean probabilities: ⟨q(π)⟩,
⟨q(A)⟩ and ⟨q(C)⟩.
Figure 3.3: (a) Hinton diagrams showing the probabilities learnt by the ML model, for the initial
state prior π, transition matrix A, and emission matrix C. (b) Hinton diagrams for the analo-
gous quantities u(π), u(A) and u(C), which are the variational parameters (counts) describing
the posterior distributions over the parameters q(π), q(A), and q(C) respectively. (c) Hinton
diagrams showing the mean/modal probabilities of the posteriors represented in (b), which are
simply row-normalised versions of u(π), u(A) and u(C).
of the learnt models of forwards and backwards character sequences. As discussed above in
section 3.4.2, computing the predictive probability for VB is intractable, and so we approxi-
mate the VB solution with the model at the mean of the variational posterior given by equations
(3.54–3.59).
We used sentences taken from Lewis Carroll’s Alice’s Adventures in Wonderland. All punctu-
ation was removed to leave 26 letters and the blank space (that is to say p = 27 ). The training
data consisted of a maximum of 32 sentences (of length between 10 and 100 characters), and
the test data a ﬁxed set of 200 sentences of unconstrained length. As an example, the ﬁrst 10
training sequences are given below:
(1) ‘i shall be late ’
(2) ‘thought alice to herself after such a fall as this i shall think nothing of tumbling down stairs ’
(3) ‘how brave theyll all think me at home ’
(4) ‘why i wouldnt say anything about it even if i fell off the top of the house ’
(5) ‘which was very likely true ’
(6) ‘down down down ’
(7) ‘would the fall never come to an end ’
(8) ‘i wonder how many miles ive fallen by this time ’
(9) ‘she said aloud ’
(10) ‘i must be getting somewhere near the centre of the earth ’
101
VB Hidden Markov Models 3.5. Experiments
ML, MAP and VB hidden Markov models were trained on varying numbers of sentences (se-
quences), n, varying numbers of hidden states,k, and for MAP and VB, varying prior strengths,
u0, common to all the hyperparameters {u(π),u(A),u(C)}. The choices were:
n∈{1,2,3,4,5,6,8,16,32}, k ∈{1,2,4,10,20,40,60}, u 0 ∈{1,2,4,8}. (3.86)
The MAP and VB algorithms were initialised at the ML estimates (as per the previous experi-
ment), both for convenience and fairness. The experiments were repeated a total of 10 times to
explore potential multiple maxima in the optimisation.
In each scenario two models were learnt, one based on forwards sentences and the other on
backwards sentences, and the discrimination performance was measured by the average fraction
of times the forwards and backwards models correctly classiﬁed forwards and backwards test
sentences. This classiﬁcation was based on the log probability of the test sequence under the
forwards and backwards models learnt by each method.
Figure 3.4 presents some of the results from these experiments. Each subplot is an examination
of the effect of one of the following: the size of the training setn, the number of hidden statesk,
or the hyperparameter setting u0, whilst holding the other two quantities ﬁxed. For the purposes
of demonstrating the main trends, the results have been chosen around the canonical values of
n= 2 , k= 40 , and u0 = 2 .
Subplots (a,c,e) of ﬁgure 3.4 show the average test log probability per symbol in the test se-
quence, for MAP and VB algorithms, as reported on 10 runs of each algorithm. Note that for
VB the log probability is measured under the model at the mode of the VB posterior. The plotted
curve is the median of these 10 runs. The test log probability for the ML method is omitted from
these plots as it is well below the MAP and VB likelihoods (qualitatively speaking, it increases
with n in (a), it decreases with k in (c), and is constant with u0 in (e) as the ML algorithm
ignores the prior over parameters). Most importantly, in (a) we see that VB outperforms MAP
when the model is trained on only a few sentences, which suggests that entertaining a distribu-
tion over parameters is indeed improving performance. These log likelihoods are those of the
forward sequences evaluated under the forward models; we expect these trends to be repeated
for reverse sentences as well.
Subplots (b,d,f) of ﬁgure 3.4 show the fraction of correct classiﬁcations of forwards sentences
as forwards, and backwards sentences as backwards, as a function of n, kand u0, respectively.
We see that for the most part VB gives higher likelihood to the test sequences than MAP, and
also outperforms MAP and ML in terms of discrimination. For large amounts of training datan,
VB and MAP converge to approximately the same performance in terms of test likelihood and
discrimination. As the number of hidden stateskincreases, VB outperforms MAP considerably,
although we should note that the performance of VB also seems to degrade slightly for k >
102
VB Hidden Markov Models 3.5. Experiments
1 2 3 4 5 6 8 16 32−3.1
−3
−2.9
−2.8
−2.7
−2.6
−2.5
−2.4
−2.3
−2.2
MAP
VB
(a) Test log probability per sequence symbol:
dependence on n. With k = 40, u0 = 2.
1 2 3 4 5 6 8 16 320.4
0.5
0.6
0.7
0.8
0.9
1
ML
MAP
VB
(b) Test discrimination rate dependence on n.
With k = 40, u0 = 2.
1 2 4 10 20 40 60−3.05
−3
−2.95
−2.9
−2.85
−2.8
−2.75
−2.7
−2.65
MAP
VB
(c) Test log probability per sequence symbol:
dependence on k. With n = 2, u0 = 2.
1 2 4 10 20 40 600.4
0.5
0.6
0.7
0.8
0.9
1
ML
MAP
VB
(d) Test discrimination rate dependence on k.
With n = 2, u0 = 2.
1 2 4 8−3.1
−3.05
−3
−2.95
−2.9
−2.85
−2.8
−2.75
−2.7
−2.65
−2.6
MAP
VB
(e) Test log probability per sequence symbol:
dependence on u0. With n = 2, k= 40.
1 2 4 80.4
0.5
0.6
0.7
0.8
0.9
1
ML
MAP
VB
(f) Test discrimination rate dependence on u0.
With n = 2, k= 40.
Figure 3.4: Variations in performance in terms of test data log predictive probability and dis-
crimination rates of ML, MAP, and VB algorithms for training hidden Markov models. Note
that the reported predictive probabilities are per test sequence symbol. Refer to text for details.
103
VB Hidden Markov Models 3.6. Discussion
20. This decrease in performance with high k corresponds to a solution with the transition
matrix containing approximately equal probabilities in all entries, which shows that MAP is
over-regularising the parameters, and that VB does so also but not so severely. As the strength
of the hyperparameter u0 increases, we see that both the MAP and VB test log likelihoods
decrease, suggesting that u0 ≤2 is suitable. Indeed at u0 = 2 , the MAP algorithm suffers
considerably in terms of discrimination performance, despite the VB algorithm maintaining
high success rates.
There were some other general trends which were not reported in these plots. For example, in
(b) the onset of the rise in discrimination performance of MAP away from .5 occurs further to
the right as the strength u0 is increased. That is to say the over-regularising problem is worse
with a stronger prior, which makes sense. Similarly, on increasing u0, the point at which MAP
begins to decrease in (c,d) moves to the left. We should note also that on increasing u0, the test
log probability for VB (c) begins to decrease earlier in terms of k.
The test sentences on which the algorithms tend to make mistakes are the shorter, and more
reversible sentences, as to be expected. Some examples are: ‘alas ’, ‘pat ’, ‘oh ’, and ‘oh dear ’.
3.6 Discussion
In this chapter we have presented the ML, MAP and VB methods for learning HMMs from
data. The ML method suffers because it does not take into account model complexity and so
can overﬁt the data. The MAP method performs poorly both from over-regularisation and also
because it entertains a single point-parameter model instead of integrating over an ensemble. We
have seen that the VB algorithm outperforms both ML and MAP with respect to the likelihood
of test sequences and in discrimination tasks between forwards and reverse English sentences.
Note however, that a fairer comparison of MAP with VB would include allowing each method
to use cross-validation to ﬁnd the best setting of their hyperparameters. This is fairer because
the effective value of u0 used in the MAP algorithm changes depending on the basis used for
the optimisation.
In the experiments the automatic pruning of hidden states by the VB method has been welcomed
as a means of inferring useful structure in the data. However, in an ideal Bayesian application
one would prefer all states of the model to be active, but with potentially larger uncertainties in
the posterior distributions of their transition and emission parameters; in this way all parameters
of the model are used for predictions. This point is raised in MacKay (2001) where it is shown
that the VB method can inappropriately overprune degrees of freedom in a mixture of Gaussians.
Unless we really believe that our data was generated from an HMM with a ﬁnite number of
states, then there are powerful arguments for the Bayesian modeller to employ as complex a
104
VB Hidden Markov Models 3.6. Discussion
model as is computationally feasible, even for small data sets ( Neal, 1996, p. 9). In fact,
for Dirichlet-distributed parameters, it is possible to mathematically represent the limit of an
inﬁnite number of parameter dimensions, with ﬁnite resources. This result has been exploited for
mixture models (Neal, 1998b), Gaussian mixture models (Rasmussen, 2000), and more recently
has been applied to HMMs (Beal et al., 2002). In all these models, sampling is used for inferring
distributions over the parameters of a countably inﬁnite number of mixture components (or
hidden states). An area of future work is to compare VB HMMs to these inﬁnite HMMs.
105
Chapter 4
Variational Bayesian Mixtures of
Factor Analysers
4.1 Introduction
This chapter is concerned with learning good representations of high dimensional data, with
the goal being to perform well in density estimation and pattern classiﬁcation tasks. The work
described here builds on work in Ghahramani and Beal (2000), which ﬁrst introduced the vari-
ational method for Bayesian learning of a mixtures of factor analysers model, resulting in a
tractable means of integrating over all the parameters in order to avoid overﬁtting.
In the following subsections we introduce factor analysis (FA), and the mixtures of factor anal-
ysers (MFA) model which can be thought of as a mixture of reduced-parameter Gaussians. In
section 4.2 we explain why an exact Bayesian treatment of MFAs is intractable, and present a
variational Bayesian algorithm for learning. We show how to learn distributions over the pa-
rameters of the MFA model, how to optimise its hyperparameters, and how to automatically
determine the dimensionality of each analyser using automatic relevance determination (ARD)
methods. In section 4.3 we propose heuristics for efﬁciently exploring the (one-dimensional)
space of the number of components in the mixture, and in section4.5 we present synthetic exper-
iments showing that the model can simultaneously learn the number of analysers and their intrin-
sic dimensionalities. In section 4.6 we apply the VBMFA to the real-world task of classifying
digits, and show improved performance over a BIC-penalised maximum likelihood approach.
In section 4.7 we examine the tightness of the VB lower bound using importance sampling es-
timates of the exact marginal likelihood, using as importance distributions the posteriors from
the VB optimisation. We also investigate the effectiveness of using heavy-tailed and mixture
distributions in this procedure. We then conclude in section 4.8 with a brief outlook on recent
research progress in this area.
106
VB Mixtures of Factor Analysers 4.1. Introduction
4.1.1 Dimensionality reduction using factor analysis
Factor analysis is a method for modelling correlations in multidimensional data, by expressing
the correlations in a lower-dimensional, oriented subspace. Let the data set bey = {y1,..., yn}.
The model assumes that each p-dimensional data vector yi was generated by ﬁrst linearly trans-
forming a k<p dimensional vector of unobserved independent zero-mean unit-variance Gaus-
sian sources (factors), xi = [ xi1,..., xik], translating by a ﬁxed amount µ in the data space,
followed by adding p-dimensional zero-mean Gaussian noise, ni, with diagonal covariance ma-
trix Ψ (whose entries are sometimes referred to as theuniquenesses). Expressed mathematically,
we have
yi = Λ xi + µ + ni (4.1)
xi ∼ N(0,I), ni ∼ N(0,Ψ) , (4.2)
where Λ ( p×k) is the linear transformation known as the factor loading matrix, and µ is the
mean of the analyser. Integrating out xi and ni, it is simple to show that the marginal density of
yi is Gaussian about the displacement µ,
p(yi|Λ,µ,Ψ) =
∫
dxi p(xi)p(yi|xi,Λ,µ,Ψ) = N( yi|µ,ΛΛ⊤ + Ψ) , (4.3)
and the probability of an i.i.d. data set y = {yi}n
i=1 is given by
p(y |Λ,µ,Ψ) =
n∏
i=1
p(yi|Λ,µ,Ψ) . (4.4)
Given a data set y having covariance matrix Σ∗ and mean µ∗, factor analysis ﬁnds the Λ, µ and
Ψ that optimally ﬁt Σ∗ in the maximum likelihood sense. Since k < p, a factor analyser can
be seen as a reduced parameterisation of a full-covariance Gaussian. The (diagonal) entries of
the Ψ matrix concentrate on ﬁtting the axis-aligned (sensor) noise in the data, leaving the factor
loadings in Λ to model the remaining (assumed-interesting) covariance structure.
The effect of the mean term µ can be assimilated into the factor loading matrix by augmenting
the vector of factors with a constant bias dimension of 1, and adding a corresponding column µ
to the matrix Λ. With these modiﬁcations, learning theΛ matrix incorporates learning the mean;
in the equations of this chapter we keep the parameters separate, although the implementations
consider the combined quantity.
107
VB Mixtures of Factor Analysers 4.1. Introduction
Dimensionality of the latent space, k
A central problem in factor analysis is deciding on the dimensionality of the latent space. If too
low a value of kis chosen, then the model has to discard some of the covariance in the data as
noise, and if kis given too high a value this causes the model to ﬁt spurious correlations in the
data. Later we describe a Bayesian technique to determine this value automatically, but here
we ﬁrst give an understanding for an upper bound on the required value for k, by comparing
the number of degrees of freedom in the covariance speciﬁcation of the data set and the degrees
of freedom that the FA parameterisation has in its parameters. We need to distinguish between
the number of parameters and the degrees of freedom, which is really a measure of how many
independent directions in parameter space there are that affect the generative probability of the
data. The number of degrees of freedom in a factor analyser with latent space dimensionality k
cannot exceed the number of degrees of freedom of a full covariance matrix,1
2 p(p+ 1), nor can
it exceed the degrees of freedom offered by the parameterisation of the analyser, which is given
by d(k),
d(k) = kp+ p−1
2k(k−1) . (4.5)
The ﬁrst two terms on the right hand side are the degrees of freedom in the Λ and Ψ matrices
respectively, and the last term is the degrees of freedom in a (k×k) orthonormal matrix. This
last term needs to be subtracted because it represents a redundancy in the factor analysis param-
eterisation, namely that an arbitrary rotation or reﬂection of the latent vector space leaves the
covariance model of the data unchanged:
under Λ →ΛU, ΛΛ⊤ + Ψ →ΛU(ΛU)⊤ + Ψ (4.6)
= Λ UU⊤Λ⊤ + Ψ (4.7)
= ΛΛ ⊤ + Ψ . (4.8)
That is to say we must subtract the degrees of freedom from degeneracies in Λ associated with
arbitrary arrangements of the (a priori identical) hidden factors{xij}k
j=1. Since a p-dimensional
covariance matrix contains p(p+ 1) /2 pieces of information, in order to be able to perfectly
capture the covariance structure of the data the number of degrees of freedom in the analyser
(4.5) would have to exceed this. This inequality is a simple quadratic problem, for k≤p
kp+ p−1
2k(k−1) ≥1
2p(p+ 1) (4.9)
whose solution is given by
kmax =
⌈
p+ 1
2
[
1 −
√
1 + 8 p
]⌉
. (4.10)
We might be tempted to conclude that we only needkmax factors to model an arbitrary covariance
in pdimensions. However this neglects the constraint that all the diagonal elements of Ψ have
108
VB Mixtures of Factor Analysers 4.1. Introduction
to be positive. We conjecture that because of this constraint the number of factors needed to
model a full covariance matrix is p−1. This implies that for high dimensional data, if we want
to be able to model a full covariance structure, we cannot expect to be able to reduce the number
of parameters by that much at all using factor analysis. Fortunately, for many real data sets we
have good reason to believe that, at least locally, the data lies on a low dimensional manifold
which we can capture with only a few factors. The fact that this is a good approximation only
locally, when the manifold may be globally non-linear, is the motivation for mixture models,
discussed next.
4.1.2 Mixture models for manifold learning
It is often the case that apparently high dimensional data in fact lies, to a good approximation,
on a low dimensional manifold. For example, consider the data set consisting of many different
images of the same digit, given in terms of the pixel intensities. This data has as many dimen-
sions as there are pixels in each image. To explain this data we could ﬁrst specify a mean digit
image, which is a point in this high dimensional space representing a set of pixel intensities, and
then specify a small number of transformations away from that digit that would cover small vari-
ations in style or perhaps intensity. In factor analysis, each factor dictates the amount of each
linear transformation on the pixel intensities. However, with factor analysis we are restricted
to linear transformations, and so any one analyser can only explain well a small region of the
manifold in which it is locally linear, even though the manifold is globally non-linear.
One way to overcome this is to use mixture models to tile the data manifold. A mixture of
factor analysers models the density for a data point yi as a weighted average of factor analyser
densities
p(yi|π,Λ,µ,Ψ) =
S∑
si=1
p(si|π)p(yi|si,Λ,µ,Ψ) . (4.11)
Here, S is the number of mixture components in the model, π is the vector of mixing propor-
tions, si is a discrete indicator variable for the mixture component chosen to model data point
i, Λ = {Λs}S
s=1 is a set of factor loadings with Λs being the factor loading matrix for analyser
s, and µ = {µs}S
s=1 is the set of analyser means. The last term in the above probability is just
the single analyser density, given in equation (4.3). The directed acyclic graph for this model is
depicted in ﬁgure 4.1, which uses the plate notation to denote repetitions over a data set of size
n. Note that there are different indicator variablessi and latent space variablesxi for each plate.
By exploiting the factor analysis parameterisation of covariance matrices, a mixture of factor
analysers can be used to ﬁt a mixture of Gaussians to correlated high dimensional data without
requiring O(p2) parameters, or undesirable compromises such as axis-aligned covariance ma-
trices. In an MFA each Gaussian cluster has intrinsic dimensionality k, or ks if the dimensions
are allowed to vary across mixture components. Consequently, the mixture of factor analysers
109
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
L1,m1
Y
L2,m2

LS,mS...
yi
xi
si p
i=1...n
Figure 4.1: Generative model for Maximum Likelihood MFA. Circles denote random variables,
solid rectangles parameters, and the dashed rectangle the plate (repetitions) over the data.
simultaneously addresses the problems of clustering and local dimensionality reduction. When
Ψ is a multiple of the identity the model becomes a mixture of probabilistic PCAs (pPCA).
Tractable maximum likelihood (ML) procedures for ﬁtting MFA and pPCA models can be de-
rived from the Expectation Maximisation algorithm, see for example Ghahramani and Hinton
(1996b); Tipping and Bishop (1999). Factor analysis and its relationship to PCA and mixture
models is reviewed in Roweis and Ghahramani (1999).
4.2 Bayesian Mixture of Factor Analysers
The maximum likelihood approach to ﬁtting an MFA has several drawbacks. The EM algorithm
can easily get caught in local maxima, and often many restarts are required before a good max-
imum is reached. Technically speaking the log likelihoods in equations (4.3) and (4.11) are not
bounded from above, unless constraints are placed on the variances of the components of the
mixture. In practice this means that the covariance matrix ΛsΛs⊤ + Ψ can become singular if
a particular factor analyser models fewer points than the degrees of freedom in its covariance
matrix. Most importantly, the maximum likelihood approach for ﬁtting MFA models has the
severe drawback that it fails to take into account model complexity. For example the likelihood
can be increased by adding more analyser components to the mixture, up to the extreme where
each component models a single data point, and it can be further increased by supplying more
factors in each of the analysers.
110
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
A Bayesian approach overcomes these problems by treating the parameters of the model as
unknown quantities and averaging over the ensemble of models they produce. Deﬁning θ =
(Λ,µ,π,Ψ), we write the probability of the data averaged over a prior for parameters:
p(y) =
∫
dθ p(θ)p(y |θ) (4.12)
=
∫
dθ p(θ)
n∏
i=1
p(yi|θ) (4.13)
=
∫
dπ p(π)
∫
dΛ p(Λ)
∫
dµ p(µ)·
n∏
i=1
[ S∑
si=1
p(si|π)
∫
dxi p(xi)p(yi|si,xi,Λ,µ,Ψ)
]
. (4.14)
Equation (4.14) is the marginal likelihood of a dataset (called the marginal probability of the
data set by some researchers to avoid confusion with the likelihood of the parameters). By in-
tegrating out all those parameters whose number increase as the model complexity grows, we
effectively penalise models with more degrees of freedom, since they can a priori model a larger
range of data sets. By model complexity, we mean the number of components and the dimen-
sionality of each component. Integrating out the parameters naturally embodies the principle of
Occam’s razor (MacKay, 1992; Jefferys and Berger, 1992). As a result no parameters are ever
ﬁt to the data, but rather their posterior distributions are inferred and used to make predictions
about new data. For this chapter, we have chosen not to integrate over Ψ, although this could
also be done (see, for example, chapter 5). Since the number of degrees of freedom in Ψ does
not grow with the number of analysers or their dimensions, we treat it as a hyperparameter and
optimise it, even though this might result in some small degree of overﬁtting.
4.2.1 Parameter priors for MFA
While arbitrary choices can be made for the priors in ( 4.14), choosing priors that are conjugate
to the likelihood terms greatly simpliﬁes inference and interpretability. Therefore we choose a
symmetric Dirichlet prior for the mixing proportion π, with strength α∗,
p(π |α∗m∗) = Dir( π |α∗m∗) , such that m∗ =
[1
S,..., 1
S
]
. (4.15)
In this way the prior has a single hyperparameter, its strengthα∗, regardless of the dimensional-
ity of π. This hyperparameter is a measure of how we expect the mixing proportions to deviate
from being equal. One could imagine schemes in which we have non-symmetric prior mixing
proportion; an example could be making the hyperparameter in the Dirichlet prior an exponen-
tially decaying vector with a single decay rate hyperparameter, which induces a natural ordering
in the mixture components and so removes some identiﬁability problems. Nevertheless for our
111
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
purposes a symmetric prior sufﬁces, and expresses the notion that each component has equal a
priori chance of being used to generate each data point.
For the entries of the factor loading matrices,{Λs}S
s=1, we choose a hierarchical prior in order to
perform automatic relevance determination (ARD). Each column of each factor loading matrix
has a Gaussian prior with mean zero and a different precision parameter (drawn from a gamma
distribution with ﬁxed hyperparameters, see equation (4.18) below):
p(Λ |ν) =
S∏
s=1
p(Λs|νs) =
S∏
s=1
ks∏
j=1
p(Λs
·j |νs
j) =
S∏
s=1
ks∏
j=1
N(Λs
·j |0,I/νs
j) , (4.16)
where Λs
·j denotes the vector of entries in the jth column of the sth analyser in the mixture,
and νs
j is the same scalar precision for each entry in the corresponding column. The role of
these precision hyperparameters is explained in section 4.2.2. Note that because the spherical
Gaussian prior is separable into each of its pdimensions, the prior can equivalently be thought
of as a Gaussian with axis-aligned elliptical covariance on each row of each analyser:
p(Λ |ν) =
S∏
s=1
p∏
q=1
p(Λs
q· |νs) =
S∏
s=1
p∏
q=1
N(Λs
q· |0,diag (νs)−1) , (4.17)
where here Λs
q· is used to denote the qth row of the sth analyser. It will turn out to be simpler
to have the prior in this form conceptually for learning, since the likelihood terms for Λ factor
across its rows.
Since the number of hyperparameters in ν = {{νs
j}ks
j=1}S
s=1 increases with the number of anal-
ysers and also with the dimensionality of each analyser, we place a hyperprior on every element
of each νs precision vector, as follows:
p(ν |a∗,b∗) =
S∏
s=1
p(νs|a∗,b∗) =
S∏
s=1
ks∏
j=1
p(νs
j |a∗,b∗) =
S∏
s=1
ks∏
j=1
Ga(νs
j |a∗,b∗) , (4.18)
where a∗ and b∗ are shape and inverse-scale hyperhyperparameters for a gamma distribution
(see appendix A for a deﬁnition and properties of the gamma distribution). Note that the same
hyperprior is used for every element in ν. As a point of interest, combining the priors for Λ and
ν, and integrating out ν, we ﬁnd that the marginal prior over each Λs is Student-t distributed.
We will not need to make use of this result right here, but will return to it in section 4.7.1.
Lastly, the means of each analyser in the mixture need to be integrated out. A Gaussian prior
with mean µ∗ and axis-aligned precision diag (ν∗) is placed on each mean µs. Note that these
112
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
hyperparameters hold 2pdegrees of freedom, which is not a function of the size of the model.
The prior is the same for every analyser:
p(µ |µ∗,ν∗) =
S∏
s=1
p(µs|µ∗,ν∗) =
S∏
s=1
N(µs|µ∗,diag (ν∗)−1) (4.19)
Note that this prior has a different precision for each dimension of the output, whereas the prior
over the entries in the factor loading matrix uses the same precision on each row, and is different
only for each column of each analyser.
If we are to use the implementational convenience of augmenting the latent space with a constant
bias dimension, and adding a further column to each factor loading matrix to represent its mean,
then the prior over all the entries in the augmented factor loading matrix no longer factorises
over rows (4.17) or columns ( 4.18), but has to be expressed as a product of terms over every
entry of the matrix. This point will be made clearer when we derive the posterior distribution
over the augmented factor loading matrix.
We use Θ to denote the set of hyperparameters of the model:
Θ = ( α∗m∗,a∗,b∗,µ∗,ν∗,Ψ) . (4.20)
The directed acyclic graph for the generative model for this Bayesian MFA is shown graph-
ically in ﬁgure 4.2. Contrasting with the ML graphical model in ﬁgure 4.1, we can see that
all the model parameters (with the exception of the sensor noise Ψ) have been replaced with
uncertain variables, denoted with circles, and now have hyperparameters governing their prior
distributions. The generative model for the data remains the same, with the plate over the data
denoting i.i.d. instances of the hidden factors xi, each of which gives rise to an output yi. We
keep the graphical model concise by also using a plate over theSanalysers, which clearly shows
the role of the hyperpriors.
As an aside, we do not place a prior on the number of components, S. We instead place a sym-
metric Dirichlet prior over the mixing proportions. Technically, we should include a (square
boxed) node S, as the parent of both the plate over analysers and the hyperparameter αm. We
have also not placed priors over the number of factors of each analyser, {ks}S
s=1; this is inten-
tional as there exists an explicit penalty for using more dimensions — the extra entries in factor
loading matrix Λs need to be explained under a hyperprior distribution ( 4.16) which is gov-
erned by a new hyperparameter νs, which itself has to be explained under the hyperhyperprior
p(νs|a,b) of equation (4.18).
113
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
ns
Ls a*m*
Y*
a*, b*
yn
xn
sn p
n=1...N
ms
m*, n*

s=1...S
Figure 4.2: A Bayesian formulation for MFA. Here the plate notation is used to denote repeti-
tions over data nand over the Sanalysers in the generative model. Note that all the parameters
in the ML formulation, except Ψ, have now become uncertain random variables in the Bayesian
model (circled nodes in the graph), and are governed by hyperparameters (square boxes). The
number of hyperparameters in the model is constant and is not a function of the number of
analysers or their dimensionalities.
4.2.2 Inferring dimensionality using ARD
Each factor analyser sin the MFA models its local data as a linear projection ofks-dimensional
spherical Gaussian noise into the p-dimensional space. If a maximum dimensionality kmax is
set, then there exist kmax ×···× kmax = ( kmax)S possible subspace conﬁgurations amongst
the S analysers. Thus determining the optimal conﬁguration is exponentially intractable if a
discrete search is employed over analyser dimensionalities. Automatic relevance determination
(ARD) solves this discrete search problem with the use of continuous variables that allow asoft
blend of dimensionalities. Each factor analyser’s dimensionality is set tokmax and we use priors
that discourage large factor loadings. The width of each prior is controlled by a hyperparameter
(explained below), and the result of learning with this method is that only those hidden factor
dimensions that are required remain active after learning — the remaining dimensions are effec-
tively ‘switched off’. This general method was proposed by MacKay and Neal (see MacKay,
1996, for example), and was used in Bishop (1999) for Bayesian PCA, and is closely related to
the method given in Neal (1998a) for determining the relevance of inputs to a neural network.
Considering for the moment a single factor analyser. The ARD scheme uses a Gaussian prior
with a zero mean for the entries of the factor loading matrix, as shown in ( 4.16), given again
here:
p(Λs|νs) =
kmax∏
j=1
p(Λs
·j |νs
j) =
kmax∏
j=1
N(Λs
·j |0,I/νs
j) , (4.21)
where νs = {νs
1,...,ν s
kmax }are the precisions on the columns of Λs, which themselves are de-
noted by {Λ·1,..., Λ·kmax }. This zero-mean prior couples within-column entries in Λs, favour-
ing lower magnitude values.
114
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
If we apply this prior to each analyser in the mixture, each column of each factor loading matrix
is then governed by a separate νs
l parameter. If one of these precisions νs
l → ∞then the
outgoing weights (column lentries in Λs) for the lth factor in the sth analyser will have to be
very close to zero in order to maintain a high likelihood under this prior, and this in turn leads the
analyser to ignore this factor, and thus allows the model to reduce the intrinsic dimensionality of
x in the locale of that analyser if the data does not warrant this added dimension. We have not
yet explained how some of these precisions come to tend to inﬁnity; this will be made clearer in
the derivations of the learning rules in section 4.2.5.
The fully Bayesian application requires that we integrate out all parameters that scale with the
number of analyser components and their dimensions; for this reason we use the conjugate prior
for a precision variable, a gamma distribution with shape a∗ and inverse scale b∗, to integrate
over the ARD hyperparameters. Since we are integrating over the hyperparameters, it now
makes sense to consider removing a redundant factor loading when the posterior distribution
over the hyperparameter νs
l has most of its mass near inﬁnity. In practice we take the mean
of this posterior to be indicative of its position, and perform removal when it becomes very
large. This reduces the coding cost of the parameters, and as a redundant factor is not used
to model the data, this must increase the marginal likelihood p(y). We can be harsher still,
and prematurely remove those factors which haveνs
l escaping to inﬁnity, provided the resulting
marginal likelihood is better (we do not implement this scheme in our experiments).
4.2.3 Variational Bayesian derivation
Now that we have priors over the parameters of our model, we can set about computing the
marginal likelihood of data. But unfortunately, computing the marginal likelihood in equation
(4.14) is intractable because integrating over the parameters of the model induces correlations
in the posterior distributions between the hidden variables in all the nplates. As mentioned in
section 1.3, there are several methods that are used to approximate such integrals, for example
MCMC sampling techniques, the Laplace approximation, and the asymptotic BIC criterion.
For MFA and similar models, MCMC methods for Bayesian approaches have only recently
been applied by Fokou´e and Titterington (2003), with searches over model complexity in terms
of both the number of components and their dimensionalities carried out by reversible jump
techniques (Green, 1995). In related models, Laplace and asymptotic approximations have been
used to approximate Bayesian integration in mixtures of Gaussians (Roberts et al., 1998). Here
our focus is on analytically tractable approximations based on lower bounding the marginal
likelihood.
We begin with the log marginal likelihood of the data and ﬁrst construct a lower bound using
a variational distribution over the parameters {π,ν,Λ,µ}, and then perform a similar lower
115
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
bounding using a variational distribution for the hidden variables {si,xi}n
i=1. As a point of
nomenclature, just as we have been using the same notation p(·) for every prior distribution,
even though they may be Gaussian, gamma, Dirichlet etc., in what follows we also use the same
q(·) to denote different variational distributions for different parameters. The form of q(·) will
be clear from its arguments.
Combining (4.14) with the priors discussed above including the hierarchical prior on Λ, we
obtain the log marginal likelihood of the data, denoted L,
L≡ ln p(y) = ln
(∫
dπ p(π |α∗m∗)
∫
dν p(ν |a∗,b∗)
∫
dΛ p(Λ |ν)
∫
dµ p(µ |µ∗,ν∗)·
n∏
i=1
[ S∑
si=1
p(si|π)
∫
dxi p(xi)p(yi|si,xi,Λ,µ,Ψ)
])
. (4.22)
The marginal likelihood Lis in fact a function of the hyperparameters (α∗m∗,a∗,b∗,µ∗,ν∗),
and the sensor noise Ψ; this dependence is left implicit in this derivation. We introduce an arbi-
trary distribution q(π,ν,Λ,µ) to lower bound (4.22), followed by a second set of distributions
{q(si,xi)}n
i=1 to further lower bound the bound,
L≥
∫
dπ dν dΛ dµ q(π,ν,Λ,µ)
(
ln p(π |α∗m∗)p(ν |a∗,b∗)p(Λ |ν)p(µ |µ∗,ν∗)
q(π,ν,Λ,µ)
+
n∑
i=1
ln
[ S∑
si=1
p(si|π)
∫
dxi p(xi)p(yi|si,xi,Λ,µ,Ψ)
])
(4.23)
≥
∫
dπ dν dΛ dµ q(π,ν,Λ,µ)
(
ln p(π |α∗m∗)p(ν |a∗,b∗)p(Λ |ν)p(µ |µ∗,ν∗)
q(π,ν,Λ,µ)
+
n∑
i=1
[ S∑
si=1
∫
dxi q(si,xi)
(
ln p(si|π)p(xi)
q(si,xi) + ln p(yi|si,xi,Λ,µ,Ψ)
)])
.
(4.24)
In the ﬁrst inequality, the term on the second line is simply the log likelihood of yi for a ﬁxed
setting of parameters, which is then further lower bounded in the second inequality using a set
of distributions over the hidden variables{q(si,xi)}n
i=1. These distributions are independent of
the settings of the parameters π,ν,Λ,and µ, and they correspond to the standard variational
approximation of the factorisation between the parameters and the hidden variables:
p(π,ν,Λ,µ,{si,xi}n
i=1 |y) ≈q(π,ν,Λ,µ)
n∏
i=1
q(si,xi) . (4.25)
The distribution of hidden variables factorises across the plates because both the generative
model is i.i.d. and we have made the approximation that the parameters and hidden variables
are independent (see proof of theorem2.1 in section 2.3.1). Here we use a further variational ap-
116
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
proximation amongst the parameters, which can be explained by equating the functional deriva-
tives of equation (4.24) with respect to q(π,ν,Λ,µ) to zero. One ﬁnds that
q(π,ν,Λ,µ) ∝p(π |α∗m∗)p(ν |a∗,b∗)p(Λ |ν)p(µ |µ∗,ν∗) ·
exp
[ n∑
i=1
S∑
si=1
⟨ln p(si|π)p(yi|si,xi,Λ,µ,Ψ)⟩q(si,xi)
]
(4.26)
= q(π)q(ν,Λ,µ) (4.27)
≈q(π)q(ν)q(Λ,µ) . (4.28)
In the second line, the approximate posterior factorises exactly into a contribution from the mix-
ing proportions and the remaining parameters. Unfortunately it is not easy to take expectations
with respect to the joint distribution over Λ and its parent parameter ν, and therefore we make
the second variational approximation in the last line, equation (4.28). The very last term q(Λ,µ)
turns out to be jointly Gaussian, and so is of tractable form.
We should note that except for the initial factorisation between the hidden variables and the
parameters, the factorisation q(ν,Λ,µ) ≈q(ν)q(Λ,µ) is the only other approximating factori-
sation we make; all other factorisations fall out naturally from the conditional independencies in
the model. Note that the complete-data likelihood for mixtures of factor analysers is in the expo-
nential family, even after the inclusion of the precision parameters ν. We could therefore apply
the results of section2.4, but this would entail ﬁnding expectations over gamma-Gaussian distri-
butions jointly over ν and Λ. Although it is possible to take these expectations, for convenience
we choose a separable variational posterior on ν and Λ.
From this point on we assimilate each analyser’s mean positionµs into its factor loading matrix,
in order to keep the presentation concise. The derivations use˜Λ to denote the concatenated result
[Λ µ]. Therefore the prior over the entire factor loadings ˜Λ is now a function of the precision
parameters {νs}S
s=1 (which themselves have hyperparameters a,b) and the hyperparameters
µ∗,ν∗. Also, the variational posterior q(Λ,µ) becomes q(˜Λ).
117
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
Substituting the factorised approximations (4.25) and (4.28) into the lower bound (4.24) results
in the following lower bound for the marginal likelihood,
L≥
∫
dπ q(π) ln p(π |α∗,m∗)
q(π)
+
S∑
s=1
∫
dνs q(νs)
[
ln p(νs|a∗,b∗)
q(νs) +
∫
d˜Λs q(˜Λs) ln p(˜Λs|νs,µ∗,ν∗)
q(˜Λs)
]
+
n∑
i=1
S∑
si=1
q(si)
[∫
dπ q(π) ln p(si|π)
q(si) +
∫
dxi q(xi|si) ln p(xi)
q(xi|si)
+
∫
d˜Λ q(˜Λ)
∫
dxi q(xi|si) ln p(yi|si,xi,˜Λ,Ψ)
]
(4.29)
≡F(q(π),{q(νs),q(˜Λs),{q(si),q(xi|si)}n
i=1}S
s=1,α∗m∗,a∗,b∗,µ∗,ν∗,Ψ,y) (4.30)
= F(q(θ),q(s,x),Θ) . (4.31)
Thus the lower bound is a functional of the variational posterior distributions over the param-
eters, collectively denoted q(θ), a functional of the variational posterior distribution over the
hidden variables of every data point, collectively denoted q(s,x), and also a function of the set
of hyperparameters in the model Θ, as given in (4.20). In the last line above, we have dropped
y as an argument for the lower bound since it is ﬁxed. The full variational posterior is
p(π,ν,Λ,µ,s,x |y) ≈q(π)
S∏
s=1
q(νs)q(˜Λs) ·
n∏
i=1
S∏
si=1
q(si)q(xi|si) . (4.32)
Note that if we had not made the factorisation q(ν,Λ,µ) ≈q(ν)q(Λ,µ), then the last term
in Fwould have required averages not over q(˜Λ), but also over the combined q(ν,˜Λ), which
would have become fairly cumbersome, although not intractable.
Decomposition of F
The goal of learning is then to maximise F, thus increasing the lower bound on L, the exact
marginal likelihood. Note that there is an interesting trade-off at play here. The last term in
equation (4.29) is the log likelihood of the data set averaged over the uncertainty we have in the
hidden variables and parameters. We can increase this term by altering Ψ and the variational
posterior distributions q(θ) and q(s,x) so as to maximise this contribution. However the ﬁrst
three lines of (4.29) contain terms that are negative Kullback-Leibler (KL) divergences between
the approximate posteriors over the parameters and the priors we hold on them. So to increase
the lower bound on the marginal likelihood (which does not necessarily imply that the marginal
likelihood itself increases, since the bound is not tight), we should also consider moving our
approximate posteriors towards the priors, thus decreasing the respective KL divergences. In
this manner Felegantly incorporates the trade-off between modelling the data and remaining
118
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
consistent with our prior beliefs. Indeed if there were no contributions from the data (i.e. the
last term in equation (4.29) were zero) then the optimal approximate posteriors would default to
the prior distributions.
At this stage it is worth noting that, with the exception of the ﬁrst term in equation (4.29), Fcan
be broken down into contributions from each component of the mixture (indexed by s). This
fact that will be useful later when we wish to compare how well each component of the mixture
is modelling its respective data.
4.2.4 Optimising the lower bound
To optimise the lower bound we simply take functional derivatives with respect to each of the
q(·) distributions and equate these to zero to ﬁnd the distributions that extremiseF(see chapter
2). Synchronous updating of the variational posteriors is not guaranteed to increase Fbut
consecutive updating of dependent distributions is. The result is that each update is guaranteed
to monotonically and maximally increase F.
The update for the variational posterior over mixing proportions π:
∂F
∂q(π) = ln p(π |α∗m∗) +
n∑
i=1
S∑
si=1
q(si) ln p(si|π) −ln q(π) + c (4.33)
= ln
[ S∏
s=1
πα∗m∗
s−1
s ·
n∏
i=1
S∏
si=1
πq(si)
si
]
−ln q(π) + c (4.34)
= ln
[ S∏
s=1
πα∗m∗
s+Pn
i=1 q(si)−1
s
]
−ln q(π) + c (4.35)
=⇒ q(π) = Dir( π |αm) , (4.36)
where each element of the variational parameter αm is given by:
αms = α∗m∗
s +
n∑
i=1
q(si) , (4.37)
which gives α= α∗ + n. Thus the strength of our posterior belief in the meanm increases with
the amount of data in a very simple fashion. For this update we have taken m∗
s = 1 /S from
(4.15), and used ∑S
s=1 ms = 1 .
119
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
The variational posterior in the precision parameter for the lth column of the sth factor loading
matrix Λs,
∂F
∂q(νs
l) = ln p(νs
l |a∗,b∗) +
∫
dΛs q(Λs) ln p(Λs
l |νs
l) −ln q(νs
l) + c (4.38)
= ( a∗ −1) ln νs
l −b∗νs
l + 1
2
p∑
q=1
[
ln νs
l −νs
l
⟨
Λs
ql
2⟩
q(Λs)
]
−ln q(νs
l) + c, (4.39)
which implies that the precision is Gamma distributed:
q(νs
l) = Ga( νs
l |a∗ + p
2, b∗ + 1
2
p∑
q=1
⟨
Λs
ql
2⟩
q(Λs)) = Ga( νs
l |a,bs
l) , (4.40)
Note that these updates constitute the key steps for the ARD mechanisms in place over the
columns of the factor loading matrices.
The variational posterior over the centres and factor loadings of each analyser is obtained by
taking functional derivatives with respect to q(˜Λ):
∂F
∂q(˜Λs)
=
∫
dνs q(νs) ln p(˜Λs|νs,µ∗,ν∗)
+
n∑
i=1
q(si)
∫
dxi q(xi|si) ln p(yi|si,xi,˜Λsi,Ψ) −ln q(˜Λs) + c (4.41)
= 1
2
∫
dνs q(νs)
p∑
q=1
k∑
l=1
[
ln νs
l −νs
lΛs
ql
2]
+ 1
2
p∑
q=1
[
ln ν∗
q −ν∗
q
(
µs
q −µ∗
q
)2]
−ln q(Λs,µs) + c
−1
2
n∑
i=1
q(si)tr

Ψ−1
⟨(
yi −
[
Λs µs
][
xi
1
])(
yi −
[
Λs µs
][
xi
1
])⊤⟩
q(xi | si)


(4.42)
where were have moved from the ˜Λ notation to using both Λ and µ separately to express the
different prior form separately. In ( 4.42), there are two summations over the rows of the factor
loading matrix, and a trace term, which can also be written as a sum over rows. Therefore the
posterior factorises over the rows of ˜Λs,
q(˜Λs) =
p∏
q=1
q(˜Λs
q·) =
p∏
q=1
N(˜Λs
q· |˜Λ
s
q·,˜Γs
q) , (4.43)
120
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
where ˜Λs
q· denotes the column vector corresponding to the qth row of ˜Λs, which has ks + 1
dimensions. To clarify the notation, this vector then has mean ˜Λ
s
q·, and covariance matrix ˜Γs
q.
These variational posterior parameters are given by:
˜Γs
q =
[
Σq,s
ΛΛ
−1 Σq,s
Λµ
−1
Σq,s
µΛ
−1 Σq,s
µµ
−1
]−1
of size (ks + 1) ×(ks + 1) (4.44)
˜Λ
s
q· =
[
Λ
s
q·
µs
q
]
of size (ks + 1) ×1 (4.45)
with
Σq,s
ΛΛ
−1 = diag ⟨νs⟩q(νs) + Ψ−1
qq
n∑
i=1
q(si)
⣨
xixi⊤
⟩
q(xi | si)
(4.46)
Σq,s
µµ
−1 = ν∗
q + Ψ−1
qq
n∑
i=1
q(si) (4.47)
Σq,s
Λµ
−1 = Ψ −1
qq
n∑
i=1
q(si) ⟨xi⟩q(xi | si) = Σ q,s
µΛ
−1⊤
(4.48)
Λ
s
q· =
[
˜Γs
q
]
ΛΛ
(
Ψ−1
qq
n∑
i=1
q(si)yi,q⟨xi⟩q(xi | si)
)
(4.49)
µs
q =
[
˜Γs
q
]
µµ
(
Ψ−1
qq
n∑
i=1
q(si)yi,q + ν∗
qµ∗
q
)
. (4.50)
This somewhat complicated posterior is the result of maintaining a tractable joint over the cen-
tres and factor loadings of each analyser. Note that the optimal distribution for each ˜Λs matrix
as a whole now has block diagonal covariance structure: even though each˜Λs is a (p×(ks+1))
matrix, its covariance only has O(p(ks + 1)2) parameters — a direct consequence of the likeli-
hood factorising over the output dimensions.
The variational posterior for the hidden factors xi, conditioned on the indicator variable si, is
given by taking functional derivatives with respect toq(xi|si):
∂F
∂q(xi|si) = q(si) ln p(xi) +
∫
d˜Λsi q(˜Λsi)q(si) ln p(yi|si,xi,˜Λsi,Ψ)
−q(si) ln q(xi|si) + c (4.51)
= q(si)

−1
2xi⊤ Ixi −1
2tr

Ψ−1
⟨(
yi −˜Λsi
[
xi
1
])(
yi −˜Λsi
[
xi
1
])⊤⟩
q(Λsi)


−ln q(xi|si)
]
+ c (4.52)
121
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
which, regardless of the value ofq(si), produces the Gaussian posterior in xi for each setting of
si:
q(xi|s) = N( xi|xs
i,Σs) (4.53)
with [Σs]−1 = I +
⣨
Λs⊤Ψ−1Λs
⟩
q(˜Λs)
(4.54)
xs
i = Σ s
⣨
Λs⊤Ψ−1(yi −µs)
⟩
q(˜Λs)
(4.55)
Note that the covariance Σs of the hidden state is the same for every data point, and is not a
function of the posterior responsibility q(si), as in ordinary factor analysis — only the mean of
the posterior over xi is a function of the data yi. Note also that the xs
i depend indirectly on the
q(si) through (4.49), which is the update for the factor loadings and centre position of analyser
s.
The variational posterior for the set of indicator variables s = {si}n
i=1 is given by
∂F
∂q(si) =
∫
dπ q(π) ln p(si|π) −
∫
dxi q(xi|si) ln q(xi|si)
+
∫
d˜Λsi q(˜Λsi)
∫
dxi q(xi|si) ln p(yi|si,xi,˜Λsi,Ψ) −ln q(si) + c (4.56)
which, utilising a result of Dirichlet distributions given in appendix A, yields
q(si) = 1
Zi
exp
[
ψ(αmsi) −ψ(α) + 1
2 ln |Σsi|
−1
2tr

Ψ−1
⟨(
yi −˜Λsi
[
xi
1
])(
yi −˜Λsi
[
xi
1
])⊤⟩
q(˜Λsi)q(xi | si)



 ,
(4.57)
where Zi is a normalisation constant for each data point, such that ∑S
si=1 q(si) = 1 , and ψ(·)
is the digamma function.
By examining the dependencies of each variational posterior’s update rules on the other distribu-
tions, it becomes clear that certain update orderings are more efﬁcient than others in increasing
F. For example, the q(xi|si), q(˜Λ) and q(si) distributions are highly coupled and it therefore
might make sense to perform these updates several times before updating q(π) or q(ν).
4.2.5 Optimising the hyperparameters
The hyperparameters for a Bayesian MFA are Θ = ( α∗m∗,a∗,b∗,µ∗,ν∗,Ψ).
122
VB Mixtures of Factor Analysers 4.2. Bayesian Mixture of Factor Analysers
Beginning with Ψ, we simply take derivatives of Fwith respect to Ψ−1, leading to:
∂F
∂Ψ−1 = −1
2
n∑
i=1
S∑
si=1
q(si)
∫
d˜Λsi q(˜Λsi)
∫
dxi q(xi|si)·
∂
∂Ψ−1


(
yi −˜Λsi
[
xi
1
])⊤
Ψ−1
(
yi −˜Λsi
[
xi
1
])
+ ln |Ψ|

 (4.58)
=⇒ Ψ−1 = diag

1
N
n∑
i=1
⟨(
yi −˜Λs
[
xi
1
])(
yi −˜Λs
[
xi
1
])⊤⟩
q(˜Λs)q(si)q(xi | si)


(4.59)
where here we use diag as the operator which sets off-diagonal terms to zero.
By writing Fas a function of a∗ and b∗ only, we can differentiate with respect to these hyper-
parameters to yield the ﬁxed point equations:
F(a∗,b∗) =
S∑
s=1
∫
dνs q(νs) ln p(νs|a∗,b∗) + c (4.60)
=
S∑
s=1
k∑
l=1
∫
dνs
l q(νs
l) [a∗ ln b∗ −ln Γ(a∗) + ( a∗ −1) ln νs
l −b∗νs
l] + c, (4.61)
∂F
∂a∗ = 0 = ⇒ ψ(a∗) = ln( b∗) + 1
Sk
S∑
s=1
k∑
l=1
⟨ln νs
l⟩q(νs
l ) (4.62)
∂F
∂b∗ = 0 = ⇒ b∗−1 = 1
a∗Sk
S∑
s=1
k∑
l=1
⟨νs
l⟩q(νs
l ) . (4.63)
Solving for the ﬁxed point amounts to setting the prior distribution’s ﬁrst moment and ﬁrst log-
arithmic moment to the respective averages of those quantities over the factor loading matrices.
The expectations for the gamma random variables are given in appendix A.
Similarly, by writing Fas a function of α∗ and m∗ only, we obtain
F(α∗,m∗) =
∫
dπ q(π) ln p(π |α∗m∗) (4.64)
=
∫
dπ q(π)
[
ln Γ(α∗) −
S∑
s=1
[ln Γ(α∗m∗
s) −(α∗m∗
s −1) ln πs]
]
. (4.65)
123
VB Mixtures of Factor Analysers 4.3. Model exploration: birth and death
Bearing in mind that q(π) is Dirichlet with parameter αm, and that we have a scaled prior
m∗
s = 1 /Sas given in (4.15), we can express the lower bound as a function of α∗ only:
F(α∗) = ln Γ( α∗) −Sln Γ
(α∗
S
)
+
(α∗
S −1
) S∑
s=1
[ψ(αms) −ψ(α)] (4.66)
Taking derivatives of this quantity with respect toα∗ and setting to zero, we obtain:
ψ(α∗) −ψ(α∗
S ) = 1
S
S∑
s=1
[ψ(α) −ψ(αms)] . (4.67)
The second derivative with respect to α∗ of (4.66) is negative for α∗ > 0, which implies the
solution of (4.67) is a maximum. This maximum can be found using gradient following tech-
niques such as Newton-Raphson. The update for m∗ is not required, since we assume that the
prior over the mixing proportions is symmetric.
The update for the prior over the centres {µs}S
S=1 of each of the factor analysers is given by
considering terms in Fthat are functions of µ∗ and ν∗:
F(µ∗,ν∗) =
∫
dµ q(µ) ln p(µ |µ∗,ν∗) (4.68)
= 1
2
S∑
s=1
∫
dµs q(µs)
[
ln |diag (ν∗)|−(µs −µ∗)⊤diag (ν∗) (µs −µ∗)
]
.
(4.69)
Taking derivatives with respect toµ∗ ﬁrst, and then ν∗, equating each to zero yields the updates
µ∗ = 1
S
S∑
s=1
⟨µs⟩q(µs) (4.70)
ν∗ = [ ν∗
1 ,...,ν ∗
p], with ν∗
q = 1
S
S∑
s=1
⟨
(µs
q −µ∗
q)(µs
q −µ∗
q)
⟩
q(µs) , (4.71)
where the update for ν∗ uses the already updated µ∗.
4.3 Model exploration: birth and death
We already have an ARD mechanism in place to discover the local dimensionality for each
analyser in the mixture, as part of the inference procedure over the precisions ν. However we
have not yet addressed the problem of inferring the number of analysers.
The advantage of the Bayesian framework is that different model structures can be compared
without having to rely on heuristic penalty or cost functions to compare their complexities;
124
VB Mixtures of Factor Analysers 4.3. Model exploration: birth and death
ideally different model structures m and m′ should be compared using the difference of log
marginal likelihoods L(m) and L(m′). In this work we use F(m) and F(m′) as guides to the
intractable log marginal likelihoods.
This has advantages over unpenalised maximum likelihood methods where, for example, in the
split and merge algorithm described in Ueda et al. (2000) changes to model complexity are
limited to simultaneous split and merge operations such that the number of components in the
mixture remain the same. Whilst this approach is unable to explore differing sizes of models,
it is successful in avoiding some local maxima in the optimisation process. For example, a
Gaussian component straddled between two distinct clusters of data is an ideal candidate for
a split operation — unfortunately their method requires that this split be accompanied with a
merging of two other components elsewhere to keep the number of components ﬁxed.
In our Bayesian model, though, we are allowed to propose any changes to the number of com-
ponents in the mixture. We look at the simple cases of incremental and decremental changes to
the total number, S, since we do not expect wild changes to the model structure to be an efﬁ-
cient method for exploring the space. This is achieved through birth and death ‘moves’, where
a component is removed from or introduced into the mixture model. This modiﬁed model is
then trained further as described in section 4.2.4 until a measure of convergence is reached (see
below), at which point the proposal is accepted or rejected based on the change in F. Another
proposal is then made and the procedure repeated, up to a point when no further proposals are
accepted. In this model (although not in a general application) component death occurs natu-
rally as a by-product of the optimisation; the following sections explain the death mechanism,
and address some interesting aspects of the birth process, which we have more control over.
Our method is similar to that of Reversible Jump Markov chain Monte Carlo (RJMCMC)
(Green, 1995) applied to mixture models, where birth and death moves can also be used to
navigate amongst different sized models (Richardson and Green, 1997). By sampling in the full
space of model parameters for all structures, RJMCMC methods converge to the exact poste-
rior distribution over structures. However, in order to ensure reversibility of the Markov chain,
complicated Metropolis-Hastings acceptance functions need to be derived and evaluated for
each proposal from one parameter subspace to another. Moreover, the method suffers from the
usual problems of MCMC methods, namely difﬁculty in assessing convergence and long simu-
lation run time. The variational Bayesian method attempts to estimate the posterior distribution
directly, not by obtaining samples of parameters and structures, but by attempting to directly
integrate over the parameters using a lower bound arrived at deterministically. Moreover, we
can obtain a surrogate for the posterior distribution over model structures, p(m|y), which is
not represented as some large set of samples, but is obtained using a quantity proportional to
p(m) exp{F(m)}, where F(m) is the optimal (highest) lower bound achieved for a model m
of particular structure.
125
VB Mixtures of Factor Analysers 4.3. Model exploration: birth and death
4.3.1 Heuristics for component death
There are two routes for a component death occurring in this model, the ﬁrst is by natural causes
and the second through intervention. Each is explained in turn below.
When optimising F, occasionally one ﬁnds that for some mixture component s′: ∑n
i=1 q(s′
i) =
0 (to machine precision), even though the component still has non-zero prior probability of
being used in the mixture, p(s′
i) =
∫
dπp(π)p(s′
i|π). This is equivalent to saying that it has
no responsibility for any of the data, and as a result its parameter posterior distributions have
defaulted exactly to the priors. For example, the mean location of the centre of the analyser
component is at the centre of the prior distribution (this can be deduced from examining ( 4.50)
for the case of q(s′
i) = 0 ∀i), and the factor loadings have mean zero and high precisions
νs′
, referring to ( 4.40). If the mean of the prior over analyser centres is not located near data
(see next removal method below), then this component is effectively redundant (it cannot even
model data with the uniquenesses matrix Ψ, say), and can be removed from the model. How
does the removal of this component affect the lower bound on the marginal likelihood,F? Since
the posterior responsibility of the component is zero it does not contribute to the last term of
(4.29), which sums over the data, n. Also, since its variational posteriors over the parameters
are all in accord with the priors, then the KL divergence terms in ( 4.29) are all zero, except for
the very ﬁrst term which is the negative KL divergence between the variational posterior and
prior distribution over the mixing proportions π. Whilst the removal of the component leaves
all other terms in Funchanged, not having this ‘barren’ dimension s′ to integrate over should
increase this term.
It seems counter-intuitive that the mean of the prior over factor analyser centres might be far
from data, as suggested in the previous paragraph, given that the hyperparameters of the prior
are updated to reﬂect the position of the analysers. However, there are cases in which the dis-
tribution of data is ‘hollow’ (see, for example, the spiral data set of section 4.5.3), and in this
case redundant components are very easily identiﬁed with zero responsibilities, and removed.
If the redundant components default to a position which is close to data, their posterior respon-
sibilities may not fall to exactly zero, being able to still use the covariance given in Ψ to model
the data. In this case a more aggressive pruning procedure is required, where we examine the
change in Fthat occurs after removing a component we suspect is becoming, or has become,
redundant. We gain by not having to code its parameters, but we may lose if the data in its locale
are being uniquely modelled by it, in which case Fmay drop. If Fshould drop, there is the
option of continuing the optimisation to see if Feventually improves (see next section on birth
processes), and rejecting the removal operation if it does not. We do not implement this ‘testing’
method in our experiments, and rely solely on the ﬁrst method and remove components once
their total posterior responsibilities fall below a reasonable level (in practice less than one data
point’s worth).
126
VB Mixtures of Factor Analysers 4.3. Model exploration: birth and death
This mechanism for (automatic) removal of components is useful as it allows the data to dictate
how many mixture components are required. However we should note that if the data is not
distributed as a mixture of Gaussian components, the size of the data set will affect the returned
number of components. Thus the number of components should not be taken to mean the
number of ‘clusters’.
4.3.2 Heuristics for component birth
Component birth does not happen spontaneously during learning, so we have to introduce a
heuristic. Even though changes in model structure may be proposed at any point during learning,
it makes sense only to do so when learning has plateaued, so as to exploit (in terms of F) the
current structure to the full. We deﬁne an epoch as that period of learning beginning with a
proposal of a model alteration, up to the point of convergence of the variational learning rules.
One possible heuristic for deciding at which point to end an epoch can be constructed by looking
at the rate of change of the lower bound with iterations of variational EM. If∆F= F(t)−F(t−1)
falls below a critical value then we can assume that we have plateaued. However it is not easy to
deﬁne such simple thresholds in a manner that scales appropriately with both model complexity
and amount of data. An alternative (implemented in the experiments) is to examine the rate of
change of the posterior class-conditional responsibilities, as given in the q(si) matrix (n ×S).
A suitable function of this sort can be such that it does not depend directly on the data size,
dimensionality, or current model complexity. In this work we consider the end of an epoch to be
when the rate of change of responsibility for each analyser, averaged over all data, falls below a
tolerance — this has the intuitive interpretation that the components are no longer ‘in ﬂux’ and
are modelling their data as best they can in that conﬁguration. We shall call this quantity the
agitation:
agitation(s)(t) ≡
∑n
i=1
q(si)(t) −q(si)(t−1)
∑n
i=1 q(si)(t) , (4.72)
where (t) denotes the iteration number of VBEM. We can see that the agitation of each analyser
does not directly scale with number of analysers, data points, or dimensionality of the data. Thus
a ﬁxed tolerance for this quantity can be chosen that is applicable throughout the optimisation
process. We should note that this measure is one of many possible, such as using squared norms
etc.
A sensible way to introduce a component into the model is to create that component in the
image of an existing component, which we shall call the parent. Simply reproducing the exact
parameters of the parent does not sufﬁce as the symmetry of the resulting pair needs to be broken
for them to model the data differently.
127
VB Mixtures of Factor Analysers 4.3. Model exploration: birth and death
One possible approach would be to remove the parent component, s′, and replace it with two
components, the ‘children’, with their means displaced symmetrically about the parent’s mean,
by a vector sampled from the parent’s distribution, its covariance ellipsoid given byΛs′
Λs′⊤
+Ψ.
We call this a spatial split. This appeals to the notion that one might expect areas of data that
are currently being modelled by one elongated Gaussian to be modelled better by two, displaced
most probably along the major axis of variance of that data. However this approach is hard to
ﬁne tune so that it scales well with the data dimensionality, p. For example, if the displacement
is slightly too large then it becomes very likely in high dimensions that both children model the
data poorly and die naturally as a result. If it is too small then the components will diverge very
slowly.
Again appealing to the class-conditional responsibilities for the data, we can deﬁne a procedure
for splitting components that is not directly a function of the dimensionality, or any length scale
of the local data. The approach taken in this work uses a partition of the parent’s posterior
responsibilities for each of the data, q(si = s′), along a direction ds′
sampled from the parent’s
covariance ellipsoid. Those data having a positive dot product with the sampled direction donate
their responsibilities to one child sa, and vice-versa for the other child sb. Mathematically, we
sample a direction d and deﬁne an allocation indicator variable for each data point,
d ∼ N(d |⟨µs′
⟩q(µs′),⟨Λs′
Λs′⊤
⟩q(Λs′) + Ψ) (4.73)
ri =



1 if (yi −µs′
)⊤d ≥0
0 if (yi −µs′
)⊤d <0
for i= 1 ,...,n. (4.74)
We then set the posterior probabilities in q(si) to reﬂect these assignments, introducing a hard-
ness parameter αh, ranging from .5 to 1:
q(sa
i) = q(s′
i) [αhri + (1 −αh)(1 −ri)] (4.75)
q(sb
i) = q(s′
i) [(1 −αh)ri + αh(1 −ri)] (4.76)
When αh = 1 , all the responsibility is transferred to the assigned child, and when αh = .5 the
responsibility is shared equally. In the experiments in this chapter we use αh = 1 .
The advantage of this approach is that the birth is made in responsibility space rather than
data-space, and is therefore dimension-insensitive. The optimisation then continues, with the s′
analyser removed and the sa and sb analysers in its place. The ﬁrst variational updates should
be for q(Λsa
) and q(Λsb
) since these immediately reﬂect the change (note that the update for
q(xi) is not a function of the responsibilities — see equation (4.53)).
The mechanism that chooses which component is to be the parent of a pair-birth operation must
allow the space of models to be explored fully. A simple method would be to pick the component
at random amongst those present. This has an advantage over a deterministic method, in that
128
VB Mixtures of Factor Analysers 4.3. Model exploration: birth and death
the latter could preclude some components from ever being considered. Interestingly though,
there is information in Fthat can be used to guide the choice of component to split: with
the exception of the ﬁrst term in equation ( 4.29), the remaining terms can be decomposed into
component-speciﬁc contributions, Fs. An ordering for parent choice can be deﬁned using Fs,
with the result that is it possible to concentrate attempted births on those components that are
not currently modelling their data well. This mirrors the approach taken in Ueda et al. (2000),
where the criterion was the (KL) discrepancy between each analyser’s local density model and
the empirical density of the data.
If, at the end of an epoch, we reject the proposed birth so returning to the original conﬁguration,
we may either attempt to split the same component again, but with a new randomly sampled di-
rection, or move on to the next ‘best’ component in the ordering. We use the following function
to deﬁne Fs, from which the ordering is recalculated after every successful epoch:
Fs = F({Q},α∗m∗,a∗,b∗,µ∗,ν∗,Ψ |Y)
=
∫
dνs q(νs)
[
ln p(νs|a∗,b∗)
q(νs) +
∫
d˜Λs q(˜Λs)p(˜Λs|νs,µ∗,ν∗)
q(˜Λs)
]
+ 1∑n
i=1 q(si)
n∑
i=1
q(si)
[∫
dπ q(π) ln p(si|π)
q(si) +
∫
dxi q(xi|si) ln p(xi)
q(xi|si)
+
∫
d˜Λs q(˜Λs)
∫
dxi q(xi|si) ln p(yi|si,xi,˜Λs,Ψ)
]
(4.77)
This has the intuitive interpretation as being the likelihood of the data (weighted by its data
responsibilities) under analyser s, normalised by its overall responsibility, with the relevant
(KL) penalty terms as in F. Those components with lower Fs are preferentially split. The
optimisation completes when all existing mixture components have been considered as parents,
with no accepted epochs.
Toward the end of an optimisation, the remaining required changes to model structure are mainly
local in nature and it becomes computationally wasteful to update the parameters of all the com-
ponents of the mixture model at each iteration of the variational optimisation. For this reason
only those components whose responsibilities are in ﬂux (to some threshold) are updated. This
partial optimisation approach still guarantees an increase in F, as we simply perform updates
that guarantee to increase parts of the Fterm in 4.29.
It should be noted that no matter which heuristics are used for birth and death, ultimately the
results are always compared in terms of F, the lower bound on the log marginal likelihood
L. Therefore different choices of heuristic can only affect the efﬁciency of the search over
model structures and not the theoretical validity of the variational approximation. For example,
although it is perfectly possible to start the model with many components and let them die, it
129
VB Mixtures of Factor Analysers 4.4. Handling the predictive density
is computationally more efﬁcient and equally valid to start with one component and allow it to
spawn more when necessary.
4.3.3 Heuristics for the optimisation endgame
In the previous subsection we proposed a heuristic for terminating the optimisation, namely
that every component should be unsuccessfully split a number of times. However, working in
the space of components seems very inefﬁcient. Moreover, there are several pathological birth-
death scenarios which raise problems when counting the number of times each component has
been split; for example, the identities of nearby components can be switched during an epoch
(parent splits into two children, ﬁrst child usurps an existing other component and models its
data, whilst that component switches to model the old parent’s data, and the second child dies).
One possible solution (personal communication, Y . Teh) is based on a responsibility accumula-
tion method. Whenever a component sis chosen for a split, we store its responsibility vector
(of length n) for all the data points q(s) = [ q(s1) q(s2) ... q(sn)], and proceed with the op-
timisation involving its two children. At the end of the epoch, if we have not increased F,
we add q(s) to a running total of ‘split data’ responsibilities, t = ( t1,t2,...,t n). That is
∀i: ti ←min(ti+ q(si),tmax), where tmax is some saturation point. If by the end of the epoch
we have managed to increase F, then the accumulator t is reset to zero for every data point.
From this construction we can derive a stochastic procedure for choosing which component to
split, using the softmax of the quantity c(s) = β∑n
i=1(tmax −ti)q(si). If c(s) is large for some
component s, then the data it is responsible for has not ‘experienced’ many birth attempts, and
so it should be a strong candidate for a split. Here β ≥0 is a temperature parameter to be set
as we wish. As βtends to inﬁnity the choice of component to split becomes deterministic, and
is based on which has least responsibility overlap with already-split data. If β is very small
(but non-zero) the splits become more random. Whatever setting of β, attempted splits will
be automatically focused on those components with more data and unexplored regions of data
space. Furthermore, a termination criterion is automatic: continue splitting components until
every entry of the t vector has reached saturation — this corresponds to splitting every data
point a certain number of times (in terms of its responsibility under the split parent), before we
terminate the entire optimisation. This idea was conceived of only after the experiments were
completed, and so has not been thoroughly investigated.
4.4 Handling the predictive density
In this section we set about trying to get a handle on the predictive density of VBMFA models
using bounds on approximations (in section 4.7.1 we will show how to estimate the density
130
VB Mixtures of Factor Analysers 4.4. Handling the predictive density
using sampling methods). In order to perform density estimation or classiﬁcation of a new test
example, we need to have access to the predictive density
p(y′ |y) = p(y′,y)
p(y) =
∫
dθ p(θ |y)p(y′ |θ) (4.78)
where y′ is a set of test examples y′ = {y′
1,..., y′
n′}, and y is the training data. This quantity
is simply the probability of observing the test examples for a particular setting of the model
parameters, averaged over the posterior distribution of the parameters given a training set. Un-
fortunately, the very intractability of the marginal likelihood in equation ( 4.14) means that the
predictive density is also intractable to compute exactly.
A poor man’s approximation uses the variational posterior distribution in place of the posterior
distribution:
p(y′ |y) ≈
∫
dθ q(θ)p(y′ |θ) . (4.79)
However we might expect this to overestimate the density of y′ in typical regions of space (in
terms of where the training data lie), as the variational posterior tends to over-neglect areas of
low posterior probability in parameter space. This is a result of the asymmetric KL divergence
measure penalty in the optimisation process.
Substituting the form for MFAs given in (4.14) into (4.79)
p(y′ |y) ≈
∫
dπ
∫
d˜Λ q(π,˜Λ)
[n′
∏
i=1
S∑
si=1
p(si|π)p(y′
i|si,˜Λ,Ψ)
]
, (4.80)
131
VB Mixtures of Factor Analysers 4.5. Synthetic experiments
which is still intractable for the same reason that the marginal likelihoods of training set were
so. We can lower bound the log of the predictive density using variational distributions over the
hidden variables corresponding to each test case:
ln p(y′ |y) ≈ln
∫
dπ
∫
d˜Λ q(π,˜Λ)
[n′
∏
i=1
S∑
si=1
p(si|π)p(y′
i|si,˜Λ,Ψ)
]
(4.81)
≥
n′
∑
i=1
∫
dπ
∫
d˜Λ q(π,˜Λ)
[
ln
S∑
si=1
p(si|π)p(y′
i|si,˜Λ,Ψ)
]
(4.82)
=
n′
∑
i=1
∫
dπ q(π)
∫
d˜Λ q(˜Λ)
[
ln
S∑
si=1
q(si)p(si|π)p(y′
i|si,˜Λ,Ψ)
q(si)
]
(4.83)
≥
n′
∑
i=1
∫
dπ q(π)
∫
d˜Λ q(˜Λ)
S∑
si=1
q(si) ln p(si|π)p(y′
i|si,˜Λ,Ψ)
q(si) (4.84)
≥
n′
∑
i=1
S∑
si=1
q(si)
[∫
dπ q(π) ln p(si|π)
q(si) +
∫
dxi q(xi|si) ln p(xi)
q(xi|si)
+
∫
d˜Λsi q(˜Λsi)
∫
dxi q(xi|si) ln p(y′
i|si,xi,˜Λsi,Ψ)
]
. (4.85)
The ﬁrst inequality is a simple Jensen bound, the second is another which introduces a set
of variational distributions q(si), and the third a further set of distributions over the hidden
variables q(xi|si). Note that these distributions correspond to the test data, indexed from i =
1,...,n ′. This estimate of the predictive density is then very similar to the lower bound of
the marginal likelihood of the training data ( 4.29), except that the training data yi has been
replaced with the test data y′
i, and the KL penalty terms on the parameters have been removed.
This carries the interpretation that the distribution over parameters of the model is decided upon
and ﬁxed (i.e. the variational posterior), and we simply need to explain the test data under this
ensemble of models.
This lower bound on the approximation to the predictive density can be optimised in just two
updates for each test point. First, infer the distribution q(xi|si) for each test data point, using
the analogous form of update ( 4.53). Then update the distribution q(si) based on the resulting
distributions over q(xi|si) using the analogous form of update ( 4.57). Since the q(xi|si) up-
date was not a function of q(si), we do not need to iterate the optimisation further to improve
the bound.
4.5 Synthetic experiments
In this section we present three toy experiments on synthetic data which demonstrate certain
features of a Bayesian mixture of factor analysers. The ﬁrst experiment shows the ability of the
132
VB Mixtures of Factor Analysers 4.5. Synthetic experiments
algorithm’s birth and death processes to ﬁnd the number of clusters in a dataset. The second
experiment shows more ambitiously how we can simultaneously recover the number of clusters
and their dimensionalities, and how the complexity of the model depends on the amount of data
support. The last synthetic experiment shows the ability of the model to ﬁt a low dimensional
manifold embedded in three-dimensional space.
4.5.1 Determining the number of components
In this toy example we tested the model on synthetic data generated from a mixture of 18 Gaus-
sians with 50 points per cluster, as shown in ﬁgure 4.3(a). The algorithm was initialised with a
single analyser component positioned at the mean of the data. Birth proposals were made using
spatial splits (as described above). Also shown is the progress of the algorithm after 7, 14, 16
and 22 accepted epochs (ﬁgures 4.3(b)-4.3(e)). The variational algorithm has little difﬁculty
ﬁnding the correct number of components and the birth heuristics are successful at avoiding
local maxima.
After ﬁnding the 18 Gaussians repeated splits are attempted and mostly rejected. Those epochs
that are accepted always involve the birth of a component followed at some point by the death
of another component, such that the number of components remain 18; the increase in Fover
these epochs is extremely small, usually due to the reﬁnement of other components.
4.5.2 Embedded Gaussian clusters
In this experiment we examine the ability of the Bayesian mixture of factor analysers to auto-
matically determine the local dimensionality of high dimensional data. We generated a synthetic
data set consisting of 300 data points drawn from each of 6 Gaussian clusters with intrinsic di-
mensionalities (7 4 3 2 2 1), embedded at random orientations in a 10-dimensional space. The
means of the Gaussians were drawn uniformly under [0,3] in each of the data dimensions, all
Gaussian variances set to 1, and sensor noise of covariance .01 added in each dimension.
A Bayesian MFA was initialised with one mixture component centred about the data mean, and
trained for a total of 200 iterations of variational EM with spatial split heuristics for the birth
proposals. All the analysers were created with a maximum dimensionality of 7. The variational
Bayesian approach correctly inferred both the number of Gaussians and their intrinsic dimen-
sionalities, as shown in ﬁgure 4.4. The dimensionalities were determined by examining the
posterior distributions over the precisions of each factor analyser’s columns, and thresholding
on the mean of each distribution.
We then varied the number of data points in each cluster and trained models on successively
smaller data sets. Table 4.1 shows how the Bayesian MFA partitioned the data set. With large
133
VB Mixtures of Factor Analysers 4.5. Synthetic experiments
(a) The data, consisting of 18 Gaussian clus-
ters.
(b) After 7 accepted epochs.
 (c) After 14 accepted epochs.
(d) After 16 accepted epochs.
 (e) After 22 accepted epochs.
Figure 4.3: The original data, and the conﬁguration of the mixture model at points during the
optimisation process. Plotted are the 2 s.d. covariance ellipsoids for each analyser in the mix-
ture. To be more precise, the centre of the ellipsoid is positioned at the mean of the variational
posterior over the analyser’s centre, and each covariance ellipsoid is the expected covariance
under the variational posterior.
134
VB Mixtures of Factor Analysers 4.5. Synthetic experiments
Figure 4.4: Learning the local intrinsic dimensionality. The maximum dimensionality of each
analyser was set to 7. Shown are Hinton diagrams for the means of the factor loading matrices
{Λ
s
}S
s=1 for each of the 6 components, after training on the data set with 300 data points per
cluster. Note that empty columns correspond to unused factors where the mass of q(νs
l) is at
very high values, so the learnt dimensionalities are (7,2,2,4,3,1).
number of points 
  per cluster		 1	 7	 4	 3	 2	 2

	   8		  	       2			       1
	   8		      1			      2
	  16		 1		       4			 2
	  32		 1	 6	 3	 3	 2	 2
	  64		 1	 7	 4	 3	 2	 2
	 128		 1	 7	 4	 3	 2	 2
intrinsic dimensionalities
Table 4.1: The recovered number of analysers and their intrinsic dimensionalities. The numbers
in the table are the dimensionalities of the analysers and the boxes represent analysers modelling
data from more than one cluster. For a large number of data points per cluster ( ≥64), the
Bayesian MFA recovers the generative model. As we decrease the amount of data, the model
reduces the dimensionality of the analysers and begins to model data from different clusters
with the same analyser. The two entries for 8 data points are two observed conﬁgurations that
the model converged on.
amounts of data the model agrees with the true model, both in the number of analysers and their
dimensionalities. As the number of points per cluster is reduced there is insufﬁcient evidence to
support the full intrinsic dimensionality, and with even less data the number of analysers drop
and they begin to model data from more than one cluster.
4.5.3 Spiral dataset
Here we present a simple synthetic example of how Bayesian MFA can learn locally linear
models to tile a manifold for globally non-linear data. We used the dataset of 800 data points
from a noisy shrinking spiral, as used in Ueda et al. (2000), given by
yi = [(13 −0.5ti) cos ti, −(13 −0.5ti) sin ti, t i)] + wi (4.86)
where ti ∈[0,4π] , wi ∼ N(0,diag ([.5 .5 .5])) (4.87)
135
VB Mixtures of Factor Analysers 4.5. Synthetic experiments
(a) An elevated view of the spiral data set (see
text for reference).
(b) The same data set viewed perpendicular to
the third axis.
Figure 4.5: The spiral data set as used in Ueda et al. (2000). Note that the data lie on a 1-
dimensional manifold embedded non-linearly in the 3-dimensional data space.
where the parameter t determines the point along the spiral in one dimension. The spiral is
shown in ﬁgure 4.5, viewed from two angles. Note the spiral data set is really a 1-dimensional
manifold embedded non-linearly in the 3-dimensional data space and corrupted by noise.
As before we initialised a variational Bayesian MFA model with a single analyser at the mean
of the data, and imposed a maximum dimensionality of k= 2 for each analyser. For this exper-
iment, as for the previous synthetic experiments, the spatial splitting heuristic was used. Again
local maxima did not pose a problem and the algorithm always found between 12-14 Gaussians.
This result was repeatable even when the algorithm was initialised with 200 randomly posi-
tioned analysers. The run starting from a single analyser took about 3-4 minutes on a 500MHz
Alpha EV6 processor. Figure 4.6 shows the state of the algorithm after 6, 9, 12 and 17 accepted
epochs.
Figure 4.7 shows the evolution of the lower bound used to approximate the marginal likelihood
of the data. Thick and thin lines in the plot correspond to accepted and rejected epochs, respec-
tively. There are several interesting aspects one should note. First, at the beginning of most of
the epochs there is a drop in Fcorresponding to a component birth. This is because the model
now has to code the parameters of the new analyser component, and initially the model is not
ﬁt well to the data. Second, most of the compute time is spent on accepted epochs, suggesting
that our heuristics for choosing which components to split, and how to split them, are good.
Referring back to ﬁgure 4.6, it turns out that it is often components that are straddling arms of
the spiral that have lowFs, as given by (4.77), and these are being correctly chosen for splitting
ahead of other components modelling their local data better (for example, those aligned on the
spiral). Third, after about 1300 iterations, most of the proposed changes to model structure are
rejected, and those that are accepted give only a small increase in F.
136
VB Mixtures of Factor Analysers 4.5. Synthetic experiments
(a) After 6 accepted epochs.
 (b) After 9 accepted epochs.
(c) After 12 accepted epochs.
 (d) After 17 accepted epochs.
Figure 4.6: The evolution of the variational Bayesian MFA algorithm over several epochs.
Shown are the 1 s.d. covariance ellipses for each analyser: these are the expected covariances,
since the analysers have distributions over their factor loadings. After 17 accepted epochs the
algorithm has converged to a solution with 14 components in the mixture. Local optima, where
components are straddled across two arms of the spiral (see (b) for example), are successfully
avoided by the algorithm.
0 500 1000 1500 2000
-7800
-7600
-7400
-7200
-7000
-6800
-6600
-6400
Figure 4.7: Evolution of the lower bound F, as a function of iterations of variational Bayesian
EM, for the spiral problem on a typical run. Drops in Fconstitute component births. The thick
and thin lines represent whole epochs in which a change to model structure was proposed and
then eventually accepted or rejected, respectively.
137
VB Mixtures of Factor Analysers 4.6. Digit experiments
Figure 4.8: Some examples of the digits 0-9 in the training and test data sets. Each digit is8 ×8
pixels with gray scale 0 to 255. This data set was normalised before passing to VBMFA for
training.
4.6 Digit experiments
In this section we present results of using variational Bayesian MFA to learn both supervised
and unsupervised models of images of 8 ×8 digits taken from the CEDAR database ( Hull,
1994). This data set was collected from hand-written digits from postal codes, and are labelled
with the classes 0 through to 9. Examples of these digits are given in ﬁgure 4.8. The entire data
set was normalised before being passed to the VBMFA algorithm, by ﬁrst subtracting the mean
image from every example, and then rescaling each individual pixel to have variance 1 across
all the examples. The data set was then partitioned into 700 training and 200 test examples for
each digit. Based on density models learnt from the digits, we can build classiﬁers for a test
data set. Histograms of the pixel intensities after this normalisation are quite non-Gaussian, and
so factor analysis is perhaps not a good model for this data. Before normalising, we could have
considered taking the logarithm or some other non-linear transformation of the intensities to
improve the non-Gaussianity, but this was not done.
4.6.1 Fully-unsupervised learning
A single VBMFA model was trained on 700 examples of every digit 0-9, using birth proposals
and death processes as explained in section4.3. The maximum dimensionality for each analyser
kmax was set to 6, and the number of components initialised to be 1. Responsibility-based splits
were used for the birth proposals (section4.3.2) as we would expect these to perform better than
spatial-splits given the high dimensionality of the data (using the fraction of accepted splits as
a criterion, this was indeed conﬁrmed in preliminary experiments with high dimensional data
sets). The choice of when to ﬁnish an epoch of learning was based on the rate of change of the
138
VB Mixtures of Factor Analysers 4.6. Digit experiments
2 4 3 4 3 3
3 5 5 2 4
1 5 3 4 3 4 3 3 2 2 1 4 5 5 5 4 4 5 5 5
5 4 5 4 5 4 5 4 5
3 4 5 5 4 2 4 5 3 2
2 4 4 5 5 4 3 5
5 5 3 4 4 4 4 4 4
3 5 5 3 4 3 4 3
4 4 5 3 4 5 5 4 4
4 5 3 5 3 5 4 3
Figure 4.9: A typical model learnt by fully-unsupervised VBMFA using the birth and death
processes. Each digit shown represents an analyser in the mixture, and the pixel intensities
are the means of the posterior distribution over the centre of the analyser, ⟨µs⟩q(µs). These
means can be thought of as templates. These intensities have been inversely-processed to show
pixel intensities with the same scalings as the training data. The number to the right of each
image is that analyser’s dimensionality. In this experiment the maximum dimensionality of the
latent space was set to kmax = 6 . As can be seen from these numbers, the highest required
dimensionality was 5. The within-row ordering indicates the creation order of the analysers
during learning, and we have arranged the templates across different rows according to the 10
different digits in 4.8. This was done by performing a sort of higher-level clustering which
the unsupervised algorithm cannot in fact do. Even though the algorithm itself was not given
the labels of the data, we as experimenters can examine the posterior responsibilities of each
analyser for every item in the training set (whose labels we have access to), and ﬁnd the majority
class for that analyser, and then assign that analyser to the row corresponding to the class label.
This is purely a visual aid — in practice if the data is not labelled we have no choice but to call
each mixture component in the model a separate class, and have the mean of each analyser as
the class template.
component posterior responsibilities (section 4.3.2). The optimisation was terminated when no
further changes to model structure managed to increase F(based on three unsuccessful splits
for every component in the model).
Figure 4.9 shows the ﬁnal model returned from the optimisation. In this ﬁgure, each row cor-
responds to a different digit, and each digit image in the row corresponds to the mean of the
posterior over the centre position of each factor analyser component of the mixture. We refer to
these as ‘templates’ because they represent the mean of clusters of similar examples of the same
digit. The number to the right of each template is the dimensionality of the analyser, determined
from examining the posterior over the precisions governing that factor loading matrix’s columns
q(νs) = [ q(νs
1),...,q (νs
kmax )].
For some digits the VBMFA needs to use more templates than others. These templates represent
distinctively different styles for the same digit. For example, some 1’s are written slanting to the
left and others to the right, or the digit 2 may or may not contain a loop. These different styles
are in very different areas of the high dimensional data space; so each template explains all the
139
VB Mixtures of Factor Analysers 4.6. Digit experiments
0 0
0 0
687 196
1 1
7 1
2 2
. .
3 3
2 .
4 4
. .
5 5
1 1
6 6
3 2
7 7
. .
8 8
. .
9 9
. .
1 1
0 0
. .
1 1
699 200
2 2
. .
3 3
. .
4 4
. .
5 5
1 .
6 6
. .
7 7
. .
8 8
. .
9 9
. .
2 2
0 0
1 .
1 1
7 1
2 2
671 186
3 3
1 1
4 4
2 1
5 5
. 1
6 6
7 2
7 7
4 1
8 8
7 6
9 9
. 1
3 3
0 0
. .
1 1
3 2
2 2
7 1
3 3
629 181
4 4
. .
5 5
27 10
6 6
1 .
7 7
2 .
8 8
30 5
9 9
1 1
4 4
0 0
. .
1 1
3 1
2 2
4 .
3 3
. .
4 4
609 175
5 5
. .
6 6
3 .
7 7
14 1
8 8
1 .
9 9
66 23
5 5
0 0
3 1
1 1
7 2
2 2
5 .
3 3
46 13
4 4
. .
5 5
618 180
6 6
5 .
7 7
. 1
8 8
16 2
9 9
. 1
6 6
0 0
. .
1 1
3 1
2 2
. .
3 3
. .
4 4
32 5
5 5
1 1
6 6
664 193
7 7
. .
8 8
. .
9 9
. .
7 7
0 0
. .
1 1
2 2
2 2
3 1
3 3
1 .
4 4
3 .
5 5
. .
6 6
. .
7 7
589 176
8 8
2 .
9 9
100 21
8 8
0 0
1 .
1 1
14 1
2 2
1 .
3 3
27 5
4 4
2 .
5 5
43 9
6 6
1 .
7 7
4 1
8 8
603 179
9 9
4 5
9 9
0 0
. .
1 1
4 4
2 2
1 .
3 3
. .
4 4
13 3
5 5
. .
6 6
. .
7 7
65 17
8 8
3 .
9 9
614 176
Training data Test data
True
True
Classified Classified
Figure 4.10: Confusion tables for digit classiﬁcation on the training (700) and test (200) sets.
The mixture of factor analysers with 92 components obtains 8.8% and 7.9% training and test
classiﬁcation errors respectively.
examples of that style that can be modelled with a linear transformation of the pixel intensities.
The number of dimensions of each analyser component for each digit template corresponds very
roughly to the number of degrees of freedom there are for that template, and the degree with
which each template’s factor analyser’s linear transformation can extrapolate to the data between
the different templates. By using a few linear operations on the pixel intensities of the template
image, the analyser can mimic small amounts of shear, rotation, scaling, and translation, and so
can capture the main trends in its local data.
When presented with a test example digit from 0-9, we can classify it by asking the model which
analyser has the highest posterior responsibility for the test example (i.e. a hard assignment), and
then ﬁnding which digit class that analyser is clustered into (see discussion above). The result
of classifying the training and test data sets are shown in ﬁgure 4.10, in confusion matrix form.
Each row corresponds to the true class labelling of the digit, and each column corresponds to the
digit cluster that the example was assigned to, via the most-responsible analyser in the trained
VBMFA model. We see that, for example, about1/7 of the training data 8’s are misclassiﬁed as
a variety of classes, and about 1/7 of the training data 7’s are misclassiﬁed as 9’s (although the
converse result is not as poor). These trends are also seen in the classiﬁcations of the test data.
The overall classiﬁcation performance of the model was 91.2% and 92.1% for the training and
test sets respectively. This can be compared to simple K-means (using an isotropic distance
measure on the identically pre-processed data), with the number of clusters set to the same as
inferred in the VBMFA optimisation. The result is that K-means achieves only 87.8% and
86.7% accuracy respectively, despite being initialised with part of the VB solution.
140
VB Mixtures of Factor Analysers 4.6. Digit experiments
Computation time
The full optimisation for the VBMFA model trained on all 7000 64-dimensional digit examples
took approximately 4 CPU days on a Pentium III 500 MHz laptop computer. We would expect
the optimisation to take considerably less time if any of the following heuristics were employed.
First, one could use partial VBEM updates for Fto update the parameter distributions of only
those components that are currently in ﬂux; this corresponds to assuming that changing the
modelling conﬁguration of a few analysers in one part of the data space often does not affect the
parameter distributions of the overwhelming majority of remaining analysers. In fact, partial
updates can be derived that are guaranteed to increase F, simply by placing constraints on the
posterior responsibilities of the ﬁxed analysers. Second, the time for each iteration of VBEM can
be reduced signiﬁcantly by removing factors that have been made extinct by the ARD priors; this
can even be done prematurely if it increasesF. In the implementation used for these experiments
all analysers always held factor loading matrix sizes of(p×kmax), despite many of them having
far fewer active factors.
4.6.2 Classiﬁcation performance of BIC and VB models
In these experiments VBMFA was compared to a BIC-penalised maximum likelihood MFA
model, in a digit classiﬁcation task. Each algorithm learnt separate models for each of the
digits 0-9, and attempted to classify a data set of test examples based on the predictive densities
under each of the learnt digit models. For the VB model, computing the predictive density is
intractable (see section 4.4) and so an approximation is required. The experiment was carried
out for 7 different training data set sizes ranging from(100,200,... 700), and repeated 10 times
with different parameter initialisations and random subsets of the full 700 images for each digit.
The maximum dimensionality of any analyser component for BIC or VB was set to kmax = 5 .
This corresponds to the maximum dimensionality required by the fully-unsupervised VB model
in the previous section’s experiments. For the BIC MFA implementation there is no mechanism
to prune the factors from the analysers, so all 5 dimensions in each BIC analyser are used all the
time.
The same heuristics were used for model search in both types of model, as described in section
4.3. In order to compute a component split ordering, the ML method used the empirical KL
divergence to measure the quality of each analyser’s ﬁt to its local data (see Ueda et al., 2000,
for details). The criterion for ending any particular epoch was again based on the rate of change
of component posterior responsibilities. The termination criterion for both algorithms was, as
before, three unsuccessful splits of every mixture component in a row. For the ML model,
a constraint had to be placed on the Ψ matrix, allowing a minimum variance of 10−5 in any
direction in the normalised space in which the data has identity covariance. This constraint was
141
VB Mixtures of Factor Analysers 4.6. Digit experiments
% correct test classiﬁcations
n BIC MLMFA VBMFA
100 88.8 ±.3 89.3 ±.5
200 90.6 ±.4 91.9 ±.3
300 91.1 ±.3 92.7 ±.2
400 91.6 ±.3 92.8 ±.2
500 92.2 ±.3 92.9 ±.2
600 93.0 ±.2 93.3 ±.1
700 93.2 ±.2 93.4 ±.2
Table 4.2: Test classiﬁcation performance of BIC ML and VB mixture models with increasing
data. The standard errors are derived from 10 repetitions of learning with randomly selected
training subsets.
introduced to prevent the data likelihood from diverging as a result of the covariance collapsing
to zero about any data points.
For the BIC-penalised likelihood, the approximation to the marginal likelihood is given by
ln p(y) ≈ln p(y |θML) −D
2 ln n (4.88)
where nis the number of training data (which varied from 100 to 700), and Dis the number of
degrees of freedom in an MFA model withSanalysers with dimensionalities {ks}S
s=1 (see d(k)
of equation (4.5)), which we approximate by
D= S−1 + p+
S∑
s=1
[
p+ pks −1
2ks(ks −1)
]
. (4.89)
This quantity is derived from: S −1 degrees of freedom in the prior mixture proportions π,
the number of parameters in the output noise covariance (constrained to be diagonal),p, and the
degrees of freedom in the mean and factor loadings of each analyser component. Note thatDis
only an approximation to the number of degrees of freedom, as discussed in section 4.1.1.
The results of classiﬁcation experiments for BIC ML and VB are given in table4.2. VB consis-
tently and signiﬁcantly outperforms BIC, and in fact surpasses the 92.1% test error performance
of the fully-unsupervised VB model on 700 training points. The latter comment is not surpris-
ing given that this algorithm receives labelled data. We should note that neither method comes
close to state-of-the-art discriminative methods such as support vector machines and convolu-
tional networks, for example LeNet (LeCun and Bengio, 1995). This may indicate limitations
of the mixture of factor analysers as a generative model for digits.
Figure 4.11 displays the constituents of the mixture models for both BIC and VB for train-
ing set sizes {100,200,. . . ,700}. On average, BIC ML tends to use models with slightly more
components than does VB, which does not coincide with the common observation that the BIC
142
VB Mixtures of Factor Analysers 4.6. Digit experiments
100 200 300 400 500 600 7000
10
20
30
40
50
60
70
(a) BIC.
100 200 300 400 500 600 7000
10
20
30
40
50
60
70 (b) VB.
Figure 4.11: The average number of components used for each digit class by the(a) BIC and (b)
VB models, as the size of the training set increases from 100 to 700 examples. As a visual aid,
alternate digits are shaded black and white. The white bottom-most block in each column corre-
sponds to the ‘0’ digit and the black top-most block to the ‘9’ digit. Note that BIC consistently
returns a greater total number of components than VB (see text).
penalty over-penalises model complexity. Moreover, BIC produces models with a dispropor-
tionate number of components for the ‘1’ digit. VB also does this, but not nearly to the same
extent. There may be several reasons for these results, listed brieﬂy below.
First, it may be that the criterion used for terminating the epoch is not operating in the same
manner in the VB optimisation as in the ML case — if the ML criterion is ending epochs too
early this could easily result in the ML model carrying over some of that epoch’s un-plateaued
optimisation into the next epoch, to artiﬁcially improve the penalised likelihood of the next
more complicated model. An extreme case of this problem is the epoch-ending criterion that
says “end this epoch just as soon as the penalised likelihood reaches what it was before we
added the last component”. In this case we are performing a purely exploratory search, as
opposed to an exploitative search which plateaus before moving on. Second, the ML model
may be concentrating analysers on single data points, despite our precision limit on the noise
model. Third, there is no mechanism for component death in the ML MFA model, since in these
experiments we did not intervene at any stage to test whether the removal of low responsibility
components improved the penalised likelihood (see section 4.3.1). It would be interesting to
include such tests, for both ML MFA and VB methods.
143
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
4.7 Combining VB approximations with Monte Carlo
In this and other chapters, we have assumed that the variational lower bound is a reliable guide to
the log marginal likelihood, using it to infer hidden states, to learn distributions over parameters
and especially in this chapter to guide a search amongst models of differing complexity. We
have not yet addressed the question of how reliable the bounds are. For example, in section
2.3.2 we mentioned that by using Ffor model selection we are implicitly assuming that the
KL divergences between the variational and exact posterior distributions over parameters and
hidden variables are constant between models. It turns out that we can use the technique of
importance sampling to obtain consistent estimators of several interesting quantities, including
this KL divergence. In this technique the variational posterior can be used as an importance
distribution from which to sample points, as it has been optimised to be representative of the
exact posterior distribution.
This section builds on basic claims ﬁrst presented in Ghahramani and Beal (2000). There it
was noted that importance sampling can easily fail for poor choices of importance distributions
(personal communication with D. MacKay, see also Miskin, 2000, chapter 4). We also present
some extensions to simple importance sampling, including using mixture distributions from
several runs of VBEM, and also using heavy-tailed distributions derived from the variational
posteriors.
4.7.1 Importance sampling with the variational approximation
Section 4.4 furnishes us with an estimate of the predictive density. Unfortunately this does not
even constitute a bound on the predictive density, but a bound on an approximation to it. How-
ever it is possible to approximate the integrals for such quantities bysampling. In this subsection
we show how by importance sampling from the variational approximation we can obtain estima-
tors of three important quantities: the exact predictive density, the exact log marginal likelihood
L, and the KL divergence between the variational posterior and the exact posterior.
The expectation εof a function f(θ) under the posterior distribution p(θ |y) can be written as
ε=
∫
dθ p(θ |y) f(θ) . (4.90)
Given that such integrals are usually analytically intractable, they can be approximated by the
Monte Carlo average:
ˆε(M) ≃ 1
M
M∑
m=1
f(θ(m)) , θ(m) ∼ p(θ |y) . (4.91)
144
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
where θ(m) are random draws from the posterior p(θ |y). In the limit of large number of
samples M, ˆεconverges to ε:
lim
M→∞
ˆε(M) = ε. (4.92)
In many models it is not possible to sample directly from the posterior, and so a Markov Chain
Monte Carlo approach is usually taken to help explore regions of high posterior probability.
In most applications this involves designing tailored Metropolis-Hastings acceptance rules for
moving about in the space whilst still maintaining detailed balance.
An alternative to ﬁnding samples using MCMC methods is to use importance sampling. In this
method we express the integral as an expectation over an importance distribution g(θ):
ε=
∫
dθ p(θ |y) f(θ) (4.93)
=
∫
dθ g(θ) p(θ |y)
g(θ) f(θ) (4.94)
ˆε(M) ≃ 1
M
M∑
m=1
p(θ(m) |y)
g(θ(m))
f(θ(m)) , θ(m) ∼ g(θ) , (4.95)
so that now the Monte Carlo estimate (4.95) is taken using samples drawn fromg(θ). Weighting
factors are required to account for each sample fromg(θ) over- or under-representing the actual
density we wish to take the expectation under. These are called the importance weights
ω(m) = 1
M
p(θ |y)
g(θ) . (4.96)
This discretisation of the integral then deﬁnes a weighted sum of densities:
ˆε(M) =
M∑
m=1
ω(m)f(θ(m)) . (4.97)
Again, if g(θ) is non-zero wherever p(θ |y) is non-zero, it can be shown that ˆεconverges to ε
in the limit of large M.
Having used the VBEM algorithm to ﬁnd a lower bound on the marginal likelihood, we have at
our disposal the resulting variational approximate posterior distribution q(θ). Whilst this distri-
bution is not equal to the posterior, it should be a good candidate for an importance distribution
because it contains valuable information about the shape and location of the exact posterior, as
it was chosen to minimise the KL divergence between it and the exact posterior (setting aside
local optima concerns). In addition it usually has a very simple form and so can be sampled
from easily. We now describe several quantities that can be estimated with importance sampling
using the variational posterior.
145
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
Exact predictive density
An asymptotically exact predictive distribution p(y′ |y) is that given by a weighted average of
the likelihood under a set of parameters drawn from the variational posterior q(θ),
p(y′ |y) =
∫
dθ p(θ |y) p(y′ |θ) (4.98)
=
∫
dθ q(θ) p(θ |y)
q(θ) p(y′ |θ)
/ ∫
dθ q(θ)p(θ |y)
q(θ) (4.99)
≃ 1
M
M∑
m=1
p(θ(m) |y)
q(θ(m))
p(y′ |θ(m))
/ 1
M
M∑
o=1
p(θ(o) |y)
q(θ(o))
(4.100)
=
M∑
m=1
ω(m) p(y′ |θ(m)) , (4.101)
where θ(m) ∼ q(θ) are samples from the variational posterior, and the ωm are given by
ω(m) = p(θ(m) |y)
q(θ(m))
/ M∑
o=1
p(θ(o) |y)
q(θ(o))
(4.102)
= p(θ(m),y)
q(θ(m))
/ M∑
o=1
p(θ(o),y)
q(θ(o))
(4.103)
= 1
Zω
p(θ(m),y)
q(θ(m))
, (4.104)
and Zω is deﬁned as
Zω =
M∑
m=1
p(θ(m),y)
q(θ(m))
. (4.105)
In the case of MFAs, each such sampleθ(m) is an instance of a mixture of factor analysers with
predictive density p(y′ |θ(m)) as given by (4.11). Since the ω(m) are normalised to sum to 1, the
predictive density for MFAs given in (4.101) represents a mixture of mixture of factor analysers.
Note that the step from (4.102) to (4.103) is important because we cannot evaluate the exact pos-
terior density p(θ(m) |y), but we can evaluate thejoint density p(θ(m),y) = p(θ(m))p(y |θ(m)).
Furthermore, note that Zω is a function of the weights, and so the estimator in equation (4.101)
is really a ratio of Monte Carlo estimates. This means that the estimate forp(y′ |y) is no longer
guaranteed to be unbiased. It is however a consistent estimator (provided the variances of the
numerator and denominator are converging) meaning that as the number of samples tends to
inﬁnity its expectation will tend to the exact predictive density.
146
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
Exact marginal likelihood
The exact marginal likelihood can be written as
ln p(y) = ln
(∫
dθ q(θ)p(θ,y)
q(θ)
)
(4.106)
= ln ⟨ω⟩q(θ) + ln Zω (4.107)
where ⟨·⟩denotes averaging with respect to the distribution q(θ). This gives us an unbiased
estimate of the marginal likelihood, but a biased estimate of the log marginal likelihood. Both
estimators are consistent however.
KL divergence
This measure of the quality of the variational approximation can be derived by writing Fin the
two ways
F=
∫
dθ q(θ) ln p(θ,y)
q(θ) (4.108)
= ⟨ln ω⟩q(θ) + ln Zω, or (4.109)
F=
∫
dθ q(θ) ln p(θ |y)
q(θ) + ln p(y) (4.110)
= −KL(q(θ)∥p(θ |y)) + ln ⟨ω⟩q(θ) + ln Zω. (4.111)
By equating these two expressions we obtain a measure of the divergence between the approxi-
mating and exact parameter posteriors,
KL(q(θ)∥p(θ |y)) = ln ⟨ω⟩q(θ) −⟨ln ω⟩q(θ) (4.112)
Note that this quantity is not a function of Zω, since it was absorbed into the difference of two
logarithms. This means that we need not use normalised weights for this measure, and base the
importance weights on p(θ,y) rather than p(θ |y), and the estimator is unbiased.
Three signiﬁcant observations should be noted. First, the same importance weights can be used
to estimate all three quantities. Second, while importance sampling can work very poorly in
high dimensions for ad hoc proposal distributions, here the variational optimisation is used in
a principled manner to provide a q(θ) that is a good approximation to p(θ |y), and therefore
hopefully a good proposal distribution. Third, this procedure can be applied to any variational
approximation.
147
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
4.7.2 Example: Tightness of the lower bound for MFAs
In this subsection we use importance sampling to estimate the tightness of the lower bound in
a digits learning problem. In the context of a mixture of factor analysers, θ = ( π,Λ,µ) =
{πs,˜Λs}S
s=1, and we sample θ(m) ∼ q(θ) = q(π)q(˜Λ). Each such sample is an instance of
a mixture of factor analysers with predictive density given by equation ( 4.11). Note that Ψ is
treated as a hyperparameter so need not be sampled (although we could envisage doing so if
we were integrating over Ψ). We weight these predictive densities by the importance weights
w(m) = p(θ(m),y)/q(θ(m)), which are easy to evaluate. When sampling the parameters θ,
one needs only to sample π vectors and ˜Λ matrices, as these are the only parameters that are
required to replicate the generative model of mixture of factor analysers (in addition to the hy-
perparameter Ψ which has no distribution in our model). Thus the numerator in the importance
weights are obtained by calculating
p(θ,y) = p(π,˜Λ)p(y |π,˜Λ) (4.113)
= p(π |α∗,m∗)
∫
dν p(˜Λ |ν,µ∗,ν∗)p(ν |a∗,b∗)
n∏
i=1
p(yi|π,˜Λ) (4.114)
= p(π |α∗,m∗)p(˜Λ |a∗,b∗,µ∗,ν∗)
n∏
i=1
p(yi|π,˜Λ) . (4.115)
On the second line we express the prior over the factor loading matrices as a hierarchical prior
involving the precisions {νs}S
s=1. It is not difﬁcult to show that marginalising out the precision
for a Gaussian variable yields a multivariate Student-t prior distribution for each row of each
˜Λs, from which we can sample directly. Substituting in the density for an MFA given in ( 4.11)
results in:
p(θ,y) = p(π |α∗,m∗)p(˜Λ |a∗,b∗,µ∗,ν∗)
n∏
i=1
[ S∑
si=1
p(si|π)p(yi|si,˜Λ,Ψ)
]
. (4.116)
The importance weights are then obtained after evaluating the density under the variational dis-
tribution q(π)q(˜Λ), which is simple to calculate. Even though we require all the training data to
generate the importance weights, once these are made, the importance weights {ω(m)}M
m=1 and
their locations {π(m),˜Λ(m)}M
m=1 then capture all the information about the posterior distribution
that we will need to make predictions, and so we can discard the training data.
A training data set consisting of 700 examples of each of the digits 0, 1, and 2 was used to train
a VBMFA model in a fully-unsupervised fashion. After every successful epoch, the variational
posterior distributions over the parameters˜Λ and π were recorded. These were then used off-line
to produce M = 100 importance samples from which a set of importance weights {ω(m)}M
m=1
were calculated. Using results of the previous section, these weights were used to estimate the
following quantities: the log marginal likelihood, the KL divergence between the variational
148
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
posterior q(π)q(˜Λ) and the exact posterior p(π,˜Λ |y), and the KL divergence between the full
variational posterior over all hidden variables and parameters and the exact full posterior. The
latter quantity is simply the difference between the estimate of the log marginal likelihood and
the lower bound Fused in the optimisation (see equation (4.29)).
Figure 4.12(a) shows these results plotted alongside the training and test classiﬁcation errors.
We can see that for the most part the lower bound, calculated during the optimisation and de-
noted F(π,ν,˜Λ,x,s) to indicate that it is computed from variational distributions over param-
eters and hidden variables, is close to the estimate of the log marginal likelihood ln p(y), and
more importantly remains roughly in tandem with it throughout the optimisation. The training
and test errors are roughly equal and move together, suggesting that the variational Bayesian
model is not overﬁtting the data. Furthermore, upward changes to the log marginal likelihood
are for the most part accompanied by downward changes to the test error rate, suggesting that the
marginal likelihood is a good measure for classiﬁcation performance in this scenario. Lastly, the
estimate of the lower bound F(π,˜Λ), which is computed by inserting the importance weights
into (4.109), is very close to the estimate of the log marginal likelihood (the difference is made
more clear in the accompanying ﬁgure4.12(b)). This means that the KL divergence between the
variational and exact posteriors over (π,˜Λ) is fairly small, suggesting that the majority of the
gap between ln p(y) and F(π,ν,˜Λ,x,s) is due to the KL divergence between the variational
posterior and exact posteriors over the hidden variables (ν,x,s).
Aside: efﬁciency of the structure search
During the optimisation, there were 52 accepted epochs, and a total of 692 proposed component
splits (an acceptance rate of only about 7%), resulting in 36 components. However it is clear
from the graph (see also ﬁgure 4.13(c)) that the model structure does not change appreciably af-
ter about 5000 iterations, at which point 41 epochs have been accepted from 286 proposals. This
corresponds to an acceptance rate of 14% which suggests that our heuristics for choosing which
component to split and how to split it are performing well, given the number of components to
chose from and the dimensionality of the data space.
Analysis of the lower bound gap
Given that 100 samples may be too few to obtain reliable estimates, the experiment was repeated
with 6 runs of importance sampling, each with 100 samples as before. Figures 4.13(a) and
4.13(b) show the KL divergence measuring the distance between the log marginal likelihood
estimate and the lower bounds F(π,ν,˜Λ,x,s) and F(π,˜Λ), respectively, as the optimisation
proceeds. Figure 4.13(c) plots the number of components, S, in the mixture with iterations of
EM, and it is quite clear that the KL divergences in the previous two graphs correlate closely
149
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
0 1000 2000 3000 4000 5000 6000 7000 8000−1.35
−1.3
−1.25
−1.2
−1.15
−1.1 x 10
5
ln p(y)
F(p,L)
F(p,n,L,{x,s})
0 1000 2000 3000 4000 5000 6000 7000 80000
0.05
0.1
0.15
0.2
0.25
0.3
fraction classification error
train error
test error
(a)
3000 4000 5000 6000 7000 8000−1.145
−1.14
−1.135
x 10
5
ln p(y)
F(p,L)
F(p,n,L,{x,s})
(b)
Figure 4.12: (a) Log marginal likelihood estimates from importance sampling with iterations
of VBEM. Each point corresponds to the model at the end of a successful epoch of learning.
The fraction of training and test classiﬁcation errors are shown on the right vertical axis, and the
lower bound F(π,ν,˜Λ,x,s) that guides the optimisation on the left vertical axis. Also plotted
is F(π,˜Λ), but this is indistinguishable from the other lower bound. The second plot (b) is
exactly the same as (a) except the log marginal likelihood axis has been rescaled to make clear
the difference between the log marginal likelihood and the bound F(π,˜Λ).
150
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
with the number of components. This observation is borne out explicitly in ﬁgures 4.13(d) and
4.13(d) where it is clear that the KL divergence between the lower bound F(π,ν,˜Λ,x,s) and
the marginal likelihood is roughly proportional to the number of components in the mixture.
This is true to an extent also for the lower bound estimate F(π,˜Λ) although this quantity is
more noisy. These two observations are unlikely to be artifacts of the sampling process, as the
variances are much smaller than the trend. In section 2.3.2 we noted that if the KL discrepancy
increases with S then the model exploration may be biased to simpler models. Here we have
found some evidence of this, which suggests that variational Bayesian methods may suffer from
a tendency to underﬁt the model structure.
4.7.3 Extending simple importance sampling
Why importance sampling is dangerous
Unfortunately, the importance sampling procedure that we have used is notoriously bad in high
dimensions. Moreover, it is easy to show that importance sampling can fail even for just one
dimension: consider computing expectations under a one dimensional Gaussian p(θ) with pre-
cision νp using an importance distribution q(θ) which is also a Gaussian with precision νq and
the same mean. Although importance sampling can give us unbiased estimates, it is simple to
show that if νq > 2νp then the variance of the importance weights will be inﬁnite! We brieﬂy
derive this result here. The importance weight for the sample drawn from q(θ) is given by
ω(θ) = p(θ)
q(θ) , (4.117)
and the variance of the importance weights can be written
var(ω) = ⟨ω2⟩q(θ) −⟨ω⟩2
q(θ) (4.118)
=
∫
dθq (θ)
(p(θ)
q(θ)
)2
−
(∫
dθq (θ)p(θ)
q(θ)
)2
(4.119)
= νp
ν1/2
q
∫
dθ exp
[
−
(
νp −1
2νq
)
θ2 + kθ+ k′
]
−1 , (4.120)
=



νpν−1/2
q (2νp −νq)−1/2 −1 for 2νp >νq
∞ for 2νp ≤νq
. (4.121)
where kand k′ are constants independent of x. For 2νp ≤νq, the integral diverges and the vari-
ance of the weights is inﬁnite. Indeed this problem is exacerbated in higher dimensions, where if
this condition is not met in any dimension of parameter space, then the importance weights will
have inﬁnite variance. The intuition behind this is that we need the tails of the sampling distribu-
tion q(θ) to fall off slower than the true distributionp(θ), otherwise there exists some probability
151
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
0 1000 2000 3000 4000 5000 6000 7000 80000
100
200
300
400
500
600
700
800
(a)
0 1000 2000 3000 4000 5000 6000 7000 80000
10
20
30
40
50
60
70
80
90 (b)
0 1000 2000 3000 4000 5000 6000 7000 80000
5
10
15
20
25
30
35
(c)
0 5 10 15 20 25 30 350
100
200
300
400
500
600
700
800
(d)
0 5 10 15 20 25 30 350
10
20
30
40
50
60
70
80
90 (e)
Figure 4.13: At the end of every accepted epoch 6 estimates of the log marginal likelihood were
calculated (see text). (a) Differences between the log marginal likelihood estimate and the lower
bound F(π,ν,˜Λ,x,s), as a function of iterations of VBEM. (b) Differences between the log
marginal likelihood estimate and the lower boundF(π,˜Λ). (c) Number of components Sin the
mixture model with iterations of VBEM.(d) The same data as in (a), plotted against the number
of components S, as given in (c). (e) As for (d) but using the data from (b) instead of (a).
152
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
that we obtain a very high importance weight. This result is clearly a setback for importance
sampling using the variational posterior distribution, since the variational posterior tends to be
tighter than the exact posterior, having neglected correlations between some parameters in or-
der to make inference tractable. To complete the argument, we should mention that importance
sampling becomes very difﬁcult in high dimensions even if this condition is met, since: ﬁrstly,
samples from the typical set of the q(θ) are unlikely to have high probability under p(θ), un-
less the distributions are very similar; secondly, even if the distributions are well matched, the
weights have a wide range that scales order exp(r1/2), where ris the dimensionality (MacKay,
1999).
The above result (4.121) is extended in Miskin (2000, chapter 4), where the ﬁnite variance con-
dition is derived for general p(θ) and q(θ) in the exponential family. Also in that work, a bound
is derived for the variance of the importance weights when using a ﬁnite mixture distribution as
the importance distribution (equation 4.31 of that manuscript). This mixture is made from the
variational posterior distribution mixed with a set of broader distributions from the same expo-
nential family. The rationale for this approach is precisely to create heavier-tailed importance
distributions. Unfortunately the bound is not very tight, and the simulations therein report no
increase in convergence to the correct expectation.
In addition to these problems, the exact posterior over the parameters can be very multi-modal.
The most benign form of such multi-modality is due to aliases arising from having likelihood
functions which are invariant to exchanges of labelling of hidden variables, for example indica-
tor variables for components in a mixture. In such cases the variational posterior tends to lock
on to one mode and so, when used in an importance sampler, the estimate represents only a
fraction of the marginal likelihood. If the modes are well-separated then simple degeneracies of
this sort can be accounted for by multiplying the result by the number of aliases. If the modes
are overlapping, then a correction should not be needed as we expect the importance distribu-
tion to be broad enough. However if the modes are only partially separated then the correction
factor is difﬁcult to compute. In general, these corrections cannot be made precise and should
be avoided.
Using heavy-tailed and mixture distributions
Here we investigate the effect of two modiﬁcations to the naive use of the variational posterior as
importance distribution. The ﬁrst modiﬁcation considers replacing the variational posterior en-
tirely by a related heavy-tailed Student-t distribution. The second modiﬁcation uses a stochastic
mixture distribution for the importance distribution, with each component being the variational
posterior obtained from a different VBEM optimisation.
153
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
The Student-t can be derived by considering the marginal probability of Gaussian distributed
variables under a conjugate gamma distribution for the precision, γ, which is for the univariate
case:
qSt(θ) =
∫
dγ p(γ|a,b)p(θ|µ,γ−1) (4.122)
=
∫
dγ Ga(γ|a,b)N(θ|µ,γ−1) (4.123)
= ba
Γ(a)
√
2π
∫
dγ e−(b+(θ−µ)2/2)γγa−1/2 (4.124)
= 1
ZSt(a,b)
(
1 + (θ−µ)2
2b
)−(a+1/2)
(4.125)
where aand bare the shape and inverse-scale respectively of the precision distribution, and ZSt
is given by
ZSt(a,b) = Γ(a+ 1
2 )
Γ(a)
√
2πb
, for a> 0, b>0 . (4.126)
It is straightforward to show that the variance of θ is given by b/(a−1) and the kurtosis by
3(a−1)/(a−2) (see appendix A). The degrees of freedom νand dispersion parameter σ2 can
be arrived at with the following equivalence:
ν = 2 a, σ 2 = b
a . (4.127)
The attraction of using this distribution for sampling is that it has heavier tails, with a polynomial
rather than exponential decay. In the limit of ν →∞ the Student-t is a Gaussian distribution,
while for ν = 1 it is a Cauchy distribution.
Three 2-dimensional data sets were generated by drawing 150 samples from 4 Gaussian clus-
ters, with varying separations of their centres, as shown in ﬁgure 4.14. For each data set, 10
randomly initialised VBEM algorithms were run to learn a model of the data. If any of the
learnt models contained fewer or more than 4 components, that optimisation was discarded and
replaced with another. We would expect that for the well-separated data set the exact posterior
distribution over the parameters would consist of tight, well-separated modes. Conversely, for
the overlapping data set we would expect the posterior to be very broad consisting of several
weakly-deﬁned peaks. In the intermediately-spaced data set we would expect the posterior to
be mostly separated modes with some overlap.
The following importance samplers were constructed, separately for each data set, and are sum-
marised in table 4.3: (1) a single model out of the 10 that were trained was randomly chosen
(once) and its variational posteriorq(π)q(˜Λ) used as the importance distribution; (2) the covari-
ance parameters of the variational posteriorq(˜Λ) of that same model were used as the covariance
parameters in t-distributions with 3 degrees of freedom to form q(3)(˜Λ), and this used in con-
junction with the same q(π) to form the importance distribution q(π)q(3)(˜Λ); (3) the same as
154
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
sampler type of each component’s
key importance dist. form dof relative variance kurtosis
1 single 1 Gaussian ∞ 1 3
2 single 1 Student-t 3 3 3.75
3 single 1 Student-t 2 ∞ 4.5
4 mixture of 10 Gaussian ∞
5 mixture of 10 Student-t 3 ditto ditto
6 mixture of 10 Student-t 2
Table 4.3: The speciﬁcations of six importance sampling distributions.
(2) but using 2 degrees of freedom; samplers (4,5,6) are the same as (1,2,3) except the opera-
tions are carried out on every one of the 10 models returned, to generate a mixture model with
10 equally weighted mixture components.
Recall that the covariance matrix for the entries of the ˜Λs matrix for each analyser is of block
diagonal form, and so each row can be sampled from independently to produce the importance
samples. Furthermore, generating the multivariate Student-t samples from these covariances is
a straightforward procedure using standard methods.
Figure 4.14 shows the results of attempting to estimate the marginal likelihood of the three
different data sets, using the 6 differently constructed importance samplers given in table 4.3,
which are denoted by the labels 1–6. The axis marks F and F′ correspond to lower bounds on
the log marginal likelihood: F is the lower bound reported by the single model used for the sin-
gle set of importance samplers (i.e. 1,2,3); and F′ is the highest reported lower bound of all 10
of the models trained on that data set. The error bars correspond to the unbiased estimate of the
standard deviation in the estimates from ﬁve separate runs of importance sampling. We can see
several interesting features. First, all the estimates (1-6) using different importance distributions
yield estimates greater than the highest lower bound (F’). Second, the use of heavier-tailed and
broader Student-t distributions for the most part increases the estimate, whether based on single
or mixture importance distributions. Also, the move from 3 to 2 degrees of freedom (i.e. (2) to
(3), or (5) to (6) in the plot) for the most part increases the estimate further. These observations
suggest that there exists mass outside of the variational posterior that is neglected with the Gaus-
sian implementations (1,4). Third, using mixture distributions increases the estimates. However,
this increase from (1,2,3) to (4,5,6) is roughly the same as the increase in lower bounds from F
to F′. This implies that the single estimates are affected if using a sub-optimal solution, whereas
the mixture distribution can perform approximately as well as its best constituent solution. It
should be noted that only the highest lower bound, F′, was plotted for each data set, as plotting
the remaining 9 lower bounds would have extended the graphs’ y-axes too much to be able to
visually resolve the differences in the methods (in all three data sets there were at least two poor
optimisations).
155
VB Mixtures of Factor Analysers 4.7. Combining VB approximations with Monte Carlo
F 1 2 3 F’ 4 5 6−1665
−1660
−1655
−1650
−1645
F 1 2 3 F’ 4 5 6
−1440
−1435
−1430
F 1 2 3 F’ 4 5 6−1160
−1155
−1150
−1145
−2 0 2
−2
0
2
−2 0 2
−2
0
2
−2 0 2
−2
0
2
Figure 4.14: (right) Importance sampling estimates of the marginal likelihoods of VBMFA
models trained on (left) three data sets of differently spaced Gaussian clusters. In the plots in the
right column, the vertical axis is the log of the marginal likelihood estimate, and the horizontal
axis denotes which importance sampling method is used for the estimate, as given in table 4.3.
The estimates are taken from ﬁve separate runs of importance sampling, with each run consisting
of 4000 samples; the error bars are the standard errors in the estimate, assuming the logarithm
of the estimates from the ﬁve runs are Gaussian distributed. The axis markF corresponds to the
lower bound from the model used for the single samplers (1,2,3), and the mark F′ corresponds
to the highest lower bound from the 10 models used in the mixture samplers (4,5,6).
156
VB Mixtures of Factor Analysers 4.8. Summary
4.8 Summary
In this chapter we have shown that how the marginal likelihood of a mixture of factor analysers is
intractable, and derived a tractable deterministic variational lower bound which can be optimised
using a variational EM algorithm. We can use the lower bound to guide a search among model
structures using birth and death moves. We can also use the lower bound to obtain a distribution
over structures if desired:p(m|y) ∝p(m)p(y |m) ≈p(m)·eFopt(m), with the caveat that there
is no guarantee that the best achieved lower bound, Fopt(m), is similarly tight across different
models m. Indeed we have found that the KL divergence between the variational and exact
posterior over parameters increases approximately linearly with the number of components in
the mixture, which suggests a systematic tendency to underﬁt (refer to page 60).
We have derived a generally applicable importance sampler based on the variational solution,
which gives us consistent estimates of the exact marginal likelihood, the exact predictive den-
sity, and the KL divergence between the variational posterior and the exact posterior. We have
also investigated the use of heavy-tailed and mixture distributions for improving the importance
sampler estimates, but there are theoretical reasons for why methods more sophisticated than
importance sampling are required for reliable estimates.
It is also possible to integrate the variational optimisation into the proposal distribution for an
MCMC sampling method (NIPS workshop: Advanced Mean Field Methods , Denver CO, De-
cember 1999; personal communication with N. de Freitas, July 2000). The combined procedures
combine the relative advantages of the two methods, namely the asymptotic correctness of sam-
pling, and the rapid and deterministic convergence of variational methods. Since the variational
optimisation can quickly provide us with an approximation to the shape of the local posterior
landscape, the MCMC transition kernel can be adapted to utilise this information to more ac-
curately explore and update that approximation. One would hope that this reﬁned knowledge
could then be used to update the variational posterior, and the process iterated. Unfortunately,
in its simplest form, this MCMC adaption can not be done inﬁnitely often, as it disrupts the sta-
tionary distribution of the chain (although see Gilks et al., 1998, for a regeneration technique).
In de Freitas et al. (2001), a variational MCMC method that includes mixture transition kernels
is described and applied to the task of ﬁnding the moments of posterior distributions in a sig-
moid belief network. There remain plenty of directions of research for such combinations of
variational and MCMC methods.
The VB mixtures formalism has been applied to more complicated variants of MFA models re-
cently, with a view to determining the number of components and the local manifold dimension-
alities. For example, mixtures of independent components analysers ( Choudrey and Roberts ,
2002), and mixtures of independent components analysers with non-symmetric sources ( Chan
et al., 2002).
157
VB Mixtures of Factor Analysers 4.8. Summary
There have been other Bayesian approaches to modelling densities using mixture distributions.
One notable example is the inﬁnite Gaussian mixture model of Rasmussen (2000), which uses
sampling to entertain a countably inﬁnite number of mixture components, rather than any par-
ticular ﬁnite number. In that work, when training on the Spiral data set (examined in section
4.5.3 of this thesis), it was found that on average about 18–20 of the inﬁnitely many Gaussian
components had data associated with them. Our VB method usually found between 12–14 anal-
yser components. Examining the differences between the models returned, and perhaps more
importantly the predictions made, by these two algorithms is an interesting direction of research.
Search over model structures for MFAs is computationally intractable if each factor analyser
is allowed to have different intrinsic dimensionalities. In this chapter we have shown how the
variational Bayesian approach can be used to efﬁciently infer the structure of the model whilst
avoiding overﬁtting and other deﬁciencies of ML approaches. We have also shown how we can
simultaneously infer both the number of analysers and their dimensionalities using birth-death
steps and ARD methods, all based on a variational lower bound on the marginal likelihood.
158
Chapter 5
Variational Bayesian Linear
Dynamical Systems
5.1 Introduction
This chapter is concerned with the variational Bayesian treatment of Linear Dynamical Systems
(LDSs), also known as linear-Gaussian state-space models (SSMs). These models are widely
used in the ﬁelds of signal ﬁltering, prediction and control, because: (1) many systems of inter-
est can be approximated using linear systems, (2) linear systems are much easier to analyse than
nonlinear systems, and (3) linear systems can be estimated from data efﬁciently. State-space
models assume that the observed time series data was generated from an underlying sequence
of unobserved (hidden) variables that evolve with Markovian dynamics across successive time
steps. The ﬁltering task attempts to infer the likely values of the hidden variables that generated
the current observation, given a sequence of observations up to and including the current obser-
vation; the prediction task tries to simulate the unobserved dynamics one or many steps into the
future to predict a future observation.
The task of deciding upon a suitable dimension for the hidden state space remains a difﬁcult
problem. Traditional methods, such as early stopping, attempt to reduce generalisation error
by terminating the learning algorithm when the error as measured on a hold-out set begins to
increase. However the hold-out set error is a noisy quantity and for a reliable measure a large
set of data is needed. We would prefer to learn from all the available data, in order to make
predictions. We also want to be able to obtain posterior distributions over all the parameters in
the model in order to quantify our uncertainty.
We have already shown in chapter 4 that we can infer the dimensionality of the hidden variable
space (i.e. the number of factors) in a mixture of factor analysers model, by placing priors on
159
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
the factor loadings which then implement automatic relevance determination. Linear-Gaussian
state-space models can be thought of as factor analysis through time with the hidden factors
evolving with noisy linear dynamics. A variational Bayesian treatment of these models provides
a novel way to learn their structure, i.e. to identify the optimal dimensionality of their state
space.
With suitable priors the LDS model is in the conjugate-exponential family. This chapter presents
an example of variational Bayes applied to a conjugate-exponential model, which therefore re-
sults in a VBEM algorithm which has an approximate inference procedure with the same com-
plexity as the MAP/ML counterpart, as explained in chapter 2. Unfortunately, the implemen-
tation is not as straightforward as in other models, for example the Hidden Markov Model of
chapter 3, as some subparts of the parameter-to-natural parameter mapping are non-invertible.
The rest of this chapter is written as follows. In section 5.2 we review the LDS model for both
the standard and input-dependent cases, and specify conjugate priors over all the parameters.
In 5.3 we use the VB lower bounding procedure to approximate the Bayesian integral for the
marginal likelihood of a sequence of data under a particular model, and derive the VBEM al-
gorithm. The VBM step is straightforward, but the VBE step is much more interesting and
we fully derive the forward and backward passes analogous to the Kalman ﬁlter and Rauch-
Tung-Striebel smoothing algorithms, which we call the variational Kalman ﬁlter and smoother
respectively. In this section we also discuss hyperparameter learning (including optimisation of
automatic relevance determination hyperparameters), and also show how the VB lower bound
can be computed. In section 5.4 we demonstrate the model’s ability to discover meaningful
structure from synthetically generated data sets (in terms of the dimension of the hidden state
space etc.). In section 5.5 we present a very preliminary application of the VB LDS model
to real DNA microarray data, and attempt to discover underlying mechanisms in the immune
response of human T-lymphocytes, starting from T-cell receptor activation through to gene tran-
scription events in the nucleus. In section 5.6 we suggest extensions to the model and possible
future work, and in section 5.7 we provide some conclusions.
5.2 The Linear Dynamical System model
5.2.1 Variables and topology
In state-space models (SSMs), a sequence (y1,..., yT) of p-dimensional real-valued observa-
tion vectors, denoted y1:T, is modelled by assuming that at each time step t, yt was generated
from a k-dimensional real-valued hidden state variable xt, and that the sequence of x’s follow
160
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
x1
y1 y2 y3 yT
x2 xTx3 ...A
C
Figure 5.1: Graphical model representation of a state-space model. The hidden variables xt
evolve with Markov dynamics according to parameters in A, and at each time step generate an
observation yt according to parameters in C.
a ﬁrst-order Markov process. The joint probability of a sequence of states and observations is
therefore given by:
p(x1:T,y1:T) = p(x1)p(y1 |x1)
T∏
t=2
p(xt|xt−1)p(yt|xt) . (5.1)
This factorisation of the joint probability can be represented by the graphical model shown in
ﬁgure 5.1. For the moment we consider just a single sequence, not a batch of i.i.d. sequences.
For ML and MAP learning there is a straightforward extension for learning multiple sequences;
for VB learning the extensions are outlined in section 5.3.8.
The form of the distribution p(x1) over the ﬁrst hidden state is Gaussian, and is described
and explained in more detail in section 5.2.2. We focus on models where both the dynamics,
p(xt|xt−1), and output functions, p(yt|xt), are linear and time-invariant and the distributions
of the state evolution and observation noise variables are Gaussian, i.e. linear-Gaussian state-
space models:
xt = Axt−1 + wt , wt ∼ N(0,Q) (5.2)
yt = Cxt + vt , vt ∼ N(0,R) (5.3)
where A(k×k) is the state dynamics matrix,C(p×k) is the observation matrix, andQ(k×k)
and R(p×p) are the covariance matrices for the state and output noise variables wt and vt.
The parameters Aand C are analogous to the transition and emission matrices respectively in
a Hidden Markov Model (see chapter 3). Linear-Gaussian state-space models can be thought
of as factor analysis where the low-dimensional (latent) factor vector at one time step diffuses
linearly with Gaussian noise to the next time step.
We will use the terms ‘linear dynamical system’ (LDS) and ‘state-space model’ (SSM) inter-
changeably throughout this chapter, although they emphasise different properties of the model.
LDS emphasises that the dynamics are linear – such models can be represented either in state-
space form or in input-output form. SSM emphasises that the model is represented as a latent-
variable model (i.e. the observables are generated via some hidden states). SSMs can be non-
161
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
x1
u1 u2 u3 uT
y1 y2 y3 yT
x2 xTx3 ...A
C
B
D
Figure 5.2: The graphical model for linear dynamical systems with inputs.
linear in general; here it should be assumed that we refer to linear models with Gaussian noise
except if stated otherwise.
A straightforward extension to this model is to allow both the dynamics and observation model
to include a dependence on a series of d-dimensional driving inputs u1:T:
xt = Axt−1 + But + wt (5.4)
yt = Cxt + Dut + vt . (5.5)
Here B(k×d) and D(p×d) are the input-to-state and input-to-observation matrices respec-
tively. If we now augment the driving inputs with a constant bias, then this input driven model is
able to incorporate an arbitrary origin displacement for the hidden state dynamics, and also can
induce a displacement in the observation space. These displacements can be learnt as parameters
of the input-to-state and input-to-observation matrices.
Figure 5.2 shows the graphical model for an input-dependent linear dynamical system. An input-
dependent model can be used to model control systems. Another possible way in which the
inputs can be utilised is to feedback the outputs (data) from previous time steps in the sequence
into the inputs for the current time step. This means that the hidden state can concentrate on
modelling hidden factors, whilst the Markovian dependencies between successive outputs are
modelled using the output-input feedback construction. We will see a good example of this
type of application in section 5.5, where we use it to model gene expression data in a DNA
microarray experiment.
On a point of notational convenience, the probability statements in the later derivations leave im-
plicit the dependence of the dynamics and output processes on the driving inputs, since for each
sequence they are ﬁxed and merely modulate the processes at each time step. Their omission
keeps the equations from becoming unnecessarily complicated.
Without loss of generality we can set the hidden state evolution noise covariance,Q, to the iden-
tity matrix. This is possible since an arbitrary noise covariance can be incorporated into the state
dynamics matrix A, and the hidden state rescaled and rotated to be made commensurate with
162
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
this change (see Roweis and Ghahramani, 1999, page 2 footnote); these changes are possible
since the hidden state is unobserved, by deﬁnition. This is the case in the maximum likelihood
scenario, but in the MAP or Bayesian scenarios this degeneracy is lost since various scalings in
the parameters will be differently penalised under the parameter priors (see section5.2.2 below).
The remaining parameter of a linear-Gaussian state-space model is the covariance matrix,R, of
the Gaussian output noise, vt. In analogy with factor analysis we assume this to be diagonal.
Unlike the hidden state noise, Q, there is no degeneracy in R since the data is observed, and
therefore its scaling is ﬁxed and needs to be learnt.
For notational convenience we collect the above parameters into a single parameter vector for
the model: θ = ( A,B,C,D,R ).
We now turn to considering the LDS model for a Bayesian analysis. From ( 5.1), the complete-
data likelihood for linear-Gaussian state-space models is Gaussian, which is in the class of ex-
ponential family distributions, thus satisfying condition 1 (2.80). In order to derive a variational
Bayesian algorithm by applying the results in chapter 2 we now build on the model by deﬁning
conjugate priors over the parameters according to condition 2 (2.88).
5.2.2 Speciﬁcation of parameter and hidden state priors
The description of the priors in this section may be made more clear by referring to ﬁgure
5.3. The forms of the following prior distributions are motivated by conjugacy (condition 2,
(2.88)). By writing every term in the complete-data likelihood ( 5.1) explicitly, we notice that
the likelihood for state-space models factors into a product of terms for everyrow of each of the
dynamics-related and output-related matrices, and the priors can therefore be factorised over the
hidden variable and observed data dimensions.
The prior over the output noise covariance matrix R, which is assumed diagonal, is deﬁned
through the precision vector ρ such that R−1 = diag ( ρ). For conjugacy, each dimension of ρ
is assumed to be gamma distributed with hyperparameters aand b:
p(ρ |a,b) =
p∏
s=1
ba
Γ(a)ρa−1
s exp{−bρs}. (5.6)
More generally, we could let R be a full covariance matrix and still be conjugate: its inverse
V = R−1 would be given a Wishart distribution with parameter Sand degrees of freedom ν:
p(V |ν,S) ∝|V|(ν−p−1)/2 exp
[
−1
2tr VS−1
]
, (5.7)
163
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
xt-1
ut
yt
xt
A
B
C
D
R
a
b
g
d
a, b
S0, m0
i=1... n
t=1... T(i)
Figure 5.3: Graphical model representation of a Bayesian state-space model. Each sequence
{y1,..., yTi}is now represented succinctly as the (inner) plate overTipairs of hidden variables,
each presenting the cross-time dynamics and output process. The second (outer) plate is over
the data set of size nsequences. For the most part of the derivations in this chapter we restrict
ourselves to n = 1 , and Tn = T. Note that the plate notation used here is non-standard since
both xt−1 and xt have to be included in the plate to denote the dynamics.
where tr is the matrix trace operator. This more general form is not adopted in this chapter as
we wish to maintain a parallel between the output model for state-space models and the factor
analysis model (as described in chapter 4).
Priors on A, B, Cand D
The row vector a⊤
(j) is used to denote the jth row of the dynamics matrix, A, and is given a
zero mean Gaussian prior with precision equal to diag (α), which corresponds to axis-aligned
covariance and can possibly be non-spherical. Each row ofC, denoted c⊤
(s), is given a zero-mean
Gaussian prior with precision matrix equal to diag (ρsγ). The dependence of the precision of
c(s) on the noise output precision ρs is motivated by conjugacy (as can be seen from the explicit
complete-data likelihood), and intuitively this prior links the scale of the signal to the noise. We
place similar priors on the rows of the input-related matrices B and D, introducing two more
hyperparameter vectors β and δ. A useful notation to summarise these forms is
p(a(j) |α) = N( a(j) |0,diag (α)−1) (5.8)
p(b(j) |β) = N( b(j) |0,diag (β)−1) for j = 1 ,...,k (5.9)
p(c(s) |ρs,γ) = N( c(s) |0,ρ−1
s diag (γ)−1) (5.10)
p(d(s) |ρs,δ) = N( d(s) |0,ρ−1
s diag (δ)−1) (5.11)
p(ρs|a,b) = Ga( ρs|a,b) for s= 1 ,...,p (5.12)
such that a(j) etc. are column vectors.
164
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
The Gaussian priors on the transition ( A) and output (C) matrices can be used to perform ‘au-
tomatic relevance determination’ (ARD) on the hidden dimensions. As an example consider
the matrix C which contains the linear embedding factor loadings for each factor in each of its
columns: these factor loadings induce a high dimensional oriented covariance structure in the
data (CC⊤), based on an embedding of low-dimensional axis-aligned (unit) covariance. Let us
ﬁrst ﬁx the hyperparametersγ = {γ1,...,γ k}. As the parameters of theCmatrix are learnt, the
prior will favour entries close to zero since its mean is zero, and the degree with which the prior
enforces this zero-preference varies across the columns depending on the size of the precisions
in γ. As learning continues, the burden of modelling the covariance in the poutput dimensions
will be gradually shifted onto those hidden dimensions for which the entries in γ are smallest,
thus resulting in the least penalty under the prior for non-zero factor loadings. When the hy-
perparameters are updated to reﬂect this change, the unequal sharing of the output covariance
is further exacerbated. The limiting effect as learning progresses is that some columns of C
become zero, coinciding with the respective hyperparameters tending to inﬁnity. This implies
that those hidden state dimensions do not contribute to the covariance structure of data, and so
can be removed entirely from the output process.
Analogous ARD processes can be carried out for the dynamics matrix A. In this case, if the jth
column of Ashould become zero, this implies that the jth hidden dimension at time t−1 is not
involved in generating the hidden state at time t(the rank of the transformation Ais reduced
by 1). However the jth hidden dimension may still be of use in producing covariance structure
in the data via the modulatory input at each time step, and should not necessarily be removed
unless the entries of the Cmatrix also suggest this.
For the input-related parameters in Band D, the ARD processes correspond to selecting those
particular inputs that are relevant to driving the dynamics of the hidden state (through β), and
selecting those inputs that are needed to directly modulate the observed data (through δ). For
example the (constant) input bias that we use here to model an offset in the data mean will
almost certainly always remain non-zero, with a correspondingly small value in δ, unless the
mean of the data is insigniﬁcantly far from zero.
Traditionally, the prior over the hidden state sequence is expressed as a Gaussian distribution
directly over the ﬁrst hidden statex1 (see, for exampleGhahramani and Hinton, 1996a, equation
(6)). For reasons that will become clear when later analysing the equations for learning the
parameters of the model, we choose here to express the prior over the ﬁrst hidden state indirectly
through a prior over an auxiliary hidden state at time t = 0 , denoted x0, which is Gaussian
distributed with mean µ0 and covariance Σ0:
p(x0 |µ0,Σ0) = N( x0 |µ0,Σ0) . (5.13)
165
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
This induces a prior over x1 via the the state dynamics process:
p(x1 |µ0,Σ0,θ) =
∫
dx0 p(x0 |µ0,Σ0)p(x1 |x0,θ) (5.14)
= N( x1 |Aµ0 + Bu1,A⊤Σ0A+ Q) . (5.15)
Although not constrained to be so, in this chapter we work with a prior covariance Σ0 that is a
multiple of the identity.
The marginal likelihood can then be written
p(y1:T) =
∫
dAdBdCdDd ρ dx0:T p(A,B,C,D, ρ,x0:T,y1:T) . (5.16)
All hyperparameters can be optimised during learning (see section 5.3.6). In section 5.4 we
present results of some experiments in which we show the variational Bayesian approach suc-
cessfully determines the structure of state-space models learnt from synthetic data, and in section
5.5 we present some very preliminary experiments in which we attempt to use hyperparameter
optimisation mechanisms to elucidate underlying interactions amongst genes in DNA microar-
ray time-series data.
A fully hierarchical Bayesian structure
Depending on the task at hand we should consider how full a Bayesian analysis we require. As
the model speciﬁcation stands, there is the problem that the number of free parameters to be ‘ﬁt’
increases with the complexity of the model. For example, if the number of hidden dimensions
were increased then, even though the parameters of the dynamics (A), output (C), input-to-state
(B), and input-to-observation ( D) matrices are integrated out, the size of the α, γ, β and δ
hyperparameters have increased, providing more parameters to ﬁt. Clearly, the more parameters
that are ﬁt the more one departs from the Bayesian inference framework and the more one risks
overﬁtting. But, as pointed out in MacKay (1995), these extra hyperparameters themselves
cannot overﬁt the noise in the data, since it is only the parameters that can do so.
If the task at hand is structure discovery, then the presence of extra hyperparameters should not
affect the returned structure. However if the task is model comparison, that is comparing the
marginal likelihoods for models with different numbers of hidden state dimensions for example,
or comparing differently structured Bayesian models, then optimising over more hyperparame-
ters will introduce a bias favouring more complex models, unless they themselves are integrated
out.
The proper marginal likelihood to use in this latter case is that which further integrates over the
hyperparameters with respect to some hyperprior which expresses our subjective beliefs over
166
VB Linear Dynamical Systems 5.2. The Linear Dynamical System model
the distribution of these hyperparameters. This is necessary for the ARD hyperparameters, and
also for the hyperparameters governing the prior over the hidden state sequence, µ0 and Σ0,
whose number of free parameters are functions of the dimensionality of the hidden state, k.
For example, the ARD hyperparameter for each matrix A,B,C,D would be given a separate
spherical gamma hyperprior, which is conjugate:
α ∼
k∏
j=1
Ga(αj |aα,bα) (5.17)
β ∼
d∏
c=1
Ga(βc|aβ,bβ) (5.18)
γ ∼
k∏
j=1
Ga(γj |aγ,bγ) (5.19)
δ ∼
d∏
c=1
Ga(δc|aδ,bδ) . (5.20)
The hidden state hyperparameters would be given spherical Gaussian and spherical inverse-
gamma hyperpriors:
µ0 ∼ N(µ0 |0,bµ0 I) (5.21)
Σ0 ∼
k∏
j=1
Ga(Σ0−1
jj |aΣ0 ,bΣ0 ) . (5.22)
Inverse-Wishart hyperpriors for Σ0 are also possible. For the most part of this chapter we omit
this fuller hierarchy to keep the exposition clearer, and only perform experiments aimed at struc-
ture discovery using ARD as opposed to model comparison between this and other Bayesian
models. Towards the end of the chapter there is a brief note on how the fuller Bayesian hierar-
chy affects the algorithms for learning.
Origin of the intractability with Bayesian learning
Since A, B, C, D, ρ and x0:T are all unknown, given a sequence of observations y1:T, an
exact Bayesian treatment of SSMs would require computing marginals of the posterior over pa-
rameters and hidden variables, p(A,B,C,D, ρ,x0:T |y1:T). This posterior contains interaction
terms up to ﬁfth order; we can see this by considering the terms in ( 5.1) for the case of LDS
models which, for example, contain terms in the exponent of the form −1
2 x⊤
t C⊤diag (ρ) Cxt.
Integrating over these coupled hidden variables and parameters is not analytically possible.
However, since the model is conjugate-exponential we can apply theorem 2.2 to derive a vari-
167
VB Linear Dynamical Systems 5.3. The variational treatment
ational Bayesian EM algorithm for state-space models analogous to the maximum-likelihood
EM algorithm of Shumway and Stoffer (1982).
5.3 The variational treatment
This section covers the derivation of the results for the variational Bayesian treatment of linear-
Gaussian state-space models. We ﬁrst derive the lower bound on the marginal likelihood, using
only the usual approximation of the factorisation of the hidden state sequence from the param-
eters. Due to some resulting conditional independencies between the parameters of the model,
we see how the approximate posterior over parameters can be separated into posteriors for the
dynamics and output processes. In section 5.3.1 the VBM step is derived, yielding approximate
distributions over all the parameters of the model, each of which is analytically manageable and
can be used in the VBE step.
In section 5.3.2 we justify the use of existing propagation algorithms for the VBE step, and
the following subsections derive in some detail the forward and backward recursions for the
variational Bayesian linear dynamical system. This section is concluded with results for hyper-
parameter optimisation and a note on the tractability of the calculation of the lower bound for
this model.
The variational approximation and lower bound
The full joint probability for parameters, hidden variables and observed data, given the inputs is
p(A,B,C,D, ρ,x0:T,y1:T |u1:T) , (5.23)
which written fully is
p(A|α)p(B|β)p(ρ |a,b)p(C|ρ,γ)p(D|ρ,δ)·
p(x0 |µ0,Σ0)
T∏
t=1
p(xt|xt−1,A,B, ut)p(yt|xt,C,D, ρ,ut) . (5.24)
168
VB Linear Dynamical Systems 5.3. The variational treatment
From this point on we drop the dependence on the input sequence u1:T, and leave it implicit.
By applying Jensen’s inequality we introduce any distribution q(θ,x) over the parameters and
hidden variables, and lower bound the log marginal likelihood
ln p(y1:T) = ln
∫
dAdBdCdDd ρ dx0:T p(A,B,C,D, ρ,x0:T,y1:T) (5.25)
≥
∫
dAdBdCdDd ρ dx0:T ·
q(A,B,C,D, ρ,x0:T) ln p(A,B,C,D, ρ,x0:T,y1:T)
q(A,B,C,D, ρ,x0:T) (5.26)
= F.
The next step in the variational approximation is to assume some approximate form for the
distribution q(·) which leads to a tractable bound. First, we factorise the parameters from the
hidden variables giving q(A,B,C,D,ρ, x0:T) = qθ(A,B,C,D,ρ )qx(x0:T). Writing out the
expression for the exact log posterior ln p(A,B,C,D, ρ,x1:T,y0:T), one sees that it contains
interaction terms between ρ, C and D but none between {A,B}and any of {ρ,C,D }. This
observation implies a further factorisation of the posterior parameter distributions,
q(A,B,C,D, ρ,x0:T) = qAB(A,B)qCDρ(C,D, ρ)qx(x0:T) . (5.27)
It is important to stress that this latter factorisation amongst the parameters falls out of the
initial factorisation of hidden variables from parameters, and from the resulting conditional
independencies given the hidden variables. Therefore the variational approximation does not
concede any accuracy by the latter factorisation, since it is exact given the ﬁrst factorisation of
the parameters from hidden variables.
We choose to write the factors involved in this joint parameter distribution as
qAB(A,B) = qB(B) qA(A|B) (5.28)
qCDρ(C,D, ρ) = qρ(ρ) qD(D|ρ) qC(C|D,ρ) . (5.29)
169
VB Linear Dynamical Systems 5.3. The variational treatment
Now the form for q(·) in (5.27) causes the integral (5.26) to separate into the following sum of
terms:
F=
∫
dB qB(B) ln p(B|β)
qB(B) +
∫
dB qB(B)
∫
dAqA(A|B) ln p(A|α)
qA(A|B)
+
∫
dρ qρ(ρ) ln p(ρ |a,b)
qρ(ρ) +
∫
dρ qρ(ρ)
∫
dDqD(D|ρ) ln p(D|ρ,δ)
qD(D|ρ)
+
∫
dρ qρ(ρ)
∫
dDqD(D|ρ)
∫
dC qC(C|ρ,D) ln p(C|ρ,γ)
qC(C|ρ,D)
−
∫
dx0:T qx(x0:T) ln qx(x0:T)
+
∫
dB qB(B)
∫
dAqA(A|B)
∫
dρ qρ(ρ)
∫
dDqD(D|ρ)
∫
dC qC(C|ρ,D) ·
∫
dx0:T qx(x0:T) ln p(x0:T,y1:T |A,B,C,D, ρ) (5.30)
= F(qx(x0:T),qB(B),qA(A|B),qρ(ρ),qD(D|ρ),qC(C|ρ,D)) . (5.31)
Here we have left implicit the dependence ofFon the hyperparameters. For variational Bayesian
learning, Fis the key quantity that we work with. Learning proceeds with iterative updates of
the variational posterior distributions q·(·), each locally maximising F.
The optimum forms of these approximate posteriors can be found by taking functional deriva-
tives of F (5.30) with respect to each distribution over parameters and hidden variable se-
quences. In the following subsections we describe the straightforward VBM step, and the
somewhat more complicated VBE step. We do not need to be able to compute Fto produce
the learning rules, only calculate its derivatives. Nevertheless its calculation at each iteration
can be helpful to ensure that we are monotonically increasing a lower bound on the marginal
likelihood. We ﬁnish this section on the topic of how to calculate Fwhich is hard to compute
because it contains the a term which is the entropy of the posterior distribution over hidden state
sequences,
H(qx(x0:T)) = −
∫
dx0:T qx(x0:T) ln qx(x0:T) . (5.32)
5.3.1 VBM step: Parameter distributions
Starting from some arbitrary distribution over the hidden variables, the VBM step obtained by
applying theorem 2.2 ﬁnds the variational posterior distributions over the parameters, and from
these computes the expected natural parameter vector, φ = ⟨φ(θ)⟩, where the expectation is
taken under the distribution qθ(θ), where θ = ( A,B,C,D, ρ).
We omit the details of the derivations, and present just the forms of the distributions that ex-
tremise F. As was mentioned in section 5.2.2, given the approximating factorisation of the
170
VB Linear Dynamical Systems 5.3. The variational treatment
posterior distribution over hidden variables and parameters, the approximate posterior over the
parameters can be factorised without further assumption or approximation into
qθ(A,B,C,D, ρ) =
k∏
j=1
q(b(j))q(a(j) |b(j))
p∏
s=1
q(ρs)q(d(s) |ρs)q(c(s) |ρs,d(s)) (5.33)
where, for example, the row vector b⊤
(j) is used to denote the jth row of the matrix B(similarly
so for the other parameter matrices).
We begin by deﬁning some statistics of the input and observation data:
¨U ≡
T∑
t=1
utu⊤
t , U Y ≡
T∑
t=1
uty⊤
t , ¨Y ≡
T∑
t=1
yty⊤
t . (5.34)
In the forms of the variational posteriors given below, the matrix quantities WA, GA, ˜M, SA,
and WC, GC, SC are exactly the expected complete data sufﬁcient statistics, obtained in the
VBE step — their forms are given in equations (5.126-5.132).
The natural factorisation of the variational posterior over parameters yields these forms for A
and B:
qB(B) =
k∏
j=1
N
(
b(j) |ΣBb(j), ΣB
)
(5.35)
qA(A|B) =
k∏
j=1
N
(
a(j) |ΣA
[
sA,(j) −GAb(j)
]
, ΣA
)
(5.36)
with
ΣA−1 = diag ( α) + WA (5.37)
ΣB−1 = diag ( β) + ¨U −G⊤
AΣAGA (5.38)
B = ˜M⊤ −S⊤
AΣAGA , (5.39)
and where b
⊤
(j) and sA,(j) are vectors used to denote the jth row of Band the jth column of SA
respectively. It is straightforward to show that the marginal for Ais given by:
qA(A) =
k∏
j=1
N
(
a(j) |ΣA
[
sA,(j) −GAΣBb(j)
]
, ˆΣA
)
, (5.40)
where ˆΣA = Σ A + ΣAGAΣBG⊤
AΣA . (5.41)
171
VB Linear Dynamical Systems 5.3. The variational treatment
In the case of either the Aand B matrices, for both the marginal and conditional distributions,
each row has the same covariance.
The variational posterior over ρ, Cand Dis given by:
qρ(ρ) =
p∏
s=1
Ga
(
ρs|a+ T
2 , b+ 1
2Gss
)
(5.42)
qD(D|ρ) =
p∏
s=1
N
(
d(s) |ΣDd(s), ρ−1
s ΣD
)
(5.43)
qC(C|D,ρ) =
p∏
s=1
N
(
c(s) |ΣC
[
sC,(s) −GCd(s)
]
, ρ−1
s ΣC
)
(5.44)
with
ΣC−1 = diag ( γ) + WC (5.45)
ΣD−1 = diag ( δ) + ¨U −G⊤
CΣCGC (5.46)
G= ¨Y −S⊤
CΣCSC −DΣDD
⊤
(5.47)
D= U⊤
Y −S⊤
CΣCGC , (5.48)
and where d
⊤
(s) and sC,(s) are vectors corresponding to the sth row of Dand the sth column of
SC respectively. Unlike the case of the Aand B matrices, the covariances for each row of the
C and Dmatrices can be very different due to the appearance of the ρs term, as so they should
be. Again it is straightforward to show that the marginal for Cgiven ρ, is given by:
qC(C|ρ) =
p∏
s=1
N
(
c(s) |ΣC
[
sC,(s) −GCΣDd(s)
]
, ρ−1
s ˆΣC
)
, (5.49)
where ˆΣC = Σ C + ΣCGCΣDG⊤
CΣC . (5.50)
Lastly, the full marginals for C and Dafter integrating out the precision ρ are Student-t distri-
butions.
In the VBM step we need to calculate the expected natural parameters, φ, as mentioned in
theorem 2.2. These will then be used in the VBE step which infers the distributionqx(x0:T) over
hidden states in the system. The relevant natural parameterisation is given by the following:
φ(θ) = φ(A,B,C,D,R ) =
[
A, A⊤A, B, A⊤B, C⊤R−1C, R−1C, C⊤R−1D
B⊤B, R−1, ln
⏐⏐R−1⏐⏐, D⊤R−1D, R−1D
]
. (5.51)
172
VB Linear Dynamical Systems 5.3. The variational treatment
The terms in the expected natural parameter vector φ = ⟨φ(θ)⟩qθ(θ), where ⟨·⟩qθ(θ) denotes
expectation with respect to the variational posterior, are then given by:
⟨A⟩=
[
SA −GAΣBB
⊤]⊤
ΣA (5.52)
⟨A⊤A⟩= ⟨A⟩⊤⟨A⟩+ k
[
ΣA + ΣAGAΣBG⊤
AΣA
]
(5.53)
⟨B⟩= BΣB (5.54)
⟨A⊤B⟩= Σ A
[
SA⟨B⟩−GA
{
⟨B⟩⊤⟨B⟩+ kΣB
}]
(5.55)
⟨B⊤B⟩= ⟨B⟩⊤⟨B⟩+ kΣB , (5.56)
and
⟨ρs⟩= ρs = aρ + T/2
bρ + Gss/2 (5.57)
⟨ln ρs⟩= ln ρs = ψ(aρ + T/2) −ln(bρ + Gss/2) (5.58)
⟨R−1⟩= diag ( ρ) , (5.59)
(5.60)
and
⟨C⟩=
[
SC −GCΣDD
⊤]⊤
ΣC (5.61)
⟨D⟩= DΣD (5.62)
⟨C⊤R−1C⟩= ⟨C⟩⊤diag (ρ) ⟨C⟩+ p
[
ΣC + ΣCGCΣDG⊤
CΣC
]
(5.63)
⟨R−1C⟩= diag ( ρ) ⟨C⟩ (5.64)
⟨C⊤R−1D⟩= Σ C
[
SCdiag (ρ) ⟨D⟩−GC⟨D⟩⊤diag (ρ) ⟨D⟩−pGCΣD
]
(5.65)
⟨R−1D⟩= diag ( ρ) ⟨D⟩ (5.66)
⟨D⊤R−1D⟩= ⟨D⟩⊤diag (ρ) ⟨D⟩+ pΣD . (5.67)
Also included in this list are several expectations which are not part of the mean natural param-
eter vector, but are given here because having them at hand during and after an optimisation is
useful.
5.3.2 VBE step: The Variational Kalman Smoother
We now turn to the VBE step: computing qx(x0:T). Since SSMs are singly connected belief
networks corollary 2.2 tells us that we can make use of belief propagation, which in the case of
SSMs is known as the Rauch-Tung-Striebel smoother ( Rauch et al., 1963). Unfortunately the
173
VB Linear Dynamical Systems 5.3. The variational treatment
implementations of the ﬁlter and smoother are not as straightforward as one might expect, as is
explained in the following subsections.
In the standard point-parameter linear-Gaussian dynamical system, given the settings of the
parameters, the hidden state posterior is jointly Gaussian over the time steps. Reassuringly,
when we differentiate Fwith respect to qx(x0:T), the variational posterior for x0:T is also
Gaussian:
ln qx(x0:T) = −ln Z+ ⟨ln p(A,B,C,D, ρ,x0:T,y1:T)⟩ (5.68)
= −ln Z′ + ⟨ln p(x0:T,y1:T |A,B,C,D, ρ)⟩, (5.69)
where
Z′ =
∫
dx0:T exp⟨ln p(x0:T,y1:T |A,B,C,D, ρ)⟩, (5.70)
and where ⟨·⟩denotes expectation with respect to the variational posterior distribution over pa-
rameters, qθ(A,B,C,D, ρ). In this expression the expectations with respect to the approximate
parameter posteriors are performed on the logarithm of the complete-data likelihood and, even
though this leaves the coefﬁcients on the xt terms in a somewhat unorthodox state, the new log
posterior still only contains up to quadratic terms in each xt and therefore qx(x0:T) must be
Gaussian, as in the point-parameter case. We should therefore still be able to use an algorithm
very similar to the Kalman ﬁlter and smoother for inference of the hidden state sequence’s suf-
ﬁcient statistics (the E-like step). However we can no longer plug in parameters to the ﬁlter and
smoother, but have to work with the natural parameters throughout the implementation.
The following paragraphs take us through the required derivations for the forward and backward
recursions. For the sake of clarity of exposition, we do not at this point derive the algorithms for
the input-driven system (though we do present the full input-driven algorithms as pseudocode
in algorithms 5.1, 5.2 and 5.3). At each stage, we ﬁrst we concentrate on the point-parameter
propagation algorithms and then formulate the Bayesian analogues.
5.3.3 Filter (forward recursion)
In this subsection, we ﬁrst derive the well-known forward ﬁltering recursion steps for the case
in which the parameters are ﬁxed point-estimates. The variational Bayesian analogue of the
forward pass is then presented. The dependence of the ﬁlter equations on the inputs u1:T has
been omitted in the derivations, but is included in the summarising algorithms.
174
VB Linear Dynamical Systems 5.3. The variational treatment
Point-parameter derivation
We deﬁne αt(xt) to be the posterior over the hidden state at time tgiven observed data up to
and including time t:
αt(xt) ≡p(xt|y1:t) . (5.71)
Note that this is slightly different to the traditional form for HMMs which isαt(xt) ≡p(xt,y1:t).
We then form the recursion with αt−1(xt−1) as follows:
αt(xt) =
∫
dxt−1 p(xt−1 |y1:t−1) p(xt|xt−1) p(yt|xt) /p(yt|y1:t−1) (5.72)
= 1
ζt(yt)
∫
dxt−1 αt−1(xt−1) p(xt|xt−1) p(yt|xt) (5.73)
= 1
ζt(yt)
∫
dxt−1 N(xt−1 |µt−1,Σt−1) N( xt|Axt−1,I) N( yt|Cxt,R) (5.74)
= N( xt|µt,Σt) (5.75)
where
ζt(yt) ≡p(yt|y1:t−1) (5.76)
is the ﬁltered output probability; this will be useful for computing the likelihood. Within the
above integrand the quadratic terms in xt−1 form the Gaussian N(xt−1 |x∗
t−1,Σ∗
t−1) with
Σ∗
t−1 =
(
Σ−1
t−1 + A⊤A
)−1
(5.77)
x∗
t−1 = Σ ∗
t−1
[
Σ−1
t−1µt−1 + A⊤xt
]
. (5.78)
Marginalising out xt−1 gives the ﬁltered estimates of the mean and covariance of the hidden
state as
αt(xt) = N( xt|µt,Σt) (5.79)
with
Σt =
[
I + C⊤R−1C−AΣ∗
t−1A⊤
]−1
(5.80)
µt = Σ t
[
C⊤R−1yt + AΣ∗
t−1Σ−1
t−1µt−1
]
. (5.81)
At each step the normalising constant ζt, obtained as the denominator in ( 5.72), contributes to
the calculation of the probability of the data
p(y1:T) = p(y1)p(y2 |y1) ...p (yt|y1:t−1) ...p (yT |y1:T−1) (5.82)
= p(y1)
T∏
t=2
p(yt|y1:t−1) =
T∏
t=1
ζt(yt) . (5.83)
175
VB Linear Dynamical Systems 5.3. The variational treatment
It is not difﬁcult to show that each of the above terms are Gaussian distributed,
ζt(yt) = N( yt|ϖt,ςt) (5.84)
with
ςt =
(
R−1 −R−1CΣtC⊤R−1
)−1
(5.85)
ϖt = ςtR−1CΣtAΣ∗
t−1Σ−1
t−1µt−1 . (5.86)
With these distributions at hand we can compute the probability of each observation yt given
the previous observations in the sequence, and assign a predictive mean and variance to the data
at each time step as it arrives. However, this predictive distribution will change once the hidden
state sequence has been smoothed on the backward pass.
Certain expressions such as equations ( 5.80), (5.81), and ( 5.85) could be simpliﬁed using the
matrix inversion lemma (see appendixB.2), but here we refrain from doing so because a similar
operation is not possible in the variational Bayesian derivation (see comment at end of section
5.3.3).
Variational derivation
It is quite straightforward to repeat the above derivation for variational Bayesian learning, by
replacing parameters (and combinations of parameters) with their expectations under the varia-
tional posterior distributions which were calculated in the VBM step (section 5.3.1). Equation
(5.74) becomes rewritten as
αt(xt) = 1
ζ′
t(yt)
∫
dxt−1 N(xt−1 |µt−1,Σt−1) ·
exp −1
2
⣨
(xt −Axt−1)⊤I(xt −Axt−1) + ( yt −Cxt)⊤R−1(yt −Cxt)
+ kln |2π|+ ln |2πR|
⟩
(5.87)
= 1
ζ′
t(yt)
∫
dxt−1 N(xt−1 |µt−1,Σt−1) ·
exp −1
2
[
x⊤
t−1⟨A⊤A⟩xt−1 −2x⊤
t−1⟨A⟩⊤xt
+ x⊤
t (I + ⟨C⊤R−1C⟩)xt −2x⊤
t ⟨C⊤R−1⟩yt + ...
]
(5.88)
where the angled brackets⟨·⟩denote expectation under the variational posterior distribution over
parameters, qθ(A,B,C,D, ρ).
176
VB Linear Dynamical Systems 5.3. The variational treatment
After the parameter averaging, the integrand is still log-quadratic in both xt−1 and xt, and so
the derivation continues as before but with parameter expectations taking place of the point
estimates. Equations (5.77) and (5.78) now become
Σ∗
t−1 =
(
Σ−1
t−1 + ⟨A⊤A⟩
)−1
(5.89)
x∗
t−1 = Σ ∗
t−1
[
Σ−1
t−1µt−1 + ⟨A⟩⊤xt
]
, (5.90)
and marginalising out xt−1 yields a Gaussian distribution over xt,
αt(xt) = N( xt|µt,Σt) (5.91)
with mean and covariance given by
Σt =
[
I + ⟨C⊤R−1C⟩−⟨A⟩Σ∗
t−1⟨A⟩⊤
]−1
(5.92)
µt = Σ t
[
⟨C⊤R−1⟩yt + ⟨A⟩Σ∗
t−1Σ−1
t−1µt−1
]
. (5.93)
This variational α-message evidently resembles the point-parameter result in (5.80) and (5.81).
Algorithm 5.1 shows the full implementation for the variational Bayesian forward recursion,
including extra terms from the inputs and input-related parameters B and D which were not
derived here to keep the presentation concise. In addition it gives the variational Bayesian
analogues of equations (5.85) and (5.86).
We now see why, for example, equation ( 5.85) was not simpliﬁed using the matrix inversion
lemma — this operation would necessarily split the R−1 and C matrices, yet its variational
Bayesian counterpart requires that expectations be taken over the combined product R−1C.
These expectations cannot be passed through the inversion lemma. Included in appendix B.2
is a proof of the matrix inversion lemma which shows clearly how such expectations would
become disjoined.
5.3.4 Backward recursion: sequential and parallel
In the backward pass information about future observations is incorporated to update the pos-
terior distribution on the current time step. This recursion begins at the last time step t = T
(which has no future observations to take into account) and recurses to the beginning of the
sequence to time t= 0 .
There are two different forms for the backward pass. The sequential form makes use of the
α-messages from the forward pass and does not need to access information about the current
observation in order to calculate the posterior over the hidden state given all the data. The
parallel form is so-called because it executes all its recursions independently of the forward
177
VB Linear Dynamical Systems 5.3. The variational treatment
Algorithm 5.1: Forward recursion for variational Bayesian state-space models with inputs u1:T
(variational Kalman ﬁlter).
1. Initialise hyperparameters µ0 and Σ0 as the mean and covariance of the auxiliary hidden
state x0
2. For t= 1 to T
(a) Compute αt(xt) = N( xt|µt,Σt)
Σ∗
t−1 =
(
Σ−1
t−1 + ⟨A⊤A⟩
)−1
Σt =
(
I+ ⟨C⊤R−1C⟩−⟨A⟩Σ∗
t−1⟨A⟩⊤
)−1
µt = Σ t
[
⟨C⊤R−1⟩yt + ⟨A⟩Σ∗
t−1Σ−1
t−1µt−1
+
(
⟨B⟩−⟨A⟩Σ∗
t−1⟨A⊤B⟩−⟨C⊤R−1D⟩
)
ut
]
(b) Compute predictive distribution of yt
ςt =
(
⟨R−1⟩−⟨R−1C⟩Σt⟨R−1C⟩⊤
)−1
ϖt = ςt
[
⟨R−1C⟩Σt⟨A⟩Σ∗
t−1Σ−1
t−1µt−1
+
(
⟨R−1D⟩+ ⟨R−1C⟩Σt
{
⟨B⟩−⟨C⊤R−1D⟩−⟨A⟩Σ∗
t−1⟨A⊤B⟩
})
ut
]
(c) Compute ζ′
t(yt) (see (5.87) and also section 5.3.7 for details)
ln ζ′
t(yt) = −1
2
[
⟨ln |2πR|⟩−ln
⏐⏐Σ−1
t−1Σ∗
t−1Σt
⏐⏐+ µ⊤
t−1Σ−1
t−1µt−1 −µ⊤
t Σ−1
t µt
+ y⊤
t ⟨R−1⟩yt −2y⊤
t ⟨R−1D⟩ut + u⊤
t ⟨D⊤R−1D⟩ut
−(Σ−1
t−1µt−1 −⟨A⊤B⟩ut)⊤Σ∗
t−1(Σ−1
t−1µt−1 −⟨A⊤B⟩ut)
]
End For
3. Output all computed quantities, including
ln Z′ = ∑T
t=1 ln ζ′
t(yt)
178
VB Linear Dynamical Systems 5.3. The variational treatment
pass, and then later combines its messages with those from the forward pass to compute the
hidden state posterior for each time step.
Sequential implementation: point-parameters
In the sequential implementation we deﬁne a set of γ-messages to be the posterior over the
hidden state given all the data. In the case of point-parameters, the recursion is then
γt(xt) ≡p(xt|y1:T) (5.94)
=
∫
dxt+1 p(xt,xt+1 |y1:T) (5.95)
=
∫
dxt+1 p(xt|xt+1,y1:T)p(xt+1 |y1:T) (5.96)
=
∫
dxt+1 p(xt|xt+1,y1:t) p(xt+1 |y1:T) (5.97)
=
∫
dxt+1
[ p(xt|y1:t)p(xt+1 |xt)∫
dx′
t p(x′
t|y1:t)p(xt+1 |x′
t)
]
p(xt+1 |y1:T) (5.98)
=
∫
dxt+1
[ αt(xt)p(xt+1 |xt)∫
dx′
t αt(x′
t)p(xt+1 |x′
t)
]
γt+1(xt+1) . (5.99)
Here the use of Bayes’ rule in (5.98) has had the effect of replacing the explicit data dependence
with functions of the α-messages computed in the forward pass. Integrating out xt+1 yields
Gaussian distributions for the smoothed estimates of the hidden state at each time step:
γt(xt) = N( xt|ωt,Υtt) (5.100)
where Σ∗
t is as deﬁned in the forward pass according to (5.77) and
Kt =
(
Υ−1
t+1,t+1 + AΣ∗
tA⊤
)−1
(5.101)
Υtt =
[
Σ∗
t
−1 −A⊤KtA
]−1
(5.102)
ωt = Υ tt
[
Σ−1
t µt + A⊤Kt
(
Υ−1
t+1,t+1ωt+1 −AΣ∗
tΣ−1
t µt
)]
. (5.103)
Note that Kt given in (5.101) is a different matrix to the Kalman gain matrix as found in the
Kalman ﬁltering and smoothing literature, and should not be confused with it.
The sequential version has an advantage in online scenarios: once the data at timet, yt, has been
ﬁltered it can be discarded and is replaced with its message, αt(xt) (see, for example, Rauch,
1963). In this way potentially high dimensional observations can be stored simply as beliefs in
the low dimensional state space.
179
VB Linear Dynamical Systems 5.3. The variational treatment
Sequential implementation: variational analysis
Unfortunately the step using Bayes’ rule in ( 5.98) cannot be transferred over to a variational
treatment, and this can be demonstrated by seeing how the term p(xt|xt+1,y1:t) in (5.97) is
altered by the lower bound operation. Up to a normalisation factor,
p(xt|xt+1,y1:t) VB→exp
⣨
ln p(xt|xt+1,y1:t)
⟩
(5.104)
= exp
⣨
ln p(xt+1 |xt) + ln αt(xt) −ln
∫
dx′
t αt(x′
t)p(xt+1 |x′
t)
⟩
(5.105)
The last term in the above equation results in a precision term in the exponent of the form:
ln
∫
dx′
t αt(x′
t)p(xt+1 |x′
t) = −1
2
[
I −A
[
Σ−1
t + A⊤A
]−1
A⊤
]
+ c. Even though this term is
easy to express for a known Amatrix, its expectation under qA(A) is difﬁcult to compute. Even
with the use of the matrix inversion lemma (see appendix B.2), which yields
(
I + AΣtA⊤)−1,
the expression is still not amenable to expectation.
Parallel implementation: point-parameters
Some of the above problems are ameliorated using the parallel implementation, which we ﬁrst
derive using point-parameters. The parallel recursion produces β-messages, deﬁned as
βt(xt) ≡p(yt+1:T |xt) . (5.106)
These are obtained through a recursion analogous to the forward pass (5.72)
βt−1(xt−1) =
∫
dxt p(xt|xt−1)p(yt|xt)p(yt+1:T |xt) (5.107)
=
∫
dxt p(xt|xt−1)p(yt|xt)βt(xt) (5.108)
∝N(xt−1 |ηt−1,Ψt−1) (5.109)
with the end condition that βT(xT) = 1 . Omitting the details, the terms for the backward
messages are given by:
Ψ∗
t =
(
I + C⊤R−1C+ Ψ−1
t
)−1
(5.110)
Ψt−1 =
[
A⊤A−A⊤Ψ∗
tA
]−1
(5.111)
ηt−1 = Ψ t−1A⊤Ψ∗
t
[
C⊤R−1yt + Ψ−1
t ηt
]
(5.112)
180
VB Linear Dynamical Systems 5.3. The variational treatment
where t = {T,..., 1}, and Ψ−1
T set to 0 to satisfy the end condition (regardless of ηT). The
last step in this recursion therefore ﬁnds the probability of all the data given the setting of the
auxiliary x0 variable.
Parallel implementation: variational analysis
It is straightforward to produce the variational counterpart of the backward parallel pass just
described. Omitting the derivation, the results are presented in algorithm5.2 which also includes
the inﬂuence of inputs on the recursions.
Algorithm 5.2: Backward parallel recursion for variational Bayesian state-space models with
inputs u1:T.
1. Initialise Ψ−1
T = 0 to satisfy end condition βT(xT) = 1
2. For t= T to 1
Ψ∗
t =
(
I + ⟨C⊤R−1C⟩+ Ψ−1
t
)−1
Ψt−1 =
(
⟨A⊤A⟩−⟨A⟩⊤Ψ∗
t⟨A⟩
)−1
ηt−1 = Ψ t−1
[
−⟨A⊤B⟩ut
+ ⟨A⟩⊤Ψ∗
t
(
⟨B⟩ut + ⟨C⊤R−1⟩yt −⟨C⊤R−1D⟩ut + Ψ−1
t ηt
)]
End For
3. Output {ηt,Ψt}T
t=0
5.3.5 Computing the single and joint marginals
The culmination of the VBE step is to compute the sufﬁcient statistics of the hidden state, which
are the marginals at each time step and the pairwise marginals across adjacent time steps.
In the point-parameter case, one can use the sequential backward pass, and then the single state
marginals are given exactly by the γ-messages, and it only remains to calculate the pairwise
marginals. It is not difﬁcult to show that the terms involving xt and xt+1 are best represented
with the quadratic term
ln p(xt,xt+1 |y1:T) = −1
2
(
x⊤
t x⊤
t+1
)(
Σ∗
t
−1 −A⊤
−A K −1
t
)(
xt
xt+1
)
+ const. , (5.113)
181
VB Linear Dynamical Systems 5.3. The variational treatment
where Σ∗
t is computed in the forward pass (5.77) and Kt is computed in the backward sequential
pass (5.101).
We deﬁneΥt,t+1 to be the cross-covariance between the hidden states at timestand t+1, given
all the observations y1:T:
Υt,t+1 ≡
⟨
(xt −⟨xt⟩) (xt+1 −⟨xt+1⟩)⊤⟩
, (5.114)
where ⟨·⟩denotes expectation with respect to the posterior distribution over the hidden state
sequence given all the data. We now make use of the Schur complements (see appendixB.1) of
the precision matrix given in (5.113) to obtain
Υt,t+1 = Σ ∗
tA⊤Υt+1,t+1 . (5.115)
The variational Bayesian implementation
In the variational Bayesian scenario the marginals cannot be obtained easily with a backward
sequential pass, and they are instead computed by combining theα- and β-messages as follows:
p(xt|y1:T) ∝p(xt|y1:t)p(yt+1:T |xt) (5.116)
= αt(xt)βt(xt) (5.117)
= N( xt|ωt,Υtt) (5.118)
with
Υt,t =
[
Σ−1
t + Ψ−1
t
]−1
(5.119)
ωt = Υ t,t
[
Σ−1
t µt + Ψ−1
t ηt
]
. (5.120)
This is computed for t= {0,...,T −1}. At t= 0 , α0(x0) is exactly the prior (5.13) over the
auxiliary hidden state; at t= T, there is no need for a calculation sincep(xT |y1:T) ≡αT(xT).
Similarly the pairwise marginals are given by
p(xt,xt+1 |y1:T) ∝p(xt|y1:t)p(xt+1 |xt)p(yt+1 |xt+1)p(yt+2:T |xt+1) (5.121)
= αt(xt)p(xt+1 |xt)p(yt+1 |xt+1)βt+1(xt+1) , (5.122)
182
VB Linear Dynamical Systems 5.3. The variational treatment
which under the variational transform becomes
VB→αt(xt) exp
⟨
ln p(xt+1 |xt) + ln p(yt+1 |xt+1)
⟩
βt+1(xt+1) (5.123)
= N
([
xt
xt+1
]
|
[
ωt
ωt+1
]
,
[
Υt,t Υt,t+1
Υ⊤
t,t+1 Υt+1,t+1
])
. (5.124)
With the use of Schur complements again, it is not difﬁcult to show that Υt,t+1 is given by
Υt,t+1 = Σ ∗
t⟨A⟩⊤
(
I + ⟨C⊤R−1C⟩+ Ψ−1
t+1 −⟨A⟩Σ∗
t⟨A⟩⊤
)−1
. (5.125)
This cross-covariance is then computed for all time steps t = {0,...,T −1}, which includes
the cross-covariance between the zeroth and ﬁrst hidden states.
In summary, the entire VBE step consists of a forward pass followed by a backward pass, during
which the marginals can be computed as well straight after each β-message.
The required sufﬁcient statistics of the hidden state
In the VBE step we need to calculate the expected sufﬁcient statistics of the hidden state, as
mentioned in theorem 2.2. These will then be used in the VBM step which infers the distribution
qθ(θ) over parameters of the system (section 5.3.1). The relevant expectations are:
WA =
T∑
t=1
⟨xt−1x⊤
t−1⟩=
T∑
t=1
Υt−1,t−1 + ωt−1ω⊤
t−1 (5.126)
GA =
T∑
t=1
⟨xt−1⟩u⊤
t =
T∑
t=1
ωt−1u⊤
t (5.127)
˜M =
T∑
t=1
ut⟨xt⟩⊤ =
T∑
t=1
utω⊤
t (5.128)
SA =
T∑
t=1
⟨xt−1x⊤
t ⟩ =
T∑
t=1
Υt−1,t + ωt−1ω⊤
t (5.129)
WC =
T∑
t=1
⟨xtx⊤
t ⟩=
T∑
t=1
Υt,t + ωtω⊤
t (5.130)
GC =
T∑
t=1
⟨xt⟩u⊤
t =
T∑
t=1
ωtu⊤
t (5.131)
SC =
T∑
t=1
⟨xt⟩y⊤
t =
T∑
t=1
ωty⊤
t . (5.132)
183
VB Linear Dynamical Systems 5.3. The variational treatment
Note that M and GC are transposes of one another. Also note that all the summations contain
T terms (instead of those for the dynamics model containing T −1). This is a consequence of
our adoption of a slightly unorthodox model speciﬁcation of linear dynamical systems which
includes a ﬁctitious auxiliary hidden variable x0.
5.3.6 Hyperparameter learning
The hyperparameters α, β, γ, δ, aand b, and the prior parameters Σ0 and µ0, can be updated
so as to maximise the lower bound on the marginal likelihood ( 5.30). By taking derivatives of
Fwith respect to the hyperparameters, the following updates can be derived, applicable after a
VBM step:
α−1
j ←1
k
[
kΣA + ΣA
[
SAS⊤
A −2GA⟨B⟩⊤S⊤
A + GA{kΣB + ⟨B⟩⊤⟨B⟩}G⊤
A
]
ΣA
]
jj
(5.133)
β−1
j ←1
k
[
kΣB + ⟨B⟩⊤⟨B⟩
]
jj
(5.134)
γ−1
j ←1
p
[
pΣC + ΣC
[
SCdiag (ρ) S⊤
C −2SCdiag (ρ) ⟨D⟩G⊤
C
+ pGCΣDG′
C + GC⟨D⟩⊤diag (ρ) ⟨D⟩G⊤
C
]
ΣC
]
jj
(5.135)
δ−1
j ←1
p
[
pΣD + ⟨D⟩⊤diag (ρ) ⟨D⟩
]
jj
(5.136)
where [·]jj denotes its (j,j)th element.
Similarly, in order to maximise the probability of the hidden state sequence under the prior, the
hyperparameters of the prior over the auxiliary hidden state are set according to the distribution
of the smoothed estimate of x0:
Σ0 ←Υ0,0 , µ0 ←ω0 . (5.137)
Last of all, the hyperparameters aand bgoverning the prior distribution over the output noise,
R= diag ( ρ), are set to the ﬁxed point of the equations
ψ(a) = ln b+ 1
p
p∑
s=1
ln ρs , 1
b = 1
pa
p∑
s=1
ρs (5.138)
where ψ(x) ≡ ∂/∂xln Γ(x) is the digamma function (refer to equations ( 5.57) and ( 5.58)
for required expectations). These ﬁxed point equations can be solved straightforwardly using
gradient following techniques (such as Newton’s method) in just a few iterations, bearing in
mind the positivity constraints on aand b(see appendix C.2 for more details).
184
VB Linear Dynamical Systems 5.3. The variational treatment
5.3.7 Calculation of F
Before we see why Fis hard to compute in this model, we should rewrite the lower bound more
succinctly using the following deﬁnitions, in the case of a pair of variables J and K:
KL(J) ≡
∫
dJ q(J) ln q(J)
p(J) (KL divergence) (5.139)
KL(J|K) ≡
∫
dJ q(J|K) ln q(J|K)
p(J|K) (conditional KL) (5.140)
⟨KL(J|K)⟩q(K) ≡
∫
dK q(K)KL(J|K) (expected conditional KL) . (5.141)
Note that in (5.140) the prior over J may need to be a function of Kfor conjugacy reasons (this
is the case for state-space models for the output parameters C and D, and the noise R). The
notation KL(J|K) is not to be confused with KL(J||K) which is the KL divergence between
distributions q(J) and q(K) (which are marginals). The lower bound F(5.26) can now be
written as
F= −KL(B) −⟨KL(A|B)⟩q(B)
−KL(ρ) −⟨KL(D|ρ)⟩q(ρ) −⟨KL(C|ρ,D)⟩q(ρ,D)
+ H(qx(x0:T))
+ ⟨ln p(x1:T,y1:T |A,B,C,D, ρ)⟩q(A,B,C,D,ρ)q(x1:T ) (5.142)
where H(qx(x0:T)) is the entropy of the variational posterior over the hidden state sequence,
H(qx(x0:T)) ≡−
∫
dx0:T qx(x0:T) ln qx(x0:T) . (5.143)
The reason why Fcan not be computed directly is precisely due to both this entropy term
and the last term which takes expectations over all possible hidden state sequences under the
variational posterior qx(x0:T). Fortunately, straight after the VBE step, we know the form of
qx(x0:T) from (5.69), and on substituting this into H(qx(x0:T)) we obtain
H(qx(x0:T)) ≡−
∫
dx0:T qx(x0:T) ln qx(x0:T) (5.144)
= −
∫
dx0:T qx(x0:T)
[
−ln Z′
+ ⟨ln p(x0:T,y1:T |A,B,C,D, ρ,µ0,Σ0)⟩qθ(A,B,C,D,ρ)
]
(5.145)
= ln Z′ −⟨ln p(x0:T,y1:T |A,B,C,D, ρ,µ0,Σ0)⟩qθ(A,B,C,D,ρ)qx(x0:T )
(5.146)
185
VB Linear Dynamical Systems 5.3. The variational treatment
where the last line follows since ln Z′ is not a function of the state sequence x0:T. Substituting
this form (5.146) into the above form for F(5.142) cancels the expected complete-data term in
both equations and yields a simple expression for the lower bound
F= −KL(B) −⟨KL(A|B)⟩q(B)
−KL(ρ) −⟨KL(D|ρ)⟩q(ρ) −⟨KL(C|ρ,D)⟩q(ρ,D)
+ ln Z′ . (5.147)
Note that this simpler expression is only valid straight after the VBE step. The various KL
divergence terms are straightforward, yet laborious, to compute (see section C.3 for details).
We still have to evaluate the log partition function, ln Z′. It is not as complicated as the in-
tegral in equation ( 5.70) suggests — at least in the point-parameter scenario we showed that
ln Z′ = ∑T
t=1 ln ζt(yt), as given in (5.83). With some care we can derive the equivalent terms
{ζ′
t(yt)}T
t=1 for the variational Bayesian treatment, and these are given in part (c) of algorithm
5.1. Note that certain terms cancel across time steps and so the overall computation can be made
more efﬁcient if need be.
Alternatively we can calculate ln Z′ from direct integration of the joint ( 5.70) with respect to
each hidden variable one by one. In principal the hidden variables can be integrated out in any
order, but at the expense of having to store statistics for many intermediate distributions.
The complete learning algorithm for state-space models is presented in algorithm5.3. It consists
of repeated iterations of the VBM step, VBE step, calculation ofF, and hyperparameter updates.
In practice one does not need to compute Fat all for learning. It may also be inefﬁcient to
update the hyperparameters after every iteration of VBEM, and for some applications in which
the user is certain of their prior speciﬁcations, then a hyperparameter learning scheme may not
be required at all.
5.3.8 Modiﬁcations when learning from multiple sequences
So far in this chapter the variational Bayesian algorithm has concentrated on just a data set
consisting of a single sequence. For a data set consisting of n i.i.d. sequences with lengths
{T1,...,T n}, denoted y = {y1,1:T1 ,..., yn,1:Tn}, it is straightforward to show that the VB
algorithm need only be slightly modiﬁed to take into account the following changes.
186
VB Linear Dynamical Systems 5.3. The variational treatment
Algorithm 5.3: Pseudocode for variational Bayesian state-space models.
1. Initialisation
Θ ≡{α,β,γ,δ}← initialise precision hyperparameters
µ0,Σ0 ←initialise hidden state priors
hss←initialise hidden state sufﬁcient statistics
2. Variational M step (VBM)
Infer parameter posteriors qθ(θ) using {hss,y1:T,u1:T,Θ}
q(B), q(A|B), q(ρ), q(D|ρ), and q(C|ρ,D)
φ ←calculate expected natural parameters using equations (5.52-5.67)
3. Variational E step (VBE)
Infer distribution over hidden state qx(x0:T) using {φ,y1:T,u1:T}
compute αt(xt) ≡p(xt|y1:t) t∈{1,...,T }(forward pass, algorithm 5.1),
compute βt(xt) ≡p(yt+1:T |xt) t∈{0,...,T −1}(backward pass, algorithm 5.2),
compute ωt,Υt,t t∈{0,...,T }(marginals), and
compute Υt,t+1 t∈{0,...,T −1}(cross-covariance).
hss←calculate hidden state sufﬁcient statistics using equations (5.126-5.132)
4. Compute F
Compute various parameter KL divergences (appendix C.3)
Compute log partition function, ln Z′ (equation (5.70), algorithm 5.1)
F= −KL(B) −⟨KL(A|B)⟩−KL(ρ) −⟨KL(D|ρ)⟩−⟨KL(C|ρ,D)⟩+ ln Z′
5. Update hyperparameters
Θ ←update precision hyperparameters using equations (5.133-5.136)
{µ0,Σ0}← update auxiliary hidden state x0 prior hyperparameters using (5.137)
{a,b}← update noise hyperparameters using (5.138)
6. While Fis increasing, go to step 2
187
VB Linear Dynamical Systems 5.3. The variational treatment
In the VBE step, the forward and backward passes of algorithms 5.1 and 5.2 are carried out on
each sequence, resulting in a set of sufﬁcient statistics for each of the nhidden state sequences.
These are then pooled to form a combined statistic. For example, equation (5.126) becomes
W(i)
A =
Ti∑
t=1
⟨xi,t−1x⊤
i,t−1⟩ =
Ti∑
t=1
Υi,t−1,t−1 + ωi,t−1ω⊤
i,t−1 , (5.148)
and then WA =
n∑
i=1
W(i)
A , (5.149)
where Υi,t,t and ωi,t are the results of the VBE step on the ith sequence. Each of the required
sufﬁcient statistics in equations (5.126-5.132) are obtained in a similar fashion. In addition, the
number of time steps T is replaced with the total over all sequences T = ∑n
i=1 Ti.
Algorithmically, the VBM step remains unchanged, as do the updates for the hyperparameters
{α,β,γ,δ,a,b }. The updates for the hyperparameters µ0 and Σ0, which govern the mean and
covariance of the auxiliary hidden state at time t = 0 for every sequence, have to be modiﬁed
slightly and become
µ0 ←1
n
n∑
i=1
ωi,0 , (5.150)
Σ0 ←1
n
n∑
i=1
[
Υi,0,0 + (µ0 −ωi,0)(µ0 −ωi,0)⊤
]
, (5.151)
where the µ0 appearing in the update for Σ0 is the updated hyperparameter. In the case of
n = 1 , equations ( 5.150) and ( 5.151) resemble their originals forms given in section 5.3.6.
Note that these batch updates trivially extend the analogous result for ML parameter estimation
of linear dynamical systems presented by Ghahramani and Hinton ( Ghahramani and Hinton ,
1996a, equation (25)), since here we do not assume that the sequences are equal in length (it is
clear from the forward and backward algorithms in both the ML and VB implementations that
the posterior variance of the auxiliary state Υi,0,0 will only be constant if all the sequences have
the same length).
Finally the computation of the lower bound Fis unchanged except that it now involves a con-
tribution from each sequence
F= −KL(B) −⟨KL(A|B)⟩q(B)
−KL(ρ) −⟨KL(D|ρ)⟩q(ρ) −⟨KL(C|ρ,D)⟩q(ρ,D) +
n∑
i=1
ln Z′(i) ,
where ln Z′(i) is computed in the VBE step in algorithm 5.1 for each sequence individually.
188
VB Linear Dynamical Systems 5.4. Synthetic Experiments
5.3.9 Modiﬁcations for a fully hierarchical model
As mentioned towards the end of section5.2.2, the hierarchy of hyperparameters for priors over
the parameters is not complete for this model as it stands. There remains the undesirable feature
that the parameters Σ0 and µ0 contain more free parameters as the dimensionality of the hidden
state increases. There is a similar problem for the precision hyperparameters. We refer the
reader to chapter 4 in which a similar structure was used for the hyperparameters of the factor
loading matrices.
With such variational distributions in place for VB LDS, the propagation algorithms would
change, replacing, for example, α, with its expectation over its variational posterior, ⟨α⟩q(α),
and the hyperhyperparameters aα,bα of equation ( 5.17) would be updated to best ﬁt the vari-
ational posterior for α, in the same fashion that the hyperparameters a,b are updated to reﬂect
the variational posterior on ρ(section 5.3.6). In addition a similar KL penalty term would arise.
For the parameters Σ0 and µ0, again KL terms would crop up in the lower bound, and where
these quantities appeared in the propagation algorithms they would have to be replaced with
their expectations under their variational posterior distributions.
These modiﬁcations were considered too time-consuming to implement for the experiments
carried out in the following section, and so we should of course be mindful of their exclusion.
5.4 Synthetic Experiments
In this section we give two examples of how the VB algorithm for linear dynamical systems
can discover meaningful structure from the data. The ﬁrst example is carried out on a data set
generated from a simple LDS with no inputs and a small number of hidden states. The second
example is more challenging and attempts to learn the number of hidden states and their dynam-
ics in the presence of noisy inputs. We ﬁnd in both experiments that the ARD mechanism which
optimises the precision hyperparameters can be used successfully to determine the structure of
the true generating model.
5.4.1 Hidden state space dimensionality determination (no inputs)
An LDS with hidden state dimensionality of k= 6 and an output dimensionality of p= 10 was
set up with parameters randomly initialised according to the following procedure.
The dynamics matrix A(k×k) was ﬁxed to have eigenvalues of (.65,.7,.75,.8,.85,.9), con-
structed from a randomly rotated diagonal matrix; choosing fairly high eigenvalues ensures that
189
VB Linear Dynamical Systems 5.4. Synthetic Experiments
10 20 30 50 100 150 200 250 300
A
C
Figure 5.4: Hinton diagrams of the dynamics ( A) and output ( C) matrices after 500 iterations
of VBEM. From left to right, the length of the observed sequence y1:T increases from T =
10 to 300. This true data was generated from a linear dynamical system with k = 6 hidden
state dimensions, all of which participated in the dynamics (see text for a description of the
parameters used). As a visual aid, the entries of Amatrix and the columns of the Cmatrix have
been permuted in the order of the size of the hyperparameters in γ.
every dimension participates in the hidden state dynamics. The output matrixC(p×k) had each
entry sampled from a bimodal distribution made from a mixture of two Gaussians with means
at (2,-2) and common standard deviations of 1; this was done in an attempt to keep the matrix
entries away from zero, such that every hidden dimension contributes to the output covariance
structure. Both the state noise covariance Qand output noise covariance Rwere set to be the
identity matrix. The hidden state at time t = 1 was sampled from a Gaussian with mean zero
and unit covariance.
From this LDS model several training sequences of increasing length were generated, ranging
from T = 10 ,..., 300 (the data sets are incremental). A VBLDS model with hidden state space
dimensionality k = 10 was then trained on each single sequence, for a total of 500 iterations
of VBEM. The resulting Aand C matrices are shown in ﬁgure 5.4. We can see that for short
sequences the model chooses a simple representation of the dynamics and output processes,
and for longer sequences the recovered model is the same as the underlying LDS model which
generated the sequences. Note that the model learns a predominantly diagonal dynamics matrix,
or a self-reinforcing dynamics (this is made obvious by the permutation of the states in the
ﬁgure (see caption), but is not a contrived observation). The likely reason for this is the prior’s
preference for the A matrix to have small sum-of-square entries for each column; since the
dynamics matrix has to capture a certain amount of power in the hidden dynamics, the least
expensive way to do this is to place most of the power on the diagonal entries.
Plotted in ﬁgure 5.5 are the trajectories of the hyperparameters α and γ, during the VB optimi-
sation for the sequence of lengthT = 300 . For each hidden dimension jthe output hyperparam-
eter γj (vertical) is plotted against the dynamics hyperparameter αj. It is in fact the logarithm
of the reciprocal of the hyperparameter that is plotted on each axis. Thus if a hidden dimension
becomes extinct, the reciprocal of its hyperparameter tends to zero (bottom left of plots). Each
component of each hyperparameter is initialised to 1 (see annotation for iteration 0, at top right
of plot 5.5(a)), and during the optimisation some dimensions become extinct. In this example,
four hidden state dimensions become extinct, both in their ability to participate in the dynamics
190
VB Linear Dynamical Systems 5.4. Synthetic Experiments
−12 −10 −8 −6 −4 −2 0−12
−10
−8
−6
−4
−2
0
2
j=1
   2
   3
   4
   5
   6
   7
   8
   9
 10
iteration 0
extinct hidden states
(a) Hidden state inverse-hyperparameter tra-
jectories (logarithmic axes).
−4 −3.5 −3 −2.5
−1
−0.5
0
0.5
1
1.5
2
iteration 1
convergence
(b) Close-up of top right corner of (a).
Figure 5.5: Trajectories of the hyperparameters for the case n= 300 , plotted as ln 1
α (horizon-
tal axis) against ln 1
γ (vertical axis). Each trace corresponds to one of k hidden state dimen-
sions, with points plotted after each iteration of VBEM. Note the initialisation of (1,1) for all
(αj,γj), j= 1 ,...,k (labelled iteration 0). The direction of each trajectory can be determined
by noting the spread of positions at successive iterations, which are resolvable at the begin-
ning of the optimisation, but not so towards the end (see annotated close-up). Note especially
that four hyperparameters are ﬂung to locations corresponding to very small variances of the
prior for both the Aand C matrix columns (i.e. this has effectively removed those hidden state
dimensions), and six remain in the top right with ﬁnite variances. Furthermore, the L-shaped
trajectories of the eventually extinct hidden dimensions imply that in this example the dimen-
sions are removed ﬁrst from the model’s dynamics, and then from the output process (see ﬁgure
5.8(a,c) also).
and their contribution to the covariance of the output data. Six hyperparameters remain useful,
corresponding to k = 6 in the true model. The trajectories of these are seen more clearly in
ﬁgure 5.5(b).
5.4.2 Hidden state space dimensionality determination (input-driven)
This experiment demonstrates the capacity of the input-driven model to use (or not to use) an
input-sequence to model the observed data. We obtained a sequence y1:T of length T = 100 by
running the linear dynamical system as given in equations ( 5.4,5.5), with a hidden state space
dimensionality of k = 2 , generating an observed sequence of dimensionality p= 4 . The input
sequence, u1:T, consisted of three signals: the ﬁrst two were π
2 phase-lagged sinusoids of period
50, and the third dimension was uniform noise ∼ U(0,1).
The parameters A, C, and Rwere created as described above (section5.4.1). The eigenvalues of
the dynamics matrix were set to (.65,.7), and the covariance of the hidden state noise set to the
identity. The parameter B(k×u) was set to the all zeros matrix, so the inputs did not modulate
191
VB Linear Dynamical Systems 5.4. Synthetic Experiments
the hidden state dynamics. The ﬁrst two columns of the D(p×u) matrix were sampled from
the uniform U(−10,10), so as to induce a random (but ﬁxed) displacement of the observation
sequence. The third column of the D matrix was set to zeros, so as to ignore the third input
dimension (noise). Therefore the only noise in the training data was that from the state and
output noise mechanisms (Qand R).
Figure 5.6 shows the input sequence used, the generated hidden state sequence, and the result-
ing observed data, over T = 100 time steps. We would like the variational Bayesian linear
dynamical system to be able to identify the number of hidden dimensions required to model
the observed data, taking into account the modulatory effect of the input sequence. As in the
previous experiment, in this example we attempt to learn an over-speciﬁed model, and make use
of the ARD mechanisms in place to recover the structure of the underlying model that generated
the data.
In full, we would like the model to learn that there are k = 2 hidden states, that the third
input dimension is irrelevant to predicting the observed data, that all the input dimensions are
irrelevant for the hidden state dynamics, and that it is only the two dynamical hidden variables
that are being embedded in the data space.
The variational Bayesian linear dynamical system was run with k = 4 hidden dimensions, for
a total of 800 iterations of VBE and VBM steps (see algorithm 5.3 and its sub-algorithms).
Hyperparameter optimisations after each VBM step were introduced on a staggered basis to
ease interpretability of the results. The dynamics-related hyperparameter optimisations (i.e. α
and β) were begun after the ﬁrst 10 iterations, the output-related optimisations (i.e. γ and δ)
after 20 iterations, and the remaining hyperparameters (i.e. a, b, Σ0 and µ0) optimised after 30
iterations. After each VBE step, Fwas computed and the current state of the hyperparameters
recorded.
Figure 5.7 shows the evolution of the lower bound on the marginal likelihood during learning,
displayed as both the value of Fcomputed after each VBE step (ﬁgure 5.7(a)), and the change
in Fbetween successive iterations of VBEM (ﬁgure 5.7(b)). The logarithmic plot shows the
onset of each group of hyperparameter optimisations (see caption), and also clearly shows three
regions where parameters are being pruned from the model.
As before we can analyse the change in the hyperparameters during the optimisation process. In
particular we can examine the ARD hyperparameter vectors α,β,γ,δ, which contain the prior
precisions for the entries of each column of each of the matrices A,B,C and D respectively.
Since the hyperparameters are updated to reﬂect the variational posterior distribution over the
parameters, a large value suggest that the relevant column contains entries are close to zero, and
therefore can be considered excluded from the state-space model equations (5.4) and (5.5).
192
VB Linear Dynamical Systems 5.4. Synthetic Experiments
0 20 40 60 80 100−1
0
1
(a) 3 dimensional input sequence.
0 20 40 60 80 100−4
−2
0
2
4
(b) 2 dimensional hidden state sequence.
0 20 40 60 80 100−20
0
20
(c) 4 dimensional observed data.
Figure 5.6: Data for the input-driven example in section 5.4.2. (a): The 3 dimensional input
data consists of two phase-lagged sinusoids of period 50, and a third dimension consisting of
noise uniformly distributed on [0,1]. Both Band Dcontain zeros in their third columns, so the
noise dimension is not used when generating the synthetic data. (b): The hidden state sequence
generated from the dynamics matrix, A, which in this example evolves independently of the
inputs. (c): The observed data, generated by combining the embedded hidden state sequence
(via the output matrix C) and the input sequence (via the input-output matrix D), and then
adding noise with covariance R. Note that the observed data is now a sinusoidally modulated
simple linear dynamical system.
193
VB Linear Dynamical Systems 5.4. Synthetic Experiments
0 100 200 300 400 500 600 700 800−1200
−1150
−1100
−1050
−1000
−950
−900
−850
(a) Evolution of F during iterations of VBEM.
0 100 200 300 400 500 600 700 80010
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
10
2 (b) Change in F between successive iterations.
Figure 5.7: Evolution of the lower bound Fduring learning of the input-dependent model of
section 5.4.2. (a): The lower bound Fincreases monotonically with iterations of VBEM. (b):
Interesting features of the optimisation can be better seen in a logarithmic plot of the change of
Fbetween successive iterations of VBEM. For example, it is quite clear there is a sharp increase
in Fat 10 iterations (dynamics-related hyperparameter optimisation activated), at 20 iterations
(output-related hyperparameter optimisation activated), and at 30 iterations (the remaining hy-
perparameter optimisations are activated). The salient peaks around 80, 110, and 400 iterations
each correspond to the gradual automatic removal of one or more parameters from the model by
hyperparameter optimisation. For example, it is quite probable that the peak at around iteration
400 is due to the recovery of the ﬁrst hidden state modelling the dynamics (see ﬁgure 5.8).
194
VB Linear Dynamical Systems 5.5. Elucidating gene expression mechanisms
Figure 5.8 displays the components of each of the four hyperparameter vectors throughout the
optimisation. The reciprocal of the hyperparameter is plotted since it is more visually intuitive
to consider the variance of the parameters falling to zero as corresponding to extinction, instead
of the precision growing without bound. We can see that, by 500 iterations, the algorithm has
(correctly) discovered that there are only two hidden variables participating in the dynamics
(from α), these same two variables are used as factors embedded in the output (from γ), that
none of the input dimensions is used to modulate the hidden dynamics (from β), and that just
two dimensions of the input are required to displace the data (from δ). The remaining third
dimension of the input is in fact disregarded completely by the model, which is exactly according
to the recipe used for generating this synthetic data.
Of course, with a smaller data set, the model may begin to remove some parameters corre-
sponding to arcs of inﬂuence between variables across time steps, or between the inputs and
the dynamics or outputs. This and the previous experiment suggest that with enough data, the
algorithm will generally discover a good model for the data, and indeed recover the true (or
equivalent) model if the data was in fact generated from a model within the class of models
accessible by the speciﬁed input-dependent linear dynamical system.
Although not observed in the experiment presented here, some caution needs to be taken with
much larger sequences to avoid local minima in the optimisation. In the larger data sets the
problems of local maxima or very long plateau regions in the optimisation become more fre-
quent, with certain dimensions of the latent space modelling either the dynamics or the output
processes, but not both (or neither). This problem is due to the presence of a dynamics model
coupling the data across each time step. Recall that in the factor analysis model (chapter 4),
because of the spherical factor noise model, ARD can rotate the factors into a basis where the
outgoing weights for some factors can be set to zero (by taking their precisions to inﬁnity). Un-
fortunately this degeneracy is not present for the hidden state variables of the LDS model, and
so concerted efforts are required to rotate the hidden state along the entire sequence.
5.5 Elucidating gene expression mechanisms
Description of the process and data
The data consists of n= 34 time series of the expressions of genes involved in a transcriptional
process in the nuclei of human T lymphocytes. Each sequence consists ofT = 10 measurements
of the expressions of p = 88 genes, at time points (0,2,4,6,8,18,24,32,48,72) hours after a
treatment to initiate the transcriptional process (see Rangel et al., 2001, section 2.1). For each
sequence, the expression levels of each gene were normalised to have mean 1, by dividing by
the mean gene expression over the 10 time steps. This normalisation reﬂects our interest in
195
VB Linear Dynamical Systems 5.5. Elucidating gene expression mechanisms
0 100 200 300 400 500 600 700 80010
−5
10
−4
10
−3
10
−2
10
−1
10
0
j=1
   2
   3
   4
(a) Prior variance on each column of A, 1
α .
0 100 200 300 400 500 600 700 80010
−5
10
−4
10
−3
10
−2
10
−1
10
0
c=1
    2
    3 (b) Prior variance on each column of B, 1
β .
0 100 200 300 400 500 600 700 80010
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
j=1
   2
   3
   4
(c) Prior variance on each column of C, 1
γ .
0 100 200 300 400 500 600 700 80010
−5
10
−4
10
−3
10
−2
10
−1
10
0
10
1
10
2
c=1
    2
    3 (d) Prior variance on each column of D, 1
δ .
Figure 5.8: Evolution of the hyperparameters with iterations of variational Bayesian EM, for
the input-driven model trained on the data shown in ﬁgure 5.6 (see section 5.4.2). Each plot
shows the reciprocal of the components of a hyperparameter vector, corresponding to the prior
variance of the entries of each column of the relevant matrix. The hyperparameter optimisation
is activated after 10 iterations of VBEM for the dynamics-related hyperparameters α and β,
after 20 iterations for the output-related hyperparametersγ and δ, and after 30 for the remaining
hyperparmeters. (a): After 150 iterations of VBEM, 1
α3
→0 and 1
α4
→0, which corresponds
to the entries in the 3rd and 4th columns of A tending to zero. Thus only the remaining two
hidden dimensions (1,2) are being used for the dynamics process. (b): All hyperparameters in
the β vector grow large, corresponding to each of the column entries in B being distributed
about zero with high precision; thus none of the dimensions of the input vector is being used
to modulate the hidden state. (c): Similar to the Amatrix, two hyperparameters in the vector
γ remain small, and the remaining two increase without bound, 1
γ3
→0 and 1
γ4
→0. This
corresponds to just two hidden dimensions (factors) causing the observed data through the C
embedding. These are the same dimensions as used for the dynamics process, agreeing with
the mechanism that generated the data. (d): Just one hyperparameter, 1
δ3
→0, corresponding
to the model ignoring the third dimension of the input, which is a confusing input unused in
the true generation process (as can be seen from ﬁgure 5.6(a)). Thus the model learns that this
dimension is irrelevant to modelling the data.
196
VB Linear Dynamical Systems 5.5. Elucidating gene expression mechanisms
1 2 3 4 5 6 7 8 9 10 11
12 13 14 15 16 17 18 19 20 21 22
23 24 25 26 27 28 29 30 31 32 33
34 35 36 37 38 39 40 41 42 43 44
45 46 47 48 49 50 51 52 53 54 55
56 57 58 59 60 61 62 63 64 65 66
67 68 69 70 71 72 73 74 75 76 77
78 79 80 81 82 83 84 85 86 87 88
Figure 5.9: The gene expression data of Rangel et al. (2001). Each of the 88 plots corresponds
to a particular gene on the array, and contains all of the recorded 34 sequences each of length
10.
the proﬁles of the genes rather than the absolute expression levels. Figure 5.9 shows the entire
collection of normalised expression levels for each gene.
A previous approach to modelling gene expression levels which used graphical models to model
the causal relationships between genes is presented in Friedman et al. (2000). However, this ap-
proach ignored the temporal dependence of the gene intensities during trials and went only as
far as to infer the causal relationships between the genes within one time step. Their method dis-
cretised expression levels and made use of efﬁcient candidate proposals and greedy methods for
searching the space of model structures. This approach also assumed that all the possibly inter-
acting variables are observed on the microarray. This precludes the existence of hidden causes
or unmeasured genes whose involvement might dramatically simplify the network structure and
therefore ease interpretability of the mechanisms in the underlying biological process.
Linear dynamical systems and other kinds of possibly nonlinear state-space models are a good
class of model to begin modelling this gene expression data. The gene expression measurements
are the noisy 88-dimensional outputs of the linear dynamical system, and the hidden states of
the model correspond to unobserved factors in the gene transcriptional process which are not
recorded in the DNA microarray — they might correspond simply to unmeasured genes, or
they could model more abstractly the effect of players other than genes, for example regulatory
proteins and background processes such as mRNA degradation.
197
VB Linear Dynamical Systems 5.5. Elucidating gene expression mechanisms
Some aspects of using the LDS model for this data are not ideal. For example, we make the
assumptions that the dynamics and output processes are time invariant, which is unlikely in a
real biological system. Furthermore the times at which the data are taken are not linearly-spaced
(see above), which might imply that there is some (possibly well-studied) non-linearity in the
rate of the transcriptional process; worse still, there may be whole missing time slices which, if
they had been included, would have made the dynamics process closer to stationary. There is
also the usual limitation that the noise in the dynamics and output processes is almost certainly
not Gaussian.
Experiment results
In this experiment we use the input-dependent LDS model, and feed back the gene expressions
from the previous time step into the input for the current time step; in doing so we attempt
to discover gene-gene interactions across time steps (in a causal sense), with the hidden state
in this model now really representing unobserved variables. An advantage of this architecture
is that we can now use the ARD mechanisms to determine which genes are inﬂuential across
adjacent time slices, just as before (in section 5.4.2) we determined which inputs were relevant
to predicting the data.
A graphical model for this setup is given in ﬁgure 5.10. When the input is replaced with the
previous time step’s observed data, the equations for the state-space model can be rewritten from
equations (5.4) and (5.5) into the form:
xt = Axt−1 + Byt−1 + wt (5.152)
yt = Cxt + Dyt−1 + vt . (5.153)
As a function only of the data at the previous time step, yt−1, the data at time tcan be written
yt = ( CB + D)yt−1 + rt , (5.154)
where rt = vt + Cwt + CAxt−1 includes all contributions from noise and previous states.
Thus to ﬁrst order the interaction between genedand gene acan be characterised by the element
[CB + D]ad of the matrix. Indeed this matrix need not be symmetric and the element represents
activation or inhibition from gene dto gene aat the next time step, depending on its sign. We
will return to this quantity shortly.
5.5.1 Generalisation errors
For this experiment we trained both variational Bayesian and MAP LDS models on the ﬁrst
30 of the 34 gene sequences, with the dimension of the hidden state ranging from k = 1 to
198
VB Linear Dynamical Systems 5.5. Elucidating gene expression mechanisms
x1
u1
y1 y2 y3 yT
x2 xTx3
...
...
B
B
D
D
C
A
Figure 5.10: The feedback graphical model with outputs feeding into inputs.
20. The remaining 4 sequences were set aside as a test set. Since we required an input at time
t = 1 , u1, the observed sequences that were learnt began from time step t = 2 . The MAP
LDS model was implemented using the VB LDS with the following two modiﬁcations: ﬁrst,
the hyperparameters α,β,γ,δ and a,b were not optimised (however, the auxiliary state prior
mean µ0 and covariance Σ0 were learnt); second, the sufﬁcient statistics for the parameters were
artiﬁcially boosted by a large factor to simulate delta functions for the posterior — i.e. in the
limit of large nthe VBM step recovers the MAP M step estimate of the parameters.
Both algorithms were run for 300 EM iterations, with no restarts. The one-step-ahead mean
total square reconstruction error was then calculated for both the training sequences and the test
sequences using the learnt models; the reconstruction of thetth observation for theith sequence,
yi,t, was made like so:
ˆyMAP
i,t = CMAP⟨xi,t⟩qx + DMAPyi,t−1 (5.155)
ˆyVB
i,t = ⟨C⟩qC ⟨xi,t⟩qx + ⟨D⟩qD yi,t−1 . (5.156)
To clarify the procedure: to reconstruct the observations for the ith sequence, we use the entire
observation sequence yi,1:T to ﬁrst infer the distribution over the hidden state sequence xi,1:T,
and then we attempt to reconstruct eachyi,t using just the hidden statexi,t and yi,t−1. The form
given for the VB reconstruction in ( 5.156) is valid since, subject to the approximate posterior:
all of the variational posterior distributions over the parameters and hidden states are Gaussian,
Cand xt are independent, and the noise is Student-t distributed with mean zero.
Thus for each value ofk, and for each of the MAP and VB learnt models, the total squared error
per sequence is calculated according to:
Etrain = 1
ntrain
∑
i∈train
Ti∑
t=2
(ˆyi,t −yi,t)2 (5.157)
Etest = 1
ntest
∑
i∈test
Ti∑
t=2
(ˆyi,t −yi,t)2 . (5.158)
199
VB Linear Dynamical Systems 5.5. Elucidating gene expression mechanisms
0 10 20 30 40 50 600
0.5
1
1.5
2
2.5
3
3.5
4
4.5
MAP
VB
(a) Training set error per sequence.
0 10 20 30 40 50 604
6
8
10
12
14
16
18
20
22
24
MAP
VB (b) Test set error per sequence.
Figure 5.11: The per sequence squared reconstruction error for one-step-ahead prediction (see
text), as a function of the dimension of the hidden state, ranging fromk= 1 to 64, on (a) the 30
training sequences, and (b) the 4 test sequences.
Figure 5.11 shows the squared reconstruction error for one-step-ahead prediction, as a function
of the dimension of the hidden state for both the training and test sequences. We see that the
MAP LDS model achieves a decreasing reconstruction error on the training set as the dimen-
sionality of the hidden state is increased, whereas VB produces an approximately constant error,
albeit higher. On prediction for the test set, MAP LDS performs very badly and increasingly
worse for more complex learnt models, as we would expect; however, the VB performance is
roughly constant with increasing k, suggesting that VB is using the ARD mechanism success-
fully to discard surplus modelling power. The test squared prediction error is slightly more than
that on the training set, suggesting that VB is overﬁtting slightly.
5.5.2 Recovering gene-gene interactions
We now return to the interactions between genes dand a– more speciﬁcally the inﬂuence of
gene don gene a– in the matrix [CB + D]. Those entries in the matrix which are signiﬁcantly
different from zero can be considered as candidates for ‘interactions’. Here we consider an
entry to be signiﬁcant if the zero point is more than 3 standard deviations from the posterior
mean for that entry (based on the variational posterior distribution for the entry). Calculating
the signiﬁcance for the combinedCB+Dmatrix is laborious, and so here we provide results for
only the Dmatrix. Since there is a degeneracy in the feedback model, we chose to effectively
remove the ﬁrst term,CB, by constraining all (but one) of the hyperparameters inβ to very high
values. The spared hyperparameter in β is used to still model an offset in the hidden dynamics
using the bias input. This process essentially enforces [CB]ad = 0 for all gene-gene pairs, and
so simpliﬁes the interpretation of the learnt model.
200
VB Linear Dynamical Systems 5.6. Possible extensions and future research
Figure 5.12 shows the interaction matrix learnt by the MAP and VB models (with the column
corresponding the bias removed), for the case of k = 2 hidden state dimensions. For the MAP
result we simply show D+ CB. We see that the MAP and VB matrices share some aspects in
terms of the signs and size of some of the interactions, but under the variational posterior only
a few of the interactions are signiﬁcantly non-zero, leading to a very sparse interaction matrix
(see ﬁgure 5.13). Unfortunately, due to proprietary restrictions on the expression data the iden-
tities of the genes cannot be published here, so it is hard to give a biological interpretation to the
network in ﬁgure 5.13. The hope is that these graphs suggest interactions which agree qualita-
tively with the transcriptional mechanisms already established in the research community. The
ultimate result would be to be able to conﬁdently predict the existence of as-yet-undocumented
mechanisms to stimulate and guide future biological experiments. The VB LDS algorithm may
provide a useful starting point for this research programme.
5.6 Possible extensions and future research
The work in this chapter can be easily extended to linear-Gaussian state-space models on trees,
rather than chains, which could be used to model a variety of data. Moreover, for multiply-
connected graphs, the VB propagation subroutine can still be used within a structured VB ap-
proximation.
Another interesting application of this body of theory could be to a Bayesian version of what
we call a switching state-space model (SwSSM), which has the following dynamics:
a switch variable st with dynamics p(st = i|st−1 = j) = Tij , (5.159)
hidden state dynamics p(xt|st−1,xt−1) = N( xt|Ast−1 xt−1,Qst−1 ) , (5.160)
and output function p(yt|st,xt) = N( yt|Cst xt,Rst ) . (5.161)
That is to say we have a non-stationary switching linear dynamical system whose parameters are
drawn from a ﬁnite set according to a discrete variable with its own dynamics. The appealing
aspect of this model is that it contains many models as special cases, including: mixtures of
factor analysers, mixtures of linear dynamical systems, Gaussian-output hidden Markov models,
and mixtures of Gaussians. With appropriate optimisation of the lower bound on the marginal
likelihood, one would hope that the data would provide evidence that one or other, or indeed
hybrids, of the above special cases was the underlying generating model, or best approximates
the true generating process in some sense. We have seen an example of variational Bayesian
learning for hidden Markov models in chapter 3.
We have not commented on how reliably we expect the variational Bayesian method to approx-
imate the marginal likelihood. Indeed a full analysis of the tightness of the variational bound
201
VB Linear Dynamical Systems 5.6. Possible extensions and future research
(a) The MAP EM solution [D + CB]ad.
 (b) Means ⟨Dad⟩ after VBEM.
(c) Variances ⟨D2
ad⟩ − ⟨ Dad⟩2 after VBEM.
 (d) Signiﬁcant entries of D under qD(D).
Figure 5.12: The gene-gene interaction matrix learnt from the (a) MAP and (b) VB models
(with the column corresponding to the bias input removed). Note that some of the entries are
similar in each of the two matrices. Also shown is(c) the covariance of the posterior distribution
of each element, which is a separable product of functions of each of the two genes’ identities.
Show in (d) are the entries of ⟨Dad⟩which are signiﬁcantly far from zero, that is the value of
zero is more than 3 standard deviations from the mean of the posterior.
202
VB Linear Dynamical Systems 5.6. Possible extensions and future research
 1  4
 6
14
19
21
22
24
25
29
30
32
34
3541
42
43
44
45
46
47
48
50
51
52
54
58
61
63
64
72
73
77
78
79
85
87
Figure 5.13: An example representation of the recovered interactions in the Dmatrix, as shown
in ﬁgure 5.12(d). Each arc between two genes represents a signiﬁcant entry in D. Red (dotted)
and green (solid) denote inhibitory and excitatory inﬂuences, respectively. The direction of the
inﬂuence is from the the thick end of the arc to the thin end. Ellipses denote self-connections.
To generate this plot the genes were placed randomly and then manipulated slightly to reduce
arc-crossing.
203
VB Linear Dynamical Systems 5.7. Summary
would require sampling for this model (as carried out in Fr¨uwirth-Schnatter, 1995, for exam-
ple). This is left for further work, but the reader is referred to chapter 4 of this thesis and also
to Miskin (2000), where sampling estimates of the marginal likelihood are directly compared to
the VB lower bound and found to be comparable for practical problems.
We can also model higher than ﬁrst order Markov processes using this model, by extending the
feedback mechanism used in section 5.5. This could be achieved by feeding back concatenated
observed data yt−d:t−1 into the current input vectorut, where dis related to the maximum order
present in the data. This procedure is common practice to model higher order data, but in our
Bayesian scheme we can also learn posterior uncertainties for the parameters of the feedback,
and can entirely remove some of the inputs via the hyperparameter optimisation.
This chapter has dealt solely with the case of linear dynamics and linear output processes with
Gaussian noise. Whilst this is a good ﬁrst approximation, there are many scenarios in which
a non-linear model is more appropriate, for one or both of the processes. For example, S¨arel¨a
et al. (2001) present a model with factor analysis as the output process and a two layer MLP
network to model a non-linear dynamics process from one time step to the next, and Valpola
and Karhunen (2002) extend this to include a non-linear output process as well. In both, the
posterior is assumed to be of (constrained) Gaussian form and a variational optimisation is
performed to learn the parameters and infer the hidden factor sequences. However, their model
does not exploit the full forward-backward propagation and instead updates the hidden state one
step forward and backward in time at each iteration.
5.7 Summary
In this chapter we have shown how to approximate the marginal likelihood of a Bayesian linear
dynamical system using variational methods. Since the complete-data likelihood for the LDS
model is in the conjugate-exponential family it is possible to write down a VBEM algorithm
for inferring the hidden state sequences whilst simultaneously maintaining uncertainty over the
parameters of the model, subject to the approximation that the hidden variables and parameters
are independent given the data.
Here we have had to rederive the forward and backward passes in the VBE step in order for them
to take as input the natural parameter expectations from the VBM step. It is an open problem
to prove that for LDS models the natural parameter mapping φ(θ) is not invertible; that is
to say there exists no ˜θ in general that satisﬁes φ(˜θ) = φ = ⟨φ(θ)⟩qθ(θ). We have therefore
derived here the variational Bayesian counterparts of the Kalman ﬁlter and Rauch-Tung-Striebel
smoother, which can in fact be supplied withany distribution over the parameters. As with other
conjugate-exponential VB treatments, the propagation algorithms have the same complexity as
the MAP point-parameter versions.
204
VB Linear Dynamical Systems 5.7. Summary
We have shown how the algorithm can use the ARD procedure of optimising precision hyperpa-
rameters to discover the structure of models of synthetic data, in terms of the number of required
hidden dimensions. By feeding back previous data into the inputs of the model we have shown
how it is possible to elucidate interactions between genes in a transcription mechanism from
DNA microarray data. Collaboration is currently underway to interpret these results (personal
communication with D. Wild and C. Rangel).
205
Chapter 6
Learning the structure of
discrete-variable graphical models
with hidden variables
6.1 Introduction
One of the key problems in machine learning and statistics is how to learn the structure of graph-
ical models from data. This entails determining the dependency relations amongst the model
variables that are supported by the data. Models of differing complexities can be rated accord-
ing to their posterior probabilities, which by Bayes’ rule are related to the marginal likelihood
under each candidate model.
In the case of fully observed discrete-variable directed acyclic graphs with Dirichlet priors on
the parameters it is tractable to compute the marginal likelihood of a candidate structure and
therefore obtain its posterior probability (or a quantity proportional to this). Unfortunately,
in graphical models containing hidden variables the calculation of the marginal likelihood is
generally intractable for even moderately sized data sets, and its estimation presents a difﬁcult
challenge for approximate methods such as asymptotic-data criteria and sampling techniques.
In this chapter we investigate a novel application of the VB framework to approximating the
marginal likelihood of discrete-variable directed acyclic graph (DAG) structures that contain
hidden variables. We call approximations to a model’s marginal likelihood scores. We ﬁrst
derive the VB score, which is simply the result of a VBEM algorithm applied to DAGs, and
then assess its performance on a model selection task: ﬁnding the particular structure (out of a
small class of structures) that gave rise to the observed data. We also derive and evaluate the
BIC and Cheeseman-Stutz (CS) scores and compare these to VB for this problem.
206
VB Learning for DAG Structures 6.2. Calculating marginal likelihoods of DAGs
We also compare the BIC, CS, and VB scoring techniques to annealed importance sampling
(AIS) estimates of the marginal likelihood. We consider AIS to be a “gold standard”, the best
method for obtaining reliable estimates of the marginal likelihoods of models explored in this
chapter (personal communication with C. Rasmussen, Z. Ghahramani, and R. Neal). We have
used AIS in this chapter to perform the ﬁrst serious case study of the tightness of variational
bounds. An analysis of the limitations of AIS is also provided. The aim of the comparison is
to convince us of the reliability of VB as an estimate of the marginal likelihood in the general
incomplete-data setting, so that it can be used in larger problems, for example embedded in a
(greedy) structure search amongst a much larger class of models.
In section 6.2 we begin by examining the model selection question for discrete directed acyclic
graphs, and show how exact marginal likelihood calculation rapidly becomes computationally
intractable when the graph contains hidden variables. In section 6.3 we brieﬂy cover the EM
algorithm for ML and MAP parameter estimation in DAGs with hidden variables, and discuss
the BIC, Laplace and Cheeseman-Stutz asymptotic approximations. We then present the VBEM
algorithm for variational Bayesian lower bound optimisation, which in the case of discrete DAGs
is a straightforward generalisation of the MAP EM algorithm. In section 6.3.5 we describe in
detail an annealed importance sampling method for estimating marginal likelihoods of discrete
DAGs. In section 6.4 we evaluate the performance of these different scoring methods on the
simple (yet non-trivial) model selection task of determining which of all possible structures
within a class generated a data set. Section 6.5 discusses some related topics which expand
on the methods used in this chapter: ﬁrst, we give an analysis of the limitations of the AIS
implementation and suggest possible extensions for it; second, we more thoroughly consider
the parameter-counting arguments used in the BIC and CS scoring methods, and reformulate
a more successful score. Finally we conclude in section 6.6 and suggest directions for future
research.
6.2 Calculating marginal likelihoods of DAGs
Consider a data set of size n, y = {y1,..., yn}, modelled by the discrete directed acyclic
graph consisting of hidden and observed variables z = {z1,..., zn}= {s1,y1,..., sn,yn}.
The variables in each plate i = 1 ,...,n are indexed by j = 1 ,..., |zi|, of which some j ∈H
are hidden and j ∈V are observed variables, i.e. si = {zij}j∈H and yi = {zij}j∈V.
On a point of nomenclature, note thatzi = {si,yi}contains both hidden and observed variables,
and we interchange freely between these two forms where convenient. Moreover, the numbers
of hidden and observed variables, |si|and |yi|, are allowed to vary with the data point index i.
An example of such a case could be a data set of sequences of varying length, to be modelled
by an HMM. Note also that the meaning of |·|varies depending on the type of its argument, for
207
VB Learning for DAG Structures 6.2. Calculating marginal likelihoods of DAGs
example: |z|is the number of data points, n; |si|is the number of hidden variables (for the ith
data point); |sij|is the cardinality (number of settings) of the jth hidden variable (for the ith
data point).
In a DAG the complete-data likelihood factorises into a product of local probabilities on each
variable
p(z |θ) =
n∏
i=1
|zi|∏
j=1
p(zij |zipa(j),θ) , (6.1)
where pa(j) denotes the vector of indices of the parents of the jth variable. Each variable in
the model is multinomial, and the parameters of the model are different vectors of probabilities
on each variable for each conﬁguration of its parents. For example, the parameter for a binary
variable which has two ternary parents is a 32 ×2 matrix with each row summing to one.
Should there be a variablejwithout any parents (pa(j) = ∅), then the parameter associated with
variable jis simply a vector of its prior probabilities. If we useθjlk to denote the probability that
variable jtakes on value kwhen its parents are in conﬁguration l, then the complete likelihood
can be written out as a product of terms of the form
p(zij |zipa(j),θ) =
|zipa(j)|∏
l=1
|zij|∏
k=1
θ
δ(zij,k)δ(zipa(j),l)
jlk (6.2)
with
∑
k
θjlk = 1 ∀{j,l}. (6.3)
Here we use
⏐⏐zipa(j)
⏐⏐to denote the number of joint settings of the parents of variablej. That is to
say the probability is a product over both all the
⏐⏐zipa(j)
⏐⏐possible settings of the parents and the
|zij|settings of the variable itself. Here we use Kronecker-δnotation which is 1 if its arguments
are identical and zero otherwise. The parameters of the model are given independent Dirichlet
priors, which are conjugate to the complete-data likelihood above (see equation ( 2.80), which
is Condition 1 for conjugate-exponential models). By independent we mean factorised over
variables and parent conﬁgurations; these choices then satisfy theglobal and local independence
assumptions of Heckerman et al. (1995). For each parameter θjl = {θjl1,...,θ jl|zij|}, the
Dirichlet prior is
p(θjl |λjl,m) =
Γ(λ0
jl)∏
kΓ(λjlk)
∏
k
θλjlk−1
jlk , (6.4)
where λ are hyperparameters:
λjl = {λjl1,...,λ jl|zij|} (6.5)
and
λjlk >0 ∀k, λ 0
jl =
∑
k
λjlk . (6.6)
208
VB Learning for DAG Structures 6.2. Calculating marginal likelihoods of DAGs
This form of prior is assumed throughout the chapter. Since the focus of this chapter is not on
optimising these hyperparameters, we use the shorthand p(θ |m) to denote the prior from here
on. In the discrete-variable case we are considering, the complete-data marginal likelihood is
tractable to compute:
p(z |m) =
∫
dθ p(θ |m)p(z |θ) (6.7)
=
∫
dθ p(θ |m)
n∏
i=1
|zi|∏
j=1
p(zij |zipa(j),θ) (6.8)
=
|zi|∏
j=1
|zipa(j)|∏
l=1
Γ(λ0
jl)
Γ(λ0
jl + Njl)
|zij|∏
k=1
Γ(λjlk + Njlk)
Γ(λjlk) (6.9)
where Njlk is deﬁned as the count in the data for the number of instances of variable jbeing in
conﬁguration kwith parental conﬁguration l:
Njlk =
n∑
i=1
δ(zij,k)δ(zipa(j),l), and Njl =
|zij|∑
k=1
Njlk . (6.10)
The incomplete-data likelihood, however, is not as tractable. It results from summing over all
settings of the hidden variables and taking the product over i.i.d. presentations of the data:
p(y |θ) =
n∏
i=1
p(yi|θ) =
n∏
i=1
∑
{zij}j∈H
|zi|∏
j=1
p(zij |zipa(j),θ) . (6.11)
This quantity can be evaluated as the product of n quantities, each of which is a summation
over all possible joint conﬁgurations of the hidden variables; in the worst case this computation
requires O(n∏
j∈H |zij|) operations (although this can usually be made more efﬁcient with the
use of propagation algorithms that exploit the topology of the model). The incomplete-data
marginal likelihood for ncases follows from marginalising out the parameters of the model:
p(y |m) =
∫
dθ p(θ |m)
n∏
i=1
∑
{zij}j∈H
|zi|∏
j=1
p(zij |zipa(j),θ) . (6.12)
This expression is computationally intractable due to the expectation over the real-valued con-
ditional probabilities θ, which couples the hidden variables across i.i.d. data. In the worst case
it can be evaluated as the sum of
(∏
j∈H |zij|
)n
Dirichlet integrals. For example, a model with
just |si|= 2 hidden variables and 100 data points requires the evaluation of 2100 Dirichlet inte-
grals. This means that a linear increase in the amount of observed data results in an exponential
increase in the cost of inference.
209
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
We focus on the task of learning the conditional independence structure of the model, that is,
which variables are parents of each variable. We compare structures based on their posterior
probabilities. In this chapter we assume that the prior, p(m), is uninformative, and so all our
information comes from the intractable marginal likelihood, p(y |m).
In the rest of this chapter we examine several methods to approximate this Bayesian integration
(6.12), in order to make learning and inference tractable. For the moment we assume that the
cardinalities of the variables, in particular the hidden variables, are ﬁxed beforehand. The related
problem of determining the cardinality of the variables from data can be addressed in the same
framework, as we have already seen for HMMs in chapter 3.
6.3 Estimating the marginal likelihood
In this section we look at some approximations to the marginal likelihood, which we refer to
henceforth as scores. We ﬁrst review ML and MAP parameter learning and brieﬂy present the
EM algorithm for a general discrete-variable directed graphical model with hidden variables.
From the result of the EM optimisation, we can construct various asymptotic approximations
to the marginal likelihood, deriving the BIC and Cheeseman-Stutz scores. We then apply the
variational Bayesian framework, which in the case of conjugate-exponential discrete directed
acyclic graphs produces a very simple VBEM algorithm, which is a direct extension of the EM
algorithm for MAP parameter learning. Finally, we derive an annealed importance sampling
method (AIS) for this class of graphical model, which is considered to be the current state-of-
the-art technique for estimating the marginal likelihood of these models using sampling — we
then compare the different scoring methods to it. We ﬁnish this section with a brief note on
some trivial and non-trivial upper bounds to the marginal likelihood.
6.3.1 ML and MAP parameter estimation
The EM algorithm for ML/MAP estimation can be derived using the lower bound interpretation
as was described in section 2.2. We begin with the incomplete-data log likelihood, and lower
bound it by a functional F(qs(s),θ) as follows
ln p(y |θ) = ln
n∏
i=1
∑
{zij}j∈H
|zi|∏
j=1
p(zij |zipa(j),θ) (6.13)
≥
n∑
i=1
∑
si
qsi(si) ln
∏|zi|
j=1 p(zij |zipa(j),θ)
qsi(si) (6.14)
= F({qsi(si)}n
i=1,θ) , (6.15)
210
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
where we have introduced a distribution qsi(si) over the hidden variables si for each data point
yi. We remind the reader that we have used si = {zij}j∈H in going from ( 6.13) to ( 6.14).
On taking derivatives of F({qsi(si)}n
i=1,θ) with respect to qsi(si), the optimal setting of the
variational posterior is given exactly by the posterior
qsi(si) = p(si|yi,θ) ∀i. (6.16)
This is the E step of the EM algorithm; at this setting of the distribution qsi(si) it can be easily
shown that the bound (6.14) is tight (see section 2.2.2).
The M step of the algorithm is derived by taking derivatives of the bound with respect to the
parameters θ. Each θjl is constrained to sum to one, and so we enforce this with Lagrange
multipliers cjl,
∂
∂θjlk
F(qs(s),θ) =
n∑
i=1
∑
si
qsi(si) ∂
∂θjlk
ln p(zij |xipa(j),θj) + cjl (6.17)
=
n∑
i=1
∑
si
qsi(si)δ(zij,k)δ(zipa(j),l) ∂
∂θjlk
ln θjlk + cjl (6.18)
= 0 , (6.19)
which upon rearrangement gives
θjlk ∝
n∑
i=1
∑
si
qsi(si)δ(zij,k)δ(zipa(j),l) . (6.20)
Due to the normalisation constraint on θjl the M step can be written
M step (ML): θjlk = Njlk
∑|zij|
k′=1 Njlk′
, (6.21)
where the Njlk are deﬁned as
Njlk =
n∑
i=1
⟨
δ(zij,k)δ(zipa(j),l)
⟩
qsi(si) (6.22)
where angled-brackets ⟨·⟩qsi(si) are used to denote expectation with respect to the hidden vari-
able posterior qsi(si). The Njlk are interpreted as the expected number of counts for observing
simultaneous settings of children and parent conﬁgurations over observed and hidden variables.
In the cases where bothjand pa(j) are observed variables,Njlk reduces to the simple empirical
count as in (6.10). Otherwise if jor its parents are hidden then expectations need be taken over
the posterior qsi(si) obtained in the E step.
211
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
If we require the MAP EM algorithm, we instead lower bound ln p(θ)p(y |θ). The E step
remains the same, but the M step uses augmented counts from the prior of the form in ( 6.4) to
give the following update:
M step (MAP): θjlk = λjlk −1 + Njlk
∑|zij|
k′=1 λjlk′ −1 + Njlk′
. (6.23)
Repeated applications of the E step ( 6.16) and the M step ( 6.21, 6.23) are guaranteed to in-
crease the log likelihood (with equation (6.21)) or the log posterior (with equation (6.23)) of the
parameters at every iteration, and converge to a local maximum.
As mentioned in section 1.3.1, we note that MAP estimation is basis-dependent. For any par-
ticular θ∗, which has non-zero prior probability, it is possible to ﬁnd a (one-to-one) reparam-
eterisation φ(θ) such that the MAP estimate for φ is at φ(θ∗). This is an obvious drawback
of MAP parameter estimation. Moreover, the use of ( 6.23) can produce erroneous results in
the case of λjlk < 1, in the form of negative probabilities. Conventionally, researchers have
limited themselves to Dirichlet priors in which every λjlk ≥1, although in MacKay (1998) it is
shown how a reparameterisation of θ into the softmax basis results in MAP updates which do
not suffer from this problem (which look identical to ( 6.23), but without the −1 in numerator
and denominator).
6.3.2 BIC
The Bayesian Information Criterion approximation, described in section 1.3.4, is the asymp-
totic limit to large data sets of the Laplace approximation. It is interesting because it does not
depend on the prior over parameters, and attractive because it does not involve the burdensome
computation of the Hessian of the log likelihood and its determinant. For the size of struc-
tures considered in this chapter, the Laplace approximation would be viable to compute, subject
perhaps to a transformation of parameters (see for example MacKay, 1995). However in larger
models the approximation may become unwieldy and further approximations would be required
(see section 1.3.2).
For BIC, we require the number of free parameters in each structure. In the experiments in this
chapter we use a simple counting argument; in section6.5.2 we discuss a more rigorous method
for estimating the dimensionality of the parameter space of a model. We apply the following
counting scheme. If a variable j has no parents in the DAG, then it contributes (|zij|−1)
free parameters, corresponding to the degrees of freedom in its vector of prior probabilities
(constrained to lie on the simplex ∑
kpk = 1 ). Each variable that has parents contributes
212
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
(|zij|−1) parameters for each conﬁguration of its parents. Thus in model mthe total number
of parameters d(m) is given by
d(m) =
|zi|∑
j=1
(|zij|−1)
|zipa(j)|∏
l=1
⏐⏐zipa(j)l
⏐⏐ , (6.24)
where
⏐⏐zipa(j)l
⏐⏐denotes the cardinality (number of settings) of thelth parent of the jth variable.
We have used the convention that the product over zero factors has a value of one to account for
the case in which the jth variable has no parents, i.e.:
|zipa(j)|∏
l=1
⏐⏐zipa(j)l
⏐⏐= 1 , if
⏐⏐zipa(j)l
⏐⏐= 0 . (6.25)
The BIC approximation needs to take into account aliasing in the parameter posterior (as de-
scribed in section 1.3.3). In discrete-variable DAGs, parameter aliasing occurs from two sym-
metries: ﬁrst, a priori identical hidden variables can be permuted; and second, the labellings of
the states of each hidden variable can be permuted. As an example, let us imagine the parents of
a single observed variable are 3 hidden variables having cardinalities (3,3,4). In this case the
number of aliases is 1728 ( = 2! ×3! ×3! ×4!). If we assume that the aliases of the posterior
distribution are well separated then the score is given by
ln p(y |m)BIC = ln p(y |ˆθ) −d(m)
2 ln n+ ln S (6.26)
where Sis the number of aliases, andˆθ is the MAP estimate as described in the previous section.
This correction is accurate only if the modes of the posterior distribution are well separated,
which should be the case in the large data set size limit for which BIC is useful. However, since
BIC is correct only up to an indeterminant missing factor, we might think that this correction is
not necessary. In the experiments we examine the BIC score with and without this correction,
and also with and without the prior term included.
6.3.3 Cheeseman-Stutz
The Cheeseman-Stutz approximation uses the following identity for the incomplete-data marginal
likelihood:
p(y |m) = p(z |m)p(y |m)
p(z |m) = p(z |m)
∫
dθ p(θ |m)p(y |θ,m)∫
dθ p(θ′ |m)p(z |θ′,m) (6.27)
which is true for any completion z = {ˆs,y}of the data. This form is useful because the
complete-data marginal likelihood, p(z |m), is tractable to compute for discrete DAGs with
213
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
independent Dirichlet priors: it is just a product of Dirichlet integrals (see equation (6.9)). Using
the results of section 1.3.2, in particular equation (1.45), we can apply Laplace approximations
to both the numerator and denominator of the above fraction to give
p(y |m) ≈p(ˆs,y |m) p(ˆθ |m)p(y |ˆθ) |2πA|−1
p(ˆθ
′
|m)p(ˆs,y |ˆθ
′
) |2πA′|−1 . (6.28)
We assume that p(y |ˆθ) is computable exactly. If the errors in each of the Laplace approxi-
mations are similar, then they should roughly cancel each other out; this will be the case if the
shape of the posterior distributions about ˆθ and ˆθ
′
are similar. We can ensure that ˆθ
′
= ˆθ by
completing the hidden data {si}n
i=1 with their expectations under their posterior distributions
p(si|y,ˆθ). That is to say the hidden states are completed as follows:
ˆsijk = ⟨δ(sij,k)⟩qsi(si) , (6.29)
which will generally result in non-integer counts Njlk on application of ( 6.22). Having com-
puted these counts and re-estimated ˆθ
′
using equation ( 6.23), we note that ˆθ
′
= ˆθ. The
Cheeseman-Stutz approximation then results from taking the BIC-type asymptotic limit of both
Laplace approximations in (6.28),
ln p(y |m)CS = ln p(ˆs,y |m) + ln p(ˆθ |m) + ln p(y |ˆθ) −d
2 ln n
−ln p(ˆθ
′
|m) −ln p(ˆs,y |ˆθ) + d′
2 ln n (6.30)
= ln p(ˆs,y |m) + ln p(y |ˆθ) −ln p(ˆs,y |ˆθ) , (6.31)
where the last line follows from the modes of the Gaussian approximations being at the same
point, ˆθ
′
= ˆθ, and also the assumption that the number of parameters in the models for complete
and incomplete data are the same, i.e. d= d′ (Cheeseman and Stutz, 1996, but also see section
6.5.2). Each term of (6.31) can be evaluated individually:
from (6.9) p(ˆs,y |m) =
|zi|∏
j=1
|zipa(j)|∏
l=1
Γ(λ0
jl)
Γ(λjl + ˆNjl)
|zij|∏
k=1
Γ(λjlk + ˆNjlk)
Γ(λjlk) (6.32)
from (6.11) p(y |ˆθ) =
n∏
i=1
∑
{zij}j∈H
|zi|∏
j=1
|zipa(j)|∏
l=1
|zij|∏
k=1
ˆθ
δ(zij,k)δ(zipa(j),l)
jlk (6.33)
from (6.1) p(ˆs,y |ˆθ) =
|zi|∏
j=1
|zipa(j)|∏
l=1
|zij|∏
k=1
ˆθ
ˆNjlk
jlk (6.34)
where the ˆNjlk are identical to the Njlk of equation (6.22) if the completion of the data withˆs is
done with the posterior found in the M step of the MAP EM algorithm used to ﬁnd ˆθ. Equation
214
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
(6.33) is simply the output of the EM algorithm, equation ( 6.32) is a function of the counts
obtained in the EM algorithm, and equation (6.34) is a simple computation again.
As with BIC, the Cheeseman-Stutz score also needs to be corrected for aliases in the parameter
posterior, as described above, and is subject to the same caveat that these corrections are only
accurate if the aliases in the posterior are well-separated.
We note that CS is a lower bound on the marginal likelihood, as shown in section 2.6.2 of this
thesis. We will return to this point in the discussion of the experimental results.
6.3.4 The VB lower bound
The incomplete-data log marginal likelihood can be written as
ln p(y |m) = ln
∫
dθ p(θ |m)
n∏
i=1
∑
{zij}j∈H
|zi|∏
j=1
p(zij |zipa(j),θ) . (6.35)
We can form the lower bound in the usual fashion using qθ(θ) and {qsi(si)}n
i=1 to yield (see
section 2.3.1):
ln p(y |m) ≥
∫
dθ qθ(θ) ln p(θ |m)
qθ(θ)
+
n∑
i=1
∫
dθqθ(θ)
∑
si
qsi(si) ln p(zi|θ,m)
qsi(si) (6.36)
= Fm(qθ(θ),q(s)) . (6.37)
We now take functional derivatives to write down the variational Bayesian EM algorithm (theo-
rem 2.1, page 54). The VBM step is straightforward:
ln qθ(θ) = ln p(θ |m) +
n∑
i=1
∑
si
qsi(si) ln p(zi|θ,m) + c , (6.38)
with ca constant. Given that the prior over parameters factorises over variables as in (6.4), and
the complete-data likelihood factorises over the variables in a DAG as in (6.1), equation (6.38)
can be broken down into individual derivatives:
ln qθjl (θjl) = ln p(θjl |λjl,m) +
n∑
i=1
∑
si
qsi(si) ln p(zij |zipa(j),θ,m) + cjl , (6.39)
215
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
where zij may be either a hidden or observed variable, and each cjl is a Lagrange multiplier
from which a normalisation constant is obtained. Equation ( 6.39) has the form of the Dirichlet
distribution. We deﬁne the expected counts under the posterior hidden variable distribution
Njlk =
n∑
i=1
⟨
δ(zij,k)δ(zipa(j),l)
⟩
qsi(si) . (6.40)
Therefore Njlk is the expected total number of times the jth variable (hidden or observed) is
in state k when its parents (hidden or observed) are in state l, where the expectation is taken
with respect to the posterior distribution over the hidden variables for each datum. Then the
variational posterior for the parameters is given simply by (see theorem 2.2)
qθjl (θjl) = Dir ( λjlk + Njlk : k= 1 ,..., |zij|) . (6.41)
For the VBE step, taking derivatives of (6.37) with respect to each qsi(si) yields
ln qsi(si) =
∫
dθ qθ(θ) ln p(zi|θ,m) + c′
i =
∫
dθ qθ(θ) ln p(si,yi|θ,m) + c′
i , (6.42)
where each c′
i is a Lagrange multiplier for normalisation of the posterior. Since the complete-
data likelihood p(zi|θ,m) is in the exponential family and we have placed conjugate Dirichlet
priors on the parameters, we can immediately utilise the results of corollary2.2 (page 74) which
gives simple forms for the VBE step:
qsi(si) ∝qzi(zi) =
|zi|∏
j=1
p(zij |zipa(j),˜θ) . (6.43)
Thus the approximate posterior over the hidden variablessiresulting from a variational Bayesian
approximation is identical to that resulting from exact inference in a model with known point
parameters ˜θ. Corollary 2.2 also tells us that ˜θ should be chosen to satisfy φ(˜θ) = φ. The
natural parameters for this model are the log probabilities {ln θjlk}, where j speciﬁes which
variable, lindexes the possible conﬁgurations of its parents, and k the possible settings of the
variable. Thus
ln ˜θjlk = φ(˜θjlk) = φjlk =
∫
dθjl qθjl (θjl) ln θjlk . (6.44)
Under a Dirichlet distribution, the expectations are given by differences of digamma functions
ln ˜θjlk = ψ(λjlk + Njlk) −ψ(
|zij|∑
k=1
λjlk + Njlk) ∀{j,l,k }. (6.45)
216
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
where the Njlk are deﬁned in ( 6.40), and the ψ(·) are digamma functions (see appendix C.1).
Since this expectation operation takes the geometric mean of the probabilities, the propagation
algorithm in the VBE step is now passed sub-normalised probabilities as parameters
|zij|∑
k=1
˜θjlk ≤1 ∀{j,l}. (6.46)
This use of sub-normalised probabilities also occurred in Chapter3, which is unsurprising given
that both models consist of local multinomial conditional probabilities. In that model, the in-
ference algorithm was the forward-backward algorithm (or its VB analogue), and was restricted
to the particular topology of a Hidden Markov Model. Our derivation uses belief propagation
(section 1.1.2) for any singly-connected discrete DAG.
The expected natural parameters become normalised only if the distribution over parameters is
a delta function, in which case this reduces to the MAP inference scenario of section 6.3.1. In
fact, if we look at the limit of the digamma function for large arguments (see appendixC.1), we
ﬁnd
lim
x→∞
ψ(x) = ln x, (6.47)
and equation (6.45) becomes
lim
n→∞
ln ˜θjlk = ln( λjlk + Njlk) −ln(
|zij|∑
k=1
λjlk + Njlk) (6.48)
which has recovered the MAP estimator for θ (6.23), up to the −1 entries in numerator and
denominator which become vanishingly small for large data, and vanish completely if MAP is
performed in the softmax basis. Thus in the limit of large data VB recovers the MAP parameter
estimate.
To summarise, the VBEM implementation for discrete DAGs consists of iterating between the
VBE step ( 6.43) which infers distributions over the hidden variables given a distribution over
the parameters, and a VBM step ( 6.41) which ﬁnds a variational posterior distribution over
parameters based on the hidden variables’ sufﬁcient statistics from the VBE step. Each step
monotonically increases a lower bound on the marginal likelihood of the data, and the algorithm
is guaranteed to converge to a local maximum of the lower bound.
The VBEM algorithm uses as a subroutine the algorithm used in the E step of the corresponding
EM algorithm for MAP estimation, and so the VBE step’s computational complexity is the same
— there is some overhead in calculating differences of digamma functions instead of ratios of
expected counts, but this is presumed to be minimal and ﬁxed.
As with BIC and Cheeseman-Stutz, the lower bound does not take into account aliasing in the
parameter posterior, and needs to be corrected as described in section 6.3.2.
217
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
6.3.5 Annealed Importance Sampling (AIS)
AIS ( Neal, 2001) is a state-of-the-art technique for estimating marginal likelihoods, which
breaks a difﬁcult integral into a series of easier ones. It combines techniques from importance
sampling, Markov chain Monte Carlo, and simulated annealing ( Kirkpatrick et al. , 1983). It
builds on work in the Physics community for estimating the free energy of systems at differ-
ent temperatures, for example: thermodynamic integration ( Neal, 1993), tempered transitions
(Neal, 1996), and the similarly inspired umbrella sampling (Torrie and Valleau, 1977). Most of
these, as well as other related methods, are reviewed in Gelman and Meng (1998).
Obtaining samples from the posterior distribution over parameters, with a view to forming a
Monte Carlo estimate of the marginal likelihood of the model, is usually a very challenging
problem. This is because, even with small data sets and models with just a few parameters, the
distribution is likely to be very peaky and have its mass concentrated in tiny volumes of space.
This makes simple approaches such as sampling parameters directly from the prior or using
simple importance sampling infeasible. The basic idea behind annealed importance sampling
is to move in a chain from an easy-to-sample-from distribution, via a series of intermediate
distributions, through to the complicated posterior distribution. By annealing the distributions in
this way the parameter samples should hopefully come from representative areas of probability
mass in the posterior. The key to the annealed importance sampling procedure is to make use
of the importance weights gathered at all the distributions up to and including the ﬁnal posterior
distribution, in such a way that the ﬁnal estimate of the marginal likelihood is unbiased. A brief
description of the AIS procedure follows:
We deﬁne a series of inverse-temperatures{τ(k)}K
k=0 satisfying
0 = τ(0) <τ (1) <··· <τ (K−1) <τ (K) = 1 . (6.49)
We refer to temperatures and inverse-temperatures interchangeably throughout this section. We
deﬁne the function:
fk(θ) ≡p(θ |m)p(y |θ,m)τ(k) , k ∈{0,...,K }. (6.50)
Thus the set of functions {fk(θ)}K
k=0 form a series of unnormalised distributions which inter-
polate between the prior and posterior, parameterised by τ. We also deﬁne the normalisation
constants
Zk ≡
∫
dθ fk(θ) =
∫
dθ p(θ |m)p(y |θ,m)τ(k) , k ∈{0,...,K }. (6.51)
218
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
We note the following:
Z0 =
∫
dθ p(θ |m) = 1 (6.52)
from normalisation of the prior, and
ZK =
∫
dθ p(θ |m)p(y |θ,m) = p(y |m) , (6.53)
which is exactly the marginal likelihood that we wish to estimate. We can estimate ZK, or
equivalently ZK
Z0
, using the identity
p(y |m) = ZK
Z0
≡Z1
Z0
Z2
Z1
... ZK
ZK−1
=
K∏
k=1
Rk , (6.54)
Each of the Kratios in this expression can be individually estimated using importance sampling
(see section 1.3.6). The kth ratio, denoted Rk, can be estimated from a set of (not necessarily
independent) samples of parameters {θ(k,c)}c∈Ck which are drawn from the higher temperature
τ(k−1) distribution (the importance distribution), i.e. each θ(k,c) ∼ fk−1(θ), and the impor-
tance weights are computed at the lower temperature τ(k). These samples are used to construct
the Monte Carlo estimate for Rk:
Rk ≡ Zk
Zk−1
=
∫
dθ fk(θ)
fk−1(θ)
fk−1(θ)
Zk−1
(6.55)
≈ 1
Ck
∑
c∈Ck
fk(θ(k,c))
fk−1(θ(k,c))
, with θ(k,c) ∼ fk−1(θ) (6.56)
= 1
Ck
∑
c∈Ck
p(y |θ(k,c),m)τ(k)−τ(k−1) . (6.57)
Here, the importance weights are the summands in ( 6.56). The accuracy of each Rk depends
on the constituent distributions {fk(θ),fk−1(θ)}being sufﬁciently close so as to produce low-
variance weights. The estimate of ZK in (6.54) is unbiased if the samples used to compute each
ratio Rk are drawn from the equilibrium distribution at each temperature τ(k). In general we
expect it to be difﬁcult to sample directly from the forms fk(θ) in (6.50), and so Metropolis-
Hastings (Metropolis et al., 1953; Hastings, 1970) steps are used at each temperature to generate
the set of Ck samples required for each importance calculation in (6.57).
Metropolis-Hastings for discrete-variable models
In the discrete-variable graphical models covered in this chapter, the parameters are multino-
mial probabilities, hence the support of the Metropolis proposal distributions is restricted to the
219
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
simplex of probabilities summing to 1. At ﬁrst thought one might suggest using a Gaussian
proposal distribution in the softmax basis of the current parameters θ:
θi ≡ ebi
∑|θ|
j ebj
. (6.58)
Unfortunately an invariance exists: with βa scalar, the transformation b′
i ←bi+ β∀ileaves the
parameter θ unchanged. Therefore the determinant of the Jacobian of the transformation (6.58)
from the vector b to the vector θ is zero, and it is hard to construct a reversible Markov chain.
A different and intuitively appealing idea is to use a Dirichlet distribution as the proposal distri-
bution, with its mean positioned at the current parameter. The precision of the Dirichlet proposal
distribution at inverse-temperature τ(k) is governed by its strength, α(k), which is a free vari-
able to be set as we wish, provided it is not in any way a function of the sampled parameters.
A Metropolis-Hastings acceptance function is required to maintain detailed balance: if θ′ is the
sample under the proposal distribution centered around the current parameter θ(k,c), then the
acceptance function is:
a(θ′,θ(k,c)) = min
(
fk(θ′)
fk(θ(k,c))
Dir(θ(k,c) |θ′,α(k))
Dir(θ′ |θ(k,c),α(k))
, 1
)
, (6.59)
where Dir(θ |θ,α) is the probability density of a Dirichlet distribution with meanθ and strength
α, evaluated at θ. The next sample is instantiated as follows:
θ(k,c+1) =



θ′ if w<a (θ′,θ(k,c)) (accept)
θ(k,c) otherwise (reject) ,
(6.60)
where w ∼ U(0,1) is a random variable sampled from a uniform distribution on [0,1]. By
repeating this procedure of accepting or rejecting C′
k ≥ Ck times at the temperature τ(k),
the MCMC sampler generates a set of (dependent) samples {θ(k,c)}
C′
k
c=1. A subset of these
{θ(k,c)}c∈Ck , with |Ck|= Ck ≤C′
k, is then used as the importance samples in the computation
above (6.57). This subset will generally not include the ﬁrst few samples, as these samples are
likely not yet samples from the equilibrium distribution at that temperature.
An algorithm to compute all ratios
The entire algorithm for computing all K marginal likelihood ratios is given in algorithm
6.1. It has several parameters, in particular: the number of annealing steps, K; their inverse-
temperatures (the annealing schedule), {τ(k)}K
k=1; the parameters of the MCMC importance
sampler at each temperature {C′
k,Ck,α(k)}K
k=1, which are the number of proposed samples,
220
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
Algorithm 6.1: AIS. To compute all ratios {Rk}K
k=1 for the marginal likelihood estimate.
1. Initialise θini ∼ f0(θ) i.e. from the prior p(θ |m)
2. For k= 1 to K annealing steps
(a) Run MCMC at temperature τ(k−1) as follows:
i. Initialise θ(k,0) ←θini from previous temp.
ii. Generate the set {θ(k,c)}
C′
k
c=1 ∼ fk−1(θ) as follows:
A. For c= 1 to C′
k
Propose θ′ ∼ Dir(θ′ |θ(k,c−1),α(k))
Accept θ(k,c) ←θ′ according to (6.59) and (6.60)
End For
B. Store θini ←θ(k,C′
k)
iii. Store a subset of these {θ(k,c)}c∈Ck with |Ck|= Ck ≤C′
k
(b) Calculate Rk ≡ Zk
Zk−1
≊ 1
Ck
∑Ck
c=1
fk(θ(k,c))
fk−1(θ(k,c))
End For
3. Output {ln Rk}K
k=1 and ln ˆZK = ∑K
k=1 ln Rk as the approximation to ln ZK
the number used for the importance estimate, and the precision of the proposal distribution,
respectively.
Nota bene: In the presentation of AIS thus far, we have shown how to compute estimates of
Rk using a set, Ck, of importance samples (see equation (6.56)), chosen from the larger set, C′
k,
drawn using a Metropolis-Hastings sampling scheme. In the original paper by Neal (2001), the
size of the set Ck is exactly one, and it is only for this case that the validity of AIS as an unbiased
estimate has been proved. Because the experiments carried out in this chapter do in fact only
use Ck = |Ck|= 1 (as described in section 6.4.1), we stay in the realm of the proven result. It
is open research question to show that algorithm 6.1 is unbiased for Ck = |Ck|> 1 (personal
communication with R. Neal).
Algorithm 6.1 produces only a single estimate of the marginal likelihood; the variance of this es-
timate can be obtained from the results of several annealed importance samplers run in parallel.
Indeed a particular attraction of AIS is that one can take averages of the marginal likelihood es-
timates from a set ofGannealed importance sampling runs to form a better (unbiased) estimate:
[ZK
Z0
](G)
= 1
G
G∑
g=1
K(g)
∏
k=1
R(g)
k . (6.61)
221
VB Learning for DAG Structures 6.3. Estimating the marginal likelihood
However this computational resource might be better spent simulating a single chain with a
more ﬁnely-grained annealing schedule, since for each k we require each pair of distributions
{fk(θ),fk−1(θ)}to be sufﬁciently close that the importance weights have low variance. Or
perhaps the computation is better invested by having a coarser annealing schedule and taking
more samples at each temperature to ensure the Metropolis-Hastings sampler has reached equi-
librium. In Neal (2001) an in-depth analysis is presented for these and other similar concerns
for estimating the marginal likelihoods in some very simple models, using functions of the vari-
ance of the importance weights (i.e. the summands in ( 6.56)) as guides to the reliability of the
estimates.
In section 6.5.1 we discuss the performance of AIS for estimating the marginal likelihood of
the graphical models used in this chapter, addressing the speciﬁc choices of proposal widths,
number of samples, and annealing schedules used in the experiments.
6.3.6 Upper bounds on the marginal likelihood
This section is included to justify comparing the marginal likelihood to scores such as MAP and
ML. The following estimates based on the ML parameters and the posterior distribution over
parameters represent strict bounds on the true marginal likelihood of a model, p(y),
p(y) =
∫
dθ p(θ)p(y |θ) . (6.62)
(where we have omitted the dependence on mfor clarity).
We begin with the ML estimate:
p(y)ML =
∫
dθ δ (θ −θML)p(y |θ) (6.63)
which is the expectation of the data likelihood under a delta function about the ML parameter
setting. This is a strict upper bound only ifθML has found the global maximum of the likelihood.
This may not happen due to local maxima in the optimisation process, for example if the model
contains hidden variables and an EM-type optimisation is being employed.
The second estimate is that arising from the MAP estimate,
p(y)MAP =
∫
dθ δ (θ −θMAP)p(y |θ) (6.64)
which is the expectation of the data likelihood under a delta function about the MAP parameter
setting. However is not a strict upper or lower bound on the marginal likelihood, since this
depends on how the prior term acts to position the MAP estimate.
222
VB Learning for DAG Structures 6.4. Experiments
The last estimate, based on the posterior distribution over parameters, is for academic interest
only, since we would expect its calculation to be intractable:
p(y)post. =
∫
dθ p(θ |y)p(y |θ) . (6.65)
This is the expected likelihood under the posterior distribution over parameters. To prove that
(6.65) is an upper bound on the marginal likelihood, we use a simple convexity bound as follows:
p(y)post. =
∫
dθ p(θ |y)p(y |θ) (6.66)
=
∫
dθ p(θ)p(y |θ)
p(y) p(y |θ) by Bayes’ rule (6.67)
= 1
p(y)
∫
dθ p(θ) [p(y |θ)]2 (6.68)
≥ 1
p(y)
[∫
dθ p(θ)p(y |θ)
]2
by convexity of x2 (6.69)
= 1
p(y) [p(y)]2 = p(y) . (6.70)
As we would expect the integral (6.65) to be intractable, we could instead estimate it by taking
samples from the posterior distribution over parameters and forming the Monte Carlo estimate:
p(y) ≤p(y)post. =
∫
dθ p(θ |y)p(y |θ) (6.71)
≈ 1
C
C∑
c=1
p(y |θ(c)) (6.72)
where θ(c) ∼ p(θ |y), the exact posterior. Had we taken samples from the prior p(θ), this
would have yielded the true marginal likelihood, so it makes sense that by concentrating samples
in areas which give rise to high likelihoods we are over-estimating the marginal likelihood;
for this reason we would only expect this upper bound to be close for small amounts of data.
An interesting direction of thought would be to investigate the mathematical implications of
drawing samples from an approximate posterior instead of the exact posterior, such as that
obtained in a variational optimisation, which itself is arrived at from a lower bound on the
marginal likelihood; this could well give an even higher upper bound since the approximate
variational posterior is likely to neglect regions of low posterior density.
6.4 Experiments
In this section we experimentally examine the performance of the variational Bayesian proce-
dure in approximating the marginal likelihood for all the models in a particular class. We ﬁrst
describe the class deﬁning our space of hypothesised structures, then chose a particular mem-
223
VB Learning for DAG Structures 6.4. Experiments
ber of the class as the “true” structure, generate a set of parameters for that structure, and then
generate varying-sized data sets from that structure with those parameters. The task is then to
estimate the marginal likelihood of every data set under each member of the class, including
the true structure, using each of the scores described in the previous section. The hope is that
the VB lower bound will be able to ﬁnd the true model, based on its scoring, as reliably as the
gold standard AIS does. We would ideally like the VB method to perform well even with little
available data.
Later experiments take the true structure and analyse the performance of the scoring methods
under many different settings of the parameters drawn from the parameter prior for the true
structure. Unfortunately this analysis does not include AIS, as sampling runs for each and every
combination of the structures, data sets, and parameter settings would take a prohibitively large
amount of compute time.
A speciﬁc class of graphical model. We look at the speciﬁc class of discrete directedbipartite
graphical models, i.e. those graphs in which only hidden variables can be parents of observed
variables, and the hidden variables themselves have no parents. We further restrict ourselves
to those graphs which have just k = |H|= 2 hidden variables, and p = |V| = 4 observed
variables; both hidden variables are binary i.e. |sij|= 2 for j ∈H, and each observed variable
has cardinality |yij|= 5 for j ∈V.
The number of distinct graphs. In the class of bipartite graphs described above, with kdis-
tinct hidden variables and pobserved variables, there are 2kp possible structures, corresponding
to the presence or absence of a directed link between each hidden and each conditionally inde-
pendent observed variable. If the hidden variables are unidentiﬁable, which is the case in our
example model where they have the same cardinality, then the number of possible graphs is
reduced. It is straightforward to show in this example that the number of graphs is reduced from
22×4 = 256 down to 136.
The speciﬁc model and generating data. We chose the particular structure shown in ﬁgure
6.1, which we call the “true” structure. We chose this structure because it contains enough links
to induce non-trivial correlations amongst the observed variables, whilst the class as a whole
has few enough nodes to allow us to examine exhaustively every possible structure of the class.
There are only three other structures in the class which have more parameters than our chosen
structure; these are: two structures in which either the left- or right-most visible node has both
hidden variables as parents instead of just one, and one structure which is fully connected. As a
caveat, one should note that our chosen true structure is at the higher end of complexity in this
class, and so we might ﬁnd that scoring methods that do not penalise complexity do seemingly
better than naively expected.
224
VB Learning for DAG Structures 6.4. Experiments
yi1
si1 si2
yi2 yi3 yi4
i=1...n
Figure 6.1: The true structure that was used to generate all the data sets used in the experi-
ments. The hidden variables (top) are each binary, and the observed variables (bottom) are each
ﬁve-valued. This structure has 50 parameters, and is two links away from the fully-connected
structure. In total there are 136 possible distinct structures with two (identical) hidden variables
and four observed variables.
Evaluation of the marginal likelihood of all possible alternative structures in the class is done
for academic interest only; in practice one would embed different structure scoring methods in
a greedy model search outer loop (Friedman, 1998) to ﬁnd probable structures. Here, we are not
so much concerned with structure search per se, since a prerequisite for a good structure search
algorithm is an efﬁcient and accurate method for evaluating any particular structure. Our aim in
these experiments is to establish the reliability of the variational bound as a score, compared to
annealed importance sampling, and the currently employed asymptotic scores such as BIC and
Cheeseman-Stutz.
The parameters of the true model
Conjugate uniform symmetric Dirichlet priors were placed over all the parameters of the model,
that is to say in equation (6.4), λjlk = 1 ∀{jlk}. This particular prior was arbitrarily chosen for
the purposes of the experiments, and we do not expect it to inﬂuence our conclusions much. For
the network shown in ﬁgure 6.1 parameters were sampled from the prior, once and for all, to in-
stantiate a true underlying model, from which data was then generated. The sampled parameters
are shown below (their sizes are functions of each node’s and its parents’ cardinalities):
θ1 =
[
.12 .88
]
θ3 =
[
.03 .03. .64 .02 .27
.18 .15 .22 .19 .27
]
θ6 =
[
.10 .08 .43 .03 .36
.30 .14 .07 .04 .45
]
θ2 =
[
.08 .92
]
θ4 =


.10 .54 .07 .14 .15
.04 .15 .59 .05 .16
.20 .08 .36 .17 .18
.19 .45 .10 .09 .17


θ5 =


.11 .47 .12 .30 .01
.27 .07 .16 .25 .25
.52 .14 .15 .02 .17
.04 .00 .37 .33 .25


where {θj}2
j=1 are the parameters for the hidden variables, and {θj}6
j=3 are the parameters
for the remaining four observed variables. Recall that each row of each matrix denotes the
225
VB Learning for DAG Structures 6.4. Experiments
probability of each multinomial setting for a particular conﬁguration of the parents. Each row
of each matrix sums to one (up to rounding error). Note that there are only two rows for θ3 and
θ6 as both these observed variables have just a single binary parent. For variables 4 and 5, the
four rows correspond to the parent conﬁgurations (in order): {[1 1],[1 2],[2 1],[2 2]}.
Also note that for this particular instantiation of the parameters, both the hidden variable priors
are close to deterministic, causing approximately 80% of the data to originate from the [2 2]
setting of the hidden variables. This means that we may need many data points before the
evidence for two hidden variables outweighs that for one.
Incrementally larger and larger data sets were generated with these parameter settings, with
n∈{10,20,40,80,110,160,230,320,400,430,
480,560,640,800,960,1120,1280,2560,5120,10240}.
The items in the n = 10 data set are a subset of the n = 20 and subsequent data sets, etc.
The particular values of nwere chosen from an initially exponentially increasing data set size,
followed by inclusion of some intermediate data sizes to concentrate on interesting regions of
behaviour.
6.4.1 Comparison of scores to AIS
All 136 possible distinct structures were scored for each of the 20 data set sizes given above,
using MAP, BIC, CS, VB and AIS scores. Strictly speaking, MAP is not an approximation
to the marginal likelihood, but it is an upper bound (see section 6.3.6) and so is nevertheless
interesting for comparison.
We ran EM on each structure to compute the MAP estimate of the parameters, and from it com-
puted the BIC score as described in section 6.3.2. We also computed the BIC score including
the parameter prior, denoted BICp, which was obtained by including a termln p(ˆθ |m) in equa-
tion (6.26). From the same EM optimisation we computed the CS score according to section
6.3.3. We then ran the variational Bayesian EM algorithm with the same initial conditions to
give a lower bound on the marginal likelihood. For both these optimisations, random parameter
initialisations were used in an attempt to avoid local maxima — the highest score over three
random initialisations was taken for each algorithm; empirically this heuristic appeared to avoid
local maxima problems. The EM and VBEM algorithms were terminated after either 1000 it-
erations had been reached, or the change in log likelihood (or lower bound on the log marginal
likelihood, in the case of VBEM) became less than 10−6 per datum.
For comparison, the AIS sampler was used to estimate the marginal likelihood (see section
6.3.5), annealing from the prior to the posterior in K = 16384 steps. A nonlinear anneal-
226
VB Learning for DAG Structures 6.4. Experiments
ing schedule was employed, tuned to reduce the variance in the estimate, and the Metropolis
proposal width was tuned to give reasonable acceptance rates. We chose to have just a single
sampling step at each temperature (i.e. C′
k = Ck = 1 ), for which AIS has been proven to give
unbiased estimates, and initialised the sampler at each temperature with the parameter sample
from the previous temperature. These particular choices are explained and discussed in detail in
section 6.5.1. Initial marginal likelihood estimates from single runs of AIS were quite variable,
and for this reason several more batches of AIS runs were undertaken, each using a different
random initialisation (and random numbers thereafter); the total of G batches of scores were
averaged according to the procedure given in section 6.3.5, equation (6.61), to give the AIS(G)
score. In total, G= 5 batches of AIS runs were carried out.
Scoring all possible structures
Figure 6.2 shows the MAP, BIC, BICp, CS, VB and AIS(5) scores obtained for each of the 136
possible structures against the number of parameters in the structure. Score is measured on the
vertical axis, with each scoring method (columns) sharing the same vertical axis range for a
particular data set size (rows).
The horizontal axis of each plot corresponds to the number of parameters in the structure (as de-
scribed in section 6.3.2). For example, at the extremes there is one structure with 66 parameters
which is the fully connected structure, and one structure with 18 parameters which is the fully
unconnected structure. The structure that generated the data has exactly 50 parameters. In each
plot we can see that several structures can occupy the same column, having the same number of
parameters. This means that, at least visually, it is not always possible to unambiguously assign
each point in the column to a particular structure.
The scores shown here are those corrected for aliases — the difference between the uncorrected
and corrected versions is only just perceptible as a slight downward movement of the low pa-
rameter structures (those with just one or zero hidden variables), as these have a smaller number
of aliases S(see equation (6.26)).
In each plot, the true structure is highlighted by a ‘◦’ symbol, and the structure currently ranked
highest by that scoring method is marked with a ‘ ×’. We can see the general upward trend
for the MAP score which prefers more complicated structures, and the pronounced downward
trend for the BIC and BICp scores which (over-)penalise structure complexity. In addition one
can see that neither upward or downward trends are apparent for VB or AIS scores. Moreover,
the CS score does tend to show a downward trend similar to BIC and BICp, and while this
trend weakens with increasing data, it is still present at n = 10240 (bottom row). Although
not veriﬁable from these plots, we should note that for the vast majority of the scored structures
227
VB Learning for DAG Structures 6.4. Experiments
MAP BIC BICp CS VB AIS (5)
10
160
640
1280
2560
5120
10240
Figure 6.2: Scores for all 136 of the structures in the model class, by each of six scoring methods.
Each plot has the score (approximation to the log marginal likelihood) on the vertical axis,
with tick marks every 40 nats, and the number of parameters on the horizontal axis (ranging
from 18 to 66). The middle four scores have been corrected for aliases (see section 6.3.2).
Each row corresponds to a data set of a different size, n: from top to bottom we have n =
10,160,640,1280,2560,5120,10240. The true structure is denoted with a ‘ ◦’ symbol, and the
highest scoring structure in each plot marked by the ‘ ×’ symbol. Every plot in the same row
has the same scaling for the vertical score axis, set to encapsulate every structure for all scores.
For a description of how these scores were obtained see section 6.4.1.
228
VB Learning for DAG Structures 6.4. Experiments
and data set sizes, the AIS(5) score is higher than the VB lower bound, as we would expect (see
section 6.5.1 for exceptions to this observation).
The horizontal bands observed in the plots is an interesting artifact of the particular model used
to generate the data. For example, we ﬁnd on closer inspection some strictly followed trends:
all those model structures residing in the upper band have the ﬁrst three observable variables
(j = 3 ,4,5) governed by at least one of the hidden variables; and all those structures in the
middle band have the third observable (j = 4 ) connected to at least one hidden variable.
In this particular example, AIS ﬁnds the correct structure at n = 960 data points, but unfor-
tunately does not retain this result reliably until n = 2560 . At n = 10240 data points, BICp,
CS, VB and AIS all report the true structure as being the one with the highest score amongst
the other contending structures. Interestingly, BIC still does not select the correct structure, and
MAP has given a structure with sub-maximal parameters the highest score. The latter observa-
tion may well be due to local maxima in the EM optimisation, since for previous slightly smaller
data sets MAP chooses the fully-connected structure as expected. Note that as we did not have
intermediate data sets it may well be that, for example, AIS reliably found the structure after
1281 data points, but we cannot know this without performing more experiments.
Ranking of the true structure
A somewhat more telling comparison of the scoring methods is given by how they rank the true
structure amongst the alternative structures. Thus a ranking of 1 means that the scoring method
has given the highest marginal likelihood to the true structure.
Note that a performance measure based on ranking makes several assumptions about our choice
of loss function. This performance measure disregards information in the posterior about the
structures with lower scores, reports only the number of structures that have higher scores, and
not the amount by which the true structure is beaten. Ideally, we would compare a quantity that
measured the divergence of all structures’ posterior probabilities from the true posterior.
Moreover, we should keep in mind that at least for small data set sizes, there is no reason to
assume that the actual posterior over structures has the true structure at its mode. Therefore it is
slightly misleading to ask for high rankings at small data set sizes.
Table 6.1 shows the ranking of the true structure, as it sits amongst all the possible structures,
as measured by each of the scoring methods MAP, BIC, BICp, CS, VB and AIS (5); this is also
plotted in ﬁgure 6.3 where the MAP ranking is not included for clarity. Higher positions in the
plot correspond to better rankings.
229
VB Learning for DAG Structures 6.4. Experiments
n MAP BIC* BICp* CS* VB* BIC BICp CS VB AIS(5)
10 21 127 55 129 122 127 50 129 115 59
20 12 118 64 111 124 118 64 111 124 135
40 28 127 124 107 113 127 124 107 113 15
80 8 114 99 78 116 114 99 78 116 44
110 8 109 103 98 114 109 103 98 113 2
160 13 119 111 114 83 119 111 114 81 6
230 8 105 93 88 54 105 93 88 54 54
320 8 111 101 90 44 111 101 90 33 78
400 6 101 72 77 15 101 72 77 15 8
430 7 104 78 68 15 104 78 68 14 18
480 7 102 92 80 55 102 92 80 44 2
560 9 108 98 96 34 108 98 96 31 11
640 7 104 97 105 19 104 97 105 17 7
800 9 107 102 108 35 107 102 108 26 23
960 13 112 107 76 16 112 107 76 13 1
1120 8 105 96 103 12 105 96 103 12 4
1280 7 90 59 8 3 90 59 6 3 5
2560 6 25 17 11 11 25 15 11 11 1
5120 5 6 5 1 1 6 5 1 1 1
10240 3 2 1 1 1 2 1 1 1 1
Table 6.1: Ranking of the true structure by each of the scoring methods, as the size of the data
set is increased. Asterisks (*) denote scores uncorrected for parameter aliasing in the posterior.
Strictly speaking, the MAP score is not an estimate of the marginal likelihood. Note that these
results are from data generated from only one instance of parameters under the true structure’s
prior over parameters.
10
1
10
2
10
3
10
4
10
0
10
1
10
2
n
rank of true structure
AIS
VB
CS
BICp
BIC
Figure 6.3: Ranking given to the true structure by each scoring method for varying data set sizes
(higher in plot is better), by BIC, BICp, CS, VB and AIS(5) methods.
230
VB Learning for DAG Structures 6.4. Experiments
For small n, the AIS score produces a better ranking for the true structure than any of the other
scoring methods, which suggests that the AIS sampler is managing to perform the Bayesian
parameter averaging process more accurately than other approximations. For almost all n, VB
outperforms BIC, BICp and CS, consistently giving a higher ranking to the true structure. Of
particular note is the stability of the VB score ranking with respect to increasing amounts of
data as compared to AIS (and to some extent CS).
Columns in table 6.1 with asterisks (*) correspond to scores that are not corrected for aliases,
and are omitted from the ﬁgure. These corrections assume that the posterior aliases are well sep-
arated, and are valid only for large amounts of data and/or strongly-determined parameters. In
this experiment, structures with two hidden states acting as parents are given a greater correction
than those structures with only a single hidden variable, which in turn receive corrections greater
than the one structure having no hidden variables. Of interest is that the correction nowhere de-
grades the rankings of any score, and in fact improves them very slightly for CS, and especially
so for the VB score.
Score discrepancies between the true and top-ranked structures
Figure 6.4 plots the differences in score between the true structure and the score of the structure
ranked top by BIC, BICp, CS, VB and AIS methods. The convention used means that all the
differences are exactly zero or negative, measured from the score of the top-ranked structure
— if the true structure is ranked top then the difference is zero, otherwise the true structure’s
score must be less than the top-ranked one. The true structure has a score that is close to the
top-ranked structure in the AIS method; the VB method produces approximately similar-sized
differences, and these are much less on the average than the CS, BICp, and BIC scores. For a
better comparison of the non-sampling-based scores, see section 6.4.2, and ﬁgure 6.6.
Computation Time
Scoring all 136 structures at 480 data points on a 1GHz Pentium III processor took: 200 seconds
for the MAP EM algorithms required for BIC/BICp/CS, 575 seconds for the VBEM algorithm
required for VB, and 55000 seconds (15 hours) for a single run of the AIS algorithm (using
16384 samples as in the main experiments). All implementations were in M ATLAB . Given the
massive computational burden of the sampling method (approx 75 hours), which still produces
fairly variable scores when averaging over ﬁve runs, it does seem as though CS and VB are
proving very useful indeed. Can we justify the mild overall computational increase for VB? This
increase results from both computing differences between digamma functions as opposed to
ratios, and also from an empirically-observed slower convergence rate of the VBEM algorithm
as compared to the EM algorithm.
231
VB Learning for DAG Structures 6.4. Experiments
10
1
10
2
10
3
10
4
−60
−50
−40
−30
−20
−10
0
n
score difference
AIS
VB
CS
BICp
BIC
Figure 6.4: Differences in log marginal likelihood estimates (scores) between the top-ranked
structure and the true structure, as reported by BIC, BICp, CS, VB and AIS (5) methods. All
differences are exactly zero or negative: if the true structure is ranked top then the difference is
zero, otherwise the score of the true structure must be less than the top-ranked structure. Note
that these score differences are not per-datum scores, and therefore are not normalised for the
data n.
6.4.2 Performance averaged over the parameter prior
The experiments in the previous section used a single instance of sampled parameters for the
true structure, and generated data from this particular model. The reason for this was that, even
for a single experiment, computing an exhaustive set of AIS scores covering all data set sizes
and possible model structures takes in excess of 15 CPU days.
In this section we compare the performance of the scores over many different sampled param-
eters of the true structure (shown in ﬁgure 6.1). 106 parameters were sampled from the prior
(as done once for the single model in the previous section), and incremental data sets generated
for each of these instances as the true model. MAP EM and VBEM algorithms were employed
to calculate the scores as described in section 6.4.1. For each instance of the true model, calcu-
lating scores for all data set sizes used and all possible structures, using three random restarts,
for BIC/BICp/CS and VB took approximately2.4 and 4.2 hours respectively on an Athlon 1800
Processor machine, which corresponds to about 1.1 and 1.9 seconds for each individual score.
The results are plotted in ﬁgure 6.5, which shows the median ranking given to the true structure
by each scoring method, computed over 106 randomly sampled parameter settings. This plot
corresponds to a smoothed version of ﬁgure 6.3, but unfortunately cannot contain AIS averages
232
VB Learning for DAG Structures 6.4. Experiments
10
1
10
2
10
3
10
4
10
0
10
1
10
2
n
median rank of true structure
VB
CS
BICp
BIC
Figure 6.5: Median ranking of the true structure as reported by BIC, BICp, CS and VB methods,
against the size of the data set n, taken over 106 instances of the true structure.
% times that \than BIC* BICp* CS* CS* † BIC BICp CS CS †
VB ranks worse 16.9 30.2 31.8 32.8 15.1 29.6 30.9 31.9
same 11.1 15.0 20.2 22.1 11.7 15.5 20.9 22.2
better 72.0 54.8 48.0 45.1 73.2 55.0 48.2 45.9
Table 6.2: Comparison of the VB score to its competitors, using the ranking of the true structure
as a measure of performance. The table gives the percentage fraction of times that the true
structure was ranked lower, the same, and higher by VB than by the other methods (rounded to
nearest .1%). The ranks were collected from all 106 generated parameters and all 20 data set
sizes. Note that VB outperforms all competing scores, whether we base our comparison on the
alias-corrected or uncorrected (*) versions of the scores. The CS score annotated with †is an
improvement on the original CS score, and is explained in section 6.5.2.
for the computational reasons mentioned above. The results clearly show that for the most part
VB outperforms all other scores on this task by this measure although there is a region in which
VB seems to underperform CS, as measured by the median score.
Table 6.2 shows in more detail the performance of VB and its alias-uncorrected counterpart
VB* in terms of the number of times the score correctly selects the true model (i.e. ranks it
top). The data was collated from all 106 sampled true model structures, and all 20 data set sizes,
giving a total of 288320 structures that needed to be scored by each approximate method. We
see that VB outperforms the other scores convincingly, whether we compare the uncorrected
(left hand side of table) or corrected (right hand side) scores. The results are more persuasive
for the alias-corrected scores, suggesting that VB is beneﬁtting more from this modiﬁcation —
it is not obvious why this should be so.
233
VB Learning for DAG Structures 6.4. Experiments
10
1
10
2
10
3
10
4
−50
−45
−40
−35
−30
−25
−20
−15
−10
−5
0
n
median score difference
VB
CS
BICp
BIC
Figure 6.6: Median difference in score between the true and top-ranked structures, under BIC,
BICp, CS and VB scoring methods, against the size of the data set n, taken over 106 instances
of the true structure. Also plotted are the 40-60% intervals about the medians.
These percentages are likely to be an underestimate of the success of VB, since on close ex-
amination of the individual EM and VBEM optimisations, it was revealed that for several cases
the VBEM optimisation reached the maximum number of allowed iterations before it had con-
verged, whereas EM always converged. Generally speaking the VBEM algorithm was found to
require more iterations to reach convergence than EM, which would be considered a disadvan-
tage if it were not for the considerable performance improvement of VB over BIC, BICp and
CS.
We can also plot the smoothed version of ﬁgure 6.4 over instances of parameters of the true
structure drawn from the prior; this is plotted in ﬁgure 6.6, which shows the median difference
between the score of the true structure and the structure scoring highest under BIC, BICp, CS
and VB. Also plotted is the 40-60% interval around the median. Again, the AIS experiments
would have taken an unfeasibly large amount of computation time, and were not carried out.
We can see quite clearly here that the VB score of the true structure is generally much closer to
that of the top-ranked structure than is the case for any of the other scores. This observation in
itself is not particularly satisfying, since we are comparing scores to scores rather than scores to
exact marginal likelihoods; nevertheless it can at least be said that the dynamic range between
true and top-ranked structure scores by the VB method is much smaller than the range for the
other methods. This observation is also apparent (qualitatively) across structures in the various
plots in ﬁgure 6.2. We should be wary about the conclusions drawn from this graph comparing
VB to the other methods: a completely ignorant algorithm which gives the same score to all
234
VB Learning for DAG Structures 6.4. Experiments
10
1
10
2
10
3
10
4
10
0
10
1
10
2
n
highest rank of true structure
VB
CS
BICp
BIC
Figure 6.7: The highest ranking given to the true structure under BIC, BICp, CS and VB meth-
ods, against the size of the data set n, taken over 106 instances of the true structure. These
two traces can be considered as the results of the min operation on the rankings of all the 106
instances for each nin ﬁgure 6.5.
possible structures would look impressive on this plot, giving a score difference of zero for all
data set sizes.
Figures 6.7 and 6.8 show the best performance of the BIC, BICp, CS and VB methods over
the 106 parameter instances, in terms of the rankings and score differences. These plots can be
considered as the extrema of the median ranking and median score difference plots, and reﬂect
the bias in the score.
Figure 6.7 shows the best ranking given to the true structure by all the scoring methods, and it is
clear that for small data set sizes the VB and CS scores can perform quite well indeed, whereas
the BIC scores do not manage a ranking even close to these. This result is echoed in ﬁgure 6.8
for the score differences, although we should bear in mind the caveat mentioned above (that the
completely ignorant algorithm can do well by this measure).
We can analyse the expected performance of a naive algorithm which simply picks any structure
at random as the guess for the true structure: the best ranking given to the true model in a set
of 106 trials where a structure is chosen at random from the 136 structures is, on the average,
roughly 1.8. We can see in ﬁgure 6.7 that CS and VB surpass this for n >30 and n >40 data
points respectively, but that BICp and BIC do so only after 300 and 400 data points. However
we should remember that, for small data set sizes, the true posterior over structures may well
not have the true model at its mode.
235
VB Learning for DAG Structures 6.5. Open questions and directions
10
1
10
2
10
3
10
4
−25
−20
−15
−10
−5
0
n
smallest score difference
VB
CS
BICp
BIC
Figure 6.8: The smallest difference in score between the true and top-ranked structures, under
BIC, BICp, CS and VB methods, against the size of the data set n, taken over 106 instances of
the true structure. These two traces can be considered as the results of themax operation on the
all the 106 differences for each nin ﬁgure 6.6.
Lastly, we can examine the success rate of each score at picking the correct structure. Figure6.9
shows the fraction of times that the true structure is ranked top by the different scoring methods.
This plot echoes those results in table 6.2.
6.5 Open questions and directions
This section is split into two parts which discuss some related issues arising from the work in
this chapter. In section 6.5.1 we discuss some of the problems experienced when using the AIS
approach, and suggest possible ways to improve the methods used in our experiments. In section
6.5.2 we more thoroughly revise the parameter-counting arguments used for the BIC and CS
scores, and provide a method for estimating the complete and incomplete-data dimensionalities
in arbitrary models, and as a result form a modiﬁed score CS†.
6.5.1 AIS analysis, limitations, and extensions
The technique of annealed importance sampling is currently regarded as a state-of-the-art method
for estimating the marginal likelihood in discrete-variable directed acyclic graphical models
(personal communication with R. Neal, Z. Ghahramani and C. Rasmussen). In this section the
236
VB Learning for DAG Structures 6.5. Open questions and directions
10
1
10
2
10
3
10
40
0.1
0.2
0.3
0.4
0.5
0.6
0.7
n
success rate at selecting true structure
VB
CS
BICp
BIC
Figure 6.9: The success rate of the scoring methods BIC, BICp, CS and VB, as measured by the
fraction of 106 trials in which the true structure was given ranking 1 amongst the 136 candidate
structures, plotted as a function of the data set size. See also table 6.2 which presents softer
performance rates (measured in terms of relative rankings) pooled from all the data set sizes and
106 parameter samples.
AIS method is critically examined as a reliable tool to judge the performance of the BIC, CS
and VB scores.
The implementation of AIS has considerable ﬂexibility; for example the user is left to specify the
length, granularity and shape of the annealing schedules, the form of the Metropolis-Hastings
sampling procedure, the number of samples taken at each temperature, etc. These and other
parameters were described in section 6.3.5; here we clarify our choices of settings and discuss
some further ways in which the sampler could be improved. Throughout this subsection we use
AIS to refer to the algorithm which provides a single estimate of the marginal likelihood, i.e.
AIS(1).
First off, how can we be sure that the AIS sampler is reporting the correct answer for the
marginal likelihood of each structure? To be sure of a correct answer one should use as long
and gradual an annealing schedule as possible, containing as many samples at each temperature
as is computationally viable (or compare to a very long simple importance sampler). In the AIS
experiments in this chapter we always opted for a single sample at each step of the annealing
schedule, initialising the parameter at the next temperature at the last accepted sample, and en-
sured that the schedule itself was as ﬁnely grained as we could afford. This reduces the variables
at our disposal to a single parameter, namely the total number of samples taken in each run of
AIS, which is then directly related to the schedule granularity. Without yet discussing the shape
237
VB Learning for DAG Structures 6.5. Open questions and directions
10
2
10
3
10
4
10
5−7.8
−7.6
−7.4
−7.2
−7
−6.8
−6.6
−6.4
−6.2
−6
−5.8
length of annealing schedule, K
log marginal likelihood estimate / n
Figure 6.10: Logarithm of AIS estimates (vertical) of the marginal likelihood for different initial
conditions of the sampler (different traces) and different duration of annealing schedules (hori-
zontal), for the true structure withn= 480 data points. The top-most trace is that corresponding
to setting the initial parameters to the true values that generated the data. Shown are also the
BIC score (dashed) and the VB lower bound (solid).
of the annealing schedule, we can already examine the performance of the AIS sampler as a
function of the number of samples.
Figure 6.10 shows several AIS estimates of the marginal likelihood for the data set of size
n= 480 under the model having the true structure. Each trace is a result of initialising the AIS
sampler at a different position in parameter space sampled from the prior ( 6.4), except for the
top-most trace which is the result of initialising the AIS algorithm at the exact parameters that
were used to generate the data (which as the experimenter we have access to). It is important
to understand the abscissa of the plot: it is the number of samples in the AIS run and, given the
above comments, relates to the granularity of the schedule; thus the points on a particular trace
do not correspond to progress through the annealing schedule, but in fact constitute the results
of runs that are completely different other than in their common parameter initialisation.
Also plotted for reference are the VB and BIC estimates of the log marginal likelihood for this
data set under the true structure, which are not functions of the annealing duration. We know
that the VB score is a strict lower bound on the log marginal likelihood, and so those estimates
from AIS that consistently fall below this score must be indicative of an inadequate annealing
schedule shape or duration.
238
VB Learning for DAG Structures 6.5. Open questions and directions
For short annealing schedules, which are necessarily coarse to satisfy the boundary require-
ments on τ (see equation ( 6.49)), it is clear that the AIS sampling is badly under-estimating
the log marginal likelihood. This can be explained simply because the rapid annealing sched-
ule does not give the sampler time to locate and exploit regions of high posterior probability,
forcing it to neglect representative volumes of the posterior mass; this conclusion is further sub-
stantiated since the AIS run started from the true parameters (which if the data is representative
of the model should lie in a region of high posterior probability) over-estimates the marginal
likelihood, because it is prevented from exploring regions of low probability. Thus for coarse
schedules of less than about K = 1000 samples, the AIS estimate of the log marginal likeli-
hood seems biased and has very high variance. Note that the construction of the AIS algorithm
guarantees that the estimates of the marginal likelihood are unbiased, but not necessarily the log
marginal likelihood.
We see that all runs converge for sufﬁciently long annealing schedules, with AIS passing the
BIC score at about 1000 samples, and the VB lower bound at about 5000 samples. Thus,
loosely speaking, where the AIS and VB scores intersect we can consider their estimates to
be roughly equally reliable. We can then compare their computational burdens and make some
statement about the advantage of one over the other in terms of compute time. At n = 480
the VB scoring method requires about 1.5sto score the structure, whereas AIS at n= 480 and
K = 2 13 requires about 100s; thus for this scenario VB is 70 times more efﬁcient at scoring the
structures (at its own reliability).
In this chapter’s main experiments a value of K = 2 14 = 16384 steps was used, and it is clear
from ﬁgure 6.10 that we can be fairly sure of the AIS method reporting a reasonably accurate
result at this value of K, at least for n = 480 . However, how would we expect these plots to
look for larger data sets in which the posterior over parameters is more peaky and potentially
more difﬁcult to navigate during the annealing?
A good indicator of the mobility of the Metropolis-Hastings sampler is the acceptance rate of
proposed samples, from which the representative set of importance weights are computed (see
(6.60)). Figure 6.11 shows the fraction of accepted proposals during the annealing run, averaged
over AIS scoring of all 136 possible structures, plotted against the size of the data set, n; the
error bars are the standard errors of the mean acceptance rate across scoring all structures. We
can see that atn= 480 the acceptance rate is rarely below 60%, and so one would indeed expect
to see the sort of convergence shown in ﬁgure 6.10. However for the larger data sets the accep-
tance rate drops to 20%, implying that the sampler is having considerable difﬁculty obtaining
representative samples from the posterior distributions in the annealing schedule. Fortunately
this drop is only linear in the logarithm of the data size. For the moment, we defer discussing
the temperature dependence of the acceptance rate, and ﬁrst consider combining AIS sampling
runs to reduce the variance of the estimates.
239
VB Learning for DAG Structures 6.5. Open questions and directions
10
1
10
2
10
3
10
40
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
n
acceptance fraction
Figure 6.11: Acceptance rates of the Metropolis-Hastings proposals along the entire annealing
schedule, for one batch of AIS scoring of all structures, against the size of the data set, n. The
dotted lines are the sample standard deviations across all structures for each n.
One way of reducing the variance in our estimate of the marginal likelihood is to pool the results
of several AIS samplers run in parallel according to the averaging in equation (6.61). Returning
to the speciﬁc experiments reported in section 6.4, table 6.3 shows the results of running ﬁve
AIS samplers in parallel with different random seeds on the entire class of structures and data
set sizes, and then using the resulting averaged AIS estimate, AIS(5), as a score for ranking the
structures. In the experiments it is the performance of these averaged scores that are compared
to the other scoring methods: BIC, CS and VB. To perform ﬁve runs took at least 40 CPU days
on an Athlon 1800 Processor machine.
By examining the reported AIS scores, both for single and pooled runs, over the 136 structures
and 20 data set sizes, and comparing them to the VB lower bound, we can see how often AIS
violates the lower bound. Table 6.4 shows the number of times the reported AIS score is below
the VB lower bound, along with the rejection rates of the Metropolis-Hastings sampler that was
used in the experiments (which are also plotted in ﬁgure 6.11). From the table we see that
for small data sets the AIS method reports “valid” results and the Metropolis-Hastings sampler
is accepting a reasonable proportion of proposed parameter samples. However at and beyond
n= 560 the AIS sampler degrades to the point where it reports “invalid” results for more than
half the 136 structures it scores. However, since the AIS estimate is noisy and we know that
the tightness of the VB lower bound increases with n, this criticism could be considered too
harsh — indeed if the bound were tight, we would expect the AIS score to violate the bound
on roughly 50% of the runs anyway. The lower half of the table shows that, by combining AIS
estimates from separate runs, we obtain an estimate that violates the VB lower bound far less
240
VB Learning for DAG Structures 6.5. Open questions and directions
n AIS(1)
#1
AIS(1)
#2
AIS(1)
#3
AIS(1)
#4
AIS(1)
#5 AIS(5)
10 27 38 26 89 129 59
20 100 113 88 79 123 135
40 45 88 77 5 11 15
80 10 47 110 41 95 44
110 1 50 8 2 62 2
160 33 2 119 31 94 6
230 103 25 23 119 32 54
320 22 65 51 44 42 78
400 89 21 1 67 10 8
430 29 94 21 97 9 18
480 2 42 14 126 18 2
560 47 41 7 59 7 11
640 12 10 23 2 23 7
800 7 3 126 101 22 23
960 1 4 1 128 8 1
1120 3 53 3 37 133 4
1280 76 2 50 7 12 5
2560 1 1 4 1 1 1
5120 12 1 24 2 16 1
10240 1 1 2 12 1 1
Table 6.3: Rankings resulting from averaging batches of AIS scores. Each one of the ﬁve
columns correspond to a different initialisation of the sampler, and gives the rankings resulting
from a single run of AIS for each of the 136 structures and 20 data set size combinations.
The last column is the ranking of the true structure based on the mean of the AIS marginal
likelihood estimates from all ﬁve runs of AIS of each structure and data set size (see section
6.3.5 for averaging details).
n 10 ... 560 640 800 960 1120 1280 2560 5120 10240
single
#AIS(1)<VB* ≤5.7 12.3 8.5 12.3 10.4 17.0 25.5 53.8 71.7
#AIS(1)<VB ≤7.5 15.1 9.4 14.2 12.3 20.8 31.1 59.4 74.5
% M-H rej. <40.3 41.5 43.7 45.9 47.7 49.6 59.2 69.7 79.2
averaged
#AIS(5)<VB* 0 0.0 0.0 0.0 0.0 0.7 3.7 13.2 50.0
#AIS(5)<VB ≤1.9 0.0 0.0 0.0 1.5 2.2 5.1 19.9 52.9
Table 6.4: AIS violations: for each size data set, n, we show the percentage of times, over the
136 structures, that a particular single AIS run reports marginal likelihoods below the VB lower
bound. These are given for the VB scores that are uncorrected (*) and corrected for aliases.
Also shown are the average percentage rejection rates of the Metropolis-Hastings sampler used
to gather samples for the AIS estimates. The bottom half of the table shows the similar violations
by the AIS score that are made from averaging the estimates of marginal likelihoods from ﬁve
separate runs of AIS (see section 6.3.5). Note that the Metropolis-Hastings rejection rates are
still just as high for each of the individual runs (not given here).
241
VB Learning for DAG Structures 6.5. Open questions and directions
10
1
10
2
10
3
10
40
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8acceptance fraction
n
1st
2nd
3rd
4th
Figure 6.12: Acceptance rates of the Metropolis-Hastings proposals for each of four quarters of
the annealing schedule, for one batch of AIS scoring of all structures, against the size of the data
set, n. Standard errors of the means are omitted for clarity.
often, and as expected we see the 50% violation rate for large amounts of data. This is a very
useful result, and obviates to some extent the Metropolis-Hastings sampler’s deﬁciency in all
ﬁve runs.
However, considering for the moment a single AIS run, for large data set sizes the VB bound
is still violated an unacceptable number of times, suggesting that the Metropolis-Hastings pro-
posals are simply not adequate for these posterior landscapes. This suggests a modiﬁcation to
the proposal mechanism, outlined below. Diagnostically speaking, this hopefully has served as
a good example of the use of readily-computable VB lower bounds for evaluating the reliability
of the AIS method post hoc.
Let us return to examining why the sampler is troubled for large data set sizes. Figure 6.12
shows the fraction of accepted Metropolis-Hastings proposals during each of four quarters of
the annealing schedule used in the experiments. The rejection rate tends to increase moving from
the beginning of the schedule (the prior) to the end (the posterior), the degradation becoming
more pronounced for large data sets. This is most probably due to the proposal width remaining
unchanged throughout all the AIS implementations: ideally one would use a predetermined
sequence of proposal widths which would be a function of the amount of data, n, and the
position along the schedule. This would hopefully eliminate or at least alleviate the pronounced
decrease in acceptance rate across the four quarters, but would also cause each individual trace
to not drop so severely with n.
242
VB Learning for DAG Structures 6.5. Open questions and directions
We can use a heuristic argument to roughly predict the optimal proposal width to use for the AIS
method. From mathematical arguments outlined in sections 1.3.2 and 1.3.4, the precision of the
posterior distribution over parameters is approximately proportional to the size of the data setn.
Furthermore, the distribution being sampled from at stepkof the AIS schedule is effectively that
resulting from a fraction τ(k) of the data. Therefore these two factors imply that the width of
the Metropolis-Hastings proposal distribution should be inversely proportional to
√
nτ(k). In
the case of multinomial variables, since the variance of a Dirichlet distribution is approximately
inversely proportional to the strength, α, (see appendix A), then the optimal strength of the
proposal distribution should be αopt ∝nτ(k) if its precision is to match the posterior precision.
Note that we are at liberty to set these proposal precisions arbitrarily beforehand without causing
the sampler to become biased.
We have not yet discussed the shape of the annealing schedule: should the inverse-temperatures
{τ(k)}K
k=1 change linearly from 0 to 1, or follow some other function? The particular annealing
schedule in these experiments was chosen to be nonlinear, lingering at higher temperatures for
longer than at lower temperatures, following the relationship
τ(k) = eτk/K
1 −k/K+ eτ
k∈{0,...,K }, (6.73)
with eτ set to 0.2 . For any setting of eτ > 0, the series of temperatures is monotonic and the
initial and ﬁnal temperatures satisfy (6.49):
τ(0) = 0 , and τ(K) = 1 . (6.74)
For large eτ, the schedule becomes linear. This is plotted for different values of eτ in ﬁgure
6.13. The particular value of eτ was chosen to reduce the degree of hysteresis in the annealing
ratios, as discussed below.
Hysteresis in the annealing ratios
As presented in section 6.3.5 and algorithm 6.1, the algorithm for computing each and every
marginal likelihood ratio in ( 6.54) did so in a forward manner, carrying over the parameter
setting θini from the calculation of the previous ratio to initialise the sampling procedure for
calculating the next ratio. However, whilst it makes sense to move from higher to lower tem-
peratures to avoid local maxima in the posterior in theory, the ﬁnal estimate of the marginal
likelihood is unbiased regardless of the order in which the ratios are tackled. In particular, we
can run the AIS algorithm in the reverse direction, starting from the posterior and warming
the system to the prior, calculating each ratio exactly as before but using the last sample from
the lower temperature as an initialisation for the sampling at the next higher temperature in the
schedule (note that by doing this we arenot inverting the fractions appearing in equation (6.54)).
243
VB Learning for DAG Structures 6.5. Open questions and directions
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.01
0.1
0.2
0.4
1
10
k/K
t(k)
Figure 6.13: Non-linear AIS annealing schedules, plotted for six different values of eτ. In the
experiments performed in this chapter, eτ = 0 .2.
What can this reverse procedure do for us? If we look at ﬁgure 6.10 again, we can see that for
any random parameter initialisation the reported marginal likelihood is much more often than
not an underestimate of the true value. This is because for coarse annealing schedules we are
unlikely to locate regions of high posterior probability by the time the system is quenched. If we
were then to run the AIS algorithm in a reverse direction, starting from where we had ﬁnished
the forward pass, we would expect on average to report a higher marginal likelihood than that
just reported by the forward pass, simply because the sampler has had longer to explore the high
probability regions.
A logical conclusion is that if the forward and reverse passes yield very different values for
the marginal likelihood, then we have most likely used too short an annealing schedule. And
furthermore, since the marginal likelihood estimates are constructed from the product of many
ratios of marginal likelihoods, we can use the discrepancies between the ratios calculated on the
forward and reverse passes to choose temperature regions where more sampling is required, and
dilate the annealing schedules in these regions accordingly. Of course we should remember that
these discrepancies are stochastic quantities, and so we should modify the schedule based on
averaged discrepancies over several runs.
This heuristic analysis was used when designing the shape and granularity of the annealing
schedule, and we found that more time was required at higher and intermediate temperatures
at the expense of lower temperatures. An area of future research is to formalise and more
fully investigate this and related arguments. For example, it would be useful to characterise the
dependence of the degree of hysteresis along the schedule for different settings of eτ.
244
VB Learning for DAG Structures 6.5. Open questions and directions
6.5.2 Estimating dimensionalities of the incomplete and complete-data models
The BICp, BIC and CS approximations take the limit of the Laplace approximation as the
amount of data tends to inﬁnity, and result in scores that depend on the dimensionalities of
the incomplete and complete models, d and d′ respectively. In the experiments in this chap-
ter, for BIC dwas calculated using a simple counting argument (see equation ( 6.24) in section
6.3.2), and for CS d and d′ were assumed to be equal, which is the assumption made in the
original implementation of Cheeseman and Stutz (1996).
In models that have no hidden variables, the value of drequired for the BIC approximation can
usually be arrived at by adding together the degrees of freedom in each parameter, taking care
to take into consideration any parameter degeneracies. However, in models that do have hidden
variables the number of free parameters in the incomplete model is much less than that in the
complete model. This is because the full effect of each hidden variable cannot always be fully
manifest in the functions produced on the observed variables. This situation can be seen in the
following discrete example: imagine the model consisting of a single k-valued hidden variable
which is the (only) parent of a p-valued observed variable. The naive counting argument would
return the complete dimensionality as d′ = ( k−1) + ( p−1) ×k. However, the incomplete
dimensionality can be no more than d= ( p−1), as a model with this many degrees of freedom
can exactly model any observed set of counts of the observed variable.
In a general setting, deducing the complete and incomplete model dimensionalities can be com-
plicated (see, for example, Settimi and Smith, 1998; Koˇcka and Zhang, 2002), since it involves
computing the rank of the Jacobian of the transformation for parameters from incomplete to
complete models. Geiger et al. (1996) describe a method by which dcan be computed in dis-
crete DAGs, by diagonalising the Jacobian symbolically; they also present a theorem that guar-
antees that a randomised version of the symbolic operation is viable as well. Unfortunately their
approach seems difﬁcult to implement efﬁciently on an arbitrary topology discrete DAG, since
both symbolic and randomised versions require diagonalisation. Furthermore it is not clear how,
if at all, it can be transferred to DAGs containing continuous variables with arbitrary mappings
between the complete and incomplete data models.
For the purposes of this chapter, we have used a simple method to estimate the dimensional-
ities of each model in our class. It is based on analysing the effect of random perturbations
to the model’s parameters on the complete and incomplete-data likelihoods. The procedure is
presented in algorithm 6.2, and estimates the number of effective dimensions,dand d′, by com-
puting the rank of a perturbation matrix. Since the rank operation attempts to ﬁnd the number
of linearly independent rows of the matrices C and C′, the random ϵ-perturbations must be
small enough such that the change in the log likelihoods are linear with ϵ. Also, the number of
samples nshould be chosen to be at least as large as the total number of parameters possible in
245
VB Learning for DAG Structures 6.5. Open questions and directions
Algorithm 6.2: d(m),d′(m): To estimate incomplete and complete model parameter dimen-
sionalities.
1. For each structure m
(b) Obtain θMAP using the MAP EM algorithm (section 6.3.1).
(a) Obtain a representative set of all possible observed data {yi}n
i=1.
(d) Randomly (spherically) ϵ-perturb ˆθMAP Rtimes, to form {ˆθ1,..., ˆθR}.
(e) Compute the matrix C(n×R) : Cir = ln p(yi|ˆθr) for all (i,r).
Estimate d(m) = rank( C) −1.
(f) Compute the matrix C′(n×R) : C′
ir = ln p(si,yi|ˆθr) for all (i,r),
where si is a randomly instantiated hidden state.
Estimate d′(m) = rank( C′) −1.
End For
the model (as the rank of a matrix can be no more than the smaller of the number of rows or
columns), and preferably several times this for reliable estimates.
This procedure was found to give reasonable results when carried out on all the model structures
used in this chapter, with a randomly generated data set of sizen= 1000 and R= 100 . Without
listing all the results, it sufﬁces to say that: for all structuresd≤d′ ≤d+2, and for the majority
of structures d′ = d+|H|— that is to say a further degree of freedom is provided for each binary
hidden variable (of which there are at most 2) on top of the incomplete dimensionality. There
are some structures for which the discrepancyd′ −dis smaller than 2, which is not as we would
expect.
There may be several reasons for this discrepancy. First the random perturbations may not have
explored certain directions from the MAP estimate, and thus the algorithm could have reported
a lower dimensionality than true (unlikely). Second, the data y only represented a subset of all
possible conﬁgurations (almost certainly since there are 54 possible realisations and 1000 data
points are generated randomly), and therefore the effective dimensionality drops.
These results support the use of a more accurate CS†score — see equation (6.30), which mod-
iﬁes the score by adding a term (d′ −d)/2 ·ln n. The effect of this is to raise the scores for
models with 2 hidden variables byln n, raise those with just 1 hidden variable by1/2 ·ln n, and
leave unchanged the single model with no hidden states.
Table 6.5 shows the improvement (in terms of ranking) of the more accurate CS †over the
original CS approximation, bringing it closer to the performance of the VB score. The table
shows the number of times in the 106 samples (see experiments in section 6.4 above) that the
246
VB Learning for DAG Structures 6.6. Summary
n BIC BICp CS CS † VB
10 0 0 0 0 0
20 0 0 0 0 0
40 0 0 0 0 0
80 0 0 0 1 1
110 0 0 0 0 1
160 0 0 1 2 3
230 0 1 3 5 6
320 0 2 8 10 12
400 1 5 8 9 11
430 1 6 10 10 11
480 3 7 12 12 15
560 3 8 14 16 18
640 5 11 14 17 23
800 7 15 22 23 29
960 9 18 28 33 36
1120 11 19 32 33 40
1280 15 24 38 41 48
2560 35 41 59 62 66
5120 56 63 76 76 80
10240 73 79 82 83 84
Table 6.5: Number of times (out of 106) that each score selects the true structure. Shown are
the performance of the original BIC, BICp, CS and VB scores, all corrected for aliasing, and
also shown is the CS †score, resulting from (further) correcting CS for the difference between
complete and incomplete data model dimensionalities.
score successfully selected the true model structure. Is it clear that CS †is an improvement
over CS, suggesting that the assumption made above is true. However, we should interpret this
experiment with some care, because our original choice of the true model having two hidden
variables may be masking a bias in the altered score; it would make sense to perform similar
experiments choosing a much simpler model to generate the data.
The improvement in performance of the CS†score, averaged over all data set sizes and all 106
generated parameter sets can be see in table6.2 (page 233), where it is compared alongside BIC,
CS and VB. It can be seen that VB still performs better. Further veriﬁcation of this result will
be left to future work.
6.6 Summary
In this chapter we have presented various scoring methods for approximating the marginal likeli-
hood of discrete directed graphical models with hidden variables. We presented EM algorithms
for ML and MAP parameter estimation, showed how to calculate the asymptotic criteria of BIC
and Cheeseman-Stutz, derived the VBEM algorithm for approximate Bayesian learning which
247
VB Learning for DAG Structures 6.6. Summary
maintains distributions over the parameters of the model and has the same complexity as the EM
algorithm, and presented a (somewhat impoverished) AIS method designed for discrete-variable
DAGs.
We have shown that VB consistently outperforms BIC and CS, and that VB performs respec-
tively as well as and more reliably than AIS for intermediate and large sizes of data. The AIS
method has very many parameters to tune and requires extensive knowledge of the model do-
main to design efﬁcient and reliable sampling schemes and annealing schedules. VB on the
other hand has not a single parameter to set or tune, and can be applied without any expert
knowledge, at least in the class of singly-connected discrete-variable DAGs with Dirichlet pri-
ors which we have considered in this chapter. Section 6.5.1 discussed several ways in which
the AIS method could be improved, for example by better matching the Metropolis-Hastings
proposal distributions to the annealed posterior; in fact a method based on slice sampling should
be able to adapt better to the annealing posterior with little or no expert knowledge of the shape
of the annealed posterior (Neal, 2003).
It may be that there exists a better AIS scheme than sampling in parameter space. To be more
speciﬁc, for any completion of the data the parameters of the model can be integrated out
tractably (at least for the class of models examined in this chapter); thus an AIS scheme which
anneals in the space of completions of the data may be more efﬁcient than the current scheme
which anneals in the space of parameters (personal communication with R. Neal). However,
this latter scheme may only be efﬁcient for models with little data compared to the number of
parameters, as the sampling space of all completions increases linearly with the amount of data.
This avenue of research is left to further work.
This chapter has presented a novel application of variational Bayesian methods to discrete
DAGs. In the literature there have been other attempts to solve this long-standing model se-
lection problem. For example the structural EM algorithm of Friedman (1998) uses a structure
search algorithm which uses a scoring algorithm very similar to the VBEM algorithm presented
here, except that for tractability the distribution over θ is replaced by the MAP estimate, θMAP.
We have shown here how the VB framework enables us to use the entire distribution overθ for
inference of the hidden variables.
In chapter 2 we proved that the Cheeseman-Stutz score is in fact a lower bound on the marginal
likelihood and, more importantly, we proved that there exists a construction which is guaranteed
to produce a variational Bayesian lower bound that is at least as tight as the Cheeseman-Stutz
score (corollary 2.5 to theorem 2.3, page 79). This construction builds a variational Bayesian
approximation using the same MAP parameter estimate used to obtain the CS score. However,
we did not use this construction in our experiments, and ran both the MAP EM and VB opti-
misations independently of each other. As a result we cannot guarantee that the VB bound is
in all runs tighter than the CS bound, as the dynamics of the optimisations for MAP learning
248
VB Learning for DAG Structures 6.6. Summary
and VB learning may in general lead even identically initialised algorithms to different optima
in parameter space (or parameter distribution space). Nevertheless we have still seen improve-
ment in terms of ranking of the true structure by VB as compared to CS. A tighter bound on
the marginal likelihood does not necessarily directly imply that we should have better structure
determination, although it certainly suggests this and is supported by the experimental results.
Empirically, the reader may be interested to know that the VB lower bound was observed to be
lower than the CS score in only 173 of the 288320 total scores calculated (about 0.06%). If the
construction derived in corollary 2.5 had been used then this number of times would of course
be exactly zero.
249
Chapter 7
Conclusion
7.1 Discussion
In this thesis we have shown how intractable Bayesian learning, inference, and model selection
problems can be tackled using variational approximations. We have described a general frame-
work for variational Bayesian learning and shown how it can be applied to several models of
interest. We have demonstrated that it is an efﬁcient and trustworthy approximation as compared
to other more traditional approaches. Before summarising the contributions of this thesis, we
spend the next few paragraphs discussing some of the evolving directions for model selection
and variational Bayes, including the use of inﬁnite models, inferring causal relationships using
the marginal likelihood, other candidates for approximating the marginal likelihood, and lastly
automated algorithm derivation procedures. These areas are expected to be active and fruitful
future research directions. We conclude in section7.2 with a summary of the main contributions
of the thesis.
Inﬁnite models
In this thesis we have focused on Bayesian learning in models that can be speciﬁed using a
ﬁnite number of parameters. However, there are powerful arguments for entertaining models
with inﬁnitely many parameters, or at least as complex models as can be handled computation-
ally. The process of Bayesian inference yields a unique answer. That is to say, given our prior
beliefs, on observing some data all inference is automatic and there is one and only one answer
to any prediction of the model. The problems of under- or overﬁtting by using too simple or
too complex a model are simply not a concern if we integrate over all uncertain variables in
the model, since applying Bayes’ rule correctly at every step is guaranteed to result in coherent
and optimal inferences given the prior beliefs. In this way the problem of model selection is
250
Conclusion 7.1. Discussion
no longer an issue, because the inﬁnite model can entertain a continuum of models and average
with respect to all of these simultaneously. This approach to modelling is discussed in Neal
(1996) where, for example, neural networks with an inﬁnite number of hidden units are shown
(theoretically and empirically) to produce sensible predictions, and on some data sets state-of-
the-art performance. In general it is difﬁcult and sometimes impossible to entertain the limit of
an inﬁnite model, except where the mathematics lends itself to analytically tractable solutions
— this is often the case for mixture models. Examples of Bayesian learning with inﬁnite mod-
els include: the neural networks mentioned above, inﬁnite mixtures of Gaussians ( Rasmussen,
2000), inﬁnite hidden Markov models (Beal et al., 2002), and inﬁnite mixtures of Gaussian pro-
cess experts (Rasmussen and Ghahramani, 2002). The basic idea of examining the inﬁnite limit
of ﬁnite models can be applied to a host of other as yet unexplored models and applications.
Unfortunately, a major drawback for these inﬁnite models is that inference is generally in-
tractable, and one has to resort to Monte Carlo sampling methods which can be computationally
costly. Also, representing an inﬁnite number of components in a mixture model, for example,
can quickly become cumbersome; even elaborate Markov chain Monte Carlo approaches be-
come very inefﬁcient in models with many parameters. One further disadvantage of employing
inﬁnite models is that it is often difﬁcult to ﬁnd ways of encapsulating prior expert knowledge
into the model. Methods such as examining the properties of data drawn from speciﬁc prior
settings are illuminating but not always entirely satisfactory for designing the prior to articulate
one’s beliefs.
An alternative to grappling with the conceptual and implementational problems of inﬁnite mod-
els is then to restrict ourselves to performing model inference, or selection amongst a ﬁnite set
of ﬁnite-size models. Each individual model is then manageable and often simpler to inter-
pret in terms of its structure. On the basis of the marginal likelihood we can obtain posterior
distributions over the different candidate models. The problems discussed in this thesis have
emphasised these model selection and structure learning tasks, as well as attempting to obtain
full posterior distributions over model structures. We have examined a selection of statistical
models, all of which contained hidden variables which cause the marginal likelihood computa-
tion to be intractable, and tackled this intractability using variational methods.
Bethe, Kikuchi, and cluster-variation methods
Variational Bayes, as described in this thesis, is just one type of variational approach that could
be used to approximate Bayesian inference. It assumes simple forms for the posterior distribu-
tions over hidden variables and parameters, and then uses these forms to construct lower bounds
on the marginal likelihood that are tractable. Algorithms for inference and learning are then de-
rived as a result of optimising this lower bound by iteratively updating the parameters of these
simpliﬁed distributions. Most of this thesis has concentrated on the ease with which the model
251
Conclusion 7.1. Discussion
parameters can be included in the set of uncertain variables to infer and integrate over, at least
for the sub-class of conjugate-exponential models.
A promising alternative direction is to explore the Bethe and Kikuchi family of variational meth-
ods (Yedidia et al., 2001), sometimes called cluster-variational methods, which may be more
accurate but do not provide the assurance of being bounds. These re-express the negative log
marginal likelihood as a “free energy” from statistical physics, and then approximate the (in-
tractable) entropy of the posterior distribution over latent variables by neglecting high order
terms. In the Bethe approximation, the entropy is approximated with an expression which de-
pends only on functions of single variables and pairs of variables. There are several procedures
for minimising the Bethe free energy as a functional of the approximate posterior distributions
to obtain estimates of the marginal likelihood. It turns out that for singly-connected graphs the
ﬁxed point equations that result from iterative minimisation of this free energy with respect to
the single and pairwise functions correspond exactly to the messages that are passed in the junc-
tion tree and sum-product algorithms. Thus the Bethe free energy is exact for singly-connected
graphs (trees). Interestingly, it has recently been shown that the belief propagation algorithm,
even when run on multiply-connected graphs (i.e. ‘loopy’ graphs), has stable ﬁxed points at the
minima of the Bethe free energy ( Heskes, 2003). While belief propagation on loopy graphs is
not guaranteed to converge, it often works well in practice, and has become the standard ap-
proach to decoding state-of-the-art error-correcting codes. Furthermore, convergent algorithms
for minimising the Bethe free energy have recently been derived (Yuille, 2001; Welling and Teh,
2001). There are other related methods, such as expectation propagation (EP, Minka, 2001a),
approximations which observe higher order correlations in the variables ( Leisink and Kappen,
2001), and other more elaborate variational schemes for upper bounds on partition functions
(Wainwright et al., 2002).
The question remains open as to whether these methods can be readily applied to Bayesian
learning problems. One can view Bayesian learning as simply treating the parameters as hidden
variables, and so every method that has been shown to be successful for inference over hidden
variables should also do well for integrating over parameters. However, there have been few
satisfactory examples of Bayesian learning using any of the other methods described above, and
this is an important direction for future research.
Inferring causal relationships
Most research in statistics has focused on inferring probabilistic dependencies between model
variables, but more recently people have begun to investigate the more challenging and contro-
versial problem of inferring causality. Causality can be understood statistically as a relationship
s →twhich is stable regardless of whether swas set through intervention / experimental ma-
nipulation or it occurred randomly. An example of this is smoking ( s) causing yellowing of
252
Conclusion 7.1. Discussion
the teeth ( t). Painting the teeth yellow does not change the probability of smoking, but forc-
ing someone to smoke does change the probability of the teeth becoming yellow. Note that
both the models s →tand s ←thave the same conditional independence structure, yet they
have very different causal interpretations. Unfortunately this has lead many researchers to be-
lieve that such causal relationships cannot be inferred from observational data alone, since these
models are likelihood equivalent (Heckerman et al. , 1995). Likelihood equivalent models are
those for which an arc reversal can be accompanied by a change in parameters to yield the same
likelihood. As a result these researchers then propose that causation can only be obtained by as-
sessing the impact of active manipulation of one variable on another. However, this neglects the
fact that the prior over parameters may cause the marginal likelihoods to be different even for
likelihood equivalent models (D. MacKay, personal communication). In this context, it would
be very interesting to explore the reliability with which variational Bayesian methods can be
used to infer such causal relationships in general graphical models. In chapter 6 we showed
that variational Bayes could determine the presence or absence of arcs from hidden variables to
observed variables in a simple graphical model class. Envisaged then is a similar investigation
for examining the directionality of arcs in a perhaps more expressive structure class.
Automated algorithm derivation
One of the problems with the variational Bayesian framework is that, despite the relative sim-
plicity of the theory, the effort required to derive the update rules for the VBE and VBM steps is
usually considerable and a hindrance to any implementation. Both the derivation and implemen-
tation have to be repeated for each new model, and both steps are prone to error. The variational
linear dynamical system discussed in chapter 5 is a good example of a simple model for which
the implementation is nevertheless cumbersome.
Our contribution of generalising the procedure for conjugate-exponential (CE) family models
(section 2.4) is a step in the right direction for automated algorithm derivation. For CE models,
we now know that the complexity of inference for variational Bayesian inference is the same as
for point-parameter inference, and that for simple models such as HMMs existing propagation
algorithms can be used unaltered with variational Bayes point parameters (see theorem 2.2).
There are a number of software implementations available or in development for inference and
general automated algorithm derivation. The BUGS software package ( Thomas et al. , 1992)
for automated Bayesian inference using Gibbs sampling is the most widely used at present; the
graphical model and functional forms of the conditional probabilities involving both discrete and
continuous variables can be speciﬁed by hand and then the sampling is left to obtain posterior
distributions and marginal probabilities. For more generic algorithm derivation, the AutoBayes
project (Gray et al. , 2003) uses symbolic techniques to automatically derive the equations re-
253
Conclusion 7.2. Summary of contributions
quired for learning and inference in the model and explicitly produces the software to perform
the task.
A similar piece of software is being developed in the VIBES project (Bishop et al., 2003). This
package explicitly uses precisely the CE variational Bayesian results presented in chapter 2 of
this thesis to automate the variational inference and learning processes, for (almost) arbitrary
models expressed in graphical form. To be fully useful, this package should be able to cope
with user-speciﬁed further approximation to the posterior, on top of just the parameter / hidden
variable factorisation. Furthermore it should be relatively straightforward to allow the user to
specify models which have non-CE components, such as logistic sigmoid functions. This would
allow for discrete children of continuous parents, and could be made possible by including
quadratic lower bounds on the sigmoid function (due to Jaakkola, 1997) to ensure that there is
still a valid overall lower bound on the marginal likelihood. Looking further in to the future,
these software applications may even be able to suggest ‘good’ factorisations, or work with a
variety of these approximations together or even hierarchically. Also an alternative for coping
with non-CE components of the model might be to employ sampling-based inferences in small
regions of the graph that are affected.
Combining the variational Bayesian theory with a user-friendly interface in the form of VIBES
or similar software could lead to the mass use of variational Bayesian methods in a wide variety
of application ﬁelds. This would allow the ready comparison of a host of different models, and
greatly improve the efﬁciency of current research on variational Bayes. However there is the
caveat, which perhaps has not been emphasised enough in this thesis, that blind applications
of variational Bayes may lead to the wrong conclusions, and that any inferences should be
considered in the context of the approximations that have been made. This reasoning may not
come easily to an automated piece of software, and the only sure answer to the query of whether
the variational lower bound is reliable is to compare it to the exact marginal likelihood. It should
not be difﬁcult to overlay onto VIBES or similar software a set of sampling components to do
exactly this task of estimating the marginal likelihood very accurately for diagnostic purposes;
one such candidate for this task could be annealed importance sampling.
7.2 Summary of contributions
The aim of this thesis has been to investigate the variational Bayesian method for approximating
Bayesian inference and learning in a variety of statistical models used in machine learning ap-
plications. Chapter 1 reviewed some of the basics of probabilistic inference in graphical models,
such as the junction tree and belief propagation algorithms for exact inference in both undirected
and directed graphs. These algorithms are used for inferring the distribution over hidden vari-
ables given observed data, for a particular setting of the model parameters. We showed that in
254
Conclusion 7.2. Summary of contributions
situations where the parameters of the model are unknown the correct Bayesian procedure is to
integrate over this uncertainty to form the marginal likelihood of the model. We explained how
the marginal likelihood is the key quantity for choosing between models in a model selection
task, but also explained that it is intractable to compute for almost all interesting models.
We reviewed a number of current methods for approximating the marginal likelihood, such as
Laplace’s method, the Bayesian information criterion (BIC), and the Cheeseman-Stutz crite-
rion (CS). We discussed how each of these have signiﬁcant drawbacks in their approximations.
Perhaps the most salient deﬁciency is that they are based on maximum a posteriori parameter
(MAP) estimates of the model parameters, which are arrived at by maximising the posterior
density of the parameters, and so the MAP estimate may not be representative of the posterior
mass at all. In addition we noted that the MAP optimisation is basis dependent, which means
that two different experimenters with the same model and priors, but with different parameter-
isations, do not produce the same predictions using their MAP estimates. We also discussed a
variety of sampling methods, and noted that these are guaranteed to give an exact answer for the
marginal likelihood only in the limit of an inﬁnite number of samples, and one often requires
infeasibly long sampling runs to obtain accurate and reliable estimates.
In chapter 2 we presented the variational Bayesian method for approximating the marginal likeli-
hood. We ﬁrst showed how the standard expectation-maximisation (EM) algorithm for learning
ML and MAP parameters can be interpreted as a variational optimisation of a lower bound on
the likelihood of the data. In this optimisation, the E step can either be exact, in which case
the lower bound is tight after each E step, or it can be restricted to a particular family of distri-
butions in which case the bound is loose. The amount by which the bound is loose is exactly
the Kullback-Leibler divergence between the variational hidden variable posterior and the ex-
act posterior. We then generalised this methodology to the variational Bayesian EM algorithm
which integrates over the parameters. The algorithm alternates between a VBE step which ob-
tains a variational posterior distribution over the hidden variables given a distribution over the
parameters, and a VBM step which infers the variational distribution over the parameters given
the result of the VBE step. The lower bound gap is then given by the KL divergence between
the variational joint posterior over hidden variables and parameters, and the corresponding exact
posterior.
Signiﬁcant progress in understanding the VB EM optimisation was made by considering the
form of the update equations in the case of conjugate-exponential (CE) models. We showed
that if the complete-data likelihood for the model is in the exponential family and the prior
over parameters is conjugate to this likelihood, then the VB update equations take on analyt-
ically tractable forms and have attractive intuitive interpretations. We showed that, in theory,
it is possible to use existing propagation algorithms for performing the VBE step, even though
we have at all times a distribution over the parameters. This is made possible by passing the
propagation algorithm the variational Bayes point parameter, θBP ≡φ−1(⟨φ(θ)⟩qθ(θ)), which
255
Conclusion 7.2. Summary of contributions
is the result of inverting the exponential family’s natural parameter mapping after averaging the
natural parameters under the variational posterior. This is a very powerful result as it means
that variational Bayesian inference (the VBE step) is possible in the same time complexity as
the standard E step for the point-parameter case (with the only overhead being that of inverting
the mapping). We also presented corollaries of this result applied to directed (Bayesian) and
undirected (Markov) networks — see corollaries 2.2 and 2.4.
In chapter 3 we presented a straightforward example of this important result applied to Bayesian
learning in a hidden Markov model. Here the variational Bayes point parameters are sub-
normalised transition and emission probabilities for the HMM, and the well-known forward-
backward algorithm can be used unchanged with these modiﬁed parameters. We carried out
experiments (some of which are suggested in MacKay, 1997) which showed that the VB algo-
rithm was capable of determining the number of hidden states used to generate a synthetic data
set, and outperforms ML and MAP learning on a task of discriminating between forwards and
backwards English sentences. This shows that integrating over the uncertainty in parameters is
important, especially for small data set sizes. The linear dynamical system of chapter 5 has the
same structure as the HMM, so we might expect it to be equally suitable for the propagation
corollary. However for this model it was not found to be possible to invert the natural parameter
mapping, but nevertheless a variational Bayesian inference algorithm was derived with the same
time complexity as the well-known Rauch-Tung-Striebel smoother. It was then shown that the
VB LDS system could use automatic relevance determination methods to successfully deter-
mine the dimensionality of the hidden state space in a variety of synthetic data sets, and that
the model was able to discard irrelevant driving inputs to the hidden state dynamics and output
processes. Some preliminary results on elucidating gene-expression mechanisms were reported,
and we expect this to be an active area of future research.
Chapter 4 focused on a difﬁcult model selection problem, that of determining the numbers
of mixture components in a mixture of factor analysers model. Search over model structures
for MFAs is computationally intractable if each analyser is allowed to have different intrinsic
dimensionalities. We derived and implemented the variational Bayesian EM algorithm for this
MFA model, and showed that by wrapping the VB EM optimisation within a birth and death
process we were able to navigate through the space of number of components using the lower
bound as a surrogate for the marginal likelihood. Since all the parameters are integrated out in
a Bayesian implementation, we are at liberty to begin the search either from the simplest model
or from a model with very many components. Including an automatic relevance determination
prior on the entries of each of the factor loading matrices’ columns allowed the optimisation
to simultaneously ﬁnd the number of components and their dimensionalities. We demonstrated
this on several synthetic data sets, and showed improved performance on a digit classiﬁcation
task as compared to a BIC-penalised ML MFA model. We noted that for this mixture model the
death process was an automatic procedure, and also suggested several ways in which the birth
processes could be implemented to increase the efﬁciency of the structure search.
256
Conclusion 7.2. Summary of contributions
Also in this chapter we presented a generally applicable importance sampling procedure for ob-
taining estimates of the marginal likelihood, predictive density, and the KL divergence between
the variational and exact posterior distributions. In the sampler, the variational posteriors are
used as proposal distributions for drawing importance samples. We found that although the
lower bound tends to correlate well with the importance sampling estimate of the marginal like-
lihood, the KL divergence (the bound gap) increases approximately linearly with the number of
components in the MFA model, which would suggest that the VB approximation has an inherent
bias towards simpler models. We note also that importance sampling can fail for poor choices
of proposal distribution and is not ideal for high dimensional parameter spaces. We attempted
to improve the estimates by using heavier tailed and mixture distributions derived from the vari-
ational posteriors, but any improvements are not very conclusive. The problems with simple
importance sampling have motivated attempts at combining variational methods with more so-
phisticated MCMC methods, but to date there have been few successful implementations, and
this is an area of future work.
We showed in chapter2 that the variational Bayesian EM algorithm is a generalisation of the EM
algorithm for ML/MAP optimisation — the standard EM algorithm is recovered by restricting
the form of the variational posterior distribution over parameters to a delta function, or a point-
estimate. There is also the interesting observation that the VB approximation reduces to the BIC
approximation in the limit of an inﬁnitely large data set, for which we provided a brief proof in
the case of CE models. However, we have also found intriguing connections between the VB
lower bound and Cheeseman-Stutz approximations to the marginal likelihood. In particular we
proved with theorem 2.3 that the CS criterion is a strict lower bound on the marginal likelihood
for arbitrary models (not just those in the CE family), which was a previously unrecognised
fact (although Minka (2001b) makes this observation in a mixture modelling context). We then
built on this theorem to show with corollary 2.5 that there is a construction for obtaining a VB
approximation which always results in at least as tight a bound as the CS criterion. This is a
very interesting and useful result because it means that all existing implementations using CS
approximations can now be made more faithful to the exact marginal likelihood by overlaying a
variational Bayesian approximation. This is only a very recent discovery, and as a result has not
yet been exploited to the full.
We saw superior performance of the variational Bayesian lower bound over the Cheeseman-
Stutz and BIC criteria in chapter 6, where the task was ﬁnding the particular structure (out of a
small class of structures) that gave rise to an observed data set, via the marginal likelihood. This
was despite not making use of the aforementioned construction derived in corollary 2.5 (which
we were not aware of when carrying out the chapter’s experiments). In these experiments we
found that VB outperformed both BIC and CS approximations, and also tended to provide more
reliable results than the sampling gold standard, annealed importance sampling. Not only does
the VB approximation provide a bound on the marginal likelihood (which in the experiments
often showed AIS estimates to be ‘invalid’), but it also arrives at this bound in a fraction (about
257
Conclusion 7.2. Summary of contributions
1%) of the time of the sampling approach. Moreover the VB approximation does not require the
tuning of proposal distributions, annealing schedules, nor does it require extensive knowledge of
the model domain to produce a reliable algorithm. We presented a number of extensions to the
AIS algorithm, including a more general algorithm for computing marginal likelihoods which
uses estimates based on more than one sample at each temperature (see algorithm 6.1). In the
near future we hope to prove whether estimates using this algorithm are biased or not (personal
communication with R. Neal).
To conclude, I hope that this thesis has provided an accessible and coherent account of the
widely applicable variational Bayesian approximation. We have derived variational Bayesian
algorithms for a variety of statistical models and provided the tools with which new models
can be tackled, especially with a view to building software for automated algorithm derivation.
This should throw open the doors to Bayesian learning in a host of models other than those
investigated here. There are many directions for this research to be taken in and much work
left to be done. The hope is that the experimental ﬁndings and insights documented in these
chapters will stimulate and guide future research on variational Bayes.
258
Appendix A
Conjugate Exponential family
examples
The following two tables present information for a variety of exponential family distributions,
and include entropies, KL divergences, and commonly required moments. Where used, tilde
symbols (e.g. ˜θ), denote the parameters of a different distribution of the same form. Therefore
KL(˜θ||θ) is shorthand for the KL divergence between the distribution with parameter˜θ and the
distribution with parameter θ (averaging with respect to the ﬁrst distribution that is speciﬁed).
The remainder of the notation should be self-explanatory.
259
Conjugate Exponential family examples
Distribution Notation & Parameters Density function Moments, entropy, KL-divergence, etc.
Exponential
Family
θ ∼ ExpFam(η,ν)
number ηand value ν
of pseudo-observations
p(θ |η,ν) = 1
Zην
g(θ)ηeφ(θ)⊤ν Hθ = ln Zην −η⟨ln g(θ)⟩−ν⊤⟨φ(θ)⟩
Uniform
θ∼ U(a,b)
boundaries a,b
with b>a
p(θ|a,b) = 1
b−a ,θ ∈[a,b] Hθ = ln( b−a)
⟨θ⟩= a+b
2 ,⟨θ2⟩−⟨θ⟩2 = (b−a)2
12
Laplace
θ∼ Laplace(µ,λ)
µmean
λdecay scale
p(θ|µ,λ) = 1
2λ e− |θ−µ|
λ
λ> 0 Hθ = 1 + ln(2 λ)
Multivariate
normal
(Gaussian)
θ ∼ N(µ,Σ)
µ mean vector
Σ covariance
p(θ |µ,Σ) = (2 π)−d/2 |Σ|−1/2 e− 1
2 tr[Σ−1(θ−µ)(θ−µ)⊤]
Hθ = d
2 (ln 2πe) + 1
2 ln |Σ|
KL(˜µ,˜Σ||µ,Σ) = −1
2
(
ln
⏐⏐⏐˜ΣΣ−1
⏐⏐⏐
+tr
[
I−
[
˜Σ + ( ˜µ −µ)(˜µ −µ)⊤
]
Σ−1
]
ln e
)
⟨θ⟩= µ
⟨θθ⊤⟩= Σ
Kθ = ⟨θ4⟩
⟨θ2⟩2 −3 = 0 (relative kurtosis)
Gamma
τ ∼ Ga(α,β)
shape α> 0
inv. scale β >0
p(τ|α,β) = βα
Γ(α) τα−1e−βτ
Hτ = ln Γ( α) −ln β+ (1 −α)ψ(α) + α
⟨τn⟩= Γ(α+n)
βnΓ(α)
⟨(ln τ)n⟩= βα
Γ(α)
∂n
∂αn
(
Γ(α)
βα
)
⟨τ⟩= α/β
⟨τ2⟩−⟨τ⟩2 = α/β2
⟨ln τ⟩= ψ(α) −ln β
KL(˜α,˜β||α,β) = ˜αln ˜β−αln β−ln Γ(˜α)
Γ(α)
+(˜α−α)(ψ(˜α) −ln ˜β) −˜α(1 −β
˜β )
260
Conjugate Exponential family examples
Distribution Notation & Parameters Density function Moments, entropy, KL-divergence, etc.
Wishart
W ∼ Wishartν(S)
deg. of freedom ν
precision matrix S
p(W|ν,S) = 1
ZνS
|W|(ν−k−1)/2 e− 1
2 tr[S−1W]
ZνS = 2 νk/2πk(k−1)/4 |S|ν/2 ∏k
i=1 Γ
(ν+1−i
2
)
HW = ln ZνS −ν−k−1
2 ⟨ln |W|⟩+ 1
2 νk
⟨W⟩= νS
⟨ln |W|⟩= ∑k
i=1 ψ
(ν+1−i
2
)
+ kln 2 + ln |S|
KL(˜ν, ˜S||ν,S) = ln ZνS
Z˜ν ˜S
+ ˜ν−ν
2 ⟨ln |W|⟩˜Q + 1
2 ˜νtr
[
S−1 ˜S−I
]
Inverse-Wishart
W ∼ Inv−Wishartν(S−1)
deg. of freedom ν
covariance matrix S
p(W|ν,S−1) = 1
Z |W|−(ν+k+1)/2 e− 1
2 tr[SW −1]
Z = 2 νk/2πk(k−1)/4 ∏k
i=1 Γ
(ν+1−i
2
)
×|S|−ν/2 ⟨W⟩= ( ν−k−1)−1S
Student-t (1)
θ∼ tν(µ,σ2)
deg. of freedom ν >0
mean µ; scale σ >0
p(θ|ν,µ,σ 2) = Γ((ν+1)/2)
Γ(ν/2)√νπσ
(
1 + 1
ν
(
θ−µ
σ
)2)−(ν+1)/2 ⟨θ⟩= µ, for ν >1
⟨θ2⟩−⟨θ⟩2 = ν
ν−2 σ2, for ν >2
Student-t (2)
θ∼ t(µ,α,β )
shape α> 0; mean µ
scale2 β >0
p(θ|µ,α,β ) = Γ(α+1/2)
Γ(α)√2πβ
(
1 + (θ−µ)2
2β
)−(α+1/2)
Hθ =
[
ψ(α+ 1
2 ) −ψ(α)
]
(α+ 1
2 )
+ ln √2βB(1
2 ,α)
Kθ = 3
α−2 (relative to Gaussian)
equiv. α→ν
2 ; β →ν
2 σ2
Multivariate
Student-t
θ ∼ tν(µ,Σ)
deg. of freedom ν >0
mean µ; scale2 matrix Σ
p(θ |ν,µ,Σ) = 1
Z
(
1 + 1
ν tr
[
Σ−1(θ −µ)(θ −µ)⊤])−(ν+d)/2
Z = Γ((ν+d)/2)
Γ(ν/2)(νπ)d/2 |Σ|−1/2
⟨θ⟩= µ, for ν >1
⟨θθ⊤⟩−⟨θ⟩⟨θ⟩⊤ = ν
ν−2 Σ, for ν >2
Beta
θ∼ Beta(α,β)
prior sample sizes
α> 0,β >0
p(θ|α,β) = Γ(α+β)
Γ(α)Γ(β) θα−1(1 −θ)β−1
θ∈[0,1] See Dirichlet with k= 2
Dirichlet
π ∼ Dir(α)
prior sample sizes
α = {α1,...,α k}
αj >0; α0 = ∑k
j=1 αj
p(π |α) = Γ(α0)
Γ(α1)···Γ(αk) πα1−1
1 ···παk−1
k
π1,...,π k ≥0; ∑k
j=1 πj = 1
⟨π⟩= α/α0
⟨ππ⊤⟩−⟨π⟩⟨π⟩⊤ = α0diag(α)−αα⊤
α2
0(α0+1)
⟨ln πj⟩= ψ(αj) −ψ(α0)
KL(˜α||α) = ln Γ(˜α0)
Γ(α0) −∑k
j=1
[
ln Γ(˜αj)
Γ(αj)
−(˜αj −αj) (ψ(˜αj) −ψ(˜α0))
]
261
Appendix B
Useful results from matrix theory
B.1 Schur complements and inverting partitioned matrices
In chapter 5 on Linear Dynamical Systems, we needed to obtain the cross-covariance of states
across two time steps from the precision matrix, calculated from combining the forward and
backward passes over the sequences. This precision is based on the joint distribution of the
states, yet we are interested only in the cross-covariance between states. If Ais of 2 ×2 block
form, we can use Schur complements to obtain the following results for the partitioned inverse
of A, and its determinant in terms of its blocks’ constituents.
The partitioned inverse is given by
(
A11 A12
A21 A22
)−1
=
(
F−1
11 −A−1
11 A12F−1
22
−F−1
22 A21A−1
11 F−1
22
)
(B.1)
=
(
A−1
11 + A−1
11 A12F−1
22 A21A−1
11 −F−1
11 A12A−1
22
−A−1
22 A21F−1
11 A−1
22 + A−1
22 A21F−1
11 A12A−1
22
)
(B.2)
and the determinant by
⏐⏐⏐⏐⏐
A11 A12
A21 A22
⏐⏐⏐⏐⏐= |A22|·|F11|= |A11|·|F22|, (B.3)
where
F11 = A11 −A12A−1
22 A21 (B.4)
F22 = A22 −A21A−1
11 A12 . (B.5)
262
Useful results from matrix theory B.2. The matrix inversion lemma
Notice that inverses of A12 or A21 do not appear in these results. There are other Schur com-
plements that are deﬁned in terms of the inverses of these ‘off-diagonal’ terms, but they are not
needed for our purposes, and indeed if the states involved have different dimensionalities or are
independent, then these off-diagonal quantities are not invertible.
B.2 The matrix inversion lemma
Here we present a sketch proof of the matrix inversion lemma, included for reference only. In the
derivation that follows, it becomes quite clear that there is no obvious way of carrying the sort
of expectations encountered in chapter 5 through the matrix inversion process (see comments
following equation (5.105)).
The matrix inversion result is most useful when A is a large diagonal matrix and B has few
columns (equivalently Dhas few rows).
(A+ BCD)−1 = A−1 −A−1B(C−1 + DA−1B)−1DA−1 . (B.6)
To derive this lemma we use the Taylor series expansion of the matrix inverse
(A+ M)−1 = A−1(I+ MA−1)−1 = A−1
∞∑
i=0
(−1)i(MA−1)i , (B.7)
where the series is only well-deﬁned when the spectral radius of MA−1 is less than unity. We
can easily check that this series is indeed the inverse by directly multiplying by (A+ M),
yielding the identity,
(A+ M)A−1
∞∑
i=0
(−1)i(MA−1)i = AA−1 [
I−MA−1 + (MA−1)2 −(MA−1)3 + ...
]
+ MA−1 [
I − MA−1 + ( MA−1)2 −...
]
(B.8)
= I . (B.9)
263
Useful results from matrix theory B.2. The matrix inversion lemma
In the series expansion we ﬁnd an embedded expansion, which forms the inverse matrix term
on the right hand side, as follows
(A+ BCD)−1 = A−1(I+ BCDA−1)−1 (B.10)
= A−1
∞∑
i=0
(−1)i(BCDA−1)i (B.11)
= A−1
(
I+
∞∑
i=1
(−1)i(BCDA−1)i
)
(B.12)
= A−1
(
I−BC
[∞∑
i=0
(−1)i(DA−1BC)i
]
DA−1
)
(B.13)
= A−1 (
I−BC(I+ DA−1BC)−1DA−1)
(B.14)
= A−1 −A−1B(C−1 + DA−1B)−1DA−1 . (B.15)
In the above equations, we assume that the spectral radii of BCDA−1 (B.11) and DA−1BC
(B.13) are less than one for the Taylor series to be convergent. Aside from these constraints,
we can post-hoc check the result simply by showing that multiplication of the expression by its
proposed inverse does in fact yield the identity.
264
Appendix C
Miscellaneous results
C.1 Computing the digamma function
The digamma function is deﬁned as
ψ(x) = d
dxln Γ(x) , (C.1)
where Γ(x) is the Gamma function given by
Γ(x) =
∫ ∞
0
dτ τx−1e−τ . (C.2)
In the implementations of the models discussed in this thesis, the following expansion is used
to compute the ψ(x) for large positive arguments
ψ(x) ≃ln x− 1
2x − 1
12x2 + 1
120x4 − 1
252x6 + 1
240x8 + ... . (C.3)
If we have small arguments, then we would expect this expansion to be inaccurate if we only
used a ﬁnite number of terms. However, we can make use of a recursion of the digamma function
to ensure that we always pass this expansion large arguments. The Gamma function has the well
known recursion:
x! = Γ( x+ 1) = xΓ(x) = x(x−1)! , (C.4)
from which the recursion for the digamma function readily follows:
ψ(x+ 1) = 1
x + ψ(x) . (C.5)
265
Miscellaneous results C.2. Multivariate gamma hyperparameter optimisation
In our experiments we used an expansion ( C.3) containing terms as far as O(1/x14), and used
the recursion to evaluate this only for arguments of ψ(x) greater than 6. This is more than
enough precision.
C.2 Multivariate gamma hyperparameter optimisation
In hierarchical models such as the VB LDS model of chapter 5, there is often a gamma hyper-
prior over the noise precisions on each dimension of the data. On taking derivatives of the lower
bound with respect to the shape aand inverse scale bof this hyperprior distribution, we obtain
ﬁxed point equations of this form:
ψ(a) = ln b+ 1
p
p∑
s=1
ln ρs , 1
b = 1
pa
p∑
s=1
ρs (C.6)
where the notation ln ρs and ρs is used to denote the expectations of quantities under the varia-
tional posterior distribution (see section 5.3.6 for details). We can rewrite this as:
ψ(a) = ln b+ c, 1
b = d
a , (C.7)
where
c= 1
p
p∑
s=1
ln ρs , and d= 1
p
p∑
s=1
ρs . (C.8)
Equation (C.7) is the generic ﬁxed point equation commonly arrived at when ﬁnding the varia-
tional parameters aand bwhich minimise the KL divergence on a gamma distribution.
The ﬁxed point for ais found at the solution of
ψ(a) = ln a−ln d+ c, (C.9)
which can be arrived at using the Newton-Raphson iterations:
anew ←a
[
1 −ψ(a) −ln a+ ln d−c
aψ′(a) −1
]
, (C.10)
where ψ′(x) is the ﬁrst derivative of the digamma function. Unfortunately, this update cannot
ensure that aremains positive for the next iteration (the gamma distribution is only deﬁned for
a> 0) because the gradient information is taken locally.
There are two immediate ways to solve this. First if a should become negative during the
Newton-Raphson iterations, reset it to a minimum value. This is a fairly crude solution. Alter-
266
Miscellaneous results C.3. Marginal KL divergence of gamma-Gaussian variables
natively, we can solve a different ﬁxed point equation fora′ where a= exp( a′), resulting in the
multiplicative updates:
anew ←aexp
[
−ψ(a) −ln a+ ln d−c
aψ′(a) −1
]
. (C.11)
This update has the same ﬁxed point but exhibits different (well-behaved) dynamics to reach
it. Note that equation C.10 is simply the ﬁrst two terms in the Taylor series of the exponential
function in the above equation.
Once the ﬁxed point a∗ is reached, the corresponding b∗ is found simply from
b∗ = a∗
d . (C.12)
C.3 Marginal KL divergence of gamma-Gaussian variables
This note is intended to aid the reader in computing the lower bound appearing in equation
(5.147) for variational Bayesian state-space models. Terms such as the KL divergence between
two Gaussian or two gamma distributions are straightforward to compute and are given in ap-
pendix A. However there are more complicated terms involving expectations of KL divergences
for joint Gaussian and gamma variables, for which we give results here.
Suppose we have two variables of interest, a and b, that are jointly Gaussian distributed. To be
more precise let the two variables be linearly dependent on each other in this sense:
q(a,b) = q(b)q(a |b) = N( b |µb,Σb) ·N(a |µa,Σa) (C.13)
where µa = y −Gb . (C.14)
Let us also introduce a prior distribution p(a |b) in this way:
p(a |b) = N( a |˜µa,˜Σa) (C.15)
where neither parameter ˜µa nor ˜Σa are functions of b.
The ﬁrst result is the KL divergence between two Gaussian distributions (given in appendixA)
KL [q(a |b) ∥p(a |b)] =
∫
da q(a |b) ln q(a |b)
p(a |b) (C.16)
= −1
2 ln
⏐⏐⏐˜Σ−1
a Σa
⏐⏐⏐+ 1
2tr ˜Σ−1
a
[
Σa −˜Σa + (µa −˜µa) (µa −˜µa)⊤
]
.
(C.17)
267
Miscellaneous results C.3. Marginal KL divergence of gamma-Gaussian variables
Note that this divergence is written w.r.t. the q(a |b) distribution. The dependence on b is not
important here, but will be required later. The important part to note is that it obviously depends
on each Gaussian’s covariance, but also on the Mahalanobis distance between the means as
measured w.r.t. the non-averaging distribution.
Consider now the KL divergence between the full joint posterior and full joint prior:
KL [q(a,b) ∥p(a,b)] =
∫
da db q(a,b) ln q(a,b)
p(a,b) (C.18)
=
∫
db q(b)
∫
da q(a |b) ln q(a |b)
p(a |b) +
∫
db q(b) ln q(b)
p(b) . (C.19)
The last term in this is equation is simply the KL divergence between two Gaussians, which
is straightforward, but the ﬁrst term is the expected KL divergence between the conditional
distributions, where the expectation is taken w.r.t. the marginal distribution q(b). After some
simple manipulation, this ﬁrst term is given by
⟨KL [q(a |b) ∥p(a |b)]⟩q(b) =
∫
db q(b)
∫
da q(a |b) ln q(a |b)
p(a |b) (C.20)
= −1
2 ln
⏐⏐⏐˜Σ−1
a Σa
⏐⏐⏐+ 1
2tr ˜Σ−1
a
[
Σa − ˜Σa + GΣbG⊤
+ (y −Gµb −˜µa) (y −Gµb −˜µa)⊤
]
. (C.21)
Let us now suppose that the covariance terms for the prior ˜Σ and posterior Σa have the same
multiplicative dependence on another variable ρ−1. This is the case in the variational state-
space model of chapter 5 where, for example, the uncertainty in the entries for the output matrix
C should be related to the setting of the output noise ρ (see equation ( 5.44) for example). In
equation (C.17) it is clear that if both covariances are dependent on the same ρ−1, then the KL
divergence will not be a function of ρ−1 provided that the means of both distributions are the
same. If they are different however, then there is a residual dependence on ρ−1 due to the ˜Σ−1
a
term from the non-averaging distribution p(a |b). This is important as there will usually be
distributions over this ρvariable of the form
q(ρ) = Ga( ρ|eρ,fρ) (C.22)
with eand f shape and precision parameters of a gamma distribution. The most complicated
term to compute is the penultimate term in (5.147), which is
⣨
⟨KL [q(a |b,ρ) ∥p(a |b,ρ)]⟩q(b)
⟩
q(ρ)
=
∫
dρq(ρ)
∫
db q(b |ρ)
∫
da q(a |b,ρ) ln q(a |b,ρ)
p(a |b,ρ) . (C.23)
In the variational Bayesian state-space model, the prior and posterior for the parameters of the
output matrix C(and Dfor that matter) are deﬁned in terms of the same noise precision variable
268
Miscellaneous results C.3. Marginal KL divergence of gamma-Gaussian variables
ρ. This means that all terms but the last one in equation ( C.21) are not functions of ρand pass
through the expectation in (C.23) untouched. The ﬁnal term has a dependence onρ, but on taking
expectations w.r.t.q(ρ) this simply yields a multiplicative factor of ⟨ρ⟩q(ρ). It is straightforward
to extend this to the case of data with several dimensions, in which case the lower bound is a
sum over all pdimensions of similar quantities.
269
Bibliography
D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines.
Cognitive Science, 9:147–169, 1985. 2.4.1
N. J. Adams, A. J. Storkey, Z. Ghahramani, and C. K. I. Williams. MFDTs: Mean ﬁeld dynamic
trees. In Proc. 15th Int. Conf. on Pattern Recognition, 2000. 2.5.1
S. L. Adler. Over-relaxation method for the Monte Carlo evaluation of the partition function for
multiquadratic actions. Physical Review D, 23:2901–2904, 1981. 1.3.6
H. Attias. Independent Factor Analysis. Neural Computation, 11:803–851, 1999a. 2.4.1
H. Attias. Inferring parameters and structure of latent variable models by variational Bayes. In
Proc. 15th Conf. on Uncertainty in Artiﬁcial Intelligence, 1999b. 2.6.1
H. Attias. A variational Bayesian framework for graphical models. In S. A. Solla, T. K. Leen,
and K. M ¨uller, editors, Advances in Neural Information Processing Systems 12, Cambridge,
MA, 2000. MIT Press. 2.3.2, 2.4.3
L. Bahl and F. Jelinek. Decoding for channels with insertions, deletions, and substitutions with
applications to speech recognition. IEEE Transactions on Information Theory , 21(4):404–
411, 1975. 3.1
D. Barber and C. M. Bishop. Ensemble learning for multi-layer networks. In M. I. Jordan,
M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems
10, pages 395–401, Cambridge, MA, 1998. MIT Press. 2.3.2
D. Barber and P. Sollich. Gaussian ﬁelds for approximate inference in layered sigmoid belief
networks. In S. A. Solla, T. K. Leen, and K. M¨uller, editors, Advances in Neural Information
Processing Systems 12, Cambridge, MA, 2000. MIT Press. 2.3.2
A. I. Barvinok. Polynomial time algorithms to approximate permanents and mixed discriminants
within a simply exponential factor. Random Structures and Algorithms, 14(1):29–61, 1999.
1.3.3
L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of ﬁnite state Markov
chains. Annals of Mathematical Statistics, 37(6):1554–1563, 1966. 3.1
270
Bibliography Bibliography
L.E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occurring in the
statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical
Statistics, 41:164–171, 1970. 2.2.2, 3.1, 3.2
M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The inﬁnite hidden Markov model. In
Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.
3.6, 7.1
A. J. Bell and T. J. Sejnowski. An information maximisation approach to blind separation and
blind deconvolution. Neural Computation, 7(6):1129–1159, 1995. 2.4.1
J. M. Bernardo and F. J. Giron. A Bayesian analysis of simple mixture problems. In J. M.
Bernardo, M. H. Degroot, A. F. Smith, and D. V . Lindley, editors, Bayesian Statistics 3 ,
pages 67–78. Clarendon Press, 1988. 2.3.2
J. M. Bernardo and A. F. M. Smith. Bayesian Theory. John Wiley & Sons, Inc., New York,
1994. 1.2.2, 2.6.1
C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995. 2.4.1
C. M. Bishop. Variational PCA. InProc. Ninth Int. Conf. on Artiﬁcial Neural Networks. ICANN,
1999. 2.3.2, 4.2.2
C. M. Bishop, N. D. Lawrence, T. S. Jaakkola, and M. I. Jordan. Approximating posterior dis-
tributions in belief networks using mixtures. In Advances in Neural Information Processing
Systems 10, Cambridge, MA, 1998. MIT Press. 2.3.2
C. M. Bishop, D. Spiegelhalter, and J. Winn. VIBES: A variational inference engine for
Bayesian networks. In Advances in Neural Information Processing Systems 15 , S. Becker
and S. Thrun and K. Obermayer, 2003. 2.4.3, 7.1
X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In Proceedings
of the 14th Conference on Uncertainty in Artiﬁcial Intelligence , Madison, Wisconsin, 1998.
2.3.2
S. Brooks. MCMC repository. Statistical Laboratory, University of Cambridge. Accessible on
the world wide web at http://www.statslab.cam.ac.uk/˜mcmc. 1.3.6
W. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002. 2.4.3
G. Casella, K. L. Mengersen, C. P. Robert, and D. M. Titterington. Perfect slice samplers for
mixtures of distributions. Journal of the Royal Statistical Society, Series B (Methodological),
64(4):777–790, 2000. 1.3.6
K. Chan, T. Lee, and T. J. Sejnowski. Variational learning of clusters of undercomplete nonsym-
metric independent components. Journal of Machine Learning Research, 3:99–114, August
2002. 4.8
271
Bibliography Bibliography
P. Cheeseman and J. Stutz. Bayesian classiﬁcation (Autoclass): Theory and results. In U. M.
Fayyad, G. Piatesky-Shapiro, P. Smyth, and R. Uthurusamy, editors,Advances in Knowledge
Discovery and Data Mining, pages 153–180, Menlo Park, CA, 1996. AAAI Press/MIT Press.
1.3.1, 1.3.5, 1.3.5, 2.6.2, 2.6.2, 6.3.3, 6.5.2
D. M. Chickering and D. Heckerman. Efﬁcient approximations for the marginal likelihood of
Bayesian networks with hidden variables.Machine Learning, 29(2–3):181–212, 1997. 2.6.2,
2.6.2
R. Choudrey and S. Roberts. Variational mixture of Bayesian independent component analysers.
Neural Computation, 15(1), 2002. 4.8
P. Comon. Independent component analysis - a new concept? Signal Processing, 36:287–314,
1994. 2.4.1
R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter.Probabilistic Networks and
Expert Systems. Springer-Verlag, New York, 1999. 1.1
R. T. Cox. Probability, frequency, and reasonable expectation.American Journal of Physics, 14
(1):1–13, 1946. 1.1
N. de Freitas, P. Højen-Sørensen, M. I. Jordan, and S. Russell. Variational MCMC. In J. S.
Breese and D. Koller, editors,Proceedings of the 17th Conference on Uncertainty in Artiﬁcial
Intelligence. Morgan Kaufmann, 2001. 4.8
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society, Series B (Methodological) , 39:
1–38, 1977. 2.2.2, 2.4.3
R. P. Feynman. Statistical Mechanics: A Set of Lectures . Perseus, Reading, MA, 1972. 2.2.1,
2.3.2
J. A. Fill. An interruptible algorithm for perfect sampling via Markov chains. The Annals of
Applied Probability, 8(1):131–162, 1998. 1.3.6
E. Fokou´e and D. M. Titterington. Mixtures of factor analysers. Bayesian estimation and infer-
ence by stochastic simulation. Machine Learning, 50(1):73–94, January 2003. 4.2.3
B. J. Frey, R. Patrascu, T. S. Jaakkola, and J. Moran. Sequentially ﬁtting“inclusive” trees for
inference in noisy-OR networks. In Advances in Neural Information Processing Systems 13,
2001. 2.3.2
N. Friedman. The Bayesian structural EM algorithm. In Proc. Fourteenth Conference on Un-
certainty in Artiﬁcial Intelligence (UAI ’98 , San Francisco, CA, 1998. Morgan Kaufmann
Publishers. 6.4, 6.6
272
Bibliography Bibliography
N. Friedman, M. Linial, I. Nachman, and D. Pe’er. Using Bayesian networks to analyze expres-
sion data. Journal of Computational Biology, 7:601–620, 2000. 5.5
S. Fr ¨uwirth-Schnatter. Bayesian model discrimination and Bayes factors for linear Gaussian
state space models. Journal of the Royal Statistical Society, Series B (Methodological) , 57:
237–246, 1995. 5.6
D. Geiger, D. Heckerman, and C. Meek. Asymptotic model selection for directed networks
with hidden variables. In Proceedings of the 12th Conference on Uncertainty in Artiﬁcial
Intelligence. Morgan Kaufmann Publishers, 1996. 6.5.2
A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman &
Hall, 1995. 2.4.1
A. Gelman and X. Meng. Simulating normalizing constants: From importance sampling to
bridge sampling to path sampling. Statistical Science, 13:163–185, 1998. 6.3.5
Z. Ghahramani. Factorial learning and the EM algorithm. In G. Tesauro, D. S. Touretzky, and
T. K. Leen, editors, Advances in Neural Information Processing Systems 7 , pages 617–624,
Cambridge, MA, 1995. MIT Press. 2.2.3
Z. Ghahramani. An introduction to hidden Markov models and Bayesian networks. Interna-
tional Journal of Pattern Recognition and Artiﬁcial Intelligence, 15(1):9–42, 2001. 3.1
Z. Ghahramani and H. Attias. Online variational Bayesian learning, 2000. Slides
from talk presented at NIPS 2000 workshop on Online Learning, available at
http://www.gatsby.ucl.ac.uk/˜zoubin/papers/nips00w.ps. 2.4.3
Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixtures of factor analy-
sers. In Advances in Neural Information Processing Systems 12, Cambridge, MA, 2000. MIT
Press. 2.3.2, 2.4.3, 4.1, 4.7
Z. Ghahramani and M. J. Beal. Propagation algorithms for variational Bayesian learning. In
Advances in Neural Information Processing Systems 13, Cambridge, MA, 2001. MIT Press.
2.4.3, 2.4.3
Z. Ghahramani and G. E. Hinton. Parameter estimation for linear dynamical systems. Techni-
cal Report CRG-TR-96-2, Department of Computer Science, University of Toronto, 1996a.
5.2.2, 5.3.8
Z. Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. Technical
Report CRG-TR-96-1, Department of Computer Science, University of Toronto, 1996b.4.1.2
Z. Ghahramani and G. E. Hinton. Variational learning for switching state-space models.Neural
Computation, 12(4), 2000. 2.2.3, 3.1
273
Bibliography Bibliography
Z. Ghahramani and M. I. Jordan. Factorial hidden Markov models. Machine Learning, 29:
245–273, 1997. 2.2.3, 2.2.3, 3.1
W. R. Gilks. Derivative-free adaptive rejection sampling for Gibbs sampling. In J. M. Bernardo,
J. O. Berger, A. P. Dawid, and A. F. M. Smith, editors,Bayesian Statistics 4, pages 641–649.
Clarendon Press, 1992. 1.3.6
W. R. Gilks, N. G. Best, and K. K. C. Tan. Adaptive rejection Metropolis sampling within Gibbs
sampling. Applied Statistics, 44:455–472, 1995. 1.3.6
W. R. Gilks, G. O. Roberts, and S. K. Sahu. Adaptive Markov chain Monte Carlo through
regeneration. Journal of the American Statistical Association, 93:1045–1054, 1998. 4.8
W. R. Gilks and P. Wild. Adaptive rejection sampling for Gibbs sampling. Applied Statistics,
41(2):337–348, 1992. 1.3.6, 1.3.6
A. G. Gray, B. Fischer, J. Schumann, and W. Buntine. Automatic derivation of statistical al-
gorithms: The EM family and beyond. In S. Becker, S. Thrun, and K. Obermayer, editors,
Advances in Neural Information Processing Systems 15. MIT Press, 2003. 2.4.3, 7.1
P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrika, 82:711–732, 1995. 1.3.6, 4.2.3, 4.3
M. Harvey and R. M. Neal. Inference for belief networks using coupling from the past. In
C. Boutilier and M. Goldszmidt, editors, Proceedings of the 16th Conference on Uncertainty
in Artiﬁcial Intelligence, pages 256–263. Morgan Kaufmann, 2000. 1.3.6
W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1):97–109, 1970. 1.3.6, 6.3.5
D. Heckerman. A tutorial on learning with Bayesian networks. Technical Report MSR-TR-95-
06 [ftp://ftp.research.microsoft.com/pub/tr/TR-95-06.PS] , Microsoft Research, 1996. 1.1
D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: the combination
of knowledge and statistical data. Machine Learning, 20(3):197–243, 1995. 6.2, 7.1
T. Heskes. Stable ﬁxed points of loopy belief propagation are minima of the Bethe free energy. In
S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing
Systems 15, Cambridge, MA, 2003. MIT Press. 7.1
G. E. Hinton and D. van Camp. Keeping neural networks simple by minimizing the description
length of the weights. In Sixth ACM Conference on Computational Learning Theory, Santa
Cruz, 1993. 1.2.1, 2.3.2
G. E. Hinton and R. S. Zemel. Autoencoders, minimum description length, and Helmholtz free
energy. In J. D. Cowan, G. Tesauro, and J. Alspector, editors,Advances in Neural Information
Processing Systems 6, San Francisco, CA, 1994. Morgan Kaufmann. 2.2.3
274
Bibliography Bibliography
J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 16(5), May 1994. 4.6
T. S. Jaakkola. Variational methods for inference and estimation in graphical models. Technical
Report Ph.D. Thesis, Department of Brain and Cognitive Sciences, MIT, Cambridge, MA,
1997. 2.2.3, 2.3.2, 2.5.2, 7.1
T. S. Jaakkola and M. I. Jordan. Improving the mean ﬁeld approximation via the use of mixture
distributions. In M. I. Jordan, editor, Learning in Graphical Models, pages 163–173. Kluwer,
1998. 2.3.2
T. S. Jaakkola and M. I. Jordan. Bayesian logistic regression: a variational approach. Statistics
and Computing, 10:25–37, 2000. 2.3.2
E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, 2003. 1.1
W. H. Jefferys and J. O. Berger. Ockham’s razor and Bayesian analysis.American Scientist, 80:
64–72, 1992. 1.2.1, 4.2
H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings
of the Royal Society of London. Series A, Mathematical and Physical Sciences , 186(1007),
1946. 1.2.2
F. V . Jensen.Introduction to Bayesian Networks. Springer-Verlag, New York, 1996. 1.1
J. L. W. V . Jensen. Sur les fonctions convexes et les inegalit´es entre les valeurs moyennes. Acta
Mathematica, 30:175–193, 1906. 2.2.1
M. Jerrum, A. Sinclair, and E. Vigoda. A polynomial-time approximation algorithm for the per-
manent of a matrix with non-negative entries. In ACM Symposium on Theory of Computing,
pages 712–721, 2001. 1.3.3
M. I. Jordan, editor. Learning in Graphical Models. MIT Press, Cambridge, MA, 1999. 1.1
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational
methods in graphical models. Machine Learning, 37:183–233, 1999. 2.3.2, 2.5.2
M. I. Jordan, Z. Ghahramani, and L. K. Saul. Hidden Markov decision trees. In M. C. Mozer,
M. I. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems 9,
Cambridge, MA, 1997. MIT Press. 3.1
M. I. Jordan and Y . Weiss. Graphical models: Probabilistic inference. In M. Arbib, editor, The
Handbook of Brain Theory and Neural Networks, 2nd edition . MIT Press, Cambridge, MA,
2002. 1.1.2
B. H. Juang and L. R. Rabiner. Hidden Markov models for speech recognition. Technometrics,
33:251–272, 1991. 3.1
275
Bibliography Bibliography
R. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association ,
90:773–795, 1995. 1.2.1, 1.3.1, 1.3.2
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science,
220(4598):671–680, 1983. 6.3.5
T. Koˇcka and N. L. Zhang. Dimension correction for hierarchical latent class models. In A. Dar-
wich and N. Friedman, editors, Proceedings of the 18th Conference on Uncertainty in Artiﬁ-
cial Intelligence, pages 267–274. Morgan Kaufmann, 2002. 6.5.2
S. L. Lauritzen and D. J. Spiegelhalter. Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society,
Series B (Methodological), 50(2):157–224, 1988. 1.1
N. D. Lawrence and M. Azzouzi. A variational Bayesian committee of neural networks. Sub-
mitted to Neural Networks, 1999. 2.3.2
N. D. Lawrence, C. M. Bishop, and M. I. Jordan. Mixture representations for inference and
learning in Boltzmann machines. In G. F. Cooper and S. Moral, editors, Proceedings of the
14th Conference on Uncertainty in Artiﬁcial Intelligence, pages 320–327, Madison, Wiscon-
sin, 1998. 2.3.2
Y . LeCun and Y . Bengio. Convolutional networks for images, speech, and time-series. In M. A.
Arbib, editor, The Handbook of Brain Theory and Neural Networks. MIT Press, 1995. 4.6.2
M. A. R. Leisink and H. J. Kappen. A tighter bound for graphical models.Neural Computation,
13(9):2149–2170, 2001. 7.1
D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4(3):415–447, 1992. 3.3, 4.2
D. J. C. MacKay. Probable networks and plausible predictions — a review of practical Bayesian
methods for supervised neural networks. Network: Computation in Neural Systems , 6:469–
505, 1995. 1.2.1, 1.3, 1.3.2, 5.2.2, 6.3.2
D. J. C. MacKay. Bayesian non-linear modelling for the 1993 energy prediction competition. In
G. Heidbreder, editor, Maximum Entropy and Bayesian Methods, Santa Barbara 1993, pages
221–234, Dordrecht, 1996. Kluwer. 4.2.2
D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report, Cavendish
Laboratory, University of Cambridge, 1997. 2.3.2, 2.4.3, 2.5.1, 3.1, 3.4, 3.4.2, 3.5.2, 7.2
D. J. C. MacKay. Choice of basis for Laplace approximation. Machine Learning, 33(1), 1998.
1.3.2, 3.3, 3.3, 6.3.1
D. J. C. MacKay. An introduction to Monte Carlo methods. In M. I. Jordan, editor, Learning in
Graphical Models. MIT Press, Cambridge, MA, 1999. 4.7.3
276
Bibliography Bibliography
D. J. C. MacKay. A problem with variational free energy minimization, 2001. 3.6
D. J. C. MacKay and L. C. Peto. A hierarchical Dirichlet language model. Natural Language
Engineering, 1(3):1–19, 1995. 3.3
N. Metropolis, A. W. Rosenbluth, M. N. Teller, and E. Teller. Equation of state calculations by
fast computing machines. Journal of Chemical Physics, 21:1087–1092, 1953. 1.3.6, 6.3.5
T. P. Minka. A family of algorithms for approximate Bayesian inference . PhD thesis, MIT,
2001a. 2.3.2, 7.1
T. P. Minka. Using lower bounds to approximate integrals, 2001b. 2.6.2, 7.2
T. P. Minka and J. Lafferty. Expectation-Propagation for the generative aspect model. In Pro-
ceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence. Morgan Kaufmann,
2002. 2.3.2
J. W. Miskin. Ensemble Learning for Independent Component Analysis. PhD thesis, University
of Cambridge, December 2000. 4.7, 4.7.3, 5.6
D. J. Murdoch and P. J. Green. Exact sampling from a continuous state space. Scandinavian
Journal of Statistics, 25(3):483–502, 1998. 1.3.6
R. M. Neal. Connectionist learning of belief networks. Artiﬁcial Intelligence, 56:71–113, 1992.
1.3.6, 2.2.3
R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report
CRG-TR-93-1, Department of Computer Science, University of Toronto, 1993. 1.3.6, 6.3.5
R. M. Neal. Bayesian Learning in Neural Networks. Springer-Verlag, 1996. 1.2.1, 1.3.6, 3.6,
6.3.5, 7.1
R. M. Neal. Assessing relevance determination methods using DELVE. In C. M. Bishop, editor,
Neural Networks and Machine Learning, pages 97–129. Springer-Verlag, 1998a. 4.2.2
R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical
Report 9815, Department of Statistics, University of Toronto, 1998b. 3.6
R. M. Neal. Annealed importance sampling. Statistics and Computing , 11:125–139, 2001.
1.3.6, 6.3.5, 6.3.5, 6.3.5
R. M. Neal. Slice sampling. Annals of Statistics, 31(3), 2003. With discussion. 6.6
R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse,
and other variants. In M. I. Jordan, editor, Learning in Graphical Models , pages 355–369.
Kluwer Academic Publishers, 1998. 2.2.1, 2.3.2
277
Bibliography Bibliography
A. O’Hagan. Monte Carlo is fundamentally unsound. Statistician, 36(2/3):247–249, 1987.
Special Issue: Practical Bayesian Statistics. 1.3.6
A. O’Hagan. Bayes-Hermite quadrature. Journal of Statistical Planning and Inference, 29(3):
245–260, 1991. 1.3.6
G. Parisi. Statistical Field Theory. Addison Wesley, 1988. 2.3.2
J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann Publishers, San Francisco, CA, 1988. 1.1, 1.1.1, 2.5
J. G. Propp and D. B. Wilson. Exact sampling with coupled Markov chains and applications to
statistical mechanics. Random Structures and Algorithms, 1&2(9):223–252, 1996. 1.3.6
L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models. IEEE Acoustics,
Speech & Signal Processing Magazine, 3:4–16, 1986. 3.2
C. Rangel, D. L. Wild, F. Falciani, Z. Ghahramani, and A. Gaiba. Modeling biological responses
using gene expression proﬁling and linear dynamical systems. InTo appear in Proceedings of
the 2nd International Conference on Systems Biology, Madison, WI, 2001. OmniPress. 5.5,
5.9
C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information
Processing Systems 12, Cambridge, MA, 2000. MIT Press. 3.6, 4.8, 7.1
C. E. Rasmussen and Z. Ghahramani. Occam’s razor. In Advances in Neural Information
Processing Systems 13, Cambridge, MA, 2001. MIT Press. 1.2.1
C. E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. InAdvances
in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. 7.1
C. E. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo. InAdvances in Neural Information
Processing Systems 14, Cambridge, MA, 2003. MIT Press. 1.3.6
H. E. Rauch. Solutions to the linear smoothing problem. IEEE Transactions on Automatic
Control, 8:371–372, 1963. 5.3.4
H. E. Rauch, F. Tung, and C. T. Striebel. On the maximum likelihood estimates for linear
dynamic systems. Technical Report 6-90-63-62, Lockheed Missiles and Space Co., Palo
Alto, California, June 1963. 5.3.2
S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number
of components. Journal of the Royal Statistical Society, Series B (Methodological) , 59(4):
731–758, 1997. 4.3
J. Rissanen. Stochastic complexity. Journal of the Royal Statistical Society, Series B (Method-
ological), 49:223–239 and 253–265, 1987. With discussion. 1.3.4
278
Bibliography Bibliography
C. P. Robert, G. Celeux, and J. Diebolt. Bayesian estimation of hidden Markov chains: a
stochastic implementation. Statistics & Probability Letters, 16(1):77–83, 1993. 3.3
S. J. Roberts, D. Husmeier, I. Rezek, and W. Penny. Bayesian approaches to Gaussian mixture
modeling. IEEE PAMI, 20(11):1133–1142, 1998. 4.2.3
S. T. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Com-
putation, 11(2):305–345, 1999. 4.1.2, 5.2.1
J. S¨arel¨a, H. Valpola, R. Vig´ario, and E. Oja. Dynamical factor analysis of rhythmic magnetoen-
cephalographic activity. In Proceedings of the 3rd International Conference on Independent
Component Analysis and Blind Signal Separation, ICA 2001 , pages 457–462, San Diego,
California, USA, 2001. 5.6
M. Sato. Online model selection based on the variational Bayes. Neural Computation, 13(7):
1649–1681, 2001. 2.4.3
L. K. Saul, T. Jaakkola, and M. I. Jordan. Mean ﬁeld theory for sigmoid belief networks.Journal
of Artiﬁcial Intelligence Research, 4:61–76, 1996. 2.2.3
G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6:461–464, 1978.
1.3.1, 1.3.4
R. Settimi and J. Q. Smith. On the geometry of Bayesian graphical models with hidden variables.
In G. F. Cooper and S. Moral, editors, Proceedings of the 14th Conference on Uncertainty in
Artiﬁcial Intelligence, pages 472–479, Madison, Wisconsin, 1998. 6.5.2
R. D. Shachter. Bayes-Ball: The rational pastime (for determining irrelevance and requisite
information in belief networks and inﬂuence diagrams). In G. F. Cooper and S. Moral, editors,
Proceedings of the 14th Conference on Uncertainty in Artiﬁcial Intelligence, pages 480–487,
Madison, Wisconsin, 1998. Morgan Kaufmann. 1.1.1
R. H. Shumway and D. S. Stoffer. An approach to time series smoothing and forecasting using
the EM algorithm. Journal of Time Series Analysis, 3(4):253–264, 1982. 5.2.2
P. Smyth, D. Heckerman, and M. I. Jordan. Probabilistic independence networks for hidden
Markov probability models. Neural Computation, 9:227–269, 1997. 3.1
M. Stephens. Bayesian methods for mixtures of normal distributions. PhD thesis, Oxford Uni-
versity, 1997. 2.3.2
A. Stolcke and S. Omohundro. Hidden Markov model induction by Bayesian model merging.
In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information
Processing Systems 5, pages 11–18, San Francisco, CA, 1993. Morgan Kaufmann. 3.3
279
Bibliography Bibliography
A. M. Storkey. Dynamic trees: A structured variational method giving efﬁcient propagation
rules. In Proc. 16th Conf. on Uncertainty in Artiﬁcial Intelligence. UAI, San Francisco, CA,
2000. Morgan Kaufmann Publishers. 2.5.1
A. Thomas, D. J. Spiegelhalter, and W. R. Gilks. BUGS: A program to perform Bayesian
inference using Gibbs sampling. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M.
Smith, editors, Bayesian Statistics 4, pages 837–842. Clarendon Press, 1992. 7.1
M. E. Tipping and C. M. Bishop. Mixtures of probabilistic principal component analyzers.
Neural Computation, 11(2):443–482, 1999. 4.1.2
G. M. Torrie and J. P. Valleau. Nonphysical sampling distributions in Monte Carlo free energy
estimation: Umbrella sampling. J. Comp. Phys., 23:187–199, 1977. 6.3.5
N. Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton. SMEM algorithm for mixture models.
Neural Computation, 12(9):2109–2128, 2000. 4.3, 4.3.2, 4.5.3, 4.5, 4.6.2
H. Valpola and J. Karhunen. An unsupervised ensemble learning method for nonlinear dynamic
state-space models. Neural Computation, 14(11):2647–2692, 2002. 5.6
A. J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory, 13(2):260–269, 1967. 3.3
M. J. Wainwright, T. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log parti-
tion function. In Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence.
Morgan Kaufmann, 2002. 7.1
C. S. Wallace and P. R. Freeman. Estimation and inference by compact coding. Journal of the
Royal Statistical Society, Series B (Methodological), 49(3):240–265, 1987. With discussion.
1.3.4
S. Waterhouse, D. J. C. MacKay, and T. Robinson. Bayesian methods for mixtures of experts.
In Advances in Neural Information Processing Systems 8, Cambridge, MA, 1996. MIT Press.
2.3.2
M. Welling and Y . W. Teh. Belief Optimisation for binary networks: A stable alternative to
loopy belief propagation. In UAI 2001, Seattle, Washington, 2001. 7.1
C. K. I. Williams and N. J. Adams. DTs: Dynamic trees. In Advances in Neural Information
Processing Systems 11, Cambridge, MA, 1999. MIT Press. 2.5.1
C. K. I. Williams and G. E. Hinton. Mean ﬁeld networks that learn to discriminate temporally
distorted strings. In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton, edi-
tors, Connectionist Models: Proceedings of the 1990 Summer School , pages 18–22. Morgan
Kaufmann Publishers, San Francisco, CA, 1991. 2.2.3
280
Bibliography Bibliography
J. Yedidia, W. T. Freeman, and Y . Weiss. Generalized belief propagation. In T. K. Leen, T. G.
Dietterich, and V . Tresp, editors, Advances in Neural Information Processing Systems 13 ,
Cambridge, MA, 2001. MIT Press. 7.1
A. L. Yuille. CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent
alternatives to belief propagation. Technical report, Smith-Kettlewell Eye Research Institute,
2001. 7.1
281