arXiv:2307.00504v1  [cs.LG]  2 Jul 2023
ON E FFI C I E N T C O M P U TAT IO N I N AC T I V E I N F E R E N CE
A P RE P RIN T
Aswin Paul1, 2, 3 , Noor Sajid4 , Lancelot Da Costa4, 5, 6 and Adeel Razi1, 4, 7
1 Turner Institute for Brain and Mental Health, Monash Univer sity, Clayton 3800, Australia
2 IITB-Monash Research Academy, Mumbai, India
3 Department of Electrical Engineering, IIT Bombay, Mumbai, India
4 W ellcome Centre for Human Neuroimaging, University Colleg e London, WC1N 3AR London, United Kingdom
5 Department of Mathematics, Imperial College London, Londo n, SW7 2AZ, UK
6 VERSES Research Lab, Los Angeles, CA 90016, USA
7 CIF AR Azrieli Global Scholars Program, CIF AR, T oronto, Can ada
July 4, 2023
ABSTRACT
Despite being recognized as neurobiologically plausible, active inference faces difﬁculties when em-
ployed to simulate intelligent behaviour in complex enviro nments due to its computational cost and
the difﬁculty of specifying an appropriate target distribu tion for the agent. This paper introduces two
solutions that work in concert to address these limitations . First, we present a novel planning algo-
rithm for ﬁnite temporal horizons with drastically lower co mputational complexity. Second, inspired
by Z-learning from control theory literature, we simplify t he process of setting an appropriate target
distribution for new and existing active inference plannin g schemes. Our ﬁrst approach leverages
the dynamic programming algorithm, known for its computati onal efﬁciency, to minimize the cost
function used in planning through the Bellman-optimality p rinciple. Accordingly, our algorithm re-
cursively assesses the expected free energy of actions in th e reverse temporal order. This improves
computational efﬁciency by orders of magnitude and allows p recise model learning and planning,
even under uncertain conditions. Our method simpliﬁes the p lanning process and shows meaning-
ful behaviour even when specifying only the agent’s ﬁnal goa l state. The proposed solutions make
deﬁning a target distribution from a goal state straightfor ward compared to the more complicated
task of deﬁning a temporally informed target distribution. The effectiveness of these methods is
tested and demonstrated through simulations in standard gr id-world tasks. These advances create
new opportunities for various applications.
Keywords Active inference ·Dynamic programming ·Stochastic control ·Reinforcement learning
1 Introduction
How should an organism perceive, learn, and act to ensure sur vival when born into a new world? How do ‘agents’
eventually learn to exhibit sentient behaviour in nature, s uch as hunting and navigation?
A prominent framework that approaches these questions is st ochastic optimal control (SOC), which determines the
best possible set of decisions—given a speciﬁc criterion—a t any given time and in the face of uncertainty. The
fundamental problem that SOC addresses can be deﬁned as foll ows: When born at time t = 1 and ahead, an ‘agent’
receives observations from its surrounding ‘environment’ . This ‘agent’ not only passively receives observations but
also is capable of responding with ‘actions’. Additionally , it may receive information or has inbuilt reward systems
that quantify its chance of survival and progress. So, this p rocess may be summarised as a stream of data from the
agent’s perspective: (o1; a1), (o2, r 2; a2), ..., (ot, r t). Here, ot stands for the observation at time t, at stands for the
agent’s action at time t, and rt stands for the ‘reward’ at time t from the external environment or agent’s inbuilt reward
structure. In this setting, the primary goal of an agent is to
On efﬁcient computation in active inference A P R E P R I N T
Maximise: Score =
t∑
1
rt.1 (1)
Eq.1 is an optimisation problem, and due to its general struc ture, it has a vast scope in various disciplines across
the sciences. Several ﬁelds of research grew around this ide a in the past decades, like reinforcement learning (RL)
[Sutton and Barto, 2018], control theory [T odorov, 2006, 20 09], game theory [Fudenberg and Tirole, 1991, Lu et al.,
2020], and economics [Mookherjee, 1984, V on Neumann and Mor genstern, 1944]. But in fact, formulating decision-
making as utility maximisation originated much earlier in t he ethical theory of utilitarianism in 18th-century philos -
ophy [Bentham, 1781, Mill, 1870], and was later applied by Pa vlov in the early 20th century to account for animal
conditioning [Pavlov , 1927]. Many current engineering met hods, such as Q-learning [W atkins and Dayan, 1992],
build upon the Bellman-optimality principle to learn prope r observation-action mappings that maximise cumulative
reward. Model-based methods in RL, like Dyna-Q Peng and Will iams [1993], employ an internal model of the ‘envi-
ronment’ to accelerate this planning process Sutton and Bar to [2018]. Similarly, efﬁcient methods, e.g., which linear ly
scales with the problem dimensions, emerged in classical co ntrol theory to compute optimal actions in similar settings
[T odorov, 2006, 2009].
Another critical and complementary research direction is s tudying systems showing ‘general intelligence’, which
abounds in nature. Indeed, we see a spectrum of behaviour in t he natural world that may or may not be accountable
by the rather narrow goal of optimising cumulative reward. B y learning more about how the brain produces sentient
behaviour, we can hope to accelerate the generation of artiﬁ cial general intelligence [Goertzel, 2014, Gershman, 2023 ].
This outlook motivates us to look into the neural and cogniti ve sciences, where an integral theory is the free energy
principle (FEP), which brings together Helmholtz’s early o bservations of perception with more recent ideas from sta-
tistical physics and machine learning [Feynman, 1998, Daya n et al., 1995] to attempt a mathematical description of
brain function and behaviour in terms of inference that has t he potential of unifying many previous theories on the sub-
ject, including but not limited to cumulative reward maximi sation [Friston, 2010, Friston et al., 2022, Da Costa et al.,
2023].
In the last decade, the FEP has been applied to model and gener ate biological-like behaviour under the banner of
active inference [Da Costa et al., 2020]. Active inference has since percolat ed into many adjacent ﬁelds owing to its
ambitious scope as a general modelling framework for behavi our [Pezzato et al., 2023, Oliver et al., 2022, Deane et al.,
2020, Rubin, 2020, Fountas et al., 2020, Matsumoto et al., 20 22]. In particular, several recent experiments posit activ e
inference as a promising approach to optimal control and exp lainable and transparent artiﬁcial intelligence Friston e t al.
[2009], Friston [2012], Sajid et al. [2021a], Mazzaglia et a l. [2022], Millidge et al. [2020], Albarracin et al. [2023]. In
this article, we consider active inference as an approach to stochastic control, its current limitations, and how they c an
be overcome with dynamic programming and the adequate speci ﬁcation of a target distribution.
In the following three sections, we consider the active infe rence framework, discuss existing ideas accounting for
perception, planning and decision-making—and identify th eir limitations. Next, in Section 5, we show how dynamic
programming can address these limitations by enabling efﬁc ient planning and can scale up existing methods. W e
formalise these ideas in a practical algorithm for partiall y observed Markov decision processes (POMDP) in Section5.1 .
Then we discuss the possibility of learning the agent’s pref erences by building upon Z-learning [T odorov, 2006] in
Section 6. W e showcase these innovations with illustrative simulations in Section 8.
2 Active inference as biologically plausible optimal contr ol
The active inference framework is a formal way of modelling t he behaviour of self-organising systems that interface
with the external world and maintain a consistent form over t ime Friston et al. [2021], Kaplan and Friston [2018],
Kuchling et al. [2020]. The framework assumes that agents em body generative models of the environment they interact
with, on which they base their (intelligent) behaviour [Tsc hantz et al., 2020, Parr and Friston, 2018]. The framework,
however, does not impose a particular structure on such mode ls. Here, we focus on generative models in the form
of partially observed Markov decision processes (POMDPs) f or their simplicity and ubiquitous use in the optimal
control literature Lovejoy [1991], Shani et al. [2013], Kae lbling et al. [1998]. In the next section, we discuss the basi c
structure of POMDPs and how the active inference framework u ses them.
1 Reward scores the desirability for a particular outcome or s tate; akin to some cost function. Brieﬂy , it can be explicitl y deﬁned
by the ’external’ environment (extrinsic reward) or intern ally by the agent itself (intrinsic reward).
2
On efﬁcient computation in active inference A P R E P R I N T
2.1 Generative models using POMDPs
Assuming agents have a discrete representation of their sur rounding environment, we turn to the POMDP framework
[Kaelbling et al., 1998]. POMDPs offer a fairly expressive s tructure to model discrete state-space environments where
parameters can be expressed as tractable categorical distr ibutions. The POMDP-based generative model can be for-
mally deﬁned as a tuple of ﬁnite sets (S, O, U, B, A):
◦s ∈S : S is a set of hidden states ( s) causing observations o.
◦o ∈O : O is a set of observations, where o = s, in the fully observable setting. In a partially observable
setting, o = f(s).
◦u ∈U : U is a set of actions ( u) Eg: U = {Left, Right, Up, Down }.
◦B : encodes the one-step transition dynamics, P (st|st− 1, ut− 1) i.e., the probability that when action ut− 1 is
taken while being in state st− 1 (at time t −1) results in st at time t.
◦A : encodes the likelihood mapping, P (oτ |sτ ) for the partially observable setting.
◦D : Encodes the prior of the agent about the hidden state factor s.
◦E : Encodes the prior of the agent about actions u.
In a POMDP , the hidden states ( s) generate observations ( o) through the likelihood mapping ( A) in the form of a
categorical distribution, P (oτ |sτ ) = Cat( A ×sτ ). B is a collection of square matrices Bu, where Bu represents
transition dynamics P (st|st− 1, ut− 1 = u): The transition matrix ( B) determines the dynamics of s given the agent’s
action u as P (st|st− 1, ut− 1) = Cat( But− 1 ×st− 1). In [A ×sτ ] and [Buτ ×sτ ], sτ is represented as a one-hot vector
that is multiplied through regular matrix multiplication 2 . The Markovianity of POMDPs means that state transitions
are independent of history (i.e. state st only depends upon the state-action pair (st− 1, ut− 1) and not st− 2, u t− 2 etc.).
In summary, the generative model can be summarised as follow s,
P (o1:t, s1:t, u1:t) = P (A)P (B)P (D)P (E)
t∏
τ =1
P (oτ |sτ , A)
t∏
τ =2
P (sτ |sτ − 1, uτ − 1, B). (2)
So, from the agent’s perspective, when encountering a strea m of observations in time, such as (o1, o2, o3, ..., ot), as
a consequence of performing a stream of actions (u1, u2, u3, ..., ut− 1), the generative model quantitatively couples
and quantiﬁes the causal relationship from action to observ ation through some assumed hidden states of the environ-
ment. These are called ‘hidden’ states because, in POMDPs, t he agent cannot observe them directly. Based on this
representation, an agent can now attempt to optimise its act ions to keep receiving preferred observations. Currently,
the generative model has no concept of ‘preference’ and ‘goa l’ [Bruineberg et al., 2018]. Rather than attempting to
maximise cumulative reward from the environment, active in ference agents minimise the ‘surprise’ of encountered
observations [Sajid et al., 2021a,b]. W e look at this idea cl osely in the next section.
2.2 Surprise and free energy
The surprise of a given observation in active inference [Fri ston, 2019, Sajid et al., 2021a] is deﬁned through the relati on
S(o) = −log(P (o)). (3)
Please note that the agent does not have access to the true pro bability of an observation: Ptrue (o). However, the
internal generative model expects an observation with a cer tain probability P (o), which quantiﬁes surprise in Eq.3.
Minimising surprise directly requires the marginalisatio n of the generative model, i.e., P (o) = ∑
s P (o, s), which is
often computationally intractable due to the large size of t he state-space [Blei et al., 2017, Sajid et al., 2022a]. Sinc e
f(x) = log(x) is a convex function, we can solve this problem by deﬁning an u pper-bound to surprise using Jensen’s
inequality 3 :
S(o) = −log
∑
s
P (o, s) ≤−
∑
s
Q(s)log P (o, s)
Q(s) = F [Q]. (4)
2 One-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the
others low (0). Here, the bit (1) is allocated to the state s= sτ
3Jensen’s inequality: If Xis a random variable and ψ is a convex function, ψ (E[X]) ≤ E [ψ(X)].
3
On efﬁcient computation in active inference A P R E P R I N T
The newly introduced term Q(s) is often interpreted as an (approximate posterior) belief a bout the (hidden) state: s.
This upper bound (F) is called the variational free energy (V FE) (it is also commonly known as evidence lower bound
– ELBO [Blei et al., 2017] 4 ). So, by optimising the belief Q(s) to minimise the variational free energy (F), an agent
is capable of minimising the surprise S(o) = −log(P (o)) or at least maintain it bounded at low values.
How is this formulation useful for stochastic control? Imagine the agent embodies a biased generative model
with ‘goal-directed’ expectations for observations. The g oal then becomes to minimise F , which can be done through
the conjunction of perception, i.e., optimising the belief Q(s), or action, i.e., controlling the environment to sample
observations that lead to a lower F [Tschantz et al., 2020]. So, instead of passively inferring what caused observations,
the agent starts to ‘actively’ infer, exerting control over the environment using available actions in U. The central
advantage of this formalism is that there is now only one sing le cost function ( F ) to optimise all aspects of behaviour,
such as perception, learning, planning, and decision-maki ng (or action selection). There are related works in the
reinforcement literature noting the use of similar informa tion-theoretic metrics for control Rhinehart et al. [2021] ,
Berseth et al. [2019]. The following section discusses this feature in detail and further develops the active inference
framework.
3 Perception and learning
3.1 Perception
From the agent’s perspective, perception means (Bayes opti mally) maintaining a belief about hidden states s causing
the observations o. In active inference, agents optimise the beliefs Q(s) to minimise F . The VFE may be rewritten
(from Eq.4), using the identity P (o, s) = P (s)P (o|s), as:
F =
∑
s
Q(s)[ logQ(s) −logP (o|s) −logP (s) ] . (5)
Differentiating F w .r.t Q(s) and setting the derivative to zero, we get (see Supplementar y A),
δF
δQ(s) =
∑
s
1 + logQ(s) −logP (o|s) −logP (s) = 0 . (6)
Using the above equation, we can evaluate the optimal Q(s) that minimises 5 F using,
logQ∗ (s) = logP (s) + logP (o|s). (7)
This equation provides the (Bayesian) belief propagation s cheme, given by
Q(st+1)  
Posterior
= σ

logP (st+1)
 
Prior
+ log(ot+1 ·Ast+1)  
Likelihood

 . (8)
Here, σ is the softmax function; that is, the exponential of the inpu t that is then normalised so that the output is a
probability distribution. Given a real-valued vector V in RK , the i-th element of σ(V ) reads:
σ(V )i = exp V i
∑ K
j=1 exp V j
, (9)
where V i corresponds to the i-th element of V . W e estimate the ﬁrst term of Eq.8, i.e. the prior using belie f Q(st) at
time t, and the action ut taken at time t. Using the transition dynamics operator But , we write:
P (st+1) = But ·Q(st). (10)
4The connection between the two lies in the fact that they are e ssentially equivalent up to a constant (the log evidence), b ut with
opposite signs. In other words, minimizing VFE is equivalen t to maximizing the ELBO. Formally this is: VFE = − ELBO +
constant
5The second derivate of Eq.6 w .r.t to Q(s) is greater than zero which corresponds to local minima of F w .r.t to Q(s).
4
On efﬁcient computation in active inference A P R E P R I N T
At the ﬁrst time step, i.e. t = 0 , we use a known prior about the hidden state D to substitute for the term P (st+1).
Similarly, the second term in Eq.8, i.e., the estimate of the hidden state from the observation we gathered from the
environment at time t + 1 can be evaluated as the dot product between the likelihood fu nction A and the observation
gathered at time t + 1 . The belief propagation scheme here is shown in the literatu re to have a degree of biological
plausibility in the sense that it can be implemented by a loca l neuronal message-passing scheme [de Vries and Friston,
2017]. The following section discusses the learning of the m odel parameters.
3.2 Learning
The parameter learning rules of our generative model are deﬁ ned in terms of the optimised belief about states Q(s).
In our architecture, the agent uses belief propagation 6 to best estimate Q(s), the belief about (hidden) states in the
environment. Given these beliefs, observations sampled, a nd actions undertaken by the agent, the agent hopes to learn
the underlying contingencies of the environment. The learn ing rules of active inference consist of inferring paramete rs
of A, B, and D at a slower time scale. W e discuss such learning rules in deta il in the following.
3.2.1 T ransition dynamics
Agents learn the transition dynamics, B, across time by maintaining a concentration parameter bu, using conjugate
update rules well documented in the active inference litera ture[Friston et al., 2017, Da Costa et al., 2020, Sajid et al. ,
2021a] such as:
bu ←bu + Q(ut− 1) ·(Q(st) ⊗Q(st− 1)) , (11)
where Q(u) is the probability of taking action u, Q(st) is belief the state at time t as a consequence of action u at t−1,
and Q(st) ⊗Q(st− 1) represents a square matrix of Kronecker product between two vectors Q(st) and Q(st− 1).
Every column of the transition dynamics Bu, can be estimated from bu column-wise as,
col(Bu)i = Dir [col(bu)i] . (12)
Here, col (X)i is the i-th column of X. Dir(bu) represents the mean of the Dirichlet distribution 7 with parameter bu.
3.2.2 Likelihood
Similar to the conjugacy update in Eq.11, the Dirichlet para meter ( a) for the likelihood dynamics ( A) is learned over
time within trials using the update rule,
a ←a + ot ⊗Q(st). (13)
Here, ot is the observation gathered from environment at time t, and Q(st) ≈P (st|o1:t) is the approximate posterior
belief about the hidden-state ( s) [Friston et al., 2017, Da Costa et al., 2020].
Like perception and learning, decision-making and plannin g can also be formulated around the cost function F and
belief Q. In the next section, we review in detail existing ideas [Fri ston et al., 2021, Sajid et al., 2022b] for planning
and decision-making. W e then identify their limitations an d, next, propose an improved architecture.
4 Planning and decision making
4.1 Classical formulation of active inference
Traditionally, planning and decision-making by active inf erence agents revolve around the goal of minimising the
variational free energy of the observations one expects in t he future. T o implement this, we deﬁne a policy space
6W e stick to the belief propagation scheme for perception in t his paper. However, general schemes like variational messa ge
passing may be used to estimate Q(s).
7 Dirichlet distributions are commonly used as prior distrib utions in Bayesian statistics given that the Dirichlet dist ribution is
the conjugate prior of the categorical distribution and mul tinomial distribution. W e use the mean of the Dirichlet dist ribution here
because we are performing a Bayesian update. Brieﬂy , the mea n of a Dirichlet distribution with parameters b = (b1,...,b K ) is
given by µ = (µ1,...,µ K ) where µk = bk∑ K
j=1 bj
. So, in this case, each entry in the estimated transition pro babilities is the
corresponding entry in bu, divided by the sum of all entries in the corresponding colum n of bu. This normalization ensures that the
columns of Bu sum to 1.
5
On efﬁcient computation in active inference A P R E P R I N T
comprising sequences of actions in time. The policy space in classical active inference [Sajid et al., 2021a] is deﬁned
as a collection of policies πn
Π = {π1, π2, ..., πN }, (14)
which are themselves sequences of actions indexed in time; t hat is, π = ( u1, u2, ..., uT ), where ut is one of the
available action in U, and T is the agent’s planning horizon. N is the total number of unique policies deﬁned by
permutations of available actions u over a time horizon of planning T .
T o enable goal-directed behaviour, we need a way to quantify the agent’s preference for sample observations o. The
prior preference for observations is usually deﬁned as a cat egorical distribution over observations,
C = Cat( o). (15)
So, if the value corresponding to an observation in C is the highest, it is the most preferred observation for the a gent.
Given these two additional parameters ( Π and C), we can deﬁne a new quantity called the expected free energy (EFE)
of a policy π similar to the deﬁnition in [Sajid et al., 2021a, Schwartenb eck et al., 2019, Parr and Friston, 2019] as,
G(π) =
T∑
t=1
DKL
[
Q(ot|πt) ||C
]
  
Risk
+ EQ(st|st− 1,πt− 1) [H [P (ot|st)]]  
Expected ambiguity
. (16)
In Eq.(16) above, πt is the t-th element in π, i.e. the action corresponding to time t for policy π. The term, Q(ot|πt)
represents the most likely observation caused by the policy π at time t. DKL stands for the KL-divergence, which,
when minimised, forces the distribution Q(ot|πt) closer towards C. This term is also called the "Risk" term, repre-
senting the goal-directed behaviour of the agent. The KL-di vergence between two distributions, P and Q, is deﬁned
as:
DKL (P ||Q) =
∑
i
P (i)log P (i)
Q(i), (17)
and P = Q if and only if DKL (P ||Q) = 0 .
In the second term of Eq.16, H [P (ot|st)] stands for the (Shannon) entropy of P (ot|st) deﬁned as,
H(P (o)) = −
∑
oǫO
P (o)logP (o). (18)
The second term is also called the ‘Expected ambiguity’ term . When the expected entropy of P (ot|st) w .r.t the belief
Q(st|st− 1, πt− 1) is less, the agent is more conﬁdent of the state-observation mapping (i.e., A) in its generative model.
Hence, by choosing the policy π to make decisions that minimise G, the agent minimises ‘Risk’ at the same time
and also its ‘ Ambiguity’ about the state-observation mappi ng. Hence, in active inference, decision-making naturally
balances the exploration-exploitation dilemma [Triche et al., 2022]. W e also note that the agent is not optimising G
but only evaluating and comparing various G over different policies π in the policy space Π . Once the best policy π
is identiﬁed, the most simple decision-making rule follows by choosing actions ut = πt at time t, where πt is the t-th
element of π.
It may already be evident that the above formulation has one f undamental problem: in the stochastic control problems
that are commonly encountered in practice, the size of possi ble action spaces U and the time horizons of planning T
make the policy space too large to be computationally tracta ble. For example, with eight available actions in U and a
time horizon of planning T = 15 , the total number of (deﬁnable) policies that need to be cons idered are ( 3.5 ∗1013)
35 trillion. Even for this relatively small-scale example, th is policy space is not computationally tractable to simulat e
agent behaviour (unless additional decision tree search me thods are considered [Fountas et al., 2020, Champion et al.,
2021a,b] or policy amortisation Fountas et al. [2020], Çata l et al. [2020]) or by eliminating implausible policy trajec -
tories using Occam’s principle. W e now turn to an improved sc heme that redeﬁnes policy space and planning all
together.
4.2 Sophisticated inference
Graduating from the classical deﬁnition of policy as a seque nce of actions in time, sophisticated inference
[Friston et al., 2021] attempts to evaluate the EFE of observ ation-action pairs at a given time t, G(ot, ut). Given
6
On efﬁcient computation in active inference A P R E P R I N T
this joint distribution, an agent can sample actions using t he conditional distribution Q(ut|ot) when observing ot at
time t,
ut ∼Q(ut|ot) = σ [−G(ot, ut)] . (19)
Given the prior-preference distribution of an agent P (s), in terms of hidden states s, the expected free energy of an
observation-action pair is deﬁned as [Friston et al., 2021] ,
G(ot, ut) = EP (ot+1|st+1)Q(st+1|u<t+1)


logQ(st+1|u<t+1) −logP (st+1)
 
Risk
−logP (ot+1|st+1)  
Ambiguity




 
EFE of action at time t
+ (20)
EQ(ut+1|ot+1)Q(ot+1|u≤ t) [G(ot+1, ut+1)]
  
EFE of future actions
. (21)
W e rewrite this equation in a familiar fashion to Eq.16. In th e above equation, the agent holds evaluated beliefs about
future hidden states given all past actions in the term Q(st+1|u<t+1). Beliefs about hidden states can be extrapolated
to observations using the likelihood mapping ( A) as
P (ot+1|st+1)Q(st+1|u<t+1) = Q(ot+1|u<t+1). (22)
Also, the prior preference of the agent is deﬁned in terms of h idden states s in Eq.20. Now the Eq.20 can be rewritten
using mappings like Eq.22 as,
G(ot, ut) = EQ(ot+1|u<t+1)


log Q(ot+1|u<t+1) −log C
 
Risk
−log P (ot+1|st+1)  
Ambiguity




 
EFE of action at time t
+ (23)
EQ(ut+1|ot+1)Q(ot+1|u≤ t) [G(ot+1, ut+1)]
  
EFE of future actions
. (24)
Note that the prior preference distribution in the equation above is over observations o, C = P (o). Rewriting Eq.23 in
a similar fashion to the previously discussed classical act ive inference we obtain
G(ot, ut) = DKL [Q(ot+1|u<t+1) ||C]  
Risk
+ EQ(st+1|u<t+1)H [P (ot+1|st+1)]  
Expected ambiguity
  
EFE of action at time t
+ (25)
EQ(ut+1 |ot+1)Q(ot+1|u≤ t) [G(ot+1, ut+1)]  
EFE of future actions
. (26)
The ﬁrst two terms can be interpreted the same way as we did for Eq.23 in the previous section. However, the third
term in Eq.25 gives rise to a recursive tree-search algorith m, accumulating free energies of the future (as deep as we
evaluate forward in time). Such an evaluation is pictoriall y represented in Fig.1 (A).
While Bellman optimal Da Costa et al. [2021], one unavoidabl e limitation of the sophisticated inference planning
algorithm is that it faces a worse curse of dimensionality fo r even relatively small planning horizons. For example,
to evaluate the goodness of an action within a period of ﬁftee n time-steps into the future, and with eight available
actions and a hundred hidden states, requires an exorbitant (100 ∗8)15 (≈3.5 ∗1043) calculations, in comparison
to 100 ∗(8)15(≈3.5 ∗1015) for classical active inference. A simple solution propose d in [Friston et al., 2021] is
to eliminate tree search branches by setting a threshold val ue to predictive probabilities such as Q(ut+1|ot+1) in
7
On efﬁcient computation in active inference A P R E P R I N T
Figure 1: Graphics to compare and contrast the differences b etween the sophisticated inference and DPEFE (Dynamic
programming in expected free energy) algorithm planning sc hemes. A: Sophisticated inference algorithm uses an
extensive tree search, going forward in time, to accumulate free energy of the future paths. So, an agent’s preference
for observations, when matched with future predictions, wi ll inform an optimal state-action trajectory, as shown in
the tree search. Light-purple states represent the preferr ed observations at that given time step, and light-blue acti ons
are the optimal actions inferred through the tree search. As noted in Friston et al. [2021], an agent can signiﬁcantly
reduce the tree search complexity by terminating the search when the action probability falls below a certain threshold .
However, this approximation does not guarantee optimal pol icy as the agent might miss preferred observations deeper
in the tree search. B: In the DPEFE algorithm, an agent starts planning backwards from a ﬁxed planning horizon. Here,
the EFE of future states informs EFE of state-action pairs on e step backward in time. Hence, the planning complexity
of tree search is avoided, but the preference for future stat es propagates to inﬂuence decisions at previous time steps.
Since the agent needs to evaluate only a table (of EFE) at ever y planning step, this planning algorithm is linear in time,
number of states, and number of actions.
Eq.25. So, for example, when Q(ut+1|ot+1) < 1/16 during planning, the algorithm terminates the search over f uture
branches. This restriction signiﬁcantly reduces the compu tational time, and a set of ensuing (meaningful) simulation s
was presented in [Friston et al., 2021].
Another limitation is that in all active or sophisticated in ference agents to facilitate desirable behaviour, a prior p refer-
ence needs to be deﬁned by the modeller or learned by the agent s Sajid et al. [2021b, 2022c] informing the agent that
some states are preferable to others, as demonstrated in Fig .2 (B) for a grid problem given in Fig.2 (A). An informed
prior preference enables the agent to solve this navigation task by only planning four or more time steps ahead. It can
take action and move towards a ‘more preferred state’ if not t he ﬁnal goal state. However, without such information,
the agent is ‘blind’ (cf. Fig.2 (C)) and can only ﬁnd the optim al move when planning the whole eight-step trajectory
for the given grid.
W e ﬁrst noticed this limitation when comparing different ac tive inference schemes to various well-known reinforce-
ment learning algorithms in [Paul et al., 2021] in a fully obs ervable setting (i.e., MDPs). In the next section, we
demonstrate how to scale the sophisticated inference schem e using dynamic programming for the general case of
POMDP-based generative models.
8
On efﬁcient computation in active inference A P R E P R I N T
Figure 2: Informed and uninformed prior preferences: A: A na vigation problem, B: A strictly deﬁned, sparse prior
preference which has information only about the ﬁnal goal st ate, C: Informed prior preference necessary for ‘pruning
of tree search’ in sophisticated inference (light colour st ates are more preferred)
5 Dynamic programming for evaluating expected free energy
The Bellman optimality principle states that the sub-polic y of an optimal policy (for a given problem) must itself
will be an optimal policy for the corresponding sub-problem [Sutton and Barto, 2018]. Dynamic programming is
an optimisation technique that naturally follows the Bellm an optimality principle; rather than attempting to solve a
problem as a whole, dynamic programming attempts to solve su b-parts of the problem and integrate the sub-solutions
into a solution to the original problem. This approach makes dynamic programming scale favourably, as we solve one
sub-problem at a time to integrate later. The more we break do wn the large problem into corresponding sub-problems,
the more computationally tractable is the solution.
Inspired by this principle, let us consider a spatial naviga tion problem that the agent needs to solve in our setting.
The optimal solution to this navigation problem is a sequenc e of individual steps. Our prior preference for the ‘goal
state’ is for the end of our time horizon of planning. So, the a gent may start planning from the last time step (a step
sub-problem) and go backwards to solve the problem. This app roach is also called planning by backward induction
Zhou et al. [2019].
So, for a planning horizon of T (i.e., the agent aims to reach goal state at time T ), the EFE of the (last) action for the
T −1th time step in a POMDP setting is written as:
G(uT − 1, oT − 1) = DKL [Q(oT |uT − 1, sT − 1) ||C]. (27)
The term, G(uT − 1|oT − 1) is the expected free energy associated with any action uT − 1, given we are in (hidden) state
sT − 1. This estimate measures how much we believe observations at time T will align with our prior preference C.
Note that, for simplicity, we ignored the ‘expected ambigui ty’ term in the equation above, i.e. the uncertainty of state -
observation mapping (or likelihood), cf. Eq.25. This does n ot affect our subsequent derivations; we can always add
it as an additional term. The following derivation provided technical details of dynamic programming while focusing
only on the ‘risk’ term in G.
T o estimate Q(oT |uT − 1, sT − 1), we make use of the prediction about states Q(sT ) that can occur at time T :
Q(sT |uT − 1, sT − 1) = BuT − 1 ·Q(sT − 1), (28)
and given the prediction Q(sT ), we write
Q(oT |uT − 1, sT − 1) = A ·Q(sT |uT − 1, sT − 1) (29)
= A ·
(
BT − 1
u ·Q(sT − 1)
)
. (30)
Next, using Eq.28, the corresponding action distribution ( for action selection) is calculated at time T ,
Q(uT − 1|oT − 1) = σ (−G(uT − 1|oT − 1)) , (31)
9
On efﬁcient computation in active inference A P R E P R I N T
where we recursively calculate the expected free energy for actions and the corresponding action-distributions for ti me
steps T −2, T −3, ..., t = 1 backwards in time, 8
G(ut|ot) = DKL [Q(ot+1 |ut, st) ||C]  
EFE of action at time t
+ EQ(ot+1,ut+1|ot,ut)[G(ut+1|ot+1)]  
EFE of next action at t + 1
. (32)
In the equation above, the second term condenses informatio n about all future observations rather than doing a forward
tree search in time. T o inform G(ut|ot), we consider all possible observation-action pairs that ca n occur in time t + 1
and use the previously evaluated G(ut+1|ot+1). In Eq.32, we evaluate Q(ot+1, ut+1|ot, ut) using,
Q(st+1, ut+1|st, ut) = Q(st+1|st, ut)  
B
·Q(ut+1|st+1)  
Action distribution
. (33)
W e then map the distribution Q(st+1, ut+1|st, ut) to the observation space and evaluate Q(ot+1, ut+1|ot, ut) using the
likelihood mapping A. In Eq.33, we assume that actions in time are independent of e ach other, i.e. ut is independent of
ut+1. Even though actions are assumed to be explicitly independe nt in time, the information (and hence desirability)
about actions are also informed backwards in time from the re cursive evaluation of expected free energy.
While evaluating the EFE, G, backwards in time, we used the action distribution in Eq.31 . This action distribution can
be directly used for action selection. Given an observation o at time t, ut may be sampled 9 from,
ut ∼Q(ut|ot = o). (34)
In the next section, we summarise the above formulation as a n ovel active inference algorithm useful for modelling
intelligent behaviour in sequential POMDP settings.
5.1 Algorithmic formulation of DPEFE
Here, we formalise a generic algorithm that can be employed f or a sequential POMDP problem. The main algorithm
(see Alg.1) works sequentially in time and brings together t hree different aspects of the agent’s behaviour, namely,
perception (inference), planning, and learning.
For planning, that is, to evaluate the expected free energy ( G) for actions (given states) in time, we employ the planning
algorithm (See. Alg.2) as a subroutine to Alg.1. In the most g eneral case, the algorithm is initialised with ’ﬂat’ priors
for the likelihood function ( A) and transition dynamics ( B). The algorithm also allows us to equip the agent with a
more informed prior about A and B. Learning C in the DPEFE algorithm is setting C as a one-hot vector with the
encountered goal state. This technique accelerates the par ameters’ learning process during trials and improves agent
performance. W e can also make available the ‘true’ dynamics of the environment to the agent whenever present. With
‘true’ dynamics available at the agent’s disposal, the agen t can infer hidden states and plan accurately. The next secti on
discusses a different approach to ameliorating the curse of dimensionality in sophisticated inference. Later, we disc uss
a potential learning rule for the prior preference distribu tion C inspired by a seminar work in the control theoretic
literature.
6 Learning prior preferences
In the previous section, we introduced a practical algorith m solution that speeds up planning in sophisticated inferen ce.
The second innovation on offer is to enable learning of prefe rences C such that smaller planning horizons become
sufﬁcient for our agent to take optimal actions, as discusse d in Fig.2. A seminal work from the literature on control
theory proposes using a ‘desirability’ function, scoring h ow desirable each state is, to compute optimal actions for a
particular class of MDPs and, importantly, showing that the planning complexity of computing those actions is linear
in time [T odorov, 2006]. When the underlying MDP model of the environment is unavailable and the agent needs to
take actions based solely on a stream of samples of states and rewards (i.e., st, rt, st+1), an online algorithm called
Z-learning, inspired from the theoretical developments in [T odorov, 2006], was proposed to solve this problem. Given
8 For times other than T− 1, the ﬁrst term in Eq.32 does not contribute to solving the par ticular instance if C only accommodates
preference to a (time-independent) goal-state. However, f or a temporally informed C, i.e. with a separate preference for reward
maximisation at each time step, this term will meaningfully inﬂuence action selection.
9 Precision of action-section may be controlled by introduci ng a positive constant inside the softmax function σ(.) in Eq.31. The
higher the constant, the higher the chance of selecting the a ction with less EFE.
10
On efﬁcient computation in active inference A P R E P R I N T
Algorithm 1 Active inference in a sequential POMDP
C ←ogoal ⊲ prior preference (Known / Learned as one-hot vector (sparse ) when encountering of goal-state
D ←sstart ⊲ Known/Learned at time t = 1
T ←Planning horizon of the DPEFE agent
a ←aprior + ǫ ⊲ The prior is usually an uninformed ’ﬂat’ distribution.
A ←Dir(a)
bu ←bprior + ǫ ∀u ⊲ ǫ is a negligible positive value to ensure numerical stabilit y
Bu ←Dir(bu) ∀u
Tmax ⊲ Threshold for episode length set in Environment
while True do ⊲ Loop forever
for t from 1 to Tmax do
Inference:
if t = 1 then
P (s1) ←D (known) ⊲ Prior at t = 1
Observe o1 = ostart from Environment
Evaluate, Q(st=1) ⊲ Inference, Ref Eq.8
else
Evaluate P (st) ⊲ Ref. Eq.10
Evaluate Q(st) ⊲ Inference, Ref Eq.8
end if
Planning and action selection
Evaluate G(ut|ot) ∀t ǫ 1, .., T −1, o ǫ O ⊲ Planning, Ref. Algorithm.2
Evaluate Q(ut) ∀t, o ⊲ Action distribution, Ref. Eq.31
Sample ut ∼Q(ut). ⊲ Sample action, Ref Eq.34
Observe ot+1 ←From-Environment by taking action ut
Learning
if t > 1 then
bu ←bu + Q(ut− 1) ·(Q(st) ⊗Q(st− 1)) ⊲ Ref Eqn.11
end if
a ←a + ot ⊗Q(st) ⊲ Ref Eqn.13
End of trial
if t = Tmax then
Environment is reset i.e sT rue ←sstart
t ←1
end if
if Goal-achieved then
C ←One-hot(ogoal)
Environment is reset i.e sT rue ←sstart
t ←1
end if
end for
A =
Dir(a) ⊲ Updating likelihood
Bu = Dir(bu) ⊲ Update beliefs about transition dynamics
end while ⊲ End of experiment
an optimal desirability function z(s), the optimal control, or policy, is analytically computabl e. The calculation of z(s)
does not rely on knowledge of the underlying MDP but instead, on the following online learning rule:
ˆz(st) ←(1 −ηt)ˆz(st) + ηt exp(rt) ˆz(st+1), (35)
where, η is a learning rate that is continuously optimised—see below . These two terms form a weighted average that
updates the estimate of ˆz(st), with ηt controlling the balance between the old estimate and the new information.
11
On efﬁcient computation in active inference A P R E P R I N T
Algorithm 2 Planning backwards in time
A ←Passed from Alg.1
B ←Passed from Algorithm.1
C ←Passed from Algorithm.1
T ←Passed from Algorithm.1
Planning
for t = T −1 to t = 1 do
if t = T −1 then
Evaluate Q(oT |uT − 1, sT − 1) ⊲ Ref. Eq.29
Evaluate G(uT − 1|oT − 1) ⊲ Ref. Eq.27
else
Evaluate Q(ot |ut− 1, ot− 1) ⊲ Ref. Eqn.29
Evaluate G(ut|ot) ⊲ Ref. Eq.32
end if
end for
Inspired by these developments, we write a learning rule for updating C which can be useful for the sophisticated
inference agent. Given the samples (ot, rt, ot+1), an agent may learn the parameter c online using a rule analogous to
Eq.35,
c(ot) ←(1 −ηt) c(ot) + ηt exp(rt) c(ot+1). (36)
In the above equation, c(ot) represents the desirability of an observation o at time t. The value of c(ot) is updated
depending on the reward received and the desirability of the observation received at the next time step c(ot+1).
The learning rate η is a time-dependent parameter in Z-learning, as given in the equation below . e is a hyperparameter
we optimise that inﬂuences how fast/slow η gets updated over time T odorov [2009]:
ηt = e
e + t. (37)
If ηt is high, the algorithm puts more weight on the new informatio n. If ηt is low , the algorithm puts more weight on
the current estimate. Using the update rule in Eq.36 with the learning rate evolving as in (37), the value of c evolves
over time and may be used to update C online, ensuring that C is a categorical distribution over observations using the
softmax function:
C = σ(c). (38)
W e use the standard grid world environments as shown in Fig.3 for the evaluation of the performance of various agents
(more details in the next sections). Fig.6, is a visualisati on that represents the learned prior preference (for the gri d
shown in Fig. 3 (A)) useful for the sophisticated inference a gent. With an informed prior preference like this, the agent
needs to plan only one time step ahead to navigate the grid suc cessfully. It should be noted that in the DPEFE setting,
we ﬁx the prior preference C either before a trial or learn it when we encounter the goal as a one-hot vector. W e are not
learning an informed prior preference for the DPEFE agent in the simulations presented in the paper. The method for
learning prior preference discussed in this section holds f or any agent, but in our paper, DPEFE is not using this feature
to demonstrate its ability to plan deeper. When we aid an acti ve inference algorithm with the learning rule for C, a
planning horizon of T = 1 sufﬁces to take desirable actions (i.e. with no deep tree sea rch like in SI or policy space ( Π )
as in CAIF). With considering only the next time step, (i.e. o nly the consequence of immediately available actions),
planning in all active inference agents (CAIF , SI, and DPEFE ) are algorithmically equivalent. In the rest of the paper,
we call this agent with planning horizon T = 1 , which is aided with the learning rule of C as active inference AIF ( T
= 1) agent. In our simulations, we compare the performance of these two approaches (i.e., deep planning with sparse
C and short-term planning with learning C). An animation that visualises the learning prior preferen ce distribution
for the grid over 50 episodes in Fig.2 can be found in this link. In the following s ection, we discuss and compare the
computational complexity of planning between existing and newly introduced schemes.
7 Computational complexity
In this section, we compare the computational complexity in evaluating the expected free energy term, used for
planning and decision making, with two other active inferen ce approaches: classical active inference Da Costa et al.
[2020],Sajid et al. [2021a], and sophisticated inference F riston et al. [2021].
12
On efﬁcient computation in active inference A P R E P R I N T
In classical active inference (Da Costa et al. [2020], Sajid et al. [2021a]), the expected free energy for an MDP (i.e., a
fully observable case) is given by,
G(π|st− 1) = DKL [Q(st|π)||P (st)]. (39)
Here, P (st) represents an agent’s prior preference and is equivalent to C in an MDP setting. In this paper, C is directly
deﬁned in terms of the hidden states. T o avoid confusion, we a lways use the notation C in this paper regarding the
observations o.
Similarly, for sophisticated inference [Friston et al., 20 21], we have,
G(ut) = DKL [Q(st+1|u<t+1)||P (st+1)] + EQ(ut+1)[G(ut+1)]. (40)
In the above equation, we restrict the recursive evaluation of the second term, forward in time, till a ‘planning horizon
(T)’ as mentioned in Friston et al. [2021]. T necessary for ’full-depth planning’ i.e planning to the end of the episode is
often required for sparsely deﬁned prior preferences. This is required since the agent would not be able to differentiat e
the desirability of actions until reaching the last step of t he episode through a tree search.
In classical active inference, to evaluate Eq.39, the compu tational complexity is proportional to: O[card(S) ×
card(U)T ]. For sophisticated inference, to evaluate Eq.40, the comple xity scales proportionally to: O[(card(S) ×
card(U))T ]. The dimensions of the quantities involved are speciﬁed in T a b.1. And recall that both Eq.39 and Eq.40
ignore the ‘ambiguity’ term for simplicity.
Sophisticated inference Classical active inference
T erm Dimension T erm Dimension
sτ +1 card(S) (cardinality of S) π card(U)T
Q(ut+1) card(U) st card(S)
Q(st+1|u<t+1) card(S) Q(st|π) card(S) ×card(U)T
P (sτ +1) card(S) P (st) card(S)
G(ut+1) card(S) G(π|t) card(U)T
O[(card(S) ×card(U))T ] O[card(S) ×card(U)T ]
T able 1: Computational complexity in evaluating EFE.
For evaluating EFE using dynamic programming, the expected free energy for an MDP can be deduced from Eq.32 as,
G(ut|st) = DKL [Q(st+1|st, ut)||P (st+1)] + EQ(st+1|st,ut)[G(ut+1|st+1)]. (41)
Since we only evaluate on time-step ahead in Eq.41, even when evaluating backwards in time, the complexity scales
as: O[card(S) ×card(U) ×T ].
8 Simulations results
8.1 Setup
W e perform simulations in the standard grid world environme nt in Fig.3 to evaluate the performance of our proposed
algorithms. The agent is born in a random start state at the be ginning of every episode and can take one of the four
available actions (North, South, East, W est) at every time s tep to advance towards the goal state until the episode
terminates either by a time-out ( 10000, 20000, and 40000 steps for the grids in Fig.3 respectively) or by reaching the
goal state. For completeness, we compare the performance of the following algorithms in the grid world:
• Q-learning: a benchmark model-free RL algorithm W atkins a nd Dayan [1992]
• Dyna-Q: a benchmark model-based RL algorithm improving up on Q-learning Peng and Williams [1993]
• DPEFE algorithm with strictly deﬁned (sparse) C (See Sec.5.1)
• Active inference algorithm aided with learning rule for C (See Sec.6) and planning horizon of T = 1 i.e with
no deep tree search like in SI, or policy space ( Π ) as in CAIF . With considering only the next time step, (i.e.
only the consequence of immediately available actions) pla nning in all active inference agents (CAIF , SI, and
DPEFE) are algorithmically equivalent. In the rest of the pa per, we call this agent with planning horizon
T = 1 aided with the learning rule of C as active inference AIF ( T = 1) agent.
13
On efﬁcient computation in active inference A P R E P R I N T
Figure 3: A: A standard grid world of 100 states with 50 valid s tates. B: A grid of 400 states with 204 valid states.
C: A grid of 900 states with 497 valid states. These three grid s are used for evaluating the performance of various
schemes.
Figure 4: The summary of agents’ performance in the two grids . A: Deterministic grid (100 states), B: Stochastic
version of the grid in A (100 states, partially observable, s tochastic transitions (POMDP)).
W e perform simulations in deterministic and stochastic gri d variations shown in Fig.3. The deterministic variation is
a fully observable grid with no noise. So, an agent fully obse rves the present state—i.e., an MDP setting. Also, the
outcomes of actions are non-probabilistic with no noise—i. e., a deterministic MDP setting. In the stochastic variatio n,
we make the environment more challenging to navigate by addi ng 25% noise in the transitions and 25% noise in the
observed state. In this case, the agent faces uncertainty at every time step about the underlying state (i.e., partially
observable) and the next possible state (i.e., stochastic t ransitions)—i.e., a stochastic POMDP setting.
8.2 Summary of results
The agents’ performance in this navigation problem is summa rised in Fig.4 and Fig.5. Performance is quantiﬁed in
terms of how quickly an agent learns to solve the grid task, i. e. the total score. The agent receives a reward of ten
points when the goal state is reached and a small negative rew ard for every step taken. The total score hence represents
how fast the agent navigated to the goal state for a given epis ode. The grid has a ﬁxed goal state throughout the
episodes in Fig.4 (A, B) and Fig.5 (A). For simulations in Fig .5 (B), the goal state is shifted to another random state
14
On efﬁcient computation in active inference A P R E P R I N T
Figure 5: The summary of agents’ performance in the two grids . A: Stochastic grid (400 states, partially observable,
stochastic transitions (POMDP)), B: Stochastic grid in A wi th goal-state randomized after every ten episodes.
every 10 episodes. This setup helps to evaluate the adaptabi lity of agents in the face of changes in the environment. It
is clear that during the initial episodes, the agents take lo nger to reach the goal state but learn to navigate quicker as t he
episodes unfold. Standard RL algorithms (i.e., Dyna-Q and Q -Learning) are used here to benchmark the performance
of active inference agents, as they are efﬁcient state-of-t he-art algorithms to solve this sort of task.
In our simulations, the DPEFE algorithm performs at par with the Dyna-Q algorithm with a planning depth of T = 80
10 (See (Fig.4 (A, B), Fig.5 (A)). The DPEFE agent performs even better when we randomised goal-states every 10
episode (Fig.5 (B)). In contrast to online learning algorit hms like Dyna-Q, active inference agents can take advantage
of the re-deﬁnable explicit prior preference distribution C. For the AIF( T = 1) agent, we observe that the performance
improves over time but is not as good as the DPEFE agent. This i s because the AIF( T = 1 ) agent plans for only one
step ahead in our trials by design. W e could also observe that the Q-Learning agent performs worse than the random
agent and recovers slower than the AIF( T = 1 ) agent when faced with uncertainty in the goal state. It is a p romising
direction to optimise the learning of prior preference C in the AIF( T = 1 ) agent, ensuring accuracy in the face of
uncertainty. All simulations were performed for 100 trials with different random seeds to ensure the reproducib ility of
results.
Besides this, we observe a longer time to achieve the goal sta te for both active inference agents (even longer than the
‘Random agent’) in the initial episodes. This is a character istic feature of active inference agents, as their explorat ory
behaviour dominates during the initial trials. The goal-di rected behaviour dominates only after the agent sufﬁcientl y
minimises uncertainty in the model parameters Tschantz et a l. [2020].
8.3 Optimising learning rate parameter for AIF (T = 1) agent
The learning rule proposed for sophisticated inference in E q.37 requires a (manually) optimised value of e for every
environment that inﬂuences the learning rate η. T odorov [2009] inspires this learning rule, where the valu e of ηt
determines how fast the parameter c converges for a given trial. The structure of learned c is crucial for the active
inference agent, as C determines how meaningful the planning is for the agent. In F ig.B.1, we plot the performance
of the AIF( T = 1) agent as a function of e for the grids in Fig. 3. A promising direction for future rese arch is to
improve the learning rule based on η and ﬁne-tune the method for learning C. The observation in Fig.B.1 is that the
10 a planning horizon more than any optimal path in this grid. Si nce the start state is randomized, optimal paths can have man y
lengths in the grid. A planning depth of T = 80ensures that the agent plans enough not to miss the length tha t needs to be covered
in any setting.
15
On efﬁcient computation in active inference A P R E P R I N T
Figure 6: A: The sparsely deﬁned preference distribution us ed by DPEFE agent in simulations, B: The learned prefer-
ence distribution by AIF ( T=1) agent over 50 episodes. Lighter colours imply a higher pr eference for the corresponding
states.
Figure 7: Comparing computational complexity of methods di scussed in this paper. A: Order of computational com-
plexity (log scale) vs Time horizon of planning (T). Here card(S) = 100 , card(U) = 4 ., B: Order of computational
complexity (linear scale) vs card(S). Here, card(U) = 4 and T = 2 (T=1 for AIF (T=1) agent). W e observe that
except for DPEFE and AIF (T=1) methods, computational compl exity becomes intractable even for T as small T = 2
and card(S) = 5 .
performance of the AIF( T = 1) agent is not heavily dependent on the value of e. W e used different values of e > 10000
in AIF( T = 1) agents in all settings in this paper.
8.4 An emphasis on computational complexity
T o understand why the classical active inference (CAIF) and SI methods cannot solve these grid environments with the
traditional planning method, we provide an exemplar settin g in T ab. 2. Consider the small grid as shown in Fig.3 with
card(S) = 100 , card(U) = 4 , T = 30 . T ab.2 summarises the computational complexity of simulat ing various active
16
On efﬁcient computation in active inference A P R E P R I N T
inference agents for this small grid world problem. The comp utational complexity exceeds practical implementations
even with a T = 2 planning horizon. W e can observe this visually in Fig.7.
Method Order dimension ( O) Approx. Ofor speciﬁc case
CAIF (T = 30) O[card(S) ×card(U)T ] 1018
SI (T = 30) O[(card(S) ×card(U))T ] 1068
SI (T = 2) O[card(S) ×card(U)2]. 106
DPEFE (T = 30) ∗ O[card(S) ×card(U) ×T ]. 103
AIF ( T = 1) ∗ O[card(S) ×card(U)]. 0.2 ∗103
T able 2: Computational complexity for evaluating EFE with card(S) = 100 , card(U) = 4 , T = 30 . CAIF: classical
active inference, SI: sophisticated inference with full tr ee search, DPEFE: dynamic programming active inference
agent, AIF ( T = 1): Active inference agent planning one time-step ahead an d learning the prior preference, *proposed
in this paper. Without tree search, the sophisticated infer ence (SI) agent is algorithmically equivalent to the classi cal
active inference agent (CAIF). In the rest of the paper, we ca ll this agent with planning horizon T = 1 as active
inference AIF( T = 1) agent.
However, we note that the proposed solution of ﬁrst learning the prior preferences (See Sec.6) using the Z-learning rule
enables the active inference (AIF) agent to learn and solve t he unknown environment by avoiding the computational
complexity of a deep tree search. It should also be noted that neither of the active inference algorithms (DPEFE and
AIF ( T=1)) was equipped with meaningful priors about the (generat ive) model parameters ( B, C, and D). Agents
start blindly with ‘uninformed’ model priors and evolve by i ntegrating all aspects of behaviour: perception, planning ,
decision-making, and learning. Y et, the fact that, like Dyn a-Q, they start with a model of the world means that they are
much less agnostic than the model-free alternative offered by Q-learning. The following section discusses the merits
and limitations of the proposed solutions to optimise decis ion-making in active inference.
9 Discussion
In this work, we explored the usefulness of active inference as an algorithm to model intelligent behaviour and its
application to a benchmark control problem, the stochastic grid world task. W e identiﬁed the limitations of some
of the most common formulations of active inference Friston et al. [2021], which do not scale well for planning and
decision-making tasks in high-dimensional settings. W e pr oposed two computational solutions to optimise planning:
harnessing the machinery offered by dynamic programming an d the Bellman optimality principle and harnessing the
Z-learning algorithm to learn informed preferences.
First, our proposed planning algorithm evaluates expected free energy backwards in time, exploiting Bellman’s op-
timality principle, considering only the immediate future as in the dynamic programming algorithm. W e present an
algorithm for general sequential POMDP problems that combi nes perception, action selection and learning under the
single cost function of variational free energy. Additiona lly, the prior preference, i.e., the goal state about the con trol
task, was strictly deﬁned (i.e., uninformed) and supplied t o the agent, unlike well-informed prior preferences as seen
in earlier formulations. Secondly, we explored the utility of equipping agents so as to learn their prior preferences.
W e observed that learning the prior preference enables the a gent to solve the task while avoiding the computationally
(often prohibitively) expensive tree search. W e used state -of-the-art model-based reinforcement learning algorith ms,
such as Dyna-Q, to benchmark the performance of active infer ence agents. Lastly, there is further potential to optimise
computational time by exploiting approximation parameter s involved in planning and decision-making. For example,
the softmax functions used while planning and decision-mak ing determine the precision of output distributions. There
is also scope to optimise further the SI agent proposed in thi s paper by learning the prior preference. Based on the
Z-learning method, the learning rule for prior preference p arameters shall be optimised and ﬁne-tuned for active infer -
ence applications in future work. Since the Z-learning meth od is ﬁne-tuned for a particular class of MDP problems
T odorov [2006], we leave a detailed comparison of the two app roaches to future work. W e conclude that the above
results advance active inference as a promising suite of met hods for modelling intelligent behaviour and for solving
stochastic control problems.
10 Acknowledgments
AP acknowledges research sponsorship from IITB-Monash Res earch Academy, Mumbai and the Department of
Biotechnology, Government of India. AR is funded by the Aust ralian Research Council (Refs: DE170100128 &
DP200100757) and Australian National Health and Medical Re search Council Investigator Grant (Ref: 1194910). AR
17
On efﬁcient computation in active inference A P R E P R I N T
is a CIF AR Azrieli Global Scholar in the Brain, Mind & Conscio usness Program. AR, NS, and LD are afﬁliated with
The W ellcome Centre for Human Neuroimaging, supported by co re funding from W ellcome [203147/Z/16/Z]. NS is
funded by the Medical Research Council (MR/S502522/1) and t he 2021-2022 Microsoft PhD Fellowship. LD is sup-
ported by the Fonds National de la Recherche, Luxembourg (Pr oject code: 13568875). This publication is based on
work partially supported by the EPSRC Centre for Doctoral Tr aining in Mathematics of Random Systems: Analysis,
Modelling and Simulation (EP/S023925/1).
11 Software note
All the code for agents, optimisation and grid environments used are custom written in Python 3.9.15 and is available
in this project repository: https://github.com/aswinpaul/dpefe_2023.
References
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . The MIT Press, second edition,
2018. URL http://incompleteideas.net/book/the-book-2nd.html.
Emanuel T odorov. Linearly-solvable markov decision probl ems. In B. Schölkopf, J. Platt, and T . Hoff-
man, editors, Advances in Neural Information Processing Systems , volume 19. MIT Press, 2006. URL
https://proceedings.neurips.cc/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf.
Emanuel T odorov. Efﬁcient computation of optimal actions. Proceedings of the National
Academy of Sciences , 106(28):11478–11483, 2009. doi:10.1073/pnas.07107431 06 . URL
https://www.pnas.org/doi/abs/10.1073/pnas.0710743106.
Drew Fudenberg and Jean Tirole. Game theory . MIT press, 1991.
Kaihong Lu, Guangqi Li, and Long W ang. Online distributed al gorithms for seeking generalized nash equilibria in
dynamic environments. IEEE T ransactions on Automatic Control , 66(5):2289–2296, 2020.
Dilip Mookherjee. Optimal incentive schemes with many agen ts. The Review of Economic Studies , 51(3):433–446,
1984. ISSN 00346527, 1467937X. URL http://www.jstor.org/stable/2297432.
J. V on Neumann and O. Morgenstern. Theory of Games and Economic Behavior . Theory of Games and Economic
Behavior. Princeton University Press, Princeton, NJ, US, 1 944.
Jeremy Bentham. An Introduction to the Principles of Morals and Legislation. History of Economic Thought Books ,
1781.
John Stuart Mill. Utilitarianism. Longmans, Green and Company, 1870. ISBN 978-1-4992-5302- 3.
P Ivan Pavlov (1927). Conditioned reﬂexes: An investigatio n of the physiological activity of the cerebral cortex.
Annals of Neurosciences , 17(3):136–141, July 2010. ISSN 0972-7531. doi:10.5214/a ns.0972-7531.1017309 .
Christopher JCH W atkins and Peter Dayan. Q-learning. Machine learning , 8:279–292, 1992.
Jing Peng and Ronald J Williams. Efﬁcient learning and plann ing within the dyna framework. Adaptive behavior , 1
(4):437–454, 1993.
Ben Goertzel. Artiﬁcial general intelligence: concept, st ate of the art, and future prospects. Journal of Artiﬁcial
General Intelligence , 5(1):1, 2014.
Samuel J Gershman. What have we learned about artiﬁcial inte lligence from studying the brain?
https://gershmanlab.com/pubs/NeuroAI_critique.pdf, 2023. Accessed: 2023-05-29.
Richard Feynman. Statistical Mechanics: A Set Of Lectures . W estview Press, Boulder, Colo, 1st edition edition,
March 1998. ISBN 978-0-201-36076-9.
Peter Dayan, Geoffrey E. Hinton, Radford M. Neal, and Richar d S. Zemel. The Helmholtz Machine. Neural Compu-
tation, 7(5):889–904, September 1995. ISSN 0899-7667, 1530-888X . doi:10.1162/neco.1995.7.5.889 .
Karl Friston. The free-energy principle: A uniﬁed brain the ory? Nature Reviews Neuroscience , 11(2):127–138,
February 2010. ISSN 1471-003X, 1471-0048. doi:10.1038/nr n2787 .
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Ka i Ueltzhöffer, Grigorios A. Pavliotis, and Thomas
Parr. The free energy principle made simpler but not too simp le. arXiv:2201.06387 [cond-mat, physics:nlin,
physics:physics, q-bio] , January 2022.
Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, an d Ryan Smith. Reward Maximization Through Discrete
Active Inference. Neural Computation , 35(5):807–852, April 2023. ISSN 0899-7667. doi:10.1162/ neco_a_01574 .
18
On efﬁcient computation in active inference A P R E P R I N T
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan V ese lic, V ictorita Neacsu, and Karl
Friston. Active inference on discrete state-spaces: A synt hesis. Journal of Mathemati-
cal Psychology , 99:102447, 2020. ISSN 0022-2496. doi:10.1016/j.jmp.202 0.102447 . URL
https://www.sciencedirect.com/science/article/pii/S0022249620300857.
Corrado Pezzato, Carlos Hernández Corbato, Stefan Bonhof, and Martijn Wisse. Active inference and behavior trees
for reactive action planning and execution in robotics. IEEE T ransactions on Robotics , 39(2):1050–1069, 2023.
doi:10.1109/TRO.2022.3226144 .
Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empi rical study of active inference on a hu-
manoid robot. IEEE T ransactions on Cognitive and Developmental Systems , 14(2):462–471, jun 2022.
doi:10.1109/tcds.2021.3049907 . URL https://doi.org/10.1109.2021.3049907.
George Deane, Mark Miller, and Sam Wilkinson. Losing oursel ves: Active inference, depersonalization, and
meditation. Frontiers in Psychology , 11, 2020. ISSN 1664-1078. doi:10.3389/fpsyg.2020.53972 6 . URL
https://www.frontiersin.org/articles/10.3389/fpsyg.2020.539726.
Sergio Rubin. Future climates: Markov blankets and active i nference in the biosphere. Journal of The Royal Society
Interface, 17:20200503, 11 2020. doi:10.1098/rsif.2020.0503 .
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using Monte-
Carlo methods. arXiv:2006.04176 [cs, q-bio, stat] , June 2020.
T akazumi Matsumoto, W ataru Ohata, Fabien CY Benureau, and J un T ani. Goal-directed planning and goal under-
standing by extended active inference: Evaluation through simulated and physical robot experiments. Entropy, 24
(4):469, 2022.
Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinf orcement learning or active inference? PLOS ONE , 4(7):
1–13, 07 2009. doi:10.1371/journal.pone.0006421 . URL https://doi.org/10.1371/journal.pone.0006421.
Karl Friston. A free energy principle for biological system s. Entropy (Basel, Switzerland) , 14:2100–2121, 11 2012.
doi:10.3390/e14112100 .
Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystiﬁed and compared.
Neural Computation , 33(3):674–712, January 2021a. ISSN 0899-7667. doi:10.11 62/neco_a_01357 . URL
https://doi.org/10.1162/neco_a_01357.
Pietro Mazzaglia, Tim V erbelen, Ozan Çatal, and Bart Dhoedt . The free energy principle for perception and action: A
deep learning perspective. Entropy, 24(2):301, 2022.
Beren Millidge, Alexander Tschantz, Anil K. Seth, and Chris topher L. Buckley. On the relationship between active
inference and control as inference. In Tim V erbelen, Pablo L anillos, Christopher L. Buckley, and Cedric De Boom,
editors, Active Inference , pages 3–11, Cham, 2020. Springer International Publishin g. ISBN 978-3-030-64919-7.
doi:https://link.springer.com/chapter/10.1007/978-3-030-64919-7_1 .
Mahault Albarracin, Inês Hipólito, Safae Essaﬁ Tremblay, J ason G. Fox, Gabriel René, Karl Friston, and Maxwell
J. D. Ramstead. Designing explainable artiﬁcial intellige nce with active inference: A framework for transparent
introspection and decision-making, 2023.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hes p, and Thomas Parr. Sophisticated inference.
Neural Computation , 33(3):713–763, February 2021. ISSN 0899-7667. doi:10.11 62/neco_a_01351 . URL
https://doi.org/10.1162/neco_a_01351.
Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological
Cybernetics, 112(4):323–343, 2018. ISSN 1432-0770. doi:10.1007/s004 22-018-0753-2 . URL
https://doi.org/10.1007/s00422-018-0753-2 .
Franz Kuchling, Karl Friston, Georgi Georgiev, and Michael Levin. Morphogenesis as bayesian infer-
ence: A variational approach to pattern formation and contr ol in complex biological systems. Physics of
Life Reviews , 33:88–108, 2020. ISSN 1571-0645. doi:https://doi.org/1 0.1016/j.plrev .2019.06.001 . URL
https://www.sciencedirect.com/science/article/pii/S1571064519300909.
Alexander Tschantz, Anil K. Seth, and Christopher L. Buckle y. Learning action-oriented models through ac-
tive inference. PLOS Computational Biology , 16(4):1–30, 04 2020. doi:10.1371/journal.pcbi.1007805 . URL
https://doi.org/10.1371/journal.pcbi.1007805.
Thomas Parr and Karl J. Friston. The discrete and continuous brain: From decisions to movement-and back again.
Neural computation , 30(29894658):2319–2347, September 2018. ISSN 0899-7667 . doi:10.1162/neco_a_01102 .
URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6115199/.
19
On efﬁcient computation in active inference A P R E P R I N T
William S. Lovejoy. A survey of algorithmic methods for part ially observed markov decision processes. An-
nals of Operations Research , 28(1):47–65, 1991. ISSN 1572-9338. doi:10.1007/BF02055 574 . URL
https://doi.org/10.1007/BF02055574.
Guy Shani, Joelle Pineau, and Robert Kaplow . A survey of poin t-based pomdp solvers. Autonomous Agents
and Multi-Agent Systems , 27(1):1–51, 2013. ISSN 1573-7454. doi:10.1007/s10458-0 12-9200-2 . URL
https://doi.org/10.1007/s10458-012-9200-2 .
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Ca ssandra. Planning and
acting in partially observable stochastic domains. Artiﬁcial Intelligence , 101(1):99–
134, 1998. ISSN 0004-3702. doi:https://doi.org/10.1016/ S0004-3702(98)00023-X . URL
https://www.sciencedirect.com/science/article/pii/S000437029800023X.
Jelle Bruineberg, Erik Rietveld, Thomas Parr, Leendert van Maanen, and Karl J Friston. Free-energy minimization in
joint agent-environment systems: A niche construction per spective. Journal of theoretical biology , 455:161–178,
2018.
Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios F ountas, and Karl Friston. Exploration and preference
satisfaction trade-off in reward-free learning. In ICML 2021 W orkshop on Unsupervised Reinforcement Learning ,
2021b.
Karl Friston. A free energy principle for a particular physi cs, 2019.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. V ariati onal Inference: A Review for Statisticians. Jour-
nal of the American Statistical Association , 112(518):859–877, April 2017. ISSN 0162-1459, 1537-274X .
doi:10.1080/01621459.2017.1285773 .
Noor Sajid, Francesco Faccio, Lancelot Da Costa, Thomas Par r, Jürgen Schmidhuber, and Karl Friston. Bayesian
brains and the rényi divergence. Neural Computation , 34(4):829–855, 2022a.
Nicholas Rhinehart, Jenny W ang, Glen Berseth, John Co-Reye s, Danijar Hafner, Chelsea Finn, and Sergey Levine.
Information is power: intrinsic control via information ca pture. Advances in Neural Information Processing Systems ,
34:1074745–10758, 2021.
Glen Berseth, Daniel Geng, Coline Devin, Chelsea Finn, Dine sh Jayaraman, and Sergey Levine. Smirl: Surprise
minimizing rl in dynamic environments. arXiv preprint arXiv:1912.05510 , 2019.
Bert de Vries and Karl J. Friston. A factor graph description of deep temporal active inference. Fron-
tiers in Computational Neuroscience , 11:95, 2017. ISSN 1662-5188. doi:10.3389/fncom.2017.00 095 . URL
https://www.frontiersin.org/article/10.3389/fncom.2017.00095.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphica l brain: Belief propagation and ac-
tive inference. Network Neuroscience , 1(4):381–414, 2017. doi:10.1162/NETN_a_00018 . URL
https://doi.org/10.1162/NETN_a_00018.
Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston . Active inference, bayesian optimal design, and
expected utility. The Drive for Knowledge: The Science of Human Information Se eking, page 124, 2022b.
Philipp Schwartenbeck, Johannes Passecker, T obias U Hause r, Thomas HB FitzGerald, Martin Kronbichler, and Karl J
Friston. Computational mechanisms of curiosity and goal-d irected exploration. Elife, 8:e41703, 2019.
Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cy-
bernetics, 113(5):495–513, 2019. ISSN 1432-0770. doi:10.1007/s004 22-019-00805-w . URL
https://doi.org/10.1007/s00422-019-00805-w .
Anthony Triche, Anthony S. Maida, and Ashok Kumar. Explorat ion in neo-hebbian reinforcement learning:
Computational approaches to the exploration–exploitatio n balance with bio-inspired neural networks. Neu-
ral Networks , 151:16–33, 2022. ISSN 0893-6080. doi:https://doi.org/1 0.1016/j.neunet.2022.03.021 . URL
https://www.sciencedirect.com/science/article/pii/S0893608022000995.
Théophile Champion, Lancelot Da Costa, Howard Bowman, and M arek Grze ´s. Branching Time Active Inference: The
theory and its generality. arXiv:2111.11107 [cs] , November 2021a.
Théophile Champion, Howard Bowman, and Marek Grze ´s. Branching Time Active Inference: Empirical study and
complexity class analysis. arXiv:2111.11276 [cs] , November 2021b.
O. Çatal, T . V erbelen, J. Nauta, C. D. Boom, and B. Dhoedt. Lea rning perception and planning with deep active
inference. In ICASSP 2020 - 2020 IEEE International Conference on Acousti cs, Speech and Signal Processing
(ICASSP), pages 3952–3956, 2020. doi:10.1109/ICASSP40776.2020.9 054364 .
20
On efﬁcient computation in active inference A P R E P R I N T
Lancelot Da Costa, Thomas Parr, Biswa Sengupta, and Karl Fri ston. Neural dynamics under active inference: Plau-
sibility and efﬁciency of information processing. Entropy, 23(4), 2021. ISSN 1099-4300. doi:10.3390/e23040454 .
URL https://www.mdpi.com/1099-4300/23/4/454.
Noor Sajid, Panagiotis Tigas, Zafeirios Fountas, Qinghai G uo, Alexey Zakharov, and Lancelot Da Costa. Modelling
non-reinforced preferences using selective attention. arXiv preprint arXiv:2207.13699 , 2022c.
Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, and Adeel Razi . Active inference for stochastic control. In Machine
Learning and Principles and Practice of Knowledge Discover y in Databases , pages 669–680, Cham, 2021. Springer
International Publishing. ISBN 978-3-030-93736-2. doi:h ttps://doi.org/10.1007/978-3-030-93736-2_47 .
Di Zhou, Min Sheng, Jie Luo, Runzi Liu, Jiandong Li, and Zhu Ha n. Collaborative data scheduling with joint forward
and backward induction in small satellite networks. IEEE transactions on communications , 67(5):3443–3456, 2019.
21
On efﬁcient computation in active inference A P R E P R I N T
A Derivation of optimal state-belief
W e want to differentiate the following w .r.t. Q(s):
F =
∑
s
Q(s)[ logQ(s) −logP (o|s) −logP (s) ] . (A.1)
First, note that the derivative of the logarithm function is 1
x . Second, observe that the derivative of Q(s) with respect
to Q(s) is 1. With these two pieces in mind, we can differentiate F :
Let’s deﬁne f(s) = log Q(s) −log P (o|s) −log P (s), then:
dF
dQ(s) =
∑
s
[ d f
dQ(s) ·Q(s) + f(s) ·dQ(s)
dQ(s) ] (A.2)
The derivative of f(s) with respect to Q(s) can be computed as:
d f
dQ(s) = 1
Q(s) −0 −0 = 1
Q(s) (A.3)
This leads to:
dF
dQ(s) =
∑
s
[ Q(s) ·
( 1
Q(s)
)
+ f(s) ] (A.4)
=
∑
s
1 + log Q(s) −log P (o|s) −log P (s) (A.5)
So, the derivative of F with respect to Q(s) is:
dF
dQ(s) =
∑
s
1 + log Q(s) −log P (o|s) −log P (s) (A.6)
The goal is to minimize the free energy F with respect to the distribution Q(s). T o ﬁnd the minimum, we can set the
derivative of F with respect to Q(s) to zero. From the previous derivation, we know that:
dF
dQ(s) =
∑
s
1 + log Q(s) −log P (o|s) −log P (s) (A.7)
Setting this equal to zero gives:
1 + log Q(s) −log P (o|s) −log P (s) = 0 (A.8)
log Q(s) = log P (o|s) + log P (s) −1 (A.9)
However, note that the log function is typically normalized such that the sum of the probabilities in Q(s) equals 1
(since Q(s) is a probability distribution), so we can safely ignore the −1 term:
log Q(s) = log P (o|s) + log P (s) (A.10)
The optimal distribution Q∗ (s) that minimizes the free energy F is thus:
log Q∗ (s) = log P (o|s) + log P (s) . (A.11)
22
On efﬁcient computation in active inference A P R E P R I N T
B Optimising learning parameter for AIF (T=1) agent
Figure B.1: A sample graph of manual optimisation for e for Z-learning in the AIF(T=1) algorithm. A: For a stochasti c
grid with 100 states, B: For a stochastic grid with 400 states . W e observe that the agent’s performance is not heavily
dependent on the value of e that controls the learning parameter ηt.
23