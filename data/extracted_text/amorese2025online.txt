Preprint submitted to the IEEE Transactions on Robotics journal.
1
Online Pareto-Optimal Decision-Making for
Complex Tasks using Active Inference
Peter Amorese‚Ä†, Shohei Wakayama‚Ä†, Nisar Ahmed, and Morteza Lahijanian
Abstract‚ÄîWhen a robot autonomously performs a complex
task, it frequently must balance competing objectives while
maintaining safety. This becomes more difficult in uncertain
environments with stochastic outcomes. Enhancing transparency
in the robot‚Äôs behavior and aligning with user preferences are
also crucial. This paper introduces a novel framework for multi-
objective reinforcement learning that ensures safe task execution,
optimizes trade-offs between objectives, and adheres to user
preferences. The framework has two main layers: a multi-
objective task planner and a high-level selector. The planning
layer generates a set of optimal trade-off plans that guarantee
satisfaction of a temporal logic task. The selector uses active
inference to decide which generated plan best complies with
user preferences and aids learning. Operating iteratively, the
framework updates a parameterized learning model based on col-
lected data. Case studies and benchmarks on both manipulation
and mobile robots show that our framework outperforms other
methods and (i) learns multiple optimal trade-offs, (ii) adheres to
a user preference, and (iii) allows the user to adjust the balance
between (i) and (ii).
Index Terms‚ÄîMulti-Objective Decision Making, Active Infer-
ence, Formal Synthesis
I. I NTRODUCTION
T
HE demand for robots to autonomously perform haz-
ardous and repetitive tasks is steadily increasing [1, 2].
Such tasks often involve multiple, possibly competing, quanti-
tative objectives, e.g., time and energy, requiring optimization
for trade-offs between the objectives, known asPareto optimal.
In unknown environments, like the Martian surface or civilian
households, however, such quantities are often stochastic and
unknown a priori . Fortunately, they are measurable and can
be learned online. Then, the robot should make decisions to
learn all optimal trade-offs, or focus on learning a single user-
preferred trade-off. Although highly desirable, accomplishing
both simultaneously is challenging since the trade-off selection
must balance exploring unknown trade-offs with exploiting
preferred trade-offs. This paper focuses on this challenge
and aims to develop a framework that simultaneously (i)
learns multiple optimal trade-offs, (ii) embeds an intuitive user
preference over a desired trade-off, and (iii) allows the user to
adjust the balance between (i) and (ii).
Example 1. Consider the robotic dishwashing scenario
in Figure 1. The robotic manipulator is equipped with a
sampling-based motion planner (e.g., RRT and PRM [3, 4])
that solves A-to-B planning queries. The robot must repeti-
tively complete a complex task of inserting the dishes into
‚Ä†Authors contributed equally to this work
Authors are with the University of Colorado Boulder.
Risk
Time
: Pareto points
: User preference
Put jar on the floor 
and pitcher on the rack
Dishwasher Towel
Drying rack
Floor
Fig. 1: Motivating robotic dishwashing scenario: a robotic
manipulator needs to compare execution time of a task and
inherent risks of dropping fragile dishes (the jar is more fragile
than the blue pitcher). The user‚Äôs preferred trade-off between
time and risk is incorporated for transparent behaviors.
the dishwasher, then properly drying them by either placing
directly up on the rack, or placing on the towel and moving
them to the floor. In such a scenario, the robot should be both
efficient (minimize the time of execution) and risk-aware when
carrying fragile dishes high above the ground. However, due
to the nature of sampling-based methods, the characteristics of
actions (time and path-height) are unknown a priori, and hence
the robot needs to construct accurate models of these quantities
during deployment. The robot can either quickly place dishes
up on a drying rack (low-time, high-risk for fragile objects),
or manually dry dishes on the ground (high-time, low-risk).
Considering a human user may prefer one trade-off over the
other, the robot must both learn the quantitative models, and
decide amongst optimal trade-off actions while adhering to the
user‚Äôs preference. Such decision making problems involving
competing objectives can be formulated as multi-objective
reinforcement learning (MORL) [5, 6, 7, 8].
Embedding the satisfaction of a task into the multi-objective
reward function often aims to capture both qualitative prop-
erties of the task (i.e. whether or not the robot completed
the task), as well as quantitative properties (i.e. how well the
robot completed the task) [9]. However, properly representing
qualitative task completion through a reward function is a
challenging problem, often requiring the use of methods such
as inverse reinforcement learning (IRL) [10]. In uncertain
environments, it is not always possible to obtain large amounts
of human teaching data. Furthermore, these learning-based
methods often fail to provide safety guarantees on the quali-
tative behavior of the robot. Nevertheless, in scenarios where
arXiv:2406.11984v1  [cs.RO]  17 Jun 2024
2
qualitative aspects of the robotic model and task are known
beforehand, e.g., manipulation problems, formal methods [11]
provide alternative approaches to both task representation and
plan synthesis using temporal logic specifications, such as
Linear Temporal Logic (LTL) [12] and LTL over finite traces
(LTLf) [13]. These planning methods can solve the qualitative
portion of the learning problem and provide guarantees on the
completion of tasks and safety while avoiding the need for
IRL. Formal synthesis approaches have also been used to solve
multi-objective quantitative planning problems [14]; however,
those approaches assume a perfectly accurate quantitative
model, which are often unavailable.
Unlike single-objective RL, MORL agents must select
among potentially many optimal trade-offs. The numerical val-
uations of these optimal trade-offs are referred to as the Pareto
front. The Multi-Objective Multi-Armed Bandit (MOMAB)
community emphasizes learning all optimal trade-off actions,
yielding valuable insight into the trade-off analysis of the
learned results [15, 16, 17, 18]. Alternatively, one can use
scalarization to convert the multi-objective problem into a
single objective problem based on a preference trade-offs
[5, 6, 7, 8]. Further description of these methods can be found
in Sec. V. To the best of our knowledge, no MORL selec-
tion methods simultaneously propose mathematically sound
adherence to a preferred trade-off while learning and exploring
portions of the true Pareto front.
In light of these gaps, this study proposes a novel MORL
framework centered around viewing optimal trade-off selec-
tion as its own high-level decision making under uncertainty
problem. Specifically, we employ active inference (AIF), re-
cently explored as a sound approach for sequential decision-
making under uncertainty [19, 20, 21, 22] that minimizes an
information-theoretic quantity known as surprise. We exploit
the decision making power of AIF to adhere to a user‚Äôs ideal
trade-off while exploring and learning localized portions of
the whole Pareto front. This, however, introduces intractable
computational complexity in optimizing for surprise for finite-
horizon tasks. Our framework integrates multi-objective plan-
ning for a complex LTLf specification with high-levelselection
of an optimal trade-off by using a Bayesian approach to
learning the quantitative model. We extend AIF to reasoning
over finite horizon planning via upper-bounding the surprise
with expected free energy (EFE). We derive computationally
tractable techniques for optimizing EFE for LTLf tasks. Using
our framework, we illustrate the performance trade-offs be-
tween biasing towards a user‚Äôs preferred Pareto point versus
learning the entire Pareto front. Additionally, we showcase the
utility of our approach using robotic case studies, including a
hardware demonstration of the scenario in Example 1.
The major contributions of this work are three-fold:
‚Ä¢ A novel MORL framework that combines formally guar-
anteed task plan synthesis with efficient learning of
Pareto-optimal behavior,
‚Ä¢ Derivation of a tractable approximation of a free energy
formulation for finite horizon planning over an uncertain
quantitative model,
‚Ä¢ Benchmarking the efficacy of the proposed AIF-based
selection strategy against the state-of-the-art methods
[6, 8, 18], along with Mars surface exploration simulation
and a robotic hardware diswashing experiment.
A detailed discussion on related work along with an
overview of AIF are provided in Sec. V.
II. P RELIMINARIES AND PROBLEM FORMULATION
This paper studies a framework that enables a robot to (i)
repetitively complete a given high-level complex task, and (ii)
learn the most efficient and user-aligned means of completing
the task. These two goals separate nicely into qualitative
behavior, i.e., whether or not the robot completes the task, and
quantitative behavior, i.e., how well the robot completes the
task. We restrict our attention to problems where qualitative
aspects of the model (i.e. robotic states, state transitions, task
related observations, etc.) are known, but quantitative aspects
(i.e. costs of executing robotic actions) are unknown and must
be learned. Below, we first formally define a robot model and
LTLf task specifications. We then connect these definitions
with cost uncertainty and user preferences to be able to fully
formulate the problem.
A. Robot Model
We consider a robotic system modeled as a deterministic
transition system (DTS), an abstraction used in many formal
approaches to robotic systems, including both mobile robots
and robotic manipulators [11, 23, 24, 25, 26, 27, 28]. To cap-
ture uncertainty in quantitative values, we augment classical
DTS with a stochastic cost function as defined below.
Definition 1 (DTS-sc). A labeled Deterministic Transition
System with a stochastic cost function (DTS-sc) is a tuple
T = (S, A, Œ¥T , C, AP, L), where
‚Ä¢ S is a finite set of states,
‚Ä¢ A is a finite set of actions,
‚Ä¢ Œ¥T : S √ó A 7‚Üí S is a deterministic transition function,
‚Ä¢ C : S √ó A 7‚Üí D(RN ) is a stochastic cost function
that maps each state-action pair to an N-dimensional
probability distribution, where N ‚àà N and D(RN ) is the
set of all probability distributions over RN ,
‚Ä¢ AP is a set of atomic propositions related to the robot
tasks, and
‚Ä¢ L : S 7‚Üí 2AP is a labeling function that assigns to each
state s ‚àà S the subset of propositions in AP that are
true in s.
A robot plan œÄ ‚àà A‚àó is a finite sequence of ac-
tions œÄ = a0a1 . . . am that induces a trajectory œÑ(s0, œÄ) =
s0s1 . . . smsm+1 from a current state s0 ‚àà S, where sk ‚àà
S and, for all 0 ‚â§ k ‚â§ m + 1, sk+1 = Œ¥T (sk, ak).
The observation of œÑ(s0, œÄ), denoted L(s0, œÄ), is a trace
L(s0, œÄ) = o0o1 . . . om+1 where ok = L(sk).
Example 2. Consider a simplified DTS-sc model in Fig. 2 of
the dishwashing scenario described in Fig. 1 with five states
capturing the locations of each dish ( Jf : jar on floor, Pr:
pitcher on rack, etc.), and two actions load and unload.
Each state observes either {dry} or {wash}. Each action from
a state takes time and incurs some risk when transporting the
3
Jd, Pd
{wash}
Jf, Pf
{dry}
Jr, Pf
{dry}
Jr, Pr
{dry}
Jf, Pr
{dry}
load
unload_1
load
unload_2
load
unload_3
load
unload_4
p(c|s, a;Œ∏s,a)
Fig. 2: The DTS-sc T described in Example 2 is shown, with
a plan (red) that satisfies the task described in Example 3.
jar, modeled with a unique multivariate cost distribution for
each state-action pair.
When robot takes action a ‚àà A at state s ‚àà S, it endures
N distinct types of costs cs,a = (c1, . . . , cN ) ‚àà RN , e.g.,
energy expenditure and time of execution. This cost vector
can generally be stochastic, i.e., the cost value of executing a
at s can differ at different instances due to, e.g., changes in the
environment or use of randomized algorithms (sampling-based
planners). Hence, cs,a is a random variable whose distribution
is given by C(s, a), i.e., cs,a ‚àº C(s, a). With an abuse of
notation, we use c as the cost random variable for all state-
action pairs, and hence (c | s, a) = cs,a.
We treat C(s, a) as a conditional distribution of a proba-
bilistic transition-cost generative model, parameterized by the
set of parameters Œò, i.e.,
C(s, a) = p(c | s, a; Œò).
For ease of presentation, we denote (s, a) specific parameters
by Œ∏s,a ‚äÜ Œò such that Œò = ‚à™s‚ààS,a‚ààAŒ∏s,a.
We assume robot model T is a Markov process. That is, c
for each state-action pair is independent, i.e., p(c|s, a; Œ∏s,a) is
independent from p(c|s‚Ä≤, a‚Ä≤; Œ∏s‚Ä≤,a‚Ä≤ ) for all (s, a) Ã∏= (s‚Ä≤, a‚Ä≤) ‚àà
S√óA. Further, we assume each p(c|s, a; Œ∏s,a) is a multivariate
normal (MVN) distribution N(¬µs,a, Œ£s,a) paramerized by a
non-negative mean ¬µs,a ‚àà RN
‚â•0 and covariance Œ£s,a ‚àà RN√óN ,
i.e., Œ∏s,a = {¬µs,a, Œ£s,a}. Note that this assumption allows
for correlation between types of costs. For instance, if the
execution of a robotic action takes a long time, it is likely that
it also uses more energy.
In this work, we consider the realistic case where Œò are un-
known, i.e., both ¬µs,a and Œ£s,a are unknown for every (s, a).
One of the goals of this work is to learn these parameters from
data collected on cs,a during execution.
Given a plan œÄ and its induced trajectory œÑ , the cumulative
cost of executing œÄ from state s0, denoted Cs0,œÄ, is the sum
of the state-action costs along œÑ, i.e., Cs0,œÄ = Pm
k=0 csk,ak .
Since each csk,ak is a random variable, Cs0,œÄ is also a random
variable, and its distribution is given by the convolution of the
csk,ak distributions, i.e.,
p(C|s0,œÄ; Œ∏s0,œÄ)= p(c|s0, a0; Œò)‚àó. . .‚àóp(c|sm, am; Œò), (1)
where Œ∏s0,œÄ is constructed by Œò. Recall that p(c|sk, ak; Œò)
is assumed to be a MVN; p(C | s0, œÄ; Œ∏s0,œÄ) is also a MVN.
Since true Œò are unknown beforehand, we learn the parameters
from experience. We denote the learned parameters by ÀúŒò.
B. Robotic Task
The robot is given a complex task that can be completed in
finite time. To specify such tasks in a formal manner, we use
Linear Temporal Logic over Finite Traces (LTLf) [13], which
combine propositional logic with temporal operators.
Definition 2 (LTLf Syntax). A Linear Temporal Logic over
Finite Traces (LTLf) formula over AP is recursively defined
as
œï := true | o | ¬¨œï | œï ‚àß œï | Xœï | œï Uœï,
where o ‚àà AP, ¬¨ (negation) and ‚àß (conjunction) are Boolean
operators, and X (next) and U (until) are temporal operators.
The commonly-used temporal operators ‚Äúeventually‚Äù ( F) and
‚Äúglobally‚Äù ( G) can be defined as F œï‚â° true Uœïand Gœï ‚â°
¬¨F¬¨œï, respectively.
The semantics of LTLf are defined over finite traces [13].
That is, an LTLf formula œï can be satisfied with a finite trace
w ‚àà (2AP )‚àó [13], denoted w |= œï. We define the language of
œï to be the set of finite traces that satisfy œï for the first time,
i.e., let |w| be the length of w and w[k] for 1 ‚â§ k ‚â§ |w| be
the prefix of w with length k; the language of œï is defined as
Lœï = {w ‚àà (2AP )‚àó | w |= œï ‚àß w[k] Ã∏|= œï ‚àÄk <|w|}.
We say plan œÄ ‚àà A‚àó executed from s satisfies œï if the resulting
trace L(œÄ, s) ‚àà Lœï.
Example 3. Following Example 2, we specify the task œï =
F(wash ‚àß F(dry)) stating ‚Äúthe robot should first wash the
dishes, and then dry the dishes. ‚Äù
We want the robot to execute task œï repeatedly. Specifically,
we are interested in synthesizing plans that episodically repeat
œï. That is, once a plan is complete ( œï is satisfied), a new plan
has to be synthesized from the current state to satisfy œï again.
We assume that such a repetition is always physically possible
for the robot, i.e., the robot always ends in a state from which
there exists a plan that satisfies œï. For instance, in the above
example, after drying dishes, the robot always ends in a state
from which it can start washing a new pile of dirty dishes.
Thus, we focus on the set of satisfying plans from every state,
and refer to it as the œï-satisfying plan set.
Definition 3 (œï-Satisfying Plan Set) . Given DTS-sc robot
model T, state s ‚àà S, and LTLf task formula œï, œï-Satisfying
Plan Set Œ†(s, œï) is defined as
Œ†(s, œï) = {œÄ ‚àà A‚àó | L(œÄ, s) ‚àà Lœï}.
C. Optimization Objectives
At each planning (decision) instance from state s, we aim to
select a plan œÄ‚ãÜ ‚àà Œ†(s, œï) that, in addition to completing task
œï, optimizes for the expected cumulative cost E[C | s, œÄ; Œò] .
4
Since Cs,œÄ is N-dimensional, it is in fact an N-objective
optimization problem. Moreover, the cost objectives may be
competing, i.e., by optimizing one objective, another becomes
sub-optimal. Hence, we want to optimize for the trade-offs
between the objectives, which is known as Pareto optimal and
best defined by the notion of vector dominance.
Definition 4 (Vector Dominance). Let C, C‚Ä≤ ‚àà RN be two
vectors and C(i) be the i-th element of C. Vector C dominates
C‚Ä≤, denoted by C ‚â∫ C‚Ä≤, iff C(i) ‚â§ C‚Ä≤(i) for every 1 ‚â§ i ‚â§ N
and there exists an i such that C(i) < C‚Ä≤(i).
Definition 5 (Pareto Optimal) . For state s ‚àà S and LTLf
formula œï, plan œÄ ‚àà Œ†(s, œï) is called Pareto optimal if
‚àÑœÄ‚Ä≤ ‚àà Œ†(s, œï) s.t. E[C | s, œÄ‚Ä≤; Œò] ‚â∫ E[C | s, œÄ; Œò] .
The set of all Pareto optimal plans from s is denoted by
Œ†‚ãÜ(s, œï) ‚äÜ Œ†(s, œï). Then, for each œÄ‚ãÜ ‚àà Œ†‚ãÜ(s, œï), E[C |
s, œÄ‚ãÜ; Œò] is called a Pareto point , and the set of all Pareto
points is called the Pareto front, denoted ‚Ñ¶(s, œï).
At every planning instance (episode), we desire to select a
Pareto optimal plan œÄ‚ãÜ ‚àà Œ†‚àó(s, œï) which ensures that the
expected cost is not dominated by another plan. However,
since Œò are not available, it is impossible to compute for œÄ‚ãÜ.
Instead, we can use ÀúŒò to compute a learned set of œï-satisfying
Pareto optimal plans denoted ÀúŒ†‚ãÜ(s, œï) ‚äÜ Œ†(s, œï). Similarly,
let Àú‚Ñ¶(s, œï) denote the set of expected cost vectors of each
œÄ ‚àà ÀúŒ†‚ãÜ(s, œï).
D. User Preference
There are possibly many Pareto optimal plans that perform
œï. We are interested in picking a plan œÄ‚ãÜ
pr that achieves the
user‚Äôs most-preferred Pareto point in every instance. Given that
our goal is autonomous operation, we require this preference
to be provided as an input before deployment. This poses
a challenge because the Pareto front is unknown a priori ,
and the user cannot know the exact Pareto point they prefer.
Instead, as suggested in recent literature [22, 29], the user can
provide a probability density function (pdf) that serves as an
expressive way of capturing preferred observed outcomes. We
extend this notion to the multi-objective domain, and leverage
the expressive pdf preference as a tie-breaker between optimal
trade-off candidates.
We specifically focus on user preference as a normal dis-
tribution ppr(C) = N(¬µpr, Œ£pr) over the objectives since it
can be fully defined by only specifying a mean ¬µpr ‚àà RN and
covariance Œ£pr ‚àà RN√óN , respectively. Thus, by providing
¬µpr and Œ£pr, the user can not only express a desired trade-off
region in the objective space, but also specify the willingness
to explore alternative trade-offs (Pareto points) that allow the
robot to learn more quickly, offering means to address the
exploration-exploitation trade-off problem.
Example 4. Suppose the dishwashing robot described in Fig. 1
is being deployed in a restaurant. If the restaurant is going to
be busy, the user may define a preference distribution with
mean in a low-time high-risk part of the objective space,
whereas if the restaurant will be slow, the user may opt for a
high-time, low-risk alternative, both of which can be informed
by past observations of how humans wash the dishes. If the
user is interested in learning other optimal trade-offs, or is
uncertain how accurately they have estimated the mean, they
can expand and orient the covariance around regions they
would like the robot to explore information rich opportunities.
To meet user‚Äôs preference, we seek a Pareto point that is not
‚Äúsuprising.‚Äù The information theoretic definition of surprise is
the negative log of model evidence [30]. Similarly, we define
the surprise of an observation obtained by following a plan œÄ
as:
Surprise(œÄ)= ‚àílog p(C | œÄ). (2)
This quantity can be derived by marginalizing the following
generative model:
(2) = ‚àílog
Z
s
Z
ÀúŒ∏s,œÄ
p(s, ÀúŒ∏s,œÄ, C | œÄ)dÀúŒ∏s,œÄds,
= ‚àílog
Z
s
Z
ÀúŒ∏s,œÄ
p(s, ÀúŒ∏s,œÄ | C, œÄ)p(C | œÄ)dÀúŒ∏s,œÄds. (3)
In active inference (AIF), one can inject a bias towards
seeing certain outcomes by substituting the predictive outcome
distribution p(C | œÄ) with the prior preference ppr(C) (a.k.a.
‚Äúevolutionary prior‚Äù) that is independent of any plan [21, 31].
We refer to the biased surprise as surprise w.r.t. ppr:
Surprise(œÄ, ppr)= ‚àílog
Z
s
Z
ÀúŒ∏s,œÄ
p(s,ÀúŒ∏s,œÄ|C,œÄ)ppr(C)dÀúŒ∏s,œÄds.
(4)
In essence, under AIF, an agent acts to see desired observa-
tions. An overview of AIF is provided in Sec. V-B.
E. Problem Statement
The problem we consider is as follows.
Problem 1. Consider a DTS-sc robot model T with unknown
cost distributions, an LTLf task specification œï that the robot
is to repeatedly complete, and a user preference distribution
ppr(C) over N cost objectives. Let K ‚àà N denote the number
of times (instances) the robot achieves task œï, and sK ‚àà S be
the start state of the robot in the K-th episode. Then, for each
K, synthesize a œï-satisfying plan œÄK ‚àà Œ†(sK, œï) such that
(i) œÄK minimizes Surprise(œÄ, ppr) in (4),
(ii) as K ‚Üí ‚àû, œÄK becomes Pareto optimal, i.e., œÄK ‚àà
Œ†‚ãÜ(sK, œï), and
(iii) as K ‚Üí ‚àû, the resulting Pareto front Àú‚Ñ¶(sK, œï) con-
verges to the true Pareto front ‚Ñ¶(sK, œï).
Note that, in Problem 1, satisfaction of œï (qualitative objec-
tive) at each instance is a hard constraint, and the optimization
(quantitative) objectives in (i)-(iii) are soft constraints. Hence,
by incorporating safety requirements in œï, safety satisfaction
is guaranteed throughout the process. Furthermore, from the
perspective of multi-objective decision making, Problem 1 is
particularly challenging since adhering to a user‚Äôs preference
and learning the Pareto front are arguably perpendicular ef-
forts. In our approach, we address this challenge by using
5
Planning Selection Execution Update
Risk
Time
: Pareto points
: User preference
Put jar on the floor and 
pitcher on the rack
s0, a0
s32, a32
s0, a0
s32, a32
p(Œ∏|c) ‚àù p(c|Œ∏)„Éªp(Œ∏)
: Trajectory
œÜ = F(load ‚àß F(dry_rack ‚à® dry_floor))
Fig. 3: Proposed multi-objective safe reinforcement learning architecture: As the four phases involving 1) Planning, 2) Selection,
3) Execution, and 4) Update are repeatedly executed, the robot gradually learns the state-action cost parameters and selects
the user preferred optimal task plan.
free energy minimization to naturally balance exploitation and
exploration over the Pareto front.
III. A PPROACH
Our approach to Problem 1 is an iterative MORL framework
with four phases: 1) planning Pareto optimal plans, 2) selection
of the preferred Pareto optimal plan, 3) execution of the
selected plan, and 4) update to the executed state-action cost
parameters, as illustrated in Fig. 3. Planning, selection, and
update all are dependent on the learning status of the robot.
Hence, we start by formalizing our approach to modeling ÀúŒò.
A. Learning the Transition Cost Model
Recall that the cost cs,a of each (s, a) is distributed as
p(c | s, a; Œ∏s,a) = N(¬µs,a, Œ£s,a). However, Œ∏s,a are unknown
and must be learned using experience. We take a Bayesian
approach to learning Œ∏s,a by modeling with random variable
ÀúŒ∏s,a. Specifically, we assume that the robot collects a realized
sample ÀÜc drawn from p(c|s, a; Œ∏s,a) each time a is executed
from s. Let Ds,a = {ÀÜc1, . . .ÀÜcl} be the set of l realizations of
cs,a. We maintain a belief over ÀúŒ∏s,a using the Normal Inverse-
Wishart (NIW) distribution, i.e.,
p(ÀúŒ∏s,a)= NIW s,a(Œª0, Œ∫0, Œõ0, ŒΩ0).
NIW is chosen because of the property of conjugacy for esti-
mating unknown mean and covariance of a MVN distribution,
i.e., the posterior is also represented analytically as a NIW
[32]
p(ÀúŒ∏s,a |Ds,a)= NIW s,a(Œª, Œ∫,Œõ, ŒΩ).
Note that some works assume the covariance is known a
priori [33], limiting the fidelity of the model. NIW strikes a
balance between an over simplified parametric model (known
covariance) and a sample inefficient non-parametric model
[34]. We detail the the update procedure in Sec. III-D. If the
user has an informed guess about the mean and covariance
of a certain state-action distribution, they can set Œª0 and Œõ0
respectively, reducing the number of decision making instances
required to learn ÀúŒò.
B. Planning
At the start of each planning (decision-making) instance,
we employ a multi-objective task planner to compute the set
of approximate Pareto-optimal plans ÀúŒ†‚ãÜ(sK, œï) described in
Def. 5. To this end, we formulate a correct-by-construction
multi-objective shortest-path graph search problem, similar to
[14], which can be solved with the following procedure.
1) Product Graph Construction: An LTLf formula can be
translated into a deterministic finite automaton (DFA) [13], a
finite state-machine that precisely captures the language of œï.
Definition 6 (Deterministic Finite Automaton) . A DFA
constructed from LTLf formula œï is a tuple Qœï =
(Œì, Œ≥0, W, Œ¥Qœï, F) where Œì is a set of states, Œ≥0 ‚àà Œì is the
initial state, W = 2AP is the alphabet, Œ¥Qœï : Œì√óW 7‚Üí Œì is a
deterministic transition function, and F is a set of accepting
states.
A trace w = w0 . . . wm‚àí1 ‚àà (2AP )‚àó induces a finite run
œÑQœï = Œ≥0Œ≥1 . . . Œ≥m on Qœï, where Œ≥k+1 = Œ¥Qœï(Œ≥k, wk). Run
œÑQœï is called accepting if Œ≥m ‚àà F. If œÑQœï is accepting, then
trace w is accepted by Qœï and satisfies œï, i.e., w |= œï.
Using T to capture the physical capability of the robotic
system, along with Qœï to capture the temporal attributes of
œï, we can construct a product automaton that intersects the
restrictions of both T and Qœï.
Definition 7 (Product Automaton). Given a DTS-sc T, current
state sK ‚àà S, and DFA Qœï, a Product Automaton is a tuple
P = (P, œÅK, A, Œ¥P , v, FP ), where
‚Ä¢ P = S √ó Œì is a set of states,
‚Ä¢ œÅK = ( sK, Œ≥‚Ä≤
0), where Œ≥‚Ä≤
0 = Œ¥Qœï(Œ≥0, L(sK)), is the
starting state in the current instance,
‚Ä¢ A is the same action set in T,
‚Ä¢ Œ¥P : P √ó A 7‚Üí P is a transition function, where for
states œÅ = ( s, Œ≥) and œÅ‚Ä≤ = ( s‚Ä≤, Œ≥‚Ä≤) and action a ‚àà A,
transition œÅ‚Ä≤ = Œ¥P (œÅ, a) exists if s‚Ä≤ = Œ¥T (s, a) and Œ≥‚Ä≤ =
Œ¥Qœï(Œ≥, L(s‚Ä≤)),
‚Ä¢ v : P √ó A 7‚Üí RN
‚â•0, is a transition cost-vector function,
and
6
‚Ä¢ FP = S √ó Fis a set of accepting states.
Note that unlike stochastic cost function C in T, cost-vector
function v(œÅ, a) ‚àà RN
‚â•0 is not a distribution. We elaborate on
v further below.
Similar to T, a plan œÄ induces a run œÑP = œÅ0 . . . œÅm+1 on
P where œÅk+1 = Œ¥P (œÅk, ak). Run œÑP is accepting iff œÅm+1 ‚àà
FP . Therefore, by construction, the set of œï-satisfying plans
Œ†(sK, œï) is equal to the set of all plans that induce paths on
P from œÅK to a œÅ ‚àà FP .
2) Transition Cost-Vector Function: We aim to define cost-
vector function v and accordingly formulate a multi-objective
graph search problem on P that enables the computation of the
set of Pareto optimal œï-satisfying plans Œ†‚àó(sK, œï). Consider
current state œÅK = (sK, Œ≥0), plan œÄ ‚àà Œ†(sK, œï), and induced
path œÑ(œÅK, œÄ) = œÅK,0 . . . œÅK,m+1. Ideally, for every œÅ =
(s, Œ≥) ‚àà P and a ‚àà A, we want v((s, Œ≥), a) = E[c|s, a; Œò]
because the total cost of induced path œÑP (œÅK, œÄ) becomes
m+1X
i=0
v(œÅK,i, œÄi) =
m+1X
i=0
E[c|sK,i, œÄi; Œò] = E[C|sK, œÄ; Œ∏sK,œÄ].
Therefore, by assigning v(œÅ, a) = E[c|s, a; Œò], the multi-
objective Pareto front of the graph search problem on P
is exactly equal to ‚Ñ¶(sK, œï) (see Def. 5). Then, existing
algorithms such as a multi-objective variant of Dijkstra‚Äôs
algorithm or A‚àó [35] can be employed to compute ‚Ñ¶. Note that
these multi-objective graph search algorithms are exponential
in the worst case, however the average branching factor for
a N-dimensional vector belonging to a set of size G, i.e.
the set of all unique cost vector edge weights, is reduced to
O((log |G|)N‚àí1), with certain conditions on the ordering [35].
However, Œò is unknown. A naive approach simply uses
learned estimates E[ÀúŒò] for v instead, but that suffers from
biasing the solution towards taking transitions that have low
estimate cost, regardless of how accurate ÀúŒò is. To address this,
we interweave exploration into the graph search problem, as
detailed below.
3) Pareto-Regret: The efficacy of learning can be analyzed
with the notion of cumulative ‚ÄúPareto-regret‚Äù, quantifying how
sub-optimal each plan is with respect to the true Pareto front
‚Ñ¶(sK, œï). Formally, given œÄ with true mean cumulative cost
¬µsK,œÄ = E[C|sK, œÄ; Œò], the Pareto regret for a given instance
is defined as
r(sK, œÄ) =
min
œµ
{¬µsK,œÄ ‚àíœµ1 | ¬µ‚ãÜ Ã∏‚â∫ ¬µsK,œÄ ‚àíœµ1 ‚àÄ¬µ‚ãÜ ‚àà ‚Ñ¶(sK, œï)}, (5)
where 1 is a vector of ones with appropriate dimension. Note
that if œÄ ‚àà Œ†‚ãÜ(sK, œï), then r(sK, œÄ) = 0 . The cumulative
regret up to instance K is simply the summed regret for every
instance, i.e. R = PK
i=0 r(si, œÄi). To minimize cumulative re-
gret, we augment the transition cost-vector with an exploration
strategy.
4) Pareto Cost-LCB: We adapt the well established Pareto-
UCB1 [15, 36] strategy for computing an estimate Pareto
front that balances the current best estimate cost model (ex-
ploitation) with a bonus reduction in cost (exploration). The
learned mean transition-cost is calculated using the estimated
Jd, Pd
Jf, Pf
Jr, Pf Jr, Pr
Jf, Pr
œÄ1=load, unload_2
œÄ2
œÄ3
œÄ4
œÄ5
œÄ6
œÄ7
œÄ8
 œÄ9
œÄ10
p(C|s, œÄ;Œ∏s,œÄ)
Fig. 4: By treating plans œÄ as macro-actions, the selection
decision making problem becomes sequential. The example
plan highlighted in Fig. 2 is embedded into œÄ1, as shown.
parameters E[ÀúŒ∏s,a]. To maintain optimality guarantees of the
aforementioned graph search method, we rectify each element
of v to be non-negative. The cost-lower confidence bound
(LCB) cost vector v(œÅ, a) is computed element-wise as:
vi(œÅ, a)=max

0, E[c|s, a; E[ÀúŒ∏s,a]]i ‚àí Œ±
q
log(kg)/n(s, a)

where Œ± is a ‚Äúconfidence‚Äù hyperparameter, kg is the current
global time step across all decision instances, and n(s, a) is
the number of times action a has been executed from state s.
Note that many multi-objective multi-armed bandit con-
fidence bound approaches provide theoretical regret bounds
that are logarithmic with respect to the number of decision
instances. It is not straight forward to extend these regret
bounds to this framework due to the exponential nature of the
planning space Œ†(sK, œï) which has size O(|A||P|). Hence, we
leave theoretical regret bounds to future research.
Using the techniques described above, we can now compute
the approximate set of Pareto optimal plans ÀúŒ†‚ãÜ(sK, œï) while
accounting for exploration of the environment. The exploration
in planning aids in the effort in addressing goals (ii) and (iii)
of Problem 1, however, to uphold proper learning of the Pareto
front (iii), the selection must also explore among trade-offs.
C. Pareto Point Selection
For the selection problem, one can think of candidate
plans œÄ ‚àà ÀúŒ†‚ãÜ(sK, œï) computed by the planner as abstract
macro-actions, such that high-level decision making can be
done sequentially for each instance. Fig. 4 provides a visual
representation of the abstracted selection problem with respect
to the evolution of the robotic system over each instance.
As described in Sec. II-D, ideally, the agent should select an
action minimizing the surprise of outcomes. However, since it
is analytically intractable to marginalize the joint distribution
to derive the surprise, its upper bound, i.e., free energy, is
minimized. Yet, the outcomes C cannot be observed until a
plan œÄ is actually executed, thus, the agents end up minimizing
the so-called expected variational free energy (EFE) in active
inference based decision-making. In this Pareto point selection
problem, the EFE is described as follows (see Appendix A for
7
detailed derivation).
EFE(sK, œÄ‚ãÜ, ppr)= ‚àíEq(C|œÄ‚ãÜ)
h
log ppr(C)
i
‚àíEq(C|œÄ‚ãÜ)
h
DKL

q(sK|C, œÄ‚ãÜ)||q(sK|œÄ‚ãÜ)
i
‚àíEq(C|œÄ‚ãÜ)
h
DKL

q(ÀúŒ∏sK,œÄ‚ãÜ|C,sK,œÄ‚ãÜ)||q(ÀúŒ∏sK,œÄ‚ãÜ|sK,œÄ‚ãÜ)
i
,
(6)
where q(¬∑) represents a proposal distribution. EFE is a varia-
tional quantity [37], yielding freedom in the choice of proposal
distribution q(ÀúŒ∏sK,œÄ|sK,œÄ‚ãÜ). As q(ÀúŒ∏sK,œÄ|sK,œÄ‚ãÜ) more closely
resembles p(ÀúŒ∏sK,œÄ|C, sK,œÄ‚ãÜ), the upper bound on (expected)
surprise decreases. The first term of EFE in (6) represents
how much the predicted cost distribution q(C|œÄ‚ãÜ) aligns with
ppr(C) (i.e. exploitation), and the second and third terms
represent how much the uncertainties of sK and ÀúŒ∏sK,œÄ‚ãÜ can be
reduced by following œÄ‚ãÜ and measuring C (i.e. exploration).
Since the user does not know the true Pareto-optimal region
a priori , EFE cannot be used to scalarize any plan in Œ†,
as it may often be sub-optimal. Therefore, as described in
Sec. V-A, the preferred plan œÄpr =argminœÄ‚ààŒ† EFE(œÄ, ppr) is
not guaranteed to also be an element in Œ†‚ãÜ and vice versa.
Consequently, in order to ensure that the user‚Äôs preference
does not bias the system away from optimal behavior, this
preference is only expressed over Pareto-optimal candidates.
So, Œ†‚ãÜ(sK, œï) is scalarized via EFE in order to adhere to ppr
and reduce uncertainties in the belief over ÀúŒ∏sK,œÄ‚ãÜ. Since T
models deterministic transitions, the second term of (6) can
be ignored. Expanding the third term of (6) yields
EFE(sK, œÄ‚ãÜ, ppr) = ‚àíEq(C|œÄ‚ãÜ)
h
log ppr(C)
i
‚àí H[q(ÀúŒ∏sK,œÄ|sK,œÄ‚ãÜ)]+Eq(C|œÄ‚ãÜ)
h
H[q(ÀúŒ∏sK,œÄ‚ãÜ|C,sK,œÄ‚ãÜ)]
i
,
(7)
where H[¬∑] represents the entropy of a pdf. Then, the preferred
plan is
œÄ‚ãÜ
pr = argmin
œÄ‚ãÜ‚ààŒ†‚ãÜ(sK)
EFE(sK, œÄ‚ãÜ, ppr). (8)
Due to the Markov property of DTS-sc and (1), ¬µsK,œÄ‚ãÜ and
Œ£sK,œÄ‚ãÜ of the convolved distribution p(C|sK, œÄ‚ãÜ; Œ∏sK,œÄ‚ãÜ) are
parameterized by P
k ¬µsk,ak and P
k Œ£sk,ak of each cost dis-
tribution respectively. According to the estimate transition cost
model, ÀúŒ∏s,a = {Àú¬µs,a, ÀúŒ£s,a} are random variables themselves;
therefore,
p(ÀúŒ∏sK,œÄ‚ãÜ|sK, œÄ‚ãÜ) = p(ÀúŒ∏s0,a0 ) ‚àó . . .‚àó p(ÀúŒ∏sm,am). (9)
As described in Section III-A, each p(ÀúŒ∏s,a) is a unique
NIW, which makes (9) analytically intractable. To enable
computation for œÄ‚ãÜ
pr in (8), we leverage both the functional
freedom in choosing proposal distributions q(¬∑) as well as three
statistical approximations of (7): certainty equivalence of the
predicted observation distributions [38], central limit theorem
(CLT) [39], and Monte Carlo sampling [40].
1) Approximating the First Term of (7): Recall
p(ÀúŒ∏sK,œÄ‚ãÜ|sK, œÄ‚ãÜ) is the belief over parameters of the
predicted observation distribution q(C|œÄ‚ãÜ). To address the
intractability, we can extract the current estimate parameters
E[ÀúŒ∏sK,œÄ‚ãÜ]. This certainty-equivalence approximation makes
q(C|œÄ‚ãÜ) a simple multivariate normal distribution
q(C|œÄ‚ãÜ) =
Z
ÀúŒ∏sK,œÄ‚ãÜ
q(C, ÀúŒ∏sK,œÄ‚ãÜ|œÄ‚ãÜ)dÀúŒ∏sK,œÄ‚ãÜ
‚âà q(C|œÄ‚ãÜ; E[ÀúŒ∏sK,œÄ‚ãÜ]). (10)
The first term can then be calculated as follows
‚àí Eq(C|œÄ‚ãÜ)
h
log ppr(C)
i
‚âà log((2œÄ)‚àík/2|Œ£pr|‚àí1/2)
+ 1
2
 
¬µT
prŒ£‚àí1
pr ¬µpr + tr(Œ£‚àí1
pr Œ£sK,œÄ‚ãÜ) + ¬µT
sK,œÄ‚ãÜŒ£‚àí1
pr ¬µsK,œÄ‚ãÜ

‚àí ¬µT
prŒ£‚àí1
pr ¬µsK,œÄ‚ãÜ, (11)
where ¬µpr and Œ£pr are the mean vector and the covariance
matrix of the user‚Äôs preference distribution (see Appendix B
for detailed derivation).
2) Calculating the Second Term of (7): The convolved dis-
tribution p(ÀúŒ∏sK,œÄ‚ãÜ|sK, œÄ‚ãÜ) is analytically intractable. However,
we can leverage the CLT to determine a suitable proposal
distribution q(¬∑) that closely models p(¬∑). Since each p(ÀúŒ∏s,a) is
NIW-distributed, both the mean and variance are well-defined
when Œ∫ > 0 and ŒΩ > N + 1. Therefore, under the CLT,
p(ÀúŒ∏sK,œÄ‚ãÜ|sK, œÄ‚ãÜ) tends towards a MVN distribution as the
plan length |œÄ‚ãÜ| becomes large [41].
Consider the following vectorization of the parameters Œ∏s,a
vec(Œ∏s,a) = (¬µ1, . . . , ¬µN , œÉ2
1,1, . . . , œÉ2
N,N ), (12)
where ¬µ1, . . . , ¬µN represent the components of ¬µs,a, and
œÉ2
1,1, . . . , œÉ2
N,N are the unique upper triangular elements of
Œ£s,a. In total, the vectorization has dimension N(N + 3)/2.
Due to the linearity of expectation and independence between
state-action pairs,
E[ÀúŒ∏sK,œÄ] =
mX
k=0
E[ÀúŒ∏sk,ak ], (13)
var[ÀúŒ∏sK,œÄ] =
mX
k=0
var[ÀúŒ∏sk,ak ]. (14)
Therefore, the distribution p(ÀúŒ∏sK,œÄ‚ãÜ|sK, œÄ‚ãÜ) can be approx-
imately represented by multivariate normal proposal distri-
bution parameterized by the vectorized cumulative mean and
variance
q(ÀúŒ∏sK,œÄ‚ãÜ|sK,œÄ‚ãÜ)‚â°N
 X
k
E[vec(ÀúŒ∏sk,ak )],
X
k
var[vec(ÀúŒ∏sk,ak )]

.
(15)
Since q(ÀúŒ∏sK,œÄ‚ãÜ|sK, œÄ‚ãÜ) is MVN, the entropy of the proposed
distribution (second term) can be represented analytically
rewritten as follows.
H[q(ÀúŒ∏sK,œÄ|sK, œÄ‚ãÜ)]=1
2 log
 
det
 X
k
var[vec(ÀúŒ∏sk,ak )]

+ N(N + 3)
4 (1 + log 2œÄ). (16)
3) Approximating the Third Term of (7): Using both the
certainty equivalence approximation in (10) and the CLT
approximation in (15), the remaining third term can be ap-
proximated using Monte Carlo sampling of the expectation.
To sample the expectation, a cumulative cost vector must
8
be sampled from q(C|œÄ‚ãÜ). Using (10), this sample (denoted
ÀÜc) can be drawn from the MVN ÀÜC ‚àº q(C|œÄ‚ãÜ; E[ÀúŒ∏sK,œÄ‚ãÜ]).
However, note that the posterior p(ÀúŒ∏sK,œÄ‚ãÜ|C, sK, œÄ‚ãÜ) is itself
a convolved NIW conditioned on c. To construct an analytical
approximation of the posterior, we must instead expand the
sampling procedure to each state-action distribution. Instead
of sampling ÀÜC directly from q(C|œÄ‚ãÜ; E[ÀúŒ∏sK,œÄ‚ãÜ]), ÀÜc can instead
be constructed from the observation distribution for each state
action pair
ÀÜC =
X
k
ÀÜcsk,ak , (17)
where ÀÜcsk,ak ‚àºp(c|sk, ak; E[ÀúŒ∏sk,ak ]). For each sample ÀÜcsk,ak ,
the respective posterior p(ÀúŒ∏sk,ak |ÀÜcsk,ak ) are computed us-
ing the equations found in Sec. III-D. The distribution
p(ÀúŒ∏sk,ak |ÀÜcsk,ak ) is the conjugate NIW posterior. Therefore,
using the same process described in Sec. III-C2, an approx-
imation of H[q(ÀúŒ∏sK,œÄ|ÀÜC, sK, œÄ‚ãÜ)] can be calculated. This
sampling procedure is repeated ns times to compute the third
term of (7). Using the aforementioned techniques, we can
optimize for œÄ‚ãÜ
pr in (8). While this sampling procedure is
computationally burdensome, the selection procedure runs in
O(nsm|ÀúŒ†‚ãÜ|), linear in the size of number of candidate Pareto
optimal plans.
Note that unlike the prior distribution in the second term
of (7), the proposal posterior q(ÀúŒ∏sK,œÄ‚ãÜ|C,sK,œÄ‚ãÜ) is not a
variational proposal distribution. In fact, the approximation
of true posterior p(ÀúŒ∏sK,œÄ‚ãÜ|C,sK,œÄ‚ãÜ) with q(ÀúŒ∏sK,œÄ‚ãÜ|C,sK,œÄ‚ãÜ)
is an approximation inherent to active inference [31], as
p(ÀúŒ∏sK,œÄ‚ãÜ|C,sK,œÄ‚ãÜ) is often not analytically tractable. This
approximation intuitively relies on the assumption that the in-
ternal model of the agent is accurate enough to well predict the
true posterior. Our empirical evaluations in Appendix C show
that the approximation of convolved NIW with convolved
MVN agrees with the CLT. That is, the approximation is fairly
accurate for the typical plan length seen in the experiments
in Sec. IV, and increases as the plan length increases (Fig.
9a) and more data is collected (Fig. 9b). Additionally, refer to
Appendix D for an error analysis of the Monte Carlo sampling
procedure described in Sec. III-C3 with respect to the number
of samples ns, plan length, and collected data.
D. Execution and Parameter Update
After obtaining the preferred optimal trade-off plan œÄ‚ãÜ
pr,
it is executed on the robotic platform (or in simulation).
During the execution, measurements for the cost of each
state-action ÀÜc are collected. Recall, the cost parameters are
distributed as p(ÀúŒ∏s,a|Ds,a) = NIW s,a(Œª, Œ∫,Œõ, ŒΩ). Given a set
of measurements Ds,a = {ÀÜc0, . . .ÀÜcl}, the conjugate posterior
is a NIW parameterized as follows.
Œª = Œ∫0Œª0 + l¬Øc
Œ∫0 + l , (18)
Œ∫ = Œ∫0 + l, (19)
Œõ = Œõ0 + Œ∫0l
Œ∫0 + l(¬Øc ‚àí Œª0)(¬Øc ‚àí Œª0)T + (20)
lX
j=0
(ÀÜcj ‚àí ¬Øc)(ÀÜcj ‚àí ¬Øc)T ,
ŒΩ = ŒΩ0 + l, (21)
By iteratively performing each described phase, the agent
can safely complete the task, guided by the user‚Äôs preference.
Through the iterative combination of exploratory planning,
surprised-based selection, execution, and Bayesian update of
cost distributions, the proposed MORL framework can achieve
all three goals described in Prob. 1. Now, we evaluate the
empirical efficacy through experiments.
IV. E XPERIMENTS
To evaluate the effectiveness of our MORL framework for
autonomous robotic decision making in unknown environ-
ments, we performed three case studies: (i) an illustrative sim-
ulation study, (ii) two numerical benchmarking experiments
for comparison against the state-of-the-art, and (iii) a hardware
experiment to demonstrate the real-world applicability of the
method1.
A. Simulated Mars Surface Exploration Study
1) Motivation: Suppose a Mars rover is tasked to collect
scientifically interesting minerals from target sample sites
(orange region in Fig. 5), designated based on past mission
data [42]. Due to the limited number of sample tubes, it is
required to go back and forth between the deposit location
(green region) and the target site. Since the closer sampling
region is in the sun, the rover must trade-off between low-time,
high-radiation for efficient science data collection, or high-
time, low-radiation to avoid long-term damage. Therefore, to
protect itself while gaining sufficient amount of science, it
must strike a balance between the two. Moreover, although the
rover knows the qualitative information about the environment
(e.g., where sand, base, washing station, etc. are located), it is
still necessary to estimate online the time and radiation cost
of collecting a sample. Informed by past mission data and
scientific expertise, the user can tune an appropriate ppr.
2) Simulation Setup: For the sake of simplicity, the rover
can travel in cardinal directions, and may not move through
obstacles (black). The task specification is given as
œïMR = F(sample ‚àß F(deposit)) ‚àß
G(sand ‚Üí (¬¨base Uwash));
in English, ‚Äú collect a sample, then deposit it, and, if sand
is visited then wash before returning to base .‚Äù Each transition
1To see a video of the simulation and hardware experiments, visit https:
//youtu.be/tCRJwqeT-f4
9
Instance 3
Base
Deposit
Sun
Wash
Sample
Sand
Candidate
Chosen
Instance 25
Instance 35
Instance 150
0 25 50 75 100
0
10
20
30
40
50 Candidate
LCB
Chosen
True
ppr
Time (m)
Radiation (¬µGy)
(a) Instance 3
0 25 50 75 100
0
10
20
30
40
50
Time (m)
(b) Instance 25
0 25 50 75 100
0
10
20
30
40
50
Time (m)
(c) Instance 35
0 25 50 75 100
0
10
20
30
40
50
Time (m)
(d) Instance 150
Fig. 5: Simulated rover sample collection: A rover is tasked with repetitively collecting a sample and delivering it to deposit.
Moving takes time and collects radiation (in the sun). The user prefers that the sample is collected in roughly 90 minutes with
about 6 ¬µGy of radiation, represented with ppr. Top row figures show the computed plans, and the bottom row figures show
the estimated Pareto points. The true optimal trade-off plans are shown for comparison in four instance (episode) snapshots.
Video: https://youtu.be/tCRJwqeT-f4.
takes time (objective 1) measured in minutes, and transitions in
the sun accumulate harmful radiation (objective 2) measured
in micrograys. The true mean costs for transitions in each
region are given as follows:
¬µsample(left) = (6, 16) (in the sun) ,
¬µsample(right) = (6, 0) (out of the sun) ,
¬µsand = (3, 7) (slows the rover in the sun) ,
¬µwash = (11, 31) (long duration of radiation exposure) ,
¬µsun = (1 , 1), and all other transitions have ¬µ = (1 , 0).
Covariance values are omitted for brevity. The user informs
the prior transition cost for all transitions to be Œª0 = (0.5, 0).
For the longevity of the rover, the user aims to endure a small
amount of radiation and prefers that the mission is completed
in roughly one and a half hours. To express this, the chosen
preference distribution is
N(¬µpr, Œ£pr), ¬µ pr = (90, 6)
œÉ1,1 = 140, œÉ 1,2 = œÉ2,1 = ‚àí2, œÉ 2,2 = 70,
where œÉi,j is the i-j element of Œ£pr. Soon after deployment
(instance 3), the planner only finds candidate plans that go
through sand to collect the sample, and then must wash off,
seen in Fig. 5a. The robot quickly learns to avoid sand as
washing takes very long and collects lots of radiation. The
robot gradually discovers more optimal behavior seen by
instance 25 in Fig. 5b. In instance 35, seen in Fig. 5c, the
rover has roughly learned the true Pareto front, and elects to
explore a less preferred, information rich trade-off. Finally,
after 150 instances, seen in Fig. 5d, the robot has obtained
a fairly accurate estimate of the true Pareto front, and has
converged to often selecting the most preferred trade-off.
B. Benchmarks
We evaluate the performance of our framework against the
state-of-the-art in two benchmark problems. The goal is to
assess in each instance: (i) how close the selected plan œÄ‚ãÜ
pr is
to being Pareto optimal, and (ii) how well Œ†‚ãÜ is represented
by ÀúŒ†‚ãÜ. Metric cumulative Pareto-regret directly evaluates (i).
To properly evaluate (ii), we define Pareto-bias metric
for a true Pareto front ‚Ñ¶ and an estimate Pareto front Àú‚Ñ¶ as
B = 1
|‚Ñ¶|
X
Ci‚àà‚Ñ¶
min
Cj‚ààÀú‚Ñ¶
(d(Ci,Cj))+ 1
|Àú‚Ñ¶|
X
Ci‚ààÀú‚Ñ¶
min
Cj‚àà‚Ñ¶
(d(Cj,Ci)),
(22)
where
d(Ci,Cj)= W2

p(Ci|sK,œÄi;Œ∏sK,œÄi),q(Cj|sK,œÄj;E[ÀúŒ∏sK,œÄj ])

is the Wasserstein-2 ( W2) distance. The first term in (22)
quantifies the estimate Pareto front‚Äôs coverage of the true
Pareto front, and the second term penalizes outlier/excess
trade-offs in the estimate Pareto front. If each true Pareto point
is exactly similar to an estimate Pareto point, and visa versa,
then B =0, otherwise B >0.
We benchmarked our active inference (AIF) selection
method against other state-of-the-art selection methods: uni-
form random selection (fair selection), TOPSIS [8] (objective
10
0 50 100 150 200 250 300
0
200
400
600Cumulative Regret
0 50 100 150 200 250 300
Instance
0
10
20
30
40
50Cumulative Bias
1e4
TOPSIS
Weights
AIF (no var)
AIF (small var)
AIF (medium var)
AIF (large var)
Uniform
(a) 10 true Pareto points
0 100 200 300
0
250
500
750
1000Cumulative Regret
0 100 200 300
Instance
0
100
200
300Cumulative Bias
1e4 (b) 32 true Pareto points
0 100 200 300
0
250
500
750
1000Cumulative Regret
0 100 200 300
Instance
0
50
100
150Cumulative Bias
1e4 (c) 72 true Pareto points
Fig. 6: Specific environment benchmark: Within our framework, we benchmark our active inference (AIF) selection method
against other state-of-the-art methods with respect to cumulative Pareto-regret (top) and Pareto-bias (bottom) for three fixed
environments with varying numbers of true Pareto points. The mean is represented by the solid line, with 1- œÉ variance shown
by the shaded region. The dark shaded bar aids in distinguishing overlapping variance bands.
preference), and linear scalarization, i.e. ‚Äúweights‚Äù (subjective
preference). Refer to Sec. V for further description of the
compared methods.
1) Fixed Environment Case Study: Three grid world envi-
ronments were designed to challenge the learning of different
size Pareto fronts. All experiments were run for 300 instances,
with a LCB confidence parameter Œ± = 0 .1, and ns = 300
Monte Carlo samples, selected via cross validation. See Ap-
pendix D for more details on the accuracy with respect to
the choice of ns. Fig. 6 shows benchmarks against each envi-
ronment. Generally, methods that accumulate less bias require
learning more of the transition cost model via exploration, and
hence accumulate more regret. When learning the small Pareto
front, seen in Fig. 6a, AIF performed better than both uniform
and the biased methods (weights and TOPSIS) in terms of
bias, with comparable regret to uniform. This is due to the
advantageous information-gain-based exploration. When the
number of Pareto points on the Pareto front increases, seen in
Fig. 6b, using small and no 2 variance selects for only a portion
of the true Pareto front, while the medium and large variance
still envelope the entire Pareto front. With the smaller variance
AIF methods, once a good estimate candidate is found, the first
term of (7) dominates the selection, and thus abandons the less
promising options that are beyond the variance. With an even
larger Pareto front seen in Fig. 6c, this effect is exacerbated.
In this case, the no variance method sees a decrease in regret
due to the heavy myopic bias after finding a candidate solution
that agrees with the preference. Thus, from this experiment,
2If the user only wants to specify a desired point in objective-space, they
can make the preference distribution covariance very small ( no variance)
we can observe that AIF methods start becoming very selective
once the preferred region (which can be varied by the size of
variance) on the Pareto front is discovered.
2) Random Environment Case Study: The behavior of
our AIF selection method can vary widely across different
scenarios depending on the diversity of the Pareto front
relative to ppr. Therefore, we illustrate the general behavior
of our approach using randomized grid worlds. Each trial, a
20√ó20 grid world robot model is generated with randomized
proposition locations and true cost distribution parameters. The
data was analyzed across 750 randomized trials.
Fig. 7 demonstrates the general trade-off between bias
and regret. Note that the standard deviation in performance
among trials is very large, since each randomly generated
environment is diverse. When the variance of ppr is large,
active inference tends to perform similarly to random selection
(Uniform) in terms of both regret and bias. With a moderate
variance, active inference tends to have better regret and
bias performance than other methods due to the intelligent
information theoretic exploration when the estimate plans ÀúŒ†‚ãÜ
are uninformed. Uniform, AIF (large var), and AIF (medium
var) stabilize in cumulative bias, meaning they effectively learn
the entire Pareto front. The remaining methods converge to a
non-constant slope cumulative bias, indicating that portions of
the Pareto front have not been properly learned. From this
experiment, we can observe that AIF methods outperform
other state-of-the-art methods in terms of both cumulative
regrets and biases as long as the size of variance is not too
large.
Akin to other expected free energy approaches, the en-
11
0 100 200 300
0
500
1000
1500
2000
2500Cumulative Regret
0 100 200 300
Instance
0.0
2.5
5.0
7.5
10.0
12.5Cumulative Bias
1e4
TOPSIS
Weights
AIF (no var)
AIF (small var)
AIF (medium var)
AIF (large var)
Uniform
Fig. 7: Randomized environment benchmark: We benchmark
our method against other selection methods for randomly
generated scenarios of varying complexity.
hanced reasoning power comes at a computational cost com-
pared to other methods [20, 31]. Uniform selection has com-
plexity O(1), weights has O(|ÀúŒ†‚ãÜ|), and TOPSIS has O(|ÀúŒ†‚ãÜ|2)
[43]. We benchmarked the aforementioned random environ-
ment case studies with respect to both planning time and
selection time per instance on a computer with AMD Ryzen
7 3800X 3.9 GHz 8-Core Processor. The average planning
time is across 400 experiments with 100 instances each is
483 ¬± 443ms (1- œÉ) with a median planning time of 369ms.
The average AIF selection time (across all four preference
distribution variances) is 610 ¬± 1051ms (1- œÉ) with a median
selection time of 179ms. The compared selection procedures
completed in less than 1ms.
Remark 1. For larger-scale robotic models, the exact multi-
objective graph search can be replaced with approximate
multi-objective search, such as A‚àópex [44], to admit a smaller
(approximate) Pareto front in less time. Additionally, a clus-
tering algorithm [45, 46] can further reduce the size of the
Pareto front. We expect both adaptations to our framework to
greatly enhance computation speed at the cost of Pareto-regret
and bias performance, however we leave this to future work.
C. Hardware Experiment
To demonstrate the efficacy and diversity of application of
our framework, we studied a complex dishwashing experiment
using a robotic manipulator described in Fig. 1. The robot must
repetitively load and unload the dishwasher with two items, a
durable pitcher, and a fragile glass jar. The LTLf specification
is
œï = œïl ‚àß œïs,
where
œïl =F(Pdw ‚àß Jdw ‚àß lidon ‚àß F(Prack ‚à® (Pdry ‚àß F Pfloor )‚àß
F(Jrack ‚à® (Jdry ‚àß F Jfloor ))),
œïs =G(¬¨lidaside ‚Üí F(Pdw ‚àß Jdw)),
which can be interpreted as ‚Äú put the pitcher and the jar in the
dishwasher, then put the lid on, then for both the pitcher and
jar, place on the drying rack, or dry the item and place it on
the ground‚Äù. The performance of the robot is evaluated by the
cumulative execution time (objective 1), as well as cumulative
risk (objective 2) measured for a given action primitive,
risk(s, a) =
(R
t h2
Jdt if holding jar,
0 otherwise, (23)
where hJ is the height of the jar above the ground. A sampling
based motion planner (PRM ‚àó [47]) is used to realize motion
action primitives, inducing unknown and random time and risk
costs. Note that placing the jar on the drying rack is likely
to have high risk, whereas manually drying the jar on the
ground will likely take more time. We trained the transition
cost model in simulation for 500 instances, using a preference
distribution of ¬µpr = (350 , 0.5) and œÉ2
1,1 = 400 , œÉ2
2,2 = 2 ,
œÉ1œÉ2 = 0, which selects for a high-time, low-risk alternative.
The converged behavior is shown in Fig. 8. As can be seen, the
robot correctly loads the dishwasher, then unloads the pitcher
on the rack, and decides to manually dry the jar, all while not
performing any sub-optimal/unnecessary actions. The video of
the execution is included in the supplementary material and
visually represents the applicability of our MORL framework
to a variety of robotic models with complex tasks.
V. R ELATED WORK
A. Multi-Objective Reinforcement Learning
The goal of MORL is to maximize a multi-objective value
or utility function [7]. MORL extends single-objective RL
methods by selecting among Pareto-optimal candidate actions.
One method of selection is casting valuations of candidate
actions to a single-objective by scalarizing, in turn requiring
a sound and interpretable utility function.
Scalarization of utility functions can be classified as sub-
jective, scalarizing based on an exogenous user‚Äôs preferred
trade-off [6], or non-subjective, scalarizing without external
preferences [5, 8]. Linear scalarization is a standard subjective
method that combines objectives objectives through a relative-
importance weighted sum. On the contrary, non-subjective
methods such as TOPSIS [8] and hyper-volume indicators [5]
rely on Euclidian distance metrics in objective space. However,
in many robotic applications where objectives have different
units of measurement (e.g. time in seconds and energy in
joules), these methods use the addition of quantities with
different units, which is generally mathematically ad-hoc and
12
Fig. 8: Hardware dishwashing experiment: A robotic manipulator must load both a durable pitcher and a fragile glass jar into a
dishwasher, then must unload each item by either placing it on the drying rack, or manually drying it on the ground. The robot
learns to minimize time and risk (holding the jar high above the ground). The user prefers a low-risk, higher-time trade-off.
After 500 instances, the system performs optimally (left to right, top to bottom), and decides to quickly place the pitcher on
the rack, and spend time to manually dry the jar. Video: https://youtu.be/tCRJwqeT-f4.
lacks interpretation. Linear scalarization can potentially com-
bat this issue by interpreting weights as reciprocal measures
(e.g. 1/second), at the cost of requiring a user to have a
rigorous understanding of how to compare the importance
of each objective and properly assign weights. Furthermore,
re-interpreting units of measurement (e.g. seconds instead of
milliseconds) alters Euclidian distance, which can dramatically
alter the selected solution, when no fundamental aspects about
the problem have changed. On the contrary, our method
avoids this issue by only describing the scalarization through
operations on probability density functions.
Many MORL methods do not distinguish between optimiz-
ing multi-objective value function and selecting the best action
from the Pareto-optimal candidate actions [6, 7]. Instead,
these methods scalarize using a monotonic utility function,
which guarantees that an optimal (preferred) action with
respect to the single-objective utility function must also be
Pareto-optimal in objective space. The use of such a utility
function effectively optimizes a single objective, which casts a
complete order on the reward space and eliminates the need for
explicit computation of the Pareto front beforehand. However,
when a non-monotonic utility function is used, or the agent
is concerned with learning the set of all optimal trade-off
solutions [15], the Pareto front must be explicitly computed
before selection, as seen in many MOMAB approaches. Our
framework computes the Pareto front using a multi-objective
task-planner.
MOMAB problems are concerned with minimizing Pareto-
Regret [15], which uses Œµ-dominance to measure how ‚Äúfar‚Äù
the chosen action is from being Pareto-optimal with respect
to the true unknown reward distributions. Besides Pareto-
regret, the MOMAB performance can also be evaluated by
fairness, which quantifies how evenly Pareto-optimal actions
were selected. Fairness is used to compare MOMAB selection
by diversity in selection and learning of all actions on a
possibly non-convex Pareto-front. To learn the entire Pareto
front, work [15] extends the single-objective Upper Confidence
Bound (UCB) algorithm [36] to multiple objectives, then rely
on random selection among the Pareto front. While random
selection effectively learns the entire Pareto front, it cannot
account for or converge to a preferred trade-off. To address this
shortcoming, our formulation of the high-level active inference
decision making agent naturally balances this exploration vs.
exploitation trade-off.
B. Active Inference and Expected Free Energy
Active inference is an uncertainty-based sequential decision
making scheme that applies the free energy principle (FEP)
[48] to the behavioral norms of physical agents [19, 20, 21].
According to the FEP, agents intend to minimize the surprise
(in our problem it is denoted as ‚àílog p(C|œÄ)) to maintain
their homeostases. However, calculating the surprise directly
requires to sum/integrate over all possible states/parameters
in their internal generative models. When these quantities
are continuous, depending on the prior and/or the likelihood
function, the integral does not have a closed-form solution.
Thus, some approximation methods are required to calculate
the surprise.
One of the approaches is to employ variational Bayesian
inference in statistical machine learning [40]. The core idea
of this methodology is to introduce a proposal/surrogate dis-
tribution, which is easily modeled, and optimize the functional
of this distribution to make it resemble a true intractable distri-
bution. More specifically, in our problem, the following free-
energy (FE, i.e. negative evidence lower bound) is minimized
such that the Kullback-Leibler Divergence (KLD) between
the proposal distribution q(s, ÀúŒ∏|œÄ) and the true distribution
13
p(s, ÀúŒ∏|C, œÄ) approaches zero (Note that here we used the
simplified notations seen in Appendices A and B).
F E=
Z
s,ÀúŒ∏
q(s, ÀúŒ∏|œÄ) log q(s, ÀúŒ∏|œÄ)
p(s, ÀúŒ∏, C|œÄ)
dÀúŒ∏ds
=
Z
s,ÀúŒ∏
q(s, ÀúŒ∏|œÄ) log q(s, ÀúŒ∏|œÄ)
p(s, ÀúŒ∏|C, œÄ)
dÀúŒ∏ds ‚àí log p(C|œÄ)
= DKL

q(s, ÀúŒ∏|œÄ)||p(s, Œ∏|C, œÄ)

‚àí log p(C|œÄ)
‚â• Surprise(œÄ). (24)
As can be seen from this equation, since KLD is greater or
equal to zero, FE is the upper bound of the surprise, i.e.
minimizing FE leads to minimize the surprise.
Computation of FE is performed given an observed outcome
observation C, which is not known until the plan is executed.
Hence, in active inference, the expected free energy (EFE)
is instead minimized, and similar to FE, if the proposal and
the true hidden posterior distributions match, the EFE term
reduces to the following,
EF E‚Üí ‚àíEq(C|œÄ)[log ppr(C)], (25)
which is ‚Äúexpected‚Äù surprise. In summary, FE allows to
make the surprise minimization tractable for continuous
state/parameter systems, and EFE allows for the minimization
FE without knowing the received observation a priori.
Additionally, as explained in (6), the EFE comprises of an
utility term governed by a prior agent‚Äôs desired observation
distribution, and an information gain term that evaluates how
much a candidate action would reduce the uncertainty of hid-
den states/parameters. Hence, by minimizing EFE, the agents
can naturally balance between two modes of behavior that are
essential for autonomous decision making under uncertainty;
exploitation (i.e. focusing on the current best action) and
exploration (i.e. trying less executed actions). Furthermore, the
degree of preference for either mode can be easily adjusted by
varying the prior preference distribution. For instance, using
a prior preference with a large variance will cause the agents
to provide more priority to exploration, and vice versa. These
characteristics are particularly useful when transparent agent
behavior is desired [49]. Using active inference for selection
marries the benefits of preferred versus fair selection described
in Sec. V-A through localized information-theoretic explo-
ration around the prior preference distribution. Additionally,
specifying a preference distribution over desired trade-offs
may be particularly intuitive, explainable, and even statistically
informed with past data. Throughout this work, we study the
benefits of using EFE in active inference for Pareto point
selection in MORL with two statistical techniques when an
intractable convolved NIW is used to represent the hidden
parameters.
VI. C ONCLUSION AND FUTURE WORK
In this work we study a systematic approach to learn-
ing Pareto-optimal behavior on an unknown multi-objective
stochastic cost model. In particular we examine an active
inference-based approach to multi-objective decision making
and we examine the resulting behavior through robotic simu-
lation and hardware experiments, and numerical benchmarks
that compare against other state-of-the-art selection methods.
Our approach marries the benefits of a user-provided preferred
optimal trade-off with exploration required to learn portions of
the entire Pareto front. We introduce a new Pareto-bias metric
that, coupled with traditional Pareto-regret, elucidates the
trade-off between accurately learning the diverse Pareto front
versus quickly learning a single optimal trade-off. Notably,
the balance between the aforementioned behaviors can be
controlled via the user‚Äôs preference.
This work gives rise to a few future directions of research
such as extension to stochastic transition models. Additionally,
integrating cost-bounds for certain objectives by embedding a
pruning procedure [14] into the multi-objective graph search
algorithm is an interesting extension. Since the costs are not
known a priori , we expect the Pareto cost upper confidence
bound to be an effective choice of pruning constraint. We
are also interested in generalizing the cost distribution to a
Gaussian mixture to account for multi-modality. Other future
directions may include convergence analysis and finite time
regret analysis, for example, adapting the Upper Credible
Limit algorithm [33] for Gaussian multi-armed bandits to the
multi-objective setting.
REFERENCES
[1] Yang Gao and Steve Chien. Review on space robotics:
toward top-level science through space exploration. Sci-
ence Robotics, 2(7), 2017.
[2] Jaeseok Kim, Anand Kumar Mishra, Raffaele Limosani,
Marco Scafuro, Nino Cauli, Jose Santos-Victor, Barbara
Mazzolai, and Filippo Cavallo. Control strategies for
cleaning robots in domestic applications: A comprehen-
sive review. International Journal of Advanced Robotic
Systems, 16(4):1729881419857432, 2019.
[3] Lydia E Kavraki, Petr Svestka, J-C Latombe, and Mark H
Overmars. Probabilistic roadmaps for path planning in
high-dimensional configuration spaces. IEEE transac-
tions on Robotics and Automation , 12(4):566‚Äì580, 1996.
[4] Sertac Karaman and Emilio Frazzoli. Sampling-based al-
gorithms for optimal motion planning. The international
journal of robotics research , 30(7):846‚Äì894, 2011.
[5] Weijia Wang and Mich `ele Sebag. Multi-objective Monte-
Carlo tree search. In Steven C. H. Hoi and Wray
Buntine, editors, Proceedings of the Asian Conference
on Machine Learning , volume 25 of Proceedings of
Machine Learning Research , pages 507‚Äì522, Singapore
Management University, Singapore, 04‚Äì06 Nov 2012.
PMLR.
[6] Axel Abels, Diederik M. Roijers, Tom Lenaerts, Ann
Now¬¥e, and Denis Steckelmacher. Dynamic weights in
multi-objective deep reinforcement learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceed-
ings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, volume 97 of Proceedings of Machine
Learning Research, pages 11‚Äì20. PMLR, 2019.
14
[7] Conor F. Hayes, Roxana R Àòadulescu, Eugenio Bargiacchi,
Johan K ¬®allstr¬®om, Matthew Macfarlane, Mathieu Rey-
mond, Timothy Verstraeten, Luisa M. Zintgraf, Richard
Dazeley, Fredrik Heintz, Enda Howley, Athirai A. Iris-
sappane, Patrick Mannion, Ann Now ¬¥e, Gabriel Ramos,
Marcello Restelli, Peter Vamplew, and Diederik M. Roi-
jers. A practical guide to multi-objective reinforcement
learning and planning. Autonomous Agents and Multi-
Agent Systems, 36(1), apr 2022.
[8] Mohammad Mirzanejad, Morteza Ebrahimi, Peter Vam-
plew, and Hadi Veisi. An online scalarization multi-
objective reinforcement learning algorithm: Topsis q-
learning. The Knowledge Engineering Review , 37:e7,
2022.
[9] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen,
George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft
actor-critic algorithms and applications. arXiv preprint
arXiv:1812.05905, 2018.
[10] Saurabh Arora and Prashant Doshi. A survey of in-
verse reinforcement learning: Challenges, methods and
progress. Artificial Intelligence, 297:103500, 2021.
[11] Hadas Kress-Gazit, Morteza Lahijanian, and Vasumathi
Raman. Synthesis for robots: Guarantees and feedback
for robot behavior. Review of Control, Robotics, and
Autonomous Systems, 1:211‚Äì236, May 2018.
[12] Christel Baier and Joost-Pieter Katoen. Principles of
Model Checking (Representation and Mind Series) . The
MIT Press, 2008.
[13] Giuseppe De Giacomo and Moshe Y Vardi. Linear tem-
poral logic and linear dynamic logic on finite traces. In
IJCAI‚Äô13 Proceedings of the Twenty-Third international
joint conference on Artificial Intelligence , pages 854‚Äì
860. Association for Computing Machinery, 2013.
[14] Peter Amorese and Morteza Lahijanian. Optimal cost-
preference trade-off planning with multiple temporal
tasks. In 2023 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pages 2071‚Äì2077,
2023.
[15] Madalina M. Drugan and Ann Nowe. Designing multi-
objective multi-armed bandits algorithms: A study. In
The 2013 International Joint Conference on Neural Net-
works (IJCNN), pages 1‚Äì8, 2013.
[16] R ¬¥obert Busa-Fekete, Bal ¬¥azs Sz ¬®or¬¥enyi, Paul Weng, and
Shie Mannor. Multi-objective bandits: Optimizing the
generalized gini index. In Proceedings of the 34th
International Conference on Machine Learning - Volume
70, ICML‚Äô17, page 625‚Äì634. JMLR.org, 2017.
[17] Eralp Turgay, Doruk ¬®Oner, and Cem Tekin. Multi-
objective contextual bandit problem with similarity in-
formation. In Amos J. Storkey and Fernando P ¬¥erez-
Cruz, editors, International Conference on Artificial In-
telligence and Statistics, AISTATS 2018, 9-11 April 2018,
Playa Blanca, Lanzarote, Canary Islands, Spain , vol-
ume 84 of Proceedings of Machine Learning Research ,
pages 1673‚Äì1681. PMLR, 2018.
[18] Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang.
Multi-objective generalized linear bandits. IJCAI‚Äô19,
page 3080‚Äì3086. AAAI Press, 2019.
[19] Karl Friston, Francesco Rigoli, Dimitri Ognibene,
Christoph Mathys, Thomas FitzGerald, and Giovanni
Pezzulo. Active inference and epistemic value. Cognitive
neuroscience, 02 2015.
[20] Raphael Kaplan and Karl J. Friston. Planning and naviga-
tion as active inference. Biol. Cybern., 112(4):323‚Äì343,
aug 2018.
[21] Karl J Friston, Maxwell JD Ramstead, Alex B Kiefer,
Alexander Tschantz, Christopher L Buckley, Mahault Al-
barracin, Riddhi J Pitliya, Conor Heins, Brennan Klein,
Beren Millidge, Dalton AR Sakthivadivel, Toby St Clere
Smithe, Magnus Koudahl, Safae Essafi Tremblay, Capm
Petersen, Kaiser Fung, Jason G Fox, Steven Swanson,
Dan Mapes, and Gabriel Ren ¬¥e. Designing ecosystems of
intelligence from first principles. Collective Intelligence,
3(1):26339137231222481, 2024.
[22] Shohei Wakayama and Nisar Ahmed. Active inference
for autonomous decision-making with contextual multi-
armed bandits. In 2023 IEEE International Conference
on Robotics and Automation (ICRA) , pages 7916‚Äì7922,
2023.
[23] Marius Kloetzer and Calin Belta. A fully automated
framework for control of linear systems from temporal
logic specifications. IEEE Transactions on Automatic
Control, 53(1):287‚Äì297, 2008.
[24] Georgios E Fainekos, Hadas Kress-Gazit, and George J
Pappas. Temporal logic motion planning for mobile
robots. In Proceedings of the 2005 IEEE International
Conference on Robotics and Automation , pages 2020‚Äì
2025. IEEE, 2005.
[25] Shuo Yang, Xiang Yin, Shaoyuan Li, and Majid Zamani.
Secure-by-construction optimal path planning for linear
temporal logic tasks. In 2020 59th IEEE Conference on
Decision and Control (CDC) , pages 4460‚Äì4466. IEEE,
2020.
[26] Georgios E Fainekos, Hadas Kress-Gazit, and George J
Pappas. Hybrid controllers for path planning: A temporal
logic approach. In Proceedings of the 44th IEEE Confer-
ence on Decision and Control , pages 4885‚Äì4890. IEEE,
2005.
[27] Keliang He, Morteza Lahijanian, Lydia E Kavraki, and
Moshe Y Vardi. Towards manipulation planning with
temporal logic specifications. In 2015 IEEE international
conference on robotics and automation (ICRA) , pages
346‚Äì352. IEEE, 2015.
[28] Keliang He, Morteza Lahijanian, E Kavraki, Lydia, and
Y Vardi, Moshe. Automated abstraction of manipulation
domains for cost-based reactive synthesis. IEEE Robotics
and Automation Letters , 4(2):285‚Äì292, Apr. 2019.
[29] Pablo Lanillos, Cristian Meo, Corrado Pezzato,
Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata,
Alexander Tschantz, Beren Millidge, Martijn Wisse,
Christopher L. Buckley, and Jun Tani. Active inference
in robotics and artificial agents: Survey and challenges.
CoRR, abs/2112.01871, 2021.
[30] Thomas M. Cover and Joy A. Thomas. Elements of
Information Theory (Wiley Series in Telecommunications
15
and Signal Processing). Wiley-Interscience, USA, 2006.
[31] Ryan Smith, Karl J. Friston, and Christopher J. Whyte.
A step-by-step tutorial on active inference and its ap-
plication to empirical data. Journal of Mathematical
Psychology, 107:102632, 2022.
[32] Kevin P. Murphy. Conjugate bayesian analysis of the
gaussian distribution. 2007.
[33] Paul B Reverdy, Vaibhav Srivastava, and Naomi Ehrich
Leonard. Modeling human decision making in gener-
alized gaussian multiarmed bandits. Proceedings of the
IEEE, 102(4):544‚Äì571, 2014.
[34] Yen-Chi Chen. A tutorial on kernel density estimation
and recent advances. Biostatistics & Epidemiology ,
1(1):161‚Äì187, 2017.
[35] Lawrence Mandow and Jos ¬¥e Luis P ¬¥erez De La Cruz.
Multiobjective a* search with consistent heuristics. Jour-
nal of the ACM (JACM) , 57(5):1‚Äì25, 2008.
[36] Peter Auer, Nicol `o Cesa-Bianchi, and Paul Fischer.
Finite-time analysis of the multiarmed bandit problem.
Mach. Learn., 47(2‚Äì3):235‚Äì256, may 2002.
[37] David M Blei, Alp Kucukelbir, and Jon D McAuliffe.
Variational inference: A review for statisticians. Journal
of the American statistical Association , 112(518):859‚Äì
877, 2017.
[38] Horia Mania, Stephen Tu, and Benjamin Recht. Certainty
equivalence is efficient for linear quadratic control. Ad-
vances in Neural Information Processing Systems , 32,
2019.
[39] Sang Gyu Kwak and Jong Hae Kim. Central limit
theorem: the cornerstone of modern statistics. Korean
journal of anesthesiology , 70(2):144‚Äì156, 2017.
[40] Christopher M. Bishop. Pattern Recognition and Ma-
chine Learning (Information Science and Statistics) .
Springer-Verlag, Berlin, Heidelberg, 2006.
[41] William L Dunn and J Kenneth Shultis. Exploring monte
carlo methods. Elsevier, 2022.
[42] Kenneth A Farley, Kenneth H Williford, Kathryn M
Stack, Rohit Bhartia, Al Chen, Manuel de la Torre, Kevin
Hand, Yulia Goreva, Christopher DK Herd, Ricardo
Hueso, et al. Mars 2020 mission overview. Space Science
Reviews, 216:1‚Äì41, 2020.
[43] Hamdani Hamdani and Retantyo Wardoyo. The complex-
ity calculation for group decision making using topsis
algorithm. In AIP conference proceedings, volume 1755.
AIP Publishing, 2016.
[44] Han Zhang, Oren Salzman, TK Satish Kumar, Ariel Fel-
ner, Carlos Hern ¬¥andez Ulloa, and Sven Koenig. A* pex:
Efficient approximate multi-objective search on graphs.
In Proceedings of the International Conference on Auto-
mated Planning and Scheduling , volume 32, pages 394‚Äì
403, 2022.
[45] Enrico Zio and Roberta Bazzo. A clustering procedure
for reducing the number of representative solutions in
the pareto front of multiobjective optimization problems.
European Journal of Operational Research, 210(3):624‚Äì
634, 2011.
[46] Lilian Astrid Bejarano, Helbert Eduardo Espitia, and
Carlos Enrique Montenegro. Clustering analysis for
the pareto optimal front in multi-objective optimization.
Computation, 10(3):37, 2022.
[47] SM LaValle. Planning Algorithms, volume 2. 2006.
[48] K. Friston. The free-energy principle: A unified brain
theory? Nature Reviews Neuroscience , 11(2):127‚Äì138,
2010.
[49] Shohei Wakayama and Nisar Ahmed. Observation-
augmented contextual multi-armed bandits for robotic
exploration with uncertain semantic data. arXiv,
(2312.12583), 2023.
[50] Robert A. Stine. Explaining normal quantile-quantile
plots through animation: The water-filling analogy. The
American Statistician, 71(2):145‚Äì147, 2017.
[51] Zafeirios Fountas, Noor Sajid, Pedro Mediano, and Karl
Friston. Deep active inference agents using monte-carlo
methods. Advances in neural information processing
systems, 33:11662‚Äì11675, 2020.
[52] Alexander Tschantz, Manuel Baltieri, Anil. K. Seth, and
Christopher L. Buckley. Scaling active inference. In
2020 International Joint Conference on Neural Networks
(IJCNN), pages 1‚Äì8, 2020.
[53] Domenico Maisto, Francesco Gregoretti, Karl J. Friston,
and Giovanni Pezzulo. Active tree search in large
pomdps. ArXiv, abs/2103.13860, 2021.
APPENDIX A
DERIVATION OF EFE FOR THE PARETO POINT SELECTION
As mentioned in Sec. III-C, marginalizing the joint distri-
bution is analytically intractable. Therefore, its bound, i.e. free
energy, is minimized instead. Yet, the outcomes C cannot be
observed until a plan œÄ‚ãÜ is actually executed, so the agent ends
up minimizing EFE. Hereafter, for the sake of simplicity, we
denote sK as s, œÄ‚ãÜ as œÄ, and ÀúŒ∏s,œÄ as ÀúŒ∏.
EFE(œÄ)
=
Z
s,ÀúŒ∏
q(s,ÀúŒ∏|œÄ)
Z
C
q(C|s,ÀúŒ∏,œÄ)log q(s,ÀúŒ∏|œÄ)
p(s,ÀúŒ∏|C,œÄ)ppr(C)
dCdÀúŒ∏ds,
(26)
where ppr(C) is the prior preference for outcomes and
q(s, ÀúŒ∏|œÄ) is the proposal distribution for s and ÀúŒ∏ given a plan
œÄ. Eq. (26) can be decomposed into three parts as follows.
(1st)= ‚àí
Z
s,ÀúŒ∏,C
q(s, ÀúŒ∏|œÄ)q(C|s, ÀúŒ∏, œÄ) logppr(C)dCdÀúŒ∏ds,
=‚àí
Z
C
log ppr(C)
nZ
s,ÀúŒ∏
q(s, ÀúŒ∏, C|œÄ)dÀúŒ∏ds
o
dC,
=‚àíEq(C|œÄ)
h
log ppr(C)
i
. (27)
(2nd)=
Z
s,ÀúŒ∏,C
q(s, ÀúŒ∏|œÄ)q(C|s, ÀúŒ∏, œÄ) log q(s|œÄ)
p(s|C, œÄ)dCdÀúŒ∏ds,
=
Z
s,C
nZ
ÀúŒ∏
q(s, ÀúŒ∏, C|œÄ)dÀúŒ∏
o
log q(s|œÄ)
p(s|C, œÄ)dCds,
=‚àí
Z
C
q(C|œÄ)
Z
s
q(s|C, œÄ) log q(s|C, œÄ)
q(s|œÄ) dsdC,
=‚àíEq(C|œÄ)
h
DKL

q(s|C, œÄ)||q(s|œÄ)
i
. (28)
16
(3rd)=
Z
s,ÀúŒ∏,C
q(s, ÀúŒ∏|œÄ)q(C|s, ÀúŒ∏, œÄ) log q(ÀúŒ∏|s, œÄ)
p(ÀúŒ∏|s, C, œÄ)
dCdÀúŒ∏ds,
=‚àí
Z
s,C
q(s, C|œÄ)
nZ
ÀúŒ∏
q(ÀúŒ∏|s,C,œÄ) log q(ÀúŒ∏|s,C,œÄ)
q(ÀúŒ∏|s, œÄ)
dÀúŒ∏
o
dCds,
=‚àíEq(s,C|œÄ)
h
DKL

q(ÀúŒ∏|s, C, œÄ)||q(ÀúŒ∏|s, œÄ)
i
. (29)
By combining these parts, (6) is finally derived.
APPENDIX B
APPROXIMATION FOR THE FIRST TERM OF (7)
In order to approximate the first term of (7), log ppr(C) can
be at first simplified as follows.
log ppr(C)=log

Zprexp
 
‚àí1
2(C‚àí¬µpr)T Œ£‚àí1
pr (C‚àí¬µpr)

,
= log(Zpr)‚àí1
2(C‚àí¬µpr)T Œ£‚àí1
pr (C‚àí¬µpr)

, (30)
where Zpr = (2œÄ)‚àíN/2|Œ£pr|‚àí1/2. Recall, the certainty equiv-
alence approximation for the predicted observation distribution
q(C|œÄ) ‚âà q(C|œÄ; E[Œ∏]) is a MVN distribution q(C|œÄ) =
N(¬µs,œÄ, Œ£s,œÄ). By substituting this result into (7), the first
term of (7) can be further transformed as
(1st)= ‚àíEq(C|œÄ;E[Œ∏])
h
log(Zpr)‚àí(C‚àí¬µpr)T Œ£‚àí1
pr (C‚àí¬µpr)
2
i
.
(31)
Since (C‚àí¬µpr)T Œ£‚àí1
pr (C‚àí¬µpr) is expanded as CT Œ£‚àí1
pr C‚àí
2¬µT
prŒ£‚àí1
pr C+¬µT
prŒ£‚àí1
pr ¬µpr, the second term of (31) is turned to
(2nd of (31)) = 1
2
Z
C
q(C|œÄ; E[Œ∏])
 
CT Œ£‚àí1
pr C

dC (32)
+ 1
2
Z
C
q(C|œÄ; E[Œ∏])
 
¬µT
prŒ£‚àí1
pr ¬µpr

dC (33)
‚àí
Z
C
q(C|œÄ; E[Œ∏])
 
‚àí ¬µT
prŒ£‚àí1
pr C

dC (34)
Finally, (32) can be reduced to
1
2

tr(Œ£‚àí1
pr Œ£) + ¬µT Œ£‚àí1
pr ¬µ

where E[Œ∏] = {¬µ, Œ£}. Also, (33) is simply
¬µT
prŒ£‚àí1
pr ¬µpr,
and (34) can be reduced to
‚àí¬µT
prŒ£‚àí1
pr ¬µ.
APPENDIX C
EMPIRICAL ANALYSIS OF ERROR INTRODUCED VIA
NIW-MVN S UBSTITUTION
To evaluate the choice of approximate posterior described
in Sec. III-C3, we numerically demonstrated the efficacy of
substituting intractable convolved NIW distributions (denoted
p(¬∑)) with convolutions of MVN (denoted q(¬∑)). Additionally,
since the same substitution technique is used for choosing the
prior proposal distribution in (15), this analysis also yields
insight into the quality of the variational upper bound.
Following (13) and (14), p(¬∑) and q(¬∑) must have the same
mean and variance, therefore, it suffices to analyze the normal-
ity of Monte Carlo samples of p(¬∑) for determining how well
q(¬∑) approximates p(¬∑). This analysis is done using a Quantile-
Quantile (Q-Q) plot [50]. The procedure for generating Monte
Carlo samples used in the Q-Q plot is outlined as follows:
1) Generate M number of samples of ÀÜÀúŒ∏s,a from each
respective NIW p(ÀúŒ∏sk,ak ). For high fidelity, we used
M = 5000 samples.
2) Sum each respective sample to get M samples of ÀÜÀúŒ∏sK,œÄ,
(empirically representing samples from p(ÀúŒ∏|s, C, œÄ)),
3) Perform a whitening transform to all ÀÜÀúŒ∏sK,œÄ such that
the such that the mean is zero and the covariance is
normalized and uncorrelated,
4) Produce a Q-Q plot to assess the normality of the
whitened empirical distribution.
The resulting Q-Q plots are shown in Fig. 9. The distributions
of the mean parameters ¬µsK,œÄ (Fig. 9a, top row) are generally
well approximated by q(¬∑). For short plans |œÄ| = 10 , p(¬∑)
the covariance parameter distributions differ from q(¬∑) slightly
(Fig. 9a, bottom left), however, due to the Central Limit
Theorem (CLT), they converge to normal as the length of the
plan increases (Fig. 9a, bottom right). Similarly, as more data
is supplied to each state-action pair, the distribution becomes
more normal due to the Inverse Wishart distribution being less
right-skewed (Fig. 9b, bottom left to bottom right). Therefore,
the MVN replacement approximation introduces less error
as the scenarios become more complex, and more data is
collected.
APPENDIX D
EMPIRICAL ANALYSIS OF ERROR INTRODUCED VIA
MONTE CARLO SAMPLING
Fig. 10 shows the percent error introduced via sampling.
Note that ns = 300 was used for the experiments and bench-
marks shown in the manuscript as well as the computation
time benchmarks. Although the specific sampling procedure is
slightly different, there are multiple studies using the Monte
Carlo sampling to evaluate the EFE [51, 52, 53].
17
-4
-2
0
2
4
-5 0 5
-2
0
2
4
6
8
-5 0 5-5 0 5
(a) Q-Q plot varying length of plan ( m = |œÄ|), while each (s, a)
has 10 data samples ( l). As the length of the plan increases, the true
distribution becomes more normal (i.e. the effect of the CLT).
-4
-2
0
2
4
-5 0 5
-4
-2
0
2
4
6
-5 0 5-5 0 5
(b) Q-Q plots of varying number of collected samples ( l) from
execution for each state-action, while m = 20. As more samples
are collected, the true distribution becomes more normal.
Fig. 9: Q-Q Plot assessing the normality of Monte Carlo approximated true posterior p(ÀúŒ∏|s, C, œÄ). For brevity, only ¬µ1 and
œÉ1,1 are shown. The closer the samples (blue) are to the quantile line (red), the more normal the empirical distribution.
0 100 200 300 400 500
Monte Carlo Samples
0.000
0.002
0.004
0.006
0.008
0.010
0.012
0.014Percent Error
m = 10
m = 20
m = 50
(a) Error for varying length of plan ( m).
0 100 200 300 400 500
Monte Carlo Samples
0.000
0.002
0.004
0.006
0.008
0.010
0.012
0.014Percent Error
l = 1
l = 2
l = 10
l = 20 (b) Error for varying number of collected samples ( l).
Fig. 10: Percent error introduced via sampling.