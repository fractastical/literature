arXiv:2107.09256v2  [cs.LG]  26 Jul 2021
Active operator inference for learning
low-dimensional dynamical-system models from
noisy data
Wayne Isaac Tan Uy, Yuepeng Wang, Yuxiao Wen, and Benjamin Peherstorfer∗
July 2021
Noise poses a challenge for learning dynamical-system models becaus e already small variations
can distort the dynamics described by trajectory data. This work builds on operator inference
from scientiﬁc machine learning to infer low-dimensional models from h igh-dimensional state tra-
jectories polluted with noise. The presented analysis shows that, u nder certain conditions, the
inferred operators are unbiased estimators of the well-studied pr ojection-based reduced operators
from traditional model reduction. Furthermore, the connection between operator inference and
projection-based model reduction enables bounding the mean-sq uared errors of predictions made
with the learned models with respect to traditional reduced models. The analysis also motivates
an active operator inference approach that judiciously samples hig h-dimensional trajectories with
the aim of achieving a low mean-squared error by reducing the eﬀect of noise. Numerical exper-
iments with high-dimensional linear and nonlinear state dynamics demo nstrate that predictions
obtained with active operator inference have orders of magnitude lower mean-squared errors than
operator inference with traditional, equidistantly sampled traject ory data.
Keywords: scientiﬁc machine learning, non-intrusive model reduction, operat or inference, design of experi-
ments, reduced models, noise
1. Introduction
Noise poses a challenge for learning dynamical-system models becaus e already small variations can distort
the dynamics described by trajectory data. In this work, we build o n operator inference [38] from scientiﬁc
machine learning to derive low-dimensional dynamical-system models f rom high-dimensional, noisy state
trajectories. We introduce a sampling scheme to query the high-dim ensional systems for data so that, under
certain conditions, in particular if the high-dimensional system dyna mics are polynomially nonlinear, the
inferred operators are unbiased estimators of the well-studied re duced operators obtained via projection of
the governing equations of the high-dimensional systems in classica l model reduction [1, 42, 8]. Additionally,
we show that the mean-squared error (MSE) of the states predic ted with the learned models can be bounded
independently of the dimensions of the high-dimensional systems an d in terms of the noise-to-signal ratio
of the trajectory data. Motivated by the analysis, we propose ac tive operator inference that queries high-
dimensional systems in a principled way to generate data with low noise -to-signal ratios, which reduces by
a factor of up to three the number of data samples that are requir ed from the high-dimensional systems to
∗ {wayne.uy,pehersto}@cims.nyu.edu, {yw3114,yw3210}@nyu.edu, Courant Institute of Mathematical Sciences, NewYork Uni-
versity, New York, NY 10012
make accurate state predictions in our numerical experiments. Fo r the same number of data samples, active
operator inference achieves orders of magnitude lower MSEs than traditional, equidistant-in-time sampled
trajectory data.
Learning models from data is an active research topic in the ﬁeld of sc ientiﬁc machine learning. A promi-
nent approach is to ﬁt dynamical-system models to data via dynamic m ode decomposition and Koopman-
based methods [47, 41, 53, 30, 59, 12]. In another research direc tion, sparse representations of governing
equations are sought with tools from sparse regression and compr essive sensing [11, 45, 46, 43]. There is also
work on non-intrusive model reduction that learns coeﬃcients of lo w-dimensional representations from data
[24, 21, 22]. If frequency-domain or impulse-response data are av ailable, then data-driven modeling methods
from the systems and control community are often used, such as the Loewner approach [2, 31, 35, 6, 3, 20, 25],
vector ﬁtting [23, 17], and eigensystem realization [27, 29].
In terms of learning from noisy data, there is the work [52] that est ablishes probabilistic recovery guarantees
via compressive sensing of sparse systems. Noise-robust data-d riven discovery of governing equations is
considered in [60, 61] using sparse Bayesian regression. A strateg y is proposed to subsample the data
utilized in solving the regression problem with the goal of reducing the inﬂuence of noise on the learned
model. A signal-noise decomposition is pursued in [44] in which a neural n etwork is trained to discover
the underlying dynamics while simultaneously estimating the noise. In s ystem identiﬁcation, works such
as [33, 57, 13, 9, 49, 50] derive probabilistic error bounds for ofte ntimes linear models using tools from,
e.g., random matrix theory. The eﬀect of the presence of noise and perturbations in frequency-domain
data have also been studied in data-driven interpolatory model red uction and Loewner methods [7, 32, 19,
18]. However, except for the interpolatory model reduction meth ods, which require frequency-domain data,
no low-dimensional models are considered in these works. In contra st, our approach based on operator
inference and re-projection [38, 36] aims to learn low-dimensional m odels that are suited for solving outer-
loop applications such as design, control, and inverse problems. Ope rator inference can learn non-Markovian
low-dimensional models [54] and it is also a building block for other learnin g methods such as lift & learn
introduced in [40, 51], which comes with a sensitivity analysis with respe ct to deterministic perturbations
in data [39, Chapter 4.3]. In [55], probabilistic a posteriori error bounds for operator-inference models are
derived for linear models; however, the bounds only hold when data a re free of noise. In the following, we
exploit the bridge between data-driven modeling with operator infer ence and traditional model reduction
[1, 42, 8] to establish probabilistic guarantees for learning from nois y data and to inform in a principled way
which data samples to query from the high-dimensional system to re duce the eﬀect of noise on the MSE of
state predictions.
This manuscript is organized as follows. Section 2 discusses preliminar ies about learning low-dimensional
dynamical-system models from data via operator inference and re- projection. Section 3 describes the sam-
pling and inference problem for learning models from noisy trajector ies with the proposed approach. Then,
bounds are derived for the MSE of the inferred operators and of t he state predictions with respect to
projection-based reduced models from traditional model reduct ion. A design of experiments approach is
proposed in Section 4, which leads to active operator inference tha t selects data samples to reduce the eﬀect
of noise on the MSE of state predictions. Numerical results presen ted in Section 5 are in agreement with the
analysis: the results indicate that active operator inference learn s low-dimensional models with MSEs that
are orders of magnitude more accurate than with an uninformed de sign of experiments.
2. Preliminaries
We review operator inference [38] for learning low-dimensional mod els from data in Section 2.1. Section 2.2
describes operator inference with the re-projection data samplin g scheme [36] to recover projection-based
reduced models from data.
2
2.1. Learning low-dimensional dynamical-system models fr om data with operator
inference
Let x1, . . . , xK ∈ RN be states at time steps k = 1 , . . . , K that are obtained by exciting a dynamical system
xk+1 = f(xk, uk) , k = 0 , . . . , K − 1, (2.1)
at the inputs u0, . . . , uK− 1 ∈ Rp and initial condition x0 ∈ RN . Let further V ⊂ RN be a subspace of the
N-dimensional state space RN . The subspace V is spanned by the orthonormal columns of the basis matrix
V = [ v1, . . . , vn] ∈ RN× n. For example, the subspace V can be obtained via principal component analysis
applied to sampled state trajectories.
Operator inference introduced in [38] learns low-dimensional dynam ical-system models with polynomial
nonlinear terms that best ﬁt the temporal evolution of the state in the subspace V with respect to the Eu-
clidean norm in a least-squares sense. Operator inference ﬁrst pr ojects the high-dimensional states x0, . . . , xK
onto the subspace V to obtain the projected states ˘x0, . . . , ˘xK with ˘xk = V T xk ∈ Rn for k = 0 , . . . , K and
then solves the least-squares problem
min
ˆA1,..., ˆAℓ, ˆB
K− 1∑
k=0




∑ ℓ
j=1
ˆAj ˘xj
k + ˆBuk − ˘xk+1




2
2
, (2.2)
where ℓ ∈ N is the polynomial order, ˆB ∈ Rn× p, ˆAj ∈ Rn× nj with
nj =
(n + j − 1
j
)
, j = 1 , . . . , ℓ ,
and ˘xj
k is obtained for k = 0 , . . . , K by forming the Kronecker product j times ˘xk ⊗ · · · ⊗ ˘xk and retaining
only the factors whose components are unique up to permutation [3 8].
2.2. Recovering projection-based reduced models from data with operator inference
and re-projection
The re-projection data-sampling scheme introduced in [36] judicio usly excites the high-dimensional system
(2.1) to generate a re-projected trajectory ˘Y = [ ˘y1, . . . , ˘yK ] ∈ Rn× K . The following description follows the
version of re-projection described in [40]. Let ¯X = [ ¯x1, . . . , ¯xK ] be a matrix where each column contains
an N-dimensional vector. For example, in [36, 40], it is proposed to gener ate ¯X by ﬁrst querying the high-
dimensional system (2.1) at an initial condition and inputs to sample th e trajectory X = [ x1, . . . , xK ] and
then setting ¯X = X. Let now U = [ u1, . . . , uK ] be an input trajectory and let ˘X = [ ˘x1, . . . , ˘xK ] be the
projected trajectory obtained as ˘X = V T ¯X from ¯X. Re-projection then computes Y = [ y1, . . . , yK ] by
querying the high-dimensional system
yk = f(V ˘xk, uk) , k = 1 , . . . , K ,
to obtain ˘Y = V T Y . The re-projection scheme can be applied to black-box dynamical s ystems that can be
queried at arbitrary initial conditions in RN and inputs in Rp.
As shown in [36, 39], if the high-dimensional system (2.1) from which da ta are sampled has polynomial
form, i.e.,
f(x, u) =
ℓ∑
j=1
Aj xj + Bu , (2.3)
and if there are suﬃciently many data samples, then the solution of t he least-squares problem
min
ˆA1,..., ˆAℓ, ˆB
¯J( ˆA1, . . . , ˆAℓ, ˆB; ˘X, ˘Y , U) (2.4)
3
with objective
¯J( ˆA1, . . . , ˆAℓ, ˆB; ˘X, ˘Y , U) =
K∑
k=1




∑ ℓ
i=1
ˆAi ˘xj
k + ˆBuk − ˘yk




2
2
(2.5)
is unique and coincides with the projected operators
˜B = V T B ,
˜Aj = V T Aj Sj (V ⊗ · · · ⊗ V )Rj , j = 1 , . . . , ℓ,
(2.6)
where the matrices Sj ∈ RNj × Nj
and Rj ∈ Rnj × nj satisfy
zj = Sj (z ⊗ · · · ⊗ z), ˜z ⊗ · · · ⊗ ˜z = Rj ˜zj
for all z ∈ RN , ˜z ∈ Rn and j = 1 , . . . , ℓ and the Kronecker is applied j times. Notice that the re-projected
trajectory ˘Y enters in the objective in the least-squares problem (2.4), wherea s only the projected trajectory
˘X enters in problem (2.2).
In traditional model reduction, see, e.g., [1, 42, 8], the projected operators ˜A1, . . . , ˜Aℓ, ˜B are computed
directly by computing the matrix-matrix products in the projection step (2.6). Thus, such traditional model
reduction methods are intrusive in the sense that they require the high-dimensional operators A1, . . . , Aℓ, B
either in assembled form or implicitly via matrix-vector products.
3. Learning low-dimensional models from noisy data
This work investigates operator inference and re-projection for learning low-dimensional models of noisy
dynamical systems,
xk+1 = f(xk, uk) + ξk , k = 0 , . . . , K − 1 , (3.1)
where ξ0, . . . , ξK− 1 represent noise. The random vectors ξ0, . . . , ξK− 1 are independent and each noise vector
ξk ∼ N(0, σ2I), for k = 0 , . . . , K −1, is an N-dimensional Gaussian random vector with a diagonal covariance
matrix and standard deviation σ > 0 in all directions. In the following, for ease of exposition, the noisy
high-dimensional system (3.1) can be queried at any initial condition in RN with any input in Rp; however,
the space of initial conditions and inputs can be restricted to subse ts of RN and Rp if necessary.
Section 3.1 applies operator inference and re-projection to learn lo w-dimensional models from noisy trajec-
tories and derives conditions under which the inferred operators a re unbiased estimators of the projection-
based reduced operators. The MSE of the learned low-dimensional operators is quantiﬁed in terms of the
noise-to-signal ratio. In Section 3.2, we derive bounds on the bias a nd the MSE of the predicted states
of the system described by the learned low-dimensional model with t he learned operators for linear and
polynomially nonlinear dynamics, respectively. The bounds scale with r espect to the noise-to-signal ratio.
3.1. Operator inference with re-projection with noisy stat e trajectories
Let ¯X be a matrix with N-dimensional columns (cf. Section 2.2) and let U = [ u1, . . . , uK ] be an input
trajectory. Note that ¯X can also be a realization, a deterministic trajectory, generated by simulating (3.1).
We then apply re-projection to obtain Z = [ z1, . . . , zK ] by querying the noisy high-dimensional system (3.1)
as
zk = f(V ˘xk, uk) + ξk , k = 1 , . . . , K (3.2)
where the columns of Ξ = [ ξ1, . . . , ξK ] are independent random noise vectors deﬁned above and ˘X =
[˘x1, . . . , ˘xK ] = V T ¯X is the projection of ¯X. The noisy re-projected state trajectory is ˘Z = V T Z =
[˘z1, . . . , ˘zK ].
4
projected
trajectory
˘x1 ˘x2 ˘x3 · · · ˘xK
noisy
re-projected
trajectory
˘z1 ˘z2 ˘z3 · · · ˘zK
query
(3.1)
query
(3.1)
query
(3.1)
query
(3.1)
Figure 1.: Applying re-projection to query the noisy high-d imensional system (3.1) leads to unbiased estimators of
the projected operators (2.6), which are the very same opera tors that are obtained with classical, intrusive
model reduction.
The corresponding operator-inference problem is
min
ˆA1,..., ˆAℓ, ˆB
J( ˆA1, . . . , ˆAℓ, ˆB; ˘X, ˘Z, U) (3.3)
where the noisy re-projected trajectory ˘Z enters in the objective (2.5). To analyze the solution of (3.3), it
is beneﬁcial to write (3.3) in matrix form as
min
O
∥DO − ˘Z
T
∥2
F , (3.4)
where the data matrix is D = [ ˘X
T
, ( ˘X
2
)T , . . . , ( ˘X
ℓ
)T , UT ] with ˘X
i
= [ ˘xi
1, . . . , ˘xi
K ] for i = 2 , . . . , ℓ . The
operators ˆA1, . . . , ˆAℓ, ˆB that we seek are submatrices of O = [ ˆA1, . . . , ˆAℓ, ˆB]T . The size of the data matrix
D is K × M with M = p + ∑ ℓ
j=1 nj . Correspondingly, the size of O is M × n.
We now characterize the solution of (3.4) with respect to the noise t hat is added during the re-projection
step. Recall that the procedure to generate ˘Z is to query the noisy high-dimensional system (3.1) at the
columns of the projected trajectory ˘X, which is deterministic because ¯X is deterministic. Thus, the data
matrix D in the regression problem (3.4) is deterministic while the noisy re-proj ected trajectory ˘Z is a
random matrix.
Following standard results of least-squares regression, the follow ing proposition summarizes that operator
inference together with re-projection leads to an unbiased estima tor of the projected operators (2.6) whose
variance grows linearly with the variance of the noise. Additionally, th e upper bound of the MSE of the
estimator is controlled by the noise-to-signal ratio σ/smin(D), where smin(·) is the minimum singular value
of the matrix argument.
Proposition 1. If K ≥ M and D is full rank, then the solution of problem (3.4) is
ˆO = [ ˆA1, . . . , ˆAℓ, ˆB]T = ˜O + (DT D)− 1DT (V T Ξ )T ,
where ˜O = [ ˜A1, . . . , ˜Aℓ, ˜B]T ∈ RM× n. In particular, the inferred operators are unbiased estima tors of the
projection-based reduced operators in the sense that E[ ˆAj ] = ˜Aj for j = 1 , . . . , ℓ and E[ ˆB] = ˜B. The
columns ˆo1, . . . , ˆon of ˆO are independent random vectors that are distributed as ˆoi ∼ N(˜oi, σ2(DT D)− 1) for
i = 1 , . . . , n where ˜o1, . . . , ˜on ∈ RM are the columns of ˜O. In addition, the MSE is bounded as
E[∥ ˆO − ˜O∥2
F ] ≤ nM
( σ
smin(D)
) 2
. (3.5)
5
Proof. The following are standard arguments from least-squares regres sion: because the data matrix D is
full rank and K ≥ M, the solution of (3.4) is given by the normal equations
ˆO = ( DT D)− 1DT ˘Z
T
= ˜O + (DT D)− 1DT ˘Ξ
T
,
where ˘Ξ = V T Ξ . Since the random vectors ξ1, . . . , ξK have zero mean, the expectation of ˆO is Eξ[ ˆO] = ˜O.
Additionally, since V T V = I is the identity matrix, the entries of ˘Ξ are iid N(0, σ2) random variables which
means that the columns of ˘Ξ
T
are independent Gaussian random vectors of dimension K with an identity
covariance matrix scaled by σ2. Thus, the columns of ˆO are Gaussian with covariance σ2(DT D)− 1, which
leads to the MSE
E[∥ ˆO − ˜O∥2
F ] =
n∑
i=1
M∑
j=1
Var[eT
j ˆoi] = n tr((DT D)− 1)σ2 ≤ Mn
( σ
smin(D)
) 2
,
where e1, . . . , eM are the canonical basis vectors of RM . The ﬁrst equality follows from the unbiasedness of
ˆO.
The independence of the columns of the random matrix ˆO leads to the independence of the rows of each
of the random matrices ˆB and ˆAj for j = 1 , . . . , ℓ . However, since the covariance matrix σ2(DT D)− 1 of
ˆoT
i is not necessarily block diagonal, the random matrices ˆB and ˆAj for j = 1 , . . . , ℓ are not necessarily
independent.
In [39, Chapter 4.3], a sensitivity analysis of lift & learn is presented th at just as well applies to operator
inference. The analysis leads to bounds with similar right-hand sides a s our bound (3.5) on the MSE;
however, the analysis in [39] is restricted to deterministic perturba tions and no bounds of the error in the
state predictions (as in Section 3.2) are presented.
3.2. Error of predicted states with respect to noise-to-sig nal ratio
We now consider the random states ˆx1, . . . , ˆxK predicted by the system described by the learned model
ˆxk+1 =
ℓ∑
j=1
ˆAj ˆxj
k + ˆBuk, k = 0 , . . . , K − 1 (3.6)
with a deterministic initial state ˆx0 ∈ Rn, which potentially is diﬀerent from the training initial conditions
used to generate the re-projected trajectory. Since the oper ators ˆB, ˆAj , j = 1 , . . . , ℓ are random matrices, ˆxk
is a random vector for k ≥ 1. In the following, we bound the bias which is the expectation of the d iﬀerence
between the states ˆx1, . . . , ˆxK and the deterministic states ˜x1, . . . , ˜xK of the reduced model from intrusive
model reduction
˜xk+1 =
ℓ∑
j=1
˜Aj ˜xj
k + ˜Buk, k = 0 , . . . , K − 1, (3.7)
with the operators ˜B, ˜Aj , j = 1 , . . . , ℓ deﬁned in (2.6). Bounds for the MSE between the random states
ˆx1, . . . , ˆxK and the deterministic states ˜x1, . . . , ˜xK are also deduced.
3.2.1. Technical preliminaries
It will be useful to account for the diﬀerence between the inferre d operators and the operators from intrusive
model reduction. Let E ˆAj
, E ˆB be n × nj and n × p random matrices, respectively, such that
ˆAj = ˜Aj + E ˆAj
, j = 1 , . . . , ℓ, and ˆB = ˜B + E ˆB.
6
The distribution of the rows of E ˆAj
, E ˆB can be described as follows. Deﬁne the selection matrices P Aj ∈
Rnj × M for j = 1 , . . . , ℓ and P B ∈ Rp× M which satisfy
P Aj
ˆO = ˆA
T
j and P B ˆO = ˆB
T
.
For i = 1 , . . . , n , the i-th row of E ˆAj
and E ˆB are zero-mean multivariate Gaussian random vectors
with covariance matrices σ2Σ ˆAj
and σ2Σ ˆB, respectively, where Σ ˆAj
= P Aj (DT D)− 1P T
Aj and Σ ˆB =
P B(DT D)− 1P T
B. Observe that
∥Σ 1/2
ˆB ∥2 = ∥P B(DT D)− 1P T
B∥1/2
2 ≤ ∥ (DT D)− 1∥1/2
2 =
√
smax((DT D)− 1) = 1
smin(D) (3.8)
where smax(·) is the largest singular value of the matrix argument. Analogously, w e have
∥Σ 1/2
ˆAj
∥2 ≤ 1
smin(D), j = 1 , . . . , ℓ. (3.9)
The following is a technical lemma derived from [56, Theorem 5.32 and Pr oposition 5.34] that provides an
upper bound for the expected value of the powers of the norm of a Gaussian random matrix, which will be
utilized in the calculations below; cf. Appendix B for the proof.
Lemma 2 (see, e.g., Theorem 5.32 and Proposition 5.34 in [56]) . Let G be an n × p random matrix whose
entries are independent standard normal random variables. For l ∈ N,
E[∥G∥l
2] ≤ (√
n + √p + 21/l√
l)l. (3.10)
3.2.2. Error in states for linear systems
In this section, we consider only systems with ℓ = 1 and therefore drop the subscript in A1, ˜A1, ˆA1. The
operator-inference model is ˆxk+1 = ˆAˆxk + ˆBuk and the model from intrusive model reduction is ˜xk+1 =
˜A˜xk + ˜Buk.
Proposition 3. Let ˆx0 = ˜x0. Suppose that the conditions of Proposition 1 hold. If the high-dimensional sys-
tem (3.1) from which data are sampled and the learned low-dimensional model have linear state dependence,
for k ∈ N with k ≥ 1, the bias of the state predictions is bounded as
∥E[ˆxk − ˜xk]∥2 ≤
k∑
l=2
Cl
( σ
smin(D)
) l
, (3.11)
where 0 < C 2, . . . , C k are constants that are not functions of σ and smin(D). The constants are
Cl = (2 √n + 21/l√
l)l
[ (k
l
)
∥ ˜A∥k− l
2 ∥˜x0∥2 +
k− 1∑
i=l
(i
l
)
∥ ˜A∥i− l
2 ∥ ˜Buk− 1− i∥2
]
+
k− 1∑
i=l− 1
( i
l − 1
)
∥ ˜A∥i− l+1
2 ∥uk− 1− i∥2
(
2√n + 2
1
2(i− l+1)
√
2(i − l + 1)
) i− l+1
(√n + √p + 2), (3.12)
for l = 2 , . . . , k .
Proof. Deﬁne the n × n random matrix G ˆA as G ˆA = 1
σ Σ − 1/2
ˆA ET
ˆA and the p × n random matrix G ˆB as
1
σ Σ − 1/2
ˆB ET
ˆB . Observe that the entries of G ˆA, G ˆB are independent standard random variables. At time step
k, the solution to the reduced system using the inferred operators is
ˆxk = ˆA
k
˜x0 +
k− 1∑
i=0
ˆA
i ˆBuk− 1− i. (3.13)
7
We now introduce the following notation: Let M, N be square matrices of the same size. For m, i ∈
N, denote by ρ1(M, N; i, m − i), . . . , ρ (m
i )(M, N; i, m − i) all the
(m
i
)
possible matrix products with i
multiplications of M and m − i multiplications of N. For example, if i = 1 , m = 3 then ρ1(M, N; 1, 2) =
MN 2, ρ2(M, N; 1, 2) = NMN , and ρ3(M, N; 1, 2) = N2M.
We then have with ˆA = ˜A + E ˆA that
ˆA
k
=
k∑
l=0
(
k
l )∑
j=1
ρj ( ˜A, E ˆA; k − l, l),
which we substitute into (3.13) at time step k, to obtain
ˆxk =
k∑
l=0
(
k
l )∑
j=1
ρj ( ˜A, E ˆA; k − l, l)˜x0 +
k− 1∑
i=0
i∑
l=0
(
i
l)∑
j=1
ρj ( ˜A, E ˆA; i − l, l) ˆBuk− 1− i
=
k∑
l=0
(k
l )∑
j=1
ρj ( ˜A, E ˆA; k − l, l)˜x0 +
k− 1∑
l=0
k− 1∑
i=l
(i
l)∑
j=1
ρj ( ˜A, E ˆA; i − l, l) ˆBuk− 1− i
=
k∑
l=0
(k
l )∑
j=1
ρj ( ˜A, E ˆA; k − l, l)˜x0 +
k− 1∑
l=0
k− 1∑
i=l
(i
l)∑
j=1
ρj ( ˜A, E ˆA; i − l, l) ˜Buk− 1− i (3.14)
+
k− 1∑
l=0
k− 1∑
i=l
(
i
l)∑
j=1
ρj( ˜A, E ˆA; i − l, l)E ˆBuk− 1− i
where in the second equality, we interchanged the order of the sum mation for the second term in the sum
and in the third equality, we used ˆB = ˜B + E ˆB. Notice that for the state obtained with intrusive model
reduction we have
˜xk =
(
k
0)∑
j=1
ρj ( ˜A, E ˆA; k, 0)˜x0 +
k− 1∑
i=0
(
i
0)∑
j=1
ρj ( ˜A, E ˆA; i, 0) ˜Buk− 1− i (3.15)
which corresponds to the ﬁrst 2 terms of (3.14) but with l = 0 ﬁxed. Thus, (3.15) consists of all terms in
(3.14) where the random matrices E ˆA, E ˆB are absent. Hence,
ˆxk − ˜xk =
k∑
l=1
(k
l )∑
j=1
ρj ( ˜A, E ˆA; k − l, l)˜x0 +
k− 1∑
l=1
k− 1∑
i=l
(i
l)∑
j=1
ρj ( ˜A, E ˆA; i − l, l) ˜Buk− 1− i
+
k− 1∑
l=0
k− 1∑
i=l
(i
l)∑
j=1
ρj ( ˜A, E ˆA; i − l, l)E ˆBuk− 1− i.
Additionally, when l = 1, the terms E[ρj ( ˜A, E ˆA; k − l, l)] and E[ρj ( ˜A, E ˆA; i − l, l)] are zero because E ˆA has
zero mean. Similarly, for l = 0, the terms E[ρj ( ˜A, E ˆA; i, 0)E ˆB ] are zero. This means that
∥E[ˆxk − ˜xk]∥2 ≤ τ1 + τ2 + τ3 (3.16)
8
where
τ1 =
k∑
l=2
(
k
l )∑
j=1
∥E[ρj ( ˜A, E ˆA; k − l, l)˜x0]∥2,
τ2 =
k− 1∑
l=2
k− 1∑
i=l
(i
l)∑
j=1
∥E[ρj ( ˜A, E ˆA; i − l, l) ˜Buk− 1− i]∥2,
τ3 =
k− 1∑
l=1
k− 1∑
i=l
(
i
l)∑
j=1
∥E[ρj ( ˜A, E ˆA; i − l, l)E ˆBuk− 1− i]∥2.
It remains to bound each of τ1, τ2, τ3. Since
∥E[ρj ( ˜A, E ˆA; k − l, l)˜x0]∥2 = ∥E[ρj ( ˜A, σGT
ˆAΣ 1/2
ˆA ; k − l, l)]˜x0∥2
≤ σl∥Σ 1/2
ˆA ∥l
2∥ ˜A∥k− l
2 ∥˜x0∥2E[∥G ˆA∥l
2]
≤
( σ
smin(D)
) l
∥ ˜A∥k− l
2 ∥˜x0∥2E[∥G ˆA∥l
2],
we obtain
τ1 ≤
k∑
l=2
(k
l
) ( σ
smin(D)
) l
∥ ˜A∥k− l
2 ∥˜x0∥2E[∥G ˆA|∥l
2]
≤
k∑
l=2
(k
l
) ( σ
smin(D)
) l
∥ ˜A∥k− l
2 ∥˜x0∥2(2√n + 21/l√
l)l
by applying Lemma 2. Likewise,
τ2 ≤
k− 1∑
l=2
k− 1∑
i=l
(i
l
) ( σ
smin(D)
) l
∥ ˜A∥i− l
2 ∥ ˜Buk− 1− i∥2E[∥G ˆA|∥l
2]
≤
k− 1∑
l=2
k− 1∑
i=l
(i
l
) ( σ
smin(D)
) l
∥ ˜A∥i− l
2 ∥ ˜Buk− 1− i∥2(2√n + 21/l√
l)l.
Finally,
∥E[ρj ( ˜A, E ˆA; i − l, l)E ˆBuk− 1− i]∥2 ≤ E[∥ρj ( ˜A, σGT
ˆAΣ 1/2
ˆA ; i − l, l)σGT
ˆB Σ 1/2
ˆB uk− 1− i∥2]
≤ σl+1∥Σ 1/2
ˆA ∥l
2∥Σ 1/2
ˆB ∥2∥ ˜A∥i− l
2 ∥uk− 1− i∥2E[∥G ˆA∥i− l
2 ∥G ˆB∥2]
≤
( σ
smin(D)
) l+1
∥ ˜A∥i− l
2 ∥uk− 1− i∥2E[∥G ˆA∥i− l
2 ∥G ˆB∥2]
so that
τ3 ≤
k∑
l=2
k− 1∑
i=l− 1
( i
l − 1
) ( σ
smin(D)
) l
∥ ˜A∥i− l+1
2 ∥uk− 1− i∥2
(
2√n + 2
1
2(i− l+1)
√
2(i − l + 1)
) i− l+1
(√n + √p + 2) (3.17)
9
because the Cauchy-Schwarz inequality and Lemma 2 lead to
⏐
⏐E[∥G ˆA∥i− l+1
2 ∥G ˆB ∥2]
⏐
⏐≤
√
E[∥G ˆA∥2(i− l+1)
2 ]E[∥G ˆB∥2
2]
≤
(
2√
n + 2
1
2(i− l+1)
√
2(i − l + 1)
) i− l+1
(√n + √p + 2).
The result follows by combining the upper bounds for τ1, τ2, τ3.
Corollary 4. Let ˆx0 = ˜x0. If ℓ = 1 and high-dimensional system (3.1) from which data are sampled is
autonomous, then
∥E[ˆxk − ˜xk]∥2 ≤
k∑
l=2
(k
l
) ( σ
smin(D)
) l
(2√n + 21/l√
l)l∥ ˜A∥k− l
2 ∥˜x0∥2, (3.18)
for k ∈ N with k ≥ 1.
Proof. The proof follows that of Proposition 3 noting that ˆB, ˜B and hence E ˆB are zero matrices.
Several remarks are in order. As the time step k increases, i.e., as we move forward in time, the bound
(3.11) also increases, which is expected because the bias of the sta te estimators of previous time steps is
accumulated. Notice that the bound also depends on n, the dimension of the reduced space, and on p, the
dimension of the input. The bound (3.11) further suggests that if t he noise-to-signal ratio σ/smin(D) < 1,
the term associated with ( σ/smin(D))2 at time step k = 2 dominates the upper bound as σ/smin(D) → 0.
Hence, we expect that for σ/smin(D) suﬃciently small, an order of magnitude decrease in the noise-to-s ignal
ratio yields at least a decrease of 2 orders of magnitude in the bias of the predicted states.
3.2.3. Re-sampling operators for unbiased state predictio ns
We now devise a strategy to sample (3.6) for ℓ = 1 that guarantees that predicted states are unbiased, i.e.,
E[ˆxk] = ˜xk for all k ∈ N.
Proposition 5. Suppose that the conditions of Proposition 1 hold. For k = 1 , . . . , K , let ˆA
(1)
, . . . , ˆA
(K)
be
independent samples of ˆA such that ˆA
(1)
, . . . , ˆA
(K)
, ˆB are mutually independent. Let the high-dimensional
system (3.1) from which data are sampled and the learned low-dimensional model be linear. If the reduced
state ˆxk is computed by integrating the time varying dynamical-syst em model
ˆxk+1 = ˆA
(k+1)
ˆxk + ˆBuk
for k = 0 , . . . , K −1, then if ˆx0 = ˜x0, the predicted states are unbiased in the sense E[ˆxk] = ˜xk, k = 0 , . . . , K .
Proof. From the given assumptions, the operators ˆA
(1)
, . . . , ˆA
(K)
, ˆB are unbiased following Proposition 1.
We now proceed via induction. When k = 0, since ˆx0 = ˜x0, ˆx1 = ˆA
(1)
˜x0 + ˆBu0. Therefore, E[ˆx1] =
E[ ˆA
(1)
]˜x0 + E[ ˆB]u0 = ˜A˜x0 + ˜Bu0 = ˜x1. For a time step j ∈ { 1, . . . , K − 1}, suppose that E[ˆxk] = ˜xk
for k = 0 , . . . , j − 1. Observe that ˆxj− 1 is a function of ˆA
(j− 1)
, . . . , ˆA
(1)
, ˆB, ˜x0. Independence of ˆA
(j)
to
ˆA
(j− 1)
, . . . , ˆA
(1)
, ˆB implies that ˆA
(j)
and ˆxj− 1 are independent as well. Therefore,
E[ˆxj ] = E[ ˆA
(j− 1)
ˆxj− 1 + ˆBuj− 1] = E[ ˆA
(j− 1)
]E[ˆxj− 1] + E[ ˆB]uj− 1
= ˜A˜xj− 1 + ˜Buj− 1 = ˜xj .
10
The random matrices ˆA
(1)
, . . . , ˆA
(K)
, ˆB, which satisfy the conditions stated in Proposition 5, can be
generated by solving (3.4) K + 1 times, each time with a new, mutually independent, re-projected noisy
state trajectories. Generating ˆA
(1)
, . . . , ˆA
(K)
can be computationally expensive because the high-dimensional
system needs to be queried for a potentially large number of trajec tories.
3.2.4. Error of state predictions with polynomially nonlin ear systems
We now derive bounds for the bias ∥E[ˆxk − ˜xk]∥2 and the MSE E[∥ˆxk − ˜xk∥2
2] of state predictions where
data are sampled from polynomially nonlinear systems.
We start by writing the state ˆxk at time step k (which only involves the initial condition and previous
inputs) as a sum of vectors, each of which is formed as a combination of matrix and Kronecker products.
Lemma 6. The state ˆxk at time step k, k ∈ N, of the polynomially nonlinear model (3.6) is
ˆxk =
Qk∑
l=0
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB), (3.19)
where Qk = ( ℓk − 1)/(ℓ − 1) and ζj1,...,jℓ+1 is a sum of vectors in Rn, where each term is a combination
of matrix and Kronecker products involving the determinist ic quantities ˆx0, u0, . . . , uk− 1, ˜A1, . . . , ˜Aℓ, ˜B and
the random matrices E ˆA1
, . . . , E ˆAℓ
, E ˆB. Each term in the sum ζj1,...,jℓ+1 consists of jl multiplications of
E ˆAl
for l = 1 , . . . , ℓ and jℓ+1 multiplications of E ˆB.
Proof. We recast the system (3.6) as
ˆxk+1 =
ℓ∑
j=1
ˆAj Sj (ˆxk ⊗ · · · ⊗ ˆxk) + ˆBuk, (3.20)
where Sj ∈ Rnj × nj
is a selection matrix such that Sj (ˆxk ⊗ · · · ⊗ ˆxk) = ˆxj
k for j = 1 , . . . , ℓ . Thus, the
state ˆxk+1 at time step k + 1 is the result of recursively applying (3.20) until the right-hand sid e contains
only the initial condition ˆx0 and the inputs u0, . . . , uk− 1. Since ˆAj = ˜Aj + E ˆAj
for j = 1 , . . . , ℓ and
ˆB = ˜B + E ˆB, the right-hand side is a sum of combinations of matrix and Kronecke r products involving the
deterministic vectors ˆx0, u0, . . . , uk− 1, the deterministic matrices ˜A1, . . . , ˜Aℓ, ˜B, S1, . . . , Sℓ and the random
matrices E ˆA1
, . . . , E ˆAℓ
, E ˆB . The right-hand side can then be ordered with respect to the numb er of times
there is a multiplication with a random matrix which is represented by th e outer sum in (3.19). The outer
sum is further partitioned according to how often there is a multiplica tion involving E ˆA1
, . . . , E ˆAℓ
, E ˆB with
corresponding frequencies of multiplications j1, . . . , j ℓ+1 times. The frequencies j1, . . . , j ℓ+1 serve as indices
of the inner sum (3.19).
It remains to show that the state at time step k is obtained using at most Qk multiplications with a
random matrix. We proceed via induction. When k = 1,
ˆx1 =
ℓ∑
j=1
˜Aj Sj (ˆx0 ⊗ · · · ⊗ ˆx0) + ˜Bu0 +
ℓ∑
j=1
E ˆA1
Sj (ˆx0 ⊗ · · · ⊗ ˆx0) + E ˆB u0,
thereby implying that there is at most one ( Q1 = 1) random-matrix multiplication to obtain ˆx1. Suppose
that at time step k = m, obtaining the state ˆxm requires at most Qm random-matrix multiplications. At
time step k = m+1, the maximum number of random-matrix multiplications is determine d by the expression
E ˆAℓ
Sℓ(ˆxm ⊗ · · · ⊗ ˆxm). From the induction step, ˆxm has at most Qm = ( ℓm − 1)/(ℓ − 1) random matrix
multiplications which means that ˆxm+1 has at most Qmℓ + 1 = ( ℓm+1 − 1)/(ℓ − 1) = Qm+1 random matrix
multiplications due to the ℓ Kronecker products of ˆxm and the random matrix E ˆAℓ
.
11
The following proposition shows that the bound for the bias ∥E[ˆxk − ˜xk]∥2 of state predictions is still poly-
nomial in terms of the noise-to-signal ratio even when data are sam pled from polynomially nonlinear systems
and polynomially nonlinear models are learned. In particular, when σ/smin(D) < 1 and σ/smin(D) → 0,
the behavior of the upper bound is dominated by the term associate d with ( σ/smin(D))2.
Proposition 7. Let ˆx0 = ˜x0. Suppose that the conditions of Proposition 1 hold. If the high-dimensional
system (3.1) from which data are sampled and the learned low-dimensional model are polynomially nonlinear,
for k ∈ N with k ≥ 1, it holds that
∥E[ˆxk − ˜xk]∥2 ≤
Qk∑
l=2
¯Cl
( σ
smin(D)
) l
for some constants 0 < ¯Cl < ∞, l = 2 , . . . , Q k, which are not functions of σ and D.
Proof. Deﬁne the nj × n random matrix G ˆAj
as G ˆAj
= 1
σ Σ − 1/2
ˆAj
ET
ˆAj
for j = 1 , . . . , ℓ and the p × n random
matrix G ˆB as G ˆB = 1
σ Σ − 1/2
ˆB ET
ˆB. Observe that the entries of G ˆAj
for j = 1 , . . . , ℓ and G ˆB are independent
standard normal random variables, however, in general, the rand om matrices are dependent due to the
dependence between ˆB, ˆAj , j = 1 , . . . , ℓ . According to Lemma 6, the state ˆxk at time step k is
ˆxk =
Qk∑
l=0
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB). (3.21)
Since the system is polynomially nonlinear, the state obtained with intr usive model reduction,
˜xk = ζ0,...,0(E ˆA1
, . . . , E ˆAℓ
, E ˆB) (3.22)
is comprised of those terms for which no random matrix is present in t he multiplications ( l = 0). We now
isolate the terms in (3.21) in which a single random matrix is involved in the multiplication. By linearity of
expectation and using that the random matrices E ˆB, E ˆAs
, s = 1 , . . . , ℓ have zero mean,
E




∑
j1,...,jℓ+1
j1+···+jℓ+1=1
ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB )



 = 0.
Therefore, with the triangle and Jensen’s inequality follows
∥E[ˆxk − ˜xk]∥2 ≤
Qk∑
l=2
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
E
[
∥ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB)∥2
]
. (3.23)
It remains to bound E
[
∥ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB )∥2
]
. We use that for matrices A, B of appropriate
dimensions, ∥AB∥2 ≤ ∥ A∥2∥B∥2 and that ∥A⊗B∥2 = ∥A∥2∥B∥2. Note also that ∥Sj ∥2 = 1 for j = 1 , . . . , ℓ .
Recall that ˜x0,u0, . . . , uk− 1, ˜A1, . . . , ˜Aℓ, ˜B, S1, . . . , Sℓ are deterministic quantities with ﬁnite norm. We
thus have, for some ﬁnite constant ¯C(j1, . . . , j ℓ+1) > 0, the bound
E
[
∥ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB)∥2
]
(3.24)
≤ ¯C(j1, . . . , j ℓ+1)E[∥E ˆA1
∥j1
2 · · · ∥ E ˆAℓ
∥jℓ
2 ∥E ˆB∥jℓ+1
2 ]
≤ ¯C(j1, . . . , j ℓ+1)σj1 +···+jℓ+1 ∥Σ 1/2
ˆA1
∥j1
2 · · · ∥Σ 1/2
ˆAℓ
∥jℓ
2 ∥Σ 1/2
ˆB ∥jℓ+1
2 E[∥G ˆA1
∥j1
2 · · · ∥ G ˆAℓ
∥jℓ
2 ∥G ˆB∥jℓ+1
2 ]
≤ ¯C(j1, . . . , j ℓ+1)
( σ
smin(D)
) j1+···+jℓ+1
E[∥G ˆA1
∥j1
2 · · · ∥ G ˆAℓ
∥jℓ
2 ∥G ˆB∥jℓ+1
2 ]
12
where we utilized (3.8), (3.9).
Recursively applying the Cauchy-Schwarz inequality and invoking con centration inequalities on ∥G ˆB∥2, ∥G ˆAs
∥2, s =
1, . . . , ℓ shows that E[∥G ˆA1
∥j1
2 · · · ∥G ˆAℓ
∥jℓ
2 ∥G ˆB∥jℓ+1
2 ] is ﬁnite. To illustrate this, consider
⏐
⏐
⏐
⏐E[∥G ˆA1
∥j1
2 · · · ∥ G ˆAℓ
∥jℓ
2 ∥G ˆB∥jℓ+1
2 ]
⏐
⏐
⏐
⏐
≤
√
E[∥G ˆA1
∥2j1
2 ]E[∥G ˆA2
∥2j2
2 · · · ∥G ˆAℓ
∥2jℓ
2 ∥G ˆB ∥2jℓ+1
2 ]. (3.25)
By invoking Lemma 2, we can obtain a bound for E[∥G ˆA1
∥2j1
2 ]. The Cauchy-Schwarz inequality is then
applied to E[∥G ˆA2
∥2j2
2 · · · ∥ G ˆAℓ
∥2jℓ
2 ∥G ˆB∥2jℓ+1
2 ] after which concentration inequalities are invoked to bound
E[∥G ˆA2
∥4j2
2 ], which is repeated ℓ times until the expected value of products is decomposed into a pro duct of
expected values. The proposition then follows from (3.24) by summin g over the indices for which j1 + · · · +
jℓ+1 = l with l = 2 , . . . , Q k.
We now derive a bound for the MSE of the predicted states, which sh ows that for polynomially nonlinear
systems, the MSE in the asymptotic regime σ/smin(D) → 0 is dominated by ( σ/smin(D))2.
Proposition 8. Let ˆx0 = ˜x0. Suppose that the conditions of Proposition 1 hold. If the high-dimensional
system (3.1) from which data are sampled and the learned low-dimensional model are polynomially nonlinear,
then
E[∥ˆxk − ˜xk∥2
2] ≤
2Qk∑
l=2
ˆCl
( σ
smin(D)
) l
, 1 ≤ k ∈ N,
for some constants 0 < ˆCl < ∞, l = 2 , . . . , 2Qk, which are not functions of σ and D.
Proof. From (3.21) and (3.22), we obtain
ˆxk − ˜xk =
Qk∑
l=1
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
ζj1 ,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB).
Thus, for some ﬁnite constant ˆC(j1, . . . , j ℓ+1) > 0,
∥ˆxk − ˜xk∥2
≤
Qk∑
l=1
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
∥ζj1,...,jℓ+1 (E ˆA1
, . . . , E ˆAℓ
, E ˆB)∥2
≤
Qk∑
l=1
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
ˆC(j1, . . . , j ℓ+1)∥E ˆA1
∥j1
2 · · · ∥E ˆAℓ
∥jℓ
2 ∥E ˆB∥jℓ+1
2
≤
Qk∑
l=1
∑
j1,...,jℓ+1∈ N
j1+···+jℓ+1=l
ˆC(j1, . . . , j ℓ+1)
( σ
smin(D)
) j1+···+jℓ+1
∥G ˆA1
∥j1
2 · · · ∥ G ˆAℓ
∥jℓ
2 ∥G ˆB∥jℓ+1
2
following calculations in (3.24) where G ˆA1
, . . . , G ˆAℓ
, G ˆB are the same random matrices deﬁned in the proof
of Proposition 7. Note that in the above inequality, the powers of th e noise-to-signal ratio range from
j1 + · · · + jℓ+1 = 1 to j1 + · · · + jℓ+1 = Qk, whereas in the proof of Proposition 7 the inequality (3.23)
starts at j1 + · · · + jℓ+1 = 2. The conclusion now follows by squaring both sides of the inequality , applying
expectation, and performing calculations similar to (3.25) to show th at the resulting constants are ﬁnite.
13
4. Active operator inference for selecting training data
This section proposes active operator inference, which selects fr om a dictionary at which initial condition
and inputs to sample the high-dimensional system for generating da ta with low noise-to-signal ratios. The
proposed active operator inference is motivated by the bounds de rived in Section 3.2, which show that the
noise-to-signal ratio σ/smin(D) controls the MSE of the learned operators as well as the bias and t he MSE
of the state dynamics.
Section 4.1 formalizes the dictionary whose elements are candidates for sampling the high-dimensional
system at. The proposed selection of elements of the dictionary is d escribed in Section 4.2 and builds on
ideas from selecting points [37] in empirical interpolation [5, 14]. The c omputational procedure for active
operator inference is presented in Section 4.3, which summarizes th e proposed workﬂow for learning low-
dimensional models from noisy data.
4.1. Dictionary of candidate states and inputs
Consider a dictionary D ∈ RL× M of candidate states and inputs given by
D = [ ˘X
T
L, ( ˘X
2
L)T , . . . , ( ˘X
ℓ
L)T , UT
L ] ,
where ˘XL ∈ Rn× L, UL ∈ Rp× L are deﬁned identically as ˘X, U in Section 3.1 but with L states and inputs,
i.e. ˘XL = [ ˘x1, . . . , ˘xL] and UL = [ u1, . . . , uL]. Let P K ∈ { 0, 1}L× K be a selection operator that selects
K ≤ L rows of D via P T
K D = D so that D can serve as a data matrix in the sense of (3.4). Observe that
using all rows of D to construct the data matrix D in the least squares problem (3.4) is computationally
expensive as the high-dimensional system has to be queried for eac h of the L initial conditions and inputs.
4.2. A design of experiments approach via oversampled empir ical interpolation
Propositions 3, 7, and 8 demonstrate that a low noise-to-signal ra tio σ/smin(D) is desirable. Since the
standard deviation σ is ﬁxed, we propose a design of experiments strategy that forms a data matrix D by
selecting rows of D so that smin(D) is large. To ﬁnd a selection of rows, we follow the procedure propos ed
in [37] that pursues an equivalent objective for selecting points for empirical interpolation [5, 14, 16]; see
[4, 34, 15, 48] for other design of experiment approaches based o n similar linear-algebra concepts. Set
K ≥ M. The method introduced in [37] constructs a selection matrix P K which selects K rows of D with
the objective of maximizing smin(P T
K D). First, the selection matrix P M ∈ RM× M is initialized with the
approach introduced in [16]. Then, new rows of D are selected in a greedy fashion. To describe the greedy
update, suppose we have the selection matrix P m, which selects m rows of D, with M ≤ m < K . Let the
SVD of P T
mD be P T
mD = Φ mΣ mΨ T
m where Φ m ∈ Rm× M is the matrix of left-singular vectors, Σ m ∈ RM× M
is the diagonal matrix of singular values s(m)
1 , . . . , s (m)
M in descending order, and Ψ m ∈ RM× M is the matrix
of right-singular vectors. Deﬁne the gap g = ( s(m)
M− 1)2 − (s(m)
M )2 and set ¯d+ = Ψ T
mdT
+, where d+ ∈ R1× M is
a candidate row of D that has not been selected by P m. Further, let e ∈ RM be the canonical basis vector
with all entries 0 except for the last component that is set to 1. It is shown in [26], see also the discussion
in [37], that
smin(P T
m+1D)2 − smin(P T
mD)2 ≥ 1
2
(
g + ∥¯d+∥2
2 −
√
(g + ∥¯d+∥2
2)2 − 4g(eT ¯d+)2
)
(4.1)
which suggests that the new row d+ should be selected that maximizes the lower bound (4.1). This greedy
step is then repeated until the desired number of rows K is reached.
Based on the just described greedy scheme, we use a modiﬁed gree dy update rule, which was proposed in
an earlier preprint version of [37]: we choose the new row d+ that maximizes
(eT ¯d+)2 , (4.2)
14
Algorithm 1. Active operator inference based on QDEIM [16] and oversampling [37 ]
1: procedure AOpInf(D, K)
2: Initialize P M with QDEIM [16]
3: for m = M, . . . , K − 1 do ⊲ Follow [37] with criterion (4.2)
4: Compute the SVD of P T
mD = Φ mΣ mΨ T
m
5: Find the row d+ of D not in P T
mD such that ¯d+ = Ψ T
mdT
+ maximizes (4.2)
6: Update P m to P m+1
7: Construct D via P T
K D = D
8: Perform re-projection as in (3.2) to generate ˘Z
9: Solve the least-squares problem (3.4) to obtain ˆA1, . . . , ˆAℓ, ˆB
return ˆA1, . . . , ˆAℓ, ˆB
which is obtained by simplifying the lower bound (4.1).
Choosing the new row d+ by maximizing the lower bound (4.1) was tested in [37] for the case whe re the
columns of D are orthonormal. Since this condition does not necessarily hold in our setting, the lower bound
in (4.1) for the greedy update can lead to cancellation errors espec ially when 4 g(eT ¯d+)2 is small, which has
been ﬁrst observed in [58].
4.3. Active operator inference
The proposed active operator inference approach to learn low-dim ensional models from noisy data is sum-
marized in Algorithm 1. The inputs of the algorithm are the dictionary D and the number of times K to
query the high-dimensional system, i.e., the number of rows of the d ata matrix D. Line 2 of Algorithm 1
initializes the sampling matrix via QDEIM [16] by computing the QR decomp osition of DT with pivoting.
For m ∈ { M, M + 1 , . . . , K − 1}, the SVD of P T
mD is obtained in line 4 and the candidate row d+ of D
that maximizes (4.2) is selected in in lines 5–6 to update P m to P m+1. In lines 7–9, re-projection (3.2) is
performed using the projected states and the inputs in the data m atrix D = P T
K D to obtain the re-projected
trajectory ˘Z. The least-squares problem (3.4) is then solved to learn the low-dime nsional operators.
5. Numerical experiments
We now numerically demonstrate that the proposed active operato r inference leads to predicted states with
orders of magnitude lower biases and MSEs than an uninformed equid istant-in-time selection of data samples.
Additionally, we demonstrate that the bias and MSE of predicted sta tes decay with the noise-to-signal ratio
in agreement with the analysis developed in Section 3.2. Numerical res ults for a linear state dynamics are
shown in Section 5.1 and for quadratic dynamics in Section 5.2. In all ex periments within one example, we
use the same basis matrix V , which ensures consistent comparisons among diﬀerent noise-to- signal ratios.
5.1. Heat transfer problem for cooling of steel proﬁles
The model and problem setup are described in Section 5.1.1 and the nu merical results are presented in
Section 5.1.2.
5.1.1. Model of cooling steel proﬁles
We describe a mathematical model for the cooling process of steel rail proﬁles in a rolling mill following [10].
Set Ω ⊂ R2 as the spatial domain and denote by x(η , t) the temperature at the spatial point η ∈ Ω and time
15
Figure 2.: Steel proﬁle domain Ω for the heat transfer proble m in Section 5.1.
t > 0. The heat transfer model is
∂x(η , t)
∂t = λ
cρ∆ x(η , t), (η , t) ∈ Ω × [0, T ], (5.1)
∇x(η , t) · n =
{
κ
λ (uj(t) − x(η , t)) for η ∈ Γ j , j = 1 , . . . , 7,
0 for η ∈ Γ 0,
x(η , 0) = 500 ,
where λ is the heat conductivity, c the speciﬁc heat capacity, ρ the proﬁle density, κ the heat transfer
coeﬃcient, Γ j , j = 0 , . . . , 7 are segments of the domain boundary ∂Ω such that ∂Ω = ∪7
j=0Γ j and uj(t), j =
1, . . . , 7 is the external temperature applied to each boundary segment. The domain is visualized in Figure 2.
The values of the constants are chosen as λ = 26 .4, c = 7620 , ρ = 654 , κ = 69 .696.
Equation (5.1) is spatially discretized using the ﬁnite element method w ith linear triangular elements and
temporally discretized with implicit Euler with step size δt = 0 .01 to yield the high-dimensional system
(3.1) with right-hand side function (2.3) with ℓ = 1, where xk ∈ RN , N = 1357 and uk ∈ Rp, p = 7. We
utilized the Python 1 code based on the FEniCS Project to generate the computational mesh and the system
matrices; see also [10].
The basis matrix V is computed from snapshots xbasis
k , k = 0 , . . . , L, L = 10000, of the high-dimensional
system driven by the input ubasis
k whose i-th component, i = 1 , . . . , 7 is given by 500(1 − tanh(kδt/i2)) +
250γi,k. Here, γi,0 = 0 while γi,k for k > 0 is a realization of a uniform random variable on [0 , 1]. The
projected states V T xbasis
k and the input ubasis
k for k = 0 , . . . , L −1 constitute the 10000 rows of the dictionary
D ∈ RL× (n+p) from which we select the rows of the data matrix D for operator inference. In the simulations
below, we consider 5 equally spaced values in the logarithm scale for th e standard deviation σ of the noise
between 1 × 10− 3 and 1 × 10− 1. The test input utest
k at time step k ∈ N has components given by 500(1 −
tanh(kδt/i2)) for i = 1 , . . . , 7.
5.1.2. Results
We learn a low-dimensional model of dimension n = 7 and n = 10 from noisy data. For n = 7, active operator
inference is applied to select 15 rows from D, which leads to a data matrix D with smin(D) = 1 .661. For
n = 10, 25 rows are selected resulting in smin(D) = 0 .8713. Denote by ˆxtest
k the predicted state at time step
k of the low-dimensional model with inferred operators ˆA, ˆB corresponding to the test input utest
k . Likewise,
1https://gitlab.mpi-magdeburg.mpg.de/models/fenicsrail/-/tree/master/
16
1e-06
1e-05
1e-04
1e-03
1e-02
1e-01
1e+00
1e+01
1e-03 1e-02
estimated bias of predicted states
noise-to-signal ratio
time step 5
time step 10
time step 20
time step 30
(a) dimension n = 7
1e-06
1e-05
1e-04
1e-03
1e-02
1e-01
1e+00
1e+01
1e+02
1e-02 1e-01
estimated bias of predicted states
noise-to-signal ratio
time step 5
time step 10
time step 20
time step 30
(b) dimension n = 10
Figure 3.: Cooling of steel proﬁles (Section 5.1). The estim ated bias decays by 2 orders of magnitude per 1 order
of magnitude decrease in the noise-to-signal ratio in the as ymptotic regime. The results are in agreement
with Proposition 3.
let ˜xtest
k be the low-dimensional state from intrusive model reduction for th e same input. Recall that ˜xtest
k
is deterministic while ˆxtest
k is a random vector.
Figure 3 shows a Monte Carlo estimate of the bias ∥E[ˆxtest
k − ˜xtest
k ]∥2 as a function of the noise-to-signal
ratio σ/smin(D) for various time steps k. A Monte Carlo estimate of the MSE E[∥ˆxtest
k − ˜xtest
k ∥2
2] is shown
in the left panel of Figure 6. We use 7 .5 × 107 samples to approximate the expected value with Monte
Carlo. The plots illustrate that in the asymptotic regime, when σ/smin(D) → 0, an order decrease in the
noise-to-signal ratio leads to a decrease of two orders of magnitu de in the approximation of the bias and the
MSE, which agrees with Propositions 3 and 8. Notice that for n = 10, the behavior of the bias and the MSE
for the largest noise value σ is already dominated by constants, rather than the noise-to-sign al ratio, which
explains the quicker error increase.
We now compare active operator inference, which carefully selects rows of the data matrix D to keep
the noise-to-signal ratio low, with a traditional sample selection tha t queries the high-dimensional system
equidistantly in time, i.e., picks columns corresponding to equidistant t imes from the dictionary D. Figure 4
compares the minimum singular value of the data matrix for both appr oaches over the number of queries
to the high-dimensional system. Equidistant sampling requires up to 3 times as many queries to the high-
dimensional system to achieve the same noise-to-signal ratio as ac tive operator inference in our experiment.
The estimate of the bias ∥E[ˆxtest
k − ˜xtest
k ]∥2 for σ = 1 × 10− 2 for the equidistant and active operator inference
approach is presented in Figure 5. The same comparison for the MSE E[∥ˆxtest
k − ˜xtest
k ∥2
2] is shown in the
right panel of Figure 6 for n = 10. The results show that active operator inference yields a redu ction in
the estimated bias and the MSE of up to 1.5 and 0.5 orders in magnitude , respectively. Lastly, we consider
the MSE of the predicted state further in time. The bottom panel o f Figure 6 plots the estimated MSE of
the predicted state at 10000 time steps for n = 10 using 10 Monte Carlo samples only. An order decay in
the noise standard deviation leads to 2 orders decay in the estimate d MSE. For ﬁxed σ, the model learned
through active operator inference achieves a smaller MSE.
In Figure 7 we visualize the 15 high-dimensional states correspondin g to the rows of D selected according
to the design of experiments schemes we compare for n = 7. The respective inputs are not shown. By
examining the segments of the steel proﬁle boundary with Robin con dition, the equidistant scheme tends to
select more states with lower temperature at the boundary, many of which correspond to later time steps.
In contrast, active operator inference selects more states at t he beginning of the cooling process.
17
0
0. 5
1
1. 5
2
2. 5
3
3. 5
4
12 14 16 18 20 22 24 26 28 30
reduction of # samples
minimum singular value
number of samples from high-dimensional system
Equidistant in time
Active OpInf
(a) dimension n = 7
0. 2
0. 4
0. 6
0. 8
1
1. 2
1. 4
1. 6
1. 8
2
20 30 40 50 60 70
reduction of # samples
minimum singular value
number of samples from high-dimensional system
Equidistant in time
Active OpInf
(b) dimension n = 10
Figure 4.: Cooling of steel proﬁles (Section 5.1). To achiev e the same noise-to-signal ratio, active operator infer-
ence requires almost 3 times fewer queries to the high-dimen sional system than a traditional selection of
equidistant-in-time samples.
1e-04
1e-03
1e-02
1e-01
1e+00
1e+01
5 10 20 30
estimated bias of predicted states
time step
Equidistant in time
Active OpInf
(a) dimension n = 7
1e-04
1e-03
1e-02
1e-01
1e+00
5 10 20 30
estimated bias of predicted states
time step
Equidistant in time
Active OpInf
(b) dimension n = 10
Figure 5.: Cooling of steel proﬁles (Section 5.1). Active op erator inference yields predictions which have a lower bias
compared to the predictions delivered by sampling equidist antly in time in the dictionary. The reduction
in the estimated bias achieved by active operator inference is up to 1.5 orders in magnitude.
18
1e-04
1e-02
1e+00
1e+02
1e+04
1e+06
1e-02 1e-01
estimated MSE of predicted states
noise-to-signal ratio
time step 5
time step 10
time step 20
time step 30
(a) decay of MSE,n = 10
1e-02
1e-01
1e+00
1e+01
1e+02
5 10 20 30
estimated MSE of predicted states
time step
Equidistant in time
Active OpInf
(b) equidistant vs Active OpInf,n = 10
1e-04
1e-03
1e-02
1e-01
1e+00
1e+01
1e+02
1e-06 1e-05 1e-04
estimated MSE of predicted states
standard deviation of noiseσ
Equidistant in time
Active OpInf
(c) time step 10000
Figure 6.: Cooling of steel proﬁles (Section 5.1). In the asy mptotic regime, the estimated MSE decays by 2 orders of
magnitude per 1 order of magnitude decrease in the noise-to- signal ratio, which agrees with Proposition 8.
The predictions obtained with active operator inference ha ve a lower MSE than those obtained from
equidistant-in-time samples.
19
(a) State 1 (b) State 2 (c) State 3 (d) State 4
(e) State 5 (f) State 6 (g) State 7 (h) State 8
(i) State 9 (j) State 10 (k) State 11 (l) State 12
(m) State 13 (n) State 14
466
470
474
478
482
486
490
494
498
temperature[◦C]
(o) State 15
Figure 7.: Cooling of steel proﬁles (Section 5.1). High-dim ensional states selected by sampling equidistant times (le ft)
and by active operator inference (right) from the dictionar y for n = 7. For equidistant sampling, a majority
of the states selected have cooler temperatures at the domai n boundary with Robin condition. In contrast,
active operator inference selects more states at the beginn ing of the cooling process, which leads to more
accurate models in our experiments.
20
t0 10 20 30 40 50
η
0.0
0.5
1.0
1.5
2.02.53.0
x1(η, t )
0.0
0.2
0.4
0.6
0.8
t0 10 20 30 40 50
η
0.0
0.5
1.0
1.5
2.02.53.0
x2(η, t )
1
2
3
4
5
6
7
t0 10 20 30 40 50
η
0.0
0.5
1.0
1.5
2.02.53.0
x3(η, t )
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Figure 8.: High-dimensional system trajectory of the popul ation dynamics of ﬁsh species (Section 5.2) with the test
initial condition.
1e-07
1e-06
1e-05
1e-04
1e-03
1e-02
1e-01
1e-03 1e-02
estimated bias of predicted states
noise-to-signal ratio
time step 10
time step 20
time step 30
time step 40
(a) dimension n = 12
1e-05
1e-04
1e-03
1e-02
1e-01
1e-03 1e-02
estimated bias of predicted states
noise-to-signal ratio
time step 10
time step 20
time step 30
time step 40
(b) dimension n = 15
Figure 9.: Population dynamics of ﬁsh species (Section 5.2) . For this quadratic system, an order of magnitude decay
in the noise-to-signal ratio causes a two orders of magnitud e decay in the estimated bias, demonstrating
the bound of Proposition 7.
5.2. Diﬀusive Lotka-Volterra model for population dynamic s of ﬁsh species
Section 5.2.1 discusses the model and the problem setup while Section 5.2.2 summarizes the results of the
numerical experiments.
5.2.1. Model description
Consider the population dynamics of three species of ﬁsh species in t he Danube river [28]. At time t > 0
and distance η from the mouth of the river, set x1(η, t), x2(η, t), x3(η, t) to be the density of forage ﬁshes,
German carp, and predators, respectively. For η ∈ [0, π] and t ∈ [0, T ], a diﬀusive Lotka-Volterra model that
21
describes the interaction between the species is given by
∂x1(η, t)
∂t = d1
∂2x1(η, t)
∂η2 + x1(a1 − a2x2 − a3x3) (5.2)
∂x2(η, t)
∂t = d2
∂2x2(η, t)
∂η2 + x2(a4 − a5x3)
∂x3(η, t)
∂t = d3
∂2x3(η, t)
∂η2 + x3(a6x1 + a7x2 − a8)
subject to the Neumann boundary condition ∂xi(0,t)
∂η = ∂xi(π,t)
∂η = 0 for i = 1 , 2, 3. The values of the constants
are a1 = 1 .01, a2 = 0 .93, a3 = 0 .1, a4 = 0 .19, a5 = 0 .2, a6 = 1 , a7 = 0 .05, a8 = 0 .2, d1 = 0 .01, d2 = 0 .03, d3 =
0.009.
The diﬀerential equation (5.2) is spatially discretized at 100 equidista nt points in [0 , π]. To temporally
discretize (5.2), we apply the Crank-Nicolson ﬁnite diﬀerence schem e to the diﬀusion term and evaluate the
nonlinear term explicitly in time with step size δt = 0 .01, resulting in an implicit-explicit scheme. This leads
to the autonomous discrete system (3.1) and (2.3) with ℓ = 2 where xk ∈ RN , N = 300.
Set x∗
3 = a4/a5, x∗
2 = ( a1a5−a3a4)/(a2a5), x∗
1 = ( a8−a7x∗
2)/a6. Observe that ( x1, x2, x3) = ( x∗
1, x∗
2, x∗
3) is a
spatially homogeneous equilibrium point of (5.2). The basis matrix V is obtained from snapshots xbasis
k of the
high-dimensional system initiated at the following 6 conditions xbasis
1,i (η, 0) = x∗
1+γ1i sin(6γ2iη)/10, xbasis
2,i (η, 0) =
x∗
2 + γ3i cos(4γ4iη)/10, xbasis
3,i (η, 0) = x∗
3 + γ5i sin(2γ6iη)/10, i = 1 , . . . , 6, where for each i, γ1i, . . . , γ 6i are re-
alizations of a uniform random variable on [0 , 1]. For each initial condition, the high-dimensional system is
simulated until T = 50 resulting in 30000 elements in D. These initial states represent perturbations around
the spatially homogeneous equilibrium. The standard deviations of th e noise σ are 5 equidistant values in
the logarithm scale between 1 × 10− 4 and 1 × 10− 2. For prediction, the initial condition we use is given by
xtest
1 (η, 0) = x∗
1 + sin(6η)/10, xtest
2 (η, 0) = x∗
2 + cos(4η)/10, and xtest
3 (η, 0) = x∗
3 + sin(2η)/10. Figure 8 shows
the high-dimensional state trajectories xtest
1 (η, t), xtest
2 (η, t), xtest
3 (η, t) for t ∈ [0, 50].
5.2.2. Results
A low-dimensional model is inferred from noisy data for dimensions n ∈ { 12, 15}. Active operator inference
is applied to select 100 rows for n = 12, leading to a data matrix with smin(D) = 0 .2794. For n = 15, 150
rows are selected, which results in smin(D) = 0 .0552. Monte Carlo estimates of the bias and MSE are shown
in Figure 9 and Figure 12, respectively. The number of Monte Carlo sa mples used is 5 × 107. The plots
are consistent with the analysis in Proposition 7 and 8, particularly fo r quadratic systems, since we observe
that an order decay in the noise-to-signal ratio leads to two order s decay in the estimated bias and MSE.
The missing value in Figure 9(a) represents a large bias in ˆxtest
k which we do not plot and is caused by the
accumulation of errors in the learned reduced operators over time . It represents a non-asymptotic regime in
which the constants in the bias dominate the behavior of the noise-t o-signal ratio. In Figure 9(b) and 12,
results for larger values of σ are not shown in the plot for the same reason.
We now compare active operator inference to a traditional equidist ant-in-time sampling from the dic-
tionary. The minimum singular value of the data matrix resulting from b oth approaches is compared in
Figure 10. In this example, active operator inference reduces the number of times the high-dimensional
system is queried by up to roughly a factor of two. The estimated bia s and the MSE for both approaches at
σ = 1 × 10− 3 is shown in Figure 11 and in the right panel of Figure 12. We also plot the estimated MSE at
T = 50 using 10 Monte Carlo samples for n = 15 in the bottom panel of Figure 12. Results are not plotted
if the corresponding models numerically led to unstable behavior with u nbounded errors. Active operator
inference provides reasonable numerical predictions in all cases, w hereas equidistant sampling quickly leads
to models that show unstable behavior. This behavior is ampliﬁed for in creasing dimension n. Overall,
the results indicate that for polynomially nonlinear systems it become s even more important than for linear
systems to carefully query the high-dimensional system.
22
0
0. 1
0. 2
0. 3
0. 4
0. 5
0. 6
0. 7
0. 8
100 120 140 160 180 200 220
reduction of # samples
minimum singular value
number of samples from high-dimensional system
Equidistant in time
Active OpInf
(a) dimension n = 12
0
0. 02
0. 04
0. 06
0. 08
0. 1
0. 12
0. 14
140 160 180 200 220 240 260 280 300 320
reduction of # samples
minimum singular value
number of samples from high-dimensional system
Equidistant in time
Active OpInf
(b) dimension n = 15
Figure 10.: Population dynamics of ﬁsh species (Section 5.2 ). Active operator inference requires up to 2 times fewer
queries to the high-dimensional system for generating data than a traditional equidistant-in-time sampling
process.
1e-05
1e-04
1e-03
1e-02
1e-01
1e+00
5 10 20 30 40
estimated bias of predicted states
time step
Equidistant in time
Active OpInf
(a) dimension n = 12
1e-04
1e-03
1e-02
1e-01
1e+00
5 10 20 30 40
estimated bias of predicted states
time step
Equidistant in time
Active OpInf
(b) dimension n = 15
Figure 11.: Population dynamics of ﬁsh species (Section 5.2 ). Selecting the data matrix by sampling equidistant in
time quickly leads to numerical instabilities in the learne d models while the selection obtained with active
operator inference leads to models that show stable and accu rate behavior in this example.
23
1e-04
1e-03
1e-02
1e-01
1e+00
1e-03 1e-02
estimated MSE of predicted states
noise-to-signal ratio
time step 10
time step 20
time step 30
time step 40
(a) decay of MSE,n = 15
1e-03
1e-02
1e-01
1e+00
1e+01
5 10 20 30 40
estimated MSE of predicted states
time step
Equidistant in time
Active OpInf
(b) equidistant vs Active OpInf,n = 15
1e-07
1e-06
1e-05
1e-04
1e-03
1e-02
1e-01
1e+00
1e+01
1e+02
1e-10 1e-09 1e-08 1e-07 1e-06
estimated MSE of predicted states
standard deviation of noiseσ
Equidistant in time
Active OpInf
(c) time step 5000
Figure 12.: Population dynamics of ﬁsh species (Section 5.2 ). An order of magnitude decay in the noise-to-signal ratio
leads to a two orders of magnitude decay in the estimated MSE, which is in agreement with Proposition 8.
Sampling the dictionary at equidistant times results in lea rned models that become numerically unstable
while active operator inference leads to models with orders of magnitude lower MSEs.
24
6. Conclusions
In this work, we established probabilistic guarantees on predictions made with low-dimensional models
learned from noisy data, which motivated an active data sampling app roach to reduce the eﬀect of noise.
The key ingredient of the analysis and the numerical approach was b uilding a bridge from data-driven
modeling via operator inference and re-projection to classical pro jection-based model reduction. Thus, the
proposed approach can be seen as an example of scientiﬁc machine le arning that demonstrates the beneﬁts
of merging traditional scientiﬁc computing concepts such as model reduction with learning methods to
eﬀectively leverage data.
Acknowledgements
We are grateful to Jens Saak for providing the code to generate t he computational mesh and the system
matrices for the heat transfer problem on steel proﬁles. We also t hank Jonathan Niles-Weed for directing us
to references for deriving upper bounds on moments of the norm o f Gaussian random matrices.
This work was partially supported by US Department of Energy, Oﬃc e of Advanced Scientiﬁc Computing
Research, Applied Mathematics Program (Program Manager Dr. St even Lee), DOE Award DESC0019334,
and by the National Science Foundation under Grant DMS-2012250 .
References
[1] A. C. Antoulas. Approximation of Large-Scale Dynamical Systems . Society for Industrial and Applied
Mathematics, 2005.
[2] A. C. Antoulas and B. D. O. Anderson. On the scalar rational inte rpolation problem. IMA Journal of
Mathematical Control & Information , 3(2-3):61–88, 1986.
[3] A. C. Antoulas, I. V. Gosea, and A. C. Ionita. Model reduction o f bilinear systems in the Loewner
framework. SIAM Journal on Scientiﬁc Computing , 38(5):B889–B916, 2016.
[4] P. Astrid, S. Weiland, K. Willcox, and T. Backx. Missing point estimat ion in models described by
proper orthogonal decomposition. IEEE Transactions on Automatic Control , 53(10):2237–2251, 2008.
[5] M. Barrault, Y. Maday, N. C. Nguyen, and A. T. Patera. An ‘empir ical interpolation’ method: appli-
cation to eﬃcient reduced-basis discretization of partial diﬀerent ial equations. Comptes Rendus Math-
ematique, 339(9):667–672, 2004.
[6] C. Beattie and S. Gugercin. Realization-independent H2-approximation. In Proc. IEEE Conf. Decis.
Control, pages 4953–4958, Maui, HI, USA, 2012.
[7] C. Beattie, S. Gugercin, and S. Wyatt. Inexact solves in interpo latory model reduction. Linear Algebra
and its Applications , 436(8):2916–2943, 2012. Special Issue dedicated to Danny Sor ensen’s 65th birthday.
[8] P. Benner, S. Gugercin, and K. Willcox. A survey of projection-b ased model reduction methods for
parametric dynamical systems. SIAM Review , 57(4):483–531, 2015.
[9] P. Benner, V. Mehrmann, V. Sima, S. Van Huﬀel, and A. Varga. Slic ot—a subroutine library in systems
and control theory. In B. N. Datta, editor, Applied and Computational Control, Signals, and Circuits:
Volume 1 , pages 499–539, Boston, MA, 1999. Birkh¨ auser Boston.
[10] P. Benner and J. Saak. Linear-quadratic regulator design for optimal cooling of steel proﬁles. Technical
Report SFB393/05-05, Sonderforschungsbereich 393 Parallele Numerische Simulation f¨ ur Physik und
Kontinuumsmechanik, TU Chemnitz, D-09107 Chemnitz (Germany), 2005.
25
[11] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering gov erning equations from data by sparse identi-
ﬁcation of nonlinear dynamical systems. Proceedings of the National Academy of Sciences , 113(15):3932–
3937, 2016.
[12] D. Burov, D. Giannakis, K. Manohar, and A. Stuart. Kernel an alog forecasting: Multiscale test prob-
lems. Multiscale Modeling & Simulation , 19(2):1011–1040, 2021.
[13] M. Campi and E. Weyer. Finite sample properties of system ident iﬁcation methods. IEEE Transactions
on Automatic Control , 47(8):1329–1334, 2002.
[14] S. Chaturantabut and D. C. Sorensen. Nonlinear model reduc tion via discrete empirical interpolation.
SIAM Journal on Scientiﬁc Computing , 32(5):2737–2764, 2010.
[15] E. Clark, S. L. Brunton, and J. N. Kutz. Multi-ﬁdelity sensor se lection: Greedy algorithms to place
cheap and expensive sensors with cost constraints. IEEE Sensors Journal , 21(1):600–611, 2021.
[16] Z. Drmaˇ c and S. Gugercin. A new selection operator for the dis crete empirical interpolation method—
improved a priori error bound and extensions. SIAM Journal on Scientiﬁc Computing , 38(2):A631–A648,
2016.
[17] Z. Drmaˇ c, S. Gugercin, and C. Beattie. Vector ﬁtting for mat rix-valued rational approximation. SIAM
Journal on Scientiﬁc Computing , 37(5):A2346–A2379, 2015.
[18] Z. Drmaˇ c and B. Peherstorfer. Learning low-dimensional dyn amical-system models from noisy
frequency-response data with Loewner rational interpolation. I n Realization and Model Reduction of
Dynamical Systems: A Festschrift in Honor of the 70th Birthd ay of Thanos Antoulas . Springer, 2020.
[19] M. Embree and A. C. Ionita. Pseudospectra of Loewner matrix pencils. arXiv, 1910.12153, 2019.
[20] I. V. Gosea and A. C. Antoulas. Data-driven model order redu ction of quadratic-bilinear systems.
Numerical Linear Algebra with Applications , 25(6):e2200, 2018.
[21] M. Guo and J. S. Hesthaven. Reduced order modeling for nonline ar structural analysis using Gaussian
process regression. Computer Methods in Applied Mechanics and Engineering , 341:807–826, 2018.
[22] M. Guo and J. S. Hesthaven. Data-driven reduced order mode ling for time-dependent problems. Com-
puter Methods in Applied Mechanics and Engineering , 345:75–99, 2019.
[23] B. Gustavsen and A. Semlyen. Rational approximation of frequ ency domain responses by vector ﬁtting.
IEEE Transactions on Power Delivery , 14(3):1052–1061, 1999.
[24] J. S. Hesthaven and S. Ubbiali. Non-intrusive reduced order mo deling of nonlinear problems using
neural networks. Journal of Computational Physics , 363:55–78, 2018.
[25] A. C. Ionita and A. C. Antoulas. Data-driven parametrized mod el reduction in the Loewner framework.
SIAM Journal on Scientiﬁc Computing , 36(3):A984–A1007, 2014.
[26] I. C. F. Ipsen and B. Nadler. Reﬁned perturbation bounds for eigenvalues of hermitian and non-hermitian
matrices. SIAM Journal on Matrix Analysis and Applications , 31(1):40–53, 2009.
[27] J.-N. Juang and R. S. Pappa. An eigensystem realization algorith m for modal parameter identiﬁcation
and model reduction. Journal of Guidance, Control, and Dynamics , 8(5):620–627, 1985.
[28] T. Kmet’ and J. Holˇ c´ ık. The diﬀusive Lotka-Volterra model a s applied to the population dynamics
of the german carp and predator and prey species in the Danube riv er basin. Ecological Modelling ,
74(3-4):277–285, 1994.
26
[29] B. Kramer and S. Gugercin. Tangential interpolation-based eig ensystem realization algorithm for MIMO
systems. Mathematical and Computer Modelling of Dynamical Systems , 22(4):282–306, 2016.
[30] J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor. Dynamic mode decomposition: Data-driven
modeling of complex systems . SIAM, 2016.
[31] S. Lefteriu and A. C. Antoulas. A new approach to modeling multip ort systems from frequency-domain
data. Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on , 29(1):14–27,
2010.
[32] S. Lefteriu, A. C. Ionita, and A. C. Antoulas. Modeling systems based on noisy frequency and time
domain measurements. In J. C. Willems, S. Hara, Y. Ohta, and H. Fuj ioka, editors, Perspectives
in Mathematical System Theory, Control, and Signal Process ing: A Festschrift in Honor of Yutaka
Yamamoto on the Occasion of his 60th Birthday , pages 365–378. Springer Berlin Heidelberg, 2010.
[33] L. Ljung. System identiﬁcation . Prentice Hall, 1987.
[34] K. Manohar, B. W. Brunton, J. N. Kutz, and S. L. Brunton. Da ta-driven sparse sensor placement
for reconstruction: Demonstrating the beneﬁts of exploiting kno wn patterns. IEEE Control Systems
Magazine, 38(3):63–86, 2018.
[35] A. J. Mayo and A. C. Antoulas. A framework for the solution of t he generalized realization problem.
Linear Algebra and its Applications , 425(2–3):634–662, 2007.
[36] B. Peherstorfer. Sampling low-dimensional markovian dynamics for pre-asymptotically recovering re-
duced models from data with operator inference. SIAM Journal on Scientiﬁc Computing , 42:A3489–
A3515, 2020.
[37] B. Peherstorfer, Z. Drmaˇ c, and S. Gugercin. Stability of disc rete empirical interpolation and gappy
proper orthogonal decomposition with randomized and determinist ic sampling points. SIAM Journal
on Scientiﬁc Computing , 42(5):A2837–A2864, 2020.
[38] B. Peherstorfer and K. Willcox. Data-driven operator inferen ce for nonintrusive projection-based model
reduction. Computer Methods in Applied Mechanics and Engineering , 306:196–215, 2016.
[39] E. Qian. A scientiﬁc machine learning approach to learning reduced m odels for nonlinear partial diﬀer-
ential equations . PhD thesis, Massachusetts Institute of Technology, 2021.
[40] E. Qian, B. Kramer, B. Peherstorfer, and K. Willcox. Lift & learn : Physics-informed machine learning
for large-scale nonlinear dynamical systems. Physica D: Nonlinear Phenomena , 406:132401, 2020.
[41] C. Rowley, I. Mezi´ c, S. Bagheri, P. Schlatter, and D. Henning son. Spectral analysis of nonlinear ﬂows.
Journal of Fluid Mechanics , 641:115–127, 2009.
[42] G. Rozza, D. Huynh, and A. T. Patera. Reduced basis approxim ation and a posteriori error estima-
tion for aﬃnely parametrized elliptic coercive partial diﬀerential equ ations. Archives of Computational
Methods in Engineering , 15(3):1–47, 2008.
[43] S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz. Dat a-driven discovery of partial diﬀerential
equations. Science Advances, 3(4), 2017.
[44] S. H. Rudy, J. N. Kutz, and S. L. Brunton. Deep learning of dyn amics and signal-noise decomposition
with time-stepping constraints. Journal of Computational Physics , 396:483–506, 2019.
[45] H. Schaeﬀer, R. Caﬂisch, C. D. Hauck, and S. Osher. Sparse d ynamics for partial diﬀerential equations.
Proceedings of the National Academy of Sciences , 110(17):6634–6639, 2013.
27
[46] H. Schaeﬀer, G. Tran, and R. Ward. Extracting sparse high-d imensional dynamics from limited data.
SIAM Journal on Applied Mathematics , 78(6):3279–3295, 2018.
[47] P. J. Schmid. Dynamic mode decomposition of numerical and expe rimental data. Journal of Fluid
Mechanics, 656:5–28, 2010.
[48] P. Seshadri, A. Narayan, and S. Mahadevan. Eﬀectively subsa mpled quadratures for least squares
polynomial approximations. SIAM/ASA Journal on Uncertainty Quantiﬁcation , 5(1):1003–1023, 2017.
[49] V. Sima and P. Benner. Fast system identiﬁcation and model red uction solvers. IFAC Proceedings
Volumes, 40(13):477–482, 2007. 9th IFAC Workshop on Adaptation and Le arning in Control and Signal
Processing.
[50] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Lea rning without mixing: Towards a sharp
analysis of linear system identiﬁcation. In S. Bubeck, V. Perchet, a nd P. Rigollet, editors, Proceedings
of the 31st Conference On Learning Theory , volume 75 of Proceedings of Machine Learning Research ,
pages 439–473. PMLR, 06–09 Jul 2018.
[51] R. Swischuk, B. Kramer, C. Huang, and K. Willcox. Learning phys ics-based reduced-order models for
a single-injector combustion process. AIAA Journal , 58(6):2658–2672, 2020.
[52] G. Tran and R. Ward. Exact recovery of chaotic systems from highly corrupted data. Multiscale
Modeling & Simulation , 15(3):1108–1129, 2017.
[53] J. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz. On dynamic mode
decomposition: Theory and applications. Journal of Computational Dynamics , 1(2):391–421, 2014.
[54] W. I. T. Uy and B. Peherstorfer. Operator inference of non- markovian terms for learning reduced models
from partially observed state trajectories. arXiv:2103.01362, 2021.
[55] W. I. T. Uy and B. Peherstorfer. Probabilistic error estimation for non-intrusive reduced models learned
from data of systems governed by linear parabolic partial diﬀerent ial equations. ESAIM: Mathematical
Modelling and Numerical Analysis (M2AN) , 55(3):735–761, 2021.
[56] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. C. Eldar and
G. Kutyniok, editors, Compressed Sensing , pages 210–268. Cambridge University Press, 2012.
[57] M. Vidyasagar and R. L. Karandikar. A learning theory approac h to system identiﬁcation and stochastic
adaptive control. Journal of Process Control , 18(3):421–430, 2008. Festschrift honouring Professor Dale
Seborg.
[58] C. R. Wentland, C. Huang, and K. Duraisamy. Investigation of s ampling strategies for reduced-order
models of rocket combustors. In AIAA Scitech 2021 Forum , pages 1–31. AIAA, 2021.
[59] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley. A data–driven ap proximation of the Koopman
operator: Extending dynamic mode decomposition. Journal of Nonlinear Science , 25(6):1307–1346,
2015.
[60] S. Zhang and G. Lin. Robust data-driven discovery of governin g physical laws with error bars. Pro-
ceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474(2217):20180305,
2018.
[61] S. Zhang and G. Lin. SubTSBR to tackle high noise and outliers for data-driven discovery of diﬀerential
equations. Journal of Computational Physics , 428:109962, 2021.
28
A. Matrix concentration inequalities
Theorem 9 (Theorem 5.32 and Proposition 5.34 in [56]) . Let A be an N × n matrix whose entries are
independent standard normal random variables. Then
E[∥A∥2] ≤
√
N + √n (1.1)
and for t ≥ 0,
P
(⏐
⏐
⏐
⏐∥A∥2 − E[∥A∥2]
⏐
⏐
⏐
⏐> t
)
≤ 2e− t2/2. (1.2)
B. Proof of Lemma 2
By using the triangle inequality for the norm E[| · | l]1/l,
(
E[∥G∥l
2]
) 1/l
=
(
E
[ ⏐
⏐∥G∥2 − E[∥G∥2] + E[∥G∥2]
⏐
⏐l]) 1/l
≤
(
E
[ ⏐
⏐∥G∥2 − E[∥G∥2]
⏐
⏐l]) 1/l
+ E[∥G∥2]
≤
(
E
[ ⏐
⏐∥G∥2 − E[∥G∥2]
⏐
⏐l]) 1/l
+ √
n + √p (2.1)
where we have used the bound (1.1).
Denote by Γ( ·) the gamma function. Recall that that Γ( x + 1) ≤ xx for x ≥ 0 and Γ( x + 1) = xΓ(x). To
bound the ﬁrst term in the right hand side of the inequality (2.1), we p roceed as follows. For t ≥ 0,
E
[ ⏐
⏐∥G∥2 − E[∥G∥2]
⏐
⏐l]
= l
∫ ∞
0
tl− 1P
(⏐
⏐
⏐
⏐∥G∥2 − E[∥G∥2]
⏐
⏐
⏐
⏐≥ t
)
dt
≤ 2l
∫ ∞
0
tl− 1e− t2/2 dt = 2 l
∫ ∞
0
(2u)
l− 2
2 e− u du
= l2l/2
∫ ∞
0
ul/2− 1e− u du = 2 l/2+1 l
2Γ
( l
2
)
= 2 l/2+1Γ
( l
2 + 1
)
≤ 2l/2+1
( l
2
) l/2
= 2 ll/2
where we utilized the concentration inequality (1.2) in Appendix A and p roperties of the gamma function
mentioned above.
This implies that
(
E
[
|∥G∥2 − E[∥G∥2]|l
]) 1/l
≤ 21/l√
l and the conclusion follows from (2.1).
29