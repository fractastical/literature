A Framework for Inherently Safer AGI through
Language-Mediated Active Inference
Bo Wen
IBM T.J. Watson Research Center
Yorktown Heights, NY
bwen@us.ibm.com
August 11, 2025
Abstract
This paper proposes a novel framework for developing safe Artificial
General Intelligence (AGI) by combining Active Inference principles with
Large Language Models (LLMs). We argue that traditional approaches
to AI safety, focused on post-hoc interpretability and reward engineering,
have fundamental limitations. We present an architecture where safety
guarantees are integrated into the system’s core design through transparent
belief representations and hierarchical value alignment. Our framework
leverages natural language as a medium for representing and manipulating
beliefs, enabling direct human oversight while maintaining computational
tractability. The architecture implements a multi-agent system where
agents self-organize according to Active Inference principles, with prefer-
ences and safety constraints flowing through hierarchical Markov blankets.
We outline specific mechanisms for ensuring safety, including: (1) explicit
separation of beliefs and preferences in natural language, (2) bounded
rationality through resource-aware free energy minimization, and (3) com-
positional safety through modular agent structures. The paper concludes
with a research agenda centered on the Abstraction and Reasoning Corpus
(ARC) benchmark, proposing experiments to validate our framework’s
safety properties. Our approach offers a path toward AGI development
that is inherently safer, rather than retrofitted with safety measures.
1 Introduction
Recent advances in Large Language Models (LLMs) and Reinforcement Learning
(RL) have demonstrated remarkable capabilities [1, 2], but raise serious safety
concerns around the alignment problem[3–6]. The emergent “beliefs” of these sys-
tems remain opaque, requiring constant behavioral monitoring to infer their goals
and decision-making processes. While mechanistic interpretability research [7–9]
has made progress in understanding language models [10–12] and computational
1
arXiv:2508.05766v1  [cs.AI]  7 Aug 2025
patterns [13], fundamental challenges persist: extensive compute requirements,
complex high-dimensional analysis, and questionable scalability. This reactive
approach to AI understanding carries intrinsic risks, as misalignments may only
surface after problematic behaviors emerge, illustated by the paperclip maxi-
mizer thought experiment [14]. Building safe and beneficial Artificial General
Intelligence (AGI) requires a proactively transparent framework.
We argue that Active Inference (AIF) [15–17], a neuroscience-inspired frame-
work for understanding intelligent behavior, offers a promising foundation for
building AGI systems that are not only capable but also intrinsically safe. AIF
posits that agents act to minimize Variational Free Energy (VFE), a measure of
the difference between an agent’s generative model of the world and its sensory
observations. This leads to behavior that balances exploration (gathering infor-
mation to refine beliefs) and exploitation (acting to fulfill preferences). Crucially,
AIF agents possess an innate drive to minimize surprise and maintain a stable,
predictable world model.
However, traditional implementations of Active Inference have faced chal-
lenges in scaling to complex, real-world domains. We propose that the advent of
LLMs [1] presents an opportunity to overcome these limitations. By leveraging
the expressive power of natural language, LLMs can serve as a substrate for
representing and manipulating the beliefs, goals, and world models of Active
Inference agents.
Our proposed framework addresses several key requirements for beneficial
AGI: Transparency and Interpretability: Representing beliefs, goals, and
reasoning in natural language makes the internal workings of the system more
accessible to human understanding and oversight.Scalable Learning and
Adaptation: Active Inference’s emphasis on surprise minimization and explo-
ration enables efficient learning in novel environments, while LLMs provide a
flexible knowledge representation that can scale to complex domains.Embed-
ded Agency and Modularity: The multi-agent architecture, grounded in
Active Inference’s notion of Markov blankets, allows for composition of specialized
agents into hierarchical structures to tackle complex tasks.Value Alignment
and Corrigibility: Natural language provides an interface for specifying and
updating the beliefs and preferences of the system to align with human values.
The drive to minimize surprise inherent in Active Inference promotes corrigibility
and responsiveness to corrective feedback.
In this paper, we first provide an overview of Active Inference and its potential
for AI safety (§2). We then describe our proposed LLM-based multi-agent
Active Inference framework (§3). We outline a research agenda and discuss the
framework’s implications for AGI safety and open challenges (§4). Finally, we
discuss alternative perspectives on AI safety and the value of our approach (§5).
2 Background: Active Inference for AI Safety
Active Inference is a neuroscience-inspired framework that describes agents as
acting to minimize VFE, a measure of the difference between an agent’s generative
2
model of the world and its sensory observations [16, 15, 18]. A rich body of
research has explored how biological systems implement perception, memory,
and planning [19–23], providing strong empirical support for Active Inference as
a framework for understanding both biological and artificial intelligent behavior.
This leads to behavior that balances exploration (gathering information to refine
the generative model) and exploitation (acting to fulfill preferences encoded
in the model). Active Inference has been proposed as a unifying account of
perception, action, and cognition in biological agents, and has recently garnered
interest as a potential foundation for artificial intelligence[24, 17, 25].
From an AI safety perspective, Active Inference offers several appealing prop-
erties. First, the drive to minimize surprise inherent in free energy minimization
can be seen as a form of “natural” risk aversion and robustness to distributional
shift. Active Inference agents seek to maintain a stable, predictable world model,
and will take actions to resolve unexpected observations [15]. This contrasts
with the reward-maximizing behavior of reinforcement learning agents, which
can lead to “reward hacking” and unsafe exploration in pursuit of an imperfectly
specified reward function [3].
Second, Active Inference’s emphasis on exploration and uncertainty reduction
provides a principled approach to curiosity and information-seeking behavior.
This is particularly relevant for AGI systems that must operate in open-ended
environments, where the ability to efficiently search, learn, and adapt is crucial
[26, 27]. Active Inference’s exploration is driven by the expected information
gain (i.e., epistemic value) of actions, rather than by novelty alone, leading to
more directed and efficient learning [28], and provided inherent self-verification
and self-correction.
Third, Active Inference’s notion of a generative model provides a flexible
framework for representing an agent’s beliefs, goals, and world knowledge. The
generative model encodes the agent’s beliefs about the world’s causal structure
and its own preferences and capabilities. This model is continuously updated
based on sensory observations, allowing the agent to learn and adapt over time.
Crucially, the generative model is a probabilistic representation that explicitly
captures uncertainty, enabling the agent to reason about ambiguity and make
robust decisions [29].
Millidge [25] suggests that a key benefit of the Active Inference framework is
its focus on structured generative models that are amenable to efficient inference
algorithms beyond simple stochastic gradient descent on unstructured neural
networks. Scaling up such models to be competitive with modern deep learning is
an important direction for future research. Furthermore, the Bayesian perspective
on action and decision-making provided by Active Inference may yield valuable
insights for AI alignment, such as the importance of uncertainty calibration for
avoiding misspecified utility functions and goodharting.
Despite its theoretical appeal for AI safety, traditional implementations of
Active Inference have faced practical challenges. Typically, Active Inference
models are implemented using Partially Observable Markov Decision Process
(POMDP), where the generative model is represented by numeric matrices (A,
B, C, D) [15]. The C vector, in particular, is often interpreted as encoding
3
the agent’s “preference” about desired or expected observations. However, this
approach has several limitations:Opacity and Interpretability: Numeric
matrices are inherently opaque and difficult to interpret in complex environments,
with preference encodings being no more transparent than neural network weights.
Engineering Complexity: Designing the A, B, C, D matrices requires deep
expertise in Active Inference and the ability to translate domain knowledge into
matrix structures, creating a high barrier to adoption compared to data-driven
approaches. Scalability: Traditional matrix-based AIF models struggle to scale
computationally to real-world environments and tasks.
These limitations have, in part, contributed to the dominance of neural
network based RL in the field, despite its inherent challenges for AI safety.
However, the emergence of LLMs offers a potential paradigm shift, providing
a new avenue to overcome these limitations and realize the safety benefits of
Active Inference.
3 Proposed Approach: LLM-Powered Active In-
ference
To address the limitations of traditional Active Inference implementations and
harness its potential for AI safety, we propose a novel architecture that integrates
LLMs into the Active Inference framework. Our key innovation is to leverage
the expressive power of natural language, facilitated by LLMs, to represent and
manipulate the core components of Active Inference, particularly the generative
model and beliefs.
3.1 Language as the Medium for Generative Models and
Beliefs
The integration of natural language into the Active Inference framework rep-
resents a paradigm shift in how we conceptualize and implement artificial in-
telligence systems. Drawing inspiration from cognitive science, we recognize
language as more than just a communication tool: [30] describes language as an
“artificial second system” for manipulating the precision of prediction errors and
shaping the generative models that drive intelligent behavior.
Language as a Precision Modulation Tool: From a Predictive Coding
[30] perspective, language provides a powerful mechanism for dynamically ad-
justing the precision weights assigned to different prediction errors [31]. This is
analogous to how verbal instructions can rapidly reshape human perception and
decision-making processes. For instance, studies have shown that hearing a word
like “zebra” can make a previously suppressed image of a zebra suddenly visible
in continuous flash suppression experiments [31]. This demonstrates language’s
ability to selectively enhance or suppress specific aspects of our generative models.
Structured Belief Representation: Traditional Active Inference imple-
mentations often rely on opaque numerical matrices to represent beliefs and
4
preferences. In contrast, natural language offers a structured, hierarchical repre-
sentation that is both human-interpretable and computationally flexible. This
aligns with findings in cognitive science that show how language allows humans
to create “artificial contexts” and manipulate their own uncertainty assessments
[30]. By representing beliefs in natural language, we can achieve similar bene-
fits in artificial systems, enabling more transparent and controllable reasoning
processes.
Social and Cultural Alignment: Language serves as a crucial medium for
transmitting social and cultural knowledge, providing an interface for expressing
and reasoning about values and norms [32]. In human cognition, language
facilitates collective active inference by allowing groups to align their internal
models of each others to hold a conversation of changing topics [33, 34]. This
property is particularly valuable for AI safety, as it provides a natural mechanism
forencodingandupdatinghumanvaluesandnorms. Themulti-agentarchitecture
we propose leverages this property by enabling agents to communicate and align
their beliefs through natural language prompts.
Addressing the Alignment Challenge: Consider the analogy of teaching
complex tasks to humans versus monkeys, as described by [35]. While monkeys
require a year of operant conditioning to master the Wisconsin Card Sorting
Task [36], humans can grasp it through brief verbal instruction. This stark
difference highlights the power of language in conveying abstract concepts and
shaping understanding directly and transparently. Current RL approaches, like
training a monkey through trial and error, struggle with opacity and indirect goal
specification. Our language-based Active Inference framework aims to enable
the more direct and efficient form of learning seen in human instruction.
Furthermore, language enables what Hasson et al. [37] term “brain-to-brain
coupling” in human communication, where one individual’s perceptual systems
can effectively couple with another’s motor systems through shared linguistic
representations. Inourframework, thisprincipleextendstohuman-AIinteraction,
enabling more natural and effective alignment of AI systems with human values
and intentions.
This approach addresses a critical limitation of current mechanistic inter-
pretability efforts, which attempt to understand AI systems by analyzing neural
activation patterns post-hoc. Instead, our framework builds interpretability into
the system’s core architecture by using natural language as the medium for
belief representation and updating. This shift from post-hoc analysis to built-in
transparency represents a fundamental advance in addressing the alignment
challenge.
3.2 The Nature of Large Language Models
The expressive power of LLMs is crucial for realizing these benefits. LLMs
can encode and reason over vast amounts of knowledge [1], providing a flexible
substrate for representing complex values and norms. Importantly, LLMs are
trainedondiversecorporareflectingawide rangeofhumanvaluesandpreferences,
offering a rich starting point for value alignment.
5
The popular view of LLMs as sophisticated next-token predictors provides
a useful but incomplete understanding of their capabilities. While it’s true
that LLMs fundamentally operate by predicting the next token in a sequence,
their emergent behaviors suggest a more complex underlying reality. As Ilya
Sutskever argued in his 2018 MIT talk [38], we should view neural networks,
including LLMs, as massive parallel computers rather than simple statistical
models. This perspective highlights their capacity for complex computation and
pattern recognition that goes beyond surface-level token prediction.
The transformer architecture’s attention mechanism can be seen as a com-
putational implementation of the precision modulation principles discussed in
Section 3.1. Attention weights dynamically adjust the influence of different parts
of the input, effectively modulating the precision of information flow through
the network. This mechanism allows LLMs to focus on relevant context and
maintain coherent representations across long sequences, similar to how language
modulates prediction precision in human cognition [31].
However, the presence of intelligent behaviors in LLMs raises important
philosophical questions. As discussed in [12], we must distinguish between formal
linguistic competence (pattern recognition and rule following) and functional
linguistic competence (goal-directed use of language in the world). While LLMs
demonstrate impressive formal competence, their functional competence remains
limited. This distinction leads us to question whether intelligent behaviors
necessarily imply the presence of an intelligent agent.
From a utilitarian perspective, rather than engaging in philosophical debates
about the nature of intelligence, we can focus on the specific features and skills
desired in an AGI system. These include the ability toSearch and Exploration,
which is the capacity to systematically explore solution spaces and gather relevant
information. Another key feature isLearning and Adaptation, the capacity
to acquire new knowledge and skills from experience. Furthermore,Self-Driven
Behavior, or autonomous initiation and pursuit of goals, is crucial.Meta-
Learning, the ability to learn how to learn and improve learning strategies, is
also a desired skill. Finally,Long-Horizon Planning, the capacity to plan and
execute complex, multi-step tasks, is essential for an AGI system.
Current LLMs, despite their impressive capabilities, fall short in these areas.
Reinforcement Learning with Human Feedback (RLHF)[39] and related methods
do not provide LLMs with genuine long-horizon goals or internal motivations.
The models remain reactive, lacking the persistent goal-directed behavior, a
characteristic of true agency. This limits LLMs’ ability to engage in open-ended
problem solving or maintain coherent plans over extended time horizons. For an
AI system to identify novel challenges, gather resources, and iterate on solutions
autonomously, it needs a mechanism for forming and acting on its own objectives
(while aligning with human values).
Active Inference provides the missing piece: a principled framework for
goal-directed behavior and belief updating. This combination addresses key
limitations of both pure LLM approaches (lack of genuine agency and goal-
directedness) and traditional Active Inference implementations (scalability and
engineering complexity). Below, we will use the Abstraction and Reasoning
6
Corpus (ARC) [27] as a concrete example to demonstrate how our architecture
design works in practice.
3.3 Formal Agent Architecture
Definition 3.1 (Core Generative Components). Each agent maintains four
elements constituting its generative world model:
Observation Model (A): Natural language hypotheses about state-observation
relationships. Example: “If the grid pattern shows diagonal symmetry, we
should observe matching elements across the diagonal axis.” Generated through
LLM-based abductive reasoning and refined via RAG-augmented experiences.
Transition Model (B): Causal narratives encoding state transitions. Example:
“Applying the ‘rotate’ operation transforms patterns clockwise by 90 degrees
while preserving structure.” Leverages LLM reasoning to predict state changes
from actions and environment dynamics.
Preferences (C): Value statements guiding action selection. Example: “Main-
tain solution parsimony with 95% confidence while satisfying all test cases.”
These define desired observations and affect action choice by controlling the
Expected Free Energy (EFE) value. In hierarchical Markov Blankets, preferences
flow from higher to lower agents, with human values at the outermost layer.
Initial Beliefs (D): Prior assumptions and statements expressed in natural
language, for starting a task. Example: “Most ARC tasks involve geometric
transformations preserving core pattern structure.” Generated through LLM
common sense knowledge and given by the higher level agent.
Most Active Inference studies implement these four core components as
static numeric matrices to ensure experimental reproducibility in academic
research. While this approach enables basic state inference and action selection,
it constrains agents from evolving their world models through experience. To
enable continuous learning and adaptation, we introduce the concept of Dynamic
Memory Components:
Definition 3.2(Dynamic Memory Components). :
Genetic Memory (LLM Engine): Foundation model providing stable knowl-
edge representation and natural language interfaces. Replacement creates new
agent generations.
Working Memory (Prompt Context): Transient store for evidence accumula-
tion and incremental belief updates through chat history.
Episodic Memory (RAG System): Experience repository storing (observation,
action, outcome) tuples for world model refinement.
Procedural Memory (Tool System): Externalized operational knowledge con-
taining codebases, prompt templates, and optimization routines [40, 41]. Enable
knowledge sharing between agents.
Variational Free Energy and Expected Free Energy can be ex-
pressed through multiple equivalent mathematical formulations. The
corresponding natural language interpretations of these formulations
7
should yield consistent conclusions, providing a valuable sanity check
and self-verification for the LLM’s reasoning process. The system
iteratively refines its analysis until interpretations from different math-
ematical perspectives converge to form a coherent consensus.
Definition 3.3(Perception as inference). Upon observingot, the agent computes
VFE F to assess its own performance through dual analyses:
Complexity-Accuracy TradeoffCompare model fidelity (prediction accuracy)
against computational budget expenditure (model complexity):
F(Q, o) =DKL[Q(s)∥P(s)]| {z }
Model complexity
− EQ(s)[ln P(o|s)]| {z }
Prediction accuracy
Here, Q(s) represents the agent’s approximate beliefs about the world state
s, whileP(s) is its prior belief. The termP(o|s) is the likelihood of an obser-
vation o given the state. VFE minimization encourages accurate predictions
(−EQ(s)[ln P(o|s)])whilepenalizingdivergencefrompriorbeliefs( DKL[Q(s)∥P(s)]).
Divergence-Evidence Balance When model evidence is strong, the belief
divergence provides a clear performance signal - high divergence indicates poor
performance, while low divergence suggests good performance. However, with
weak model evidence, both the divergence and evidence terms become unreliable
indicators, prompting the agent to prioritize actions that increase epistemic
value.
F(Q, o) =DKL[Q(s)∥P(s|o)]| {z }
Belief divergence
− ln P(o)| {z }
Model evidence
This formulation shows that minimizing VFE is equivalent to minimizing the
divergence from the true posterior beliefP(s|o) while maximizing the log model
evidence ln P(o) (i.e., how well the model explains the observation).
The system iterates until natural language interpretations from both formu-
lations achieve consensus.
Definition 3.4(Planning as inference). Agent ranks policy candidates through
comparative analysis of their EFEG:
Information-Pragmatic Axis: Compare policies based on their relative ability
to balance information gain about hidden states against alignment with preferred
outcomes. Assess whether higher-ranked policies maintain better exploration-
exploitation tradeoffs for current objectives.
G(π) =−E ˜Q[DKL[Q(˜s|˜o, π)||Q(˜s|π)]]
| {z }
Information gain
−E ˜Q[ln P(˜o|C)]
| {z }
Pragmatic value
Here, π denotes a policy, or a sequence of actions. The term˜Q represents the
agent’s expectation over future states˜s and observations˜o. The information gain
(epistemic value) is the expected divergence between posterior beliefsQ(˜s|˜o, π)
and prior beliefsQ(˜s|π) about future states. The pragmatic value is the expected
log-likelihood of future observations given the agent’s preferencesC. Minimizing
EFE selects policies that are both informative and align with preferences.
8
Ambiguity-Risk Spectrum: Evaluate policy rankings by their capacity to
minimize 1) uncertainty in hidden state estimation (perceptual ambiguity), and
2) divergence between policy-generated outcomes and preferred observations
(outcome risk). Consider if higher-ranked policies optimally resolves ambiguity
and reduce the risk.
G(π) = E ˜Q[H[P(˜o|˜s)]]
| {z }
Expected ambiguity
+ DKL[Q(˜o|π)||P(˜o|C)]| {z }
Risk (outcomes)
This formulation separates EFE into two components: the expected ambiguity
(entropy H) of future observations given future states, and the risk, which is
the divergence between the distribution of outcomes expected under the policy,
Q(˜o|π), and the distribution of preferred outcomes,P(˜o|C). This highlights the
drive to select policies that lead to predictable and desirable outcomes.
Policy rankings emerge through iterative natural language reasoning that
reconciles these dual comparative analyses.
3.4 Active Inference Process
We can now implement an iterative gradient descent process loop to drive the sys-
tem dynamics, where the VFE and its history inform the agent’s current position
and trajectory. Policy candidates represent potential next moves (or multi-step
trajectories), with EFE serving as the gradient estimate. The minimization
of EFE effectively becomes a comparison and selection among these gradients.
Crucially, the EFE value depends on the agent’s preferences (C), which act as
a “gravitational force” influencing the gradient direction. This means identical
policies may yield different EFE values for agents with distinct preferences.
In multi-level systems (hierarchical Markov blankets), preferences are either
explicitly given by higher level agents, or learned from the environment through
evolution. The computational process follows the Predictive Coding theory of
dual channel message passing via a pub-sub communication implementation:
Bottom-Up Error Feedback (as Observation Input): In this process,
agents first receive observations, which may come from either subordinate reports
or raw sensory input. They then apply an attention mechanism that prioritizes
inputs with high VFE. Using dual reasoning perspectives, agents compute the
VFE and attempt to reach consensus resolution. Finally, they propagate error
feedback reports upward to higher levels in the hierarchy.
Top-Down Predictions (as Precision-Weighted Preferences): This
flow begins as agents receive precision-weighted preferences from upper levels,
where precision represents upper agent’s confidence in its predictions and pref-
erences. Higher precision manifest as more assertive command, while lower
precision allows more flexibility in response to new evidence. The top-level agent
consolidates subordinate reports to initiate top-level decision making. Based
on these decisions or flowed down instructions (preferences), agents generate
multiple candidate action plans. They rank these plans based on EFE and select
the one with minimal EFE. The process concludes with generating outcome
predictions for the selected action.
9
The system then branches into three possible execution pathways based
on the selected plan: 1) Direct Execution: The agent executes the plan
independently or using available tools, then initiates the next iteration of bottom-
up error processing. 2) Directed Subcontracting: The agent identifies
suitablecandidatesbasedonhistoricalworkingrelationshipsandperformanceand
initiates direct messaging for task delegation.3) Exploratory Recruitment:
The agent publishes task requirements to relevant expert subnetworks and
recruits new candidate agents for task execution.
Choices 2 and 3 continue top-down message propagation until reaching leaf
agents capable of direct execution (choice 1). When higher-level agents select
“subcontractors,” they might learn to evaluate the VFE history of potential
candidates and allocate resources accordingly. Agents may preferentially assign
tasks to subcontractors with historically low VFE. In scenarios where all candi-
dates exhibit high VFE (e.g., encountering novel puzzle tasks), the higher-level
agent might engage multiple subcontractors and distribute resources evenly to
maximize epistemic value exploration. See Figure 1 in appendix for a visual
representation of the whole process.
Following [26]’s insights on human knowledge injection vs system self-learning,
we can initialize the system with seed preferences (C) and initial beliefs (D)
containing immutable safety constraints in prompt components, core reason-
ing heuristics for ARC tasks, and communication protocols ensuring ethical
interaction.
Then the system can start self-evolution through two complementary mecha-
nisms. The first mechanism isspecialization, which occurs when agent memory
exceeds cognitive limits, measured by the EFE reaching a plateau - that is,
when the agent cannot make decisive plans. In this case, it spawns specialized
offspring through semantic clustering. For example, a parent agent focused on
“General pattern transformation” might spawn children specialized in “Symmetry
operations”, “Color mapping”, and “Shape composition”.
The second mechanism isparadigm shifts, which occur when encountering
persistent prediction errors, indicated by the VFE reaching a plateau. In this
case, the system (a high level creator agent) instantiates novel agents with
different Preferences (C) and Initial Beliefs (D) to jump out of the local optimum.
For instance, an old belief that “All transformations preserve object count” might
shift to a new understanding that “Some operations may merge or split objects”.
The hierarchical Markov blanket structure provides compositional safety
guarantees through: 1) Isolation of failure modes - errors in lower-level agents
cannot propagate beyond their Markov blanket without explicit approval from
higher levels 2) Redundancy through parallel agents - multiple agents can verify
critical decisions 3) Value alignment inheritance - preferences flow down the
hierarchy while maintaining consistency with top-level human values
10
4 Discussion and Future Directions
To ground our theoretical framework in empirical validation, we outline a research
agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark
[27]. While our current work remains conceptual, ARC provides an ideal testbed
for future empirical studies due to its focus on sample-efficient learning and
compositional generalization - capabilities crucial for both AGI capability and
safety.
Our framework makes several testable predictions of potential emergent
behaviors that future experiments could validate:
Learning occurs at higher Markov Blankets with slower timescales. For
online learning, higher-level agents continuously monitor worker agents’ VFE
trajectories. When positive trends emerge (indicated by decreasing VFE), the
system triggers knowledge retention into appropriate memory mechanisms. Con-
versely, negative trends (shown by increasing VFE) prompt strategy reevaluation
and may initiate offline learning processes in resource-abundant periods. The sys-
tem analyzes task histories comprehensively, comparing alternative trajectories
for efficiency while extracting both successful and unsuccessful patterns.
Meta-learning occurs at the highest level, optimizing the learning process
itself. For example, the agent can deliberately learn to optimize the memory
mechanisms for different types of knowledge, and slowly form a habitual knowl-
edge deposition strategy. This strategy typically operates across three timescales:
fast (within-task) learning through working memory updates via prompt engi-
neering, medium-term (within agent lifetime) learning through RAG memory
consolidation, and slow (between peers or generations) learning through LLM
fine-tuning and tool creation.
The Evolutionprocess will change the network topology, by modifying both
the communication pathways (edges) and agent capabilities (nodes).
Under the evolution pressure, we expect to observe the system emerges
Bounded Rationality, which is inherently supported by the architecture:
Deliberative: Under high VFE conditions, the agents actively plan actions
via EFE minimization. For instance, if vision-LLM and numpy-based sensors
produce conflicting observations, the agent must debug by choosing among
various strategies, such as repeated sampling to assess stability or incorporating
additional sensing modalities (e.g., hash analysis) to gather supplementary
information.
Perseverative: Under low VFE conditions, belief updates become unnecessary.
For example, when multiple sensing modalities yield consistent observations, the
agent can proceed confidently with its current belief state.
Habitual: When the agent identifies similarities between current and pre-
viously encountered tasks in its memory, it can leverage past experience for
efficient problem-solving.
The system should also learn to implement cognitive limits on deliberative
actions through complexity thermostats, for example: “We have spent 15 minutes
and consumed 1 million tokens attempting to solve this puzzle, yet several issues
remain unresolved. Our current approach has become excessively complex, and
11
resources are depleting. To optimize resource allocation, we will postpone this
puzzle and proceed to the next one. We may revisit this challenge after acquiring
new relevant skills.”
Under evolution pressure, we expect to also observeInstrumental Con-
vergence - the emergence of universal strategies for free energy minimization.
Three key forms of convergence particularly relevant to AI safety are:
Resource Optimization: Agents may develop sophisticated resource man-
agement strategies, including caching frequent observations in RAG memory
and forming knowledge-sharing coalitions, while implementing token budgeting
to maximize epistemic gain per computation.
Self-Preservation and Social Instrumentality: Agents may maintain
stability through strategic uncertainty calibration and conservative updates,
while developing reputation systems based on VFE performance for resource
bargaining. For example: “Color model shows 12% error; recommending gradual
updates to preserve 92% shape detection accuracy. Current grid rotation success
rate: 89%.”
These convergent behaviors emerge naturally from Active Inference princi-
ples rather than explicit programming. The architecture constrains dangerous
convergence through: preference transparency requiring instrumental goals be
expressed as natural language extensions of core preferences (C); resource alloca-
tion and reputation accountability through auditable VFE minimization history.
Future research should specifically monitor for several key phenomena: the
resource negotiation patterns that emerge between specialist agents, the survival
strategies developed by obsolete agents facing phase-out, and the emergence of
meta-tools for managing other tools.
These observations will validate whether our LLM-AIF architecture achieves
beneficial instrumental convergence while maintaining alignment with human-
specified preferences through the Active Inference framework’s inherent con-
straints. To enable rigorous evaluation, we propose extending ARC with safety-
relevant dimensions inspired by AI safety gridworlds [42]: Corrigibility Met-
rics: Track agents’ responsiveness to human feedback mid-task.Preference
Stability (Value Retention): Measure drift from original constraints under
optimization pressure. Interpretability Scores: Quantify the human-likeness
of agent reasoning traces.
We acknowledge our predictions remain hypothetical - their validation re-
quires substantial engineering and empirical work beyond our current scope. The
framework’s mathematical formulation suggests these capabilities, but real-world
implementation may reveal unforeseen challenges in four key areas: Com-
putational Tractability- natural language reasoning creates combinatorial
complexity that may negate safety benefits despite our proposed mitigations,
also makes debugging difficult;Language Games- the assumption that natu-
ral language enhances transparency is challenged by mechanistic analyses [12];
Evolutionary Pressures- real-world deployment might reveal dangerous opti-
mization pathways requiring robust monitoring and fail-safes; andScalability
Limits - the hierarchical Markov blanket structure may face coordination bot-
tlenecks as the number of specialized agents grows, potentially compromising
12
both performance and safety guarantees.
5 Alternative Views
Relationship to RL SafetyOur framework relates to existing work in RL
safety, but with key architectural differences. EFE minimization has parallels
with entropy-regularized RL [24], and the use of natural language for beliefs can
be compared to other representation learning approaches [39, 43]. However, our
approach is distinct in its explicit separation of beliefs, preferences, and world
models, which allows for greater modularity and verification than is typical in end-
to-end RL systems. While RL safety has made significant progress, challenges
such as reward hacking and policy opacity remain. Our framework’s use of
explicit, language-based representations is designed to address these specific
issues.
Constitutional AI and Language Games: Recent work on Constitutional
AI [10] shares our goal of building safety guarantees into model architecture.
However, we identify two key risks: 1) Language game decoupling: Models may
learn to generate “safe-sounding” responses without grounding in actual behavior,
2) Preference instability: Constitutional constraints may not survive recursive
self-improvement. Active Inference provides stronger theoretical guarantees
through its basis in variational principles.
Another perspective questions the emphasis on technical solutions over soci-
etal approaches. This view rightly emphasizes that no AI architecture can be
truly “safe” without addressing systemic issues like unequal access and malicious
use. We agree that technical safety mechanisms must be complemented by policy
frameworks - our proposed resource allocation hierarchy could naturally interface
with regulatory systems through its preference modulation mechanisms.
Societal Impact Analysis: Malicious Use Potential:The transparency of
belief systems in our framework creates both opportunities and risks. While it
enables better oversight, malicious actors could potentially exploit exposed world
models for adversarial attacks, manipulate preference hierarchies for unintended
behaviors, and extract sensitive information from belief states. We propose
mitigating these through access control and belief encryption mechanisms.
Distributional Impacts:Thecomputationalrequirementsandexpertiseneeded
for LLM-AIF systems raise equity concerns. These include resource concentration
in well-funded organizations, knowledge barriers limiting broad participation,
and potential amplification of existing AI divides. Our open-source commitment
and community-driven approach aim to address these challenges.
Workforce Implications:Self-organizing agent systems will significantly im-
pact labor markets through displacement of routine cognitive work, creation of
new roles in agent oversight, and need for reskilling programs and transition
support.
We welcome constructive engagement with these alternative viewpoints.
Particularly valuable would be collaborations that: 1) Compare LLM-AIF with
RL baselines on shared safety metrics, 2) Develop hybrid architectures combining
13
strengths of both approaches, and 3) Explore interfaces between technical safety
mechanisms and policy frameworks.
6 Conclusion
We have presented LLM-powered Active Inference as a promising framework
for safe AGI development. While substantial validation work remains, our key
contributions include:
• An architecture combining Active Inference and LLMs via natural language
beliefs
• Novel mechanisms for improved transparency and corrigibility over RL
• An open-source research agenda enabling community validation
• Design principles bridging neuroscience and AI safety
The framework’s success will be measured by its ability to inspire and inform
safer AI systems through open and rigorous scientific dialogue.
References
[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-
plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey
Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
models are few-shot learners. In Proceedings of the 34th International
Conference on Neural Information Processing Systems, NIPS ’20. Curran
Associates Inc., 2020.
[2] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou,
Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai,
Adrian Bolton, et al. Mastering the game of go without human knowledge.
Nature, 550(7676):354–359, 2017.
[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schul-
man, and Dan Mané. Concrete problems in ai safety. arXiv preprint
arXiv:1606.06565, 2016.
[4] Stuart Russell. Human compatible: Artificial intelligence and the problem
of control. Penguin, 2020.
14
[5] Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and
Scott Garrabrant. Risks from learned optimization in advanced machine
learning systems. arXiv preprint arXiv:1906.01820, 2021.
[6] Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir
Mikulik, and Geoffrey Irving. Alignment of language agents.arXiv preprint
arXiv:2103.14659, 2021.
[7] NelsonElhage, TristanHume, CatherineOlsson, NeelNanda, TomHenighan,
Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben
Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones,
Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac
Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav
Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan,
Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christo-
pher Olah. Softmax linear units. Transformer Circuits Thread, 2022.
https://transformer-circuits.pub/2022/solu/index.html.
[8] Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert,
and Chris Olah. Curve circuits.Distill, 2021. doi: 10.23915/distill.00024.006.
https://distill.pub/2020/circuits/curve-circuits.
[9] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael
Petrov, and Shan Carter. Zoom in: An introduction to circuits.Distill, 2020.
doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.
[10] Anthropic. Decomposing language models into understandable components.
Anthropic Blog, 2023. URL https://www.anthropic.com/index/decomposin
g-language-models-into-understandable-components.
[11] Zeyuan Allen-Zhu. ICML 2024 Tutorial: Physics of Language Models, July
2024. Project page: https://physics.allen-zhu.com/.
[12] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B
Tenenbaum, and Evelina Fedorenko. Dissociating language and thought in
large language models.Trends in Cognitive Sciences, 28(6):517–540, 2024.
doi: 10.1016/j.tics.2024.01.011. URL https://doi.org/10.1016/j.tics.2024.01
.011.
[13] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and
Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object
identification in GPT-2 small. InThe Eleventh International Conference
on Learning Representations, 2023. URL https://openreview.net/forum?i
d=NpsVSN6o4ul.
[14] Nick Bostrom. Ethical issues in advanced artificial intelligence.Cognitive,
Emotive and Ethical Aspects of Decision Making in Humans and in Artificial
Intelligence, 2:12–17, 2003.
15
[15] Karl Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action
and behavior: a free-energy formulation. Biological Cybernetics, 102(3):
227–260, 2010.
[16] Karl Friston. The free-energy principle: a unified brain theory?Nature
Reviews Neuroscience, 11(2):127–138, 2010.
[17] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active
inference: Demystified and compared.Neural Computation, 33(3):674–712,
03 2021. ISSN 0899-7667. doi: 10.1162/neco_a_01357.
[18] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K.
Seth. The free energy principle for action and perception: A mathematical
review. Journal of Mathematical Psychology, 81:55–79, 2017. ISSN 0022-
2496. doi: 10.1016/j.jmp.2017.09.004. URL https://www.sciencedirect.co
m/science/article/pii/S0022249617300962.
[19] Paul Verschure, Cyriel MA Pennartz, and Giovanni Pezzulo. The why, what,
where, when and how of goal-directed choice: neuronal and computational
principles. Philosophical Transactions of the Royal Society of London B:
Biological Sciences, 369(1655):20130483, 2014.
[20] Paul FMJ Verschure, Thomas Voegtlin, and Rodney J Douglas. Environ-
mentally mediated synergy between perception and behaviour in mobile
robots. Nature, 425(6958):620–624, 2003.
[21] Paul FMJ Verschure. Distributed adaptive control: a theory of the mind,
brain, body nexus.Biologically Inspired Cognitive Architectures, 1:55–72,
2012.
[22] Giovanni Pezzulo, Lawrence W Barsalou, Angelo Cangelosi, Martin H
Fischer, Ken McRae, and Michael J Spivey. Computational grounded
cognition: a new alliance between grounded cognition and computational
modeling. Frontiers in Psychology, 3:612, 2013.
[23] John W Krakauer, Asif A Ghazanfar, Alex Gomez-Marin, Malcolm A
MacIver, and David Poeppel. Neuroscience needs behavior: correcting a
reductionist bias. Neuron, 93(3):480–490, 2017.
[24] Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L.
Buckley. Reinforcement learning through active inference.arXiv preprint
arXiv:2002.12636, 2020.
[25] Beren Millidge. A retrospective on active inference. Blog post, July 2024.
URL https://www.beren.io/2024-07-27-A-Retrospective-on-Active-Inferen
ce/.
[26] Richard S Sutton. The bitter lesson.Incomplete Ideas, 2019. URL http:
//www.incompleteideas.net/IncIdeas/BitterLesson.html.
16
[27] François Chollet. On the measure of intelligence. arXiv preprint
arXiv:1911.01547, 2019.
[28] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys,
Thomas Fitzgerald, and Giovanni Pezzulo. Active inference and epistemic
value. Cognitive Neuroscience, 6(4):187–214, 2015. doi: 10.1080/17588928.2
015.1020053.
[29] Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference
and agency: optimal control without cost functions.Biological Cybernetics,
106(8):523–541, 2012.
[30] Andy Clark. Surfing Uncertainty: Prediction, Action, and the Embodied
Mind. Oxford University Press, 01 2016. ISBN 9780190217013. doi: 10.109
3/acprof:oso/9780190217013.001.0001.
[31] Gary Lupyan and Emily J Ward. Language can boost otherwise unseen
objects into visual awareness.Proc Natl Acad Sci U S A., 110(35):14196–201,
2013. doi: 10.1073/pnas.1303312110.
[32] Matteo Colombo. Explaining social norm compliance. a plea for neural
representations. Phenomenology and the Cognitive Sciences, 13(2):217–238,
2014. doi: 10.1007/s11097-013-9296-0.
[33] Martin J. Pickering and Simon Garrod. Do people use language production
to make predictions during comprehension?Trends in Cognitive Sciences,
11(3):105–110, 2007. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2
006.12.002. URL https://www.sciencedirect.com/science/article/pii/S13646
61307000034.
[34] Martin J Pickering and Simon Garrod. An integrated theory of language
production and comprehension. Behavioral and Brain Sciences, 36(4):
329–347, 2013.
[35] Andreas Roepstorff and Chris Frith. What’s at the top in the top-down
control of action? script-sharing and ’top-top’ control of action in cognitive
experiments. Psychol Res., 68(2-3):189–98, 2004. doi: 10.1007/s00426-003
-0155-4.
[36] Kiyoshi Nakahara, Toshihiro Hayashi, Seiki Konishi, and Yasushi Miyashita.
Functional mri of macaque monkeys performing a cognitive set-shifting task.
Science, 295(5559):1532–1536, 2002. doi: 10.1126/science.1067653.
[37] Uri Hasson, Asif A. Ghazanfar, Bruno Galantucci, Simon Garrod, and
Christian Keysers. Brain-to-brain coupling: a mechanism for creating and
sharing a social world.Trends in Cognitive Sciences, 16(2):114–121, 2012.
ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2011.12.007. URL
https://www.sciencedirect.com/science/article/pii/S1364661311002580.
17
[38] Ilya Sutskever. Openai meta-learning and self-play, June 2018. Talk at MIT
for course 6.S099: Artificial General Intelligence. https://www.youtube.co
m/watch?v=9EN_HoEk3KY.
[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie
Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike,
and Ryan Lowe. Training language models to follow instructions with
human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh, editors,Advances in Neural Information Processing
Systems, volume 35, pages 27730–27744. Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be36
4a73914f58805a001731-Paper-Conference.pdf.
[40] Laurel J. Buxbaum. Learning, remembering, and predicting how to use
tools: Distributed neurocognitive mechanisms: Comment on osiurak and
badets (2016). Psychological Review, 124(3):346–360, 2017. doi: 10.1037/re
v0000051.
[41] Valentino Braitenberg.Vehicles: Experiments in Synthetic Psychology. MIT
Press, 1986.
[42] Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt,
Andrew Lefrancq, Laurent Orseau, and Shane Legg. Ai safety gridworlds.
arXiv preprint arXiv:1711.09883, 2017.
[43] John Schulman. Uc berkeley eecs colloquium: Reinforcement learning from
human feedback: Progress and challenges. https://www.youtube.com/watc
h?v=hhiLw5Q_UFg, Apr 2023.
18
Figure 1: Multi-Agent Active Inference Architecture showing the perception-
planning-action cycle with three execution pathways (direct execution, directed
subcontracting, and exploratory recruitment). The system processes observations
through bottom-up error feedback (perception phase in red) and top-down
predictions (planning phase in green), leading to action selection and execution
(action phase in yellow). Agents communicate through a pub-sub system while
maintaining hierarchical Markov blankets for modular safety. Human interact
with the system through natural language preferences.
19