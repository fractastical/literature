Active Inference in Hebbian Learning Networks
Ali Safa1,2,3, Tim Verbelen4, Lars Keuninckx2, Ilja Ocket2, Andr´ e Bourdoux2,
Francky Catthoor1,2, Georges Gielen1,2, and Gert Cauwenberghs3
1 imec, Leuven, Belgium
2 ESAT, KU Leuven, Belgium
3 University of California at San Diego, La Jolla, USA
4 VERSES Research Lab, Los Angeles, California, USA
Ali.Safa@imec.be
Abstract. This work studies how brain-inspired neural ensembles equipped
with local Hebbian plasticity can perform active inference (AIF) in order
to control dynamical agents. A generative model capturing the environ-
ment dynamics is learned by a network composed of two distinct Hebbian
ensembles: a posterior network, which infers latent states given the obser-
vations, and a state transition network, which predicts the next expected
latent state given current state-action pairs. Experimental studies are
conducted using the Mountain Car environment from the OpenAI gym
suite, to study the effect of the various Hebbian network parameters
on the task performance. It is shown that the proposed Hebbian AIF
approach outperforms the use of Q-learning, while not requiring any
replay buffer, as in typical reinforcement learning systems. These results
motivate further investigations of Hebbian learning for the design of
AIF networks that can learn environment dynamics without the need for
revisiting past buffered experiences.
Keywords: Active Inference · Hebbian Learning · Sparse Coding.
1 Introduction
The study of Sparse Coding [ 1], [2], [3], [4] and Predictive Coding [ 5], [6], [7]
networks has gained much attention for understanding the mechanisms underlying
learning and inference in the brain [ 8]. In particular, it has been shown that the
learning of the weight dictionary used to project the input signals into sparse
codes can be conducted via the biologically-plausible Hebbian learning mechanism
[9], with experimental evidence behind this mechanism observed in the brain
[10], [11]. Hebbian learning differs from the widely-used back-propagation of error
(backprop) technique due to its local nature [7], [12], [13], where the weight wj
of neuron i is modified via a combination f of the weight’s input xj and the
neuron’s output yi (with ηd the learning rate parameter):
wj ← −wj + ηdf(yi, xj) (1)
When applied to layers that embark some form of competition between their
neurons, the Hebbian mechanism in (1) leads to the unsupervised learning of
complementary features from the input signals [14].
At the same time, Active Inference (AIF) has gained huge interest as a
first-principle theory, explaining how biological agents evolve and perform ac-
tions in their environment [ 15], [ 16]. In recent years, the use of deep neural
arXiv:2306.05053v2  [cs.NE]  22 Jun 2023
2 A. Safa et al.
networks (DNNs) for parameterizing generative models has gained much atten-
tion in AIF research [ 17], [18], [19]. Deep AIF systems are typically composed
of a posterior network qΦP (sl|ol−1, al−1), inferring the latent state sl given an
incoming observation-action pair {ol−1, al−1}, and a state-transition network
pΦS (sl|sl−1, al−1), predicting the next latent state sl given the current state-
action pair {sl−1, al−1} [17]. The state-transition network is used to generate
the agent’s roll-outs for different policies in order to compute the Expected Free
Energy associated to each policy [ 17]. Finally, a likelihood network pΦL(ol|sl)
reconstructing the input observation ol from the latent state sl can also be
implemented [20]. Each network parameterizes its respective density function
through weight tensors ΦP , ΦS and ΦL.
In this work, we aim to study how AIF can be performed in Hebbian learning
networks without resorting to backprop (as typically used in deep AIF systems).
Experiments conducted in the OpenAI Mountain Car environment [ 21] show
that the proposed Hebbian AIF approach outperforms the use of Q-learning and
compares favorably to the backprop-trained Deep AIF system of [ 17], while not
requiring any replay buffer, as in typical reinforcement learning systems [ 22]. Our
derivations and experiments add to a growing number of work addressing the
study of Hebbian Active Inference [23], [24].
This paper is organized as follows. Background theory about Hebbian learning
networks is provided in Section 2. Our Hebbian AIF methods are covered in
Section 3. Experimental results are shown in Section 4. Conclusions are provided
in Section 5.
2 Background Theory on Hebbian Learning Networks
Inspired by previous works that model the neural activity of biological agents
through Sparse Coding [ 5], [9] (such as in the mushroom body of an insect’s brain
[25]), we model each individual Hebbian Ensemble layer of our networks as an
identically-distributed Gaussian likelihood model with a Laplacian prior on the
neural activity c:
p(c|o, Φ) ∼ exp (−||Φc − o||2
2) exp (−λ||c||1) (2)
where o is the input of dimension N, c is the output of dimension M, Φ is the
N × M weight matrix of the layer (also called dictionary), and λ is a hyper-
parameter setting the scale of the Laplacian prior. Choosing a Laplacian prior is
motivated by the fact that it promotes sparsity in the output neural code in a way
similar to how sparsity is induced in networks of Spiking Leaky Integrate-and-Fire
neurons, modelling cortical neural activity [9].
Under Sparse Coding (2), inference of c and learning of Φ is carried via [9]:
C, Φ= arg min
C,Φ
X
l
||Φcl − ol||2
2 + λ||cl||1 with C = {cl, ∀l} (3)
which can be solved via Proximal Stochastic Gradient Descent [26], by alternating
between: a) the inference of cl, given the current input ol and the weight Φ and
b) the learning of Φ, given the current cl and ol.
Hence, we instantiate Hebbian layers as the dynamical system given in (4),
where T denotes the transpose, ηc is the coding rate, ηd is the learning rate and
Proxλ||.||1 is the proximal operator to the l1 norm (non-linearity) [27]. For each
input oi, the neural and weight dynamics of the Hebbian network follows the
Active Inference in Hebbian Learning Networks 3
update rules in (4) for an arbitrary number of iterations (set to 100 in this work
as a good balance between speed of convergence and convergence quality), in
order to infer the corresponding cl and learn Φ [9].
cl ← Proxλ||.||1 {cl − ηcΦT (Φcl − ol)}
Φ ← Φ − ηd(Φcl − ol)cT
l
(4)
with Proxλ||.||1 acting as the neural non-linearity:
Proxλ||.||1 (ci) = sign(ci) max(0, |ci| −ηcλ), ∀i (5)
From a neural point of view, the dynamical system of (4) can be implemented
as the network architecture in Fig. 1, where all weight updates follow the standard
Hebbian rule (1) [9].
Fig. 1.Baseline Hebbian network architecture used in this work. The dynamics
of the network follow (4) and minimize (3), given subsequent input vectorso. Each layer
possesses its own weight matrixΦ, Ψwhich evolve through Hebbian plasticity (Ψ ∼ ΦT
in (4), as an independent, local set of weights).
3 Active Inference in Hebbian Learning Networks
In this Section, we show how the Hebbian network described above in Section 2 is
utilized in order to build an AIF system. First, we describe how Variational Free
Energy minimization can be performed by a cascade of two Hebbian networks: a
state-transition network predicting the next latent states given the previous ones,
and a posterior network providing latent states given input observations. Crucially,
it is shown that Free Energy minimization necessitates top-down Hebbian learning
connections from the state-transition network towards the posterior network,
steering the posterior output activity towards the state-transition output during
learning. Then, we show how the Expected Free Energy is computed by generating
state transition roll-outs.
4 A. Safa et al.
3.1 Minimizing the Variational Free Energy
The Variational Free Energy can be decomposed as [ 17] [28] (where E denotes
the expected value):
F = DKL[qΦP (sl|ol−1, al−1)||pΦS (sl|sl−1, al−1)] − Eq[log(pΦL(ol|sl))] (6)
with the parametrized densities qΦP , pΦS , pΦL described in Section 1. Since the
Hebbian network architecture used in this work intrinsically provides a mean to
reconstruct its input xl in (3) (i.e., likelihood modelling) from its produced latent
code cl in (3) (i.e., posterior modelling), using the same dictionary parameter
matrix Φ in (3) that was used to generate cl via (4), we have ΦL = ΦP in (6) and
we will solely use ΦP below to denote the posterior weight matrix.
Under the assumption of Gaussian likelihood with identity covariance in (2),
the KL divergence DKL in F can be simplified to [29]:
F ∼ ||ΦScS,l − {sl(ΦP ), al}||2
2 + ||ΦP sl − {ol, al}||2
2 (7)
where sl(ΦP ) explicits the dependency of sl on ΦP (sl being the posterior network
output activity) and cS denotes the output activity of the state-transition network
given sl. The Free Energy in (7) must be minimized with regard to the state
transition weights ΦS and the posterior weights ΦP during learning:
ΦS, ΦP = arg min
ΦS,ΦP
||ΦScS,l − {sl(ΦP ), al}||2
2 + ||ΦP sl − {ol, al}||2
2, ∀l (8)
This indicates that it is not only the state transition model that must be steered
towards the posterior model, but also, the posterior model must be steered
towards the output of the state-transition network. This effect can be achieved
by re-formulating the optimization in (8) as:
ΦS = arg minΦS ||ΦScS,l − {sl, al}||2
2, ∀l (a)
ΦP = arg minΦP ||ΦP sl − {ol, al}||2
2 + ||ΦP (ΦScS,l) − {ol, al}||2
2 (b) (9)
Intuitively, the right-hand term in (9 b) steers the posterior model towards
the state-transition model by first re-projecting the output activity of the state-
transition network cS into the latent space as ΦScS (considering ΦS fixed). Then,
minimizing ||ΦP (ΦScS,l) − {ol, al}||2
2 modifies ΦP in order to steer its posterior
output sl towards the re-projected state-transition activity ΦScS (considering
{ol, al} fixed).
State-Transition Model Inspired by prior work on dictionary-based sequence
modeling [30], we implement the transition model pΦS (sl|˜sl−1, ˜al−1) as an auto-
regressive Hebbian network (see Fig. 2 a), taking as input a sequence of state-and-
action history ˜sl−1 = [sl−1, . . . , sl−Lbuf ], ˜al−1 = [al−1, . . . , al−Lbuf ] and inferring
the next state ˜sl = [sl, . . . , sl−Lbuf ] as the re-projection of its internal sparse code
cS in the input space through the network weights ΦS:
˜sl = ΦScS,l (10)
where ΦS,j and cS,j respectively denote the weight vector and the sparse code
of each layer j in the state transition network. Therefore, the state-transition
network effectively projects the Lbuf previous states (noted ˜sl−1) into a common
Active Inference in Hebbian Learning Networks 5
internal sparse code cS and reconstructs the next states ˜sl by re-projection of cS
into the input space.
The state-transition network learns its weights ΦS following (9 a) and infers
its output activity cS via sparse coding (see Section 2):
ΦS, cS,l = arg min
ΦS,cS,l
||ΦScS,l − ˜sl||2
2 + λP ||cS,l||1, ∀l (11)
where λP is a parameter that sets the strength of the sparsity of the state-
transition output activity. (11) can therefore be implemented via the Sparse
Coding-based Hebbian learning ensemble described in Section 2. This auto-
regressive strategy enables the network to learn state predictions using Hebbian
learning, without the need for non-bio-plausible back-propagation through time
(BPTT) [30], [31].
In order to prevent the vanishing or exploding of the state transition model
when producing roll-outs further in time, we regularize the norm of the recon-
structed states sl to an arbitrary magnitude α using (12). We keep α = 5 in our
experiments in Section 4, giving a good balance for the dynamic range of the
network output activity (adjusted empirically).
sl = α sl
||sl||2
(12)
Fig. 2.Hebbian Active Inference Architecture. a) The state-transition network
takes as input theLbuf previous latent states produced by the posterior network, and
projects them onto its internal representationcS via the learnedΦS. When producing
roll-outs, the state-transition network estimates the next stateˆsl by re-projecting the
output activity cS due to [sl−Lbuf , . . . , sl−1] back onto the input space via (10). b) The
posterior network takes observation-action pairs as input and produces latent statess
(corresponding to the output in Fig. 1). In addition to the Hebbian mechanisms depicted
in Fig. 1, the weightsΦP of the posterior network are also subject to a top-down Hebbian
learning mechanism for minimizing the second term in (9 b).
6 A. Safa et al.
Posterior Model Similar to the state-transition model, we use a Hebbian
ensemble as posterior model, where the internal sparse code cP (see Fig. 2 b)
is identified as the hidden state sl ≡ cP,l inferred by the posterior network
qν(sl|ol−1, al−1), given the observation and action pair {ol−1, al−1} in (2).
Therefore, the posterior network learns its weights ΦP following (9 b) and
infers its output activity sl = cP,l via sparse coding (see Section 2):
ΦP , cP,l = arg min
ΦP ,cP,l
||ΦP cP,l − {ol, al}||2
2 + λQ||cP,l||1| {z }
Standard Sparse Coding
+
Top-down Connection
z }| {
||ΦP (ΦScS,l) − {ol, al}||2
2
(13)
for all l, where λQ sets the strength of the sparsity of the posterior output activity.
The left-hand standard sparse coding term in (13) can be implemented via the
Hebbian learning ensemble described in Section 2, while the right-hand term in
(13) can be implemented using top-down connections from the state-transition
output activity cS towards the posterior network, via the state-transition weight
matrix ΦS (see Fig. 2 b).
Finally, here again, we apply the homeostasis rule (12) to the inferred posterior
state, effectively constraining sl to lie on the α-sphere manifold.
3.2 Minimizing the Expected Free Energy
Given a policy π, the Expected Free Energy G(π) (EFE) can be written as [ 32]:
G(π) =
X
l
Eq(ol,sl|π)[log q(sl|π) − log p(sl, ol|π)]
=
X
l
−H{q(sl|π)} −Eq(ol,sl|π)[log p(sl, ol|π)] (14)
where H denotes the Shannon entropy. It can be seen in (14) that selecting a
policy that minimizes the EFE entails the maximization of the posterior entropy
(promoting exploration) and the joint posterior over the states and observations
(reaching the desired goal) [32].
In order to reach the desired goal, we produce roll-outs of states sl given a
certain policy and approximate the term −Eq[log p(sl, ol|π)] to be minimized as:
−Eq(ol,sl|π)[log p(sl, ol|π)] ∼ ||sl − s∗||2
2 (15)
where s∗is the desired state that the agent must reach, corresponding to a desired
observation (e.g., the agent’s position). Since the observation ol can encompass
more than just the goal to be reached (i.e., the observation ol could be both the
position and the velocity of an agent, even though the desired goal is to reach a
specific position regardless of the velocity), we compute the desired goal state s∗
as:
s∗ = arg max
s
Z
ω∈Dω
q(s|Ω∗, ω)dω (16)
where Ω∗ contains all observations that must be reached in order to attain the
desired goal and ω designates all observation modalities that are not taking part
in defining the goal that must be reached, with Dω their domain of definition.
Active Inference in Hebbian Learning Networks 7
In practice, (16) is estimated by averaging the output of the posterior network,
while sweeping ω for a grid of possible values and keeping Ω∗ fixed.
Regarding the exploration term in (14), our Hebbian network does not directly
allow the estimation of the entropy H{q(ol, sl|π)}, since the network does not
infer standard deviations as in a variational auto-encoder (VAE) [17]. We propose
to replace the maximization of the entropy H{q(ol, sl|π)} with a surrogate term,
crafted to promote exploration as well. As a surrogate for H{q(ol, sl|π)}, we
choose to maximize the variance (notedVar) of the state trajectory sl, ∀l = 1, ..., L
along time during the roll-outs. Intuitively, a state trajectory that presents lots of
variation in time will promote the exploration of new states, providing a similar
qualitative effect as maximizing H{q(ol, sl|π)}. Therefore, we select the policy π
such that the distance to the desired state is minimized, while achieving a state
trajectory variance larger than a certain threshold tv.
π∗ = arg min
π
G(π) =
LX
l=1
||sl − s∗||2
2 s.t. Var( ||sl − s∗||2
2, l= 1, ..., L) ≥ tv
(17)
Given a set of Np policies to try, tv can be determined in an adaptive way as:
tv = β × 1
2[max
π
(Var(||sl(π) − s∗||2
2, ∀l)) + min
π
(Var(||sl(π) − s∗||2
2, ∀l))] (18)
where β is the strength hyper-parameter (empirically set to 0.5 in our experiments
reported below).
4 Experimental Results
The aim of our experimental studies is to determine i) how the main network
hyper-parameters (number of neurons, sparsity in output activity,...) impact the
success rate of the proposed Hebbian AIF system; ii) to what extent Hebbian AIF
is robust when learning without using a replay buffer and iii) how Hebbian AIF
compares to Q-learning (which uses dense rewards versus unsupervised learning
in Hebbian AIF).
4.1 Mountain Car Environment
We perform experiments in the Mountain Car environment from the OpenAI
gym suite [21]. In this task, a car starts at a random position at the bottom of a
hill and is expected to reach the top of a mountain within 200 time steps. The
agent is subject to gravity and cannot reach the goal trivially, just by accelerating
towards it. Rather, the agent must learn to gain momentum before accelerating
towards the goal.
In this environment, the x-axis position x and the velocity vx of the car
constitute the input observations to the Hebbian AIF network. Before feeding
the observation tuple (x, vx) to our Hebbian network, we normalize ( x, vx) using
(19) in order to equalize the dynamic range of the position and velocity signals:
(
x ← −x−µx
σx
vx ← −x−µvx
σvx
(19)
8 A. Safa et al.
where (µx, σx) and ( µvx, σvx) denote the mean and standard deviation of the
position and velocity signals respectively (estimated during random environment
runs).
We use an action space constituted by two discrete actions:accelerate to the left
and accelerate to the right. In addition, each action is repeated for 10 consecutive
time steps once selected during the Expected Free Energy minimization in (17).
In order to compute the Expected Free Energy, we generate roll-outs of
L = 200 time step predictions for 100 different random policies πj, j= 1, ...100
with equal probability of selecting the accelerate to the left or the accelerate to
the right actions.
As learning rate for the Hebbian learning mechanism (4), we use ηd = 10−4
with a decay rate of 0.8 applied at the end of each successful episode, i.e. if the
episode terminates successfully, ηd ← −ηd × 0.8 (else no decay is applied on ηd).
All weights are initialized randomly from a normal distribution with standard
deviation 0.01.
In the remainder of this Sections, we perform all our experiments using a
10-fold validation approach, by reporting the success rate curves as averages over
10 different runs (with 35 episodes per runs), with different random network
initializations. For each run, we compute the success rate curve using a moving
average window of size 5, and report the mean success rate curve by averaging
over the 10 runs, alongside with its standard deviation (see e.g. Fig. 3). We will
now study the impact of the various network hyper-parameters on the achieved
success rates.
4.2 Impact of the Number of Neurons in the Posterior and State
Transition Networks
Fig. 3 and 4 show the effect of sweeping the number of coding neurons MQ and
MP in both the posterior and state-transition networks. Fig. 3 shows that for
MQ < 8, the success rate is sub-optimal, but reaches a steady plateau around
MQ = 8 (orange curve in Fig. 3). Then, as MQ is increased for MQ > 8, the
success rate becomes sub-optimal again, with dips in the performance along the
episodes (e.g., red curve in Fig. 3). This phenomenon can be explained as follows:
for MQ < 8, the posterior network does not have enough parameters to capture
the input dynamics into its latent space and under-fits, while for MQ > 8, the
posterior network starts over-fitting, reducing the success rate again.
Regarding the state-transition network, Fig. 4 shows that the higher the
number of neurons MP , the flatter the success rate curves become, leading to
higher performance. The state transition network does not seem to over-fit as
MP is increased (for λP = 10−4 kept fixed). Rather, Fig. 4 indicates that a higher
state-transition network capacity is beneficial for capturing important dynamics
in the latent space, at the output of the posterior network.
4.3 Impact of the Sparsity of the Output Activity in the Posterior
and State Transition Networks
Fig. 5 and 6 show the effect of sweeping the sparsity-defining hyper-parameters
λQ and λP in both the posterior and state-transition networks. For the posterior
network, Fig. 5 shows that the success rate performance initially grows as λQ
is increased from λQ = 10−6 to λQ = 10−5. Doing so, the non-linearity of the
posterior network is increased, better capturing observation features into its
Active Inference in Hebbian Learning Networks 9
Fig. 3. Impact on the success rate when changing thenumber of neurons MQ
in theposterior network.
Fig. 4. Impact on the success rate when changing thenumber of neurons MP
in thestate transition network .
10 A. Safa et al.
Fig. 5. Impact on the success rate when changing thesparsity hyper-parameter
λQ in theposterior network.
latent space. Then, as λQ grows past λQ = 10−4, the success rate degrades again,
indicating a too strong posterior network non-linearity.
Regarding the state-transition network, Fig. 6 shows that the lower λP , the
higher the success rate becomes. This suggests that making the state-transition
network more linear (i.e., lower λP ) better captures the dynamics of the latent
space produced by the posterior network (other parameters kept fixed).
4.4 Impact of the Time-Lag Buffer Length on Task Performance
Fig. 7 shows how the length Lbuf of the time-lag buffer impacts the achieved
success rate. Initially, as Lbuf increases, the success rate increases as well, due
to an increased availability of past latent states used by the state-transition
network to estimate the next expected state. Then, as Lbuf is further increased
for Lbuf > 20, the success rate drops again due to the addition of latent states
from deep in the past that are less useful for estimating the present dynamics.
4.5 Comparing Hebbian AIF against the Use of a Replay Buffer and
against Q-learning
Fig. 8 compares the success rate obtained using our proposed Hebbian AIF system
against a) the use of a replay buffer during learning and b) the use of a Q-learning
agent. Experience replay is done by saving the history of observation-action pairs
in a buffer after each episode. After the end of the episode, a past experience is
randomly selected and used to train the Hebbian AIF system for one episode.
Regarding the Q-learning setup, we use a standard Q-table learning approach
[22], with the python implementation proposed in [33].
Active Inference in Hebbian Learning Networks 11
Fig. 6. Impact on the success rate when changing thesparsity λP in thestate
transition network.
Fig. 7. Success rate when changing thetime-lag buffer length Lbuf .
12 A. Safa et al.
Fig. 8 shows that our Hebbian AIF system converges much faster than the
Q-learning system and behaves in a comparable manner to the Hebbian AIF setup
with a replay buffer. Indeed, the Q-learning agent needs much more episodes in
order to converge, despite the fact that it utilizes the dense rewards provided by
the Mountain Car environment [ 21] (versus unsupervised learning in the case of
Hebbian AIF). This confirms prior observations about the efficient convergence
of AIF systems, due to their ability to learn a generative model of environment
dynamics (versus supervised learning of a Q-table) [17].
Finally, it is interesting to note that, compared to the Deep AIF results
reported in [17] (using a fully-connected 2-hidden-layer network trained through
backprop), the Hebbian AIF system proposed in this work eventually reaches
∼ 100% success rate (see red curve in Fig. 6) while the system in [ 17] reaches
∼ 95%, motivating further investigations of Hebbian learning for AIF systems.
Fig. 8. Hebbian AIF versus the use of areplay buffer and Q-learning (a).
Q-learning needs two orders of magnitude more episodes in order to converge (b).
5 Conclusion
This paper has investigated how neural ensembles equipped with local Hebbian
plasticity can perform active inference for the control of dynamical agents. First,
a Hebbian network architecture performing joint dictionary learning and sparse
coding has been introduced for implementing both the posterior and the state-
transition models forming our generative Active Inference system. Then, it has
been shown how Free Energy minimization can be performed by the proposed
Hebbian AIF system. Finally, extensive experiments for parameter exploration
and benchmarking have been performed to study the impact of the network
parameters on the task performance. Experimental results on the Mountain Car
environment show that the proposed system outperforms the use of Q-learning,
while not requiring the use of a replay buffer during learning, motivating future
investigations of using Hebbian learning for designing active inference systems.
Active Inference in Hebbian Learning Networks 13
Acknowledgement
This research was partially funded by a Long Stay Abroad grant from the
Flemish Fund of Research - Fonds Wetenschappelijk Onderzoek (FWO) - grant
V413023N. This research received funding from the Flemish Government under
the “Onderzoeksprogramma Artifici¨ ele Intelligentie (AI) Vlaanderen” programme.
References
1. Bruno A. Olshausen, David J. Field (1997). ”Sparse coding with an overcomplete
basis set: A strategy employed by V1?.” Vision Research, 37(23), 3311-3325.
2. Fang, M.S., Mudigonda, M., Zarcone, R., Khosrowshahi, A., Olshausen, B. (2022).
”Learning and Inference in Sparse Coding Models With Langevin Dynamics.” Neural
Computation, 34(8), 1676-1700.
3. Lee, H., Battle, A., Raina, R., Ng, A. (2006). ”Efficient sparse coding algorithms.”
In Advances in Neural Information Processing Systems. MIT Press.
4. Ali Safa, Ilja Ocket, Andr´ e Bourdoux, Hichem Sahli, Francky Catthoor, Georges
Gielen. (2022). ”A New Look at Spike-Timing-Dependent Plasticity Networks for
Spatio-Temporal Feature Learning.”
5. Friston, K., Kiebel, S. (2009). ”Predictive coding under the free-energy principle.”
Philosophical transactions of the Royal Society of London. Series B, Biological
sciences, 364, 1211-21.
6. Friston, K. Does predictive coding have a future?. Nat Neurosci 21, 1019–1021 (2018).
https://doi.org/10.1038/s41593-018-0200-7
7. Umais Zahid, Qinghai Guo, Zafeirios Fountas.(2023). ”Predictive Coding as a Neu-
romorphic Alternative to Backpropagation: A Critical Evaluation.”
8. Olshausen, B., Field, D. (1996). ”Emergence of simple-cell receptive field properties
by learning a sparse code for natural images.” Nature, 381, 607-609.
9. Safa, A., Ocket, I., Bourdoux, A., Sahli, H., Catthoor, F., Gielen, G. (2022). ”Event
Camera Data Classification Using Spiking Networks with Spike-Timing-Dependent
Plasticity.” In 2022 International Joint Conference on Neural Networks (IJCNN)
(pp. 1-8).
10. Guo-qiang Bi, Mu-ming Poo (1998). ”Synaptic Modifications in Cultured Hippocam-
pal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic
Cell Type.” Journal of Neuroscience, 18(24), 10464–10472.
11. Rao, R., Ballard, D. (1999). ”Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-field effects.” Nature neuroscience,
2(1), 79–87.
12. A. Safa, I. Ocket, A. Bourdoux, H. Sahli, F. Catthoor and G. G. E. Gielen,
”STDP-driven Development of Attention-based People Detection in Spiking Neural
Networks,” in IEEE Transactions on Cognitive and Developmental Systems, 2022,
doi: 10.1109/TCDS.2022.3210278.
13. Neftci, E., Das, S., Pedroni, B., Kreutz-Delgado, K., Cauwenberghs, G. (2014).
”Event-driven contrastive divergence for spiking neuromorphic systems.” Frontiers
in Neuroscience, 7.
14. Dmitry Krotov, John J. Hopfield (2019). ”Unsupervised learning by competing
hidden units.” Proceedings of the National Academy of Sciences, 116(16), 7723-7731.
15. Parr, T., Pezzulo, G., Friston, K. (2022). ”Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior.” The MIT Press.
16. Isomura, T., Shimazaki, H. Friston, K.J. ”Canonical neural networks perform active
inference.” Commun Biol 5, 55 (2022). https://doi.org/10.1038/s42003-021-02994-2
17. C ¸atal, O., Wauthier, S., De Boom, C., Verbelen, T., Dhoedt, B. (2020). ”Learning
Generative State Space Models for Active Inference.” Frontiers in Computational
Neuroscience, 14.
18. Ueltzh¨ offer, K. ”Deep active inference.” Biol Cybern 112, 547–573 (2018).
https://doi.org/10.1007/s00422-018-0785-7
14 A. Safa et al.
19. Fountas, Z., Sajid, N., Mediano, P., Friston, K. (2020). ”Deep active inference
agents using Monte-Carlo methods.” In Advances in Neural Information Processing
Systems (pp. 11662–11675). Curran Associates, Inc..
20. Van de Maele, T., Verbelen, T., C ¸atal, O., De Boom, C., Dhoedt, B. (2021).
”Active Vision for Robot Manipulators Using the Free Energy Principle.” Frontiers
in Neurorobotics, 15.
21. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
Zaremba, W. (2016). ”Openai gym.” arXiv preprint arXiv:1606.01540.
22. Sutton, R., Barto, A. (2018 ). ”Reinforcement Learning: An Introduction.” The
MIT Press.
23. Ororbia, A. G., Mali, A. (2022). ”Backprop-Free Reinforcement Learning with
Active Neural Generative Coding.” Proceedings of the AAAI Conference on Artificial
Intelligence, 36(1), 29-37.
24. Alexander Ororbia, Ankur Mali. ”Active Predicting Coding: Brain-Inspired Rein-
forcement Learning for Sparse Reward Robotic Control Problems.” IEEE Interna-
tional Conference on Robotics and Automation (ICRA) 2023.
25. Yuchen Liang, Chaitanya Ryali, Benjamin Hoover, Leopold Grinberg, Saket
Navlakha, Mohammed J Zaki, Dmitry Krotov (2021). ”Can a Fruit Fly Learn
Word Embeddings?.” In International Conference on Learning Representations.
26. Ablin, P., Moreau, T., Massias, M., Gramfort, A. (2019). ”Learning step sizes for
unfolded sparse coding.” In Advances in Neural Information Processing Systems.
Curran Associates, Inc.
27. Tsung-Han Lin, Ping Tak Peter Tang (2019). ”Sparse Dictionary Learning by Dy-
namical Neural Networks.” In International Conference on Learning Representations.
28. Friston, K. ”The free-energy principle: a unified brain theory?.” Nat Rev Neurosci
11, 127–138 (2010). https://doi.org/10.1038/nrn2787
29. J. R. Hershey and P. A. Olsen, ”Approximating the Kullback Leibler Divergence
Between Gaussian Mixture Models,” 2007 IEEE International Conference on Acous-
tics, Speech and Signal Processing - ICASSP ’07, Honolulu, HI, USA, 2007, pp.
IV-317-IV-320, doi: 10.1109/ICASSP.2007.366913.
30. Kim, E., Lawson, E., Sullivan, K., Kenyon, G. (2019). ”Spatiotemporal Sequence
Memory for Prediction Using Deep Sparse Coding.” In Proceedings of the 7th Annual
Neuro-Inspired Computational Elements Workshop. Association for Computing
Machinery.
31. P. J. Werbos, ”Backpropagation through time: what it does and how to do it,” in Pro-
ceedings of the IEEE, vol. 78, no. 10, pp. 1550-1560, Oct. 1990, doi: 10.1109/5.58337.
32. Schwartenbeck, P., FitzGerald, T., Dolan, R., Friston, K. (2013). ”Exploration,
novelty, surprise, and free energy minimization.” Frontiers in Psychology, 4.
33. https://gist.github.com/gkhayes/3d154e0505e31d6367be22ed3da2e955 (ac-
cessed May 1 2023)