arXiv:2104.11798v1  [cs.LG]  23 Apr 2021
Realising Active Inference in Variational Message Passing :
the Outcome-blind Certainty Seeker
Th´ eophile Champion TMAC3@KENT.AC.UK
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´ s M.GRZES@KENT.AC.UK
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.BOWMAN@KENT.AC.UK
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a state-of-the-art framework in neuroscienc e that oﬀers a uniﬁed theory
of brain function. It is also proposed as a framework for planning in A I. Unfortunately, the
complex mathematics required to create new models — can impede app lication of active
inference in neuroscience and AI research. This paper addresses this problem by providing
a complete mathematical treatment of the active inference frame work — in discrete time
and state spaces — and the derivation of the update equations for any new model. We
leverage the theoretical connection between active inference an d variational message passing
as describe by John Winn and Christopher M. Bishop in 2005. Since, va riational message
passing is a well-deﬁned methodology for deriving Bayesian belief upda te equations, this
paper opens the door to advanced generative models for active inf erence. We show that
using a fully factorized variational distribution simpliﬁes the expecte d free energy — that
furnishes priors over policies — so that agents seek unambiguous st ates. Finally, we consider
future extensions that support deep tree searches for sequen tial policy optimisation — based
upon structure learning and belief propagation.
Keywords: Active Inference, Variational Message Passing, Free Energy Prin ciple, Rein-
forcement Learning, Kullback Leibler Control
1. Introduction
The free energy principle aims to provide a uniﬁed theory of t he brain based on Bayesian
probability theory (Friston, 2010; Buckley et al., 2017). I t takes root in Helmholtz’s ar-
gument that observations are produced by hidden causes that must be inferred — and
the predictive coding formulation which argues that infere nce and learning emerges from
the reduction of the error between predicted and actual obse rvations. Active inference
extends predictive coding to consider generative models of actions (Friston et al., 2016;
Da Costa et al., 2020a).
1
Champion et al.
In brief, active inference is a probabilistic framework tha t describes how agents should
act in their environment. It starts with the deﬁnition of a ge nerative (probabilistic) model
that encodes the agent’s beliefs about its environment. How ever, active inference does not
rely on one particular generative model, instead it refers t o a class of generative models
that consider the impact of their actions in their environme nt. Active inference also relies
on learning and inference to estimate the most likely states of the world and values of the
model parameters. However, the concept behind active infer ence does not dependent on a
particular inference method, which means that both variati onal inference (Fox and Roberts,
2012) and Monte Carlo Markov chains (Fountas et al., 2020) ca n, in principle, be used.
Active inference has been successfully applied in neurosci ence to explain a wide range of
brain phenomena such as habit formation (Friston et al., 201 6), Bayesian surprise (Itti and Baldi,
2009), curiosity (Schwartenbeck et al., 2018), and dopamin ergic discharges (FitzGerald et al.,
2015). Active inference is also a form of planning as inferen ce (Botvinick and Toussaint,
2012) consistent with Occam’s Razor (Blumer et al., 1987) an d can be seen as a generali-
sation of reinforcement learning (van Hasselt et al., 2015; Lample and Chaplot, 2016) and
Kullback Leibler control (Rawlik et al., 2013). This framew ork has also been used to ground
active vision (Ognibene and Baldassare, 2015; Heins et al., 2020; Van de Maele et al., 2021;
Mirza et al., 2016, 2018) within a strong theoretical framew ork.
This paper focuses on active inference using variational (a .k.a approximate Bayesian)
inference and highlights its connection to variational mes sage passing (Winn and Bishop,
2005). This ubiquitous message passing algorithm builds on the variational inference lit-
erature by leveraging the structure of the generative model to split the update equations
into messages. Those messages transmit information about t he new observations and —
by summing those messages — it is possible to compute the post erior distribution over the
parameters. The decomposition of the updates into messages formalises the modularity of
the method, while remaining biologically plausible (Frist on et al., 2017b). Indeed, a key
question in machine learning and computational neuroscien ce is how to identify composi-
tional models — an issue that was identiﬁed early in the devel opment of connectionism
(Bowman and Li, 2011; Fodor and Pylyshyn, 1988). The central requirement being that
higher-order representations (whether syntactic, semant ic, perceptual, etc) can be con-
structed by “plugging together” lower order representatio ns, in such a way that the mean-
ings of lower-order representations do not change (e.g. the “Jane” in “Jane loves John” is
the same “Jane” as in “John loves Jane”). It may be that the str uctural modularity pro-
vided by message passing implementations of Bayesian netwo rks enable compositionality of
representations. According to modern trends, we use the for malism of Forney factor graphs
(Forney, 2001) to represent the updates as messages sent alo ng the graph edges.
Forney factor graphs are graphical representations used to realise generative models.
They comprise of two kinds of round nodes that represent the o bservations and the latent
variables of the model. If the notion of observations can be u nderstood as the data available
to the model, the notion of latent variables is a bit more abst ract. As an example, let us
consider the MNIST dataset (LeCun and Cortes, 2010) compose d of images of hand written
digits. In this example, the pixels are observations made by the model and latent variables
could be any variables encoding the digit being represented , such as its orientation or size.
The last type of nodes — square nodes — represent the dependen cy between observed and
latent variables. In other words, how does the digit being re presented generate the pixels?
2
Active Inference and Variational Message Passing
The ﬁrst goal of this paper is to provide the reader with a full intuition of the mathe-
matics underlying active inference and variational messag e passing. Then, this paper shows
how to derive the update equations for any new generative mod els. The hope is to facil-
itate the development of new models that could, for example, play Atari games or model
new brain mechanisms. Finally, we use our new generative mod el to prove that the up-
date equations of active inference can be understood as vari ational message passing. This
formal proof complements previous work that frames active i nference as belief propagation
(Friston et al., 2017b) and enables us to create an automatic and modular implementation
of active inference (van de Laar and de Vries, 2019a; Cox et al ., 2019). This message pass-
ing formulation has particular consequences for the expect ed free energy, which is eﬀectively
reduced by the change, resulting in an agent that seeks certa inty, without any concern for
outcomes, whether preferred or not. We argue that the result ing behaviour may have simi-
larities to repetitive actions (sometimes called stimming ) that are common, for example, in
autism (Gabriels, 2005).
Section 2 describes the problem used to present the (classic ) model widely used in the
active inference literature. Sections 3 and 4 introduce var iational inference and Forney
factor graphs, respectively. Next, Section 5 presents acti ve inference as a decision theory
based on the Bayesian view of probability, followed by Secti on 6 that introduces the notion
of variational message passing. Then, Section 7 formulates active inference as variational
message passing under a fully factorised approximate poste rior (i.e. variational distribu-
tion), and explains the implications of this approximation for the expected free energy that
underwrites policy selection. Before starting the next sec tion, readers new to the active
inference literature might want to read Appendix D, which us es Bayes theorem to present
the simplest generative model suﬃcient for active inferenc e.
2. Problem statement
Active inference crops up in many areas that require an agent to interact with its envi-
ronment. Throughout this paper, the explanations will be ba sed on an agent named Bob,
whose goal is to solve the food problem presented in section 2 .2. But before we investigate
this problem, let us have a look at how to simulate the interac tion between Bob and his
environment.
2.1 Simulating active inference
Most living beings are able to sense their environment throu gh sensory inputs, and process
this sensory information to act in the world. For example, ca rnivorous ﬂowers use tiny
trigger hairs on their leaves to detect ﬂies (sensing). When those hairs are stimulated,
the ion concentrations in the leaves increase (processing) resulting in an electrical current
that closes the leaf trapping the ﬂy (acting). Similarly, hu mans gather sensory information
through their ﬁve senses (sensing), process this informati on to understand their environment
(processing), and ﬁnally, make use of this understanding to act with intelligence (acting).
Sensing, processing and acting correspond to the three step s of the Action-Perception cy-
cle. This cycle conveniently casts active inference as an in ﬁnite loop (van de Laar and de Vries,
2019b). Each iteration begins by sampling the environment t o obtain an observation, which
is provided to the agent. Then, the observation is used to per form inference (and learning)
3
Champion et al.
that produce a higher level of understanding, for example, a n image might be mapped to a
representation of the objects that it contains. And ﬁnally, this representation is exploited
when acting to prepare your diner, drive your kids to school o r solve your favourite maths
problem.
2.2 The food problem
Speaking of which, this section is concerned with the food pr oblem initially proposed by
Oleg Solopchuk (2018). This problem concerns an agent, name d Bob, striving to survive.
To produce the energy needed by his body, Bob needs to ingest n utriments. During periods
of starvation, Bob’s stomach produces an hormone called ghr elin. This hormone travels to
the brain through the blood and reaches a part of the brain, na med the hippocampus. This
area has been shown to monitor the level of ghrelin in the bloo d (Kojima and Kangawa,
2005). At the moment ghrelin reaches the hippocampus, Bob’s brain can estimate the
content of his stomach. This information can then be exploit ed to choose between eating
and sleeping. However, the best action depends on the outcom es that Bob wants to witness
in the future. This paper assumes that mother nature has kind ly set Bob’s preferences to
be biased towards the sensation of feeling fed (i.e. Bob enjo ys observing low levels of ghrelin
in his blood), which is arguably a favourable traits under a D arwinism view of evolution.
Figure 1 summarises the food problem.
Bob
Observation: fed vs hungry
Hidden state: full vs empty
Action: eat vs sleep
Figure 1: This ﬁgure illustrates the food problem, where the goal of our agent — Bob — is
to keep his stomach full. The ﬁrst thing Bob needs to achieve h is goal is to guess the state of
his stomach, which can either be full or empty. This guess is i nformed by the observations
he makes, when feeling hungry (high level of ghrelin) or fed ( low level of ghrelin). Finally,
once Bob has reduced his uncertainty about his stomach state , he can engage in exploitative
behaviour by taking action in his environment, such as sleep ing or eating.
4
Active Inference and Variational Message Passing
3. Variational Inference
In Bayesian statistics, one assumes a prior distribution ov er latent (a.k.a hidden) variables
that represent the process generating the data. When collec ting more data, new observations
bring information, allowing us to update our prior knowledg e. The process of computing
the most likely values of the hidden variables is called infe rence. A simple inference method
is to use Bayes theorem to obtain the posterior probability d istribution over the latent
variable(s) of the model:
P(S|O)
 
posterior
=
likelihood
  
P(O|S)
prior

P(S)
P(O)
evidence
= P(O|S)P(S)∑
S P(O|S)P(S).
Since Bayes theorem is a corollary of the product rule of prob ability and no approxima-
tion is needed, it belongs to the ﬁeld of exact inference. How ever, the computation of the
evidence requires the marginalisation over all hidden vari ables, which makes it intractable
for all but the simplest models.
To address this intractability, one can turn to approximate or sampling based methods.
Variational inference belongs to the former and relies on an assumption of independence. As
will be explained in Section 6.1, the idea behind variationa l inference is to use a distribution
Q(S) to approximate the true posterior P(S|O). This can be accomplished by minimising
the Kullback-Leibler (KL) divergence between some approxi mate and the true posterior:
DKL [ Q(S)||P(S|O)] .
Minimising this KL divergence is impossible because the tru e posterior P(S|O) is un-
known. Fortunately however, it is equivalent to minimising the variational free energy F ,
known in machine learning as the negative evidence lower bou nd (ELBO). The variational
free energy is deﬁned as the Kullback-Leibler divergence be tween the variational distribution
Q(S) and the generative model P(O,S ):
F = DKL [ Q(S)||P(O,S )] = −ELBO
= DKL [ Q(S)||P(S|O)] + ln P(O).
The variational distribution Q(S) is used to approximate the true posterior P(S|O). In
addition to the introduction of this approximate posterior , the mean-ﬁeld approximation
makes the computation tractable by assuming that all latent variables are independent:
Q(S) =
∏
i
Qi(Si),
where Qi(Si) is the distribution over the i-th hidden state of the model a nd Q(S) is the
joint distribution over all latent variables. This assumpt ion of independence constrains the
expressiveness of the variational distribution, but allow s the derivation of update equations,
which can be evaluated eﬃciently.
At this point, an analogy might be useful to furnish an intuit ive understanding of varia-
tional inference. Imagine you drop some coﬀee on a table, prod ucing a stain with a complex
5
Champion et al.
shape. To compute the area of the stain, it might be useful to ﬁ rst assume an elliptic shape
for the stain. However, since the stain is not actually ellip tic, the solution will only be
an approximation. In this analogy, the stain is the true post erior, and the ellipse is the
approximate posterior.
This analogy should help with the understanding of Figure 2 t hat illustrates the kind
of results obtained by variational methods. As will be demon strated in Section 6.2, it
is possible to prove (Fox and Roberts, 2012) that minimising the variational free energy F
with respect to Qk(Sk) can be performed by iterating one of the following update eq uations:
ln Qk(Sk) ←ln Q∗
k(Sk) = ⟨ln P(O,S )⟩∼Qk (1)
⇔ Qk(Sk) ←Q∗
k(Sk) = 1
Z exp⟨ln P(O,S )⟩∼Qk ,
where Q∗
k(Sk) is the optimal posterior, Z is a normalisation constant and ⟨•⟩∼Qk is the
expectation over all factors but Qk. Importantly, it is the coupling of the above update
equations (i.e. one update per hidden variable Sk) that justiﬁes the iteration of the updates
until convergence to the free energy minimum.
Q(S)P (S|O)
S
Figure 2: This ﬁgure illustrates the kind of result obtained using variational inference. The
true posterior drawn in red has a complex shape and is approxi mated by the variational
distribution drawn in blue. The grey area depicts the error m ade when using the variational
distribution to approximate the true posterior.
4. Forney Factor Graphs
Typically, generative models are represented graphically using a graphical model (Koller and Friedman,
2009) or Forney factor graph (Forney, 2001). This section fo cuses on the latter represen-
tation introduced by David Forney in 2001, which uses three k inds of nodes. The nodes
representing hidden and observed variables are depicted by white and grey circles, respec-
tively. And factors are represented using white squares, wh ich are linked to variable nodes
by arrows or lines. Arrows are used to connect factors to thei r target variable, while lines
link factors to their predictors. Figure 3 shows an example o f a Forney factor graph corre-
sponding to the following generative model:
P(O,S ) =
PO(O|S)PS (S). (2)
Generally, factor graphs only describe the model’s structu re — in terms of the variables
and their dependencies — but not the individual factors. For example, the deﬁnitions of
6
Active Inference and Variational Message Passing
PO and PS are not given by Figure 3, and additional information is requ ired, e.g. PS(S) =
N(S; µ,σ ) speciﬁes PS as a Gaussian distribution.
Initially, variables could only connect to a limited number of factors. However, a special
kind of factor, called an equality node, dissolves this limi tation. Purists tend to represent
all equality nodes, while others make them implicit by allow ing the variables to connect to
an arbitrary number of factors. For sake of clarity, this pap er keeps equality nodes implicit.
Finally, factors — along with hidden and observed variables — are sometimes called
constraint, state and symbol, respectively. As explained b y Yedidia (2011), those two
terminologies refer to two views on Forney factor graphs, wh ere factors encode probabilities
and constraints encode costs. Inﬁnite costs represent hard constraints, while ﬁnite costs
encode soft constraints. Here, hard constraints deﬁne whic h conﬁgurations of the state
space are forbidden (i.e. has a probability of zero) and soft constraints encode preferences
over the state conﬁgurations (i.e. the higher the cost the sm aller the state probability). This
reveals an interesting link between Bayesian statistics an d symbolic artiﬁcial intelligence,
and prompts the question of whether Bayesian statistics can be regarded as a generalisation
of symbolic artiﬁcial intelligence. For example, one could start by framing the problem of
constraint satisfaction, as an inference process on a Forne y factor graph that encodes the
problem constraints.
PS
S
PO
O
Line
Arrow
Hidden variable
Factor
Observed variable
Figure 3: This ﬁgure illustrates the Forney factor graph cor responding to the following
generative model: P(O,S ) = PO(O|S)PS (S). The hidden state is represented by a white
circle with the variable’s name at the centre, and the observ ed variable is depicted similarly
but with a grey background. The factors of the generative mod el are represented by squares
with a white background and the factor’s name at the centre. F inally, arrows connect the
factors to their target variable and lines link each factor t o its predictor variables.
5. Active Inference
So far, we have discussed variational inference and Forney f actor graphs. We now present
the intuition behind the various equations that comprise th e active inference framework.
We will be working with the food problem that was introduced i n Section 2.
7
Champion et al.
5.1 Generative model
We begin by presenting the generative model introduced by Fr iston et al. (2013). Instead
of presenting the full generative model at once, the next sub sections build this model pro-
gressively. This should help the reader to understand both t he model and its corresponding
Forney factor graph.
5.1.1 The D vector
As we shall see shortly, the full generative model represent s the world as a sequence of
hidden states, and those states generate the observations m ade by the agent. For the sake
of organisation, those states are arranged chronologicall y using the index τ that runs from
the initial state ( S0) to the state of the last time step ( ST ). This section focuses on the
initial state, whose distribution is a categorical, deﬁned as follows:
PS0 (S0|D) = Cat( S0; D), (3)
where D is a vector containing the parameters of the categorical dis tribution. In addition
to the categorical distribution, the model assumes a Dirich let prior over the parameters D,
leading to:
PD(D) = Dir( D; d). (4)
In this context, the parameters dof the Dirichlet distribution are called hyperparameters,
because they control the distribution of the parameters D. Figure 4 summarises this part
of the model by presenting an example of the vector D, and the Forney factor graph
corresponding to the two distributions constituting Bob’s generative model.
PD
D
PS0
S0
D =
states0.2
0.8
P (S0 = empty)
P (S0 = full)
Figure 4: This ﬁgure illustrates the vector D that deﬁnes Bob’s beliefs about the initial
hidden state, and the Forney factor graph corresponding to ( 3) and (4). Since the probability
of S0 being full is higher than the probability of it being empty, B ob thinks that at the
beginning of each trial, his stomach is more likely to be full than empty.
8
Active Inference and Variational Message Passing
5.1.2 The A matrix
We have already mentioned that the probability of an observa tion (a.k.a outcome), such
as feeling hungry, depends on the value of the hidden state, i .e. whether Bob’ stomach is
full or empty. This dependency is represented by a condition al distribution, such that the
likelihood of an observation — given a particular value of th e hidden states — is deﬁned
by a categorical distribution, as follows:
POτ (Oτ |Sτ = j, A) = Cat( Oτ ; A•j ),
where the j-th column of A, denoted A•j, contains the parameters of the categorical distri-
bution encoding the probability of the outcomes given that Sτ = j. Additionally, we can
re-write the above equation more concisely by letting Sτ be a one hot vector, whose j-th
element is equal to one, such that:
POτ (Oτ |Sτ , A) = Cat( Oτ ; ASτ ),
where because Sτ is a one hot vector, the multiplication of A and Sτ selects the j-th column
of A. Similarly to the treatment of the vector D, a prior over the columns of A is used. To
ensure the conjugacy between the distributions of the model , a Dirichlet prior is used for
each column. The probability of the overall matrix is then gi ven by the following product
of Dirichlet:
PA(A) =
∏
i
Dir(A•i; a•i),
where ais a matrix containing the parameters of the Dirichlet distr ibutions, i.e., each column
of a contains the parameters of one Dirichlet distribution. Not e that because each column
of the matrix A is a categorical distribution, then the conjugate prior of e ach column is
a Dirichlet distribution. Assuming independence of the col umns of A, the conjugate prior
of the entire matrix A is a product of Dirichlet distributions. Importantly, the p rior over
A is not a Dirichlet distribution whose parameters are obtain ed by concatenation of the
columns of A. Indeed, if we sample from such a (concatenated) prior, then the elements
of the entire matrix will sum up to one but the columns would no t. This is problematic
because each column of A is supposed to be a categorical distribution that sum up to on e.
We conclude this section with Figure 5 that illustrates the l ikely matrix A, along with the
resulting version of the generative model for Bob’s problem .
9
Champion et al.
PA A PO0
O0
S0
PS0
D
PD
0.9
0.1
0.2
0.8
P (Oτ = hungry|Sτ = full)
P (Oτ = fed|Sτ = full)
P (Oτ = hungry|Sτ = empty)
P (Oτ = fed|Sτ = empty)
A =
outcomes
states
Figure 5: This ﬁgure illustrates the matrix A that deﬁnes how the hidden states generate
the observations. In our example with Bob, this matrix deﬁne s the probability of Bob
feeling hungry or fed while his stomach is full or empty. Furt hermore, the new version of
the generative model is shown on the right.
5.1.3 The B matrices
Now that the reader is familiar with the deﬁnition of the like lihood matrix A, we focus on the
temporal transitions between any pair of successive states . Those transitions are modelled
similarly to the matrix A that concerns the generation of observations from hidden st ates.
However here, we are concerned with the transition matrices that maps from states at one
point on time to the next. Crucially, there are as many of thes e matrices as the number
of allowable actions on the state in question. This follows f rom the idea that each action
has the potential to modify Bob’s stomach diﬀerently: for exa mple, eating is more likely
to change Bob’s stomach from empty to full than sleeping. Acc ordingly, the transition
between two consecutive hidden states is deﬁned by a set of ma trices, called the transition
or B matrices, such that:
PSτ +1 (Sτ+1|Sτ = i,π = j, B) = Cat( Sτ+1; B[Uj
τ ]•i)
=∆ Cat(Sτ+1; B[U]•i), (5)
where =∆ means equal by deﬁnition, U =∆ Uj
τ is the action predicted at time step τ by the
j-th policy, and B[U] is the matrix corresponding to the action U. Furthermore, active
inference deﬁnes policies as action sequences (cf. next sec tion). By replacing the index iby
a one hot vector as in the previous section, Equation 5 can be r e-written as:
PSτ +1 (Sτ+1|Sτ ,π, B) = Cat( Sτ+1; B[U]Sτ ).
A Dirichlet prior is assumed for each column of the transitio n matrices B, leading to
the following prior:
PB(B) =
∏
i,j
Dir(B[i]•j; b[i]•j ),
where b are the parameters of the Dirichlet distributions, i and j iterate over all possi-
ble actions and states, respectively. Finally, Figures 6 an d 7 conclude this subsection by
illustrating the matrices B, and the updated version of the generative model.
10
Active Inference and Variational Message Passing
0.1
0.9
0.2
0.8
P (Sτ+1 = empty|Sτ = empty)
P (Sτ+1 = full|Sτ = empty)
B[eating] =
states
states
0.9
0.1
0.8
0.2
P (Sτ+1 = empty|Sτ = full)
P (Sτ+1 = full|Sτ = full)
B[sleeping] =
states
states
Figure 6: This ﬁgure illustrates the matrices B that deﬁne the transition between any
two consecutive hidden states. In the context of the food pro blem, those matrices encode
the probability of transitioning from a full or empty stomac h at time τ to a full or empty
stomach at time τ+ 1.
S0
PS0
... PSt St ... PST ST
PO0
O0
POt
Ot
A
PA
D
PD
B
PB
π
Pπ
Figure 7: This ﬁgure shows the next version of the generative model, where the transition
between hidden states is speciﬁed by a set of B matrices and the policies π. At this point,
it should be emphasized that the generation of outcomes thro ugh the matrix A stops after
the current time step t. This follows naturally from the idea that we cannot observe future
outcomes. Finally, the factor Pπ has not been deﬁned yet: it will be the subject of the next
section.
5.1.4 The prior over policies
We now consider the prior over the policy that was left undeﬁn ed in Figure 7. But what do
we exactly mean by policies? In active inference, a policy is a sequence of actions over time,
i.e. {Ut,...,U T −1}. As a consequence, even if the agent expects the environment to be in
the same state at two diﬀerent time steps, picking two diﬀerent actions at those time steps
is still possible. Therefore, an active inference agent can perform an epistemic action as
long as there is some uncertainty to be reduced and then switc h to exploitative behaviours.
Note that this deﬁnition of policy is in opposition to most of the model-free reinforcement
learning literature, where a policy is a mapping from states to actions. In particular, states
11
Champion et al.
in the context of model-free reinforcement learning are obs erved and therefore are closer to
the notion of observations in active inference. Technicall y, active inference takes us out of
the world of ﬁxed state-action policies (where the same acti on is taken from each state) into
the world of sequential policy optimisation, where diﬀerent actions can be taken from the
same state — crucially, in a way that depends upon (Bayesian) beliefs about hidden states.
The last ingredient required to obtain the prior over the pol icies is a notion of policy
quality. In active inference, good policies are the ones tha t minimise the expected free
energy; that is, the free energy expected in the future, whic h is deﬁned as follows:
G(π) ≈
T∑
τ=t+1
[
DKL[
expected outcomes
  
Q(Oτ |π) ||
prior preferences
  
P(Oτ ) ]  
risk
+ EQ(Sτ |π )[H[P(Oτ |Sτ )]]  
ambiguity
]
, (6)
where H[ ·] is the Shannon entropy, G is a vector containing as many elements as the number
of policies, and the i-th element of G represents the quality of the i-th policy. The reader
interested in the derivation of the expected free energy is r eferred to Appendix C. We should
mention here that Q(Oτ |π) and Q(Sτ |π) are computed based on the result of the inference
process of the previous action-perception cycle. Therefor e, G can be regarded as a model
parameter and is not represented as a random variable in the F orney factor graph. The
deﬁnition and justiﬁcation of the expected free energy are p rovided in Appendix C and a
recent paper by Millidge et al. (2020). Also, the expected fr ee energy arises naturally in
mathematical treatments of the free energy principle, when considering self-organisation at
non-equilibrium steady-state (Friston, 2019; Parr et al., 2020). At this point, we should
take a moment to understand the intuition behind the expecte d free energy.
Let us begin with the second term of Equation 6. For each value of the hidden state,
P(Oτ |Sτ = i) is a categorical distribution whose parameters correspon d to the i-th column
of A. This distribution deﬁnes the probability of future outcom es. Thus, the closer this
distribution is to a uniform distribution, the more uncerta in we are about future outcomes.
This uncertainty is measured by the Shannon entropy, and the average of this quantity over
all possible values of Sτ is called the ambiguity. Therefore, the ambiguity quantiﬁe s the
degree to which a particular observation disambiguates amo ng its hidden or latent causes.
Next, we need to encode Bob’s preferences over future outcom es, which are called prior
preferences. Formally, those preferences are deﬁned as a ca tegorical distribution whose
parameters are stored in the vector C. Figure 8 illustrates this vector. It should be noted
that those preferences deﬁne the goodness of future outcome s, and we shall come back
to this when discussing the link between active inference an d reinforcement learning, cf.
Appendix A.
12
Active Inference and Variational Message Passing
C =
0.2
0.8
P (Oτ = hungry)
P (Oτ = fed)
Figure 8: This ﬁgure illustrates the vector C that deﬁnes Bob’s prior preferences over future
outcomes. This vector corresponds to the case where Bob pref ers to feel fed rather than
hungry, and the intensity of those preferences can be change d by tweaking the probabilities
of the vector C. For example, C = (0 , 1) corresponds to an extreme preference towards
feeling fed.
To conclude, we need to consider the predicted or expected ou tcomes. One way to
predict future outcomes would be to compute the marginal dis tribution over Oτ using for
example the sum product algorithm (Kschischang et al., 2001 ). However, this might be
computationally expensive, so we will proceed with the foll owing formula:
Q(Oτ |π) =
∑
i
P(Oτ |Sτ = i,A )Q(Sτ = i|π) = Asπ
τ ,
where as will be discussed in Section 5.2, Q(Sτ |π) =∆ Cat(Sτ ; sπ
τ ). This equation can be
understood as a form of marginalization, where the approxim ate posterior Q(Sτ |π) is our
most informed belief about the hidden states. Finally, the K L divergence between the
expected outcomes and the prior preference is called risk (c f. Appendix A for additional
details). The risk part of expected free energy is simply the divergence between the expected
outcomes and the preferred outcomes. It is this part of expec ted free energy that underwrites
policies that lead to preferred outcomes under uncertainty . Minimising expected free energy
therefore minimises risk (i.e., the divergence between ant icipated and preferred outcomes)
and ambiguity (i.e., the conditional uncertainty about out comes, given the causes). The
resulting prior over the policies is deﬁned as:
Pπ (π|γ) = σ(−γG),
where σ(·) is the softmax function, G is the expected free energy, γdetermines the sensitivity
of policy selection to the expected free energy of each polic y, and the negative sign gives
high probability to policies minimising expected free ener gy. Importantly, the prior over
policies is an empirical prior because the expected free ene rgy depends on the observations,
which means that it must be re-evaluated each time a new obser vation is made by the
agent. In other words, the prior over the policies is a Boltzm ann distribution with γ being
the inverse temperature. Taking this view, small values for γ means a high temperature
and less precise prior beliefs about which policy should — or is — being pursued. Figure 10
shows an example of this distribution and Figure 9 illustrat es the current generative model.
13
Champion et al.
S0
PS0
... PSt St ... PST ST
PO0
O0
POt
Ot
A
PA
D
PD
B
PB
π
Pπ
γ
Pγ
Figure 9: This ﬁgure illustrates the Forney factor graph of t he entire generative model
of the sort presented by Friston et al. (2016). Section 5.1.1 described how the probability
of the initial states is deﬁned by the vector D, and as discussed in Section 5.1.2, the
matrix A deﬁnes the probability of the observations given the hidden states. Section 5.1.3
explained that the B matrices deﬁne the transition between any successive pair o f hidden
states. This transition depends on the action performed by t he agent, i.e. on the policy π.
Furthermore, the prior over the policies has been chosen in S ection 5.1.4, such that policies
minimising the expected free energy are more probable. Fina lly, we see in section 5.1.5 that
the precision parameter γ (which modulates the stochasticity of the agent behaviour) is
distributed according to a gamma distribution.
π1 = { U1
t : eat, U 1
t+1 : eat }
π2 = { U2
t : eat, U 2
t+1 : sleep}
π3 = { U3
t : sleep, U 3
t+1 : eat }
π4 = { U4
t : sleep, U 4
t+1 : sleep}
policies
P (π|γ)
π1 π2 π3 π4
0
0. 125
0. 25
0. 375
0. 5
Figure 10: A distribution over the policies that gives high p robability to policies fulﬁlling
Bob’s preferences in the future. For example, the ﬁrst polic y where Bob is constantly
eating has high probability, while the fourth policy where B ob is constantly sleeping has
low probability. This is congruent with the notion that eati ng is more likely to make Bob
feel fed than hungry, and similarly, sleeping is more likely to make Bob feel hungry than
fed.
14
Active Inference and Variational Message Passing
5.1.5 The prior over the precision parameter
We now turn to the last part of the generative model, i.e. the p rior over the precision param-
eter γ. Importantly, this precision parameter has been associate d with the neuromodulator
dopamine through what is called the “precision hypothesis” (FitzGerald et al., 2015). This
association of dopamine and the precision parameter claims to unify two perspectives on
the role of dopamine. The ﬁrst frames dopamine as an error sig nal on predicted reward
(Schultz et al., 1997) and uses the framework of TD-learning . The second, called the in-
centive salience hypothesis, frames dopamine as “associat ing salience and attractiveness to
visual, auditory, tactile, or olfactory stimuli” (Berridg e, 2007).
But, let us come back to the prior over the precision paramete rs γ. In neurobiological
treatments, this prior usually takes the form of a gamma dist ribution with a rate parameter
β and a shape parameter ﬁxed to one:
Pγ (γ) = Γ( γ; 1,β ).
The graph on the right of Figure 11 illustrates two variation s of this prior for β = 1 and
β = 2. Also, we should mention that a more ﬂexible prior can be ob tained by removing the
constraint on the shape parameter (Friston et al., 2015), an d the left hand side of Figure 11
illustrates this extension. However, in most artiﬁcial int elligence applications (that are not
concerned with biological implementation or dopamine), γ is usually assumed to be one.
Mainly, this design choice is made for the sake of simplicity , even if in practice forcing γ to
be one reduces the model ﬂexibility, i.e. γ can no longer be learnt.
Γ( γ; 2, 0. 5)
Γ( γ; 2, 1)
0 2 4 6
γ
Γ( γ; 1, 1)
Γ( γ; 1, 2)
0 2 4 6
γ
Figure 11: This ﬁgure illustrates four gamma distributions where the values of the parame-
ters have been changed. The graph on the right shows the kind o f prior the model believes
in by forcing the shape parameter to equal one.
15
Champion et al.
Notation Meaning
T The time horizon
t The current time steps
τ An iterator over time step
O0:t The sequence of observations between time step 0 and t
S0:T The sequence of hidden states between time step 0 and T
π The policies
Um
τ =∆ U The action or control state predicted by the m-th policy at ti me step τ
A The matrix deﬁning the likelihood mapping from the hidden st ates to the
observations
A•i The i-th column of the matrix A
B The set of transition matrices deﬁning the mappings between any two consecutive
hidden states
B[U]•i The i-th column of the transition matrix B[U] corresponding to action U
D The prior over the initial hidden states
a, b, d The parameters of the prior over A, B and D
a•i The i-th column of the matrix a
b[U]•i The i-th column of the matrix b[U] corresponding to action U
γ The precision parameter related to neuromodulators such as dopamine
σ(x) The softmax function
G The expected free energy
Γ(γ; α,β ) Gamma distribution with shape and inverse scale parameter s α and β
Cat(S0; D) Categorical distribution over S0 with parameter D
Dir(D; d) Dirichlet distribution
Table 1: Generative Model notation
16
Active Inference and Variational Message Passing
5.1.6 The entire generative model
Throughout this section, we have assembled incrementally t he generative model usually
used in active inference, whose Forney factor graph is repre sented in Figure 9. The last step
is to write down the equations that constitute its formal deﬁ nition:
P(O0:t,S 0:T ,π, A, B, D,γ ) = P(π|γ)P(γ)P(A)P(B)P(S0|D)P(D)
t∏
τ=0
P(Oτ |Sτ , A)
T∏
τ=1
P(Sτ |Sτ−1, B,π ), (7)
where:
P(π|γ) = σ(−γG) P(γ) = Γ( γ; 1,β )
∏
i
P(A) =
∏
i
Dir(A•i; a•i) P(B) =
∏
i,j
Dir(B[i]•j ; b[i]•j)
P(S0|D) = Cat( S0; D) P(D) = Dir( D; d)
∏
i
P(Oτ |Sτ , A) = Cat( Oτ ; ASτ ) P(Sτ |Sτ−1, B,π ) = Cat( Sτ ; B[U]Sτ−1)
∏
i
Note that to keep the notation uncluttered, we have dropped t he subscripts such that
PS0 (S0|D) becomes P(S0|D), PA(A) becomes P(A) and so forth. Table 1 provides a
complete description of the notation used to deﬁne the gener ative model.
5.2 Variational Distribution
We now turn to the deﬁnition of the variational distribution , which is used to approximate
the true posterior during variational inference (a.k.a app roximate Bayesian inference), i.e.
Q(x) ≈P(x|o) where x and o denote the hidden variables and the observations, respec-
tively. Let us ﬁrst recall that variational inference lever ages independence between latent
variables in what is known as a mean-ﬁeld approximation. A st ructured approximation,
often made in the active inference literature 1 to simplify computations is that all latent
variables are independent except for the hidden states and t he policy. This leads to the
following variational distribution:
Q(S0:T ,π, A, B, D,γ ) = Qπ (π)QA(A)QB(B)QD(D)Qγ (γ)
T∏
τ=0
QSτ (Sτ |π), (8)
where:
QSτ (Sτ |π) = Cat( Sτ ; sπ
τ ) Qπ (π) = Cat( π; π )
Qγ (γ) = Γ( γ; 1, β ) QD(D) = Dir( D; d)
QA(A) =
∏
i
Dir(A•i; a•i) QB(B) =
∏
i,j
Dir(B[i]•j; b[i]• j )
1. An instance where this general assumption is not made can be found in (Parr et al., 2019).
17
Champion et al.
Once again, for the sake of compactness, the subscript will b e dropped, e.g. QSτ (Sτ |π)
will be replaced by Q(Sτ |π). Table 2 summarises the notation used to deﬁne this variati onal
distribution. It is much easier to understand this distribu tion by comparing it to the
deﬁnition of the generative model in Equation 7. Indeed, the distributions over A, B
and D remain Dirichlet distributions, and the distributions ove r γ and Sτ remain gamma
and categorical distributions, respectively. Only the dis tribution over π changes from a
Boltzmann to a categorical distribution. However, both the Boltzmann and the categorical
are discrete distributions.
Notation Meaning
sπ
τ The parameters of the posterior over Sτ for each policy, i.e. a vector
s
•
τ The parameters of the posterior over Sτ for all policies, i.e. a matrix
π The parameters of the posterior over π, i.e. a vector
a, b, d The parameters of the posterior over A, B and D, i.e. a matrix,
a set of matrices and a vector, respectively
β The (inverse temperature) parameter of the posterior over γ
Table 2: Variational distribution notation
5.3 Variational Free Energy
Above, we have unpacked the generative model and variationa l distribution used in active
inference. This section combines those two concepts to form the second cornerstone of the
active inference framework, i.e. the variational free ener gy. Section 6.1 will explain how
the following equation can be derived from the Kullback-Lei bler divergence between the
variational distribution and the true posterior. However, this section explains the intuition
behind the variational free energy, which is deﬁned as follo ws:
F = EQ[ln Q(S0:T ,π, A, B, D,γ ) −ln P(O0:t,S 0:T ,π, A, B, D,γ )]
= DKL [ Q(x)||P(x|o)]
 
relative entropy
− ln P(o)  
log evidence
, (9)
where x = {S0:T ,π, A, B, D,γ }refers to the model’s hidden variables, and o = {O0:t}
refers to the sequence of observations made by the agent. Equ ation 9 highlights some
important properties of the variational free energy. Indee d, the relative entropy (a.k.a KL
divergence) ensures that the variational distribution Q(x) tends to get closer to the true
posterior P(x|o), as the free energy is reduced. Furthermore, it shows that t he variational
free energy is an upper bound on the negative log evidence, be cause the relative entropy
cannot be negative. Also, if the variational distribution i s equal to the true posterior, then
the variational free energy is equal to the (-ve) log evidenc e. The variational free energy
can also be re-arranged as:
F = DKL [ Q(x)||P(x)]  
complexity
−EQ(x)[ln P(o|x)]  
accuracy
, (10)
18
Active Inference and Variational Message Passing
showing the trade-oﬀ between complexity and accuracy. The c omplexity penalises the
divergence of the posterior Q(x) from the prior P(x). The accuracy scores how likely the
observations are given the generative model and current bel ief of the hidden states. Inter-
estingly, in opposition to the Akaike information criterio n (AIC) and Bayesian information
criterion (BIC), the complexity does not depend on the numbe r of parameters. Conse-
quently, a model with a lot of parameters, but that does not va ry from the prior will have
zero complexity, and a model with a small number of parameter s that moves away a lot
from the prior will have a large complexity. Taking this view , a model is complex whenever
the knowledge encoded by the prior fails to explain the obser ved data accurately. In other
words, complexity scores the degree of belief updating that moves posterior beliefs away
from prior beliefs to provide an accurate account of any obse rvations.
Comparison of the expression for expected free energy and va riational free energy re-
veals an intimate relationship. One can see that the risk is t he expected complexity, while
ambiguity is expected inaccuracy. These expectations are u nder the posterior predictive
beliefs about outcomes in the future under the policy in ques tion. This is why G is called
expected free energy.
5.4 Update equations
All the update equations presented below come from the minim isation of the variational
free energy. This section presents the intuition behind tho se updates using the notations
summarized in Table 3. Let us start with the optimal updates o f A, B and D that are
given by:
Q∗(D) = Dir( D; d) where d = d+ s0 (11)
Q∗(A) =
∏
i
Dir(A•i, a•i) where a = a+
t∑
τ=0
oτ ⊗sτ (12)
Q∗(B) =
∏
u,i
Dir(B[u]•i, b[u]•i) where b[u] = b[u] +
∑
(k,τ )∈Ω u
sk
τ ⊗sk
τ−1π k (13)
Looking at the above equations, these updates can be underst ood as counting the number
of times an event appears. For example, the update of A counts the number of times a
pair of states-observations have been observed. Taking thi s view, a is the pseudo count
of previously occurring states-observations pairs, and oτ ⊗sτ takes into account the new
observations. Similarly, the update of the B and D matrices, respectively count how many
times the state transitions and initial states have been obs erved. Additionally, the updates
of the hidden states are:
Q∗(S0|π) = σ
(
¯D + I(0 ≤t) o0 · ¯A + sπ
1 ·¯B[Uπ
0 ]
)
T∑
i
(14)
Q∗(Sτ |π) = σ
(
¯B[Uπ
τ−1]sπ
τ−1 + I(τ ≤t) oτ · ¯A + sπ
τ+1 ·¯B[Uπ
τ ]
)
(15)
Q∗(ST |π) = σ
(
¯B[Uπ
T −1]sπ
T −1
 
past or prior
+ I(T ≤t) oT · ¯A  
likelihood
+ ¯B[Uπ
T −1]  
future
)T∑
(16)
19
Champion et al.
Notation Meaning
A⊗B = ABT , A·B = AT B outer and inner products
/llbracketa,b /rrbracket all the natural numbers between a and b
Ω u =
{
(k,τ ) : Uk
τ−1 = u,τ ∈/llbracket1,T /rrbracket
} all ( k,τ ) such that the k-th policy predicts
action u at time τ−1
sτ = s
•
τ ·π the expected state at time τ
⟨f(X)⟩PX =∆ EPX [f(X)] is the expectation of f(X) over PX
ψ(x) the digamma function used to compute
analytical solutions, e.g. for ⟨ln Di⟩QD .
¯Di = ⟨ln Di⟩QD = ψ(di) −ψ(∑
i di) the expected logarithm of D
¯Aij = ⟨ln Aij ⟩QA = ψ(aij ) −ψ(∑
k akj ) the expected logarithm of A
¯B[u]ij = ⟨ln B[u]ij ⟩QB = ψ(b[u]ij ) −ψ(∑
k b[u]kj ) the expected logarithm of B
Table 3: Update equations notation
where t can be thought of as a global variable referring to the presen t time point, and I(•)
is an indicator function that equals one if the condition is t rue and zero otherwise. A closer
look at these updates reveals that the hidden states are upda ted by gathering information
from the past, the future, and the likelihood mapping. In Equ ation 14, the information
from the past is replaced by some information from the prior o ver the initial state, and in
Equation 16, the information from the future disappears bec ause we have reached the limits
of the time horizon (i.e. τ== T). Similarly, in Equations 15 and 16, the indicator function
ensures that there is no information from the likelihood map ping after the current time
step t because no observations are available. For additional info rmation about the above
updates, the reader is referred to Sections 7.7 and 7.8 as wel l as Appendix G. Interestingly,
Parr and Friston (2018) proposed a model in which future obse rvations are latent variables,
and in this case, information will be sent along the edges con necting future states and future
observations. Finally, the update of γ and π takes the following form:
Q∗(γ) = Γ
(
γ; 1,β + G ·(π −π 0)
)
Q∗(π) = σ
(
−1
β G −F
)
where π 0 = σ(−γ·G), σ(·) is the softmax function, and Fis a vector whose π-th element
is deﬁned as:
Fπ = sπ
0 ·(ln sπ
0 −¯D) +
T∑
τ=1
sπ
τ ·(ln sπ
τ −¯B[U]sπ
τ−1) −
t∑
τ=0
oτ · ¯Asπ
τ .
Section 7 will derive update equations similar to those abov e that can be decomposed as a
sum of messages coming from the parent, children and co-pare nts of each node.
20
Active Inference and Variational Message Passing
5.5 Action selection
This section focuses on the various strategies available to pick the next action(s) that the
agent will then perform. In active inference, the action sel ection process is performed
after iteration of the update equations. Indeed, according to the Action-Perception cycle
presented in Section 2, the agent ﬁrst minimises the variati onal free energy and then acts in
its environment. The ﬁrst strategy entails summing the post erior evidence for the policies
predicting each action, and to execute the action with the hi ghest sum of posterior evidence:
u∗
t = arg max
u
|π |∑
m=1
δu,U m
t Q(π = m),
where |π|is the number of policies, Um
t is the action predicted at the current time step
by the policy π, and δu,U m
t is an indicator function that equals one if u = Um
t and zero
otherwise. Since the model knows the posterior over the poli cies (i.e. sequences of actions)
another strategy is to simply sample an entire policy (e.g. a sequence of actions) without
re-computing the posterior at each timestep, i.e. Bob selec ts a policy, closes his eyes and
performs the sequence of actions entailed by that multi-ste p policy. In the case of single-
step policies, this is equivalent to the ﬁrst strategy. This leads to a trade-oﬀ between
computational time and quality of the actions selected. Ind eed, the more actions selected
at once, the less computational time required, but the less i nformed those actions will be.
Another strategy used in planning is called a Monte Carlo tre e search (Browne et al.,
2012). The most well-known example of Monte Carlo tree searc h is probably the victory of
AlphaGo against Lee Sedol — the go world champion — in 2016 (Si lver et al., 2016). Inter-
estingly, this method has been used recently with an active i nference agent (Fountas et al.,
2020). The simplest version of this algorithm starts with an empty tree, i.e. a single node
representing the current state. Then, the root node is expan ded such that the states that
are reachable from the current state become its children. Th ose children are linked to the
root node by edges representing the actions leading to those states. Afterwards, simulations
of the environment are run to evaluate how good those new chil d states are. In the context
of reinforcement learning, the goodness of the states corre sponds to whether or not reward-
ing terminal states are reached during the simulations. Sim ilarly, in the context of active
inference, the expected free energy scores the goodness of o utcomes. Finally, the reward or
EFE is back-propagated upward in the tree. Iterating this fo ur-steps process (i.e. selection,
expansion, simulation and backpropagation) furnishes a po sterior over the best action to
perform next.
6. Variational Message Passing
In the previous sections, our focus was on explaining the int uition behind active infer-
ence. The current section is more technical. We begin with th e KL divergence between
the variational distribution Q(x) and the true posterior P(x|o), which underwrites the min-
imisation of the variational free energy. Then, we derive tw o update equations well known
from the Bayesian statistics community. The ﬁrst explains h ow the approximate poste-
rior can be computed using variational inference. And the se cond reveals that the optimal
posterior can be thought of as a sum of messages. Finally, the message based equation is
21
Champion et al.
specialised for the class of exponential conjugate models t hat we use to describe the method
of Winn and Bishop (2005) as a ﬁve-step process. During this s ection, we will be using a
few properties that are summarised in Appendix B.
6.1 Justiﬁcation of the Variational Free Energy
As mentioned in Section 3, the computation of the true poster ior — using Bayes theorem
quickly becomes intractable as the number of hidden states i ncreases. The variational free
energy (VFE), or equivalently, the negative evidence lower bound (-ELBO), aims to solve
this intractability problem by approximating the true post erior with another distribution:
the variational distribution. To justify the use of the vari ational free energy, let us ﬁrst note
that the following expression can be obtained from the produ ct rule:
P(x|o) = P(o,x )
P(o) . (17)
Since the KL divergence measures the distance between two di stributions, we can min-
imise the KL divergence between the variational distributi on and the true posterior. And
this will keep the variational distribution close to the tru e posterior. Starting with this KL
divergence, and substituting Equation 17 within it, we obta in:
DKL [ Q(x)||P(x|o)] = DKL [ Q(x)||P(x,o )] + EQ(x)[ln P(o)]
= DKL [ Q(x)||P(x,o )]  
VFE = -ELBO
+ ln P(o)  
log evidence
,
where the expectation over the log evidence can be dropped du e to the lack of a depen-
dence of ln P(o) on Q(x). Because the log evidence does not depend on the latent vari ables,
it can be safely ignored during the minimisation process. In other words, minimising the
variational free energy is equivalent to minimising the KL d ivergence between the varia-
tional distribution and the true posterior, and ensuring th at the variational distribution is
a good approximation of the true posterior.
6.2 Variational Inference Updates
As we have just noted, variational methods rely on the minimi sation of the variational free
energy, or equivalently, the maximisation of an evidence lo wer bound. So, let us start with
the former:
DKL [ Q(x)||P(o,x )] = EQ(x)[ln Q(x) −ln P(x,o )].
Using the mean-ﬁeld assumption Q(x) = ∏
i Qi(xi), the log property, and the linearity
of expectation. The above equation can be rewritten as:
DKL [ Q(x)||P(o,x )] = EQ(x)[ln Qk(xk)] + EQ(x)[ln
∏
j̸=k
Qj(xj )] −EQ(x)[ln P(x,o )].
Note that ln Qk(xk) is a constant w.r.t all factors but Qk(xk), and ln ∏
j̸=k Qj(xj ) is
a constant w.r.t Qk(xk). Using the expectation of a constant, the above equation ca n be
22
Active Inference and Variational Message Passing
rewritten as:
DKL [ Q(x)||P(o,x )] = EQk(xk)[ln Qk(xk)] + E∼Qk(xk)[ln
∏
j̸=k
Qj (xj)] −EQ(x)[ln P(x,o )],
where E∼Qk(xk)[·] is the expectation over all factors but Qk(xk). If the goal is to minimise
the free energy w.r.t Qk(xk), the second term can be safely considered as a constant C.
Also, using the factorisation of the variational distribut ion, the third term can be rewritten
as EQk(xk)[E∼Qk(xk)[ln P(x,o )]], leading to:
DKL [ Q(x)||P(o,x )] = EQk(xk)[ln Qk(xk)] −EQk(xk)[E∼Qk(xk)[ln P(x,o )]] + C
= EQk(xk)
[
ln Qk(xk) −E∼Qk(xk)[ln P(x,o )]
]
+ C
=∆ EQk(xk)
[
ln Qk(xk) −ln Q∗
k(xk)
]
+ C
= DKL [ Qk(xk)||Q∗
k(xk)] + C,
where =∆ means equal by deﬁnition, and ln Q∗
k(xk) =∆ E∼Qk(xk)[ln P(x,o )]. The KL diver-
gence can not be negative which means that Qk(xk) = Q∗
k(xk) minimises the free energy,
and for this reason Q∗
k(xk) is called the optimal posterior.
6.3 Variational Message Passing Updates
Restarting with the deﬁnition of Q∗
k(xk) and using the factorisation of the generative model,
we get:
ln Q∗
k(xk) =∆ E∼Qk(xk)[ln P(x,o )]
= E∼Qk(xk)[ln
∏
i
P(Ni|pai)],
where Ni iterates over all nodes, i.e. all latent and observed variab les, and pa i are the
parents of Ni. The term in the above product can be classiﬁed into three gro ups: the terms
that do not depend on xk, the terms whose target variable ( Ni) is xk and the terms whose
predictors (pa i) contains xk. Building on this observation, one can use the log property a nd
the linearity of expectation to isolate the terms that depen d on xk:
ln Q∗
k(xk) = ⟨ln
∏
i
P(Ni|pai)⟩∼Qk
= ⟨ln P(xk|
pak)⟩∼Qk +
∑
cj ∈chk
⟨ln P(cj |xk, cpkj )⟩∼Qk + C, (18)
where ⟨·⟩∼Qk is just another notation for E∼Qk(xk)[·], and the constant C comes from the
terms of the product that do not depend on xk. Equation 18 is the variational message
passing equation that tells us how to compute the optimal pos terior of any hidden state xk
based on its Markov blanket, i.e. xk’s parents pak, children chk and co-parents cpkj . For
readers unfamiliar with the notion of Markov blankets, Figu re 12 provides a visual depiction
of the underlying notion.
23
Champion et al.
A
D C
E
F G
B
Markov blanket ofA
Parent ofA
Co-parent of A
Child of A
Figure 12: This ﬁgure illustrates the Markov blanket of node A, which is drawn in grey
surrounded by a dashed line. The nodes F and G are the parents of A and the nodes C
and D are the children of A. The node E is the co-parent of A with respect to D and the
node B is the co-parent of A with respect to C.
6.4 Conjugate exponential model
The variational message passing algorithm can be derived fo r the class of conjugate expo-
nential models (Winn and Bishop, 2005). Those models have a l ikelihood function and a
prior in the exponential family. Furthermore, the prior and the likelihood are conjugate,
meaning that the posterior will have the same form as the prio r. We follows the steps
in Winn and Bishop, while referring the interested reader to (Winn and Bishop, 2005) for
more details. The derivations in equations 19-23 are clariﬁ ed in the example in Figure 13.
Returning to our goal of computing the posterior over xk (cf. Equation 18), we assume
that P(xk|pak) and P(cj |xk, cpkj ) are in the exponential family, i.e.
ln P(xk|pak) =
µk(pak) ·uk(xk) + hk(xk) + zk(pak) (19)
ln P(cj |xk, cpkj ) = µj(xk, cpkj ) ·uj(cj ) + hj (cj ) + zj (xk, cpkj ) (20)
where µk(pak), uk(xk), hk(xk) and zk(pak) are the parameters, the suﬃcient statistics, the
underlying measure and the log partition, respectively. Fo r a speciﬁc example, Equation 25
shows the Dirichlet distribution written in the form of the e xponential family. The ﬁrst step
of the Winn and Bishop method takes advantage of the conjugac y constraint to re-arrange
Equation 20 as a function of uk(xk) that appears in Equation 19:
ln P(cj |xk, cpkj ) = µj→k(cj , cpkj ) ·uk(xk) + λ(cj , cpkj ), (21)
where µj→k(cj , cpkj ) and λ(cj , cpkj ) emerge from the re-arrangement. For a speciﬁc example
of this ﬁrst step, the reader is referred to the derivation fr om (26) to (27), Figure 13 also
provides an example of µj→k(cj , cpkj ). The second step substitutes Equations 21 and 19
24
Active Inference and Variational Message Passing
within the variational message passing equation leading to :
ln Q∗
k(xk) = ⟨
µk(pak) ·uk(xk) + hk(xk) + zk(pak)⟩∼Qk
+
∑
cj ∈chk
⟨µj→k(cj , cpkj ) ·uk(xk) + λ(cj , cpkj )⟩∼Qk + Const.
The third step relies on taking the exponential of both sides , using the linearity of
expectation and factorising by uk(xk) to obtain:
Q∗
k(xk) = exp
{[
⟨
µk(pak)⟩∼Qk +
∑
cj ∈chk
⟨µj→k(cj , cpkj )⟩∼Qk
]
·uk(xk) + hk(xk) + Const
}
,
(22)
where the above constant just absorbed zk(pak) and λ(cj , cpkj ), which does not depend
on xk. At this point, we already see that the prior (19) and the appr oximate posterior
(22) have the same functional form, i.e., only their paramet ers diﬀer. The fourth step
re-parameterizes µk(pak) and µj→k(cj , cpkj ) in terms of the expectation of the suﬃcient
statistics of the children, parents and the co-parents:
Q∗
k(xk) = exp
{
µ∗
k
·uk(xk) + hk(xk) + Const
}
µ∗
k
= ˜µk({⟨ui(i)⟩Qi }i∈pak ) +
∑
cj ∈chk
˜µj→k(⟨uj (cj )⟩Qj , {⟨ul(l)⟩Ql }l∈cpkj ), (23)
where ˜µk is a re-parameterization of µk(pak) in terms of the expectation of the suﬃcient
statistic of the parents of xk, and similarly ˜µj→k is a re-parameterization of µj→k. The
exact form of ˜µk and µj→k vary from distribution to distribution. An example of those
re-parameterizations is visible from Equation 28 to 29.
To understand the intuition behind (23), let us consider the following example: given
the Forney factor graph illustrated in Figure 13, we wish to c ompute the posterior of Y.
Then, the only parent of Y is Z, the only child of Y is X and the only co-parent of Y
with respect to X is W. Therefore, applying equation 23 to our example leads to the
equation presented in Figure 13 whose components can be inte rpreted as messages. Indeed,
each variable (i.e. X, Z and W) sends the expectation of their suﬃcient statistic (i.e. a
message) to the square node in the direction of Y (i.e. either PX or PY ). Those messages
are then combined using a function (i.e. either ˜ µY or ˜µX→Y ) whose output (i.e. another
set of messages) are summed to obtain the optimal parameters µ∗
Y . The computation of
the optimal parameters (23) can then be understood as a messa ge passing procedure.
25
Champion et al.
YPYZPZ PX W PW
X
m1 m2 m3 m4
m5
µ∗
Y
= ˜µY (⟨uZ (Z)⟩QZ ) +˜µX→ Y (⟨uX (X)⟩QX , ⟨uW (W )⟩QW )
m1
m2 m3
m4m5
Figure 13: This ﬁgure illustrates the computation of the opt imal posterior parameters for
the variable Y as a message passing procedure, which requires the transmis sion of messages
from the parent ( m2) and child ( m3) factors. Additionally, the message from the child
factor ( m3) requires the computation of messages from the co-parent ( m4) and child ( m5)
variables. Also, the message from the parent ( m2) factor requires the computation of a
message ( m1) from the parent variable. Set notation and associated brac kets {}have been
dropped, since there is only ever one parent or co-parent.
Returning to the Winn and Bishop (2005) method, the last step computes the (set of)
expectations associated with {⟨uj (j)⟩Qj }j∈paY , ⟨uX (X)⟩QX , and {⟨uj (j)⟩Qj }j∈cpY X . Be-
cause all nodes of the model are in the exponential family, th e moment generating function
can be used to prove the following:
⟨uN (N)⟩QN = −∂˜zN (θN )
∂θN
, (24)
where N is any node of the graphical model, θN are the natural parameters of the distri-
bution over N, and ˜zZ (θN ) is a re-parameterisation of the log partition w.r.t the nat ural
parameters of the distribution over Z. Note that another way to compute those expectations
will be presented in Section 7.3.
7. The link between Active Inference and Variational Messag e Passing
The previous sections have presented the theory behind acti ve inference and variational
message passing. This section focuses on the link between th ose two frameworks. First, we
slightly modify the generative model and the variational di stribution. These modiﬁcations
concern a small part of the generative model and to ensure con jugacy between the random
variables of the model. Then, we derive new update equations based on the Winn and
Bishop method (Winn and Bishop, 2005). As we will see, those u pdates can be interpreted
as a passing of messages that highlight the connection betwe en variational message passing
and belief updating in (planning as) active inference.
26
Active Inference and Variational Message Passing
7.1 Generative model modiﬁcations
In order to perform variational message passing, we have mad e three modiﬁcations to the
generative model described by Equation 7. First, the prior o ver the precision parameter γ
is removed. Second, the softmax function forming the prior o ver the policies is transformed
into a categorical distribution with parameters α. This is a mild modiﬁcation because
the softmax function is frequently used to represent a categ orical distribution, e.g. neural
classiﬁers using a softmax function as output layer or simil arly to the updates of Q(sτ )
and Q(π) presented in Section 5.4. Finally, we assume a Dirichlet di stribution over the
parameters α. Figure 14 illustrates this new generative model where:
P(π|α) = Cat( π; α)
P(α) = Dir( α; θ).
The conjugacy between the Dirichlet and categorical distri butions enables us to derive
update equations that can be interpreted as messages. Recal l that the prior over policies was
used to bias the policy selection towards the policies that m inimise expected free energy.
This can be implemented in a straightfoward way — while prese rving conjugacy — by
setting the parameters of the Dirichlet as follows:
θ= − →c −G,
where G is the expected free energy and − →c is a vector of constants whose elements satisfy
the following properties:
1. ∀i,j : − →ci = − →cj , i.e. all elements are equal;
2. ∀j : − →cj >maxi Gi, i.e. all θj are strictly positive.
To better understand the inﬂuence of P(α) on the selection of policies, we imagine a
Dirichlet with K parameters as a distribution over a ( K−1)-simplex. Assuming that all θi
are greater than one, the point of this simplex with the highe st probability, i.e. the mode
mα , has the following coordinates:
mα =
[ θ1−1
(∑ K
k=1 θk
)
−K
... θK −1
(∑ K
k=1 θk
)
−K
]
.
Studying a few special cases of the above equation sheds some light on how policy
selection is inﬂuenced by P(α). If the i-th numerator of the coordinates, i.e. θi −1, equal
one and all others equal zero, then the mode mα is at the corner of the simplex corresponding
to the i-th axis. If all numerators are equal to one, then the m ode is at the centre of the
simplex. Intuitively, this means that the bigger θi is relative to the other θj ∀j ̸= i, the
closer mα is to the i-th corner of the simplex. Additionally, the close r mα is to the i-th
corner of the simplex, the more likely the i-th policy will be . Therefore, the bigger θi the
more likely the i-th policy. Finally, the only part of the num erators that is not a constant
is Gi and the smaller Gi the bigger the i-th numerator. Thus, in accord with the activ e
inference literature, P(α) favours policies that minimise the expected free energy.
Another perspective on this parameterisation of priors ove r policies is to think of − →c as
pseudo-counts that ‘promote’ each policy according to how o ften it was previously pursued,
27
Champion et al.
before adding (-ve) expected free energy. If these pseudo-c ounts are suitably small, adding
expected free energy will have a greater eﬀect in the sense tha t expected free energy scores
the number of times each policy would be pursued. Quantitati vely, this means that a
diﬀerence in the expected free energy between one policy and a nother can now be interpreted
in terms of Dirichlet parameters or pseudo-counts.
It could be argued that the Dirichlet parameterisation of th e prior over policies is a
more natural parameterisation than the gamma distribution used to explain dopamine.
Furthermore, as noted above, in most applications, gamma is set to one. More importantly,
the precision parameter is only relevant for generative mod els where policies entail past
transitions. In look-ahead policies or tree search impleme ntations of planning, policies only
concern future states. This means the precision of prior bel iefs about policies relative to
posterior beliefs (based upon the evidence a particular pol icy is being pursued) becomes
irrelevant. In this case, the Dirichlet parameterisation a bove may be preferred.
S0
PS0
... PSt St ... PST ST
PO0
O0
POt
Ot
A
PA
D
PD
B
PB
π
Pπ
α
Pα
Figure 14: The new generative model obtained after replacin g the gamma distribution by
a Dirichlet distribution.
7.2 Variational distribution modiﬁcations
The variational distribution presented in Section 5.2 is an example of a structured varia-
tional distribution, because factors such as Q(Sτ ,π ) = Q(Sτ |π)Q(π) model the (posterior)
dependency between Sτ and π. Performing inference with such a joint distribution falls
under the category of structured variational inference (Wi egerinck, 2000; Xing et al., 2012)
and will not be covered in this paper. Instead, we assume a ful ly factorised distribution
28
Active Inference and Variational Message Passing
such that:
Q(S0:T ,π, A, B, D,γ ) = Q(π)Q(A)Q(B)Q(D)Q(γ)
T∏
τ=0
Q(Sτ ),
where Q(π) = Cat( π; ˜α), Q(Sτ ) = Cat( Sτ ; ˜Dτ ) and all the other factors remain unchanged.
This is a rather severe mean-ﬁeld approximation: although i t allows for straightforward
application of variational message passing, removing the c onditional dependencies of hidden
states in the future on action means the agent cannot individ uate the consequences of action.
Under this functional form the expected free energy reduces to:
G(π) =
T∑
τ=1
EQ(Sτ − 1, B)
[
H[P(Sτ |Sτ−1, B,π )]
]
.
Namely, the expected conditional entropy of the hidden stat es. Also, we refer the interested
reader to Appendix H for a derivation of the above equation. I ntuitively, this means that
good policies select actions that lead to unambiguous hidde n states. This highlights a major
limitation of the mean-ﬁeld approximation required by the v ariational message passing
proposed by (Winn and Bishop, 2005) in the context of active i nference. In other words,
when removing key structure from the variational distribut ion, the factor over the hidden
states Q(Sτ |π) no longer depends on the policy π and most of the terms in the expected
free energy become constants w.r.t π. Figure 15 illustrates an alternative generative model,
implementing tree search as a form of structure learning, wh ich is not impacted by this
issue because the future states in this model still depend up on the action undertaken by the
agent. We refer the reader to our companion paper (Champion e t al., 2021) for details. A
related treatment that performs exact Bayesian inference b y considering a slightly diﬀerent
generative model can be found in (Friston et al., 2020).
Before we turn to the derivation of the messages, we highligh t the diﬀerences between
active inference as presented in Section 5 and the current tr eatment. The former is an
example of structured variational inference ( ∗). In contrast, the work presented in this
section assumes a fully factorised variational distributi on and will be strictly framed as a
message passing algorithm, i.e. variational message passi ng ( ∗). Figure 16 illustrates those
diﬀerences. Finally, in the remaining sections, we present t he derivation of the messages for
D, A, π and α, and we refer the reader to Appendices F and G for the derivati ons of the
messages for B and Sτ , respectively.
29
Champion et al.
S0
PS0
PS...
S...
PSt
St
U0PU0
U...PU...
PO0 O0
PO... O...
POt Ot
PS{1}
S{1}
PS{2}
S{2}PO{1}O{1} PO{2} O{2}
PS{22}
S{22}
PS{11}
S{11}
PS{12}
S{12}PO{11}O{11}
Figure 15: This ﬁgure illustrates an alternative new (expan dable) generative model allowing
planning under active inference. In this model, the future i s now a tree like generative model
whose branches correspond to the policies considered by the agent. Each edge connecting
two states in the future correspond to an action and the nodes in light grey represent
possible expansions of the current generative model.
exact inference approximate inference
message based methods
direct methods
BPT
E
SVMP
SVI (∗)
VMP (× )
VI
Figure 16: This ﬁgure illustrates the diﬀerences between the framework presented in Section
5 that belongs to the ﬁeld of structured variational inferen ce (Bishop and Winn, 2003) de-
noted by ( ∗), and the work presented below that belongs to the ﬁeld of var iational message
passing (Winn and Bishop, 2005) denoted by ( ×). The other abbreviations BPT, E, VI
and SVMP correspond to belief propagation on tree graphical models (Kschischang et al.,
2001), the elimination algorithm (Cozman, 2000), variatio nal inference (Blei et al., 2017)
and structured (or cluster) variational message passing (L in et al., 2018), respectively. Im-
portantly, note that BPT is a speciﬁc kind of belief propagat ion which does not involve
generalized BP (Yedidia et al., 2000) or loopy belief propag ation (Murphy et al., 2013).
30
Active Inference and Variational Message Passing
7.3 Messages for D
This section applies the method of Winn and Bishop discussed in Section 6.4 to compute the
messages of D. Let us start with the deﬁnition of the Dirichlet and categor ical distributions
written in the form of the exponential family:
ln P(D; d) =


d1 −1
...
d|S| −1



 
µ D (d)
·


ln D1
...
ln D|S|



 
uD(D)
−ln B(d)  
zD(d)
(25)
ln P(S0; D) =


ln D1
...
ln D|S|



 
µ S0(D)
·


[S0 = 1]
...
[S0 = |S|]



 
uS0(S0)
(26)
where B(d) is the Beta function and |S|is the number of values a hidden state can take. The
ﬁrst step requires us to re-write Equation 26 as a function of uD(D), this is straightforward
because µS0(D) is just another name for uD(D). Using the fact that the inner product is
commutative:
ln P(S0; D) =


[S0 = 1]
...
[S0 = |S|]



 
µ S0→ D(S0)
·


ln D1
...
ln D|S|



 
uD(D)
. (27)
The second step aims to substitute Equations 25 and 27 within the variational message
passing equation (18), i.e.
ln Q∗(D) =
⣨


d1 −1
...
d|S| −1



 
µ D (d)
·


ln D1
...
ln D|S|



 
uD(D)
−ln B(d)  
zD(d)
⟩
+
⣨


[S0 = 1]
...
[S0 = |S|]



 
µ S0→ D(S0)
·


ln D1
...
ln D|S|



 
uD(D)
⟩
+ Const,
where ⟨•⟩refers to ⟨•⟩∼QD . Note that in the above equation, di are ﬁxed parameters, therefore
there is not any posterior over dand the ﬁrst expectation ⟨·⟩∼QD can be removed. The third
step rests on taking the exponential of both sides, using the linearity of expectation and
factorising by uD(D) to obtain:
Q∗(D) = exp
{ 

d1 −1 + ⟨[S0 = 1] ⟩
...
d|S| −1 + ⟨[S0 = |S|]⟩

·uD(D) + Const
}
, (28)
where zD(d) have been absorbed into the constant term because it does no t depend on
D. The fourth step is a re-parameterisation done by observing that ⟨[S0 = i]⟩is the i-th
element of the expectation of the vector uS0(S0), i.e. ⟨uS0(S0)⟩i = ⟨[S0 = i]⟩:
31
Champion et al.
Q∗(D) = exp
{ 

d1 −1 + ⟨uS0(S0)⟩1
...
d|S| −1 + ⟨uS0(S0)⟩|S|



 
˜µ D (... )+˜µ S0→ D(... )
·uD(D) + Const
}
. (29)
The last step consists of computing the expectation of ⟨uS0(S0)⟩i for all i. This can
be achieved by realising that the probability of an indicato r function for an event is the
probability of this event, i.e ⟨uS0 (S0)⟩i = ⟨[S0 = i]⟩= Q(S0 = i) = ˜D0i. Substituting this
result in Equation 29, leads to the ﬁnal result:
Q∗(D) = exp
{ 

d1 −1 + ˜D01
...
d|S| −1 + ˜D0|S|

·uD(D) + Const
}
.
Indeed, the above equation is in fact a Dirichlet distributi on in exponential family form,
and can be re-written into its usual form to obtain the ﬁnal up date equation:
Q∗(D) = Dir( D;
d+ ˜D0).
In the following sections, we provide derivations for the me ssages of A, B, π, α, and Sτ .
Those derivations are similar to the one presented above. We encourage technical readers
to go through those derivations because they constitute the main contribution of this paper.
However, a reader uninterested in the algebraic details of t he proofs may want to jump to
Section 7.7.
7.4 Messages for A
In the previous section, we have shown how to compute the mess ages for D, which are based
on the conjugacy between a categorical P(S0|D) and a Dirichlet P(D; d) distributions. In
this section, we dive into the derivation of the messages of A, which relies on the same
kind of conjugacy. We start with the deﬁnition of P(A; a), which is a product of Dirichlet
distributions. This product can be turned into a sum by takin g the logarithm of both sides
and using the log property to obtain:
ln P(A; a) = ln
∏
i
P(A•i; a•i) =
∑
i
ln Dir(A•i; a•i)
=
∑
i


a1i −1
...
a|O|i −1

·


ln A1i
...
ln A|O|i

−ln B(a•i)

 
Logarithm of Dirichlet
=


a11 −1
...
a|O||S| −1



 
µ A(a)
·


ln A11
...
ln A|O||S|



 
uA(A)
−
∑
i
ln B(a•i)
  
zA(a)
, (30)
32
Active Inference and Variational Message Passing
where |O|is the number of possible outcomes. Note that the vectors uA(A) and µA(a) step
through all the elements of the matrices A and a, respectively. Also, for each time step τ
up to the present time t, the random matrix A has one child Oτ (see Figure 14), and its
probability mass function P(Oτ |A,S τ ) is a product of categorical distributions that can be
written as:
ln P(Oτ = k|A,S τ = l) = ln Akl
=
∑
i,j
[Oτ = i][Sτ = j] ln Aij
=


[Sτ = 1] ln A11
...
[Sτ = |S|] ln A|O||S|



 
µ Oτ (A,S τ )
·


[Oτ = 1]
...
[Oτ = |O|]



 
uOτ (Oτ )
. (31)
Finally, the re-parameterisation in the fourth step will re quire the probability mass
function of Sτ (see Figure 14), i.e. the co-parent of A with respect to Oτ , to be written in
the form of the exponential family as follows:
ln P(Sτ = k|B,S τ−1 = l,π = m) = ln B[Um
τ−1]kl
=
∑
i,j,k,u
[Sτ = i][Sτ−1 = j][π = k][Uk
τ−1 = u] ln B[u]ij
= µSτ (B,S τ−1,π ) ·uSτ (Sτ ), (32)
where:
µSτ (B,S τ−1,π ) =


∑
j,k,u [Sτ−1 = j][π = k][Uk
τ−1 = u] ln B[u]1j
...∑
j,k,u [Sτ−1 = j][π = k][Uk
τ−1 = u] ln B[u]|S|j

,
and:
uSτ (Sτ ) =


[Sτ = 1]
...
[Sτ = |S|]

.
The ﬁrst step requires us to re-write Equation 31 as a functio n of uA(A), this is done
by expanding the inner product and re-arranging:
ln P(Oτ |A,S τ ) =


[Oτ = 1][ Sτ = 1]
...
[Oτ = |O|][Sτ = |S|]



 
µ Oτ → A(Oτ ,S τ )
·


ln A11
...
ln A|O||S|



 
uA(A)
. (33)
The second step aims to substitute Equations 30 and 33 within the variational message
passing equation (18), i.e.
ln Q∗(A) =
⣨


a11 −1
...
a|O||S| −1

·uA(A)
⟩
+
t∑
τ=0
⣨


[Oτ = 1][ Sτ = 1]
...
[Oτ = |O|][Sτ = |S|]

·uA(A)
⟩
+ Const,
33
Champion et al.
where ⟨•⟩refers to ⟨•⟩∼QA . The third step builds on this equation by pulling the sum ove r
all time steps τ inside the vector, using the linearity of expectation, fact orising uA(A), and
taking the exponential of both sides:
Q∗(A) = exp
{ 

a11 −1 + ∑ t
τ=0⟨
[Oτ = 1] ⟩⟨[Sτ = 1] ⟩
...
a|O||S| −1 + ∑ t
τ=0⟨
[Oτ = |O|]⟩⟨[Sτ = |S|]⟩

·uA(A) + Const
}
,
where we used that aji are hyperparameters that are constant w.r.t the expectatio n ⟨•⟩∼QA .
The fourth step consists of two re-parameterisations perfo rmed by observing that ⟨[Oτ = j]⟩
and ⟨[Sτ = i]⟩are the expectations of the j-th and i-th elements of the vect ors uOτ (Oτ ) and
uSτ (Sτ ), respectively (cf. Equation 31 and 32). Substituting thos e re-parameterisations in
the above equation leads to:
Q∗(A) = exp
{ 

a11 −1 + ∑ t
τ=0⟨
uOτ (Oτ )⟩1⟨uSτ (Sτ )⟩1
...
aKN −1 + ∑ t
τ=0⟨
uOτ (Oτ )⟩|O|⟨uSτ (Sτ )⟩|S|



 
˜µ A(... )+∑
τ ˜µ Oτ → A(... )
·uA(A) + Const
}
. (34)
The last step consists of computing the expectation of ⟨uOτ (Oτ )⟩i and ⟨uSτ (Sτ )⟩j for
all i and j. Since, the probability of an indicator function for an even t is the probability
of this event, we are searching for the probabilities of Oτ = j and Sτ = i. The probability
of Oτ = j is the j-th element of the vector oτ , which is a one hot vector containing the
observation from the environment at time τ. The posterior probability of Sτ is by deﬁnition
Q(Sτ ) = ˜Dτ . Substituting the probabilities of Oτ = j and Sτ = i in Equation 34, leads to:
Q∗(A) = exp
{ 

a11 −1 + ∑ t
τ=0
oτ1 ˜Dτ1
...
a|O||S| −1 + ∑ t
τ=0
oτ|O| ˜Dτ|S|

·uA(A) + Const
}
(35)
=
∏
i
exp
{ 

a1i −1 + ∑ t
τ=0
oτ1 ˜Dτi
...
a|O|i −1 + ∑ t
τ=0
oτ|O| ˜Dτi

·


ln A1i
...
ln A|O|i

+ Const
}
. (36)
Finally, one can recognise in Equation 36 the product of Diri chlet distributions written
into their exponential form, i.e.
Q∗(A) =
∏
i
Dir(A•i, a•i) where a =
a+
∑
τ
oτ ⊗ ˜Dτ .
The origin of the outer product in the computation of the para meters can be understood
by considering Pτ the outer product between oτ and sτ such that Pτ
ij = oτi sτj . Then,
Equation 35 shows that: aij = aij + ∑
τ Pτ
ij ⇔a = a+ ∑
τ oτ ⊗sτ .
7.5 Messages for π
We now turn to the messages for π. Note, that the deﬁnition of the P(Sτ |B,S τ−1,π ) and
P(π|α) are given by Equations 32 and 44, respectively. The ﬁrst ste p requires us to re-write
34
Active Inference and Variational Message Passing
Equation 32 as a function of uπ (π). Using the inner product deﬁnition and re-arranging we
obtain:
ln P(Sτ = k|B,S τ−1 = l,π = m) =



∑
i,j,u [Sτ = i][U1
τ−1 = u][Sτ−1 = j] ln B[u]ij
...∑
i,j,u [Sτ = i][U|π |
τ−1 = u][Sτ−1 = j] ln B[u]ij


·uπ (π).
(37)
The second step aims to substitute Equations 44 and 37 within the variational message
passing equation, i.e.
ln Q∗(π) =
⣨


ln α1
...
ln α|π |

·uπ (π)
⟩
+
T∑
τ=1
⣨



∑
i,j,u
[Sτ = i][U1
τ−1 = u][Sτ−1 = j] ln B[u]ij
...∑
i,j,u [Sτ = i][U|π |
τ−1 = u][Sτ−1 = j] ln B[u]ij


·uπ (π)
⟩
+ Const,
where ⟨•⟩refers to ⟨•⟩∼Qπ . The third step relies on pulling the summation over all time steps
inside the vector, taking the exponential of both sides, usi ng the linearity of expectation
and factorising by uπ (π) to obtain:
Q∗(π) ∝exp
{ 


⟨
ln α1⟩+ ∑
τ,i,j,u [U1
τ−1 = u]⟨[Sτ = i]⟩⟨[Sτ−1 = j]⟩⟨ln B[u]ij ⟩
...
⟨ln α|π |⟩+ ∑
τ,i,j,u [U|π |
τ−1 = u]⟨[Sτ = i]⟩⟨[Sτ−1 = j]⟩⟨ln B[u]ij ⟩




 
µ ∗
π
·uπ (π)
}
.
The fourth step is a re-parameterisation implemented by obs erving that ⟨ln αk⟩, ⟨[Sτ =
i]⟩, ⟨[Sτ−1 = j]⟩and ⟨ln B[u]ij ⟩are elements of the vectors ⟨uα (α)⟩, ⟨uSτ (Sτ )⟩, ⟨uSτ − 1 (Sτ−1)⟩
and ⟨uB(B)⟩, respectively:
µ∗
π =



⟨
uα (α)⟩1 + ∑
τ,i,j,u [U1
τ−1 = u]⟨uSτ (Sτ )⟩i⟨uSτ − 1 (Sτ−1)⟩j ⟨uB(B)⟩u,i,j
...
⟨uα (α)⟩|π | + ∑
τ,i,j,u [U|π |
τ−1 = u]⟨uSτ (Sτ )⟩i⟨uSτ − 1 (Sτ−1)⟩j ⟨uB(B)⟩u,i,j


. (38)
The last step consists of computing the expectation of ⟨uα (α)⟩k, ⟨uSτ (Sτ )⟩i, ⟨uSτ − 1 (Sτ−1)⟩j
and ⟨uB(B)⟩u,i,j for all i, j, k and u:
• ⟨uα (α)⟩k = ⟨ln αk⟩= ψ(˜αk) −ψ(∑
l ˜αl) =∆ ¯αk
• ⟨uSτ (Sτ )⟩i = ⟨[Sτ = i]⟩= ˜Dτi
• ⟨uSτ − 1 (Sτ−1)⟩j = ⟨[Sτ−1 = j]⟩= ˜D(τ−1)j
• ⟨uB(B)⟩u,i,j = ⟨ln B[u]ij ⟩= ψ(b[u]ij ) −ψ(∑
l b[u]lj ) =∆ ¯B[u]ij
35
Champion et al.
Furthermore, the indicator function in the k-th row of Equat ion 38 ﬁlters out all elements
where u̸= Uk
τ−1. Substituting those results in Equation 38, leads to the ﬁna l result:
Q∗(π) ∝exp
{ 


¯α1 + ∑
τ,i,j ˜Dτi ˜D(τ−1)j ¯B[U1
τ−1]ij
...
¯α|π | + ∑
τ,i,j ˜Dτi ˜D(τ−1)j ¯B[U|π |
τ−1]ij


·uπ (π)
}
.
Indeed, the above equation is a Categorical distribution in the exponential family form,
and can be re-written into its usual form as follows:
Q∗(π) = Cat( π; α∗) where α∗ = σ
(
¯α +
T∑
τ=1
Fτ
)
and Fτ =


⟨˜Dτ ⊗ ˜Dτ−1, ¯B[U1
τ−1]⟩F
...
⟨˜Dτ ⊗ ˜Dτ−1, ¯B[U|π |
τ−1]⟩F

,
where it should be stressed that ⟨•, •⟩F is not an expectation but the Frobenius product, i.e.
a generalisation of the inner product to matrices.
7.6 Messages for α
In this section, we focus on the messages for α, whose derivation is identical to the messages
of D. To see this, note that P(D) was a Dirichlet with parameters d. Furthermore, the only
child of D was S0 whose prior and posterior were categorical distributions w ith parameters
D and ˜D. Similarly, note that P(α) is a Dirichlet with parameters θ. Furthermore, the
only child of α is π whose prior and posterior are categorical distributions wi th parameters
α and ˜α. From this observation, we directly obtain the following re sult:
Q∗(α) = Dir( α;
θ+ ˜α).
7.7 Summary of messages
Next, we focus on explaining the intuition behind the result ing equations. The ﬁrst point
is the coloration of the equations in orange and purple. The o range colour corresponds
to messages from the parent factors, which correspond to mes sages of type m 2 in Figure
13. This means that each orange message is a function of the ex pectation of the suﬃcient
statistic of the parent variables, i.e. a function of messag es of type m 1. Similarly, the
purple colour corresponds to messages from the child factor s, which correspond to messages
of type m 3 in Figure 13. Once again, this means that each purple message is a function of
the suﬃcient statistics of the co-parent and child variable s, i.e. a function of messages of
type m 4 and m 5, respectively. Let’s see how these play out in our newly deri ved equations.
Messages for α:
Q∗(α) = Dir( α;
θ+ ˜α)
Recall that µα = θ is an m 2 message (orange colour). However, α does not have any
parent variables thus µα is a constant, i.e. a function of zero m 1 messages. Furthermore,
we know that α has only one child variable ( π) and no co-parent variables. Therefore,
µπ →α (˜α) = ˜α is the only m 3 message (purple colour) for α, where ˜α = ⟨uπ (π)⟩Qπ is an m 5
message.
36
Active Inference and Variational Message Passing
Messages for D:
Q∗(D) = Dir( D; d+ ˜D0)
Similarly for the messages of α, µD = d and µS0→D( ˜D0) = ˜D0, where ˜D0 should be
thought of as a message from a child variable (m 5 message).
Messages for A:
Q∗(A) =
∏
i
Dir(A•i, a•i) where a = a+
∑
τ
oτ ⊗ ˜Dτ
Following the same reasoning, µA = a is an m 2 message and because A does not have
any parent variables then µA is a constant. Also, A has one child variable ( Oτ ) for each
time step τ ∈/llbracket0,t /rrbracketand one co-parent variable ( Sτ ) for each of them, which implies that
there are t+ 1 m 3 messages for A, i.e. µOτ →A(oτ , ˜Dτ ) = oτ ⊗ ˜Dτ ∀τ ∈/llbracket0,t /rrbracket. Because
the Oτ are observed, we know that the m 5 messages transmitted by this node will be the
observation made at time τ (oτ ). Additionally, the m 4 message from the hidden variables
Sτ are the expectation of their suﬃcient statistics, i.e. ⟨uSτ (Sτ )⟩QSτ = ˜Dτ . This conﬁrms
the idea that µOτ →A is a function of the suﬃcient statistics of the child and co-p arent
variables. Figure 17 concludes this paragraph with a visual representation of the messages
for A.
S0
PS0
... PSt St ... PST ST
PO0
O0
POt
Ot
A
PA
D
PD
B
PB
π
Pπ
α
Pα
a = a + ∑ t
τ=0 oτ ⊗ ˜Dτ
m2 m3
m5 m4
Figure 17: This ﬁgure illustrates the passing of messages re quired to update the posterior
over A. The messages of type m2, m3, m4 and m5 come from the parent factors, child
factors, co-parent variables and child variables, respect ively.
37
Champion et al.
Messages for B:
Q∗(B) =
∏
u,i
Dir(B[u]•i, b[u]•i) where b[u] = b[u] +
∑
(k,τ )∈Ω u
˜αk ˜Dτ ⊗ ˜Dτ−1
Sticking with this reasoning, µB = bis an m 2 message and because B does not have any
parent variables then µB is a constant equal to b. Also, B has one child variable ( Sτ ) for
each time step τ ∈/llbracket1,T /rrbracketand all policies ∀π ∈/llbracket1, |π|/rrbracket, along with two co-parent variables
(Sτ−1 and π) for each of those child variables. This implies that there a re T ×|π|m3
messages for B, i.e. µSτ →SB (˜αk, ˜Dτ , ˜Dτ−1) = ˜αk ˜Dτ ⊗ ˜Dτ−1, ∀τ ∈/llbracket1,T /rrbracket, ∀π ∈/llbracket1, |π|/rrbracket
where ˜Dτ is an m 5 message and ˜αk along with ˜Dτ−1 are m 4 messages.
Messages for π:
Q∗(π) = Cat( π; α∗) where α∗ = σ
(
¯α +
T∑
τ=1
Fτ
)
and Fτ =


⟨˜Dτ ⊗ ˜Dτ−1, ¯B[U1
τ−1]⟩F
...
⟨˜Dτ ⊗ ˜Dτ−1, ¯B[U|π |
τ−1]⟩F


If we keep applying the same reasoning, we see that µπ (¯α) = ¯α is an m 2 message, which
is a function of the suﬃcient statistics of the parent variab le α (m1 message). Moreover,
π has one child variable ( Sτ ) for each time step τ ∈/llbracket1,T /rrbracket, and for each of those child
variables, π has two co-parent variables ( Sτ−1 and B). Therefore, µSτ →π = Fτ ∀τ∈/llbracket1,T /rrbracket
correspond to T m3 messages. Those messages are function of two m 4 messages ( ˜Dτ−1 and
¯B) and one m 5 message ( ˜Dτ ).
Messages for Sτ :
Q∗(Sτ ) = Cat( Sτ ; σ(µ∗
Sτ ))
µ∗
Sτ = [ τ = 0]
¯D + [τ ̸= 0]
∑
k
˜αk ¯B[Uk
τ−1] ˜Dτ−1 + [τ≤t]oτ · ¯A + [τ̸= T]
∑
k
˜αk ˜Dτ+1 · ¯B[Uk
τ ]
To understand the above equation, we can consider two cases: τ= 0 and τ̸= 0. In the
ﬁrst case, S0 only has one parent variable ( D), and µS0 ( ¯D) = ¯D where ¯D = ⟨uD(D)⟩QD
is a message from a parent variable (m 1 message). In the second case, Sτ has three parent
variables ( Sτ−1, B and π), and µSτ ( ˜Dτ−1, ¯B, ˜α) = ∑
k ˜αk ¯B[Uk
τ ] ˜Dτ−1 where ˜Dτ−1, ¯B and
˜α are also m 1 messages. Let us now think about the child variable(s) of Sτ . If τ ≤t, then
Sτ has a child variable from the likelihood mapping and µOτ →Sτ (oτ , ¯A) = oτ · ¯A, where oτ
is a message from the child variable (m 5 message) and ¯A is a message from the co-parent
variable (m 4 message). Additionally, if τ ̸= T, then Sτ receives a message from the future
µSτ +1→Sτ (˜αk, ˜Dτ+1, ¯B) = ∑
k ˜αk ˜Dτ+1 ·¯B[Uk
τ ], where ˜αk and ¯B are m 4 messages and ˜Dτ+1
is a m 5 message. Figure 18 concludes this section with an illustrat ion the message passing
procedure for S0.
38
Active Inference and Variational Message Passing
S0
PS0
... PSt St ... PST ST
PO0
O0
POt
Ot
A
PA
D
PD
B
PB
π
Pπ
α
Pα
Q∗(S0|π) =σ
(
¯D + o0 · ¯A + ∑
k ˜α k ˜D1 · ¯B(Uk
0 )
)
m1
m2 m3
m4m5
m3
m4 m4m5
Figure 18: This ﬁgure illustrates the passing of messages re quired to update the posterior
over S0. The messages of type m1, m2, m3, m4 and m5 come from the parent variables,
parent factors, child factors, co-parent variables and chi ld variables, respectively.
7.8 Messages vs update equations
In this section, we present a side by side comparison of the me ssages obtained using vari-
ational message passing and the update equations that under write belief updating in the
active inference literature. Throughout this section, the messages will always be presented
ﬁrst, followed by the equivalent update equations. Let us st art with the random variable
D:
Q∗(D) = Dir( D;
d+ ˜D0)
Q∗(D) = Dir( D; d+ s0)
These two equations only diﬀer in terms of labels, i.e. s0 and ˜D0 conceptually represent
the same quantity. Similarly, the updates of A are recovered up to a change of label:
Q∗(A) =
∏
i
Dir(A•i, a•i) where a = a+
t∑
τ=0
oτ ⊗ ˜Dτ
Q∗(A) =
∏
i
Dir(A•i, a•i) where a = a+
t∑
τ=0
oτ ⊗sτ
39
Champion et al.
The update of B slightly diﬀers from the messages obtained from variational message pass-
ing, which follows from the fact that we modiﬁed the variatio nal distribution:
Q∗(B) =
∏
u,i
Dir(B[u]•i, b[u]•i) where b[u] = b[u] +
∑
(k,τ )∈Ω u
˜αk ˜Dτ ⊗ ˜Dτ−1
Q∗(B) =
∏
u,i
Dir(B[u]•i, b[u]•i) where b[u] = b[u] +
∑
(k,τ )∈Ω u
π ksk
τ ⊗sk
τ−1
The only conceptual diﬀerence here is that sk
τ depended upon the policy, while ˜D does not.
Concerning Sτ , we have re-arranged the update equation to highlight the si milarity with
the messages:
Q∗(Sτ ) = Cat( Sτ ; σ(µ∗
Sτ ))
µ∗
Sτ = [ τ = 0]
¯D + [τ ̸= 0]
∑
k
˜αk ¯B[Uk
τ−1] ˜Dτ−1 + [τ≤t]oτ · ¯A + [τ̸= T]
∑
k
˜αk ˜Dτ+1 · ¯B[Uk
τ ]
µ∗
Sτ = [ τ = 0] ¯D + [τ ̸= 0] ¯B[Uπ
τ−1]sπ
τ−1 + [τ ≤t]oτ · ¯A + [τ ̸= T] sπ
τ+1 ·¯B[Uπ
τ ]
There are two main diﬀerences here. First, as for B, sk
τ is replaced by ˜D, which does not
depend on the policies. Second, the past and future messages have an average over the
policies, while the updates do not. Unsurprisingly, since w e replaced γ by α and changed
the type of distributions, the updates are quite diﬀerent:
Q∗(α) = Dir
(
α;
θ+ ˜α
)
Q∗(γ) = Γ
(
γ; 1,β + G ·(π −π 0)
)
We conclude this section with the messages and updates of π, which are formally distinct.
These diﬀerences come from the fact that we moved G from P(π|γ) to P(α) and turned
P(π|γ) into a categorical distribution P(π|α):
Q∗(π) = Cat( π; α∗)
α∗ = σ
(
¯α +
T∑
τ=1
Fτ
)
and Fτ =


⟨˜Dτ ⊗ ˜Dτ−1, ¯B[U1
τ−1]⟩F
...
⟨˜Dτ ⊗ ˜Dτ−1, ¯B[U|π |
τ−1]⟩F


α∗ = σ
(
−1
β G +
T∑
τ=1
Fτ
)
and Fτ = sπ
τ ·¯B[U]sπ
τ−1
However, the general form of the updates remains unchanged w ith information coming
from the parent through ¯ α and −1
β G, and from each child through the summation over
time steps.
40
Active Inference and Variational Message Passing
8. Conclusion
The increasing use of active inference in neuroscience has c ast many brain processes as
Bayesian inference, the update equations of which can be tho ught of as a message passing
procedure. The ﬁrst goal of this paper was to present a comple te overview of the active
inference framework in discrete time and state space (Secti on 5) as well as a formal intro-
duction to the variational message passing literature (Sec tion 6). Then, we simpliﬁed the
generative model and the variational distribution usually adopted in the active inference to
derive a new set of update equations using the method of Winn a nd Bishop (2005) — and
highlight the connection between active inference and vari ational message passing (Section
7).
We hope that the ﬁrst few sections of this paper could be usefu l as an introduction
to variational inference, Forney factor graphs, active inf erence or/and variational message
passing. Section 7 might also be of interest to researchers s earching for a clear link be-
tween active inference and variational message passing or r esearchers seeking to derive the
update equations of new generative models. Section 7 explai ns why a fully factorised varia-
tional distribution simpliﬁes the expected free energy in a way that precludes risk sensitive
behaviour but preserves ambiguity avoidance. Finally, we n ote that this issue does not
confound generative models implementing tree search.
One might ask why previous formulations of belief updating o r message passing in active
inference have not exploited the simpliﬁcations considere d in the current paper. For exam-
ple, using a Dirichlet distribution to parameterise Bayesi an beliefs over policies — or a fully
factorised variational distribution that would simplify m essage passing. One answer is that
much of the legacy literature in active inference is concern ed with neuronal process theories
and biological implementation. For example, the only reaso n a Gibbs form was used for
the distribution over policies was to link the implicit temp erature or sensitivity parameter
to dopaminergic discharges. Similarly, the minimisation o f variational free energy — using
a gradient descent to implement structured variational mes sage passing — was motivated
by the need to cast belief updating in terms of diﬀerential equ ations that could be plausi-
bly associated with neuronal dynamics (and accompanying el ectrophysiological responses to
observations). However, if one frees oneself from the const raints of biological implementa-
tion, the repertoire of established schemes in machine lear ning and Bayesian statistics can,
in principle, be leveraged to reproduce kinds of choice beha viour active inference is trying
to explain and emulate. This paper has highlighted the putat ive usefulness of variational
message passing under a rationalisation of generative mode ls.
It is interesting to consider whether the simpliﬁed expecte d free-energy — resulting
from our message passing formulation of active inference — c an be linked in any sense to
human behaviour, whether normative or pathological. In par ticular, the free-energy we have
obtained reﬂects a very speciﬁc functional impoverishment . The full factorisation that is
necessary for vanilla message passing precludes the abilit y to conditionalize the variational
posterior on policies. This suggests a particular deﬁcit in the ability to plan, and a blindness
to future possibilities, the uncertainty associated with t hose possibilities and their potential
to satisfy preferences. As a result, the agent’s objective b ecomes to seek out unambiguous
cues, with no concern for outcome.
41
Champion et al.
In fact, humans do exhibit patterns of behaviour that — due to their repetitiveness —
seem to reﬂect a desire for high predictability. Additional ly, some of these patterns do not
seem obviously connected to rewarding or punishing outcome s. For example, those with
autism can exhibit very stereotyped repetitive behaviour: hand ﬂapping, hand clapping,
rocking, etc (Gabriels, 2005), which is often described as s timming (Sundar Rajagopalan et al.,
2013). These repetitive and ritualistic behaviours (Lam, 2 007) suggest an objective to avoid
exploration and the associated uncertainty.
This work naturally leads to future directions of research. For example, one could im-
plement the new generative model proposed in this paper and c ompare its performance with
the model presented in Section 5. Furthermore, additional r esearch needs to be done to
connect the original update equations of active inference t o the cluster variational message
passing literature. Much work has already been done on struc tured variational message
passing; particularly relation to marginal message passin g — and its advantages over re-
lated approaches based upon Bethe free energy (Yedidia, 200 5; Parr et al., 2019). Another
interesting direction of research would be to design new gen erative models that can tackle
more complex tasks, such as playing Atari games, human-mach ine interaction using nat-
ural language and automatic structure learning. Partial an swers to these directions of
research have already been provided with the use of deep acti ve inference (Fountas et al.,
2020; Ueltzh¨ oﬀer, 2018; Tschantz et al., 2020), deep tempor al models (Friston et al., 2018;
Heins et al., 2020) and Bayesian model reduction (Friston et al., 2018; Friston et al., 2017a;
Wauthier et al., 2020). Nevertheless, we anticipate that ad ditional work will pursue these
avenues of research. Finally, one could also compare the upd ate schemes under VMP to
belief propagation (Yedidia, 2011) or marginal message pas sing (Parr et al., 2019).
Acknowledgments
We would like to thank Karl Friston as well as the reviewers fo r their valuable feedback,
which greatly improved the quality of the present paper.
Appendix A: Active Inference, KL Control and Reinforcement Learning.
This appendix focuses on the relationship between Active In ference, KL Control and Rein-
forcement Learning (cf. Da Costa et al. (2020b) and Levine (2 018) for more details). Let
us restart with the expected free energy given by Equation 6:
G(π) ≈
T∑
τ=t+1
DKL[
expected outcomes

 
Q(Oτ |π) ||
prior preferences
  
P(Oτ ) ]  
expected risk
+ EQ(Sτ |π )[H[P(Oτ |Sτ )]]  
expected ambiguity
.
If the expected ambiguity is equal to zero, then the expected free energy reduces to
the expected risk, which is the cost function minimised in th e KL control literature. This
highlights that active inference generalises KL control (R awlik et al., 2013) by taking into
account the ambiguity of the mapping between the hidden stat es and the observations.
Active inference therefore selects policies leading to una mbiguous states. Furthermore, the
42
Active Inference and Variational Message Passing
expected risk can be re-written as follows:
expected risk = DKL[Q(Oτ |π)||P(Oτ )] = EQ(Oτ |π )[ln Q(Oτ |π)]  
negative entropy
−EQ(Oτ |π )[P(Oτ )]  
expected rewards
.
If the negative entropy is zero, then the expected free energ y reduces to the negative
expected prior preference. Those preferences encode the no tion of good outcomes, or equiv-
alently, the notion of rewarding observations. This highli ghts why active inference can be
thought of as a generalisation of reinforcement learning (M nih et al., 2013). Another view
on the expected free energy is:
G(τ,π ) =
(-ve) epistemic value
  
E ˜Q[ln Q(Sτ |π) −ln P(Sτ |Oτ ,π )] −
extrinsic value
  
E ˜Q[ln P(Oτ |π)], (39)
where ˜Q= P(Oτ |Sτ )Q(Sτ ). The extrinsic value is another term for expected prior pre fer-
ences, which is equivalent to expected rewards in reinforce ment learning. It is worth looking
in more detail at the negative epistemic value (-EV), which d iﬀerentiates the learning ob-
jectives of reinforcement learning and active inference:
−EV = −
epistemic value
  
−E ˜Q[ln Q(Sτ |π) −ln P(Sτ |Oτ ,π )]
⇔ EV = E ˜Q[ln P(Sτ |Oτ ,π ) −ln Q(Sτ |π)]
  
mutual information between Sτ and Oτ
.
Thus, the epistemic value is approximately equal to the mutu al information between Sτ
and Oτ . The mutual information encodes the expected information g ain over one variable
by knowing the value of another. Therefore, the epistemic va lue tells us how knowing future
observations reduces our uncertainty over future hidden st ates. The following should help
to see that the epistemic value is approximately equal to the mutual information between
Sτ and Oτ :
I(S; O) = DKL [ P(Sτ ,O τ )||P(Sτ )P(Oτ )]
= EP (Sτ ,O τ )[ln P(Sτ |Oτ ) + ln P(Oτ ) −ln P(Sτ ) −ln P(Oτ )]
= EP (Sτ ,O τ )[ln P(Sτ |Oτ ) −ln P(Sτ )].
Intuitively, the more an observation tells us about future s tates, the more valuable this
observation is. The negative epistemic value from equation 39 directly reﬂects this intuition,
and favours the policies with high mutual information. More importantly, equation 39 allows
the agent to compare the information gain and the reward on th e same scale, i.e. using
nats from information theory. This creates a sense in which a n active inference agent deals
optimally with the trade-oﬀ between exploration and exploi tation.
Appendix B: Useful Properties.
This appendix quickly reviews the properties used througho ut this paper.
43
Champion et al.
Product rule: P(X,Y ) = P(X|Y)P(Y),
where X and Y are random variables.
Linearity of expectation: EP (Y )[aY + b] = aEP (Y )[Y] + b,
where a and b are constants, and Y is a random variable.
Expectation of a constant: EP (Y )[a] = a,
where a is a constant, and Y is a random variable
Log property: ln(ab) = ln( a) + ln( b),
where a and b are real numbers
Exponential product property: exp(a+ b) = exp( a) exp(b),
where a and b are real numbers
Exponential power property: exp(ab) = exp( a)b,
where a and b are real numbers
Appendix C: Deﬁnition and Justiﬁcation of the Expected Free Energy.
In this appendix, we focus on the deﬁnition of the expected fr ee energy and the justiﬁcation
of Equation 6. Another good resource on the subject is the “ex pected free energy” appendix
of Smith et al. (2021). For the sake of simplicity, we assume t he following generative model
and variational distribution:
P(O0:T ,S 0:T , B|π) = P(B)P(S0)
T∏
τ=1
P(Sτ |Sτ−1, B,π )
T∏
τ=0
P(Oτ |Sτ )
Q(S0:T , B|π) = Q(B)
T∏
τ=0
Q(Sτ |π).
Furthermore, we let X = {B,S 0:T }denote the set of hidden variables of the model.
Note that in this appendix, we restrict ourself to the hidden variables X but new variables
such as A and D can be added without changing the idea of the following deriv ation.
Initially, the expected free energy was deﬁned as the variat ional free energy conditioned on
the policy, i.e.
G(π) = DKL [ Q(X|π)||P(O0:t,X |π)] .
However, the above deﬁnition does not take into account that observations will be made
in the future. To make up for this, the expected free energy ca n be extended as follows:
G(π) = E ˜Q
[
DKL [ Q(X|π)||P(O0:T ,X |π)]
]
where ˜Q=∆ ˜Q(Ot+1:T |π). (40)
Since the future observations ( Ot+1:T ) have not been made yet, we need to predict what
they could look like. This prediction relies on a predictive distribution ˜Q(Ot+1:T |π) that
encodes our best guess about future outcomes, and is general ly deﬁned as follows:
˜Q(Ot+1:T |π) =∆
T∏
τ=t+1
˜Q(Oτ |π),
44
Active Inference and Variational Message Passing
where ˜Q(Oτ |π) =∆ ∑
Sτ
˜Q(Oτ ,S τ |π) and ˜Q(Oτ ,S τ |π) =∆ P(Oτ |Sτ )Q(Sτ |π).
Note that the deﬁnition of ˜Q(Ot+1:T |π) assumes independence between time steps and
˜Q(Oτ |π) is obtained by marginalisation of ˜Q(Oτ ,S τ |π). By recalling the deﬁnition of the
generative model as well as the deﬁnition of the variational distribution, we obtain the
following from Equation 40:
G(π) = E ˜Q
[
DKL [ Q(S0:T , B|π)||P(O0:T ,S 0:T , B|π)]
]
= DKL [ Q(B)||P(B)] + DKL [ Q(S0|π)||P(S0)]
∑
+
t∑
τ=1
EQ(Sτ − 1, B|π )
[
DKL [ Q(Sτ |π)||P(Sτ |Sτ−1, B,π )]
]
+
t∑
τ=0
EQ(Sτ |π )
[
H[P(Oτ |Sτ )]
]
+
T∑
τ=t+1
EQ(Sτ − 1, B|π )
[
DKL [ Q(Sτ |π)||P(Sτ |Sτ−1, B,π )]
]
+ EQ(Sτ |π )
[
H[P(Oτ |Sτ )]
]
.
It must now be mentioned that the policy does not have much of a n impact on the
past and current hidden states ( S0:t). The terms relying on those states are then removed
from the expected free energy to avoid unnecessary computat ional costs. Additionally,
the divergence between Q(B) and P(B) does not depend on the policy and can be safely
ignored, leading to:
G(π) =
T∑
τ=t+1
G(π,τ ) (41)
where:
G(π,τ ) =∆ EQ(Sτ − 1, B|π )
[
DKL [ Q(Sτ |π)||P(Sτ |Sτ−1, B,π )]
]
+ EQ(Sτ |π )
[
H[P(Oτ |Sτ )]
]
.
We now focus on G(π,τ ) to bridge the gap between Equations 6 and 41. First, we merge
the two terms of the above equation together:
G(π,τ ) =∆ EP (Oτ |Sτ )Q(Sτ ,S τ − 1, B|π )
[
ln Q(Sτ |π) −ln P(Oτ ,S τ |Sτ−1, B,π )
]
.
Then, we break the second term within the expectation using t he product rule. Addi-
tionally, we realise that the following equation can be obta ined from the product rule:
P(Oτ |Sτ−1, B,π ) = P(Oτ ,S τ−1, B,π )
P(Sτ−1, B,π ) = P(Sτ−1, B,π |Oτ )
P(Sτ−1, B,π ) P(Oτ ) ≈P(Oτ ),
where we assumed that the fraction is equal to one. Doing this assumption means that
the observation Oτ brings us very little information, i.e. the posterior is clo se to the prior.
45
Champion et al.
Using the above result we get:
G(π,τ ) = E
[
ln Q(Sτ |π) −ln P(Sτ |Oτ ,S τ−1, B,π ) −ln P(Oτ |Sτ−1, B,π )
]
≈E
[
ln Q(Sτ |π) −ln P(Sτ |Oτ ,S τ−1, B,π ) −ln P(Oτ )
]
,
where the expectation is still over P(Oτ |Sτ )Q(Sτ ,S τ−1, B|π). Then, we uses Bayes theorem
on the second term, the fact that ( Oτ ⊥ ⊥Sτ−1, B,π )|Sτ and the log properties to get:
G(π,τ ) = E
[
ln Q(Sτ |π) −ln P(Sτ |Oτ ,S τ−1, B,π ) −ln P(Oτ )
]
= E
[
ln Q(Sτ |π) −ln P(Oτ |Sτ ,S τ−1, B,π )P(Sτ |Sτ−1, B,π )
P(Oτ |Sτ−1, B,π ) −ln P(Oτ )
]
≈E
[
ln Q(Sτ |π) −ln P(Oτ |Sτ )Q(Sτ |π)
Q(Oτ |π) −ln P(Oτ )
]
= E
[
ln Q(Oτ |π) −ln P(Oτ ) −ln P(Oτ |Sτ )
]
,
where we assumed that P(Sτ |Sτ−1, B,π ) ≈Q(Sτ |π) and P(Oτ |Sτ−1, B,π ) ≈Q(Oτ |π).
The ﬁrst assumption can be supported by the variational free energy (VFE) decomposition
in term of accuracy and complexity. Indeed, the VFE penalise s the divergence between
Q(Sτ |π) and P(Sτ |Sτ−1, B,π ). The second assumption can be supported as follows:
P(Oτ |Sτ−1, B,π ) =
∑
Sτ
P(Oτ ,S τ |Sτ−1, B,π )
≈
∑
Sτ
Q(Oτ ,S τ |π)
= Q(Oτ |π).
Assuming that the posterior P(Oτ ,S τ |Sτ−1, B,π ) can be approximated by Q(Oτ ,S τ |π).
The last step relies on the linearity of expectation and the e xpectation of a constant, leading
to the ﬁnal result:
G(π,τ ) = DKL [ Q(Oτ |π)||P(Oτ )] + EQ(Sτ |π )
[
H[P(Oτ |Sτ )]
]
.
Appendix D: The simplest generative model.
This appendix provides the reader with the smallest generat ive model that can be considered
as an active inference agent and aims to solve the k-armed ban dit problem. As shown in
Figure 19, this problem is composed of k slot machines or equi valently k actions that the
agent can perform. Each machine has a diﬀerent probability of producing a reward and
the agent must chose the action to perform to maximize the rew ards obtained. The agent
only observes either a reward or a punishment after the execu tion of an action. Additional
information related to the usage of active inference in the c ontext of the multi-arms bandit
(MAB) task can be found in (Markovic et al., 2021) where activ e inference was compared to
other major algorithms for solving MABs such as UCB sampling and Thompson sampling.
46
Active Inference and Variational Message Passing
U
O
O = 1 O = 2
U = 1 U = 2 U = 3
Actions
Outcomes
Figure 19: This ﬁgure illustrates the 3-armed bandit proble m and the generative model
used by the agent. Three slot machines are available to the ag ent and each machine has a
diﬀerent probability of producing a reward. Additionally, t here are two possible outcomes
when pulling a lever, the agent either wins plenty of money or gets nothing. The generative
model is composed of two nodes representing the possible out comes and actions. Finally,
the agent’s goal is to maximize the rewards obtained, by pick ing the best strategy.
To solve the bandit problem using active inference, the ﬁrst step is to create the gen-
erative model that encodes the agent’s beliefs of the enviro nment. Two random variables
are used for this purpose, O represents the possible outcomes and U the available actions.
Furthermore, P(O|U) determines how the observation depends on the action perfo rmed by
the agent, and P(U) encodes any prior preference over the available actions. M ore precisely,
P(O|U) and P(U) are categorical distributions deﬁned as follows:
P(O= i|U = j) = Aij and P(U = j) = aj ,
where Aij deﬁnes the probability of the i-th outcome given that the j-t h action is performed,
and aj encodes the prior over the j-th action. Note that even if the a ctive inference frame-
work provides a way to learn the matrix A, this section assumes that it is given to the
agent. The next step is to pick an inference method to compute the posterior over the
hidden state U. This section keeps things simple and uses Bayes theorem:
P(U = j|O= 1) = P(O= 1 |U = j)P(U = j)
P(O= 1) = P(O= 1 |U = j)P(U = j)∑
k P(O= 1 |U = k)P(U = k) = A1j aj∑
k A1kak
,
where the deﬁnition of the generative model has been used in t he last step and we condi-
tioned on O = 1 to infer the action that is more likely to be rewarding. At t his point, it
is possible to act in our environment either by sampling the n ext action to perform from
the posterior P(U|O = 1) or by picking the action with the highest posterior proba bility.
Additionally, the posterior can be reused as an empirical pr ior for the next time step as
follows:
P(U = j) ←P(U = j|O= 1) = A1j aj∑
k A1kak
.
This simple example does not capture the entire theoretical power of the active inference
framework. Nevertheless, it illustrates four important co ncepts related to the design and
47
Champion et al.
use of an active inference agent, namely, the design of a gene rative model, the inference
of the latent variable(s), the action selection process, an d the use of the posterior as an
empirical prior.
Appendix E: Possible future research.
In this appendix, we propose future research directions aim ing to understand the relation-
ship between P(π|γ) and P(π|α). The ﬁrst direction relies on the following link between
Dirichlet and gamma distributions. If we let X1,... ,Xk be mutually independent random
variables, each having a gamma distribution with parameter s θi for i = 1 ,...,k and if we
deﬁne Yi = Xi
X1+... +Xk
for i = 1 ,...,k , then ( Y1,...,Y k) ∼Dir(θ1,...,θ k). This naturally
leads to the hypothesis that the new generative model might b e a generalisation of the old
generative model when all θi are equal.
Another interesting fact that could be studied in more detai l comes from studying the
variance of the Dirichlet distribution. Recall that the var iance of the random variable Yi is
given by:
Var[Yi] =
˜θi(1 −˜θi)
θ0 + 1 ,
where ˜θi = θi
θ0
and θ0 = ∑ k
j=1 θj. If we stick to our deﬁnition of θ, i.e. θj = c−Gj with
c= − →cj ∀j, then we can study how the variance of Yj behaves as c goes to inﬁnity. Let us
begin with:
lim
c→+∞
˜θi = lim
c→+∞
θi
∑ k
j=1 θj
= lim
c→+∞
c−Gi
∑ k
j=1 c−Gj
= lim
c→+∞
c−Gi
kc−∑ k
j=1 Gj
= lim
c→+∞
c
kc = 1
k,
where we note that Gi and ∑ k
j=1 Gj become negligible as c→+∞. Returning to the limit
of the variance:
lim
c→+∞
Var[Yi] = lim
c→+∞
˜θ(1 −˜θ)
θ0 + 1 = lim
c→+∞
˜θ(1 −˜θ)(∑ k
j=1 c−Gj
)
+ 1
= 0 ,
where we used the fact that ˜θi tends towards 1
k (i.e. a constant w.r.t c) and therefore
the variance is only inﬂuenced by the c in the denominator, which tends towards + ∞.
Additionally, from the deﬁnition of the mode of the Dirichle t, we see that as c →+∞
then the mode of the distribution tends towards the centre of the simplex because the Gi
becomes negligible, i.e.
lim
c→+∞
mα =
[ 1
k ... 1
k
]
.
Combining the behaviour of the variance and the mode as c →+∞, we see that as c
increases the prior becomes more and more compact around the centre of the simplex. In
other words, the policy selection becomes more and more stoc hastic as c increases. This is
not without recalling the role of γ as highlighted previously in the caption of Figure 9.
Appendix F: Messages for B.
In this appendix, we provide the derivation of the messages f or B, which relies on the
conjugacy between a categorical and a Dirichlet distributi on. Let us start with the deﬁnition
48
Active Inference and Variational Message Passing
of P(B; b), which is a product of Dirichlet distributions that can be w ritten in the following
form:
ln P(B; b) = ln
∏
i,u
P(B[u]•i; b[u]•i) =
∑
i,u
ln Dir(B[u]•i; b[u]•i)
=
∑
i,u


b[u]1i −1
...
b[u]|S|i −1

·


ln B[u]1i
...
ln B[u]|S|i

−ln B(b[u]•i)

 
Logartithm of Dirichlet
=


b[1]11 −1
...
b[|U|]|S||S| −1



 
µ B (b)
·


ln B[1]11
...
ln B[|U|]|S||S|



 
uB(B)
−
∑
i,u
ln B(b[u]•i)
  
zB (b)
, (42)
where |U|is the number of possible actions. Let /llbracketa,b /rrbracketdenotes all the natural numbers
between a and b (inclusive). The random matrix B[u] has one child Sτ for each time
step τ ∈/llbracket1,T /rrbracketwhere action u has been predicted by the m-th policy, and its probability
mass function is given by Equation 32. Similarly, the probab ility mass function of Sτ−1 is
obtained from Equation 32 by decreasing all indexes τ by one. The ﬁrst step requires us to
re-write Equation 32 as a function of uB(B). This can be done by using the deﬁnition of
the dot product and re-arranging to obtain:
ln P(Sτ = k|B,S τ−1 = l,π = m) =


∑
k[π = k][Uk
τ−1 = 1][ Sτ−1 = 1][ Sτ = 1]
...∑
k[π = k][Uk
τ−1 = |U|][Sτ−1 = |S|][Sτ = |S|]



 
µ Sτ → B (Sτ ,S τ − 1,π )
·uB(B).
(43)
The second step aims to substitute Equations 42 and 43 within the variational message
passing equation (18), i.e.
ln Q∗(B) =
⣨


b[1]11 −1
...
b[|U|]|S||S| −1

·uB(B)
⟩
+
T∑
τ=1
⣨


∑
k[π = k][Uk
τ−1 = 1][ Sτ−1 = 1][ Sτ = 1]
...∑
k[π = k][Uk
τ−1 = |U|][Sτ−1 = |S|][Sτ = |S|]

·uB(B)
⟩
+ Const,
where ⟨•⟩refers to ⟨•⟩∼QB . Note that in the above Equation, b[u]ij are hyper parameters
that can therefore be considered as constants with respect t o the expectation ⟨•⟩∼QB . The
third step builds on this insight, by pulling the summation o ver time steps inside the vector,
factorising by uB(B), using the linearity of expectation and by taking the expon ential of
both sides to obtain:
Q∗(B) ∝exp{µ∗
B ·uB(B)}
49
Champion et al.
µ∗
B =


b[1]11 −1 + ∑
k,τ ⟨[π = k][Uk
τ−1 = 1][ Sτ−1 = 1][ Sτ = 1] ⟩
...
b[|U|]|S||S| −1 + ∑
k,τ ⟨[π = k][Uk
τ−1 = |U|][Sτ−1 = |S|][Sτ = |S|]⟩

.
By looking at Equations 32, one can see that ⟨[Sτ = i]⟩and ⟨[Sτ−1 = j]⟩are the i-th
and j-th elements of the vector ⟨uSτ (Sτ )⟩and ⟨uSτ −1(Sτ −1)⟩, respectively. Furthermore,
because P(π) is a categorical distribution it can be expressed as:
P(π|α) =


ln α1
...
ln α|π |



 
µ π (α )
·


[π = 1]
...
[π = |π|]



 
uπ (π )
, (44)
where |π|is the number of policies. The above equation highlights tha t ⟨[π = k]⟩is the
k-th element of ⟨uπ (π)⟩. Using those three insights, we proceed with the following r e-
parameterization (i.e. the fourth step):
µ∗
B =


b[1]11 −1 + ∑
k,τ [Uk
τ−1 = 1] ⟨uπ (π)⟩k⟨uSτ (Sτ )⟩1⟨uSτ −1(Sτ −1)⟩1
...
b[|U|]|S||S| −1 + ∑
k,τ [Uk
τ−1 = |U|]⟨uπ (π)⟩k⟨uSτ (Sτ )⟩|S|⟨uSτ −1(Sτ −1)⟩|S|

, (45)
where we focused on the optimal parameters because the rest r emains unchanged. The last
step consists of computing the expectation of ⟨uSτ −1(Sτ −1)⟩i, ⟨uSτ (Sτ )⟩j , and ⟨uπ (π)⟩k
for all i, j and k:
• ⟨uSτ −1(Sτ −1)⟩i = ⟨[Sτ −1 = i]⟩= ˜D(τ−1)i
• ⟨uSτ (Sτ )⟩j = ⟨[Sτ = j]⟩= ˜Dτj
• ⟨uπ (π)⟩k = ⟨[π = k]⟩= ˜αk
One last thing we need to look at is the interaction between th e summation and the
indicator function in the i-th line of Equation 45. Indeed, t he sum iterates over all time
steps τ and all policies k, but the indicator function ﬁlters out all elements where th e k-th
policy does not predict the i-th action at time τ−1. Building on this insight, we can now
substitute the above results in Equation 45:
Q∗(B) ∝exp
{ 


b[1]11 −1 + ∑
(k,τ )∈Ω 1 ˜αk ˜Dτ1 ˜D(τ−1)1
...
b[|U|]|S||S| −1 + ∑
(k,τ )∈Ω |U|
˜αk ˜Dτ|S| ˜D(τ−1)|S|


·uB(B)
}
.
Finally, one can recognise in the above equation the logarit hm of a product of Dirichlet
distributions written into their exponential form, i.e.
Q∗(B) =
∏
u,i
Dir(B[u]•i, b[u]•i) where b[u] =
b[u] +
∑
(k,τ )∈Ω u
˜αk ˜Dτ ⊗ ˜Dτ−1.
50
Active Inference and Variational Message Passing
Appendix G: Messages for Sτ .
This appendix shows how to derive the messages for Sτ for all time steps. We will use
Equations 26 and 32 that describe P(S0|D) and P(Sτ |Sτ−1, B,π ) as a function of uSτ (Sτ ).
The ﬁrst step requires us to re-arrange Equation 31 and P(Sτ+1|Sτ , B,π ) as a functions of
uSτ (Sτ ), where P(Sτ+1|Sτ , B,π ) is obtained by adding one to all instances of τin Equation
32. Those two re-arrangements lead to the following results :
ln P(Oτ = k|A,S τ = l) =


∑
i[Oτ = i] ln Ai1
...∑
i[Oτ = i] ln Ai|S|

·


[Sτ = 1]
...
[Sτ = |S|]



 
uSτ (Sτ )
(46)
ln P(Sτ+1 = k|B,S τ = l,π = m) =


∑
j,k,u [π = k][Uk
τ = u][Sτ+1 = j] ln B[u]1j
...∑
j,k,u [π = k][Uk
τ = u][Sτ+1 = j] ln B[u]|S|j

·uSτ (Sτ ).
(47)
For the second step, we need to substitute Equations 26, 32, 4 6 and 47 into the vari-
ational message passing equation. If τ = 0, the parent message will come from the prior
(i.e. Equation 26) otherwise from the past (i.e. Equation 32 ). Also, for all time steps such
that τ ≤t there is a message from the likelihood mapping (i.e. Equatio n 46) and for all
time steps except τ = T there is a message from the future (i.e. Equation 47). Puttin g
everything together we obtain:
ln Q∗(Sτ ) =
⣨
[τ = 0]


ln D1
...
ln D|S|

·uSτ (Sτ )
⟩
+
⣨
[τ ̸= 0]


∑
j,k,u
[π = k][Uk
τ−1 = u][Sτ−1 = j] ln B[u]1j
...∑
j,k,u [π = k][Uk
τ−1 = u][Sτ−1 = j] ln B[u]|S|j

·uSτ (Sτ )
⟩
+
⣨
[τ ≤t]


∑
i
[Oτ = i] ln Ai1
...∑
i [Oτ = i] ln Ai|S|

·uSτ (Sτ )
⟩
+
⣨
[τ ̸= T]


∑
j,k,u
[π = k][Uk
τ = u][Sτ+1 = j] ln B[u]1j
...∑
j,k,u [π = k][Uk
τ = u][Sτ+1 = j] ln B[u]|S|j

·uSτ (Sτ )
⟩
+ Const.
The third step requires us to factorise by uSτ (Sτ ), use the linearity of expectation and
take the exponential of both sides:
Q∗(Sτ ) ∝exp
{[
[τ= 0] µ∗
1 + [τ ̸= 0] µ∗
2 + [τ≤t]µ∗
3 + [τ̸= T]µ∗
4
]
·uSτ (Sτ )
}
, (48)
51
Champion et al.
where:
µ∗
1 =


⟨
ln D1⟩
...
⟨ln D|S|⟩


µ∗
2 =


∑
j,k,u
[Uk
τ−1 = u]⟨[π = k]⟩⟨[Sτ−1 = j]⟩⟨ln B[u]1j ⟩
...∑
j,k,u [Uk
τ−1 = u]⟨[π = k]⟩⟨[Sτ−1 = j]⟩⟨ln B[u]|S|j⟩


µ∗
3 =


∑
i
[Oτ = i]⟨ln Ai1⟩
...∑
i [Oτ = i]⟨ln Ai|S|⟩


µ∗
4 =


∑
j,k,u
[Uk
τ = u]⟨[π = k]⟩⟨[Sτ+1 = j]⟩⟨ln B[u]1j ⟩
...∑
j,k,u [Uk
τ = u]⟨[π = k]⟩⟨[Sτ+1 = j]⟩⟨ln B[u]|S|j⟩

.
The fourth step is the re-parameterization relying on the fa ct that ⟨ln Di⟩, ⟨[π = j]⟩,
⟨[Sτ−1 = k]⟩, ⟨ln B[l]mn⟩, ⟨ln Aop⟩and ⟨[Sτ+1 = q]⟩are elements of ⟨uD(D)⟩, ⟨uπ (π)⟩,
⟨uSτ − 1 (Sτ−1)⟩, ⟨uB(B)⟩, ⟨uA(A)⟩and ⟨uSτ +1 (Sτ+1)⟩, respectively. Focusing on the µ∗
i be-
cause the rest remains unchanged, the result of the the re-pa rameterisation is:
µ∗
1 =


⟨
uD(D)⟩1
...
⟨uD(D)⟩|S|


µ∗
2 =


∑
j,k,u
[Uk
τ−1 = u]⟨uπ (π)⟩k⟨uSτ − 1 (Sτ−1)⟩j ⟨uB(B)⟩u1j
...∑
j,k,u [Uk
τ−1 = u]⟨uπ (π)⟩k⟨uSτ − 1 (Sτ−1)⟩j ⟨uB(B)⟩u|S|j⟩


µ∗
3 =


∑
i
[Oτ = i]⟨uA(A)⟩i1
...∑
i [Oτ = i]⟨uA(A)⟩i|S|


µ∗
4 =


∑
j,k,u
[Uk
τ = u]⟨uπ (π)⟩k⟨uSτ +1 (Sτ+1)⟩j ⟨uB(B)⟩u1j
...∑
j,k,u [Uk
τ = u]⟨uπ (π)⟩k⟨uSτ +1 (Sτ+1)⟩j ⟨uB(B)⟩u|S|j

.
Finally, the last step consists of computing the expectatio ns of all suﬃcient statistics as
follows:
52
Active Inference and Variational Message Passing
• ⟨uD(D)⟩i = ⟨ln Di⟩= ψ(di) −ψ(∑
r dr) =∆ ¯Di
• ⟨uπ (π)⟩j = ⟨[π = j]⟩= ˜αj
• ⟨uSτ − 1 (Sτ−1)⟩k = ⟨[Sτ−1 = k]⟩= ˜D(τ−1)k
• ⟨uB(B)⟩lmn = ⟨ln B[l]mn⟩= ψ(b[l]mn) −ψ(∑
r b[l]rn) =∆ ¯B[l]mn
• ⟨uA(A)⟩op = ⟨ln Aop⟩= ψ(aop) −ψ(∑
r arp) =∆ ¯Aop
• ⟨uSτ +1 (Sτ+1)⟩q = ⟨[Sτ+1 = q]⟩= ˜D(τ+1)q
Substituting those expectations into the equations for the µ∗
i leads to the following
results: µ∗
1 = ¯D, µ∗
2 = ∑
k ˜αk ¯B[Uk
τ ] ˜Dτ−1, µ∗
3 = oτ · ¯A and µ∗
4 = ∑
k ˜αk ˜Dτ+1 · ¯B[Uk
τ ].
Where oτ is a one hot vector containing the observation made by the age nt and we used the
fact that the indicator function [ Uk
τ = u] ﬁlters out elements from the sum where u̸= Uk
τ .
The ﬁnal result is obtained by substituting the values of the µ∗
i ’s in Equation 48 to obtain
the following categorical distribution:
Q∗(Sτ ) ∝exp
{
µ∗
Sτ ·uSτ (Sτ )
}
µ∗
Sτ = [ τ = 0]
¯D + [τ̸= 0]
∑
k
˜αk ¯B[Uk
τ−1] ˜Dτ−1 + [τ ≤t]oτ · ¯A + [τ̸= T]
∑
k
˜αk ¯B[Uk
τ ] ˜Dτ+1.
Appendix H: Derivation of the new expected free energy.
In this appendix, we derive the expected free energy of our ne w model. First, we restate
the factorisation of the generative model and the variation al distribution:
P(O0:t,S 0:T ,π, A, B, D,α ) = P(π|α)P(α)P(A)P(B)P(S0 |D)P(D)
t∏
τ=0
P(Oτ |Sτ , A)
T∏
τ=1
P(Sτ |Sτ−1, B,π ) (49)
Q(S0:T ,π, A, B, D,α ) = Q(π)Q(A)Q(B)Q(D)Q(α)
T∏
τ=0
Q(Sτ ). (50)
Remembering from Appendix C that the expected free energy is deﬁned as:
G(π) = E ˜Q
[
DKL [ Q(X|π)||P(O0:T ,X |π)]
]
, (51)
where the latent variables are X = {S0:T , A, B, D,α }, ˜Q = ˜Q(Ot+1:T ) =∆ ∏ T
τ=t+1 ˜Q(Oτ )
and ˜Q(Oτ ) =∆ ∑
Sτ
˜Q(Oτ ,S τ ). Now we substitute Equation 49 and 50 into Equation 51 and
53
Champion et al.
simplify by removing the terms that are constant w.r.t the po licy π:
G(π) = E ˜Q
[
DKL [ Q(S0:T , A, B, D,α |π)||P(O0:T ,S 0:T , A, B, D,α |π)]
] ∑
= DKL [ Q(A)||P(A)] + DKL [ Q(B)||P(B)] + DKL [ Q(D)||P(D)]
∑
+ DKL [ Q(α)||P(α)] + EQ(D)
[
DKL [ Q(S0)||P(S0|D)]
] ∑
+
T∑
τ=1
EQ(Sτ − 1, B)
[
DKL [ Q(Sτ )||P(Sτ |Sτ−1, B,π )]
]
−
T∑
τ=0
EQ(Sτ , A) ˜Q(Ot+1:T )
[
ln P(Oτ |Sτ , A)
]
=
T∑
τ=1
EQ(Sτ − 1, B)
[
DKL [ Q(Sτ )||P(Sτ |Sτ−1, B,π )]
]
+ C
T∑
τ=0
=
T∑
τ=1
EQ(Sτ ,S τ − 1, B)
[
ln Q(Sτ ) −ln P(Sτ |Sτ−1, B,π )
]
+ C
T∑
τ=0
=
T∑
τ=1
EQ(Sτ − 1, B)
[
−EQ(Sτ )
[
ln P(Sτ |Sτ−1, B,π )
]
  
H[•]
]
+ C
T∑
τ=0
=
T∑
τ=1
EQ(Sτ − 1, B)
[
H
[
P(Sτ |Sτ−1, B,π )
] ]
+ C
T∑
τ=0
,
where H[•] refer to −EQ(Sτ )
[
ln P(Sτ |Sτ−1, B,π )
]
in the last equation.
References
Kent C. Berridge. The debate over dopamine’s role in reward: the case for incentive
salience. Psychopharmacology, 191(3):391–431, Apr 2007. ISSN 1432-2072. doi: 10.
1007/s00213-006-0578-x. URL https://doi.org/10.1007/s00213-006-0578-x .
Christopher Bishop and John Winn. Structured variational d istributions in vibes. In
Proceedings Artiﬁcial Intelligence and Statistics . Society for Artiﬁcial Intelligence
and Statistics, Society for Artiﬁcial Intelligence and Sta tistics, January 2003. URL
https://www.microsoft.com/en-us/research/publication/structured-variational-distributions-in-vibes/.
ISBN 0-9727358-0-1.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliﬀe. Variation al infer-
ence: A review for statisticians. Journal of the American Statistical Asso-
ciation, 112(518):859–877, 2017. doi: 10.1080/01621459.2017.12 85773. URL
https://doi.org/10.1080/01621459.2017.1285773.
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Ma nfred K. War-
muth. Occam’s razor. Information Processing Letters , 24(6):377 – 380,
54
Active Inference and Variational Message Passing
1987. ISSN 0020-0190. doi: https://doi.org/10.1016/0020 -0190(87)90114-1. URL
http://www.sciencedirect.com/science/article/pii/0020019087901141.
Matthew Botvinick and Marc Toussaint. Planning as inferenc e. Trends in Cognitive Sci-
ences, 16(10):485 – 488, 2012. ISSN 1364-6613. doi: https://doi. org/10.1016/j.tics.2012.
08.006.
Howard Bowman and Su Li. Cognition, concurrency theory and r everberations in the
brain: in search of a calculus of communicating (recurrent) neural systems. In An-
drei Voronkov and Margarita Korovina, editors, Higher-Order Workshop on Auto-
mated Runtime Veriﬁcation and Debugging, EasyChair Proceed ings, Festschrift celebrat-
ing Howard Barringer’s 60th Birthday , volume 1. EasyChair, December 2011. URL
https://kar.kent.ac.uk/30708/.
C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowl ing, P. Rohlfshagen,
S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in Game s, 4(1):1–43,
2012.
Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and A nil K. Seth. The free
energy principle for action and perception: A mathematical review. Journal of Mathe-
matical Psychology , 81:55 – 79, 2017. ISSN 0022-2496. doi: https://doi.org/10 .1016/j.
jmp.2017.09.004.
Th´ eophile Champion, Howard Bowman, and Marek Grze´ s. Active inference and tree search,
2021.
Marco Cox, Thijs van de Laar, and Bert de Vries. A factor graph approach to automated de-
sign of Bayesian signal processing algorithms. Int. J. Approx. Reason. , 104:185–204, 2019.
doi: 10.1016/j.ijar.2018.11.002. URL https://doi.org/10.1016/j.ijar.2018.11.002.
F. G. Cozman. Generalizing variable elimination in Bayesia n networks. Proc.
IBERAMIA/SBIA-2000 Workshops (Workshop on Probabilistic Rea soning in
Artiﬁcial Intelligence) , 2000. doi: 10.1016/S0004-3702(00)00029-1. URL
https://ci.nii.ac.jp/naid/30008396546/en/.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Vese lic, Victorita Neacsu, and
Karl Friston. Active inference on discrete state-spaces: a synthesis, 2020a.
Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, an d Ryan Smith. The rela-
tionship between dynamic programming and active inference : the discrete, ﬁnite-horizon
case, 2020b.
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopamine,
reward learning, and active inference. Frontiers in Computational Neuro-
science, 9:136, 2015. ISSN 1662-5188. doi: 10.3389/fncom.2015.00 136. URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
55
Champion et al.
Jerry A. Fodor and Zenon W. Pylyshyn. Connectionism and cogn itive ar-
chitecture: A critical analysis. Cognition, 28(1):3 – 71, 1988. ISSN
0010-0277. doi: https://doi.org/10.1016/0010-0277(88) 90031-5. URL
http://www.sciencedirect.com/science/article/pii/0010027788900315.
G. D. Forney. Codes on graphs: normal realizations. IEEE Transactions on Information
Theory, 47(2):520–548, 2001.
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference
agents using Monte-Carlo methods, 2020.
Charles W. Fox and Stephen J. Roberts. A tutorial on variatio nal bayesian inference.
Artiﬁcial Intelligence Review , 38(2):85–95, Aug 2012. ISSN 1573-7462. doi: 10.1007/
s10462-011-9236-8. URL https://doi.org/10.1007/s10462-011-9236-8 .
Karl Friston. The free-energy principle: a uniﬁed brain the ory? Nature Reviews Neu-
roscience, 11(2):127–138, Feb 2010. ISSN 1471-0048. doi: 10.1038/nr n2787. URL
https://doi.org/10.1038/nrn2787.
Karl Friston. A free energy principle for a particular physi cs, 2019.
Karl Friston, Philipp Schwartenbeck, Thomas Fitzgerald, M ichael Moutoussis, Tim
Behrens, and Raymond Dolan. The anatomy of choice: active in ference and agency. Fron-
tiers in Human Neuroscience , 7:598, 2013. ISSN 1662-5161. doi: 10.3389/fnhum.2013.
00598. URL https://www.frontiersin.org/article/10.3389/fnhum.2013.00598.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christo ph Mathys, Thomas Fitzger-
ald, and Giovanni Pezzulo. Active inference and epistemic v alue. Cognitive
Neuroscience, 6(4):187–214, 2015. doi: 10.1080/17588928.2015.102005 3. URL
https://doi.org/10.1080/17588928.2015.1020053. PMID: 25689102.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philip p Schwartenbeck, John O Do-
herty, and Giovanni Pezzulo. Active inference and learning . Neuroscience & Biobehavioral
Reviews, 68:862 – 879, 2016. ISSN 0149-7634. doi: https://doi.org/ 10.1016/j.neubiorev.
2016.06.022.
Karl Friston, Thomas Parr, and Peter Zeidman. Bayesian mode l reduction. arXiv e-prints ,
art. arXiv:1805.07092, May 2018.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hes p, and Thomas Parr. Sophis-
ticated inference, 2020.
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni P ezzulo, J. Allan Hobson,
and Sasha Ondobaka. Active Inference, Curiosity and Insigh t. Neural Computa-
tion, 29(10):2633–2683, 10 2017a. ISSN 0899-7667. doi: 10.1162 /neco a 00999. URL
https://doi.org/10.1162/neco_a_00999.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphica l brain: Belief propagation
and active inference. Network Neuroscience , 1(4):381–414, 2017b. doi: 10.1162/NETN \
a\00018. URL https://doi.org/10.1162/NETN_a_00018.
56
Active Inference and Variational Message Passing
Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, an d Howard Bowman. Deep
temporal models and active inference. Neuroscience & Biobehavioral Reviews , 90:486 –
501, 2018. ISSN 0149-7634. doi: https://doi.org/10.1016/ j.neubiorev.2018.04.004. URL
http://www.sciencedirect.com/science/article/pii/S0149763418302525.
Michael L; Hill Dina E; Ivers Bonnie J; Goldson Edward Gabrie ls, Robin L; Cuccaro.
Repetitive behaviors in autism: relationships with associ ated clinical features. Research
in developmental disabilities , 2005. ISSN 0891-4222.
R. Conor Heins, M. Berk Mirza, Thomas Parr, Karl Friston, Igo r Kagan, and Are-
zoo Pooresmaeili. Deep active inference and scene construc tion. Frontiers in Arti-
ﬁcial Intelligence , 3:81, 2020. ISSN 2624-8212. doi: 10.3389/frai.2020.5093 54. URL
https://www.frontiersin.org/article/10.3389/frai.2020.509354.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts h uman at-
tention. Vision Research , 49(10):1295 – 1306, 2009. ISSN 0042-
6989. doi: https://doi.org/10.1016/j.visres.2008.09.0 07. URL
http://www.sciencedirect.com/science/article/pii/S0042698908004380. Vi-
sual Attention: Psychophysics, electrophysiology and neu roimaging.
Masayasu Kojima and Kenji Kangawa. Ghrelin: Structure and f unction. Phys-
iological Reviews , 85(2):495–522, 2005. doi: 10.1152/physrev.00012.2004. URL
https://doi.org/10.1152/physrev.00012.2004. PMID: 15788704.
D Koller and N Friedman. Probabilistic graphical models, ma ssachusetts, 2009.
F. R. Kschischang, B. J. Frey, and H. . Loeliger. Factor graph s and the sum-product
algorithm. IEEE Transactions on Information Theory , 47(2):498–519, 2001. doi: 10.
1109/18.910572.
K. S. Lam. The repetitive behavior scale-revised : Independ ent validation in individuals
with autism spectrum disorders. Journal of Autism and Developmental Disorders , 37:
855–866, 2007. URL https://ci.nii.ac.jp/naid/20001501751/en/.
Guillaume Lample and Devendra Singh Chaplot. Playing fps ga mes with deep reinforcement
learning, 2016.
Yann LeCun and Corinna Cortes. MNIST handwritten digit data base. 2010. URL
http://yann.lecun.com/exdb/mnist/.
Sergey Levine. Reinforcement learning and control as proba bilistic inference: Tutorial and
review, 2018.
Wu Lin, Nicolas Hubacher, and Mohammad Emtiyaz Khan. Variat ional message passing
with structured inference networks, 2018.
Dimitrije Markovic, Hrvoje Stojic, Sarah Schwoebel, and St efan J. Kiebel. An empirical
evaluation of active inference in multi-armed bandits, 202 1.
57
Champion et al.
Beren Millidge, Alexander Tschantz, and Christopher L Buck ley. Whence the expected free
energy?, 2020.
M. Berk Mirza, Rick A. Adams, Christoph D. Mathys, and Karl J. Friston. Scene
construction, visual foraging, and active inference. Frontiers in Computational Neu-
roscience, 10:56, 2016. ISSN 1662-5188. doi: 10.3389/fncom.2016.00 056. URL
https://www.frontiersin.org/article/10.3389/fncom.2016.00056.
M. Berk Mirza, Rick A. Adams, Christoph Mathys, and Karl J. Fr iston. Hu-
man visual exploration reduces uncertainty about the sense d world. PLOS
ONE, 13(1):1–20, 01 2018. doi: 10.1371/journal.pone.0190429 . URL
https://doi.org/10.1371/journal.pone.0190429.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Grav es, Ioannis Antonoglou,
Daan Wierstra, and Martin Riedmiller. Playing atari with de ep reinforcement learning,
2013.
Kevin Murphy, Yair Weiss, and Michael I. Jordan. Loopy belie f propagation for approximate
inference: An empirical study, 2013.
D. Ognibene and G. Baldassare. Ecological active vision: Fo ur bioinspired principles to
integrate bottom–up and adaptive top–down attention teste d with a simple camera-arm
robot. IEEE Transactions on Autonomous Mental Development , 7(1):3–25, 2015.
Thomas Parr and Karl J Friston. Generalised free energy and a ctive inference:
can the future cause the past? bioRxiv, 2018. doi: 10.1101/304782. URL
https://www.biorxiv.org/content/early/2018/04/23/304782.
Thomas Parr, Markovic Dimitrije, Stefan J. Kiebel, and Karl J. Friston. Neu-
ronal message passing using mean-ﬁeld, Bethe, and marginal approxima-
tions. Scientiﬁc Reports (Nature Publisher Group) , 9(1), Dec 2019. URL
http://library.kent.ac.uk/cgi-bin/resources.cgi?url=https://www.proquest.com/scholarly-journals/neuronal-message-passing-using-mean-field-bethe/docview/2 179737260/se-2?accountid=7408.
Copyright - This work is published under http://creativeco mmons.org/licenses/by/4.0/
(the “License”). Notwithstanding the ProQuest Terms and Co nditions, you may use this
content in accordance with the terms of the License.
Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov bla nkets, in-
formation geometry and stochastic thermodynamics. Philosophical Trans-
actions of the Royal Society A: Mathematical, Physical and E ngineering
Sciences, 378(2164):20190159, 2020. doi: 10.1098/rsta.2019.0159 . URL
https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2019.0159.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On st ochastic optimal control
and reinforcement learning by approximate inference (exte nded abstract). In Proceedings
of the Twenty-Third International Joint Conference on Artiﬁcia l Intelligence , IJCAI ’13,
page 3052–3056. AAAI Press, 2013. ISBN 9781577356332.
Wolfram Schultz, Peter Dayan, and P. Read Montague. A neural substrate of prediction
and reward. Science, 275(5306):1593–1599, 1997. ISSN 0036-8075. doi: 10.1126 /science.
275.5306.1593. URL https://science.sciencemag.org/content/275/5306/1593.
58
Active Inference and Variational Message Passing
Philipp Schwartenbeck, Johannes Passecker, Tobias U Hause r, Thomas H B FitzGer-
ald, Martin Kronbichler, and Karl Friston. Computational m echanisms of curios-
ity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272. URL
https://www.biorxiv.org/content/early/2018/09/07/411272.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Lau rent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonogl ou, Vedavyas Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nha m, Nal Kalchbren-
ner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach , Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with de ep neural networks
and tree search. Nature, 529(7587):484–489, 2016. doi: 10.1038/nature16961. URL
https://doi.org/10.1038/nature16961.
Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step -by-step tutorial on active in-
ference and its application to empirical data, 2021. URL https://psyarxiv.com/b4jm6/.
Oleg Solopchuk. Tutorial on active inference, 2018. URL
https://medium.com/@solopchuk/tutorial-on-active-in ference-30edcf50f5dc.
Shyam Sundar Rajagopalan, Abhinav Dhall, and Roland Goecke . Self-stimulatory be-
haviours in the wild for autism diagnosis. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV) Workshops , June 2013.
A. Tschantz, M. Baltieri, A. K. Seth, and C. L. Buckley. Scali ng active inference. In
2020 International Joint Conference on Neural Networks (IJCNN) , pages 1–8, 2020. doi:
10.1109/IJCNN48605.2020.9207382.
Kai Ueltzh¨ oﬀer. Deep active inference. Biological Cybernetics , 112(6):547–
573, Dec 2018. ISSN 1432-0770. doi: 10.1007/s00422-018-07 85-7. URL
https://doi.org/10.1007/s00422-018-0785-7 .
Thijs van de Laar and Bert de Vries. Simulating active infere nce processes by message
passing. Front. Robotics and AI , 2019, 2019a. doi: 10.3389/frobt.2019.00020. URL
https://doi.org/10.3389/frobt.2019.00020.
Thijs W. van de Laar and Bert de Vries. Simulating active infe r-
ence processes by message passing. Frontiers in Robotics and AI , 6:
20, 2019b. ISSN 2296-9144. doi: 10.3389/frobt.2019.00020 . URL
https://www.frontiersin.org/article/10.3389/frobt.2019.00020.
Toon Van de Maele, Tim Verbelen, Ozan C ¸ atal, Cedric De Boom, and Bart Dhoedt.
Active vision for robot manipulators using the free energy p rinciple. Frontiers in
Neurorobotics, 15:14, 2021. ISSN 1662-5218. doi: 10.3389/fnbot.2021.64 2780. URL
https://www.frontiersin.org/article/10.3389/fnbot.2021.642780.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinfo rcement learning with double
Q-learning, 2015.
59
Champion et al.
Samuel T. Wauthier, Ozan C ¸ atal, Cedric De Boom, Tim Verbele n, and Bart Dhoedt. Sleep:
Model reduction in deep active inference. In Tim Verbelen, P ablo Lanillos, Christo-
pher L. Buckley, and Cedric De Boom, editors, Active Inference , pages 72–83, Cham,
2020. Springer International Publishing. ISBN 978-3-030- 64919-7.
Wim Wiegerinck. Variational approximations between mean ﬁ eld theory
and the junction tree algorithm. In Craig Boutilier and Mois ´ es Gold-
szmidt, editors, UAI ’00: Proceedings of the 16th Conference in Uncer-
tainty in Artiﬁcial Intelligence, Stanford University, Sta nford, California, USA,
June 30 - July 3, 2000 , pages 626–633. Morgan Kaufmann, 2000. URL
https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=73&proceeding_id=16.
John Winn and Christopher Bishop. Variational message pass ing. Journal of Machine
Learning Research, 6:661–694, 2005.
Eric P. Xing, Michael I. Jordan, and Stuart J. Russell. A gene ralized mean ﬁeld algo-
rithm for variational inference in exponential families. CoRR, abs/1212.2512, 2012. URL
http://arxiv.org/abs/1212.2512.
J. S. Yedidia. Constructing free-energy approximations an d generalized belief propagation
algorithms. IEEE Trans. Information Theory , 51(7):2282–2312, 2005. doi: 10.1109/TIT.
2005.850085. URL https://ci.nii.ac.jp/naid/30019661350/en/.
Jonathan S. Yedidia. Message-passing algorithms for infer ence and optimization. Jour-
nal of Statistical Physics , 145(4):860–890, Nov 2011. ISSN 1572-9613. doi: 10.1007/
s10955-011-0384-7. URL https://doi.org/10.1007/s10955-011-0384-7 .
Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Gen eralized belief propagation.
In Proceedings of the 13th International Conference on Neural Inf ormation Processing
Systems, NIPS’00, page 668–674, Cambridge, MA, USA, 2000. MIT Press .
60