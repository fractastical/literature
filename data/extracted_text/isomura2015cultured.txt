RESEARCH ARTICLE
Cultured Cortical Neurons Can Perform Blind
Source Separation According to the Free-
Energy Principle
Takuya Isomura1,2*, Kiyoshi Kotani3,4, Yasuhiko Jimbo5
1 Department of Human and Engineered Environmental Studies, Graduate School of Frontier Sciences, The
University of Tokyo, Hongo, Bunkyo-ku, Tokyo, Japan,2 Research Fellow of Japan Society for the
Promotion of Science (JSPS), Kojimachi, Chiyoda-ku, Tokyo, Japan,3 Research Center for Advanced
Science and Technology, The University of Tokyo, Komaba, Meguro-ku, Tokyo, Japan,4 PRESTO, Japan
Science and Technology Agency, Honcho, Kawaguchi-shi, Saitama, Japan,5 Department of Precision
Engineering, School of Engineering, The University of Tokyo, Hongo, Bunkyo-ku, Tokyo, Japan
* isomura@neuron.t.u-tokyo.ac.jp
Abstract
Blind source separation is the computation underlying the cocktail party effect–– a partygoer
can distinguish a particular talker’s voice from the ambient noise. Early studies indicated
that the brain might use blind source separation as a signal processing strategy for sensory
perception and numerous mathematical models have been proposed; however, it remains
unclear how the neural networks extract particular sources from a complex mixture of
inputs. We discovered that neurons in cultures of dissociated rat cortical cells could learn to
represent particular sources while filtering out other signals. Specifically, the distinct classes
of neurons in the culture learned to respond to the distinct sources after repeating training
stimulation. Moreover, the neural network structures changed to reduce free energy, as pre-
dicted by the free-energy principle, a candidate unified theory of learning and memory, and
by Jaynes’principle of maximum entropy. This implicit learning can only be explained by
some form of Hebbian plasticity. These results are the firstin vitro(as opposed toin silico)
demonstration of neural networks performing blind source separation, and the first formal
demonstration of neuronal self-organization under the free energy principle.
Author Summary
The ‘cocktail party’effect is a phenomenon by which one is able to pick out and listen to a
single person’s speech in a noisy room. In information engineering, this is termed blind
source separation. Numerous computational studies demonstrate that simulated neural
networks can perform blind source separation. However, if or how a living neural network
learns to perform blind source separation remains unknown. Using a microelectrode array
(MEA) system that allowed us to apply composite inputs and record responses from neu-
rons throughout a cultured neural network, we discovered that even neurons in cultures of
dissociated rat cortical cells can separate individual sources from a complex mixture of
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 1 / 29
OPEN ACCESS
Citation: Isomura T, Kotani K, Jimbo Y (2015)
Cultured Cortical Neurons Can Perform Blind Source
Separation According to the Free-Energy Principle.
PLoS Comput Biol 11(12): e1004643. doi:10.1371/
journal.pcbi.1004643
Editor: Jeff Beck, Duke University, UNITED STATES
Received: July 27, 2015
Accepted: November 3, 2015
Published: December 21, 2015
Copyright: © 2015 Isomura et al. This is an open
access article distributed under the terms of the
Creative Commons Attribution License, which permits
unrestricted use, distribution, and reproduction in any
medium, provided the original author and source are
credited.
Data Availability Statement:All relevant data are
within the paper and its Supporting Information files
(S1 Dataset). All spike number train files (detailed
data more than 50 MB) are available from our web
site (http://neuron.t.u-tokyo.ac.jp/).
Funding: This work was partially supported by the
Japan Society for the Promotion of Science (https://
www.jsps.go.jp/english/) through Grants-in-Aid for
Scientific Research (KAKENHI), Grants 23240065
and 26560202 (YJ), and Grant-in-Aid for JSPS
Fellows, Grant 26-8435 (TI). The funders had no role
in study design, data collection and analysis, decision
to publish, or preparation of the manuscript.
inputs in the absence of teacher signals. Given these findings, we then determined that the
neural networks adapted to reduce free energy, as predicted by the free energy principle
and Jaynes’principle of maximum entropy. These results provide evidence that cultured
neural networks can perform blind source separation and that they are governed by the
free-energy principle, providing a compelling framework for understanding how the brain
identifies and processes signals hidden in complex multivariate information.
Introduction
Blind source separation is a problem of separating independent sources from a complex mix-
ture of inputs without knowledge about sources [1– 4] and is the computation underlying the
cocktail party effect–– a phenomenon by which one is able to listen to a single person’s speech
in a noisy room [5– 8]. Understanding the basis of blind source separation, as well as other
learning and memory processes, requires characterization of the underlying functional network
architecture. Presumably, this can be directly accomplished by measuring the activity of indi-
vidual neurons during blind source separation processing to establish the role of each neuron
in the network. In practice, this is enormously challenging, given both the large number of neu-
rons that may reside in a network and the technical limitations encountered in attempting to
distinguish the activity of neurons that perform blind source separation from others through-
out the network. As a result, most studies of blind source separation rely on simulations and on
computational models, and the possible electrophysiological basis for any such information
processing in real neurons remains poorly understood.
Theoretically, blind source separation is classed as unsupervised learning, a type of learning
that does not require teacher signals [9– 11]. Blind source separation is modeled as principal
component analysis (PCA) [12], as independent component analysis (ICA) [13, 14], or as
sparse coding [15, 16]. These are widely used for signal processing where separation of sources
from a complex mixture of inputs is desired. Neural network models that include neurons with
linear firing rates can perform PCA, a model that describes how neurons in artificial networks
can strengthen or weaken their interconnections over time [12]. In contrast, ICA, which can be
represented using model neurons with nonlinear firing rates [13, 14], maximizes Shannon
entropy among outputs in order to detect several independent sources, thus separating a multi-
variate signal into individual components. The sparse coding model detects independent
sources [15, 16] using a calculation similar to that proposed by the predictive coding hypothesis
of the cerebral cortex [17]. What all these models of unsupervised learning have in common is
that they can be implemented with a form of Hebbian or associative plasticity [18] and that
they are instances of the free energy principle–– a candidate unified theory of learning and
memory [19, 20]. Moreover, blind source separation, whether by PCA, ICA, or sparse coding,
is one of the simplest problems that the free-energy principle addresses. Additionally, numer-
ous computational studies have demonstrated that simulated neural networks can perform
blind source separation. PCA, ICA, and sparse coding have been demonstrated in both firing-
rate models and spiking-neuron models [21– 27]. However, although early studies indicated
that cortical neurons might use an ICA-like signal processing strategy for sensory perception
[5– 8] and described the relationship of sparse- and predictive coding to biological properties
[28, 29], examinations of the neural basis of ICA-like learning are few.
Experimental studies onin vivoor in vitronetworks have demonstrated that neural net-
works can perform learning and memory tasks, when learning is defined as the process of
changing activity or behavior by experiencing something, as it is in this study. One of the
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 2 / 29
Competing Interests:The authors have declared
that no competing interests exist.
simplest networks can be constructed from actual cultured neurons, and such real neural net-
works can exhibit stimulation-dependent synaptic plasticity [30, 31], supervised learning [32],
adaptation to inputs [33], associative memory [34], aspects of logical operation [35, 36], short-
term memory [37], and homeostatic plasticity [38, 39]. However, it is uncertain whether these
biological neural networks can perform blind source separation. Previously, we have used the
microelectrode array (MEA) to simultaneously stimulate and record from multiple neurons
over long periods [30, 40]. The MEA enables random electrical stimulation from 64 electrodes
in parallel and the recording of evoked spikes immediately after each stimulation. Thus, by
varying probabilities during stimulation trains, the MEA makes it possible to apply spatiotem-
poral inputs synthesized from hidden sources while measuring the response evoked from the
entire neural network. Through this capability, we demonstrate here that cultured rat cortical
neurons receiving multiple inputs can perform blind source separation, thereby providing an
in vitromodel of neural adaptation.
In brief, our approach consisted of two parts. First, we tried to establish whether single neu-
ron responses preferred mixtures of sources or the individual sources per se. To address this,
we examined the Kullback-Leibler divergence [11] between the probabilities of neuronal
responses conditioned upon one of two sources. We hoped to see that neurons were able to dis-
criminate between sources rather than mixtures, because this would imply a blind source sepa-
ration–– or the inversion of a generative model of stimulation patterns (i.e., sources). We were
able to show that neurons preferred hidden sources, as opposed to mixtures of sources. This
then allowed us to quantify the probabilistic encoding of sources by assuming that the expected
amplitude of each hidden source was encoded by the mean activity of neuronal populations
preferring one source or the other. By assuming a rate coding model, where mean firing rates
encode the mean of a mixture of Gaussians, we were able to compute the variational free energy
of the neuronal encodings in terms of energy and entropy. Crucially, the free energy principle
suggests that with learning, energy should decrease and entropy should increase (where the
free energy is the difference) [19, 20]. In this instance, the energy can be thought of as level of
prediction error. Conversely, the entropy refers to the average uncertainty of the encoding.
According to Jaynes’maximum entropy principle [41, 42], entropy should increase to ensure a
generalizable inference that is in accordance with Occam’s principle. In short, we hoped to see
an increase in the entropy of the probabilistic encoding that was offset by a decrease in energy
(an increase in accuracy)–– producing an overall decrease in free energy.
Results
Generation and definition of neural stimuli
Rat cortical cells were cultured on MEAs (Fig 1A and 1B) and electrical stimulation and record-
ings were conducted. Typical stimulus-evoked responses of cultured neural networks recorded
at the stimulated electrode are shown inFig 1C. In accordance with previous studies, we
observed tri-phasic responses [30, 40].
To study ICA-like learning in networks created in these neuronal cultures, we designed a
generative process constructed from two independent binary sourcesu(t)=( u1(t), u2(t))T
2 {0,1}, 32 inputs produced by the MEAs(t)=( s1(t), ... , s32(t))T, and a 32×2 matrixA, where
(Ai1, Ai2)=( a,1 – a) fori =1 ,... , 16 and (Ai1, Ai2)=( 1– a, a) fori = 17,... ,3 2(Fig 2A). Note
that t [s] is discrete time (a natural number) between 1 and 256.
In brief, we had an array of (8×8) 64 recording electrode sites of which half (32) were stimu-
lated. The remaining 32 were for recording neural activities at non-stimulated electrodes. The
detailed neural response properties are discussed in the next section. The stimuli were formed
by mixing two underlying patterns, or hidden sources, to create stochastic stimulus patterns.
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 3 / 29
These were mixed separately for each of two groups of 16 stimulation electrodes, such that the
stimulation pattern comprised of probabilistic mixtures of the underlying sources. The
responses from the 64 electrodes and 23 cultures were pooled, yielding over 1000 electrode
responses to various mixtures of hidden sources.
In other words,u(t) was generated from the stationary Poisson process, whiles(t) obeyed
the non-stationary Poisson process with the time varying intensity ofA u(t). The generative
model ensured that the two sources contributed to the stimuli with an equal probabilityρ.W e
used mixtures of these sources to produce stimulus patterns that contained no signal, one of
the two sources, and a fully mixed source. Unless specifically mentioned, we usedρ = 1/2 and
a = 3/4. Electrical stimulations with 256-s pulse trains were applied at 1-s intervals for 100 tri-
als. A schematic image of how inputss(t) were obtained from sourcesu(t) is shown inFig 2B
and 2C. A detailed description is provided in theFig 2legend and the Methods section.
Evoked responses show preferences to individual stimuli
Neural responses evoked by the input trains were recorded using a 64-electrode MEA. We used
23 cultures for a training group and a total of 37 cultures as control groups. We performed 100
trials (500 s for 1 trial; about 14 h in total) for each culture. An overview of the experimental
paradigm is shown inFig 3and S1 and S2 Movies. A raster plot and post stimulus time histo-
gram (PSTH) detailing the spike timing of evoked response recorded at a representative elec-
trode (x
i(τ); τ, continuous time) is shown inFig 4A and 4B. Evoked response increased
immediately after each stimulation for both stimulated and non-stimulated neuron groups.
The peak of evoked responses was observed 10-to-20 ms after each stimulation in all trials.
Fig 1. Description of the cultured neuron network paradigm.Panels show culture dish (top left),
microscopic view of rat cortical cells (top right), and evoked voltage trace from one electrode (bottom).(A)
Image of a microelectrode array (MEA) dish. Cells were seeded in the center of the MEA dishes. The
microelectrode sampling frequency was 25 kHz and a 500– 2000 Hz band-pass filter was applied to the
recordings. (B) Phase-contrast microscopic images of cultured rat cortical cells on MEA dishes after 52 days
in culture. Note the high concentration of cells near the electrode terminals. Black squares are electrodes.(C)
A typical waveform of the extracellular potential. After a biphasic-pulse electrical stimulation (τ = 0), several
stimulation-evoked spikes were observed at the stimulated electrode. The dashed line indicates the detection
threshold. Red circles indicate detected spikes.
doi:10.1371/journal.pcbi.1004643.g001
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 4 / 29
Fig 2. Schematic images of experimental protocol.The signal distribution scheme (top), signal generation protocol (bottom left), and training timeline
(bottom right) are shown.(A) A schematic image of stimulation signals and neurons on an MEA.u1 and u2 are hidden sources (left),s1, ... , s16 and s17, ... ,
s32 are two groups of inputs synthesized by computing weighted combinations of the sources (middle), andx1, ... , x64 are the measures at the electrodes
(right). By manipulating the inputs to the MEA, we induced an ICA-like processing system in neuronal cultures. Specifically, we created four conditions of
stimulation by independently varying the probabilities of two binary hidden signals (given by a 2 × 2 matrix) such that neurons received no signal (0,0), a
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 5 / 29
Compared to the results of the first trial (Fig 4A), the evoked response for the hidden source of
u = (0,1) (blue curve) decreased after the training stimulation (Fig 4B), indicating that neurons
recorded at this electrode tuned their activity to only respond to the (1,0) and (1,1) states, i.e.,
only tou1.
According to previous studies, the directly evoked responses occur immediately after stimu-
lation and their jitters are relatively small; thus, large numbers of spikes that appear more than
signal in which the two hidden signals (0,1) or (1,0) were differentially weighted, or a fully merged signal (1,1). The schematic heads illustrate theanalogy
between the experimental setup and the cocktail party effect:u1 and u2 are analogous to two voices to identify (left), the mapping fromu to s (a matrixA)i s
analogous to the addition of background noise at a cocktail party, so thats represents sounds (sensory inputs) heard from the left and right ears (middle), and
x is analogous to the listener’s auditory system performing blind source separation (right).(B) A schematic image of the generation ofs(t) fromu1(t) andu2(t).
Each random variableω is independently generated from a uniform distribution (0/C20ω < 1). u1(t) andu2(t) will be 1 ifω < ρ or 0 otherwise. Then,s1(t), ... ,
s16(t) will beu1(t)i fω < a or u2(t) otherwise. In contrast,s17(t), ... , s32(t) will beu1(t)i fω < 1– a or u2(t) otherwise. The discrete timet is over one and 256.(C) A
training timeline. As electrodes on the MEA are distributed as an 8 × 8 matrix, we illustrate the stimulating sites corresponding tos1(t), ... , s32(t)o n8×8
matrices. Thus, half (32) were dual-use electrodes of stimulating and recording, while the remaining 32 were for recording only. Red or blue squares indicate
the electrode stimulated in a given time period, which is provided fromu1(t)o ru2(t), respectively. A trial is composed of 256 stimulation patterns with 1-s
intervals. Overall, the training period is composed of 100 trials, where the stimulation pattern is common for all trials.
doi:10.1371/journal.pcbi.1004643.g002
Fig 3. Overview of the experiment.Snapshots ofS1 Movieat theu = (0,0), (1,0), (0,1), and (1,1) state, respectively, are shown. Setup is with the same as
that described inFig 2A: hidden sourcesu1 and u2 (left), merged inputss1, ... , s16 (middle top),s17, ... , s32 (middle bottom), and unit activities of cultured
neurons x1, ... , x64 (right). Each of the 64 panels on the right shows a raster plot of neural activity (unit activity) recorded at the electrode, where horizontal
and vertical axes are 400 ms time window and trials 1– 10, respectively.(A) When u = (0,0), evoked responses were not observed since there was no input,
although spontaneous activities were recorded.(B) When u = (1,0), a group ofs1, ... , s16, became 1 (red circles in the middle) with a high probability (namely,
a = 3/4 probability), while a group ofs17, ... , s32 became 1 with a low probability (1– a = 1/4). Ifsi (i =1 ,... , 32) was 1, an electrical pulse stimulation was
induced into a fixed corresponding electrode. Consequently, evoked responses were observed immediately after each stimulation.(C) When u = (0,1), a
situation exactly opposite to that described in (B) occurs.(D) When u = (1,1), all stimulated electrodes (s1, ... , s32) were stimulated, providing the largest
evoked response.
doi:10.1371/journal.pcbi.1004643.g003
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 6 / 29
Fig 4. Examples of neural responses to different stimulus inputs.Raster plots of successive evoked
responses before and after learning (A and B) for four source states; spikes-per-event transition over
learning, average and histogram, for four source states and two different electrodes (C and D).(A) Top panel:
A raster plot showing a typical pattern of stimulation-evoked spikes in cultured neurons recorded with an
electrode at trial 1 (before training). Red circles indicate the timing of spikes. The horizontal axis corresponds
to time (ms), and the vertical axis is the stimulation number sorted by source state. White, red, blue, and
green areas indicate responses when the state of the source wasu = (0,0), (1,0), (0,1), and (1,1),
respectively. Betweenτ = – 3 and 3 ms (area surrounded by dashed lines), reliable data were not obtained
because of switching noise (artifact). Bottom panel: Post stimulus time histogram (PSTH) at trial 1. Black, red,
blue, and green curves are PSTH when the state of the source wasu = (0,0), (1,0), (0,1), and (1,1),
respectively. (B) Same as (A), but at trial 100 (after training).(C) Left panel: example of a typical transition
over trials of the conditional expectation of an evoked response recorded with the same electrode as in (A)
and (B). Black, red, blue, and green circles give the conditional expectation of evoked responses when the
state of the source wasu = (0,0), (1,0), (0,1), and (1,1), respectively. Center and right panels: The conditional
probability distributions of evoked responses recorded with the same electrode during trials 1 to 10 (center
panel) and trials 91 to 100 (right panel). The four curves correspond to the four states ofu. (D) Same as (C),
but recorded with a different electrode.
doi:10.1371/journal.pcbi.1004643.g004
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 7 / 29
10 ms after stimulation are generated by synaptic inputs [43]. Therefore, the change in number
of evoked spikes generated 10– 30 ms after each stimulation, defined as evoked response,
occurred gradually over training (Fig 4Cleft). The center and right panels inFig 4Cillustrates
a typical transition of a conditional probability distribution of evoked responses, i.e., the num-
ber of evoked spikes recorded at the electrode before and after training. In this case, a typical
shift of a peak of the (0,1) type (blue curve) is presented.Fig 4Dshows the transition of
responses over training at another stimulated electrode. In contrast toFig 4C, a shift of a peak
of the (1,0) type (red curve) is shown. The transition of response at each electrode can be found
in S1 Dataset.
These results suggested that neurons near stimulated electrodes had preferences to one of
the two hidden signals, but not the other. Specifically, most neurons from electrodes 1– 16
(x
1, ... , x16) preferredu1 signals (neurons were activated more largely whenu = (1,0) than when
u = (0,1)), most neurons from electrodes 17– 32 (x17, ... , x32) preferredu2 signals, and most neu-
rons at electrodes 33– 64 (non-stimulated;x33, ... , x64) showed no preference (Fig 5A and 5B).
Note thatxi
u indicates the conditional expectation with the source stateu and xi
u is its over-trial
average. Neurons near stimulated electrodes exhibited larger responses compared to these near
non-stimulated electrodes. Inu
1-preferring neurons, the increase in response strength was
larger when the state of the source wasu = (1,0) than when it wasu = (0,1) (Fig 5C and 5D),
while the exact opposite alteration proﬁle was observed inu2-preferring neurons (Fig 5E and
5F). Moreover, at 50 electrodes out of 371u1-preferring electrodes,xi
1;0 was 3 times larger than
xi
0;1 , and at 44 electrodes out of 345u2-preferring electrodes,xi
1;0 was 3 times larger thanxi
1;0 as
all trial average (S1A Fig). Additionally, the number of such electrodes increased during training
(S1B Fig). If a neuron responded tosi (i =1 ,... ,1 6 ) ,xi
1;0 should be 3 times as large asxi
0;1 by
the relationship betweensi and u, while if a neuron responded tosi (i = 17,... , 32),xi
0;1 should
be 3 times as large asxi
1;0 . Therefore, this indicates that at approximately 13% ofu1-o ru2-pre-
ferring electrodes, neural responses (xi) were more likely to be determined by the state of hidden
sources (u) rather than by induced stimulation itself (si) in the strict sense of the word. Taken
together, these results suggest that neural responses were more likely determined by the state of
hidden sources estimated based on inputs from multiple electrodes, termed source-coding,
rather than the input from an electrode, e.g., the nearest electrode.
Increased response specificity to discrete stimuli in cultured neuron
networks
The difference between the probability distribution atu = (1,0) and (0,1) is a well-established
criterion to evaluate response preference, which in information theory is often defined by the
Kullback-Leibler divergence (KLD) [11]. We calculated KLD of the evoked response at each
electrode under the assumption that these conditional probabilities conformed to a Poisson
distribution. We observed a significant change in KLD (represented asD
KLi, wherei =1 ,... ... ,
64 is the index of electrodes) between distributions given the (1,0) state and (0,1) state (P(xi(t)|
u = (1,0)) andP(xi(t)| u = (0,1), respectively). The values ofDKLi were increased in some elec-
trodes after the training period (red circles inFig 6A), where trained neuron cultures are
labeled as TRN. Moreover, the mean values forDKLi averaged across all recording electrodes
increased after training (Fig 6B and 6C). The increase in the value ofDKLi in trained neuron
cultures in the presence of 20μM 2-Amino-5-phosphonopentanoic acid (APV), an N-methyl-
D-aspartic acid (NMDA)-receptor inhibitor, was significantly smaller than in nontreated TRN
cultures (black circles inFig 6B;
/C3/C3/C3/C3 , p < 10−5). We confirmed that the alterations in KLD
were maintained for a long time by comparing continuously stimulated trained neurons to par-
tially trained (PRT) neurons. PRT neurons were trained for only 10 trials, then went
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 8 / 29
Fig 5. Response preference all-trial summary, transitions, and overall changes. (A)Response
preference. Horizontal and vertical axes are the all-trial averages of the expectation of response whenu =
(1,0) and (0,1), respectively. Red circles are responses recorded with electrodesx1, ... , x16, blue circles, with
electrodes x17, ... , x32, and black circles, with electrodesx33, ... , x64. Circles are superimposed data from all
cultures (n = 1035 electrodes from 23 cultures; the total number of electrodes was 1472, but 437 electrodes
were not available, see (B)). The solid line is the identity line. Dashed lines indicate± 0.5 spike/event.(B) The
expectation of the numbers of four types of electrode responses in a culture. Red, blue, and gray correspond
to the number of electrodes that recordu1-preferring, u2-preferring, and neither-preferring responses,
respectively. White shows the number of electrodes that were not available or suitable for analysis because
of insufficient spikes. Neural activities were recorded from the majority of the electrodes.(C) Transitions of the
expectation of response averaged over just theu1-preferring electrodes. As inFig 4C, the four curve colors
correspond to the four states ofu. (D) The change in responses between trials 1 and 100. Horizontal and
vertical axes plot the difference in the conditional expectations between trials 1 and 100 whenu = (1,0) and
(0,1), respectively. Atu1-preferring electrodes, the increase ofxi
1,0 was significantly greater than that ofxi
0,1
(****, p < 10−15; n = 371 electrodes from 23 cultures). The dashed diagonal line is the identity line.(E) and
(F) Same as (C) and (D), but averaged over just theu2-preferring electrodes. Atu2-preferring electrodes, the
increase ofxi
0,1 was significantly larger than that ofxi
1,0 (****, p < 10−15; n = 345 electrodes from 23
cultures). In (C) and (E), shadowed areas are S.E.M.
doi:10.1371/journal.pcbi.1004643.g005
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 9 / 29
unstimulated for 18– 24 h (i.e. the resting period), and then went through 10 additional training
trials. In PRT cultures, the values ofDKLi at trial 91 (i.e., first trial after the resting period) were
significantly larger than that at trial 1 (Fig 6D); however, the difference was significantly
smaller than the difference inDKLi observed between trial 1 and 91 in TRNs (white circles in
Fig 6B; /C3/C3/C3/C3 , p < 10−4). Interestingly, the values ofDKLi at trial 100 in PRTs were almost same
level as that at trial 100 in TRNs (p = 0.268). The transition of KLD at each electrode can be
found inS1 Dataset.
KLD was affected by the merged balance of inputs (a) and the frequency of inputs (ρ). Spe-
cifically, we varied input balance by comparing the change of thea:1– a = 3/4:1/4 balance con-
dition with that of the 0:1 and 1/2:1/2 balance conditions and the source condition with aρ =
1/2 probability with a 1/4 and 3/4 probability (Fig 6E and 6F). Compared to the initial values
(trial 1 vs. trial 100), KLD was not altered by inputs with 1/2:1/2 ratio of merged balance ((a, ρ)
= (1/2, 1/2)) (black circles inFig 6E; p = 0.515;n = 147 from 4 cultures), suggesting that input
variance was necessary to elicit these changes. As both PCA and ICA rely on variations of
Fig 6. Alterations in Kullback-Leibler divergence (KLD) indicate distinct response sensitivity to different source stimuli.Panels show typical (A) and
mean (B) KLD transition, overall change in full (C) and partial (D) training, and transitions with alternative parameter settings (E and F).(A) Typical transition
of the KLD recorded with an electrode. Red circles are KLDs recorded in a trained culture (TRN). Black circles are KLDs recorded in a culture trained in the
presence of APV. White circles are KLDs recorded in a partially trained culture (PRT), where the culture was trained for 10 trials, then not stimulatedfor 18–
24 h, then trained for a further 10 trials.(B) The mean transition of the KLD. Red circles are the mean KLDs averaged over electrodes in the TRN group
(n = 1035 electrodes from 23 cultures). Black circles are the mean KLDs in the presence of APV (n = 435 electrodes from 9 cultures). White circles are the
mean KLDs in the PRT group (n = 473 electrodes from 10 cultures). At trial 100, KLDs in TRN were significantly larger than those in APV (****, p < 10−5). (C)
The change of KLDs in TRN (trial 1 vs. trial 100). Circles are KLDs for each electrode. The dashed line is the identity line. KLDs significantly increased after
training (****, p < 10−15). (D) The change in the KLDs in PRT (trial 1 vs. trial 91). KLDs at trial 91 were significantly larger than those at trial 1 (****,
p < 10−4), indicating that the increase in KLD was maintained over the resting time.(E) The mean transition of the KLD with alternative parameter settings.
Red, white, and black circles are KLDs with (merged balance (a), source firing probability (ρ)) = (3/4, 1/4), (3/4, 3/4), and (1/2, 1/2), respectively. The black
curve is the mean KLD in the TRN group.(F) The mean transition of the KLD. Circles are KLDs with (a, ρ) = (1, 1/2). In (B), (E), and (F), bars are S.E.M.
doi:10.1371/journal.pcbi.1004643.g006
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 10 / 29
input, these results are consistent with the hypothesis that cultured neural networks use ICA-
like signal processing. When sources summed to one with a probability of 1/4, i.e., (a, ρ) = (3/4,
1/4) (red circles inFig 6E), KLD increased after training (/C3/C3/C3 , p < 10−3; n = 139 from 4 cultures;
trial 1 vs. trial 100). Similarly, when (a, ρ) = (3/4, 3/4) (white circles inFig 6E), KLD increased
after training (/C3/C3/C3/C3 , p < 10−7; n = 234 from 6 cultures; trial 1 vs. trial 100). The change in KLD
with (a, ρ) = (3/4, 1/4) was slightly smaller than when (a, ρ) = (3/4, 1/2) (p = 0.469, at trial 100),
while the change in KLD with (a, ρ) = (3/4, 3/4) was slightly larger than when (a, ρ) = (3/4, 1/2)
(p = 0.166, at trial 100). When the input balance was 1:0 (not merged; (a, ρ) = (1, 1/2)), a large
increase of KLD was observed (Fig 6F; /C3/C3/C3/C3 , p < 10−4; n = 161 from 4 cultures; trial 1 vs. trial
100), which is an analog of conventional pattern separation [34]. Note that to calculateFig 6F,
when the change in KLD form trial 1 was larger than 10 or smaller than– 10, it was shifted to
10 or– 10, respectively.
A recognition model used by cultured neural networks
We then set out to build a population-based model of neural network assembly based on our
experimental paradigm. We defined the population model as~x ¼ð ~x1; ~x2Þ
T
, where~x1 and ~x2
represent mean evoked responses of neurons inu1- andu2-preferring neuron groups in each
culture preparation. Distribution of~xðtÞ at trial 1 and 100 are shown inFig 7A and 7B, which
represents the recognition density [19, 20]o f~x, qð~xÞ. Alterations observed inqð~xÞ over the
trial periods are show inS3 Movie. Notably, the total evoked response from all available elec-
trodes (~x1 þ ~x2) was almost proportional to the total input (i.e., the number of stimulated elec-
trodes) (S2A Fig).
Early computational studies proposed several learning models (recognition models)
employing blind source separation. These models can be roughly separated into two types: the
inverse recognition model [12– 14, 44] and the feed-forward recognition model [15– 17, 19, 20].
Considering the fact that inputss were instantaneously induced in cultured neural networks
and evoked responses recorded at stimulated electrodes decreased 20– 30 ms after each stimula-
tion (Fig 4A and 4B), the feed-forward recognition model was not suitable in this situation, as
it requires the dynamics of neural networks to converge towards an equilibrium state for learn-
ing. Moreover, large populations of neurons that we observed were state-coding and correlated
with sources (u) (96.2% of electrodes were corr(x
i, u1) > 0.4 or corr(xi, u2) > 0.4), while only a
small population of neurons were correlated with estimation errors (e1 or e2, wheree1 and e2
are estimation errors ofxi from u1 and u2; only 1.8% of neurons were |corr(xi, e1)| > 0.4 or |
corr(xi, e2)| > 0.4) (Fig 7C). Therefore, our results indicated that the recognition model used by
cultured neural networks is more consistent with the inverse model, as the inverse model does
not require the equilibrium state of~x or the existence of error-coding neurons. Based on this
evidence, we generated an inverse recognition model of cultured neural networks, as we show
in Fig 7D. Schematic images of the model’s dynamics are shown inFig 7E. Taken together our
results indicated that cultured neural networks implement ICA-like learning and that their
dynamics can be described by an inverse recognition model.
Connection strengths are altered according to the principle of free
energy minimization
Estimations of effective connectivity help in understanding neural dynamics [45, 46]. To esti-
mate parameters of the inverse model from observed evoked responses, we calculated the maxi-
mum likelihood estimator of connectivityW (a 2×2 matrix) to analyze the averaged synaptic
connection strengths within and between assemblies. Changes in estimated connection
strengths are shown inFig 8A. After training (relative to trial 1), intrinsic connection strengths
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 11 / 29
Fig 7. Population activity of cultured neural networks.Absence of stimulus classification before learning
(A) and its presence after (B); summary of classification statistics (C); summary of the inverse recognition
model (D) and dynamics of the inverse recognition model (E).(A) Evoked responses of populations of
cultured neurons before training. Horizontal and vertical axes are the averaged responses ofu1- andu2-
preferring electrodes (~x1 and ~x2). Red, blue, and green indicate evoked responses when the state ofu is
(1,0), (0,1), and (1,1), respectively. The Fig corresponds to a superimposition of responses att =1 ,... , 256
from 23 cultures. Plus-marks and ellipses are the means and standard deviations of~x given u averaged over
23 cultures. The scale indicates the averaged spike number per stimulation.(B) Evoked responses after
training. The evoked response transient throughout training is shown inS3 Movie. (C) The distribution of
correlation. Red circles plot the correlation ofxi with u1 (horizontal) andu2 (vertical) (n = 1035 electrodes from
23 cultures). Black circles plot the correlation ofxi with e1 (horizontal) ande2 (vertical), wheree1 and e2 are
the error ofxi from u1 and u2, respectively.(D) Schematic image of a population model under the assumptions
of the inverse recognition model. Our model assumes that~s is a column vector of inputs,~s’ and ~x are column
vectors of the population activity of neuron groups, andW is a 2 × 2 matrix of connection strengths. Also, we
assume that~x can be represented as a multiplication of the connection strength matrixW by ~s’ (linear, ﬁring-
rate neuron model). Based on the recognition model,W was calculated from the relationship between the
amplitude of the stimulation and the evoked response using the maximum likelihood estimation.(E)
Dynamics of the inverse recognition model. Upper, middle, and lower time courses represent input, direct
response, and synaptic response, respectively.
doi:10.1371/journal.pcbi.1004643.g007
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 12 / 29
(W11, W22) increased significantly, while connectivity between different neuron groups (W12,
W21) tended to decrease (Fig 8B). Notably, if we assumed a constraint on total synaptic
strengths with aγ-norm (the 1/γ power of theγ power sum of synaptic strengths), and ifγ was
between 2 and 4, theγ-norm of the connection strengths maintained almost same value during
the latter part of the training period (S2B Fig).
As the model and connection parameters are well defined, we could calculate the internal
energy and the Shannon entropy for these neural networks. To do this, we assumed thatqð~xÞ
obeys a Gaussian mixture model with four peaks corresponding to the four states ofu.I n t e r -
nal energy,U ¼ Uð~s; ~x; WÞ,i sd eﬁned as the negative log likelihood function of prediction
error at a moment, where~s and ~x are input and output, respectively. Shannon entropy,H,i s
deﬁned byH ¼ H½qð~xÞ/C138. Friston’s free energy,F,i sd eﬁned as the difference betweenhUi
and H [19, 20], wherehi is an expectation underqð~xÞ.T h e r e f o r e ,F is represented as
Fð~s; ~x; WÞ¼h Uð~s; ~x; WÞi /C0 H½qð~xÞ/C138. Generally, free energy gives an upper bound on
Fig 8. Free energy properties in cultured neural networks.Transitions during learning (left) and overall
before-and-after changes (right) in connection strengths (top) and in three descriptors of evoked responses
modeled on information-theoretic functions (bottom).(A) Connection strengths of the neural population
estimated from the number of evoked response per trial. Black circles and squares areW11 and W22. White
circles and squares areW12 and W21. Bars are S.E.M.(B) The change in connection strengths (trial 1 vs. trial
100). W11 and W22 significantly increased after training (****, p < 10−6; n = 46 from 23 cultures), whileW12
and W21 tended to decrease (p = 0.069;n = 46 from 23 cultures).(C) Transition of the expectation of internal
energy (hUi; white circles), Shannon entropy (H; black circles), and free energy (F; red circles) in cultured
neural networks estimated from evoked responses per trial. Bars are S.E.M.(D) The change inhUi, H, andF
(trial 1 vs. trial 100). After training, the expectation of internal energy decreased (*, p = 0.023;n = 23 cultures),
Shannon entropy significantly increased (****, p < 10−4; n = 23 cultures), and free energy significantly
decreased (****, p < 10−4; n = 23 cultures). These results suggest that learning in cultured neural networks
was governed by the free-energy principle.
doi:10.1371/journal.pcbi.1004643.g008
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 13 / 29
‘surprise’of inputs, so the decrease of free energy implies that the system is changing to adapt
to (or learn) its environment [19, 20]. The full details of these calculations are fully described
in the Methods. These components of free energy changed dramatically over training trials
(Fig 8C). We found that the expectation of internal energyhUi decreased, Shannon entropy
H increased signiﬁcantly, and free energyF decreased signiﬁcantly after training (Fig 8D),
which is consistent with the principle of free-energy minimization [19, 20]. These data thus
indicate that connectivities in neural networks were established such that they minimize free
energy (F).
As expected, as learning proceeds over trials, the implicit entropy of the probabilistic encod-
ing increases in accord with Jaynes’maximum entropy principle [41, 42]. Crucially, this is
accompanied by a profound decrease in energy (i.e., the amount of prediction error). There-
fore, the decrease in the energy and the increase in the entropy both contributed to produce an
overall reduction in free energy–– that can only be attributed to learning or plasticity. This
assertion was verified empirically by quantifying free energy changes in the presence of APV.
Remarkably, free energy did not change at all during training under APV (S3 Fig).
Learning rule of cultured neural networks
The changes in KLD and free energy we observed are indicative of synaptic plasticity and sug-
gested that cultured neural networks are capable of performing blind source separation. These
findings further suggested the existence of a transformation matrix (W) in cultured neural net-
works, which transforms merged inputs to independent outputs [12– 14, 44]. However, it is
unclear whether the blind source separation is realized only by Hebbian learning [18]. To esti-
mate the learning rule of cultured neural networks, we first considered a simple Hebbian plas-
ticity model, where a learning efficacyα
u becomes 0 foru = (0,0) andα for other states (α-
model; see also theMethods). We then estimatedα for each culture sample. The estimated val-
ues ofα are shown inFig 9Aleft and the Bayesian information criterion (BIC) [47]i nα-model
is shown inFig 9B. In thisα-model, connections between different neuron groups (W12, W21)
were expected to increase substantially, because Hebbian learning operates by simply increas-
ing the correlation among neurons that fire together (Fig 9C). However, we did not observe
substantial increases between neuron groups, indicating that a simple Hebbian rule could not
explain our experimental results.
These results therefore suggested that blind source separation in our cultured neural net-
works required another mechanism. We thus considered a modified version of Hebbian plas-
ticity (β-model), where a learning efficacyβ
u depends on the state ofu, 0 foru = (0,0),β1 for u
= (1,0), (0,1), andβ2 for u = (1,1).β1 and β2 were estimated for each culture. Interestingly, we
found that estimated values ofβ2 were significantly smaller than the estimated values ofβ1
(approximately 27% ofβ1; Fig 9Aright). Moreover, the BIC was significantly smaller than in
the α-model (Fig 9B). Accordingly, theβ-model successfully explained the increase of intrinsic
connections within neuron groups (W11, W22), and the absence of increases inter-connections
between different groups (W12, W21)( Fig 9C). Furthermore, as an additional Bayesian model
comparison, we showed that Hebbian plasticity with state-dependent efficacy (theβ-model) is
better than Hebbian plasticity withγ-norm constraint on total synaptic strength (theα’-model)
to explain our experimental results (seeS1 Noteand S4 Fig).
These results suggest that cultured neural networks do not use the simplest form of the Heb-
bian plasticity rule (theα-model), but rather a state-dependent Hebbian plasticity rule (theβ-
model) in which learning efficacy is modified according to the state of sources. A conceptual
conclusion is that the depression in inter-connections between different groups and the forma-
tion of cell assemblies are crucial to achieve blind source separation. Generally, the potentiation
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 14 / 29
in connections makes the correlation between a neuronal group and a source stronger, while
their depression makes the correlation between the neuronal group and the other source
weaker. In our analysis, because theβ-model encouraged stronger depression in connections
from the other source and induced stronger competition between different neuronal groups,
the β-model was better able to explain the results than theα-model. Moreover, this result sup-
ports the hypothesis that neurons render their activity independent of each other. This is con-
sistent with early work on decorrelating or lateral interactions in PCA/ICA learning rules,
which, importantly, can be formulated as variational free energy minimization [48].
Fig 9. The governing learning rule in cultured neural networks.The rule was determined from (A)
learning efficacies, (B) the Bayesian information criterion (BIC), and (C) model predictions of connectivity
changes, and summarized in (D).(A) The expectation of learning efficacy estimated from connection
strengths under the assumption that synaptic plasticity in cultured neural networks obeys either a Hebbian
(μ
α; left) or state-dependent Hebbian (μβ1, μβ2; right) rule.μβ2 was correlated withμβ1 (*, p = 0.037;n =2 3
cultures, Spearman test) and their ratio was 27.1%.(B) The BIC of theα- andβ-models. BIC of theβ-model
was significantly smaller than that of theα-model (****, p < 10−6; n = 23 cultures).(C) Measured changes in
connection strengths and those estimated from Hebbian and state-dependent Hebbian plasticity rules. Black
bars are the mean± S.E.M of the change (trial 100– trial 1) inW11 and W22 (n = 46 from 23 cultures), which
increased in all cases (****, p < 10−5). White bars are the mean± S.E.M of the change inW12 and W21
(n = 46 from 23 cultures).W12 and W21 tended to decrease after training when calculated from response data
(left; p = 0.069).W12 and W21 estimated from theα-model significantly increased (center;****, p < 10−4).
W12 and W21 estimated from theβ-model significantly decreased (right;*, p = 0.023), in agreement with the
experimentally determined results.(D) Suggested learning rule, which is based on state-dependent Hebbian
plasticity, and leads to blind source separation. Synaptic plasticity in cultured neural networks almost
followed a strict Hebbian rule. However, experimental data indicate that learning efficacy was not the same
for each input state. Substantial Hebbian plasticity occurred whenu = (1,0) oru = (0,1), while limited or anti-
Hebbian plasticity occurred whenu = (1,1). From the estimated efficaciesμβ1 and μβ2, the efficacy ofu = (1,1)
was apparently only 27% of that ofu = (1,0) or (0,1).
doi:10.1371/journal.pcbi.1004643.g009
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 15 / 29
Discussion
In this study, we discovered that cultured neural networks were able to identify and separate
two hidden sources. We found that the distinct classes of neurons learned to respond to the dis-
tinct hidden sources and that this was reflected in differences in the Kullback-Leibler diver-
gence (KLD). We then sought to determine how connection strength is determined between
cultured neurons and found that connectivities are established such that they minimize free
energy. Finally, we integrated these data to construct a model of learning in cultured neural
networks and determined that learning is established by a modified Hebbian plasticity rule.
Taken together these data indicate that cultured neural networks can infer multivariate hidden
signals through blind source separation.
Although cultured neural networks are random and may not have functional structures for
signal processing before training, our data indicated that the process of training enables them
to self-organize and obtain functional structures to separate two hidden signals though activ-
ity-dependent synaptic plasticity, such as spike-timing dependent plasticity (STDP) [49– 51].
This process was a clear example of unsupervised learning [9– 11] in cultured neural networks.
Previous studies have reported that the response electrode almost agrees with the stimu-
lating electrode [43] and that the increase in response strength at stimulated electrode is
larger than in the non-stimulated electrodes [52]; our results are consistent with these find-
ings. Synaptic plasticity and inputs with different merged points of balance are necessary
f o rl e a r n i n gt oo c c u r .A ss p i k e so b s e r v e dl e s st h a n1 0m sa f t e rs t i m u l a t i o ni no u rc u l t u r e
system corresponded to responses directly evoked by electrical stimulation and artifacts
(switching noise), we only assessed spikes more than 10 ms after stimulation. This allowed
the analysis of changes in neural activity related to mechanisms of synaptic plasticity. Indeed,
we observed that changes in KLD were inhibited by APV, strongly suggesting that learning
mechanism was mediated by long-term synaptic plasticity regulated by NMDA-receptor sig-
naling. As a further indication of a role for long-term synaptic plasticity, assays of partially
stimulated cultures indicated the changesbrought about by neural activation were main-
tained after 18– 24 h without stimulation. Additionally, our results indicated that differences
in the size of inputs were necessary for blind source separation in cultured neurons. Neurons
with larger initial states of KLD tended to exhibit greater changes, suggesting that learning is
nuanced by the initial input strengths as would be consistent with most forms of Hebbian
learning [18].
Although the specificity of the neuronal response to hidden sources increases significantly,
there remains a possibility that the neurons merely responded to their neighbor input stimula-
tion. In fact, responding to neighbor stimulation might be enough to increase the response
specificity in the current stimulation design. Indeed, in a large portion of electrodes, neural
responses were affected by the input from an electrode. However, we found that at least at 13%
of u
1-o ru2-preferring electrodes, neural responses were more likely to be determined by the
state of hidden sources rather than by the input from an electrode, typically the nearest one, in
the strict sense of the word. Moreover, the number of such electrodes increased during training.
In short, this means we might be observing the superimposition of the response to the input
from an electrode and the response corresponding to the state of hidden sources. Hence, to
reduce the effect of neighbor stimulation site and emphasize the response determined by the
state of hidden sources, we should search the optimal stimulation design for investigating blind
source separation as future work.
Even in the presence of APV, KLD increased slightly. One explanation is that this is the
result of an NMDA-R-independent form of learning. For example, it is known that synaptic
plasticity independent of NMDA-R activity occurs at GABAergic synapses [53, 54], and could
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 16 / 29
alter the neural network state to some degree. However, it could also be related to the drug’s
imperfect blockade of NMDA-Rs.
In our experiments, evoked activities of cultured neurons were only synchronously gener-
ated immediately after each stimulation. This would be expected for both forward and inverse
recognition models, given that the input was synchronous and instantaneous (discrete-time
system), but the dynamics did not reach an equilibrium as is required for learning of a feed-for-
ward model. Moreover, most neurons we observed were highly correlated with one of two
sources (source-coding neurons). Taken together, these findings suggest that for our experi-
mental protocol, the structure of cultured neural networks can be represented as a two-layer
feed-forward network constructed from input and output layers and functioning as an inverse
recognition model. However, it remains unclear which model applies to cultured neural net-
works with non-synchronous input.
Although some ICA models use information via non-local connections, several studies have
proposed local rules that ICA can be constructed only using biologically plausible local connec-
tions [48, 55]. Internal energy, or negative log likelihood, also decreased after training, indicat-
ing that our culture neural networks also performed a maximum likelihood estimation or a
maximum a posteriori estimation. Consequently, the free energy of the population model
decreased significantly after training as predicted by the free energy principle [19, 20], which
can also be regarded as an increase in mutual information between input and output (infomax
principle) [56, 57]. Taken together these results suggest that in response to synchronous input,
cultured neural networks perform ICA-like learning using an inverse recognition model con-
structed from local connections, and they adhere to the free-energy principle.
Experimental results suggest that the change in synaptic connection strengths in our model
is better explained by a state-dependent Hebbian plasticity rule rather than the simplest Heb-
bian rule (Fig 9D). A possible explanation is as follows: Initially, many neurons may respond
strongly to the nearest electrode but may also respond weakly to distant electrodes. According
to Hebbian plasticity, synapses that respond to stimulation of the nearest electrode (and thus, to
the source that tends to activate the nearest electrode) are likely potentiated because of the large
postsynaptic response to the nearest electrode. If depression is induced in the other synapses in
accordance with a plasticity rule, the neural response to the source that effectively stimulates the
nearest electrode will be facilitated, while that to the other source will be depressed. This sce-
nario seems to qualitatively explain the experimental results; however, our analysis implies that
the simplest Hebbian plasticity (theα-model) cannot change synaptic strengths in this manner
because of larger LTP in theu = (1,1) state than LTD in theu = (1,0) and (0,1) states, and that
state-dependent Hebbian plasticity (theβ-model) better explains the results since it suppresses
LTP in theu = (1,1) state, providing stronger competition between neurons. Nevertheless, a
more biologically plausible Hebbian plasticity model, such as the STDP model [21], should be
analyzed in a systematic future study. It is unclear whether such a model fully explains the
experimental results, and we would like to investigate this in the future.
Indeed, cultured neurons could not directly know the state ofu; however, they could distin-
guish the state ofu = (1,1) from other states as the total of evoked activity was significantly
larger for this state than the other states. It is likely cultured neurons use the total of evoked
activity to determine learning efficacy. Although we considered a 2-state model of learning effi-
cacy, since theu = (1,0) and (0,1) states are symmetrical in our experimental setup, a 3-state
model could be considered if presented with an asymmetrical stimulation pattern. Some
mathematical models of ICA [44, 48, 55] conform to a modified form of Hebbian plasticity as
well. Moreover, modulation of synaptic plasticity by GABAergic input [58, 59] may operate
learning-efficacy modulation; nevertheless, additional experiments are necessary to determine
the physiology of the modulation of Hebbian plasticity we observed.
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 17 / 29
It is known that animals have a high aptitude for pattern separation [60]. Spontaneous prior
activity of a visual area learns the properties of natural pictures [28]. In the visual and olfactory
systems, structures that decorrelate inputs and raise contrast are functional from birth [61, 62].
Although many studies of the pattern separation have been conducted, there is little research
investigating blind source separation in biological neural circuits, as the decomposition of
merged inputs is a more complex process than simple pattern separation. Owing to the simple
properties of cultured neural networks, we observed the process by which neural networks
actually learn to perform blind source separation. Evoked responses likewise changed after
training to correspond to sources of the generative model. Practically, sensory inputs are a mix-
ture of several sources except in a few ideal cases. Without blind source separation, these sig-
nals cannot be processed appropriately because the brain would fail to adequately decorrelate
inputs. Therefore, our findings may be very important in understanding sensory perception.
Alternatively, one might consider that blind source separation can occur online, and seems
to be more a matter of attention rather than learning, e.g., one can separate voices with differ-
ent timbers at a cocktail party without experiences those particular timbers before. However, to
direct one’s attention to a specific voice, the brain needs to separate a mixture of signals in
advance. Therefore, although additional studies are required to explain the difference in the
time scale of blind source separation between that considered in a cocktail party problem and
that we observed, it is likely that ICA-like learning is necessary for blind source separation and
a cocktail party effect.
Recently, the free-energy principle was proposed [19, 20], which specifically includes PCA/
ICA, and has been applied to explain recognition models with highly hierarchical structure
[17]. Cultured neural networks are useful to examine these theories as they can easily build any
network structure [35, 36] and reproduce a variety of functions [30– 39]. In addition, dynamic
causal modeling for spike data [63] helps to investigate the detail structure of the recognition
models of cultured neural networks. Moreover, there remains the possibility that cultured neu-
ral networks can perform even more complex types of unsupervised learning. These findings
contribute not only to an increased understanding of learning and memory from a neurosci-
ence perspective, but also in examining the free-energy principle at the cellular level.
In summary, we found that dissociated cultures of cortical neurons have the ability to carry
out blind source separation in response to hidden signals. Learning in this paradigm used an
inverse recognition model and was carried out according to a modified form Hebbian plastic-
ity, which is likely regulated, at least in part, by NMDA signaling. These results are entirely
consistent with the free-energy principle, suggesting that cultured neural networks perform
blind source separation according to the free-energy principle. Most importantly, the free
energy formulation allows us to quantify probabilistic encoding at the neuronal level in terms
of information theory, and to test hypotheses about the changes in energy and entropy that are
implicit in Bayes-optimal perception. We could have also assessed the accuracy and complexity
of these representations with a slight change of variables. The free energy formalism prescribes
Bayes-optimal update rules for the connection strengths that are associative in nature. Taken
together these data provide a compelling framework for understanding the process by which
the brain interprets hidden signals from complex multivariate information.
Methods
Cell cultures
All animal experiments were performed with the approval of the animal experiment ethics
committee at the University of Tokyo (approval number, C-12-02, KA-14-2) and according to
the University of Tokyo guidelines for the care and use of laboratory animals. The procedure
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 18 / 29
for preparing dissociated cultured of cortical neurons was based on a modified version of the
procedure described in a previous study [30]. Pregnant females of Wistar rat (Charles River
Laboratories, Japan) were anaesthetized with isoflurane and immediately sacrificed. 19-day-old
embryos (E19) were extracted and sacrificed by decapitation under ice-cold anesthesia. Cortical
cells were removed from embryos and dissociated into single cells with Trypsin (Life Technolo-
gies) at 37°C for 20 min. The density of cells was adjusted to 1 × 10
7 cells/mL. 5 × 105 of the
dissociated cells in 50μL were seeded on the center of MEA dishes (Fig 1 and 1B), where the
surface of MEA was previously coated with polyethyleneimine (Sigma-Aldrich) overnight.
Note that to prepare high-density cultures, cells were dropped on the region where electrode
terminals were disposed. The culture medium consisted of Dulbecco’s modified Eagle’s
medium (DMEM) (Life Technologies) containing 10% heat-inactivated fetal bovine serum
(FBS) (Cosmo Bio), 5% heat-inactivated horse serum (HS) (Life Technologies), and 5– 40 U/ml
penicillin/streptomycin (Life Technologies). After sitting undisturbed in the MEA dishes for 30
min, the fresh culture medium and medium conditioned for 3 days in glial cell cultures, were
added into MEA dishes at a ratio of 1:1. The cells were cultivated in a CO
2 incubator, an
environment of 37°C and a 5% CO2/95% air concentration. Half of the culture medium
was changed once every third day. These cultures were cultivated for 18 to 83 days before
electrophysiological measurements. Although the electrophysiological properties of cultured
cortical neurons change during development, it has been reported that at the stage of culture
using our experiments, the spontaneous firing patterns of neurons have reached a developmen-
tally stable period [64– 66]. Note that same cultures were used more than once for experiments
with other stimulation-pattern conditions since learning history with other stimulation-pattern
did not affect our experiments and evaluations of results. We used 27 different cultures for 7
experiments, which were performed 40 ± 18 days after seeding.
Recording
The MEA system (NF Corporation, Japan) was used for extracellular recording of cultured
neural networks. Electrode terminals and circuits on MEA dishes were handmade using a pho-
tolithography technique. The 8×8 electrode terminals of MEA were disposed on a grid with
250-μm distance. Platinum black was coated on all 50-μm-each side electrode terminals. Neu-
ral signals were recorded with a 25 kHz sampling frequency and band-pass filtered between
500– 2000 Hz, and were recorded over 14 h. All recordings and stimulation were conducted in
aC O
2 incubator. From the spike sorting analysis [67], an electrode was expected to record the
activities from up to four neurons. For more details of MEA recording, see previous studies
[30, 40].
Electrical stimulation
Electrical stimulation was applied through 32 electrodes in pulse trains with 1 s intervals (Fig
2). Pulses were biphasic with each phase having a duration of 0.2 ms, and were delivered with 1
V amplitudes. Stimuli were delivered for each stimulating electrode only once in 1 s (1 Hz).
Before making inputs, we created hidden sourcesu1(t), u2(t), which corresponded to
two independent random binary sources,u1(t), u2(t) 2 {0,1} (t =1 ,2 ,... , 256 [s]). In this equa-
tion, u1(t) andu2(t) are signal patterns such thatu1(t) = 0,1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,... and
u2(t) = 1,1,0,0,1,0,0,1,1,0,1,1,1,0,1,1,... as shown inFig 2B and 2C. The value ofu1(t) andu2(t)
will be 1 with a probability ofρ = 1/2 at each time period. The termss1(t), ... , s32(t) correspond
to merged inputs (electrical pulses), which were what we actually applied to cultured neurons.
Therefore, cultured neurons did not know directly what the exact state of (u1(t), u2(t)) was
because we did not induce (u1(t), u2(t)) directly.
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 19 / 29
The electrical stimulations (s1(t), ... , s32(t)) were constructed from two independent binary
sources, u1(t) andu2(t), in the following manner:
1. Values for half of the input train (s1(t), ... , s16(t)) were randomly selected asu1(t)w i t ha
a = 3/4 probability, oru2(t)w i t ha1– a = 1/4 probability for each time period. This indi-
cates, for example, that whenu1(1) = 1 andu2(1) = 0,s1(1) = 1 would be expected to occur
with 75% certainty ands1(1) = 0 would be expected to occur with 25% certainty. These
expectations are common amongs1(1), ... , s16(1). As each component ofs1(1), ... , s16(1)
was independently randomly selected,s1(1), ... , s16(1) would become something like
1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1, which means, as population, 75% would be 1 and 25% would
be 0 (although it is one example and the percentage would move stochastically). Thus, sti-
muli were chosen at random at each time period.
2. The values for the rest of trains (s
17(t), ... , s32(t)) were randomly selected byu1(t), with a
1– a = 1/4 probability, oru2(t) with that ofa = 3/4. In other terms, the expectations ofs17(t),
... , s32(t) were exactly opposite to that ofs1(t), ... , s16(t).
3. The location of 32 stimulated electrodes corresponding tos1(t), ... , s32(t) were randomly selected
and fixed over trials. Stimulus evoked responses were recorded with the 64 MEA electrodes.
In other words, the generative model was composed of two hidden sourcesu(t) generated
from the stationary Poisson process with theρ intensity, u(t)~ Po((ρ, ρ)T), 32 merged inputss(t)
generated from the non-stationary Poisson process with the time varying intensity ofA u(t), s(t)
~ Po(A u(t)), and a 32 × 2 transform matrixA,i nw h i c h(Ai1, Ai2)=( a,1 – a)f o ri =1 ,... ,1 6a n d
(Ai1, Ai2)=( 1– a, a)f o ri =1 7 ,... , 32. Unless specifically mentioned, we usedρ = 1/2 anda =3 / 4 .
Pharmacology
In the control condition, 2-Amino-5-phosphonopentanoic acid (APV) (a glutaminergic
NMDA-receptor antagonist; Sigma-Aldrich) was used. APV was adjusted to 20 mM using
PBS, and induced 2μL into culture medium in an MEA dish to make a final concentration of
20 μM. After the injection, cultured neurons were placed for 30 min in a CO
2 incubator, and
stable activity of cultured neurons was confirmed before recording.
Analysis
Spike detection. Before spike detection, artifacts were removed as follows: (i) values in sat-
urated regions in raw data were detected and modified to 0, (ii) 500– 2000 Hz band-pass filter
were applied for the data, and (iii) values in regions that were modified in the first step were
shifted to 0 again (seeFig 1C). Mean (μ) and standard deviation (σ) of extracellular potential
(v) were calculated for each second. A spike was defined as the lowest point of a valley (dv/dτ =
0 anddv2/dτ2 > 0) that was lower than 5 times standard deviation (v − μ < −5 σ). Similar to
previous study [67], if more than two spikes were detected during 0.25 ms, only a spike with
lowest valley was chosen.
Conditional probability and expectation of a response.The Firing probability of neu-
rons recorded at electrodei (i =1 ,... , 64) is shown asxi(τ) [spike/ms]. The strength of evoked
response against thetth stimulus (xi(t) [spike/event];t =1 ,... , 256) is defined as the number of
spikes generated until 10– 30 ms after each stimulation,
xiðtÞ¼
ð1000tþ30
1000tþ10
xiðtÞdt: ð1Þ
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 20 / 29
Using histogram method, conditional probability distributionP(xi(t)| u(t)= u) is non-para-
metrically calculated (Fig 4C and 4D), whereu =( u1, u2)T is a column vector of the source
state. Moreover, as the parametric method, we assume that the probability distribution ofxi(t)
given u(t) obeys the Poisson distribution, which is given by
PðxiðtÞjuðtÞ¼ uÞ¼ ðxi
uÞ
xiðtÞ
expð/C0 xi
uÞ
ðxiðtÞÞ! ; ð2Þ
where xi
u (a parameter of Poisson distribution) was a conditional expectation ofxi(t) when the
state ofu is given. The maximum likelihood estimator ofxi
u was deﬁned asxi
u = E[xi(t)| u(t)=
u, t =1 ,... ,256], whereE[] indicates the expectation, i.e.,xi
u is a mean value ofxi(t) whenu(t)
= u. xi
u was calculated for each trial. All trial average ofxi
u is represented asxi
u . We only evalu-
ated electrode withðxi
0;0 þ xi
1;0 þ xi
0;1 þ xi
1;1 Þ=4 /C211 spike/event as a recording electrode to
be used for analysis. We assumed that a neuron group recorded at electrodei was u1-preferring
when xi
1;0 /C0 xi
0;1 /C210:5 spike/event, u2-preferring whenxi
1;0 /C0 xi
0;1 /C20/C0 0:5 spike/event,
and no preference when otherwise. These neurons were categorized intoG1 (u1-preferring), G2
(u2-preferring), andG0 (no preference), respectively.
Kullback-Leibler divergence. The Kullback-Leibler divergence (KLD) is the distance of
two probability distributions [11]. KLD betweenP(xi(t)| u(t) = (1,0)) andP(xi(t)| u(t) = (0,1))
was defined by
DKLi ¼ DKL½PðxiðtÞjuðtÞ¼ð 1; 0ÞÞjjPðxiðtÞjuðtÞ¼ð 0; 1ÞÞ/C138
¼
X1
m¼0
log PðxiðtÞ¼ mjuðtÞ¼ð 1; 0ÞÞ
PðxiðtÞ¼ mjuðtÞ¼ð 0; 1ÞÞ PðxiðtÞ¼ mjuðtÞ¼ð 1; 0ÞÞ
¼h logPðxiðtÞjuðtÞ¼ð 1; 0ÞÞ /C0 logPðxiðtÞjuðtÞ¼ð 0; 1ÞÞiPðxiðtÞjuðtÞ¼ð1;0ÞÞ;
ð3Þ
where hiP(xi(t)| u(t) = (1,0))is an expectation aroundP(xi(t)| u(t) = (1,0)) (Malkov bracket). Since
we assume thatP(xi(t)| u(t) = (1,0)) andP(xi(t)| u(t) = (0,1)) obey Poisson distribution,Eq 3
was calculated as
DKLi ¼ð logxi
1;0 /C0 logxi
0;1Þxi
1;0 /C0 xi
1;0 þ xi
0;1: ð4Þ
KLD is a non-negative value and becomes 0 if and only if two probability distributions are
exactly equal. When the difference between two distributions is small, KLD becomes a small
value; when the difference is large KLD becomes a large value.
Statistical test. The Wilcoxon signed-rank test was used as a paired testing. The Mann-
Whitney U test was used as an unpaired testing. The Spearman test was used as a test of no
correlation.
Modeling. Neurons in a culture that respond to stimulation with the same property were
assumed to be in the same cell assembly, such that we considered the population model con-
structed from groups ofu
1- andu2-preferring neurons. Thus, we defined~xðtÞ¼
ð~x1ðtÞ; ~x2ðtÞÞ
T
[spike/event] by
~x1ðtÞ¼ E½xiðtÞji 2 G1/C138;
~x2ðtÞ¼ E½xiðtÞji 2 G2/C138:
ð5Þ
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 21 / 29
Furthermore, we assumed that the recognition model used by cultured neurons is the
inverse model with linear firing function, which is represented as
~s0ðtÞ¼ ~sðt /C0 ddÞ;
~xðtÞ¼ W ~s0ðt /C0 dsÞþ ξðtÞ;
ð6Þ
where ~xðtÞ, ~s’ðtÞ [spike/ms] and~sðtÞ [event/ms] are column vectors of synaptic response,
direct response, and input with continuous time (τ [ms]). ξ(τ) [spike/ms] is a background
noise and were assumed to obey a Gaussian distributionξ(τ)~ N(ξ; 0, Σξ). W [spike/event] is a
2×2 connection strength matrix representing identical connections and connections between
two groups (Fig 7D). Note thatd
d [ms] andds [ms] were latencies of responses directly
evoked by stimulations and indirectly evoked via synaptic connections. It is known that direct
responses evoked by extracellular stimulation are highly reproducible with small time variance,
while indirect responses via synaptic connection have larger time variance [43]. Therefore,
although the direct response~s’ðtÞ was difﬁcult to observe, due to the artifact and saturation,
evoked responses against pulse inputs could be regarded as a two-layer feed-forward model,
which is the same form as a linearﬁring rate neuron model constructed from input and output
layers [12– 14, 44]. As input was induced at a moment (assumingτ = 0), using the discrete time
t, the response aroundτ = d
d + ds could be represented as
~xðtÞ¼ W ~sðtÞþ ξðtÞ; ð7Þ
where ~sðtÞ, a column vector, is deﬁned by
~sðtÞ¼ð ~s1;~s2Þ
T
¼ð E½siðtÞj i ¼ 1; ... ; 16/C138; E½siðtÞj i ¼ 17; ... ; 32/C138Þ
T
. A schematic image
of the dynamics of the model is shown inFig 7E.
Generally, inverse recognition models [12– 14, 44] (e.g.,x = Winv s, wheres and x are input
and output vectors, andWinv is a transform matrix corresponding to synaptic connection
strengths) learn the inverse of a transformation matrixA (Winv = A– 1), i.e.,Winv converges to
A– 1 after learning, whereA is a transform matrix of sources (u) to inputs (s) in the generative
model, s = A u. Whereas, feed-forward recognition models [15– 17] (e.g., an equilibrium state
can be represented asWfor x = s) learnA itself, i.e., a connection strength matrixWfor converges
to A after learning. Because the model we assumed was constructed from a two-layer feed-for-
ward model andW was expected to converge toA– 1, our model is categorized into the inverse
model.
Cross-correlation between each electrode and population.Cross-correlation between
xi(t) andu(t) is defined by
corrðxi; uÞ¼ð covðxi; u1Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðxiÞVarðu1Þ
p
; covðxi; u2Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðxiÞVarðu2Þ
p
Þ,
where cov(xi, u) is covariance betweenxi(t) andu(t), and Var(xi) and Var(u) are variance of
them. Then, error of~x from u is deﬁned by
eiðtÞ¼ ~xiðtÞ/C0ð St¼1
256 ~xiðtÞÞ=ðSt¼1
256 uiðtÞÞ uiðtÞ, i = 1, 2. We also deﬁned cross-correlation
between xi(t) ande(t)b y
corrðxi; eÞ¼ð covðxi; e1Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðxiÞVarðe1Þ
p
; covðxi; e2Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðxiÞVarðe2Þ
p
Þ. corr(xi, u) and
corr(xi, e) were used for evaluating whetherxi(t) was state-coding (representingu1, u2)o r
error-coding (representinge1, e2)( Fig 7C).
Estimation of connection strengths.Internal energyUð~s; ~x; WÞ is deﬁned as a negative
log likelihood function,Uð~s; ~x; WÞ¼ /C0 log pðξj WÞ. Note thatξ is regarded as the difference
between an actual output,~x, and an expected outputW ~s. Thus,ξ is the error for a kind of opti-
mal decoder, andUð~s; ~x; WÞ indicates an amount of prediction error. Since we assumedξ
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 22 / 29
obeys Gaussian distribution,ξ ~ N(ξ; 0, Σξ), we get
Uð~sðtÞ; ~xðtÞ; WÞ¼ 1
2 ξT Σx/C0/C1 /C0 1ξ þ 1
2 logð2pÞ
N
jΣxj
¼ 1
2 ð~xðtÞ/C0 W ~sðtÞÞ
T
Σx/C0/C1 /C0 1ð~xðtÞ/C0 W ~sðtÞÞ þ 1
2 logð2pÞ
N
jΣxj:
ð8Þ
As there are no hidden states and hyper-parameters, the expectation ofW can be estimated
using the conventional maximum a posteriori estimation, which is analog of the conventional
model-based connection strength estimation [63, 68]. Since we assumedW obeys a Gaussian
distribution W ~ q(W)= N(W; μ
W, ∑W) and the change inW during a trial is small, the mean
value ofW, μW, is given byW that minimizes the internal action
U ¼ St¼1
256 Uð~sðtÞ; ~xðtÞ; WÞ. By solving the extreme value ofU , @U =@W ¼ 0, we obtain
mW ¼
X256
t¼1
~xðtÞ~sðtÞ
T
 ! X256
t¼1
~sðtÞ~sðtÞ
T
 ! /C0 1
: ð9Þ
μW was calculated for each trial. Thereby, we obtained the model, states, and parameters for
both the generative and recognition models.
Estimation of internal energy, Shannon entropy, and free energy for neurons.Next, we
calculated the free energy for neurons according to the free-energy principle [19, 20]. As above,
the internal energy for neurons was defined byUð~s; ~x; WÞ¼ /C0 log pðξj WÞ, which describes
amount of prediction error. In the recognition model of neuronsqð~x; WÞ, the posterior on the
activity of state coding neurons~x and the posterior on parametersW can be regarded as inde-
pendent, qð~x; WÞ¼ qð~xÞ qðWÞ. We have already obtainedq(W) as a Gaussian distribution.
On the other hand, as shown inFig 7A and 7B, qð~xÞ cannot be readily regarded as a Gaussian
distribution. Thus, we assumedqð~xÞ would be a Gaussian mixture model with 4 peaks corre-
sponding to 4 stimulus source states. Speciﬁcally, qð~xÞ is represented as
qð~xÞ¼ 1
4
X4
m¼1
Nð~x; μxm; ΣxmÞ
¼ 1
4
X4
m¼1
exp
(
/C0 1
2 ð~x /C0 μxmÞ
T
ðΣxmÞ
/C0 1
ð~x /C0 μxmÞ/C0 N
2 log2p /C0 1
2 logjΣxmj
); ð10Þ
where qð~xj u ¼ð 0; 0ÞÞ, qð~xj u ¼ð 1; 0ÞÞ, qð~xj u ¼ð 0; 1ÞÞ, andqð~xj u ¼ð 1; 1ÞÞ are repre-
sented asNð~x; μx1; Σx1Þ, Nð~x; μx2; Σx2Þ, Nð~x; μx3; Σx3Þ, andNð~x; μx4; Σx4Þ, respectively.
Estimators ofμxms, Σxms andΣξ are calculated as
μxm ¼ E½~xðtÞjuðtÞ¼ u; t ¼ 1; ... ; 256/C138¼ð E½xi
uji 2 G1/C138; E½xi
uji 2 G2/C138Þ
T
;
Σxm ¼ E½ð~xðtÞ/C0 μxmÞð ~xðtÞ/C0 μxmÞ
T
juðtÞ¼ u; t ¼ 1; ... ; 256/C138;
Σx ¼ E½ð~xðtÞ/C0 mW~sðtÞÞ ð~xðtÞ/C0 mW~sðtÞÞ
T
juðtÞ¼ u; t ¼ 1; ... ; 256/C138;
ð11Þ
where u becomes (0,0), (1,0), (0,1), and (1,1) whenm is 1, 2, 3, and 4, respectively. The expecta-
tion ofUð~s; ~x; WÞ is given by
hUð~s; ~x; WÞiqð~x;WÞ ¼ 1 þ log 2p þ 1
2 logjΣxj: ð12Þ
Shannon entropy ofqð~xÞ is given by
H½qð~xÞ/C138 ¼ h/C0 logqð~xÞiqð~xÞ; ð13Þ
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 23 / 29
which is approximated asH½qð~xÞ/C138 ¼ /C0 1=256 St¼1
256 log qð~xðtÞÞ. As Shannon entropy ofq
(W), H[q(W)], only depends on~sðtÞ and is a constant over trials, we omitH[q(W)]. Accord-
ingly, the free energy for neuronsFð~s; ~x; mW Þ is represented as
Fð~s; ~x; mW Þ¼h Uð~s; ~x; WÞiqð~x;WÞ /C0 H½qð~xÞ/C138
¼ 1
2 logjΣxjþ 1
2
X256
t¼1
log qð~xðtÞÞ þ const:
ð14Þ
Fð~s; ~x; mW Þ is an upper bound of surprise of input and becomes minimum if and only if
qð ~x; WÞ is the same as the generative model (the true distribution of source).
Estimation of learning efficacy.Learning of cultured neural networks is assumed to obey
Hebbian plasticity [18], which is represented as
dW ¼ auhð~x /C0h ~xiÞ ð~s /C0h ~siÞ
T
iþ εa; ð15Þ
where hi is an expectation aroundpð~xÞ and αu is a learning efﬁcacy depending on state ofu
(α0,0, α1,0, α0,1, andα1,1 are efﬁcacies at the condition ofu = (0,0), (1,0), (0,1), and (1,1), respec-
tively). εα is a 2×2 matrix that represents the error and its elements are assumed to be indepen-
dent of each other. We deﬁned Eq 15as anα-model. Eq 15can be derived from the additive
STDP model [21] when the source state changes rapidly. The aim is to estimate the value ofαu.
Let us setzij
u as zij
u ¼ St2ftjuðtÞ¼ug ð~xiðtÞ/C0h ~xiiÞ ð~siðtÞ/C0h ~siiÞ, which is an element of a 2×2
matrix zu. We assumeα0,0 = 0 since without activation, activity-dependent synaptic plasticity
does not occur. As a simple Hebbian rule, we also assumedα10 = α01 = α11 = α, i.e., learning
efﬁcacies were common for all states ofu except u = (0,0). AsEq 15is rewritten asdWij = α
(zij
1,0 + zij
0,1 + zij
1,1)+ εα, under the assumption thatp(εα
ij| α) is a Gaussian distributionN(εα
ij;
0, Σεαij), the negative log likelihood function forα is deﬁned by
La ¼/C0 P
i;jlog Nðεa
ij;0 ; ΣεaijÞ
¼ 1
2
X100
l¼1
X
i;j
1
2ΣεaijfdWijðlÞ/C0 aðzij
1;0ðlÞþ zij
0;1ðlÞþ zij
1;1ðlÞÞg
2
þ 50
X
i;j
log2pjΣεaijj;
ð16Þ
where dWij(l) andzij
u(l) are the change ofWij in lth trial andzij
u in lth trial. SincedWij is noisy
and saturated in latter part, we assumedWij(l)=( Wij(100) − Wij(1))/100. Additionally, we
assume Σεαijs are common among alli and j. FromEq 16, under the assumption thatα obeys a
Gaussian distributionq(α)= N(α; μα, Σα), the expectation ofα that gives the minimum ofLα is
given by
ma ¼
X100
l¼1
X
i;j dWijðlÞ zij
1;0ðlÞþ zij
0;1ðlÞþ zij
1;1ðlÞ
/C16/C17
X100
l¼1
X
i;j zij
1;0ðlÞþ zij
0;1ðlÞþ zij
1;1ðlÞ
/C16/C17 2 : ð17Þ
Next, we considered the situation where learning efficacies were different depending on the
condition ofu (β-model). We define a learning efficacyβu (β0,0, β1,0, β0,1, andβ1,1) as a function
of u. We assumedβ0,0 =0 ,β1,0 = β0,1 = β1 since u = (1,0) and (0,1) are symmetric, andβ1,1 = β2.
The learning rule of aβ-model is given by
dW ¼ buhð~x /C0h ~xiÞ ð~s /C0h ~siÞ
T
iþ εb; ð18Þ
where εβ is a matrix of error and its elements obeyp(εβ
ij| β1, β2)= N(εβ
ij;0 ,∑εβij). The negative
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 24 / 29
log likelihood function forβ is deﬁned by
Lb ¼/C0 P
i;jlog Nðεb
ij;0 ; ΣεbijÞ
¼ 1
2
X100
l¼1
X
i;j
1
2ΣεbijfdWijðlÞ/C0 b1ðzij
1;0ðlÞþ zij
0;1ðlÞÞ /C0 b2zij
1;1ðlÞg
2
þ 50
X
i;j
log2pjΣεbijj: ð19Þ
We also assumeΣεβijs are common among alli and j. Under the assumption that (β1, β2)T
obeys q((β1, β2)T)= N((β1, β2)T;( μβ1, μβ2)T, Σβ), the expectation of (β1, β2)T that gives the mini-
mum ofLβ is given by
mb1
mb2
 !
¼
X100
l¼1
X
i;j
ðzij
1;0 þ zij
0;1Þ
2
ðzij
1;0 þ zij
0;1Þzij
1;1
zij
1;1ðzij
1;0 þ zij
0;1Þð zij
1;1Þ
2
 !"# /C0 1
X100
l¼1
X
i;j
dWijðzij
1;0 þ zij
0;1Þ
dWijzij
1;1
 !
; ð20Þ
where dWij(l) andzij
u(l) are simpliﬁed asdWij and zij
u.
Supporting Information
S1 Movie. A schematic movie of experimental procedure at trial 1–10. Setup is the same as
that described inFig 3.
(MP4)
S2 Movie. A schematic movie of experimental procedure at trial 91–100.
(MP4)
S3 Movie. The evoked response transient of populations of cultured neurons throughout
training. Axes and colors are same as those inFig 7A and 7B.
(MP4)
S1 Dataset. Summarized dataset of responses of cultured neurons.Data are composed of
conditional expectation transients for each condition (x_u_trn.csv,... , x_u_alt4.csv), Kull-
back-Leibler divergence transients for each condition (kld_trn.csv,... , kld_alt4.csv), and trains
of evoked spike number at trial 1, 11,... , 91 in each culture in the TRN group (x(t)_trn_1.csv,
... , x(t)_trn_23.csv). In file names, alt1, alt2, alt3, and alt4 indicate data under the alternative
conditions where (a, ρ) = (1/2, 1/2), (3/4, 1/4), (3/4, 3/4), and (1, 1/2), respectively.
(ZIP)
S1 Note. Estimation of learning rule.
(DOCX)
S1 Fig. Response properties of cultured neurons to a mixture set of hidden sources. (A)Dis-
tribution. Red circles (open and filled) areu1-preferring electrodes (n = 371 electrodes from 23
cultures). Blue circles (open and filled) areu2-preferring electrodes (n = 345 electrodes from 23
cultures). As all trial average, the response of 13.5% ofu1-preferring electrodes to theu = (1,0)
state was 3 times larger than that to the (0,1) state (filled red circles;n = 50 electrodes from 23
cultures). In addition, the response of 12.8% ofu2-preferring electrodes to the (0,1) state was 3
times larger than that to the (1,0) state (filled blue circles;n = 44 from 23 cultures). A black
solid line,xi
0;1 ¼ xi
1;0 . Black dashed lines,xi
0;1 ¼ xi
1;0 /C6 0:5. A red line,3 /C1xi
0;1 ¼ xi
1;0 . A blue
line, xi
0;1 ¼ 3 /C1xi
1;0 . (B) Transient. A red curve is the ratio of electrodes with 3/C1xi
0,1(l) < xi
1,0(l)
to u1-preferring electrodes. A blue curve is the ratio of electrodes withxi
0,1(l) >3/C1xi
1,0(l)t ou2-
preferring electrodes. Both curves increased during training. Shadowed areas are S.E.M.
(TIFF)
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 25 / 29
S2 Fig. Properties of the neural population model. (A)I/O function of evoked response of
the neural population model. Horizontal axis, total inputs (~s1 þ ~s2). Vertical axis, total outputs
of neural population (~x1 þ ~x2). A black curve is the mean of total output for each total input.
The shadowed area is the standard deviation. Total neural output is almost proportional to
total input except when~s1 þ ~s2 = 0, i.e., whenu = (0,0) state. Since we assume that Hebbian
plasticity does not occur whenu = (0,0) state, effectively, we can regard the I/O function as lin-
ear for considering learning rule of neural networks.(B) γ-norm of connection strengths. Nota-
bly, we deﬁne γ-norm byðjW11j
g
þj W12j
g
þj W21j
g
þj W22j
g
Þ
1=g
. Red, black, and gray curves
are transients of norms withγ = 1, 2, and 4, respectively. The red curve gradually decreased
between trial 20 and 100, while the black and gray curves maintained almost same value
between trial 20 and 100. Therefore, if there is a constraint on total synaptic strength as pre-
dicted by theoretical studies [9], norm withγ =2 – 4 is more consistent with experimental data
than that withγ =1 .
(TIFF)
S3 Fig. Free energy properties in cultured neural networks in the presence of 20-μM APV.
(A) Connection strengths of the neural population. Black circles and squares areW
11 and W22.
White circles and squares areW12 and W21.B a r sa r eS . E . M .(B) The change in connection
strengths (trial 1 vs. trial 100). In the presence of 20-μM APV,W11 and W22 increased after train-
ing (/C3/C3 , p < 10−2; n = 18 from 9 cultures), andW12 and W21 also increased (/C3 , p =0 . 0 2 4 ;n =1 8
from 9 cultures).(C) Transition of the expectation of internal energy (hUi; white circles), Shan-
non entropy (H; black circles), and free energy (F; red circles). Bars are S.E.M.(D) The change in
hUi, H,a n dF (trial 1 vs. trial 100) in the presence of 20-μM APV. After training, the expectation
of internal energy did not change (p =0 . 2 5 0 ;n = 9 cultures), Shannon entropy slightly increased
(/C3 , p =0 . 0 2 7 ;n = 9 cultures), and free energy did not change (p =1 . 0 0 0 ;n = 9 cultures).
(TIFF)
S4 Fig. Hebbian plasticity with aγ-norm constraint on total connection strength. (A)The
expectations ofα’ and λ when we change the degree ofγ-norm constraint. Black and gray
curves are the mean ofμα’(γ) andμλ(γ), respectively.(B) BIC of theα’-model when we change
the degree ofγ-norm constraint. A black curve is the mean of BIC. A dashed line is BIC of the
β-model. Red, black, and gray arrows correspond to red, black, and gray curves inS2B Fig,
respectively. (C) Bayesian model comparison betweenα’- andβ-models. For the wide range of
γ, theβ-model is more plausible than theα’-model to represent experimental data (/C3 , p = 0.035
for γ = 1, red circles;/C3/C3/C3/C3 , p < 10−5 for γ = 2, black circles;/C3/C3/C3/C3 , p < 10−5 for γ = 4, gray circles).
Circle colors correspond to the arrow colors in (B).(D) The change in connection strengths
estimated from theα’-model. A black curve, the mean ofW11 and W22. A gray curve, the mean
of W12 and W21. Solid lines, the true change. Dashed lines, the change estimated from theβ-
model, same asFig 9C. In (A), (B), (D), shadowed areas are S.E.M.
(TIFF)
Author Contributions
Conceived and designed the experiments: TI. Performed the experiments: TI. Analyzed the
data: TI. Wrote the paper: TI KK YJ.
References
1. Belouchrani A, Abed-Meraim K, Cardoso JF, Moulines E (1997) A blind source separation technique
using second-order statistics. Signal Processing IEEE Trans on 45(2): 434– 444.
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 26 / 29
2. Choi S, Cichocki A, Park HM, Lee SY (2005) Blind source separation and independent component
analysis: A review. Neural Information Processing-Letters and Reviews 6(1): 1– 57.
3. Cichocki A, Zdunek R, Phan AH, Amari SI (2009) Nonnegative matrix and tensor factorizations: appli-
cations to exploratory multi-way data analysis and blind source separation. ( John Wiley & Sons).
4. Comon P, Jutten C (Eds.) (2010) Handbook of Blind Source Separation: Independent component anal-
ysis and applications. ( Academic press).
5. Bronkhorst AW (2000) The cocktail party phenomenon: A review of research on speech intelligibility in
multiple-talker conditions. Acta Acustica united with Acustica 86(1): 117– 128.
6. Narayan R, Best V, Ozmeral E, McClaine E, Dent M, Shinn-Cunningham B, Sen K (2007) Cortical inter-
ference effects in the cocktail party problem. Nat Neurosci 10(12): 1601– 1607. PMID:17994016
7. Mesgarani N, Chang EF (2012) Selective cortical representation of attended speaker in multi-talker
speech perception. Nature 485(7397): 233– 236. doi:10.1038/nature11020 PMID: 22522927
8. Golumbic EMZ, Ding N, Bickel S, Lakatos P, Schevon CA, McKhann GM, Schroeder CE (2013) Mecha-
nisms underlying selective neuronal tracking of attended speech at a“cocktail party”. Neuron 77(5):
980– 991. doi:10.1016/j.neuron.2012.12.037 PMID: 23473326
9. Dayan P, Abbott LF (2001) Theoretical neuroscience: computational and mathematical modeling of
neural systems. ( MIT Press, London).
10. Gerstner W, Kistler WM (2002) Spiking neuron models. ( Cambridge University Press, Cambridge).
11. Bishop CM, Nasrabadi NM (2006) Pattern recognition and machine learning. ( Springer, New York).
12. Oja E (1982) A simplified neuron model as a principal component analyzer. J Math Biol 15(3): 267–
273. PMID:7153672
13. Bell AJ, Sejnowski TJ (1995) An information-maximization approach to blind separation and blind
deconvolution. Neural Comput 7(6): 1129– 1159. PMID:7584893
14. Bell AJ, Sejnowski TJ (1997) The“independent components” of natural scenes are edge filters. Vision
Res 37(23): 3327– 3338. PMID:9425547
15. Olshausen BA (1996) Emergence of simple-cell receptive field properties by learning a sparse code for
natural images. Nature 381(6583): 607– 609. PMID:8637596
16. Olshausen BA, Field DJ (1997) Sparse coding with an overcomplete basis set: A strategy employed by
V1?. Vision Res 37(23): 3311– 3325. PMID:9425546
17. Bastos AM, Usrey WM, Adams RA, Mangun GR, Fries P, Friston KJ (2012) Canonical microcircuits for
predictive coding. Neuron 76(4): 695– 711. doi:10.1016/j.neuron.2012.10.038 PMID: 23177956
18. Hebb DO (1949) Organization of Behavior: A Neurophysiological Theory ( John Wiley & Sons, New
York).
19. Friston KJ (2008) Hierarchical model in the brain. PLoS Comput Biol 4(11): e1000211. doi:10.1371/
journal.pcbi.1000211 PMID: 18989391
20. Friston KJ (2010) The free-energy principle: a unified brain theory?. Nat Rev Neurosci 11(2): 127– 138.
doi: 10.1038/nrn2787 PMID: 20068583
21. Song S, Miller KD, Abbott LF (2000) Competitive Hebbian learning through spike-timing-dependent
synaptic plasticity. Nat Neurosci 3(9): 919– 926. PMID:10966623
22. Toyoizumi T, Pfister JP, Aihara K, Gerstner W (2005) Generalized Bienenstock-Cooper-Munro rule for
spiking neurons that maximizes information transmission. Proc Natl Acad Sci USA 102(14): 5239–
5244. PMID:15795376
23. Clopath C, Büsing L, Vasilaki E, Gerstner W (2010) Connectivity reflects coding: a model of voltage-
based STDP with homeostasis. Nat Neurosci 13(3): 344– 352. doi:10.1038/nn.2479 PMID: 20098420
24. Savin C, Joshi P, Triesch J (2010) Independent component analysis in spiking neurons. PLoS Comput
Biol 6(4): e1000757. doi:10.1371/journal.pcbi.1000757 PMID: 20421937
25. Gilson M, Fukai T (2011) Stability versus neuronal specialization for STDP: Long-tail weight distribu-
tions solve the dilemma. PLoS ONE 6(10): e25339. doi:10.1371/journal.pone.0025339 PMID:
22003389
26. Gilson M, Fukai T, Burkitt AN (2012) Spectral analysis of input spike trains by spike-timing-dependent
plasticity. PLoS Comput Biol 8(7): e1002584. doi:10.1371/journal.pcbi.1002584 PMID: 22792056
27. Boerlin M, Machens CK, Deneve S (2013) Predictive coding of dynamical variables in balanced spiking
networks. PLoS Comput Biol 9(11): e1003258. doi:10.1371/journal.pcbi.1003258 PMID: 24244113
28. Berkes P, Orban G, Lengyel M, Fiser J (2011) Spontaneous cortical activity reveals hallmarks of an
optimal internal model of the environment. Science 331(6013): 83– 87. doi:10.1126/science.1195870
PMID: 21212356
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 27 / 29
29. Barth AL, Poulet JFA (2012) Experimental evidence for sparse firing in the neocotex. Trends Neurosci
35(6): 345– 355. doi:10.1016/j.tins.2012.03.008 PMID: 22579264
30. Jimbo Y, Tateno T, Robinson HPC (1999) Simultaneous induction of pathway-specific potentiation and
depression in networks of cortical neurons. Biophys J 76(2): 670– 678. PMID:9929472
31. Johnson HA, Goel AG, Buonomano DV (2010) Neural dynamics of in vitro cortical networks reflects
experienced temporal patterns. Nat Neurosci 13(8): 917– 919. doi:10.1038/nn.2579 PMID: 20543842
32. Shahaf G, Marom S (2001) Learning in networks of cortical neurons. J Neurosci 21(22): 8782– 8788.
PMID: 11698590
33. Eytan D, Brenner N, Marom S (2003) Selective adaptation in networks of cortical neurons. J Neurosci
23(28): 9349– 9356. PMID:14561862
34. Ruaro ME, Bonifazi P, Torre V (2005) Toward the neurocomputer: image processing and pattern recog-
nition with neuronal cultures. IEEE Trans Biomed Eng 52(3): 371– 383. PMID:15759567
35. Feinerman O, Moses E (2006) Transport of information along unidimensional layered networks of dis-
sociated hippocampal neurons and implications for rate coding. J Neurosci 26(17): 4526– 4534. PMID:
16641232
36. Feinerman O, Rotem A, Moses E (2008) Reliable neuronal logic devices from patterned hippocampal
cultures. Nat Phys 4(12): 967– 973.
37. Dranias MR, Ju H, Rajaram E, VanDongen AMJ (2013) Short-term memory in networks of dissociated
cortical neurons. J Neurosci 33(5): 1940– 1953. doi:10.1523/JNEUROSCI.2718-12.2013 PMID:
23365233
38. Turrigiano GG, Nelson SB (2004) Homeostatic plasticity in the developing nervous system. Nat Rev
Neurosci 5(2): 97– 107. PMID:14735113
39. Fong MF, Newman JP, Potter SM (2015) Upward synaptic scaling is dependent on neurotransmission
rather than spiking. Nat Comm 6: 6339.
40. Jimbo Y, Kasai N, Torimitsu K, Tateno T, Robinson HPC (2003) A system for MEA-based multisite stim-
ulation. IEEE Trans Biomed Eng 50(2): 241– 248. PMID:12665038
41. Jaynes E. T. (1957). Information theory and statistical mechanics. Physical Review, 106(4): 620.
42. Jaynes E. T. (1957). Information theory and statistical mechanics. II. Physical Review, 108(2): 171.
43. Bakkum DJ, Chao ZC, Potter SM (2008) Long-term activity-dependent plasticity of action potential
propagation delay and amplitude in cortical networks. PLoS ONE 3(5): e2088. doi:10.1371/journal.
pone.0002088 PMID: 18461127
44. Amari SI, Cichocki A, Yang HH (1996) A new learning algorithm for blind signal separation. Adv Neural
Inf Proc Sys 8: 757– 763.
45. Feldt S, Bonifazi P, Cossart R (2011) Dissecting functional connectivity of neuronal microcircuits:
experimental and theoretical insights. Trends Neurosci 34(5): 225– 236. doi:10.1016/j.tins.2011.02.
007 PMID: 21459463
46. Stetter O, Battaglia D, Soriano J, Geisel T (2012) Model-free reconstruction of excitatory neuronal con-
nectivity from calcium imaging signals. PLoS Comput Biol 8(8): e1002653. doi:10.1371/journal.pcbi.
1002653 PMID: 22927808
47. Schwarz G (1978) Estimating the dimension of a model. Ann Stat 6(2): 461– 464.
48. Földiak P (1990) Forming sparse representations by local anti-Hebbian learning. Biol Cybern 64(2):
165– 170. PMID:2291903
49. Markram H, Lübke J, Frotscher M, Sakmann B (1997) Regulation of synaptic efficacy by coincidence of
postsynaptic APs and EPSPs. Science 275(5297): 213– 215. PMID:8985014
50. Bi GQ, Poo MM (1998) Synaptic modifications in cultured hippocampal neurons: dependence on spike
timing, synaptic strength, and postsynaptic cell type. J Neurosci 18(24): 10464– 10472. PMID:
9852584
51. Butts DA, Kanold PO, Shatz CJ (2007) A burst-based“Hebbian” learning rule at retinogeniculate synap-
ses links retinal waves to activity-dependent refinement. PLoS Biol 5(3): e61. PMID:17341130
52. Goel A, Buonomano DV (2013) Chronic electrical stimulation homeostatically decreases spontaneous
activity, but paradoxically increases evoked network activity. J Neurophysiol 109(7): 1836– 2013.
53. Lourenço J., Pacioni S., Rebola N., van Woerden G. M., Marinelli S., DiGregorio D., & Bacci A. (2014).
Non-associative Potentiation of Perisomatic Inhibition Alters the Temporal Coding of Neocortical Layer
5 Pyramidal Neurons. PLoS Biol 12(7): e1001903. doi:10.1371/journal.pbio.1001903 PMID:
25003184
54. D’amour J. A., & Froemke R. C. (2015). Inhibitory and excitatory spike-timing-dependent plasticity in
the auditory cortex. Neuron, 86(2), 514– 528. doi:10.1016/j.neuron.2015.03.014 PMID: 25843405
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 28 / 29
55. Linsker R (1997) A local learning rule that enables information maximization for arbitrary input distribu-
tions. Neural Comput 9(8): 1661– 1665.
56. Linsker R (1992) Local synaptic learning rules suffice to maximize mutual information in a linear net-
work. Neural Comput 4(5): 691– 702.
57. Lee TW, Girolami M, Bell AJ, Sejnowski TJ (2000) A unifying information-theoretic framework for inde-
pendent component analysis. Comput Math Appl 39(11): 1– 21.
58. Hayama T, Noguchi J, Watanabe S, Takahashi N, Hayashi-Takagi A, Ellis-Davies GC, Matsuzaki M,
Kasai H (2013) GABA promotes the competitive selection of dendritic spines by controlling local Ca2+
signaling. Nat Neurosci 16(10): 1409– 1416. doi:10.1038/nn.3496 PMID: 23974706
59. Paille V, Fino E, Du K, Morera-Herreras T, Perez S, Kotaleski JH, Venance L (2013) GABAergic Cir-
cuits Control Spike-Timing-Dependent Plasticity. J Neurosci 33(22): 9353– 9363. doi:10.1523/
JNEUROSCI.5796-12.2013 PMID: 23719804
60. Leutgeb JK, Leutgeb S, Moser MB, Moser EI (2007) Pattern separation in the dentate gyrus and CA3 of
the hippocampus. Science 315(5814): 961– 966. PMID:17303747
61. Wiechert MT, Judkewitz B, Riecke H, Friedrich RW (2010) Mechanisms of pattern decorrelation by
recurrent neuronal circuits. Nat Neurosci 13(8): 1003– 1010. doi:10.1038/nn.2591 PMID: 20581841
62. Pitkow X, Meister M (2012) Decorrelation and efficient coding by retinal ganglion cells. Nat Neurosci 15
(4): 628– 635. doi:10.1038/nn.3064 PMID: 22406548
63. Isomura T, Ogawa Y, Kotani K, Jimbo Y (2015) Accurate connection strength estimation based on vari-
ational Bayes for detecting synaptic plasticity. Neural Comput 27(4): 819– 844. doi:10.1162/NECO_a_
00721 PMID: 25710089
64. Kamioka H, Maeda E, Jimbo Y, Robinson HPC, Kawana A (1996) Spontaneous periodic synchronized
bursting during formation of mature patterns of connections in cortical cultures. Neurosci Lett 206(2):
109– 112.
65. Mukai Y, Shiina T, Jimbo Y (2003) Continuous monitoring of developmental activity changes in cultured
cortical networks. Electr Eng Jpn 145(4): 28– 37.
66. Tetzlaff C, Okujeni S, Egert U, Wörgötter F, Butz M (2010) Self-organized criticality in developing neu-
ronal networks. PLoS Comput Biol 6(12): e1001013. doi:10.1371/journal.pcbi.1001013 PMID:
21152008
67. Takekawa T, Isomura Y, Fukai T (2010) Accurate spike sorting for multi-unit recordings. Eur J Neurosci
31(2): 263– 272. doi:10.1111/j.1460-9568.2009.07068.x PMID: 20074217
68. Paninski L, Pillow JW, Simoncelli EP (2004) Maximum likelihood estimation of a stochastic integrate-
and-fire neural encoding model. Neural Comput 16(12): 2533– 2561. PMID:15516273
Blind Source Separation by Neural Nets In Vitro
PLOS Computational Biology | DOI:10.1371/journal.pcbi.1004643 December 21, 2015 29 / 29