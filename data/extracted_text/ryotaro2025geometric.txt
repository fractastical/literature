A Geometric Free-Energy Framework
for Pragmatic Neural Population Codes
Ryotaro Saito
Independent Researcher
ryotarosaito136@gmail.com
November 26, 2025
Abstract
Biological systems must act and communicate under pervasive uncertainty. The Free
Energy Principle (FEP) and active inference describe such systems as minimising variational
and expected free energy, but it remains unclear how this view constrains pragmatic com-
munication and concrete neural population codes. Here we develop a geometric framework
that links an expected free energy (EFE) control objective, probabilistic speaker–listener
models, and information-theoretic measures of communication quality that can be evaluated
on large-scale neural data.
On the “speaker” side, we posit a message-level EFEG(u; w,c) for triples of messageu,
world statew, and contextc, and show how an EFE-based softmax over messages induces
a pragmatic speaker. We introduce an explicit parametrisation ofG(u; w,c) in terms of
a literal listener, production cost, and information-gain term, and identify a structural
degeneracy: standard formulations entangle semantic and normalisation terms in the literal
listener. We resolve this by a behaviour-preserving reparameterisation that replaces the literal
log-posterior with (i) a pure generative semantics term that depends only onP(u|w,c), (ii)
a normalisation-adjusted production cost, and (iii) an explicit information-gain predictor. In
the resulting speaker law, these predictors are structurally independent and, in principle,
jointly estimable from data.
On the “listener” side, we embed messages into a latent metric space and endow this
space with a Gaussian prototype geometry. An ideal listener combines latent likelihoods and
block-wise task priors via Bayes’ rule in latent space, while an actual listener is implemented
as a logistic decoder on the same latent codes. From their posteriors we derive two unitful
trial-wise diagnostics: an information gainIG(u,c), defined as the reduction in entropy of the
ideal listener from prior to posterior, and a pragmatic lossℓ∗(u,c), defined as the Kullback–
Leibler divergence between ideal and actual posteriors. Becauseℓ∗ is a KL divergence, its
numerical scale is fixed by information theory; there is no free slope parameter that could
absorb discrepancies between ideal and actual listeners.
We instantiate this framework on Neuropixels recordings from 13 probe insertions in
the International Brain Laboratory visual decision-making task, treating the well-isolated
units on each probe as a sender population. Trial-by-neuron spike counts in a peri-stimulus
window are embedded into ad= 10 dimensional latent space using factor analysis, Gaussian
prototypes and a shared covariance are estimated in that space, an ideal listener is constructed
from Mahalanobis distances and block priors, and an actual listener is fitted as a logistic
regression on the same latent codes. Across 8,476 valid trials and 2,344 neurons, latent codes
cluster reliably by world state and support robust ideal decoding (session-wise mean accuracy
≈0.79), while the logistic listener performs worse on average (mean test accuracy≈0.68),
leaving an accuracy gap of about0.10 in favour of the geometric ideal.
The information-theoretic metrics provide a complementary view. Session-wise mean
information gain is modest but non-zero (IGsess ≈0.086 ±0.060 nats, ≈0.13 bits per trial),
whereas mean pragmatic loss is somewhat larger (ℓ∗
sess ≈0.130 ±0.023 nats, ≈0.19 bits),
indicating that this particular discriminative listener systematically deviates from the ideal
1
latent-space decoder. Both quantities vary systematically with context: when trials are
grouped by block priorP(Left |c) ∈{0.2,0.5,0.8}, information gain remains of similar
magnitude in strongly biased and unbiased blocks, but pragmatic loss is almost negligible
in unbiased blocks and increases by an order of magnitude in biased blocks. A prior-aware
control decoder that receives an explicit encoding of the block prior as an additional feature
both improves test accuracy (to≈0.79) and collapses mean pragmatic loss to≈0.02 nats,
showing that much of the apparent loss under biased priors is attributable to our deliberate
choice to withhold block priors from the main actual listener, rather than to a fundamental
limitation of the latent geometry.
We interpret information gain and pragmatic loss as model-based diagnostics of how
effectively information encoded in a sender population could, in principle, be exploited
by particular classes of downstream decoders, rather than as direct measurements of any
specific biological circuit. Although our empirical instantiation focuses on a binary world
state, a linear–Gaussian latent geometry, and simple logistic decoders in a specific visual
decision-making task, the construction itself is substrate-agnostic: wherever world states,
contexts, messages, and an appropriate geometric representation can be defined, the same
pair (IG,ℓ∗) can, in principle, be used to quantify communication quality. We argue that this
framework provides a practical bridge between FEP, probabilistic pragmatics, and systems
neuroscience, and we outline how similar tools could be applied to other forms of biological
communication.
1 Introduction
Biological systems must act, perceive, and communicate under pervasive uncertainty. Sensory
inputs are noisy and incomplete, internal states are only partially observable, and the future
consequences of actions are often ambiguous or delayed. The Free Energy Principle (FEP) and
active inference propose a unifying account of such systems: they are described as maintaining
their organisation and generating adaptive behaviour by minimising variational free energy and
its forward-looking counterpart, expected free energy (EFE). [3–6] In this view, behaviour and
perception arise from a single imperative to render sensory data unsurprising under a generative
model. Although this framework has been influential as a high-level description of adaptive
control, its implications for concrete communicative acts and neural population codes remain
comparatively under-specified.
In parallel, pragmatic theories of communication — in linguistics, animal signalling, and
systems neuroscience — emphasise that signals are not mere reports about the world, but actions
chosen to reshape a listener’s beliefs and subsequent behaviour in a context-dependent manner.
[2, 7, 8] Gricean accounts cast communication as cooperative inference over intentions: speakers
choose utterances expecting listeners to reason about their goals and knowledge. Modern
probabilistic models, such as the Rational Speech Act (RSA) family, make this explicit by
postulating a literal listener and a pragmatic speaker who selects messages via a softmax over a
utility that combines informativeness and production cost. [2, 7] Related ideas appear in neural
coding and animal communication, where one asks not only what a population encodes about
latent variables, but also how those codes are used by downstream decoders to guide behaviour.
[1, 9, 10]
Despite superficial similarities, the connections between FEP/active inference, probabilistic
pragmatics, and neural population coding are still incomplete. From the FEP side, the EFE
formalism provides a scalar objectiveG(π) over policiesπ, and a Boltzmann distribution over
policies Π(π) ∝exp[−βG(π)], but it does not by itself specify how individual messages or
neural codes should be interpreted as communicative actions, nor how their “value” should be
decomposed into semantic, energetic, and epistemic contributions. From the pragmatic side,
RSA-style models typically introduce a literal listener, production costs, and information-gain
terms by assumption: they posit a particular log-utility for utterances and then apply a softmax,
without showing how these components might arise from, or be constrained by, an EFE-based
2
control objective. Even within this class, semantic and cost components are often structurally
entangled because Bayes’ rule couples likelihood and normalisation terms in the literal listener,
making it difficult to attribute variation in behaviour uniquely to “semantics” as opposed to
“cost” or “informativeness”.
A second gap concerns empirical applicability to neural data. Systems neuroscience has
developed a rich toolkit for quantifying how much task-relevant information a neural population
carries about latent variables, often in terms of decoding accuracy or mutual information between
neural activity and task variables. [1] These analyses are typically framed in terms of an ideal
decoder: one asks how accurately the world state can be recovered from a population if a
decoder is tuned optimally. This reveals whatcould be read out from the population, but it says
much less about how closely actual downstream processes implement such an ideal, or about
how this efficiency depends on context and priors. Put differently, standard decoding tells us
whether a population carries information, but not whether that information is used efficiently by
biologically plausible listeners, nor how inefficiencies should be quantified in a principled, unitful
way.
In this paper we address these gaps by developing a geometric framework that links EFE-
based control, probabilistic pragmatics, and neural population codes, and by instantiating this
framework on large-scale spiking data from the International Brain Laboratory (IBL) Neuropixels
dataset. [9, 10] Our starting point is an EFE-inspired softmax overmessages. We introduce
an explicit modelling assumption at the message level: for each triple of messageu, world
state w, and contextc, there exists a message-level scoreG(u; w,c) that plays the role of an
expected free energy for that communicative action. We then parametriseG(u; w,c) in a way that
mirrors standard risk/ambiguity/epistemic decompositions: as a sum of (i) a term that penalises
messages which would lead a listener to misidentify the world state, (ii) a context-dependent
production cost, and (iii) an information-gain term that rewards expected reductions in the
listener’s uncertainty. This parametrisation should be read as a convenient, FEP-compatible
modelling choice rather than a unique consequence of the general EFE expression, and we keep
this distinction explicit throughout.
Substituting this parametrisation into an EFE-based softmax yields a pragmatic speaker
whose log-odds of choosing one message over another decompose into three components: how
well a listener would recoverw from u, how costly it is to produceuin contextc, and how much
information u is expected to provide. However, if the listener is defined in the usual way by
Bayes’ rule on a generative semanticsP(u |w,c) and a priorP(w |c), then its log-posterior
contains both a log-likelihood and a log-normalisation term. Treating these as independent
predictors leads to a structural collinearity: only theirdifference is behaviourally observable. To
resolve this, we introduce a behaviour-preserving reparameterisation that replaces the literal-log-
posterior term with a pure generative-semantics term (a log-likelihood difference that depends
only onP(u|w,c)) and a redefined production cost that absorbs the normalisation constant.
In the resulting speaker law, semantic, cost, and information-gain predictors are structurally
independent and, in principle, jointly estimable from data. Our use of “reparameterisation
invariance” refers exclusively to invariance of choice probabilities under such transformations; no
additional physical fields or spacetime symmetries are invoked.
To connect this theory to neural data, we endow messages with a geometric representation.
We assume that each messageu is associated with a codez(u) in a latent metric space and
that the generative semanticsP(u|w,c) is a monotone function of distance betweenz(u) and
prototype points associated with world states and contexts. In the Gaussian case, this amounts
to a latent variable model in which codes cluster around state-dependent means with a shared
covariance, so that log-likelihoods become quadratic functions of Mahalanobis distance. [1] On
top of this geometry we define two types of listeners. Anideal listener combines the latent
likelihood with a task priorP(w |c) via Bayes’ rule in latent space, yielding a fully specified
posterior Lideal(w|u,c). One or moreactual listeners are inferred from downstream decoding; in
3
this work we instantiate them as logistic regression decoders on the same latent codes, either with
or without explicit access to block priors. From these listeners we derive information-theoretic
diagnostics in natural units (nats): a single-trial information gainIG(u,c) that measures how
much the ideal listener’s entropy over world states is reduced by observingu in contextc, and
several forms ofpragmatic lossdefined as Kullback–Leibler divergences between ideal and actual
listeners.
A central technical contribution of this paper is afour-way decomposition of pragmatic
loss that separates the effects of priors, geometry, and decoder class. Previous versions of our
analyses, and many standard decoding studies, compare an ideal listener that uses block priors
with an actual listener that does not, implicitly attributing large Kullback–Leibler divergences in
biased blocks to “inefficient” neural communication. However, such comparisons confound two
distinct manipulations: withholding priors from the decoder and differences in latent geometry.
Here we therefore compute pragmatic losses for all combinations of prior-aware vs. prior-agnostic
ideal listeners, and prior-aware vs. prior-agnostic actual listeners. Concretely, we consider: (i) a
prior-aware ideal listener with block priors, (ii) an “ideal” listener constrained to use a uniform
prior over world states, (iii) a prior-agnostic logistic decoder operating on latent codes only, and
(iv) a prior-aware logistic decoder that receives both latent codes and block priors as features.
From these we obtain four KL-based losses that quantify, respectively, the cost of withholding
priors from the actual listener, the cost of forcing the ideal listener to ignore block structure,
the residual mismatch between latent geometry and decoder class, and the small irreducible
differences that remain even when both listeners have access to the same prior information. This
four-pattern analysis allows us to answer, in a principled way, whether large measured losses
reflect genuine biological inefficiencies or artefacts of modelling choices.
We instantiate this framework on Neuropixels recordings from mice performing the IBL
visual contrast discrimination task [9,10]. For each probe insertion we treat the population
of well-isolated units on a single probe as a sender. We select valid trials, construct a trial-
by-neuron spike-count matrix in a fixed peri-stimulus window, and embed these counts into a
low-dimensional latent space using factor analysis. [1] On this latent space we estimate Gaussian
prototypes for task states and build ideal listeners that depend only on Mahalanobis distances
and block priors. We then fit both prior-agnostic and prior-aware logistic decoders on the same
latent codes, and, for every trial, compute information gain and all four forms of pragmatic loss.
Across thousands of trials and thirteen probe insertions, latent codes cluster reliably by world
state and support robust ideal decoding, while the various pragmatic losses reveal a non-trivial,
context-dependent gap between what an ideal listener could infer and what these particular
logistic decoders in fact implement. The four-pattern analysis shows that much of the loss
observed in strongly biased blocks can indeed be attributed to withholding priors from the
decoder, but that a residual loss remains even under “fair” comparisons in which ideal and actual
listeners receive matched prior information.
We interpret information gain and pragmatic loss not as direct measurements of any particular
biological circuit, but as model-based diagnostics of how effectively a chosen sender–listener pair
could, in principle, support communication under uncertainty, given an explicit latent geometry
and decoder class. By construction, these diagnostics have fixed units and scales: information
gain reduces to a single-trial contribution to mutual information, and pragmatic loss is a KL
divergence between posteriors. The framework itself is substrate-agnostic: wherever one can
define world states, contexts, messages, and a geometric representation, the same constructions
apply. Although we focus here on cortical spike trains in a specific decision-making task, we
briefly discuss how similar tools could be applied to other forms of biological communication,
such as cell–cell signalling, hormonal pathways, or animal vocalisation.
The remainder of the paper is organised as follows. We first summarise the theoretical
framework, from EFE-based softmax policies to a reparameterised pragmatic speaker and
geometric listeners, with the four-way loss decomposition made explicit. We then describe the
4
dataset and analysis pipeline, including trial selection, latent geometry, listener construction,
and the computation of our information-theoretic metrics. Next we report empirical results
on the IBL Neuropixels data, focusing on decoding accuracy, information gain, the pattern
of pragmatic losses across priors and listener types, and exploratory links to behaviour. We
conclude by discussing conceptual implications, methodological limitations, and directions for
applying this geometric pragmatic framework to other neural and biological communication
systems.
2 Theoretical framework
Our goal in this section is to collect the theoretical ingredients that connect (i) expected free
energy (EFE) based control, (ii) probabilistic speaker–listener models in pragmatics, and (iii)
geometric codes for neural population activity.
We deliberately keep the assumptions minimal. First, we recall how EFE induces a softmax
choice rule over policies and introduce an analogous message-level score. Second, we show
how a pragmatic parametrisation of this score yields a speaker whose log-odds decompose into
semantic, cost, and epistemic components, and how a simple reparameterisation removes a
structural degeneracy between semantics and normalisation. Third, we equip messages with a
latent geometry and define families ofideal and actual listeners in this space. Finally, we define
information-theoretic diagnostics that compare these listeners and, in particular, introduce a
four-pattern family of pragmatic losses that will be central in the empirical analysis.
Throughout, w ∈W denotes a world state,c∈C a context, andu∈U a message. In the
Neuropixels application,w will encode which side carried more visual contrast (Left vs. Right),c
will index block priors overw, anduwill be a population response on a single probe. For clarity,
we distinguish carefully between assumptions that follow standard active-inference practice
and modelling choices that are introduced specifically for the present geometric–pragmatic
framework.
2.1 From expected free energy to message-level control
Active inference describes agents as selecting policies that minimise an expected free energy
functional G(π) defined on policiesπ over trajectories of observations and states. One convenient
definition is
G(π) := Ep(o1:T ,s1:T |π)
[
ln q(s1:T |π) −ln p(o1:T,s1:T |π)
]
, (1)
where p is a predictive generative model,q is a variational density, and the expectation is taken
with respect top(o1:T,s1:T |π). Rearrangements of(1) yield the familiar “risk / ambiguity /
epistemic value” decompositions, but here we only use two generic properties: (i)G(π) assigns a
scalar score to each policy, and (ii) a Boltzmann policy
Π(π) ∝ exp
[
−βG(π)
]
, β > 0, (2)
provides a convenient stochastic choice rule in which lower-EFE policies are exponentially
favoured over higher-EFE ones.
In this paper we do not attempt to construct a full generative model for the IBL task.
Instead, we adopt thestructure of (2) at the level of individual messages. Intuitively, we treat
each messageu as a short-horizon policy that is available when the world is in statew and the
agent occupies contextc (e.g. a particular block).
Message-level expected free energy. We make the following assumption.
Assumption A1 (message-level EFE).For every triple(u,w,c ) ∈U×W×C there exists a
scalar quantity
G(u; w,c), (3)
5
interpreted as the expected free energy that would be incurred if, in world statew and context
c, the agent emits messageu.
We never need to specify the underlying generative model that gives rise toG(u; w,c).
For what follows only theordering of messages matters: messages with smallerG(u; w,c) are
preferred.
By analogy with (2) we define a message-level softmax
S(u|w,c) ∝ exp
[
−αG(u; w,c)
]
, α> 0, (4)
which we interpret as a pragmatic speaker distribution over messages. For two alternativesu+
and u−, the log-odds obey the “difference law”
log S(u+ |w,c)
S(u−|w,c) = −α
[
G(u+; w,c) −G(u−; w,c)
]
. (5)
Relative preferences therefore depend only on EFEdifferences, not on their absolute levels.
2.2 Pragmatic parametrisation and reparameterisation invariance
The EFE expression(1) can be rearranged into several equivalent forms emphasising accuracy,
complexity, risk, ambiguity, and so on. For our purposes it is more useful to introduce an explicit
pragmatic parametrisationof the message-level scoreG(u; w,c) that mirrors these decompositions
while making contact with standard models of pragmatic language use.
A pragmatic decomposition. We posit that for fixed(w,c) the quantityG(u; w,c) can be
written as
G(u; w,c) = −
[
log Llit(w|u,c) −CS(u,c) + IG(u,c)
]
+ g(w,c), (6)
where
• Llit(w |u,c) is the posterior of aliteral listener who interprets u under a generative
semantics and a prior;
• CS(u,c) is a context-dependent production cost (energetic, mechanical, or opportunity
cost);
• IG(u,c) is a message-level information-gain term; and
• g(w,c) is a baseline independent ofu.
This decomposition is a modelling assumption, not a new axiom of the FEP. It is designed so
that good messages (lowG) are ones that help a literal listener recover the true world state, are
cheap to produce, and are expected to reduce uncertainty.
Substituting (6) into the softmax(4) and absorbingg(w,c) into the normalisation yields a
pragmatic speaker law
S(u|w,c) ∝ exp
{
α
[
log Llit(w|u,c) −CS(u,c) + IG(u,c)
]}
. (7)
For two messagesu+ and u−,
log S(u+ |w,c)
S(u−|w,c) = α∆ logLlit − α∆CS + α∆IG, (8)
where ∆ denotes a difference betweenu+ and u−. Messages are favoured when they score well
under the literal listener, are cheap, and are epistemically valuable.
6
Deriving the pragmatic form from an EFE-based softmax.For completeness, it is
helpful to see how a decomposition of the form(6) can arise from the EFE-based softmax(4).
For each(w,c) define the partition function
ZG(w,c) :=
∑
u′∈U
exp
[
−αG(u′; w,c)
]
,
so that the softmax can be written with an explicit normaliser as
S(u|w,c) = exp
[
−αG(u; w,c)
]
ZG(w,c) .
Taking logarithms yields
log S(u|w,c) = −αG(u; w,c) −log ZG(w,c).
Independently, the pragmatic form (7) admits a normalised representation
S(u|w,c) = exp
{
α[log Llit(w|u,c) −CS(u,c) + IG(u,c)]
}
ZS(w,c) ,
with partition function
ZS(w,c) :=
∑
u′∈U
exp
{
α[log Llit(w|u′,c) −CS(u′,c) + IG(u′,c)]
}
.
Taking logarithms again gives
log S(u|w,c) = α[log Llit(w|u,c) −CS(u,c) + IG(u,c)] −log ZS(w,c).
Since these two expressions describe the same speakerS(u |w,c), their logarithms must
coincide. Solving forG(u; w,c) gives
G(u; w,c) = −
[
log Llit(w|u,c) −CS(u,c) + IG(u,c)
]
+ g(w,c),
with a world- and context-dependent baseline
g(w,c) := 1
α
[
log ZS(w,c) −log ZG(w,c)
]
.
The offset g(w,c) depends only on (w,c) and not on the messageu. Adding such a term
to G(u; w,c) leaves the softmax (4) unchanged, because it cancels between numerator and
denominator. In this sense (6) can be viewed as a behaviour-preserving reparameterisation
of an underlying EFE-based policy, withg(w,c) absorbing any world- and context-dependent
baseline. Closely related assumption ladders and generalisations of this derivation for linguistic
and biological signalling problems are developed in separate preprints on the FEP–to–RSA
bridge and on pragmatic constraints for biological communication.[11, 12]
Literal listener, semantics, and normalisation.The literal listener is defined by Bayes’
rule on a generative semanticsP(u|w,c) and a priorP(w|c):
Llit(w|u,c) = P(u|w,c) P(w|c)
Zlit(u,c) , Z lit(u,c) =
∑
w′∈W
P(u|w′,c) P(w′|c). (9)
Taking logarithms,
log Llit(w|u,c) = log P(u|w,c) + logP(w|c) −log Zlit(u,c). (10)
For a fixed (w,c), the prior term does not depend on u, so the only message-dependent
contributions are alog-likelihood term log P(u |w,c) and a normalisation term log Zlit(u,c).
Consequently,∆ log Llit in (8) can always be written as a difference between these two pieces.
This implies a structural degeneracy: any attempt to assign separate coefficients to the log-
likelihood and log-normalisation parts would be unidentifiable from behaviour, because only
their difference can be observed.
7
Reparameterisation by pure semantics and adjusted cost.To make this degeneracy
explicit, and to decouple semantics from normalisation, we introduce two new predictors:
∆(g)
sem := log P(u+ |w,c) −log P(u−|w,c), (11)
¯CS(u,c) := CS(u,c) + logZlit(u,c). (12)
The first is apure generative semanticsdifference depending only on the likelihoods; the second
is anormalisation-adjusted costthat absorbs the log-partition function. Their difference is
∆ ¯CS = ¯CS(u+,c) −¯CS(u−,c) = ∆ CS + ∆ logZlit, (13)
so that
∆ logLlit = ∆ (g)
sem −∆ logZlit. (14)
Adding and subtractingα∆ logZlit in (8) yields
log S(u+ |w,c)
S(u−|w,c) = α∆(g)
sem − α∆ ¯CS + α∆IG. (15)
Equation (15) expresses the same speaker as(8), but in terms of three structurally independent
predictors: a pure semantic advantage, a normalisation-adjusted production cost, and an
information-gain difference. Adding the same correction∆h(u+,u−,c) to both∆(g)
sem and ∆ ¯CS
leaves the right-hand side of(15) unchanged. We use the termreparameterisation invariancefor
this behavioural invariance under such transformations. Unlike gauge invariance in physics, no
additional fields or spacetime symmetries are introduced: the invariance here concerns only how
a fixed log-odds is split into semantic and cost predictors.
In the remainder of the paper we will not attempt to infer the full speaker law(15) from
behaviour. Instead we focus on thelistener side of the framework, which is already sufficient to
define falsifiable, unitful diagnostics on neural data.
2.3 Geometric codes and Gaussian ideal listeners
To connect the abstract speaker–listener construction to neural population activity, we now
endow messages with a geometric representation and build ideal listeners directly in this space.
Latent geometric codes. We assume a latent metric space(Z,d) and an encoder
φ: U→Z , z (u) := φ(u), (16)
which maps each message (here: trial-by-neuron spike-count vector) to a latent codez(u) ∈Rd.
In the empirical work this encoder is given by a factor-analysis model fitted separately for each
session.
We further assume that, conditional on a world statew and contextc, the latent codes follow
a Gaussian distribution with a state- and context-dependent mean and a shared covariance:
p(z|w,c) = N
(
z; µ(w,c),Σ
)
. (17)
The negative log-likelihood is, up to an additive constant, the squared Mahalanobis distance
d2
w(z; c) :=
(
z−µ(w,c)
)⊤Σ−1(
z−µ(w,c)
)
. (18)
Identifying the generative semanticsP(u |w,c) in (9) with p(z(u) |w,c) makes semantic
advantages purely geometric: differences in log-likelihood reduce to differences in squared
Mahalanobis distance between latent codes and state-dependent prototypes.
8
A family of ideal listeners.Given the latent likelihood(17) and a priorπ(w|c) over world
states, we define anideal listenerin latent space by Bayes’ rule:
Lideal
π (w|z,c) := exp
(
−1
2 d2
w(z; c)
)
π(w|c)
∑
w′∈Wexp
(
−1
2 d2
w′ (z; c)
)
π(w′|c). (19)
Composing with the encoder yields an ideal listener over messages,Lideal
π (w|u,c) := Lideal
π (w|
z(u),c).
Two choices ofπ will play a special role:
• Block prior. πprior(w |c) equals the task prior over world states in contextc (e.g.
block-wise probabilities over Left/Right). We denote the corresponding ideal listener by
Lideal,prior := Lideal
πprior.
• Uniform prior.πunif(w|c) is uniform overWfor allc; for a binary world,πunif(w|c) =
1/2. The associated ideal listenerLideal,unif := Lideal
πunif depends only on latent geometry and
ignores block priors.
Both listeners share the same latent prototypes and covariance, differing only in the priors they
combine them with.
2.4 Actual listeners and logistic decoding
An actual listener represents how some downstream system — another neural population, a
behavioural controller, or an explicit decoder — in fact maps messages to beliefs over world
states. Mathematically, an actual listener is any family of distributionsL0(w |u,c) with the
same support as the ideal listener.
In this paper we instantiateL0 as logistic regression classifiers acting on latent codes. This
choice is intentionally simple: logistic regression is a standard, well-understood decoder that
can be fit robustly with the available trial counts, and it provides a reproducible benchmark for
comparisons with the ideal listener.
Prior-agnostic actual listener. Our main actual listenerLagn
0 is prior-agnostic: it operates
on latent codes alone and does not receive block priors as input. Concretely, we fit a binary
logistic regression
Lagn
0 (w|z) = softmax w
(
θ⊤z+ b
)
, (20)
where (θ,b) are fitted parameters andsoftmax returns a probability vector overW= {0,1}.
Training is carried out on a fixed subset of trials, and performance is evaluated on held-out test
trials as described in Section 3.
Prior-aware control listener. To separate effects of latent geometry from the handling
of block priors we also consider aprior-aware logistic decoderLprior
0 (w|z,c) that receives an
additional scalar feature encoding the block prior, for example the log-odds ofP(w=0 |c). The
feature vector for each trial is then
˜z(u,c) =
[
z(u)⊤, ϕprior(c)
]⊤, (21)
and the decoder is
Lprior
0 (w|z(u),c) = softmax w
(˜θ⊤˜z(u,c) +˜b
)
. (22)
By construction,Lprior
0 has access to the same block prior information asLideal,prior, but uses
the same model class (logistic regression) asLagn
0 . Comparing the behaviour of these listeners
will allow us to disentangle the contribution of priors from the contribution of latent geometry
and decoder capacity.
9
2.5 Information-theoretic metrics
Given an ideal listener and one or more actual listeners, we now define information-theoretic
quantities that characterise communication quality at the level of single trials. All metrics are
measured in nats, using natural logarithms.
2.5.1 Information gain for different priors
Let π(w |c) be a prior over world states andLideal
π (w |u,c) the corresponding ideal listener.
The prior entropy in contextc is
Hπ
prior(c) := −
∑
w∈W
π(w|c) log π(w|c), (23)
and the entropy of the ideal posterior after observingu is
Hπ
post(u,c) := −
∑
w∈W
Lideal
π (w|u,c) log Lideal
π (w|u,c). (24)
The information gainfor the pair(u,c) is then
IGπ(u,c) := Hπ
prior(c) −Hπ
post(u,c). (25)
For a binary world,IGπ(u,c) is bounded above bylog 2 ≈0.693 nats (one bit) when the prior
is symmetric. Averaging over messages drawn fromp(u |c) recovers a conditional mutual
information: Ep(u|c)[IGπ(u,c)] = Iπ(W; U |c).
In the empirical analysis we will use two instances:
IGprior(u,c) := IGπprior(u,c), (26)
IGunif(u,c) := IGπunif (u,c), (27)
corresponding to ideal listeners that do and do not incorporate block priors, respectively.
Comparing these quantities lets us separate the contribution of context-dependent priors from
the contribution of latent geometry to uncertainty reduction.
2.5.2 Pragmatic loss as a KL divergence between listeners
To quantify how closely an actual listener tracks a given ideal listener, we define apragmatic
loss as a Kullback–Leibler divergence between their posteriors.1
Let Lideal(w|u,c) be a chosen ideal listener andL0(w|u,c) an actual listener. The trial-wise
pragmatic loss is
ℓ(u,c; Lideal,L0) := KL
(
Lideal(·| u,c) ∥L0(·| u,c)
)
=
∑
w∈W
Lideal(w|u,c) log Lideal(w|u,c)
L0(w|u,c) .
(28)
Standard properties of the KL divergence guarantee thatℓ(u,c; Lideal,L0) ≥0, with equality if
and only if the two listeners agree exactly on all states. Because this loss is a KL divergence, its
unit and scale are fixed by information theory: there is no extra slope parameter that could be
tuned to absorb systematic discrepancies between ideal and actual posteriors.
1We use “loss” purely in an information-theoretic sense; no claim about psychological cost or reward is implied.
10
2.5.3 A four-pattern family of pragmatic losses
The reviewer concerns that motivated the present work centred on the role of block priors. In
the original analysis we compared a single ideal listener thatdid see block priors to a single
actual listener thatdid not, so that the resulting pragmatic loss conflated three effects:
1. the influence of block priors on the ideal listener;
2. the omission of these priors from the decoder; and
3. any mismatch between latent geometry and the logistic model class.
To disentangle these contributions we consider a2 ×2 family of listener pairs obtained by
crossing prior choice (block vs. uniform) on the ideal side with prior access (prior-agnostic vs.
prior-aware) on the actual side. This yields four pragmatic losses:
ℓ1(u,c) := ℓ
(
u,c; Lideal,prior,Lagn
0
)
, (29)
ℓ2(u,c) := ℓ
(
u,c; Lideal,prior,Lprior
0
)
, (30)
ℓ3(u,c) := ℓ
(
u,c; Lideal,unif,Lagn
0
)
, (31)
ℓ4(u,c) := ℓ
(
u,c; Lideal,unif,Lprior
0
)
. (32)
The first quantity,ℓ1, coincides with the pragmatic loss analysed in the earlier version of the
preprint: it compares a prior-aware ideal listener to a prior-agnostic decoder. The additional
three losses serve as controls:
• ℓ2 compares an ideal listener and a decoder thatboth receive block priors. Because the two
listeners share the same prior information, residual loss inℓ2 can be attributed to geometric
and functional mismatches (e.g. the restricted form of the logistic decoder) rather than to
an unfair withholding of context.
• ℓ3 compares an ideal listener and a decoder that both ignore block priors. Here the
focus is on how well a prior-agnostic decoder can approximate a prior-agnostic ideal; this
emphasises the role of latent geometry alone.
• ℓ4 compares a uniform-prior ideal to a prior-aware decoder. This configuration asks how
much additional divergence is introduced when the decoder is more sensitive to block priors
than the ideal listener itself. Although less central thanℓ1 and ℓ2, it provides a useful
“mirror image” that helps to diagnose asymmetries between priors on the ideal and actual
sides.
By examining how session- and context-wise averages ofℓ1,...,ℓ 4 co-vary with decoding
accuracy and information gain, we will be able to ask more precise questions than in the original
analysis. For example, a largeℓ1 together with a smallℓ2 would indicate that most of the
apparent pragmatic loss arises from denying priors to the decoder, whereas a largeℓ2 would
point instead to a genuine geometric or modelling mismatch that persists even when priors are
fairly shared.
In summary, the theoretical framework provides (i) a principled route from EFE-based
control to a pragmatic speaker law, (ii) a latent geometric representation that supports ideal
and actual listeners, and (iii) a family of information-theoretic diagnostics — including the four
pragmatic losses in(29)–(32) — that quantify how effectively information present in a sender
population could in principle be used by particular classes of downstream decoders. The next
section describes how these constructions are instantiated on Neuropixels recordings from the
IBL visual decision-making task.
11
3 Methods
All analyses were implemented in Python using a dedicated packagefep_geo_4-pattern, which
wraps the public data interface of the International Brain Laboratory (IBL), standard scientific
libraries (NumPy, SciPy, pandas, scikit–learn), and custom code for latent geometry, listeners,
and information-theoretic metrics. Conceptually, each probe insertion is treated as asender
population that emits messages in response to task variables; these messages are embedded in
a latent geometric space and interpreted by ideal and actual listeners. The central high-level
object is aSessionAnalysis class that runs the entire pipeline for a single Neuropixels probe
insertion and returns aSessionResult dataclass. Multi-session analyses iterate this pipeline
over a fixed list of probe IDs and aggregate the resulting tables and figures.
3.1 Dataset, sessions, and data access
We analysed Neuropixels recordings from mice performing the IBL visual contrast discrimination
task, using the publicly released spike-sorting benchmark probe insertions. [9, 10] Each insertion
is identified by a probe UUID (PID) in the IBL database. In the code, these PIDs are stored in
a configuration listMULTI_SESSION_TAGS, and the multi-session entry point simply loops over
this list, running the full pipeline for each tag.
Data are fetched on demand from the public IBL OpenAlyx server via the officialONE Python
client. For a given PID, a helper functionload_raw_session_from_pid retrieves
• spike-sorting output: spike times and cluster IDs (spikes), cluster metadata including
quality labels and anatomical acronyms (clusters), and channel information (channels);
• trial-wise task variables from thetrials object, including signed left and right contrasts
(contrastLeft, contrastRight), stimulus onset times (stimOn_times), block priors over
left choice (probabilityLeft), and choices (choice).
Theseobjectsarewrappedina RawSessionData dataclassandstoredinsidethe SessionResult
so that all downstream analyses can be reproduced without re-downloading the data.
3.2 World states, contexts, and valid trials
For each trial we define a binary world statew∈{0,1}, a contextc, and a valid-trial mask.
World state. Let cL and cR denote the signed Michelson contrasts on the left and right
monitors provided by the IBL task. We treat missing values as zero (no stimulus on that side)
and work with absolute values˜cL = |cL|, ˜cR = |cR|. A raw label is
world_rawi := sign
(
˜cR,i −˜cL,i
)
, (33)
so that trials with a stronger right contrast(˜cR > ˜cL) are labelled “Right” and encoded as
wi = 1, whereas trials with a stronger left contrast(˜cL >˜cR) are labelled “Left” and encoded as
wi = 0. Trials with zero contrast on both sides or non-zero contrast on both sides are excluded
by the valid-trial mask described below.
Context and block prior. The IBL task is structured into blocks with different priors on
the side of the higher contrast. We define the contextci for triali implicitly via the block prior
over the left side,
P(Left |ci) = probabilityLefti ∈{0.2,0.5,0.8}. (34)
In the code, the vector of per-trial block priors is stored aspriors_left.
12
Valid trials. The helper functionselect_valid_trials applies three inclusion criteria:
1. exactly one side has a non-zero stimulus:(|˜cL|>0)⊕(|˜cR|>0), where⊕denotes exclusive
OR;
2. the animal makes a left or right choice:choice ∈{−1,+1}(no-go trials withchoice = 0
are excluded);
3. the stimulus onset timestimOn_times is finite and not NaN.
The function returns a boolean mask over original trial indices, the vector of world labelswi for
valid trials (encoded as0 for Left and1 for Right), the corresponding valid stimulus onset times,
and the block priorsP(Left |ci).
Behaviouralcorrectness. Giventhevalid-trialmaskandworldlabels, behaviouralcorrectness
is computed by comparing the animal’s choice to the true side. In terms of the IBL choice
variable, which encodes left as−1 and right as+1, we define
correcti =
{
1, if (choicei = −1 ∧wi = 0) or (choicei = +1 ∧wi = 1),
0, otherwise. (35)
For robustness to possible sign inversions in the source dataset, the implementation evaluates
correctness under both sign conventions(−1,+1) = (Left,Right) and (−1,+1) = (Right,Left)
and adopts the convention that yields the higher fraction of correct trials. The resulting0/1
vector is stored asis_correct and later used to relate neural metrics to behaviour.
3.3 Peri-stimulus windows, spike counts, and unit selection
For each valid triali we define a peri-stimulus analysis window aligned to stimulus onset,
[t(i)
start,t(i)
end] = [t(i)
stim + t0, t(i)
stim + t1], (36)
where t(i)
stim is the stimulus onset time and(t0,t1) is specified in aSessionConfig object as
time_window. In the main analysis we used(t0,t1) = (0.0,0.2) s, i.e. we counted spikes from
stimulus onset to200 ms after onset. A later window(0.1,0.3) s was used in robustness checks.
Spikecountsarecomputedusing brainbox.population.decode.get_spike_counts_in_bins.
Let Ntrials denote the number of valid trials andNunits the number of candidate units on the
probe. For each clusterj and triali we define
Xij := #{spikes of unitj with times in[t(i)
start,t(i)
end]}. (37)
These counts are assembled into a trial-by-neuron matrixX ∈RNtrials×Nunits with rows indexed
by trials and columns by units.
Unit selection follows IBL quality annotations. The helper functionbuild_spike_matrix
restricts the analysis to “good” clusters,clusters["label"] == 1. Optionally, a region filter
can be applied by requiring the anatomical acronymclusters["acronym"] to match a specified
region (e.g. "VISp"); this is controlled by theregion field of SessionConfig. In the main
analysis we setregion = None and used all good units on each probe.
Very weakly active units are removed by thresholding the total spike count per unit,
Sj :=
Ntrials∑
i=1
Xij, (38)
and discarding units withSj <min_total_spikes (default 5). After this filtering, the columns
of X correspond to the final set of analysed units. The numbers of trials and neurons after
filtering are stored asn_trials and n_neurons.
13
3.4 Train–test split
To evaluate discriminative listeners fairly, we split trials into training and test sets. For each
session we construct an index vectori = (1,...,N trials) and perform a stratified train–test split
using sklearn.model_selection.train_test_split:
(idx_train,idx_test) = train_test_split(i, w, test_size = 0.2, random_state = 0, stratify = w),
(39)
so that class proportions are preserved in both splits. The resulting index arrays are stored
in the latent-geometry object and reused both for fitting logistic decoders and for computing
“test-only” communication metrics.
3.5 Latent geometry via factor analysis
The spike-count matrixX is typically high-dimensional. We assume that trial-to-trial variabil-
ity is driven by a lower-dimensional latent code plus noise and use factor analysis (FA) for
dimensionality reduction.
FA model. For each triali we posit a latent vectorzi ∈Rd and model
zi ∼N(0,Id), (40)
xi |zi ∼N(Wzi + µ, Ψ), (41)
where xi is theith row ofX, W ∈RD×d is a loading matrix,µ∈RD is a mean vector,Ψ is a
diagonal covariance, andD is the number of retained units.
Fitting and latent codes. We fit FA usingsklearn.decomposition.FactorAnalysis. The
target latent dimensionality is specified bySessionConfig.n_latent (default 10), but to avoid
degenerate cases we constrain
d= min
(
n_latent, max(1,D −1)
)
, (42)
so that we never request more latent dimensions thanD−1.
In the main analysis we fit FA only on training trials (fa_fit_mode = "train") and
then project all valid trials into latent space. Concretely, we fit the FA model onXtrain =
X[idx_train], then callfa.transform(X) to obtain posterior meanszi = E[zi |xi] for every
trial. Stacking these vectors yields a latent code matrix
Z ∈RNtrials×d, (43)
with oned-dimensional point per trial. Estimating the geometry on training data only prevents
information from test trials leaking into the representation used for fairness analyses.
Gaussian prototypes and shared covariance.Each trialihas a latent codezi and a world
label wi ∈{0,1}. To connectZ to the Gaussian geometry in the theoretical framework, we
assume a Gaussian prototype model with a shared covariance.
For each world statew we compute the empirical mean
ˆµw := 1
Nw
∑
i:wi=w
zi, (44)
where Nw is the number of trials with world labelw. These means are stored asmu_left (for
w= 0) andmu_right (for w= 1).
14
We estimate a single covariance by pooling all latent codes,
Σ := 1
Ntrials −1
Ntrials∑
i=1
(zi −¯z)(zi −¯z)⊤, (45)
where ¯z is the global mean. For numerical stability we add a small ridge term λId with
λ= gaussian_reg ≈10−6 before inversion and store the inverse covarianceˆΣ−1. For each trial
i and statew we then define a squared Mahalanobis distance
d2
w(zi) := (zi −ˆµw)⊤ˆΣ−1(zi −ˆµw), (46)
which plays the role of a geometric energy in the ideal listeners.
Ideal-geometry estimation and cross-validation. In the main analyses, the Gaussian
prototypes ˆµw and the shared covarianceΣ are estimated from all valid latent codeszi, i.e.
from both training and test trials. This asymmetric treatment of the encoder and the Gaussian
geometry is deliberate. The factor-analysis encoder is fitted on training trials only (see above),
so that the mapping from spike counts to latent codes can be evaluated fairly on held-out data.
By contrast, the latent Gaussian geometry underlying the ideal listener is used as anormative
benchmark rather than as a predictive model whose out-of-sample generalisation error we wish
to estimate. Using all available trials to estimateˆµw and Σ therefore stabilises the geometry
and the corresponding ideal posteriors, while generalisation performance is assessed exclusively
for the logistic “actual” listeners via the train–test split. Consistent with this interpretation,
Section 4.5 shows that recomputing information gain and pragmatic losses on held-out test trials
only yields session-level summaries that are almost indistinguishable from the all-trial values.
3.6 Ideal listeners with and without block priors
Given the latent geometry and block priors, we define two ideal listeners in latent space: a
prior-aware ideal listenerLprior
ideal that uses the empirical block priorP(w|ci), and a prior-agnostic
ideal listenerLunif
ideal that uses a uniform prior over world states.
Prior-aware ideal listener. We treat the class-conditional density in latent space as
p(zi |w) ∝exp
(
−1
2 d2
w(zi)
)
(47)
and combine this with the per-trial priorP(w|ci). The prior-aware ideal posterior is
Lprior
ideal(w|zi,ci) =
exp
(
−1
2 d2
w(zi) + logP(w|ci)
)
∑
w′∈{0,1}exp
(
−1
2 d2
w′ (zi) + logP(w′|ci)
). (48)
In the implementation this computation is vectorised over all trials and returns a posterior
matrix of shape(Ntrials,2) together with log-posteriors and maximum-a-posteriori (MAP) labels.
The MAP labels and true world labels are used to compute the “ideal” decoding accuracy for
each session.
Uniform-prior ideal listener. To probe the role of block priors, we also define an ideal
listener that combines the same Gaussian likelihood with a symmetric prior over world states.
Let Punif(w= 0) = Punif(w= 1) = 1/2. The uniform-prior ideal posterior is
Lunif
ideal(w|zi) =
exp
(
−1
2 d2
w(zi)
)
∑
w′∈{0,1}exp
(
−1
2 d2
w′ (zi)
). (49)
This listener ignores block structure altogether and reflects only the latent geometry. Its
posteriors are used to define a second information-gain metric and two additional pragmatic
losses (Section 3.8).
15
3.7 Actual listeners: prior-agnostic and prior-aware logistic decoders
The actual listeners represent how some downstream process (another neural population, a
behavioural controller, or a fitted decoder) in fact uses the latent code. In this paper we
instantiate them as logistic regression classifiers from latent codes to world labels, with and
without access to block priors.
Prior-agnostic logistic listenerLagnostic
0 . The main actual listener uses only the latent code
zi as input. Using the train–test split defined above, we fit a logistic regression classifier fromZ
to w:
1. wetraina sklearn.linear_model.LogisticRegression onZ[idx_train] andw[idx_train];
2. we compute test accuracy onZ[idx_test] and w[idx_test];
3. we evaluate class probabilities for all trials and reorder columns so that the first column
always corresponds tow= 0 (Left) and the second tow= 1 (Right).
We use the standardℓ2-penalised logistic regression implementation with inverse regularisation
strength C = 1.0, solver"lbfgs", maximum iterations1000, and class_weight=None. The
resulting probability matrix is stored asLagnostic
0 and defines the prior-agnostic actual listener.
Prior-aware logistic listenerLprior
0 . To model an actual listener that has explicit access
to block priors, we augment the latent code with a scalar feature encoding the prior over left
choices. For each trial we compute a clipped log-odds
ϕ(ci) := log
˜P(Left |ci)
1 −˜P(Left |ci), (50)
where ˜P(Left |ci) is a version ofP(Left |ci) clipped away from{0,1}for numerical stability.
The augmented feature vector is then
˜zi =
[
z⊤
i , ϕ(ci)
]⊤. (51)
A second logistic regression with the same hyperparameters is fitted on˜Z[idx_train] and
w[idx_train], yielding a prior-aware listenerLprior
0 (w|zi,ci) with an associated test accuracy.
When present,Lprior
0 is used to define additional pragmatic losses (Section 3.8) and a “prior-
control” summary.
3.8 Information gain and four pragmatic losses
Given the ideal and actual listeners, the package computes trial-wise information gain and four
variants of pragmatic loss. All quantities are measured in nats.
Information gain. For each triali the prior entropy under the block prior is
Hprior(ci) := −
∑
w∈{0,1}
P(w|ci) logP(w|ci), (52)
and the posterior entropy under the prior-aware ideal listener is
Hprior
post (i) := −
∑
w∈{0,1}
Lprior
ideal(w|zi,ci) logLprior
ideal(w|zi,ci). (53)
The prior-aware information gain is then
IGi := Hprior(ci) −Hprior
post (i), (54)
16
which quantifies how much the ideal listener with access to block priors reduces its uncertainty
about w upon observing the messageui.
Analogously, for the uniform-prior ideal listener we define the prior entropy
Hunif := −
∑
w∈{0,1}
Punif(w) logPunif(w) = log 2, (55)
and the posterior entropy
Hunif
post(i) := −
∑
w∈{0,1}
Lunif
ideal(w|zi) logLunif
ideal(w|zi). (56)
The uniform-prior information gain is
IGunif
i := Hunif −Hunif
post(i). (57)
Intheimplementationthesequantitiesarecomputedbyahelperfunction compute_information_gain,
which operates on matrices of ideal posteriors and vectors of priors, clips probabilities away from
{0,1}for numerical stability, and returns trial-wise arraysIG and IG_uniform.
Four pragmatic losses. To quantify how closely actual listeners track each ideal listener, we
define four per-trial Kullback–Leibler divergences, following the definitions in the theoretical
framework.
Let p(w) and q(w) denote two Bernoulli distributions overw ∈{0,1}. The discrete KL
divergence is
KL(p∥q) :=
∑
w∈{0,1}
p(w) log p(w)
q(w). (58)
Using this, for each triali we define
ℓ(1)
i = KL
(
Lprior
ideal(·| zi,ci)
Lagnostic
0 (·| zi)
)
, (59)
ℓ(2)
i = KL
(
Lprior
ideal(·| zi,ci)
Lprior
0 (·| zi,ci)
)
, (60)
ℓ(3)
i = KL
(
Lunif
ideal(·| zi)
Lagnostic
0 (·| zi)
)
, (61)
ℓ(4)
i = KL
(
Lunif
ideal(·| zi)
Lprior
0 (·| zi,ci)
)
. (62)
We refer to these as the fourpragmatic losses. Intuitively: ℓ(1) and ℓ(3) measure how far a
prior-agnostic decoder is from prior-aware and uniform-prior ideals, respectively, whereasℓ(2)
and ℓ(4) measure how far a prior-aware decoder is from those same ideals.
In the code, these divergences are computed by a helpercompute_pragmatic_loss, which
first clips both arguments to the range[ε,1 −ε] with ε= metrics_eps ≈10−9 to avoid infinities,
then applies the vectorised KL formula. The results are bundled into aTrialMetrics dataclass
with fields
• IG — the prior-aware information gain(IGi);
• loss — ℓ(1)
i = KL
(
Lprior
ideal∥Lagnostic
0
)
;
• loss_prior — ℓ(2)
i = KL
(
Lprior
ideal∥Lprior
0
)
;
• IG_uniform — the uniform-prior information gain(IGunif
i );
• loss_uniform — ℓ(3)
i = KL
(
Lunif
ideal∥Lagnostic
0
)
;
• loss_uniform_prior — ℓ(4)
i = KL
(
Lunif
ideal∥Lprior
0
)
.
17
3.9 Session- and context-level summaries
For each session–probe pair, a reporting function generates summary tables and figures from the
SessionResult object.
Session-wise summaries. A per-session summary tablesummary_all.csv contains one row
per insertion with the session ID, probe name, region (if specified), numbers of trials and neurons,
latent dimensionality, ideal decoding accuracy (all trials and test-only), actual (prior-agnostic)
test accuracy, and the mean and standard deviation ofIGi and ℓ(1)
i across trials. Additional
session-wise tables record the mean and standard deviation of the other three pragmatic losses
and their test-only counterparts when needed.
Context-wise summaries. To examine dependence on block priors, trials are grouped by
block identity within each session, and block-wise means and standard deviations ofIGi and
ℓ(1)
i are computed. Aggregating these across sessions with trial-weighted averages yields a table
by_context_all.csv with one row per distinct priorP(Left |c).
Test-only metrics. Using the train–test split, we recomputeIGi and ℓ(1)
i on held-out test
trials only. For each session, a tabletest_only_summary.csv records the actual test accuracy,
and the mean and standard deviation ofIGi and ℓ(1)
i on test trials, together with the number of
test trials. Analogous test-only summaries can be generated forℓ(2)–ℓ(4) if required.
Prior-control summary. When the prior-aware listenerLprior
0 is fitted, a table
prior_control_summary.csv summarises, for each session, the test accuracy of the prior-aware
decoder and the mean and standard deviation ofℓ(2)
i ; this quantifies how much of the pragmatic
loss in biased blocks can be explained by withholding block priors from the main actual listener.
Behavioural link. Finally, a tablebehaviour_link.csv links neural communication metrics
to behavioural performance. For each session it records behavioural accuracy (fraction of correct
choices across valid trials) and the mean information gain and pragmatic losses separately for
correct and error trials. These summaries are used in the Results section for exploratory analyses
of how neural communication efficiency relates to behaviour.
3.10 Robustness analyses
The package includes convenience commands for robustness analyses over latent dimensionality,
time window, and decoder hyperparameters. For each configuration in a small grid (e.g.
d∈{5,10,20}and (t0,t1) ∈{(0.0,0.2),(0.1,0.3)}), the full multi-session pipeline is rerun and
cross-session means of decoding accuracies, information gains, and pragmatic losses are written to
a grid summary file. Additional runs vary the logistic regression hyperparameters (regularisation
strength C and class weighting). In practice, these manipulations only change numerical values
slightly and do not alter the qualitative relationships between ideal and actual listeners, the four
pragmatic losses, or their dependence on context.
4 Results
4.1 Toy example: separating information gain and pragmatic loss
Before turning to the neural data, we first illustrate the geometric listener construction on a
simple toy problem (Section 2). In this setting, we explicitly control the mapping from stimuli
to latent states, the block-wise priors over left vs. right choices, and the noise corrupting neural
18
responses. This allows us to visualise how information gain and pragmatic loss behave under
four different combinations of ideal and actual listeners: (i) a prior-aware ideal listener compared
to a prior-agnostic actual listener (ℓ1), (ii) a prior-aware ideal listener compared to a prior-aware
actual listener (ℓ2), (iii) a uniform-prior ideal listener compared to a prior-agnostic actual listener
(ℓ3), and (iv) a uniform-prior ideal listener compared to a prior-aware actual listener (ℓ4).
In the toy model, the mutual information between the block identity and the latent state
increases monotonically as the latent clusters become more separated (Fig. 1, left). When we
quantify pragmatic loss under the four listener pairings, we observe two qualitatively distinct
behaviours (Fig. 1, right). First, losses that compare a prior-aware ideal listener to a prior-
agnostic actual listener (ℓ1 and ℓ4) grow large whenever the block priors are strongly biased,
reflecting a mismatch in the priors rather than a failure of the actual listener to use information
in the latent representation. Second, losses that compare ideal and actual listeners using the
same prior (ℓ2 and ℓ3) remain small over a broad range of latent separations, and increase only
when the ideal posterior becomes extremely peaked and therefore more difficult to approximate.
This toy example motivates our strategy in the neural data: we will use the four loss patterns
to disentangle contributions from prior mismatch and from genuine information-processing
inefficiency.
(a) Information gain as a function of latent cluster
separation.
(b) Pragmatic loss for four ideal–actual listener
pairings.
Figure 1: Toy example illustrating the geometric listener framework. Panel (a) shows how infor-
mation gain increases with the separation of latent clusters. Panel (b) shows the corresponding
behaviour of pragmatic loss under four combinations of prior-aware vs. prior-agnostic ideal and
actual listeners.
4.2 Single-session geometry and decoding performance
We first examine a representative session (session IDfece187f-b47f-4870-a1d6-619afe942a7d,
probe 01) to illustrate the geometry of the learned latent space and the associated listener
behaviour. This session contained1333 trials and214 simultaneously recorded neurons. The
factor-analysis encoder was trained with a 10-dimensional latent space and a fixed post-stimulus
time window (0–200ms), and the logistic decoder was trained on the same latent representation.
Projecting the neural population responses into the latent space reveals well-separated clusters
corresponding to the combinations of block identity and stimulus category (Fig. 2, left). The
ideal listener defined on this latent representation achieved an overall decoding accuracy of0.854
on all trials and0.846 on the held-out test set, while the actual logistic decoder achieved0.876
accuracy on both all trials and test trials. In the same session, the animal’s behavioural accuracy
19
was 0.947 (error rate0.053), indicating that both ideal and actual listeners recover the world
state (which side has higher contrast) reliably, but still fall short of the animal’s performance in
some sessions.
Trial-level information gain in this session was substantial: the mean information gain for the
prior-aware ideal listener was0.199 nats (standard deviation0.186 nats), and the corresponding
quantity for a uniform-prior ideal listener was0.210 nats (standard deviation0.128 nats). Thus,
most of the information about the world state is carried by the likelihood function rather than
by the prior.
For the original pragmatic loss that compares a prior-aware ideal listener to a prior-agnostic
actual listener (ℓ1), the mean loss in this session was0.098 nats (standard deviation0.177 nats).
When we instead compare the prior-aware ideal listener to a prior-aware actual listener (ℓ2), the
mean loss drops to0.038 nats. Comparing a uniform-prior ideal listener to a prior-agnostic actual
listener (ℓ3) yields a mean loss of0.052 nats, whereas comparing a uniform-prior ideal listener to
a prior-aware actual listener (ℓ4) yields0.081 nats. In other words, in this representative session
the majority of the original lossℓ1 can be attributed to a mismatch between the priors used by
the ideal and actual listeners; once the actual listener is made prior-aware, the residual lossℓ2
becomes considerably smaller.
The joint distribution of information gain and pragmatic loss in this session shows that trials
with high information gain tend to have small loss, and vice versa (Fig. 2, right). This negative
relationship is most pronounced forℓ1 and ℓ4, which include a prior mismatch, whereas the
distributions ofℓ2 and ℓ3 are narrower and closer to zero. These observations anticipate the
cross-session analyses below.
(a) Latent geometry for one session.
(b) Trial-level information gain vs.ℓ1 pragmatic
loss.
(c) ℓ2
 (d) ℓ3
 (e) ℓ4
(f) Histogram of ℓ1 over
trials.
Figure 2: Single-session illustration of the geometric listener. Panels show the latent representa-
tion, the relationship between information gain and pragmatic loss for four listener pairings, and
the distribution ofℓ1 across trials for a representative session.
20
4.3 Across-session distribution of information gain and pragmatic loss
We next aggregated results across all 13 session–probe combinations. Each session contained
between 367 and 1333 trials (mean652 ±244) and between77 and 353 neurons (mean180 ±82),
for a total of8476 trials. Ideal decoding accuracy on all trials ranged from0.752 to 0.854, with a
mean of0.785 ±0.028. The actual logistic decoder achieved accuracies between0.495 and 0.876
across sessions, with a mean of0.676 ±0.116.
Trial-averaged information gain for the prior-aware ideal listener (IG) varied from0.006
to 0.199 nats per trial, with a cross-session mean of0.086 ±0.060 nats. The corresponding
measure for a uniform-prior ideal listener (IGuniform) was similar (0.082 ±0.061 nats), reflecting
the fact that most information about which side has higher contrast is already conveyed by
the observation likelihood. When we weight each session by its number of trials, the overall
mean information gain is0.098 nats for the prior-aware ideal listener and0.094 nats for the
uniform-prior ideal listener.
Across sessions, the original pragmatic lossℓ1 (prior-aware ideal vs. prior-agnostic actual)
had a mean of0.130 ±0.023 nats and ranged from0.098 to 0.162 nats. The prior-matched lossℓ2
(prior-aware ideal vs. prior-aware actual) was much smaller, with a mean of0.019 ±0.012 nats.
The lossℓ3 (uniform-prior ideal vs. prior-agnostic actual) had a mean of0.015 ±0.017 nats, while
ℓ4 (uniform-prior ideal vs. prior-aware actual) had a mean of0.125 ±0.045 nats. Trial-weighted
averages were very similar:ℓ1 = 0.128 nats, ℓ2 = 0.021 nats, ℓ3 = 0.019 nats andℓ4 = 0.123 nats.
These metrics exhibited systematic relationships across sessions (Fig. 3). Actual decoding
accuracy was strongly and positively correlated with information gain (Pearsonr = 0.96 for
IG and r = 0.95 for IGuniform). As expected from the geometric construction, sessions with
larger information gain tended to have smaller prior-mismatched lossesℓ1 and ℓ4 (correlations
r= −0.95 and r= −0.85, respectively), indicating that when the latent representation carries
more information, the cost of ignoring the block prior becomes more severe and thus more
clearly revealed. By contrast, the prior-matched lossesℓ2 and ℓ3 were positively correlated with
information gain (r= 0.91 and r= 0.90). This behaviour is consistent with the toy model: as
IG increases, ideal posteriors become more peaked, making it harder for any smooth decoder
(even a prior-aware one) to exactly match the ideal listener; residual deviations therefore grow
with IG, but remain numerically small.
Taken together, these results suggest that the originally reported lossℓ1 is dominated by the
misalignment between the priors used by the ideal and actual listeners. Once the actual listener
is allowed to be prior-aware and/or the ideal listener is redefined under a uniform prior, the
residual lossesℓ2 and ℓ3 are an order of magnitude smaller, and are therefore more appropriate
as measures of biological information-processing inefficiency.
21
(a) Ideal vs. actual decoding accuracy across ses-
sions.
 (b) Session-averaged IG vs.ℓ1.
(c) IG vs.ℓ2.
 (d) IG vs.ℓ3.
 (e) IG vs.ℓ4.
Figure 3: Cross-session summary of decoding accuracy, information gain, and four versions of
pragmatic loss. Each point corresponds to a session–probe pair.
4.4 Block-wise priors, information gain, and loss patterns
The experimental paradigm comprises three block types with different priors on left vs. right
choices: P(Left) ∈{0.2,0.5,0.8}. We therefore asked how information gain and pragmatic losses
depend on these block-wise priors. For each session and block type, we computed trial-averaged
metrics and then aggregated them across sessions with trial-based weights.
Information gain for the prior-aware ideal listener was higher in biased blocks than in the
unbiased block. The trial-weighted mean IG was0.101 nats for blocks withP(Left) = 0 .2
and 0.103 nats for blocks withP(Left) = 0.8, compared to0.065 nats for the unbiased blocks
(P(Left) = 0.5). The corresponding values for the uniform-prior ideal listener were0.096 nats (0.2
blocks), 0.100 nats (0.8 blocks), and0.065 nats (0.5 blocks). Thus, biased priors are associated
with slightly higher information gain, independent of whether we evaluate the ideal listener
under the true block prior or a uniform prior.
The behaviour of the four pragmatic losses across block types is more revealing (Fig. 4). For
the original lossℓ1, the mean trial-weighted value was0.165 nats in0.2 blocks and0.125 nats in
0.8 blocks, but only0.014 nats in unbiased blocks. A similar asymmetry was visible forℓ4, which
reached 0.131 nats in0.2 blocks and0.144 nats in0.8 blocks, versus0.015 nats in unbiased blocks.
By contrast, the prior-matched losses were small and nearly independent of the block prior:ℓ2
took values of0.024 nats (0.2 blocks), 0.015 nats (0.5 blocks), and0.019 nats (0.8 blocks), while
ℓ3 was 0.017 nats, 0.014 nats, and0.022 nats for the three block types.
These results confirm the concern raised in the initial review: the large values of the original
pragmatic loss in biased blocks are largely a consequence of comparing a prior-aware ideal listener
to a prior-agnostic actual listener. When this prior mismatch is removed, the residual lossesℓ2
22
and ℓ3 remain modest and show only weak dependence on the block prior. We therefore interpret
ℓ1 and ℓ4 primarily as measures of prior mismatch, whereasℓ2 and ℓ3 quantify the “intrinsic”
inefficiency of the neural listener relative to its ideal counterpart under matched priors.
(a) IG andℓ1 by block prior.
 (b) ℓ2 by block prior.
(c) ℓ3 by block prior.
 (d) ℓ4 by block prior.
Figure 4: Dependence of information gain and four pragmatic losses on block-wise priors. Bars
show trial-weighted averages across all sessions.
4.5 Test-only metrics and prior-aware control analyses
To ensure that our metrics were not driven by overfitting, we recomputed information gain and
pragmatic losses on held-out test trials only. Across sessions, the mean test accuracy of the
actual decoder was0.676 ±0.116, effectively identical to the accuracy reported on all trials. The
session-averaged information gain on test trials was0.081 ±0.059 nats; trial-weighted across
sessions, IG on test trials was0.094 nats, compared to0.098 nats when all trials were used. For
the four pragmatic losses, the trial-weighted test-only means wereℓ1 = 0.132 nats, ℓ2 = 0.020 nats,
ℓ3 = 0.019 nats, andℓ4 = 0.127 nats, again almost indistinguishable from the all-trial values.
Thus, both information gain and the four loss patterns are stable under cross-validation.
We next examined a stricter prior-aware control in which the logistic decoder was trained
with block identity explicitly available, so that the actual listener could in principle implement
the same prior as the ideal listener. Under this control, the trial-weighted decoding accuracy
increased from0.699 on the original test-only decoder to0.801 when the prior was available.
The corresponding trial-weighted prior-matched losses wereℓ2 = 0.021 nats andℓ4 = 0.123 nats,
23
closely matching the values obtained in the main analysis. This demonstrates that giving the
decoder explicit access to the block prior improves its accuracy substantially, but leaves the
residual prior-matched lossℓ2 at a low and stable level. The remaining deviation between ideal
and actual listeners is therefore not an artefact of prior mismatch, but reflects genuine limits of
the decoder with respect to the information encoded in the latent representation.
Figure 5: Session-wise information gain and pragmatic loss on held-out test trials. Points
illustrate the stability of IG andℓ1 under cross-validation; analogous patterns hold forℓ2–ℓ4.
4.6 Relationship between neural listener and behaviour
Finally, we examined how neural information gain and pragmatic losses relate to the animal’s
trial-by-trial choices. For each session we divided trials into correct and error trials based on
behaviour, and recomputed the metrics separately for the two sets. Behavioural accuracy across
all sessions was high, with a trial-weighted mean of0.850.
Information gain was substantially larger on correct trials than on error trials. Trial-
weighted across sessions, IG on correct trials averaged0.105 nats, whereas IG on error trials
averaged0.041 nats, a difference of−0.064 nats (error minus correct). For the uniform-prior ideal
listener, IGuniform averaged 0.097 nats on correct trials and0.069 nats on error trials (difference
−0.028 nats). These differences indicate that the neural population carries more information
about the correct choice on trials in which the animal chooses correctly, consistent with the idea
that the latent representation supports the behaviourally relevant computation.
For the original pragmatic lossℓ1, the trial-weighted mean was0.124 nats on correct trials
and 0.157 nats on error trials, so that error trials exhibited about0.033 nats more loss than
correct trials. The prior-matched lossℓ2 showed the same pattern at a reduced scale (means
of 0.020 nats and0.029 nats on correct and error trials, respectively). Forℓ3, the ordering was
slightly reversed (means of0.019 nats on correct trials and0.015 nats on error trials), whereasℓ4
showed only a very small difference between correct and error trials (0.122 nats vs.0.123 nats).
Thus, both IG and the prior-aware lossℓ2 are informative about behavioural success: trials with
higher information gain and smaller prior-matched loss are more likely to be correct.
Across sessions, these relationships manifested as monotonic trends when plotting behavioural
accuracy against the four metrics (Fig. 6). Behavioural accuracy was positively associated with
24
IG and negatively associated with the prior-mismatched lossℓ1; similar but weaker trends
were observed for IGuniform and ℓ2. By contrast,ℓ3 and ℓ4 showed weaker and less consistent
relationships to behaviour. This pattern supports our interpretation that IG andℓ2 capture the
behaviourally relevant quality of the neural representation, whereasℓ1 and ℓ4 are dominated by
prior mismatch effects.
(a) Behaviour vs. IG.
 (b) Behaviour vs. IGuniform.
 (c) Behaviour vs.ℓ1.
(d) Behaviour vs.ℓ2.
 (e) Behaviour vs.ℓ3.
 (f) Behaviour vs.ℓ4.
Figure 6: Link between neural metrics and behaviour. Each point corresponds to a session–probe
pair, with behavioural accuracy on thex-axis and neural metrics on they-axis.
4.7 Robustness to hyperparameters of the latent model and decoder
Finally, we assessed the robustness of our conclusions with respect to key hyperparameters of
the latent-space model and decoder. Using a grid of models differing in the latent dimensionality
(5, 10, or 20 dimensions), the post-stimulus time window (0–200ms vs. 100–300ms), the
regularisation strength of the logistic decoder (inverse penaltyC ∈{0.1,1.0,10.0}), and the use
or absence of class weights, we recomputed decoding accuracy, information gain, and pragmatic
loss for each configuration and session (Fig. 7).
Varying the latent dimensionality had predictable but modest effects on performance. Aver-
aged over sessions and using the default decoder settings (0–200ms window,C = 1.0, no class
weights), ideal accuracy increased from0.778 to 0.785 to 0.795 as the latent dimensionality
increased from5 to 10 to 20. The corresponding actual test accuracies increased from0.641
to 0.676 to 0.706. Information gain rose from0.069 nats at 5 dimensions to0.104 nats at 20
dimensions, while ℓ1 decreased slightly from 0.136 nats to 0.130 nats. Thus, increasing the
representational capacity improves both IG and decoding accuracy, while leaving pragmatic loss
essentially unchanged.
Changing the time window from 0–200ms to 100–300ms (with 10 latent dimensions,C = 1.0,
no class weights) had more pronounced effects: ideal accuracy increased from0.785 to 0.814,
actual test accuracy from0.676 to 0.760, IG from0.086 nats to0.129 nats, and ℓ1 decreased
from 0.130 nats to0.122 nats. This suggests that later post-stimulus activity carries additional
information about the world state, and that the decoder can exploit this information to reduce
pragmatic loss.
25
By contrast, the choice of logistic regularisation and class weighting had minimal influence
on the aggregate metrics. For example, with 10 latent dimensions and the default time window,
varying C over {0.1,1.0,10.0}changed actual test accuracy only in the third decimal place
(from 0.680 to 0.676 to 0.677) and alteredℓ1 by less than0.001 nats. Introducing balanced class
weights changed actual test accuracy from0.676 to 0.672 and ℓ1 from 0.130 nats to0.132 nats.
Across all hyperparameter settings, the qualitative relationships between IG, the four losses, and
behaviour remained the same.
Overall, these robustness analyses indicate that our main conclusions are not sensitive
to reasonable choices of latent dimensionality, time window, or decoder regularisation. The
four-pattern decomposition of pragmatic loss into prior-mismatched (ℓ1, ℓ4) and prior-matched
(ℓ2, ℓ3) components, and the interpretation ofℓ2 and IG as biologically meaningful measures of
information use, are stable across the hyperparameter configurations we explored.
(a) Effect of latent dimensionality on IG andℓ1 for
a representative session.
(b) Effect of time window on IG andℓ1 for the
same session.
Figure 7: Illustrative examples of robustness to latent model and decoder hyperparameters.
Aggregate statistics across the full grid are reported in the main text.
5 Discussion
We developed and instantiated a geometric framework that links the Free Energy Principle
(FEP), probabilistic pragmatics, and neural population codes, and we applied it to large-scale
Neuropixels recordings from the International Brain Laboratory (IBL) visual decision-making
task. On the theoretical side, we showed how an EFE-based softmax over messages can be
reparametrised into a pragmatic speaker model whose log-odds decompose into contributions
from generative semantics, production cost, and expected information gain, once a small number
of modelling assumptions are made. On the empirical side, we embedded trial-wise spike-count
vectors into a latent geometric space, constructed ideal and actual listeners in that space, and
defined a family of information-theoretic diagnostics—information gain and four variants of
pragmatic loss—that quantify, in nats, how efficiently a particular listener exploits the information
available in a sender population.
In this section we first summarise what the data tell us about neural communication efficiency
when viewed through this geometric–pragmatic lens. We then discuss what is gained by the four-
pattern analysis of pragmatic loss (block-prior vs. uniform priors, prior-agnostic vs. prior-aware
actual listeners), and how this clarifies the role of task priors and modelling choices. Finally,
we consider methodological limitations, the relation to existing work on FEP and probabilistic
26
pragmatics, and possible extensions to other forms of biological communication.
5.1 Information gain and pragmatic loss as complementary efficiency metrics
The core listener-side quantities in our framework are the ideal information gainIG(u,c) and
the pragmatic lossesℓk(u,c) that quantify the mismatch between ideal and actual listeners on
a trial-by-trial basis. Because all of these are measured in nats, their numerical values can be
compared to simple information-theoretic benchmarks.
Across the 13 Neuropixels probe insertions, the latent Gaussian geometry combined with
block-wise task priors supported robust decoding of the binary world state by an ideal listener,
with session-wise accuracies typically around0.75–0.85, well above chance. The main prior-
agnostic logistic listener performed less well on average, with mean test accuracies closer to
0.65–0.70, leaving a gap of roughly ten percentage points in favour of the geometric ideal
(Section 4). This already suggests that, for this particular decoder class, the sender populations
carry more task-relevant information than is actually exploited.
The information gain statistics refine this picture. Session-wise meanIG(u,c) values were
consistently below one nat but clearly non-zero, with typical means on the order of∼0.1 nats
(about 0.13 bits) per trial. In a binary decision with a symmetric prior, the upper bound would
be log 2 ≈0.693 nats (one bit), corresponding to a perfectly informative message that always
reveals the world state. Our observed range therefore indicates that each message reduces
uncertainty for the ideal listener by roughly 10–20% of this theoretical maximum. A simple
Bernoulli thought experiment helps to calibrate this scale: a binary channel that reports the
true state with probability0.7 and flips it with probability0.3 achieves an expected single-trial
information gain of around0.08 nats. In this sense, the latent code on each probe behaves, for
the ideal listener, roughly like a moderately noisy but still useful Bernoulli observation.
Pragmatic loss is systematically larger. When measured as a Kullback–Leibler divergence
between the ideal listener with block priors and the prior-agnostic logistic listener, session-wise
mean losses typically fall in the range∼0.1–0.15 nats (about0.15–0.2 bits) per trial. In a binary
setting, a KL divergence of0.13 nats corresponds, for example, to the mismatch between a
Bernoulli(p) distribution withp≈0.75 and an approximate posterior stuck at the uninformative
prior of p = 0.5. Thus even when the latent code carries enough information to support a
reasonably confident ideal posterior, the actual listener can remain significantly more uncertain
or even biased in the wrong direction.
Importantly, information gain and pragmatic loss are not independent. Probes whose latent
geometry yields largerIG tend also to exhibit lower pragmatic loss and higher actual decoding
accuracy: when the sender population provides clearer geometric evidence about the world
state, the logistic listener both decodes more accurately and tracks the ideal posterior more
closely. Conversely, sessions with near-zero information gain also show near-chance decoding
and large losses. These cross-session relationships suggest thatIG(u,c) and theℓk(u,c) family
of losses jointly describe where a sender–listener pair sits along a continuum from efficient,
geometry-aligned communication to weak or misaligned communication, rather than providing
redundant statistics.
Conceptually, we therefore interpretIG(u,c) as a diagnostic of thepotential informativeness
of a message for a chosen ideal listener, and the pragmatic losses as diagnostics of how much
of that potential is realised by a specific class of actual listeners with particular constraints
(here, regularised logistic regression with or without access to block priors). Because the losses
are defined as KL divergences, their numerical scale is fixed by information theory rather than
by arbitrary slopes in a speaker law. This avoids a confound that would otherwise blur the
distinction between “weak pragmatic behaviour” and a merely “small loss penalty” in the
generative model.
27
5.2 What the four-pattern analysis reveals about priors and modelling choices
A central concern in the review process was that our original pragmatic loss ℓprior
agnostic =
KL(Lprior
ideal∥Lagnostic
0 ), computed between an ideal listener that receives block priors and an
actual listener that does not, might conflate biological inefficiencies with artefacts of this asym-
metric modelling choice. To address this, we extended the analysis to a full2 ×2 crossing of
priors and decoder types, yielding four pragmatic loss patterns:
1. ℓ1 = KL(Lprior
ideal∥Lagnostic
0 ): ideal listener with block priors vs. prior-agnostic logistic listener
(our original metric);
2. ℓ2 = KL(Lprior
ideal∥Lprior
0 ): ideal listener with block priors vs. prior-aware logistic listener;
3. ℓ3 = KL(Lunif
ideal∥Lagnostic
0 ): ideal listener with a uniform prior vs. prior-agnostic logistic
listener;
4. ℓ4 = KL(Lunif
ideal∥Lprior
0 ): ideal listener with a uniform prior vs. prior-aware logistic listener.
We also computed information gain for both the block-prior ideal (IGprior) and the uniform-prior
ideal (IGunif). This2×2 grid was implemented explicitly in the code (Section 3) and summarised
in the multi-session tables (Section 4).
This four-pattern analysis makes it possible to separate three distinct contributions to
pragmatic loss:
• a prior-mismatch component, arising when the ideal and actual listeners do not condition
on the same block priors;
• a geometric or representational component, arising when the actual decoder class (logistic
regression on latent codes plus optional prior feature) is too restricted to realise the ideal
latent-space posterior even when given the same priors;
• a residual component, capturing small numerical or estimation mismatches when both
priors and geometry are closely aligned.
The dominant empirical pattern is thatℓ1 is large and strongly context-dependent, whereas
ℓ2 and ℓ4 are uniformly small across blocks and sessions, andℓ3 occupies an intermediate regime.
More concretely:
• ℓ1 is largest in strongly biased blocks (P(Left |c) ∈{0.2,0.8}) and much smaller in
unbiased blocks (P(Left |c) = 0.5), mirroring our original observation that pragmatic loss
explodes precisely where block priors are extreme (Section 4.3).
• When we equip the actual listener with the same block prior feature as the ideal listener,
ℓ2 collapses to values close to zero in all blocks, and its cross-session mean is an order
of magnitude smaller than that ofℓ1 (Section 4.4). This shows that most of the large
pragmatic loss seen in biased blocks underℓ1 is indeed attributable to withholding priors
from the logistic decoder, rather than to a fundamental incompatibility between the latent
geometry and a linear decision boundary.
• The uniform-prior ideal,Lunif
ideal, serves as a complementary control. Underℓ3, the ideal
and the prior-agnostic logistic listener now agree on priors (both effectively behaving as
if P(Left) = 0.5), so differences reflect how the geometry interacts with the logistic class.
Empirically, ℓ3 remains clearly above zero, but it is less strongly modulated by block
identity thanℓ1. This suggests that there is a genuine representational gap between the
Gaussian prototype model and logistic decoding, over and above prior handling.
28
• Finally,ℓ4 is consistently small: when both the ideal and the prior-aware logistic listener
adopt the same uniform prior, the remaining KL divergence is negligible. In this sense,ℓ4
provides a lower-bound benchmark for how closely the logistic decoder can track the ideal
latent-space posterior when assisted by priors.
Taken together, these patterns support a more nuanced interpretation of pragmatic loss in this
dataset. The originalℓ1 was intentionally defined as a discrepancy between a block-prior ideal
and a prior-agnostic decoder, because this highlights the pragmatic consequences of withholding
priors from the listener. However, it would be misleading to interpret its raw magnitude as a
pure measure of “biological inefficiency”. The four-pattern analysis demonstrates that a large
fraction ofℓ1 in biased blocks is mechanistically attributable to this modelling choice: once the
decoder is given access to priors, the corresponding lossℓ2 becomes very small. What remains in
ℓ3 (and to a lesser extent inℓ2 and ℓ4) reflects the more interesting question of how the chosen
latent geometry and logistic decoder class deviate from the Gaussian ideal.
In light of this, we suggest the following interpretation:
• IGprior and IGunif describe, under two reasonable prior assumptions, how much uncertainty
reduction the sender population could, in principle, provide to a Bayesian listener living in
latent space.
• ℓ1 captures a worst-case discrepancy that emphasises the cost of omitting priors from the
decoder; it is useful as an upper bound on how much information a naive downstream
listener fails to use when priors are behaviourally relevant.
• ℓ2 and ℓ4 quantify how far a biologically plausible decodercould deviate from the latent-
space ideal once it is supplied with the same priors. Their small values argue that the
latent Gaussian assumptions and logistic class are jointly sufficient to capture most of the
structure relevant for this binary decision.
• ℓ3 isolates a geometry-vs.-decoder mismatch under a common uniform prior, and therefore
provides a cleaner probe of the representational adequacy of the chosen latent geometry
and decoder class.
From the perspective of the reviewer’s concern, this four-way decomposition shows that the
key claims of the paper do not stand or fall with any particular choice of “standard” actual
listener. Instead, the framework naturally accommodates multiple, complementary listener
models, and the collection{IGprior,IGunif,ℓ1,...,ℓ 4}allows us to attribute discrepancies to
priors, geometry, or decoder class in a controlled way.
5.3 Relation to FEP, probabilistic pragmatics, and reparameterisation invari-
ance
Our analysis was motivated by an EFE-based view of control, but the listener-side constructions
and metrics do not depend on a strong commitment to the FEP as a general theory. On the
FEP side, we follow standard derivations of variational free energy as a bound on surprise and of
expected free energyG(π) as a policy-selective functional. From there, we introduced a message-
level quantityG(u; w,c) and an associated softmax speakerS(u |w,c) ∝exp[−αG(u; w,c)],
which should be read as a modelling assumption that treats individual messages as one-step
policies, rather than as a unique algebraic consequence of the FEP.
The pragmatic parametrisation ofG(u; w,c) in terms of a literal listener, a production cost,
and an information-gain term further aligns this FEP-inspired score with familiar notions in
probabilistic pragmatics. A key theoretical contribution is the explicit treatment of the literal
listener’s normalisation: becauseLlit(w|u,c) depends both on a generative semanticsP(u|w,c)
and on a normalising constantZ(u,c), including log Llit as a single predictor in a softmax
29
speaker law hides a structural degeneracy between “semantic” and “cost” contributions. Our
reparameterisation replaceslog Llit by a pure generative-semantics term and a normalisation-
adjusted cost term, leaving the expected information gain as a third, structurally independent
predictor. Importantly, this reparameterisation is behaviour-preserving: it does not change the
choice probabilities but clarifies which components of the log-odds are, in principle, separately
identifiable from data.
The geometric listener construction sits somewhat orthogonally to these FEP considerations.
Once messages are embedded in a latent metric space and a Gaussian prototype model is adopted,
the ideal listener is fully specified by distances and priors; the actual listeners are specified by
the chosen decoder class; and the information-theoretic metrics follow directly from the pair
(Lideal,L0). These constructions can therefore be used by readers who are agnostic about the
FEP or who prefer to treatG(u; w,c) purely as a convenient scoring function.
5.4 Methodological limitations and future directions
Several aspects of our empirical instantiation are intentionally conservative and point to obvious
extensions.
First, we restricted attention to a binary world state (Left vs. Right) and a single type of
context (block priors). Extending the framework to multi-class or continuous state spaces would
require richer latent geometries and ideal listeners, and likely new diagnostics that generalise
information gain and pragmatic loss beyond the binary case. Mutual information and KL
divergence remain well defined in these settings, but choice of latent model and decoder class
becomes more consequential.
Second, our latent geometry is deliberately simple: factor analysis with a shared covariance,
fitted to trial-by-neuron spike counts in a fixed peri-stimulus window. This choice keeps the link
between Mahalanobis distance and Gaussian likelihoods transparent but may under-represent
nonlinear or dynamical structure in cortical population activity. In the main analyses, the
encoder is fitted on training trials only, whereas the Gaussian prototypes and shared covariance
that define the ideal listener are estimated from all valid trials. This reflects our use of the
latent Gaussian geometry as a description of the entire session rather than as an object whose
generalisation error we seek to cross-validate; predictive generalisation is evaluated only for the
logistic listeners. Consistent with this interpretation, recomputingIG and the four pragmatic
losses on held-out test trials yields session-level summaries that are almost indistinguishable
from the all-trial values (Section 4.5). Nonlinear embeddings, sequential latent-variable models,
and multi-area manifolds are natural candidates for future work. The main conceptual challenge
will be to preserve the interpretability of the ideal listener and the identifiability of the metrics
as models become more flexible.
Third, our actual listeners are regularised logistic regressions. This is not intended as a
detailed model of any particular downstream circuit, but as a simple, widely used decoder that
can be fitted robustly with the available number of trials. Richer decoders (e.g. shallow neural
networks, kernel methods, or recurrent models) might track the ideal listener more closely and
therefore reduce the pragmatic losses, but our aim here is not to find the optimal decoder.
Rather, once an ideal listener is specified, any choice of decoder defines a pragmatic loss that
can be used to quantify how closely that decoder approximates the ideal. The four-pattern
analysis underscores that different decoder classes and prior-handling schemes can and should
be compared within the same framework.
Fourth, our behavioural analysis is intentionally coarse. We related session-wise information
gain and pragmatic loss to overall behavioural accuracy, finding that sessions with higherIG and
lower pragmatic losses tend to exhibit better performance, but these correlations are weaker than
the purely neural relationships and should be interpreted cautiously given the modest number of
sessions. A more ambitious programme would relate trial-level(IG,ℓk) values to psychometric
and chronometric measures, asking, for example, whether trials with unusually large pragmatic
30
loss correspond to behavioural lapses or slow responses, or whether perturbations that selectively
degrade latent geometry vs. downstream decoding have distinct signatures in behaviour.
Finally, although we focused here on cortical spike trains from a particular visual decision-
making task, the framework is substrate-agnostic. Wherever one can define world states,
contexts, messages, and a geometric representation—for example, in cell–cell signalling, hormonal
communication, or animal vocalisation—the same ideal/actual listener constructions and metrics
could, in principle, be applied. For instance, one could treat patterns of hormone concentrations
as messages in a latent log-concentration space, or acoustic syllables as messages in a latent
acoustic space, and ask how much information they carry about ecologically relevant states and
how closely downstream systems track the corresponding ideal listeners.
5.5 Overall implications
The present study shows that a geometric, FEP-compatible pragmatic framework can be
brought into contact with large-scale neural data and can yield interpretable, unitful measures
of communication quality. Information gain and pragmatic loss, taken together and extended to
the full2 ×2 grid of priors and decoder classes, provide a structured way to ask:what could be
communicated in principle from a given population code, how is it encoded geometrically, and
how efficiently is it decoded by particular downstream listeners?In our IBL instantiation, we find
that latent geometry learned from trial-wise spike counts supports robust ideal decoding, that an
actual logistic listener leaves a non-trivial fraction of this potential information unused, and that
discrepancies between ideal and actual listeners can be cleanly decomposed into contributions
from priors, geometry, and decoder class.
We hope that this combination of FEP-inspired control ideas, pragmatic speaker–listener
models, and latent geometric codes will prove useful not only for analysing neural communication
under uncertainty, but also for studying communication in other biological systems where world
states, contexts, and message geometries can be meaningfully defined.
6 Conclusion
This work set out to build an explicit bridge between the Free Energy Principle, pragmatic
theories of communication, and geometric models of neural population codes, and to test that
bridge on large-scale spiking data. Starting from an expected free energy (EFE) control objective,
we introduced a message-level scoreG(u; w,c) over world states, contexts, and messages, and
showed how an EFE-based softmax over this score induces a pragmatic speaker whose log-odds
can be decomposed into contributions from semantics, production cost, and expected information
gain. By making explicit how a literal listener depends on a generative semantics and its
normalisation, we identified a structural degeneracy that entangles semantic and normalisation
terms, and resolved it via a behaviour-preserving reparameterisation. In the reparameterised
speaker law, a pure generative-semantics term, a normalisation-adjusted cost term, and an
information-gain difference are structurally independent and, in principle, jointly estimable from
behaviour.
On the listener side, we endowed messages with a latent geometric representation and defined
both ideal and actual listeners directly in this space. In the linear–Gaussian instantiation used
here, each trial is mapped to a low-dimensional latent code, class-conditional densities are
Gaussian with a shared covariance, and semantic fit reduces to squared Mahalanobis distance to
world-state prototypes. Combining these likelihoods with block-wise task priors via Bayes’ rule
yields an ideal listenerLideal(w|z,c) without introducing new free parameters beyond those of
the latent model and the empirical priors. An actual listenerL0 is defined by a discriminative
decoder on the same latent codes. From the pair(Lideal,L0) we derived two unitful diagnostics
of communication quality: a single-trial information gainIG(u,c) = Hprior(c) −Hpost(u,c), and
31
a pragmatic lossℓ⋆(u,c) = KL(Lideal(·| u,c) ∥L0(·| u,c)), both measured in nats. Becauseℓ⋆
is defined as a Kullback–Leibler divergence, its numerical scale is fixed by information theory:
there is no arbitrary slope that could be adjusted to hide discrepancies between ideal and actual
posteriors.
We instantiated this geometric–pragmatic framework on Neuropixels recordings from the
International Brain Laboratory visual decision-making task. Treating the well-isolated units
on each probe as a sender population, we constructed trial-by-neuron spike-count matrices in
peri-stimulus windows, embedded them into a low-dimensional latent space by factor analysis,
and built Gaussian prototypes and a shared covariance on that space. The resulting latent
geometry supported robust decoding of the binary world state by an ideal listener, with session-
wise accuracies generally above 0.75. A simple logistic decoder on latent codes served as the
actual listener and, although it performed well above chance, systematically underperformed the
geometric ideal on most probes. Across sessions, the ideal and actual accuracies were tightly
coupled, indicating that when the latent geometry carries more information about task variables,
even a simple discriminative listener can exploit this structure more effectively.
The information-theoretic metrics provided a complementary, unitful view of this channel.
Session-wise mean information gain was modest but reliably positive, corresponding to a small
fraction of the one-bit upper bound available in a symmetric binary decision. Pragmatic loss
was somewhat larger in magnitude and quantified a substantial mismatch between the posteriors
of the ideal and actual listeners on a typical trial. Importantly, these quantities were not
independent. Sessions with higher information gain tended to exhibit lower pragmatic loss
and higher actual decoding accuracy, while sessions with almost vanishing information gain
showed near-chance decoding and large divergences between ideal and actual posteriors. In this
sense, the pair(IG,ℓ⋆) places each sender–listener pair on a spectrum that ranges from efficient,
geometry-aligned communication to weak or misaligned communication.
Block structure in the task allowed us to probe how communication quality depends on
context, here captured by block priorsP(Left |c) ∈{0.2,0.5,0.8}. From the perspective of
the ideal listener, messages in strongly biased blocks were slightly more informative than those
in unbiased blocks, but the overall scale ofIG remained modest. By contrast, pragmatic loss
was strongly context dependent: it was nearly negligible in unbiased blocks and an order of
magnitude larger in biased blocks. This pattern reflects a deliberate asymmetry in our modelling
choices: the ideal listener receives the block prior as input, whereas the main actual listener is
prior-agnostic. As a result, the ideal listener shifts its posterior toward the more likely state
in extreme blocks, while the actual listener can only respond to changes in latent geometry.
To separate the influence of priors from that of geometry and decoder class, we extended the
analysis to a full2 ×2 crossing of prior-aware vs. prior-agnostic ideal and actual listeners. In
this four-pattern decomposition, pragmatic losses are computed between (i) a prior-aware ideal
and a prior-agnostic actual listener, (ii) a prior-aware ideal and a prior-aware actual listener,
(iii) a uniform-prior ideal and a prior-agnostic actual listener, and (iv) a uniform-prior ideal and
a prior-aware actual listener.
This decomposition showed that the largest losses occur when the ideal and actual listeners
disagree about priors while sharing the same latent geometry, confirming that much of the
context dependence in the originalℓ⋆ is driven by how priors are handled. When we provide the
same block prior to the actual listener via a prior-aware logistic decoder, both decoding accuracy
and pragmatic alignment improve markedly, and the corresponding loss pattern collapses to
small values in all contexts. At the same time, losses computed between a uniform-prior ideal
and the prior-agnostic decoder remain non-zero, revealing a residual component that cannot be
attributed to priors alone and instead reflects limitations of the chosen decoder relative to the
latent Gaussian ideal. Taken together, the four-pattern analysis clarifies which aspects of the
observed inefficiency are artefacts of the modelling comparison and which reflect a genuine gap
between encoded and exploited information.
32
Additional control analyses support a cautious but optimistic interpretation of these metrics.
Recomputing information gain and pragmatic loss on held-out test trials only yielded very
similar session-level summaries, with the expected slight increase in loss when training trials
were excluded, indicating that our conclusions do not hinge on using all valid trials. A prior-
aware decoder that receives block priors as an extra feature nearly saturates the geometric
ideal, demonstrating that the latent codes themselves are compatible with a Bayesian use of
priors. Exploratory correlations between neural communication metrics and coarse behavioural
performance suggest that sessions with higherIG and lower pragmatic loss tend to exhibit better
behaviour, although the limited number of sessions and the simplicity of the behavioural summary
mean that these relationships should be interpreted as descriptive rather than definitive.
There are several directions in which the present framework could be developed. On
the theoretical side, the reparameterised speaker law suggests concrete behavioural tests in
paradigms where both neural messages and overt choices are observed, enabling joint estimation
of semantic, cost, and information-gain components ofG(u; w,c). On the representational side,
more expressive latent geometries—nonlinear embeddings, dynamical latent-variable models, or
multi-area manifolds—could be used to examine how(IG,ℓ⋆) evolve along processing hierarchies
or across learning, at the cost of more complex and potentially less interpretable ideal listeners.
On the empirical side, extending the analysis beyond binary world states and visual decision-
making, and beyond cortical spike trains to other modalities and substrates, would help to assess
how widely applicable these metrics are. Because the construction only requires world states,
contexts, messages, and a geometric representation, the same tools could, in principle, be applied
to cell–cell signalling, hormonal communication, or animal vocalisation.
In summary, we have shown that an FEP-compatible, geometric, and pragmatic framework
can be coupled to real neural data and can yield interpretable, falsifiable, and unitful measures of
communication quality. Information gain and pragmatic loss—in their original and four-pattern
forms—provide a principled way to quantify, within a single mathematical language, how much
a sender population could communicate in principle and how closely a given class of listeners
realises that potential. We hope that this combination of latent geometry, ideal and actual
listeners, and information-theoretic diagnostics will prove useful not only for analysing neural
codes, but also for studying communication in other biological systems where world states,
contexts, and geometric messages can be meaningfully defined.
Data and computing environment
• Code / Data:10.5281/zenodo.17718560
• Python: 3.11.14.
• Key libraries: NumPy 2.2.0; Pandas 2.3.3; scikit-learn 1.7.1; Matplotlib 3.10.6; ibllib 3.4.3;
ONE-api 3.4.1
• OS / CPU: Windows 11, 20 logical cores.
Disclosure
Funding: None. Competing interests: The author declares no competing interests. Ethics: This
study analyzes a public, de-identified dataset (IBL neuropixels dataset); no new human subjects
data were collected. AI/LLM use: OpenAI GPT-5.1 Pro was used for internal mock peer review,
LLM-assisted exploratory theme surfacing, and language/LaTeX polishing; all outputs were
manually vetted and rewritten by the author, who assumes full responsibility.
© 2025 Ryotaro Saito. This work is licensed under CC BY 4.0.
33
References
[1] J. P. Cunningham and B. M. Yu. Dimensionality reduction for large-scale neural recordings.
Nature Neuroscience, 17(11):1500–1509, 2014.
[2] M. C. Frank and N. D. Goodman. Predicting pragmatic reasoning in language games.
Science, 336:998, 2012.
[3] K. Friston. The free-energy principle: A unified brain theory?Nature Reviews Neuroscience,
11(2):127–138, 2010.
[4] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. FitzGerald, and G. Pezzulo. Active
inference and epistemic value.Cognitive Neuroscience, 6(4):187–214, 2015.
[5] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, J. O’Doherty, and G. Pezzulo.
Active inference and learning.Neuroscience and Biobehavioral Reviews, 68:862–879, 2016.
[6] K. J. Friston, R. Rosch, T. Parr, C. Price, and H. Bowman. Deep temporal models and
active inference.Neuroscience and Biobehavioral Reviews, 77:388–402, 2017.
[7] N. D. Goodman and M. C. Frank. Pragmatic language interpretation as probabilistic
inference. Trends in Cognitive Sciences, 20(11):818–829, 2016.
[8] H. P. Grice. Logic and conversation.Syntax and Semantics 3: Speech Acts, pages 41–58.
1975.
[9] International Brain Laboratory; V. Aguillon-Rodriguez, D. Angelaki, H. Bayer, et al.
Standardized and reproducible measurement of decision-making in mice.eLife, 10:e63711,
2021.
[10] J. J. Jun, N. A. Steinmetz, J. H. Siegle, et al. Fully integrated silicon probes for high-density
recording of neural activity.Nature, 551(7679):232–236, 2017.
[11] R. Saito. Recovering the Rational Speech Act Framework from the Free En-
ergy Principle: Testable Assumptions and Predictions. Preprint, Zenodo, 2025.
doi:10.5281/zenodo.17194606.
[12] R. Saito. Pragmatic Constraints Provide Partial Falsifiability for the Free Energy Principle
in Biological Signaling. Preprint, Zenodo, 2025. doi:10.5281/zenodo.17198775.
34