Factorised Active Inference for Strategic Multi-Agent Interactions
Jaime Ruiz-Serra
Centre for Complex Systems,
The University of Sydney
Sydney, Australia
jaime.ruizserra@sydney.edu.au
Patrick Sweeney
Centre for Complex Systems,
The University of Sydney
Sydney, Australia
pswe2854@uni.sydney.edu.au
Michael S. HarrÃ©
Centre for Complex Systems,
The University of Sydney
Sydney, Australia
michael.harre@sydney.edu.au
Abstract
Understanding how individual agents make strategic decisions
within collectives is important for advancing fields as diverse as eco-
nomics, neuroscience, and multi-agent systems. Two complemen-
tary approaches can be integrated to this end. The Active Inference
framework (AIF) describes how agents employ a generative model
to adapt their beliefs about and behaviour within their environment.
Game theory formalises strategic interactions between agents with
potentially competing objectives. To bridge the gap between the
two, we propose a factorisation of the generative model whereby
each agent maintains explicit, individual-level beliefs about the
internal states of other agents, and uses them for strategic planning
in a joint context. We apply our model to iterated general-sum
games with two and three players, and study the ensemble effects
of game transitions, where the agentsâ€™ preferences (game payoffs)
change over time. This non-stationarity, beyond that caused by
reciprocal adaptation, reflects a more naturalistic environment in
which agents need to adapt to changing social contexts. Finally,
we present a dynamical analysis of key AIF quantities: the varia-
tional free energy (VFE) and the expected free energy (EFE) from
numerical simulation data. The ensemble-level EFE allows us to
characterise the basins of attraction of games with multiple Nash
Equilibria under different conditions, and we find that it is not
necessarily minimised at the aggregate level. By integrating AIF
and game theory, we can gain deeper insights into how intelligent
collectives emerge, learn, and optimise their actions in dynamic
environments, both cooperative and non-cooperative.
Keywords
free energy principle; game theory; theory of mind
ACM Reference Format:
Jaime Ruiz-Serra, Patrick Sweeney, and Michael S. HarrÃ©. 2025. Factorised
Active Inference for Strategic Multi-Agent Interactions . In Proc. of the 24th
International Conference on Autonomous Agents and Multiagent Systems (AA-
MAS 2025), Detroit, Michigan, USA, May 19 â€“ 23, 2025 , IFAAMAS, 10 pages.
1 Introduction
Collective intelligence, the emergent ability of groups to solve prob-
lems more effectively than individuals, is a phenomenon observed
across biological, social, and artificial systems. Understanding the
mechanisms that drive this collective behaviour is essential for
This work is licensed under a Creative Commons Attribution Inter-
national 4.0 License.
Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2025), Y. Vorobeychik, S. Das, A. NowÃ© (eds.), May 19 â€“ 23, 2025, Detroit, Michigan,
USA. Â© 2025 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org).
advancing fields as diverse as neuroscience, economics, and multi-
agent systems.
Individual-level preferences incentivise the behaviour that shapes
collective outcomes. While these preferences may not always con-
flict, tensions between cooperation and non-cooperation lead to
emergent higher-level structures. Game theory models incentivised
social interactions with potentially competing objectives, where a
utility function maps behaviour to the real numbers. A Nash equilib-
rium represents the point where agents, independently maximising
their utility, have no incentive to change their strategy.
Bridging the gap between idealised game-theoretic models and
the often messy realities of agents interacting in complex envi-
ronments presents a persistent challenge. Traditional game theory
often falters when agents deviate from perfect rationality [10]. This
challenge becomes particularly salient in the face of strategic uncer-
tainty, where agents grapple with uncertainty about the actions and
intentions of others, and equilibrium selection , where multiple po-
tential equilibria exist without clear mechanisms for convergence.
Shoham et al. [88] drew attention to a key issue with equilibrium
selection: â€œIt seems to us that sometimes there is a rush to investi-
gate the convergence properties, motivated by the wish to anchor
the central notion of game theory in some process, at the expense
of motivating that process rigorouslyâ€.
The Active Inference framework (AIF), a process theory rooted
in neuroscience, can offer a compelling perspective on these chal-
lenges. AIF provides an empirically informed account of perception,
action, and learning under uncertainty that has rapidly matured
in recent years [24], but lacks a clear framework for multi-agent
strategic interactions [17, 25]. Game theory often assumes perfect
rationality and complete information, which AIF relaxes. Employ-
ing game theory as a model of incentivised decision-making, and
AIF as the cognitive process underlying individual decisions allows
for experiments with dynamic agent preferences while providing
access to how their beliefs and precision change in response to
others. Our results shed light on the mutual influence between
individual cognition and structural dynamics at the collective level.
We begin by reviewing recent work on the intersection of AIF
and Bayesian agents, game theory, and multi-agent systems (Â§2).
Integrating the two ends of the spectrum, we propose a factorisa-
tion of the generative model whereby an agent maintains explicit,
individual-level beliefs about the internal states of other agents and
uses them for strategic planning in a joint context (Â§3).
We apply our model to iterated general-sum games with two and
three players and study the ensemble effects of game transitions ,
where the agentsâ€™ preferences (game payoffs) and their associated
equilibria change over time (Â§4). We present a dynamical analysis
of two key AIF quantities: the variational free energy (VFE) (Â§4.1)
and the expected free energy (EFE) (Â§4.2) from numerical simulation
arXiv:2411.07362v2  [cs.MA]  20 May 2025
data. The ensemble-level EFE allows us to characterise the basins
of attraction of games with multiple Nash Equilibria (such as the
Stag Hunt) under different conditions, and we find that it is not
necessarily minimised at the aggregate level [47].
2 Background
2.1 Iterated normal-form games
Iterated normal-form games (INFG) [41] provide a structured frame-
work to study strategic interactions between agents, allowing for
the analysis of decision-making processes over repeated encounters.
INFG extend the basic framework of normal-form games, where
players (agents) simultaneously choose strategies (actions), and
their payoffs depend on the combination of all chosen strategies.
In an iterated setting, this process repeats over multiple rounds,
allowing agents to observe outcomes and potentially adapt their
strategies over time. INFG are defined by a set of agents, a set of
allowable actions for each agent, Uğ‘– , a game payoff function, g,
mapping every joint outcome (actions of all players involved) to
a real number (payoff value for a given outcome)â€”thus encoding
(often as a matrix) the incentives or preferences of the agentsâ€”,
and possibly the total number of rounds (time steps) the game is
played for. We use the term â€˜egoâ€™ to refer to any arbitrary agent
from whose perspective we are describing the game, and â€˜alterâ€™ to
refer to any other agent participating in the interaction.
The simplest games are (symmetric) two-player, two-action (2Ã—2)
games1. Here actions are ğ‘¢ âˆˆU = {0,1}â‰¡{ c,d}(â€˜cooperateâ€™ and
â€˜defectâ€™) for each agent. Egoâ€™s payoffs for each of the four possible
outcomes in these games are commonly referred to as reward (ğ‘…)
when both agents cooperate, temptation (ğ‘‡) when ego defects and
alter cooperates, sucker (ğ‘†) when ego cooperates but alter defects,
and penalty (ğ‘ƒ) when both defect. From egoâ€™s point of view, her
payoffs are:
g =
ğ‘… ğ‘†
ğ‘‡ ğ‘ƒ

(1)
Canonical games can be determined by the relative ordering of
these payoff values [ 42], for example the well-known Prisonerâ€™s
Dilemma (PD) has ğ‘‡ > ğ‘… > ğ‘ƒ > ğ‘†, the Chicken game (Ch) has
ğ‘‡ > ğ‘… > ğ‘† > ğ‘ƒ, and the Stag Hunt (SH) has ğ‘… > ğ‘‡ > ğ‘ƒ > ğ‘†. By
setting each as an integer between 1 and 4 as in [17], we have:
PD =
3 1
4 2

, Ch =
2 3
4 1

, SH =
4 1
3 2

, (2)
from which we can obtain the payoff function, e.g.
gCh (d,c)= ğ‘‡Ch = 4.
In the single-shot version of a normal-form game, agents typi-
cally maximise their payoff for that round alone. However, in iter-
ated games, agents must consider long-term outcomes [31]. This
opens up new possibilities for strategic behaviour, including learn-
ing (agents can learn from previous interactions adjusting their
strategy to improve future outcomes), reciprocity (agents might
cooperate if they believe others will reciprocate in future rounds,
balancing short-term losses for long-term gains), and reputation
1Two-action settings (e.g., cooperate/defect) are standard in game theory due to their
simplicity, analytical tractability, and clarity of incentives. These settings focus on
fundamental dynamics and are widely applicable to real-world scenarios abstracted
into binary decisions.
and trust (the interaction history can influence future decisions,
where agents may choose to trust or punish based on previous
behaviour) [4].
2.2 Bayesian learning in games
Bayesian learning in strategic games builds on Savageâ€™s founda-
tional work in Bayesian decision theory, which originally addressed
games against nature [86]. In multi-agent settings, outcomes de-
pend on the strategic interplay of agents, where each agentâ€™s actions
influence and respond to those of others. This interdependence cre-
ates a dynamic, non-stationary environment, requiring agents to
adapt continually as strategies coevolve [3, 45, 61].
The simulation literature focuses on how agents can learn equi-
libria through repeated interactions. The choice of priors signifi-
cantly influences which equilibria agents achieve [16, 27, 68]. Un-
der certain conditions, rational learning may converge asymptot-
ically to a Nash equilibrium if agentsâ€™ priors contain a â€˜grain of
truthâ€™ [51, 52, 66]. A fundamental model in this area is fictitious
play, where agents estimate opponentsâ€™ strategies by averaging
past actions and choosing their best response [9, 82]. This frame-
work can be interpreted as sequential Bayesian inference, where
each agent assumes opponents follow an unknown, independent,
and stationary strategy [95]. Extensions of fictitious play introduce
stochastic action selection [28, 30, 63], exponential forgetting [29],
non-stationary strategies [87], and variational inference [80].
In AI, the multi-agent systems literature explores Bayesian meth-
ods for coordination and learning, though they have not yet gained
the same traction as in single-agent reinforcement learning [ 32].
A common approach is to adapt single-agent algorithms, such as
the Bayes-Adaptive Markov Decision Process [19] and its exten-
sions [12, 39, 81, 83]. These algorithms, however, often struggle in
multi-agent environments due to the non-stationarity introduced
by agentsâ€™ coevolving strategies, making it challenging for any
single agent to converge to an optimal policy.
To address these challenges, type-based reasoning has emerged
as a prominent approach for explicitly modelling other agents. In
this approach, agents model othersâ€™ behaviours as mappings from
interaction histories to action probabilities [1, 44], allowing them to
anticipate and respond to a wide range of strategies. This method
addresses the heterogeneity of multi-agent systems by classify-
ing agents according to their learning capabilities and information
structures [61]. By starting with a prior over these types, agents sys-
tematically update their beliefs based on observed actions, refining
their predictions and strategies as they gather more information
about their opponentsâ€™ behaviors [2, 11, 46, 90, 92].
Recursive reasoning builds on type-based methods by incorpo-
rating not only an agentâ€™s beliefs about others but also their beliefs
about the beliefs of others, forming a hierarchical structure. This
approach underlies models like the Interactive Partially Observ-
able Markov Decision Process [34â€“36], where agents maintain and
update beliefs about othersâ€™ beliefs and strategies across multiple
levels. By modelling nested beliefs, agents better anticipate othersâ€™
actions, laying the foundation for Theory of Mind models that sup-
port more sophisticated and adaptive interactions [6, 18, 67, 76, 92].
Another line of research extends graphical models to multi-agent
contexts, leveraging conditional independencies for efficient repre-
sentation and inference. Multi-agent influence diagrams and graph-
ical games capture dependencies among agents, enabling efficient
computation by focusing on local interactions [ 54, 56, 57]. Simi-
larly, action-graph games and expected utility networks optimise
inference by structuring interactions around shared actions within
agent subsets, which is particularly advantageous in sparsely cou-
pled games [49, 50, 58].
Finally, the intersection of Bayesian learning and bounded ratio-
nality frames decision-making as constrained optimisation under
uncertainty. Product Distribution Theory applies the Maximum En-
tropy principle [48] to derive equilibria where agents balance utility
maximisation with computational costs [91]. GrÃ¼nwald and Dawid
demonstrate that maximising entropy and minimising worst-case
expected loss are dual problems, structured as zero-sum games
between a decision-maker and nature [38]. Thermodynamic Deci-
sion Theory expands these principles, integrating utility (energy)
and information-processing costs (entropy) within a variational
framework where agents minimise free energy. This approach
naturally extends to variational Bayesian inference, enabling ef-
ficient, approximate posterior updates under bounded rationality
constraints [74]. This framework generalises to risk-sensitive con-
trol [20, 53, 60, 69, 70] and adversarial contexts [71, 72], illustrating
its versatility across diverse decision-making scenarios.
2.3 Game theory and Active Inference
This section outlines how AIF and game theory have been combined
to model strategic decision-making in social interactions. Yoshida
et al. [93] examine how individuals infer the intentions of others
in the spatial Stag Hunt game, highlighting that people engage in
recursive thinking about othersâ€™ beliefs. This approach connects
strategic thinking in game theory with Bayesian inference and
bounded rationality in cognitive psychology, providing insights
into how people make decisions in uncertain social environments.
Moutoussis et al. [65] develop a formal model of interpersonal
inference based on active inference principles, where agents infer
their partnerâ€™s likely type (cooperative or defecting) by observing
past actions and updating their beliefs. This demonstrates how the
computational models used to describe individual decision-making
can be extended to the complexities of social interaction, where
understanding othersâ€™ mental states is crucial.
Demekas et al. [17] build on this by showing how AIF agents can
learn effective strategies in the iterated Prisonerâ€™s Dilemma by con-
tinuously updating beliefs about transition probabilities between
game states. Their generative model tracks how learning rates influ-
ence strategy development, offering analytical clarity through belief
updates. Hyland et al. [47] introduce the â€˜Free-Energy Equilibriaâ€™
framework, extending the EFE to strategic contexts by conditioning
predictions on the joint policies of agents. This framework merges
Nash equilibria with bounded rationality, proposing that coopera-
tion could emerge as agents align actions through joint free-energy
minimization.
Fields and Glazebrook [21] further explore how physical inter-
actions can be framed as games using the Free Energy Principle,
drawing attention to the undecidability of achieving Nash equilibria
in classical and quantum contexts. This complexity helps explain
why real-world systems often fail to converge to stable outcomes.
Grounding strategic decision-making in AIF provides a more
realistic model for social interactions, moving beyond the assump-
tions of perfect rationality and complete information in traditional
game theory, with a foundation in neuroscience.
3 Model description
In INFG, ğ‘ agents interact by selecting an action each time step
with the goal of maximising their payoffs as determined by the
game payoff function, g. The agents observe the actions taken by
each of the agents (including themselves) in the preceding step in
order to decide how to act in the current step.
In our model 2, these observations are perceived via different
modalities, ğ‘š, one for each agent. For example, for ğ‘ = 3 agents
ğ‘š âˆˆ{ğ‘–,ğ‘—,ğ‘˜ }, |U|= 2 actions, and taking an egocentric perspective,
each observation is o = (ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ )âˆˆ{ c,d}ğ‘ , with the first modal-
ity being egoâ€™s action ğ‘œğ‘–,ğ‘¡ = ğ‘¢ğ‘–,(ğ‘¡ âˆ’1), and subsequent modalities
ğ‘œğ‘—,ğ‘¡ = ğ‘¢ğ‘—, (ğ‘¡ âˆ’1)each pertaining to an alter (ğ‘—,ğ‘˜). Further details are
provided in Â§3.2.1. In the following, we omit time and agent indices
where implied by context.
To act effectively in response to her counterparts, ego must take
into account each of her opponentsâ€™ propensity for playing each
action at a given time (e.g. for ğ‘— to play â€˜cooperateâ€™, or ğ‘(ğ‘¢ğ‘— = c)).
This propensity is driven by the opponentâ€™s â€˜internal worldâ€™, ğœ“ğ‘— ,
which is not observable to ego [62].
An appropriate way to model ğœ“ğ‘— is as a hidden state using a
Partially-Observable Markov Decision Process (POMDP), where, each
time step, agents infer the current hidden state ğ‘  âˆˆS based on an
observation ğ‘œ âˆˆO, and can influence it through their actionsğ‘¢ âˆˆU
to maximise their payoff. AIF distinguishes between the external
(ontological) generative process, which represents the actual dynam-
ics of the environment, and the internal (epistemic)generative model
of each agent, which encodes its beliefs about those dynamics, and
as such is a good match for the POMDP formalism [15, 89].
3.1 Generative Model
An agentâ€™s generative model consists of a joint distribution over
hidden states, observations, policies (action sequences), and model
parameters. The short timescale dynamics are encoded in
ğ‘(ğ‘ 0:ğ‘¡,ğ‘œ0:ğ‘¡ |ğ‘¢0:ğ‘¡ âˆ’1)= ğ‘(ğ‘ 0)
ğ‘¡Ã–
ğœ=1
ğ‘(ğ‘œğœ |ğ‘ ğœ )ğ‘(ğ‘ ğœ |ğ‘ ğœ âˆ’1,ğ‘¢ğœ âˆ’1),
including the agentâ€™s prior beliefs about the initial state of the world
ğ‘(ğ‘ 0)encoded in D; the transition model ğ‘(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğ‘¢ğ‘¡ ), encoded in
B; and the observation likelihood ğ‘(ğ‘œ|ğ‘ ), encoded in A. For discrete
POMDPs, the distributions can be obtained from their encoding as a
categorical distribution, e.g. ğ‘(ğ‘œ|ğ‘ )= Cat(A)where A is a |O|Ã—|S|
matrix, B is a |S|Ã—|S|Ã—|U| tensor, and D a vector in the simplex
Î”S.
In our application to strategic interactions, ego infers the hidden
state of each agent [6, 84] in a corresponding factor, ğ‘› âˆˆ{ğ‘–,ğ‘—,ğ‘˜ },
of the generative model. Ego must make vast simplifications in
modelling each alterâ€™s true and complex internal world ğœ“ğ‘— â€”which
2Code available at GitHub/RuizSerra/factorised-MA-AIF
encompasses all the components in alterâ€™s generative model, en-
coding his beliefs (A,B,D,ğ‘; see Â§3.2), preferences (C; see Â§3.3), and
constraints (such as habits E, or level of rationalityğ›½1; see Â§3.4) [33],
all depicted in Figure 1. Our approach is to model this internal
world (external to ego) as a categorical distribution over different
types, with parameters s, placing the opponent type on the simplex
Î”S. In the simplest such model, which we adopt here, these types
may be â€˜cooperatorâ€™ or â€˜defectorâ€™, and the opponent could be any-
where between the two (i.e., we model the opponentâ€™s propensity
for playing each action)3.
Ego further assumes that her opponents play the actions they
mean to play, ruling out the possibility of â€˜trembling handâ€™ imper-
fections. This simplifies our model such that there is no â€˜ambiguityâ€™
in the environment, so the likelihood model for each factorAğ‘› is an
identity matrix, or equivalently that the observation likelihood is a
Kronecker delta distribution, ğ‘(ğ‘œğ‘š |ğ‘ ğ‘›)= ğ›¿ğ‘œğ‘š,ğ‘ ğ‘› , âˆ€ğ‘š= ğ‘› âˆˆ{ğ‘–,ğ‘—,ğ‘˜ }.
3.2 Variational inference
Every time step, having observed the actions of each agent,ğ‘œğ‘› = ğ‘¢ğ‘›,
ego has to infer the underlying ğ‘ ğ‘› â‰¡ğ‘(ğ‘¢ğ‘›), for each factor. This
entails updating her posterior beliefs ğ‘ğ‘– (ğ‘ ğ‘— |ğ‘œğ‘— )about each hidden
state factor through Bayesian inversion. The true posterior may be
intractable, so it is approximated by a variational posterior ğ‘ğ‘– (ğ‘ ğ‘— ).
This is achieved by minimising the Variational Free Energy (VFE),
which is expressed as (having omitted the agent ğ‘– and factor ğ‘—
indices for brevity):
ğ¹[ğ‘,ğ‘œ]= Eğ‘(ğ‘ )[âˆ’log ğ‘(ğ‘œ,ğ‘ )]
|                   {z                   }
energy
âˆ’ğ»[ğ‘(ğ‘ )]
|   {z   }
entropy
(3a)
= Dkl

ğ‘(ğ‘ )
ğ‘(ğ‘ |ğ‘œ)

|                  {z                  }
divergence
âˆ’log ğ‘(ğ‘œ)
|   {z   }
evidence
â‰¥âˆ’log ğ‘(ğ‘œ)
|   {z   }
evidence
, (3b)
with ğ‘(ğ‘ )the prior over hidden states, and ğ‘(ğ‘ )the variational pos-
terior, or the agentâ€™s beliefs about hidden states. We model beliefs
via a Dirichlet(ğœ½)distribution with variational parameters ğœ½. We
approximate the VFE using the following Monte Carlo sampling
procedure: let ğ‘ğ‘™ âˆ¼Dirichlet(ğœ½)be the ğ‘™-th sample from the Dirich-
let distribution. With ğ¿samples {ğ‘ğ‘™ }ğ¿
ğ‘™=1, we can write an unbiased
estimate of the VFE4 as
ğ¹[ğ‘,ğ‘œ]= âˆ’Eğ‘(ğ‘ )

log ğ‘(ğ‘œ|ğ‘ )+log ğ‘(ğ‘ )âˆ’logğ‘(ğ‘ )

(4a)
â‰ˆË†ğ¹

{ğ‘ğ‘™ }ğ¿
ğ‘™=1,ğ‘œ

= 1
ğ¿
ğ¿âˆ‘ï¸
ğ‘™=1
ğ¹[ğ‘ğ‘™,ğ‘œ] (4b)
and optimise the variational parameters ğœ½ through stochastic gradi-
ent descent on Ë†ğ¹. Having found ğœ½âˆ—upon completing the procedure,
we can recover the inferred posterior ğ‘(ğ‘ )as the expected value of
Dirichlet(ğœ½âˆ—), which serves as the optimal point estimate under a
quadratic loss function [8].
Since variational inference occurs every time step, the priorğ‘(ğ‘ )
is obtained from the previously inferred state and the transition
3The support of this distribution need not be limited to the number of actions. For
example, an agent could consider four possible types for policy length 2, or more
abstract types such as â€˜Tit for Tatâ€™ [4].
4The chosen form of the VFE for optimisation is derived from (3a) by replacing
ğ‘ (ğ‘œ, ğ‘ )= ğ‘ (ğ‘œ |ğ‘ )ğ‘ (ğ‘ )and subsuming all terms into a single expectation operator in
(4a).
model Bğ‘¢, i.e.
ğ‘(ğ‘ ğ‘¡ )â‰ˆ ğ‘(ğ‘ ğ‘¡ |ğ‘¢ğ‘¡ âˆ’1)=
âˆ‘ï¸
ğ‘ ğ‘¡âˆ’1
ğ‘(ğ‘ ğ‘¡ |ğ‘ ğ‘¡ âˆ’1,ğ‘¢ğ‘¡ âˆ’1)ğ‘(ğ‘ ğ‘¡ âˆ’1) (5)
3.2.1 Factorised model Generative models in previous AIF-adjacent
applications to game theory [17, 22, 65] assume the state space is
joint across all agents in the game (i.e. {cc, cd, ... dd} for their two
agents). Is a mean-field factorisation of the variational posterior,
ğ‘(ğ‘ ğ‘– )ğ‘(ğ‘ ğ‘— )ğ‘(ğ‘ ğ‘˜ ), adequate in the game-theoretic context, or should
a joint distribution, ğ‘(ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜ ), be assumed [80]? That is, can the
hidden states be considered independent of each other? Recall that,
when our agents perform inference, they approximate the poste-
rior ğ‘(ğ‘ |ğ‘œ)â‰ˆ ğ‘(ğ‘ ). In the case of repeated normal-form games, the
hidden state ğ‘ ğ‘— is (the parameterisation of) the posterior distribu-
tion over actions (policy) ğ‘(ğ‘¢ğ‘— )of opponent ğ‘—, and the observation
ğ‘œğ‘— is the action ğ‘¢ğ‘— taken by this opponent. So inferring a distri-
bution over a single opponentâ€™s hidden state involves finding a
ğ‘(ğ‘ ğ‘— )â‰ˆ ğ‘(ğ‘ ğ‘— |ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ ).
The Markov Blanket (MB) of a node or set of nodes Jis de-
fined as the set of Jâ€™s parents, Jâ€™s children, and any other par-
ents of Jâ€™s children [78]. Under this definition, if we let Jbe the
nodes â€˜insideâ€™ agent ğ‘—, including ğ‘ ğ‘— = ğ‘(ğ‘¢ğ‘— ), the MB consists of
{ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜,ğ‘¢ğ‘— }, which â€˜shieldâ€™ the internal states of ğ‘— from the ex-
ternal world. In INFG, taking dynamics into account, this MB set is
actually {ğ‘¢ğ‘– (ğ‘¡ âˆ’1),ğ‘¢ğ‘— (ğ‘¡ âˆ’1),ğ‘¢ğ‘˜ (ğ‘¡ âˆ’1),ğ‘¢ğ‘—ğ‘¡ }. Thus, we can say that
ğ‘ ğ‘— âŠ¥{ğ‘ ğ‘–,ğ‘ ğ‘˜ }
 {ğ‘¢ğ‘– (ğ‘¡ âˆ’1),ğ‘¢ğ‘— (ğ‘¡ âˆ’1),ğ‘¢ğ‘˜ (ğ‘¡ âˆ’1),ğ‘¢ğ‘—ğ‘¡ },
i.e., ğ‘ ğ‘— is conditionally independent of ğ‘ ğ‘– and ğ‘ ğ‘˜ (and any other
node outside ğ‘—) given {ğ‘¢ğ‘– (ğ‘¡ âˆ’1),ğ‘¢ğ‘— (ğ‘¡ âˆ’1),ğ‘¢ğ‘˜ (ğ‘¡ âˆ’1),ğ‘¢ğ‘—ğ‘¡ }. Furthermore,
at time ğ‘¡ when ğ‘– infers ğ‘ ğ‘— , the action ğ‘¢ğ‘—ğ‘¡ has not happened yet, so
that node does not exist in the Dynamic Bayesian Network. There-
fore, ğ‘(ğ‘ ğ‘— ) â‰ˆğ‘(ğ‘ ğ‘— |ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ ), i.e. given the MBs of agents, their
internal states are conditionally independent.
Accordingly, in our model, each agentğ‘–infers the hidden state for
every factor ğ‘› âˆˆ{ğ‘–,ğ‘—,ğ‘˜ }individually, and thus retains a collection
of parameters ğœ½ âˆˆRğ‘ Ã—|U|
+ , with ğ‘ the number of factors (agents
being tracked) and |U|the number of actions.
3.3 Preferences and planning
AIF extends Bayesian learning by integrating action and decision-
making, allowing agents to actively reduce uncertainty and achieve
goal-directed behaviour in dynamic environments. Agents plan and
select actions that both gather information and satisfy their pref-
erences over observations ğ‘âˆ—(ğ‘œ), encoded in C (such that ğ‘âˆ—(ğ‘œ)=
Cat(C)in discrete settings). This requires counterfactual thinking:
â€˜what would I be likely to observe if I were to doğ‘¢ğ‘– ?â€™. The generative
model employed in inference can be used for planning by predicting
future states (via the transition modelBË†ğ‘¢) and observations (via the
likelihood A) given counterfactual actions Ë†ğ‘¢ğ‘– (distinguished from
actual actions ğ‘¢ğ‘– ). In the following, we denote predictive variables
with a bar, e.g. Â¯ğ‘œğ‘— is what ğ‘– might observe ğ‘— doing in the next time
step.
Agents achieve the desired exploration-exploitation trade-off
by selecting actions that minimise an Expected Free Energy (EFE),
comprising salience and pragmatic value terms. In what follows, we
describe these terms in more detail and adapt them to INFG.
Figure 1: The perception-action loop; egoâ€™s â€˜internal worldâ€™,
ğœ“ğ‘– . The agent observes the actions of all agents, from which
she updates her beliefs ğ‘(ğ‘ )to minimise VFE. These beliefs
are used to plan her next action by minimising EFE.
3.3.1 Salience Otherwise known as â€˜epistemic valueâ€™, salience (ğœ)
captures the information gain about hidden statesâ€”or how an ac-
tion is anticipated to change oneâ€™s beliefsâ€”with greater changes in
beliefs holding higher epistemic value [77]. It can be decomposed
into a difference between two entropic terms:
ğœ[Ë†ğ‘¢]= Eğ‘(Â¯ğ‘œ |Ë†ğ‘¢)

Dkl

ğ‘(Â¯ğ‘ |Ë†ğ‘¢,Â¯ğ‘œ)
ğ‘(Â¯ğ‘ |Ë†ğ‘¢)

(6)
= ğ» ğ‘(Â¯ğ‘œ|Ë†ğ‘¢) âˆ’Eğ‘(Â¯ğ‘  |Ë†ğ‘¢)

ğ» ğ‘(Â¯ğ‘œ|Â¯ğ‘ )
(7)
Since our INFG environment is unambiguous (cf. Â§3.1), the sec-
ond term (ambiguity) is zero. There is, however, information to
be gained still from acting to maximise the first term, leading to
exploratory behaviour. This term captures the uncertainty about
the next observation in the event thatË†ğ‘¢ğ‘– is the action taken. Actions
whose outcomes we are most uncertain about are preferred, as we
stand to gain the most information from them. Salience is additive
over factors:
ğœ[Ë†ğ‘¢ğ‘– ]=
âˆ‘ï¸
ğ‘š
ğ» ğ‘(Â¯ğ‘œğ‘š |Ë†ğ‘¢ğ‘– ) (8)
3.3.2 Pragmatic value By choosing actions that maximise prag-
matic value (ğœŒ), agents actively pursue their preferences, closing the
gap between predicted and preferred observations. In normal-form
games, preferences are determined by the game payoffs g, and we
can convert directly from one to the other with
ğ‘âˆ—(ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ )= ğœ g(ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ ) (9)
âˆ€(ğ‘œğ‘–,ğ‘œğ‘—,ğ‘œğ‘˜ )âˆˆU ğ‘ , where ğœ is the softmax function5 [17, 73].
This implies a joint interaction context , since the preferences for
each observation modality come from a joint distribution and are
generally not independent. This highlights the core principle of
game theory, namely the interdependence of agentsâ€™ preferences
and actions. Here the preferences for each observation modality are
derived from a joint distribution, meaning that agentsâ€™ outcomes
are interconnected, and their strategies cannot be considered in
isolation. The essence of game theory lies in analyzing how these
dependencies shape decision-making and the resulting equilibria
in strategic interactions. Agents track the â€˜mental statesâ€™ of other
agents individually (in ğ‘(ğ‘ ğ‘— )), but they must learn how they inter-
play in a given joint context defined byg (e.g. when two agents play
a Prisonerâ€™s Dilemma, or when three agents play Chicken). Unlike in
classical game theory, however, our agents do not know the payoff
function (preferences) of their opponents.
The predicted observations, as a posterior predictive distribu-
tion ğ‘(Â¯ğ‘œ|Ë†ğ‘¢)for each modality, need to be merged into a single
ğ‘(Â¯ğ‘œğ‘–,Â¯ğ‘œğ‘—,Â¯ğ‘œğ‘˜ |Ë†ğ‘¢ğ‘– )to be able to compare to the preferences. The con-
ditional independence between the internal states of the agents,
determined by their respective MBs (Â§3.2.1), allows us to define this
joint posterior as the product [91]
ğ‘(Â¯ğ‘œğ‘–,Â¯ğ‘œğ‘—,Â¯ğ‘œğ‘˜ |Ë†ğ‘¢ğ‘– )=
Ã–
ğ‘šâˆˆ{ğ‘–,ğ‘—,ğ‘˜ }
ğ‘(Â¯ğ‘œğ‘š |Ë†ğ‘¢ğ‘– ) (10)
where each factorâ€™s posterior predictive observation is obtained
from the (factorâ€™s) likelihood model Ağ‘— as6
ğ‘(Â¯ğ‘œğ‘— |Ë†ğ‘¢ğ‘– )= Eğ‘(Â¯ğ‘ ğ‘— |Ë†ğ‘¢)[ğ‘(Â¯ğ‘œğ‘— |Â¯ğ‘ ğ‘— )]. (11)
The pragmatic value is thus defined as the negative cross-entropy
between the posterior predictive observation and the preference
distributions, which agents aim to maximise (cf. log-loss minimisa-
tion):
ğœŒ[Ë†ğ‘¢ğ‘– ]= Eğ‘(Â¯ğ‘œğ‘–,Â¯ğ‘œğ‘— ,Â¯ğ‘œğ‘˜ |Ë†ğ‘¢ğ‘– )

log ğ‘âˆ—(Â¯ğ‘œğ‘–,Â¯ğ‘œğ‘—,Â¯ğ‘œğ‘˜ )

. (12)
Furthermore, for the egoâ€™s own factor,Â¯ğ‘œğ‘– = Ë†ğ‘¢ğ‘– is guaranteed with
full certainty in the counterfactual where Ë†ğ‘¢ğ‘– is played (i.e. where ğ‘¢ğ‘–
5The softmax operation here is not strictly necessary; we could just as well go with
ğ‘âˆ—(ğ‘œğ‘–, ğ‘œğ‘— , ğ‘œğ‘˜ )= exp  g(ğ‘œğ‘–, ğ‘œğ‘— , ğ‘œğ‘˜ ) under an energy function interpretation. How-
ever, by ensuring ğ‘âˆ—is a probability distribution, we ensure the EFE values are in the
positive range (i.e. in Nats) [37, 59].
6ğ‘(Â¯ğ‘œğ‘— |Ë†ğ‘¢ğ‘– )= Ã
Â¯ğ‘ ğ‘— ğ‘(Â¯ğ‘œğ‘— , Â¯ğ‘ ğ‘— |Ë†ğ‘¢ğ‘– )= Ã
Â¯ğ‘ ğ‘— ğ‘ (Â¯ğ‘œğ‘— |Â¯ğ‘ ğ‘— )ğ‘(Â¯ğ‘ ğ‘— |Ë†ğ‘¢ğ‘– )= Eğ‘(Â¯ğ‘ ğ‘— |Ë†ğ‘¢)[ğ‘ (ğ‘œğ‘— |Â¯ğ‘ ğ‘— )]
would in actuality be Ë†ğ‘¢ğ‘– ). Accordingly, we can set ğ‘(Â¯ğ‘œğ‘– |Ë†ğ‘¢ğ‘– )= ğ›¿Â¯ğ‘œğ‘–, Ë†ğ‘¢ğ‘– ,
the Kronecker delta distribution7. This makes the pragmatic value
ğœŒ[Ë†ğ‘¢]â‰¡ Eğ‘(Â¯ğ‘œğ‘— )ğ‘(Â¯ğ‘œğ‘˜ )[log ğ‘âˆ—(Â¯ğ‘œğ‘–,Â¯ğ‘œğ‘—,Â¯ğ‘œğ‘˜ )|Â¯ğ‘œğ‘– = Ë†ğ‘¢ğ‘– ], (13)
i.e., equivalent to the game-theoretic expected utility , under the
interpretation that log ğ‘âˆ—is (proportional to) the utility function
(i.e., the game payoffs, as we did in Eq. 9). Finally, we have the EFE,
ğº[Ë†ğ‘¢ğ‘– ]= âˆ’ğœŒ[Ë†ğ‘¢ğ‘– ]âˆ’ğœ[Ë†ğ‘¢ğ‘– ], (14)
which the agents minimise through their actions, effectively max-
imising salience (ğœ) and pragmatic value (ğœŒ). For the zero-ambiguity
case under consideration, the EFE reduces to (in shorthand)
ğº[Ë†ğ‘¢ğ‘– ]= âˆ’Eğ‘âˆ’ğ‘– [log ğ‘âˆ—|Ë†ğ‘¢ğ‘– ]âˆ’
âˆ‘ï¸
ğ‘š
ğ» ğ‘ğ‘š
, (15)
highlighting the relationship with previous information-theoretic
treatments of bounded rationality in game theory [75, 91].
3.4 Action selection
The selection of actions is driven by the precision-modulated EFE
of each possible action ğº[Ë†ğ‘¢ğ‘– ](or G in vector notation), and the
agentâ€™s habits E (uniform):
ğ‘¢ğ‘– âˆ¼ğ‘(Ë†ğ‘¢ğ‘– )= ğœ log E âˆ’ğ›¾G, (16)
where ğ›¾ is a precision parameter updated each time step,
ğ›¾ = ğ›½1
ğ›½0 âˆ’âŸ¨GâŸ©, (17)
with fixed hyperparameters (ğ›½0,ğ›½1). ğ›½0 (shape) represents a base-
line level of uncertainty or noise, andğ›½1 (rate) reflects how strongly
the agentâ€™s precision (or confidence) is influenced by environmental
feedback. Intuitively, ğ›½0 controls the threshold at which the agent
starts doubting its action selections, while ğ›½1 modulates the rate
of precision updating based on the difference between expected
and observed outcomes. Higher values of ğ›½1 correspond to greater
sensitivity to discrepancies, making the agent more â€˜rationalâ€™ and
noise-averse in refining its action policy [23, 26]. External feedback
is accounted for in âŸ¨GâŸ©= Eğ‘(Ë†ğ‘¢ğ‘– )

ğº[Ë†ğ‘¢ğ‘– ]

, the expected EFE under
the current action probabilities for this agent.
3.5 Learning
Learning in AIF entails updating model parameters, and occurs at a
slower rate than inference. In our model, agents update the transi-
tion model B (initially uniform) every ğ‘‡ğ¿ steps, where ğ‘‡ğ¿ is an inte-
ger sampled uniformly from18 â‰¤ğ‘‡ğ¿ â‰¤30 each time learning occurs.
This random offset ensures agents do not learn in lockstep, which
might cause artifacts in the dynamics. The parameters are updated
based on past transitions, â„ğ‘¡:ğ‘¡+ğ‘‡ğ¿ = (sğ‘¡,ğ‘¢ğ‘–,ğ‘¡ ,sğ‘¡+1,ğ‘¢ğ‘–,ğ‘¡+1,..., sğ‘¡+ğ‘‡ğ¿ ),
via
Bâ€²
ğ‘¢,ğ‘› = Bğ‘¢,ğ‘› +
ğ‘¡+ğ‘‡ğ¿ âˆ’1âˆ‘ï¸
ğœ=ğ‘¡
ğ›¼ğ‘™ ğ›¿ğ‘¢,ğ‘¢ğ‘–,ğœ (sğ‘›,ğœ+1 âŠ—sğ‘›,ğœ) (18)
for each factor ğ‘›, with learning rate ğ›¼ğ‘™ = 1, and where âŠ—is the
outer product and sğ‘›,ğ‘¡ sufficient statistics for ğ‘(ğ‘ ğ‘›)at time ğ‘¡. We
refer the reader to [15] for further details.
7or, equivalently, 1(Ë†ğ‘¢ğ‘– ), the one-hot encoding of the action under consideration
3.5.1 Novelty With learning present, the agents can consider how
their actions are likely to influence their generative model. In this
case, an agent predicts how the transition model might change if
they were to play action Ë†ğ‘¢ğ‘– (for each factor ğ‘›):
â€”BË†ğ‘¢ğ‘–,ğ‘› = BË†ğ‘¢ğ‘–,ğ‘› +ğ›¼ğ‘™ (Â¯sğ‘› âŠ—sğ‘›) (19)
Novelty is an additional term in the EFE, constituting additional
epistemic value:
ğœ‚[Ë†ğ‘¢ğ‘– ]=
âˆ‘ï¸
ğ‘›
Dkl
â€”BË†ğ‘¢ğ‘–,ğ‘›
BË†ğ‘¢ğ‘–,ğ‘›

(20)
summed over factors. Including novelty in the EFE, we have
ğº[Ë†ğ‘¢ğ‘– ]= âˆ’ğœŒ[Ë†ğ‘¢ğ‘– ]âˆ’ğœ[Ë†ğ‘¢ğ‘– ]âˆ’ğœ‚[Ë†ğ‘¢ğ‘– ] (21)
3.5.2 Bayesian model reduction A Bayesian model reduction proce-
dure [15] applied to B (for each factor and action) at learning time
helps reduce overfitting. A â€˜reducedâ€™ model ~Bğ‘¢,ğ‘› = ğœ(ğ›¼âˆ’1ğ‘Ÿ Bğ‘¢,ğ‘›)
is proposed (with reduction rate ğ›¼ğ‘Ÿ = 1.25) and their evidence
difference is computed as
log Ëœğ‘(ğ‘œğ‘›)âˆ’log ğ‘(ğ‘œğ‘›)= log EBâ€²
ğ‘¢,ğ‘›
 ~Bğ‘¢,ğ‘›
Bğ‘¢,ğ‘›

. (22)
If the difference is positive (respectively negative), the evidence
for the reduced (resp. original) model is greater, so it (resp. the
posterior model) is selected as the updated model.
4 Results and discussion
Game transitions increase the non-stationarity of the environ-
ment beyond that caused by reciprocal adaptation, resulting in
role reversals, strategic uncertainty, and eventual equilibrium se-
lection. We use a time-based linear interpolation of the payoff
matrices of two games g and gâ€²to transition between them. Given
a time of transition ğ‘¡x and a transition duration ğ‘‡x, the preferences
ğ‘âˆ—of each agent are updated each time step within the interval
(ğ‘¡x âˆ’ğ‘‡x
2 )â‰¤ ğ‘¡ â‰¤(ğ‘¡x +ğ‘‡x
2 ), via mixing parameterğ‘™ =  ğ‘¡âˆ’(ğ‘¡x âˆ’ğ‘‡x
2 )/ğ‘‡x,
with
ğ‘âˆ—= ğœ

(1 âˆ’ğ‘™)g +ğ‘™gâ€²

. (23)
4.1 VFE and strategic uncertainty
To illustrate the dynamics of transitioning between games, we run
a two-player Ch game for 500 iterations, followed by a SH game for
another 500, with payoffs as per Eq. 2. The transition occurs over
ğ‘‡x = 10 iterations. The outcome is shown as time series plots for
various quantities for each of the two agents in Figure 2.
The VFE, ğ¹[ğ‘,ğ‘œ], quantifies how surprising an observation ğ‘œ is
under the current beliefs ğ‘(ğ‘ ). As shown in Fig. 2 (first row)â€”and
further highlighted by the stylised bounds in Fig. 3â€”, the ensem-
ble is initially in a mixed equilibrium. The symmetry is broken
at ğ‘¡ â‰ˆ250, when, following a model parameter update, ğ‘–â€™s policy
moves towards defection, and ğ‘— appropriately responds by moving
towards cooperation (incentivised by the game payoffs). The en-
semble remains in the selected dc equilibrium for the remainder of
the Ch game.
If the agentsâ€™ ğ›½1 values were much higher, the VFE would follow
the green stylised curve in Fig. 3 much more closely. However,
the chosen ğ›½1 = 15 value causes some â€˜suboptimalâ€™ actions to
be sampled from ğ‘(Ë†ğ‘¢)occasionally, so some observations deviate
Figure 2: Dynamics of a game transition with two agents
(ğ›½1 = 15). 500 steps of Ch followed by 500 steps of SH with a 10-
step transition. In the EFE plots, blue represents â€˜cooperateâ€™,
and pink represents â€˜defectâ€™. In the policy heatmap plots, a
lighter colour indicates a higher probability.
from the â€˜status quoâ€™ (Fig. 3, green) in one (orange) or both (red)
observation modalities, showing different levels of surprise (VFE)
each8.
After the game transition, there is a role reversal: the cooperating
agent becomes a defector, and vice versa. This is explained by
the EFE, ğº[Ë†ğ‘¢](Fig. 2, second row), where ğ‘–â€™s (negative) pragmatic
value of cooperating (âˆ’ğœŒ[c], blue) drops dramatically, surpassing
the pragmatic value of defecting ( âˆ’ğœŒ[d], pink). This is because
the preferences ğ‘âˆ—have now changed, and because ğ‘— has been
cooperating up to that pointâ€”i.e.,ğ‘–believes ğ‘—is a cooperator, that he
can be trusted due to his reputation, making it more appealing forğ‘–
to cooperate as well. However, the obverse is also at play forğ‘—, which
transitions to defecting. We note that this role reversal happens
regardless of the transition duration, ğ‘‡x. With the transition, there
is a sudden increase in the epistemic value (salience ğœ and novelty
ğœ‚) of actions, leading to exploratory behaviour.
8The VFE is additive over the factors of the generative model
dc
dc
dd
cc
cd
cd
cc/ddmixed cd/dc
dd
cc
status
quo
role 
reversal
strategic
confusion
equilibrium
selection
equilibrium
selection
mixed
equilibrium
status
quo
VFE
game transition
Figure 3: Stylised bounds on the dynamics of the VFE.
0.0
2.5
5.0
7.5
/u1D50A Ch SH
SH2
Ch SHg
SHg
0.0
2.5
5.0
7.5
/u1D50A Ch SHr
SHr
Ch SHr
SHp
0 200 400 600 800 1000/u1D461
0.0
2.5
5.0
7.5
/u1D50A Ch SHg SHr
SHgSHr
SH2 SHg SHr SHp
SHgSHr
/u1D461=1000
Figure 4: The ensemble-level expected EFE, ğ”Š, highlights (the
relative size of the basin of attraction of) the equilibria of
a game ( ğ›½1 = 30). The bottom-right plot shows the kernel
density estimate (Gaussian kernel, 0.08 bandwidth) of the
PDF of final values under each condition.
As the agents persist with their new strategies, the pragmatic
value of each action changes to reflect them. The absolute differenceğœŒğ‘— [c]âˆ’ ğœŒğ‘— [d]
 diminishes, increasing the entropy of the policy,
ğ‘(Ë†ğ‘¢ğ‘— ), up to a point of maximal â€˜strategic confusionâ€™, where the VFE
is approximately the same for any possible observation (everything
is just as surprising as anything else). This confusion is resolved by
equilibrium selection, with a small spike in novelty followed by a
decrease in salience as the two agentsâ€™ policies converge to their
final values.
In this particular trial, the ensemble arrives at a payoff-dominant
equilibrium with predominantly cc strategies (modulated by the
rationality of the agents). But this may not always be the case, as
we show next.
4.2 EFE and equilibrium selection
The ensemble-level expected EFE, ğ”Š =Ã
ğ‘– âŸ¨GâŸ©(ğ‘–), closely related to
the joint objective recently proposed in [47], provides a compact
measure of the state of the ensemble over time. By running several
trials of an INFG we can use this statistic to characterise the different
equilibria of the game, as well as (an approximation of) the relative
size of their basin of attraction.
In Figure 4, we show values of ğ”Š under different experimental
conditions. The data were obtained by running 50 trials of a cho-
sen (sequence of) INFG for each condition, which in the plots are
shown superimposed (for a given condition). Data points that are
superimposed on the plots appear darker, highlighting the relative
number of trials that take similar values. The chosen INFG are again
Ch followed by SH, with a two-player condition (SH2; as in Fig. 2),
and the following three-player conditions using three variants of
the SH game:
â€¢a â€˜greenâ€™ variant (SHg) where only two agents are required
in order to successfully hunt a stag,
â€¢a â€˜redâ€™ variant (SHr) where all three agents are required, and
â€¢a â€˜penaltyâ€™ variant (SHp), where all three agents are required
and the temptation to defect is lowered (by setting ğ‘‡ = ğ‘ƒ).
Their respective payoff matrices are:
SHg =
ğ‘… ğ‘…
ğ‘… ğ‘†

,
ğ‘‡ ğ‘ƒ
ğ‘ƒ ğ‘ƒ

; SHr =
ğ‘… ğ‘†
ğ‘† ğ‘†

,
ğ‘‡ ğ‘ƒ
ğ‘ƒ ğ‘ƒ

;
SHp =
ğ‘… ğ‘†
ğ‘† ğ‘†

,
ğ‘ƒ ğ‘ƒ
ğ‘ƒ ğ‘ƒ

such that e.g. gSHg (c,d,c) = ğ‘…, but gSHr (c,d,c) = ğ‘†, with ğ‘… >
ğ‘‡ > ğ‘ƒ > ğ‘† assigned the integers from 1 to 4. The payoff matrix
for the three-player Ch has the same form as SHr, except with
ğ‘‡ > ğ‘… > ğ‘† > ğ‘ƒ(as per Â§2.1). A fifth and final experimental condition
is included, with an additional transition from SHg to SHr post-Ch.
The five time-series plots (one for each condition) in Fig. 4 show
superimposed series for 50 repeats. The bottom-right plot shows a
side-by-side comparison of the values of ğ”Š at ğ‘¡ = 1000 under each
condition.
We first look at the Ch period, where the ensemble starts at a
mixed equilibrium ( ğ‘¡ = 0) and reaches a pure equilibrium ( ğ‘¡ â‰¤
500). In the two-agent case, the ensemble tends toward one of cd
or dc fairly quickly, although in one of the trials it stays in the
mixed equilibrium through to the end. There are no underlying
configuration changes for Ch between the three-agent cases, so any
differences are caused by stochasticity. In the final Ch equilibrium,
invariably, one of the agents defects and the rest cooperate (up
to symmetry). Since ğ”Š is additive, it decreases for the two-player
condition, as only one of two agents has to choose the â€˜worseâ€™
action, compared to the increase under the three-player conditions,
where two-thirds of the ensemble select the â€˜worseâ€™ action. This is
an instance where the Nash Equilibrium is not socially optimal.
The final Ch equilibrium sets the prior conditions for the sub-
sequent SH game, amounting to a â€˜pre-equilibriumâ€™ [94, p. 3]. The
SH game has a risk-dominant equilibrium (RDE) with a higher ğ”Š
(i.e. worse overall), and a payoff-dominant equilibrium (PDE) with
a lower ğ”Š (i.e. better overall). In the SH2 and SHp conditions, we
see a bifurcation occur where a portion of the trials ends in each
equilibrium, with the majority of SH2 (resp. SHp) ending in the PDE
(resp. RDE). This shows the relative size of their basins of attraction
(noting that SH2 may need beyond ğ‘¡ = 1000 to converge further).
All the SHg trials end in the PDE; all of SHr, in the RDE. This is
somewhat paradoxical: when two are required to cooperate (SHg), all
three cooperate; conversely, when three are required to cooperate
(SHr), none cooperates. Interestingly, the SHp variant could be seen
as an attempt to direct the ensemble towards a better equilibrium
via penalising certain behaviour (cf. mechanism design), with mild
success. On the other hand, including an interim transition through
SHg generates trust and thus â€˜bootstrapsâ€™ cooperative behaviour,
stewarding the collective [5] towards the PDE without needing to
resort to penalties.
5 Conclusion
We have proposed a factorisation of the generative model of AIF
agents that brings the framework in closer alignment with game
theory, particularly for multi-agent interactions. In this factori-
sation, each agent has explicit, individual-level beliefs about the
internal states of others, and uses them to plan strategically in a
joint (game-theoretic) context. This allows ego to flexibly track
the internal states of others outside the joint interaction context,
and incorporate that information as required by the interaction.
This would be particularly useful when the agents involved in a
given interaction change or if the agent participates in multiple
interactions at a given time (cf. network games).
We have applied our proposed model to two- and three-agent
INFG and included transitions between games that the agents have
to adapt to. We have shown how the VFE and EFE can be used to
analyse the dynamics of ensembles of agents interacting strategi-
cally [7]â€”in particular, to highlight the equilibria of games and the
relative size of their attractorsâ€”, and provided an example where
this is used to motivate an intervention leading the ensemble to a
better outcome based on trust, rather than punishment. The use
of these measures may help in the conceptualization of groups
of agents as collective agents with self-organizing dynamics and
operational closure [47, 55, 79].
AIF and game theory applied to multi-agent systems offer a
rich theoretical and experimental landscape for exploring adaptive
behaviours in intelligent agent interactions. This intersection of
cognitive science and artificial intelligence not only provides in-
sights into individual decision-making but also paves the way for
understanding the collective dynamics that shape social behaviour
in complex environments.
Egoâ€™s beliefs about her own policy ğ‘(Ë†ğ‘¢ğ‘– )reflect a form of intro-
spection: inferring oneâ€™s internal mental states by observing oneâ€™s
actions [43, 85]. This is contrasted with interoception: the percep-
tion of internal bodily sensations, which would provide direct access
to internal states likeğ‘(Ë†ğ‘¢ğ‘– ). By endowing ego with interoceptive ac-
cess to ğ‘(Ë†ğ‘¢ğ‘– ), one could bypass the need for further inference about
her internal state in future steps of the model. Future work shall
explore how learning the observation model could capture the ra-
tionality of an opponent; the potential for more complex transition
models conditioned on the actions of all agents, or for modelling
other hidden variables such as opponent preferences [14, 84]; or the
effects of different EFE formulations [13, 40, 64] on game outcomes.
Acknowledgments
JRS is supported by an Australian Government Research Training
Program (RTP) Scholarship. We acknowledge the Gadi people of
the Eora nation as the traditional custodians of the land on which
The University of Sydney now stands, and that their sovereignty
was never ceded.
References
[1] Stefano V Albrecht, Jacob W Crandall, and Subramanian Ramamoorthy. 2016.
Belief and truth in hypothesised behaviours. Artificial Intelligence 235 (2016),
63â€“94.
[2] Stefano V Albrecht and Subramanian Ramamoorthy. 2015. A game-theoretic
model and best-response learning method for ad hoc coordination in multiagent
systems. arXiv preprint arXiv:1506.01170 (2015).
[3] Stefano V Albrecht and Peter Stone. 2018. Autonomous agents modelling other
agents: A comprehensive survey and open problems. Artificial Intelligence 258
(2018), 66â€“95.
[4] Robert Axelrod and William D Hamilton. 1981. The evolution of cooperation.
Science 211, 4489 (1981), 1390â€“1396.
[5] Joseph B Bak-Coleman, Mark Alfano, Wolfram Barfuss, Carl T Bergstrom,
Miguel A Centeno, Iain D Couzin, Jonathan F Donges, Mirta Galesic, Andrew S
Gersick, Jennifer Jacquet, Albert B Kao, Rachel E Moran, Pawel Romanczuk,
Daniel I Rubenstein, Kaia J Tombak, Jay J Van Bavel, and Elke U Weber. 2021.
Stewardship of Global Collective Behavior.Proceedings of the National Academy of
Sciences 118, 27 (July 2021), e2025764118. https://doi.org/10.1073/pnas.2025764118
[6] Chris Baker, Rebecca Saxe, and Joshua Tenenbaum. 2011. Bayesian theory of mind:
Modeling joint belief-desire attribution. In Proceedings of the Annual Meeting of
the Cognitive Science Society , Vol. 33.
[7] Wolfram Barfuss. 2022. Dynamical Systems as a Level of Cognitive Analysis
of Multi-Agent Learning. Neural Computing and Applications 34, 3 (Feb. 2022),
1653â€“1671. https://doi.org/10.1007/s00521-021-06117-0
[8] JosÃ© M Bernardo and Adrian FM Smith. 2009. Bayesian theory . Vol. 405. John
Wiley & Sons.
[9] George W Brown. 1951. Iterative Solution of Games by Fictitious Play. InActivity
Analysis of Production and Allocation , Tjalling C. Koopmans (Ed.). John Wiley &
Sons, New York, 374â€“376.
[10] Colin F Camerer. 2011.Behavioral game theory: Experiments in strategic interaction .
Princeton University Press.
[11] David Carmel and Shaul Markovitch. 1999. Exploration strategies for model-
based learning in multi-agent systems: Exploration strategies.Autonomous Agents
and Multi-agent systems 2 (1999), 141â€“172.
[12] Georgios Chalkiadakis and Craig Boutilier. 2003. Coordination in multiagent
reinforcement learning: A Bayesian approach. InProceedings of the Second Interna-
tional Joint Conference on Autonomous Agents and Multiagent Systems . 709â€“716.
[13] ThÃ©ophile Champion, Howard Bowman, Dimitrije MarkoviÄ‡, and Marek GrzeÅ›.
2024. Reframing the Expected Free Energy: Four Formulations and a Unification.
arXiv:2402.14460 [cs]
[14] Alex J Chan and M Schaar. 2021. Scalable Bayesian Inverse Reinforcement
Learning. In ICLR.
[15] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu,
and Karl J Friston. 2020. Active Inference on Discrete State-Spaces: A Synthesis.
Journal of Mathematical Psychology 99 (Dec. 2020), 102447. https://doi.org/10.
1016/j.jmp.2020.102447
[16] Eddie Dekel, Drew Fudenberg, and David K. Levine. 2004. Learning to play
Bayesian games. Games and Economic Behavior 46, 2 (2004), 282â€“303.
[17] Daphne Demekas, Conor Heins, and Brennan Klein. 2024. An Analytical Model
of Active Inference in the Iterated Prisonerâ€™s Dilemma. InActive Inference, Christo-
pher L. Buckley, Daniela Cialfi, Pablo Lanillos, Maxwell Ramstead, Noor Sajid,
Hideaki Shimazaki, Tim Verbelen, and Martijn Wisse (Eds.). Springer Nature
Switzerland, Cham, 145â€“172. https://doi.org/10.1007/978-3-031-47958-8_10
[18] Prashant Doshi, Piotr Gmytrasiewicz, and Edmund Durfee. 2020. Recursively
modeling other agents for decision making: A research perspective. Artificial
Intelligence 279 (2020), 103202.
[19] Michael Oâ€™Gordon Duff. 2002. Optimal Learning: Computational procedures for
Bayes-adaptive Markov decision processes . University of Massachusetts Amherst.
[20] Matthew Fellows, Anuj Mahajan, Tim GJ Rudner, and Shimon Whiteson. 2019.
VIREL: A variational inference framework for reinforcement learning. Advances
in Neural Information Processing Systems 32 (2019).
[21] Chris Fields and James F. Glazebrook. 2024. Nash Equilibria and Undecidability
in Generic Physical Interactionsâ€”A Free Energy Perspective. Games 15, 5 (Oct.
2024), 30. https://doi.org/10.3390/g15050030
[22] Ismael T. Freire, X. Arsiwalla, J. PuigbÃ², and P. Verschure. 2019. Modeling Theory
of Mind in Multi-Agent Games Using Adaptive Feedback Control. ArXiv (2019).
[23] Karl J Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck,
Giovanni Pezzulo, et al. 2016. Active inference and learning. Neuroscience &
Biobehavioral Reviews 68 (2016), 862â€“879.
[24] Karl J Friston, Conor Heins, Tim Verbelen, Lancelot Da Costa, Tommaso Salvatori,
Dimitrije Markovic, Alexander Tschantz, Magnus Koudahl, Christopher Buckley,
and Thomas Parr. 2024. From Pixels to Planning: Scale-Free Active Inference.
https://doi.org/10.48550/arXiv.2407.20292 arXiv:2407.20292 [cs, q-bio]
[25] Karl J Friston, Thomas Parr, Conor Heins, Axel Constant, Daniel Friedman,
Takuya Isomura, Chris Fields, Tim Verbelen, Maxwell Ramstead, John Clip-
pinger, and Christopher D. Frith. 2024. Federated Inference and Belief Shar-
ing. Neuroscience & Biobehavioral Reviews 156 (Jan. 2024), 105500. https:
//doi.org/10.1016/j.neubiorev.2023.105500
[26] Karl J Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas
Fitzgerald, and Giovanni Pezzulo. 2015. Active Inference and Epistemic Value.
Cognitive Neuroscience 6, 4 (Oct. 2015), 187â€“214. https://doi.org/10.1080/17588928.
2015.1020053
[27] Drew Fudenberg and David K Levine. 1993. Self-confirming equilibrium. Econo-
metrica: Journal of the Econometric Society (1993), 523â€“545.
[28] Drew Fudenberg and David K Levine. 1995. Consistency and cautious fictitious
play. Journal of Economic Dynamics and Control 19, 5-7 (1995), 1065â€“1089.
[29] Drew Fudenberg and David K. Levine. 1998. The theory of learning in games .
Vol. 2. MIT Press.
[30] Drew Fudenberg and David K Levine. 1999. Conditional universal consistency.
Games and Economic Behavior 29, 1-2 (1999), 104â€“130.
[31] Drew Fudenberg and Jean Tirole. 1991. Repeated games. In Game Theory . MIT
Press, Cambridge, Massachusetts, 150â€“192.
[32] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. 2015.
Bayesian reinforcement learning: a survey. Foundations and Trends Â® in Machine
Learning 8, 5-6 (2015), 359â€“483.
[33] Herbert Gintis. 2006. The Foundations of Behavior: The Beliefs, Preferences,
and Constraints Model. Biological Theory 1, 2 (June 2006), 123â€“127. https:
//doi.org/10.1162/biot.2006.1.2.123
[34] Piotr J Gmytrasiewicz and Prashant Doshi. 2004. Interactive POMDPs: Proper-
ties and preliminary results. In International Conference on Autonomous Agents:
Proceedings of the Third International Joint Conference on Autonomous Agents and
Multiagent Systems , Vol. 3. 1374â€“1375.
[35] Piotr J Gmytrasiewicz and Prashant Doshi. 2005. A framework for sequential
planning in multi-agent settings. Journal of Artificial Intelligence Research 24
(2005), 49â€“79.
[36] Piotr J Gmytrasiewicz and Edmund H Durfee. 2000. Rational coordination in
multi-agent environments. Autonomous Agents and Multi-Agent Systems 3 (2000),
319â€“350.
[37] Sebastian Gottwald and Daniel A Braun. 2020. The Two Kinds of Free Energy
and the Bayesian Revolution. PLOS Computational Biology 16, 12 (Dec. 2020),
e1008420. https://doi.org/10.1371/journal.pcbi.1008420
[38] Peter D GrÃ¼nwald and A Philip Dawid. 2004. Game theory, maximum entropy,
minimum discrepancy and robust Bayesian decision theory.the Annals of Statistics
32, 4 (2004), 1367â€“1433.
[39] Arthur Guez, David Silver, and Peter Dayan. 2012. Efficient Bayes-adaptive rein-
forcement learning using sample-based search. Advances in Neural Information
Processing Systems 25 (2012).
[40] Danijar Hafner, Pedro A Ortega, Jimmy Ba, Thomas Parr, Karl J Friston, and
Nicolas Heess. 2022. Action and Perception as Divergence Minimization. https:
//doi.org/10.48550/arXiv.2009.01791 arXiv:2009.01791 [cs, math, stat]
[41] James Hannan. 1957. Approximation to Bayes Risk in Repeated Play.Contributions
to the Theory of Games 3 (1957), 97â€“139.
[42] Michael S HarrÃ©. 2018. Multi-Agent Economics and the Emergence of Critical
Markets. arXiv:1809.01332 [nlin, q-fin]
[43] Michael S HarrÃ©. 2022. What Can Game Theory Tell Us about an AI â€˜Theory of
Mindâ€™? Games 13, 3 (June 2022), 46. https://doi.org/10.3390/g13030046
[44] John C Harsanyi. 1967. Games with incomplete information played by â€œBayesianâ€
players, Iâ€“III Part I. The basic model. Management science 14, 3 (1967), 159â€“182.
[45] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz
De Cote. 2017. A survey of learning in multiagent environments: Dealing with
non-stationarity. arXiv preprint arXiv:1707.09183 (2017).
[46] Trong Nghia Hoang and Kian Hsiang Low. 2013. A general framework for inter-
acting Bayes-optimally with self-interested agents using arbitrary parametric
model and model prior. arXiv preprint arXiv:1304.2024 (2013).
[47] David Hyland, TomÃ¡Å¡ GavenÄiak, Lancelot Da Costa, Conor Heins, Vojtech Ko-
varik, Julian Gutierrez, Michael J. Wooldridge, and Jan Kulveit. 2024. Free-Energy
Equilibria: Toward a Theory of Interactions Between Boundedly-Rational Agents.
In ICML 2024 Workshop on Models of Human Feedback for AI Alignment .
[48] Edwin T Jaynes. 2003. Probability theory: The logic of science . Cambridge Univer-
sity Press.
[49] Albert Jiang and Kevin Leyton-Brown. 2010. Bayesian action-graph games.
Advances in Neural Information Processing Systems 23 (2010).
[50] Albert Xin Jiang, Kevin Leyton-Brown, and Navin AR Bhat. 2011. Action-graph
games. Games and Economic Behavior 71, 1 (2011), 141â€“173.
[51] James S Jordan. 1991. Bayesian learning in normal form games. Games and
Economic Behavior 3, 1 (1991), 60â€“81.
[52] Ehud Kalai and Ehud Lehrer. 1993. Rational learning leads to Nash equilibrium.
Econometrica: Journal of the Econometric Society (1993), 1019â€“1045.
[53] Hilbert J Kappen, VicenÃ§ GÃ³mez, and Manfred Opper. 2012. Optimal control as a
graphical model inference problem. Machine learning 87 (2012), 159â€“182.
[54] Michael Kearns, Michael L Littman, and Satinder Singh. 2013. Graphical models
for game theory. arXiv preprint arXiv:1301.2281 (2013).
[55] Julian Kiverstein, Michael D Kirchhoff, and Tom Froese. 2022. The Problem of
Meaning: The Free Energy Principle and Artificial Agency. Frontiers in Neuro-
robotics 16 (June 2022). https://doi.org/10.3389/fnbot.2022.844773
[56] Daphne Koller and Brian Milch. 2001. Structured models for multi-agent interac-
tions. In Proceedings of the 8th Conference on Theoretical Aspects of Rationality
and Knowledge . 233â€“248.
[57] Daphne Koller and Brian Milch. 2003. Multi-agent influence diagrams for repre-
senting and solving games. Games and Economic Behavior 45, 1 (2003), 181â€“221.
[58] Pierfrancesco La Mura and Yoav Shoham. 2013. Expected utility networks. arXiv
preprint arXiv:1301.6714 (2013).
[59] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc Aurelio Ranzato, and Fu Jie
Huang. 2006. A Tutorial on Energy-Based Learning. InPredicting Structured Data ,
G. Bakir, T. Hofman, B. Scholkopt, A. Smola, and B. Taskar (Eds.). MIT Press.
[60] Sergey Levine. 2018. Reinforcement learning and control as probabilistic infer-
ence: Tutorial and review. arXiv preprint arXiv:1805.00909 (2018).
[61] Tao Li, Yuhan Zhao, and Quanyan Zhu. 2022. The role of information structures
in game-theoretic multi-agent learning. Annual Reviews in Control 53 (2022),
296â€“314.
[62] Marlize Lombard and Peter GÃ¤rdenfors. 2023. Causal cognition and theory
of mind in evolutionary cognitive archaeology. Biological Theory 18, 4 (2023),
234â€“252.
[63] Richard D McKelvey and Thomas R Palfrey. 1995. Quantal response equilibria
for normal form games. Games and Economic Behavior 10, 1 (1995), 6â€“38.
[64] Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. 2021. Whence
the Expected Free Energy? Neural Computation 33, 2 (Feb. 2021), 447â€“482. https:
//doi.org/10.1162/neco_a_01354
[65] Michael Moutoussis, Nelson Trujillo-Barreto, Wael El-Deredy, Raymond Dolan,
and Karl J Friston. 2014. A Formal Model of Interpersonal Inference. Frontiers in
Human Neuroscience 8 (2014).
[66] John H Nachbar. 2005. Beliefs in repeated games. Econometrica 73, 2 (2005),
459â€“480.
[67] Brenda Ng, Kofi Boakye, Carol Meyers, and Andrew Wang. 2012. Bayes-adaptive
interactive POMDPs. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, Vol. 26. 1408â€“1414.
[68] Yaw Nyarko. 1994. Bayesian learning leads to correlated equilibria in normal
form games. Economic Theory 4 (1994), 821â€“841.
[69] Brendan Oâ€™Donoghue. 2021. Variational Bayesian reinforcement learning with
regret bounds. Advances in Neural Information Processing Systems 34 (2021),
28208â€“28221.
[70] Brendan Oâ€™Donoghue, Ian Osband, and Catalin Ionescu. 2020. Making sense of re-
inforcement learning and probabilistic inference. arXiv preprint arXiv:2001.00805
(2020).
[71] Daniel A Ortega and Pedro A Braun. 2011. Information, utility and bounded
rationality. In Artificial General Intelligence: 4th International Conference, AGI
2011, Mountain View, CA, USA, August 3-6, 2011. Proceedings 4 . Springer, 269â€“274.
[72] Pedro Ortega and Daniel Lee. 2014. An adversarial interpretation of information-
theoretic bounded rationality. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 28.
[73] Pedro A Ortega and Daniel A Braun. 2009. A Conversion between Utility and
Information. https://doi.org/10.48550/arXiv.0911.5106 arXiv:0911.5106 [cs, math]
[74] Pedro A Ortega and Daniel A Braun. 2013. Thermodynamics as a theory of
decision-making with information-processing costs. Proceedings of the Royal Soci-
ety A: Mathematical, Physical and Engineering Sciences 469, 2153 (2013), 20120683.
[75] Pedro A Ortega, Daniel A Braun, Justin Dyer, Kee-Eung Kim, and Naftali Tishby.
2015. Information-Theoretic Bounded Rationality. arXiv:1512.06789 [cs, math,
stat]
[76] Alessandro Panella and Piotr Gmytrasiewicz. 2017. Interactive POMDPs with
finite-state models of other agents. Autonomous Agents and Multi-Agent Systems
31 (2017), 861â€“904.
[77] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. 2022. Active Inference: The
Free Energy Principle in Mind, Brain, and Behavior . The MIT Press. https:
//doi.org/10.7551/mitpress/12441.001.0001
[78] Judea Pearl. 2014. Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference . Elsevier.
[79] Maxwell JD Ramstead, Michael D Kirchhoff, Axel Constant, and Karl J Friston.
2021. Multiscale Integration: Beyond Internalism and Externalism. Synthese 198,
1 (Jan. 2021), 41â€“70. https://doi.org/10.1007/s11229-019-02115-x
[80] Iead Rezek, David S Leslie, Steven Reece, Stephen J Roberts, Alex Rogers, Ra-
jdeep K Dash, and Nicholas R Jennings. 2008. On Similarities between Inference
in Game Theory and Machine Learning. Journal of Artificial Intelligence Research
33 (Oct. 2008), 259â€“283. https://doi.org/10.1613/jair.2523
[81] Marc Rigter, Bruno Lacerda, and Nick Hawes. 2021. Risk-averse Bayes-adaptive
reinforcement learning. Advances in Neural Information Processing Systems 34
(2021), 1142â€“1154.
[82] Julia Robinson. 1951. An iterative method of solving a game. Annals of Mathe-
matics 54, 2 (1951), 296â€“301.
[83] Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. 2007. Bayes-adaptive
POMDPs. Advances in Neural Information Processing Systems 20 (2007).
[84] Jaime Ruiz-Serra and Michael S HarrÃ©. 2023. Inverse Reinforcement Learning as
the Algorithmic Basis for Theory of Mind: Current Methods and Open Problems.
Algorithms 16, 2 (Feb. 2023), 68. https://doi.org/10.3390/a16020068
[85] Lars Sandved-Smith, Casper Hesp, JÃ©rÃ©mie Mattout, Karl J Friston, Antoine Lutz,
and Maxwell J D Ramstead. 2021. Towards a Computational Phenomenology of
Mental Action: Modelling Meta-Awareness and Attentional Control with Deep
Parametric Active Inference. Neuroscience of Consciousness 2021, 1 (Jan. 2021),
niab018. https://doi.org/10.1093/nc/niab018
[86] Leonard J. Savage. 1954. The foundations of statistics . Wiley, New York.
[87] Jeff S Shamma and GÃ¼rdal Arslan. 2005. Dynamic fictitious play, dynamic gradient
play, and distributed convergence to Nash equilibria.IEEE Trans. Automat. Control
50, 3 (2005), 312â€“327.
[88] Yoav Shoham, Rob Powers, and Trond Grenager. 2007. If Multi-Agent Learning
Is the Answer, What Is the Question? Artificial Intelligence 171, 7 (May 2007),
365â€“377. https://doi.org/10.1016/j.artint.2006.02.006
[89] Ryan Smith, Karl J Friston, and Christopher J Whyte. 2022. A Step-by-Step
Tutorial on Active Inference and Its Application to Empirical Data. Journal of
Mathematical Psychology 107 (April 2022), 102632. https://doi.org/10.1016/j.jmp.
2021.102632
[90] Finnegan Southey, Michael P Bowling, Bryce Larson, Carmelo Piccione, Neil
Burch, Darse Billings, and Chris Rayner. 2012. Bayesâ€™ bluff: Opponent modelling
in poker. arXiv preprint arXiv:1207.1411 (2012).
[91] David H Wolpert. 2006. Information Theory â€” The Bridge Connecting Bounded
Rational Game Theory and Statistical Physics. In Complex Engineered Systems:
Science Meets Technology , Dan Braha, Ali A. Minai, and Yaneer Bar-Yam (Eds.).
Springer, Berlin, Heidelberg, 262â€“290. https://doi.org/10.1007/3-540-32834-3_12
[92] Sarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C
Parkes, and Max Kleiman-Weiner. 2021. Too many cooks: Bayesian inference for
coordinating multi-agent collaboration. Topics in Cognitive Science 13, 2 (2021),
414â€“432.
[93] Wako Yoshida, Ray J Dolan, and Karl J Friston. 2008. Game Theory of Mind.
PLOS Computational Biology 4, 12 (Dec. 2008), e1000254. https://doi.org/10.1371/
journal.pcbi.1000254
[94] H Peyton Young. 2004. The Interactive Learning Problem. In Strategic Learning
and Its Limits , H Peyton Young (Ed.). Oxford University Press, 0. https://doi.org/
10.1093/acprof:oso/9780199269181.003.0001
[95] H Peyton Young. 2004. Strategic learning and its limits . Oxford University Press.