LExI: Layer-Adaptive Active Experts for
Efficient MoE Model Inference
Krishna Teja Chitty-Venkata, Sandeep Madireddy, Murali Emani, Venkatram Vishwanath
schittyvenkata@anl.gov, smadireddy@anl.gov, memani@anl.gov, venkat@anl.gov @anl.gov
Argonne National Laboratory, Lemont, IL 60439, USA
Abstract
Mixture-of-Experts (MoE) models scale efficiently by activating only a subset of
experts per token, offering a computationally sparse alternative to dense archi-
tectures. While prior post-training optimizations, such as inter- and intra-expert
pruning, reduce memory usage but provide limited gains in inference-time compute
efficiency. Moreover, existing MoE architectures typically activate a fixed number
of experts uniformly across all layers, resulting in redundant computation and
suboptimal performance. In this work, we first demonstrate that MoE pruning
strategies improve only the memory footprint but do not significantly improve
inference performance on GPU using optimized frameworks such as vLLM. To
address this, we introduce LExI, a data-free optimization technique that determines
the optimal number of active experts per layer in a pretrained MoE model. LExI
leverages only the model‚Äôs weights to estimate the relative importance of each
layer and adaptively assigns the number of active experts accordingly per layer.
Experiments on state-of-the-art language and vision MoE benchmarks demonstrate
that LExI significantly outperforms traditional MoE pruning approaches in terms
of inference efficiency with negligible accuracy loss. For example, using LExI,
Qwen1.5-MoE achieves the same throughput on Nvidia H100 GPU with 10%
better accuracy than traditional expert pruning.
1 Introduction
Large Language Models (LLMs) and Vision Language Models (VLMs) have achieved remarkable
performance through model scaling, but require tremendous compute and memory resources. Mixture-
of-Experts (MoE) models have emerged as a promising approach to increase model capacity without
a proportional rise in inference cost. In an MoE, multiple expert subnetworks are trained, and a
sparse gating or router network activates only a small subset of experts (k experts) per input token.
This sparse computation allows MoEs to outperform dense models with the same number of active
model parameters. Prominent examples of MoE-based LLM include Mixtral (Jiang et al. [2024]),
Qwen1.5-MoE-A2.7B (Team [2024]), OLMoE (Muennighoff et al. [2024]), and Vision Language
Models (VLM) such as MolmoE (Deitke et al. [2024]) and DeepSeek-VL2 (Wu et al. [2024]).
One commonly used post-training optimization strategy for MoE models is expert pruning, which
removes redundant experts. Recent methods such as NAEE (Lu et al. [2024]), MoE-Pruner (Xie et al.
[2024]) EEP (Liu et al. [2024b]) and MoE-I2 (Yang et al. [2024]) introduce various pruning strategies.
For example, NAEE identifies and removes entire experts from a pretrained MoE model, while
MoE-I2 prunes the inner dimensions within each expert‚Äôs MLP. Although these methods reduce the
memory footprint, our performance evaluation across several state-of-the-art MoE models using the
widely adopted vLLM inference framework reveals a critical limitation:pruning does not consistently
translate into faster inference, and in some cases, it even degrades performance. This degradation
is primarily due to the sparse structure of the MoE models itself. In expert pruning, the input token
still needs to be routed to the same number of top- k experts, as determined by the router. While
some experts are pruned, the remaining ones must process a disproportionately larger number of
tokens, increasing their computational load. In the batched inference scenario, this load imbalance
can lead to longer processing time per expert, thereby increasing overall latency. While aggressive
Preprint. Under review.
arXiv:2509.02753v1  [cs.LG]  2 Sep 2025
w1 w3 w1 w3 w1 w3
(a) Baseline
Inactive Expert Pruned WeightSkipped ExpertActive Expert
(b) Unstructured Expert Pruning (c) Inter Expert and Gate Pruning 
w1 w3 If w1 > ùõÉw3 
(d) Dynamic Expert Skipping (f) Varying TopK Per Layer (Ours)
w1 w3
(e) Intra Expert Pruning
Gating NetworkInput
Layer i 
Layer j 
Layer k 
Figure 1: Overview of MoE Optimization Methods. (a) Baseline Trained Model (b) Unstructured
Expert Pruning (SparseGPT (Frantar and Alistarh [2023]), Wanda (Sun et al. [2023]), MoE-Pruner
(Xie et al. [2024])) (c) Inter Expert Pruning (NAEE (Lu et al. [2024])) (d) Dynamic Expert Skipping
(NAEE (Lu et al. [2024])) (e) Intra Expert Pruning (MoE-I2 (Yang et al. [2024])) (f) LExI: Static
Varying Active Experts (Topk) Per Layer (Ours)
expert pruning levels can yield noticeable speedups, they typically result in significant accuracy
degradation, making them impractical. Moreover, these expert pruning approaches usually rely on
training data for pruning experts. Current MoE architectures employ a fixed top-k routing mechanism,
where the same number of experts are activated for each input token across all layers. This static
design is suboptimal as different layers may require varying levels of expert capacity depending on
computational needs (Guo et al. [2024]). Beyond computation, inter-GPU communication overhead
also becomes a significant performance bottleneck in MoEs. Increasing the number of active experts
per token increases the volume of communication operations such as all-reduce and broadcast, further
adding to the inference cost. To address some of these limitations, NAEE (Lu et al. [2024]) proposed
a token-aware dynamic expert skipping strategy, which selectively skips an expert during inference.
However, this strategy is highly tailored to the dataset and cannot work beyond top-k=2. In summary,
existing MoE optimization strategies often rely on calibration sets for pruning or routing adjustments,
making them unsuitable in deployment settings where access to data or retraining is infeasible. In
addition, the dataset-driven solutions will make pruned models more optimized to that calibration
dataset, potentially degrading the performance in unseen settings.
Motivated by these insights, we propose LExI, a novel post-training layer-adaptive active expert
allocation mechanism that determines the optimal number of active experts per layer without depend-
ing on any dataset. Our method relies on the key observation that not all layers contribute equally
to the final model performance and that expert redundancy varies significantly across depth. This
raises a fundamental question: Can we reduce the number of active experts per layer irrespective of
the input token without sacrificing accuracy? In particular, is it possible to statically assign different
top-k values to each layer, so that every layer uses just enough experts to retain its contribution,
while improving overall inference efficiency? By making layer-adaptive expert allocation decisions,
LExI reduces computational overhead across the model irrespective of the input token, offering a
more efficient alternative to the traditional fixed top-K routing. Our experiments show that LExI
outperforms existing expert pruning techniques in both task performance and runtime efficiency.
By reducing the average number of activated experts per layer, LExI reduces latency and memory
bandwidth usage while maintaining competitive task accuracy across diverse tasks.
Contributions. Our key contributions are as follows:
‚Ä¢ We introduce LExI, a novel dataset-free optimization technique for static active expert assignment
in pretrained MoE models. LExI is simple to implement and serves as an efficient, plug-and-play
solution for inference across various frameworks.
2
‚Ä¢ We propose a data-free profiling strategy to estimate the sensitivity of each expert using only model
expert weights. LExI combines this one-time profiling with evolutionary search to determine the
optimal active experts layer in a computationally efficient manner.
‚Ä¢ Unlike prior methods that demonstrate improvements on a narrow set of MoE models (Mixtral-
8x7B), our approach generalizes across multiple state-of-the-art MoE architectures in both language
and vision domains.
‚Ä¢ We empirically show that expert pruning in MoE models does not significantly improve inference
performance, and in some cases, can degrade it due to architectural sparsity and load imbalance.
Our method provides a viable alternative to pruning, improving both accuracy and hardware
efficiency without requiring retraining or access to calibration data.
2 Background and Related Work
Mixture of Experts. Mixture-of-Experts (MoE) architectures improve the scalability and efficiency
of LLMs/VLMs by introducing sub networks or experts. For a given input x, the output y of an MoE
module is computed as a weighted sum over all the active top-k experts: y = Ptop‚àík
i=1 G(x)i ¬∑ Ei(x),
where G(x) := Softmax(TopK[x ¬∑ Wg]). The TopK[¬∑] function selects the top-k experts with the
highest gating scores, and Softmax normalizes their scores into a probability distribution.G(x) ‚àà RN
is the gating vector representing the importance weights assigned to each expert in the top-k selected
ones, and Ei(x) denotes the output of the i-th expert given input x. Each expert Ei is typically an
FFN and constitutes the dominant portion of the parameters model (e.g., up to 96% in Mixtral).
Pruning Large Language Models. Model pruning is a well-established technique to reduce inference
costs by removing less important parameters. Recent works such as SparseGPT (Frantar and Alistarh
[2023]) and Wanda (Sun et al. [2023]) demonstrated one-shot pruning methods that introduce
unstructured or semi-structured sparsity in weight matrices, cutting up to 50% of parameters in
GPT-scale models with minimal perplexity loss. These approaches solve layer-wise reconstruction
or use weight magnitude heuristics to remove weights, and have been applied to models as large
as GPT-175B. However, the irregular weight sparsity pattern they induce often requires specialized
hardware support to realize actual runtime speedups (Zhou et al. [2021]), and may suffer degraded
efficiency on general-purpose accelerators.
Expert Pruning and Compression in MoE Models. Since MoE models typically allocate the vast
majority of their parameters to the expert sub-networks, pruning even a subset of experts can lead to
substantial memory savings. NAEE (Lu et al. [2024]) is a post-training expert pruning framework
to permanently remove unimportant experts without needing to re-train the model. By evaluating
each expert‚Äôs contribution to the model‚Äôs output on a small calibration set, NAEE identifies and
permanently prunes the least significant experts. Furthermore, NAEE also introduced an inference-
time policy to dynamically skip experts for certain tokens on the fly, effectively adjusting the active
expert count based on the input token. Another recent approach is MoE-I2 (Yang et al. [2024]),
which introduces a two-stage compression pipeline tailored for MoEs. In the inter-expert pruning
stage, MoE-I2 performs a layer-wise analysis to prune a fraction of experts to prune. The authors
also introduce intra-expert compression to reduce the inner dimensionality of an expert‚Äôs FFN. These
advances in MoE-specific pruning highlight the growing interest in expert-level model trimming.
Our proposed LExI method shares the overarching goal of exploiting expert redundancy to improve
efficiency. However, rather than relying on static pruning, it focuses on adaptive expert utilization
at inference time, offering a flexible and data-free alternative that preserves task performance while
reducing computational cost.
3 Experimental Setup and MoE Expert latency Profiling
In this section, we evaluate the hardware performance of the MoE benchmarks to motivate our
varying top-k solution. We first provide a detailed experimental setup used in all our evaluations.
Mixture-of-Experts Benchmarks. We evaluate our method across a diverse set of MoE models
spanning both language and vision-language domains. For LLMs, we consider Mixtral-8x7B-
Instruct (Jiang et al. [2024]), Qwen1.5-MoE-A2.7B-Chat (Team [2024]), OLMoE-1B-7B-0924-
Instruct (Muennighoff et al. [2024]), MiniCPM-MoE-8x2B (Hu et al. [2024]), and DeepSeek-V2-Lite-
Chat (Liu et al. [2024a]). For VLMs, we use DeepSeekVL2-Tiny model (Wu et al. [2024]). These
models exhibit a wide range of MoE architectures, varying number of experts and active experts per
token, enabling a robust evaluation of our proposed method across heterogeneous settings.
Evaluation Benchmarks. We benchmark LLMs on nine widely adopted language understanding
tasks from the lm-eval (Gao et al. [2024]) suite: ARC-c (Clark et al. [2018]), ARC-e (Clark et al.
3
Baseline
 
12.5% Inter Pruning
12.5% Intra Pruning
25% Inter Pruning
25% Intra Pruning
50% Inter Pruning
50% Intra Pruning
1 2
Active Experts (T opk)
825
850
875
900
925
950
975Throughput (T ok/Sec)
MiniCPM-MoE-8x2B 
 (#Experts = 8)
(a) MiniCPM-MoE-8x2
1 2
Active Experts (T opk)
2000
2200
2400
2600
2800Throughput (T ok/Sec)
Mixtral-8x7B 
 (#Experts = 8) (b) Mixtral-8x7B
1 2 3 4
Active Experts (T opk)
3800
4000
4200
4400
4600Throughput (T ok/Sec)
Qwen1.5-MoE 
 (#Experts = 60) (c) Qwen1.5-MoE-A2.7B
1 2 3 4 5 6 7 8
Active Experts (T opk)
6800
7000
7200
7400
7600
7800Throughput (T ok/Sec)
OLMoE-1B-7B 
 (#Experts = 64)
(d) OlmoE-1b-7b-0924
1 2 3 4 5 6
Active Experts (T opk)
2.8
3.0
3.2
3.4Throughput (K T ok/Sec)
1e7
DeepSeek-V2-Lite 
 (#Experts = 64) (e) DeepSeek-V2-Lite
1 2 3 4 5 6
Active Experts (T opk)
13
14
15
16
17
18
19Throughput (Samples/Sec)
Deepseek-VL2-Tiny 
 (#Experts = 64) (f) DeepSeekVL2-tiny
Figure 2: Throughput vs. Active Experts under Inter and Intra Expert Pruning
[2018]), BoolQ (Clark et al. [2019]), HellaSwag (Zellers et al. [2019]), MMLU (Hendrycks et al.
[2021]), OpenBookQA (Mihaylov et al. [2018]), RTE (Wang et al. [2018]), WinoGrande (Sakaguchi
et al. [2019]). We report average accuracy across all these tasks to assess general-purpose language
reasoning. For long content understanding, we employ Qasper dataset (Dasigi et al. [2021]) from the
LongBench suite (Bai et al. [2024]) and report the F1 score as the evaluation metric. Additionally, we
include a passkey retrievaltask (Peng et al. [2023]), where the accuracy is measured as the percentage
of instances (over 100 iterations and varying depths) in which the model correctly identifies a passkey
from the garbage context. To evaluate language modeling quality, we computeperplexity on the C4
(Dodge et al. [2021]), PTB (Marcus et al. [1993]), and WikiText-103(Merity et al. [2016]) datasets.
For vision-language models, we use three benchmarks from the VLMEvalKit (Duan et al. [2024])
suite: MME (Yin et al. [2023]), MMMU Yue et al. [2024], andScienceQA (Lu et al. [2022]). These
datasets span a broad spectrum of multimodal reasoning tasks and allow us to evaluate our method‚Äôs
effectiveness in VLM setting.
Hardware and Software Setup. All inference performance evaluations are conducted on NVIDIA
H100 GPUs with 80GB of HBM memory per GPU, supporting Tensor Cores for optimized matrix
operations. We use vLLM (Kwon et al. [2023]) as our inference engine which is a high-performance
framework with native support for MoE models via FusedMoE, which fuses expert computation and
routing to improve efficiency. Unless otherwise specified, all LLMs are deployed on 4 GPUs, while
DeepSeek-V2-Lite-Chat and DeepSeekVL2-Tiny use 2 GPUs. We employ tensor parallelism across
devices for all models. During inference, we use a batch size of 16, with input and output sequence
lengths varied across models to comply with each model‚Äôs maximum context length constraints. We
report throughput as our primary hardware performance metric, defined as the total number of tokens
(input + output) processed per second. To calculate this, we first measure the end-to-end latency,
defined as the time elapsed from input prompt submission to the generation of the final output token,
and then convert this latency into throughput. The metric for VLMs is the number of input (image +
text) samples process per second.
Inter and Intra Expert Pruning. Inter-pruning (Lu et al. [2024]) removes entire experts and their
routing weights, reducing memory footprint while maintaining the same number of active experts
per token during inference. Intra-pruning (Yang et al. [2024]) targets the inner dimensions (FFN
intermediate size) within each expert, preserving the expert count while reducing individual expert
complexity. In our evaluation, we consider the following percentages of these pruning: {12.5%, 25%,
50%}. 12.5% inter pruning removes 1/8th of the experts in each layer, whereas 25% intra pruning
prunes 1/4th of the FFN dimension in each expert of each layer. The top-k search space in our paper
includes every integer from 1 up to the baseline pretrained top-k: 1, 2, . . . ,top-kbaseline.
Figure 2 illustrates the throughput of the six benchmark MoEs under varying degrees of pruning
and top-k. Models with fewer active experts (e.g., MiniCPM, Mixtral) show marginal gains with
4
aggressive pruning, while models with more active experts (e.g., Qwen, OLMoE) exhibit complex
interactions where pruning can improve or degrade throughput depending on token-to-expert routing
balance and compute saturation. Notably, DeepSeek-VL2-Tiny shows throughput instability under
pruning, suggesting higher sensitivity to expert load balance. The performance degradation stems
from load imbalance across experts, leading to an increased number of tokens processed by each
active expert.
4 LExI
LExI implements a two-stage pipeline to determine the optimal number of active experts per
layer. In the first stage, it performs a one-time profiling to assess each layer‚Äôs sensitivity to different
top-k values. The sensitivity profiling methodology extends beyond expert allocation, serving as a
foundation for diverse optimization problems such as layer-specific mixed-precision quantization or
layer-wise pruning. The second stage employs a low-cost evolutionary search algorithm leveraging
these sensitivity values as efficient proxies to identify the best performing top-k for each layer.
Stage 1: Per Layer MoE Top-K Perturbation Profiling: Algorithm 1 outlines our Monte Carlo-
based method to evaluate the sensitivity of each MoE layer under varying top-k configurations. For
each layer, we sample a random synthetic input tensorX ‚àº N(0, 1)B√óL√óH from the standard normal
distribution. We first compute the baseline output using the default top-k configuration, followed by
computing outputs for each top-k in the target search space. The perturbation induced by each top-k
is quantified using the Frobenius norm between the baseline output and the corresponding perturbed
output. This process is repeated over millions of random input samples to obtain a statistically robust
estimate of the average deviation for each candidate top-k. The Frobenius norm serves as a precise
metric for capturing output magnitude shifts in high-dimensional space, while Monte Carlo sampling
ensures diverse inputs. This sensitivity analysis provides a principled estimate of how active expert
selection affects per-layer behavior.
Algorithm 1: LExI Stage 1: Per Layer Top-k Perturbation Loss Computation
Input:
Mmoe: Mixture of Experts module (Gate G and Experts E) T: List of target top-k values
kbase: Baseline Pretrained top-k B : Batch size
H: Hidden Size L: Sequence length
Niter: Number of iterations
Output: Average Frobenius Norm per top-k
D ‚Üê {k : ‚àÖ ‚àÄk ‚àà T};
for i ‚Üê 1 to Niter do
Sample a input tensor from Normal Distribution: X ‚àº N(0, 1)B√óL√óH;
UpdateTopk(Mmoe, kbase);
Ybase ‚Üê fmoe(X)
foreach k ‚àà T do
UpdateTopk(Mmoe, k);
Yj
perturbed ‚Üê fmoe(X);
‚àÜ ‚Üê ‚à•Yj
perturbed ‚àí Ybase‚à•F ;
D[k] ‚Üê D[k] ‚à™ {‚àÜ}
foreach k ‚àà T do
¬Ø‚àÜk ‚Üê 1
Niter
P
Œ¥‚ààD[k] Œ¥;
D[k] ‚Üê ¬Ø‚àÜk
return D
Top-k Perturbation Sensitivity Analysis: Figure 3 visualizes the normalized sensitivity of different
MoEs under various top-ks in the search space, measured using the Perturbation Loss (‚àÜk). Higher
values of ‚àÜk indicate greater deviation from the baseline behavior, suggesting stronger sensitivity to
changes in top-k. The sensitivity profiles vary notably across MoEs. For Mixtral-8x7B, early layers
demonstrate greater sensitivity to reductions in the number of active experts, as indicated by lower
perturbation loss, while later layers are more sensitive under top-k perturbation. Interestingly, this
finding diverges from prior works (Dong et al. [2019]), which suggests that early layers are typically
more susceptible to architectural or precision perturbations. In contrast, Qwen1.5-MoE-A2.7B ex-
hibits a reversed pattern where early layers are particularly sensitive to top-k perturbations. DeepSeek
5
0 1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031
Layer Index
1
2T opK
Mixtral-8x7B
0.0
0.5
Loss
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Layer Index
1
2
3
4 T opk
Qwen1.5-MoE-A2.7B
0.0
0.5
Loss
0 1 2 3 4 5 6 7 8 9 101112131415
Layer Index
1
2
3
4
5
6
7
8 T opk
OLMoE-1B-7B-0924
0.0
0.2
0.4
0.6
0.8
Loss
1 2 3 4 5 6 7 8 9 10 11
Layer Index
1
2
3
4
5
6 T opk
DeepSeek-VL2 Tiny
0.0
0.2
0.4
0.6
0.8
Loss
Figure 3: Top- k sensitivity analysis. The heatmap plots depict the layer-wise output deviation
with respect to changing the top- k. The initial layers in Mixtral model are less sensitive to top- k
perturbation than deeper layers, while OLMoE exhibits a bell curve pattern where initial and last
layers are more sensitive. Heatmaps for MiniCPM and DeepSeekV2 are shown in Appendix A.2.
model displays a bell-shaped sensitivity profile, with both initial and final layers demonstrating higher
perturbation loss, while intermediate layers remain relatively stable. These findings have practical
implications for adaptive expert selection strategies suggesting that latency can be can be optimized
by reducing the number of active experts in more robust (low-sensitivity) layers.
Algorithm 2: LExI Stage 2: Evolutionary Top-k Allocation Optimization with Proxy
Input:
D: TopK Perturbed Frobenius Norm Loss B: Total Active Expert budget
kmin: Minimum topk per layer kmax: Maximum topk per layer
Npop: Population size Gmax: Maximum generations
Œ∑mut: Mutation rate L: Number of Layers
Output: Optimal topk allocation k‚àó = (k1, ..., kL)
Initialize population P ‚Üê {ki} where ki satisfies:PL
j=1 kj = B (Model budget constraint) and kj
min ‚â§ kj ‚â§ kj
max ‚àÄj (layer constraints)
for g ‚Üê 1 to Gmax do
Evaluate fitness: œï(k) = PL
j=1 Dj(kj)
Select parents via tournament: p1, p2 ‚Üê arg mink‚ààP œï(k)
Generate offspring via CROSSOVER :
k‚Ä≤
j ‚Üê Œ±jp1,j + (1 ‚àí Œ±j)p2,j Œ±j ‚àº Bernoulli(0.5)
Apply MUTATION :
k‚Ä≤‚Ä≤
j ‚Üê k‚Ä≤
j + ‚àÜj ‚àÜj ‚àà {‚àí1, 0, +1} with P
j ‚àÜj = 0
Project to feasible space:
k‚Ä≤‚Ä≤‚Ä≤ ‚Üê Proj(k‚Ä≤‚Ä≤) s.t. constraints hold
Update population:
P ‚Üê P ‚à™ {k‚Ä≤‚Ä≤‚Ä≤}
return k‚àó = arg mink‚ààP œï(k)
Stage 2: Evolutionary Search with Proxy: LExI utilizes the proxies generated in the previous
step to guide the evolutionary algorithm to allocate layer-wise top- k as described in Algorithm
2. We define the total active expert budget (i.e. total number of active experts across all layers)
B, minimum ( kmin) and maximum ( kmax) number of top- k per layer. The objective is to find a
feasible allocation k‚àó = (k,
1 . . . , kL) which minimizes the total layer-wise loss PL
j=1 Dj(kj) (the
6
Base Model Inter Pruning (12.5%, 25%, 50%) Intra Pruning (12.5%, 25%, 50%) LExI (Ours)
51 56 60 65 70
Average Score
6.8k
7.0k
7.1k
7.3k
7.4kThroughput
OLMoE-1B-7B
(a)
OlmoE-1B-7B-Instruct
(B = 64, 72, 100)
55 58 61 64 67
Average Score
3.7k
3.9k
4.0k
4.2k
4.4kThroughput
Qwen1.5-MoE (b)
Qwen1.5-MoE-A2.7B
(B = 48, 56, 72, 80)
43 49 55 61 67
Average Score
819
861
903
945
987Throughput
MiniCPM-MoE (c)
MiniCPM-MoE-8x2B
(B = 50, 60, 70)
49 56 63 71 78
Average Score
1.8k
2.0k
2.3k
2.5k
2.8kThroughput
Mixtral-8x7B (d)
Mixtral-8x7B-Instruct
(B = 40, 48, 56)
54 59 63 68 73
Average Score
2.7k
2.9k
3.1k
3.3k
3.5kThroughput
DeepSeek-V2-Lite (e)
DeepSeekV2-Lite-Chat
(B = 78, 104, 130)
Figure 4: Average Accuracy (‚Üë) vs Throughput (‚Üë) on 9 LM-Eval Tasks (ARC-c, ARC-e, BoolQ,
HellaSwag, MMLU, OBQA, RTE, WinoGrande). B: Active Expert Budget
sum of TopK Perturbed Frobenius Norm losses) across L layers, subject to the budget constraintPj = 1Lkj = B and per-layer limits kmin ‚â§ kj ‚â§ kmax for all j. We initialize a population ofNpop
allocations (each satisfying the constraints) and then evolve this population over Gmax generations.
In each generation, every candidate solution is evaluated by its fitness œï(k) = PL
j=1 Dj(kj), and
parent solutions are chosen (e.g. via tournament selection) to produce offspring. A pair of parents is
recombined using uniform crossover, wherein each layer‚Äôs allocation kj in the offspring is inherited
from one of the two parents. The offspring is then mutated and ensured that the total budgetB remains
unchanged (any increment in one layer‚Äôs kj is balanced by a decrement in another). After mutation,
the new solution is added to the population. This evolutionary loop of selection, crossover, mutation,
and repair is repeated for up to Gmax generations, after which the best found allocation k‚àó (the one
minimizing œï(k)) is returned. By maintaining the proxies, our LExI algorithm effectively navigates
the combinatorial search space of discrete allocations and finds solutions fast without needing to load
the actual model, making it well-suited for optimizing top-k selection under various global active
expert budgets where gradient-based methods require tremendous computational resources.
5 Results and Evaluation
This section presents our evaluation results and key insights from these runs. We compare our
results with the baseline model, Inter Pruning (Lu et al. [2024]) and Intra Pruning (Yang et al. [2024]).
LM-Eval Results. Figure 4 presents a comprehensive comparison of average accuracy versus
throughput across five MoE models on nine LM-Eval benchmarks. Traditional pruning methods,
inter-expert (red) and intra-expert (blue), consistently demonstrate a trade-off: reducing the number
of parameters improves throughput but significantly degrades accuracy. In contrast, our proposed
method, LExI (green), achieves a more favorable accuracy-throughput balance across all models.
On OLMoE-1B-7B (Figure 4a), LExI with the active expert budget ( B) = 100 achieves the same
throughput as 50% intra-pruning while delivering +10% higher accuracy, and outperforms the 50%
inter-pruning baseline by +15% higher accuracy. On Qwen1.5-MoE, LExI offers at least +5.1%
higher throughput compared to both inter and intra pruning, with a consistent +0.5 % accuracy
gain. For MiniCPM-MoE, LExI achieves +15% higher accuracy than 25% inter-pruned models at
equivalent throughput, demonstrating superior accuracy-throughput tradeoffs. For Mixtral-8x7B,
LExI surpasses the inter-pruned baseline by+10% accuracy at nearly identical throughput. Finally, on
DeepSeekV2-Lite, LExI outperforms inter pruning with +6.5% higher throughput at equal accuracy,
and recovers +6 accuracy points over intra pruning with only a minor throughput compromise.
Notably, in Qwen1.5-MoE and MiniCPM-MoE, LExI not only avoids accuracy degradation but also
achieves throughput comparable to or better than 50% inter- or intra-expert pruning. In summary,
LExI preserves accuracy close to the base model while delivering substantial throughput gains,
often surpassing both pruning approaches. These results highlight the robustness and effectiveness
of LExI‚Äôs expert budget reallocation in preserving model performance while improving inference
efficiency, demonstrating its superiority over uniform pruning.
Long context Evaluation. Figure 5 presents an accuracy-throughput comparison on the Qasper
dataset for three different MoE models. Inter and intra pruning methods consistently reduce through-
put but often come at a sharp cost in F1 score, particularly under higher pruning ratios. In contrast,
our LExI method achieves a more favorable trade-off by dynamically reducing the number of active
experts per layer based on sensitivity, leading to Pareto improvements (F1 score and throughput) in
multiple cases. For example, in Qwen1.5 and DeepSeek models, LExI achieves higher throughput
than the base model while maintaining competitive or superior F1 scores, demonstrating its efficacy
in preserving task accuracy while enhancing inference efficiency. On Qwen1.5-MoE-A2.7B, LExI
7
Base Model Inter Pruning (12.5%, 25%, 50%) Intra Pruning (12.5%, 25%, 50%) LExI (Ours)
22 26 30 34 38
F1 Score
3.7k
3.9k
4.0k
4.2k
4.3kThroughput
Qwen1.5-MoE
(a)
Qwen1.5-MoE-A2.7B
(B = 48, 56, 72, 80)
12 17 22 26 31
F1 Score
1.8k
2.0k
2.3k
2.5k
2.8kThroughput
Mixtral-8x7B (b)
Mixtral-8x7B-Instruct
(B = 40, 48, 56)
11 17 23 29 35
F1 Score
2.7k
2.9k
3.1k
3.3k
3.5kThroughput
DeepSeek-V2-Lite (c)
DeepSeekV2-Lite-Chat
(B = 78, 104, 130)
Figure 5: F1 Score (‚Üë) vs Throughput (‚Üë) on Qasper Dataset in LongBench. B: Active Expert Budget
Base Model Inter Pruning (12.5%, 25%, 50%) Intra Pruning (12.5%, 25%, 50%) LExI (Ours)
0 25 50 75 100
Average Accuracy
6.8k
7.0k
7.1k
7.3k
7.4kThroughput
OLMoE-1B-7B
(a)
OlmoE-1B-7B-Instruct
(B = 64, 72, 100)
0 25 50 75 100
Average Accuracy
3.7k
3.9k
4.0k
4.2k
4.4kThroughput
Qwen1.5-MoE (b)
Qwen1.5-MoE-A2.7B
(B = 48, 56, 72, 80)
0 25 50 75 100
Average Accuracy
819
861
903
945
987Throughput
MiniCPM-MoE-8x2B (c)
MiniCPM-MoE-8x2B
(B = 50, 60, 70)
0 25 50 75 100
Average Accuracy
1.8k
2.0k
2.3k
2.5k
2.8kThroughput
Mixtral-8x7B (d)
Mixtral-8x7B-Instruct
(B = 40, 48, 56)
0 25 50 75 100
Average Accuracy
2.7k
2.9k
3.1k
3.3k
3.5kThroughput
DeepSeek-V2-Lite (e)
DeepSeekV2-Lite-Chat
(B = 78, 104, 130)
Figure 6: Passkey Retrieval Average (‚Üë) vs Throughput (‚Üë) Comparison. B: Active Expert Budget
achieves a score of 35.5 at ‚àº 4.1k throughput, outperforming inter pruning (F1 34 at ‚àº 3.9k) and
intra pruning (F1 30 at ‚àº 3.75k) with a +0.5‚Äì5.5 F1 gain and +5.1%‚Äì9.3% higher throughput.
Passkey Retrieval Task. Figure 6 illustrates the trade-off between throughput and average accuracy
for the Passkey Retrieval task across five MoE models. This task evaluates a model‚Äôs ability to
extract precise key information embedded in distractive or noisy contexts, demanding both precision
and robustness. Traditional inter- and intra-pruning approaches generally degrade performance,
with noticeable accuracy drops even at moderate pruning levels. In contrast, our proposed LExI
method consistently outperforms these baselines by achieving higher or comparable accuracy while
significantly improving throughput. Notably, in models like OLMoE-1B-7B and Qwen1.5-MoE,
LExI nearly restores or surpasses the base model‚Äôs accuracy while offering improved efficiency.
These results highlight LExI‚Äôs strength in preserving critical retrieval capabilities under expert budget
constraints, making it a superior choice for precision-sensitive tasks like passkey extraction.
Perplexity Evaluation. Figure 7 presents a comparison of perplexity vs. throughput across multiple
MoE models and datasets, highlighting the limitations of traditional pruning techniques. Across
models, LExI consistently offers a better accuracy-efficiency benefit, outperforming both the pruning
baselines. Notably, pruning methods often yield modest throughput gains but at the cost of substantial
perplexity degradation, especially evident in the OLMoE and Mixtral cases. In contrast, LExI
achieves throughput improvements close to aggressive pruning levels while almost preserving the
perplexity of the base model. For example, on Mixtral-8x7B, LExI achieves‚àº2.4k throughput at ‚àº23
perplexity on C4 dataset and ‚àº2.2k throughput at ‚àº10 perplexity on WikiText, whereas inter pruning
attains the same speedup with double the perplexity. This indicates that static expert reduction via
LExI enables smarter compute allocation, unlike pruning, which disrupts the expert-token mapping
and leads to suboptimal routing and load imbalance.
Ablation Study on Vision Language Domain. Figure 8 evaluates the LExI method on
DeepSeekVL2-Tiny across four vision-language tasks. While intra-pruning peaks at 25% intra
pruning at the cost of sharp accuracy drops, its performance is highly unstable and inconsistent
across tasks. Inter-pruning, on the other hand, exhibits a flat trade-off curve that fails to provide
significant speedups. In contrast, LExI consistently achieves superior accuracy and throughput
balance, yielding improvements without compromising performance. Unlike pruning, which disrupts
expert specialization and introduces fragility, LExI leverages structured top-k expert budget selection
that preserves model capacity while enabling efficient routing, making it a more robust alternative for
optimizing sparse VLMs.
6 Limitations
Our approach has two primary limitations. First, it does not reduce the memory footprint of the
MoE model. Unlike prior expert pruning methods that explicitly target improving memory efficiency
by removing parameters, our method focuses solely on optimizing computational performance during
8
Base Model Inter Pruning (12.5%, 25%, 50%) Intra Pruning (12.5%, 25%, 50%) LExI (Ours)
40 60 80
Perplexity
850
900
950Throughput
MiniCPM on C4
(a)
MiniCPM-MoE-8x2B
(B = 50, 60, 70)
50 75 100125150
Perplexity
850
900
950Throughput
MiniCPM on PTB (b)
MiniCPM-MoE-8x2B
(B = 50, 60, 70)
10 20 30 40 50 60
Perplexity
850
900
950Throughput
MiniCPM on WT (c)
MiniCPM-MoE-8x2B
(B = 50, 60, 70)
50 100 150 200
Perplexity
7000
7200
7400Throughput
OLMoE-1B-7B on C4 (d)
MiniCPM-MoE-8x2B
(B = 50, 60, 70)
50 100 150
Perplexity
7000
7200
7400Throughput
OLMoE-1B-7B on PTB
(e)
OlmoE-1B-7B-Instruct
(B = 64, 72, 100)
50 100 150
Perplexity
7000
7200
7400Throughput
OLMoE-1B-7B on WT (f)
OlmoE-1B-7B-Instruct
(B = 64, 72, 100)
20 25 30 35 40
Perplexity
2000
2200
2400
2600Throughput
Mixtral-8x7B on C4 (g)
Mixtral-8x7B-Instruct
(B = 40, 48, 56)
10 15 20 25
Perplexity
2000
2200
2400Throughput
Mixtral-8x7B on WT (h)
Mixtral-8x7B-Instruct
(B = 40, 48, 56)
Figure 7: Perplexity (‚Üì) vs Throughput (‚Üë) on C4, PTB & WikiText(WT)B: Active Expert Budget
Base Model Inter Pruning (12.5%, 25%, 50%) Intra Pruning (12.5%, 25%, 50%) LExI (Ours)
868 1048122814081587
Score
14
15
16
17Throughput
MME Perception
(a)
DeepSeekVL2-Tiny
(B = 44, 48, 55)
182 231 280 329 378
Score
14
15
16
17Throughput
MME Reasoning (b)
DeepSeekVL2-Tiny
(B = 44, 48, 55)
14 21 28 35 42
Score
14
15
16
17Throughput
MMMU (c)
DeepSeekVL2-Tiny
(B = 44, 48, 55)
44 55 67 79 91
Score
14
15
16
17Throughput
ScienceQA (d)
DeepSeekVL2-Tiny
(B = 44, 48, 55)
Figure 8: DeepSeekVL2-Tiny: Average Accuracy ( ‚Üë) vs Throughput (‚Üë) on MME, MMMU, Sci-
enceQA Tasks in VLMEvalKit. B: Active Expert Budget
inference. This means that while our approach can significantly speed up inference by reducing the
number of expert computations, it does not reduce model size. As a result, it is less effective in
memory-constrained deployment scenarios. Nevertheless, our method can be effectively combined
with existing MoE pruning methods, enabling a joint optimization of both computational efficiency
and model memory. Second, our method may underperform in settings where the top-$k$ expert
search space is inherently limited. Our approach relies on selectively reducing the number of active
experts during inference to gain computational efficiency. However, in architectures such as Llama-4,
where each MoE layer is pretrained with only a single active expert, there is no flexibility to reduce
active experts further. In such scenarios, our method becomes inapplicable.
7 Conclusion
In this paper, we propose LExI, a framework designed to determine the optimal number of active
experts per layer of a pretrained MoE model. LExI achieves better throughput than both inter and
intra expert pruning baseline methods across six different MoE models. Unlike uniform expert
pruning, which can yield speedups only at the cost of substantial accuracy loss, our method delivers
substantial throughput gains while preserving model accuracy/perplexity close to the original baseline.
For example, on OLMoE-1B-7B, Mixtral-8x7B, and DeepSeek, LExI maintains accuracy nearly
identical to the unpruned model yet attains notably higher throughput, often surpassing the accuracy
of pruned models. In certain scenarios, this layer-adaptive approach achieves higher throughput than
the base model with the same task performance, highlighting the robustness of the proposed approach.
For example, on OLMoE-1B-7B model, our approach matches the throughput of 50% Intra pruned
model with 10% better accuracy. Also, LExI‚Äôs benefits come with no retraining or calibration dataset
requirements. It is a data-free post-training method that optimizes without any calibration dataset or
fine-tuning. This makes LExI a practical inference-time optimization technique that reduces latency,
inter-GPU communication overhead, and memory bandwidth usage by determining the optimal
number of active experts per layer, all while incurring negligible accuracy loss.
9
Acknowledgements
This research used resources of the Argonne Leadership Computing Facility, a U.S. Department
of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on
research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research
Program, under Contract No. DE-AC02-06CH11357
References
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual,
multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3119‚Äì3137,
Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/
2024.acl-long.172. URL https://aclanthology.org/2024.acl-long.172.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint
arXiv:1905.10044, 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
ArXiv, abs/1803.05457, 2018.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset
of information-seeking questions and answers anchored in research papers. arXiv preprint
arXiv:2105.03011, 2021.
Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Moham-
madreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open
weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146,
2024.
Jesse Dodge, Maarten Sap, Ana Marasovi ¬¥c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,
Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the
colossal clean crawled corpus. arXiv preprint arXiv:2104.08758, 2021.
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian
aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 293‚Äì302, 2019.
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang
Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large
multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia,
pages 11198‚Äì11201, 2024.
Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in
one-shot. In International Conference on Machine Learning, pages 10323‚Äì10337. PMLR, 2023.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,
Laurence Golding, Jeffrey Hsu, Alain Le Noac‚Äôh, Haonan Li, Kyle McDonell, Niklas Muennighoff,
Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,
Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation
harness, 07 2024. URL https://zenodo.org/records/12608602.
Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, and Tao Lin. Dynamic mixture of ex-
perts: An auto-tuning approach for efficient transformer models. arXiv preprint arXiv:2405.14297,
2024.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. Proceedings of the International
Conference on Learning Representations (ICLR), 2021.
10
Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,
Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models
with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.
Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems
Principles, pages 611‚Äì626, 2023.
Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong
Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-
experts language model. arXiv preprint arXiv:2405.04434, 2024a.
Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew B Blaschko, Shengen Yan, Guohao Dai,
Huazhong Yang, and Yu Wang. Efficient expert pruning for sparse mixture-of-experts language
models: Enhancing performance and reducing inference costs. arXiv preprint arXiv:2407.00945,
2024b.
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,
Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
science question answering. Advances in Neural Information Processing Systems, 35:2507‚Äì2521,
2022.
Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, and Hongsheng
Li. Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large
language models. arXiv preprint arXiv:2402.14800, 2024.
Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus
of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models, 2016.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In EMNLP, 2018.
Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia
Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language
models. arXiv preprint arXiv:2409.02060, 2024.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
extension of large language models. arXiv preprint arXiv:2309.00071, 2023.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.
Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for
large language models. arXiv preprint arXiv:2306.11695, 2023.
Qwen Team. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters",
February 2024. URL https://qwenlm.github.io/blog/qwen-moe/.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,
pages 353‚Äì355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.
Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang
Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language
models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024.
11
Yanyue Xie, Zhi Zhang, Ding Zhou, Cong Xie, Ziang Song, Xin Liu, Yanzhi Wang, Xue Lin, and
An Xu. Moe-pruner: Pruning mixture-of-experts large language model using the hints from its
router. arXiv preprint arXiv:2410.12013, 2024.
Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin,
Yu Cheng, and Bo Yuan. Moe-i2: Compressing mixture of experts models through inter-expert
pruning and intra-expert low-rank decomposition. arXiv preprint arXiv:2411.01016, 2024.
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on
multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu
Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal under-
standing and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 9556‚Äì9567, 2024.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine
really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 2019.
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and
Hongsheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv
preprint arXiv:2102.04010, 2021.
A Appendix
A.1 Mixture of Experts Model Setup
Table 1 illustrates the hyperparamters of each MoE model we utilized in our evaluation.
Table 1: LLM and VLM MoE Models
Model #P (B) #Layers #Experts TopK FFN Dim
DeepSeek VL2-Tiny 3 12 64 6 896
OLMoE-1B-7B-0125-Instruct 6.92 16 64 8 1024
Qwen1.5-MoE-A2.7B-Chat 14.3 24 60 4 1408
DeepSeek-V2-Lite-Chat 15.7 27 64 6 1408
MiniCPM-MoE-8x2B 17 40 8 2 5760
Mixtral-8x7B-Instruct-v0.1 46.7 32 8 2 14336
A.2 Additional Heatmaps for top-K sensitivity
Figure 9 illustrates the topk sensitivity heatmaps for MiniCPM-MoE and DeepSeekV2 Lite Chat
model based on Algorithm 1.
0 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930313233343536373839
Layer Index
1
2T opK
MiniCPM-MoE-8x2B
0.0
0.5
1.0
Loss
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
Layer Index
1
2
3
4
5
6 T opk
DeepSeek-V2-Lite
0.00
0.25
0.50
0.75
Loss
Figure 9: Top-k sensitivity analysis across MiniCPM-MoE-8x2B and DeepSeekV2-Lite. The plots
depict the layer-wise output deviation with respect to changing the top- k. The initial layers in
MiniCPM model are less sensitive to topk perturbation than deeper layers, while DeepSeekV2
exhibits a bell curve pattern where initial and last layers are more sensitive.
12