Exploring action‑oriented models via active 
inference for autonomous vehicles
Sheida Nozari1,2*  , Ali Krayani1, Pablo Marin2, Lucio Marcenaro1, David Martin Gomez2 and Carlo Regazzoni1 
1 Introduction
Autonomous vehicles (AVs) have received much attention from both industry and 
academia due to their potential to improve the safety and efficiency of driving expe -
riences. Autonomous driving systems (ADS) are generally partitioned into a hierar -
chical structure, including perception, decision-making, action planning, and vehicle 
control [1 ]. Perception and navigation in a dynamic environment have been a long-
standing challenge in AVs. The complexity of the decision-making systems might 
provoke errors causing performance degradation and lead to severe situations (e.g., 
collisions) [2 ]. Performing suitable actions according to the dynamic environmental 
changes around the AV significantly impacts error minimization. Thus, action plan -
ning is still a challenging task responsible for safety and efficiency. It should con -
sider the feasibility constraints in a kinematic and dynamic manner based on the 
Abstract 
Being able to robustly interact with and navigate a dynamic environment has been 
a long-standing challenge in intelligent transportation systems. Autonomous agents 
can use models that mimic the human brain to learn how to respond to other par-
ticipants’ actions in the environment and proactively coordinate with the dynamics. 
Modeling brain learning procedures is challenging for multiple reasons, such as sto-
chasticity, multimodality, and unobservant intents. Active inference may be defined 
as the Bayesian modeling of a brain with a biologically plausible model of the agent. Its 
primary idea relies on the free energy principle and the prior preference of the agent. 
It enables the agent to choose an action that leads to its preferred future observations. 
An exploring action-oriented model is introduced to address the inference complex-
ity and solve the exploration–exploitation dilemma in unobserved environments. It 
is conducted by adapting active inference to an imitation learning approach and find-
ing a theoretical connection between them. We present a multimodal self-awareness 
architecture for autonomous driving systems where the proposed techniques are 
evaluated on their ability to model proper driving behavior. Experimental results 
provide the basis for the intelligent driving system to make more human-like decisions 
and improve agent performance to avoid a collision.
Keywords: Active inference, Imitation learning, Action-oriented model, Bayesian 
filtering, Autonomous driving
Open Access
© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits 
use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original 
author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third 
party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-
rial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or 
exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://
creativecommons.org/licenses/by/4.0/.
RESEARCH
Nozari et al. 
EURASIP Journal on Advances in Signal Processing         (2024) 2024:92  
https://doi.org/10.1186/s13634‑024‑01173‑9
EURASIP Journal on Advances
in Signal Processing
*Correspondence:   
sheida.nozari@edu.unige.it
1 Department of Electrical, 
Electronic, Telecommunications 
Engineering and Naval 
Architecture, University of Genoa, 
Genoa 16145, Italy
2 Intelligent Systems Laboratory, 
Carlos III University of Madrid, 
Leganés 28911, Spain
Page 2 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
information about the perceived environment and the reasonable prediction of the 
other contributor’s behaviors. Moreover, it should be able to generate optimal or 
semi-optimal maneuvers that provide suitable driving quality, such as exactitude and 
consistency.
Satisfying the earlier requirements mandates an efficient theory capable of represent -
ing causal relationships in the world and providing optimal behavior in highly uncer -
tain environments. In addition, for an AV to reach a high level of autonomy, it must be 
equipped with self-awareness to understand its own situation, performance limits and 
the effects of decisions on its surroundings  [3]. Self-awareness refers to the system’s 
capability to know not only its external context (i.e., other objects), but also its state 
based on its sensations and internal models [4]. Computational self-awareness methods 
are a promising field that allows an autonomous agent to identify dynamic situations, 
create internal models of its world, and modify its behavior autonomously to contextual 
conditions [5].
Recent progress in signal processing and machine learning allows an intelligent learn -
ing agent to achieve a self-awareness model by observing multisensorial data from an 
accomplished task by an expert agent. Observational learning is an online learning 
approach that occurs as a process of observing an expert agent performing a specific 
task that another agent retains  [6]. It is imperative that autonomous agents are capa -
ble of learning from and adapting to a variety of safe and desired behaviors. Imitation 
learning (IL) is an essential paradigm for learning preferences strategies via imitating the 
expert demonstrations [7]. IL might utilize diverse and large-scale driving data to handle 
huge state-action space and complicated driving scenarios. However, its performance 
never exceeds the demonstrator’s skills. Moreover, it is vulnerable to distributional shifts 
between demonstration and execution [8].
A self-aware autonomous system constantly deals with continuous and potentially 
overwhelming signals from the agent’s sensors and their interaction with the dynamic 
surrounding. For learning and adaptation, the intelligent agent must transform the sen -
sory inputs into a reliable perception of the world. One of the ultimate goals of artifi -
cial intelligence is to construct autonomous agents capable of human-level performance. 
Motivated by this, cognitive science has debated how exactly the brain carries out the 
learning activities. While previous researches propose perception primarily as a bottom-
up readout of sensory signals, emerging Bayesian models suggest that perception is cog -
nitively modulated instead. It might be best viewed as a process of prediction based on 
an integration of sensory inputs, prior experience, and contextual cues [9].
The brain executes the Bayes rule to perceive the world by continuously generating 
a top-down cascade of encoded hypotheses about environmental states and processing 
bottom-top projections of sensory inputs compared with the prior hypothesis’s top-
down flow [10]. Any mismatch between top-down predictions and bottom-up sensory 
responses results in prediction errors, prompting the system to refine its hypotheses. 
Thus, there is a strong link between bottom-up perception and top-down prediction, 
allowing to continuously update the priors to better predict the subsequent incoming 
sensory inputs and minimize errors. In this view, experiences are necessary because they 
assess how good the model is and give a hint to correct future predictions through the 
computation of prediction errors. As a result, ascending projections do not capture the 
Page 3 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
characteristics of a stimulus but rather how surprised the brain is by it, given the strong 
link between surprise and model uncertainty [11].
Consequently, active inference has emerged as a novel theory explaining the idea that 
the brain is essentially a prediction and inference machine that actively attempts to pre -
dict, experiment with, and comprehend its surroundings [12]. Perception and action are 
strongly linked in active inference, both coming from the brain’s beliefs about the world 
and being constrained by sensory inputs from the environment  [13]. Active inference 
argues that action influences perception in order to minimize the free energy (FE) [14]. 
The FE principle has emerged as a unified brain theory and a new perspective on life and 
self-organization in complex far-from-equilibrium systems [15].
In this paper, motivated by the above discussion and previous work [16], we introduce 
a self-awareness framework empowered by active inference to improve autonomous 
driving (AD). The proposed framework consists of three main modules: a multimodal 
perception module, a global learning module (world model) and an active learning 
module. Thus, an AV (learning agent) equipped with self-awareness is capable of learn -
ing how to self-drive in a dynamic environment while interacting with another mov -
ing object. The multimodal perception module allows the AV to perceive the external 
world as a bundle of exteroceptive and proprioceptive sensations from multiple sensory 
modalities (e.g., positional information from GPS sensors, images from cameras, point 
clouds from Lidar, etc.) and to be able to integrate information from different sensory 
inputs and match them appropriately. In this work, the AV integrates proprioceptive 
stimuli (i.e., AV’s positions) with exteroceptive stimuli (i.e., the relative distance between 
AV and another object), describing the integration process using Bayesian inference. 
The AV relies on the global world module to encode the dynamics of the surrounding 
environment that is structured in a hierarchical representation. The idea is to use hier -
archical representations underlying multisensory integration to explain best how sen -
sory data are caused in multiple modalities. The global learning module consists of: (i) a 
situation model representing the dynamic driving behavior of an expert agent interact -
ing with another agent in the environment that is learned from demonstrations and (ii)  
a first-person model (FP-M) enabling the third agent (i.e., AV) in first person, so that 
AV can experience a certain driving task from the expert’s real perspective. The situa -
tion and the first-person models are represented in coupled generalized dynamic Bayes -
ian networks (C-GDBNs). The former is composed of two GDBNs representing the two 
agents interacting in the environment where their hidden variables are stochastically 
coupled (variables are uncorrelated but have coupled means), and each GDBN has its 
own private observation. Likewise, the first-person model represents the stochastic cou -
pling of the interaction between AV and another agent and the AV’s behavior using a 
C-GDBN. The active learning module connects the internal models that the AV holds 
with the decision-making process by enriching the FP-M with active variables represent-
ing the set of actions that the AV can perform and so creating the active first-person 
model (AFP-M). This endows the AV with the capability to predict what will happen 
next in the surroundings and evaluate the environmental situation to understand how it 
should behave in first person. Hence, the AV can either follow an offline planned task by 
executing expert-like maneuvers during normal situations (i.e., situations experienced by 
the expert) or by planning at run-time and learning incrementally to resolve uncertainty 
Page 4 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
during unexpected situations (i.e., situations not experienced by the expert). To this 
purpose, we implement a hybrid mechanism by pulling together imitation learning 
and active inference, inspired by the brain learning procedure that typically integrates 
the agent’s prior knowledge and its actual observations. The AV uses the mismatches 
between prediction and observations to jointly improve future predictions and actions 
to minimize future FE (i.e., prediction errors).
The major contributions of this paper are as follows:
• It advances a probabilistic computational account of action, observation and imita -
tion abilities grounded in the framework of active inference. While our proposal is 
domain-general, in this paper, we illustrate it using driving tasks (i.e., lane chang -
ing) in a dynamic environment, where a naive learning agent infers and imitates the 
actions executed by an expert agent.
• The proposed approach enables AV to follow an offline planned task by executing 
expert-like overtaking maneuvers in automated driving systems while still taking 
autonomous decisions at run-time and learning incrementally to adapt to unex -
pected situations.
• A probabilistic framework is developed to solve the exploration–exploitation 
dilemma by foreseeing actions that minimize the prediction errors and establish a 
solid foundation for further research on the representation and learning of concepts 
in a cognitive environment by an autonomous agent.
• An online evaluation of joint state predictions is applied to update the belief during 
the back-projection of detected errors at continuous and discrete levels. We employ a 
Bayesian sequential decision-making model (i.e., particle filter, Kalman filter) to dis -
tinguish exploration and exploitation processes, which train AV to generate the pre -
ferred performance or explore a new course of actions based on its sensory observa -
tions and new information provided by the perception of the surrounding.
• Extensive simulations on various overtaking tasks illustrate that the performance of 
the proposed approach outperforms that of the reinforcement learning (RL) method. 
Furthermore, we discuss how clustering of new experiences might affect the perfor -
mance of the AV in generalizing what has been learned so far to unseen situations.
2  Related works
AD requires the resolution of perception and motion planning issues in the presence 
of dynamic objects interacting in the environment. The complex interactions between 
multiple agents are significant challenges due to the difficulty of predicting their future 
motions. Most model-based AD approaches necessitate manually designing the driving 
policy model [17, 18] or employing safety assessments [19, 20]. While designing a deci -
sion and planning system for AVs is complex, an alternative is to learn the driving policy 
from an expert agent using IL. The existing IL methods can handle simple driving tasks 
such as lane following [21, 22]. However, if the agent is dealing with a new environment 
or a more complicated task (such as line-changing), it is required that the human has to 
take control, or the system fails ultimately [23, 24]. More particular, a typical IL proce -
dure is direct learning, where the main goal is to learn a mapping from states to actions 
that mimic the demonstrator explicitly [25, 26]. Direct learning methods are categorized 
Page 5 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
into classification methods when the learner’s actions can be classified into discrete 
classes [27, 28], and regression methods which are used to learn actions in a continu -
ous space [29]. Direct learning often is not adequate to reproduce proper behavior due 
to insufficient demonstrations and environmental changes. Besides, indirect learning 
can complement direct approaches by refining the policies based on sub-optimal expert 
demonstrations [30].
The critical drawbacks of IL are that the policy never exceeds the sub-optimal expert 
performance, and the learning policy is vulnerable to distributional shift [31]. Therefore, 
IL frequently involves another step that requires the learning agent refinement of the 
estimated policy based on its current situation. This self-improvement can be achieved 
by a quantifiable reward or learned from instances. Many of these approaches come 
under the RL methods. RL allows encoding desired behavior—such as reaching the 
target and avoiding collisions—and relies not only on perfect expert demonstrations. 
In addition, RL maximizes the overall expected return on an entire trajectory, while IL 
treats every observation independently [32], which conceptually makes RL superior to 
IL. RL does not have prior knowledge from an expert. Therefore, the learning agent has 
no clue to realize desired behaviors in sparse-reward settings [33]. Even when RL suc -
ceeds in reward maximization, the policy does not necessarily achieve behaviors that 
the reward designer has expected. In addition, learning through trial and error requires 
reward functions designed specifically for each task. Defining rewards for such problems 
is complex and still unknown in many cases. Thus, behavior learning, such as IL and 
RL, would be complex without representation or model learning from the environment. 
To overcome the mentioned limitations, autonomous systems can employ a generalized 
model of the world and compute the FE to explain perception, action, and model learn -
ing in a Bayesian probabilistic way  [34, 35], allowing to handle behavior learning and 
model teaching at the same time in a dynamic environment. To highlight the discrete -
ness of the presented study, a brief comparison of this work with other existing works in 
the literature is provided in Table 1.
3  Proposed framework
The proposed self-awareness architecture depicted in Fig. 1 is composed of several mod-
ules forming the perception-action cycle that links an AV to its environment. When fac -
ing a new situation, an AV makes sense of the external world by creating and testing 
hypotheses about how the world evolves. It makes predictions based on prior knowl -
edge acquired from past experiences, takes actions based on those hypotheses, perceives 
Table 1 Comparison with existing methods from literature
functionalities Ours [23] [22] [26] [30] [32] [21] [19] [24] [33] [25] [20]
Indirect learning ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✓
No expert intervention ✓ ✗ ✓ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✗
Self-improvement ✓ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Adapt to dynamic environ-
ment/behavior
✓ ✓ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Incremental learning ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓
Calculate future FE/reward ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗
Page 6 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
the consequences, and adjusts the hypotheses. The different modules in the architecture 
can be seen as different areas of the biological brain, each one handling particular func -
tionalities. Some parts handle sensory perception, such as seeing, while others handle 
planning and decision-making. All parts of the brain work together, with messages fol -
lowing between them. As shown in Fig.  1, the hierarchical message passing through the 
levels is not regarded as a straightforward action-feedback mapping. It is represented by 
inferences and perceptions across different modalities of proprioceptive and exterocep -
tive sensory signals. Learning this association allows the model to predict the percep -
tual consequences of acting. Additionally, the model must use these representations to 
reduce prediction errors and predict how sensory signals change under specific actions. 
The following sections present a detailed description of the different modules involved 
in the architecture.
3.1  Dynamic world
We approach self-awareness from a multisensory signal processing perspective in a non-
stationary environment. The environment is considered dynamic due to the changing 
through the agent transitions and simultaneous other processes operating on it. The 
agent is equipped with exteroceptive sensors to observe the environment and proprio -
ceptive sensors to measure the internal parameters. Accordingly, the agent continuously 
collects multisensorial data by observing itself and its surroundings and processes the 
collected data to learn a contextual dynamic representation.
Fig. 1 A general schematic of the proposed self-awareness architecture
Page 7 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
3.2  Multimodal perception
A perception system is employed for learning the interaction between an agent and 
another dynamic object based on multimodal perception using multisensorial informa -
tion. Multimodality enables the model to leverage the presented sensors to identify cau -
salities between multisensory data perceived by the agent. Leveraging multiple sensors 
to perceive information about the environment is thus crucial when building a model to 
perform predictions about the agent’s dynamics to do motion planning. The perception 
of multimodal stimuli is an important capability that provides multimodal information 
in various conditions to enrich the scene library of AD models.
We combine exteroceptive and proprioceptive perception to model a contextual view-
point for making inferences about future perceived information. Consequently, the con -
text comprises the internal and external perceptions of the agent at each time instant. 
The main idea is to use such information to predict the following internal or external 
states. Therefore, the movement of both agent and dynamic object is simulated at each 
instant by interacting rules that depend on their positions and motions to generate 
coupled trajectory data. The purpose of analyzing such multisensory data is to encode 
the coupled agents’ dynamic interaction as probabilities into a C-GDBN model. The 
obtained dynamic interaction model is self-aware due to its ability to measure the abnor-
malities and incrementally learn newly interacting behavior derived from an initial one 
that affects the agent’s decision-making.
3.3  World model
The world model (WM) plays a simulator role in the brain, and such consideration leads 
us to take inspiration from the mechanism whereby the brain learns to perform sensori -
motor behaviors [36]. In the presented architecture, we obtain the WM using generative 
models through the interacting experiences from multimodal sensory information. The 
WM consists of two models, the situation model (SM) and the first-person model (FP-
M). The SM is an input module that demonstrates the collected sub-optimal information 
of an expert AV (E) and its interaction with a moving vehicle (O) in a continuous envi -
ronment, where E change-lane frequently and overtake O without an accident. E motion 
features, O and their interaction are incorporated in a graphical model (i.e., C-GDBN), 
and the intention of the vehicles can be estimated through probabilistic reasoning. The 
second model (FP-M) is a transferred generative model. Our focus is attempting to 
transfer E ’s knowledge across the first-person point of view, where an intelligent vehicle 
(L) learns by interacting with its surroundings via observing the expert behavior and col -
lecting prior knowledge to incorporate into the environment (Table 2).
3.3.1  Situation model
The SM is an interactive dynamic model encoding the interactions between two vehicles, 
namely, E and O (refer to Fig. 2). The proposed model assumes synchronized sensory data 
from both agents’ locations. Accordingly, the movement of both agents is simulated at each 
time instant by interacting rules that depend on their positions and motions. From the E ’s 
Page 8 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
perspective, it is possible to consider its location measurements as proprioceptive data, 
whereas the relative position of O represents the exteroceptive information.
The dynamic behavior of how the two vehicles interact in the environment is described 
by a generalized hierarchical state-space model in discrete-time comprised of the following 
equations: 
(1a)D k = f(D k−1) + w k ,
Table 2 Mathematical notations
Symbol Description
D Joint discrete states
˜X Generalized state
Z Observation
w Gaussian noise
F State evaluation matrix
U Control unit matrix
ν Zero-mean Gaussian
H Observation matrix
k Time index
S Discrete states
/Pi1 Transition probability
x, y Position components
˙x, ˙y Velocity components
a Action of agent
w Weight of particle
/afii9838˜X Diagnostic message at the continuous level
/afii9838˜D Diagnostic message at the discrete level
π ˜X Predictive message at the continuous level
π ˜D Predictive message at the discrete level
ϒ˜D Abnormality indicator at the discrete level
ϒ˜X Abnormality indicator at the continuous level
DB Bhattacharyya distance
A Set of actions
ǫ Exploration rate
α Maximum weight of particles at each instant
β Index of the particle with the maximum weight
DKL Kullback–Leibler divergence
BC Bhattacharyya coefficient
˜E˜D Generalized error at the discrete level
˜E˜X Generalized error at the continuous level
˜E˜Z Generalized error at the observation level
˜µ ˜E˜Z
Kalman innovation
/Sigma1˜E˜Z
Innovation covariance
Ŵ Active inference table
Ea Generalized error at the active states
/afii9838(a) Diagnostic message at the active states
Page 9 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
 In (1a), Dk is a latent discrete state evolving from the previous state Dk−1 by a non -
linear state evolution function f(·) representing the transition dynamic model and by a 
Gaussian process noise w k ∼ N(0, Q) . The discrete state variables D k =[ SE
k ,SO
k ] repre-
sent jointly the discrete states of E and O where SE
k ∈ SE , SO
k ∈ SO , Dk ∈ D , and SE and 
SO are learned according to the approach discussed in [37], while D ={ D 1,D 2,... ,D m} 
is the set that represents the dictionary consisting of all the possible joint discrete states 
(i.e., configurations) and m is the total number of configurations. Observing the configu-
ration’s evolution (i.e., joint activated clusters of E and O ) over time makes it possible to 
estimate the transition matrix encoding the probability of switching from one configura-
tion to another, which is defined as:
where /Pi1∈ Rm,m , P(Di |Dj ) represents the transition probability from configuration i to 
configuration j and ∑m
k=1 P(Di|Dk) = 1 ∀i.
In (1b), the continuous latent state ˜Xk =[ ˜X
E
k ,˜X
O
k ]∈R nx represents a joint belief 
state where ˜X
E
k and ˜X
O
k  denote the hidden generalized states (GSs) of E and O , respec-
tively. The GSs consist of the vehicles’ position and velocity where ˜X i
k =[ xi
k ,yi
k ,Pxi
k ,Pyi
k ] 
and i ∈{ E, O} . The continuous variables ˜Xk evolve from the previous state ˜Xk−1 by 
the linear state function g(·) and by a Gaussian noise wk . F ∈ Rnx,nx in (1b) is the state 
evolution matrix and UDk =˙µDk is the control unit vector. In (1c ), Zk ∈ Rnz is the gen-
eralized observation, which is generated from the latent continuous states by a linear 
(1b)˜Xk = g( ˜Xk−1,Dk ) = F ˜Xk−1 + BU D k + w k ,
(1c)Zk = h( ˜Xk ) + νk = H ˜Xk + νk .
(2)� =


P(D 1 |D 1 ), ... ,P (D 1 |D m)
... ... ...
P (D m|D 1 ), ... ,P (D m|D m)


Fig. 2 C-GDBN composed of two GDBNs representing the dynamic interaction between two AVs
Page 10 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
function h(·) corrupted by Gaussian noise νk ∼ N(0, R) . Since the observation trans -
formation is linear, there exists the observation matrix H ∈ Rnz,nz mapping hidden 
continues states to observations.
3.3.2  First‑Person model
The FP-M organizes a descriptive dynamic model that enables the third-person (i.e., 
the learner L ) in first person. So L can experience a driving task from E ’s real perspec -
tive, which facilitates more precise imitative behavior and allows L to respond quickly 
and appropriately during the driving task while interacting with another moving vehicle 
V . The FP-M is initialized by mapping the hierarchical levels of the SM into FP (refer 
to Fig.  3). The top level of the hierarchy (discrete level) in FP-M represents previously 
learned configurations ( D ). So, L through the FP-M can regenerate expected interac -
tive maneuvers that can be used as a reference to evaluate its own interactions with V 
and infer how the interaction with the external world should be performed. The hidden 
continuous states in the FP-M represent the dynamic interaction in terms of generalized 
relative distance consisting of relative distance and relative velocity, which is defined as:
Likewise, the observations in the FP-M depict the measured relative distance 
between the two vehicles defined as Zk =[ ZE
k − ZO
k ].
3.4  Active First‑Person model
AFP-M connects the world model that L holds with the decision-making block by 
enriching the FP-M with active states representing the L ’s actions. Thus, AFP-M rep -
resents a generative model P ( ˜Z, ˜X, ˜D,a) of the environment (represented graphically 
in Fig.  4) which is modeled as a partially observed Markov decision process (POMDP). 
AFP-M encompasses joint probability distributions over observations, environmental 
hidden states at multiple levels and actions performed by L , which is factorized as:
(3)X k =[ ˜X E
k − ˜X O
k ]=[ (xE
k − xO
k ),(˙xE
k −˙xO
k )].
Fig. 3 First-person model. It is composed of an uncoupled proprioceptive model (right side) and the learned 
joint configuration (left side)
Page 11 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
In a POMPD, 1) L does not always have access to the true environmental states but 
might instead receive observations which are generated according to P( ˜Zk | ˜Xk ) to infer 
the real states of the environment. 2) L operates on beliefs about the hidden environ -
mental states ( ˜Dk, ˜Xk ) that evolve according to P ( ˜Xk | ˜Xk−1, ˜Dk ) and P ( ˜Dk| ˜Dk−1,ak−1) . 3) 
L interacts with the external world by seeking to take actions that minimize abnormali -
ties and prediction errors.
Joint Prediction and Perception Initially (at k = 1 ), L relies on prior probability dis -
tributions ( P( ˜D0) , P( ˜X0) ) to predict the environmental states according to ˜D0 ∼ P( ˜D0) 
and ˜X0 ∼ P( ˜X0) , respectively, using a hybrid Bayesian filter called the modified Markov 
jump particle filter (M-MJPF) [38] consisting of particle filter (PF) and Kalman filter 
(KF). In the successive time instants ( k > 1 ), L relies on the a priori acquired knowl -
edge of the configurations’ evolution given by P( ˜Dk | ˜Dk−1) which is encoded in (2). PF 
propagates N equally weighted particles drawn from the importance density distribution 
π( ˜Dk) = P( ˜Dk| ˜Dk−1,ak−1) forming the so-called set of particles 
{ ˜D(i)
k ,w (i)
k
}N
i=1 . A bank 
of KFs is employed for the set of particles to predict the corresponding continuous GSs { ˜X(i)
k
}N
i=1 where the prediction of GSs is guided by the upper level as pointed out in (1b) 
that can be expressed in probabilistic form as P ( ˜X(i)
k | ˜X(i)
k −1, ˜D(i)
k ) . The posterior distribu -
tion associated with the predicted GSs is given by:
(4)
P ( ˜Z, ˜X, ˜D ,a) = P( ˜D0)P( ˜X0)
K∏
k=2
P ( ˜Zk| ˜Xk)
P ( ˜Xk| ˜Xk−1, ˜Dk)P( ˜Dk| ˜Dk−1,ak−1)P(a k−1| ˜Dk−1).
(5)π( ˜X(i)
k ) = P( ˜X(i)
k , ˜D(i)
k | ˜Zk −1) =
∫
P( ˜X(i)
k | ˜X(i)
k −1, ˜D(i)
k )/afii9838(˜X(i)
k −1)d ˜X(i)
k −1,
Fig. 4 Graphical representation of the active first-person model
Page 12 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
where /afii9838( ˜X(i)
k −1) = P( ˜Zk −1| ˜X(i)
k −1) is the diagnostic message propagated previously 
after observing ˜Zk−1 at time k − 1 . Consequently, once a new observation ˜Zk is 
received, multiple diagnostic messages propagate in a bottom-up manner to update 
L ’s belief in hidden environmental states. Thus, updated belief in GSs is given by: 
P ( ˜X(i)
k , ˜D(i)
k | ˜Zk ) = π( ˜X(i)
k ) × /afii9838(˜X(i)
k ) , whereas belief in discrete hidden states can be 
updated by updating the particles’ weights according to w (i)
k = w (i)
k × /afii9838(˜Dk ) where /afii9838(˜Dk ) 
is a discrete probability distribution defined as:
such that
where DB is the Bhattacharyya distance and P ( ˜Xk | ˜Dk ) ∼ N(µ˜Dk
,� ˜Dk
).
Action Selection L ’s choice of whether to explore or exploit is guided by its awareness 
of the interaction with the surrounding environment, which is conditioned directly 
onto particle beliefs. L uses the updated particles’ weights to evaluate the encountered 
situation among familiar with (i.e., already seen by E ) or not familiar with (i.e., a novel 
situation not seen by E ) as illustrated in Fig.  5 and Fig.  6. Thus, L selects an action ak 
according to:
(6)/afii9838( ˜Dk) =
[ /afii9838(˜D(1)
k )
∑ m
i=1 /afii9838(˜D(i)
k )
, /afii9838(˜D(2)
k )
∑ m
i=1 /afii9838(˜D(i)
k )
,... , /afii9838(˜D(m )
k )
∑ m
i=1 /afii9838(˜D(i)
k )
]
,
(7)
/afii9838( ˜D(i)
k ) = /afii9838(˜X(i)
k )P( ˜X(i)
k | ˜D(i)
k ) = DB
(
/afii9838(˜X(i)
k ),P( ˜X(i)
k | ˜D(i)
k )
)
=
− ln
∫ √
/afii9838(˜X(i)
k ) × P( ˜X(i)
k | ˜D(i)
k )d ˜X(i)
k ,
Fig. 5 Observing a familiar configuration. The learning agent has proper knowledge about its current 
interaction with the other dynamic object
Fig. 6 Observing a novel configuration. The learning agent experiences a new interaction with the dynamic 
object than the learned configurations
Page 13 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
In (8), if ǫk <ρ  , this means that L is facing similar situation encountered by E and so it 
will imitate E ’s action selected from the active inference table Ŵ defined as:
where ∑m
i=1 P(ai|Dk ) = 1 ∀k , P(ai|Dj ) = 1
m  is the probability of selecting action a i ∈ A 
conditioned to be in configuration Dj ∈ D , A ={ ˙µD 1 ,˙µD 2 ,... ,˙µD m} is the set of avail -
able actions, ǫk is the exploration rate given by ǫk = 1 − αk where αk is the weight of the 
winning particle computed as:
such that 0 ≤ αk ≤ 1 . In addition, β denotes the index of the particle with the maximum 
weight given by:
In (8), if ǫk ≥ ρ , this means that L is facing a novel situation not seen before by E and so 
L will explore new actions by using the generalized errors (GEs) as explained in the com-
ing sections.
Abnormality Indicators and GEs The predictive messages (i.e., π( ˜Dk ) , π( ˜X(i)
k ) ) propa-
gated top-down the hierarchy are compared against sensory responses signaled via diag -
nostic messages (i.e., /afii9838( ˜X(i)
k ) , /afii9838( ˜Dk ) ) passing from bottom to up the hierarchy, resulting 
in multiple abnormality indicators and GEs. Evaluating the abnormality measurement 
at a certain node allows evaluating to what extent the current observations support the 
model’s predictions, while the GEs allow understanding of how we can suppress those 
abnormalities in the future. The multilevel abnormality indicators are defined as:
where DKL is the Kullback–Leibler divergence and BC is the Bhattacharyya coefficient. 
The GE associated with (12) and conditioned on transiting from ˜Dk−1 is defined as:
where ˙E ˜Dk
 is an aleatory variable described by a discrete probability density function 
(pdf) P( ˙E ˜Dk
) , while the GE projected on the GS space and associated with (13) can be 
expressed as:
(8)ak =



∗argmax
ak∈A
P
�
ak|D (β)
k
�
, if ǫk <ρ (exploitation),
q
�
ak−1 ,˜E ˜X (β)
k
�
, if ǫk ≥ ρ (exploration )
.
(9)Ŵ =


P(a1|D 1),P (a2|D 1), ... ,P (am |D 1)
...
... ... ...
P(a1|D m),P (a2|D m), ... ,P (am |D m)


(10)αk = max
i
{w (i)
k }N
i=1,
(11)β = *argmax
i
{w (i)
k }N
i=1 .
(12)ϒ ˜D k
= DKL
(
π( ˜Dk ),/afii9838(˜Dk )
)
+ DKL
(
/afii9838(˜Dk ),π( ˜Dk )
)
,
(13)ϒ ˜X (i)
k
=− ln
(
BC(π( ˜X(i)
k ),/afii9838(˜X(i)
k ))
)
,
(14)˜E ˜D k
=[ ˜Dk ,P( ˙E ˜D k
)]=[ ˜Dk ,/afii9838(˜Dk ) − π( ˜Dk )],
Page 14 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
where ˙E ˜X(i)
k
 is an aleatory variable described by a continuous pdf P( ˙E ˜X(i)
k
) and 
˜E ˜Z k
∼ N( ˜µ ˜E ˜Z k
,� ˜E ˜Z k
) characterized by the following statistical properties: 
˜µ ˜E ˜Zk
= ˜Zk − H ˜Xk , /Sigma1˜E ˜Zk
= H/Sigma1˜E ˜Zk
H⊺ + R , where ˜µ ˜E ˜Zk
 is the Kalman innovation com -
puted in the measurement space, and /Sigma1˜E ˜Zk
 is the innovation covariance.
Incremental Active Learning and Inference Active learning and active inference aim to 
reduce surprises (or abnormalities), either by developing a reliable world model or actively 
engaging with the environment [39]. Active learning allows an agent to build a predictive 
model capturing the novel world’s regularities through model parameter exploration. In 
contrast, active inference allows using the world model to infer the current context and 
consequently to infer what to do through the active states exploration. These two types of 
exploration provide a balanced trade-off between adaptive behavior that aims to minimize 
abnormalities by fulfilling the learner’s preferences on the one hand and acquiring informa-
tion about the world on the other hand.
Active states exploration: When encountering surprising conditions, L can discover 
new actions to avoid future abnormal situations. While L is exploring, its new actions 
evolve from the previous actions and current GEs by a linear function q(·) as pointed out 
in (8), which is calculated with the first-order Euler integration as follows:
where /Delta1k is the step size, ak−1 is the previous performed action, and P( ˙E ˜X(β)
k
) is the GE’s 
pdf defined in (15).
Model parameter exploration: Under abnormal conditions and during exploration, L 
can cluster the novel situations and encode them incrementally in the WM by updating 
the transition matrix and the active inference matrix, respectively. It is to note that dur -
ing abnormal situations, new configurations might appear representing novel relative 
distances between L and the other dynamic object not experienced by E . Thus, clustering 
the observed relative distance along with the new actions will lead to discovering new 
configurations and learning how to behave by facing them in the future (refer to Fig.  7). 
Consequently, a set C consisting of the relative distance-action pair can be performed 
during the abnormal period T (i.e., during exploration) as C ={ ˜Zk, ak}T
t  which can be 
used as input to the growing neural gas (GNG) for unsupervised clustering. GNG out -
puts a set of new configurations defined as 
D′ ={ D m+1,D m+2,... ,D m+n}={ D 1′,D 2′,... ,D n′} , where n is the total number of the 
newly acquired configurations and D ′
l ∼ N(µD ′
l
,�D ′
l
) such that D′
l ∈ D′ . Analyzing the 
dynamic evolution of the new configurations allows estimating the transition probability 
P( ˜Dt | ˜Dt−1) encoded in /Pi1
′
 , which is defined as:
(15)˜E ˜X(i)
k
=[ ˜X(i)
k ,P( ˙E ˜X(i)
k
)]=[ ˜X(i)
k ,H−1 ˜E ˜Zk
],
(16)q
(
ak−1, ˜E ˜X (β)
k
)
= ak−1 + �kP( ˙E ˜X (β)
k
),
(17)/Pi1
′
=


P(D
′
1|D
′
1), ... ,P (D
′
1|D
′
n)
... ... ...
P(D
′
n|D
′
1), ... ,P (D
′
n|D
′
n)

 ,
Page 15 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
where ∑m
k=1 P(D
′
i|D
′
k ) = 1 ∀i . Consequently, the updated global transition matrix 
/Pi1
′′
∈ R(m+n),(m+n) is expressed as:
where /Pi1 is the original transition matrix and /Pi1
′
 is the newly acquired one.
Likewise, the newly discovered action-configuration pairs characterized by P(a
′
k|D
′
1) 
are encoded in Ŵ
′′
 according to:
and the active inference table can be adjusted as follows:
where Jm ×n =[ aij]m ×n and Jn×m =[ bji]n×m  are the unit matrices, such that a ij = b ji = 1 
∀i, j . It is to note that Ŵ
′′
 ’s row do not summing 1 due to the addition of the unit matrices 
and Ŵ
′
 . Thus, normalization is needed, and it can be performed as:
where ˆγ ij =
γ ij∑n
j=1 γ ij
 ∀i.
Action Update L relies on the abnormality indicators calculated at time k and defined 
in (12) and (13) to evaluate the performed actions at time k − 1 . Under abnormal condi-
tions, L learns how to avoid those abnormalities in the future by seeking information 
(18)/Pi1
′′
=
[ /Pi10m,n
0n,m /Pi1
′
]
,
(19)Ŵ
′
=


P(a
′
1|D
′
1),P (a
′
2|D
′
1), ... ,P (a
′
n|D
′
1)
...
... ... ...
P(a
′
1|D
′
n),P (a
′
2|D
′
n), ... ,P (a
′
n|D
′
n)

 ,
(20)Ŵ
′′
=
� Ŵ 1
n(Jm×n)
1
n(Jn×m) Ŵ
′
�
=


γ 11 ... γ 1n
... ... ...
γ n1 ... γ nn


(21)ˆŴ
′′
=


ˆγ 11 ... ˆγ 1n
... ... ...
ˆγ n1 ... ˆγ nn

,
Fig. 7 A schematic view of an overtaking situation example used in our study: (a) exploratory behavior 
minimizing the divergence with the predicted trajectory. (b) associating exploratory clusters to the learning 
model
Page 16 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
about the surrounding environment and how to engage inside it based on the two types 
of exploration discussed previously.
In contrast, during exploitation and under abnormal conditions, L updates the exist -
ing active inference table and transition matrix using the diagnostic messages ( /afii9838(˜Dk ) , 
/afii9838(ak−1) ). The existing transition matrix can be updated using the GE defined in (14) as 
follows:
The active inference table Ŵ can be adjusted according to:
where α ∈[ 0, 1] , π(ak) = P(·| ˜Dk ) is a specific row in Ŵ and P( ˙Eak ) is the GE’s pdf related 
to the active states that can be calculated as [40]:
where /afii9838(ak−1) = /afii9838( ˜Dk ) × P( ˜Dk |ak−1).
4  Experiments and results analysis
In this section, we consider the proposed generalized hierarchical model in different set-
tings. We discuss the action-oriented model and the associated learning costs to evalu -
ate the learning model performance. Additionally, the proposed approach is compared 
with two benchmark schemes, AIL [16] and Q-learning [41], to highlight the improve -
ments of the introduced approach in terms of efficiency and free energy.
4.1  Experimental dataset
The employed dataset was obtained from real experiments carried out on a university 
campus for different maneuvering situations, including two Intelligent Campus Automo-
bile (iCab) vehicles [42]. The expert demonstrations are collected during the experiments 
by considering two AVs, iCab1 and iCab2, interacting in the environment to perform a 
specific driving maneuver, i.e., iCab2 overtakes from the left side iCab1 where iCab2 is 
playing the role of an expert ( E ) and iCab1 represents a dynamic object ( O ). Each AV (i) 
is equipped with exteroceptive and proprioceptive sensors from which odometry trajec -
tories and control parameters are collected to analyze the interactions between AVs. The 
sensory modules provide four-dimensional information, including the positions of AV 
in (x, y) coordinates, and the control parameters in terms of the AV’s velocity (i.e., ˙x , ˙y).
4.2  Offline learning phase
The expert demonstrations collected from the interactions between the two AVs 
(i.e., iCab1 and iCab2) are used to learn the SM in an offline manner as explained in 
Sect. 3.3.1. The acquired SM consists of 24 joint clusters encoding the dynamic interac -
tion between the two AVs, and the corresponding transition matrix is shown in Fig.  8. 
Consequently, FP-M is initialized using 24 learned configurations, encoding the position 
data and control parameters, as explained in Sect. 3.3.2.
(22)π∗( ˜Dk ) = απ( ˜Dk ) + (1 − α)P( ˙E ˜D k
).
(23)π∗(ak) = απ(ak) + (1 − α)P ( ˙Eak ),
(24)Eak−1 =[ ak−1 ,P( ˙Eak−1 )]=[ ak−1 ,/afii9838(ak−1 ) − π(ak−1 )],
Page 17 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
4.3  Online learning phase
The FP-M acquired offline is enriched with active states to build the AFP-M as dis -
cussed in Sect.  3.4. AFP-M enables the learner agent ( L ) in first person during the 
online phase. While the L is experiencing a specific driving task, it starts evaluating 
the situation based on its belief about the environmental state’s evolution and actual 
observations. During normal situations (i.e., when observations match predictions), 
L decides to execute expert-like maneuvers by imitating the expert’s actions (i.e., 
exploitation). In contrast, when an abnormal condition occurs, i.e., an unexpected 
situation, L starts exploring a new course of actions based on observations and gener -
alized errors, allowing to avoid abnormal situations in the future and to reach the goal 
successfully (e.g., overtaking a dynamic object).
4.3.1  Action‑oriented model
Figure  9a depicts an example of how L agent switches between exploration and 
exploitation while performing a driving task where the initial position is different 
from that seen in the demonstrations (i.e., from where the expert agent started). 
The learning agent recognizes an abnormal situation after comparing its current 
configuration (which will lead to an unexpected trajectory) with the expected one. 
Fig. 8 a Autonomous vehicles: iCab1 and iCab2. b An example of overtaking scenario. c the generated 
clusters, and d) the reference transition matrix based on AVs movements
Page 18 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
Fig. 9 A driving task example. In a L explores new actions based on generalized errors and b shows the 
original and explored clusters of expected and new trajectories followed by L
Page 19 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
Therefore, L adopts an exploratory policy to minimize the divergence between the 
observed configurations and the expected ones. It can be seen from the figure how 
the newly explored actions allowed L to approach the reference model (i.e., go back 
to the normal situation) and switch back to exploitative behavior, demonstrating that 
the learned action-oriented model adapts efficiently to changing variability in the 
environment.
Moreover, Fig. 9b illustrates the newly generated configurations and their correspond-
ing mean actions obtained from the new experience. The novel learned configurations 
obtained by clustering both the performed actions and the generalized errors allow the L 
later to infer the best actions when facing similar situation.
The evolution of the transition matrix ( /Pi1 ) is shown in Fig.  10 starting from the origi -
nal one (learned offline from demonstrations) shown in Fig.  8d. Figure  10a shows the 
updated matrix during exploitation, while Fig.  10b displays the new matrix estimated 
after clustering the trajectory that was followed during exploration, encoding the novel 
configurations. Figure  10c shows the updated transition matrix obtained after combin -
ing Fig. 10a and b and so by appending the explored configurations to the original transi-
tion matrix. To avoid memorizing similar configurations, we used a predefined threshold 
to evaluate how similar the new configurations are to the existing ones before deciding 
whether to append them. This process will lead to a refined transition matrix with fewer 
elements, as shown in Fig. 10d and Table 3.
Figure 11 illustrates the exploratory behavioral responses during subsequent actions 
performed in the experiments shown in Fig. 9a. The maximum particle weight after each 
performed action is displayed in Fig.  11a. It can be shown that at the beginning of the 
experiment, the distinction between the L ’s performance and the expectation is high, 
resulting in low particles’ weights and so leading the L to explore new actions allowing 
to resolve uncertainties about the environment. Figure  11b demonstrates that L makes 
exploratory and novelty-seeking choices at the beginning of the experiment and tends 
to minimize the exploration probability after updating its belief about the best way to 
interact with the environment (after 15 trails). Therefore, L is confident to behave imi -
tatively, and compels it to choose exploitative actions. The panel presented in Fig.  11b 
Table 3 Transition matrix evolution during active learning
Original /Pi1 Explored /Pi1 Merged /Pi1 Updated /Pi1
24 clusters 6 clusters 30 clusters 28 clusters
Fig. 10 Transition matrix evolution during the active learning process. a Original; b Exploration; c Merging; d 
Updating
Page 20 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
illustrates whether L performs exploratory or exploitative actions as indicated by the 
blue dots. Darker background implies higher certainty about selecting an exploitative 
action. Moreover, Fig.  11d shows that imitative behavior causes decreasing mean error 
that represents the divergence between the agent’s observation and prediction.
4.3.2  Cost of learning
Updating and correcting the beliefs about the agent’s surroundings minimize the FE 
measurement via hierarchical processing in which prior expectations generate top-down 
predictions of likely observations and where discrepancies between predictions and 
observations ascend to hierarchically higher levels as prediction errors. In this section, 
the efficiency of four action-oriented models is studied in terms of cumulative FE meas -
urement during 2000 training episodes as follows: (i)  Model A applies GNG to cluster 
novel experienced trajectories and employs GEs to calculate the exploratory actions, 
(ii) Model B applies GNG to cluster novel experienced trajectories and uses predefined 
Fig. 11 Exploratory behavioral responses during subsequent actions in terms of a maximum particles 
weights, b exploitation–exploration choices, c probability of exploration, and d mean error quantifying the 
divergence between L ’s observations and its prediction
Page 21 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
actions during exploration, (iii)  Model C only employs GEs to calculate the exploratory 
actions, and iv) Model D only uses predefined actions during exploration.
Figure  12 displays the comparison among the four models defined previously in 
terms of FE measurement. Comparing Fig.  12a with the provided results from other 
Fig. 12 Cumulative free energy. Models A–D, respectively
Page 22 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
models (see Fig.  12b–d) validates clustering the novel configuration and calculating 
the associated actions using the GEs has a big impact on minimizing the FE measure -
ment during the online learning phase.
Moreover, Fig.  13 evaluates the performance of the proposed model in terms of 
reward compared to the Q-learning algorithm [41] and AIL [16] method (action-ori -
ented Model B). The results demonstrate that by employing GEs in the action selec -
tion procedure, we accelerate the learning of the unseen environment, achieving it 
within around 400 episodes through reduced exploration time. Furthermore, the 
findings demonstrate that the hierarchical model is capable of inferring exploitative 
and exploratory actions properly, thereby minimizing FE measurements (maximizing 
reward), which outperforms two other methods. To this end, the results affirm that by 
minimizing the GEs during the online learning phase, the agent learns accurate repre -
sentation for the inferences and predictions during interaction with its surroundings. 
These representations are effectively utilized within the hierarchical model to reduce 
the divergence between the observation and prediction and generate valid beliefs that 
confirm the adaptability of the proposed model.
5  Conclusion
This paper introduced a hierarchical self-awareness ADS that advances a probabil -
istic computational account of action, observation, and imitation abilities grounded 
in an active inference framework. For learning and adaptation, the agent must trans -
form the sensory inputs into a reliable perception of the world. The proposed model 
comprises several modules forming the perception-action cycle that links the autono -
mous vehicle to its environment. With inspiration from the biological brain, the dif -
ferent modules are linked together via message passing, where each module handles 
specific functionalities. The hierarchical message passing is represented by inferences 
and perceptions across different modalities of multisensorial information (i.e., extero -
ceptive and proprioceptive data). Learning this association allows the learning model 
to predict the perceptual consequences of acting. These representations are employed 
to minimize prediction errors and accurately predict how sensory signals will change 
in response to specific actions. The experimental evaluations show that modify -
ing and updating the belief during the online learning phase via the back-projection 
of detected errors at the multilevel hierarchy solves the exploration–exploitation 
Fig. 13 Cumulative reward during 2000 training episodes
Page 23 of 24
Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 
dilemma. It also shows that the generalized hierarchical model learns incrementally 
from new information provided by the perception of the surroundings to adapt to 
unexpected situations.
Abbreviations
AD  Autonomous driving
ADS  Autonomous driving system
AFP-M  Active First-Person Model
AV  Autonomous vehicle
C-GBDN  Coupled Generalized Dynamic Bayesian Networks
E  Expert agent
FE  Free energy
FP-M  First-person model
GDBN  Generalized Dynamic Bayesian Networks
GE  Generalized error
GS  Generalized state
IL  Imitation learning
KF  Kalman filter
L  Learner agent
M-MJPF  Modified Markov jump particle filter
O  Dynamic Object
PF  Particle filter
POMDP  Partially Observed Markov Decision Process
RL  Reinforcement learning
SM  Situation model
WM  World model
Author Contributions
Sheida Nozari analyzed the data, conducted the experiments, and wrote the manuscript, and Ali Krayani, Pablo Marin, 
Lucio Marcenaro, David Martin Gomez and Carlo Regazzoni provided critical revisions.
Funding
The project is funded by University of Genoa.
Availability of data and materials
Not applicable. Data and material restricted by the University Carlos III of Madrid
Declarations
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
Not applicable.
Received: 20 June 2023   Accepted: 10 July 2024
References
 1. M. Buehler, K. Iagnemma, S. Singh, The DARPA Urban Challenge: Autonomous Vehicles in City Traffic, 1st edn. (Springer 
Publishing Company Incorporated, Cham, 2009)
 2. J. Garcia, et al., A comprehensive study of autonomous vehicle bugs. In: 2020 IEEE/ACM 42nd International Conference 
on Software Engineering (ICSE), pp. 385–396 (2020)
 3. J. Schlatow, et al., Self-awareness in autonomous automotive systems. In: Design, Automation & Test in Europe Confer-
ence & Exhibition (DATE), 2017, pp. 1050–1055 (2017). https:// doi. org/ 10. 23919/ DATE. 2017. 79271 45
 4. C.S. Regazzoni et al., Multisensorial generative and descriptive self-awareness models for autonomous systems. Proc. 
IEEE 108(7), 987–1010 (2020). https:// doi. org/ 10. 1109/ JPROC. 2020. 29866 02
 5. M. Ravanbakhsh, et al., Learning multi-modal self-awareness models for autonomous vehicles from human driving. In: 
2018 21st International Conference on Information Fusion (FUSION), pp. 1866–1873 (2018). https:// doi. org/ 10. 23919/ 
ICIF. 2018. 84556 67
 6. S. Nozari, L. Marcenaro, D. Martin, C. Regazzoni, Observational learning: Imitation through an adaptive probabilistic 
approach. In: 2021 IEEE International Conference on Autonomous Systems (ICAS), pp. 1–5 (2021). IEEE
 7. H. Ravichandar, A. Polydoros, S. Chernova, A. Billard, Recent advances in robot learning from demonstration. Annu. Rev. 
Control Robot. Auton. Syst. 3, 297–330 (2020)
Page 24 of 24Nozari et al. EURASIP Journal on Advances in Signal Processing         (2024) 2024:92 
 8. Brantley, K., Sun, W., Henaff, M., Disagreement-regularized imitation learning. In: International Conference on Learning 
Representations (2019)
 9. G. Ongaro, T.J. Kaptchuk, Symptom perception, placebo effects, and the bayesian brain. Pain 160(1), 1 (2019)
 10. J. Hohwy, The Predictive Mind, OUP Oxford (2013)
 11. M. Priorelli, I.P . Stoianov, Flexible intentions in the posterior parietal cortex: an active inference theory. bioRxiv (2022)
 12. B. Millidge, Combining active inference and hierarchical predictive coding: a tutorial introduction and case study (2019)
 13. L.F. Barrett, W.K. Simmons, Interoceptive predictions in the brain. Nat. Rev. Neurosci. 16(7), 419–429 (2015)
 14. K. Friston et al., Active inference and learning. Neurosci. Biobehav. Rev. 68, 862–879 (2016)
 15. K. Friston, The free-energy principle: A unified brain theory? Nat. Rev. Neurosci. 11(2), 127–138 (2010)
 16. S. Nozari, A. Krayani, P . Marin-Plaza, L. Marcenaro, D.M. Gómez, C. Regazzoni, Active inference integrated with imitation 
learning for autonomous driving. IEEE Access 10, 49738–49756 (2022)
 17. B. Paden et al., A survey of motion planning and control techniques for self-driving urban vehicles. IEEE Trans. Intell. Veh. 
1(1), 33–55 (2016)
 18. D. González et al., A review of motion planning techniques for automated vehicles. IEEE Trans. Intell. Transp. Syst. 17(4), 
1135–1145 (2015)
 19. E. Pakdamanian, et al., Deeptake: prediction of driver takeover behavior using multimodal data. In: Proceedings of the 
2021 CHI Conference on Human Factors in Computing Systems, pp. 1–14 (2021)
 20. Y. Wang et al., Trajectory planning and safety assessment of autonomous vehicles based on motion prediction and 
model predictive control. IEEE Trans. Veh. Technol. 68(9), 8546–8556 (2019)
 21. T. Onishi, et al., End-to-end Learning Method for Self-Driving Cars with Trajectory Recovery Using a Path-following Func-
tion. In: 2019 International Joint Conference on Neural Networks, pp. 1–8 (2019). https:// doi. org/ 10. 1109/ IJCNN. 2019. 
88523 22
 22. Z. Chen, X. Huang, End-to-end learning for lane keeping of self-driving cars. In: 2017 IEEE Intelligent Vehicles Symposium 
(IV), pp. 1856–1860 (2017). IEEE
 23. M. Bojarski, et al., End to end learning for self-driving cars. arXiv preprint arXiv: 1604. 07316 (2016)
 24. A. Sauer, N. Savinov, A. Geiger, Conditional affordance learning for driving in urban environments. In: Conference on 
Robot Learning, pp. 237–252 (2018). PMLR
 25. D. Vogt, H. Ben Amor, E. Berger, B. Jung, Learning two-person interaction models for responsive synthetic humanoids. J. 
Virtual Real. Broadcast. 11(1), 1–11 (2014)
 26. A. Droniou, et al., Learning a repertoire of actions with deep neural networks. In: 4th International Conference on Devel-
opment and Learning and on Epigenetic Robotics, pp. 229–234 (2014). IEEE
 27. M. Liu, W. Buntine, G. Haffari, Learning how to actively learn: a deep imitation learning approach. In: Proceedings of the 
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1874–1883 (2018)
 28. B.D. Argall, S. Chernova, M. Veloso, B. Browning, A survey of robot learning from demonstration. Robot. Auton. Syst. 57(5), 
469–483 (2009)
 29. S. Ross, D. Bagnell, Efficient reductions for imitation learning. In: Proceedings of the Thirteenth International Conference 
on Artificial Intelligence and Statistics, pp. 661–668 (2010). JMLR Workshop and Conference Proceedings
 30. T. Gangwani, J. Peng, State-only imitation with transition dynamics mismatch. arXiv preprint arXiv: 2002. 11879 (2020)
 31. R. Ogishima, I. Karino, Y. Kuniyoshi, Combining imitation and reinforcement learning with free energy principle (2020)
 32. A. Kuefler, J. Morton, T. Wheeler, M. Kochenderfer, Imitating driver behavior with generative adversarial networks. In: 2017 
IEEE Intelligent Vehicles Symposium (IV), pp. 204–211 (2017). IEEE
 33. Y. Schroecker, M. Vecerik, J. Scholz, Generative predecessor models for sample-efficient imitation learning. arXiv preprint 
arXiv: 1904. 01139 (2019)
 34. K. Friston, J. Kilner, L. Harrison, A free energy principle for the brain. J. Physiol. 100(1–3), 70–87 (2006)
 35. K.J. Friston, J. Daunizeau, J. Kilner, S.J. Kiebel, Action and behavior: a free-energy formulation. Biol. Cybern. 102(3), 
227–260 (2010)
 36. S. Kim, C. Laschi, B. Trimmer, Soft robotics: a bioinspired evolution in robotics. Trends Biotechnol. 31(5), 287–294 (2013)
 37. S. Nozari, et al., Active inference integrated with imitation learning for autonomous driving. IEEE Access 10, 49738–
49756 (2022). https:// doi. org/ 10. 1109/ ACCESS. 2022. 31727 12
 38. A. Krayani, M. Baydoun, L. Marcenaro, A.S. Alam, C. Regazzoni, Self-learning bayesian generative models for jammer 
detection in cognitive-UAV-radios. In: GLOBECOM 2020 - 2020 IEEE Global Communications Conference, pp. 1–7 (2020). 
https:// doi. org/ 10. 1109/ GLOBE COM42 002. 2020. 93225 83
 39. A. Ofner, S. Stober, Balancing active inference and active learning with deep variational predictive coding for EEG. In: 
2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pp. 3839–3844 (2020). https:// doi. org/ 10. 
1109/ SMC42 975. 2020. 92831 47
 40. A. Krayani, A.S. Alam, L. Marcenaro, A. Nallanathan, C. Regazzoni, A novel resource allocation for anti-jamming in 
cognitive-UAVs: an active inference approach. IEEE Commun. Lett. 26(10), 2272–2276 (2022). https:// doi. org/ 10. 1109/ 
LCOMM. 2022. 31909 71
 41. C.J. Watkins, P . Dayan, Q-learning. Mach. Learn. 8(3), 279–292 (1992)
 42. P . MarÃ n-Plaza, et al., Stereo vision-based local occupancy grid map for autonomous navigation in ROS. In: Proceedings 
of the 11th Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 3: 
VISAPP , (VISIGRAPP 2016), pp. 701–706. SciTePress (2016). https:// doi. org/ 10. 5220/ 00057 87007 010706 . INSTICC
Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.