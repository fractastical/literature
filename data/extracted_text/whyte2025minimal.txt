 On the Minimal Theory of Consciousness Implicit in Active Inference     Christopher J. Whyte1,2,3, Andrew W. Corcoran1, Jonathan Robinson1, Ryan Smith4, Rosalyn J. Moran5, Thomas Parr6, Karl J. Friston7, Anil K. Seth8,9, Jakob Hohwy1*      Author Affiliations 1Monash Centre for Consciousness & Contemplative Studies, Monash University,  Melbourne, Australia 2Brain and Mind Centre, University of Sydney, Sydney, Australia 3Centre for Complex Systems, University of Sydney, Sydney, Australia 4Laureate Institute for Brain Research, Tulsa, OK, USA 5Department of Neuroimaging, Kingâ€™s College London, London, United Kingdom 6Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, UK 7Queen Square Institute of Neurology, University College London, UK 8Sussex Centre for Consciousness Science and Department of Informatics, University  of Sussex, Brighton, UK 9Program on Brain, Mind, and Consciousness, Canadian Institute for Advanced  Research, Toronto, Canada *Corresponding author: jakob.hohwy@monash.edu       Abstract The multifaceted nature of subjective experience poses a challenge to the study of consciousness. Traditional neuroscientific approaches often concentrate on isolated facets, such as perceptual awareness or the global state of consciousness and construct a theory around the relevant empirical paradigms and findings. Theories of consciousness are, therefore, often difficult to compare; indeed, there might be little overlap in the phenomena such theories aim to explain.  Here, we take a different approach: starting with active inference, a first principles framework for modelling behaviour as (approximate) Bayesian inference, and building up to a minimal theory of consciousness, which emerges from the shared features of computational models derived under active inference. We review a body of work applying active inference models to the study of consciousness and argue that there is implicit in all these models a small set of theoretical commitments that point to a minimal (and testable) theory of consciousness.  
2  
 2 
1. Introduction  Conscious experience is heterogeneous and multifaceted. At first pass, the scientific study of consciousness can be divided into three related (but in practice, largely independent) research programs: the study of contents, state, and self (Seth, 2021). The contents of consciousness are the qualities or elements within experience that an agent is conscious of (e.g., the image of a red rose against a green background, or the aroma of freshly brewed coffee). Contents are studied by controlling for physical stimulus attributes and the overall state of consciousness (such as drowsiness), while varying subjective percepts (Baars, 2002). Conscious organisms also have different global states of consciousness, which are often assessed behaviourally (e.g., through the Glasgow Coma Scale; Teasdale et al., 2014) and are crucial to assess patients with disorders of consciousness. Such states include the vegetative state, various states of sleep, normal waking states, and perhaps states like delirium or psychedelia (for discussion, see Bayne et al 2016). Consciousness (at least in humans) is typically also accompanied by some form of minimal and/or narrative self-awareness  (Gallagher, 2000) along with experiences of embodiment, selfhood and personhood (Ciaunica et al., 2022; Seth, 2013; Seth & Tsakiris, 2018).   Most neuroscientific theories of consciousness take some subset of these phenomena as their explanatory target and construct a theory around the relevant empirical paradigms and findings (Seth & Bayne, 2022). For example, global workspace theory (Baars, 2005; Baars et al., 2013), and its contemporary incarnation, global neuronal workspace theory (Dehaene et al., 2011; Mashour et al., 2020), were constructed around the method of contrastive analysis, which treats the awareness of perceptual contents as the dependent variable; that is, varying whether the participant is conscious of some particular content evoked by a stimulus. The global state of consciousness was initially regarded as a background condition (Dehaene et al., 2006), making global neuronal workspace theory chiefly a theory of conscious contents. Since then, the theory has also been applied to experiments that manipulate the global state of consciousness through anaesthesia (for review see Mashour et al., 2020). However, the initial assumption that conscious state is a background condition for awareness of contents, rather than a contextual construct that constrains content, is arguably still present in contemporary versions of the theory (see Bayne et al., 2016; Bayne & Carter, 2018). Higher order theories (Brown et al., 2019; Fleming, 2020; Lau & Rosenthal, 2011) are, likewise, chiefly theories of how contents become conscious and are so far silent on the relationship between the contents and state of consciousness. Similarly, integrated information theory (Albantakis et al., 2023; Oizumi et al., 2014; Tononi et al., 2016), another leading theory of consciousness, was developed with the explicit aim of solving the hard problem of consciousness (i.e., explaining why some physical structures generate subjective experience and others do not). Integrated information theory treats consciousness in wholly intrinsic terms, thereby downplaying the role of overt behaviour, which arguably plays a major role in shaping not only what we are conscious of but also in determining the qualitative character of conscious contents (Oâ€™Regan & NoÃ«, 2001; Seth, 2014). Other theories, such as the Self-model Theory of Subjectivity (Metzinger, 2004) or the Projective Consciousness model (Rudrauf et al., 2017), focus on explaining the (seeming) presence of a self or first person perspective. There are still other theories of consciousness beyond those cited above, many of which 
3  
 3 
privilege specific explanatory targets and methodological approaches. This state of affairs thus poses a double challenge: not only are theories of consciousness difficult to arbitrate between â€” on the basis of empirical evidence (Yaron et al., 2022) â€” it is also sometimes not clear whether such theories are aiming to explain the same empirical data (Seth & Bayne, 2022).  Instead of starting with one or more specific properties of consciousness, as our primary explanatory target, in this paper we start with active inference, a framework for modelling adaptive behaviour as (approximate) Bayesian inference and ask whether we can build toward a theory of consciousness. Active inference was developed in the context of theories of predictive coding (Rao & Ballard, 1999; Srinivasan et al., 1982) and the â€˜Helmholtz machineâ€™ (Dayan et al., 1995) that framed brain function as inference, drawing from advances in variational inference and message passing in statistics (Beal, 2003; Wainwright & Jordan, 2008; Winn & Bishop, 2005). The novelty of the framework lies not in providing unique predictions. Indeed, many of the alternative but domain-specific normative modelling frameworks in computational neuroscience and related disciplines are consistent with, or equivalent to, active inference, but tend to apply to a narrower range of contexts (Da Costa, Sajid, et al., 2020; Sajid, Ball, et al., 2021; Sajid, Da Costa, et al., 2021). Instead, the novelty of active inference lies in its generality. An extraordinarily diverse suite of behaviours can be modelled under the framework through the minimisation of the same free energy functionals (Parr et al., 2022): namely, variational free energy and expected free energy. Indeed, the active inference framework has been applied to phenomena across the cognitive and neural sciences in areas as diverse as visual search (Cullen et al., 2020; Mirza et al., 2018; Parr et al., 2021) and the comprehension and generation of language (Friston, Parr, et al., 2020; Friston, Sajid, et al., 2020). Because these phenomena all emerge when minimising the same objective functions â€” which can be decomposed into a small set of interpretable (quasi teleological) terms â€” active inference allows us to expose commonalities and differences across diverse phenomena of interest that may be obscured in a less general modelling framework. Thus, we will argue that active inference, precisely because it is not a theory of consciousness per se, is able to do justice to the diverse properties of conscious experience (c.f. Hohwy & Seth, 2020; Vilas et al., 2021). The argument is then that, by beginning with a general framework for modelling adaptive behaviour, the contours of a theory of consciousness will nevertheless emerge naturally, as individual conscious phenomena (i.e., canonical paradigms in consciousness science) begin to be explained by a singular formal (i.e. mathematical) formulation.    Practically, a natural first step (which is already very much underway) â€” in the construction of a minimal theory of consciousness from active inference â€” is to deploy the framework to model individual phenomena that are paradigmatic in the field of consciousness science. Having developed such models, one can then explore their computational properties and behaviour, and map variables and parameters in each model to phenomenological aspects of the specific conscious processes being modelled. Each model then becomes a building block for the theory, which grows as more models accumulate. Through this incremental process, a fully-fledged active inference theory of consciousness may gradually take shape. A key component of this process therefore lies in identifying the computational properties common among models that have both 
4  
 4 
explanatory power and systematicity across the diverse phenomena within consciousness science (Atkinson et al., 2000; Seth, 2009). Importantly, in line with the initially minimal nature of the theory, we will restrict ourselves to discussing well-studied paradigms within the neuroscience of consciousness and refrain from speculating about more general functions of consciousness (including potential evolutionary origins) beyond these experimental tasks. In effect, our approach is not so much about the emergence of consciousness (see, for example, (Fleming & Michel, 2024)) but about the properties of a system that exhibits phenomena associated with consciousness.â€  This paper aims to speak to two key audiences which are, at present, largely independent of one another; experimental neuroscientists and cognitive scientists working on consciousness, and neuronal modellers and theorists working within the active inference framework. For the experimental community we wish to provide a precise but accessible guide to active inference and its relationship to consciousness; with particular emphasis on the theoryâ€™s empirical commitments. For the community of theorists working on active inference and not currently working on consciousness we wish to highlight both the ready applicability of the framework to consciousness and the areas in need of further theoretical and formal development.   We begin in sections 2-3 by giving a brief introduction to the active inference modelling framework highlighting the decompositions of variational free energy and expected free energy into a small set of teleologically meaningful terms. We also introduce general features of the generative model architectures that underwrite active inference and their potential neural implementation. These two sections are the most technical and may be skipped by readers already familiar with the formalism of active inference. We encourage readers unfamiliar with active inference to persevere through these opening sections. Active inference is, at bottom, a mathematical framework and to discuss it with precision we must use the relevant formalism. Section 4 then surveys the modelling literature relevant to consciousness science and presents a novel extension of an existing model that allows active inference to contact research on the global state of consciousness. Finally, in section 5 we argue that implicit in all these models is a set of assumptions and theoretical commitments that, once made explicit, entail a minimal but empirically productive theory of consciousness. To make the relationship between the minimal theory and experiment as clear as possible we follow recent work in the philosophy of consciousness science (Negro, 2024) and discuss the theory and its empirical commitments through the lens of Lakatosâ€™ account of â€˜scientific research programsâ€™ (Lakatos, 1968).  Finally, we note that the primary aim of this paper is didactic rather than polemic. That is, we will not argue for the inadequacy of other competing theories of consciousness and then offer active inference as a replacement. Rather we aim to provide a positive account of the (minimal) theory of consciousness implicit in the active inference framework. We are of the opinion that the most productive way forward is to spell out, in as precise a manner as possible, the structure of the theory, and its relationship to empirical data in the service of facilitating empirically driven theory comparison.   
5  
 5 
2. Variational and expected free energy  An organism that maintains its bodily integrity must be able to stay within the narrow range of states that are consistent with its existence (e.g., for mammals, maintaining a relatively constant internal body temperature); this entails that an organism will spend the majority of its time in a relatively restricted set of characteristic states (Friston, 2013; Tschantz et al., 2020). Under active inference, this is modelled by interpreting the organismâ€™s phenotype as a generative model of its niche (i.e. a model of how its sensory input is generated), which assigns a high probability to the observations associated with frequently occupied states; i.e., the states that are characteristic of the kind of organism in question (Corcoran & Hohwy, 2018; Ramstead et al., 2020; Ramstead et al., 2018, 2020, 2021). The perception-action cycle is, therefore, cast as an optimisation problem, where the primary objective function being extremised â€” variational free energy â€” stands in for the (log) evidence for a generative model of how an organism's observations are generated (Da Costa, Parr, et al., 2020). Crucially, the generative model entails a prior belief that action sequences are more plausible when they minimise expected free energy. This means there are two objective functions to consider. Specifically, (negative) variational free energy is a computationally tractable lower bound on log model evidence, and (negative) expected free energy can be thought of as approximating expected log model evidence (Parr & Friston, 2018a). Minimising variational free energy equips the organism with an approximation to model evidence that can be leveraged to infer hidden states of the world and learn various statistics of the environment. Beliefs in this model can then be used to select actions that minimise expected free energy and thus keep the agent in phenotypically characteristic states consistent with its continued survival. This licences an interpretation of behaviour as self-evidencing (Hohwy, 2016, 2020, 2021), where organisms take actions that maximise the evidence for their model of the world. The quantities that go into this self-evidencing process are crucial and we will examine each in turn.  Variational free energy (eq.1) is the expected difference, under the approximate posterior (q), between the log of the approximate posterior and the log of the generative model (p) â€” i.e., the joint probability of observations (o) and their causes â€” or hidden states (s). The approximate posterior reflects the inferred hidden state of the world given sensory data.  ğ¹=Î•ğ‘(ğ‘ )[lnğ‘(ğ‘ )âˆ’lnğ‘(ğ‘œ,ğ‘ )]                       (1)     =ğ·ğ¾ğ¿[ğ‘(ğ‘ )||ğ‘(ğ‘ |ğ‘œ)]âŸâŸâŸâŸâŸâŸâŸğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦+(âˆ’lnğ‘(ğ‘œ))âŸâŸâŸâŸâŸğ‘ ğ‘¢ğ‘Ÿğ‘ğ‘Ÿğ‘–ğ‘ ğ‘’                =ğ·ğ¾ğ¿[ğ‘(ğ‘ )||ğ‘(ğ‘ )]âŸâŸâŸâŸâŸâŸâŸğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦âˆ’Eğ‘(ğ‘ )[lnğ‘(ğ‘œ|ğ‘ )]âŸâŸâŸâŸâŸâŸâŸğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦                 
Variational free energy admits two key decompositions, each of which highlights complementary perspectives on its essential properties. Line two of equation 1 uses the product rule of probability and the fact that model evidence does not depend on hidden states (allowing us to drop the expectation operator in the second term) to decompose variational free energy into two terms: the relative entropy or KL divergence between 
6  
 6 
the approximate posterior and the true posterior, and the surprise or negative model evidence (i.e. the probability of sensory data averaged over hidden states). The KL divergence is always greater than or equal to zero, so variational free energy is always greater than or equal to surprise (a.k.a., surprisal or self information). Thus, variational free energy is an upper bound on surprise, and will be identical to surprise when the approximate posterior matches the true posterior.   This first decomposition is didactically useful in understanding the properties of variational free energy, but it is not useful for describing how variational free energy is minimised. This is because the value of the true posterior is computationally intractable and therefore cannot be known by the agent. In the third line we decompose variational free energy into two terms, complexity and accuracy, each of which are computationally tractable. Complexity is the KL divergence between the approximate posterior over states and the prior over states (i.e. beliefs about hidden states prior to receiving sensory data) and scores the difference between the prior and the approximate posterior. One can think of this as a regularisation term on the magnitude of belief updating. A large change between the prior and approximate posterior results in a high complexity. Accuracy is the expected log likelihood of observations (i.e. the probability of current observations under each hidden state). Minimising variational free energy therefore requires organisms to trade off between maximising accuracy and minimising complexity. This is a crucial point. Sensory input is noisy and ambiguous; given noisy data it is always possible to increase the accuracy by shifting the approximate posterior to fit current data. However, constantly making large changes to the posterior greatly reduces the ability of the model to generalise to new observations. Without penalising for large (Bayesian) belief updates, a maximally â€œaccurateâ€ model will over-fit to noise and be in need of constant revision (Sengupta et al., 2013). Minimising variational free energy, therefore, ensures that agents are equipped with a generalisable explanation for â€” or model of â€” their sensed world.   In virtue of being self-organising creatures, agents must select actions that, on average, minimise variational free energy (Friston, Rigoli, Ognibene, Mathys, Fitzgerald & Pezzulo, 2015; Parr & Friston, 2019). Purely reactive actions such as reflex arcs can be formulated in terms of variational free energy minimisation by taking actions that bring about observations consistent with a (homeostatic or proprioceptive) set point encoded in prior beliefs (Buckley et al., 2017; Tschantz et al., 2022). However, more sophisticated actions or action sequences (i.e., policies) require some form of counterfactual computation over future observations (i.e., planning). This planning rests on expected free energy (eq. 2), which treats the observable consequences of a policy as random variables (because they have yet to be observed). Expected free energy takes a weighted sum over expected observations to approximate the expected outcome of actions under each policy; hence 'expected' free energy. One can think of variational free energy as a special case of expected free energy that pertains to the present; when observations are known and do not depend on future actions.  Thus, according to active inference, action selection, like perception, is a process of (planning as) inference. Crucially, instead of inferring the hidden states that maximise the probability of sensory outcomes, agents must infer their most probable course of action (Friston, et al 2017), where actions can include everything from overt bodily 
7  
 7 
movements, such as saccades, to covert mental actions, such as the direction of attention. This scheme reverses the usual framework for modelling action selection, instead of searching for the action that will reach some preferred state, it starts by assuming that the agent will achieve their preferred state and then infers the most probable course of action to get there (c.f. Millidge et al., 2020). The most probable course of action is the policy that minimises the expected free energy of plausible policies.   Line two of eq. 2 shows the decomposition of expected free energy into its most intuitive component parts: risk, ambiguity, and novelty. As the derivation is somewhat involved we refer interested readers to the appendix of (Da Costa, Parr, et al., 2020).     ğºğœ‹= Î•q[lnğ‘(ğ‘ ,ğ‘¨|ğœ‹)âˆ’lnğ‘(ğ‘œ,ğ‘ ,ğ‘¨|ğœ‹)]                                                                     (2)             â‰ˆğ·ğ¾ğ¿[ğ‘(ğ‘œğœ|ğœ‹)||ğ‘(ğ‘œğœ|ğ¶)]âŸâŸâŸâŸâŸâŸâŸâŸâŸğ‘Ÿğ‘–ğ‘ ğ‘˜+Eğ‘(ğ‘ ğœâˆ£ğœ‹)[H[ğ‘(ğ‘œğœ|ğ‘ ğœ)]âŸâŸâŸâŸâŸâŸâŸâŸâŸğ‘ğ‘šğ‘ğ‘–ğ‘”ğ‘¢ğ‘–ğ‘¡ğ‘¦ âˆ’Eğ‘(ğ‘œğœâˆ£ğ‘ ğœ)ğ‘(ğ‘ ğœâˆ£ğœ‹)[ğ·ğ¾ğ¿[ğ‘(ğ‘¨|ğ‘œğœ,ğ‘ ğœ,ğœ‹)||ğ‘(ğ‘¨)]]âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦  
The first term, risk, is the KL divergence between the predictive posterior over observations conditioned on a specific policy and the agentâ€™s preferred observations (specified by ğ¶), including homeostatic set points encoded in the agents phenotype (for discussion see; Smith, Ramstead, et al., 2021; Smith et al., 2022). Smaller values therefore indicate greater similarity between the observations expected under a policy and those the agent would find most phenotypically characteristic or rewarding. The need to minimise this term promotes a preference for goal seeking behaviour. The second term, ambiguity, is the expected (conditional) entropy of the likelihood, that is, the mapping between hidden states and observations. To minimise the ambiguity term, agents will select policies that result in a precise mapping between states and observations (e.g., turning the light on in a dark room). The third and final term, novelty, is the expected KL divergence between the posterior distribution over the parameters of a model (conditioned on states, observations, and policies), and the marginal posterior over model parameters. Here, we show this for the parameters of the likelihood distribution, denoted by A (the concentration parameters of a Dirichlet prior over a categorical likelihood matrix described below), but similar terms can be included for other parameters. Here, novelty scores the shift in beliefs about the parameters of the generative model afforded by states and observations expected under each policy. Because novelty is a negative term, to minimise expected free energy, agents are driven to maximise the difference between the posterior conditioned on observations and states, and the marginal posterior, by seeking out novel observations that are expected to lead to the largest shift in posterior beliefs about model parameters. This illustrates the dual imperatives that underwrite active inference; namely, goal-seeking and information-seeking that are subsumed under a single objective function. Minimising expected free energy therefore requires agents to trade off between these imperatives; this means agents dissolve the exploration-exploitation dilemma by choosing policies that strike an optimal balance between minimising risk (i.e., maximising preferences), minimising ambiguity (i.e., maximising information gain about states), and seeking novel observations (i.e., maximising information gain about parameters); Schwartenbeck et al., 2019). For a more general formulation of expected 
8  
 8 
free energy as the marginal likelihood of states under the agentâ€™s control given previous actions and observations see (Da Costa et al., 2024).   In sum, agents equipped with a limited repertoire of automatic actions (e.g., simple organisms or automated subsystems of more complex organisms) can be modelled through the minimisation of variational free energy by having an organism/agent take actions that bring about observations consistent with prior beliefs (e.g., homeostatic set-points; Buckley et al., 2017; Corcoran et al., 2020). Variational free energy can therefore be used to model perceptual inference in both continuous and discrete generative models, and simple reflex-like actions in continuous models. However, as soon as action selection requires any element of counterfactual computation, we turn to expected free energy (Corcoran et al., 2020), which (normally) requires a discrete generative model (usually a Categorical-Dirichlet model; Koudahl et al., 2021).  3. Generative models, belief updating, and neural dynamics  The current state of the art in active inference depicts the brain as a hierarchical â€œmixed modelâ€ consisting of interacting generative models (Friston et al., 2017; Parr et al., 2021, 2022; Parr & Friston, 2018c). The generative model underlying low-level sensory inference takes the form of a predictive coding network that performs inferences about continuous quantities, such as motion and contrast. Continuous low-level sensory systems then interface (via link functions that map continuous quantities to discrete latent variables) with higher-level discrete generative models (e.g., partially observable Markov decision processes, POMDPs), which perform categorical sensory inferences and select discrete action sequences (i.e., policies), which are then translated back (again through link functions) into continuous motor commands (Parr & Friston, 2018c). For tutorial-style reviews on continuous models, see Bogacz (2017) and Buckley et al, (2017). For a detailed mathematical review of discrete models see Da Costa, Parr, et al, (2020), and for a more accessible tutorial-style review, see Smith et al, (2022). For a book length treatment of both continuous and discrete models see Parr et al., (2022).  To illustrate the general principles underlying the derivation of predictive coding under active inference, here we rehearse the derivation of a single-layer predictive coding network from variational free energy assuming a static generative model and fixed priors (Bogacz, 2017; Friston, 2005). The same principles apply to the derivation of dynamical models (Buckley et al., 2017), but requires considerably more formal machinery (e.g., generalised coordinates of motion).   To arrive at a tractable expression for variational free energy we make the following three assumptions: 1) that the generative model (i.e., likelihood and prior) and approximate posterior are Gaussian; 2) that variational free energy is well approximated by a second-order Taylor series expansion around the mean of the posterior (the Laplace approximation); and 3) that variance is stationary. In fact, these are three different ways of phrasing the same assumption. Under these assumptions, the expression for (negative) variational free energy reduces to the log of the generative model evaluated at the posterior mode.   
9  
 9 
   ğ¹=âˆ’ln(ğ‘(ğ‘œ|ğ‘ )ğ‘(ğ‘ ))=âˆ’ lnğ’©(ğ‘œ;ğ‘”(ğ‘ ),Î£ğ‘œ) âˆ’lnğ’©(ğ‘ ;ğ‘ ğ‘,Î£ğ‘)             (3) 
       =âˆ’ln[1âˆš2ğœ‹Î£ğ‘œexp(âˆ’(ğ‘œâˆ’ğ‘”(ğ‘ ))22Î£ğ‘œ)]          âˆ’ln[1âˆš2ğœ‹Î£ğ‘exp(âˆ’(ğ‘ âˆ’ğ‘ ğ‘)22Î£ğ‘)]                             = 12(ln(Î£ğ‘œ)+(ğ‘œâˆ’ğ‘”(ğ‘ ))2Î£ğ‘œ +ln(Î£ğ‘)+(ğ‘ âˆ’ğ‘ ğ‘)2Î£ğ‘)+ğ¶  Where ğ‘œ and ğ‘  denote the value of sensory observations (as before) and the a posteriori most likely hidden state, respectively, ğ‘”(ğ‘ ) is a (typically non-linear) function mapping the value of the hidden state to sensory observations, and Î£ğ‘œ and Î£ğ‘ denote the variance of the likelihood and prior. We next take the partial derivative of variational free energy with respect to ğ‘  and set up a gradient system ğ‘ Ì‡=âˆ’ğœ•ğ¹ğœ•ğ‘  equipping us with equations of motion for the value of the hidden state that moves downhill on variational free energy and in so doing approximates the true posterior over the hidden states.   ğ‘ Ì‡= ğ‘œâˆ’ğ‘”(ğ‘ )Î£ğ‘œğ‘”â€²(ğ‘ )+ğ‘ ğ‘âˆ’ğ‘ Î£ğ‘   (4) = ğœ€ğ‘œğ‘”â€²(ğ‘ )+ğœ€ğ‘                 Line two of eq. 4 rewrites hidden state dynamics (ğ‘ Ì‡)â€”i.e., the dynamics of beliefs about the most likely hidden statesâ€”in terms of a mixture of precision-weighted prediction errors encoding the difference between the hidden state observation mapping and the hidden state prior mapping. To obtain an expression that could plausibly be computed by a neural circuit we also need two additional differential equations with neuron-like dynamics that have stationary points when ğœ€ğ‘œ= ğ‘œâˆ’ğ‘”(ğ‘ )Î£ğ‘ and ğœ€ğ‘ =ğ‘ ğ‘âˆ’ğ‘ Î£ğ‘ leading to: ğœ€ğ‘ Ì‡= ğ‘ ğ‘ âˆ’ ğ‘ âˆ’Î£ğ‘ğœ€ğ‘  (5)      ğœ€ğ‘œÌ‡= ğ‘œâˆ’ğ‘”(ğ‘ )âˆ’Î£ğ‘œğœ€ğ‘œ    (6)  This provides us with a set of three ordinary differential equations describing the behaviour of three neuron-like nodes whose dynamics perform a gradient decent on variational free energy. This furnishes us with a simple model of neuronal dynamics that approximate the posterior distribution over hidden states. The mapping between these equations and the cortical microcircuit is a matter of ongoing research. Broadly speaking, however, the dynamics for hidden states (implemented in â€œexpectation nodesâ€) are typically associated with deep layers of cortex that project laterally to error nodes in superficial layers within the same level of the cortical hierarchy and backwards to error nodes in superficial layers of subordinate levels of the cortical hierarchy. Prediction error dynamics (implemented in â€œerror nodesâ€) are typically associated with superficial layers of cortex and project laterally to deep layers in the same level of the cortical hierarchy and forwards to deep layers higher in that hierarchy 
10  
 10 
(Fig 1a). The expectation node at each level serves as an observation for the level above and as a prior for the level below (for review and discussion, see Bastos et al., 2012; Hodson et al., 2023; Shipp, 2016; Walsh et al., 2020).   
 Fig 1. Belief updating architectures. a) Static predictive coding network, error nodes (light purple) receive excitatory feed-forward drive, and are modulated by estimates of the precision of the likelihood (i.e. the incoming evidence) and the prior. Error nodes send excitatory projections to expectations nodes (red) and receive (silencing) inhibitory feedback from the same units. b) Bayesian network representation of a single level, and c) hierarchical POMDP. Filled circles denote variables, open circles parameters, open squares functions, and arrows conditional dependence. In the hierarchical POMDP hidden states at the first level act as observations for the second level.  Moving on from continuous models, we turn next to discrete models. Unlike predictive coding networks, POMDPs (Fig 1b) operate in discrete time and assume discrete state and outcome spaces. Inference in these models is based upon a likelihood mapping ğ‘(ğ‘œğœ|ğ‘ ğœ) that quantifies the conditional probability of observations ğ‘œğœ given a set of hidden states ğ‘ ğœ, a transition probability ğ‘(ğ‘ ğœ|ğ‘ ğœâˆ’1,ğœ‹) describing how hidden states change over time dependent on the previous state and the agentâ€™s policy ğœ‹, and an initial state probability ğ‘(ğ‘ 1). Each of these mappings is encoded in a matrix. By convention the likelihood matrix is denoted by A, the (prior) transition matrix by B, and the initial (prior) state vector by D. Each hidden state distribution (hidden state factor) is assumed to be independent (i.e., the mean field approximation) and assigned to a distinct D vector and B matrix. Likewise, each outcome modality (i.e. variety of sensory input) is assigned a distinct likelihood encoded in the A matrix. Action is modelled by making a subset of the state transitions controllable by the agent (the agentâ€™s policy space), which is why the state transitions are conditionally dependent on both the previous state and the policy. To model an agentâ€™s preferences, the generative model is equipped with a prior over preferred observations in the future ğ‘(ğ‘œğœ|ğ¶), which quantifies the degree to which agents are averse to, or prefer, each observation. For each A matrix there is a C vector encoding the agentâ€™s preferences for each possible modality of observation.   As with predictive coding, the goal of perceptual inference is to infer an (approximate) posterior distribution over states given a set of observations by minimising variational free energy. Here, we use the time independent definition of (marginal) variational free energy introduced by Parr, Markovic, et al., (2019) and write the generative model in matrix form.  

11  
 11 
 ğ¹ğœ‹= ğ‘ ğœ‹,ğœâ‹…(lnğ‘ ğœ‹,ğœâˆ’12(lnğğœ‹,ğœâˆ’1ğ‘ ğœ‹,ğœâˆ’1+lnğğœ‹,ğœâ€ ğ‘ ğœ‹,ğœ+1)âˆ’lnğ€Tğ‘œğœ)         (7) 
Again, like predictive coding, the generative model is inverted by performing a gradient descent on variational free energy with respect to states. Dropping constants, eq. 8 gives the expression for the marginal free energy gradient.  âˆ‡ğ‘ ğœ‹,ğœFğœ‹=lnğ‘ ğœ‹,ğœ âˆ’12(lnğğœ‹,ğœâˆ’1ğ‘ ğœ‹,ğœâˆ’1+lnğğœ‹,ğœâ€ ğ‘ ğœ‹,ğœ+1)âˆ’lnğ€Tğ‘œğœ         (8) 
With an expression for the free energy gradient, we can now write down an algorithm that: 1) performs a gradient decent on variational free energy with respect to states, thereby inferring the value of the approximate posterior over states; and 2) furnishes us with a simple model of neuronal dynamics (for details, see Friston et al., 2017; Parr, Markovic, et al., 2019). To this end, we define a â€œdepolarisationâ€ variable ğ‘£ğœ‹,ğœ= lnğ‘ ğœ‹,ğœ (eq. 9) which represents the mean membrane potential of the neuronal population encoding the posterior distribution over states (like voltage, this variable ğ‘£ğœ‹,ğœ takes both positive and negative values when we leave the log probability unnormalised). We then use the free energy gradient âˆ‡ğ‘ ğœ‹,ğœğ¹ğœ‹ to define the dynamics of the neuronal population by setting up a difference equation that adds the free energy gradient âˆ’âˆ‡ğ‘ ğœ‹,ğœğ¹ğœ‹=ğœ€ğœ‹,ğœ (eq. 10) to the membrane potential ğ‘£ğœ‹,ğœ (eq. 11), moving the log posterior over states in the direction of steepest decent on variational free energy. Finally, for the membrane potential to be interpretable as a probability distribution, it needs to be normalised by passing it through a softmax (normalized exponential) function (eq. 12). We interpret this final step as giving the normalised firing rate of the underlying population. Pulling this all together (Fig 2a and eq.9 â€“ eq.12), we have a simple iterative algorithm for inferring the approximate posterior.  ğ‘£ğœ‹,ğœâ† lnğ‘ ğœ‹,ğœ (9) ğœ€ğœ‹,ğœâ†12(lnğğœ‹,ğœâˆ’1ğ‘ ğœ‹,ğœâˆ’1+lnğğœ‹,ğœâ€ ğ‘ ğœ‹,ğœ+1)+lnğ€Tğ‘œğœâˆ’ğ‘£ğœ‹,ğœ (10) 
ğ‘£ğœ‹,ğœâ† ğ‘£ğœ‹,ğœ+ğœ€ğœ‹,ğœ (11) ğ‘ ğœ‹,ğœâ† ğœ(ğ‘£ğœ‹,ğœ) (12) The use of the softmax function (which is a generalisation of the logistic function to vectors) to simulate average firing rate is based on the assumption made in mean-field models of large-scale brain dynamics that the average firing rate of a population can be treated as a sigmoid function of the average membrane potential (Breakspear, 2017; Da Costa et al., 2021; Hopfield, 1982; Wilson & Cowan, 1972). Event-related potentials (ERPs) in electroencephalography (EEG) research, and local field potentials in intracranial recording studies are both taken as the time derivative (i.e., rate of change) of the normalised firing rate. For a brief sketch of a possible neural implementation, see Fig 2b. For in depth discussion and review, see Parr and Friston, (2018a).   
12  
 12 
 Fig 2. Belief updating and neuronal dynamics in a POMDP. a) Message passing and belief updating equations underlying hidden state and policy inference in a POMDP. b) Potential neural implementation of the belief updating scheme in two idealised cortical columns that interface with a subcortical network to perform policy selection.   So far, we have derived two simple algorithms for minimising variational free energy in cases of continuous and discrete perceptual inference, respectively: each of which have straightforward interpretations in terms of neural dynamics. However, as outlined above for action selection, we have not, by definition, received (future) observations and must therefore minimise expected free energy (eq. 13) to select policies. We show expected free energy below in matrix form to highlight the link between the generative model components and the components of expected free energy.   ğºğœ‹,ğœâ‰ˆâˆ‘ââœâœâ›ğ€ğ‘ ğœ‹,ğœâ‹…(lnğ€ğ‘ ğœ‹,ğœâˆ’lnğ‚ğœ)âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸğ‘Ÿğ‘–ğ‘ ğ‘˜ âˆ’ğ‘‘ğ‘–ğ‘ğ‘”(ğ€Tlnğ€) â‹…ğ‘ ğœ‹,ğœâŸâŸâŸâŸâŸâŸâŸâŸâŸğ‘ğ‘šğ‘ğ‘–ğ‘”ğ‘¢ğ‘–ğ‘¡ğ‘¦ âˆ’ğ€ğ‘ ğœ‹,ğœâ‹…ğ–ğ‘ ğœ‹,ğœâŸâŸâŸâŸâŸâŸâŸğ‘›ğ‘œğ‘£ğ‘’ğ‘™ğ‘¡ğ‘¦â âŸâŸâğœ        (13)   Where ğ– âˆ¶=12(ğ’‚â¨€(âˆ’1)âˆ’ğ’‚ğ‘ ğ‘¢ğ‘šğ‘ â¨€(âˆ’1)) with â¨€ denoting the element-wise power, ğ’‚ is a matrix containing the concentration parameters of the likelihood, and ğ’‚ğ‘ ğ‘¢ğ‘šğ‘  is a matrix with the same dimensions as ğ’‚ but where each entry is the sum of the values of the associated column in ğ’‚. The matrix ğ’‚ is updated on each trial by accumulating the co-occurrence of states and observations in a neurobiologically plausible fashion (i.e., associative plasticity). One can understand the operation of the novelty term heuristically by noticing that when agents have extensive exposure to a set of observations, corresponding to large values in ğ’‚ and ğ’‚ğ‘ ğ‘¢ğ‘šğ‘ , the ğ– term is small as it holds the inverse values of ğ’‚ and ğ’‚ğ‘ ğ‘¢ğ‘šğ‘ , generating a small novelty value. In contrast, when agents have a limited exposure to a set of observations, the ğ– term is large. As the novelty term is negative, agents are driven to maximise novelty by seeking out state-observation pairs they have had limited exposure to. For a derivation, see Da Costa, Parr, et al, (2020) and for worked examples and discussion, see Smith et al, (2022).  

13  
 13 
The posterior over policies for one timestep policies is then simply a softmax function of expected free energy (eq. 14), which effectively translates expected free energy into a probability distribution, from which the next action can be sampled. ğ‘(ğœ‹)=ğœ(âˆ’âˆ‘ğºğœ‹,ğœğ‘‡1 ) (14)   Here, T denotes the future time horizon of the policy. The policy that minimizes (the path integral of) expected free energy, will have the highest posterior probability. Actions are then selected by sampling from the posterior over policies at each timestep.   4. Computational models of conscious phenomena  Having outlined the key quantities driving the perception-action loop, the structure of continuous (predictive coding-based) and discrete (POMDP-based) generative models, and the relationship between model dynamics and measures of neural dynamics (e.g. ERPs and firing rates), we turn now to a discussion of previous work that has employed active inference to model tasks paradigmatic to the science of consciousness. In general these studies have focused on conscious contents and conscious self. For brevity, we focus exclusively on numerical (i.e., simulation) studies at the expense of the many valuable qualitative and conceptual models relevant to the neuroscience of consciousness (e.g., Ciaunica et al., 2022; Safron, 2020; Seth & Tsakiris, 2018), and accounts of the metaphysics of consciousness implied by active inference (Friston, Wiese, et al., 2020; Ramstead et al., 2023). For relevant reviews of content not covered in this article, see (Nikolova et al., 2022; Ramstead et al., 2023; Rorot, 2021). In addition, we note that although the models we review focus on visual and interoceptive sensory modalities, this selection reflects the visual and interoceptive focus of the study of conscious contents and conscious self generally. It does not reflect limitations in the applicability of the active inference framework to other sensory modalities. Indeed, to illustrate the explanatory generality of the framework â€” and to highlight potential applications to conscious state at the end of this section â€” we extend an existing model of the conscious processing of auditory regularities to account for the disruption to this process in sleep and anaesthesia.  4.1 Models of conscious content  In the preceding section, we cast the perception-action loop as an iterative process of 1) inferring the approximate posterior that best minimises variational free energy (placing an upper bound on model evidence); and 2) sampling the world in a way that minimises expected free energy (thereby maximising model evidence). This motivates a perspective change on several key phenomena in consciousness science. It draws attention to the role of active sampling and anticipation in many perceptual phenomena that have commonly been thought of as largely passive.   The shift from passive to active perception is particularly prominent in models of bistable perception, which typically cast perceptual switches as noise and/or adaptation-driven oscillations (e.g. Moreno-Bote et al., 2007; Wilson, 2007), rather than states that are, at least to some extent, driven by an agentâ€™s actions. For example, Parr et al, (2019) proposed a model of two instances of Troxler fading and binocular 
14  
 14 
rivalry (Fig 3a), that casts changes in conscious contents in terms of policy driven changes in the precision of posterior beliefs that occur as a function of saccade policies (in Troxler fading) and attention policies (in binocular rivalry). Troxler fading is a phenomenon in which percepts associated with peripherally presented stimuli fade from awareness when participants are required to maintain central fixation. Binocular rivalry occurs when incongruous stimuli are presented to each eye: instead of experiencing a superposition of stimulus percepts, participants experience discrete alternations between singular percepts with only brief periods of mixing.   Under active inference, such perceptual alternations are explained by incorporating two prima facie mundane observations into the task-specific generative models. The first is that an agentâ€™s estimate of the precision of state transitions (i.e., B matrix precision) should never be entirely certain in a capricious world (cf. Hohwy et al., 2016). The second is that the precision of sensory input depends on the location of the fovea and/or the focus of attention (modelled by making the precision of the A matrix conditionally dependent on saccade or attentional states). The incorporation of these two simple assumptions into a generative model results in epistemic behaviour through the ambiguity term in expected free energy. In the absence of precise sensory input, uncertainty (predictive entropy) about the perceptual content of a location of visual space accumulates, increasing the epistemic value of policies (e.g. saccade or attention policies) that will solicit informative sensory input from that part of visual space. In the case of Troxler fading, the agent is forced to maintain central fixation, precluding access to precise sensory information about the periphery. Over time, this drives the posterior over states corresponding to peripheral locations in visual space toward a uniform distribution through iterative belief updates based on imprecise state transitions and sensory input. Assuming a correspondence between the posterior over states and the content of awareness, and associating perception with a mixture of high precision states this would give rise to the fading of stimuli from awareness (we discuss the related issue of distinguishing between conscious and unconscious states within active inference in section 5). Similarly, discrete switches between percepts typical of binocular rivalry emerge from a generative model when agents are restricted to select between (covert) attention policies that enhance the precision (i.e., the A matrix mapping) of the attended stimulus at the expense of the unattended stimulus (cf. the biased competition model of attention; Desimone, 1998). The agent receives precise information about the attended stimulus, resulting in a precise posterior over states for the attended stimulus and at the same time â€” denied precise information about the unattended stimulus â€” the posterior over states for the unattended stimulus dissipates towards a uniform distribution. Crucially, as uncertainty accumulates, the unattended stimulus becomes progressively more epistemically attractive via the ambiguity reduction term in expected free energy, driving an eventual attentional policy switch. Again, assuming a correspondence between the agentâ€™s posterior over states and conscious perceptual content, this results in attention-dependent perceptual switches.   
15  
 15 
 Figure 3. Models of conscious contents and self under active inference. a) Belief updating dynamics in a generative model of binocular rivalry. Upper rows show posterior beliefs about visual stimuli entering each eye and bottom row shows posterior beliefs about attentional state. Adapted from Parr, Corcoran et al, (2019). b) Simulation of Leveltâ€™s second (upper) and fourth (lower) laws under active inference. c) Comparison of empirical and simulated ERPs under report and no-report conditions. Adapted from Whyte et al, (2022). d) Macroscale connectivity scheme underlying the generative model of visual hemineglect neglect. e) Comparison of â€œhealthyâ€ and aberrant eye-movement dynamics resulting from simulated A matrix lesion. d - e Adapted from Parr & Friston, (2018b). f) Belief updating dynamics in a deep-parametric generative model of auditory processing adapted from Sandved-Smith et al, (2021).  This active formulation of binocular rivalry neatly explains several experimental findings that are difficult to accommodate in passive models of rivalry that do not have an explicit role for an agential process like policy selection. Specifically, the slowing of rivalry in the absence of attention (Paffen et al., 2006; Zhang et al., 2011), and the modulation of dominance durations by reward (for review see Safavi & Dayan, 2022) can both be understood as instances of expected free energy minimisation. If perceptual switches are driven by switches in attention policies, then the slowing of rivalry in the presence of a distractor task (Paffen et al., 2006) can be explained by a reduction in the precision of sensory input â€“ thus increasing the amount of time agents need to sample the input from each eye to reach a precise posterior over states. Similarly, the addition of a reward to one of the stimuli will minimise the risk term of expected free energy in an additive manner, explaining the biasing effect of reward on rivalry (Marx & Einhauser, 2015; Wilbertz & Sterzer, 2018). In addition, although not presented in the original paper by Parr et al., (2019), in appendix 1 and Fig 3b we show that the model readily explains Leveltâ€™s laws (Brascamp et al., 2015; Levelt, 1965) â€” a compact set of propositions that summarise the lawlike relationship between stimulus properties (e.g., luminance contrast) and perceptual dominance durations. Importantly, in addition to retrospectively explaining a large variety of existing 

16  
 16 
phenomena the model provides an empirical prediction about reward driven violations to Leveltâ€™s laws (appendix 2) which are readily testable in the domain of human psychophysics. For an extension of this modelling strategy to the Necker cube illusion â€” another often-used bistability paradigm in consciousness science â€” see Novicky et al, (2023). For related approaches based upon reinforcement learning see (Haas, 2021; Martin et al., 2021; Safavi & Dayan, 2022, 2024). The precise relationship between reinforcement learning based models and active inference models will depend on the choice of objective functions and inference algorithms (Chou et al., 2025; Malekzadeh & Plataniotis, 2024; Tschantz, Millidge, et al., 2020). In principle, the two approaches may be formally equivalent (Da Costa et al., 2024; Da Costa, Sajid, et al., 2020). As we highlighted in the introduction, it will always be possible to derive formally equivalent, or at least very similar, models in specific domains. What active inference uniquely offers is a principled formulation of the objective function used in policy selection (i.e. expected free energy) that casts epistemic and reward-based drives within a common, information theoretic, currency that is shared across models of different tasks. Furthermore, it offers a neuronal process theory that allows one to relate belief updating processes to neuronal dynamics and plasticity.   Turning from the minimisation of ambiguity in an established generative model to novelty maximisation and parameter learning, Parr & Friston (2018b) proposed that hemineglect, a neurological syndrome characterised by a patientâ€™s neglect (i.e., unawareness) of one side of visual space (typically the left, following right hemisphere damage), can be explained by deficits in the novelty component of expected free energy. Clinically, hemineglect is often assessed through saccade cancelation tasks that require patients to circle (i.e., cancel out) all stimuli presented on a piece of paper. Here, patients with neglect will be unaware of the one side of visual space and leave stimuli on the neglected side uncancelled. To model this, Parr and Friston simulated a saccade cancelation task designed to resemble the clinical test for hemineglect. Their model used a grid to represent possible saccade locations. At the beginning of each simulation, all locations were novel (i.e., low parameter certainty), driving agents to saccade to each location and in so doing accumulate counts in the Dirichlet prior over the A matrix for each location (reducing the novelty component of expected free energy for saccade policies to â€œcancelledâ€ locations of visual space). Lesioning the A matrix mapping between hidden states and visual outcomes by increasing the concentration parameters for the left side of space effectively modelled the disconnect between dorsal and ventral attention networks; removing any capacity for novelty-based saccade policies to the left side of space, thus emulating the empirical phenomenology of visual neglect (Fig 3E-D).  The motivation for increasing concentration parametersâ€”or increasing confidence in the likelihood mappingâ€”is that disconnection is a state of maximal confidence in the sense that no amount of data can change the synaptic efficacy. A key prediction of the model that is testable in healthy participants â€” who can update likelihood parameters via synaptic efficacy â€“ is that individuals who become familiar with a visual scene should evoke increasingly larger belief updates, manifesting in larger event related electrophysiological responses. The underlying neural system encoding parameter certainty is expected to include afferent (feedforward) connections from the ventral attentional network in parietal cortex (TJP) to the frontal eye field (FEF) component 
17  
 17 
of the dorsal attentional network. The aforementioned empirical prediction was subsequently tested in a healthy population by Parr, Mirza, et al, (2019) using a saccade cancellation task in combination with DCM and MEG. In line with model predictions, they found that when the scene was more predictable, the dorsal component of FEF actively disinhibited the (right) ventral TPJ, generating larger evoked responses. The disinhibition was, by construction of the canonical microcircuit network for DCM, most pronounced in deep layers of TPJ, which is known to receive descending input from FEF.   So far, we have discussed three general examples â€“ Troxler fading, binocular rivalry, and visual neglect â€“ where attention and saccade policies (arguably) play a defining role in determining the content of consciousness (For a related reinforcement learning based approach to attention see (Chalk et al., 2013). Content is determined via a process of selective sampling that generates high precision input about one location in the visual scene at the expense of other locations. However, the relationship between attention and changes in conscious content is far from one-to-one. Empirically, there is evidence that attended items can remain unconscious (Koch & Tsuchiya, 2007), and that certain stimuli can trigger corresponding perceptual experiences in the near absence of attention (Matthews et al., 2018). What then separates conscious from unconscious percepts under active inference? Plausibly, the answer comes from the hierarchical structure of the generative model underlying perceptual synthesis and the resulting separation of timescales in belief updating.  In an effort to link active inference to the broader literature on the neural correlates of consciousness, Whyte and Smith, (2021) developed a two-level POMDP model of visual awareness. This model cast conscious perception as relying on the bidirectional propagation of precise posteriors between different hierarchical levels in a generative model. Importantly, the second level of the model had sufficient temporal depth to generate goal-directed actions (such as subjective reports of the visual scene), which necessarily evolve over a longer timescale than stimulus presentation. Using this model, they simulated Dehaene et al, (2006)â€™s empirically derived taxonomy of the relationship between attention, stimulus strength, and conscious access. By manipulating the precision of the A matrix mapping between first-level hidden states and observations, representing the interplay between stimulus strength and attention, they replicated the nonlinear scaling of subjective reports and neural correlates. Consistent with empirical findings, the nonlinear increase in reported stimulus visibility was linked to a high firing rate and P3b-like ERP at the second level, resembling an "ignition" response in frontoparietal regions. Expanding on Dehaene et al, (2006)â€™s taxonomy, Whyte and Smith (2021) introduced expectation into the taxonomy, predicting that valid expectations would reduce P3b amplitude compared to neutral and invalid conditions, when attention is present and the stimulus well above threshold. This prediction was independently confirmed by Schlossmacher et al, (2020).  Initial work on the neural correlates of consciousness (including the work forming the backbone of Dehaeneâ€™s taxonomy modelled by Whyte and Smith (2021)), supported the idea that both PFC activity and late-ERPs (e.g., the P3b) robustly correlate with conscious perception, with studies across a variety of paradigms reporting similar 
18  
 18 
results (Bisenius et al., 2015; Sergent et al., 2005). However, the advent of no-report paradigms (Tsuchiya et al., 2015) challenged these findings. Under such conditions, PFC activity diminished or disappeared (Brascamp et al., 2015; Frassle et al., 2014), and late ERPs such as the P3b, once considered indices of conscious access, were no longer present (Cohen et al., 2020; Pitts et al., 2014; also see Fig 3C). Findings like these have led some researchers to reject PFC engagement as a necessary condition for conscious perception (Boly et al., 2017). Importantly, however, the evidential pendulum has begun to swing back in the other direction, with subsequent evidence from non-human primate electrophysiology showing that even in the absence of report, the contents of consciousness can be decoded from PFC (Kapoor et al., 2020), and that fluctuations in PFC activity precede perceptual switches (Dwarakanath et al., 2020), which is suggestive of a causal role. In an attempt to reconcile these findings, Whyte et al, (2022) modified their previous model of conscious access so that the working memory requirements of report were treated as a variety of mental action arrived at through policy selection. Available policies regarding what information to maintain in working memory controlled both the precision of the second-level A matrix that mapped first-level stimulus states to the second level of the model (i.e., corresponding to the gating of information from visual cortex into working memory), and the precision of the second-level B matrix (corresponding to the voluntary maintenance of items in working memory).   Whyte and colleagues used this modified model to simulate a visual masking task that had both report and no-report conditions. These simulations recapitulated the neural correlates of awareness under report conditions, wherein â€œconsciously perceivedâ€ stimuli were accompanied by high firing rates at the second level of the model and a large P3b-like ERP. Crucially, Whyte et al, (2022) were able to assess the visibility of the stimulus in the model in the absence of report by simulating the task across a large range of stimulus precisions (i.e., A matrix precisions), thus allowing them to construct a mapping between the posterior probability at the second level of the model and the corresponding report frequency. Simulating the same task in the no-report condition, when the model did not have to provide explicit reports of its own perceptual states (and therefore did not have to maintain items in working memory), they found that the model expressed lower second-level firing rates (i.e., reduced prefrontal activity), and did not generate a P3b-like ERP, because of the reduction in message passing precision (Fig 3C). Importantly, however, the posterior at the second level of the model was still well above the threshold for close to 100% reportability of the stimulus, reproducing the key finding that in the absence of report, conscious access is associated with reduced prefrontal activity and no late ERPs.   This model makes two key predictions. One, prefrontal activity and late ERPs should dissociate as a function of reporting instructions, since imposing a requirement for report means that agents must increase the precision of the messages being passed between sensory and prefrontal cortices (as well as within prefrontal cortices) in a goal-directed manner; thereby altering the neural correlates of conscious access. Indeed, this is exactly what was observed in a simultaneous EEG-fMRI experiment . Namely, when the stimulus was conscious, but not task relevant, there was strong activation of visual regions, and a large N170, but only weak prefrontal activation and no P3b. In contrast, when the stimulus was conscious, and task relevant, there was strong prefrontal 
19  
 19 
activation, and a large P3b. The second model prediction is that the feed-forward component of the bidirectional messages passed between prefrontal and visual cortex should contain precise information about the content of consciousness even in the absence of report, a prediction subsequently confirmed by Rowe et al, (2024).  4.2 Models of conscious self  We move now from models of exteroceptive awareness, to models of meta-awareness, interoception, and emotion, where agents infer and make policy decisions about their own internal cognitive and bodily states. As consciousness is inherently subjective, self-related processing is a key area of consciousness science. It is also an area of interest to clinicians, as disturbances to interoceptive inference and policy decisions are closely tied to psychiatric symptoms and phenomenology (i.e. aberrant inferences about self-efficacy in depression (Barrett et al., 2016; Ramstead, Wiese, et al., 2023) or disrupted policy selection in rumination (Hesp et al., 2020)). For conceptual clarity, we organize the discussion thematically rather than chronologically. We direct readers interested in related approaches to (Ainley et al., 2016; Allen, 2020; Barrett & Bar, 2009; Barrett & Simmons, 2015; Critchley & Garfinkel, 2017; Theriault et al., 2021)  Based upon initial work by Seth (2013) proposing a basis for emotional content in interoceptive inference â€” and later work by Stephan et al, (2016) linking disorders of homeostasis and allostasis to fatigue and depression â€” Tschantz et al. (2022) conducted a series of simulations interrogating the capacity of different generative model structures to account for homeostatic and allostatic processes (cf. Corcoran et al., 2020). Reactive autonomic responses to changes in bodily state were modelled by a predictive coding network, which were generalised to simple forms of anticipatory action by conditioning the homeostatic setpoint on the inferred exteroceptive state. Crucially, alterations in the precision of interoceptive prediction error affected sensitivity to interoceptive state changes, leading to failures in homeostatic adjustment. The balance between descending proprioceptive prediction errors and ascending interoceptive prediction errors dictated whether the agent adjusted its prior beliefs about physiological state or executed autonomic actions to align physiological states with set points. Favouring ascending sensory prediction errors over autonomic adjustmentsâ€”i.e., when the prior over the current set point is sufficiently imprecise compared to the anticipated interoceptive dataâ€”prompted agents to change their beliefs about the set points themselves. This failure of autonomic regulation may speak to paradoxical sensory dysfunction in conditions like autism (Gu & FitzGerald, 2014), where hypersensitivity to sensory input co-occurs with diminished autonomic responses and aberrant allostasis. Finally, employing a POMDP to interface with a predictive coding network allowed the model to account for goal-directed interoceptive control, where agents make decisions about actions by anticipating deviations between the preference for maintaining zero body temperature (the preferred homeostatic setpoint) and the anticipated body temperature.  Complementing the insights from this proof-of-principle modelling approach, recent studies fitting active inference models to empirical data have inferred disrupted interoceptive precision across various disorders, including depression, anxiety, eating disorders, and substance abuse disorders (Smith et al., 2020). Notably, in a heartbeat 
20  
 20 
tapping task, the best-fitting model (a simple hidden Markov model inverted through the minimization of variational free energy) exhibited a failure to adjust interoceptive precision in the face of a breath-hold perturbation in the patient sample, whereas the healthy controls successfully increased their interoceptive precision. This finding was recently replicated in a preregistered study with a large transdiagnostic sample (Lavalley et al., 2024) and in a cohort of healthy controls (Smith, Kuplicki, Teed, et al., 2020).   In an important study testing some basic predictions of the neural process theory associated with active inference, Smith et al, (2021) leveraged a novel gastrointestinal perception paradigm that had participants report the presence (or absence) of vibrations of varying magnitude in their stomachs delivered via an ingestible vibrating capsule whilst recording neural responses with simultaneous EEG. Inspection of the best-fitting model parameters (again a simple hidden Markov model) found evidence in support of the neural process theory. Specifically, despite not being fit to either reaction times or neuronal responses, increased interoceptive precision (A-matrix precision) was positively correlated with participant reaction times and the magnitude of evoked responses in sensory (parietal-occipital) electrodes.   Shifting to a more cognitive context, Allen et al, (2022) constructed a model of cardio-visual sensory integration that inferred the phase of its cardiac cycle (diastole vs. systole) when presented with arousing or non-arousing visual stimuli. Based on stimulus type, the model inferred its cardiac policy, controlling the state transitions between cardiac cycles. The hidden state of the cardiac cycle, in turn, controlled the precision of the visual A matrix. This minimal model reproduced several otherwise unrelated empirical findings. Arousing stimuli led to immediate cardiac acceleration (defensive startle reflex; Graham & Clifton, 1966) and a synthetic lesion to the A-matrix mapping, corresponding to a reduction in interoceptive precision, produced â€œpsychosomatic hallucinationsâ€ and increased false inferences or metacognitive biases (e.g., Allen et al., 2016; Hauser et al., 2017). In support of the assumption underlying the model â€” that agents infer their cardiac policy based upon the visual stimulus they are presented with â€” Corcoran et al., (2021) found that both tonic heart rate and (high frequency) heart rate variability decreased as a function of sensory ambiguity.  Formalising a body of theoretical and empirical work relating interoceptive, perceptual, and contextual inferences to emotion, Smith et al, (2019) showed that active inference agents could acquire a range of emotion concepts over the course of an â€œin silico childhoodâ€. In particular, each agent started with a flat A matrix mapping between hidden emotion states (â€œemotion conceptsâ€) and interoceptive observations, and over the course of hundreds of trials consisting of observed conjunctions of interoceptive observations of arousal, valence, and behaviour, and exteroceptive observations of context, the agent learned a mapping between hidden emotion states and interoceptive observations. Interestingly, they found that an impoverished â€œin silico childhoodâ€, in which the outcome statistics that the model was exposed to were biased to one particular emotion (e.g., sadness), led to reduced accuracy of the model in a subsequent emotional inference task, even after being exposed to other emotions. In a similar but more  empirically focused context, Smith, Lane, et al. (2019) used a hierarchical model to simulate inference in an emotional working memory task. The agent was required 
21  
 21 
to categorise, and then compare, two successive emotional states. Intriguingly, they showed that even in their relatively simple model there were at least seven distinct underlying neural mechanisms capable of producing the phenotype of reduced emotional awareness. For example, when the agent had a high prior expectation of uncharacteristic bodily states (as is the case in some forms of anxiety), it reliably miscategorised its own internal states of sadness and panic as sickness and heart attack, respectively.   In both of the models described above, valence was treated as an observation, not something that was itself inferred â€” a reasonable simplification given that the target of explanation was emotional inference (as opposed to affective inference). To complement this treatment, Hesp et al, (2021) introduced a model of affective inference proposing that valence was inferred (in part) from the mismatch between their prior and posterior over policies. The prior over policies was based on expected free energy alone (e.g. eq.14), while the posterior was based on expected free energy and a post-hoc belief based upon successive observations. The change from prior to posterior therefore reflects a prediction error indicating how consistent a new observation was with prior expectations under each policy. The direction of this error was then used as an observation in a higher-level model that inferred valence states. Here, lower-level observations supporting priors over policies promoted positive valence, while those inconsistent with priors promoted negative valence. This kind of (reward) prediction error or "affective charge" had been previously associated with phasic dopamine discharges (Friston et al., 2014; Schwartenbeck et al., 2015).   Simultaneously, this affective charge was used to iteratively update a precision parameter on expected free energy, for which valence states acted as priors. Here, negative valence states and unexpected observations both reduced this precision, decreasing the subsequent influence of expected free energy on policy selection. This decrease plays a few complementary roles. First, it promotes probability matching behaviour, which can be adaptive if one has low confidence in beliefs about policies or plans. Second, it permits a stronger influence of learned habits (usually denoted by E), if specified. Finally, it optimises the relative influence of expected free energy on policy selection. Namely, when expected model evidence is higher under the posterior over policies than under the prior over policies, affective charge (and valence) is positive, indicating that the agent has an increased confidence in their plans, increasing the precision of expected free energy relative to other terms. Conversely, when expected model evidence is higher under the prior over policies than under the posterior over policies, the valence down-weights expected free energy (i.e., it reduces the contribution of risk, ambiguity, and novelty to policy selection). Crucially, because the two-level model allowed valence states to act as prior beliefs about policy precision, the agent could contextualise the contribution of expected free energy to policy selection, which improved performance in a reversal learning task. Put another way, by giving the agent a rolling estimate of the degree of confidence they should have in their internal model of the mapping between actions and the observations they generate, agents could optimally weight the contribution of current observations (variational free energy) and expected future observations (expected free energy) to the selection of policies.   
22  
 22 
The form of deep parametric inference in Hesp et al.â€™s work was subsequently extended by Sandved-Smith et al, (2021) to model a form of meta-awareness underlying the cyclical phenomenology of focus â†’ distraction â†’ awareness of distraction â†’ focus, that is commonly studied in contemplative neuroscience. The key contribution of the model was to cast attention, the control of attention, and awareness of attentional control in terms of the hierarchical control of A matrix precision. A matrix precision at the first level of the model, mapping sensory observations to hidden states, was controlled by the attentional state of the agent at the second level, which consisted of two idealised states: â€˜focusedâ€™ and â€˜distractedâ€™. The agent had a preference (corresponding to a task instruction or goal state) for maintaining itself in a focused attention state that, through an imprecise second level B matrix, occasionally transitioned the agent into an unfocused state. Policy selection at the second level â€” mental action â€” allowed the agent to reorient their attentional state and transition back into a focused state, once the agent became aware of itself occupying the unfocused state: a key aspect of many forms of contemplative practice. Meta-awareness then consisted of a third level which, in turn, controlled the precision of the second-level awareness states.  Hidden states at this third level consisted of two states: high meta-awareness, which entailed a high-precision second-level A matrix, and low meta-awareness, which entailed a low-precision second-level A matrix (via a second-to-third level A matrix mapping). As a proof of principle, they simulated a simple auditory oddball task under differing third-level attentional states and showed that the agent spent longer in a distracted attentional state under low meta-awareness conditions than under high meta-awareness conditions. Indeed, in line with the finding that mind wandering increases under conditions of low perceptual demand (Lin et al., 2016), in the low meta-awareness state the agent only noticed the switch from focused to distracted when auditory oddballs occurred. In other words, oddballs induced larger prediction errors â€” and resulted in stronger ascending evidence â€” forcing large belief updates to the approximate posterior over hidden states across the hierarchy. Importantly, unlike the models discussed above, which aim to unify existing findings and generate novel predictions, the explanatory aim of this model is best thought of as providing conceptual clarity to areas of research, in early phases of development, where theory is largely verbally defined and has at best a loose connection to empirical data. This type of modelling is, therefore, best thought of as a kind of computational conceptual analysis that is a pre-requisite to modelling aimed at unifying existing results or providing empirical predictions.   4.3 Models of conscious state  Active inference models have so far not been applied to experiments that manipulate conscious state (but see Hobson and Friston (2012) for a theoretical review). Importantly, however, manipulations of conscious state are characterised by sensory and motor disconnects (Cirelli & Tononi, 2023) that are at least prima facie well suited to explanations cast in the language of active inference. For example, neuronal responses to stimulus pattern violations that occur over long time scales vary across sleep and anaesthetic states while pattern violations that occur over shorter time scales  are preserved (Boly et al., 2011; Dehaene et al., 2011).  To highlight the explanatory 
23  
 23 
generality of active inference, in this section, we present a simple extension of the hierarchical model of auditory regularity perception presented in Smith et al, (2022). We show how the targeted manipulation of precision reproduces the finding that the late P3b ERP component, typically associated with the detection of violations of â€œglobalâ€ (long-timescale) auditory patterns in wakefulness (Bekinschtein et al., 2009), is absent across NREM and REM sleep states, whilst ERP responses to â€œlocalâ€ violations are preserved (Strauss et al., 2015). We provide a brief overview of the model structure to contextualise the results, and refer interested readers to Smith et al, (2022) for a step-by-step description of the model.   The model consists of two hierarchical levels (Fig 4a); the first level tracked moment-to-moment changes in auditory tone and the second level tracked the pattern of tones at the first level, inferring the overall trial type (i.e., whether all stimuli had the same â€œstandardâ€ tone or whether there was an â€œoddballâ€). We allowed the model to accumulate concentration parameters in the second-level D vector over the course of 10 trials. On trial 10, the tone either conformed to â€” or violated â€” the expected trial type. In line with empirical findings, violations of â€œglobalâ€ patterns induced large P3b like ERPs at the second level of the model, and violations of â€œlocalâ€ within-trial expectations induced a mismatch negativity (MMN)-like ERP at the first level of the model. Based on the finding that NREM and REM sleep are low adrenergic states (Cirelli & Tononi, 2023), we leveraged the process theory associated with active inference to perform a targeted manipulation of the model designed to reproduce the absence of noradrenaline by reducing the precision of the second-level B matrix (by applying a softmax function with precision parameter ğ=0.9), which has been linked both theoretically (Parr & Friston, 2017a, 2018a), and empirically (Vincent et al., 2019), to adrenergic tone. In line with empirical findings (Strauss et al., 2015), the reduction of B matrix precision eliminated P3b-like ERPs at the second level of the model but left the MMN-like ERP at the first level intact (Fig 4b). As dynamics at the first level do not depend upon time-step by time-step dynamics, first level B matrix lesions have no effect on ERPs at either level of the model. 
 Figure 4. Active inference model of disrupted auditory processing across sleep states. a) Representation of the local-global paradigm encoded in the generative model of auditory regularity processing. The first level of the model infers the mapping between the tone observations and hidden states, while the second level of the model infers the longer-timescale pattern in states at the first level. The model learns the initial state probability encoded in the D vector across trials. Adapted from Smith et al, (2022). b) Simulated ERP responses to auditory pattern violations in â€œsleepâ€ and â€œwakeâ€ conditions. First-level ERPs are relatively unaffected by the reduction in second level B matrix precision 

24  
 24 
(chosen to represent the low-adrenergic conditions of both REM and NREM sleep), while second-level P3b-like ERPs are close to abolished.  As described, this model is simply an explanation of the absence of late ERPs in sleep states, and does not in-itself explain why participants lose consciousness in parts of sleep. Although, if we follow the line of argument put forward in the proceeding section â€” that conscious access requires a level of temporal depth that abstracts away from the moment-to-moment flux in sensory input (Friston, 2018; Friston et al., 2012; Whyte & Smith, 2021) â€” the model on offer here may also provide an algorithmic-level explanation for why NREM sleep is associated with fewer dream reports indicative of an absence of consciousness (we discuss what makes states conscious rather than unconscious under active inference in more detail in section 5), and why REM sleep is characterised by more frequent dream reports with bizarre and seemingly incoherent phenomenology (Hobson, 2009).   Specifically, NREM sleep is a low cholinergic state, which under active inference would result in a disconnection between the hierarchical levels of the generative model via a reduction in A matrix precision (i.e., the likelihood mappings between successive hierarchical levels. This would effectively take the temporally deep level â€˜offlineâ€™ and greatly minimise belief updating, plausibly corresponding to dramatic reduction or loss of consciousness in NREM sleep. REM sleep, by contrast, is a high cholinergic state  which would allow hierarchical message passing to occur between levels. Crucially, a model in a high cholinergic state â€” with imprecise sensory constraints â€” combined with low noradrenergic tone would show imprecise transitions between hidden states (via low B matrix precision), plausibly explaining the bizarre phenomenology of dreams in REM sleep. Interestingly, REM sleep behaviour disorderâ€”often thought of as a precursor to Parkinsonâ€™s diseaseâ€”manifests as acting out of such dreams, possibly reflecting a failure to balance precision across hierarchical levels, and particularly to suppress the precision associated with proprioceptive prediction errors during dreaming (Peever et al., 2014). Please see (Parr & Friston, 2017a) for a brief review of modulatory neurotransmitters and the precision of belief updating in the brain.  This minimal example neatly illustrates the typical strategy of modelling under active inference: start with a target phenomenon that is sufficiently well-defined to allow the construction of a generative model and, based on a combination of empirical and theoretical neurobiology, simulate a targeted manipulation of the model designed to mimic an experimental manipulation. Given the rich neurobiology of sleep and anaesthetics, this strategy can, as illustrated above, be straightforwardly applied to experimental manipulations of conscious state. The model presented here is, of course, just one example of a generative model of neural dynamics across conscious states. Ideally, future work will build and compare multiple competing models to empirical data.   5. Active inference as a minimal theory of consciousness  We started this paper by introducing active inference with the aspiration of building toward a theory of consciousness, using active inference models of different facets of consciousness as building blocks. With a clear idea of the framework and the type of 
25  
 25 
explanations offered by active inference models in place, we now address this aspiration. To make the structure of the theory and its relation to empirical data as clear as possible, we follow recent work in the philosophy of consciousness science (Negro, 2024) and leverage Lakatosâ€™ account of â€˜scientific research programsâ€™ (Lakatos, 1968), which is generally viewed among contemporary philosophers of science (Godfrey-Smith, 2003) as the successor to Popperâ€™s falsificationist program (Popper, 2008).   According to Lakatos (1968), research programs have two components: a hard core and a protective belt. The hard core is constituted by the set of fundamental concepts that form the foundation of the theory and need not themselves be directly testable. The protective belt, in contrast, is made up of the less central concepts and assumptions that generate testable predictions when combined with the hard core. Changes must only be made to the protective belt of a research program if it is to survive. Importantly, the changes that are made must be progressive. That is, the changes must make the theory more precise or expand its explanatory reach to increase its predictive power. If, however, changes to a theory only seek to explain away contradictory results and do not generate new predictions or contribute other explanatory virtues, then the research program is said to be â€˜degenerativeâ€™ and may eventually need to be abandoned. For example, the hard core of integrated information theory (Albantakis et al., 2023; Oizumi et al., 2014; Tononi et al., 2016) is constituted by the postulates that lead to the derivation of Phi as a measure of consciousness. The periphery is constituted by the bridging assumptions and approximations that make Phi computable. It would be a serious challenge to integrated information theory if Phi did not reliably co-vary with conscious state. Importantly, however, it would not be a deathblow to integrated information theory, as it may very well be that one of the assumptions underlying the derivation of a computable approximation to Phi is responsible for the failure, rather than it being a failure of the hard core of the theory. The health of the research program would then be decided by the ability of integrated information theory to alter the protective belt in a manner that generated additional predictions.   At the centre of the active inference framework is the imperative to minimise two objective functions, expected free energy and variational free energy. These two objective functions, and the imperative to extremise them, must therefore be included in the hard core of any theory that is based upon active inference. This entails that all behaviour, both conscious and unconscious, must, in some minimal sense, be aimed at minimising these quantities (cf. Hohwy, 2021). The neuronal and computational process by which this occurs is, at least currently, a part of the protective belt of the theory.   The next component we need to move towards a theory of consciousness, is a link between the quantities that enter into the objective functions and conscious experiences. The most minimal such link implicit in all the models reviewed above is between the contents of consciousness and the inferred hidden state (i.e., the approximate posterior over states ğ‘(ğ‘ )) of the world, body, and brain. It follows therefore that all changes in conscious contents must result from a change in the inferred state of the world, body, or brain; this would include changes from not having 
26  
 26 
conscious content to having conscious content, as in the model of conscious state, as well as exteroceptive and interoceptive contents, as in the models of perception and emotion and affect, and contents arising from the state of the brain itself, as in the case of meta-awareness. This is a minimal link because it only suggests that the kind of selective sampling characteristic of active inference is necessary, rather than sufficient, for a change in conscious content. The minimal link rules out that a change in consciousness could occur without a change in the posterior over states (but not that a change in posterior could occur without a change in conscious experience), and is consistent with non-conscious creatures exhibiting behaviour governed by active inference.   Finally, in order for active inference to be a theory of consciousness as such, and not just a theory of adaptive behaviour, we need a criterion to distinguish between conscious states and unconscious states (Doerig et al., 2021; Seth & Bayne, 2022). In the language of active inference, we need an account of what makes some posterior beliefs conscious and others unconscious. Here, again we look to the assumptions made in computational models of conscious content derived under active inference. In all the models reviewed above, the posterior beliefs that best correspond to the contents of consciousness are those that drive policy selection, specifically those that drive the selection of subjective report policies. Minimally, the computations underlying subjective reports require a discrete generative model capable of performing inference over mutually exclusive, distinct, states of the world. These discrete alternative states can also generate trajectories through a continuous space within mixed models (Friston et al., 2017). Continuous generative models, lacking this counterfactual (discrete) element, can drive reflex-like actions, but the types of computations underlying subjective reports (e.g. confidence wagering, or judgements of presence vs. absence) are explicitly counterfactual. As such, they require a discrete generative model capable of considering one or more mutually exclusive alternatives. Importantly, deep levels of discrete generative models evolve at too slow a timescale to match perceptual phenomenology (Whyte & Smith, 2021). We posit, therefore, that the conscious/unconscious distinction is tied to the mechanism(s) of conscious access, and that it occurs at the discrete-continuous interface where continuous posterior beliefs driven by moment-by-moment changes in the sensorium are transformed into a discrete format that can drive action selection. For a posterior belief to become conscious, it must, therefore occur at a timescale that is sufficiently slow so as to abstract away from the immediate sensory flux making it available to inform counterfactual policy selection, and also be suitably precise to reliably distinguish between different counterfactual states of the world. This sits well with previous philosophical on work active inference arguing that the computational role of conscious access in a hierarchical generative model is to force an agent to transiently settle on a particular posterior belief about the hidden state of the world in order to drive action selection (Hohwy, 2013; Marchi & Hohwy, 2022; Whyte, 2019). The precision requirement can be seen as a computational analogue of the observation that conscious experience is composed of differentiated, often mutually exclusive, perceptual objects (Canales-Johnson et al., 2017; Oizumi et al., 2014; Seth, 2021; Seth, 2014; Tononi, 1998). Crucially, this also implies that there is no hard threshold at which point a posterior belief will become available for report. Rather, the threshold for conscious access will vary with task demands and the type of decision an agent is required to perform. In 
27  
 27 
addition, we wish to be clear that we are not claiming that being the target of a counterfactual policy selection process is what makes a posterior belief conscious. Rather it is the capacity (in a counterfactual sense) for such a posterior belief to inform policy selection processes at the right timescale and with sufficient precision that makes it conscious (c.f. dispositional theories of consciousness; Carruthers, 2003; Prinz, 2012). We regard this as a necessary and sufficient condition for content to become conscious in humans (and non-human animals with similar neuronal architectures) but we remain agnostic as to whether this condition is both necessary and sufficient in systems vastly different to ourselves. The specifics of the decision criterion that best explains subjective reports is a matter of ongoing empirical debate and is best thought of as a part of the protective belt of the theory (for a possible implementation see Whyte et al., 2022). For a discussion of the relationship between phenomenology and the depth of counterfactual policy selection see (Seth, 2014).  Importantly, the minimal links between conscious processes and active inference contained in the hard core of the theory should be thought of as foundational building blocks in the discovery and construction of a more comprehensive theory of consciousness currently implicit in the periphery of active inference. The first two components of the theory (the imperative to minimise variational free energy and expected free energy, and the link between conscious perception and the posterior over states) are in some sense logically entailed by the formal machinery of the active inference framework. The third component of the hard core (that conscious access occurs at the discrete-continuous interface) is, in contrast, an inference to the best explanation based upon the types of generative model needed to model paradigmatic experimental paradigms used to study consciousness. In other words, although there are hints in the fundamental setup of active inference, a complete theory of consciousness needs more than the principles of optimising an internal model through action and perception. It must specify the properties of the model itself to be scientifically useful. The process of discovery involves incorporating, testing, and discarding models (including those reviewed above) and including further refinements to existing models (such as the modifications made to the model of binocular rivalry and hierarchical auditory processing; see appendix 1-2). Inspection of these models then informs a richer theory, suggesting which properties are pivotal in explanations of conscious content, self, and state.   The approach we advocate here is first and foremost one of open-ended theory-building and discovery, where the theoretical construct is continually expanded and finessed, as long as it remains a progressive research program, and stays in the race for model evidence against other theories of consciousness (see Corcoran et al., 2023). Key future developments will include modelling further conscious phenomena in more detail, including the application of structurally similar models to unify diverse conscious and unconscious phenomena. This is exemplified by the model of rivalry and Troxler fading (Parr, Corcoran, et al., 2019); and by the hierarchical expansion of the model of attention-driven perception used to accommodate no-report paradigms (Whyte et al., 2022). Further work should also investigate the performance of models that combine elements from models of conscious content, self, and state. Additionally, it will be important to refine conceptual models that target distinctive kinds of phenomenology (e.g., emotion and affect, objecthood, meta cognition; Barrett et al., 2016; Nikolova et 
28  
 28 
al., 2022; Pezzulo et al., 2018; Seth, 2014; Seth et al., 2012; Seth & Friston, 2016) to allow greater computational specificity and the ability to generate testable predictions.   The protective belt of the theory is constituted by the formalisms that allow specific models of conscious phenomena to be written down and simulated, and the assumptions that map the results of the simulations to empirically derived behavioural and neural data. This includes the structure of the generative model, the process theory that maps the generative model and the dynamics of state and policy inference to neural data, and the assumptions that link the quantities arrived at through simulation to reported phenomenology (e.g., does the content of perception correspond to the mode of the posterior and/or does the precision of the distribution contribute to sensory phenomenology?).   This current framing of the theory is deliberately minimal, leaving room for substantive disagreement between active inference theorists about issues that are empirically or theoretically underdetermined. It also acknowledges that the vast majority of explanatory work will be done by the protective belt of the theory and, as such, is largely open for productive revision and refinement (given the repertoire of possible generative model structures as well as the flexibility of the process theory mapping between simulated and empirical data). At the level of individual generative models, it is almost always possible to formulate a large set of candidate models that can explain the relevant empirical data to a greater or lesser degree, and in extreme cases may actually generate mutually contradictory predictions. This is a feature, not a bug, and is complemented by the fact that active inference as a modelling framework has been developed alongside a suite of variational (Bayesian) methods for model fitting and model comparison (Daunizeau, 2011; Friston et al., 2007; Zeidman et al., 2022). Such methods make it possible to adjudicate between families of competing generative models, each of which embody a hypothesis about the processes generating observed empirical data. The development and deployment of individual generative models â€” and the development of active inference as a theory of consciousness â€” therefore go hand in hand, leaving room for the theory to both contribute to, and evolve alongside, empirical debates.   For example, as we alluded to above, the variety of decision criteria underlying conscious access to perceptual states is a matter of ongoing discussion, with recent evidence pointing to a potential metacognitive mechanism involving abstract inferences about the presence or absence of a stimulus independent of the structure of the perceptual space itself (Dijkstra et al., 2023). This mechanism is contrary to a previously proposed active inference architecture (Whyte et al., 2022), which modelled conscious access through a policy space that is explicitly linked to the structure of the agentâ€™s perceptual space. Current evidence is insufficient to disambiguate between the two hypotheses in a more general setting, but if conscious access does indeed turn out to rest on a kind of metacognitive decision independent of the structure of a perceptual state space, then this would pose a significant (possibly insurmountable) challenge to this specific active inference model. Crucially, however, this component of the model is idiosyncratic and is not an intrinsic feature of the structure of generative models under active inference, making it possible to build future models that respect novel findings (and in so doing, productively extend the explanatory reach of active inference 
29  
 29 
into the metacognitive domain). We regard this flexibility as an explanatory virtue of the theory rather than a deficit, and one that is entirely appropriate given the early stage of the science of consciousness and the ongoing revision of empirical findings as methods and data quality improve.  These considerations leave open the worry that active inference may be too flexible to be productive as a theory of consciousness (rather than just a useful modelling framework). Or, put another way, the link between the hard core of the theory and its protective belt leaves too many degrees of freedom to be able to lead toward a recognisable theory of consciousness in its own right. There are, however, two key considerations which assuage this worry. First, the aforementioned example of a potential revision to an active inference model of conscious access involves revising an assumption about an agentâ€™s policy space in only one model, and thus does not represent a substantive blow to the protective belt of the theory. Crucially this is not true of all assumptions. The fact that distinct models share objective functions which are composed of a small number of semantically interpretable quantities (i.e., accuracy, complexity, risk, ambiguity, novelty) limits the number of parameters that can plausibly contribute to specific behaviours. For example, in modelling selective attention, the salience of a stimulus has been attributed to the precision of the A matrix â€“ which will exert an influence over behaviour through the epistemic (ambiguity minimising) component of expected free energy. This assumption is shared by almost all active inference models that have a role for attention and visual search (Allen et al., 2019; Holmes et al., 2021; Mirza et al., 2018, 2018, 2019; Parr et al., 2021; Parr & Friston, 2017a, 2017b, 2018b; Whyte et al., 2022; Whyte & Smith, 2021). Finding an example of an attentional effect that cannot be modelled in this way would therefore represent a very substantial blow to the active inference theory of consciousness, as the effect of the revision would impact the vast majority of existing models.   The second important consideration is that there are, in fact, both quantitative and qualitative predictions generated by the theory. Although not derived directly from the hard core, these predictions are close enough that if they were found to be false, they would require the revision of bridging assumptions made in the vast majority modelling studies â€“ even those that may appear to have nothing to do with consciousness science. A revision of this kind would put the theory dangerously close to being a degenerate research program (Godfrey-Smith, 2003). The existence of an explore-exploit trade off in active vision is one quantitative example of such a prediction. We give an explicit example of such a prediction in the context of binocular rivalry in (appendix 2). If it were discovered that the reward associated with a stimulus did not trade off against the epistemic content of a competing stimulus (e.g., luminance contrast) in the selection of attention policies then active inference would be seriously challenged, as the trade off between risk and ambiguity in expected free energy is a fundamental component of the hard core of the theory. Of course, this prediction would be difficult to test outright, since there are individual differences between participants in learning rates, pessimistic priors, and susceptibility to reward association, among other things. Also, the extent to which attention plays a role in early visual processing is still uncertain, meaning that, in practice, it will often be necessary to make an inference to the best explanation across a wide variety of experiments. If, however, it was robustly shown that the best fitting models across participants and experimental 
30  
 30 
paradigms (e.g., binocular rivalry and Troxler fading) did not respect the expected trade off between risk and ambiguity when selecting attention policies, then the active inference theory of consciousness would become less plausible. Hence, there is a sense in which the active inference modelling framework â€”and the model fitting and comparison tools that accompany it â€“ hold the key to serious testing of active inference as a theory of consciousness. This is a virtuous position to be in. The tools for model fitting and comparison in no way assume the truth of the theory used to derive the models being fit. The best fitting model in a specific context could very well be a model that violates core tenets of the active inference theory of consciousness.  We can also derive a second qualitative prediction from the theory. Specifically, we noted above a core commitment of active inference as a theory of consciousness is that if there is any change in conscious contents, then there must be a change in the inferred state of the body, brain, or world. A corollary of this commitment is that, in the absence of sensory (A matrix) precision, posterior beliefs about the state of the world should not be moved from their prior trajectories (e.g., our ability to silently count in our heads implies we can change our posterior beliefs moment to moment based upon priors about the transitions from one number to the next, but we would not expect to deviate from this numerical trajectory in the absence of precise sensory data), and therefore, conscious contents should not change. A handy slogan for the theory is therefore that â€œTo See is to Lookâ€ (and likewise for other sensory modalities, such as â€œTo Hear is to Listenâ€, â€œTo Feel is to Touchâ€ etc.). That is, to consciously perceive is to employ precision assignments characteristic of some variety of counterfactual policy selection process, across overt and covert actions. The complete assignment of zero precision to a sensory modality is likely too much of an idealisation to be empirically tractable; however, a related prediction that is nonetheless related to the core of the theory is that a reduction in precision, will attenuate belief updating; manifesting as delayed changes in conscious contents. If we associate the endogenous assignment of precision to covert and overt attention policies (Hohwy, 2012), which is an assumption that is made throughout active inference models of active vision, then a stimulus that has been assigned high precision in the selection of an attention/saccade policy will enter the contents of consciousness sooner than a retinally-matched stimulus that is not the target of a saccade. This prediction is also unique to active inference as a theory of consciousness and stands in contrast to other potentially more targeted theories of consciousness such as global workspace theory or integrated information theory, which have no explicit role for action, whereas under active inference the allocation of precision comes through the explicit selection of attention or saccade policies. This kind of empirical prediction is indeed part of an adversarial collaboration testing integrated information theory against both passive predictive processing theories of conscious content and active inference (INTREPID CONSORTIUM, 2021; project number TWCF0646).  Finally, it is important to note that the success or failure of the theory does not depend entirely on the success of these two close-to-core predictions. It would be possible for these predictions to be validated and active inference to still fail as a theory of consciousness. Indeed, there are a variety of less dramatic â€” and, we suspect, more likely â€” ways the theory could fail. For instance, it would be indicative of a degenerative research program if active inference models of key phenomena were overly 
31  
 31 
complex (in the sense that they overfit to idiosyncratic experimental results and do not generalise), or if models of conscious and non-conscious perception look indistinguishable, or if models of different conscious phenomena fail to display consistent commonalities and differences, such that the theory would offer little in terms of a unifying or distinctive explanation. In turn, if the theory should turn out successful, we would expect to eventually be able to reproduce all the key behavioural and neural correlates of consciousness with a relatively small set of models that vary along a small number of dimensions reflecting differences and similarities in reported phenomenology and neural correlates. This also implies a direction of explanatory travel in the nature of the explanation. Neural and behavioural variables are mapped to phenomenology (and vice versa) via the explanatory tools of active inference.   Conclusion   Here, we have argued that active inference, precisely because it is not a theory of consciousness, is uniquely placed among current theoretical approaches to consciousness to do justice to the richness and diversity of phenomena studied under the banner of consciousness science. Crucially, we have argued that active inference is not simply a framework for modelling consciousness. The assumptions implicit in the interpretation of these models â€” in the context of consciousness science â€” entail a set of empirical predictions that, if found to be false, would require the revision of the majority of models constructed under active inference. The core of the theory is constituted by an imperative to minimise variational free energy in perception and expected free energy in policy selection, along with the conjecture that the contents of consciousness, including exteroceptive and interoceptive experiences, as well as the state of the brain itself, must â€” in some way â€” correspond to the inferred states of the world, at the interface between continuous sensory computations and discrete counterfactual policy selection processes.   The scientific study of consciousness is, ultimately, an empirical science. If active inference is to furnish us with a useful theory of consciousness, then the translation of hard-won theoretical insights into empirical insights must be a priority. At present, there are clear bottlenecks in the translation of abstract models aimed at conceptual clarification into models that provide qualitative explanations of empirical data. And, in turn, translating these more complex theoretical models aimed at qualitative explanation and prediction into minimal models that can be quantitatively compared to data. The majority of explanatory work is still to be done. Nevertheless, we have argued here that the contours of a theory are already beginning to emerge. We are optimistic, therefore, that active inference, as a theory of consciousness, has the resources to eventually provide a minimal set of models that explain and unify the study of conscious contents, self, and state.  Acknowledgements   The authors thank Mac Shine and members of the Monash M3CS active inference reading group for providing invaluable feedback during the writing of this manuscript. CW is supported by a University of Sydney DVCR strategic postgraduate research scholarship. AWC, JR, and JH are supported by the Three Springs Foundation, and 
32  
 32 
Templeton World Charity Foundation. TP is supported by a NIHR Academic Clinical Fellowship (ref: ACF-2023-13-013). KF is supported by funding for the Wellcome Centre for Human Neuroimaging [203147/Z/16/Z]. AS is supported by the European Research Council (ERC Advanced Investigator Grant 101019254).   Code and data availability The novel computational models reported here were implemented with standard model inversion routines (spm_MDP_VB_X.m) available as MATLAB code in the latest version of SPM academic software: http://www.fil.ion.ucl.ac.uk/spm/. Complete code necessary to reproduce the simulations reported in this paper will be made available upon full publication.    Appendix 1: Leveltâ€™s laws under active inference  To show the compatibility of the model of binocular rivalry proposed by Parr, Corcoran et al, (2019) with Leveltâ€™s laws, we re-implemented the original model. Briefly, the model had three hidden state factors encoding the identity of the stimulus presented to each eye (with states {left, blank} and {right, blank}) and the locus of attention (with states {left, right}). In other words, one can attend to features consistent with the right stimulusâ€”or to data from the right eyeâ€”or to those from the left. The model had two outcome modalities (A matrices), one for each eye hidden state factor, which mapped the conditional probability of the observations associated with the stimulus entering each eye to the respective hidden states. The precision of the A matrix mapping was conditionally dependent on the attention hidden state factor. When attention was absent the A matrices were uniform (e.g. with columns given by [0.50.5]ğ‘‡), and when attention was present the mapping was comparatively precise ([0.70.3]ğ‘‡). This renders the hidden states conditionally independent of unattended observation modalities. The B matrices for the hidden state factors associated with each eye were initialised as identity matrices and then passed through a softmax function ğ‘ (ğ‘¥ğ‘–;Ï‰)=ğ‘’ğğ‘¥ğ‘–/âˆ‘ğ‘’ğğ‘¥ğ‘—ğ¾ğ‘—  with precision parameter ğ=0.8 introducing sufficient uncertainty into the agentâ€™s transition beliefs to guarantee that in the absence of precise input (i.e. in the absence of attention) uncertainty about hidden states would accumulate over time and the non-attended stimulus would become increasingly epistemically attractive. The B matrix for the attention hidden state factor was conditionally dependent on the agentâ€™s attention policy {left, right} placing attention transitions under the agentâ€™s control. The C vector was uniform for both outcome modalities.  To simulate the experimental conditions described by Leveltâ€™s propositions we systematically reduced the A matrix precision for one (Leveltâ€™s second proposition) or both (Leveltâ€™s fourth proposition) of the A matrix mappings. Leveltâ€™s second proposition states that reducing the â€œstrengthâ€ (e.g. contrast) of the stimulus entering one eye whilst leaving the strength of the stimulus entering the other eye unaltered should sharply increase of the dominance duration of the percept associated with the unaltered stimulus and lead to a comparatively minor reduction in the dominance 
33  
 33 
duration of the percept associated with the reduced strength stimulus. Consistent with this, reducing the precision of one of the A matrices â€” whilst leaving the other intact â€” resulted in a sharp increase in the amount of time the stimulus entering the more precise eye was sampled (i.e. attended) and a gradual reduction in the amount of time the stimulus associated with the less precise eye was sampled (Fig 3b upper panel). This asymmetry is due to the relative decrease in epistemic value of the less precise stimulus. Leveltâ€™s fourth proposition states that increasing the â€œstrengthâ€ of the stimulus entering both eyes will result in a reduction in the dominance duration of both stimulus percepts. Consistent with Leveltâ€™s fourth proposition, reducing the precision of both A matrices increased the amount of time the agent spent sampling the stimulus before switching (Fig 3b lower panel). The symmetric reduction in A matrix precision drove the agent to spend more time sampling each stimulus to reduce its uncertainty leading to longer dominance durations. Put another way, under reduced A matrix precision each stimulus retains its epistemic (i.e. ambiguity reducing) value for longer.   Appendix 2: Binocular rivalry and the exploration-exploitation dilemma   Based on the trade off between exploration and exploitation (i.e. ambiguity and risk) negotiated by agents minimising expected free energy we sought to generate a novel experimental prediction by extending the simulations of Leveltâ€™s propositions described in appendix 1 to include reward. Specifically, we simulated the conditions described by Leveltâ€™s second proposition but in comparison to the first set of simulations which assumed a uniform C vector we associated the reduced precision stimulus (e.g. the stimulus with reduced contrast in experimental settings) with reward. At each time step of the simulation the observation associated with the â€œrightâ€ state of the right eye outcome modality was given a value of ğ’„ğŸ=.25 leading to an initial bias in the dominance duration for the rewarded stimulus percept when the A matrix precision was matched between eyes (i.e. when the epistemic value of each stimulus was matched but risk was lower for the rewarded eye). Importantly, because risk and ambiguity trade off against each other this bias could be compensated for by reducing the precision of the rewarded eye reducing its epistemic (ambiguity reducing) value (Fig A.2). We therefore predict that; 1) with matched stimulus strength (contrast) rivalry will be biased to the percept associated with the rewarded stimulus; and 2) that this bias will be compensated for by reducing the epistemic value of the rewarded stimulus. Together, this results in a predicted violation of Leveltâ€™s second proposition under reward.  
 Figure A2. a) Simulation of Leveltâ€™s second proposition with uniform C vector. b) Simulation of Leveltâ€™s second proposition showing an initial bias to the rewarded stimulus percept (dashed line; ğ’„ğŸ=.25) that is compensated for by reducing the precision of the rewarded stimulus. 

34  
 34 
References   Ainley, V., Apps, M. A. J., Fotopoulou, A., & Tsakiris, M. (2016). â€˜Bodily precisionâ€™: A predictive coding account of individual differences in interoceptive accuracy. Philosophical Transactions of the Royal Society B: Biological Sciences, 371(1708), 20160003. https://doi.org/10.1098/rstb.2016.0003 Albantakis, L., Barbosa, L., Findlay, G., Grasso, M., Haun, A. M., Marshall, W., Mayner, W. G. P., Zaeemzadeh, A., Boly, M., Juel, B. E., Sasai, S., Fujii, K., David, I., Hendren, J., Lang, J. P., & Tononi, G. (2023). Integrated information theory (IIT) 4.0: Formulating the properties of phenomenal existence in physical terms. PLOS Computational Biology, 19(10), e1011465. https://doi.org/10.1371/journal.pcbi.1011465 Allen, M. (2020). Unravelling the Neurobiology of Interoceptive Inference. Trends in Cognitive Sciences, 24(4), 265â€“266. https://doi.org/10.1016/j.tics.2020.02.002 Allen, M., Frank, D., Schwarzkopf, D. S., Fardo, F., Winston, J. S., Hauser, T. U., & Rees, G. (2016). Unexpected arousal modulates the influence of sensory noise on confidence. eLife, 5, e18103. https://doi.org/10.7554/eLife.18103 Allen, M., Levy, A., Parr, T., & Friston, K. J. (2019). In the Bodyâ€™s Eye: The Computational Anatomy of Interoceptive Inference [Preprint]. Neuroscience. https://doi.org/10.1101/603928 Allen, M., Levy, A., Parr, T., & Friston, K. J. (2022). In the Bodyâ€™s Eye: The computational anatomy of interoceptive inference. PLOS Computational Biology, 18(9), e1010490. https://doi.org/10.1371/journal.pcbi.1010490 
35  
 35 
Atkinson, A. P., Thomas, M. S. C., & Cleeremans, A. (2000). Consciousness: Mapping the theoretical landscape. Trends in Cognitive Sciences, 4(10), 372â€“382. https://doi.org/10.1016/S1364-6613(00)01533-3 Baars, B. J. (2002). The conscious access hypothesis: Origins and recent evidence. Trends in Cognitive Sciences, 6(1), 47â€“52. https://doi.org/10.1016/S1364-6613(00)01819-2 Baars, B. J. (2005). Global workspace theory of consciousness: Toward a cognitive neuroscience of human experience. In Progress in Brain Research (Vol. 150, pp. 45â€“53). Elsevier. https://doi.org/10.1016/S0079-6123(05)50004-9 Baars, B. J., Franklin, S., & Ramsoy, T. Z. (2013). Global Workspace Dynamics: Cortical â€œBinding and Propagationâ€ Enables Conscious Contents. Frontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00200 Barrett, L. F., & Bar, M. (2009). See it with feeling: Affective predictions during object perception. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1521), 1325â€“1334. https://doi.org/10.1098/rstb.2008.0312 Barrett, L. F., Quigley, K. S., & Hamilton, P. (2016). An active inference theory of allostasis and interoception in depression. Philosophical Transactions of the Royal Society B: Biological Sciences, 371(1708), 20160011. https://doi.org/10.1098/rstb.2016.0011 Barrett, L. F., & Simmons, W. K. (2015). Interoceptive predictions in the brain. Nature Reviews Neuroscience, 16(7), 419â€“429. https://doi.org/10.1038/nrn3950 
36  
 36 
Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical Microcircuits for Predictive Coding. Neuron, 76(4), 695â€“711. https://doi.org/10.1016/j.neuron.2012.10.038 Bayne, T., & Carter, O. (2018). Dimensions of consciousness and the psychedelic state. Neuroscience of Consciousness, 2018(1). https://doi.org/10.1093/nc/niy008 Bayne, T., Hohwy, J., & Owen, A. M. (2016). Are There Levels of Consciousness? Trends in Cognitive Sciences, 20(6), 405â€“413. https://doi.org/10.1016/j.tics.2016.03.009 Beal, M. J. (2003). Variational algorithms for approximate Bayesian inference [Ph.D., University of London, University College London (United Kingdom)]. https://www.proquest.com/docview/1775215626/abstract/168FB6386C76493EPQ/1 Bekinschtein, T. A., Dehaene, S., Rohaut, B., Tadel, F., Cohen, L., & Naccache, L. (2009). Neural signature of the conscious processing of auditory regularities. Proceedings of the National Academy of Sciences, 106(5), 1672â€“1677. https://doi.org/10.1073/pnas.0809667106 Bisenius, S., Trapp, S., Neumann, J., & Schroeter, M. L. (2015). Identifying neural correlates of visual consciousness with ALE meta-analyses. NeuroImage, 122, 177â€“187. https://doi.org/10.1016/j.neuroimage.2015.07.070 Bogacz, R. (2017). A tutorial on the free-energy framework for modelling perception and learning. Journal of Mathematical Psychology, 76, 198â€“211. https://doi.org/10.1016/j.jmp.2015.11.003 
37  
 37 
Boly, M., Garrido, M. I., Gosseries, O., Bruno, M.-A., Boveroux, P., Schnakers, C., Massimini, M., Litvak, V., Laureys, S., & Friston, K. (2011). Preserved Feedforward But Impaired Top-Down Processes in the Vegetative State. Science, 332(6031), 858â€“862. https://doi.org/10.1126/science.1202043 Boly, M., Massimini, M., Tsychiya, N., Postle, B. R., Koch, C., & Tononi, G. (2017). Are the neural correlates of consciousness in the front or in the back of the cerebral cortex? Clinical and neuroimaging evidence [Preprint]. Neuroscience. https://doi.org/10.1101/118273 Brascamp, J., Blake, R., & Knapen, T. (2015). Negligible fronto-parietal BOLD activity accompanying unreportable switches in bistable perception. Nature Neuroscience, 18(11), 1672â€“1678. https://doi.org/10.1038/nn.4130 Brascamp, J. W., Klink, P. C., & Levelt, W. J. M. (2015). The â€˜lawsâ€™ of binocular rivalry: 50 years of Leveltâ€™s propositions. Vision Research, 109, 20â€“37. https://doi.org/10.1016/j.visres.2015.02.019 Breakspear, M. (2017). Dynamic models of large-scale brain activity. Nature Neuroscience, 20(3), 340â€“352. https://doi.org/10.1038/nn.4497 Brown, R., Lau, H., & LeDoux, J. (2019). The Misunderstood Higher-Order Approach to Consciousness [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/xpy8h Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017a). The free energy principle for action and perception: A mathematical review. Journal of Mathematical Psychology, 81, 55â€“79. https://doi.org/10.1016/j.jmp.2017.09.004 
38  
 38 
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017b). The free energy principle for action and perception: A mathematical review. arXiv:1705.09156 [q-Bio]. http://arxiv.org/abs/1705.09156 Canales-Johnson, A., Billig, A. J., Olivares, F., Gonzalez, A., del Carmen Garcia, M., Silva, W., Vaucheret, E., Ciraolo, C., Mikulan, E., Ibanez, A., Huepe, D., Chennu, S., & Bekinschtein, T. A. (2017). Dissociable neural information dynamics of perceptual integration and differentiation during bistable perception [Preprint]. Neuroscience. https://doi.org/10.1101/133801 Carruthers, P. (2003). Phenomenal Consciousness: A Naturalistic Theory. Cambridge University Press. Chalk, M., Murray, I., & SeriÃ¨s, P. (2013). Attention as Reward-Driven Optimization of Sensory Processing. Neural Computation, 25(11), 2904â€“2933. https://doi.org/10.1162/NECO_a_00494 Chou, K.-P., Hakimi, N., Hsu, T.-Y., & Smith, R. (2025). A Systematic Empirical Comparison of Active Inference and Reinforcement Learning Models in Accounting for Decision-Making Under Uncertainty (SSRN Scholarly Paper No. 5174669). Social Science Research Network. https://doi.org/10.2139/ssrn.5174669 Ciaunica, A., Seth, A., Limanowski, J., Hesp, C., & Friston, K. J. (2022). I overthinkâ€”Therefore I am not: An active inference account of altered sense of self and agency in depersonalisation disorder. Consciousness and Cognition, 101, 103320. 
39  
 39 
Cirelli, C., & Tononi, G. (2023). The Many Unknowns of Partial Sensory Disconnection during Sleep [Preprint]. Biology and Life Sciences. https://doi.org/10.20944/preprints202311.1906.v1 Cohen, M. A., Ortego, K., Kyroudis, A., & Pitts, M. (2020). Distinguishing the Neural Correlates of Perceptual Awareness and Postperceptual Processing. The Journal of Neuroscience, 40(25), 4925â€“4935. https://doi.org/10.1523/JNEUROSCI.0120-20.2020 Corcoran, A. W., & Hohwy, J. (2018). Allostasis, interoception, and the free energy principle: Feeling our way forward. In M. Tsakiris & H. De Preester (Eds.), The Interoceptive Mind: From Homeostasis to Awareness (p. 0). Oxford University Press. https://doi.org/10.1093/oso/9780198811930.003.0015 Corcoran, A. W., Hohwy, J., & Friston, K. J. (2023). Accelerating scientific progress through Bayesian adversarial collaboration. Neuron, 111(22), 3505â€“3516. https://doi.org/10.1016/j.neuron.2023.08.027 Corcoran, A. W., Macefield, V. G., & Hohwy, J. (2021). Be still my heart: Cardiac regulation as a mode of uncertainty reduction. Psychonomic Bulletin & Review, 28(4), 1211â€“1223. https://doi.org/10.3758/s13423-021-01888-y Corcoran, A. W., Pezzulo, G., & Hohwy, J. (2020). From allostatic agents to counterfactual cognisers: Active inference, biological regulation, and the origins of cognition. Biology & Philosophy, 35(3), 32. https://doi.org/10.1007/s10539-020-09746-2 Critchley, H. D., & Garfinkel, S. N. (2017). Interoception and emotion. Current Opinion in Psychology, 17, 7â€“14. https://doi.org/10.1016/j.copsyc.2017.04.020 
40  
 40 
Cullen, M., Monney, J., Mirza, M. B., & Moran, R. (2020). A Meta-Bayesian Model of Intentional Visual Search. arXiv:2006.03531 [Cs]. http://arxiv.org/abs/2006.03531 Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., & Friston, K. (2020). Active inference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology, 99, 102447. https://doi.org/10.1016/j.jmp.2020.102447 Da Costa, L., Parr, T., Sengupta, B., & Friston, K. (2021). Neural Dynamics under Active Inference: Plausibility and Efficiency of Information Processing. Entropy, 23(4), 454. https://doi.org/10.3390/e23040454 Da Costa, L., Sajid, N., Parr, T., Friston, K., & Smith, R. (2020). The relationship between dynamic programming and active inference: The discrete, finite-horizon case. arXiv:2009.08111 [Cs, Math, q-Bio]. http://arxiv.org/abs/2009.08111 Da Costa, L., Tenka, S., Zhao, D., & Sajid, N. (2024). Active Inference as a Model of Agency (No. arXiv:2401.12917). arXiv. https://doi.org/10.48550/arXiv.2401.12917 Daunizeau, J. (2011). The variational Laplace approach to approximate Bayesian inference. 25. Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889â€“904. https://doi.org/10.1162/neco.1995.7.5.889 Dehaene, S., Changeux, J.-P., & Naccache, L. (2011). The Global Neuronal Workspace Model of Conscious Access: From Neuronal Architectures to 
41  
 41 
Clinical Applications. In S. Dehaene & Y. Christen (Eds.), Characterizing Consciousness: From Cognition to the Clinic? (pp. 55â€“84). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-18015-6_4 Dehaene, S., Changeux, J.-P., Naccache, L., Sackur, J., & Sergent, C. (2006). Conscious, preconscious, and subliminal processing: A testable taxonomy. Trends in Cognitive Sciences, 10(5), 204â€“211. https://doi.org/10.1016/j.tics.2006.03.007 Desimone, R. (1998). Visual attention mediated by biased competition in extrastriate visual cortex. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 353(1373), 1245â€“1255. https://doi.org/10.1098/rstb.1998.0280 Dijkstra, N., Warrington, O., Kok, P., & Fleming, S. M. (2023). A hierarchical structure for perceptual awareness in the human brain. Doerig, A., Schurger, A., & Herzog, M. H. (2021). Hard criteria for empirical theories of consciousness. Cognitive Neuroscience, 12(2), 41â€“62. https://doi.org/10.1080/17588928.2020.1772214 Dwarakanath, A., Kapoor, V., Werner, J., Safavi, S., Fedorov, L. A., Logothetis, N. K., & Panagiotaropoulos, T. I. (2020). Prefrontal state fluctuations control access to consciousness [Preprint]. Neuroscience. https://doi.org/10.1101/2020.01.29.924928 Fleming, S. M. (2020). Awareness as inference in a higher-order state space. Neuroscience of Consciousness, 2020(1), niz020. https://doi.org/10.1093/nc/niz020 
42  
 42 
Fleming, S. M., & Michel, M. (2024). Sensory horizons and the functions of conscious vision. PsyArXiv. https://doi.org/10.31234/osf.io/4tqxs Frassle, S., Sommer, J., Jansen, A., Naber, M., & Einhauser, W. (2014). Binocular Rivalry: Frontal Activity Relates to Introspection and Action But Not to Perception. Journal of Neuroscience, 34(5), 1738â€“1747. https://doi.org/10.1523/JNEUROSCI.4403-13.2014 Friston, K. (2005). A theory of cortical responses. Philosophical Transactions of the Royal Society B: Biological Sciences, 360(1456), 815â€“836. https://doi.org/10.1098/rstb.2005.1622 Friston, K. (2013). Life as we know it. Journal of The Royal Society Interface, 10(86), 20130475. https://doi.org/10.1098/rsif.2013.0475 Friston, K. (2018). Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?). Frontiers in Psychology, 9, 579. https://doi.org/10.3389/fpsyg.2018.00579 Friston, K., Breakspear, M., & Deco, G. (2012). Perception and self-organized instability. Frontiers in Computational Neuroscience, 6. https://doi.org/10.3389/fncom.2012.00044 Friston, K. J., Parr, T., & de Vries, B. (2017). The graphical brain: Belief propagation and active inference. Network Neuroscience, 1(4), 381â€“414. https://doi.org/10.1162/NETN_a_00018 Friston, K. J., Parr, T., Yufik, Y., Sajid, N., Price, C. J., & Holmes, E. (2020). Generative models, linguistic communication and active inference. Neuroscience & Biobehavioral Reviews, 118, 42â€“64. https://doi.org/10.1016/j.neubiorev.2020.07.005 
43  
 43 
Friston, K. J., Sajid, N., Quiroga-Martinez, D. R., Parr, T., Price, C. J., & Holmes, E. (2020). Active Listening [Preprint]. Neuroscience. https://doi.org/10.1101/2020.03.18.997122 Friston, K. J., Wiese, W., & Hobson, J. A. (2020). Sentience and the Origins of Consciousness: From Cartesian Duality to Markovian Monism. Entropy, 22(5), 516. https://doi.org/10.3390/e22050516 Friston, K., Mattout, J., Trujillo-Barreto, N., Ashburner, J., & Penny, W. (2007). Variational free energy and the Laplace approximation. NeuroImage, 34(1), 220â€“234. https://doi.org/10.1016/j.neuroimage.2006.08.035 Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., & Dolan, R. J. (2014). The anatomy of choice: Dopamine and decision-making. Philosophical Transactions of the Royal Society B: Biological Sciences, 369(1655), 20130481. https://doi.org/10.1098/rstb.2013.0481 Gallagher, S., & ips. (2000). Philosophical conceptions of the self: Implications for cognitive science. Trends in Cognitive Sciences, 4(1), 14â€“21. https://doi.org/10.1016/S1364-6613(99)01417-5 Godfrey-Smith, P. (2003). Theory and Reality: An Introduction to the Philosophy of Science. University of Chicago Press. https://doi.org/10.7208/chicago/9780226300610.001.0001 Gu, X., & FitzGerald, T. H. B. (2014). Interoceptive inference: Homeostasis and decision-making. Trends in Cognitive Sciences, 18(6), 269â€“270. https://doi.org/10.1016/j.tics.2014.02.001 
44  
 44 
Haas, J. (2021). Can hierarchical predictive coding explain binocular rivalry? Philosophical Psychology, 34(3), 424â€“444. https://doi.org/10.1080/09515089.2020.1827228 Hauser, T. U., Allen, M., Purg, N., Moutoussis, M., Rees, G., & Dolan, R. J. (2017). Noradrenaline blockade specifically enhances metacognitive performance. eLife, 6, e24901. https://doi.org/10.7554/eLife.24901 Hesp, C., Smith, R., Parr, T., Allen, M., Friston, K. J., & Ramstead, M. J. D. (2021). Deeply Felt Affect: The Emergence of Valence in Deep Active Inference. Neural Computation, 33(2), 398â€“446. https://doi.org/10.1162/neco_a_01341 Hesp, C., Tschantz, A., Millidge, B., Ramstead, M., Friston, K., & Smith, R. (2020). Sophisticated Affective Inference: Simulating Anticipatory Affective Dynamics of Imagining Future Events. In T. Verbelen, P. Lanillos, C. L. Buckley, & C. De Boom (Eds.), Active Inference (pp. 179â€“186). Springer International Publishing. https://doi.org/10.1007/978-3-030-64919-7_18 Hobson, J. A. (2009). REM sleep and dreaming: Towards a theory of protoconsciousness. Nature Reviews Neuroscience, 10(11), 803â€“813. https://doi.org/10.1038/nrn2716 Hobson, J. A., & Friston, K. J. (2012). Waking and dreaming consciousness: Neurobiological and functional considerations. Progress in Neurobiology, 98(1), 82â€“98. https://doi.org/10.1016/j.pneurobio.2012.05.003 Hodson, R., Mehta, M., & Smith, R. (2023). The Empirical Status of Predictive Coding and Active Inference. Neuroscience & Biobehavioral Reviews, 105473. https://doi.org/10.1016/j.neubiorev.2023.105473 
45  
 45 
Hohwy, J. (2012). Attention and Conscious Perception in the Hypothesis Testing Brain. Frontiers in Psychology, 3. https://doi.org/10.3389/fpsyg.2012.00096 Hohwy, J. (2013). The Predictive Mind. OUP Oxford. Hohwy, J. (2016). The Self-Evidencing Brain. NoÃ»s, 50(2), 259â€“285. https://doi.org/10.1111/nous.12062 Hohwy, J. (2020). Self-supervision, normativity and the free energy principle. Synthese. https://doi.org/10.1007/s11229-020-02622-2 Hohwy, J. (2021). Conscious Self-Evidencing. Review of Philosophy and Psychology. https://doi.org/10.1007/s13164-021-00578-x Hohwy, J., Paton, B., & Palmer, C. (2016). Distrusting the present. Phenomenology and the Cognitive Sciences, 15(3), 315â€“335. https://doi.org/10.1007/s11097-015-9439-6 Hohwy, J., & Seth, A. (2020). Predictive processing as a systematic basis for identifying the neural correlates of consciousness. Philosophy and the Mind Sciences, 1(II), Article II. https://doi.org/10.33735/phimisci.2020.II.64 Holmes, E., Parr, T., Griffiths, T. D., & Friston, K. J. (2021). Active inference, selective attention, and the cocktail party problem. Neuroscience & Biobehavioral Reviews, 131, 1288â€“1304. https://doi.org/10.1016/j.neubiorev.2021.09.038 Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8), 2554â€“2558. https://doi.org/10.1073/pnas.79.8.2554 INTREPID CONSORTIUM. (2021). OSF Registries | Accelerating research on consciousness: An adversarial collaboration to test contrasting predictions of 
46  
 46 
the Integrated Information Theory and Predictive Processing accounts of consciousness. https://osf.io/35rhx?mode=&revisionId=&view_only= Kapoor, V., Dwarakanath, A., Safavi, S., Werner, J., Besserve, M., Panagiotaropoulos, T. I., & Logothetis, N. K. (2020). Decoding the contents of consciousness from prefrontal ensembles [Preprint]. Neuroscience. https://doi.org/10.1101/2020.01.28.921841 Koch, C., & Tsuchiya, N. (2007). Attention and consciousness: Two distinct brain processes. Trends in Cognitive Sciences, 11(1), 16â€“22. https://doi.org/10.1016/j.tics.2006.10.012 Koudahl, M. T., Kouw, W. M., & de Vries, B. (2021). On Epistemics in Expected Free Energy for Linear Gaussian State Space Models. Entropy, 23(12), Article 12. https://doi.org/10.3390/e23121565 Lakatos, I. (1968). Criticism and the Methodology of Scientific Research Programmes. Proceedings of the Aristotelian Society, 69, 149â€“186. Lau, H., & Rosenthal, D. (2011). Empirical support for higher-order theories of conscious awareness. Trends in Cognitive Sciences, 15(8), 365â€“373. https://doi.org/10.1016/j.tics.2011.05.009 Lavalley, C. A., Hakimi, N., Taylor, S., Kuplicki, R., Forthman, K. L., Stewart, J. L., Paulus, M. P., Khalsa, S. S., & Smith, R. (2024). Transdiagnostic failure to adapt interoceptive precision estimates across affective, substance use, and eating disorders: A replication and extension of previous results. Biological Psychology, 191, 108825. https://doi.org/10.1016/j.biopsycho.2024.108825 
47  
 47 
Levelt, W. J. M. (1965). On Binocular Rivalry [Institute for Perception]. https://www.mpi.nl/world/materials/publications/levelt/Levelt_Binocular_Rivalry_1965.pdf Lin, C.-T., Chuang, C.-H., Kerick, S., Mullen, T., Jung, T.-P., Ko, L.-W., Chen, S.-A., King, J.-T., & McDowell, K. (2016). Mind-Wandering Tends to Occur under Low Perceptual Demands during Driving. Scientific Reports, 6(1), Article 1. https://doi.org/10.1038/srep21353 Malekzadeh, P., & Plataniotis, K. N. (2024). Active Inference and Reinforcement Learning: A Unified Inference on Continuous State and Action Spaces Under Partial Observability. Neural Computation, 36(10), 2073â€“2135. https://doi.org/10.1162/neco_a_01698 Marchi, F., & Hohwy, J. (2022). The Intermediate Scope of Consciousness in the Predictive Mind. Erkenntnis, 87(2), 891â€“912. https://doi.org/10.1007/s10670-020-00222-7 Martin, J. M., Solms, M., & Sterzer, P. (2021). Useful misrepresentation: Perception as embodied proactive inference. Trends in Neurosciences, 44(8), 619â€“628. https://doi.org/10.1016/j.tins.2021.04.007 Marx, S., & Einhauser, W. (2015). Reward modulates perception in binocular rivalry. Journal of Vision, 15(1), 11â€“11. https://doi.org/10.1167/15.1.11 Mashour, G. A., Roelfsema, P., Changeux, J.-P., & Dehaene, S. (2020). Conscious Processing and the Global Neuronal Workspace Hypothesis. Neuron, 105(5), 776â€“798. https://doi.org/10.1016/j.neuron.2020.01.026 Matthews, J., SchrÃ¶der, P., Kaunitz, L., van Boxtel, J. J. A., & Tsuchiya, N. (2018). Conscious access in the near absence of attention: Critical extensions on the 
48  
 48 
dual-task paradigm. Philosophical Transactions of the Royal Society B: Biological Sciences, 373(1755), 20170352. https://doi.org/10.1098/rstb.2017.0352 Metzinger, T. (2004). Being No One: The Self-Model Theory of Subjectivity. MIT Press. Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Whence the Expected Free Energy? arXiv:2004.08128 [Cs]. http://arxiv.org/abs/2004.08128 Mirza, M. B., Adams, R. A., Friston, K., & Parr, T. (2019). Introducing a Bayesian model of selective attention based on active inference. Scientific Reports, 9(1), 13915. https://doi.org/10.1038/s41598-019-50138-8 Mirza, M. B., Adams, R. A., Mathys, C., & Friston, K. J. (2018). Human visual exploration reduces uncertainty about the sensed world. PLOS ONE, 13(1), e0190429. https://doi.org/10.1371/journal.pone.0190429 Moreno-Bote, R., Rinzel, J., & Rubin, N. (2007). Noise-Induced Alternations in an Attractor Network Model of Perceptual Bistability. Journal of Neurophysiology, 98(3), 1125â€“1139. https://doi.org/10.1152/jn.00116.2007 Negro, N. (2024). (Dis)confirming theories of consciousness and their predictions: Towards a Lakatosian consciousness science. Neuroscience of Consciousness, 2024(1), niae012. https://doi.org/10.1093/nc/niae012 Nikolova, N., Waade, P. T., Friston, K. J., & Allen, M. (2022). What Might Interoceptive Inference Reveal about Consciousness? Review of Philosophy and Psychology, 13(4), 879â€“906. https://doi.org/10.1007/s13164-021-00580-3 
49  
 49 
Novicky, F., Parr, T., Friston, K., Mirza, M. B., & Sajid, N. (2023). Bistable perception, precision and neuromodulation. Cerebral Cortex, bhad401. https://doi.org/10.1093/cercor/bhad401 Oizumi, M., Albantakis, L., & Tononi, G. (2014). From the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0. PLoS Computational Biology, 10(5), e1003588. https://doi.org/10.1371/journal.pcbi.1003588 Oâ€™Regan, J. K., & NoÃ«, A. (2001). A sensorimotor account of vision and visual consciousness. Behavioral and Brain Sciences, 24(5), 939â€“973. https://doi.org/10.1017/S0140525X01000115 Paffen, C. L. E., Alais, D., & Verstraten, F. A. J. (2006). Attention Speeds Binocular Rivalry. Psychological Science, 17(9), 752â€“756. https://doi.org/10.1111/j.1467-9280.2006.01777.x Parr, T., Corcoran, A. W., Friston, K. J., & Hohwy, J. (2019). Perceptual awareness and active inference. Neuroscience of Consciousness, 2019(1), niz012. https://doi.org/10.1093/nc/niz012 Parr, T., & Friston, K. J. (2017a). Uncertainty, epistemics and active inference. Journal of The Royal Society Interface, 14(136), 20170376. https://doi.org/10.1098/rsif.2017.0376 Parr, T., & Friston, K. J. (2017b). Working memory, attention, and salience in active inference. Scientific Reports, 7(1), 14678. https://doi.org/10.1038/s41598-017-15249-0 
50  
 50 
Parr, T., & Friston, K. J. (2018a). The Anatomy of Inference: Generative Models and Brain Structure. Frontiers in Computational Neuroscience, 12, 90. https://doi.org/10.3389/fncom.2018.00090 Parr, T., & Friston, K. J. (2018b). The Computational Anatomy of Visual Neglect. Cerebral Cortex, 28(2), 777â€“790. https://doi.org/10.1093/cercor/bhx316 Parr, T., & Friston, K. J. (2018c). The Discrete and Continuous Brain: From Decisions to Movementâ€”And Back Again. Neural Computation, 30(9), 2319â€“2347. https://doi.org/10.1162/neco_a_01102 Parr, T., Markovic, D., Kiebel, S. J., & Friston, K. J. (2019). Neuronal message passing using Mean-field, Bethe, and Marginal approximations. Scientific Reports, 9(1), 1889. https://doi.org/10.1038/s41598-018-38246-3 Parr, T., Mirza, M. B., Cagnan, H., & Friston, K. J. (2019). Dynamic Causal Modelling of Active Vision. The Journal of Neuroscience, 39(32), 6265â€“6275. https://doi.org/10.1523/JNEUROSCI.2459-18.2019 Parr, T., Pezzulo, G., & Friston, K. J. (2022). Active inference: The free energy principle in mind, brain, and behavior. The MIT Press. Parr, T., Sajid, N., Da Costa, L., Mirza, M. B., & Friston, K. J. (2021). Generative Models for Active Vision. Frontiers in Neurorobotics, 15, 651432. https://doi.org/10.3389/fnbot.2021.651432 Peever, J., Luppi, P.-H., & Montplaisir, J. (2014). Breakdown in REM sleep circuitry underlies REM sleep behavior disorder. Trends in Neurosciences, 37(5), 279â€“288. https://doi.org/10.1016/j.tins.2014.02.009 
51  
 51 
Pezzulo, G., Rigoli, F., & Friston, K. J. (2018). Hierarchical Active Inference: A Theory of Motivated Control. Trends in Cognitive Sciences, 22(4), 294â€“306. https://doi.org/10.1016/j.tics.2018.01.009 Pitts, M. A., Padwal, J., Fennelly, D., MartÃ­nez, A., & Hillyard, S. A. (2014). Gamma band activity and the P3 reflect post-perceptual processes, not visual awareness. NeuroImage, 101, 337â€“350. https://doi.org/10.1016/j.neuroimage.2014.07.024 Popper, K. R., & Popper, K. R. (2008). The Logic of scientific discovery (Repr. 2008 (twice)). Routledge. Prinz, J. (2012). The Conscious Brain. OUP USA. Ramstead, M. J., Albarracin, M., Kiefer, A., Williford, K., Safron, A., Fields, C., Solms, M., & Friston, K. (2023). Steps towards a minimal unifying model of consciousness: An integration of models of consciousness based on the free energy principle. https://doi.org/10.31234/osf.io/6eqxh Ramstead, M. J. D., Albarracin, M., Kiefer, A., Klein, B., Fields, C., Friston, K., & Safron, A. (2023). The inner screen model of consciousness: Applying the free energy principle directly to the study of conscious experience (No. arXiv:2305.02205). arXiv. https://doi.org/10.48550/arXiv.2305.02205 Ramstead, M. J. D., Badcock, P. B., & Friston, K. J. (2018). Answering SchrÃ¶dingerâ€™s question: A free-energy formulation. Physics of Life Reviews, 24, 1â€“16. https://doi.org/10.1016/j.plrev.2017.09.001 Ramstead, M. J. D., Friston, K. J., & HipÃ³lito, I. (2020). Is the Free-Energy Principle a Formal Theory of Semantics? From Variational Density Dynamics 
52  
 52 
to Neural and Phenotypic Representations. Entropy, 22(8), 889. https://doi.org/10.3390/e22080889 Ramstead, M. J. D., Hesp, C., Tschantz, A., Smith, R., Constant, A., & Friston, K. (2021). Neural and phenotypic representation under the free-energy principle. Neuroscience & Biobehavioral Reviews, 120, 109â€“122. https://doi.org/10.1016/j.neubiorev.2020.11.024 Ramstead, M. J. D., Wiese, W., Miller, M., & Friston, K. J. (2023). Deep Neurophenomenology: An Active Inference Account of Some Features of Conscious Experience and of Their Disturbance in Major Depressive Disorder. In Expected Experiences. Routledge. Ramstead, M. J., Kirchhoff, M. D., & Friston, K. J. (2020). A tale of two densities: Active inference is enactive inference. Adaptive Behavior, 28(4), 225â€“239. https://doi.org/10.1177/1059712319862774 Rao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1), 79â€“87. https://doi.org/10.1038/4580 Rorot, W. (2021). Bayesian theories of consciousness: A review in search for a minimal unifying model. Neuroscience of Consciousness, 2021(2), niab038. https://doi.org/10.1093/nc/niab038 Rowe, E. G., Garrido, M. I., & Tsuchiya, N. (2024). Feedforward connectivity patterns from visual areas to the front of the brain contain information about sensory stimuli regardless of awareness or report. Cortex, 172, 284â€“300. https://doi.org/10.1016/j.cortex.2023.11.016 
53  
 53 
Rudrauf, D., Bennequin, D., Granic, I., Landini, G., Friston, K., & Williford, K. (2017). A mathematical model of embodied consciousness. Journal of Theoretical Biology, 428, 106â€“131. https://doi.org/10.1016/j.jtbi.2017.05.032 Safavi, S., & Dayan, P. (2022). Multistability, perceptual value, and internal foraging. Neuron, 110(19), 3076â€“3090. https://doi.org/10.1016/j.neuron.2022.07.024 Safavi, S., & Dayan, P. (2024). A decision-theoretic model of multistability: Perceptual switches as internal actions. Neuroscience. https://doi.org/10.1101/2024.12.06.627286 Safron, A. (2020). An Integrated World Modeling Theory (IWMT) of Consciousness: Combining Integrated Information and Global Neuronal Workspace Theories With the Free Energy Principle and Active Inference Framework; Toward Solving the Hard Problem and Characterizing Agentic Causation. Frontiers in Artificial Intelligence, 3. https://www.frontiersin.org/articles/10.3389/frai.2020.00030 Sajid, N., Ball, P. J., Parr, T., & Friston, K. J. (2021). Active inference: Demystified and compared. Neural Computation, 33(3), 674â€“712. https://doi.org/10.1162/neco_a_01357 Sajid, N., Da Costa, L., Parr, T., & Friston, K. (2021). Active inference, Bayesian optimal design, and expected utility (No. arXiv:2110.04074). arXiv. http://arxiv.org/abs/2110.04074 Sandved-Smith, L., Hesp, C., Mattout, J., Friston, K., Lutz, A., & Ramstead, M. J. D. (2021). Towards a computational phenomenology of mental action: Modelling meta-awareness and attentional control with deep parametric active 
54  
 54 
inference. Neuroscience of Consciousness, 2021(1), niab018. https://doi.org/10.1093/nc/niab018 Schlossmacher, I., Dellert, T., Pitts, M., Bruchmann, M., & Straube, T. (2020). Differential Effects of Awareness and Task Relevance on Early and Late ERPs in a No-Report Visual Oddball Paradigm. The Journal of Neuroscience, 40(14), 2906â€“2913. https://doi.org/10.1523/JNEUROSCI.2077-19.2020 Schwartenbeck, P., FitzGerald, T. H. B., Mathys, C., Dolan, R., & Friston, K. (2015). The Dopaminergic Midbrain Encodes the Expected Certainty about Desired Outcomes. Cerebral Cortex, 25(10), 3434â€“3445. https://doi.org/10.1093/cercor/bhu159 Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H., Kronbichler, M., & Friston, K. J. (2019). Computational mechanisms of curiosity and goal-directed exploration. eLife, 8, e41703. https://doi.org/10.7554/eLife.41703 Sengupta, B., Stemmler, M. B., & Friston, K. J. (2013). Information and Efficiency in the Nervous Systemâ€”A Synthesis. PLoS Computational Biology, 9(7), e1003157. https://doi.org/10.1371/journal.pcbi.1003157 Sergent, C., Baillet, S., & Dehaene, S. (2005). Timing of the brain events underlying access to consciousness during the attentional blink. Nature Neuroscience, 8(10), 1391â€“1400. https://doi.org/10.1038/nn1549 Seth, A. (2009). Explanatory Correlates of Consciousness: Theoretical and Computational Challenges. Cognitive Computation, 1(1), 50â€“63. https://doi.org/10.1007/s12559-009-9007-x Seth, A. (2021). Being You: A New Science of Consciousness. Penguin Publishing Group. 
55  
 55 
Seth, A. K. (2013). Interoceptive inference, emotion, and the embodied self. Trends in Cognitive Sciences, 17(11), 565â€“573. https://doi.org/10.1016/j.tics.2013.09.007 Seth, A. K. (2014). A predictive processing theory of sensorimotor contingencies: Explaining the puzzle of perceptual presence and its absence in synesthesia. Cognitive Neuroscience, 5(2), 97â€“118. https://doi.org/10.1080/17588928.2013.877880 Seth, A. K., & Bayne, T. (2022). Theories of consciousness. Nature Reviews Neuroscience, 23(7), 439â€“452. https://doi.org/10.1038/s41583-022-00587-4 Seth, A. K., & Friston, K. J. (2016). Active interoceptive inference and the emotional brain. Philosophical Transactions of the Royal Society B: Biological Sciences, 371(1708), 20160007. https://doi.org/10.1098/rstb.2016.0007 Seth, A. K., Suzuki, K., & Critchley, H. D. (2012). An Interoceptive Predictive Coding Model of Conscious Presence. Frontiers in Psychology, 2. https://doi.org/10.3389/fpsyg.2011.00395 Seth, A. K., & Tsakiris, M. (2018). Being a Beast Machine: The Somatic Basis of Selfhood. Trends in Cognitive Sciences, 22(11), 969â€“981. https://doi.org/10.1016/j.tics.2018.08.008 Shipp, S. (2016). Neural Elements for Predictive Coding. Frontiers in Psychology, 7. https://doi.org/10.3389/fpsyg.2016.01792 Smith, R., Friston, K. J., & Whyte, C. J. (2022). A step-by-step tutorial on active inference and its application to empirical data. Journal of Mathematical Psychology, 107, 102632. https://doi.org/10.1016/j.jmp.2021.102632 
56  
 56 
Smith, R., Kuplicki, R., Feinstein, J., Forthman, K. L., Stewart, J. L., Paulus, M. P., Tulsa 1000 investigators, & Khalsa, S. S. (2020). A Bayesian computational model reveals a failure to adapt interoceptive precision estimates across depression, anxiety, eating, and substance use disorders. PLOS Computational Biology, 16(12), e1008484. https://doi.org/10.1371/journal.pcbi.1008484 Smith, R., Kuplicki, R., Teed, A., Upshaw, V., & Khalsa, S. S. (2020). Confirmatory Evidence that Healthy Individuals Can Adaptively Adjust Prior Expectations and Interoceptive Precision Estimates. In T. Verbelen, P. Lanillos, C. L. Buckley, & C. De Boom (Eds.), Active Inference (pp. 156â€“164). Springer International Publishing. https://doi.org/10.1007/978-3-030-64919-7_16 Smith, R., Lane, R. D., Parr, T., & Friston, K. J. (2019). Neurocomputational mechanisms underlying emotional awareness: Insights afforded by deep active inference and their potential clinical relevance [Preprint]. Neuroscience. https://doi.org/10.1101/681288 Smith, R., Mayeli, A., Taylor, S., Al Zoubi, O., Naegele, J., & Khalsa, S. S. (2021). Gut inference: A computational modelling approach. Biological Psychology, 164, 108152. https://doi.org/10.1016/j.biopsycho.2021.108152 Smith, R., Parr, T., & Friston, K. J. (2019). Simulating emotions: An active inference model of emotional state inference and emotion concept learning [Preprint]. Neuroscience. https://doi.org/10.1101/640813 Smith, R., ramstead,  maxwell, & Kiefer, A. (2021). Active inference models do not contradict folk psychology [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/kr5xf 
57  
 57 
Srinivasan, M. V., Laughlin, S. B., & Dubs, A. (1982). Predictive Coding: A Fresh View of Inhibition in the Retina. Proceedings of the Royal Society of London. Series B, Biological Sciences, 216(1205), 427â€“459. Stephan, K. E., Manjaly, Z. M., Mathys, C. D., Weber, L. A. E., Paliwal, S., Gard, T., Tittgemeyer, M., Fleming, S. M., Haker, H., Seth, A. K., & Petzschner, F. H. (2016). Allostatic Self-efficacy: A Metacognitive Theory of Dyshomeostasis-Induced Fatigue and Depression. Frontiers in Human Neuroscience, 10. https://doi.org/10.3389/fnhum.2016.00550 Strauss, M., Sitt, J. D., King, J.-R., Elbaz, M., Azizi, L., Buiatti, M., Naccache, L., van Wassenhove, V., & Dehaene, S. (2015). Disruption of hierarchical predictive coding during sleep. Proceedings of the National Academy of Sciences, 112(11), E1353â€“E1362. https://doi.org/10.1073/pnas.1501026112 Teasdale, G., Maas, A., Lecky, F., Manley, G., Stocchetti, N., & Murray, G. (2014). The Glasgow Coma Scale at 40 years: Standing the test of time. The Lancet Neurology, 13(8), 844â€“854. https://doi.org/10.1016/S1474-4422(14)70120-6 Theriault, J. E., Young, L., & Barrett, L. F. (2021). The sense of should: A biologically-based framework for modeling social pressure. Physics of Life Reviews, 36, 100â€“136. https://doi.org/10.1016/j.plrev.2020.01.004 Tononi, G. (1998). Consciousness and Complexity. Science, 282(5395), 1846â€“1851. https://doi.org/10.1126/science.282.5395.1846 Tononi, G., Boly, M., Massimini, M., & Koch, C. (2016). Integrated information theory: From consciousness to its physical substrate. Nature Reviews Neuroscience, 17(7), 450â€“461. https://doi.org/10.1038/nrn.2016.44 
58  
 58 
Tschantz, A., Barca, L., Maisto, D., Buckley, C. L., Seth, A. K., & Pezzulo, G. (2022). Simulating homeostatic, allostatic and goal-directed forms of interoceptive control using active inference. Biological Psychology, 169, 108266. https://doi.org/10.1016/j.biopsycho.2022.108266 Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement Learning through Active Inference. arXiv:2002.12636 [Cs, Eess, Math, Stat]. http://arxiv.org/abs/2002.12636 Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). Learning action-oriented models through active inference. PLOS Computational Biology, 16(4), e1007805. https://doi.org/10.1371/journal.pcbi.1007805 Tsuchiya, N., Wilke, M., FrÃ¤ssle, S., & Lamme, V. A. F. (2015). No-Report Paradigms: Extracting the True Neural Correlates of Consciousness. Trends in Cognitive Sciences, 19(12), 757â€“770. https://doi.org/10.1016/j.tics.2015.10.002 Vilas, M. G., Auksztulewicz, R., & Melloni, L. (2021). Active Inference as a Computational Framework for Consciousness. Review of Philosophy and Psychology. https://doi.org/10.1007/s13164-021-00579-w Vincent, P., Parr, T., Benrimoh, D., & Friston, K. J. (2019). With an eye on uncertainty: Modelling pupillary responses to environmental volatility. PLOS Computational Biology, 15(7), e1007126. https://doi.org/10.1371/journal.pcbi.1007126 Wainwright, M. J., & Jordan, M. I. (2008). Graphical Models, Exponential Families, and Variational Inference. Foundations and TrendsÂ® in Machine Learning, 1(1â€“2), 1â€“305. https://doi.org/10.1561/2200000001 
59  
 59 
Walsh, K. S., McGovern, D. P., Clark, A., & Oâ€™Connell, R. G. (2020). Evaluating the neurophysiological evidence for predictive processing as a model of perception. Annals of the New York Academy of Sciences, 1464(1), 242â€“268. https://doi.org/10.1111/nyas.14321 Whyte, C. J. (2019). Integrating the global neuronal workspace into the framework of predictive processing: Towards a working hypothesis. Consciousness and Cognition, 73, 102763. https://doi.org/10.1016/j.concog.2019.102763 Whyte, C. J., Hohwy, J., & Smith, R. (2022). An active inference model of conscious access: How cognitive action selection reconciles the results of report and no-report paradigms. Current Research in Neurobiology, 3, 100036. https://doi.org/10.1016/j.crneur.2022.100036 Whyte, C. J., & Smith, R. (2021). The predictive global neuronal workspace: A formal active inference model of visual consciousness. Progress in Neurobiology, 199, 101918. https://doi.org/10.1016/j.pneurobio.2020.101918 Wilbertz, G., & Sterzer, P. (2018). Differentiating aversive conditioning in bistable perception: Avoidance of a percept vs. salience of a stimulus. Consciousness and Cognition, 61, 38â€“48. https://doi.org/10.1016/j.concog.2018.03.010 Wilson, H. R. (2007). Minimal physiological conditions for binocular rivalry and rivalry memory. Vision Research, 47(21), 2741â€“2750. https://doi.org/10.1016/j.visres.2007.07.007 Wilson, H. R., & Cowan, J. D. (1972). Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons. Biophysical Journal, 12(1), 1â€“24. https://doi.org/10.1016/S0006-3495(72)86068-5 Winn, J., & Bishop, C. M. (2005). Variational Message Passing. 
60  
 60 
Yaron, I., Melloni, L., Pitts, M., & Mudrik, L. (2022). The ConTraSt database for analysing and comparing empirical studies of consciousness theories. Nature Human Behaviour, 6(4), Article 4. https://doi.org/10.1038/s41562-021-01284-5 Zeidman, P., Friston, K., & Parr, T. (2022). A Primer on Variational Laplace (VL). 35. Zhang, P., Jamison, K., Engel, S., He, B., & He, S. (2011). Binocular Rivalry Requires Visual Attention. Neuron, 71(2), 362â€“369. https://doi.org/10.1016/j.neuron.2011.05.035   