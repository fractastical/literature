IOP Conference Series:
Materials Science and
Engineering
      
PAPER • OPEN ACCESS
Active inference, preference learning and adaptive
behaviour
To cite this article: Noor Sajid et al 2022 IOP Conf. Ser.: Mater. Sci. Eng. 1261 012020
 
View the article online for updates and enhancements.
You may also like
Introduction to the special issue on
volatility modelling
Rama Cont and Marco Avellaneda
-
Volatility analysis of the flight block time
based on the stochastic volatility model
Ya Sun, Meiyi Wang and Hua Xie
-
Planning with tensor networks based on
active inference
Samuel T Wauthier, Tim Verbelen, Bart
Dhoedt et al.
-
 
This content was downloaded from IP address 143.105.117.198 on 13/12/2025 at 08:10
Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence. Any further distribution
of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
Published under licence by IOP Publishing Ltd
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
1
Active inference, preference learning and adaptive
behaviour
Noor Sajid1, Panagiotis Tigas2 & Karl Friston1
1WCHN, University College London, UK
2OATML, Oxford University, UK
Abstract. The ability to adapt to a changing environment underwrites sentient
behaviour e.g., wearing a raincoat when walking in the rain but removing it when
indoors. In such instances, agents act to satisfy some preferred mode of behaviour that
leads to predictable states necessary for survival, i.e., states that are characteristic
of that agent. In this chapter, we describe how active inference agents, equipped
with preference learning, can exhibit these distinct behavioural modes – inﬂuenced
by environment dynamics – to aptly trade-oﬀ between preference satisfaction and
exploration. We validate this in a modiﬁed OpenAI Gym FrozenLake environment
(without any extrinsic signal) with and without volatility under a ﬁxed model of the
environment. In a static (i.e., without volatility) environment, preference-learning
agents accumulate conﬁdent (Bayesian) beliefs about their behaviour and act to satisfy
them. In contrast, volatile dynamics led to preference uncertainty and exploratory
behaviour. This demonstrates that active inference agents, equipped with preference
learning, have the appropriate machinery to (i) engage in adaptive behaviour under
appropriate levels of volatility, and (ii) learn context-dependent subjective preferences.
1 Introduction
Biological agents exhibit adaptive behaviour when engaging with their environment. These in-
teractions are underwritten by extrinsic and intrinsic motivations that prescribe Bayes-optimal
behaviour, in the sense of Bayesian design and active learning (Conant and Ross Ashby,
1970; Friston, 2019; Bruineberg et al., 2018b; Sajid et al., 2021b). Accordingly, agents have
preferred modus operandi (or prior preferences) that inﬂuence their exchanges with the
environment and may update under volatile conditions (Ashby, 1961; Schrodinger et al.,
1992; Friston, 2013). These preferences can persist despite the absence of explicit signals
from the environment (or sometimes in spite of it).
Brieﬂy, preferences are a subjective assessment of what the agent would like to experience
(i.e., desired states or outcomes). These are inherently diﬀerent from, and should not be
conﬂated with, objective rewards found in reinforcement learning and expected utility theory
(Kahneman and Tversky, 2013; Fleming and Sheu, 2002) (Table 1). This is because objective
rewards are set from the perspective of the designer (or environment), where the agent is
not equipped with the ability to learn subjective preferences (i.e., what outcomes it prefers).
Formulations in terms of reward or utility are diﬃcult to reconcile with biological learning,
where agents can gradually acquire preferences, settle into their ecological niche, and update
their preferences as environmental contingencies change. Note a fundamental diﬀerence
between reward—outcomes that explain observed behaviour–and preferences, which are an
attribute of an agent’s beliefs. Furthermore, reward is a uni-dimensional construct, while
preferences apply to every state or outcome entertained by the agents generative or world
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
2
T able 1.Glossary
T erm Deﬁnition
Generative process This describes transitions among latent states in the environment that
generate observations.
Generative model A statistical model of the generative process. The generative model
is speciﬁed as a joint probability distribution over actions, states,
and observations. This joint probability is usually expressed in terms
of a prior and likelihood. In our (discrete) case, the prior speciﬁes
action-dependent state transitions, while the likelihood speciﬁes the
probabilistic mapping from states to outcomes or observations
Objective rewards Particular outcomes determined by the environment e.g., an extrinsic
teaching signal, reward signal, et cetera. Rewards are part of the
generative process.
Subjective preferences Beliefs about outcomes that the agent anticipates, i.e., what the agent
‘likes’ . These are inherently diﬀerent from rewards, which are outcomes.
Preferences are probabilistic speciﬁcations of every state or outcome
an agent expects, a priori, to encounter. Preferences are part of the
generative model.
Preference satisfaction Consistent realisation of behaviour leading to preferred outcomes, i.e.,
exploitation of the environment to realise preferences.
Conjugate priors Conjugate priors are chosen to ensure posterior is in the same family of
distributions as the prior. This can sometimes ﬁnesse the tractability
of deriving Bayesian belief updates.
model. In other words, an agent’s preferences are just its a priori beliefs about the likelihood
of any state or outcome. A preferred state is one that, a priori, the agent considers itself
likely to occupy.
From a technical perspective, these preferred states are simply the attracting set of a
pullback attractor that the agent returns to, despite external perturbations. Our formulation
is motivated by this generic formulation of behaviour, i.e., how biological preferences shape
and agent’s experiences. For example, imagine a continuously changing world due to recurrent
ﬂooding. An agent equipped with preference learning could ensure its survival by appropriate
niche construction (i.e., building a dam) or moving away from the ﬂoodplain. In this example,
surviving agents would implicitly learn an aversion to drowning.
To investigate how preferences shape adaptive behaviour (under a ﬁxed world model),
we turn to active inference (Friston et al., 2017; Da Costa et al., 2020; Sajid et al., 2021a)
and preference learning (Bruineberg et al., 2018b; Sajid et al., 2021a,c). Our aim here is to
illustrate a generalisation of reward learning, in which preferences are themselves learned: i.e.,
‘learning to prefer’ . This may sound like an oxymoron; in the sense that reinforcement learning
operationally deﬁnes rewards in terms of their ability to reinforce behaviour. However, in
active inference, the imperative that underwrites optimal behaviour is simply to maximise
the (marginal) likelihood of an agent’s generative or world model—thereby ensuring the
likelihood of continued exchange with the world; i.e., survival. This means that an active
inference agent can learn behavioural outcomes that are sustainable by the environment.
In this setting, preferred outcomes are learnable. However, preference learning will clearly
depend on environmental volatility. This speaks to a trade-oﬀ between exploration and
preference satisfaction (i.e., seeking out preferred outcomes) that is constrained by the
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
3
predictability of the environment (Sajid et al., 2021c). This trade-oﬀ is the focus of what
follows.
Brieﬂy, active inference is a formal account of how self-organising agents’ interface
with their environment and maintain a particular steady-state over time (Friston, 2019;
Da Costa et al., 2020). It stipulates a feedback loop between the agent, and its interactions
with the environment (Bruineberg et al., 2018b). Explicitly, the agent’s actions shape the
environment and shift the agents perception that inﬂuences future behaviour. For this,
active inference agents optimise two complementary free energy functionals across time:
variational and expected free energy. The variational free energy measures the ﬁt between the
agent’s generative model and observable outcomes sampled from a generative process (i.e.,
the environment). Here, the generative process refers to the structure of the environment
that generated the agent’s observations, and the generative model embodies the agent’s
expectations about the causes of those observations. Conversely, the expected free energy
scores possible action trajectories in terms of their ability to reach preferred outcomes via
maximisation of extrinsic and intrinsic values. This equips the agent with a formal way to
assess diﬀerent hypothesis about the types of behaviour that can be pursued.
In short, one could summarise active inference as the inversion of a generative model that
includes the consequences of action. Because consequences postdate causes, this necessarily
entails a generative model of agency that has temporal depth. Inversion or (active) inference
under these models evinces a kind of planning—or control—as inference (Attias, 2003;
Botvinick and Toussaint, 2012; Millidge, 2020) that is Bayes optimal, in the dual sense of
optimal Bayesian design and decision-making. In other words, plans or policies are inferred on
the basis of optimally reducing uncertainty about the causes of outcomes, while minimising
Bayesian risk, in relation to prior preferences.
Preference learning can be incorporated by equipping the agent’s generative model
with the right set of conjugate priors that facilitate the learning of preferences over states
across time (Sajid et al., 2021a,c). During planning, these prior preferences inﬂuence how
policies (i.e., action sequences) are evaluated, and shape subsequent behaviour. The ability
to learn preferences, contingent upon environment dynamics, introduces an additional level
of dependence between the agent-environment interactions. In other words, active inference
with preference learning (Bruineberg et al., 2018a; Sajid et al., 2021c) changes the epistemic
dispositions of an agent. In this instance, the agent expects the environment to unfold in
a particular kind of way and – through its actions – ensures that those expectations are
realised. For example, if the agent has prior preferences for residing in mild temperatures,
then it will actively choose to alter the state of the environment to satisfy these preferences
e.g., installing an air conditioning unit in hot weather, etc.
Here, we describe how preference learning can be introduced using conjugate priors. For
this, we build upon the (state) preference learning mechanism — introduced in (Sajid et al.,
2021c) — to provide a complementary exposition of why preference learning, over particular
latent (i.e., hidden) states, can inﬂuence the agent’s behaviour given changes in environmental
contingencies. Speciﬁcally, we illustrate how: i) behaviour changes systematically with
increased environmental volatility, and ii) how learnt preferences underwrite these behaviour
shifts. We demonstrate this in a modiﬁed OpenAI Gym FrozenLake environment (without
any extrinsic teaching or reward signal), with and without volatility. Our simulations reveal
that in static environments, the agent learns conﬁdent (i.e., precise) preferences and acts
to satisfy them. However, preference uncertainty leads to exploratory behaviour in a more
volatile setting despite no changes to the agent’s generative model. This speaks to a trade-oﬀ
between exploration and preference satisfaction (i.e. exploitation of desired states) that is
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
4
constrained by the predictability of the environment (Sajid et al., 2021c). We conclude with
a brief discussion of the relevance of this formulation for characterising adaptive behaviour
under evolving dynamics, and how preference learning may inﬂuence this behaviour.
2 Active Inference
Active inference describes how adaptive agents interact with their environments, described by
some generative process. The theory rests on a generative model of the agent’s environment
that is ﬁtted to sampled observations ( o) from the generative process to reduce surprisal or
the variational free energy (Friston et al., 2017):
F(τ)= Eq(s|π)
[
log q(sτ |π) − log p(oτ ,s τ |π)
]
, (1)
where q(·) is an approximate density over latent or hidden states ( s), given some policy
(π). Variational free energy plays the role of an upper bound on the log (marginal) likelihood
of outcomes or evidence for the generative model (Winn et al., 2005). In this sense, active
inference has been described in terms of self-evidencing (Hohwy, 2016). Additionally, these
agents engage with their environment by selecting actions that minimise their uncertainty
i.e., expected free energy (Parr and Friston, 2019):
G(π,τ )= Ep(oτ |sτ ,θ)q(sτ ,θ|π)
[
log q(sτ ,θ |π) − log p(oτ ,s τ ,θ )
]
. (2)
This objective is obtained from Eq.1 by supplementing the expectation under the ap-
proximate posterior with the likelihood, resulting in the following predictive distribution
p(oτ |sτ ,θ )q(sτ ,θ |π). Here, the next action is selected by sampling from the distribution:
p(π)= σ
(
− G(π)
)
= σ
(
−
∑
τ>t
G(π,τ )
)
, (3)
where σ(·) is the softmax function, and p(π) has a Gibb’s distribution.
3 Preference learning
Active inference agents do not have access to the generative process but can alter the
environmental state by selecting particular actions that minimise expected free energy. To
equip the agent with the ability to learn preferences, we extend the expected free energy
functional to incorporate preferences over latent states by introducing conjugate priors over
prior beliefs (i.e., hyper-priors) (Friston et al., 2017; Sajid et al., 2021c):
G(π,τ )= − E˜q
[
log p(oτ |sτ )
]
(4a)
+ E˜q
[
log q(sτ |π) − log p(s|C)
]
(4b)
+ E˜q
[
log q(θ|sτ ,π ) − log p(θ|sτ ,o τ )
]
. (4c)
where, ˜q = qφ(oτ ,s τ ,θ |π)= q(θ|π)q(sτ |,π )qφ(oτ |sτ ,π ), and p(s|C) is the probability of a
particular state given (learnt) prior preferences ( C). Practically, because our distribution of
interest is Categorical, we specify the Dirichlet distribution as the conjugate prior. This is
deﬁned as, P(s) ∼ Cat(D):
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
5
P(Di|di)= Dir(di) ⇒
⎧
⎨
⎩
EP (Di|di)
[
Di
ij
]
=
di
ij∑
k di
kj
EP (Di|di)
[
log(Di
ij)
]
= Ϝ (di
ij) − Ϝ (∑
k di
kj )
(5)
where, Ϝ is the digamma function, d ∼ R+ and same parameterisation holds for P(r) ∼
Cat(C). The posteriors for the Dirichlet hyper–parameters are evaluated by updating the
prior using the following rule di,j + α ∗ si,j where si,j are the observations for that particular
category and α the learning rate. This allows the agent to accumulate contingencies (i.e.,
pseudocounts) and learn what it prefers.
Using this, a preference learning mechanism can be instantiated; introduced as Pepper in
Sajid et al. (2021c). Brieﬂy, Pepper stipulates a circular agent-environment dependence via
a double loop preference learning. Here, the ﬁrst loop is across time-steps during a particular
interaction with the environment (i.e., a single episode). The second loop, evolving at a slower
timescale, is across episodes and entails updating prior preferences using the information
collected during each interaction. In subsequent time-steps, the updated preferences are used
to select the next action via their inﬂuence on the expected free energy—and interactions
with the environment used to update preferences. In turn, these preferences are used to select
actions during the next episode, and so on. This formulation of preference learning highlights
the circular causality between the agent and its environment. Speciﬁcally, the environment
shapes the agent’s behaviour via preference learning and the learnt preferences inﬂuence the
environment via the agent’s action on the environment. See Sajid et al. (2021c) for technical
details.
4 Simulations
We used a modiﬁed OpenAI Gym’s FrozenLake environment to evaluate how preference
learning inﬂuences behaviour: in particular, the trade-oﬀ between exploration and preference
satisfaction, under diﬀerent environment dynamics.
4.1 FrozenLake environment
In the original FrozenLake task, the agent has to navigate a grid world comprised of frozen,
hole and goal tiles, using 4 actions (left, right, down or up). Upon reaching the goal tile, the
agent receives a reward of 10 or a penalty −0.25 if it moves to the hole. In our simulations, we
included a sub-goal tile but removed the extrinsic reward signal (Fig.1). That is, although the
agent can diﬀerentiate between tile categories – parameterised by its generative model – it does
not receive reward from the environment: in active inference, reward or reinforcement signals
are replaced by preferences for particular states or outcomes. These learnable preferences
were encoded by an initial uniform (i.e., prior) Dirichlet concentration parameters encoding
preferences over states, at the onset of each experiment. Eﬀectively, this means that agents
learn to prefer the states that they can reliably solicit through their behaviour.
To simulate diﬀerent environmental dynamics, we introduced environmental volatility
by switching the FrozenLake conﬁguration every K-th step (Table 2). Additionally, the
agent’s starting position was varied across each episode. Note, this is diﬀerent to the original
FrozenLake environment, where the agent always starts the episode from the top right corner
of the grid. This provided an appropriate test-bed to evaluate how adaptive behaviour was a
natural consequence of shifts in prior preferences while engaging with volatile and non-volatile
environments.
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
6
Fig. 1. The four panels show examples of the diﬀerent OpenAI Gym FrozenLake 16x16 environments
used for our simulations. Here, light green represents the goal tile, bottle green the sub-goal, blue
the hole and red the frozen tiles.
T able 2.Training and Preference learning parameters
Parameter T raining Preference learning
Planning Horizon 15 steps 15 steps
Episode Length 50 steps 50 steps
Reset Every 5 steps 1, 10, 20, 40, 50 steps
No. Episodes 50 episodes 50 episodes
No. State Categories 64 categories 64 categories
No. State Dimensions 50 dimensions 50 dimensions
No. Agents · 10 agent
We assessed behaviour with and without volatility. Furthermore, the trained generative
model network weights, optimised Eq.1 using ADAM (Kingma and Ba, 2014), were frozen
during our experiments 1. Thus, adaptive behaviour is a direct consequence of preference
learning that induces diﬀerences in expected free energy estimation – and ensuing policy
selection.
1 Our agent was implemented as an extension to Dreamer V2 (Hafner et al., 2020) public imple-
mentation. Speciﬁcally, Dreamer’s generative model training loop was used with an open-loop
policy. However, the actor learning part of Dreamer was not incorporated, and the generative
model was trained using Plan2Explore Sekar et al. (2020). The exact implementation details are
available Sajid et al. (2021c).
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
7
Fig. 2. Visualisation of the posterior latent states (estimated using qφ(st|.)) during state preference
learning over episodes. The states have been projected onto their ﬁrst two principal components,
and the black circles represent the k-mean centroid of resulting clusters. Here, the accompanying
graphics present a representative agent trajectory with tiles visited from the cluster highlighted in
red.
4.2 Preference learning
Two types of behaviours were observed: exploratory and satisfaction of preferences (Fig.2).
Here, preference satisfaction entailed restricted movement within a small section of the grid
with gradual accumulation of prior preferences. Figure 3 presents examples of learnt states
preferences across static and volatile environment. These accumulated preferences speak to
the self-evidencing nature of our agents (Hohwy, 2016). That is as the agent sees similar
observations across episodes it grows increasingly conﬁdent (via increased precision over the
prior preference) that these are the outcomes it prefers. Conversely, we observe exploratory
behaviour under the volatile setting with gradual preference accumulation of previously
unseen states (Fig.3).
4.3 Exploration and preference satisfaction trade-oﬀ
We used Hausdorﬀ distance (Blumberg, 1920), to evaluate the agent’s adaptive behaviour, i.e.,
ability to trade-oﬀ between exploration and preference satisfaction (Fig.4). Hausdorﬀ distance
measures the maximum distance between the agents position in a particular trajectory with
the nearest position taken in another trajectory. Thus, high Hausdorﬀ distance denotes
increased exploration. This is because trajectories observed across episodes diﬀer from one
other and a low distance entails consistent repetition of trajectories across episodes. Using this
metric, we observed an inverted u-shaped association, between volatility in the environment
and preference satisfaction for preference learning over the states. Here, 50% volatility
shifted the agent behaviour from preference satisfaction to exploration, when faced with high
uncertainty and an implicit inability to predict the future. Accordingly, agents in this setting
pursued long paths from the initial location, and had the highest Hausdorﬀ distance. (Fig.2).
Under complete volatility, the agents behaviour shifted back to satisfying its preferences.
Interestingly, this is undergirded by bi-modal preference learning. In other words, regardless
of how the environment changed, agents moved directly to a particular location given
the initial position. We illustrate exemplar trajectories in Fig.5. This ability to disregard
random, noisy information – with continuous environmental changes – highlights preference
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
8
Fig. 3. An example of learnt state preferences for a single agent in a static and highly volatile
setting. Here, 64 state categories are presented on the x-axis and episodes on the y-axis. The ﬁrst
panel is for preferences learnt under a static setting, and the second for preferences learnt under
a volatile setting. The scale goes from white (i.e., high Dirichlet concentration) to black (i.e., low
Dirichlet concentration), and grey indicates gradations between these.
satisfaction motivations beyond exploration under state preference learning. Conversely, the
active inference agent without preference learning capacity, exhibited a shift in behaviour as
the environment became volatile. This exploratory drive reﬂects the imperative to resolve
state uncertainty i.e., the mutual information between the agent’s beliefs before and after
making a new observation.
5 Concluding remarks
Active inference agents, equipped with preference learning, prefer modes of behaviour that lead
to predictable states, necessary for survival. In other words, these agents have certain kinds
of expectation about the states of aﬀair and act to realise them. Importantly, in a changing
context, they can learn context-dependent subjective preferences. Here, these subjective
preferences constitute what is preferred by the agent, instead of objective preferences, i.e.,
what is considered optimal by the environment ’designer’ . This introduces an agent-centred
formulation that allows the agent to construct its niche when interacting with the environment
(Bruineberg et al., 2018b; Constant et al., 2018). We illustrate this through simulations,
where preference learning agents develop bi-modal preferences when faced with extreme
uncertainty but act to satisfy preferences in predictable settings.
Related work Reinforcement learning (RL) is another plausible framework for building
adaptive agents. However, it relies on an extrinsic signal to reinforce modes of behaviour that
are diﬃcult to pre-deﬁne in real environments. Without these extrinsic signals, the agent
must rely on intrinsic motivation to exhibit adaptive, exploratory behaviour (Ryan and Deci,
2000; Singh et al., 2004, 2005). Popular methods focus on exploration based on information
gain (Houthooft et al., 2016; Still and Precup, 2011), prediction error (Achiam and Sastry,
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
9
Fig. 4. A: The violin plot presents the preference satisfaction and exploration trade-oﬀ measured
using Hausdorﬀ distance (Blumberg, 1920) at diﬀerent levels of volatility in the environment. The
x-axis denotes environment volatility at constant map (0%), change in map every 40 steps (25%), 20
steps (50%), 10 steps (75%) and every step (100%). The y-axis denotes the Hausdorﬀ distance. Here,
red reports the agent optimising the standard expected free energy (EFE), and green for preference
learning. B: The line plot depicts the entropy of (approximate) posterior beliefs over states, across
varying levels of volatility in the environment. The x-axis represents the episodes, and the y-axis
entropy (in natural units). Here, the dark lines represent the mean (across 10 seeds), and shaded
area the 95% conﬁdence interval. The pink line is for 0%, blue for 25%, green for 50%, black for
75% and red for 100% volatility in the environment.
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
10
Fig. 5. Representative example of trajectories observed during preference learning in volatile settings.
Each ﬁgure illustrates an agent’s trajectory for a particular episode. Here, purple is the starting
position, pink the trajectory, cyan square denotes the ﬁrst learnt preference and dark blue denotes
the second learnt preference.
2017; Pathak et al., 2017; Stadie et al., 2015), novelty search (Lehman and Stanley, 2008;
Shyam et al., 2019), curiosity (Schmidhuber, 1991, 2007), entropy (Haarnoja et al., 2018; Lee
et al., 2019), empowerment (Klyubin et al., 2005; Mohamed and Rezende, 2015) or control
as inference Levine (2018); Todorov (2008).
Recently, the focus has shifted to self-supervised reward-free learning (Jin et al., 2020;
Wang et al., 2020), but the ambition of such approaches is to learn representations that
can be exploited in diﬀerent (downstream) tasks. Conversely, our approach relies on agents
learning subjective preferences that shape interactions with the environment. Thus, any
observed exploration is simply an emergent behaviour of the agent, and not a contrived
procedure for improving future task performance. Conceptually, preference learning active
inference agents are most similar to open-ended learning algorithms Schmidhuber (2010);
Standish (2003); Stanley et al. (2017), where agents are responsible for never-ending learning
opportunities.
Limitations and future directions By equipping the agent with the capacity to learn
subjective preferences, we forego control over the agent’s behaviour. That is, by removing
the ability to manipulate the agent preferences – regarding what is considered appropriate
behaviour – we have removed the only clear communication channel that can be used to
control agent behaviour and/or deﬁne task goals. However, given the ﬂexibility of the active
inference formulation, precise preferences – under direct supervision – could be installed as
required. Practically, this simply involves equipping an agent with precise preferences over
states (or outcomes) with suitable Dirichlet parameters (i.e., concentration parameters). To
create agents that have conﬁdent beliefs in their preferences—and that are therefore more
resistant to learning new preferences—the initial (prior) Dirichlet parameters can be set to
high values. Eﬀectively, this means that agents have to experience many more outcomes
before they come to learn these are preferred or characteristic outcomes for the agent in
question. Conversely, starting with very low Dirichlet parameters creates an impressionable
agent who is predisposed to learning its preferences. Our simulations revealed that rich
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
11
category spaces were required for learning preferences that establish distinct behavioural
strategies; speciﬁcally, within a volatile setting. Thus, future research could evaluate the
parameterisation of latent states suitable for equipping the agent with the capacity to learn
apt preferences.
Preference learning of the sort considered in this work, emerges from minimising vari-
ational free energy across time, when agents are equipped with hyper-priors over their
preferences. When parameterised using a conjugate (Dirichlet) hyper-prior learning corre-
sponds to accumulating the relative frequency of diﬀerent outcomes after enacting policies.
The functional form of the ensuing variational updates is the same as commonly used learn-
ing rules, known as associative or Hebbian plasticity (Friston et al., 2016). Here, synaptic
eﬃcacy is reinforced by the simultaneous ﬁring of pre-and post-synaptic neurons e.g., as
more frozen tiles are observed, more evidence is accumulated in the synaptic connection to
support the hypothesis that these are the sort of preferentially observed tiles. Future work
could investigate how diﬀerent learning strategies inﬂuence preference accumulation, and
ultimately agent behaviour. Importantly, the current preference learning strategy did not
employ ’reset’ or removal of redundant preferences (i.e., Dirichlet concentration parameters):
having accrued a preference for frozen tiles the agent could not remove these preferences
even if the environment changed. The resetting of Dirichlet hyper-priors was evaluated in
(Sajid et al., 2021c) and produces consistently exploratory agents that respond to epistemic
(uncertainty reducing) aﬀordances. The interesting question now is whether one can equip
generative models with a further hierarchical level to produce agents that optimally reset
their prior preferences, when moving to a new environment or context. Future research
can investigate how the timescale, at which redundant preferences are removed, inﬂuences
adaptive behaviour.
Acknowledgements
NS acknowledges funding from the Medical Research Council, UK (MR/S502522/1). PT is
supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems (grant
reference EP/L015897/1). KJF is funded by the Wellcome Trust (Ref: 203147/Z/16/Z and
205103/Z/16/Z).
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
12
Bibliography
Joshua Achiam and S. Sastry. Surprise-based intrinsic motivation for deep reinforcement
learning. ArXiv, abs/1703.01732, 2017.
W Ross Ashby. An introduction to cybernetics. Chapman & Hall Ltd, 1961.
Hagai Attias. Planning by probabilistic inference. In International Workshop on Artiﬁcial
Intelligence and Statistics, pages 9–16. PMLR, 2003.
Henry Blumberg. Hausdorﬀ’s grundzüge der mengenlehre. Bulletin of the American Mathe-
matical Society, 27(3):116–129, 1920.
Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in cognitive sciences,
16(10):485–488, 2012.
Jelle Bruineberg, Julian Kiverstein, and Erik Rietveld. The anticipating brain is not a
scientist: the free-energy principle from an ecological-enactive perspective. Synthese, 195
(6):2417–2444, 2018a.
Jelle Bruineberg, Erik Rietveld, Thomas Parr, Leendert van Maanen, and Karl J Friston. Free-
energy minimization in joint agent-environment systems: A niche construction perspective.
Journal of theoretical biology, 455:161–178, 2018b.
Roger C Conant and W Ross Ashby. Every good regulator of a system must be a model of
that system. International journal of systems science, 1(2):89–97, 1970.
Axel Constant, Maxwell JD Ramstead, Samuel PL Veissiere, John O Campbell, and Karl J
Friston. A variational approach to niche construction. Journal of the Royal Society
Interface, 15(141):20170685, 2018.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl
Friston. Active inference on discrete state-spaces: a synthesis. Journal of Mathematical
Psychology, 99:102447, 2020.
WH Fleming and SJ Sheu. Risk-sensitive control and an optimal investment model ii. The
Annals of Applied Probability, 12(2):730–767, 2002.
Karl Friston. Life as we know it. Journal of the Royal Society Interface, 10(86):20130475,
2013.
Karl Friston. A free energy principle for a particular physics.arXiv preprint arXiv:1906.10184,
2019.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni Pezzulo,
et al. Active inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879,
2016.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni
Pezzulo. Active inference: a process theory. Neural computation, 29(1):1–49, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-
policy maximum entropy deep reinforcement learning with a stochastic actor. In ICML,
2018.
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari
with discrete world models. arXiv preprint arXiv:2010.02193, 2020.
Jakob Hohwy. The self-evidencing brain. Noûs, 50(2):259–285, 2016.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel.
Vime: Variational information maximizing exploration. In NIPS, 2016.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration
for reinforcement learning. In International Conference on Machine Learning, pages 4870–
4879. PMLR, 2020.
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
13
Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In
Handbook of the fundamentals of ﬁnancial decision making: Part I, pages 99–127. World
Scientiﬁc, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A universal
agent-centric measure of control. In 2005 IEEE Congress on Evolutionary Computation,
volume 1, pages 128–135. IEEE, 2005.
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan
Salakhutdinov. Eﬃcient exploration via state marginal matching. ArXiv, abs/1906.05274,
2019.
J. Lehman and K. Stanley. Exploiting open-endedness to solve problems through the search
for novelty. In ALIFE, 2008.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and
review. arXiv preprint arXiv:1805.00909, 2018.
Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical
Psychology, 96:102348, 2020.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for
intrinsically motivated reinforcement learning. In NIPS, 2015.
Thomas Parr and Karl J Friston. Generalised free energy and active inference. Biological
Cybernetics, 113(5-6):495–513, 2019.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven
exploration by self-supervised prediction. 2017 IEEE Conference on Computer Vision and
Pattern Recognition Workshops (CVPR W), pages 488–489, 2017.
Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic deﬁnitions
and new directions. Contemporary educational psychology, 25(1):54–67, 2000.
Noor Sajid, Philip J Ball, Thomas Parr, and Karl J Friston. Active inference: demystiﬁed
and compared. Neural Computation, 33(3):674–712, 2021a.
Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, bayesian
optimal design, and expected utility. arXiv e-prints, pages arXiv–2110, 2021b.
Noor Sajid, P. Tigas, A. Zakharov, Z. Fountas, and Karl J. Friston. Exploration and
preference satisfaction trade-oﬀ in reward-free learning. ArXiv, abs/2106.04316, 2021c.
Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building
neural controllers. 1991.
Jürgen Schmidhuber. Simple algorithmic principles of discovery, subjective beauty, selective
attention, curiosity & creativity. In Discovery Science, 2007.
Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990–2010).
IEEE Transactions on Autonomous Mental Development, 2(3):230–247, 2010.
Roger Schrodinger, Erwin Schrödinger, and Erwin Schr Dinger. What is life?: With mind
and matter and autobiographical sketches. Cambridge University Press, 1992.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak
Pathak. Planning to explore via self-supervised world models. In International Conference
on Machine Learning, pages 8583–8592. PMLR, 2020.
Pranav Shyam, Wojciech Jaskowski, and Faustino Gomez. Model-based active exploration.
In ICML, 2019.
Satinder Singh, A. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning.
In NIPS, 2004.
International Workshop on Embodied Intelligence 2021
IOP Conf. Series: Materials Science and Engineering 1261  (2022) 012020
IOP Publishing
doi:10.1088/1757-899X/1261/1/012020
14
Satinder Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated
reinforcement learning. Technical report, MASSACHUSETTS UNIV AMHERST DEPT
OF COMPUTER SCIENCE, 2005.
Bradly C. Stadie, Sergey Levine, and P. Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. NIPS, 2015.
Russell K Standish. Open-ended artiﬁcial evolution. International Journal of Computational
Intelligence and Applications, 3(02):167–175, 2003.
Kenneth O Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge
you’ve never heard of. While open-endedness could be a force for discovering intelligence,
it could also be a component of AI itself, 2017.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven
reinforcement learning. Theory in Biosciences, 131:139–148, 2011.
Emanuel Todorov. General duality between optimal control and estimation. In 2008 47th
IEEE Conference on Decision and Control, pages 4286–4292. IEEE, 2008.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free rein-
forcement learning with linear function approximation. arXiv preprint arXiv:2006.11274,
2020.
John Winn, Christopher M Bishop, and Tommi Jaakkola. Variational message passing.
Journal of Machine Learning Research, 6(4), 2005.