Deriving time-averaged active inference from
control principles
Eli Sennesh1[0000−0001−7014−8471], Jordan Theriault1[0000−0002−4680−0172],
Jan-Willem van de Meent2[0000−0001−9465−5398], Lisa Feldman
Barrett1[0000−0003−4478−2051], and Karen Quigley 1[0000−0001−8844−990X]
1 Northeastern University, Boston MA 02115, USA
{sennesh.e,jordan theriault,l.barrett,k.quigley}@northeastern.edu
2 University of Amsterdam, 1090 GH Amsterdam, the Netherlands
j.w.vandemeent@uva.nl
Abstract. Active inference oﬀers a principled account of behavior as
minimizing average sensory surprise over time. Applications of active
inference to control problems have heretofore tended to focus on ﬁnite-
horizon or discounted-surprise problems, despite deriving from the inﬁnite-
horizon, average-surprise imperative of the free-energy principle. Here
we derive an inﬁnite-horizon, average-surprise formulation of active in-
ference from optimal control principles. Our formulation returns to the
roots of active inference in neuroanatomy and neurophysiology, formally
reconnecting active inference to optimal feedback control. Our formula-
tion provides a uniﬁed objective functional for sensorimotor control and
allows for reference states to vary over time.
Keywords: Hierarchical control · path-integral control · Inﬁnite-time
average-cost.
1 Introduction
Adaptive action requires the integration and close coordination of sensory with
motor signals in the nervous system. Active inference [17] provides one of the few
available unifying theories of sensorimotor control; it says that the nervous sys-
tem encodes both sensory and motor signals as aﬀerent predictions and reaﬀerent
prediction errors. Sensory predictions induce errors that can only be quashed by
updating the predictions, while motoric predictions induce errors that can be
quashed by simply moving the body to conform to the predicted trajectory [1].
The free energy principle, following the logic of active inference, says that organ-
isms maintain their self-organization as a whole over time by avoiding surprising
interactions between their internal and external environments [16]. This entails
maintaining bodily states within homeostatic ranges [41] by issuing sensory, pro-
prioceptive, and interoceptive predictions that minimize errors under a “prior
preference” [11] or “non-equilibrium steady-state” [19] density. Such a density
must be stationary throughout time.
arXiv:2208.10601v1  [eess.SY]  22 Aug 2022
2 E. Sennesh et al.
Early “non-equilibrium steady-state” formulations of active inference pro-
vided probability densities over full trajectories of movement and interaction
[19,20]. In regulatory terms, this corresponds to covariation of bodily states un-
der a “just enough, just in time” [54] mode of regulation that physiologists have
labelled homeostasis [8] with time-varying set points, rheostasis [36], and re-
cently allostasis [54,10,48,60]. A control theorist would call these trajectories or
set-points a reference trajectory or “reference signal” that a controller tries to
track. However, many more recent formulations of active inference use state-
space models with ﬁxed “prior preferences” that correspond to homeostatic set-
points or ranges [11]. They also typically employ either ﬁnite time horizons or
exponential discounting of expected free energy, unlike the original formulation
of active inference in terms of average surprise over time. A control theorist
would refer to these as reference states rather than reference trajectories.
This paper will rederive active inference as minimization of path-entropy
over an inﬁnite time horizon. The paper’s formulation will derive from the ﬁrst
principles of inﬁnite-horizon, average-cost optimal control; will allow preferences
to vary according to their own generative model, and will unify motor active
inference [1] (mAI) with decision active inference [52] (dAI). This will also unify
the computational principles behind motor active inference - the “equilibrium
point” [14,29] or “reference conﬁguration” [15] hypotheses - with the higher-level
study of sensorimotor behavior as optimal feedback control. Finally, the paper’s
formalism will provide a uniﬁed free energy functional for perception, motor
action, and decision making over time.
Section 2 will explain this paper’s notation and lay out an example gener-
ative model supporting the necessary features for the intended formulation of
active inference. Section 3 will summarize belief updating in generative models,
give a recognition model to match the generative model, describe the free en-
ergy principle for perceptual inference, and ﬁnish by describing active inference.
Section 4 will then extend active inference to the setting of an explicit reference
model prescribing behavior and give the control criterion corresponding to ac-
tive inference under the free energy principle. Section 5 will derive the resulting
free energy bounds whose optimization will yield a Bellman-optimal feedback
controller based on the generative and recognition models. Section 6 will discuss
related work; consider implementation issues for inﬁnite-horizon, average-cost
active inference; and conclude. Appendix A will provide derivations for equa-
tions that would otherwise have broken the ﬂow of the paper.
2 Preliminaries and notation
This paper will explain its formulation of active inference in terms of the discrete-
time graphical model in Figure 1. Like many generative models used to lay out
active inference [27,42], this model employs a hierarchy of temporal scales. We
number these timescales from the shortest to the longest, while numbering ran-
dom variables with discrete timestepst∈1 ...T from left to right. For simplicity,
we also restrict our graphical model to only three levels of hierarchy: observable
variables, fast latent variables, and slow random variables. Following those rules,
Deriving time-averaged active inference from control principles 3
t = 1, …, ∞
Increasing Timescale
Passage of Time
ota(1)
t
at
s(1)
ta(2)
t
s(2)
t s(2)
t + 2
s(2)
t + 1
s(1)
t + 1
Fig. 1.A hierarchical generative model we use as an example in this paper. Two
variables (s(1)
t ,s(2)
t ) denote unobserved latent states, and each a(k+1)
t parameterizes a
reference model for s(k)
t . ot represents observed sensory outcomes, and at represents
the feedback control actions generated by motor reﬂex-arcs.
observations ot and feedback motor actions at are 1-Markov; they “tick” at every
time-step. The fast latent variables s(1)
t and a(1)
t also change at every time-step.
At the next level up, slow latent variables s(2)
t and a(2)
t are 2-Markov; they
“tick” every second time-step t+ 2. We assume arbitrary state spaces for all
random variables, latent and observed, without any discrete or linear-Gaussian
assumptions about their conditional densities. Some evidence suggests [24] that
the brain may in fact represent time by learning a combination of frequencies in
the Laplace domain [51], and so the use of only three levels in the model should
not be taken to describe anything biological.
We write the combined latent states
s(1:2)
t = (s(1)
t ,s(2)
t )
and the “actions” or reference states
a(0:2)
t = (at,a(1)
t ,a(2)
t ).
We can therefore write the complete state at a time-step t as
st = (ot,s(1:2)
t ,a(0:2)
t ).
We will denote probability densities over actions as policies π and probability
densities in the generative model as pθ (with arbitrary parameters θ). The lowest
level of conditional probability densities then consists of
pθ(at,ot |a(1)
t ,s(1)
t ) = π(at |ot,a(1)
t )pθ(ot |a(1)
t ,s(1)
t ),
4 E. Sennesh et al.
the fast latent state level consists of
pθ(a(1)
t ,s(1)
t |s(1)
t−1,a(2)
t ,s(2)
t ,at−1) = π(a(1)
t |s(1)
t ,a(2)
t )pθ(s(1)
t |s(1)
t−1,s(2)
t ,at−1),
and the slow latent state level consists of
pθ(a(2)
t ,s(2)
t |s(2)
t−1,at−1) = π(a(2)
t |s(2)
t )pθ(s(2)
t |s(2)
t−1,at−1).
We write the complete state of the generative model pθ at a time-step twith its
associated conditional densities as
pθ(st |st−1) = pθ(at,ot |a(1)
t ,s(1)
t )pθ(a(1)
t ,s(1)
t |s(1)
t−1,a(2)
t ,s(2)
t ,at−1)
pθ(a(2)
t ,s(2)
t |s(2)
t−1,at−1), (1)
and the joint density over time (conditioned on a ﬁxed initial state s0) as
pθ(s1:T |s0) =
T∏
t=1
pθ(st |st−1). (2)
The model treats outcomes ot as observed, at as a feedback-driven motor action,
and other variables as latent. Inspired by the referent conﬁguration account of
motor control [15,30], the model treats a(1:2)
t as parameterizing “prior prefer-
ences” or referent conﬁgurations
R(st) = R(ot |a(1)
t )R(s(1)
t |a(2)
t ). (3)
at models the feedback control action of motor reﬂexes. a(1)
t parameterizes a ref-
erence state for ot. a(2)
t parameterizes a reference model for s(1)
t . Since reference
trajectories direct action, we consider their distributions to be policies
π(at,a(1:2)
t |ot,s(1:2)
t ) = π(at |a(1)
t ,ot)π(a(1)
t |s(1)
t ,a(2)
t )π(a(2)
t |s(2)
t ). (4)
s(2)
t , as the highest level latent state, has no reference model. In neuroscience,
it might correspond to predictive modeling at the highest level of the neuraxis
or cortical hierarchy [3,31,44]. In an engineering setting, it might contain both
environment and task state [37,46,56] or reward machine [25,7] state.
The likelihood pθ(ot |a(1)
t ,s(1)
t ) does not specify the reference model; it in-
stead provides the statistical grounding for both the latent states and the refer-
ence model parameters. The model here does not assume that reference densities
at all levels are prespeciﬁed or learned, but instead leaves that issue open.
We then designate as cost functions the surprisals over complete states (under
the reference model) and over observations (under the generative model)
J(st) = −log R(st), (5)
L(st) = −log pθ(ot |a(1)
t ,s(1)
t ). (6)
Deriving time-averaged active inference from control principles 5
pθ Probability density for the generative model
qφ Probability density for the recognition model
R Probability density for the reference model
π Policy density over actions and references
t Discrete time-step index
ot Observations
s(1:2)
t Unobserved model states
a(1:2)
t Parameters to a reference model R
at Control actions
st A complete model state for time t
Table 1.Random variable names used in this paper
Equation 5 equals the negative of the reward function used in distribution-
conditioned reinforcement learning [38] and can represent any control objective.
This paper will condition behavioral trajectories upon an initial state s0 as
context. This initial state corresponds to the beginning of a behavioral episode.
The following states from time 1 until time T, sampled from a generative model
with parameters θ, are then written as sampled from the joint density
s1:T ∼pθ(s1:T |s0).
This section has described a generative model and a decision objective under
which to formulate active inference. Table 1 summarizes the notation the rest
of the paper will use. The next section will lay out belief updating for the gen-
erative model, a recognition model to represent updated beliefs, and the free
energy principle for perceptual inference. Later sections will show how to extend
free-energy minimization to approximate a feedforward planner (in the gener-
ative model) and feedback controller (in the recognition model) that minimize
surprisal under the reference model.
3 Surprise minimization and the free energy principle
Section 2 gave a generative model and a way of writing arbitrary preferences as
probability densities. However, the formalism constructed so far would induce a
merely feedforward model-based planner, one which could not correct upcoming
movements in light of observations. Bayes’ rule speciﬁes how to update proba-
bilistic beliefs about unobserved variables in light of observations:
pθ(s(1:2)
1:t ,a(1:2)
1:t |o1:t,s0) = pθ(o1:t,s(1:2)
1:t ,a(1:2)
1:t |s0)
pθ(o1:t |s0) . (7)
The denominator of Equation 7 is called the marginal likelihood, and its negative
logarithm is the surprise under the generative model
h(o1:t) = −log pθ(o1:t |s0).
6 E. Sennesh et al.
Friston’s free energy principle [21] posits that a system, organism, or agent in a
changing environment preserves its structure against the randomness of its en-
vironment by embodying a generative model of its environment and minimizing
that model’s long-term average surprise
H(ot) = lim
T→∞
1
T −log pθ(o1:T |s0). (8)
In most generative models, neither the denominator of Equation 7 nor the sur-
prise of Equation 8 are analytically tractable, and Bayesian inference requires
approximation. Active inference in particular approximates optimal belief up-
dating by substituting a tractable recognition model qφ (with parameters φ) for
the posterior distribution
s(1:2)
1:T ,a(1:2)
1:T ∼qφ(s(1:2)
1:T ,a(1:2)
1:T |o1:T,a1:T,s0),
qφ(s(1:2)
1:T ,a(1:2)
1:T |o1:T,a1:T,s0) =
T∏
t=1
qφ(s(1:2)
t ,a(1:2)
t |ot,at,st+1,st−1).
This recognition model is conditioned on both the previous time-step t−1 and
the next time-step t+ 1, and can therefore perform retroactive belief updates.
To improve the recognition model’s approximation to the posterior distri-
bution, active inference entails evaluating and minimizing the variational free
energy (Equation 9, derivation in Proposition 1 in Appendix A)
Fθ,φ(t) = Eqφ
[
−log pθ(ot |a(1)
t ,s(1)
t )
]
+
DKL
(
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)∥pθ(s(1:2)
t ,a(1:2)
t |st−1)
)
. (9)
The free energy serves as a tractable upper bound to the surprise
H(ot) ≤Fθ,φ(t).
Intuitively, given an observation at each time-step t, minimizing the free energy
amounts to updating the beliefs of the recognition model qφ to approximate the
posterior distribution of the generative model pθ. A model-based controller can
then use those updated beliefs to revise or plan actions into the future. Active
inference has therefore often been formulated as using action to minimize this
free energy bound. Such a move then prompts the question of how to encode a
desirable reference trajectory into the generative model or another term of the
free energy bound [18]. The next section will deﬁne notions of surprise and free
energy that encode ﬁt to an explicitly speciﬁed reference trajectory.
4 Active inference with an explicit reference
Minimizing free energy ﬁts a model-based controller’s generative and recogni-
tion models to ongoing trajectories of observations. However, for the updated
Deriving time-averaged active inference from control principles 7
beliefs to determine action, the controller must use them to evaluate the ﬁt to
the reference trajectory (Equation 5) and emit motor actions. Fortunately, Thi-
jssen [58] gave an interpretation of probabilistic updating in terms of control:
the recognition model qφ acts as a state-feedback controller, for which the varia-
tional free energy becomes a running control cost. This section will show how to
evaluate ﬁt to the reference trajectory under the recognition model, and specify
the functional it must optimize to serve as a feedback controller.
The generative model in Section 2 and recognition model in Section 3 use
discrete time-steps and explicitly specify the “pathwise” reference model sepa-
rately from the generative and recognition models. The surprise to minimize is
therefore the long run average of the cross-entropy
H(qφ,R) = lim
T→∞
1
T
T∑
t=1
Est∼qφ [−log R(st)] . (10)
Equation 10 gives the long-term average surprise of using the reference model to
approximate the posterior beliefs of the recognition model. Replacing the refer-
ence model with the forward generative model would then amount to minimizing
the long-term average surprise (entropy); this generalization treats the reference
model as specifying a trajectory for the feedback controller to track.
Standard properties of free energy functionals imply that a desirable objective
functional would upper bound the sum of reference surprise and sensory surprise
H(R(st)) + H(ot) ≤J(t). (11)
Such a free energy functional would balance the reference model’s surprise (the
ﬁrst term) with the generative model’s surprise (the second term). In fact it can
be formed simply by adding Equation 10 to Equation 9
Jθ,φ(t) = H(qφ,R) + Fθ,φ(t) (12)
= Est∼qφ [J(st)] + Fθ,φ(t), (13)
and expanding the term for Equation 9 will yield a long-form expression
Jθ,φ(t) = Eqφ [J(st)] + Eqφ [L(st)] +
DKL
(
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)∥pθ(s(1:2)
t ,a(1:2)
t |st−1)
)
. (14)
Equation 14 gives an objective functional in terms of
– The reference surprisal under the recognition model,
– The observation surprisal under the recognition model, and
– The deviation of the recognition model from the generative model.
Neuroscientists [12,50] and ecologists [53] have found evidence that animals
optimize a global capture rate ¯Jin many decisions: rewards minus costs, divided
by time. Active inference modelers typically ground the construct of “reward”
8 E. Sennesh et al.
in reduction of surprise [35], and so a broad ﬁeld of evidence comes together
to support the time-averaging functional form implied by Bayesian mechanics
in both their “steady-state density” and “pathwise” formulations [45]. The next
section will therefore apply the principles of stochastic optimal feedback con-
trol for the partially observed setting and average-cost criterion, and solve the
resulting control problem to formulate active inference.
5 Deriving time-averaged active inference from optimal
control
The average-cost criterion for optimality entails minimizing the indeﬁnite sur-
prise rate with respect to the generative model pθ(s1:T |s0)
˜J(s0) = lim
T→∞
Epθ(s1:T|s0)
[¯Jθ,φ(s1:T)
]
. (15)
This minimization requires estimating Equation 15 for each behavioral episode
in context, a “global surprise rate” in terms of Jθ,φ(t)
¯Jθ,φ(s1:T) = 1
T
T∑
t=1
Jθ,φ(t). (16)
Plugging Equation 14 into Inequality 11 shows that minimizing Equation 16
will, by proxy, minimize the reference and sensory surprise in the context of a
sampled state trajectory s0:T. This estimation does not require a prespeciﬁed
episode length T, and can be performed under the generative model
¯Jθ,φ(s0) = Es1:T∼pθ(s1:T|s0)
[¯Jθ,φ(s1:T)
]
. (17)
Having estimates of Equation 17 will enable minimizing the mean-centered sur-
prise at each time-step
h(t; s0) = Jθ,φ(t) − ¯J(s0). (18)
The diﬀerential Bellman equation [59] deﬁnes optimal behavior as recursively
minimizing the mean-centered surprise at each time-step, or surprise-to-go
˜H∗(t; s0) = h(t; s0) + min
at
Est+1∼pθ(·|st)
[
˜H∗(t+ 1;s0)
]
. (19)
The minimization over actions in Equation 19 assumes a ﬁxed action space and
feedforward planning, which may result in very high-dimensional recursive opti-
mization problems. These assumptions also prove empirically, as well as compu-
tationally, problematic. Organisms are not born knowing all their aﬀordances [9];
they learn them [40]. Noise [13,32], uncertainty [23], and variability [47] are ubiq-
uitous in motor control, and so movement must be stabilized by online feedback.
Deriving time-averaged active inference from control principles 9
Stochastic optimal feedback control therefore requires an optimality principle
that allows for integrating observations between action steps. Rather than recur-
sively optimize individual actions, Equation 20 below therefore instead considers
optimality of the feedback-stabilized transition density
˜H∗(t; s0) = h(t; s0) + min
qφ
Est+1∼qφ(·|st)
[
˜H∗(t+ 1;s0)
]
. (20)
Equation 20 deﬁnes an optimal controller as one that achieves optimal state
transitions; individual actions act only as parameters to the optimal transition
density. These optimal state transitions take the form of a generative model
for agency, in which the generative model pθ(st+1 |st) produces feasible state
transitions and the Bellman optimality criterion “weighs” them according to
their surprise-to-go
q∗(st+1 |st) =
exp
(
−˜H∗(t+ 1;s0)
)
pθ(st+1 |st)
Est+1∼pθ(·|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]. (21)
The denominator of Equation 21 would typically correspond to the marginal
probability of an observation. Here it consists of the present state’s expected
surprise-to-go weight under the generative model. Potential future states that
lead to high surprise under the reference model will have high surprise-to-go and
therefore low weight under Equation 21. Present states that lead to states closely
ﬁtting the reference trajectory will have low surprise-to-go, resulting in a high
denominator that spreads weight around among possible future states.
The availability of a closed-form density for the optimal transition density
will help simplify the diﬀerential Bellman equation itself. Proposition 3 (in Ap-
pendix A) shows that by substituting Equation 21 into Equation 20 we can
obtain a path-integral expression for the optimal diﬀerential surprise-to-go with
both the feedforward controller pθ
˜H∗(s0) = −log Epθ(s1:T|s0)
[
exp
( T∑
t=1
(J(st) + L(st)) − ¯J(s0)
)]
, (22)
and the feedback controller qφ
˜H∗(s0) = −log Eqφ(s1:T|s0)
[
exp
( T∑
t=1
Jθ,φ(t) − ¯J(s0)
)]
. (23)
These equations employ “smooth” minimization rather than “hard” recursive
minimization, and so they support feedforward planning, feedback-driven up-
dating, and sensitivity of behavior to risk [57,39]. Jensen’s inequality will then
yield a tractable upper bound on the optimal diﬀerential surprise-to-go under
the feedback controller qφ
˜H∗(s0) ≤−Eqφ(s1:T|s0)
[ T∑
t=1
h(t; s0)
]
= ˜F∗
θ,φ. (24)
10 E. Sennesh et al.
Minimizing this diﬀerential free energy ˜F∗
θ,φ minimizes both the sensory sur-
prise and the optimal surprise-to-go function by proxy. This kind of information-
theoretic upper bound on a surprisal term is precisely what predictive coding
process theories [4,6] posit that the brain can optimize by updating θ and φ.
6 Discussion
Related work Our formulation follows in a tradition of unifying active inference
with optimal control approaches. Our hierarchical graphical model follows most
closely from the one featured by Friston [22] and Pezzulo [42] for hierarchical
active inference in decision making and motor control. In contrast to theirs, our
model includes only a single observation at the lowest hierarchical level rather
than one observed variable per level.
We also draw inspiration from information-theoretic control schemes not la-
belled by their authors as “active inference”. Piray and Daw [43] considered a
path-integral control approach to planning and reinforcement learning, which
they related to grid cells in the entorhinal cortex. Mitchell et al [34] modeled
motor learning as minimization of a free energy functional. Nasriany et al’s work
on distribution-conditioned reinforcement learning gave us our scheme for pa-
rameterizing reference distributions [38], and Sennesh et al [49] applied such an
objective to active inference modeling of interoception and allostatic regulation.
Implementations We employed the inﬁnite-horizon, average-surprise criterion to
ﬁt with the apparent time-averaging of dopamine signals in the brain [12,50], but
algorithms for this control criterion remain an active research area with no stan-
dard approach. A recent survey [28] showed that most software implementations
of active inference models still involve either ﬁnite horizons or exponential dis-
counting criteria. Those which do support inﬁnite horizons and nonlinear model
families mostly take algorithmic inspiration from reinforcement learning (RL).
In that domain, Tadepalli and Ok [55] published the ﬁrst model-based RL
algorithm for our criterion in 1998, while Baxter and Bartlett [5] gave a biased
policy gradient estimator. It took another decade for Alexander and Brown [2]
to give a recursive decomposition for average-cost temporal-diﬀerence learn-
ing. Zhang and Ross [61] have only recently published the ﬁrst adaptation of
“deep” reinforcement learning algorithms (based on function approximation) to
the average-cost criterion, which remains model free. Jafarnia-Jahromi et al [26]
recently gave the ﬁrst algorithm for inﬁnite-horizon, average-cost partially ob-
servable problems with a known observation density and unknown dynamics.
Conclusion This concludes the derivation of an inﬁnite-horizon, average-surprise
formulation of active inference. Since our formulation contextualizes behavioral
episodes, it only requires planning and adjusting behavior in context (e.g. from
timesteps 1 to T), despite optimizing a “global” (indeﬁnite) surprise rate (Equa-
tion 15). We suggest that this formulation of active inference can advance a
probabilistic approach to model-based, hierarchical feedback control [40,33].
Deriving time-averaged active inference from control principles 11
A Detailed derivations
This appendix provides detailed derivations for equations used elsewhere, par-
ticularly where doing so would have distracted from the ﬂow of the paper.
Proposition 1 (Variational free energy as divergence from an unnor-
malized joint distribution). The variational free energy (Equation 9) is de-
ﬁned as the Kullback-Leibler divergence of the recognition model qφ from the
unnormalized joint distribution of the generative model pθ
Fθ,φ(t) = DKL
(
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)∥pθ(st |st−1)
)
,
and therefore equals a sum of the cross entropy between the recognition model
and the sensory likelihood and the exclusive KL divergence from the recognition
model to the generative model over the latent variables
Fθ,φ(t) = Eqφ
[
−log pθ(ot |a(1)
t ,s(1)
t )
]
+
DKL
(
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)∥pθ(s(1:2)
t ,a(1:2)
t |st−1)
)
.
Proof. Taking a divergence between the (normalized) recognition model and the
(unnormalized) joint generative model will yield
Fθ,φ(t) = DKL
(
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)∥pθ(st |st−1)
)
= Eqφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)
[
−log pθ(st |st−1)
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)
]
= Eqφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)
[
−log pθ(ot |a(1)
t ,s(1)
t )pθ(s(1:2)
t ,a(1:2)
t |st−1)
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)
]
= Eqφ
[
−log pθ(ot |a(1)
t ,s(1)
t )
]
−Eqφ
[
log pθ(s(1:2)
t ,a(1:2)
t |st−1)
qφ(s(1:2)
t ,a(1:2)
t |ot,st+1,st−1)
]
,
as required.
Proposition 2 (KL divergence of the optimal feedback controller from
the feedforward controller). The exclusive Kullback-Leibler divergence of the
optimal feedback controller q∗ from the feedforward generative model pθ is
DKL (q∗(st+1 |st)∥pθ(st+1 |st)) = −Eq∗(st+1|st)
[
˜H∗(t+ 1;s0)
]
−
log Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
. (25)
Proof. We begin by writing out the deﬁnition of a KL divergence
DKL (q∗(st+1 |st)∥pθ(st+1 |st)) = Eq∗(st+1|st)
[
−log pθ(st+1 |st)
q∗(st+1 |st)
]
.
12 E. Sennesh et al.
The deﬁnition ofq∗in terms ofpθ (Equation 21) allows the inner ratio of densities
to simplify to
pθ(st+1 |st)
q∗(st+1 |st) = pθ(st+1 |st) (q∗(st+1 |st))−1
= ((((((pθ(st+1 |st)


Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
exp
(
−˜H∗(t+ 1;s0)
)
((((((pθ(st+1 |st)


pθ(st+1 |st)
q∗(st+1 |st) =
Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
exp
(
−˜H∗(t+ 1;s0)
) .
This simpliﬁed ratio therefore has the logarithm
log pθ(st+1 |st)
q∗(st+1 |st) = log Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
+ ˜H∗(t+ 1;s0)
and the divergence becomes
DKL (q∗(st+1 |st)∥pθ(st+1 |st)) =
−Eq∗(st+1|st)
[
˜H∗(t+ 1;s0)
]
−log Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
.
Proposition 3 (Path-integral expression for the optimal diﬀerential
surprise-to-go). The optimal diﬀerential surprise-to-go function deﬁned by the
Bellman equation (Equation 20)
˜H∗(t; s0) = h(t; s0) + min
qφ
Est+1∼qφ(·|st)
[
˜H∗(t+ 1;s0)
]
can be simpliﬁed by substituting in q∗ to obtain a path-integral expression
˜H∗(s0) = −log Epθ(s1:T|s0)
[
exp
( T∑
t=1
(J(st) + L(st)) − ¯J(s0)
)]
,
= −log Eqφ(s1:T|s0)
[
exp
( T∑
t=1
Jθ,φ(t) − ¯J(s0)
)]
.
Proof. Substituting Equation 21 into Equation 20 yields
˜H∗(t; s0) = ¯J(s0) −Jθ,φ(t) + Eq∗(st+1|st)
[
˜H∗(t+ 1;s0)
]
, (26)
whose recursive term is Eq∗(st+1|st)
[
˜H∗(t+ 1;s0)
]
. The divergence term in J
(Equation 14) will cancel this term. By Proposition 2 the divergence equals
DKL (q∗(st+1 |st)∥pθ(st+1 |st)) =
−Eq∗(st+1|st)
[
˜H∗(t+ 1;s0)
]
−log Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
.
Deriving time-averaged active inference from control principles 13
Substituting Equation 25 into Equation 14 will yield
−Jθ,φ(t) = Eq∗(st+1|st)
[
˜H∗(t+ 1;s0)
]
+log Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
+ Eqφ [−J(st)] + Eqφ [−L(st)] ,
whose ﬁrst term will cancel the recursive optimization when substituted into
Equation 26. The result will be a “smoothly minimizing” expression for the
optimal diﬀerential surprise-to-go
˜H∗(t; s0) = ¯J(s0) −(J(st) + L(st))
−log Epθ(st+1|st)
[
exp
(
−˜H∗(t+ 1;s0)
)]
,
and after unfolding of the recursive expectation, a path-integral expression for
the optimal diﬀerential surprise-to-go
˜H∗(s0) = −log Epθ(s1:T|s0)
[
exp
( T∑
t=1
(J(st) + L(st)) − ¯J(s0)
)]
.
Sampling a trajectory of states from a feedback controller qφ instead of the
feedforward planner pθ will then result in a nonzero divergence term
˜H∗(s0) = −log Eqφ(s1:T|s0)
[
exp
( T∑
t=1
Jθ,φ(t) − ¯J(s0)
)]
.
References
1. Adams, R.A., Shipp, S., Friston, K.J.: Predictions not commands: active inference
in the motor system. Brain Structure and Function 218(3), 611–643 (2013)
2. Alexander, W.H., Brown, J.W.: Hyperbolically discounted tempo-
ral diﬀerence learning. Neural Computation 22(6), 1511–1527 (2010).
https://doi.org/10.1162/neco.2010.08-09-1080
3. Barrett, L.F., Simmons, W.K.: Interoceptive predictions in the brain. Nature
Reviews Neuroscience 16(7), 419–429 (2015). https://doi.org/10.1038/nrn3950,
http://dx.doi.org/10.1038/nrn3950%5Cnhttp://dx.doi.org/10.1038/nrn3950%
5Cnpapers3://publication/doi/10.1038/nrn3950
4. Bastos, A.M., Usrey, W.M., Adams, R.A., Mangun, G.R., Fries, P., Friston, K.J.:
Canonical microcircuits for predictive coding. Neuron 76(4), 695–711 (2012)
5. Baxter, J., Bartlett, P.L.: Inﬁnite-horizon policy-gradient estimation.
Journal of Artiﬁcial Intelligence Research 15, 319–350 (Nov 2001).
https://doi.org/10.1613/jair.806
6. Bogacz, R.: A tutorial on the free-energy framework for modelling perception and
learning. Journal of mathematical psychology 76, 198–211 (2017)
7. Camacho, A., Icarte, R.T., Klassen, T.Q., Valenzano, R., McIlraith, S.A.: LTL
and beyond: Formal languages for reward function speciﬁcation in reinforcement
learning. In: IJCAI International Joint Conference on Artiﬁcial Intelligence. vol.
2019-Augus, pp. 6065–6073 (2019). https://doi.org/10.24963/ijcai.2019/840
14 E. Sennesh et al.
8. Carpenter, R.: Homeostasis: a plea for a uniﬁed approach. Advances in physiology
education 28(4), 180–187 (2004)
9. Cisek, P., Kalaska, J.F.: Neural mechanisms for interacting with a world
full of action choices. Annual Review of Neuroscience 33, 269–298 (2010).
https://doi.org/10.1146/annurev.neuro.051508.135409
10. Corcoran, A.W., Hohwy, J.: Allostasis, interoception, and the free energy princi-
ple: Feeling our way forward. In: The Interoceptive Mind: From homeostasis to
awareness, pp. 272–292. Oxford University Press (2019)
11. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active in-
ference on discrete state-spaces: a synthesis. Journal of Mathematical Psychology
99, 102447 (2020)
12. Daw, N.D., Touretzky, D.S.: Behavioral considerations suggest an average reward
td model of the dopamine system. Neurocomputing 32, 679–684 (2000)
13. Faisal, A.A., Selen, L.P., Wolpert, D.M.: Noise in the nervous system. Nature
reviews neuroscience 9(4), 292–303 (2008)
14. Feldman, A.G.: Once more on the equilibrium-point hypothesis ( λ model)
for motor control. Journal of Motor Behavior 18(1), 17–54 (1986).
https://doi.org/10.1080/00222895.1986.10735369
15. Feldman, A.G.: Referent control of action and perception (2015).
https://doi.org/10.1007/978-1-4939-2736-4
16. Friston, K.: The free-energy principle: a uniﬁed brain theory? Nature reviews neu-
roscience 11(2), 127–138 (2010)
17. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active in-
ference: a process theory. Neural computation 29(1), 1–49 (2017)
18. Friston, K., Samothrakis, S., Montague, R.: Active inference and agency: opti-
mal control without cost functions. Biological Cybernetics 106(8–9), 523–541 (Oct
2012). https://doi.org/10.1007/s00422-012-0512-8
19. Friston, K., Stephan, K., Li, B., Daunizeau, J.: Generalised ﬁltering. Mathematical
Problems in Engineering 2010 (2010)
20. Friston, K.J., Daunizeau, J., Kiebel, S.J.: Reinforcement learning or active infer-
ence? PloS one 4(7), e6421 (2009)
21. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behav-
ior: A free-energy formulation. Biological Cybernetics 102(3), 227–260 (2010).
https://doi.org/10.1007/s00422-010-0364-z
22. Friston, K.J., Rosch, R., Parr, T., Price, C., Bowman, H.: Deep temporal models
and active inference. Neuroscience and Biobehavioral Reviews 77(April), 388–402
(2017). https://doi.org/10.1016/j.neubiorev.2017.04.009, citation Key: Friston2017
23. Gallivan, J.P., Chapman, C.S., Wolpert, D.M., Flanagan, J.R.: Decision-making
in sensorimotor control. Nature Reviews Neuroscience 19(9), 519–534 (2018)
24. Howard, M.W.: Formal models of memory based on temporally-varying represen-
tations. In: The new handbook of mathematical psychology, Volume 3. Cambridge
University Press (2022)
25. Icarte, R.T., Klassen, T.Q., Valenzano, R., McIlraith, S.A.: Using reward machines
for high-level task speciﬁcation and decomposition in reinforcement learning. In:
35th International Conference on Machine Learning, ICML 2018. vol. 5, pp. 3347–
3358 (2018)
26. Jafarnia-Jahromi, M., Jain, R., Nayyar, A.: Online learning for unknown partially
observable mdps. In: Proceedings of the 25th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS). vol. 151, p. 21. Proceedings of Machine
Learning Research, Valencia, Spain (2022)
Deriving time-averaged active inference from control principles 15
27. Kiebel, S.J., Daunizeau, J., Friston, K.J.: A hierarchy of time-scales
and the brain. PLOS Computational Biology 4(11), 1–12 (11 2008).
https://doi.org/10.1371/journal.pcbi.1000209, https://doi.org/10.1371/journal.
pcbi.1000209
28. Lanillos, P., Meo, C., Pezzato, C., Meera, A.A., Baioumy, M., Ohata, W., Tschantz,
A., Millidge, B., Wisse, M., Buckley, C.L., Tani, J.: Active inference in robotics
and artiﬁcial agents: Survey and challenges (arXiv:2112.01871) (Dec 2021), http:
//arxiv.org/abs/2112.01871, arXiv:2112.01871 [cs]
29. Latash, M.L.: Motor synergies and the equilibrium-point hypothesis. Motor Control
14(3), 294–322 (2010). https://doi.org/10.1123/mcj.14.3.294
30. Latash, M.L.: Physics of Biological Action and Perception. Academic Press (2019).
https://doi.org/10.1016/C2018-0-04663-0
31. Livneh, Y., Sugden, A.U., Madara, J.C., Essner, R.A., Flores, V.I., Sugden,
L.A., Resch, J.M., Lowell, B.B., Andermann, M.L.: Estimation of Current and
Future Physiological States in Insular Cortex. Neuron 105(6), 1094–1111.e10
(2020). https://doi.org/10.1016/j.neuron.2019.12.027, https://doi.org/10.1016/j.
neuron.2019.12.027
32. Manohar, S.G., Chong, T.T.J., Apps, M.A., Batla, A., Stamelou, M., Jarman, P.R.,
Bhatia, K.P., Husain, M.: Reward pays the cost of noise reduction in motor and
cognitive control. Current Biology 25(13), 1707–1716 (2015)
33. Merel, J., Botvinick, M., Wayne, G.: Hierarchical motor control in
mammals and machines. Nature Communications 10(1), 1–12 (2019).
https://doi.org/10.1038/s41467-019-13239-6, http://dx.doi.org/10.1038/
s41467-019-13239-6
34. Mitchell, B.A., Lauharatanahirun, N., Garcia, J.O., Wymbs, N., Grafton, S., Vet-
tel, J.M., Petzold, L.R.: A minimum free energy model of motor learning. Neural
computation 31(10), 1945–1963 (2019)
35. Morville, T., Friston, K., Burdakov, D., Siebner, H.R., Hulme, O.J.: The homeo-
static logic of reward. bioRxiv p. 242974 (2018)
36. Mrosovsky, N.: Rheostasis: the physiology of change. Oxford University Press
(1990)
37. Nasiriany, S., Lin, S., Levine, S.: Planning with Goal-Conditioned Policies. In:
Advances in Neural Information Processing Systems. No. NeurIPS (2019)
38. Nasiriany, S., Pong, V.H., Nair, A., Khazatsky, A., Berseth, G., Levine, S.: DisCo
RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Poli-
cies. In: IEEE International Conference on Robotics and Automation (2021),
http://arxiv.org/abs/2104.11707
39. Pan, Y., Theodorou, E.A.: Nonparametric inﬁnite horizon Kullback-Leibler
stochastic control. IEEE SSCI 2014 - 2014 IEEE Symposium Series on Com-
putational Intelligence - ADPRL 2014: 2014 IEEE Symposium on Adaptive
Dynamic Programming and Reinforcement Learning, Proceedings 2(2) (2014).
https://doi.org/10.1109/ADPRL.2014.7010616
40. Pezzulo, G., Cisek, P.: Navigating the Aﬀordance Landscape: Feedback Control as
a Process Model of Behavior and Cognition. Trends in Cognitive Sciences 20(6),
414–424 (2016). https://doi.org/10.1016/j.tics.2016.03.013, http://dx.doi.org/10.
1016/j.tics.2016.03.013
41. Pezzulo, G., Rigoli, F., Friston, K.: Active inference, homeostatic regulation and
adaptive behavioural control. Progress in neurobiology 134, 17–35 (2015)
42. Pezzulo, G., Rigoli, F., Friston, K.J.: Hierarchical active inference: A the-
ory of motivated control. Trends in Cognitive Sciences 22(4), 294–306 (2018).
https://doi.org/10.1016/j.tics.2018.01.009
16 E. Sennesh et al.
43. Piray, P., Daw, N.D.: Linear reinforcement learning in planning, grid ﬁelds, and
cognitive control. Nature communications 12(1), 1–20 (2021)
44. Quigley, K.S., Kanoski, S., Grill, W.M., Barrett, L.F., Tsakiris, M.: Functions
of Interoception: From Energy Regulation to Experience of the Self. Trends
in Neurosciences 44(1), 29–38 (2021). https://doi.org/10.1016/j.tins.2020.09.008,
https://doi.org/10.1016/j.tins.2020.09.008
45. Ramstead, M.J., Sakthivadivel, D.A., Heins, C., Koudahl, M., Millidge, B.,
Da Costa, L., Klein, B., Friston, K.J.: On bayesian mechanics: A physics of and
by beliefs. arXiv preprint arXiv:2205.11543 (2022)
46. Ringstrom, T.J., Hasanbeig, M., Abate, A.: Jump operator planning:
Goal-conditioned policy ensembles and zero-shot transfer. arXiv preprint
arXiv:2007.02527 (2020)
47. Scholz, J.P., Sch¨ oner, G.: The uncontrolled manifold concept: identifying control
variables for a functional task. Experimental brain research126(3), 289–306 (1999)
48. Schulkin, J., Sterling, P.: Allostasis: a brain-centered, predictive mode of physio-
logical regulation. Trends in neurosciences 42(10), 740–752 (2019)
49. Sennesh, E., Theriault, J., Brooks, D., van de Meent, J.W., Barrett, L.F., Quigley,
K.S.: Interoception as modeling, allostasis as control. Biological Psychology 167,
108242 (2021)
50. Shadmehr, R., Ahmed, A.A.: Vigor: Neuroeconomics of movement control. MIT
Press (2020)
51. Shankar, K.H., Howard, M.W.: A scale-invariant internal representation of time.
Neural Computation 24(1), 134–193 (2012)
52. Smith, R., Ramstead, M.J., Kiefer, A.: Active inference models do not contra-
dict folk psychology. Synthese 200(2) (2022). https://doi.org/10.1007/s11229-022-
03480-w, https://doi.org/10.1007/s11229-022-03480-w
53. Stephens, D.W., Krebs, J.R.: Foraging theory. In: Foraging theory. Princeton uni-
versity press (2019)
54. Sterling, P.: Allostasis: a model of predictive regulation. Physiology & behavior
106(1), 5–15 (2012)
55. Tadepalli, P., Ok, D.K.: Model-based average reward reinforcement learning.
Artiﬁcial Intelligence 100(1–2), 177–224 (1998). https://doi.org/10.1016/s0004-
3702(98)00002-2
56. Tang, Y., Kucukelbir, A.: Hindsight Expectation Maximization for Goal-
conditioned Reinforcement Learning. In: Proceedings of the 24th International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS). vol. 130 (2021),
http://arxiv.org/abs/2006.07549
57. Theodorou, E.: Relative entropy and free energy dualities: Connections to path
integral and kl control. 2012 IEEE 51st IEEE Conference p. 1466–1473 (2012)
58. Thijssen, S., Kappen, H.J.: Path integral control and state-dependent feedback.
Physical Review E - Statistical, Nonlinear, and Soft Matter Physics 91(3), 1–7
(2015). https://doi.org/10.1103/PhysRevE.91.032104
59. Todorov, E.: Eﬃcient computation of optimal actions. Proceedings of the National
Academy of Sciences of the United States of America106(28), 11478–11483 (2009).
https://doi.org/10.1073/pnas.0710743106
60. Tschantz, A., Barca, L., Maisto, D., Buckley, C.L., Seth, A.K., Pezzulo,
G.: Simulating homeostatic, allostatic and goal-directed forms of intero-
ceptive control using active inference. Biological Psychology 169, 108266
(2022). https://doi.org/https://doi.org/10.1016/j.biopsycho.2022.108266, https://
www.sciencedirect.com/science/article/pii/S0301051122000084
Deriving time-averaged active inference from control principles 17
61. Zhang, Y., Ross, K.W.: On-policy deep reinforcement learning for the average-
reward criterion. In: Proceedings of the 38th International Conference on Machine
Learning. p. 11 (2021)