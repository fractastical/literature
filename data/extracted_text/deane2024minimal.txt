Minimal phenomenal experience and the
synthetic data hypothesis
George Deane
Daphne Demekas
September 30th, 2024
Appearances represent the play of creative energy or inexhaustible potentiality
of Awareness (rigpa). They are not “mind” as in the Chittamartin view, but
rather they are manifestations of mind, something constructed by mind out of
the raw material of sense data... Whatever may arise, appearing as external
phenomena to the individual, is merely one’s own internal state of existence
manifesting externally, that is to say, it is merely the potentiality or creative
energy of Awareness (rigpa) becoming visible to the individual.
— John Myrdhin Reynolds,The Golden Letters
1 Abstract
Minimal Phenomenal Experience (MPE) refers to the simplest form of conscious
experience, characterized by the absence of time, self, and sensory content, where
only pure awareness or consciousness remains. In this paper, we present a com-
putational neurophenomenological account of minimal phenomenal experience
within the active inference framework. We propose a generative mechanism
that allows active inference agents to leverage capacities akin to those in gen-
erative adversarial networks, which use a dual framework of data generation
and discrimination to improve perceptual accuracy. Our hypothesis posits that
the brain continuously generates synthetic data to fill perceptual and cognitive
gaps, and we provide a computational mechanism within the active inference
framework to support this. We explore how synthetic data generation, when
amplified by meditation, psychedelics, or other altered states, can the brain to
‘defabricate’ entrenched conceptual structures and approach pure consciousness
at the limit. We provide simulations of an active inference agent embedded in
a virtual environment, we demonstrate that recursive loops of synthetic data
generation can lead to a breakdown in conceptualization — ’seeing as’ — while
enhancing the system’s ability to predict its own observation stream. We argue
that this model can characterize the essential features of MPE, including con-
tentlessness, epistemic openness, and the unique valence profile associated with
minimal phenomenal experiences.
1
2 Introduction: minimal phenomenal experience
and computational neurophenomenology
Minimal phenomenal experience (MPE) refers to a hypothesized state of con-
sciousness characterized by the absence of specific content or qualia, yet retaining
a basic form of subjective awareness [64]. Metzinger (2020) [30] defines MPE
as "a form of conscious experience characterized by the absence of temporal
self-location, of spatiotemporal self-location, and of multimodal object percep-
tion" (p. 7). This definition emphasizes the lack of typical features associated
with ordinary waking consciousness, such as a sense of self, spatial and temporal
orientation, and perception of distinct objects or sensory qualities. Instead,
MPE is conceptualized as a bare form of awareness or "pure consciousness" that
may underlie all other forms of conscious experience. The study of MPE is
particularly intriguing because it potentially points to the most fundamental
aspects of consciousness. By stripping away all specific contents and features,
MPE might reveal the essential nature of consciousness itself, providing crucial
insights into what consciousness fundamentally is and how it arises [? ][30] [64].
The concept of MPE has been explored in various contexts, including medi-
tation practices, certain sleep states, and altered states of consciousness. For
instance, studies on lucid dreamless sleep have provided empirical support for
the existence of states that might qualify as MPE [64]. In these states, subjects
report being aware of their consciousness without any specific content or sense
of self. As described by one participant in a study by Windt et al. (2015): "I
am aware of an absence of dream imagery or any other specific content. I know
I exist, but there’s nothing else - no thoughts, no dreams, no perceptions. Just a
bare awareness of being" (p. 875).
MPE has also been associated with certain meditative states, particularly
those described in contemplative traditions as "pure awareness" or "consciousness
itself" [56]. These states are characterized by a non-dual awareness that tran-
scends the subject-object dichotomy typically present in ordinary consciousness.
For example, in Tibetan Buddhism, the state of "rigpa" or pristine awareness
is described as a contentless, non-conceptual state of pure knowing [22]. This
aligns closely with the concept of MPE, as both emphasize a fundamental form
of awareness devoid of specific contents or subject-object distinctions.
Studying minimal phenomenal experience presents unique challenges. As
Bachmann and Metzinger (2020) [2] note, the very nature of MPE as a content-
less state makes it difficult to study using traditional methods of consciousness
research, which often rely on reportable content. This has led to the development
of novel approaches, including neurophenomenological methods that combine
rigorous first-person reports with neuroimaging techniques [26]. Understanding
MPE has significant implications for theories of consciousness, potentially pro-
viding insights into the basic structure of phenomenal experience and the neural
correlates of consciousness. It challenges content-based theories of consciousness
and suggests that awareness itself might be more fundamental than specific
conscious contents [22].
2
While these features describe the "purest" form of MPE, actual experiences
may vary. As Gamma and Metzinger’s [13] study showed, experiences of "pure
awareness" can include a range of phenomenological features, some of which (like
thoughts or sensory perceptions) seem to contradict the core definition of MPE.
This variability suggests that MPE may be best understood as a prototypical
state that real experiences approximate to varying degrees.
To further illustrate the nature of MPE, consider this description from a
participant in Gamma and Metzinger’s study: "It’s absolutely alive to itself,
fully conscious, and it’s conscious of itself as No-thing, colorless, featureless,
totally empty and totally full. The Space is conscious of itself at the near end,
but it has no personal characteristics" (p. 11). This description captures many
of the key features of MPE: the sense of pure consciousness, the absence of self
and specific content, and the paradoxical quality of being both empty and full.
In essence, MPE can be understood as consciousness in its most basic form -
a state of pure awareness without object, self, or spatio-temporal location. It
represents the hypothetical "ground state" of consciousness, upon which all more
complex conscious experiences are built.
3 Computational phenomenology
Computational phenomenology, a burgeoning field at the intersection of phe-
nomenology and computational neuroscience, represents a novel approach to
bridging the gap between first-person lived experience and third-person scientific
accounts of consciousness. This interdisciplinary endeavor aims to formalize
phenomenological descriptions using generative modeling techniques developed
in computational neuroscience and biology [45]. Building upon earlier efforts
to naturalize phenomenology [39] [47], computational phenomenology extends
the neurophenomenological project initiated by Varela [59] by leveraging recent
advancements in generative modeling [10] [18]. At its core, this approach seeks to
construct computational models of the inferential or interpretive processes that
best explain various kinds of lived experience, without necessarily committing to
specific metaphysical assumptions about the nature of consciousness [? ]. By
casting phenomenological constitution as a form of Bayesian or variational infer-
ence, computational phenomenology offers a promising framework for creating
what Varela (1997) [60] termed "generative passages" between phenomenological
and neuroscientific descriptive registers.
Thisapproachleveragestheframeworkofgenerativemodeling, whichhasbeen
successfully applied in various domains of cognitive science and neuroscience [11]
[1]. In the context of computational phenomenology, the ’data’ to be explained
by these generative models are the first-person phenomenological experiences
themselves, often described as ’hyletic data’ in Husserlian terms [20]. The
process of phenomenological constitution, or the disclosure of things to conscious
experience, is modeled as an inferential process whereby subjects move from raw
sensory data to interpreted lived experiences (noema) [? ]. This formalization
allows researchers to construct computational models that can generate the
3
structure and dynamics of specific types of lived experience, providing a novel
way to bridge phenomenological descriptions with formal mathematical and
computational frameworks [38] [63].
One of the most exciting aspects of computational phenomenology is its poten-
tial to bridge the gap between the contents of consciousness and their functional
roles, a crucial step in the broader project of naturalizing the mind. By formaliz-
ing phenomenological descriptions using generative models, this approach allows
researchers to link the qualitative aspects of lived experience with computational
processes that may underlie them [? ]. This bridging function is particularly
evident in the way computational phenomenology models phenomenological
constitution as a form of inference, aligning with predictive processing theories
of cognition [7] [51]. For instance, the experience of perceiving an object can
be modeled as an inferential process that generates predictions about sensory
inputs, potentially explaining both the phenomenal content and the functional
role of perception in guiding behavior [36]. Similarly, computational models of
metacognition and self-awareness [49] offer a way to understand how reflective
consciousness might emerge from and contribute to cognitive control processes.
This approach also resonates with embodied and enactive views of cognition [61]
[57], as it can model how conscious experiences arise from and guide an organism’s
interactions with its environment [44]. By providing a formal framework that
can accommodate both the phenomenological richness of conscious experience
and its potential computational underpinnings, computational phenomenology
offers a promising path towards a more integrated, naturalized understanding of
the mind, addressing long-standing challenges in consciousness research [6]and
contributing to the broader project of bridging phenomenology with cognitive
science [12].
In this paper, we examine minimal phenomenal experience through the lens
of the ‘synthetic data hypothesis’, which posits that the brain continuously
generates synthetic data for itself. We argue that this hypothesis not only aligns
with phenomenological insights but also offers functional advantages. To ground
our model in both phenomenology and its functional capabilities we model a
functional active inference agent embedded in an virtual environment. This
agent engages in ‘seeing as’ interpreting objects according to its internal states,
preferences, and environmental contexts. We investigate the effects of synthetic
data generation, demonstrating that while some synthetic data generation is
functional in some contexts, too much is dysfunctional. We show how recursively
looping synthetic data generation can lead to the defabrication of conceptual
structures and a breakdown in ‘seeing as’. At the extreme, this defabrication
results in a state we propose is analogous to the contentless nature of minimal
phenomenal experience. Finally, we connect our model and simulation to the
phenomenology of minimal phenomenal experience, offering insights into its
underlying mechanisms.
4
4 The synthetic data hypothesis
The synthetic data hypothesis is the hypothesis that the brain is constantly
generating its own synthetic data for the process of inference and learning.
According to the view, the brain consistently uses its own generative model to
generate synthetic data that it can use for perceptual filling in, rapid perceptual
recognition and categorization, imagination, offline planning and mental time
travel.
This section will briefly outline what synthetic data is in the context of
artificial intelligence and machine learning, before specifying the mechanism of
synthetic data generation within the active inference framework. We will then
consider its implications and with simulations show effects of recursive synthetic
data generation on inference, looking both at advantages and implications via the
mathematics and simulations. Finally, we will close the section by arguing that
the effects of psychedelics in active inference should best be understood as an
artefact of the synthetic data generation, and that this is consonant with existing
accounts of how psychedelics work computationally in the brain — namely the
REBUS model and the entropic brain [5], [4] .
4.1 Synthetic data in AI and machine learning
Synthetic data in artificial intelligence (AI) refers to artificially generated infor-
mation that mimics the statistical properties and characteristics of real-world
data. This approach has gained significant traction in AI and machine learn-
ing applications, offering solutions to challenges such as data scarcity, privacy
concerns, and the need for diverse training sets [66]. Synthetic data is often gen-
erated using advanced machine learning techniques like Generative Adversarial
Networks (GANs) or Variational Autoencoders (VAEs) [15] [23].
One crucial application of synthetic data in AI is addressing missing or
incomplete information. AI systems can use synthetic data to fill in gaps in real-
world datasets, enabling more robust and complete analyses. This is particularly
valuable in fields like computer vision or speech recognition, where incomplete
data is common but excluding or ignoring missing information could lead to
erroneous interpretations [34]. However, this approach also carries risks, such as
the potential introduction of artificial patterns or relationships that don’t exist
in the real world, which could lead to overfitting or erroneous conclusions.
The human brain faces similar challenges in processing sensory information
and may employ analogous strategies to those used in AI systems with synthetic
data. The brain often encounters incomplete or noisy sensory inputs but is
remarkably adept at constructing coherent perceptions. This ability suggests
that the brain might be generating its own form of synthetic data to fill in
gaps and maintain a consistent representation of the environment. Part of the
plausibility of the synthetic data hypothesis comes from phenomenology: for
instance, although our fovea only provides high-resolution vision for a small
portion of our visual field (approximately 2 degrees of visual angle), we perceive
the entire visual scene as clear and detailed [27]. This "grand illusion" of panoptic
5
vision can be explained by the brain’s ability to generate synthetic data to fill
in the gaps in our perception [35]. Similarly, the synthetic data hypothesis
accounts for our lack of awareness of the blind spot in our visual field [43] and
our susceptibility to change blindness, where significant changes in a visual scene
go unnoticed [53] [46].
The phenomenology makes sense from a functionality perspective. For
instance, in visual processing, the brain routinely deals with occlusions, motion,
and variable lighting conditions. Filling in missing information based on context
and prior knowledge, in a process similar to how AI systems might use synthetic
data to complete partial inputs [25], leads to a more complete picture of the
current context and circumstance. The phenomenon of perceptual filling-in,
where the brain completes missing parts of the visual field, such as the blind spot,
is a prime example of this capability [37]. This process extends beyond simple
gap-filling; the brain actively constructs our visual experience, inferring depth,
shape, and even predicting motion trajectories of partially obscured objects
[50]. In auditory processing, the brain demonstrates a similar ability, filling in
gaps in speech or music, known as the ‘phonemic restoration effect’ [62]. These
phenomena suggest that the brain is constantly generating its own "synthetic"
sensory data to maintain a coherent and continuous perception of the world. This
is analogous to how AI systems might use synthetic data to maintain consistent
performance in the face of incomplete or noisy inputs.
In normal functioning, observations that are more incongruent with other
states of the system either propagate through the system changing beliefs in a
manner similar to a global workspace (Dehaene, 2001), or are drowned out and
revised inline with dominant (precise) beliefs in the system (i.e. are not granted
access to the workspace). Each cycle of synthetic data generation enables the
system to further resolve conflict across the system (illustrative simulations
removed due to space constraints). This can be functional, for example in
detecting conflict between different beliefs or in ’seeing as’ which we will describe
in a later section.
Overreliance on synthetic data perceptual can lead to illusions or hallucina-
tions, analogous to how AI systems might generate false patterns when overfitting
to synthetic data [16] For example, in the famous McGurk effect, visual informa-
tion about lip movements can override auditory perception, causing people to
"hear" sounds that were never actually presented [28]. In more extreme cases,
conditions like Charles Bonnet syndrome showcase how the brain, deprived of
normal visual input, can generate complex hallucinations, essentially creating ex-
tensive synthetic visual experiences [9]. These phenomena parallel the challenges
faced in AI, where models trained on synthetic data may hallucinate features or
relationships that don’t exist in reality. For instance, in image recognition tasks,
AI systems have been known to fixate on textures rather than object shapes,
leading to misclassifications that would seem absurd to humans [14].
The synthetic data hypothesis is closely connected to Gershman’s (2019)
notion of a generative adversarial brain, which posits that the brain uses an
internal generator to simulate possible sensory inputs and a discriminator to
distinguish between real and generated data, thereby improving perceptual
6
accuracy and learning efficiency. Gershman argues that not only is the generative
adversarial brain phenomenologically plausible, but that the model also provides
a novel perspective on how the brain might implement approximate Bayesian
inference, overcoming limitations of explicit density models that can be difficult
to learn and compute with. Gershman suggests that the adversarial framework
allows for more efficient inference compared to standard Monte Carlo methods
and more flexibility than typical variational approaches. He also argues that
this model aligns well with existing theories of cortical function, mapping onto
feedforward and feedback pathways in the brain [14].
4.2 Psychedelics and the synthetic data hypothesis
The synthetic data hypothesis aligns with existing models of psychedelic action
in active inference and neuroscience — such as the REBUS model ("Relaxed
Beliefs Under Psychedelics") [4] and the entropic brain hypothesis [5] — while
suggesting a different mechanism of action. The REBUS model suggests that
psychedelics work by relaxing the brain’s high-level priors, allowing bottom-up
sensory information to flow more freely and loosening the grip of top-down
constraints. Meanwhile, the entropic brain hypothesis, posits that psychedelics
increase the overall entropy, or disorder, in the brain’s activity, leading to a more
flexible and less rigid mental state. However, unlike these models, the synthetic
data hypothesis does not treat the relaxation of priors or entropy as a direct
cause but rather as a byproduct of the excess synthetic data generation the brain
engages in under psychedelics.
Incorporating synthetic data into active inference highlights how psychedelics,
under different conditions, can both relax and strengthen prior beliefs, causing
both vivid phenomenology and at higher doses a breakdown in the conceptual
structures that scaffold experience. This includes an account of ‘cognitive pene-
tration’, where higher-level abstractions influence lower-level sensory perceptions
— akin to Google DeepDream (see Figure 1). This perspective can also accommo-
date extensions like the SEBUS model [48] and points toward a more dynamic
interplay between levels of abstraction and hypothesis strength.
4.3 The active inference framework
The section will introduce the active inference framework. At the heart of the
synthetic data hypothesis is the generative model, a mathematical construct
that is central to active inference, and the means in which an active inference
agent infers its internal states and actions given its sensory inputs [11]. The
generative model represents the relationship between the hidden causes of sensory
observations (such as environmental states) and the actual observations or sensory
data. By leveraging this model, an agent can infer the most likely hidden causes
of its sensory experiences and choose actions that optimize its future interactions
with the environment. Essentially, the generative model allows the agent to
minimize uncertainty, or "free energy", by continuously updating its beliefs about
the world based on incoming sensory information and its prior expectations [3].
7
Figure 1: Google DeepDream enhances patterns in images by recursively feeding
the output of the neural network back into itself to amplify patterns and features
in the image with each iteration. According to the synthetic data hypothesis,
psychedelic phenomenology may works by a similar mechanism: recursive loops
generating self-fullfilling observation streams.
In the context of active inference, the generative model is a probabilistic model
that describes how observations are generated by hidden states. State estimation
in this model has two key components: the prior beliefs an agent holds about the
hidden states (either pre-defined or generated via the transition distribution from
the previous time-step) and the likelihood of observing certain sensory inputs
given those hidden states. The prior, denoted asP(s), represents the probability
distribution over the hidden statess before new observations are made, capturing
the agent’s expectations based on past experience or knowledge. The likelihood,
P(o|s), specifies the probability of receiving particular sensory observationso,
given specific hidden statess. These two components are combined through
Bayesian inference to form the posterior distributionP(s|o), which updates the
agent’s estimate of the hidden state based on both its prior beliefs and the new
sensory evidence.
The generative model in active inference leverages the likelihood distribution
to handle multi-modal input by encoding how each modality relates to the hidden
states. Crucially, different modalities can be conditioned on different subsets
of the full state space, allowing for a flexible and efficient representation of the
environment.
Formally, leto = {o1, . . . , oM } denotethesetofobservationsacross M sensory
modalities, and s = {s1, . . . , sF } represent the hidden state factorized intoF
state factors. The likelihood for each sensory modalityom, wherem indexes
different sensory channels, is combined multiplicatively under the assumption of
conditional independence between modalities given the states.
8
In this framework, each observation modalityom can be conditioned on any
joint distribution over some combination of state factors. Letsm ⊆ s denote
the subset of state factors that them-th modality depends on. Then, the full
likelihood can be expressed as:
P(o|s) =
MY
m=1
P(om|sm), where sm = {sj, . . . , sk} ⊆ {s1, . . . , sF } (1)
This formulation allows for great flexibility in specifying the generative
model, and it also is an intuitive way of conceptualizing how any agent may
learn associations between subsets of factorized states and observations without
having to condition every observation modality on the joint distribution over
all states in the state space. In particular, this formulation naturally captures
the idea that some sensory modalities might be irrelevant for inferring certain
aspects of the environment, as reflected in the choice ofsm for each modality.
In practice when building simulations, this likelihood structure is often
represented using multidimensional arrays or tensors, where the dimensions
correspond to the relevant state factors for each modality. The exact form of the
observation likelihood depends on the specific generative process being modeled,
and can range from simple categorical distributions to more complex continuous
distributions. In Section 6 we will present an example of a simulation using
Categorical distributions, implemented with pymdp [17].
4.4 The mechanism of synthetic data generation in active
inference
The process of synthetic data generation under the active inference framework
involves three key steps:
1. Initial State Estimation: The system first receives an observation or set of
observations from its available modalities and performs state estimation.
This involves computing the posterior distribution over possible states,
Q(s), based on the incoming observations. This posterior reflects the
system’s belief about the states given the data. This is a normal step in
active inference, and is understood as perceptual inference [54].
2. Synthetic Observation Generation: Using the inferred posteriorQ(s), the
system combines it with its likelihood function,P(o|s), to generate new,
synthetic observations. These synthetic observations are created as if
they were real sensory inputs and are based on the generative model’s
assumptions about how states produce observations.
3. State Estimation on Synthetic Data: The system then processes these
synthetic observations in the same manner as real data, performing state
estimation again. This results in an updated posterior distributionQ(s),
9
Figure 2: Conceptual diagram of synthetic data generation. Top: Regular
state inference in active inference. Given an observation from the generative
process, (often a discrete sample), the agent infers a posterior over states given
its observation likelihoodA and transition likelihoodB. Middle: Synthetic data
generation. The agent uses its inferred posterior over hidden states to generate
a distribution over observations most likely to be generated by the current state
posterior. Bottom: The agent iteratively updates its posterior over states given
generated synthetic data. With more iterations, the agent will effectively ‘dream’
online, iteratively generating synthetic data which may or may not align with
environmental observations.
which reflects the system’s updated belief about its states given the syn-
thetic observations. This process can be repeated multiple times, with each
cycle generating new synthetic observations from the posterior distribution.
These synthetic observations feed back into the system, allowing it to
continually update its state estimates.
As the loop repeats, several outcomes may occur. The system might begin to
converge on particular states, reinforcing them with each iteration. This repeated
feedback loop can cause certain states to dominate, similar to the pattern
amplification observed in techniques like Google DeepDream. This process may
explain why perception becomes highly suggestible under psychedelics, where
certain sensory states are continuously reinforced.
In more complex generative models, however, repeated iterations can lead to
10
destabilization of these initial convergences. As the system loops over synthetic
observations, reinforced states may eventually break down, leading to what can
be described as "defabrication." This occurs when the system loses coherence in
state estimation, potentially due to overfitting or divergence in the feedback loop.
This breakdown might explain why prolonged feedback cycles in perception or
cognition can lead to increasingly distorted or chaotic representations of reality.
When it comes to generating synthetic data, we start with a particular
posterior over states,P(s|o) and then we multiply that with the observation
likelihood P(o|s) as follows
P(osynthetic) =
X
s
P(oreal|s)P(s|oreal) (2)
When using categorical distributions, we can accomplish this with a simple
dot product operation
P(o) =A · s (3)
where A = P(o|s) and s = P(s|o).
This results in a probability distribution over expected observations, which
are the most likely observations given the currently inferred posterior over states.
If the observation likelihood is configured in a way where there are sparsities
in the associations between observation modalities and state factors, then this
distribution can be indicative of interesting associations that the model can make
between its current state estimation and other possible observations from the
environment.
This section has introduced a particular sense of synthetic data generation.
It is worth noting that another form of synthetic data generation is already
a crucial part of the active inference framework in the expected free energy
calculation, as the expected states (contingent on actions) are used to generate
expected observations (via the likelihood) in order to calculate the expected free
energy of different action policies. However, in existing active inference models
and implementations, this synthetic data is never actually observed by the agent,
it is only used algorithmically in the calculation of expected free energy.
4.5 Seeing as
According to Buddhist philosophy, fabrication (sa˙ nkh¯ ara) is understood as the
construction of conscious perception and experience that arises due to some
form of contraction or grasping at how one is perceiving experience [42]. This
fabrication process is responsible for creating our lived phenomenal experience,
including our sense of self and the world around us. Thanissaro [55] explains that
fabrication is the construction of conscious perception and experience that arises
due to some form of contraction or grasping at how one is perceiving experience.
Buddhism holds that all experience is transient, and that all phenomena are
fabricated [67].
11
Figure 3: Synthetic data generation, given one observation modality and one
state factor. The grayscale in the posterior represents the probability values -
darker colors represent high probability. The observation from the environment is
a one-hot distribution, which creates an inferred state posterior. Then, through
synthetic data generation, the agent generates a distribution over observations
from the inferred posterior over states. Given this, the agent can update its
posterior over states. Repeated iterations can lead to further departures from
the original observations and the the corresponding posterior over states. The
evolution of these distributions throughout recursive loops of synthetic data
generation are be most significant in generative models that include multiple
observation modalities, state factors, and sparse conditional dependencies. For
an extended diagram including multiple observation modalities and state factors
see Figure 4.
Defabrication, on the other hand, is described as a progressive deconstruction
process of releasing grasping and identification with one’s perceptual experience
[? ]. It involves a gradual process of letting go of perceptual construction, where
coarse or ’sticky’ fabrications are progressively released, resulting in more subtle
modes of perceiving experience [? ]. This process leads to a stilling of fabrications
and an increase in equanimity, which is the capacity to remain non-reactive yet
open to what arises in experience.
Aspect seeing is a concept from philosophy, particularly associated with
Ludwig Wittgenstein, that refers to the phenomenon where the way we perceive
something can change depending on how we interpret it [65]. This shift in
perception often occurs when we see an image or object in one way, and then
suddenly notice a different aspect of it, which alters our understanding or
perception of that object.
Mooney images [32], are a compelling example of how "seeing as" operates in
perception. These two-tone images, which present objects or scenes using only
12
Figure 4: The process of synthetic data generation and synthetic state estimation
when we have sparsities in the observation likelihood. In beige, we have a
observation likelihood mapping observation modalityo(1) to state factors(1),
such thato(1) is only predictive ofQ(s(1)). Similarly, in green, we have that
o(2) is only predictive ofQ(s(2)). However, the third observation modalityo(3),
pictured in purple, is predictive of a joint distribution overQ(s(1), s(2)). Because
of this, when the agent performs synthetic generation, the distribution over
Q(o(3)) will be informed by both state posteriors, each of which are inferred
based on evidence from multiple observation modalities. This factorization of
the observation likelihood is common in active inference and likely accounts for
efficient learning and generalization, and it can become ever-more complex as
the world model representation grows. Given these sparsities, new associative
information will be discovered throughout iterations of synthetic date, which
could in principle drastically change the agent’s posterior beliefs about hidden
states of the world.
black and white without intermediate shading, are often difficult to interpret
at first glance. Recognition of the object in a Mooney image typically requires
top-down processing, where prior knowledge or contextual information plays a
key role in making sense of the visual input. Before recognition, viewers struggle
to identify what the image represents, but once recognized, the image becomes
clear, and the perception of it changes permanently. This shift embodies the
essence of aspect seeing—once you "see as" or recognize the object, it is difficult
to revert to a state where the object is unrecognized.
Factorized conditional dependencies in the observation likelihood can be
closely linked with ‘aspect seeing’ or ‘seeing as’, and fabrication. Conditioning
observations across joint distributions over states can change the way that things
are perceived under particular other contexts (conditioned on other state factors),
13
Figure 5: Unless you’ve seen this image before, it will likely look formless like
black and white splodges. Once you see that it’s a cow, you can’t unsee it.
and generating synthetic data to uncover associations between the possibilities
of other observations and the current posterior over states can shed light on the
process of fabrication (and defabrication) in the perceiver.
Predictive processing and active inference are well-placed to conceptualize
fabrication in Buddhist philosophy, as the mental constructions are understood
to be driven by the needs of the organism, rather than a direct reflection of reality
[19], [51], [52], [52]. By considering how fabrication occurs, we can consider how
synthetic data generation leads to the extreme end of defabrication.
In alignment with various schools of Buddhism, we associate the extreme end
of defabrication as coextensive with pure awareness. In the following section,
we illustrate this notion with a simulation of an agent capable of ‘aspect seeing’
or ‘seeing as’ — where contextual factors modulate both what it sees, and the
motivational salience or what it ’means’ to the agent [8].
5 Simulation
In this section we illustrate the concept of ‘seeing as’ in the active inference
framework. We present a simulation of an active inference embedded into a grid
world environment. The agent is in the environment with two objects, and its
task is to navigate to the desirable object, and avoid the aversive object, given
the current context.
The agent is endowed with a generative model that allows it to navigate via
motion actions which are moving forwards and backwards, and rotating. The
state factors of the generative model are distance, angle, object type, and context
(see Table 1). Crucially in this model, the mapping between the observation
of the object features and the hidden state of object type are conditioned on a
context hidden state. This represents the phenomenon of ‘seeing as’, because
the agent will infer the type of a particular object given its beliefs about the
current context.
The agent is motivated by its preference distributionC = P(o) to take
14
actions that are most likely to transition it into states maximize the probability
of observing the preferred observations. The observation likelihood that maps
high valence observations to the hidden states of object types are also conditioned
on the context hidden state (as well as distance and angle). This leads to the
situation where both the inferred object type, and whether that object type is
desirable, depends on the given context. Here, both categorization of the object,
and the motivational salience of the object are baked into the state inference.
Figure 6: Here we show the agent in a 10x10 grid world (note that we can
arbitrarily increase the size of the world without changing the patterns in the
results). The agent has a restricted visual field (in blue) and is accompanied
by two objects - visualized as a circle and a square. The agent must infer the
type of the objects when observing them, in order to navigate whether or not to
approach the object or steer away. Whether the object is desirable will depend
on its type, and its type will depend on the given context, which is received as
an observation from the generative process.
During the simulation, when the agent is in a position in the grid world
where an object is visible, the agent will observe its distance to that object,
its angle to the object (within its visual field) as well as an observation that
corresponds to the object features. The agent also observes, at all times, an
observation of the current context. During state estimation, the agent infers
the corresponding distances and angles, as well as the object type (given the
observed object features), and the hidden state of context. Then, in action
15
inference, the agent trades off the utility and epistemic gain of taking particular
actions in order to try to generate the highest valence observation, which will
be the action that brings it closer to what it believes to be the desirable object
type in the given context. The context in the generative process switches every
20 timesteps.
Variable Name Notation
Hidden State factors s = {sdistance
j , sangle
j , stype
j , scontext}
Observations o = {odistance
j , oangle
j , ofeatures
j , ovalence
j , ocontext
j }
Actions u = {umotion, urotation}
Observation Model P(ot = i|s= j) = [A]ij
Transition Model P(st+1 = i|st = j, ut = k) = [B]ijk
Reward C = P(ovalence) = [1, 0]
Table 1: Generative model variables and notation. The subscript j in the
observations and states shows that we have one of these observations per object
in the grid.
Table 1 outlines the overall structure of the generative model, and Figure 7
provides visualizations of the observation likelihoods, the transition likelihoods
and the preference distribution. The agent’s likelihoods are configured such that
in context 1, the agent is likely to believe that the object of type 1 is desirable,
such that it has high probability of observing high valence when it is in close
proximity to that object, and that being in proximity to object of type 2 is
undesirable (high probability of low valence).
Similarly, in context 2, the agent is likely to believe that being in close
proximity to object of type 2 lends evidence to high valence, and being in close
proximity to object of type 1 lends evidence to negative valence.
Note that this valence observation is never generated by the generative
process, it is purely an expected observation. In reality, the generative process
always sends the agent a null valence observation. Given a model of how hidden
states are likely to lead to preferred observations, the agent is able to navigate its
world without needing to actually observe the reward from the environment at
all. Next, we consider what happens when we include synthetic data generation
loops within state inference.
Below we will show the results for three different cases of this simulation.
In the first case, the agent navigates the world with regular active inference;
doing state inference and active inference, without doing explicit inference on
16
Figure 7: On the left, we show the observation likelihoods. The agent has a
noisy observation likelihood over distance, angle, and context observations, and
no information about states given the null observations. The object modality
observation likelihoodAfeatures (written here asAid) is conditioned on context.
The observation likelihood over the valence observationAvalence is different for
the two object types. On the right we have the transition likelihoods for motion
and rotation, and the preference over valence observations, which shows positive
preference to observe high valence and negative preference to observe low valence.
generated synthetic data. We show that in this case, the agent is able to navigate
the environment and accumulate reward over time. The agent is generally more
certain about the type of the object when it is in that object’s given "desirable"
context (since it will be navigating towards it, and meanwhile uncertain about
the identity of the other.
In the second case, we perform synthetic data generation after a context-
switch. Here, we show that the synthetic data generation is useful for the agent
because it allows for more rapid updating of its beliefs about the new context,
given the incoming observations, and by doing so it is able to take better actions
more efficiently and accumulate more reward over its lifetime. This is a showcase
of the functionality of synthetic data generation in times of uncertainty.
In the final case, we perform synthetic data generation for 30 iterations
every 15 timesteps throughout the simulation. By doing these longer runs of
synthetic data generation throughout the simulation, the agent loses its ability
to differentiate object types, simulating defabrication or breakdown in aspect
seeing. Doing this makes the agent converge on high precision beliefs about both
object types, without being able to actually infer their true nature. Even so, the
agent has low variational free energy, and high accuracy. Figure 8 shows the
overall trends in reward, variational free energy and the entropy of the posterior
17
distribution over the object type hidden statesstype over time. Figure 9 shows
the variational free energy and accuracy during the process of synthetic data
generation.
5.1 Case 1
The first case does not include any synthetic data generation (other than that
which is generated in the process of the expected free energy calculation). This
means that the agent is never actually observing any of its imagined observations,
which include observations carried through from the previous time-step (about
where the object is relative to the agent after having rotated or moved) as well
as the valence observations. Thus, when the agent is not looking at the object
(it is not in its field of view, as shown in Figure 6) then the agent observes null
observations for that particular object. The agent can still carry through some
information about the object’s distance and angle from its prior via its transition
distributions, but in general, more time looking away from the object leads to
less knowledge about the object’s type. In Figure 8(a), the posterior entropies
over stype fluctuate during context switches, because when the agent is confident
about context, it will be nearby the desirable object and therefore more likely to
have it in its field of view, and less likely to have the other object in its field of
view (therefore will "forget" what type it is over time, leading to a high entropy
posterior).
5.2 Case 2
When we perform synthetic data generation after context switches, the agent
is able to keep generating "internal" observations of the objects that it has
previously looked at, even after it has stopped observing them. This can help
reinforce the beliefs about the changing object types and helps the agent navigate
to the newly preferred location. We can see in Figure 8(b) how the agent is
able to accumulate more reward during its lifetime through observing its own
imagined observations of distance, angle, context, object features and valence,
after a context switch. This also leads to lower variational free energy (more
accurate state estimation).
Interestingly, nearingtheendofthesimulation, thestateposteriorsoverobject
types end up with quite high entropy after the final context switch, suggesting
that the periodic synthetic data generation may be leading to uncertainty in
the agent’s state estimation. We can also see that after this context switch the
reward begins to taper off. This suggests that repeated synthetic data generation
may be begin to simulate defabrication.
5.3 Case 3
In the final case, we perform recursive synthetic data generation for 30 iterations
every 15 timesteps throughout the lifespan of the agent. This periodic and
drawn-out inference over imagined observations leads the agent to lose the ability
18
to differentiate between objects. The agent is continuously observing its own
internal observations and is no longer able to make correct inferences about
the observations it receives from the environment. Importantly, no part of the
generative model (observation likelihoods, transition likelihoods, preferences)
are changing during this process. It is purely the synthetic data being observed
in place of the generative process that drives the agent to essentially get stuck
and lose the ability to differentiate, converging on high entropy in the posterior
over states for the object type. Recall, variational free energy is composed
of an accuracy term, which measures how well a model’s predictions match
observations, and a complexity term, which penalizes the model for being overly
complex or diverging from prior beliefs. Despite the fact that the agent can
no longer infer the nature of its observations or navigate towards a preferable
location, it is predicting its own internal observations with high accuracy (and
low complexity) and therefore its variational free energy drops to almost 0 -
lower than it did in either of the other cases. This is an interesting finding
which speaks to the ‘contentless’ states of "pure awareness" in which, through
repeatedly observing internal observations and treating them as information,
beings can have high accuracy predictions without having any fabricated model
of the world.
While Figure 8 shows results for the agent interacting with the environment
(with and without synthetic data generation interspersed throughout), we also
measured the variational free energy and the accuracy of state estimation during
the recursive synthetic data generation, when the state estimation that the
agent is doing is on its own imagined observations. We can see in Figure 9
that throughout the synthetic data generation process, variational free energy
is decreasing, and accuracy1 is increasing, showing that the phenomenological
experience of this ‘looping’ itself is satisfying the agent’s ultimate perceptual
goal. This process of losing the ability to differentiate between states while
decreasing variational free energy can be indicative of defabrication as a result
of the inability to ‘see as’.
6 The phenomenology of MPE
The most essential phenomenological characteristics of pure consciousness, as
described in Thomas Metzinger’sThe Elephant and the Blind, revolve around the
direct experience of awareness itself, without the usual contents of perception,
thoughts, or emotions. Space constraints mean we can only give a concise
overview of these characteristics:
Timelessness: In MPE, there is no experience of time passing or of being
located at a particular moment in time. The usual sense of past, present, and
future dissolves. Metzinger (2020) notes that MPE involves "the absence of
1Accuracy here is the accuracy term of the variational free energy, which is essentially
computing an expected log-likelihood under a variational distribution. Negative values are
normal and expected in this context. They typically indicate that the current estimate of the
variational distribution results in a low likelihood of the data.
19
Figure 8: We ran 50 trials of 100 time-steps each and we show the average
behaviour over trials. Here, reward increments by 1 whenever the agent is on
the location of the desirable object given the context, and decrements by 1
whenever the agent is on the location of the undesirable object given the context.
The desirable object begins as type 1 in blue, and switches every 20 timesteps
(context switches are shown by the dotted vertical lines). (a) In regular active
inference, the reward is increasing over time as the agent successfully navigates
to its desired object. VFE is mostly steady and slightly decreasing over time.
When the context switches at timestep 20, the agent navigates towards the newly
desirable location, leading to a change in entropy instype. (b) Here we perform
synthetic data generation for 5 iterations slightly after the context changes. This
allows for more precise posteriors and overall allows it to better navigate the
world and accumulate more reward. (c) Here we perform recursive synthetic data
generation for 30 iterations every 15 time steps, leading to the agent no longer
being able to correctly navigate its environment, converging on high entropy
posteriors, and small variational free energy.
temporal self-location" (p. 7).
Spacelessness: Along with the absence of temporal experience, MPE also
lacks spatial self-location. There is no sense of being in a particular place or
having a body located in space. The experience is often described as "unbounded"
or "infinite."
Contentlessness: In its purest form, MPE lacks any specific sensory, emo-
tional, or cognitive content. There are no thoughts, no feelings, no perceptions
of the external world. As Metzinger (2020) puts it, MPE is characterized by
"the absence of multimodal object perception" (p. 7).
20
Figure 9: Free energy and accuracy during the process of recursive synthetic
data generation. Here we show 30 time-steps from one of the various periods of
synthetic data generation throughout the simulation in case 3, described above.
We show that VFE decreases and accuracy increases over time.
Non-dual Awareness: MPE often involves a sense of non-duality, where
the usual distinction between subject and object, or self and world, dissolves.
This is reflected in descriptions of "pure awareness" or "consciousness itself" in
contemplative traditions.
Luminosity or Clarity:Many reports of MPE include descriptions of a
quality of "luminosity" or "clarity." This is not visual brightness, but rather a
sense of pristine awareness or knowing.
Epistemicity: Despite the absence of specific content, MPE involves a
form of "knowing" or awareness. This is often described as a non-conceptual,
immediate form of cognizance. Metzinger (2020) refers to this as "the mere
phenomenal experience of knowing" (p. 26).
Peace and Silence:MPE is associated with a profound sense of peace,
silence, or stillness.
Ineffability: This experience is often reported as beyond words, difficult to
describe, and ineffable.
Selflessness, timelessness, spacelessness, contentless and non-duality aligns
with the view of pure awareness in terms of defabrication : the system is unable
to constrain or structure experience in the usual way. The usual ‘seeing as’
which permeates consciousness perception is recedes. Not only are sensory
perceptions unable to be categorized, but the system is without even the most
fundamentalconstructionssuchaslocationinspaceandtime, orthesenseofbeing
a boundaried self. Pure consciousness is the end of this defabrication process,
whereby even the deepest and invariant fabrications structuring experience —
including the experience of being a self — are lost through the deconstruction of
‘seeing as’.
In our simulation, recursive synthetic data generation causes the system to, for
a period, entirely generate its own observations. The result is convergence: after
several rounds of synthetic data generation insulated from outside observations,
the system comes to perfectly predict its own sensory data, for the simple reason
that it is endogenously generated: low variational free energy here stems from
high accuracy (due to the data being generated by the inferred states), and low
complexity. This is true even when the predictions are maximally entropic: even
21
where there is mere possibility of conceptual representation, the system is able
to converge into highly accurate predictions of its own observations.
This is key when considering the defining characteristic of MPE: epistemic
openness. In The Elephant and the Blindthe notion of epistemic openness is
described as a specific form of openness related to a non-conceptual experience
of the pure potentiality of knowing. It is the experience of the potential to know
without any active exertion of knowledge-seeking. This openness represents a
space of potentiality rather than actualized knowledge, characterized by the
implicit capacity to orient oneself in space, time, and self-awareness, but without
doing so. Epistemic openness is about non-conceptually representing the possibil-
ity of an experience, as mere epistemic capacity. “What you actually experience
is a potentiality plus a sense of confidence.” p43 [31]. This concept is closely
related to traditional notions of "emptiness" in Buddhism, where emptiness is
not a void but a space of open possibilities lacking in definitive boundaries or
objects.
In this context, epistemic openness refers to the system’s orientation toward
all potential states and configurations it could assume, rather than being confined
to a specific, fixed representation of any particular content. The system, poised
at the edge of possibility rather than actuality, holds within it the capacity to
generate content without having to fully specify or commit to any particular
instantiation. This state emerges when the system’s synthetic data generation
no longer produces fully-formed, concrete representations. Instead, as the
system continues through recursive loops of synthetic data generation, there
is a dissolution of representational content while the observation stream itself
becomes increasingly predictable. The system’s representational capacity shifts
from producing actual content to embodying the potential for content: the
system is still actively generating possibilities, but these possibilities do not
crystallize into fixed objects, thoughts, or experiences.
Turningupthesyntheticdataloopingprocess—eitherbymeditation, psychedelics,
or other means—can be understood as a means by which the system comes into
contact with its own generative profile. As the system continually creates and
interacts with its self-generated content, it deepens its understanding of itself as
a field of generative possibility. This can be understood as a form of opacification
of the modelas a model [29].
This space of potentiality to generate content without specification—content
as possibility or latent potential rather than actuality—is identified with the
MPE. In this state, the system is in a condition of knowing, but what it knows is
the potential for experience, rather than any specific experience itself. The view
is concordant with other notions of ’richness’ and ’presence’ in the literature
that equate richness with entropy with the notions of counterfactual richness
and ’conscious presence’. For example Seth (2014, 2015) has argued, ability
to encode a rich repertoire of counterfactual predictions—connecting potential
actions to their sensory consequences—is crucial for the feeling of presence. In
a state of pure awareness, the system, though in a minimal state, still holds
this richness because it contains within it the potential to resolve into particular
forms of knowledge. A pure consciousness state, then, can be entirely simple
22
while being rich in virtue of its potentiality to collapse into particular states.
Our results also align with a recent paper by Ji et al. (2024) that characterizes
the richness of conscious experience using information theory, characterizing it
as the entropy or amount of information present in a conscious state. Richness
of a conscious experience can be quantified as the diversity of possible states
the system could occupy, with higher entropy reflecting a larger repertoire of
potential experiences or mental states. This aligns with the an MPE on the one
hand being simultaneously extremely simple and also extremely rich.
Ji et al. (2024) conceptualize also ineffability as the information lost when
attempting to translate the rich, high-dimensional content of conscious experi-
ences into verbal or symbolic representations. Using information theory, they
argue that conscious states contain far more information than can be captured
by language, leading to ineffability as a result of this compression. Essentially,
ineffability reflects the gap between the complexity of conscious experience and
the limitations of communication. This makes sense of the ineffability of MPEs
as described by our model: while they may be very simply characterized and
lacking in conceptual content, they can still be very rich due to high entropy,
and as such ineffable due to an inability to compress this highly entropic state
into a representational format
The question arises as to how to account for the valenced nature of these
experiences. Many reports of MPE describe it as characterized by a profound
sense of peace, silence, stillness. There are states of consciousness that are
perhaps best described as being minimal phenomenal experience but involve
rapid transformation and can be extremely valenced. 5-MeO-DMT induces
experiences characterized by the dissolution of the self, the absence of time,
and the complete void of sensory or conceptual content, while simultaneously
eliciting intense, highly valenced states of bliss or awe. Users often report a
profound sense of unity or boundless presence, accompanied by overwhelming
positive (and sometimes negative) emotions, despite the lack of typical conscious
experiences or egoic awareness.
Within the active inference literature, valence is frequently associated with
the ’rate’ or ’slope’ of variational free energy reduction [24] [58] [21] Joffily and
Coricelli (2013) present a formal model linking emotional valence to the free-
energy principle. Emotional valence is defined as the negative rate of change of
free-energy over time, meaning that positive emotions correspond to a reduction
in free-energy (better prediction of sensory inputs), while negative emotions
arise when free-energy increases (worse prediction). The dynamic regulation of
emotional valence is linked to how agents adjust their learning rates in response to
environmental changes, reflecting the role of emotions in modulating the agent’s
adaptation to unexpected stimuli. If these views are on the right track, then
the sometimes extremely high emotional valence found in minimal phenomenal
experiences such as the 5-MeO-DMT experience could be explained by the rapid
rate of variational free energy reduction through sustained recursive synthetic
data looping (see Figure 9). The ‘luminosity and clarity’, characteristic of MPEs,
can be accounted for by the fact that in our simulation we observed a reduction
in the variational free energy over the looping cycle. A natural consequence of
23
repeated synthetic data looping is that the system comes to perfectly predict its
generated data.
This mirrors Mumford’s (1991) idea of an "ultimate stable state," where
top-down predictions perfectly match lower-level cortical predictions [33]. Picard
and Friston (2014) propose that this state may explain the ecstatic experiences
observed in certain epileptic phenomena, particularly in cases of insular epilepsy,
where patients experience an absence of prediction errors, leading to a state of
clarity and oneness with the world [41]. This idea is expanded in Picard (2024),
suggesting that the interruption of the insula’s predictive coding mechanism
could give rise to mystical or ecstatic states, where the brain perfectly predicts
its interoceptive states, resulting in a sensation of bliss and mental clarity [40].
In our model, this stable state is explained by the fact that the observations are
directly caused by its own predictions, and as a result, get less and less surprising
through iteration.
7 Conclusion and future directions
This paper has advanced a view that the brain is likely to be routinely generating
synthetic data in the aid of inference, learning, and action selection. We provided
computational mechanisms as to how this is likely to work in the active inference
framework through recursive synthetic data generation, and computational
simulations of how this idea can aid our understanding of minimal phenomenal
experience. A picture of experience emerges as an inherently generative ‘creative
energy’ of ‘inexhaustible potentiality’, to echo the opening quote of the paper.
While this is not apparent in usual experience, this quality becomes ‘visible to
the individual’ in minimal phenomenal experience.
This is very much preliminary work, opening up several research directions.
The model we provided gives a proof of principle, and we aim to expand it
to more representative neurocomputational models of conscious experiences,
including hierarchies and models of the self, to provide more clarity on the
exact dynamics and etiology of different experiences. Our account is explicitly
targeted at states of miminal phenomenal experience states of consciousness;
meaning localized experiences of consciousness without content, as opposed to
modes; more pervasive, global ways of being conscious that dominate an entire
experience.
Future work will look at exactly how synthetic data generation relates to
mental constructions, and the ways in which it may lead to their defabrications.
For instance, we may consider more complex and hierarchical generative models,
where synthetic data generation can be passed through layers of abstractions,
from sensorimotor fast frequency levels to slower frequency mental and self-
construction levels, which connect back through to the sensorimotor level -
such that the agent is always observing its imagined self-observations in all
sensorimotor experience. This could form a rich ground to model defabrication
on deeper layers of abstraction.
24
References
[1] J. Ashburner and K. J. Friston. Computing average shaped tissue probability
templates. Neuroimage, 18(3):713–719, 2003.
[2] T. Bachmann and T. Metzinger. Minimal phenomenal experience: The
temporal dynamics of phenomenal states without conscious self, content,
and task-related processing.Philosophy and the Mind Sciences, 1(I):1–24,
2020.
[3] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth. Active infer-
ence, sensory attenuation and illusions.PLoS computational biology, 13(8):
e1005740, 2017.
[4] R. L. Carhart-Harris and K. J. Friston. Rebus and the anarchic brain:
toward a unified model of the brain action of psychedelics.Pharmacological
reviews, 71(3):316–344, 2019.
[5] R. L. Carhart-Harris, R. Leech, P. J. Hellyer, M. Shanahan, A. Feilding,
E. Tagliazucchi, D. R. Chialvo, and D. Nutt. The entropic brain: a theory
of conscious states informed by neuroimaging research with psychedelic
drugs. Frontiers in human neuroscience, 8:20, 2014.
[6] D. J. Chalmers. Facing up to the problem of consciousness.Journal of
consciousness studies, 2(3):200–219, 1995.
[7] A. Clark. Whatever next? predictive brains, situated agents, and the future
of cognitive science.Behavioral and brain sciences, 36(3):181–204, 2013.
[8] G. Deane. Consciousness in active inference: Deep self-models, other minds,
and the challenge of psychedelic-induced ego-dissolution.Neuroscience of
Consciousness, 2021(2):niab024, 2021.
[9] D. H. Ffytche. Visual hallucinations and the charles bonnet syndrome.
Current psychiatry reports, 7(3):168–179, 2005.
[10] K. Friston. A free energy principle for a particular physics.arXiv preprint
arXiv:1906.10184, 2019.
[11] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo.
Active inference: a process theory.Neural computation, 29(1):1–49, 2017.
[12] S. Gallagher and D. Zahavi.The phenomenological mind. Routledge, 2020.
[13] A. Gamma and T. Metzinger. An empirical study of pure consciousness
experiences in meditation.Philosophy and the Mind Sciences, 2:1–40, 2021.
[14] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann,
and W. Brendel. Imagenet-trained cnns are biased towards texture; in-
creasing shape bias improves accuracy and robustness. arXiv preprint
arXiv:1811.12231, 2019.
25
[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio. Generative adversarial nets. InAdvances in
neural information processing systems, pages 2672–2680, 2014.
[16] R. L. Gregory. Knowledge in perception and illusion.Philosophical Trans-
actions of the Royal Society of London. Series B: Biological Sciences, 352
(1358):1121–1127, 1997.
[17] C. Heins, B. Millidge, D. Demekas, B. Klein, K. Friston, I. Couzin, and
A. Tschantz. pymdp: A python library for active inference in discrete state
spaces. arXiv preprint arXiv:2201.03904, 2022.
[18] C. Hesp, M. Ramstead, A. Constant, P. Badcock, M. Kirchhoff, and K. Fris-
ton. A multi-scale view of the emergent complexity of life: A free-energy
proposal. Biological Reviews, 96(6):2447–2465, 2021.
[19] J. Hohwy.The predictive mind. Oxford University Press, 2013.
[20] E. Husserl. Ideas: General introduction to pure phenomenology. Routledge,
2012.
[21] M. Joffily and G. Coricelli. Emotional valence and the free-energy principle.
PLoS computational biology, 9(6):e1003094, 2013.
[22] Z. Josipovic. Nondual awareness: Consciousness-as-such as non-
representational reflexivity.Progress in brain research, 244:273–298, 2019.
[23] D. P. Kingma and M. Welling. Auto-encoding variational bayes.arXiv
preprint arXiv:1312.6114, 2013.
[24] J. Kiverstein, M. Miller, and E. Rietveld. The feeling of grip: novelty, error
dynamics, and the predictive brain.Synthese, 196:2847–2869, 2019.
[25] H. Komatsu. The neural mechanisms of perceptual filling-in.Nature reviews
neuroscience, 7(3):220–231, 2006.
[26] A. Lutz, A. P. Jha, J. D. Dunne, and C. D. Saron. Investigating the phe-
nomenological matrix of mindfulness-related practices from a neurocognitive
perspective. American Psychologist, 70(7):632, 2015.
[27] S. Martinez-Conde, S. L. Macknik, and D. H. Hubel. The role of microsac-
cades in high-resolution visual tasks.Nature, 429(6993):744–745, 2004.
[28] H. McGurk and J. MacDonald. Hearing lips and seeing voices.Nature, 264
(5588):746–748, 1976.
[29] T. Metzinger. Being no one: The self-model theory of subjectivity. mit
Press, 2004.
[30] T. Metzinger. Minimal phenomenal experience: Meditation, tonic alertness,
and the phenomenology of "pure" consciousness.Philosophy and the Mind
Sciences, 1(I):1–44, 2020.
26
[31] T. Metzinger. The Elephant and the Blind. Placeholder Publisher, 2024.
Hypothetical future publication.
[32] C. M. Mooney. Age in the development of closure ability in children.
Canadian Journal of Psychology/Revue canadienne de psychologie, 11(4):
219, 1957.
[33] D. Mumford. On the computational architecture of the neocortex.Biological
cybernetics, 65(2):135–145, 1991.
[34] S. I. Nikolenko. Synthetic data for deep learning. arXiv preprint
arXiv:2105.00566, 2021.
[35] J. K. O’Regan and A. Noë. A sensorimotor account of vision and visual
consciousness. Behavioral and brain sciences, 24(5):939–973, 2001.
[36] T. Parr, R. V. Rikhye, M. M. Halassa, and K. J. Friston. Prefrontal
computation as active inference.Cerebral Cortex, 29(11):4571–4585, 2019.
[37] L. Pessoa, E. Thompson, and A. Noë. Finding out about filling-in: A guide
to perceptual completion for visual science and the philosophy of perception.
Behavioral and brain sciences, 21(6):723–748, 1998.
[38] J. Petitot. Morphological eidetics for phenomenology of perception.Phe-
nomenology and the cognitive sciences, 3(4):367–399, 2004.
[39] J. Petitot, F. J. Varela, B. Pachoud, and J.-M. Roy.Naturalizing phe-
nomenology: Issues in contemporary phenomenology and cognitive science.
Stanford University Press, 1999.
[40] F.Picard. Mysticalstatesandthepredictivecodingoftheinsula. Placeholder
Journal, 2024. Hypothetical future publication.
[41] F. Picard and K. Friston. Ecstatic epileptic seizures: A potential window on
the neural basis for human self-awareness.Epilepsy & Behavior, 37:210–214,
2014.
[42] S. Prest and K. Berryman. Towards and active inference account of deep
meditative deconstruction. 2024.
[43] V. S. Ramachandran. Blind spots.Scientific American, 266(5):86–91, 1992.
[44] M. J. Ramstead, P. B. Badcock, and K. J. Friston. Variational ecology and
the physics of sentient systems.Physics of life reviews, 31:188–205, 2019.
[45] M. J. Ramstead, A. K. Seth, C. Hesp, L. Sandved-Smith, J. Mago, M. Lif-
shitz, G. Pagnoni, R. Smith, G. Dumas, A. Lutz, et al. From generative
models to generative passages: a computational approach to (neuro) phe-
nomenology. Review of Philosophy and Psychology, 13(4):829–857, 2022.
27
[46] R. A. Rensink, J. K. O’Regan, and J. J. Clark. To see or not to see: The
need for attention to perceive changes in scenes.Psychological science, 8(5):
368–373, 1997.
[47] J.-M. Roy, J. Petitot, B. Pachoud, and F. J. Varela. Beyond the gap: An
introduction to naturalizing phenomenology.Naturalizing phenomenology:
Issues in contemporary phenomenology and cognitive science, pages 1–80,
1999.
[48] A. Safron. On the varieties of conscious experiences: altered beliefs under
psychedelics (albus). 2020.
[49] L. Sandved-Smith, C. Hesp, J. Mattout, K. Friston, A. Lutz, and M. J.
Ramstead. Towards a computational phenomenology of mental action:
modelling meta-awareness and attentional control with deep parametric
active inference.Neuroscience of consciousness, 2021(1):niab018, 2021.
[50] A. B. Sekuler and S. E. Palmer. Perception of partly occluded objects: A
microgenetic analysis. Journal of Experimental Psychology: General, 121
(1):95, 1992.
[51] A. K. Seth. A predictive processing theory of sensorimotor contingencies:
Explaining the puzzle of perceptual presence and its absence in synesthesia.
Cognitive neuroscience, 5(2):97–118, 2014.
[52] A. K. Seth. The cybernetic bayesian brain.Open MIND. Frankfurt am
Main: MIND Group, 2015.
[53] D. J. Simons and D. T. Levin. Change blindness. Trends in cognitive
sciences, 1(7):261–267, 1997.
[54] R. Smith, K. J. Friston, and C. J. Whyte. A step-by-step tutorial on active
inference and its application to empirical data.Journal of mathematical
psychology, 107:102632, 2022.
[55] B. Thanissaro. The Buddha’s Teachings: An Introduction. Metta Forest
Monastery, 2020.
[56] E. Thompson. Waking, dreaming, being: Self and consciousness in neuro-
science, meditation, and philosophy. Columbia University Press, 2015.
[57] E. Thompson and F. J. Varela. Radical embodiment: neural dynamics and
consciousness. Trends in cognitive sciences, 5(10):418–425, 2001.
[58] S. Van de Cruys. Affective value in the predictive mind. 2017.
[59] F. J. Varela. Neurophenomenology: A methodological remedy for the hard
problem. Journal of consciousness studies, 3(4):330–349, 1996.
28
[60] F. J. Varela. The naturalization of phenomenology as the transcendence
of nature: Searching for generative mutual constraints.Alter: Revue de
phénoménologie, 5:355–385, 1997.
[61] F. J. Varela, E. Thompson, and E. Rosch.The embodied mind: Cognitive
science and human experience. MIT press, 1991.
[62] R. M. Warren. Perceptual restoration of missing speech sounds.Science,
167(3917):392–393, 1970.
[63] K. Williford, D. Bennequin, K. Friston, and D. Rudrauf. The projective
consciousness model and phenomenal selfhood.Frontiers in Psychology, 9:
2571, 2018.
[64] J. M. Windt. Just in time—dreamless sleep experience as pure subjective
temporality. Open MIND, 2015.
[65] L. Wittgenstein. Philosophical investigations. John Wiley & Sons, 2009.
Original work published 1953.
[66] L. Xu and K. Veeramachaneni. Synthesizing tabular data using generative
adversarial networks.arXiv preprint arXiv:1811.11264, 2018.
[67] B. Ñ¯ an.amoli and B. Bodhi.The middle length discourses of the Buddha: A
translation of the Majjhima Nikaya. Wisdom Publications, 1995.
29