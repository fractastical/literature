Article https://doi.org/10.1038/s41467-024-54257-3
A hierarchical active inference model of
spatial alternation tasks and the
hippocampal-prefrontal circuit
Toon Van de Maele1,2, Bart Dhoedt1, Tim Verbelen2,4 & Giovanni Pezzulo3,4
Cognitive problem-solving beneﬁts from cognitive maps aiding navigation and
planning. Physical space navigation involves hippocampal (HC) allocentric
codes, while abstract task space engages medial prefrontal cortex (mPFC) task-
speciﬁc codes. Previous studies show that challenging tasks, like spatial
alternation, require integrating theset w ot y p e so fm a p s .T h ed i s r u p t i o no ft h e
HC-mPFC circuit impairs performance. We propose a hierarchical active
inference model clarifying how this circuit solves spatial interaction tasks by
bridging physical and task-space maps. Simulations demonstrate that the
model’sd u a ll a y e r sd e v e l o pe f f e c t i v ec o gnitive maps for physical and task
space. The model solves spatial alternation tasks through reciprocal interac-
tions between the two layers. Disrupting its communication impairs decision-
making, which is consistent with empirical evidence. Additionally, the model
adapts to switching between multiple alternation rules, providing a mechan-
istic explanation of how the HC-mPFC circuit supports spatial alternation tasks
and the effects of disruption.
To solve cognitive problems, such asﬁnding the shortest route to a
goal destination in a busy city, we can use so-called cognitive maps of
the situation that affordsﬂexible planning. The concept of the cogni-
tive map has been initially popularized by Tolman, especially in the
context of spatial navigation
1. An extensive body of literature has
investigated the neuronal underpinnings of cognitive maps for spatial
navigation in humans, rodents, and other animals. This research has
established the crucial importance of structures of the medial tem-
poral lobe and especially of the hippocampal formation in the creation
of codes for allocentric space, such as place cells (in the
hippocampus)
2,3 and grid cells (in the entorhinal cortex)4.A ne m e r g i n g
perspective to understand theseﬁndings is that cognitive mapping in
the hippocampal formation might be described at the computational
level as a sequence learning problem and that place cells, grid cells and
other specialized codes might emerge from such sequence learning
5–13.
Furthermore, it has been suggested that the same hippocampal codes
that support spatial navigation might also support navigation in cog-
nitive domains
14–16.
In parallel, the concept of a cognitive map can apply to the
abstract state spaces that describe the sequential stages of a task at
hand (for example, buying a gift in one’s preferred shop and then
bringing it to a friend’s house) as opposed to the moreﬁne-grained
spatial codes found in the hippocampal formation. Recent studies
reported that prefrontal structures, such as the orbitofrontal cortex,
might host such coarse task codes, which might permit representing
(for example) the current and the next stages of the task
17,18 as well as
future navigational goals19, as opposed to a physical location.
Crucially, during goal-directed spatial navigation (and other
cognitive problems), it is often necessary to combine cognitive
maps of task space (toﬁnd a sequence of coarse-grained actions to
progress in task space, such as ensuring the correct sequence of
destinations to buy and deliver a gift) and physical space (toﬁnd a
path that reaches the next intended destination)
20–22. However, we
still lack a comprehensive theory that explains the ways cognitive
maps of physical and task space interact when executing cogni-
tive tasks.
Received: 2 September 2023
Accepted: 4 November 2024
Check for updates
1IDLab, Department of Information Technology, Ghent University - imec, Ghent, Belgium.2VERSES Research Lab, Los Angeles, USA.3Institute of Cognitive
Sciences and Technologies, National Research Council, Rome, Italy.4These authors contributed equally: Tim Verbelen, Giovanni Pezzulo.
e-mail: giovanni.pezzulo@istc.cnr.it
Nature Communications|         (2024) 15:9892 1
1234567890():,;
1234567890():,;
A useful starting point to understand the interaction of task-
related and spatial codes is rodent memory-guided, spatial alternation
tasks. A prominent example is the spatial alternation task in the
W-maze studied in ref.23. As visualized in Fig.1, the W-maze comprises
three corridors. To collect rewards, the animal has to visit the corridors
in the correct order, according to an alternation rule (e.g., left, center,
right, center, left, etc) that is initially unknown and has to be learned by
trial and error.
A series of experiments showed that solving the spatial alternation
task engages coordinated patterns of neural activity in the hippo-
campus (HC), which putatively encodes the spatial aspects of the task,
and the prefrontal cortex (mPFC), which putatively learns the alter-
nation rule and uses it to guide behavior
24–28. For example, the HC -
mPFC interaction is selectively enhanced during epochs requiring
spatial working memory
29. Furthermore, the coordination of neural
activity in both structures spans multiple timescales, from theta
sequences during navigation to reactivation (replay) activity during
inter-trial periods prior to navigation— and is considered crucial for the
information exchange between the two areas during the task
30.
Notably, disrupting awake hippocampal reactivations (sharp wave
ripples, SWR) at decision points impaired the animal’s performance,
but only for the aspects of the alternation task that require spatial
working memory
23.S p e c iﬁcally, disrupting awake SWRs impaired
outbound decisions (from the central corridor to the left or right
corridor), which require memory for immediate past outer arm loca-
tion. Rather, the same SWR disruption did not impair inbound deci-
sions (from the left or right corridor to the central corridor) that do not
require memory of the past corridor; nor did it impair self-
localization
23. The study concludes that impairing SWRs prevents the
hippocampus from providing information about past locations (and/
or future options) to the prefrontal cortex, which is responsible for
learning the alternation rule and to use it to guide behavior. In other
words, the SWR disruption impairs communication between the hip-
pocampus and the prefrontal cortex, hence preventing the latter from
correctly inferring the current stage in task space.
The importance of hippocampal-prefrontal communication for
memory-guided decisions is conﬁrmed by another study in which
animals had to learn and subsequently switch between three spatial
alternation rules in the same three-arm maze
31. This study showed that
disrupting neural activity in the mPFC directly following hippocampal
sharp-wave ripples (SWRs) (but not after a random delay) impairs the
animal’s ability to switch between learned rules.
In this work, we advance a computational theory that casts the
interactions between the hippocampus (HC) and the medial prefrontal
cortex (mPFC) during spatial alternation tasks, in terms of hierarchical
active inference (HAI)
32–38. Our theory has two main tenets. First, the
HC and the mPFC learn cognitive maps for navigation in physical and
task space, respectively, using the same statistical computations
modeled as Partially Observable Markov Decision Processes,
(POMDPs
39), but based on different inputs (see Section“Methods”).
Second, the neural circuit formed by the HC and the mPFC realizes a
HAI architecture, which can solve spatial alternation tasks. HAI rests on
reciprocal, bottom-up, and top-down message passing. In the bottom-
up pathway, the lower hierarchical level (encoding the HC map) infers
the current location based on sensory information and communicates
it to the higher level (encoding the mPFC map). In turn, the higher level
infers the next goal location (in the mPFC map) and sets it as a goal for
spatial navigation (in the HC map), in a top-down manner. Impairments
of the message passing prevent the architecture from correctly solving
spatial alternation tasks. The separation of timescales automatically
emerges from the structure of HAI. The lower level passes the bottom-
up message when a free-energy threshold is met, indicating it is con-
ﬁdent it has reached its goal.
We exemplify the new theory by presenting two simulations
of rodent spatial alternation tasks, with intact and impaired HC-
mPFC interactions
23,31.O u rﬁrst simulation, presented in Section
“Experiment 1: solving the spatial alternation task”,b r i n g st h r e e
main conclusions. First, the model is able to learn the spatial
structure of the maze (HC map) and spatial alternation rules
(mPFC) by navigating in the environment. Second, when the HC -
mPFC circuit is conﬁgured as a HAI system, it effectively solves
spatial alternation tasks, through bottom-up and top-down mes-
sage passing. Third, disruptingthe interaction from the HC to the
mPFC breaks the spatial memory and impairs the animal’sa b i l i t y
to make outbound but not inbound decisions, as observed
empirically
23. Our second simulation, presented in Section
“Experiment 1: solving the spatial alternation task ”,s h o w st w o
additional features of the model. First, in a task in which three
alternative spatial alternation rules are in play, the HC - mPFC
circuit permits inferring the current rule. Second, selectively
inhibiting the mPFC impairs this ability and provokes distinct
behavioral patterns, as observed empirically in the rodent study
reported in
31.
Results
The W-maze setup and the hierarchical active inference
(HAI) agent
In this work, we consider the spatial alternation task in the W-maze of
ref. 23,s e eF i g .1. In this task, an animal (or an artiﬁcial agent) can
acquire rewards at the end of each corridor, providing that the corri-
dors are visited in the correct order (e.g., left, center, right,
center, left).
In this section, we present a HAI agent
36 for solving the spatial
alternation task. Central to this framework is the fact that agents are
endowed with a generative model, describing how observed outcomes
are generated from a hidden, unobserved state, which is inﬂuenced by
the agent’s actions. The agent infers these hidden states by minimizing
the free energy functional with respect to its belief over the state and
Fig. 1 | Spatial alternation tasks (LCRC).This ﬁgure exempliﬁes the W-maze used
in ref.23 to study spatial alternation and the role of the hippocampal - prefrontal
dynamics. The W-maze is characterized by three separate corridors. The animal can
acquire the reward at the end of each corridor, however, to receive the reward it
must do so in the correct order (e.g., Left (L), Center (C), Right (R), Center (C), Left
(L), etc). The spatial alternation task implies that when the animal is in the center
position, it has to make a (difﬁcult) outbound decision: it has to go either to the left
or the right, depending on where it comes from. This outbound decision, therefore,
requires a spatial memory component. In contrast, when the animal is in one of the
side corridors (left or right), the only correct action is to move to the center. This
inbound decision is considered simpler since it does not require a memory com-
ponent. Trajectory order is indicated by the shade where lighter is earlier.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 2
acts to minimize its expected free energy over time36. For more details
on these mechanisms, the reader is referred to Section“Hierarchical
active inference”.
In our simulations, the HAI agent is endowed with a hierarchical
generative model of two layers, which is schematically illustrated in
Fig. 2 (see Section“Hierarchical active inference” for a formal intro-
duction). In this generative model, the lowest layer takes up the
functional role of the hippocampus, namely encoding the spatial
structure of the maze
2. In contrast, the highest layer takes the func-
tional role of the prefrontal cortex, namely encoding the structure of
Fig. 2 | Illustration of the hierarchical active inference (HAI) agent and the
W-maze setup. aThe hierarchical generative model of the HAI agent. Theﬁgure
shows that the two layers of the model include cognitive maps of physical space
(level 1) and task space (level 2). These cognitive maps essentially represent tran-
sitions between learned locations and stages of the task, respectively. Both maps
are learned using a probabilistic sequence learning algorithm: the clone structured
cognitive graph (CSCG
7). Within each block, a subset of states from the CSCG is
visualized, namely, the active states when pursuing the alternation rule (LCRC). The
dynamics between the physical (level 1) and task space (level 2) CSCG in the hier-
archical model are regulated by bottom-up message passing (that supports infer-
ence) and top-down message passing (that supports planning). Bottom-up
inference messages from the physical space model are passed to the task space
model, while top-down actions drive planning from the task-space model to the
physical space model. An essential aspect of these interactions is that the two levels
operate on a different temporal scale. Transitions at the physical level, through the
agent’s movement, occur at every time step. Rather, the task level only transitions
when the intermediate goal (of reaching a certain position in the maze) is achieved.
This abstraction allows for hierarchical planning. Finally, the interaction with the
environment using an action-perception loop with the world, the W-maze, is shown
on the right.b The 20 distinct observations the agent can encounter in the W-maze
created in the Minigrid environment
40.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 3
task space18. The agent learns both maps using the same sequence
learning algorithm: the clone structured cognitive graphs (CSCG7), see
Section“Methods” for details. The latent states learned by the CSCG at
the two levels are schematically illustrated within the two boxes
of Fig.2.
Crucially, the HAI agent performs inference (of where it is in
physical space, i.e., its pose, and in task space) and planning (of the
next goal in physical and task space) through message-passing
between the two hierarchical levels. As shown in Fig.2a, level 1
observes the structural aspects of the environment through its
bottom-up observations, i.e., the three-by-three grid around the agent
a ss h o w ni nF i g .2b and selects local movements in physical space such
as turn left, turn right or move forward to reach any goal location
(which is fed by the highest level, see below).
Once level 1 has navigated towards its local goal, it passes a
bottom-up message containing the current state of the agent to level 2.
In turn, level 2 processes this message and infers the next stage of the
task to achieve a reward (as it is endowed with a prior preference to
achieve reward at each step, see Section“Hierarchical active infer-
ence”). Actions at level 2 are abstract and are matched to the bottom-
up received observable states from level 1. Since level 1 states encode
the agent’s pose, these actions essentially mean move to target loca-
tion. We speciﬁcally encode only actions corresponding to target
locations at the end of a corridor (i.e., observation 1 in Fig.2b). The
selected action is sent to level 1 in a top-down manner and corresponds
to the goal location for level 1. Once level 1 reaches the goal location,
this process repeats.
Finally, the right part of Fig. 2 illustrates the fact that the
lowest level of the model is responsible for the action-perception
loop with the environment — here, an implementation of the
W-maze in the Minigrid
40 simulator. Observations in the Minigrid40
are acquired as a square around the agent. In this work, we chose a
range of three, yielding three-by-three observations which are then
mapped to a unique one-hot index. For each step, the agent also
observes a binary value indicating the reward presence. The actions
the agent can perform are turn left, turn right and move forward,
which are also represented as a one-hot index. See Section“Hier-
archical active inference ” for more details on the model
implementation
Experiment 1: solving the spatial alternation task
We replicate the task and experiment from a study on rodents23 where
t h ea n i m a li st a u g h tt h i ss p e c iﬁc alternation task. In this study, dis-
ruption of the hippocampal SWR reduces the performance on out-
bound trajectories, but not on inbound trajectories.
We ﬁrst allow the HAI agent to learn the two cognitive maps for
solving the spatial alternation task, using two CSCGs (see
7 and Section
“Methods”for details on the learning procedure). The learned maps for
the W-maze are shown within the two boxes of Fig.2.F o re a s eo f
visualization, within each box, only a subset of states from the CSCG is
shown, namely, the states that are active when correctly pursuing the
alternation rule. We denote the sequence of this alternation rule as
LCRC (for Left, Center, Right, Center).
The hippocampal map on the lowest level organizes the 20
observations that the agent encounters during navigation (illustrated
in Fig.2b) into a coherent graph, which nicely reﬂects the 3 corridors of
the W-maze. In this map, the circles reﬂect the states learned by the
CSCG, while the transitions reﬂect the agent’ss p a t i a la c t i o n s( e . g . ,t u r n
left, turn right, or move forward). The nodes are color-coded (and
numbered) according to the observations that they encode. Note that
while the agent encounters the same observations multiple times
during navigation (i.e., the observations are aliased), the CSCG cor-
rectly reﬂects the fact that they are part of different behavioral
sequences. For example, observation 1 (that corresponds to the loca-
tion in which a reward can be collected) appears three times, at the
apex of each of the latent sequences that represent each of the three
corridors.
The prefrontal map on the highest level corresponds to a much
smaller graph that encodes the task-speciﬁc sequence of corridors that
the agent has to visit to secure rewards. In this graph, the nodes cor-
respond to (color-coded) corridors in which rewards have been col-
lected and the edges correspond to higher-order actions (go to
corridor). Note that there are two nodes of the same color: both cor-
respond to the central corridor but in different phases in task space
(i.e., after the left and the right corridors, respectively). This indicates
that the agent has successfully learned a low-dimensional representa-
tion of (or aﬁnite state machine for) the task rule LCRC.
We then investigate whether theH A Ia g e n te q u i p p e dw i t ht h e
learned cognitive maps for physical and task space is able to solve
spatial alternation tasks. For this, we endow the HAI agent with a prior
preference for maximizing the reward and place it in a random pose in
the W-maze. We then let the agent run for 300 steps, and record
whether it selected the correct or the incorrect corridor toﬁnd the
reward. We repeat this experiment for 20 trials, to collect statistics.
Figure 3a shows that the HAI agent has near-perfect performance
during the easiest, inbound decisions and very high (above 80%) per-
formance during the harder, outbound decisions, closely approx-
imating the empirical results reported in
23.F u r t h e r m o r e ,a se x p e c t e d ,
it signiﬁcantly outperforms a Random agent that selects the next
corridor randomly (t-test p =0 ) .F i g u r e3b shows that after a few time
steps, once the HAI agent has inferred where it is (in both physical and
task space), it consistently follows the rule and is able to collect
rewards at every step.
We performed two control experiments, which reveal that the HAI
model is robust to a wide choice of parameters and to greater levels of
uncertainty (see Supplementary Materials). Our control experiments
also illustrate that in scenarios characterized by greater uncertainty,
the expected free energy functional used by the HAI model to select
policies is more advantageous compared to the sole objective of
reward maximization that is common in economic and reinforcement
learning settings. The expected free energy functional effectively bal-
ances two components: a pragmatic imperative to maximize reward
(e.g., to reach the next subgoal to secure reward) and an epistemic
imperative to gain information (e.g., to reduce uncertainty about the
current pose, by going to places where unambiguous observations
could be found, e.g., the T-junction), see Section“Hierarchical active
inference”. By comparing HAI agents with and without the epistemic
imperative, we found that the pragmatic imperative to secure rewards
is sufﬁcient to address a W-maze with low uncertainty, but adding the
epistemic imperative to reduce uncertainty about one’sh i d d e ns t a t e
(here, the pose) signiﬁcantly increases performance in a W-maze with
greater uncertainty (see Supplementary Materials). This is because an
agent endowed with epistemic uncertainty can explicitly plan to self-
localize (e.g., by visiting unambiguous locations, such as the bottom of
corridors), which is especially advantageous with greater
uncertainty
36,41.
Having established that the HAI agent correctly solves the spatial
alternation task, we next ask whether an impairment of spatial memory
disrupts its performance in outbound decisions, as shown experi-
mentally by ref.23. In this study, the experimenters disrupted hippo-
campal SWR at decision points and observed that the disruption
prevented access to spatial memory— rendering the rodents unable to
make correct outbound decisions— but left hippocampal spatial codes
intact. The experimenters, therefore, concluded that the disruption
was caused by a failure of communication or updating at the level of
the PFC.
In analogy with this procedure, we realized a variant of the HAI
agent (HAI disrupt) in which we implement the SWR disruption by
preventing the belief updates (through the transition model) at the
second level of the hierarchical model. By doing this, we remove the
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 4
agent’s spatial memory about task space. However, the agent is still
able to receive the bottom-up message from level 1 and thus knows
where it currently is in physical space— in keeping with theﬁnding that
spatial codes are intact after SWR disruptions
23.
When we compare the HAI agent with and without disruption in
the spatial alternation task, we observe the same behavior as the
experimental study: the HAI disrupted agent correctly addresses
inbound decisions but makes outbound decisions at a chance level, see
Fig. 3a. The simulation shows that the disruption only impairs the
outbound trajectories (please see the Supplementary Materials for
additional simulations exploring how the choice of the gamma para-
meter inﬂuences the results). A more detailed example of this pattern
of results can be appreciated in Fig.3b, which shows that the HAI
disrupted agent tends to miss rewards, but only during outbound
decisions.
Finally, we performed a control simulation to identify more clearly
why the disruption impairs the ability to make the outbound decisions.
It is worth noting that in the higher-level (prefrontal) map of the HAI
model, the task variables (color-coded nodes in Fig.2) are hidden (i.e.,
they do not have corresponding observations), unlike the spatial states
in the hippocampal map that have corresponding observations.
Hence, the agent’s estimate of the hidden state at the level of task
space depends entirely on spatial memory, as encoded in the model’s
transition function– which we disrupt.
To assess whether the partial observability of task space is key to
explain the effects of the disruption, we considered an alternative
(control) environment where the task variable is observable, however,
the component required for spatial memory is unobserved. We con-
sider the following variant of the W-maze environment: the maze is
augmented with a light signal. Depending on where the agent just
collected reward, a different color is shown. We implement this by
adding an additional observation modality containing a categorical
variable, indicating the previous rewarding corridor.
We repeated the same disruption experiment as before and found
that the disrupted HAI agent maintains a high success rate for inbound
decisions, but makes outbound decisions at chance level (Fig.3c). This
control experiment therefore indicates that disruption of spatial
memory impairs the ability to make the outbound decisions, even
when task variables are observable.
Experiment 2: learning multiple rules andﬂexible switching
between them
In the next experiment, we investigate whether the HAI agent is able to
learn multiple alternation rules as proposed in31. We consider three
spatial alternation rules, where for each rule, a different corridor serves
as the alternation point (LCLR, LCRC, and RCRL). Note that for sim-
plicity, we use the same structure and notation as in the previous
experiment with the W-maze, despite the study of multiple alternation
rules of ref.31 has been conducted on a three-arm radial maze, not
aW - m a z e .
In this experiment, the HAI agent is endowed with a cognitive map
of physical space which is the same as Experiment 1 (since the maze is
always the same). However, it has to learn a new cognitive map of task
space, which now comprises the dynamics of the three rules. For this,
Fig. 3 | Experiment 1: solving the spatial alternation task with intact and dis-
rupted spatial memory. aThe success rate in the spatial alternation task for the
HAI agent without disruption, the HAI agent with disruption (HAI disrupted), and
the Random agent, separated for both in- and outbound trajectories. The success
rate is computed over 20 trials per agent, with 300 steps per trial, only recording
the presence of reward at the end of the selected corridors. Data is represented as
the mean success rate, box limits indicate the central 50% of the data, with the
horizontal line representing the median value. The whiskers represent the mini-
mum and maximum values and outliers are visualized as circles.b The reward over
time for the HAI agent and a disrupted HAI agent. The shaded areas in the plot
indicate outbound trajectories.c The success rate for the HAI agent with (HAI
disrupted) and without (HAI) disruption in the alternative light environment. Data
is represented as the mean success rate, box limits indicate the central 50% of the
data, with the horizontal line representing the median value. The whiskers repre-
sent the minimum and maximum values and outliers are visualized as circles. The
success rate is computed over 20 trials per agent, with 300 steps per trial, only
recording the presence of reward at the end of the selected corridors. The agent is
disrupted by disabling the connection from the transition model of the POMDP,
separated for in- and outbound trajectories. We observe that the performance on
inbound trajectories does not change, while for outbound trajectories this drops to
random chance.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 5
we let the agent explore the maze for 8000 steps, with alternating task
rules after blocks of 1000 steps (in task space, i.e., visits to corridors).
To aid the learning process, we guide the sequence and select the
correct action 75% of the time, and random otherwise.
The learned cognitive map of task space is visualized in Fig.4a. For
ease of visualization, we extract the states that are active when the
agent has correctly inferred the rule, after a warmup period (15 steps)
in each block. Similar to Section“Experiment 1: solving the spatial
alternation task”, the states of the task space correspond to the distinct
colors, however since the different rules have distinct transition
dynamics, these are mapped to unique states. Figure4a uses different
colors to visualize the states of the three different rules (red for rule 1,
green for rule 2, and blue for rule 3). The smaller dark gray states are
states used for transitioning between rules, or encode the corridors
where no reward is found. We observe that for each of the individual
rules, four states are active. This is an indication of correct learning
since four is the optimal number required for each alternation
problem.
T h e n ,w ee v a l u a t ew h e t h e rt h eH A Ia g e n ti sa b l et oi n f e rw h i c hr u l e
is currently in place— in order to continue collecting rewards when the
rule switches. For this, we test the HAI agent (endowed with a pre-
ference to maximize reward) in a task in which the rule switches every
300 steps.
Figure4b shows the success rate of various agents for aﬁxed rule
over 20 trials. From the success rate of the HAI agent trained on the
three rules (HAI 3 rules), we can conclude that the agent properly
encodes the rule. When we apply the same disruption as in Experiment
1 (Section “Experiment 1: solving the spatial alternation task”), the
success rate for inbound trajectories is now also affected (HAI 3 rules
disrupt). This drop in performance is caused by the fact that when
Fig. 4 | Experiment 2: Multiple spatial alternation rules (LCLR, LCRC,
and RCRL). aRepresentation of the states used in the CSCG for the three distinct
rules. States again map to the conjunction of the visited corridor and the presence
of reward. Colors in the graph indicate the rule for which the state is used (red for
rule 1, green for rule 2, and blue for rule 3). The gray nodes represent other states,
either without reward or in transition between multiple rules.b Disruption
experiment for the model that learned three rules, evaluated on a single rule. The
success rate is computed over 20 trials per agent, with 300 steps per trial.Data is
represented as the mean success rate, box limits indicate the central 50% of the
data, with the horizontal line representing the median value. The whiskers repre-
sent the minimum and maximum values and outliers are visualized as circles.
c Average reward for the agents trained on 3 rules, 1 rules and random selected. The
case where the rule is constant (left), and the rule switches every 300 steps to a
randomly selected next rule (right) is considered. This experiment was repeated
over 20 trials of 900 steps. Data is represented as the mean success rate, box limits
indicate the central 50% of the data, with the horizontal line representing the
median value. The whiskers represent the minimum and maximum values and
outliers are visualized as circles.d Performance during a single representative trial
of 1000 steps, during which the rule switches randomly, every 150 steps. The top
plot shows the rule that is currently in play. The center plot shows the belief over
rule, which is computed byﬁrst extracting states belonging to a single rule, and
then measuring the likelihood of being in one of these states. The bottom plot
shows the rewards collected over time.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 6
considering multiple rules, there is also ambiguity over which rule is
currently in play, for which the spatial memory component is crucial.
Figure 4c compares the performance of various agents in two
scenarios: in the former, there is a single rule, which remainsﬁxed
throughout the experiment (left plot), whereas in the latter, there are
three rules, which switch randomly every 300 steps, over 20 trials. The
left plot shows that both the Active Inference agents trained on three
rules (HAI 3 rules) and on one rule (HAI 1 rule) achieve a very high
success rate in theﬁrst scenario and both outperform a random agent
(random). This result indicates that learning multiple rules does not
decrease performance when a single rule is in place. The right plot
shows that the Active Inference agent trained on multiple rules (HAI 3
rules) achieves a high success rate in the second scenario, too, indi-
cating it correctly learned all the rules and successfully alternates
them. The Active Inference agent trained on a single rule (HAI 1 rule)
achieves a lower success rate, as expected, but still it outperforms a
random agent. This result shows that learning only a subset of the task
rules could lead to a relatively good performance in the W-maze, but
still it is possible to discriminate between agents that learn the task
completely, partially or select randomly from their behavior.
Figure 4d illustrates the behavior of the agent during a single,
representative trial. As shown in theﬁgure, shortly after the rule
switches (top panel), the HAI agent is able to correctly update its
belief about the current rule in its plan (center panel) and secure
rewards (bottom panel). The inference of the rule currently in play
follows standard Bayesian approach: when the rule changes, the
agent receives unexpected observations (since expected rewards are
not delivered) and correctly makes a transition to the subspace of the
task rule map that encodes the most likely rule. In the center panel of
Fig. 4d, we visualize the belief over each rule as the probability of
being in one of the four states used in a particular rule (the rules are
color-coded as Fig.4a) in or in another phase if the state does not
occur in either of the rules (grey line). When the agent has a high
probability of a rule, it no longer misses the reward. In some cases, it
misses the reward because there is a potential overlap between the
two rules. For example, around step 200 it could be either of the
rules, as it only picks the right one once, and the wrong one after—
which could happen for each corridor in each of the rules. Note that
in this architecture, the belief about the currently active rule is
implicit in the model, but it could be made explicit by adding a
hierarchical layer that maintains a probability distribution over the
map or the task one is currently in, as in
10.
Another importantﬁnding of the study of ref.31 is that disrupting
optogenetically the medial prefrontal cortex (mPFC) immediately after
a hippocampal SWR is detected signiﬁcantly impairs performance, by
increasing the occurrence of three distinct maladaptive patterns: (i)
rotated alternation, where the agent follows a different rule, (ii) back
and forth, where the agent alternates between two corridors, and (iii)
circling behavior, where the agent iterates over all corridors in a cyclic
pattern (see the Supplementary Materials for a graphical illustration of
these patterns). The authors reported a proportion of 35% alternation,
20% rotated alternation, 15% back and forth, and 10% circling.
The behavioral patterns that are generated from the same type of
disruption as conducted in experiment 1 - where the transition model
of the POMDP is lesioned - are investigated. We match the patterns
observed in the biological counterpart of this experiment
31, and eval-
uate the trajectories generated by the agent trained on the three rules,
for an agent with and without the spatial memory disruption from
Section“Experiment 1: solving the spatial alternation task”,a sw e l la sa
random agent. The results are reported in Table1. This shows that,
when lesioned, our model reproduces all four behavioral patterns seen
in lesioned animals
31 (alternation, rotated alternation, back and forth,
and circling) although in slightly different proportions compared to
the empirical study (please see the Supplementary Materials for a
simulation how the choice ofγ affects these proportions). We observe
that for the disrupted agent, a large part of the behaviors can be
classiﬁed as back-and-forth behavior. The speciﬁc proportions of
categorization are a function of the agents’habit policy E. This habit
acts as a prior over the selected action (of level 2). When no temporal
information is considered through the lesion, this prior will dominate,
and the agent will act according to the statistics of the training data.
This can be observed by looking at HAI disrupt (1), where the agent
behavior falls back to either the alternation or the back-and-forth case.
Finally, we also note that the patterns generated by the random agent
are generally classiﬁed as the other category, indicating that the
behavior generated by our disruption is not random.
Furthermore, we performed the same experiment for an agent
trained on a single rule. We observe that both rotated alternation and
circling behavior drops to near 0% as shown in the table indicated by
HAI control (1) and HAI disrupt (1). This is due to the agent not con-
sidering other rules, when falling back to its habit policy. To the best of
our knowledge, the pattern of errors in animals trained with a single
rule have not been systematically studied and hence our results could
be considered as a prediction of the HAI model.
The previous simulations illustrate that disruptions of the HAI
model permit reproducing empiricalﬁndings of rodent studies
23,31.
However, our model permits realizing synthetic experiments— for
example, other types of disruptions— that have not been studied
empirically so far, but which could be interpreted as predictions of
the model.
Here, we investigate the potential effects of a disruption of
hippocampal-prefrontal communication that completely impairs the
communication between the two levels of the HAI model. Speciﬁcally,
in this simulation, we prevent level 1 from sending any bottom-up
information to level 2. This is a severe impairment, since hierarchical
inference rests upon the reciprocal messages passing between the two
levels— and level 2 only receives observations from level 1, not from the
environment. Figure5 shows the differences between the physical
trajectory of the intact HAI agent (left) and the disrupted HAI agent
(right) in which communication between the two levels is impaired.
The shade of the trajectory indicates the order of visits (lighter is
earlier). Theﬁgure shows that while the intact HAI agent follows the
Table 1 | Behavioral patterns during disruption
HAI HAI HAI HAI Random
control (1) disrupt (1) control
(3)
disrupt
(3)
Alternation 86.33% 49.07% 76.00% 16.80% 3.47%
Rotated
alternation
0.00% 0.80% 1.07 % 13.55% 8.24%
Back and forth 2.68% 38.40% 2.40 % 38.48% 8.86%
Circling 1.61% 1.60% 8.80 % 17.34% 5.86%
Other 9.38% 10.13% 11.73% 13.82% 73.75%
Classiﬁed observed behavioral patterns according to31 in different scenarios for agents trained
on the three rules indicated by (3) and a single rule indicated by (1).
Fig. 5 | Disruption experiment for rule switching.when communication between
the level 1 and level 2 model is suppressed, perseveration behavior is observed.
Trajectory order is indicated by the shade where lighter is earlier, noise was added
to the trajectory to visualize overlapping visits.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 7
correct rule, the disrupted HAI agent perseveres in choosing the center
corridor.
Interestingly, the agent does check at the unambiguous
T-junction to self-localize and ensure that it is still in the correct cor-
ridor (which is a feature of the epistemic dynamics of active inference,
see Section“Hierarchical active inference”), but never selects the next
goal in the task rule. This is because, without the bottom-up message
from level 1, level 2 is unable to correctly recognize that one phase of
the task has been achieved and hence never updates the top-down
plan. This result illustrates the possibility to use the model to generate
predictions— in this case, about how disrupting communication
between the two levels produces perseveration— which could be sub-
sequently tested empirically.
Discussion
Here, we advanced a computational theory of hippocampal (HC) -
prefrontal (PFC) interactions during cognitive tasks that require navi-
gating in both physical and task space— such as spatial alternation
tasks. Empirical studies of spatial alternation have assessed that they
depend on the animal’s spatial memory, which is at least in part
maintained by HC-PFC communication
25,29,30,42–44 and that disruption of
this communication can impair difﬁcult (outbound) decisions23 and
the ability to switch between multiple rules31.
Our computational model is based on— and unites— two estab-
lished theories. The former is a theory of cognitive map formation,
based on a statistical sequence learning algorithm: the clone-
structured cognitive graph (CSCG)
7. Previous studies assessed that
CSCG are computationally effective and have biological validity since
they are able to successfully reproduce a number of empirical obser-
vations about cognitive map formation in the hippocampus, including
for example the emergence of place cell coding and splitter cells
8,45 and
orthogonalized state representations45 (see also5,6 for alternative pro-
posals about the computational principles that underlie spatial map
formation). Here we extended this body of work by showing that
CSCGs can be used in a hierarchical scheme to learn not just cognitive
maps for physical space (putatively linked to hippocampal computa-
tions), but also more abstract cognitive maps for task space (putatively
linked to prefrontal computations). Furthermore, we used learned
CSCGs as components of a hierarchical generative model for active
inference
36. Active inference is a normative framework to model sen-
tient behavior in terms of free energy minimization and (approximate
Bayesian) inference over a generative model, which is gaining popu-
larity in cognitive neuroscience. We have shown that by combining two
learned CSCG maps (for physical and task space), it is possible to
design an effective HAI agent able to solve spatial alternation tasks.
Interestingly, this scheme affords (hierarchical) planning by only using
local computations— that is, top-down and bottom-up message-pas-
sing between the two hierarchical levels.
Furthermore, by simulating the interruption of HC - PFC com-
munication in our model, we were able to correctly reproduce
impairments of difﬁcult (outbound) decisions
23 (Experiment 1) and of
c o r r e c tr u l es w i t c h i n g ,u n v e i l i n gt h es a m ek i n do fm a l a d a p t i v eb e h a -
vior— rotated alternation, back and forth, and circling behavior
(Experiment 2)— observed empirically in rodents
31.
Our model suggests that the selective impairment of outbound
decisions provoked by hippocampal SWR disruptions23 is due to the
fact that the SWRs convey messages to higher structures, like the PFC,
which are used to update a belief about the current stage of the task
(speciﬁcally, this message is key to propagate the belief about task
state at level 2 over time). This interpretation is in keeping with the idea
of
23 that the impairment is at the level of spatial memory, not of hip-
pocampal place coding, but to our knowledge, ours is theﬁrst work
that provides a mechanistic model of this theoretical proposal. Fur-
thermore, our model suggests that the maladaptive behavior found in
ref. 31 could be due to the impossibility for the higher, prefrontal
component (level 2) to correctly update its belief, based on bottom-up
message passing from the hippocampal component (level 1). This
perspective is coherent with theﬁnding of ref.31 that the only dis-
ruption of mPFC that prevents spatial rule switching is one that directly
follows hippocampal SWRs— hence highlighting the importance of
coherent HC-PFC reactivations to solve spatial alternation tasks.
Finally, our simulations suggest that other, more severe interruptions
of HC-PFC communication could produce a speciﬁc pattern of mala-
daptive behavior— namely, perseverative behavior— that differs from
those observed in the above disruption experiments. This is a predic-
tion that remains to be tested empirically.
Besides helping understand HC-PFC interactions, our simulations
suggest that looking at the animal’s epistemic behavior (e.g., the
selection of actions to self-localize and reduce uncertainty about the
current pose) during uncertain tasks could be important to elucidate
its strategy; in particular, whether it aims to maximize reward or also to
minimize its uncertainty– as predicted by active inference
36,41,46 (see
the Supplementary Materials). Future studies might assess whether
epistemic imperatives are important drivers of behavior during spatial
alternation or similar tasks, especially in conditions of high uncer-
tainty, such as when the animal is randomly placed in a maze (as in our
simulations) or when spatial contingencies or task rules change
unexpectedly.
It could, in principle, be possible to solve the navigation tasks
studied in this article using a non-hierarchical generative model with a
single map (and a single CSCG) that encompasses both spatial and
task-related components. However, the hierarchical structure of the
HAI generative model used in this study better reﬂects the implicit
division of labor between HC and PFC circuits, which is well-
established empirically in rodent studies. For example, inactivating
prefrontal structures during navigation tasks tends to disrupt rule-
related contingencies and deliberative behavior, while sparing spatial
representation
31,47. Therefore, our model reﬂects the widespread per-
spective that goal-directed navigation depends on the coordinated
interplay between (inferential) processes at two levels, which could be
associated with HC and PFC structures
20,25,30,43.T h ed i v i s i o no fl a b o r
between HC and PFC is also central to other prominent accounts, such
as the Complementary Learning Systems framework, which highlights
that faster learning in HC facilitates slower learning in neocortex— with
the latter integrating across episodes to extract semantic structure48.
Furthermore, from a computational perspective, the hierarchical
structure of the HAI model affords a useful factorization: learning a
new rule in a known maze, as we did in Experiment 2, only requires re-
train the higher-level CSCG, while leaving the lower-level CSCG
unchanged. This might be more problematic when using a single CSCG
that combines spatial and task-related information.
The current study has several limitations that will need to be
a d d r e s s e di nf u t u r es t u d i e s .F i r s t ,f o re fﬁciency reasons, we learned the
CSCGs ofﬂine (before embedding them into the HAI), using a simpli-
ﬁed procedure: we used predeﬁned trajectories that exhaustively
covered the W-maze as inputs for the cognitive map of physical space
and 75% correct trajectories as inputs for the cognitive map of task
space. In the future, it would be interesting to train CSCG online,
similar to ref.49, by guiding the exploration through active inference
dynamics
33,36,41. A second research avenue is to relax the separation of
the timescales between the two levels, by selecting their inputs (e.g.,
level 1 takes all sensory observations as inputs, whereas level 2 only
considers observations that could be rewarding— and in particular,
observation 1 in Fig.2b). In the future, it would be interesting to
explore methods to learn hierarchical models with multiple
timescales
50,51 and effective state spaces for navigation and for task
rules in self-supervised (and/or reward-guided) ways, as shown in prior
work
10,17,52,53. This might also help understand the reciprocal inﬂuences
between cognitive map learning at different levels and in different
(e.g., prefrontal versus hippocampal) brain structures. A third
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 8
challenge is to avoid having the agent learn from scratch each new
maze or rule. Recent work in transfer learning shows that it is possible
to reuse existing cognitive maps or latent task representations to learn
novel and similar tasks much faster
10,53–55. Extending our architecture
with transfer learning abilities would be important to provide more
accurate models of how animals learn cognitive maps, especially given
the strong evidence for the reuse of existing neural sequences and
cognitive maps in the hippocampus
56,57.
Another limitation of the model presented in this study is that in
the CSCG of the task layer, we made explicit the fact that the relevant
task states are the ends of corridors because the animals can (only)
acquire rewards in such states. This design choice reﬂects the ani-
mals’knowledge, given that in the spatial alternation task
23, the three
arms had reward food wells at their endpoints. However, future work
should consider generative models that learn autonomously what
the relevant task states are, even if they are not explicitly cued as in
23.
There is an established literature showing that latent task states
relevant for cognitive control can be learned autonomously using
Bayesian (nonparametric) methods
52,53,58,59 and that task rules can be
learned using deep reinforcement learning60. However, it remains to
be assessed in future research how to integrate these methods in the
HAI model.
Future studies might consider how to adapt the HAI agent to
robot navigation and planning. There is increasing interest in applying
active inference to robotic domains, given the appeal of its uniﬁed
approach to control, state estimation, and world model learning
61–63.
HAI is especially appealing since it facilitates planning over long time
horizons. Indeed, the computational burden required to calculate the
expected free energy of long policies is high, but it can be substantially
reduced by splitting work between (higher-level) policies that select (a
sequence of) subgoal(s) along with (lower-level) policies that select the
speciﬁc actions to reach each subgoal
64. For example, one study built a
hierarchical model for robot navigation, using multiple layers of
recurrent state space models
65. Another study realized a hierarchical
model for next-frame video prediction, using a subjective timescale for
the predictions
66. In another study67, a single layer CSCG was embed-
ded within the active inference framework and was able to support
navigation in different mazes. An interesting direction of future work
could be adding a (learned) hierarchical layer that maps the high-
dimensional observations space to a discrete state space and stack the
proposed HAI on top with multiple layers of CSCGs. This would
potentially afford abstract reasoning and planning for complex tasks,
directly from sensory observations such as pixels.
Finally, future studies might consider in more detail the functional
role and content of hippocampal reactivations and replay in the hip-
pocampus and other brain structures, such as the prefrontal cortex
and the ventral striatum
68–81. Previous studies suggested that replay
might play different roles, which range from memory functions to
planning, compositional computation, and the optimization of the
brain’s generative models10,82–87. However, these works have mostly
considered hippocampal replay, not coordinated replay in the hippo-
campus and the prefrontal cortex (and other brain areas). It might be
interesting to combine the insights of these studies with the hier-
archical architecture of the HAI agent, to test whether (for example)
planning and model optimization beneﬁt from the combined replay of
multiple brain structures, as opposed to local replay in the
hippocampus.
Methods
In this paper, we develop a hierarchical planning agent by combining
two components: we use CSCG to learn cognitive maps of physical and
task space; and HAI to form hierarchical plans. In the next sections, we
explain the two key components of our approach: CSCG (Section
“Clone-structured cognitive graphs (CSCG) for learning cognitive
maps of physical and task space”) and HAI (Section“Hierarchical active
inference”). Finally, we explain how we combined these (Section
“Casting CSCGs as partially observable Markov decision processes”).
Clone-structured cognitive graphs (CSCG) for learning cognitive
maps of physical and task space
Clone-structured cognitive graphs (CSCGs) are a probabilistic model
for representing sequences of data, e.g., a sequence of action-
observation pairs
7,88. They are a special case of Hidden Markov Mod-
els (HMM), where each observation maps to a subset of the hidden
states: the so-called clones of this observation. While these states have
the same observation likelihood, they differ in the implied dynamics
encoded in their transition model. Through the sequence of action-
observation pairs, speciﬁc clones will have a higher likelihood and can
therefore disambiguate the aliased observations.
The CSCGs are learned using the expectation-maximization (EM)
algorithm for HMMs (the Baum–Welch algorithm) which maximizes
the evidence lower bound (ELBO)
7. During the E-step, the posteriors
over states are estimated through smoothing, i.e., the forward-
backward algorithm. The M-step then selects the optimal parameters
for the transition model, given this sequence of visited states. For the
update equations, the reader is referred to ref.7.
We consider two cognitive maps, that represent the W-maze task
at two distinct levels. Theﬁrst level considers the structure of the
environment (i.e., where can the agent walk and where are the walls),
and the second level encodes the task rule (i.e., in which order the
corridors yields the reward). We learn these two maps in a sequential
fashion, using two distinct CSCGs.
We ﬁrst collect a sequence of data for learning the spatial struc-
ture by exploring the maze. We designed the exploration sequence to
exhaustively cover the W-maze such that a path from each pose to each
possible other pose is present. This sequence was used to learn the
parameters of a CSCG for the ﬁrst (physical) level, using the
expectation-maximization mechanism described in ref.7.W ei n i t i a l i z e
the level 1 CSCG with 20 clone states per observation. After learning,
the model is pruned using a Viterbi-decoding algorithm
7 and mapped
to the POMDP of theﬁrst level of the hierarchical generative model (as
will be explained in Section“Casting CSCGs as partially observable
Markov decision processes”) for engaging in active inference. The
learned graph is visualized in Fig.6a .T h en o d e si nt h eg r a p hc o r r e -
spond to the learned nodes and their colors represent the observa-
tions encountered in the states, during the correct execution of the
spatial alternation task. The gray nodes represent other states required
for transitioning between states or trajectories outside the correct
spatial alternation. Note that the graph shown in Fig.2 only shows the
colored nodes, but not the gray nodes.
We use the learned cognitive map of physical space to learn the
cognitive map of task space. Weﬁrst extract the states that are dis-
tinctly representing the end of each corridor (i.e., matching with
observation 1 in Fig.2b) and use them as the actions for the task level.
When an action is selected, this means that the agent is moving toward
one of these states, and thus the end of one of the corridors. The
observations at this level are the presence of reward ("1” if the agent is
following the rule,“0” otherwise), and the reached level 1 state (phy-
sical level). To aid the learning process, we guide the agent to follow
t h er u l e7 5 %o ft h et i m e .W en o wl e a r naC S C Gw i t h1 0c l o n e sp e r
observation using the same expectation-maximization scheme for the
single rule scheme, and 32 clones for the scenario with three rules.
After learning, the model is pruned using a Viterbi algorithm
7 and
mapped to the POMDP of the second level of the hierarchical gen-
erative model to engage in active inference (see Section“Casting
CSCGs as partially observable Markov decision processes). For a
robustness analysis on the model capacity with respect to amount of
rules, rule length and amount of clones per state, the reader is referred
to the Supplementary Materials. We visualize the learned cognitive
g r a p hi nF i g .6b, where the states used when pursuing the rule are
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 9
colored in red. It can be observed that the spatial alternation rule is
encoded in this graph, using four states, as this is the optimal amount
required for the spatial alternation rule (two states for the center, one
for the left, and one for the right corridor). Note that the graph shown
in Fig.2 only shows the colored nodes, but not the gray nodes.
Hierarchical active inference
Active inference is a normative framework to describe cognitive pro-
cessing and brain dynamics in living organisms36. It assumes that
action and perception both minimize a common functional: the
minimization of variational free energy (which is an upper bound to
the organisms’ surprise). Central to active inference is the idea that
living organisms are equipped with generative models, capturing the
causal relations between observable outcomes, the agent’sa c t i o n s ,
and hidden states. It is important to note the difference between the
agent’s generative model and the generative process. The former
represents the internal generative model that the agent uses to attri-
bute consequences to its actions, while the latter represents the true
process from which outcomes are generated in the world. Crucially,
the agent is unable to observe the hidden state of the generative
process, as they are separated through a Markov blanket given the
observation and action hidden variables. However, the agent is able to
perform actions and observe the generated outcomes
36,89,90.
The hierarchical generative model.W ee n d o wt h ea c t i v ei n f e r e n c e
agent with the hierarchical generative model depicted in Fig.7.I nt h i s
section, we provide a high-level overview of the generative model,
whereas in Section“Casting CSCGs as partially observable Markov
decision processes” we discuss implementation details.
The hierarchical generative model illustrated in Fig.7 is split into
two distinct levels. The highest hierarchical level (level 2) reasons at a
more abstract level, e.g., about which corridor to visit, while the lowest
level (level 1) considers the step-by-step navigation in the environment.
Each of these levels can be interpreted as an individual POMDPs
39,
operating at different timescales: faster for level 1 and slower for level
2. However, crucially, the two levels interact reciprocally by exchan-
ging messages, as described below. The hierarchical model supports
the hierarchical selection of abstract policies that move the agent from
one goal to the other, following the task rule(s) (at level 2) and from
one spatial location to the other, based on the spatial map (at level 1).
Conceptually, this hierarchical selection starts when the agent’s pre-
ferred observation is set toﬁnd the reward (technically, in active
inference, this is done by assuming a strong prior for the reward
observation) and it involves both levels.
The most fundamental goal of Level 2 is to select a policyπ
2 that
moves the agent through task space, by steering abstract actionsa2
t
that follow the rule learned by the agent (or in the case of multiple
rules, as in Experiment 2, the rule currently inferred to be in place).
These actions indicate the location that the agent will then try to reach
in the spatial map (level 1). In this way, the level 2-action conditions the
level 1 policyπ
1. The level 2 states2
t is a latent representation of the
corridor in which the agent is currently located in, and where it comes
from (i.e., where the agent is in rule space). This abstraction is enforced
by the choice of considering the reachable states to be matched to
observation 1 (Fig.2b), reﬂecting the fact that the agent knows the
potential reward locations (which, in the animal experiments, are
baited with food wells). The agent only considers reward on level 2 and
thus generates the presence of reward directly from the level 2 state. At
level 1, the policyπ
1 is conditioned by level 2, by setting the preferred
state as reaching one of the spatial locations encoded in the cognitive
graph. The policyπ
1 generates the low-level actions that navigate the
agent from one pose (position and orientation, encoded in hidden
Fig. 6 | Learned cognitive graphs.The arrows indicate possible transitions from
one state to the next. The node in the center where all nodes point to, is the added
and absorbing dummy state.a The learned CSCG for the physical space model
(level 1). The color in the graph represents distinct observations, only the states that
are active when pursuing the spatial alternation rule (LCRC) are colored, the other
states are marked in gray.b The learned CSCG for the task space model (level 2).
The color in the graph indicates the states that were active when pursuing the
spatial alternation rule (LCLR). While only four states are necessary to encode the
rule, it can be observed that many more states are added by learning the other
paths that might have been visited during training.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 10
state s1
t ) to the next. Then, the agent receives observationsot from the
environment, which reﬂect its current pose in the maze and which
permit updating the hidden state estimate. Note that reaching the
preferred spatial location requires making multiple transitions at level
1, but level 2 only makes a transition between states when the pre-
ference at level 1 is reached (or equivalently when the level 2 action is
performed). This creates a separation of timescales because multiple
transitions at level 1 are nested between two subsequent actions at
level 2.
Active inference.A ss h o w ni nF i g .7, the generative model considered
in this paper is a hierarchically stacked (two levels) POMDP. In order to
reduce complexity, weﬁrst discuss active inference for a single layer
POMDP, as depicted in Fig.8.A tt i m es t e pt,w eh a v eo b s e r v a t i o no
t,
actionat generated from policyπ, and statest. The generative model is
factorized as:
Pð~s, ~a, ~oÞ = Pðs0Þ
Y
t =1
P ot jst
/C0/C1
P st jst/C0 1, at/C0 1
/C0/C1
P at/C0 1
/C0/C1
, ð1Þ
where the tilde represents a sequence over time. As computing the
posterior distribution over the state is intractable for large state
spaces, we resort to variational inference and introduce the approx-
imate posterior Qð~sj~o, ~aÞ, the variational free energy measures the
discrepancy between the joint distribution and the approximate pos-
terior:
F = E
Qð~sj~o, ~aÞ ln Qð~sj~o, ~aÞ/C0 ln Pð~s, ~a, ~oÞ
/C2/C3
ð2Þ
Active inference agents minimize this quantity through learning (by
optimizing the model parameters), perception (by estimating the most
Fig. 8 | Factor graph of a POMDP.The conditional dependencies in Hidden
Markov Models are parameterized by a set of matrices. TheA matrix parameterizes
the likelihood model, i.e., how statesst map to observationsot.T h eB matrix
parameterizes the transition model, i.e., how statest+1 changes at each timestep,
dependent on policyπ.T h eC vector denotes the preferred observation or state,
and directly inﬂuences theG factor which conditionsπ. The policyπ depends on
the habitE. Orange circles denote that the variable is observed.
Level 2
Level 1 Level 1
Fig. 7 | The hierarchical generative model.This generative model consists of two
hierarchical levels, where the top level operates at a slower timescale than the lower
level. The policy at the highest level sets the actions for the top level, which
determines temporal transitions between states at level 2. The level 2 state then
generates the presence of reward and the level 1 state. At level 1, the states generate
the observations and the policy generates the actions, depending upon the selected
level 2 action. Finally, when a policy is pursued, a message is passed to the upper
level, transitioning the upper level to the next state. This highlights the different
timescales, whereas level 1 operates in the timescale of the agent’s movement, the
upper level operates at the timescale of the reached subgoals (visited corridors).
The policies and actions at future timesteps are unobserved for both levels, and are
inferred using the expected free energy. Orange circles denote observed variables,
while white circles denote unobserved variables.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 11
likely state), and planning (by selecting the policy or action sequence
that results in the lowest expected free energy).
The expected free energyG is a quantity that is only used during
planning and represents the agent’s variational free energy expected
after pursuing a policyπ. It is distinct from the variational free energy,
because it requires considering future observations generated from
the selected policy, as opposed to the current (and past) observations.
Active inference realizes goal-directed behavior by selecting
policies that minimize expected free energy— and that are expected to
yield observations that are closer to preferred observations (or prior
preferences). This is done by setting the approximate posterior of the
policy proportional to this quantity:
QðπÞ = σðγGðπÞ +l nEÞ, ð3Þ
where σ is the softmax function,γ is a temperature variable, andE is a
prior distribution over actions, or habit. Actions are sampled accord-
ing to this posterior distribution, where low temperatures yield more
deterministic behavior. At a given timestepτ, G is computed for a given
policy according to the following quantity as formalized in
91:
Gðπ, τÞ = /C0 EQðoτ jπÞ DKL½Qðsτ joτ , πÞjjQðsτ jπÞ/C138
/C2/C3
/C0 EQðoτ jπÞ ln PðoÞ
/C2/C3
ð4Þ
+ EQðoτ jπÞ DKL½Qðsτ joτ ÞjjPðsτ joτ , πÞ/C138
/C2/C3
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ}
Expected Approximation Error≥0
ð5Þ
≥ /C0 EQðoτ jπÞ DKL½Qðsτ joτ , πÞjjQðsτ jπÞ/C138
/C2/C3
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Epistemic value
/C0 EQðoτ jπÞ ln PðoÞ
/C2/C3
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Pragmatic Value
ð6Þ
This quantity is decomposed into an epistemic value and a pragmatic
value33. When evaluatingG, we compute the upper bound shown in Eq.
(6). We thus make the assumption that the expected approximation
error (Eq. (5)) is zero91. The epistemic value computes the expected
information gain (Bayesian surprise) between the priorQ(sτ∣π)a n d
posteriorQ(sτ∣oτ, π). Intuitively, this quantity represents how much the
agent expects the belief over the state to shift when pursuing this
policy. The pragmatic value then measures the expected log prob-
ability of observing the preferred observation under the selected
policy, intuitively computing how likely it is that this policy will drive
t h ea g e n tt oi t sp r i o rp r e f e r e n c e s .T h ef u l le x p e c t e df r e ee n e r g yf o ra
ﬁnite time horizonT is computed asP
T
τ =1 Gðπ, τÞ.
Hierarchical active inference. Here, we discuss how the active infer-
ence scheme introduced above is extended to realize hierarchical
perception and planning, through bottom-up and top-down message
passing between the two levels of the hierarchical generative model
shown in Fig.7.
During perception,ﬁrst, the low-level states
1 is inferred given
observationsot and level 1 actionsa1
t using the inference mechanism
implemented in PyMDP91. When the free energy of the lowest level
reaches a pre-speciﬁed threshold, a message containing the most likely
level 1 states1
t t h ea g e n ti si ni sp a s s e dt ot h el e v e la b o v e( l e v e l2 ) .T h i s
threshold is chosen to be reached when the level 1-preference is
reached, and uncertainty is below aﬁxed level. When level 2 gets this
bottom-up message, it queries the environment for the presence of
reward and observesr
τ. This conjunction of observations, together
with high-level actiona2 is used to infer the belief over the level 2 state
Q(s2), in the same way as done for level 1.
During planning, i.e., generating a sequence of actions that
(should) drive the agent toward its goal, policies are generated in a top-
down manner, in the sense that goals set at level 2 determine the prior
preference (and then in turn the policy) at level 1. First, the level 2
policy is selected by sampling from the approximate posterior overπ
2
proportional to its expected free energy. The action selected at this
level then conditions the policy at level 1π1, for which in turn the
approximate posteriorQ(π1) is computed. The low-level actiona1
t is
then sampled according to this distribution, similar to the hierarchical
generative model described by ref.92.
As depicted in Fig.7, the policy of the lower level sets a limited
amount of steps. In particular, when the preference of the lower level is
reached, a message is passed to the upper level, transitioning it into the
next state. In this way, inferring the policy at the lower level only
considers one action of the upper level at a time.
The posterior over the level 2 policyQ(π
2)i sﬁrst inferred, as it has
no dependencies from above. To do this, the expected free energy is
computed, for which we look one time step ahead in our imple-
mentation as this could predict the next corridor in which the agent
encounters reward. We consider Eq. (7), for which the observations are
now a conjunction of rewardr
τ and level 1-statess1:
G2ðπ2, τÞ ≥ /C0 EQ rτ , s1
t = τ jπ2ðÞ DKL Qðs2
τ jrτ , s1
t , π2ÞjjQ s2
τ jπ2/C0/C1/C2/C3/C2/C3
/C0 EQ rτ , s1
t = τ jπ2ðÞ ln P rτ , s1
t = τ
/C0/C1/C2/C3
,
ð7Þ
Note that the level 1 state is synchronized with the level 2 state,
and only the state observed at this synchronized time is considered,
h e n c ew ed e n o t e dt h et i m ei n d e xf o rt h el e v e l1s t a t eb yt = τ.I n
practice, we used a temperature (γ) value of 0.5 for this level. For the
habit policyE, we use a categorical distribution over action, that is
conditioned on the current state of the agent. The actiona
2
τ is then
sampled according toQ(π2).
The agent, however, is not able to act upon the physical world yet
with this level 2 action. The agent now has to infer the posterior dis-
tribution over the policy at level 1Q(π
1). Because we now want to move
towards a speciﬁc state, i.e., a disambiguated observation, we set the
preference in state space for computing the expected free energy at
level 1. In practice, Eq. (6), for level 1 then boils down to:
G1ðπ1, tÞ ≥ /C0 EQ ot jπ1ðÞ DKL Q s1
t jot , π1/C0/C1
jjQ s1
t jπ1/C0/C1/C2/C3/C2/C3
/C0 EQ s1
t jπ1ðÞ ln P s1
t
/C0/C1/C2/C3
,
ð8Þ
where theﬁrst term yields the expected information gain after pur-
suing policyπ1, and the latter the expected utility. In other words, how
close the expected state is from the preferred state, set by the action of
the level above. This quantity is then used to approximate the pos-
terior over the policy at level 1 from which actiona
1
t is sampled. At this
level, we set a temperature (γ) value of 0.5 and a uniform habit dis-
tribution. Crucially, in order to evaluateG1 at planning depth 1, we set
the prior preference (C matrix) as such that the preference for each
state is proportional to the distance to the goal, i.e., intuitively this
means that the agent can follow a breadcrumb trail towards the goal,
given that it properly inferred the current state.
Casting CSCGs as partially observable Markov decision
processes
CSCGs can be used directly to plan7. However, in this work, we
use the two learned CSCGs to create a hierarchical generative
model for active inference— and then use HAI to solve the spatial
alternation tasks.
Using the two CSCGs to create a hierarchical generative model for
active inference requires mapping them into two POMDPs39,t h e
mathematical framework used in discrete-time active inference.
Practically, a POMDP is described by a set of four arrays as shown
in Fig.8. We describe these arrays using the symbol notation typically
adopted in discrete time active inference36.T h eA matrix encompasses
the likelihood modelP(o∣s), or how states are mapped to observations.
The B tensor entails the transition modelP(st+1∣st, at), or how states
change over time, conditioned on the selected action. TheC vector
sets the prior over future observations or states, depending on the
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 12
implementation. This vector is used within active inference to embed
the preference or goal state/observation into the agent. Finally, theD
vector describes the initial belief over the stateP(s).
As mentioned earlier, clone-structured cognitive graphs are a
special case of HMM and can be therefore easily mapped to POMDPs
(which extend HMMs, too). First, we consider the mapping of the
likelihood. ThisA matrix represents the mapping of observation to
state, speciﬁcally A
i,j = P(oi∣sj). The matrix can be constructed through
the deﬁnition of the CSCG, a deterministic mapping of clone state to its
observation is set:P(oi∣sj)=1 ∀ sj ∈ C(oi)a n dP(oi∣sj)=0 ∀ sj ∉ C(oi),
where C(oi) denotes all clone states for observationoi.W h e nc o n -
structing the model we only consider states for which the marginalized
probability P(s
j) surpasses a threshold of 0.0001. Finally, we add an
additional dummy state to the additional dummy observation, to
which unlikely actions are mapped (see below).
To deﬁne the transition model, orB tensor, we use the learned
parameters of the model. Through the learning process described in
Section “Clone-structured cognitive graphs (CSCG) for learning cog-
nitive maps of physical and task space”, the CSCG has learned the
transition probabilityP(s
t+1∣st, at). This is parameterized as a tensor for
which Bi,j,k = P(si∣sj, ak). We construct this tensor with the learned
probabilities, for the same states that are considered in the likelihood
matrix. Note that for ease of implementation, the active inference
routines that we used require that any action could be executed from
any state. However, in our task, some actions are not available or highly
unlikely in some states (e.g., turning actions in a corridor). To handle
this, we created a dummy state in the POMDP, which corresponds to
observation 20 in Fig.2bt ow h i c hw em a pt h e“highly unlikely” actions
and to which we assign a large negative value (see below) to ensure
that it is never selected during planning. In practice, we set a transition
probability of 10
−12 from every state to the dummy state (after which we
re-normalize the tensor to sum to one for each action and state). We
also set a self-transition of 1 for the dummy state, therefore ensuring
that it is“absorbing”.
We endow the active inference agent with prior preferences to
secure rewards in the spatial alternation tasks in theC matrix. For ease
of implementation, we set these prior preferences manually, rather
than extracting them from the learned parameters of the CSCG. For
level 2, we set a preference over the conjunction of observations where
the reward is 1 to encourage the agent to follow the learned rule. As
described above, when the level 2 action is selected, it sets theC matrix
of level 1 to a large preference for the preferred state and to a value
proportional to the distance (in the learned state space of level 1) from
said given state to the preferred state (except for the dummy state, for
which theC vector has aﬁx e dv a l u e .T h i si ss e tt ot h el o w e s tv a l u e ,
making the dummy state the least preferred state).
The prior over the state is parameterized by theD matrix. We
parameterize this as a uniform distribution over all the states, except
for the dummy state. This reﬂects the fact that in our simulations, the
active inference agent is placed in the W-maze with a random pose and
has to self-localize.
Finally, the habit, or the prior over action is parameterized by the
E tensor. For level 1, we parameterize this as a uniform distribution. For
level 2, we model this habit as a Dirichlet distribution, that is condi-
tioned on the current state of the agent and is proportional to actions
that were rewarding during learning.
Using these tensors, inference at each single level can be imple-
mented as a Bayesﬁlter, iteratively computing the posterior over state
at each timestep using the following update:
qs
t = σðA /C1 ot + Bπt/C0 1
/C1 qst/C0 1Þ, ð9Þ
where qst denotes the parameters of the categorical distribution over
state, i.e.,Qðsτ joτ , πÞ =C a tðqs1
τ Þ, ot represents the observation as a
one-hot vector,σ denotes the softmax function, and theB-tensor is
sliced by the policyπt−1
33.I nt h eﬁrst timestep,qs0 is initialized as the
prior matrixD.
The expected free energyG for a considered policyπt can then be
evaluated using these tensors for each future timestepτ.A sw es p e c i f y
the preference over state for theﬁrst level, and over observation for
the second level, the distinction is made explicitly. For theﬁrst level,
this boils down to36:
G1
τ =d i a gA1>
/C1 ln A1
/C16/C17
/C1 qs1
τ /C0 qo1
τ /C1 ln qo1
τ + qs1
τ /C1 ln C1, ð10Þ
where the superscript1 denotes theﬁrst level,qs1
τ denotes the para-
meters of the categorical distribution over state, i.e.,
Qðs
1
τ jo1
τ , π1
t Þ =C a tðqs1
τ Þ, which is computed as qs1
τ = B1, π2
t
/C1 qs1
t .T h e
parameters of the categorical over the observation is denoted byqo1
τ ,
i.e., Qðo1
τ Þ =C a tðqo1
τ Þ,a n di sc o m p u t e da sqo1
τ = A1 /C1 qs1
τ . The same
equation can be used for the expected free energy at the second level,
except now the preference is speciﬁed as an expected observation:
G2
τ =d i a gA2>
/C1 ln A2
/C16/C17
/C1 qs2
τ /C0 qo2
τ /C1 ln qo2
τ + qo2
τ /C1 ln C2, ð11Þ
where the superscript 2 denotes the second level.
Qðs2
τ jo2
τ , π1
t Þ =C a tðqs2
τ Þ, which is computed asqs2
τ = B2, π2
t
/C1 qs2
t .T h e
parameters of the categorical over the observation at level 2 are
denoted by qo
2
τ , i.e., Qðo2
τ Þ =C a tðqo2
τ Þ, and is computed as
qo2
τ = A2 /C1 qs2
τ . The expected free energyG for a policyπ c a nt h e nb e
acquired by summing over time horizonT: G = PT
τ = t +1 Gτ and the
posterior over the policies is achieved throughQðπÞ = σðγ /C1 G +l nEÞ,
where G is a vector with the expected free energy for all considered
policies.
Reporting summary
Further information on research design is available in the Nature
Portfolio Reporting Summary linked to this article.
Code availability
We provide the code for replicating the simulations athttps://github.
com/toonvdm/bridging-cognitive-maps93.
References
1. Tolman, E. C. Cognitive maps in rats and men.Psychol. Rev.55,
189 (1948).
2. O ’Keefe, J. & Nadel, L. Précis of O’Keefe & Nadel’s The hippocampus
as a cognitive map.Behav. Brain Sci.2,4 8 7–494 (1979).
3. O ’Keefe, J. & Dostrovsky, J. The hippocampus as a spatial map.
Preliminary evidence from unit activity in the freely-moving rat.
Brain Res.34,1 7 1–175 (1971).
4 . H a f t i n g ,T . ,F y h n ,M . ,M o l d e n ,S . ,M o s e r ,M . - B .&M o s e r ,E .I .M i c r o -
structure of a spatial map in the entorhinal cortex.Nature 436,
801–806 (2005).
5. Whittington, J. C. et al. The Tolman-Eichenbaum machine: unifying
space and relational memory through generalization in the hippo-
campal formation.Cell 183,1 2 4 9–1263.e23 (2020).
6 . W h i t t i n g t o n ,J .C .R . ,M c C a f f a r y ,D . ,B a k e r m a n s ,J .J .W .&B e h r e n s ,T .E .
J. How to build a cognitive map.Nat. Neurosci.25,1 2 5 7–1272 (2022).
7. George, D. et al. Clone-structured graph representations enable
ﬂexible learning and vicarious evaluation of cognitive maps.Nat.
Commun. 12,2 3 9 2( 2 0 2 1 ) .
8. Raju, R. V., Guntupalli, J. S., Zhou, G., Lazaro-Gredilla, M. & George,
D. Space is a latent sequence: a theory of the hippocampus.Sci.
Adv. 10,3 1( 2 0 2 4 ) .
9. Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. The hippo-
campus as a predictive map.Nat. Neurosci.20,1 6 4 3–1653 (2017).
10. Stoianov, I., Maisto, D. & Pezzulo, G. The hippocampal formation as
a hierarchical generative model supporting generative replay and
continual learning.Progr. Neurobiol.217, 102329 (2022).
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 13
11. Chen, Y., Zhang, H., Cameron, M. & Sejnowski, T. Predictive
sequence learning in the hippocampal formation.Neuron 112,
2645–2658 (2024).
12. Levenstein, D., Efremov, A., Eyono, R. H., Peyrache, A. & Richards, B.
A. Sequential predictive learning is a unifying theory for hippo-
campal representation and replay. Preprint athttps://www.biorxiv.
org/content/10.1101/2024.04.28.591528v1(2024).
13. Recanatesi, S. et al. Predictive learning as a network mechanism for
extracting low-dimensional latent space representations.Nat.
Commun. 12,1 4 1 7( 2 0 2 1 ) .
1 4 . B e l l m u n d ,J .L . ,G ä r d e n f o r s ,P . ,M o s e r ,E .I .&D o e l l e r ,C .F .N a v i -
gating cognition: spatial codes for human thinking.Science 362,
eaat6766 (2018).
15. Epstein, R. A., Patai, E. Z., Julian, J. B. & Spiers, H. J. The cognitive
map in humans: spatial navigation and beyond.Nat. Neurosci.20,
1504–1513 (2017).
16. Buzsáki, G. & Moser, E. I. Memory, navigation and theta rhythm in
the hippocampal-entorhinal system.Nat. Neurosci.16,
130–138 (2013).
17. Niv, Y. Learning task-state representations.Nat. Neurosci.22,
1544–1553 (2019).
18. Schuck, N., Cai, M., Wilson, R. & Niv, Y. Human orbitofrontal cortex
represents a cognitive map of state space.Neuron 91,
1402–1412 (2016).
19. Basu, R. et al. The orbitofrontal cortex maps future navigational
goals. Nature 599, 449–452 (2021).
2 0 . I t o ,H .T . ,Z h a n g ,S . - J . ,W i t t e r ,M .P . ,M o s e r ,E .I .&M o s e r ,M . - B .A
prefrontal–thalamo–hippocampal circuit for goal-directed spatial
navigation.Nature 522,5 0–55 (2015).
21. Pezzulo, G., Verschure, P. F., Balkenius, C. & Pennartz, C. M. The
principles of goal-directed decision-making: from neural mechan-
isms to computation and robotics.P h i l o s .T r a n s .R .S o c .BB i o l .S c i .
369,2 0 1 3 0 4 7 0( 2 0 1 4 ) .
22. Verschure, P. F., Pennartz, C. M.&P e z z u l o ,G .T h ew h y ,w h a t ,w h e r e ,
when and how of goal-directed choice: neuronal and computa-
tional principles.P h i l o s .T r a n s .R .S o c .BB i o l .S c i .369,
20130483 (2014).
23. Jadhav, S. P., Kemere, C., German, P. W. & Frank, L. M. Awake hip-
pocampal sharp-wave ripples support spatial memory.Science
336,1 4 5 4–1458 (2012).
24. Benchenane, K., Tiesinga, P. H. & Battaglia, F. P. Oscillations in the
prefrontal cortex: a gateway to memory and attention.Curr. Opin.
Neurobiol.
21,4 7 5–485 (2011).
25. Shin, J. D. & Jadhav, S. P. Multiple modes of hippocampal-prefrontal
interactions in memory-guided behavior.Curr. Opin. Neurobiol.40,
161–169 (2016).
26. Colgin, L. L. Oscillations and hippocampal–prefrontal synchrony.
Curr. Opin. Neurobiol.21,4 6 7–474 (2011).
27. Siapas, A. G., Lubenov, E. V. & Wilson, M. A. Prefrontal phase locking
to hippocampal theta oscillations.Neuron 46,1 4 1–151 (2005).
28. Khodagholy, D., Gelinas, J. N. & Buzsáki, G. Learning-enhanced
coupling between ripple oscillations in association cortices and
hippocampus.Science 358,3 6 9–372 (2017).
29. Jones, M. W. & Wilson, M. A. Theta rhythms coordinate
hippocampal-prefrontal interactions in a spatial memory task.PLoS
Biol. 3,e 4 0 2( 2 0 0 5 ) .
30. Tang, W., Shin, J. D. & Jadhav, S. P. Multiple time-scales of decision-
making in the hippocampus and prefrontal cortex.eLife 10,
e66227 (2021).
31. Den Bakker, H., Van Dijck, M., Sun, J.-J. & Kloosterman, F.
Sharp-wave ripple associated activity in the medial prefrontal
cortex supports spatial rule switching.Neuroscience 42,
112959 (2022).
32. Friston, K. The free-energy principle: a uniﬁed brain theory?Nat.
Rev. Neurosci.11,1 2 7–138 (2010).
3 3 . F r i s t o n ,K . ,F i t z G e r a l d ,T . ,R i g o l i ,F . ,S c h w a r t e n b e c k ,P .&P e z z u l o ,G .
Active inference: a process theory.Neural Comput.29,1 –49 (2017).
34. Bogacz, R. A tutorial on the free-energy framework for modelling
perception and learning.J. Math. Psychol.76,1 9 8–211 (2017).
35. Buckley, C. L., Kim, C. S., McGregor, S. & Seth, A. K. The free energy
principle for action and perception: a mathematical review.J. Math.
Psychol.81,5 5–79 (2017).
3 6 . P a r r ,T . ,P e z z u l o ,G .&F r i s t o n ,K .J .Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior(The MIT Press, 2022).
37. Smith, R., Friston, K. J. & Whyte, C. J. A step-by-step tutorial on
active inference and its application to empirical data.J. Math. Psy-
chol. 107, 102632 (2022).
38. Isomura, T., Kotani, K., Jimbo, Y. & Friston, K. J. Experimental vali-
dation of the free-energy principle with in vitro neural networks.
Nat. Commun.14,4 5 4 7( 2 0 2 3 ) .
39. Kaelbling, L. P., Littman, M. L. & Cassandra, A. R. Planning and acting
in partially observable stochastic domains.Artif. Intell.101,
99–134 (1998).
40. Chevalier-Boisvert, M., Willems,L. & Pal, S. Minimalistic gridworld
environment for gymnasium.Adv. Neural Inf. Process. Syst.101,
8024–8035 (2018).
41. Schwartenbeck, P. et al. Computational mechanisms of curiosity
and goal-directed exploration.eLife 8,e 4 1 7 0 3( 2 0 1 9 ) .
42. Spiers, H. J. Keeping the goal in mind: prefrontal contributions to
spatial navigation.Neuropsychologia46,2 1 0 6–2108 (2008).
4 3 . P a t a i ,E .Z .&S p i e r s ,H .J .T h ev e r s a t i l ew a yﬁnder: prefrontal con-
tributions to spatial navigation.Trends Cogn. Sci.25,
520–533 (2021).
44. Simons, J. S. & Spiers, H. J. Prefrontal and medial temporal lobe
interactions in long-term memory.Nat. Rev. Neurosci.4,
637–648 (2003).
45. Sun, W. et al. Learning produces a hippocampal cognitive map in
the form of an orthogonalized state machine. Preprint athttps://
www.biorxiv.org/content/10.1101/2023.08.03.551900v2(2023).
46. Rens, N. et al. Evidence for entropy maximisation in human free
choice behaviour.Cognition232, 105328 (2023).
47. Schmidt, B., Duin, A. A. & Redish, A. D. Disrupting the medial pre-
frontal cortex alters hippocampal sequences during deliberative
decision making.J. Neurophysiol.121,1 9 8 1–2000 (2019).
48. McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are
complementary learning systems in the hippocampus and neo-
cortex: insights from the successes and failures of connectionist
models of learning and memory.Psychol. Rev.102, 419 (1995).
49. Lazaro-Gredilla, M., Deshpande, I., Swaminathan, S., Dave, M. &
George, D. Fast exploration and learning of latent graphs with
aliased observations. Preprint athttps://arxiv.org/abs/2303.
07397 (2023).
50. Yamashita, Y. & Tani, J. Emergence of functional hierarchy in a
multiple timescale neural network model: a humanoid robot
experiment.PLoS Comput. Biol.4, e1000220 (2008).
51. Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for
deep belief nets.Neural Comput.18,1 5 2 7–1554 (2006).
52. Stoianov, I. P., Pennartz, C. M., Lansink, C. S. & Pezzulo, G. Model-
based spatial navigation in the hippocampus-ventral striatum cir-
cuit: A computational analysis.PLoS Comput. Biol.14,
e1006316 (2018).
53. Stoianov, I., Genovesio, A. & Pezzulo, G. Prefrontal goal codes
emerge as latent states in probabilistic value learning.J. Cogn.
Neurosci.28,1 4 0–157 (2016).
54. Guntupalli, J. S. et al. Graph schemas as abstractions for transfer
learning, inference, and planning. Preprint athttps://arxiv.org/abs/
2302.07350(2023).
55. Swaminathan, S. et al. Schema-learning and rebinding as
mechanisms of in-context learning and emergence. Preprint at
https://arxiv.org/abs/2307.01201(2023).
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 14
56. Liu, K., Sibille, J. & Dragoi, G. Preconﬁgured patterns are the primary
driver of ofﬂine multi-neuronal sequence replay.Hippocampus29,
275–283 (2019).
57. Farzanfar, D., Spiers, H. J., Moscovitch, M. & Rosenbaum, R. S. From
cognitive maps to spatial schemas.Nat. Rev. Neurosci.24,
63–79 (2023).
58. Collins, A. G. & Frank, M. J. Cognitive control over learning: creat-
ing, clustering, and generalizing task-set structure.Psychol. Rev.
120,1 9 0( 2 0 1 3 ) .
59. Gershman, S. J. & Blei, D. M. A tutorial on Bayesian nonparametric
models. J. Math. Psychol.56,1 –12 (2012).
60. Wang, J. X. et al. Prefrontal cortex as a meta-reinforcement learning
system. Nat. Neurosci.21,8 6 0–868 (2018).
61. Lanillos, P. et al. Active inference in robotics and artiﬁcial agents:
survey and challenges. Preprint athttps://arxiv.org/abs/2112.
01871 (2021).
62. Da Costa, L., Lanillos, P., Sajid, N., Friston, K. & Khan, S. How active
inference could help revolutionise robotics.Entropy24, 361 (2022).
63. Taniguchi, T. et al. World models and predictive coding for cogni-
tive and developmental robotics: frontiers and challenges.Adv.
Robot. 37,7 8 0–806 (2023).
6 4 . D o n n a r u m m a ,F . ,M a i s t o ,D .&P e z z u l o ,G .P r o b l e ms o l v i n ga s
probabilistic inference with subgoaling: explaining human suc-
cesses and pitfalls in the tower of hanoi.PLoS Comput. Biol.12,
e1004864 (2016).
65. Çatal, O., Verbelen, T., Van de Maele, T., Dhoedt, B. & Safron, A.
Robot navigation as hierarchical active inference.Neural Netw.142,
192–204 (2021).
66. Zakharov, A., Guo, Q. & Fountas, Z. Variational predictive routing
with nested subjective timescales. Preprint athttps://arxiv.org/abs/
2110.11236(2021).
67. Van de Maele, T., Dhoedt, B., Verbelen, T. & Pezzulo, G. Integrating
cognitive map learning and active inference for planning in
ambiguous environments. InInternational Workshop on Active
Inference,2 0 4–217 (Springer Nature Switzerland, Cham, 2023).
68. Wilson, M. A. & McNaughton, B. L. Reactivation of hippocampal
ensemble memories during sleep.Science 265,6 7 6–679 (1994).
69. Foster, D. J. Replay comes of age.Annu. Rev. Neurosci.40,
581–602 (2017).
70. Pfeiffer, B. E. & Foster, D. J. Hippocampal place-cell sequences
depict future paths to remembered goals.Nature 497,
74–79 (2013).
71. Liu, Y., Dolan, R. J., Kurth-Nelson, Z. & Behrens, T. E. Human replay
spontaneously reorganizes experience.Cell 178,
640–652.e14 (2019).
72. Lansink, C. S., Goltstein, P. M., Lankelma, J. V., McNaughton, B. L. &
Pennartz, C. M. Hippocampus leads ventral striatum in replay of
place-reward information.PLoS Biol.7, e1000173 (2009).
73. Peyrache, A., Khamassi, M., Benchenane, K., Wiener, S. I. & Battaglia,
F. P. Replay of rule-learning related neural patterns in the prefrontal
cortex during sleep.Nat. Neurosci.12,9 1 9–926 (2009).
74. Wittkuhn, L. & Schuck, N. W. Dynamics of fmri patterns reﬂect sub-
second activation sequences and reveal replay in human visual
cortex. Nat. Commun.12, 1795 (2021).
75. Liu, K., Sibille, J. & Dragoi, G. Generative predictive codes by mul-
tiplexed hippocampal neuronal tuplets.Neuron 99,
1329–1341 (2018).
76. Buzsáki, G. Hippocampal sharp wave-ripple: a cognitive biomarker
for episodic memory and planning.Hippocampus25,
1073–1188 (2015).
77. Buzsáki, G.The Brain from Inside Out(Oxford University
Press, 2019).
78. Gupta, A. S., Van Der Meer, M. A., Touretzky, D. S. & Redish, A. D.
Hippocampal replay is not a simple function of experience.Neuron
65, 695–705 (2010).
79. Nour, M. M., Liu, Y., Arumuham, A., Kurth-Nelson, Z. & Dolan, R. J.
Impaired neural replay of inferred relationships in schizophrenia.
Cell 184,4 3 1 5–4328 (2021).
8 0 . P e z z u l o ,G . ,V a nd e rM e e r ,M .A . ,L a n s i n k ,C .S .&P e n n a r t z ,C .M .
Internally generated sequences in learning and executing goal-
directed behavior.Trends Cogn. Sci.18, 647–657 (2014).
8 1 . P e z z u l o ,G . ,K e m e r e ,C .&V a nD e rM e e r ,M .A .I n t e r n a l l yg e n e r a t e d
hippocampal sequences as a vantage point to probe future-
oriented cognition.Ann. N. Y. Acad. Sci.1396,1 4 4–165 (2017).
82. Shin, H., Lee, J. K., Kim, J. & Kim, J. Continual learning with deep
generative replay.Advances in neural information processing sys-
tems 30 (2017).
83. Mattar, M. G. & Daw, N. D. Prioritized memory access explains
planning and hippocampal replay.Nat. Neurosci.21,
1609–1617 (2018).
8 4 . P e z z u l o ,G . ,D o n n a r u m m a ,F . ,M a i s t o ,D .&S t o i a n o v ,I .P l a n n i n ga t
decision time and in the background during spatial navigation.
Curr.
O p i n .B e h a v .S c i .29,6 9–76 (2019).
85. Pezzulo, G., Zorzi, M. & Corbetta, M. The secret life of predictive
brains: what’s spontaneous activity for?Trends Cogn. Sci.25,
730–743 (2021).
86. Wittkuhn, L., Chien, S., Hall-McMaster, S. & Schuck, N. W. Replay in
minds and machines.Neurosci. Biobehav. Rev.129,3 6 7–388 (2021).
87. Kurth-Nelson, Z. et al. Replay and compositional computation.
Neuron 111,4 5 4–469 (2023).
88. Gothoskar, N., Guntupalli, J. S., Rikhye, R. V., Lázaro-Gredilla, M. &
George, D. Different clones for different contexts: hippocampal
cognitive maps as higher-order graphs of a cloned hmm. Preprint at
https://www.biorxiv.org/content/10.1101/745950v1(2019).
89. Pezzulo, G., Rigoli, F. & Friston, K. J. Hierarchical active inference: a
theory of motivated control.Trends Cogn. Sci.22,2 9 4–306 (2018).
9 0 . P e z z u l o ,G . ,R i g o l i ,F .&F r i s t o n ,K .A c t i v ei n f e r e n c e ,h o m e o s t a t i c
regulation and adaptive behavioural control.Prog. Neurobiol.134,
17–35 (2015).
91. Heins, C. et al. pymdp: A Python library for active inference in dis-
crete state spaces. Preprint athttps://arxiv.org/abs/2201.
03904 (2022).
9 2 . F r i s t o n ,K .J . ,R o s c h ,R . ,P a r r ,T . ,P r i c e ,C .&B o w m a n ,H .D e e pt e m -
poral models and active inference.Neurosci. Biobehav. Rev.77,
388–402 (2017).
93. Van de Maele, T. & Verbelen, T. toonvdm/bridging-cognitive-maps:
v1.0.0 (2024).https://zenodo.org/doi/10.5281/zenodo.13769225.
Acknowledgements
G.P. received funding from the European Union’s Horizon 2020 Frame-
work Programme for Research and Innovation under the Speciﬁc Spe-
ciﬁc Grant Agreements No. 945539 (Human Brain Project SGA3) and No.
952215 (TAILOR); the European Research Council under the Grant
Agreement No. 820213 (ThinkAhead), the Italian National Recovery and
Resilience Plan (NRRP), M4C2, funded by the European Union - Next-
GenerationEU (Project IR0000011, CUP B51E22000150006,“EBRAINS-
Italy”; Project PE0000013, CUP B53C22003630006,“FAIR”;P r o j e c t
PE0000006, CUP J33C22002970002“MNESYS”), and the PRIN PNRR
P20224FESY. The GEFORCE Quadro RTX6000 and Titan GPU cards used
for this research were donated by the NVIDIA Corporation. T.VdM was
supported by a grant for a research stay abroad by the Flanders
Research Foundation (FWO), and from the Flemish Government (AI
Research Program). The funders hadno role in study design, data col-
l e c t i o na n da n a l y s i s ,d e c i s i o nt op u b l i s h ,o rp r e p a r a t i o no ft h e
manuscript.
Author contributions
The study was conceptualized by T.VdM., T.V., and G.P., with contribu-
tions from B.D. T.VdM. wrote the software for the simulations, conducted
the experiments, did the formal analysis, and created the visualizations.
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 15
T.VdM., T.V., and G.P. designed the experiment methodology. T.V. and
G.P. guided the study. T.VdM. and G.P. wrote the manuscript, T.V. and
B.D. reviewed and commented on the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary informationThe online version contains
supplementary material available at
https://doi.org/10.1038/s41467-024-54257-3.
Correspondenceand requests for materials should be addressed to
Giovanni Pezzulo.
Peer review informationNature Communicationsthanks Sho Aoki, and
the other, anonymous, reviewers for their contribution to the peer review
of this work. A peer reviewﬁle is available.
Reprints and permissions informationis available at
http://www.nature.com/reprints
Publisher’s noteSpringer Nature remains neutral with regard to jur-
isdictional claims in published maps and institutional afﬁliations.
Open AccessThis article is licensed under a Creative Commons
Attribution-NonCommercial-NoDerivatives 4.0 International License,
which permits any non-commercial use, sharing, distribution and
reproduction in any medium or format, as long as you give appropriate
credit to the original author(s) and the source, provide a link to the
Creative Commons licence, and indicate if you modiﬁed the licensed
material. You do not have permission under this licence to share adapted
material derived from this article or parts of it. The images or other third
party material in this article are included in the article’s Creative
Commons licence, unless indicatedotherwise in a credit line to the
material. If material is not included in the article’s Creative Commons
licence and your intended use is not permitted by statutory regulation or
exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visithttp://
creativecommons.org/licenses/by-nc-nd/4.0/.
© The Author(s) 2024
Article https://doi.org/10.1038/s41467-024-54257-3
Nature Communications|         (2024) 15:9892 16