SIMULATING BIOLOGICAL INTELLIGENCE : ACTIVE INFERENCE
WITH EXPERIMENT -INFORMED GENERATIVE MODEL
A PREPRINT
Aswin Paul1, 2, Moein Khajehnejad1, Forough Habibollahi3, Brett J. Kagan3 and Adeel Razi1,4
1 Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University, Clayton 3800, Australia
2 VERSES, Los Angeles, California, USA
3 Cortical Labs Pty Ltd, Melbourne 3056, Australia
4 CIFAR Azrieli Global Scholars Program, CIFAR, Toronto, Canada
August 12, 2025
ABSTRACT
With recent and rapid advancements in artificial intelligence (AI), understanding the foundation of
purposeful behaviour in autonomous agents is crucial for developing safe and efficient systems. While
artificial neural networks have dominated the path to AI, recent studies are exploring the potential
of biologically based systems, such as networks of living biological neuronal networks. Along with
promises of high power and data efficiency, these systems may also inform more explainable and
biologically plausible models. In this work, we propose a framework rooted in active inference,
a general theory of behaviour, to model decision-making in embodied agents. Using experiment-
informed generative models, we simulate decision-making processes in a simulated game-play
environment, mirroring experimental setups that use biological neurons. Our results demonstrate
learning in these agents, providing insights into the role of memory-based learning and predictive
planning in intelligent decision-making. This work contributes to the growing field of explainable AI
by offering a biologically grounded and scalable approach to understanding purposeful behaviour in
agents.
Keywords Generative Models ¬∑ Active Inference ¬∑ Counterfactual Learning ¬∑ Biologically Inspired AI ¬∑ Explainable
Artificial Intelligence
1 Introduction
The rapid advancement of artificial intelligence (AI) underscores a critical need for a more robust foundation for
purposeful decision-making in autonomous agents, essential for developing safe and efficient systems. While artificial
neural networks (ANNs) have paved many paths in AI, their limitations in emulating the nuanced, adaptive behaviours
observed in biological systems are becoming increasingly apparent. This recognition has spurred exploration into
biological paradigms, where living neuronal networks (BNNs) may offer profound insights and efficiencies currently
beyond the reach of conventional ANNs.
Recent experimental work, such as the DishBrain system Kagan et al. [2022], exemplifies this potential. In these
experiments, cortical neurons cultured on silicon chips and integrated into a closed-loop simulated game environment
demonstrated the ability to modify their behaviour in real time. This finding, where even minimal biological substrates
exhibit adaptive intelligence through sensory feedback Kagan et al. [2022], has helped establish synthetic biological
intelligence (SBI), a paradigm leveraging living neural systems as computational substrates Kagan et al. [2023]. Such
biologically grounded approaches, alongside novel explorations in natural computation like DNA-based molecular
learning systems Evans et al. [2024], are expanding the horizons of AI by emphasising biological plausibility.
Despite these significant experimental strides, a crucial gap persists: the lack of theoretical frameworks capable of
comprehensively modelling and explaining the emergent intelligence observed in these biological systems. Existing
AI models, particularly those based on deep learning, often fall short in capturing the adaptive, memory-driven, and
arXiv:2508.06980v1  [cs.AI]  9 Aug 2025
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
embodied nature inherent in biological decision-making. To address this, our work introduces a generative model
inspired by the DishBrain experiments (see Section 2.2), leveraging active inference as a theoretical framework for
modelling behaviour. This approach allows us to simulate decision-making processes, focusing on how agents learn and
engage in predictive planning within a simulated game-play environment mirroring biological experiments (as detailed
in Section 3.1).
Our simulations demonstrate how these agents learn, offering insights into the interplay of memory-based learning
and predictive planning (Section 3.3). Furthermore, we systematically compare various decision-making schemes
(Section 3.2) and discuss the broader implications of our findings for neuroscience, AI, and embodied intelligence in
Section 4. Ultimately, this study aims to advance the development of explainable, biologically inspired AI by integrating
biologically plausible mechanisms with robust computational modelling. Our results highlight the pivotal role of
memory in decision-making, offering a contrast to conventional planning-centric AI, and contribute to the creation of
more interpretable and adaptive intelligent systems.
2 Methods
2.1 Generative models based on POMDPs
A generative model is a scaled-down version of the natural world, but not too simple Friston et al. [2023a]. The purpose
is to enable the agent to have prior expectations (about incoming observations) and simultaneously predict future
observations to control them in its favour. So, active inference, at its core, is a model-based framework. However, it
does not impose a particular structure on such models. Several models are viable for these problems, including Partially
Observable Markov Decision Processes (POMDPs) Kaelbling et al. [1998], Lovejoy [1991] and Gaussian mixture
models Pernkopf and Bouchaffra [2005], depending on the specific requirements of the task. Generative models are
gaining popularity in machine learning for various reasons, like the emergence of ChatGPT Koco¬¥n et al. [2023]. Their
popularity is also emerging in fields including weather modelling Jin et al. [2023] and protein engineering Ingraham
et al. [2023]. In this paper, we stick to the POMDP architecture, assuming the agent encodes a discrete world model
Da Costa et al. [2020] (See Sec.2.1).
POMDPs offer a universal structure to model discrete state-space environments where parameters can be expressed as
tractable categorical distributions [Kaelbling et al., 1998]. A POMDP can be formally defined as a tuple of finite sets
(S, O, U,B, A):
‚ó¶ s ‚àà S : S is a set of hidden states (s) causing observations o.
‚ó¶ o ‚àà O : O is a set of observations, where o = s, in the fully observable setting. In a partially observable
setting, o = f(s).
‚ó¶ u ‚àà U : U is a set of actions (u).
‚ó¶ B : encodes the one-step transition dynamics, P(st|st‚àí1, ut‚àí1) i.e., the probability of transitioning to state st
at time t given that action ut‚àí1 is taken in state st‚àí1 at time t ‚àí 1.
‚ó¶ A : encodes the likelihood mapping, P(oœÑ |sœÑ ), for the partially observable setting.
‚ó¶ D : Encodes the prior of the agent about the hidden state factor s.
‚ó¶ E : Encodes the prior of the agent about actions u.
In a POMDP, the hidden states ( s) generate observations ( o) through the likelihood mapping ( A) in the form of a
categorical distribution, P(oœÑ |sœÑ ) = Cat(A √ó sœÑ ).
B is a collection of square matrices Bu, where Bu represents transition dynamics P(st|st‚àí1, ut‚àí1 = u): (B) determines
the dynamics of s given the agent‚Äôs action u as P(st|st‚àí1, ut‚àí1) = Cat(But‚àí1 √ó st‚àí1). In [A √ó sœÑ ] and [BuœÑ √ó sœÑ ],
sœÑ is represented as a one-hot vector that is multiplied through regular matrix multiplication 1.
The Markovianity of POMDPs means that state transitions are independent of history (i.e. state st only depends upon
the state-action pair (st‚àí1, ut‚àí1) and not st‚àí2, ut‚àí2 etc.). The generative model can be summarised as follows,
P(o1:t, s1:t, u1:t) = P(A)P(B)P(D)P(E)
tY
œÑ=1
P(oœÑ |sœÑ , A)
tY
œÑ=2
P(sœÑ |sœÑ‚àí1, uœÑ‚àí1, B). (1)
1One-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the
others low (0). Here, the bit (1) is allocated to the state s = sœÑ
2
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
So, from the agent‚Äôs perspective, when encountering a stream of observations in time, such as(o1, o2, o3, ..., ot), as a
consequence of performing a series of actions (u1, u2, u3, ..., ut‚àí1), the generative model quantitatively couples and
quantifies the causal relationship from action to observation through some assumed hidden states of the environment.
These are called ‚Äòhidden‚Äô states because the agent cannot directly observe the hidden states s in the environment. The
agent maintains beliefs about s, using the observations o. In the experiment (Fig. A.1), the observations directly
represent the state parameters, such as the ball position without noise. Hence, the problem is more control-oriented than
a perception problem. So, throughout the paper, we assume A : that encodes the likelihood mapping (P(oœÑ |sœÑ )) is an
identity distribution.
Based on this representation, an agent can now attempt to optimise its actions to keep receiving preferred observations.
So far, the introduced generative model has no concept of ‚Äòpreference‚Äô and ‚Äògoal‚Äô. In the next few sections, we focus on
decision-making schemes in active inference, addressing variations of algorithms used to model ‚Äògoal-directed‚Äô (i.e.
purposeful) behaviour. In the next section, we lay out the experiment-informed generative model we designed and used
throughout the paper.
2.2 Experiment informed generative model
D. Y-coordinate of 
paddle
(paddle-y)
(Assumed to be part of 
the generative model as 
control is exerted. 
Modality is similar to 
ball‚Äôs y-coordinate, i.e
8 states.)
C. Y-coordinate of 
ball
(ball-y)
(Assumed since ball-y 
communicated through 
8 stimulus electrodes 
which are evenly 
spaced along the y-axis 
generating 8 possible 
place-coded positions.)
A. Structure:
‚óè State space 
(38 * 8 * 8 = 
2432 states)
‚óè Action space 
‚óè (Up, Down, and 
Stay)
‚óè Observations (Fully 
observable states.)
B. X-coordinate of ball
(ball-x)
(Assumed since ball-x is 
communicated to 
‚ÄòDishBrain‚Äô through 38 
distinct frequencies (4-41 
Hz) denoting evenly 
spaced positions, i.e
38 states.)
E. Parameters of 
interest:
‚óè B 
(Transition 
dynamics in 
POMDP)
‚óè CL (Counterfactual 
state-action 
mapping)
‚óè ùö™ 
(Risk, CL method)
‚óè C 
(Prior preference 
distribution, 
learned for DPEFE 
and AIF)
y
x
Figure 1: A: Layouts the basic structure of the generative model with dimensions of difference modalities. The
corresponding dimensions (38,8, and 8) are justified in B, C, and D. B, C, D: The observation modalities involved
are ‚Äòball-x‚Äô, ‚Äòball-y‚Äô, and ‚Äòpaddle-y‚Äô. The dimensions of each modality are determined according to the experiment
specifications. E: The parameters of interest in the generative model used in each decision-making algorithm mentioned
in this paper.
State-outcome modalities The information about the Pong game states was communicated to the population of
neurons via the DishBrain system using biologically safe voltage and frequency levels in the experiment. We assume
three state-observation modalities corresponding to the input signals in the generative model: ‚Äô ball-x‚Äô, ‚Äòball-y‚Äô, and
‚Äòpaddle-y‚Äô. They represent the ball‚Äôs x and y coordinates and the y coordinate of the middle of the paddle, respectively.
In the experiment, the x coordinate of the ball represents its distance from the paddle along the x-axis. The experiment
communicates it through a rate-coded component of 38 distinct frequencies ranging from4‚àí41 Hertz. The y coordinate
of the ball is transmitted through one of the 8 evenly distributed stimulation electrodes on the y-axis of HD-MEA (See
Fig. 1 B, C and D). Using this information, we can set up the structure of the generative model.
3
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
Structure We assume the state space of the generative model has 2432 states (See Fig. 1, A) in correspondence with
the dimensions mentioned above. 38 distinct states representing ‚Äòball-x‚Äô, and 8 different states for each of the ‚Äòball-y‚Äô
and ‚Äòpaddle-y‚Äô, respectively. The action space has three elements, namely ‚ÄòUp‚Äô, ‚ÄòDown‚Äô, and ‚ÄòStay‚Äô, the same as in the
simulated game environment of DishBrain. Finally, the observations are assumed to be fully observable states, as there
is no noise in communicating the states to the chip.
Parameters of interest In this work, we are particularly interested in some parameters of the POMDP critical to
decision-making schemes. For the full structure of the generative model, refer to Sec.2.1 in Methods. The parameters of
interest are:
‚Ä¢ B: The transition dynamics represent the knowledge about state transitions in the system. An informed B
helps predict the future states in the hope of controlling them.
‚Ä¢ CL: The counterfactual learning method Isomura et al. [2023], Paul et al. [2024a] (CFL) which depends on
the state-action mapping CL for decision-making.
‚Ä¢ Œì: The Risk term in counterfactual learning represents the agent‚Äôs confidence. A decrease/increase in Œì can
indicate a change in strategy, influencing the learning of CL mapping used in decision-making.
‚Ä¢ C: The prior preference distribution which drives control in the classical active inference agents.
We limited ourselves to the discrete-time, discrete-state POMDP architecture captivated by their explicit structure.
Several candidates exist, such as Gaussian mixture models Friston et al. [2018] for continuous problems and ANNs
Millidge et al. [2020] for representing the dynamics for high dimensional problems. An interesting observation made
while experimenting with active inference models is the influence of tiny changes to the structure and dimensions of the
model in performance. The freedom of model structure comes with the cost of fine-tuning details but opens up endless
ground for exploration. Algorithms for self-learning the structure of generative models are a promising direction to
pursue Friston et al. [2023b].
2.3 AIF: Classical formulation of decision-making in active inference
Active inference is a first-principle approach for modelling behaviour and revolves around a central postulate: every
‚Äòagent‚Äô ensures survival by minimising observations (o) that are high in ‚Äòentropy‚Äô Friston [2010]. This definition calls for
a method to evaluate the entropy of an incoming observation (o). The entropy of observation, also known as ‚Äòsurprise‚Äô
in the active inference literature Friston et al. [2023a], is defined as:
S(o) = ‚àílog(P(o)). (2)
Recent experimental works Kagan et al. [2022], Strong et al. [2024], Isomura et al. [2023] use the concept of ‚Äòsurprise‚Äô
in the active inference literature to design feedback signals for embodied agents that demonstrate improved learning
with time. In Eq. 2, P(o) represents the probability of a given observation o. It should be noted that an agent is ignorant
of the actual likelihood (Ptrue(o)) of any observation, as it is not in full control of the environment. The agent is not
directly informed of how probable an incoming observation is. Hence, to estimate P(o), the agent must internally
encode and use a model to learn and predict the probability of incoming observations. This internal model is called the
generative model Friston et al. [2012], Da Costa et al. [2020].
Traditionally, planning and decision-making by active inference agents revolve around minimising the variational free
energy of the observations one expects in the future. To implement this, we define a policy space comprising sequences
of actions in time. The policy space in classical active inference [Sajid et al., 2021] is defined as a collection of policies
œÄn:
Œ† = {œÄ1, œÄ2, ..., œÄN }, (3)
which are themselves sequences of actions indexed in time; that is,œÄ = (u1, u2, ..., uT ), where ut is one of the available
actions in U, and T is the agent‚Äôs planning horizon. N is the total number of unique policies defined by permutations of
available actions u over a time horizon of planning T.
To enable goal-directed behaviour, we must quantify the agent‚Äôs preference for sample observations o. The prior
preference for observations is usually defined as a categorical distribution over observations,
C = Cat(o). (4)
So, if the value corresponding to an observation in C is the highest, it is the most preferred observation for the agent.
Given these two additional parameters (Œ† and C), we can define a new quantity called the expected free energy (EFE)
of a policy œÄ similar to the definition in [Sajid et al., 2021, Parr and Friston, 2019] as,
4
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
G(œÄ) =
TX
t=1
DKL

Q(ot|œÄt) || C

| {z }
Risk
+ EQ(st|st‚àí1,œÄt‚àí1) [H[P(ot|st)]]| {z }
Expected ambiguity
. (5)
In Eq.(5) above, œÄt is the t-th element in œÄ, i.e. the action corresponding to time t for policy œÄ. The term, Q(ot|œÄt)
represents the most likely observation caused by the policyœÄ at time t. DKL stands for the KL-divergence, which, when
minimised, forces the distribution Q(ot|œÄt) closer towards C. This term is also called the "Risk," which measures the
risk of an action that leads to outcomes that diverge from the agent‚Äôs preferred states. Thus, actions are selected based
on their expected KL divergence ‚Äî essentially choosing actions that minimize this divergence, thereby minimizing
"risk. This represents the goal-directed behaviour of the agent. The KL-divergence between two distributions, P and Q,
is defined as:
DKL(P||Q) =
X
i
P(i)logP(i)
Q(i), (6)
and P = Q if and only if DKL(P||Q) = 0.
In the second term of Eq.5, H[P(ot|st)] stands for the (Shannon) entropy of P(ot|st) defined as,
H(P(o)) = ‚àí
X
oœµO
P(o)logP(o). (7)
The second term is also called the ‚ÄòExpected ambiguity‚Äô term. When the expected entropy of P(ot|st) w.r.t the belief
Q(st|st‚àí1, œÄt‚àí1) is lower, the agent is more confident of the state-observation mapping (i.e., A) in its generative model.
It may already be evident that the above formulation has one fundamental limitation: in the stochastic control problems
commonly encountered in practice, the size of possible action spaces, U, and the time horizons of planning, T, make
the policy space too large to be computationally tractable. For example, with eight available actions in U and a time
horizon of planning T = 15, the total number of (definable) policies that need to be considered are ( 3.5 ‚àó 1013) 35
trillion. Even for this relatively small-scale example, this policy space is not computationally tractable to simulate agent
behaviour (unless additional decision tree search methods [Fountas et al., 2020, Champion et al., 2021a,b] or policy
amortisation Fountas et al. [2020], √áatal et al. [2020] are considered or by eliminating implausible policy trajectories
using Occam‚Äôs principle). In this work, we only consider one-step planning as the problem dimensions are intractable
for planning with this approach. We refer to the agent using this method for planning as ‚ÄòAIF-1‚Äô abbreviating ‚ÄòActive
inference agent with one-step planning‚Äô.
We now describe an improved scheme that uses dynamic programming principles Bellman [1966] to redefine policy
space and planning altogether.
2.4 DPEFE: Dynamic programming in expected free energy
In a recent work Paul et al. [2021], an efficient planning algorithm was proposed using dynamic programming principles.
The algorithmic formulation generalised for a POMDP setting can be found in Paul et al. [2024b]. We summarise this
method below:
For a planning horizon of T (i.e., the agent aims to reach goal state at time T), the EFE of the (last) action for the
T ‚àí 1th time step can be written as:
G(uT‚àí1, oT‚àí1) = DKL[Q(oT | uT‚àí1, sT‚àí1) || C]. (8)
The term, G(uT‚àí1|oT‚àí1) is the expected free energy associated with any action uT‚àí1, given we are in (hidden) state
sT‚àí1. This estimate measures how much we believe that the observations at time T will align with our prior preference
C.
To estimate Q(oT |uT‚àí1, sT‚àí1), we make use of the prediction about states that can occur at time T, Q(sT ):
Q(sT | uT‚àí1, sT‚àí1) = BuT‚àí1 ¬∑ Q(sT‚àí1), (9)
and given the prediction Q(sT ), we get
Q(oT | uT‚àí1, sT‚àí1) = A ¬∑ Q(sT | uT‚àí1, sT‚àí1) (10)
= A ¬∑
 
BT‚àí1
u ¬∑ Q(sT‚àí1)

. (11)
5
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
Next, using Eq.9, the corresponding action distribution (for action selection) is calculated at time T,
Q(uT‚àí1|oT‚àí1) = œÉ (‚àíG(uT‚àí1|oT‚àí1)) . (12)
Here, we recursively calculate the expected free energy for actions and the corresponding action-distributions for time
steps T ‚àí 2, T‚àí 3, ..., t= 1 backwards in time,2
G(ut|ot) = DKL[Q(ot+1 | ut, st) || C]| {z }
EFE of action at time t
+ EQ(ot+1,ut+1|ot,ut)[G(ut+1|ot+1)]| {z }
EFE of next action at t + 1
. (13)
In the equation above, the second term condenses information about all future observations rather than doing a forward
tree search in time. To inform G(ut|ot), we consider all possible observation-action pairs that can occur in time t + 1
and use the previously evaluated G(ut+1|ot+1). In Eq.13, we evaluate Q(ot+1, ut+1|ot, ut) using,
Q(st+1, ut+1|st, ut) = Q(st+1|st, ut)| {z }
B
¬∑Q(ut+1|st+1)| {z }
Action distribution
. (14)
We then map the distribution Q(st+1, ut+1|st, ut) to the observation space and evaluate Q(ot+1, ut+1|ot, ut) using the
likelihood mapping A. In Eq.14, we assume that actions in time are independent of each other, i.e. ut is independent of
ut+1. Even though actions are assumed to be explicitly independent in time, the information (and hence desirability)
about actions is also informed backwards from the recursive evaluation of expected free energy.
While evaluating the EFE, G, backwards in time, we used the action distribution in Eq.12. This action distribution can
be directly used for action selection. Given an observation o at time t, ut may be sampled 3 from,
ut ‚àº Q(ut|ot = o). (15)
We refer to agents using the DPEFE method for decision making as ‚ÄòDP-T‚Äô, abbreviating ‚ÄòActive inference agent using
dynamic programming with planning horizon T.
2.5 CFL: Counterfactual learning
A different approach to decision-making does not rely on planning but on counterfactual learning (CFL). In the
counterfactual learning method, the agent learns a state-action mapping CL instead of evaluating EFE directly 4. This
state-action mapping is learned using a ‚ÄôRisk‚Äô parameter Œì(t) using the update equation as given in Isomura and Friston
[2020] as:
CL ‚Üê CL + t ‚ü®(1 ‚àí 2 Œì(t))‚ü®ut ‚äó st‚àí1‚ü©‚ü©. (16)
Here, ‚ü®¬∑‚ü© refers to the average over time, and ‚äó is the Kronecker-product operator. Given the state-action mapping CL,
agent samples actions from the distribution,
P(u|st‚àí1)CL = œÉ (ln CL ¬∑ st‚àí1) . (17)
The free parameter in our model is the number of past instances (of state-action pairs) the agent stores in memory use
in every time-step to learn CL in Eq.16. In this paper, ‚ÄòCFL-T‚Äô represents the active inference agent with a memory
horizon of T.
The functional form of Œì(t) used in the simulations of this work is:
Œì(t)prior = 0.55 (18)
A value of Œì greater than 0.55 corresponds to ‚Äúhigher risk‚Äù in the CFL method. An initial value greater than 0.5 is
necessary to enable learning Isomura and Friston [2020].
2For times other than T ‚àí 1, the first term in Eq.13 does not contribute to solving the particular instance if C only accommodates
preference to a (time-independent) goal-state. However, for a temporally informed C, i.e. with a separate preference for reward
maximisation at each time step, this term will meaningfully influence action selection.
3Precision of action-section may be controlled by introducing a positive constant inside the softmax function œÉ(.) in Eq.12. The
higher the constant, the higher the chance of selecting the action with less EFE.
4For the exact form of the generative model and free energy, refer to Isomura and Friston [2020]
6
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
For updating Œì, we use the equation,
Œì(t) ‚Üê Œì(t) ‚àí 1
Tgoal ‚àí t. (19)
Here, Tgoal is when the agent receives a positive environmental reward. So, the sooner the agent reaches the ‚Äògoal-state‚Äô,
the quicker the Œì(t), i.e., risk converges to zero. All the update rules this paper defines can be derived from the postulate
that the agent tries to minimise the (variational) free energy w.r.t the generative model Isomura et al. [2023], Paul et al.
[2024b]. It was observed in Paul et al. [2024a] and Khajehnejad et al. [2024] that this variation of the CFL method is
well suited for environments that require spontaneous decision-making and is data-efficient (Environments like the Cart
Pole (OpenAIGym) Barto et al. [1983], game of Pong Khajehnejad et al. [2024]).
The algorithms implemented in this paper for decision-making (DPEFE Paul et al. [2024b] and CFL Paul et al. [2024b])
are low-cost computational methods against classical active inference algorithms. The improvement in computational
complexity of these models enabled simulations in the state space consisting of more than two thousand states and has
the potential to scale to more demanding environments.
2.6 Explainability: Normalised total entropy
To examine some parameters, we define the total entropy (TE) of a parameter as,
TE(Z) =
X
XœµZ
H(X) = ‚àí
X
XœµZ
X
xœµX
p(x)log p(x). (20)
In Eq.20, Z is a parameter like the CL mapping in the counterfactual learning method, and H, the Shannon entropy.
X is a probability distribution collection comprising the parameter Z. For example, the CL mapping is made up
of probability distributions P(u|st) ‚àÄ sœµS, representing how probable an action u is, given a state s at time t. For
visualising the evolution of total entropy with time, we scale the vector storing total entropies in each time step to values
between zero and one, i.e. normalise5 it. Normalisation is advantageous to us here because it emphasises the underlying
trends rather than the absolute values. Scaling also enables comparison between groups. We call this measure the
normalised total entropy (NTE).
3 Results
3.1 Counterfactual learning method with memory demonstrates significant improvement in learning
Firstly, we simulate active inference agents using the counterfactual learning algorithm Isomura et al. [2023] for
decision-making. We use a memory-augmented version of the algorithm developed in Paul et al. [2024a], in which the
agent uses the last T time steps for learning the CL mapping, referred to as ‚ÄòCFL-T‚Äô in the paper. For more details,
refer to Sec.2.5 in Methods.
To standardise the simulation results against the experimental results, we time-stamp every trial, matching a total trial
length of twenty minutes as in the DishBrain experiments. In the biological experiments, an average of 69.04 ¬± 7.95
game episodes comprised a trial of 20 minutes Kagan et al. [2022], Khajehnejad et al. [2024]. In our simulations,
we conduct trials with 70 episodes each and add time labels to match a total trial length of 20 minutes. To ensure
reproducibility and minimise the impact of hyperparameter selection, we conduct 100 trials with different random seeds.
Given the assigned time stamps, we can compare the performance of in vitro agents to our in silico agents.
From Fig.2, we observe improvement in the performance of ‚ÄòCFL-T‚Äô agents with higher memory horizon (T) across
all game metrics. Considering only lower memory horizons (top row), the CFL-4 agent outperforms the ‚Äôbiological
agents‚Äô (HCC and MCC) across all the performance matrices. The memory horizon of 4 and above essentially implies
a higher ability to use the information from the past to improve performance. However, a memory horizon of 4 may
not be biologically plausible, given that the presence of long-term memory in the DishBrain system has not yet been
demonstrated. Notably, CFL agents with the lowest memory horizons (1-2) perform at par with the MCC and HCC
groups, and these observations motivate us to study the role of memory in systems like the DishBrain and inspire future
experiments dependent on memory.
In Fig.2 (bottom row), we observe an improvement in performance with a higher memory horizon. Long memory
horizons, for example, 16 and 32, are biologically implausible for systems like DishBrain. However, these results stress
the relevance of further studying the role of memory in such embodied systems. In Sec. 3.2, we study the performance
5To normalise a vector V , we use the sklearn machine toolkit, specifically: sklearn.preprocessing.normalize(V , norm=‚Äômax‚Äô).
7
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
A B C
FED
Figure 2: Memory horizon influences performance in CFL agents compared with MCC and HCC groups.
Changes in agent performance are assessed by comparing the first 5 minutes (green) to the last 15 minutes (orange) of
the trials. The average number of A, D: hits per rally, B, E: percentage of aces (balls missed after the initial serve),
and C, F: percentage of long rallies (‚â• 3 consecutive hits) are plotted over a 20-minute real-time equivalent for CFL
agents and biological MCC and HCC cultures. Top row (A, B, C): CFL agents with shorter memory horizons exhibit
a linear improvement in performance across all game metrics, with CFL-4 outperforming MCC and HCC groups.
However, memory horizons of 4 or higher may not be biologically plausible, as long-term memory has not yet been
demonstrated in DishBrain. Bottom row (D, E, F): Agents with longer memory horizons (e.g., 16 and 32) show
continued performance gains, although such horizons are implausible for DishBrain-like systems. These findings
highlight the role of memory in decision-making under active inference and suggest avenues for further experimental
investigations in embodied biological systems.
of different decision-making algorithms in active inference and interpret their performance. We use linear regression to
further explore agents‚Äô performance in continuous time (See Fig.B.2). Next, we perform a comparison across other
popular active inference algorithms.
3.2 Comparison across other popular active inference algorithms
In this section, we compare the classical decision-making scheme in active inference (AIF-1) Sajid et al. [2022],
Da Costa et al. [2020] and dynamic programming in expected free energy (DPEFE) Paul et al. [2024b], Friston et al.
[2023c]. For more details, refer to Sec.2.3 and Sec.2.4 in Methods.
Fig. 3 summarises the simulation results with the above-mentioned agents. For these figures, we select agents with
performance comparable to the ‚ÄòMCC‚Äô and ‚ÄòHCC‚Äô groups. From Fig. 3 we observe the following:
‚Ä¢ From Fig. 3 (top row), we observe that all ‚ÄòDP-T‚Äô agents improve similarly to the ‚ÄòHCC‚Äô group. A higher
planning horizon does not seem to help significantly with performance. Also, over a planning horizon of 10,
the performance declines. This effect can be attributed to over-planning.
8
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
A B C
FED
Figure 3: Performance of in-silico active inference agents compared to in-vitro MCC and HCC groups in a
simulated Pong environment. The game performance is evaluated across three key metrics: A, D: Average rally length
(higher is better), B, E: Percentage of long rallies (higher is better), and C, F: Percentage of aces (lower is better). Top
row (A, B, C): Active inference agents with different planning horizons (‚ÄòAIF-1‚Äô and ‚ÄòDP-T‚Äô) show improvements
comparable to the HCC group. However, a planning horizon beyond 10 leads to a decline in performance, likely due to
over-planning. Additionally, DPEFE agents perform similarly across the task, suggesting that increased planning depth
does not significantly enhance performance in Pong. Bottom row (D, E, F): The ‚ÄòCFL-3‚Äô agent outperforms all other
groups across all game metrics, reinforcing the idea that memory-based decision-making is more effective for in-game
environments like Pong than planning-based approaches. These results highlight the trade-offs between memory and
planning in active inference-based control.
‚Ä¢ Similarly, from Fig.3 (top row), all DPEFE agents perform similarly in the pong game task. This suggests
that a higher planning horizon is not helpful in the pong game environment. This observation suggests that
memory-based decision-making is more useful for in-game environments like Pong than planning algorithms
like DPEFE.
‚Ä¢ From From Fig.3 (bottom row), ‚ÄòCFL-3‚Äô group outperforms other groups in all measured game metrics
(‚ÄòAverage rally length‚Äô, ‚Äò% of Long Rallies‚Äô, and ‚Äò% of Aces‚Äô).
‚Ä¢ From Fig.3 (bottom row), we observe that the ‚ÄòAIF-1‚Äô agent demonstrates a relative improvement comparable
with the control ‚ÄòCTL‚Äô group. Evidently, with one-step planning, the agent seems to make no difference in
performance over time.
See Fig. B.1 for a different representation of the same data. In Fig. B.1 (C), we compare the relative improvement of
CFL agents with different memory horizons. We see that ‚ÄòCFL-4‚Äô outperforms every other group regarding relative
improvement.
In the next section, we examine the evolution of model parameters and draw insights from them. We leverage the model
parameters to understand the basis of performance on a deeper level.
9
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
3.3 Model parameters reveal explainable insights into the performance of the active inference agents
A B C
FED
Figure 4: Evolution of key parameters in CFL, DP-5, and AIF-1 agents. A: Evolution of the average risk term
(Œìt) in CFL agents (log scale). A significant drop in risk is observed only for the ‚ÄòCFL-4‚Äô agent, aligning with its
superior performance. B, C: Evolution of the normalised total entropy (NTE) of the transition dynamics for B: DP-5
and C: AIF-1 agents. The decreasing entropy in both cases indicates that agents are progressively learning about the
environment. D, E, F: Evolution of the NTE for key model parameters: the CL vector (D), and prior preferences (E,
F) in DP-5 and AIF-1 agents, respectively. D: The CL vector‚Äôs entropy decreases significantly for higher memory
horizons, highlighting goal-directed learning in CFL agents. E, F: In both DP-5 and AIF-1 agents, the entropy of prior
preference distributions increases over time, suggesting that no specific ball or paddle position is inherently preferred,
reinforcing that the primary objective is defending the ball rather than favouring particular game states. These findings
underscore differences in how memory-based (CFL) and planning-based (DP-5, AIF-1) agents learn and adapt in the
game environment.
Since all the parameters in our models are non-approximate, we can examine how they evolve to draw insights into the
performance and learning of the agents. We examine the parameters in time frames similar to the biological experiment.
In Fig. 4 (A), we look at the average Œìt (Risk) in CFL agents on a log scale. We see a significant drop in the risk
parameter over time, only for the ‚ÄòCFL-4‚Äô agent. This observation qualitatively matches the significant increase in
performance of the ‚ÄòCFL-4‚Äô agent as observed in Fig. 2.
To study the evolution of parameters that encode probability distributions (such asB, CL, C), we propose a measure
called the ‚ÄôNormalised total entropy (NTE)‚Äô in this paper. For more details, see Sec.2.6. NTE represents how informed
a probability distribution is; hence, a lower NTE corresponds to a more informed distribution relative to an uninformed
prior with the highest entropy. In Fig. 4 (D), we see that the NTE of the CL vector decreases with time and has the
highest decrease for higher memory horizons. This is evidence for the emergence of goal-directed learning in the CFL
agents and also results in improved performance over time, which is observed in Fig.2. We also observe the significant
decrease in the NTE for the ‚ÄòCFL-4‚Äô agent in Fig.4 (D) matching the higher performance of ‚ÄòCFL-4‚Äô in Fig.2.
10
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
Additionally, the dynamic programming agent‚Äôs model parameters reveal the method‚Äôs limitation in this environment.
In Fig. 4 (second column), we examine the parameters of the DP-5 agent. There is no particular reason for choosing
this group over other DP-T groups. In Fig. 4 (B), we look at the normalised total entropy of the transition dynamics B.
The entropy corresponding to all state-observation modalities decreases, indicating that the agent is learning about the
environment. A decreased entropy represents the probability distributions becoming more informed than a uniform
prior with maximum entropy. Similarly, in Fig. 4 (E), we see that the entropy of the prior preference distribution C
increases for all state-observation modalities. This observation is counter-intuitive as we would expect learning of some
sort, but a possible explanation is that learning a prior preference is not possible in this game since no particular ball or
paddle position is preferred. Only defending the ball in the game gives positive rewards, and no particular position of
the ball and paddle is advantageous. We interpret it as the agent is not learning a particular preference about any of the
observations, as defending the ball is the goal, and a particular position of the ball and paddle needs to be favoured.
Similar to the DP-5 agent, in Fig.4 (last column), we examine the model parameters of the ‚ÄòAIF-1‚Äô agent and observe a
similar trend. From Fig.4 (C), we see that the entropy of the transition dynamics of all modalities decreases, which
provides evidence that the agent is learning more about the environment. Interestingly, the entropy of the prior preference
distribution increases, as seen in Fig.4(B).
We leave the detailed analysis of parameters and testing different structures of the generative models to future work.
The insights from this work will enable analysis of similar future experiments, opening a paradigm of probing into the
basis of intelligence.
4 Discussion
We successfully demonstrate the efficacy of the active inference framework in modelling purposeful decision-making
within a synthetic biological intelligence context, marking a significant step in understanding such systems. Our
simulations reveal that agents employing memory-based learning and predictive planning can efficiently navigate and
adapt within dynamic environments. The active inference approach provides a biologically plausible and inherently
interpretable model for how systems, akin to biological neuronal networks, can achieve continuous learning and
real-time responsiveness. By unifying perception, action, and learning under a single probabilistic calculus, active
inference emerges as a compelling alternative to traditional machine learning paradigms, particularly for developing
biologically inspired AI.
The advantages of this approach are underscored when considering the remarkable sample efficiency of biological
systems, a trait many artificial systems, including deep reinforcement learning (RL) agents, struggle to emulate
Khajehnejad et al. [2024]. Our adoption of active inference aims to capture not only this efficiency but also the crucial
elements of biological plausibility and interpretability, aspects where opaque, extensively-tuned RL models often fall
short Friston [2010, 2012]. This makes active inference particularly well-suited for modelling systems like DishBrain,
where the interplay of external stimuli and emergent behaviour necessitates a probabilistic framework adept at managing
uncertainty and facilitating structured learning.
The analysis of key model parameters, such as the risk term ( Œì) and the normalized total entropy (NTE) of the CL
vector, offers explainable insights into the agents‚Äô learning dynamics. For instance, the observed reduction in risk and
NTE in high-performing agents signifies increasing confidence and refinement in their decision-making strategies over
time Paul et al. [2024a], supporting the emergence of goal-directed behaviour. These findings are pivotal for advancing
safe and transparent AI systems Albarracin et al. [2023], as they allow us to trace how uncertainty is minimised through
adaptive exploration, thereby bridging experimental neuroscience with theoretical models of intelligence.
A critical consideration for future work is the biological instantiation of memory mechanisms. While our simulations
show enhanced performance in CFL agents with extended memory, current experimental evidence from systems like
DishBrain indicates rapid network reorganisation and population-wide dynamics rather than explicit memory recall
Kagan et al. [2022], Habibollahi et al. [2023]. This highlights an important avenue for investigation: determining
whether sophisticated memory, as modelled, is essential for biological learning, or if rapid adaptive restructuring alone
underpins the observed behaviours.
Future research should therefore focus on dissecting the roles of different memory types and adaptive dynamics in both
synthetic and biological agents. Learning more about these distinctions will be instrumental in refining our models and
could pave the way for innovative hybrid systems that synergise the adaptive strengths of biological intelligence with
the explanatory power of computational frameworks like active inference. Ultimately, this work contributes to a more
integrated understanding of purposeful behaviour, pushing forward the development of AI that is not only intelligent
but also transparent and grounded in biological principles.
11
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
5 Acknowledgments
AP currently works at VERSES Inc., California, USA and acknowledges PhD fellowship from the Department of
Biotechnology, Government of India received from 2019 to 2023. AR is funded by the Australian Research Council
(Refs: DE170100128 & DP200100757) and the Australian National Health and Medical Research Council Investigator
Grant (Ref: 1194910). AR is a CIFAR Azrieli Global Scholar in the Brain, Mind & Consciousness Program. AR, NS,
and LD are affiliated with The Wellcome Centre for Human Neuroimaging, supported by core funding from Wellcome
[203147/Z/16/Z]. B.J.K. & F.H. are employees of and hold options or shares in Cortical Labs Pte Ltd.
6 Software note
All the code for active inference agents, pong-game environment, and visualisation used are custom written in Python
3.9.15 and is available in this project repository: https://github.com/aswinpaul/pong_ai_2023.
References
Brett J. Kagan, Andy C. Kitchen, Nhi T. Tran, Forough Habibollahi, Moein Khajehnejad, Bradyn J. Parker, Anjali
Bhat, Ben Rollo, Adeel Razi, and Karl J. Friston. In vitro neurons learn and exhibit sentience when embodied in a
simulated game-world. Neuron, 110:3952‚Äì3969.e8, Dec 2022.
Brett J Kagan, Christopher Gyngell, Tamra Lysaght, Victor M Cole, Tsutomu Sawai, and Julian Savulescu. The
technology, opportunities and challenges of synthetic biological intelligence. Biotechnology advances, page 108233,
2023.
Constantine Glen Evans, Jackson O‚ÄôBrien, Erik Winfree, and Arvind Murugan. Pattern recognition in the nucleation
kinetics of non-equilibrium self-assembly. Nature, 625(7995):500‚Äì507, January 2024.
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzh√∂ffer, Grigorios A. Pavliotis, and Thomas
Parr. The free energy principle made simpler but not too simple. Physics Reports, 1024:1‚Äì29, 2023a. ISSN 0370-
1573. doi:https://doi.org/10.1016/j.physrep.2023.07.001. URL https://www.sciencedirect.com/science/
article/pii/S037015732300203X. The free energy principle made simpler but not too simple.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(1):99‚Äì134, 1998. ISSN 0004-3702. doi:https://doi.org/10.1016/S0004-
3702(98)00023-X. URL https://www.sciencedirect.com/science/article/pii/S000437029800023X.
William S. Lovejoy. A survey of algorithmic methods for partially observed markov decision processes. Annals of
Operations Research, 28(1):47‚Äì65, 1991. ISSN 1572-9338. doi:10.1007/BF02055574. URL https://doi.org/
10.1007/BF02055574.
F. Pernkopf and D. Bouchaffra. Genetic-based em algorithm for learning gaussian mixture models. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 27(8):1344‚Äì1348, 2005. doi:10.1109/TPAMI.2005.162.
Jan Koco¬¥n, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd≈Ço, Joanna Baran, Julita Bielaniewicz,
Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Koco ¬¥n, Bart≈Çomiej Koptyra, Wiktoria Mieleszczenko-
Kowszewicz, Piotr Mi≈Çkowski, Marcin Oleksy, Maciej Piasecki, ≈Åukasz Radli ¬¥nski, Konrad Wojtasik, Stanis≈Çaw
Wo¬¥ zniak, and Przemys≈Çaw Kazienko. Chatgpt: Jack of all trades, master of none.Information Fusion, 99:101861,
2023. ISSN 1566-2535. doi:https://doi.org/10.1016/j.inffus.2023.101861. URL https://www.sciencedirect.
com/science/article/pii/S156625352300177X.
Kyo-Hoon Jin, Kyung-Su Kang, Baek-Kyun Shin, June-Hyoung Kwon, Soo-Jin Jang, Young-Bin Kim, and Han-Guk
Ryu. Development of robust detector using the weather deep generative model for outdoor monitoring system.Expert
Systems with Applications, 234:120984, 2023. ISSN 0957-4174. doi:https://doi.org/10.1016/j.eswa.2023.120984.
URL https://www.sciencedirect.com/science/article/pii/S0957417423014860.
John B. Ingraham, Max Baranov, Zak Costello, Karl W. Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M.
Lord, Christopher Ng-Thow-Hing, Erik R. Van Vlack, Shan Tie, Vincent Xue, Sarah C. Cowles, Alan Leung, Jo√£o V .
Rodrigues, Claudio L. Morales-Perez, Alex M. Ayoub, Robin Green, Katherine Puentes, Frank Oplinger, Nishant V .
Panwar, Fritz Obermeyer, Adam R. Root, Andrew L. Beam, Frank J. Poelwijk, and Gevorg Grigoryan. Illuminating
protein space with a programmable generative model. Nature, 623(7989):1070‚Äì1078, 2023. ISSN 1476-4687.
doi:10.1038/s41586-023-06728-8. URL https://doi.org/10.1038/s41586-023-06728-8 .
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active in-
ference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology , 99:102447, 2020. ISSN
12
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
0022-2496. doi:10.1016/j.jmp.2020.102447. URL https://www.sciencedirect.com/science/article/
pii/S0022249620300857.
Takuya Isomura, Kiyoshi Kotani, Yasuhiko Jimbo, and Karl J. Friston. Experimental validation of the free-energy princi-
ple with in vitro neural networks. Nature Communications, 14(1):4547, 2023. ISSN 2041-1723. doi:10.1038/s41467-
023-40141-z. URL https://doi.org/10.1038/s41467-023-40141-z .
Aswin Paul, Takuya Isomura, and Adeel Razi. On predictive planning and counterfactual learning in active inference.
Entropy, 26(6), 2024a. ISSN 1099-4300. URL https://www.mdpi.com/1099-4300/26/6/484.
Karl J Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and active
inference. Neuroscience & Biobehavioral Reviews, 90:486‚Äì501, 2018.
Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. On the relationship between active
inference and control as inference. In Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom,
editors, Active Inference, pages 3‚Äì11, Cham, 2020. Springer International Publishing. ISBN 978-3-030-64919-7.
doi:https://link.springer.com/chapter/10.1007/978-3-030-64919-7_1.
Karl J Friston, Lancelot Da Costa, Alexander Tschantz, Alex Kiefer, Tommaso Salvatori, Victorita Neacsu, Magnus
Koudahl, Conor Heins, Noor Sajid, Dimitrije Markovic, et al. Supervised structure learning, 2023b.
Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127‚Äì138, 2010.
ISSN 1471-0048. doi:10.1038/nrn2787. URL https://doi.org/10.1038/nrn2787.
Vincent Strong, William Holderbaum, and Yoshikatsu Hayashi. Electro-active polymer hydrogels exhibit emergent
memory when embodied in a simulated game environment. Cell Reports Physical Science, 2024/08/28 2024. ISSN
2666-3864. doi:10.1016/j.xcrp.2024.102151. URL https://doi.org/10.1016/j.xcrp.2024.102151.
Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: optimal control without cost
functions. Biological Cybernetics, 106(8):523‚Äì541, 2012. ISSN 1432-0770. doi:10.1007/s00422-012-0512-8. URL
https://doi.org/10.1007/s00422-012-0512-8 .
Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active inference: Demystified and compared. Neural
Computation, 33(3):674‚Äì712, January 2021. ISSN 0899-7667. doi:10.1162/neco_a_01357. URL https://doi.
org/10.1162/neco_a_01357.
Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics , 113
(5):495‚Äì513, 2019. ISSN 1432-0770. doi:10.1007/s00422-019-00805-w. URL https://doi.org/10.1007/
s00422-019-00805-w .
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using Monte-Carlo
methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.
Th√©ophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grze¬¥s. Branching Time Active Inference: The
theory and its generality. arXiv:2111.11107 [cs], November 2021a.
Th√©ophile Champion, Howard Bowman, and Marek Grze¬¥s. Branching Time Active Inference: Empirical study and
complexity class analysis. arXiv:2111.11276 [cs], November 2021b.
O. √áatal, T. Verbelen, J. Nauta, C. D. Boom, and B. Dhoedt. Learning perception and planning with deep active
inference. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 3952‚Äì3956, 2020. doi:10.1109/ICASSP40776.2020.9054364.
Richard Bellman. Dynamic programming. Science, 153(3731):34‚Äì37, 1966. doi:10.1126/science.153.3731.34. URL
https://www.science.org/doi/abs/10.1126/science.153.3731.34.
Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, and Adeel Razi. Active inference for stochastic control. In Machine
Learning and Principles and Practice of Knowledge Discovery in Databases, pages 669‚Äì680, Cham, 2021. Springer
International Publishing. ISBN 978-3-030-93736-2. doi:https://doi.org/10.1007/978-3-030-93736-2_47.
Aswin Paul, Noor Sajid, Lancelot Da Costa, and Adeel Razi. On efficient computation in active inference. Expert
Systems with Applications, 253:124315, 2024b. ISSN 0957-4174. URL https://www.sciencedirect.com/
science/article/pii/S0957417424011813.
Takuya Isomura and Karl Friston. Reverse-Engineering Neural Networks to Characterize Their Cost Functions.
Neural Computation, 32(11):2085‚Äì2121, 11 2020. ISSN 0899-7667. doi:10.1162/neco_a_01315. URL https:
//doi.org/10.1162/neco_a_01315.
Moein Khajehnejad, Forough Habibollahi, Aswin Paul, Adeel Razi, and Brett J. Kagan. Biological neurons compete
with deep reinforcement learning in sample efficiency in a simulated gameworld, 2024. URL https://arxiv.org/
abs/2405.16946.
13
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements that can solve difficult
learning control problems. IEEE Transactions on Systems, Man, and Cybernetics , SMC-13(5):834‚Äì846, 1983.
doi:10.1109/TSMC.1983.6313077.
Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, bayesian optimal design, and expected
utility. The Drive for Knowledge: The Science of Human Information Seeking, page 124, 2022.
Karl J. Friston, Tommaso Salvatori, Takuya Isomura, Alexander Tschantz, Alex Kiefer, Tim Verbelen, Magnus T.
Koudahl, Aswin Paul, Thomas Parr, Adeel Razi, Brett J. Kagan, Christopher L. Buckley, and Maxwell James D.
Ramstead. Active inference and intentional behaviour. ArXiv, abs/2312.07547, 2023c. URL https://api.
semanticscholar.org/CorpusID:266191299.
Karl Friston. A free energy principle for biological systems. Entropy (Basel, Switzerland), 14:2100‚Äì2121, 11 2012.
doi:10.3390/e14112100.
Mahault Albarracin, In√™s Hip√≥lito, Safae Essafi Tremblay, Jason G. Fox, Gabriel Ren√©, Karl Friston, and Maxwell
J. D. Ramstead. Designing explainable artificial intelligence with active inference: A framework for transparent
introspection and decision-making, 2023. URL https://arxiv.org/abs/2306.04025.
Forough Habibollahi, Brett J Kagan, Anthony N Burkitt, and Chris French. Critical dynamics arise during structured
information presentation within embodied in vitro neuronal networks. Nature Communications, 14(1):5287, 2023.
Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement learning or active inference? PLOS ONE, 4(7):
1‚Äì13, 07 2009. doi:10.1371/journal.pone.0006421. URL https://doi.org/10.1371/journal.pone.0006421.
Emre O Neftci and Bruno B Averbeck. Reinforcement learning in artificial and biological systems. Nature Machine
Intelligence, 1(3):133‚Äì143, 2019.
14
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
A An overview of the DishBrain experiment and results
CTL / IS / RST: Baseline groups
MCC / HCC: Mice/Human cortical culture.
A B
Figure A.1: A: A high-level schematic of the ‚ÄòDishBrain‚Äô experiment. Live neuron populations are cultured on a silicon
chip and embodied in a simulated game environment of ‚ÄòPong‚Äô, where they receive information about game states and
feedback signals to control the paddle through electrophysiological stimulation and recording. B: Relative improvement
of groups in one of the game metrics (average rally length). We observe that both the mouse cortical culture (‚ÄòMCC‚Äô)
and human cortical culture (‚ÄòHCC‚Äô) groups demonstrate improvement in performance with time. The performance
is compared across two blocks of time in the experiment: the first five minutes and the next fifteen minutes of the
twenty-minute trial. The ‚ÄòHCC‚Äô group demonstrated higher relative performance improvement than other groups. The
baseline groups are control (‚ÄòCTL‚Äô), in-silico (‚ÄòIS‚Äô), and rest (‚ÄòRST‚Äô). Figures adapted from Kagan et al. [2022].
The ‚ÄòDishBrain‚Äô system (Fig. A.1 (A)) contains active neuron populations coupled to a simulated game environment of
‚ÄòPong‚Äô. In this setup, the neurons control paddle movements with the goal of intercepting the ball. With no opponent
player, the ball bounces off the back wall, returning towards the paddle after each successful hit. When the ball is
missed, the game restarts with the ball at a random location. The game performance is evaluated across three key
metrics: average rally length (longer rallies indicating better performance), percentage of long rallies (the proportion
of rallies lasting more than three hits, higher the better), and percentage of aces (instances where the ball is missed
immediately, with fewer aces being better). The ball‚Äôs x and y-axis location is encoded to the neurons via electrical
stimulation, combining place-coded and rate-coded signals. Through game-outcome dependent feedback, learning
and improvement in the performance of ‚ÄòMCC‚Äô and ‚ÄòHCC‚Äô 6 is reported as measurable improvements, particularly in
average rally length, as demonstrated in Fig. A.1 (B).
DishBrain is an ideal platform for studying the emergence of purposeful behaviour, as it demonstrates the ability to
learn and adapt to its environment de novo without any prior exposure. To theoretically analyse this phenomenon, we
turn to active inference, a first principle approach to modelling behaviour that emerged in neuroscience and was initially
proposed as a unified theory of brain functionFriston [2010, 2012]. Active inference posits that behaviour is driven by
an agent‚Äôs beliefs about its environment rather than being a separate process, as seen in frameworks like reinforcement
learning Friston et al. [2009], Neftci and Averbeck [2019]. This approach allows us to model the dynamics of learning
and decision-making in a biologically plausible manner, offering a deeper understanding of how systems like DishBrain
exhibit adaptive behaviour.
6‚ÄòMCC‚Äô and ‚ÄòHCC‚Äô: Mouse and human cortical cells
15
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
B Additional figures
A B C
Figure B.1: Relative improvement in performance (Average rally length) of in-silico active inference agents
compared to in-vitro groups of interest in the simulated game environment of Pong. A: Performance of the ‚ÄòAIF-1‚Äô
agent compared to other groups across key game metrics. The AIF-1 agent demonstrates a relative improvement over
time, but its planning-based approach does not yield significant advantages in the Pong environment, suggesting that
memory-based strategies may be more beneficial. B: Performance of ‚ÄòDP-T‚Äô agents against significant groups. While
DP-T agents exhibit initial learning, performance gains plateau with increased planning horizons, and over-planning
(beyond a horizon of 10) results in diminished effectiveness, likely due to over-planning. C: Performance of ‚ÄòCFL-T‚Äô
agents compared to significant groups. CFL agents with higher memory horizons consistently outperform other groups
across all metrics, reinforcing the role of memory in adaptive decision-making. These findings highlight the trade-offs
between planning-based and memory-driven decision-making approaches, with memory-based strategies proving more
effective in real-time interactive tasks like Pong.
16
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
A B C
FED
Figure B.2: Continuous-time analysis of memory horizon effects on CFL agent performance. Changes in agent
performance are tracked over a 20-minute real-time equivalent using linear regression. The average number of A, D:
hits per rally, B, E: percentage of aces (balls missed after the initial serve), and C, F: percentage of long rallies (‚â• 3
consecutive hits) are analyzed for CFL agents and biological MCC and HCC cultures. Top row (A, B, C): CFL agents
with shorter memory horizons exhibit a steady, linear improvement across all game metrics, with CFL-4 surpassing
MCC and HCC groups. However, memory horizons of 4 or greater may not be biologically plausible, as DishBrain‚Äôs
long-term memory capabilities remain to be studied. Bottom row (D, E, F): Agents with longer memory horizons (e.g.,
16 and 32) continue to show performance gains, though such horizons are implausible for DishBrain-like systems. These
results reinforce the role of memory in decision-making under active inference and provide a dynamic, time-continuous
perspective on learning in embodied systems.
17
Active Inference for Biologically Plausible Decision-Making Models A PREPRINT
A B
FED
C
Figure B.3: Box plot comparison of in-silico active inference agents and in-vitro MCC and HCC groups in a
simulated Pong environment. Game performance is summarized using box plots across three key metrics: A, D:
Average rally length (higher is better), B, E: Percentage of long rallies (higher is better), and C, F: Percentage of aces
(lower is better). Top row (A, B, C):Active inference agents with varying planning horizons (‚ÄòAIF-1‚Äô and ‚Äò DP-T‚Äô)
exhibit performance improvements comparable to the HCC group. However, planning horizons exceeding 10 lead
to a decline in performance, likely due to over-planning. Additionally, DPEFE agents show consistent performance
across all metrics, suggesting that deeper planning does not yield significant benefits in this task. Bottom row (D, E,
F): The ‚ÄòCFL-3‚Äô agent achieves the highest performance across all metrics, further emphasizing that memory-based
decision-making is more effective than planning-based approaches in dynamic environments like Pong. Box plots
provide a clearer visualization of variability and consistency across trials, reinforcing the observed trade-offs between
memory and planning in active inference-based control.
18