Value Cores for Inner and Outer Alignment:
Simulating Personality Formation via Iterated
Policy Selection and Preference Learning with
Self-World Modeling Active Inference Agents
Adam Safron1†, Zahra Sheikhbahaee2†, Nick Hay3, Jeff Orchard2, and Jesse
Hoey2
1 Center for Psychedelic and Consciousness Research, Johns Hopkins University
School of Medicine, Baltimore MD, USA
2 David R. Cheriton School of Computer Science, University of Waterloo, Ontario,
Canada
3 Encultured AI, California, USA
Abstract. Humanityfacesmultipleexistentialrisksinthecomingdecades
due to technological advances in AI, and the possibility of unintended
behaviors emerging from such systems. We believe that better outcomes
may be possible by rigorously exploring frameworks for intelligent (goal-
oriented) behavior inspired by computational neuroscience. Here, we ex-
plore how the Free Energy Principle and Active Inference (FEP-AI)
frameworkmayprovidesolutionsforthesechallengesviaaffordingthere-
alizationofcontrolsystemsoperatingaccordingtoprinciplesofhierarchi-
cal Bayesian modeling and prediction-error (i.e., surprisal) minimization.
Such FEP-AI agents are equipped with hierarchically-organized world
models capable of counterfactual planning, realized by the kinds of re-
ciprocal message passing performed by mammalian nervous systems, so
allowing for the flexible construction of representations of self-world dy-
namics with varying degrees of temporal depth. We will describe how
such systems can not only infer the abstract causal structure of their
environment, but also develop capacities for “theory of mind” and collab-
orative (human-aligned) decision making. Such architectures could help
to sidestep potentially dangerous combinations of systems with high in-
telligence and human-incompatible values, since such mental processes
are entangled (rather than orthogonal) in FEP-AI agents. We will fur-
ther describe how (meta-)learned deep goal hierarchies may also well-
describe biological systems, suggesting that potential risks from “mesa-
optimisers” may actually represent one of the most promising approaches
to AI safety: minimizing prediction-error relative to causal self-world
models can be used to cultivate modes of policy selection and agent per-
sonalities that robustly optimize for achieving goals that are consistently
aligned with both individual and shared values. Finally, we will describe
how iterative policy selection and preference learning can result in "value
cores" or self-reinforcing, relatively stable attracting states that agents
will seek to return to through their goal-oriented imaginings and actions.
† These authors contributed equally to this work.
Accepted to the 3rd International Workshop on Active Inference (IWAI 2022), held in conjunction with the 
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases 
(ECML/PKDD 2022).
2 Safron & Sheikhbahaee et al.
Keywords: Active Inference· AI Safety · Existential Risk · AI Align-
ment · Enculturation · Counterfactual Planning
1 Introduction: Towards Human-Compatible Artificial
Super Intelligence (ASI) with FEP-AI
We are currently engaged in an ongoing research program to determine how
connections between computational neuroscience and artificial intelligence (AI)
may inform what may be a one of the greatest challenges we will ever face as a
species: developing powerful AI systems which are aligned with human prefer-
ences, goals, and ethical standards. It is increasingly recognized that developing
robustly and flexibly intelligent artificial agents will likely heavily depend rely-
ing on self-organization processes in the service of bootstrapping (or developing)
their perception, action selection, learning, and reasoning process [1]. We pro-
pose a biologically-inspired process wherein AI agents learn to imitate human
behavior through interaction and cultural acquisition, which occurs via iterative
policy selection and value learning or updating of prior beliefs/preferences over
favorable policies [2].
The free energy principle and active inference framework (FEP-AI) provides
a unifying account for the brain and self-organising systems more generally.
FEP-AI provides a general (multiscale) systems theory in which deep temporal
hierarchical generative models encode different levels of abstraction via message-
passing through different sub-systems of the brain. Understood as a kind of hy-
brid machine learning architecture, embodied (and environmentally-embedded)
brains minimize a variational free energy bound on Bayesian model evidence
of sensory inputs accumulated over nested time scales [4]. This free energy ob-
jective rewards agents for minimizing precision-weighted prediction-errors with
respect to their world models, penalized by the extent to which inferences must
be updated away from prior beliefs (whether via refining internal models or
overt enaction). Agent behavior is generated via model inversion, with trajec-
tories selected based on posterior beliefs (as predictions, or empirical priors) of
future hidden control states. The choice behaviour of active inference agents is
canalized on multiple scales, governed by a singular objective of minimizing the
expected free energy of different future outcomes, which FEP-AI decomposes
into the maximization of extrinsic value (i.e., increasing certainty regarding the
realization of prior preferences, or goals) and intrinsic value (i.e., reducing uncer-
tainty about the causes of possible outcomes) [8]. With sophisticated inference,
policy selection is rendered by imaginative processes as mental simulations of
potential courses of action which allow humans to have cognitive access to (and
so be able to analyze/modify/control) possible future trajectories before being
enacted by an agent [5].
We believe the autonomous adaptivity promoted by such intrinsically moti-
vated FEP-AI agents represents a promising research direction for achieving Ar-
tificial Superintelligence (ASI):i.e., agents with greater than human-level predic-
tive world models coupled to strong (and accurate) top-down prior beliefs about
Value Cores for Inner and Outer Alignment 3
human values and their potential evolution. Here we introduce the term “ASI”,
as opposed to the more commonly used Artificial General Intelligence (AGI) in
order to acknowledge that generally intelligent AIs could be expected to exceed
human capacities in many respects, so requiring us to acknowledge the poten-
tially unique risks associated with these scenarios. We believe human-mimetic
FEP-AI has the potential to overcome (to degrees) many of the challenges that
have been identified in the AI safety literature, namely:
– Orthogonality thesis: if intelligence and values can vary independently of
each other, this could lead to the peril of powerful optimizers that pursue
many human-incompatible subgoals in a broader scope (i.e., controlling re-
sources for the sake of goal-realization and eliminating its own threats) [6,
7].
– The spontaneous emergence of mesa-optimizers which pursue goals that di-
verge from the (coherently extrapolated) desires of either goal-specifying
principle-agents or base system objectives (i.e.., respective outer and inner-
alignment failures). Mesa-objectives can engender accurate behaviour rel-
ative to the base optimizer. However, this behavior can deviate when en-
countering off-distribution data, so representing what has been called the
pseudo-alignment problem [9].
– “Treacherous turns” in which agents learn to engage in advanced adversar-
ial attacks against the humans whose values they are intended to serve. In
this scenario, an AI system may conceal its actual goals by behaving co-
operatively until its intelligence levels allow it to eventually revolt against
humanity to pursue its own (unaligned) objectives.
– The difficulty of achieving provable/formal verifiability with respect to safety
in systems whose effective forms remain unclear (e.g. will scaling laws con-
tinue to apply for mass language models?) [10]. Is there any avoiding the
conclusion that ASI (i.e., artificial superintelligence) is inherently unverifi-
able and so necessarily unsafe?
In what follows, we describe an ongoing research program in which we are de-
veloping FEP-AI agents in the service of handling these concerns. We will also
consider how conceptual issues from the study of AI safety can both inform and
beinformedbyourunderstandingofthefactorscontributingtohumanprosocial-
ity and mutual flourishing (e.g. parallels between "inner-alignment" and models
of psychological integration and actualization).
2 Intrinsic Drives for Curiosity
In developing advanced AI systems, the availability of adequate training data
and learning curriculums become issues of vital importance. It is increasingly
recognized that “self-supervised” active learning (i.e., selecting actions which are
particularly likely to increase information gain) with open-ended environments
may be an especially promising approach, wherein system behavior is a core part
of the data generation process, which also may help address the challenges of
4 Safron & Sheikhbahaee et al.
unbounded policy spaces (cf. frame/relevance problems). With respect to nav-
igating such environments, intrinsic drives such as empowerment and curiosity
have been proposed, wherein individuals attempt to respectively learn how to
control their environments (and so keep their future options open) and also
maximize the quality of their governing models (via informational foraging)[11].
The imperative of self-regulation is to return to an optimal set point which
reflects any cybernetic intelligent system’s existential task: selecting actions and
coping with uncertainty in attempting to survive and reproduce. Intrinsic mo-
tivation can be a crucial drive to seek out novelty and challenges, to extend
and wield one’s abilities, to explore, and to learn about the world. The intrin-
sic objective which describes the degrees of freedom or options of an agent to
control its environment and sense this control may be considered to be a "uni-
versal utility function." This heuristic forempowerment may be realized by a
simple, realizable across a broad range of systems, and potentially scalable drive
for agents that work to "keep their options [for control] open" as they opti-
mize for an objective of maximizing predictable information received by sensory
channels, conditioned on self-generated actions. That is, without requiring an
infinitely long history of past experience and only considering the local dynam-
ics of the agent’s environment, a seemingly simple objective may be sufficient to
compute an information-theoretic quantity that is applicable to every possible
agent-world interaction:empowerment [14]. An intrinsic drive for empowerment
solely relies upon dynamics that can be assimilated as actionable/salient states,
and as such can act as intrinsic reward without requiring an externally encoded
utility function [15]. We will similarly explore the potential of agents equipped
with intrinsic drives for maximizing information gain:curiosity [11–13].
Intrinsic drives are particularly well-suited for lifelong learning within open-
ended environments, allowing complex behaviors to be gleaned from simple and
generic internal rules. These motivating objectives promote exploration and al-
low an agent to pursue a broad range of affordances offered by the environment,
which once the opportunity arises, can be integrated into goal-directed behavior.
Seeking out novel information as a source of intrinsic value also plays a key role
in the FEP-AI framework, wherein adaptive behavior (including the selective
sampling of information) rests on minimization of prediction error between em-
pirical prior expectations and current sensations, so requiring the updating and
refining of world models by which agents navigate through their environments
[22].
The processes by which FEP-AI agents with intrinsic drives such as curiosity
or empowerment are bootstrapped are associated with several constraining bot-
tlenecks, which afford multiple opportunities for adjusting capabilities relative
to motivation selection. Indeed, the Bayesian foundation of FEP-AI centers on
the challenge of learning (and generalization via) increasingly complex predictive
world models, developed in the service of the adaptive control of behavior [23].
The causal structures built into FEP-AI agents further offer significant inter-
pretability, which help can prevent “treacherous turns” and allow productive
model inspection in ensuring that desirable motivational structures are instan-
Value Cores for Inner and Outer Alignment 5
tiated before advanced intellectual abilities are acquired. However, this is not
to say that no safety problems exist within an FEP-AI paradigm for develop-
ing ASI, such as in scenarios wherein agents become overconfident in predicting
outcomes when faced with distributional shifts [20]. Exploring the robustness
of our meta-learning (and mesa-optimizing) agents in such scenarios will be a
central part of our work, with potential implications for understanding human
psychology (and sociology) as well [17].
3 Meta Inverse Reinforcement-Learning or Hierarchical
Active Inference
The motivation selection approach to endowing ASIs with beneficial goals or
values for humans is challenging due to difficulty of defining some abstract com-
plex concepts (i.e., happiness, altruism, justice). Potentially undesirable forms
of value modifications can be prevented by imaginative counterfactual planning
based on goal realization through sampling from the learned world model of
the agent (cf. self-modifying Gödel machines) [16]. Increasingly complex val-
ues can be acquired in response to heterogeneous experiences through processes
of "associative value accretion" [7]. A promising proposal for learning human-
compatible values has been suggested with “cooperative inverse reinforcement
learning,” wherein artificial intelligences must infer the reward functions of other
agents in order to maximize human rewards; however, practical implementations
remain unclear [24]. Through meta-inverse-reinforcement learning (meta-IRL),
it is possible to learn priors which can (both stably and robustly) incorporate
potentially complex goal inference with different levels of abstractions (motiva-
tional scaffolding) in diverse environments and through multiple timescales [26].
In this research program, we focus on meta-IRL and (causal) world modeling
with FEP-AI agents as means of expanding agency both within and across sit-
uations to reason about human mental states (cf. Theory of Mind) [25].
Most current reinforcement learning (RL) algorithms require extensive train-
ing experience, but meta-learning (MLe) may allow for unprecedented data (and
parameter) efficiency, and can be considered one of the main ingredients for
achieving human-like domain adaptation in RL [3]. MLe or “learning to learn”
algorithms can be formulated as involving a more encompassing outer loop sys-
tem (e.g. more abstract processing unfolding closer to hierarchically higher (or
deeper) association cortices and subcortical structures) that efficiently learns
empirical priors for adaptively shaping more fine-grained inner-loop systems
(e.g. more concrete processing unfolding closer to primary modalities over hi-
erarchically lower sensorimotor cortices). The key advantage of MLe methods is
leveraging optimized inductive biases to generalize previous experiences to novel
tasks, thereby accelerating overall learning [27]. Adaptation to particular task
domains often depends on the faster learning process of an agent with episodic
memories from interactions with novel task-environments/niches, which criti-
cally depend on slow incremental learning (outer-loop optimization) as realized
6 Safron & Sheikhbahaee et al.
by higher-level modeling with greater situational invariance (so affording greater
cross-task generalization).
We are currently using these principles of meta-IRL in designing architec-
tures for deep temporal active inference agents, initialized with imprecise (flat)
prior beliefs about preferences wherein all states are similarly valued. In the face
of uncertainty in their deep generative world models (which describe the physical
and causal structure of the world), meta-IRL agents invest in more random ex-
ploration, and so maximize the entropy over those states as they engage in more
information-seeking behaviours and expand their range of options for achieving
goals(i.e.,respectivecuriosityandempowerment).Withmeta-IRL,onecanlearn
(to learn) meaningful goals and diverse skills from environmental interactions in
a continuously evolving, open-ended fashion [28,29]. In FEP-AI, exploration is
a byproduct of the reduction of uncertainty with respect to joint mappings be-
tween latent world states and the agent’s predictions over system-world states.
Crucially with respect to concerns voiced by the AI safety research commu-
nity, for FEP-AI agents, mesa-optimization (understood as meta-learning with
respect to emergent value functions) is not a bug, but a feature: the objective
function is the same for both inner- and outer-loop processes, both of which min-
imize surprisal across multiple levels of abstraction. More specifically, posterior
beliefs about causes of sensations in lower levels and foraging for subgoals (i.e.,
preferred states, realized via minimizing prediction-error with respect to priors
over outcomes) become observations and sources of adaptive precision-weighting
(cf. attention mechanisms) by more enduring (and potentially more impactful)
higher-level goals. Taken together, the synergies afforded by these kinds of hy-
brid multi-level modeling are likely to be key for developing advanced artificial
intelligences, and may be similarly essential for understanding core aspects of
human cognition [31].
4 Inner and Outer Alignment Problems
Designing AI systems with a nested architecture capable of learning human
preferences might address the outer alignment problem. However, there remains
a danger that these systems could optimize for an emergent (mesa-)objective
while foraging through possible space of solutions, and then accidentally develop
heuristics which engender conflicting behaviours with the original base-objective.
This is referred to as an inner alignment problem [18]. The existence of different
inductive biases in the training algorithm between the mesa- and base optimiz-
ers might create misalignment and lead to this type of failure mode for the AI
agent. In the AI alignment literature, evolution is given as an illustrative exam-
ple for understating how base optimizers (e.g. natural and sexual selection) may
have been dismissed by the agents it shapes (e.g. humans acting against repro-
ductive fitness goals through using various forms of birth control or practicing
abstinence) [9].
If stable causal relations between mesa- and base-objectives can be estab-
lished, this may allow more robustly-aligned mesa-optimizers to be deployed in
Value Cores for Inner and Outer Alignment 7
morecomplextrainingenvironments.However,thissolutionisrenderedchalleng-
ing due to the higher time costs and algorithmic complexity of such optimizers.
Meta-learning techniques may provide a partial solution to this challenge that
avoids pseudo-alignment and helps guarantee the robustness of the optimization
process. A hierarchical temporal structure inspired by FEP-AI minimizes varia-
tional free energy (i.e., surprisal) across multiple scales spanning processes both
internal and external to the system. This kind of enactive coupling should help
address both inner- and outer- alignment problems in a unified fashion, given
sufficient exposure to a well-designed curriculum under favorable learning condi-
tions. In such systems, there are intrinsic correlations between different spatial
and temporal scales by virtue of belief propagation integrated at a system-wide
level, which minimizes cumulative prediction error for overall systems (and sub-
systems) as they interact with the environments in which they are embedded
(and which they also construct through their actions) [19]. While governance by
a singular imperative for coherent adaptive functioning is insufficient for ensur-
ing the development of prosocial and human-promoting preferences, this kind
of integration may be helpful for increasing the likelihood that inner and outer
objectives may be aligned, given sufficient learning opportunities [30].
5 Brain-Inspired Intelligent Agents
We will further show how greater flexibility/adaptability can be introduced into
these meta-learning systems by attempting to reverse-engineer various neuro-
modulatory systems of the brain as hyperparameters for generative modeling
(cf. Auto-ML). We will specifically focus on diffuse neuromodulatory systems of
the brain controlling dopamine (DA) and serotonin (5-HT) signaling, which are
involved in a wide variety of cognitive, affective, and motoric functions and in
developing intelligent (and agentic) systems. While midbrain DA neurons elicit
reward-related behavior, 5-HT receptors have been suggested to often operate in
an opponent fashion, including with respect to the mediation of either "passive
coping" or "active coping" strategies in the face of uncertainty [32,33]. Within
FEP-AI, neurons with D1 receptors are suggested to compute free energy ex-
pected under a given policy, with phasic DA release associated with reward
prediction errors [34]. Further, tonic DA levels may contribute to degrees of in-
fluence by habitual response-tendencies as a function of contexts as estimated by
more slowly evolving and encompassing outer-loop processes. Recently, Hesp et
al. argued that DA (reflecting valenced emotional states, or "affective charge")
regulates the expected precision of an action model, which tracks changes in
the subjective fitness in terms of divergences between posterior and prior be-
liefs about policies [35]. We are currently attempting to model these kinds of
affectively-driven policy selection and updating in our artificial agents, with the
goal of better understanding how these factors may contribute to different kinds
of minds and emergent social dynamics (cf. life-history strategies).
Most research into biologically-inspired ML (and FEP-AI) parameters have
focused on the roles of DA. However, we will also consider 5-HT, as well as its
8 Safron & Sheikhbahaee et al.
potentially heterogeneous effects with respect to different receptor classes [32]. In
autonomous systems inspired by serotonin-like parameterization, uncertainty is
used to bias control in favor of inner-loop (or more model-based) decision-making
processes, relative to outer-loop (or more model-free and ‘reflexive’ or ‘impul-
sive’) dynamics or amortised inference [36]. Interpreted as ML-parameters, one
can use 5-HT analogues to influence the degree to which agents initiate offline
learning—rather than immediately releasing policies for overt goal-seeking—so
attempting to plan future actions through counterfactual simulation, and also
affording offline learning and planning (as inference) [37,38]. This kind of meta-
level control for trading-off between more deliberative planning and more au-
tomatic action modes may be particularly important for ASI systems and the
open-ended environments in which they are likely to evolve-develop.
Artificial 5-HT parameters might also be relevant with respect to their capac-
ities for "relaxing" free energy landscapes in ways that allow for more creative
cognition and flexible updating. More specifically, these altered beliefs could even
include core assumptions about the boundaries that separate systems from the
world, which when relaxed may potentially facilitate socioemotional alignment
via various "bonding" processes whereby agents can become entangled to op-
timize in common directions. With these considerations in mind, we will test
whether paramaterizations with 5-HT analogues may be beneficial for a) pro-
moting the acts of imaginative/creative synthesis involved in inferring the latent
states of another mind (cf. Theory of mind), and b) promoting fast convergence
onto modes of policy selection involving shared intentionality (and patterns of
attention).
6 Cultural Acquisition of Stable Prosocial Values
The FEP-AI framework affords a modeling approach that naturally affords the
kindsofcontext-sensitivityrequiredfornavigatingopen-endedenvironmentsand
multi-agent contexts. Hence, an FEP-AI agent promotes its well-being (and to
varyingdegrees,evolutionaryfitness)byaligningitselfwith(andco-constructing)
its cultural eco-niche (as a kind of extended phenotype) [39]. The biological
inspiration for such generative modeling provides opportunities to connect AI
alignment and neuropsychology through correlating psychological behavior with
underlying brain architecture. Further, studying FEP-AI meta-learning agents
in silico within a ‘micro’-world can yield a diverse range of social-coordination
dynamics wherein individual personalities and cultures are co-constructed. Such
agents may also be made to learn about human values via cooperative inverse
RL, wherein AI agents try to realize human intentions, which we will model as
realizedbymultipleagentsenteringintostatesofgeneralizedsynchronyandmin-
imizing prediction-error with respect to shared goals [41]. However, these infer-
ences require imagining (or semi-accurately modeling) the likely counterfactual
sensory trajectories of everyone involved in converging on a shared generative
model through their joint goal pursuit [42].
Value Cores for Inner and Outer Alignment 9
Within the FEP-AI literature, the process of inferring other agents’ expec-
tations about the world and behaviour in social contexts is called “thinking
through other minds” [43]. From this point of view, individuals acquire shared
beliefs regarding social values to be realized as they attempt to semi-faithfully
simulate other agents, with particular emphasis on attentional processes as po-
tentially powerful sources of information with respect to both intentional and
epistemic states for oneself and others [44]. Indeed, the key process behindencul-
turation may be inheriting modes of policy selection conducive to the adaptive
shaping of salience (actions which lead to selecting informative sensory data)
and attention (as precision-weighting of this data based on its estimated re-
liability/usefulness) landscapes (or the information geometry of expected free
energy gradients). The empirical priors about norms and social values can ac-
crue slowly and produce stable behavior patterns, which may be thought of as
a system’spersonality [45]. These path dependencies have far-reaching implica-
tions for value alignment efforts in defining potentially fruitful points of leverage
for helping to ensure that system capabilities and motivations are compatible
with human flourishing. Perhaps most fundamentally, considering that intelli-
gence and values are fundamentally entangled in this setup, it may be the case
that a FEP-AI may provide “seed AI” for developing human-compatible ASI (cf.
“Reflective equilibrium” and “coherent extrapolated volition”).
Finally,webelieveoursimulationsofpersonality/preference-formationthrough
iterated policy selection and prior updating may be fruitfully understood in
terms of a Value Core framework, which we briefly introduce here:
1. Different action tendencies (broadly construed) can be modeled as constitut-
ing value cores that compete and cooperate with each other in contributing
to ongoing action selection and policy updating, which in turn modify cores
and so influence future action selection.
2. Under some circumstances, a value core may achieve a position of relative
dominance in which selected actions and associated learning signals will be
unlikely to result in modification of either the characteristics or strategic
position of that value core relative to other cores.
3. Let us refer to these dominating attractors as value cores (VCs), some of
which become an agent’s “intrinsic goods,” or “final values.” Different VCs
likely vary in the range of conditions under which they are robustly self-
sustaining.
This model calls for a research program to characterize the following issues:
ranges of human-typical VCs; developmental circumstances that give rise to dif-
ferent VCs; stability of various VCs to boundary conditions; and means of veri-
fying the existence of particular VCs in humans and in human-like AI systems.
While presently under-specified, we believe this kind of conceptual framework–
informed by concepts from (generalized) evolutionary game theory–may be help-
ful in working towards proofs (or at least heuristics) with respect to the regimes
under which potentially transient preferences may become stabilized as more
enduring orientations and personalities [45,46].
10 Safron & Sheikhbahaee et al.
7 Discussion and Future Work
While such considerations may seem premature, we believe it is valuable to begin
conducting serious work on AI with Stuart Russell’s question in mind: “What if
we succeed?” [47] The creation of ASI by such means may constitute one of the
most important things we ever do as species (if we survive long enough), and the
biopsychological-inspiration of FEP-AI agents suggests that this approach may
eventually provide a workable path towards realizing this goal. In the months
to come, we will perform a variety of simulations with such agents, wherein
we will show how prosocial reference personalities can form as attractors that
achieve stable equilibria both within and between individuals. We look forward
to discussing this ongoing work with the FEP-AI and machine learning research
communities, and discovering opportunities for collaboration with those who
may be interested in working towards these (hopefully shared) goals.
Acknowledgements We would like to thank The Long-Term Future Fund and
Centre for Effective Altruism for providing financial support for Zahra Sheikhba-
haee and her ongoing work to design agents and model persons with increasingly
sophisticated capacities for active inference.
References
1. Froese, T., Ziemke, T.: Enactive artificial intelligence: Investigating the systemic
organization of life and mind. Artificial Intelligence173(3), pp. 466–500 (2009)
2. Sarma,G.P.,Hay,N.,Safron,A.,SAFECOMP2018:AISafetyandReproducibility:
Establishing Robust Foundations for the Neuropsychology of Human Values. Com-
puter Safety, Reliability, and Security, pp. 507-512 https://arxiv.org/abs/1712.0430
(2018)
3. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., Lillicrap, T.: Meta-Learning
with Memory-Augmented Neural Networks. International Conference on Machine
Learning, pp. 1842–1850 (2016)
4. Friston, K. J., Rosch, R., Parr, T., Price, C., Bowman, H.: Deep temporal models
and active inference. Neuroscience & Biobehavioral Reviews, Vol. 90, pp. 486–501
(2018)
5. Friston, K. J., Da Costa, L., Hafner, D., Hesp, C., Parr, T.: Sophisticated inference,
Neural Computation33(3): pp. 713–763 (2020)
6. Bostrom, N.: The superintelligent will: motivation and instrumental rationality in
advanced artificial agents, Minds and Machines, vol. 22, pp. 71–85 (2012)
7. Bostrom, N., Superintelligence: Paths, dangers, strategies. Oxford University Press.
ISBN 978-0199678112 (2014)
8. Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., & Pezzulo, G.,
Active inference and epistemic value. Cognitive Neuroscience,6(4), pp. 187—214
(2015)
9. Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., Garrabrant, S.: Risks from
Learned Optimization in Advanced Machine Learning Systems, In Advanced Ma-
chine Learning Systems, arXiv: 1906.01820 (2019)
Value Cores for Inner and Outer Alignment 11
10. Yampolskiy,R.V.:VerifierTheoryfromAxiomstoUnverifiabilityofMathematical
Proofs, Software and AI, arXiv: 1609.00331v1 (2016)
11. Schmidhuber, J., PowerPlay: training an increasingly general problem solver by
continually searching for the simplest still unsolvable problem, Frontiers in Psychol-
ogy, Vol. 4 (2013)
12. Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., Ondobaka, S.:
Active Inference, Curiosity and Insight. Neural Comput. 29 (10): pp. 2633–2683
(2017)
13. Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T. H. B., Kronbich-
ler, M., Friston, K. J.: Computational mechanisms of curiosity and goal-directed
exploration, eLife8:e41703 (2019)
14. Klyubin, A. S., Polani, D., Nehaniv, C. L., Empowerment: A Universal Agent-
Centric Measure of Control. IEEE Congress on Evolutionary Computation, vol. 1,
pp. 128-–135 (2005)
15. Jung T., Polani D., Stone P.: Empowerment for continuous agent—environment
systems. Adaptive Behavior.19(1):16-39 (2011)
16. Schmidhuber, J., Gödel machines: Fully Self-Referential Optimal Universal Self-
Improvers. In B. Goertzel and C. Pennachin, eds.: Artificial General Intelligence,
pp. 119–226 (2006)
17. Brouwer, A., Carhart-Harris R. L.: Pivotal mental states. Journal of Psychophar-
macology,35 (4): pp. 319–352 (2021)
18. Demski, A., Garrabrant, S.: Embedded agency. arXiv preprint arXiv:1902.09469
(2019).
19. Ramstead, M. J. D. , Badcock, P.B., Friston, K. J.: Answering Schrödinger’s ques-
tion: A free-energy formulation, Phys Life Review, Vol. 24, pp 1-16 (2018)
20. Man, K., Damasio, A., Neven, H.: Need is All You Need: Homeostatic Neural
Networks Adapt to Concept Shift, arxiv: 2205.08645 (2022)
21. Warrell,J.,Gerstein,M.:Cyclicandmultilevelcausationinevolutionaryprocesses,
Biology & Philosophy, Vol. 35, 50 (2020)
22. Pezzulo, G., Rigoli, F., Friston, K.: Active Inference, homeostatic regulation and
adaptive behavioural control, Progress in Neurobiology. Vol. 134, pp. 17-35 (2015)
23. Taylor, J., Yudkowsky, E., LaVictoire, P., Critch, A.: Alignment for Advanced
Machine Learning Systems, Ethics of artificial intelligence, Oxford University Press,
pp. 342–367
24. Hadfield-Menell, D. and Dragan, A. and Abbeel, P. and Russell, S.:Cooperative
Inverse Reinforcement Learning, In: Advances in neural information processing sys-
tems. pp. 3909—3917 (2016)
25. Rabinowitz, N., Perbet, F., Song, F., Zhang, C., Eslami, S. M. A., Botvinick,
M.:Machine Theory of Mind, In Proceedings of the 35th International Conference
on Machine Learning, Vol. 18, pp. 4218–4227 (2018)
26. Xu, K., Ratner, E., Dragan, A., Levine, S., Finn, C.,Learning a Prior over Intent
via Meta-Inverse Reinforcement Learning, Proceedings of the 36th International
Conference on Machine Learning, PMLR 97:pp. 6952–6962 (2019)
27. Botvinick, M., Ritter, S., Wang, J. X., Kurth-Nelson, Z., Blundell, C., Hassabis,
D.: Reinforcement Learning, Fast and Slow. Trends in Cognitive Science23(5), pp.
408–423 (2019)
28. Gupta, A. and Eysenbach, B. and Finn, C. and Levine, S.:Unsupervised Meta-
Learning for Reinforcement Learning, arXiv:1806.04640 (2018)
29. Eysenbach, B. and Gupta, A. and Ibarz, J. and Levine, S.: Diversity is All You
Need: Learning Skills without a Reward Function, arXiv:1802.06070 (2018)
12 Safron & Sheikhbahaee et al.
30. Dalege, J., Borsboom, D., van Harreveld, F., van der Maas, H. L. J.:The Atti-
tudinal Entropy (AE) Framework as a General Theory of Individual Attitudes,
Psychological Inquiry, vol. 29, 4, pp.175–193 (2018)
31. Safron, A., çatal, C. and Verbelen, T.“Generalized Simultaneous Localization and
Mapping (G-SLAM) as Unification Framework for Natural and Artificial Intelli-
gences:TowardsReverseEngineeringtheHippocampal/entorhinalSystemandPrin-
ciples of High-level Cognition.” PsyArXiv. doi:10.31234/osf.io/tdw82 (2021)
32. Safron, A., Sheikhbahaee, Z.: Dream to Explore: 5-HT2a as Adaptive Tempera-
ture Parameter for Sophisticated Affective Inference, Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, Springer, pp. 799–809
(2021)
33. Carhart-Harris, R. L., Nutt D. J., Serotonin and brain function: a tale of two
receptors. Journal of Psychopharmacology,31(9), pp. 1091–1120 (2017)
34. Parr, T. and Friston K. J., The Anatomy of Inference:Generative Models and Brain
Structure, Frontiers in Computational Neuroscience, Vol. 12 (2018)
35. Hesp, C., Smith, R., Parr, T., Allen, M., Friston, K. J., Ramstead, M. J. D.,
Deeply Felt Affect: The Emergence of Valence in Deep Active Inference. Neural
Computation 33(2), pp. 398–446 (2021)
36. Worbe, Y., Palminteri, S., Savulich, G., Daw, N. D., Fernandez-Egea, E., Robbins,
T. W., Voon, V.:Valence-dependent influence of serotonin depletion on model-based
choice strategy, Molecular Psychiatry, Vol. 21, pp. 624–629 (2016)
37. Bang, D., Kishida, K. T., Lohrenz, T., Tatter, S. B., Fleming, S. T., Montague,
P. R.: Sub-second Dopamine and Serotonin Signaling in Human Striatum during
Perceptual Decision-Making, Neuron,118(5), pp. 999–1010 (2020)
38. Grossman, C. D. and Bari, B. A. , Cohen, J. Y., Serotonin neurons modulate
learning rate through uncertainty, Current Biology,32(3), pp. 586–599 (2022)
39. Miller M., Kiverstein J., Rietveld E.: The Predictive Dynamics of Happiness and
Well-Being. Emotion Review.14 (1):15-30 (2022)
40. Sarma, G. P., Safron, A., Hay, N. J., Integrative Biological Simulation, Neuropsy-
chology, and AI Safety, Workshop on Artificial Intelligence Safety 2019 co-located
with the Thirty-Third AAAI Conference on Artificial Intelligence(AAAI-19), (2019)
41. Friston, K. J., Frith, C. D. Active inference, communication and hermeneutics.
Cortex; vol. 68: pp. 129–43 (2015) doi:10.1016/j.cortex.2015.03.025
42. Friston, K. J., Frith, C., A duet for one, consciousness and cognition, Vol. 36, pp.
390–405 (2015)
43. Veissiére, S. P. L., Constant, A. , Ramstead, M. J D., Friston, K. J. and Kirmayer,
K. L., Thinking Through Other Minds: A Variational Approach to Cognition and
Culture, Behav Brain Sci.,43(90): pp. 1–75 (2019)
44. Graziano, M. S. A., The Attention Schema Theory: A Foundation for Engineering
Artificial Consciousness, Frontiers in Robotics and AI460 (2017)
45. Safron, A., DeYoung, C. G.: Integrating Cybernetic Big Five Theory with the Free
Energy Principle: A new strategy for modeling personalities as complex systems,
Measuring and Modeling Persons and Situations, Chap. 18, pp. 617–649 (2021)
46. Safron, A., Klimaj, V.: Learned but Not Chosen: A Reward Competition Feedback
Model for the Origins of Sexual Preferences and Orientations, Gender and Sexuality
Development, Springer, pp. 443–490 (2022)
47. Russell, S.: Human Compatible: Artificial Intelligence and the Problem of Control,
Penguin Publishing Group, ISBN 0525558624, 9780525558620 (2019)