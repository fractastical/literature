arXiv:2505.03856v1  [cs.CV]  6 May 2025
An Active Inference Model of Covert and Overt
Visual Attention
Tin Miˇsi´c, Karlo Koledi ´c, Fabio Bonsignorio, Ivan Petrovi ´c, and Ivan Markovi ´c1
Abstract—The ability to selectively attend to relevant stimuli
while filtering out distractions is essential for agents that process
complex, high-dimensional sensory input. This paper introduces a
model of covert and overt visual attention through the framework
of active inference, utilizing dynamic optimization of sensory
precisions to minimize free-energy. The model determines visual
sensory precisions based on both current environmental beliefs
and sensory input, influencing attentional allocation in both
covert and overt modalities. To test the effectiveness of the
model, we analyze its behavior in the Posner cueing task and a
simple target focus task using two-dimensional(2D) visual data.
Reaction times are measured to investigate the interplay between
exogenous and endogenous attention, as well as valid and invalid
cueing. The results show that exogenous and valid cues generally
lead to faster reaction times compared to endogenous and
invalid cues. Furthermore, the model exhibits behavior similar to
inhibition of return, where previously attended locations become
suppressed after a specific cue-target onset asynchrony interval.
Lastly, we investigate different aspects of overt attention and show
that involuntary, reflexive saccades occur faster than intentional
ones, but at the expense of adaptability.
Index Terms—active inference, visual attention, Posner cueing
task
I. I NTRODUCTION
Attention as a cognitive process allows agents to selectively
focus on specific stimuli while ignoring others. This ability
helps humans avoid sensory overload, and as robots acquire
more complex sensory channels it could help decrease the
computational load required to perform in daily tasks, such as
object tracking and visual search, as well as social interactions
[1]–[3]. Attention is often separated into top-down, or goal-
driven attention, and bottom-up or stimulus-driven attention,
with some theories including hysteresis as a third component
[4]. Top-down attention bilaterally activates dorsal posterior
parietal and frontal regions of the brain, while bottom-up
attention activates the right-lateralized ventral system, with
the dorsal frontoparietal system combining the two into a
“salience map” during visual search [5], [6]. Furthermore,
visual attention is separated into overt and covert attention [7],
[8], with overt attention involving saccadic eye movements to
the attentional target, and covert attention referring to attention
shifts to the target while the eyes remain fixated elsewhere.
Multiple approaches exist to model attention, more numerous
being those that are based on Bayesian inference [9]–[16].
While previous studies have modeled visual attention and
This research has been supported by the H2020 project AIFORS under
Grant Agreement No 952275
1University of Zagreb Faculty of Electrical Engineering and Computing,
Croatia; Correspondence: tin.misic@fer.hr, ivan.markovic@fer.hr
Fig. 1: At the core of the proposed model are the beliefs about
the causes of sensory inputs. These beliefs and action signals
are updated through attractor goals and error updates to min-
imize free-energy. The dedicated bottom-up attention module
regulates attention through dynamic sensory precisions.
active saccades in visual search [9]–[12], the integration of vi-
sual attention and bottom-up action from raw two-dimensional
visual data within the active inference framework remains
unexplored. This is particularly important in robotics, as vision
is a fundamental sensory modality, with images serving as a
primary source of perceptual input for decision-making and
interaction with the environment.
Visual attention and its models are most often tested using
the Posner cueing task, i.e., the Posner paradigm. The Posner
cueing task is an experimental paradigm used to study covert
visual attention [17], [18]. Participants are asked to fixate on a
central point while a cue directs attention to a location where
a target may appear. The cue can either be endogenous –
meaning that attention is voluntarily guided based on symbolic
cues (e.g., an arrow pointing left or right), or exogenous –
meaning that attention is automatically drawn by a sudden,
peripheral stimulus (e.g. a bright flash or a flickering box).
Endogenous cueing is considered to be top-down because it
requires cognitive processing and active interpretation of the
cue, while exogenous cueing is considered to be bottom-up
because it does not require conscious interpretation. Reaction
times and accuracy are measured to assess how cues influence
attentional shifts.
Through the original Posner paradigm [17], [18] and its
variations, valuable insights have been gained about attentional
processes. Covert attentional shifts to a target area occur prior
to any eye movement [18], [19], and valid cues produce faster
responses than invalid cues [17], [18]. Exogenous cues were
shown to produce faster reaction times than endogenous cues
[20], [21], showing that bottom-up attention is faster because
it requires no conscious processing. The question of weather
attentional selection is object-based or location-based has also
been thoroughly researched, and the consensus is that both
types are not mutually exclusive, but are dependent on the
current task [22]–[24]. Research supporting location-based
attention has shown that the distance from the focus point
plays a role in reaction time, with reaction times increasing as
target eccentricity increased [25]–[27].
In this paper we propose a model of visual attention, shown
in Fig. 1, viewed through the lens of active inference [28]
– a computational approach derived from the free-energy
principle (FEP). According to the FEP, systems adapt and
act in a way that minimizes their free-energy [29]. Free-
energy is a concept borrowed from physics, statistics, and
information theory that limits the surprise on a sample of data
given a generative model. This principle helps to explain how
biological systems resist the natural tendency to disorder, and
their action, perception, and learning processes [30]. In the
FEP, attention is theoretically achieved by optimizing sensory
precisions, their parameters, and mutual precision weighing
[9]–[14], [31]. Biased competition and endogenous/exogenous
attention have been studied in this context, and the precision
optimization produces behaviors similar to human attention
[9], [15].
The contribution of this paper is an active inference model
of overt and covert visual attention by investigating precision
optimization for visual data and how it generates endoge-
nous/exogenous attention and action control. The proposed
model includes both top-down and bottom-up visual attention,
as well as covert and overt shifts in attention. These properties
are demonstrated through the Posner cueing task and a simple
target focus task on visual 2D data. A variational auto-encoder
(V AE) was used for the visual generative model, and model
training and experiments were done in the Gazebo simulator
in the Robot Operating System (ROS).
The paper is organized as follows. In Sec. II we give
an overview of the theoretical background and elaborate the
proposed approach that is based on free-energy minimization
with 2D precision optimization and overt saccades through
active inference. Section III shows the results of the Posner
cueing tasks and active attention trials. Section IV provides
the discussion of the results while Sec. V concludes the paper
and provides directions for future work.
II. P ROPOSED METHOD
A. Free-energy Minimization
Free-energy is defined as the negative evidence lower bound
(ELBO), or as the sum of the Kullback-Leibler (KL) diver-
gence and the surprise [9], [29], [30]:
F(z, s) =−L(q)
= DKL[q(z)||p(z|s)] − ln p(s), (1)
where z and s represent latent system states and sensory ob-
servations, respectively, while the KL-divergence is computed
between the posterior p(z|s) and the approximate variational
density q(z). Given that, the surprise is defined as the negative
log-probability of an outcome −ln p(s). If the variational
density q(z) is assumed to factor into Gaussian probability
density functions (pdfs) [9], [29], [32]:
q(z) =
Y
i
q(zi) =
Y
i
N(µi, Π−1
i ), (2)
the free-energy then becomes dependent only on the most
probable hypotheses, beliefs µi, and precision matrices Πi
of the latent system states z [9], [32]:
F(µ, s) =−ln p(s, µ) +C
= −ln p(s|µ) − ln p(µ) +C. (3)
Furthermore, sensory observations s and beliefs µ are
defined in the context of hierarchical dynamic models [9], [29],
[30], [32]:
˜s = ˜g(˜µ) +ws
D˜µ = ˜f(˜µ) +wµ. (4)
Here, ˜µ indicates generalized coordinates of beliefs with
multiple temporal orders, ˜µ = {µ, µ′, µ′′, ···} , which allow
for a richer approximation of the environment dynamics, D
stands for the differential shift operator D˜µ = {µ′, µ′′, ···}
in the generalized equation of system dynamics ˜f(˜µ), while
˜g(˜µ) is the sensor model that maps current beliefs to sensory
observations. The amplitudes of random fluctuations ws and
wµ are state dependent and are defined as Gaussian pdfs with
covariances Σs and Σµ, respectively [9], [32]:
ws ∼ N(µi, Σs(z, s, γ))
wµ ∼ N(µi, Σµ(z, s, γ)). (5)
The precision matrices Πi are the inverses of these covari-
ances, Πi := Πi(z, s, γ) = Σi(z, s, γ)−1, with precision
parameters γ that control the amplitudes [9], [15]. The pre-
cisions are dynamic and depend on the current states and
sensory input. It is through optimization of precisions and their
parameters that attention is achieved [9]–[14], [31].
B. Perceptual and Active Inference
Perception, action, and learning can all be optimized
through the minimization of free-energy. In this paper we
only consider perception and action, and leave the learning
processes of attention for future work. Action and beliefs are
optimized through gradient descent [28]–[30], [32]:
˙˜µ − D˜µ = −∂˜µF(˜µ, ˜s)
˙a = −∂aF(˜µ, ˜s). (6)
The likelihood and prior in (3) also become generalized
and can be partitioned within and across temporal orders d,
respectively [32]:
p(˜s|˜µ) =
Y
d
p(s[d]|µ[d])
p(˜µ) =
Y
d
p(µ[d+1]|µ[d]).
(7)
These partitions are also assumed to take the following Gaus-
sian pdf form:
p(s[d]|µ[d]) = |Πs
[d]|
1
2
q
(2π)L
exp

−1
2e[d]
s
T
Πs
[d]e[d]
s

p(µ[d+1]|µ[d]) = |Πµ
[d]|
1
2
q
(2π)M
exp

−1
2e[d]
µ
T
Πµ
[d]e[d]
µ

,
(8)
where L and M are the respective dimensions of sensory
observations s and internal beliefs µ. Therein, e[d]
s and e[d]
µ
represents sensory and system dynamics prediction errors:
e[d]
s = s[d] − g[d](µ[d]) =s[d] − p[d]
e[d]
µ = µ[d+1] − f[d](µ[d]),
(9)
where p[d] = g[d](µ[d]) are sensory predictions generated by
the generative sensor model. Note that in our case the system
dynamics model is defined through flexible intentions h(k)
[32], where for each intention k ∈ (0, K− 1):
f(k)(µ) =l · E(k)
i + w(k)
µ = l · (h(k) − µ) +w(k)
µ , (10)
with l being the gain of intention errors E(k)
i . The imple-
mentation of the generative sensor models g[d] is presented in
subsection III-A.
1) Belief update: With state- and sensory-dependent preci-
sions, the belief update takes the following form:
˙˜µ =D˜µ + ∂˜g
∂ ˜µ
T
˜Πs˜es + ∂ ˜f
∂ ˜µ
T
˜Πµ˜eµ − DT ˜Πµ˜eµ
+ 1
2Tr
"
˜Π−1
s
∂ ˜Πs
∂ ˜µ
#
− 1
2 ˜eT
s
∂ ˜Πs
∂ ˜µ ˜es
+ 1
2Tr
"
˜Π−1
µ
∂ ˜Πµ
∂ ˜µ
#
− 1
2 ˜eT
µ
∂ ˜Πµ
∂ ˜µ ˜eµ,
(11)
with Tr being the trace of a matrix. The terms that comprise
the belief update equation are:
• ∂˜g
∂ ˜µ
T ˜Πs˜es : likelihood error computed at the sensory
level, representing the free-energy gradient of the like-
lihood relative to the belief ˜µ[d] in (9)
• ∂ ˜f
∂ ˜µ
T
˜Πµ˜eµ : backward error from the next temporal order,
representing the free-energy gradient relative to the belief
˜µ[d+1] in (9)
• −DT ˜Πµ˜eµ : forward error coming from the previous
temporal order, representing the free-energy gradient rel-
ative to the belief ˜µ[d] in (9)
• 1
2 Tr
h
˜Π−1
s
∂ ˜Πs
∂ ˜µ
i
− 1
2 ˜eT
s
∂ ˜Πs
∂ ˜µ ˜es: free-energy gradients
from the sensory precisions, serves as bottom-up attention
• 1
2 Tr
h
˜Π−1
µ
∂ ˜Πµ
∂ ˜µ
i
− 1
2 ˜eT
µ
∂ ˜Πµ
∂ ˜µ ˜eµ: free-energy gradients
from the system dynamics precisions, serves as top-down
attention.
2) Action update: Action is also updated through the min-
imization of free-energy [28]–[30], [32]:
a = arg min
a
F(µ, s), (12)
with the action update taking the following form:
˙a = − ∂aF(µ, s) =−∂˜s
∂a
T
˜Πs˜es
+ 1
2Tr
"
˜Π−1
s
∂ ˜Πs
∂˜s
#
∂˜s
∂a − 1
2 ˜eT
s
∂ ˜Πs
∂˜s ˜es
∂˜s
∂a ,
(13)
with bottom-up attention components in relation to sensory
input, analogous to those in relation to belief in (11). These
control signals act as reflexive saccades [33], [34]. The gra-
dient ∂˜s
∂a is an inverse mapping from sensory data to actions,
which is usually considered a ”hard problem” [35].
The implementations of all gradients in terms of belief,
action and sensory input are elaborated in Appendix A.
III. R ESULTS
A. Implementation of the proposed model
The graphical representation of the developed model 1 can
be seen in Fig. 1. The current belief µ is passed as input
to exteroceptive, proprioceptive, and interoceptive generative
models. The predictions p of these models are compared to
the actual sensory input s and the prediction errors es are
used to drive action, as well as to update the current beliefs.
The generative models for proprioceptive (camera pitch and
yaw) and interoceptive (symbolic cue signals) sensory input
are trivial identity matrices, while the generative model for
the exteroceptive visual sensory input is the decoder of a
disentangled variational auto-encoder (V AE). The V AE has
been trained to disentangle the position of the target in the
image, as well as the target’s presence in the image. This
disentanglement simplifies the conversion from intrinsic image
coordinates to extrinsic camera orientation angles. The V AE
architecture, training and latent space encoding are elaborated
in Appendix B.
The belief state is composed of the following components:
• Symbolic cue belief – interoceptive endogenous cues will
present the cue position on the image, and this belief
should mirror that from the sensory input
• Camera orientation belief – proprioceptive belief over
the extrinsic pitch and yaw angles of the camera viewing
the environment
• Visual belief - an encoding of the exteroceptive visual
input, disentangled to encode the target position and
presence so they are easily interpreted
1The implemented model is available at: https://github.com/TinMisic/
AIF---visual-attention/tree/ICDL
• Covert attention belief - belief over the amplitude and
center of a radial basis function (RBF) used to calculate
the visual sensory precisions.
The sensory data and belief shapes are elaborated in Appendix
C. The beliefs are updated through bottom-up prediction error
gradients, as well as through top-down attractors α generated
from the current beliefs, according to the flexible intentions
theory proposed in [32]. These goal-directed intentions en-
courage action through the proprioceptive camera orientation,
as well as covert attention through the shifts of the RBF center
and amplitude.
Sensory precision Πs for the visual input is dynamic and
calculated based on the current overt attention belief and
sensory input. We assume that there is no correlation between
individual pixels, so Πs is defined as:
Πs =


π1(µ, s) 0 ··· 0
0 π2(µ, s) ··· 0
... ... ... ...
0 0 ··· πL(µ, s)


L×L
, (14)
where L = 32× 32 (×3) is the dimensionality of the visual
data. We further assume that the individual precision functions
πi(µ, s) are determined by RBFs based on the covert attention
center and the presence of a target-specific property, in our
case the color red:
πi(µ, s) =π(x, y,µ, s) =
µamp
2

ln

−(x − µu)2 + (y − µv)2
b2 + 1

+ c

+ 1
2

ln

−(x − ru(s))2 + (y − rv(s))2
b2 + 1

+ c

,
(15)
where [µamp, µu, µv] are covert attention beliefs,
[ru(s), rv(s)] is the centroid of the biggest red object.
The parameters of the precision function, b = 2.6 and c = 1,
are empirically chosen to ensure that the RBF values span
from 0 to 1 across the image area. The shape of the RBF was
chosen so that the belief update pushes the covert attention
toward the area of the image with the highest error, while
a Gaussian RBF would push it away from the error. The
sensory precision matrix generated by this RBF and the
resulting free-energy gradient caused by a prediction error
can be seen in Fig. 2. The precision nevertheless decreases
the further a point is from the focus center, mimicking human
foveation [26], [27].
B. Simulating the Posner Cueing Task
The Posner cueing task is used to demonstrate the proposed
model’s exogenous and endogenous covert attention. The
model’s sensory inputs are its current camera orientation, a
symbolic cue signal and visual data of an empty scene in
which a red sphere might appear as a target. Note that the
endogenous cue is given through the interoceptive sensory
channel, not as an arrow in the visual channel as illustrated
in Fig.3. We performed four variations of the cueing task, for
(a) Visual sensory precision matrix
 (b) Sensory precision free-energy gra-
dient, ˙µu = −0.839
Fig. 2: The center of the RBF is (-0.25, 0.0), while the error
appears at (-0.75, 0.0). The u-component of the RBF center
is pushed toward the error with the update ˙µu = −0.839.
Fig. 3: Trial sequence of events. The model is first initialized
for 10 steps, then a cue appears for 50 simulation steps. The
cue is then removed for a variable interval, known as cue-
target onset asynchrony (CTOA). After that the target appears
until it is detected by the model or 1000 steps have passed.
both endogenous and exogenous cueing in valid and invalid
settings. The endogenous cue is given through the symbolic
cue signal which has to be processed into an intention that
moves both the center of covert attention and the belief over
the sphere’s position. The exogenous cue is a brief appearance
of the target object, which moves the center of covert attention
through the bottom-up free-energy gradient from the sensory
precision, and the belief over the sphere’s position through the
likelihood error from the V AE. A valid cue setting is when the
target appears at the same position as the cue, and an invalid
cue setting when the target appears at a position opposite of
the cue with respect to the central focus point.
For each of the four task variations, N = 200 trials were
conducted. For each trial, the position of the target is randomly
generated with varying distance from the focus point. Fig. 3
shows the sequence of events in a single trial. As this cueing
task is meant to test covert attention, overt attention through
action signals was disabled.
The reaction time in simulation steps as a function of
distance from the focus point is shown in Fig. 4, for each of the
four task variations. Since the internal beliefs about the covert
attention and the sphere position are easily interpretable, we
can easily see the shifts of covert attention and sphere position
belief for the valid task variations in Fig. 5.
0 2 4 6 8 10
0
100
200
300
Target distance from focus point (px)
Reaction Time (steps)
Endogenous-Valid
Endogenous-Invalid
Exogenous-Valid
Exogenous-Invalid
Fig. 4: Reaction times and their averages as a function of target
distance from focus point (CTOA = 100 for each trial)
0 50 100 150 200 250
0
2
4
6
Simulation Time (steps)
Distance from center (px) Endogenous
Exogenous
Fig. 5: Covert attention center (dashed lines) and sphere
position beliefs (solid lines) during valid trials, for both
endogenous and exogenous cues. The horizontal line is the
true target distance from center, and the vertical lines indicate
trial events as in Fig. 3: the cue appears at step 10, disappears
at step 60, target appears at step 160.
To examine the effect that the CTOA interval plays in
reaction time, the previous trial variations were performed for
various CTOA lengths. The average reaction times are shown
in Fig. 6.
C. Action Signals from Bottom-up Attention
Since action can be determined from free-energy optimiza-
tion, overt attention in the form of eye saccades or camera
orientation changes can be as well implemented. Here we ex-
amined focus reach times for two action-update contributions:
• Top-down proprioceptive action signals: −∂˜s
∂a
T ˜Πs˜es –
these are determined from the prediction error of the
proprioceptive channel, between the proprioceptive input
0 100 200 300 400 500 600
100
120
140
160
180
200
Cue-Target Onset Asynchrony (steps)
Reaction Time (steps)
Endogenous-Valid
Endogenous-Invalid
Exogenous-Valid
Exogenous-Invalid
Fig. 6: Average trial reaction time as a function of CTOA.
Results are shown for endogenous-valid, endogenous-invalid,
exogenous-valid, exogenous-invalid task variations.
0 2 4 6 8 10 12 14 16
100
200
300
400
Target distance from focus point (px)
Reach Time (steps)
Bottom-up Action
Top-down action
Fig. 7: Reach times and their averages for different initial
target distances.
and current proprioceptive beliefs (which are attracted to
higher intentions)
• Bottom-up visual precision action signals:
1
2 Tr
h
˜Π−1
s
∂ ˜Πs
∂˜s
i
∂˜s
∂a − 1
2 ˜eT
s
∂ ˜Πs
∂˜s ˜es ∂˜s
∂a – these are
determined through the bottom-up derivative of the
precision matrix. Since the action update is dependent
only on the sensory input, only the second half of (15)
contributes to the action update.
The trials start with a 10-step initialization interval, after
which the target appears at a random position in the agent’s
field of view. The trial is finished when the agent successfully
focuses the target at the center of its field of view. The reach
times as a function of the initial target distance can be seen
in Fig. 7.
IV. D ISCUSSION
Our proposed model was tested on exogenous, endogenous,
valid and invalid variations of the Posner paradigm, as well as
on a simple target reach task. It captures the effects of both
endogenous and exogenous attention, as well as the impact of
cue validity, along with overt attention behaviors in involuntary
actions, all of which have been observed in location-based
models and human experimental data. From the results in Fig.
4 we can conclude the following:
• On average, valid cues produce faster reaction times than
invalid ones [17], [18]. This can be explained by the
location-based encoding of the target and the location-
based covert focus in the visual image. This produces
a spotlight effect suggested in location-based models
of attention [22]–[24]. An invalid cue causes a greater
shift of the ”spotlight” upon target onset, thus increasing
reaction time.
• Bottom-up exogenous cues produce faster reaction times
than top-down endogenous cues [20], [21]. Bottom-up
exogenous cues by error gradients through the V AE de-
coder are faster and require no interpretation in higher in-
tentional areas, unlike top-down endogenous cues which
require intentional interpretation of symbolic cues to
update target belief.
• reaction times for every trial variation increase as target
eccentricity increases [25]–[27]. This is a result of the
location-based object encoding, as well as the shape of
the RBF used for the precision matrix.
Regarding the shifts in covert attention demonstrated in Fig.
5, covert attentional focus is much faster to update than the
belief over the target’s location, in the case of both endogenous
and exogenous cues. This mirrors the findings that covert
attentional shifts occur quickly, before conscious perception
of target [18], [19] or active overt shifts in attention [33],
[34].
Fig. 6 illustrates the effect of different CTOA intervals on
reaction times, with invalid cues leading to faster reaction
times than valid ones in the exogenous variation after longer
CTOA intervals (∼350 steps), and in the endogenous variation
at a slightly later stage. Although not explicitly modeled, this
behavior is similar to an attentional mechanism of inhibition
of return (IOR) [24], [25], where a previously cued visual area
becomes attentionally supressed after longer CTOA intervals
in exogenous cues. Since this was not explicitly modeled, this
model behavior will be examined in future work.
Overt visual attention in the form of camera orientation
action signals was examined in a simple target reach task. The
results in Fig. 7 show that bottom-up overt orienting is overall
faster than top-down intentional orienting, which is explained
by the sensitivity of the precision to red objects (or any
predetermined visual object of interest, like faces [34]). This is
similarly reflected in how reaction time changes with distance.
Both forms of orienting exhibit an increasing trend in reaction
time as distance increases; however, top-down orienting shows
a steeper rise, indicating a greater sensitivity to distance
compared to bottom-up orienting. Although bottom-up overt
orienting is faster, it can only effectively orient to one point
in the visual area, while top-down overt orienting can handle
multiple objects through multiple flexible intentions (at the
cost of speed). We leave multiple-object overt attention for
future work.
V. C ONCLUSION
In this paper, we have proposed an active inference model
of covert and overt visual attention. The proposed model
successfully demonstrates known attentional phenomena and
mechanisms in the context of the Posner cueing task and a sim-
ple active orienting task. It shows that valid cues produce faster
reaction times than invalid cues, and that exogenous cues pro-
duce faster reaction times than endogenous cues. The model
also successfully demonstrates location-based attention, with
reaction times increasing with target eccentricity. Although
not modeled, the developed model exhibits behavior similar
to inhibition of return, with previously cued areas becoming
suppressed after a certain cue-target onset asynchrony interval.
Future work will investigate this emergence of inhibition of
return, as well as extend the model with multiple possible
targets/intentions to further test object-based and location-
based effects. Overt saccades will also be examined further,
with a focus on varying attraction to different objects. We
plan to further develop and test this framework as a model of
perception, learning, and action in autonomous robots.
REFERENCES
[1] P. Lanillos, J. F. Ferreira, and J. Dias, “Designing an artificial attention
system for social robots,” in 2015 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), IEEE, Sept. 2015.
[2] P. Lanillos, E. Dean-Leon, and G. Cheng, “Multisensory object dis-
covery via self-detection and artificial attention,” in 2016 Joint IEEE
International Conference on Development and Learning and Epigenetic
Robotics (ICDL-EpiRob), IEEE, Sept. 2016.
[3] P. Lanillos, J. F. Ferreira, and J. Dias, “Multisensory 3d saliency for
artificial attention systems,” 09 2015.
[4] S. Shomstein, X. Zhang, and D. Dubbelde, “Attention and platypuses,”
Wiley Interdiscip. Rev. Cogn. Sci., vol. 14, p. e1600, Jan. 2023.
[5] M. Corbetta and G. L. Shulman, “Control of goal-directed and stimulus-
driven attention in the brain,” Nat. Rev. Neurosci., vol. 3, pp. 201–215,
Mar. 2002.
[6] P. Mengotti, A.-S. K ¨asbauer, G. R. Fink, and S. V ossel, “Lateraliza-
tion, functional specialization, and dysfunction of attentional networks,”
Cortex, vol. 132, pp. 206–222, Nov. 2020.
[7] L. V . Kulke, J. Atkinson, and O. Braddick, “Neural differences between
covert and overt attention studied using EEG with simultaneous remote
eye tracking,” Front. Hum. Neurosci., vol. 10, p. 592, Nov. 2016.
[8] C. D. Blair and J. Ristic, “Attention combines similarly in covert and
overt conditions,” Vision (Basel), vol. 3, p. 16, Apr. 2019.
[9] H. Feldman and K. J. Friston, “Attention, uncertainty, and free-energy,”
Front. Hum. Neurosci., vol. 4, 2010.
[10] T. Parr, D. A. Benrimoh, P. Vincent, and K. J. Friston, “Precision and
false perceptual inference,” Front. Integr. Neurosci., vol. 12, p. 39, Sept.
2018.
[11] T. Parr and K. J. Friston, “Uncertainty, epistemics and active inference,”
J. R. Soc. Interface, vol. 14, p. 20170376, Nov. 2017.
[12] M. B. Mirza, R. A. Adams, K. Friston, and T. Parr, “Introducing a
bayesian model of selective attention based on active inference,” Sci.
Rep., vol. 9, p. 13915, Sept. 2019.
[13] T. Parr and K. J. Friston, “Attention or salience?,” Curr. Opin. Psychol.,
vol. 29, pp. 1–5, Oct. 2019.
[14] D. Parvizi-Wayne, “How preferences enslave attention: calling into
question the endogenous/exogenous dichotomy from an active inference
perspective,” Phenomenol. Cogn. Sci., Sept. 2024.
[15] M. W. Spratling, “Predictive coding as a model of biased competition
in visual attention,” Vision Res., vol. 48, pp. 1391–1408, June 2008.
[16] L. Itti and P. Baldi, “Bayesian surprise attracts human attention,” Vision
Res., vol. 49, pp. 1295–1306, June 2009.
[17] M. Posner, M. Nissen, and W. Ogden, “Attended and unattended pro-
cessing modes: The role of set for spatial location,” Modes of Perceiving
and Processing Information, vol. 137, 01 1978.
[18] M. I. Posner, “Orienting of attention,” Q. J. Exp. Psychol., vol. 32,
pp. 3–25, Feb. 1980.
[19] M. S. Peterson, A. F. Kramer, and D. E. Irwin, “Covert shifts of attention
precede involuntary eye movements,” Percept. Psychophys., vol. 66,
pp. 398–405, Apr. 2004.
[20] J. Jonides, “V oluntary versus automatic control over the mind’s eye’s
movement,” in Attention and Performance IX, pp. 187–203, 1981.
[21] M. Cheal and D. R. Lyon, “Central and peripheral precuing of forced-
choice discrimination,” Q. J. Exp. Psychol. A, vol. 43, pp. 859–880, Nov.
1991.
[22] S. P. Vecera and M. J. Farah, “Does visual attention select objects or
locations?,” J. Exp. Psychol. Gen., vol. 123, no. 2, pp. 146–160, 1994.
[23] R. Egly, J. Driver, and R. D. Rafal, “Shifting visual attention between
objects and locations: Evidence from normal and parietal lesion sub-
jects,” J. Exp. Psychol. Gen., vol. 123, no. 2, pp. 161–177, 1994.
[24] I. Reppa, W. C. Schmidt, and E. C. Leek, “Successes and failures
in producing attentional object-based cueing effects,” Atten. Percept.
Psychophys., vol. 74, pp. 43–69, Jan. 2012.
[25] R. M. Klein, “Inhibition of return,” Trends Cogn. Sci., vol. 4, pp. 138–
147, Apr. 2000.
[26] S. Pinker and C. J. Downing, Attention and Performance XI: Mecha-
nisms of attention and visual search. Hillsdale, NJ: Erlbaum, 1985.
[27] M. Carrasco, D. L. Evert, I. Chang, and S. M. Katz, “The eccentricity
effect: target eccentricity affects performance on conjunction searches,”
Percept. Psychophys., vol. 57, pp. 1241–1261, Nov. 1995.
[28] T. Parr, G. Pezzulo, and K. J. Friston, Active inference. The MIT Press,
2022.
[29] K. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
brain,” J. Physiol. Paris, vol. 100, pp. 70–87, July 2006.
[30] K. Friston, “The free-energy principle: a unified brain theory?,” Nat.
Rev. Neurosci., vol. 11, pp. 127–138, Feb. 2010.
[31] R. Kanai, Y . Komura, S. Shipp, and K. Friston, “Cerebral hierarchies:
predictive processing, precision and the pulvinar,” Philos. Trans. R. Soc.
Lond. B Biol. Sci., vol. 370, p. 20140169, May 2015.
[32] M. Priorelli and I. P. Stoianov, “Flexible intentions: An active inference
theory,” Front. Comput. Neurosci., vol. 17, p. 1128694, Mar. 2023.
[33] R. Walker, D. G. Walker, M. Husain, and C. Kennard, “Control of
voluntary and reflexive saccades,” Exp. Brain Res., vol. 130, pp. 540–
544, Feb. 2000.
[34] L. Kauffmann, C. Peyrin, A. Chauvin, L. Entzmann, C. Breuil, and
N. Guyader, “Face perception influences the programming of eye
movements,” Sci. Rep., vol. 9, p. 560, Jan. 2019.
[35] K. J. Friston, J. Daunizeau, J. Kilner, and S. J. Kiebel, “Action and
behavior: a free-energy formulation,” Biol. Cybern., vol. 102, pp. 227–
260, Mar. 2010.
APPENDIX
A. Implementation of gradients
The gradients with respect to beliefs, action and sensory
data given in (11) and (13) depend on the different imple-
mentations of system dynamics, generative models, sensory
precisions and the type of sensory data:
• ∂ ˜f
∂ ˜µ : The gradient of the system dynamics function
defined in (10) w.r.t. the belief µ is fairly simple, seeing
as it is defined as an affine transformation of the belief.
• ∂˜g
∂ ˜µ : The gradients of the generative models w.r.t. the
belief for the proprioceptive and interoceptive models
are simple identity matrices. However, the gradient of
the visual generative model is the gradient of the V AE
decoder computed by backpropagation.
• ∂ ˜Πs
∂ ˜µ : Since the sensory precision matrix is assumed to
be diagonal, this greatly simplifies calculation of the gra-
dients ∂πi
∂µ for each pixel i from the individual precision
functions πi(µ, s). The sensory precision gradient ∂ ˜Πs
∂ ˜µ
is a tensor of shape L × L × M.
• ∂ ˜Πµ
∂ ˜µ : The optimization of system dynamics precisions
˜Πµ is left for future work, and they are assumed to be
constant. Their gradients are therefore zero.
• ∂ ˜Πs
∂˜s : The gradient is calculated in a way similar to ∂ ˜Πs
∂ ˜µ ,
with the gradient being a tensor of shape L × L × L.
• ∂˜s
∂a : The inverse mapping from sensory data to actions
is generally considered a “hard problem” [35]. However,
it is fairly simple in our case: the centroid of the color
red is converted into pitch and yaw angles (assuming we
know the intrinsic parameters of the camera model).
B. Variational Autoencoder
The encoder consists of a convolutional layer 3 × 3 (in
channels: 3, out channels: 32), followed by four residual down-
sampling blocks (32 →64, 64→128, 128→256, 256→512). A
fully connected layer maps the 512-dimensional feature vector
to 64, followed by another producing a 2 × 8-dimensional
latent space output. The decoder mirrors this structure, with
a fully connected layer expanding 8 to 64, reshaped into a
512 × H/16 × W/16 feature map, followed by four residual
upsampling blocks (512 →256, 256 →128, 128 →64, 64 →32)
and a final 3×3 convolutional layer (32 output). The V AE was
implemented and trained in pytorch on 240,000 32 × 32 × 3
images randomly generated in the Gazebo simulator. The latent
space was disentangled with manual encodings of sphere’s
image coordinates for each of the training images.
C. Sensory Data and Belief Shape
The three different kinds of sensory input are as follows:
• Proprioceptive: pitch and yaw angles of the camera’s
orientation in the simulator, expressed in radians.
• Visual: a 32 × 32 × 3 RGB image captured by the
simulated camera model.
• Symbolic cue: a floating-point array with two elements,
containing the image coordinates that cue where the target
may appear.
The belief is a concatenation of the following elements:
• Symbolic cue belief: two elements that mirror the sensory
input for the symbolic cue
• Proprioceptive belief: two elements that mirror the sen-
sory input for the pitch and yaw angles
• Visual encoding belief: the visual encoding used by the
decoder to generate visual predictions. The first three
elements encode the sphere’s position and presence, while
the rest are free latent variables
• Covert focus belief: represents the center and amplitude
of the RBF used in the calculation of the sensory preci-
sion.