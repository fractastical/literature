Science & Society
Shared worlds, shared minds
Strategies to develop physically and socially embedded AI
Giovanni Pezzulo 1 ✉,T h o m a sP a r r2 & Karl J Friston3
T
here is a growing expectation that
humans will soon participate in the
widespread use of advanced — and
potentially “intelligent”— artiﬁcial intelli-
gence (AI) systems. Large language models
( L L M s ) ,a sw e l la sm o d e l sc a p a b l eo f
generating images and videos, are already
enjoying broad adoption. So far, users
mostly interact with AI through textual
interfaces, such as chatbots. Future AI
systems are expected to have more
advanced capabilities, functioning as
“agents” that can search the web, buy or
sell goods, engage in dialogue and make
inquiries. They might be adapted for
broader roles, such as virtual assistants,
personal coaches, or arti ﬁcial scientists.
We can also expect ﬁne-tuning through
human feedback to better align their out-
puts with human imperatives and, poten-
tially, human values. Future AI systems
may operate as coordinated collectives:
federated networks of AI agents working
together toward shared goals. Even more
transformative are robotic and embodied
AI systems: robots that can perceive and
act in the physical world — for example,
at home, in factories or hospitals — alone
or in collaboration with other AIs and
humans.
.........................................................
Future AI systems may operate as
coordinated collectives: federated
networks of AI agents working
together toward shared goals.
.........................................................
Many of these developments are already
underway, and there is an expectation that
these advanced AI models will transform
our societies. Looking further ahead, we can
imagine a future where such systems
interact with humans in increasingly inte-
grated ways, potentially augmenting human
cognitive capabilities — such as perception,
decision-making or memory — and situa-
tional awareness, based upon an under-
standing of the world. These systems may
go beyond the traditional notion of“tools”,
as they require a degree of autonomy,
pursue epistemic and instrumental goals
and continuously adapt. Our interactions
with them may evolve from simple tool
use to collaborative or even competitive
relationships towards hybrid human-AI
societies.
Our focus here is on a potential para-
digm shift: from the current approach of
training generative AI models in a largely
passive manner using massive datasets of
human-generated content toward more
interactive learning. This emerging
approach more closely mirrors how biolo-
gical agents learn through deliberative
engagement with their physical and social
environments. Such a shift raises profound
scientiﬁc, technological and societal ques-
tions. What would it mean to build AI
systems that learn not merely by observing,
but by acting, experimenting and adapting
in real-time? What are the ethical and social
implications of developing AI agents cap-
able of modifying the world around them,
much like biological organisms do? In this
commentary, we examine the foundations,
opportunities and challenges of this new
direction in AI research.
.........................................................
What are the ethical and social
implications of developing AI
agents capable of modifying the
world around them, much like
biological organisms do?
.........................................................
The challenge of scaling up
generative AI to embodied settings
The success of generative AI systems is
driven by the development of large-scale
predictive models trained on massive data-
sets. A prominent example is LLMs, which
are trained to generate text by predicting the
next word in a sequence. These models are
often referred to as “foundation models ”
because their learned representations can be
adapted for a wide range of downstream
tasks. For instance, a foundation model can
be ﬁne-tuned to power chatbots that engage
in human-like conversations, or to create
domain-speciﬁc assistants capable of
answering questions about topics such as
medicine or sports. The same methodology
has led to foundation models developed for
image and video generation, or integrating
multiple modalities, such as text and videos.
Current generative AI systems are
trained on an immense quantity of
human-generated content, including text,
images or videos. In this sense, they are
based on a large portion of what humans
have done and know. However, training
almost exclusively uses passive methodolo-
gies without intrinsic goals or agency. These
models cannot actively choose what to
attend to, read or ignore, nor can they
direct their own learning. They have no
intentions.
The implicit absence of active inference
and learning has sparked debate over
whether such systems are actually able to
understand the content they process (Pez-
zulo et al,2023). Do these models generate
their own meaning or is meaning imbued by
users— for instance, the authors who wrote
the books they learn from? Does a book
about carpentry enable an AI to know how
to make a chair? Do they converge on
consistent internal representations and
1Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy.2Nufﬁeld Department of Clinical Neurosciences, University of Oxford, Oxford, UK.
3Queen Square Institute of Neurology, University College London, London, UK.✉E-mail: giovanni.pezzulo@istc.cnr.it
https://doi.org/10.1038/s44319-025-00549-8 | Published online: 20 August 2025
1234567890();,:
© The Author(s) EMBO reports Volume 26 | September 2025 | 4197–4202 4197
form explicit models of the world outside—
like integrated spatial maps — or do they
merely appear to do so? Is meaning some-
thing these systems possess intrinsically, or
is it something we attribute to them,
projecting human-like understanding onto
pattern-matching machines? Can these sys-
tems truly generalize, discover new knowl-
edge, or are they conﬁned to interpolating
between data points they ’ve seen during
training? Can the models support planning
and reasoning and maintain beliefs over
time? Do they understand the consequences
of their action and are they accountable for
them? If they are limited, is this due to
current methods, or could scaling up —
adding more data, parameters or modal-
ities— eventually overcome these
limitations?
.........................................................
Can these systems truly generalize,
discover new knowledge, or are
they conﬁned to interpolating
between data points they’ve seen
during training?
.........................................................
There is signi ﬁcant enthusiasm — and
encouraging preliminary results — suggest-
ing that the same methodologies for train-
ing current generative AI can also power the
next generation of embodied AI agents and
robots. The engineering pipeline is already
in place and no fundamentally new con-
cepts may be required. Progress could come
from scaling up existing approaches — by
repurposing LLMs and foundation models
developed for image and video generation,
and incorporating larger and higher-quality
datasets. In the context of embodied AI, the
datasets used are expected to be multi-
modal, incorporating exteroceptive signals
— such as vision and sound— proprioceptive
information about the body and its position
in space (Driess et al, 2023) and more
egocentric, dynamic and temporally rich
signals such as video sequences instead of
static images.
These datasets can be acquired in part
through human demonstrations and by
leveraging physical simulators, which would
reduce the cost of training robots in the
physical world. Such datasets might gener-
ate foundation models that support percep-
tion and action. For example, “artiﬁcial
visual cortices ” mimic some aspects of
biological visual processing where the
learned representations can later be used
to train robotic controllers (Nair et al,2022;
Majumdar et al, 2023). These multimodal
systems pave the way for richer robotic
behaviours and alleviate issues about “true
understanding ”; for example, compared to
models trained just on language, models
trained with additional exteroceptive and
proprioceptive modalities might develop a
more grounded understanding of physical
notions such as“force” or “temperature”.
Another key challenge for embodied AI
and robotics is developing predictive mod-
els that support not only short-term actions,
but also long-term planning and goal-
directed behaviour. To achieve this, the
concept of a world, a.k.a. a generative
model, has gained particular traction. A
world model is a predictive representation
of the environment and how it changes in
response to the agent ’s actions. An agent
can use a world model to guide decision-
making, plan action sequences and improve
its behaviour by internally simulating
actions and their consequences — learning
“in imagination ” rather than from direct
experience (Hafner et al, 2025;H aa n d
Schmidhuber, 2018; Schrittwieser et al,
2020;V a nd eM a e l ee ta l ,2024). Generative
world models are already being applied in
practical domains such as autonomously
driving cars (Russell et al, 2025;H ue ta l ,
2023). Recent developments include world
models trained through self-supervised
learning in egocentric environments, which
can predict not only future states based on
human actions, but also the actions them-
selves, based on learned state transitions
(Chen et al,2025;L e C u n ,2022).
These developments — particularly those
guided by the notion of multimodal and
world models— represent a promising tra-
jectory for embodied AI. While starting
from passive training regimes, they start to
include some interactive dynamics, albeit in
a limited form. Yet, if we zoom out from the
engineering accomplishments behind
advanced generative AI and instead ask
how living organisms develop intelligence,
we ﬁnd both important parallels and
fundamental differences.
Learning like living organisms
The idea of generative world models is a
point of contact between current progress in
embodied AI and our understanding of
biological organisms. In living creatures,
sentient behaviour — the capacity to infer
the state of the world and act upon it with
purpose— is fundamentally predictive and
relies on grounded world models that
anticipate the consequences of one’sa c t i o n s
and support adaptive, intentional engage-
ment with the environment (Taniguchi et al,
2023). This aligns with active inference,
which frames perception, action and learn-
ing as agentive and predictive (Parr et al,
2022). However, generative models in biol-
ogy are distinct from those — passively—
learned by current generative AI systems.
Living organisms develop their intelli-
gence— and learn grounded world models—
through active interaction with the world,
which includes other living beings. They
learn while they survive and remain opera-
tional for sustained periods of time. As
such, they engage with their environment at
multiple timescales. At the fastest timescale
of perception, every sensation is typically
preceded by an action — such as an eye
movement— making perception inherently
enactive. At the timescale of goal-directed
behaviour, actions are used to change the
world to satisfy needs ranging from
immediate homoeostatic goals to more
complex, long-term objectives. Organisms
must not only predict the world but do so in
a way that supports adaptive action and
survival. At the timescale of learning,
knowledge is acquired through agentive
experience by interacting with the environ-
ment and selecting one’s own sensory and
behavioural trajectories.
Even in situations that appear passive—
for instance, a child watching others or a
teacher— learning occurs within social and
goal-directed contexts. At the timescale at
which societies operate, organisms modify
their environments: creating ecological
niches, forming groups and participating
in cultural niche construction (Constant
et al, 2018; Friston et al, 2024). These
changes, which are particularly important in
sophisticated human societies but are also
apparent in other animals, shape future
interactions and learning environments. At
the slowest evolutionary timescale, struc-
tures and strategies for perception, action
and learning are shaped and passed down
biologically, providing a scaffold for more
rapid, individual-level learning (Pezzulo
et al, 2022; Cisek, 2019). Generative models
in biological systems rely on dedicated
architectures selected by evolution. Com-
mon to these are internal feedback loops
and spontaneous activity: key features —
largely absent in most current AI systems—
EMBO reports Giovanni Pezzulo et al
4198 EMBO reports Volume 26 | September 2025 | 4197–4202 © The Author(s)
that contribute to the robustness and
adaptability of biological intelligence. At
all these timescales, biological intelligence is
characterized by active participation —
selecting, predicting, experimenting and
interacting with others. In doing so, living
creatures adapt in real time to a complex
and dynamic physical and social
environment.
.........................................................
At all these timescales, biological
intelligence is characterized by
active participation— selecting,
predicting, experimenting and
interacting with others.
.........................................................
The central role of action means that
learned models emerge from continuous
action and perception loops. Knowledge is
grounded in sensorimotor experience,
through action possibilities that enable
generalizable representations of how the
physical world works (Barsalou, 2008). As
such, they depend upon the anatomy and
physiology of the creature whose model it is,
reﬂecting the emphasis on embodiment in
psychology and neuroscience. This means
that a creature’s world model must include
their body and internal states (interocep-
tion), the body ’s position and movement
(proprioception) and the sensed conse-
quences of that movement (exteroception).
Part of this model may include the narrative
summarization of one ’s personal experi-
ences. These models are crucial for support-
ing basic functions, such as forming a body
schema for purposeful, goal-directed action.
Interestingly, body models are developed in
the context of constantly changing bodies as
they grow, adapt and evolve. Embodiment is
dynamic, and so are these models. In
contrast, most current robotic learning
assumes ﬁxed, adult-like bodies operating
in constrained environments.
Crucially, such models also enable the
capacity to distinguish the self from the
environment, forming the basis for minimal
selfhood and a sense of self. On this view,
biological agents form self-models bounded
by a so-called Markov blanket: a statistical
boundary separating an agent ’si n t e r n a l
states from the external world (Parr et al,
2022). Once this separation emerges, the
system can behave autonomously, striving
to maintain its integrity through processes
like autopoiesis (self-creation),
homoeostasis (internal stability), and allos-
tasis (anticipatory regulation). Even in
primitive organisms, relatively simple
mechanisms (based on corollary discharge
signals) provide the ability to distinguish
self-produced effects from external events: a
foundational step in building a deep under-
standing of the lived world.
The development of internal models of
the body and self is central toagency— that
is, it ’s my action that matters, not just
action. The fact that learning is rooted in
the perspective of an agent becomes parti-
cularly compelling in the context of achiev-
ing goals ranging from basic homoeostatic
imperatives to preserve the organism’so w n
integrity to complying with sophisticated
cognitive and social norms. Interoception
plays a key role here: organisms learn what
matters to them by sensing internal bodily
states and using this feedback to guide
behaviour. In this view, the brain’sp r i m a r y
imperative is constructing a model that
supports adaptive interaction, not necessa-
rily a high-ﬁdelity map of the world. These
models inherently include what is good or
bad for the organism, making them value-
laden and action-oriented, as they have to
afford purposeful and adaptive interaction
with the world, and not justunderstand it
(Pezzulo et al, 2024).
Finally, social interaction offers funda-
mental differences between people and
current AI systems. Human interaction is
grounded in sensorimotor experience
shared with others , forming the basis for
mutual understanding of the physical and
social world. Early in life, humans engage in
what has been called aninteraction engine—
a system of nonverbal sensorimotor com-
munication— such as gaze, gesture, turn-
taking—
which later scaffolds the develop-
ment of linguistic communication. This
process contrasts sharply with LLMs’ pas-
sive acquisition of language without
grounding in the physical or conversational
contexts in which their training data were
generated. As a result, social organisms
develop rich world models that go beyond
individualistic representations: models that
are inherently prosocial, shaped through
direct engagement with other agents. Cru-
cially, this social knowledge is not built on
top of pre-existing, non-social representa-
tions; it is deeply integrated from the outset
and fundamental to how the world is
modelled and understood. In fact, one
might argue that the world models of
advanced social organisms like us are, in
many respects, inherently social and
extended— much of the knowledge that
underpins our understanding of reality
resides not in our own minds, but in the
minds of others and in external artifacts
such as books, tools or the internet (Clark
and Chalmers, 1998).
.........................................................
… social organisms develop rich
world models that go beyond
individualistic representations:
models that are inherently
prosocial, shaped through direct
engagement with other agents.
.........................................................
T h r e er o u t e st of u t u r ee m b o d i e dA I
and robotic systems
Given these premises, one might ask
whether we should follow the same pathway
as nature in developing future embodied AI
a n dr o b o t i cs y s t e m s .B r o a d l y ,t h r e er o u t e s
can be envisioned. First, continue training
AIs passively on large, human-curated
datasets, as is currently done with LLMs.
Second, shift toward interactive learning, in
which AIs learn by actively engaging with
and modifying their physical environment.
Third, extend this interactive approach to
include rich social contexts, where AIs learn
in collaboration with other AIs and humans
— much like social organisms.
First route: scale what already works
The success of foundation models in gen-
erative AI — particularly in language and
vision— has suggested a clear engineering
pathway for robotics: scale up what already
works. Researchers can continue to scale
and ﬁne-tune systems trained passively on
large, human-curated datasets and extend
the same methodologies that have produced
powerful predictive models by incorporat-
ing more datasets, modalities and comput-
ing power. This approach can be
complemented with learning from demon-
stration, simulations and limited real-world
interactions. Foundation models, in parti-
cular, offer the beneﬁt of integrating data
across robotic systems with different embo-
diments, enabling generalization and reuse.
From an engineering standpoint, what
matters most is building functional systems.
If robots can act in the world and respond
Giovanni Pezzulo et al EMBO reports
© The Author(s) EMBO reports Volume 26 | September 2025 | 4197–4202 4199
to human instructions— such as clearing a
house or assisting with caregiving tasks —
then, for many applications, it may not
matter how that functionality was achieved,
w h e t h e ro rn o ti tr e s e m b l e st h el e a r n i n g
processes of living organisms, or whether
the system’s internal operations qualify as
‘understanding ’ in a cognitive or
philosophical sense.
However, this view may overlook a key
variable: the role of agency in learning. Does
the nature of the learning regime— passive
ingestion versus active, goal-directed inter-
action— shape the kinds of models that
emerge? One possibility is that, given
sufﬁciently large and diverse datasets,
agency may be unnecessary: every relevant
contingency could be encoded and systems
might converge on effective internal repre-
sentations regardless of how the data was
acquired. Yet, this assumption might come
with limitations. Learning exclusively from
human-generated data may impose a ceil-
ing, as the learner is con ﬁned to what
humans know and express. Without the
capacity to experiment or interact, the
system cannot exceed the bounds of its
training data or uncover novel insights.
By contrast, when agents engage with
their environment by selecting inputs, gen-
erating new experiences, and pursuing self-
determined goals, they build richer and
more adaptive predictive models, particu-
larly of what matters to them as embodied
agents. Such interaction allows them not
only to discover new regularities, but also to
infer causal structures and develop repre-
sentations that go beyond the statistical
patterns present in curated datasets. Cru-
cially, the requisite world models should be
maximally efﬁcient and compressed repre-
sentations of the cause–effect structures that
matter for the agent in question. This points
in the opposite direction to the engineering
solution of scaling up. It points towards the
principles that underwrite the biomimetic
efﬁciency of learning and inference.
Finally, training outside the challenging
conditions of the real world may limit a
system’s ability to adapt to environments
characterized by noise, uncertainty and
ambiguity. It is therefore possible that,
rather than building robots capable of
navigating unstructured, dynamic environ-
m e n t sa sh u m a n sd o ,w em i g h tn e e dt o
simplify the environments to better suit the
capabilities of current AI systems. This
could involve designing homes, factories or
roads that align more closely with the
strengths and limitations of artiﬁcial agents.
The same consideration applies to interac-
tion communication: rather than expecting
reﬁned sensorimotor and linguistic interac-
tion, we might adapt our exchanges to ﬁt
the kinds of statistical regularities that the
AI systems can handle more reliably.
Second route: go interactive
Learning through interactive engagement
might offer an alternative, potentially more
grounded and adaptive path forward, than
current passive approaches. This holds for
physically embodied systems, such as living
organisms that must learn to plan and act
through sensorimotor interaction, but also
for non-embodied AI systems. For example,
a digital sport coach could learn through
experience— by modelling and predicting
game performance indices— which advice is
most effective or helpful for athletes.
When considering the possibility of
training embodied AI systems through
purposeful interaction with their environ-
ment, one opens the door to“superhuman”
solutions that go beyond human-devised
strategies, where human data is no longer a
ceiling (Silver and Sutton, 2025). Such a
shift could also address an additional
limitation of other methods: learning from
human-generated data may eventually hit a
plateau, while sensorimotor interaction with
the world presents a pathway to effectively
inﬁnite data, continuously shaped by the
agent’s own experience.
However, this approach comes with
substantial costs — particularly in time,
energy and compute— especially when rely-
ing on current AI technology, which is often
inefﬁcient in terms of data and computa-
tional resources. Moreover, interactive
learning in physical environments presents
challenges for which existing techniques
might be too brittle. Perceiving and acting
in real time and adapting to dynamically
changing setups remains difﬁcult for most
current AI systems. Real-world settings are
noisy and ambiguous and require ﬂexible
adaptation to novel contingencies in real
time— conditions under which today’sg e n -
erative AI algorithms tend to perform
poorly. Unlike simulation, real-world learn-
ing involves physical constraints such as
friction, mechanical wear and actuation
latency, which signi ﬁcantly complicate
motor learning. Even when leveraging
autonomous systems trained by trial and
error and world models that perform well in
domains with well-de ﬁned performance
metrics— such as games — scaling beyond
these domains is challenging. In sum, the
strategies that underpin the success of
current AI approaches may not easily
translate to real-world interactive domains.
Although nature provides a proof of con-
cept— biological brains succeed at precisely
this kind of learning— the design principles
of natural intellige nce are not currently
implemented in an artiﬁcial setting. Hence,
a more principled approach, not just new
datasets or more computing power, will
likely be required.
If successful, a biomimetic approach
c o u l dl e a dt oas h i f tf r o mA Is y s t e m s
functioning merely as tools — designed to
follow instructions — toward systems that
exhibit superhuman capabilities, pursue
their own goals, and act as autonomous
agents within hybrid human-AI societies.
However, beyond functionality, there are
broader though not universally agreed upon
desiderata in AI development, such as
interpretability and value-alignment. Inter-
pretability asks whether we can understand
why systems behave as they do, while value-
alignment concerns whether arti ﬁcial sys-
tems can adopt goals consistent with human
norms. Generative AI systems are already
largely considered “black boxes”, and align-
ing them with human values remains a
signiﬁcant challenge. Systems that learn
through autonomous interaction offer fewer
opportunities for human oversight, requir-
ing greater effort to mitigate risks and
ensure safety. While such systems may
develop a deeper “understanding ” of the
world, this understanding could remain
opaque to us, unless they also learn to align
with the shared world models that we, as
humans, co-construct. They may also devise
impenetrable strategies to achieve their
goals, raising critical questions about align-
ment with human values and ethical
principles.
.........................................................
Generative AI systems are already
largely considered “black boxes,”
and aligning them with human
values remains a signiﬁcant
challenge.
.........................................................
These concerns become even more
pressing in the context of hybrid human-
AI societies, where future embodied AI
EMBO reports Giovanni Pezzulo et al
4200 EMBO reports Volume 26 | September 2025 | 4197–4202 © The Author(s)
systems are expected to support human
goals, augment our capabilities, and con-
tribute positively to our shared world. The
key question is whether we will be able to
coordinate with these systems, understand
their behaviour and trust them as genuine
partners— or whether we can embed align-
ment and interpretability into their design
from the outset.
Third route: interaction with humans
in the loop
What is between learning to model and act
in the world from human data and learning
through autonomous experience without
humans? Perhaps it’s learningwith humans,
as we do in our societies. In this paradigm,
h u m a n sa r en o tt h eo n l ys o u r c eo fi n p u t— as
would be the case if generative AIs only
imitated human-curated datasets — nor are
they removed from the loop entirely — as
would be the case if experiential AIs
developed their knowledge fully by them-
selves. Instead, humans remainin the loop.
We are a deeply social species. While we
learn autonomously, we rarely do so in
isolation. Our exploration is embedded in
social dynamics— whether in close relation-
ships, such as parent –child or
teacher–student interactions, or in broader
collectives like teams and societies. We
don’t just form personal world models: we
live and learn through cooperation, aligning
our thoughts and forming shared models of
the world, grounded in collaboration and
communication (Pezzulo et al, 2025).
Learning in these contexts is not merely
about individual cognition, but about form-
ing world models that are public, commu-
nicable and cooperative. It’sn o ts i m p l ya n
individual modelling challenge but a collec-
tive alignment problem— one that involves
calibrating our understanding not just with
the physical world, which offers hard
constraints for all, but also with the minds
of others. It ’s like learning to play team
sports: a soccer player develops the ability to
pass in the context of others being ready to
receive— not in a vacuum. Humans rely on
shared mental models— not just of language,
but of common sense, physical dynamics
and persistent objecthood, as well as more
subtle, culturally embedded moral and
social norms. Even our subjective experi-
ences, like emotions, are shaped not only by
private, such as interoceptive, signals but
also by public interpretation and social
sharing.
.........................................................
Humans rely on shared mental
models— not just of language, but
of common sense, physical
dynamics and persistent
objecthood, as well as more subtle,
culturally embedded moral and
social norms.
.........................................................
A compelling hypothesis is that if
embodied AI systems are trained— at least
in part— in social settings through ongoing
interaction with people, they may develop
aligned representations from the outset:
shared, negotiated and embedded in human
norms. This may also lead to more robust
communication and social interaction, as
these skills are learned in the context of
embodied, cooperative activities.
Artiﬁcial learning through social inter-
action— not just physical engagement — is
something that is still largely unexplored
and it remains to be established whether it is
feasible and effective. Current methodolo-
gies for developing autonomous systems—
such as self-driving cars or home-assistant
robots— still rely on highly simpliﬁed mod-
els of social interaction, even when learning
is under human guidance. However, it may
be a strategy for developing AI systems that
form genuinely shared world models with
humans. Such sharing is central to human
societies. We implicitly assume that others
understand the world in ways similar to us
— basic physics, object permanence or the
rules of games— and, more importantly, that
they grasp the social and moral norms that
guide behaviour. This mutual understand-
ing is so fundamental that we often take it
for granted. When it’s lacking, as might be
the case with AI systems, trust and coordi-
nation break down. Even though we don’t
fully understand how other people work
internally, we still trust them because we
share a common experiential grounding,
goals, and social context.
But can we place similar trust in AI
systems that lack this grounding? It would
be much harder to trust, collaborate with or
delegate to agents that do not share our
understanding of the physical, social and
moral dimensions of the world. Notably, the
ﬂuency with which current generative AIs
converse can create the illusion of align-
ment. But ﬂuency alone does not imply
shared understanding and common ground.
Much like the classic ELIZA effect — the
tendency to project human traits onto
computer programs — coherent language
use can mask a lack of genuine comprehen-
sion, leading us to overestimate alignment
where it may not exist.
Perhaps some superhuman abilities may
be sacriﬁced when AI systems are designed
to align with human expectations and
constraints. But if we are to treat future
embodied AIs not just as tools but as agents
or partners, then shared understanding,
communication and value alignment are
not optional — they are foundational. In
democratic societies, collaboration is built
on alignment, trust and communication —
not blind delegation. These considerations
suggest that human-AI alignment should be
addressed early in the design of new AI
systems— especially those that learn through
interactive engagement with the world— not
as an afterthought.
.........................................................
… if we are to treat future
embodied AIs not just as tools but
as agents or partners, then shared
understanding, communication
and value alignment are not
optional— they are foundational.
.........................................................
Peer review information
A peer reviewﬁle is available athttps://doi.org/
10.1038/s44319-025-00549-8
References
Barsalou LW (2008) Grounded cognition. Annu Rev
Psychol 59:617–645
Chen L, Wang Y, Tang S, Ma Q, He T, Ouyang W,
Zhou X, Bao H, Peng S (2025) EgoAgent: a joint
predictive agent model in egocentric worlds.
Preprint athttps://arxiv.org/abs/2502.05857
Cisek P (2019) Resynthesizing behavior through
phylogenetic reﬁnement. Attent Percept
Psychophys 81:2265–2287
Clark A, Chalmers DJ (1998) The extended mind.
Analysis 58:10–23
Constant A, Ramstead MJ, Veissiere SP, Campbell
JO, Friston KJ (2018) A variational approach to
niche construction. J R Soc Interface 15:20170685
Driess D, Xia F, Sajjadi MSM, Lynch C, Chowdhery A,
Ichter B, Wahid A, Tompson J, Vuong Q, Yu T
et al (2023) PaLM-E: an embodied multimodal
language model. Preprint athttps://arxiv.org/
abs/2303.03378
Giovanni Pezzulo et al EMBO reports
© The Author(s) EMBO reports Volume 26 | September 2025 | 4197–4202 4201
Friston KJ, Parr T, Heins C, Constant A, Friedman D,
Isomura T, Fields C, Verbelen T, Ramstead M,
Clippinger J et al. (2024) Federated inference and
belief sharing. Neurosci Biobehav Rev 156:105500
Ha D, Schmidhuber J (2018) World models. Preprint
at https://arxiv.org/abs/1803.10122
Hafner D, Pasukonis J, Ba J, Lillicrap T (2025)
Mastering diverse control tasks through world
models. Nature 640:647–653
Hu A, Russell L, Yeo H, Murez Z, Fedoseev G, Kendall
A, Shotton J, Corrado G (2023) Gaia-1: a
generative world model for autonomous driving.
Preprint athttps://arxiv.org/abs/2309.17080
LeCun Y (2022) A path towards autonomous
machine intelligence. Open Rev 62:1–62
Majumdar A, Yadav K, Arnaud S, Ma YJ, Chen C,
Silwal S, Jain A, Berges V-P, Abbeel P, Malik J et al
(2023) Where are we in the search for an
artiﬁcial visual cortex for embodied intelligence?
Preprint athttps://arxiv.org/abs/2303.18240
Nair S, Rajeswaran A, Kumar V, Finn C, Gupta A
(2022) R3m: a universal visual representation for
robot manipulation. Preprint athttps://arxiv.org/
abs/2203.12601
Parr T, Pezzulo G, Friston KJ (2022) Active inference:
the free energy principle in mind, brain, and
behavior. MIT Press, Cambridge, MA
Pezzulo G, D’Amato L, Mannella F, Priorelli M, Van de
Maele T, Stoianov IP, Friston K (2024) Neural
representation in active inference: using
generative models to interact with— and
understand— the lived world. Ann NY Acad Sci
1534:45–68
Pezzulo G, Knoblich G, Maisto D, Donnarumma F,
Pacherie E, Hasson U (2025) A predictive
processing framework for joint action and
communication. Preprint at PsyArXivhttps://
doi.org/10.31234/osf.io/q4jnr_v1
Pezzulo G, Parr T, Cisek P, Clark A, Friston K (2023)
Generating meaning: active inference and the
scope and limits of passive AI. Trends Cogn Sci.
28(2):97–112
Pezzulo G, Parr T, Friston K (2022) The evolution of
brain architectures for predictive coding and
active inference. Philos Trans R Soc B
377:20200531
Russell L, Hu A, Bertoni L, Fedoseev G, Shotton J,
Arani E, Corrado G (2025) Gaia-2: a controllable
multi-view generative world model for
autonomous driving. Preprint athttps://
arxiv.org/abs/2503.20523
Schrittwieser J, Antonoglou I, Hubert T, Simonyan K,
Sifre L, Schmitt S, Guez A, Lockhart E, Hassabis D,
Graepel T et al. (2020) Mastering Atari, Go,
chess and shogi by planning with a learned model.
Nature 588:604–609
Silver D, Sutton RS (2025) Welcome to the era of
experience. Google AI 1
Taniguchi T, Murata S, Suzuki M, Ognibene D,
Lanillos P, Ugur E, Jamone L, Nakamura T, Ciria A,
Lara B et al (2023) World Models and predictive
coding for cognitive and developmental robotics:
frontiers and challenges. In: Advanced robotics.
Taylor & Francis, Abingdon, p 780–806
Van de Maele T, Dhoedt B, Verbelen T, Pezzulo G
(2024) A hierarchical active inference model of
spatial alternation tasks and the hippocampal-
prefrontal circuit. Nat Commun 15:9892
Acknowledgements
This research received funding from the European
Research Council under the Grant Agreement No.
820213 (ThinkAhead) to GP. KF is supported by
funding from the Wellcome Trust (Ref: 226793/Z/
22/Z). TP is supported by an NIHR Academic
Clinical Fellowship (ref: ACF-2023-13-013).
Disclosure and competing interests
statement
The authors declare no competing interests.
Open AccessThis article is licensed under a Creative
Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution
and reproduction in any medium or format, as long as
you give appropriate credit to the original author(s)
and the source, provide a link to the Creative
Commons licence, and indicate if changes were
made. The images or other third party material in this
article are included in the article’s Creative
Commons licence, unless indicated otherwise in a
credit line to the material. If material is not included
in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation
or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To
view a copy of this licence, visithttp://
creativecommons.org/licenses/by/4.0/. Creative
Commons Public Domain Dedication waiverhttp://
creativecommons.org/publicdomain/zero/1.0/
applies to the data associated with this article, unless
otherwise stated in a credit line to the data, but does
not extend to the graphical or creative elements of
illustrations, charts, orﬁgures. This waiver removes
legal barriers to the re-use and mining of research
data. According to standard scholarly practice, it is
recommended to provide appropriate citation and
attribution whenever technically possible.
© The Author(s) 2025
EMBO reports Giovanni Pezzulo et al
4202 EMBO reports Volume 26 | September 2025 | 4197–4202 © The Author(s)