TYPE Original Research
PUBLISHED 12 November 2025
DOI 10.3389/fncom.2025.1408836
OPEN ACCESS
EDITED BY
Kechen Zhang,
Johns Hopkins University, United States
REVIEWED BY
Guozhang Chen,
Graz University of Technology, Austria
Dodi Devianto,
Andalas University, Indonesia
*CORRESPONDENCE
Bailu Si
bailusi@bnu.edu.cn
RECEIVED 23 April 2024
ACCEPTED 14 October 2025
PUBLISHED 12 November 2025
CITATION
Z h uC ,Z h o uK ,T a n gF ,T a n gY ,L iXa n dS iB
(2025) A hierarchical Bayesian inference
model for volatile multivariate exponentially
distributed signals.
Front. Comput. Neurosci.19:1408836.
doi: 10.3389/fncom.2025.1408836
COPYRIGHT
© 2025 Zhu, Zhou, Tang, Tang, Li and Si. This
is an open-access article distributed under the
terms of theCreative Commons Attribution
License (CC BY). The use, distribution or
reproduction in other forums is permitted,
provided the original author(s) and the
copyright owner(s) are credited and that the
original publication in this journal is cited, in
accordance with accepted academic practice.
No use, distribution or reproduction is
permitted which does not comply with these
terms.
A hierarchical Bayesian inference
model for volatile multivariate
exponentially distributed signals
Changbo Zhu1,2,3,K eZ h o u4,F e n g z h e nT a n g1,2,3,
Yandong Tang1,2,3, Xiaoli Li5 and Bailu Si6,7*
1State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences,
Shenyang, China,2Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences,
Shenyang, China,3University of Chinese Academy of Sciences, Beijing, China,4Beijing Key Laboratory
of Applied Experimental Psychology, School of Psychology, Beijing Normal University, Beijing, China,
5State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing,
China, 6School of Systems Science, Beijing Normal University, Beijing, China,7Chinese Institute for
Brain Research, Beijing, China
Brain activities often follow an exponential family of distributions. The
exponential distribution is the maximum entropy distribution of continuous
random variables in the presence of a mean. The memoryless and peakless
properties of an exponential distribution impose difficulties for data analysis
methods. To estimate the rate parameter of multivariate exponential distribution
from a time series of sensory inputs (i.e., observations), we constructed
a hierarchical Bayesian inference model based on a variant of general
hierarchical Brownian ﬁlter (GHBF). To account for the complex interactions
among multivariate exponential random variables, the model estimates the
second-order interaction of the rate intensity parameter in logarithmic space.
UsingvariationalBayesianscheme,afamilyofclosed-formandanalyticalupdate
equations are introduced. These update equations also constitute a complete
predictive coding framework. The simulation study shows that our model has
the ability to evaluate the time-varying rate parameters and the underlying
correlationstructureofvolatilemultivariateexponentiallydistributedsignals.The
proposedhierarchicalBayesianinferencemodelisofpracticalutilityinanalyzing
high-dimensional neural activities.
KEYWORDS
online Bayesian learning, hierarchical ﬁlter, Brownian motion, exponential distribution,
adaptive observation
1 Introduction
Decoding of the states of neural systems is a critical task for many applications in
neural engineering, ranging from cognitive assessment, brain–machine interface to deep
brain stimulation (Haynes and Rees, 2006; Qi et al., 2019; Youseﬁ et al., 2019; Xu et al.,
2021; Zhang et al., 2022; Pan et al., 2022; Li and Le, 2017). However, there are several
criticalchallengesfacedbymentalstatedecodingmethods.First,brainactivitiesarehighly
non-stationary, often showing transient dynamics. Second, responses of diﬀerent brain
regions are correlated, due to the dense complex anatomical connectivity patterns. Third,
imaging processes of brain activities imposed additional spatial temporal transformations
on neural signals, calling for appropriate inference methods to uncover the underlying
brain states. To tackle these diﬃculties, methods that are capable of tacking and inferring
multi-dimensionaldynamicbrainsignalsareindispensable.
Frontiersin ComputationalNeuroscience 01 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
Brain activities are shown to follow particular types of
distributions that are distinctive from Gaussian distributions
(Roxinetal.,2011 ).Extracellularrecordingsofbrainvoltagesignals
of various brain regions from diﬀerent animals could be described
by an exponential family of distributions, with tails falling oﬀ
according to exponential distributions (Swindale et al., 2021). The
distributionsoftheelectromyographyandelectroencephalography
signals from human subjects are found to have fatter tails than
that of a Gaussian distribution and are ﬁtted well by a generalized
extremevaluedistribution( Nazmietal.,2015 ).Theinnatestatistics
of the measured neural activities lead the direct application of
classic tracking and inference methods, such as Kalman ﬁltering,
to be suboptimal (Li et al., 2009; Malik et al., 2010). It is therefore
a valuable research direction to develop inference methods that
closelymatchthecharacteristicsofbrainactivities.
Exponential distributions well describe empirical data in
neuroscience. Neurons in many regions, such as middle temporal
and medial superior temporal visual areas in monkeys, ﬁre in
a Poisson-like fashion, with exponential distributed interspike
intervals (Maimon and Assad, 2009; Ouyang et al., 2023). The
sleep episode durations of human and other mammals, such
as cats and rats, follow exponential distributions ( Lo et al.,
2004). The locomotion activity of cells in vitro displays a
universalexponentialdistribution( Cziróketal.,1998 ).Inaddition,
exponential distribution provides a good description of waiting
times in the physical world, including lifespans, counts within
a ﬁnite time period and so on. Therefore, researchers employ
exponential distribution as lifetime distribution model to describe
the lifetimes of manufactured products (Davis, 1952; Epstein and
Sobel, 1953; Varde, 1969) and the survival or remission times in
chronic diseases (Shanker et al., 2015). In physics, an exponential
distribution is the best model of the times between successive ﬂaps
of a ﬂag for a variety of wind speeds (McCaslin and Broussard,
2007). In ﬁnance, accumulating evidences have suggested that
ﬁnancial data can be quantiﬁed by exponential distributions. A
study of tax and census data shows an exponential distribution of
individualincomeintheUnitedStates( Dr˘agulescuandYakovenko,
2001).Anexponentialdistributionalsoagreeswellwithincomefor
familieswithtwoearners( Dr˘agulescuandYakovenko,2001 ).
In this article, we aim to develop an inference model
particularly to deal with the problem of volatility and multi-
dimensionalityindataspace.Importantly,weassumethatthedata
followamultivariateexponentialdistribution,capturingthefattail
characteristicsofneuralsignals.Theproposedmodelcanbeapplied
tostateestimationtasksinpsychophysics,brainactivityanalysis,as
wellasothernon-lineartimeseriesmodelingtasks.
In probability theory, exponential distribution is a maximum
entropy distribution of a continuous random variable with a
bounded mean (Jaynes, 1982; Conrad, 2004; Stein et al., 2015).
The exponential distribution has several interesting and important
properties (Johnson et al., 2002; Ibe, 2014; Marshall and Olkin,
1967b):
Abbreviations: GHBF, general hierarchical Brownian ﬁlter; STD, standard
deviation;iid,independentlyidenticallydistribution;SI,samplinginterval;PDF,
probability density function.
• An exponential distribution is governed by a rate parameter
(interpretedastheinverseofaveragewaitingtime).Themean
of an exponential random variable is equal to the standard
deviation(std).
• Exponential distribution is peakless. The probability density
function of an exponential distribution is monotonously
decreasing. The expectation of an exponential random
variableisnotatthemaximumpointofitsprobabilitydensity
function.Thismeansthatsamplesdrawnfromanexponential
distributioncontainhighnoise,resultinginafattail.
• Anexponentialrandomvariableismemoryless,i.e.,
P(x > t +ϵ | x >ϵ ) = P(x > t),∀t,ϵ> 0.
In a Poisson process, this memoryless property means that
the probability of waiting time until the next event is not
aﬀected by start time (Kingman, 1992). All waiting times are
independentlyidenticallydistribution(iid).
Due to these characteristics, ﬁtting models of multivariate
exponential distribution is a diﬃcult problem encountered in
variousdisciplines.TheMarshall–Olkinexponentialdistributionis
introduced based on shock models and the constraint that residual
life and age are independent (Marshall and Olkin, 1967a). An
exponential distribution with exponential minimums provides a
model to describe the reliability of a coherent system (Esary and
Marshall, 1974). Abivariate generalizedexponential distributionis
alsointroducedtoanalyzelifetimedataintwodimensions( Kundu
and Gupta, 2009). However, these models are complex in form
and are not robust for non-stationary data. More importantly, the
interactions among the components of a multivariate exponential
variable are not trivial to estimate. These classical studies took
the assumption of static distributions, without considering the
dynamic changes of the underlying distributions. Robust methods
for the estimation of multivariate exponential distribution in
volatileenvironmentsarestillsparse.
“Observing the observer” is a meta Bayesian
framework (Daunizeau et al., 2010b,a) and furnishes a uniﬁed
programming and modeling framework that unites perception
and action based on the variational free energy principle (Beal,
2003; Friston, 2010; Mathys et al., 2011; Friston et al., 2017).
Perceptual and response models are two major parts of this
framework. Inversion of the perceptual and response models can
map from sensory inputs (i.e., observations) into response actions.
Following this framework, the general hierarchical Brownian ﬁlter
(GHBF) was proposed as a model for state estimate in dynamic
multi-dimensional environments with Gaussian distribution
assumption(Zhuetal.,2025 ).Animportantfunctionofthismodel
istocapturetemporaldynamicsoflowerorderinteractionsamong
sensoryinputs(i.e.,observations).
Inthisarticle,weextendthegeneralhierarchicalBrownianﬁlter
to non-Gaussian case and develop an inference model for volatile
multivariateexponentiallydistributedsignals.Theinferencemodel
incorporatesahierarchicalperceptualmodelandaresponsemodel
into the “observing the observer” framework. The model receives
a series of multidimensional sensory inputs or observations and
is asked to infer rate parameter of a multivariate exponential
distribution in a complex volatile environment. The perceptual
Frontiersin ComputationalNeuroscience 02 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
model represents rate parameter and covariance of the logarithm
of rate parameter. The response model is a stochastic mapping
to reproduce a series of sensory inputs. Compared with previous
hierarchical Bayesian methods (Beal, 2003; Friston, 2010; Mathys
et al., 2011; Friston et al., 2017), the proposed model is able to
deal with multidimensional signals and dynamically uncover the
potentialcorrelationstructureinthedata.
The contribution of this article is two-fold. First, we develop
a hierarchical Bayesian model to estimate the parameters of
multivariateexponentialdistributionswhicharesubjecttodynamic
changes. Through variational Bayesian learning, the model infers
the rate parameters and the pairwise correlations of multivariate
exponentially distributed signals at the same time; therefore, it is
able to robustly track the distribution dynamically. The proposed
model is valuable for its potential applications in estimating neural
andbehavioralresponses.Second,theeﬃciencyandtherobustness
of the proposed inference model is tested in simulations with
synthetic dynamic data. Compared with a simpliﬁed model of
constant volatility parameters, the proposed model is better in
explaining the data, demonstrating the importance role of higher
order variables, such as correlations, in estimating the parameters
ofthesignal.
The rest of this article is structured as follows. The
mathematical notations used in this study is deﬁned in Section2.
Section 3 introduces the hierarchical Bayesian perceptual model
in multivariate exponential distribution environment. Section4
derives a set of closed form update equations for perceptual
inference. Simulations results are given in Section6. Finally, the
articleisconcludedafterdiscussions.
2 Notations
Throughout this article, we use the following conventional
mathematicalnotations:
• Aboldcapitalletterisamatrixwhileaboldlowercaseletteris
avector.
• A hollow capital letter denotes a set, which is also denoted
by{}.
• Aprobabilitydensityfunction(PDF)isdenotedby q(·)or p(·).
• A multivariate Gaussian PDF ofx is denoted byN (x;μ,/Sigma1)
with meanμ and variance/Sigma1, while a multivariate Gaussian
randomvectorisdenotedby x ∼ N (μ,/Sigma1).
• An multivariate exponential PDF of x can be denoted
by E(x;r) with a rate parameter r, while an multivariate
exponentialrandomvectorisdenotedby x ∼ E(r).
• A sequence of variables over time are denoted by “:,"
forexample,
o1: K = o(t1),o(t2),··· ,o(tK ).
• Eq(x)(v)meanstheexpectationof v underthedistribution q(x).
• The operator ⊙ is the Hadamard product, the operation
diag(v)istotransformavector v intoadiagonalsquarematrix
withtheelementsof v ontheprincipaldiagonal.
• The functionvec(Mm×n) is the vectorization of a matrixM,a
linear operation, to obtain a column vector of lengthm × n
by concatenating the columns of the matrixM consecutively
fromcolumn1tocolumn n.Theoperator ⊗istheKronecker
product.
• Thefunction lvec(L)istotransformalowertriangularmatrix
L into a column vectorlvec(L) obtained by stacking columns
withoutzeroelementsintheuppertrianglepartofthematrix.
3 Hierarchical Bayesian perceptual
model
3.1 Parameterization of multivariate
exponential distribution
Given a random multivariate exponential variablex0 without
crossdimensioninteractionsamongcomponents,wecaneasilyget
the joint probability of all components by directly multiplying all
marginalexponentialdistributions:
E(x0;r0) =
d0∏
i=1
r(i)
0 exp(−r(i)
0 x(i)
0 ) = exp(−rT
0x0)
d0∏
i=1
r(i)
0 , (1)
wherex(i)
0 isthe i-thcomponent(i.e.,randomexponentialvariable)
of x0. The rate parameterr(i)
0 is the expectation of thei-th random
exponential variablex(i)
0 . r0 is the expected rate vector of random
vector x0. The integer d0 is the number of dimensions of the
random vectorx0. However, this independent model is incapable
of capturing the pairwise probabilistic correlation among the
components ofx0. If we introduce non-independent exponential
model with interactions among the components of x0, it will
lead to high model complexity. Since the rate parameterr0 is of
primary interest, we aim to learn the rate parameter by explicitly
consideringthepairwiseinteractionsamongthecomponentsof r0.
To keep the positive constraint of the rate parameter, we convert
the constrained learning problem into an unconstrained learning
in logarithmic space. More speciﬁcally, the rater0 is mapped from
apoint x1 initslog-space
r0(t) = exp(W1x1(t)+b1), (2)
where the notationexp(·) denotes the element-wise exponential
function. The coeﬃcient matrix W1 is a diagonal matrix with
positiveelementsontheprincipaldiagonal.Thismatrixrepresents
the coupling strength between x0 and x1. The bias b1 is a
shiftparameter.
3.2 Perceiving tendency and volatility of
the rate parameter
Volatile signals ﬂuctuate over time, showing variations. The
ﬂuctuations of the signals are again subject to changes, and so
forth. The nested nature of volatility is a hallmark of collective
phenomena as observed in many complex systems like brain
network, animal swarm and ﬁnancial market. To quantitatively
describe volatility and pairwise correlations of multi-dimensional
signals, general hierarchical volatility model could be constructed
Frontiersin ComputationalNeuroscience 03 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
FIGURE 1
Overview of the hierarchical perceptual model.
based on nested Brownian motions ( Zhu et al., 2025). The
basic idea is that the variable of interest is represented by a
Brownian motion,while thechangesof thevariable ispredictedby
higher order variables that are again subject to Brownian motions.
Following this framework, we develop a hierarchical perceptual
model to estimate the tendency and volatility of multivariate
exponentially distributed signals (Figure1). More speciﬁcally, the
logarithms of rate parametersx
1 of the underlying multivariate
exponential distribution is modeled by a general Brownian motion
withdiﬀusionmatrix /Sigma1
1 ∈ Rd1×d1
x1 = B(t;/Sigma11). (3)
This Brownian motion captures the tendency of the learned
parameter vectorx1. The volatility (i.e., uncertainties and pairwise
correlations) inx1 is given by/Sigma11 ∈ Rd1×d1, which isa symmetric
positive deﬁnite matrix by deﬁnition. Considering the fact that the
diﬀusionmatrix /Sigma1
1 isasymmetricpositivedeﬁnitematrix,itcould
be uniquely represented by a lower triangular matrixL1 ∈ Rd1×d1
according to Cholesky decomposition (Tanabe and Sagae, 1992;
JungandO’Leary,2006):
/Sigma11 = L1L1T .
To further evaluate the volatility/Sigma11 in x1, we assume that
its decomposition L1 is modeled by a general Brownian motion
in its parameterized space. To be exact, the elements of L1 is
parametrizedbya d2 = d1(d1 +1)/2dimensionalvector y2,which
results from concatenating the lower triangle elements ofL1 in a
column-wise fashion. The element ini-th row andj-th column of
L1 isparameterizedby
L1(i,j) = l(i,j)
1 =
⎧
⎨
⎩
2sinh(y
((2d1−j+2)(j−1)
2 +i−j+1)
2 ), 1 ≤ j < i ≤ d1
exp(y
((2d1−i+2)(i−1)
2 +1)
2 ), j = i
(4)
where sinh(·) denotes the hyperbolic sine function. Note that
Equation4 transformsL1 into logarithmic space, while conserving
non-negativity for diagonal elements and allowing arbitrary values
foroﬀ-diagonalelopementsof L1.
The vector y2 represents the volatility of the signal in
logarithmic space, therefore constitutes a parameterization of the
volatility. y
2 is given by the following mapping in the second level
ofthemodel:
y2 = W2x2 +b2, (5)
where b2 and x2 ∈ Rd2 represent the trend and time-varying
ﬂuctuation in log-volatility of x1, respectively. The coeﬃcient
matrixW2 isa d2-by-d2 diagonalmatrixrepresentingthecoupling
strength from level two to level one.W2 can simply take the form
of a diagonal matrix spanned from a column vectorw2 with all
positiveelements
W2(i,i) = w(i)
2 .
Wecanrewritethecoupling( Equations4, 5)as
L1 = F2(x2;w2,b2).
In the second level of the model, we further assume thatx2
evolves as a general Brownian motion with diﬀusion matrix/Sigma12 ∈
Rd2×d2
x2 = B(t;/Sigma12). (6)
The diﬀusion matrix /Sigma12 is chosen as a diagonal matrix for
simplicity.Let L2 ∈ Rd2×d2 betheuniqueCholeskydecomposition
of /Sigma12. We simply assume thatL2 is a constant diagonal matrix
spannedbyvector λ ∈ Rd2 withallelementsbeingpositive.
Figure1 shows an overview of the hierarchical perceptual
model. With this model, a Bayesian agent receives a series of
sensory inputs or observationso
1: T . At timetk, the sensory input
ok totheagentisdeterminedbyadeltadistribution δ(·)
P(ok | x0,k) = δ(ok = x0,k). (7)
The initial priori states p(x1,0,x2,0) are Gaussian distributions
asfollows:
q(xh,0) =N (xh,0;μh,0,Ch,0),h = 1,2. (8)
In summary, the hierarchical perceptual model constitutes a
generative model for sensory observationso(t) based on hidden
Frontiersin ComputationalNeuroscience 04 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
representations of the tendency (x1) and the volatility (x2)o f
the observations. . To simplify the notations, we introduced the
notation X to denote the set of all hidden states, P for the
hyperparametersandthepriorstatesofthemodel:
X ={x
0,x1,x2},
P =
{
w1,b1,w2,b2,λ,μ1,0,C1,0,μ2,0,C2,0
}
where μ1,0,C1,0,μ2,0, andC2,0 are the prior states of the model
deﬁnedin Equation8 andSupplementarymaterialSection2 .
4 Perceptual inference approximated
by variational approximation
The aforementioned hierarchical perceptual model is
constructed based on general continuous Brownian motions.
It remains to derive update rules to estimate the posterior
distributions for the hidden representationsx
1 and x2. In order to
deriveafamilyofanalyticalandeﬃcientupdaterules,wediscretize
continuous Brownian motions by applying the Eulerian method.
The sampling interval (SI)ϵ
k = tk − tk−1 is deﬁned by the time
that elapses between the arrival of consecutive sensory inputsok−1
andok.
We use the variational Bayesian method (Beal, 2003; Friston,
2010; Daunizeau et al., 2010b; Mathys et al., 2011)t or e a c ha n
approximation to the posterior distributions of x1(t) and x2(t)
given the sensory inputo(t) (i.e., observation). To this end, we
maximizethenegativefreeenergy,whichisthelowerboundoflog-
model evidence, to yield variational approximation posterior (cf.
SupplementarymaterialSection1 ):
q(x
h,k) = 1
Zh
exp(Vh(xh,k)),h = 1,2, (9)
where Zh is a normalization constant.Vh(xh,k) is the variational
energygivenby
Vh(xh,k) = Eq(X\h,k)
[
lnp(Xk,ok | P,ϵk)
]
. (10)
Here we introduced the notationX\h,k for excluding xh,k from
the setXk, Then under Brownian and Gaussian assumptions, the
approximationvariationalposterior( Zhuetal.,2025 )is
xh,k | ok,P ∼ N (μh,k,Ch,k),
h = 1,2.
(11)
Under this approximation, the inference of the posterior
distributions ofxh is reduced to the estimation of the meanμh,k
andthecovariancematrix Ch,k,orequivalentlytheprecisionmatrix
Ph,k ≡ (Ch,k)−1. Following (Zhu et al., 2025), the update rules for
theposteriordistributionsof x1 andx2 arederived.
At the bottom (zeroth) level of the hierarchical perceptual
model, we can directly determine multivariate exponential
distributionq(x
0,k)withtheexpectation:
μ0,k = ok. (12)
Attheﬁrstlevel,following Equation10,V1(x1)iscalculatedas
V1(x1,k) = Eq(X\2,k)[lnp(Xk,ok | P,ϵk)]
=lnp(ok | x0,k)+Eq(x0,k)[lnp(x0,k | x1,k)]
+Eq(x2,k)[lnp(x1,k | x2,k,W2,b2,ϵk)]
≈1T (W1x1,k +b1)−μT
0,kexp
(
W1x1,k +b1
)
− 1
2(x1,k −μ1,k−1))T (ϵk ˆ/Sigma11,k +C1,k−1)−1(x1,k −μ1,k−1)
+const (13)
where 1 is ad0 dimensional column vector in which all elements
are1.Hereweusetheapproximation
(
ϵk/Sigma11,k +C1,k−1
)−1 ≈
(
ϵk ˆ/Sigma11,k +C1,k−1
)−1
, (14)
with ˆ/Sigma11,k computedfromthesecondlevel
ˆ/Sigma11,k = ˆL1,k ˆL
T
1,k,
ˆL1,k = F2(μ2,k−1;w2,b2).
(15)
The variational energy V1(x1,k) is not a standard Gaussian
quadratic form, so we have to employ a Gaussian quadratic form
to approximate it (Zhu et al., 2025). To obtain this approximation
form, we give the gradient and Hessian matrix of V
1(x1,k)
asfollows:
∇V1(x1,k) =WT
1
[
1 −μ0,k ⊙ exp
(
W1x1,k +b1
)]
−(ϵk ˆ/Sigma11,k +C1,k−1)−1(x1,k −μ1,k−1), (16)
and
∇2V1(x1,k) =− WT
1 diag
(
μ0,k ⊙ exp
(
W1x1,k +b1
))
W1
−(ϵk ˆ/Sigma11,k +C1,k−1)−1, (17)
UndertheGaussianquadraticformapproximation,whichisbased
on a single step Newton method (Zhu et al., 2025), the tendency of
x0,k iscapturedby
μ1,k = μ1,k−1 +C1,kWT
1 PE0,k, (18)
wherePE0,k isthepredictionerror:
PE0,k = 1 −μ0,k ⊙ ˆr0,k. (19)
ˆr0,k ≡ [ˆr(1)
0,k,ˆr(2)
0,k,··· ,ˆr(d0)
0,k ]T isthepredictiongivenbythemapping
inEquation2:
ˆr0,k = exp
(
W1μ1,k−1 +b1
)
. (20)
Unpacking prediction error PE0,k results in a
meaningfulformula,
PE(i)
0,k = 1−μ(i)
0,kˆr(i)
0,k = 1−
μ(i)
0,k
1
ˆr(i)
0,k
.
Frontiersin ComputationalNeuroscience 05 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
The inverse of the predicted rate 1
ˆr(i)
0,k
gives the expectation of
sensory input, and the ratio
μ(i)
0,k
1
ˆr(i)
0,k
measures the accuracy of the
prediction. If the ratio is greater than 1 (i.e., the predicted
expectation of sensory input is less than the actual sensory input),
the prediction error is negative, and the agent should decrease
μ(i)
1 . If the ratio is less than 1, the prediction error is positive,
the agent should increaseμ(i)
1 , so that the predicted expectation
of sensory input could be decreased. Ideally, the ratio is equal to
1,andthepredictionerrorvanishes,whichmeansthatthepredicted
expectationofthesensoryinputisequaltotheactualsensoryinput.
InEquation18,thepredictionerrorisscaledandrotatedbythe
covariance matrixC1,k of the approximate Gaussian distribution,
whichisconvertedfromtheprecisionmatrix:
C1,k ≡
(
P1,k
)−1 ,
P1,k = ˆ/Pi11,k +WT
1 diag
(
μ0,k ⊙ ˆr0,k
)
W1.
(21)
Herepredictionprecision ˆ/Pi11,k isgivenby
ˆ/Pi11,k = (ϵk ˆ/Sigma11,k +C1,k−1)−1. (22)
Note that the oﬀ-diagonal elements of the inverse prediction
precisionmatrix ˆ/Pi11,k givethepredictioncorrelations.
Atthesecondlevel,thevolatility,consistingoftheuncertainties
and pairwise correlations in natural parameters, is inferred by
similar variational approximation method (Zhu et al., 2025). The
meanisupdatedby
μ2,k = μ2,k−1 +ϵkC2,kWT
2 ˆLg1,k
(
ˆ/Omega11,k ⊗Id1
)
vec
(
/Delta1T
1,k
)
. (23)
Here/Delta11,k isgivenby
/Delta11,k =
[
C1,k +PE1,kPET
1,k
]
ˆ/Pi11,k −Id1. (24)
The constant matrixId1 is ad1-by-d1 unit square matrix.PE1,k is
thepredictionerroronthehiddenstate x1
PE1,k = μ1,k −μ1,k−1. (25)
ˆLg1,k isgivenby
ˆLg1,k =
⎡
⎢⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢⎣
exp
(
(W
(1)
2 )T μ2,k−1 +b(1)
2
)
eT
2 (1)
2cosh
(
(W(2)
2 )T μ2,k−1 +b(2)
2
)
eT
2(2)
exp
(
(W(3)
2 )T μ2,k−1 +b(3)
2
)
eT
2 (3)
2cosh
(
(W(4)
2 )T μ2,k−1 +b(4)
2
)
eT
2(4)
...
exp
(
(W(d2)
2 )T μ2,k−1 +b(d2)
2
)
eT
2(d2)
⎤
⎥⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥⎦
, (26)
where the constant vectore
2(d2)i sad 2
1-dimension column vector.
Thej-th component ineT
2 (d2)i s1i fj = i o r0i fj̸=i. The column
vector W(i)
2 is the i-th row in the coeﬃcient matrixW2. ˆ/Omega11,k is
deﬁnedas
ˆ/Omega11,k = ˆL
T
1,k ˆ/Pi11,k. (27)
FIGURE 2
Overview of the ablation model.
Theprecisionmatrixisupdatedby
P2,k = ˆ/Pi12,k +WT
2 ˆLg1,k { ϵ2
k Kd1d1
[
ˆ/Omega1
T
1,k ⊗[ˆ/Omega11,k/Delta11,k]+[/Delta1T
1,k ˆ/Omega1
T
1,k]⊗ ˆ/Omega11,k + ˆ/Omega1
T
1,k
⊗ ˆ/Omega11,k
]
+ϵ2
k
[
[ˆL
T
1,k/Delta1T
1,k ˆ/Omega1
T
1,k]⊗ ˆ/Pi11,k +[ˆL
T
1,k ˆ/Omega1
T
1,k]⊗[ ˆ/Pi11,k/Delta11,k]
+ [ˆL
T
1,k ˆ/Omega1
T
1,k]⊗ ˆ/Pi11,k
]
−ϵk
[
Id1 ⊗[ ˆ/Pi11,k/Delta11,k]
]
}
ˆL
T
g1,kW2 −WT
2 diag
(
lvec
(
δ1,k
))
W2,
(28)
where
δ1,k = ϵk[/Delta1T
1,k ˆ/Omega1
T
1,k]⊙ ˆL1,k
Theprecisionmatrixoftheprediction ˆ/Pi12 isgivenby
ˆ/Pi12,k = (ϵk/Sigma12 +C2,k−1)−1. (29)
The notation Kmn denotes a mn-by-mn commutation
matrix(MagnusandNeudecker,1979).
5 Variational Bayesian learning
A modelM with a set of parametersP receives and encodes
sensory inputo(t). We can arrange all elements ofP into a vector
ξ.Here,weintroducethefollowingmeanﬁeldapproximationtoﬁt
theparametersofthemodelwiththesensoryinputs o1: K
q(P) =q(ξ) = q(w1)q(b1)q(w2)q(b2)q(λ)
·q(μ1,0)q(C1,0)q(μ2,0)q(C2,0).
(30)
Frontiersin ComputationalNeuroscience 06 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
A
B
FIGURE 3
Time-varying rate parameter and sensory inputs of volatile multivariate exponentially distributed signals. Panels(A, B)represent two dimensions of
input signal. In each panel, blue dots are the sensory inputso(i) of thei-th dimension of the signal. Red lines represent the expected rater(i)
0 (t). The
black dashed lines are the expectation of the sensory inputo(i), i.e., the inverse of the expected rater(i)
0 (t). Note that the expected rate in the two
dimensions ﬂuctuates in time, synchronously before and anti-synchronously after trial 200.
Then
lnp(o1: K |M) = ln
∫
p(o1: K ,ξ|M)dξ
= ln
∫ p(o1: K ,ξ|M)q(ξ)
q(ξ) dξ
≥
∫
q(ξ)ln( p(o1: K ,ξ|M)
q(ξ) )dξ
=
∫
q(ξ)ln p(o1: K ,ξ|M)−q(ξ)ln q(ξ)dξ
≜ FM(ξ)
. (31)
WeusetheLagrangemultipliermethodtoworkouttheoptimal
variationalposteriorasfollows:
q(ξ) = 1
Zξ
exp(V(ξ))
V(ξ) = lnp(o1: K ,ξ|M).
(32)
Then we execute a Laplacian approximation to determine a
Gaussian approximation of the variational posterior solution
(Equation33)
μξ = argmax
ξ
V(ξ) = argmax
ξ
lnp(o1: K ,ξ|M)
= argmax
ξ
lnp(o1: K |ξ,M)p(ξ)
= argmax
ξ
K∑
k=1
lnp(ok|ξ,M)+lnp(ξ)
= argmax
ξ
K∑
k=1
lnp(ok|ˆr0,k,ξ,M)+lnp(ξ),
Cξ =− ∂2V(μξ )
∂ξ∂ξT ,
(33)
where lnp(ok|ξ,M) is the logarithm of the predictive distribution
ok ∼ E
(
ˆr0,k
)
andisgivenby
lnp(ok|ˆr0,k,ξ,M) = 1T lnˆr0,k −oT
k ˆr0,k. (34)
Finally, the maximum valueFM(μξ ,Cξ ) of the negative free
energyFM(ξ)isgivenby
FM(ξ) ≤ FM(μξ ,Cξ ) = V(μξ )+ dξ
2 ln2π e + 1
2 lndet(C ξ ).
(35)
6 Simulation study
Toverifytheeﬀectivenessoftheproposedmodel,weconducted
simulations on synthetic data to assess the model’s ability to
capture time-varying rate parameters of multivariate exponential
distribution. The purpose of using simulation is to validate the
model on precisely deﬁned data, so that the results given by the
modelcouldbecomparedwithgroundtruth.
6.1 An ablation model
To assess the ability of our hierarchical Bayesian modelM,
we deﬁne an ablation modelMa as a baseline model to evaluate
the role of the top (volatility) level of the hierarchical Bayesian
modelM.Putsimply,anablationmodel Ma isthesimpleversion
of the hierarchical Bayesian modelM with a constant volatility
x2(t) = μ2. In this case, we can remove the variablex2,k and keep
a constant likelihood matrix/Sigma11. The modelMa can be deﬁned
by Equations1–3. Figure2 shows the overall framework of the
ablationmodel Ma.
Frontiersin ComputationalNeuroscience 07 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
TABLE 1 Parameters of the hierarchical Bayesian model.
Name Description Initial value Fixed or free
Parameters of the hierarchical Bayesian model
do Dimensionof o 2 Constant
d1 Dimensionof x1 2 Constant
d2 Dimensionof x2 3 Constant
ϵk Samplinginterval ϵk 1 Constant
αλ Upperboundon λ 0.04·1 constant
λ Volatilityof x2 Fixed
μλG Meanof λG 0
CλG Covarianceof λG Id2
αw2 Upperboundon w2 1 constant
w2 Couplingstrength Fixed
μwG
2
Meanof wG
2 0
CwG
2
Covarianceof wG
2 Id2
b2 Couplingbias 0 Fixed
μb2 Meanof b2 0
Cb2 Covarianceof b2 O3
μ2,0 Priormeanof x2 Free
μμ2,0 Meanof μ2,0 [0,0,0] T
Cμ2,0 Covarianceof μ2,0 1×10−1 ·I3
C2,0 Priorcovarianceof x2 Fixed
μcG
2
Meanof cG
2 [0,0,0] T
CcG
2
Covarianceof cG
2 Id2
μ1,0 Priormeanof x1 Free
μμ1,0 Meanof μ1,0 [25,25]T
Cμ1,0 Covarianceof μ1,0 Id1
C1,0 Priorcovarianceof x1 Free
μcG
1
Meanof cG
1 0
CcG
1
Covarianceof cG
1 Id1
w1 Couplingstrength Fixed
μwG
1
Meanof wG
1 [ln(0.06),ln(0.06)]T
CwG
2
Covarianceof wG
1 Od1
b1 Couplingbias 0 Fixed
μb1 Meanof b1 0
Cb1 Covarianceof b1 Od1
AllparametersoftheproposedhierarchicalBayesianmodelarelistedinthetable.Parameters
labeled by “Free" are optimized by the inversion of the model. Fixed parameters are constant
andwerenotoptimized.Thenotation 1 isaconstantcolumnvectorinwhichallcomponents
are 1.0 is a zero vector. The bold letterOd represents ad by d constant matrix in which all
elements are 0. Given all the initial priors, we can search the optimal priors on all optimized
parametersμξ accordingtofreeenergyprinciple( Equations31,33).
The update equations for the ablation model are similar to
Equations12, 18–22 with ˆ/Sigma11 = /Sigma11. Put simply, we assume that
/Sigma11 isadiagonalmatrixwithpositivediagonalelements.Therefore,
/Sigma11 can be determined by a vectorσ1 with positive elements. The
TABLE 2 Parameters of the ablation model.
Name Description Initial value Fixed or free
Parameters of the ablation model
do Dimensionof o 2 Constant
d1 Dimensionof x1 2 Constant
ϵk Samplinginterval ϵk 1 Constant
σ1 Volatilityof x1 Fixed
μσG
1
Meanof σG
1 [ln0.01,ln0.01] T
CσG
1
Covarianceof σG
1 Id1
μ1,0 Priormeanof x1 Free
μμ1,0 Meanof μ1,0 [25,25]T
Cμ1,0 Covarianceof μ1,0 Id1
C1,0 Priorcovarianceof x1 Free
μcG
1
Meanof cG
1 [ln0.25,ln0.25] T
CcG
1
Covarianceof cG
1 Id1
w1 Couplingstrength Fixed
μwG
1
Meanof wG
1 [ln(0.06),ln(0.06)]T
CwG
2
Covarianceof wG
1 Od1
b1 Couplingbias 0 Fixed
μb1 Meanof b1 0
Cb1 Covarianceof b1 Od1
All parameters of the ablation model are listed in the table. Parameters labeled by “Free"
are optimized by the inversion of the model. Fixed parameters are constant and were not
optimized. The notation1 is a constant column vector in which all components are 1.0 is a
zero vector. The bold letterOd represents ad by d constant matrix in which all elements are
0.Givenalltheinitialpriors,wecansearchtheoptimalpriorsonalloptimizedparameters μξ
accordingtofreeenergyprinciple( Equations31,33).
priordistributionof /Sigma11 isdeﬁnedby
q(/Sigma11) = q(lnσ1) = N (lnσ1;μlnσ1,Clnσ1) (36)
where μlnσ1,Clnσ1 are the parameters of the prior
distribution. Other parameters of this model are the same
prior model with the above hierarchical Bayesian model (cf.
SupplementarymaterialSection2 ).
6.2 Simulation setup
In detail, simulations were carried out in four steps
asfollows:
1. Generatingsyntheticsensoryinputs.Werandomlygenerated
a sequence of bivariate exponential variable o1: K =
o(t1),o(t2),o(t3),··· ,o(tK )(K = 400)(Figure3):
p(o(t)) = E(o(t),r0(t)), (37)
Frontiersin ComputationalNeuroscience 08 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
FIGURE 4
Temporal dynamics of the tendencyμ1 of the log-rate vectorx1(t) at the ﬁrst level. Panels(A,B) represent two dimensions of the expectationμ1.I n
details, each panel shows one component ofμ1 in red, andPE1 in blue. The light-red shaded area represents the uncertainty of each component
(i.e., μ(i)
1 (t)±
√
C(i,i)
1 (t),i ∈ {1,2}). The red markers△ ,◦ represent the priors on the standard deviation and the mean of each component respectively.
where the time-varying rate vector r0(t) was governed by
cosinewavesandwasdeﬁnedby
r(1)
0 (tk) = 2.5+2cos( 7π
K tk),
r(2)
0 (tk) =
{
2.5+2cos( 7π
K tk) k ≤ 200
2.5−2cos( 7π
K tk) k ≥ 201
.
2. Initializing the suﬃcient statistics of all random parameters.
We must choose particular initial suﬃcient statistics of a
parameter vector ξ (Table1 for the hierarchical Bayesian
model and Table2 for the ablation model) to make the
models work well on a sequence of sensory inputs. Then
we determined the prior distribution of ξ. All parameter
conﬁgurations for the two models (Figures1, 2) are shown in
Tables1,2.
3. Maximizing negative free energy. We employed optimization
methods to obtain the optimal suﬃcient statistics
(
μ
ξ ,Cξ
)
of the prior parameter ξ. The quasi-Newton Broyden-
Fletcher-Goldfarb-Shanno method based on a line search
framework (Nocedal and Wright, 2006) was adopted to
maximize negative free energy (Equations31, 33, 34)( Beal,
2003;Friston,2010).
4. Generating the optimal trajectories of all states. We use the
optimalpriorparameters μ
ξ tocharacterizeaparticularmodel
(Figures1, 2).Thetwomodelsarecomparedoninferenceand
decision-makingtasks.
6.3 Perceiving volatile multivariate
exponentially distributed signals
The proposed hierarchical Bayesian inference model endowed
with the optimal parameterμξ constitutes a hierarchical Bayesian
agent.WeaskedthehierarchicalBayesianagenttoperceivevolatile
multivariateexponentiallydistributedsignalsasshownin Figure3.
The dynamic tendency μ1(t) of the log-rate vector x1(t)i s
tracked online by the hierarchical Bayesian agent (Figure4). μ1
follows the varying trend of the expected rate in logarithmic
space. The uncertainty ofμ1(t) is stable (light-red shaded area in
Figure4). The prediction error PE1 ﬂuctuates around a baseline
(bluelinein Figure4).
Overall, the agent perceives the expected rate vector well
(Figure5). For a majority of the trials, both of the belief
expectations μ(1)
0 ,μ(2)
0
(solid lines in Figures5A, C) ﬂuctuates
around the expected rate (dashed lines inFigures5A, C). In the
initial stage, the agent quickly adjusts itself to adapt to the input
signal and tracks the expected states. Due to the stochasticity, the
sample rate intensity in sensory inputs deviates from the expected
rate intensity, leading to the estimated belief rate intensityμ(i)
0 ,i =
0,1 to deviate from the expected rate intensity. From trial 120 to
trial 165, the sample rate intensity in sensory inputso(1) is larger
than the expected rate intensity inFigure3A. The agent’s belief is
higher than the expected rate (Figure5A). From trial 116 to trial
158 (trial 296 to trial 308), the sample rate intensity in sensory
inputs o(2) is greater than the expected rate intensity inFigure3B,
leadingtheagenttohavehigherbeliefoftherateintensitythanthe
expectedratevalue.
Frontiersin ComputationalNeuroscience 09 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
AC
BD
FIGURE 5
Temporal dynamics of the expectation of the logarithm of volatilityμ2 in the statex1 at the second level. Panels(A–C) represent three dimensions of
the expectationμ2. Each panel shows the evolution of one element ofμ2 in red and the corresponding element ofPE2 in blue. Light-red shaded area
represents the uncertainty of each dimension (i.e.,μ(i)
2 (t)±
√
C(i,i)
2 (t),i ∈ {1,2,3 }). The red markers△ ,◦ represent the priors of the standard deviation
and mean of each dimension.
FIGURE 6
Temporal dynamics of the expectation of the logarithm of volatilityμ2 in the statex1 at the second level. Each panel shows the evolution of one
element ofμ2 in red and the corresponding element ofPE2 in blue. Light-red shaded area represents the uncertainty of each dimension (i.e.,
μ(i)
2 (t)±
√
C(i,i)
2 (t),i ∈ {1,2,3 }). The red markers△ ,◦ represent the priors of the standard deviation and mean of each dimension.
The expectations of log-volatilities in the logarithms of the
rate vector (μ(1)
2 and μ(2)
2
, i.e., internal representation of the
expected states) has notable changes, stabilized for most of the
time (Figure6). From trial 1 to trial 200, changes in rater(1)
0 are
consistent with changes in rater(2)
0 (Figure3). In theory, they are
positively correlated during this period. From trial 1 to trial 186,
thepredictioncorrelation ˆρ
1 continuestoincrease(Figure7).From
trial 187 to trial 200, asynchronous local ﬂuctuations (or noise)
Frontiersin ComputationalNeuroscience 10 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
FIGURE 7
Prediction correlationˆρ1(t) is extracted from the inverse prediction
precision ˆ/Pi11(t) generated by the second (log-volatility) level.
lead to a decrease in prediction correlationˆρ1. From trial 201 to
trial 400, changes in rater(1)
0 are the opposite with the changes in
rater(2)
0 (Figure3).Thetwodimensionsofthesignalarenegatively
correlatedduringthisperiod.Asaresult,thepredictioncorrelation
ˆρ1 of the agent continues to decrease from trial 201 to trial 359.
From trial 359 to trial 365, prediction errorsPE(1)
1 andPE(2)
1 are
positive numbers, and drive prediction correlationˆρ1 to jump to a
largervalue(Figure7).ThehierarchicalBayesianagentthereforeis
abletouncoverthecorrelationstructuresofthesignaldynamically.
6.4 Bayesian model selection
To compare the performance of the proposed hierarchical
Bayesianmodel M andtheablationmodel Ma,weperformed100
independent simulations for each model using diﬀerent seeds of
random number generators. Based on these simulations, Bayesian
factors were calculated. Figure8 shows the histogram of the
Bayesian factorsBF(
M,Ma). According to the criteria suggested
by Harold Jeﬀreys (cf. SupplementarymaterialSection4), M is
betterthan Ma.
7 Discussion
7.1 Contributions of this study
In this article, we developed a hierarchical Bayesian model to
infer and track online the tendency and volatility in multivariate
exponential signals. The bottom level of the hierarchical Bayesian
model is to learn the expected rate parameter vector of
the multivariate exponential signal. The logarithm of the rate
parameter vectorx
1 is modeled to evolve as a general Brownian
motion at the ﬁrst level. Under the Brownian and Gaussian
assumption on x1, the volatility in x1 can be computed by
the Cholesky decomposition of the diﬀusion matrix of the
Brownian motionx1. Therefore, we introduce a parameterization
of the volatility in x1 in logarithmic space after the Cholesky
decompositionofthediﬀusionmatrixof x1.Thevolatilityin x1 can
be represented byx2, which again evolves as a Brownian motion.
The low-order interactions among the components of the log-rate
parametervectoranduncertaintiesarecapturedby x
2 atthesecond
levelofthemodel.
FIGURE 8
Histogram of Bayesian factors. Bayesian factor with the Bayesian
information criterionBF(M,Ma).
The hierarchical Bayesian model assumes that the log-rate
parameter vectorx1(t) evolves as a general Brownian motion and
canbeupdatedby Equation18,wherepredictionerror PE0,k drives
the agent to diminish the diﬀerence between the agent’s belief and
the sensory input. The coeﬃcient matrixW1 plays the role of
scalingfactorstoweightpredictionerror PE0,k.Thecovariance C1,k
functionsascomplexadaptivelearningratein Equation21.
In principle, the proposed model could be easily generalized
to a Bayesian framework for decision making in high-dimensional
volatile environments by deﬁning appropriate form of response
models (Berger, 2013;Mathys et al., 2014; Zhu et al., 2022). In
this article, we deﬁne a simple random response model based on
bivariate exponential distribution. For other problems of interest,
itissuﬃcienttoconstructacompatibleresponsemodeladdressing
theparticularoptimizationcriteriaofthequestion.
7.2 Limitations and strengths
The peakless and memoryless properties of the exponential
distribution bring diﬃculties for an online agent to predict, since
historical sensory inputs can only provide weak evidence for a
prediction. The proposed hierarchical Bayesian agent internally
integrates historical sensory inputs and the current sensory input
to infer the changes in the signal. The agent estimates the dynamic
volatility in the sensory inputs and adjusts the learning rate based
on the evidence of the volatility, so that the information from the
signalisintegratedintotheinternalstateseﬃciently.Theproposed
hierarchical Bayesian agent is able to eﬃciently and accurately
capture the characteristics of volatile multivariate exponentially
distributedsignals.
In the simulation, we observed that the proposed hierarchical
Bayesian agent has good suppression eﬀect on small volatility, but
it is also swayed by the local variation of the rate intensity caused
by the stochasticity of the signal. The prediction correlation is not
Frontiersin ComputationalNeuroscience 11 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
only determined by changes in the trend of the sensory inputs
but is also aﬀected by volatility. Large local ﬂuctuations can also
cause jumps in prediction correlations. Asynchronous persistent
small local ﬂuctuations will also reduce the prediction correlation,
while synchronous persistent small ﬂuctuations will increase the
predictioncorrelation.
Inthisstudy,wesimplyconsideredsimulateddata,whichaims
tocapturedynamicandmultidimensionalaspectsofnonstationary
multivariate exponential signals and cannot cover other important
features observed in real data set. The results obtained from
simulationspavewaysforfurtherinvestigationsofmanyestimation
problemsinneuroscienceresearch.Thepossibleapplicationsofthe
method include ﬁring rate estimation, functional brain connection
estimation,etc.
8 Conclusions
We have introduced the mathematical basis of a hierarchical
Bayesian model for inferring and tracking rate intensity parameter
of multivariate exponential signals and illustrated its functionality.
A family of interpretable closed form update rules were derived.
In particular, we provided a full theoretical scenario that consists
of inference in the perceptual model and learning optimal hyper-
parameters by inversion of the hierarchical Bayesian model. The
proposed theoretical framework was validated on synthetic data,
and it turned out that the hierarchical Bayesian model worked
well in tracking volatile multi-variate exponential signals. The
preliminary study here points to the practical utility of our
approach in analyzing high-dimensional neural activities, which
oftenfollowasdistributionsinexponentialfamily.
Data availability statement
The original contributions presented in the study are included
in the article/Supplementarymaterial, further inquiries can be
directedtothecorrespondingauthor.
Author contributions
CZ: Conceptualization, Investigation, Software, Writing –
original draft, Writing – review & editing, Methodology. KZ:
Writing–originaldraft,Writing–review&editing.FT:Writing–
original draft, Writing – review & editing. YT: Writing – original
draft, Writing – review & editing. XL: Methodology, Writing –
original draft, Writing – review & editing. BS: Conceptualization,
Funding acquisition, Writing – original draft, Writing – review &
editing.
Funding
Theauthor(s)declarethatﬁnancialsupportwasreceivedforthe
researchand/orpublicationofthisarticle.Thisworkwassupported
bySTI2030-MajorProjects2022ZD0205005.
Conﬂict of interest
The authors declare that the research was conducted in the
absence of any commercial or ﬁnancial relationships that could be
construedasapotentialconﬂictofinterest.
The author(s) declared that they were an editorial board
memberofFrontiers,atthetimeofsubmission.Thishadnoimpact
onthepeerreviewprocessandtheﬁnaldecision.
Generative AI statement
The author(s) declare that no Gen AI was used in the creation
ofthismanuscript.
Any alternative text (alt text) provided alongside ﬁgures in
this article has been generated by Frontiers with the support of
artiﬁcial intelligence and reasonable eﬀorts have been made to
ensureaccuracy,includingreviewbytheauthorswhereverpossible.
Ifyouidentifyanyissues,pleasecontactus.
Publisher’s note
All claims expressed in this article are solely those of the
authors and do not necessarily represent those of their aﬃliated
organizations, or those of the publisher, the editors and the
reviewers. Any product that may be evaluated in this article, or
claim that may be made by its manufacturer, is not guaranteed or
endorsedbythepublisher.
Supplementary material
The Supplementary Material for this article can be found
online at: https://www.frontiersin.org/articles/10.3389/fncom.
2025.1408836/full#supplementary-material
References
Beal,M.J.(2003). Variational Algorithms for Approximate Bayesian Inference[PhD
thesis].UniversityCollegeLondon(UCL),London.
Berger, J. O. (2013). Statistical Decision Theory and Bayesian Analysis.C h a m :
SpringerScience&BusinessMedia.
Conrad, K. (2004). Probability distributions and maximum entropy.Entropy 6:10.
Availableonlineat: https://kconrad.math.uconn.edu/blurbs/analysis/entropypost.pdf
Czirók, A., Schlett, K., Madarász, E., and Vicsek, T. (1998). Exponential
distribution of locomotion activity in cell cultures.P h y s .R e v .L e t t. 81, 3038–3041.
doi:10.1103/PhysRevLett.81.3038
Daunizeau, J., Den Ouden, H. E., Pessiglione, M., Kiebel, S. J., Friston, K. J., and
Stephan, K. E. (2010a). Observing the observer (II): deciding when to decide.PLoS
ONE 5:e15555.doi:10.1371/journal.pone.0015555
Frontiersin ComputationalNeuroscience 12 frontiersin.org
Zhu et al. 10.3389/fncom.2025.1408836
Daunizeau,J.,denOuden,H.E.M.,Pessiglione,M.,Kiebel,S.J.,Stephan,K.E.,and
Friston, K. J. (2010b). Observing the observer (I): meta-Bayesian Models of learning
anddecision-making. PLoS ONE5:e15554.doi:10.1371/journal.pone.0015554
Davis,D.J.(1952).Ananalysisofsomefailuredata. J. Am. Stat. Assoc.47,113–150.
doi:10.1080/01621459.1952.10501160
Dr˘agulescu, A., and Yakovenko, V. M. (2001). Evidence for the exponential
distribution of income in the USA.Eur. Phys. J. B-Condens. Matter Complex Syst. 20,
585–589.doi:10.1007/PL00011112
Epstein, B., and Sobel, M. (1953). Life testing.J. Am. Stat. Assoc. 48, 486–502.
doi:10.1080/01621459.1953.10483488
Esary,J.D.,andMarshall,A.W.(1974).Multivariatedistributionswithexponential
minimums.Ann. Stat.2,84–98.doi:10.1214/aos/1176342615
Friston, K. (2010). The free-energy principle: a uniﬁed brain theory?Nat. Rev.
Neurosci.11,127–138.doi:10.1038/nrn2787
Friston, K. FitzGerald, T., Rigoli, F., Schwartenbeck, P., and Pezzulo,
G. (2017). Active inference: a process theory. Neural Comput . 29, 1–49.
doi:10.1162/NECO_a_00912
Haynes, J.-D., and Rees, G. (2006). Decoding mental states from brain activity in
humans.Nat. Rev. Neurosci.7,523–534.doi:10.1038/nrn1931
Ibe, O. (2014). Fundamentals of Applied Probability and Random Processes .
Cambridge,MA:AcademicPress.doi:10.1016/B978-0-12-800852-2.00012-2
Jaynes,E.T.(1982).Ontherationaleofmaximum-entropymethods. Proc. IEEE70,
939–952.doi:10.1109/PROC.1982.12425
Johnson, R. A., Wichern, D. W., et al. (2002). Applied Multivariate Statistical
Analysis.UpperSaddleRiver,NJ:PrenticeHall.
Jung, J. H., and O’Leary, D. P. (2006). Cholesky Decomposition and Linear
Programming on a GPU.ScholarlyPaper.CollegePark,MD:UniversityofMaryland.
Kingman, J. F. C. (1992).Poisson Processes, Volume 3. Oxford: Clarendon Press.
doi:10.1093/oso/9780198536932.001.0001
Kundu,D.,andGupta,R.D.(2009).Bivariategeneralizedexponentialdistribution.
J. Multivar. Anal.100,581–593.doi:10.1016/j.jmva.2008.06.012
Li, S., and Le, W. (2017). Milestones of parkinson’s disease research: 200 years of
historyandbeyond. Neurosci. Bull.33,598–602.doi:10.1007/s12264-017-0178-2
Li, Z., O’Doherty, J. E., Hanson, T. L., Lebedev, M. A., Henriquez, C. S., Nicolelis,
M. A., et al. (2009). Unscented kalman ﬁlter for brain-machine interfaces.PLoS ONE
4:e6243.doi:10.1371/journal.pone.0006243
L o ,C . - C . ,C h o u ,T . ,P e n z e l ,T . ,Sc a m m e l l ,T .E . ,S t r e c k e r ,R .E . ,S t a n l e y ,H .E . ,e ta l .
(2004).Commonscale-invariantpatternsofsleep-waketransitionsacrossmammalian
species.Proc. Nat. Acad. Sci.101,17545–17548.doi:10.1073/pnas.0408242101
Magnus,J.R.,andNeudecker,H.(1979).Thecommutationmatrix:someproperties
andapplications. Ann. Stat.7,381–394.doi:10.1214/aos/1176344621
Maimon, G., and Assad, J. A. (2009). Beyond poisson: Increased
spike-time regularity across primate parietal cortex. Neuron 62, 426–440.
doi:10.1016/j.neuron.2009.03.021
Malik, W. Q., Truccolo, W., Brown, E. N., and Hochberg, L. R. (2010). Eﬃcient
decodingwithsteady-statekalmanﬁlterinneuralinterfacesystems. IEEE Trans. Neural
Syst. Rehabil. Eng.19,25–34.doi:10.1109/TNSRE.2010.2092443
Marshall, A. W., and Olkin, I. (1967a). A generalized bivariate exponential
distribution.J. Appl. Probab.4,291–302.doi:10.2307/3212024
Marshall, A. W., and Olkin, I. (1967b). A multivariate exponential distribution.J.
Am. Stat. Assoc.62,30–44.doi:10.1080/01621459.1967.10482885
Mathys, C. D., Daunizeau, J., Friston, K. J., and Stephan, K. E. (2011). A Bayesian
foundation for individual learning under uncertainty.Front. Hum. Neurosci. 5:39.
doi:10.3389/fnhum.2011.00039
M athys,C.D.,Lomakina,E.I.,Daunizeau,J.,Igle sias,S.,Broder sen,K.H.,Friston,
K.J.,etal.(2014).UncertaintyinperceptionandthehierarchicalGaussianﬁlter. Front.
Hum. Neurosci.8:825.doi:10.3389/fnhum.2014.00825
McCaslin, J. O., and Broussard, P. R. (2007). Search for chaotic behavior in a
ﬂappingﬂag. arXiv.Availableonlineat: https://arxiv.org/abs/0704.0484
Nazmi, N., Mazlan, S. A., Zamzuri, H., and Rahman, M. A. A. (2015).
Fitting distribution for electromyography and electroencephalography
signals based on goodness-of-ﬁt tests. Procedia Comput. Sci . 76, 468–473.
doi:10.1016/j.procs.2015.12.317
Nocedal, J., and Wright, S. J. (2006).Numerical Optimization, 2nd Edn. New York,
NY:Springer.
Ouyang, G., Wang, S., Liu, M., Zhang, M., and Zhou, C. (2023). Multilevel and
multifacetedbrainresponsefeaturesinspiking,erpanderd:experimentalobservation
and simultaneous generation in a neuronal network model with excitation-inhibition
balance.Cogn. Neurodyn.17,1417–1431.doi:10.1007/s11571-022-09889-w
Pan, H., Fu, Y., Zhang, Q., Zhang, J., and Qin, X. (2022). The decoder design
andperformancecomparativeanalysisforclosed-loopbrain-machineinterfacesystem.
Cogn. Neurodyn.18,147–164.doi:10.1007/s11571-022-09919-7
Qi, Y., Liu, B., Wang, Y., and Pan, G. (2019). “Dynamic ensemble modeling
approachtononstationaryneuraldecodinginbrain-computerinterfaces,"in Advances
in Neural Information Processing Systems, Vol. 32, eds. H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett (Curran Associates,
Inc.). Available online at:https://proceedings.neurips.cc/paper_ﬁles/paper/2019/ﬁle/
3f7bcd0b3ea822683bba8fc530f151bd-Paper.pdf
Roxin, A., Brunel, N., Hansel, D., Mongillo, G., and Vreeswijk, C. V. (2011).
On the distribution of ﬁring rates in networks of cortical neurons.J. Neurosci. 31,
16217–16226.doi:10.1523/JNEUROSCI.1677-11.2011
Shanker, R., Hagos, F., and Sujatha, S. (2015). On modeling of lifetimes
data using exponential and Lindley distributions. Biom. Biostat. Int. J . 2, 1–9.
doi:10.15406/bbij.2015.02.00042
Stein, R. R., Marks, D. S., and Sander, C. (2015). Inferring pairwise interactions
from biological data using maximum-entropy probability models.PLoS Comput. Biol.
11:e1004182.doi:10.1371/journal.pcbi.1004182
Swindale, N. V., Rowat, P., Krause, M., Spacek, M. A., and Mitelut, C. (2021).
Voltagedistributionsinextracellularbrainrecordings. J. Neurophysiol.125,1408–1424.
doi:10.1152/jn.00633.2020
Tanabe, K., and Sagae, M. (1992). An exact cholesky decomposition and
the generalized inverse of the variance-covariance matrix of the multinomial
distribution, with applications. J. R. Stat. Soc. B (Methodological) 54, 211–219.
doi:10.1111/j.2517-6161.1992.tb01875.x
Varde, S. D. (1969). Life testing and reliability estimation for the
two parameter exponential distribution. J. Am. Stat. Assoc . 64, 621–631.
doi:10.1080/01621459.1969.10501000
Xu, L., Xu, M., Jung, T.-P., and Ming, D. (2021). Review of brain encoding and
decoding mechanisms for EEG-based brain-computer interface.Cogn. Neurodyn. 15,
569–584.doi:10.1007/s11571-021-09676-z
Youseﬁ, A., Basu, I., Paulk, A. C., Peled, N., Eskandar, E. N., Dougherty, D. D.,
et al. (2019). Decoding hidden cognitive states from behavior and physiology using
aBayesianapproach. Neural Comput.31,1751–1788.doi:10.1162/neco_a_01196
Zhang,Y.-J.,Yu,Z.-F.,Liu,J.K.,andHuang,T.-J.(2022).Neuraldecodingofvisual
informationacrossdiﬀerentneuralrecordingmodalitiesandapproaches. Mach. Intell.
Res.19,350–365.doi:10.1007/s11633-022-1335-2
Zhu,C.,Zhou,K.,Han,Z.,Tang,Y.,Tang,F.,Si,B.,etal.(2025). General Hierarchical
Brownian Filter in Multi-dimensional Volatile Environments.submitted.
Zhu, C., Zhou, K., Tang, F., Tang, Y., Li, X., Si, B., et al. (2022). A hierarchical
Bayesianmodelforinferringanddecisionmakinginmulti-dimensionalvolatilebinary
environments.Mathematics 10:4775.doi:10.3390/math10244775
Frontiersin ComputationalNeuroscience 13 frontiersin.org