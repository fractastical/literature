ALINE: Joint Amortization for Bayesian Inference
and Active Data Acquisition
Daolang Huang1,2 Xinyi Wen1,2 Ayush Bharti2 Samuel Kaski∗1,2,4 Luigi Acerbi∗3
1 ELLIS Institute Finland
2 Department of Computer Science, Aalto University, Finland
3 Department of Computer Science, University of Helsinki, Finland
4 Department of Computer Science, University of Manchester, UK
{daolang.huang, xinyi.wen, ayush.bharti, samuel.kaski}@aalto.fi
luigi.acerbi@helsinki.fi
Abstract
Many critical applications, from autonomous scientific discovery to personalized
medicine, demand systems that can both strategically acquire the most informa-
tive data and instantaneously perform inference based upon it. While amortized
methods for Bayesian inference and experimental design offer part of the solution,
neither approach is optimal in the most general and challenging task, where new
data needs to be collected for instant inference. To tackle this issue, we intro-
duce the Amortized Active Learning and Inference Engine (ALINE), a unified
framework for amortized Bayesian inference and active data acquisition. ALINE
leverages a transformer architecture trained via reinforcement learning with a
reward based onself-estimatedinformation gain provided by its own integrated
inference component. This allows it to strategically query informative data points
while simultaneously refining its predictions. Moreover, ALINEcanselectively
direct its querying strategy towards specific subsets of model parameters or des-
ignated predictive tasks, optimizing for posterior estimation, data prediction, or
a mixture thereof. Empirical results on regression-based active learning, classi-
cal Bayesian experimental design benchmarks, and a psychometric model with
selectively targeted parameters demonstrate that ALINEdelivers both instant and
accurate inference along with efficient selection of informative points.
1 Introduction
Bayesian inference [28] and Bayesian experimental design [61] offer principled mathematical means
for reasoning under uncertainty and for strategically gathering data, respectively. While both are
foundational, they introduce notorious computational challenges. For example, in scenarios with
continuous data streams, repeatedly applying gold-standard inference methods such as Markov Chain
Monte Carlo (MCMC) [ 13] to update posterior distributions can be computationally demanding,
leading to various approximate sequential inference techniques [10, 18], yet challenges in achieving
both speed and accuracy persist. Similarly, in Bayesian experimental design (BED) or Bayesian
active learning (BAL), the iterative estimation and optimization of design objectives can become
costly, especially in sequential learning tasks requiring rapid design decisions [21, 30, 55], such as in
psychophysical experiments where the goal is to quickly infer the subject’s perceptual or cognitive
parameters [70, 57]. Moreover, a common approach in BED involves greedily optimizing for single-
step objectives, such as the Expected Information Gain (EIG), which measures the anticipated
∗ Equal contribution.
39th Conference on Neural Information Processing Systems (NeurIPS 2025).
arXiv:2506.07259v2  [stat.ML]  21 Oct 2025
Table 1: Comparison of different methods for amortized Bayesian inference and data acquisition.
Method Amortized Inference Amortized Data Acquisition
Posterior Predictive Posterior Predictive Flexible
Neural Processes [27, 26, 52, 53] ✗ ✓ ✗ ✗ ✗
Neural Posterior Estimation [49, 54, 34, 60] ✓ ✗ ✗ ✗ ✗
DAD [23], RL-BOED [9] ✗ ✗ ✓ ✗ ✗
RL-sCEE [8], vsOED [65] ✓ ✗ ✓ ✗ ✗
AAL [46] ✗ ✗ ✗ ✓ ✗
JANA [59], Simformer [31], ACE [15] ✓ ✓ ✗ ✗ ✗
ALINE(this work) ✓ ✓ ✓ ✓ ✓
reduction in uncertainty. However, this leads tomyopicdesigns that may be suboptimal, which do not
fully consider how current choices influence future learning opportunities [23].
To address these issues, a promising avenue has been the development ofamortizedmethods,
leveraging recent advances in deep learning [75]. The core principle of amortization involves pre-
training a neural network on a wide range of simulated or existing problem instances, allowing the
network to “meta-learn” a direct mapping from a problem’s features (like observed data) to its solution
(such as a posterior distribution or an optimal experimental design). Consequently, at deployment,
this trained network can provide solutions for new, related tasks with remarkable efficiency, often
in a single forward pass. Amortized Bayesian inference (ABI) methods—such as neural posterior
estimation [49, 54, 34, 60] that target the posterior, or neural processes [27, 26, 52, 53] that target the
posterior predictive distribution—yield almost instantaneous results for new data, bypassing MCMC
or other approximate inference methods. Similarly, methods for amortized data acquisition, including
amortized BED [23, 41, 40, 9] and BAL [46], instantaneously propose the next design that targets
the learning of the posterior or the posterior predictive distribution, respectively, using a deep policy
network—bypassing the iterative optimization of complex information-theoretic objectives.
While amortized approaches have significantly advanced Bayesian inference and data acquisition,
progress has largely occurred in parallel. ABI offers rapid inference but typically assumes passive
data collection, not addressing strategic data acquisition under limited budgets and data constraints
common in fields such as clinical trials [ 7] and material sciences [48, 44]. Conversely, amortized
data acquisition excels at selecting informative data points, but often the subsequent inference update
based on new data is not part of the amortization, potentially requiring separate, costly procedures
like MCMC. This separation means the cycle of efficiently deciding what data to gather and then
instantaneously updating beliefs has not yet been seamlessly integrated. Furthermore, existing
amortized data acquisition methods often optimize for information gain acrossallmodel parameters
or a fixed predictive target, lacking the flexibility to selectively target specific subsets of parameters or
adapt to varying inference goals. This is a significant drawback in scenarios with nuisance parameters
[58] or when the primary interest lies in particular aspects of the model or predictions—which might
not be fully known in advance. A unified framework that jointly amortizes both active data acquisition
and inference, while also offering flexible acquisition goals, would therefore be highly beneficial.
In this paper, we introduceAmortized ActiveLearning andINferenceEngine (ALINE), a novel
framework designed to overcome these limitations by unifying amortized Bayesian inference and
active data acquisition within a single, cohesive system (Table 1). ALINEutilizes a transformer-based
architecture [69] that, in a single forward pass, concurrently performs posterior estimation, generates
posterior predictive distributions, and decides which data point to query next. Critically, and in
contrast to existing methods, ALINEoffersflexible, targeted acquisition: it can dynamically adjust
at runtime its data-gathering strategy to focus on any specified combination of model parameters
or predictive tasks. This is enabled by an attention mechanism allowing the policy to condition on
specific inference goals, making it particularly effective in the presence of nuisance variables [58] or
for focused investigations. ALINEis trained using a self-guided reinforcement learning objective; the
reward is the improvement in the log-probability of its own approximate posterior over the selected
targets, a principle derived from variational bounds on the expected information gain [24]. Extensive
experiments on diverse tasks demonstrate ALINE’s ability to simultaneously deliver fast, accurate
inference and rapidly propose informative data points.
2
2 Background
Consider a parametric conditional model defined on some space Y ⊆RdY of output variables y given
inputs (or covariates) x∈ X ⊆RdX , and parameterized by θ∈Θ⊆R L. Let DT ={(x i, yi)}T
i=1
be a collection of T data points (orcontext) and p(DT |θ)≡p(y 1:T |x 1:T , θ)denote the likelihood
function associated with the model, which we assume to be well-specified in this paper (i.e., it matches
the true data generation process). Given a prior distribution p(θ), the classical Bayesian inference or
prediction problem involves estimating either theposteriordistribution p(θ|D T )∝p(D T |θ)p(θ) , or
theposterior predictivedistribution p(y⋆
1:M |x ⋆
1:M ,DT ) =E p(θ|D T )[p(y⋆
1:M |x ⋆
1:M , θ,DT )] overtar-
get outputs y⋆
1:M := (y⋆
1, ..., y⋆
M ) corresponding to a given set oftarget inputs x⋆
1:M := (x⋆
1, ..., x⋆
M ).
Estimating these quantities repeatedly via approximate inference methods such as MCMC can be
computationally costly [28], motivating the need for amortized inference methods.
Amortized Bayesian inference (ABI).ABI methods involve training a conditional density network
qϕ, parameterized by learnable weights ϕ, to approximate either the posterior predictive distribu-
tion qϕ(y⋆
1:M |x⋆
1:M ,DT )≈p(y ⋆
1:M |x⋆
1:M ,DT ) [26, 42, 33, 11, 38, 12, 53, 52], the joint posterior
qϕ(θ|D T )≈p(θ|D T ) [49, 54, 34, 60], or both [59, 31, 15]. These networks are usually trained by
minimizing the negative log-likelihood (NLL) objective with respect toϕ:
L(ϕ) =
−Ep(θ)p(DT |θ)p(x ⋆
1:M,y⋆
1:M |θ) [logq ϕ(y⋆
1:M |x ⋆
1:M ,DT )],(predictive tasks)
−Ep(θ)p(DT |θ) [logq ϕ(θ|D T )],(posterior estimation) (1)
where the expectation is over datasets simulated from the generative process p(DT |θ)p(θ) . Once
trained, qϕ can then perform instantaneous approximate inference on new contexts and unseen data
points with a single forward pass. However, these ABI methods do not have the ability to strategically
collect the most informative data points to be included in DT in order to improve inference outcomes.
Amortized data acquisition.BED [ 47, 14, 63, 61] methods aim to sequentially select the next
input (or design parameter) x to query in order to maximize theExpected Information Gain(EIG),
that is, the information gained about parametersθupon observingy:
EIG(x) :=E p(y|x) [H[p(θ)]−H[p(θ|x, y)]],(2)
where H is the Shannon entropyH[p(·)] =−E p(·) [logp(·)] . Directly computing and optimizing EIG
sequentially at each step of an experiment is computationally expensive due to the nested expectations,
and leads to myopic designs. Amortized BED methods address these limitations by offline learning a
design policy network πψ :X × Y → X, parameterized by ψ [23, 40, 9], such that at any step t the
policy πψ proposes a query xt ∼π ψ(·|D t−1, θ)to acquire a data point yt ∼p(y|x t, θ), forming
Dt =D t−1 ∪ {(xt, yt)}. To propose non-myopic designs, πψ is trained by maximizing tractable
lower bounds of the total EIG overT-step sequential trajectories generated by the policyπ ψ:
sEIG(ψ) =E p(DT |π ψ) [H[p(θ)]−H[p(θ|D T )]].(3)
By pre-compiling the design strategy into the policy network, amortized BED methods allow for
near-instantaneous design proposals during the deployment phase via a fast forward pass. Typically,
these amortized BED methods are designed to maximize information gain about the full set of model
parameters θ. Separately, for applications where the primary interest lies in reducing predictive
uncertainty rather than parameter uncertainty, objectives like theExpected Predictive Information
Gain(EPIG) [67] have been proposed, so far in non-amortized settings:
EPIG(x) =E p⋆(x⋆)p(y|x) [H[p(y⋆ |x ⋆)]−H[p(y ⋆ |x ⋆, x, y)]].(4)
This measures the EIG about predictions y⋆ at target inputs x⋆ drawn from a target input distribution
p⋆(x⋆). Notably, current amortized data acquisition methods are inflexible: they are generally trained
to learn about all parameters θ (via objectives like sEIG) and lack the capability to dynamically target
specific subsets of parameters or adapt their acquisition strategy to varying inference goals at runtime.
Related work.A major family of ABI methods is Neural Processes (NPs) [ 27, 26], that learn a
mapping from the observed context data points to a predictive distribution for new target points.
Early NPs often employed MLP-based encoders [27, 26, 38, 11], while recent works utilize more
advanced attention and transformer architectures [ 42, 53, 52, 19, 20, 5, 4]. Complementary to
3
...
...
...
D ata Emb edder
T r a ns f ormer
Se lec ti v e M as k
N e x t qu er y A p pr o xim ate inf er enc e
P osterior im pr o v emen tU p d ated histor y
Inf er enc e
H e ad
P o lic y
H e ad
A L I N E
P osterior Ex p erimen tation
P osterior P r edic ti v e
S te p 1
S te p 1
S te p t
S te p t
insta n t insta n t
Figure 1: Conceptual workflow of ALINE, demonstrating its capability to sequentially query informa-
tive data points and perform rapid posterior or predictive inference based on the gathered data.
NPs, methods within simulation-based inference [17] focus on amortizing the posterior distribution
[54, 49, 34, 60, 51, 75]. More recently, methods for amortizing both the posterior and posterior
predictive distributions have been proposed [ 31, 59, 15]. Specifically, ACE [ 15] shows how to
flexibly condition on diverse user-specified inference targets, a method ALINEincorporates for its
own flexible inference capabilities. Building on this principle of goal-directed adaptability, ALINE
advances it by integrating a learned policy that dynamically tailors the data acquisition strategy to the
specified objectives. Existing amortized BED or BAL methods [23, 40, 9, 46] that learn an offline
design policy do not provide real-time estimates of the posterior, unlike ALINE. Recent exceptions
include methods like RL-sCEE [8] and vsOED [65], that use a variational posterior bound of EIG to
provide amortized posterior inference via a separate proposal network. Compared to these methods,
ALINEuses a single, unified architecture where the same model performs amortized inference for
both posterior and posterior predictive distributions, and learns the flexible acquisition policy.
3 Amortized active learning and inference engine
Problem setup.We aim to develop a system that intelligently acquires a sequence of T informative
data points, DT ={(x i, yi)}T
i=1, to enable accurate and rapid Bayesian inference. This system must
be flexible: capable of targeting different quantities of interest, such as subsets of model parameters
or future predictions. To formalize this flexibility, we introduce atarget specifier, denoted by ξ∈Ξ ,
which defines the specific inference goal. We consider two primary types of targets: (1)Parameter
targets (ξθ
S)with the goal to infer a specific subset of model parameters θS, where S⊆ {1, . . . , L}is
an index set of parameters of interest. For example, ξθ
{1,2} would target the joint posterior of θ1 and
θ2, while ξθ
{1,...,L} targets all parameters, aligning with standard BED. We defineS={S 1, . . . , S|S|}
as the collection of all predefined parameter index subsets the system can target. (2)Predictive
targets (ξy⋆
p⋆ ), where the objective is to improve the posterior predictive distributionp(y⋆|x⋆,DT ) for
inputs x⋆ drawn from a specified target input distributionp⋆(x⋆). For simplicity, and following Smith
et al. [67], we consider a single target distribution p⋆(x⋆) in this work. The set of all target specifiers
that ALINEis trained to handle is thus Ξ ={ξ θ
S}S∈S ∪ {ξy⋆
p⋆ }. We assume a discrete distribution p(ξ)
over these possible targets, reflecting the likelihood or importance of each specific goal.
To achieve both instant, informative querying and accurate inference, we propose to jointly learn an
amortized inference model qϕ and anacquisition policy πψ within a single, integrated architecture.
Given the accumulated data history Dt−1 and a specific target ξ∈Ξ , the policy πψ selects the next
query xt designed to be most informative for that target. Subsequently, the new data point (xt, yt) is
observed, and the inference model qϕ updates its estimate of the corresponding posterior or posterior
predictive distribution. A conceptual workflow of ALINEis illustrated in Figure 1. In the remainder
of this section, we detail the objectives for training the inference network qϕ (Section 3.1) and the
acquisition policy πψ (Section 3.2), discuss their practical implementation (Section 3.3), and describe
the unified model architecture (Section 3.4).
4
3.1 Amortized inference
We use the inference network qϕ to provide accurate approximations of the true Bayesian posterior
p(θ|D T ) or posterior predictive distribution p(y⋆ |x ⋆,DT ), given the acquired data DT . We train qϕ
via maximum-likelihood (Eq. 1). Specifically, for parameter targetsξ=ξ θ
S, our objective is:
Lθ
S(ϕ) =−E p(θ)p(DT |θ) [logq ϕ(θS |DT )]≈ −Ep(θ)p(DT |θ)
P
l∈S logq ϕ(θl |DT )

,(5)
where we adopt a diagonal ormean fieldapproximation, where the joint distribution is obtained as a
product of marginals qϕ(θS |DT )≈ Q
l∈S qϕ(θl |DT ). Analogously, for predictive targets ξ=ξ y⋆
p⋆ ,
we assume a factorized likelihood over targets sampled from the target input distributionp⋆(x⋆):
Ly⋆
p⋆ (ϕ) =−E p(θ)p(DT |θ)p ⋆(x⋆
1:M)p(y⋆
1:M |x ⋆
1:M,θ) [logq ϕ(y⋆
1:M |x ⋆
1:M ,DT )]
≈ −Ep(θ)p(DT |θ)p ⋆(x⋆
1:M)p(y⋆
1:M |x ⋆
1:M,θ)
hPM
m=1 logq ϕ(y⋆
m |x ⋆
m,DT )
i
.(6)
The factorized form of these training objectives is a common scalable choice in the neural process
literature [26, 52, 53, 15] and is more flexible than it might seem, as conditional marginal distributions
can be extended to representfull jointsautoregressively [ 53, 11, 15]. However, a full autoregressive
model would require multiple forward passes to compute the reward signal for our policy at each
training step, making the learning process computationally intractable. Therefore, for simplicity and
tractability, within the scope of this paper we focus on the marginals, leaving the autoregressive exten-
sion to future work. Eqs. 5 and 6 form the basis for training the inference component qϕ. Optimizing
them minimizes the Kullback-Leibler (KL) divergence between the true target distributions (posterior
or predictive) defined by the underlying generative process and the model’s approximations qϕ [52].
Learning an accurate qϕ is crucial as it not only determines the quality of the final inference output
but also serves as the basis for guiding the data acquisition policy, as we see next.
3.2 Amortized data acquisition
The quality of inference from qϕ depends critically on the informativeness of the acquired dataset
DT . The acquisition policy πψ is thus responsible for actively selecting a sequence of query-data
pairs(x t, yt)to maximize the information gained about a specific targetξ.
When targeting parameters θS (i.e., ξ=ξ θ
S), the objective is the total Expected Information Gain
(sEIGθS ) aboutθ S over theT-step trajectory generated byπ ψ (see Eq. 3):
sEIGθS (ψ) =E p(θ)p(DT |π ψ,θ) [logp(θ S |DT )] +H[p(θ S)].(7)
For completeness, we include a derivation of sEIGθS in Section A.1 which is analogous to that of
sEIG in [23]. Directly optimizing sEIG is generally intractable due to its reliance on the unknown
true posterior p(θS |DT ). We circumvent this by substituting p(θS |DT ) with its approximation
qϕ(θS |DT )(from Eq. 5), yielding the tractable objectiveJ θ
S for trainingπ ψ:
J θ
S (ψ) :=E p(θ)p(DT |π ψ,θ)
P
l∈S logq ϕ(θl |DT )

+H[p(θ S)].(8)
The inference objective (Eq. 5) and this policy objective are thus coupled: Lθ
S(ϕ) depends on data
DT acquired throughπ ψ, andJ θ
S (ψ)depends on the inference networkq ϕ.
Similarly, when targeting predictions for ξ=ξ y⋆
p⋆ , we aim to maximize information about
p(y⋆ |x ⋆,DT ) for x⋆ ∼p ⋆(x⋆). We extend the Expected Predictive Information Gain (EPIG)
framework [67] to the amortized sequential setting, defining the total sEPIG:
Proposition 1.The total expected predictive information gain for a design policy πψ over a data
trajectory of lengthTis:
sEPIG(ψ) :=E p⋆(x⋆)p(DT |π ψ)[H[p(y⋆ |x ⋆)]−H[p(y ⋆ |x ⋆,DT )]]
=E p(θ)p(DT |π ψ,θ)p⋆(x⋆)p(y⋆ |x ⋆,θ) [logp(y ⋆ |x ⋆,DT )] +Ep⋆(x⋆)[H[p(y⋆ |x ⋆)]].
This result adapts Theorem 1 in [23] for the predictive case (see Section A.2 for proof) and, unlike
single-step EPIG (Eq. 4), considers the entire trajectoryD T given the policyπ ψ.
5
Now, similar to Eq. 8, we use the inference network qϕ(y⋆ |x ⋆,DT ) to replace the true posterior
predictive distributionp(y ⋆ |x ⋆,DT )in Proposition 1 to obtain our active learning objective:
J y⋆
p⋆ (ψ) :=E p(θ)p(DT |π ψ,θ)p⋆(x⋆)p(y⋆ |x ⋆,θ) [logq ϕ(y⋆ |x ⋆,DT )] +Ep⋆(x⋆)[H[p(y⋆ |x ⋆)]].(9)
Finally, the following proposition proves that our acquisition objectives, J θ
S and J y⋆
p⋆ , are variational
lower bounds on the true total information gains ( sEIGθS and sEPIG, respectively), making them
principled tractable objectives for our goal. The proof is given in Section A.3.
Proposition 2.Let the policy πψ generate the trajectory DT . With qϕ(θS |DT ) approximating
p(θS |DT ), and qϕ(y⋆ |x ⋆,DT ) approximating p(y⋆ |x ⋆,DT ), we have J θ
S (ψ)≤sEIG θS (ψ) and
J y⋆
p⋆ (ψ)≤sEPIG(ψ). Moreover,
sEIGθS (ψ)− Jθ
S (ψ) =E p(DT |π ψ)[KL(p(θS |DT )||qϕ(θS |DT ))],and
sEPIG(ψ)− Jy⋆
p⋆ (ψ) =E p⋆(x⋆)p(DT |π ψ)[KL(p(y⋆ |x ⋆,DT )||qϕ(y⋆ |x ⋆,DT ))].
This principle of using approximate posterior (or predictive) distributions to bound information gain
is foundational in Bayesian experimental design (e.g., [ 24]) and has been extended to sequential
amortized settings [8, 65]. Maximizing these J objectives thus encourages policies that increase
information about the targets. The tightness of these bounds is governed by the expected KL
divergence between the true quantities and their approximation, with a more accurate qϕ leading
to tighter bounds and a more effective training signal for the policy. Additionally, our objectives
solely rely on ALINE’s variational posterior, which does not require an explicit likelihood, making it
naturally applicable to problems with implicit likelihoods where only forward sampling is possible.
Data acquisition objective for ALINE.To handle any target ξ∈Ξ from a user-specified set, we
unify the previously defined acquisition objectives. We define J(ψ, ξ)based on the type of target ξ:
J(ψ, ξ) =
J θ
S (ψ),ifξ=ξ θ
S
J y⋆
p⋆ (ψ),ifξ=ξ y⋆
p⋆ .
The final objective for learning the policy network πψ, denoted as J Ξ, is the expectation of J(ψ, ξ)
taken over the distribution of possible target specifiersp(ξ):J Ξ(ψ) =E ξ∼p(ξ)[J(ψ, ξ)].
3.3 Training methodology for ALINE
The policy and inference networks,πψ and qϕ, in ALINEare trained jointly. Training πψ for sequential
data acquisition over a T-step horizon is naturally framed as a reinforcement learning (RL) problem.
Policy network training (πψ).To guide the policy, we employ a dense, per-step reward signal
Rt rather than relying on a sparse reward at the end of the trajectory. This approach, common in
amortized experimental design [9, 8, 37], helps stabilize and accelerate learning. The reward Rt
quantifies the immediate improvement in the inference quality provided by qϕ upon observing a new
data point (xt, yt), specifically concerning the current target ξ. It is defined based on the one-step
change in the log-probabilities from our acquisition objectives (Eqs. 8 and 9):
Rt(ξ) =
(
1
|S|
P
l∈S(logq ϕ(θl |Dt)−logq ϕ(θl |Dt−1)),ifξ=ξ θ
S
1
M
PM
m=1(logq ϕ(y⋆
m |x ⋆
m,Dt)−logq ϕ(y⋆
m |x ⋆
m,Dt−1)),ifξ=ξ y⋆
p⋆ . (10)
As per common practice, for gradient stabilization we take averages (not sums) over predictions,
which amounts to a constant relative rescaling of our objectives. The policy πψ is then trained using
a policy gradient (PG) algorithm with per-episode loss:
LPG(ψ) =− PT
t=1 γtRt(ξ) logπψ(xt|Dt−1, ξ),(11)
which maximizes the expected cumulative γ-discounted reward over trajectories [68]. Gradients from
this policy loss only update the policy parameters ψ. They are not propagated back to the inference
networkq ϕ to ensure each component has a clear and distinct objective.
6
Emb edding La y er s
T r a ns f ormer La y er s
A c quisition H e ad Inf er enc e H e ad
or... ......
Con te x t Set Qu er y Set T a r g et Set
++
Figure 2: The ALINEarchitecture. The model takes historical observations (context set), candidate
points (query set), and the current inference goal (target set) as inputs. These are transformed by
embedding layers and subsequently by transformer layers. Finally, an acquisition head determines
the next data point to query, while an inference head performs the approximate Bayesian inference.
Inference network training (qϕ).For the per-step rewards Rt to be meaningful, the inference net-
work qϕ must provide accurate estimates of posteriors or predictive distributions ateach intermediate
step t of the acquisition sequence, not just at the final step T. Consequently, the practical training
objective for qϕ, denoted LNLL(ϕ), encourages this step-wise accuracy. In practice, training proceeds
in episodes. For each episode: (1) A ground truth parameter set θ is sampled from the prior p(θ). (2)
A target specifier ξ is sampled from p(ξ). (3) If the target is predictive (ξ=ξ y⋆
p⋆ ), M target inputs
{x⋆
m}M
m=1 are sampled from p⋆(x⋆), and their corresponding true outcomes {y⋆
m}M
m=1 are simulated
using θ, similarly as [ 67]. The negative log-likelihood loss LNLL(ϕ) for qϕ in an episode is then
computed by averaging over the T acquisition steps and predictions, using the Monte Carlo estimates
of the objectives defined in Eqs. 5 and 6:
LNLL(ϕ)≈
(
− 1
T
1
|S|
PT
t=1
P
l∈S logq(θ l |Dt),ifξ=ξ θ
S
− 1
T
1
M
PT
t=1
PM
m=1 logq(y ⋆
m |x ⋆
m,Dt),ifξ=ξ y⋆
p⋆ . (12)
Joint training.To ensure qϕ provides a reasonable reward signal early in training, we employ an
initial warm-up phase where only qϕ is trained, with data acquisition (xt, yt) guided by random
actions instead of πψ. After the warm-up, qϕ and πψ are trained jointly. A detailed step-by-step
training algorithm is provided in Section B.1.
3.4 Architecture
ALINEemploys a single, integrated neural architecture based on Transformer Neural Processes (TNPs)
[53, 15], a network architecture that has been successfully applied to various amortized sequential
decision-making settings [37, 1, 50, 39, 76]. ALINEleverages TNPs to concurrently manage historical
observations, propose future queries, and condition predictions on specific, potentially varying,
inference objectives. An overview of ALINE’s architecture is provided in Figure 2.
The model inputs are structured into three sets. Following standard TNP-based architecture, the
context set Dt ={(x i, yi)}t
i=1 comprises the history of observations, and thetarget set T contains
the specific target specifier ξ. To facilitate active data acquisition, we incorporate aquery set
Q={x q
n}N
n=1 of candidate points. In this paper, we focus on a discrete pool-based setting for
consistency, though it can be straightforwardly extended to continuous design spaces (e.g., [ 64]).
Details regarding the embeddings of these inputs are provided in Section B.2.
7
5 10 15 20 25 30
0.0
0.2
0.4
0.6RMSE
Synthetic 1D
10 20 30 40 50
0.0
0.2
0.4
0.6
Synthetic 2D
5 10 15 20 25 30
0.0
0.2
0.4
0.6
Higdon 1D
10 20 30 40 50
Number of Steps t
0.0
0.2
0.4
0.6
0.8RMSE
Goldstein-Price 2D
10 20 30 40 50
Number of Steps t
0.00
0.25
0.50
0.75
1.00
Ackley 2D
10 20 30 40 50
Number of Steps t
0.000
0.025
0.050
0.075
0.100
Gramacy 2D
GP-RS GP-US GP-VR GP-EPIG ACE-US ALINE (ours)
Figure 3: Predictive performance on active learning benchmark functions (RMSE ↓). Results show
the mean and 95% confidence interval (CI) across 100 runs.
Standard transformer attention mechanisms process these embedded representations. Self-attention
operates within the context set, capturing dependencies within Dt. Both the query and target set then
employ cross-attention to attend to the processed context set representations. To enable the policy πψ
to dynamically adapt its acquisition strategy based on the specific inference goalξ, we introduce an
additionalquery-target cross-attentionmechanism to allow the query candidates to directly attend to
the target set. This allows the evaluation of each candidate xq
n to be informed by its relevance to the
different potential targetsξ. Examples of the attention mask are shown in Figure A1.
Finally, two specialized output heads operate on these processed representations. Theinference head
(qϕ), following ACE [15], uses a Gaussian mixture to parameterize the approximate posteriors and
posterior predictives. Theacquisition head( πψ) generates a policy over the query set πψ(xt+1|Dt),
drawing on principles from policy-based design methods [ 37, 50]. This unified design, which
leverages a shared transformer backbone with specialized heads, significantly improves parameter
efficiency by avoiding the need for separate encoders for the two tasks, leading to faster training and
more efficient deployment.
4 Experiments
We now empirically evaluate ALINE’s performance in different active data acquisition and amortized
inference tasks. We begin with the active learning task in Section 4.1, where we want to efficiently
minimize the uncertainty over an unknown function by querying T data points. Then, we test
ALINE’s policy on standard BED benchmarks in Section 4.2. In Section 4.3, we demonstrate the
benefit of ALINE’s flexible targeting feature in a psychometric modeling task [ 72]. Finally, to
demonstrate the scalability of ALINE, we test it on a high-dimensional task of actively exploring
hyperparameter performance landscapes; the results are presented in Section D.1. The code to
reproduce our experiments is available at:https://github.com/huangdaolang/aline.
4.1 Active learning for regression and hyperparameter inference
For the active learning task, ALINEis trained on a diverse collection of fullysynthetic functions
drawn from Gaussian Process (GP) [62] priors (see Section C.1.1 for details). We evaluate ALINE’s
performance under bothin-distributionandout-of-distributionsettings. For thein-distributionsetting,
ALINEis evaluated on synthetic functions sampled from the same GP prior that is used during
training. In theout-of-distributionsetting, we evaluate ALINEon benchmark functions (Higdon,
Goldstein-Price, Ackley, and Gramacy)unseenduring training, to assess generalization beyond the
training regime. We compare ALINEagainst non-amortized GP models equipped with standard
acquisition functions such as Uncertainty Sampling (GP-US), Variance Reduction (GP-VR) [ 74],
EPIG (GP-EPIG) [67], and Random Sampling (GP-RS). Additionally, we include an amortized neural
8
Table 2: Results on BED benchmarks. For the EIG lower bound, we report the mean ± 95% CI
across 2000 runs (200 for VPCE). For deployment time, we use the mean±95% CI from 20 runs.
Location Finding Constant Elasticity of Substitution
EIG lower
bound (↑)
Training
time (h)
Deployment
time (s)
EIG lower
bound (↑)
Training
time (h)
Deployment
time (s)
Random 5.17±0.05 N/A N/A 9.05±0.26 N/A N/A
VPCE [25] 5.25±0.22 N/A 146.59±0.09 9.40±0.27 N/A 788.90±1.03
DAD [23] 7.33±0.06 7.24 0.0001±0.00 10.77±0.15 13.70 0.0001±0.00
vsOED [65] 7.30±0.06 4.31 0.0002±0.00 12.12±0.18 0.49 0.0003±0.00
RL-BOED [9] 7.70±0.06 63.29 0.0003±0.00 14.60±0.10 67.28 0.0004±0.00
ALINE(ours) 8.91±0.04 21.20 0.03±0.00 13.50±0.15 13.29 0.04±0.00
process baseline, the Amortized Conditioning Engine (ACE) [15], paired with Uncertainty Sampling
(ACE-US), to specifically evaluate the advantage of ALINE’s learned acquisition policy over using a
standard acquisition function with an amortized inference model. The performance metric is the root
mean squared error (RMSE) of the predictions on a held-out test set.
5 10 15 20 25 30
Number of Steps t
-0.6
-0.4
-0.2
0.0
0.2
Log Probability
ACE-RS
ACE-US
ALINE (ours)
Figure 4: Hyperparameter in-
ference performance on syn-
thetic GP functions.
Results for the active learning task in Figure 3 show that ALINE
performs comparably to the best-performing GP-based methods for
thein-distributionsetting. Importantly, for theout-of-distribution
setting, ALINEoutperforms the baselines in 3 out of the 4 bench-
mark functions. These results highlight the advantage of ALINE’s
end-to-end learning strategy, which obviates the need for kernel spec-
ification using GPs or explicit acquisition function selection. Further
evaluations on additional benchmark functions (Gramacy 1D, Branin,
Three Hump Camel), visualizations of ALINE’s sequential querying
strategy with corresponding predictive updates, and a comparison of
average inference times are provided in Section D.2.
Additionally, for thein-distributionsetting, we test ALINE’s capability
to infer the underlying GP’s hyperparameters—without retraining, by leveraging ALINE’s flexible
target specification at runtime. For baselines, we use ACE-US and ACE-RS, since ACE [15] is also
capable of posterior estimation. Figure 4 shows that ALINEyields higher log probabilities of the true
parameter value under the estimated posterior at each step compared to the baselines. This is due to
the ability to flexibly switch ALINE’s acquisition strategy to parameter inference, unlike other active
learning methods. We also visualize the obtained posteriors in Section D.2.
4.2 Benchmarking on Bayesian experimental design tasks
We test ALINEon two classical BED tasks: Location Finding [ 66] and Constant Elasticity of
Substitution (CES) [3], with two- and six-dimensional design space, respectively. As baselines, we
include a random design policy, a gradient-based method with variational Prior Contrastive Estimation
(VPCE) [25], and three amortized BED methods: Deep Adaptive Design (DAD) [23], vsOED [65],
and RL-BOED [9]. Details of the tasks and the baselines are provided in Section C.2.
To evaluate performance, we compute a lower bound of the total EIG, namely the sequential Prior
Contrastive Estimation lower bound [23]. As shown in Table 2, ALINEsurpasses all the baselines in
the Location Finding task and achieves competitive performance on the CES task, outperforming most
other methods, except RL-BOED. Notably, ALINE’s training time is reduced as its reward is based
on the internal posterior improvement and does not require a large number of contrastive samples to
estimate sEIG. While ALINE’s deployment time is slightly higher than MLP-based amortized methods
due to the computational cost of its transformer architecture, it remains orders of magnitude faster
than non-amortized approaches like VPCE. Visualizations of ALINE’s inferred posterior distributions
are provided in Section D.3.
4.3 Psychometric model
Our final experiment involves the psychometric modeling task [ 72]—a fundamental scenario in
behavioral sciences, from neuroscience to clinical settings [ 29, 57, 73], where the goal is to infer
parameters governing an observer’s responses to varying stimulus intensities. The psychometric
9
10 20 30
Number of Steps t
0.5
1.0
1.5
RMSE ↓
(a)
QUEST+
Psi-marginal
ALINE (ours)
10 20 30
Number of Steps t
−5
0
5Stimulu Values
(b)
True threshold
Query points
10 20 30
Number of Steps t
0.20
0.25
0.30
RMSE ↓
(c)
10 20 30
Number of Steps t
−5
0
5Stimuli Values
(d)
Threshold & Slope Guess Rate & Lapse Rate
Figure 5: Results on psychometric model. RMSE (mean±95% CI) when targeting (a) threshold &
slope and (c) guess & lapse rates, with ALINE’s corresponding query strategies shown in (b) and (d).
function used here is characterized by four parameters: threshold, slope, guess rate, and lapse rate;
see Section C.3.1 for details. Different research questions in psychophysics necessitate focusing on
different parameter subsets. For instance, studies on perceptual sensitivity primarily target precise
estimation of threshold and slope, while investigations into response biases or attentional phenomena
might focus on the guess and lapse rates. This is where ALINE’s unique flexible querying strategy
can be used to target specific parameter subsets of interest.
We compare ALINEwith two established adaptive psychophysical methods: QUEST+ [ 70], which
targets all parameters simultaneously, and Psi-marginal [58], which can marginalize over nuisance
parameters to focus on a specified subset, a non-amortized gold-standard method for flexible acquisi-
tion. We evaluate scenarios targeting either the threshold and slope parameters or the guess and lapse
rates. Details of the baselines and the experimental setup are in Section C.3.2.
Figure 5 shows the results. When targeting threshold and slope (Figure 5a), which are generally
easier to infer, ALINEachieves results comparable to baselines. When targeting guess and lapse rates
(Figure 5c), QUEST+ performs sub-optimally as its experimental design strategy is dominated by the
more readily estimable threshold and slope parameters. In contrast, both Psi-marginal and ALINE
lead to significantly better inference than QUEST+ when explicitly targeting guess and lapse rates.
Moreover, ALINEoffers a 10× speedup over these non-amortized methods (See Section D). We
also visualize the query strategies adopted by ALINEin the two cases: when targeting threshold and
slope (Figure 5b), stimuli are concentrated near the estimated threshold. Conversely, when targeting
guess and lapse rates (Figure 5d), ALINEappropriately selects ‘easy’ stimuli at extreme values where
mistakes can be more readily attributed to random behavior (governed by lapses and guesses) rather
than the discriminative ability of the subject (governed by threshold and slope). To further demonstrate
ALINE’s runtime flexibility, we conduct two additional investigations detailed in Section D.4. First,
we show that a single pre-trained ALINE model can dynamically switch its acquisition target mid-
experiment. Second, we validate its ability to generalize to novel combinations of targets that were
not seen during training, showing the effectiveness of the query-target cross-attention mechanism.
5 Conclusion
We introduced ALINE, a unified amortized framework that seamlessly integrates active data acqui-
sition with Bayesian inference. ALINEdynamically adapts its strategy to target selected inference
goals, offering a flexible and efficient solution for Bayesian inference and active data acquisition.
Limitations & future work.Currently, ALINEoperates with pre-defined, fixed priors, necessitating
re-training for different prior specifications. Future work could explore prior amortization [15, 71], to
allow for dynamic prior conditioning. As ALINEestimates marginal posteriors, extending this to joint
posterior estimation, potentially via autoregressive modeling [11, 35], is a promising direction. Note
that, at deployment, we may encounter observations that differ substantially from the training data,
leading to degradation in performance. This issue can potentially be tackled by combining ALINE
with robust approaches such as [ 22, 36]. Lastly, ALINE’s current architecture is tailored to fixed
input dimensionalities and discrete design spaces, a common practice with TNPs [ 53, 50, 76, 37].
Generalizing ALINEto be dimension-agnostic [45] and to support continuous experimental designs
[64] are valuable avenues for future research.
10
Acknowledgements
DH, LA and SK were supported by the Research Council of Finland (Flagship programme: Finnish
Center for Artificial Intelligence FCAI, 359207). The authors acknowledge the research environment
provided by ELLIS Institute Finland. LA was also supported by Research Council of Finland grants
358980 and 356498. SK was also supported by the UKRI Turing AI World-Leading Researcher
Fellowship, [EP/W002973/1]. AB was supported by the Research Council of Finland grant no.
362534. The authors wish to thank Aalto Science-IT project, and CSC–IT Center for Science,
Finland, for the computational and data storage resources provided.
References
[1] Andersson, T. R., Bruinsma, W. P., Markou, S., Requeima, J., Coca-Castro, A., Vaughan, A.,
Ellis, A.-L., Lazzara, M. A., Jones, D., Hosking, S., et al. (2023). Environmental sensor placement
with convolutional gaussian neural processes.Environmental Data Science, 2:e32. 7
[2] Arango, S. P., Jomaa, H. S., Wistuba, M., and Grabocka, J. (2021). Hpo-b: A large-scale
reproducible benchmark for black-box hpo based on openml. InThirty-fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track. 24
[3] Arrow, K. J., Chenery, H. B., Minhas, B. S., and Solow, R. M. (1961). Capital-labor substitution
and economic efficiency.The Review of Economics and Statistics, 43(3):225–250. 9, 21
[4] Ashman, M., Diaconu, C., Kim, J., Sivaraya, L., Markou, S., Requeima, J., Bruinsma, W. P.,
and Turner, R. E. (2024a). Translation equivariant transformer neural processes. InInternational
Conference on Machine Learning, pages 1924–1944. PMLR. 3
[5] Ashman, M., Diaconu, C., Weller, A., and Turner, R. E. (2024b). In-context in-context learning
with transformer neural processes. InSymposium on Advances in Approximate Bayesian Inference,
pages 1–29. PMLR. 3
[6] Barlas, Y . Z. and Salako, K. (2025). Performance comparisons of reinforcement learning
algorithms for sequential experimental design.arXiv preprint arXiv:2503.05905. 27
[7] Berry, S. M., Carlin, B. P., Lee, J. J., and Muller, P. (2010).Bayesian adaptive methods for
clinical trials. CRC press. 2
[8] Blau, T., Bonilla, E., Chades, I., and Dezfouli, A. (2023). Cross-entropy estimators for sequential
experiment design with reinforcement learning.arXiv preprint arXiv:2305.18435. 2, 4, 6
[9] Blau, T., Bonilla, E. V ., Chades, I., and Dezfouli, A. (2022). Optimizing sequential experimental
design with deep reinforcement learning. InInternational conference on machine learning, pages
2107–2128. PMLR. 2, 3, 4, 6, 9, 21, 22, 23
[10] Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., and Jordan, M. I. (2013). Streaming
variational bayes.Advances in neural information processing systems, 26. 1
[11] Bruinsma, W., Markou, S., Requeima, J., Foong, A. Y ., Andersson, T., Vaughan, A., Buonomo,
A., Hosking, S., and Turner, R. E. (2023). Autoregressive conditional neural processes. InThe
Eleventh International Conference on Learning Representations. 3, 5, 10
[12] Bruinsma, W., Requeima, J., Foong, A. Y ., Gordon, J., and Turner, R. E. (2020). The gaussian
neural process. InThird Symposium on Advances in Approximate Bayesian Inference. 3
[13] Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker,
M., Guo, J., Li, P., and Riddell, A. (2017). Stan: A probabilistic programming language.Journal
of statistical software, 76:1–32. 1
[14] Chaloner, K. and Verdinelli, I. (1995). Bayesian experimental design: A review.Statistical
science, pages 273–304. 3
[15] Chang, P. E., Loka, N., Huang, D., Remes, U., Kaski, S., and Acerbi, L. (2025). Amortized
probabilistic conditioning for optimization, simulation and inference. InInternational Conference
on Artificial Intelligence and Statistics. PMLR. 2, 3, 4, 5, 7, 8, 9, 10, 21
11
[16] Chen, X., Wang, C., Zhou, Z., and Ross, K. W. (2021). Randomized ensembled double q-
learning: Learning fast without a model. InInternational Conference on Learning Representations.
23
[17] Cranmer, K., Brehmer, J., and Louppe, G. (2020). The frontier of simulation-based inference.
Proceedings of the National Academy of Sciences, 117(48):30055–30062. 4
[18] Doucet, A., De Freitas, N., Gordon, N. J., et al. (2001).Sequential Monte Carlo methods in
practice, volume 1. Springer. 1
[19] Feng, L., Hajimirsadeghi, H., Bengio, Y ., and Ahmed, M. O. (2023). Latent bottlenecked
attentive neural processes. InThe Eleventh International Conference on Learning Representations.
3
[20] Feng, L., Tung, F., Hajimirsadeghi, H., Bengio, Y ., and Ahmed, M. O. (2024). Memory efficient
neural processes via constant memory attention block. InInternational Conference on Machine
Learning, pages 13365–13386. PMLR. 3
[21] Filstroff, L., Sundin, I., Mikkola, P., Tiulpin, A., Kylmäoja, J., and Kaski, S. (2024). Targeted
active learning for bayesian decision-making.Transactions on Machine Learning Research. 1
[22] Forster, A., Ivanova, D. R., and Rainforth, T. (2025). Improving robustness to model misspecifi-
cation in bayesian experimental design. In7th Symposium on Advances in Approximate Bayesian
Inference Workshop Track. 10
[23] Foster, A., Ivanova, D. R., Malik, I., and Rainforth, T. (2021). Deep adaptive design: Amortizing
sequential bayesian experimental design. InInternational Conference on Machine Learning, pages
3384–3395. PMLR. 2, 3, 4, 5, 9, 16, 22, 27
[24] Foster, A., Jankowiak, M., Bingham, E., Horsfall, P., Teh, Y . W., Rainforth, T., and Goodman,
N. (2019). Variational bayesian optimal experimental design.Advances in Neural Information
Processing Systems, 32. 2, 6, 21
[25] Foster, A., Jankowiak, M., O’Meara, M., Teh, Y . W., and Rainforth, T. (2020). A unified stochas-
tic gradient approach to designing bayesian-optimal experiments. InInternational Conference on
Artificial Intelligence and Statistics, pages 2959–2969. PMLR. 9, 22
[26] Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan, M., Teh, Y . W.,
Rezende, D., and Eslami, S. A. (2018a). Conditional neural processes. InInternational conference
on machine learning, pages 1704–1713. PMLR. 2, 3, 5
[27] Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y . W.
(2018b). Neural processes.arXiv preprint arXiv:1807.01622. 2, 3
[28] Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013).
Bayesian Data Analysis. CRC Press. 1, 3
[29] Gilaie-Dotan, S., Kanai, R., Bahrami, B., Rees, G., and Saygin, A. P. (2013). Neuroanatomical
correlates of biological motion detection.Neuropsychologia, 51(3):457–463. 9
[30] Giovagnoli, A. (2021). The bayesian design of adaptive clinical trials.International journal of
environmental research and public health, 18(2):530. 1
[31] Gloeckler, M., Deistler, M., Weilbach, C. D., Wood, F., and Macke, J. H. (2024). All-in-one
simulation-based inference. InInternational Conference on Machine Learning, pages 15735–
15766. PMLR. 2, 3, 4
[32] Glorot, X. and Bengio, Y . (2010). Understanding the difficulty of training deep feedforward
neural networks. In Teh, Y . W. and Titterington, M., editors,Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volume 9 ofProceedings of
Machine Learning Research, pages 249–256, Chia Laguna Resort, Sardinia, Italy. PMLR. 22
[33] Gordon, J., Bruinsma, W. P., Foong, A. Y ., Requeima, J., Dubois, Y ., and Turner, R. E.
(2020). Convolutional conditional neural processes. InInternational Conference on Learning
Representations. 3
12
[34] Greenberg, D., Nonnenmacher, M., and Macke, J. (2019). Automatic posterior transformation
for likelihood-free inference. InInternational conference on machine learning, pages 2404–2414.
PMLR. 2, 3, 4
[35] Hassan, C., Loka, N., Li, C.-Y ., Huang, D., Chang, P. E., Yang, Y ., Silvestrin, F., Kaski, S., and
Acerbi, L. (2025). Efficient autoregressive inference for transformer probabilistic models.arXiv
preprint arXiv:2510.09477. 10
[36] Huang, D., Bharti, A., Souza, A., Acerbi, L., and Kaski, S. (2023a). Learning robust statistics
for simulation-based inference under model misspecification.Advances in Neural Information
Processing Systems, 36:7289–7310. 10
[37] Huang, D., Guo, Y ., Acerbi, L., and Kaski, S. (2025). Amortized bayesian experimental design
for decision-making.Advances in Neural Information Processing Systems, 37:109460–109486. 6,
7, 8, 10
[38] Huang, D., Haussmann, M., Remes, U., John, S., Clarté, G., Luck, K., Kaski, S., and Acerbi, L.
(2023b). Practical equivariances via relational conditional neural processes.Advances in Neural
Information Processing Systems, 36:29201–29238. 3
[39] Hung, Y . H., Lin, K.-J., Lin, Y .-H., Wang, C.-Y ., Sun, C., and Hsieh, P.-C. (2025). Boformer:
Learning to solve multi-objective bayesian optimization via non-markovian rl. InThe Thirteenth
International Conference on Learning Representations. 7
[40] Ivanova, D. R., Foster, A., Kleinegesse, S., Gutmann, M. U., and Rainforth, T. (2021). Implicit
deep adaptive design: Policy-based experimental design without likelihoods.Advances in Neural
Information Processing Systems, 34. 2, 3, 4, 21
[41] Ivanova, D. R., Hedman, M., Guan, C., and Rainforth, T. (2024). Step-DAD: Semi-Amortized
Policy-Based Bayesian Experimental Design.ICLR 2024 Workshop on Data-centric Machine
Learning Research (DMLR). 2, 21
[42] Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D., Vinyals, O., and Teh,
Y . W. (2019). Attentive neural processes. InInternational Conference on Learning Representations.
3
[43] Kontsevich, L. L. and Tyler, C. W. (1999). Bayesian adaptive estimation of psychometric slope
and threshold.Vision research, 39(16):2729–2737. 24
[44] Krause, A., Guestrin, C., Gupta, A., and Kleinberg, J. (2006). Near-optimal sensor placements:
Maximizing information while minimizing communication cost. InProceedings of the 5th
international conference on Information processing in sensor networks, pages 2–10. 2
[45] Lee, H., Jang, C., Lee, D. B., and Lee, J. (2025). Dimension agnostic neural processes. InThe
Thirteenth International Conference on Learning Representations. 10
[46] Li, C.-Y ., Toussaint, M., Rakitsch, B., and Zimmer, C. (2025). Amortized safe active learning
for real-time decision-making: Pretrained neural policies from simulated nonparametric functions.
arXiv preprint arXiv:2501.15458. 2, 4
[47] Lindley, D. V . (1956). On a measure of the information provided by an experiment.The Annals
of Mathematical Statistics, 27(4):986–1005. 3
[48] Lookman, T., Balachandran, P. V ., Xue, D., and Yuan, R. (2019). Active learning in mate-
rials science with emphasis on adaptive sampling using uncertainties for targeted design.npj
Computational Materials, 5(1):21. 2
[49] Lueckmann, J.-M., Goncalves, P. J., Bassetto, G., Öcal, K., Nonnenmacher, M., and Macke,
J. H. (2017). Flexible statistical inference for mechanistic models of neural dynamics.Advances
in neural information processing systems, 30. 2, 3, 4
[50] Maraval, A., Zimmer, M., Grosnit, A., and Bou Ammar, H. (2024). End-to-end meta-bayesian
optimisation with transformer neural processes.Advances in Neural Information Processing
Systems, 36. 7, 8, 10
13
[51] Mittal, S., Bracher, N. L., Lajoie, G., Jaini, P., and Brubaker, M. (2025). Amortized in-context
bayesian posterior estimation.arXiv preprint arXiv:2502.06601. 4
[52] Müller, S., Hollmann, N., Arango, S. P., Grabocka, J., and Hutter, F. (2022). Transformers can
do bayesian inference. InInternational Conference on Learning Representations. 2, 3, 5
[53] Nguyen, T. and Grover, A. (2022). Transformer neural processes: Uncertainty-aware meta
learning via sequence modeling. InInternational Conference on Machine Learning, pages 16569–
16594. PMLR. 2, 3, 5, 7, 10
[54] Papamakarios, G. and Murray, I. (2016). Fast ε-free inference of simulation models with
bayesian conditional density estimation.Advances in neural information processing systems, 29.
2, 3, 4
[55] Pasek, J. and Krosnick, J. A. (2010). Optimizing survey questionnaire design in political science:
Insights from psychology. 1
[56] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V ., et al. (2011). Scikit-learn: Machine learning in python.
the Journal of machine Learning research, 12:2825–2830. 21, 27
[57] Powers, A. R., Mathys, C., and Corlett, P. R. (2017). Pavlovian conditioning–induced hal-
lucinations result from overweighting of perceptual priors.Science, 357(6351):596–600. 1,
9
[58] Prins, N. (2013). The psi-marginal adaptive method: How to give nuisance parameters the
attention they deserve (no more, no less).Journal of vision, 13(7):3–3. 2, 10, 24
[59] Radev, S. T., Schmitt, M., Pratz, V ., Picchini, U., Köthe, U., and Bürkner, P.-C. (2023a). Jana:
Jointly amortized neural approximation of complex bayesian models. InUncertainty in Artificial
Intelligence, pages 1695–1706. PMLR. 2, 3, 4
[60] Radev, S. T., Schmitt, M., Schumacher, L., Elsemüller, L., Pratz, V ., Schälte, Y ., Köthe, U., and
Bürkner, P.-C. (2023b). Bayesflow: Amortized bayesian workflows with neural networks.Journal
of Open Source Software, 8(89):5702. 2, 3, 4
[61] Rainforth, T., Foster, A., Ivanova, D. R., and Bickford Smith, F. (2024). Modern bayesian
experimental design.Statistical Science, 39(1):100–114. 1, 3
[62] Rasmussen, C. E. and Williams, C. K. (2006).Gaussian Processes for Machine Learning. MIT
Press. 8
[63] Ryan, E. G., Drovandi, C. C., McGree, J. M., and Pettitt, A. N. (2016). A review of modern
computational algorithms for bayesian optimal design.International Statistical Review, 84(1):128–
154. 3
[64] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy
optimization. InInternational conference on machine learning, pages 1889–1897. PMLR. 7, 10
[65] Shen, W., Dong, J., and Huan, X. (2025). Variational sequential optimal experimental de-
sign using reinforcement learning.Computer Methods in Applied Mechanics and Engineering,
444:118068. 2, 4, 6, 9, 22, 27
[66] Sheng, X. and Hu, Y .-H. (2004). Maximum likelihood multiple-source localization using
acoustic energy measurements with wireless sensor networks.IEEE transactions on signal
processing, 53(1):44–53. 9, 21
[67] Smith, F. B., Kirsch, A., Farquhar, S., Gal, Y ., Foster, A., and Rainforth, T. (2023). Prediction-
oriented bayesian active learning. InInternational Conference on Artificial Intelligence and
Statistics, pages 7331–7348. PMLR. 3, 4, 5, 7, 8, 17, 20
[68] Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y . (1999). Policy gradient methods for
reinforcement learning with function approximation.Advances in neural information processing
systems, 12. 6
14
[69] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and
Polosukhin, I. (2017). Attention is all you need.Advances in neural information processing
systems, 30. 2
[70] Watson, A. B. (2017). Quest+: A general multidimensional bayesian adaptive psychometric
method.Journal of Vision, 17(3):10–10. 1, 10, 24
[71] Whittle, G., Ziomek, J., Rawling, J., and Osborne, M. A. (2025). Distribution transformers: Fast
approximate bayesian inference with on-the-fly prior adaptation.arXiv preprint arXiv:2502.02463.
10
[72] Wichmann, F. A. and Hill, N. J. (2001). The psychometric function: I. Fitting, sampling, and
goodness of fit.Perception & Psychophysics, 63(8):1293–1313. 8, 9
[73] Xu, C., Hülsmeier, D., Buhl, M., and Kollmeier, B. (2024). How does inattention influence the
robustness and efficiency of adaptive procedures in the context of psychoacoustic assessments via
smartphone?Trends in Hearing, 28:23312165241288051. 9
[74] Yu, K., Bi, J., and Tresp, V . (2006). Active learning via transductive experimental design. In
Proceedings of the 23rd international conference on Machine learning, pages 1081–1088. 8, 20
[75] Zammit-Mangion, A., Sainsbury-Dale, M., and Huser, R. (2024). Neural methods for amortised
parameter inference.arXiv e-prints, pages arXiv–2404. 2, 4
[76] Zhang, X., Huang, D., Kaski, S., and Martinelli, J. (2025). Pabbo: Preferential amortized
black-box optimization. InThe Thirteenth International Conference on Learning Representations.
7, 10
15
Appendix
The appendix is organized as follows:
• In Section A, we provide detailed derivations and proofs for the theoretical claims made
regarding information gain and variational bounds.
• In Section B, we present the complete training algorithm and the specifics of the ALINE
model.
• In Section C, we provide comprehensive details for each experimental setup, including task
descriptions and baseline implementations.
• In Section D, we present additional experimental results, including further visualizations,
performance on more benchmarks, and analyses of inference times.
• In Section E, we provide an overview of the computational resources and software depen-
dencies for this work.
A Proofs of theoretical results
A.1 Derivation of total EIG forθ S
Following Eq. 3, we can write the expression for the total expected information gain sEIGθS about a
parameter subsetθ S ⊆θgiven dataD T generated under policyπ ψ as:
sEIGθS (ψ) =H[p(θ S)] +Ep(θS,DT |π ψ)[logp(θ S |DT )]
| {z }
E1
,(A1)
where p(θS) is the marginal prior for θS, and p(θS,DT |π ψ) is the joint distribution of θS and DT
under πψ. Now, let θR =θ\θ S be the remaining component of θ not included in θS. Then, we can
expressE 1 from Eq. A1 as
E1 =
Z
logp(θ S |DT )p(θS,DT |π ψ)dθS
=
Z
logp(θ S |DT )
Z
p(θS, θR,DT |π ψ)dθR

dθS
=
Z
logp(θ S |DT )
Z
p(θ,D T |π ψ)dθ
=E p(θ,DT |π ψ)[logp(θ S |DT )].(A2)
Plugging the above expression in Eq. A1 and noting that p(θ,D T |π ψ) =p(θ)p(D T |θ, πψ), we
arrive at the expression for sEIGθS in Eq. 7.
A.2 Proof of Proposition 1
Proposition(Proposition 1).The total expected predictive information gain for a design policy πψ
over a data trajectory of lengthTis:
sEPIG(ψ) :=E p⋆(x⋆)p(DT |π ψ)[H[p(y⋆ |x ⋆)]−H[p(y ⋆ |x ⋆,DT )]]
=E p(θ)p(DT |π ψ,θ)p⋆(x⋆)p(y⋆ |x ⋆,θ) [logp(y ⋆ |x ⋆,DT )] +Ep⋆(x⋆)[H[p(y⋆ |x ⋆)]].
Proof. Let p⋆(x⋆) be the target distribution over inputs x⋆ for which we want to improve predictive
performance. Let y⋆ be the corresponding target output. The single-step EPIG for acquiring data(x, y)
measures the expected reduction in uncertainty (entropy) about y⋆ for a random target x⋆ ∼p ⋆(x⋆):
EPIG(x) =E p⋆(x⋆)p(y|x) [H[p(y⋆ |x ⋆)]−H[p(y ⋆ |x ⋆, x, y)]].
Following Theorem 1 in [23], the total EPIG, is the total expected reduction in predictive entropy
from the initial predictionp(y ⋆ |x ⋆)to the final prediction based on the full historyp(y ⋆ |x ⋆,DT ):
sEPIG(ψ) =E p⋆(x⋆)p(DT |π ψ)[H[p(y⋆ |x ⋆)]−H[p(y ⋆ |x ⋆,DT )]](A3)
=E p⋆(x⋆)p(DT |π ψ)[Ep(y⋆ |x ⋆,DT )[logp(y ⋆ |x ⋆,DT )]] +Ep⋆(x⋆)[H[p(y⋆ |x ⋆)]](A4)
=E p⋆(x⋆)p(DT ,y⋆ |π ψ,x⋆)[logp(y ⋆ |x ⋆,DT )] +Ep⋆(x⋆)[H[p(y⋆ |x ⋆)]].(A5)
16
Here, Eq. A3 follows from conditioning EPIG on the entire trajectory DT instead of a single data
point y, Eq. A4 follows from the definition of entropy H[·], and Eq. A5 follows from noting that
p(DT , y⋆ |π ψ, x⋆) =p(D T |π ψ)p(y⋆ |x ⋆,DT ). Next, we combine the expectations and express the
joint distribution p(DT , y⋆ |π ψ, x⋆) =
R
p(θ)p(DT |π ψ, θ)p(y⋆ |x ⋆, θ)dθ, where, following [ 67],
we assume conditional independence betweenD T andy ⋆ givenθ. This yields:
sEPIG(ψ) =E p(θ)p(DT |π ψ,θ)p⋆(x⋆)p(y⋆ |x ⋆,θ) [logp(y ⋆ |x ⋆,DT )] +Ep⋆(x⋆)[H[p(y⋆ |x ⋆)]],
which completes our proof.
A.3 Proof of Proposition 2
Proposition(Proposition 2).Let the policy πψ generate the trajectory DT . With qϕ(θS |DT )
approximating p(θS |DT ), and qϕ(y⋆ |x ⋆,DT ) approximating p(y⋆ |x ⋆,DT ), we have J θ
S (ψ)≤
sEIGθS (ψ)andJ y⋆
p⋆ (ψ)≤sEPIG(ψ). Moreover,
sEIGθS (ψ)− Jθ
S (ψ) =E p(DT |π ψ)[KL(p(θS |DT )||qϕ(θS |DT ))],and(A6)
sEPIG(ψ)− Jy⋆
p⋆ (ψ) =E p⋆(x⋆)p(DT |π ψ)[KL(p(y⋆ |x ⋆,DT )||qϕ(y⋆ |x ⋆,DT ))].(A7)
Proof. Using the expressions for sEIGθS and J θ
S from Eq. 7 and Eq. 8, respectively, and noting that
logq ϕ(θS |DT ) = P
l∈S logq ϕ(θl |DT ), we can write the expression for sEIGθS (ψ)− Jθ
S (ψ)as:
sEIGθS (ψ)− Jθ
S (ψ) =E p(θ)p(DT |π ψ,θ)[logp(θ S |DT )]−E p(θ)p(DT |π ψ,θ)[logq ϕ(θS |DT )]
=E p(DT ,θ|π ψ)

log p(θS |DT )
qϕ(θS |DT )

(A8)
=E p(DT ,θS |π ψ)

log p(θS |DT )
qϕ(θS |DT )

(A9)
=E p(DT |π ψ)

Ep(θS |DT )

log p(θS |DT )
qϕ(θS |DT )

(A10)
=E p(DT |π ψ)[KL(p(θS |DT )||qϕ(θS |DT ))].(A11)
Here, Eq. A8 follows from the fact p(θ)p(DT |π ψ, θ) =p(DT , θ|πψ), Eq. A9 follows from Eq. A2,
Eq. A10 follows from the fact that p(DT , θS |π ψ) =p(D T |π ψ)p(θS |DT ), and Eq. A11 follows
from the definition of KL divergence.
Since the KL divergence is always non-negative (KL(P||Q)≥0 ), its expectation over trajectories
p(DT |π ψ)must also be non-negative. Therefore:
J θ
S (ψ)≤sEIG θS (ψ).(A12)
Now, we consider the difference between sEPIG(ψ)andJ y⋆
p⋆ (ψ):
sEPIG(ψ)− Jy⋆
p⋆ (ψ) =E p(θ)p(DT |π ψ,θ)p⋆(x⋆)p(y⋆ |x ⋆,θ)

log p(y⋆ |x ⋆,DT )
qϕ(y⋆ |x ⋆,DT )

=E p⋆(x⋆)p(DT |π ψ)

Ep(y⋆ |x ⋆,DT )

log p(y⋆ |x ⋆,DT )
qϕ(y⋆ |x ⋆,DT )

.
(A13)
Similar to the previous case, the inner expectation is the definition of the KL divergence between the
true posterior predictivep(y ⋆ |x ⋆,DT )and the variational approximationq ϕ(y⋆ |x ⋆,DT ):
sEPIG(ψ)− Jy⋆
p⋆ (ψ) =E p⋆(x⋆)p(DT |π ψ)[KL(p(y⋆ |x ⋆,DT )||qϕ(y⋆ |x ⋆,DT ))].(A14)
Since the KL divergence is always non-negative, therefore:
J y⋆
p⋆ (ψ)≤sEPIG(ψ),(A15)
which completes the proof.
17
B Further details on ALINE
B.1 Training algorithm
Algorithm 1ALINETraining Procedure
1: Input:Prior p(θ), likelihood p(y|x, θ), target distribution p(ξ), query horizon T, total training
episodesE max, warm-up episodesE warm.
2:Output:Trained ALINEmodel (q ϕ, πψ).
3:forepoch = 1 toE max do
4:Sample parametersθ∼p(θ).
5:Sample target specifier setξ∼p(ξ)and corresponding targetsθ S or{x ⋆
m, y⋆
m}M
m=1.
6:Initialize candidate query setQ.
7:fort= 1toTdo
8:ifepoch≤E warm then
9:Select next queryx t uniformly at random fromQ.
10:else
11:Select next queryx t ∼π ψ(·|D t−1, ξ)fromQ.
12:end if
13:Sample outcomey t ∼p(y|x t, θ).
14:Update historyD t ← Dt−1 ∪ {(xt, yt)}.
15:Update query setQ ← Q \ {x t}.
16:ifepoch≤E warm then
17:L=L NLL (Eq. 12)
18:else
19:Calculate rewardR t (Eq. 10)
20:L=L NLL (Eq. 12)+L PG (Eq. 11)
21:end if
22:Update ALINEusingL.
23:end for
24:end for
B.2 Architecture and training details
In ALINE, the data is first processed by different embedding layers. Inputs (context xi, query
candidates xq
n, target locations x⋆
k) are passed through a shared nonlinear embedder fx. Observed
outcomes yi are embedded using a separate embedder fy. For discrete parameters, we assign a
unique indicator ℓl to each parameter θl, which is then associated with a unique, learnable embedding
vector, denoted as fθ(ℓl). We compute the final context embedding by summing the outputs of
the respective embedders: EDt ={(f x(xi) +f y(yi))}t
i=1. Query and target sets are embedded as
EQ ={(f x(xq
n))}N
n=1 and ET (either {(fx(x⋆
m))}M
m=1 or {fθ(ℓl)}l∈S). Both fx and fy are MLPs
consisting of an initial linear layer, followed by a ReLU activation function, and a final linear layer.
For all our experiments, the embedders use a feedforward dimension of 128 and project inputs to an
embedding dimension of 32.
The core of our architecture is a transformer network. We employ a configuration with 3 transformer
layers, each equipped with 4 attention heads. The feedforward networks within each transformer
layer have a dimension of 128. The model’s internal embedding dimension, consistent across
the transformer layers and the output of the initial embedding layers, is 32. These transformer
layers process the embedded representations of the context, query, and target sets. The interactions
between these sets are governed by specific attention masks, visually detailed in Figure A1, where a
shaded element indicates that the token corresponding to its row is permitted to attend to the token
corresponding to its column.
ALINEhas two specialized output heads. The inference head, responsible for approximating posteriors
and posterior predictive distributions, parameterizes a Gaussian Mixture Model (GMM) with 10
components. The embeddings corresponding to the inference targets are processed by 10 separate
MLPs, one for each GMM component. Each MLP outputs parameters for its component: a mixture
weight, a mean, and a standard deviation. The standard deviations are passed through a Softplus
18
...
...
...
...
...
...
......
( a ) (b )
... ...
Figure A1: Example attention masks in ALINE’s transformer architecture. (a) Mask for a predictive
targetξ=ξ y⋆
p⋆ (b) Mask for a parameter targetξ=ξ θ
{1}. Shaded squares indicate allowed attention.
activation function to ensure positivity, and the mixture weights are normalized using a Softmax
function. The policy head, which generates a probability distribution over the candidate query points,
is a 2-layer MLP with a feedforward dimension of 128. Its output is passed through a Softmax
function to ensure that the probabilities of all actions sum to unity. The architecture of ALINEis
shown in Figure 2.
ALINEis trained using the AdamW optimizer with a weight decay of 0.01. The initial learning rate is
set to 0.001 and decays according to a cosine annealing schedule.
C Experimental details
This section provides details for the experimental setups. Section C.1 outlines the specifics for the
active learning experiments in Section 4.1, including the synthetic function sampling procedures
(Section C.1.1), implementation details for baseline methods (Section C.1.2), and training and
evaluation details for these tasks (Section C.1.3). Next, in Section C.2 we describe the details of BED
tasks, including the task descriptions (Section C.2.1), implementation of the baselines (Section C.2.2),
and the training and evaluation details (Section C.2.3). Lastly, Section C.3 contains the specifics of
the psychometric modeling experiments, detailing the psychometric function we use (Section C.3.1)
and the setup for the experimental comparisons (Section C.3.2).
C.1 Active learning for regression and hyperparameter inference
C.1.1 Synthetic functions sampling procedure
For active learning tasks, ALINEis trained exclusively on synthetically generated Gaussian Process
(GP) functions. The procedure for generating these functions is as follows. First, the hyperparameters
of the GP kernels, namely the output scale and lengthscale(s), are sampled from their respective prior
distributions. For multi-dimensional input spaces (dx >1 ), there is a piso = 0.5 probability that an
isotropic kernel is used, meaning that all input dimensions share a common lengthscale. Otherwise,
an anisotropic kernel is employed, with a distinct lengthscale sampled for each input dimension.
Subsequently, a kernel function is chosen randomly from a pre-defined set, with each kernel having a
uniform probability of selection. In our experiments, we utilize the Radial Basis Function (RBF),
Matérn 3/2, and Matérn 5/2 kernels.
The kernel’s output scale is sampled uniformly from the intervalU(0.1,1) . The lengthscale(s) are
sampled from U(0.1,2)× √dx. Input data points x are sampled uniformly within the range [−5,5]
for each dimension. Finally, Gaussian noise with a fixed standard deviation of 0.01 is added to
19
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 1: ls = 1.20, scale = 0.33
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 2: ls = 1.51, scale = 0.29
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 3: ls = 1.92, scale = 0.69
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 4: ls = 0.38, scale = 0.81
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 5: ls = 0.34, scale = 0.18
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 6: ls = 1.01, scale = 0.68
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 7: ls = 0.86, scale = 0.99
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 8: ls = 0.65, scale = 0.44
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 9: ls = 0.78, scale = 0.74
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 10: ls = 1.69, scale = 0.70
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 11: ls = 1.95, scale = 0.53
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 12: ls = 0.51, scale = 0.89
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 13: ls = 0.61, scale = 0.34
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 14: ls = 0.54, scale = 0.51
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 15: ls = 0.88, scale = 0.41
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 16: ls = 0.24, scale = 0.46
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 17: ls = 1.02, scale = 0.17
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 18: ls = 1.19, scale = 0.54
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 19: ls = 0.16, scale = 0.72
−5.0 −2.5 0.0 2.5 5.0
−2
0
2
Sample 20: ls = 1.62, scale = 0.66
Figure A2: Examples of randomly sampled 1D synthetic GP functions used to train ALINE.
the true function output y for each sampled data point. Figure A2 illustrates some examples of the
synthetic GP functions generated using this procedure.
C.1.2 Details of acquisition functions
We compare ALINEwith four commonly used AL acquisition functions. ForRandom Sampling
(RS), we randomly select one point from the candidate pool as the next query point.
Uncertainty Sampling (US)is a simple and widely used AL acquisition strategy that prioritizes
points where the model is most uncertain about its prediction:
US(x) =
p
V[y|x,D],(A16)
whereV[y|x,D]is the predictive variance atxgiven the current training dataD.
Variance Reduction (VR) [74]aims to select a candidate point that is expected to maximally reduce
the predictive variance over a pre-defined test set{x ⋆
m}M
m=1, which is defined as:
VR(x) =
PM
m=1 (Covpost(x⋆, x))2
V[y|x,D] ,(A17)
Covpost(x⋆, x)is the posterior covariance between the latent function values at x⋆ and x, given the
history D={(X train, ytrain)}, where Xtrain comprises all currently observed inputs with ytrain being
their corresponding outputs. It is computed as:
Covpost(x⋆, x) =k(x⋆, x)−k(x⋆, Xtrain)(Ktrain +αI) −1k(Xtrain, x).(A18)
Here,k(·,·)is the GP kernel function,K train =k(X train, Xtrain), andαis the noise variance.
Expected Predictive Information Gain (EPIG) [67]measures the expected reduction in predictive
uncertainty on a target input distributionp⋆(x⋆). Following Smith et al.[67], for a Gaussian predictive
distribution, the EPIG for a candidate point can be expressed as:
EPIG(x) =E p⋆(x⋆)[1
2 log V[y|x,D]V[y ⋆ |x ⋆,D]
V[y|x,D]V[y ⋆ |x ⋆,D]−Cov post(x⋆, x)2 ].(A19)
20
In practice, we approximate it by averaging overmsampled test points:
EPIG(x)≈ 1
2M
MX
m=1
log V[y|x,D]V[y ⋆
m |x ⋆
m,D]
V[y|x,D]V[y ⋆m |x ⋆m,D]−Cov post(x⋆m, x)2 .(A20)
C.1.3 Training and evaluation details
For both 1D and 2D input scenarios, ALINEis trained for 2·10 5 epochs using a batch size of 200.
The discount factor γ for the policy gradient loss is set to 1. For the GP-based baselines, we utilized
Gaussian Process Regressors implemented via the scikit-learn library [56]. The hyperparameters
of the GP models are optimized at each step. For the ACE baseline [ 15], we use a transformer
architecture and an inference head design consistent with our ALINEmodel.
All active learning experiments are evaluated with a candidate query pool consisting of 500 points.
Each experimental run commenced with an initial context set consisting of a single data point. The
target set size for predictive tasks is set to 100.
C.2 Benchmarking on Bayesian experimental design tasks
C.2.1 Task descriptions
Location Finding[ 66] is a benchmark problem commonly used in BED literature [24, 40, 9, 41]. The
objective is to infer the unknown positions of K hidden sources, θ={θ k ∈R d}K
k=1, by strategically
selecting a sequence of observation locations, x∈R d. Each source emits a signal whose intensity
attenuates with distance following an inverse-square law. The total signal intensity at an observation
locationxis given by the superposition of signals from all sources:
µ(θ, x) =b+
KX
k=1
αk
m+∥θ k −x∥ 2 ,(A21)
where αk are known source strength constants, andb, m >0are constants controlling the background
level and maximum signal intensity, respectively. In this experiment, we useK= 1 , d= 2 , αk = 1,
b= 0.1 and m= 10 −4, and the prior distribution over each component of a source’s location
θk = (θk,1, ..., θk,d)is uniform over the interval[0,1].
The observation is modeled as the log-transformed total intensity corrupted by Gaussian noise:
logy|θ, x∼ N(logµ(θ, x), σ2),(A22)
where we useσ= 0.5in our experiments.
Constant Elasticity of Substitution (CES)[ 3] considers a behavioral economics problem in which
a participant compares two baskets of goods and rates the subjective difference in utility between the
baskets on a sliding scale from 0 to 1. The utility of a basket z, consisting of K goods with different
values, is characterized by latent parameters θ= (ρ,α, u). The design problem is to select pairs of
baskets,x= (z, z′)∈[0,100] 2K, to infer the participant’s latent utility parameters.
The utility of a basketzis defined using the constant elasticity of substitution function, as:
U(z) =
 KX
i=1
zρ
i αi
!1
ρ
.(A23)
The prior of the latent parameters is specified as:
ρ∼Beta(1,1)
α∼Dirichlet(1 K)
logu∼ N(1,32).
(A24)
The subjective utility difference between two baskets is modeled as follows:
η∼ N

u·(U(z)−U(z ′)), u2 ·τ 2 ·(1 +∥z−z ′∥)2
y= clip(sigmoid(η), ϵ,1−ϵ).
(A25)
In this experiment, we chooseK= 3,τ= 0.005andϵ= 2 −22.
21
C.2.2 Implementation details of baselines
We compare ALINEwith four baseline methods. ForRandom Designpolicy, we randomly sample a
design from the design space using a uniform distribution.
VPCE[ 25] iteratively infers the posterior through variational inference and maximizes the myopic
Prior Contrastive Estimation (PCE) lower bound by gradient descent with respect to the experimental
design. The hyperparameters used in the experiments are given in Table A1.
Table A1: Additional hyperparameters used in VPCE [25].
Parameter Location Finding CES
VI gradient steps 1000 1000
VI learning rate10 −3 10−3
Design gradient steps 2500 2500
Design learning rate10 −3 10−3
Contrastive samplesL500 10
Expectation samples 500 10
Deep Adaptive Design (DAD)[ 23] learns an amortized design policy guided by sPCE lower bound.
For a design policy π, and L≥0 contrastive samples, sPCE over a sequence of T experiments is
defined as:
LT (π, L) =Ep(θ0,DT |π)p(θ1:L)
"
log p(DT |θ 0, π)
1
L+1
PL
ℓ=0 p(DT |θ ℓ, π)
#
,(A26)
where the contrastive samples θ1:L are drawn independently from the prior p(θ). The bound becomes
tight asL→ ∞, with a convergence rate ofO(L−1).
The design network comprises an MLP encoder that encodes historical data into a fixed-dimensional
representation, and an MLP emitter that proposes the next design point. The encoder processes the
concatenated design-observation pairs from history and aggregates their representations through a
pooling operation.
Following the work of Foster et al. [23], the encoder network consists of two fully connected layers
with 128 and 16 units with ReLU activation applied to the hidden layer. The emitter is implemented
as a fully connected layer that maps the pooled representation to the design space. The policy is
trained using the Adam optimizer with an initial learning rate of 5·10 −5, β= (0.8,0.998) , and an
exponentially decaying learning rate, reduced by a factor of γ= 0.98 every 1000 epochs. In the
Location Finding task, the model is trained for 105 epochs, and 104 contrastive samples are utilized
in each training step for the estimation of the sPCE lower bound. Note that, for the CES task, we
applied several adjustments, including normalizing the input, applying the Sigmoid and Softplus
transformations to the output before mapping it to the design space, increasing the depth of the
network, and initializing weights using Xavier initialization [32]. However, DAD failed to converge
during training in our experiments. Therefore, we report the results provided by Blau et al. [9].
vsOED[ 65] is an amortized BED method that employs an actor-critic reinforcement learning
framework. It utilizes separate networks for the actor (policy), the critic (value function), and the
variational posterior approximation. The reward signal for training the policy is the incremental
improvement in the log-probability of the ground-truth parameters under the variational posterior,
which is estimated at each step of the experiment. Following the original implementation, a distinct
posterior network is trained for each design stage, while the actor and critic share a common backbone.
For our implementation, the hidden layers of all networks are 3-layer MLPs with 256 units and
ReLU activations. The posterior network outputs the parameters for an 8-component Gaussian
Mixture Model (GMM). The input to the actor and critic networks is the zero-padded history of
design-observation pairs, concatenated with a one-hot encoding of the current time step. We train for
104 epochs with a batch size of 104 and a 106-sized replay buffer. The learning rate starts at 10−3
with a 0.9999 exponential decay per epoch, and the discount factor is 0.9. To encourage exploration
for the deterministic policy, Gaussian noise is added during training; the initial noise scale is 0.5
22
for the Location Finding task and 5.0 for the CES task, with decay rates of 0.9999 and 0.9998,
respectively.
RL-BOED[ 9] frames the design policy optimization as a Markov Decision Process (MDP) and
employs reinforcement learning to learn the design policy. It utilizes a stepwise reward function to
estimate the marginal contribution of thet-th experiment to the sEIG.
The design network shares a similar backbone architecture to that of DAD, with the exception that
the deterministic output of the emitter is replaced by a Tanh-Gaussian distribution. The encoder
comprises two fully connected layers with 128 units and ReLU activation, followed by an output
layer with 64 units and no activation. Training is conducted using Randomized Ensembled Double
Q-learning (REDQ) [16], the full configurations are reported in Table A2.
Table A2: Additional hyperparameters used in RL-BOED [9].
Parameter Location Finding CES
Critics 2 2
Random subsets 2 2
Contrastive samplesL10 5 105
Training epochs10 5 5·10 4
Discount factorγ0.9 0.9
Target update rate10 −3 5·10 −3
Policy learning rate10 −4 3·10 −4
Critic learning rate3·10 −4 3·10 −4
Buffer size10 7 106
C.2.3 Training and evaluation details
In the Location Finding task, the number of sequential design steps, T, is set to 30. For all evaluated
methods, the sPCE lower bound is estimated using L= 10 6 contrastive samples. The ALINEis
trained over 105 epochs with a batch size of 200. The discount factor γ for the policy gradient loss
is set to 1. During the evaluation phase, the query set consists of 2000 points, which are drawn
uniformly from the defined design space. For the CES task, each experimental run consists ofT= 10
design steps. The sPCE lower bound is estimated using L= 10 7 contrastive samples. The ALINEis
trained for2·10 5 epochs with a batch size of 200, and we use 2000 for the query set size.
C.3 Psychometric model
C.3.1 Model description
In this experiment, we use a four-parameter psychometric function with the following parameteriza-
tion:
π(x) =θ 3 ·θ 4 + (1−θ 4)F( x−θ 1
θ2
),
where:
•θ 1 (threshold): The stimulus intensity at which the probability of a positive response reaches
a specific criterion. It represents the location of the psychometric curve. We use a uniform
priorU[−3,3]forθ 1.
•θ 2 (slope): Describes the steepness of the psychometric function. Smaller values of θ2
indicate a sharper transition, reflecting higher sensitivity around the threshold. We use a
uniform priorU[0.1,2]forθ 2.
•θ 3 (guess rate): The baseline response probability for stimuli far below the threshold,
reflecting responses made by guessing. We use a uniform priorU[0.1,0.9]forθ 3.
•θ 4 (lapse rate): The rate at which the observer makes errors independent of stimulus intensity,
representing an upper asymptote on performance below 1. We use a uniform prior U[0,0.5]
forθ 4.
23
We employ a Gumbel-type internal link function F= 1−e −10z
where z= x−θ1
θ2
. Lastly, a binary
response y is simulated from the psychometric function π(x) using a Bernoulli distribution with
probability of successp=π(x).
C.3.2 Experimental details
We compare ALINEagainst two established Bayesian adaptive methods:
• QUEST+ [70]: QUEST+ is an adaptive psychometric procedure that aims to find the stimulus
that maximizes the expected information gain about the parameters of the psychometric
function, or equivalently, minimizes the expected entropy of the posterior distribution
over the parameters. It typically operates on a discrete grid of possible parameter values
and selects stimuli to reduce uncertainty over this entire joint parameter space. In our
experiments, QUEST+ is configured to infer all four parameters simultaneously.
• Psi-marginal [58]: The Psi-marginal method is an extension of the psi method [ 43] that
allows for efficient inference by marginalizing over nuisance parameters. When specific
parameters are designated as targets of interest, Psi-marginal optimizes stimulus selection to
maximize information gain specifically for these target parameters, effectively treating the
others as nuisance variables. This makes it highly efficient when only a subset of parameters
is critical.
For each simulated experiment, true underlying parameters are sampled from their prior distributions.
Stimulus values x are selected from a discrete set of size 200 drawn uniformly from the range [−5,5] .
D Additional experimental results
D.1 Active Exploration of High-Dimensional Hyperparameter Landscapes
To demonstrate ALINE’s utility on complex, high-dimensional tasks, we conduct a new set of
experiments on actively exploring hyperparameter performance landscapes. This experiment aims
to efficiently characterize a machine learning model’s overall behavior on a new task, allowing
practitioners to quickly assess a model family’s viability or understand its sensitivities. The task
is to actively query a small number of hyperparameter configurations to build a surrogate model
that accurately predicts performance for a larger, held-out set of target configurations. We use
high-dimensional, real-world tasks from the HPO-B benchmark [2], evaluating on rpart (6D), svm
(8D), ranger (9D), and xgboost (16D) datasets. ALINEis trained on their multiple pre-defined
training sets. We then evaluate its performance, alongside non-amortized GP-based baselines and an
amortized surrogate baseline (ACE-US), on the benchmark’s held-out and entirely unseen test sets.
Table A3 shows the RMSE results after 30 steps, averaged across all test tasks for each dataset. First,
both amortized methods, ALINEand ACE-US, significantly outperform all non-amortized GP-based
baselines across all tasks. This highlights the power of meta-learning in this domain. GP-based
methods must learn each new performance landscape from scratch, which is highly inefficient in high
dimensions. In contrast, both ALINEand ACE-US are pre-trained on hundreds of related tasks, and
their Transformer architectures meta-learn the structural patterns common to these landscapes. This
shared prior knowledge allows them to make far more accurate predictions from sparse data. Second,
while ACE-US performs strongly due to its amortized nature, ALINEconsistently achieves the best or
joint-best performance. This demonstrates the additional, crucial benefit of our core contribution: the
learned acquisition policy. ACE-US relies on a standard heuristic, whereas ALINE’s policy is trained
end-to-end to learn how to optimally explore the landscape, leading to more informative queries and
ultimately a more accurate final surrogate model.
D.2 Active learning for regression and hyperparameter inference
AL results on more benchmark functions.To further assess ALINE, we present performance
evaluations on an additional set of active learning benchmark functions, see Figure A3. The results on
Gramacy and Branin show that we are on par with the GP baselines. For the Three Hump Camel, we
see both ALINEand ACE-US showing reduced accuracy. This is because the function’s output value
range extends beyond that of the GP functions used during pre-training. This highlights a potential
24
Table A3: RMSE (↓) on the HPO-B benchmark after 30 active queries. Results show the mean and
95% CI over all test tasks for each dataset.
GP-RS GP-US GP-VR GP-EPIG ACE-US ALINE(ours)
rpart (6D)0.07±0.03 0.04±0.02 0.04±0.02 0.05±0.020.01±0.000.01±0.00
svm (8D)0.22±0.11 0.11±0.05 0.12±0.07 0.15±0.08 0.04±0.010.03±0.01
ranger (9D)0.10±0.02 0.07±0.01 0.08±0.02 0.08±0.020.02±0.010.02±0.01
xgboost (16D)0.09±0.02 0.09±0.02 0.09±0.02 0.09±0.02 0.04±0.010.03±0.01
5 10 15 20 25 30
0.0
0.1
0.2
0.3
0.4
0.5RMSE
Gramacy 1D
10 20 30 40 50
0.0
0.2
0.4
0.6
0.8
Branin 2D
10 20 30 40 50
0.0
0.5
1.0
1.5
2.0
2.5
Three Hump Camel 2D
GP-RS GP-US GP-VR GP-EPIG ACE-US ALINE (ours)
Figure A3: Predictive performance in terms of RMSE on three other active learning benchmark
functions. Results show the mean and 95% confidence interval (CI) across 100 runs. Notably, on
the Three Hump Camel function, the performance of amortized methods like ALINEand ACE-US
is limited, as its output scale significantly differs from the pre-training distribution, highlighting a
scenario of distribution shift.
area for future work, such as training ALINEon a broader prior distribution of functions, potentially
leading to more universally capable models.
Acquisition visualization for AL.To qualitatively understand the behavior of our model, we
visualize the query strategy employed by ALINEfor AL on a randomly sampled synthetic function
Figure A4. This visualization illustrates how ALINEiteratively selects query points to reduce
uncertainty and refine its predictive posterior.
Hyperparameter inference visualization.We now visualize the evolution of ALINE’s estimated
posterior distributions for the underlying GP hyperparameters for a randomly drawn 2D synthetic
GP function (see Figure A5). The posteriors are shown after 1, 15, and 30 active data acquisition
steps. As ALINEstrategically queries more informative data points, its posterior beliefs about these
generative parameters become increasingly concentrated and accurate.
Inference time.To assess the computational efficiency of ALINE, we report the inference times for
the AL tasks in Table A4. The times represent the total duration to complete a sequence of 30 steps
for 1D functions and 50 steps for 2D functions, averaged over 10 independent runs. As both ALINE
and ACE-US perform inference via a single forward pass per step once trained, they are significantly
faster compared to traditional GP-based methods.
Table A4: Comparison of inference times (seconds) for different AL methods on 1D (30 steps) and
2D (50 steps) tasks. Values are averaged over 10 runs (mean±standard deviation).
Methods Inference time (s)
1D & 30 steps 2D & 50 steps
GP-US 0.62±0.09 1.72±0.23
GP-VR 1.41±0.14 4.03±0.18
GP-EPIG 1.34±0.11 3.43±0.24
ACE-US 0.08±0.00 0.19±0.02
ALINE0.08±0.00 0.19±0.02
25
4
 2
 0 2 4
1
0
1
y
Step 1
Prediction
Ground Truth
Targets
Context
Next Query
4
 2
 0 2 4
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Step 2
4
 2
 0 2 4
1.5
1.0
0.5
0.0
0.5
1.0
1.5
Step 3
4
 2
 0 2 4
1
0
1
2
Step 4
4
 2
 0 2 4
1
0
1
Step 5
4
 2
 0 2 4
1
0
1
2
y
Step 6
4
 2
 0 2 4
1.0
0.5
0.0
0.5
1.0
1.5
Step 7
4
 2
 0 2 4
1.0
0.5
0.0
0.5
1.0
1.5
Step 8
4
 2
 0 2 4
1.0
0.5
0.0
0.5
1.0
Step 9
4
 2
 0 2 4
0.5
0.0
0.5
1.0
Step 10
4
 2
 0 2 4
0.5
0.0
0.5
1.0
y
Step 11
4
 2
 0 2 4
0.5
0.0
0.5
1.0
Step 12
4
 2
 0 2 4
0.5
0.0
0.5
1.0
Step 13
4
 2
 0 2 4
0.5
0.0
0.5
1.0
Step 14
4
 2
 0 2 4
0.5
0.0
0.5
1.0
Step 15
4
 2
 0 2 4
x
0.5
0.0
0.5
1.0
y
Step 16
4
 2
 0 2 4
x
0.5
0.0
0.5
1.0
Step 17
4
 2
 0 2 4
x
0.5
0.0
0.5
1.0
Step 18
4
 2
 0 2 4
x
0.5
0.0
0.5
1.0
Step 19
4
 2
 0 2 4
x
0.5
0.0
0.5
1.0
Step 20
Figure A4: Sequential query strategy of ALINEon a 1D synthetic GP function over 20 steps. As
more points are queried, the model’s prediction increasingly aligns with the ground truth, and the
uncertainty is strategically reduced.
D.3 Benchmarking on Bayesian experimental design tasks
In this section, we provide additional qualitative results for ALINE’s performance on the BED
benchmark tasks. Specifically, for the Location Finding task, we visualize the sequence of designs
chosen by ALINEand the resulting posterior distribution over the hidden source’s location (Figure A6).
For the CES task, we present the estimated marginal posterior distributions for the model parameters,
comparing them against their true underlying values (Figure A7). We see that ALINEoffers accurate
parameter inference.
D.4 Psychometric model
Demonstrations of flexibility.We conduct two ablations to explicitly validate ALINE’s flexible
targeting capabilities.
First, we test the ability to switch targets mid-rollout. We configure a single experiment where for the
first 15 steps, the target isthreshold & slopeparameters, and at step 16, the target is switched to the
guess rate & lapse rate. As shown in Figure A8(a), ALINE’s acquisition strategy adapts immediately
and correctly, shifting its queries from the decision threshold region to the extremes of the stimulus
range to gain maximal information about the new targets.
Second, we test generalization to novel target combinations. A single ALINEmodel is trained
to handle two distinct targets separately: (1)threshold & slopeand (2)guess & lapse rate. At
deployment, we task this model with a novel, unseen combination: targeting all four parameters
simultaneously. As shown in Figure A8(b), the resulting policy is a sensible mixture of the two
underlying strategies it has learned, strategically alternating queries between points near the decision
threshold and points at the extremes. This confirms that ALINEcan successfully compose its learned
strategies to generalize to new inference goals at runtime.
Inference time.We additionally assess the computational efficiency of each method in proposing
the next design point. The average per-step design proposal time, measured over the 30-step
psychometric experiments across 20 runs, is 0.002±0.00 s for ALINE, 0.07±0.00 s for QUEST+,
and 0.02±0.00 s for Psi-marginal. Methods like QUEST+ and Psi-marginal, which often rely on
26
t = 1 t = 15
 t = 30
Figure A5: Estimated posteriors for the two lengthscales and the output scale obtained from ALINE
after t= 1 , t= 15 , and t= 30 active query steps. The posteriors progressively concentrate around
the true parameter values as more data is acquired.
grid-based posterior estimation, face rapidly increasing computational costs as the parameter space
dimensionality or required grid resolution grows. ALINE, however, estimates the posterior via the
transformer in a single forward pass, making its inference time largely insensitive to these factors.
Thus, this computational efficiency gap is anticipated to become even more pronounced for more
complex psychometric models.
E Computational resources and software
All experiments presented in this work, encompassing model development, hyperparameter opti-
mization, baseline evaluations, and preliminary analyses, are performed on a GPU cluster equipped
with AMD MI250X GPUs. The total computational resources consumed for this research, in-
cluding all development stages and experimental runs, are estimated to be approximately 5000
GPU hours. For each experiment, it takes around 20 hours to train an ALINEmodel for 105
epochs. The core code base is built using Pytorch ( https://pytorch.org/, License: modi-
fied BSD license). For the Gaussian Process (GP) based baselines, we utilize Scikit-learn [ 56]
(https://scikit-learn.org/, License: modified BSD license). The DAD baseline is adapted
from the original authors’ publicly available code [23] (https://github.com/ae-foster/dad;
MIT License). Our implementations of the RL-BOED and vsOED baselines are adapted from the of-
ficial repositories provided by [6] (https://github.com/yasirbarlas/RL-BOED; MIT License)
and [65] (https://github.com/wgshen/vsOED; MIT License), respectively. We use questplus
package (https://github.com/hoechenberger/questplus, License: GPL-3.0) to implement
QUEST+, and usePsi-staircase(https://github.com/NNiehof/Psi-staircase, License:
GPL-3.0) to implement the Psi-marginal method.
27
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Location Finding
xt
0
5
10
15
20
25
30
Time step  t
1800
1500
1200
900
600
300
0
Posterior  logq(|T)
Figure A6: Visualization of ALINE’s design policy and resulting posterior for the Location Finding
task. The contour plot shows the log posterior probability density of the source location θ (true
location marked by blue star) after T= 30 steps. Queried locations, with color indicating the time
step of acquisition, demonstrating a concentration of queries around the true source.
0.0 0.5 1.0
0
5
10
0.0 0.5 1.0
0
20
40
-5.0 0.0 5.0
log(u)
0
1
2
u
p( )
Figure A7: ALINE’s estimated marginal posterior distributions for the parameters of the CES task
after T= 10 query steps. The dashed red lines indicate the true parameter values. The posteriors are
well-concentrated around the true values, demonstrating accurate parameter inference.
10 20 30
Number of Steps t
−5
0
5Stimuli Values
(a)
10 20 30
Number of Steps t
−5
0
5
(b)
Figure A8: Demonstration of ALINE’s runtime flexibility on the psychometric task. (a) The acqui-
sition strategy adapts after the inference target is switched mid-rollout from (threshold & slope) to
(guess & lapse rate). (b) When tasked with a novel combined target (all four parameters), the policy
generalizes by mixing the two distinct strategies it learned during training.
28