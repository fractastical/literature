Blind CT Image Quality Assessment Using DDPM-derived 
Content and Transformer-based Evaluator
Yongyi Shi,
Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180 USA
Wenjun Xia,
Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180 USA
Ge Wang,
Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180 USA
Xuanqin Mou
Institute of Image Processing and Pattern Recognition, Xi’an Jiaotong University, Xi’an 710049, 
China
Abstract
Lowering radiation dose per view and utilizing sparse views per scan are two common CT scan 
modes, albeit often leading to distorted images characterized by noise and streak artifacts. Blind 
image quality assessment (BIQA) strives to evaluate perceptual quality in alignment with what 
radiologists perceive, which plays an important role in advancing low-dose CT reconstruction 
techniques. An intriguing direction involves developing BIQA methods that mimic the operational 
characteristic of the human visual system (HVS). The internal generative mechanism (IGM) 
theory reveals that the HVS actively deduces primary content to enhance comprehension. In this 
study, we introduce an innovative BIQA metric that emulates the active inference process of IGM. 
Initially, an active inference module, implemented as a denoising diffusion probabilistic model 
(DDPM), is constructed to anticipate the primary content. Then, the dissimilarity map is derived 
by assessing the interrelation between the distorted image and its primary content. Subsequently, 
the distorted image and dissimilarity map are combined into a multi-channel image, which is 
inputted into a transformer-based image quality evaluator. By leveraging the DDPM-derived 
primary content, our approach achieves competitive performance on a low-dose CT dataset.
Keywords
Blind image quality assessment; denoising diffusion probabilistic model (DDPM); primary 
content; transformer-based image quality evaluator
I. INTRODUCTION
X-RAY image quality assessment (IQA) plays a crucial role in computed tomography (CT) 
imaging, facilitating the advancement of novel algorithms for low-dose CT reconstruction. 
Corresponding authors: Ge Wang; Xuanqin Mou, wangg6@rpi.edu; xqmou@mail.xjtu.edu.cn. 
HHS Public Access
Author manuscript
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Published in final edited form as:
IEEE Trans Med Imaging. 2024 October ; 43(10): 3559–3569. doi:10.1109/TMI.2024.3418652.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Two strategies for low-dose CT scans are either reducing the X-ray tube current or acquiring 
sparse views. However, these strategies often introduce noise and/or streak artifacts in 
the filtered backprojection (FBP) images. To address these problems, deep learning-based 
methods have been used for low-dose CT image denoising [1-6] and sparse view CT 
reconstruction [7-13]. In the field of low-dose CT image denoising, Chen et al. devised 
a residual encoder-decoder convolutional neural network (RED-CNN) [1]. Yang et al. 
introduced a CT image denoising method based on the generative adversarial network 
(GAN) with Wasserstein distance and perceptual similarity [3]. Wang et al. proposed 
a convolution-free token2token dilated vision transformer (CTformer) utilizing token 
rearrangement to capture local contextual information [6]. On the other hand, in the field 
of sparse-view CT reconstruction, Han et al. demonstrated that the framing U-Net approach 
satisfies the frame condition, leading to effective recovery of high frequency information in 
sparse-view CT [8]. Wu et al., proposed a deep embedding attention refinement (DEAR) 
network comprising three modules including deep embedding, deep attention, and deep 
refinement [12]. Li et al. proposed a transformer-based module DDPTransformer to capture 
long-range dependencies within an image for high-quality CT reconstruction from sparsely 
sampled sinograms [13]. Recently, several score-based methods have also been employed 
for few-view CT reconstruction [14-16]. When the quality of these reconstructed images 
is assessed, radiologist’s opinions serve as the gold standard. Nonetheless, collecting these 
opinions is an expensive and intricate process, making it impractical for real-time and large-
scale IQA tasks. Consequently, peak signal-to-noise (PSNR) and structural similarity index 
measure (SSIM) have been widely used as surrogate metrics [17]. However, both PSNR and 
SSIM have shown limited correlation with radiologists’ opinions on image quality, primarily 
due to their reliance on mathematical models that do not account for the intricacies of human 
perception. Moreover, the requirement for reference images to calculate these metrics poses 
challenges in clinical environments; for example, obtaining high-quality images is infeasible 
without increasing patient radiation exposure. To overcome these limitations, one direction 
is to develop no-reference image quality metrics that correlates well with radiologists’ 
opinion on image quality.
No-reference image quality assessment (NR-IQA), also known as blind IQA (BIQA), 
is widely used to evaluate the quality of natural images [18-19]. Traditional BIQA 
methods typically comprise three steps. First, some handcrafted descriptors are employed 
to extract quality-aware features of training images. Then, the statistical distribution of the 
extracted features serves as the guidance. Some approaches also involve parameterizing 
this distribution through modeling. Finally, a mapping function, such as support vector 
regression (SVR) [20], is designed to convert the distributions into a quality score. In 
traditional BIQA, features are derived from various sources including discrete wavelet 
transform (DWT) [21-23], discrete cosine transform (DCT) [24] and spatial domain 
measures [25-26] to predict the perceptual quality. Alternatively, it is assumed that the 
human visual system (HVS) gauges image quality by discerning features like gradient [27], 
luminance contrast [28] or local binary pattern [29]. Certain approaches combine these 
extracted features to improve image quality assessment further [30-31]. Nevertheless, due to 
the complexities of image contents and distortion patterns, the representation capabilities of 
handcrafted features are often unsatisfactory.
Shi et al. Page 2
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
In recent years, the convolutional neural network (CNN) has gained major attention in 
BIQA tasks because of its potent feature representation power [32-33] and started gaining 
momentum [34-39]. Rank-IQA utilizes synthetically generated distortions and a Siamese 
network to rank images based on their quality [35]. DBCNN demonstrates effectiveness 
with both synthetic and authentic image distortions [36]. Hyper-IQA segregates features 
into low-level and high-level categories, then transforms the latter to reshape the former’s 
influence [37]. Meta-IQA employes meta-learning to train the networks on distinct types 
of distortions, thereby acquiring prior knowledge [38]. More recently, vision transformers 
(ViTs) [39] have emerged as competitive alternatives to CNNs. Their self-attention 
mechanism empowers ViT to grasp global contextual information from an entire image, 
ensuring a comprehensive consideration of all image features of significance for task-
specific prediction. This attribute aligns well with the requirements of BIQA task, which 
involves predicting a quality index globally [40-46]. MSTIQA leverages a Swin transformer 
to amalgamate features from multiple stages to enhance quality assessment [44]. MANIQA 
introduces a multi-dimensional attention network for BIQA. This approach introduces the 
transposed attention block and the multiscale Swin transformer block to strengthen global 
and local interactions [45]. MAMIQA employs a lightweight attention mechanism that 
utilizes decomposed large kernel convolutions to extract multiscale features. Furthermore, 
it includes a feature enhancement module to enrich local fine-grained details and global 
semantic information at multi-scales [46]. Among the various transformer-based BIQA 
methods, MANIQA has not only achieved the state-of-the-art performance in BIQA tasks 
but also secured the first place in the no-reference track of NTIRE 2022 perceptual image 
quality assessment challenge [47]. Intriguingly, the top three methods in this challenge all 
rely on transformer-based techniques, highlighting the efficacy of the transformers in BIQA 
tasks. Nonetheless, the lack of reference information poses a challenge, preventing these 
methods from aligning seamlessly with the HVS and may adversely impacting their overall 
performance.
To perceive distorted images, the HVS leverages an internal generative mechanism (IGM) 
[48-50] based on prior knowledge in the brain to reconstruct the primary content as 
accurately as possible. Although the primary content is not pixel-level accurate like 
a pristine image, it consists of essential scene information, representing the structural, 
meaningful elements of the image. Subsequently, the primary content is transported to 
the high level of HVS for interpretation [51]. Evaluation is performed by analyzing the 
discrepancies between the primary content and the distorted image [52], with greater 
differences indicating more severe image distortion. Thus, the ability to generate high-
quality primary content becomes critical in emulating the HVS. To acquire prior knowledge 
of pristine images, some researchers proposed using generative adversarial networks 
(GANs) to generate primary content [53-54]. To further improve the generative/restoration 
performance, a multitask image restoration sub-network is used to generate the primary 
content from a distorted image [55]. However, there is still room to improve the quality 
of the recovered primary content, especially for severely distorted images. Notably, recent 
successes in adapting and applying the denoising diffusion probabilistic model (DDPM) and 
other score-based generative models showcased their amazing capabilities in both image 
generation and restoration [56-59]. These qualities underscore a huge potential of diffusion 
Shi et al. Page 3
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
models in generating high-quality primary content, particularly in the context of low-dose 
CT imaging [60-63] which is featured by uniquely intricate noise and streak artifacts, quite 
different from what in natural images.
Given the globalized artifact patterns in low-dose CT images, we utilized a transformer-
based quality evaluator [45] to assess the image quality. We found in our study that 
directional artifacts may sometimes be misleading by resembling genuine anatomical 
structures. To further improve the performance, inspired by IGM here we propose a novel 
approach utilizing a DDPM-based active inference for low-dose CT BIQA [64-65]. At the 
outset, given DDPM’s capability to simultaneously eliminate noise and streak artifacts, 
we employ DDPM to generate high-quality primary content, closely mimicking the HVS. 
Recognizing the heightened sensitivity of HVS to structures, the dissimilarity map is 
extracted from the distorted image and its primary content. After that, incorporating such 
diverse prior information as input, a multi-channel image is synthesized, amalgamating 
content, distortion, and structural characteristics for comprehensive quality prediction. The 
final step involves employing a transformer-based quality evaluator to predict the score. 
Empirical trials conducted on the low-dose CT BIQA dataset systematically affirm the 
efficacy of our proposed method [66].
In brief, the main contributions of this paper can be summarized as follows:
1) We propose a conditional DDPM to emulate the active inference process of IGM. The 
proposed DDPM-based active inference module can effectively predict the primary content 
of distorted images which contain intricate noise and streak artifacts from both low-dose and 
sparse view CT imaging.
2) Based on the primary content, we introduce a transformer-based quality evaluator to 
predict the image quality on a low-dose CT BIQA dataset. On that basis, we have improved 
the image quality assessment performance even further, as explained in detail below.
The remainder of the paper is organized as follows. Section II outlines the process of 
utilizing a conditional DDPM to acquire both the primary content and the dissimilarity 
map, while also detailing the transformer-based quality evaluator. Section III reports our 
evaluation results on the low-dose CT BIQA dataset. Section IV discusses relevant issues 
and makes the conclusion.
II. Methodology
A. Conditional DDPM for Low-Dose CT
To generate high-quality primary content for emulating the HVS, a typical deep learning 
method trains a network to learn a mapping from low-dose CT images to normal-dose CT 
images. Let us assume that y∈ ℝN and x∈ ℝN are paired low-dose CT and normal-dose CT 
images. The parameters of the network can be trained as follows:
min
θ
‖Dθ(y) −x‖2
2
Shi et al. Page 4
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
(1)
where Dθ is the network parameterized by θ, which is the key for producing high-quality 
images.
Recently, DDPM has demonstrated superior performance in generating high-quality images 
from their distorted counterparts, particularly in complex scenarios involving multiple 
distortions, such as CT imaging.
The architecture of the conditional DDPM is shown in Fig. 1. DDPM starts with a forward 
process that gradually adds noise to the normal-dose CT image x0 ∼ q(x0) over the course of 
T timesteps according to a variance schedule β1, ⋯,βT:
q(xt ∣ xt − 1) =N(xt; 1 −βtxt − 1, βtI)
(2)
q(x1: T ∣ x0) = ∏
t = 1
T
q(xt ∣ xt − 1)
(3)
where x1, ⋯,xT are latent variables of the same dimensionality as the sample x0 ∼ q(x0).
According to the properties of the Gaussian distribution, the sampling result xt at an arbitrary 
timestep t can be written in the following closed form:
q(xt ∣ x0) =N(xt; α¯tx0, (1 −α¯t)I)
(4)
where αt = 1 −βt and α¯t = ∏i = 1
t
αi.
After the forward process, xT follows a standard normal distribution when T is large enough. 
Thus, if we know the conditional distribution q(xt − 1 ∣ xt), we can use the reverse process 
to get a sample under q(x0) from xT ∼ N(0, I). However, q(xt − 1 ∣ xt) depends on the entire 
data distribution, which is hard to calculate. Hence, a neural network was designed to 
learn a latent data distribution by gradually denoising a normal distribution variable, which 
corresponds to learning the reverse process of a fixed Markov Chain of length T conditioned 
on a low-dose CT image y. The reverse process can be defined as:
pθ(xt − 1 ∣ xt, y) =N(xt − 1; μθ(xt, y, t), σt
2I)
(5)
Shi et al. Page 5
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
pθ(x0: T ∣ y) =p(xT) ∏
t = 1
T
pθ(xt − 1 ∣ xt, y)
(6)
where p(xT) is the density function of xT. In Eq. (5), μθ(xt, y, t) and σt2 are needed to solve 
pθ(xt − 1 ∣ xt, y). According to the Bayes theorem, the posterior q(xt − 1 ∣ xt, x0) are defined as
q(xt − 1 ∣ xt, x0) =N(xt − 1; μt(xt, x0), σt
2I)
(7)
where
μt(xt, x0) = αt(1 − α¯t − 1)
1 − α¯t
xt + α¯t − 1(1 − αt)
1 − α¯t
x0
(8)
σt
2 = (1 − α¯t − 1)(1 − αt)
1 − α¯t
(9)
Since σt2 is a constant, the most natural parameterization of μθ(xt, y, t) is a neural network 
that predicts μt(xt,x0) directly. Alternatively, given that xt = α¯tx0 + 1 −α¯tϵ, ϵ ∼ N(0, I) the 
posterior expectation in Eq. (8) can be expressed as
μt(xt, x0) =μt xt, 1
α¯t
(xt − 1 − α¯tϵ)
= 1
αt
xt − 1 − αt
1 − α¯t
ϵ
(10)
Since μt(xt,x0) can be represented by ϵ, we can also use a neural network model Dθ to predict 
the noise ϵ, which has been proved to work well by Ho et al. [56]. Hence, the corresponding 
objective can be simplified to
ℒ =Ex, yEϵ, t[ (1 −αt)2
2σt
2α(1 −α¯)‖ ϵ − Dθ( α¯tx0 + 1 −α¯tϵ, y, t)‖2
2
]
(11)
with t uniformly sampled as {1, ⋯,T}.
Shi et al. Page 6
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
In this study, the latent space is diffused into Gaussian noise using T = 1000 steps. A U-Net 
[67] model Dθ is trained to predict the noise ϵ in the latent space. To obtain high-quality 
primary contents, samples can be computed as follows:
xt − 1 = 1
αt
xt − 1 −αt
1 −α¯t
Dθ(xt, y, t) + σtz
(12)
where z ∼ N(0, I). In the training step, we follow the same setting with low-dose CT BIQA 
dataset to simulate on both reducing the X-ray tube current and acquiring sparse views for 
the low-dose CT image y, which lead to noise and streak artifacts. Fig. 1 shows one example 
in the training dataset which contains both noise and streak artifacts. In the inference step, 
we use the image in the low-dose CT BIQA dataset as the conditional to obtain the primary 
contents.
B. Dissimilarity Map
Recently, the free energy principle and Bayesian brain hypothesis has revealed an active 
reference process within the IGM of the HVS [48-50]. When presented with an input 
image, IGM initiates its analysis by scrutinizing pixel correlations. By incorporating 
this information with inherent prior knowledge, IGM actively deduces the corresponding 
primary content to enhance its comprehension of the input image. In this paper, we employ 
a conditional DDPM to emulate the active inference mechanism of IGM and obtain a good 
estimate of the primary content. It is crucial to emphasize that the conditional DDPM is 
not intended to completely restore a distorted image to its reference image. However, the 
predicted primary content usually portrays the reference image more accurately than the 
distorted input image. In our pursuit of providing a much-improved representation of an 
input image, IGM seeks to minimize mismatches in image content, as indicated by the 
prediction error between the distorted input image and its primary content. Consequently, 
our objective is to compute a dissimilarity map in reference to this estimated primary 
content, focusing on capturing distinctions rather than merely quantifying the pixel-wise 
disparity between the distorted image and its primary content. The pipeline for computing 
the dissimilarity map is illustrated in Fig. 2.
After we obtain the primary content with the conditional DDPM, we calculate the SSIM 
map between the distorted image and the primary content. Absolute is employed to assign 
values between 0 and 1 to each pixel, with 0 indicating the lowest dissimilarity and 1 
representing the perfect similarity. As dissimilarity indicates the presence of distortion, 
a critical factor in assessing image quality, we subtract the SSIM map from an all-one 
matrix I to obtain the dissimilarity weights, where larger values signify more severe 
distortions. Finally, we calculate the Hadamard product between the distorted image and the 
dissimilarity weights to obtain the dissimilarity map. With this dissimilarity map as an input, 
the subsequent image quality evaluator can more effectively leverage the characteristics of 
IGM for BIQA.
Shi et al. Page 7
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
C. Transform-based Quality Evaluator
After obtaining the dissimilarity map, we integrate it with the distorted image, resulting in a 
multi-channel image (a tensor in general). Our primary goal is to create a quality evaluator 
that can effectively process the multi-channel data tensor consisting of these input images. 
To comprehensively utilize information with both the spatial and channel dimensions, we 
introduce a transformer-based model tailored for perceptual quality prediction based on 
MANIQA [45]. The architecture of this transformer-based quality evaluator is depicted in 
Fig. 3. The core components of the transformer-based quality evaluator include: 1) vision 
transformer; 2) transposed attention block; 3) scale Swin transformer block, and 4) patch-
weighted quality prediction.
1) Vision Transformer— In the context of a given multi-channel image, denoted as 
E ∈ ℝH × W × 3, where H and W represent the height and width respectively, we employ 
the ViT denoted by fφ with a learnable vector of parameters φ. From the ViT, we extract 
features Fi ∈ ℝb × ci × HiWi corresponding to the i-th layer, where i ranges from 1 to 12, 
b denotes the batch size, ci represents channel size, Hi and Wi denote the dimensions of 
the i-th feature. Specifically, we utilize 4 layers to extract features with varying semantic 
significance. Leveraging the excellent performance demonstrated by the MANIQA method 
[45], we adhere to their configuration and concatenate Fi, where i ∈ {7, 8, 9, 10}, yielding a 
composite feature tensor denoted as F ∈ ℝb × ∑i ci × HiWi.
2) Transposed Attention Block— To enhance channel interaction within these 
concatenated features, we employ a transposed attention block. Unlike the conventional 
spatial attention, this block facilitates self-attention across channels, effectively computing 
cross-covariance between channels to generate an attention map that encodes global 
contextual relations. Beginning with the feature tensor F ∈ ℝb × ∑i ci × HiWi, the transposed 
attention block first generates query (Q), key (K) and value (V) projections through 
independent linear projections. These projections are utilized to encode pixel-wise cross-
channel dependency. The query and key projections are reshaped for dot-product interaction, 
producing a transposed-attention map A ∈ ℝC × C. Note that C is equal to ci. It is noteworthy 
that the layer normalization and multi-layer perceptron components from the original 
transformer structure are omitted. Mathematically, the transposed attention block is defined 
as follows:
X = W pAttn(Q, K, V ) +X
(13)
Attn(Q, K, V ) =V ⋅ Softmax(K ⋅ Q ∕ α)
(14)
where α denotes the root of spatial dimension of Q, K and V.
Shi et al. Page 8
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
3) Scale Swin Transformer Block— The scale Swin transformer block consists of 
Swin transformer layers and convolutional layers. Given an input feature Fi, 0, the scale Swin 
transformer block first encodes the feature tensor through 2 Swin transformer layers:
Fi, j = HSTLi, j Fi, j , j = 1, 2,
(15)
where HSTLi, j( ⋅ ) represents the j-th Swin transformer layer in the i-th stage, i ∈ {1, 2}. After 
the encoding process, a convolutional layer is applied before the residual connection. The 
output of the scale Swin transformer block is formulated as
Fout = α ⋅ HCONV HSTL (Fi, 2) + Fi, 0
(16)
where HCONV ( ⋅ ) is the convolutional layer, and α denotes a scale factor for the output of the 
Swin transformer layer.
4) Patch-Weighted Quality Prediction— Given the feature tensor, we generate weight 
and score projections, which are achieved through 2 independent linear projections. The 
final patch score of the distorted image is generated by multiplying the score and weight of 
each patch, then the final score of the whole image is generated by the summation of all the 
final patch scores.
To align the input sizes with the pre-trained transformer model, we employed a process 
of cropping and resizing the image. Additionally, we created a three-channel image tensor 
by merging a dissimilarity map with two replicated distorted images. We call our method 
distortion-based BIQA or D-BIQA. Fig. 4 shows the overall framework of the approach.
D. Dataset
1) DDPM Training Dataset— For training DDPM, we simulated images with varying 
levels of dosage, by adding realistic noise to the dataset used in the 2016 low-dose CT 
grand challenge [68]. With the assumed use of a monochromatic source, the projection 
measurements from a CT scan follow the Poisson distribution, which can be expressed as
ni ∼ Poisson{bie−li + ri}, i = 1, ⋯,I
(17)
where ni is the measurement along the i-th ray path. bi are the air scan photons, ri denotes 
read-out noise. In Eq. (17), the noise level can be controlled by bi. This allowed us to obtain 
three additional noise levels equivalent to 50%, 25% and 10% of the normal dose for the 
2016 low-dose CT grand challenge dataset. Streak artifacts are generated using a similar 
pipeline to noise insertion, but by reconstructing with different numbers of projections. 
There are three different numbers of projection views used in our study, which are 720, 360, 
and 180 views equiangularly distributed over a full scan. For each noise level we obtained 3 
Shi et al. Page 9
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
different sparse views, hence we obtained 12 reconstructed CT images at different qualities 
from each normal dose sinogram. We selected 1,222 abdomen normal dose images from the 
2016 low-dose CT grand challenge dataset and simulated 14,664 images with different noise 
levels and numbers of projection views to train our DDPM model.
2) Low-dose CT BIQA Dataset— The low-dose CT BIQA dataset comprises a total 
of 1,000 distorted abdominal images, which exhibit both noise and streak artifacts. Given 
the challenges associated with collecting a real dataset for the low-dose CT BIQA task, 
including ethical considerations, patient privacy concerns, radiation dose implications, and 
the limited availability of low-dose cases, these images were simulated at four different 
dose levels: 100%, 50%, 25%, and 10%. Additionally, three numbers of projection views 
were uniformly selected over a full scan, resulting in a total of 12 different types of image 
degradation. Out of these images, 900 were allocated for the training phase, while the 
remaining 100 were reserved for the testing phase. In this dataset, CT values exceeding 350 
Hounsfield Units (HU) were capped at 350 HU, resulting in a CT value range from - 1000 
HU to 350 HU. These CT values were then transformed to a normalized range from 0 to 1. 
The CT image quality assessment was performed using the abdominal soft-tissue window, 
defined by the width/level setting of 350/40, and evaluated by five proficient radiologists. 
The ultimate human perceptual score for each image was determined by averaging the 
individual ratings by these radiologists. The specific criteria for this assessment are detailed 
in Table I. Fig. 5 displays representative samples from the low-dose CT BIQA dataset. In 
Fig. 5(a), the normal-dose image with 720 views achieves an outstanding image quality 
score of 4.0. Figs. 5(b) and 5(c) exhibit images with varying noise levels. The dose level 
of Fig. 5(b) is lighter than that of Fig. 5(c), resulting in respective scores of 2.8 and 1.0. 
Figs. 5(d) and 5(e) showcase images with different views. The spare-view reconstruction in 
Fig. 5(d) introduces some streak artifacts, while Fig. 5(e) exhibits stronger streak artifacts, 
leading to a lower score of 1.6. Fig. 5(f) combines low-dose and sparse-view, revealing poor 
image quality with a score of 0.0. It’s noteworthy that the score is not solely influenced by 
dose level and sparse view but is also affected by image context and other factors, mirroring 
real clinical scenarios.
3) Real Clinical Data— We also evaluated the effectiveness of our proposed model in 
real clinical scenarios. Given the challenge of collecting a real dataset with radiologists’ 
scores, we adopted an alternative approach involving four images with identical content 
but varying radiation dose levels. However, obtaining four fully registered images remains 
challenging due to patient movement and radiation dose involved in repeated scans. To 
address this challenge, an anonymous cadaver underwent four repeated scans on a GE 
Discovery 750 HD scanner at Massachusetts General Hospital, each conducted at a different 
radiation dose with noise indices of 10, 20, 30, and 40 [69]. Here the noise index 
approximates the standard deviation of CT number in the central region of the image of 
a uniform phantom and is used to characterize image quality. Given the identical content 
across the four images, we anticipate that the quality score will be solely influenced by the 
dose level. Lower doses are expected to correlate with higher noise levels and subsequently 
result in diminished image quality scores.
Shi et al. Page 10
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
E. Network Training
For training the DDPM model, we set the total number of time steps T to 1,000. The model 
underwent training using the Adam optimizer, with a learning rate of 1 × 10−4. The training 
process demonstrated convergence after 5 × 105 iterations on a computing server equipped 
with four Nvidia Tesla V100 GPUs. Upon completing the training process, we employed 
the trained DDPM to obtain the primary contents from the low-dose CT BIQA dataset. 
Subsequently, we computed the dissimilarity map, resulting in a multi-channel image.
To train the transformer-based quality evaluator, we selected the ViT-B/8 model as our 
pre-trained model. This model was initially trained on ImageNet-21k and fine-tuned on 
ImageNet-1k, using a patch size of 8. To accommodate the varying input sizes of our 
datasets, we applied central cropping to resize the images to 448×448, as the edges of 
the images typically represent air and have limited influence on the overall image quality 
score. Furthermore, we resized these cropped images to a final size of 224×224. Then, we 
combined a dissimilarity map and two duplicated distorted images to form a three-channel 
image tensor.
The transformer-based quality evaluator consists of two stages, each comprising 2 
transposed attention blocks and 1 scale Swin transformer block. The dimensions of the 
hidden layer, the number of heads, and the window size are set to 768, 4, and 4 in each scale 
Swin transformer block. We set the scale parameter to 0.8 in the scale Swin transformer 
block. For training, we established the learning rate at 1 × 10−5 and used a batch size of 
8. The Adam optimizer was used with a weight decay of 1 × 10−5 and cosine annealing 
for learning rate scheduling. The selected loss function was the mean square error (MSE). 
As our transformer-based quality evaluator was built upon the foundation of MANIQA 
[45], we still call the transformer-based quality assessment method without primary content 
MANIQA throughout this paper. Our experiments were conducted on an NVIDIA RTX 
A4000 using PyTorch 2.0.1 and CUDA 11.8 for both training and testing.
F. Evaluation Criteria
We use Pearson’s linear correlation coefficient (PLCC), the absolute value of the 
Spearman’s rank order correlation coefficient (SROCC), and the Kendall rank-order 
correlation coefficient (KROCC) as the metrics to evaluate the performance of our models. 
The PLCC is defined as
PLCC =
∑i = 1
M
(si − μsi)(si − μsi)
∑i = 1
M
(si − μsi)2 ∑i = 1
M
(si − μsi)2
(18)
where si and si respectively indicate the ground-truth and predicted quality scores of the i-th 
image, μsi and μsi indicate their means, and M denotes the testing images. Let di denote the 
difference between the ranks of the i-th test image in ground-truth and the predicted quality 
score. The SROCC is defined as
Shi et al. Page 11
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
SROCC = 1 − 6∑i = 1
M
di
2
M(M2 − 1)
(19)
The KROCC is defined as:
KROCC = 2(Mc − Md)
M(M − 1)
(20)
where Mc is the count of data pairs sharing the same rank, and Md is the count of data 
pairs with different rank. We use the absolute values of all the metrics, PLCC, SROCC and 
KROCC, are all in the range of [0, 1]. A higher value indicates better performance. The 
overall metric is defined as
Overall = PLCC + SROCC + KROCC
(21)
III. Results
A. Inspection of Primary Contents
As obtaining high-quality primary contents is of paramount importance in mimicking the 
HVS, our initial focus is on evaluating primary contents across various quality scores 
and comparing the outcomes using distinct reconstruction methods. We employed the four 
classic methods for this purpose: 1) FBP: Reconstructing the original image from the 
low-dose CT BIQA dataset using FBP; 2) RED-CNN: Utilizing CNN for low-dose CT 
reconstruction; 3) SU-Net: Combining the U-Net architecture with the Swin transformer 
for low-dose CT reconstruction; and 4) DDPM: Known for its powerful image generation 
capabilities in low-dose CT reconstruction.
In Fig. 6, we present three representative samples from the low-dose CT BIQA dataset, 
with assigned quality scores of 0.2, 2.0, and 4.0 respectively. The FBP-reconstructed image 
with a quality score of 0.2 exhibits distortions caused by both noise and streak artifacts. 
Crucial features are not discernible. At a quality score of 2.0, the FBP-reconstructed image 
displays noticeable streak artifacts, but the main structural features are still recognizable. 
In the FBP reconstruction with a quality score of 4.0, the anatomical structure is highly 
visible, qualified as a reference image. The RED-CNN method successfully mitigates noise 
artifacts but sometimes amplifies streak artifacts. This effect is particularly pronounced in 
the image with a quality score of 0.2, exerting a negative impact on image quality. SU-Net 
effectively suppresses both noise and streak artifacts. However, the resultant image appears 
overly smooth, deviating from the authentic appearance of the real clinical counterpart. 
The DDPM approach emerges as an accurate yet robust solution, effectively suppressing 
noise and streak artifacts while simultaneously preserving intricate details. Furthermore, 
the distribution of the images closely aligns with that of real clinical reference images. As 
Shi et al. Page 12
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
a result, opting for DDPM to generate high-quality primary contents proves feasible and 
advantageous, facilitating an accurate emulation of the IGM principle within the HVS.
B. Visualization of the Prior Information
By employing primary contents generated through DDPM, we can glean insights into the 
proposed method. In Fig. 7, the first row showcases distorted images across varying levels of 
distortions. From the second to the last row, we present corresponding primary contents and 
dissimilarity maps. The effectiveness of the IGM becomes evident in its ability to mitigate 
noise and infer high-quality primary content, as evidenced in Fig. 7(d)-(f). Concurrently, as 
depicted in Fig. 7(g)-(i), when subjected to distinct levels of artifacts, the dissimilarity map 
exhibits diverse patterns. Notably, Fig. 7(i) demonstrates lower dissimilarity, implying less 
content degradation in Fig. 7(c).
C. Visualization of Exemplary Images
In Fig. 8, we visualize distorted images and weight maps for five represetative examples 
from the low-dose CT BIQA test dataset. These weight maps clearly highlight salient 
subjects, which are significant areas that strongly influence human perception. When 
humans view an image, features of interest greatly affects our perception, and therefore 
these features receive higher weights as depicted in Fig. 8. Furthermore, as the score 
increases, the weight also becomes larger. Regarding the predicted scores for the images, we 
observed some bias relative to the radiologists’ scores, indicating that there is still room for 
improvement.
D. Performance Comparison within the Low-dose CT BIQA Dataset
Given the absence of reference images in the dataset, we straightforwardly partition the 
dataset based on distorted images. We compared our method against one traditional BIQA 
method and two CNN-based methods, along with one transformer-based method. For 
the traditional approach, we selected the natural image quality evaluator (NIQE) as a 
representative method as implemented in the official code. Among the CNN-based methods, 
we used DBCNN and Hyper-IQA as representative networks, using the implementations 
provided in [70]. As for the transformer-based method, we employed MANIQA. Referring 
to Table II, limited by the handcrafted features, the traditional NIQE demonstrates the lowest 
performance. The CNN-based methods show remarkable improvements over the NIQE 
approach. Between these two CNN-based methods, DBCNN outperforms in PLCC and 
KROCC metrics, while Hyper-IQA excels in SROCC. The performance of these two CNN-
based methods is closely comparable. As a representative of transformer-based methods, 
MANIQA exhibits superior performance over the CNN-based methods. Remarkably, 
our proposed D-BIQA method achieves the highest performance. In summary, D-BIQA 
demonstrates strong efficacy on the low-dose CT BIQA dataset, thereby affirming the 
effectiveness of our proposed approach. Additionally, a statistical significance test was 
performed to assess the significance of performance differences between each pair of 
methods. The SROCC values of each method served as input for t-test. The resulting t-test 
outcomes are presented in Table III, where the notations ‘1/-1/0’ denote whether the model 
in the corresponding row is statistically better/worse than or indistinguishable from a method 
Shi et al. Page 13
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
in the respective column within a 95% confidence level. The analysis reveals that D-BIQA 
consistently outperforms the other four methods.
E. Ablation Study
To demonstrate the crucial role of combining prior and input information in our proposed 
method, we conducted an ablation study to validate this type of synergy. The experimental 
findings, detailed in Table IV , reveal the performance of different approaches. The “Ori” 
method solely utilizes the distorted image as input, serving as the baseline model. 
The “Pri” and “Dis” methods solely use the primary content and dissimilarity map as 
input, respectively. These two methods do not directly include the original distorted 
image. Regarding the primary content, all images exhibit good quality, leading to scores 
inconsistent with human evaluations. The “Ori+Pri” method integrates both the distorted 
image and primary content to predict the quality score, resulting in an improved overall 
score. However, the PLCC value is inferior to the “Ori” method since the primary content 
is not the reference image. In contrast, our dissimilarity map captures overall image quality 
distinctions rather than merely quantifying the pixel-wise disparity between the distorted 
image and its primary content. As anticipated, our proposed D-BIQA method combines the 
distorted image and predicted image content into the dissimilarity map and yields the best 
performance.
F. Real Data Evaluation
Fig. 9 presents the outcomes in real clinical scenarios. As the dose level decreases, the noise 
level increases, leading to diminished image quality. The primary contents generated by 
DDPM effectively remove the noise and attain a comparable image quality as confirmed 
by visual inspection. As expected, the quality score predicted by our model becomes 
increasingly worse with the decrement of the radiation dose. Given the identical content 
of the four images, it is clearly shown that our proposed model effectively ranks image 
quality in the real clinical scenario.
IV Discussions and Conclusion
Given the widespread artifacts in low-dose CT images, we have opted for a transformer-
based approach to comprehend these artifacts globally. It is worth emphasizing the 
importance of pretraining when utilizing transformers. In this study, we have chosen to 
use a pretrained ViT-B/8 model with a 224×224 configuration for further training on a 
low-dose CT dataset. In typical natural image BIQA, researchers often employ a random 
crop of 224×224 to align with the requirement for training a transformer. However, in 
the case of CT images, the peripheral region of the image predominantly corresponds to 
air and should not influence the quality assessment. As a result, we have cropped each 
original 512 × 512 images down to a centralized 448 × 448 image, excluding the air and 
less important features from consideration. To ensure the quality of neural network training 
while simultaneously minimizing computational complexity, we have employed a common 
down-sampling technique [71]. Specifically, all images were consistently down-sampled to 
a uniform size of 224×224. This enables us to effectively address the globalized artifacts in 
low-dose CT images while losing little information in this medical imaging domain.
Shi et al. Page 14
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Directional streak artifacts in low-dose CT images could mislead radiologists because they 
might resemble actual anatomical structures. To address this issue, we have implemented a 
DDPM model to mitigate these artifacts. While DDPM has demonstrated the capability to 
generate high-quality images that closely resemble reality, it is important to clarify that our 
intention here is not to restore a perfect distortion-free image. Our DDPM efforts may still 
retain compromised information especially when dealing with severely distorted images. 
Nevertheless, our primary objective is to emulate the active inference process employed 
by IGM to predict the vital content within the image and help image quality assessment. 
Indeed, the IGM works to avoid disorders such as noise and streak artifacts, which primarily 
evoke uncomfortable sensation and contribute to quality degradation. Therefore, we utilize 
the dissimilarity map to characterize this aspect. As long as the DDPM results are useful 
in this way, our goal is achieved, as evidenced in our ablation study. However, directly 
concatenating the two channel images may hinder the effective fusion of features from 
different images. In our future work, we will develop advanced fusion methods for better 
integration of multi-channel images.
In conclusion, we have introduced a novel D-BIQA model for image quality assessment 
based on DDPM-driven active inference. Leveraging the amazing image synthesis capability 
of DDPM, our active inference module seems adeptly emulating the IGM theory to forecast 
the primary contents from distorted images. Through the amalgamation of information 
derived from distorted images and dissimilarity maps, a multi-channel image tensor 
is formed and fed into a transformer-based quality evaluator. A comprehensive set of 
experiments substantiates the efficacy and superiority of our proposed approach. Compared 
to our reported transformer-based quality evaluator, this much-improved approach for D-
BIQA has further enhanced performance guided by the IGM theory, which is the main 
innovation of this work.
Acknowledgments
This work was supported in part by the National Institute of Biomedical Imaging and Bioengineering/National 
Institutes of Health under Grant R01 EB016977. The work of Xuanqin Mou was supported in part by the Natural 
Science Foundation of China, under Grant 62071375.
References
[1]. Chen H, Zhang Y , Kalra MK, Lin F, Chen Y , Liao P, Zhou J, and Wang G, “Low-dose CT with a 
residual encoder-decoder convolutional neural network,” IEEE Transactions on Medical Imaging, 
vol. 36, no. 12, pp. 2524–2535, 2017. [PubMed: 28622671] 
[2]. Jin KH, McCann MT, Froustey E, and Unser M, “Deep convolutional neural network for inverse 
problems in imaging,” IEEE Transactions on Image Processing, vol. 26, no. 9, pp. 4509–4522, 
2017. [PubMed: 28641250] 
[3]. Yang Q, Yan P, Zhang Y , Yu H, Shi Y , Mou X, Kalra MK, Zhang Y , Sun L, and Wang G, 
“Low-dose CT image denoising using a generative adversarial network with Wasserstein distance 
and perceptual loss,” IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1348–1357, 
2018. [PubMed: 29870364] 
[4]. Kang E, Chang W, Yoo J, and Ye JC, “Deep convolutional framelet denoising for low-dose CT 
via wavelet residual network,” IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1358–
1369, 2018. [PubMed: 29870365] 
[5]. Shan H, Zhang Y , Yang Q, Kruger U, Kalra MK, Sun L, Cong W, and Wang G, “3-d convolutional 
encoder-decoder network for low-dose CT via transfer learning from a 2-d trained network,” 
Shi et al. Page 15
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1522–1534, 2018. [PubMed: 
29870379] 
[6]. Wang D, Fan F, Wu Z, Liu R, Wang F, Yu H, “CTformer: convolution-free Token2Token dilated 
vision transformer for low-dose CT denoising,” Physics in Medicine & Biology, vol. 68, no. 6, p. 
065012, 2023.
[7]. Han Y , Ye JC, “Framing U-Net via deep convolutional framelets: Application to sparse-view 
CT,” IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1418–1429, 2018. [PubMed: 
29870370] 
[8]. Zhang Z, Liang X, Dong X, Xie Y , Cao G, “A sparse-view CT reconstruction method based on 
combination of DenseNet and deconvolution,” IEEE Transactions on Medical Imaging, vol. 37, 
no. 6, pp. 1407–1417, 2018. [PubMed: 29870369] 
[9]. Zhang H, Liu B, Yu H, Dong B, “MetaInv-Net: Meta inversion network for sparse view CT image 
reconstruction,” IEEE Transactions on Medical Imaging, vol. 40, no. 2, pp. 621–634, 2020.
[10]. Hu D, Liu J, Lv T, Zhao Q, Zhang Y , Quan G, et al. , “Hybrid-domain neural network 
processing for sparse-view CT reconstruction,” IEEE Transactions on Radiation and Plasma 
Medical Sciences, vol. 5, no. 1, pp. 88–98. 2020.
[11]. Lee M, Kim H, Kim HJ, “Sparse-view CT reconstruction based on multi-level wavelet 
convolution neural network,” Physica Medica, vol. 80, pp. 352–362, 2020. [PubMed: 33279829] 
[12]. Wu W, Guo X, Chen Y , Wang S, Chen J, “Deep embedding-attention-refinement for sparse-
view CT reconstruction,” IEEE Transactions on Instrumentation and Measurement, vol. 72, p. 
4501111, 2022.
[13]. Li R, Li Q, Wang H, Li S, Zhao J, Yan Q et al. , “DDPTransformer: dual-domain with 
parallel transformer network for sparse view CT image reconstruction,” IEEE Transactions on 
Computational Imaging, vol. 8, pp. 1101–1116, 2022.
[14]. Wu W, Pan J, Wang Y , Wang S, Zhang J, “Multi-channel Optimization Generative Model for 
Stable Ultra-Sparse-View CT Reconstruction,” IEEE Transactions on Medical Imaging, early 
access, 2024.
[15]. Zhang J, Mao H, Wang X, Guo Y , Wu W, “Wavelet-inspired multi-channel score-based model for 
limited-angle CT reconstruction,” IEEE Transactions on Medical Imaging, early access, 2024.
[16]. Pan J, Yu H, Gao Z, Wang S, Zhang H, Wu W, “Iterative Residual Optimization Network for 
Limited-angle Tomographic Reconstruction,” IEEE Transactions on Image Processing, vol. 33, 
pp. 910–925, 2024. [PubMed: 38224516] 
[17]. Wang Z, Bovik AC, Sheikh HR, Simoncelli EP, “Image quality assessment: from error visibility 
to structural similarity,” IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600–612. 
2004. [PubMed: 15376593] 
[18]. Kamble V , Bhurchandi KM, “No-reference image quality assessment algorithms: A survey,” 
Optik, vol. 126, no. 11–12, pp. 1090–1097, 2015.
[19]. Xu S, Jiang S, Min W, “No-reference/blind image quality assessment: a survey,” IETE Technical 
Review, vol. 34, no. 3, pp. 223–245, 2017.
[20]. Smola AJ and Schölkopf B, “A tutorial on support vector regression,” Statistics Computing, vol. 
14, no. 3, pp. 199–222, 2004.
[21]. Moorthy AK and Bovik AC, “Blind image quality assessment: From natural scene statistics to 
perceptual quality,” IEEE Transactions on Image Processing, vol. 20, no. 12 2011.
[22]. Wang G, Wang Z, Gu K, Li L, Xia Z, Wu L, “Blind quality metric of DIBR-synthesized images 
in the discrete wavelet transform domain,” IEEE Transactions on Image Processing, vol. 11, no. 
29 pp. 1802–1814, 2019.
[23]. Deng C, Wang S, Bovik AC, Huang GB, Zhao B, “Blind noisy image quality assessment using 
sub-band kurtosis,” IEEE Transactions on Cybernetics, vol. 50, no. 3, pp. 1146–1156, 2019. 
[PubMed: 30629529] 
[24]. Saad MA, Bovik AC, Charrier C, “Blind image quality assessment: A natural scene statistics 
approach in the DCT domain,” IEEE Transactions on Image Processing, vol. 21, no. 8, pp. 
3339–3352, 2012. [PubMed: 22453635] 
Shi et al. Page 16
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
[25]. Mittal A, Moorthy AK, and Bovik AC, “No-reference image quality assessment in the spatial 
domain,” IEEE Transactions on Image Processing, vol. 21, no. 12, pp. 4695–4708, Dec. 2012. 
[PubMed: 22910118] 
[26]. Mittal A, Soundararajan R, and Bovik AC, “Making a ‘completely blind’ image quality 
analyzer,” IEEE Signal Processing Letter, vol. 22, no. 3, pp. 209–212, 2013.
[27]. Khan M, Nizami IF, and Majid M, “No-reference image quality assessment using gradient 
magnitude and wiener filtered wavelet features,” Multimedia Tools and Applications, vol. 78, pp. 
14485–14509, 2019.
[28]. Li Q, Lin W, Xu J, and Fang Y , “Blind image quality assessment using statistical structural and 
luminance features,” IEEE Transactions on Multimedia, vol. 18, no. 12, pp. 2457–2469, 2016.
[29]. Wu Q, Wang Z, Li H, “A highly efficient method for blind image quality assessment,” 
Proceedings of the IEEE International Conference on Image Processing, pp. 339–343, 2015.
[30]. Zhang M, Muramatsu C, Zhou X, Hara T, Fujita H, “Blind image quality assessment using the 
joint statistics of generalized local binary pattern,” IEEE Signal Processing Letters, vol. 22, no. 2, 
pp. 207–210, 2014.
[31]. Xue W, Mou X, Zhang L, Bovik AC, Feng X, “Blind image quality assessment using joint 
statistics of gradient magnitude and Laplacian features,” IEEE Transactions on Image Processing, 
vol. 23, no. 11, pp. 4850–4862, 2014. [PubMed: 25216482] 
[32]. Kang L, Ye P, Li Y , Doermann D, “Convolutional Neural Networks for No-Reference Image 
Quality Assessment,” Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 1733–1740, 2014.
[33]. Hou W, Gao X, Tao D, Li X, “Blind image quality assessment via deep learning,” IEEE 
Transactions on Neural Networks and Learning Systems, vol. 26, no. 6, pp. 1275–1286, 2014. 
[PubMed: 25122842] 
[34]. Yang X, Li F, Liu H, “A survey of DNN methods for blind image quality assessment,” IEEE 
Access, vol. 7, pp. 123788–123806, 2019.
[35]. Liu X, Weijer J, Bagdanov AD, “Rank-IQA: Learning from Rankings for No-Reference Image 
Quality Assessment,” Proceedings of the IEEE International Conference on Computer Vision, pp. 
1040–1049, 2017.
[36]. Zhang W, Ma K, Yan J, Deng D, Wang Z, “Blind image quality assessment using a deep 
bilinear convolutional neural network,” IEEE Transactions on Circuits and Systems for Video 
Technology, vol. 30, no. 1, pp. 36–47, 2018.
[37]. Su S, Yan Q, Zhu Y , Zhang C, Ge X, Sun J, et al., “Blindly assess image quality in the wild 
guided by a self-adaptive hyper network,” Proceedings of the IEEE Conference on Computer 
Vision and Pattern Recognition, pp. 3667–3676, 2020.
[38]. Zhu H, Li L, Wu J, Dong W, Shi G, “Meta-IQA: Deep meta-learning for no-reference image 
quality assessment,” Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 14143–14152, 2020.
[39]. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, “An 
image is worth 16x16 words: Transformers for image recognition at scale,” arXiv reprint 
arXiv:2010.11929, 2020.
[40]. You J, Korhonen J, “Transformer for image quality assessment,” Proceeding of the IEEE 
International Conference on Image Processing, pp. 1389–1393, 2021.
[41]. Ke J, Wang Q, Wang Y , Milanfar P, Yang F, “MUSIQ: Multi-scale image quality transformer,” 
Proceedings of the IEEE International Conference on Computer Vision, pp. 5148–5157, 2021.
[42]. Golestaneh SA, Dadsetan S, Kitani KM, “No-reference image quality assessment via 
transformers, relative ranking, and self-consistency,” Proceedings of the IEEE Winter Conference 
on Applications of Computer Vision, pp. 1220–1230, 2022.
[43]. Conde MV , Burchi M, Timofte R, “Conformer and blind noisy students for improved image 
quality assessment,” Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 940–950, 2022.
[44]. Wang J, Fan H, Hou X, Xu Y , Li T, Lu X, “MSTRIQ: No reference image quality assessment 
based on Swin transformer with multi-stage fusion,” Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition, pp. 1269–1278, 2022.
Shi et al. Page 17
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
[45]. Yang S, Wu T, Shi S, Lao S, Gong Y , Cao M, et al., “MANIQA: Multi-dimension attention 
network for no-reference image quality assessment,” Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition, pp. 1191–1200, 2022.
[46]. Yu L, Li J, Pakdaman F, Ling M, Gabbouj M, “MAMIQA: No-Reference Image Quality 
Assessment Based on Multiscale Attention Mechanism with Natural Scene Statistics,” IEEE 
Signal Processing Letters, vol. 30, pp. 588–592, 2023.
[47]. Gu J, Cai H, Dong C, Ren JS, Timofte R, Gong Y et al., “NTIRE 2022 challenge on perceptual 
image quality assessment,” Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 951–967, 2022.
[48]. Zhai G, Wu X, Yang X, Lin W, and Zhang W, “A psychovisual quality metric in free-energy 
principle,” IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 41–52, Jan. 2012. 
[PubMed: 21724508] 
[49]. Wu J, Lin W, Shi G, and Liu A, “Perceptual quality metric with internal generative mechanism,” 
IEEE Transactions on Image Processing, vol. 22, no. 1, pp. 43–54, Jan. 2013. [PubMed: 
22910116] 
[50]. Gu K, Zhai G, Yang X, and Zhang W, “Using free energy principle for blind image quality 
assessment,” IEEE Transactions on Multimedia, vol. 17, no. 1, pp. 50–63, Jan. 2015.
[51]. Eckert MP and Bradley AP, “Perceptual quality metrics applied to still image compression,” 
Signal Processing, vol. 70, no. 3, pp. 177–200, 1998.
[52]. Xu L, Lin W, Ma L, Zhang Y , Fang Y , Ngan KN KN, et al. , “Free-energy principle inspired 
video quality metric and its use in video coding,” IEEE Transactions on Multimedia, vol. 18, no. 
4, pp. 590–602, 2016.
[53]. Lin KY , Wang G, “Hallucinated-IQA: No-reference image quality assessment via adversarial 
learning,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 
732–741, 2018.
[54]. Ma J, Wu J, Li L, Dong W, Xie X, Shi G, “Blind image quality assessment with active 
inference,” IEEE Transactions on Image Processing,” vol. 30, pp. 3650–3663, 2021. [PubMed: 
33705313] 
[55]. Chen F, Fu H, Yu H, Chu Y , “No-Reference Image Quality Assessment Based on a Multitask 
Image Restoration Network,” Applied Sciences, vol. 13, no. 11, p. 6802, 2023.
[56]. Ho J, Jain A, Abbeel P, “Denoising diffusion probabilistic models,” Advances in Neural 
Information Processing Systems, vol. 33, pp. 6840–51, 2020.
[57]. Song Y , Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B, “Score-based generative 
modeling through stochastic differential equations,” Proceedings of the International Conference 
on Learning Representations, 2021.
[58]. Dhariwal P, Nichol A, “Diffusion models beat GANs on image synthesis,” Advances in Neural 
Information Processing Systems, vol. 34, pp. 8780–94, 2021.
[59]. Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B, “High-resolution image synthesis with 
latent diffusion models,” Proceedings of the IEEE/CVF Conference on Computer Vision and 
Pattern Recognition, pp. 10684–10695, 2022.
[60]. Song Y , Shen L, Xing L, Ermon S, “Solving Inverse Problems in Medical Imaging with 
Score-Based Generative Models,” Proceeding of the International Conference on Learning 
Representations, 2022.
[61]. Xia W, Lyu Q, Wang G, “Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20x 
Speedup,” arXiv preprint arXiv:2209.15136, 2022.
[62]. Shi Y , Wang G, “Conversion of the Mayo LDCT Data to Synthetic Equivalent through the 
Diffusion Model for Training Denoising Networks with a Theoretically Perfect Privacy,” arXiv 
preprint arXiv:2301.06604, 2023.
[63]. Wu W, Wang Y , Liu Q, Wang G, Zhang J, “Wavelet-improved Score-based Generative Model for 
Medical Imaging,” IEEE Transactions on Medical Imaging, early access, 2023.
[64]. Gao Q, Li S, Zhu M, Li D, Bian Z, Lyu Q, et al., “Blind CT image quality assessment via deep 
learning framework,” Proceeding of the IEEE Nuclear Science Symposium and Medical Imaging 
Conference, pp. 1–4, 2019.
Shi et al. Page 18
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
[65]. Li S, He J, Wang Y , Liao Y , Zeng D, Bian Z, et al. , “Blind CT image quality assessment via 
deep learning strategy: initial study,” Proceeding of Society of Photo-Optical Instrumentation 
Engineers, vol. 10577, pp. 293–297, 2018.
[66]. Online available: https://ldctiqac2023.grand-challenge.org/.
[67]. Ronneberger O, Fischer P, Brox T, “U-net: Convolutional networks for biomedical image 
segmentation,” In International Conference on Medical Image Computing and Computer-
Assisted Intervention, vol. 9351, pp. 234–241, 2015.
[68]. Online available: https://www.aapm.org/grandchallenge/lowdosect/.
[69]. Yang Q, Kalra MK, Padolc A, Li J, Hilliard E, Lai R, et al. , “Big data from CT scanning,” JSM 
Biomedical Imaging Data Papers, vol. 2, no. 1, p. 1003, 2015.
[70]. Chen C, Pytorch toolbox for image quality assessment, 6, 2022.
[71]. Sheikh HR, Bovik AC, Veciana GD, “An information fidelity criterion for image quality 
assessment using natural scene statistics,” IEEE Transactions on Image Processing, vol. 14, no. 
12, pp. 2117–2128, 2005. [PubMed: 16370464] 
Shi et al. Page 19
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 1. 
Architecture of conditional denoising diffusion probabilistic model.
Shi et al. Page 20
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 2. 
The pipeline for computing the dissimilarity map.
Shi et al. Page 21
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 3. 
Architecture of the transformer-based quality evaluator.
Shi et al. Page 22
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 4. 
The overall framework of the proposed D-BIQA method, with DMG representing the 
dissimilarity map generator depicted in Fig. 2.
Shi et al. Page 23
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 5. 
Representative image samples in the low-dose CT BIQA dataset, with the corresponding 
radiologist’s score listed below each image.
Shi et al. Page 24
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 6. 
Visualization of the primary contents produced using different reconstructed methods. (a1)–
(a3) the input distorted images at different distortion levels, of which quality scores are 
0.2, 2.0 and 4.0, respectively; (b1)–(b3) the primary contents reconstructed by RED-CNN; 
(c1)–(c3) the primary contents reconstructed by SU-Net; and (d1)–(d3) the primary contents 
reconstructed by DDPM. The display window (width/level) is set to 350/40 HU.
Shi et al. Page 25
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 7. 
Generative prior information with DDPM. (a)–(c) the input distorted images at different 
distortion levels, with image quality scores of 0.2, 2.0 and 4.0 respectively. The lower the 
quality score, the higher the distortion level and the worse the perceptual quality; (d)–(f) 
the primary contents generated by DDPM; and (g)–(i) the dissimilarity maps calculated by 
SSIM.
Shi et al. Page 26
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 8. 
Visualization of several exemplary images from the test dataset. From left to right are the 
distorted images, the weight maps, and scores. From top to bottom are five representative 
example images with the radiologist scores of 0.0, 1.0, 2.0, 3.0 and 4.0 and our model 
predictions respectively.
Shi et al. Page 27
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Figure 9. 
Clinical cadaver images at four different dose levels. From left to right, the images have the 
dose indices of 10, 20, 30, and 40 respectively. The first row shows the original distorted 
images. The second row lists the primary contents generated using our DDPM model. The 
quality scores predicted by our model are listed below the corresponding images.
Shi et al. Page 28
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Shi et al. Page 29
Table I.
Image scoring criteria.
Score Quality Diagnostic quality criteria
0 Bad Desired features are not shown
1 Poor Diagnostic interpretation is impossible
2 Fair Suitable for compromised interpretation
3 Good Good for diagnostic interpretation
4 Excellent Anatomical features are clearly visible
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Shi et al. Page 30
Table II.
Comparisons on the Low-dose CT BIQA Dataset.
Methods PLCC SROCC KROCC Overall
NIQE 0.9181 0.9335 0.7897 2.6414
DBCNN 0.9716 0.9693 0.8692 2.8103
HyperIQA 0.9680 0.9694 0.8672 2.8045
MANIQA 0.9789 0.9792 0.9041 2.8622
D-BIQA 0.9814 0.9816 0.9122 2.8753
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Shi et al. Page 31
Table III.
Results of statistical significance test.
NIQE DB-
CNN
Hyper-
IQA
MAN-
IQA
D-
BIQA
NIQE 0 −1 −1 −1 −1
DBCNN 1 0 −1 −1 −1
HyperIQA 1 1 0 −1 −1
MANIQA 1 1 1 0 −1
D-BIQA 1 1 1 1 0
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript
Shi et al. Page 32
Table IV.
Ablation experiments about different inputs.
Methods PLCC SROCC KROCC Overall
Ori 0.9789 0.9792 0.9041 2.8622
Pri 0.7543 0.7480 0.5624 2.0647
Dis 0.9690 0.9700 0.8784 2.8175
Ori+Pri 0.9779 0.9798 0.9061 2.8639
D-BIQA 0.9814 0.9816 0.9122 2.8753
IEEE Trans Med Imaging. Author manuscript; available in PMC 2025 October 28.