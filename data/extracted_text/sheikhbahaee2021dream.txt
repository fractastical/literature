Dream toExplore: Adaptive Simulations forAutonomous
Systems
A Preprint
Zahra Sheikhbahaee
David R. Cheriton School of Computer Science
University of Waterloo
200 University Avenue West
zsheikhb@uwaterloo.ca
Dongshu Luo
David R. Cheriton School of Computer Science
University of Waterloo
200 University Avenue West
d33luo@uwaterloo.ca
Blake VanBerlo
David R. Cheriton School of Computer Science
University of Waterloo
200 University Avenue West
bvanberlo@uwaterloo.ca
S. Alex Yun
David R. Cheriton School of Computer Science
University of Waterloo
200 University Avenue West
alex.yun@uwaterloo.ca
Adam Safron
Johns Hopkins University School of Medicine
733 N Broadway, Baltimore
asafron1@jhmi.edu
Jesse Hoey
David R. Cheriton School of Computer Science
University of Waterloo
200 University Avenue West
jhoey@cs.uwaterloo.ca
October 28, 2021
Abstract
One’s ability to learn a generative model of the world without supervision depends on the extent to
which one can construct abstract knowledge representations that generalize across experiences. To
this end, capturing an accurate statistical structure from observational data provides useful inductive
biases that can be transferred to novel environments. Here, we tackle the problem of learning to
control dynamical systems by applying Bayesian nonparametric methods, which is applied to solve
visual servoing tasks. This is accomplished by ﬁrst learning a state space representation, then inferring
environmental dynamics and improving the policies through imagined future trajectories. Bayesian
nonparametric models provide automatic model adaptation, which not only combats underﬁtting and
overﬁtting, but also allows the model’s unbounded dimension to be both ﬂexible and computationally
tractable. By employing Gaussian processes to discover latent world dynamics, we mitigate common
data eﬃciency issues observed in reinforcement learning and avoid introducing explicit model bias
by describing the system’s dynamics. Our algorithm jointly learns a world model and policy by
optimizing a variational lower bound of a log-likelihood with respect to the expected free energy
minimization objective function. Finally, we compare the performance of our model with the
state-of-the-art alternatives for continuous control tasks in simulated environments.
Keywords Bayesian Nonparametric ·World Models ·Reinforcement Learning ·Unsupervised Learning ·Latent
Dynamics ·Imagination
1 Introduction
In environments with high-dimensional observations, such as raw pixel visualizations, model-free reinforcement
learning (RL) methods lack sample e ﬃciency as observations must directly map to values or actions. Model-free
arXiv:2110.14157v1  [cs.LG]  27 Oct 2021
Dream to Explore A Preprint
methods are particularly undesirable for solving problems, where data collection is a challenge. Updating the policy or
value function requires several environment interactions, leading to high sample complexity. Even after a long training
period, the resultant policies lack the ﬂexibility to adapt to novel tasks in the same environment [1].
In model-based planning, on the other hand, an agent learns the dynamics of an unknown environment, often requiring
an order of magnitude less data than model-free counterparts. Furthermore, model-based RL has exhibited a promising
capability to transfer to other tasks [ 2, 3]. Developing a generative model that is able to learn abstract spatial and
temporal representations of the environment and utilizing it for policy learning with a simulated or more generalized
version of reality can address the shortcomings of model-free RL algorithms [4]. An agent that is equipped with such
a generative model of the world can synthesize alternative counterfactual simulations of possible future states in the
environment and can perform planning on the rollouts of the probable scenarios [5]. A built-in perception-action cycle
for such agents promotes predictive ability, which allows for abstract representations to be employed as the relevant
information of the world for reasoning about the future. In this perspective, an action can be formulated as a computable
objective function—namely, the path integral of variational free energy, which captures the expected evidence for
pursuing a policy, given the world model of an agent [6].
An enduring goal of artiﬁcial intelligence (AI) is to design autonomous robots. The task of autonomous agent planning
must be treated as a collection of three major components: (1) latent representation learning; (2) learning the dynamics
of the environment (prediction); and (3) planning. These three components are combined as a single end-to-end
training procedure. In practice, the collected high-dimensional observations are the consequence of latent processes
present in the environment. By directly mapping raw sensory observations to a low-dimensional latent space, one can
encode many of the high-level semantic features of the scene for the planning procedure [7]. These latent temporal
patterns of momentary changes in the environment can be modelled by recurrent algorithms, creating internal memory
representations of the dynamics of the world. We suggest that these stochastic hidden states may encapsulate uncertainty
about the state variables, so facilitating the reconstruction of observations via inverse mapping.
Given that a plethora of sensory observations are generated from a multimodal prior distribution, Gaussian mixture
priors are a reasonable choice to represent the multimodal structure present in the latent space data. A human’s ability
to learn from a stream of novel data can be mimicked by allowing the number of parameters in the generative model to
dynamically expand through the use of Bayesian nonparametric methods. Accommodating for the possibility of growth
in model complexity enables adaptation in the number of cluster components of observational data [8].
Recurrent neural network (RNN) architectures can capture temporal dependences of the input data in their dynamical
behaviour. Due to their inherent determinism, they cannot capture uncertainty in the latent space. Dynamic Bayesian
networks (DBNs), such as the state space models, may have less expressive power compared to RNNs; however, they
can represent both a transition function that describes the evolution of the internal latent state of the environment, as
well as a projection from the hidden state to the ﬁnal output [9]. Gaussian processes (GPs) delineate the probabilistic
relations between the observed output data with respect to the inputs that consider the uncertainty-as being inherited
from the data collection modality or the modelling assumptions- while also achieving good performances with smaller
datasets [10]. GP models have remarkable predictive power in describing the nonlinear dynamical evolution of complex
systems without being prone to overﬁtting. They are eﬃcient in adapting to complex environments and they resemble a
human’s ability to ﬂexibly learn dynamics for planning in changing environments [11]. We consider a deep probabilistic
autoregressive method by assuming a GP prior on the unknown transition function and on the observational model of
an environment. We introduce a tractable variational approximation to compute latent state posterior and temporal
correlations. This approach entails back propagation of errors back through time, chained from the predicted back to
the observed output of the system [12].
Structural (inductive) biases can potentially enhance (or hinder) the performance of model-based RL algorithms,
compared to their model-free counterparts. Learning the transition probabilities of the environment and incorporating
the optimization of the value functions in policy selection can embrace the best of the model-free and model-based RL
methods [13].
We tackle the planning problem for autonomous systems from the perspective of probabilistic inference and learning,
and assess our method in partially observable environments. Our contributions consist of three parts:
1. We construct an informal mental representation of the world, which can be used for planning as a kind of
value-guided construal [11]. We employ a variational autoencoder with an inﬁnite mixture of Gaussian priors
on the abstract environmental representation to garner the intricate structure of multimodal (visual) sensory
inputs. We employ a Bayesian nonparametric algorithm that is rooted in the foundations of how a rational
learners adaptively categorizes representations of the observed stimuli in a potentially unbounded number of
clusters [14].
2
Dream to Explore A Preprint
2. The agent uses a recurrent Gaussian process (RGP) model that acts as working memory to imagine the
counterfactual policies. This structure captures the propagating uncertainty across time. Further, as a Bayesian
nonparametric approach, GP can ﬂexibly model uncertainty by inferring distributions over functions. RGP
oﬀers unique meta-learning abilities that enable the learning of a stochastic processes eﬃciently from limited
observations, while generalizing over multiple tasks. This ingredient of our model captures the ordering
and recurrent features from observed sequences of data. Using imagination, one can plan on counterfactual
trajectories with a learned world model. These imagined experiences can use encoded prior knowledge to unfold
the evolution of possible action-conditioned states of the world driven by the encoded prior knowledge [15].
This is functionally similar to the default mode network of the brain, which is responsible for internally oriented
mental processes for deliberate, goal-directed tasks. This core functional network can also include episodic
memory retrieval and constructive mental simulations by taking into account environmental dynamics [16].
3. We consider control as an inference problem [17]; wherein we derive he Bellman function from an expected
free energy objective functional over augmented data, which we generate from future rollouts of potential
actions. Our hybrid model uses a stochastic actor-critic algorithm, jointly optimizing the value function and
policy. In this construct, the policy network is referred to as the actor, while the value function as the critic.
2 Related Work
2.1 Model-Free Reinforcement Learning
Eﬀective policies can be learned without explicitly constructing a dynamical model of the world [ 18]. The soft
actor-critic algorithm was proposed to enhance sample eﬃciency and reduce the sensitivity of on-policy methods to
changes in hyperparameters by introducing a maximum entropy term to the standard RL objective function, as well as
an oﬀ-policy actor-critic method for a continuous state-action spaces [19]. A policy evaluation network (i.e. critic),
which estimates the value function for a given policy, is optimized in tandem with a stochastically updated policy
network (i.e. actor).
Lee et al. [20] addressed the challenges of learning policies in high-dimensional observation spaces by introducing
an algorithm to separately learn abstract representations and deploy RL models in the learned latent space. They
derived a Bellman backup formulation of control (as inference) and employed amortized variational inference to train a
Markovian model using the actor-critic method.
In the probabilistic inference formulation of the RL problem, the variational posterior of trajectories can be computed,
conditioned on a desired outcomes [21, 22]. An expectation-maximization (EM) style algorithm was proposed wherein
the E-step the dynamics of the environment computes the value function (critic update) and the variational policy (actor
update) [23]. In the M-step, the baseline policy is updated given the variational distribution of policy and the transition
function.
2.2 Model-Based Reinforcement Learning
Model-based algorithms have addressed the sample e ﬃciency challenge in deep RL by leveraging simple function
approximators or probabilistic methods. That said, model bias can induce worse asymptotic performance under
conditions of high sample complexity. Zhang et al. [24] jointly learned the global dynamics, cost models, and abstract
representations from high-resolution images. They employed simpler linear models to allow the direction of gradients
to improve the local policies. Similarly, by using deep spatial autoencoders, low-dimensional state representations can
be learned from visual sensory data in a supervised fashion to optimize linear-Gaussian controllers, which was shown
to ﬁnd optimal trajectories for agents when combined with a guided policy search [25]. The Deep Planning Network
(PlaNet) was proposed to learn the dynamics of compact representations of world dynamics, strengthening long-term
predictions with a latent overshooting procedure in which multiple-step predictions are aﬀorded in latent space [2].
While many model-based methods suﬀer from model bias, probabilistic inference for learning control (PILCO) addresses
this shortcoming by learning a probabilistic dynamics model. PILCO utilizes GPs to explicitly incorporate model
uncertainty and estimates the policy gradients to ﬁnd locally optimal solutions for control problems [26]. Despite its
eﬃcacy for some tasks, this method suﬀers in environments with high-dimensional spaces or discernible nonlinear
dynamics. To improve performance under conditions of high sample complexity, the union of model-based controllers
(such as model predictive control) with a policy gradient algorithm for model-free ﬁne-tuning has achieved high
performance for some challenging problems [27].
3
Dream to Explore A Preprint
Xt
zt
ct
µct ,Σct
wt
θt α
i = 1,...,N
k = 1,..,∞
Figure 1: The generative model, with an inﬁnite mixture of Gaussians as the prior for the variational autoencoder:
We use a stick-breaking construction and learn the low-dimensional representation of the environment. Here Xt is
environmental observation and zt is the continuous latent space.
2.3 Counterfactual Planning
World models can be built by learning both the latent representations of temporal and spatial sensory observations [5, 2].
The application of RNNs entails prediction on how the world evolves based on previous memories [ 28]. Of note
is Ha & Schmidhuber’s work, in which training took place inside hallucinated dreams generated by an agent’s own
world model, utilizing a low-parameter controller. Also signiﬁcant is the DreamerV2 model, in which an actor-critic
algorithm leveraged a generative world model to learn optimal behaviours, achieving human-level performance on Atari
benchmarks [4]. Further, these agents were able to learn policies using data augmentation from imagined dynamics,
enabling adaptation to novel environments [29].
3 Preliminaries
We study autonomous systems in a partially observable Markov decision process (POMDP) setting. We deﬁne a tuple
M= (X,Z,A,P,r,γ), where xt ∈X and zt ∈Z are observations and states, respectively. A is a set of continuous actions,
P delineates the transition probabilities P(zt+1|zt,at), r : Z ×A →R deﬁnes the reward function, and γis a discount
factor to guarantee the ﬁnite sum of expected rewards for an inﬁnite horizon. The goal of conventional RL algorithm is
to learn some policy, π(at|zt) under a trajectory distribution ρπ, which seeks to maximize the sum of future rewards.
3.1 Learning the World Model with an Inﬁnite Gaussian Mixture Variational Autoencoder
Impeccable perception is a key quality that an autonomous agent must possess to have successful interactions with the
world. In our model, the latent representations of the sensory data are generated by considering an inﬁnite mixture-of-
Gaussians prior for learning a reliable representation of the world. Such a method oﬀers immense modelling ﬂexibility,
structured interpretability, and avoids the loss of rich semantics present in visual data [30]. The assumption of a single
Gaussian, despite imposing strong constraints on the true posterior (which is often multimodal), can lead to poor model
performance.
The Dirichlet process (DP) is a prior probability distribution to capture nonparametric Bayesian problems. Construction
of the DP using a stick-breaking process can be introduced as an inﬁnite sum of an atomic distribution [31]. The DP
comprises a concentration parameter α∈R+, and a probability measure G0, which is referred to as the base measure,
where G ∼DP(α,G0). The stick-breaking representation of G is given by
θi(ν) = νi
i−1∏
j=1
(1 −νj)
G =
∞∑
i=1
θi(ν) δψi
(1)
4
Dream to Explore A Preprint
where θi(ν) is the inﬁnite vector of mixing proportions and ψ1,ψ2,.. deﬁnes the atoms representing the mixture
components [32].
The perception in our model is designed by considering a ﬂexible prior to accommodate many diﬀerent structures in the
world, while preferring to expand parsimoniously. The dimension of latent representation can grow adaptively once
the agent encounters novel observations. The generative process that predicts future observations given the perceptual
world model of an agent can be deﬁned as
xt |zt; φ∼N
(
µ,diag(σ2)
)
zt |ct,wt ∼
N∏
n=1
∞∏
k=1
N
(
µck
t
(wt; β),diag(σ2
ck
t
(wt; β)))cnk
t
wt ∼N
(
0,I
)
ct |θt ∼Cat
(
θ(ν)
)
νi ∼Beta(1,α)
(2)
where θk is the prior mixing weights for cluster ct (∑
k=1 θK = 1), µc and σ2
c are the mean and the variance of the
Gaussian distribution corresponding to cluster ct in the latent space, and observation x is a neural network whose input
is z and is parametrized by φ.
The prior distribution over the latent space z|w is a Gaussian mixture model whose means and variances are speciﬁed
by another neural network, parametrized by βwith input variable w, which can capture more subtle properties of data
in the latent space such as “style” [ 33]. We assume the mean-ﬁeld variational family to approximate the posterior
distributions. The variational lower bound for ﬁnding the abstract latent states of the environment can be written as
LiGMMV AE= Eq(zt|xt)
[
log p(xt|zt)
]
                                      
the reconstruction term
−DKL
[
q(wt|xt)||p(wt)
]
−Ep(ct|zt,wt)q(wt|xt)
[
DKL
[
q(zt|xt)||p(zt|ct,wt)
]]
−Eq(θt|xt)q(wt|xt)q(zt|xt)
[
DKL
[
p(ct|zt,wt)||p(ct|θt)
]]
−DKL
[
q(θt|xt)||p(θt|α)
]
(3)
The reconstruction term in this objective function encourages the decoder to reconstruct the data when employing
samples from the latent distribution z and the KL divergence terms can be viewed as regularization terms. Following
the assumption in [33], the posterior distribution of the indicator variable of each cluster can be obtained by
qφ(ct|xt) = pβ(ct,j = 1|zt,wt) =
πjN
(
µj,diag(σ2
j )
)
∑∞
k=1 πjN
(
µk,diag(σ2
k )
) (4)
We employ the Gumbel-Softmax distribution as categorical prior which is diﬀerentiable and allows us to backpropagate
through c [34].
We compute the ﬁnal KL divergence term in Equation (4) between the Kumaraswamy and the Beta distribution, which
has a closed-form [35]. The Kumaraswamy distribution q(ν|x) is considered as a doubly bounded continuous distribution
which resembles the Beta distribution when its hyperparameters are equal to one and we employed this distribution to
use its fully diﬀerentiable property for backpropagation.
3.2 Gaussian Process State-Space Model
An agent must learn the dynamics of the environments for planning, which can be transferable to di ﬀerent tasks.
The world model is the compressed sequence of observations, which can predict the consequences of actions. A
recurrent structure acts as internal memory to capture the spatial and temporal patterns in the environment. We apply an
autoregressive structure on the latent states of world which incorporates GP priors to learn the uncertainty present in
data as well as the nonlinearity in the transition dynamics, reward, and controller functions. We introduce a variational
inference procedure to integrate GP into an RNN architecture and use it for counterfactual action planning.
5
Dream to Explore A Preprint
¯xi−1 f(1)
i f(2)
i f(H+1)
i ri
¯z(1)
i ¯z(2)
i
ε(1)
i,f ε(2)
i,f
ε(r)
i,f
g(1)
i g(2)
i
a(1)
i a(2)
i
ε(1)
i,g ε(2)
i,g
Figure 2: The graphical model of Deep GP-recurrent which comprises of 2H + 1 layers for transition, reward (emission)
and controller layers. The transition, observation (f (.)) as well as controller (g(.)) functions have Gaussian process priors.
3.2.1 Recurrent Gaussian Processes
For mental simulation, one requires a predictive map of the environment. An agent’s decision-making process consists
of combining the multi-step abstract representation of the world with the evaluation of the reward for the given trajectory.
A similar method is the successor representation-Dyna oeuvre, where low-dimensional representations are stored in
the temporal context model of memory, enabling one to learn ﬂexible aﬃliated reward dynamics via oﬄine episodic
replay [36, 37]. With this idea as inspiration, we employ the recurrent GP model proposed by [38], which is similar to
RNNs.
We represent the deep structure of our probabilistic recurrent state-space model by considering H time steps for the
planning horizon, which necessitates the inclusion of a hidden layer for each transition function in our construct.
z(h)
i = f (h)(ˆz(h)
i
) + ϵ(h)
i,f , f(h) ∼N
(
0,K(h)
f
)
, 1 ≤h ≤H
a(h)
i = g(h)( ˆw(h)
i
) + ϵ(h)
i,g , g(h) ∼N
(
0,K(h)
g
)
, 1 ≤h ≤H
ri = f (H+1)(ˆz(H+1)
i
) + ϵ(H+1)
i,f , f(H+1) ∼N
(
0,K(H+1)
f
)
(5)
Above, ϵ(h)
i,f is the transition noise and ϵ(h)
i,g is the noise present in the controller function. The noise in each layer is
delineated by ϵ(h)
i ∼N (0,σ2
h). We deﬁne that the GP priors have zero mean and K(h)
f where K(h)
g are covariance
matrices with entries Ki j
f = k(zi,zj) – usually radial basis function (RBF) kernels. The inputs for each layer are given
by
ˆz(h)
i =

[¯z(1)
i−1,¯xi−1]T =[[z(1)
i−1,..,z(1)
i−M
],[xi−1,..,xi−Mx
]]T
,
h=1[¯z(h)
i−1,¯z(h−1)
i ]T =[[z(h)
i−1,..,z(h)
i−M
],[z(h−1)
i ,..,z(h−1)
i−M+1
],[a(h−1)
i−1 ,..,a(h−1)
i−La
]]T
,
1<h≤H
¯w(h)
i =[z(h)
i−1,..,z(h)
i−M+1
]T,
1<h≤H
¯z(h)
i =[[z(H)
i−1,..,z(H)
i−M+1
],[a(H)
i−1,..,a(H)
i−La
]]T
,
h=H+1
where M is the number of lagging
steps for the latent dynamical variable z(h)
i , the lag orders Mx is chosen as the exogenous inputs x(h)
i (i.e. action or
actions or both), ˆz(h)
i is the endogenous input of each layer, and ¯wi is the input of a single layer which acts as a controller
to determine the course of actions. The vector ¯z(h)
i expresses the autoregressive latent state related to the layer h in the
instant i. The key diﬀerence between this structure with previous works is the inclusion of separate functions to model
the controller in each step with its own independent noise model [39, 10].
6
Dream to Explore A Preprint
We exploit variational inference to obtain analytical forms for the posterior of f and g functions and compute a lower
bound for the marginal likelihood of f. We follow Titsias’s [40] variational sparse GP framework and we introduce ζ
and λas inducing points. The joint distribution of all the variables in the generative process is
p
(
r,a, f H+1,ζ(H+1),{zh, f h,ζh,gh,λh}|H
h=1
)
=
N∏
i=M+1
p(ri|f (H+1)
i
)p(f (H+1)
i |ζ(H+1),ˆz(H)
i
)
H∏
h=1
p(z(h)
i |f (h)
i
)p(f (h)
i |ζ(h),ˆz(h)
i
)p(a(h)
i |g(h)
i
)p(g(h)
i |λ(h),ˆz(h)
i
)
(H+1)∏
h=1
p(ζ(h))
(H)∏
h=1
p(λ(h)) M∏
i=1
H∏
h=1
p(z(h)
i
).
(6)
If we consider the mean-ﬁeld approximation, then the variational posterior distribution Q is given by
Q =
( H∏
h=1
q(z(h)))(H+1∏
h=1
q(ζ(h)))(H+1∏
h=1
N∏
i=L+1
p(f (h)
i |ζ(h),ˆz(h)
i
))
( H∏
h=1
q(λ(h)))( H∏
h=1
N∏
i=L+1
p(g(h)
i |λ(h),ˆz(h)
i
)) (7)
where inducing points ζ(h) in each layer h can be parametrized as a distribution q(ζ(h)) = N
(
ζ(h)|m(h),S(h)
)
where
h ∈{1,..., H + 1}for obtaining a non-collapsed bound which is suitable for large datasets.
Following the mean-ﬁeld approximation, the distribution of the latent variable is deﬁned as q(z(h)) =∏N
i=1 N (z(h)
i |µ(h)
i ,β(h)
i
), where µ(h) and β(h) are variational parameters. We utilize multilayer perceptron network as
recognition models for the variational parameters [10, 41]. Therefore, we have
µ(h)
i = ψµ,2
(
W(h)T
µ,2 ψµ,1
(W(h)
µ,1 ˆz(h)
i−1
))
β(h)
i = ψβ,2
(
W(h)T
β,2 ψβ,1
(W(h)
β,1 ˆz(h)
i−1
)) (8)
where W(h)T
µ,l , W(h)T
β,l , l ∈{1,2}are the weights of neural networks ad ψµ,l
(
.
)
and ψβ,l
(
.
)
express element-wise activation
functions. For β(h)
i the output of the network must be non-negative values. The lower bound of log marginal likelihood
is deﬁned as
LRGP =
log p(r,a) ≥
N∑
i=1
{ H∑
h=1
⣨
p(f (h)
i |ζ(h),ˆz(h)
i
)log p(z(h)
i |f (h)
i
)⟩
q(z)q(ζ)                                                                                      
L(h)
i
+
⣨
p(f (H+1)
i |ζ(H+1),ˆz(H)
i
)log p(ri|f (H+1)
i
)⟩
q(z)q(ζ)                                                                                                        
L(H+1)
i
}
+
N∑
i=1
H∑
h=1
⣨
p(g(h)
i |λ(h),ˆz(h)
i
)log p(a(h)
i |g(h)
i
)⟩
q(z)q(λ)                                                                                      
J(h)
i
−
N∑
i=1
H∑
h=1
⣨
log q(z(h)
i )
⟩
q(z)                          
H (h)
i
+
L∑
i=1
H∑
h=1
⣨
log p(z(h)
i )
⟩
q(z)                          
L(h)
0i
−
H+1∑
h=1
KL
[
q(ζ(h))||p(ζ(h))
]
−
H∑
h=1
KL
[
q(λ(h))||p(λ(h))
]
(9)
where H h
i |H
h=1 contains the entropy terms and L(h)
0i expresses the initial conditions imposed by computing the expected
value of the priors p(z(h)
i
) = N
(
z(h)
i |µ(h)
0i ,β(h)
0i
)
which can be optimized with the variational parameters or they can be
7
Dream to Explore A Preprint
ﬁxed, e.g., µ(h)
0i = 0 and β(h)
0i = 1. The terms L(h)
i |H
h=1 include learning the transition dynamics, the L(H)
i comprises the
reward in the environment and J(h)
i is responsible for inferring the controller functions at each time step. Here we
can compute p(f (h)
i |ζ(h),ˆz(h)
i
) = N (
b(h)
f
                          
K(h)
zζ (K(h)
ζ )−1ζ(h),
Σ(h)
f
                                              
K(h)
z −K(h)
zζ (K(h)
ζ )−1K(h)
ζz
) and similarly we have p(g(h)
i |λ(h),ˆz(h)
i
) =
N (K(h)
zλ (K(h)
λ )−1λ(h),K(h)
z −K(h)
zλ (K(h)
λ )−1K(h)
λz
).
The derivation of diﬀerent components of the bound which are given in eq. (9) was provided inside section (7) of the
supplementary material.
In order to employ the model for a large dataset, we can consider a mini-batch B, which can be a set of B sequential
indexes sampled from the training data. We can perform stochastic optimization over mini-batches on a non-collapsed
variational bound. Although this structure is closely related to that of traditional RNNs, it takes into account the
stochasticity of the data and the environment.
The loss functions for the inﬁnite Gaussian Mixture V AE, LiGMMV AE, and for the deep recurrent Gaussian Process,
LRGP, are optimized jointly using the Adam optimiser to learn the world model.
4 Planning
Similar to Hafner et al. [ 2], we can use model-predictive control for planning based on new observations. That is,
we can replan at each step by simply utilizing the RGP algorithm for a limited time horizon or trajectory rollouts.
However, we combine both model-based and model-free methods to increase robustness of our model to imperfections
and improve sample eﬃciency [1, 42]. The RGP serves as a predictive model of the future and generates imaginative
rollouts a few steps in the future with diﬀerent random initializations. The forward predictions contain a sequence of
{zt,at,rt}H
t=1 in the latent space and are fed to a stochastic latent actor-critic to compute the optimal policy and estimate
the value function.
We begin by considering planning as inference and we formalize it in terms of the expected free energy [43, 21]. We
can derive the the Bellman backup as a recursive logic over the agent’s trajectory and we deﬁne the binary variable
Ot which denotes that time step t is optimal given p(Ot = 1|zt,at) = exp(r(zt,at)). First, we start with the evidence
lower-bound (ELBO) of the log-likelihood function
G(aτ,zτ)
= Eqφ(aτ:T |zτ:T )qφ(zτ:T |aτ:T )
[
log
( T∏
t=τ
[
pθ(zt|at−1,zt−1)pθ(at|zt)
]
exp
[
η
T∑
t=τ
r(zt,at)
])
−log
( T∏
t=τ
qφ
(at|zt
) qφ
(zt+1|at,zt
))]
=
T∑
t=τ
Eqφ
(
zt+1|at,zt
)
[
ηEqφ
(
at|zt
)[
r(zt,at
)]
−DKL
[
qφ(at|zt)
⏐⏐⏐⏐⏐⏐pθ(at|zt)
]]
                                                                                                                                                        
policy objective term
−Eqφ
(
at|zt
)
[
DKL
[
qφ
(zt+1|at,zt
)⏐⏐⏐⏐⏐⏐pθ
(zt+1|at,zt
)]]
                                                                                                    
dynamic objective term
(10)
where ηis a temperature parameter which identiﬁes the relative importance of the entropy term against the reward in
the environment, aﬀecting the degree of stochasticity in the optimal policy. The third term is the cross-entropy between
the posterior dynamics p(zt+1|at,zt,O1:T ) and the true dynamics p(zt+1|at,zt). In our setting, the agent’s goal is to ﬁnd
a policy which maximizes the expected return by estimating the probability of a system trajectory under each policy
computed in the latent space.
We can rewrite the above RL objective in terms of backward messages to obtain standard Bellman-like operator [23]
which is provided in the supplementary materials section (8).
By passing messages backward through time, we can minimize the soft Bellman residual, while the KL divergence term
as a regularizer becomes minimized when two distributions become as close as possible to each other. The goal is to
update the policy in order to be optimized with respect to the critic; therefore, the optimal policy minimizes the KL
divergence in the last line of above equation [20].For the policy improvement step, we optimize the variational policy
8
Dream to Explore A Preprint
Algorithm Dream to Explore
Input:
1: p(zt|zt−1,at−1) ⊿Transition model
2: P(xt|zt) ⊿Observation model
3: P(rt|zt,at) ⊿Reward model
4: Initialize dataset D with M random seed episodes
5: while not converged do
6: for update step z = 1,..., R do
7: Draw chunks
{[
at,rt,zt
]Chunk
k=1
}Batch
i=1
8: Run recurrent GP model
9: Compute LiGMMV AE+ LRGP and update world model parameters
10: end for
11: o1 ←env.reset()
12: for time step t = 1,..., T do
13: at ← Planner(ot, iGMMVAE, RGP)
14: ot+1,rt ←env.step(at)
15: end for
16: D ←D ∪
{
(ot,at,rt)T
t=1,oT+1
}
17: for each gradient step do
18: φ←φ−λπ∇φJπ(φ) ⊿Update Actor
19: θ←θ−λQ∇Q JQ(θ)
20: ψ←ψ−λV ∇ψJV (ψ) ⊿Update Critic
21: end for
22: end while
distribution by minimizing the following loss function
Jπ(φ) = Ezτ+1∼P(.|zτ,aτ)
[
Ea∼P(.|zτ+1)
{
log q(aτ+1|zτ+1)
p(aτ+1|zτ+1)
−Q(zτ+1,aτ+1) + V(zτ+1)
}] (11)
where the soft value function can be optimized using the following loss objective [23]:
JV (ψ) = Ezτ+1∼P(.|zτ,aτ)
[1
2
{
V(zτ+1) −Eqφ(aτ+1|zτ+1)
(
Q(zτ+1,aτ+1)
−log qφ(aτ+1|zτ+1)
p(aτ+1|zτ+1)
−log q(zτ+2|aτ+1,zτ+1)
p(zτ+2|aτ+1,zτ+1)
)}2]
(12)
The second and the third terms in the objective function guarantee incorporation of risk-seeking policies and improve
exploration by promoting diverse behaviors.
Similar to [20], we minimize the temporal diﬀerence error using the critic network with the following loss objective:
JQ(θ) = Ezt∼q(.|zt−1,at−1),at∼q(.|zt)
[1
2
{
Q(zt,at) −
(
ηr(at,zt)
+ γlog Ezt+1∼q(zt+1|zt,at)
[
exp
(
V(zt+1)
)]) }2] (13)
separate policy and value function networks. Dream to Explore is outlined in Algorithm (1).
Haarnoja et al. [19] assumed that value functions can be modelled as neural networks, with the policy as a Gaussian
with mean and covariance given by neural networks. We consider Gaussian process priors for the transition distribution,
while for the policy distribution we take advantage of amortized variational inference (A VI). Speciﬁcally, we use MLPs
to represent its posterior distribution.
9
Dream to Explore A Preprint
5 Experimental Evaluation
The performance and sample complexity of D2E was compared against a model-based method (PlaNet) [ 2] and
model-free method (DreamerV2) [4]. We evaluate these algorithms on four challenging continuous benchmark OpenAI
Gym environments (Hopper, Humanoid, Walker2D, and HalfCheetah) [ 44]. Details on hyperparameters, network
architecture, ablation study and further results are included in Section 9 of Supplementary Materials. As in [ 4],
observations were a 64 ×64 visual representation of the environment, rather than the default low-dimensional state.
The Humanoid environment is particularly hard to solve and our model outperforms DreamerV2 within signiﬁcantly
fewer steps. Note that in the HalfCheetah environment, the agent starts the training process in the later steps in D2E
compared to DreamerV2; this is because the episode length for HalfCheetah environment is much longer than in the
other environments. Since D2E collects 100 episodes as prior experience and DreamerV2 initially collects 10000 steps,
the number of steps in D2E’s prior experience greatly outnumbers those in DreamerV2.
Figure 3: We compare dream to explore method (D2E) against baselines for diﬀerent continuous benchmarks.
10
Dream to Explore A Preprint
6 Conclusion
We present Dream to Explore, a nonparametric RL algorithm which provides improved sample eﬃciency compared
to deep model-free RL methods. We empirically compare the performance of our model against a model-free and a
model-based RL method. By incorporating stochasticity in our objectives, we allow an agent to explore unknown states
in environments, which can improve the robustness and stability of the model. Dream to Explore encourages an agent
to discover to an optimal policy with less number of interactions with a novel environment. Our model mimics the
ﬂexibility of a human brain by enabling the expansion of inductive priors and applying them to perform tasks entirely
inside of an imagined latent space world. In future work, we plan to produce rigorous theoretical analyses for each
component of our algorithm.
References
[1] T. Weber, S. Racanière, D. P. Reichert, L. Buesing, A. Guez, A. P. Rezende, D. J. andd Badia, O. Vinyals, N. Heess,
Y . Li, R. Pascanu, P. Battaglia, D. Hassabis, D. Silver, and D. Wierstra. Imagination-augmented agents for deep
reinforcement learning. In Advances in Neural Information Processing Systems, 2017.
[2] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, DD. Ha, H. Lee, and J. Davidson. Learning latent dynamics
for planning from pixels. Proceedings of the 36th International Conference on Machine Learning (ICML) ,
97:2555–2565, 2019.
[3] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination.
International Conference on Learning Representations (ICLR), 2020.
[4] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. International
Conference on Learning Representations (ICLR), 2021.
[5] D. Ha and Schmidhuber. World models. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
[6] K. Friston, R. J. Moran, Y . Nagai, T. Taniguchi, H. Gomi, and J. Tenenbaum. World model learning and inference.
Neural Networks, 2021.
[7] A. Amini, W. Schwarting, G. Rosman, B. Araki, S. Karaman, and D. Rus. Variational autoencoder for end-to-end
control of autonomous driving with novelty detection and training de-biasing. International Conference on
Intelligent Robots and Systems (IROS), 2018.
[8] M. E. Abbasnejad, A. Dick, and A. van den Hengel. Inﬁnite variational autoencoder for semi-supervised learning.
Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
[9] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and Bengio. A recurrent latent variable model for sequential
data. In Advances in Neural Information Processing Systems (NeurIPS), 2015.
[10] C. L. Mattos and G. A. Barreto. A stochastic variational framework for recurrent gaussian processes models.
Neural Networks, 112:54–72, 2019.
[11] M. K. Ho, D. Abel, C. G. Correa, M. L. Littman, J. D. Cohen, and T. L. Griﬃths. Control of mental representations
in human planning. arXiv preprint arXiv:2105.06948, 2021.
[12] A. Doerr, C. Daniel, M. Schiegg, D. Nguyen-Tuong, S. Schaal, M. Toussaint, and S. Trimpe. Probabilistic
recurrent state-space models. International Conference on Machine Learning (ICML), 2018.
[13] A. H. Farahmand. Iterative value-aware model learning. Advances in Neural Information Processing Systems 31,
page 9072–9083, 2018.
[14] T. L. Griﬃths, A. N. Sanborn, K. R. Canini, D. J. Navarro, and J. B. Tenenbaum. Nonparametric bayesian models
of categorization. In E. M. Pothos and A. J. Wills, editors, Formal Approaches in Categorization, chapter 8, page
173–198. Cambridge University Press, 2011.
[15] J. Jin, D. Graves, C. Haigh, J. Luo, and M. Jagersand. Oﬄine learning of counterfactual perception as prediction
for real-world robotic reinforcement learning. arXiv preprint arXiv:2011.05857, 2020.
[16] K. Christoﬀ, Z. C. Irving, K. C. R. Fox, R. N. Spreng, and J. R. Andrews-Hanna. Mind-wandering as spontaneous
thought: a dynamic framework. Nature Reviews Neuroscience, 17:718–73, 2016.
[17] J. Fu, A. Singh, D. Ghosh, L. Yang, and S. Levine. Variational inverse control with events: A general framework
for data-driven reward deﬁnition. In Advances in Neural Information Processing Systems, 2018.
[18] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of
the 32nd International Conference on Machine Learning, page 1889–1897, 2015.
11
Dream to Explore A Preprint
[19] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Oﬀ-policy maximum entropy deep reinforcement
learning with a stochastic actor. International Conference on Machine Learning (ICML), 2018.
[20] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement learning
with a latent variable model. Advances in Neural Information Processing Systems, 2020.
[21] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv:1805.00909,
2018.
[22] LK. Friston, L. Da Costa, D. Hafner, C. Hesp, and T. Parr. Sophisticated inference. Neural Computation,
33:713–763, 2021.
[23] Y . Chow, B. Cui, M. Ryu, and M. Ghavamzadeh. Variational model-based policy optimization.arXiv preprint
arXiv:2006.05443v2, 2020.
[24] M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. J. Johnson, and S. Levine. Solar: Deep structured representations
for model-based reinforcement learning. arXiv preprint arXiv:1808.09105, 2018.
[25] C. Finn, X. Y . Tan, Y . Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for visuomotor
learning. International Conference on Robotics and Automation (ICRA), 2016.
[26] M. C. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-e ﬃcient approach to policy search. 28th
International Conference on Machine Learning (ICML), 2011.
[27] A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforce-
ment learning with model-free ﬁne-tuning. IEEE International Conference on Robotics and Automation (ICRA),
2018.
[28] L. Kaiser, M. Babbaeizadeh, P. Milos, B. Osi´nski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Koza-
kowski, S. Levine, A. Mohiuddin, R. Sepassi, G. Tucker, and H. Michalewski. Model-based reinforcement
learning for atari. Ninth International Conference on Learning Representations (ICLR), 2021.
[29] P. J. Ball, C. Lu, J. Parker-Holder, and S. Roberts. Augmented world models facilitate zero-shot dynamics
generalization from a single oﬄine environment. Proceedings of the 38th International Conference on Machine
Learning, 2021.
[30] Z. Jiang, Y . Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: An unsupervised and generative
approach to clustering. IJCAI’17: Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence,
2017.
[31] D. Görür and C. E. Rasmussen. Dirichlet process gaussian mixture models: Choice of the base distribution.
Journal Of Computer Science and Technology, 25:615–626, 2010.
[32] J. Sethuraman. A constructive deﬁnition of dirichlet priors. Statistica Sinica, 4:639–650, 1994.
[33] N. Dilokthanakul, P. A. M. Mediano, M. Garnelo, M. C. H. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan.
Deep unsupervised clustering with gaussian mixture variational autoencoders. International Conference on
Learning Representations, 2017.
[34] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. International Conference on
Learning Representations, 2017.
[35] E. Nalisnick, L. Hertel, and P. Smyth. Approximate inference for deep latent gaussian mixtures. NeurIPS
Workshop: Bayesian Deep Learning, 2016.
[36] I. Momennejad, E.M. Russek, J.H. Cheong, M. M. Botvinick, N. D. Daw, and S. J. Gershman. The successor
representation in human reinforcement learning. Nature Human Behaviour, 1:680–692, 2017.
[37] I. Momennejad. Learning structures: Predictive representations, replay, and generalization. Current Opinion in
Behavioral Sciences, 32:155–166, 2020.
[38] C. L. C. Mattos, Z. Dai, A. Damianou, J. Forth, G. A. Barreto, and N. D. Lawrence. Recurrent gaussian processes.
International Conference on Learning Representations, 2016.
[39] C. L. C. Mattos. Recurrent gaussian processes and robust dynamical modeling. PhD thesis, 2017.
[40] M. K. Titsias. Variational learning of inducing variables in sparse gaussian processes. In International Conference
on Artiﬁcial Intelligence and Statistics, page 567–574, 2009.
[41] T. D. Bui and R. E. Turner. Stochastic variational inference for gaussian process latent variable models using back
constraints. NeurIPS Workshop on Black Box Learning and Inference, 2015.
[42] L. Buesing, T. Weber, S. Racanière, S. M. A. Eslami, D. Rezende, D. Reichert, F. Viola, F. Besse, K. Gregor,
D. Hassabis, and D. Wierstra. Learning and querying fast generative models for reinforcement learning. arXiv
preprint arXiv:1802.03006, 2018.
12
Dream to Explore A Preprint
[43] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo. Active inference: A process theory.Neural
Computation, 29:1–49, 2017.
[44] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and
machine learning. http://pybullet.org, 2016–2021.
13
Dream to Explore A Preprint
Supplementary Materials
7 Derivation of Bound for the Recurrent Gaussian Processes
The L(H+1)
i is related to the observation function which here it maps the states into the reward r as one component of
the sparse GP variational bound which is given in Eq. (9). We have
L(H+1)
i =
∫
q(z(H))q(ζ(H+1))[
−1
2 log 2πσ2
H+1 − 1
2σ2
H+1
(
rT r −2rT b(H+1)
f + (b(H+1)
f )T b(H+1)
f + Tr(Σ(H+1)
f )
)]
dz(H) dζ(H+1)
= −1
2 log 2πσ2
H+1 +
∫
q(ζ(H+1))[
− 1
2σ2
H+1
(
rT r −2rT ⟨K(H+1)
zζ
⟩
q
(
z(H)
)(K(H+1)
ζ )−1ζ(H+1)
+ (ζ(H+1))T (K(H+1)
ζ )−1⟨K(H+1)
ζz K(H+1)
zζ
⟩
q
(
z(H)
)(K(H+1)
ζ )−1ζ(H+1) + Tr
(⟨K(H+1)
z
⟩
q
(
z(H)
)
)
−Tr
((K(H+1)
ζ
)−1⟨K(H+1)
zζ K(H+1)
ζz
⟩
q
(
z(H)
)))]
dζ(H+1)
(14)
where K(H+1)
ζ is the covariance matrix calculated from the pseudo-inputs and K(H+1)
zζ = k(z(H+1),ζ(H+1)). Now we deﬁne
Ψ(H+1)
0 = ⟨K(H+1)
z
⟩
q
(
z(H)
), Ψ(H+1)
1 = ⟨K(H+1)
zζ
⟩
q
(
z(H)
) and Ψ(H+1)
2 = ⟨K(H+1)
zζ K(H+1)
ζz
⟩
q
(
z(H)
) and rewrite the above equation
similar to [38]. Then we add the KL divergence term related to the ζ(H+1) in the last line of to Eq. (9) the above equation
L(H+1)
i
∗
= −1
2 log 2πσ2
H+1 − 1
2σ2
H+1
rT r − 1
2σ2
H+1
Ψ(H+1)
0 + 1
2σ2
H+1
Tr
((K(H+1)
ζ
)−1Ψ(H+1)
2
)
+
∫
q(ζ(H+1))[
− 1
2σ2
H+1
{
(ζ(H+1))T (K(H+1)
ζ )−1Ψ(H+1)
2 (K(H+1)
ζ )−1ζ(H+1) −2rT Ψ(H+1)
1 (K(H+1)
ζ )−1ζ(H+1)
}
                                                                                                                                                                                                                    
P(H+1)
−log q(ζ(H+1))
p(ζ(H+1))
]
dζ(H+1)
= −1
2 log 2πσ2
H+1 − 1
2σ2
H+1
rT r − 1
2σ2
H+1
Ψ(H+1)
0 + 1
2σ2
H+1
Tr
((K(H+1)
ζ
)−1Ψ(H+1)
2
)
+ 1
2σ2
H+1
2rT Ψ(H+1)
1 (K(H+1)
ζ )−1m(H+1) − 1
2σ2
H+1
(K(H+1)
ζ )−1Ψ(H+1)
2 (K(H+1)
ζ )−1
((m(H+1))T m(H+1) + S(H+1)
)
−1
2
[
Tr
((K(H+1)
ζ
)−1S(H+1)
)
+ (m(H+1))T (K(H+1)
ζ
)−1m(H+1) −M + log
|K(H+1)
ζ |
|S(H+1)|
]
(15)
here, by maximizing the bound with respect to q(ζ(H+1)), one can obtain the variational distribution
q∗(ζ(H+1)) ∝p(ζ(H+1))exp(P(H+1)). (16)
The KL term with respect to the latent variable takes the following form:
KL
[
q(z(h)
i )||p(z(h)
i )
]
= H (h)
i + L(h)
0i
= 1
2
[
Tr
{(β(h)
0i
)−1β(h)
i + (µ(h)
0i −µ(h)
i
)T (β(h)
0i
)−1(µ(h)
0i −µ(h)
i
)}
+ log
|β(h)
0i |
|β(h)
0i |
−N
]
.
(17)
14
Dream to Explore A Preprint
and now we can also calculate J(h)
i term which is done in similar fashion as the L(H+1)
i term. In fact, we compute the
components of the bound which is directly responsible for learning the non-linear controller function in our model
J(h)
i =
∫
q(z(h))q(λ(h))[
−1
2 log 2πσ2
g,h − 1
2σ2
g,h
(
aT a −2aT b(h)
g + (b(h)
g )T b(h)
g + Tr(Σ(h)
g )
)]
dz(h) dλ(h)
= −1
2 log 2πσ2
g,h +
∫
q(λ(h))[
− 1
2σ2
g,h
(
aT a −2aT ⟨K(h)
zλ
⟩
q
(
z(h
)(K(h)
λ )−1λ(h)
+ (λ(h))T (K(h)
λ )−1⟨K(h)
λz K(h)
zλ
⟩
q
(
z(h)
)(K(h)
λ )−1λ(h) + Tr
(⟨K(h)
z
⟩
q
(
z(h)
)
)
−Tr
((K(h)
λ
)−1⟨K(h)
zλ K(h)
λz
⟩
q
(
z(h)
)))]
dλ(h)
(18)
where extra inducing points are deﬁned to be evaluated in pseudo-points λand those are used to sample the GP that
models g(h)(.), the controller function. The remainder of the computation of above equation is similar to Equation (15).
Finally, we must compute
⣨
p(f (h)
i |ζ(h),ˆz(h)
i
)⟩
q(z)q(ζ) in order to make predictions by ﬁrst estimating the expectation with
respect to q(ζ) = N(m(h),S(h)). Now we have:
p(f (h)
∗ |ˆz(h)
∗
) = N( f (h)
∗ |
µ(h)
                            
K(h)
zζ (K(h)
ζ )−1m(h),
(σ(h))2
                                                                                                                                  
K(h)
z −K(h)
zζ (K(h)
ζ )−1K(h)
ζz + K(h)
zζ (K(h)
ζ )−1S(h)(K(h)
ζ )−1K(h)
ζz ) (19)
Next, we must estimate p(f (h)
∗
) =
⣨
p(f (h)
∗ |ˆz(h)
∗
)⟩
q(z∗) ≈N
(
f (h)
∗ |µ(h)
f ∗,λ(h)
f ∗
)
for each layer:
p(f (h)
∗
) =
⣨
p(f (h)
∗ |ˆz(h)
∗
)⟩
q(z∗)
µ(h)
f ∗ = E
[
p(f (h)
∗ |ˆz(h)
∗
)]
= ⟨K(h)
zζ
⟩(K(h)
ζ )−1m(h) = Ψ(h)
1∗ (K(h)
ζ )−1m(h)
                
B(h)
λ(h)
f ∗ = Var
[
p(f (h)
∗ |ˆz(h)
∗
)]
= E
[
(σ(h))2]
+ var
[
µ(h)]
= Ψ(h)
0∗ −Tr
((
K(h)
ζ
)−1(
I −S(h)(
K(h)
ζ
)−1)
Ψ(h)
2∗
)
+
(
B(h))T
(
Ψ(h)
2∗ −
(
Ψ(h)
1∗
)T
Ψ(h)
1∗
)
B(h)
(20)
where Var[.] denotes the variance. Due to intractability of the exact distribution, a Gaussian approximation has been
used by matching the ﬁrst two moments of this distribution. Similar approximation is applied to compute g(h) as well.
8 Bellman-like Operators
Any value function satisﬁes the Bellman equationTπVπ(.) = Vπ(.), where Tπ
soft B ηr(a,s)−Es′,a′∼p(s′|a,s)p(a|s)[log q(a|s)−
log p(a|s)] is the Bellman operator
Tq(at|zt)[V](z) = Eqφ(zt|at−1,zt−1)
{
−DKL
[
qφ(at|zt)
⏐⏐⏐
⏐⏐⏐p(at|zt) exp(ηr(zt,at))]}
+ max
qφ(zt|at)
Eqφ(at|zt)
{
Eqφ(zt|at−1,zt−1)
[
V(zt)
]
−DKL
[
qφ(zt|at−1,zt−1)
⏐⏐⏐
⏐⏐⏐pθ(zt|at−1,zt−1)
]}
,
T[V](z) B max
q(at|zt)
Tq(at|zt)[V](z)
(21)
which is monotonic.
15
Dream to Explore A Preprint
We can express the policy objective term in Eq. (10) based on message passing:
E(zτ:T ,aτ:T )∼q
{ T∑
t=τ
(
log p(Ot|zt,at
) + log p(at|z1:t,a1:t−1
) −log q(at|z1:t,a1:t−1
))}
= Ezτ+1∼q(zτ+1|zτ,aτ)
{
Eaτ+1∼q(aτ+1|z1:τ+1,a1:τ)
[
Q(zτ+1,aτ+1) + log p(aτ+1|zτ+1
)p(zτ+1|z1:τ,a1:τ
)
q(aτ+1|zτ+1
)q(zτ+1|z1:τ,a1:τ
)
]}
= Eaτ+1∼q(.|z1:τ+1,a1:τ)
{
Ezτ+1∼q(.|zτ,aτ)
[
Q(zτ+1,aτ+1) + log p(zτ+1|z1:τ,a1:τ
)
q(zτ+1|z1:τ,a1:τ
) −log q(aτ+1|zτ+1
)
p(aτ+1|zτ+1
)
]}
= −DKL
[
q(aτ+1|zτ+1)
⏐⏐⏐⏐
⏐⏐⏐⏐
exp
(
Ezτ+1∼q(.|zτ,aτ)
[
log p(aτ+1|zτ+1
) + Q(aτ+1,zτ+1
)])
exp
(
Ezτ+1∼q(.|zτ,aτ)
[
V(zτ+1)
])
]
+ Ezt+1∼qt(.|zt,at)
[
V(zt+1) −log qt
(zt+1|at,zt
)
p(zt+1|at,zt
)
]
(22)
where we obtain the policy distribution as a closed-form related to the value function and the corresponding action-value
function
q(aτ|zτ) =
exp
(
Ezτ∼q(.|zτ−1,aτ−1)
[
log p(aτ|zτ
) + Q(aτ,zτ
)])
exp
(
Ezτ∼q(.|zτ−1,aτ−1)
[
V(zτ)
]) (23)
9 ADDITIONAL EXPERIMENTS
9.1 Impact of Observation Encoding Strategy
We use an inﬁnite Gaussian mixture variational autoencoder (iGMMV AE) to learn the latent representation of the
image observations for all the environments. We also implemented a version of the algorithm that employs a regular
variational autoencoder [2] which models the latent space as a single Gaussian mixture. We conducted an ablation study
comparing the two versions and the results are shown in Figure 4. In the autoencoder, we use 4 convolutional layers and
2 fully connected layers to compress the image observations.
9.2 Hyperparameter Description
Following the preprocessing strategy taken in PlaNet1, we resize image observation to 64 ×64. After applying the
iGMMV AE to encode the resized image observations to produce latent vectors of size 10. We employ a recurrent
Gaussian process model (RGP) [38] to learn to predict cumulative rewards and trajectories for a future with a ﬁnite
horizon. The RGP model learns entirely from observations encoded in the latent space. We adopt a horizon size of 5
and lagging size of 2 in RGP. Note that horizon size refers to the number of future steps that the RGP imagines, and the
lagging size refers to the number of past latent state/action pairs that the RGP model accepts as input to predict future
trajectories.
The iGMMV AE and RGP models are jointly learned during the ﬁrst phase of each iteration. In this phase, we perform
100 steps of parameter optimization. In each step, we sample 50 batches of episode chunks of length 10 from the replay
buﬀer and feed them to the iGMMV AE and RGP models to perform parameter updates. We use the Adam optimizer [3]
with learning rate of 1 ×10−3, epsilon value of 1 ×10−4, and gradient clipping norm of 1000 (as in PlaNet [2]).
In the second phase of the algorithm’s iteration, we use RGP and stochastic latent actor-critic [20] to collect 1 episode
as additional data and feed this episode back to the replay buﬀer. Although our RGP model imagines for 5 steps in the
future, we only plan one step ahead in the planning part of the algorithm. We use a discount factor of 0.999 and the
temperature factor of 1. Also, we use the Gaussian process module in2 [4] to model the transition distribution in the
actor-critic part of the planning algorithm.
9.3 More Results
We compare the performance of DreamerV2 and our algorithm in three additional environments: Cartpole, SMARTS
loop scenario, and SMARTS intersection 4lane scenario [1]. Further, we include the performance of our algorithm
for both the case when an inﬁnite Gaussian mixture is used in the V AE and when the single Gaussian mixture is used.
Therefore, there are 3 algorithms in each plot.
1https://github.com/Kaixhin/PlaNet
2https://github.com/EmbodiedVision/dlgpd
16
Dream to Explore A Preprint
References
[1] Zhou, M. and Luo, J. and Villella, J. and Yang, Y . and Rusu, D. and Miao, J. and Zhang, W. and Alban, M. and
Fadakar, I. and Chen, Z. et al., SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for
Autonomous Driving, In Proceedings of the 4th Conference on Robot Learning (CoRL), 2020.
[2] Kingma, D. P. and Welling, M., Auto-encoding variational bayes, arXiv preprint arXiv:1312.6114, 2013.
[3] Kingma, D. P. and Ba, J., Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014.
[4] Bosch, N. and Achterhold, J. and Leal-Taixe, L. and Stueckler, J., Planning from Images with Deep Latent
Gaussian Process Dynamics, Proceedings of the 2nd Conference on Learning for Dynamics and Control, 120,
640–650, 2020.
17
Dream to Explore A Preprint
Figure 4: We compare dream to explore method with a regular V AE and inﬁnite Gaussian Mixture V AE (D2E) against
dreamer V2.
18