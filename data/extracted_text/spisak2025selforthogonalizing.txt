5202
yaM
82
]CN.oib-q[
1v94722.5052:viXra
Self-orthogonalizing attractor neural networks
emerging from the free energy principle
Tamas Spisak1
Center for Translational Neuro- and Behavioral Sciences (C-TNBS), University Medicine Essen, Germany
Karl Friston
Queen Square Institute of Neurology, University College London, WC1N 3AR, UK
VERSES, Los Angeles, CA 90067, USA
Wednesday 28th May, 2025
Abstract
Attractor dynamics are a hallmark of many complex systems, including the brain. Un-
derstanding how such self-organizing dynamics emerge from first principles is crucial for
advancingourunderstandingofneuronalcomputationsandthedesignofartificialintelligence
systems. Here we formalize how attractor networks emerge from the free energy principle
applied to a universal partitioning of random dynamical systems. Our approach obviates the
need for explicitly imposed learning and inference rules and identifies emergent, but efficient
and biologically plausible inference and learning dynamics for such self-organizing systems.
These result in a collective, multi-level Bayesian active inference process. Attractors on the
free energy landscape encode prior beliefs; inference integrates sensory data into posterior
beliefs; and learning fine-tunes couplings to minimize long-term surprise. Analytically and
via simulations, we establish that the proposed networks favor approximately orthogonalized
attractor representations, a consequence of simultaneously optimizing predictive accuracy
and model complexity. These attractors efficiently span the input subspace, enhancing
generalization and the mutual information between hidden causes and observable effects.
Furthermore, while random data presentation leads to symmetric and sparse couplings,
sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings offer a
unifying theory of self-organizing attractor networks, providing novel insights for AI and
neuroscience.
Keywords attractor networks, active inference, orthogonal representations, self-organization
Interactive manuscript: https://pni-lab.github.io/fep-attractor-network
Graphical Abstract
1Correspondence to: tamas.spisak@uk-essen.de
1 Highlights
• Attractor networks are derived from the Free Energy Principle (FEP) applied to a universal parti-
tioning of random dynamical systems.
• This approach yields emergent, biologically plausible inference and learning dynamics, forming a
multi-level Bayesian active inference process.
• The networks favor approximately orthogonalized attractor representations, optimizing predictive
accuracy and model complexity.
• Sequentialdatapresentationleadstoasymmetriccouplingsandnon-equilibriumsteady-statedynamics,
extending conventional Boltzmann Machines.
• Simulationsdemonstrateorthogonalbasisformation,generalization,sequencelearning,andresistance
to catastrophic forgetting.
2 Introduction
Fromwhirlpoolsandbirdflockstoneuronalandsocialnetworks,countlessnaturalsystemscanbecharacterized
bydynamicsorganizedaroundattractorstates[Haken,1978]. Suchsystemscanbedecomposedintoacollection
of - less or more complex - building blocks or “particles” (e.g. water molecules, birds, neurons, or people),
which are coupled through local interactions. Attractors are an emergent consequence of the collective
dynamics of the system, arising from these local interactions, without any individual particles exerting global
control.
Attractors are a key concept in dynamical systems theory, defined as a set of states in the state space of
the system to which nearby trajectories converge [Guckenheimer et al., 1984]. Geometrically, the simplest
attractors are fixed points and limit cycles (representing periodic oscillations). However, the concept extends
to more complex structures like strange attractors associated with chaotic behavior, as well as phenomena
arising in stochastic or non-equilibrium settings, such as probability distributions over states (stochastic
attractors),transientstatesreflectingpastdynamics(ghostattractorsorattractorruins),andtrajectoriesthat
cycle through sequences of unstable states (sequential attractors or heteroclinic cycles). Artificial attractor
neural networks [Amit, 1989] represent a class of recurrent neural networks specifically designed to leverage
attractor dynamics. While the specific forms and behaviors of these networks are heavily influenced by the
chosen inference and learning rules, self-organization is a key feature of all variants, as the stable states
emerge from the interactions between network elements without explicit external coordination. This property
makes them particularly relevant as models for self-organizing biological systems, including the brain. It
is clear that the brain is also a complex attractor network. Attractor dynamics have long been proposed
to play a significant role in information integration at the circuit level [Freeman, 1987, Amit, 1989, Deco
and Rolls, 2003, Nartallo-Kaluarachchi et al., 2025, Tsodyks, 1999] and have become established models
for canonical brain circuits involved in motor control, sensory amplification, motion integration, evidence
integration, memory, decision-making, and spatial navigation (see Khona and Fiete [2022] for a review).
For instance, the activity of head direction cells - neurons that fire in a direction-dependent manner - are
known to arise from a circular attractor state, produced by a so-called ring attractor network [Zhang, 1996].
Multi- and metastable attractor dynamics have also been proposed to extend to the meso- and macro-scales
[Rolls, 2009], “accommodating the coordination of heterogeneous elements” [Kelso, 2012], rendering attractor
dynamics an overarching computational mechanism across different scales of brain function. The brain, as an
instance of complex attractor networks, showcases not only the computational capabilities of this network
architecture but also its ability to emerge and evolve through self-organization.
When discussing self-organization in attractor networks, we will differentiate two distinct levels. First, we can
talk about operational self-organization: the capacity of a pre-formed network to settle into attractor states
during its operation. This however does not encompass the network’s ability to “build itself” – to emerge
from simple, local interactions without explicit programming or global control, and to adaptively evolve its
structure and function through learning. This latter level of self-organization is what we will refer to as
adaptive self-organization. Such architectures would mirror the nervous system’s capacity to not just function
as an attractor network, but to become - and remain - one through a self-directed process of development
and learning. Further, adaptive self-organization would also be a highly desirable property for robotics and
artificial intelligence systems, not only boosting their robustness and adaptability by means of continuous
learning, but potentially leading to systems that can increase their complexity and capabilities organically
over time (e.g. developmental robotics). Therefore, characterizing adaptive self-organization in attractor
2
networks is vital for advancing our understanding of the brain and for creating more autonomous, adaptive,
brain-inspired AI systems.
The Free Energy Principle (FEP) offers a general framework to study self-organization to nonequilibrium
steady states as Bayesian inference (a.k.a., active inference). The FEP has been pivotal in connecting the
dynamics of complex self-organizing systems with computational and inferential processes, especially within
the realms of brain function [Friston et al., 2023, Friston and Ao, 2012, Palacios et al., 2020]. The FEP posits
that any ‘thing’ - in order to exist for an extended period of time - must maintain conditional independence
from its environment. This entails a specific sparsely coupled structure of the system, referred to as a
particular partition, that divides the system into internal, external, and boundary (sensory and active) states
(see Figure 1A). It can be shown that maintaining this sparse coupling is equivalent to executing an inference
process, where internal states deduce the causes of sensory inputs by minimizing variational free energy (see
Friston et al. [2023] or [Friston et al., 2023] for a formal treatment).
Here, we describe the specific class of adaptive self-organizing attractor networks that emerge directly from
theFEP,withouttheneedforexplicitlyimposedlearningorinferencerules. First,weshowthatahierarchical
formulation of particular partitions - a concept that is applicable to any complex dynamical system - can give
rise to systems that have the same functional form as well-known artificial attractor network architectures.
Second, we show that minimizing variational free energy (VFE) with regard to the internal states of such
systems yields a Boltzmann Machine-like stochastic update mechanism, with continuous-state stochastic
Hopfield networks being a special case. Third, we show that minimizing VFE with regard to the internal
blanket or boundary states of the system (couplings) induces a generalized predictive coding-based learning
process. Crucially, this adaptive process extends beyond simply reinforcing concrete sensory patterns; it
learns to span the entire subspace of key patterns by establishing approximately-orthogonalized attractor
representations, which the system can then combine during inference. We use simulations to identify the
requirements for the emergence of quasi-orthogonal attractors and to illustrate the derived attractor networks’
ability to generalize to unseen data. Finally, we highlight, that the derived attractor network can naturally
produce sequence-attractors (if the input data is presented in a clear sequential order) and exemplify its
potential to perform continual learning and to overcome catastrophic forgetting by means of spontaneous
activity. We conclude by discussing testable predictions of our framework, and exploring the broader
implications of these findings for both natural and artificial intelligence systems.
3 Main Results
3.1 Background: particular partitions and the free energy principle
Our effort to characterize self-organizing attractor networks calls for an individuation of ‘self’ from nonself.
Particular partitions, a concept that is at the core of the Free Energy Principle (FEP) [Friston et al., 2022,
2023,FristonandAo,2012,Palaciosetal.,2020],isanaturalwaytoformalizethisindividuation. Aparticular
partition is a partition that divides the states of a system x into a particle or ‘thing’ (s,a,µ) ⊂ x and its
external states η ⊂x, based on their sparse coupling:
η˙(τ) f (η,s,a) ω (τ)
η η
s˙(τ) f (η,s,a) ω (τ)
 = s + s 
a˙(τ) f (s,a,µ) ω (τ)
a a
µ˙(τ) f (s,a,µ) ω (τ)
µ µ
where µ, s and a are the internal, sensory and active states of the particle, respectively. The fluctuations
ω ,i ∈ (µ,s,a,η) are assumed to be mutually independent. The particular states µ, s and a are coupled
i
to each other with particular flow dependencies; namely, external states can only influence themselves and
sensory states, while internal states can only influence themselves and active states (see Figure 1A). It can be
shown that these coupling constraints mean that external and internal paths are statistically independent,
when conditioned on blanket paths [Friston et al., 2022]:
η ⊥µ|s,a. (1)
As shown by Friston et al. [2023], such a particle, in order to persist for an extended period of time, will
necessarily have to maintain this conditional independence structure, a behavior that is equivalent to an
3
inference process in which internal states infer external states through the blanket states (i.e., sensory and
active states) by minimizing free energy [Friston, 2009, 2010, Friston et al., 2023]:
η ⊥µ|s,a ⇒ µ˙ =−∇ F(s,a,µ) (2)
µ
where F(s,a,µ) is the variational free energy (VFE):
F(s,a,µ)=E [lnq (η)−lnp(s,a,η)] (3)
qµ(η) µ
with q (η) being a variational density over the external states that is parameterized by the internal states
µ
and p(s,a,η) being the joint probability distribution of the sensory, active and external states, a.k.a. the
generative model [Friston et al., 2016].
Figure 1: Deep Particular Partitions.
A Schematic illustration of a particular partition of a system into internal (µ) and external states (η),
separated by a Markov blanket consisting of sensory states (s) and active states (a). The tuple (µ,s,a)
is called a particle. A particle, in order to persist for an extended period of time, will necessarily have to
maintainitsMarkovblanket,abehaviorthatisequivalenttoaninferenceprocessinwhichinternalstatesinfer
external states through the blanket states. The resulting self-organization of internal states corresponds to
perception, while actions link the internal states back to the external states. B The internal states µ⊂x can
be arbitrarily complex. Without loss of generality, we can consider that the macro-scale µ can be decomposed
into set of overlapping micro-scale subparticles (σ ,s ,a ,s ,a ), so that the internal state of subparticle
i i i ij ij
σ ⊂µ can be an external state from the perspective of another subparticle σ ⊂µ. Some, or all subparticles
i j
can be connected to the macro-scale external state η, through the macro-scale Markov blanket, giving a
decomposition of the original boundary states into s ⊂s and a ⊂a. The subparticles are connected to each
i i
other by the micro-scale boundary states s and a . Note that this notation considers the point-of-view of
ij ij
the i-th subparticle. Taking the perspective of the j-th subparticle, we can see that s =a and a =s .
ji ij ji ij
Whilethefiguredepictsthesimplestcaseoftwonestedpartitions, thesameschemecanbeappliedrecursively
to any number of (possibly nested) subparticles and any coupling structure amongst them.
3.2 Deep particular partitions and subparticles
Particular partitions provide a universal description of complex systems, in a sense that the internal states
µ behave as if they are inferring the external states under a generative model; i.e., a ‘black box’ inference
process (or computation), which can be arbitrarily complex. At the same time, the concept of particular
partitions speaks to a recursive composition of ensembles (of things) at increasingly higher spatiotemporal
scales [Friston, 2019, Palacios et al., 2020, Clark, 2017], which yields a natural way to resolve the internal
complexityofµ. Partitioningthe“macro-scale”particleµintomultiple,overlapping“micro-scale”subparticles
{π }n - that themselves are particular partitions - we can unfold the complexity of the macro-scale particle
i i=1
to an arbitrary degree. As subparticles can be nested arbitrarily deep - yielding a hierarchical generative
model - we refer to such a partitioning as a deep particular partition.
As illustrated in Figure 1B, each subparticle π has internal states σ , and the coupling between any two
i i
subparticles i and j is mediated by micro-scale boundary states: sensory states s (representing the sensory
ij
4
information in i coming from j) and active states a (representing the action of i on j). The boundary states
ij
of subparticles naturally overlap; the sensory state of a subparticle σ is the active state of σ and vice versa,
i j
i.e. a = s and s = a . This also means that, at the micro-scale, the internal state of a subparticle
ji ij ji ij
σ ⊂µ is part of the external states for another subparticle in σ ⊂µ. Accordingly, the internal state of a
i j
subparticle σ is conditionally independent of any other internal states σ with j ̸=i, given the blanket states
i j
of the subparticles:
σ ⊥σ |a ,s (4)
i j ij ij
Note that this definition embraces sparse couplings across subparticles, as a and s may be empty for
ij ij
a given j (no direct connection between the two subparticles), but we require the subparticles to yield a
complete coverage of µ:
Sn
π =µ.
i=1 i
3.3 The emergence of attractor neural networks from deep particular partitions
Next, we establish a prototypical mathematical parametrization for an arbitrary deep particular partition,
shown on Figure 2, with the aim of demonstrating that such complex, sparsely coupled random dynamical
systems can give rise to artificial attractor neural networks.
Figure 2: Parametrization of subparticles in a deep particular partition.
The internal state σ of subparticle π follows a continuous Bernoulli distribution, (a.k.a. a truncated
i i
exponential distribution supported on the interval [−1,+1]⊂R, see Appendix 1), with a prior “bias” b that
i
can be interpreted as a priori log-odds evidence for an event (stemming from a macro-scale sensory input
s - not shown, or from the internal dynamics of σ itself, e.g. internal sequence dynamics). The state σ
i i i
is coupled to the internal state of another subparticle σ through the micro-scale boundary states s and
j ij
a . The boundary states simply apply a deterministic scaling to their respective σ state, with a weight (J )
ij ij
implemented by a Dirac delta function shifted by J (i.e. we deal with conservative subparticles, in the sense
ij
of Friston et al. [2023]). The state σ is influenced by its sensory input s in a way that s gets integrated
i i i
into its internal bias, updating the level of evidence for the represented event.
In our example parametrization, we assume that the internal states of subparticles in a complex particular
partition are continuous Bernoulli states (also known as a truncated exponential distribution, see Appendix 1
for details), denoted by σ ∼CB :
i bi
p(σ )∝ebσi (5)
i
Here, σ ∈[−1,+1]⊂R, and b ∈R represents an a priori bias (e.g. the level of prior log-odds evidence for
i i
an event) in σ Figure 2. Zero bias represents a flat prior (uniform distribution over [-1,1]), while positive or
i
negative biases represent evidence for or against an event, respectively. The above probability is defined up
to a normalization constant, b /(2sinh(b )). See Appendix 2 for details.
i i
Next, we define the conditional probabilities of the sensory and active states, creating the boundary between
two subparticles σ and σ : s |σ ∼δ and a |σ ∼δ , where δ is the Dirac delta function and J is a
i j ij j Jijσj ij i Jjiσi
weight matrix. The elements J contains the weights of the coupling matrix (see Appendix 3).
ij
Expressed as PDFs:
p(s |σ )=δ(s −J σ ) (6)
ij j ij ij j
p(a |σ )=δ(a −J σ ) (7)
ij i ij ji i
5
The choice of this deterministic parametrization means that we introduce the assumption of the subparticles
being conservative particles, as defined in Friston et al. [2023].
Toclosetheloop, wedefinehowtheinternalstateσ dependsonitssensoryinputs . Weassumethesensory
i ij
input simply adds to the prior bias σ |s ∼CB :
i ij bi+sij
p(σ |s )∝e(bi+sij)σi (8)
i ij
With the continuous Bernoulli distribution, this simply means that the sensory evidence s adds to (or
ij
subtracts from) the prior belief b .
i
We now write up the direct conditional probability describing σ given σ , marginalizing out the sensory and
i j
active states:
Z Z
p(σ |σ )= p(σ |s )p(s |σ )ds ∝ e(sij+bi)σiδ(s −J σ )ds ∝e(bi+Jijσj)σi (9)
i j i ij ij j ij ij ij j ij
1
Given that p(σ ,σ )=p(σ |σ )p(σ ), and using equations (5) and (9), we can express the joint distribution
i j i j j
as follows:
p(σ ,σ )=e(bi+Jijσj)σiebjσj =ebiσi+Jijσiσj+bjσj (10)
i j
Next, we observe that the states s and a (∀i,j) are the ’blanket states’ of the system, forming a Markov
ij ij
blanket [Hipólito et al., 2021]. The corresponding conditional independence structure implies that the joint
Q
probability for all σ nodes can be written as the product of the individual joint probabilities, P(σ ,σ ),
i,j i j
which results in:
P P
p(σ)∝e i
biσi+
i̸=j
Jijσiσj
(11)
However, since σ σ = σ σ , each undirected pair {i,j} appears twice in the sum over directed pairs (i,j)
i j j i
with i̸=j. This mathematical fact leads to an important consequence for the steady-state distribution.
Since σ σ =σ σ , we can rearrange the double sum over distinct pairs:
i j j i
X X(cid:16) (cid:17)
J σ σ = J +J σ σ . (12)
ij i j ij ji i j
i̸=j i<j
Thus, even though we started with non-symmetric couplings J and J , the joint distribution ends up
ij ji
depending only on the sum J +J :
ij ji
( )
X X
p(σ)∝exp b σ + J†σ σ (13)
i i ij i j
i ij
| {z } | {z }
biasterm interactionterm
| {z }
-veenergy
with J† = 1(J+J⊺) (meaning that the diagonal elements J† =0 for all i).
2 ii
This joint probability distribution takes the functional form of a stochastic continuous-state Hopfield network
(a specific type of Boltzmann machines). As known in such systems, regions of high probability density in
this stationary distribution will constitute “stochastic attractors”, which are the regions of the state space
that the system will tend to converge to. Furthermore, in case of asymmetric couplings, the antisymmetric
1The expected value of p(σ |σ ) is a sigmoid function of σ Appendix 4, specifically the Langevin function. This
i j j
property allows it to function as an activation function in neural networks, enabling the network to model more
complex patterns and decision boundaries.
6
part of the coupling matrix induces “solenoidal flows”, extending the attractor repertoire with “sequence
attractors” (heteroclinic chains).
Importantly, our derivation shows that, while solenoidal flows that emerge with asymmetric couplings can
break detailed balance and induce non-equilibrium dynamics, the stationary distribution retains a Boltzmann-
like form determined by the symmetric part J†. This relies on the assumption that the solenoidal flows are
divergence-free - i.e. it does not alter the Boltzmann form of the stationary distribution but, instead it drives
persistent, divergence-free probability currents along iso-potential surfaces. This condition is argued to hold
for conservative particles under a particular partition (see Appendix 5 and Friston et al. [2023]; but also Ao
[2004] and Xing [2010]). Under this condition, the antisymmetric part of the coupling acts only tangentially
to the iso-potential surfaces defined by the symmetric part and, therefore, does not alter the form of the
stationary distribution, merely driving probability fluxes along these surfaces. Nevertheless, as we will see in
the next sections, the dynamics derived from local free energy minimization in this system still depend on
the potentially asymmetric couplings J.
3.4 Inference
Sofarourderivationonlyreliedonthesparselycoupledstructureofthesystem(deepparticularpartition),but
did not utilize the free energy principle itself. We now consider the implications of free energy minimization
on the dynamics of the derived recurrent neural network. We start by expressing variational free energy
(VFE, eq. (3)) from the point of view of a single node of the attractor network σ , given observations from all
i
other nodes σ :
\i
F =E [lnq(σ )−lnp(σ ,σ )]=D [q(σ )||p(σ )]−E [lnp(σ |σ )] (14)
q(σi) i \i i KL i i q(σ) \i i
| {z } | {z }
complexity accuracy
Where q(σ ) is the approximate posterior distribution over σ , which we will parametrize as a CB with
i i
variational bias b . We are now interested in how node σ must update its bias, given the state of all other
q i
nodes, σ and the weights J . Intuitively, the last part of eq. (14) tells us that minimizing free energy
\i ij
will tend to pull the system towards a (local) minimum of the Boltzmann-like energy functional (eq. (13))
of the attractor network (accuracy term), with the constraint that this has to lie close to the initial state
(complexity term).
Let us verify this intuition by substituting our parametrization into eq. (14). First, we rearrange eq. (13) to
get:
 
X 1XX
p(σ \i |σ i )∝exp (b j +J i,j σ i )σ j + 2 J j,k σ j σ k (15)
j̸=i j̸=i k̸=i
Then, we express the accuracy term:
X X 1XX
E [lnp(σ |σ )]=const+ b σ +L(b ) J σ + J σ σ (16)
q(σi) \i i j j q ij j 2 jk j k
j̸=i j̸=i j̸=i k̸=i
The complexity term in eq. (14) is simply the KL-divergence term between two CB distributions:
D [q(σ )∥p(σ )]= (cid:2) ln (cid:0) bq (cid:1) +b L(b ) (cid:3) − (cid:2) ln (cid:0) b (cid:1) +bL(b ) (cid:3) (17)
KL i i sinh(bq) q q sinh(b) q
where L(·) is the expected value of the CB, a sigmoid function of the bias, specifically the Langevin function
(Appendix 4).
Combining the complexity and accuracy terms leads to the following expression for VFE:
X 1XX
F =(b −b)L(b )− (b +L(b )J )σ − J σ σ +C (18)
q q j q ij j 2 jk j k
j̸=i j̸=i k̸=i
where C denotes all constants in the equation that are independent of σ.
7
For details on the derivation, see Appendix 6.
Now we take the partial derivative of the free energy with respect to the variational bias:
 
∂F X dL
∂b =b q −b− J ij σ j db (19)
q q
j̸=i
Setting the derivative to zero, solving for b , and substituting the expected value of the CB distribution, we
q
get:
 
 
 X 
E [σ ]=L(b )=L b + J σ  (20)
q i q  i ij j 
|{z} 
bias j̸=i 
| {z }
weightedinput
| {z }
sigmoid(Langevin)
In the case of symmetric couplings, the above equation reduces to a Boltzmann-style update rule (specifically,
that of a continuous-state stochastic Hopfield network, with the special sigmoid function L). While the
deterministic variant of the above inference rule can be derived directly as a gradient descent on the negative
exponent of eq. (13), the presented FEP-based derivation naturally extends this to a probabilistic framework,
with an emerging sigmoid function (through the complexity term). Thus, the FEP minimization provides
the full probabilistic machinery, instead of just moving down deterministically on an energy gradient. The
resulting stochastic dynamics leads to the optimal expected belief under variational inference, naturally
incorporating prior biases, state constraints (sigmoid due to the {−1,1} state space) and equals to a local
approximate Bayesian inference, where the approximate posterior belief b balances prior information (b )
q i
P
and evidence from neighbours ( J σ ). As we will show later in the manuscript, the inherently stochastic
ij j
characteristics of inference is what allows the network as a whole as well, to escape local energy minima over
time and thereby perform macro-scale Bayesian inference.
3.5 Learning
At optimum, q would match p, causing the VFE’s derivative to vanish. Learning happens, when there is a
systematic change in the prior bias b that counteracts the update process. This can correspond, for instance,
i
to an external input (e.g. sensory signal representing increased evidence for an external event), but also as
the result of the possibly complex internal dynamics of a subparticle (e.g. internal sequence dynamics or
memory retrieval). In this case, a subparticle can take use of another (slower) process, to decrease local VFE:
it can change the way its action states are generated; and rely on its vicarious effects on sensory signals.
In our parametrization, this can be achieved by changing the coupling strength J corresponding to the
ji
action states. Of note, while changing J corresponds to a change in action-generation at the local level
ji
of the subparticle (micro-scale), at the macro-scale, it can be considered as a change in the whole system’s
generative model.
Let’s revisit VFE from the perspective of node i:
h i
F =E lnq(σ )−lnp(σ |σ ) (21)
q(σi) i \i i
and parameterize the distributions as:
lnq(σ )∝b σ , lnp(σ |σ )∝u σ (22)
i q i \i i i i
P
with u being the net weighted input to node i: u =b+ J σ .
i i j̸=i ij j
We obtain:
h i
F =E (b −u )σ +ϕ(u )−ϕ(b ) (23)
q(σi) q i i i q
8
Atequilibrium(i.e. whenb =u ),wehaveE [σ ]=L(u ). Toconstructastochastic(sample-based)estimate,
q i q i i
wecanreplacetheexpectationE [σ ]withtheinstantaneousvalueσ . Aperturbation δJ producesachange
q i i ij
δu =σ δJ , and by applying the chain rule we get:
i j ij
dF = ∂F ∂u i = (cid:2) L(u )−σ (cid:3) σ (24)
dJ ∂u ∂J i i j
ij i ij
Substituting back u and rearranging we get:
i
X
∆J ∝ σ σ − L(b + J σ )σ (25)
ij i j i ik k j
|{z}
k̸=i
observedcorrelation(Hebbian)
| {z }
predictedcorrelation(anti-Hebbian)
Thislearningresemblesthefamilyof“Hebbian/anti-Hebbian”or“contrastive”learningrulesanditexplicitly
implements predictive coding (akin to prospective configuration [Song et al., 2024, Millidge et al., 2022a]).
However,asopposedtoe.g. contrastivedivergence(acommonmethodfortrainingcertaintypesofBoltzmann
machines, Hinton [2002]), it does not require to contrast longer averages of separate “clamped” (fixed inputs)
and “free” (free running) phases, but rather uses the instantaneous correlation between presynaptic and
postsynaptic activation to update the weight, lending a high degree of scalability for this architecture. As
we demonstrate below with Simulation 1, 2 and 4, this learning rule converges to symmetric weights (akin
to a classical stochastic continuous-state Hopfield network), if input data is presented in random order and
long epochs. At the same time, if data is presented in rapidly changing fixed sequences (Simulation 3), the
learning rule results in temporal predictive coding and learns asymmetric weights, akin to [Millidge et al.,
2024]. As discussed above, in this case the symmetric component of J encodes fixed-point attractors and
the probability flux induced by the antisymmetric component results in sequence dynamics (conservative
solenoidal flow), without altering the steady state of the system.
Another key feature of this rule is its resemblance to Sanger’s rule [Sanger, 1989], hinting that it imposes an
approximate orthogonality across attractor states. We motivate this theoretically and with simulations in the
next sections.
3.6 Emergence of approximately orthogonal attractors
Under the FEP, learning not only aims to maximize accuracy but also minimizes complexity (14), leading
to parsimonious internal generative models (encoded in the weights J and biases b) that offer efficient
representations of environmental regularities. A generative model is considered more complex (and less
efficient) if its internal representations, specifically its attractor states σ(µ) - which loosely correspond to
the modes of p(σ) - are highly correlated or statistically dependent. Such correlations imply redundancy, as
distinct attractors would not be encoding unique, independent features of the input space. Our learning rule
(eq. (25))-byminimizingmicro-scaleVFE-inherentlyalsominimizesthecomplexitytermD [q(σ )||p(σ )],
KL i i
which regularizes the inferred state q(σ ) to be close to the node’s current prior p(σ ). This not only
i i
induces sparsity in the coupling matrix, but - as we motivate below - also penalizes overlapping attractor
representations and favours orthogonal representations. Hebbian learning - in itself - can not implement such
a behavior; as it simply aims to store the current pattern by strengthening connections between co-active
nodes. This has to be counter-balanced by the anti-Hebbian term, that subtracts the variance that is already
explained out by the network’s predictions.
To illustrate how this dynamic gives rise to efficient, (approximately) orthogonal representations of the
external states, suppose the network has already learned a pattern s(1), whose neural representation is the
attractor σ(1) and associated weights J(1). When a new pattern s(2) is presented that is correlated with s(1),
the network’s prediction for σ(2) will be σˆ =L(b + P J(1)σ ). Because inference with J(1) converges to
i i i k̸=i ik k
σ(1) andσ(2) iscorrelatedwithσ(1), thepredictionσˆ willcapturevarianceinσ(2) thatis‘explained’byσ(1).
Thelearningruleupdatestheweightsbasedonlyontheunexplained(residual)componentofthevariance,the
prediction error. In other words, σˆ approximates the projection of σ(2) onto the subspace already spanned by
σ(1). Therefore,theweightupdateprimarilystrengthensweightscorrespondingtothecomponentofσ(2) that
isorthogonaltoσ(1). Thus,thelearningeffectivelyencodesthisresidual,σ(2),ensuringthatthenewattractor
⊥
components being formed tend to be orthogonal to those already established. As we demonstrate in the
next section with Simulation 1-2, repeated application of this rule during learning progressively decorrelates
9
the neural activities associated with different patterns, leading to approximately orthogonal attractor
states. This process is analogous to online orthogonalization procedures (e.g., Sanger’s rule for PCA [Sanger,
1989]) and results in a powerful stochastic approximation of the “projection network” of Personnaz et al.
[1985], Kanter and Sompolinsky [1987], which offers maximal memory capacity and retrieval without errors.
Whileorthogonalityenhancesrepresentationalefficiency,itraisesthequestionofhowthenetworkretrievesthe
originalpatternsfromthese(approximately)orthogonalizedrepresentations-akeyrequirementtofunctionas
associative memory. As discussed next, stochastic dynamics enable the network to address this by sampling
from a posterior that combines these orthogonal bases.
3.7 Stochastic retrieval as macro-scale Bayesian inference
As a consequence of the FEP, the inference process described above - where each subparticle σ updates
i
P
its state based on local information (its bias b and input J σ ) - can be seen as a form of micro-scale
i j ij j
inference, in which the prior - defined by the node’s internal bias, gets updated by the evidence collected
from the neighboring subparticles to form the posterior. However, as the whole network itself is also a
particular partition (specifically, a deep one), it must also perform Bayesian inference, at the macro-scale.
While the above argumentation provides a simple, self-contained proof, the nature of macro-scale inference
can be elucidated by using the equivalence of the derived attractor network - in the special case of symmetric
couplings - to Boltzmann machines (without hidden units). Namely, the ability of Boltzmann machines to
perform macro-scale approximate Bayesian inference through Markov Chain Monte Carlo (MCMC) sampling
has been well established in the literature [Ackley et al., 1985, Hinton, 2002].
Let us consider the network’s learned weights J (and potentially its baseline biases bbase) as defining a prior
distribution over the collective states σ:
( )
X 1X
p(σ)∝exp bbaseσ + J σ σ (26)
i i 2 ij i j
i ij
Now, suppose the network receives external input (evidence) s, which manifests as persistent modulations δb
i
to the biases, such that the total bias is b =bbase+δb . This evidence can be associated with a likelihood
i i i
function:
!
X
p(s|σ)∝exp δb σ (27)
i i
i
According to Bayes’ theorem, the posterior distribution over the network states given the evidence is
p(σ|s)∝p(s|σ)p(σ):
( )
X 1X
p(σ|s)∝exp b σ + J σ σ (28)
i i 2 ij i j
i ij
Asexpected,thisposteriordistributionhasthesamefunctionalformasthenetwork’sequilibriumdistribution
undertheinfluenceofthetotalbiasesb . Thus, thestochasticupdaterulederivedfromminimizinglocalVFE
i
(eq. (14)) effectively performs Markov Chain Monte Carlo (MCMC) sampling - specifically Gibbs sampling
- from the joint posterior distribution defined by the current VFE landscape. In the presence of evidence
s, the network dynamics therefore sample from the posterior distribution p(σ|s). The significance of the
stochasticity becomes apparent when considering the network’s behavior over time. The time-averaged state
⟨σ⟩ converges to the expected value under the posterior distribution:
1 Z T
⟨σ⟩= lim σ(t)dt≈E [σ] (29)
T→∞T
0
p(σ|s)
Noise or stochasticity allows the system to explore the posterior landscape, escaping local minima inherited
from the prior if they conflict with the evidence, and potentially mixing between multiple attractors that are
compatiblewiththeevidence. Theresultingaverageactivity⟨σ⟩thusrepresentsaBayesianintegrationofthe
priorknowledgeencodedintheweightsandthecurrentevidenceencodedinthebiases. Thiscontrastssharply
withdeterministicdynamics, whichwouldmerelysettleintoasingle(potentiallysuboptimal)attractorwithin
the posterior landscape.
Furthermore, if the learning process shapes the prior p(σ) to have less redundant (e.g., more orthogonal)
attractors, this parsimonious structure of the prior naturally contributes to a less redundant posterior
10
distribution p(σ|s). When the prior belief structure is efficient and its modes are distinct, the posterior
modes formed by integrating evidence s are also less likely to be ambiguous or highly overlapping. This leads
to more robust and interpretable inference, as the network can more clearly distinguish between different
explanations for the sensory data.
Withthis,theloopisclosed. ThestochasticattractornetworkemergingfromtheFEPframework(summarized
on Figure 3A) naturally implements macro-scale Bayesian inference through its collective sampling dynamics,
providing a robust mechanism for integrating prior beliefs with incoming sensory evidence. This reveals the
potential for a deep recursive application of the Free Energy Principle: the emergent collective behavior of
the entire network, formed by interacting subparticles each minimizing their local free energy, recapitulates
the inferential dynamics of a single, macro-scale particle. This recursion could extend to arbitrary depths,
giving rise to a hierarchy of nested particular partitions and multiple emergent levels of description, each
level performing Bayesian active inference according to the same fundamental principles.
Figure 3: Free energy minimizing, adaptively self-organizing attractor network
A Schematic of the network illustrating inference and learning processes. Inference and learning are two
faces of the same process: minimizing local variational free energy (VFE), leading to dissipative dynamics
and approximately orthogonal attractors. B A demonstrative simulated example (Simulation 1) of the
network’s attractors forming an orthogonal basis of the input data. Training can be performed by introducing
the training data (top left) through the biases of the network. In this example, the input data consists
of two correlated patterns (Pearson’s r = 0.77). During repeated updates, micro-scale (local, node-level)
VFE minimization implements a simultaneous learning and inference process, which leads to approximate
macro-scale (network-level) free energy minimization (bottom graph). The resulting network does not simply
store the input data as attractors, but it stores approximately orthogonalized varieties of it (top right,
Pearson’s r = -0.19). C When the trained network is introduced a noisy version of one of the training
patterns(left), itisinternallyhandledastheLikelihoodfunction, andthenetworkperformsanMarkov-Chain
Monte-Carlo (MCMC) sampling of the posterior distribution, given the priors defined by the network’s
attractors (top right), which can be understood as a retrieval process. D Thanks to its orthogonal attractor
representation, the network is able to generalize to new patterns - as long as they are sampled from the
sub-space spanned by the attractors - by combining the quasi-orthogonal attractor states (bottom right) by
multistable stochastic dynamics during the MCMC sampling.
11
4 In silico demonstrations
We illustrate key features of the proposed framework with computer simulations. All simulations are based
on a simple python implementation of the network, available at https://github.com/tspisak/fep-attractor-
networks. The implementation favors clarity over efficiency - it implements both σ and boundary states as
separate classes, and is not optimized for performance. In all simulations, we train an attractor network with
the derived rules in a continuous-learning fashion (i.e simultaneously performing inference and learning). To
be able to control the precision during inference and the speed of learning, we introduce two coefficients for
eq.-s (20) and (25), the inverse temperature parameter iT and a learning-rate α.
4.1 Simulation 1: demonstration of orthogonal basis formation, macro-scale free energy
minimization and Bayesian inference
In Simulation 1, we construct a network with 25 subparticles (representing 5x5 images) and train it with 2
different, but correlated images (Pearson’s r = 0.77, see Figure 3B), with a precision of 0.1 and a learning
rate of 0.01. The training phase consisted of 500 epochs, each showing a randomly selected pattern from the
training set through 10 time steps of simultaneous inference and learning. As shown on Figure 3B, local,
micro-scale VFE minimization performed by the simultaneous inference and learning process leads to a
macro-scale free energy minimization. Next, we obtained the attractor states corresponding to the input
patterns by means of deterministic inference (updating with the expected value, instead of sampling from
the CB distribution, akin to a vanilla Hopfield network). As predicted by theory, the attractor states were
not simple replicas of the input patterns, but approximately orthogonalized versions of them, displaying a
correlation coefficient of r=-0.19. Next, we demonstrated that the network (with stochastic inference) is not
only able to retrieve the input patterns from noisy variations of them (fig. Figure 3C), but also generalizes
well to reconstruct a third pattern, by combining its quasi-orthogonal attractor states (fig. Figure 3D). Note
that this simulation only aimed to demonstrate some of the key features of the proposed architecture, and a
comprehensive evaluation of the network’s performance, and its dependency on the parameters is presented
in the next simulation.
4.2 Simulation 2: systematic evaluation of learning regimes
In Simulation 2, we trained the network on 10 images of handwritten digits (a single example of each of
the 10 digits from 0 to 9, 8x8 pixels each, as distributed with scikit-learn, see Figure 4C, upper row). The
remaining 1787 images were unseen in the training phase and only used as a test set, in a one-shot learning
fashion. The network was trained with a fixed learning rate of 0.01, through 5000 epochs, each consisting of
10 time steps with the same, randomly selected pattern from the training set of 10 images, while performing
simultaneous inference and learning. We evaluated the effect of the inverse temperature parameter iT (i.e.
precision) and the strength of evidence during training, i.e. the magnitude of the bias changes δb . The
i
precision parameter iT was varied with 19 values between 0.01 and 1, and the strength of evidence during
training varied by changing the magnitude of the biases from 1 to 20, with increments of 1. The training
patterns were first preprocessed by squaring the pixel values (to enhance contrast) and normalizing each
image to have zero mean and unit variance. We performed a total of 380 runs, varying these parameters in a
grid-search fashion. All cases were evaluated in terms of (i) stochastic (Bayesian) pattern retrieval from noisy
variations of the training images; and (ii) one-shot generalization to reconstruct unseen handwritten digit
examples. In both types of evaluation, the network was presented a noisy variant of a randomly selected
(training or test) image through its biases. The noisy patterns were generated by adding Gaussian noise with
a standard deviation of 1 to the pixel values of the training images (see “Examples” in Figure 4B C and D).
The network’s response was obtained by averaging 100 time steps of stochastic inference. The performance
was quantified as the improvement in the proportion of variance in the original target pattern (without noise)
explainedbythenetwork’sresponse,comparedtothatexplainedbythenoisyinputpattern. Bothforretrieval
and generalization, this approach was repeated 100 times, with a randomly sampled image from the training
(10 images) and test set (1787 images), respectively. The median improvement across these 100 repetition
was used as the primary performance metric. The retreival and 1-shot generalization performance of models
trained with different iT and α parameters is shown on Figure 4A, top row). We found that, while retrieval
of a noisy training pattern was best with precision values between 0.1 and 0.5, generalization to new data
peferredlowerprecisionduringlearning(iT<0.1,i.e. morestochatsicdynamics). Furtermore,inallsimulation
cases, we seeded the networks with the original test patterns and obtained the corresponding attractor states,
by means of deterministic inference. We then computed the pairwise correlation and dot product between
the attractor states. The dot product was converted to degrees. Orthogonality was finally quantified by the
12
Figure 4: Adaptive self-organization and generalization in a free-energy minimizing attractor
network.
Simulation results from training the network on a single, handwritten example for each of the 10 digits (0-9),
withvariationsintrainingprecisionandevidencestrengthtoexploredifferentlearningregimes(Simulation2).
A: Performance landscapes as a function of inference temperature (inverse precision) and training evidence
strength (bias magnitude). Retrieval performance (reconstructing noisy variants of the 10 training patterns,
top left), one-shot generalization (reconstructing a noisy variants of unseen handwritten digits, top right),
attractor orthogonality (mean squared angular difference from 90° indicating higher orthogonality for lower
values, bottom left), and the number of attractors (when initialized with the 10 training patterns, bottom
right) are shown. Optimal regions (contoured) highlight parameter settings that yield good generalization
andhighlyorthogonalattractors. Contoursinthetopleftandtoprighthighlightthemostefficientparameter
settings for retrieval and generalization, respectively. Both contours are overlaid on the two bottom plots. B:
Conceptual illustration of training regimes. With low temperature (high precision) high model complexity is
allowed (“accuracy pumping”) and attractors will tend to exactly match the training data. On the contrary,
high temperatures (low precision) result in a single fixed point attractor and reduced recognition performance.
However, such networks will be able to generalize to new data, suggesting the existance of “soft attractors”
(e.g. saddle-like structures) that are not local minima on the free energy landscape, yet affect the steady-state
posterior distribution in a non-negligible way (especially with longer mixing-times).
13
Figure 4: (continued) A balanced regime can be found with intermediate precision during training, where
both recognition and generalization performance are high. This is exactly the regime that promotes attractor
orthogonalization, crucial for efficient representation and generalization. The complexity restrictions on
these models cause them to re-use the same attractors to represent different patterns (see e.g. the single
attractor belonging to the digits 5 and 7 in the example on panel D), which eventually leads to approximate
orthogonality. Panels C-E provide examples of network behavior on a handwritten digit task across different
regimes, including (i) training data (same in all cases); (ii) fixed-point attractors (obtained with deterministic
update);(iii)attractor-orthogonality(polarhistogramofthepairwiseanglesbetweenattractors);(iv)retrieval
and 1-shot generalization performance (R2 between the noisy input pattern and the network output after
100 time steps, for 100 randomly sampled patterns) and (v) illustrative example cases from the recognition
and 1-shot generalization tests (noisy input, network output and true pattern). C: High complexity:
Attractors are sharp and similar to training data; good recognition, limited generalization. D: Balanced
complexity (orthogonalization): Attractors are distinct and quasi-orthogonal, enabling strong recognition
and generalization from noisy inputs. The balanced regime clearly demonstrates the network’s ability to
form an orthogonal basis, facilitating effective generalization as predicted by the free-energy minimization
framework. E: Low complexity: There is only a single fixed-point attractor. Recognition performance is
lower, but generalization remains considerable.
mean correlation among attractors and and the mean squared deviation from orthogonality (in degrees). To
establishareferencevalue,thesameprocedurewasalsorepeatedfortheoriginalpatterns(afterpreprocessing),
which displayed a mean correlation a 29.94 degree mean squared deviation from orthogonality. Attractor
orthogonality and the number of attractors for each simulation case is shown on Figure 4A, bottom row.
We found that depending on the temperature of the network during the learning phase, the network can
be in characteristic regimes of high, low and balanced complexity (Figure 4B). With low temperature (high
precision), high model complexity is allowed (“accuracy pumping”) and attractors will tend to exactly match
the training data Figure 4C. On the contrary, high temperatures (low precision) result in a single fixed point
attractor and reduced recognition performance Figure 4E. However, such networks were found to be able to
generalize to new data, suggesting the existence of “soft attractors” (e.g. saddle-like structures) that are not
localminimaonthefreeenergylandscape,yetaffectthesteady-stateposteriordistributioninanon-negligible
way (especially with longer mixing-times). A balanced regime Figure 4D can be found with intermediate
training precison, where both recognition and 1-shot generalization performance are high (similarly to the
“standard regime” of prospective configuration [Millidge et al., 2022a]). This is exactly the regime that
promotes attractor orthogonalization, crucial for efficient representation and generalization. The complexity
restrictions on these models cause them to re-use the same attractors to represent different patterns (see e.g.
the single attractor belonging to the digits 5 and 7 in the example on panel D), which eventually leads to less
attractors, with each having more explanatory power, and being approximately orthogonal to each other.
Panels C-E on Figure 4 provide examples of network behavior on a handwritten digit task across different
regimes, including (i) training data (same in all cases); (ii) fixed-point attractors (obtained with deterministic
update); (iii) attractor-orthogonality (polar histogram of the pairwise angles beteen attractors); (iv) retrieval
and 1-shot generalization performance (R2 between the noisy input pattern and the network output after 100
time steps, for 100 randomly sampled patterns) and (v) illustrative example cases from the recognition and
1-shot generalization tests (noisy input, network output and true pattern).
4.3 Simulation 3: demonstration of sequence learning capabilities
In Simulation 3, we demonstrate the sequence learning capabilities of the proposed architecture. We
trained the network on a sequence of 3 handwritten digits (1,2,3), with a fixed order of presentation
(1→2→3→1→2→3→...), for 2000 epochs, each epoch consisting of a single step (Figure 5A). This
rapidpresentationof theinputsequence forced thenetwork to modelthe current attractorfromthe network’s
response to the previous pattern, i.e. to establish sequence attractors. The inverse temperature was set to 1
and the learning rate to 0.001 (in a supplementary analysis, we saw a considerable robustness of our results
to the choice of these parameters). As shown on Figure 5B, this training approach led to an asymmetric
coupling matrix (it was very close to symmetric in all previous simulations). Based on eq.-s (12) and (13), we
decomposed the coupling matrix into a symmetric and antisymmetric part (Figure 5C and D). Retrieving the
fixed-point attractors for the symmetric component of the coupling matrix, we obtained three attractors,
correspondingtothethreedigits(Figure5CandE).Theantisymmetriccomponentofthecouplingmatrix,on
the other hand, was encoding the sequence dynamics. Indeed, letting the network freely run (with zero bias)
resulted in a spontanously emerging sequance of variations of the digits 1→2→3→1→2→3→1→...,
14
reflecting the original training order (Figure 5F). This illustrates that the proposed framework is capable of
producing and handling assymetric couplings, and thereby learn sequences.
Figure 5: Sequential Dynamics in Free-Energy Minimizing Attractor Networks.
Simulation results (Simulation 3) demonstrate the framework’s ability to learn temporal sequences. Ordered
training data leads to asymmetric coupling matrices, where the symmetric component establishes fixed-point
attractors for individual patterns, and the antisymmetric component encodes the transitional dynamics,
enabling spontaneous sequence recall. A: Ordered training data (digits 1, 2, 3) presented sequentially to the
network. B: The emergent coupling matrix, displaying asymmetry as a consequence of sequential training. C:
The symmetric component of the coupling matrix. This part is responsible for creating stable, fixed-point
attractors for each pattern in the sequence. D: The antisymmetric component of the coupling matrix. This
part drives the directional transitions between the attractors, encoding the learned order. E: The sequence
attractors themselves – fixed-point attractors (digits 1, 2, 3) derived from the symmetric coupling component.
F: Example of spontaneous network activity with zero external bias, showcasing the autonomous recall of the
learned sequence (1→2→3→...) driven by the interplay of symmetric and antisymmetric couplings.
4.4 Simulation 4: demonstration of resistance to catastrophic forgetting
In Simulation 4, we took a network trained in Simulation 2 with inverse temperature 0.17 and evidence level
11 (the same network that is illustrated on Figure 4D) and let it run for 50000 epochs (the same number of
epochs as the training phase), with unchanged learning rate, but zero bias. We expected that, as the network
spontanouslytraversesarounditsattractors,itreinforcesthemandpreventsthemfrombeingfully“forgotten”.
Indeed, as shown on Figure 6, the network’s coupling matrix (panel A), retrieval performance (panel B) and
one-shot generalization performance (panel C) were all very similar to the original network’s performance.
However, the network’s attractor states were not exactly the same as the original ones, indicating that some
of the original attractors become “soft attractors” (or “ghost attractors”), which do not represent explicit
local minima on the free energy landscape anymore, but their influence on the network’s dynamics is still
significant (see 06-simulation-digits-catastrophic-forgetting.ipynb).
5 Discussion
The crucial role of attractor dynamics in elucidating brain function mandates a foundational question: what
types of attractor networks arise naturally from the first principles of self-organization, as articulated by
the Free Energy Principle [Friston et al., 2023, Friston, 2010]? Here we aimed to address this question. We
demonstrated mathematically, by recourse to a prototypical parametrization, that the ensuing networks
generally manifest as non-equilibrium steady-state (NESS) systems [Xing, 2010, Ao, 2004] that have a
stationary state probability distribution governed by the symmetric component of their synaptic efficacies,
conforming to a Boltzmann-like form [Amit, 1989, Hochreiter and Schmidhuber, 1997]. This renders the
resulting self-organizing attractor networks a generalization of canonical, single-layer Boltzmann machines or
stochasticHopfieldnetworks[Hinton,2002,Hopfield,1982],butdistinguishedbytheircapacityforasymmetric
couplingandcontinuous-valuedneuronalstates. Themainassumptionsunderpinningthisderivation-namely,
the existence of a (deep) particular partition and the ensuing imperative to minimize variational free energy -
15
Figure 6: Demonstration of resistance to catastrophic forgetting via spontaneous activity.
Simulation results (06-simulation-digits-catastrophic-forgetting.ipynb) illustrate the network’s ability to
mitigate catastrophic forgetting. When allowing the network to “free-run” (e.g. with zero external bias)
while performing continous weight adjustment, spontaneous activity reinforces existing attractors, largely
preserving learned knowledge even in the absence of the repeated presentation of previous training patterns.
A: Coupling matrices. The top panel displays the coupling matrix immediately after training on digit
patterns (50000 steps with bias corresponding to digits, the same simulation case as on D). The bottom
panel shows the coupling matrix after an additional 50000 steps of free-running (zero bias, but active weight
adjustment), indicating that the learned structure is largely maintained. B: Recognition performance. This
panel compares the R2 values for reconstructing noisy versions of the trained digit patterns. It shows the
similarity between the noisy input and the true pattern (left boxplots in each sub-panel) versus the similarity
betweenthenetwork’soutputandthetruepattern(rightboxplotsineachsub-panel). Performanceisrobustly
maintained after the free-running phase (bottom) compared to immediately after training (top). C: One-shot
generalization performance. This panel shows the R2 values for reconstructing noisy versions of unseen
handwritten digit patterns (after seeing only a single example per digit). Similar to recognition, the network’s
ability to generalize to novel inputs is well-preserved after the free-running phase (bottom) compared to
immediately after training (top).
are not merely parsimonious, but arguably fundamental for any random dynamical system that maintains its
integrity despite a changing environment [Friston et al., 2023].
Our formulation reveals that the minimization of variational free energy at the micro-scale, i.e., by individual
network nodes (“subparticles”), gives rise to a dual dynamic within the active inference framework [Friston,
2009]. Firstly, it prescribes Bayesian update dynamics for the individual network nodes, homologous to the
stochastic relaxation observed in Boltzmann architectures (e.g. stochastic Hopfield networks), with high
neuroscientificrelevance, especiallyforcoarse-grainedbrainnetworks, whereactivityacrossbrainregionshave
been reported to “flow” following a similar rule [Cole et al., 2016, Sanchez-Romero et al., 2023, Cole, 2024].
Secondly, it engenders a distinctive coupling plasticity - a local, incremental learning rule - that continuously
adjusts coupling weights to preserve low free energy in anticipation of future sensory encounters, effectively
implementing action selection in the active inference sense [Friston et al., 2016]. The learning rule itself
displays a high neurobiological plausibility, as it resembles both generalized Hebbian-anti-Hebbian learning
[Földiák, 1990, Sanger, 1989] and predictive coding [Rao and Ballard, 1999, Millidge et al., 2022b, 2024].
By contrasting the current correlations of inputs with that predicted by the network’s generative model in
the previous time step, the rule efficiently implements sequence learning. Furthermore, by capitalizing on
its natural tendency to spontaneously (stochastically) revisit its attractors, the network is able to mitigate
catastrophic forgetting [Aleixo et al., 2023] - the tendency of current deep learning architectures to lose
old representations when learning new ones. This phenomenon may serve as a model for the spontaneous
fluctuations observed during resting state brain activity (also related to "day-dreaming") and shows promise
for leveraging similar mechanisms in artificial systems.
Importantly, we show that in our adaptive self-organizing network architecture, micro-scale free energy
minimization manifests in (approximate) macro-scale free energy minimization - as expected from the fact
that the macro-scale network itself is also a particular partition. This entails that the network performs
Bayesian inference not only locally in its nodes, but also on the macro-scale - a result well known from
16
the literature on Boltzmann machines and also spiking neural networks [Ackley et al., 1985, Hinton, 2002,
Buesing et al., 2011]. Our work extends these previous results with a holistic view: the free energy landscape,
sculpted by the coupling efficacies and manifested as the repertoire of (soft-) attractors, encodes the system’s
prior beliefs. Sensory inputs or internal computations, presented as perturbations to the internal biases
of network nodes, constitute the likelihood, while the network’s inherent stochastic dynamics explore the
posterior landscape via a process akin to Markov Chain Monte Carlo sampling [Gelman and Rubin, 1992].
The stochastic nature of these dynamics is not a mere epiphenomenon; it empowers the network to engage
in active inference by dynamically traversing trajectories in its state space that blend the contributions
of disparate attractor basins [Friston, 2009]. Consequently, for inputs that, while novel, reside within the
subspace spanned by the learned attractors, the network generalizes by engaging in oscillatory activity
- a characteristic signature of neural computations in the brain. While the notion of oscillations being
generated by multistable stochastic dynamics has been previously proposed by e.g. [Liu et al., 2022], the
quasi-orthogonal basis spanned by the attractors of the system could render such a mechanism especially
effective.
The FEP formalisation via deep particular partitions intrinsically accommodates multiple, hierarchically
nestedlevelsofdescription. Onemayarbitrarilycoarse-grainthesystembycombiningsubparticles,ultimately
arrivingatasimpleparticularpartition. Drawinguponconceptssuchasthecentermanifoldtheorem[Wagner,
1989], it is posited that rapid, fine-grained dynamics at lower descriptive levels converge to lower-dimensional
manifolds, upon which the system evolves via slower processes at coarser scales. This inherent separation
of temporal scales offers a compelling paradigm for understanding large-scale brain dynamics, where fast
neuronal activity underwrites slower cognitive processes through hierarchical active inference [Man et al.,
2018]. Indeed, empirical investigations have provided manifold evidence for attractor dynamics in large-scale
brain activity [Rolls, 2009, Kelso, 2012, Haken, 1978, Breakspear, 2017, Deco and Jirsa, 2012, Kelso, 2012,
Gosti et al., 2024, Chen et al., 2025].
As motivated mathematically and demonstrated in silico, a pivotal outcome of the FEP-driven simultaneous
learning and inference process is the propensity of the network to establish approximately orthogonal attractor
states. We argue that this remarkable property is not simply a side effect; it is an unavoidable result of
reducing free energy of conservative particles [Friston et al., 2023]; which entails minimizing model complexity
andmaximizingaccuracysimultaneouslyor-eqivalently-minimizeredundancyviaamaximizationofmutual
information. This key characteristic makes free energy minimizing attractor networks naturally approximate
one of the most efficient attractor network architectures, the projection networks articulated by Kanter
and Sompolinsky [Kanter and Sompolinsky, 1987, Personnaz et al., 1985]. We propose the approximate
orthogonality of attractor states as a signature of free energy minimizing attractor networks, potentially
detectable in natural systems (e.g. neural data). There is initial evidence from large-scale brain network data
pointing into this direction. Using an attractor network model very similar to the herein presented - a recent
paper has demonstrated that standard resting-state networks (RSNs) are manifestations of brain attractors
that can be reconstructed from fMRI brain connectivity data [Englert et al., 2023]. Most importantly, these
empirically reconstructed large-scale brain attractors were found to be largely orthogonal - the key feature
of the self-organizing attractor networks described here. Future research needs to carefully check if these
large-scale brain attractors, and the computing abilities that come with them, are truly a direct signature of
an underlying FEP-based attractor network.
Besides neuroscientific relevance, our work has also manifold implications for artificial intelligence research.
In general, there is growing interest in predictive coding-based neural network architectures - akin to the
architecturepresentedherein. Recentstudieshavedemonstratedthatsuchapproachescannotonlyreproduce
backpropagation as an edge case [Millidge et al., 2022c], but scale efficiently - even for cyclic graphs - and
can outperform traditional backpropagation approaches in several scenarios [Salvatori et al., 2023, 2021].
Furthermore, in our framework - in line with recent formulations of predictive coding for deep learning
[Millidge et al., 2022b] - learning and inference are not disparate processes but rather two complementary
facets of variational free energy minimization through active inference. As we demonstrated with simulations,
this unification naturally endows the proposed architecture with characteristics of continual or lifelong
learning - the ability for machines to gather data and fine-tune its internal representation continuously during
functioning [Wang et al., 2023]. As inference in the proposed network involves spontaneously “replaying”
its own attractors (or sequences thereof), even if no external input is introduced (zero bias), the proposed
architecture may naturally overcome catastrophic forgetting [Aleixo et al., 2023]. To examine how these
properties of FEP-based attractor networksscale to more complex, diverse, and extended learning scenarios is
a promising direction for further studies. Stochasticity is another key property of our network, implementing
the precision of inference and allowing it to strike a balance between stability and flexibility. This inherent
17
stochasticity yields an exceptional fit with energy-efficient neuromorphic architectures [Schuman et al., 2022],
particularly within the emerging field of thermodynamic computing [Melanson et al., 2025]. Finally, the
recursive nature of our FEP-based formal framework provides a principled way to build hierarchical, multi-
scale attractor networks which may be exploited for boosting the efficiency, robustness and explainability of
large-scale AI systems. In general, the proposed architecture inherently embodies all the previously discussed
advantages of active inference and predictive coding [Millidge et al., 2022b, Salvatori et al., 2023]. At every
level of description the network dynamically balances accuracy and complexity and may naturally exhibit
“information-seeking” behaviors (curiosity) [Friston et al., 2017]. Furthermore, the architecture may offer a
foundation for exploring long-term philosophical implications of the qualities of active inference associated
with sentience [Pezzulo et al., 2024].
In conclusion, by deriving the emergence of adaptive, self-organizing - and self-orthogonalizing - attractor
networks from the FEP, this work offers a principled synthesis of self-organization, Bayesian inference, and
neural computation. The intrinsic tendency towards attractor orthogonalization, the multi-scale dynamics,
and the continuous learning capabilities present a compelling, theoretically grounded outlook for better
understanding natural intelligence and inspiring artificial counterparts through the lens of active inference.
6 Manuscript Information
6.1 Data availability
Simulation 1 is based on simulated data. Simulation 2-4 is based on the ’handwritten digits’ dataset available
in scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html,
and originally published as part of the thesis of C. Kaynak., 1995:
https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits.
6.1.1 Interactive manuscript
An interactive version of this manuscript (based on Myst Markdown) is available at: https://pni-
lab.github.io/fep-attractor-network/
6.1.2 Simulation source code
• Simulation 1: https://pni-lab.github.io/fep-attractor-network/simulation-demo
• Simulation 2: https://pni-lab.github.io/fep-attractor-network/simulation-digits
• Simulation 3: https://pni-lab.github.io/fep-attractor-network/simulation-digits-continuous-sequence
• Simulation 4: https://pni-lab.github.io/fep-attractor-network/simulation-digits-catastrophic-
forgetting
Acknowledgments
TSwassupportedbyfundingfromtheDeutscheForschungsgemeinschaft(DFG,GermanResearchFoundation)
— Project-ID 422744262 - TRR 289; and Project-ID 316803389 – SFB 1280 “Extinction Learning”. KF is
supported by funding from the Wellcome Trust (Ref: 226793/Z/22/Z).
References
Hermann Haken. Synergetics: An Introduction Nonequilibrium Phase Transitions and Self-Organization in
Physics, Chemistry and Biology. Springer Berlin Heidelberg, 1978. ISBN 9783642964695. doi:10.1007/978-
3-642-96469-5. URL http://dx.doi.org/10.1007/978-3-642-96469-5.
John Guckenheimer, Philip Holmes, and M. Slemrod. Nonlinear Oscillations Dynamical Systems, and
Bifurcations of Vector Fields. Journal of Applied Mechanics, 51(4):947–947, 12 1984. ISSN 1528-9036.
doi:10.1115/1.3167759. URL http://dx.doi.org/10.1115/1.3167759.
Daniel J. Amit. Modeling Brain Function: The World of Attractor Neural Networks. Cambridge University
Press, 9 1989. ISBN 9780511623257. doi:10.1017/cbo9780511623257. URL http://dx.doi.org/10.1017/
CBO9780511623257.
18
W. J. Freeman. Simulation of chaotic EEG patterns with a dynamic model of the olfactory system.
Biological Cybernetics, 56(2–3):139–150, 5 1987. ISSN 1432-0770. doi:10.1007/bf00317988. URL http:
//dx.doi.org/10.1007/BF00317988.
Gustavo Deco and Edmund T. Rolls. Attention and working memory: a dynamical model of neuronal activity
in the prefrontal cortex. European Journal of Neuroscience, 18(8):2374–2390, 10 2003. ISSN 1460-9568.
doi:10.1046/j.1460-9568.2003.02956.x. URL http://dx.doi.org/10.1046/j.1460-9568.2003.02956.x.
Ramón Nartallo-Kaluarachchi, Morten L. Kringelbach, Gustavo Deco, Renaud Lambiotte, and Alain Goriely.
Nonequilibrium physics of brain dynamics, 2025. URL https://arxiv.org/abs/2504.12188.
Misha Tsodyks. Attractor neural network models of spatial maps in hippocampus. Hippocampus, 9(4):
481–489, 1999. ISSN 1098-1063. doi:10.1002/(sici)1098-1063(1999)9:4<481::aid-hipo14>3.0.co;2-s. URL
http://dx.doi.org/10.1002/(SICI)1098-1063(1999)9:4%3C481::AID-HIPO14%3E3.0.CO;2-S.
Mikail Khona and Ila R. Fiete. Attractor and integrator networks in the brain. Nature Reviews Neuroscience,
23(12):744–766, 11 2022. ISSN 1471-0048. doi:10.1038/s41583-022-00642-0. URL http://dx.doi.org/10.
1038/s41583-022-00642-0.
K Zhang. Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble:
a theory. The Journal of Neuroscience, 16(6):2112–2126, 3 1996. ISSN 1529-2401. doi:10.1523/jneurosci.16-
06-02112.1996. URL http://dx.doi.org/10.1523/JNEUROSCI.16-06-02112.1996.
Edmund T. Rolls. Attractor networks. WIREs Cognitive Science, 1(1):119–134, 12 2009. ISSN 1939-5086.
doi:10.1002/wcs.1. URL http://dx.doi.org/10.1002/wcs.1.
J. A. Scott Kelso. Multistability and metastability: understanding dynamic coordination in the brain.
Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1591):906–918, 4 2012. ISSN
1471-2970. doi:10.1098/rstb.2011.0351. URL http://dx.doi.org/10.1098/rstb.2011.0351.
Karl Friston, Lancelot Da Costa, Dalton A.R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis, Maxwell
Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things. Physics of Life Reviews,
47:35–62, 12 2023. ISSN 1571-0645. doi:10.1016/j.plrev.2023.08.016. URL http://dx.doi.org/10.1016/
j.plrev.2023.08.016.
Karl Friston and Ping Ao. Free Energy, Value, and Attractors. Computational and Mathematical Methods
in Medicine, 2012:1–27, 2012. ISSN 1748-6718. doi:10.1155/2012/937860. URL http://dx.doi.org/10.
1155/2012/937860.
Ensor Rafael Palacios, Adeel Razi, Thomas Parr, Michael Kirchhoff, and Karl Friston. On Markov blankets
and hierarchical self-organisation. Journal of Theoretical Biology, 486:110089, 2 2020. ISSN 0022-5193.
doi:10.1016/j.jtbi.2019.110089. URL http://dx.doi.org/10.1016/j.jtbi.2019.110089.
Karl Friston, Lancelot Da Costa, Dalton A. R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis,
Maxwell Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things. 2022.
doi:10.48550/ARXIV.2210.12761. URL https://arxiv.org/abs/2210.12761.
Karl Friston. The free-energy principle: a rough guide to the brain? Trends in Cognitive Sciences, 13(7):
293–301, 7 2009. ISSN 1364-6613. doi:10.1016/j.tics.2009.04.005. URL http://dx.doi.org/10.1016/j.
tics.2009.04.005.
Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127–138,
1 2010. ISSN 1471-0048. doi:10.1038/nrn2787. URL http://dx.doi.org/10.1038/nrn2787.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John ODoherty, and Giovanni
Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879, 9 2016. ISSN
0149-7634. doi:10.1016/j.neubiorev.2016.06.022. URL http://dx.doi.org/10.1016/j.neubiorev.2016.
06.022.
Karl Friston. A free energy principle for a particular physics, 2019. URL https://arxiv.org/abs/1906.
10184.
Andy Clark. How to Knit Your Own Markov Blanket:. Philosophy and Predictive Processing, 2017.
doi:10.15502/9783958573031. URL http://www.predictive-mind.net/DOI?isbn=9783958573031.
InêsHipólito,MaxwellJDRamstead,LauraConvertino,AnjaliBhat,KarlFriston,andThomasParr. Markov
blankets in the brain. Neuroscience & Biobehavioral Reviews, 125:88–97, 2021.
P Ao. Potential in stochastic differential equations: novel construction. Journal of Physics A: Mathematical
and General, 37(3):L25–L30, 1 2004. ISSN 1361-6447. doi:10.1088/0305-4470/37/3/l01. URL http:
//dx.doi.org/10.1088/0305-4470/37/3/L01.
19
Jianhua Xing. Mapping between dissipative and Hamiltonian systems. Journal of Physics A: Mathematical
and Theoretical, 43(37):375003, 7 2010. ISSN 1751-8121. doi:10.1088/1751-8113/43/37/375003. URL
http://dx.doi.org/10.1088/1751-8113/43/37/375003.
Yuhang Song, Beren Millidge, Tommaso Salvatori, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz.
Inferring neural activity before plasticity as a foundation for learning beyond backpropagation. Nature
Neuroscience, 27(2):348–358, 1 2024. ISSN 1546-1726. doi:10.1038/s41593-023-01514-1. URL http:
//dx.doi.org/10.1038/s41593-023-01514-1.
Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, and Rafal Bogacz. A Theoretical
Framework for Inference and Learning in Predictive Coding Networks, 2022a. URL https://arxiv.org/
abs/2207.12316.
GeoffreyE.Hinton. TrainingProductsofExpertsbyMinimizingContrastiveDivergence. NeuralComputation,
14(8):1771–1800, 8 2002. ISSN 1530-888X. doi:10.1162/089976602760128018. URL http://dx.doi.org/
10.1162/089976602760128018.
Beren Millidge, Mufeng Tang, Mahyar Osanlouy, Nicol S. Harper, and Rafal Bogacz. Predictive coding
networks for temporal prediction. PLOS Computational Biology, 20(4):e1011183, 4 2024. ISSN 1553-7358.
doi:10.1371/journal.pcbi.1011183. URL http://dx.doi.org/10.1371/journal.pcbi.1011183.
Terence D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network.
Neural Networks, 2(6):459–473, 1 1989. ISSN 0893-6080. doi:10.1016/0893-6080(89)90044-0. URL http:
//dx.doi.org/10.1016/0893-6080(89)90044-0.
L. Personnaz, I. Guyon, and G. Dreyfus. Information storage and retrieval in spin-glass like neural networks.
JournaldePhysiqueLettres,46(8):359–365,1985. ISSN0302-072X. doi:10.1051/jphyslet:01985004608035900.
URL http://dx.doi.org/10.1051/jphyslet:01985004608035900.
I.KanterandH.Sompolinsky. Associativerecallofmemorywithouterrors. Physical Review A,35(1):380–392,
1 1987. ISSN 0556-2791. doi:10.1103/physreva.35.380. URL http://dx.doi.org/10.1103/physreva.35.
380.
D Ackley, G Hinton, and T Sejnoski. A learning algorithm for boltzmann machines. Cognitive Science, 9
(1):147–169, 3 1985. ISSN 0364-0213. doi:10.1016/s0364-0213(85)80012-4. URL http://dx.doi.org/10.
1016/S0364-0213(85)80012-4.
Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–1780,
11 1997. ISSN 1530-888X. doi:10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10.1162/neco.
1997.9.8.1735.
J J Hopfield. Neural networks and physical systems with emergent collective computational abili-
ties. Proceedings of the National Academy of Sciences, 79(8):2554–2558, 4 1982. ISSN 1091-6490.
doi:10.1073/pnas.79.8.2554. URL http://dx.doi.org/10.1073/pnas.79.8.2554.
Michael W Cole, Takuya Ito, Danielle S Bassett, and Douglas H Schultz. Activity flow over resting-state
networks shapes cognitive task activations. Nature Neuroscience, 19(12):1718–1726, 10 2016. ISSN
1546-1726. doi:10.1038/nn.4406. URL http://dx.doi.org/10.1038/nn.4406.
Ruben Sanchez-Romero, Takuya Ito, Ravi D. Mill, Stephen José Hanson, and Michael W. Cole. Causally
informed activity flow models provide mechanistic insight into network-generated cognitive activations.
NeuroImage, 278:120300, 9 2023. ISSN 1053-8119. doi:10.1016/j.neuroimage.2023.120300. URL http:
//dx.doi.org/10.1016/j.neuroimage.2023.120300.
Michael W. Cole. The explanatory power of activity flow models of brain function, 2024. URL https:
//arxiv.org/abs/2402.02191.
P. Földiák. Forming sparse representations by local anti-Hebbian learning. Biological Cybernetics, 64(2):165–
170, 12 1990. ISSN 1432-0770. doi:10.1007/bf02331346. URL http://dx.doi.org/10.1007/BF02331346.
Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: a functional interpretation
of some extra-classical receptive-field effects. Nature Neuroscience, 2(1):79–87, 1 1999. ISSN 1546-1726.
doi:10.1038/4580. URL http://dx.doi.org/10.1038/4580.
BerenMillidge,TommasoSalvatori,YuhangSong,RafalBogacz,andThomasLukasiewicz. PredictiveCoding:
Towards a Future of Deep Learning beyond Backpropagation?, 2022b. URL https://arxiv.org/abs/
2202.09467.
Everton L. Aleixo, Juan G. Colonna, Marco Cristo, and Everlandio Fernandes. Catastrophic Forgetting in
Deep Learning: A Comprehensive Taxonomy, 2023. URL https://arxiv.org/abs/2312.10549.
20
LarsBuesing,JohannesBill,BernhardNessler,andWolfgangMaass. NeuralDynamicsasSampling: AModel
for Stochastic Computation in Recurrent Networks of Spiking Neurons. PLoS Computational Biology, 7
(11):e1002211, 11 2011. ISSN 1553-7358. doi:10.1371/journal.pcbi.1002211. URL http://dx.doi.org/10.
1371/journal.pcbi.1002211.
Andrew Gelman and Donald B. Rubin. Inference from Iterative Simulation Using Multiple Sequences.
Statistical Science, 7(4), 11 1992. ISSN 0883-4237. doi:10.1214/ss/1177011136. URL http://dx.doi.org/
10.1214/ss/1177011136.
Zilu Liu, Fang Han, and Qingyun Wang. A review of computational models for gamma oscillation dynamics:
from spiking neurons to neural masses. Nonlinear Dynamics, 108(3):1849–1866, 3 2022. ISSN 1573-269X.
doi:10.1007/s11071-022-07298-6. URL http://dx.doi.org/10.1007/s11071-022-07298-6.
David H. Wagner. The Existence and Behavior of Viscous Structure for Plane Detonation Waves. SIAM
Journal on Mathematical Analysis, 20(5):1035–1054, 9 1989. ISSN 1095-7154. doi:10.1137/0520069. URL
http://dx.doi.org/10.1137/0520069.
Kenneth K.C. Man, Esther W. Chan, Patrick Ip, David Coghill, Emily Simonoff, Phyllis K.L. Chan,
Wallis C.Y. Lau, Martijn J. Schuemie, Miriam C.J.M. Sturkenboom, and Ian C.K. Wong. Prenatal
antidepressant exposure and the risk of attention-deficit hyperactivity disorder in children: A systematic
review and meta-analysis. Neuroscience & Biobehavioral Reviews, 86:1–11, 3 2018. ISSN 0149-7634.
doi:10.1016/j.neubiorev.2017.12.007. URL http://dx.doi.org/10.1016/j.neubiorev.2017.12.007.
Michael Breakspear. Dynamic models of large-scale brain activity. Nature Neuroscience, 20(3):340–352, 2
2017. ISSN 1546-1726. doi:10.1038/nn.4497. URL http://dx.doi.org/10.1038/nn.4497.
Gustavo Deco and Viktor K. Jirsa. Ongoing Cortical Activity at Rest: Criticality, Multistability,
and Ghost Attractors. The Journal of Neuroscience, 32(10):3366–3375, 3 2012. ISSN 1529-2401.
doi:10.1523/jneurosci.2523-11.2012. URL http://dx.doi.org/10.1523/JNEUROSCI.2523-11.2012.
GiorgioGosti,EdoardoMilanetti,ViolaFolli,FrancescodePasquale,MarcoLeonetti,MaurizioCorbetta,Gian-
carloRuocco,andStefaniaDellaPenna. ArecurrentHopfieldnetworkforestimatingmeso-scaleeffectivecon-
nectivity in MEG. Neural Networks, 170:72–93, 2 2024. ISSN 0893-6080. doi:10.1016/j.neunet.2023.11.027.
URL http://dx.doi.org/10.1016/j.neunet.2023.11.027.
Ruiqi Chen, Matthew Singh, Todd S. Braver, and ShiNung Ching. Dynamical models reveal anatomically
reliable attractor landscapes embedded in resting-state brain networks. Imaging Neuroscience, 3, 2025.
ISSN 2837-6056. doi:10.1162/imag_a_00442. URL http://dx.doi.org/10.1162/imag_a_00442.
RobertEnglert,BalintKincses,RavitejaKotikalapudi,GiuseppeGallitto,JialinLi,KevinHoffschlag,Choong-
WanWoo,TorD.Wager,DagmarTimmann,UlrikeBingel,andTamasSpisak.Connectome-BasedAttractor
Dynamics Underlie Brain Activity in Rest, Task, and Disease. 11 2023. doi:10.1101/2023.11.03.565516.
URL http://dx.doi.org/10.1101/2023.11.03.565516.
Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, and Rafal Bogacz. Backpropagation
at the Infinitesimal Inference Limit of Energy-Based Models: Unifying Predictive Coding, Equilibrium
Propagation, and Contrastive Hebbian Learning, 2022c. URL https://arxiv.org/abs/2206.02629.
Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl
Friston, and Alexander Ororbia. A Survey on Brain-Inspired Deep Learning via Predictive Coding, 2023.
URL https://arxiv.org/abs/2308.07870.
Tommaso Salvatori, Yuhang Song, Yujian Hong, Simon Frieder, Lei Sha, Zhenghua Xu, Rafal Bogacz, and
Thomas Lukasiewicz. Associative Memories via Predictive Coding, 2021. URL https://arxiv.org/abs/
2109.08063.
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A Comprehensive Survey of Continual Learning:
Theory, Method and Application, 2023. URL https://arxiv.org/abs/2302.00487.
Catherine D. Schuman, Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay.
Opportunities for neuromorphic computing algorithms and applications. Nature Computational Science, 2
(1):10–19, 1 2022. ISSN 2662-8457. doi:10.1038/s43588-021-00184-y. URL http://dx.doi.org/10.1038/
s43588-021-00184-y.
Denis Melanson, Mohammad Abu Khater, Maxwell Aifer, Kaelan Donatella, Max Hunter Gordon, Thomas
Ahle, Gavin Crooks, Antonio J. Martinez, Faris Sbahi, and Patrick J. Coles. Thermodynamic computing
system for AI applications. Nature Communications, 16(1), 4 2025. ISSN 2041-1723. doi:10.1038/s41467-
025-59011-x. URL http://dx.doi.org/10.1038/s41467-025-59011-x.
21
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka.
Active Inference, Curiosity and Insight. Neural Computation, 29(10):2633–2683, 10 2017. ISSN 1530-888X.
doi:10.1162/neco_a_00999. URL http://dx.doi.org/10.1162/neco_a_00999.
Giovanni Pezzulo, Thomas Parr, and Karl Friston. Active inference as a theory of sentient behavior.
Biological Psychology, 186:108741, 2 2024. ISSN 0301-0511. doi:10.1016/j.biopsycho.2023.108741. URL
http://dx.doi.org/10.1016/j.biopsycho.2023.108741.
22
Appendix
Library imports for inline code.
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
import scipy.integrate as integrate
Appendix 1
Continuous Bernoulli distribution
x∼CB(L) ⇐⇒ P(x)∝eLx, x∈[−1,1]⊂R
Below we provide a Python implementation of the continuous Bernoulli distributions: CB(L), parametrized
by the log odds L and adjusted to the [-1,1] interval.
def CB(x, b, logodds=True):
if not logodds:
b=np.log(b/(1 -b))
if np.isclose(b, 0):
return np.ones_like(x)/2
else:
return b * np.exp(b*x) / (2*np.sinh(b))
# Plot some examples
delta = 100
sigma = np.linspace( -1, 1, delta)
for b in np.linspace( -2, 2, 5):
p_sigma = CB(sigma, b)
sns.lineplot(x=sigma, y=p_sigma, label=b)
23
Appendix 2
Continuous Bernoulli distribution full derivation
Derivation of the exponential form of the continuous Bernoulli distribution, parametrized by the log odds L
and adjusted to the [-1,1] interval.
P(x;L)= R1 eL e x Lx = eL e − L e x −L =L 2si e n L h x (L)
−1 L
Appendix 3
Visualization of the likelihood function with various parameters, in python.
delta = 100
s_i = np.linspace( -1, 1, delta)
fig, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)
for idx, mu in enumerate([0.1, 0.5, 1]):
for w_i in np.linspace( -2, 2, 5):
p_mu = CB(s_i, w_i*mu)
sns.lineplot(x=s_i, y=p_mu, ax=axes[idx], label=w_i).set(title=f"$\\mu={mu}$")
Appendix 4
Expected value of the CB
(cid:16) (cid:17)
E [σ]= Z σ ebσ dσ = b (b− b 1 2 )eb + (b+1 b2 )e−b =coth(b)− 1 (30)
CB(b) 2sinh(b) 2sinh(b) b
bs = np.linspace( -10, 10, 100)
plt.figure(figsize=(4, 1))
sns.lineplot(x=bs, y=1/np.tanh(bs) - 1/bs) # coth(b) - 1/b == 1/2 sech^2(b)
plt.show()
24
Appendix 5
Conservative dynamics
Let x∈Rn denote the system’s states (internal, blanket, and external states). The drift can be decomposed
into a gradient part (from a potential U) and a solenoidal part R:
x˙ =−∇U(x)+R(x)
where R=−R⊺ is antisymmetric in state-space.
The probability density p(x,t) over states evolves according to the Fokker–Planck equation:
∂ p(x,t)=−∇·[(−∇U +R)p(x,t)]+diffusion terms.
t
To determine the stationary distribution p (x), we set ∂ p(x,t) = 0. The Fokker-Planck equation
s t
then implies that the net probability current J (x)=(−∇U(x)+R(x))p (x)−D∇p (x) (assuming isotropic
s s s
diffusion D) must be divergence-free: ∇·J (x)=0. If we propose p (x)=Z−1exp(−U(x)/D) (where Z is a
s s
normalization constant), then the diffusive current −D∇p (x) becomes p (x)∇U(x).
s s
Substituting this into J (x), we get:
s
J (x)=(−∇U(x)+R(x))p (x)+p (x)∇U(x)=R(x)p (x).
s s s s
Thus, p (x) ∝ exp(−U(x)/D) is the stationary distribution if and only if the solenoidal component
s
of the probability current, J (x)=R(x)p (x), is itself divergence-free: ∇·(R(x)p (x))=0.
R s s
It is a key result from the study of non-equilibrium systems under the Free Energy Principle, particularly
thosepossessingaMarkovblanketandinvolvingconservativeparticles,thatthiscondition∇·(R(x)p (x))=0
s
holds (see [Friston et al., 2023]; but also related: Ao [2004]; Xing [2010]). The conditional independence
structure imposed by the Markov blanket, along with other FEP-specific assumptions (like conservative
particles), constrains the system’s dynamics such that solenoidal forces R(x) do not alter the Boltzmann
form of the stationary distribution. Instead, they drive persistent, divergence-free probability currents J (x)
R
along iso-potential surfaces.
In steady state (∂ p = 0), only the gradient term −∇U influences p(x). The stationary (nonequi-
t
librium) distribution is
p(x)∝exp−U(x),
determined solely by the symmetric (potential) part −∇U. The antisymmetric R does not reweight p(x); it
just circulates probability around isocontours of U.
In most NESS systems, antisymmetric flows do alter the stationary measure. Under particular-partition
(Markov-blanket) constraints, however, the internal–external factorization ensures that solenoidal (antisym-
metric) flows remain divergence-free, leaving the Boltzmann-like steady distribution intact. This is why,
in these Markov-blanketed systems, one can have persistent solenoidal currents (nonequilibrium flows) yet
preserve a stationary distribution that depends only on the symmetric part of the couplings.
Appendix 6
∂F
Detailed derivation of
∂b
q
1. Subsitute our parametrization into F
Let’s start with substituting our parametrization into eq. .
1a. Accuracy term
25
From the RBM marginalization (eq. ):
X X 1 X
E(σ)=−b σ − J σ σ − b σ − J σ σ
i i ij i j j j 2 jk j k
j̸=i j̸=i j,k̸=i
| {z }| {z }
Termswithσi Termswithoutσi
Here, −b σ becomes constant, since σ is fixed. So we get:
i i i
(cid:16) (cid:17)
P(σ |σ )∝exp
P
(b +J σ )σ +
1P
J σ σ
\i i j̸=i j ij i j 2 j,k̸=i jk j k
Taking expectation of lnP(σ |σ ) under q(σ ):
\i i i
Eq[lnP(σ |σ )]=const+ P b σ +S(b ) P J σ + 1P J σ σ
\i i j̸=i j j q j̸=i ij j 2 j,k̸=i jk j k
Where S(b ) = E [σ ] = cothb − 1/b is the expected value of the CB, a sigmoid function of the
q q i q q
bias (#supplementary-information-4)).
1b. Complexity term
The complexity term in eq. is simply the KL-divergence term between two CB distributions. For
CB distributions:
q(x)= bq ebqx, p(x)= b ebx
2sinhbq 2sinhb
· KL divergence definition:
D = R1 q(x)ln q(x)dx=E [lnq(x)−lnp(x)]
KL −1 p(x) q
· Expand log terms:
lnq(x)=lnb −ln(2sinhb )+b x
q q q
lnp(x)=lnb−ln(2sinhb)+bx
· Subtract log terms:
ln q(x) =lnbq +ln sinhb +(b −b)x
p(x) b sinhbq q
· Take expectation under q(x):
D =lnbq +ln sinhb +(b −b)E [x]
KL b sinhbq q q
· Compute expectation E [x]:
q
E [x]= R1 x bqebqx dx= 1 h ebqx(b x−1) i1
q −1 2sinhbq 2sinhbq b2
q
q
−1
· Evaluate at bounds:
= 1 (cid:16) ebq(bq−1)−e−bq(−bq−1) (cid:17)
2sinhbq b2
q
· Simplify using hyperbolic identities:
= (bqcoshbq−sinhbq) =cothb − 1
b2
q
sinhbq q bq
· Final substitution for the complexity term:
(cid:16) (cid:17)
D =lnbqsinhb +(b −b) cothb − 1
KL bsinhbq q q bq
26
1c. Combining the two terms
Combining the two terms, we get the following expression for the free energy:
(cid:16) (cid:17) (cid:16) (cid:17) 1
F =ln bq +ln sinh(b) +(b −b)S(b )− P (b +S(b )J )σ − P P J σ σ +C
b sinh(bq) q q j̸=i j q ij j 2 j̸=i k̸=i jk j k
where C denotes all constants in the equation that are independent of σorb .
q
2. Free Energy partial derivative calculation
· First, we differentiate the log terms:
h i
∂ lnbq +ln sinhb = 1 −cothb
∂bq b sinhbq bq q
· Then, the KL core term:
∂ [(b −b)S(b )]=S(b )+(b −b)dS
∂bq q q q q dbq
· The linear terms:
h i
∂ − P (b +S(b )J )σ =− P J σ dS
∂bq j̸=i j q ij j j̸=i ij jdbq
· The constants vanish.
· Now, combining all terms:
(cid:16) (cid:17) (cid:16) (cid:17)
∂F = 1 −cothb + S(b )+(b −b)dS − P J σ dS
∂bq bq q q q dbq j̸=i ij jdbq
· Substituting S(b )=cothb −1/b :
q q q
(cid:16) (cid:17) (cid:16) (cid:17)
= 1 −cothb + cothb − 1 +(b −b)dS − P J σ dS
bq q q bq q dbq j̸=i ij jdbq
· Cancel terms:
(cid:19)b
1(cid:19)
q
(cid:24)−(cid:24)cot (cid:24) h (cid:24)(cid:24) b
q
+(cid:24)co(cid:24)th (cid:24)(cid:24) b
q(cid:26)
− (cid:26)
b
1
q
+(b
q
−b)
d
d
b
S
q
− P
j̸=i
J
ij
σ
jd
d
b
S
q
Gives us the final derivative:
(cid:16) (cid:17)
∂F = b −b− P J σ dS
∂bq q j̸=i ij j dbq
Where dS =−csch2b + 1.
dbq q b2
q
P
Setting the derivative to zero and solving for b , we get: b =b+ J σ
q q j̸=i ij j
NowwerememberthattheexpectedvalueoftheCBistheLangevinfunctionofitsbias-,E(x)=coth(b)−1/b
(). For simplicity, we will denote it as L(x). Now we can write:
(cid:16) (cid:17)
E [σ ]=L(b )=L b+ P J σ
q i q j̸=i ij j
Q.E.D.
27