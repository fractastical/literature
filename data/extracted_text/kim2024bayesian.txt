4202
tcO
3
]CN.oib-q[
1v27920.0142:viXra
Bayesian Mechanics of Synaptic Learning under the
Free Energy Principle
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186,Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract. Thebrainisabiologicalsystemcomprisingnervecellsandorchestratesits
embodiedagent’sperception,behavior,andlearninginthedynamicenvironment. The
free energy principle (FEP) advocated by Karl Friston explicates the local, recurrent,
andself-supervisedneurodynamicsofthebrain’shigher-orderfunctions. Inthispaper,
we continue to finesse the FEP through the physics-guided formulation; specifically,
we apply our theoryto synaptic learningby consideringit aninference problemunder
the FEP and derive the governing equations, called Bayesian mechanics. Our study
uncovers how the brain infers weight change and postsynaptic activity, conditioned
on the presynaptic input, by deploying the generative models of the likelihood and
prior belief. Consequently, we exemplify the synaptic plasticity in the brain with a
simple model: we illustrate that the brain organizes an optimal trajectory in neural
phasespaceduringsynapticlearningincontinuoustime,whichvariationallyminimizes
synaptic surprisal.
Keywords free energy principle; synaptic learning; Bayesian mechanics; continuous-
state formulation
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 2
1. Introduction
The brain’s perception, body movement, and learning are conjointly organized to
ensure the embodied agents’ homeostasis and adaptive fitness in the environment. It
is tempting to imagine a neural observer in the brain presiding over higher animals’
cognitive control. Such a homunculus idea is untenable and must be discarded in the
present-day brain theory [1]. However, there is still a much distance to the complete
scientific understanding of the emergent higher-order functions from the brain matter; it
demands a comprehension of the profound interplay between the scientific reductionism
and teleological holism standpoints [2, 3].
The brain-inspired FEP is a purposive theory that bridges the gap between top-
down teleology and bottom-up scientific constructionism. According to the FEP [4, 5],
allliving systems areself-organizedtotendtoavoidanatypical niche intheenvironment
forexistence. TheFEPadoptstheautopoietichypothesis[6]andscientificallyformalizes
the abductive rationale of organisms’ making optimal predictions and behavior from
incomplete sensory data. To be precise, the FEP suggests an information-theoretic,
variational measure of environmental atypicality, termed free energy (FE). The FE
objective is technically defined as a functional of the probabilistic generative density
specifying the brain’s internal model of sensory-data generation and environmental
change and an online auxiliary density actuating variation. The Bayesian brain
computes the posterior of the environmental causes of uncertain sensory data by
minimizing the FE, whose detailed continuous-state description can be found in [7]. For
discrete-state models of the FEP with discrete time, we recommend [8, 9] to readers.
When a Gaussian probability is employed for the variational density [10], the FE
becomes a L 2 norm specified by the Gaussian means and variances and termed the
Laplace-encoded FE [7]. Thus, the Laplace-encoded FE provides a scientific base of the
L 2 objectives in a principled manner, which are widely used in machine learning and
artificial intelligence. For instance, the optimization function in the predictive-coding
framework is proposed to be a sum of the squared prediction errors [11]. Also, the
loss function of a typical artificial neural network (ANN) is often written as a sum of
squareddifferencesbetweenthegroundtruthandthepredictiveentriesfromthenetwork
[12]. Furthermore, it is argued that the Gaussian sufficient statistics are encoded by
the biophysical brain variables, which form the brain’s low-dimensional representations
of environmental states. This way the brain acquires access to the encoded FE for
minimization as it becomes fully specified in terms of the brain’s internal states.
Our research over the years has been devoted to developing continuous-state
implementation of the FE minimization in a manner guided by physics laws and
principles [13, 14, 15]. We endeavored to advance the FEP to the point where it
coalesces into a unified principle of top-down architecture and material base. Moreover,
to promote the FEP to nonstationary problems, we incorporated the fact that the
physical brainis ina nonequilibrium (NEQ) stationary stateandis generally continually
aroused by nonstationary sensory stimuli. The functional brain must perform the
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 3
variational Bayesian inversion of nonstationary sensory data to compute the posterior
mentioned above. Previously, we accounted for the brain behavior of perception and
motor control as described by attractor dynamics and termed the governing equations
Bayesian mechanics (BM). The BM coordinates the brain’s sensory estimation and
motor prediction in neural phase space. In this paper, we make further progress by
incorporating the brain’s synaptic learning into the BM, which we did not accommodate
in our earlier studies. Learning constitutes the crucial brain function of consolidating
memory, e.g., via Hebbian plasticity [16].
This paper aims to provide a simple but insightful model for synaptic learning
in the brain. Our agendas are that the functional brain operates continually using
continuous environmental representations and that synaptic learning is a cognitive
phenomenon that may very well be understood when statistical-physical laws guide
it. The notion cognition throughout this paper is meant to be the brain’s higher-
order capability that involves a top-down, internal model. We consider the NEQ brain
a problem-solving matter, cognitively interacting with the environment. To quantify
the synaptic cognition, we will specify the generative densities furnishing the Laplace-
encoded FEin a manner to meet the NEQ stationarity and present the FE minimization
scheme by practicing the principle of least action (Hamilton’s principle) [17]. The novel
contributions worked out in this paper are discussed in Section 7.
The rest of the paper is organized as follows. In Section 2, the single-synapse
structure of our interest is described. The essence of the FEP is recapitulated with
revision made for synaptic learning in Section 3. In Section 4, an NEQ formulation
is presented, which determines the likelihood and prior densities in the physical brain.
Next, Section 5 identifies the FEobjective asa classical action andderives the governing
equations of synaptic dynamics by exercising the Hamilton principle. The utility of our
theory is demonstrated in Section 6 using a simple model. After the discussion in
Section 7, a conclusion is given in Section 8.
2. Single Synapse Model
This workconcerns thebrain’ssynapticlearning without considering howenvironmental
processes arouse stimuli at the sensory interface; as a parsimonious model, we focus
on a single synapse within the brain’s internal environment. For instance, in the
hippocampus, the postsynaptic action potential in the dentate gyrus is evoked by a
presynaptic signal from the entorhinal cortex caused by a neural signal from other
brain areas. Accordingly, the synaptic coupling between two pyramidal neurons in the
hippocampus constitutes a single synaptic assembly of interest.
We depict the single synaptic model in Figure 1, where the presynaptic and
postsynaptic signals are denoted by s and µ, respectively; both are the brain’s
representations of noisy synaptic signals. In addition, thesynaptic plasticity is mediated
by the weight strength denoted by w. The synaptic structure considered is generic for
all neurons; accordingly, the ensuing formulation below applies to other brain regions.
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 4
Figure 1. Single synaptic assembly. The postsynaptic neural state µ is
neurophysicallyevokedbythepresynapticsignals,mediatedbytheweightchange∆w
accordingtoHebb’srule9sµ. WeadopttheBayesian-inferenceperspective,suggesting
thatthe brainstate µ infers the causeofthe presynapticinput s, andthe weightstate
w makes up the synaptic input-output interface.
Note that we will handle the weight variable w as a neurophysical degree of freedom
like s and µ; this handling contrasts with ANN models, where the weights are treated
as a static parameter.
3. Free Energy Principle for Synaptic Learning
The brain-inspired FEP is built on three hypotheses: 1) surprisal hypothesis, 2)
representation hypothesis, and 3) computability hypothesis, which we recapitulate here
with the revision applying to the synaptic learning problem.
3.1. Surprisal Hypothesis
We assume that the presynaptic signals s streaming into the synaptic interface are
prescribed and focus on the resulting synaptic dynamics. For convenience, here we
introduce the notation ϑ˜ , by which we collectively denote the postsynaptic variable M
and the weight variable W:
ϑ˜ M,W .
“ t u
The variables M and W are stochastic and unknown, so they are hidden from the
brain’s perspective.
The brain’s cognitive goal is to compute the posterior p ϑ˜s , which the FEP fulfills
p | q
via variational Bayes in the following manner: First, we define the information-theoretic
measure called the Kullback-Leibler (KL) divergence:
q ϑ˜
D q ϑ˜ p ϑ˜s dϑ˜q ϑ˜ ln p q , (1)
KL p q} p | q “ p q p ϑ˜s
´ ¯ ż p | q
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 5
which is always positive [18], where dϑ˜ means dMdW. Some terminologies: q ϑ˜ is
p q
called R-density, which approximates the trueposterior p ϑ˜s inthe variational scheme.
p | q
The posterior makes up the so-called G-density p ϑ˜,s p ϑ˜s p s together with the
p q “ p | q p q
marginal density p s [7]. Second, using the preceding product rule, the above KL
p q
divergence can be decomposed to
D q ϑ˜ p ϑ˜s F q ϑ˜ ,p ϑ˜,s lnp s .
KL
p q} p | q “ r p q p qs` p q
The functional F o ´ n the right-h ¯ and side (RHS), which is identified to be
q ϑ
F q ϑ˜ ,p ϑ˜,s dϑ˜ q ϑ˜ ln p q , (2)
r p q p qs ” p q p ϑ˜,s
ż p q
istheinformational freeenergy(FE).Third, thepositivityofD leadstotheinequality
KL
lnp s F q ϑ˜ ,p ϑ˜,s . (3)
´ p q ď r p q p qs
Equation (3) is the mathematical statement of the brain-inspired FEP accounting
for life and cognitive phenomena in a universal manner, which comprises the surprisal
hypothesis. In the present context, the preceding inequality enunciates that synaptic
learningcorrespondstothebrain’sminimizing F, whichisaproxyforsynaptic surprisal,
lnp s , as an upper bound. In practice, it is intractable to determine the marginal
´ p q
density p s , which provides synaptic evidence to the brain. Note here that F is called
p q
FE by mimicking thermodynamic FE in physics, which monotonically decreases upon
spontaneous changes in a macroscopic open system, conforming to the second law of
thermodynamics [15].
3.2. Representation Hypothesis
Accordingtotheinequality[Equation(3)], thebrainvariationallyminimizesF bymeans
of the R-density q ϑ˜ : when the synaptic interface is elicited by the presynaptic stream
p q
s, the brain launches q ϑ˜ , an online approximation of the posterior; in the face of the
p q
synaptic stream, the R-density probabilistically represents the uncertain, hidden causes
ϑ˜ M,W , trying to match best with the posterior. Here, we adopt the Laplace
“ t u
approximation for the R-density, which assumes a Gaussian form [19]:
1 1
q ϑ˜ exp ϑ˜ µ˜ 2 , (4)
p q “ ?2πσ˜2 r´2σ˜2p ´ q s
where µ˜ and σ˜ are the sufficient statistics of the Gaussian density. In particular, we
intend that the means denoted by
µ˜ µ,w
“ t u
are the coarse-grained representations of high-dimensional ϑ˜ M,W ; they are the
“ t u
latent brain variables in low dimensional neural space [20]. Also, the dependence on
the variances σ˜ can be eliminated by further manipulation as elaborated in [7]. Then,
under the Laplace approximation, the FE functional reduces to
F q ϑ˜ ,p ϑ˜,s lnp µ˜,s constants.
r p q p qs “ ´ p q`
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 6
The nontrivial part in the reduced expression is the Laplace-encoded FE denoted by F:
F µ,w,s lnp µ,w,s , (5)
p q “ ´ p q
which is a function of only the brain variables µ and w. In the present work, the
presynaptic input s is not a dynamical variable but is handled as an external time-
dependent input.
Here, the brain is assumed to be endowed with the generative density p µ,w,s
p q
encoded over the evolutionary and developmental time scales. The FEP inequality
given in Equation (3) now becomes
lnp s F µ,w,s ; (6)
´ p q ď p q
the brain has an access to F µ,w,s by means of its internal variables µ and w. The
p q
preceding expression comprises the representation hypothesis in the FEP; the brain
uses the coarse-grained representations µ and w in variationally minimizing synaptic
surprisal. Then, by applying the product rule p µ,w,s p w µ,s p µ,s , the Laplace-
p q “ p | q p q
encoded FE is completed as
F µ,w s lnp w µ,s p µ,s , (7)
p | q “ ´ p | q p q
where p w µ,s is the likelihood of the weight strength w given a postsynaptic signal µ,
p | q
andp µ,s istheprioraboutthepostsynapticdynamics, bothsubject tothepresynaptic
p q
input s.
Equation(7)istheobjectivefunctionforsynapticlearningundertheFEP,furnished
with only brain variables, which makes the brain-inspired FEP a biologically plausible
theory. Previously, we suggested that all the involved probabilities be specified as NEQ
stationary densities derived from the Fokker–Planck equation [15]. This work takes a
different approach to determining the NEQ densities in Section 4.
3.3. Computability Hypothesis
The brain is endowed with the mechanism that actuates the FE minimization, which
comprises the computability hypothesis in the FEP. The conventional continuous-state
implementation assumes that the brain employs gradient descent (GD) methods to
execute theFEminimization [5]. The GDschemes updatethe neural activity µ downhill
on the FE landscape, which is woven by the generalized coordinates of motion of all
dynamical orders surpassing the second order, namely, acceleration [21, 22]. It is argued
that generalized motion can effectively incorporate the temporal correlation of random
fluctuationsinstochasticdynamics beyond whitenoise. However, theidea ofgeneralized
motion transcends normative Newtonian physics; thus, its theoretical ground draws
critical attention in the literature [23, 14]. For the weight variable w, to incorporate its
slower change than the neural activity, a different update rule is applied: for instance,
instead of the weight (parameters or hyper-parameters), its rate may be updated under
the GD scheme [7]. Recently, researchers have extended the applicability of FEP-based
GD algorithms to robotics and artificial intelligence problems, emphasizing colored-
noise modeling [24]. However, it is significant to note that using GD methods is not
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 7
legitimate when the environmental inputs vary fast so that the FE landscape becomes
non-static (see, for further discussion, Section 7). Our formulation aims at the general
time-dependent situation and, thus, avoids using a GD scheme; instead, we identify the
FE objective to be a classical action in mechanics and exercise Hamilton’s principle
for the FE minimization according to the standard theory [17]. The details are given
in Section 5, where we derive the governing equations of motion for synaptic inference
regarding the canonical physical variables without invoking the generalized motion.
4. Nonequilibrium Generative Densities
We argued that the physical brain probabilistically encodes the representations of the
internal and external hidden states (Section 3.2). The encoded probabilities constitute
thegenerative densities thatfurnishthebrainwithFEobjective forvariationalBayesian
inference. Therefore, the generative densities must be specified in terms of the
biophysical brain variables in an NEQ stationary state. Here, we present a stochastic
thermodynamic model for the NEQ densities, viewing the brain as a soft material
consisting of neural constituents. This perspective brings us closer to understanding
the brain’s NEQ states.
4.1. Prior for Postsynaptic Activity
For a simple description, we assume that the brain variable µ obeys an overdamped
Langevin dynamics on a mesoscopic scale:
dµ
f µ,θ ξ, (8)
dt “ p q`
where f and ξ on the RHS are the deterministic and random forces, respectively,
causing the neural change; θ encapsulated in f denotes an input parameter affecting the
system dynamics. The solution to Equation (8) describes a stochastic path or trajectory
µ µ t in continuous state space. Recall that the neural variable µ is the mean of
“ p q
the R-density probabilistically representing external environmental states online, which
may be viewed as a mean field. Also, it is evident that the state transition between two
arbitrarily-close times described by Equation (8) is Markovian. Further assumptions
imposed are i) the noise ξ is Gaussian about zero mean, rendering ξ 0, and ii) the
x y “
noise is delta-correlated, a.k.a. white, through
ξ t1 ξ t σ 2 δ t1 t , (9)
µ
x p q p qy “ p ´ q
where
σ2
is the noise strength. Strictly considering, the biological brain is in an
µ
NEQ stationary state, whose temperature T is distinct from the environmental value;
however, here, we consider that the brain is locally in equilibrium characterized by its
body temperature. Also, we assume that the noise strength is given, according to the
fluctuation-dissipation theorem [25], as
σ
2 2γ´1
k T, (10)
µ “ µ B
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 8
where γ is the frictional coefficient of the brain matter, and k is the Boltzmann
µ B
constant.
Under the prescribed assumptions, we build the transition probability along a
trajectory µ µ t as time t elapses. To proceed with the derivation, we first note
“ p q
a technical subtlety involved in the white noise ξ t : it is mathematically ill-defined
p q
becausethevarianceisdivergent [seeEquation(9)]. Toaddressthis, theWiener process,
definedthrough∆W ξ∆t,isoftenconceived. Thisprocessintroducesaformofcoarse-
”
graining over a short time interval ∆t, effectively bypassing the singularity of the white
noise at an instant time. The Wiener process is also Gaussian about zero mean with
the well-defined variance ∆W 2 σ2 ∆t. However, one must pay the price for the
µ
xp q y “
Wiener recipe when the Riemann integral is performed for state functions over a finite-
time elapse. In our derivation, we adopt the Ito convention that interprets the integral
of Equation (8) over the time interval ∆t t n`1 t n as
“ ´
∆µ f µ ,θ ∆t ∆W ,
n n n n
“ p q `
where the first term on the RHS was approximated by choosing the value for f µ at
p q
the initial time t n ; other terms are ∆µ n µ n`1 µ n and ∆W n W n`1 W n . Next,
“ ´ “ ´
using the Gussianity of ∆W , we define the transition probability p n 1 n from the
n
p ` | q
Wiener state W n to the next W n`1 as [26]
1 2
p n 1 n exp ∆µ f µ ,θ ∆t .
p ` | q » ´2σ2∆t n ´ p n n q
" µ *
´ ¯
Then, the full Markovian transition over N t ∆t time steps during the finite time
p“ { q
0 t1 t can be built as
ď ď
N´1
∆t ∆µ 2
n
p n 1 n exp f µ ,θ .
p ` | q » ´2σ2 ∆t ´ p n n q
n
ź
“0 # µ
ÿ
n
´ ¯
+
As a final step, we take the continuous limit ∆t 0 in the preceding expression and
Ñ
obtain the path probability p µ,θ , up to a normalization constant, as
p q
1 t dµ 2
p µ,θ exp dt1 f µ,θ t1 , (11)
p q „ ´2σ2 dt1 ´ p p qq
# µ ż 0 ˆ ˙ +
which is known as the Onsager-Machlup function [27].
The above Onsager–Machlup expression specifies the transition probability of the
neural state µ, given initial condition µ 0 , along the continuous path µ µ t .
p q “ p q
When the parameter θ is replaced with s, it represents the prior density p µ,s in
p q
Equation (7) accounting for the brain’s belief about or already-acquired knowledge of
how the postsynaptic activity µ behaves.
4.2. Likelihood of Synaptic Change
Neurotransmitter transport at the synaptic interface mediates synaptic coupling
between two neurons, which is often effectively described by the weight variable w. We
assume that the brain is endowed with an internal model of weight dynamics leveraging
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 9
learning; learning constitutes the crucial brain function of consolidating memory, e.g.,
via long-term potentiation.
We consider the synaptic weight w a time-dependent variable rather than a static
parameter, and the synaptic plasticity is described by its the rate w9 dw dt. We
“ {
proposethatsimilartoEquation(8), thesynapticplasticityisgovernedbythestochastic
equation:
dw
h w,θ χ, (12)
dt “ p q`
where h is the biophysical force causing the weight change, and χ is the additive
white noise associated with the synaptic process; again, θ is a time-dependent input
parameter. The noise is assumed to be Gaussian about zero mean and delta-correlated:
χ t χ t1 σ2δ t t1 with σ2 being the noise strength.
x p q p qy “ w p ´ q w
Next, to smooth the temporal singularity associated with the white noise χ, we
consider the Wiener process ∆W χ∆t, which is also Gaussian about zero mean
“
with the well-defined variance, ∆W 2 σ2 ∆t. Then, we proceed with the same
xp q y “ w
formulation with Section 4.1 to specify the NEQ likelihood density p w θ . The result
p | q
is given as
1 t dw 2
p w θ exp dt1 h w,θ t1 , (13)
p | q „ ´2σ2 dt1 ´ p p qq
# w ż 0 ˆ ˙ +
which represents the Onsager–Machlup transition probability along the continuous path
w w t , subject to initial condition w 0 .
“ p q p q
In obtaining the above likelihood and prior densities, Equations (11) and (13),
respectively, we assumed that the random fluctuations in the neuronal dynamics were
delta-correlated, i.e., white noises. The brain signals, by contrast, evidently reveal
the frequency spectrum reflecting color-correlated dynamics [28], which supports the
criticality idea in the brain [29]. In this work, we consider only the ideal white noise for
a practical illustration of determining the NEQ brain densities in a physics-grounded
manner. To obtain an analytic expression for the NEQ densities is intractable under
generalconditionseveninthesteadystate[15]; theyareusuallyassumedtobeaninstant
Gaussian set by the Gaussian random noises imposed on the Langevin description [4, 5].
5. Bayesian Mechanics: Computability of Synaptic Learning
The FE landscape becomes nonstatic when the input parameter θ in the generative
densities [Equations (11) and (13)] is explicitly time-dependent. In this case, it
is anticipated that the GD implementation on the FE landscape will fail. Here,
we formulate the brain’s computability under nonstationary conditions, facilitating
nonautonomous neural computation.
In the synaptic learning problem, the presynaptic signal s acts as the input
parameter θ. Accordingly, we replace θ with s in the Onsager–Maclup representations
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 10
forthe NEQ densities and substitute the results into Equation(7) to obtaintheLaplace-
encoded FE. The outcome is given as
t
F L µ,w;µ9,w9 ;s dt1, (14)
“ p q
0
ż
where the integrand L is expressed as
2 2
1 dµ 1 dw
L µ,w;µ9,w9 ;s f µ,w;s t1 h µ,w,s t1 .(15)
p q ” 2σ2 dt1 ´ p p qq `2σ2 dt1 ´ p p qq
µ ˆ ˙ w ˆ ˙
Note that in Equation (15), we concretely displayed the autonomous dependence on
the variables µ and w and the nonautonomous dependence on the input s through the
generative functions f and h.
Equation (14) manifests a specific association of the FE objective F with the
mathematical object L; namely, F is given as a time integral of L. This observation
is reminiscent of the relation between the action and Lagrangian in classical mechanics
[17]. Accordingly, by analogy, if we identify F as an effective action S and the integrand
Lasaneffective Lagrangian forthebrain’scognitive computation, theFEminimization,
which is mathematically performed by δF 0 under the FEP, is precisely mapped to
“
exercising Hamilton’s principle, δS 0. Then, the Euler-Lagrange equations of motion
“
for determining the optimal trajectories µ t and w t will follow straightforwardly,
p q p q
constituting the synaptic BM. Note the temperature dependence of the Lagrangian
[Equation (15)] via the noisy strengths σ2 and σ2 , see Equation (10), which makes L a
µ w
thermal Lagrangian [27].
Here, working in the Hamiltonian description is more suitable for our purposes. To
this end, we carried out a Legendre transformation to derive an effective Hamiltonian
H; the outcome is expressed as
p2 p2
H µ w p f µ,w;s p h w,µ;s . (16)
µ w
“ 2m ` 2m ` p q` p q
µ w
In the preceding expression of Hamiltonian, the new variables p and p appear, which
µ w
are mechanically conjugate to the variables µ and w, respectively; they are determined
from the definitions:
L L
p B and p B . (17)
µ
“
µ9 w
“
w9
B B
Also, the constants, m and m were defined to be
µ w
2 2
m 1 σ and m 1 σ , (18)
µ “ { µ w “ { w
which are a measure of respective precision of the probabilistic generative models,
Equations (11) and (13). Equation (10) suggests that the generative precisions are
a biophysical constant specified by the body temperature and the friction of the brain
matter. A few points about the Hamiltonian H are noteworthy: The variables (µ, w)
and (p , p ) correspond to positions and momenta, respectively, and the generative
µ w
precisions m and m may be interpreted as a neural mass as a metaphor. The
µ w
Hamiltonian is not breakable into the kinetic and potential energies because the third
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 11
and fourth terms on the RHS in Equation (16) are given as a product of momentum
and position variables. The H function does not furnish a conservative-energy surface
because of its explicit time dependence through thepresynaptic signal s t , which makes
p q
synaptic learning a nonautonomous problem.
The generative functions f and h for synaptic learning were introduced in
Equations (8) and (12) without specifying them; they are the biophysical forces driving
synaptic dynamics at the neuronal level. We now specify them by the following models:
f µ,w;s γ µ µ ws, (19)
µ d
p q “ ´ p ´ q`
h µ,w;s γ w w sµ. (20)
w d
p q “ ´ p ´ q`
The first terms on the RHSs, involving the damping coefficients γ and γ , prevent
µ w
an unlimited growth of µ and w [30]. The linear damping models may be replaced
with a nonlinear alternative; for instance, the modified γ s2 w w may be used in
w d
´ p ´ q
Equation (20) [31]. The second term ws on the RHS of Equation (19) describes the
presynaptic evoking weighted by w. Moreover, the term sµ in Equation (20) accounts
for Hebb’s rule; one can explore anti-Hebbian learning by inverting its sign. The extra
parameters µ and w are the steady-state values of µ and w, respectively, without
d d
driving terms ws and sµ. After substituting Equations (19) and (20) into Equation (15)
and by evaluating Equation (17), one can determine the neural representations of the
momenta p and p . The results are given as
µ w
p m µ9 f , (21)
µ µ
“ p ´ q
p m w9 h . (22)
w w
“ p ´ q
Note that momentum represents the discrepancy between the state rate and its
prediction from the generative model, modulated by precision, which corresponds to
prediction error in predictive coding theory (see discussion in Section 7).
Having specified the synaptic Hamiltonian given in Equation (16), we now derive
Hamilton’s equations of motion by practicing the standard procedure [17]. Here, we
present only the outcome without intermediate steps:
1
µ9 p γ µ µ ws, (23)
µ µ d
“ m ´ p ´ q`
µ
1
w9 p γ w w sµ, (24)
w w d
“ m ´ p ´ q`
w
p9 γ p sp , (25)
µ µ µ w
“ ´
p9 γ p sp . (26)
w w w µ
“ ´
The resulting Equations (23)–(26) are a set of coupled differential equations for four
dynamical variables µ, w, p , and p , subject to the time-dependent input source
µ w
s, which constitute the synaptic BM governing co-evolution of the state and weight
variables. In Figure 2, we show the neural circuitry implied by the derived BM. We
argue that the functional behavior depicted in the circuitry is generic in every synapse
inthebrainlike every corticalcolumn intheneocortexbehaves asa sensorimotor system
performing the same intrinsic function [32].
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 12
Figure 2. Schematic of the neural circuitry. The diagram manifests the workings
of the synaptic BM: the presynaptic input sptq drives the interconnected, recurrent
dynamicsamongthestatepw,µqandmomentumpp ,p qvariables. Thelinksdepicted
w µ
by arrowheads indicate an excitatory coupling within a neural unit or between two
neural units, whereas the dot-head links indicate an inhibitory coupling.
For a more compact description, we shall define the cognitive state Ψ as a column
vector in four-dimensional phase space:
ΨT µ,w,p
µ
,p
w
ψ
1
,ψ
2
,ψ
3
,ψ
4
,
“ p q ” p q
where T denotes a transpose operation. Then, the preceding Equations (23)–(26) can
be compactly expressed as
9
Ψ RΨ I, (27)
“ `
where R is a 4 4 matrix identified as
ˆ
γ s 1 m 0
µ µ
´ {
s γ 0 1 m
R ¨ ´ w { w ˛, (28)
“ 0 0 γ s
µ
˚ ´ ‹
˚ 0 0 s γ w ‹
˚ ´ ‹
and the inhomogen ˝ eous vector I is identified to ‚ be
IT γ µ ,γ w ,0,0 . (29)
µ d w d
“ p q
Equation (27) can be formally integrated to bring about the solution:
t
Ψ t e 0 tRpt1qdt1 Ψ 0 dt1e t t 1 RpτqdτI, (30)
p q “ p q`
ş ż 0 ş
where the first term on the RHS is a homogeneous solution, given the initial condition
Ψ 0 , and the second term is the inhomogeneous solution, driven by the source I.
p q
The formal solution represents a continuous path in 4-dimensional phase space, which
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 13
variationally optimizes the FE objective [Equation (14)]. Note that the trace of R
vanishes identically, i.e., Tr R 0; accordingly, the sum of its eigenvalues must equal
p q “
zero, which we use as a consistency condition in the numerical calculation presented in
Section 6. In addition, when the presynaptic signal is constant or saturates in time, the
fixed point Ψ can be obtained analytically:
eq
µ s w γ w s µ γ
ΨT d ` 8 d { µ , d ` 8 d { w ,0,0 , (31)
eq “ p 1 s2 γ γ 1 s2 γ γ q
´ 8{ µ w ´ 8{ µ w
where we used the notation s s t .
8
“ p Ñ 8q
6. Numerical Illustration
To exemplify the workings of the BM conducting synaptic inference, we numerically
integrated Equations (23)–(26). The results are presented below.
6.1. Free Parameters
Table 1. Parameter values we used to produce the data
m m γ γ µ w
µ w µ w d d
Solid 5 0.5 1 0.1 5 5
Dotted 5 0.5 1 0.1 10 0
Six free parameters appear in the BM and need to be fixed for numerical purposes,
of which values we choose as displayed in Table 1. The neural masses m and m are a
µ w
measure of inferential precision defined to be the inverse noise strengths [Equation (18)].
The frictional coefficients denoted by γ and γ appear in the generative functions
µ w
[Equations (19) and (20)], which we set γ 10γ to account for the slower weight
µ w
“
dynamics compared to the neuronal activity. Also, the parameters µ and w in
d d
the inhomogeneous vector [Equation (29)] represent the brain’s prior belief about the
postsynaptic and weight values before the presynaptic input arrives.
6.2. Static Presynaptic Input
We first present the numerical outcome when the synapse delineated in Figure 1 is
evoked by a static presynaptic signal, which we set as s 5.
“
Figure 3 shows the synaptic response of w and µ to the prescribed input from two
different parameter sets displayed in Table 1. Both cases exhibit transient harmonic
behaviors: The figure manifests that in response to the static input, the magnitude of
the output signals initially increases from the starting value 0; then, they approach the
corresponding fixed points in a sinusoidal manner, w ,µ 1.0,0.1 for the solid
eq eq
p q “ p´ q
curve and w ,µ 2.0,0.0 for thedotted curve. The transient harmonicbehavior
eq eq
p q “ p´ q
is attributed to imaginary eigenvalues of the matrix R for the chosen parameters. The
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 14
WeightDynamics PostsynapticActivity
w(t) µ(t)
0.0 t
2 4 6 8 10 1.5
-0.5
1.0
-1.0
-1.5 0.5
-2.0 0.0 t
2 4 6 8 10
-2.5
-0.5
-3.0
-1.0
-3.5
Figure 3. Synaptic dynamics evoked by the static presynaptic input s “ 5. The
parameter values that we used to produce the graphs are displayed in Table 1; the
initial condition was chosen as µp0q “ 0 and wp0q “ 0 [All curves are in arbitrary
units].
6
4
2
0
-2
-4
-6
-6 -4 -2 0 2 4 6
w
µ
Figure 4. Continuous path drivenby the static presynaptic input s“5: The initial
conditionwaschosenatpw,µq“p5,5q,markedby the blue dot; also,for illustrational
purposes, we set µ “ 0 and w “ 0 while other parameter values are the same as in
d d
Table 1.
plots for p and p are not shown because we exploit dynamics near the fixed points,
µ w
where their values are zero [see Equation (31)]. In general, the full synaptic dynamics
undergoes in 4-dimensional phase space.
In Figure 4, we illustrate a trajectory in the state space spanned by w,µ ; the
p q
numerical conditions are described in the caption. One can observe the spiral approach
to the fixed point 0,0 , starting from the initial condition 5,5 arbitrarily chosen for
p q p q
the illustrational purpose. Again, the momentum representations arenot drawn because
their values remain near the equilibrium point in the considered linear dynamics. Note
that the irregularity in the background streamlines is due to the noise in the presynaptic
input, reflecting the fact that the brain deterministically predicts cognitive outcomes
only on average. The trajectory we have illustrated is critical to understanding the
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 15
WeightDynamics PostsynapticActivity
w(t) µ(t)
8
6
5
4
2
0 t
2 4 6 8 10 0 t
2 4 6 8 10
-2
-5
-4
-6
Figure5. Synapticdynamicsevokedbysptq“5cost`η,whereηrepresentsarandom
fluctuation: The blue solid and red dotted curves are the results from the parameter
valuesdisplayedinTable1;inaddition,weincludetheblackdottedcurvefromµ “0
d
and w “ 0, while other parameter values remain the same. For all data, the initial
d
condition was chosen at pw,µq“p5,5q [All curves are in arbitrary units].
unconscious cognition of weight change and postsynaptic output. The temporal course
is conditioned on the presynaptic input in the Bayesian brain, a crucial context for our
research.
6.3. Nonstationary Presynaptic Inputs
Here, we present the numerical results of when the nonstationary presynaptic inputs
drive the BM.
First, in Figure 5, we illustrate the weight dynamics and the postsynaptic activity
resulting from the sinusoidally varying presynaptic input. In this case, the continual
harmonic driving causes the output signals to retain their oscillatory behavior and not
tend to a fixed point. The output signals exhibit both positive and negative portions
becauseweconsideredthevoltage-dependentplasticity, aimingatthecontinuouschange,
which could induce the negative voltage response [33]. In contrast, only positive
signals would be produced if we considered the spike-timing-dependent plasticity. The
momentum variables are not drawn because we follow dynamics near the fixed point
in neural phase space, where they remain nearly zero. Also, it needs to be understood
that the weight dynamics is ten times slower than the postsynaptic activity because
we assumed the postsynaptic signal decays ten times faster (see Table 1). The same
interpretation applies to Figure 3.
Next, in Figure 6, we illustrate the neural trajectory in two-dimensional state space
produced by the transient input signal that is shown as an inset. It discloses that the
brain’s synaptic computation follows a continuous approach to the origin, the fixed
point in this case, starting from the chosen initial state w,µ 5,5 . In numerically
p q “ p q
integrating the synaptic BM to obtain the trajectory, the parameter values were chosen
from Table 1 except that, for illustrational purposes, the values for µ and w were
d d
set to be both 0. Notice that we did not draw the streamlines in the figure because
the presynaptic input is time-dependent, so the streamlines vary at every moment
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 16
µ
6
4 Presynapticinput
6
4
2
0
2 -2
-4
-6
0 2 4 6 8 10
w
-6 -4 -2 2 4 6
-2
-4
Figure 6. Continuoustrajectoryinneuralstatespace. Theinsetshowsthetransient
input signal driving synaptic dynamics, sptq “ 5e´t{5cost ` η, where η denotes a
noise. The initial values of the weight w and postsynaptic signal µ were chosen at
pw,µq “ p5,5q, marked by a red dot; the neural trajectory manifests a continuous
approachto the fixed point p0,0q [All curves are in arbitrary units].
in the trajectory’s course. In Figure 4, by contrast, the input was static so that we
could delineate the streamlines. Notably, our BM theory allowed us to handle the
nonautonomousprobleminducedbythenonstationarypresynaptic inputs. Ontheother
hand, the computability of the usual minimization scheme for the present problem is
questionable because theFElandscapeisnon-static, notallowing a GDimplementation,
as described in Section 3.3.
The neural paths we numerically illustrated in the current Section are an optimal
trajectory minimizing synaptic surprisal; the actual minimization was performed on the
variational FE objective [see Equation (14)] under the FEP [see Equation (6)]. Here, we
emphasize that the learned trajectories were self-organized, given the values of material
parameters of the brain matter. In essence, the brain’s learning process is unsupervised
in the language of machine learning; this means that the brain does not require any
external label to guide its learning, demonstrating its self-learning efficiency.
7. Discussion
The idea that the brain is a neural observer (reinforcer or problem-solving matter) is
implicit in the brain-inspired FEP, capable of perceiving, learning, and acting on the
external and internal milieus; this renders the FEP a purposive theory. On the other
hand, brain functions must emerge from the brain matter obeying physical laws and
principles; the neural substrates afford the biological base for the brain’s high-order
capability. Thus, it is significant to recognize that the working FE objective is not a
single measure but an architecture hybridizing the teleological rationale and biophysical
realities.
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 17
In this paper, we continued our endeavor on thecontinuous-state implementation of
thepromising FEPasauniversal biological principle. Specifically, weapplied ourtheory
to synaptic learning and exemplified the learning dynamics as an inference problem
under the FEP. The noteworthy contributions from our effort are discussed below:
(i) Equation (14) is the FE objective in our theory, which suggests that the FE
conducts itself as a classical action in Hamilton’s principle, i.e., S F. We obtained the
“
resultbyderivingtheOnsager-MachluprepresentationsfortheNEQgenerativedensities
and inserting them into the Laplace-encoded FE. In our previous studies [13, 14, 15],
by contrast, the action was identified as a time-integral of the FE, S Fdt, under the
“
ergodic assumption. The ergodicity asserts that the ensemble average of surprisal over
ş
the sensory evidence equals the corresponding temporal average; however, it is difficult
to justify the ergodicity idea in the brain. In the present work, we bypassed the ergodic
assumption using the more physics-grounded NEQ densities and avoided employing the
generalized coordinates of motion; this makes the FEP computability we proposed more
physics-grounded than the other conventional approach.
(ii) The weight variables change in time due to biophysical factors such as an
opening of channels at the synapse, through which neurotransmitters transfer in a
complex time-dependent manner. Accordingly, we treated the synaptic weights w as a
dynamical variable co-evolving with the state variables in completing the synaptic BM.
In contrast, the weights are handled as a static parameter in the widely exercised ANNs
in machine learning. Also, in the frameworks of ANNs, a nonlinear activation scheme,
e.g., the sigmoidal function or ReLU (rectified linear unit), rectifies the network output
value [12]. Our biophysics-informed treatment does not use engineering manipulation to
regulatetheoutcome; instead, thelearningsmoothlyfollowsthecontinuous BM.Weadd
that one may employ different biophysical models from our Langevin dynamics, such
as Izhikevich neurons [34] at the neuronal level or neural field models on a mesoscopic
scale [35], and apply our framework to derive a desired BM.
(iii) The momentum representations we unveiled [see Equations (21) and (22)]
match with the theoretical construct of prediction error in predictive coding theory
[11, 36]. Recently, empirical evidence of error neurons was reported, which encodes
predictionerrorsinthemouseauditorycortex[37]. Suchafindingprovides aneuralbase
forourtheory. However, thedifferentiationbetweenthepredictiveanderrorunitswithin
a cortical column is still controversial, mainly because of insufficient electrophysiological
recordings. Although there is no concrete agreement, the compartmental neuron
hypothesis seems to suit the neuronal scenario of functional distinguishability [38, 39],
which argues that pyramidal neurons in the cortex are functionally organized such that
feedback and feed-forward signals are sent to outer layers (L1) and middle layers (L5),
respectively. In this case, our state representations correspond to feedback channels
via apical dendrites and momentum representations to feed-forward channels via basal
dendritesinsomas. TheHebbiansignofEquation(20)canbeeitherpositiveornegative;
one canimplement spiking predictive coding withthe former and thedendrite predictive
coding with the latter.
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 18
(iv) Data learning via ANNs has become a formidable scientific tool [40], and much
attention is drawn to theoretical questions on how and why they work [41]. This paper
suggested that the brain-inspired FEPunderlies thewidely used L 2 objective in machine
learning algorithms. The L 2 minimization is implemented using a GD with respect to
the weights connecting layer by layer, rendering back-propagation of the input-output
error reduction in a feed-forward architecture. However, strictly speaking, the validity
of GD updating is limited to situations when the inputs are static or quasi-static. For
continuous nonstationary inputs such as a video stream, a bidirectional, recurrent NN
(RNN) is employed [42]; RNN sends converted time-series inputs to a pre-structured
deep network and performs a GD by incorporating a feedback loop to predict the
sequential outputs. Our BM formulation, in contrast, handles nonstationary learning in
a genuinely continuous manner, offering a fresh perspective. The brain integrates the
BMtolearnacontinuous optimaltrajectory inneural phase space byminimizing theFE
objective rather than estimating a sequential output. We hope that our physics-guided
approach will provide further useful insight into the practice of ANN methodologies in
continuous time.
8. Conclusion
Within the continuous-state FEP framework, we cast synaptic learning into minimizing
the FE objective, the upper bound for synaptic surprisal in the brain, and derived the
BM implementing the FE minimization in a physics-guided manner. Consequently, we
revealed that the brain conducts synaptic learning by integrating the BM to find an
optimal trajectory in the reduced-dimensional neural phase space.
References
[1] Crick, F.; Koch, C. A framework for consciousness. Nat. Neurosci. 2003, 6, 119–126.
[2] Clark,A. Whatever next? Predictive brains, situatedagents, andthe future of cognitive science.
Behav. Brain Sci. 2013, 36, 181–204.
[3] Buzs´aki, G. The Brain from Inside Out; Oxford University Press: New York, U.S.A., 2019.
[4] Friston, K. The free-energy principle: a unified brain theory?. Nat. Rev. Neurosci. 2010, 11,
127–138.
[5] Friston, K.; Da Costa, L.; Sajid, N; Heins, C., Ueltzho¨ffer, K.; Pavliotis, G.A.; Parr,T. The free
energy principle made simpler but not too simple. Physics Reports 2023, 1024, 1–29,
[6] Maturana, H.; Varela, F. Autopoiesis and cognition: The realization of the living; Reidel:
Dordrecht, Holland, 1980.
[7] Buckley, C.L.; Kim, C.S.; McGregor, S.; Seth, A.K. The free energy principle for action and
perception: A mathematical review. J. Math. Psychol. 2017, 81, 55–79.
[8] Da Costa,L., Parr,T.; Sajid, N.; Veselic, S.; Neacsu, V.; Friston, K. Active inference on discrete
state-spaces: a synthesis. J. Math. Psychol. 2020, 99, 102447.
[9] Smith, R.; Friston, K.J.; Whyte, C.J. A step-by-step tutorial on active inference and its
application to empirical data, J. Math. Psychol. 2022, 107, 102632.
[10] Friston, K.; Mattout, J.; Trujillo-Barreto, N,; Ashburner, J,; Penny, W. Variational free energy
and the Laplace approximation. Neuroimage 2007, 34, 220–234.
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 19
[11] Rao, R.; Ballard, D. Predictive coding in the visual cortex: a functional interpretation of some
extra-classicalreceptive-field effects. Nat. Neurosci. 1999, 2, 79–87.
[12] LeCun, Y.; Bengio, Y.; Hinton, Deep learning, G. Nature 2015, 521, 436–444.
[13] Kim, C.S. Recognition dynamics in the brain under the free energy principle. Neural Comput.
2018, 30, 2616–2659.
[14] Kim,C.S.Bayesianmechanicsofperceptualinferenceandmotorcontrolinthebrain.BiolCybern.
2021, 115, 87–102.
[15] Kim, C.S. Free energy and inference in living systems. Interface Focus 2023, 13, 20220041.
[16] Hebb, D.O. The organization of Behavior: A Neuropsychological Theory, 1st ed.; Psychology
Press: East Sussex, UK, 2002.
[17] Landau, L.P.; Lifshitz. E.M. Classical Mechanics, 3rd ed.; Elsevier Ltd.: Amsterdam, The
Netherlands, 1976.
[18] Cover, T.M.; Thomas, J.A. Elements of information theory, Wiley-Interscience: New York:
U.S.A., 1991.
[19] Tierney,L.;Kadane,J.B.Accurateapproximationsforposteriormomentsandmarginaldensities.
J. Am. Stat, Assoc. 1986, 81, 82–86.
[20] Chaudhuri, R.; Ger¸cek, B.; Pandey, B. et al. The intrinsic attractor manifold and population
dynamics of a canonical cognitive circuit across waking and sleep. Nat. Neurosci. 2019, 22,
1512–1520.
[21] Friston, K.J.; Stephan, K. E. Free-energy and the brain. Synthese 2007, 159, 417–458.
[22] Friston, K.J.; Trujillo-Barreto, N.; Daunizeau, J. DEM: a variational treatment of dynamic
systems. Neuroimage 2008, 41, 849–885.
[23] Aguilera, M,; Millidge, B.; Tschantz, A.; Buckley, C.L. How particular is the physics of the free
energy principle? Phys. Life Rev. 2021 40, 24–50.
[24] Anil Meera, A.; Wisse, M. Dynamic expectation maximization algorithmfor estimation of linear
Systems with colored noise. Entropy 2021, 23, 1306.
[25] Kubo,R.; Toda,M.; Hashitsume,N. Statistical Physics II. Nonequilibrium Statistical Mechanics,
2nd Edition, Springer-Verlag: Berlin: Germany, 1991.
[26] Adib, A.B. Stochastic actions for diffusive dynamics: reweighting, sampling, and minimization.
J. Phys. Chem. B 2008, 112, 5910–5916.
[27] Hunt, K.L.C.; Ross, J. Path integral solutions of stochastic equations for nonlinear irreversible
processes: The uniqueness of the thermodynamic Lagrangian. J. Chem. Phys. 1981, 75, 976–
984.
[28] Buzs´aki, G.; Draguhn, A. Neuronal Oscillations in Cortical Networks. Science 2004, 304,1926–
1929.
[29] Beggs, J.M. The criticality hypothesis: how local cortical networks might optimize information
processing. Phil. Trans. R. Soc. A 2008, 366, 329–343.
[30] Miller,K.D.;MacKay,D.J.C.TheroleofconstraintsinHebbianlearning.Neural Comput.1994,
6, 100–126.
[31] Oja, E. Simplified neuron model as a principal component analyzer. J. Math. Biol. 1982, 15,
267–273.
[32] Hawkins, J.; Ahmad, S.; Cui, Y. A theory of how columns in the neocortex enable learning the
structure of the world. Front. Neural Circuits 2017, 11, 81.
[33] Garg.N.;Balafrej,I.;Stewart,T.C.;Portal,J.-M.;Bocquet,M.;Querlioz,D.;Drouin,D.;Rouat,
J.; Beilliard, Y.; Alibart, F. Voltage-dependent synaptic plasticity: Unsupervised probabilistic
Hebbian plasticity rule based on neurons membrane potential. Front. Neurosci. 2022, 16,
983950.
[34] Izhikevich,E.M.DynamicalSystemsinNeuroscience: TheGeometryofExcitabilityandBursting,
The MIT Press: Cambridge, U.S.A., 2006.
[35] Deco, G.; Jirsa, V.K.; Robinson, P.A.; Breakspear, M.; Friston, K. The dynamic brain: from
spiking neurons to neural masses and cortical fields. PLoS Comput. Biol. 2008, 4, e1000092.
Bayesian Mechanics of Synaptic Learning under the Free Energy Principle 20
[36] Shipp, S. Neural elements for predictive coding. Frontiers in Psychology 2016, 7, 1792.
[37] Audette, N.J.; Schneider, D.M. Stimulusspecific prediction error neurons in mouse auditory
cortex. Journal of Neuroscience 2023, 43, 7119–7129.
[38] Larkum, M.E. A cellular mechanism for cortical associations: An organizing principle for the
cerebral cortex. Trends Neurosci. 2013, 36, 141–151.
[39] Gillon, C.; Pina, J.; Lecoq, J.; Ahmed, R. et al. Responses to Pattern-Violating Visual Stimuli
Evolve Differently Over Days in Somata and Distal Apical Dendrites. J. Neurosci. 2024, 44,
e1009232023.
[40] Schmidhuber, J. Deep learning in neuralnetworks: an overview,Neural Netw. 2015,61, 85–117.
[41] Zdeborov´a, L. Understanding deep learning is also a job for physicists. Nat. Phys. 2020, 16,
602–604.
[42] Sherstinsky, A. Fundamentals of recurrent neural network (RNN) and long short-term memory
(LSTM) network. Phys. D: Nonlinear Phenom. 2020, 404, 132306.