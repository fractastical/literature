Phi fluctuates with surprisal:
Anempiricalpre-studyforthesynthesisof theFreeEnergyPrincipleand
Integrated Information Theory
Short title:Empirically relating the Free Energy Principle and Integrated Information Theory
Christoffer Lundbak Olesen
¬∂
*1
, Peter Thestrup Waade
¬∂
*1,2
,Larissa Albantakis
3 
and Christoph Mathys
1
¬∂
These authors contributed equally to this work
1
Interacting Minds Centre, Aarhus University, Aarhus,Denmark
2
School of Communication and Culture, Aarhus University, Aarhus, Denmark
3
Department of Psychiatry, University of Wisconsin,Madison, Wisconsin, United States of America
*Corresponding authors:
Christoffer Lundbak Olesen: clo@css.au.dk
Peter Thestrup Waade: ptw@cas.au.dk
1
Abstract
The FreeEnergyPrinciple(FEP) andIntegratedInformationTheory(IIT) aretwoambitious
theoretical approaches. The first aims to make a formal framework for describing
self-organizing and life-like systems in general, and the second attempts a mathematical
theoryof consciousexperiencebasedontheintrinsicpropertiesofasystem.Theyareeach
concerned with complementary aspects of the properties of systems, one with life and
behavior, the other with meaning and experience, so combining them has potential for
scientificvalue. Inthispaper, wetakeafirststeptowardssuchasynthesisbyexpandingon
the results of the evolutionary simulation study by Albantakis et al. (2014), which showa
relationshipbetweenIIT-measuresandfitnessindifferingcomplexitiesof tasks.Werelatea
basic information theoretic measure fromthe FEP, surprisal, to this result, finding that the
surprisal of simulated agents‚Äô observations is inversely related to the general increase in
fitness and integration over evolutionary time. Moreover, surprisal fluctuates together with
IIT-basedconsciousnessmeasuresinwithin-trial time.Thissuggeststhattheconsciousness
measures used in IIT indirectly dependontherelationbetweentheagent andtheexternal
world, andthatitshouldthereforebepossibletorelatethemtothetheoretical conceptsused
in the FEP. Lastly, we suggest a future approach for investigating this relationship
empirically.
Keywords:
Consciousness, Integrated Information Theory, The Free Energy Principle, Surprisal,
Œ¶
Author Summary
Twoinfluential theoretical frameworksincognitivescience,neuroscienceandcomputational
biology, are the Free Energy Principle and Integrated Information Theory. The first is a
formal approach to self-organization and adaptive behavior - in short, life - based on first
2
principles from statistical physics. The second is an attempt at formally describing the
intrinsic experience of a given system, that is, howit feels to be that system. In thisway,
these two theories provide tools for understanding two complementary aspects of a given
organism; namely, howit acts in a goal-directed manner based onstatistical beliefsabout
the world, and howit feels to be that systemin that process. Inthispaper, weprovidean
initial numerical investigation of the potential relation of these theoretical frameworks. We
simulate agents that undergo evolution, and show that as their level of integration (Œ¶, a
measure from Integrated Information Theory) increases, information theoretic surprisal (a
quantity used in the Free Energy Principle) decreases. We also see that Œ¶and surprisal
fluctuate together, and that these fluctuations depend onsensoryinput. Finallyweprovide
considerationsfor futuresimulationwork,andhowtobringthesetwotheoretical frameworks
closer together.
3
Introduction
In recent years, two influential theoretical fields have emerged which provide tools for
understanding the relation between self-organization, complexity, beliefs and experience
withinanygivensystem. Theseare IntegratedInformationTheory(IIT)(Tononi etal.,2016;
Oizumi et al., 2014) and the Free Energy Principle (FEP) (Friston, 2010; Friston, 2012;
Friston, 2019). They have much in common: both are substrate-agnostic theories that
generalize beyond human brains; they take hierarchical and multi-scale perspectives; and
they provide formalizations and quantitative measures with origins in information theory
(FEP: Kirchhoff, 2018; Hesp, 2019; IIT: Marshall et al., 2018; Albantakisetal.,2020).Atthe
same time, they have potentially complementary outlooks. IIT is grounded in axioms from
phenomenological consciousness, the FEP in first principles fromstatistical physics. IITis
primarily occupied with a system‚Äôs internal causal structure and corresponding conceptual
structure and intrinsic experience; The FEP has an extrinsic focus on a system‚Äôs
sensorimotor environmental exchanges and the generative model structure and statistical
beliefs they imply. In IIT, existence is an irreducible causal structure (Tononi et al., 2022),
while in the FEP it is the presence of a stable Markov Blanket and corresponding
(non-equlibrium) steadystate(Friston, 2019). Whiletheyareverydifferent,theperspectives
of IIT and the FEP are not necessarily mutually exclusive. Indeed, a synthesis of the two
theoriescouldprovidetoolsfor aformal explorationof therelationbetweenexperienceand
behavior in self-organizing systems in general. We therefore in this paper make an initial
empirical explorationoftherelationbetweentheoretical constructsfromthetwotheories.We
first outline the two theories fromtheir own perspectives, and give some initial theoretical
considerationsof howthetheoriescouldcomplement orenricheachother.Wethenpresent
a simulation study that shows a relation between quantitative measures related to each
theory, namely Œ¶ and information theoretic surprisal, on both evolutionary and behavioral
timescales. Finally, we discuss suggestions for further steps to empirically relate the Free
Energy Principle and Integrated Information Theory.
4
Integrated Information Theory (IIT)
Thefundamental questionaskedinIITiswhichpropertiesaphysical systemmustexhibitin
order to support consciousness. It is thus not assumed that consciousnessisexclusiveto
brains. Instead, IIT focuses on the relationship between consciousness and physical
systems in general (Koch, 2019; Tononi &Koch, 2015). At theheart of IITliesadistinction
between intrinsic and extrinsic descriptions of asystem, i.e., what asystemistoitself and
what it is to an outside observer. Theclaimisthat consciousexperienceexistsexclusively
for the beholder, so it cannot be modeled asanextrinsicpropertyandmust beaddressed
from the intrinsic perspective of a system. One of the novelties of IIT compared to other
theoriesof consciousnessisthat it takesitsoutset inthephenomenologyofconsciousness
itself byproposingfiveaxiomsthat arenecessarilytrueforanyconsciousexperience(inthe
senseof beinglogicallyindubitable). Theseaxiomsarethentranslatedintoacorresponding
set of postulates about the characteristics that physical systems must have in order to
comply with eachof theaxioms, whichinturnformsthebasisof theformalismof IIT(for a
full technical descriptionof theseaxiomsandpostulatesseeOizumi et al., 2014(‚ÄúIIT3.0‚Äù),
which we rely on in the following).
InessenceIITclaimsanidentitybetweenintrinsicexistenceandconsciousness.The
theorybuildsontheideathat toexist istoexert causal power, i.e. inorderforsomethingto
be a single coherent thing, it must, as that thing, have thepotential toaffect somethingin
someway(notethat toexert causal powerisnotnecessarilytocausesomething,butjustto
have the power to do so). According to this view, the difference between extrinsic and
intrinsic existence lies in whether the ‚Äúthing‚Äù exerts causal power upon somethingelse, in
which caseit existsfor theother, or exertscausal power uponitself, inwhichcaseit exists
for itself. Inaddition, for somethingtoexist asacoherent whole, it mustbeirreducibletoits
parts. This is key towhat IITattemptswithitsformal analysis. Theformalismof IITcanbe
viewedasanattempt at modelingsuchintrinsicexistencefromanobserver‚Äôspoint of view.
Sinceexistenceisdescribedincausal terms, thisformalismthenamountstoananalysisof
5
the causal structure of the system itself (not the way the system interacts with its
environment). As the analysis specifically needs to represent the intrinsic nature of the
system, it is not appropriatetomerelyuseconventional informationtheoreticmeasures, as
theseareinherentlyextrinsic(Tononi et al., 2016; Barbosaetal.,2020).Instead,IITdefines
an intrinsic information measure, loosely described as ‚Äòdifferences that make a difference‚Äô
(Oizumi et al., 2014). It isimportant tostressthat thisisverydifferent from,andnotdirectly
comparable with, other information measures. Simply put, the intrinsic information of IIT
measures howmuch something, in causal terms, constrains thestateof thesystemit isa
part of.
Assuch, therearetwolevelsof analysis: themechanismlevel andthesystemlevel.
The aim of the mechanism level analysis is to identify all the parts of the system that
intrinsically exist in a compositional manner. To exist within the system, the parts must
specifyirreducibleintrinsicinformationaboutotherpartsofthesystem(suchpartsarecalled
‚Äúconcepts‚Äù), whichismeasuredbythemechanismintegratedinformationœÜ. Thebasicidea
behind integrated information œÜ is to find the minimum difference in intrinsic information
between the constraints specified by a mechanism within the system and any possible
partition of it. If thereisawaytopartitionit whichresultsinnolossof intrinsicinformation,
the mechanism can be reduced to those parts. However, if there is no such partition, it
means that the mechanismspecifies information above andbeyonditsconstituentsandis
therefore irreducible to those constituents. Next, one must evaluate whether the causal
structurecomposedof all thesepartsisinitself irreducible. All conceptsof asystem, taken
together, forma joint causal structure for the system, calledaconceptual structure. At the
system level of analysis, this structure is evaluated according to the same principles of
integrated information just outlined. The integrated conceptual informationŒ¶quantifiesthe
extent of irreducibility in the conceptual structure.
Note that due to IIT‚Äôs fifth axiom and postulate of consciousness (exclusion), the
causal structuredoesn‚Äôt just needtobeirreducible,itneedstobethemostirreducibleoutof
all overlappingsetsand(inprinciple)acrossall spatial andtemporal grains.Thus,according
6
to IIT a conscious system is one which specifies a causal structure that is maximally
irreducible. Thewholeanalysisisthusrepeatedfor all possiblecandidatesystems(subsets
of elements), while treating the elements outside the candidate systemunder analysis as
background conditions(consideringabrain, thesensoryinput, therest of thebodyandthe
environment areconsideredbackgroundconditions). Asystemwithanon-zeroŒ¶thatdoes
not overlapwithanother set of elementswithagreater Œ¶value(inbothtimeandspace) is
called a ‚Äúcomplex‚Äù. In general, the Œ¶value of a complex is interpreted as the size or the
amount of consciousness, whereastheconceptual structureisinterpretedasthecontentor
richnessof that consciousstate, whiletheindividual conceptsarethedistinguishablethings
or features constituting this content (Tononi et al., 2016; Haun & Tononi, 2019).
Notethat thecomputational procedureofcalculatingŒ¶iscostly,asitisiteratingover
all possiblecombinationsof systemelementsat multiplelevels, resultinginanexplosionof
the time required to calculate Œ¶as systems get larger (SeveniusNilsenet al., 2019). This
meansthat it isusuallyfar fromfeasibletoactuallycalculateŒ¶onanynetworkmuchlarger
thanabout 10nodes, far belowtheroughly100billionneuronsof thehumanbrain.Thus,it
is as yet not possible to relate the findings of any given IIT analysis to the concrete
phenomenology that we are familiar with, or toconcretehumanbehavior (but seeHaunet
al., 2019; Ellia et al., 2021). IITevaluatescausal constraintswithinthesystem. Ingeneral,
the relationship between IIT‚Äôs intrinsicmeasuresandtheextrinsicallyobservedbehavior of
evensimplesystemsisyettobedetermined,whichisamotivationforrelatingIITtotheories
of behavior.
The Free Energy Principle (FEP)
The Free Energy Principle (FEP) is a theoretical framework, basedonfirst principlesfrom
statistical physics and information theory, which attempts to bridge as diverse fields as
evolutionarybiology, cognitivescienceandphilosophyofmind(Hohwy,2013;Friston,2019).
It wasoriginallyproposedasaunifyingframeworkfor understandingbrainfunction(Friston,
2009; Friston, Kilner, & Harrison, 2006), but has since been expanded to encompass
7
organisms and self-organizing systems in general (Friston, 2012; Ramstead, Badcock &
Friston, 2018; Friston, 2019). The FEP builds ontheclaimthat anyself-organizingsystem
continually returns to a non-equilibrium steady state, i.e., a small set of non-equilibrium
statesout of all thestatesitcouldpossiblyinhabit,inordertoresistentropicfluctuationsand
stay in existence inarecognizableform. Thiscanbeformalizedassolvinganoptimization
problem, minimizing the long-term average of the information theoretic surprisal of the
system‚Äôssensoryexchangeswiththeexternal world,basedonanimplicitgenerativemodel.
However, evaluating this surprisal is normally not computationally feasible, so a different,
computable quantity is used instead, thevariational freeenergy, whichtrackshowwell the
generative model is able to explain sensations, and which upper-bounds the surprisal
(Friston, 2010). Consequently, anylivingi.e. self-organizingorganismcanbedescribedasif
it tracked and minimized the variational free energy of its sensory states relative to a
generative model, thus providing a general principle for understanding the behavior of
self-organizing systems and life in general (Ramstead, Badcock & Friston, 2018).
A crucial component of the FEP is the presence of a temporally stable Markov
blanket. AMarkovblanketisastatistical separationbetweeninternal andexternal statesofa
system, separatedbyanother set of statesthatformtheboundarybetweenthetwo.Internal
statesarenot affectedby, anddonot affect, external states,exceptwhenmediatedthrough
the blanket states. Blanket states are denoted as sensory states whentheyaffect internal
statesandareaffectedbyexternal states,andaredenotedasactivestateswhenthecausal
direction is opposite. It can be shown that maintaining a Markov blanket is necessary for
keeping structural integrity, that it emerges fromsimple randomdynamical processes, and
that maintaining a Markov blanket entails maintaining a non-equilibriumsteady state and
exhibiting behavior whichimplicitlyminimizesvariational freeenergy(Friston, 2019). Inthis
process, thedynamicsofthesystem‚Äôsinternal statescometostatisticallymodel thoseofthe
external states (Friston, 2013). It is argued that biological systems can be construedasa
nestedhierarchyofMarkovblanketedsystems,wheresmallersystemsformthecomponents
of larger systems. Thus a collective of Markov-blanketed macromolecules mayformacell
8
wall creating a stable markov blanket on thelevel above. Thiscancontinueupwards, with
cells forming blanketed organs, which in turn form human bodies, social groups, cities,
ecologies etc., with every level of the nested hierarchy seemingly acting to minimize the
variational free energy of its blanket‚Äôs sensory states by existing (Kirchhoff et al., 2018).
Thevariational freeenergyof asystem‚Äôssensorystatesisalwaysevaluatedrelative
to a generative model of the external (hidden) statesgeneratingpredictionsabout sensory
states. One way of minimizing the variational free energy is to (variationally) make state
estimates, updateparametersandinfer model structures,whichcorrespondsto(perceptual)
inference, learningandstructurelearning, respectively. Anotherwayofreducingfreeenergy
is to act on the environment in order to produce sensory inputs that are expected by the
organism, andthereforepreferred. Inactiveinference,aframeworkbasedintheFEPwhere
agentschooseactionsthat minimizeexpectedfreeenergy,preferencesareimplementedas
prior expectationsfor sensoryinputs- herecalledgoal priors-thatareimmutable,andmust
thereforeberealizedinordertominimizefreeenergy(Fristonetal.,2013).Thesegoal priors
are thought to reflect the kindsof sensationstheorganismusuallyencounters, astatistical
‚Äòphenotype‚Äô, andarethought tobeevolutionarilyselected. Thisprovidesaninterpretationof
evolutionasBayesianmodel selection, wherevariational freeenergyisminimizedovertime
byproducingorganismsthatimplymoreadaptivegenerativemodels(Hespetal.,2019).The
FEP and active inference can thus be used to understandprocessesat bothevolutionary,
developmental and mechanistic scales (Badcock, Friston & Ramstead, 2019; Badcock,
Friston, Ramstead, Ploeger & Hohwy, 2019), as well as howorganisms and their niches
mutually adapt to, and model, each other (Constant, Ramstead, Veissiere, Campbell &
Friston, 2018). Finally, it shouldbenotedthat thereisanongoingargument onwhetherthe
FEP and active inference should beseeninstrumentallyasjust amathematical framework
for describingthedynamicsofsystemsatnon-equilibriumsteadystates,orwhetheritshould
rather be seen as acontingent descriptionof nature(Ramstead, Kirchhoff &Friston, 2019;
Andrews, 2020; van Es & Hip√≥lito, 2020).
9
A few different approaches have been taken to apply the FEP to questions of
consciousness. Firstly, avarietyof activeinferenceMarkovDecisionProcess(MDP)models
havebeendeveloped, whereagentsmodel theworldasdiscretestatetransitionsandselect
actionsthat minimizevariational freeenergy(Smith, Friston&Whyte,2021;Parr,Pezzulo&
Friston, 2022). These canbetakenasabstract modelsthat describethebeliefsabout the
worldimplicit inasystem‚Äôsbehavior; it isthought, however, thatthesystem‚Äôsinternal states
parameterize these models (Friston, Wiese & Hobson, 2020), and they can be related to
variational messagepassingschemesfor predictivecoding(Smith, Friston&Whyte,2021).
Because inferences about the world are thought tobethebasisfor consciousexperience,
MDP modelsarethought toreflect thesubjectiveexperienceof thesystemthat implythem
(asfor exampleinFristonet al. (2013)).Ingeneral,itisarguedthatconsciousexperienceis
aproduct ofinferential,ratherthansensory,processes,andthatitemergeswhenasystem‚Äôs
implied generative model is temporally and counterfactually deep, that is, includes
inferences about what will happeninthefutureandwhat couldhavehappenedinthepast
given different actions (Friston, 2017; Friston, 2018). Additionally, it has been argued that
consciousness is tentatively linkedtothesystem‚Äôsmodel beingresponsivetointeroceptive
information(Nikolovaetal.,2021),andtothefactthatcomplexagentscometoattributehigh
certainty to mid-level predictions while allowing higher-level beliefs tovary, thuscreatinga
chasmbetweentheworlditself andimmediateexperiencesof it(Clark,Friston&Wilkinson,
2019). It has also been argued that consciousness is related to affect, and that affect is
relatedtomovingtowardsavariational freeenergyminimum,suggestingthatconsciousness
ariseswhenexplicitevaluationoftheexpectedvariational freeenergyassociatedwithaction
policiesisrequired, asopposedtowhenbehaviorisautomaticorreflexive(Solms&Friston,
2018; Solms, 2021). Anactiveinferenceversionof theglobal neuronal workspacetheoryof
consciousness has been formulated (Whyte & Smith, 2021), and an active inference
neurocomputational Projective Consciousness Model, also based axiomatically on
phenomenology, hasbeenproposedtoexplaintheperspectival andintegratedqualityoffirst
person experience (Rudrauf et al., 2017; Wiliford et al., 2018). There has also been an
10
attempt at employing generative modeling techniques on phenomenological experience,
linking‚Äòraw‚Äô immediateexperiencetoanactiveinferenceagent‚Äôsobservations,andthetotal
experiencetothebest explanationor mostlikelymodel giventhoseexperiences(Ramstead
et al., 2021). A more detailed discussion of howthe mathematical constructs of the FEP
potentially relates to consciousness can be found inFriston, Wiese&Hobson(2020), with
the essential point being that the mind-matter duality can be grounded in the difference
between the intrinsic information geometry of the probabilistic evolution of internal states,
and the extrinsic information geometry of probabilistic beliefs about external states that is
parameterized by the internal states (note that ‚Äòintrinsic‚Äô and ‚Äòextrinsic‚Äô are here used ina
different sensethanwhenusedinIIT).Theseaccounts,evenastheydiffersubstantially,are
all basedinpredictiveprocessingaccountsof brainfunction, whichisarguedtobeafruitful
framework within which to identify neural correlates of consciousness (Hohwy & Seth, 2020).
Wedonot inthispaper goindepthwiththetheoretical relationshipbetweenIITand
theFEP,butinsteadfocusonanumerical investigationofsomecoreconstructsfromthetwo
theories. We will first, however, take a moment to outline some considerations as to the
potential for mutual enrichment betweenIntegratedInformationTheoryandtheFreeEnergy
Principle.
Motivation for relating the FEP and IIT
TherearemultiplewaysinwhichIITandtheFEPappeartobecompatible,andwherethere
seemstobepotential for mutual contribution.Inabriefdiscussion,Friston,Wiese&Hobson
(2020) arguethat thereisaconstructvaliditybetweenthetwoapproaches,claimingthatthe
five axioms of IIT all are compatible with Markov Blanketed variational free energy
minimizationsystemsasconceptualizedundertheFEP.ItisspeculatedthatrelatingtheFEP
to IIT might help distinguish between consciousandunconscioustypesof activeinference
processes, and possibly be a step towards a unitaryconcept of consciousness. Theyalso
point out that both theories rest upon partitions of causally related systems, which, in
addition to further indicating that the theories might be mutually intelligible, also highlights
11
possibleavenuesfor research. SeeAlbantakis(2020) for abrief discussionandcriticismof
these claims.
The two different conceptualizations of when something exists as a ‚Äòthing‚Äô, in FEP
meaning maintaining a Markov Blanket over time, and in IIT meaning being a maximally
integrated cause-effect structure over a background of environmental influences, might be
complementary concepts (see also Marshall et al., 2017). It could be conjectured that
complexes do not usually cross stable Markov Blankets, so that consciousness is usually
contained within, as opposed to extending across, the dynamically maintained borders of
organisms. MethodsfromIITmight alsocomplement theFEPbydeterminingatwhichlevel
of ahierarchyof nestedMarkovBlanketsconsciousnessislocated, namelythelevel where
intrinsic integration is the highest.
Another placewherethetwotheoriesmight contributetoeachotherisintherelation
betweenconsciousexperienceandtheexternal world. IITisconcernedwiththeexperience
of a systemregardless of itssurroundings(but givenitsbackgroundconditions), but under
the FEP, self-organizing systems must imply models of the environment (parametrized by
their internal states) inorder topersist. ThisindicatesthatIIT-basedformal descriptionsofa
system‚Äôsintrinsicexperiencemight,tosomedegree,reflectitsexternal environment.Itmight
evenbepossibletoshowthat asystem‚Äôsimpliedgenerativemodel andbeliefsaretosome
degree reflected in its conceptual structure, a kind of integrated representationalist
interpretation of the FEP. Additionally, in FEP organisms are often thought to align their
implied generative models (and consequently the internal states that parametrize them)
through (social) interaction with their conspecifics, which could involve alignment of their
internal causal structures, conceptual structures and intrinsic experience.
Finally, IIT is agnostic as to whether or not a conscious system self-organizes
(although Marshall et al. (2017) relate integration to self-maintenance). The FEP states,
however, that self-organizingsystemsmaintaintheir structureandreturntoastablesubset
of statesbyminimizingvariational freeenergy.Onemightsurmisethatminimizingvariational
12
free energy might then be related to maintaining a stable, self-similar and spatially bound
consciousness across time, that is, perhaps, a sense of self.
It is also suggested by Friston, Wiese & Hobson (2020) that one might find a
relationship between Œ¶and variational free energy for a given system, so that minimizing
variational freeenergysimultaneouslyleadstoanincreaseinŒ¶.GiventhatAlbantakisetal.
(2014) suggest that being more integrated might have evolutionary advantages, which
should lead to a better ability to minimize variational freeenergy, sucharelationshipdoes
seemlikely. Therelationshipmight, however,becomemorecomplex,giventhatbothimplicit
statistical beliefs and surprisal, and alsotheamount of integrationresultingfromevolution,
varywiththecomplexityof agiventaskaswell astheconstraintsonthesystem,ratherthan
only how well the system performs a task (Albantakis et al., 2014). In this paper, we
therefore take first stepstowardsanempirical investigationof therelationbetweenthetwo
constructs. Importantly, we do not yet compare Œ¶ to the variational free energy of the
system, but insteadcompareittoanempirical approximationofthesurprisal associatedwith
the system‚Äôs sensory states, since this is the quantity that variational free energy
minimization ultimately seeks to minimize. We then leave it for further work to properly
incorporate further aspects of the FEP, for example by calculating Œ¶ for a system that
performsactiveinferenceunderaknowngenerativemodel oftheworld(SeeLimitationsand
Further Work).
It is noteworthy that Safron (2019a,b) has attempted to synthesize IIT, FEP and
Global Neuronal Workspace theory into a new Integrated World Modelling Theory. He
argues that active inference and the Free Energy Principle can be used to bridge the
otherwise contesting extrinsic and intrinsic perspectives of thetwoother theories, andthat
this synthesis has applications across a number of areas. It is beyond the scope of this
paper, however, to contribute to theoretical work on synthesizing thetheories. Instead, we
focus on investigating the numerical relation between Œ¶ and empirically approximated
surprisal, and discuss the types of research this might lead to in the future.
13
Evolving Animats
To investigate the relationship between the measures of IIT and FEP we replicated the
evolutionary simulation presented in Albantakis et al. (2014), where it was found that
animats(artificial adaptiveagents) onaverageevolvedmoreconceptsandhigher valuesof
Œ¶ when the task environment was more complex and difficult, compared to simpler and
easier tasks. The animats in the simulation were evolved to performa simple perceptual
categorization task, shown in Fig 1. Within the world, animats inhabited 3 squares of the
bottomrowof a tetris-likespace. Oneachtrial, ablockwithacertainwidthwouldfall from
the top to the bottom, eventually either hittingor missingtheanimat at theendof thetrial.
Each trial the block would move consistentlytotheleft or right whilefalling(at onesquare
per timestep). Thetaskoftheanimatwastoeithercatchoravoidtheblockattheendofthe
trial dependingonthewidthof theblock. This,togetherwiththedifferentfallingdirectionsof
theblock, gavefourdifferenttasktypes(rightcatch,rightavoid,leftcatchandleftavoid)and
atotal of 128trials(giventhemaximumof 4differentblocksizes).Internallytheanimatwas
aMarkovbrain(Hintzeetal.,2017),anetworkconsistingof2sensorynodes,2motornodes
and4hiddennodes.Eachsensorynoderespondedonseeingtheblockdirectlyaboveitand
were placed on the outermost squares of theanimat (thus, nosensoryinput wasreceived
fromabove the middle of the animat). The hidden and motor nodes each had an internal
logic determining how different combinations of inputs resulted in either activation or
deactivation of the node. The motor notes controlled howtheanimat movedaroundinthe
space, such that when one motor node was active, it would move in a direction
correspondingtothatnode(leftorright),andwhilenoneorbothwasactivetheanimatwould
not move. More details on the animatsandthesimulationenvironment areprovidedinthe
methods section, and we recommend visiting
http://integratedinformationtheory.org/animats.html for a visual presentation of the taskand
the animat.
14
Fig1:Schematicoftheblock-catchingtaskinthesimulation.Ablockisfallingfromtoptobottom,eithertowards
theleftortheright.Theanimathastwosensorynodes,whichareactivatedwhentheblockisabovethem.Italso
hastwomotornodes,whichallowit toeithermovetotherightorleft.Dependingonthesizeoftheblock,the
animat‚Äôs task is to either catch or avoid the block. Reproduced from Albantakis et al. (2014).
In Albantakis et al. (2014), there were 4 tasks, which differed by the sizes of the falling
blocks, eachwithincreasingdifficulty. Hereweonlysimulatedtheeasiest task(task1) and
thehardest task(task4). Weevolved150linesof descent (LOD), 50forthe easytaskand
100 for the hard task. Each LOD was evolved over 60000 generations, starting froman
unconnected network. For each generation a set of animats would mutate by slightly
changing theconnectionsandlogicsof thehiddennodes, andthebest performinganimats
would form the basis of the next generation. Each LOD represents an independent
evolutionary simulationandcanfor thisreasonbeseenasdifferent species, independently
evolved to do the same task. Again, more details are provided in the method section.
Finally, we consider how to extract FEP-related measures for the animat at each
timepoint, in order to compare them with Œ¶. The immediate challenge here is that the
variational free energy (and also the surprisal which it upper bounds) is based on the
animat‚Äôs implicit generativemodel of theworld, whichwedonot apriori haveaccessto. In
the literature, simulated agents are often constructed with a generative model of their
15

surroundings, based on which they calculate expected free energy given different action
policiesandselect thepolicywiththelowest expectation(likeinFristonet al, 2013). Inthis
case it is possible to track the beliefs and the variational free energy of the agent as it
interactswithitsenvironment. Theanimatsareconstructedasdeterministiclogicnetworks,
however, and do not by construction possessagenerativemodel. If onecouldreconstruct
the generative model that the animats must have had in order to generate their observed
behavior - that is, their implied generative model - one could directly access their implicit
beliefs andfreeenergyoneachtimestep. It isnosimpletasktoreliablyreconstruct sucha
generative model, however, so we do not do it here(althoughseeLimitationsandFurther
Work for a suggestion on howto do it). Instead, we use anapproximationof theanimat‚Äôs
model-conditional surprisal, the quantity which is minimized by minimizing variational free
energy. This approximation does not require access to the animat‚Äôs implied generative
model, and therefore makes possible an initial numerical relation of the IIT and FEP.
Surprisal iscalculatedasthenegativelogprobabilityofanobservation(i.e.sensorystate)
occurring, given the animat‚Äôs generative model:
‚Ñë(ùëú|ùëö) =  ‚àí ùëôùëõ(ùëÉ(ùëú|ùëö))
Where is the surprisal, and P(o|m) is the probability P of outcome o occuring on the‚Ñë
timestep, giventhegenerativemodel m. CalculatingP(o|m) requiresaccesstotheanimat‚Äôs
generativemodel, andisthereforenotaccessiblehere,soweinsteadapproximateitwithan
empirical goal prior. We construct these empirical goal priors by using the distributions of
observed sensory states of (perfectly) adaptive animats. The sensory states that perfect
animatssampleshouldbethekindsofobservationsthatanimatsexpect,assumingthatthey
a priori expect to perform their task well, and have a useful model of their environment.
Animats are therefore ‚Äúsurprised‚Äù whensamplingsensorystatesthat areuncommonunder
adaptive behavior i.e. different from what the perfect animats encounter. Importantly, we
construct different probability distributions for each of the four tasktypes(right catch, right
avoid, left catchandleft avoid) andat eachtimestepinatrial,sinceadaptiveanimatsought
tohavedifferentexpectationsatdifferenttasktypesandtimesteps.Weadditionallycalculate
16
surprisal usingthesensorystatedistributionfromtheperfectanimatthatresultsinthelowest
surprisal.Thismeansthatsurprisal isevaluatedrelativetothekindofperfectbehaviorthatis
the most similar to its own, avoiding that rare but still perfect strategies result in high
surprisal. Ingeneral, thesurprisal measureweuseheremight beinterpretedasadistance
to optimal behavior, as well as the general entropy of the optimal sensory distribution.
Interestingly,usinganempirical goal priortoapproximateanimats‚Äô surprisal allowsus
to bypass the step of variational free energy entirely. This also means, however, that our
measure, evendisregardingthewaysinwhichitmightbeapoorapproximationofsurprisal,
does not directly reflect theepistemiccomponent of thevariational freeenergy(althoughit
might do so implicitly by making animats expect epistemic behavior to the degree that
perfect animats exhibit it). Our comparison here should mainly be considered as aninitial
comparison, until measures based directly on a (reconstructed) generative model can be
used.
In the following, we present results froman evolutionary simulation where animats
perform the task as described. We replicate some of the findings from Albantakis et al.
(2014), andadditionallyshowthat surprisal decreasesoverevolutionarytime.Moreover,we
assess how Œ¶ and surprisal change on within-trial time and how these fluctuations
cross-correlate, andrelate, inanexploratorymanner,thesecross-correlationstothegeneral
patterns of fluctuations in Œ¶ and surprisal.
Results
We first evaluate howaverage Œ¶, surprisal andfitnesschangeover evolutionarytime, and
then evaluate their fluctuationsintrial timeat thelast generationintheevolution. Here, we
consider the average development of Œ¶ and surprisal across trials, as well as their
correlations. Finally, we split the animats into 3 groups based on correlation profiles
(negative, positive or no correlation between Œ¶ and surprisal) and investigate howthese
groups differ in both evolutionary and trial time.
17
Our data set consists of two task environments (easy and hard) with 50/100
independent evolutions, respectively (Table 1). The distributions of Œ¶and surprisal at the
final generationareshowninFig2. It canbeseenthatthemeanandmedianofŒ¶ishigher
in the hardtask, andevenhigher for theperfect animats. Incontrast, surprisal ishigher on
thehardtask, but lower for perfect agents. Wealsoseethat themeanandmediancapture
differences between the distributions differently. For comparability with Albantakis et. al
(2014), wesubsequentlypresent meanvaluesinthemaintext(seeFigA-CinS1Figurefor
the same analyses using the median value).
Easy task Hard task
Total number of LODs 50 100
Number of LODs with
perfect fitness on final
generation
34 8
Number of LODs with mean
Œ¶=0 on final generation
10 6
Number of LODs with at
least 1 trial of mean Œ¶=0 on
final generation
20 31
Percentage of trials with
mean  Œ¶=0 on final
generation (out of 128 total
trials)
22% 8.3%
Quartiles of Œ¶ values on
final generation
min: 0
Q1: 0
min: 0
Q1: 0
18
Q2: 0.09
Q3: 0.69
max: 2.49
Q2: 0.18
Q3: 0.56
max: 4.11
Quartiles of surprisal values
on final generation
min: 0.09
Q1: 0.09
Q2: 0.22
Q3: 0.54
max: 3.58
min: 0.09
Q1: 0.36
Q2: 0.69
Q3: 1.97
max: 3.58
Table 1: Descriptivestatisticsof thedata.Notethatthenumberof perfectanimatsin bothtasksare
substantiallylargerthaninAlbantakisetal.(2014).Thisis likelyduetooptimizationoftheMABEframework,
used for the evolutionary simulation, between then and now.
Fig 2: DistributionsofŒ¶andsurprisalatthelastgenerationofevolutiondepictedwithhalfviolinplotandhalf
boxplot.Distributionsareshownforeachtaskandforthehardtaskanimatsthatevolvedperfectfitness.Thegray
dots with horizontal lines represent the calculated average of the distributions.
19

Changes in evolutionary time
Fig 3showschangesinŒ¶andsurprisal over evolutionarytime, averagedacrossLOD‚Äôsfor
eachgeneration. Fig3aand3bshowsourreplicationofthemainfindingsinAlbantakisetal.
(2014). Fitnessincreasesover evolutionarytimeandishigherintheeasytaskthanthehard
task. The red line shows the 8 perfect animats in the hard task, which reachnear-perfect
behavior about halfwaythroughthesimulation. WealsoseethatŒ¶ishigherinthehardtask
than in the easy task, and slightly higher for perfect animatsinthehardtaskat theendof
evolution. ComparedtotheresultsofAlbantakisetal.(2014),weseehigheraveragevalues
of fitnessandŒ¶inall conditions.Additionally,thesevaluesincreasefasteroverevolutionary
time and are more similar across tasks. This is likely due to a recent increase in the
efficiency of MABE‚Äôs evolutionary algorithm (Schossau & Hintze, 2020).
Fig3:Changeinaveragefitness(a),Œ¶(b)andsurprisal(c)overevolutionarytimefortheeasytask(black),the
hardtask(blue)andperfectanimatsinthehardtask(red).Thevariablesarefirstaveragedacrossallvalues
withineachanimat,thenaveragedacrossallLODsateachgeneration.Theribbonsaroundthelinesshowthe
standard error from averaging across LODs.
Our goal inthisstudyistocomparechangesinIITquantitiesover thecourseofthe
animats evolution to FEP related quantities. As shown in Fig 3c, surprisal decreasesover
evolutionary time and is lower in the easy task than in the hard task. In the subset of 8
20

perfect animats in the hard task, surprisal decreases more than on average. However, it
never gets as low as in the easy task.
Fluctuations in trial time
Fig4showsŒ¶andsurprisal (averagedover trialsandLODs)plottedovertrial time(36time
steps) at the end of evolution (generation 60000). Fig 4a shows that surprisal decreases
over trial time. The easy task has lower surprisal overall, and decreases further around
halfway through the trial. Surprisal is higher in the hard task, but surprisal of the perfect
animats decreases rapidly after the middle of the trial, and ends on a similar level as for
animats in the easy task. This suggests a shift towards lesser entropy in the goal priors
towards the end of thetrial (seethemethodssectionfor moreonthispoint). Fig4bshows
surprisal whentrial timehasbeencenteredaroundtheanimat‚Äôsfirstobservationoftheblock
(marked withadashedline). Hereit canbeseenthat surprisal increasessharplywhenthe
blockisobserved, but decreasesafterwards. Sincegoal priorsaremuchlessentropicatthe
end of the trial, animats that deviate fromthe goal prior have higher surprisal than in the
earlytimesteps, leadingtoanotherincreaseforanimatsinthehardtask.Thesurprisal ofthe
perfect hard task animats decreases after seeing the block, again ending at a level
resembling that of animats of the easy task.
21
Fig4: AverageŒ¶andsurprisalovertrialtimefortheeasytask(black),thehardtask(blue)andperfectanimats
inthehardtask(red).a)averagesurprisalovertrialtime.b)averagesurprisalovertrialtime,centeredaroundthe
firstobservationoftheblock.c)averageŒ¶overtrialtime.d)averageŒ¶overtrialtime,centeredaroundthefirst
observationoftheblock.Shadingaroundlinesisthestandarderrorofthemean.SeeFigAinS3Figureforthe
variabilityacrossLODs.Notethattheaveragesonbanddarebasedonadecreasingnumberoftrialsasthe
relativetimestepgetsfurtherawayfrom0.Inorderfortherelativetimesteptogoto30,atagiventrial,theanimat
needs to see the block within the first few real timesteps.
InFig4c, weseetheaverageŒ¶over trial time.Herethelinesarealmostflatforbothtasks,
withthehardtaskbeingveryslightlyhigher at thebeginningof thetrial, whichcorresponds
to the difference in Œ¶ levels we see at the end of evolutionary time (Fig 3b). For perfect
animats there is a general increase in Œ¶ towards the middle of the trial, after which it
decreases again. This suggests that therelativelyhigher averageŒ¶of theperfect animats
can be contributed to somethingthat happensduringthemiddleof thetrial, asopposedto
beingageneral increaseinthelevel of Œ¶for thistypeof animat. Fig4dshowstheaverage
Œ¶acrosstrials, withtrial timecenteredaroundtheanimat‚Äôsfirst observationoftheblock.At
thetimeof sensingtheblock, Œ¶increasesslightlyforanimatsintheeasytask,andstrongly
for perfect animats in the hard task, while there seems to be no averagedifferencewhen
including all animats in the hard task. This suggests that theincreaseinŒ¶weseefor the
22

perfect animats in the Fig 4c is related to the animat‚Äôs observation of theblock. Notethat
even though there are differences in average Œ¶ and surprisal, with very small standard
errors on the mean due to thelargenumber of simulatedtrials, thereisstill largevariation
between individual animats (see Fig A-B in S3 Figure).
Correlations between Œ¶ and surprisal
Before presenting the correlation analysis, we showexamples of per-timestep fluctuations
betweenŒ¶andsurprisal for afewindividual trials(Fig5).Hereweseeexamplesofdifferent
strategies for solving the task. The most common for adaptive animats is the
‚Äúfollow-strategy‚Äù, whereanimatsfolloweither behindor underneaththeblockdependingon
whether they must catch or avoid it (Fig 5b,d,f). The second is the ‚Äúpass-over-strategy‚Äù,
where animats let the block pass over them, which can be equallyadaptiveincatchtrials
(Fig 5a,c,e; see Methods for visualizations of the behavior of perfect animats). These
examplesalsoexemplifythewaythat bothŒ¶andsurprisal oftenchangeat thetimeswhen
the animat observes the block. Finally, we can see examples of positive correlations (Fig
5c,d,f), negative correlations (Fig 5b) and lack of correlations (Fig 5a,e) between Œ¶and
surprisal for a particular single trial.
23
Fig 5: Within-trialrepresentationof animatbehavior, Œ¶ andsurprisal.Thegrayboxof eachplotshows
informationaboutthetrialandtheanimat,andthecorrelationbetweenŒ¶ andsurprisal.Thex-axisshows
timestepswithinthetrial.Theorangelinedenotessurprisal,andthegreenlinedenotesŒ¶.Eachisplottedonan
arbitraryy-axisforbettercomparisonwhereonlythezeropointisindicated.Thelowerhalfofeachplotshowsthe
rightandleftsensorystates(SR,SL)andmotorstates(MR,ML),withblacklinesindicatingactivation.Sensory
statesareactivewhentheblockisperceived.Motorstatesareactivatedbytheanimat,andmakesitmoveinthe
givendirection;if bothmotorstatesareactivetheanimatstandsstill.Plotsarechosentoshowa varietyof
patterns of behavior, Œ¶ and surprisal. All examples are taken from animats in the hard task.
24

Fig 6 shows the distribution of cross-correlations of Œ¶and surprisal acrossall trialsof the
final generation for all LOD‚Äôs. For timelagsclosetozero, therearemanycorrelationsthat
arefarfrom0inboththepositiveandthenegativedirection,indicatingthatthereisarelation
betweenthetwomeasures, but that it variesbetweentrialsandLODs. Ingeneral thereisa
positive skewfor perfect animats in the hard task, and anevenstronger positiveskewfor
animats in the easy task. At larger time lags, correlation distributions are increasingly
centeredaround0, indicatingthat whenŒ¶andsurprisal correlate, theydosobyfluctuating
at the same times, as opposed to in a lagged manner.
Fig6:ThedistributionofcorrelationstrengthsbetweenŒ¶andsurprisal,acrossalltrialsonthelastgenerationof
all LODs.Shownfortimelagsaround0,denotedaboveeachofthegraphs.Correlationsareshownforall
animatsintheeasytask(black),forallanimatsinthehardtask(blue)andforonlyperfectanimatsinthehard
task(red).ThelaggingvariablewasŒ¶,meaningthatfornegativelagsthecorrelationsmeasuretherelationship
betweenŒ¶andfuturesurprisal,andviceversaforpositivelags.NotethattrialswhereŒ¶=0areexcludedfrom
this analysis.
25

Correlation profiles
Inorder toexplorethemechanismbehindthedifferentdirectionsofcorrelation(positiveand
negative) between Œ¶ and surprisal, we here again present the results of the animats
performing the hard task, but now split into three groups based on correlation profiles
(negative, neutral and positive correlations between Œ¶ and surprisal).
Fig7showstheresultsat evolutionarytime. Here, thecorrelationprofilesarebased
on the average correlation coefficients of animats at the end of evolution. The average
fitnessissimilar inthethreegroups(Fig7a), however, bothŒ¶andsurprisal areonaverage
lower for animatswithapositiveprofile(Fig7b,c). Wealsoseethatanimatswithanegative
profile seemto have slightly higher surprisal on average than those with a neutral profile,
which again are slightly higher than the positive correlation profile animats.
Fig 7: Changeinfitness(a),Œ¶(b)andsurprisal(c)overevolutionarytimeaveragedacrossLODssortedinto
groupsdependingonanimatcorrelationprofilesatlastgeneration.Thegroupsare:Negative(purple),neutral
(grey) and positive (brown) correlations.
InFig8weseethedifferencesbetweencorrelationprofilesattrial time.Here,thecorrelation
profilesarebaseddirectlyonthecorrelationcoefficientateachtrial separately.Sinceagiven
animat canhavepositivecorrelationsonsometrials,andnegativeonothers,theseplotsare
not comparabletotheendpointofevolutioninFig7before.Consideringsurprisal (Fig8a,b),
there is almost no difference betweenthegroups, besidespositivecorrelationtrialshaving
26

lower average surprisal towards the end. However, inŒ¶thereisamuchclearer difference
betweenthegroups(Fig8c,d).ThemoststrikingdifferenceisseeninFig8dwheretrial time
is centered around the first observation of the block. We see that on trials with positive
correlations, Œ¶ is generally on a low baseline. Average Œ¶ then rises when the block is
observed, andfallsagaintoastablelevel.Thepatternistheoppositeontrialswithnegative
correlations. Average Œ¶isthemost similar betweencorrelationprofilegroupsintheperiod
between the first block observation and the point at which average Œ¶returns to baseline
level. ThisexplainswhyweseenoaveragefluctuationinŒ¶whenincludingall animatsinthe
hard task at the first block observation (Fig 4d), while still getting a broad distribution of
correlations coefficients. The inverse patterns of positively and negatively correlated trials
cancel eachother out intheunifiedaverage. Additionally, thegeneral differenceofaverage
Œ¶baseline levels between positive and negative correlation profiles explainstherelatively
bigdifferencebetweenthesetwogroupsinbothevolutionarytime(Fig7b)andtrial time(Fig
8c).
Fig8: AveragefluctuationsinŒ¶andsurprisalontrialtimesplitintotrialswithnegative(purple,coef.<-0.1,N=
4489),neutral(grey, coef.in(-0.1,0.1),N = 1532)andpositive(brown,coef.> 0.1,N = 5611)correlations
betweenŒ¶andsurprisal.a)averagesurprisalovertrialtime.b)averagesurprisalovertrialtime,centeredaround
27

thefirstobservationoftheblock.c)averageŒ¶overtrialtime.d)averageŒ¶overtrialtime,centeredaroundthe
first observation of the block. Shading around lines is the standard error of the mean.
Discussion
We evolved 50 animats in the easy task, out of which34reachedperfect fitness, and100
animats in the hard task, out of which only 8 reached perfect fitness (see Table 1). This
shows that there is a substantial difference indifficultybetweenthetasks, whichseemsto
affect Œ¶.Feweranimatsshowednointegrationatthefinal generationinthehardtaskthanin
theeasytask, andonly8.3%of all trialsat thefinal generationhadameanŒ¶valueof0in
the hard task, whereas the easy task had a much higher percentageof suchtrials(22%).
The maximum value of Œ¶ found in the hard task (Œ¶ = 4.11) was much higher than the
maximumvaluefoundintheeasytask(Œ¶=2.49).Thereisaslightbutcleardifferenceinthe
distributionof Œ¶valuesatthefinal generation(Fig2),seeminglydrivenbyadifferenceinthe
number of zero- Œ¶statesbetweenthetasks, rather thanageneral level of higher Œ¶values.
As such, it seems that the hard task puts more evolutionary pressure on the presence of
integration rather than the level of integration, compared to the easy task.
On the evolutionary timescale we see similar results for Œ¶as in Albantakis et. al
(2014), althoughaconsiderablysmaller differencebetweenthetasks.Notethatwhenusing
the median (see Fig A-Cin S1 Figure) the difference between tasks is much clearer, but
theretheperfectanimatsofthehardtaskaremoresimilartotherestofthegroup.Thesame
is true for Œ¶- levels in trial time. There are both upper and lower bounds for Œ¶, so the
deviationsfromlowor highbaselinesuponobservingtheblockmightbeduetoaregression
towardsthemean, whereanimatswithhigher baselinesonaveragedecreaseinŒ¶whenits
internal statechanges, andviceversa. It isstill interesting, andnot apriori tobeexpected,
however, that thereisabaselineat all, andthatanimatsseemtoreturntoitafterseeingthe
block.
In both evolutionaryandtrial time, theresultsregardingour surprisal measurewere
largely expected. Average surprisal decreased asfitnessincreasedover evolutionarytime,
28
which reflects the fact that average surprisal measures the difference fromtheempirically
observed stably optimal behavior. Surprisal also increased at thetimepoint whenanimats
first observed the block; even perfect animats cannot control when this happens, which
results in more variance andthereforemoresurprisal, andwhichalsomakesthisthemost
informative time at a given trial, in a classicShannoninformationsense. At theendof the
trial, surprisal goesdownonaverage,asanimatsbecomeabletoactinwaysthatmaketheir
sensoriummoresimilartothatofadaptiveagents,andlessentropicaswell.Surprisal isalso
lower in the easy task; that it is easier means that more animats successfully act in
accordance with the empirical goal prior.
Technically, the goal prior is meant to be an evolutionarily selected expectation for
receiving sensory inputs corresponding to self-maintaining and self-replicating behavior. In
thisexample, that wouldbeanexpectationforsucceedingontrials,fromwhichexpectations
for catching or following blocks would follow, in turn giving rise to expectations of specific
sensory-motor patterns. Our empirical goal priors, however, are directly onthesensorium,
and are not technically expectations on what is relevant for survival (success on tasks).
However, they implicitly are expectationsfor trial success, giventhat theyareexpectations
for behavior observed in the agents which perform the task perfectly.
Applying the Free Energy Principle
Inorder torelateour resultstotheFreeEnergyPrinciple,itisimportanttoconsiderwhether
the animats fulfill the criteria for its application. The animats clearly and by construction
possessstableMarkovBlankets, composedofthesensoryandmotornodes.However,they
donot activelymaintaintheir MarkovBlanketsthroughtheir owninternal dynamics.Thus,it
is harder to determine whether claiming they possess a non-equilibrium steady state is
sensical and we will not try to do so formally here. However, empirically we observe that
animat behavior converges towards stable patterns over evolutionary time, indicating a
movement towards a NESS. Similarly, within a single trial, a successful animat also
manages toenter astablesensorimotor stateof seeingor not seeingtheblock, depending
29
onthetask. Empirically, at least, then, animatsseemtobothpossessaMarkovBlanketand
act in ways that makes thementer andmaintainanevolutionarilyadaptivenon-equilibrium
steady state; it is indeed in these stable states whereour surprisal measureisthelowest,
further indicating that the measure isagoodapproximationof thegenerative-model based
surprisal that isupper-boundedbyvariational freeenergyintheFreeEnergyPrinciple.This
is useful, given that re-constructing the generative model implied by a given system‚Äôs
actions, which is necessary for directly getting the variational free energy, is rarely easy.
Importantly, inasense,theonlyprobabilistic,continuousadaptationhappeninginour
simulation happens on the evolutionarytimescale, sinceanimatsaredeterministicsystems
on the trial timescale. That raises the question of whether applying the Free Energy
Principle, and the accompanying interpretation of the system as a probabilistic Bayesian
belief updater, makes sense on the deterministic trial timescale. We will not make strong
conclusions here, except to note that even though the animats are deterministically wired
intrinsically, they might still look as if, or be well describable as if they were doing
probabilistic Bayesian inference. Indeed, it can be difficult not to interpret the animats‚Äô
block-identifying and -following behavior as goal-directed, planning based and inferential
processes, despite knowing of their deterministic internal structure. In other words, the
animats might, extrinsically, seemto be goal-directed Bayesian believers, even if theyare
just a simple network on the inside. This might seem like an unjustified
anthropomorphization of a purely mechanical system, but should rather be read as an
interpretationof thesystem‚Äôsfunctional relevance.UndertheFEP,systemsarenotsomuch
anthropomorphizedastheyarestatisticomorphized, thatis,deliberatelyinterpretedasdoing
statistical inference. Thismight sometimesbeauseful reframingof themechanical system,
partly because it can still be used when the entire internal structure of the system isn‚Äôt
known, and partly because it lends a functional and goal-oriented interpretation to the
behavior and structure of a network, which otherwise is just an arbitrary set of connected
nodes to an outside observer. Here, theanimatsinternal mechanisticstructureisknownin
all detail. Moreover, multiple mechanistic implementations can lead to perfect fitness and
30
similar behavior intheblockcatchingtask. Comparingare-constructedgenerativemodel of
individual animatstotheir actual mechanisticimplementationmayelucidatehowgenerative
models should be interpreted more generally.
IIT and the FEP
TheFEPis, bydesign, afunctional theory, inthesensethat it providesadescriptionof the
goal-directedbehaviorofagivensystem.Itisclearfromthelanguageofasystemlookingas
if it performsBayesianinference, that FEP, at least ascurrentlyformulated,takesastrongly
extrinsicperspective. Importantly, however, thisisnot tosaythataknowledgeofasystem‚Äôs
external environment isnecessaryfor makinganactiveinferenceinterpretationofasystem,
as it is usuallythecasewithextrinsicapproaches. All that istechnicallyimportant toactive
inference is the blanket states - the sensorimotor exchanges with the environment. The
systemcomesupwithstatistical explanationsforitssensoriumwithoutusinganyinformation
that isnot availabletoit; indeed, it isamajor pointthattheexternal worldis‚Äòhidden‚Äô behind
the Markov Blanket, and that the system must work around this. What makes the FEP
extrinsic, isnot just that informationfromoutsidethesystemissometimesusedtointerpret
thesystemitself. What makesactiveinferenceandtheFEPextrinsicmoregenerally,isthat
it is explicitlyoneamongmultiplepossibleinterpretationsof thefunctionalityof thesystem,
asseenbytheresearcher. Thisisincontrast toIIT‚Äôsinterest inwhat thesystemistoitself,
basedonthecausal structureof itsinternal states,aqualitywhichdependsonthesystem‚Äôs
mechanistic implementation, but should ideally be independent of any outside interpreter.
Anobvious, andimportant, questionwhichbecomesapparent whenrelatingthetwo
frameworks is then: is the intrinsic experience of a systemhomomorphic to the statistical
beliefs implicit in its behavior? This, or even a complete identity betweenthetwo, isoften
casuallyassumedbyfunctionalisttheoriesofconsciousness.Howeverthis,atleastintheory
andperhapsalsoinpractice,doesnotalwaysneedtobethecase.AccordingtoIIT,systems
withdifferent internal causal structures, andthereforedifferent intrinsicexperiences, canbe
functionally identical (Oizumi et al., 2014), and therefore seemtoholdthesameBayesian
31
beliefs. However, there might be good reason to believe that, at least most of the time,
intrinsic experience and functionally implicit statistical beliefs are closely related. Systems
that seemfunctionally identical in a given context can have widely different circumstantial
constraints, suchasenergycost, becauseof different internal structures, meaningthatthey
wouldnot beequallyabletokeepexistinginagivenenvironment. Apart frommakingsuch
systems functionally heterogeneous on this longer timescale, this suggests that the more
efficient solution(e.g. lowest energyconsumption) will probablybethemost likely,reducing
the amount of functionally identical but internallydifferent systemsthat inpracticeexistsin
nature. FEP‚Äôsfindingsthat internal structuresofsystemscometoresembletheenvironment
(Friston, 2013; Fristonet al., 2021) alsoindicatethatsystemsinsharedenvironmentsmight
come to have similar intrinsic experiences. However, that the animats in our study show
different baseline levels of Œ¶, despite evolving in identical environmentsandwithidentical
constraints, suggests that the intrinsic structures of such integrated systems can vary in
ways potentially substantial for the quality of experience (although it ishardtoassessthe
magnitude of differences in Œ¶ values).
IITdoesnot inherentlyclaimthat intrinsicexperiencewill necessarilydependdirectly
onsensoryinputs.However,sensoryinputscontributetodeterminingtheinternal stateofthe
system, which in turn determines the system‚Äôs state-dependent causal structure and Œ¶
value. Under theFEP,morespecifically,sensoryobservationsshouldaffecttheconfiguration
of internal states in ways that reflect themost probablecauseof theobservation, whichin
turn might facilitate a stronger relation between contextualizedsensoryinputsandintrinsic
experiences. Inour simulation, weseeinitial evidenceforthis:Œ¶indeedfluctuateswhenthe
observationof theblockaffectstheinternal states(whichshouldalsobethetimewhenthere
is high extrinsic information content of the animat‚Äôs sensory states).
Venues for further work
The relationship between intrinsic experience and adaptive behavior guided by implicit
statistical beliefs is difficult and important, but is often casually assumed or ignored by
32
moderncognitivescienceandneuroscience. WorkingwithboththeFEPandIITmight help
elucidate this relation. One obvious direction is to get a generative model for an active
inference agent where Œ¶ can also be calculated. This would first require constructing an
active inference model which can reproduce the behavior of the animats on the block
catchingtask. Giventhattheenvironment,actionspaceandtimeareall discrete,thiscanbe
done as a Markov Decision Process (MDP) model. Subjecting such an optimal active
inference agent to the exact same sensorimotor exchanges as a givenanimat wouldthen
provide the optimal Bayesian beliefs, precisions and variational free energy of an active
inference agent in that situation, which can then be compared to Œ¶ and the conceptual
structure. It isalsoanoptiontofit suchMDP-basedactiveinferencemodelstothebehavior
of the animats, as onemight toahumanparticipant (asinSmith, Friston, &Whyte, 2021).
Comparing different models like this would allowfor inferringwhichanimatsseemtohave
whichmodel structures, andfor examplecorrelateŒ¶withhavingaconceptual structurewith
a longer temporal and counterfactual depth (as is suggested by Firston (2018)), which
seemscompatiblewiththefact that themoreadvancedanimatbehaviorsindeeddisplayan
ability to distinguish between more contexts, and to plan further ahead in time.
Ingeneral, theworktorelatetheFEPandIIT, andprobablymuchofconsciousness
scienceandneuroscienceingeneral, wouldbenefit greatlyfromsomethoroughconceptual
workclarifyingtherelationsbetweenthetermsandconceptsinthedifferenttheories.Words
such as complexity, intrinsicality, and‚Äòthing‚Äô areusedwithpotentiallydifferent meanings, or
at least different operationalizations, in the two theories. There is much to do in terms of
investigatingtheconceptual andformal relationsbetweentheconstructsinIITandFEP,but
this comes with potential for great gains. As an example, there might be in practice a
relationshipbetweenastableMarkovBlanket andthebordersofasystem‚Äôsmaincomplex-
perhapsmaintainingacausal borderliketheMarkovBlanketalsooftenresultsincomplexes
not stretching beyond that border. Perhapsnot. Therelationbetweenextrinsicandintrinsic
causal borders, that is, what it means to be a thing to the outside observer, and what it
meanstobeathingtooneself, issofar not clear. Ingeneral thereismuchworktobedone
33
in order to bridge the two frameworks, not to mentionthecolossal workbeingdonewithin
each framework to further develop and improve them separately.
Methods
Simulation Details
ThesimulationwasdoneusingthesimulationframeworkMABE(Bohm,Lalejini,Schossau&
Ofria, 2019). This framework, and the particular settings and environment implemented,
have been used to study IIT in an evolutionary context before (Juel, Comolatti, Tononi &
Albantakis, 2019). Webaseour studyonthecodeusedinAlbantakiset al.(2014);all code
and simulated data can be found on a time-stamped repository at osf.io/uzpca.
The task environment was a two-dimensional space that had a width of 16square
units and a height of 36 square units. The left and right side of the environment were
connected, so if something moved across the edgeof theenvironment it wouldappear on
theoppositeside. Oneachtrial, ahorizontal blockof varyinglengthwouldmoveacrossthe
taskenvironment inaseriesof timesteps. Oneachtimesteptheblockwouldmoveoneunit
down and one unit to the side until it reached the bottomof thetaskenvironment. Blocks
would move consistently and unidirectionally during each trial reaching the bottom in 35
timesteps. Thefirst timestep, wheretheanimatsareinitializedinthesameall-off state,and
the last timestep, where animats are no longer performing the task but instead enters a
win-or-lose state, were not used, meaning that the analyzed trials consisted of 33 timesteps.
Withintheenvironment, ananimat isrepresentedasathreeunitblockatthebottom
of thetaskenvironment.Itconstitutedasmall Markovbrain(Hintzeetal.,2017)consistingof
twosensorynodes, twomotor nodesandfourhiddennodes,all withtwopossiblestates(on
and off). The animats‚Äô sensorswerepositionedontheoutermost unitsof theanimat block.
Theywouldactivatethecorrespondingsensorynodesifablockwasinadirectlineaboveit.
Theanimat wouldthenmoveleftorrightwhenthecorrespondingmotornodewasactivated.
If both motor nodes were in the same state, either on or off, the animat would not move.
34
Withinthenetworkoftheanimat‚ÄôsMarkovbrainitwaspossibletoformconnectionsbetween
all nodes, except that sensorynodescouldnothaveinputsandmotornodescouldnothave
outputs. Thehiddennodesandthemotornodeswouldactivatebasedoninputsaccordingto
a specific logic, which is also adapted during evolution.
When the falling block reached the bottom of the task environment, it would be
consideredacatchif theanimat wasoverlappingwiththeblock,otherwisetheanimatwould
haveavoidedtheblock. Thetaskwasfor theanimat tocatchblocksofacertainlengthand
avoid blocks of other lengths. In Albantakis et al. (2014) there were 4 different task
conditions, but hereweonlyfocusontask1(blocklengths1:catchand3:avoid)andtask4
(blocklengths3, 6: catch, and4, 5: avoid), whichinour paper iscalledthe‚Äúeasytask‚Äùand
the ‚Äúhard task‚Äù for simplicity.
150evolutionaryrunsweresimulated.Eachrunconsistedof60,000generationsand
each generation of 100 animats. In the first generation, the animats had no connections
betweentheir systemnodes. After eachgeneration,theanimatsofthenextgenerationwere
sampledfromtheanimatsof thecurrent generationwithreplacement. Thebetterananimat
performed, the more it was sampled compared to other animats. After the animats of the
next generation had been sampled they would mutate according to a genetic algorithm,
resulting in changes in the nodes‚Äô internal logic and their connections. For eachrun, data
wasrecordedonthebest performinganimat‚Äôslineofdescent(LOD)every500generations.
Thus, 121 animats were recorded for each of the 150 LODs. Each animat performed128
trials, eachof 33timesteps(excludingthefirstandthelasttimestep).Dependingonthetask
difficulty, the animat wouldencounter twoor four blocksof different lengths. Out of the150
simulation runs, there were 50 runs of the easy task, which is the same number of
simulationsasinAlbantakiset al. (2014). However, wesimulated100runsofthehardtask,
inorder tobothget abetter samplesizewhengroupingthedataandgetalargernumberof
perfect animats. For more details on the simulation environment and the evolutionary
algorithm, see the methods section of  Albantakis et al. (2014).
35
Calculating Œ¶
The IIT analysis was carried out in pythonusingthePyPhi package(Mayner et al., 2018).
Here we use the third iteration of theIITformalism(‚ÄúIIT3.0‚Äù) asdescribedinOizumi et al.
(2014). The following section will describe in short the procedure for calculating Œ¶,
summarized in Fig 9. At themoment, calculatingŒ¶isonlypossiblefor discretesystemsin
discrete time (Gomezet al., 2021). Œ¶must becalculatedfor eachtimestepof interest, and
thus, is state dependent. First a Transition Probability Matrix (TPM) is made, representing
the internal logic of the system‚Äôs elements. Every set of elementswithinthelarger system
are then evaluated as candidate systems, including the whole system. Elements outside
eachcandidatesystemarecalledbackgroundconditionsandarefixedintheircurrentstate
duringtheanalysisofeachspecificcandidatesystem.Integratedconceptual informationŒ¶is
thencalculatedforeachcandidatesystem,wherethecandidatesystemwiththehighestŒ¶is
the system‚Äôs main complex. In principle, non-overlapping sets of elements may form
additional complexes that would also be considered conscious.
36
Fig 9:A summary of the procedure for calculatingŒ¶.
When analyzing a candidate system, one first derivesitsTPMfromthefull system‚ÄôsTPM.
From the candidate system‚Äôs TPM one can calculate its unconstrained cause and effect
repertoires. Acauserepertoireisa(product) probabilitydistributionacrosssystemstatesat
theprevioustimestep, theprobabilityof eachstatebeingtheprobabilitythatitprecededthe
current state (for details see (Oizumi et al., 2014)). An effect repertoire is a (product)
probability distribution of how likely system states at the following timepoint are to occur
37

given the current state. Theunconstrainedcauseandeffect repertoirearecalculatedgiven
no information about the current state.
Next, the powerset of candidate mechanisms are found, i.e. all possiblesubsetsof
the candidate systemincluding the whole system. Then for eachcandidatemechanismall
possible purviews of the mechanismare evaluated. Apurviewconsistsof amechanismat
the current timestep and the same or a different set of elements at the previous timestep
(causepurview) or thenexttimestep(effectpurview).Thepurviewsareevaluatedinorderto
find the purviews over which a maximum of integrated information œÜ is specified for the
cause andeffect separately(œÜcause
andœÜeffect
). œÜiscalculatedastheearthmover'sdistance
between the cause or effect repertoire of the purview(theprobabilitydistributiongiventhe
purview) andthepartitionedcauseoreffectpurviewundertheminimuminformationpartition
(with the minimumdistance to therepertoireof thenon-partitionedpurview). Theminimum
information partition is found by iterating over all possible partitions and calculating this
distance. The sets of elements pertaining to the cause and effect purviews that specify a
maximumof œÜ are the mechanism‚Äôs core cause or core effect, respectively. Theoverall œÜ
value of the mechanismitself is the minimumof œÜcause
and œÜeffect
. In thecasethat thisisa
non-zero value, the mechanism and its cause-effect repertoire constitute a concept.
When all concepts of the candidate systemare found, these concepts constitutea
conceptual structure.Tocalculateintegratedconceptual informationŒ¶,thecandidatesystem
is partitioned in all possible ways. The partitioning is nowunidirectional, so that either the
input or the output of an element can be cut, but not both. For each of the partitioned
systems, the conceptual structure isderived. Œ¶isthencalculatedasthedistancebetween
the conceptual structure of the non-partitioned systemand the conceptual structureof the
system under the minimum information partition, using an extended version of the earth
mover‚Äôsdistance.Again,theminimuminformationpartitionhereisthewaytopartitioningthe
system, such that there is a minimum distance. Note that both here and at the level of
mechanisms, using the minimuminformationpartitionquantifieshowmuchthemechanism
38
or the system cannot be reduced to its parts and thus how much it is integrated to be
‚Äúsomething‚Äù above and beyond its parts.
Approximating Surprisal
In order to calculate surprisal, we first construct empirical goal priors, an empirical
approximationof theexpectedsensorystatesanadaptiveanimatwouldhave.Basedonthis
probability distribution, we calculate the surprisal at each timestep in each trial for each
animat.
Specifically, the empirical goal prior is the probability distribution over sensory states
observedinperfect animatsat theendof evolution(withfour possiblestatesbeingthefour
constellations of either active or inactive sensory nodes). The distribution is created by
counting how often each sensory state occurs across trials on the final generation for all
animats with perfect fitness. Separate probability distributions are created for eachperfect
animat, for eachcombinationoftasktype(catchoravoid)andblockmovement(leftorright),
and for each timestep during the trial. We make separateprobabilitydistributionsfor each
perfect animat becausetherearemultipleoptimal strategies, sothataveragingacrossthem
would result in probabilitydistributionsthat donot appear for anyoneanimat. Similarly, we
createdifferent probabilitydistributionsfor different tasktypesandblockdirectionsbecause
these contextual conditions result in different kinds of sensory states being related to
adaptive behavior. Finally, we make different probability distributions for each timestep
becausetherearequalitativelydifferent patternsofsensorystatesinthebeginningofatrial,
comparedtotheendof atrial, whereadaptiveanimatsareabletoconvergeonapatternof
sensorystates.Wedonotdistinguishbetweendifferentblockstartingpositions,norbetween
different block lengths within the same trial type, as these do not affect the kind of
observations that are related to adaptive behavior.
Surprisal is then calculated as the negative log probability of a given sensory state
occurring given the timestep, task type and block direction:
‚Ñë(ùëú) =  ‚àí ùëôùëõ(ùëÉ(ùëú))
39
with being thesurprisal, otheobservationsandP(o) theempiricallyobservedprobability‚Ñë
over sensory states, given a timestep, a task type and a block direction. Surprisal is
calculatedseparatelyfor eachoftheprobabilitydistributionsbelongingtoeachoftheperfect
animats. Thedistributionwhichresultsinthelowest averagesurprisal foroneanimatatone
generation across trials is then used as its surprisal score at eachtimestep, ensuringthat
surprisal is calculated relative to the perfect strategy which is the closest to the strategy
employedbythat animat. Fig10showsdepictionsof thesegoal priors,relativetowhichthe
empirical surpriseiscalculated. Hereweseesensorystateprobabilitydistributionsforeach
of the perfect animats, in each of the four types of trials, over trial time. Weseehowgoal
priors become much less entropic as trials end, because animats are better abletoact in
ways that ensure their sensorium remains in a state that is associated with adaptive
behavior.Wealsoclearlyseethedifferentstrategiesemployedbytheanimats:mostanimats
usethefollowingstrategy, wheretheyeither seeordonotseetheblock(dependingontask
type) for theentiretyofthetrial afterinferringitssize;ortheyuseapass-overstrategywhere
they see the block twice.
40
Fig 10: Empiricalgoalpriors(EGPs)oftheLODsinthehardtask,whichreachedperfectfitness.Foreach
perfectanimat,4EGPswereconstructedbasedondistinguishingfirst,catchandavoidtrials,butalso,leftand
righttrials.ThetoprowofblocksrepresenttheavoidEGPsandthebottomrowthecatchEGPs.Thosebasedon
lefttrialsareontheleftandthosebasedontherighttrialsareontheright.Eachblockconsistsof8heatmaps,
oneforeachoftheLODs.Theheatmapshave33columns,oneforeachtimestep(minusthefirstandlast),and
4 rows,oneforeachpossiblesensorystate.Thesensorystatesaredenotedbytwobinarydigits,thefirst
representingtheleftsensorandthesecondtherightsensor(0=off,1=on).Eachcellintheheatmaprepresents
theprobabilityofthegivensensorystateatthegiventimestep.Thecolorcodingisconstructedsuchthatwhite
representsa0.25probability. Astheprobabilitydecreasesbelow0.25colorsbecomeincreasinglygray, andas
41

theprobabilityincreasesabove0.25colorsbecomeincreasinglyorange.Probabilitieswithavalueof1havea
dark purple color to indicate states of certainty.
This is used as an approximation of the surprisal that would result from an animat
encountering the same sensory states, if it weretobeperformingactiveinferenceunder a
generative model of itsenvironment. Suchamodel issimplyaprobabilisticspecificationof
the relations between environment states, sensory states and animat action states:
. Here, oaretheanimat‚Äôsobservations(thesensorystates), sarethestatesofùëÉ(ùëú, ùë†, ùë¢, Œ∏)
the environment (block length, block direction, block position, own positionetc.), uarethe
active states of theanimat (movingleft or right or standingstill), and aretheprobabilisticŒ∏
relationsbetweeneachof these, theparametersof thegenerativemodel. Notethatsandu
would be inferred by an animat at within-trial time, whiletheparameters wouldbelearntŒ∏
over evolutionary time. Note also that the generative model would not necessarily
correspondexactlytothegenerativeprocessintheenvironment,sincemodelsinadditionto
becoming more accurate also become simpler with free energy minimization, abstracting
away irrelevant parts of the environment.
The marginalization of required to produce is in most casesùëÉ(ùëú, ùë†, ùë¢, Œ∏) ùëÉ(ùëú)
computationallyintractable. Thisisthereasonthat avariational freeenergyupper boundis
usedinstead, formingtheheart of activeinferenceunder thefreeenergyprinciple.Wehere
circumvent this step by approximating empirically. This allows us to investigate theùëÉ(ùëú)
animats‚Äô surprisal without reconstructing their generative models , but alsoùëÉ(ùëú, ùë†, ùë¢, Œ∏)
distances us somewhat from the Free Energy Principle in two ways. Firstly, the
approximationisnot necessarilyperfect. Theactual generativemodelsthatbestaccountfor
the animats‚Äô behavior might vary, either in their goal priors, so that some animats might
expect tofail thetask,orinhowtheyexpectstatesoftheenvironmenttointeract,sothatthe
animat might havebadpredictionsabout theconsequencesof itsactions. Theformer case
would of course be removedbyevolution, andthelatter caseisexactlywhat distinguishes
adaptive fromnon-adaptive animats, for those animats withill-specifiedgenerativemodels
42
will often have incorrect expectations, leading to higher surprisal despite their attempts at
minimizingit. Thesecondwayinwhichwedepart fromtheFreeEnergyPrincipleisthatthe
variational free energy is an upper bound on surprisal, where the difference between
variational freeenergyandsurprisal isthedivergencebetweentheanimat‚Äôscurrentmodel of
theworld, relativetotheBayes-optimal model giventhesensorium.Thedifferencebetween
surprisal andthevariational freeenergyisalsonotcapturedbyourapproximation;thisisnot
necessarily a problem, however, since the goal of free energy minimization is after all the
implicit minimization of surprisal.
Computational analysis of the relation between Œ¶ and surprisal
Thecomputational analysesbeyondcalculatingŒ¶andsurprisal,aswell astheproductionof
all plots, was coded in the programming language R (R Core Team, 2021). All analysis
scripts can be found at the project's github page: https://github.com/clolesen/phi-surprisal.
The cross-correlation analysis was carried out using the ccf function of the tsibble
package(Wang, Cook&Hyndman, 2020).Cross-correlationswithlagsrangingbetween-16
and16werecalculatedforeachtrial ofall lastgenerationanimats(seeFigAinS2Figurefor
distributionsacrossall lags).Sincethecorrelationcoefficientcan‚Äôtbecorrelatedwhenoneof
the variables is always 0, all trials with a constant Œ¶value of 0 were excluded (24.8%of
trials in the easy task and 8.9%of trials in the hard task). The lagging variable was Œ¶,
meaning that for negative lags the correlations measure the relationship between Œ¶and
futuresurprisal, andviceversafor positivelags.Correlationprofileswerederivedfromthe0
lagcorrelationcoefficients.Neutral profileshadcorrelationcoefficientsbetween-0.1and0.1,
negativeprofileshadcoefficientsbelowthatrangeandpositiveprofilesaboveit.Foranalysis
intrial timeeachtrial wasassignedacorrelationprofileindependentofwhatLODoranimat
it belongedto. For analysisonevolutionarytimeeachLODwasassignedaprofilebasedon
the average correlation coefficient across all trials of the final generation animat.
For the analysisintrial time, onlytrialsbelongingtoanimatsonthefinal generation
were used. For the analysis in evolutionary time, all Œ¶ and surprisal values were first
43
averagedacrossall timestepsfor eachanimattocreateanevolutionarytimeseriesforeach
LOD. Theseweresmoothedbyaveragingeachgenerationwiththeprevious5generations.
Thiswasdoneinorder toresemblethemethodsinAlbantakiset al. (2014). Thenall LODs
were averaged across each generation to create the average evolutionaryrun. Thewhole
processwasdoneseparatelyfor eachgroup. Althoughtheanimatsof thehardtaskending
with perfect fitness were a group on their own throughout the analyses, they were also
included in the general group of hard task animats.
Acknowledgements
References
Andrews, M. (2020). The Math is not the Territory: Navigating the Free Energy Principle.
Albantakis, L. (2020, October 6). Review: Sentience and the Origins of Consciousness:
From Cartesian Duality to Markovian Monism [Blog post]. Retrieved from
https://www.consciousnessrealist.com/sentience-and-the-origins-of-consciousness/
Albantakis, L., Hintze, A., Koch, C., Adami, C., &Tononi, G. (2014). Evolutionof integrated
causal structures in animats exposed to environments of increasing complexity. PLoS
computational biology, 10(12).
Albantakis, L., Massari, F., Beheler-Amass, M., &Tononi, G. (2020). Amacroagent andits
actions. ArXiv:2004.00058 [Cs, q-Bio]. http://arxiv.org/abs/2004.00058
Badcock, P. B., Friston, K. J., & Ramstead, M. J. (2019). The hierarchically mechanistic
mind: A free-energy formulation of the human psyche. Physics of life Reviews.
44
Badcock, P. B., Friston, K. J., Ramstead, M. J., Ploeger, A., & Hohwy, J. (2019). The
hierarchically mechanistic mind: An evolutionary systems theory of the human brain,
cognition, and behavior.Cognitive, Affective, & Behavioral Neuroscience,19(6), 1319-1351.
Barbosa, L. S.,Marshall,W.,Streipert,S.,Albantakis,L.,&Tononi,G.(2020).Ameasurefor
intrinsic information. Scientific reports, 10(1), 1-9.
Bohm, C., Lalejini, A., Schossau, J., & Ofria, C. (2019, July). MABE 2.0: an introduction to
MABE and a road map for the future of MABE development. In Proceedings of the Genetic
and Evolutionary Computation Conference Companion (pp. 1349-1356).
Clark, A., Friston, K., &Wilkinson, S. (2019). Bayesingqualia: Consciousnessasinference,
not raw datum. Journal of Consciousness Studies, 26(9-10), 19-33.
Constant, A., Ramstead, M. J., Veissiere, S. P., Campbell, J. O., &Friston, K. J. (2018). A
variational approachtonicheconstruction. Journal of TheRoyal SocietyInterface, 15(141),
20170685.
  Ellia, F., Hendren, J., Grasso, M., Kozma, C., Mindt, G., P. Lang, J., M. Haun, A., Albantakis,
L., Boly, M., & Tononi, G. (2021). Consciousness and the fallacy of misplaced objectivity.
Neuroscience of Consciousness,2021(2), niab032.https://doi.org/10.1093/nc/niab032
van Es, T., & Hip√≥lito, I. (2020). Free-Energy Principle, Computationalismand Realism: a
Tragedy.
Friston, K., Kilner, J., &Harrison, L. (2006). Afreeenergyprinciplefor thebrain. Journal of
Physiology-Paris,100(1-3), 70-87.
Friston, K. (2009). Thefree-energyprinciple:aroughguidetothebrain?.Trendsincognitive
sciences,13(7), 293-301.
45
Friston, K., & Kiebel, S. (2009). Predictive coding under the free-energy principle.
Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1521),
1211-1221.
Friston, K. (2010). The free-energy principle: a unified brain theory? Nature reviews
neuroscience,11(2), 127-138.
Friston, K. (2012). A free energy principle for biological systems. Entropy, 14(11),
2100-2121.
Friston, K. (2013). Life as we know it. Journal of the Royal Society Interface, 10(86),
20130475.
Friston, K., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., &Dolan, R. J.
(2013). The anatomy of choice: active inference and agency. Frontiers in human
neuroscience,7, 598.
Friston, K., & Frith, C. (2015). A duet for one. Consciousnessand cognition,36, 390-405.
Friston, K. (2017, May). Consciousness is not a thing but a process of inference. AEON.
Retrieved from:
https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference
Friston, K. J., Rosch, R., Parr, T., Price, C., &Bowman, H. (2018). Deeptemporal models
and active inference.Neuroscience & Biobehavioral Reviews,90, 486-501.
Friston, K. (2018). Am I self-conscious? (Or does self-organization entail
self-consciousness?). Frontiers in psychology, 9, 579.
Friston, K. (2019). A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184.
46
Friston, K. J., Wiese, W., & Hobson, J. A. (2020). Sentience and the Origins of
Consciousness: From Cartesian Duality to Markovian Monism. Entropy, 22(5), 516.
Friston, K., Heins, C., Ueltzh√∂ffer, K., DaCosta, L., &Parr, T. (2021). Stochasticchaosand
markov blankets. Entropy, 23(9), 1220.
Gomez, J. D., Mayner, W. G., Beheler-Amass, M., Tononi, G., & Albantakis, L. (2021).
Computing Integrated Information ( Œ¶) in Discrete Dynamical Systems with Multi-Valued
Elements. Entropy, 23(1), 6.
Haun, A., & Tononi, G. (2019). Why does space feel the way it does? Towards a principled
account of spatial experience. Entropy, 21(12), 1160.
Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I., & Tschantz, A. (2022).
pymdp: A Python library for active inference in discrete state spaces. arXiv preprint
arXiv:2201.03904.
Hesp, C., Ramstead, M., Constant, A., Badcock, P., Kirchhoff, M., & Friston, K. (2019). A
multi-scale view of the emergent complexity of life: A free-energy proposal. In Evolution,
Development and Complexity(pp. 195-227). Springer,Cham.
Hohwy, J. (2013).The predictive mind. Oxford UniversityPress.
Hohwy, J. (2016). The self ‚Äê evidencing brain. No√ªs,50(2), 259-285.
Hohwy, J., &Seth, A. (2020). Predictiveprocessingasasystematicbasisforidentifyingthe
neural correlates of consciousness. Philosophy and the Mind Sciences, 1(II).
Juel, B. E., Comolatti, R., Tononi, G., & Albantakis, L. (2019, July). When is an action caused
from within? Quantifying the causal chain leading to actions in simulated agents. In The
2018 Conference on Artificial Life: A Hybrid of the European Conference on Artificial Life
(ECAL) and the International Conference on the Synthesis and Simulation of Living Systems
47
(ALIFE) (pp. 477-484). One Rogers Street, Cambridge, MA 02142-1209 USA journals-info@
mit. edu: MIT Press.
Kirchhoff, M., Parr, T., Palacios, E., Friston, K., &Kiverstein,J.(2018).TheMarkovblankets
of life: autonomy, activeinferenceandthefreeenergyprinciple.Journal ofTheroyal society
interface,15(138), 20170792.
Marshall, W., Kim, H., Walker, S. I., Tononi, G., & Albantakis, L. (2017). How causal analysis
can reveal autonomy in models of biological systems. Philosophical Transactions of the
Royal Society A: Mathematical, Physical and Engineering Sciences, 375(2109), 20160358.
Marshall, W., Albantakis, L., & Tononi, G. (2018). Black-boxing and cause-effect power.
PLOS Computational Biology, 14(4), e1006114. https://doi.org/10.1371/journal.pcbi.1006114
Mayner, W. G. P., Marshall, W.,&Albantakis,L.(n.d.).CalculatingŒ¶[PDFslides].Retrieved
from
https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1006343.s001&typ
e=supplementary
Mayner, W. G., Marshall, W., Albantakis, L.,Findlay,G.,Marchman,R.,&Tononi,G.(2018).
PyPhi: A toolbox for integrated information theory. PLoS computational biology, 14(7),
e1006343.
Nikolova, N., Waade, P. T., Friston, K., & Allen, M. (2021). What might interoceptive
inference reveal about consciousness?.
Oizumi, M., Albantakis, L., & Tononi, G. (2014). From the phenomenology to the
mechanismsof consciousness: integratedinformationtheory3.0. PLoSComput Biol,10(5),
e1003588.
48
Parr, T., Pezzulo, G., & Friston, K. J. (2022). Active inference: the freeenergyprinciplein
mind, brain, and behavior. MIT Press.
RCoreTeam(2021).R:Alanguageandenvironmentforstatistical computing.RFoundation
for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
Ramstead, M. J., Veissi√®re, S. P., &Kirmayer, L. J. (2016).Cultural affordances:scaffolding
local worldsthroughsharedintentionalityandregimesof attention. FrontiersinPsychology,
7, 1090.
Ramstead, M. J. , Badcock,P.B.,&Friston,K.J.(2018).AnsweringSchr√∂dinger'squestion:
A free-energy formulation. Physics of life reviews,24, 1-16.
Ramstead, M. J., Kirchhoff, M. D., & Friston, K. J. (2019). A tale of two densities: Active
inference is enactive inference.Adaptive Behavior,1059712319862774.
Ramstead, M. J., Hesp, C., Sandved-Smith, L., Mago, J., Lifshitz, M., Pagnoni, G., ... &
Constant, A. (2021). From generative models to generative passages: A computational
approach to (neuro) phenomenology.
Rudrauf, D., Bennequin, D., Granic, I., Landini, G., Friston, K., & Williford, K. (2017). A
mathematical model of embodied consciousness. Journal of theoretical biology, 428,
106-131.
Safron, A. (2019a). An Integrated World Modeling Theory (IWMT) of consciousness:
Combining Integrated Information and Global Neuronal Workspace Theories withtheFree
Energy Principle and Active Inference Framework; towards solving the Hard problemand
characterizing agentic causation.
Safron, A. (2019b). Integrated World Modeling Theory (IWMT) Revisited.
https://doi.org/10.31234/osf.io/kjngh
49
Sevenius Nilsen, A., Juel, B. E., & Marshall, W. (2019). Evaluating Approximations and
Heuristic Measures of Integrated Information. Entropy, 21(5), 525.
  Smith, R., Friston, K., & Whyte, C. (2021). A step-by-step tutorial on active inference and its
application to empirical data.
Schossau, J., & Hintze, A. (2020). Small Implementation Differences Can Have Large
Effects on Evolvability. In W. Banzhaf, B. H. C. Cheng, K. Deb, K. E. Holekamp, R. E. Lenski,
C. Ofria, R. T. Pennock, W. F. Punch, & D. J. Whittaker (Eds.), Evolution in Action: Past,
Present and Future: A Festschrift in Honor of Erik D. Goodman (pp. 423‚Äì434). Springer
International Publishing.https://doi.org/10.1007/978-3-030-39831-6_28
Solms, M. (2021). The hidden spring: A journey to the source of consciousness. Profile
books
Solms, M., & Friston, K. (2018). Howandwhyconsciousnessarises: someconsiderations
from physics and physiology.
Tononi, G. (2004). An information integrationtheoryof consciousness. BMCneuroscience,
5(1), 42.
Tononi, G. (2012). Phi: A Voyage from the Brain to the Soul. Pantheon.
Tononi, G., & Koch, C. (2015). Consciousness: Here, there and everywhere? Philosophical
Transactions of the Royal Society B: Biological Sciences, 370(1668), 20140167.
https://doi.org/10.1098/rstb.2014.0167
Tononi, G., Boly, M., Massimini, M., & Koch, C. (2016). Integrated information theory: From
consciousness to its physical substrate. Nature Reviews Neuroscience, 17(7), 450‚Äì461.
https://doi.org/10.1038/nrn.2016.44
50
Tononi, G., Albantakis, L., Boly, M., Cirelli, C., & Koch, C. (2022). Only what exists can
cause: An intrinsic view of free will. arXiv preprint arXiv:2206.02069.
Vasil, J., Badcock, P. B., Constant, A., Friston, K., &Ramstead, M. J. (2020). AWorldUnto
Itself: Human Communication as Active Inference. Frontiersin Psychology,11, 417.
Veissi√®re, S. P., Constant, A., Ramstead, M. J., Friston, K. J., & Kirmayer, L. J. (2019).
Thinking through other minds: A variational approach to cognition and culture. Behavioral
and Brain Sciences, 1-97.
Wang E, Cook D, Hyndman RJ (2020). ‚ÄúA new tidy data structure to support exploration and
modeling of temporal data.‚Äù Journal of Computational and Graphical Statistics, 29(3),
466-478. doi: 10.1080/10618600.2019.1695624
Whyte, C. J., & Smith, R. (2021). The predictive global neuronal workspace: A formal active
inference model of visual consciousness. Progress in neurobiology, 199, 101918.
Williford, K., Bennequin, D., Friston, K., & Rudrauf, D. (2018). The projective consciousness
model and phenomenal selfhood. Frontiers in Psychology, 9, 2571.
51
Supporting Information Captions
S1 Figure. Median plots.Versions of core figuresin the text using median instead of meanvalues.
S2 Figure. Correlation plot with all lags.Cross-correlationplot between surprisal and Œ¶,with all lag sizes.
S3 Figure. Trial time fluctuations split separately for LOD‚Äôs.Trial-time fluctuations shownwith each line of descent displayed separately.
52
S1 Figure. Median plots
This appendix contains all plots from the results section where averages were used, using
the median instead, for comparability. Most importantly, we see that using the median makes
the difference in Œ¶ between the easy and the hard task more evident, but hides the
difference to the perfect animats in the hard task. Plots are also less smooth because Œ¶ and
surprisal values in practice only enter on a set of discrete values, because there is a limited
set of possible states visited by the animats.
Fig A: EvolutionofŒ¶andsurprisaloverevolutionarytime.AlternativeversiontoFig3inthemaintext,using
median instead.
53

Fig B: FluctuationsinŒ¶andsurprisalovertrialtime,splitbytaskdifficulty. AlternativeversiontoFig4inthe
main text, using median instead.
FigC: FluctuationsinŒ¶andsurprisalovertrialtime,splitbycorrelationprofile.AlternativeversiontoFig8in
the main text, using median instead.
54

S2 Figure. Correlation plot with all lags
FigA: ExtendedversionofFig6inthemaintext,displayingalllagsfrom-16to16,butwithoutthegroup‚ÄùHard
Task- Perfect‚Äù.Thesamegeneralpatternholds,wherecorrelationsbecomemorestronglycenteredaround0as
lag sizes increase.
55

S3 Figure. Trial time fluctuations split separately for LOD‚Äôs
In order to get a better idea of the trial time variability between animats, we here presentthree figures representing the animats at the end point of each LOD. Note that 6 out of 100LODs in the hard task had a constant Œ¶ value of 0. These are not shown in the followingfigures where correlation profiles are used, due to their exclusion from the correlationanalysis.
FigA: AverageŒ¶andsurprisalforeachanimatatthefinalgenerationinthehardtask,shownrelativetothe
firstobservationoftheblock.Fora andb,eachanimatis assigneda profilebasedonthatanimatsaverage
correlationcoefficientandeachplotshowsalineforeachanimatwiththegivenprofile.Inc anddthesameis
shownbutforthedifferenttaskandtheperfectanimatsinthehardtask(withoutdividingthemintocorrelation
profiles).
56

57

FigB: AverageŒ¶foreachanimatatthefinalgenerationinthehardtask,shownrelativetothefirstobservation
of theblock.Hereeachplotrepresentsa uniqueanimat,wherethegivenanimat‚Äôs trialsareseparatedinto
correlation profiles.
58
59

Fig C: Averagesurprisalforeachanimatatthefinalgenerationinthehardtask,shownrelativetothefirst
observationof theblock.Hereeachplotrepresentsa uniqueanimat,wherethegivenanimat‚Äôs trialsare
separated into correlation profiles.
60