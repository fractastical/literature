Implicit Coordination using Active Epistemic Inference
for Multi-Robot Systems
Lauren Bramblett, Jonathan Reasoner, and Nicola Bezzo
Abstract— A Multi-robot system (MRS) provides significant
advantages for intricate tasks such as environmental moni-
toring, underwater inspections, and space missions. However,
addressing potential communication failures or the lack of com-
munication infrastructure in these fields remains a challenge. A
significant portion of MRS research presumes that the system
can maintain communication with proximity constraints, but
this approach does not solve situations where communication
is either non-existent, unreliable, or poses a security risk.
Some approaches tackle this issue using predictions about
other robots while not communicating, but these methods
generally only permit agents to utilize first-order reasoning,
which involves reasoning based purely on their own obser-
vations. In contrast, to deal with this problem, our proposed
framework utilizes Theory of Mind (ToM), employing higher-
order reasoning by shifting a robot’s perspective to reason
about a belief of others observations. Our approach has two
main phases: i) an e fficient runtime plan adaptation using
active inference to signal intentions and reason about a robot’s
own belief and the beliefs of others in the system, and ii) a
hierarchical epistemic planning framework to iteratively reason
about the current MRS mission state. The proposed framework
outperforms greedy and first-order reasoning approaches and is
validated using simulations and experiments with heterogeneous
robotic systems.
Note—Videos are provided in the supplementary material and
also at https://www.bezzorobotics.com/lb-smcs24. Code
will be available on the same website upon publication.
I. I ntroduction
Multi-robot systems (MRS) have the potential to trans-
form current robotics applications, performing tasks more
effectively and e fficiently than a single robot. Central to
MRS research is the idea that robots work together to
accomplish common goals. The need for cooperative teaming
is evident in numerous applications, such as search and
rescue missions, firefighting, and underwater exploration.
Effective collaboration requires robots to communicate and
synchronize their actions, but di fficulties often occur when
communication is restricted, disrupted, or presents a secu-
rity concern. Especially for a heterogeneous MRS where
different robots have di fferent operating or sensing capa-
bilities. Humans have an inherent ability to “see things
from another’s perspective” by understanding and sharing
the beliefs of others without communicating. Imagine a
parent attempting to convey a message to a child solely
through their movements. The parent might understand the
child’s short attention span and limited observational skills,
Lauren Bramblett, Jonathan Reasoner, and Nicola Bezzo are with the
Departments of Systems and Information Engineering and Electrical and
Computer Engineering, University of Virginia, Charlottesville, V A 22904,
USA. Email: {qbr5kx, vqh7rx, nb6be}@virginia.edu
Fig. 1. Pictorial representation of the problem presented in the paper.
The robots are unable to explicitly communicate their beliefs and must
convey their intentions through sensorimotor communication. In the left
frame, the red and blue robot are unable to converge to a correct belief
state using only first-order reasoning. In the right frame, the red and blue
robot clearly display their intentions by using higher-order reasoning about
their observations.
thus making exaggerated movements to communicate their
intentions. In standard multi-robot missions, there are usually
no strategies in place for communication breakdowns, or
the strategies that do exist rely solely on each robot’s first-
order understanding of the environment and system. Planning
actions socially requires a robot to infer the intentions
and beliefs of other agents, empathizing to predict what
other agents want and know about each other. The capacity
to reason about the perspective of another agent is the
foundation of theory of mind (ToM) which enables the “I
know that you know that I know” paradigm without the
need for explicit communication among actors. Formally,
as described in [1], there are di fferent orders of reasoning
possible: zero-order is a belief about oneself, first-order is
a belief about others, second-order is a belief about what
others believe about oneself, and third-order is a belief about
what others believe about each other. By using a second
and higher order reasoning architecture, we can increase
the operational e ffectiveness of multi-robot systems during
disconnected operations, allowing robots to reason about
the capability of other robots and plan according to local
observations and distributed beliefs. Epistemic planning is an
approach that integrates each agent’s belief into a planning
paradigm, enabling them to plan based not only on their
perceptions but also on what they believe other agents know
or intend to do [2].
Theory of mind and epistemic planning can enable robots
to reason about the probable knowledge and intentions of
others. Our previous work shows that in environments with
arXiv:2501.03907v2  [cs.RO]  1 Feb 2025
limited communication and uncertain operating conditions,
epistemic planning can enable an MRS to achieve exploration
and task allocation missions [3], [4]. In conjunction, active
inference can be used to compute belief-action pairs. Active
inference operates on the principle that all entities strive to
minimize variational free energy. This results in straightfor-
ward update rules for actions, perceptions, policy choices,
learning processes, and the representation of uncertainty,
which can extend to multi-agent processes [5].
In this work, we focus on the following question: How
can we ensure cooperative and e fficient behavior for multi-
robot tasks when robots cannot explicitly communicate? Our
proposed solution has two main components: 1) an e fficient
online planning mechanism that leverages active inference to
signal to others their own knowledge and intentions based on
the current epistemic state and probable goals to accomplish
the mission, and 2) heirarchical epistemic planning that
leverages our recent research [6] which allows the robots
to reason about the system goals and adapt its belief about
the system at runtime.
Consider the example in Fig. 1, where three robots cannot
communicate and the mission is defined by two tasks, one
requiring two robots and the other requiring only one robot.
During the operation, each robot maintains a belief about the
system defined by the likelihood that any robot is moving
toward one of the two tasks in the environment. In the left
frame, each robot uses only its first-order observations, rea-
soning about each robot based only on its own observations.
In the right frame, each robot uses higher-order reasoning
to not only infer other robots’ goals based on its own
observations but also by empathizing with how other robots’
beliefs would change based on its own actions and their
subsequent observations. We note that by using only first-
order reasoning, the red and blue robots cannot determine
the goals of the other, and they converge to an incorrect
belief. Using higher-order reasoning, the red and blue robots
both account for the observation of the other in deciding their
next actions and clearly indicate their intent and beliefs. In
this way, the MRS is able to converge to a correct belief
state using only local observations. In this work, we utilize
up to the third level of reasoning to allow robots to abstract
the perceptions of other agents and more accurately derive
the beliefs of the system.
The contribution of this work is two-fold: i) a higher-order
active inference framework for multi-robot task allocation
without communication, and ii) an epistemic planning frame-
work for belief updates and iterative task allocation. To the
best of our knowledge, this is the first paper combining epis-
temic logic and active inference with runtime task allocation
adaptations and no communication. We show that our higher-
order reasoning method outperforms traditional greedy task
allocation and first-order active inference algorithms to allo-
cate tasks in an environment without communication.
The rest of the paper is organized as follows: in Section II,
we provide an overview of current research in multi-robot
task allocation, epistemic planning, and active inference. In
Section III, we formally define the problem followed by
the framework for epistemic planning, active inference for
decision-making and task allocation in Section IV. Simula-
tions and experiments validating our method are presented
in Sections V and VI, respectively. Finally, conclusions and
future work are discussed in Section VII.
II. R elated Work
Theory of Mind (ToM) and epistemic planning are closely
related concepts in artificial intelligence and cognitive sci-
ence [7]. ToM refers to the ability of an agent to attribute
mental states, such as beliefs, desires, and intentions to
others, enabling it to predict and interpret their actions [8]
while epistemic planning is a type of automated planning in
artificial intelligence that deals with knowledge and beliefs
of agents [2]. Integrating ToM into epistemic planning allows
agents to anticipate and respond to the knowledge and beliefs
of other agents, leading to more e ffective coordination and
decision-making in multi-agent systems. The authors in [9]
and [10] show that nested beliefs and reasoning in multi-
agent planning can better equip agents to work in teams and
show that this integration is crucial for applications requiring
sophisticated interaction and collaboration among multiple
intelligent agents. This paper characterizes ToM for a multi-
robot system similar to [11] in that we employ epistemic
planning as a logical mechanism to account for the system’s
knowledge and beliefs. Epistemic planning can adopt the per-
spectives of other robots within the system, reasoning about
their knowledge and uncertainties, thereby preventing first-
order reasoning deadlock. Previous multi-agent planners,
such as in [12]–[15] typically maintain separate knowledge
bases for each agent in the scenario. However, these static
first-order representations lack the expressiveness required
for more complex scenarios involving nested perspective-
taking [16] and when the environment or the system changes
over time.
Dynamic Epistemic Logic (DEL) extends the concepts of
epistemic planning and ToM by providing a formal logical
framework to reason about changes in knowledge and beliefs
over time [17]. While epistemic planning focuses on devising
plans that consider the current epistemic states of agents,
DEL specifically addresses how these states evolve through
actions and observations [18]. DEL, as a result, is a more
flexible representation of the dynamics of knowledge and
belief, enabling adaptive planning in scenarios where agents
must continuously update their understanding of the world
and each other’s mental states [19]. In multi-robot systems,
DEL allows each robot in the MRS to reason and plan
using its beliefs of other robots’ beliefs of the system
while disconnected, updating its beliefs and policy if new
actions or events are observed, and routing to communicate
when necessary [17]. DEL has recently been integrated
into robotics applications. The method presented in [20]
recreates the Sally-Anne psychological test for human-robot
interactions. Typical DEL-based multi-agent research uses
epistemic planning for game theory-based policies [21].
Active inference provides a probabilistic model for agents
to make decisions and update beliefs by minimizing uncer-
tainty and surprise [22]. Using Bayesian inference, agents
predict sensory inputs and select actions aligned with their
goals, incorporating their own and others’ knowledge and
beliefs. This inherently involves Theory of Mind and DEL,
as agents model and anticipate others’ mental states for
effective interaction [23]. Integrating active inference with
ToM and epistemic planning enables sophisticated planning
and decision-making, allowing agents to refine their un-
derstanding and actions for optimal outcomes in complex,
multi-agent environments such as in [24]. This connection
is essential for developing intelligent systems capable of
adaptive and cooperative behavior in uncertain and dynamic
settings. Previous work on active inference in [25] utilized
Policy Belief Learning to improve conveyance of intent
through action, paired with a reward system, which incen-
tivized actions that improve the overall understanding of the
operational environment. Additionally, [26] evaluated how
active inference could be performed in robot teams which can
typically communicate and how the lack of communication
can be exploited to better decrease uncertainty in an agent’s
beliefs of the world. Previous work was also performed in
both leader-follower and leaderless models in [5] of which
the similar approach to their leaderless work was used
in our approach. Several works have incorporated realistic
applications, including the authors in [27] who use active
inference and behavior trees were shown to improve the
robustness of plans for a mobile manipulator. Additionally,
authors in [28] showed that perception, path generation,
localization, and mapping naturally emerge from using active
inference and minimizing free energy.
Recently, a new perspective on the theory of mind (ToM)
called the Bayesian mind has attracted attention, being sug-
gested as a feedforward model for decision-making [29]. In
this work, we extend the concept of the Bayesian mind using
active inference as in [30] and [5], and incorporate dynamic
epistemic tasking with higher-order reasoning. We demon-
strate that while first-order reasoning can yield good results,
higher-order reasoning provides more robust outcomes even
in the absence of communication and presence of uncertain
sensor measurements.
III. P roblem Formulation
Consider a MRS of n robots in the set R. We let xi denote
the state variable of the robot i that evolves according to
general dynamics at time t such that:
˙xi(t) = fi(xi(t),ui(t),νi) (1)
where ui(t) ∈Rdu and the variable νi ∈Rdν denote the control
input and zero-mean Gaussian process uncertainty, respec-
tively. The function fi represents the stochastic dynamics of
robot i given a control input and process uncertainty. We also
assume that all robots are equipped with sensors that allow
them to ascertain certain measurements from other robots in
the system. A robot’s continuous observation yi(t) at time t
depends on its own position xi(t) and sensor configuration
ωi (e.g., camera, lidar) such that:
yi(t) = hi(xi(t),ωi) + ζ. (2)
where the function hi maps a robot i’s position to its
observation yi(t) given noise ζ.
In addition, we let the set Gi ⊆G represent the subset of
all tasks in G that robot i believes is assigned to the MRS. All
possible combinations of these assignments are represented
by the power set P(G) and valid configurations for the multi-
robot mission are denoted as G⊆P (G).
For ease of discussion, in this work, we assume that all
robots know the location of all tasks G present and the
sensor configuration ωi of each robot i in the system. To
methodically allocate tasks to the multi-robot system, we
first assign a subset of all tasks to the multi-robot system
for time τ to decrease the complexity of assigning a distinct
set of tasks to each robot. We represent this problem as a
bi-level resource optimization problem [31]:
Problem 1: Upper-Level – Epistemic System Tasking:
Design an epistemic strategy for an MRS to allocate a subset
of tasks from Vto the system at any given time t, accounting
for uncertainty in local observations and considering that
robots are unable to communicate.
Problem 2: Lower-Level – Intent Signaling for Subtask
Assignment: Given the subset of tasks to complete from
the upper-level optimization, formulate a policy for e ffective
intent signaling for each robot and e fficient task completion.
The mathematical formulation of Problem 1 and 2 can be
expressed as:
min
x,z
|G|X
τ=1
cτxτ +
|R|X
i=1
GX
τ=1
c′
iτziτ (3)
subject to
|G|X
τ=1
xτ ≤K (4)
xτ ∈{0,1}∀τ∈{1,..., |G|}, (5)
ziτ ∈arg min
z′
iτ
|R|X
i=1
|G|X
τ=1
biτz′
iτ (6)
The variable ci represents the cost associated with selecting
task τ represented by the binary variable xτ. c′
iτ represents
the cost associated with assigning robot i to task τ where
the assignment for robot i at time τ is denoted by the binary
variable ziτ. biτ represents the cost in the lower-level problem
for assigning robot i to task τ. K is a constant representing
the maximum number of tasks that can be selected from the
set G. The upper-level objective function minimizes the total
cost of selecting tasks and assigning robots, while the lower-
level problem ensures the optimal assignment of robots to the
selected subset of tasks. The constraints ensure that each task
is assigned to exactly one robot if selected from the set G
and that each robot is assigned to at most one task.
IV . Approach
Our proposed framework is designed for a task allocation
problem (TAP) in which robots are unable to communicate
explicit information, but must instead signal their intent and
infer other robot’s intent in the system. When solving the
TAP, robots must update their beliefs about the state of the
system and also empathize with what others might believe.
To realize these changes, each robot observes the observable
states of each robot and reasons about the system in a hierar-
chical manner. Initially, each robot evaluates the subtasks that
need to be allocated by the system at time t, using epistemic
reasoning to converge a common belief about the allocation
of tasks within the multi-robot system. Subsequently, each
robot examines the evidence related to the movements of
all robots in the system and ultimately signals its own
intent, employing active inference to reduce the system’s free
energy. The diagram in Fig. 2 illustrates this decentralized
framework, where robots first gather observations about the
MRS. These observations are then filtered based on previous
measurements before generating and assessing allocations
for the MRS to execute at time t. Upon generating these
solutions, the resulting perspective of robot i is denoted as
si and represents the possible assignments of robots to tasks.
Fig. 2. Diagram of the proposed approach
In the next sections, we will initially concentrate on the
epistemic framing of the problem and how a robot can use
higher-order reasoning to update its beliefs. We then show
how active inference can be used to model how to measure
the results of deeper reasoning to enhance the accuracy
of non-communicative robots. We will also explore scal-
able runtime tools for employing this belief-based inference
method before discussing the online epistemic distribution of
subtasks to enable more effective reasoning during operation.
A. Epistemic Structures for Multi-Robot Reasoning
In this section, we frame the problem using dynamic
epistemic logic (DEL) and epistemic planning to enable
multi-robot goal selection in environments where direct
communication between robots is not feasible. To overcome
the challenges of coordination in this communication-limited
environment, we use dynamic epistemic logic (DEL) and
epistemic planning within each agent in our system. Our ap-
proach leverages DEL to model the knowledge, beliefs, and
intentions of robots, integrating this with planning algorithms
for goal selection and sequence optimization. To limit the
possible combinations of execution policies for each robot
to infer about the MRS, we use epistemic logic and allow
the robots to reason about the system state. For this work,
the epistemic language, L(Ψ,A P,R) is obtained as follows in
Backus-Naur form [32]:
ϕF H(η) |ϕ∧ϕ |¬ϕ |Kiϕ |Biϕ
where i ∈R. H ∈Ψ with Ψ being a set of functions that
describe the system state. η generally denotes arguments for
the functions in Ψ. ¬ϕand ϕ∧ϕare propositions that can be
negated and form logical conjunctions, where ϕ∈A Pand A P
is a finite set of atomic propositions. We denote the set of
possible worlds by W, where each world w ∈W represents
a distinct state of the system where each robot is assigned
to a subset of tasks. Kiϕ and Biϕ are interpreted as “robot i
knows ϕ” and “robot i believes ϕ”, respectively. Practically,
we consider ϕ to be the generic assignment of a robot to a
task.
We represent the global epistemic state as s =
(W,(Ri)i∈R,L,w) where L : W →A Passigns a label to each
world defined by its true propositions and the accessibility
relation Ri represents the uncertainty of robot i at run-time.
An accessibility relation Ri for robot i defines which worlds
are indistinguishable to i; that is, Ri(w,v) holds if robot i
cannot distinguish between worlds w and v. A robot may
not be able to distinguish worlds if the evidence associated
with both worlds is equivalent or similar according to the
uncertainty associated with our observations from (2).
Sequences of relations are used to represent higher-order
knowledge. For example, the statement “robot i knows that
robot j knows ϕ” is true in s if and only if s |= KiKjϕ. This
condition is satisfied when ϕ is true in all worlds accessible
from w through the composite relation of Ri and Rj. The
perspective of robot i on the system state is notated as si.
Dynamic epistemic logic is expanded from epistemic logic
through action models [20]. These models a ffect a robot’s
perception of an event and influence its set of reachable
worlds, Ri. A robot may plan to reduce the run-time uncer-
tainty by taking actions. An action a transforms a world w to
a world w′such that if w |= [a]ϕ, then ϕholds in the resultant
world w′. Robots generate plans πi that are sequences of
actions πi = ⟨ai1,ai2,..., aik⟩leading from an initial state s0
i
to a goal state s∗
i ∈Γi. Here Γi represents the set of epistemic
goal states for the robot which are defined as the possible
goal configurations the system aims to achieve.
πi = ⟨ai1,ai2,..., aip ⟩ (7)
such that
s0
i
ai1
−−→si1
ai2
−−→···
aip
−−→s∗
i ∈Γi. (8)
Coordination among robots is achieved through continuous
observation and nested belief updates:
Bi ←yi (9)
which represents what robot i believes about robot j’s goal
assignments, noting that Bi represents robot i’s nested beliefs
about the system such that Biϕ |= Bi ... Brϕ where the
subscript denotes the nested belief of the rth robot from the
perspective of robot i. The planning process incorporates the
robots’ knowledge and beliefs:
πi = Plan(s0
i ,Γi,Bi) (10)
This allows robots to infer each other’s goals:
∀i, j ∈R, Bi[yj](Γj) (11)
If robot i observes that robot j is moving towards goal
gm, it updates its beliefs to Bi(Γj = gm) where if
Bi( j is moving towards gm) then Bi(Γj = gm). Robots up-
date their beliefs based on actions, a, and observations, yi,
using DEL:
Bi ←a or Bi ←yi (12)
This framework guarantees both soundness and complete-
ness. The inference rules accurately represent the environ-
ment’s state, and with enough observations, the goals can be
inferred correctly. Combining DEL with epistemic planning
offers an effective strategy for managing multi-robot systems
without the need for direct communication. However, refin-
ing these beliefs and developing a logical policy requires a
probabilistic method that can manage complex reasoning. In
the subsequent section, we merge the epistemic framework
into the active inference model, utilizing nested beliefs and
data gathering to signal a robot’s intentions and infer the
goals of other robots.
B. Active Inference for Decision Making
Active inference robots perform perception and action
planning by minimizing variational free energy. To mini-
mize free energy, these robots utilize a generative model
that depicts the joint probability of the stochastic variables
responsible for their perceptions [33]. Fig. 3 shows our
generative model for this framework where a robot receives
observations yi ∈O, ∀i ∈R. Observations are then processed
through generalized Bayesian filtering [34], leading to an
update in the beliefs of the system. Each robot can then
utilize these revised beliefs to forecast the system’s behavior.
This process results in a robot i creating a set of policies for
all robots, but only able to control its own policy. However,
these actions affect the environment and, in turn, the state of
the system as perceived by other robots. This cycle continues,
enabling the robots to infer the intentions of other robots
and to use their own control policies to influence the beliefs
of others. Active inference is distinct from perception or
learning because it involves an active process driven by the
goal of producing observations that are minimally surprising.
Fig. 3. Overview of generative model and process used in our multi-robot
application. We assume that the state is hidden and the robot is only able
to observe using their own on-board sensing capability (e.g., depth sensors,
cameras).
The generative model of any ith robot in our approach
is mathematically defined similar to the formalism first
introduced by [5]; however, we augment this model with con-
tinuous states, observations, and actions represented in [35].
We let the dynamics model for the generative model be
represented by (1), influenced by control inputs ui(t) and
process noise. Next, we formulate the observation likelihood
for a robot i’s observations yi(t) at time t as:
P(yi(t) |xi(t),ωi) ∼N(yi(t); hi(xi(t),ωi),Σi) (13)
where hi maps the robot i’s position xi(t) to its observation
yi(t) and Σi denotes the covariance matrix that represents the
effect of noise on the observation.
The two main components of active inference are belief
updates and active selection. In our application, we note
that each robot maintains a belief over the possible goal
configurations for the multi-robot system. Depending on
the application, these goal configurations should represent
possible states that will accomplish a pre-defined mission.
Each robot maintains this belief about the possible goal
configurations that the system is performing at time t. We
represent this posterior belief as:
Q(G| yi(t),ωi) (14)
where G is a subset of possible goal configurations that
would accomplish the task allocation problem in (6). We
use Bayes’ rule to update the prior belief P(G) based on the
likelihood P(yi(t) |G,ωi) derived from the observations and
sensor configurations of robot i. This is modeled as follows:
Q(G| yi(t),ωi) ∝P(yi(t) |G,ωi)P(G). (15)
These posterior updates are then used in the active selection
component; however, we can only approximate the posterior
given that we do not have direct access to the true system
state. Therefore, we approximate the posterior q(G) as fol-
lows:
q(Gi) ∝Li(G| yi(t),Ω)P(G) (16)
where robot i’s approximate posterior q(G) is proportional
to the product of the likelihood function Li(yi(t),Ω,G) and
prior P(G). The likelihood function is discussed further in
the following section, but first we define how we use the
belief update for the free energy calculation.
By employing active inference, a robot executes a
perception-policy loop through the application of the afore-
mentioned matrices to hidden states and observations. In our
scenario, perception involves estimating which of the valid
goal configurations the system is achieving. At the start of
any mission, the MRS might have access to a prior over
goal configurations providing each robot with an initial state
estimate, which is then refined by subsequent observations.
For anticipated future states, the robot deduces the current
hidden goal configuration Gtaking into account the expected
transitions defined by the control ut and general dynamics
in (1). Active inference utilizes an approximate posterior for
hidden states and control policies. As demonstrated by the
authors in [36], the distribution is most accurately approx-
imated by minimizing the variational free energy (VFE),
which is defined at time t as:
F(xi(t),ui(t),G,yi(t),ωi) =
H[q(G)] + DKL [q(G) ∥P(xi(t) |yi(t),ωi)] (17)
where H is the model uncertainty computed using Shan-
non entropy and DKL denotes the Kullback-Leibler (KL)
divergence. This can be further generalized to expected free
energy for a policy πi:
Eπi [F] =Eπi [H[q(G)]]+
Eπi [DKL [q(G) ∥P(xi(t) |yi(t),ωi)]] (18)
The expected free energy (EFE) is a metric that integrates the
entropy of the variational distribution Q(G) with the expected
log-likelihood of the generative model. By minimizing F,
robots adjust their beliefs to better approximate the true pos-
terior distribution, balancing model complexity with align-
ment to observed data. The EFE comprises two components
that assess the quality of the policy. The first component is
the expected Kullbeck-Leibler divergence, which promotes
low-risk policies by minimizing the discrepancy between the
approximate posterior and the desired outcome. The second
component is the expected entropy of the posterior over
hidden states, representing the epistemic aspect of the quality
score and encouraging policies that reduce uncertainty in
future outcomes.
In this paper, we conceptualize the beliefs over hidden
states Q(G) for a multi-robot system (MRS) as a probabilistic
framework that represents the likelihood of various goal
configurations ˜g ∈G being the true state of the system. This
approach allows us to encode a discrete array of probabilities
that correspond to the di fferent ways in which the robots
might be arranged to achieve their respective objectives, par-
ticularly in scenarios where direct communication between
robots is not feasible.
To illustrate, consider the scenario depicted in Fig. 1. In
this example, there are two distinct goals: one that requires
the collaboration of two robots and the other that can be ac-
complished by a single robot. The set of valid configurations
in this scenario is given by G= [(0,1,1),(1,0,1),(1,1,0)].
Each tuple in Grepresents a possible state of the system,
where the elements of the tuple indicate the allocation of
robots to each goal. For example, the configuration (0 ,1,1)
implies that one robot is assigned to the first goal, while
the other two are assigned to the second goal. Thus, the
belief distribution Q(G) captures the probability that any of
these configurations reflects the true state of the system. This
probabilistic representation serves as a critical mechanism
for decision-making, guiding robots in selecting actions that
align with the most probable configurations. In doing so, the
system can dynamically adapt to uncertainties and variations
in the environment, optimizing the overall mission outcome
despite the absence of explicit communication between
robots. This method enables a form of implicit coordination
in which robots rely on their shared probabilistic understand-
ing of the mission to achieve complex objectives.
In the subsequent section, we discuss the particular like-
lihood function from (16) used to revise a robot’s belief
regarding the hidden states of the system. By employing
this function, robots can methodically gather evidence and
deduce the actual state of the system, or, in this work, the
correct goal configuration that assigns a goal to each robot.
C. Higher-Order Evidence-Based Reasoning
As previously mentioned in (16), given a set of valid
goal configurations G, a likelihood function is an important
function to update a robot’s approximate posterior belief
about the hidden states Q(G). A factor in interpreting likeli-
hood based on observations is salience. Salience describes
how prominent or emotionally striking something is. In
neuroscience, salience is an attentional mechanism that helps
organisms learn and survive by allowing them to focus on
the most relevant sensory data. In our application, salience
is the evidence that a robot is aligned with a goal gj ∈G.
The set G is different in that G is the set of all goals in an
environment, but Gis the valid goal configurations for the
multi-robot mission such that G⊆P (G) from G.
We note that previous salience functions used in active
inference and robotics literature such as in [5], [37], typically
only use up to first-order reasoning to define their evidence
and subsequent posterior belief. We begin our formulation
generally with the salience value defined as:
e(k)
i ←υ(k)
i (yi,Ω,G) = exp
 
−1
ηh(k)
i (yi,Ω,G)
!
(19)
where the array e(k)
i ∈R|G| is the mapping of observations
to evidence for goals in G from the perspective of robot
i and given the sensor configurations of all robots in the
system Ω. The superscript k denotes the level of reasoning at
which the robot is evaluating its observations. Since a robot’s
observations are independent of other robot’s observations,
we can aggregate the evidence associated with each robot i’s
perspective of other robots. For example, evidence can be
evaluated as metrics such as distance to a goal or relative
angles to a goal as shown in Fig. 4(a) and Fig. 4(b), re-
spectively. Generally, the following function maps a positive
evidence value to each goal configuration in G such that:
h(k)
i (yi,Ω,G) =
X
a∈Rk
h
h(k)
i j,a(yi,Ω,gj)
i|G|
j=1 (20)
where Rk
i denotes the subset of robots that are considered for
kth-order reasoning from the perspective of robot i, the value
hi j,r(ωi,Ω,gj) is positive for a goal gj ∈G, and robot r that
indicates if a robot is aligned with a goal gj. The notation
|G|denotes the number of elements in the set G.
One can observe that in a heterogeneous system, a robot
may not always be able to consider other robots’ perspectives
if the perspective is not measurable. As such, evidence
h(k)
i j,r(yi,Ω,G) = 0 when a robot i is unable to compute the
evidence from the perspective of robot r (ωr ≻ ωi). For
example, consider Fig. 4, a vehicle that might only be able
to measure angles cannot abstract the depth information that
other vehicles are able to observe, but angles are able to be
abstracted from depth information. This information is repre-
sented in the set Rk
i ∈R and, as a result, no new information
is mapped by robot i from robot r’s perspective and is not
able to inform the variational distribution for non-measurable
perspectives for second- and third-order reasoning.
(a) Zero and First Order
 (b) Second and Third Order
Fig. 4. Pictorial depiction of observation mapping to evidence and depth
of reasoning.
Higher-order reasoning can be iteratively aggregated to
form a comprehensive joint probability distribution. To cal-
culate the likelihood of a robot being aligned with any
particular goal in G, we let the probability distribution for
kth-order reasoning be represented as:
Pi(G) = σ

X
k
e(k)
i
 (21)
where e(k)
i represents the evidence gathered using (19) and
σ is representative of the softmax function. The belief over
the goal configurations specified by Gcan be inferred using
a joint probability distribution of the result from (21). We
formulate the joint probability distribution we first initialize
as:
PJ
i (g) = 1 ∀g ∈Gn (22)
where Gn represents all the possible combinations of n robots
assigned to |G|tasks. Then we calculate the joint probability
distribution for all possible goal configurations as
PJ
i (˜g) = PJ
i (g1,..., gn) =
nY
i=1
Pi(gi) (23)
where Pi(gi) ∈ Pi(G) and ˜ g ∈ G. Additionally, the goals
g1,..., gn represents the allocation of a goal in the set G to
each robot. The result in (23) gives the probability for all
configurations in Gand we extract and normalize the subset
of valid configurations to form a distribution:
Li (G| yi(t),Ω) = PJ
i (˜g)
P
˜g′∈GPJ
i (˜g′) ∀˜g ∈G (24)
where the likelihood Li (G| yi(t),Ω) can be used to update
the prior from (16).
The joint likelihood associated with higher-order reasoning
can increase the robustness of the overall system because of
the integration of information across multiple layers. Suppose
each independent likelihood has variance (σ2
i ). The combined
variance in the joint likelihood can be lower due to the
aggregation of information. In addition, joint likelihood can
better manage the bias-variance tradeo ff. While independent
likelihoods may lead to higher variance due to lack of
dependency modeling, a joint likelihood balances the bias
introduced by modeling dependencies and the variance re-
duction due to joint estimation. The joint likelihood provides
a more accurate expectation for each step, leading to more
stable estimates.
Lemma 1: In a multi-robot system, incorporating higher-
order reasoning (second- and third-order) reduces the vari-
ance of the robots’ belief distributions compared to first-
order reasoning by leveraging joint likelihoods that account
for dependencies among robots, assuming that the errors in
the observations from other robots are bounded and have a
mean of zero.
Proof: We aim to prove this result by direct proof.
Specifically, we will show that higher-order reasoning leads
to a reduction in the variance of the belief distributions over
goal configurations compared to first-order reasoning, under
the assumption that the errors in the observations from other
robots are bounded and have a mean of zero.
Consider a multi-robot system where each roboti evaluates
the evidence e(1)
i for each goal g ∈ G based on its own
observations, leading to a first-order probability distribution:
P(1)
i (g) = exp(e(1)
i (g))
P
g′∈G exp(e(1)
i (g′))
. (25)
This first-order distribution, P(1)
i (g), has a variance denoted
by σ2
i . However, this distribution does not incorporate the
dependencies or the information that might be inferred from
the perspectives of other robots.
Assume that the observations made by a robot i are subject
to an error term ζ from (2) and is proportional to the error
of the salience measure for each pair of robots and goals,
ϵj(g), which is bounded and has a mean of zero:
ϵi(g) ∼N(0,σ2
ϵ) and |ϵi(g)|≤ ϵmax for all g ∈G. (26)
When higher-order reasoning is introduced, robot i adjusts
its belief by considering the evidence from other robots,
which includes their respective error terms. For instance, in
second-order reasoning, the probability distribution becomes:
P(2)
i (g) =
exp

e(1)
i (g) + P
j,i E[P(1)
j (g) + ϵj(g)]

P
g′∈G exp

e(1)
i (g′) + P
j,i E[P(1)
j (g′) + ϵj(g′)]
.
(27)
Because the errors ϵj(g) are bounded and have a mean
of zero, their contribution to the variance of P(2)
i (g) will
average out as more robots’ beliefs are considered. This
process reduces the overall variance in the joint probability
distribution, which can be expressed as:
PJ
i (˜g) =
Y
g∈˜g
Pi(g). (28)
To quantify the reduction in variance, we examine the
Kullback-Leibler (KL) divergence between the robots’ joint
beliefs and their expected beliefs:
DKL

PJ
i (˜g) ∥E[PJ
i (˜g)]

. (29)
Since the errors in observations are bounded and mean-
zero, the KL divergence decreases as the robots’ beliefs
become more aligned, indicating a convergence towards a
common understanding. This reduced divergence leads to a
decrease in the variance of the belief distribution:
σ2
joint = Var

Y
g∈˜g
Pi(g)
 <σ2
first-order. (30)
Finally, the reduced variance is reflected in the decreased
entropy of the joint belief distribution:
H(PJ
i (˜g)) = −
X
˜g∈G
PJ
i (˜g) logPJ
i (˜g), (31)
where H(PJ
i (˜g)) ≤H(P(1)
i (˜g)). Lower entropy indicates that
the robots’ beliefs are more certain and less dispersed, which
corresponds to reduced variance.
In summary, by incorporating higher-order reasoning and
considering bounded, mean-zero errors in observations from
other robots, the system reduces the KL divergence and
variance in the belief distribution, leading to more stable,
accurate predictions. This proves that higher-order reasoning
is beneficial for reducing uncertainty and improving the
overall performance of the multi-robot system.
Higher-order reasoning models, even when based on first-
order measurements, reduce the overall variance of param-
eter estimates by capturing dependencies and interactions
between different layers of reasoning. This leads to enhanced
robustness, as the model can provide more stable and reliable
estimates in the presence of noise and uncertainties.
We motivate these formulations by again considering the
example shown in Fig. 4. Consider a multi-robot system con-
sisting of two ground vehicles equipped with depth sensors
and one aerial vehicle equipped with a monocular camera.
The robots are attempting to allocate tasks without commu-
nication and ground robot R1 is assessing evidence between
a goal g1 and other two robots R2,R3. In Fig. 4(a), we
show the observations and subsequent evidence calculation
for R1’s sensor configuration which allows R1 to calculate
the distance to the goal for both itself (zero-order reasoning)
and other robots (first-order reasoning). In Fig. 4(b), since
R1 is equipped with a depth sensor, it is also able to abstract
the sensor measurements of R3, allowing R1 to estimate the
change in R3’s belief.
We note that through minimizing the expected free energy
and using the likelihood function to update the posterior we
will maximize the probability of converging to a common
goal configuration without any communication. However, the
Bayesian method inherently suffers from the curse of dimen-
sionality, since the number of possible joint configurations
grows as the number of robots and /or the number of goals
grows. Thus, in the next section, we introduce epistemic
planning for reducing the goal configurations possible in each
iteration.
D. Epistemic Allocation for Dimensionality Reduction
Despite the benefits of salience in managing information
overload, the challenge of dimensionality remains. To ad-
dress this, we use epistemic allocation, which leverages the
principles of epistemic logic to reduce the set of possible
configurations in G, thereby enhancing the system’s scal-
ability and robustness. Epistemic planning involves robots
making decisions based on their knowledge and beliefs, with
the aim of reducing uncertainty and achieving goals in a
shared environment. In this context, the function for choosing
subset goals plays a crucial role. Let Bi(gj) represent robot
i’s belief about goal gj. The belief update mechanism uses a
softmax function over the evidence ei j between robot i and
goal j, formalized as:
Bi(gj) = σ

X
k
e(k)
i j
 (32)
This update represents the probability that robot i believes
goal gj is achievable, given the evidence. With higher-order
reasoning, robots can collectively maximize the diversity of
evidence. This involves each robot considering not just their
perspective but the perspectives of all robots to select goals
that maximize the collective knowledge. Formally, the set of
chosen goals Gc can be described as:
Gc = {gj |arg max
j
Bi(gj),∀i} (33)
This selection ensures that the chosen goals maximize the
diversity and coverage of evidence across all robots, thus
enhancing the collective knowledge and reducing overall
uncertainty.
V . Simulations
This section showcases the outcomes obtained through
Python simulations of our method executed by multi-robot
team. We compare zero-order reasoning [38], a first-order
reasoning baseline derived from [5], [30], and our method
employing higher-order reasoning to reach a valid goal con-
figuration. This section compares these levels of reasoning in
three different scenarios: rendezvous, where robots converge
to a common goal; task allocation, where robots converge
to separate goals; and multi-task allocation, where robots
individually complete sequential goals. Each scenario and
its respective comparison are discussed in the following
subsections.
A. Rendezvous
In the first scenario comparison, robots are tasked with
converging to a single goal amongst several possible choices.
Random configurations of goal locations, robot sensor con-
figurations, and starting positions of robots are generated
for 50 trials per each combination of robots and goals. The
number of robots and goals varies between two and five. The
size of the environment for each test is set at 30m ×30m and
the maximum number of iterations or time steps per simula-
tion is set to 150 iterations. The maximum velocity for each
robot is 1m/s, and the multi-robot system has converged if all
robots reach a single goal within 150 iterations and are within
1.5m of the position of the goal. The observation error is
normally distributed as N(0,0.5) for distance measurements
and N(0,0.1) for angular measurements. The multi-robot
system is randomly spawned with one of two di fferent types
of sensor configurations. One sensor configuration is a range
sensor ( ω1) which can observe distance measurements to
other robots, while the other configuration ( ω2) can measure
relative angles to the observing robot’s position. We consider
that ω1 ≻ ω2 since the robots are capable of abstracting
angle measurements from distance measurements. We show
a sample result in Fig. 5 comparing first-order reasoning and
higher-order reasoning in a sample environment where two
robots with two di fferent sensor configurations are trying to
converge to a single goal. The red UA V can observe angles
(ω2) while the blue UGV can measure distances ( ω1).
(a) First-order reasoning
 (b) Higher-order reasoning
Fig. 5. Sample comparison of where in first-order reasoning the red UA V
does not consider the blue UGV’s perception of its movements
As shown in Fig. 5(a), the red UA V does not consider
the blue UGV’s perception of its movements to gain more
certainty about its observations before the robots end up
converging to a goal farther away. In contrast, higher-order
reasoning allowed the red UA V to make small movements to
gain more certainty about its observations before converging
to the closer goal. The results shown in this example explain
how first-order reasoning is more prone to fail when the
objective is to rendezvous at a goal. The results of all trials
are depicted in Fig. 6 which show that higher-order reasoning
results in a higher success rate and that an increase in
complexity does not result in a significant decrease in suc-
cess, while the performance of zero and first-order reasoning
continues to decrease exponentially as the number of robots
increases.
B. Task Allocation
In the second set of trials, we show a comparison be-
tween the same zero- and first-order reasoning baselines and
higher-order reasoning when converging to separate goals.
Goal locations, robot sensor configurations, and initial robot
positions are randomly generated for 50 trials per number
of robots, which range from two to five robots. In these
comparisons, the number of tasks is equal to the number
Fig. 6. Comparison of using zero- and first-order versus higher-order
reasoning for a rendezvous mission.
of robots. The environment size for each trial is set at
30m×30m and the maximum number of iterations is set
to 100 iterations. The maximum velocity for each robot
is 1m /s and the multi-robot system has converged if all
robots reach a separate goal within 150 iterations and are
within 1.5m of the position of the goal. Observation error is
normally distributed as N(0,0.5) for distance measurements
and N(0,0.1) for angular measurements. The multi-robot
system is randomly spawned with one of two di fferent types
of sensor configurations as in previous sections. We show a
sample result in Fig. 7 comparing first-order reasoning and
higher-order reasoning in a sample environment where two
robots with two di fferent sensor configurations are trying to
converge to a single goal.
(a) First-order reasoning
 (b) Higher-order reasoning
Fig. 7. Illustration of our simulations showing that first-order reasoning
fails to allow the blue and green UA Vs to convey their intentions or reason
from the other robot’s perspective, resulting in both robots converging on
the same task. Higher-order reasoning allows the UA Vs to interpret and
signal clear intentions, successfully completing di fferent tasks.
As illustrated in Fig. 7(a), the blue and green UA Vs
are incapable of resolving their belief discrepancies. In
contrast, Fig. 7(b) demonstrates that through higher-order
reasoning, the UA Vs can convey and understand intentions,
allowing them to resolve task assignment conflicts without
communication. The results of all trials are depicted in Fig. 8
which show that higher-order reasoning improves the success
rate and that an increase in complexity does not result in a
significant decrease in success similar to the results from
Fig. 6.
C. Multi-Robot Multi-Task Allocation
Lastly, we show our method’s performance for a multi-
robot multi-task assignment problem where robots must
Fig. 8. Comparison of using zero- and first-order versus higher-order
reasoning for a task allocation mission.
decide without explicit communication or a centralized al-
gorithm to accomplish tasks in the environment. Target
locations, robot sensor configurations, initial robot positions,
and task locations are randomly generated for 30 trials
per number of robots ranging between 2 and 8 robots,
as well as number of tasks ranging between 20 and 45
tasks. In total, the data for each level of reasoning and the
number of robots is aggregated for 150 trials per category.
In Fig. 9, we show that utilizing higher-order reasoning
allows robots to decrease redundancy when completing tasks
in the environment and accomplish all tasks more quickly
than just zero- or first-order reasoning. We observe that
while the performance di fference is minimal for teams of
two robots, significant improvements are evident as the
team size increases, especially for groups with 4 or more
robots. The following example in Fig. 10 illustrates that
employing lower-order reasoning prevents robots from un-
derstanding the intentions of others, leading to duplicated
tasks and extending the time required to accomplish tasks in
the environment. In contrast, higher-order reasoning enables
robots to communicate their intentions, thereby reducing
redundancy and increasing system e fficiency.
Fig. 9. Comparison of using zero- and first-order versus higher-order
reasoning for a task allocation mission.
VI. E xperiments
Our approach was also validated through several labora-
tory experiments with a multi-robot team. The team consists
of several Husarion ROSbot 2.0s that used a Vicon motion
(a) First-order reasoning
 (b) Higher-order reasoning
Fig. 10. Illustration of our simulations for multi-robot multi-task scenarios.
In (a), the robots are unable to discern each others’ intentions and causes
redundant task completion. In (b), the robots are able to identify distinct
tasks to accomplish and decipher other robots’ intentions.
capture system for localization. Vehicles start at various
positions in the environment. The experiments were carried
out in a 4m×5.5m space. The results of a sample experiment
with two potential equidistant rendezvous locations and three
ground vehicles are shown in Fig. 11.
Fig. 11. Experiment where three robots rendezvous at one of two
equidistant locations without explicit communication.
As shown in the figure, each robot initially is uncertain
about which goal the system should converge to. After a
small number of measurements, the robots makes their in-
tentions explicit by minimizing free energy and maximizing
evidence that they are moving toward the green rendezvous
point. The robots accomplish this by taking exaggerated
paths toward the green rendezvous point. This is also the
case depicted in Fig. 12 where robots move toward distinct
goals and signal their intentions using exaggerated paths.
We show similarly that we can perform multiple tasks
per robot with heterogeneous sensing capabilities. Fig. 13
shows several snapshots of the results of this sample virtual
experiment using the RotorS Firefly and Clearpath Jackal
models in Gazebo and RViz where an aerial vehicle can
only observe the angles of other robots from tasks, while
the ground robots can observe the depth. Similarly to the
simulations, we assume that the ground robots can abstract
the angle measurements of the aerial vehicle. Fig. 13(a)
shows the starting location of all the robots and the tasks.
Fig. 13(b) shows that the robots initially move to signify
Fig. 12. A four robot experiment where each robot needs to accomplish
a distinct task without explicit communication.
which tasks they are going to complete while Fig. 13(c)
shows the majority of tasks accomplished without explicit
communication. In Fig. 13(d) all tasks have been accom-
plished and the robots return to their initial location. In
this way, the robots cooperatively complete all tasks in the
environment, accounting for the heterogeneity of the robots
in the system and without explicit communication.
(a)
 (b)
(c)
 (d)
Fig. 13. RViz and Gazebo snapshots of virtual experiment with het-
erogeneous vehicles. As shown, the robots use higher-order reasoning to
accomplish multiple tasks, even with di fferent sensor configurations.
VII. C onclusion
In this work, we demonstrated the effectiveness of utilizing
higher-order reasoning for multi-robot systems (MRS) oper-
ating under communication constraints. By integrating theory
of mind (ToM) and epistemic planning, our proposed frame-
work allows robots to infer the knowledge and intentions of
others based on their observations and last known states. This
approach enables robots to cooperate and achieve common
goals even when explicit communication is not possible.
Our findings show that higher-order reasoning, extending
up to the third level, significantly enhances the ability of
MRS to converge to correct belief states and complete tasks
efficiently. The hierarchical epistemic planning combined
with active inference for runtime plan adaptation provides
a robust solution to mitigate the challenges of limited
communication in heterogeneous robot teams. Future work
will focus on optimizing epistemic planning techniques and
exploring the integration of even higher levels of reasoning.
Additionally, we aim to extend our framework to more com-
plex scenarios and larger robot teams, such as sensors with
limited field-of-views and cluttered environments, further
enhancing the decision-making capabilities of multi-agent
systems.
VIII. A cknowledgements
This work is based on research sponsored by Northrop
Grumman through the University Basic Research Program.
References
[1] A. Valle, D. Massaro, I. Castelli, and A. Marchetti, “Theory of mind
development in adolescence and early adulthood: The growing com-
plexity of recursive thinking ability,” Europe’s journal of psychology ,
vol. 11, no. 1, p. 112, 2015.
[2] T. Bolander and M. B. Andersen, “Epistemic planning for single-
and multi-agent systems,” Journal of Applied Non-Classical Logics ,
vol. 21, no. 1, pp. 9–34, 2011.
[3] L. Bramblett, S. Gao, and N. Bezzo, “Epistemic prediction and plan-
ning with implicit coordination for multi-robot teams in communica-
tion restricted environments,” in 2023 IEEE International Conference
on Robotics and Automation (ICRA) , 2023, pp. 5744–5750.
[4] L. Bramblett and N. Bezzo, “Epistemic planning for multi-robot sys-
tems in communication-restricted environments,” Frontiers in Robotics
and AI, vol. 10, p. 1149439, 2023.
[5] D. Maisto, F. Donnarumma, and G. Pezzulo, “Interactive inference:
a multi-agent model of cooperative joint actions,” IEEE Transactions
on Systems, Man, and Cybernetics: Systems , 2023.
[6] L. Bramblett, B. Miloradovic, P. Sherman, A. V . Papadopoulos,
and N. Bezzo, “Robust online epistemic replanning of multi-robot
missions,” arXiv preprint arXiv:2403.00641 , 2024.
[7] M. K. Ho, R. Saxe, and F. Cushman, “Planning with theory of mind,”
Trends in Cognitive Sciences , vol. 26, no. 11, pp. 959–971, 2022.
[8] J. R. Anderson, D. Bothell, M. D. Byrne, S. Douglass, C. Lebiere,
and Y . Qin, “An integrated theory of the mind.” Psychological review,
vol. 111, no. 4, p. 1036, 2004.
[9] C. Muise, V . Belle, P. Felli, S. McIlraith, T. Miller, A. R. Pearce,
and L. Sonenberg, “Efficient multi-agent epistemic planning: Teaching
planners about nested belief,” Artificial Intelligence , vol. 302, p.
103605, 2022.
[10] Y . Zhang and B. Williams, “Adaptation and communication in human-
robot teaming to handle discrepancies in agents’ beliefs about plans,”
in Proceedings of the International Conference on Automated Planning
and Scheduling, vol. 33, no. 1, 2023, pp. 462–471.
[11] T. Engesser, T. Bolander, R. Mattm ¨uller, and B. Nebel, “Coopera-
tive epistemic multi-agent planning for implicit coordination,” arXiv
preprint arXiv:1703.02196, 2017.
[12] K. Talamadupula, G. Briggs, T. Chakraborti, M. Scheutz, and
S. Kambhampati, “Coordination in human-robot teams using mental
modeling and plan recognition,” in 2014 IEEE /RSJ International
Conference on Intelligent Robots and Systems. IEEE, 2014, pp. 2957–
2962.
[13] G. Buisan and R. Alami, “A human-aware task planner explicitly
reasoning about human and robot decision, action and reaction,”
in Companion of the 2021 ACM /IEEE International Conference on
Human-Robot Interaction, 2021, pp. 544–548.
[14] J. Hwang, J. Kim, A. Ahmadi, M. Choi, and J. Tani, “Dealing with
large-scale spatio-temporal patterns in imitative interaction between a
robot and a human by using the predictive coding framework,” IEEE
Transactions on Systems, Man, and Cybernetics: Systems , vol. 50,
no. 5, pp. 1918–1931, 2018.
[15] Y . Liu, X. Xie, J. Sun, and D. Yang, “Event-triggered privacy preser-
vation consensus control and containment control for nonlinear mass:
An output mask approach,” IEEE Transactions on Systems, Man, and
Cybernetics: Systems, 2024.
[16] S. Lemaignan and P. Dillenbourg, “Mutual modelling in robotics:
Inspirations for the next steps,” in Proceedings of the Tenth Annual
ACM/IEEE International Conference on Human-Robot Interaction ,
2015, pp. 303–310.
[17] H. Van Ditmarsch, W. van Der Hoek, and B. Kooi, Dynamic epistemic
logic. Springer Science & Business Media, 2007, vol. 337.
[18] J. Van Benthem, “Games in dynamic-epistemic logic,” Bulletin of
Economic Research, vol. 53, no. 4, pp. 219–248, 2001.
[19] I. A. Ciardelli and F. Roelofsen, “Inquisitive dynamic epistemic logic,”
Synthese, vol. 192, no. 6, pp. 1643–1687, 2015.
[20] T. Bolander, L. Dissing, and N. Herrmann, “Del-based epistemic
planning for human-robot collaboration: Theory and implementation,”
in Proceedings of the International Conference on Principles of
Knowledge Representation and Reasoning , vol. 18, no. 1, 2021, pp.
120–129.
[21] B. Maubert, S. Pinchinat, F. Schwarzentruber, and S. Stranieri, “Con-
current games in dynamic epistemic logic,” in Proceedings of the
Twenty-Ninth International Joint Conference on Artificial Intelligence,
2021, pp. 1877–1883.
[22] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzulo
et al., “Active inference and learning,” Neuroscience & Biobehavioral
Reviews, vol. 68, pp. 862–879, 2016.
[23] G. Pezzulo, F. Rigoli, and K. J. Friston, “Hierarchical active inference:
a theory of motivated control,” Trends in cognitive sciences , vol. 22,
no. 4, pp. 294–306, 2018.
[24] M. Albarracin, D. Demekas, M. J. Ramstead, and C. Heins, “Epistemic
communities under active inference,” Entropy, vol. 24, no. 4, p. 476,
2022.
[25] Z. Tian, S. Zou, I. Davies, T. Warr, L. Wu, H. B. Ammar, and J. Wang,
“Learning to communicate implicitly by actions,” in Proceedings of
the AAAI Conference on Artificial Intelligence , vol. 34, no. 05, 2020,
pp. 7261–7268.
[26] M. A. Schack, J. G. Rogers, and N. T. Dantam, “The sound of
silence: Exploiting information from the lack of communication,”
IEEE Robotics and Automation Letters , 2024.
[27] C. Pezzato, C. H. Corbato, S. Bonhof, and M. Wisse, “Active inference
and behavior trees for reactive action planning and execution in
robotics,” IEEE Transactions on Robotics , vol. 39, no. 2, pp. 1050–
1069, 2023.
[28] O. C ¸ atal, T. Verbelen, T. Van de Maele, B. Dhoedt, and A. Safron,
“Robot navigation as hierarchical active inference,” Neural Networks,
vol. 142, pp. 192–204, 2021.
[29] G. Pezzulo and P. Cisek, “Navigating the a ffordance landscape:
feedback control as a process model of behavior and cognition,”Trends
in cognitive sciences , vol. 20, no. 6, pp. 414–424, 2016.
[30] M. Priorelli and I. P. Stoianov, “Flexible intentions: An active inference
theory,”Frontiers in Computational Neuroscience, vol. 17, p. 1128694,
2023.
[31] P.-Q. Huang, Q. Zhang, and Y . Wang, “Bilevel optimization via col-
laborations among lower-level optimization tasks,” IEEE Transactions
on Evolutionary Computation , 2023.
[32] D. E. Knuth, “Backus normal form vs. backus naur form,” Communi-
cations of the ACM , vol. 7, no. 12, pp. 735–736, 1964.
[33] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo,
“Active inference: a process theory,” Neural computation , vol. 29,
no. 1, pp. 1–49, 2017.
[34] K. Friston, K. Stephan, B. Li, and J. Daunizeau, “Generalised filter-
ing.” Mathematical Problems in Engineering , vol. 2010, 2010.
[35] T. Parr, K. Friston, and G. Pezzulo, “Generative models for sequential
dynamics in active inference,” Cognitive Neurodynamics , pp. 1–14,
2023.
[36] R. Smith, K. J. Friston, and C. J. Whyte, “A step-by-step tutorial
on active inference and its application to empirical data,” Journal of
mathematical psychology, vol. 107, p. 102632, 2022.
[37] P. Lison, C. Ehrler, and G.-J. M. Kruij ff, “Belief modelling for
situation awareness in human-robot interaction,” in 19th International
Symposium in Robot and Human Interactive Communication . IEEE,
2010, pp. 138–143.
[38] G. Gutin, A. Yeo, and A. Zverovich, “Traveling salesman should not
be greedy: domination analysis of greedy-type heuristics for the tsp,”
Discrete Applied Mathematics , vol. 117, no. 1-3, pp. 81–86, 2002.