entropy
Article
Information and Self-Organization II: Steady State and
Phase Transition
Hermann Haken 1 and Juval Portugali 2, *
/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001
/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046
Citation: Haken, H.; Portugali, J.
Information and Self-Organization II:
Steady State and Phase Transition.
Entropy 2021, 23, 707. https://
doi.org/10.3390/e23060707
Academic Editor: Ra√∫l Alcaraz
Received: 6 May 2021
Accepted: 29 May 2021
Published: 2 June 2021
Publisher‚Äôs Note:MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afÔ¨Ål-
iations.
Copyright: ¬© 2021 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
1 Center of Synergetics, Institute for Theoretical Physics, Stuttgart University, 70550 Stuttgart, Germany;
hakenhermann@gmail.com
2 Department of Geography and the Human Environment, The Raymond and Beverly Sackler Faculty of
Exact Sciences, School of Geosciences, Tel Aviv University, Tel Aviv 69978, Israel
* Correspondence: juval@tauex.tau.ac.il
Abstract: This paper starts from Schr√∂dinger‚Äôs famous question ‚Äúwhat is life‚Äù and elucidates answers
that invoke, in particular, Friston‚Äôs free energy principle and its relation to the method of Bayesian
inference and to Synergetics 2nd foundation that utilizes Jaynes‚Äô maximum entropy principle. Our
presentation reÔ¨Çects the shift from the emphasis on physical principles to principles of information
theory and Synergetics. In view of the expected general audience of this issue, we have chosen a
somewhat tutorial style that does not require special knowledge on physics but familiarizes the
reader with concepts rooted in information theory and Synergetics.
Keywords: free energy principle; maximum entropy principle; Synergetics; what is life; pragmatic in-
formation
1. Introduction
This is our second study on information and self-organization . In the Ô¨Årst [ 1], our
focus was on the exchange of information between a system and its environment. We
did so from the perspective, Ô¨Årstly, of the Synergetics 2nd foundation (see deÔ¨Ånition
below) and our Information Adaptation theory [2] with its notions of Shannon information
(SHI), semantic information (SI), and pragmatic information (PI). Secondly, we did so from
the perspective of SIRNIA that links the synergetic inter-representation networks (SIRN)
approach with IA (information adaptation). We laid our emphasis on phase transitions,
i.e., qualitative changes of the state of the system. In this second round on information
and self-organization, we further elaborate on a complex system‚Äôs exchange of information
with its environment, this time, however, in relation to other forms of exchange, namely
of matter, energy, and entropy. We do so following Friston‚Äôs [3] free-energy principle (FEP),
which entailed renewed interest in recent years in the notion of free energy, its relation to
Schr√∂dinger‚Äôs [4] question What is Life with his concepts of negative entropy as free energy
(see below). Here, the focus is on the maintenance of a steady state.
This is also our second study that relates Friston‚Äôs FEP to Synergetic 2nd foundation
and to our SIRNIA theory: Chapters 7, 8 of our book Synergetic Cities [5] explore the
relationships, similarities and differences between Friston‚Äôs FEP , Bayes, Jaynes, Synergetic
2nd foundation, and SIRNIA. Thus, our discussion below starts with short reminders
(Section 2): Ô¨Årstly, of Friston‚Äôs FEP , Synergetics 2nd foundation and SIRNIA‚Äîthe basic
theoretical frameworks at the center of this study, and, secondly, with the above noted
comparison between them. Next, we look at Schr√∂dinger‚Äôs [4] ‚Äòwhat is life‚Äô that, due to his
emphasis on the properties of ‚Äòopenness‚Äô, negative entropy, free energy, and ‚Äòorder from
disorder‚Äô, is considered one of the forerunners of complexity theory; and, we examine
Friston‚Äôs attempt to respond to the question ‚Äòwhat is life?‚Äô from the perspective of his FEP .
In Section 2.5, we point out that, because of insufÔ¨Åcient information, complex systems can
be dealt with only by a probabilistic approach. There are two pathways (micro/macro)
Entropy 2021, 23, 707. https://doi.org/10.3390/e23060707 https://www.mdpi.com/journal/entropy
Entropy 2021, 23, 707 2 of 27
to derive the appropriate probability distribution P, both of which will be followed up
in the subsequent sections of our study. In Section 3, thus, we present the Ô¨Årst pathway,
namely the microscopic theory that derives P(q; Œõ) of an open complex system by means
of Synergetic‚Äôs Ô¨Årst foundation. As is well recorded [6], Synergetics uses the phenomenon
of the laser light as its canonical case study.
Central to Friston‚Äôs FEP is the notion of free energy. In Section 4, therefore, we elaborate
on the notion of free energy and follow its metamorphosis from the original formulation
of this concept by Helmholtz [ 7] in the context of Thermodynamics, to Feynman‚Äôs [ 8]
formulation of it in the context of Statistical mechanics, to MacKay [ 9,10] (information
theory), and, recently, to Friston in the context of life sciences. As it will transpire below,
FEP as originally deÔ¨Åned by Feynman, is a terminus technicus to characterize a speciÔ¨Åc
mathematical procedure to calculate an energy, e.g., of a ferromagnet. In numerous other
cases, including Friston‚Äôs, the quantity to be calculated is not an energy, but, e.g., num-
bers of populations of excited neurons, etc. Nevertheless, the formalism is that used by
Feynman. We close Section 4 by a short discussion of the Synergetics 2nd Foundation,
that is, Synergetics‚Äô macroscopic approach that employs Jaynes‚Äô Maximum (Information)
Entropy Principle.
In Section 5, we discuss another source of inspiration that enables to deal with complex
systems‚Äîthe Bayesian inference [11], that plays an important role in Friston‚Äôs FEP also.
In Section 6, we return to the question of life that was mentioned above in connection
with Schr√∂dinger‚Äôs and Friston‚Äôs usage of the notion of free energy. This time, however, we
look at the question of life from the perspective of the discussions in Sections 2‚Äì5 above,
focusing on the way a living system maintains its steady state. We Ô¨Årst scrutinize the issue
from Friston‚Äôs main example of the perception-action cycle. Secondly, we examine it from
the perspective of Synergetics, and, thirdly, we compare the two approaches. Finally, we
illustrate the latter by means of experimental results.
We conclude our paper in Section 7 by emphasizing that homeostasis is not the only
characteristics of life. Namely, to understand life, there is a need to deal with phase
transitions, taking into account the fundamental role of information, and the principles of
self-organization as theorized by Synergetics.
2. Reminders
2.1. A Concise Introduction of the Basic Terms
Friston‚Äôs FEP , Synergetic 2nd foundation, and SIRNIA are the basic theoretical frame-
works at the center of this study, as noted. The FEP was proposed recently by Karl Friston
as a uniÔ¨Åed brain theory ‚Äúthat accounts for action, perception and learning‚Äù [3] (Abstract).
As such, it was applied to a variety of aspects in the domains of cognition, neurology,
and biology (see Reference [12] for bibliography). Haken‚Äôs Synergetics [13], on the other
hand, originated in physics but from the start was designed as a general theory of complex
self-organizing systems. As such, it was applied to a variety of domains ranging from
physics, through life sciences, sociology, psychology, cognition, AI among others, and also
to cities [5]. This latter application to cities entailed the notions of SIRN, IA, and their
conjunction SIRNIA. While there is no room in the present paper for a full-scale description
of each, a few introductory words on these three basic terms is in place.
Friston‚Äôs FEP .The FEP is essentially a mathematical formulation of how complex,
adaptive, self-organizing systems resist ‚Äúa natural tendency to disorder . . . in the face of
a constantly changing environment‚Äù [3] (p. 127). The principle that refers to biological
systems, speciÔ¨Åcally to the dynamics of brains, suggests ‚Äúthat any selforganizing system
that is at equilibrium with its environment must minimize its free energy . . . ‚Äù (ibid). The
principle further says ‚Äúthat biological agents must avoidsurprises . . . [and, that] free energy
is an upper bound on surprise, which means that if agents minimize free energy, they
implicitly minimize surprise‚Äù (italics added).
Synergetics, the ‚Äúscience of cooperation‚Äù, is Haken‚Äôs [6,13] theory of complex systems.
It originated in physics with the aim to unearth the general principles underlying self-
Entropy 2021, 23, 707 3 of 27
organization in open complex systems. Using the phenomenon of laser as its canonical
case study, it describes the dynamics of such systems as a circularly causal process by
which, Ô¨Årstly, the parts by means of their interaction give rise, in a bottom-up manner, to
the global structure of a system, that can be mathematically described by an order parameter
(OP). Secondly, once an OP emerges, it top-down prescribes the behavior of the parts‚Äî
a process termed the slaving principle. This latter bottom-up ‚Äúmicroscopic‚Äù approach is
termed Synergetics 1st foundation. At a later stage, following the application of Synergetics
to phenomena of cognition and brain functioning, the Synergetics 2nd foundation [14] was
developed as a top-down ‚Äúmacroscopic‚Äù approach. Building on Shannon‚Äôs [15,16] theory
of information, it theorizes about the way a complex cognitive system (e.g., a brain) extracts
information out of the sparse or big data furnished by the environment and on the basic of
this data behaves and acts. As can be seen, the two foundations complement each other:
the 1st focuses mainly on the internal dynamics, while the 2nd adds the interaction with
the environment.
SIRNIA. The notions SIRN, IA, and their conjunction SIRNIA were designed to deal
with a property that is speciÔ¨Åc to human agents as complex systems, namely that, as
complex adaptive systems (CASs), they adapt not only by means of behavior and action, but
also by the production of artifacts‚Äîthe elements of culture and of cultural evolution. Thus,
while all living systems (including humans) are subject to the slow process of Darwinian
evolution, humans are subject also to the very fast process of cultural evolution‚Äîthe
evolution of artifacts [5,17,18]. The two components of SIRNIA were speciÔ¨Åcally designed
to capture and model this process: SIRN by focusing on the exchange of information with
the environment, while IA on the way the mind/brain/body process this information.
Figure 1 describes this basic SIRNIA model.
Entropy 2021, 23, 707 4 of 26 
 
 
 
Figure 1. The SIRNIA basic model. For details, see text. 
2.2. Previous Comparisons between Friston‚Äôs FEP and Synergetics‚Äô 2nd Foundation 
Complex systems are typically characterized by long periods of steady state 
interrupted by short periods of phase transition. From this perspective, Friston‚Äôs FEP 
refers, and adds insight, to the dynamics of steady states but not to that of phase 
transitions. It suggests that CASs  (complex adaptive systems) have innate tendency to 
avoid phase transition and perpetuate their steady state. Synergetics accepts this view and 
indeed shows that steady states emerge and are maintained by means of circular causality. 
However, being a general theory of comple x systems, Synergetics also deals with phase 
transitions‚Äîthe processes associated with qualitative change. More specifically , 
Synergetics pays special attention and elaborates on the inter -relations between steady 
state and phase transition.  
FEP is specifically akin to Synergetic 2nd foundation: both consider the mind/brain 
as an inference device that operates by means of an action-perception play that ongoingly 
updates the brain‚Äôs information: following an update the mind/brain produces  series of 
predictions (internal states) about the environment, and updating them in light of the 
information that comes from the environment (external states). In other words, top-down 
predictive models are compared with bottom -up representations by means of em bodied 
action-perception. In this process, the prediction rule for the next step is based on the 
previously learned Free Energy, or, in terms of Synergetics‚Äô 2nd Foundation, on the 
potential landscape, V. Thus, FE and V may be interpreted as ‚Äúgenerative models‚Äù. The 
difference between Friston ‚Äôs FE and synergetics‚Äô V is: the learning of V is completed or 
supposedly completed, whereas, in Friston‚Äôs approach, it is ongoing.  
2.3. Previous Comparison between FEP and SIRNIA 
Both FEP and SIRNIA conceptualize a ci rcularly causal play between information 
constructed in an agents‚Äô mind/brain ( internal states in the language of FEP and internal 
representations in terms of SIRNIA) v ersus the information that flows from the 
environment (external states in the language of FEP and external representations in terms of 
SIRNIA). Yet, the FEP ‚Äòstates‚Äô and the SIRNIA ‚Äòrepresentations‚Äô differ from each other. 
The internal representations are mind/brain constructs of the external world, while 
external representations might take t he form of agents‚Äô behavior and action in the 
Figure 1. The SIRNIA basic model. For details, see text.
In Figure 1, the outer solid line rectangle refers to SIRN, that is, to an agent who is
subject to two Ô¨Çows of information: One that comes from the environment and one that
comes from the agent‚Äôs mind/brain. The interaction between these two Ô¨Çows gives rise to
behavior, action, and production in the environment, as well as to feedback information to
Entropy 2021, 23, 707 4 of 27
the agent‚Äôs mind/brain, and so on, in circular causality. The inner dotted line rectangle
refers to IA, that is, to that bottom-up process by which the agent‚Äôs mind/brain trans-
forms the data Ô¨Çow from the environment into Shannonian information (SHI), which then
triggers a top-down Ô¨Çow of semantic information (SI) that, by means of the inÔ¨Çation or
deÔ¨Çation of the SHI, gives rise to two forms of output: SI that feeds back to the mind/brain,
and pragmatic information (PI) in the form of behavior, action, and production in the
environment, and so on, in circular causality. As can be seen in Figure 1, the SIRNIA model
integrates the two foundations of Synergetics: the SIRN component is in line with the 1st,
while the IA component with the 2nd.
2.2. Previous Comparisons between Friston‚Äôs FEP and Synergetics‚Äô 2nd Foundation
Complex systems are typically characterized by long periods of steady state inter-
rupted by short periods of phase transition. From this perspective, Friston‚Äôs FEP refers, and
adds insight, to the dynamics of steady states but not to that of phase transitions. It suggests
that CASs (complex adaptive systems) have innate tendency to avoid phase transition and
perpetuate their steady state. Synergetics accepts this view and indeed shows that steady
states emerge and are maintained by means of circular causality. However, being a general
theory of complex systems, Synergetics also deals with phase transitions‚Äîthe processes
associated with qualitative change. More speciÔ¨Åcally, Synergetics pays special attention
and elaborates on the inter-relations between steady state and phase transition.
FEP is speciÔ¨Åcally akin to Synergetic 2nd foundation: both consider the mind/brain
as an inference device that operates by means of an action-perception play that ongoingly
updates the brain‚Äôs information: following an update the mind/brain produces series
of predictions (internal states) about the environment, and updating them in light of the
information that comes from the environment (external states). In other words, top-down
predictive models are compared with bottom-up representations by means of embodied
action-perception. In this process, the prediction rule for the next step is based on the
previously learned Free Energy, or, in terms of Synergetics‚Äô 2nd Foundation, on the potential
landscape, V. Thus, FE and V may be interpreted as ‚Äúgenerative models‚Äù. The difference
between Friston‚Äôs FE and synergetics‚ÄôV is: the learning of V is completed or supposedly
completed, whereas, in Friston‚Äôs approach, it is ongoing.
2.3. Previous Comparison between FEP and SIRNIA
Both FEP and SIRNIA conceptualize a circularly causal play between information
constructed in an agents‚Äô mind/brain (internal states in the language of FEP and internal
representations in terms of SIRNIA) versus the information that Ô¨Çows from the environment
(external states in the language of FEP and external representations in terms of SIRNIA). Yet,
the FEP ‚Äòstates‚Äô and the SIRNIA ‚Äòrepresentations‚Äô differ from each other. The internal
representations are mind/brain constructs of the external world, while external represen-
tations might take the form of agents‚Äô behavior and action in the environment, but also
stand-alone artiÔ¨Åcial objects the agents produce by means of their bodies, minds, and tools
(e.g., buildings, neighborhoods, and whole cities).
Both FEP and SIRNIA employ Shannon‚Äôs theory of information [15,16]. In Friston‚Äôs
FEP , ‚Äòinformation‚Äô (similarly to ‚Äòfree energy‚Äô) is a mathematical construct that maintains the
system in a steady state, away from phase transition, thus minimizing surprise. In SIRNIA,
Shannon information (SHI) explicitly refers to quantity of information when the task of
the system is to produce qualitative semantic and pragmatic information (e.g., pattern
recognition and action/behavior, respectively). From the latter difference follows a major
gap: Friston‚Äôs FE is based on a perception-action play that gradually updates the ‚Äòinternal
state‚Äô in the face of external inÔ¨Çow of information from the environment, thus keeping
the system in steady state. In SIRNIA, often, the perception-action play indeed leads to
a steady state, however, in other cases, to phase transition. This is so since in SIRNIA
semantic and pragmatic information are generated by means of the inÔ¨Çation or deÔ¨Çation of
Entropy 2021, 23, 707 5 of 27
SHI; in some instances, this process maintains the system is steady state, while, in others, it
is associated with a sequence of phase transitions, as illustrated in Figure 2.
Entropy 2021, 23, 707 5 of 26 
 
 
environment, but also stand-alone artificial objects the agents produce by means of their 
bodies, minds, and tools (e.g., buildings, neighborhoods, and whole cities). 
Both FEP and SIRNIA employ Shannon‚Äôs theory of information [15,16]. In Friston‚Äôs 
FEP, ‚Äòinformation‚Äô (similarly to ‚Äòfree energy‚Äô) is a mathematical construct that maintains 
the system in a steady state, away from phase transition, thus minimizing surprise. In 
SIRNIA, Shannon information (SHI) explicitly refers to quantity of information when the 
task of the system is to produce qualitative semantic and pragmatic information (e.g. , 
pattern recognition and action/behavior, respectively). From the latter difference follows 
a major gap: Friston‚Äôs FE is based on a perception-action play that gradually updates the 
‚Äòinternal state‚Äô in the face of external inflow of information from the environment, thus 
keeping the system in steady state. In SIRNIA, often , the perception-action play indeed 
leads to a steady state, however, in other case s, to phase transition. This is so since in 
SIRNIA semantic and pragmatic information are generated by means of the inflation or 
deflation of SHI; in some instances, this process maintains the system is steady state, while, 
in others, it is associated with a sequence of phase transitions, as illustrated in Figure 2. 
 
Figure 2. Here is a thought experiment. Imagine a sequence of phase transitions in the approaching 
lady thought experime nt (abscissa = amount of data vs. ordinate = recognized category/pattern). 
The broken line indicates the other dismissed options. For details, see Reference [1]. 
A more prominent example concerns the face -girl case of hysteresis ( Figure 3): In 
processes of  visual perception, repeated observations may either lead to an improved 
recognition of an object (in line with Friston‚Äôs FEP), or , as in the case of hysteresis, in a 
first step to saturation, i.e., to fading away of the image to give way to the recognitio n of 
another object, that is to say, to a phase transition (Fading away after few seconds happens 
also when a person‚Äôs gaze is fixed. Saccades renew time and again the recognition 
process.). The result is a kind of dialectics in which the tendency to maintain a steady state 
and avoid surprise is the very cause of surprise, that is, of phase transition. The whole 
process is reminiscent of the random walk of a person in the dark in a landscape with one 
or two attractors.  
 
Figure 3. Hysteresis in pattern recognition. When the sequence of figures is visually scanned from 
the upper left to the lower right, the switch from a face to a girl occurs in the lower row. When 
scanned in the reverse direction, the switch occurs in the upper row. 
To conclude, the FEP emphasizes the role of feedback in the perception -action 
process: given a certain predictive perception , the associated action feeds back by 
confirming or correcting the prediction. If it confirms, it reproduces and consolidates the 
model; otherwise, it corrects and , thus, improves the model. Such a feedback exists also 
Figure 2. Here is a thought experiment. Imagine a sequence of phase transitions in the approaching
lady thought experiment (abscissa = amount of data vs. ordinate = recognized category/pattern).
The broken line indicates the other dismissed options. For details, see Reference [1].
A more prominent example concerns the face-girl case of hysteresis (Figure 3): In
processes of visual perception, repeated observations may either lead to an improved
recognition of an object (in line with Friston‚Äôs FEP), or, as in the case of hysteresis, in a
Ô¨Årst step to saturation, i.e., to fading away of the image to give way to the recognition of
another object, that is to say, to a phase transition (Fading away after few seconds happens
also when a person‚Äôs gaze is Ô¨Åxed. Saccades renew time and again the recognition process.).
The result is a kind of dialectics in which the tendency to maintain a steady state and
avoid surprise is the very cause of surprise, that is, of phase transition. The whole process
is reminiscent of the random walk of a person in the dark in a landscape with one or
two attractors.
Entropy 2021, 23, 707 5 of 26 
 
 
environment, but also stand-alone artificial objects the agents produce by means of their 
bodies, minds, and tools (e.g., buildings, neighborhoods, and whole cities). 
Both FEP and SIRNIA employ Shannon‚Äôs theory of information [15,16]. In Friston‚Äôs 
FEP, ‚Äòinformation‚Äô (similarly to ‚Äòfree energy‚Äô) is a mathematical construct that maintains 
the system in a steady state, away from phase transition, thus minimizing surprise. In 
SIRNIA, Shannon information (SHI) explicitly refers to quantity of information when the 
task of the system is to produce qualitative semantic and pragmatic information (e.g. , 
pattern recognition and action/behavior, respectively). From the latter difference follows 
a major gap: Friston‚Äôs FE is based on a perception-action play that gradually updates the 
‚Äòinternal state‚Äô in the face of external inflow of information from the environment, thus 
keeping the system in steady state. In SIRNIA, often , the perception-action play indeed 
leads to a steady state, however, in other case s, to phase transition. This is so since in 
SIRNIA semantic and pragmatic information are generated by means of the inflation or 
deflation of SHI; in some instances, this process maintains the system is steady state, while, 
in others, it is associated with a sequence of phase transitions, as illustrated in Figure 2. 
 
Figure 2. Here is a thought experiment. Imagine a sequence of phase transitions in the approaching 
lady thought experime nt (abscissa = amount of data vs. ordinate = recognized category/pattern). 
The broken line indicates the other dismissed options. For details, see Reference [1]. 
A more prominent example concerns the face -girl case of hysteresis ( Figure 3): In 
processes of  visual perception, repeated observations may either lead to an improved 
recognition of an object (in line with Friston‚Äôs FEP), or , as in the case of hysteresis, in a 
first step to saturation, i.e., to fading away of the image to give way to the recognitio n of 
another object, that is to say, to a phase transition (Fading away after few seconds happens 
also when a person‚Äôs gaze is fixed. Saccades renew time and again the recognition 
process.). The result is a kind of dialectics in which the tendency to maintain a steady state 
and avoid surprise is the very cause of surprise, that is, of phase transition. The whole 
process is reminiscent of the random walk of a person in the dark in a landscape with one 
or two attractors.  
 
Figure 3. Hysteresis in pattern recognition. When the sequence of figures is visually scanned from 
the upper left to the lower right, the switch from a face to a girl occurs in the lower row. When 
scanned in the reverse direction, the switch occurs in the upper row. 
To conclude, the FEP emphasizes the role of feedback in the perception -action 
process: given a certain predictive perception , the associated action feeds back by 
confirming or correcting the prediction. If it confirms, it reproduces and consolidates the 
model; otherwise, it corrects and , thus, improves the model. Such a feedback exists also 
Figure 3. Hysteresis in pattern recognition. When the sequence of Ô¨Ågures is visually scanned from
the upper left to the lower right, the switch from a face to a girl occurs in the lower row. When
scanned in the reverse direction, the switch occurs in the upper row.
To conclude, the FEP emphasizes the role of feedback in the perception-action process:
given a certain predictive perception, the associated action feeds back by conÔ¨Årming
or correcting the prediction. If it conÔ¨Årms, it reproduces and consolidates the model;
otherwise, it corrects and, thus, improves the model. Such a feedback exists also in SIRNIA,
though implicitly. On the other hand, SIRNIA emphasizes the play between information
inÔ¨Çation and deÔ¨Çation in the process of perception‚Äîa play that is missing in FEP . (For a
further detailed comparison, see Reference [5] (Chapters 7, 8).
Entropy 2021, 23, 707 6 of 27
2.4. On Schr√∂dinger‚ÄôsWhat Is Life and Friston‚Äôs FEP
Schr√∂dinger‚Äôs [4] What is Life can be described as a physicist‚Äôs view on life: Life,
suggested Schr√∂dinger, is delayed entropy; by means of the process of metabolism, an
organism‚Äîa living system‚Äî‚Äú . . . feeds upon negative entropy, attracting . . . a stream of
negative entropy upon itself, to compensate the entropy increase it produces by living and
thus to maintain itself on a stationary and fairly low entropy level.‚Äù This notion of ‚Äònegative
entropy‚Äô attracted criticism from physicist colleagues and in response Schr√∂dinger wrote:
‚Äú... if I had been law catering for them alone [e.g., physicist colleagues] I should have
let the discussion turn on free energy instead [italics added]. It is the more familiar notion
in this context. But this highly technical term seemed linguistically too near to energy for
making the average reader alive to the contrast between the two things. He is likely to
take free as more or less an epitheton ornans without much relevance, while, actually, the
concept is a rather intricate one, whose relation to Boltzmann‚Äôs order-disorder principle is
less easy to trace than for entropy and ‚Äòentropy taken with a negative sign‚Äô, which by the
way is not my invention.‚Äù
With his emphasis on life as open system, his notion of negative entropy (later termed
negentropy by Brillouin [19]), his suggestion that ‚ÄúOrganization is maintained by extracting
‚Äòorder‚Äô from the environment‚Äù, and his notion of ‚Äòorder from disorder‚Äô, Schr√∂dinger‚ÄôsWhat
is Life can be considered one of the forerunners of complexity theory [ 18]. For example,
his ‚Äòorder from disorder‚Äô anticipatedorder out of chaos that became a prominent motto of
complexity theory. However, while ‚Äòorder from disorder‚Äô and his other aforementioned
concepts were embraced by the various theories of complexity, the notion of free energy
was not and had no signiÔ¨Åcant inÔ¨Çuence. Recently, however, the notion of free energy
re-appeared in Friston‚Äôs FEP and in relation to Schr√∂dinger‚Äôs questionWhat is Life. In a
recent study of the Friston‚Äôs group, Ramstead et al. [ 20] have suggested that ‚Äúthe FEP
affords a unifying perspective that explains the dynamics of living systems across spatial
and temporal scales.‚Äù
As noted above and shown below, the free energy principle to which Schr√∂dinger
refers has its origin in the work of Helmholz in thermodynamics and of Feynman in
statistical mechanics. Originally, it was a ‚Äòterminus technicus‚Äô to characterize a speciÔ¨Åc
mathematical procedure to calculate an energy, e.g., of a ferromagnet. However, in numer-
ous other cases, including Friston‚Äôs, the quantity to be calculated is not an energy, but, e.g.,
numbers of populations of excited neurons, etc. Nevertheless, the formalism is that used
by Feynman. The implication is that taking Schr√∂dinger‚Äôs view on complex systems as
starting point is too narrow. How can we describe a complex system composed of many
interacting parts away from thermal equilibrium, but in steady state?
2.5. The System‚Äôs Probability Distribution
In our article, we deal with complex systems, e.g., an animal, brain, city, society, etc. As
noted above (Section 2.1), according to Synergetics 1st foundation, a central property of such
systems is that their order emerges spontaneously, that is, by means of self-organization.
More speciÔ¨Åcally, the interaction between the parts of the system gives rise to an order
parameter that then enslaves the parts, and so on, in circular causality. In order to deal with
such systems operationally, thus, it is crucial to identify and deÔ¨Åne the order parameter(s).
But, here, we are facing a difÔ¨Åculty: Because of their complexity, our knowledge on such
systems is inexhaustible in the sense that we have only incomplete knowledge on their
structure, function, and properties. To cope with this uncertainty, i.e., to be able to make
predictions, nevertheless, we rely on guesses that are formulated by means of probabilities.
More precisely speaking, we are in search for a global probability distribution for the
entire system.
The systems we consider are open, i.e., they exchange matter, energy, and information
with their surroundings. What are the quantities P depends on? They must refer to the
properties of the system, per se, but also to the impact of the surround.
Entropy 2021, 23, 707 7 of 27
In the Ô¨Årst place, we aim at a quantitative approach based on observed (measured) data
on the proper system and its surroundings. (Note that there is the problem of quantiÔ¨Åcation
of some qualities, e.g., qualia). Actually, in other approaches, ‚Äúhidden variables‚Äù are
also included, e.g., in neural net theories or in Friston‚Äôs work. In such cases, contact to
observables must be made by speciÔ¨Åc architectures (e.g., Hinton) or by ‚Äúgenerative models‚Äù
(see below), respectively.
We deal with self-organizing systems. These are systems that acquire their structure
or perform functions without direct external ‚Äústeering‚Äù. To model self-organization, two
kinds of quantities are considered.
(1) Parameters Œõ = (Œõ1, . . . Œõk) that are Ô¨Åxed externally (some of them are used as
control parameters).
(2) Variables q =
(
q1, . . . qj
)
that describe the dynamic response of the system to Œõ.
In, e.g., our ‚Äúlaser paradigm‚Äù, Œõ may quantify the energy input into the laser crystal,
and q the laser light intensity. All in all, P is a function of Œõ and q,
P = P(q; Œõ).
The two basic problems are:
(1) How to derive P = P(q; Œõ)?
(2) How to utilize the information contained in P = P(q; Œõ)?
To answer (1), we may proceed along two pathways: bottom-up (microscopic ap-
proach) or top-down (macroscopic approach). Our microscopic approach based on the laser
paradigm helps us to answer the Schr√∂dinger‚Äôs ‚Äúwhat is life‚Äù question: The life process is
enabled by an inÔ¨Çux of (free) energy. Note that the inÔ¨Çux rate is the crucial parameter and
not the free energy, per se. The main part of our paper deals with macroscopic approaches
and their interrelations; cf. Figure 4. Thus, we shed light on the derivation of P, as well as
on its use.
In Section 3, we sketch the microscopic theory of an open system. Thus, the calcu-
lated probability distribution P will inspire us to formulate Synergetics 2nd Foundation
in Section 4.7.
Entropy 2021, 23, 707 8 of 27
Entropy 2021, 23, 707 8 of 26 
 
 
 
Figure 4. Pathways to ( ùëû; Œõ). This table shows the logical connection between the various 
approaches. The left column refers to the microscopic theory with its focus on open systems and its 
mathematical tools, i.e. , Langevin and Fokker -Planck equations. The solution of the latter is the 
probability distribution P. The middle part is based on Jaynes‚Äô maximum  (information) entropy 
principle, which contains Feynman‚Äôs principle and Synergetics 2nd Foundation as special cases. 
Both, jointly with Bayesian inference, form a basis of Friston‚Äôs FEP. A common denominator of these 
approaches is the probability P, but now derived in a top-down fashion. 
3. Microscopic Theory 
3.1. Goal 
In this section , we derive  ùëÉ(ùëû; Œõ)  of an open complex system by means of 
microscopic/first principles theory. Our example is laser light [ 21]. For the sake of 
completeness, we mention that the underlying theory belongs to quantum mechanics 
(motion of electrons in atoms), quantum electronic s (the electro-magnetic (light-) field)), 
and the coupling of atoms and field to reservoirs, which act as sources or sinks of energy 
of the individual components, atoms , and light waves ( ‚Äúopen system ‚Äù). To make our 
Figure 4. Pathways to (q; Œõ). This table shows the logical connection between the various approaches. The left column
refers to the microscopic theory with its focus on open systems and its mathematical tools, i.e., Langevin and Fokker-Planck
equations. The solution of the latter is the probability distribution P. The middle part is based on Jaynes‚Äô maximum
(information) entropy principle, which contains Feynman‚Äôs principle and Synergetics 2nd Foundation as special cases. Both,
jointly with Bayesian inference, form a basis of Friston‚Äôs FEP . A common denominator of these approaches is the probability
P, but now derived in a top-down fashion.
3. Microscopic Theory
3.1. Goal
In this section, we derive P(q; Œõ) of an open complex system by means of micro-
scopic/Ô¨Årst principles theory. Our example is laser light [21]. For the sake of completeness,
we mention that the underlying theory belongs to quantum mechanics (motion of electrons
in atoms), quantum electronics (the electro-magnetic (light-) Ô¨Åeld)), and the coupling of
Entropy 2021, 23, 707 9 of 27
atoms and Ô¨Åeld to reservoirs, which act as sources or sinks of energy of the individual
components, atoms, and light waves (‚Äúopen system‚Äù). To make our contribution under-
standable to readers unfamiliar with the strange world of quantum physics, we use a
formulation in terms of classical physics, that can be justiÔ¨Åed by means of the principle of
quantum-classical correspondence. This principle establishes a one-to-one correspondence be-
tween the quantum-mechanical density matrix and the Wigner distribution function so that
all quantum-mechanical results can be translated into classical (‚Äúc-numbers‚Äù) expressions.
Having established this, we sketch the steps that lead us to P(q; Œõ).
3.2. Experimental Set-Up
The heart of a laser (Figure 5) is a rod (of some length L) that contains light-emitting
atoms, e.g., atoms positioned in a crystal, such as ruby (its red color stems from those atoms
(or ions)). The endfaces of the rod are covered by mirrors, one of them semitransparent
so that light can be emitted. The mirrors hold those light-waves that are emitted in axial
direction of the rod, so that it can interact with the atoms over a sufÔ¨Åcient period of time.
The atoms (or ions) are excited by light from lamps surrounding the rod.
3.3. Basic Variables, Parameters and Processes
The crucial variable is the electric Ô¨Åeld strength E(x, t) (perpendicular to the rod‚Äôs
axis) of the eventually produced laser wave at position x along the rod‚Äôs axis at timet. We
decompose E into
E(x, t) = q(t) exp(‚àí2œÄiŒΩt)(hv)1/2 L‚àí1/2 exp(2œÄix/L) (1)
+ Conj. Complex.
q(t) is the variable we are interested in to derive P(q; Œõ)! All other quantities in (1) are
considered Ô¨Åxed (ŒΩ: frequency determined by the atomic transition (cf. below), h: Planck‚Äôs
constant). Note that q is complex, q = q1 + iq2,qj real.
The Ô¨Åeld E, or, equivalently, q, is generated by the light-emitting atoms that act
like miniature antennas, where electric currents oscillate to generate radiowaves. These
‚Äúdipole‚Äù oscillations occur at the Ô¨Åxed frequency ŒΩ as above, but are modulated by a time-
dependent variable Œ±(t) so that we use a decomposition analogous to (1), but without
the x-dependence. We distinguish the atoms by an index ¬µ, so that we consider Œ±¬µ(t),
¬µ = 1, 2, . . . ,N. N total number of atoms.
All these little antennas generate the Ô¨Åeld, i.e., in our representation, q(t), so that
.
q(t) = ‚àíig ‚àë
¬µ
Œ±¬µ(t) . (2)
The dot . means time derivative.
Where g is a coupling constant between Ô¨Åeld and atoms, most important, g is of the
dimension (1/time). The factor i Ô¨Åxes the phase-relation between Œ±¬µ and q.
Now, we have to take a crucial aspect into account. The laser is an open system. In the
case of Equation (2), it means that the laser light is emitted from the rod to the surrounding
(that has a certain temperature). This means that the Ô¨Åeld is coupled to a loss-reservoir. Its
modeling is a formidable task; here, it must sufÔ¨Åce that this coupling has two effects on q:
(1) it gives rise to damping‚Äî Œ∫q,
(2) and to a Ô¨Çuctuating force F(t), where the statical average is (These Ô¨Çuctuating forces
change very quickly and are sometimes referred to as random Ô¨Çuctuations).
‚ü®F(t)‚ü©= 0, ‚ü®F(t)F‚àó(
t‚Ä≤)
‚ü©= Qth Œ¥
(
t ‚àít‚Ä≤)
,
Entropy 2021, 23, 707 10 of 27
where ‚Äúth‚Äù refers to ‚Äúthermal‚Äù, andŒ¥ is Dirac‚Äôs function. Thus, all in all, we arrive at our
Ô¨Årst fundamental equation:
.
q = ‚àíŒ∫q ‚àíig ‚àë
¬µ
Œ±¬µ + F(t) . (3)
Entropy 2021, 23, 707 9 of 26 
 
 
contribution understandable to readers un familiar with the strange world of quantum 
physics, we use a formulation in terms of classical physics, that can be justified by means 
of the principle of quantum-classical correspondence. This principle establishes a one -to-one 
correspondence between the quantum-mechanical density matrix and the Wigner 
distribution function so that all quantum -mechanical results can be translated into 
classical (‚Äúc-numbers‚Äù) expressions. Having established this, we sketch the steps that lead 
us to ùëÉ(ùëû; Œõ). 
3.2. Experimental Set-Up 
The heart of a laser (Figure 5) is a rod (of some length ùêø) that contains light-emitting 
atoms, e.g., atoms positioned in a crystal , such as ruby (its red color stems from those 
atoms (or ions)). The endfaces of the rod are covere d by mirrors, one of them 
semitransparent so that light can be emitted. The mirrors hold those light -waves that are 
emitted in axial direction of the rod, so that it can interact with the atoms over a sufficient 
period of time. The atoms (or ions) are excited by light from lamps surrounding the rod. 
 
Figure 5. The laser paradigm. ( a) Typical setup of a gas laser. A glass tube is filled with gas atoms , and two mirrors are 
mounted at its end faces. The gas atoms are excited by an electric discharge. Through one of the semi -reflecting mirrors, 
the laser light is emitted. ( b) An excited atom emits light wave (signal). ( c) When the light wave hits an excite d atom, it 
Figure 5. The laser paradigm. (a) Typical setup of a gas laser. A glass tube is Ô¨Ålled with gas atoms, and two mirrors are
mounted at its end faces. The gas atoms are excited by an electric discharge. Through one of the semi-reÔ¨Çecting mirrors, the
laser light is emitted. (b) An excited atom emits light wave (signal). (c) When the light wave hits an excited atom, it may
cause the atom to amplify the original light wave. (d) A cascade of amplifying processes. (e) The incoherent superposition
of ampliÔ¨Åed light waves produces still rather irregular light emission (as in a conventional lamp). When sufÔ¨Åciently
many waves are ampliÔ¨Åed, they strongly compete for further energetic supply. That wave that ampliÔ¨Åes fastest wins the
competition initiating laser action. (f) In the laser, the Ô¨Åeld amplitude is represented by a sinusoidal wave with practically
stable amplitude and only small phase Ô¨Çuctuations. The result: a highly ordered, i.e., coherent, light wave is generated.
(g) Illustration of the slaving principle. The Ô¨Åeld acts as an order parameter and prescribes the motion of the electrons in the
atoms. Thus, the motion of the electrons is ‚Äúenslaved‚Äù by the Ô¨Åeld. (h) Illustration of circular causality. On the one hand, the
Ô¨Åeld acting as order parameter enslaves the atoms. On the other hand, the atoms by their stimulated emission generate the
Ô¨Åeld [5] (Figure 3.2).
Entropy 2021, 23, 707 11 of 27
Note that Œ±¬µ is a time-dependent variable, Œ±¬µ(t). Its time-dependence is determined
by an equation that, in analogy to (3), contains three parts:
(a) stemming from the interaction of atom ¬µ with the Ô¨Åeld represented by q(t);
(b) coupling of atom ¬µ to a reservoir leading to damping with a rate constant Œ≥;
(c) and to a Ô¨Çuctuating force Œì¬µ(t), characterized by
‚ü®Œì¬µ(t)‚ü©= 0, ‚ü®Œì¬µ(t)Œì‚àó
¬µ‚Ä≤
(
t‚Ä≤)
‚ü©= Q¬µ Œ¥¬µ¬µ‚Ä≤Œ¥
(
t ‚àít‚Ä≤)
. (4)
Of particular interest is the form of (a).
As is known from electrodynamics, an oscillating electro (- magnetic) Ô¨Åeld gives
rise to an oscillating dipole, which, in our somewhat simpliÔ¨Åed approach, would lead to
an equation: .
Œ±¬µ = igq . (5)
Here, we must take a peculiarity into account that is due to quantum theory.
The laser atoms are quantum systems having discrete energy levels. Here, we consider
atoms with only two levels, the ground state (with label 1) and the excited level 2, and their
corresponding (average) occupation numbers are N1 and N2. Emission of a light wave by
‚Äúour‚Äù atom means that the difference (called inversion)d = N2 ‚àíN1 goes from +1 to ‚àí1,
whereas, in case of absorption, d changes from ‚àí1 to +1. As some discussion shows, this
effect entails that the r.h.s of (5) must be equipped with a factor d¬µ.
Taking (a)‚Äì(c) together, we arrive at our second set of fundamental equations:
.
Œ±¬µ = ‚àíŒ≥Œ±¬µ + igqd ¬µ + Œì¬µ(t) . (6)
Clearly, d¬µ is a time-dependent variable so that we need an equation for it. This equation
has the same structure as (6) and is outlined above (a)-(c). With h: Planck‚Äôs constant,
ŒΩ: frequency, and hŒΩd¬µ is the energy content of atom ¬µ. The following equation for d¬µ (7)
can be interpreted as energy balance equation (provided we multiply both sides by hŒΩ).
The change of energy of atom ¬µ is given by (up to factor hŒΩ)
.
d¬µ = Œ≥1
(
d0 ‚àíd¬µ
)
I
+ 2ig
(
Œ±¬µq‚àó‚àíŒ±‚àó¬µq
)
II
+ Œì¬µd
III
. (7)
(* means: complex conjugated, i ‚Üí‚àíi) .
The term (II) represents the conversion of energy of atom ¬µ into that of the Ô¨Åeld:
generation of laser light, and has a classical analogue: work done by the Ô¨Åeld on a dipole.
(I) is the energy Ô¨Çux from external energy sources that restore a requisite amount of
inversion (~energy of the atom) within a relaxation time T = 1/Œ≥1. (III) is the Ô¨Çuctuations
caused by the reservoir (external energy source).
3.4. Summary of the Basic Laser Equations
Equations (3), (6) and (7) are, in the present context, the basic laser equations. The
variables are q(t), Œ±¬µ(t), d¬µ(t), ¬µ = 1, . . . ,N.
Typically, the numberN of laser atoms is of order 1017 or larger, where each atom by
itself is a complicated (quantum) system.
Equations (3), (6) and (7) describe processes, e.g., of energy transfer as (7), when
multiplied by hŒΩ.
The relevant constants are g, Œ≥1, Œ≥, all of the dimension (1/time). Most important is
the parameter d0 which is regulated by the external energy input and serves as control
parameter‚Äîthat the experimenter can adjust.
Now, having a microscopic theory at hand, how can we derive P(q; Œõ), thus returning
to our goal stated in the beginning of this section?
As Equation (3) reveals, the Ô¨Åeld q (or E) is generated by the collective action of the
atoms (‚àë
¬µ
Œ±¬µ!), i.e., very many microscopic elements, where q(t) is a directly measurable
Entropy 2021, 23, 707 12 of 27
macroscopic quantity. As close analysis of Equations (3), (6) and (7) shows, we are dealing
here with a process of self-organization, where q(t), in the sense of Synergetics, plays the
role of an order parameter, and the numerous variables Œ±¬µ, d¬µ can be eliminated by means
of the slaving principle. In the present context, the details are not relevant so that we quote
the Ô¨Ånal result: .
q = (‚àíŒ∫ + G)q ‚àíCq(q‚àóq) + Ftot . (8)
To achieve laser action, the ‚Äúgain‚Äù rate, G, must be larger than the loss rate Œ∫ (this is a
condition derived by Schawlow and Townes [22]). In our presentation,
G = g2D0/Œ≥, D0 = Nd0 . (9)
C is a ‚Äúsaturation‚Äù constant,
C = 4g4D0/Œ≥2Œ≥1 . (10)
Note that Œ∫, G, C have the dimension (1/time).
The stochastic properties of Ftot (the total Ô¨Çuctuation) are characterized by
‚ü®F‚àótot (t)‚ü©= ‚ü®Ftot (t)‚ü©= 0
‚ü®F‚àótot (t)Ftot
(
t‚Ä≤)
‚ü©= 2Œ∫
(
nth + nsp
)
Œ¥
(
t ‚àít‚Ä≤)
= Qtot Œ¥
(
t ‚àít‚Ä≤)
, (11)
where nth is the number of photons at temperature T, and nsp the number of spontaneously
emitted photons. In what follows, we may treat nth and nsp as Ô¨Åxed parameters.
Equation (8) is of the type of Langevin equation, which describes a process that has:
(a) a deterministic cause: the Ô¨Årst two brackets in (8);
(b) a stochastic cause, Ftot .
Now, we can implement the most important step.
3.5. Derivation of the Probability Distribution P
This is achieved by converting an equation of the Langevin type (cf. (a) and (b) above)
into a Fokker-Planck equation. Just as a reminder and for illustration: let the Langevin
equation for a real variable x(t) be:
.
x = ‚àíŒ≥x + F(t) , (12)
‚ü®F(t)‚ü©= 0, ‚ü®F(t)F
(
t‚Ä≤)
‚ü©= Q Œ¥
(
t ‚àít‚Ä≤)
. (13)
The corresponding Fokker-Planck equation of the time-dependent probability distribution
function P(x; t) reads
.
P = ‚àí‚àÇ
‚àÇx (‚àíŒ≥xP) + 1
2 Q ‚àÇ2
‚àÇx2 P . (14)
Its steady state solution reads
.
P = 0:
.
P(x) = N exp(‚àíŒ≥x2
Q ) . (15)
N: normalization factor.
We skip the Fokker-Planck equation belonging to (8) and immediately write down its
steady state solution, which is the probability distribution P,
P(q) = N exp
( 1
Qtot
{1
2 (G ‚àíŒ∫)(q‚àóq) ‚àí1
4 C(q‚àóq)2
})
. (16)
Qtot is the total Ô¨Çuctuation intensity.
* This distribution function can be linked to the photon distribution function that has been
precisely measured, conÔ¨Årming (16).
Entropy 2021, 23, 707 13 of 27
The constants Qtot , G, C are deÔ¨Åned in (11), (9), (10), respectively, and are all of the
same dimension (1/time) so that the exponent of (16) is dimensionless as it must be.
When, in (16), we multiply numerator and denominator by hŒΩ, the exponent acquires
the form:
energy f lux due to dynamics
energy f lux due to noise .
The importance of our results lies in the fact that it allows the comparison between P of an
open system, away from thermal equilibrium, with that of a system in thermal equilibrium,
e.g., a ferromagnet (see below). Here, P can also be expressed by an exponential function
for which the exponent is written as ratio, but this time as
‚àí 1
kT f ree energy . (17)
k: Boltzmann‚Äôs constant,T: absolute temperature.
Thus, though P (nonequilibrium) and P (equilibrium) have the same mathematical
form, the meaning of the quantities and their dimensions are quite different. As we will
show below, this difference has far-reaching consequences on the application of general
principles, such as Maximum Entropy, Free Energy Minimization, Friston‚Äôs approach to
biology, e.g., action/perception, Feynman‚Äôs Free Energy approach, and so on (see Figure 4).
4. The Free Energy Principle and Its Metamorphosis: From Helmholtz
(Thermodynamics) and Feynman (Statistical Mechanics) to MacKay (Information
Theory) and Friston (Life Sciences)
4.1. Helmholtz Free Energy
The Free Energy Principle has its roots in thermodynamics, the theory of heat. It deals
with relations between measurable quantities of physical objects, e.g., a gas in a volume,
a ferromagnet, a liquid, etc. Typical quantities are energy (e.g., mechanical: kinetic and
potential energy), heat as a form of energy E, temperature T, and less directly accessible to
our senses, entropy, S. The introduction of this quantity is necessitated by our observation
of relaxation processes/irreversibility. When we bring two bodies at different temperatures
in contact, heat Ô¨Çows from the warmer to the colder body until both acquire the same
temperature. The reverse process is never observed. This is reÔ¨Çected by the principle that,
in a closed system, the entropy S can never decrease. S can be measured, and its dimension
is energy/temperature. In a nutshell: the free energy F is deÔ¨Åned by Helmholtz [7] as
F = E ‚àíTS , (18)
where E is the ‚Äúinner energy‚Äù. F is that maximum amount of energy of a physical object
(say gas in a tube) that can be converted into mechanical work, e.g., shifting the piston of
the tube (cylinder) (‚Äúbottle‚Äù), against an external force. According to thermodynamics, F of
a closed system acquires a minimum value.
4.2. Thermodynamics and Information Theory Have the Same Root: Combinatorics and
Large Numbers
Thermodynamics is a phenomenological, macroscopic theory: it does not care what
matter consists of. This changed when Boltzmann [23] took into account that matter, e.g.,
a gas or a piece of metal, is composed of atoms or molecules. To make a long story short:
Planck [24] cast Boltzmann‚Äôs result in the form
S = k ln W , (19)
where S is the entropy, k Boltzmann‚Äôs constant, and W is the number of ‚Äúmicrostates‚Äù
(properly normalized; cf. Reference [24]). For illustration: consider a chain of N molecules
of the sort 1 and 2 with numbers N1, N2, respectively, with N = N1 + N2.
Entropy 2021, 23, 707 14 of 27
Then, there are
W = N!
N1! N2! (20)
different chains. According to Boltzmann [23], such macrostates occur that can be realized
by a maximum of W microstates:
maximum of entropy S (19) !
For practical application of S, we need a handier expression. First, we note that
‚Äúproperly normalized‚Äù means thatW must be divided by the total number of atoms, and
the limit N1, N2, N ‚Üí‚àû must be taken. By use of Stirling‚Äôs formula in the approximation
ln N! ‚âàN(ln N ‚àí1) , (21)
we may cast (19) in the form
S = k(‚àíP1 ln P1 ‚àíP2 ln P2), (22)
Pj = Nj
N f or N ‚Üí‚àû.
Pj is the relative frequency that atom j (=1 or 2) occurs. When there are j different types of
atoms, then (22) generalizes to the well-known formula for Gibbs entropy:
S = ‚àík
j
‚àë
j=1
Pj ln Pj . (23)
Let us turn to the concept/deÔ¨Ånition of information as deÔ¨Åned by Shannon [15,16]. How
many messages per second can be sent through a ‚Äúchannel‚Äù, e.g., a cable? A message is
represented/encoded by a speciÔ¨Åc sequence of symbols, such as dashes and dots of the
Morse alphabet. How many different sentences of N symbols with N1 dashes and N2 dots
can we form? Obviously, W(20). Since the transmission of each symbol takes some time,
the larger the channel capacity is, the larger W‚Ä≤= W/N is. Thus, a measure of Shannon
information depends on W‚Ä≤. In a last step, following Shannon, we require that Shannon
information is deÔ¨Åned by
I = K ln W‚Ä≤, (24)
where K is a constant. In complete analogy to the thermodynamic case, we consider W‚Ä≤
in the limit N ‚Üí‚àû that leads us to (22) with k replaced by K, or, in case of J symbols, to
(23, k ‚ÜíK ). The only difference between Boltzmann and Shannon entropies consists in
the different factors k and K, respectively. To obtain ‚ÄúShannon‚Äù, we choose K such that
K ln becomes log2 so that, Ô¨Ånally,
I = ‚àí
j
‚àë
j=1
Pj log2 Pj . (25)
This entails that I is measured in bits (i.e., number of yes/no decisions). In the following,
we write P(q) instead of Pj when j is replaced by a set of indices (cf. Equation (26)).
4.3. Jaynes‚Äô Maximum (Information) Entropy Principle
This principle allows us to make the best guess on a complex system of which only a
limited set of data is known. In its original form, Jaynes [25,26] applied it to thermodynam-
ics, i.e., systems in thermal equilibrium. We present this approach by means of an explicit
example: the ferromagnet. It is a complex system: on the microscopic level, it is composed
of tiny magnets (‚Äúspins‚Äù with ‚Äúmagnetic moment‚Äù) situated on a three-dimensional lattice.
These magnets may point in only two directions, ‚Äúup‚Äù and ‚Äúdown‚Äù, and have the same
Entropy 2021, 23, 707 15 of 27
size (‚Äúmagnetic moment‚Äù). To describe a microstate, we label the elementary magnets by
l = 1, . . . , N.
N: total number. A speciÔ¨Åc microstate is characterized by a ‚Äústate vector‚Äù:
q = (q1, q2, . . . ,qN ), where ql = +1, or ‚àí1 . (26)
Since the magnets interact with each other, with each conÔ¨Åguration represented by q, an
energy E(q) is connected (For an explicit example, see below, Section 4.5).
Finally, because the ferromagnet has some temperature (it is assumed to be in thermal
equilibrium), its elementary magnets Ô¨Çip randomly between up and down. Thus, the states
q obey a temperature-dependent probability distribution P(q; T). Once we know P, we
can calculate all macroscopic quantities, such as the total energy of the ferromagnet, the
magnetic Ô¨Åeld it produces, etc.
Jaynes‚Äô principle assumes (in accordance with Boltzmann‚Äôs principle) that those
macrostates occur that can be realized by a maximum number of microstates (23). The
number of microstates has been calculated above and appears in Boltzmann‚Äôs or Shannon‚Äôs
Formulas (6) and (8), respectively‚Äîup to different constant factors k, K, or the use of ln
or log2.
While Jaynes used Shannon entropy, thus, referring to information theory, here, we
start right away from the thermodynamic form (6).
The crucial point is that not all conÔ¨Ågurationsq are equally probable but areconstrained
by the condition that they give rise to the macroscopically measured (or measurable)
quantities, such as the total energy. These measurable quantities fj, j = 1, . . ., J, are linked
to P(q):
fj = ‚àë
q
P(q) fj(q) , (27)
where, e.g., f1 is the expression for the energy E(q) of the conÔ¨Åguration q. These considera-
tions lead us in a straightforward way to Jaynes‚Äô principle: to deriveP(q) by maximizing
the number of microstates S, as expressed by (23) under the constraints (27) and the
normalization condition:
‚àë
q
P(q) = 1 . (28)
This task is solved by means of Lagrange multipliers Œõ.
We obtain
P(q) = exp
(
Œõ0 ‚àí‚àë
j
Œõj fj(q)
)
. (29)
For systems in thermal equilibrium, one constraint is always total energy so that, typically,
P(q) = exp (Œõ0) exp(‚àíŒõE(q)) , (30)
which is just the Boltzmann distribution function, so that we can identify
exp Œõ0 = N, N normalization
Œõ = 1/kT, k = Boltzmann‚Ä≤s constant, T = absolute temperature.
The relations (29) or (30) enfold still more important relations.
Inserting (30) in (28) yields
exp(‚àíŒõ0) = ‚àë
q
(‚àíE(q)/kT) ‚â°Z , (31)
where Z is the partition function. Knowing it, we can calculate all thermodynamic quanti-
ties, such as total energy at T, the magnetic Ô¨Åeld of a ferromagnet at T, etc. (e.g., putting
Entropy 2021, 23, 707 16 of 27
Œ≤ = 1/kT, the total energy is given by ‚àí‚àÇ
‚àÇŒ≤ ln Z). When we insert (30) in (23), we Ô¨Ånd,
using (32),
S = ‚àíkŒõ0 + 1
T ‚àë
q
E(q)P(q) , (32)
‚àë
q
E(q)P(q) = ‚ü®E‚ü© (33)
is the energy of the system at temperature T. Rearrangement of (32) yields
kŒõ0 = 1
T ‚ü®E‚ü©‚àíS . (34)
To reveal the meaning of Œõ0, we put, in (34),
Œõ0 = F
kT (35)
and obtain
F = ‚ü®E‚ü©‚àíTS , (36)
that is just the expression of Helmholtz Free Energy! This macroscopic relation is brought
about via a microscopic theory!
As stated after (31), knowing Z, we can calculate important macroscopic quantities.
But, according to (31), Z can be expressed by Œõ0, which, in turn, can be expressed by the
free energy so that, Ô¨Ånally,
F = ‚àíkT ln Z . (37)
This reveals the central role played by F. So, the crucial question arises:
How can we calculate F by means of the microscopic theory? For instance, in the
case of the ferromagnet, this problem turns out to be extremely difÔ¨Åcult; it is here where
Feynman‚Äôs principle comes in.
4.4. Feynman‚Äôs Free Energy Principle
Since, in important cases, the calculation of F by means of an exact microscopic theory
is a ‚Äúhard‚Äù problem, the question arises: how far can we calculate F approximately?
To answer this question, we recall the deÔ¨Ånition of F(1) in terms of the microscopic
expression for E (33) and S (23),
F = ‚àë
q
E(q)P(q) ‚àíkT ‚àë
q
P(q) ln P(q) . (38)
We obtain an approximate expression for F, ÀúF, when, in (38), we replace P(q) by another
probability distribution, Q(q), we can (more) easily handle:
ÀúF = ‚àë
q
E(q)Q(q) ‚àíkT ‚àë
q
Q(q) ln Q(q) . (39)
To get an insight into the quality of our approximation, we derive a suitable expression for
ÀúF ‚àíF. Replacing E(q) in (38), using P(q) = Z‚àí1 exp(‚àíE(q)/kT), we arrive at:
ÀúF ‚àíF = kT Dkl (Q||P) , (40)
where
Dkl (Q||P) = ‚àë
q
Q(q) ln Q(q)
P(q) (41)
is the Kullback-Leibler divergence. It has the crucial property
Dkl ‚â•0, (‚ÄúGibbs inequality‚Äù), (42)
Entropy 2021, 23, 707 17 of 27
= 0 only if P(q) = Q(q) . (43)
This means that any approximate free energy ÀúF is always larger than the true free energy F.
This is the basis of Feynman‚Äôs Free Energy Principle [8].
In practical applications, Q is chosen in form of an explicit function with adjustable
parameters so that ÀúF becomes minimized. Note that the primary goal is to achieve a good
approximation to the true probability distribution P(q).
4.5. An Example: The Ising Model
This is the model of a ferromagnet in a homogeneous magnet Ô¨Åeld. The energy
function can be written in the form:
E(q) = ‚àí1
2 ‚àë
mn
Jmnqm qn ‚àí‚àë
h
hqn . (44)
Each qm can acquire only the value +1 (‚Äúspin up‚Äù) or ‚àí1 (‚Äúspin down‚Äù). Jmn represents
the interaction energy between spins (or elementary magnets) at lattice sites labeled by m
(or n). It is assumed that Jmn = J > 0 for neighboring lattice sites, and = 0 otherwise. To
calculate the approximate free energy (39), Q is chosen as
Q(q) = 1
ZQ
exp
(
‚àë
n
anqn
)
, (45)
i.e., as a product of functions each containing only one variable, qn.
The constants an are variational parameters to be chosen such that ÀúF is minimized.
As a little analysis shows, the expressions for E and S in (39) can be explicitly calculated,
yielding simple expressions of an. Because of the equal role of all spins, an = a is the only
variational parameter.
The exponent of exp in (45) is an example of a typical hypothesis onQ; we will discuss
more examples below. The product entails that each individual spin is subject to a mean
Ô¨Åeld generated by all other spins. This ‚ÄúMean Ô¨Åeld approximation‚Äù ignores correlations
among the individual spins.
4.6. The Free Energy Principle beyond Physics
The FEP owes its designation to Equation (38), which is of the form:
f ree energy = energy ‚àíconst x entropy . (46)
In (39), energy appears as average (33) (‚Äúexpectation value‚Äù of an energy expression, such
as (44)), and entropy as presented in (23) and (25) is, up to the factors k or K, nothing
but a measure for the amount of microstates. The relative weight with which energy and
entropy enter (39) is temperature, T. While maximal entropy requires that all microstates
(‚ÄúconÔ¨Ågurations‚Äù) are realizable and equally important, the energy minimization prefers
some microstates over others (Think of a pearl necklace with an equal number of white
and black pearls. Then, a necklace where all white pearls are linked together may be more
boring (‚Äúless priced‚Äù) than one where the white and black pearls appear in an altering
sequence.). The FE principle seeks a compromise by minimizing (46). This kind of balance
principle holds for many systems, e.g., biological, economic, etc.
Depending on the application, the energy expression may be replaced by the one of
interest, e.g., number of sequelae.
In general, the typical entropy expression (23) without the factor k may be invoked,
but there may be also other ‚Äúcost‚Äù functions.
A Ô¨Årst application of the free energy principle to a purely information theoretical
problem outside the natural sciences is due to MacKay [ 9,14]. This underlines that this
principle is a purely mathematical approach.
Entropy 2021, 23, 707 18 of 27
4.7. Synergetics 2nd Foundation
As we have shown in Section 4.3, Jaynes‚Äô Maximum (Information) Entropy Princi-
ple [25,26] allows a straightforward derivation of fundamental relations of thermodynamics.
This special application rests on the use of energy as constraint, which is assumed to be
a (measured) time-independent, Ô¨Åxed quantity. But this assumption is no more justiÔ¨Åed
when we deal with open systems, which exchange energy, matter, and/or information
with their surroundings. So, the basic question arises of whether Jaynes‚Äô principle can be
applied to open systems, provided we use other constraints than energy. A Ô¨Årst hint comes
from the microscopic theory of an open system in which the prototype is the laser. We have
found an explicit expression for the probability distribution of the laser Ô¨Åeld amplitude q
(16) of Section 2 that is of the type
P(q) = N exp
(
aq2 ‚àíbq4
)
(47)
(here, for simplicity, q: real). Which constraint(s) do we need to derive (47)? As a glance
at the general formulation of Jaynes‚Äô principle (29) of Section 4.3 reveals, there are two
constraints (besides normalization), namely the moments
‚ü®q2‚ü©, ‚ü®q4‚ü©, (48)
where
‚ü®qm‚ü©=
‚à´
P(q)qmdq . (49)
When there are several variables, q1, . . ., qL, involved, the constraints (48) can be general-
ized to polynomials of some order. Most important expressions are correlation functions,
‚ü®ql qk‚ü©, (50)
which can be linked to the variation ŒΩ(ql,qk):
ŒΩ(ql, qk) = ‚ü®qlqk‚ü©‚àí‚ü®ql‚ü©‚ü®qk‚ü©. (51)
All in all, Synergetics 2nd Foundation is an application of Jaynes‚Äô principle where the
constraints are polynomials that appear in the function fj(q) in the exponent of (29) of
Section 4.3. If the polynomials are of Ô¨Årst and second order, (29) becomes a Gaussian
which allows us to evaluate the constraints ‚ü®P(q) f (q)‚ü©explicitly. On the other hand, (29) of
Section 4.3 as Gaussian cannot represent multistable states‚Äîthis requires the inclusion of
4th order terms (see, e.g., Haken and Portugali in Reference [5]), where pattern recognition
is dealt with.
A major task may be the calculation of the Lagrange parameters in (29) of Section 4.3.
While, in the Gaussian case (linear and quadratic constraints), this problem can be reduced
to linear algebra (see, e.g., Haken and Portugali [ 5]), approximation methods must be
invoked in the general case. An explicit example is this: Let the true, but unknown,
probability distribution be given by P(q). We try to approximate it by
ÀúP(q; Œª) = ÀúZ(q; Œª)‚àí1 exp (‚àí‚àë
j
ŒªjVj(q)) , (52)
where, in the present context the functions, Vj are polynomials. To Ô¨Ånd the best Ô¨Åt between
P and ÀúP, we use the method of steepest descent, i.e., we try to lower the Kullback-Leibler
divergence Dkl stepwise. We introduce an iteration parameter that we might interpret as
time t, so that, formally, the steepest descent can be written as
dŒªj(t)
dt = ‚àíŒ≥ gradjDkl (Œª), (53)
Entropy 2021, 23, 707 19 of 27
where Œ≥ is a parameter related to the stepsize.
Inserting P and (52) in Dkl (41) (where Q ‚ÜíP, P ‚ÜíÀúP ) and rearranging terms, we
readily obtain
dŒªj(t)
dt = ‚àíŒ≥
(
‚ü®ÀúVj‚ü©P ‚àí‚ü®ÀúVj‚ü©ÀúP
)
. (54)
In (54), ‚ü®ÀúVj‚ü©ÀúP is the expectation value of Vj that can be measured, while ‚ü®ÀúVj‚ü©P must be
calculated. For the derivation of (54), we need to recall
‚àÇ
‚àÇŒªj
ln ÀúZ = ‚ü®ÀúVj‚ü©P . (55)
4.8. Summary
All the above relations are based on an application of Jaynes‚Äô MIE Principle that may be
considered a consequent mathematization of Boltzmann‚Äôs principle: study all microstates
that are compatible with constraints. There is, however, another source of inspiration of
how to deal with complex systems: Bayesian inference, that we will study next.
5. Bayesian Inference
We consider an example of daily life: a trafÔ¨Åc light for pedestrians with colors red and
green implying ‚Äústop‚Äù and ‚Äúgo‚Äù, respectively. Conventionally, when a pedestrian does not
stop, we infer a ‚Äúgreen light‚Äù. However, a certain (in principle, measurable) percentage of
pedestrians go on in spite of ‚Äúred‚Äù. When we see a pedestrian going, how sure can we be
that the trafÔ¨Åc light was green? To put it quantitively, how large is theprobability that the
light was green?
Another typical problem of neuroscience: a sensory neuron Ô¨Åres when it receives
input from yellow light. But it may also Ô¨Åre when the input stems from green light. We
observe that neuron Ô¨Åring. With which probability does the light stem from a green source?
These are typical questions that can be answered by Bayesian inference that is based
on Bayes‚Äô [11] theorem. This rests on a mathematical tautology. Let the joint probability of
two variables r and s be P(r, s). (r: pedestrian walk/go; s: trafÔ¨Åc light green/red). Then,
we can write
P(r, s) = P(r|s) P(s) , (56)
= P(s|r) P(r) , (57)
where, e.g., P(r|s) is the conditional probability that ‚Äúr‚Äù happens ‚Äúunder the hypothesis‚Äù
that ‚Äús‚Äù happens. Equating the right-hand sides of (56) and (57) and rearranging terms
leads us to
P(s|r) = P(r|s) P(s)
P(r) . (58)
We may eliminate P(r) because of probability theory
P(r) = ‚àë
s
P(r, s) = ‚àë
s
P(r|s)P(s) . (59)
So, that, Ô¨Ånally,
P(s|r) = P(r|s) P(s)
‚àës P(r|s) P(s) . (60)
This formula allows us to answer our above questions.
P(s|r) means: when we observe a speciÔ¨Åc ‚Äúreaction‚Äù r (e.g., walks), what is the
probability of a ‚Äúsignal‚Äùs, e.g., green? The ‚Äúposterior‚Äù P(s|r) can be calculated by means of
the right-hand side of (60), provided we know (have measured) the conditional probability
P(r|s) and prior distribution function P(s).
Entropy 2021, 23, 707 20 of 27
In our examples, r and s can be represented by discrete variables, e.g., 0 and 1.
P(s|r) can be plotted as a matrix with elements Mrs. So far, the formalism holds for any set
of discrete variables. A major extension results when the variables r, s are continuous.
In such a case, a ‚Äúgenerative model‚Äù for the joint probability P(r, s) is required. To get
some insight into such a model, consider a sensory neurons and an action neuron r. r and s
are bidirectionally coupled. s receives inputs from an external sources0. The variables r and
s represent neural activities (e.g., measured in Ô¨Åring rates). In many practical applications
(for computational reasons), the generative model is chosen as a Gaussian. Because of the
relations between r, s, s0, it has the form
P(r, s) = N exp
(
‚àíŒ±r2 ‚àí2Œ≤rs ‚àíŒ≥s2 ‚àíss0
)
, (61)
where Œ±, Œ≤, Œ≥, s0 are parameters that must be Ô¨Åxed by (Ô¨Åtting) experimental data.
To this end, the approach described by Equations (53)‚Äì(55) can be used. In the last
step, the desired posterior can be calculated by means of Bayes‚Äô theorem (60).
5.1. Interlude: How to Find a Generative Model?
Our above formulation of a generative model may seem rather arbitrary, an essential
motivation is the computational advantage offered by the use of Gaussians‚Äîoften known
as the Laplace assumption in Bayesian statistics [ 27]. There is, however, a much more
profound guide to arrive at a (the) generative model, based on Synergetics‚Äô 2nd Foundation.
There we use, in Jaynes‚Äô Maximum Information Principle, moments [25,26] and correlation
functions as constraints.
On the other hand, the exploration of correlations lies at the heart of Bayesian inference.
How does the behavior of pedestrians correlate with trafÔ¨Åc light? How does the activity of
action neurons correlate with that of sensory neurons? According to Section 4.7, Synergetics‚Äô
2nd Foundation, the relevant probability distribution is given by (29), Section 4.3, where
the functions f (q) are polynomials of the variables, e.g., r and s. In this way, we are quite
naturally led to the formulation of a generative model, where the Lagrange parameters are
the free parameters. In many cases, it sufÔ¨Åces to use polynomials of Ô¨Årst and second order,
e.g., (61).
The whole procedure requires that the constraints have to be sufÔ¨Åciently exactly Ô¨Åxed
(sufÔ¨Åciently many observations). We recall that, in the case of Gaussian, the Œõs can be
calculated by linear algebra (cf., e.g., Reference [5]).
5.2. Variational Bayesian Inference
As we have seen above, Bayesian inference allows us, via Formula (60), to calculate
the posterior, P(s|r), in our notation/examples. In practice, an alternative procedure has
turned out to be more ‚Äúeasily‚Äù feasible. In this case, one tries to directly approximateP(s|r)
by means of a test function Q and to check the quality of the approximation. To explain the
procedure, we specialize (3) to the case of speciÔ¨Åc value r = rm so that
P(s|rm) = P(rm|s)P(s)
P(rm) ‚â°P(rm, s)
P(rm) . (62)
In what follows, we consider rm and, thus, P(rm) as Ô¨Åxed parameters. Thus, Q is also a
function of s only (for each parameter value rm).
A quality measure for Q(s) is the Kullback-Leibler divergence
‚àë
s
Q(s) ln Q(s)
P(s|rm) , (63)
which, due to (62), can be written as
(63) = ln P(rm) + ‚àë
s
Q(s) ln Q(s)
P(rm, s) . (64)
Entropy 2021, 23, 707 21 of 27
This means that the Kullback-Leibler divergence between Q(s) and P(s|rm) (63) and the
Kullback-Leibler divergence between Q(s) and P(rm, s) (64) differ only by a constant. This
entails that (63) and
‚àë
s
Q(s) ln Q(s)
P(rm, s) (65)
attain their minima at the same Q(s). Thus,
Qmin (s) = P(s|rm) , (66)
where Qmin minimizes (65).
This is the variational Bayesian inference.
If s is a continuous variable, the sum over s must be replaced by an integral.
For more details, cf. MacKay [ 10], and, for a comparison between the free energy
principle and Bayesian inference, cf. Gottwald and Brown [28].
6. Life as a Steady State
In biology, there are three fundamental questions.
1. How did life originate?
2. How does it evolve?
3. How does it maintain a steady state, be it at the level of phylogenesis or ontogenesis?
1. is still an unsolved mystery.
2. is a decisive answer that has been given by Darwin.
3. is the object of ‚Äúautopoiesis‚Äù, a Ô¨Åeld founded by Maturana and Varela [29]. More
recently, the maintenance of life mainly on the ontological level, or homeostasis in other
words, has been dealt with by Friston from a fundamental perspective based on a Free
Energy Principle.
6.1. Friston‚Äôs Free Energy Principle. An Example
Friston and his coworkers have dealt with his principle in a series of papers [3,30,31].
In the following, we scrutinize an important application of his approach, mainly by means
of ‚Äúhis‚Äù example of a perception-action cycle. At its bottom lies a model of the action of an
animal to maintain its homeostasis. The ‚Äúbrain‚Äù consists of a (group of) sensory neuron
(s) with activity (Ô¨Åring rate) s, and bidirectionally coupled to a (group of) action neuron
(s) with activity r. In this model, the action of the environment on the sensory neuron is
represented by means of a ‚Äúhidden‚Äù variable œà. A sensation s is associated with food. An
action r evokes a response œà of the environment with a conditional probability P(œà|r). This
is achieved best if P(œà|r) matches P(œà|s) best. To this end, we use the Kullback-Leibler
divergence and identify in (63) of Section 5.2
Q(s) ‚ÜíP(œà|r), (67)
P(s|rm) ‚ÜíP(œà|s). (68)
Invoking variational Bayesian inference, we have to minimize (65) of Section 5, where,
in addition to (67), (68), we replace P(s|rm) by P(œà|s). Because of P(œà, s) = P(œà|s)P(s),
Ô¨Ånally, Friston‚Äôs Free Energy expression [3] takes the form
F(r, s) =
‚à´
dœàP(œà|r) ln(P(œà|r)/ P(œà|s))dœà ‚àíln P(s) . (69)
Quite remarkably, none of the probabilities in (69) can be measured directly. The relations
between r, s, œà come to light only with help of a generative model. In our book, Haken
and Portugali, 2021, we used the model
V(r, s, œà) = r2 ‚àí2Œ≤rs + s2 ‚àí2sœà + a(œà ‚àís0)2 (70)
Entropy 2021, 23, 707 22 of 27
as example, where Œ≤, a, and s0 are free parameters. Because of the quadratic form (70), F(69)
can be explicitly calculated. For a ‚â´1, but Ô¨Ånite, we obtain
F(r, s) = Const . + 1
a (s0 + Œ≤r ‚àís)2 +
(
1 ‚àíŒ≤2
)(
s ‚àí s0
1 ‚àíŒ≤2
)2
. (71)
Since F is the sum of two non-negative functions, F acquires its minimum if
(s0 + Œ≤r ‚àís)2 = 0 , (72)
and (
s ‚àí s0
1 ‚àíŒ≤2
)2
= 0 . (73)
To discuss the signiÔ¨Åcance of (72), (73), we recall that s0, Œ≤ are parameters, while s and r
are variables. What does this mean for an animal‚Äôs behavior?
(1) A learning phase: by choosingr and measurings, it can determines0 and Œ≤ (with limits!)
(2) A prediction phase: Having Ô¨Åxed s0, Œ≤ (which happens at the neuronal level), the
animal may predict s when it has selected action r.
Our example explicitly shows that the optimal action strategy to reach the minimum
of F is just to ‚Äúgo downhill‚Äù (steepest decent; also cf. Reference [32]).
dr
dt = ‚àí‚àÇF
‚àÇr . (74)
In a number of practical cases, however, this procedure is not executed. Think of the
marshmallow effect (gratiÔ¨Åcation of children) or of a shift of food uptake by animals in
favor of an expected bigger prey. For a recent experiment with cuttleÔ¨Åsh, cf. Reference [33].
Quite generally, the free energy function can be visualized as a mountainous landscape
with hills and valleys. Ideally, this landscape has a Janus face: either we consider the
parameters Ô¨Åxed, or the variables (measured).
In the learning phase, we consider the variables (technically ‚Äúdata‚Äù) as given, and the
task is to Ô¨Ånd those parameter values that minimize F, i.e., the ‚Äúcoordinates‚Äù of a valley.
This may be, in contrast to our simple example, a demanding task. The free energy principle,
per se, does not provide us with instructions about the sequence of the individual steps.
Humans or animals may start from the assumption of Ô¨Åxed parameters (e.g., s0, Œ≤), choose
a value of r, predict some s, compare it with the actually measured s, and correct the error
made by a change of s0 and/or Œ≤. With improved s0, Œ≤, they may, in a next step, continue.
But the sequence of these steps is beyond the FEP instructions. The selection of the step
sequence may require further criteria (depending on previous experience). Quite generally,
for explicit calculation, an explicit form of the generative model is needed. As long as it is
‚ÄúGaussian‚Äù, all integrals can be expressed via linear algebra, and the determination of the
(Lagrange) parameters be left to the solution of algebraic equations. However, Gaussian
distributions have only one maximum, whereas, in reality, the probability distributions may
be multimodular (multistable states). Such cases have been studied, e.g., in approaches to
pattern recognition (see e.g., Reference [5]). In the context of Friston‚Äôs FEP , multistability
has been studied recently by Da Costa et al. [12].
‚ÄúThe above formulation of free energy minimization is often cast as prediction error
minimization [34‚Äì36]. This is especially true for generative models with additive Gaussian
noise. In this setting, the mechanics we have been describing reduces to predictive coding
or Kalman Ô¨Åltering. The focus on the prediction error minimization‚Äîin the cognitive
neurosciences (and beyond)‚Äîis fully licensed by the general observation that the gradients
of free energy can always be expressed as a prediction error of one form or another, namely
the difference between some sampled sensory data and that predicted under a generative
model. In short, minimizing free energy can be construed as minimizing prediction errors.
In turn, this casts self-organization as the ability of certain systems to destroy the free
Entropy 2021, 23, 707 23 of 27
energy gradients that created them [ 37]. This Synergetics perspective was, in fact, one
of the inspirations for the free energy principle‚Äù (Friston K. (Welcome Trust, London),
personal communication, 2021).
In some papers, such as Da Costa et al. (ibid), instead of the values of single neurons
(Ô¨Åring rates), the mean values of whole neural populations are considered. An early
detailed approach dealing with the dynamics of excitory and inhibitory neurons was
formulated by Wilson and Cowan [38,39]. For later work, cf., e.g., Reference [40].
6.2. The FEP and Synergetics 2nd Foundation: A Comparison
Both approaches use (in a nutshell):
1. a variational principle;
2. a generative model.
(a) In Friston‚Äôs case, the variational principle is FEP , and the formulation of the
generative model is in the hands of the ‚Äúmodeler‚Äù.
(b) In Synergetics 2nd Foundation, the variational principle is Jaynes‚Äô Maximum
(Information) Entropy Principle. The formulation of the generative model
may be reduced to the selection of constraints in the form of moments and
correlation functions, but it can also be chosen freely depending on appropriate
constraints.
When expressed like this, one can see that there is an intimate relationship between
the FEP and Synergetics: in the sense that minimizing free energy is just an expression of
the maximum entropy principle under constraints. In the Synergetics formulation, these
constraints are supplied in the form of Lagrange multipliers that inherit from the physics of
the problem at hand. In the free energy principle, these constraints are expressed in terms
of prior constraints under a Bayesian generative model. In both instances, the imperative is
to provide an accurate account of‚Äîusually sparse‚Äîdata, while maintaining the maximum
entropy of that account (i.e., maximizing the entropy of an approximate posterior density).
One could conceive of the maximum entropy principle as furnishing a unique solution to
an inference or measurement problem in the spirit of Occam‚Äôs principle.
Aside from mathematical details, (a) and (b) differ in their basic preconditions. Namely,
(b) assumes that the collection of data (measured values of variables) is terminated, whereas
(a) deals (at least in principle) with the stepwise data acquisition.
If the generative models of (a) and (b) coincide, in the limiting case of sufÔ¨Åcient data,
the (Lagrange) parameters are expected to acquire the same numerical values. For an
explicit example, cf. Haken and Portugali [5].
Exploration of a Room by Blindfolded Persons
The notion of SIRN (and, thus, SIRNIA‚Äîsee Section 2 above) was originally suggested
as a theory regarding the construction of internal spatial representation (e.g., a cognitive
map) by means of action-perception‚Äîin terms of SIRNIA, by means of the interaction
between internal and external representations. A basic feature of a cognitive map is that
usually the environment concerned (e.g., a maze or a city) is large to the extent that it
cannot be captured by means of a single visual act as in pattern recognition of small-
scale objects (e.g., a face). Consequently, the internal representation (cognitive map) is
constructed in the brain sequentially as the agent moves in, explores the environment,
and accumulates information about it. Movement in the environment is also the basis
of the phenomenon of exploratory behavior that refers to the innate tendency of animals
to perform a highly structured pattern of exploratory movements when introduced to a
new environments [41]. Thus, Yaski, Portugali, and Eilam [42] suggested a link between
the processes of cognitive mapping and exploratory behavior. As part of their attempt to
explore the role of exploratory behavior in the cognitive mapping process, they conducted
a set of experiment with rats [ 43‚Äì45] and one with human subjects [ 42]. In the latter
experiment, 10 blindfolded adult human subjects (5 males and 5 females) were introduced
into an unfamiliar room, where they were asked to move incessantly for 10 min. Observing
Entropy 2021, 23, 707 24 of 27
the locomotor activity of the subjects, Ô¨Åve sequential exploratory behavior patterns were
exposed: (1) ‚Äòlooping‚Äô, (2) ‚Äòwall-following‚Äô, (3) ‚Äòstep-counting‚Äô, (4) ‚Äòcross-cutting‚Äô, and,
Ô¨Ånally, (5) ‚Äòfree traveling‚Äô. Figure 6 illustrates a typical such process performed by one of
the 10 subjects. Similar exploratory behavior patterns were performed by all subjects. After
the experiment, the subjects Ô¨Ålled a questionnaire from which it was found that all 10 have
correctly described the environment as rectangular, 4 have correctly estimated the size of
the room, 8 indicated wall-following as their dominant method, and all 10 reported that
the layout of the environment was apparent to them only by the end of the test.
Entropy 2021, 23, 707 23 of 26 
 
 
the maximum entropy principle under constraints. In the Synergetics formulation, these 
constraints are supplied in the form of Lagrange multipliers that inherit from the physics 
of the problem at hand. In the free energy principle , these constraints are expressed in 
terms of prior constraints under a Bayesian generative model. In both instances , the 
imperative is to provide an accurate account of‚Äîusually sparse‚Äîdata, while maintaining 
the maximum entropy of that account (i.e., maximi zing the entropy of an approximate 
posterior density). One could conceive of the maximum entropy principle as furnishing a 
unique solution to an inference or measurement problem in the spirit of Occam ‚Äôs 
principle. 
Aside from mathematical details, (a) and (b) differ in thei r basic preconditions. 
Namely, (b) assumes that the collection of data (measured values of variables) is 
terminated, whereas (a) deals (at least in principle) with the stepwise data acquisition. 
If the generative models of (a) and (b) coincide, in the limiting case of sufficient data, 
the (Lagrange) parameters are expected to acquire the same numerical values. For an 
explicit example, cf. Haken and Portugali [5]. 
Exploration of a Room by Blindfolded Persons 
The notion of SIRN (and , thus, SIRNIA‚Äîsee Section 2 above) was originally 
suggested as a theory regarding the construction of internal spatial representation (e.g., a 
cognitive map) by means of action -perception‚Äîin terms of SIRNIA, by means of the 
interaction between internal and external representations.  A basic feature of a cognitive 
map is that usually the environment concerned (e.g., a maze or a city) is large to the extent 
that it cannot be captured by means of a single visual act as in pattern recognition of small-
scale objects (e.g. , a face). Consequently, the internal representation (cognitive map) is 
constructed in the brain sequentially as the agent moves in, explores the environment, and 
accumulates information about it. Movement in the environment is also the basis of the 
phenomenon of exploratory behavior that refers to the innate tendency of animals to 
perform a highly structured pattern of exploratory movements when introduced to a new 
environments [41]. Thus, Yaski, Portugali, and Eilam [ 42] suggested a link between the 
processes of cognitive mapping and exploratory behavior. As part of their attempt to 
explore the role of exploratory behavior in the cognitive mapping process, they conducted 
a set of experiment with rats [43‚Äì45] and one with human subjects [ 42]. In the latter 
experiment, 10 blindfolded adult human subjects (5 males and 5 females) were introduced 
into an unfamiliar room, where they were asked to move incessantly for 10 min. 
Observing the locomotor activity of the subjects, five  sequential exploratory behavior 
patterns were exposed: (1) ‚Äòlooping‚Äô , (2) ‚Äòwall-following‚Äô, (3) ‚Äòstep -counting‚Äô, (4) ‚Äòcross-
cutting‚Äô, and , finally, (5) ‚Äòfree traveling‚Äô. Figure 6 illustrates a typical such process 
performed by one of the 10 subjects. Simi lar exploratory behavior patterns were 
performed by all subjects. After the experiment, the subjects filled a questionnaire from 
which it was found that all 10 have correctly described the environment as rectangular, 4 
have correctly estimated the size of the room, 8 indicated wall-following as their dominant 
method, and all 10 reported that the layout of the environment was apparent to them only 
by the end of the test. 
 
Figure 6. Four cumulative stages in the exploratory behavior of a blindfolded female subject in an
unfamiliar environment. Four exploratory behaviors are represented from left to right. The end
time of each phase is depicted at the bottom of each plot. The start point of testing is marked by the
Ô¨Ålled square. As can be seen, this subject started by progressing counterclockwise closing a loop (A).
During this travel, she arrived twice at the room‚Äôs wall. Next, shefollowed the walls, encompassing
the entire room (B). Then, she performed a cross-cut (C) and, Ô¨Ånally, traveled freely across the test
room (D). Source: Reference [42], Figure 1.
We introduce the above problem‚Äîof self-organization through exploration‚Äînot to
provide any particular solutions but to highlight outstanding challenges for the three
convergent formulations of sentient behavior considered in this synthesis. The problems of
exploring the world and resolving uncertainty‚Äîabout the exogenous forces and Ô¨Çuctua-
tions encountered‚Äîis an inherent part of any self-organized behavior, ranging from how
we explore our environment in the visual domain, with saccadic eye movements, through
to physical exploration and learning about what would happen if I did that [46]. These are
challenging and deep issues that should, at some level, be resolved from a synergetic and
free energy principle perspective.
In neurobiology, some progress has been made in terms of minimizing the free energy
expected following an action on the world. The functional form of the free energy functional
means that, when data are treated as a random variable (from the future), free energy mini-
mization entails a resolution of uncertainty or information gain [47]. This may address the
exploratory and epistemic aspect of self-organization. In terms of scaling up this kind of for-
mulation to the level considered by SIRNIA, provisional work using variational free energy
has started to address the exchange of an agent with the environment‚Äìand indeed other
agents. This usually rests upon some form of niche construction and joint minimization of
free energy‚Äîin the sense that interacting agents (or interactions between a phenotype and
her environment) can be seen as a mutual destruction of free energy gradients as agents
(and environments) self-organize to learn about each other, e.g., References [48‚Äì50]. We
will not pursue this here but suggest that the formal similarities between the different
approaches addressed in this article mean that the synergetic second foundation and free
energy minimization should underwrite the scale free aspects of self-organization among
conspeciÔ¨Åcs, communities, and ensembles at any scale.
Entropy 2021, 23, 707 25 of 27
7. Conclusions and Outlook
Homeostasis requires the internal and external regulation of all needed life processes
to guarantee the maintenance of order in contrast to thermodynamics with its second law
of increasing entropy. An answer to this puzzle was given by Schr√∂dinger: a biological
system is an open system whose structure and function is maintained by free energy‚Äîor,
more precisely, as outlined above, by an inÔ¨Çux of free energy.
The requirement of homeostasis entails speciÔ¨Åc kinds of animal behavior, e.g., acqui-
sition of food. Friston‚Äôs FEP may provide a general principle for homeostasis provided
the quantity ‚Äúfree energy‚Äù is replaced by other quantities, such as, e.g., in brain theory, the
Ô¨Åring rate of a group of neurons.
But is homeostasis the only characteristics of life? In fact, there is another class of life
processes, namely, on the ontological level, the development of an individual, and on the
phylogenetic level the evolution of species.
Here, we draw attention to the pronounced transitions in the development. These may
be dramatic changes of structure (morphogenesis) and/or of function/behavior. To cope
with morphogenesis, there are models on the formation of patterns on furs, on sea shells,
segment formation of insects, etc. Pattern formation is also observed in the inanimate world,
such as in Ô¨Çuids (think of the B √©nard convection instability) and in chemical reactions
(e.g., ‚ÄúBelousov‚ÄìZhabotinsky‚Äù). In physics, chemistry, and biology, the basic explanation
of these processes can be traced back to Turing‚Äôs [51] model, which was further developed
by Prigogine and Nicolis [52] by the formulation of diffusion-reaction equations.
A systematic, uniÔ¨Åed approach to the solution of these equations has been developed
by Synergetics 1st Foundation based on the concept of order parameters and the slaving
principle. In this way, hierarchies of structures have been dealt with, e.g., in lasers and Ô¨Çuids.
When a speciÔ¨Åc control parameter C (e.g., energy inÔ¨Çux) is increased more and more, at
speciÔ¨Åc values of C, a structure is replaced by a new one. The focus of Synergetics 1st
Foundation was in the formation of dynamic structures: they collapse when the inÔ¨Çux of
energy and/or matter is stopped. Thus, these systems cannot establish solid structures nor
may they process some kind of memory. In physics, solidiÔ¨Åcation, such as crystal growth,
growth of dendrites, etc., have been modeled.
We are confronted with the requirement of a theory that deals with biological phase
transitions, such as from a caterpillar to a butterÔ¨Çy, to mention a striking example. Such
a theory must consider the interplay between dynamic and solid structures taking into
account the fundamental role of information, and show how this process is governed by
principles of self-organization/Synergetics.
Author Contributions: The two authors have equally contributed to the conceptual parts; H.H. has
developed the mathematical parts. All authors have read and agreed to the published version of
the manuscript.
Funding: This research received no external funding.
Data Availability Statement: Not relevant.
Acknowledgments: The authors thank a reviewer for substantial suggestions improving our manuscript.
ConÔ¨Çicts of Interest: The authors declare no conÔ¨Çict of interest.
References
1. Haken, H.; Portugali, J. Information and Selforganization: A Unifying Approach and Applications. Entropy 2016, 18, 197.
[CrossRef]
2. Haken, H.; Portugali, J. Information Adaptation. The Interplay between Shannon and Semantic Information in Cognition ; Springer:
Berlin/Heidelberg, Germany, 2015.
3. Friston, K. The free-energy principle: A uniÔ¨Åed brain theory? Nat. Rev. Neurosci. 2010, 11, 127‚Äì138. [CrossRef]
4. Schr√∂dinger, E. What is Life? Cambridge Univ Press: Cambridge, UK, 1944.
5. Haken, H.; Portugali, J. Synergetic Cities: Information, Steady State and Phase Transition: Implications to Urban Scaling, Smart Cities and
Planning; Springer: Berlin/Heidelberg, Germany, 2021.
6. Haken, H. Synergetics. Introduction and Advanced Topics; Springer: Berlin/Heidelberg, Germany; New York, NY, USA, 2004.
Entropy 2021, 23, 707 26 of 27
7. Helmholtz, H. Physical Memoirs, Selected and Translated from Foreign Sources; Kessinger Publishing, LLC: WhiteÔ¨Åsh, MT, USA, 1882.
8. Feynman, R.P .; Lebowitz, J.L. Statistical Mechanics: A Set of Lectures; W. A. Benjamin: Reading, MA, USA, 1972.
9. MacKay, D.J.C. Free-energy minimization algorithm for decoding and cryptoanalysis. Electron. Lett. 1995, 31, 445‚Äì447. [CrossRef]
10. MacKay, D.J.C. Information Theory, Inference & Learning Algorithms; Cambridge University Press: Cambridge, UK, 2002.
11. Bayes, T. An Essay towards Solving a Problem in the Doctrine of Chances ; Prince, Ed.; Philosophical Transactions Royal Society:
London, UK, 1763; Volume 53, pp. 370‚Äì418.
12. Da Costa, L.; Parr, T.; Sengupta, B.; Friston, K. Neural Dynamics under Active Inference: Plausibility and EfÔ¨Åciency of Information
Processing. Entropy 2021, 23, 454. [CrossRef]
13. Haken, H. Synergetics‚ÄîAn Introduction ; Springer: Berlin/Heidelberg, Germany; New York, NY, USA, 1977.
14. Haken, G. Information and Self-Organization: A Macroscopic Approach to Complex Systems , 3rd ed.; enlarged ed.; Springer:
Berlin/Heidelberg, Germany, 2006.
15. Shannon, C.E. A Mathematical Theory of Communication. Bell Syst. Tech. J. 1948, 27, 379‚Äì423. [CrossRef]
16. Shannon, C.E.; Weaver, W. The Mathematical Theory of Communication; University of Illinois Press: Urbana, IL, USA, 1949.
17. Portugali, J. Complexity, Cognition and the City; Springer: Berlin/Heidelberg, Germany, 2011.
18. Portugali, J. What makes cities complex? In Complexity, Cognition Urban Planning and Design; Portugali, J., Stolk, E., Eds.; Springer:
Berlin/Heidelberg, Germany, 2016.
19. Brillouin, L. The Negentropy Principle of Information. J. Appl. Phys. 1953, 24, 1152‚Äì1163. [CrossRef]
20. Ramstead, M.J.D.; Badcock, P .J.; Friston, K.J. Answering Schr√∂dinger‚Äôs question: A free-energy formulation.Phys. Life Rev. 2018,
24, 1‚Äì16. [CrossRef]
21. Haken, H. Light Vol. 2. Laser Light Dynamics; Elsevier Science: Amsterdam, The Netherlands; New York, NY, USA, 1985.
22. Schawlow, A.L.; Townes, C.H. Optical maser. Phys. Rev. 1958, 112, 1940. [CrossRef]
23. Boltzman, S. √ºber die Beziehung zwischen dem Zweiten Hauptsatze der mechanischen W√§rmetheorie und der Wahrschein-
lichkeitsrechnung resp. den S√§tzen √ºber das W√§rmegleichgewicht, Sitzungsber. Akad. Wiss. Wien II 1877, 76, 428.
24. Planck, M. The Genesis and Present State of Development of the Quantum Theory. Nobel Lecture. 1920. Available online:
https://www.nobelprize.org/prizes/physics/1918/planck/lecture/ (accessed on 29 May 2021).
25. Jaynes, E.T. Information theory and statistical mechanics I. Phys. Rev. 1957, 106, 620‚Äì630. [CrossRef]
26. Jaynes, E.T. Information theory and statistical mechanics II. Phys. Rev. 1957, 108, 171‚Äì190. [CrossRef]
27. Friston, K.; J√©r√©mie, M.; Trujillo-Barreto, N.; Ashburner, J.; Penny, W. Variational free energy and the Laplace ap-proximation.
NeuroImage 2007, 34, 220‚Äì234. [CrossRef] [PubMed]
28. Gottwald, S.; Braun, D.A. The two kinds of free energy and the Bayesian revolution. PLoS Comput. Biol. 2020, 16. [CrossRef]
29. Maturana, H.R.; Varela, F.J. Autopoiesis and cognition: The realization of the living. In Boston Studies in the Philosophy and History
of Science, 1st ed.; Reidel: Dordrecht, The Netherlands, 1972.
30. Friston, K.; Mattout, J.; Kilner, J. Action understanding and active inference. Biol. Cybern. 2011, 104, 137‚Äì160. [CrossRef]
31. Friston, K.J.; Rosch, R.; Parr, T.; Price, C.; Bowman, H. Deep temporal models and active inference. Neurosci. Biobehav. Rev. 2017,
77, 388‚Äì402. [CrossRef] [PubMed]
32. Parr, T.; Friston, K.J. Generalized free energy and active inference. Biol. Cybern. 2019, 113, 495‚Äì513. [CrossRef]
33. Schnell, A.K.; Boeckle, M.; Rivera, M.; Clayton, N.S.; Hanlon, R.T. CuttleÔ¨Åsh exert self-control in a delay of gratiÔ¨Åcation task. Proc.
R. Soc. B Biol. Sci. 2021, 288, 20203161. [CrossRef] [PubMed]
34. Rao, R.P .; Ballard, D.H. Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-Ô¨Åeld
effects. Nat. Neurosci. 1999, 2, 79‚Äì87. [CrossRef]
35. Clark, A. Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behav. Brain Sci. 2013, 36, 181‚Äì204.
[CrossRef]
36. Hohwy, J. The Predictive Mind; Oxford University Press: Oxford, UK, 2013.
37. Tschacher, W.; Haken, H. Intentionality in non-equilibrium systems? The functional aspects of self-organised pattern formation.
New Ideas Psychol. 2007, 25, 1‚Äì15. [CrossRef]
38. Wilson, H.R.; Cowan, J.D. Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons. Biophys. J. 1972, 12,
1‚Äì24. [CrossRef]
39. Wilson, H.R.; Cowan, J.D. A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue. Biol. Cybern.
1973, 13, 55‚Äì80. [CrossRef]
40. Jirsa, V .K.; Haken, H. A derivation of a macroscopic Ô¨Åeld theory of the brain from the quasi-microscopic neural dynamics.Phys.
D Nonlinear Phenom. 1997, 4, 503‚Äì526. [CrossRef]
41. Golani, I.; KafkaÔ¨Å, N.; Drai, D. Phenotyping stereotypic behaviour: Collective variables, range of variation and predictability.
Appl. Anim. Behav. Sci. 1999, 65, 191‚Äì220. [CrossRef]
42. Yaski, O.; Portugali, J.; Eilam, D. The dynamic process of cognitive mapping in the absence of visual cues: Human data compared
with animal studies. J. Exp. Biol. 2009, 212, 2619‚Äì2626. [CrossRef] [PubMed]
43. Yaski, O.; Portugali, J.; Eilam, D. City rats: Insight from rat spatial behavior into human cognition in urban environments. Anim.
Cogn. 2011, 14, 655‚Äì663. [CrossRef]
44. Yaski, O.; Portugali, J.; Eilam, D. Arena geometry and path shape: When rats travel in straight or in circuitous paths? Behav. Brain
Res. 2011, 225, 449‚Äì454. [CrossRef]
Entropy 2021, 23, 707 27 of 27
45. Yaski, O.; Portugali, J.; Eilam, D. Traveling in the dark: The legibility of a regular and predictable structure of the en-vironment
extends beyond its borders. Behav. Brain Res. 2012, 229, 74‚Äì81. [CrossRef] [PubMed]
46. Schmidhuber, J. Curious model-building control systems. In Proceedings of the International Joint Conference on Neural
Networks, Singapore, 18‚Äì21 November 1991.
47. Friston, K.J.; Fitzgerald, T.H.B.; Rigoli, F.; Schwartenbeck, P .; Pezzulo, G. Active Inference: A Process Theory.Neural Comput. 2017,
29, 1‚Äì49. [CrossRef]
48. Bruineberg, J.; Rietveld, E.; Parr, T.; van Maanen, L.; Friston, K.J. Free-energy minimization in joint agent-environment systems:
A niche construction perspective. J. Theor. Biol. 2018, 455, 161‚Äì178. [CrossRef]
49. Constant, A.; Ramstead, M.J.D.; Veissiere, S.P .L.; Campbell, J.O.; Friston, K.J. A variational approach to niche construction.J. R.
Soc. Interface 2018, 15. [CrossRef] [PubMed]
50. Kuchling, F.; Friston, K.; Georgiev, G.; Levin, M. Morphogenesis as Bayesian inference: A variational approach to pattern
formation and control in complex biological systems. Phys. Life Rev. 2019, 33, 88‚Äì108. [CrossRef] [PubMed]
51. Turing, A. The chemical basis of morphogenesis. Philos. Trans. R. Soc. Lond. B 1952, 237, 37‚Äì72.
52. Prigogine, I.; Nicolis, G. On Symmetry-Breaking Instabilities in Dissipative Systems. J. Chem. Phys. 1967, 46, 3542. [CrossRef]