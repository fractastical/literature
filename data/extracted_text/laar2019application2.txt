Application of the Free Energy Principle to
Estimation and Control
Thijs van de Laar∗, Ay¸ ca¨Oz¸ celikkale†, and Henk Wymeersch‡
∗Dept. of Electrical Engineering, Eindhoven University of
Technology, The Netherlands
†Dept. of Electrical Engineering, Uppsala University, Sweden
‡Dept. of Electrical Engineering, Chalmers University of
Technology, Sweden
October 21, 2020
Abstract
Based on a generative model (GM) and beliefs over hidden states, the
free energy principle (FEP) enables an agent to sense and act by minimiz-
ing a free energy bound on Bayesian surprise. Inclusion of prior beliefs
in the GM about desired states leads to active inference (ActInf). In this
work, we aim to reveal connections between ActInf and stochastic optimal
control. We reveal that, in contrast to standard cost and constraint-based
solutions, ActInf gives rise to a minimization problem that includes both
an information-theoretic surprise term and a model-predictive control cost
term. We further show under which conditions both methodologies yield
the same solution for estimation and control. For a case with linear Gaus-
sian dynamics and a quadratic cost, we illustrate the performance of Act-
Inf under varying system parameters and compare to classical solutions
for estimation and control.
Index terms— Active Inference, Stochastic Optimal Control, Message Pass-
ing, Factor Graphs
∗TvdL acknowledges the support from GN Hearing A/S and the Netherlands Organization
for Scientiﬁc Research, project number 13925.
†A ¨O acknowledges the support from the Swedish Research Council, Grant 2015-04011.
‡HW acknowledges the support from the Swedish Research Council, Grant 2018-03701.
The authors thank Themistoklis Charalambous for valuable comments and Magnus Koudahl
for the stimulating discussions.
1
arXiv:1910.09823v3  [eess.SY]  20 Oct 2020
1 Introduction
Bayesian graphical models (BGMs) constitute an important family of tools in
signal processing, as they allow learning of models as well as inference of hidden
states in a uniﬁed way, often with low complexity. BGMs have been widely
applied for a wide variety of estimation and detection problems in signal pro-
cessing [1,2], with applications that include sensor networks [3], surveillance [4],
and information-seeking control [5]. They also naturally unify several stan-
dard methods from statistical estimation theory, such as the forward-backward
algorithm [6], the Kalman ﬁlter [7], the particle ﬁlter [8], and the Viterbi algo-
rithm [7].
Beyond learning, estimation and detection, BGMs have also found applica-
tions in stochastic control problems, which involve not only estimation of the
state of a system, but also determination of suitable control actions in the pres-
ence of uncertainty. Applications include control of vehicles, robots, factories, or
teams of agents. Often, the control problem and inference/estimation problem
are considered separate, whereby the controller assumes an estimate of the state
and the inference occurs without knowledge of future control. Such a separation
principle is only valid in certain cases, such as the celebrated linear quadratic
Gaussian (LQG) control [9]. Over the past decades, several approaches have
been proposed to unify inference and control [10–16], largely based on BGMs.
The core ideas of these approaches can already be found in the early work [10],
which posed the role of a controller as follows: “A controller of a stochastic
system ‘shapes’ the joint pdf describing the closed-loop behaviour. The ‘optimal’
controller should make this joint pdf as close as possible to a desired pdf.” With
this in mind, [10] poses an ideal state distribution and control distribution, after
which an optimized controller can be found be minimizing a Kullback-Leibler
divergence (KLD). In terms of mathematical tractability, an important improve-
ment was the use of Bayesian graphical models [2], which led to the methods
in [12, 13, 16]. In [12], a similar idea as [10] was proposed, which allowed the
formulation of control cost as a KLD, which could be solved by approximate
inference on the corresponding graphical model. In [13], a linear transforma-
tion of the control cost function is replaced by a log-likelihood function and an
optimized controller is found by an expectation-maximization procedure over
the corresponding factor graph. A similar idea was introduced in [14,15] where
an artiﬁcial observation and associated likelihood was introduced so that state
trajectories with highest posterior probability also have lowest associated con-
trol cost. In [13, 15] controllers similar to LQG were found. In [16] the LQG
control problem was targeted speciﬁcally, and under perfect knowledge of the
current state, the exact LQG controller was recovered by an inference-based
controller. It should be noted that some of the above works aim to ﬁnd a policy
(i.e. a mapping from state estimate to control) while others aim to determine
an optimal control sequence.
More generally, stochastic optimal control problems have been solved using
a diverse range of approaches, where model-predictive control (MPC) [17] and
reinforcement learning (RL) [18] are arguably the most prominent approaches.
2
When a model of the system is available, the control problem becomes a Markov
decision process, which can, in principle, be solved through dynamic program-
ming [19]. If no model is available, RL can provide model-free solutions that
learn state-action mappings from interactions with the system [20]. Recently,
there has been work combining these two approaches, originating either from
the control theory community [21] or the computer science community [22].
In addition to MPC and RL, a third and more recent path is the free en-
ergy principle (FEP) , which originates from cognitive neuroscience as a way to
explain biological behavior [23, 24]. The main hypothesis is that agents (i) in-
ternalize a generative model (GM) of the system, and (ii) perceive and act in
such a way as to minimize a free-energy bound on surprise relative to the GM.
Interestingly, free-energy minimization is a concept that is also used in RL to
encourage exploration and model building [25]. Objective functions for any kind
of system or application can be included in the GM in the form of a goal prior,
which results in formulations of active inference (ActInf) [26]. Despite a large
number of publications in the ActInf ﬁeld including applications in robotics [27]
and synthesis with reinforcement learning [28], there have been only few eﬀorts,
e.g., [16,29–33], to apply it to more traditional stochastic control settings, such
as linear quadratic Gaussian (LQG) control.
In this paper, our main aim is to reveal the connections between inference
and control over BGM from an ActInf perspective. Speciﬁcally, we have the
following contributions:
1. We propose an ActInf joint inference and control formulation that casts
the control problem as an inference problem and explicitly encodes the
control cost in the FEP framework;
2. We show under which conditions the ActInf-based joint inference and con-
trol method yields the solution to the original stochastic optimal control
problem;
3. We prove that LQG can be expressed as a special case of the proposed
ActInf joint inference and control method.
The article is structured as follows: In the remainder of this section, we
provide an overview of the notation. Sec. 2 introduces the model and optimal
control objective. Sec. 3 introduces the ActInf objective and further notation
related to probabilistic model formulations. Sec. 4 formally relates the objective
function of ActInf with stochastic optimal control. Sec. 5 then applies these
formulations to a LQG control problem, which is illustrated by the numerical
results in Sec. 6. We conclude with Sec. 7.
Notation
We write a sequence of variables as st1:t2 = {st1 ,...,s t2 }. At any current time
t, we consider a sequence of states, observations and controls as x = x0:t+T,
y = y1:t+T, u= u0:t+T−1, with xk ∈Rnx , controls uk ∈Rnu , and observations
3
Table 1: Common notations for distributions and functions.
Short Description Normalized
p joint distribution yes
pt generative model at time t yes
˜p goal priors yes
ft goal-constrained generative model no
pp posterior distribution of hidden variables yes
q belief / variational posterior yes
πt stochastic policy mapping yes
yk ∈Rny respectively, with a time horizon of T time-steps into the future. Note
that the start and the end points between the state, observation and control
sequence diﬀer slightly. When explicitly required, we denote the realizations of
the random variables, such as (past) observed values, estimates and performed
actions, by a bold script.
In order to easily distinguish between the past and future variables, we
adopt the following convention: we divide the observationsyinto past (including
present) variables yt = y1:t and future variables yt = yt+1:t+T. Similarly, the
state sequence x consists of xt = x0:t and xt = xt+1:t+T. The control sequence
u consists of ut = u0:t−1 and ut = ut:t+T−1 (with present control included).
For notational convenience, we drop the dependence on the current time t. For
instance, we use xinstead of xt. We use s\t to denote the sequence obtained by
removing st from s.
Similar to the notation for the sequences, some of the probability density
functions (pdfs) are expressed using the notation p(.) and pt(.) to emphasize
functions of past and future variables, respectively. As usual, marginal and
conditional pdfs associated with a given joint pdf are denoted using the same
letter/subscript. For instance, the marginal obtained by marginalizing (i.e., inte-
grating) pa(s1,s2) over s2 is denoted by pa(s1) =
∫
pa(s1,s2) ds2. To avoid clut-
ter, we drop the distribution arguments (i.e., we write pa instead of pa(s1,s2))
whenever these dependencies are clear from the context.
2 System Model
2.1 Dynamical System Model
We consider the following dynamical system with the state-space model (SSM):
xt+1 ∼p(xt+1|xt,ut), t ≥0 , (1a)
yt ∼p(yt|xt), t ≥1 , (1b)
where x0 ∼p(x0) and u0 = 0. Using the system deﬁnition in (1), the prob-
abilistic system model for the state and outcome sequence for a given control
4
sequence over a time window of k∈[0,t + T] can be expressed as follows
p(y,x|u) = p(x0)
t+T−1∏
k=0
p(yk+1|xk+1)p(xk+1|xk,uk). (2)
At time t, we have the probabilistic system model
pt(y,x|u) =
p(yt = yt,yt,x|ut = ut,ut)∫
p(yt = yt,yt,x|ut = ut,ut) dytdx, (3)
where yt and ut are set to their realizations (yt and ut). We will generally omit
the explicit dependence on yt and ut and instead rely on the sub-script t to
indicate that past controls and observations are ﬁxed in pt. Since pt is obtained
by plugging in the values of the realizations, we re-normalize. An overview of
the common distributions used in this paper together with their normalization
status is provided in Table 1.
2.2 Control Objective
We consider stochastic policy mappings in the form of πk(uk|y1:t) from the set
of measurements (up to the current time t) to the control at time k where
k ∈[t,t + T]. The objective is to ﬁnd the mappings πk that minimize the
expected cost Jt over current and future states xk and controls uk, deﬁned as:
Jt =
t+T∑
k=t
Ept,πk [ℓk(xk,uk)] , (4)
where pt is the probabilistic system model as expressed in (3), andℓk(xk,uk) ≥0
is the cost function at time-step k that encodes the cost of being in state xk
and applying the control uk. The realization for the current control (action) is
then determined using the stochastic policy mapping πt.
In particular, the control is ut = u∗
t,π, where
u∗
t,π = g(π∗
t) (5)
with
π∗
t = arg min
πt
Jt (6)
and where g(·) represents the mapping from the probability distribution to a
single action ut, which can be chosen, for instance, as the mean or the mode of
π∗
t or as a sample (i.e. realization) from π∗
t [18,30,34,35]. This article considers
a sliding horizon, i.e., after taking the action at the current time instant t and
obtaining the next observation, the stochastic policies are again determined by
looking T steps ahead.
Example: A typical cost function is the quadratic cost:
ℓk(xk,uk) = ℓ(xk,uk) = 1
2 xT
kQxk + 1
2 uT
kRuk, (7)
for Q∈Rnx×nx ,Q ⪰0 and R∈Rnu×nu ,R ≻0.
5
3 Active Inference
In this section, we describe the ActInf approach and the FEP. The concepts
and approaches in this section have similarities to the control as inference lit-
erature [12, 13, 16], but are here presented from the ActInf perspective. The
main idea of ActInf is that at each time t, the controller minimizes the free
energy functional Ft[q], deﬁned as [23] Ft[q] = D[ q||ft], where D[ q||ft] is the
Kullback-Leibler divergence, q represents a variational distribution (the opti-
mization variable) and ft represents the known generative model pt with sub-
stituted observations or with modiﬁcations with more general constraints. Each
of these concepts will now be explained in detail. Minimization of the free en-
ergy, and the well-established framework of minimization of Bayesian surprise
are closely connected. We further discuss this relationship in Remark 1.
3.1 Generative Model and Goal Priors
3.1.1 The Generative Model
The notation introduced earlier allows us to concisely write the system model
at time t (3) in terms of a past and a future contribution:
pt(y,x|u) = pt(y,y,x,x|u,u), (8a)
= pt(y,x|u,u) pt(y,x|y,x,u,u), (8b)
= pt(y,x|u) pt(y,x|xt,u) . (8c)
We note that the ﬁrst factor depends on past controls and the second on the
future controls. Both factors condition on the controls, and pt does not incor-
porate the control cost Jt.
3.1.2 Goal Priors
The designer of the agent should govern the system behaviour towards desirable
system states, e.g. the exit of a maze. In order to achieve this, ActInf introduces
the concept of a prior belief on the future outcomes [36–38] (or equivalently
referred as a goal prior) which constrains the system model (2). This goal
prior is set by the designer of the agent and encodes the future states that are
desirable, in other words the states that we want to be unsurprising for the
agent. Actions selected as a result of free energy minimization will then move
the agent as close as possible to these unsurprising (desired) states. A goal prior
is added as an additional factor to the system model description [37,38], leading
to the goal-constrained (unnormalized) GM:
ft(y,x, u|u) = pt(y,x|u)  
generative
model
˜p(y,xt,x,u)  
goal prior
. (9)
6
In order to relate the goal prior to the traditional control cost, a natural
choice is:
˜p(y,xt,x,u) = 1
Γ exp
(
−λ
t+T∑
k=t
ℓk(xk,uk)
)
, (10)
where Γ the the normalization constant and λ ≥0 is the scaling factor. For
the quadratic cost of (7) the goal prior factors into independent Gaussians (i.e.,
consists of factors in the form ∝exp(−λ1
2 xT
kQxk) exp(−λ1
2 uT
kRuk)), where the
weighting matrices Q and R take the role of (scaled) precisions. A related
probabilistic approach is described in [15], where a binary reward is deﬁned by
using a cost function.
3.2 Free Energy Objective
Consider the latent (hidden) variables at time t: y,x, u. Note that the state
sequence xis unknown for both the future and the past, whereas for the observa-
tions and the controls, only the future variables are unknown. Let us consider a
variational posterior distribution q(y,x, u|y,u) deﬁned over the latent variables.
Here, the label variational refers to the fact that the objective function (11)
is optimized by variations in the conditional [39]. Note that q(y,x, u|y,u) is a
posterior conditioned on the past observations and controls. To avoid notational
clutter, we adopt a mainstream notational convention in probabilistic inference
where conditioning is dropped from the variational posterior distribution, and
represent q(y,x, u|y,u) as q(y,x, u) or simply as q.
The free energy Ft[q] is deﬁned as follows [23]:
Ft[q] = D[q||ft] , (11)
where D[ q||ft] ≜
∫
q(s) log(q(s)/ft(s)) ds is the Kullback-Leibler (KL) diver-
gence (i.e., relative entropy). The KL divergence is an information-theoretic
concept that quantiﬁes how much one probability distribution diﬀers from an-
other distribution [40]. By straightforward manipulation, the free energy can
be decomposed as follows:
Ft[q] = −log Z  
surprise
+ Eq
[
log q(y,x, u)
pp(y,x, u|y,u)
]
  
posterior divergence
, (12)
= −log Z+ D[q||pp], (13)
where pp denoted the exact (Bayesian) posterior, and whereZ =
∫
ft(y,x, u|u) dydxdu,
with substituted past observations y and controls u; and pp(y,x, u|y,u) =
1
Zft(y,x, u|u). Since the posterior (KL) divergence term is always positive,
the free energy provides an upper bound on the exact (Bayesian) surprise. This
decomposition is often employed to justify the use of free energy as a tool for
(approximate) inference and model selection [41].
7
... = = = ... = =xt−1
ut−1
yt
xt
ut
yt+1
xt+1 xt+T−1
ut+T−1
yt+T
xt+T
p(y,x|u)
p(y,x|xt,u)
˜p(y,xt,x,u)
Figure 1: Forney-style factor graph representation of the goal-constrained gen-
erative model (9) with indicated factorizations. Observations are indicated by
small solid nodes.
... = = = ... = =xt−1
→
ut−1↓
→
↑
yt↑
xt
↓
→ ut
←↑
yt+1↑
xt+1 xt+T−1
↓
←
ut+T−1↓
←↑
yt+T↑
xt+T↓A→
E ↓ D↑
B
←
C
←
Figure 2: Forney-style factor graph speciﬁcation of the inference algorithm on
the goal-constrained generative model. Here, message A represents a state esti-
mate that summarizes past control and observations. The product of messages
D and E yields a posterior belief over the current control, the mode of which
is taken as the present action.
8
Remark 1 (Relation between FEP and surprise) . Generally, the generative
model p is a pdf over hidden states (say x) and observations (say y), while q
is a pdf only over hidden states. Hence, p(x,y) = p(y|x)p(x), in which p(x)
represents a prior. Substituting an observation y = y in the model, yields
ft(x) = p(y = y|x)p(x), which represents the product of a likelihood and the
prior. We are interested in obtaining a posterior belief pp(x) = ft(x)/Z. How-
ever, the normalizing constant (Bayesian evidence) Z =
∫
ft(x) dx is often
intractable to compute, because it involves an integral over all hidden state con-
ﬁgurations. As a result, it is often prohibitively expensive (in terms of computa-
tional power) to obtain an exact solution for the posterior pp. Instead, posterior
inference is often cast as a free energy optimization problem, where the free en-
ergy factorizes as Ft[q] = −log Z + D[q||pp] as in (12)–(13). Minimization of
Ft thus maximizes Bayesian evidence, while minimizing posterior divergence,
making q a close approximation to the (usually intractable) posterior pp.
3.3 Control
At time t, the objective is to ﬁnd the q that minimizes the free energy, i.e.,
q∗
t = arg min
q
Ft[q]. (14)
Looking at (9) and (11), we observe that after optimization, the variational
distribution q∗
t simultaneously accounts for the constraints enforced by the sys-
tem model (2) and the goal prior (10). The posterior for the current control is
obtained by marginalization, i.e., q∗
t(ut) =
∫
···
∫
q∗
t(y,x, u) dy,dx,du\t, where
u\t denotes the sequence obtained by removing ut from u. The current action
is then chosen as
u∗
t,q = g(q∗
t) , (15)
where g(.) is the same as in (5).
3.4 Free Energy Minimization by Message Passing on a
Forney-style factor graph
It is instructive to separate inference relating to the past/present from the in-
ference relating to the future. To this end, we substitute the GM ft(y,x, u|u) of
(9) into (11) and use (8c) to factorize the free energy in the following form with
separate contributions from the present ( Vt[q]) and (expected) future ( Gt[q]):
Ft[q] = Eq
[
log q(x)
pt(y= y,x|u= u)
]
  
Vt[q]
+ Eq
[
log q(y,x,u|x)
pt(y,x|xt,u) ˜p(y,xt,x,u)
]
  
Gt[q]
.
(16)
In practice, the optimization of Ft is often intractable and a speciﬁc choice
for the factorization of qis made to aid computation [42]. In our case, due to the
9
factorization of the GM, we can optimize Ft exactly by belief propagation over
the factor graph of the GM [43,44]. To minimize Vt and Gt, a Forney-style factor
graph (FFG) oﬀers a convenient visual representation of a factorized function
[45], and is especially well-suited for representing probabilistic models [46]. In an
FFG, edges represent variables and nodes (factors) represent relations between
variables. The FFG representation of the GM (9) with substituted factorizations
(2) and included goal priors (10) is drawn in Fig. 1. The free energy (11) is then
minimized by message passing [38, 43, 44, 47] on the FFG representation of the
goal-constrained GM. Message passing can be interpreted as ﬁrst minimizing
Vt and then minimizing a modiﬁed version of Gt, based on the outcome of
minimizing Vt [2].
Minimizing Vt[q]
Message passing yields a message A (Fig. 2) that represents the current state
estimate, given past observations and controls. This message summarizes the
information contained within the dashed box:
µA (xt) ≜
∫
pt(y= y,x|u= u) dx0:t−1 . (17)
From the perspective of stochastic optimal control, message A connects with
the estimator. Moreover, for a linear Gaussian state-space model, the recursive
message updates for computing A constitute a Kalman ﬁlter [48].
Minimizing Gt[q]
In order to minimize Vt[q]+ Gt[q], we re-normalize the message µA (xt) to obtain
a prior pe(xt), i.e.,
pe(xt) ≜ 1
Ce
µA (xt) (18)
where
Ce =
∫
pt(y= y,x|u= u) dx0:t. (19)
Here, the subscript e emphasizes the fact that pe represents the pdf of an esti-
mate (of the current state). We then deﬁne a modiﬁed objective for the expected
future
˜Gt[q] = Eq
[
log q(y,x,u,xt)
pe(xt)pt(y,x|xt,u) ˜p(y,xt,x,u)
]
, (20)
which can again be minimized by message passing. This yields messages B
– E (Fig. 2) by a backward recursion (smoothing pass) over the GM of future
variables. The product of D and E then leads to a marginal belief q∗
t(ut). Then,
the current control action is obtained using (15).
10
4 From Active Inference To Stochastic Optimal
Control
The main question we’re interested in is the following: “When does (15) provide
the same control actions as (5)?”. In other words, when can the ActInf frame-
work be used to solve the traditional stochastic control problem? Below, we
investigate this question. Since the goal priors only appear in ˜Gt and Vt can be
minimized independently, we focus exclusively on the minimization of ˜Gt. We
formulate two conditions under which minimizing ˜Gt reduces to minimizing the
stochastic optimal control objective (4).
Note that ˜Gt[q] can be written as
˜Gt[q] = Eq
[
log q(y,x,u,xt)
pe(xt)pt(y,x|xt,u)
]
−Eq[log ˜p(y,xt,x,u)] .
Then, minimizing ˜Gt[q] is equivalent to minimizing G†
t[q]:
G†
t[q] = Eq
[
log q(y,x,u,xt)
pe(xt)pt(y,x|xt,u)
]
+λEq
[t+T∑
k=t
ℓ(xk,uk)
]
, (21)
where we substituted the goal prior from (10) and omitted the additive constants
that do not depend on the optimization variables.
A striking diﬀerence between the optimal control and ActInf objective is
that the optimal control objective (4) involves an expectation w.r.t. the system
model pt and policy mapping π, while the free energy involves an expectation
w.r.t. the variational distribution q. In order to compare the solutions, we need
to create an equal footing.
4.1 Rewriting G†
t
We start by noting that all arguments ofq(y,x, u) that are not within the expec-
tation brackets in (21) are marginalized, i.e., x\t is marginalized. Therefore, ˜Gt
is eﬀectively only optimized with respect to q(y,x,u,xt). We then rewrite the
variational distribution in terms of the policy by making use of a region-based
approximation [43, 49]. Note that, for a model that is a tree (which is the case
for ft), the region-based approximation is exact. Without loss of generality, we
write:
q(y,x,u,xt) =
∏t+T−1
k=t q(yk+1,xk,xk+1,uk)
∏t+T−1
k=t+1 q(xk)
(22)
=
[t+T−1∏
k=t
q(uk)
]
  
π(u)
[∏t+T−1
k=t q(yk+1,xk,xk+1|uk)
∏t+T−1
k=t q(xk)
]
  
φ(y,x|xt,u)
q(xt) ,
11
where we simply applied the Bethe factorization to write the variational distribu-
tion in terms of a control posteriorπ, a system posterior φ, and the current-state
posterior q(xt).
We now use (22) to rewrite the second term of (21), as:
λEq
[t+T∑
k=t
ℓ(xk,uk)
]
= λEq
[
pe(xt)
pe(xt)
pt(y,x|xt,u)
pt(y,x|xt,u)
t+T∑
k=t
ℓ(xk,uk)
]
(23a)
= λEπ
[
Eq(xt),φ
[
pe(xt)
pe(xt)
pt(y,x|xt,u)
pt(y,x|xt,u)
t+T∑
k=t
ℓ(xk,uk)
]]
, (23b)
= λEpe,pt,π
[
q(xt)
pe(xt)
φ(y,x|xt,u)
pt(y,x|xt,u)
t+T∑
k=t
ℓ(xk,uk)
]
, (23c)
where in (23b) we made the expectations due to diﬀerent terms ofq(y,x,u,xt) =
π(u)φ(y,x|xt)q(xt) from (22) explicit and in the last step we interchanged dis-
tributions in the expectation subscript with distributions in the numerators,
i.e., we used Eq
[
p(s)
p(s) f(s)
]
= Ep
[
q(s)
p(s) f(s)
]
for a function f and probability dis-
tributions q and p.
We now turn to the ﬁrst term of (21). Again using the factorizationq(y,x,u,xt) =
π(u)φ(y,x|xt)q(xt) from (22), we rewrite this ﬁrst term as the sum of the neg-
ative policy entropy and a posterior divergence:
Eq
[
log q(y,x,u,xt)
pe(xt)pt(y,x|xt,u)
]
= (24)
Eq[log π(u)] + D(q(xt)∥pe(xt)) + Eq
[
log φ(y,x|xt,u)
pt(y,x|xt,u)
]
.
Substituting (23c) and (24) in (21) then reveals the full expression for the
ActInf controller objective:
G†
t[q] = Eq[log π(u)] + D(q(xt)∥pe(xt)) + Eq
[
log φ(y,x|xt,u)
pt(y,x|xt,u)
]
+
λEpe,p,π
[
q(xt)
pe(xt)
φ(y,x|xt,u)
pt(y,x|xt,u)
t+T∑
k=t
ℓ(xk,uk)
]
. (25)
Remark 2 (Interpretation of the FEP objective) . The FEP objective can be
seen as a trade-oﬀ between two terms: one pulls q towards p (under unin-
formative future values for the controls) and another that minimizes the cost∑t+T
k=t ℓ(xk,uk). Minimization of only the ﬁrst three terms in (25) (i.e. the case
with λ = 0) leads to an undetermined variational distribution q∗. Namely, be-
cause the ﬁrst three terms in (25) directly stem from the ﬁrst term of Eqn. (21),
we have q∗(y,x,u,xt) = pe(xt)pt(y,x|xt,u). Then, we have G†
t[q∗] = 0 (with
λ= 0).
Conversely, minimization of only the last term in (25) (with λ >0) leads
to a degenerate variational distribution q∗with mass only at a global minimizer
12
of ∑t+T
k=t ℓ(xk,uk), because this last term is equal to Eq
[∑t+T
k=t ℓ(xk,uk)
]
(recall
that the last term in (25) is only a rewritten version of the last term in (21)).
In particular, while minimizing Eq
[∑t+T
k=t ℓ(xk,uk)
]
over q, since there are no
other constraints on q, and q can be directly chosen as a distribution with point
mass at a global minimizer of ∑t+T
k=t ℓ(xk,uk). For instance, with ℓ(.) deﬁned as
(7), q with point mass at xk = 0, uk = 0 ∀k is a minimizer.
We now present two sets of conditions under which the actions chosen by
the ActInf agent by (15) are the same as the stochastic control actions chosen
using (5).
4.2 First Condition: Deterministic Model with Point-Estimate
Let mode(·) represent the mode of a distribution, where ties between multiple
modes (if any) are resolved by uniform random selection.
Theorem 1. Let (i) λ> 0, (ii) φ= pt, (iii) qe = pe, and (iv) g(·) = mode(·).
Then, u∗
t,q = u∗
t,π.
Proof. See Appendix A.
The below result shows that Theorem 1 implies that the optimal solution is
recovered for deterministic systems with a point-estimate.
Corollary 1. The conditions (ii) φ= pt, and (iii) qe = pe of Theorem 1 occur
in the case of a deterministic model pt in conjunction with a point estimate for
the current state.
Proof. In the case of a deterministic model, pt of (1) is constrained to delta
functions; i.e., p(xk+1|xk,uk) = δ(xk+1 −fx(xk,uk)) and p(yk|xk) = δ(yk −
fy(xk)), for some deterministic functions fx(·) and fy(·). A point estimate for
the current state (after the minimization of Vt) is chosen as pe(xt) ≜ δ(xt−ˆxt).
Then, any condition other than (ii) φ = pt, (iii) qe = pe will lead to inﬁnite
divergence in (24), and hence in (25). By contradiction, (ii) φ = pt and (iii)
qe = pe are the only viable solutions to the minimization of ˜Gt under the choice
of a deterministic model with a point estimate for the current state.
4.3 Second Condition: Vanishing State and Control Cost
We now consider minimizers of (25) as a function of λ, and deﬁne
rλ(y,x,xt,u) ≜ q∗(xt)φ∗(y,x|xt,u)
pe(xt)pt(y,x|xt,u) = q∗(y,x,xt|u)
pt(y,x,xt|u) . (26)
Note that the distribution q is an argument of (25). Hence, the optimal q
depends on λ. In light of Remark 2, we see that for λ = 0, rλ(y,x,xt,u) = 1,
while for λ> 0, rλ(y,x,xt,u) ̸= 1.
13
Figure 3: Results comparing LQG with ActInf control, where the time-axis is
log-scaled. The more aggressive LQG control (bottom left), leads to faster state
adjustments (top left). ActInf control for small but nonzero λ reduces to LQG
control. Notably, although ActInf control with λ = 1 accumulates higher cost
in terms of ℓ(xk,uk) in (7) (bottom right), it achieves lower free energy than
ActInf control with small λ (top right).
Theorem 2.Let (i) limλ→0+ 1
λEq[log rλ(y,x,xt,u)] = 0, (ii) limλ→0+ rλ(y,x,xt,u) =
1, ∀y,x,xt,u, and (iii) ut = mode πt. Then, u∗
t,q = u∗
t,π.
Proof. See Appendix B.
Condition (ii) requires that, under a vanishing λ, the second term of (25)
grows to zero faster than λitself. Hence, under (ii), the last term will dominate
over the second term, retaining the dependence of ˜Gt on ℓ(see the proof for de-
tails). Note that if we outright require λ= 0, all dependence on ℓis immediately
lost. Instead, the limit ensures that the inﬂuence of the cost ℓ is retained.
It is not straightforward to see when the conditions (i) – (ii) of Theorem 2
apply. In the subsequent sections, we further discuss the implications of Theo-
rem 1 and Theorem 2 for the special case of a linear Gaussian SSM.
5 Relationship Between LQG Control And Ac-
tive Inference For A Linear Gaussian SSM
We now investigate the behavior of the ActInf controller under a linear Gaussian
state-space model. We assume a linear Gaussian system with the respective
transition and observation precisions Ww and Wv, as follows:
p(xt+1|xt,ut) = N(xt+1|Axt + But,W−1
w ) (27a)
p(yt|xt) = N(yt|Cxt,W−1
v ) . (27b)
14
where the notation N(z|m,V ) represents a Gaussian distribution with the mean
m and the covariance (inverse precision) matrix V = W−1 for the variable z.
We consider the quadratic cost in (7), leading to independent Gaussian goal
priors (10).
5.1 Algebraic Results for the Active Inference Controller
A closed-form expression for the resulting ActInf regulator is obtained by prop-
agating the messages of Fig. 2 algebraically as follows:
Theorem 3. The ActInf solution for the system in (27) is given by
u∗
t,q = −Ktˆxt (28a)
Kt =
[
BT
(
AˆV′
tAT + P−1
t+1 + W−1
w
)−1
B+ λR
]−1
×
BT
(
AˆV′
tAT + P−1
t+1 + W−1
w
)−1
AˆV′
t ˆWt (28b)
ˆV′
t =
(
ˆWt + λQ
)−1
, (28c)
where ˆxt and ˆWt are the respective mean and precision of pe (the normalized
message A). Here, Pk, i.e. the precision of the backward message over state
xk, is given by
Pt+T = λQ (29a)
Pk−1 = −ATPkB
(
R′+ BTPkB
)−1
BTPkA+
ATPkA+ λQ (29b)
R′=
(
[λR]−1 +
[
BTWwB
]−1)−1
. (29c)
Proof. See Appendix C.
Note that this result provides an iterative procedure for ﬁnding the ActInf
solution. In particular, we initialize Pt+T with (29a). Then, Pk’s can be cal-
culated iteratively and oﬄine, i.e., without obtaining the measurements. Then,
the action is found using (28). Here, calculation of Kt requires calculation of
ˆWt. In the LQG case, pe is a Gaussian pdf with a mean and covariance that
are given by the standard Kalman ﬁltering equations, see for instance [48,50].
We now investigate the conditions implied by the two theorems and the
corollary from Sec. 3.1.2. Theorem 1 assumes a deterministic model and a
point estimate for the current state (Cor. 1), which corresponds to ˆWt =
Ww = ϵI2,ϵ → ∞. Theorem 2 investigates the dependence on λ, and lets
λ → 0+. In both cases (29c) reduces to R′ = λR, and (28b) reduces to
Kt =
[
BTPt+1B+ λR
]−1
BTPt+1A, thus recovering the classically optimal LQG
solution in the form of the discrete-time ﬁnite horizon Ricatti equations [34].
Note that compared to the standard LQG solution, both Qand Rappear to be
15
scaled with λ in the above equations, which has no eﬀect on the optimal solu-
tion. This can be seen for instance by recognizing that this scaling corresponds
to the scaling of both matrices with λ in (7), which corresponds to a simple
scalar scaling of the objective function.
6 Numerical Results
6.1 Scenario
In this section, we illustrate the performance of the ActInf controller for vary-
ing positive values of λ and compare the results with the standard LQG sce-
nario. The ActInf simulations are performed with the ForneyLab probabilistic
programming toolbox [51], and follow the experimental protocol in [38]. The
protocol at each time t consists of three main steps: (i) ﬁnd A and the current
state estimate pe by minimizing Vt (16), (ii) from the estimate, ﬁnd a control
posterior q∗
t(ut) by minimizing ˜Gt (20), and (iii) pass a selected action to the
system (1) to obtain a new observation.
For the system, we use (27), with C = R = Q = Wv = Ww = I2, A =(1 0 .1
0 1
)
, B =
(0.1 0 .5
0.05 0 .5
)
. The GM follows the system assumptions and uses
a lookahead of T = 10. We initialize the system relatively far from equilibrium,
at x0 = (25,25)T and choose a vague prior for the initial state x0.
6.2 Discussion
The results are presented in Fig. 3, which leads to several interesting observa-
tions. Firstly, for small but nonzero λ, the results (controls, state trajectory, the
accumulated cost for ℓ(xk,uk) and also Gt) of the ActInf controller approaches
the results of the LQG controller as expected; see Sec. 3.1.2 and also the discus-
sions at the end of Sec. 5. We note that, for the current system, λ= 0.01 (not
plotted) already renders the results of the ActInf and LQG controller nearly
visually indistinguishable.
Secondly, the LQG controller is more aggressive than the ActInf regulator
in terms of the controls, i.e., the magnitude of the LQG controls are relatively
large compared to those of the ActInf regulator. The explicit inclusion of the
process noise in ActInf is in contrast to the LQG scenario where the process and
estimation noise only aﬀect the state estimation directly but not the regulator
[16]. In particular, (29a–28c) depend explicitly upon Ww and ˆWt, whereas these
terms are not present in the original Ricatti equations. These terms make the
ActInf controller more conservative.
Thirdly, the accumulated cost in terms of ℓ(xk,uk) for the ActInf controller
approaches the optimal cost of the LQG controller under decreasing λ. This
observation is consistent with Theorem 2, which formulates suﬃcient conditions
for making the ActInf solution coincide with the LQG solution. Interestingly,
and perhaps counter intuitively, the terminal free energy for λ= 1 is improved
16
(lower) compared to the λ= 0.1 case. This eﬀect can be interpreted in light of
the good regulator theorem, which states that “every good regulator of a system
must be a model of that system” [52]. Namely, where the LQG cost function (7)
measures a quadratic cost, the free energy (25) oﬀers an approximate measure
of model ﬁtness (12). This then implies that the ActInf regulator with λ = 1
better models the system properties than the ActInf regulator with λ = 0 .1,
leading to lower free energy.
7 Conclusions
ActInf and the free energy principle provide a ﬂexible and general framework
for stochastic optimal control problems. By including the control cost as goal
priors, the control cost appears as an additive term in the free energy. The
resulting free energy minimization problem can be solved by belief propagation
over the associated factor graph, leading to an elegant and tractable approach
to solve stochastic optimal control problems. In general, the ActInf controller
does not solve the underlying stochastic optimal control problem. To address
this, we provide suﬃcient conditions for which ActInf reduces to traditional
stochastic optimal control. In other words, under certain conditions, stochastic
optimal control is a subset of ActInf control. Finally, while it is not known for
which classes of problem the suﬃcient conditions hold, we prove and numerically
demonstrate that the ActInf controller is a generalization of the important case
of the LQG controller.
At the heart of these methods lies the fact that ActInf allows us to directly
control the modeling assumptions. Therefore, we can explicitly include the
anticipated eﬀect of the costs and the noise in the control policy. Controlling
these assumptions allows us to reproduce traditional stochastic optimal control
solutions, such as the LQG controller.
References
[1] H.-A. Loeliger, J. Dauwels, V. M. Koch, and S. Korl, “Signal processing
with factor graphs: examples,” in First International Symposium on Con-
trol, Communications and Signal Processing, 2004. , pp. 571–574, IEEE,
2004.
[2] H.-A. Loeliger, J. Dauwels, J. Hu, S. Korl, L. Ping, and F. R. Kschischang,
“The factor graph approach to model-based signal processing,” Proceedings
of the IEEE , vol. 95, no. 6, pp. 1295–1322, 2007.
[3] A. Swami, Q. Zhao, Y. Hong, and L. Tong, Graphical Models and Fusion
in Sensor Networks , pp. 215–249. John Wiley & Sons, 2007.
[4] G. Ferri, A. Munaf` o, A. Tesei, P. Braca, F. Meyer, K. Pelekanakis,
R. Petroccia, J. Alves, C. Strode, and K. LePage, “Cooperative robotic
17
networks for underwater surveillance: an overview,” IET Radar, Sonar &
Navigation, vol. 11, no. 12, pp. 1740–1761, 2017.
[5] F. Meyer, H. Wymeersch, M. Fr¨ ohle, and F. Hlawatsch, “Distributed esti-
mation with information-seeking control in agent networks,” IEEE Journal
on Selected Areas in Communications, vol. 33, no. 11, pp. 2439–2456, 2015.
[6] L. R. Rabiner, “A tutorial on hidden markov models and selected appli-
cations in speech recognition,” Proceedings of the IEEE , vol. 77, no. 2,
pp. 257–286, 1989.
[7] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger, “Factor graphs and
the sum-product algorithm,” IEEE Transactions on information theory ,
vol. 47, no. 2, pp. 498–519, 2001.
[8] A. T. Ihler, J. W. Fisher, R. L. Moses, and A. S. Willsky, “Nonparametric
belief propagation for self-localization of sensor networks,” IEEE Journal
on Selected Areas in Communications, vol. 23, no. 4, pp. 809–819, 2005.
[9] P. E. Caines, Linear stochastic systems, vol. 77. SIAM, 2018.
[10] M. K´ arn` y, “Towards fully probabilistic control design,”Automatica, vol. 32,
no. 12, pp. 1719–1722, 1996.
[11] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum entropy
inverse reinforcement learning,” AAAI, 2008.
[12] H. J. Kappen, V. Gomez, and M. Opper, “Optimal control as a graphical
model inference problem,” Machine Learning, vol. 87, pp. 159–182, May
2012.
[13] J. Watson, H. Abdulsamad, and J. Peters, “Stochastic Optimal Control as
Approximate Input Inference,” Conf. on Robot Learning, 2019.
[14] S. Levine, “Reinforcement Learning and Control as Probabilistic Inference:
Tutorial and Review,” arXiv:1805.00909, 2018.
[15] M. Toussaint, “Robot trajectory optimization using approximate infer-
ence,” in Int. Conf. on Machine Learning (ICML) , pp. 1–8, 2009.
[16] C. Hoﬀmann and P. Rostalski, “Linear Optimal Control on Factor Graphs -
a Message Passing Perspective,” in 20th IFAC World Congress, (Toulouse,
France), July 2017.
[17] J. H. Lee, “Model predictive control: Review of the three decades of devel-
opment,” IJCAS, vol. 9, no. 3, p. 415, 2011.
[18] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .
MIT press, 2018.
18
[19] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic
Programming. John Wiley & Sons, 2014.
[20] T. Degris, P. M. Pilarski, and R. S. Sutton, “Model-free reinforcement
learning with continuous action in practice,” in 2012 American Control
Conference (ACC), pp. 2177–2182, 2012.
[21] G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots,
and E. A. Theodorou, “Information theoretic MPC for model-based rein-
forcement learning,” in 2017 IEEE ICRA , pp. 1714–1721, May 2017.
[22] S. Kamthe and M. P. Deisenroth, “Data-Eﬃcient Reinforcement Learn-
ing with Probabilistic Model Predictive Control,” arXiv:1706.06491, June
2017.
[23] K. J. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
brain,” Journal of Physiology, Paris , vol. 100, pp. 70–87, Sept. 2006.
[24] M. J. D. Ramstead, P. B. Badcock, and K. J. Friston, “Answering
Schr¨ odinger’s question: A free-energy formulation,” Physics of Life Re-
views, 2018.
[25] B. Sallans and G. E. Hinton, “Using free energies to represent Q-values in
a multiagent reinforcement learning task,” in Adv. in neural information
process. systems, pp. 1075–1081, 2001.
[26] K. J. Friston, “The free-energy principle: a uniﬁed brain theory?,” Nature
Reviews Neuroscience, vol. 11, pp. 127–138, Feb. 2010.
[27] G. Oliver, P. Lanillos, and G. Cheng, “Active inference body perception
and action for humanoid robots,” Arxiv:1906.03022, 2019.
[28] O. C ¸ atal, J. Nauta, T. Verbelen, P. Simoens, and B. Dhoedt, “Bayesian
policy selection using active inference,” arXiv:1904.08149, 2019.
[29] K. Ueltzh¨ oﬀer, “Deep Active Inference,” Biological Cybernetics, vol. 112,
pp. 547–573, Dec. 2018.
[30] S. Schw¨ obel, S. Kiebel, and D. Markovic, “Active Inference, Belief Prop-
agation, and the Bethe Approximation,” Neural Computation , vol. 30,
pp. 2530–2567, Sept. 2018.
[31] M. Baltieri and C. L. Buckley, “Active Inference: Computational Models
of Motor Control without Eﬀerence Copy,” in 2019 Conf. on Cognitive
Computational Neuroscience, 2019.
[32] B. Millidge, A. Tschantz, A. K. Seth, and C. L. Buckley, “On the relation-
ship between active inference and control as inference,” in1st International
Workshop on Active Inference , 2020.
19
[33] A. Imohiosen, J. Watson, and J. Peters, “Active inference or control as
inference? a unifying view,” in 1st International Workshop on Active In-
ference, 2020.
[34] T. Glad and L. Ljung, Control Theory: Multivariable and Nonlinear Meth-
ods. Taylor-Francis, 2000.
[35] H. Attias, “Planning by Probabilistic Inference,” in Inter. Conf. on Artiﬁ-
cial Intelligence and Statistics (AISTATS) , 2003.
[36] B. de Vries and K. J. Friston, “A Factor Graph Description of Deep Tem-
poral Active Inference,” Frontiers in Computational Neuroscience, vol. 11,
Oct. 2017.
[37] T. Parr and K. J. Friston, “Generalised free energy and active inference:
can the future cause the past?,” bioRxiv, Apr. 2018.
[38] T. W. van de Laar and B. de Vries, “Simulating Active Inference Processes
by Message Passing,” Frontiers in Robotics and AI , vol. 6, p. 20, 2019.
[39] D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe, “Variational Inference: A
Review for Statisticians,” Journal of the American Statistical Association ,
vol. 112, pp. 859–877, Apr. 2017.
[40] T. M. Cover and J. A. Thomas, Elements of Information Theory . Wiley,
1991.
[41] H. Attias, “A variational Bayesian framework for graphical models,” in
Adv. in neural information process. systems , pp. 209–215, 2000.
[42] C. M. Bishop, Pattern recognition and machine learning . Springer, 2006.
[43] J. S. Yedidia, W. T. Freeman, and Y. Weiss, “Constructing free-energy ap-
proximations and generalized belief propagation algorithms,” IEEE Trans.
Inf. Theory, vol. 51, pp. 2282–2312, July 2005.
[44] T. Heskes, “Stable ﬁxed points of loopy belief propagation are local minima
of the Bethe free energy,” in Adv. in neural information process. systems ,
pp. 359–366, 2003.
[45] G. D. Forney, “Codes on graphs: normal realizations,” IEEE Trans. on
Information Theory, vol. 47, pp. 520–548, Feb. 2001.
[46] H.-A. Loeliger, “An introduction to factor graphs,” Signal Processing Mag-
azine, IEEE, vol. 21, no. 1, pp. 28–41, 2004.
[47] J. Dauwels, “On Variational Message Passing on Factor Graphs,” in IEEE
Inter. Symp. on Information Theory , pp. 2546–2550, June 2007.
[48] S. Korl, A factor graph approach to signal modelling, system identiﬁcation
and ﬁltering. ETH Zurich, 2005.
20
[49] R. Cowell, “Introduction to inference for Bayesian networks,” in Learning
in graphical models, pp. 9–26, Springer, 1998.
[50] S. M. Kay, Fundamentals of Statistical Signal Processing: Estimation The-
ory. Prentice-Hall, 1993.
[51] M. Cox, T. W. van de Laar, and B. de Vries, “A factor graph approach
to automated design of Bayesian signal processing algorithms,” IJAR,
vol. 104, pp. 185–204, Jan. 2019.
[52] R. C. Conant and W. R. Ashby, “Every good regulator of a system must
be a model of that system,” Intl. J. Systems Science , pp. 89–97, 1970.
Appendix
A Proof of Theorem 1
First, we substitute (ii), (iii) in (25) which removes the second and third term
and the factors within the expectation of the last term resulting in
G†
t[q] = Eq[log π(u)] + λEpe,pt,π
[t+T∑
k=t
ℓ(xk,uk)
]
. (30)
Let LT
t (x,xt,u) ≜ ∑t+T
k=t ℓ(xk,uk). We now focus on the second term in (30)
λ
∫
pe(xt)p(y,x|xt,u)π(u)LT
t (x,xt,u) dudxdydxt (31a)
= Ceλ
∫
pt(y,x|u)pt(y,x|xt,u)π(u)LT
t (x,xt,u) dydxdu (31b)
= Ceλ
∫
pt(y,x|u)π(u)LT
t (x,xt,u) dydxdu (31c)
= CeλJt, (31d)
where in (31b), we have used (19) and (18); and in (31c) we have used (8c); and
in (31d) we have used (4), i.e. the deﬁnition of Jt.
Since the mode of the policy is used for the current action by (iv), the
negative policy entropy termEq[log π(u)] in (30) does not aﬀect action selection.
We therefore absorb the policy entropy in a constant C. Hence, (25) reduces
to a function of the form G†
t[q] = λCeJt[q] + C. Scaling of the scalar optimal
control objective Jt does not aﬀect regulator behavior. Hence, the standard
stochastic control solution u∗
t,π is the same as the ActInf solution u∗
t,q.
B Proof of Theorem 2
Recall from Theorem 1 that under condition (iii), the ﬁrst term in (25) does not
aﬀect the optimal solution. Furthermore, (ii) removes the ratio rλ(y,x,xt,u)
21
from the last term in (25). Hence, substituting in these modiﬁcations and
multiplying the objective with 1 /λ (note that multiplications with 1 /λ >0 do
not change optimal solutions), we have following objective function:
1
λD(q∗(xt)∥pe(xt)) + 1
λEq∗
[
log φ∗(y,x|xt,u)
pt(y,x|xt,u)
]
+ Ep∗e,p∗,π∗
[t+T∑
k=t
ℓ(xk,uk)
]
(32a)
= 1
λEq∗
[
log q∗(xt)φ∗(y,x|xt,u)
pe(xt)pt(y,x|xt,u)
]
+ Ep∗e,p∗,π∗
[t+T∑
k=t
ℓ(xk,uk)
]
, (32b)
where in (32b) we have used
D(q∗(xt)∥pe(xt))= Eq∗(xt)
[
log q∗(xt)
pe(xt)
]
= Eq∗
[
log q∗(xt)
pe(xt)
]
.
Taking the limit withλ→0+ and substituting (ii), we obtainEp∗e,p∗,π∗
[∑t+T
k=t ℓ(xk,uk)
]
as desired. Then, the optimal control objective (4) is again (proportionally) re-
covered, and hence u∗
t,q = u∗
t,π.
C Proof of Theorem 3
The algebraic result for the ActInf regulator, (29) and (28), is obtained by mes-
sage passing (Fig. 2). We derive this result by following the standard belief
propagation update rules as summarized by [48, Table 4.1]. For notational con-
venience, we write mean-variance and mean-precision parameterized Gaussian
distributions as NV and NW respectively, where distribution variable arguments
are left implicit.
Backward Recursion
The backward recursion (29) follows from message passing in a section of the
model as visualized in Fig. 4. Note that our speciﬁc choice of goal prior (10) is
independent of yk. As a result, messages 2 and 3 are uninformative, and do
not contribute to the end result.
The messages of Fig. 4 are computed as follows:
1 ∝NW(0,Pk)
2 ∝1
3 ∝1
4 ∝NW(0,Pk)
5 ∝NV
(
0,P−1
k + W−1
w
)
6 ∝NW(0,λR)
7 ∝NV
(
0,B(λR)−1BT)
22
... =
NW
A +
B
NW
NW =
C
NW
...xk−1
→
0
←
λQ
uk−1
→
0
←
λR
↓ Ww
→
Wv
↑yk
xk
1
←
2↑
3↑4
←
5
←
6 ↓
7 ↓
8
←
9
←
10 ↓
11
←
Figure 4: Message passing schedule for recursive backward propagation in a
single (future) section of a linear Gaussian state-space model (27).
8 ∝NV
(
0,P−1
k + B(λR)−1BT + W−1
w
)
9 ∝NW
(
0,AT(P−1
k + B(λR)−1BT + W−1
w )A
)
10 ∝NW(0,λQ)
11 ∝NW
(
0,AT(P−1
k + B(λR)−1BT + W−1
w )A+ λQ  
Pk−1
)
,
where we identify a recursion overPk−1. The recursion is initialized with Pt+T =
λQ, and terminates when Pt+1 is computed. The result simpliﬁes further:
Pk−1 = AT(P−1
k + B
[R′]−1
  
[(λR)−1 + (BTWwB)−1] BT)A
+ λQ (using BB−1 = I)
= ATPkA−ATPkB[R′+ BTPkB]−1BTPkA
+ λQ (using Woodbury identity) ,
with
R′= [(λR)−1 + (BTWwB)−1]−1 .
This concludes the derivation of (29). The computation of R′can be simpliﬁed
by making use of Searle’s identity.
23
Control Law
The control law (28) follows from message passing in a section of the model as
visualized in Fig. 5.
... =
NW
A +
B
NW
NW =
C
NW
...xt
→
0
←
λQ
ut
→
0
←
λR
↓ Ww
→
Wv
↑yt+1
xt+1
1
←
2↑
3↑4
←
5
←
6 ↓
7
→
8 ↓
9
→
10
→
11↑
12↑
Figure 5: Message passing schedule for the control law in a single (present)
section of a linear Gaussian state-space model (27).
The messages of Fig. 5 are computed as follows:
1 ∝NW(0,Pt+1)
2 ∝1
3 ∝1
4 ∝NW(0,Pt+1)
5 ∝NV
(
0,P−1
t+1 + W−1
w
)
6 ∝NW(0,λR)
7 ∝NW
(
ˆxt, ˆWt
)
8 ∝NW(0,λQ)
9 ∝NV
(
[ ˆWt + λQ]−1 ˆWtˆxt,[ ˆWt + λQ]−1
)
10 ∝NV
(
A[ ˆWt + λQ]−1 ˆWtˆxt,A[ ˆWt + λQ]−1AT
)
11 ∝NV
(
−A[ ˆWt + λQ]−1 ˆWtˆxt,
A[ ˆWt + λQ]−1AT + P−1
t+1 + W−1
w
)
24
12 ∝NW
(
−B−1A[ ˆWt + λQ]−1 ˆWtˆxt,
BT[A[ ˆWt + λQ]−1AT + P−1
t+1 + W−1
w ]−1B
)
.
The current control then follows from
q∗
t(ut) ∝ 6 ×12
ut = mode q∗
t(ut)
= −Ktˆxt,
where (using the Gaussian equality rule)
Kt = [BT(AˆV′
tAT + P−1
t+1 + W−1
w )−1B+ λR]−1×
BT(AˆV′
tAT + P−1
t+1 + W−1
w )−1AˆV′
t ˆWt,
with
ˆV′
t = ( ˆWt + λQ)−1 .
This concludes the derivation of (28).
25