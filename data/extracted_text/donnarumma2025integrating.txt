Integrating large language models and active inference to
understand eye movements in reading and dyslexia
Francesco Donnarumma1,†, Mirco Frosolone1,†, and Giovanni Pezzulo1,*
1Institute of Cognitive Sciences and Technologies, National Research Council, Rome, Italy
†Shared first authorship
*Corresponding author: giovanni.pezzulo@istc.cnr.it
August 11, 2025
Abstract
We present a novel computational model employing hierarchical active inference to simulate read-
ing and eye movements. The model characterizes linguistic processing as inference over a hierarchical
generative model, facilitating predictions and inferences at various levels of granularity, from syllables to
sentences. Our approach combines the strengths of large language models for realistic textual predictions
and active inference for guiding eye movements to informative textual information, enabling the testing
of predictions. The model exhibits proficiency in reading both known and unknown words and sentences,
adhering to the distinction between lexical and nonlexical routes in dual route theories of reading. Our
model therefore provides a novel approach to understand the cognitive processes underlying reading and
eye movements, within a predictive processing framework. Furthermore, our model can potentially aid
in understanding how maladaptive predictive processing can produce reading deficits associated with
dyslexia. As a proof of concept, we show that attenuating the contribution of priors during the reading
process leads to incorrect inferences and a more fragmented reading style, characterized by a greater
number of shorter saccades, aligning with empirical findings regarding eye movements in dyslexic indi-
viduals. In summary, our model represents a significant advancement in comprehending the cognitive
processes involved in reading and eye movements, with potential implications for understanding dyslexia
in terms of maladaptive inference.
Keywords: Hierarchical active inference, Predictive coding, Large Language Models, Reading,
Dyslexia
1 Introduction
Processing natural language – encompassing understanding, reading, and producing linguistic content –
represents a fundamental ability of our species. Extensive research in psychology, neuroscience, linguis-
tics, and machine learning has explored the intricate ways we process natural language. Neuroscientific
studies have revealed that natural language processing is inherently hierarchical, involving multiple brain
regions and the integration of various sensory inputs [8, 7, 60]. This hierarchical processing spans from
1
arXiv:2308.04941v3  [q-bio.NC]  8 Aug 2025
individual letters, phonemes, and words to complete sentence comprehension, with a hierarchy of brain
areas actively maintaining these elements in working memory across multiple time scales [32].
Recent studies have increasingly supported the idea of hierarchical predictive coding as a formal the-
ory describing perception as an inferential process, involving reciprocal exchanges between predictions
and prediction errors across brain hierarchies [69, 21]. In particular, various computational neuroimaging
studies employing large language models (LLMs) during linguistic tasks have provided compelling evi-
dence for both the predictive nature of language processing and the prediction hierarchies proposed by
predictive coding [29, 77, 1, 90, 10, 9, 34]. A recent computational neuroimaging study investigated hu-
man electrocorticographic (ECoG) responses to narratives using LLMs trained to predict the next word.
Remarkably, the study revealed that like the LLMs, the brain engages in next-word prediction before
word onset, computes prediction error signals, and utilizes latent representations of words (embeddings)
contextualized based on the sequence of prior words [29].
Converging evidence emerges from three recent fMRI studies utilizing deep learning models during
linguistic tasks. These studies not only confirm the predictive nature of language processing but also lend
support to the prediction hierarchies proposed by predictive coding. The first fMRI study [77] trained a
LLM to predict the next word at multiple time scales, identifying event boundaries as high surprise (also
explored in [1, 90, 10]). Analyzing human functional magnetic resonance data during story listening,
the study revealed an event-based hierarchy of surprise signals evolving along temporoparietal regions,
with surprise signals gating bottom-up and top-down connectivity across neighboring time scales. Fur-
thermore, another fMRI study provided compelling evidence of the brain’s ability to predict a hierarchy
of representations spanning multiple timescales in the future [9]. Enhancing LLMs with the capability
to predict beyond the next word increased their fit with human data. The study also highlighted the
hierarchical organization of brain predictions, ranging from temporal cortices predicting shorter-range
representations (e.g., the next word) to frontoparietal cortices predicting higher-level, longer-range, and
more contextual representations.
Notably, a separate fMRI study reported that evoked brain responses to words are influenced by
linguistic predictions and a metric of unexpectedness, closely aligning with the hierarchical predictive
processing schemes, where lower-level predictions are informed by higher-level predictions [34]. Addition-
ally, the study demonstrated that these hierarchical predictions can be well-aligned with standard levels
of analysis in psycholinguistics, including meaning, grammar, words, and speech sounds, reinforcing the
validity of the standard decomposition. Taken together, these studies, alongside others [43, 88, 42, 89, 19],
provide compelling support for the significance of prediction and hierarchical predictive coding in lan-
guage processing. Moreover, they underscore the growing relevance of LLMs in comprehending linguistic
processing [29].
Despite these advancements, the above studies have primarily focused on LLMs that passively receive
sensory information, rather than actively searching for it. However, linguistic tasks, such as reading
written text and listening to speech, are inherently active processes [18, 51, 27]. For example, during
reading, eye movements (saccades) actively guide attention to relevant parts of the text, rather than
processing every piece of text linearly. This active reading process suggests that saccades play a crucial
role in hypothesis testing, selecting informative parts of the text to test predictions [18, 51, 14, 24].
Several reading models incorporating eye movements, such as E-Z Reader [72], SWIFT [17, 79, 65],
¨Uber-Reader [71, 87], Glenmore [73, 74], SEAM [66], the rational model of eye movements [2] and OB1-
Reader [81] significantly advanced our understanding of the role of reading dynamics. These models have
successfully replicated numerous empirical findings, highlighting how word-level attributes like length,
frequency, and predictability impact reading dynamics. However, these models do not exploit the gener-
2
ative capabilities of recent large language models and do not fully align with the aforementioned evidence
of hierarchical predictive processing supporting reading and eye movements.
In this paper, we propose a novel computational model that unifies hierarchical predictive processing
and hypothesis testing during reading by integrating the LLM BERT (Bidirectional Encoder Represen-
tations from Transformers) [13] with active inference [23, 55]: a theory of perception and action based
on Bayesian principles, whereby agents minimize expected surprise (or variational free energy) through
iterative belief updating and action selection. Our model views reading as an active (Bayesian) inference
problem, employing a hierarchical generative model to represent causal relationships between textual
elements at different levels (letters, syllables, words, and sentences). By generating predictions at each
level and testing them through saccades, our model actively simulates reading.
The model incorporates three significant insights. Firstly, it conceptualizes linguistic processing as
inference, employing a hierarchical generative model that allows for predicting and inferring at different
timescales, such as syllables, words, and sentences. By integrating the LLM BERT [13] at the highest
hierarchical level, our model can handle realistic reading tasks effectively, processing sentences of arbitrary
length. Additionally, thanks to its hierarchical structure, our model can read both known words word-by-
word and unknown words syllable-by-syllable, akin to lexical and nonlexical routes in dual route theories
of reading [11].
Secondly, the model utilizes its generative capabilities not only for language recognition and pre-
diction, similar to LLMs, but also for simulating eye movements and saccades. In our model, reading
involves an active, hypothesis testing process: the model generates saccades to the most informative
parts of the text to validate its predictions and disambiguate among competing hypotheses about the
content being read [18, 1, 14, 15, 24, 51].
Thirdly, our model can be potentially used to gain insights on reading deficits, such as dyslexia.
Dyslexia, a common reading disorder affecting 5-10% of the population, is associated not only with
atypical brain activation patterns during language processing [60] but also with atypical eye movement
patterns, such as increased numbers of forward saccades and decreased saccade lengths compared to
control groups [92, 12, 45, 20, 36, 70]. Crucially, it has been suggested that dyslexia could be characterized
as a disorder of inference: a computational difficulty in effectively combining prior information, such
as implicit memory of previous words, with noisy observations like the currently perceived word [39].
Specifically, the proposal is that the reading difficulties might arise because prior information is assigned
excessively low weighting (or precision) relative to its internal noise. The low precision-weighting increases
the time required to form a coherent understanding of text (or to perform auditory discrimination, as
in [39]), because novel observations consistently surprise dyslexics. By incorporating this proposal in
our model, we provide a proof of concept that it can qualitatively reproduce eye movement patterns
associated with dyslexia.
In the following sections, we demonstrate the model’s capabilities through simulations of word reading
(Simulation 1 ), sentence reading (Simulation 2 ), reading unknown words and sentences ( Simulation 3 ),
and reading with prior information about the topic ( Simulation 4 ).
2 Methods
In this section, we provide a summary description of the model, a brief introduction to active inference
[55], and a detailed discussion of the hierarchical active inference reading model used in this paper.
3
2.1 Brief explanation of the model
We present a novel hierarchical active inference model for reading and eye movements during reading
[23]. Consistent with evidence of hierarchical language processing in the brain [34], our model consists
of three levels representing syllables, words, and sentences, as illustrated in Figure 1(a-b). At each level,
the inference process involves integrating three types of messages: bottom-up messages from nodes at
the level below (conveying observations), top-down messages from nodes at the level above (convey-
ing predictions), and lateral messages from nodes representing the previous timestep at the same level
(providing memory). For instance, the inference about the current syllable is informed by bottom-up
observations about the currently observed letter, top-down predictions from the currently inferred word,
and lateral information about the previous syllable. Similarly, the inference about the current word relies
on bottom-up observations (i.e., the currently estimated syllable), top-down predictions from the level
above (i.e., at the sentence level), and lateral information about the previous word.
To ensure that our model is capable of reading realistic text, we incorporate the large language model
BERT [13] to generate prior probability distributions over the current word when reading single words,
and over the next words based on previously inferred words when reading sentences. BERT was cho-
sen because it offers computationally efficient access to explicit probability distributions over upcoming
words and sentence continuations. Given an initial context—a sequence of already read words—and a
predictive horizon of H steps, BERT produces K candidate sentence completions. These candidates
are used to construct nested multinomial distributions over sentences, their constituent words, and the
syllables comprising those words. These distributions then serve as priors for the three levels of the hier-
archical active inference model, thereby initializing the inference process to estimate the most plausible
continuation of the sentence. Inference proceeds over successive time steps as new words are observed.
After a fixed number of inference steps, BERT is queried again with the updated context—including the
newly read words—and the process is repeated. This iterative procedure enables the model to read and
infer sentences of varying lengths.
Lastly, our model has the capability to determine where to make (forward or backward) saccades and
select the next letter observation [14]. The process of selecting where to direct the next saccade (i.e.,
which position in the syllable) is guided by the expected information gain of lexical elements across all
three levels of the model. Specifically, saccades are directed to the syllable position where the resulting
letter observation is expected to reduce uncertainty about the current syllable (Level 1), word (Level 2),
and sentence (Level 3); see Section 2 for further details.
Figure 1(c) visually represents the saccades generated by our model while reading an example sentence:
Active inference model of eye movements . The model demonstrates uncertainty about the first word
(active) and therefore reads it using two saccades to the first ( a) and fourth ( i) letters. It follows the
same process for the subsequent two words ( inference and model). However, at this stage, it skips the
next two words, as it perceives the most informative word to be the one following ( movements). This
mechanism showcases the model’s ability to dynamically select the most relevant information during the
reading process.
Figure 1(d) illustrates the probabilities of correctly recognized syllables (blue), words (red), and
sentences (yellow) during the reading process. Over time, these probabilities approach one, indicating
successful recognition. Additionally, the figure demonstrates that the levels operate at different time
scales, with lower levels processing information more quickly than higher levels. This temporal separation
arises because each level sends messages to the level above only after accumulating sufficient confidence
(as shown by the vertical bars in the figure), a process that typically requires multiple rounds of inference
(a)
(b)
(c)
(d)
Figure 1: The hierarchical active inference model of reading and eye movements. (a) The hierarchical
model comprises three layers for syllables (Level 1), words (Level 2), and sentences (Level 3), represented
using the formalism of Partially Observable Markov Decision Processes (POMDPs). Empty nodes indicate
hidden variables, filled nodes represent observations, and edges depict probabilistic relations between the
nodes, as detailed in B. (b) Model parameters. (c) The sequence of saccades generated by the model while
reading an example sentence. Yellow dots indicate the positions of the saccades in the text, and colored lines
trace the sequence of saccades from red to yellow. (d) Evolution of the probabilities of correctly recognized
syllables (blue lines), words (orange lines), and sentences (yellow lines) over time, while reading the sentence
of Panel c. The blue circles at the bottom level indicate saccades. Once the first level confidently infers a
syllable (or when the maximum level of iterations is reached), it sends a bottom-up signal to the second level
(indicated by the orange vertical bar) to aid in word inference. Similarly, the second level sends a signal to
the third level (indicated by the yellow vertical bar) upon confidently inferring a word. Please refer to the
main text for further explanation.
[25]. Note that the time it takes to arrive at a high probability of syllable can be interpreted as saccade
duration – implying that the longer the time required to resolve uncertainty about the fixated syllable,
the longer the saccade duration.
2.2 Brief introduction to Active Inference
Active Inference is a framework that models an agent’s action-perception loop by minimizing variational
free energy [55]. The key objective is to minimize free energy, and to achieve this, the agent possesses
a generative model (like the one illustrated in Figure 1), which captures the joint probability of the
stochastic variables (hidden states and observations) using the formalism of probabilistic graphical models
[3]. The agent’s perception involves making inferences about the hidden states based on the observed
sensory inputs, while action involves selecting actions that can change the hidden states and optimize
the model’s predictions. The generative model for active inference is defined as follows:
P (o0:K, s0:K, u1:K, γ|Θ) =
P (γ|Θ) P (π|γ, Θ) P (s0|Θ)
KY
t=0
P (ot|st, Θ) P (st+1|st, πt, Θ) (1)
where
• P (ot|st, Θ) = A,
• P (st+1|st, πt, Θ) = B (ut = πt),
• P (πt|γ, Θ) = σ
 
ln E − γ · GΘ (πt)

,
• P (γ, Θ) ∼ Γ (α, β), and
• P (s0|Θ) = D.
The set Θ = {A, B, C, D, E, α, β} parametrizes the generative model:
• The (likelihood) matrix A encodes the relations between the observations O and the hidden causes
of observations S.
• The (transition) matrix B defines how hidden states evolve over time t, as a function of a control
state (action) ut; note that a sequence of control states u1,u2,. . .,ut,. . .defines an action policy (or
policy for short) πt.
• The matrix C encodes an a-priori probability distribution over observations P (oτ ) and typically
encodes the agent’s preferences; this prior distribution parameter is needed for the resolution of
Equation (5), with P (oτ ) = C.
• The matrix D is the prior belief about the initial hidden state, before receiving any observation.
• E encodes a prior over the policies (reflecting habitual components of action selection).
• γ ∈ R is a precision that regulates action selection and is sampled from a Γ distribution, with
parameters α and β.
In active inference, the process of perception involves estimating hidden states based on observations
and previous hidden states. At the start of the simulation, the model has access to an initial state
estimate s0 through D and receives an observationo0 that helps refine the estimate by using the likelihood
matrix A. Subsequently, for each time step t = 1, . . . , K, the model infers its current hidden state st by
considering the transitions determined by the control stateut, as specified in B. Active inference employs
an approximate posterior over (past, present, and future) hidden states and parameters ( s0:K, u1:K, γ).
This approach utilizes a factorized form of variational inference, which corresponds to a framework
developed in physics known as mean field theory [54]. Using a mean field approximation, namely assuming
that all variables are independent, the factorized approximated posterior can be expressed as:
Q (s0:K, u1:K, γ) = Q (π) Q (γ)
KY
t=0
Q (st|πt) (2)
where the sufficient statistics are encoded by the expectations µ = (˜ sπ, π, γ), with ˜ sπ = ˜ sπ
0 , . . . ,˜ sπ
K.
Following a variational approach, the approximate posterior Q(st|πt) over hidden states under a given
policy πt at each time step t is optimized by minimizing the variational free energy (F) (see [55]) defined
as:
Ft = DKL[Q(st|πt) ∥ P(st|πt)] − EQ(st|πt)[ln P(ot|st)] (3)
The first term, DKL [Q(st|πt) ∥P(st|πt)], is the Kullback-Leibler (KL) divergence between the approxi-
mate posterior and the prior over hidden states. This term quantifies thecomplexity of the posterior—i.e.,
how much information (in bits or nats) is required to move from prior beliefs P(st|πt) to the posterior
Q(st|πt). Minimizing this term encourages the posterior belief to have low complexity, i.e., to remain
close to prior expectations.
The second term, −EQ(st|πt) [ln P(ot|st)], corresponds to the expected negative log-likelihood (or
inaccuracy) of the observation ot given the inferred hidden states. Minimizing this term encourages the
posterior beliefs to be predictive of the actual sensory observations, thus promoting accuracy. In sum,
minimizing the variational free energy trades off complexity against accuracy.
The approximate posterior Q(st|πt) is optimized when the sufficient statistics are:
sπ
t ≈ σ (ln A · ot + ln (B (πt−1) · sπ
t−1)) (4a)
π = σ (ln E − γ · G (πt)) (4b)
γ = α
β − G(π) (4c)
Action selection in the active inference framework involves choosing a policy, represented by a sequence
of control states u1, u2, . . . , ut, that is expected to minimize free energy most effectively in the future.
The policy distribution π is defined by the Softmax function σ(·). In this equation, a crucial rule is
played by the expected free energy (EFE) of the policies, denoted by G. The EFE incorporates goal-
directed components of action selection and can be interpreted as the expected variational free energy
under future outcomes. Additionally, in this equation the precision term γ plays a role in encoding the
confidence of beliefs concerning G.
The EFE G(πt) of each policy πt is defined as:
G (πt) =
KX
τ=t+1
DKL [Q (oτ |π) ∥ P (oτ )] + E ˜Q [H [P (oτ |sτ )]] (5)
where DKL [· ∥ ·] and H [·] are, respectively, the Kullback-Leibler divergence and the Shannon entropy,
Q (oτ , sτ |π) ≜ P (oτ , sτ ) Q (sτ |π) is the predicted posterior distribution, Q (oτ |π) = P
sτ Q (oτ , sτ |π)
is the predicted outcome, P (oτ ) is a categorical distribution representing the preferred outcome and
encoded by C, and P (oτ |sτ ) is the likelihood of the generative model encoded by the matrix A.
The EFE in (5) can be used as a quality score for the policies and has two terms:
• Expected Cost. The first term of (5) is the Kullback-Leibler divergence between the (approximate)
posterior and prior over the outcomes and it constitutes the pragmatic (or utility-maximizing)
component of the quality score. This term favours the policies that entail low risk and minimise
the difference between predicted ( Q (oτ |π)) and preferred ( P (oτ ) ≡ C) future outcomes.
• Expected Ambiguity. The second term of (5) is the expected entropy under the posterior over hidden
states and it represents the epistemic (or uncertainty-minimizing) component of the quality score.
This term favours policies that lead to states that maximize information gain and diminish the
uncertainty of future outcomes H [P (oτ |sτ )].
After scoring all the policies using the EFE, action selection is performed by drawing over the action
posterior expectations derived from the sufficient statisticπ computed via (4b). Then, the selected action
is executed, the model receives a novel observation and the perception-action cycle starts again. We refer
the reader to [55] for a complete derivation of the equations presented in this section.
2.3 Hierarchical Active Inference model for reading
The active inference model employed in this study is structured hierarchically, consisting of three levels
denoted by i ∈ 1, 2, 3 in our simulations. At each level, the model encodes hidden variables associated
with different aspects of textual content: syllables at Level 1, words at Level 2, and sentences at Level
3. This hierarchical organization allows the model to effectively capture and process information at
multiple linguistic scales, enabling the inference and generation of syllables, words, and sentences during
the reading process. For a visual representation of the model’s architecture, refer to Figure 1(a-b).
The hidden states of each level are obtained by the tensorial product: S(i) = S(i)
1 ⊗S(i)
2 ⊗S(i)
3 among
three factors:
• the Content S(i)
1 , i.e., the textual content proper: Syllable, Word or Sentence,
• the Location S(i)
2 , i.e. the position of a portion of the textual content (Location of the Letter in
the syllable, Location of the Syllable in the word, and Location of the Word in the Sentence), thus
corresponding to a S(i−1)
1 -type content of the next lower level if i >1 or to location of a Letter in
a Syllable if i = 1.
• the Topic S(i)
3 , i.e. the context to which each Content variable is associated. Please note that we
only use this factor in Simulation 4.
The Content is the crucial element of the chain: it is the text that is obtained from the recursive
concatenation of the elements from the level below. A Syllable is a concatenation of N(1) Letters c(0)
(which are not hidden states but observations, belonging to a finiteAlphabet S(0)
1 , see below). A Word is a
concatenation of N(2) syllables c(1) belonging to the set of Syllables S(1)
1 and a Sentence is concatenation
of N(3) Words c(2) belonging to the set S(2)
1 . In general, the i-th content c(i) ∈ S(i)
1 is a concatenation of
N(i) sub-contents c(i−1)
n ∈ S(i−1)
1 such that
c(i) = c(i−1)
1 , . . . c(i−1)
N(i)
The observations O(i) = O(i)
1 ⊗ O(i)
2 ⊗ O(i)
3 consists of the tensorial product among:
• the observation O(i)
1 of the content of the level below S(i−1)
1 . Note that only Level 1 of the model
receives an actual textual observation: namely, a letter c(0) belonging to the Alphabet S(0)
1 . The
other two levels receive as observations the content at the level below: the observations at levels 2
and 3 are the inferred syllables and words, respectively;
• the observation O(i)
2 corresponding to the location S(i)
2 of the S(i−1)
1 content;
• the feedback response r ∈ O(i)
3 , (“correct“ or “wrong”) that reports whether the currently inferred
content C(i) has been correctly classified (i.e., assigned to the correct topic) or not. We only use
this observation at Level 3 (i.e., O3
3) of Simulation 4, in order to compare models with or without
prior information about the topic of the sentence.
The likelihood mapping p(O(i)|S(i)) between hidden states S(i) and observations O(i) is specified
through the tensor A(i), defined as the tensorial product A(i) = A(i)
1 ⊗ A(i)
2 ⊗ A(i)
3 where
• A(i)
1 is a 4-order tensor mapping the hidden states S(i) (3-order tensors) to the observation O(i)
1
corresponding to the content S(i−1)
1 if i > 1 or to the observation of a letter if i = 1; In our
simulations, this probability is typically an identity matrix: the probability of observing content
c(i) ∈ S(i)
1 if the reader agent is in the corresponding location l(i) ∈ S(i)
2 is set to 1. Note that
adding noise to the likelihood function would allow modeling the effects of perceptual deficits or
poor familiarity with words.
• A(i)
2 is a 4-order tensor mapping the hidden states S(i) to observation O(i)
2 corresponding to the
locations S(i)
2 ; Also in this case we assume no noise, so the probability of observing a given location
when the reader agent eye point to that location is set to 1.
• A(i)
3 is a 4-order tensor mapping the hidden states S(i) to the feedback response in O(i)
3 . In our
simulation we adopt a binary probability in our simulation: the likelihood is set to 1 if the content
belongs to the correct topic and to 0 otherwise. Please note that in this paper this factor is used
only in Simulation 4.
The mapping p(s(i)|s(i)
t−1, πt) between hidden states given the control state u is specified by the tensor
B(i), defined as the tensorial product B(i) = B(i)
1 ⊗ B(i)
2 ⊗ B(i)
3 where
• B(i)
1 maps the transition between content states at successive time steps. Note that in our simula-
tions, we use transition models without noise (for the control model) and with noise added at one or
more hierarchical levels (for the dyslexic models). For this, we formalize the mapping p(c(i)
t |c(i)
t−1, πt)
by assigning 1 − δ to the correct target transition and assigning the value δ/(M(i) − 1) across the
other M(i) −1 states, where M(i) is the total number of Content statesS(i)
1 . This modulating factor
δ(i) ∈ [0, 1], is set to 0 in the control model and to 0.15 in the dyslexic models.
• B(i)
2 maps the transitions between locations. We equip the model with the probabilityp(l(i)
t |l(i)
t−1, πt)
to jump from one location l(i) ∈ S(i)
2 to another, without noise (i.e., the act of jumping to the
selected location is executed with no errors). While in principle one can jump to any location, here
we restrict the jump possibilities, by setting priors over locations D(i) and policies E(i), see below.
• B(i)
3 maps the probability p(tp(i)
t |tp(i)
t−1, πt) to jump from one topic tp(i) ∈ S(i)
3 to another. We
consider this transition to be noiseless. This action is only relevant in Simulation 4, in which we
compare the model with or without prior information about the topic. Please note that we only
use this factor in Simulation 4.
The tensor C(i) = C(i)
1 ⊗ C(i)
2 ⊗ C(i)
3 encodes the priors over the observations that encode preferred
outcomes:
• C(i)
1 , prior preferences on the outcomes O(i)
1 , (e.g. for Level 3 encoding sentences, prior on expected
words), which is flat in our simulations;
• C(i)
2 , prior preferences on the outcomes O(i)
2 (e.g. for Level 3 encoding sentences, prior on expected
word locations), which is flat in our simulations;
• C(i)
3 , prior preferences on the feedback response O(i)
3 (e.g. for Level 3 encoding sentences, a higher
prior for having classified the sentence according to the “correct” topic than the “wrong” topic.
Please note that we only use this factor in Simulation 4.
The tensor D(i) = D(i)
1 ⊗ D(i)
2 ⊗ D(i)
3 encodes the priors over the hidden states:
• D(i)
1 , priors over the Content at the corresponding level. It corresponds to the initial distribution
of the content variable (e.g. at Level 3, it is the prior over the initial sentence distribution). For
simplicity, here we use flat priors. However, it is possible to use this feature to model the fact that
syllabes, words and sentences have different frequencies.
• D(i)
2 , priors on the Locations. For simplicity, here we assign a very high prior to the first element
of the content, following the assumption that people start reading (for example) a word from the
first syllable. During reading, after each loop the D(3)
1 is initialized to the next word not yet read.
• D(i)
3 , priors on the Topics. We only use this feature in Simulation 4, in which we show that
having prior information about the topic of the sentence can speed up its reading. In all the other
simulations, we set flat priors over Topics.
The matrix E(i) encodes the priors over the policies.
2.4 Saccade selection
Saccade selection arises from a competition among policies spanning all hierarchical levels, determining
the location of the next word in the sentence (Level 3), syllable in the word (Level 2), and letter in the
syllable (Level 1). The number of policies at each level varies depending on the simulation. For example,
when reading 8-word sentences in Simulation 2, the model uses 8 policies to jump among words, 4 policies
to jump among syllables and 5 policies to jump among letters.
Policy selection considers the Expected Free Energy (EFE) described in (5). In our simulations, we
employ flat prior preferences on Content and Location (denoted asC(i)
1 and C(i)
2 ), respectively), resulting
in policy selection that solely weighs the information gain associated with the following word, syllable,
or letter. However, in Simulation 4, policy selection also depends on the greater preference ( C(i)
3 ) for
receiving a “correct” than a “wrong” feedback response (we remind that this factor is absent in the other
simulations).
Furthermore, policy selection considers the prior distribution over policies (denoted as E(i)). Our
simulations adopt flat priors for Level 1 ( E(1)) and Level 2 ( E(2)), signifying no inherent constraints
on transitioning between letters within a syllable or between syllables within a word. To discourage
overly long saccades, a finite moving window for favored transitions among word locations is set for Level
3 ( E(3)). This is achieved through a Poisson distribution P ois(λ), where λ equals 6 (as displayed in
Figure 2, depicted by red bars). Additionally, the probability of reading the same word after having
already recognized it is set to zero, resulting in the normalization of the distribution (note however
that it the distribution allows backtracking to a previous word during reading). This culminates in the
distribution depicted by the black bars in Figure 2. The chosen distribution enables the creation of a
finite window to determine probabilities related to the content under consideration. Iterating this window
at each step enables the processing of sentences of arbitrary length.
Furthermore, Figure 3 presents a schematic illustration of the textual elements inferred by the model
while reading the sentence “Active inference model of eye movements” as depicted in Figure 1. The
diagram distinguishes between two factors of the generative model – Content and Location – illustrated
by the left and right trees, respectively. The left (content) tree in Figure 3 demonstrates that the model
-5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10
locations
0
0.05
0.1
0.15p( )
E distribution
Figure 2: Priors on the policies that make transitions between word locations at Level 3 of the generative
model. We first set the prior as a Poisson distribution with λ = 6 (red bars), centered on the current location
(i.e., the value of the maximum of the distribution is in 0). We next set the probability of reading the same
word two consecutive times to zero and re-normalize the distribution, leading the distribution shown by the
black bars.
Figure 3: Graphical explanation of the model’s reading process for the example sentence “Active inference
model of eye movements” introduced in Figure 1. The figure illustrates the distinction between two factors
of the generative model: content (left tree) and location (right tree). Refer to the main text for a detailed
explanation.
infers the first word “active” at Level 2 by examining the letter “a”. This, in turn, leads to the inference
of the syllable “ac” at Level 1, followed by the letters “t” and “e”, enabling the inference of the syllable
“tive” at Level 2. The right (location) tree in Figure 3 shows the respective locations of these three
letters “a”, “t”, and “e” within their syllables at Level 1, and the positions of “ac” and “tive” within the
word at Level 2. This process continues for the subsequent words (excluding “of” and “eye”, which are
skipped during reading, as seen in Figure 3), until the entire sentence is recognized.
3 Results
3.1 Simulation 1: Reading single words
In this task, the objective is to read and recognize 100 words consisting of either 4 or 8 letters, selected
from a pool of 726 words ranging from 1 to 8 letters (see Tab. S.14 for the list of words used in this
simulation, all of which are part of the dictionary of BERT). Syllables for each word are generated from
4-letter 8-letter
Number of letters in the word
0.94
0.96
0.98
1
Probability of the correct word
Control Model
Dyslexic Model
(a)
4-word 8-word
Number of words in the sentence
0.88
0.9
0.92
0.94
0.96
0.98
1
Probability of the correct sentence
Control Model
Dyslexic Model (b)
Figure 4: Results of Simulations 1 and 2: Probabilities assigned to the correct words. (a) Simulation 1:
Probabilities assigned by the Control model (CM) and the Dyslexic model (DM) while reading 4-letter and
8-letter words. (b) Simulation 2: Probabilities assigned by the two models while reading 4-word and 8-word
sentences. In this and the subsequent boxplots, the horizontal line represents the mean µ. The edges of the
rectangle represent µ±σ, where σ is the standard deviation of the mean. The area within the boxplot above
the mean represents the interval between µ+σ, while that below the red line represents the interval between
µ − σ. The vertical black lines limits extend within the range of µ ± 3σ.
words using the methods described in [44]. For ease of evaluation, each word is assigned the same a-priori
probability of 1/726 in the model.
The model’s performance is assessed using four metrics: word recognition accuracy, the probability
assigned to the correct word, the number of saccades (forward, backward, or total), and their amplitude
(i.e., the number of locations between letters). It is important to note that for this specific simulation,
only levels 1 and 2 of the model are utilized, while Level 3 is not included.
To better evaluate the model, we compare two versions: (i) a Control model (CM) with no noise in
its transition functions, allowing it to adequately consider the prior context during reading, and (ii) a
Dyslexic model (DM), aiming to capture qualitatively key aspects of this disorder. Previous research has
demonstrated that individuals with dyslexia exhibit a reading style characterized by more fragmented
and laborious patterns of eye movement, with a higher number of shorter saccades when reading words,
and these difficulties are more pronounced with longer words [92, 46, 20]. To provide a proof of concept
that our DM model can replicate this pattern of results, here we follow the proposal that dyslexics
struggle to integrate prior linguistic context [39] – and introduce noise in the transition functions of both
the first (syllable) level and the second (word) levels. For this, we set δ(1) = δ(2) = 0 .15 in the DM
model, see Methods section.
The CM demonstrates perfect accuracy (100%), whereas the DM exhibits slightly lower accuracy for
both 4-letter (99%) and 8-letter words (97%). Additionally, the CM assigns a 100% probability to the
correct words, whereas the DM assigns a significantly lower probability to the correct word, especially
for longer words. Please refer to Figure 4(a) and Table S.1 for detailed results.
Moreover, the DM exhibits a significantly higher number of total saccades (Figure 5(a)) and backward
saccades (i.e., saccades to any of the preceding letters, Figure 5(b)) while reading both 4- and 8-letter
words compared to the CM. Detailed statistical comparisons are available in Tab. S.2 and S.3. To delve
(a)
 (b)
Figure 5: Simulation 1 results: Total number of saccades (a) and number of backward saccades (b) while
reading 4-letter and 8-letter words. The figure compares the Control Model (CM) with no noise, the Dyslexic
Model (DM) with noise at all hierarchical levels, and two versions of the DM, with noise only at the level
of syllables ( DM - Noise on Level 1 ) or words ( DM - Noise on Level 2 ). See Table S.2 and Table S.3 for
detailed results and statistical comparisons.Horizontal bars indicate statistically significant differences from
the Control Model (CM). Significance levels are indicated as follows: * p <0.05, ** p <0.01, *** p <0.001.
deeper into the impact of noise on eye movement dynamics during word reading, we created two variants
of the DM, one with noise in the transition functions at the syllable level ( DM - Noise on Level 1) and
another with noise at the word level ( DM - Noise on Level 2). This analysis reveals that noise at the
first (syllable) level has a more significant effect than noise at the second (word) level.
In addition, the DM exhibits significantly shorter forward saccades (Figure 6(a)) but longer backward
saccades (Figure 6(b)) while reading both 4- and 8-letter words in comparison to the CM. Detailed
statistical comparisons are available in Table S.4 and in Table S.5.
In summary, our model successfully reproduces a wide range of empirical findings regarding how
dyslexic individuals read single words, encompassing accuracy, reaction times, and eye movements. The
lower probability assigned to words by the DM compared to the CM, as depicted in Figure 4(a), closely
mirrors the results reported by [92, Fig. 1(B)]. The accuracy results (Table S.1) are also consistent with
the finding that dyslexic individuals fail to correctly read a small percentage of single words. Moreover,
our simulations reveal that dyslexic individuals read significantly slower than controls [92, 52, Fig. 1(A)].
This slower reading arises from the DM making more saccades than the CM, as illustrated in Figure 5,
with reading time being proportional to the number of saccades.
Furthermore, the results displayed in Figure 6(a) closely match the finding that dyslexic individuals
4-letter 8-letter
Number of letters in the word
0.5
1
1.5
2
2.5Amplitude of forward saccades [#]
**
***
Control Model
Dyslexic Model
(a)
4-letter 8-letter
Number of letters in the word
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8Amplitude of backward saccades [#]
***
***
Control Model
Dyslexic Model (b)
Figure 6: Simulation 1 results: Amplitude of forward (a) and backward (b) saccades while reading 4-letter
and 8-letter words. The figure compares two models: the Control Model (CM) with no noise, and the
Dyslexic Model (DM) with noise at both hierarchical levels. See Table S.4 and Table S.5 for detailed results
and statistical comparisons.
make shorter forward saccades compared to controls, particularly evident for longer words [46, Fig. 4].
Lastly, the results presented in Figure 6(b) closely resemble the finding that dyslexic individuals make
significantly more and larger backward saccades than controls during reading [59, Fig. 5]. Collectively,
these outcomes demonstrate that our model effectively captures possible aspects of reading skill dif-
ferences and/or development, providing valuable insights into the underlying mechanisms of reading
disorders like dyslexia.
Significantly, our model offers a mechanistic understanding for all these observed findings, including
the fragmented reading style of dyslexics. The DM’s shorter forward saccades stem from its poor con-
textual memory, making it less capable of predicting the next word efficiently. Consequently, it requires
more saccades and time to read a text. Additionally, the increased need for backward saccades arises
because the DM occasionally has to backtrack in the text to retrieve lost context. Moreover, our model
readily explains why dyslexic reading impairments are more pronounced for longer words: noise in the
transition function(s) accumulates over time, making reading longer words progressively more challeng-
ing. This comprehensive mechanistic explanation underscores the potential usefulness of our model in
shedding light on the underlying cognitive processes contributing to dyslexia and its impact on reading
behavior.
3.2 Simulation 2: Reading Sentences
This task involves reading and recognizing 100 sentences composed of 4 or 8 words (all of which are
included in the dictionary of BERT [13]), with each word consisting of 1 to 4 syllables. The sentences
with 4 words have an average length of 25.96±1.96 letters within the range of [21, 31]. Similarly, sentences
with 8 words have an average length of 50.24±1.64 letters, in the interval of [47, 55] (see Table S.13). To
make the text challenging, we carefully designed sentences with a substantial word overlap (Table S.15).
For this task, we utilize the comprehensive model depicted in Figure 1, encompassing three hierarchical
levels. At the beginning of the simulation, we use BERT to generate prior probability distributions over
upcoming sentences, comprising the next 4 or 8 words. These distributions are continuously updated
during reading as new syllables and words are inferred through informative saccades.
Previous studies have shown that proficient readers can scan lines of text using only a few saccades,
while dyslexic individuals exhibit an increased number of shorter saccades [36, 70, 12] and fewer word
skipping occurrences [6, 40, 33] compared to controls. Additionally, their performance decrease and
increased number of saccades are influenced by the number of words in the text (see [12], Fig. 2). To
verify whether our model accurately replicates these findings, we compare a Control model (CM) with no
noise and a Dyslexic model (DM) with noise introduced in the transition functions of all three hierarchical
levels (setting the parameter δ(1) = δ(2) = δ(3) = 0.15, see Methods section).
Consistent with the empirical findings, the DM assigns significantly lower probabilities to the correct
sentences compared to the CM, and this effect is more pronounced for longer sentences (Figure 4(b) and
Table S.6). Additionally, the DM exhibits a significantly higher number of total saccades (Figure 7(a-c))
and backward saccades (Figure 7(b-d)) than the CM during sentence reading. When comparing different
variants of the DM with noise introduced at various levels, we find that noise at the level of sentences
has the most significant impact on impairing the reading performance.
Moreover, the DM exhibits significantly shorter forward saccades (Figure 8(a)) and longer backward
saccades (Figure 8(b)) when reading both 4-word and 8-word sentences, in comparison to the CM.
Detailed statistical comparisons can be found in Table S.9 and in Table S.10.
Collectively, these results build upon those observed for single words (Simulation 1) and successfully
(a)
 (b)
Figure 7: Simulation 2 results: Total number of saccades (a) and number of backward saccades (b) while
reading 4-word and 8-word sentences. The figure compares the Control Model (CM) with no noise, the
Dyslexic Model (DM) with noise at both hierarchical levels, and three alternative versions of the Dyslexic
Model, where we added noise only at the level of syllables ( DM - Noise on Level 1 ), words ( DM - Noise
on Level 2 ), or sentences ( DM - Noise on Level 3 ). See Table S.7 and Table S.8 for detailed results and
statistical comparisons.
4-word 8-word
Number of words in the sentence
4
5
6
7
8
9
10
11
12
13Amplitude of forward saccades [#]
***
***
Control Model
Dyslexic Model
(a)
4-word 8-word
Number of words in the sentence
0
5
10
15
20
25Amplitude of backward saccades [#]
***
***
Control Model
Dyslexic Model (b)
Figure 8: Results of Simulation 2: amplitude of forward (a) and backward (b) saccades when reading
4-word and 8-word sentences. The figure compares the Control Model (CM) with no noise and the Dyslexic
Model (DM) with noise at all the hierarchical levels. See Table S.9 and Table S.10 for detailed results and
statistical comparisons.
replicate empirical findings on dyslexic individuals’ reading of sentences. The trends illustrated in Fig-
ure 7(a) closely resemble the outcomes reported [12, Fig. 3(A)], which demonstrates that dyslexics make
more saccades compared to controls while reading single lines of text (equivalent to reading 8-word sen-
tences in Figure 7(a)). Moreover, the number of saccades exhibited by the DM in our simulations aligns
well with the numerical outcomes from the same study. Similarly, the outcomes in Figure 8(a) closely
resemble the findings depicted in [12, Fig. 3(B)], revealing that dyslexics make shorter forward saccades
than controls. In this context, the amplitudes of both dyslexics and the DM model closely correspond.
However, the larger amplitude of backward saccades observed in our DM (Figure 8b) is in contrast with
the shorter amplitude of regressions observed in dyslexic readers in [12, Fig. 3d].
In summary, our results successfully replicate the observation that dyslexic individuals tend to produce
a higher number of shorter saccades while reading sentences, and that their reading performance is
influenced by the number of words in the sentence (see [12, Fig. 2]). The underlying causes of these
impairments are similar to those identified in the single-word reading task (Simulation 1), but their
effects are amplified in sentence reading, where comprehension depends on integrating information over
longer timescales. This also helps explain why noise at the sentence level significantly disrupts task
performance (Figure 7). While these qualitative results are promising, further work is needed to achieve
a closer—and more quantitative—alignment between the model and empirical findings on dyslexia, as well
as to calibrate model parameters appropriately. For instance, our DM generates a relatively high number
of regressive (backward) saccades. This occurs because the model often loses contextual information and
must backtrack to earlier words in the sentence to reestablish it. However, this pattern is not commonly
observed in dyslexic readers ([12, Fig. 3d]), and the reasons for this discrepancy remain to be explored.
3.3 Simulation 3. Reading novel words and sentences
Up to this point, our simulations involved reading known words and sentences, already present in the
BERT vocabulary. Hence, the model could confidently assign probabilities at the second and third layers.
Nevertheless, our model can also read unknown words and sentences, despite its inability to assign them
a prior probability. When confronted with unknown words or sentences, the model reads syllable-by-
syllable (or word-by-word) and simultaneously acknowledges uncertainty at the second (or third) level.
This mode aligns with the nonlexical (or sublexical) route in dual-route theories of reading [11].
We conducted a simulation consisting of 100 trials, each involving the reading and recognition of
a four-word sentence, designed for comparison with the corresponding tests in Simulation 2. In each
trial, one of the four words was unknown—specifically, it was removed from the model’s vocabulary.
This resulted in a set of 100 unique unknown words, each ranging from 1 to 4 syllables in length (see
Table S.16 for the complete list). The sentences had an average length of 25.46±1.92 letters, with a range
of [22, 32]. A two-sample t-test comparing sentence lengths between this set and the set with only known
words used in Simulation 2 revealed no significant difference (p ≈ 0.27), confirming that the two sets were
well matched in terms of both word and sentence length. Trials were terminated upon either successful
completion of the sentence or failure of the model to recognize a word. The simulation results indicate
that the total number of saccades increased substantially when reading sentences containing unknown
words (mean ≈ 12.04) compared to Simulation 2 (mean ≈ 4.36); see Table 1 for statistical comparisons.
Furthermore, while sentence recognition at Level 3 in Simulation 2 was nearly perfect ( p(C3|O) ≈ 1), it
dropped to approximately 0 .49 (on average) in the presence of unknown words, indicating a failure to
correctly recognize the sentence.
Table 1:Simulation 3. Comparison of two conditions: reading 100 sentences from Simulation 2 (4 words –
all known) versus 100 sentences in which one word was unknown, i.e., removed from the model’s vocabulary
(4 words – 1 unknown). The table summarizes the comparison between these two conditions, reporting
the mean, standard error, and p-value of t-tests for three key measures: sentence length, total number of
saccades during reading, and the probability of sentence recognition.
Sentence Length Total number of Saccades Probability
4 words – Known 25.96 ±1.96 4.36 ±0.11 1.00 (SE < 10−15)
4 words – 1 Unknown 25.46 ±1.92 12.04 ±0.44 0.49 ±0.04
p-value p ≈ 0.27 p <0.001 p <0.001
To illustrate the model’s behavior, we describe a representative example involving the reading of the
final four words of the sentence This paper is also framed in an offbeat manner , comparing two versions
of the model: one in which all the words exist in the model’s vocabulary (Figure 9(a)) and another in
which the word offbeat was removed from the vocabulary (Figure 9(b)). In the version with all known
words, the model behaves similarly to the previous simulations, accurately recognizing all the words and
the sentence. This is evident in Figure 9(a), where probabilities at Levels 2 and 3 quickly converge to
one after a few iterations.
Conversely, the version without knowledge of the word offbeat repeatedly makes saccades to the
syllables of this word (at Level 1) until it reaches the maximum number of iterations set for this simulation.
It correctly recognizes each syllable and could therefore potentially read the word aloud “non-lexically”
(and potentially add it to its vocabulary) [11]. However, it does not recognize the word (at Level 2) or the
sentence (at Level 3). As shown in Figure 9(b), both the probability of the unknown word at Level 2 and
the probability of the sentence at Level 3 remain below 0.5, indicating a failure of accurate recognition.
After failing to recognize the unknown word ( offbeat), the Level 2 inference settles on the most probable
alternative in the model’s vocabulary (e.g., upbeat). Unlike the simulation described earlier, here the
inference process continues even after an unrecognized word. As a result, the model is able to correctly
identify the final word in the sentence ( manner).
The example in Fig. 9 illustrates the distinction between reading a known versus a novel word and
the termination conditions for inference at each hierarchical level. The inference can terminate in two
ways: first, when the model reaches a threshold of confidence about the to-be-recognized syllable, word,
or sentence (i.e., the Expected Ambiguity over hidden states of Equation 5 falls below a fixed threshold,
χ(i) = 1/8). This happens when the model successfully infers a known syllable, word, or sentence.
Second, the inference can terminate when it reaches a maximum number of iterations K(i)
max at each
i-th level. This sets the maximum number of times Level 1 can jump between letters to recognize a
syllable, the maximum number of times Level 2 can jump between syllables to recognize a word, or the
maximum number of times Level 3 can jump between words to recognize a sentence. In our simulations,
for illustrative purposes, we set K(i)
max to very high values ( K(3)
max = 7 and K(2)
max = K(1)
max = 6) but these
values can be fine tuned to fit individual participants or participant groups.
As shown in Figure 9, Level 1 successfully infers all the syllables within 3 iterations and never reaches
K(1)
max. However, Level 2 fails to recognize the unknown word offbeat: it continues jumping between
the syllables off and beat until it reaches the maximum number of allowed iterations (6 iterations,
indicated in the grey panel). When the Level 2 inference of the current word is halted, a low confidence
message is reported to the level above while inference may still proceed with the next words (that are
successfully recognized). Similarly, Level 3 fails to infer the unknown sentence after the maximum number
of iterations (7 iterations), despite correctly recognizing most of the words. At the end of the simulation,
the model identifies that the cause of failure is at Level 2, as Level 1 correctly recognized all the syllables.
Consequently, the model reads the unknown word syllable-by-syllable, i.e. using the nonlexical route in
dual route theories of reading [11].
3.4 Simulation 4: Knowing the topic improves sentence recognition
In this section, we showcase another capability of the model that has not been discussed yet. The model
can be enriched with priors regarding the linguistic topic at each hierarchical level, such as the topic
of the sentence at the third level. This prior knowledge makes certain sentences a-priori more (or less)
likely, thus expediting the recognition process if the context aligns correctly.
To illustrate this, we conduct a task involving the recognition of 100 sentences, each containing 9
words, generated by BERT. These sentences comprise an average of 69.55±5.02 letters, ranging between
61 and 82 characters (see Table S.13). We allocate one-third of the sentences to each of the three topics
of the European Research Council (ERC): Physical Sciences and Engineering (PE), Life Sciences (LS),
and Social Sciences and Humanities (SH); see Table S.17 for a full specification of the sentences and their
0 0.5 1 1.5 2
0
0.5
1
p(C3|O)
Sentence
0 0.5 1 1.5 2
0
0.5
1
p(C2|O)
Word
0 0.5 1 1.5 2
time [s]
0
0.5
1
p(C1|O)
Syllable
(a)
(b)
Figure 9: Simulation 3 results: Reading known vs. unknown words. Evolution of the probabilities of
correctly recognized syllables (blue lines), words (orange lines), and sentences (yellow lines) over time, while
reading the final four words of the sentence This paper is also framed in an offbeat manner , in two cases: (a)
when all the words are known and (b) when the wordoffbeat is unknown (i.e., removed from the vocabulary).
The blue circles at the bottom level indicate saccades. The figure format is consistent with Figure 1(d).
8
8.5
9
9.5
10
10.5
11
11.5
12
Total number of saccades [#]
Flat
Priors
Informative
Priors
**
(a)
0.5
1
1.5
2
Number of backward saccades [#]
Flat
Priors
Informative
Priors (b)
Figure 10: Simulation 4 results: How prior information affects the reading process. The figure displays
the total saccades (a) and backward saccades (b) of two model versions: one with a flat prior about the
topic of the text (flat prior) and another with certain knowledge of the topic (informative prior). The
comparison highlights the impact of prior information on the number and nature of saccades during reading.
See Table S.11 and Table S.12 for detailed results and statistical comparisons.
respective topics. Please note that in this simulation, we only use topics at the third level.
We compare the performance of two versions of the model: one with a flat prior (33.3%) regarding
the sentence being read, and another with an informative prior that assigns a probability of 100% to the
correct topic. As depicted in Figure 10, the model with an informative prior requires significantly fewer
total saccades and backward saccades than the model with a flat prior (see Table S.11 and Table S.12 for
statistical comparisons). This advantage arises because knowing the topic allows the model to predict
certain words with high confidence, enabling it to skip unnecessary saccades during the reading process.
Intriguingly, despite the model with an informative prior skipping several words, it does not necessitate
more backward (corrective) saccades compared to the model with a flat prior . This indicates that the
prior information promotes a better speed-accuracy trade-off.
4 Discussion
In this paper, we presented a novel hierarchical active inference model of reading, which combines the
robust generative abilities of large language models [13, 67] with the inferential and information-gathering
capacities of active inference [23, 55].
Large language models, like BERT [13] and GPT (Generative Pre-trained Transformer) [67], have
demonstrated remarkable performance in various natural language processing tasks, including question
answering, language translation, text classification, and sentence prediction. These models can learn es-
sential linguistic structures without external supervision, making them valuable tools to explore language
processing in the brain [49, 29]. However, they lack the active strategies employed by human readers,
such as the ability to make saccades selectively on the most informative parts of the text [18, 51].
To capture active reading, we integrate BERT [13] into a hierarchical active inference framework
[23]. Active inference is a theoretical framework that describes how the brain employs probabilistic
inference over a generative model to effectively sample sensory inputs, minimizing variational free energy
or prediction errors – and that has been applied too many tasks, such as perceptual processing, goal-
directed navigation, robot control, and social interaction [37, 48, 53, 86, 61, 55]. For modeling reading,
we utilize a hierarchical generative model, aligning with evidence suggesting that language processing in
the brain operates through hierarchically organized predictions and prediction errors [77, 9, 34].
By placing BERT at the top of the hierarchy, we ensure accurate next-word predictions and the
model’s capability to tackle real-world reading tasks. Beyond next-word prediction, our hierarchical
model facilitates both inferring the content being read and predicting future elements across different
linguistic levels, such as syllables, words, and sentences. Our simulation results demonstrate the model’s
perfect accuracy in inferring words (Simulation 1) and sentences (Simulation 2) during reading. Addition-
ally, this hierarchical structure enables the model to read novel words not present in BERT’s vocabulary
(Simulation 3), assembling them syllable-by-syllable, representing the nonlexical (or sublexical) route
in dual route theories of reading [11]. Moreover, the model can incorporate prior knowledge, like the
topic of the sentence, to expedite the reading process (Simulation 4). Significantly, the model offers in-
terpretability through its alignment of distinct levels with established linguistic constituents (sentences,
words, syllables), which could correspond to distributed neural populations or cell assemblies within the
brain [34, 64]. While delving into the neural mechanisms that underlie this proposed framework extends
beyond the confines of this article, future studies could encompass the simulation of neuronal dynamics
during the inference process, in terms of variational free energy minimization [25, 38].
Using active inference, our model characterizes reading as an active, hypothesis testing process,
allowing us to simulate eye movements during reading. The core idea is that saccades are directed to the
most informative parts of the text, enabling the model to test its predictions and reduce uncertainty about
the text being read [18, 51, 14, 24]. Remarkably, this epistemic objective, aimed at uncertainty reduction,
emerges naturally from active inference, as explained in Section 2. In summary, our model offers a
comprehensive account of prediction-based written text processing, involving hierarchical inference of the
content being read, prediction of upcoming textual elements, and active testing of predictions through
saccades, which, in turn, inform further inference.
Interestingly, our model also enables the simulation of abnormal eye movement patterns observed in
reading disorders such as dyslexia [47, 35, 60]. While the simulations presented here serve as a proof of
concept, they qualitatively replicate empirical findings showing that individuals with dyslexia, compared
to neurotypical readers, exhibit fragmented text processing. This is characterized by an increased number
of shorter saccades during the reading of both single words (Simulation 1) and sentences (Simulation 2)
[92, 12, 36, 70]. Our model attributes these reading deficits to a specific disorder of predictive processing:
namely, an attenuation of the influence of prior information during inference [39]. By introducing noise
into the model’s transition functions, we impair its working memory of prior inference steps, thereby
forcing it to repeatedly gather evidence. This mechanism, which reflects the cumulative impact of noise
on inference, accounts for why perceptual difficulties in dyslexia become increasingly pronounced with
longer words and more complex sentence structures [92, 12].
However, it is crucial to note that our simulations only aim to provide a proof of concept that disorders
of reading such as dyslexia can be aligned with hierarchical predictive processing theories. Dyslexia is
highly heterogeneous, and its underlying causes are still heavily debated [56, 93, 68, 82, 83, 91]. It is
unlikely that the attenuation of priors [39] explored in this article would provide a comprehensive account
of all aspects of dyslexia. However, our model is easily extendable to incorporate other (non-alternative)
mechanisms that could contribute to reading impairments. For instance, various proposals point to
disorders of low-level information sampling and attention shifting [31], or a visual attention span disorder
characterized by a reduction in the number of letters that can be processed in parallel [59]. Additionally,
errors in eye movements (overshooting and undershooting) can be incorporated, by introducing noise in
the (transition) function mapping eye movements to subsequent spatial positions. These proposals (and
others) can be readily integrated into the model by modifying the likelihood function that maps letters
to syllables, as described in Section 2. Furthermore, while this study used predefined levels of noise in
the transition models, these parameters can be fit to human data. Such extensions may pave the way
for more comprehensive investigations of dyslexia within the predictive processing framework.
A limitation of this study is that for simplicity, it uses a relatively simple LLM: BERT [13]. We
selected BERT because it is computationally efficient and provides easy access to explicit probability
distributions over the next words and sentences. Future work could adopt more advanced LLM models,
to provide more robust linguistic predictions. Furthermore, we made some simplifying assumptions. For
example, we assumed that syllables, words and sentences are represented as sequence of letters and that
reading uses an absolute letter position coding. Future work could investigate alternative assumptions, see
for example [80] for a discussion of the absolute versus relative position coding in reading. Furthermore,
future work could replace the the priors over policies that make transitions between word locations
illustrated in Figure 2 with a more realistic distribution derived from empirical data. Future studies
could also explore more systematically the role of the different model parameters and hyperparameters
(e.g., word and sentence length, amount of noise) and address more complex linguistic datasets.
Additionally, future work could compare side by side the model presented here with established models
in the field, such as connectionist personalized models, which have been very successful in explaining
deficits observed in dyslexia [56, 91], and models that simulate eye movements, such as E-Z Reader
[72], SWIFT [17, 79, 65], ¨Uber-Reader [71, 87], Glenmore [73, 74], SEAM [66], the rational model of eye
movements [2], and OB1-Reader [81]. By fitting specific parameters of the presented model (e.g., different
levels of noise in the transition and/or likelihood functions), it would be possible to develop personalized
models (or computational phenotypes [78]), potentially shedding light on the mechanisms underlying
reading difficulties and informing the development of effective interventions to improve reading skills.
The model presented here could be expanded to capture hierarchically higher and more abstract
aspects of language processing, such as those related to shared narratives, frames and scripts, which
characterize our cultural niches [50, 76]. When opportunely placed within hierarchical architectures,
these higher-level constructs could produce a cascade of predictions that contextualize and guide lower-
level linguistic predictions and eye movements [5, 26, 57]. The presence of these additional constructs
creates a complex interplay between prior predictions at different hierarchical levels – for example, those
induced by scripts, statistical regularities of sentences and word frequencies – and between prediction
errors arising at different levels, which remain to be investigated in future studies.
In principle, the model presented here could also be extended to investigate developmental aspects
of language and reading acquisition. Using a pretrained LLM such as BERT can be interpreted as
mimicking the capabilities of adult readers. To simulate developmental trajectories, one could impose
constraints on BERT’s vocabulary or its ability to generate accurate sentence completions. Limiting
the vocabulary would force the model to rely more heavily on syllable-by-syllable reading, engaging a
non-lexical processing route. Reducing prediction accuracy would result in less confident priors, leading
the model to skip fewer words, commit more reading errors, and engage in more fixations and backward
saccades. Whether a model with these constraints can successfully replicate the patterns observed during
reading development remains to be evaluated in future studies.
Another important direction for future research is the integration of more realistic perceptual modules.
This would allow the model to capture the complexity of the feature extraction process (e.g., [84]), the
parallel processing of multiple letters (e.g., [30]), and other perceptual factors known to influence dyslexia,
such as the beneficial effects of increased letter spacing (e.g., [93]). A possible approach to realize this
integration is to employ neural networks as front-end perceptual modules that provide probabilistic
outputs (e.g., categorical distributions over letters or syllables), which can then be incorporated as
likelihoods within a Bayesian model [28, 75, 62].
Finally, it is worth noting that the hierarchical control architecture proposed here can be extended
to a wide range of cognitive tasks beyond reading. Many such tasks can be naturally decomposed
in hierarchical control terms by considering the separation of timescales between slower, higher-level
perceptual and action processes and faster, lower-level ones [4, 58, 41, 22]. For instance, spatial navigation
often involves high-level planning over goals or subgoals and low-level motor actions required to reach
them [86, 16, 85, 14]. Similarly, action observation and understanding can be structured hierarchically
by distinguishing between distal goals, proximal intentions, and immediate motor acts [63]. Future work
could adapt the hierarchical framework introduced here to explore these and other cognitive domains,
shedding light on how flexible, multi-timescale processing supports complex behavior.
Acknowledgments
This research received funding from the European Research Council under the Grant Agreement No.
820213 (ThinkAhead), the Italian National Recovery and Resilience Plan (NRRP), M4C2, funded by
the European Union – NextGenerationEU (Project IR0000011, CUP B51E22000150006, “EBRAINS-
Italy”; Project PE0000013, “FAIR”; Project PE0000006, “MNESYS”), and the Ministry of University
and Research, PRIN PNRR P20224FESY, PRIN 20229Z7M8N and PRIN 2020529PCP. The GEFORCE
Quadro RTX6000 and Titan GPU cards used for this research were donated by the NVIDIA Corporation.
We used a Generative AI model to correct typographical errors and edit language for clarity.
References
[1] Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Ken-
neth A Norman. Discovering event structure in continuous narrative perception and memory. Neu-
ron, 95(3):709–721, 2017.
[2] Klinton Bicknell and Roger Levy. A rational model of eye movement control in reading. In Proceed-
ings of the 48th annual meeting of the Association for Computational Linguistics , pages 1168–1178,
2010.
[3] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning. Springer,
2006.
[4] Matthew M Botvinick, Yael Niv, and Andew G Barto. Hierarchically organized behavior and its
neural foundations: A reinforcement learning perspective. cognition, 113(3):262–280, 2009.
[5] Nabil Bouizegarene, Maxwell JD Ramstead, Axel Constant, Karl J Friston, and Laurence J Kir-
mayer. Narrative as active inference: an integrative account of cognitive and social functions in
adaptation. Frontiers in Psychology, 15:1345480, 2024.
[6] Maria Pia Bucci, Dominique Br´ emond-Gignac, and Zo¨ ı Kapoula. Poor binocular coordination of
saccades in dyslexic children. Graefe’s archive for clinical and experimental ophthalmology, 246:417–
428, 2008.
[7] Reese Butterfuss and Panayiota Kendeou. The role of executive functions in reading comprehension.
Educational Psychology Review, 30:801–826, 2018.
[8] Cheryl M Capek, Daphne Bavelier, David Corina, Aaron J Newman, Peter Jezzard, and Helen J
Neville. The cortical organization of audio-visual sentence comprehension: an fmri study at 4 tesla.
Cognitive brain research, 20(2):111–119, 2004.
[9] Charlotte Caucheteux, Alexandre Gramfort, and Jean-R´ emi King. Evidence of a predictive coding
hierarchy in the human brain listening to speech. Nature human behaviour, 7(3):430–441, 2023.
[10] Claire HC Chang, Samuel A Nastase, and Uri Hasson. Information flow across the cortical
timescale hierarchy during narrative construction. Proceedings of the National Academy of Sciences,
119(51):e2209307119, 2022.
[11] Max Coltheart. Modeling reading: The dual-route approach. The science of reading: A handbook ,
1:6–23, 2005.
[12] Maria De Luca, Enrico Di Pace, Anna Judica, Donatella Spinelli, and Pierluigi Zoccolotti. Eye
movement patterns in linguistic and non-linguistic tasks in developmental surface dyslexia. Neu-
ropsychologia, 37(12):1407–1420, 1999.
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. NAACL HLT 2019 - 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies - Proceedings of the Conference, 1:4171 – 4186, 2019. Cited by: 22093.
[14] Francesco Donnarumma, Marcello Costantini, Ettore Ambrosini, Karl Friston, and Giovanni Pez-
zulo. Action perception as hypothesis testing. Cortex, 89:45–60, 2017.
[15] Francesco Donnarumma, Haris Dindo, Pierpaolo Iodice, and Giovanni Pezzulo. You cannot speak
and listen at the same time: A probabilistic model of turn-taking. Biological cybernetics, 111(2):165–
183, 2017.
[16] Francesco Donnarumma, Domenico Maisto, and Giovanni Pezzulo. Problem solving as probabilistic
inference with subgoaling: explaining human successes and pitfalls in the tower of hanoi. PLoS
computational biology, 12(4):e1004864, 2016.
[17] Ralf Engbert, Antje Nuthmann, Eike M Richter, and Reinhold Kliegl. Swift: a dynamical model of
saccade generation during reading. Psychological review, 112(4):777, 2005.
[18] Marcello Ferro, Dimitri Ognibene, Giovanni Pezzulo, and Vito Pirrelli. Reading as active sensing:
a computational model of gaze planning during word recognition. Frontiers in Neurorobotics, 4:6,
2010.
[19] Stefan L Frank, Leun J Otten, Giulia Galli, and Gabriella Vigliocco. The erp response to the amount
of information conveyed by words in sentences. Brain and language , 140:1–11, 2015.
[20] Leon Franzen, Zoey Stark, and Aaron P Johnson. Individuals with dyslexia use a different visual
sampling strategy to read text. Scientific reports, 11(1):6449, 2021.
[21] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B:
Biological sciences, 360(1456):815–836, 2005.
[22] Karl Friston. Hierarchical models in the brain. PLoS computational biology, 4(11):e1000211, 2008.
[23] Karl Friston. The free-energy principle: a unified brain theory? Nature reviews neuroscience ,
11(2):127–138, 2010.
[24] Karl Friston, Rick A Adams, Laurent Perrinet, and Michael Breakspear. Perceptions as hypotheses:
saccades as experiments. Frontiers in psychology, 3:151, 2012.
[25] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo.
Active inference: a process theory. Neural computation, 29(1):1–49, 2017.
[26] Karl J Friston, Maxwell JD Ramstead, Alex B Kiefer, Alexander Tschantz, Christopher L Buckley,
Mahault Albarracin, Riddhi J Pitliya, Conor Heins, Brennan Klein, Beren Millidge, et al. Designing
ecosystems of intelligence from first principles. Collective Intelligence , 3(1):26339137231222481,
2024.
[27] Karl J Friston, Noor Sajid, David Ricardo Quiroga-Martinez, Thomas Parr, Cathy J Price, and
Emma Holmes. Active listening. Hearing research, 399:107998, 2021.
[28] Dileep George, Wolfgang Lehrach, Ken Kansky, Miguel L´ azaro-Gredilla, Christopher Laan, Bhaskara
Marthi, Xinghua Lou, Zhaoshi Meng, Yi Liu, Huayan Wang, et al. A generative vision model that
trains with high data efficiency and breaks text-based captchas. Science, 358(6368):eaag2612, 2017.
[29] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A
Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational principles for
language processing in humans and deep language models. Nature neuroscience, 25(3):369–380,
2022.
[30] T Hannagan, A Agrawal, L Cohen, and S Dehaene. Emergence of a compositional neural code for
written words: Recycling of a convolutional neural network for reading. Proceedings of the National
Academy of Sciences, 118(46):e2104779118, 2021.
[31] Riitta Hari, Hanna Renvall, and Topi Tanskanen. Left minineglect in dyslexic adults. Brain,
124(7):1373–1380, 2001.
[32] Uri Hasson, Janice Chen, and Christopher J Honey. Hierarchical process memory: memory as an
integral component of information processing. Trends in cognitive sciences, 19(6):304–313, 2015.
[33] Stefan Hawelka, Benjamin Gagl, and Heinz Wimmer. A dual-route perspective on eye movements
of dyslexic readers. Cognition, 115(3):367–379, 2010.
[34] Micha Heilbron, Kristijan Armeni, Jan-Mathijs Schoffelen, Peter Hagoort, and Floris P De Lange.
A hierarchy of linguistic predictions during natural language comprehension. Proceedings of the
National Academy of Sciences , 119(32):e2201968119, 2022.
[35] Tzipi Horowitz-Kraus, Nicole Cicchino, Merav Amiel, Scott K Holland, and Zvia Breznitz. Reading
improvement in english-and hebrew-speaking children with reading difficulties after reading accel-
eration training. Annals of dyslexia , 64:183–201, 2014.
[36] Florian Hutzler and Heinz Wimmer. Eye movements of dyslexic children when reading in a regular
orthography. Brain and language , 89(1):235–242, 2004.
[37] Jungsik Hwang, Jinhyung Kim, Ahmadreza Ahmadi, Minkyu Choi, and Jun Tani. Dealing with
large-scale spatio-temporal patterns in imitative interaction between a robot and a human by using
the predictive coding framework. IEEE Transactions on Systems, Man, and Cybernetics: Systems ,
50(5):1918–1931, 2018.
[38] Takuya Isomura, Kiyoshi Kotani, Yasuhiko Jimbo, and Karl Friston. Experimental validation of the
free-energy principle with in vitro neural networks. bioRxiv, pages 2022–10, 2022.
[39] Sagi Jaffe-Dax, Ofri Raviv, Nori Jacoby, Yonatan Loewenstein, and Merav Ahissar. A computa-
tional model of implicit memory captures dyslexics’ perceptual deficits. Journal of Neuroscience ,
35(35):12116–12126, 2015.
[40] Stephanie Jainta and Zo¨ ı Kapoula. Dyslexic children are confronted with unstable binocular fixation
while reading. PloS one, 6(4):e18694, 2011.
[41] Etienne Koechlin and Christopher Summerfield. An information theoretical approach to prefrontal
executive function. Trends in cognitive sciences, 11(6):229–235, 2007.
[42] Miika Koskinen, Mikko Kurimo, Joachim Gross, Aapo Hyv¨ arinen, and Riitta Hari. Brain activity
reflects the predictability of word sequences in listened continuous speech. Neuroimage, 219:116936,
2020.
[43] Marta Kutas and Steven A Hillyard. Brain potentials during reading reflect word expectancy and
semantic association. Nature, 307(5947):161–163, 1984.
[44] Franklin Mark Liang. Word Hy-phen-a-tion by Com-put-er . PhD thesis, Citeseer, 1983.
[45] G Reid Lyon, Sally E Shaywitz, and Bennett A Shaywitz. A definition of dyslexia. Annals of
dyslexia, 53:1–14, 2003.
[46] Manfred MacKeben, Susanne Trauzettel-Klosinski, Jens Reinhard, Ute D¨ urrw¨ achter, Martin Adler,
and Gunther Klosinski. Eye movement control during single-word reading in dyslexics. Journal of
Vision, 4(5):4–4, 2004.
[47] Jos´ e M Maisog, Erin R Einbinder, D Lynn Flowers, Peter E Turkeltaub, and Guinevere F Eden. A
meta-analysis of functional neuroimaging studies of dyslexia. Annals of the new York Academy of
Sciences, 1145(1):237–259, 2008.
[48] Domenico Maisto, Francesco Donnarumma, and Giovanni Pezzulo. Interactive inference: a multi-
agent model of cooperative joint actions. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, 2023.
[49] Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emer-
gent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the
National Academy of Sciences , 117(48):30046–30054, 2020.
[50] Marvin Minsky. Society of mind . Simon and Schuster, 1986.
[51] Dennis Norris. The bayesian reader: explaining word recognition as an optimal bayesian decision
process. Psychological review, 113(2):327, 2006.
[52] Beth A O’Brien, J Stephen Mansfield, and Gordon E Legge. The effect of print size on reading
speed in dyslexia. Journal of Research in Reading , 28(3):332–349, 2005.
[53] Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a
humanoid robot. IEEE Transactions on Cognitive and Developmental Systems, 14(2):462–471, 2021.
[54] Giorgio Parisi. Statistical field theory . Frontiers in physics. Addison-Wesley, Redwood City, CA,
1988.
[55] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in
mind, brain, and behavior . MIT Press, 2022.
[56] Conrad Perry, Marco Zorzi, and Johannes C Ziegler. Understanding dyslexia through personalized
large-scale computational models. Psychological science, 30(3):386–395, 2019.
[57] Giovanni Pezzulo, G¨ unther Knoblich, Domenico Maisto, Francesco Donnarumma, Elisabeth
Pacherie, and Uri Hasson. A predictive processing framework for joint action and communication.
PsyArXiv Preprints, 2025.
[58] Giovanni Pezzulo, Francesco Rigoli, and Karl J Friston. Hierarchical active inference: a theory of
motivated control. Trends in cognitive sciences, 22(4):294–306, 2018.
[59] Chlo´ e Prado, Matthieu Dubois, and Sylviane Valdois. The eye movements of dyslexic children during
reading and visual search: impact of the visual attention span. Vision research, 47(19):2521–2530,
2007.
[60] Cathy J Price. A review and synthesis of the first 20 years of pet and fmri studies of heard speech,
spoken language and reading. Neuroimage, 62(2):816–847, 2012.
[61] Matteo Priorelli, Federico Maggiore, Antonella Maselli, Francesco Donnarumma, Domenico Maisto,
Francesco Mannella, Ivilin Peev Stoianov, and Giovanni Pezzulo. Modeling motor control in
continuous-time active inference: a survey. IEEE Transactions on Cognitive and Developmental
Systems, 2023.
[62] Matteo Priorelli and Ivilin Peev Stoianov. Flexible intentions: An active inference theory. Frontiers
in Computational Neuroscience, 17:1128694, 2023.
[63] Riccardo Proietti, Giovanni Pezzulo, and Alessia Tessari. An active inference model of hierarchical
action understanding, learning and imitation. Physics of Life Reviews , 46:92–118, 2023.
[64] Friedemann Pulverm¨ uller. Words in the brain’s language.Behavioral and brain sciences, 22(2):253–
279, 1999.
[65] Maximilian M Rabe, Johan Chandra, Andr´ e Kr¨ ugel, Stefan A Seelig, Shravan Vasishth, and Ralf
Engbert. A bayesian approach to dynamical modeling of eye-movement control in reading of normal,
mirrored, and scrambled texts. Psychological Review, 128(5):803, 2021.
[66] Maximilian M Rabe, Dario Paape, Daniela Mertzen, Shravan Vasishth, and Ralf Engbert. Seam: An
integrated activation-coupled model of sentence processing and eye movements in reading. Journal
of Memory and Language , 135:104496, 2024.
[67] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[68] Franck Ramus. Developmental dyslexia: specific phonological deficit or general sensorimotor dys-
function? Current opinion in neurobiology , 13(2):212–218, 2003.
[69] Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpre-
tation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79–87, 1999.
[70] Keith Rayner. Eye movements, perceptual span, and reading disability. Annals of Dyslexia , pages
163–173, 1983.
[71] Erik D Reichle. Computational models of reading: A handbook . Oxford University Press, 2021.
[72] Erik D Reichle, Keith Rayner, and Alexander Pollatsek. Toward a model of eye movement control
in reading. Psychological review, 105(1):125, 1998.
[73] Ronan G Reilly and R Radach. Glenmore: An interactive activation model of eye movement control
in reading. In Proceedings of the 9th International Conference on Neural Information Processing,
2002. ICONIP’02., volume 3, pages 1194–1200. IEEE, 2002.
[74] Ronan G Reilly and Ralph Radach. Some empirical tests of an interactive activation model of eye
movement control in reading. Cognitive Systems Research, 7(1):34–55, 2006.
[75] Cansu Sancaktar, Marcel AJ Van Gerven, and Pablo Lanillos. End-to-end pixel-based deep active
inference for body perception and action. In 2020 Joint IEEE 10th International Conference on
Development and Learning and Epigenetic Robotics (ICDL-EpiRob) , pages 1–8. IEEE, 2020.
[76] Roger C Schank and Robert P Abelson. Scripts, plans, goals, and understanding: An inquiry into
human knowledge structures . Psychology press, 2013.
[77] Lea-Maria Schmitt, Julia Erb, Sarah Tune, Anna U Rysop, Gesa Hartwigsen, and Jonas Obleser.
Predicting speech from a cortical hierarchy of event-based time scales. Science Advances ,
7(49):eabi6070, 2021.
[78] Philipp Schwartenbeck and Karl Friston. Computational phenotyping in psychiatry: a worked
example. eneuro, 3(4), 2016.
[79] Joshua D Seelig, Tyler Jay, Alison R Preston, Gabriele Fey, and Robert L Goldstone. Bayesian
surprise predicts human event segmentation in story listening. bioRxiv, 2020.
[80] Joshua Snell, Jonathan Grainger, and Martijn Meeter. Relative letter-position coding revisited.
Psychonomic Bulletin & Review , 29(3):995–1002, 2022.
[81] Joshua Snell, Sam van Leipsig, Jonathan Grainger, and Martijn Meeter. Ob1-reader: A model of
word recognition and eye movements in text reading. Psychological review, 125(6):969, 2018.
[82] John Stein and Vincent Walsh. To see but not to read; the magnocellular theory of dyslexia. Trends
in neurosciences, 20(4):147–152, 1997.
[83] Paula Tallal. Improving language and literacy is a matter of time. Nature Reviews Neuroscience,
5(9):721–728, 2004.
[84] Alberto Testolin, Ivilin Stoianov, and Marco Zorzi. Letter perception emerges from unsupervised
deep learning and recycling of natural image features. Nature human behaviour, 1(9):657–664, 2017.
[85] Momchil S Tomov, Samyukta Yagati, Agni Kumar, Wanqian Yang, and Samuel J Gershman. Discov-
ery of hierarchical representations for efficient planning.PLoS computational biology, 16(4):e1007594,
2020.
[86] Toon Van de Maele, Bart Dhoedt, Tim Verbelen, and Giovanni Pezzulo. Bridging cognitive maps:
A hierarchical active inference model of spatial alternation tasks and the hippocampal-prefrontal
circuit. arXiv preprint arXiv:2308.11463 , 2023.
[87] Aaron Veldre, Lili Yu, Sally Andrews, and Erik D Reichle. Towards a complete model of reading:
Simulating lexical decision, word naming, and sentence reading with ¨ uber-reader. InProceedings of
the 42nd annual conference of the cognitive science society . Cognitive Science Society, 2020.
[88] Hugo Weissbart, Katerina D Kandylaki, and Tobias Reichenbach. Cortical tracking of surprisal
during continuous speech comprehension. Journal of cognitive neuroscience, 32(1):155–166, 2020.
[89] Roel M Willems, Stefan L Frank, Annabel D Nijhof, Peter Hagoort, and Antal Van den Bosch.
Prediction during natural language comprehension. Cerebral Cortex, 26(6):2506–2516, 2016.
[90] Asieh Zadbood, Janice Chen, Yuan Chang Leong, Kenneth A Norman, and Uri Hasson. How we
transmit memories to other brains: constructing shared neural representations via communication.
Cerebral cortex, 27(10):4988–5000, 2017.
[91] Johannes C Ziegler, Conrad Perry, and Marco Zorzi. Learning to read and dyslexia: From theory
to intervention through personalized computational models. Current Directions in Psychological
Science, 29(3):293–300, 2020.
[92] Pierluigi Zoccolotti, Maria De Luca, Enrico Di Pace, Filippo Gasperini, Anna Judica, and Donatella
Spinelli. Word length effect in early reading and in developmental dyslexia. Brain and language ,
93(3):369–373, 2005.
[93] Marco Zorzi, Chiara Barbiero, Andrea Facoetti, Isabella Lonciari, Marco Carrozzi, Marcella Mon-
tico, Laura Bravar, Florence George, Catherine Pech-Georgel, and Johannes C Ziegler. Extra-
large letter spacing improves reading in dyslexia. Proceedings of the National Academy of Sciences ,
109(28):11455–11459, 2012.
Supplementary materials
Integrating Large Language Models
and Active Inference
to understand eye movements
in Reading and Dyslexia
Francesco Donnarumma
Mirco Frosolone
Giovanni Pezzulo
Institute of Cognitive Sciences and Technologies
National Research Council
Via Gian Domenico Romagnosi, 18A
00196 Rome Italy
Corresponding author: Giovanni Pezzulo E-mail: giovanni.pezzulo@istc.cnr.it
1
Supplementary info
In this document, we provide the tables of statistics from the simulations presented in the main article
(found in Supplementary Tables section) and the Datasets of sentences and words used in the simulation
(found in Datasets section).
Supplementary tables
Below are the supplementary tables presenting statistics related to the simulations discussed in the main
manuscript.
Supplementary tables for Simulation 1
Table S.1 displays the accuracy and probability assigned to the correct words. Table S.2 shows the
T-tests on total saccades and Table S.3 the backward saccades. Table S.4 and Table S.5 presents the
T-tests for the amplitude of forward and backward saccades respectively. The p-values lower to 0.05 are
indicated in bold.
Table S.1: Simulation 1: Reading words of 4 or 8 letters. The table presents the accuracy and the
probability assigned to the correct words by 4 different models: the Control model (CM), the Dyslexic model
(DM) variant with noise at Level 1 only, the Dyslexic model (DM) variant with noise at Level 2 only, and
the Dyslexic model (DM) with noise at both levels. See Figure 4 and the main text for details.
4 - letter word 8 - letter word
AccuracyProbability AccuracyProbability
Control Model 100 1.00±1.03·10−14 100 1.00±7.85·10−15
DM -Noise on Level 1100 1.00±3.32·10−04 100 0.99±3.82·10−04
DM -Noise on Level 299 0.99±3.73·10−15 97 0.97±1.38·10−02
Dyslexic Model 99 0.99±3.32·10−04 97 0.97±1.15·10−02
Table S.2: Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays
the results of T-tests conducted on the total number of saccades for different models with respect to the
Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 5 and the main text for details.
Total number of saccades
4 - letter word 8 - letter word
DM - Noise Level 1 F [1, 198] = 17 .6, p ≈ 4.1 · 10−05 F [1, 198] = 12 .7, p ≈ 4.5 · 10−04
DM - Noise Level 2 F [1, 196] = 4 .4, p ≈ 3.6 · 10−02 F [1, 190] = 5 .2, p ≈ 2.4 · 10−02
Dyslexic Model F [1, 196] = 16 .7, p ≈ 6.4 · 10−05 F [1, 192] = 10 .7, p ≈ 1.3 · 10−03
Supplementary tables for Simulation 2
Table S.6 display the accuracy and probability assigned to the correct words. Table S.7 and Table S.8
shows the T-tests on total saccades and back saccades. Table S.9 presents the T-tests for amplitude of
forward and Table S.10 the T-test for amplitude of backward saccades respectively. The p-values lower
to 0.05 are indicated in bold.
2
Table S.3: Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays
the results of T-tests conducted on the number of backward saccades for different models with respect to
the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 5 and the main text for details.
Number of backward saccades
4 - letter word 8 - letter word
DM - Noise Level 1 F [1, 198] = 21 .5, p ≈ 6.5 · 10−06 F [1, 198] = 17 .5, p ≈ 4.4 · 10−05
DM - Noise Level 2 F [1, 198] = 4 .8, p ≈ 3.0 · 10−02 F [1, 190] = 5 .7, p ≈ 1.8 · 10−02
Dyslexic Model F [1, 196] = 21 .7, p ≈ 5.8 · 10−06 F [1, 192] = 13 .3, p ≈ 3.4 · 10−04
Table S.4: Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays
the results of T-tests conducted on the amplitude of forward saccades for different models with respect to
the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 6 and the main text for details.
Amplitude of forward saccades
4 - letter word 8 - letter word
Dyslexic Model F [1 , 844] = 9 .4, p ≈ 2.2 · 10 −03 F [1 , 678] = 46 .5, p ≈ 2.0 · 10 −11
Table S.5:Simulation 1. T-tests for simulation of 4-letter and 8-letter word reading. The table displays the
results of T-tests conducted on the amplitude of + backward saccades for different models with respect to
the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, and the Dyslexic model (DM) with noise at both
levels. See Figure 6 and the main text for details.
Amplitude of backward saccades
4 - letter word 8 - letter word
Dyslexic Model F [1, 321] = 133 .5, p ≈ 4.6 · 10 −26 F [1, 312] = 24 .5, p ≈ 1.2 · 10 −06
Table S.6: Simulation 2: Reading sentences of 4 or 8 words. The table presents the accuracy and the
probability assigned to the correct words by 4 different models: the Control model (CM), the Dyslexic model
(DM) variant with noise at Level 1 only, the Dyslexic model (DM) variant with noise at Level 2 only, the
Dyslexic model (DM) variant with noise at Level 3 only, and the Dyslexic model (DM) with noise at all
levels. See Figure 4 and the main text for details.
4 - word sentence 8 - word sentence
Accuracy Probability Accuracy Probability
Control Model 100 1.00 ± 3.66 · 10−16 100 1.00 ± 3.21 · 10−12
DM - Noise on Level 1 100 1.00 ± 3.64 · 10−06 100 1.00 ± 5.69 · 10−08
DM - Noise on Level 2 100 1.00 ± 3.40 · 10−07 100 1.00 ± 3.21 · 10−12
DM - Noise on Level 3 97 0.97 ± 4.25 · 10−03 93 0.93 ± 1.04 · 10−02
Dyslexic Model 96 0.96 ± 4.41 · 10−03 93 0.93 ± 1.04 · 10−02
3
Table S.7:Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the total number of saccades for different models with respect to the
Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 7 and the main text for
details.
Total number of saccades
4 - word sentence 8 - word sentence
DM - Noise Level 1 F [1, 198] = 0 .02 , p ≈ 0.90 F [1, 198] = 0 .01 , p ≈ 0.92
DM - Noise Level 2 F [1, 198] = 0 .97 , p ≈ 0.45 F [1, 198] = 0 .45 , p ≈ 0.58
DM - Noise Level 3 F [1, 198] = 66 .8, p ≈ 3.5 · 10 −14 F [1, 196] = 105 .9, p ≈ 3.9 · 10 −20
Dyslexic Model F [1, 198] = 75 .5, p ≈ 1.4 · 10 −15 F [1, 196] = 113 .6, p ≈ 3.2 · 10 −21
Table S.8:Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the number of backward saccades for different models with respect to
the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 7 and the main text for
details.
Number of backward saccades
4 - word sentence 8 - word sentence
DM - Noise Level 1 F [1, 198] = 0 .45 , p ≈ 0.52 F [1, 198] = 0 .82 , p ≈ 0.46
DM - Noise Level 2 F [1, 198] = 0 .01 , p ≈ 0.90 F [1, 198] = 1 .86 , p ≈ 0.17
DM - Noise Level 3 F [1, 198] = 44 .7, p ≈ 2.3 · 10 −10 F [1, 196] = 119 .4, p ≈ 5.1 · 10 −22
Dyslexic Model F [1, 198] = 53 .2, p ≈ 7.1 · 10 −12 F [1, 196] = 117 .7, p ≈ 8.8 · 10 −22
Table S.9:Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the amplitude of forward saccades for different models with respect to
the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 8 and the main text for
details.
Amplitude of forward saccades
4 - word sentence 8 - word sentence
Dyslexic Model F [1 , 878] = 18 .0, p ≈ 2.5 · 10 −05 F [1 , 1238] = 10 .9, p ≈ 9.8 · 10 −04
Simulation 4
Table S.11 and Table S.12 presents T-tests on total number of saccades and the number backward
saccades when the model is augmented with the topic of the sentence at Level 3 of the hierarchy. The
p-values lower to 0.05 are indicated in bold.
4
Table S.10:Simulation 2. T-tests for simulation of 4-word and 8-word sentence reading. The table displays
the results of T-tests conducted on the amplitude of backward saccades for different models with respect to
the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only, the Dyslexic
model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with noise at Level
3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 8 and the main text for
details.
Amplitude of backward saccades
4 - word sentence 8 - word sentence
Dyslexic Model F [1 , 318] = 14 .7, p ≈ 1.5 · 10 −04 F [1 , 350] = 42 .5, p ≈ 2.5 · 10 −10
Table S.11:Simulation 4. T-tests for simulation of a 9-word sentence reading, belonging to different topics.
The table displays the results of T-tests conducted on the total number of saccades for different models with
respect to the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable) only,
the Dyslexic model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant with
noise at Level 3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 10 and
the main text for details.
Total number of saccades
Flat Priors VS Informative Priors F [1 , 198] = 4 .1, p ≈ 4.1 · 10 −03
Table S.12:Simulation 4. T-tests for simulation of a 9-word sentence reading, belonging to different topics.
The table displays the results of T-tests conducted on the number of backward saccades for different models
with respect to the Control model (CM): the Dyslexic model (DM) variant with noise at Level 1 (syllable)
only, the Dyslexic model (DM) variant with noise at Level 2 (word) only, the Dyslexic model (DM) variant
with noise at Level 3 (sentence) only, and the Dyslexic model (DM) with noise at all levels. See Figure 10
and the main text for details.
Number of backward saccades
Flat Priors VS Informative Priors F [1, 198] = 2 .6, p ≈ 0.11
Datasets
In this section, we present the datasets utilized in our simulations. Information regarding content length
in the datasets can be found in Table S.13. Furthermore, Table S.14 displays the dictionary of words used
in Simulation 1, while Table S.15 provides insight into the sentences used in Simulation 2. Additionally,
Simulation 4’s sentences and corresponding labels are showcased in Table S.17.
Table S.13: This table reports the mean number of characters, the standard deviation, minimum and
maximum number of characters that compose the relative set of sentences used in the simulations.
Mean Standard
Deviation Min Max
Simulation 2 (4 words) 25.96 1.96 21 31
Simulation 2 (8 words) 50.24 1.64 47 55
Simulation 3 (4 words - 1 Unknown)25.46 1.92 22 32
Simulation 4 69.55 5.02 61 82
5
Table S.14: Dictionary used in Simulation 1 (words extracted from BERT Dictionary)
1 - letter 2- letter 3- letter 4- letter 5- letter 6- letter 7- letter 8- letter
A AC AMC ARES AGREE ACTING AIRPORT ACQUIRED
B AL ARM ARTS ANZAC ADVENT ALCOHOL ACTUALLY
C AM ATA ASKS ARDEN AIRMEN ANGRILY ADVANCED
D AR AUS BEDS ARIEL ANGLIA ARCADIA ALPHABET
E AW BAO BORE AZURE ARMAND ASSURED AMERICAN
F AX BEG BRET BARED AUSTEN AVENUES APPLETON
G BF BEN BUSY BEGUN BECKER BALANCE ARGUMENT
H BI BIT BUTT BIRDS BEIRUT BENGALS ARRANGED
I BK BOP CALE BLAND BERMAN BERWICK ATLANTIC
J BO BUY CARL BUICK BIGGER BOUNCED BALANCED
K BR CAB COAL BULGE BISHOP BURUNDI BASILICA
L BT CAD COLA BUTCH BUMPER CAREERS BENEDICT
M CA CAI COOL CELLO BURDEN CEILING BOTSWANA
N CB CEO CUBS CLEAR CANTOR CHANCEL BOULDERS
O CH COE DIRT CLUBS CAUSAL CLICKED BREACHED
P CL CPU DOIN CODES CEREAL CLIENTS BREAKING
Q CM DEX DOVE COSTS CESARE COMMITS CADILLAC
R CN DIG DRIP CRIED CESSNA CROOKED CAMPBELL
S CO DIP DRUM CURLY CHICKS DERRICK CARRYING
T CP EKI ELLE DANNY CLOCKS DESCENT CATALINA
U CU EPA EMIR DATED COINED DESPAIR COLISEUM
V ED ETA EMMA DEBUT DECKER DEVISED COLUMBIA
W EH FIG FANS DISCS DISMAY EDUARDO CONNECTS
X EL FLU FEES DOUBT DRINKS EMBASSY CONVINCE
Y EM FUN FIJI DUCKS EXEMPT EMERSON CUMMINGS
Z EU HAN FINS DUMMY FACADE EMPLOYS CURRENCY
FI HBO FLOP ECOLE FINLEY ENEMIES CYCLONES
FL HEM FLUX ELIZA GENTRY ESCAPES CYLINDER
FT HIT FRAN ESSEN HAILEY EXHAUST DARKENED
FU HOP GIFT FEMME HANNAH FAINTLY DECEASED
FX HUE GIGS FISTS HASSAN FLEDGED DECIDING
GE IBN GLOW FLOOR HATRED FORSTER DISABLED
GI ICE GRAM FLOYD HELMUT FREEWAY DISASTER
GO ICH GREY GOUGH HOOVER FRONTED DONNELLY
GS ICT HAAS GROWS HUTTON GALILEO EMERITUS
GT ILE HANK GRUNT HYBRID GEOLOGY EMPLOYER
GU ION HILL GUARD INCOME GLIDING EPILOGUE
HA IRA HISS HELLO INPUTS HAMBURG EREBIDAE
HC KIM HUEY HIRES INTERN HANDLED EVACUATE
HM LAL INDY HORDE JOSEPH HIGHEST EVOLVING
HO LAY JACE JOINT JUAREZ HONNEUR EXAMINER
HU LEG JAIN JOYCE KHYBER HORNETS FABULOUS
IK LES JEAN LATER LANDON HOWEVER FIGHTERS
IN LOU JING LEANS LENGTH HOWLING FRANKLIN
IO LTD KRIS LEAVE LENNON HUNCHED FRICTION
IR MED LAMA LOOSE LIKELY INHALED GESTURES
IX MPH LANG LOVER LILITH IRANIAN GROUPING
JA MPS LETO LUCHA LITTER JEALOUS HOMICIDE
JD MSC LIAR LYDIA MADRAS JUSTINE IMPERIAL
KE MUD MACK MAHAL MANUEL KNIGHTS INCURRED
KG NBA MARS MERGE MARGIN LABELED INITIATE
KN NHL MASK MINSK MARKED LASTING INJURIES
KO NHS MEAT MISSY MCLEAN LEBANON INSANITY
KS NIK MIKA MIXED MOLINA MANNING JUDICIAL
LC NOS MOJO MOMMA MUSEUM MCBRIDE LABRADOR
LI NOV MOOD NAILS MYSELF MORALLY MAGAZINE
6
LP NPR MUCH NORMA NEWELL MUSCLED MARATHON
MG NYC NEAT OLDER ONIONS NATALIE MEANINGS
MI OFF NINO OUTTA OPENLY NEURONS MOHAMMAD
ML OUT NOSE PAPUA OPPOSE NEVILLE MONSIEUR
MR PAT NRHP PATIO PANZER NIKOLAI MOROCCAN
NA PAU OBOE PIANO PAVING NILSSON MOUNTING
NH PBA OWLS PILES PAYTON NOMINAL MUSHROOM
NI PBS PASS PLUTO PETALS OFFENSE OLYMPIAD
NO PCS PERU PROSE PILOTS ONSTAGE PACKAGES
NS PEI PORN QAEDA PLANES PATCHES PATERSON
NT PEW RAAF RALLY POLITE PEPTIDE PHILLIPS
NZ PHI RAMP REEFS POTION PERFECT POPULACE
OG PHP RAMS REVUE PRAGUE PEUGEOT POUNDING
OH POD RUDD ROACH PRICED PIONEER PREACHER
OL PUN SANK ROUEN PULSES PLANNED PRESTIGE
OP ROE SAXE RUSSO PUPILS PLAYFUL QUARTERS
OS ROM SITU SCOTT RASHID PORTICO RAILROAD
OZ RUM SIZE SHORE REPORT PROTEST RECEIVES
PH RYE SLID SINGH RIPLEY RAPIDLY REJOINED
PO SAO SODA SITED ROBERT READERS SCENARIO
PR SEA SOON SLAMS RUNNER RECITAL SEMINOLE
RC SHI STAY SPRAY SAILOR REDWOOD SHOOTOUT
RR SHU STYX SQUAT SAXONY REISSUE SHOWERED
RY SPP SUFI STADE SCARES ROOSTER SINGULAR
SE SSR SWAM STAFF SCREAM RUINING SOMERSET
SF SUE TEAM STALL SEIZED SECURED SOUTHEND
SM TAI THRU STATE SESAME SILENCE SPECIALS
SQ TEA TODD STERN SEVERN SMASHED SPEEDING
SR TEX TOLD STORM SIGNAL SPELLED STRANDED
TC TIS TOPS STUMP SPEAKS STATION STRIPPED
TU TNA TRIP TOUGH SPINES STEWART SYMBOLIC
TV UFO TUNA TRANS STALLS STRANGE TARGETED
TX VAR UGLY TREAT STARTS STRIKES TAXATION
UP VII USER TREES SURFER SWELLED THROTTLE
UR WAT VEGA VAULT TORINO TIGHTLY TIMELINE
VA WAY VIDA VIJAY TREVOR TOPICAL TOURISTS
VE WOW VISA VISAS TURTLE TURTLES UNCOMMON
VP XII WEEP VOMIT VENDOR ULYSSES VALENTIN
VU XVI WHOA WEARS VERBAL UNITING VIGOROUS
WC YES WIFE WHORE WANTED UNNAMED VILLAINS
XI YET WINS WRAPS WASHED VACANCY WARRIORS
YA YOO WITS WRIST WIRING VISIBLE WEREWOLF
YE ZEE YUKI YEMEN WORTHY WALKING WORRYING
YO ZEV ZOOM YOUTH XAVIER WARTIME ZIMBABWE
7
Table S.15: Dictionary used in Simulation 2
FAITH CONQUERS ALL OBSTA-
CLES
FAITH CONQUERS ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
FAITH CONQUERS EACH HURDLE FAITH CONQUERS ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
FAITH CONQUERS EACH SET-
BACK
FAITH CONQUERS EACH HURDLE BUT TRASCENDS ALL MISFORTUNE
FAITH CONQUERS EVERY CHAL-
LENGES
FAITH CONQUERS EACH SETBACK BUT SURMOUNTS ALL OBSTACLES
FAITH CONQUERS EVERY OB-
STACLES
FAITH CONQUERS EVERY OBSTACLES AND PREVAILS OVER ADVERSITY
FAITH DEFEATS ALL OBSTACLES FAITH CONQUERS EVERY OBSTACLES AND SURMOUNTS ALL OBSTACLES
FAITH OVERCOMES ALL HIN-
DRANCES
FAITH DEFEATS ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
FAITH OVERCOMES ALL OBSTA-
CLES
FAITH DEFEATS ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
FAITH OVERPOWERS ALL LIMI-
TATIONS
FAITH OVERCOMES ALL HINDRANCES AND TRASCENDS OVER MISFORTUNE
FAITH PREVAILS AGAINST BAR-
RIERS
FAITH OVERCOMES ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
FAITH PREVAILS AGAINST OB-
STACLES
FAITH OVERCOMES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
FAITH PREVAILS OVER EVERY
ADVERSITY
FAITH OVERPOWERS ALL LIMITATIONS BUT TRASCENDS ALL MISFORTUNE
FAITH PREVAILS OVER OBSTA-
CLES
FAITH PREVAILS AGAINST BARRIERS AND TRASCENDS OVER MISFORTUNE
FAITH SURMOUNTS ALL OBSTA-
CLES
FAITH PREVAILS AGAINST OBSTACLES AND TRASCENDS ALL MISFORTUNE
FAITH SURPASSES ALL OBSTA-
CLES
FAITH PREVAILS EVERY ADVERSITY BUT SURMOUNTS ALL OBSTACLES
FAITH SURPASSES ALL OPPOSI-
TION
FAITH PREVAILS OVER ADVERSITY BUT SURMOUNTS ALL OBSTACLES
FAITH TRIUMPHS AGAINST OB-
STACLES
FAITH PREVAILS OVER OBSTACLES AND TRASCENDS ALL MISFORTUNE
FAITH TRIUMPHS OVER CHAL-
LENGES
FAITH SURMOUNTS ALL OBSTACLES AND SURMOUNTS ALL MISFORTUNE
FAITH TRIUMPHS OVER OBSTA-
CLES
FAITH SURPASSES ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
FAITH VANQUISHES ALL OBSTA-
CLES
FAITH SURPASSES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
FAITH VANQUISHES EVERY DIF-
FICULTY
FAITH SURPASSES ALL OBSTACLES BUT SURMOUNTS ALL ADVERSITY
HOPE CONQUERS ALL LIMITA-
TIONS
FAITH SURPASSES ALL OPPOSITION BUT SURMOUNTS ALL MISFORTUNE
HOPE CONQUERS ALL OBSTA-
CLES
FAITH TRIUMPHS AGAINST OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE CONQUERS EACH CHAL-
LENGES
FAITH TRIUMPHS OVER CHALLENGES AND TRASCENDS OVER MISFORTUNE
HOPE CONQUERS EACH HIN-
DRANCES
FAITH TRIUMPHS OVER OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE CONQUERS EACH HURDLE FAITH TRIUMPHS OVER OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE CONQUERS EACH INHIBI-
TION
FAITH VANQUISHES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE CONQUERS EACH LIMITA-
TIONS
FAITH VANQUISHES EVERY DIFFICULTY BUT TRASCENDS ALL MISFORTUNE
HOPE CONQUERS EACH SET-
BACK
HOPE CONQUERS ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE CONQUERS EVERY AD-
VERSITY
HOPE CONQUERS ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
8
HOPE CONQUERS EVERY BARRI-
ERS
HOPE CONQUERS EACH SETBACK BUT SURMOUNTS ALL OBSTACLES
HOPE CONQUERS EVERY CHAL-
LENGES
HOPE DEFEATS ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE CONQUERS EVERY DIFFI-
CULTY
HOPE DEFEATS ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE CONQUERS EVERY HUR-
DLE
HOPE OVERCOMES ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE CONQUERS EVERY OBSTA-
CLES
HOPE OVERCOMES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE CONQUERS EVERY RE-
STRAINT
HOPE PREVAILS OVER OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE DEFEATS ALL OPPOSITION HOPE PREVAILS OVER OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE OVERCOMES ALL CHAL-
LENGES
HOPE SURPASSES ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE OVERCOMES EACH
BOUNDARY
HOPE SURPASSES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE OVERCOMES EACH HIN-
DRANCES
HOPE TRIUMPHS OVER OBSTACLES AND TRASCENDS ALL ADVERSITY
HOPE PREVAILS AGAINST AD-
VERSITY
HOPE TRIUMPHS OVER OBSTACLES AND TRASCENDS ALL MISFORTUNE
HOPE PREVAILS AGAINST BAR-
RIERS
LOVE CONQUERS ALL ADVERSITY AND TRASCENDS ALL ADVERSITY
HOPE PREVAILS AGAINST IM-
PEDIMENT
LOVE CONQUERS ALL ADVERSITY AND TRASCENDS ALL MISFORTUNE
HOPE PREVAILS OVER OBSTA-
CLES
LOVE CONQUERS ALL ADVERSITY BUT TRASCENDS ALL MISFORTUNE
HOPE SURMOUNTS ALL DIFFI-
CULTIES
LOVE CONQUERS ALL ADVERSITY BUT TRASCENDS EVERY MISFORTUNE
HOPE SURPASSES ALL OBSTA-
CLES
LOVE CONQUERS ALL BOUNDARIES AND TRASCENDS ALL ADVERSITY
HOPE SURPASSES EVERY CON-
FINEMENT
LOVE CONQUERS ALL BOUNDARIES AND TRASCENDS ALL MISFORTUNE
HOPE SURPASSES EVERY OBSTA-
CLES
LOVE CONQUERS ALL BOUNDARIES BUT TRASCENDS ALL MISFORTUNE
HOPE TRANSCENDS EACH RE-
STRICTION
LOVE CONQUERS ALL BOUNDARIES BUT TRASCENDS EVERY MISFORTUNE
HOPE TRIUMPHS AGAINST OB-
STACLES
LOVE CONQUERS ALL DIFFERENCES AND TRASCENDS ALL ADVERSITY
HOPE TRIUMPHS OVER CHAL-
LENGES
LOVE CONQUERS ALL DIFFERENCES AND TRASCENDS ALL MISFORTUNE
HOPE VANQUISHES ALL HIN-
DRANCES
LOVE CONQUERS ALL DIFFERENCES BUT TRASCENDS ALL MISFORTUNE
LOVE OVERCOMES EACH HIN-
DRANCES
LOVE CONQUERS ALL DIFFERENCES BUT TRASCENDS EVERY MISFORTUNE
LOVE CONQUERS ALL ADVER-
SITY
LOVE CONQUERS ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
LOVE CONQUERS ALL BOUND-
ARIES
LOVE CONQUERS ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
LOVE CONQUERS ALL DIFFER-
ENCES
LOVE CONQUERS ALL OBSTACLES BUT TRASCENDS ALL ADVERSITY
LOVE CONQUERS ALL OBSTA-
CLES
LOVE CONQUERS ALL OBSTACLES BUT TRASCENDS EVERY ADVERSITY
LOVE CONQUERS ANY OBSTA-
CLES
LOVE CONQUERS EVERY BARRIERS AND PREVAILS OVER ADVERSITY
LOVE CONQUERS EACH OBSTA-
CLES
LOVE CONQUERS EVERY OBSTACLES BUT TRASCENDS ALL MISFORTUNE
LOVE CONQUERS EVERY BARRI-
ERS
LOVE DEFEATS ALL OBSTACLES AND PREVAILS OVER ADVERSITY
9
LOVE CONQUERS EVERY CHAL-
LENGES
LOVE OVERCOMES ALL OBSTACLES AND CONQUERS ALL ADVERSITY
LOVE CONQUERS EVERY CON-
FINEMENT
LOVE OVERCOMES ALL OBSTACLES AND CONQUERS ALL MISFORTUNE
LOVE CONQUERS EVERY DIFFI-
CULTY
LOVE OVERCOMES ALL OBSTACLES AND CONQUERS EVERY OBSTACLES
LOVE CONQUERS EVERY HIN-
DRANCES
LOVE OVERCOMES ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
LOVE CONQUERS EVERY HUR-
DLE
LOVE OVERCOMES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
LOVE CONQUERS EVERY LIMITA-
TIONS
LOVE OVERCOMES ALL OBSTACLES BUT CONQUERS ALL ADVERSITY
LOVE CONQUERS EVERY OBSTA-
CLES
LOVE OVERCOMES ALL OBSTACLES BUT CONQUERS EVERY ADVERSITY
LOVE CONQUERS EVERY SET-
BACK
LOVE OVERCOMES ALL OBSTACLES BUT TRASCENDS ALL ADVERSITY
LOVE DEFEATS ALL OBSTACLES LOVE OVERCOMES ALL OBSTACLES BUT TRASCENDS EACH ADVERSITY
LOVE DEFEATS ALL OPPOSITION LOVE OVERCOMES ALL OBSTACLES BUT TRASCENDS EVERY ADVERSITY
LOVE OVERCOMES ALL OBSTA-
CLES
LOVE PREVAILS AGAINST CHALLENGES AND PREVAILS OVER ADVERSITY
LOVE PREVAILS AGAINST CHAL-
LENGES
LOVE PREVAILS AGAINST OBSTACLES AND OVERCOMES OVER MISFORTUNE
LOVE PREVAILS AGAINST OB-
STACLES
LOVE PREVAILS AGAINST OBSTACLES AND PREVAILS OVER ADVERSITY
LOVE PREVAILS OVER OBSTA-
CLES
LOVE PREVAILS OVER OBSTACLES AND TRASCENDS ALL MISFORTUNE
LOVE SURMOUNTS ALL OBSTA-
CLES
LOVE SURMOUNTS ALL OBSTACLES BUT TRASCENDS ALL ADVERSITY
LOVE SURPASSES ALL OBSTA-
CLES
LOVE SURMOUNTS ALL OBSTACLES BUT TRASCENDS ALL MISFORTUNE
LOVE TRIUMPHS AGAINST OB-
STACLES
LOVE SURPASSES ALL OBSTACLES AND PREVAILS OVER ADVERSITY
LOVE TRIUMPHS OVER OBSTA-
CLES
LOVE TRIUMPHS AGAINST OBSTACLES AND DEFEATS ALL ADVERSITY
LOVE VANQUISHES ALL OBSTA-
CLES
LOVE TRIUMPHS AGAINST OBSTACLES AND DEFEATS ALL OBSTACLES
UNITY BREAKS THROUGH ALL
BARRIERS
LOVE TRIUMPHS AGAINST OBSTACLES AND DEFEATS EACH ADVERSITY
UNITY CONQUERS AGAINST
ODDS
LOVE TRIUMPHS AGAINST OBSTACLES AND DEFEATS OVER ADVERSITY
UNITY CONQUERS ALL OBSTA-
CLES
LOVE TRIUMPHS AGAINST OBSTACLES AND DEFEATS OVER OBSTACLES
UNITY CONQUERS DESPITE
ODDS
LOVE TRIUMPHS AGAINST OBSTACLES BUT DEFEATS ALL ADVERSITY
UNITY DEFEATS AGAINST ODDS LOVE TRIUMPHS AGAINST OBSTACLES BUT DEFEATS ALL OBSTACLES
UNITY DEFEATS ALL OPPOSI-
TION
LOVE TRIUMPHS AGAINST OBSTACLES BUT DEFEATS EACH ADVERSITY
UNITY OVERCOMES AGAINST
ODDS
LOVE TRIUMPHS AGAINST OBSTACLES BUT DEFEATS OVER ADVERSITY
UNITY OVERCOMES ALL CHAL-
LENGES
LOVE TRIUMPHS AGAINST OBSTACLES BUT DEFEATS OVER OBSTACLES
UNITY OVERPOWERS ALL RESIS-
TANCE
LOVE TRIUMPHS OVER OBSTACLES AND PREVAILS OVER ADVERSITY
UNITY PREVAILS AGAINST ODDS UNITY CONQUERS ALL ADVERSITY AND TRASCENDS ALL MISFORTUNE
UNITY PREVAILS AMIDST ODDS UNITY CONQUERS ALL ADVERSITY AND TRASCENDS EACH MISFORTUNE
UNITY PREVAILS DESPITE ODDS UNITY CONQUERS ALL BOUNDARIES AND TRASCENDS ALL ADVERSITY
UNITY PREVAILS OVER ODDS UNITY CONQUERS ALL BOUNDARIES AND TRASCENDS ALL MISFORTUNE
UNITY RISES ABOVE ALL OBSTA-
CLES
UNITY CONQUERS ALL BOUNDARIES AND TRASCENDS EACH ADVERSITY
10
UNITY SUBDUES ALL DIFFICUL-
TIES
UNITY CONQUERS ALL BOUNDARIES AND TRASCENDS EACH MISFORTUNE
UNITY SUCCEEDS AGAINST
ODDS
UNITY CONQUERS ALL DIFFERENCES AND TRASCENDS ALL MISFORTUNE
UNITY SURMOUNTS ALL OBSTA-
CLES
UNITY CONQUERS ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
UNITY SURPASSES AGAINST
ODDS
UNITY OVERCOMES ALL OBSTACLES AND CONQUERS ALL ADVERSITY
UNITY TRIUMPHS AGAINST
ODDS
UNITY OVERCOMES ALL OBSTACLES AND CONQUERS ALL MISFORTUNE
UNITY TRIUMPHS OVER BARRI-
ERS
UNITY OVERCOMES ALL OBSTACLES AND TRASCENDS ALL ADVERSITY
UNITY VANQUISHES ALL HIN-
DRANCES
UNITY OVERCOMES ALL OBSTACLES AND TRASCENDS ALL MISFORTUNE
11
Table S.16: Dictionary used in Simulation 3 (new unknown words not included in previous dictionary)
NEW WORDS
ANOTHER COURAGE HANDLE MOMENT SILENT
BANNER CULTURE HEALTH MOTHER SILVER
BEFORE CUSTOM HEAVEN MOTION SIMPLE
BENEATH DANGER HONEST NATION SPIRIT
BETWEEN DEEPER HUNGER NATURE STRENGTH
BLESSED DESERT INSIDE NUMBER STRONG
BORDER DIFFERENT INSIGHT OFFBEAT THOUGHT
BOTTLE ECHOES ISLAND OPTION THROUGH
BREEZE EMPIRE JOURNEY PATIENT TRAVEL
BRIDGE FAMILY JUSTICE PEACE TUNNEL
BUTTER FAMOUS KINDNESS PEOPLE VALUES
BUTTON FATHER KINGDOM POSSIBLE VISION
CANDLE FELLOW LADDER REASON VOICES
CENTER FOREST LANTERN REMOTE WARMTH
CHANGE FORGIVE LEGEND RHYTHM WINDOW
CHAPTER FREEDOM LIGHTS SECRET WINTER
CHOICE FROZEN LITTLE SERVICE WITHOUT
CIRCLE FUTURE MARKET SHADOW WONDER
CLOSED GENTLE MEMORY SHELTER WONDER
CORNER GROWTH MIRACLE SHELVE WORLD
12
Table S.17: Dictionary used in Simulation 4
Sentences ERC Topic
DEDICATED TO ADVANCING ACADEMIC KNOWLEDGE AND ADDRESSING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING APPLIED KNOWLEDGE AND ADDRESSING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING BASIC KNOWLEDGE AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING BIOMEDICAL KNOWLEDGE AND ADDRESSING SO-
CIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING COMPUTATIONAL CHANGE AND ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL DIVERSITY AND ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL IDENTITY AND ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL OUTCOMES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND MANAGING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND OVERCOM-
ING TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND SOLVING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE AND UNDER-
STANDING TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE BY ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE FOR ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE THROUGH AD-
DRESSING TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL KNOWLEDGE WHILST AD-
DRESSING TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL LITERACY AND ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING COMPUTATIONAL STUDIES AND ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING CULTURAL AWARENESS AND ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL CHANGE AND ADDRESSING SOCIETAL
CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL CONSCIOUSNESS AND ADDRESSING SO-
CIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND ADDRESSING SOCI-
ETAL OUTCOMES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND ADDRESSING SOCI-
ETAL PROBLEMS
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND ADDRESSING SOCI-
ETAL QUESTIONS
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND CONFRONTING SO-
CIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND MANAGING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND OVERCOMING SO-
CIETAL CHALLENGES
Physical Sciences and En-
gineering
13
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE AND RESOLVING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE IN ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE OR ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE THROUGH ADDRESSING
SOCIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL KNOWLEDGE WHILST ADDRESSING
SOCIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL STUDIES AND ADDRESSING SOCIETAL
CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CULTURAL VALUES AND ADDRESSING SOCIETAL
CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING CURRENT KNOWLEDGE AND ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING CURRENT KNOWLEDGE AND ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING MEDICAL AWARENESS AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL CHANGE AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND ADDRESSING
HEALTH OUTCOMES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND ADDRESSING
HEALTH QUESTIONS
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND CHALLENGING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND CONFRONTING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND MEETING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND RESOLVING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND SOLVING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE AND UNDERSTANDING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE BY ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE IN ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE OR ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE THROUGH ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL KNOWLEDGE WHILST ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING MEDICAL VALUES AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING NEW KNOWLEDGE AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING NEW KNOWLEDGE AND ADDRESSING SOCIETAL
CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC ADVANCES AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
14
DEDICATED TO ADVANCING SCIENTIFIC ADVANCES AND ADDRESSING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC CONSENSUS AND ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC CONSENSUS AND ADDRESSING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC DISCIPLINES AND ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC DISCIPLINES AND ADDRESSING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC INQUIRY AND ADDRESSING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC INQUIRY AND ADDRESSING SOCIETAL
CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC INQUIRY AND ADDRESSING TECHNO-
LOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
HEALTH GOALS
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
HEALTH ISSUES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
HEALTH OBJECTIVES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
HEALTH PROBLEMS
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO-
CIETAL CONCERNS
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO-
CIETAL OBJECTIVES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO-
CIETAL PROBLEMS
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING SO-
CIETAL QUESTIONS
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL BOUNDARIES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL CONCERNS
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL GOALS
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL ISSUES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL OBJECTIVES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND ADDRESSING
TECHNOLOGICAL PROBLEMS
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CHALLENGING SO-
CIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CHALLENGING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CONFRONTING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND CONFRONTING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND FACING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
15
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND FACING TECHNO-
LOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND MANAGING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND MANAGING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND MANAGING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND MEETING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND OVERCOMING SO-
CIETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND OVERCOMING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND REDUCING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND REDUCING TECH-
NOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND SOLVING HEALTH
CHALLENGES
Social Sciences and Hu-
manities
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND SOLVING SOCIETAL
CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE AND UNDERSTANDING
TECHNOLOGICAL CHALLENGES
Life Sciences
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE BY ADDRESSING SOCI-
ETAL CHALLENGES
Physical Sciences and En-
gineering
DEDICATED TO ADVANCING SCIENTIFIC KNOWLEDGE FOR ADDRESSING
HEALTH CHALLENGES
Social Sciences and Hu-
manities
16