   
 
 1 
 
Active Inference Tree Search in Large POMDPs 
 
 
Domenico Maisto1, Francesco Gregoretti2, Karl Friston3,4, Giovanni Pezzulo1,* 
 
1) Institute of Cognitive Sciences and Technologies, National Research Council, Via Gian Domenico 
Romagnosi 18/A, Rome 00196, Italy 
 
2) Institute for High Performance Computing and Networking, National Research Council, Via Pietro 
Castellino 111, Naples 80131, Italy 
 
3) The Wellcome Centre for  Human Neuroimaging, Institute of Neurology, University College 
London, London, WC1N 3AR UK 
 
4) VERSES AI Research Lab, Los Angeles, CA 90016, USA 
 
 
 
 
 
 
 
 
 
 
 
 
* Corresponding author: 
 
Giovanni Pezzulo 
ISTC-CNR 
Via San Martino della Battaglia 44, 00185 Rome, Italy 
Email: giovanni.pezzulo@istc.cnr.it 
Phone: +39 6 44595206 
 
 
  
   
 
 2 
Abstract 
 
The ability to plan ahead efficiently is key for both living organisms and artificial systems. Model-
based p lanning and prospection are widely studied in cognitive neuroscience and artificial 
intelligence (AI), but from different perspectivesâ€”and with different desiderata in mind (biological 
realism versus scalability) that are difficult to reconcile. Here, we introduce a novel method to plan 
in POMDPs â€”Active Inference Tree Search  (AcT)â€”that combines the normative character and 
biological realism of a leading planning theory in neuroscience (Active Inference) and the scalability 
of tree search methods in AI.  This unification enhances both approaches. On the one hand, tree 
searches enable the biologically grounded, first principle method of active inference to be applied to 
large-scale problems. On the other hand, active inference  provides a  principled solution to the 
explorationâ€“exploitation dilemma, which is often addressed heuristically in tree search methods. Our 
simulations show that AcT successfully navigates binary trees that are challenging for sampling-
based methods , problems that require adaptive exploration, and  the large POMDP  problem 
â€˜RockSampleâ€™â€”in which AcT reproduces state-of-the-art POMDP solutions. Furthermore, we 
illustrate how AcT can simulate neurophysiological responses (e.g.,  in the  hippocampus and 
prefrontal cortex) of humans and other animals that solve large planning problems. These numerical 
analyses show that Active Tree Search is a principled realisation of neuroscientific and AI planning 
theories, offering biological realism and scalability. 
 
 
  
 
Keywords: Active Inference, Tree Search, model-based planning, POMDP 
 
  
   
 
 3 
1 Introduction 
 
Model-based planning problems have received substantial attention in various disciplines, including 
AI, machine learning, robotics, cognitive science, and neuroscience. The interdisciplinary exchanges 
between these disciplines have been numerous [1â€“6], but yet we still lack a theoretical synthesis that 
unites their desiderata (e.g., biological realism in neuroscience versus efficiency in AI) [7]. Here, we 
take a step in this direction by showing that a prevalent theory of model -based control and planning 
in neuroscience â€”active inference [8]â€”can be extended straightforward ly to address large -scale 
planning problems using tree search methods. Our novel approach â€”Active Inference Tree Search 
(AcT)â€”bridges computational neuroscience and AI requirements by combining the normative 
character and biological realism of active inference with the scalability and efficiency of tree search 
methods.  
 
Active inference is an increasingly popular computational neuroscience framework that characterises 
perception, action, and planning in terms of approximate (variational) Bayesian inference  under a 
generative model  [9â€“12]. Active inference is related to a family of recent approaches to  solving 
POMDP problems in machine learning and AI, which exploit a general duality between control and 
inference problems [13] and include control as inference [14,15], planning as inference [16,17], risk-
sensitive and KL control [18]. A peculiarity of active inference is that it implements a principled form 
of model-based planning: it infers posteriors over action sequences (or policies) by considering an 
expected free energy  functional, effectively balancing  exploration and exploita tion in a context -
sensitive and optimal fashion . Previous studies have  established that the computations underlying 
active inference are biologically plausible and can reproduce various findings in functional brain 
anatomy, neuronal dynamics and behaviour [11,19â€“21], as well as furnishing sophisticated forms of 
inference under hierarchical and temporally deep generative models [12,19,21â€“25]. 
 
However, the active inference framework has been developed with cognitive and biological realism 
in mind, not scalability or implementational efficiency. Its current implementations require the 
exhaustive evaluation of all  allowable policies and hence can only address  small-scale POMDP 
problems. Here, we develop an extension of active inference aiming to  address large r POMDPs: 
Active Inference Tree Search (AcT). Our novel algorithm retains the key aspects of active inferenceâ€” 
namely, the use of expected free energy to infer the posterior probability of policiesâ€”but relaxes the 
exhaustive evaluation of all policies, using tree search planning methods that are popular in AI  [1â€“
6].  
 
Tree search AI methods perform a look-ahead search over a  planning tree, which describes the 
possible courses of actions and their associated outcome values. The tree is expanded during planning 
from the root node (i.e., the state where planning starts) to the leaves. In most practical applications, 
the tree cannot be explored exhaustively. Various heuristic procedures have been proposed to decide 
what actions to consider next, how to expand the planning tree,  and how to balance exploration and 
exploitation to find an almost-optimal sequential policy [26,27]. A common way to approximate the 
value of possible policies is using Monte-Carlo sampling [28], which  permits sampling rewards 
obtained by following a given branch of the tree (corresponding to a given course of action) and 
storing their  statistics in the tree nodes.  We will show that  Active Inference Tree Search can 
contextualise these heuristic methods within a normative  and biologically realistic approach, using 
   
 
 4 
an (expected) free energy functional that automatically entails the appropriate level of exploration, 
rendering the use of Monte Carlo methods unnecessary. 
 
The main contribution of this article is a proof of principle that the novel AcT methodâ€”that combines 
active inference and tree searchâ€”can augment both approaches. On the one hand, using a planning 
tree enables active inference to handle larger problems than previously. On the other hand, active 
inference provides a principled approach to dissolve the exploration-exploitation dilemma, which is 
addressed heuristically in tree search methods. 
 
In the following Sections, we first review tree search planning methods in AI and active inference. 
We then introduce Active Inference Tree Search formally and validate it using three simulations, 
showing that it can  handle (i) deceptive binary trees (that are challen ging for sampling -based 
methods), (ii) problems that require adaptive exploration , and (iii) larger-scale POMDP problems 
(i.e., RockSample). Finally, to highlight the  potential of Active Inference Tree Search for studying 
biological phenomena, we use  the scheme to simulate neuronal responses in humans (and other 
animals) that solve large planning problems. 
 
2. Methods: technical background 
 
2.1 Partially observed Markov decision processes (POMDP) 
 
Several real world and biological  problems can be cast as sequential decisions under uncertainty. 
Formally, they can be treated as extensions of Markov Decision Process, where the observed action 
outcomes provide only partial information about the state of the environment; this corresponds to the 
framework of Partially Observed Markov Decision Process (POMDP) [5] (Sutton & Barto, 1998). 
 
A POMDP can be defined as a tuple âŒ©ğ‘†,ğ´,ğ‘‡,ğ‘,ğ‘‚,ğ‘…âŒª where: 
 
â€¢ ğ‘† denotes the set of environment states that generate information for the agent to accomplish 
a task; 
 
â€¢ ğ´ is the set of actions potentially executable by the agent; 
 
â€¢ ğ‘‡âˆ¶ ğ‘† Ã— ğ´ Ã— ğ‘† â†’ [0,1], such that ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²) = ğ‘ƒğ‘Ÿ(ğ‘ â€²|ğ‘ ,ğ‘), is the transition probability of 
being in a state ğ‘ â€² after performing the action ğ‘ from state ğ‘ ;  
 
â€¢ ğ‘ denotes the set of observations.  
 
â€¢ ğ‘‚âˆ¶ ğ‘† Ã— ğ´ Ã— ğ‘ â†’ [0,1], such that ğ‘‚(ğ‘ â€²,ğ‘,ğ‘§) = ğ‘ƒğ‘Ÿ(ğ‘§|ğ‘,ğ‘ â€²), is the probability of 
observing ğ‘§âˆˆğ‘ in the transited state ğ‘ â€² by performing an action ğ‘; 
 
â€¢ ğ‘…âˆ¶ ğ‘† Ã— ğ´ â†’ â„, where ğ‘…(ğ‘ ,ğ‘) is the reward obtained by taking the action ğ‘ from a 
particular state ğ‘  [5]. 
 
   
 
 5 
In a POMDP, the state of the environment cannot be observed directly but can be inferred based on 
partial observations that the agent solicits through action. Since the agentâ€™s state information can be 
noisy or incomplete, it is helpful for an agent to consider a  probability distribution over the states it 
could be in. This probability distribution, called a (Bayesian) belief, is defined as the posterior  
ğ‘ğ‘¡(ğ‘ ) = ğ‘ƒğ‘Ÿ(ğ‘ ğ‘¡ = ğ‘ |â„ğ‘¡,ğ‘0) given the  initial belief ğ‘0 and a complete sequence  or history â„ğ‘¡ =
{ğ‘0,ğ‘§1,â€¦,ğ‘§ğ‘¡âˆ’1,ğ‘ğ‘¡âˆ’1,ğ‘§ğ‘¡} of past actions and observations. At any time ğ‘¡, it is possible to write down 
the belief state ğ‘ğ‘¡ as a Bayesian update ğœ(ğ‘ğ‘¡âˆ’1,ğ‘ğ‘¡âˆ’1,ğ‘§ğ‘¡) of the previous belief state ğ‘ğ‘¡âˆ’1, given the 
action ğ‘ğ‘¡âˆ’1 and the current observation ğ‘§ğ‘¡.  
 
Generally, solving a POMDP problem means finding a plan or policy by predicting the situations the 
agent could encounter in the future, conditioned on the actions it executes. One can specify a policy 
as a function ğœ‹âˆ¶â„¬âŸ¶ğ´ that associates beliefs ğ‘âˆˆ â„¬ to actions ğ‘âˆˆğ´. In Reinforcement Learning 
(RL), the value function ğ‘‰ğœ‹(ğ‘) of a policy ğœ‹, evaluated in a belief state ğ‘, corresponds to the expected 
total discounted reward ğ‘‰ğœ‹(ğ‘)=ğ”¼[âˆ‘ ğ›¿ğ‘¡ğ‘…(ğ‘ ğ‘¡,ğœ‹(ğ‘ğ‘¡))ğ‘‡
ğ‘¡=0 ], where ğ›¿âˆˆ(0,1] is a discount factor, and T 
is a finite (or infinite) value if the POMDP problem has a finite (or infinite) time horizon. Following 
the RL setup, a POMDP plan or policy is optimal when ğœ‹âˆ— maximises the value function ğ‘‰ğœ‹âˆ—(ğ‘). 
 
2.2 Online POMDP planning 
 
There are two main  approaches to POMDP problems: offline and online. In offline methods  
[29][30][31][32], the policy is computed before execution by considering every possible belief state. 
Offline methods achieve good results for small-size scenarios but are not suitable for large POMDP 
problems. In online methods, there is an alternation between policy construction, whose goal is 
discovering a good short -path policy (often a single action) for the current belief; and execution, 
where the selected policy is executed. These methods scale up to large POMDP problems but usually 
result in suboptimal policies, as they are computed based on a subset of beliefs. The two approaches 
are complementary and can coexist. For example, online methods can be complemented by initial 
approximations acquired via some offline algorithm. However, online methods are generally more 
widely used, given their scalability.  
 
 
Figure 1. AND -OR tree for a POMDP with 2 observations {ğ‘§1,ğ‘§2} and 2 actions {ğ‘1,ğ‘2}. 
Triangular (OR) nodes represent belief states, whereas circular (AND) nodes represent actions. 

   
 
 6 
The numerical values shown on the edges that stem from OR -nodes represent rewards ğ‘…(ğ‘,ğ‘), 
whereas the numerical values shown on the edges that stem from AND-nodes represent conditional 
probabilities ğ‘ƒ(ğ‘§|ğ‘,ğ‘). 
 
 
Ross et al . (Ross, Pineau, Paquet, et al., 2008)  established a general scheme for online planning 
algorithms. In their scheme, every online algorithm can be understood as a procedure in which policy 
construction is a routine implementing  a predefined set of steps:  1) visit, 2) expansion , and 3) 
estimation of an AND -OR tree , with the OR nodes representing beliefs and AND nodes 
corresponding to actions (Figure 1). The algorithm starts by setting the current belief as the root node 
of a tree; then builds new belief n odes generated by action nodes.  Every time a new belief node is 
allocated, it is evaluated, and its value is transmitted up to the belief ancestors â€”up to the rootâ€”to 
update the value of the corresponding policy (i.e., action sequence corresponding to a specific branch 
of the tree).  
 
The most popular online-planning approaches are Heuristic Search, Branch-and-Bound pruning, and 
Monte Carlo sampling [33]. In Heuristic Search methods  [34][35], a routine explores the belief tree 
using a heuristic to detect relevant nodes to branch out (frequently, for a single forward step). It 
successively updates the heuristic value associated with its ancestors (which differs between heuristic 
search algorithms) . However, this procedure  can be computationally expensive, reducing  the 
effectiveness of the heuristic-based node selection. Branch -and-bound approaches instead rely on a 
general search technique that constrains the search tree expansion by pruning suboptimal branches  
[36]. They assign every belief tree node an upper and a lower bound of a quality value function. If a 
branch leads to a node with an upper boundâ€”that is lower than the lower bound of another node of 
a different branch â€”then the first node is labelled as the root of a suboptimal subtree that can be 
pruned. Finally, Monte Carlo algorithms randomly sample a subset of observations each time . This 
procedure constrains the branching expansion of the belief tree and the depth of the search.  
 
While Monte Carlo algorithms follow the same general procedure, they sample outcomes in different 
ways [37][38]. One of the earliest Monte -Carlo-sampling-based algorithms for solving POMDP â€”
the Sparse Sampling algorithm of Kearns et al . [27]â€”builds a fixed depth tree search in one stage  
(i.e., from the root to the leaves) using  a "black-box" simulator (a generative model) for modelling 
state transitions and simulating reward returns. To improve the performance of the Sparse Sampling 
algorithm, Kocsis and SzepesvÃ¡ri proposed the Upper Confidence Tree (UCT) algorithm [26], which 
introduced two essential novelties. First,  it uses Monte Carlo Tree Search  (MCTS) [39]: a rollout-
based Monte Carlo planning  method inspired by game strategy searches but builds the belief tree 
progressively and iteratively. Second, it selects actions during the planning phase in a stochastic way 
rather than by drawing from a uniform distribution ( consistent with theoretical results on sequential 
decision making under uncertainty [40]). 
 
2.3 Related works 
 
UCT [39] is an online decision -making algorithm that works by constructing a tree of simulated 
histories â„ğ‘¡ that expands from an initial belief state ğ‘0 cast as root, and alternating state and action 
nodes, eventually drawn by a generative model, i.e., a probabilistic model that statistically describes 
   
 
 7 
the POMDP distributions ğ‘‡ and ğ‘. To select which node (and branch corresponding to some history 
â„ğ‘¡) to expand next in â„ğ‘¡ğ‘={ğ‘0,ğ‘§1,â€¦,ğ‘§ğ‘¡,ğ‘ğ‘¡+1} , the algorithm uses the value function ğ‘‰ğœ‹(â„ğ‘¡ğ‘) and 
evaluates the expected return from the initial belief ğ‘0 following the policy ğœ‹ furnished by â„. A 
peculiarity of UCT, as of every MCTS algorithm, is the way the value function ğ‘‰ğœ‹(â„ğ‘¡ğ‘) is calculated. 
Instead of bootstrapping ğ‘‰ğœ‹(â„ğ‘¡ğ‘) using dynamic programming, it is estimated by Monte Carlo 
sampling, where multiple stochastic rollouts approximate a mean value. The computed value is then 
propagated back to each branch node and averaged with contributions from other histories branching 
off from the same  node. Concurrently, a visitation count ğ‘(â„ğ‘¡ğ‘) is updated , such that ğ‘(â„ğ‘¡)=
âˆ‘ ğ‘(â„ğ‘¡ğ‘)ğ‘  is the number of simulations ran through the node representing ğ‘ . In UCT, node visit 
counts are used effectively: node selection is seen as a Multi -armed Bandit problem for which the 
optimal choice uses the Upper Confidence Bound (UCB)  ğ‘‰ğœ‹(â„ğ‘¡ğ‘)+ğ‘ğ‘âˆšlogğ‘(â„ğ‘¡)/ğ‘(â„ğ‘¡ğ‘). UCB 
augments the value function with an exploration term favouring less-visited nodes [40]. 
 
A UCT -based planning algorithm that has received attention in the last decade is POMCP [41]. 
POMCP can handle POMDPs with large state spaces. It adopts MCTS to generate an AND-OR belief 
tree, where the AND nodes (actions) are selected through the UCB algorithm, and the OR nodes 
represent a set of sampled states (not a full probability distribution), which are iteratively maintained 
by a particle filter. Although it can handle problems of considerable size, POMCP has some implicit 
limitations. By representing the POMDP problem as a belief tree, POMCP needs to visit every 
potential observation related to a belief state at least once. Furthermore, as it uses the UCB heuristic, 
its worst-case is computationally challenging [42]. 
 
DESPOT [43] is another state -of-art MCTS -based a lgorithm that tries to overcome  (at least 
theoretically) the above limitations  by operating on a sparse belief tree generated on a subset of 
sampled observations. As in POMCP, the nodes of such a reduced tree â€”called DESPOT tree â€” 
approximate distributions over belief states using particles. An MCTS planning routine progressively 
constructs the DESPOT tree by iterating the following three stages: a forward search that traverses 
the tree until it encounters a leaf node according to the heuristic values (which includes  a pre-
computed regularisation term to prevent overfitting ); a leaf initialisation, where a Monte Carlo 
sampling estimates the upper and lower bounds of the selected leaf node;  and a backup, that passes 
back through the path tracked in the forward search and updates the upper and lower bounds of each 
visited node, according to the Bellman optimality principle. These three stages are analogous to the 
selection, expansion, and backpropagation phases of POMCP. However, they are iterated  until the 
difference between the upper and lower bound s of the belief root state is sufficiently small  (as in a 
Branch-and-Bound method).  
 
More recently, new approaches to online POMDP planning allow the parallelisation of  extant 
methods [44][45] or use deep learning to extract and aggregate relevant information from the 
environment to speed up and improv e policy inference [46][47]. Furthermore, applied research in 
decision making for autonomous urban vehicles has engendered novel solutions to the online POMDP 
and approximate solutions [48][49][50]. 
 
  
   
 
 8 
2.4 Active inference 
 
Active inference integrates cybernetic feedback and error control concepts with Bayesian inference 
[51] with a Bayesian inferential scheme [19,52,53]. It involves a closed -loop process where 
perception and action selection operate through approximate Bayesian inference [17,54â€“56], 
employing a variational approximation under the free -energy-minimisation principle (Friston et al., 
2012). The scheme has various proposed variants, and its biological plausibility is under investigation 
(Friston et al., 2017; Friston et al., 2016a) 
 
Essentially, active inference is a theory of decision -making under uncertainty, positing that agents 
minimise the expected free energy of future outcomes [10]. It aligns with optimal decision theory and 
can be viewed as a partially observable Markov decision process (POMDP), wherein the agent holds 
beliefs about the probability of associating observations with hidden states, with rewards (or cost 
functions) absorbed into beliefs about initial state distribution and terminal observations [57]. 
 
In this setting, active inference can be formally represented by a tuple âŒ©ğ‘†,ğ‘‚,ğ‘ˆ,ğ›¾,ğ‘…,ğ‘ƒ,ğ‘„âŒª where: 
 
â€¢ ğ‘† is the set of agentâ€™s hidden states ğ‘  by which the agent infers the environmental state. Where 
a sequence of hidden states is denoted by ğ‘ Ìƒ=(ğ‘ 0,â€¦,ğ‘ ğ‘‡); 
 
â€¢ ğ‘‚ is a finite set of outcomes (i.e., observations from the environment) ğ‘œ, and ğ‘œÌƒ=(ğ‘œ0,â€¦,ğ‘œğ‘‡); 
 
â€¢ ğ‘ˆ is a finite set of control states ğ‘¢ executable by the agent to control the environment. A 
sequence of control states ğ‘¢Ìƒ=[ğ‘¢ğ‘¡,â€¦,ğ‘¢ğ‘‡] is called policy and denoted as ğœ‹. Thus, ğœ‹=ğ‘¢Ìƒ=
[ğœ‹(ğ‘¡),â€¦,ğœ‹(ğ‘‡)]; 
 
â€¢ ğ›¾âˆˆâ„, is an additional variable denoted as precision, introduced to self-tune the control-state 
selection process adaptively; 
 
â€¢ ğ‘…(ğ‘œÌƒ,ğ‘ Ìƒ,ğ‘Ìƒ) is a generative process that generates probabilistic sequences of outcomes from 
hidden states and actions ğ‘Ìƒ=(ğ‘0,â€¦,ğ‘ğ‘‡) corresponding to the activations of the control states 
ğ‘¢Ìƒ in the environment.  
 
â€¢ ğ‘ƒ(ğ‘œÌƒ,ğ‘ Ìƒ,ğœ‹,ğ›¾|Î˜) is a generative model with parameters Î˜={ğ€,ğ,ğ‚,ğƒ,ğ„,ğ›¼,ğ›½} (defined later 
on), over outcomes, hidden states, control states and precision;  
 
â€¢ ğ‘„(ğ‘ Ìƒ,ğœ‹,Î³) is an approximate posterior distribution over states, control states , and precision, 
with expectations (ğ’”0
ğœ‹,â€¦,ğ’”ğ‘‡
ğœ‹,ğ…,ğœ¸), approximating the posterior of the generative model 
ğ‘ƒ(ğ‘ Ìƒ,ğ‘¢Ìƒ,ğ›¾|ğ‘œÌƒ,Î˜).  
 
It is worth noting that the generative process  describes transitions in the environment due to the 
agent's actions, generating observed outcomes. In contrast, the generative model reflects the agent's 
beliefs, encoding states and policies as expectations. Notably, there is a distinction between control 
   
 
 9 
states ğ‘¢ in the generative model and actions ğ‘ in the generative process. Action is a real variable that 
acts on the environment, while the corresponding hidden cause in the generative model is a control 
state. This means the agent needs to infer its behaviour by forming beliefs about control states using 
the observed consequences of its action. That allows for the formulation of action in terms of beliefs 
about policies and transforms an optimal control problem into an optimal inference problem, known 
as planning as inference (Attias, 2003). 
 
2.4.1 Generative models for active inference 
 
As shown in Fig. 2, the generative model used in active inference includes hidden states ğ‘  as causes 
of the observed outcomes ğ‘œ. Hidden states move forward in time under a policy ğœ‹ that depends on 
the precision ğ›¾. A series of factorisations permits writing down the modelâ€™s joint density as:  
 
 
ğ‘ƒ(ğ‘œÌƒ,ğ‘ Ìƒ,ğ‘¢Ìƒ,ğ›¾|Î˜)=ğ‘ƒ(ğ›¾|Î˜)ğ‘ƒ(ğœ‹|ğ›¾,Î˜)âˆğ‘ƒ(ğ‘œğ‘¡|ğ‘ ğ‘¡,Î˜)ğ‘ƒ(ğ‘ ğ‘¡|ğ‘ ğ‘¡âˆ’1,ğœ‹,Î˜)
ğ‘‡
ğ‘¡=0
 
(1) 
 
where: 
 
ğ‘ƒ(ğ‘œğ‘¡ |ğ‘ ğ‘¡,Î˜)= ğ€ 
 
ğ‘ƒ(ğœ‹|ğ›¾,Î˜)=ğœ(lnğ„âˆ’ğ›¾âˆ™ğ†ğœ‹|Î˜) 
 
ğ‘ƒ(ğ›¾|Î˜)~Î“(ğ›¼,ğ›½) 
 
In Equations (2), the matrix ğ€ (with size |ğ‘‚|Ã—|ğ‘†|) encodes the likelihood of observations given a 
hidden state, while ğ‚, a vector of size |ğ‘‚|, represents their prior distribution. Importantly, the prior 
distribution of observations, encoded in the vector ğ‚, specifies the agentâ€™s preferred observations, 
which play an analogous role to rewards in Reinforcement Learning  and can be defined in a task -
dependent manner. The matrices ğ(ğ‘¢ğ‘¡) (of size |ğ‘†|Ã—|ğ‘†|) define ğ‘¢-specific state transitions, while ğƒ 
is a |ğ‘†|-long vector encoding the prior distribution of the initial state, and ğ„ is the prior expectation 
of each policy that can be specified according to the problem at hand. Finally, ğœ is the Boltzmann 
distribution and ğ›¼ and ğ›½ correspond to the shape and the rate parameters of the gamma density, which 
underlies the ğ›¾-distribution, respectively.  
 
ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡,ğœ‹,Î˜)= ğ(ğ‘¢ğ‘¡ =ğœ‹(ğ‘¡)) for ğ‘¡>0  and  ğ‘ƒ(ğ‘ 0|Î˜)=ğƒ otherwise 
 
ğ‘ƒ(ğ‘œğ‘¡|Î˜)=ğ‚ 
 
(2) 
   
 
 10 
 
 
Figure 2. Graphical model for active inference. See the main text for an explanation. 
 
 
The quantity ğ†ğœ‹ is a score of the â€œqualityâ€ of a generic policy. It can be viewed as the log prior of a 
given policy, conditioned on the future state and observations, together with preferred outcomes (see 
below for more details). 
 
2.4.2 Approximate Bayesian inference and variational free energy minimisation 
 
Perception and action in Active Inference are read as predictive processes, in which an agent infers 
the states of the external world. Active inference is an application of the free energy principle; namely 
a variational principle in which the (path integr al) of free energy is minimised. The free energy in 
question is an upper bound on the negative log marginal likelihood of sensory samples. This means 
that â€” under active inference â€” action and perception minimise the self-information (a.k.a., surprisal 
or surprise) of sensory data. This is equivalent to maximising the Bayesian model evidence of a 
generative model of those data. In terms of minimising surprise, agents continuously compare 
predictions (from the generative model) and sensory stimuli (from the generative process). 
Discrepancies between sensations and predictions of those sensations are minimised either by 
modifying predictions (i.e., perception) or acting to change the world and subsequent sensory samples 
(i.e., action).  
 
In most cases, exact Bayesian inference cannot be realised because the computation of model 
evidence P(oÌƒ|Î˜) and requisite posterior P(sÌƒ,Ï€,Î³|oÌƒ,Î˜) and are (generally) intractable. Active 
Inference therefore appeals to approximate Bayesian inference in which intractable inference problem 
is transformed into an optimization problem by substituting P(sÌƒ,Ï€,Î³|oÌƒ,Î˜) and P(oÌƒ|Î˜) with two 
quantities â€” an approximate posterior and a variational free energy â€” that are computable [58]. 
Under this variational formulation, the optimality principle underlying Active inference becomes the 
minimisation of variational free energy, defined as follows: 

   
 
 11 
 
 
where EQ[âˆ™] denotes an expectation under the approximate posterior Q, DKL[âˆ™||âˆ™] is the Kullback -
Leibler divergence, and âˆ’lnP(Î˜)  (i.e., the negative logarithm of the model evidence P(Î˜)) is called 
self-information, surprisal or, more simply, surprise. Equation (3) implies that one needs to minimise 
the variational free energy to approximate the model evidence 1. The approximate posterior is 
equipped with a known functional form, which generally factorises over hidden states and parameters 
(sÌƒ,Ï€,Î³): 
 
 ğ‘„(ğ‘ Ìƒ,ğœ‹,Î³)= ğ‘„(ğœ‹)ğ‘„(ğ›¾)âˆ ğ‘„(ğ‘ ğ‘¡|ğœ‹)ğ‘‡
ğ‘¡=0 ; (4) 
 
This factorisation is known as a mean -field approximation . When ğ‘„(ğ‘ Ìƒ,ğœ‹,ğ›¾) converges on the 
posterior ğ‘ƒ(ğ‘ Ìƒ,ğœ‹,Î³|ğ‘œÌƒ,Î˜), the variational free energy decreases. If they match  exactly, and their 
divergence is zero, free energy becomes the surprise. Therefore, one could summarise variational 
inference [58] as minimising free energy to approximate the posterior ğ‘ƒ with ğ‘„ while, at the same 
time, evaluating a bound on the log -model evidence lnğ‘ƒ(ğ‘œ|ğ›©) (often called an evidence bound in 
machine learning). Effectively, this converts an intractable marginalisation problem into an 
optimisation problem with easy -to-compute approximations to log evidence (a.k.a., approximate 
Bayesian inference). 
 
It is possible to demonstrate  [8] that the optimal solution that minimises   
ğ¹(ğ‘œÌƒ,ğ‘ Ìƒ,ğœ‹,ğ›¾) is represented by the approximate posterior  ğ‘„ with sufficient statistics  ğ=(ğ’”Ìƒğœ‹,ğ…,ğœ¸), 
with ğ’”Ìƒğœ‹ =ğ’”0
ğœ‹,â€¦,ğ’”ğ‘»
ğ… set, at any time ğ‘¡, as: 
 
ğ’”ğ‘¡
ğœ‹ â‰ˆ{ğœ(lnğ€âˆ™oğ‘¡ +ln(ğ(ğœ‹(ğ‘¡âˆ’1))âˆ™ğ’”ğ‘¡âˆ’1
ğœ‹ ))    if   ğ‘¡>1
ğœ(lnğ€âˆ™oğ‘¡ +lnğƒ)                      if   ğ‘¡=1                 
 
ğ…=ğœ(lnğ„âˆ’ ğœ¸âˆ™ğ†ğœ‹) (5) 
 
ğœ¸=
ğ›¼
ğ›½âˆ’ğ†ğœ‹
  
 
Here, we use the symbol â€œâ‹…â€ to denote the inner product, defined as ğ€â‹…ğ=ğ€ğ‘‡ğ, where A and B are 
two arbitrary matrices. The first equation defines the expected hidden state and corresponds to the 
part of active inference that implements perception. For ğ‘¡=1, by considering Equation (2), we have 
ğ’”ğ‘¡
ğœ‹ â‰ˆğœ(lnğ€âˆ™oğ‘¡ +lnğƒ).  
 
 
1 Generally, policies are treated as sequences of actions from the current time point, where realised 
actions â€” taken in the past â€” are absorbed into observations when necessary. 
ğ¹(ğ‘œÌƒ) =ğ”¼ğ‘„[lnğ‘„(ğ‘ Ìƒ,ğœ‹,ğ›¾)âˆ’lnğ‘ƒ(ğ‘œÌƒ,ğ‘ Ìƒ,ğœ‹,ğ›¾|Î˜)]  
 =ğ·KL[ğ‘„(ğ‘ Ìƒ,ğœ‹,ğ›¾)||ğ‘ƒ(ğ‘ Ìƒ,ğœ‹,ğ›¾|ğ‘œÌƒ,Î˜)]âˆ’lnğ‘ƒ(ğ‘œÌƒ|Î˜) 
â‰¥ âˆ’lnğ‘ƒ(ğ‘œÌƒ|Î˜) 
(3) 
   
 
 12 
The hidden state ğ’”ğ‘¡
ğœ‹ estimated at a certain time point depends both on the expected hidden state at the 
previous time and the actual outcome ğ‘œğ‘¡ observed after executing the action predicted by ğœ‹(ğ‘¡âˆ’1). The 
second equation derives (as the expected hidden state) from a Boltzmann distribution of the policies' 
quality values. The expected value of ğ›¾ is the  distribution's sensitivity (or inverse temperature 
parameter): it adjusts the tendency to select a policy with greater or lesser confidence. The last 
equation tunes the expected precision value on the base of policy quality  values (in a nonbiological 
setting, this precision is usually set to 1, especially for policies that only look ahead).  
 
The term ğ†ğœ‹ is the policiesâ€™ expected free energy (EFE). It is used to score the quality of a generic 
policy with respect to the future outcomes and states that are expected under such policies. It controls 
the optimism bias in the first equation, determines policy selection in the second equation, and 
nuances an agent's confidence in action selection in the third equation. With greater differences 
among the values of ğ†ğœ‹, the precision is greaterâ€”and the agent is more confident about what to do 
next (in a non-biological setting, the selected policy is the one with the smallest EFE).  
 
Note a fundamental difference between active inference and RL based approaches to POMDP : RL 
approaches are based upon a value function of future states, while active inference infers the best 
policy using an expected free energy functional of beliefs about future states. Technically, this means 
replacing the Bellman optimality principle with a straightforward principle of least action, where the 
action is the path integral of expected free energy. Teleologically, this means active inference  
considers optimal sequences of belief states that subsume information -seeking and preference -
seeking imper atives into the same functional,  thereby dis solving the exploration â€“exploitation 
dilemma. 
 
To get a closed form for ğ†ğœ‹ we need to premise  that the Evidence Lower B Ound â€“ the ELBO â€“ of 
the model evidence ğ‘ƒ(ğ‘œ|ğœ‹) conditioned by the policy ğœ‹ corresponds to the negative variational free 
energy âˆ’ğ¹(ğ‘œ,ğœ‹). Using the Jensen inequality, we can write down: 
 
logğ‘ƒ(ğ‘œ|ğœ‹)= logğ”¼Q(ğ‘ |ğœ‹)[ğ‘ƒ(ğ‘ ,ğ‘œ|ğœ‹)
ğ‘„(ğ‘ |ğœ‹) ]â‰¥ğ”¼Q(ğ‘ |ğœ‹)[logğ‘ƒ(ğ‘ ,ğ‘œ|ğœ‹)
ğ‘„(ğ‘ |ğœ‹) ]=âˆ’ğ¹(ğ‘œ,ğœ‹) 
 
Considering that EFE is a variational free energy of future outcomes and future states under a fixed 
policy ğœ‹, from the current instant ğ‘¡ to some horizon ğ‘‡, we can define ğ†ğœ‹ (in analogy with the 
definition of variational free energy given in Equation (3)) as: 
 
 
 
ğ†ğœ‹ =âˆ‘G(ğœ‹,ğœ)
ğ‘‡
ğœ=ğ‘¡
= âˆ‘ğ”¼ğ‘„(ğ‘ ğœ,ğ‘œğœ |ğœ‹)[ln ğ‘„(ğ‘ ğœ|ğœ‹)
ğ‘ƒ(ğ‘ ğœ,ğ‘œğœ|ğœ‹,ğ‚)]
ğ‘‡
ğœ=ğ‘¡
 (6) 
 
where 
 
G(ğœ‹,ğœ) =ğ”¼ğ‘„Ìƒ[lnğ‘„(ğ‘ ğœ|ğœ‹)âˆ’lnğ‘ƒ(ğ‘ ğœ,ğ‘œğœ|ğœ‹,ğ‚)]  
 = ğ”¼ğ‘„Ìƒ[lnğ‘„(ğ‘ ğœ|ğœ‹)âˆ’lnğ‘ƒ(ğ‘ ğœ|ğ‘œğœ,ğœ‹)âˆ’lnğ‘ƒ(ğ‘œğœ|ğ‚)] (7) 
   
 
 13 
â‰ˆğ”¼ğ‘„Ìƒ[lnğ‘„(ğ‘ ğœ|ğœ‹)âˆ’lnğ‘„(ğ‘ ğœ|ğ‘œğœ,ğœ‹)]âˆ’ğ”¼ğ‘„Ìƒ[lnğ‘ƒ(ğ‘œğœ|ğ‚)] 
=ğ”¼ğ‘„Ìƒ[lnğ‘„(ğ‘œğœ|ğœ‹)âˆ’lnğ‘„(ğ‘œğœ|ğ‘ ğœ,ğœ‹)]âˆ’ğ”¼ğ‘„Ìƒ[lnğ‘ƒ(ğ‘œğœ|ğ‚)] 
=ğ·KL[ğ‘„(ğ‘œğœ|ğœ‹)||ğ‘ƒ(ğ‘œğœ|ğ‚)]+ğ”¼ğ‘„Ìƒ[ğ»[ğ‘ƒ(ğ‘œğœ|ğ‘ ğœ)]]. 
   
Here, ğ‘ƒ(ğ‘ ğœ,ğ‘œğœ|ğœ‹,ğ‚)â‰œğ‘ƒ(ğ‘œğœ|ğ‚)ğ‘ƒ(ğ‘ ğœ|ğ‘œğœ,ğœ‹) and ğ”¼ğ‘„Ìƒ[âˆ™] is the expected value under the predicted 
posterior distribution ğ‘„Ìƒ=ğ‘„(ğ‘œğœ,ğ‘ ğœ|ğœ‹)â‰œğ‘ƒ(ğ‘œğœ|ğ‘ ğœ)ğ‘„(ğ‘ ğœ|ğœ‹) over hidden states and their outcomes 
under a specific policy ğœ‹. The final identity in Equation (7) provides an interpretation of the expected 
free energy as a sum of two terms. T he former is the Kullback -Leibler divergence between 
(approximate) posterior and prior over the outcomes; it constitutes the quality score's pragmatic (or 
utility-maximising) component, favouring policies that realise expected outcomes under the 
generative model. The latter is the expec ted entropy under the posterior  over hidden states ; it  
represents the quality score's epistemic (or ambiguity-minimising) component, favouring policies that 
realise unambiguous outcomes. In other words, the former (pragmatic) term represents the risk that 
the anticipated outcomes ğ‘„(ğ‘œğœ|ğœ‹) diverge from prior preferences ğ‘ƒ(ğ‘œğœ), while the latter (epistemic) 
minimises ambiguity. In summary, risk measures the difference between predicted  outcomes ğ’ğœ
ğœ‹ in 
the future and preferred outcomes encoded in the C vector, while ambiguity quantifies to what extent 
a future state ğ’”ğœ
ğœ‹ diminishes uncertainty about future outcomes. From a machine learning perspective, 
this would be equivalent to saying that ğ†ğœ‹ embodies a â€œregularisationâ€ term, which balances between 
exploitive (pragmatic) and exploratory (epistemic) behaviour. 
 
The expected free energy of a policy ğ†ğœ‹ can be expressed in terms of linear algebra  by considering 
the equations for the free energy minimising sufficient statistics above, together with the generative 
model: 
 
 G(ğœ‹,ğœ)=ğ’ğœ
ğœ‹â‹…(lnğ’ğœ
ğœ‹ âˆ’lnğ‘ƒ(ğ‘œğœ))+ğ’”ğœ
ğœ‹ â‹…ğ‡ (8) 
with 
 
ğ’”ğœ
ğœ‹ =ğ(ğ‘¢ğœ =ğœ‹(ğœ))âˆ™ğ’”ğœâˆ’1
ğœ‹  
 
ğ’ğœ
ğœ‹ =ğ€âˆ™ğ’”ğœ
ğœ‹ (9) 
 
lnğ‘ƒ(ğ‘œğœ)=lnğ‚ 
 
ğ‡=âˆ’diag(ğ€âˆ™lnğ€)  
 
where lnğ‘ƒ(ğ‘œğœ) is the log-vector of preferred outcomes, and ğ‡ is the entropy matrix  pertaining to 
future outcomes. This provides a convenient way to evaluate a policy's expected free energy.  
 
Planning in Active Inference entails simulating the future, using Equation (5) and posterior beliefs 
about the hidden state ğ’”ğ‘¡
ğœ‹ at the current time and computing â€”for each policy ğœ‹(ğœ), with ğœ>ğ‘¡â€”the 
expected free energy ğº(ğœ‹,ğœ) of Equation (8). In other words, expected free energy is evaluated using 
the predictive posterior over future states (and outcomes). This means the predicted observations ğ’ğœ
ğœ‹ 
   
 
 14 
used for planning are not actual observations; rather, they are random variables in the future, with a 
predictive posterior for each policy. This calls for an expectation or averaging; hence, expected free 
energy. 
 
Crucially, standard implementations of active inference (with a few exceptions [59â€“62]) assume that 
an agent evaluates all policies ğœ‹ for any possible future state an agent could be in. This entails 
computing the expected free energy ğ‘®ğœ‹ of each policy ğœ‹. Once every plausible (i.e., allowable) policy 
has been scored (and its associated â€œqualityâ€ evaluated), the agent uses a Boltzmann distribution â€”
defined by the last two expressions of Equation (5) â€”to form posterior beliefs over policies, from 
which the next  action is selected for execution (by sampling from the posterior distribution). This 
approach has been used to address a variety of cognitive phenomena, including decision -making 
[20,63], habitual behaviour, salience, and curiosity-driven planning [64â€“67], and to develop a process 
theory for neural computation [11,68]. However, the necessity to evaluate all policies exhaustively 
renders the approach unable to solve large POMDP problems. To fill this gap, we introduce Active 
Inference Tree Search below. 
 
3. Active Inference Tree Search 
 
A straightforward method to overcome the limitations of active inference in large POMDP s is to 
elude the exhaustive evaluation of all allowable policies using a heuristic procedure [69] [24]. The 
efficacy of such a proposal needs to be assessed, considering the quality of the approximation and the 
gains in terms of computational costs and tractability. Here, we develop and evaluate a novel tree 
search scheme to render active inference in large POMDPs tractable: namely, Active Inference Tree 
Search (AcT). 
 
AcT solves POMDP problems using a planning tree: an abstract structure in which nodes ğ‘£ğœ 
correspond to beliefs ğ’”ğœ
ğœ‹ about the states that can be reached from the current state and where branches 
represent possible actions that can be taken to reach future states. The planning tree used in AcT 
differs from the tree commonly used in Reinforcement Learning. The tree is a structure that brings 
together all the instances of the POMDP problem generated by applying a sequence of possible 
actions (the policies) to a single instance (the root). Consequently, every path from the root to a leaf 
corresponds to a potential evolution of the POMDP problem, given a specific policy.  
 
The goal of using a planning tree is estimating the posterior over control states ğ‘ƒ(ğ‘¢) from which the 
best action ğ‘ğ‘¡ can be sampled. This estimate is obtained through simulations, which start from the 
current state ğ‘ ğ‘¡ (with a related observation ğ‘œğ‘¡) and proceed forward through specific branches of the 
decision tree, corresponding to a series of paths or histories â„ğœ =(ğ‘£ğ‘¡ğ‘¢ğ‘¡,ğ‘£ğ‘¡+1ğ‘¢ğ‘¡+1,â‹¯,ğ‘£ğœ). Here, we 
use ğœ to indicate the planning time steps, whereas we use ğ‘¡ to indicate root quantities; namely, 
universal or clock time in the environment. The simulations approximate, statistically, the expected 
free energy values G of the policies ğœ‹â‰¡(ğ‘¢ğ‘¡,ğ‘¢ğ‘¡+1,ğ‘¢ğ‘¡+2,â€¦,ğ‘¢ğœâˆ’1); the larger the number of 
simulations, the more reliable the approximation of G. This planning process is iterated until one or 
more halting conditions are satisfied . The depth ğ‘‘ of the planning tree depends on  two control 
parameters: the discount factor ğ›¿, and the discount horizon ğœ€. In our simulations, the maximum depth 
of a planning treeâ€”and consequently the maximum number of simulations employed to generate itâ€”
is fixed by imposing ğ›¿ğ‘‘ <ğœ€.  
 
   
 
 15 
Note that AcT is an algorithm to build a planning tree,  not to select actions. Here, we assume that 
after the planning tree has been built, the agent selects an action by sampling from the distribution of 
control states inferred at the root node and executes it. At this point, the agent makes a transition to a 
new state and receives a new observationâ€”and can start planning again. 
 
 
3.1 The four stages of Active Inference Tree Search  
 
Active Inference Tree Search comprises four successive stagesâ€”Variational Inference, Expansion, 
Evaluation, and Path Integrationâ€”applied iteratively at each time step ğ‘¡ (Figure 3) for a time ğœ>ğ‘¡ 
depending on the exit condition related to a discount factor ğ›¿, widely adopted in Monte Carlo 
Tree Search [26][70]. In the following, we examine each stage in detail. Furthermore, in Algorithm 
1, we report the pseudocode of the AcT algorithm (with subroutines) as a function of the parameters 
(ğ€, ğ,ğ‚,ğƒ) commonly adopted in active inference [10][11] and the aforementioned discount factor. 
 
Please note that from now on, we use ğ±ğœ rather than ğ’”ğœ
ğœ‹ to denote beliefs, to avoid confusions 
with the actual states ğ‘ ğ‘¡. 
 
3.1.1 First stage: Variational Inference 
 
The goal of the  first (variational inference) stage  (implemented in VARIATIONALINFERENCE of 
Algorithm 1) is to select the next non-terminal leaf node ğ‘£ğœ of the tree to expand. From the rootâ€”and 
recursively until reaching an expandable node of the planning tree â€”, by using the second one of 
 
 
Figure 3. The four stages of the Active Inference Tree Search ( AcT) algorithm. Note that the first 
two stages (variational inference and expansion) could be considered as two parts of  a single 
TreePolicy procedure of MCTS. The third (evaluation) and the fourth (path integration) stages 
could be called Eval and PahtIntegration procedures, respectively . The third (evaluation) stage 
can also be called an Eval procedure. See the main text for an illustration of each of the four stages. 
 

   
 
 16 
Equations (5), this stage samples an action over a Boltzmann distribution ğœ(ğœ…ğ‘lnğ„âˆ’ğ›¾ğœ âˆ™G(ğ‘£ğœ)) 
that depends on three terms: 1) the EFE G(ğ‘£ğœ)=(ğ€âˆ™ğ±ğœ)âˆ™(ln (ğ€âˆ™ğ±ğœ)âˆ’lnğ‚)âˆ’ğ±ğœâˆ™
(âˆ‘ ğ€ğ‘–ğ‘—lnğ€ğ‘–ğ‘—ğ‘– )ğ‘—, defined in Equations (8) and (9), of the policy ğœ‹ğœ assembled so far, 2) the precision 
ğ›¾ğœ computed at each depth of the tree visit, and 3) the prior belief about the policy ğ„ [71].  
 
 
 
Algorithm 1.   Active Inference Tree Search algorithm with subroutines 
 
function AcT(ğ€, ğ,ğ‚,ğƒ,ğ›¿) 
ğ‘¡âŸµ0 
while halting conditions are not satisfied do 
ğ±ğ‘¡ âŸµ update expected state belief ğ±ğ‘¡âˆ’1 by using the first line of Equation (5)  
create a node ğ‘£(ğ±ğ‘¡,ğ‘¢ğ‘¡âˆ’1) 
while ğ›¿ğœ <ğœ€ do 
ğ‘£ğœ âŸµTREEPOLICY(ğ‘£,ğ) 
GÎ” âŸµEVAL(ğ‘£ğœ,ğ€,ğ,ğ‚,ğ›¿) 
PATHINTEGRATION(ğ‘£ğœ, GÎ”) 
End 
(ğ‘ ğ‘¡+1,ğ‘œğ‘¡+1,ğ±ğ‘¡,ğ‘ğ‘¡)âŸµextract information saved in ğ‘£ 
ğ‘¡âŸµğ‘¡+1 
end 
end 
 
 
function TREEPOLICY(ğ‘£) 
while ğ‘£ is nonterminal do 
if  ğ‘£ not fully expanded then 
return EXPANSION(ğ‘£,ğ) 
else 
ğ‘£â€²âŸµVARIATIONALINFERENCE(ğ‘£) 
end 
end 
return  ğ‘£ğœ âŸµğ‘£â€² 
end 
 
 
function EXPANSION(ğ‘£,ğ) 
draw randomly an unused action ğ‘¢â€² on ğ‘£ 
for the parent ğ‘£, generate a new child ğ‘£â€²(ğ±â€²,ğ‘¢â€²) with ğ±â€² =ğ(ğ‘¢â€²)âˆ™ğ± 
return  ğ‘£â€² 
end 
 
 
   
 
 17 
 
Taken together, these three terms  define the estimated quality of a policy and consider 1) the 
divergence between preferences encoded in ğ‚ and the expected outcomes ğ€âˆ™ğ±ğœ, and expected 
entropy of observations (respectively, first and second terms of G(ğ‘£ğœ)), 2) a modulation of the policy 
quality distribution that controls the stochasticity of action selection, and 3) a confidence bound that 
regulates exploration.  
 
Note that in standard implementations of active inference, the latter term ( ğ‘¬(ğ‘£â€²)) usually encodes 
prior beliefs about policies that have become habitual (i.e., habitual priors combined with empirical 
priors furnished by the expected free energy). Instead, in AcT, we use the term ( ğ‘¬(ğ‘£â€²))  to promote 
exploration during tree search, analogous to the exploration bonus used in the UCB1 algorithm for 
multi-armed bandits [40][72]. Specifically, we define a probabilistic distribution ğ‘¬(ğ‘£â€²)=
âˆš2ğ‘™ğ‘›ğ‘(ğ‘£) ğ‘(ğ‘£â€²)â„ , where ğ‘£â€² denotes a child node, ğ‘(ğ‘£â€²) denotes the number of visits of ğ‘£â€², and 
ğ‘(ğ‘£) denotes the number of visits of the parent node ğ‘£. Given this definition of ğ‘¬, the probability of 
every child node decreases if it is visited frequently (i.e., with high ğ‘(ğ‘£â€²)) and increases when its 
number of visits is sufficiently lower than that of the other children (i.e., a large ratio ğ‘™ğ‘›ğ‘(ğ‘£) ğ‘(ğ‘£â€²)â„ ). 
The effects of ğ‘¬ are modulated by an exploration factor ğœ…ğ‘ and they promote exploration, along with 
the exploratory (epistemic) term of the expected free energy of policies ğ‘®ğœ‹ detailed in Section 2.  
Therefore, analogous to the UCT algorithm [26][70], the Variational Inference stage may select every 
node with a probability different from zero, increasing in time for less visited states.  
 
3.1.2 Second stage: Expansion 
 
function VARIATIONALINFERENCE(ğ‘£) 
build the distribution ğ„ via the probability mass function âˆš
2lnğ‘(ğ‘£)
ğ‘(ğ‘£â€²)   
 ğ‘¢â€²~ğœ(ğœ…ğ‘ lnğ„(ğ‘£â€²)âˆ’ğ›¾âˆ™GÎ”(ğ‘£â€²)) 
return ğ‘£â€²(ğ‘¢â€²) 
end 
 
 
 
function EVAL(ğ‘£ğœ,ğ€,ğ,ğ‚,ğ›¿) 
compute the expected free energy G(ğ‘£ğœ) through ğ€,ğ,ğ‚ 
return  GÎ” = ğ›¿ğœâˆ™G(ğ‘£ğœ) 
end 
 
function PATHINTEGRATION(ğ‘£ğœ, GÎ”) 
while ğ‘£ğœ is not ğ‘£ do 
ğ‘(ğ‘£ğœ)âŸµ ğ‘(ğ‘£ğœ)+1 
G(ğ‘£ğœ)âŸµ G(ğ‘£ğœ)+ 1
ğ‘(ğ‘£ğœ)(GÎ”âˆ’G(ğ‘£ğœ)) 
ğ‘£ğœ âŸµ parent of ğ‘£ğœ  
end 
end 
   
 
 18 
The second (expansion) stage  (the function EXPANSION in Algorithm 1) aims to expand the non-
terminal leaf node ğ‘£ğœ selected during the former (variational inference) stage . Expansion of a leaf 
node ğ‘£ğœ corresponds to instantiating a new child node ğ‘£â€² by implementing a random action ğ‘¢â€² among 
those previously unused. Each of these children stands for a future state ğ±â€² an agent can  visit, 
according to the transitions defined in the matrix ğ. By adopting a Bayesian terminology, expanding 
could mean defining new predictable events over the space of future policies, thereby expanding the 
horizon of possible events. Note that the first (variational inference) and the second (expansion) 
stages return the same outputâ€”a nodeâ€”but the former stage selects a node, whereas the latter creates 
a node. We implemented these two stages into a unique routine, TREEPOLICY in Algorithm 1, 
analogous to TreePolicy routine [39] in MCTS. In Bayesian statistics, this is not unlike the procedures 
implicit in nonparametric Bayes, based upon stick-breaking processes that allow for an expansion of 
latent states [73]. 
 
3.1.3 Third stage: Evaluation  
 
The goal of the third (evaluation) stage â€“ implemented by the function EVAL of Algorithm 1 â€“ is to 
assign a value to the leaf node ğ‘£ğœ expanded in the previous phase. The evaluation considers the 
expected free energy G(ğ‘£ğœ), a function of the state and the observation associated with ğ‘£ğœ. Note that 
G(ğ‘£ğœ) scores the EFE of the node ğ‘£ğœ, not the sum of the EFEs of all the nodes from the root to the 
node ğ‘£ğœ. The EFE is then weighted by its â€˜temporal precisionâ€™:  a discounted factor equal to ğ›¿ğœ that 
depends on a n arbitrary parameter ğ›¿ and the depth  ğœ of the tree node ğ‘£ğœ. The resulting GÎ” = ğ›¿ğœâ‹…
G(ğ‘£ğœ) value, denoted as â€˜predictive EFEâ€™, is finally assigned to ğ‘£ğœ. Please note that, unlike the original 
MCTS algorithm, evaluating the quality of nodes does not require random policies (or rollouts). This 
is because the EFE functional computed in the node ğ‘£ğœ is a sort of an information-based measure that 
permits simultaneously estimating both the exploitive and the explorative (epistemic) statistical 
characteristics of the problem configuration represented by the node. 
 
3.1.4 Fourth stage: Path Integration 
 
The fourth (path integration) stage (corresponding to the routine PATHINTEGRATION reported in 
Algorithm 1) aims to adjust the ğ† values of the tree nodes up to the root node ğ‘£ğ‘¡ by considering the 
new values obtained during the third ( evaluation) stage. The value estimated by EVAL is used to 
update the quality ğ† and the number of visits ğ‘ of the nodes on the tree path ğ‘£ğœ,â‹¯,ğ‘£ğ‘¡ obtained during 
the two phases of variational inference and expansion (which jointly form the  TREEPOLICY 
procedure). Such updating is the statistical analogue of â€œpath-integrationâ€ formulations in active 
inference [10], where one sums up the expected free energy at each time step in the future.  
 
The four stages are repeated iteratively until a criterion is met to provide estimates of the ğ† values of 
a tree node subnet.  
 
3.2 Computational resources required by Active Tree Search 
 
Most on-line planning algorithms employ a belief tree like the one shown in Figure 1, to encode the 
POMDP problem in a manageable form. The on-line algorithms implement multiple lookahead visits 
   
 
 19 
on the tree to plan the next action to execute . A belief tree of depth ğ· contains ğ’ª(|ğ‘ˆ|ğ· |ğ‘|ğ·) nodes 
where |ğ‘ˆ| and |ğ‘| are the cardinalities of the action and observation set, respectively; therefore, the 
tree size influences the performance of every algorithm relying on belief tree visits in their planning 
phase. For example, the popular POMCP algorithm [41], which grandfathers the algorithms based on 
tree visit sampling, is prone to this complexity. The R-DESPOT algorithm [43] generates a subtree 
of the belief tree of size ğ’ª(|ğ‘ˆ|ğ· ğ¾) that encompasses the executions of all policies by ğ¾ abstract 
simulations called scenarios. In contrast, AcT works on trees with ğ’ª(|ğ‘ˆ|ğ·) nodesâ€”of considerably 
less complexityâ€”but uses lossless probabilistic representations for beliefs and observation s. This 
requires greater memory resources and longer updating operations during the run. On the other hand, 
as noted in  [43], POMCP and R -DESPOT, which approximate belief states and  outcomes with a 
particle filter, have problems dealing with  large observation spaces because beliefs could collapse 
into single particles, causing convergence to suboptimal policies. 
 
The total computational cost of AcT depends on the number of simulations controlled through the 
discount condition ğ›¿ğ‘‘ <ğœ€, and on the EFE computation in the (computationally expensive) 
Evaluation routine. The number of requisite floating -point operations is proportional to the size of 
the generative model (e.g., the number of hidden states |S| times the number of outcomes |O|). Note 
that it would be possible to approximate or amortise EFE computations to reduce computational 
demands (not explored in this article). 
 
4. Results of the simulations using Active Inference Tree Search 
 
We tested the Active Inference Tree Search on three exemplar problems. The first two problems (a 
deceptive binary tree and a non Lipschitzian function) exemplify deceptive â€œtrapsâ€ that are known to 
challenge UCT and similar algorithms. The notion of â€œtrapâ€ is used in adversarial games strategy 
search [74] to indicate those states of a game whose instantaneous utility is deceptive, with respect to 
their future outcomes. The third problem is a POMDP benchmark, the RockSample [34], which is 
often used to evaluate  the effectiveness and scalability of planning algorithms as the problem 
complexity increases2.  
 
4.1 Active Inference Tree Search avoids traps in deceptive binary trees 
 
Ramanujan et al. [76] noted that the performance of the UCT algorithm is limited in games where the 
best proximal decisions do not necessarily correspond to winning strategies. This is t he case, for 
example, in chess, where exhaustive search (e.g., minimax) yields better performance than sampling-
based approaches . T his is due to particular game configurations in which  an unfortunate move  
unavoidably leads to a defeat. These game states are called traps because, as noticed above, their  
instantaneous utility is deceptive with respect to the future outcomes that they lead to. Being able to 
escape from traps is a crucial feature of successful planning algorithms. 
 
 
2 Our simulations are carried out using a C++ implementation of AcT that extends a header library 
described in [75]. The library implements a multi -core parallelisation of the most demanding 
computational kernels. Note that most of AcT's computational complexity depends on the 
multidimensional inner products involved in EFE computation and state estimation. 
   
 
 20 
Coquelin & Munos [42] introduced an example challenging problem for sampling -based planning 
algorithms. This problem has just two actions, 2ğ·+1 states parameterised by ğ·âˆˆâ„•, and 
deterministic rewards. At each time step ğ‘‘, one can get a reward of (ğ·âˆ’ğ‘‘)/ğ· by choosing action 2; 
alternatively, one can move forward by choosing action 1. At time ğ·âˆ’1, action 1 corresponds to an 
absorbing state with maximum reward 1 and action 2 to another absorbing sta te with reward 0.  
Intuitively, the state space of the problem can be described as a binary tree of depth ğ· (Figure 4). The 
optimal plan involves always selecting action 1, to move along all the branch levels ğ‘‘ and reach the 
final (maximum) reward. Finding this solution is challenging for sampling methods, as the suboptimal 
action 2 is much more rewarding in the proximity of the rootâ€”and this immediate reward influences 
planning following the first moves. Coquelin & Munos proved by induction that UCT has a hyper -
exponential dependency concerning the depth D of the binary tree . Considering the worst  case, it 
takes Î©(exp(â€¦(exp(1))â€¦)) â€“ composed by ğ·âˆ’1 exponential functions to get the reward.  
 
To check whether AcT suffers the same limitation, we compared UCT, AcT , and a reduced version 
of AcT, called FE, which does not use the policy prior beliefs ğ„ during the exploration stage (or, 
analogously, with ğœ…ğ‘ =0). To render this problem suitable for AcT, we reformulated it as an MDP 
problem, i.e., a Markov Decision P roblem that is fully observable. The MDP comprises  2ğ·+1 
hidden states, a corresponding set of 2ğ·+1 observations (consequently ğ€ is diagonal), action â€œ1â€ 
and â€œ2â€ to move from one state to another (reported in ğ) and a vector ğ‚ where rewards are spread 
over observations by a probabilistic distribution encoding preferences. 
 
 
 
We considered three problems of increasing depth: ğ·=10, 100, and 1000. For each problem, we 
collected the results of 1000 executions of the thr ee algorithms (UCT, AcT and FE)  using a fixed 
 
Figure 4. A binary tree representing the state space of a challenging problem for sampling-based 
planning methods (adapted from [39]). From the root (left node) toward the deepest level D, action 
2 at each level leads to a deceptive leaf node with reward (ğ·âˆ’ğ‘‘)/ğ·. The optimal policy involves 
always selecting action 1, which yields reward 1.  
 

   
 
 21 
number of simulations or playouts (5000). We used a discount factor of ğ›¿=0.95, and set the 
exploration parameter ğœ…ğ‘ =1 for both UCT and AcT. The results are shown in Figure 5 by plottingâ€”
for each algorithm â€”the modes of the occupied  states, the occupancy probabilities, and the failure 
rate (defined as the relative difference  (ğ·âˆ’ğ‘‘)/ğ· between the depth ğ‘‘ of the visited state and ğ·, 
with values in [0,1] where 0 states for no failure), as a function of the simulation number.  
 
Our results show that UCT experiences problems starting from ğ·=10 selecting the first deceptive 
states. Conversely, both algorithms using active inference reached the deepest state, despite 
performance decreases with greater ğ·. FE exhibits a more pronounced greedy behaviour. At the same 
time, AcT keeps exploring due to the prior distribution ğ„. This numerical analysis suggests that AcT 
has the best performance: compared to the other algorithmsâ€”it reaches deeper states of the deceptive 
tree and does so faster. 
 
   
 
 22 

   
 
 23 
 
Figure 5. Experimental results in the deceptive binary tree of [42] of depths ğ·=10, 100,1000 (top, 
middle, and bottom panels, respectively). Each panel plots state occupation (top), occupation 
probability (mid dle), and failure rate (bottom)  as a function of the number of simulations (or 
playouts). 
 
 
4.2 Active Inference Tree Search reaches an adaptive level of exploration when finding the global 
maximum of a non Lipschitzian function  
 
The above problem can be considered illustrative of a whole class of MDP domains on which 
sampling algorithms manifest shortcomings [77]. These problems are all characterised by the lack of 
smoothness of the objective or value function, where the notion of â€œsmoothnessâ€  corresponds to a  
well-behaved analytic or continuous value function. Formally, this condition can be  expressed 
through the Lipschitz continuity , according to which a value function ğ‘‰(ğ‘ ) defined over the state -
space ğ‘† is ğ‘€-Lipschitz continuous if âˆ€ ğ‘ 1,ğ‘ 2 âˆˆğ‘†, |ğ‘‰(ğ‘ 1)âˆ’ğ‘‰(ğ‘ 2)|â‰¤ğ‘€â€–ğ‘˜(ğ‘ 1)âˆ’ğ‘˜(ğ‘ 2)â€–, where ğ‘€ 
is a constant and ğ‘˜(âˆ™) is a mapping from ğ‘† to some normed vector space [78]. The challenge for any 
optimisation scheme is to find the global maximum of a non-Lipschitzian function. The function: 
 
 
ğ‘”(ğ‘¥)={
0.5+0.5|sin 1
ğ‘¥5 |, 0<ğ‘¥<0.5
0.35+0.5|sin 1
ğ‘¥5 |, 0.5â‰¤ğ‘¥â‰¤1
 (10) 
 
introduced as a test in [77], has two distinct behaviours over its domain (see panel A in Figure 6). In 
the (left) interval [0,0.5], there exist numerous global optima, but their functional form is quite rough, 
 

   
 
 24 
whereby in the (right) interval [0.5,1], the function is smooth, but the extrema are suboptimal. In this 
case, an effective search algorithm should explore every domain region.  
 
As for the binary-tree test used before, we cast this optimisation problem as MDP problem: each state 
represents some interval [ğ‘,ğ‘] within this unit square, with the starting state representing [0, 1]. We 
assume that there are two available actions at each state, the former resulting in a transition to the 
new state [ğ‘,(ğ‘âˆ’ğ‘)/2 ] and the second resulting in a transition to [(ğ‘âˆ’ğ‘)/2 ,ğ‘]. For example, at 
the starting state , the agent has the choice between a "left" action to make a transition to the state 
[0,0.5] and a "right" action to make a transition to the state [0.5,1]. After it selects the left action, it 
has a choice between another "left" action to mak e a transition to [0,0.25] or  a "right" action to 
[0.25,0.5], and so on. Consequently, with increasingly deeper planning trees, the agent explores more 
fine-grained intervals. An efficient planner should visit the left interval [0,0.5] extensively and deeply 
(i.e., approach zero), as it encompasses many maxima.  
 
The state-space ğ‘† can be represented as a binary tree whose depth is constrained by a trade-off set by 
the condition ğ‘âˆ’ğ‘<10âˆ’5. Transitions between states, which move from a state ğ‘  at a depth ğ‘‘ of 
the binary tree to a state ğ‘ â€² at depth ğ‘‘+1 are controlled through the matrix B. Analogous to the 
function â€œğ‘”â€ shown in Equation 10 , also the transition function ğ‘ â€² =ğµ(ğ‘ ,ğ‘¢,ğ‘¡) is Lipschitzian but 
only for domain values larger than 0.5. This is evident by plotting (Figure 6, panel B) the Lipschitzian 
constant ğ‘€ averaged over all the transitions between two consecutive depths, for ğ‘¥<0.5 and for ğ‘¥>
0.5 (blue and red lines, respectively). For ğ‘¥âˆˆ [0.5,1], ğ‘€ increases until it reaches  the upper bound 
value of 10 ( for ğ‘‘>10). Instead, for ğ‘¥âˆˆ[0,0.5), ğ‘€ shows an exponential behaviour and rapidly 
reaches much greater values. Each state corresponds to one observation, resulting in an MDP where 
the matrix ğ€ is diagonal. The a priori distribution ğ‚ is computed empirically by considering the value 
of ğ‘”(ğ‘¥) in the midpoint of the domain interval encoded by the states. The discount factor ğ›¿ was set 
to 0.95. 
   
 
 25 
 
We compared  the UCT and AcT algorithms, for 1000 executions each, with three levels of the 
exploration factor ğœ…ğ‘ (ğœ…ğ‘ = 1, 5, 10). Unlike [77], we found that UCT explores the whole domain of 
ğ‘”, although it mostly visits a state corresponding to an ğ‘¥ value around 0.9; see the element (1,1) of 
the matrix of plots in Figure 6C. AcT explores deeper parts of  the tree search (plot (2,1) in Figure 
 
 
 
Figure 6. The function â€œgâ€ used to tes t the efficacy of the AcT and UCT algorithms in problems 
with a rough landscape. A) â€œgâ€ function defined in [0,1] is Lipschitz-continuous for values larger 
than 0.5, yet it is not otherwise. B) The average value of the Lipschitz constant M over all the state 
transitions at a given depth of the binary tree used to encode the optimi sation problem as MDP. 
The red curve is for ğ‘¥âˆˆ[0.5,1], whereas the blue plot is for x âˆˆ[0,0.5]. C) A matrix plot whose 
elements are the scatter plots of the values encoded by the node states visited by UCT and AcT 
(1000 executions each); the bottom parts show  the histograms ğœˆ(ğ‘¥) of their domain values. The 
matrix plot arranges the rows b y the  algorithm (first row for UCT, second row for AcT) and 
columns according to the values of the exploration factor ğœ…ğ‘ set for the executions. 
 

   
 
 26 
6C) and is able to find maxima in the whole domain of ğ‘”. Compared to UCT, AcT shows greater 
exploitative behaviour in correspondence with specific significant ğ‘¥ values, for instance, 0.5, 0.9, and 
0.2 (that are the modes of the visited ğ‘¥ distributions). 
 
This optimi sation problem illustrates the effects of the exploration factor ğœ…ğ‘ on algorithm 
performance. Reading Figure 6C out by columns, one can evaluate the effects of ğœ…ğ‘ on the algorithms 
UCT (first row) and AcT (second row). The performance of UCT remains relatively stable across all 
the values of ğœ–, in the sense that despite the increase of exploration with greater values of ğœ…ğ‘, the 
statistical distribution ğœˆ(ğ‘¥) of the visited domain points remains unchanged. Instead, the effects of 
the parameter ğœ– on AcT are more significant. For ğœ…ğ‘ =1, AcT shows limited exploration, similar to 
the FE algorithm used in the deceptive binary tree example. This is because, in this particular problem, 
the term used to sample the actions (related to the ğ„ and controlled by ğœ…ğ‘) is numerically much smaller 
than the one related to the policy value ğ†. When ğœ– is set to 5 or 10, AcT explores significantly more 
and (with ğœ…ğ‘ =10) it visits the ğ‘” codomain uniformly. In this latter case, AcT also visits the 
unrewarded branches of the binary tree, even if this implies a reduction of performance; this becomes 
apparent by noticing that ğœˆ(ğ‘¥) is almost flat for ğœ…ğ‘ =10. 
 
4.3 Active Tree Search in large POMDP problems: the case of RockSample 
 
RockSample(ğ‘›,ğ‘˜) [34] is a well-known benchmark problem for assessing POMDP solvers and their 
scalability. It simulates a rover whose task is collecting samples of ğ‘˜ scientifically valuable rocks 
deployed on an ğ‘›Ã—ğ‘› alien soil grid â€”and then leaving the area. Samples come in two varieties: 
valuable or invaluable. The rover earns a reward for each valuable sample it collects and a penalty 
for each invaluable sample. The rover knows the locations of the rocks but can only evaluate whether 
they are valuable via a long-range sensor, whose measures are affected by an error , which increases 
exponentially with the distance between the rover and the rock examined. We considered two variants 
of this problem, with (ğ‘›,ğ‘˜) equal to (7,8) and to (11,11).  
 
Representing this problem in a format suitable for active inference is straightforward. In principle, 
the problem state space ğ‘† can be factorised to reduce the total number of states [79]. Still, we decided 
to retain both variants without factorised representation s to leverage the problem's difficulty. 
Therefore, the cardinality of the state space is |ğ‘†|=ğ‘›2âˆ—2ğ‘˜ +1 (12,544 in the case (7,8) and 247,808 
in (11,11)), necessary to encode every possible combination of locations and the scientific value of 
the rocks plus an additional â€œexitâ€ state. This cardinality is needed to define the initial belief state ğƒ 
and the transition state matrix B, which is also conditioned on the control state (actions) ğ‘âˆˆğ‘ˆ that 
the agent can make.  
 
The set ğ‘ˆ contains the four actions (go north, go south, go east, go west) that the agent uses to move 
around the square, ğ‘˜ actions that the agent uses to evaluate the rock s remotely (one action for each 
rock), and a sampling action to collect a rock sample. Observations are factorised into three factors, 
which relate to the positions on the grid, the configuration (2ğ‘˜) encoding the scientific quality of the 
rocks, and their associated rewards, respectively. Accordingly, the likelihood A is decomposed into 
three factors, each one encoded as a cubic matrix (generally as a tensor when the state space, in turn, 
is subdivided in to factors), where the first dimension represents the observations, the second the 
   
 
 27 
states, and the last the actions. Introducing a dependency of ğ€ on the actions is uncommon in active 
inference but useful in many POMDP problems,  including RockSample(ğ‘›,ğ‘˜). This is because the 
observation one gets by sampling a rock (with an action ğ‘˜) is a function of the distance from the rock; 
encoding this contingency would require a considerable number of states if one does not express  ğ€ 
as a function of actions. Reward contingencies expressed in ğ€ are action-dependent, too, as the agent 
obtains a â€œgoodâ€ observation when it samples a good rock (and a â€œbadâ€ observation otherwise)â€”and 
when it exits the game. Finally, ğ‚ encodes preferred observations and comprises three modalities: in 
the first two, observations are uniformly preferred, while in the last, they are drawn from a Bernoulli 
distribution, with a success probability almost equal to 1. See the Appendix for an example generative 
model for RockSample, with ğ‘›=2 and ğ‘˜=1. 
 
We used the same parameters for both RockSample(7,8) and RockSample(11,11). We used a 
discount factor ğ›¿ equal to 0.95 and a â€˜discount horizonâ€™ ğœ€ of 0.4 so that the depth ğ‘‘ of the tree search 
developed during planning is about 19 steps (a threshold computed by considering that ğ›¿ğ‘‘ <ğœ€). We 
evaluated 1000 executions of AcT, with different seeds from a pseudorandom number generator and 
with different (random) arrangements of rocks in the grid. 
 
In keeping with previous works [41][43], we augmented the AcT algorithm with a domain-specific, 
heuristic policy that prioriti ses some selected actions during the simulations . Specifically, the  
heuristic policy prioritises actions that approach the rocks with more â€œgoodâ€ observations and actions 
that check rocks with uncertain outcomes (ensuing from inconsistent observations). When the rover 
is in the same place as a rock evaluated as â€œgoodâ€ by most observations, the heuristic policy prioritises 
sampling actions. Finally, when all the rocks in the scenario have been sampled, the heuristic policy 
prioritises actions heading toward the exit.  
 
 
 
Figure 7.  Total discounted reward achieved by the AcT algorithm augmented with the heuristic policy 
for RockSample(7,8) and RockSample(11,11) (in blue and red, respectively). Results are shown as a 
function of the time steps (at the bottom axis) and the number of simulations (in logarithmic scale, top 
axis). All results are averaged over 1000 executions.  

   
 
 28 
 
Figure 7  shows the model performance, expressed as  the total discounted reward  (ADR) ğ‘…Ì…ğ‘¡
ğ›¿ =
1
ğ‘âˆ‘ âˆ‘ ğ›¿ğœğ‘Ÿğ‘–
ğ‘¡
ğœ=0 (ğœ)ğ‘
ğ‘–=1  averaged over ğ‘ executions, as a function of time steps ğ‘¡ required to complete 
the task. In RockSample(7,8), AcT achieves 18.35Â±4.17 ADR in 38.92 time steps and 52,672.5 
simulations ( on average). In RockSample(11,11), AcT achieves 15.71Â±3.86 ADR in 74.93 
timesteps, with 238,461 simulations (on average)3. In both RockSample(7,8) and 
RockSample(11,11), AcT scales up smoothly with the size of the problem, as eviden ced by the fact 
that the algorithm's performance shows the same trend in both problems. 
 
We analysed the performance of AcT in RockSample(7,8) and RockSample(11,11) as a function of 
the horizon discount parameter ğœ€  (Figure 8A). As expected, the performance of AcT decreases when 
ğœ€ increases (and consequently, the maximum depth of the planning trees decreases). It could be noted 
that there is a threshold (which is plausibly domain-specific), after which the increase of ğœ€ becomes 
catastrophic. Indeed, AcT preserves its effectiveness  between ğœ€=0.4 and ğœ€=0.7 but becomes  
unsuccessful with ğœ€=0.9 when the planning tree becomes excessively small. 
 
3 The AcT algorithm without the domain -specific heuristic policy achieves significantly lower scores: 
13.4251Â±5.89 ADR for RockSample(7,8) with ğœ€=0.7, and 6.15488Â±3.82329 ADR for 
RockSample(11,11) with ğœ€=0.9. Please note that these results are obtained with ğœ€ values significantly higher 
than those reported in the main simulation (ğœ€=0.4). This is because using ğœ€=0.4 without the heuristic policy 
entails significant memory demands. Increasing ğœ€ permits decreasing memory deman ds by constraining the 
depth of the planning tree. 
 
 
 
 
Figure 8.  Total discounted reward  (ADR) averaged over 1000 execution, obtained by AcT as a 
function of its control parameters. Left panel:  ADR as a function of the discount horizon  ğœ€ (with 
ğœ…ğ‘ =1). Right panel: ADR as a function of the exploration factor ğœ…ğ‘ (with ğœ€=0.4). In both panels, 
the blue and the red lines represent the results obtained for RockSample(7,8) and 
RockSample(11,11), respectively. 
 
 

   
 
 29 
 
Furthermore, we analysed the performance of AcT in RockSample(7,8) and RockSample(11,11) as 
a function of the exploration factor ğœ…ğ‘ (Figure 8B) while keeping the horizon discount parameter ğœ€   
fixed (ğœ€=0.4). The performance of AcT decreases when the exploration factor ğœ…ğ‘ is increased from 
1 to 5 or 10 (the results for ğœ…ğ‘ =5 and ğœ…ğ‘ =10 are almost indistinguishable). This result indicates 
that an explorative approach is ineffective in RockSample, as it leads AcT to overextend the tree 
width, disregarding the most rewarding branches. 
 
In sum, the simulations reported in this section provide a proof of principle that active inference can 
be scaled to deep planning problems. In the next section, we consider the Active Inference Tree 
Search algorithm from the perspective of neuronal dynamics. 
 
4.4 Simulated neuronal dynamics of Active Inference Tree Search 
 
This section illustrate s the usage of Active Inference Tree Search to simulate  behavioural and 
neurophysiological responses during human planning. To exemplify this, we apply Active Inference 
Tree Search to â€œTigerâ€: a popular POMDP problem introduced in [80], to illustrate the importance of 
epistemic, information-gathering actions (that aim to acquire information to reduce uncertainty) 
during planning. In the Tiger problem, an agent stands in front of two doors and has to decide which 
one to open. The agent knows that one of the two doors hides a treasure, whereas the other conceals 
a tiger. If the agent opens the door with the treasure, it receives a reward, but if it opens the door with 
the tiger, it receives a penalty. The agent does not know where the tiger is but can resolve uncertainty 
by "listening for animal's noises" (which induces a small cost). 
 
The domain of this problem is usually represented as a POMDP with 2 states (tiger behind the left or 
right doors), 3 actions (to open the two doors or listen) , and 2 observations (reward or penalty). To 
ensure compatibility with previous active inference studies, we recast the problem as a T-maze with 
8 states, 4 actions and 16 observations [10]. The 8 states result from the multiplication of 4 locations 
times 2 hidden contexts. The 4 locations correspond to the centre (i.e., start location), the left and the 
right arms (analogous to the two doors, with treasure and tiger , respectively ), and the lower arm 
(analogous to a listening location, where a cue can be found that discriminates the tiger location). The 
2 hidden contexts correspond to the 2 possible reward locations (i.e., the reward at the left or the right 
arm, respectively). The 4 actions move  the agent deterministically to each of the 4 locations (but 
cannot change hidden context) . Finally, the 16 observations result from the multiplication of 4 
positional observations (that correspond 1 -to-1 to the 4 locations ) by 4 outcomes (i.e., reward, 
penalty, cue for the tiger at left, and cue for the tiger at right) that are obtained in different states, see 
below.  
 
The matrices ğ€, ğ, and the vectors ğ‚ and ğƒ specify the agent's generative model. The (likelihood) 
matrix ğ€ is a probabilistic mapping  from states to outcomes . It specifies that the centre location 
provides an ambiguous cue (i.e., a cue that is identical if the agent is in either of the 2 hidden contexts 
and hence does not provide any information about the reward location). Furthermore, it specifies that 
the lower arm provides a disambiguating cueâ€”that discloses which of the 2 hidden contexts the agent 
is in (and hence the reward location). Finally, the likelihood specifies that if the agent is in the first 
   
 
 30 
hidden context ("reward at the right arm"), the right and the left arms provide a reward and a penalty, 
respectively, with probability ğ‘=0.90. On the contrary, if the agent is in the second hidden context 
("reward at the left"), the right and the left arms provide a penalty and a reward, respectively, with 
probability ğ‘=0.90. 
 
The ğ(ğ‘¢) (transition) matrices define 4 action-specific stochastic transitions between states. These 
move the agent deterministically to each of the 4 locations (but cannot change its hidden context). 
However, there is a peculiarity: given that the task ends when the agent is in one of the upper arms 
(i.e., opens one of the two doors) , we consider the corresponding hidden states as absorbing states 
that cannot be left, whatever the action.  
 
The vector ğ‚ encodes the probability mass over preferred outcomes. It is determined by applying the 
Softmax function over a utility vector having 2 and âˆ’2, respectively, for rewarding and penalty 
outcomes and zeros otherwise. Finally, ğƒ represents the agent's belief about its initial state. The agent 
knows that it starts from the centre location, butâ€”cruciallyâ€”it does not know in which of the 2 
hidden contexts it is in (i.e., it does not know the reward location, left or right).  This is why it is 
optimal for the agent to go to the lower arm to  solicit a cue (i.e., "listen")  that disambiguates the 
hidden context before deciding whether to visit the left or right arms.  As in the previous 
simulations ğ›¿=0.9. 
 
Figure 9 illustrates the results of Active Inference Tree Search simulations, in which the reward is in 
the right arm. The upper panel shows the agent's state-belief distribution over time and the true states 
(cyan circles). At the first epoch, the agent knows it is in the centre location but does not know its 
current context. This is evident when considering that in the first column of the upper panel, the belief 
distribution spans states 1 and 2 (i.e., centre location in the first and second hidden contexts). The 
agent then selects an action to visit the lower arm to collect a cue; and it discovers that the hidden 
context is the first (reward at the right arm). This is evident in the second column of the upper panel, 
where the belief distribution is concentrated in state 7 (i.e., lower arm, first hidden context). Note that 
the agent decided to explore the lower arm to secure a cue instead of guessing which of the two arms 
is rewarding.  Although this entails a cost  (a delay in reaching the reward location later) , this 
â€œepistemic behaviourâ€ ensures the selection of the rewarding arm at the next epoch, see the third 
column of the upper panel. This epistemic behaviour emerges automatically in active inference (K. 
J. Friston, 2010) because policy selection considers the expected reduction of uncertainty along with 
utility maximisation (see Equation (7) of the section â€œactive inferenceâ€).  
 
The second panel of Figure 9 shows the â€œsearch treesâ€ that AcT  generates during each epoch. The 
left picture of the second panel shows the search tree generated during the first epoch, where the 
thickness of the edges connecting levels reports the probabilities of going to one of the 4 locations. 
The preferred plan at depth one is to make an â€œepistemicâ€ move to visit the lower arm. At depth two, 
the two preferred actions are to visit the lower arm and (to a lesser extent) the centre location. This is 
because the tree search has not yet received any observation from th e generative process and , 
therefore, has no information about the tiger location â€”hence, it avoids states that include potential  
penalties. 
 
   
 
 31 
The centre graphic of the second panel shows the search tree generated during the second epoch after 
the agent has visited the lower arm and has observed an informative cue. At this point, the agent 
constructs a new search tree where the plan to reach the right arm is highly probable. The choice 
remains unchanged at the last epoch (see the right picture of the third panel) , and the agent collects 
the rewarding outcome. 
 
 
 
   
 
 32 
 
 
 
Figure 9.  Simulated behavioural and neuronal responses of Active Inference Tree Search in the 
â€œTigerâ€ problem, when the tiger is behind the left door. The first (top) panel shows the belief 
distribution over the eight hidden states (4 locations, each one in two possible contexts: â€œreward 
at the rightâ€ on top and â€œreward at the leftâ€ on bottom) as a function of time steps or epochs. The 

   
 
 33 
 
The third panel of Figure 9 illustrates simulated neurophysiological responses during the simulation 
shown in the first two panels. We assume that outcomes are sampled every 250 ms : a timescale 
compatible with hippocampal theta cycles, where place cells represent ing current and prospective 
locations can be decoded [81â€“83]. The figure illustrates a raster plot of simulated neuronal activity 
for units encoding hidden states.  The image is organised as a matrix, with 24 rows /neurons ( 4 
locations times 2  contexts times 3 epochs, corresponding to the  planning horizon ) and as many  
columns as the number of rolloutsâ€”as implemented by AcT during the three decision epochs (16 in 
this simulation). In other words, the presence of 24 rows/neurons  indicates that at each epoch, the 
agent represents its current  epoch and the two  closest epochs (e.g., during the first epoch, it also 
represents the subsequent two epochs; during the second epoch, it represents the previous and the 
subsequent epochs). Furthermore, separate neuronal populations encode the same hidden states at 
different epochs (e.g., the first hidden state at the first and second epochs correspond to the 1st and 
the 9th rows/neurons, respectively). In effect, this endows the agent with a form of working memory 
that is both predictive and postdictive.  
 
The rows and the columns of the third panel can be grouped to cluster the matrix in 3Ã—3 blocks of 
length 8 and 16, respectively. In this format, the elements  shown in the main diagonal of the block 
matrix are beliefs about the present and correspond to the hidden states shown in the first panel. The 
elements shown in the upper and lower diagonal blocks correspond to ( postdictive and predictive) 
beliefs about the past and the future, respectively. Note that the elements under the main diagonal 
correspond to the beliefs shown in the search trees of the second panel.  
 
The fourth panel of Figure 9 reports the simulated firing rates of two selected units, which correspond 
to the states representing the left (dashed line) and the right arm (solid line) during the third epoch. 
These are the states that will be visited (right arm) and not visited (left arm) during the third epoch. 
second panel illustrates the search trees generated by AcT in the three epochs of the simulation. 
The cyan dots represent the actual states the agent is in, at each time step. The third panel shows 
simulated neurophysiological responses associated with pla nning, following the encoding of 
probabilistic states by neuronal activity described in [11]. This assumes that, at each epoch, an 
agent represents probabilistic beliefs about the present, the past and the future, in distinct neuronal 
populations. These are displayed as firing rates of 24 (sets of) single cell units (neurons) that 
encode hidden states over the 3 epochs in a raster plot format. Specifically, (sets of) units 1 to 8 
are related to hidden states of the first epoch, (sets of) units 9 to 16 to hidden states of the second 
epoch, and (sets of) units 17 to 24 to hidden states of the third epoch. The first two (sets of) units 
for each epoch encode the central location, i.e., (sets of) units 1 and 2 encode the central location 
in the first epoch, (sets of) units 9 and 10 encode the central location in the second epoch, and so 
onâ€¦ Finally, odd and even units encode the â€œreward at the rightâ€ and â€œreward at the leftâ€ hidden 
context, respectively. For example, (sets of) units 1 and 2 encode the central location in the first 
epoch, when the context is â€œreward at the rightâ€ and â€œreward at the leftâ€, respectively. The fourth 
panel plots the firing rates of two neurons encoding the right arm (solid line) and left arm (dashed 
line), on the third epoch. These are the states that will be finally selected (right arm) and unselected 
(left arm). These correspond to neurons at lines 21 and 19 in the third panel, respectively. Please 
see the main text for an explanation.  
 
   
 
 34 
Initially (first column of the fourth panel), both units have the same firing rates because the agent is 
uncertain about the state it will visit next. However, this uncertainty is resolved during the second 
epoch and confirmed during the third (second and third columns, respectively). It is evident from this 
panel that expectations about future visitations (corresponding to the firing rates of the two units ) 
diverge during the epochs, following a stepwise evidence accumulation [11].  
 
These simulations exemplify the possibility of establishing a mapping between algorithmic methods 
of AcT and neuronal processes relevant to neuroscience. For example, the neurophysiological 
responses shown in the last two panels of Figure 9 exemplify prospective (and retrospective) 
representations of states that have been consistently reported in rodents [84â€“87], monkeys [88â€“90], 
and humans [91,92] engaged in a sequential decision or navigation tasks. From a cognitive science 
perspective, postdictive and predictive representations of this sort could be construed as working 
memory for past or future events or goals [93]. 
  
While computational modelling is widely used in neuroscience, there is still a paucity of methods that 
can both address large -scale problems relevant to AI and generate predictions relevant to 
neuroscience. Some recent studies using powerful deep learning [94â€“96] and Bayesian methods [97] 
are already bridging this gap, but they mostly address specific domains, such as visual perception and 
motor control. Addressing tasks such as RockSample or Tiger requires  designing complete agent 
architectures instead, as exemplified by AcT (this paper) and deep reinforcement learning models 
[98]. AcT and deep reinforcement learning models appeal to different principlesâ€”e.g., free energy 
minimisation versus reward maximisation; inference vers us trial-and-error learning ; appeal to the 
Bellman optimality principle versus variational principles of least action, and so onâ€”to design agent 
architectures that solve complex tasks , hence  speaking to different views of neuronal dynamics . 
Comparing the empirical validity of these assumptions side-by-side is an important objective for 
future research. 
 
5. Discussion 
 
Model-based planning is a widely interdisciplinary topic. However, synthesising ideas and methods 
from disciplines as diverse as AI, machine learning, and cognitive and computational neuroscience 
has been challenging , given their different focus (e.g., scalability and efficiency in AI, biological 
realism in neuroscience). 
 
Here, we offer a significant step in this direction by extending a prominent neurobiological theory of 
model-based control and planning â€”active inferenceâ€”to scale it up to POMD P problems of much 
larger size. This extension exploits tree search to elude the extensive evaluation of action policies 
often countenanced in active inference. The theoretical synthesis of active inference and tree search 
planning methodsâ€”called Active Inference Tree Search â€”benefits both. On the one h and, 
augmenting active inference with tree search methods permits the realising of a novel and appealing 
process model for approximate planning. This renders it scalable and potentially useful for explaining 
bounded forms of cognition and reasoning [56,99â€“101]. On the other hand, active inference provides 
a theoretically motivated and biologically grounded framework  to balance exploration and 
exploitation, which contextualises heuristic methods widely adopted in tree search planning, permits 
   
 
 35 
avoiding rollouts (as in Monte-Carlo methods) and obtains remarkable results in challenging POMDP 
problems. 
 
We validated Active Inference Tree Search in three simulative studies. The first study's results show 
that AcT successfully addresses deceptive binary trees that challenge most sampling-based planning 
methods, as it require s an accurate balance o f exploration and exploitation. In AcT, the balance of 
exploration and exploitation depends  implicitly on a single free energy functional used for policy 
evaluation. The results of the second study confirm the adaptivity of the exploration strategy used in 
AcT. They suggest that AcT can resolve challenging problems whose value functions are not smooth 
and are, therefore, challenging to explore systematically. The results of the third study show that AcT 
can successfully address POMDP p roblems (here, RockSample). The performance of AcT scales 
gracefully with problem size and is quite comparable with state-of-the-art POMDP algorithms, such 
as SARSOP [32], AEMS2 [31], POMCP, and DESPOT [43]; see also [41,102].  
 
Finally, we used Active Inference Tree Search to simulate neuronal responses during a representative 
planning task. This simulation illustrates the ability to map the algorithmic-level planning dynamics 
of AcT to neuronal-level representations putatively found in the hippocampus (and other areas, such 
as the prefrontal cortex) of animals that solve equivalent tasks [81â€“83]. Indeed, active inference  
originated in computational and systems neuroscience, intending to characterise brain processes from 
a normative perspective. AcT retains the neurobiological motivation of active inference while aiming 
to expand it to large-scale problems that previous implementations could not address. Using AcT to 
address large scale planning problems and explain neuronal activity can help establish a much-needed 
bridge between AI and computational neuroscience. 
 
In sum, our results show that AcT can deal with large state spaces, despite using off-the-shelf active 
inference methods that were not optimized for such problems. However, the performance of bespoke, 
state-of-the-art POMDP algorithms [41,102], such as DESPOT [43] remains superior. An important 
challenge â€” for future work â€“ is adopting AcT to address vision-based POMDP benchmarks, which 
deal with high dimensional image input. Addressing vision-based POMDP tasks might be feasible by 
adapting solutions from previous studies demonstrating active inference â€œfrom pixelsâ€ [103â€“105] and 
the possibility of using target images (or imagined images) as preferred outcomes for robotics 
[106,107]. Another objective for future research would be benchmarking AcT by comparing it wit h 
state-of-the-art POMDP schemes, especially those using deep learning. In this respect, a promising 
direction consists not just of comparing AcT with POMDP and deep learning methods but also 
hybridizing them, given that they might have complementary strengths. AcT and state -of-the-art 
POMDP algorithmsâ€”based on reinforcement learning â€” rest on different principles (variational 
principles of least action and Bellman optimality, respectively) and use different objectives; namely, 
expected free energy and expected reward. However, there are various aspects of AcT that might be 
potentially useful for planning in large scale domains. First, the minimisation of free energy naturally 
encompasses both exploration and exploitation under a single imperative, alleviating the need for 
explicit exploration terms â€”or the need to balance expected information gain against expected 
reward. The inherent exploration c apabilities of AcT are illustrated in the better performance, 
compared to UCT, when addressing deceptive binary tr ees, as in Figure 4 (noting that UCT could 
achieve the same results with a higher exploration constant). Intrinsic exploration capabilities could 
be especially beneficial in tasks that are ambiguous or lack explicit rewards [67,108]. Second, AcTâ€”
along with other methods to formulate reinforcement learning and control problems as probabilistic 
inference [109], such as planning -as-inference [15] and KL control [18]â€”uses a probabilistic 
   
 
 36 
formulation that could be leveraged in Tree Searches. For example, the expected free energy scores 
the probability of alternative policies and can therefore be used to automatically terminate searches 
at a particular node in the tree, in a way that eschews  sampling. Third, representing preferences (or 
rewards) as priors over observations provides flexibility in modeling reward functions; especially in 
complex nonstationary environments [108,110]. One aspect of casting rewards as prior preferences 
is that rewards can be specified as constraints over all outcomes, enabling a form of multiple 
constraint satisfaction. 
 
On the other hand, AcT  extensions could benefit from incorporating tools from deep learning, such 
as methods to approximate or amortise EFE computations to reduce computational demands. 
Furthermore, future AcT extensions could potentially address continuous spaces by adapting 
solutions developed for Monte Carlo Tree Search, such as the UCT extension called 'Prog ressive 
Widening' [111], the use of a kernel regression for estimating the utility of a random continuous 
action [112], or the value -gradient based variant proposed in [113]. These solutions require 
modifications to the tree search procedure, the nature of the distributions involved, and their 
evaluation. It remains to be seen if these methods can be deployed to improve AcT, and whether they 
could be integrated (or augmented)  with active inference to handle mixed (i.e., discrete and 
continuous state space) generative models [8,114,115]. These extensions of AcT might shed light on 
the conceptual and mathematical relations between different formulations of the same planning 
tasksâ€”as reinforcement learning, control or inference problems [13]â€”hence, creating synergies 
between distinct research fields. 
 
Declaration of conflict-of-interest 
 
The authors have no conflict-of-interest to declare. 
 
Acknowledgements 
 
KJF is supported by funding for the Wellcome  Centre for Human Neuroimaging (Ref: 
205103/Z/16/Z) and a Canada-UK Artificial Intelligence Initiative (Ref: ES/T01279X/1) . KJF and 
GP are supported by the European Unionâ€™s Horizon 2020 Framework Programme for Research and 
Innovation under the Specific Grant Agreement No. 945539 (Human Brain Project SGA3) . GP is 
supported by the European Research Council under the Grant Agreement No. 820213 (ThinkAhead), 
the European Unionâ€™s Horizon 2020 Framework Programme for Research and Innovation under the 
Specific Grant Agreement No. 952215 (TAILOR), the Italian National Recovery and Resilience Plan 
(NRRP), M4C2, funded by the European Union â€“ NextGenerationEU (Project IR0000011, CUP 
B51E22000150006, â€œEBRAINS -Italyâ€; Project PE0000013, â€œFAIRâ€; Project PE0000006, 
â€œMNESYSâ€), and the Ministry of University and Research, PRIN PNRR P20224FESY.  The 
GEFORCE Quadro RTX6000 and Titan GPU cards used for this research were donated by the 
NVIDIA Corporation. The funders had no role in study design, data collection and analysis, decision 
to publish, or preparation of the manuscript. 
  
   
 
 37 
References 
 
[1] G. Pezzulo, F. Donnarumma, D. Maisto, I. Stoianov, Planning at decision time and in the 
background during spatial navigation, Curr. Opin. Behav. Sci. 29 (2019) 69â€“76. 
https://doi.org/10.1016/j.cobeha.2019.04.009. 
[2] H. Geffner, Model-free, Model-based, and General Intelligence, ArXiv Prepr. 
ArXiv180602308 (2018). 
[3] N.D. Daw, P. Dayan, The algorithmic anatomy of model-based evaluation, Philos. Trans. R. 
Soc. B Biol. Sci. 369 (2014) 20130478. 
[4] S.J. Russell, P. Norvig, Artificial intelligence: a modern approach (International Edition), 
(2002). http://www.citeulike.org/group/1104/article/115157 (accessed September 13, 2017). 
[5] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge 
MA, 1998. 
[6] H. Geffner, Computational models of planning, Wiley Interdiscip. Rev. Cogn. Sci. 4 (2013) 
341â€“356. 
[7] D. Hassabis, D. Kumaran, C. Summerfield, M. Botvinick, Neuroscience-Inspired Artificial 
Intelligence, Neuron 95 (2017) 245â€“258. https://doi.org/10.1016/j.neuron.2017.06.011. 
[8] T. Parr, G. Pezzulo, K.J. Friston, Active Inference: The Free Energy Principle in Mind, 
Brain, and Behavior, MIT Press, 2022. 
[9] K.J. Friston, The free-energy principle: a unified brain theory?, Nat Rev Neurosci 11 (2010) 
127â€“138. https://doi.org/10.1038/nrn2787. 
[10] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, G. Pezzulo, Active inference 
and epistemic value, Cogn. Neurosci. 6 (2015) 187â€“214. 
https://doi.org/10.1080/17588928.2015.1020053. 
[11] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzulo, Active Inference: A 
Process Theory, Neural Comput. 29 (2017) 1â€“49. https://doi.org/10.1162/NECO_a_00912. 
[12] K. Friston, L. Da Costa, D. Hafner, C. Hesp, T. Parr, Sophisticated Inference, 
ArXiv200604120 Cs Q-Bio (2020). http://arxiv.org/abs/2006.04120 (accessed January 30, 2021). 
[13] E. Todorov, General duality between optimal control and estimation, in: 2008 47th IEEE 
Conf. Decis. Control, IEEE, 2008: pp. 4286â€“4292. 
[14] S. Levine, Reinforcement Learning and Control as Probabilistic Inference: Tutorial and 
Review, ArXiv180500909 Cs Stat (2018). http://arxiv.org/abs/1805.00909 (accessed July 18, 
2020). 
[15] K. Rawlik, M. Toussaint, S. Vijayakumar, On stochastic optimal control and reinforcement 
learning by approximate inference, in: Twenty-Third Int. Jt. Conf. Artif. Intell., 2013. 
[16] H. Attias, Planning by Probabilistic Inference, in: Proc. Ninth Int. Workshop Artif. Intell. 
Stat., 2003. 
[17] M. Botvinick, M. Toussaint, Planning as inference, Trends Cogn. Sci. 16 (2012) 485â€“488. 
https://doi.org/10.1016/j.tics.2012.08.006. 
[18] H.J. Kappen, V. GÃ³mez, M. Opper, Optimal control as a graphical model inference problem, 
Mach. Learn. 87 (2012) 159â€“182. 
[19] G. Pezzulo, F. Rigoli, K. Friston, Hierarchical Active Inference: A Theory of Motivated 
Control, Trends Cogn. Sci. 22 (2018) 294â€“306. https://doi.org/10.1016/j.tics.2018.01.009. 
[20] K.J. Friston, P. Schwartenbeck, T. FitzGerald, M. Moutoussis, T. Behrens, R.J. Dolan, The 
anatomy of choice: dopamine and decision-making, Philos. Trans. R. Soc. Lond. B Biol. Sci. 369 
(2014) 20130481. https://doi.org/10.1098/rstb.2013.0481. 
[21] K.J. Friston, T. Parr, B. de Vries, The graphical brain: Belief propagation and active 
inference, Netw. Neurosci. 1 (2017) 381â€“414. https://doi.org/10.1162/NETN_a_00018. 
[22] K.J. Friston, R. Rosch, T. Parr, C. Price, H. Bowman, Deep temporal models and active 
inference, Neurosci. Biobehav. Rev. 77 (2017) 388â€“402. 
https://doi.org/10.1016/j.neubiorev.2017.04.009. 
   
 
 38 
[23] K. UeltzhÃ¶ffer, Deep Active Inference, ArXiv170902341 Q-Bio (2017). 
http://arxiv.org/abs/1709.02341 (accessed September 8, 2017). 
[24] B. Millidge, Deep Active Inference as Variational Policy Gradients, ArXiv190703876 Cs 
(2019). http://arxiv.org/abs/1907.03876 (accessed February 10, 2020). 
[25] A. Clark, Surfing Uncertainty: Prediction, Action, and the Embodied Mind, Oxford 
University Press, Incorporated, 2015. 
[26] L. Kocsis, C. SzepesvÃ¡ri, Bandit Based Monte-Carlo Planning, in: J. FÃ¼rnkranz, T. Scheffer, 
M. Spiliopoulou (Eds.), Mach. Learn. ECML 2006, Springer, Berlin, Heidelberg, 2006: pp. 282â€“
293. https://doi.org/10.1007/11871842_29. 
[27] M. Kearns, Y. Mansour, A.Y. Ng, A sparse sampling algorithm for near-optimal planning in 
large Markov decision processes, Mach. Learn. 49 (2002) 193â€“208. 
[28] S. Gelly, D. Silver, Monte-Carlo tree search and rapid action value estimation in computer 
Go, Artif. Intell. 175 (2011) 1856â€“1875. https://doi.org/10.1016/j.artint.2011.03.007. 
[29] J. Pineau, G. Gordon, S. Thrun, Point-based value iteration: An anytime algorithm for 
POMDPs, in: Proc Int Jnt Conf Artif. Intell. IJCAI, 2003: pp. 477--484. 
[30] M.T.J. Spaan, N. Vlassis, Perseus: Randomized Point-based Value Iteration for POMDPs, J. 
Artif. Intell. Res. 24 (2005) 195â€“220. https://doi.org/10.1613/jair.1659. 
[31] G. Shani, J. Pineau, R. Kaplow, A survey of point-based POMDP solvers, Auton. Agents 
Multi-Agent Syst. 27 (2013) 1â€“51. https://doi.org/10.1007/s10458-012-9200-2. 
[32] O. Brock, J. Trinkle, F. Ramos, SARSOP: Efficient Point-Based POMDP Planning by 
Approximating Optimally Reachable Belief Spaces, in: Robot. Sci. Syst. IV, MITP, 2009: pp. 65â€“
72. https://ieeexplore.ieee.org/document/6284837. 
[33] S. Ross, J. Pineau, S. Paquet, B. Chaib-draa, Online Planning Algorithms for POMDPs, J. 
Artif. Intell. Res. 32 (2008) 663â€“704. https://doi.org/10.1613/jair.2567. 
[34] T. Smith, R. Simmons, Heuristic Search Value Iteration for POMDPs, in: Proc. 20th Conf. 
Uncertain. Artif. Intell., AUAI Press, Banff, Canada, 2004: pp. 520â€“527. 
[35] S. Ross, J. Pineau, B. Chaib-draa, Theoretical Analysis of Heuristic Search Methods for 
Online POMDPs, Adv. Neural Inf. Process. Syst. 20 (2008) 1216â€“1225. 
[36] S. Paquet, L. Tobin, B. Chaib-Draa, An online POMDP algorithm for complex multiagent 
environments, in: Proc. Fourth Int. Jt. Conf. Auton. Agents Multiagent Syst., 2005: pp. 970â€“977. 
[37] D.A. McAllester, S. Singh, Approximate Planning for Factored POMDPs using Belief State 
Simplification, in: Proc. 15th Annu. Conf. Uncertain. Artif. Intell. UAI-99, n.d.: pp. 409â€“416. 
[38] S. Yoon, A. Fern, R. Givan, S. Kambhampati, Probabilistic planning via determinization in 
hindsight, in: Proc. Natl. Conf. Artif. Intell., Chicago, IL; United States;, 2008: pp. 1010â€“1016. 
[39] C.B. Browne, E. Powley, D. Whitehouse, S.M. Lucas, P.I. Cowling, P. Rohlfshagen, S. 
Tavener, D. Perez, S. Samothrakis, S. Colton, A Survey of Monte Carlo Tree Search Methods, 
IEEE Trans. Comput. Intell. AI Games 4 (2012) 1â€“43. 
https://doi.org/10.1109/TCIAIG.2012.2186810. 
[40] P. Auer, N. Cesa-Bianchi, P. Fischer, Finite-time Analysis of the Multiarmed Bandit 
Problem, Mach. Learn. 47 (2002) 235â€“256. https://doi.org/10.1023/A:1013689704352. 
[41] D. Silver, J. Veness, Monte-Carlo Planning in Large POMDPs, in: J.D. Lafferty, C.K.I. 
Williams, J. Shawe-Taylor, R.S. Zemel, A. Culotta (Eds.), Adv. Neural Inf. Process. Syst. 23, 
Curran Associates, Inc., 2010: pp. 2164â€“2172. http://papers.nips.cc/paper/4031-monte-carlo-
planning-in-large-pomdps.pdf (accessed October 3, 2018). 
[42] P.-A. Coquelin, R. Munos, Bandit Algorithms for Tree Search, ArXiv14082028 Cs (2014). 
http://arxiv.org/abs/1408.2028 (accessed February 13, 2020). 
[43] N. Ye, A. Somani, D. Hsu, W.S. Lee, DESPOT: Online POMDP Planning with 
Regularization, J. Artif. Intell. Res. 58 (2017) 231â€“266. https://doi.org/10.1613/jair.5328. 
[44] P. Cai, Y. Luo, D. Hsu, W. Sun Lee, HyP-DESPOT: A Hybrid Parallel Algorithm for 
Online Planning under Uncertainty, in: Robot. Sci. Syst. XIV, Robotics: Science and Systems 
Foundation, 2018. https://doi.org/10.15607/RSS.2018.XIV.004. 
   
 
 39 
[45] T. Lee, Y.J. Kim, Massively parallel motion planning algorithms under uncertainty using 
POMDP, Int. J. Robot. Res. 35 (2016) 928â€“942. https://doi.org/10.1177/0278364915594856. 
[46] M. Igl, L. Zintgraf, T.A. Le, F. Wood, S. Whiteson, Deep Variational Reinforcement 
Learning for POMDPs, in: J. Dy, A. Krause (Eds.), Proc. 35th Int. Conf. Mach. Learn., PMLR, 
StockholmsmÃ¤ssan, Stockholm Sweden, 2018: pp. 2117â€“2126. 
http://proceedings.mlr.press/v80/igl18a.html. 
[47] P. Karkus, D. Hsu, W.S. Lee, QMDP-Net: Deep Learning for Planning under Partial 
Observability, in: I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. 
Garnett (Eds.), Adv. Neural Inf. Process. Syst. 30, Curran Associates, Inc., 2017: pp. 4694â€“4704. 
http://papers.nips.cc/paper/7055-qmdp-net-deep-learning-for-planning-under-partial-
observability.pdf. 
[48] E. Galceran, A.G. Cunningham, R.M. Eustice, E. Olson, Multipolicy decision-making for 
autonomous driving via changepoint-based behavior prediction: Theory and experiment, Auton. 
Robots 41 (2017) 1367â€“1382. https://doi.org/10.1007/s10514-017-9619-z. 
[49] W. Schwarting, J. Alonso-Mora, D. Rus, Planning and Decision-Making for Autonomous 
Vehicles, Annu. Rev. Control Robot. Auton. Syst. 1 (2018) 187â€“210. 
https://doi.org/10.1146/annurev-control-060117-105157. 
[50] C. Badue, R. Guidolini, R.V. Carneiro, P. Azevedo, V.B. Cardoso, A. Forechi, L. Jesus, R. 
Berriel, T. PaixÃ£o, F. Mutz, L. Veronese, T. Oliveira-Santos, A.F. De Souza, Self-Driving Cars: A 
Survey, ArXiv190104407 Cs (2019). http://arxiv.org/abs/1901.04407 (accessed July 25, 2020). 
[51] N. Wiener, Cybernetics: or Control and Communication in the Animal and the Machine, 
The MIT Press, Cambridge, MA, 1948. 
[52] G. Pezzulo, F. Rigoli, K.J. Friston, Active Inference, homeostatic regulation and adaptive 
behavioural control, Prog. Neurobiol. 136 (2015) 17â€“35. 
[53] A.K. Seth, The cybernetic Bayesian brain: from interoceptive inference to sensorimotor 
contingencies, MIND Proj. Eds T Metzinger J Windt (2014). 
[54] F. Donnarumma, D. Maisto, G. Pezzulo, Problem Solving as Probabilistic Inference with 
Subgoaling: Explaining Human Successes and Pitfalls in the Tower of Hanoi, PLOS Comput. Biol. 
12 (2016) e1004864. https://doi.org/10.1371/journal.pcbi.1004864. 
[55] G. Pezzulo, F. Donnarumma, P. Iodice, D. Maisto, I. Stoianov, Model-Based Approaches to 
Active Perception and Control, Entropy 19 (2017) 266. https://doi.org/10.3390/e19060266. 
[56] G. Pezzulo, F. Rigoli, F. Chersi, The Mixed Instrumental Controller: using Value of 
Information to combine habitual choice and mental simulation, Front. Cogn. 4 (2013) 92. 
https://doi.org/10.3389/fpsyg.2013.00092. 
[57] K. Friston, S. Samothrakis, R. Montague, Active inference and agency: optimal control 
without cost functions, Biol. Cybern. 106 (2012) 523â€“541. 
[58] M.J. Beal, Variational algorithms for approximate Bayesian inference, University of 
London, 2003. http://www.cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf (accessed August 5, 
2015). 
[59] Z. Fountas, N. Sajid, P.A.M. Mediano, K. Friston, Deep active inference agents using 
Monte-Carlo methods, ArXiv200604176 Cs Q-Bio Stat (2020). http://arxiv.org/abs/2006.04176 
(accessed September 3, 2020). 
[60] K. UeltzhÃ¶ffer, Deep Active Inference, Biol. Cybern. 112 (2018) 547â€“573. 
https://doi.org/10.1007/s00422-018-0785-7. 
[61] B. Millidge, Deep Active Inference as Variational Policy Gradients, ArXiv190703876 Cs 
(2019). http://arxiv.org/abs/1907.03876 (accessed September 2, 2020). 
[62] T. Champion, H. Bowman, M. GrzeÅ›, Branching time active inference: Empirical study and 
complexity class analysis, Neural Netw. (2022). 
[63] K. Friston, P. Schwartenbeck, T. FitzGerald, M. Moutoussis, T. Behrens, R.J. Dolan, The 
anatomy of choice: active inference and agency, Front. Hum. Neurosci. 7 (2013) 598. 
https://doi.org/10.3389/fnhum.2013.00598. 
   
 
 40 
[64] K.J. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, J. Oâ€™Doherty, G. Pezzulo, Active 
inference and learning, Neurosci. Biobehav. Rev. 68 (2016) 862â€“879. 
https://doi.org/10.1016/j.neubiorev.2016.06.022. 
[65] D. Maisto, K. Friston, G. Pezzulo, Caching mechanisms for habit formation in Active 
Inference, Neurocomputing 359 (2019) 298â€“314. https://doi.org/10.1016/j.neucom.2019.05.083. 
[66] T. Parr, K.J. Friston, Uncertainty, epistemics and active inference, J. R. Soc. Interface 14 
(2017) 20170376. https://doi.org/10.1098/rsif.2017.0376. 
[67] P. Schwartenbeck, J. Passecker, T.U. Hauser, T.H. FitzGerald, M. Kronbichler, K.J. Friston, 
Computational mechanisms of curiosity and goal-directed exploration, eLife 8 (2019) e41703. 
https://doi.org/10.7554/eLife.41703. 
[68] P. Schwartenbeck, T.H. FitzGerald, C. Mathys, R. Dolan, K. Friston, The dopaminergic 
midbrain encodes the expected certainty about desired outcomes, Cereb. Cortex (2014) bhu159. 
[69] A. Tschantz, M. Baltieri, A.K. Seth, C.L. Buckley, Scaling active inference, 
ArXiv191110601 Cs Eess Math Stat (2019). http://arxiv.org/abs/1911.10601 (accessed December 4, 
2019). 
[70] L. Kocsis, C. Szepesvari, J. Willemson, Improved Monte-Carlo Search, University of Tartu, 
Tartu, Estonia, 2006. 
[71] T. Parr, K.J. Friston, The Computational Anatomy of Visual Neglect, Cereb. Cortex 28 
(2018) 777â€“790. https://doi.org/10.1093/cercor/bhx316. 
[72] V. Kuleshov, D. Precup, Algorithms for multi-armed bandit problems, ArXiv14026028 Cs 
(2014). http://arxiv.org/abs/1402.6028 (accessed February 11, 2020). 
[73] Y.W. Teh, M.I. Jordan, M.J. Beal, D.M. Blei, Hierarchical dirichlet processes, J. Am. Stat. 
Assoc. 101 (2006) 1566â€“1581. 
[74] D.S. Nau, An investigation of the causes of pathology in games, Artif. Intell. 19 (1982) 257â€“
278. https://doi.org/10.1016/0004-3702(82)90002-9. 
[75] F. Gregoretti, G. Pezzulo, D. Maisto, cpp-AIF: A multi-core C++ implementation of Active 
Inference for Partially Observable Markov Decision Processes, Neurocomputing 568 (2024) 
127065. https://doi.org/10.1016/j.neucom.2023.127065. 
[76] R. Ramanujan, A. Sabharwal, B. Selman, On adversarial search spaces and sampling-based 
planning, in: Proc 20th Int Conf Autom Plan Sched, Toronto, ON, Canada, 2010: pp. 242â€“245. 
[77] J. Steven, G. Konidaris, B. Rosman, An analysis of monte carlo tree search, in: Thirty-First 
AAAI Conf. Artif. Intell., 2017. 
[78] J. Pazis, R. Parr, Non-parametric approximate linear programming for MDPs, in: Proc. 
Twenty-Fifth AAAI Conf. Artif. Intell., AAAI Press, San Francisco, California, 2011: pp. 459â€“464. 
[79] S.C.W. Ong, S.W. Png, D. Hsu, W.S. Lee, POMDPs for Robotic Tasks with Mixed 
Observability, in: Robot. Sci. Syst., 2009. 
[80] A.R. Cassandra, L.P. Kaelbling, M.L. Littman, Acting optimally in partially observable 
stochastic domains, in: Aaai, 1994: pp. 1023â€“1028. 
[81] D.J. Foster, M.A. Wilson, Hippocampal theta sequences, Hippocampus 17 (2007) 1093â€“
1099. https://doi.org/10.1002/hipo.20345. 
[82] G. Pezzulo, M.A.A. van der Meer, C.S. Lansink, C.M.A. Pennartz, Internally generated 
sequences in learning and executing goal-directed behavior, Trends Cogn. Sci. 18 (2014) 647â€“657. 
https://doi.org/10.1016/j.tics.2014.06.011. 
[83] G. Pezzulo, C. Kemere, M. van der Meer, Internally generated hippocampal sequences as a 
vantage point to probe future-oriented cognition, Ann. N. Y. Acad. Sci. 1396 (2017) 144â€“165. 
[84] R. Basu, R. Gebauer, T. Herfurth, S. Kolb, Z. Golipour, T. Tchumatchenko, H.T. Ito, The 
orbitofrontal cortex maps future navigational goals, Nature 599 (2021) 449â€“452. 
https://doi.org/10.1038/s41586-021-04042-9. 
[85] B.E. Pfeiffer, D.J. Foster, Hippocampal place-cell sequences depict future paths to 
remembered goals, Nature 497 (2013) 74â€“79. https://doi.org/10.1038/nature12112. 
[86] A.D. Redish, Vicarious trial and error, Nat. Rev. Neurosci. 17 (2016) 147â€“159. 
   
 
 41 
https://doi.org/10.1038/nrn.2015.30. 
[87] K.J. Miller, M.M. Botvinick, C.D. Brody, Dorsal hippocampus contributes to model-based 
planning, Nat. Neurosci. 20 (2017) 1269â€“1276. https://doi.org/10.1038/nn.4613. 
[88] Y. Xie, P. Hu, J. Li, J. Chen, W. Song, X.-J. Wang, T. Yang, S. Dehaene, S. Tang, B. Min, 
L. Wang, Geometry of sequence working memory in macaque prefrontal cortex, Science 375 
(2022) 632â€“639. https://doi.org/10.1126/science.abm0204. 
[89] H. Mushiake, N. Saito, K. Sakamoto, Y. Itoyama, J. Tanji, Activity in the lateral prefrontal 
cortex reflects multiple steps of future events in action plans., Neuron 50 (2006) 631â€“641. 
https://doi.org/10.1016/j.neuron.2006.03.045. 
[90] N. Saito, H. Mushiake, K. Sakamoto, Y. Itoyama, J. Tanji, Representation of immediate and 
final behavioral goals in the monkey prefrontal cortex during an instructed delay period., Cereb 
Cortex 15 (2005) 1535â€“1546. https://doi.org/10.1093/cercor/bhi032. 
[91] Z. Kurth-Nelson, M. Economides, R.J. Dolan, P. Dayan, Fast Sequences of Non-spatial 
State Representations in Humans, Neuron 91 (2016) 194â€“204. 
https://doi.org/10.1016/j.neuron.2016.05.028. 
[92] N.W. Schuck, Y. Niv, Sequential replay of nonspatial task states in the human hippocampus, 
Science 364 (2019). https://doi.org/10.1126/science.aaw5181. 
[93] B. Liu, Z.-S. Alexopoulou, F. van Ede, Jointly looking to the past and the future in visual 
working memory, eLife 12 (2024). https://doi.org/10.7554/eLife.90874.2. 
[94] D.L.K. Yamins, J.J. DiCarlo, Using goal-driven deep learning models to understand sensory 
cortex, Nat. Neurosci. 19 (2016) 356â€“365. https://doi.org/10.1038/nn.4244. 
[95] D. Sussillo, M.M. Churchland, M.T. Kaufman, K.V. Shenoy, A neural network that finds a 
naturalistic solution for the production of muscle activity, Nat. Neurosci. 18 (2015) 1025â€“1033. 
https://doi.org/10.1038/nn.4042. 
[96] K.R. Storrs, N. Kriegeskorte, Deep learning for cognitive neuroscience, ArXiv Prepr. 
ArXiv190301458 (2019). 
[97] D. George, W. Lehrach, K. Kansky, M. LÃ¡zaro-Gredilla, C. Laan, B. Marthi, X. Lou, Z. 
Meng, Y. Liu, H. Wang, A. Lavin, D.S. Phoenix, A generative vision model that trains with high 
data efficiency and breaks text-based CAPTCHAs, Science 358 (2017). 
https://doi.org/10.1126/science.aag2612. 
[98] M. Botvinick, J.X. Wang, W. Dabney, K.J. Miller, Z. Kurth-Nelson, Deep Reinforcement 
Learning and Its Neuroscientific Implications, Neuron 107 (2020) 603â€“616. 
https://doi.org/10.1016/j.neuron.2020.06.014. 
[99] Q.J.M. Huys, N. Lally, P. Faulkner, N. Eshel, E. Seifritz, S.J. Gershman, P. Dayan, J.P. 
Roiser, Interplay of approximate planning strategies, Proc. Natl. Acad. Sci. U. S. A. 112 (2015) 
3098â€“3103. https://doi.org/10.1073/pnas.1414219112. 
[100] M. Keramati, P. Smittenaar, R.J. Dolan, P. Dayan, Adaptive integration of habits into depth-
limited planning defines a habitual-goalâ€“directed spectrum, Proc. Natl. Acad. Sci. U. S. A. 113 
(2016) 12868â€“12873. https://doi.org/10.1073/pnas.1609094113. 
[101] P.A. Ortega, D.A. Braun, Thermodynamics as a theory of decision-making with 
information-processing costs, Proc. R. Soc. Math. Phys. Eng. Sci. 469 (2013). 
https://doi.org/10.1098/rspa.2012.0683. 
[102] S.C.W. Ong, Shao Wei Png, D. Hsu, Wee Sun Lee, Planning under Uncertainty for Robotic 
Tasks with Mixed Observability, Int. J. Robot. Res. 29 (2010) 1053â€“1068. 
https://doi.org/10.1177/0278364910369861. 
[103] C. Sancaktar, M. van Gerven, P. Lanillos, End-to-End Pixel-Based Deep Active Inference 
for Body Perception and Action, 2020 Jt. IEEE 10th Int. Conf. Dev. Learn. Epigenetic Robot. 
ICDL-EpiRob (2020) 1â€“8. https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105. 
[104] N. van Hoeffelen, P. Lanillos, Deep active inference for pixel-based discrete control: 
Evaluation on the car racing problem, in: Jt. Eur. Conf. Mach. Learn. Knowl. Discov. Databases, 
Springer, 2021: pp. 843â€“856. 
   
 
 42 
[105] T. Taniguchi, S. Murata, M. Suzuki, D. Ognibene, P. Lanillos, E. Ugur, L. Jamone, T. 
Nakamura, A. Ciria, B. Lara, G. Pezzulo, World Models and Predictive Coding for Cognitive and 
Developmental Robotics: Frontiers and Challenges, Adv. Robot. 37 (2023). 
https://doi.org/10.48550/arXiv.2301.05832. 
[106] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, S. Levine, ViNT: A 
foundation model for visual navigation, ArXiv Prepr. ArXiv230614846 (2023). 
[107] A.V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, S. Levine, Visual reinforcement learning with 
imagined goals, Adv. Neural Inf. Process. Syst. 31 (2018). 
[108] A. Tschantz, B. Millidge, A.K. Seth, C.L. Buckley, Reinforcement learning through active 
inference, ArXiv Prepr. ArXiv200212636 (2020). 
[109] S. Levine, Reinforcement learning and control as probabilistic inference: Tutorial and 
review, ArXiv Prepr. ArXiv180500909 (2018). 
[110] K. Friston, What is optimal about motor control?, Neuron 72 (2011) 488â€“498. 
[111] T.M. Moerland, J. Broekens, A. Plaat, C.M. Jonker, A0c: Alpha zero in continuous action 
space, ArXiv Prepr. ArXiv180509613 (2018). 
[112] T. Yee, V. Lisá»³, M.H. Bowling, S. Kambhampati, Monte Carlo Tree Search in Continuous 
Action Spaces with Execution Uncertainty., in: IJCAI, 2016: pp. 690â€“697. 
[113] J. Lee, W. Jeon, G.-H. Kim, K.-E. Kim, Monte-carlo tree search in continuous action spaces 
with value gradients, in: Proc. AAAI Conf. Artif. Intell., 2020: pp. 4561â€“4568. 
[114] K.J. Friston, T. Parr, B. de Vries, The graphical brain: Belief propagation and active 
inference, Netw. Neurosci. 1 (2017) 381â€“414. https://doi.org/10.1162/NETN_a_00018. 
[115] M. Priorelli, F. Maggiore, A. Maselli, F. Donnarumma, D. Maisto, F. Mannella, I.P. 
Stoianov, G. Pezzulo, Modeling motor control in continuous-time Active Inference: a survey, IEEE 
Trans. Cogn. Dev. Syst. (2023). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
   
 
 43 
Appendix  
 
 
 
A.1 An example generative model: the case of RockSample(2,1).  
 
Here, we provide an example of how a POMDP problem can be represented in Active Inference. We 
focus on the RockSample(ğ‘›,ğ‘˜) problem with ğ‘›=2 and ğ‘˜=1. In this scenario, the rover explores 
an alien soil grid of side 2, with a unique rock to analy se, see Figure A.1.  The rover starts its 
exploration from the top -left corner and has to  reach the exit on the right side after (optionally) 
collecting a sample of the rock. The position of the rock in the maze and its quality ("good" or "bad") 
are selected randomly. In our example, the rock is placed in the bottom-right corner of the maze, and 
its quality is "good". Figure A.1 shows the initial configuration of the problem:  the locations of the 
rover are denoted with "Rğ‘–â€ (with R1 being the initial location), the location of the rock is denoted 
with â€œâ‚¬â€, the exit state is denoted with  â€œEXITâ€, and the grid border (that the rover cannot cross ) is 
denoted with the symbol â€œ*â€. 
 
 
 
  *   *  *  *  *  *  *  *  *  *  *   
 
 
 
E 
X 
I 
T 
  * 
  * 
  *   
  *        
  * 
  * 
  * 
  * 
 
R1 
 
R2 
 
R0 
 
â‚¬ 
 
*   *   *   *   *   *   *   * 
 
Figure A.1. The configuration of the RockSample(2,1) problem that we consider in our example. See 
the main text for illustration of the symbols. 
 
The generative model used by the Active Inference agent is defined by setting  the hidden states ğ‘†, 
the observations ğ‘‚, the control states ğ‘ˆ, and the parameters Î˜={ğ€,ğ,ğ‚,ğƒ}.  
 
As discussed in Section 4.3 of the main text, we decided not to factorize the hidden states ğ‘†. Hence, 
with ğ‘›=2 and ğ‘˜=1, the cardinality of ğ‘† is (ğ‘›2+1)â‹…2ğ‘˜ +1=11. See Fig. A.2. Note that each 
location corresponds to two states  (e.g., the top-left location corresponds to states 4 and 5) : one in 
which the only rock of the problem is bad (state 4) and the other in which the rock is good (state 5).  
The states 8, 9, and 10 are absorbing states; the former two states correspond to the EXIT location 
when the rock is bad (state 8) and when it is good (state 9) , and the latter  state corresponds to the 
border. The agent's initial belief is the vector ğƒ=[0,0,0.5,0.5,0,0,0,0,0,0,0]T, which implies that it 
knows it starts from the top-left location, but it does not know whether the rock is bad or good (hence 
it considers equally probably starting from state 4 or 5). 
 
    
   
 
 44 
 
  
 
 
 
 
 
{8,9} 
   
 
 
 
10 
 
{4,5} 
 
{6,7} 
 
{0,1} 
 
{2,3} 
 
 
 
Fig. A.2 Mapping between the 11 hidden states and the 6 grid locations of the RockSample(2,1) 
problem. See the main text for explanation. 
 
 
Observations ğ‘‚ are organized in three factors, that is: 
 
 ğ‘‚â‰¡[oR0,ğ‘œâ‚¬,ğ‘œR1,ğ‘œR2,ğ‘œEXIT,ğ‘œâˆ—]
ğ‘‡
â¨‚[bad,good]ğ‘‡â¨‚[reward,penalty]ğ‘‡.   
 
The first factor describes the 6 locations of the maze  and therefore has size 6 . The second factor 
describes all the possible 2ğ‘˜ possible combinations of rock qualities, good or bad (for ğ‘˜=1, this 
factor has size 2).  The third factor describes the utility (reward or penalty) of the current state . By 
considering that the main goal of the rover is obtaining rewards, the prior over (preferred) 
observations is:  
 
ğ‚=[1/6,1/6,1/6,1/6,1/6,1/6]ğ‘‡â¨‚[0.5,0.5]ğ‘‡â¨‚[c,âˆ’c]ğ‘‡, with ğ‘>0.  
 
The control states ğ‘ˆ encode the actions of the agent. These are: go north (gn), go south (gs), go west 
(gw), go east (ge), check the rock from remote (cr) with variable accuracy, and collect a sample of 
the rock (sr). Note that for ğ‘˜>1, there are ğ‘˜ additional remote sensing actions, one for each rock. 
Note also that the action cr changes the quality of the sampled rock from "good" to "bad" , or leaves 
it unaltered if the rock was already bad.  
 
The control states ğ‘ˆ determine the transitions from one state to another , as specified by  the set of 
matrices ğu shown below, one for each control state ğ‘¢.  
 
 
 
 
 
   
 
 45 
Bgn =
(
 
 
 
 
 
 
 
 
0
â‹®
0 â‹®
1 0 â‹®
0 1 0 â‹®
â‹® 0 1 0 â‹®
â‹® 0 1 0 â‹®
â‹® 0 â‹® â‹® â‹® â‹® 1 0 â‹®
0 0 0 0 0 0 1 0
0 1 1 1 1 0 0 1)
 
 
 
 
 
 
 
 
 
 
 
Bgs =
(
 
 
 
 
 
 
 
 
0 â‹¯ 0 1 0 â‹¯
â‹® 0 1 0 â‹¯
â‹® 0 1 0
â‹® 0 1
â‹® 0
â‹®
â‹®
0 â‹®
â‹® â‹® â‹® â‹® 1 0 â‹®
0 0 0 0 â‹¯ 0 1 0
1 1 1 1 0 â‹¯ â‹¯ 0 1)
 
 
 
 
 
 
 
 
 
 
 
Bgw =
(
 
 
 
 
 
 
 
 
0 0 1 0 â‹¯
â‹® â‹® 0 1
â‹® 0 â‹®
â‹® 0 â‹®
1 0
0 1
â‹® 0 â‹®
â‹® 0 â‹®
â‹® â‹® â‹® â‹® 1 0 â‹®
0 0 0 0 â‹® â‹® 0 1 0
1 1 1 1 0 0 0 0 1)
 
 
 
 
 
 
 
 
 
 
 
Bge =
(
 
 
 
 
 
 
 
 
0
0 â‹®
1 0
0 1
â‹® 0 â‹®
â‹® 0 â‹®
â‹® 1 0 â‹® â‹®
0 â‹® 0 1 0 â‹® 0 â‹®
1 0 â‹® 0 1 0 1 0 â‹®
0 1 â‹® 0 1 0 1 0
0 0 0 0 0 0 1)
 
 
 
 
 
 
 
 
 
 
   
 
 46 
Bcr = 
(
 
 
 
 
 
 
 
 
1 0 â‹¯
0 â‹±
â‹®
â‹±
1
â‹±
â‹®
â‹± 0
â‹¯ 0 1)
 
 
 
 
 
 
 
 
 
 
 
Bsr = 
(
 
 
 
 
 
 
 
 
1 0 0 0
0 1 0 0
â‹® 0 1 1 â‹®
â‹® 0 0 0 â‹®
â‹® â‹® 1 0 â‹®
0 1 0 â‹®
â‹® 0 1 0 â‹®
â‹® 0 1 0 â‹®
â‹® 0 1 0 â‹®
â‹® 0 1 0
0 0 1)
 
 
 
 
 
 
 
 
 
 
 
Furthermore, the control states ğ‘ˆ determine the likelihood mapping from hidden states to 
observations, as specified in the matrix ğ€. Note that the matrix ğ€ has three components, one for each 
factor of the observations. The first component Au
1, which describes the 6 locations of the maze, is 
the same for each ğ‘¢: 
 
 
Ağ‘¢
1 = 
(
  
 
1 1 0 0 0 0
0 0 1 1 0 0 â‹® â‹®
â‹® â‹® 0 0 1 1 0 0 â‹® â‹®
â‹® â‹® 0 0 1 1 0 0 â‹®
â‹® â‹® 0 0 1 1 0
0 0 0 0 1)
  
 
,âˆ€ ğ‘¢âˆˆğ‘ˆ  
 
 
The second component Ağ‘¢
2, which describes the observed rock configuration, is different depending 
on the control state ğ‘¢ set. For ğ‘¢ expressing movements, namely ğ‘¢âˆˆ{gn,gs,gw,ge}, Ağ‘¢
2 is: 
 
 
A{gn,gs,gw,ge}
2 = (1 0 1 0 1 0 1 0 1 0 0.5
0 1 0 1 0 1 0 1 0 1 0.5). 
 
 
For ğ‘¢=cr, the Acr
2  elements are computed by considering that the probability of observing the rock 
quality depends only on the rover position R and is independent of the other rock qualities. The 
probability that the sensor is accurate on the rock is ğ‘ƒğ‘ğ‘|Rğ‘–,cr â‰¡ğ‘ƒ(accurateâ”‚Rğ‘– ,cr)=  (1+
   
 
 47 
ğœ‚(Rğ‘–,â‚¬))â„2; where ğœ‚(Rğ‘–,â‚¬)= 2âˆ’ğ‘‘(Rğ‘–,â‚¬) ğ‘‘0â„ , with ğ‘‘(Rğ‘–,â‚¬) denoting the Euclidean distance between 
the positions Rğ‘– and  â‚¬, and ğ‘‘0 is a constant specifying the half accuracy distance. In our case,  there 
is only one rock, and its quality is â€œgoodâ€. Therefore, Acr
2  is: 
 
 
Acr
2 =(ğ‘ƒÌ…ğ‘ğ‘|R0,cr ğ‘ƒÌ…ğ‘ğ‘|R0,cr 0 0 ğ‘ƒÌ…ğ‘ğ‘|R1,cr ğ‘ƒÌ…ğ‘ğ‘|R1,cr ğ‘ƒÌ…ğ‘ğ‘|R2,cr ğ‘ƒÌ…ğ‘ğ‘|R2,cr 1 0 0.5
ğ‘ƒğ‘ğ‘|R0,cr ğ‘ƒğ‘ğ‘|R0,cr 1 1 ğ‘ƒğ‘ğ‘|R1,cr ğ‘ƒğ‘ğ‘|R1,cr ğ‘ƒğ‘ğ‘|R2,cr ğ‘ƒğ‘ğ‘|R2,cr 0 1 0.5) 
 
 
where ğ‘ƒÌ…ğ‘ğ‘|Rğ‘–,cr =1âˆ’ğ‘ƒğ‘ğ‘|Rğ‘–,cr. 
 
The third component Ağ‘¢
3, which describes the utility value associated with the observed outcome, is 
different depending on the control state ğ‘¢ set. The Asr
3  matrix for the control state ğ‘¢=sr is the 
following: 
 
Asr
3 = (0 0 0 1 0 0 0 0 0.5 0.5 0.5
1 1 1 0 1 1 1 1 0.5 0.5 0.5) 
 
 
The Asr
3  matrix for all the other control states ğ‘¢âˆˆ{gn,gs,gw,ge,cr} is the following: 
 
 
A{gn,gs,gw,ge,cr}
3 = (0.5 â‹¯ â‹¯ 0.5 1 0 0
0.5 â‹¯ â‹¯ 0.5 0 1 1) 
 
 
where the first and second rows encode the probability of observing a reward and a penalty, 
respectively.  
 
The overall Ağ‘¢
3 matrix reflects the fact that the only two ways to observe a reward are collecting a 
sample of the good rock (i.e., being in state 3) and going to the EXIT after having collected a sample 
of the good rock (i.e., being in state 8, which implies that the rock is bad - which is possible after a 
sample of the good rock has been collected and the good rock has been changed in a bad rock). Rather, 
the agent gets a penalty if it reaches the EXIT without having collected a sample of the good rock, or 
if it reaches the border at any time. 
 
 
 
 