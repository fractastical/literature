# Actively Inferring Optimal Measurement Sequences

**Authors:** Catherine F. Higham, Paul Henderson, Roderick Murray-Smith

**Year:** 2025

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [higham2025actively.pdf](../pdfs/higham2025actively.pdf)

**Generated:** 2025-12-14 00:21:17

**Validation Status:** ✓ Accepted
**Quality Score:** 1.00

---

### OverviewThis paper investigates actively inferring optimal measurement sequences for data acquisition, aiming to reduce measurement time and cost while maintaining data quality. The authors develop an active sequential inference algorithm that uses a low-dimensional representational latent space from a variational autoencoder (VAE) to choose which measurement to make next. The algorithm is illustrated using the Fashion MNIST dataset and a novel convolutional Hadamard pattern measurement basis.### MethodologyThe authors state: "In many circumstances, data collection incurs a cost. This cost may be in terms of acquisition time, invasion of or damage to the environment and storage." Identifying which, out of a range of data measurements, to collect next is potentially a valuable cost-saving activity. Importantly, it also facilitates fast decision making(Horvitz&Barry,1995). Dataminimisationisanimportantconsiderationforcompliancewithdata protectionlawsworldwide1. Indefensesituations,therequirementforcoverthumanintelligencemeansthat the act of taking measurements can also pose a security risk. The authors state: "We develop an active sequential inference algorithm that uses a low dimensional data representation to infer its high dimensional state conditional on partial measurement of that state." Reducing dimension allows for more efficient exploration of the space of state possibilities. The state and types of task we are primarily interested in are image/scene reconstruction and related classification tasks using photon based imaging and sensing technology such as a singlepixelcamera(Highametal.,2018). However, the method is applicable to a broader range of activities. The overall aim is to customise a generative probabilistic model to provide the agent or user with different reconstructionscenariosconditionalonpartialmeasurements. Givenasuitablylargedatasetoftaskrelevant data, an appropriate generative model is the variational autoencoder (VAE) (Kingma & Welling,2014). A VAElearnstoencodedataintheconvenientformofalowdimensionalmultivariateGaussianandtodecode this representation back to data space. This provides a means to obtain different reconstructions through manipulation of a low dimensional latent space. This neural network model is trained on relevant data to learn the underlying distribution of the training data and used to generate new data. A VAE comprises an encoder (a map from data space to a low dimensional latent representative space) and a decoder (a map from the low dimensional space back to data space). To achieve this the VAE introduces latent variables and the objective of a VAE is to understand the true posterior distribution of the latent variables. A VAE accomplishes this by employing an encoder network to approximate the genuine posterior distribution with a learned approximation. Once trained, samples from the prior on the latent space can be pushed through the decoder to obtain new generated data. Similarly, samples from the posterior can be pushed through the decoder to obtain data conditional on the input data. Hereweadaptetheencoder to map partially measured data to the latent space. This necessitates creating a training set of partially measured data. Once trained, the partial encoder and the original decoder are used by the algorithm to sequentially reason about the full state and choose the next measurement. The probabilistic model we are inferring over is capturing both the (approximate) data-generating process and the noisy measurement model.### ResultsThe authors state: "We see that useful patterns are chosen within10 steps, leading to the convergence of the guiding generative images." The algorithm is illustrated using the Fashion MNIST dataset and a novel convolutional Hadamard pattern measurement basis. The authors state: "Compared with using stochastic variational inference to infer the parameters of the posterior distribution foreachgenerateddatapointindividually, the partial VAE framework can efficiently process batches of generated data and obtains superior results with minimal measurements." The authors state: "At step k =0, for each of N starting points, sample z from the prior for step i k =0, push z through the decoder to obtain a generated image xˆ and estimate possible measurements i iyˆ for each pattern not yet measured (e.g. yˆ , yˆ and yˆ illustrated above). Use the partial encoder to approximate the posterior with probability distributions and estimate probability densities q1(z |yˆ ),11q2(z |yˆ ) and q3(z |yˆ ) (illustrated above). Select the pattern which maximises the chosen expression and take the measurement associated with this pattern. Update the measurement set and update the predictive prior for the next step. By repeating these steps, we move towards the target distribution p(z|x)." The authors state: "The main idea of variational methods is to cast inference as an optimization problem (Blei et al.,2017). Stochastic variational inference (SVI) (Hoffman et al.,2013) can be used to infer the posterior probability distribution for specific data and a given set of measurements but requires multiple computation steps and thus is prohibitively slow when required to estimate many possible measurement sets." The authors propose a hybrid approach, in a similar spirit to Kim et al. (2018) but novel in our context, to combine the strengths of VAE and SVI. We use the partial encoder to choose between patterns and integrate robust SVI when a pattern has been selected for inferring the posterior distribution parameters based on actual measurements. The algorithm is developed to actively select the next best measurement. It starts by pushing samples from the prior distribution on the latent space, p = N(0,1), to the generative images, xˆ, to obtain N generated images. Simulated measurements, y[Bk], are made on the chosen measurement basis indexes not yet taken. These simulated measurements are pushed through the encoder, E, to approximate the posterior distribution, qj, for each measurement index j. The algorithm evaluates the expression and selects the measurement index which maximises the expression. The measurement index, j, is updated and the predictive prior for the next step is updated. The algorithm repeats these steps to converge to the target distribution. The authors state: "We see that useful patterns are chosen within10 steps, leading to the convergence of the guiding generative images." The authors state: "

The main idea of variational methods is to cast inference as an optimization problem (

Blei et al.,2017)
