=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Neurons as hierarchies of quantum reference frames
Citation Key: fields2022neurons
Authors: Chris Fields, James F. Glazebrook, Michael Levin

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Conceptual and mathematical models of neurons have lagged behind empirical understand-
ing for decades. Here we extend previous work in modeling biological systems with fully
scale-independent quantum information-theoretic tools to develop a uniform, scalable rep-
resentation of synapses, dendritic and axonal processes, neurons, and local networks of
neurons. In this representation, hierarchies of quantum reference frames act as hierarchical
active-inference systems. The resulting model enables ...

Key Terms: hierarchies, university, quantum, neurons, eres, france, remodeling, frames, reference, activity

=== FULL PAPER TEXT ===

Neurons as hierarchies of quantum reference
frames
Chris Fieldsa∗ , James F. Glazebrookb,c and Michael Levind
a 23 Rue des Lavandi`eres, 11160 Caunes Minervois, FRANCE
ORCID: 0000-0002-4812-0744
b Department of Mathematics and Computer Science,
Eastern Illinois University, Charleston, IL 61920 USA
c Adjunct Faculty, Department of Mathematics,
University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA
d Allen Discovery Center at Tufts University, Medford, MA 02155 USA
ORCID: 0000-0001-7292-8084
January 5, 2022
Abstract
Conceptual and mathematical models of neurons have lagged behind empirical understand-
ing for decades. Here we extend previous work in modeling biological systems with fully
scale-independent quantum information-theoretic tools to develop a uniform, scalable rep-
resentation of synapses, dendritic and axonal processes, neurons, and local networks of
neurons. In this representation, hierarchies of quantum reference frames act as hierarchical
active-inference systems. The resulting model enables specific predictions of correlations
between synaptic activity, dendritic remodeling, and trophic reward. We summarize how
the model may be generalized to nonneural cells and tissues in developmental and regener-
ative contexts.
Keywords
Activity-dependent remodeling; Bayesian inference; Bioelectricity; Computation; Learning;
Memory
∗Correspondingauthorat: 23RuedesLavandi`eres,11160CaunesMinervois,FRANCE;E-mailaddress:
fieldsres@gmail.com
1
2202
naJ
4
]CN.oib-q[
1v12900.1022:viXra
1 Introduction
Neurons are canonical biological information processors. Theoretical and, particularly,
conceptualmodelsofneuralinformationprocessing,however,lagincreasinglyfarbehindour
developing empirical understanding of neurons as electrically-excitable cells. Experimental
work over the past two decades has, for example, firmly established that dendrites undergo
activity-dependent remodeling [1, 2, 3], particularly alterations of spine location, density,
and function [4], even in adult humans. This ontogenic process is functionally analogous
to the evolution of structural and positional diversity of dendrites as they have adapted to
a spectrum of functional roles [5], e.g. the implementation of deep learning via synaptic
plasticity [6, 7]. Neurons are not, therefore, static structures, but rather can be regarded
as undergoing continuous development throughout the life cycle. This dynamic process has
significant consequences for both neuron-level and organism-level function. For example, in
organisms (such as caterpillars transitioning into butterflies or moths) that exhibit drastic
remodeling and reconstruction of their brains, some of their learned memories remain and
survive the process [8]. In other contexts, memories can be imprinted on newly regenerating
brains from other tissues [9, 10], underscoring the plasticity of large-scale neural structure
and of the information stored within it. These effects of remodeling are not just an issue for
so-called lower animals, as applications in regenerative medicine are likely to soon produce
human patients in whom some portion of the brain has been replaced by the progeny of
na¨ıve stem cells to treat degenerative disease or brain damage.
It has also been shown that subnetworks of dendrites can compute local logical operations
includingexclusive-or(XOR)[11]. Nonetheless,theideathatdendritictreeseffectivelycom-
pute weighted sums, dating back to [12], continues not only to dominate artificial neural
network (ANN) development, but also to guide biological thinking. Explicit cable-theory
modeling can reproduce time-dependent signal propagation and processing in simplified
representations of dendritic trees [13], but rapidly becomes intractable with increasing geo-
metriccomplexity. Increasinglysophisticatedhardwaremodelsofneuronsallowexploration
of phase coding, frequency modulation, and other forms of hybrid analog-digital computing
[14, 15], but are not readily mapped to biological neural networks and are not standard
components of a theoretical neuroscientist’s toolkit.
Conceptualizing neurons as biological wires – even functionally sophisticated wires – pro-
vides, moreover, no insight into the fundamental question of cognitive neuroscience: the
question of how information present in the external world is encoded, via some combination
of evolution, development, remodeling, and learning, into the functional architecture of a
nervous system. Brains are not constructed, as ANNs are, in the absense of functional
activity, nor do they “begin learning” from a default initial state of uniform connectivity.
Brains are not “wired up” into some starting configuration, after which they are turned on
and exposed to the world. Brains instead develop from neural primordia that are already
functionally complex multicellular microenvironments. The function of the nervous system,
from its earliest phylogenetic and ontogenetic role in directing morphogenesis [16] to adult
cognition, depends on the ability of individual neurons to negotiate this microenvironment,
2
both structurally and functionally. The local microenvironment, with its diverse cell types,
biochemicaland(micro)anatomicalcomplexity, andnetworkofbiomolecularandbioelectric
signaling, is the “world” with which each individual neuron interacts, and is each neuron’s
sole source of nongenomic information. There are many interesting scenarios supporting
this viewpoint; some are of the ‘socialization’ and decision making type. For instance,
assortative neural networks demonstrate collective resilience, whereby nodes of a certain
degree have an affinity to team up with those of the same topological type, and thus often
contribute to formation of a small world network structure [17, 18]. Neural network model-
ing of independent tasks in the prefrontal cortex in [19], for example, reveals how seemingly
separate neuronal groups engaged in their respective tasks may sometimes be drawn into a
coalition, casting votes for a stimulus, and then enacting a committee-like decision.
In line with the Free Energy Principle (FEP) and the idea of Bayesian active inference
[20, 21, 22, 23, 24], individual neurons can be considered cognitive agents that minimize
aggregate uncertainty about their future states (variational free energy or VFE) by actively
exploring and developing predictive models of their local microenvironments. It is the joint
activityofhundreds(inC. elegans[25])totensofbillions(inprimates[26])ofneuronswithin
this jointly-constructed environment that encodes information sourced from the external
world. Hence an adequate conceptual model of neurons as biological systems must explain
how each neuron’s construction of a predictive model of its local microenvironment results
in the joint encoding of a predictive model of the external environment at the scale of the
entire brain. The utility of FEP based models of cell sorting in morphogenesis [27, 28],
cortical minicolumn and local-circuit organization [29, 30, 31, 32, 33], and network and
whole-brain function [21, 34, 35, 36, 37] suggests that a scale-free conceptual framework is
the right way to approach this question of functional coherence between microenvironment
models constructed by individual neurons and external environment models constructed by
brains. The FEP is, however, fundamentally a statement about error minimization; it is an
implementation-independent principle of thermodynamics [24]. It does not tell us anything
specific about neuronal functional architecture, or about how neurons interact with either
each other or the non-neuronal components of their microenvironments.
Here we suggest that the idea of a quantum reference frame (QRF) developed within quan-
tum information theory [38, 39] is usefully applied to characterize neurons both architec-
turally and functionally. While quantum information theory employs the formalism of
standard quantum theory, it makes no scale-dependent assumptions and is applicable from
sub-microscopic to cosmological scales. The formalism of QRFs, in particular, is applicable
at any scale. A QRF is a physical system that assigns units of measurement, and hence
an operational semantics, to each observational outcome, enabling outcomes obtained at
different times or places, or with different QRFs, to be compared. Macroscopic systems
such as meter sticks, clocks, gyroscopes, and the Earth with its gravitational and magnetic
fields are canonical examples of QRFs. While reference frames are typically thought of as
fully-specifiable abstractions in classical physics, this is not the case in quantum theory:
QRFs are physical systems that encode unmeasurable but functionally relevant quantum
phase information. Every QRF is, therefore, in an important sense unique. A QRF cannot,
3
in particular, be fully specified either structurally or functionally by any finite bit string;
it is “nonfungible” in the terminology of [39]. Alice can, in this case, share a QRF with a
distant observer Bob only by physically transferring the QRF to Bob; sending any finite de-
scription is provably insufficient. Alice can, moreover, transfer her QRF to Bob successfully
only if Bob already possesses a functionally equivalent QRF [40]; Alice cannot, for example,
successfully communicate the meaning of “1 meter” to Bob unless Bob already has a QRF
that effectively functions as a meter stick. From an information processing point of view,
a QRF is a quantum computer: it outputs an outcome value that is reproducible within
some fixed resolution, and that is assigned standardized units that confer an operational
semantics, when given a “raw” measurement, or simply a physical interaction, as input.
We propose, in particular, that neurons are usefully regarded as hierarchies of QRFs. Neu-
rons in this representation are hierarchical measurement devices that sample their sur-
rounding cellular microenvironments at multiple scales and encode scale-specific expecta-
tions about how those microenvironments will behave. The output of a particular neuron
encodes, for each cell that receives it, a specific, nonfungible representation of the trans-
mitting neuron’s local measurements. These representations are encoded in the “units of
measurement” defined by the sending neuron’s particular combination of QRFs. They im-
pose operational semantic constraints on all downstream processing. These constraints are
inherited by, and give a particular operational meaning to, the activity patterns of the
neural system as a whole.
We develop this proposal using a general formalism, that of Barwise-Seligman classifiers
[41] organized into networks with well-defined limits and colimits (cone-cocone diagrams
or CCCDs) developed in [42] and applied to human cognitive architecture in [43, 44, 45].
Barwise-Seligman classifiers are naturally interpreted as representing either logical or prob-
abilistic constraints; hence CCCDs represent maximal collections of mutually-consistent
constraints, and indeed generalize hierarchical Bayesian inference [45]. To model neurons
as hierarchies of QRFs is to represent them as 1) quantum computers that 2) implement
hierarchical Bayesian inferences determined by 3) their specific three-dimensional (3d) ge-
ometries and 4) the differential signal-transduction velocities that their geometries impose.
This formalism extends naturally to networks of neurons, at any scale from a local circuit
– e.g. a minicolumn – up to an entire brain.
We begin in §2 by briefly reviewing the implementation of QRFs by generic quantum sys-
tems [46, 47, 48] and their representations by CCCDs. We then turn to biological systems,
reviewingtherepresentationofsimplehomeostaticsetpointsasQRFs[49,50]andextending
this treatment to perisynaptic processes in neurons in §3. This allows us in §4 to represent
the hierarchical structures of dendrites as hierarchical QRFs that detect the states – activ-
ity patterns – of particular “objects” – spatially-organized aggregates of presynaptic axon
terminals – in their microenvironments. We then extend this representation to neurons
in §5, showing how the integration of signals from multiple dendritic branches implements
an effectively tomographic computation. This view of neurons as state identifiers scales
upwards to functional networks, the global activity of which provide coarse-grained repre-
sentations of particular components of the external environment. We then briefly review
4
in §6 results from a variety of systems showing that with suitable spatial and temporal
scaling, this model applies to electrically-excitable cells generally. We consider simulation
modeling and experimental approaches suggested by this framework in §7.
2 QRFs and their representation by CCCDs
2.1 QRFs as generic representations of measurement
Information processing in biological systems has traditionally been represented as classi-
cal; despite theoretical arguments from a variety of perspectives [51, 52, 53, 54, 55] and
experimental evidence for the functional relevance of quantum coherence in photoreception
and magnetoreception [56, 57, 58], quantum biology remains in its infancy [59, 60, 61, 62].
Motivated in part by recent analyses showing that cellular bioenergetic resources fall orders
of magnitude short of those needed for fully-classical computation at macromolecular scales
[63], here we adopt a quantum-theoretic perspective from the start. This perspective allows
full use of the quantum information toolkit, including the concept of a QRF; it offers the
advantages of full generality and applicability at any scale [50].
Consider an isolated, finite, bipartite system AB for which the interaction H is weak
AB
enough and the time interval of interest short enough that considering AB to be separable,
i.e. having a joint state |AB(cid:105) that factors as |AB(cid:105) = |A(cid:105)|B(cid:105), is a good approximation. In
this case, it is always possible to choose a basis in which the interaction can be written:
N
(cid:88)
H = βkk Tk αkMk, (1)
AB B i i
i
where k = A or B, the Mk are N Hermitian operators with eigenvalues in {−1,1},
i
the αk ∈ [0,1] are such that (cid:80)N αk = 1, and βk ≥ ln 2 is an inverse measure of k’s
i i i
thermodynamic efficiency that depends on the internal dynamics H [40, 46, 49]. In the
k
form given by Eq. (1), the interaction can, without loss of generality, be regarded as
defined at a holographic screen B separating A from B; the operators Mk can in this
i
case be regarded as “preparation” and “measurement” operators that alternately write and
read bit values encoded on B [46, 64]. The screen B can be realized as an ancillary qubit
array as in Fig. 1. The thermodynamic factor βkk Tk in Eq. (1) assures compliance with
B
Landauer’s Principle [65, 66, 67], i.e. assures that the per-bit free-energy cost of classical
bit erasure is paid on each cycle (see [44, 50] for discussion).
5
Figure 1: A holographic screen B separating systems A and B with an interaction H
AB
given by Eq. (1) can be realized by an ancillary array of noninteracting qubits that are
alternately prepared by A (B) and then measured by B (A). There is no requirement that
A and B share preparation and measurement bases, i.e. QRFs. Adapted from [45] Fig. 1,
CC-BY license.
NowconsiderAtobean“observer”thatdecomposesB intoa“systemofinterest”S andits
surrounding “environment” E. As a familiar example, S may be some item of laboratory
apparatus and E the surrounding laboratory. We suppose further that A is primarily
interested in the state |P(cid:105) of some proper component P of S, conventionally called the
“pointer” of S, that is separable from the remaining proper component R (the “reference”
component), i.e. S = PR and |PR(cid:105) = |P(cid:105)|R(cid:105) over the observation times of interest. Under
these conditions, repeated observations of a fixed state |R(cid:105) of R allow the identification of S
as a system; in our example, repeated observations of the fixed size, shape, location, etc. of
an item of laboratory apparatus allow its identification over time and hence unambiguous
observations of its pointer state [47, 48]. These repeated observations effect decoherence
of S, and hence enforce separability of S from E [46]. Nothing in the above changes when
pure states |P(cid:105) and |R(cid:105) are replaced by densities ρ and ρ , interpreted as distributions
P R
over time series of pure states, with the separability condition ρ = ρ ρ .
PR P R
Assigning mutually-exclusive subsets of bits encoded on B to P, R, and E is clearly a
computational process that must be implemented by the internal interaction H of A.
A
IndeedthisprocessiscompletelyindependentofH or, equivalently, ofthedecompositional
B
6
structure of B [47]. Tracking the states of P, R, and E over time requires an adequate
memory resource and a comparison function capable of detecting changes at some suitably
coarse-grained resolution. As shown in [47, 48], these computations can be regarded as
implementing QRFs for P, R, and E, the functions of which can, without loss of generality,
be specified by networks (formally, cocones) of Barwise-Seligman classifiers as described in
§2.3 below.
It is worth emphasizing here that the decomposition B = PRE is not “objective” in any
sense; the components P, R, and E are defined, relative to and hence “for” A, by their re-
spective QRFs. These QRFs can, therefore, themselves be labeled ‘P’, ‘R’, and ‘E’ without
ambiguity. Similarly, thestates|P(cid:105), |R(cid:105), and|E(cid:105)encodedonB inthebasisspecifiedbyEq.
(1) are not “objective” but rather defined as the domains, respectively, of the QRFs P, R,
and E. They are, therefore, also strictly relative to A (cf. the characterization of quantum
states as personal in [68, 69] or as observer-relative in [70]). The computations implemented
by P, R, and E result in classical, i.e. irreversible encodings of state information to A’s
memory; therefore they require free energy as discussed in [47]. This free energy can only
come from B; hence some fraction of the bits encoded on B must be viewed as supplying
the free energy required for computation. The values of these bits cannot, therefore, affect
the computational outcomes of P, R, and E; they are effectively traced over. This obligate
trace operation renders the outputs of P, R, and E coarse-grained representations ρ , ρ ,
P R
and ρ that can be viewed as probability distributions, and effectively as weighted aver-
E
ages, in the basis specified by Eq. (1), or as pure states in a “computational basis” with
reduced dimension. The dimension reduction or coarse-graining induced by the action of
any QRF, solely in consequence of its free-energy requirements, enables the construction of
QRF hierarchies with distinguishable layers as discussed in the case of neurons in §4 and
§5 below.
2.2 Commutativity constraints on QRFs
AsystemS isonlyuseful,inpractice, asameasurementapparatusifitcanbeidentifiedover
macroscopic time, i.e. has a stable coarse-grained reference state ρ . “Stability” requires,
R
in particular, that observing the environment E cannot affect ρ ; hence the QRFs R and
R
E must commute. The reference state ρ that allows identification of S must, moreover,
R
remain stable when the pointer component P is observed; hence R and P must commute.
Similarly P must commute with E. These commutativity conditions are, effectively, con-
sequences of separability: they follow directly from the assumption that P, R, and E are
each distinguishable from the others.
These commutativity conditions on the QRFs P, R, and E can be summarized in diagram-
7
matic form by:
ρ M P (cid:47)(cid:47) ρ(cid:48) ρ M R (cid:47)(cid:47) ρ(cid:48) ρ M E (cid:47)(cid:47) ρ(cid:48)
P(cid:79)(cid:79) P(cid:79)(cid:79) R(cid:79)(cid:79) R(cid:79)(cid:79) E(cid:79)(cid:79) E(cid:79)(cid:79)
(2)
P P R R E E
|B(cid:105) PU (cid:47)(cid:47) |B(cid:105)(cid:48) |B(cid:105) PU (cid:47)(cid:47) |B(cid:105)(cid:48) |B(cid:105) PU (cid:47)(cid:47) |B(cid:105)(cid:48)
where |B(cid:105) is the bit string encoded on B by the action of H , P is the time propagator
AB U
of the joint system U = AB, and M , M , and M are Markov kernels on the state
P R E
spaces of ρ , ρ , and ρ , respectively. The boundary B functions as a Markov Blanket
P R E
(MB) [71, 72] that renders the system state |A(cid:105) conditionally independent of |B(cid:105) at the
“microscale”definedbyH [73], whileρ , ρ , andρ areeffectiveMBstatesatthecoarse-
AB P R E
grained “computational scale” at which A writes observed state information to memory.
The probabilities encoded by the M , M , and M are posterior probabilities for A from
P R E
a Bayesian perspective; the task of A as an observer implementing active inference [20, 21,
22, 23, 24] is to learn priors, or equivalently, to implement actions on B, that predict M ,
P
M , and M as accurately as possible.
R E
The operators M , M , and M are, clearly, Markovian if but only if the joint density
P R E
ρ factors as ρ = ρ ρ ρ , i.e. if but only if P, R, and E can be assumed to
PRE PRE P R E
be separable. As noted above, separability can be assumed only in the limit of weak
interactions and/or short observation times. Hence Markovian evolution of P, R, and
E is an assumption valid in this limiting case, not a fact. Evolution being Markovian
corresponds, by definition, to probabilities satisfying the Kolmogorov axioms and hence
to the Bell [74] and Leggett-Garg [75] inequalities being satisfied, not violated. Hence
physically, the Markovian evolution corresponds to phase correlations being negligible, i.e.
the absence of quantum entanglement or changes of “intrinsic” measurement context [76,
77, 78] as discussed further below. These conditions can be summarized as observations
being “sufficiently coarse-grained” at the computational scale to be treated as classical.
The commutativity conditions given by Eq. (2) are instances of the general condition
required to interpret any physical process as computation [79]. We can, therefore, regard
ρ , ρ , and ρ as encoding a “semantic” representation of the microscale states |P(cid:105), |R(cid:105),
P R E
and |E(cid:105) encoded on B. It is with respect to this completely-general notion of semantics
that hierarchies of QRFs can be considered semantic, and hence virtual machine (VM)
hierarchies [80], in §4 - 5 below.
2.3 QRFs as constraint networks
In recent work [42, 43, 44, 45] we have drawn extensively upon the semantically enriched
Channel Theory of information flow developed in [41], outlining many examples and appli-
cations (see also in particular [47, 48]). Here we summarize the basic ideas, starting with
8
that of a (Barwise-Seligman) “classifier” (or “classification”), which categorically accom-
modates a “context” in terms of its constituent “tokens” in some language and the “types”
to which they belong.
Definition 1. A classifier A is a triple (cid:104)Tok(A),Typ(A),|= (cid:105) where Tok(A) is a set of
A
“tokens”, Typ(A) is a set of “types”, and |= is a “classification” relation between tokens
A
and types.
Note that this definition specifies a classifier/classification as an object in the category of
Chu spaces [81, 82, 83] where ‘|= ’ is realizable by a satisfaction relation valued in some
A
set K having no structure assumed.
Morphisms between classifiers are specified by the following:
Definition2. GiventwoclassifiersA = (cid:104)Tok(A),Typ(A),|= (cid:105)andB = (cid:104)Tok(B),Typ(B),|=
A B
→− ←−
(cid:105), an infomorphism f : A → B is a pair of maps f : Tok(B) → Tok(A) and f :
→−
Typ(A) → Typ(B) such that ∀b ∈ Tok(B) and ∀a ∈ Typ(A), f (b) |= a if and only if
A
←−
b |= f (a).
B
This last definition can be represented schematically as the requirement that the following
diagram commutes:
−→
Typ(A) f (cid:47)(cid:47) Typ(B)
(3)
|=A |=B
←−
Tok(A) (cid:111)(cid:111) f Tok(B)
Following these definitions, Channel Theory can be represented as a category comprising
classifiers as objects and infomorphisms as arrows, which can be seen to be equivalent to
the category comprising Chu spaces as objects and Chu morphisms as arrows [41].
Much of the formulism of [41] revolves around the idea of a distributed system in which the
classifiersandinfomorphismsbetweenthemfunctionas‘logicalgates’asfurtherexemplified
in [42, 43, 44, 45]. Significantly instrumental in this schemata is a finite, commuting cocone
diagram (CCD) depicting a flow of infomorphisms sending inputs to a core C(cid:48) that is the
colimit of the underlying classifiers if such exists:
(cid:56)(cid:56)C(cid:79)(cid:79) (cid:48) (cid:103)(cid:103)
f1
f2
f
k (4)
(cid:47)(cid:47) (cid:47)(cid:47)
A A ... A
1 g12 2 g23 k
There is a dual construction to this CCD, namely a commuting finite cone diagram (CD)
of infomorphisms on the same classifiers, where all arrows are reversed. In this case the
core of the (dual) channel is the limit of all possible downward-going structure-preserving
9
maps to the classifiers A . Hence we can define the central idea of a finite, commuting
i
cone-cocone diagram (CCCD) as consisting of both a cone and a cocone on a single finite
set of classifiers A linked by infomorphisms as depicted below:
i
(cid:54)(cid:54)C(cid:79)(cid:79) (cid:48) (cid:105)(cid:105)
f1 f
k
f2
A (cid:104)(cid:104) (cid:111)(cid:111) g21 (cid:47)(cid:47) A (cid:111)(cid:111) g32 (cid:47)(cid:47) ... A (5)
1 g12 (cid:79)(cid:79)2 g23 (cid:53)(cid:53) k
h2
h1 h
k
D(cid:48)
Significantly, the CD functions as a “memory-write system” in Eq. (4); this is used to
represent the time-stamped “write” operations of a QRF in [47, 48]. These diagrams can
beextendedtoobtaina“bowtie”diagramasbelowin(6)whichrepresentsacoarse-graining
of the semantics of A via C(cid:48), into a compressed representation A(cid:48).
i i
A(cid:48) (cid:104)(cid:104) (cid:111)(cid:111) g 2 (cid:48) 1 (cid:47)(cid:47) A(cid:48) (cid:111)(cid:111) g 3 (cid:48) 2 (cid:47)(cid:47) ... A(cid:48)
1 g(cid:48) (cid:79)(cid:79)2 g(cid:48) (cid:53)(cid:53) j
12 23
h(cid:48)
h(cid:48) 2 h(cid:48)
1 j
(cid:54)(cid:54)C(cid:79)(cid:79) (cid:48) (cid:105)(cid:105) (6)
f1 f
k
f2
A
(cid:111)(cid:111) g21 (cid:47)(cid:47)
A
(cid:111)(cid:111) g32 (cid:47)(cid:47)
... A
1 g12 2 g23 k
Diagrams such as (4) (6) can be further extended into hierarchical networks by adding
intermediate layers of classifiers and infomorphisms. In this way they resemble artificial
neuralnetworks(ANNs),andinthe“bowtie”formofDiagram(6),theyresemblevariational
autoencoders (VAEs) [42]. The core C(cid:48) in (6) can be viewed as both an “answer” computed
by the f from inputs to the A and, dually, as an “instruction” propagated by the h (or in
i i i
Diagram (6), the h(cid:48)), to drive outputs from the A (or in Diagram (6), the A(cid:48)). This kind
i i i
of dual input/output is precisely that of a QRF. Crucially, a “bow tie” diagram within the
distributed systems described above, is a descriptive mechanism for broadcasting control
signals to multiple recipients. The Global Neuronal Workspace, as a massively parallel,
distributed system, performs precisely this function for the mammalian central nervous
system [84, 85, 86].
We refer the reader to the concepts of a (regular) theory and local logic as introduced in
[41, Chs 9 12] (and extensively reviewed in [42, 45]) to specify the logical structure of a
given situation. Basically, a local logic is a classifier with a theory, along with a subset of
tokens as specified by a sequent; namely, a classification M |= N of a classifier given by
A
a pair of subsets M,N of Typ(A), such that ∀x ∈ Tok(A),x |= M ⇒ x |= N. In fact,
A A
any classifier generates its own natural local logic. Below, we will adopt the sequent as a
‘conditional’ when defining probabilities.
10
2.4 CCCDs implement Bayesian inferences in defined contexts
In general, given an information flow channel:
−→ A −→ A −→ A −→ ··· (7)
α−1 α α+1
the semantic content can be extended by postulating local logics L = L(A ) generated by
α α
the corresponding classifiers A (assumed, in principle, to be in relationship to a (regular)
α
theory associated to the individual A , as specified in [41, Ch 9] and reviewed in [45,
α
Appendix A]), thus postulating a flow of logic infomorphisms:
··· −→ L −→ L −→ L −→ ··· (8)
α−1 α α+1
which may then comprise a CCD as in Eq. (4).
A probabilistic interpretation of information flow in Eq. (8) results when the sequent
relation is weakened to require only that if x |= M, there is some probability P(N|M)
A
that x |= N. 1 In effect, |= becomes a conditional probability that inherits the semantics
A A
of the local logic [87] (cf. [88]); that is, given a sequent in a classifier A, along with its
satisfaction relation satisfying:
M |= N , ∀x(x |= M ⇒ x |= N), (9)
A A A
we can define the conditional probability as:
M |=P N := P(M|N). (10)
A
In fundamental Bayesian terms, M above can be regarded as an unobserved (or as be-
low, unobservable) event, and N an observable datum, in which case P(M) becomes the
prior, and P(N) the evidence, that together generate a prediction. Given the likelihood
P(N|M) as the conditional obtained from weakening the sequent, Bayes’ theorem specifies
this conditional as the posterior:
P(N|M)P(M)
P(M|N) = . (11)
P(N)
We introduce the idea of “contexts” of observation, following [45, §7.1] (see also [48, Ap-
pendix A.1]), by considering the following sets: i) X is a set of events in a very general sense
(e.g. a set of observed value combinations or atomic events); ii) Y is a set of conditions
(specifying objects/contents or influences; iii) Z is a set of contexts (e.g. detectors, mea-
surements, or methods); iv) W := Y ×Z, where X,Y and Z are taken to be subsets of some
(very) large probability space. Note that here Y can be decomposed as Y = Y+∪Y− (dis-
joint union) where Y+ consists of observed objects/contents/characteristics of the context
1We use the upper case ‘P’ to denote a probability in relationship to classifiers, and a lower case ‘p’ for
that pertaining to e.g. events or states as below.
11
Y, and Y− consists of what is not observed about Y. Hence W := Y ×Z = (Y+∪Y−)×Z.
The classifier in question is then A = (cid:104)X,W,|= (cid:105) and represents an observable in context.
A
Among several possible interpretations for the classification relation ‘|= ’ is valuation by
A
the conditional probability p(a|x) = p(a|{b,c}), whenever defined, for a ∈ X,b ∈ Y and
c ∈ Z [45] (see also below).
As a brief example, consider arbitrary classifiers A(a),...,A(e) in some part of an infor-
1 5
mation channel where the classifiers correspond to events a,b,c,d,e, respectively, together
with logic infomorphisms f ,...,f between them, and where the sequents are relaxed to
13 45
become conditional probabilities as above:
A(a) A(b)
1 2
f13 f14 f24
(cid:125)(cid:125) (cid:33)(cid:33) (cid:125)(cid:125)
A(c) A(d) (12)
3 4
(cid:15)(cid:15)
f45
A(e)
5
Following e.g. [89], this particular channel then generates a joint probability distribution
given by:
p(abcde) = p(a)p(b)p(c|a)p(d|ab)p(e|d). (13)
As developed in detail in [45], the above constructions provide a formal basis for classical
Bayesian inference. Specifically, the diagrams (4) and (5) depict channels of logic infomor-
phisms, whenonrelaxingthesequentineachcase, theclassifications|= arenowexplicitly
Aα
realized as conditional probabilities p (·|·), whenever these are defined. Putting the above
α
details concerning conditionals into the diagrammatic framework reveals a typical portion
of a CCD computing a hierarchical Bayesian inference from a set of (posterior) observations
A to an outcome C(cid:48), as having the form:
i
(cid:56)(cid:56)C(cid:79)(cid:79) (cid:48) (cid:103)(cid:103)
p10(·|·) p
k0
(·|·)
p20(·|·) (14)
(cid:47)(cid:47) (cid:47)(cid:47)
A A ... A
1 2 k
p12(·|·) p23(·|·)
The requirement that diagrams (4) and (14) commute amounts to the requirement that
branching “upward” to an overall logic L along any one of the f , is equivalent to following
α
the inferential sequence (8) up to its termination, and then following the last of the f to
α
L. Thus the f can be seen as shortcuts to reaching L: “insights” that allow the rest of
α
the inferences in (8) to be bypassed. The local logic L can be seen as the “answer” to the
problem (8) addresses: formally, it is the logic that solves the problem in one step. The
probability that the answer is “right” is the product of the probabilities along (8). The
12
commutativity requirement on (14) requires, in addition, that this overall probability is
conserved; hence the probability associated with each “insight” f must be the combined
α
probability of the inferential steps it replaces.
Commutativity of a CCD thus enforces inferential coherence, and the same clearly applies
when the dual objects and maps of a CD are combined in constructing a CCCD as in
diagram (5) to implement a recurrent Bayesian network (see also [43] where CCCDs are
employed to model the interaction between bottom-up and top-down processing in visual
object categorization). In contrast, non-commutativity of (4) implies that there is no
consistently definable (conditional) probability distribution across the system in question,
and characterises intrinsic contextuality [76, 77, 78] for that system [45, Th 7.1 and Coroll
7.1].
2.5 QRFs as memory resources
A QRF is only useful operationally, and hence only meaningful as a reference frame, if it
can be deployed consistently over multiple episodes of observation. A meter stick or a volt-
meter is, clearly, useful only if yields consistent measurements; the notion of a “standard”
to which a device can be periodically recalibrated captures this consistency requirement
operationally.
Recalling that a QRF is by definition a physical system, not merely an abstraction [39],
the above condition can be stated as a requirement that a QRF have stable dynamics
over the relevant time period, or if it is a subsystem of a larger system A, that it is an
(or a component of an) attractor of the dynamics of A, i.e. of the self-interaction H .
A
In computational language, a QRF must be a memory structure, one that is active (and
hence executed as a computation) at all times, or one that is executable on demand via
some higher-level control structure. We can interpret a CCD identifying S = RP that is
implemented by A as encoding an expectation, and hence a prior probability for A, that
bit patterns specifying S will be encoded on the holographic screen B separating A from
its complement B. As emphasized earlier, neither the existence of this expectation not its
satisfaction in practice imply that S exists in any “ontic” sense as an observer-independent
component of B (cf. [90]).
Operational utility and hence meaningfulness also require that a QRF write its outcomes to
a classical memory in a way that allows comparison of outcomes obtained at different times.
Following [47, 48], we consider the transition from writing an observed pointer outcome P
i
with timestamp τ to writing P with timestamp τ to be implemented by an operator G ,
i k j ij
thecompletesetofwhichformsagroupoidundercomposition. “Timestamping”herecanbe
consideredmerelytobeimpositionofanorderontherecordeddata; aseparate, observation
independent clock is not assumed (cf. [50] for a general discussion of time QRFs). As
compliance with Eq. (1) requires all classical data to be written on the holographic screen
B, the memory-write CD must write on B as shown in Fig. 2. Comparing previous
with current data, in this case, requires reversing the arrows of the relevant memory-write
13
CD to reconstruct the previously-written data. The comparison itself becomes a “meta”
operation, i.e. one defined at the next-higher hierarchical level as described in the case of
neurons below (cf. [43, 44, 45] for examples from human cognitive processing).
Figure 2: A CD W (green triangle) specifies a memory-write operation of the time-
kj
stamped state (ρRP k,τ ) to B. The timestamp τ is generated by the groupoid action G
j j ij
ontheprevious(atτ )outputfromtheCCDPR. Adaptedfrom[48]Fig. 9, CC-BYlicense.
i
We can, therefore, consider QRFs to provide two distinct memory resources: 1) the stable
QRF itself as an executable computation, and 2) the ordered sequence of classical data
accumulated by deploying the QRF to make measurements. The executable QRF is, in
general, a“quantum”memory, asitisaquantumcomputationimplementedbytheoperator
H . The classical data, in contrast, are written on B and subject to both space and free-
A
energy constraints. As discussed in [50], these latter memories are effectively stigmergic.
Any representation of the “self” that is readable as a classical memory, in particular, must
be written on B, and read from B when recalled [50]. These distinct memory resources
supportdistinctexpectations: QRFsasexecutablecomputationsencodeexpectationsabout
how the “world” faced by an agent is (or more properly, usefully can be) decomposed into
identifiable “systems” while the classical data encode expectations about the states in
which these systems will be encountered. These expectations can be represented as Markov
kernels that are learned incrementally; the difference between these “expected” kernels and
the “true” kernels given by Diagram (2) provides a representation of VFE to be minimized
14
byactiveinference, i.e. byexploration(noveltyseekingthatenlargesthe“trainingset”)and
further learning. This definition of VFE for generic quantum systems allows a formulation
of the FEP [20, 21, 22, 23, 24] for such systems that is developed in detail elsewhere [91].
3 QRFs at the macromolecular scale
We now turn from generalities to the specific case of mammalian cortical neurons, and
show how these can be conceptualized as hierarchies of QRFs, i.e. hierarchies of physically-
implemented computations, each of which can be regarded as a measurement, at a defined
scale and with respect to defined units, of some aspect of its local environment. We then
generalize this treatment to arbitrary cells in §6.
3.1 QRFs as homeostatic setpoints
As discussed in [49, 50], the simplest biological QRFs are switches that induce opposing
behaviors whenever some parameter value is above or below some threshold value. The
CheY system regulating bacterial chemotaxis provides a familiar example: local concen-
trations of the phosphorylated form CheY-P above or below a essentially fixed (at the
relevant timescales) default concentration induce swimming or tumbling behavior, respec-
tively [92, 93]. From a computational perspective, the CheY system implements a switch
betweendepth-firstandbreadth-firstsearch, andhenceagenericmechanismforavoidinglo-
cal maxima in a search space. More biologically, it implements both approach to resources
and avoidance of toxins and other irritants, and hence serves as a homeostatic setpoint.
Such setpoints are, from the present perspective, expectations consistent with continuing
structural and functional integrity, i.e. continuing separability from the surrounding world.
They can, therefore, be expected to arise in any system that maintains separability, i.e.
any system to which Eq. (1) applies.
A second familiar QRF is the default or “resting” membrane voltage V0 across any local
mem
patchofbiologicalmembrane. Fluctuationsaboveorbelowthissetpointinduceactions, e.g.
opening or closing voltage-gated ion channels. This QRF is obviously relevant to neuron
function as discussed below; at larger scales, it becomes a critical regulator of multicellular
morphology; see e.g. [94, 95, 96] and further discussion in §6.
Guided by these examples, it is straightforward to consider any system implementing linear
threshhold or sigmoid kinetics as a QRF that switches between “negative” and “positive”
responses as its input fluctuates below or above a default value (Fig. 3). With the ad-
vent of two-component signaling pathways, early enough in prokaryotic evolution that they
appear in all analyzed lineages [97], the default value becomes adjustable, i.e. context-
dependent via a classical “direct influence” mechanism [78] (but see [98] for evidence that
lactose-glucose interference in E. coli [99] exhibits nonclassical contextuality). The multi-
component systems typical of eukaryotes, e.g. the Wnt [100], ERK [101], notch [102], BMP
15
[103] pathways and the interactions between them, introduce further, multifactorial context
dependence (see e.g. [104, 105] for general reviews). Such systems embody expectations
about the relationships between multiple, simultaneously-measured values, each effectively
calibrated to an underlying standard, e.g. a molecular concentration or local value of V .
mem
Figure 3: a) A generic system executing sigmoid kinetics. b) Reflecting the response curve
at its inflection point redescribes the system as a QRF that switches behavior at a default
value.
As noted earlier, cellular-scale biochemical and bioelectric signaling pathways have tradi-
tionally been conceptualized and depicted as fully classical, with each component occupying
some determinate state at all relevant times. Cellular energy budgets cannot, however, sup-
port the thermodynamic cost of classicality, even at ms timescales, indicating that quantum
coherence and hence quantum computation may be significant up to timescales of seconds
[63]. Classical data can, in this case, only be encoded on intra- or intercellular boundaries
as illustrated schematically in Fig. 1. “Chunking” the cell into functional components that
process and exchange classical information or serve as classical memory structures (e.g. the
genome, proteome, transcriptome, and “architectome” [106]) is, effectively, identifying the
boundaries at which classical information is encoded, i.e. the boundaries that function as
MBs. A signal transduction pathway that measures some environmental parameter at the
cell membrane and transfers a context-dependent representation of this information to the
genome is, in this picture, a single quantum computation, a QRF that detects, calibrates,
and reports an observational outcome. It can be treated as a black box characterized by
inputs and outputs, including free energy inputs and dissipated heat outputs. We adopt
this approach to characterize the functional architecture of neurons in what follows.
3.2 The postsynaptic complex as a QRF
Asdiscussedin[107,108,109], theMBofasingleneuronincludesthepre-andpostsynaptic
membranes that mediate interactions with other neurons, as well as the overall cell mem-
16
brane that mediates interactions with the neuron’s local microenvironment. Single neurons
are, however, large, spatially-extended, computationally-complex structures (Fig. 4); it is
the need for improved models of such structures that motivates this paper. Hence we will
consider how the neuron’s MB is constructed from those of its components, identifying in
this process sites at which intermediate steps in neuronal computations may be classically
encoded.
Figure 4: Cartoon structure of a mammalian cortical neuron; cf. [110, 111] for anatom-
ical details. a) Voltage gated channels and pumps are the primary input (sensory) and
output (active) MB components; b) Specialized postsynaptic structures, e.g. on dendritic
spines, collect incoming information from other neurons; c) dendritic subtrees integrate and
process this incoming information; d) dendritic information is integrated at the soma and
distributed to outgoing axonal channels. Postsynaptic receptors may also appear on the
soma or on axons; this level of detail is neglected here.
Starting from the input side, the smallest cellular-scale component is the specialized post-
synaptic area, typically located on a spine morphologically. Its computational role is to
contribute a positive (excitatory) or negative (inhibitory) time-dependent V gradient to
mem
the overall electrical activity of its local dendritic branch. Performing this function requires
17
energetic and molecular flows in addition to ∆V (t), as illustrated in Fig. 5. At this level
mem
of abstraction, the presynaptic specialization at the axonal bouton is essentially equivalent,
up to reversing the flow of small molecules from the cell membrane.
Figure 5: Cartoon structure of a mammalian postsynaptic specialization; cf [4] for anatom-
ical and functional details. Primary energy (red), molecular (blue) and V gradient
mem
(orange) flows are shown. Considered as a system, the spine’s MB includes the spine mem-
brane (solid line) and the cytoplasmic/cytoskeletal interface between the spine and the bulk
dendritic cytoplasm (dashed line).
Making the reasonable assumption of balanced mass flows, i.e. assuming that the volume
and density of the postsynaptic specialization, or in quantum terms, its Hilbert-space di-
mension remains constant, we can focus on information flows only, and treat the interaction
between the postsynaptic specialization and its surroundings using Eq. (1). The bound-
ary B in this case has two components, B = CD, where C separates the postsynaptic
specialization from the external microenvironment and D separates it from the bulk den-
dritic cytoplasm. Both components support both inward sensory flows and outward active
flows. As these information flows are both physically implemented and referenced to de-
fault homeostatic/allostatic setpoints, they can be considered to be implemented by QRFs.
Hence they can be represented as input – output or perception – action flows as in Fig. 6.
In this representation, the boundary B is decomposed functionally into B = IO, where I
and O are “input” and “output” components respectively, each of which includes regions of
18
both anatomical components C and D. While each flow Q has its own characteristic time
constant τQ as in Fig. 2, the cross-modulatory couplings between pathways, and hence mu-
tual dependence between setpoints, requires that they be mutually coherent. Hence we can
represent the postsynaptic specialization as executing a single QRF with a vector output
and a time constant τPS = max τQ. Presynaptic specializations can be treated similarly.
Q
Figure 6: Information flows abstracted to QRFs (X, Y, Z; colored triangles) that read from
an input boundary component I (right hand blue ellipse) and write to a output boundary
component O (left hand blue ellipse), where the overall system boundary B = IO. Each
QRF Q has an associated time constant τQ and a groupoid-structured set of operators GQ
ij
as depicted in Fig. 2.
As discussed in §2 above, all classical information in the system must be encoded on B.
Hence Fig. 6 depicts the ideal case in which the boundary component O encodes all out-
put information, i.e. the computations implemented by the QRFs X, Y, and Z have no
classically-encoded intermediate states. In this ideal case, the free-energy cost of compu-
tation per unit time is given by the output bandwidth in bits, i.e. the area in bits of O.
Assuming a minimal thermal energy ∆E = ln2k T, the minimum dissipation timescale
th B
is given by the time-energy uncertainty relation [112] as ∆t ≥ π(cid:126)/(2∆E ) ≈ 50 fs.
diss th
Neuronal energy budgets, however, are insufficient for classical encoding at this timescale;
assuming 30,000 synapses per cortical neuron [111] and a maximum power consumption
of 250 Gbits/sec (where as a power unit 1 bit = ln2k T ≈ 3 · 10−21 J at T = 310 K)
def B
[26], each synapse could encode at most 8.3 Mbits/sec if the cell’s metabolic resources were
devoted entirely to synaptic encoding. In fact the bulk of neuron energy usage is devoted to
19
actionpotential(AP)generationandtransmission, reducingthecodingcapacityofsynapses
by up to two orders of magnitude (see [63, 113, 114, 115] for further discussion of neuronal
classical encoding costs and capacity). We can expect, therefore, that individual postsynap-
tic specializations can realistically encode at most about 100 kbits/sec, or 100 bits per unit
time at the ms scales of dendritic postsynaptic potentials. Hence the idealization of Fig. 6
is not unrealistic from an energetic perspective, suggesting that consistent with the general
considerations in [63], QRFs implemented at the scale of cellular compartments perform
essentially pure quantum computations, i.e. employ quantum coherence as a short-term
memory resource.
The form of Fig. 6 suggests a cobordism with I and O as boundaries. If the QRFs X,
Y, and Z perform pure quantum computation without classical intermediate steps, we
can treat them in a path-integral formalism [116], and hence as defining a manifold of
mappings from I to O. This suggests that information processing in neurons is amenable
to a treatment using topological quantum field theory [117] and the emerging theory of
topological quantum neural networks [118]. We defer this possibility to future work.
4 Dendrites as spatially-organized QRFs
Dendritic branches are traditionally viewed as lossy but otherwise passive integrators of rel-
atively slow (10s of ms) postsynaptic potentials (PSPs), though faster spike-like activations
and backwards propagation of APs are also possible [110]; see [119] for detailed dendritic
spike models. This passive view of dendritic activity renders the execution of complex
logical operations like XOR [11] surprising and activity-dependent remodeling [2, 3, 4] mys-
terious. It decouples the processes required to maintain a successful synapse: learning is
largely localized to the postsynaptic side, while target search and synapse formation are lo-
calized to the presynaptic side. Finally, it decouples dendritic function from the dendrite’s
trophic requirements. Branches that transmit only low-amplitude noise signals cannot, on
the passive model, be regarded as less successful or less worthy of continued metabolic
maintenance in comparison to those branches that consistently transmit high-amplitude,
highly informative signals.
Dendritic branches satisfy the physical requirements for implementing QRFs and hence
engaging in active inference. Viewing dendrites in this way raises two immediate questions:
1. What is a dendritic branch measuring?
2. What is the VFE function that a dendritic branch is minimizing?
However, while dendrites can be described as coincidence and hence activity-correlation
detectors [110] even on the passive view, the second of these questions is difficult to even
formulate from a passive perspective. A partial answer to both questions is discussed in
[29, 30]: basically, dendrites can self-organize to minimize the VFE on surprise of their
20
presynaptic inputs, showing that postsynaptic gain is itself optimized with respect to VFE.
Extending the modeling approach of the previous section to dendritic branches suggests an-
swers to these questions that can be summarized by the hypothesis that dendritic branches
identify“objects”withrelativelysmallnumbersofstates,andexecuteapproximatelybinary
logic on the measured state vectors.
4.1 Colocating consistently correlated synapses minimizes VFE
While both dendritic arborization and spine density vary with neuron type and location
[120, 121, 122, 123], both cortical and hippocampal pyramidal cells can exhibit dendritic
branches with on the order of 1,000 spines and full dendritic trees with up to 30,000 spines
[111]. Consider such a branch, neglecting information and resource flows through the non-
spine membrane to focus on flows to and from the spines as modeled in Fig. 5. As
the magnitudes of resource flows correlate with activity and hence with the magnitude of
∆V , we can further focus on the latter. In this case, we can think of the branch as
mem
“observing” on the order of 1,000 points of active signaling, analogous to 1,000 points of
light in a visual field.
There are clearly two limiting cases for the activity observed by a branch. If every spine
receives input from a distinct, independently-activated neuron, the correlation C between
ij
inputs i and j can be expected to be small, with C → 0 in the limit. If all spines receive
ij
input from a single neuron, C → 1 in the limit. Assuming roughly constant frequencies
ij
(though random phases) of presynaptic activity, the former limit yields noise with an input-
frequency dependent amplitude as an output PSP from the branch; the latter yields an
essentially digital signal encoding the activity of the single input neuron. An essentially
constant noise signal is effectively a “dark room” from a VFE perspective, while a variable
digital signal poses a well-defined, potentially high-information prediction problem.
Microconnectome methods [124] allow, in principle, counting the numbers of presynaptic
(i.e. incoming) and postsynaptic (outgoing) partners for any neuron. Measured numbers
of presynaptic partners range up to 50 in C. elegans [25] and from tens to several hundred
in mammalian cortical cells [125, 126] to thousands for cerebellar Purkinje cells [124]. If
as estimated [127] neocortical pyramidal cells typically receive 4 or 5 synaptic connections
from each presynaptic partner, these cells may also have several thousand presynaptic
partners. Such cells would be operating very close to the noise limit of C → 0 unless
ij
the activity of the presynaptic partners is already highly correlated. Perhaps significantly,
most microscale connectome mapping has been achieved in sensory cortices in which highly
correlated incoming information can be expected.
Following [110] and considering each dendritic branch to be a coincidence detector, the
fundamental problem faced by a branch is distinguishing between i) correlated signals from
a given presynaptic partner, ii) “true” correlates from multiple partners, and iii) “ran-
dom” correlates. We can treat this uncertainty as a VFE function, and ask how it can
be minimized. For a relatively short, distal, terminal branch exhibiting all three kinds of
21
correlates, the answer is shown in Fig. 7: localizing partially-correlated regular inputs to
distinct, non-branched branches and moving random inputs as far as possible from branch
junctions disambiguates signals from different synaptic partners, allows “true” correlations
to be identified specifically at branch junctions, and reduces the amplitude of noise signals.
Figure 7: a) Least and b) most efficient arrangements of synapses with partially-correlated
(red and blue) and random (green) activity on a two-sided symmetrical, terminal dendritic
branch. By minimizing the ambiguity of correlation, arrangement b) allows maximally
digital processing at the branch junction.
A terminal branch in an initial state similar to Fig. 7a has synapse-level learning and spine
remodeling available as tools for moving toward a state similar to Fig. 7b, with trophic re-
ward as the imposed quality function. This is active inference, with spine remodeling as the
“action” that alters environmental input, i.e. synaptic activity, independently of synapse-
level priors. Success (given enough time) can be expected if trophic reward increases with
the overall informativeness of the branch’s activity, i.e. if trophic reward to dendrites as
functional units has the same activity-dependence as trophic reward to neurons as func-
tional units [1, 128, 129], as studies of dendritic remodeling already suggest [3, 4]. This
is mechanistically reasonable, as it transfers both the task of determining informativeness
and that of adjusting trophic reward to the proximal part of the dendritic tree, and hence
effectively to the soma itself. Both are, from the branch’s perspective, outside of its MB
and hence part of the environment.
We can, therefore, state the following:
Prediction: Trophic reward to dendritic branches correlates with informative-
ness of the signal from the branch for more proximal dendritic branches, with
informativeness to the soma as a limit.
What the soma needs to know is whether to fire an AP. A high-amplitude signal on a
low-amplitude background, i.e. an effectively digital PSP, provides this information most
22
efficiently. Hence we can predict that dendritic trees will remodel in the direction of maxi-
mizing digital processing.
4.2 Branches as object detectors
A branch that has segregated its non-noise inputs into different compartments as in Fig.
7b is, effectively, an object detector. The object it detects is a presynaptic partner, or a
spatially-localized collection of axonal processes from a presynaptic partner. It measures
the detectable state of this object, its binned-average activity at some binning timescale,
typically a few ms. Hence we can think of it on the model of Eq. (2) or Fig. 2, with the
input correlation as the reference R, the binned-average activity as the pointer P, and any
background noise as the environment E. Hence a compartmentalized branch is a canonical
collection of QRFs.
From this perspective, we can see spine remodeling within dendritic branches as a process of
assemblingQRFsthatidentifyobjectsandhenceenablemeasurementsoftheirstates. Spine
remodeling is thus an instance of the second, relatively neglected, form of active inference:
the minimization of uncertainty about the decomposition of an agent’s world. A branch
with maximally-ambiguous inputs as in Fig. 7a decomposes its world only into “points of
light”; a branch that has remodeling to approach Fig. 7b sees instead two objects with
well-defined states that can be compared in both amplitude and time dimensions. Spine
remodelingis, asaretheprocessesofobjectsegregationandobjectpersistenceduringinfant
development, learning what to see, and hence learning what coherent things populate the
perceived world.
5 Neurons as measurement devices
5.1 Dendritic geometry imposes salience
As the “cut” and hence the boundary imposed to render a branch “terminal” is moved
toward the soma, the branch accumulates junctions separating sub-branches, and object-
identity information is replaced by coincidence information. At each such sub-branch junc-
tion, the post-junction sector of the dendrite is faced with the task of reducing VFE by
distinguishing “true” from “random” coincidences. Learning and remodeling driven by
trophic reward remain the tools available for amplifying the former at the expense of the
latter, thus increasing the signal-to-noise ratio and rendering signal processing effectively
moredigital. Henceonecanexpectneuronstoremodelinthedirectionofsegregatinginputs
to their dendritic trees in a way that maximizes true coincidences at branch junctions. To
the extent that they do this, they increasingly operate as hierarchies of spatially-segregated
QRFs
Dendriticgeometrygivesproximalsynapsesmoreinfluenceoversomaticactivitythandistal
ones; the gating function of proximal inhibitory synapses, in particular, is well known [110].
23
Distal signals can only be processed with high (Bayesian) precision in periods of proximal
silence. Hence it is natural to think of dendritic geometry as imposing a salience gradient
from proximal to distal synapses. Stereotypical differences in arborization pattern between
pyramidal cells from different cortical or hippocampal layers reflect different salience as-
signments, with cortical cells typically giving distal inputs, and hence distal presynaptic
partners, higher salience than do hippocampal cells [110]; see [130, 131, 132] for specific
differences between cortical cells from different layers. Higher distal salience is only useful
from an information processing perspective if it is low-noise, i.e. if PSPs from distal den-
dritic branches, e.g. from the elaborate apical tufts of Layer V cortical pyramidal cells, are
(primarily) time-convolutions of true coincidence signals. As remodeling takes time, one
can expect neurons that compute the same function over long periods, i.e. that are required
to exhibit only slow learning, to assign the greatest salience to distal inputs.
In the spiking neurons of interest here, somatic signal integration results, given sufficient
amplitude, in AP generation. While APs are relatively high-amplitude, low-noise signals,
outside of myelinated axonal segments differential AP degradation can be expected to
contribute to differential presynaptic signal strength. Weak presynaptic signals are weak
actions on the signaling neuron’s environment, and cannot be expected to yield strong
confirmatory signals in return. Hence relatively weak synapses can be expected to be
clustered, consistent with Fig. 7b.
5.2 Neurons as tomographic computers
As discussed in §2, a QRF corresponds to a specific choice of basis for a subset of the
operators MA deployed by an agent A to measure the state of its MB, or boundary B. We
i
can, therefore, think of the hierarchy of QRFs implemented by a neuron’s dendritic tree as
a hierarchy of basis choices, with the “low-level” basis components corresponding to detec-
tions of correlated inputs from local clusters of synapses from single presynaptic partners
and the “high-level” basis components corresponding to coincidences between increasingly
more highly “chunked” aggregates of input signals. Each neuron can thus be thought of
as deploying a specific hierarchy of basis vectors to measure the presynaptic activity of its
environment.
Consider now a set N of neurons sampling the “same” environment, e.g. a set of Purkinje
cells each sampling the axonal outputs of a set of cerebellar granule cells. The causal
antecedentsoftheactivityoftheinputneuronsareclearly“unknowns”fromtheperspective
of the neurons in N; the input activities can, therefore, be considered from this perspective
an ensemble E of samples from an unknown, time-varying state. We can, in this case,
think of N as making a set of measurements, each in a distinct basis, of the ensemble E of
time-varying states.
If we think of the states in E as quantum states, then the set N of neurons can be regarded
as performing quantum state tomography on E, and hence quantum process tomography on
the process generating time variation in E [133]. Quantum state tomography generalizes
24
classical tomography by generalizing the choice of basis away from “slices” of a three-
dimensional geometry in which the system of interest is embedded. It is expensive in
terms of basis dimensionality, requiring d2 basis vectors for quantum states of (binary)
dimensiond, and∼ d4 basisvectorsforprocessesonstatesofdimensiond. Thisdimensional
cost translates well to the case of ensembles of neural activity, since as in the case of
quantum states, it is phase correlation information that the tomographic measurement
aims to extract.
Viewing neural computation as tomographic provides an immediate explanation for the
enormously high input and output bandwidths and apparent redundancy of neuronal ar-
chitectures, which are difficult to understand on the basis of simple logic-circuit models
[127, 134]. An input space with 100-bit states (d = 100) would require ∼ 1004 = 108 binary
dimensions for process tomography, or roughly 105 neurons at 1,000 distinct presynaptic
partners per neuron. Following [135] and assuming ∼ 108 minicolumns with 100 neurons
each in human neocortex, analyzing a 100-bit state would require a minimum of ∼ 103
minicolumns or 0.001% of neocortical capacity. We can infer from this that tomographic
computation is approximate, with substantial reduction of the input dimension by salience
systems, e.g. active attention.
5.3 Scaling upwards: minicolumns to functional networks
The stereotypically layered, only sparsely interconnected minicolumns of mammalian cor-
tex, and in particular, human neocortex, are widely viewed as computational as well as
neuroanatomical units, with modeling studies increasingly suggesting that minicolumns ex-
ecute Bayesian predictive coding [30, 107, 109, 123]. Extending the model outlined above,
wecanthinkofminicolumnsasfunctionallyanalogoustoneurons, gatheringinputfromand
sending output to other minicolumns. As does a neuron, a minicolumn “sees” a spatially-
distributed collection of (positive or negative) excitations, spatial and temporal correlations
between which are informative to the extent that they are non-random. Hence a minicol-
umn is faced with a coarse-grained version of the VFE minimization problem faced by
neuron: that of disambiguating “true” coincidences from random ones. Minicolumns can
be expected to functionally remodel to increase signal-to-noise ratio for the same reasons
neurons can, with trophic reward from the environment as the selective criterion.
Functional networks spanning multiple cortical regions, e.g. sensory pathways, impose
a higher-level hierarchical organization. Predictive coding across layers of this higher-
level hierarchy have been analyzed in terms of typical connections between minicolumns in
adjacent layers; see e.g. [109, Fig. 4] or [123, Fig. 2] for interlayer connection maps. Top-
down connections encoding likelihood at pathway layer i arise mostly from minimcolumn
Layer V pyramidal cells and target minicolumn Layer III cells at pathway layer i − 1.
Reciprocally, bottom-up connections arise mainly in minicolumn Layer II at pathway layer
i, and project to networks of Layer IV cells at pathway layer i + 1. Hence at any level
of the pathway, empirical priors are localized in minicolumn Layer III, and likelihoods (or
predictions) in minicolumn Layer V. This is further analyzed in [107] in terms of how MBs
25
influence connectivity of microcircuits. Internal and external states are implemented by
spiny stellate cells and interneurons of each minicolumn; via connections from and to these,
respectively, superficial pyramidal cells of the next minicolumn become the (blanket) active
states, while the deep pyramidal cells of the previous minicolumn become the (blanket)
sensory states. Effectively, the minicolumns are the functional units comprising an MB of
networks (either seen as e.g. a MB of MBs, or a Matryoshka of MBs). Once the MBs
are functional, the overriding principle is that neurons, microcolumns and networks appear
to dynamically self-organize thanks to the FEP. There are many interesting outcomes:
consider for instance the visual network as composed of internal states influencing the
dorsal and ventral attention networks, while the default-mode network plays the role of a
sensory system mediating the influence between these former and external, sensorimotor
states. Just as in the case of lower-level structures, such high-level, multi-layer processing
pathways exhibit activity-dependent trophic reward and remodel as necessary to achieve it,
as shown e.g. by studies of large-scale pathway remodeling following injuries that remove
expected inputs [136] (cf. [109, 137]).
SinceFriston’sproposalthatthemammalianbrainisanactive-inferencedevice[20], numer-
ous studies have explored the implementation of active inference by large-scale networks,
up to and including the global neuronal workspace [44, 45]. What we have shown here
is that these principles extend downward to the scale of the individual synapse. Trophic
rewards select for informative activity, i.e. high signal-to-noise ratios, at every scale. We
can expect these considerations to generalize from neurons to non-neural cells, and from
neural communication to communications between biological structures at every scale.
6 Generalizing neural computation to cellular compu-
tation
The prior discussion was framed in the context of neurons; however, it becomes more
generally applicable when we note that neurons did not appear de novo but in fact evolved
slowly from other cell types which already shared many of their features (reviewed in
[16]). Not only are the basic molecular components of neurons (ion channels, electrical
synapses, neurotransmitter machinery) already present in most cells including unicellulars,
but all cells produce bioelectric gradients and comprise tissues with propagating changes
in resting potential [95]. It has been suggested that developmental bioelectricity is the
evolutionary precursor to neural dynamics, and indeed that evolution speed-optimized the
slow bioelectrical signaling that was first used to solve problems in morphospace (exerting
anatomical control over body shape by regulating cell behaviors) before it was exapted to
solve problems in 3-dimensional behavior space (by regulating muscle function) [138, 139].
Consistentwiththishypothesis, bioelectricsignalinginnon-neuralcellshasbeenimplicated
in control of morphogenetic decisions in embryogenesis, regeneration, and cancer [140, 141,
142]. Recent work targeting the native ion channels and gap junctions in tissue has shown
that bioelectric states can be readily modulated to predictably alter organ identity, induce
26
regeneration of limbs, control the axial patterning of whole bodies, and repair complex
structuressuchascraniofacialbirthdefects[143]. Indeed, ithasbeensuggestedthatparallel
to the architecture of brains, non-neural bioelectricity is the medium implementing the
information processing that enables collective intelligence of cellular swarms during body
construction and remodeling [144].
Morphological change is a deeply computational process that relies on calibrated measure-
ment, and hence QRFs, at multiple organizational levels. While the DNA determines the
micro-level hardware that each cell gets to have (e.g., its complement of ion channels),
genomes do not directly encode morphology. Instead, they specify machines that have to
deploy considerable intelligence, using William James’ definition of intelligence as the abil-
ity to reach the same outcome from different starting conditions and despite perturbations.
The physiology of cellular collectives implements very robust problem-solving capacities
by making important decisions about collective macrostates that are not defined at the
level of individual cells. For example, mammalian embryos cut in half do not result in two
half-embryos – the system regulates to make complete, normal bodies of monozygotic twins
(regulative development). Tadpoles that are developmentally disrupted to have all of their
craniofacial organs in the wrong positions still result in normal frogs after metamorphosis
because the eyes, nostrils, mouth, etc. move through novel paths to get to the same invari-
ant outcome – a correct frog face [145, 146, 147]. Salamanders whose arms are amputated
at the shoulder or wrist regenerate precisely what is needed and then stop when a correct
limb is complete. These are just a few examples of a nearly ubiquitous property of morpho-
genetic systems: anatomical homeostasis toward a specific pattern memory, and the ability
to reach that state despite sometimes drastic, unpredictable perturbations. This degree of
anatomical control requites fundamentally computational functions by cell collectives: they
need to be able to measure the current state (e.g. the length of a limb, configuration of the
face, etc.), remember the correct state (represent aspects of the correct target morphology),
and execute a kind of means-ends analysis to reduce error by controlling cell proliferation,
migration, and differentiation. Trophic reward plays a critical role in such processes: struc-
tures or bodies that are functionally insufficient to obtain nutrients and other resources
from their environments do not survive.
Consistent with a functional continuity between neural and non-neural dynamics, morpho-
genesis shares a number of major features with behavior in addition to the reliance on bio-
electric networks to implement large-scale coordination. The first is the hardware/software
distinction: genomes do not encode final outcomes, they encode the structure of a system
with plasticity, context-sensitivity, and the flexibility to produce different outcomes from
the exact same hardware. This is why genetically wild-type flatworm cells can generate
head structures appropriate to other species when their bioelectric signaling is shifted to
different attractors [148], why normal skin cells liberated from frogs spontaneously self-
assemble into different, motile proto-organisms (“Xenobots”) without any genomic editing
[149,150], andwhyembryoswithseveregeneticdefectscanbebioelectricallycoaxedtonor-
mal brain morphogenesis by reinforcing specific voltage patterns [151, 152]. Developmental
bioelectric circuits also feature a kind of re-writable memory. In addition to the default
27
voltage patterns (like the “electric face” [153, 154]), new ones can be written into tissues
and maintained by the circuit, such as the two-headed planaria that result in genetically
wild-type worms when a different V pattern memory is temporarily incepted into the
mem
tissue [155, 156]. These two-headed worms continue to regenerate as two-headed in perpe-
tuity (without additional treatment) or can be bioelectrically switched back to one-headed
[155, 157], illustrating the stable but re-writable aspects of the information in the tissue
that guides behavior.
Bioelectric signaling in all tissues, like in nervous systems, integrates information across
distance to enable decisions and behavior toward adaptive outcomes in novel circumstances.
This basic scheme was already discovered by evolution as far back as the time of bacterial
biofilms [158, 159], and is exploited very widely across the web of life from microbes to
humans [96]. Many aspects of cognitive neuroscience have clear parallels in developmental
biology [160], suggesting that much of the reasoning in §3–5 above applies not just to
nervous systems but in fact to all cells solving problems in various spaces. Indeed, all living
systems are deeply hierarchical, exhibiting aspects of basal cognition and decision-making
at levels including molecular pathways [161, 162], physiological problem-solving by cells
[163, 164], and tissue and organ memory [165, 166, 167] among others.
7 Conclusions and future directions
Neurons, networks of neurons, and biological structures generally exchange information
with their environments via physical interactions interpretable in terms of measurement (or
the gathering of sensory input) and manipulative action. This interpretation of biological
activity forms the basis of the active-inference principle [20, 21] and has achieved wide
currency in the biophysics, evo-devo, and neuroscience communities. We have shown here
how to formulate this view of biological activity in the very general yet powerful language
of quantum information theory, employing in particular the idea of a QRF as a calibrated
measurement, and in the output direction, a calibrated action. These results extend the
analysis of QRFs as hierarchical Bayesian systems developed in [43, 44, 45, 47]. They
allow us to model processes implemented by neurons and networks of neurons, from the
biomolecular pathway scale upward, as hierarchies of QRFs. This representation makes
explicitwhereandhowclassicalinformationisencodedonsuccessivelayersofMBs,enabling
models that explicitly comply with energy-budget considerations. Such models require that
cells employ quantum coherence as a resource for bulk computation [63]. Hence the current
framework represents neurons as explicitly quantum devices.
Our goal here has been to develop a framework for building detailed models of specific
neuronal cell types in specific environments; these will be pursued in future work. Even
at the current abstract level, however, we are able to predict generically that dendritic
trees will remodel in the direction of maximal informativeness, with trophic reward as the
selection criterion. While this prediction is generally supported by the phenomenology of
dendritic remodeling [3, 4], it is specifically testable by correlating measures of branch level
28
activity and branch level active transport. As branch level remodeling is now known to be
involved in neurodegenerative conditions [3, 4], an understanding of the relations between
architecture, computation, and trophic reward at this scale may be useful in ameliorating
these conditions.
From a more general perspective, treating neurons in explicitly quantum-theoretic terms
introduces new possibilities for mathematical modeling, e.g. with topological field theory
as discussed in §3. Neuroscience and computer science have both benefitted from the ab-
straction of neurons to sum-threshold units connected in layered ANNs. As interest in and
tractable architectures for quantum computing continue to develop, we may expect a com-
parable, but considerably deeper, connection between biological and artificial computing
systems.
Acknowledgements
ML gratefully acknowledges funding from the Guy Foundation and the Finding Genius
Foundation.
Conflict of interest
The authors declare no competing, financial, or commercial interests in this research.
References
[1] Butz M, Wo¨rgo¨tter F, van Ooyen A. 2009 Activity-dependent structural plasticity.
Brain Res. Rev. 60, 287–305.
[2] Carulli, D.; Foscarin, S.; Rossi, F. 2011 Activity-dependent plasticity and gene ex-
pression modifications in the adult CNS. Front. Mol. Neurosci. 4, 50.
[3] Hogan, M.K.; Hamilton, G. F.; Horner, P .J. 2020 Neural stimulation and molecular
mechanisms of plasticity and regeneration: A review. Front. Cell. Neurosci. 14, 271.
[4] Runge K; Cardoso C; de Chevigne A. 2020 Dendritic spine plasticity: Function and
mechanisms. Front. Synaptic Neurosci. 12, 36.
[5] Wittenberg, G. M.; Wang, S. S.-H. 2016. Evolution and scaling of dendrites. In (G.
´
Stuart, N. Spuston and M. H’auser , eds.) Dendrites. Oxford University Press, Oxford
UK.
[6] Guerguiev, J; Lillicrap, T. P., Richards, B. A. 2017. Towards deep learning with
segregated dendrites. eLife 6:e22901, 37 pages.
29
[7] Sardi, S.; Vardi, R.; Goldental, A.; Tugendhaft, Y.; Uzan, H.; Kanter, I. 2018 Den-
dritic learning as a paradigm shift in brain learning. ACS Chem. Neurosci. 9, 1230–
1232.
[8] Blackiston, D.; Shomrat, T.; Levin, M. 2015 The stability of memories during brain
remodeling: A perspective. Commun. Integr. Biol. 8, e1073424.
[9] Shomrat, T.; Levin, M. 2013 An automated training paradigm reveals long-term
memory in planarians and its persistence through head regeneration. J. Expt. Biol.
216, 3799–3810.
[10] McConnell, J. V. 1967 A Manual of Psychological Experimentation on Planarians.
Journal of Biological Psychology (Publisher), Ann Arbour, USA.
[11] Gidon, A.; Zolnik, T.A.; Fidzinski, P.; Bolduan, F.; Papoutsi, A.; Poirazi, P.;
Holtkamp, M.; Vida, I.; Larkum, M. E. 2020 Dendritic action potentials and compu-
tation in human layer 2/3 cortical neurons. Science 367, 83–87.
[12] McCulloch, W. S.; Pitts, W. 1943 A logical calculus of the ideas immanent in nervous
activity. Bull. Math. Biophys 5, 115–133.
[13] Segev, I; London, M. 2000 Untangling dendrites with quantitative models. Science
290, 744–750.
[14] Schuman, C.D.; Potok, T. E.; Patton, R. M.; Birdwell, D.; Dean, M. E.; Rose, G.
S.; Plank, J. S. 2017 A survey of neuromorphic computing and neural networks in
hardware. Preprint arXiv:1705.06963v1 [cs.NE].
[15] Tang, J.; Yuan, F.; Shen, X.; Wang, Z.; Rao, M.; He. Y.; Sun, Y.; Li, X.; Zhang, W.;
Li, Y.; Gao, B.; Qian, H.; Bi, G.; Song, S.; Yang, J.; Wu, H. 2019 Bridging biological
and artificial neural networks with emerging neuromorphic devices: Fundamentals,
progress, and challenges. Adv. Mater. 31, 1902761.
[16] Fields, C.; Bischof, J.; Levin, M. 2020 Morphological coordination: A common an-
cestral function unifying neural and non-neural signaling. Physiology (Bethesda) 35,
16–30.
[17] Barrat, A.; Barth´elemy, M.; Vespignani, A. 2008 Dynamical processes on complex
networks. Cambridge University Press, Cambridge UK.
[18] Rubinov, M.; Sporns, O. 2010 Complex network measures of brain connectivity: Uses
and interpretations. NeuroImage 52, 1059–1069.
[19] Latham, P.; Dayan, P. 2005 Touch´e: the feeling of choice. Nature Neuroscience 8(4),
408–409.
[20] Friston, K. J. 2010 The free-energy principle: A unified brain theory? Nature Reviews
Neuroscience 11, 127–138.
30
[21] Friston, K. J. 2013 Life as we know it. Journal of The Royal Society Interface 10,
20130475.
[22] Friston, K. J.; Stephan, K. E. 2007 Free-energy and the brain Synthese 159(3), 417–
458
[23] Ramstead,M.J.D.; Friston,K.J.; Hipo´lito,I.2020Isthefreeenergyprincipleaformal
theory of semantics? From variational density dynamics to neural and phenotypic
representations. Entropy 22, 889.
[24] Friston, K. J. 2019 A free energy principle for a particular physics. Preprint
arXiv:1906.10184 [q-bio.NC]. https://arxiv.org/abs/1906.10184
[25] Varshney, L. R.; Chen, B. L.; Paniagua, E.; Hall, D. H.; Chklovskii, D. B. 2011
Structural properties of the Caenorhabditis elegans neuronal network. PLoS Comp.
Biol. 7, e1001066.
[26] Herculano-Houzel, S. 2011 Scaling of brain metabolism with a fixed energy budget
per neuron: Implications for neuronal activity, plasticity and evolution. PLoS One 6,
e17514.
[27] Friston, K.; Levin, M.; Sengupta, B.; Pezzulo, G. 2015 Knowing one’s place: A free-
energy approach to pattern regulation. J. R. Soc. Interface 12, 20141383.
[28] Kuchling, F.; Friston, K.; Georgiev, G.; Levin, M. 2020 Morphogenesis as Bayesian
inference: A variational approach to pattern formation and control in complex bio-
logical systems. Phys. Life Rev. 33, 88–108.
[29] Kiebel, S. J., Friston, K, J. 2011 Free energy and dendritic self-organization. Frontiers
in Systems Neuroscience 5, 80 (13 pp).
[30] BastosAM;UsreyWM;AdamsRA;MangunGR;FriesP;FristonKJ.2012Canonical
microcircuits for predictive coding. Neuron 76, 695–711.
[31] Shipp, S., Adams, R. A., Friston, K. J. (2013). Reflections on agranular architecture:
Predictive coding in the motor cortex. Trends in Neuroscience 36, 706–716.
[32] Kanai, R., Komura, Y., Shipp, S., Friston, K. (2015). Cerebral hierarchies: Predictive
processing,precisionandthepulvinar.Philosophical Transactions of the Royal Society
B 370, 20140169.
[33] Adams, R.A., Friston, K.J., Bastos, A.M.(2015).Activeinference, predictivecoding
and cortical architecture. In M. F. Casanova, I. Opris (Eds.), Recent advances in the
modular organization of the cortex (pp. 97–121). Berlin: Springer.
[34] Clark A. 2013 Whatever next? Predictive brains, situated agents, and the future of
cognitive science. Behav Brain Sci 36, 181–204.
31
[35] Hohwy J. 2013 The predictive mind. Oxford University Press, Oxford, UK.
[36] Seth AK. 2013 Interoceptive inference, emotion, and the embodied self. Trends Cogn
Sci 17(11), 565–573.
[37] Friston KJ, Rigoli F, Ognibene D, Mathys C, FitzGerald T, Pezzulo G. 2015 Active
inference and epistemic value. Cognit Neurosci 6, 187–214.
[38] Aharonov, Y.; Kaufherr, T. 1984 Quantum frames of reference. Phys. Rev. D 30,
368–385.
[39] Bartlett, S.D.; Rudolph, T.; Spekkens, R.W. 2007 Reference frames, superselection
rules, and quantum information. Rev. Mod. Phys. 79, 555–609.
[40] Fields, C.; Marciano`, A. 2019 Sharing nonfungible information requires shared non-
fungible information. Quant. Rep. 1, 252–259.
[41] Barwise, J.; Seligman, J. 1997 Information Flow: The Logic of Distributed Systems
(CambridgeTractsinTheoreticalComputerScience44).CambridgeUniversityPress,
Cambridge, UK.
[42] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory I:
Category-theoretic concepts and tools. J. Expt. Theor. Artif. intell. 31, 177–213.
[43] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory II:
Applications to object identification and mereological complexity. J. Expt. Theor.
Artif. intell. 31, 237–265.
[44] Fields, C.; Glazebrook, J. F. 2020 Do Process-1 simulations generate the epistemic
feelings that drive Process-2 decision making? Cogn. Proc. 21, 533–553.
[45] Fields, C.; Glazebrook, J. F. 2021 Information flow in context-dependent
hierarchical Bayesian inference. J. Expt. Theor. Artif. intell. in press (doi:
10.1080/0952813X.2020.1836034).
[46] Fields, C.; Marcian`o, A. 2019 Holographic screens are classical information channels.
Quant. Rep. 2, 326–336.
[47] Fields, C.; Glazebrook, J. F. 2020 Representing measurement as a thermodynamic
symmetry breaking. Symmetry 12, 810.
[48] Fields, C.; Glazebrook, J. F.; Marciano`, A. 2021 Reference frame induced symmetry
breaking on holographic screens. Symmetry 13, 408.
[49] Fields, C; Levin, M. 2020 How do living systems create meaning? Philosophies 5, 36.
[50] Fields, C.; Glazebrook, J. F.; Levin, M. 2021 Minimal physicalism as a scale-free
substrate for cognition and consciousness. Neurosci. Cons. 7(2), niab013.
32
[51] Schr¨odinger, E. 1944 What is Life? Cambridge, UK: Cambridge University Press.
[52] Hameroff, S.; Penrose, R. 1996 Orchestrated reduction of quantum coherence in brain
microtubules: A model for consciousness Math. Comput. Simul. 40, 453–480. (doi:
10.1016/0378-4754(96)80476-9)
[53] Bordonaro, B.; Ogryzko, V. 2013 Quantum biology at the cellular
level – Elements of the research program. BioSystems 112, 11–30. (doi:
10.1016/j.biosystems.2013.02.008)
[54] Tononi, G.; Koch, C. 2015 Consciousness here, there and everywhere? Philos. Trans.
R. Soc. B 215, 216–242.
[55] Georgiev, D.D. 2020 Quantum information theoretic approach to the mind-brain
problem. Prog. Biophys. Mol. Biol. 18, 16–32.
[56] Arndt M. T.; Juffmann, T.; Vedral, V. 2009 Quantum physics meets biology. HFSP
J. 3, 386–400. (doi: 10.2976/1.3244985)
[57] Lambert, N.; Chen, Y.-N.; Cheng, Y.-C.; Li C.-M.; Chen, G.-Y.; Nori, F. 2012
Quantum biology. Nat. Phys. 9, 10–18. (doi: 10.1038/NPHYS2474)
[58] Melkikh, A. V.; Khrennikov, A. 2015 Nontrivial quantum and quantum-like effects
in biosystems: Unsolved questions and paradoxes. Prog. Biophys. Mol. Biol. 119,
137–161. (doi: 10.1016/j.pbiomolbio.2015.07.001)
[59] Marais, A. et al. 2018 The future of quantum biology. J. R. Soc. Interface 15,
20180640. (doi: 10.1098/rsif.2018.0640)
[60] Cao, J. et al. 2020 Quantum biology revisited. Science Adv. 6 eaaz4888 (doi:
10.1126/sciadv.aaz4888)
[61] Brookes J. C. 2017 Quantum effects in biology: Golden rule in enzymes, olfaction,
photosynthesis and magnetodetection. Proc. R. Soc. A 473, 20160822.
[62] McFadden J, Al-Khalili J. 2018 The origins of quantum biology. Proc. R. Soc. A 474,
20180674.
[63] Fields, C.; Levin, M. 2021 Metabolic limits on classical information processing by
biological cells. BioSystems 209, 104513.
[64] Addazi, A.; Chen, P.; Fabrocini, F.; Fields, C.; Greco, E.; Lulli, M.; Marcian`o, A.;
Pasechnik, R. 2021 Generalized holographic principle, gauge invariance and the emer-
gence of gravity a` la Wilczek. Front. Astron. Space Sci. 8, 563450. (doi: 10.3389/fs-
pas.2021.563450)
[65] Landauer, R. 1961 Irreversibility and heat generation in the computing process. IBM
J. Res. Dev. 5, 183–195.
33
[66] Landauer, R. 1999 Information is a physical entity. Physica A 263, 63–67.
[67] Bennett, C.H. 1982 The thermodynamics of computation. Int. J. Theor. Phys. 21,
905–940.
[68] Fuchs, C. A.; Schack, R. 2013 Quantum-Bayesian coherence. Rev. Mod. Phys. 85,
1693–1715.
[69] Mermin, N. D. 2019 Making better sense of quantum mechanics. Rep. Preg. Phys. 82,
012002.
[70] Rovelli, C. 1996 Relational quantum mechanics. Int. J. Mod. Phys. 35, 1637–1678.
[71] Pearl, J. 1988 Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo CA: Morgan Kaufmann.
[72] Clark A. 2017 How to knit your own Markov blanket: Resisting the second law with
metamorphic minds. In (T. Wetzinger and W. Wiese, eds.) Philosophy and Predictive
Processing 3, 19pp. Frankfurt am Mainz Mind Group.
[73] Fields, C.; Marcian`o, A. 2020 Markov blankets are general physical interaction sur-
faces. Phys. Life Rev. 33, 109–111.
[74] Bell, J. S. 1964 On the Einstein–Podolsky–Rosen paradox. Physics 1, 195–200.
[75] Emary, C.; Lambert, N.; Nori, F. 2014 Leggett–Garg inequalities. Rep. Prog. Phys.
77, 039501.
[76] Kochen, S.; Specker, E.P. 1967 The problem of hidden variables in quantum mechan-
ics. J. Math. Mech. 17, 59–87.
[77] Mermin, D. 1993 Hidden variables and the two theorems of John Bell. Rev. Mod.
Phys. 65, 803–815.
[78] Dzharfarov, E.N.; Kon, M. 2018 On universality of classical probability with contex-
tually labeled random variables. J. Math. Psych. 85, 17–24.
[79] Horsman C; Stepney S; Wagner RC; Kendon V. 2014 When does a physical system
compute? Proc. R. Soc. A 470, 20140182.
[80] Smith, J. E.; Nair, R. 2005 The architecture of virtual machines. IEEE Comp. 38(5),
32–38.
[81] Barr, M. 1979 *-Autonomous Categories, with an Appendix by Po Hsiang Chu; Lec-
ture Notes in Mathematics 752; Springer: Berlin, Germany.
[82] Pratt, V. 1999 Chu spaces. In School on Category Theory and Applications (Coimbra
1999); Volume 21 of Textos Mat. S´er. B; University of Coimbra: Coimbra, Portugal,
pp. 39–100.
34
[83] Pratt, V. 1999 Chu spaces from the representational viewpoint. Ann. Pure Appl. Log.
96, 319–333.
[84] Baars,B.J.;Franklin,S.2003Howconsciousexperienceandworkingmemoryinteract.
Trends in Cognitive Science 7, 166–172.
[85] Dehaene, S.; Naccache, L. 2001 Towards a cognitive neuroscience of consciousness:
Basic evidence and a workspace framework. Cognition 79, 1–37.
[86] Mashour, G. A.;Roelfsema, P.; Changeux, J.-P.; Dehaene, S. 2020 Conscious process-
ing and the Global Neuronal Workspace hypothesis. Neuron 105, 776–798.
[87] Allwein, G.; Moskowitz, I.S.; Chang, L.-W. 2004 A New Framework for Shannon
Information Theory. Technical Report A801024 Naval Research Laboratory, Wash-
ington, DC, USA, 17p.
[88] Barwise,J.1997Informationandimpossibilities.Notre Dame Journal of Formal Logic
38(4), 488–515.
[89] Cherniak, E. 1991 Bayesian networks without tears. AI Magazine 12(4), 50–63.
[90] Prakash, C.; Fields, C.; Hoffman, D. D.; Prentner, R.; Singh, M. 2020 Fact, fiction,
and fitness. Entropy 22, 514.
[91] Fields C, Friston K, Glazebrook JF, Levin M 2021 A free energy principle for generic
quantum systems. In review (available as arXiv:2112.15242 [quant-ph]).
[92] Lyon, P. 2015 The cognitive cell: Bacterial behavior reconsidered. Front. Microbiol.
6, 264.
[93] Micali, G.; Endres, R.G. 2016 Bacterial chemotaxis: Information processing, thermo-
dynamics, and behavior. Curr. Opin. Microbiol. 30, 8–15.
[94] Levin,M.2012Morphogeneticfieldsinembryogenesis,regeneration,andcancer: Non-
local control of complex patterning. Biosystems 109, 243–261.
[95] Levin, M.; Pezzulo, G.; Finkelstein, J. M. 2017 Endogenous bioelectric signaling
networks: Exploiting voltage gradients for control of growth and form. Annu. Rev.
Biomed. Eng. 19, 353–387.
[96] Srivastava, P.; Kane, A.; Harrison, C.; Levin, M. 2021 A meta-analysis of bioelectric
data in cancer, embryogenesis, and regeneration. Bioelectricity 3, 42–67.
[97] Wuichet, K.; Cantwell, B. J.; Zhulin, I. B. 2010 Evolution and phyletic distribution
of two-component signal transduction systems. Curr. Opin. Microbiol. 13, 219–225.
[98] Basieva, I; Khrennikov, A; Ohya, M; Yamato, O. 2011 Quantum-like interference
effect in gene expression: glucose-lactose destructive interference. Syst. Synth. Biol.
5, 59–68.
35
[99] Inada, T;Kimata, K;Aiba, H.1996Mechanismresponsibleforglucose-lactosediauxie
in Eschericha coli challenge to the cAMP model. Genes Cell 1, 293–301.
[100] Loh, K. M.; van Amerongen, R.; Nusse, R. 2016 Generating cellular diversity and
spatial form: Wnt signaling and the evolution of multicellular animals. Dev. Cell. 38,
643–655.
[101] Kolch, W.2005CoordinatingERK/MAPKsignalingthroughscaffoldsandinhibitors.
Nat. Rev. Mol. Cell Biol. 6, 827–838.
[102] Schwanbeck, R.; Martini, S.; Bernoth, K.; Just, U. 2011 The notch signaling pathway:
molecular basis of cell context dependency. Eur Cell Biol 90(6-7), 572–581.
[103] Guo, X.; Wang, X.-F. 2009 Signaling cross-talk between TGF-β/BMP and other
pathways. Cell Res. 19, 71–88.
[104] Hunter, T. 2000 Signaling and beyond. Cell 100, 113–127.
[105] Adamska, M.2015Developmentalsignallingandemergenceofanimalmulticellularity.
In: Evolutionary Transitions to Multicellular Life, edited by Ruiz-Trillo I, Nedelcu
AM. Dordrecht: Springer, pp. 425–450
[106] Fields, C.; Levin, M. 2018 Multiscale memory and bioelectric error correction in the
cytoplasm-cytoskeleton-membrane system. WIRES Syst. Biol. Med. 10, e1410.
[107] Hip´olito, I; Ramstead, M. J. D.; Convertino, L.; Bhat. A.; Friston, K.; Parr, T. 2021
Markov blankets in the brain. Neuroscience and Biobehavioral Reviews 125, 88–97.
[108] Palacios, E. R.; Razi, A.; Parr, T.; Kirchoff, M.; Friston, K. 2020 On Markov blankets
and hierarchical self-organization. J. Theoretical Biology 486, 110089.
[109] Peters, A.; McEwen, B. S.; Friston, K. 2017 Uncertainty and stress: why it causes
diseases and how it can be mastered by the brain. Progress in Neurobiology 156,
164–188.
[110] Spruston, N. 2008 Pyramidal neurons: Dendritic structure and synaptic integration.
Nat. Rev. Neurosci. 9, 206–221.
[111] Rasia-Filho, A. A.; Guerra, K. T. K.; Va´squez, C. E.; Dall’Oglio, A.; Reberger,
R.; Jung, C. R.; Calcagnotto, M. E. 2021 The subcortical-allocortical-neocortical
continuum for the emergence and morphological heterogeneity of pyramidal neurons
in the human brain. Front. Synapt. Neurosci. 13, 616607.
[112] Lloyd, S. 2000 Ultimate physical limits to computation. Nature 406, 1047–1054.
[113] Attwell, D.; Laughlin S. B. 2001 An energy budget for signaling in the grey matter
of the brain. J. Cereb. Blood Flow Metab. 21, 1133–1145
36
[114] Sengupta, B.; Stemmler, M. B.; Friston, K. J. 2013 Information and efficiency in the
nervous system: A synthesis. PLoS Comput Biol 9(7), e1003157.
[115] Georgiev, D; Kolev, S; Cohen, E; Glazebrook, JF. 2020 Computational capacity of
pyramidal neurons in the cerebral cortex. Brain Research 1748, 147069.
[116] Deutsch, D. 2002 The structure of the multiverse. Proc. R. Soc. A 458, 2911–2923.
`
[117] Atiyah, M. 1988 Topological quantum field theory. Pub. Math. IHES 68, 175–186.
[118] Marcian`o, A.; Chen, D.; Fabrocini, F.; Fields, C.; Greco, E.; Gresnigt, N.; Jinklub,
K.; Lulli,M.,Terzidis,K.; Zappala,E.2021Deepneuralnetworksasthesemi-classical
limit of quantum neural networks. Preprint arXiv:2007.00142v2 [cond-mat.diss-nn].
https://arxiv.org/abs/2007.00142
[119] Eyal, G.; Verhoog, M. B.; Testa-Silva, G.; Deitcher, Y.; Benavides-Piccione, R.;
DeFelipe, J.; de Kock, C. P.J.; Mansvelder, H. D.; Segev, I. 2018 Human cortical
pyramidal neurons:from spines to spikes via models. Front. Cellular Neurosci. 12,
181.
[120] Galloni, A. R.; Laffere, A.; Rancz, E. 2020 Apical length governs computational
diversity of layer 5 pyramidal neurons. eLife 9, e55761.
[121] Major, G; Larkum, M. E.; Schiller, J. 2013 Active properties of neocortical pyramidal
neuron dendrites. Annu. Rev. Neurosci. 36, 1–24.
[122] Shipp, S. 2007 Structure and function of the cerebral cortex. Curr. Biol. 17, R443–
R449.
[123] Shipp, S. 2016 Neural elements for predictive coding. Front. Psychol. 7, 1792.
[124] Swanson, L. W.; Lichtman, J. W. 2016 From Cajal to connectome and beyond. Annu.
Rev. Neurosci. 39, 197–216.
[125] V´elez-Fort, M.; Rousseau, C. V.; Niedworok, C. J.; Wickersham, I. R.; Rancz, E.
A.; Brown, A. P. Y.; Strom, M.; Margrie, T. W. 2014 The stimulus selectivity and
connectivityofLayerSixprincipalcellsrevealscorticalmicrocircuitsunderlyingvisual
processing. Neuron 83, 1431–1443.
[126] De Nardo, L. A.; Berns, D. C.; DeLoach, K.; Luo, L. 2015 Connectivity of mouse
somatosensory and prefrontal cortex examined with trans-synaptic tracing. Nat. Neu-
rosci. 18, 1687–1697.
[127] Harris, K. D.; Shepherd, G. M. G. 2015 The neocortical circuit: Themes and varia-
tions. Nat. Neurosci. 18, 170–181.
[128] Mennerick S, Zorumsky CF. 2000 Neural activity and survival in the developing
nervous system. Mol. Neurobiol. 22, 41–54.
37
[129] Faust TE, Gunner G, Schafer DP. 2021 Mechanisms governing activity-dependent
synaptic pruning in the developing mammalian CNS. Nature Rev. Neurosci. 22, 657–
673.
[130] DeitcherY.; EyalG.; KanariL.; etal.2007Comprehensivemorpho-electronicanalysis
shows 2 distinct classes of L2 and L3 pyramidal neurons in human temporal cortex.
Cereb. Cortex 27, 5398–5414.
[131] Mohan, H.; Verhoog, M. B.; Doreswamy, K. K.; et al. 2015 Dendritic and axonal
architecture of individual pyramidal neurons across layers of adult human neocortex.
Cereb. Cortex 25, 4839–4853.
[132] Beaulieu-Laroche, L; Toloza, E. H. S.; van der Goes, M. S. et al. 2018 Enhanced
dendritic compartmentalization in human cortical neurons. Cell 175, 643–651.
[133] Nielsen, M.A.; Chuang, I.L.2000Quantum Computation and Quantum Information.
New York, Cambridge University Press.
[134] Luo, L. 2021 Architectures of neuronal circuits. Science 373, eabg7285.
[135] Johansson, C.; Lansner, A.2007Towardscortexsizedartificialneuralsystems.Neural
Networks 20, 48–61.
[136] Chen, R.; Cohen, L. G.; Hallett, M. 2002 Nervous system reorganization following
injury. Neuroscience 111, 761–773.
[137] Demekas, D.; Parr, T.; Friston, K. J. 2020 An ivestigation of the free energy principle
for emotional recognition. Frontiers in Computational Neuroscience 14, 30.
[138] Levin, M.2019Thecomputationalboundaryofa“self”: Developmentalbioelectricity
drives multicellularity and scale-free cognition. Front. Psychol. 10, 1688.
[139] Levin, M. 2014 Endogenous bioelectrical networks store non-genetic patterning infor-
mation during development and regeneration. J. Physiol. 592, 2295–2305.
[140] Levin, M. 2021 Bioelectric signaling: Reprogrammable circuits underlying embryoge-
nesis, regeneration, and cancer. Cell 184, 1971–1989.
[141] Bates, E. 2015 Ion channels in development and cancer. Annu. Rev. Cell. Devel. Biol.
31, 231–247.
[142] Harris, M. P. 2021 Bioelectric signaling as a unique regulator of development and
regeneration. Development 148, dev180794.
[143] Mathews, J.; Levin, M.2018Thebodyelectric2.0: Recentadvancesindevelopmental
bioelectricity for regenerative and synthetic bioengineering. Curr. Opin. Biotechnol.
52, 134–144.
38
[144] Levin, M. 2021 Life, death, and self: Fundamental questions of primitive cognition
viewedthroughthelensofbodyplasticityandsyntheticorganisms.Biochem. Biophys.
Res. Commun. 564, 114–133.
[145] Vandenberg, L. N.; Adams, D. S.; Levin, M. 2012 Normalized shape and location of
perturbed craniofacial structures in the Xenopus tadpole reveal an innate ability to
achieve correct morphology. Devel. Dyn. 241, 863–878.
[146] Pinet, K.; McLaughlin, K. A. 2019 Mechanisms of physiological tissue remodeling
in animals: Manipulating tissue, organ, and organism morphology. Devel. Biol 451,
134–145.
[147] Pinet, K.; Deolankar, M.; Leung, B.; McLaughlin, K. A. 2019 Adaptive correction
of craniofacial defects in pre-metamorphic Xenopus laevis tadpoles involves thyroid
hormone-independent tissue remodeling. Development 146, dev175893.
[148] Emmons-Bell, M.; Durant, F.; Hammelman, J.; Bessonov, M.; Volpert, V.; Mo-
rokuma, J.; Pinet, K.; Adams, D. S.; Pietak, A.; Lobo, D.; Levin, M. 2015 Gap
junctional blockade stochastically induces different species-specific head anatomies in
genetically wild-type Girardia dorotocephala flatworms. Int. J. Mol. Sci. 16, 27865–
27896.
[149] Kriegman, S.; Blackiston, D.; Levin, M.; Bongard, J. 2020 A scalable pipeline for
designing reconfigurable organisms. Proc. Natl. Acad. Sci. USA 117, 1853–1859.
[150] Blackiston, D.; Lederer, E.; Kriegman, S.; Garnier, S.; Bongard, J.; Levin, M. 2021
A cellular platform for the development of synthetic living machines. Sci. Robot. 6,
eabf1571.
[151] Pai, V. P.; Pietak, A.; Willocq, V.; Ye, B.; Shi, N.-Q.; Levin, M. 2018 HCN2 Rescues
brain defects by enforcing endogenous voltage pre-patterns. Nat. Comms. 9, 998.
[152] Pai, V. P.; Lemire, J. M.; Par´e, J.-F.; Lin, G.; Chen,Y.; Levin, M. 2015 Endogenous
gradients of resting potential instructively pattern embryonic neural tissue via Notch
signaling and regulation of proliferation. J. Neurosci 35, 4366–4385.
[153] Vandenberg, L. N.; Morrie, R. D.; Adams, D. S. 2011 V-ATPase-dependent ecto-
dermal voltage and pH regionalization are required for craniofacial morphogenesis.
Devel. Dyn. 240, 1889–1904.
[154] Pezzulo, G.; Lapalme, J.; Durant, F.; Levin, M. 2021 Bistability of somatic pattern
memories: Stochastic outcomes in bioelectric circuits underlying regeneration. Phil.
Proc. R. Soc. B 376, 20190765.
[155] Durant F.; Morokuma, J.; Fields, C.; Williams, K.; Adams, D. S.; Levin, M. 2017
Long-term, stochastic editing of regenerative anatomy via targeting endogenous bio-
electric gradients. Biophys. J. 112, 2231–2243.
39
[156] Oviedo, N. J.; Morokuma, J.; Walentek, P.; Kema, I. P.; Gu, M. B.; Ahn, J.-M.;
Hwang, J. S.; Gojobori, T.; Levin, M. 2010 Long-range neural and gap junction
protein-mediated cues control polarity during planarian regeneration. Devel. Biol.
339, 188–199.
[157] Durant, F.; Bischof, J.; Fields, C.; Morokuma, J.; LaPalme, J.; Hoi, A.; Levin,
M. 2019 The role of early bioelectric signals in the regeneration of planarian ante-
rior/posterior polarity. Biophys. J. 116, 948–961.
[158] Koshland, D. E. 1983 The bacterium as a model neuron. Trends Neurosci 6, 133–137.
[159] Prindle, A.; Liu, J.; Asally, M.; Ly, S.;Garcia-Ojalvo, J.; Su¨el, G.M.2015Ionchannels
enable electrical communication in bacterial communities. Nature 527, 59–63.
[160] Pezzulo, G.; Levin, M. 2015 Re-membering the body: Applications of computational
neuroscience to the top-down control of regeneration of limbs and other complex
organs. Integr. Biol. (Cambridge) 7, 1487–1517.
[161] Watson, R. A.; Buckley, C. L.; Mills, R.; Davies, A. 2010 In Artificial Life Conference
XII. (Odense, Denmark, 2010), pp. 194–201.
[162] Biswas, S.; Manicka, S.; Hoel, E.; Levin, M. 2021 Gene regulatory networks exhibit
several kinds of memory: Quantification of memory in biological and random tran-
scriptional networks. iScience 24, 102131.
[163] Emmons-Bell, M., Durant, F.; Tung, A.; Pietak, A.; Miller, K.; Kane, A.; Martyniuk,
C. J.; Davidian, D.; Morokuma, J.; Levin, M. 2019 Regenerative adaptation to elec-
trochemical perturbation in planaria: A molecular analysis of physiological plasticity.
iScience 22, 147–165.
[164] Jacob,E.B.; Aharonov,Y.; Shapira,Y.2004Bacteriaharnessingcomplexity.Biofilms
1, 239–263.
[165] Goel, P.; Mehta, A. 2013 Learning theories reveal loss of pancreatic electrical connec-
tivity in diabetes as an adaptive response. PLoS One 8, e70366.
[166] Zoghi, M. 2004 Cardiac memory: Do the heart and the brain remember the same? J.
Interv. Card. Electrophysiol. 11, 177–182.
[167] Chakravarthy, S. V.; Ghosh, J. 1997 On Hebbian-like adaptation in heart muscle: A
proposal for ‘cardiac memory’. Biol. Cybern. 76, 207–215.
40

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Neurons as hierarchies of quantum reference frames"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
