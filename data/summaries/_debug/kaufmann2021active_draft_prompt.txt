=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: An active inference model of collective intelligence
Citation Key: kaufmann2021active
Authors: Rafael Kaufmann, Pranav Gupta, Jacob Taylor

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2021

Key Terms: system, collective, agents, university, alignment, cognitive, inference, active, scale, model

=== FULL PAPER TEXT ===

An Active Inference Model of Collective Intelligence
Rafael Kaufmann1, Pranav Gupta2, and Jacob Taylor 3,4*
1 Independent researcher
2 Tepper School of Business, Carnegie Mellon University
3 Institute of Cognitive & Evolutionary Anthropology, University of Oxford
4 Crawford School of Public Policy, Australian National University
* Correspondence: jacob.taylor@anthro.ox.ac.uk
Abstract: Collective intelligence, an emergent phenomenon in which a composite system of multiple
interacting agents performs at levels greater than the sum of its parts, has long compelled research
efforts in social and behavioral sciences. To date, however, formal models of collective intelligence
have lacked a plausible mathematical description of the relationship between local-scale interactions
between highly autonomous sub-system components (individuals) and global-scale behavior of the
composite system (the collective). In this paper we use the Active Inference Formulation (AIF), a
framework for explaining the behavior of any non-equilibrium steady state system at any scale, to
posit a minimal agent-based model that simulates the relationship between local individual-level
interaction and collective intelligence (operationalized as system-level performance). We explore
the effects of providing baseline AIF agents (Model 1) with specific cognitive capabilities: Theory of
Mind (Model 2); Goal Alignment (Model 3), and Theory of Mind with Goal Alignment (Model 4).
These stepwise transitions in sophistication of cognitive ability are motivated by the types of
advancements plausibly required for an AIF agent to persist and flourish in an environment
populated by other AIF agents, and have also recently been shown to map naturally to canonical
steps in human cognitive ability. Illustrative results show that stepwise cognitive transitions
increase system performance by providing complementary mechanisms for alignment between
agents' local and global optima. Alignment emerges endogenously from the dynamics of interacting
AIF agents themselves, rather than being imposed exogenously by incentives to agents' behaviors
(contra existing computational models of collective intelligence) or top-down priors for collective
behavior (contra existing multiscale simulations of AIF). These results shed light on the types of
generic information-theoretic patterns conducive to collective intelligence in human and other
complex adaptive systems.
Keywords: Collective Intelligence; Free Energy Principle; Active Inference; Agent-Based Model;
Complex Adaptive Systems; Multiscale Systems; Computational Model
1. Introduction
Subjectively, we perceive ourselves to be autonomous individuals at the same time
that we actively participate in collectives. Families, organizations, sports teams, and
polities exert agency over our individual behavior [1,2] and are even capable, under
certain conditions, of intelligence that cannot be explained by aggregation of individual
intelligence [3,4]. To date, however, formal models of collective intelligence have lacked a
plausible mathematical description of the functional relationship between individual and
collective behavior.
In this paper, we use the Active Inference Formulation (AIF), a process theory of the
Free Energy Principle (FEP), to develop a clearer understanding of the relationship
between patterns of individual interaction and collective intelligence. We adopt a
definition of collective intelligence established within organizational psychology, as
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 2 of 33
groups of individuals capable of acting collectively in ways that seem intelligent and that
cannot be explained by individual intelligence [5] (p.3) [3]. In more general terms,
collective intelligence is an emergent phenomenon in which a composite system of
multiple interacting agents performs at levels greater than the sum of its parts. We have a
particular interest in elucidating mechanisms and dynamics that explicate human
collective intelligence, but the universality of our formal computational approach makes
it relevant to collective intelligence in any complex adaptive system.
We suggest that a formal account of collective intelligence must crucially entail a
testable framework for explaining relationships between behavior across multiple
scalesâ€”e.g., between individual interaction and collective performance. Existing accounts
of collective intelligence, particularly those focused on human collective intelligence, are
marred precisely by a lack of alignment between individual and collective scales of
analysis. Accounts of local-scale interactions tend to construe individuals as 1st-person,
goal-directed agents endowed with discrete cognitive mechanisms (specifically social
perceptiveness or Theory of Mind and shared intentionality; see [6,7]) that allow
individuals to establish and maintain adaptive connections with other individuals in
service of shared goals [3â€“5,8â€“10].1 Researchers conjecture that these mechanisms allow
collectives to derive and utilize more performance-relevant information from the
environment than could be derived by an aggregation of the same individuals acting
without such connections (for example, by facilitating an adaptive, system-wide balance
between cognitive efficiency and diversity; see [4]). Empirical substantiation of such
claims has proven difficult, however. Most investigations rely heavily on laboratory-
derived summaries or â€œsnapshotsâ€ of individual and collective behavior that flatten the
complexity of local scale interactions [11] and make it difficult to examine causal
relationships between individual scale mechanisms and collective behavior as they
typically unfold in real world settings [12,13].
Accounts of global-scale (collective) behavior, by contrast, tend to adopt system-
based (rather than agent-based) perspectives that render collectives as random dynamical
systems in phase space [14â€“17]. Only rarely deployed to assess the construct of human
collective intelligence specifically (e.g., [18]), these approaches have been fruitful for
identifying the types of phase-space dynamics (such as synchrony, metastability, or
symmetry breaking) that correlate with collective performance more generally construed
[19â€“23]. However, on their own, such analyses are limited in their ability to generate
testable predictions for multiscale behavior, such as how global-scale dynamics (rendered
in phase-space) translate to specific local-scale interactions (in state-space), or how local-
scale interactions between individuals translate to evolution and change in global-scale
dynamics [17].
In sum, the substantive differences between these two analytical perspectives
(individual and collective) on collective intelligence make it difficult to formulate a formal
description of how details of local-scale interactions between individuals relate to global-
scale collective behavior and vice versa. Most urgent for the development of a formal
model of collective intelligence, therefore, is a common mathematical framework capable
1 Reidl and colleagues [10] report a recent analysis of 1356 groups that found social perceptiveness and group interaction processes to be strong predictors of collective
intelligence measured by a psychometric test.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 3 of 33
of operating between individual-level cognitive mechanisms and system-level dynamics
of the collective [4].
1.1 The Free Energy Principle and an Active Inference Formulation of Collective Intelligence
FEP has recently emerged as a candidate for this type of common mathematical
framework for multiscale behavioral processes [24â€“26]. FEP states that any non-
equilibrium steady state system self organizes as such by minimizing variational free
energy in its exchanges with the environment [27]. The key trick of FEP is that the
principle of free energy minimization can be neatly translated into an agent-based process
theory, AIF, of approximate Bayesian inference [28] and applied to any self-organizing
biological system at any scale [29]. The upshot is that, in theory, any AIF agent at one
spatio-temporal scale could be simultaneously composed of nested AIF agents at the scale
below, and a constituent of a larger AIF agent at the scale above it [30â€“32]. In effect, AIF
allows you to pick a composite agent A that you want to understand, and it will be
generally true both that: A is an approximate, global minimizer of free energy at the scale
at which that agent reliably persists; and A is composed of subsystems {A_i} that are
approximate, local minimizers of free energy (which is composed of the remainder of A).
Thus, supposing that human individuals and the collectives they constitute both
minimally satisfy the necessary assumptions of FEP [pertaining to (a) ergodic and (b) non
equilibrium steady state systems; see 24], collective intelligence could conceivably be
modelled as a case of individual AIF agents that interact withinâ€”or indeed, interact to
produceâ€”a superordinate AIF agent at the scale of the collective [33,34]. AIF thus
provides a framework within which an agent-based model of collective intelligence could
be developed.
An AIF model of collective intelligence begins with the depiction of a minimal AIF
agent. Specifically, an AIF agent denotes any set of states enclosed by a â€œMarkov
blanketâ€â€”a statistical partition between a systemâ€™s internal states and external states
[35]â€”that infers beliefs about the causes of (hidden) external states by developing a
probabilistic generative model of external states [27]. A Markov blanket is composed of
sensory states and active states that mediate the relationship between a systemâ€™s internal
states and external states: external states (ğœ“) act on sensory states (ğ‘ ), which influence, but
are not influenced by internal states (ğ‘). Internal states couple back through active states
(ğ‘), which influence but are not influenced by external states. Through conjugated
repertoires of perception and action, the agent embodies and refines (learns) a generative
model of its environment [36] and the environment embodies and refines its model of the
agent (akin to a circular process of environmental niche construction; see [37]).
Having established the notion of an agent, the next step is to consider the existence
of multiple nested AIF agents across multiple scales of behavioral organization, beginning
with individual and collective scales. Existing multiscale treatments of AIF provide a clear
account of â€œdownward reachingâ€ causation of multiscale biological systems, whereby
superordinate AIF agents systematically determine (or â€œenslave;â€ see [38]) the behavior
of subordinate AIF agents [30,33,39]. For instance, while cells or neurons could be
modelled as free energy minimizing agents in their own right, the behavioral degrees of
freedom available to these individual agents as members of a superordinate entity (a
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 4 of 33
multicellular organism or a brain) are fundamentally constrained by the dynamics of the
superordinate entity [40].
While useful for depicting the behavior of neurons within brains or cells within
multicellular organisms, this general account of multiscale AIF is yet to be specified for
modelling other behaviors of interest, including intelligent behavior of human collectives
[34,41]. Human interaction unfolds between highly autonomous individuals whose
statistical boundaries for self-evidencing are often transient, distributed, and multiple [42â€“
45]. Unlike cells or neurons, which rely on simple autoregulatory mechanisms to sustain
participation in collective ensembles, human agents participate in collectives by
leveraging an array of phylogenetic (evolutionarily) and ontogenetic (developmental)
mechanisms, and socio-culturally constructed regularities or affordances [37,46,47].
Human agents' cognitive abilities and sociocultural niches create avenues for active
participation in functional collective behavior (e.g., the pursuit of shared goals), as well as
avenues to shirk global constraints in the pursuit of local (individual) goals.
Existing toy models that simulate the emergence of collective behavior under AIF
do so by simply using the statistical constraints from one scale to drive behavior at
another, either explicitly(e.g., by endowing AIF agents with a genetic prior for functional
specialization within a superordinate system; see [33]) or by constructing a scenario in
which a superordinate agent (or shared generative model) is predestined by virtue of the
fact that agentsâ€™ environment is constituted only by the sensory generated by each other
[48,49]. While these scenarios successfully depict the formation and persistence of
collectives (as a function of local-scale interactions between individual AIF agents), and
while that collective may be capable of intelligence, these dual characteristics depend on
the type and quality of instructions supplied exogenously (or from the â€œtop-downâ€), more
so than from the basic information-theoretic patterns of complex adaptive systems
implied by AIF (and emergent from the â€œbottom-upâ€). In this sense, extant models of AIF
bear a closer resemblance to Searle's [50] "Chinese Room Argument" than to what we
would recognize as emergent collective intelligence.
Currently missing from models of multiscale AIF are specifications for how a
system's emergent cognitive capabilities causally relate to individual agents' emergent
cognitive capabilities, and thus how local-scale interactions between individual AIF
agents give rise, endogenously, to superordinate AIF agents that exhibit behaviors of
interest [34]. Specifically, existing approaches lack a description of the key cognitive
mechanisms that might provide a mechanistic "missing link" for collective intelligence
under AIF. The key puzzle of collective intelligence that we attempt to explain is when
and how intelligent behavior at the individual agent level gives rise to intelligent behavior
at the collective level. Thus, in this paper we ask: what basic information theoretic patterns
of interaction between individual AIF agents create opportunities for collective
intelligence at the global scale?
1.2 Our approach
To operationalize AIF in a way that is useful for investigating this question, we begin
by examining what minimal features of individual AIF agents are required to achieve
collective intelligence. We conjecture that very generic information theoretic patterns of
an environment in which individual AIF agents exploit other AIF agents as affordances
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 5 of 33
of free energy minimization should support the emergence of collective intelligence,
operationalized as active inference (or free energy minimization) at the level of the global-
scale system. Importantly, we expect that these patterns emerge under very general
assumptions and from the dynamics of AIF itselfâ€”without the need for exogenously
imposed fitness or incentive structures on local-scale behavior, contra extant
computational models of collective intelligence (that rely on cost or utility functions; e.g.,
Reia et al., 2019; Krafft, 2019) or other common approaches to reinforcement learning (that
rely on exogenous parameters of the Bellman equation; see [51,52]).
To justify our modelling approach, we draw upon recent research that systematically
maps the complex adaptive learning process of AIF agents to empirical social scientific
evidence for cognitive mechanisms that support adaptive human social behavior. We
posit a series of stepwise progressions or â€œhopsâ€ in the individual cognitive ability of any
AIF agent in an environment populated by other self-similar AIF agents. These hops
represent evolutionarily plausible â€œadaptive priorsâ€ [32] (p.109) that would likely guide
action-perception cycles of AIF agents in a collective toward unsurprising states:
â€¢ Baseline AIF - AIF agents, to persist as such, will minimize immediate free
energy by accurately sensing and acting on salient affordances of the
environment. This will require a general ability for â€œperceptivenessâ€ of the
(physical) environment.
â€¢ Folk Psychology - AIF agents in an environment populated by other AIF agents
would fare better by minimizing free energy not only relative to their physical
environment, but also to the "social environment" composed of their peers [46].
The most parsimonious way for AIF agents to derive information from other
agents would be to (i) assume that other agents are self-similar, or are
â€œcreatures like meâ€ [53], and (ii) differentiate other-generated information by
calculating how it diverges from self-generated information (akin to a process
of â€œalterityâ€ or self-other distinction). This ability aligns with the notion of a
â€œfolk psychological theory of society,â€ in which humans deploy a combination
of phylogenetic and ontogenetic modules to process social information [54,55].
â€¢ Theory of Mind - AIF agents that develop â€œsocial perceptivenessâ€ or an ability
to accurately infer beliefs and intentions of other agents will likely outperform
agents with less social perceptiveness. Social perceptiveness, also commonly
known in cognitive psychology as â€œTheory of Mind,â€ would minimally require
cognitive architecture for encoding the internal belief states of other agents as
a source of self-inference (for game-theoretical simulations of this proposal, see
[56,57]). As discussed above, experimental evidence suggests that social
perceptiveness or Theory of Mind (measured using the â€œReading the Mind in
the Eyesâ€ test; see [58]) is a significant predictor of human collective intelligence
in a range of in-person and on-line collaborative tasks [4].
â€¢ Goal Alignment - It is possible to imagine scenarios in which the effectiveness
of Theory of Mind would be limited, such as situations of high informational
uncertainty (in which other agents hold multiple or unclear goals), or in
environments populated by more agents than would be computationally
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 6 of 33
tractable for a single AIF agent to actively theorize [59]. AIF agents capable of
transitioning from merely encoding internal belief states of other AIF agents to
recognizing shared goals and actively aligning goals with other AIF agents
would likely enjoy considerable coordination benefits and (computational)
efficiencies [7,60] that would also likely translate to collective-level
performance [61,62].
â€¢ Shared Norms - Acquisition of capacities to engage directly with the reified
signal of sharedness (a.k.a., â€œnormsâ€) between agents as a stand-in for (or in
addition to) bottom-up discovery of mutually viable shared goals would also
likely confer efficiencies to individuals and collectives [37]. Humans appear
unique in their ability to leverage densely packaged socio-cultural installed
affordances to cue regimes of perception and action that establish and stabilize
adaptive collective behavior (a process recently described as â€œThinking
Through Other Mindsâ€; see [47]).
The clear resonance between generic information-theoretic patterns of basic AIF
agents and empirical evidence of human social behavior is remarkable, and gives credence
to the extension of seemingly human-specific notions such as "alterity", "shared goals",
â€œalignmentâ€, "intention," and "meaning" to a wider spectrum of bio-cognitive agents [63].
In effect, the universality of FEPâ€”a principle that can be applied to any biological system
at any scaleâ€”makes it possible to strip-down the complex and emergent behavioral
phenomenon of collective intelligence to basic operating mechanisms, and to clearly
inspect how local-scale capabilities of individual AIF agents might enable global-scale
state optimization of a composite system.
In the following section we use AIF to model the relationship between a selection of
these hops in cognitive ability and collective intelligence. We construct a simple 1D search
task [based on 64], in which two AIF agents interact as they pursue individual and shared
goals. We endow AIF agents with two key cognitive abilitiesâ€”Theory of Mind and Goal
Alignmentâ€”and vary these abilities systematically in four simulations that follow a 2x2
(Theory of Mind x Goal Alignment) progression: Model 1 (Baseline AIF, no social
interaction), Model 2 (Theory of Mind without Goal Alignment), Model 3 (Goal
Alignment without Theory of Mind), and Model 4 (Theory of Mind with Goal Alignment).
We use a measure of free energy to operationalize performance at the local (individual)
and global (collective) scales of the system [65]. While our goals in this paper are
exploratory (these models and simulations are designed to be generative, not to test
hypotheses), we do generally expect that increases in sophistication of cognitive abilities
at the level of individual agents will correspond with an increase in local- and global-scale
performance. Indeed, illustrative results of model simulations (Section 3) show that each
hop in cognitive ability improves global system performance, particularly in cases of
alignment between local and global optima.
2. Materials and Methods
2.1 Paradigm and set-up
Our AIF model builds upon the work of McGregor and colleagues, who develop a
minimal AIF agent that behaves in a discrete one-dimensional time world [64]. In this set-
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 7 of 33
up, a single agent senses a chemical concentration in the environment and acts on the
environment by moving one of two ways until it arrives at its desired state, the position
in which it believes the chemical concentration to be highest, denoting a food source. We
adapt this paradigm by modelling two AIF agents (Agent A and Agent B) that occupy the
same world and interact according to parameters described below (see Figure 1). The
McGregor et al. paradigm and AIF model is attractive for its computational
implementability and tractability as a simple AIF agent with minimum viable complexity.
It is also accessible and reproducible; whereas most existing agent-based implementations
of AIF are implemented in MATLAB, using the SPM codebase (e.g., [52]), an
implementation of the McGregor et al. AIF model is widely available in the open-source
programming language Python, using only standard open source numerical computing
libraries [66].
We extend the work of McGregor and colleagues to allow for interactions not only
between an agent and the "physical" environment, but also between an agent and its
"social" environment (i.e., its partner). Accordingly, we make minor simplifications to the
McGregor et al. model that are intended to reduce the number of independent parameters
and make interpretation of phenomena more straightforward (alterations to the
McGregor et al. model are noted throughout).
Agent A Agent B
A B
SHARED
TARGET
Figure 1. A minimal collective system of two AIF agents (adapted from McGregor et al.). We
implement two agents (Agent A and Agent B) that have one common target position (Shared
Target) and one individual target position (Aâ€™s Target; Bâ€™s Target). All targets are encoded with
equal desirability. This figure is notional: our simulation environment contains 60 cells instead of
the 12 depicted here. Note: we randomize the location of the shared target while preserving
relative distances to unshared targets to ensure that the agentsâ€™ behavior is not an artefact of its
location in the sensory environment.
2.2 Conceptual outline of AIF model
Our model consists of two agents. Descriptively, one can think of these as simple
automata, each inhabiting a discrete "cell" in a one-dimensional circular environment
where there are predefined targets (food sources). As agents aren't endowed with a frame
of reference, an agent's main cognitive challenge is to situate itself in the environment (i.e.,
to infer its own position). Both agents have the following capabilities:
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 8 of 33
â€¢ Physical capabilities:
â€¢ "Chemical sensors" able to pick up a 1-bit chemical signal from the food source
at each time step;
â€¢ "Actuators" that allow agents to "move" one cell at each time step;
â€¢ "Position and motion sensors" that allow agents to detect each other's position
and motion.
â€¢ Cognitive capabilities:
â€¢ Beliefs about their own current position; we construe this as a "self-
actualization loop" or Sense->Understand->Act cycle: (1) sense environment;
(2) optimize belief distribution relative to sensory inputs (by minimizing free
energy given by an adequate generative model); and (3) act to reduce FE
relative to desired beliefs, under the same generative model.
â€¢ Desires (also described as "desired beliefs") about their own position relative to
their prescribed target positions;
â€¢ Ability to select the actions that will best "satisfy" their desires;
â€¢ "Theory of Mind": they possess beliefs about their partner's position,
knowledge of their partner's desires, and therefore, the ability to imagine the
actions that their partners are expected to take. We implement this as a
"partner-actualization loop" that is formally identical to the self-actualization
loop above;
â€¢ "Goal Alignment": the ability to alter their own desires to make them more
compatible with their partner's.
2.3 Model preliminaries
Throughout, we use the following shorthand:
â€¢ ğ‘ğ‘ ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ â‰œ ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ğ‘ ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡) for any superscript index, where
ğ‘’ğ‘ğ‘–âˆ’ğ‘šğ‘–ğ‘›(ğ‘)
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘) â‰œ . This converts a belief represented as a vector in â„N
ğ‘– ğ›´ ğ‘’ğ‘ğ‘–âˆ’ğ‘šğ‘–ğ‘›(ğ‘)
to the equivalent probability distribution over [1..N]. ğ‘ğ‘ ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ =
ğ‘™ğ‘› ğ‘ğ‘ ğ‘¢ğ‘ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡+ğ¾ converts back.
â€¢ Beliefs are implicitly constrained to the range ğ‘ğ‘– âˆˆğ=[âˆ’10,0].
â€¢ ğœ‘ â‰œ ğœ“ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿwhen necessary, to disambiguate between it and ğœ“ğ‘œğ‘¤ğ‘›.
â€¢ (ğ‘£ ) â‰œ ğ‘£ to denote shifting a vector.
+ğ‘¥ ğ‘– ğ‘–+ğ‘¥
1âˆ’ğ›¼
â€¢ ğ›© (ğ‘)â‰œ ğ›¼ ğ‘ + to denote "re-ranging" a probability distribution,
ğ›¼ ğ‘
1âˆ’ğ›¼ 1âˆ’ğ›¼
squishing its range from [0, 1] to [ ,ğ›¼âˆ’ ].
ğ‘ ğ‘
â€¢ All arithmetic in the space of positions and actions (ğœ“,ğ‘,ğ›¥) is considered to be
mod N.
2.4 State space
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 9 of 33
These capabilities are implemented as follows. Each agent Ai is represented by a tuple
Ai = (Ïˆi, si, bi, ai). In what follows we'll omit the indices except where there is a relevant
difference between agents. These tuples form the relevant state space (see Figure 2):
â€¢ Ïˆ âˆˆ [0..N-1] is the agent's external state, its position in a circular environment
with period N. Crucially, the agent doesn't have direct access to its external
state, but only to limited information about the environment afforded through
the sensory state below.
â€¢ s = (sown âˆˆ {0, 1}, Î” âˆˆ [0..N-1], app âˆˆ {-1, 0, 1}) is the agent's sensory state. sown is a
one-bit sensory input from the environment; Î” is the perceived difference
between the agent's own position and its partner's; app is the partner's last
action.
â€¢ b = (bown âˆˆ ğN, b*own âˆˆ ğN, bpartner âˆˆ ğN, b*partner âˆˆ ğN) is the agent's internal or
"belief" state. bown and b*own are, respectively, its actual and desired beliefs about
its own position; equivalently, bpartner and b*partner are its actual and desired
beliefs about its partner's position.
â€¢ a = (aown âˆˆ {-1, 0, 1}, apartner âˆˆ {-1, 0, 1}) is the partner's action state: aown is its own
action; apartner is the action it expects from the partner.
Figure 2. AIF agent based on McGregor et al. 2015. A Markov blanket defines conditional
independencies between a set of internal belief states (ğ‘) and a set of environment states (ğœ“) with
target encoding or â€œdesiresâ€ (b*).
2.5 Agent Evolution
These states evolve according to a discrete-time free energy minimization procedure,
extended from McGregor et al. (Figure 3).
procedure simulate(Ïˆ, b, bâˆ—)
loop
sown i â† random value using Pi(sown | Ïˆ)
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 10 of 33
ai â† argmin Fi(b*, b, si, a)
a
bâ€² i â† optimise(bi, si, ai)
Ïˆâ€² â† Ïˆ + a âŠ² Unlike McGregor et al., we assume agents always act as intended
Î” â† (Ïˆ1 - Ïˆ2, Ïˆ2 - Ïˆ1) âŠ² Agents' "position sensors" are assumed to be perfect
app â† (Ïˆ'2 - Ïˆ2, Ïˆ'1 - Ïˆ1) âŠ² Agents' "motion sensors" are assumed to be perfect
end loop
end procedure
function optimize(b, s, a)
bâ€² = b âŠ² starting from previous epoch's belief
for i âˆˆ {1 Â· Â· Â· k} do
bâ€² â† bâ€² âˆ’ Î· Â·âˆ‚/âˆ‚bâ€™ F(bâ€², b, s, a) âŠ² gradient descent on bâ€²
end for
return bâ€²
end function
Figure 3. Pseudo code for agent evolution (adapted from McGregor et al., 2015). Note that the loop
is run for both agents in lockstep, but each agent selects actions and optimizes beliefs individually.
2.6 Sensory model
Let us recapitulate McGregor et. al's definition of the free energy for a single-agent
model:
ğ¹(ğ‘â€²,ğ‘,ğ‘ ,ğ‘) = ğ· ğ¾ğ¿ (ğ‘(ğœ“â€² | ğ‘â€²) â€– ğ‘(ğœ“â€²,ğ‘  | ğ‘,ğ‘)) (1)
where q(b) = softmax(b) is the "variational (probability) densityâ€ encoded by b, and
p(Ïˆ', s | b, a) is the "generative (probability) densityâ€ representing the agent's mental
model of the world (Friston et al. 2006). ğ· is the Kullbackâ€“Leibler (KL) divergence or
ğ¾ğ¿
relative entropy between the variational and generative densities (see Friston et al., 2009).
Evidently, as b is an arbitrary real vector, the optimal b' is the one which produces q = p,
that is, bi = - log(pi) + K.
In order to respect the causal relationships prescribed by the Markov blanket (see
Figure 2), the generative density may be decomposed as:
p(', s | b, a) =P(Ïˆâ€² | s, b, a, Ïˆ) P(s | b, a, Ïˆ) P(Ïˆ | b, a) (2)
where the three terms within the summation are arbitrary functions of their variables.
In the single-agent model, where the only source of information is the environment, we
follow McGregor's model, in a slightly simplified form:
1. ğ‘ƒ(ğœ“â€² | ğ‘ ,ğ‘,ğ‘,ğœ“) =ğ›¿(ğœ“â€²,ğœ“+ğ‘): the agent's actions are always assumed to have the
intended effect, ğ›¿being the discrete Kronecker delta.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 11 of 33
2. ğ‘ƒ(ğ‘  | ğœ“) =ğ‘˜ğ‘ (1âˆ’ğ‘˜)1âˆ’ğ‘  ğ‘’âˆ’ğœ” |ğœ“âˆ’ğœ“ğ‘šğ‘–ğ‘‘|: the agent assumes the probability of s = 1
(sensoria triggered) is higher for regions near the "center" of the environment. This is
identical to the real "physical" probability of chemical signals, meaning the agent's
generative distribution is correct.
3. ğ‘ƒ(ğœ“ | ğ‘,ğ‘)=ğ‘(ğ‘), in agreement with the definition of b as encoding the belief
distribution over Ïˆ.
From list item 1 directly above, this generative density can also be read as a simple
Bayesian updating plus a change of indexes to reflect the effects of the action:
ğ‘(ğœ“â€²,ğ‘  | ğ‘,ğ‘) =ğ‘ƒ(ğ‘  | ğœ“â€² âˆ’ğ‘) ğ‘ƒ(ğœ“â€²âˆ’ğ‘ | ğ‘) or even more simply, ğ‘ğ‘ğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ =ğ‘ğ‘  ğ‘ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ .
ğœ“â€² ğœ“â€²âˆ’ğ‘ ğœ“â€²âˆ’ğ‘
In our model, both agents implement their own copies of the generative density
above (we leave it to the reader to add "â–¯own" indices where appropriate). The parameter
k, denoting the maximum sensory probability, is assumed agent-specific; we naturally
identify it with an agent's "perceptiveness". Ï‰ and ğœ“ , on the other hand, are
0
environmental parameters.
2.7 Partner model
In addition to the sensory model, we will define a new generative density
implementing the agentâ€™s inference of its partnerâ€™s behavior, or â€œTheory of Mindâ€ (ToM;
see Figure X). An agent with a sensory and partner model will adopt the following form:
p(â€˜, Î”, app | b, a) =P(â€² | Î”, app, b, apartner, ) P(Î” | b, apartner, ) P(app | b, apartner, ) P( | b, apartner) (3)
Where:
1. ğ‘ƒ(ğœ‘â€² | ğ›¥,ğ‘ğ‘ğ‘,ğ‘,ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ,ğœ‘) =ğ›¿(ğœ‘â€²,ğœ‘+ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ): the partnerâ€™s actions are always
assumed to have the intended effect.
2. ğ‘ƒ(ğ›¥ | ğ‘,ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ,ğœ‘)=ğ‘ƒ(ğœ“ =ğœ‘ +ğ›¥ | ğ‘,ğœ‘)=ğ‘ğ‘œğ‘¤ğ‘›; the probability of observing a
âˆ’ğ›¥
given ğ›¥, given the partnerâ€™s position ğœ‘, is equal to the probability the agent ascribes
to itself being in the corresponding position ğœ“ =ğœ‘ +ğ›¥. This holds as we assume ğ›¥
is a deterministic variable of the external states.
3. ğ‘ƒ(ğ‘ğ‘ğ‘ | ğ‘,ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ,ğœ‘) =ğ‘ƒ(ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ | ğœ‘ âˆ’ğ‘ğ‘ğ‘,ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ), where
ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
ğ‘ƒ(ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ =0 | ğœ‘) =ğœ‰
ğ‘šğ‘ğ‘¥(ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ)
ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ 1
ğ‘ƒ(ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ =Â±1 | ğœ‘)={1âˆ’ğœ‰ } ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
ğ‘šğ‘ğ‘¥(ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ) ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ+ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ âˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
âˆ’1 +1
4. ğ‘ƒ(ğœ‘ | ğ‘,ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ)=ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿdefines the â€œpriorâ€ and is analogous to (3) in the sensory
model.
The unwieldy formula in list item 3 above deserves some additional discussion.
Unlike the sensory model, where we can assume the agent â€œknowsâ€ the correct probability
distribution for s, here the agent must produce probabilities of the partnerâ€™s actions in
abstract, whereas the â€œtrueâ€ probabilities are only truly known in the partnerâ€™s mind (i.e.,
given the partnerâ€™s actual internal states). To do so, we provide the agent with a simple
mechanistic model of the partnerâ€™s actions.
Remember the agent is assumed to know the partnerâ€™s desires,
ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
. We
correspondingly assume the partner will act according to those desires, i.e., the higher a
partnerâ€™s desire for its current location, the more likely it is to stay put. In order to
eliminate spurious dependence on absolute values of ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ , we set ğ‘ƒ(ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ =0) to
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 12 of 33
be proportional to ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ/ğ‘šğ‘ğ‘¥(ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ), fixing the maximum probability at ğœ‰ (when
ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ achieves its global maxima). This leaves the remainder ğœ‰
ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
to be
ğ‘šğ‘ğ‘¥(ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ)
allocated across the other actions (Â±1), which we do by assuming the probability of
moving in a given direction is proportional to the desires in the adjacent locations (see
Figure 4 below).
Although these formulas seem complicated, especially list item 3 above, they result
in a generative density has the same form as the original generative density from the
baseline sensory model,
ğ‘ğ‘ğ‘œğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘œğ‘Ÿ =ğ‘ğ›¥,ğ‘ğ‘ğ‘ ğ‘ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ
. This is consistent with our
ğœ‘â€² ğœ‘â€²âˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ ğœ‘â€²âˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
modeling decision to make the â€œother-evidencing loopâ€ functionally identical to the
â€œselfâ€”actualization loopâ€, as discussed above (Section 2.2).
As before, each agent implements its own copy of the partner model. ğœ‰ is assumed
equal for both agents; they have the same capability to interpret the partnerâ€™s actions.
Figure 4. Illustrative plot of ğ‘ƒ(ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ).
2.8 Agent-level free energy
We are finally ready to define the free energy for our individual-level model. For
each agent:
ğ¹ = ğ· ğ¾ğ¿ (ğ‘â€² ğ‘œğ‘¤ğ‘› â€– ğ‘ğ‘œğ‘¤ğ‘› ğ›© ğ›¼ (ğ‘ + ğ‘ğ‘ ğ›¥ ğ‘Ÿ â€² ğ‘¡ğ‘›ğ‘’ğ‘Ÿ))+ğ· ğ¾ğ¿ (ğ‘â€² ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ â€– ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ ğ›© ğ›¼2 (ğ‘ âˆ’ ğ‘œğ‘¤ ğ›¥â€² ğ‘›)) (4)
Where:
1. ğ‘ ğœ“ ğ‘œğ‘¤ â€² ğ‘› =ğ‘ƒ(ğ‘ ğ‘œğ‘¤ğ‘› | ğœ“â€² âˆ’ğ‘ğ‘œğ‘¤ğ‘›) ğ‘ ğœ“ ğ‘œğ‘¤ â€²âˆ’ ğ‘› ğ‘ğ‘œğ‘¤ğ‘› is the sensory model (outlined above in Section
2.6).
2. ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ =ğ‘ğ‘œğ‘¤ğ‘› ğ‘ƒ(ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ | ğœ‘â€² âˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ âˆ’ğ‘ğ‘ğ‘,ğ‘â‹†ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ) ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ is the
ğœ‘â€² ğœ‘â€²âˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ+ğ›¥ ğœ‘â€²âˆ’ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
partner model (outlined above in subsection 2.7).
3. The "reranging" function, ğ›© , serves to moderate the influence of the partner
ğ›¼
model on the agent's own beliefs, and vice-versa. ğ›¼ is an agent-specific parameter,
which, as we will see in Subsection 2.9, is identified with each agent's degree of
"alterity."
4. The right-hand side of each KL divergence (i.e., the products of generative densities)
is implicitly constrained to [ğ‘’âˆ’10,1], to ensure the resulting beliefs remain within
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 13 of 33
their range ğ‘©. This is interpreted as preventing overconfidence and is implemented
as a simple maximum.
We interpret eq. 4 as follows: The agent's sensory and partner models jointly
constrain its beliefs both about its own position and its partner's position. Thus, at each
step, the agent: (a) refines its beliefs about both positions, in order to best fit the evidence
provided by all of its inputs (i.e., its "chemical" sensor for the physical environment and
"position and motion" sensor for its partner); and (b) selects the "best" next pair of actions
(for self and partner), i.e., that which minimizes the "difference" (the KL divergence)
between its present beliefs and the desired beliefs.2
2.9 Theory of Mind
In this section we motivate the parameterization of an agentâ€™s Theory of Mind ability
with ğ›¼, or simply, its degree of alterity.
Note that when considered as a discrete-time dynamical system evolution, the
process of refining beliefs about own and partner positions in the environment (step (a)
in Section 2.8 above) potentially involves multiple recursive dependencies: the updated
variational densities ğ‘â€² ğ‘œğ‘¤ğ‘›and ğ‘â€² ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ both depend on the previous ğ‘ğ‘œğ‘¤ğ‘› (via both
ğ‘ğ‘œğ‘¤ğ‘› and ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ), as well as on the previous ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ (via ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ). This is by design:
the dependencies ensure that ğ‘â€² ğ‘œğ‘¤ğ‘›and ğ‘â€² ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ are consistent with each other, as well
as with their counterparts across time steps. However, too much of a good thing can be a
problem. If left unconstrained, ğ‘â€² ğ‘œğ‘¤ğ‘›and ğ‘â€² ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿcan easily evolve towards spurious
fixed points (Kronecker deltas), which can be interpreted as overfitting on prematurely
established priors.3 On the other hand, if ğ‘â€² ğ‘œğ‘¤ğ‘›were to depend only on ğ‘ğ‘œğ‘¤ğ‘›, it would
eliminate the spurious fixed points: without the crossed dependence, the first term of the
partner model (Section 2.7) only has fixed points at (ğ‘â€² ğ‘œğ‘¤ğ‘› =
ğ›¿(ğœ“â€²,ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(ğ‘â‹† ğ‘œğ‘¤ğ‘›)),ğ‘ğ‘œğ‘¤ğ‘› =0), meaning that the agent has achieved a local desire
optimum. Effectively, this "shuts down" the agent's ability to use the partner's information
to shape its own beliefs, or its theory of mind, making it equivalent to MacGregor's
original model.
Thus, there would appear to be no universal "best" value for an agentâ€™s Theory of
Mind; an appropriate level of Theory of Mind would depend on a trade-off between the
risk of overfitting and that of discarding valid evidence from the partner. The appropriate
level of Theory of Mind would also depend on the agent's other capabilities (in this case,
its perceptiveness, k).
This motivates the operationalization of ğ›¼ as a parameter for the intensity to which
Theory of Mind shapes the agent's beliefs. ğ›¼ can be understood simply as an agentâ€™s
degree of alterity, or propensity to see the "other" as an agent like itself. In simulations
with values of ğ›¼ close to 0, we expect the partner's behavior to be dominated by its own
"chemical" sensory input. Increasing ğ›¼, we expect to see an agentâ€™s behavior being more
2 For reasons of numerical stability, we follow McGregor et al. in implementing (b) before (a): The agent chooses the next actions based on current beliefs, then updates
beliefs for the next time-step, based on the expected effects of those actions [64] (pp.6-7)
3 In this case, it could be possible to observe scenarios such as â€œthe blind leading the blindâ€ in which a weak agent fixates on the movement trajectory of a strong agent
who is overconfident about its final destination.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 14 of 33
heavily influenced by inputs from its partner, driving ğ‘ğ‘œğ‘¤ğ‘› to become sharper as soon as
ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ does so. Past a certain threshold, this could spill over into premature overfitting.
Finally, note the ğ›¼2 in the second term of agent-level free energy (eq. 4). This
represents the notion that the agent is using "second-order theory of mind" or thinking
about what its partner might be thinking about it.4 Here, ğ‘ğ‘œğ‘¤ğ‘› comes in as "my model of
my partner's model of my behavior". It seems appropriate for the agent to believe the
partner to possess the same level of alterity as itself; we then represent this as applying
the rearranging function (the "squishing" of the probability distribution) twice, ğ›© â€¢ ğ›© =
ğ›¼ ğ›¼
ğ›©
ğ›¼2
.
2.10 Goal Alignment
In this section we motivate the parameterization of the degree of goal alignment
between agents.
Recall that ğ‘â‹† ğ‘œğ‘¤ğ‘› is an arbitrary (exogenous) real vector; the implied desire
distribution can have multiple maxima, leading to a generally challenging optimization
task for the agent. Theory of Mind can help, but it can also make matters worse: if
ğ‘â‹† ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ also has multiple peaks, the partner's behavior can easily become ambiguous, i.e.,
it could appear coherent with multiple distinct positions. This ambiguity can easily lead
the agent astray.
This problem is reduced if the agents have the ability to align goals with each other,
that is, to avoid pursuing targets that are not shared between them. We implement this
as:
ğ‘â‹† ğ‘œğ‘¤ğ‘› â†ğ‘â‹† ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘+(1âˆ’ğ›¾)ğ‘
ğ‘
â‹†
ğ‘Ÿ
ğ‘œ
ğ‘–
ğ‘¤
ğ‘£ğ‘
ğ‘›
ğ‘¡ğ‘’ (5)
ğ‘â‹† ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ â†ğ‘â‹† ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘+(1âˆ’ğ›¾)ğ‘â‹† ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘›ğ‘’ğ‘Ÿ
(6)
ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’
where ğ›¾ is a parameter representing the degree of alignment between this specific agent
pair, and we assume each agent has knowledge of what goals are shared vs private to
itself or its partner. That is, with ğ›¾ =0, the agent is equally interested in its private goals
and in the shared ones (and assumes the same for the partner); with ğ›¾=0, the agent is
solely interested in the shared goals (and also assumes the same for the partner).
This operation may seem quite artificial, especially as it implies a leap of faith on the
part of the agent to effectively change its expectations about the partner's behavior (eq. 6).
However, if we accept this assumption, we see that the task is made easier: in the general
case, alignment reduces the agent-specific goal ambiguity, leading to better ability to focus
and less positional ambiguity coming from the partner. Of course, one can construct
examples where alignment does not help or even hurts; for instance, if both agents share
all of their peaks, alignment not only will not help reduce ambiguity, but it can make the
peaks sharper and hard to find. And as we will see, in the context of the system-level
model, alignment becomes a natural capability.
4 First-order ToM involves thinking about what some-one else is thinking or feeling; second-order ToM involves thinking about what someone is thinking or feeling
about what someone else is thinking or feeling [95].
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 15 of 33
2.11 System-level free energy
Up until now, we have restricted ourselves to discussing our model at the level of
individual agents and their local-scale interactions. We now take a higher vantage point
and consider the implications of these local-scale interactions for global-scale system
performance. We posit an ensemble of M identical copies of the two-agent subsystem
above (i.e., 2M), each in its own independent environment, also assumed to be identical
except for the position of the food source (see Figure 5).
Figure 5. M identical copies of the two-agent subsystem
From this vantage point, each of the 2M agents is now a "point particle", described
only by its position Ïˆi. More practically, we can change coordinates to ğœ‘ğ‘– =ğœ“ğ‘–âˆ’ğœ“ ğ‘–; the
0
tuple ğ‘ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š =(ğœ‘ğ‘–) is then the set of internal states of the system as a whole. We
ğ‘–âˆˆ[1...2ğ‘€]
can then define a system-level free energy as
ğ¹ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š = ğ· ğ¾ğ¿ (ğ‘ğ‘’ğ‘šğ‘ğ‘–ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™(ğœ‘) â€– ğ‘ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š(ğœ‘)), (7)
where ğ‘ğ‘’ğ‘šğ‘ğ‘–ğ‘Ÿğ‘–ğ‘ğ‘ğ‘™(ğœ‘) = 1 #{ğœ‘ğ‘– | ğœ‘ğ‘– =ğœ†}, the system's "variational density", is simply the
ğœ†
2ğ‘€
empirical distribution of the various agents across all possible positions relative to their
respective local food sources.
Note that the system is considered in isolation, and therefore the active states of the
system are empty, ğ‘ ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š =ğ‘ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š =âˆ…. Thus, if we are to analyze it as a single agent,
the only thing it can do to minimize its free energy is to rearrange its internal states ğœ‘ğ‘–.
This is analogous to a single time step in the simulation loop described at the individual
level in the model pseudo code (see Figure 3). We can say that the 2M agents' aggregate
behavior over an entire run of the model at the individual level implements a single step of
approximate Bayesian inference for the system as a whole.
This in turn motivates defining the system's "generative density" as
ğ‘ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š(ğœ“ | ğœ“)âˆ¼ğ‘(ğœ“ ,ğœ): at any given point in time, the system "expects" the agent
0 0
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 16 of 33
positions and food positions to be suitably close to each other. Thus, we can see that, to
the extent that the agents move closer to their food positions, the system performs
approximate Bayesian inference over this generative density, and we can evaluate the
degree to which this inference is effective, by evaluating whether, and how quickly,
ğ¹ğ‘ ğ‘¦ğ‘ ğ‘¡ğ‘’ğ‘š is minimized. We return to the topic of system-level (active) inference in the
discussion.
2.12 Simulations
We have thus defined this system at two altitudes, enabling us to perform
simulations at the agent level and analyze their implied performance at the system level
(as measured by system-level free energy). We can now use this framework to analyze the
extent to which the two novel agent-level cognitive capabilities we introduced ("Theory
of Mind" and "Goal Alignment") increase the system's ability to perform approximate
inference at local and global scales. To explore the effects of agent-level cognitive
capabilities on collective performance, we create four experimental conditions according
to a 2x2 (Theory of Mind x Goal Alignment) matrix: Model 1 (Baseline), Model 2 (Theory
of Mind), Model 3 (Goal Alignment), and Model 4 (Theory of Mind and Goal Alignment;
see Table 1).
Table 1. 2x2 (Theory of Mind x Goal Alignment) permutations of our model
-Theory of Mind +Theory of Mind
-Goal Alignment Model 1 (Baseline) Model 2 (Theory of Mind, No Goal
Alignment)
+Goal Alignment Model 3 (Goal Alignment, No ToM) Model 4 (Theory of Mind x Goal
Alignment)
Throughout, we use the same two agents, Agent A and Agent B. To establish
meaningful variation in agent performance at the individual-scale, we parameterize an
agentâ€™s perceptiveness to the physical environment (i.e., to the reliability of the
information derived from its "chemical sensors"), by assigning one agent with â€œstrongâ€
perceptiveness (Agent A - Strong;) and the other agent with â€œweakâ€ perceptiveness
(Agent B - Weak).
We assign each agent with two targets, one shared (Shared Target) and one unshared
(individual target or Target A and Target B). Accordingly, we assume each agent's desire
distributions have both a shared peak (corresponding to a Shared Target) and an unshared
peak (corresponding to Target A or Target B). Throughout, we measure both the collective
performance (system-level free energy), as well as individual performance (distance from
their closest target). In addition, we also capture their end-state desire distribution.
We implement simulations in Python (V3.7) using Google Colab (V1.0.0). As noted
above, our implementation draws upon and extends an existing AIF model
implementation developed in Python (V2.7) by van Shaik [66]. To ensure that the agent
behavior is not an artefact of their specific location in the environment, we run 180 runs
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 17 of 33
for each simulation for each experimental condition by randomizing their starting
locations throughout the environment. The environment size was held constant at 60 cells.
2.13 Model parameters
Our four models were created by setting physical perceptiveness for the strong and
weak agent and varying their ability to exhibit social perceptiveness and align goals. The
parameter settings are summarized at the individual agent level as follows (see Figure 6
and Table 2):
â€¢ Model 1 contains a self-actualization loop driven by physical perceptiveness.
Physical perceptiveness (individual skill parameter; range [.01,.99]) is varied
such that Agent A is endowed with strong perceptiveness (.99) and Agent B is
endowed with weak perceptiveness (.05).
â€¢ Model 2 is made up of a self-actualization loop and a partner-actualization loop
(instantiating ToM). The other-actualization loop is implemented by setting the
value of alterity (ToM or social perceptiveness parameter; range [0.01,.99]) as
.20 for the weak agent and 0 for the strong agent. This parameterization helps
the weak agent use social information to navigate the physical environment.
These two loops implement a single (non-separable) free energy functional: The
weak agent's inferences from their stronger partner's behavior serve to refine
its beliefs about its position in the environment.
â€¢ Model 3 entails a self-actualization loop (but no partner-actualization loop) as
well as enforces the pursuit of a common goal (set alignment = 1) by fully
suppressing their unshared goals (alignment parameter; range [0,1]). In this
simplified implementation, we assume that goal alignment is a
relational/dyadic property such that both partners exhibit the same level of
alignment towards each other. This is akin to partners fully exploring each
otherâ€™s targets and agreeing to pursue their common goal. Setting alignment
lower the 1 will increase the relative weighting of unshared goals and cause
them to compete with their shared goals
â€¢ Model 4 includes both cognitive features: self- and partner-actualization loops
for the weak agent (instantiating ToM; alterity = 0.2) and complete goal
alignment between agents
Table 2. Parameterization of agent abilities Models 1- 4
Model 1 Model 2 Model 3 Model 4
Baseline Theory of Mind Goal Alignment ToM x Goal
Alignment
Parameter Agent B Agent B Agent A Agent B Agent A Agent B Agent A Agent B
Physical perceptiveness (.01, .99) .99 .05 .99 .05 .99 .05 .99 .05
Alterity, ğ›¼ (.01, .99) .00 .00 .00 .20* .00 .00 .00 .20*
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 18 of 33
Goal Alignment, ğ›¾ (0, 1) 0 0 0 0 1 1 1 1
*Alternative results for simulations with alterity set at ğ›¼= .5 exhibit a similar pattern of results for Model 2 and Model 4.
(a)
(b)
(c)
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 19 of 33
(d)
Figure 6. Models. (a) Model 1 - Baseline; (b) Model 2 â€“ Theory of Mind; (c) Model 3 â€“ Goal Alignment; (d) Model 4 â€“
Theory of Mind with Goal Alignment
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 20 of 33
Figure 7. Results from a single run of Model 4 over 200 epochs. Agentsâ€™ Shared Target position is set at location 15. Actual
agent positions are illustrated as single dots for each epoch on the top graph, colored white when s=1 and gray when s=0.
The background of the top graphs plots the agents' belief distribution of their own position, from dark blue (0) to bright
yellow (1). The bottom graphs plot the agents' belief distribution of their partner's position, on the same scale.
3. Results
3.1 Illustration of Agent-level Behavior
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 21 of 33
In Figure 7, we show typical results from a single run of a single two-agent subsystem
(Model 4: ToM with Goal Alignment) to illustrate qualitatively how the two cognitive
capabilities introduced enable agent-level performance. In this example, Goal Alignment
enters the picture at the outset; although each agent has two targets, they both only ever
pursue their shared target.
The evolution of the two agents' behavior and beliefs over this run demonstrates the
key features of interplay between sensory and partner inputs, and how ToM moderates
the influence of partner inputs on an agent's behavior. Using its high perceptiveness, A
identifies its own position around epoch 25-50, and quickly thereafter, directs itself
towards the food position and remains stable there (top left). Meanwhile, for most of the
run, B has no strong sense of its own position, and therefore its movement is highly
random and undirected; at around epoch 150, it finally starts exhibiting a sharper (light
blue) belief and converging to the target (top right). This is the same moment when B is
finally able to disambiguate A's behavior (from green to yellow), which, via ToM, enables
B's belief to become sharper (bottom right). Meanwhile, A can't make sense of B's random
actions: the partner distribution it infers is unstable. But because A has ToM = 0, it doesn't
take any of these misleading cues into account when deciding its own beliefs (bottom left).
Figure 8. Simulation results of Agent A (strong; blue) and Agent B (weak; orange) in all four models. Row 1: Individual
performance as time taken to reach a target position. Row 2: End state belief distribution of target location (Shared Target = 30; Aâ€™s
Target = 15; Bâ€™s Target = 45). Row 3: Distribution of targets pursued in 180 runs.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 22 of 33
3.2 Simulation Results
Model 1 lends face validity to the two-agent simulation setup. Figure 8 (Row 1, Model
1) demonstrates that, on average, the strong agent (endowed with high physical
perceptiveness) converges to an end-state belief faster more accurately (closer to one of
their individual targets) than the weak agent with severely diminished physical
perceptiveness. This difference in individual performance can be attributed to the stark
difference in agentsâ€™ ability to form strong beliefs about the location of their target (see
Figure 8: Row 2, Model 1). Agents show no clear preference for either shared or unshared
targets (Figure 8: Row 3, Model 1).
In model 2, the weak agent possesses â€˜Theory of Mindâ€™. This allows it to infer
information about their own location in the environment by observing their partnerâ€™s
actions. This is evidenced by the emergence of two-sharp peaks in the weak agentâ€™s end-
state belief distribution (Figure 8: Row 2, Model 2). Consequently, we see an improvement
in the weak agentâ€™s individual performance (the agent converges faster on an end-state
belief faster than in Model 1). Collective performance (Figure 9: Systemâ€™s free energy) does
not appear to improve between Model 1 and Model 2. This may be because agents solely
focus on achieving their individual goals (and do not understand any distinction between
individual and system level goals). This is evidenced by the fact that of the 180 simulation
runs each of Model 1 and Model 2, both agents end up pursuing their shared and
unshared targets with roughly equal probability (Figure 8: Row 3, Model 1 and 2) .
In Model 3, when both agents possess an ability for Goal Alignment, but the weak
agent does not have the benefit of Theory of Mind, we see that both agents are biased
towards pursuing the shared system goal (Figure 8: Row 3, Model 3). Accordingly, at the
system level we see naturally higher collective performanceâ€”Model 3 clearly has lower
system-level free energy compared to both Model 1 and Model 2 (see Figure 9). At the
individual-level, however, the weak agent performs worse on average than it did in
Model 2 and converges more slowly towards its goals (Figure 8: Row 1, Model 3). It
appears that Goal Alignment helps improve system performance by reducing the
ambiguity of multiple possible targets, but Goal Alignment does not help the weak agent
compensate for low physical perceptiveness.
Finally, as expected, in Model 4, which combines Theory of Mind and Goal
Alignment, we see a clear improvement in both individual and collective performance
(Figure 8: Row 1, Model 4 and Figure 9: Model 4, respectively). The combination of Theory
of Mind (for the weak agent) and Goal Alignment (for both agents) appears to enable the
weak agent to overcome its poor physical perceptiveness and converge on a single
unambiguous end-state belief. This achievement is illustrated by the sharp and
overlapping single-peaked end-state belief structure achieved by both agents in model 4
(Figure 8: Row 2, Model 4). This model suggests that collective performance is highest
when individual agentsâ€™ individual states align with the global system state.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 23 of 33
Figure 9. Collective performance plotted as the system's free energy. Lower free energy denotes
higher system performance. Gradient descent on free energy suggests a system that performs
(active) Bayesian inference.
4. Discussion
A formal understanding of collective intelligence in complex adaptive systems
requires a formal description, within a single multiscale framework, of how the behavior
of a composite system and its subsystem components co-inform each other to produce
behavior that cannot be explained at any single scale of analysis. In this paper we make a
contribution toward this type of formal grasp of collective intelligence, by using AIF to
posit a computational model that connects individual-level constraints and capabilities to
collective-level behavior. Specifically, we provide an explicit, fully specified two-scale
system where free energy minimization occurs at both scales, and where the aggregate
behavior of agents at the faster/smaller scale can be rigorously identified with the belief-
optimization (a.k.a. â€œinferenceâ€) step at the slower/bigger scale. We introduce social
cognitive capabilities at the agent level (Theory of Mind and Goal Alignment), which we
implement directly through AIF. Further, illustrative results of this novel approach
suggest that such capabilities of individual agents are directly associated with
improvements in the system's ability to perform approximate Bayesian inference or
minimize variational free energy. Significantly, improvements in global-scale inference
are greatest when local-scale performance optima of individuals align with the system's
global expected state (e.g., Model 4). Crucially, all of this occurs "bottom-up", in the sense
that our model does not provide exogenous constraints or incentives for agents to behave
in any specific way; the system-level inference emerges as a product of self-organizing
AIF agents endowed with simple social cognitive mechanisms. The operation of these
mechanisms improves agent-level outcomes by enhancing agentsâ€™ ability to minimize free
energy in an environment populated by other agents like it.
Of course, our account does not preclude or dismiss the operation of "top-down"
dynamics, or the use of exogenous incentives or constraints to engineer specific types of
individual and collective behavior. Rather, our approach provides a principled and
mechanistic account of bio-cognitive systems in which "bottom-up" and "top-down"
mechanisms may meaningfully interplay to inform accounts of behavior such as collective
intelligence
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 24 of 33
[4]. Our results suggest that models such as these may help establish a mechanistic
understanding of how collective intelligence evolves and operates in real-life systems, and
provides a plausible lower bound for the kind of agent-level cognitive capabilities that are
required to successfully implement collective intelligence in such systems.
4.1 We demonstrate AIF as a viable mathematical framework for modelling collective intelligence
as a multiscale phenomenon
This work demonstrates viability of AIF as a mathematical language that can
integrate across scales of a composite bio-cognitive system to predict behavior. Existing
multiscale formulations of AIF [29,30], while more immediately useful for understanding
the behavior of docile subsystem components like cells in a multicellular organism or
neurons in the brain, do not yet offer clear predictions about the behavior of collectives
composed of highly autonomous AIF agents that engage in reciprocal self-evidencing
with each other as well as with the physical (non-social) environment [34]. Whatâ€™s more,
existing toy simulations of multiscale AIF engineer collective behavior as a
predestinationâ€”either as a prior in an agentâ€™s generative model [33], or by default of an
environment that consists solely of other agents [48,49]. We build upon these accounts by
using AIF to first posit the minimal information-theoretical patterns (or â€œadaptive priorsâ€;
see [32]) that would likely emerge at the level of the individual agent to allow that agent
to persist and flourish in an environment populated by other AIF agents [53]. We then
examine the relationship between these local-scale patterns and collective behavior as a
process of Bayesian inference across multiple scales. Our models show that collective
intelligence can emerge endogenously in a simple goal-directed task from interaction
between agents endowed with suitably sophisticated cognitive abilities (and without the
need for exogenous manipulation or incentivization).
Key to our proposal is the suggestion that collective intelligence can be understood
as a dynamical process of (active) inference at the global-scale of a composite system. We
operationalize self-organization of the collective as a process of free energy minimization
or inference based on sensory (but not active) states (for a previous attempt to
operationalize collective behavior as both active and sensory inference, see [65]). In a
series of four models, we demonstrate the responsiveness of this system-level measure to
learning effects over time; the progression of each Model exhibits a pattern akin to a
gradient descent on free energy, evoking the notion that a system that performs (active)
Bayesian inference. Further, stepwise increases in cognitive sophistication at the
individual level show a clear reduction in free energy, particularly between Model 1
(Baseline) and Model 4 (Theory of Mind x Goal Alignment). These illustrative results
establish a formal, causal link between behavioral processes across multiple scales of a
complex adaptive system.
Going further, we can imagine an extension of this model where the collective
system interacts with a non-trivial environment, but at a slower time scale, such that a
complete simulation run of all 2M agents corresponds to a single belief optimization step
for the whole system, after which it acts on the environment and receives sensory
information from it (manifested, for example, as changes in the agents' food sources). In
this extended model (see Figure 10), and if the agent-specific parameters (alterity/Theory
of Mind (Î±), and Goal Alignment (Î³)) could be made endogenous (either via selective
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 25 of 33
mechanisms via some other learning mechanisms; see [39,67]) we would expect to see the
system finding (non-zero) values of these parameters that optimize its free energy
minimization. For example, it is likely that a system would select for higher values of Î³
(Goal Alignment) when both agentsâ€™ end-state beliefs and actual target locations mutually
cohere, or higher values of Î± for agents with weaker perceptiveness. Interestingly, this
would show that degrees of Theory of Mind and Goal Alignment are capabilities that
would be selected for or boosted at these longer time scales, providing empirical support
for the heuristic arguments made for their existence in our model and in human collective
intelligence research more generally [4].
Figure 10. A notional complete two-scale model where agent-specific parameters are endogenized.
4.2 AIF sheds light on dynamical operation of mechanisms that underwrite collective intelligence
In this way, AIF offers a paradigm through which to move beyond the
methodological constraints associated with experimental analyses of the relationship
between local interactions and collective behavior [12]. Even our very rudimentary 2-
Agent AIF model proposed here offers insight into the dynamic operation and function
of individual cognitive mechanisms for individual and collective level behavior. In
distinct contrast to laboratory paradigms that usually rely on low-dimensional behavioral
â€œsnapshotsâ€ or summaries of behavior to verify linearly causal predictions about
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 26 of 33
individual and collective phenomena, our computational model can be used to explore
the effects of fine-grained, agent- and collective-level variations in cognitive ability on
individual and collective behavior in real time.
For example, by parameterizing key cognitive abilities (Theory of Mind and Goal
Alignment), our model shows that it is not necessarily a case of â€œmore is betterâ€ when it
comes to cognitive mechanisms underlying adaptive social behavior and collective
intelligence. If an agentâ€™s level of social perceptiveness (Theory of Mind) were too low, it
is likely that agents would miss vital performance-relevant information about the
environment populated by other agents; if an agentâ€™s Theory of Mind were too high, it
may instead over-index on partner belief states as an affordance for own beliefs (a scenario
of â€œblind leading the blindâ€). We show that canonical cognitive abilities such as Theory of
Mind and Goal Alignment can function across multiple scales to stabilize and reduce the
computational uncertainty of an environment made up of other AIF agents, but only when
these abilities are optimally tuned to a â€œgoldilocksâ€ level that is suitable to performance
in that specific environment.
The essence of this proposal is captured by empirical research of attentional processes
of human agents that engage in sophisticated joint action [68,69]. For instance, athletes in
novice basketball teams are found to devote more attentional resources to tracking and
monitoring their own teammates, while expert teams spend less time attending to each
other and more time instead attending to the socio-technical task environment [70].
Viewed from the perspective of AIF, in both novice and expert teams, agents likely
differentially deploy physical and social perceptiveness at levels that make sense for
pursuing collective performance in a given situation; novices may stand to gain more from
attending to (and therefore learning from) their teammates (recall our Agent B in Model
2 who leverages Theory of Mind to overcome weak physical perceptiveness, for example);
while experts might stand to gain more from down-regulating social perceptiveness and
redirecting limited attentional resources to physical perception of the task or (adversarial)
social environment [71,72].
To circle back to the specific domain of human collective intelligence examined in
organizational psychology and management, it seems likely that social perceptiveness
may indeed be an important factor (among many) that underwrites collective intelligence.
But this may be especially the case in the context of unacquainted teams of â€œWEIRDâ€
experimental subjects [73] who coordinate for a limited number of hours in a contrived
laboratory setting [3]. If the experimental task were to be translated to a real-world
performance setting (e.g. one involving high-stakes or elite performance requirements),
or if that same team of experimental subjects were to persist over time beyond the lab in
a randomly fluctuating environment, it is conceivable that a premium for social
perceptiveness may give way to demands for other types of abilities needed to continue
to gain performance-relevant information from the task environment (e.g., through
physical perceptiveness of the task environment). Viewed from this perspective, the true
â€œspecial sauceâ€ of collective intelligence (and individual intelligence, for that matter; see
[74]) may turn out not to be one or other discrete or reified individual or team level ability
per se (e.g., social perceptiveness), but instead a collective ability to nimbly adjust the
volumes of multiple parameters to foster specific information-theoretic patterns
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 27 of 33
conducive to minimizing free energy across multiple scales and over specific,
performance-relevant time periods.
In this spirit, the computational approach we adopt here under AIF affords a
dynamical and situational perspective on team performance that may offer important
insights into long-standing and nascent hypotheses concerning the causal mechanisms of
collective intelligence. For instance, our model is well positioned to investigate the long-
proposed (but hitherto unsubstantiated) claim that successful team performance, and by
extension, collective intelligence, depends on balancing a tradeoff between cognitive
diversity and cognitive efficiency [4] (p. 421). Likewise, our approach could help elucidate
mechanisms and dynamics through which memory, attention, and reasoning capabilities
become distributed through a collective, and the conditions in which these â€œtransactiveâ€
processes [75] facilitate emergence of intelligent behavior [70,76,77]. In either case, our
model would simply require specification with the appropriate individual-level cognitive
abilities or priors. For example, to better understand the causal relationship between
transactive knowledge systems and collective intelligence, our model could leverage
recent empirical research that observes a connection between individual agentsâ€™
metacognitive abilities (e.g., perception of othersâ€™ skills, focus, and goals), the formation
of transactive knowledge systems, and a collectiveâ€™s ability to adapt to a changing task
environment [78]. On an important and related note to these opportunities for future
research, efforts to simulate human collective intelligence should strive to develop models
composed of two or more agents to better mimic human-like coordination dynamics
[43,79].
4.3 Increases in system performance correspond with alignment between an agentâ€™s local and
global optima.
A key insight from our models, and worthy of further investigation, is that the
greatest improvement in collective intelligence (Model 4; measured by global-scale
inference) occurs when local-scale performance optima of individuals align with the
system's global expected state. This effect can be understood as individuals jointly
implementing approximate Bayesian inference of the system's expectations. In effect, our
model suggests that multi-scale alignment between lower- and higher-order states may
contribute to the emergence of collective intelligence.
Alignment between local and global states might sound like an obvious prerequisite
for collective intelligence, particularly for more docile AIF agents such as neurons or cells
(it is near impossible to imagine a scenario in which a neuron or cell could meaningfully
persist without being spatially aligned with a superordinate agent; see Palacios et al.,
2020). But our model exemplifies a more subtle form of alignment, based on a loose
coupling between scales through a systemâ€™s generative model (section 2.11), enabling the
extension of this idea to scenarios where the local and global optimizations may be taking
place in arbitrarily distinct and abstract state spaces [42,44]. By now it is well understood
that coordinated human behavior relies for its stability and efficacy on an intricate web of
biologically evolved physiological and cognitive mechanisms [80,81], as well as culturally
evolved affordances of language, norms, and institutions [82]. But precisely how these
various mechanisms and affordancesâ€”particularly those that are separated across
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 28 of 33
scalesâ€”coordinate in real or evolutionary time to enable human collective phenomena
remains poorly understood [29,83,84].
Computational models capable of formally representing multiscale alignment may
help reorganize and clarify causal relationships between the various hypothesized
physiological, cognitive, and cultural mechanisms hypothesized to underpin human
collective behavior [85]. For example, a computational model such as the one proposed
here could conceivably be adapted to help more systematically test the burgeoning
hypothesis that coordination between basal physiological, metabolic and homeostatic
processes at one scale of organization and linguistically mediated processes of interaction
and exchange at another scale determine fundamental dynamics of individual and
collective behavior [86â€“88].
Future research should aspire to examine causal connections between a fuller range
of meaningful scales of behavior. In the case of human collectives, meaningful scales of
behavior could extend from the basal mechanisms of physiological energy, movement,
and emotional regulation on the micro scale [89,90], to linguistically- (and now digitally-)
mediated social informational systems at the meso scale [91] to global socio-ecological
systems at the macro scale [84,92,93]. As we have demonstrated here, the key requirement
for the development of such multiscale models under AIF is faithful construction of the
appropriate generative models at each scale to provide the mechanistic â€œmissing linksâ€
between AIF and the phenomena to be explainedâ€”a task that will require tremendously
innovative and intelligent collective behavior on the part of a diverse range of agents.
The patterns that crop up again and again in successful space are there because they are in
fundamental accord with characteristics of the human creature. They allow him to function as a
human. They emphasize his essenceâ€”he is at once an individual and a member of a group. They
deny neither his individuality nor his inclination to bond into teams. They let him be what he is.
- DeMarco and Lister [94](1987, p.90)
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 29 of 33
Author Contributions: Conceptualization, R.K., J.T., and P.G.; methodology, R.K., P.G. and J.T;
software, R.K., P.G. validation, R.K., P.G; formal analysis, R.K.; investigation, R.K., P.G. and J.T;
resources, R.K., J.T., and P.G.; data curation, P.G. and R.K.; writingâ€”original draft preparation, J.T.
& R.K.; writingâ€”review and editing, and J.T, R.K. and P.G.; visualization, P.G., R.K., J.T. All authors
have read and agreed to the published version of the manuscript.
Funding: This research received no external funding
Data Availability Statement: Model code can be found and implemented via this link to Google
Colab:
https://colab.research.google.com/drive/1CKdPTy8LD-Mpxc7kXy47m_fmCq44BT5u?usp=sharing
Acknowledgments:
Conflicts of Interest: The authors declare no conflict of interest
References
1. Kelso, J.A.S. Dynamic patterns: The self-organization of brain and behavior; MIT press, 1995; ISBN 0262611317.
2. Riley, M.A.; Richardson, M.J.; Shockley, K.; Ramenzoni, V.C. Interpersonal synergies. Front. Psychol. 2011, 2,
doi:10.3389/fpsyg.2011.00038.
3. Woolley, A.W.; Chabris, C.F.; Pentland, A.; Hashmi, N.; Malone, T.W. Evidence for a Collective Intelligence Factor in the
Performance of Human Groups. Science (80-. ). 2010, 330, 686.
4. Woolley, A.W.; Aggarwal, I.; Malone, T.W. Collective Intelligence and Group Performance. Curr. Dir. Psychol. Sci. 2015, 24,
420â€“424, doi:10.1177/0963721415599543.
5. Malone, T.W.; Bernstein, M.S. Introduction. In Handbook of Collective Intelligence; 2015.
6. Baron-Cohen, S.; Tager-Flusberg, H.; Cohen, D.J. Understanding other minds: Perspectives from autism. In Proceedings of
the Most of the chapters in this book were presented in draft form at a workshop in Seattle, Apr 1991.; Oxford University
Press, 1994.
7. Tomasello, M.; Carpenter, M.; Call, J.; Behne, T.; Moll, H. Understanding and sharing intentions: the origins of cultural
cognition. Behav. Brain Sci. 2005, 28, 675â€“91; discussion 691-735, doi:10.1017/S0140525X05000129.
8. Chikersal, P.; Tomprou, M.; Kim, Y.J.; Woolley, A.W.; Dabbish, L. Deep structures of collaboration: Physiological correlates
of collective intelligence and group satisfaction. Proc. ACM Conf. Comput. Support. Coop. Work. CSCW 2017, 873â€“888,
doi:10.1145/2998181.2998250.
9. Engel, D.; Malone, T.W. Integrated information as a metric for group interaction. PLoS One 2018, 13, 1â€“19,
doi:10.1371/journal.pone.0205335.
10. Riedl, C.; Kim, Y.J.; Gupta, P.; Malone, T.. W.; Woolley, A.W. Quantifying Collective Intelligence in Human Groups (In
Press). Proc. Natl. Acad. Sci.
11. Rozin, P. Social psychology and science: Some lessons from solomon asch. Personal. Soc. Psychol. Rev. 2001, 5, 2â€“14,
doi:10.1207/S15327957PSPR0501_1.
12. Kozlowski, S.W.J.; Chao, G.T. Unpacking team process dynamics and emergent phenomena: Challenges, conceptual
advances, and innovative methods. Am. Psychol. 2018, 73, 576â€“592, doi:10.1037/amp0000245.
13. Oâ€™Bryan, L.; Beier, M.; Salas, E. How approaches to animal swarm intelligence can improve the study of collective
intelligence in human teams. J. Intell. 2020, 8, doi:10.3390/jintelligence8010009.
14. Richardson, M.J.; Schmidt, R.C.; Richardson, M.J. Dynamics of interpersonal coordination. Coord. Neural, Behav. Soc. Dyn.
2008, 281â€“308.
15. Kelso, J.A.S. Coordination dynamics. In Encyclopedia of complexity and systems science; Springer, 2009; pp. 1537â€“1565.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 30 of 33
16. Coey, C.A.; Varlet, M.; Richardson, M.J. Coordination dynamics in a socially situated nervous system. Front. Hum. Neurosci.
2012, 6, 164, doi:10.3389/fnhum.2012.00164.
17. Gorman, J.C.; Dunbar, T.A.; Grimm, D.; Gipson, C.L. Understanding and modeling teams as dynamical systems. Front.
Psychol. 2017, 8, 1â€“18, doi:10.3389/fpsyg.2017.01053.
18. Reinero, D.A.; Dikker, S.; Van Bavel, J.J. Inter-brain synchrony in teams predicts collective performance. Soc. Cogn. Affect.
Neurosci. 2021, 16, 43â€“57, doi:10.1093/scan/nsaa135.
19. Gorman, J.C.; Amazeen, P.G.; Crites, M.J.; Gipson, C.L. Deviations from mirroring in interpersonal multifrequency
coordination when visual information is occluded. Exp. Brain Res. 2017, 235, 1209â€“1221, doi:10.1007/s00221-017-4888-5.
20. Wiltshire, T.J.; Butner, J.E.; Fiore, S.M. Problem-Solving Phase Transitions During Team Collaboration. Cogn. Sci. 2018, 42,
129â€“167, doi:10.1111/cogs.12482.
21. Wiltshire, T.J.; Steffensen, S.V.; Fiore, S.M. Multiscale movement coordination dynamics in collaborative team problem
solving. Appl. Ergon. 2019, 79, 143â€“151, doi:10.1016/j.apergo.2018.07.007.
22. Zhang, M.; Kelso, J.A.S.; Tognoli, E. Critical diversity: Divided or united states of social coordination. 2018, 1â€“19,
doi:10.17605/OSF.IO/SC9P6.
23. Demir, M.; Mcneese, N.J.; Gorman, J.C.; Cooke, N.J.; Myers, C.; Grimm, D.A. Exploration of Team Trust and Interaction in
Human-Autonomy Teaming. 2017, 1â€“10.
24. Friston, K.J. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 2010, 11, 127â€“138, doi:10.1038/nrn2787.
25. Friston, K.J. Life as we know it. J. R. Soc. Interface 2013, 10, 10.
26. Friston, K.J. A free energy principle for a particular physics. arXiv Prepr. arXiv1906.10184 2019.
27. Friston, K.J.; Kilner, J.; Harrison, L. A free energy principle for the brain. J. Physiol. Paris 2006, 100, 70â€“87,
doi:10.1016/j.jphysparis.2006.10.001.
28. Hohwy, J. The self-evidencing brain. Nous 2016, 50, 259â€“285, doi:10.1111/nous.12062.
29. Ramstead, M.J.D.; Badcock, P.B.; Friston, K.J. Answering SchrÃ¶dingerâ€™s question: A free-energy formulation. Phys. Life Rev.
2018, 24, 1â€“16, doi:10.1016/j.plrev.2017.09.001.
30. Kirchhoff, M.D.; Parr, T.; Palacios, E.; Friston, K.; Kiverstein, J. The markov blankets of life: Autonomy, active inference and
the free energy principle. J. R. Soc. Interface 2018, 15, doi:10.1098/rsif.2017.0792.
31. Hesp, C.; Ramstead, M.; Constant, A.; Badcock, P.; Kirchhoff, M.; Friston, K. A multi-scale view of the emergent complexity
of life: A free-energy proposal. In Evolution, Development and Complexity; Springer, 2019; pp. 195â€“227.
32. Badcock, P.B.; Friston, K.J.; Ramstead, M.J.D. The hierarchically mechanistic mind: A free-energy formulation of the human
psyche. Phys. Life Rev. 2019, 31, 104â€“121, doi:10.1016/j.plrev.2018.10.002.
33. Palacios, E.R.; Razi, A.; Parr, T.; Kirchhoff, M.D.; Friston, K. On Markov blankets and hierarchical self-organisation. J. Theor.
Biol. 2020, 486, 110089, doi:10.1016/j.jtbi.2019.110089.
34. Sims, M. How to count biological minds: symbiosis, the free energy principle, and reciprocal multiscale integration.
Synthese 2020, doi:10.1007/s11229-020-02876-w.
35. Pearl, J. Probabilistic reasoning in intelligent systems: networks of plausible inference; Elsevier, 1988; ISBN 0080514898.
36. Friston, K.J. What is optimal about motor control? Neuron 2011, 72, 488â€“498, doi:10.1016/j.neuron.2011.10.018.
37. Constant, A.; Ramstead, M.J.D.; VeissiÃ¨re, S.P.L.; Friston, K. Regimes of expectations: An active inference model of social
conformity and human decision making. Front. Psychol. 2019, 10, doi:10.3389/fpsyg.2019.00679.
38. Haken, H. Synergetics. In Self-Organizing Systems; Springer, 1987; pp. 417â€“434.
39. Ramstead, M.J.D.; Constant, A.; Badcock, P.B.; Friston, K.J. Variational ecology and the physics of sentient systems. Phys.
Life Rev. 2019, 31, 188â€“205.
40. Kirchhoff, M.D.; Kiverstein, J. How to determine the boundaries of the mind: a Markov blanket proposal. Synthese 2019,
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 31 of 33
doi:10.1007/s11229-019-02370-y.
41. VeissiÃ¨re, S.P.L. Cultural Markov blankets? Mind the other minds gap!. Comment on" Answering SchrÃ¶dingerâ€™s question:
A free-energy formulation" by Maxwell James DÃ©sormeau Ramstead et al. Phys. Life Rev. 2018, 24, 47â€“49.
42. Clark, A. How to Knit Your Own Markov Blanket : Resisting the Second Law with Metamorphic Minds. Philos. Predict.
Coding 2017, 1â€“19, doi:10.15502/9783958573031.
43. Zhang, M.; Beetle, C.; Kelso, J.A.S.; Tognoli, E. Connecting empirical phenomena and theoretical models of biological
coordination across scales. 2018, 1â€“33.
44. Krakauer, D.; Bertschinger, N.; Olbrich, E.; Flack, J.C.; Ay, N. The information theory of individuality. Theory Biosci. 2020,
139, 209â€“223, doi:10.1007/s12064-020-00313-7.
45. Ramstead, M.J.D. Have we lost our minds? An approach to multiscale dynamics in the cognitive sciences. Maxwell James
D. Ramstead, 2019.
46. Ramstead, M.J.D.; VeissiÃ¨re, S.P.L.; Kirmayer, L.J. Cultural affordances: Scaffolding local worlds through shared
intentionality and regimes of attention. Front. Psychol. 2016, 7, 1â€“21, doi:10.3389/fpsyg.2016.01090.
47. VeissiÃ¨re, S.P.L.; Constant, A.; Ramstead, M.J.D.; Friston, K.J.; Kirmayer, L.J. Thinking Through Other Minds: A Variational
Approach to Cognition and Culture. Behav. Brain Sci. 2019, doi:10.1017/S0140525X19001213.
48. Friston, K.J.; Frith, C.D. A Duet for one. Conscious. Cogn. 2015, 36, 390â€“405, doi:10.1016/j.concog.2014.12.003.
49. Friston, K.J.; Frith, C.D. Active inference, Communication and hermeneutics. Cortex 2015, 68, 129â€“143,
doi:10.1016/j.cortex.2015.03.025.
50. Searle, J.R. Minds and brains without programs. Mindwaves 1980, 3, 1â€“19.
51. Friston, K.J.; Daunizeau, J.; Kiebel, S.J. Reinforcement learning or active inference? PLoS One 2009, 4,
doi:10.1371/journal.pone.0006421.
52. Sajid, N.; Ball, P.J.; Parr, T.; Friston, K.J. Active Inference: Demystified and Compared. Neural Comput. 2021, 44, 1â€“39,
doi:10.1162/neco_a_01357.
53. Vasil, J.; Badcock, P.B.; Constant, A.; Friston, K.; Ramstead, M.J.D. A World Unto Itself: Human Communication as Active
Inference. Front. Psychol. 2020, 11, 1â€“26, doi:10.3389/fpsyg.2020.00417.
54. Hirschfeld, L.A. On a Folk Theory of Society: Children, Evolution, and Mental Representations of Social Groups. Personal.
Soc. Psychol. Rev. 2001, 5, 107â€“117, doi:10.1207/S15327957PSPR0502_2.
55. Sperber, D. Intuitive and reflective beliefs. 1997, 12, 67â€“83, doi:10.1111/1468-0017.00036.
56. Yoshida, W.; Dolan, R.J.; Friston, K.J. Game theory of mind. PLoS Comput. Biol. 2008, 4, doi:10.1371/journal.pcbi.1000254.
57. Press, W.H.; Dyson, F.J. Iterated Prisonerâ€™s Dilemma contains strategies that dominate any evolutionary opponent. Proc.
Natl. Acad. Sci. U. S. A. 2012, 109, 10409â€“10413, doi:10.1073/pnas.1206569109.
58. Baron-Cohen, S.; Wheelwright, S.; Hill, J.; Raste, Y.; Plumb, I. The â€œReading the Mind in the Eyesâ€ Test revised version: A
study with normal adults, and adults with Asperger syndrome or high-functioning autism. J. Child Psychol. Psychiatry Allied
Discip. 2001, 42, 241â€“251.
59. Dunbar, R.I.M. The Social Brain: Mind, Language, and Society in Evolutionary Perspective. Annu. Rev. Anthropol. 2003, 32,
163â€“181, doi:10.2307/25064825.
60. Pesquita, A.; Whitwell, R.L.; Enns, J.T. Predictive joint-action model : A hierarchical predictive approach to human
cooperation. Psychol. Bull. 2017.
61. Krafft, P.M. A Simple Computational Theory of General Collective Intelligence. Top. Cogn. Sci. 2019, 11, 374â€“392,
doi:10.1111/tops.12341.
62. Angus, S.D.; Newton, J. Emergence of Shared Intentionality Is Coupled to the Advance of Cumulative Culture. PLoS
Comput. Biol. 2015, 11, 1â€“12, doi:10.1371/journal.pcbi.1004587.
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 32 of 33
63. Fields, C.; Levin, M. How Do Living Systems Create Meaning? Philosophies 2020, 5, 36, doi:10.3390/philosophies5040036.
64. McGregor, S.; Baltieri, M.; Buckley, C.L. A Minimal Active Inference Agent. 2015, 1â€“19.
65. Levchuk, G.; Pattipati, K.; Serfaty, D.; Fouse, A.; McCormack, R. Active Inference in Multi-Agent Systems: Context-Driven
Collaboration and Decentralized Purpose-Driven Team Adaptation. 2018, 157â€“165.
66. van Schaik, A. Python implementation of a Minimal Active Inference Agent 2018.
67. Badcock, P.B. Evolutionary systems theory: A unifying meta-theory of psychological science. Rev. Gen. Psychol. 2012, 16, 10â€“
23, doi:10.1037/a0026381.
68. Sebanz, N.; Bekkering, H.; Knoblich, G. Joint action: Bodies and minds moving together. Trends Cogn. Sci. 2006, 10, 70â€“76,
doi:10.1016/j.tics.2005.12.009.
69. Vesper, C.; Abramova, E.; BÃ¼tepage, J.; Ciardo, F.; Crossey, B.; Effenberg, A.; Hristova, D.; Karlinsky, A.; McEllin, L.;
Nijssen, S.R.R.; et al. Joint Action: Mental Representations, Shared Information and General Mechanisms for Coordinating
with Others. Front. Psychol. 2017, 07, 1â€“7, doi:10.3389/fpsyg.2016.02039.
70. Bourbousson, J.; Râ€™Kiouak, M.; Eccles, D.W. The dynamics of team coordination: A social network analysis as a window to
shared awareness. Eur. J. Work Organ. Psychol. 2015, 24, 742â€“760, doi:10.1080/1359432X.2014.1001977.
71. Bourbousson, J.; Fortes-Bourbousson, M. How do Co-agents Actively Regulate their Collective Behavior States? Front.
Psychol. 2016, 7, 1732, doi:10.3389/fpsyg.2016.01732.
72. Râ€™Kiouak, M.; Saury, J.; Durand, M.; Bourbousson, J. Joint action of a pair of rowers in a race: Shared experiences of
effectiveness are shaped by interpersonal mechanical states. Front. Psychol. 2016, 7, 1â€“17, doi:10.3389/fpsyg.2016.00720.
73. Henrich, J.; Heine, S.J.; Norenzayan, A. The weirdest people in the world? Behav. Brain Sci. 2010, 33, 61â€“83; discussion 83-
135, doi:10.1017/S0140525X0999152X.
74. Friston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.; Oâ€™Doherty, J.; Pezzulo, G. Active inference and learning. Neurosci.
Biobehav. Rev. 2016, 68, 862â€“879, doi:10.1016/j.neubiorev.2016.06.022.
75. Wegner, D.M. Transactive memory: A contemporary analysis of the group mind. In Theories of group behavior; Springer,
1987; pp. 185â€“208.
76. AraÃºjo, D.; Bourbousson, J. Theoretical Perspectives on Interpersonal Coordination for Team Behaviour. In Theoretical
Perspectives on Interpersonal Coordination; 2016.
77. Semin, G.R.; Garrido, M.V. Socially Situated Cognition : Imagining New. In Theory and Explanation in Social Psychology.;
2015; Vol. 36, pp. 774â€“777.
78. Gupta, P.; Woolley, A.W. The Emergence of Collective Intelligence Behavior. In Proceedings of the Paper presented at the
8th ACM Collective Intelligence (CI) Conference, Virtual Event. June 18; 2020.
79. Richardson, M.J.; Garcia, R.L.; Frank, T.D.; Gergor, M.; Marsh, K. Measuring group synchrony: a cluster-phase method for
analyzing multivariate movement time-series. Front. Physiol. 2012, 3, 405, doi:10.3389/fphys.2012.00405.
80. Frith, U.; Frith, C.D. The social brain: allowing humans to boldly go where no other species has been. Philos. Trans. R. Soc. B
Biol. Sci. 2010, 365, 165â€“176, doi:10.1098/rstb.2009.0160.
81. Taylor, J.; Davis, A. Social Cohesion. In The International Encyclopedia of Anthropology; Wiley, 2018; pp. 1â€“7.
82. Henrich, J. The secret of our success: how culture is driving human evolution, domesticating our species, and making us smarter;
Princeton University Press, 2015; ISBN 1400873290.
83. Badcock, P.B. Evolutionary Systems Theory : A Unifying Meta-Theory of Psychological Evolutionary Systems Theory : A
Unifying Meta-Theory of Psychological Science. 2017, doi:10.1037/a0026381.
84. Doolittle, F.W.; Inkpen, A.S. Processes and patterns of interaction as units of selection: An introduction to ITSNTS thinking.
Proc. Natl. Acad. Sci. U. S. A. 2018, 115, 4006â€“4014, doi:10.1073/pnas.1722232115.
85. VeissiÃ¨re, S.P.L.; Constant, A.; Ramstead, M.J.D.; Friston, K.J.; Kirmayer, L.J. Thinking Through Other Minds: A Variational
Kaufmann, Gupta, & Taylor (2021). An active inference model of collective intelligence 33 of 33
Approach to Cognition and Culture. Behav. Brain Sci. 2019, 1â€“97, doi:10.1017/S0140525X19001213.
86. Barrett, L.F.; Simmons, W.K. Interoceptive predictions in the brain. Nat Rev Neurosci 2015, 16, 419â€“429,
doi:10.1038/nrn3950.Interoceptive.
87. KrahÃ©, C.; Springer, A.; Weinman, J. a; Fotopoulou, A. The social modulation of pain: others as predictive signals of
salience - a systematic review. Front. Hum. Neurosci. 2013, 7, 386, doi:10.3389/fnhum.2013.00386.
88. Taylor, J.; Cohen, E. Social bonding through joint action: When the team clicks. OSF Pre Print 2019.
89. Allen, M. Unravelling the Neurobiology of Interoceptive Inference. Trends Cogn. Sci. 2020, 24, 265â€“266,
doi:10.1016/j.tics.2020.02.002.
90. Barrett, L.F.; Quigley, K.S.; Hamilton, P. An active inference theory of allostasis and interoception in depression. Philos.
Trans. R. Soc. B 2016, doi:10.1098/rstb.2016.0011.
91. Mesoudi, A. Cultural evolution: integrating psychology, evolution and culture. Curr. Opin. Psychol. 2016, 7, 17â€“22,
doi:10.1016/j.copsyc.2015.07.001.
92. Kaufmann, R. Gaianomics, or the self-designing Earth. In The Great Redesign: Frameworks for the Future; Schrader, M.,
Martens, V., Eds.; Edition NFO; Next Factory Ottensen, 2020 ISBN 9783948580841.
93. Rubin, S.; Parr, T.; Da Costa, L.; Friston, K. Future climates: Markov blankets and active inference in the biosphere: Future
climates: Markov blankets and active inference in the biosphere. J. R. Soc. Interface 2020, 17, 13â€“16,
doi:10.1098/rsif.2020.0503.
94. Lister, T.R.; DeMarco, T. Peopleware: Productive projects and teams; Dorset House New York, 1987; ISBN 0932633056.
95. Westby, C.E. Social neuroscience and theory of mind. Folia Phoniatr. Logop. 2014, 66, 7â€“17, doi:10.1159/000362877.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "An active inference model of collective intelligence"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
