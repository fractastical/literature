=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Approximate information maximization for bandit games
Citation Key: barbierchebbah2023approximate
Authors: Alex Barbier-Chebbah, Christian L. Vestergaard, Jean-Baptiste Masson

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: Entropymaximizationandfreeenergyminimizationaregeneralphysicsprinciples
for modeling dynamic systems. Notable examples include modeling decision-
makingwithinthebrainusingthefree-energyprinciple,optimizingtheaccuracy-
complexitytrade-offwhenaccessinghiddenvariableswiththeinformationbottle-
neckprinciple[Tishbyetal.,2000],andnavigationinrandomenvironmentsusing
information maximization [Vergassola et al., 2007]. Building on this principle,
weproposeanewclassofbanditalgorithmsthatmaximizeanapproxim...

Key Terms: chebbah, bandit, parisfrance, epimetheeinriaparisfrance, dynamic, maximization, information, modeling, approximate, games

=== FULL PAPER TEXT ===

Approximate information maximization for bandit
games
AlexBarbier–Chebbah ChristianL.Vestergaard
InstitutPasteur,UniversitéParisCité InstitutPasteur,UniversitéParisCité
EpimetheeINRIAParisFrance EpimetheeINRIAParisFrance
CNRSUMR3571,ParisFrance CNRSUMR3571,ParisFrance
alex.barbier-chebbah@pasteur.fr
Jean-BaptisteMasson EtienneBoursier
InstitutPasteur,UniversitéParisCité INRIA,UniversitéParisSaclay
EpimetheeINRIAParisFrance LMO,Orsay,France
CNRSUMR3571,ParisFrance
Abstract
Entropymaximizationandfreeenergyminimizationaregeneralphysicsprinciples
for modeling dynamic systems. Notable examples include modeling decision-
makingwithinthebrainusingthefree-energyprinciple,optimizingtheaccuracy-
complexitytrade-offwhenaccessinghiddenvariableswiththeinformationbottle-
neckprinciple[Tishbyetal.,2000],andnavigationinrandomenvironmentsusing
information maximization [Vergassola et al., 2007]. Building on this principle,
weproposeanewclassofbanditalgorithmsthatmaximizeanapproximationto
theinformationofakeyvariablewithinthesystem. Tothisend,wedevelopan
approximated,analyticalphysics-basedrepresentationoftheentropytoforecast
theinformationgainofeachactionandgreedilychoosetheonewiththelargest
information gain. This method yields strong performances in classical bandit
settings. Motivatedbyitsempiricalsuccess,weproveitsasymptoticoptimalityfor
themulti-armedbanditproblemwithGaussianrewards. Sinceitencompassesthe
system’spropertiesinasingle,globalfunctional,thisapproachcanbeefficiently
adaptedtomorecomplexbanditsettings. Thiscallsforfurtherinvestigationof
informationmaximizationapproachesforbanditproblems.
1 Introduction
Multi-armed bandit problems have attracted wide attention in the past decades. They embody
thechallengeofbalancingexplorationandexploitationandhavebeenappliedtovarioussettings
suchasonlinerecommendation[Bresleretal.,2014],medicaltrials[Thompson,1933],dynamic
pricing[DenBoer,2015],andreinforcementlearning-baseddecisionmaking[Silveretal.,2016,
Ryzhovetal.,2012]. Besidestheclassicstochasticversionofthemulti-armedbanditproblem,many
subsequentextensionshavebeendeveloped,providingrichermodelsforspecificapplications. These
extensionsincludelinearbandits[Lietal.,2010], many-armedbandits[Bayatietal.,2020], and
pure exploration problems such as thresholding bandits [Locatelli et al., 2016] or top-K bandits
[Kalyanakrishnanetal.,2012,Kaufmannetal.,2016].
Intheclassicsetting,anagentchoosesanarmateachtimestepandobservesastochasticreward.
Sincetheyonlyobservethepayoffofthechosenarm,theagentshouldregularlyexploresuboptimal
arms. Thisisoftenreferredtoastheexploration-exploitationtrade-off. Anagentcanexploitits
Preprint.Underreview.
4202
voN
92
]LM.tats[
3v36521.0132:viXra
currentknowledgetooptimizegainsbydrawingthecurrentempiricallybestarmorexploringother
armstopotentiallyincreasefuturegains.
OptimalstrategiesarecharacterizedasymptoticallybytheLaiandRobbinsbound[Laietal.,1985].
Among them, upper confidence bound [UCB, Auer, 2000, Garivier and Cappé, 2011] methods
greedilypullthearmmaximizingsometunedconfidenceindex;Thompsonsampling[Kaufmann
etal.,2012a,AgrawalandGoyal,2013]reliesonsamplingmeanrewardsfromaposteriordistribution
andchoosesthearmwiththelargestrandomsample;deterministicminimumempiricaldivergence
[DMED,HondaandTakemura,2010]buildsonabalancebetweenthemaximumlikelihoodofan
armbeingthebestandtheposteriorexpectationoftheregret.
Eveniftheseapproachesefficientlyutilizecurrentavailableinformation,theydonotaimdirectly
toacquiremoreinformation. Wehighlight,however,theinformationdirectedsamplingapproach
(IDS) of Russo and Van Roy [2014], which relies on a measure of the information gain of the
optimalactions. Byleveraginganinformationmeasurethatconsistentlycapturesthespecificproblem
structure,IDScanaddressgeneralclassesofproblems,particularlythosewithacomplexinformation
structure where classic bandit methods fall short. Surprisingly, IDS can even outperform UCB
andThompsonsamplinginclassicbanditproblems. However,likeDMED,thismethodexplicitly
balancesinformationgainwithexpectedlossesinducedbyexploration,andtheefficiencyofpure
information-maximizingstrategiesthusremainstobeproven.
Information-maximizationapproachesprovideadecision-makingstrategyinwhichtheagenttriesto
maximizeinformationaboutoneormorerelevantstochasticvariables. Theinformation-maximizing
principlehasshowntobeefficientinabroadrangeofdomains[HeliasandDahmen,2020,Parretal.,
2022,Hernández-Lobatoetal.,2015,Vergassolaetal.,2007]wheredecisionshavetobetakenin
fluctuatingorunknownenvironments. Thesedomainsinclude,e.g.,roboticsapplications[Zhang
etal.,2015],wheretheabilitytoshareapproximateinformationimprovescollectivedecisions,and
thesearchforolfactorysourcesinturbulentflows[Masson,2013,Reddyetal.,2022].
Inthespecificsettingofbanditproblems,informationmaximizationhasshownpromisingempirical
results, and heuristic arguments support its asymptotic optimality [Reddy et al., 2016, Barbier-
Chebbahetal.,2023]. AsIDS,theyleverageinformationstructuretoprovideaversatiledecision
frameworkwiththecapabilitytoaddressvariousbanditssettings. However,theefficiencyofsuch
a“pureexploration”strategyintermsofregretminimizationhasyettobeproven,andithasbeen
previously argued that it would result in a linear regret [Russo and Van Roy, 2014]. Moreover,
currentinformation-basedalgorithmsoftenrelyoncomplexnumericalintegration,leadingtohigh
computational costs, a significant challenge that information-based methods must overcome. In
thiscontext,weaimtoleveragenewstrategiesderivedfrominformationmaximizationprinciples
focusingonglobalobservables,i.e. variablesdependingonmorethanonebanditarm,thatalleviate
thecomputationalburdenofnumericalevaluationofcomplexfunctionalsandtorigorouslyprove
theirefficiency.
Contributions. Ourmaincontributionisintroducinganewclassofasymptoticallyoptimalalgo-
rithmsthatrelyonapproximationsofafunctionalrepresentingthecurrentinformationofinterest
aboutthewholebanditsystem. Thisapproachisbasedontheentropyoftheposteriormeanvalueof
thebestarm,forwhichweprovideanapproximateexpressiontoenablerobust,easilytunable,and
extendablealgorithmswithadirectanalyticalformulation. Wefocushereonthemulti-armedbandit
problemwithGaussianrewards,forwhichwederiveasimpleapproximateinformationmaximization
algorithm(AIM)andprovideanupperboundonitspseudo-regret,ensuringthatAIMisasymptoti-
callyoptimal. Theinformationfromeacharmisincorporatedinauniqueentropyfunctional,which
showspromisefortacklingmorecomplexbanditsettingssuchaslinearbanditormany-armedbandits.
Thus,ourmainmotivationistodesignananalyticalfunctional-basedalgorithmicprinciple,whichcan
potentiallyaddressproblemswithmorecorrelatedinformationstructuresinthefuture. Additionally,
anotherstrengthofAIMliesinitsshort-timebehavior,whereitshowsstrongperformancesaswe
illustratenumericallyforbothBernoulliandGaussianrewards.
Organization. InSection2,webrieflyreviewtheK-armedbanditsetting. Section3presentsthe
generalprincipleofinformationmaximizationapproaches,originallyinspiredbyboththeinformation
bottleneckprincipleandnavigationinturbulentplumes. Section4upperboundstheregretofAIM,
showing it attainsLai and Robbin’sasymptotic bound. In Section 5, theperformance of AIMis
2
numerically compared with known baselines on multiple examples. Finally, Section 6 discusses
extensionsofAIMtovariousbanditsettings.
2 Setting
WeconsidertheclassicK-armedstochasticbanditgame. Ineachroundt,theagentselectsanarm
a [K] = 1,...,K amongasetofK choicessolelybasedontherewardsofthepreviously
t
∈ { }
pulledarms. ThechosenarmkthenreturnsastochasticrewardX (k),drawnindependentlyofthe
t
previousrounds,accordingtoadistributionν ofmeanµ . WedenotebyN (t)thenumberoftimes
k k k
thearmkhasbeenpulled. Whenclearfromcontext,weomitthedependenceontforsimplicity.
Thegoaloftheagentistomaximizeitscumulativereward,orequivalently,tominimizeitspseudo-
regretuptoroundT,definedas
T
(cid:88)
R(T)=µ
∗
T
−
E[µ
at
], (1)
t=1
whereµ = max µ . Hence,theagentwilloptimizeitschoiceofa relyingontheprevious
∗ i [K] i t
observations up to∈t. For a large family of reward distributions, the asymptotic pseudo-regret is
lower-boundedforanyuniformlygoodpolicyby
liminf R(T) (cid:88) µ ∗ − µ k , (2)
T →∞ ln(T) ≥ k,µk<µ∗ D KL (ν k ∥ ν k∗)
w
tw
h
e
e
e
r
n
e
t
k
h ∗ e r ∈ ew
a
a
r
rd
gm
di
a
s
x
tr i i∈b [ u K t ] io
µ
n i s
,,
o
a
f
n
t
d
he
D
a K rm L
(
s
ν
k k ∥ a
ν
n k d
∗)
k
de
[
n
L
o
a
t
i
es
et
th
a
e
l.,
K
1
u
9
l
8
lb
5
a
].
ck
I
-
n
L
t
e
h
ib
e
le
p
r
ar
d
ti
i
c
v
u
e
l
r
a
g
r
en
ca
c
s
e
e
b
o
e
f
-
∗
Gaussian rewards with equal variances, i.e., ν = (µ ,σ2), the Kullback-Leibler divergence is
i i
D
KL
(ν
k
ν k∗)=(µ
∗
µ
k
)2/(2σ2). N
∥ −
3 Informationmaximizationstrategies
Here,weintroduceentropy-based,informationmaximizationstrategiesandtheirunderlyingphysical
principles. Wethendetailapproximationsleadingtoananalyticalandsimplifiedentropyfunctional,
whichisthebasisoftheAIMalgorithm.
3.1 Algorithmdesignprinciple: physicalintuition
Weaimtodesignafunctionalencompassingthecurrentavailableinformationofthefullsystem.
Inspired by the information maximization principle [Vergassola et al., 2007, Reddy et al., 2016]
whichhasrevealedeffectiveintaxisstrategieswheretheagentneedstofindanemittingodoursource
[Martinezetal.,2014,Cardé,2021,Murlisetal.,1992],werelyonanentropicfunctionalforpolicy
decision. Moreprecisely,wechooseS ,theentropyoftheposteriordistributionofthevalueof
max
themaximalmeanreward,denotedp .
max
Thealgorithmreliesonanarbitrarypriordistributiononthearmmeanrewards. Withindependent
armpriors,theposteriordistributionofthevalueofthemaximalmeanrewardcanbeexpressedas
(cid:18) (cid:19) K
(cid:88) (cid:89)
p
max
(θ)dθ =dP maxµ
k
=θ
t 1
= dP(µ
k
=θ
t 1
) P(µ
j
θ
t 1
), (3)
k |F − |F − ≤ |F −
k=1 j=k
̸
where =σ(X (a ),...,X (a ))denotesthefiltrationassociatedtotheobservationsup
t 1 1 1 t 1 t 1
totime F t− 1. Theassociatedentro−pyre−ads
−
(cid:90)
S = p (θ)lnp (θ)dθ, (4)
max max max
−
Θ
whereΘ = [µ ,µ ]isthesupportofp (whichdependsonthenatureofthegameandcan
inf sup max
beinfinite). Notethat,asexemplifiedbyEquation(3),p includesthearms’priorsanddirectly
max
3
dependsontherewarddistributions.1 Theentropy,S ,isameasureoftheinformationcarriedby
max
allarmsinasinglefunctional,providingaglobalstatedescriptionofthegame.
Ourpolicyaimstominimizetheentropyofp . Forthat,itgreedilychoosesthearmprovidingthe
max
largestexpecteddecreaseinentropy,conditionedonthecurrentknowledgeofthegame,
argminE[S
max
(t) S
max
(t 1)
t 1
,a
t
=k]. (5)
k [K] − − |F −
∈
SimilartoThompsonsampling,itreliesonaBayesianrepresentation. Yet,itdistinguishesitselfby
providingadeterministicdecisionproceduregivenpastobservations. WestressthatS quantifies
max
theavailableinformationabouttheaveragerewardofthebestarm. Thischoicecontrastswithusing
theentropyoftheprobabilityofthebestarm,whichisknowntooverexploreandissuboptimalfor
regretminimization[Reddyetal.,2016]. Becauseofthissuboptimality,approachesbasedonthe
informationonthebestarmfixthisconcernbyincludingtheexpectedregretinthefunctionalto
favorexploitation[RussoandVanRoy,2014]. Furthermore,wearguethatbythedefinitionofp ,
max
theinformationcarriedbythearms’posteriorsissufficientlymixedtoensureanoptimalbehaviour,
asprovedinSection4. Sincethepolicyaimstomaximizetheinformationaboutthebestarm’smean,
itmainlypullsthecurrentbestarmtolearnmoreaboutitsvalue. Onthecontrary,policiesaimingto
identifythebestarmpullworseempiricalarmsmoreoftenbecausetheyareonlyconcernedabout
thearms’order.
The information maximization policy based on Equation (5) has been empirically shown to be
competitive with state-of-the-art algorithms [Reddy et al., 2016] and robust to variations of the
prior[Reddyetal.,2016,Barbier-Chebbahetal.,2023]inclassicbanditgames. However,while
Equation(5)canbenumericallyevaluated,itcannotbecomputedinclosedform,preventingthe
gradient from being analytically tractable. This makes intricate to theoretically bound the regret
eveninthetwo-armedsettinganditalsopreventsthepolicy’sextensiontomorecomplicatedbandit
settings. Additionally, it induces a high computational cost [a trait shared with IDS Russo and
VanRoy,2014],whichbecomesdisadvantageouswhenconsideringalargenumberofarmsandat
largetimes(whenp ispeaked),whereonehastomanagevanishingnumericalprecision,making
max
thenumericalintegrationevenlonger. Finally,theintegralformofS preventsfine-tuning,which
max
couldprovecrucialforachievingorsurpassingtheempiricalstate-of-the-artperformances.
A second simplified and analytical functional mirroring S has to be derived to address these
max
concerns.Thisanalyticalresultstrengthenstheinformationmaximizationprinciple,bothbyproviding
novelalgorithmsthatareanalytical,tractableandcomputationallyefficientwhileconservingthemain
advantagesoftheexactentropy[Reddyetal.,2016]andbymakingtheoreticalanalysistractable.
3.2 Mainelementsoftheentropyanalyticalapproximation
Here,wedeviseasetofapproximationsofp andS togetatractableanalyticalalgorithm.
max max
Giventhatthebestempiricalarmandtheworseempiricalarmshavenotablydistinctcontributionsto
p (Figure1(a)),weapproximatep whileconsideringthecurrentarms’order. Wesortthem
max max
basedontheircurrentposteriormeans,labellingthehighestoneasM (withanempiricalrewardof
t
µˆ )and =[K] M thesetofworseempiricalarms. Ofcourse,M mightdifferfromthe
Mt
A
t
\{
t
}
t
actualoptimalarmk duetotherandomnessintheobservedrewards. Wefocusonapproximating
∗
Equations(3)and(4)whenthebestempiricalarmhasalreadybeenextensivelydrawnmoreoften
thantheotherarms.
The entropy is then decomposed into two tractable terms corresponding to distinct behaviors of
p (θ)whenθvaries:
max
S˜ =S˜ +S˜ , (6)
max body tail
The first term, S˜ , approximates the contribution around the mode of p , while the second
body max
term,S˜ ,quantifiestheinformationcarriedbythetailofp (correspondingtohighrewards,see
tail max
Figure1). Eachofthesetermsthencorrespondstoapartoftheentropywherethedominanttermof
Equation(3)isdistinct(seeAppendixA.1fordetails).
1Intheremainderofthepaper,weconsideranimproperuniformprioroverR,asoftenconsideredwith
Gaussianrewards.
4
Moreprecisely,bydenotingp
i
(θ)dθ =dP(µ
i
=θ
t
)themeanposteriordensityoftheassociated
|F
armi,thetailtermisapproximatedas:
(cid:88) (cid:90) µsup
S˜ = p (θ)lnp (θ)dθ. (7)
tail m m
−
m ∈At µ˜eq,m
whereµ˜ ,giveninAppendixA.6,approximatesµ¯ ,thevalueofθwheretheempiricalbest
eq,m eq,m
armM andtheselectedworsearmmhavethesameprobabilityofbeingthebestarm(seeredand
t
orangecurvesinFigure1(b)). Here,p (θ)istheposteriordensityofthecurrentworsearmevaluated
m
atθ. Roughly,becausethebetterempiricalarmhasbeenpredominantlydrawn,p (θ)decaysfaster
Mt
thanp (θ),resultinginatailterm(seeEquation(7))whosemaincontributionistheworseempirical
m
arm. Theapproximateentropyofthebodycomponentis:
(cid:90)
S˜ = (cid:0) 1 (cid:88) [1 C (θ)] (cid:1) p (θ)lnp (θ)dθ (8)
body
− − −
m Mt Mt
Θ m ∈At
where C
i
(θ) = P(θ > µ
i t
) is the cumulative posterior probability of the mean of the arm i.
| F
Equation(8)istheleading-ordertermofthemodeofp ,whichismainlycontributedtobythe
max
bestempiricalarm.
ThisapproximationofEquation(3)isgoodwhenthebestempiricalarmhasbeenextensivelydrawn
comparedtotheworseempiricalones,correspondingtothesituationencounteredasymptoticallyfor
uniformlygoodalgorithms. Surprisingly,theapproximationcapturedbyEquation(6)isstillaccurate
enoughoutsidethisasymptoticregimetoprovideahigh-performancedecisionscheme.
101
100
10− 1
10− 2
10− 3
10− 4
10− 5
0.2 0.4 0.6 0.8
θ
)θ=
i µ(P
100
10− 1
10− 2
10− 3 pmax
p p M m t 10− 4
µˆMt
µˆm 10− 5
0.75 0.80 0.85 0.90
θ
)θ=
i µ(P
(b) Body Body
Tail Tail
pmax
pMt Cm pmCMt
µ¯eq,m
µ˜eq,m
Figure1: (a)Posteriordistributionsofatwo-armedbanditwithGaussianrewards. Thedottedlines
representtheindividualposteriordistributionsofeacharm,p andp ,whilethecontinuousline
Mt m
representstheposteriorofthemaximummeanrewardofallarms,p (Equation(3)). (b)Zoomof
max
(a)aroundthepointµ¯ wherebotharmshavethesameposteriorprobabilityofbeingthebestone.
eq,m
p C (p C )istheprobabilitythatthemaximalvalueisgivenbythebetter(worse)empirical
Mt m m Mt
arm,andµ˜ istheapproximationtoµ¯ giveninAppendixA.6.
eq,m eq,m
ForGaussianrewarddistributions,onecanderiveananalyticalexpressionforµ˜ (seeAppendixA.2
eq
fordetails),Equations(7)and(8)canbecomputedexactly(seeAppendixA.4). However,atthis
stage,evenifwehavealreadyobtainedaclosed-formexpressionforS ,itremainstooinvolvedto
max
directlycomputeitsexact(discrete)gradientforourdecisionpolicy. Tofinallyderiveasimplified
gradient,weretainonlytheasymptotictermsofEquations(7)and(8)andoftheobtainedgradient
(seeAppendixA.5forderivationdetails). Finally,theexpressionofourapproximatedifferenceof
gradientsoftheentropy,whethercalculatedalongagivenworseempiricalarmkoralongthebest
empiricalarm,reads:
(cid:32) (cid:33)
∆ =
1
ln(
N
Mt
)+
1
min
1(cid:88)At
erfc(δµ˜ ), 1
1
Mt,k
2 N +1 2N 2
eq,m
− K
Mt Mt
m (9)
+Q(N k− 1,ln(N Mt ),δµ˜ eq,k )e − δµ˜2 eq,k +
(cid:88)At
P(N m 1/2,N M− 1 t ,ln(N Mt ),δµ˜ eq,m )e − δµ˜2 eq,m
m
5
whereQandP arepolynomialsgiveninAppendixA.6andδµ˜
eq,i
= √Ni(
√
µ˜
2
eq
σ
,
2
i− µˆi) arestandardized
variableswithµ˜ giveninAppendixA.6. Inwords,∆ approximatesthedifference
eq,i Mt,k
∆
Mt,k
≈
E[S
max
(t+1)
|F
t
,a
t+1
=M
t
]
−
E[S
max
(t+1)
|F
t
,a
t+1
=k],
which is directly related to greedily maximizing theentropy decrease, described in Equation (5).
The decision procedure can be summarized as follows: if ∆ is negative for all k , the
Mt,k
∈ A
t
betterempiricalarmischosenasitreducesthemosttheexpectedvalueoftheapproximateentropy.
Inversely,ifatleastonevalue∆ ispositive,thearmkmaximizing∆ ischosen.
Mt,k Mt,k
In conclusion, we have derived an analytical expression for the information available about the
maximumexpectedrewardofallarms. Weisolatedananalyticallytractablegradientactingasa
decisionprocedurethateludedpreviousapproximatedinformationderivations[Barbier-Chebbah
etal.,2023]. Ourschemeleadstoanefficientnumericalimplementationbyeliminatingnumerical
integrals,substantiallyimprovingthecomputationalspeedofinformationmaximization,acrucial
challengeforinformationmethods,whichisalsostressedbyRussoandVanRoy[2014]fortheIDS
algorithm. WenowprovidethefullimplementationofAIMandbounditsregretinthenextsection.
3.3 Approximateinformationmaximizationalgorithm
Thepseudo-codefortheAIMalgorithmispresentedinAlg.1below.
Algorithm1:AIMAlgorithmforK Gaussianarms
Draweacharmonce,observerewardX (t)andupdatestatisticsµˆ
t t
fort=K+1toT do // Arm selection
ifN N then a M
Mt
≤
m t
←
t
else
M argmax µˆ
Ev t al ← uatem=a k r ∈ g [ m K] ax k ∆ followingEquation(9)
if∆ 0thena k ∈AM t Mt,k
Mt,m
≤
t
←
t
elsea m
t
←
Pulla andobserveX (a )
t t t
µˆ
µˆat Nat +Xt(at),N
N +1 // Update statistics
at ← Nat +1 at ← at
Thebestempiricalarmisdrawnbydefaultifthereexistsoneempiricalarmmthathasbeendrawn
morefrequently N N . Insuchacase,bothentropycomponentsinEquation(6)aremainly
Mt
≤
m
contributedtobyM .
t
4 Regretbound
ThissectionprovidestheoreticalguaranteesontheperformanceofAIM. Moreprecisely,Theorem1
belowstatesthatAIMisasymptoticallyoptimalonthemulti-armedbanditsproblemwithGaussian
rewards.
Theorem1. ForGaussianrewarddistributionswithvarianceσ2,theregretofAIMsatisfiesforany
meanvectorµµµ R K
∈ R(T) (cid:88) 2σ2
limsup ,
ln(T) ≤ µ µ
T →∞ k,µk<µ∗ ∗− k
whereµ =max µ .
∗ k [K] k
∈
WithGaussianrewards,theasymptoticregretofAIMthusexactlyreachesthelowerboundofLai
etal.[1985]givenbyEquation(2). Anon-asymptoticversionofTheorem1isgivenbyTheorem2in
AppendixB.WebrieflysketchtheproofideabelowandrefertoAppendixBforthecompleteproof.
Sketchoftheproof. Weassumeforsakeofclarityinthissketchthatµ >µ foranyk 2. The
1 k
≥
structureoftheproofissimilartotheonefoundinKaufmannetal.[2012a]. Inparticular,thefirst
6
mainstepshowsthattheoptimalarmispulledatleast√ttimeswithhighprobability. Thisresult
holdsbecauseotherwise,thecontributionofarm1tothetailofthedistributionwoulddominatethe
contributionofotherarmsintheapproximateinformation. Inthatcase,pullingthefirstarmwould
naturallyleadtoalargerdecreaseinentropy,whichensuresthattheoptimalarmisalwayspulleda
significantamountoftimes.
Then,weonlyneedtoworkintheasymptoticregimewherearm1ispulledatleast√ttimesandwe
aimatboundingthenumberofpullsonthearmk 2. Additionally,werestrictourselvestoalarge
≥
number(inlog(T))ofpullsonarmkandautomaticallycountthepullsbeforethatpointintheregret.
Asaconsequence,wecanshowthatwithhighprobability:
(cid:115)
6σ2lnt
µˆ µ and µˆ µ +ε
Mt
≥
∗
− √t
k
≤
k
forsomearbitraryε>0. Animportantpropertyofentropyisthatitapproximatesthebehaviourof
theboundofLaietal.[1985]. Moreprecisely,intheasymptoticregime,thedifferenceoftheentropy
incrementsbehavesas
1 Nk(µ1−µk)2 (cid:88) Ni(µ1−µi)2
∆ Mt,k
≈−2N
+Q(N k )e− 2σ2 + P(N i )e− 2σ2 , (10)
Mt
i
̸
=Mt
whereQ andP arepolynomialsthatalsodependonextravariables(seeEquation9). Manipulating
k i
thesepolynomialtermsaltogetherisintricate,butwecanstillshowthatifthearmkispulled,this
meanstheterme−
Nk(µ
2
1
σ
−
2
µk)2
somewhatdominatestheotherexponentsinthesumofEquation(10).
ThisthenimpliesthatN isoforderatmost
2σ2lnT
,asarmkisonlypulledif∆ 0.
k (µ∗
−
µk)2 Mt,k ≥
Our policy is deterministic at each time step while displaying a logarithmic regret, showing that
intuitionsfromRussoandVanRoy[2014]oflinearregretsforstationary(inthesensetheyonly
depend on the posterior distribution) deterministic algorithms was inexact. Moreover, our regret
boundisfrequentist,inoppositiontotheBayesianregretboundobtainedforIDS[RussoandVanRoy,
2014]. As a consequence, AIM does not need a well-specified prior: using a uniform prior, as
doneinourwork,isawell-suitedchoice. Also,therequiredformoftheentropyfortheproofis
general. Thealgorithmyieldsanoptimalregretaslongasweareguaranteedthattheoptimalarmis
pulledasignificantamountoftimeswithhighprobabilityandthattheasymptoticregimebehaves
asEquation(10). Hence,Theorem1willholdforalargefamilyofentropyapproximations[and
likelyforgeneralizationstofreeenergiestoo,asinMasson,2013]aslongastheapproximationis
accurateenoughtonotyieldtrivialbehaviorsintheshorttimeregime. Additionally,theapproximate
frameworkdevisedhereallowsfine-tuningtheformulastoimproveshort-timeperformanceallthe
whileensuringasymptoticoptimalitybykeepingthecorrectasymptoticterms.
5 Experiments
ThissectioninvestigatestheempiricalperformanceofAIM(Alg.1)onnumericalexamples. All
detailsofthenumericalexperimentsaregiveninAppendixD.
We start by considering two arms with Gaussian rewards [Honda and Takemura, 2010] of unit
varianceandmeansµ drawnuniformlyfrom[0,1]. Figure2comparestheBayesianregret(i.e.,
k
the regret averaged over all values of (µ ,µ ) in [0,1] [0,1]) of Alg. 1 with the state-of-the-
1 2
×
art algorithms UCB-tuned, Thompson sampling, Thompson sampling+, KLUCB++, and MED
[Kaufmann et al., 2012b, Pilarski et al., 2021, Cappé et al., 2013, Jin et al., 2022, Honda and
Takemura,2011,MénardandGarivier,2017]. WerefertoAppendixD.4foranoverviewanddetailed
descriptionsofthesebanditalgorithms. TheBayesianregretofAIMempiricallyscalesaslog(T).
Itslong-timeperformancematchesThompsonsampling,asimpliedbyTheorem1,whilerelyingon
a(conditionally)deterministicdecisionprocess. Additionally,AIMoutperformsThompsonsampling
atbothshortandintermediatetimes(seeAppendixD.5.3forfinermeasurements). AIMparticularly
outperformsThompsonsamplingwhenthearmsaredifficulttodistinguishduetotheirmeanrewards
beingclose(seeexamplesinAppendixD.5.1withsingleinstanceregretexperiments).
AIMyieldsstrongperformanceinbothtwo-armedGaussianand50-armedGaussianrewardscase,
aspredictedbyourtheoreticalanalysis. Wenowaimtoextendourmethodtootherbanditsettings.
7
250
200
150
100
50
0
101 103 105 107
t
)t(R i
h
(a) (b)3500
AIM AIM
3000
Thompson Thompson
Thompson+ 2500 Thompson+
Med Med
2000
UCB-Tuned KLUCB
KLUCB++ 1500 KLUCB++
1000
500
Gaussianrewards
Gaussianrewards,K=2 K=50
0
101 103 105
t
Figure2: EvolutionoftheBayesianregretfor(a)2-armedand(b)50-armedbanditwithGaussian
rewards under a uniform mean prior. Regret is averaged over 8000 for (a) and 2000 runs for (b)
Confidenceintervalsshowthestandarddeviation.
Figure3presentstheperformanceofAIMwhenadaptedtoBernoullirewards[Pilarskietal.,2021]
witharmmeansdrawnuniformlyin[0,1]. ThisadaptationisdescribedindetailinSection6below.
TheperformanceofAIMiscomparabletoThompsonsamplinghere. Additionally,AIMperforms
comparablytoThompsonsamplingforclosemeanrewards(seeAppendixD.5.2). Additionally,for
50armswithBernoullirewardsAIM’sshort-timeefficiencyiscomparabletoThompsonsampling,
anditissignificantlymoreefficientatintermediarytimeswhileshowingthesamelogarithmicscaling
atlongtimesasThompsonsampling.
Hence,ouralgorithmshowsstrongempiricalperformancescomparedtostate-of-the-artbaselines
for both Bernoulli and Gaussian rewards while providing outstanding effectiveness when facing
multiple arms with Bernoulli rewards. Experiments suggest that AIM displays the same typical
worst-caseregretasThompsonsampling(whichisminimaxoptimalupto√lnK forsub-Gaussian
rewards), but proving a theoretical bound remains challenging and left for future work. Of note,
similarobservationsaredrawninAppendicesD.5.1andD.5.2fornon-Bayesianversionsoftheregret,
withfixedbanditinstances. TheseobservationssupporttherobustnessofAIManditspotentialfor
extensionstomorecomplexbanditsettings.
50
40
30
20
10
0
101 103 105 107
t
)t(R
i
h
(a) (b)
AIM AIM
300
Thompson Thompson
Thompson+ 250 Thompson+
Med Med
200
KLUCB KLUCB
KLUCB++ 150 KLUCB++
100
50
Bernoullirewards,K=2, Bernoullirewards,K=50
0
101 103 105 107
t
Figure3: EvolutionoftheBayesianregretfor(a)2-armedand(b)50-armedbanditwithBernoulli
rewardsunderauniformmeanprior. Theregretisaveragedover16000runsfor(a)and2000runs
for(b). Confidenceintervalsshowthestandarddeviation.
8
6 Extensions
WeapplyourinformationmaximisationapproachtoBernoullibanditsbothwithtwoandwithmany
arms,whereitshowsstrongempiricalperformances(seeFigure3above). Thissectiondescribesthe
extensionsofAIMtothiscaseanddiscussespotentialextensionstomoregeneralbanditsettings.
Exponentialfamilybandits. SinceEquation(3)explicitlyreliesonthearms’posteriordistributions,
information maximization methods can be directly extended to various reward distributions. In
particular,whentherewarddistributionsbelongtotheexponentialfamily[seeKordaetal.,2013,
andAppendixC.1fordetailsonsuchdistributions],anasymptoticandanalyticalexpressionofthe
entropycanbederivedforthecaseofuniformpriors(seeAppendixCformoredetails),yielding
(cid:34) (cid:35)
S˜ = 1 ln(2πσ¯2) 1 e − NmKL(θˆ mt ,θ˜ eq) + KL(θˆ mt ,θ˜ eq )e − NmKL(θˆ mt ,θ˜ eq) . (11)
max 2 i − N ∂ KL(θˆ ,θ˜ ) (cid:112) 2πσ¯2 ∂ KL(θˆ ,θ˜ ) (cid:112) 2πσ¯2
m 2 mt eq m 2 mt eq m
HereKL(θˆ,θ˜ )istheKullback-Leiblerdivergencebetweentherewarddistributionparameterizedby
i eq
θˆ andθ˜ whereθisthefamilyparameter,and∂ KLdenotesitsderivativew.r.t.thesecondvariable.
i eq 2
AllthestepsleadingtoEquations(7)and(8)inSection3arenotspecifictoGaussianrewards. The
maindifferenceliesintheirasymptoticsimplificationsobtainedafterwardswithLaplace’smethod.
OurimplementationofAIMtoBernoullirewards(aspecificcaseoftheexponentialfamily)with
Equation(11)showscomparableperformancetostate-of-the-artalgorithms(seeFigure2),supporting
itsadaptabilitytogeneralsettings. WebelievethatAIMshouldbeoptimalforallexponentialfamily
rewarddistributionsandgeneralpriordistributionsandthatsimilarprooftechniquescanbeused(see
AppendixCforadetaileddiscussion). However,significantworkstillremainstoensurethatthe
asymptoticregime,whereallarmshavebeensufficientlydrawn,isreachedforanyrewarddistribution
andwillbeaddressedinfuturework.
Other bandit settings. Here, we provide a quick overview of several other bandit settings for
whichapproximateinformationmaximization,adaptedtothespecificbanditproblem,shouldprovide
efficientalgorithms. First,weemphasizethatAIM’s partitioningbetweenbodyandtailcomponents
remainsrelevantevenwhendealingwithheavy-tailed[Leeetal.,2023]ornon-parametricreward
distributions[Baudryetal.,2020].Itshouldthusbeabletoprovidestrongguaranteesinthesesettings,
similarly to Thompson sampling. Secondly, let us stress that information can also be quantified
forunpulledarms,whichmayprovecrucialwhenfacinglargenumbersofarms. Theagentcould
quantifytheinformationofthe“reservoir”ofunpulledarmstoanticipatetheinformationgainedfrom
exploringtheseunpulledarms. Additionally,iftheagenthasaccesstotheremainingtime,itcannot
onlyevaluatetheexpectedinformationgainwhenpullinganarmforasingleroundbutalsoevaluate
theinformationgainofmultiplepullsofthesamearm. Webelievethatsuchaconsiderationmightbe
pivotalwhenfacingmanyarms,sincethelimitedamountoftimedoesnotallowtopullallthearms
[Bayatietal.,2020]sufficiently. Thirdly,inlinearbandits,wherearmsarecorrelatedwitheachother
[Lietal.,2010],AIMwillbeefficientbecausepullingaspecificdirectionprovidesinformationon
correlateddirections,thesharedinformationgaincouldbeleveragedbyinformation-basedmethods
toyieldstrongperformances. Finally,wecouldconsiderpureexplorationproblems[Bubecketal.,
2011,Locatellietal.,2016,Kalyanakrishnanetal.,2012]wheretheagent’sgoalisdirectlylinkedto
aninformationgain,thusmakingtheinformationmaximizationprincipleaninherentcandidatewhen
asuitableentropyisderivedfromtheunderlyingbanditstructureandproblemobjective.
AlastadvantageofAIMliesinitspossibleextensiontomultipleconstraintsthatwouldbeintroduced
usingLagrangemultipliers(orborrowedfromphysicsreasoningbydefiningfreeenergy),further
improvingitsadaptabilitytovarioussettingsandspecificrequirements.
7 Conclusion
Thispaperintroducesanewalgorithmclass,ApproximateInformationMaximization(AIM),which
leverages approximate information maximization of the whole bandit system to achieve optimal
regret performances. This approach builds on the entropy of the posterior of the arms’ maximal
mean,fromwhichweextractasimplifiedandanalyticalfunctionalatthecoreofthedecisionscheme.
Itenableseasilytunableandtractablealgorithms,whichweprovetobeoptimalformulti-armed
9
Gaussianbandits. NumericalexperimentsforBernoullirewardswithtwoorseveralarmsemphasize
therobustnessandefficiencyofAIM.AnadditionalstrengthofAIMliesinitsefficiencyatshorttimes
andwhenthearmshaveclosemeanrewardswhereitoutperformsexistingstate-of-theart. Further
researchshouldfocusonadjustingtheinformationmaximizationframeworktomorecomplexbandit
settings,includingmany-armedbandits,linearbanditsandthresholdingbandits,whereappropriately
selectedinformationmeasurescanefficientlyapprehendthegames’structureandcorrelations.
10
References
ShipraAgrawalandNavinGoyal. ThompsonSamplingforContextualBanditswithLinearPayoffs.
InProceedingsofthe30thInternationalConferenceonMachineLearning,pages127–135.PMLR,
May2013.
P.Auer. Usingupperconfidenceboundsforonlinelearning. InProceedings41stAnnualSymposium
onFoundationsofComputerScience,pages270–279,RedondoBeach,CA,USA,2000.IEEE
Comput.Soc. ISBN978-0-7695-0850-4. doi: 10.1109/SFCS.2000.892116.
AlexBarbier-Chebbah,ChristianL.Vestergaard,andJean-BaptisteMasson.Approximateinformation
forefficientexploration-exploitationstrategies,July2023.
DorianBaudry,EmilieKaufmann,andOdalric-AmbrymMaillard. Sub-samplingforefficientnon-
parametricbanditexploration. InProceedingsofthe34thInternationalConferenceonNeural
InformationProcessingSystems,NIPS’20,pages5468–5478,RedHook,NY,USA,December
2020.CurranAssociatesInc. ISBN978-1-71382-954-6.
MohsenBayati,NimaHamidi,RameshJohari,andKhashayarKhosravi. Unreasonableeffectiveness
ofgreedyalgorithmsinmulti-armedbanditwithmanyarms. AdvancesinNeuralInformation
ProcessingSystems,33:1713–1723,2020.
GuyBresler,GeorgeHChen,andDevavratShah. Alatentsourcemodelforonlinecollaborative
filtering. Advancesinneuralinformationprocessingsystems,27,2014.
SébastienBubeck,RémiMunos,andGillesStoltz. Pureexplorationinfinitely-armedandcontinuous-
armedbandits. TheoreticalComputerScience,412(19):1832–1852,April2011. ISSN0304-3975.
doi: 10.1016/j.tcs.2010.12.059.
OlivierCappé,AurélienGarivier,Odalric-AmbrymMaillard,RémiMunos,andGillesStoltz. Kull-
back–Leiblerupperconfidenceboundsforoptimalsequentialallocation. TheAnnalsofStatistics,
41(3):1516–1541,June2013. ISSN0090-5364,2168-8966. doi: 10.1214/13-AOS1119.
RingT.Cardé. NavigationAlongWindbornePlumesofPheromoneandResource-LinkedOdors.
AnnualReviewofEntomology,66(1):317–336,2021. doi: 10.1146/annurev-ento-011019-024932.
ArnoudV.DenBoer. Dynamicpricingandlearning: historicalorigins,currentresearch,andnew
directions. Surveysinoperationsresearchandmanagementscience,20(1):1–18,2015.
AurélienGarivier. Informationalconfidenceboundsforself-normalizedaveragesandapplications.
In2013IEEEInformationTheoryWorkshop(ITW),pages1–5.IEEE,2013.
Aurélien Garivier and Olivier Cappé. The kl-ucb algorithm for bounded stochastic bandits and
beyond. InProceedingsofthe24thannualconferenceonlearningtheory,pages359–376.JMLR
WorkshopandConferenceProceedings,2011.
MoritzHeliasandDavidDahmen. StatisticalFieldTheoryforNeuralNetworks, volume970of
LectureNotesinPhysics.SpringerInternationalPublishing,Cham,2020.ISBN978-3-030-46443-1
978-3-030-46444-8. doi: 10.1007/978-3-030-46444-8.
JoséMiguelHernández-Lobato,MichaelA.Gelbart,MatthewW.Hoffman,RyanP.Adams,and
ZoubinGhahramani.PredictiveentropysearchforBayesianoptimizationwithunknownconstraints.
InProceedingsofthe32ndInternationalConferenceonInternationalConferenceonMachine
Learning-Volume37,ICML’15,pages1699–1707,Lille,France,July2015.JMLR.org.
JunyaHondaandAkimichiTakemura. AnAsymptoticallyOptimalBanditAlgorithmforBounded
SupportModels. InAdamTaumanKalaiandMehryarMohri,editors,COLT2010-The23rd
ConferenceonLearningTheory,Haifa,Israel,June27-29,2010,pages67–79.Omnipress,2010.
JunyaHondaandAkimichiTakemura. Anasymptoticallyoptimalpolicyforfinitesupportmodelsin
themultiarmedbanditproblem. MachLearn,85(3):361–391,December2011. ISSN1573-0565.
doi: 10.1007/s10994-011-5257-4.
11
TianyuanJin,PanXu,XiaokuiXiao,andAnimaAnandkumar.Finite-TimeRegretofThompsonSam-
plingAlgorithmsforExponentialFamilyMulti-ArmedBandits. AdvancesinNeuralInformation
ProcessingSystems,35:38475–38487,December2022.
ShivaramKalyanakrishnan,AmbujTewari,PeterAuer,andPeterStone. PACSubsetSelectionin
StochasticMulti-armedBandits. Proceedingsofthe29thInternationalConferenceonMachine
Learning,ICML2012,1,January2012.
EmilieKaufmann, NathanielKorda, andRémiMunos. Thompsonsampling: Anasymptotically
optimalfinite-timeanalysis. InInternationalconferenceonalgorithmiclearningtheory,pages
199–213.Springer,2012a.
Emilie Kaufmann, Nathaniel Korda, and Rémi Munos. Thompson Sampling: An Asymptoti-
cally Optimal Finite-Time Analysis. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and
Thomas Zeugmann, editors, Algorithmic Learning Theory, Lecture Notes in Computer Sci-
ence, pages 199–213, Berlin, Heidelberg, 2012b. Springer. ISBN 978-3-642-34106-9. doi:
10.1007/978-3-642-34106-9_18.
EmilieKaufmann,OlivierCappé,andAurélienGarivier. Onthecomplexityofbest-armidentification
inmulti-armedbanditmodels. J.Mach.Learn.Res.,17(1):1–42,January2016. ISSN1532-4435.
Nathaniel Korda, Emilie Kaufmann, and Remi Munos. Thompson sampling for 1-dimensional
exponential family bandits. In Proceedings of the 26th International Conference on Neural
InformationProcessingSystems-Volume1,NIPS’13,pages1448–1456,RedHook,NY,USA,
December2013.CurranAssociatesInc.
TzeLeungLai,HerbertRobbins,etal. Asymptoticallyefficientadaptiveallocationrules. Advances
inappliedmathematics,6(1):4–22,1985.
JongyeongLee,JunyaHonda,Chao-KaiChiang,andMasashiSugiyama. Optimalityofthompson
samplingwithnoninformativepriorsforparetobandits. arXivpreprintarXiv:2302.01544,2023.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalizednewsarticlerecommendation. InProceedingsofthe19thinternationalconferenceon
Worldwideweb,pages661–670,2010.
AndreaLocatelli,MaurilioGutzeit,andAlexandraCarpentier. AnoptimalalgorithmfortheThresh-
oldingBanditProblem.InProceedingsofThe33rdInternationalConferenceonMachineLearning,
pages1690–1698.PMLR,June2016.
DominiqueMartinez,LotfiArhidi,ElodieDemondion,Jean-BaptisteMasson,andPhilippeLucas.
UsingInsectElectroantennogramSensorsonAutonomousRobotsforOlfactorySearches. JoVE
(JournalofVisualizedExperiments),(90):e51704,August2014. ISSN1940-087X. doi: 10.3791/
51704.
Jean-BaptisteMasson. Olfactorysearcheswithlimitedspaceperception. ProceedingsoftheNational
AcademyofSciences,110(28):11261–11266,July2013. doi: 10.1073/pnas.1221091110.
PierreMénardandAurélienGarivier. Aminimaxandasymptoticallyoptimalalgorithmforstochastic
bandits. InProceedingsofthe28thInternationalConferenceonAlgorithmicLearningTheory,
pages223–237.PMLR,October2017.
JohnMurlis,JosephS.Elkinton,andRingT.Cardé.OdorPlumesandHowInsectsUseThem.Annual
ReviewofEntomology,37(1):505–532,January1992. doi: 10.1146/annurev.en.37.010192.002445.
EdwardW.NgandMurrayGeller. AtableofintegralsoftheErrorfunctions. J.RES.NATL.BUR.
STAN.SECT.B.MATH.SCI.,73B(1):1,January1969. ISSN0098-8979. doi: 10.6028/jres.073B.
001.
ThomasParr,GiovanniPezzulo,andKarlJ.Friston. ActiveInference: TheFreeEnergyPrinciple
in Mind, Brain, and Behavior. The MIT Press, March 2022. ISBN 978-0-262-36997-8. doi:
10.7551/mitpress/12441.001.0001.
12
Sebastian Pilarski, Slawomir Pilarski, and Dániel Varró. Optimal Policy for Bernoulli Bandits:
Computation and Algorithm Gauge. IEEE Transactions on Artificial Intelligence, 2(1):2–17,
February2021. ISSN2691-4581. doi: 10.1109/TAI.2021.3074122.
GautamReddy,AntonioCelani,andMassimoVergassola. InfomaxStrategiesforanOptimalBalance
BetweenExplorationandExploitation. JournalofStatisticalPhysics,163(6):1454–1476,April
2016. doi: 10.1007/s10955-016-1521-0.
GautamReddy,VenkateshN.Murthy,andMassimoVergassola. OlfactorySensingandNavigation
inTurbulentEnvironments. AnnualReviewofCondensedMatterPhysics,13:191–213,March
2022. doi: 10.1146/annurev-conmatphys-031720-032754.
DanielRussoandBenjaminVanRoy. LearningtoOptimizeviaInformation-DirectedSampling. In
Z.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Q.Weinberger,editors,Advancesin
NeuralInformationProcessingSystems,volume27.CurranAssociates,Inc.,2014.
IlyaO.Ryzhov,WarrenB.Powell,andPeterI.Frazier. TheKnowledgeGradientAlgorithmfora
GeneralClassofOnlineLearningProblems. OperationsResearch,60(1):180–195,February2012.
ISSN0030-364X. doi: 10.1287/opre.1110.0999.
DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,GeorgevandenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,SanderDieleman,
DominikGrewe,JohnNham,NalKalchbrenner,IlyaSutskever,TimothyLillicrap,Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go
with deep neural networks and tree search. Nature, 529(7587):484–489, January 2016. ISSN
1476-4687. doi: 10.1038/nature16961.
WilliamRThompson. Onthelikelihoodthatoneunknownprobabilityexceedsanotherinviewof
theevidenceoftwosamples. Biometrika,25(3-4):285–294,1933.
NaftaliTishby,FernandoC.Pereira,andWilliamBialek. Theinformationbottleneckmethod,April
2000.
MassimoVergassola,EmmanuelVillermaux,andBorisI.Shraiman. ‘Infotaxis’asastrategyfor
searchingwithoutgradients. Nature,445(7126):406–409,January2007. ISSN1476-4687. doi:
10.1038/nature05464.
SiqiZhang,DominiqueMartinez,andJean-BaptisteMasson. Multi-RobotSearchingwithSparse
BinaryCuesandLimitedSpacePerception.FrontiersinRoboticsandAI,2,2015.ISSN2296-9144.
13
Appendix
Table of Contents
A Towardsananalyticalapproximationoftheentropy 14
A.1 Thepartitioningapproximation . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Asymptoticsoftheintersectionpoint . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Closed-formexpressionsforthemainmode’scontribution . . . . . . . . . . . . 16
A.4 Closedformandasymptoticexpressionforthetail’sentropy . . . . . . . . . . . 18
A.5 Derivationoftheincrementfortheclosed-formexpressionofentropy . . . . . . 18
A.6 Finalexpressionfortheincrementcomparison . . . . . . . . . . . . . . . . . . 21
B ProofofTheorem1 21
B.1 AuxiliaryLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C Generalizationoftheinformationmaximizationapproximation 27
C.1 Asymptoticexpressionforexponentialfamilyrewards . . . . . . . . . . . . . . 28
C.2 Thepartitioningapproximation . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.3 Asymptoticintersectionpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.4 Generalizationofthemainmode’scontribution . . . . . . . . . . . . . . . . . . 29
C.5 Generalizedexpressionfortheentropytail . . . . . . . . . . . . . . . . . . . . 30
C.6 Generalizedformoftheentropyapproximation . . . . . . . . . . . . . . . . . . 30
C.7 Derivationoftheincrementfortheclosed-formexpressionofentropy . . . . . . 31
D Numericalexperiments 31
D.1 Numericalsettings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.2 AIMimplementationdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
D.3 InformationmaximizationapproximationforBernoullirewardswithmorethan
twoarms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
D.4 Overviewofbaselinebanditalgorithms . . . . . . . . . . . . . . . . . . . . . . 35
D.5 Additionalexperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
A Towardsananalyticalapproximationoftheentropy
Inthissection,werecapitulateallthestepsleadingtotheanalyticalexpressionconstitutiveofour
AIMalgorithm. Westress,thatitinvolvesexactderivationsbutalsosimplificationstoconsiderably
simplifythefinalformofAIM.Therefore,alternativeapproachescouldleadtoaslightlydifferent
versionofAIM.However,ourchosenmethodretainstheessentialfeatureswhichemergesinthe
asymptoticregimewhileprovidingasimpleversionofAIM.
A.1 Thepartitioningapproximation
Westartbycommentingonthepartitionschemeandtheapproximationsleadingtothefollowing
body/tailexpressions. Wefirstrecalltheexpressionforp (θ),withthearmsorderedalongM ,
max t
 
(cid:89)At (cid:88)At (cid:89)At
p
max
(θ)=p
Mt
(θ) C
m
(θ)+ C
Mt
(θ)p
m
(θ) C
j
(θ). (12)
m m j=m
̸
whereweremindthatC (θ)isthecumulativeposteriorprobabilityofthemeanofarmm
m
Becauseofitsdependencyalongallarms,thereisnouniquedominantterminEquation(12),and
distinctregimesemergedependingonθandthestateofthegame. Then,weassumetoisolatedistinct
regimes contributing asymptotically to the entropy while significantly simplifying them. It will
considerablysimplifythederivationofananalyticalexpressionforthebody/tailcomponentsinthe
nextsection. Thenextparagraphswillthenpresentheuristicargumentsjustifyingoursimplification
scheme.
14
WestartbyrewritingtheexactentropyexpressionisolatingM :
t
 
(cid:90) (cid:89)At (cid:89)At (cid:88)At (cid:89)At
S
max
=
−
p
Mt
(θ) C
j
(θ)lnp
Mt
(θ) C
j
(θ)+ p
m
(θ)C
Mt
(θ) C
j
(θ)dθ
Θ m j m j=m
̸
 
(cid:88)At (cid:90) (cid:89)At (cid:89)At (cid:88)At (cid:89)At
−
p
i
(θ)C
Mt
(θ) C
j
(θ)lnp
Mt
(θ) C
j
(θ)+ p
m
(θ)C
Mt
(θ) C
j
(θ)dθ.
m Θ j=m j m j=m
̸ ̸
(13)
LetusbrieflycommentonthedifferentcontributionstoEquation(13). Weaimtokeeptheleading
orders of p (θ) when N N 1 and µˆ > µˆ for all m in the set of current worse
max Mt
≫
m
≫
Mt m
empirical arms . Here, the posterior distributions are assumed uni-modal. The first term is
t
A
the leading order in the vicinity of the mode of µˆ . Also, since N > N , p (θ) is more
Mt Mt m Mt
concentratedthanallp (θ),resultinginthedominanceofthesecondterminthedistribution’stail
m
(i.e.,forhighrewards).
Wenowdecomposetheentropyinthebody/tailcomponentsdefinedinthemaintext,thefirstterm
ofEquation(13)willformthebodycomponent, andthesumoverallworseempiricalarmswill
compose the tail. We now define µ¯ the intersection point associated to the arm m verifying
eq,m
C (µ¯ )p (µ¯ )=C (µ¯ )p (µ¯ ). Then,intheasymptoticregime,µ¯ willverify
m eq,m Mt eq,m Mt eq,m m eq,m eq,m
p (θ) p (θ)forθ > µ¯ andp (θ) p (θ)forθ < µ¯ . Again, wewillassumeto
m
≫
Mt eq,m m
≪
Mt eq,m
neglectthetransitionregimewhereµ¯ θwherebothdistributionsareofthesameorderbecause
eq,m
∼
itisnarrow(intheasymptoticregime)andhasverylittleinfluenceonthetotalvalueoftheentropy.
Togetthebodycomponent,weconsiderthefirsttermofEquation(13). Weneglectalltheinnerterms
insidethelogarithmwhichisthendominatedbyM .Next,bynoticingthatC (θ) 1isinthevicinity
t i
≈
ofµˆ ,wemakeafirst-orderexpansionoftheremainingproductalongalltheworseempiricalarms.
Mt
Sincetheinnertermofthebodycomponentisnegligibleforθ >min( µ¯ ,m )(becauseof
eq,m t
{ ∈A }
itsdependencyalongp ),weignorethatoursimplificationisnomorevalidinthisspecificregime
Mt
withoutlossofconsistency. Takentogetherweobtainthebodyexpressionofthemaintext:
S˜ =
(cid:90)
(cid:0) 1
(cid:88)At
[1 C (θ)] (cid:1) p (θ)lnp (θ)dθ. (14)
body
− − −
m Mt Mt
Θ m
Then,weconsidertheadditionalterms(eachdenotedasm)inEquation(13). First,eachtermofthe
sumisnegligibletothefirstoneforθ <µ¯ ,wethenonlykeeptheupperpartoftheintegralwhere
eq,m
θ >µ¯ . BecauseN 1andθ >min( µ¯ ,i=M )>µˆ >µˆ ,weapproximateallthe
eq,m m
≫ {
eq,m
̸
t
}
Mt j
cumulativebyone. Finally,togetasimplifiedexpressionfortheincrement,weassumetoneglectall
theposteriordistributionsexceptforp (θ)insidethelogarithmofthei-thtermandapproximates
i
µ¯ (seenextsection)whichleadstothetailexpression:
eq,m
S˜ =
(cid:88)At (cid:90) µsup
p (θ)lnp (θ)dθ. (15)
tail m m
−
m µ˜eq,m
notethatsomeoftheseposteriordistributions(j = m,M )arenotnegligiblecomparedtop (θ)
t m
̸
atagivenθ. However,thiscross-informationbetweencurrentsuboptimalarmsisasymptotically
negligibleregardingthedecisionprocedure(whichlargelyresumesasbalancingexploitingthebest
empiricalsolutioncomparedtoexploringworseempiricalarms)whileunnecessarilycomplicating
theincrementevaluation.
Finally,weobtainthefullexpressionoftheentropyapproximation:
S˜ =
(cid:90) (cid:18)
1
(cid:88)At
[1 C (θ)]
(cid:19)
p (θ)lnp (θ)dθ
(cid:88)At (cid:90) µsup
p (θ)lnp (θ)dθ. (16)
max
− − −
m Mt Mt
−
i i
Θ m m µ˜eq,m
15
A.2 Asymptoticsoftheintersectionpoint
Inthissection,wederivetheasymptoticexpressionoftheintersectionpoint(definedaboveasµ˜ )
eq,m
wherethedistributionsC (µ˜ )p (µ˜ )andC (µ˜ )p (µ˜ )intersect(attheirhighest
m eq,m Mt eq,m Mt eq,m m eq,m
valueiftheyintersectmorethanonce). Here,weconsiderGaussianrewardsandtheintersection
between M and a given worse empirical arm denoted m. The exact equation verified by the
t
intersectionpointµ¯ is:
eq,m
(cid:112)
N Mt e−
NMt (µ¯eq
2
,
σ
m
2
−µˆMt )2
1
(cid:20)
1+erf
(cid:18)
√N m (µ¯ eq,m
−
µˆ m )
(cid:19)(cid:21)
=
√2πσ2 2 √2σ2
(17)
Nm(µ¯eq,m−µˆm)2 (cid:34) (cid:32)(cid:112) (cid:33)(cid:35)
√N m e− 2σ2 1 1+erf N Mt (µ¯ eq,m − µˆ Mt ) .
√2πσ2 2 √2σ2
TakingthelogarithmofEquation(17)andnormalizingthelasttermleadsto:
 
(cid:16) (cid:17)
N m (µ¯ eq,m
−
µˆ m )2 N Mt (µ¯ eq,m
−
µˆ Mt )2
+
1
ln
N Mt
+ln


1+er
(cid:18)
f √Nm( √ µ¯e 2 q σ ,m 2 − µˆm)
(cid:19)

=0.
2σ2 − 2σ2 2 N m  1+erf √NMt (µ¯eq,m− µˆMt ) 
√2σ2
(18)
Thedistributionsareuni-modal,andassumingthatµˆ >µˆ ,N >N andrecallingthatµ¯
Mt m Mt m eq,m
isthehighestintersection,wegetthatµ¯ >µˆ >µˆ . Botherrorfunctionsarethenboundedin
eq,m Mt m
[0,1],makingthelasttermboundedaswell. Wethenapproximateµ¯ withµ˜ byneglecting
eq,m eq,m
thelastterm,whichleadstothefollowingsolution:
(cid:115)
(cid:18) (cid:19)
N (µˆ µˆ ) N N σ2 N
µ˜ =µˆ + m Mt − m + Mt m (µˆ µˆ )2+ ln Mt .
eq,m Mt N N (N N )2 Mt − m N N N
Mt
−
m Mt
−
m Mt
−
m m
(19)
NotethatEquation(19)reliesonbothµˆ > µˆ andN > N . ForN N ,evenifthe
Mt m Mt m Mt
≤
m
above µ˜ can be computed, it does not quantify the tail contribution. As a matter of fact, for
eq,m
N N ,thetailisalwaysdominatedbyp ,whichmeansthatithasalreadybeenincludedin
Mt
≤
m Mt
themainmodeS˜ . Then,inthisspecificconfiguration,wetakethecontributionofthearmmto
body
S˜ equalsto0(inotherwordsµ˜ =µ ).
tail eq,m sup
A.3 Closed-formexpressionsforthemainmode’scontribution
Here, we derive the S˜ expression given in the main text for Gaussian rewards distribution.
body
InsertingtheGaussianformoftheposteriorintoEquation(8)gives:
S˜ = (cid:90) + ∞ (cid:112) N Mt e− NMt (θ 2σ − 2 µˆMt )2 (cid:18) 1 ln( 2πσ2 ) N Mt (θ − µˆ Mt )2(cid:19)
body − −∞ √2π (cid:32) σ2 (cid:88)At 1 (cid:20) −2 N (cid:18) M √ t N
m
− (θ µˆ
m
2 ) σ (cid:19) 2 (cid:21)(cid:33) × (20)
1 1 erf − dθ,
− 2 − √2σ2
m
Weintegratetheconstantpartofthefirstterm,denotedT bytheuseofthefollowingidentity[Ng
1
andGeller,1969]:
(cid:90) (cid:20) (cid:18) (cid:19)(cid:21)
(θ−θ2)2
(cid:20) (cid:18) (cid:19)(cid:21)
∞ θ θ 1 e− 2V2 θ 2 θ 1 (21)
1+erf − dθ = 1+erf − ,
√2V √2πV √2√V +V
1 2 2 1
−∞
whichleadsto
16
T =
1
ln
(cid:18) 2πσ2(cid:19)(cid:90) + ∞ (cid:112) N Mt e− NMt (θ 2σ − 2 µˆMt )2 (cid:20)
1
1(cid:88)At
1 erf
(cid:18) √N m (θ
−
µˆ m ) (cid:19)(cid:21)
dθ
1
2 N
Mt
−∞
√2πσ2 − 2
m
− √2σ2
   
=
1
ln
(cid:18) 2πσ2(cid:19)
1
(cid:88)At 1
1 erf(cid:113)
√N
m
(µˆ
Mt
−
µˆ
m
)

2 N Mt −
m
2 − 2σ2(
N
1
Mt
+
N
1
m
)
  
=
1
ln
(cid:18) 2πσ2(cid:19)
1
(cid:88)At 1
erfc(cid:113)
√N
m
(µˆ
Mt
−
µˆ
m
)

2 N Mt −
m
2 2σ2(
N
1
Mt
+
N
1
m
)
(22)
Next,weseparatethesecondtermintwopartsT andT ,first:
2,1 2,2
T =
(cid:90) + ∞ (cid:112) N Mt e− NMt (θ 2σ − 2 µˆMt )2 N Mt (θ
−
µˆ Mt )2
dθ =
1
.
(23)
2,1 √2πσ2 2σ2 2
−∞
Then,weintegratebypartstheremainingtermT toobtain:
2,2
T = (cid:88)At (cid:90) ∞ N M 3/ t 2(θ − µˆ Mt )2e− NMt (θ 2σ − 2 µˆMt )2 (cid:20) 1 erf (cid:18) √N m (θ − µˆ m ) (cid:19)(cid:21) dθ
2,2 − 4σ2 √2πσ2 − √2σ2
m −∞
=
(cid:88)At (cid:90) ∞ 1 (cid:112) N Mt e− NMt (θ 2σ − 2 µˆMt )2 (cid:20)
1 erf
(cid:18) √N m (θ
−
µˆ m ) (cid:19)(cid:21)
− 4 √2πσ2 − √2σ2
m −∞ (24)
(cid:112) NMt (θ−µˆMt )2 Nm(θ−µˆm)2
(θ
−
µˆ Mt ) N Mt N m e− 2σ2 − 2σ2
dθ
− 2 2πσ2
= − (cid:88) M At t 1 4 erfc  (cid:113) 2σ µˆ 2 M ( N t 1 M − t µˆ + m N 1 m )   − 2N Mt √ (µˆ 2 M π t ( − N σ M 2 µˆ t m + )σ N 2 σ m 2 )3/2 e −2σ2 (µ ( ˆM NM 1 t − t µˆ + m N ) 1 m 2 ) ,
wherewealsorelyontheidentityofEquation(21).
CombiningEquations(22)to(24)leadstotheanalyticalexpressionofthebodycomponent.
(cid:34) (cid:32)(cid:112) (cid:33)(cid:35)
S˜ =
1
ln(
2πσ2e
) 1
1(cid:88)At
erfc
N
m
N
Mt
(µˆ
Mt −
µˆ
m
)
body (cid:112)
2 N Mt − 2 m 2σ2(N m +N Mt ) (25)
(cid:88)At (cid:112) N Mt N m 3/2(µˆ Mt − µˆ m ) e− Nm 2 N σ2 M (N t ( m µˆM +N t − M µˆ t m ) )2 .
− 2σ√2π(N +N )3/2
m Mt m
Tofinallygetanasymptoticandsimplifiedexpressionofthebodycomponent,weneglectthesecond
term. Then, sinceµ˜ µˆ andN N asymptotically, weapproximatethefirst
termas:
eq,m
NM −
t
→
→∞
Mt m
≪
Mt
S˜ =
1
ln(
2πσ2e
)
(cid:34)
1
1(cid:88)At
erfc
(cid:18) √N
m
(µ˜
eq,m −
µˆ
m
) (cid:19)(cid:35)
. (26)
b
2 N
Mt
− 2
m
√2σ2
Thislastapproximationwillenabletoprovideananalyticallytractablegradientwithoutalteringthe
asymptoticbehaviorexpectedatlargetimesfortheentropymeasure.
17
A.4 Closedformandasymptoticexpressionforthetail’sentropy
Thecontributionfromthetailcanbederivedexactlyandreads:
S˜ = (cid:88)At (cid:90) ∞ √N m e− Nm(θ 2σ − 2 µˆm)2 (cid:20) 1 ln( 2πσ2 )+ N m (θ − µˆ m )2(cid:21) dθ
tail
m µ˜eq,m
√2πσ2 2 N
m
2σ2
(cid:88)At 1 (cid:18) 2πσ2e (cid:19) (cid:18) √N m (µ˜ eq,m µˆ m ) (cid:19) (27)
= ln erfc −
4 N
m
√2σ2
m
√N
m
(µ˜
eq,m
µˆ
m
) Nm(µ˜eq,m−µˆm)2
+ − e− 2σ2 .
2√2πσ2
Togetasimplifiedanalyticalexpressionofthetailcomponent,weonlykeepthesecondtermsinceit
dominatestheothersasymptotically,
S˜ t = (cid:88)At √N m (µ˜ eq,m − µˆ m ) e− Nm(µ˜eq 2 , σ m 2 −µˆm)2 . (28)
2√2πσ2
m
Takenaltogether,Equations(26)and(28)leadtothedesiredsimplifiedapproximationoftheentropy:
S˜ =S˜ +S˜ . (29)
m b t
A.5 Derivationoftheincrementfortheclosed-formexpressionofentropy
SinceEquations(26)and(28)exhibitsimpleclosed-formexpressions,itbecomespossibletoderive
anexplicitexpressionofitsexpectedincrement. Here,weagainconsidercontinuousGaussianreward
distributions.
Westartbyderivingtheincrementalongthebetterempiricalarm, ∆ S˜ . Theposteriorofthe
Mt m
rewardobtainedattimet+1isapproximatedasaGaussianofvarianceσ2andcentredaroundµˆ ,
Mt
leadingto:
(cid:90) µ2 (cid:20) (cid:21)
∆ S˜ = ∞ e−2σ2 S˜ (µˆ + µ ,N +1,...) S˜ (µˆ ,N ,...) dµ. (30)
Mt m
√2πσ2
m Mt
N
Mt
+1
Mt
−
m Mt Mt
−∞
wherethedotsrunsoveralltheworseempiricalarmsvariablesremainingconstantwhenthebest
empiricalarmisdrawnattimet+1.
For the sake of simplicity, we neglect the variations of all the subdominant terms inside
all µ˜ meaning we approximate them as µ˜ (µˆ + µ ,N + 1,µˆ ,N )
µ˜ e ( q µˆ ,m ,N ,µˆ ,N )+ µ , after observ e i q n ,m g a r M ew t ard N µ M w t + h 1 en p M ul t ling the m arm M m fo ≈ r
eq,m Mt Mt m m NMt +1 t
the(N +1)thtime.
Mt
ByuseoftheidentityEquation(21),thegradientofthebodycomponent∆ S˜ canberewrittenas:
Mt b
  
∆ Mt S˜ b = 1
2
ln(
N
2
M
π
t
σ
+
2e
1
)1
−
1
2
(cid:88)A
m
t erfc
√
√
2
N
σ2
m (cid:113) (µ˜
1
e
+
q,m
(N
−
M N t
µˆ
m +
m
1
)
)2

(31)
1 2πσ2e (cid:34) 1(cid:88)At (cid:18) √N
m
(µ˜
eq,m
µˆ
m
) (cid:19)(cid:35)
ln( ) 1 erfc − .
− 2 N
Mt
− 2
m
√2σ2
18
Theincrementofthetailcomponentalongthebetterempiricalarmcanbecalculatedas:
∆ Mt S˜ t = (cid:88)At (cid:90) ∞
√
e−
2π
2 µ σ
σ
2 2
2
√N m ( NM µ t
2
+
√
1
2
+
πσ
µ˜
2
eq,m − µˆ m ) e− Nm(µ˜eq,m+ 2 N σ M 2 µ t +1−µˆm)2 dµ
m −∞
(cid:88)At √N
m
(µ˜
eq,m
µˆ
m
) Nm(µ˜eq,m−µˆm)2
− e− 2σ2
− 2√2πσ2
m
(32)
= (cid:88)At e − Nm 2σ2 ( (cid:32) µ˜ 1 e + q,m (1+ − N N µˆ m m M ) t 2 )2 (cid:33)(cid:114) N m (µ˜ eq,m − µˆ m )
8πσ2(1+ Nm )3/2
m (NMt +1)2
(cid:88)At √N
m
(µ˜
eq,m
µˆ
m
) Nm(µ˜eq,m−µˆm)2
− e− 2σ2 .
− 2√2πσ2
m
Next,weconsidertheincrementevaluationalongagivenworseempiricalarmdenotedbyk,
(cid:90) µ2 (cid:20) (cid:21)
∆ S˜ = ∞ e−2σ2 S˜ (...,µˆ + µ ,N +1,...) S˜ (...,µˆ ,N ,...) dµ. (33)
k m m k k m k k
√2πσ2 N
k
+1 −
−∞
Weherealsoneglectthevariationsofthesubdominantterminsideµ˜ . Westartbyconsidering
eq,m
theincrementofthebodycomponent:
(cid:34) (cid:32) (cid:33)(cid:35)
1 2πσ2e 1 (N +1)(µ˜ µˆ )
∆ k S˜ b = ln( ) 1 erfc k (cid:112) eq,k − k
2 N Mt − 2 2σ2(N k +2) (34)
1 2πσ2e (cid:20) 1 (cid:18) √N (µ˜ µˆ ) (cid:19)(cid:21)
k eq,k k
ln( ) 1 erfc − .
− 2 N
Mt
− 2 √2σ2
Ofnote,allothertermsinthesumindependentofindexkremainconstant,showingnoincrement.
Finally,weconsidertheassociatedtailcomponentoftheincrementalongk:
∆ k S˜ t = (cid:90) ∞ e−2 µ σ 2 2 √N k +1( Nk µ +1 +µ˜ eq,k − µˆ k ) e− (Nk+1)(µ˜eq, 2 k σ + 2 Nk µ +1−µˆk)2 dµ
√2πσ2 2√2πσ2
−∞
√N
k
(µ˜
eq,k
µˆ
k
) Nk(µ˜eq,k−µˆk)2
− e− 2σ2
− 2√2πσ2
(35)
(Nk+1)2(µ˜eq,k−µˆk)2 (1+N
k
)2(µ˜
eq,k
µˆ
k
)
=e− (Nk+2) 2σ2 −
√8πσ2(2+N )3/2
k
√N
k
(µ˜
eq,k
µˆ
k
) Nk(µ˜eq,k−µˆk)2
− e− 2σ2
− 2√2πσ2
Asforthebodyincrement,allothertermsinthesumindependentofindexkremainconstant,showing
noincrement.
Takenaltogether,Equations(31),(32),(34)and(35)leadtothefinalanalyticalexpressionofthe
increment:
19
 
∆ Mt,k = 1
2
ln(
N M
N
t
M
+
t
1
)
−
1
4
ln(
N
2
M
π
t
σ
+
2e
1
) (cid:88)A
m
t erfc(cid:113) √
2
N
σ
m
2(
(
1
µ˜
+
eq,
(
m
NM
−
N
t
m +
µˆ
1
m
)2
)
)

1 2πσ2 (cid:88)At (cid:20) (cid:112) µ˜ eq,m µˆ m (cid:21)
+ ln( ) erfc N −
m
4 N
Mt
m
√2σ2
+ (cid:88)At e − Nm 2σ2 (µ˜ (1 e + q,m (1+ − N N µˆ m m M ) t 2 )2) (cid:114) N m (µ˜ eq,m − µˆ m )
8πσ2(1+ Nm )3/2
m (NMt +1)2
(cid:88)At √N
m
(µ˜
eq,m
µˆ
m
) Nm(µ˜eq,m−µˆm)2
− e− 2σ2
− √8πσ2
m
1 2πσ2e (cid:34) (cid:32) (N k +1)(µ˜ eq,k µˆ k ) (cid:33) (cid:18) (cid:112) µ˜ eq,k µˆ k (cid:19)(cid:35)
+ ln( ) erfc (cid:112) − erfc N k −
4 N Mt 2σ2(2+N k ) − √2σ2
(Nk+1)2(µ˜eq,k−µˆk)2 (1+N
k
)2(µ˜
eq,k
µˆ
k
) √N
k
(µ˜
eq,k
µˆ
k
) Nk(µ˜eq,k−µˆk)2
e− (Nk+2) 2σ2 − + − e− 2σ2
− √8πσ2(2+N )3/2 2√2πσ2
k
(36)
To obtain a simplified expression, we expand to the first order each component of the different
componentsofEquation(36)denotedT ,T ,T ,T . Theformerisgivenby:
1 2 3 4
 
1 2πσ2e (cid:88)At √N
m
(µ˜
eq,m
µˆ
m
)
T 1 = ln( ) erfc(cid:113) − 
−4 N Mt +1 m 2σ2(1+ (NM N
t
m +1)2 )
1 2πσ2e (cid:88) K (cid:20) (cid:112) µ˜ eq,m µˆ m (cid:21)
+ ln( ) erfc N −
m
4 N
Mt m
̸
=Mt
√2σ2
(37)
(cid:88)At 1 (cid:18) √N
m
(µ˜
eq,m
µˆ
m
) (cid:19)
erfc −
≈
m
4N
Mt
√2σ2
1 2πσ2e N
m
√N
m
(µ˜
eq,m
µˆ
m
) Nm(µ˜eq,m−µˆm)2
− 4
ln(
N
Mt
)
N
M
2
t
√2πσ2
− e− 2σ2 .
Nextweconsiderthesecondcomponent,whichreads:
T 2 = (cid:88)At e − Nm 2σ2 (µ˜ (1 e + q,m (1+ − N N µˆ m m M ) t 2 )2) (cid:114) 8 N π m σ2(1 ( + µ˜ eq,m N − m µˆ m ) ) 3/2
m (NMt +1)2
(cid:88)At √N
m
(µ˜
eq,m
µˆ
m
) Nm(µ˜eq,m−µˆm)2
(38)
− e− 2σ2
− 2√2πσ2
m
≈ (cid:88)At e− Nm (µ˜eq,m 2σ − 2 µˆm)2 (cid:114) 8 N π m σ2 (µ˜ eq,m − µˆ m ) (cid:20) − 3 2N N 2 m + (µ˜ eq,m 2σ − 2 µˆ m )2 N N 2 m 2 (cid:21) .
m Mt Mt
Nextweconsiderthethirdterm,denotedT ,whichreads:
3
1 2πσ2e (cid:34) (cid:32) (N k +1)(µ˜ eq,k µˆ k ) (cid:33) (cid:18) (cid:112) µ˜ eq,k µˆ k (cid:19)(cid:35)
T 3 = ln( ) erfc (cid:112) − erfc N k −
4 N Mt 2σ2(2+N k ) − √2σ2 (39)
1 2πσ2e 1 (cid:18) √N
k
(µ˜
eq,k
µˆ
k
) (cid:19) Nk(µ˜eq,k−µˆk)2
≈−4
ln(
N
Mt
)
N
k
2 √2πσ2
− e− 2σ2
20
Finally,thelasttermT reads
4
(Nk+1)2(µ˜eq,k−µˆk)2 (1+N
k
)2(µ˜
eq,k
µˆ
k
) √N
k
(µ˜
eq,k
µˆ
k
) Nk(µ˜eq−µˆk)2
T 4 = e− (Nk+2) 2σ2 − + − e− 2σ2
− √8πσ2(2+N )3/2 2√2πσ2
k (40)
≈ e− Nk (µ˜eq, 2 k σ − 2 µˆk)2 (cid:114) 8 N πσ k 2 (µ˜ eq,k − µˆ k ) (cid:20) N 1 + N 1 (µ˜ eq,k 2σ − 2 µˆ k )2(cid:21) .
k k
Takenaltogether,wefinallyobtainthefollowingsimplifiedincrement:
∆∼ =
1
ln(
N
Mt )+
1 (cid:88)At 1
erfc
(cid:18) √N
m
(µ˜
eq,m −
µˆ
m
) (cid:19)
Mt,k
2 N
Mt
+1 2N
Mt
m
2 √2σ2
+ (cid:88)At N m 3/2(µ˜ eq,m − µˆ m ) e− Nm (µ˜eq,m 2σ − 2 µˆm)2 (cid:20) 1 ln( N Mt ) 3 + N m (µ˜ eq,m − µˆ m )2(cid:21) (41)
√2πσ2N2 4 2πσ2e − 4 4σ2
m Mt
+ µ˜ eq,k − µˆ k e− Nk (µ˜eq, 2 k σ − 2 µˆk)2 (cid:20) 1 ln( N Mt )+ 1 + (µ˜ eq,k − µˆ k )2(cid:21)
√2πσ2N
k
4N
k
2πσ2e 2 4σ2
By noticing that the sum of second term should account for the tail contribution along the body
increment,itshouldn’tbeallowedtobesuperiortoone. Thenweassumetobounditbytakingthe
minimumcomparedto1 1/K.
−
A.6 Finalexpressionfortheincrementcomparison
Takenaltogether,itleadstoexpressionusedforAIMformultiplegaussianarms:
∆ =
1
ln(
N
Mt
)+
1
min
(cid:32) (cid:88)At 1
erfc
(cid:18) √N
m
(µ˜
eq,m
−
µˆ
m
) (cid:19)
,1
1 (cid:33)
Mt,k
2 N
Mt
+1 2N
Mt
m
2 √2σ2 − K
+ (cid:88)At N m 3/2(µ˜ eq,m − µˆ m ) e− Nm (µ˜eq,m 2σ − 2 µˆm)2 (cid:20) 1 ln( N Mt ) 3 + N m (µ˜ eq,m − µˆ m )2(cid:21) (42)
√2πσ2N2 4 2πσ2e − 4 4σ2
m Mt
+ µ˜ eq,k − µˆ k e− Nk (µ˜eq, 2 k σ − 2 µˆk)2 (cid:20) 1 ln( N Mt )+ 1 + (µ˜ eq,k − µˆ k )2(cid:21)
√2πσ2N
k
4N
k
2πσ2e 2 4σ2
with
(cid:118)
µ˜ =µˆ + N i (µˆ Mt − µˆ i ) + (cid:117) (cid:117) (cid:116) N Mt N i (µˆ Mt − µˆ i )2 + σ2ln (cid:16) N N M i t (cid:17) . (43)
eq,i Mt N N (N N )2 N N
Mt
−
i Mt
−
i Mt
−
i
B ProofofTheorem1
ThissectionprovidesthecompleteproofofTheorem1. Moreprecisely,itprovesthemorerefined
Theorem2below.
Theorem2. Foranymulti-armedbanditswithGaussianrewardsofvarianceσ2andmeanvector
µµµ R K,foranyε (0,1),thereexistsaconstantC(µµµ,ε) Rdependingsolelyonµµµandεsuch
∈ ∈ 2 ∈
thatforanyT N
∈
(cid:34) (cid:35)
(cid:88) 2σ2 lnT 2σ2 lnlnT
R(T) + +C(µµµ,ε).
≤ (1 ε)(µ µ ) (1 ε)(µ µ )
k,µk<µ∗ − ∗− k − ∗− k
21
Proof. Wedenoteinthewholeproof = k [K] µ =µ . For∆ =µ µ ,theregret
∗ k ∗ k ∗ k
M { ∈ | } −
canthenbewrittenas
(cid:88)
R(T)= ∆ kE[N
k
(T)].
k,∆k>0
Wedecomposethisexpectationin4termsasfollows
(cid:32) (cid:115) (cid:33)
(cid:88) T (cid:16) (cid:17) (cid:88) T 6σ2lnt
E[N k (T)] P i ∗ ,N i (t) √t + P µˆ k (t) µ ∗ ,a t =k
≤ ∀ ∈M ≤ ≥ − √t
t=1 t=1
(cid:32) (cid:115) (cid:33)
(cid:88) T 6σ2lnt (cid:88) T
+ P i ∗ ,µˆ i (t) µ i + P( k (t)),
∃ ∈M ≤ − N (t) E
i
t=1 t=1
where
(cid:40) (cid:115) (cid:41)
6σ2lnt
(t):= i ,N (t) √tandµˆ (t) µ µˆ (t),a =k .
k ∗ i i ∗ k t
E ∃ ∈M ≥ ≥ − N (t) ≥
i
Thisinequalitycomessimplybynoticingtheevent a =k isincludedintheunionofthe4other
t
{ }
events. Lemmas1,3and4allowtorespectivelyboundthefirst,secondandthirdsumsbyaconstant
C(µµµ)dependingsolelyonµµµ,sothat
T
(cid:88)
E[N
k
(T)] P(
k
(t))+C(µµµ).
≤ E
t=1
ThankstoLemma5,thereexistconstantst(µµµ),n(µµµ)dependingsolelyonK and∆ suchthat
k
T T
(cid:88) (cid:88)
P(
k
(t)) t(µµµ)+ P(
1
(t))+P(
2
(t)),
E ≤ G G
t=1 t=1
where
(t)= µ µˆ (t) ε∆ ,a =k ,
1 k k k t
G { − ≤− }
2σ2
(cid:0) (cid:1)
(t)= N (t) lnt+lnlnt +n(µµµ),a =k .
G 2 { k ≤ (1 2ε)2∆2 t }
− k
Now, we bound individually the sum corresponding to each of these 2 events. The first one can
be bounded using Hoeffding’s inequality. Indeed, for independent random variables Z (n)
k
(µ ,σ2),itreadsas: ∼
k
N
(cid:32) (cid:33)
T T n
(cid:88) (cid:88) 1 (cid:88)
P(µ k µˆ k (t) ε∆ k ,a t =k) P Z k (i) µ k ε∆ k
− ≤− ≤ n − ≥
t=1 n=1 i=1
(cid:88) T nε2∆2 k
e− 2σ2
≤
n=1
1
.
≤ e ε 2 2 σ ∆ 2 2 k − 1
Theboundofthesecondtermisboundedas
(cid:34) (cid:35)
(cid:88) T P( E k (t)) ≤ E (cid:88) T 1 { N k (t) ≤ (1 2 2 σ ε 2 )2∆2 (cid:0) lnt+lnlnt (cid:1) +n(µµµ),a t =k }
t=1 t=1 − k
2σ2
(cid:0) (cid:1)
lnT +lnlnT +n(µµµ)+1.
≤ (1 2ε)2∆2
− k
WrappingupeverythingfinallyyieldsthatforsomeconstantC(µµµ,ε)dependingsolelyonµµµ,ε,
2σ2
R(T) (lnT +lnlnT)+C(µµµ,ε).
≤ (1 2ε)2∆
k
−
ThisconcludestheproofofTheorem2withthereparameterizationε 1 (1 2ε)2.
← − −
22
B.1 AuxiliaryLemmas
SimilarlytotheproofofThompsonsampling,thefirstpartoftheproofshowsthattheoptimalarmis
atleastpulledapolynomialnumberoftimeswithhighprobability. Werecallthatwedenoteinthis
wholesection =argmax µ .
∗ k k
M
Lemma1. ThereexistsaconstantC (µµµ)dependingsolelyonthemeanvectorµµµsuchthat
0
(cid:88)∞
P( i
∗
,N
i
(t) √t) C
0
(µµµ).
∀ ∈M ≤ ≤
t=1
Proof. Lett (µµµ)bealargeconstantthatdependssolelyonµµµ. Intheremainingoftheproof, we
0
assumeatsomepointsthatt (µµµ)ischosenlargeenough(butonlylargerthanathresholddepending
0
onµµµ)suchthatsomeinequalitieshold. Wealsoassumeinthefollowing,withoutlossofgenerality,
thatµ =µ ,i.e.,1 .
1 ∗ ∗
∈M
Assumethatfort t (µµµ),N (t) √tforalli . Letthenkbethemostpulledarmattimet,
0 i ∗
≥ ≤ ∈M
i.e.,k argmax N (t)(ifmultiplearmsmaximisethenumberofpulls,weselecttheonesuchthat
∈ j j
itslastpullhappenedtheearliest). NecessarilyN (t) t . Wecanchooset (µµµ)largeenoughso
k ≥ K 0
that t >√tandthus∆ >0. Lett tbethelasttimekwaspulled. Bydesign,kalsomaximised
K k ′ ≤
thenumberofpullsthen,sothatk argmax µˆ (t). Moreover,N (t) √tforalli and
∈ j j ′ i ′ ≤ ∈M ∗
N k (t ′ ) ≥ K t − 1. Fort 0 (µµµ)largeenough,thisyieldsN k (t ′ ) ≥ N i (t ′ )foralli ∈M ∗ anda t′ =k.
Thearmkisthuspulledattimet,inparticularbecauseS S (i.e.,∆ S 0),where
′ k 1 k,1
≥ ≤
(cid:18) (cid:19)  (cid:32)(cid:112) (cid:33) 
S
k
=
1
ln 1+
1 1
min
1(cid:88)
erfc
N
i
(t
′
)(µ˜
eq,i −
µˆ
i
)
, 1
1
,
2 N
k
(t
′
) − 2N
k
(t
′
) 2 √2σ2 − K
i=k
̸
S 1 = (cid:88) (cid:114) N 2π i ( σ t ′ 2 ) (µ˜ eq,i − µˆ i )e− Ni(t′) (µ˜eq 2 ,i σ − 2 µˆi)2 (cid:20) 1 4 ln( N 2π k σ (t 2 ′ e ) ) N N 2 i ( ( t t ′ ) ) − 3 4N N 2 i ( ( t t ′ ) ) + (µ˜ eq, 4 i σ − 2 µˆ i )2N N i 2 2 ( ( t t ′ ) ) (cid:21)
i=k k ′ k ′ k ′
̸
+e− N1(t′) (µ˜eq, 2 1 σ − 2 µˆ1)2 (cid:114) N 2π 1 ( σ t 2 ′ ) (µ˜ eq,1 − µˆ 1 ) (cid:20) 1 4 ln( N 2π k σ (t 2 ′ e ) ) N2 1 (t) + 2N 1 (t) + N 1 (t) (µ˜ eq, 4 1 σ − 2 µˆ 1 )2(cid:21) .
1 ′ 1 ′ 1 ′
Tosimplify,notethatS 1 . MoreoversinceN (t) t 1 2πe4σ2foralargeenough
k ≤ 2Nk(t′) k ′ ≥ K − ≥
choiceoft (µµµ),S canbeeasilylowerboundedas
0 1
1 (µ˜
eq,1
µˆ
1
) N1(t′)(µ˜eq,1−µˆ1)2
S 1 (cid:112) − e− 2σ2 .
≥ 2 2πσ2N (t)
1 ′
Sowefinallyhavethefollowinginequalityattimet:
′
1 (cid:112) (µ˜ eq − µˆ 1 ) e− N1(t′)( 2 µ˜ σ e 2 q−µˆ1)2 . (44)
N 2 (t ′ ) ≥ 2πσ2N 1 (t ′ )
RecallthatN (t) t 1,sothatEquation(44)canberewrittenas
2 ′ ≥ K −
t x˜
N (t) ( 1) e
x˜2
, (45)
1 ′ −
≥ K − √π
where x˜ = √N1(t √ ′) 2 ( σ µ˜ 2 eq− µˆ1) . In the following, we will show that x˜ ∈ [x˜ min ,x˜ max ] ⊂ R+ . By
analysingthevariationsofx xe x2,thiswillimplythat
−
(cid:55)→
t 1
N 1 (t ′ ) K − min x˜ min e − x˜2 min,x˜ max e − x˜2 max . (46)
≥ √π { }
Forthelowerbound,thedefinitionofµ˜ andthefactthatN (t) 1directlyimpliesthat
eq,1 1 ′
≥
(cid:118)
x˜
(cid:117) (cid:117)
(cid:116)
ln(N N k 1( ( t t ′ ′ ) ))
=Ω
(cid:32)(cid:114) ln(t) (cid:33)
.
≥ 2(Nk(t′) 1) t
N1(t′) −
23
Moreover,bysubadditivityofthesquareroot:
(cid:118)
x˜ (cid:114) N 1 (t ′ ) (µˆ µˆ ) (cid:32) 1+ N 1 (t ′ )+ (cid:112) N 1 (t ′ )N k (t ′ ) (cid:33) + (cid:117) (cid:117) (cid:116) N 1 (t ′ )ln(N N k 1( ( t t ′ ′ ) )) (47)
≤ 2σ2 2 − 1 N (t) N (t) 2(N (t) N (t))
k ′ − 1 ′ k ′ − 1 ′
(cid:114) (cid:32)(cid:112) (cid:33)
N (t) (cid:16) (cid:17) Kln(t)
≤ 2 1 σ2 ′ (µˆ k − µˆ 1 ) 1+Kt − 1 4 + O t1
4
. (48)
Letusnowconsidertheevents,for∆ =min ∆ ,
min j,∆j>0 j
(cid:40) (cid:115) (cid:41)
2σ2(ln(t) lnln(t)) ∆
(t):= i ∗ , s t,µˆ i (s) µ i − min , (49)
H∗ ∃ ∈M ∃ ≤ − ≤− N
i
(s) − 3
(cid:26) (cid:27)
t ∆
(t):= s t, 1 N (s) tandµˆ (s) µ k . (50)
k k k k
H ∃ ≤ K − ≤ ≤ − ≥ 3
Assumeinthefollowingthat (t) (t). Thisimpliesthat
k
¬H∗ ∩¬H
(cid:115)
∆ 2σ2(ln(t) lnln(t))
µˆ µˆ k + − . (51)
k 1
− ≤− 3 N (s)
1
Inparticular,
(cid:114)
N 1 (t ′ ) (µˆ µˆ ) (cid:112) ln(t) lnln(t),
2σ2 2 − 1 ≤ −
whichimpliesthatx˜ (cid:112) ln(t) lnln(t)+ (cid:16) K (cid:112) ln(t)t −4 1 (cid:17) . Usingthelowerandupperbounds
≤ − O
onx˜,wehavethankstoEquation(46)thatunder (t) (t),
k
¬H∗ ∩¬H
3
ln2(t)
N (t)=Ω( ).
1 ′
K
For a large enough choice of t (µµµ), this last equality along with Equation (51) actually yield
0
µˆ µˆ <0,whichcontradictsthebeginningoftheproof(kbeingbestempiricalarmattimet).
k 1 ′
−
Bycontradiction,wethusshowedthefollowingeventinclusionfort t (µµµ):
0
≥
(cid:110) (cid:111)
i ,N (t) √t (t) (t). (52)
∗ i k
∀ ∈M ≤ ⊂H∗ ∪H
Lemma1thenfollows,thankstoLemma2below,
(cid:88)∞ (cid:88)∞
P( i
∗
,N
i
(t) √t) t
0
(µµµ)+ P( (t))+P(
k
(t)).
∀ ∈M ≤ ≤ H∗ H
t=1 t=t0(µµµ)+1
Lemma2. Foranyb (0,1)andtheevents (t), (t)definedinEquations(49)and(50),there
k
existconstantsc and ∈ c dependingsolelyon H µµµ∗such H that
1 2
(cid:88)∞ (cid:88)∞
P( (t)) c
1
and P(
k
(t)) c
2
foranyk
∗
.
H∗ ≤ H ≤ ̸∈M
t=1 t=1
Proof. ThetwoboundsdirectlyresultfromHoeffding’sinequality. Considerindependentrandom
variables(Z (n)) whereZ (n) (µ ,σ2). Letusfirstboundtheprobabilityof (t),
j n N,j [K] j j k
whichissimpler. ∈ ∈ ∼N H
(cid:32) (cid:33)
t n
(cid:88) (cid:88) n∆ k
P( k (t)) P (Z k (i) µ k )
H ≤ − ≥ 3
n= t tb 1 i=1
⌈ − − ⌉
(cid:88) t n∆2 k
e−18σ2
≤
n= t 1
⌈K− ⌉
⌈K t−1⌉∆2 k
e− 18σ2
.
≤ ∆2
k
1 e−18σ2
−
24
ThesecondinequalityofLemma2thenfollowsbynotingthatthelasttermissummableovert. For
thesecondbound,wealsohavebyHoeffding’sinequality
(cid:32) (cid:33)
n
P( (t)) (cid:88) (cid:88)∞ P (cid:88) (Z j (i) µ j ) (cid:112) 2nσ2(ln(t) lnln(t)) n∆ min
H∗ ≤ − ≤− − − 3
j ∗n=1 i=1
∈M
(cid:88) (cid:88)∞ exp (cid:18) ln(t)+lnln(t) (cid:112) 2nσ2(ln(t) lnln(t)) ∆ min n∆2 min (cid:19)
≤ − − − 3σ2 − 18σ2
j ∗n=1
∈M
ln(t) (cid:18) (cid:112) ∆ min (cid:19) (cid:88)∞ n∆2 min
∗ exp 2(ln(t) lnln(t)) e− 18σ2 .
≤|M | t − − 3√σ2
n=1
(cid:112)
The last sum is obviously finite. Moreover, 2(ln(t) lnln(t)) = ω(lnln(t)), so that
(cid:16) (cid:112) (cid:17) (cid:16) (cid:17) −
exp 2(ln(t) lnln(t))∆min = 1 for any α > 0. By comparison with series
− − 3√σ2 O lnα(t)
oftheform 1 ,theterm ln(t)exp (cid:16) (cid:112) 2(ln(t) lnln(t)) ∆2 (cid:17) issummableovert,which
nlnα(n) t − − 3√σ2
leadstothefirstboundofLemma2.
Lemma3. Foranyk ,thereexistsaconstantC (µµµ)dependingsolelyonµµµsuchthat
∗ 1
̸∈M
(cid:32) (cid:115) (cid:33)
(cid:88)∞ 6σ2lnt
P µˆ k (t) µ ∗ ,a t =k C 1 (µµµ).
≥ − √t ≤
t=1
Proof. AunionboundonthesumyieldsforanyT N
∈
(cid:32) (cid:115) (cid:33)
(cid:88) T 6σ2lnt
P µˆ 2 (t) µ ∗ ,a t =k
≥ − √t
t=1
(cid:32) (cid:115) (cid:33)
(cid:88) T (cid:88) t 6σ2lnt
P µˆ k (t) µ ∗ ,N k (t)=n,N k (t+1)=n+1
≤ ≥ − √t
t=1n=0
 
(cid:115)
(cid:88) T (cid:88) T   6σ2min s n lns  
Pµˆ k (t) µ ∗ ≥ ,N k (t)=n,N k (t+1)=n+1.
≤  ≥ − √s 
n=0t=n  
(cid:124) (cid:123)(cid:122) (cid:125)
:= G1(t,n)
Nownotethatthe (t,n)aredisjointfordifferentt. Inparticular,
1
G
(cid:88) T (cid:32) (cid:114) 6σ2min s n lns (cid:33)
P( G 1 (t,n))=P ∃ t ∈ [n,T],µˆ k (t) ≥ µ ∗ − sb ≥ ,N 2 (t)=n .
t=n
ForindependentrandomvariablesZ (n) (µ ,σ2),wehavebyindependenceoftheX anda ,
k k t t
∼N
andthenbyHoeffdinginequality:
(cid:32) (cid:115) (cid:33) (cid:32) (cid:115) (cid:33)
(cid:88) T 6σ2lnt (cid:88) T 1 (cid:88) n 6σ2min s n lns
P µˆ k (t) µ ∗ ,a t =k 1+ P (Z k (i) µ k ) ∆ k ≥ ,
≥ − √t ≤ n − ≥ − √s
t=1 n=1 i=1
 (cid:16) (cid:113) (cid:17)2
T n ∆
6σ2mins≥nlns
)
1+ (cid:88) exp k − √s .
≤ − 2σ2 
n=1
Obviously,thissumcanbeboundedforanyT Nbyaconstantsolelydependingon∆
k
.
∈
Lemma4. Foranyi [K],thereexistsauniversalconstantC suchthat
2
∈
(cid:32) (cid:115) (cid:33)
(cid:88)∞ 6σ2lnt
P µˆ i (t) µ i ) C 2 .
≤ − N (t) ≤
i
t=0
25
Proof. ThisisadirectconsequenceofGarivier[2013],whichstatesthatforGaussianrewardswith
varianceσ2:
P(N i (t) (µˆ i (t 2 ) σ − 2 µ i )2 ≥ (1+α)lnt) ≤ 2 (cid:24) ln(1 ln + t η) (cid:25) t − (1 − η 1 2 6 )(1+α) foranyt ∈ N∗ andα,η >0.
Inparticularwithα=2=η,thisimplies
(cid:32) (cid:115) (cid:33)
6σ2lnt ln(t)+1
9
P µˆ i (t) µ i 2 t −4.
≤ − N (t) ≤ ln(3)
i
ThistermisobviouslysummablesothatthereexistsaconstantC suchthat
2
(cid:32) (cid:115) (cid:33)
(cid:88)∞ 6σ2lnt
P µˆ 1 (t) µ 1 C 2 .
≤ − N (t) ≤
1
t=1
Lemma4directlyfollowsbytheinclusionoftheconsideredevents.
Foranyk ,Lemma5belowgivesaneventinclusionfortheevent (t)thatwerecallhere,
∗ k
̸∈M E
(cid:40) (cid:115) (cid:41)
6σ2lnt
(t):= i ,N (t) √tandµˆ (t) µ µˆ (t),a =k, .
k ∗ i i ∗ k t
E ∃ ∈M ≥ ≥ − N (t) ≥
i
Lemma5. Thereexistconstantst(µµµ)andn(µµµ)dependingsolelyonµµµsuchthatforanyk ,t
∗
t(µµµ)andε (0,1), ̸∈M ≥
∈ 3
2σ2
(cid:0) (cid:1)
(t) µ µˆ (t) ε∆ ,a =k N (t) lnt+lnlnt +n(µµµ),a =k .
E k ⊂{ k − k ≤− k t }∪{ k ≤ (1 2ε)2∆2 t }
− k
Proof. Assume in the following that (t) holds for some t t(µµµ). Let i [K] be an arm
k
E ≥ (cid:113) ∈
maximisingtheempiricalmeanattimet. Necessarilyµˆ (t) µ 6σ2lnt. Moreover,a =kso
i ≥ ∗ − Ni(t) t
thatialsomaximisesthenumberofpulls,inparticularN (t) t . Moreover,aswepullthearmk,
i ≥ K
S S where
k i
≥
(cid:18) (cid:19)  (cid:32)(cid:112) (cid:33) 
1 1 1 1(cid:88) N
j
(t)(µ˜
eq,j
µˆ
j
) 1
S
i
= ln 1+ min erfc − , 1 ,
2 N
i
(t) − 2N
i
(t) 2 √2σ2 − K
j=i (53)
̸
(cid:88)
S =g (t)Q (t)+ g (t)P (t),
k k k j j
j=i
̸
whereforallj =i
̸
(cid:114)
g j (t)= N 2π j σ (t 2 ) (µ˜ eq,j − µˆ j )e− Nj(t) (µ˜eq, 2 j σ − 2 µˆj)2 ,
(cid:34) (cid:35)
1 N (t) N (t) 3 N (t) (µ˜ µˆ )2N2(t)
P (t)= ln( i ) j j + eq,j − j j
j 4 2πσ2e N2(t) − 4N2(t) 4σ2 N2(t)
i i i
(cid:34) (cid:35)
1 N (t) 1 1 1 (µ˜ µˆ )2
andQ (t)= ln( i ) + + eq,j − j .
j 4 2πσ2e N2(t) 2N (t) N (t) 4σ2
j j j
Alsonotethataswepullthearmk,wehaveforanyj i,
≤
g (t)Q (t) g (t)Q (t). (54)
j j k k
≤
(cid:113)
Asaconsequence,wecanwriteforanyδ >0andx˜ = Nj(t)(µ˜ µˆ ):
j 2σ2 eq,j − j
(cid:18) (cid:19)
N (t) N (t)
g j (t)P j (t) ≤ g j (t) ln(t) 4N j 2(t) +x˜3 j e − x˜2 j 2N j 2(t)
i i
(cid:18) (cid:19)
1 N (t) N (t)
j j
g (t) 2ln(t)+ + δ,
≤ j δ 2N2(t) 2N2(t)
i i
26
where we used the fact that x˜3 j e − x˜2 j ≤ x˜je δ
−x˜2
j +δ. Moreover, note that for t(µµµ) large enough,
Q (t) 1 . Asaconsequence,
j ≥ 2Nj(t)
(cid:18) (cid:19)
1 N (t) N (t)
j j
g (t)P (t)=g (t) 2ln(t)+ Q (t)+ δ
j j j δ 2N2(t)Q (t) j 2N2(t)
i j i
(cid:18) (cid:19)
1 Kδ
Q (t)g (t) 2ln(t)+ +
j j
≤ δ 2t
(cid:18) (cid:19)
1 Kδ
Q (t)g (t) 2ln(t)+ + ,
k k
≤ δ 2t
wherethelastinequalitycomesfromEquation(54). Inparticular,
(cid:18) 1 (cid:19) K2δ
S K 2ln(t)+ Q (t)g (t)+ . (55)
k k k
≤ δ 2t
Also,S 1 K2 sinceN (t) t andln(1+x) x x2 forx [0,1].
i ≥ 2t − 4t2 i ≥ K ≥ − 2 ∈
Nowassumethatµ µˆ (t) ε∆ . Itthenholds
k k k
− ≥−
µ˜ µˆ (t) µˆ (t) µˆ (t)
eq,k k i k
− ≥ −
(cid:115)
6σ2lnt
∆ +µ µˆ (t)
k k k
≥ − − √t
(cid:115)
6σ2lnt
(1 ε)∆ .
k
≥ − − √t
Again,wecanchooset(µµµ)largeenoughsothatµ˜ µˆ (t) (1 2ε)∆ . Moreover,notethat
eq,k k k
− ≥ −
the functions x e−x2 ,x xe x2 ,x x3e x2 are all decreasing on an interval of the form
(cid:55)→ x (cid:55)→ − (cid:55)→ −
[M,+ ]. Asaconsequence,wecanchoosen(µµµ)largeenoughsothat
√n(µµµ)((1
−
2ε)∆k)2
M. If
∞ √2σ2 ≥
N (t) n(µµµ),wethenhavefromEquation(55),foraconstantc(K,∆ )solelydependingonK
k k
≥
and∆ :
k
Nk(t)(1−2ε)2∆2
k
(cid:20) 1 (cid:21) K2δ
S k c(K,∆ k )e− 2σ2 lnt+ + .
≤ δ 2t
TheinequalityS S thenimplies,thankstotheabovebounds:
k i
≥
Nk(t)(1−2ε)2∆2
k
(cid:20) 1 (cid:21) 1 K2δ K2
c(K,∆ k )e− 2σ2 lnt+ δ ≥ − 2t − 4t2 .
Inparticular,forδ = 1 ,
2K2
2σ2
N (t) (lnt+lnlnt+ (1)),
k ≤ (1 2ε)2∆2 O
− k
wherethe hidesconstantsdependinginK and∆ . ThisconcludestheproofofLemma5aswe
k
O
justshownthatif (t)holds,atleastoneofthetwofollowingeventsholdswhenN (t) n(µµµ):
k k
E ≥
• µ µˆ (t) ε∆
k k k
− ≤−
• N (t)
2σ2
(lnt+lnlnt+ (1)).
k ≤ (1 − 2ε)2∆2 k O
C Generalizationoftheinformationmaximizationapproximation
In this section, we will generalize the approach derived in Appendix A to bandit settings with a
rewarddistributionbelongingtotheexponentialfamily. Wewillretraceallthepreviousstepsmadein
AppendixA,insistingonthedifferenceswiththeGaussianrewardcase. Wewillalsodiscussbandit
settingswithnon-uniformpriorsandwithmorethantwoarms.
27
C.1 Asymptoticexpressionforexponentialfamilyrewards
Wederiveanasymptoticexpressionfortheone-dimensionalcanonicalexponentialfamilyfromwhich
wewillderiveananalyticalapproximationoftheentropy. Wethusfocusonarewarddistribution
densityf withrespecttosomereferencemeasureν belongingtosomeone-dimensionalcanonical
exponentialfamily,i.e.,
(cid:0) (cid:1)
f(xθ)=A(x)exp T(x)θ F(θ) , (56)
| −
whereF istwicedifferentiableandstrictlyconvex.Additionally,letusrecallthattheKullback-Leibler
divergenceverifies: [Kordaetal.,2013]:
KL(θ,θ )=F(θ ) F(θ) F (θ)(θ θ), (57)
′ ′ ′ ′
− − −
whereKL(θ,θ )istheKullback-Leiblerdivergencebetweentherewarddistributionparameterized
′
byθandtheoneparameterizedbyθ .
′
Givenapriorπ(θ)andtherewardrealizations(x ,...,x ),theassociatedposteriordistributionon
1 n
θ,denotedp,reads:
(cid:32) (cid:33)
n
1 (cid:88)
p(θ x ,..,x )= π(θ)exp θ T(x ) nF(θ) , (58)
1 n k
| C −
k=1
(cid:82) (cid:80)
whereC = π(θ)exp(θ T(x ) nF(θ))dθisanormalizationconstant. Next,wederivethe
k
−
maximumaposteriorifortheparameterθ,denotedθˆ,whichverifies:
l
(cid:88) n T(x )=nF (θˆ) π ′ (θˆ l ) . (59)
i ′ l − π(θˆ)
i=1 l
Atthisstage,weassumethatthereexitssuchθˆ verifyingEquation(59). Inpractice,forareward
l
distributionthatdoesnotmeetthiscriteria,onecanreplaceθˆ byaseriesθˆ which,forsufficiently
l n,l
largevaluesofn,asymptoticallyconformstotheaforementioneddefinition. Forexample,aBernoulli
armthatconsistentlyfailsunderauniformprior,willresultinanundefinedθˆ. Toaddressthis,one
l
mayredefineθˆ suchthat(1+ (cid:80) i=1nT(x ))=(n+2)F (θˆ),effectivelyreplacingtheempirical
l i ′ l
meaninEquation(59)withtheposteriormean.
ReplacingthesuminEquation(58)leadsto
(cid:32) (cid:33)
1 π (θˆ)
p(θ x ,..,x )= π(θ)exp θnF (θˆ) θ ′ l nF(θ)
| 1 n C ′ l − π(θˆ) −
l
= enθˆ lF′(θˆ l) − nF(θˆ l) π(θ)e− θπ π ′ ( ( θˆ θˆ l l ) ) e − nKL(θˆ l,θ) (60)
C
= 1 π(θ)e− θπ π ′ ( ( θˆ θˆ l l ) ) e − nKL(θˆ l,θ),
C
2
where C also acts as a normalization constant of Equation (60). For n 1, the distribution
2
concentratesinthevicinityofθˆ fromwhichwewillderivetheasymptotics ≫ calingofC . Wethen
l 2
integrateEquation(60)afterachangeofvariableθ(u)=θˆ + u ,
l √n
1= (cid:90) p(θ x 1 ,..,x n )dθ = (cid:90) (θb− θˆ l)√n 1 π(θˆ l + u )e− (θˆ l+√u n )π π ′ ( ( θˆ θˆ l l ) ) e− nKL(θˆ l,θˆ l+√u n )du
Θ |
−
(θb− θˆ l)√n C 2 √n √n
(cid:90)
−
θb (cid:90) µsup
+ p(θ x ,..,x )dθ+ p(θ x ,..,x ).dθ
1 n 1 n
| |
µinf θb
(61)
Taking(θ θˆ) n b withb < 1/2, wegetridofthetailcomponentsintheasymptoticlimit.
b l −
− ∼
Secondly, by noticing that F′′(θˆ l) = limK(θˆ,θ)/θ θˆ 2 from Equation (57), we make an
expansiontothelowestorderof 2 theKul θ lb→a θˆ c l k-Lei l blerd | iv − erge l n | ce,whichgives:
1= lim (cid:90) (θb− θˆ l)√n 1 π(θˆ l )e− θˆ l π π ′ ( ( θˆ θˆ l l ) ) e− F′′(θ 2 ˆ l)u2 θˆ l+ O (cid:16) √1 n (cid:17) du. (62)
θ → θˆ l − (θb− θˆ l)√n C 2 √n
28
Thus,weobtain:
C 2 (cid:113) √2π π(θˆ l )e− θˆ l π π ′ ( ( θˆ θˆ l l ) ) . (63)
∼ nF (θˆ)
′′ l
Ofnote,thegaussianlimitalsogivesthatσ¯ i 2 ∼ F ′′ (θˆ l ) − 1N i− 1.
Thus,weassumetodevelopanapproximationschemeforaposteriordistributionp asymptotically
i
verifying:
(cid:115)
1
p
i
(θ)
Ni ∼
→∞
2πσ¯
i
2
H(θ,θˆ
l
)e
−
NiKL(θˆ l,θ), (64)
whereH isafunctionaccountingforthepriordistribution. Forthefollowing,wetakeauniform
prioronΘ,whichleadstoH(θ,θˆ)=1.
l
Inthefollowing,wewilldenoteµˆ andµˆ asthemaximumaposterioriestimatesassociatedto
Mt m
theirrespectivearms(insteadoftheempiricalmeans).
C.2 Thepartitioningapproximation
Sinceallthestepsleadingtothepartitioningapproximationareindependentofthetypeofreward
distribution,S˜ andS˜ havethesamegeneralformasgiveninAppendixA.1.Here,weconsider
tail body
allthedistributionsunderθparameterforwhichwereplaceµˆ ,µˆ andµ˜ bythereequivalents
Mt m eq
θˆ ,θˆ andθ˜ .
mt Mt eq
C.3 Asymptoticintersectionpoint
ByuseofEquation(64),theequationverifiedbytheintersectionpointθ¯ asymptoticallyreads:
eq
e − NMt KL(θˆ Mt ,θ¯ eq) (cid:90) θ¯ eq e − NmKL(θˆ mt ,θ′) e − NmKL(θˆ mt ,θ¯ eq) (cid:90) θ¯ eq e − NMt KL(θˆ Mt ,θ′)
dθ = dθ .
(cid:113) (cid:112) ′ (cid:112) (cid:113) ′
2πσ¯ M 2 t µinf 2πσ¯ m 2 2πσ¯ m 2 µinf 2πσ¯ M 2 t
(65)
TakingthelogarithmofEquation(65)leadsto
N KL(θˆ ,θ¯ ) N KL(θˆ ,θ¯ )+ 1 ln σ¯ m 2 +ln (cid:82) µ θ¯ i e n q f (cid:113) σ¯ M 2 t e − NmKL(θˆ mt ,θ′)dθ ′ =0.
m mt eq − Mt Mt eq 2 σ¯ M 2 t (cid:82) µ θ¯ i e n q f (cid:112) σ¯ m 2 e − NMt KL(θˆ Mt ,θ′)dθ ′
(66)
Employing the same arguments as the ones exposed in Appendix A.2, we approximate θ¯ by
eq
neglectingthelastterm. Furthermore,intheconsideredasymptoticscalingregime(N N ),
Mt
≫
m
θ¯ willbeinthevicinityofθˆ whereaGaussianexpansionoftheKullback-Leiblerdivergence
eq Mt
isrelevant(seeEquation(61)). Thus,weapproximateKL(θˆ ,θ˜ )byKL(θˆ ,θˆ )andexpand
mt eq mt Mt
KL(θˆ Mt ,θ¯ eq )tolowestorderinθ˜ eq (withσ¯ i 2 ∼ F ′′ (θˆ l ) − 1N i− 1),leadingto:
(cid:115)
(cid:20) (cid:21)
1 σ¯2
θ˜ =θˆ + 2σ¯2 N KL(θˆ ,θˆ )+ ln m . (67)
eq Mt Mt m mt Mt 2 σ¯2
Mt
C.4 Generalizationofthemainmode’scontribution
Westartbyrecallingtheexpressionforthebodycomponentoftheentropy:
(cid:90)
S˜ = p (θ)C (θ)lnp (θ)dθ. (68)
body
−
Mt m Mt
Θ
WithoutanyadditionalinformationontheexpressionforKL,Equation(68)cannotbecomputedina
closedform. Thus,wewillrelyontheasymptoticscalingN N 1toprovideatractable
Mt
≫
m
≫
29
expression. First,weneglectvariationsofC (θ)inEquation(68)integralbyevaluatingitatµ˜ .
m eq
Then,bynoticingthattheresultingintegralistheentropyofthebetterempiricalarm’smean,we
approximateitbyitsleadingorder,proportionaltoln(2πσ¯2 )/2:
Mt
(cid:34) (cid:35)
S˜ 1 ln(2πσ¯2 ) 1 (cid:90) µsup e − NmKL(θˆ mt ,θ′) dθ . (69)
body ≈ 2 Mt − θ¯
eq
(cid:112) 2πσ¯
m
2 ′
WefinallyconsiderthelastintegralinEquation(69). Bynoticingthatitisconcentratedinthevicinity
ofµ˜ forN 1,weTaylorexpandKL(θˆ ,θ )atµ˜ toobtain:
eq m
≫
mt ′ eq
(cid:90) µsup e − N (cid:112) mKL(θˆ mt ,θ′) dθ e − Nm (cid:112) KL(θˆ mt ,µ˜eq) (cid:90) µsup e − Nm(θ′ − µ˜eq)∂2KL(θˆ mt ,µ˜eq)dθ ′
θ¯
eq
2πσ¯
m
2 ≈ 2πσ¯
m
2 θ¯
eq (70)
e − NmKL(θˆ mt ,µ˜eq)
.
≈ (cid:112) 2πσ¯2 N ∂ KL(θˆ ,µ˜ )
m m 2 mt eq
InsertingEquation(70)intoEquation(69)leadstothebodycomponent:
(cid:34) (cid:35)
S˜ = 1 ln(2πσ¯2 ) 1 e − NmKL(θˆ mt ,θ˜ eq) . (71)
b 2 Mt − (cid:112) 2πσ¯2 N ∂ KL(θˆ ,θ˜ )
m m 2 mt eq
C.5 Generalizedexpressionfortheentropytail
Westartbyrecallingtheexpressionforthetailcomponent:
(cid:90) µsup
S˜ = p (θ)lnp (θ)dθ. (72)
tail m m
− θ˜
eq
AsforEquation(70),weTaylorexpandKL(θˆ ,θ )atθ˜ intheexponentialtermtoobtain:
mt ′ eq
S˜ = lnp (θ˜ ) e − NmKL(θˆ mt ,θ˜ eq) . (73)
t − m eq (cid:112) 2πσ¯2 N ∂ KL(θˆ ,θ˜ )
m m 2 mt eq
Keepingtheleadingorderof lnp (θ˜ ) N KL(θˆ ,θ˜ )leadstotheexpectedtailexpression
−
m eq
∼
m mt eq
usedinthemaintext.
C.6 Generalizedformoftheentropyapproximation
To summarize, by combining Equations (71) and (73) we obtain an asymptotic expression for
exponentialfamilybanditswithauniformprior:
(cid:34) (cid:35)
S˜ = 1 ln(2πσ¯2 ) 1 e − NmKL(θˆ mt ,θ˜ eq) + KL(θˆ mt ,θ˜ eq )e − NmKL(θˆ mt ,θ˜ eq) .
max 2 Mt − N ∂ KL(θˆ ,θ˜ ) (cid:112) 2πσ¯2 ∂ KL(θˆ ,θ˜ ) (cid:112) 2πσ¯2
m 2 mt eq m 2 mt eq m
(74)
Finally,dependingimplementation,weproposeforconveniencetoreplaceinS˜ themaximuma
max
posterioriestimatesofeacharmbyeithertheirempiricalmean,theirmeanposteriorvaluesorthe
maximumofthelog-likelihood. Thisdoesnotalterthealgorithm’sefficiencyinpractice,whileit
maysimplifytheimplementationprocedureforspecificrewarddistributions.
Notethatallthesestepscanbeadaptedtonon-uniformpriors(inparticularbymultiplyingthetail
bytheprioreffectsevaluatedinθ˜ ). Finally,letusunderlinethatourapproximationschemeholds
eq
foranyposteriordistributionsverifyingEquation(64),apropertywebelievetobesharedformore
generalrewarddistributions.
30
C.7 Derivationoftheincrementfortheclosed-formexpressionofentropy
First,westressthereisnouniqueguidelinetocomputetheexpectedincrementofEquation(74),and
multiplesolutionsemergedependingonthetypeoftherewarddistribution.Inparticular,ifthereward
distributioniscontinuous,onecouldintegratetheincrementasithasbeendoneforGaussianrewards
above. But,iftheintegrationcannotbesolvedanalytically,onecouldapproximatetheincrementsby
takingdiscreterewardvaluesoftheorderof σ. Similarly,iftherewardtakesdiscretevalues,the
±
incrementsarealreadydiscrete,butasymptoticsimplificationsortakingthecontinuouslimitcanalso
beconsidered.
Finally,iftheincrementevaluationisdiscreteorapproximated,onecouldencounterrarecaseswhere
thealgorithmgetstrapped. Itcouldoccurwhenthealgorithmobservesaworsesuboptimalarmclose
tothebestempiricalarmwhenithasalreadyextensivelybeendrawn. Becausetheentropycould
increasedrasticallyifanarminversionoccurs,thegradientsignsmayoccasionallyswitch,leadingto
thefailureoftheminimizationprocedure. Topreventsuchcases,wechangethedecisionprocedure
bymaximizingtheentropyvariationratherthanitsdirectminimization. Anexampleisgivenforthe
implementationofBernoullirewardsinthenextsection.
Lastly, dependingontherewarddistributionitmaybestraightforwardtoexpresstheincrements
alongtheusualempiricalorposteriormeanasopposedtothefamilyparameterθˆ. Often,thiscanbe
achievedthroughabasicvariabletransformation. TheBernoullidistributionexampleprovidedbelow
servesasanillustrationofthisapproach.
D Numericalexperiments
Here, we provide all the information regarding numerical experiments. This includes details on
thenumericalsettings,implementationdetailsforAIMintheinvestigatedsettings,anoverviewof
investigatedclassicalbanditalgorithms,andadditionalexperimentsfocusingonclose-armmeans.
D.1 Numericalsettings
In Figure 1, the posterior distributions are drawn with µˆ 0.65, N (t) = 374, µˆ 0.29,
Mt
≈
1 m
≈
N =26,whereµ andN (t)are,respectively,theempiricalmeanandnumberofdrawsofarmi
m i i
andhavebeenobtainedwiththeAIMalgorithm.
For the Gaussian two-armed cases in Figure 2 the arm means are chosen from a uniform grid in
(0,1) (0,1)usingaSobolsequence(wehaveavoidedthevalues0and1butithasnoimpacton
theob × tainedresults). Theregretisaveragedover8192gamesandobservedduring108stepstoattest
thelogarithmicscaling. ForBernoullirewardswithtwo-armedFigure3,theregretisaveragedover
16384gamesandobservedduring108steps. ItisworthnotingthatforGaussianrewards,theprior
informationofarmmeansbeingonlybetween0and1isnotgiventoAIMnortotheThompson
samplingalgorithmtoallowadirectcomparison.
Forthefifty-armedcaseinFigures2and3,thearmmeansaredrawnfromauniformprior,andthe
regretisaveragedover2000gamesandobservedduring106stepsinFigure2and107inFigure3.
ForclosearmmeansinFigures4and5,themeanvaluesarefixedwithµ =0.79andµ =0.8,but
1 2
thispriorinformationisnotgiventotheinvestigatedalgorithms. Theregretisaveragedover105
gamesandobservedduring106steps.
For the two-armed cases in Figures 6 and 7, the arm means are chosen from a uniform grid in
(0,1) (0,1)usingaSobolsequence. Theregretisaveragedovermorethan105gamesandobserved
during × 106stepstoenhancemeasurementaccuracy.
Finally, in the fifty-armed case in Figure 8, the arm means are drawn from a uniform prior, and
theregretisaveragedover4.104 gamesandobservedduring5.104 stepstoenhancemeasurement
accuracy.
Ofnote,foralltheexperiments,seedvaluesarenotsharedthroughoutthealgorithms. Toobtaina
sufficientnumberofruns,thecodewasparallelizedonacluster(asynchronously),witheachrun
operatingindependentlywhileensuringthatseedvaluesarenotcommonbetweenruns. Becauseit
31
reliesonananalyticalexpression,AIMshowsanexecutiontimeofthesameorderofThompson
sampling(measuredthreetimesslowerfortwo-armedBernoullirewards).
Forcompleteness,animplementationofAIMforbothBernoulliandGaussianrewardsandmorethan
twoarmsaregiveninthesupplementarymaterial(AIM Bernoulli bandits and AIM Gaussian
folders).
D.2 AIMimplementationdetails
Here,werecapbelowAIMsetupsforthedifferentsettingsevokedinthemaintext.
D.2.1 ApproximateinformationmaximizationforthetwoarmGaussianrewards
Specificallyforthetwo-armedcase,onecansimplifiestheexpressionsgiveninthemaintext. µ˜
eq
definedthevalueofθwherebotharmshavethesameprobabilityofbeingthemaximaloneandreads
(cid:115)
(cid:18) (cid:19)
N (µˆ µˆ ) N N (µˆ µˆ )2 σ2 N
µ˜ =µˆ + m Mt − m + Mt m Mt − m + ln Mt . (75)
eq Mt N N (N N )2 N N N
Mt
−
m Mt
−
m Mt
−
m m
Hence,followingidenticalapproximationsoftheonesderivedinAppendixA.1forGaussianreward
distributions,thetailcomponentgivenbyEquation(7)simplifiesinto:
S˜ tail = 1 ln( 2πσ2e )erfc (cid:18) √N m (µ˜ eq − µˆ m ) (cid:19) + √N m (µ˜ eq − µˆ m ) e− −Nm(µ˜ 2 e σ q 2 −µˆm)2 . (76)
4 N
m
√2σ2 2√2πσ2
SimilarlyforS˜ ,weobtain:
body
1 (cid:18) 2πσ2e (cid:19) (cid:34) 1 (cid:32)(cid:112) N N (µˆ µˆ ) (cid:33)(cid:35)
S˜ = ln 1 erfc m Mt Mt − m
body (cid:112)
2 N Mt × − 2 2σ2(N m +N Mt )
(77)
(cid:112) N Mt N m 3/2(µˆ Mt − µˆ m ) e− Nm 2 N σ2 M (N t ( m µˆM +N t − M µˆ t m ) )2 .
− 2σ√2π(N +N )3/2
Mt m
Finally, it allows us to derive the approximation of the gradient difference of the entropy for the
two-armedcase:
(cid:18) (cid:19)
1 N 1 √N (µ˜ µˆ ) √N (µ˜ µˆ )
m m eq m m eq m
∆= ln( )+ erfc − + −
2 N
m
+1 4N
Mt
√2σ2 √2πσ2 ×
e− Nm(µ˜ 2 eq σ − 2 µˆm)2 (cid:20)2N 4 M N 2 t − N 3 2 N m 2 + 1 4 ln (cid:18) 2 N πσ M 2 t e (cid:19)N N m 3 2 + N N 2 M 2 t + (N m 3 + 4 N σ2 M 2 N t )(µ N ˜ eq 2 − µˆ m )2(cid:21) ,
m Mt m Mt m Mt
(78)
andtheassociatedpseudo-codeusedforFigures2,4and6ispresentedinAlg.2below.
D.2.2 ApproximateinformationmaximizationforBernoullirewards
Wedenotebyµ¯ theposteriormean,givenby:
i
E (cid:2) X B (ri+1,Ni− ri+1) (cid:3) = N r i i + + 1 2 =µ¯ i , (79)
wherer isthecumulativerewardattimet,N thenumberofdrawsandX followsaBeta
i i (a,b)
distributionwithparameters(a,b). Thevarianceverifies: B
(cid:2) (cid:3) r i +1 N i r i +1 1
Var X = −
B (ri+1,Ni− ri+1) N i +2 N i +2 N i +3 (80)
µ¯ (1 µ¯ )
i i
= − ,
N¯
i
32
Algorithm2:AIMAlgorithmfor2Gaussianarm
Draweacharmonce;observerewardX (t)andupdatestatistics
t
µˆ X (t),N 1 t 1,2
t t t
← ← ∀ ∈{ }
fort=3toT do
/* Arm selection */
M argmax µˆ ,m argmin µˆ ;
t ← k=1,2 k ← k=1,2 k
ifN N then
Mt
≤
m
a M
t t
←
else
Evaluate∆followingEquation(9);
if∆<0then
a M
t t
←
else
a m
t
←
Pulla andobserveX (a )
t t t
/* Update statistics */
µˆ
µˆat Nat +Xt(at),N
N +1
at ← Nat +1 at ← at
whereN¯ =N +3.
i i
ForBernoullirewards,weapproximatethegradientasfollows:
| ∆ i S˜ max | = (cid:12) (cid:12) (cid:12) (cid:12) µ¯ i (N¯ N¯ i − 1 3 ) − 1 S˜ max ( µ¯ i N¯ i + N¯ 1 − µ¯ i ,N¯ i +1,µ¯ j ,N¯ j )
i i
−
+ N¯ i − 2 N − ¯ µ¯ i ( 3 N¯ i − 1) S˜ max ( µ¯ i (N¯ N¯ i − 1) ,N¯ i +1,µ¯ j ,N¯ j ) − S˜ max (µ¯ i ,N¯ i ,µ¯ j ,N¯ j ) (cid:12) (cid:12) (cid:12) (cid:12) ,
i i
−
(81)
with S˜ given by Equation (74) expressed along µ with µ = eθ . For Bernoulli rewards the
max 1+eθ
equationreads:
S˜ max (µ¯ Mt ,N¯ Mt ,µ¯ m ,N¯ m )= (cid:32) 1 − √N m ∂ 2 KL e ( − µ¯ m N¯ m ,µ˜ K e L q ( ) µ¯ (cid:112) m, 2 µ˜ π eq µ¯ ) m (1 − µ¯ m ) (cid:33) 1 2 ln (cid:18) 2πµ¯ Mt N ( ¯ 1 M − t µ¯ Mt ) (cid:19)
(cid:112) N¯
m
KL(µ¯
m
,µ˜
eq
)e
−
N¯ mKL(µ¯m,µ˜eq)
+ ,
(cid:112)
∂ KL(µ¯ ,µ˜ ) 2πµ¯ (1 µ¯ )
2 m eq m m
−
(82)
withKL(θ,θ ′ )=θln(θ/θ ′ )+(1 − θ)ln([1 − θ]/[1 − θ ′ ])andσ i 2 = µ¯i(1 N¯− i µ¯i).
Briefly, the expected gradient is evaluated along arm i with a returned reward equal to 1 with
probability µ¯i(N N ¯ ¯ i i − − 1 3 ) − 1 (whichistheempiricalmean)orequalto0withprobability1 − µ¯i(N N ¯ ¯ i i − − 1 3 ) − 1.
Ofnote,byaddingabsolutevalues,weseektomaximizetheentropyvariationratherthanitsdirect
minimizationtoavoidfallingintoanentrapmentscenario(seeAppendixC.7forfurtherdiscussion).
We draw some additional observations on the practical implementation of the code. First, in the
gradientevaluationof∆ followingEquation(81)wemayfindaµ˜ valuetobeundefined(because
i eq
N >N orµ˜ >1),whichisunusableforBernoullirewards. Inthiscase,µ˜ istakentobe
m Mt eq,i
(cid:16) (cid:17)
eq,i
equalto1,resultinginS max = 2 1ln 2πµ¯M N t ¯ ( M 1 − t µ¯Mt ) .
Second,atlargetimes,noticingthatthebetterempiricalarmisdrawnextensively,onecanincrement
thealgorithmbymultiplestepsatatimetospeedupAIM.Indeed, letusassumethatthebetter
empiricalarmisdrawnT timessuccessivelywhilealwaysreturninganullreward,whichistheworst
scenarioforthereturnedrewardofthebetterempiricalarm. Then,iftheincrementevaluationat
t+T ofAlg.3stillreturnsM ,thenitensuresthatallincrementevaluationsbetween[t,t+T]of
t
33
Algorithm3:AIMAlgorithmfor2Bernoulliarm
Draweacharmonce;observerewardX (t)andupdatestatistics
t
µ¯ Xt(t)+1,N¯ 4 t 1,2
t ← 3 t ← ∀ ∈{ }
fort=3toT do
/* Arm selection */
M argmax µ¯ ,m argmin µ¯ ;
t ← k=1,2 k ← k=1,2 k
ifN N then
Mt
≤
m
a M
t t
←
else
Evaluate∆= ∆ S˜ ∆ S˜ followingEquation(81);
|
Mt max
|−|
m max
|
if∆>0then
a M
t t
←
else
a m
t
←
Pulla andobserveX (a )
t t t
/* Update statistics */
µ¯ at ← µ¯at (N¯ a N¯ t a − t 1)+Xt,N¯ at ← N¯ at +1
Alg.3willalwaysreturnM independentlyofitsreturnedrewards. Then,usingadichotomysearch
t
onthevariableT,wecandiminishthenumberofincrementevaluationsofAIMatlargetimes,thus
improvingAIM’s performance.
D.3 InformationmaximizationapproximationforBernoullirewardswithmorethantwo
arms
Westartbyremindingtheobtainedentropyapproximationformorethantwoarms:
S˜ = (cid:90) (cid:0) 1 (cid:88) K [1 C (θ)] (cid:1) p (θ)lnp (θ)dθ (cid:88) K (cid:90) µsup p (θ)lnp (θ)dθ. (83)
max
− − −
i Mt Mt
−
i i
Θ i
̸
=Mt i
̸
=Mt µ˜eq,i
Wefirstconsidertheincrementalongaworseempiricalarm,whichsimplifies:
(cid:20) (cid:90) (cid:90) µsup (cid:21)
∆ S˜ =∆ C (θ)p (θ)lnp (θ)dθ p (θ)lnp (θ)dθ , (84)
|
i max
|
i
−
i Mt Mt
−
i i
Θ µ˜eq,i
whichisexactlytheincrementevaluatedinthetwo-armedcasegiveninEquation(82).
Finally,weconsidertheincrementalongthebetterempiricalarm. Forsimplicity,weneglectµ˜
eq,i
variationsfortheincrementsevaluation. ByuseofEquation(82)weobtain
|
∆ Mt S max
|
= (cid:12) (cid:12) (cid:12)
(cid:12)
1
−
(cid:88) K
√N ∂ KL
e −
(µ¯
N¯ m
,µ
K
˜
L(
)
µ¯ (cid:112) i,µ˜
2
eq
π
)
µ¯ (1 µ¯ )
(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12)
(cid:12)
∆ Mt H(µ¯ Mt ,N¯ Mt ) (cid:12) (cid:12) (cid:12)
(cid:12)
, (85)
i
̸
=Mt m 2 i eq i − i
where
(cid:12) (cid:12) (cid:12) (cid:12) ∆ Mt H(µ¯ i ,N¯ i ) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:12) µ¯ i (N¯ N¯ i − 1 3 ) − 1 H( µ¯ i N¯ i + N¯ 1 − µ¯ i ,N¯ i +1,µ¯ j ,N¯ j )
i i
−
+ N¯ i − 2 N − ¯ µ¯ i ( 3 N¯ i − 1) H( µ¯ i (N¯ N¯ i − 1) ,N¯ i +1,µ¯ j ,N¯ j ) − H(µ¯ i ,N¯ i ,µ¯ j ,N¯ j ) (cid:12) (cid:12) (cid:12) (cid:12) ,
i i
−
(86)
34
(cid:16) (cid:17)
withH(µ¯ Mt ,N¯ Mt )= 2 1ln 2πµ¯M N t ¯ ( M 1 − t µ¯Mt ) .
Algorithm4:AIMAlgorithmforK >2Bernoulliarm
Draweacharmonce;observerewardX (t)andupdatestatistics
t
µ¯ Xt(t)+1,N¯ 4 t 1,..K
t ← 3 t ← ∀ ∈{ }
fort=K+1toT do
/* Arm selection */
M argmax µ¯ ;Evaluate∆ S˜ followingEquation(85);
Ev t al ← uatem=a k r = g { m 1,. a ., x K ( } ∆ k S˜ ,i=M M ) t w m it a h x ∆ S˜ followingEquation(81);
i max t i max
| | ̸ | |
if∆ S˜ >∆ S˜ then
Mt| max
|
m
|
max
|
a M
t t
←
else
a m
t
←
Pulla andobserveX (a )
t t t
/* Update statistics */
µ¯ at ← µ¯at (N¯ a N¯ t a − t 1)+Xt,N¯ at ← N¯ at +1
Ofnoteinthegradientevaluationof∆ followingEquation(81),ifonesfindsaµ˜ valueundefined
i eq,i
(becauseN >N orµ˜ >1whichisunusableforBernoullireward),then,µ˜ istakentobe
m Mt eq,i
(cid:16) (cid:17)
eq,i
equalto1resultinginS max = 1 2 ln 2πµ¯M N t ¯ ( M 1 − t µ¯Mt ) . Finally,ifM t ← argmax k= { 1,..,K } µ¯ k has
multiplesolutions,wesuggestchoosingtheonedisplayingthelowestnumberofdraws.
D.4 Overviewofbaselinebanditalgorithms
Here,webrieflyreviewseveralbaselinealgorithmsandtheirchosenparameterstoprovideabench-
markofourinformationmaximizationmethod.
D.4.1 UCB-Tuned
Thisalgorithmfallsunderthecategoryofupperconfidencebound(UCB)algorithms,whichselect
thearmmaximizingaproxyfunctiontypicallydefinedasF =µˆ +B . ForUCB-tuned,B isgiven
i i i i
by:
(cid:115) (cid:115)
(cid:18) (cid:19)
ln(t) 1 2ln(t)
R =c(µ ,µ ) min ,s (t) , s (t)=σˆ2+ , (87)
i 1 2 i i i
N (t) 4 N (t)
i i
whereσˆ2istherewardvarianceandcahyperparameter. ForGaussianrewards,bytestingvariousc
i
valuesforuniformpriorsinEquation(87),weendupwithc=2.1andσˆ2 = σ2 .
i Ni(t)
D.4.2 KL-UCB
Thisalgorithmisanothervariantoftheupperconfidencebound(UCB)classspecificallydesignedfor
boundedrewards. Inparticular,itisknowntobeoptimalforBernoullidistributedrewards[Garivier
andCappé,2011,Cappéetal.,2013]. ForKL-UCB,F isexpressedasfollows:
i
(cid:110) (cid:18) r (t) (cid:19) (cid:111)
F =max θ Θ:N (t)KL i ,θ ln(t)+c(µ ,µ )ln(ln(t)) , (88)
i i 1 2
∈ N (t) ≤
i
whereΘdenotesthedefinitionintervaloftheposteriordistribution. Bytestingvariouscvaluesfor
uniformpriors,weendupwithc(µ ,µ )=0.00001(c=0forthe50-armedGaussiansettingand
1 2
c = 10 6 forthe2-armedBernoullisetting). Ofnote, themaximumisfoundusingadichotomy
−
methodusingaprecisionof10 5andamaximumnumberofiterationsof50.
−
35
ForKL-UCB++[MénardandGarivier,2017],thefunctionF isexpressedasfollows
i
(cid:110) (cid:18) r (t) (cid:19) (cid:18) T (cid:18) T (cid:19) (cid:19)(cid:111)
F =max θ Θ:N (t)KL i ,θ ln ln2 +1 , (89)
i ∈ i N (t) ≤ + KN (t)(t) + KN (t)(t)
i i i
whereln (x)=max(ln(x),0),andT isthestoppingtimeofthebanditgame.Therefore,KLUCB++
+
is not an anytime algorithm, but it still underperforms when compared to AIM and Thompson
sampling.
D.4.3 Thompsonsampling
Ateachstep,Thompsonsampling[Thompson,1933,Kaufmannetal.,2012a,b]selectsanarmat
random, basedontheposteriorprobabilitythatismaximizestheexpectedreward. Inpractice, it
drawsK randomvaluesaccordingtoeacharm’smeanposteriordistributionandselectsthearmwith
thehighestsampledvalueas:
(cid:18) (cid:19)
a =argmax Z (µˆ (t),N (t)) , (90)
t i i i
i=1..K
whereZ (t)isdrawnaccordingtotheposteriordistributionoftheitharm’smean. Here,weused
i
auniformprioron[0,1]forBernoullirewardsandauniformprioronRforGaussianrewardsto
provideadirectcomparisonwithAIM.
Finally,forThompsonsamplingplus[Jinetal.,2022],denotedTS+,eachsampledvalueforthe
comparisonisdrawnaccordingtoZ (µˆ (t),N (t))withaprobability1/K ortakenequaltoµˆ (t),
i i i i
otherwise.
D.4.4 MED
At each step, the minimal empirical divergence (MED) algorithm [Honda and Takemura, 2011],
selectsanarmatrandom,basedonatailoreddistributionbuildingontheKullback-Leiblerdistance
tothebetterempiricalarm. Inpractice,thearma =iisbedrawnwithaprobability:
t
(cid:104) (cid:16) (cid:17)(cid:105)
exp N (t)KL ri(t),µ¯
p i = (cid:80)K ex − p (cid:104) i N (t)KL N (cid:16) i(t r ) j(t) M , t µ¯ (cid:17)(cid:105). (91)
j=0 − j Nj(t) Mt
D.5 Additionalexperiments
D.5.1 ApproximateinformationmaximizationforGaussianrewardsandclosearms
Forcompleteness,weprovideinFigure4belowregretperformancesinwhichthearms’meanvalues
areclose(∆µ=0.01),andarethusdifficulttodistinguish,forGaussianrewarddistributions. Here,
AIMshowsstate-of-the-artperformancecomparabletoThompsonsampling,evenoutperformingit
atlongertimes.
D.5.2 ApproximateinformationmaximizationforBernoullirewardsandclosearms
Forcompleteness,weprovideinFigure5belowregretperformancesinwhichthearmsmeanvalue
are close(∆µ = 0.01) forBernoulli reward distributions. As forGaussian rewards, AIM shows
state-of-the-artperformancecomparabletoThompsonsamplingevenwhenarmsmeanrewardsare
difficulttodistinguish.
D.5.3 Investigatingapproximateinformationmaximizationforlargesimulationvolumes
TorefinethenumericalinvestigationofAIM’sregretperformance,wereplicatetheexperimentsof
themaintextusingalargervolumeofsimulations(butonshortertimescales). Forthe2-armedbandit
withGaussianandBernoullirewards,theregretperformanceunderauniformpriorisaveragedover
morethan105runs. Similarly,forthe50-armedbanditwithBernoullirewards,theregretisaveraged
over4 104. ThisleadstotheresultsshowninFigures6to8,confirmingtheresultsofFigures2
×
to3.
36
600
500
400
300
200
100
0
101 102 103 104 105 106
t
)t(R
i
h
AIM
Thompson
Gaussianrewards,K =2
Figure4: Temporalevolutionoftheregretfor2-armedbanditwithGaussianrewards(σ = 1)for
closemeanparameters. InblueAIM,inredThompsonsampling. Armmeanrewardvaluesarefixed
withµ =0.8andµ =0.79,theregretisobtainedbyaveragingover105realizations.
1 2
140
120
100
80
60
40
20
0
101 102 103 104 105 106
t
)t(R
i
h
AIM
Thompson
Bernoullirewards,K =2
Figure 5: Temporal evolution of the regret for 2-armed bandit with Bernoulli rewards for close
mean parameters. In blue AIM, in red Thompson sampling. Arm mean reward values are fixed
withµ =0.8andµ =0.79,theregretisobtainedbyaveragingover105realizations. Confidence
1 2
intervalsshowsthestandarddeviation.
140
120
100
80
60
40
20
0
101 102 103 104 105 106
t
)t(R
i
h
AIM
Thompson
UCB-Tuned
Gaussianrewards,K =2
Figure6: EvolutionoftheBayesianregretfor2-armedbanditwithGaussianrewardsunderauniform
meanprior. Theregretisaveragedovermorethan105runs. Confidenceintervalsshowsthestandard
deviation. Confidenceintervalsshowthestandarddeviation.
37
25
20
15
10
5
0
101 102 103 104 105 106
t
)t(R i
h
AIM
Thompson
KL-UCB
Bernoullirewards,K =2
Figure7:EvolutionoftheBayesianregretfor2-armedbanditwithBernoullirewardsunderauniform
meanprior. Theregretisaveragedovermorethan105runs. Confidenceintervalsshowthestandard
deviation.
160
140
120
100
80
60
40
20
0
101 102 103 104
t
)t(R
i
h
AIM
Thompson
Bernoullirewards,K =50
Figure 8: Evolution of the Bayesian regret for 50-armed bandit with Bernoulli rewards under a
uniformmeanprior. Theregretisaveragedover4 104runs. Confidenceintervalsshowthestandard
×
deviation.
38

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Approximate information maximization for bandit games"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
