=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: Intrinsic motivation as constrained entropy maximizationCitation Key: kiefer2025intrinsicAuthors: Alex B. KieferREMEMBER: Extract quotes, claims, and findings ONLY from the text provided below.Year:2025Key Terms: empowerment, article, motivation, alexb, entropy, intrinsic, maximization, verses, intrinsicmotivation, constrained=== FULL PAPER TEXT ===ArticleIntrinsic motivation as constrained entropymaximizationAlexB.Kiefer1,2*1 VERSES,LosAngeles,California,USA2 MonashCentreforConsciousnessandContemplativeStudies,MonashUniversity,Melbourne,VIC3800,Australia* Correspondence:alex.kiefer@monash.eduAbstract: “Intrinsicmotivation”referstothecapacityforintelligentsystemstobemotivatedendoge-nously,i.e. byfeaturesofagentialarchitectureitselfratherthanbylearnedassociationsbetweenactionandreward. Thispaperviewsactiveinference,empowerment,andotherformalaccountsofintrinsicmotivationasvariationsonthethemeofconstrainedmaximumentropyinference,providingageneralperspectiveonintrinsicmotivationcomplementarytoexistingframeworks. Theconnectionbetweenfreeenergyandempowermentnotedinpreviousliteratureisfurtherexplored,anditisarguedthatthemaximum-occupancyapproachinpracticeincorporatesanimplicitmodel-evidenceconstraint.Keywords: intrinsicmotivation;activeinference;empowerment;entropy1. IntroductionInpsychology,“intrinsicmotivation”referstothetendencyforintelligentcreaturestobemoti-vatedtodocertainthings(suchasexplore,learn,andgrow)evenintheabsenceofspecificexternalrewardsignals[1]. Thisparadigmhasincreasinglygainedtractioninmachinelearning,whereitisoperationalized as the idea that policies for action may be optimized based on structural featuresofagentsandagent-environmentinteractions,asagainsttraditionalapproacheslikereinforcementlearning,whichoptimizepoliciesbasedonadhocrewardfunctions.Anearly,andincreasinglyinfluential,formalaccountofintrinsicmotivationisbasedonempower-ment,definedasthecapacityofthechannellinkingagents’actions(actuatorstates)tosensoryfeedback(observations)[2,3]. Oneinterpretationofthisobjectiveisthatempoweredagents“keeptheiroptionsopen”,aswideaction-conditionedchannelcapacityentailsthatagentsareabletorealizeavarietyofstates(forwhichobservationsareaproxy).Theactiveinferenceframework[4]sharessimilarmotivations,andprovidesaBayesianmethodfor combining a general form of intrinsic motivation (i.e. curiosity or “epistemic drive”) [5] withagent-specific prior distributions over states or outcomes [6], which model homeostatic set pointsand can function like explicit rewards. The expected (variational) free energy (EFE), as discussedbelow,guidespolicyselectioninthisframeworkbysupplyinganempiricalprioroverpolicies,givenobservations.Morerecently,theobjectiveofmaximumpathoccupancyhasbeenproposedasaframeworkforintrinsicmotivation[7]. Onthisaccount,agentsaremotivatedtomaximizefutureaction-statepathoccupancy, whichcanbemeasuredintermsofboththeentropyoftheactiondistributionandtheentropyoftheensuingstatedistribution,givenaninitialstate.Thissomewhatmoreradicalperspectiveexplicitlyinvertstheperhapsnaturalassumptionthatdrivesforexplorationandcuriosityhaveevolvedasameanstoachievingreward,andineffectviewsrewardingstatesasinstrumentallyvaluableinenablingfutureexploration,i.e. avoidingabsorbingstatesthataffordlittleornoactionvariability(e.g.death).There are many other formal treatments of intrinsic motivation in the literature on machinelearning,somecloselyrelatedtothosejustdiscussed,suchaspioneeringworkonartificialcuriosity(seee.g. [8,9])andtreatmentsintermsofBayesiansurprise[10,11]. Here,thefocusismainlyontherelationshipbetweenactiveinferenceandempowerment,andontherelationshipofbothtomaximumoccupancywhichhasrecentlybeenproposedexplicitlyasanalternative.While[12]conductsacomparativeempiricalstudyofthesethreeframeworksforintrinsicmoti-vationonatoyproblemand[13]considershowactiveinference maybeformallyrelatedtobroaderschemesforintrinsicmotivation,comparativelylittleworkexistsontheformalandconceptualrela-tionsamongtheseframeworks. Here,Ihighlightthefactthatallthreecanbeunderstoodasvariationson the theme of constrained entropy maximization, a principle with deep connections to the freeenergyprincipleandactiveinference[14]. Iexploretheconnectionbetweenempowermentandactiveinference[15]bycastingtheempowermentobjectiveitselfexplicitlyasaformofvariationalinference.Ialsoarguethattheabilityofoccupancy-maximizingagents toexhibitapparentlygoal-directedbe-haviordepends on a “survival instinct” ormodel-evidence constraintimplicitinthefactorizationoftheoverall systemintoactionsandstates. Theseconsiderationsframeentropymaximization,underlocalconstraints,asthekernelofintelligenceandagency,withparticularfacetsofthisprocesssuchasempowerment,perception,curiosityandthe“willtolive”ascorollaries.Thefirstsectionbelowunpacksthethreeframeworksforintrinsicmotivationmentionedabove(empowerment,activeinference,andmaximumoccupancy)insomedetail,bothformallyandintermsofconceptualmotivation,andarticulatestheirtiestoconstrainedentropymaximization. Sectiontwolookscloselyatsomeconnectionsamongthesetheories,thendistillsafewgeneralconclusions.2. Threeformalaccountsofintrinsicmotivation2.1. EmpowermentTheempowermentobjectiveforintrinsicmotivation,originallyproposedin[2],isdefinedasthecapacityoftheinformationchannellinkinganagent’sactionstoitsobservationoftheeffectsofthoseactions. That is, given aspaceofpossibleobservationsO atfuturetimestep T anda sequence ofTactions A fromthepresenttimestepttothefuture,thereissomedistributionP(O |A )capturingt:T T t:Ttheprobabilisticdependenceofthefutureobservationontheactionstaken,andtheempowermentE ofanagentismeasuredasthecapacityCoftheinformationtransmissionchanneldefinedbythisdistribution:(cid:16) (cid:17)E =C P(O |A ) =maxI(A ;O )t T t:T t:T TP(A)Inthiscase,Cisdefinedasthemaximummutualinformationbetweenactionsandfutureobservations,I(A ),whentheconditionaldistributionP(O |A )isheldfixedandthedistributionoveractions,t:T;OT T t:TP(A),isallowedtovary.It is worth taking a moment to unpack this, as a detailed understanding will be useful forcomparisonsbelow. Themutualinformationisstandardlydefined,fortworandomvariablesXandY,asadouble sum equivalent to the KL divergence from the joint density P(X,Y) to the product ofthe marginals over X and Y:∑ ∑(cid:16) P(x,y) (cid:17)I = P(x,y)logP(x)P(y)y∈Yx∈X(cid:16) (cid:17)= D P(X,Y)||P(X)P(Y)KLIntuitively,thisexpressionmeasureshowdifferenttheactualjointdistributionisfromwhatitwouldbewerethetwovariablesindependent,i.e. howmuchinformationthevariablescarryaboutoneanother. Whilethismeasureis symmetric(i.e. thesameforXandY),itcanbebrokendownintermsofconditionalprobabilitiesineitherdirection. Sincethejointdensitycanbefactorizedintoapriorandaconditionaldensity,i.e. P(X,Y) = P(X)P(Y|X) = P(Y)P(X|Y),themutualinformation3of13canalsobeexpressedasanexpectedKLdivergencefromaconditionaldensityP(Y|X)tothemarginaloverY:∑ ∑(cid:16)P(x)P(y|x)(cid:17)I = P(x)P(y|x)log FactorizejointdistributionP(x)P(y)y∈Yx∈X(cid:34) (cid:35)∑ ∑(cid:16)P(y|x)(cid:17)= P(x) P(y|x)log CanceloutP(x