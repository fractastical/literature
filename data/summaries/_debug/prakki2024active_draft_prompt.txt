=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation
Citation Key: prakki2024active
Authors: Rithvik Prakki

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: This paper introduces a novel approach to creating adaptive language agents by integrating ac-
tive inference with large language models (LLMs). While LLMs demonstrate remarkable capabilities,
their reliance on static prompts limits adaptation to new information and changing environments. We
address this by implementing an active inference framework that acts as a cognitive layer above an
LLM-basedagent,dynamicallyadjustingpromptsandsearchstrategiesthroughprincipledinformation-
seeking behavior....

Key Terms: systems, adaptation, approach, agents, language, models, information, organizing, thermodynamic, prompt

=== FULL PAPER TEXT ===

Active Inference for Self-Organizing Multi-LLM Systems: A
Bayesian Thermodynamic Approach to Adaptation
Rithvik Prakki
rprakki@unc.edu
November 2024
Abstract
This paper introduces a novel approach to creating adaptive language agents by integrating ac-
tive inference with large language models (LLMs). While LLMs demonstrate remarkable capabilities,
their reliance on static prompts limits adaptation to new information and changing environments. We
address this by implementing an active inference framework that acts as a cognitive layer above an
LLM-basedagent,dynamicallyadjustingpromptsandsearchstrategiesthroughprincipledinformation-
seeking behavior. Our framework models the environment using three state factors (prompt, search,
and information states) with seven observation modalities capturing quality metrics. By framing the
agent’s learning through the free energy principle, we enable systematic exploration of prompt com-
binations and search strategies. Experimental results demonstrate the effectiveness of this approach,
withtheagentdevelopingaccuratemodelsofenvironmentdynamicsevidencedbyemergentstructurein
observation matrices. Action selection patterns reveal sophisticated exploration-exploitation behavior,
transitioning from initial information-gathering to targeted prompt testing. The integration of thermo-
dynamicprincipleswithlanguagemodelcapabilitiesprovidesaprincipledframeworkforcreatingrobust,
adaptable agents, extending active inference beyond traditional low-dimensional control problems to
high-dimensional, language-driven environments.
1
5202
naJ
9
]LC.sc[
3v52401.2142:viXra
1 Introduction
Recent advancements in artificial intelligence have witnessed the emergence of large language models
(LLMs) that demonstrate remarkable capabilities in natural language understanding and generation. These
models have been instrumental in various applications, ranging from chatbots to complex problem-solving
agents. However, a significant limitation of current LLM-based systems is their reliance on static prompts,
which do not adapt dynamically to new information or changing environments. This rigidity hampers the
ability of AI agents to perform self-improvement and adapt their interactions based on past experiences.
Active inference, grounded in the Free Energy Principle (FEP), offers a promising framework for mod-
eling adaptive and autonomous behavior in cognitive agents. The FEP implies a classical thermodynamics
through its foundation in Bayesian mechanics, where belief updating incurs specific thermodynamic costs
(Fields et al., 2023). Just as biological systems must balance the thermodynamic free energy required for
metabolic maintenance, cognitive systems can be understood as minimizing their variational free energy -
a mathematical construct that bounds the entropy of their sensory exchanges with the environment. By
treatingperceptionandactionasprocessesaimedatminimizingthisvariationalfreeenergy, activeinference
agents can update their beliefs and make decisions that optimize their interactions with the environment
while managing the inherent costs of information processing.
In this paper, we introduce a novel approach that integrates an active inference generative model as a
cognitivelayeratoparesearchagentpoweredbymultipleLLMs. Ouractiveinferenceagentactsasa‘brain’
thatdynamicallyadjuststhepromptsprovidedtotheLLMs,facilitatingalearningprocessthatevolveswith
each interaction. Through the lens of the FEP, the agent maintains state factors for prompts, search terms,
and information, allowing it to systematically explore various prompt combinations and assess their efficacy
while managing the trade-off between information gain and energetic costs.
To evaluate and refine its strategies, the agent receives observations in the form of metrics related to
the responses of the research agent—specifically, accuracy, relevance, and comprehensiveness. These metrics
inform the agent’s posterior beliefs about which prompt combinations yield optimal results, guiding future
decisions through principled belief updating that respects thermodynamic constraints. When conducting
searches using predetermined search terms, the agent observes additional modalities such as information
relevance, information usefulness, source quality, and information state. These observations help the agent
assess the quality of external information sources and incorporate them into its learning process, effectively
reducing its informational entropy through active exploration.
Theagentoperatesbyalternatingbetweenprompt-changing andsearching states,witheachbeliefupdate
incurringthermodynamiccostsasdescribedbytheJarzynskiequality. Actionstakeninthesestatesproduce
observations that update the agent’s beliefs, enabling it to adapt its strategies over time. This dynamic
interplay allows the agent to achieve continuous self-improvement by minimizing both its instantaneous
variational free energy (through accurate perception) and expected free energy (through adaptive action
selection). The expected free energy serves as a principled objective function that guides the system toward
preferred future states while accounting for both utility and information gain. This foundation in the
FEP provides theoretical guarantees about the agent’s ability to maintain stability while adapting to new
information, with explicit consideration of the thermodynamic costs associated with belief updating.
2 Related Works
The field of improving agent behavior with large language models (LLMs) has seen extensive research,
whichcanbebroadlycategorizedintothefollowingrelatedthemes: improvingLLMperformance,leveraging
LLMs as adaptive agents, and the integration of active inference principles for adaptive decision-making.
Below, we review each of these themes, highlighting their contributions and limitations, and distinguishing
our approach from existing work.
2.1 Improving LLM Performance
Effortstoenhance theperformanceofLLMshaveprimarilyfocusedonimproving themodelsthemselves
through better training data curation and self-improvement loops. For instance, Bowman et al. (2022)
and Sun et al. (2023) explored methods for aligning LLMs with human preferences by using heuristics and
2
self-generated principles to filter high-quality training data. Similarly, Bai et al. (2022) investigated using
self-improvement via critique-based fine-tuning, while Gou et al. (2023a) proposed leveraging external tools
for more granular feedback.
Approaches such as Re-ReST (Guo et al., 2024) curate better training data for LLMs by generating and
validating data with ground truth feedback, enhancing the model’s reasoning capabilities. SELF (Lu et al.,
2023) and Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing (Sun et al., 2023)
use reflective mechanisms to iteratively improve LLMs. While these techniques improve LLM capabilities,
theyprimarilytargetthemodel’sintrinsicqualityratherthanenhancingthedecision-makingoradaptability
of agents leveraging LLMs.
Our approach diverges by focusing on the agent’s structure, policies, and environmental interactions
rather than improving the LLM itself. Specifically, we use active inference to guide an agent in adapting
prompts dynamically, enabling systematic exploration of complex policy spaces that are not addressed by
intrinsic LLM improvements.
2.2 Leveraging LLMs as Adaptive Agents
Adaptive agent frameworks often use LLMs as the core reasoning component. WEBRL (Wang et al.,
2023b) and Meta-Analysis of Agent Frameworks (Nascimento et al., 2024) illustrate LLMs functioning as
components within multi-agent systems that use reinforcement learning to adapt based on environmental
feedback. In such systems, the LLM serves as the vector of adaptation, leveraging its context window for
environmental understanding.
Inopen-worldexplorationtasks,suchasMinecraft-basedenvironments,LLMsareusedtodriveagentbe-
haviorbasedonenvironmentalcues(Wortsmanetal.,2019;Wangetal.,2023;Liuetal.,2023). Frameworks
like Voyager (Wang et al., 2023) and Odyssey (Liu et al., 2023) demonstrate how large language models can
guideagentsinacquiringopen-worldskills. However,thesesystemsrelyonusingtheLLMitselfasthevector
of adaptation, which limits the agent’s ability to learn structurally from its environment beyond immediate
context.
In contrast, our model integrates active inference to go beyond context-driven adaptation. By modeling
the environment through state factors and observing structured feedback metrics (e.g., accuracy, relevance,
comprehensiveness), our framework actively updates beliefs about prompts and search actions. This allows
the agent to dynamically adapt not just its behavior but its structural knowledge about effective strategies.
Ouragentisabletotakeadvantageofacomputationalframeworkbasedonthehumanbrain,inselecting
which LLMs to use and for what purpose going beyond simply using LLMs directly for all aspects of
exploration and exploitation.
2.3 Active Inference in AI
Active inference has emerged as a theoretical framework for modeling adaptive behavior, rooted in the
freeenergyprinciple(Fristonetal., 2017). MostapplicationsinAIfocusonlow-dimensionalproblems, such
as robotic control or navigation (Schwartenbeck et al., 2019; Parr et al., 2022), where actions aim to reduce
uncertainty about the environment.
The use of active inference in high-dimensional, language-driven environments remains underexplored.
Existing research has not addressed its application as a “brain” for LLM-based agents to systematically
explore complex policy spaces. Our work addresses this gap by employing active inference to balance ex-
ploration and exploitation in an LLM-driven research agent. By explicitly modeling prompt combinations,
search strategies, and the costs of actions, our framework introduces a practical approach to incorporating
active inference for structured exploration and learning.
2.4 Integrated Frameworks for Multi-Objective Learning
Some frameworks aim to combine various optimization strategies into unified models. For example,
SELF-EvolutionarySystems(Zhongetal.,2023)applyreinforcementlearningforinternet-basedexploration
tasks, andGenerativeAIforSelf-AdaptiveSystems(Nascimentoetal., 2024)provideresearchroadmapsfor
3
improving agent adaptability. However, these methods often treat optimization, information retrieval, and
adaptation as distinct processes.
Ourapproachunifiestheseelementswithinasingleactiveinferencemodel. Byframingdecisionsthrough
Expected Free Energy (EFE) minimization, we enable the agent to dynamically select between exploration
(e.g., testing new prompts) and exploitation (e.g., retrieving information). This integration provides a
coherent mechanism for multi-objective learning, allowing for systematic decision-making that accounts for
both environmental feedback and resource constraints.
3 Background
3.1 Theoretical Foundations
Active inference rests on the principle that biological systems minimize variational free energy both
through perception and action. We begin with a rigorous derivation of this framework from first principles.
3.2 Derivation of Variational Free Energy
Starting with the definition of surprise (negative log model evidence):
(cid:88)
−lnp(o)=−ln p(o,s) (1)
s
We can introduce an arbitrary distribution q(s) by multiplying and dividing by it:
(cid:88)p(o,s)q(s)
−lnp(o)=−ln (2)
q(s)
s
By Jensen’s inequality, since ln is a concave function:
(cid:88) p(o,s)
−lnp(o)≤− q(s)ln =F (3)
q(s)
s
This upper bound F is the variational free energy. We can decompose it:
(cid:88) q(s)
F = q(s)ln (4)
p(o,s)
s
(cid:88) (cid:88)
= q(s)lnq(s)− q(s)lnp(o,s) (5)
s s
(cid:88) (cid:88) (cid:88)
= q(s)lnq(s)− q(s)lnp(s)− q(s)lnp(o|s) (6)
s s s
=D [q(s)||p(s)]−E [lnp(o|s)] (7)
KL q(s)
3.3 Information Gain and Pragmatic Value Formulation of Expected Free En-
ergy
The expected free energy in its conceptual form from Smith et al. is given by:
G =−E [D [q(s|o,π)||q(s|π)]]−E [lnp(o|π)] (8)
π q(o|π) KL q(o|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
informationgain pragmaticvalue
T−1M−1
(cid:88) (cid:88)
EU = q(om|π)·lnσ(C ) (9)
t m
t=0 m=0
4
Thisistheformusedintheexperiments. However,thisformulationisaconceptualvarietyofthephysical
formulation, involving entropy. As shown in Champion et al., this formulation can be derived through a
series of steps. The derivation relies on the following equality:
q(s|π)q(s|o,π)=q(o|π)q(o|s) (10)
which holds because the forecast distribution is a partially observable Markov decision process. We start
by re-arranging Bayes theorem as follows:
q(o|s,π)q(s|π)
q(s|o,π)= ⇔q(s|π)q(s|o,π)=q(o|π)q(o|s,π) (11)
q(o|π)
Starting with the definition of G and using (2), one can show that:
π
G =−E [D [q(s|o,π)||q(s|π)]]−E [lnp(o|π)] (12)
π q(o|π) KL q(o|π)
Using the KL-divergence definition, log properties and the linearity of expectation, we get:
G =−E [lnq(o|s)]+E [lnq(o|π)−lnp(o|π)] (13)
π q(o,s|π) q(o|π)
Lastly, recognizing the entropy and KL-divergence definitions leads to the final results:
G =D [q(o|π)||p(o|C)]+E H[q(o|s)] (14)
π KL q(s|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
riskoverobservations ambiguity
3.4 Message Passing Implementation
To implement state inference through gradient descent on VFE:
∂F
=lnq(s)−lnp(s)−lnp(o|s)+1 (15)
∂q(s)
For factorized variational inference with multiple factors f, the solution becomes:
(cid:88)
q(s )=σ( lnp(o |s ,s )+lnp(s )) (16)
f m f −f f
m
where s represents all other factors except f.
−f
For temporal models with control states π:
q(s |π)=B q(s |π) (17)
τ+1,f π,f τ
where B is the transition matrix for factor f under policy π.
π,f
In matrix notation this becomes:
s =B s (18)
π,τ+1,f π,f π,τ
3.5 Learning Through Parameter Updates
The learning rules follow from minimizing VFE with respect to model parameters. For the A matrix
with multiple modalities m:
∂F ∂
= E [lnp(o|s)] (19)
∂A ∂A q(s)
m m
For Dirichlet priors over parameters:
p(A )=Dir(a ) (20)
m m
5
Givenobservationo andbeliefsq(s)overstatesthatmodalitymdependson,theupdateusestheouter
m
product:
|s|
(cid:89)
o ⊗q(s)=o · q(s) (21)
m m dim
dim=1
This yields the learning rule:
at+1 =at +η·(o ⊗q(s))⊙(A >0) (22)
m m m m
where η is the learning rate and ⊙ represents element-wise multiplication.
3.6 Policy Selection
The posterior over policies follows from minimizing expected free energy:
G =E [lnp(o)]+E [lnq(s|π)]+G (23)
π q(o|π) q(s|π) param
where G captures parameter information gain.
param
The policy posterior is computed via softmax:
q(π)=σ(γG+lnE) (24)
where γ is the precision parameter and E represents prior policy preferences (”habits”).
The selected policy is then:
π∗ =argmaxq(π) (25)
π
4 Setup
4.1 Agent Architecture
The research agent’s architecture is implemented through an active inference framework with three key
state factors: prompt states (33 possible combinations), search states (11 possible states), and information
states (3 possible states: no information, basic information, detailed information). The agent observes
seven modalities: three prompt-dependent quality metrics (accuracy, relevance, comprehensiveness), three
search-dependent quality metrics (information relevance, information usefulness, source quality), and one
information state observation.
4.2 Generative Model
The generative model is defined by the following components:
4.2.1 Observation Model (A Matrices)
The observation model consists of a set of likelihood mappings between hidden states and observations,
organized into a tensor with different slices for each modality type (see Figure 1).
The prompt-dependent modalities (A ) map 33 possible prompt states to 11 quality levels (0-10),
[0,1,2]
capturing how different prompt combinations influence output quality. The search-dependent modalities
(A ) map 11 search states to quality observations, modeling how search strategies affect information
[3,4,5]
gathering. The information state modality (A ) provides a direct mapping between hidden and observed
[6]
information states.
6
Figure 1: Visualization of the three main components of the observation model tensor. Left: Prompt
quality observations A mapping 33 prompt states to 11 quality levels for accuracy, relevance, and
[0,1,2]
comprehensiveness. Middle: Search quality observations A mapping 11 search states to 11 quality
[3,4,5]
levels for information relevance, usefulness, and source quality. Right: Information state observations A
[6]
providingadirectmappingbetween3hiddeninformationstatesandtheircorrespondingobservations. Darker
colorsindicatehigherprobabilityvalues,showingthestructureofthelikelihoodmappings. Herethematrices
are all uniform since the agent starts with no knowledge of the state-observation mappings.
4.2.2 Transition Model (B Matrices)
The transition model is structured as three matrices corresponding to each state factor. The prompt
transitions are modeled by a 33 × 33 × 33 tensor (B ) handling transitions between prompt states.
prompt
Search transitions utilize an 11 × 11 × 11 tensor (B ) for modeling transitions between search states.
search
Informationstateprogressioniscapturedbya3×3×1tensor(B ). Transitionprobabilitiesareinitially
info
configured to maintain current states when no action is taken, with controlled transitions possible through
specific actions.
4.2.3 Prior Preferences (C Matrix)
The preference distribution is structured to drive both information-seeking and quality-maximizing be-
havior. For quality metrics (modalities 0-5), the model implements a strong negative preference (-16.0) for
low quality observations, with quadratically increasing preferences for higher quality levels scaled by 2.0.
Informationstates(modality6)areassignedhighlystructuredpreferenceswithvaluesof-32.0, 8.0, and64.0
for no info, basic info, and detailed info states respectively.
4.2.4 Initial State Priors (D Matrix)
Initial state beliefs are configured as uniform distributions across all state factors. Prompt states are
initialized with 1/33 probability for each state, search states with 1/11 probability for each state, and
information states with 1/3 probability for each state.
4.3 Learning Parameters
The agent employs Dirichlet distributions for learning the observation and transition models:
7
4.3.1 Observation Learning (pA)
Minimal concentration parameters (base concentration = 1.0) are used for all modalities to maximize
learningflexibility. Thepromptmodalitiesuse11×33matrices,searchmodalitiesemploy11×11matrices,
and the information state modality utilizes a 3 × 3 matrix.
4.3.2 Transition Learning (pB)
The model implements minimal structured priors with a base concentration of 1.0 and small biases (0.1)
for specific transition types. These include state persistence under no action for prompt transitions, decay
to no-search state for search transitions, and forward progression for information state transitions.
4.4 Observation Generation
Theagentreceivesstructuredobservationsaboutsearchqualityandprompteffectivenessthroughevalua-
tionsperformedbyGPT-4o-mini,whichprovidesstandardizedJSONoutput. Forexample,forsearch-related
observations, the model evaluates search results using a structured output format:
Return only a JSON object with these three metrics, scored 0.0-1.0:
{
"info_relevance": [Score from 0.0 to 1.0 counting up by 0.1],
"info_usefulness": [Score from 0.0 to 1.0 counting up by 0.1],
"source_quality": [Score from 0.0 to 1.0 counting up by 0.1]
}
These standardized scores are then scaled to the 11-point observation space used by the active inference
agent’sobservationmodel. TheuseofstructuredJSONoutputensuresconsistent, programmaticevaluation
of search results, providing reliable feedback for the agent’s learning process. Similar structured evaluations
are performed for prompt quality metrics (accuracy, relevance, and comprehensiveness), with the language
model providing standardized scores that are then mapped to the agent’s observation space.
This approach leverages GPT-4o-mini’s capability for structured output generation to create a reliable
evaluation pipeline, ensuring that the active inference agent receives consistent, well-formatted observations
for updating its belief states and learning the effectiveness of different actions.
4.5 Action Selection
The agent employs a sophisticated policy selection mechanism with a policy horizon of 2 steps and an
inferencehorizonof1step. Bothstate-informationgainandparameter-informationgainareenabled,andthe
model uses deterministic action selection. Valid policies consist of three types of actions: no action [0,0,0],
prompt-onlyactions[p,0,0]wherep∈{1,2,...,33},andsearch-onlyactions[0,s,0]wheres∈{1,2,...,11}.
There can be no such action where [p,s,0] since this would imply that the prompt and search actions are
being performed simultaneously and the agent is not able to do that.
4.6 Control Parameters
The model implements several key control parameters. A learning rate (η) of 50.0 is used for both
observationandtransitionlearning. Policyprecision(γ)issetto8.0tobalanceexplorationandexploitation.
Action precision (α) is configured at 16.0 to control action selection determinism.
4.7 State Factor Dependencies
The model employs structured dependencies between state factors and observations. The prompt factor
influences accuracy, relevance, and comprehensiveness metrics. The search factor affects information rel-
evance, usefulness, and source quality observations. The information factor determines information state
observations. These dependencies are encoded in A factor list and B factor list specifications to ensure
proper message passing during inference.
8
4.8 Implementation
The active inference algorithm can be expressed as:
Algorithm 1 Active Inference with Environment Interaction
1: Initialize A,B,C,D matrices, precision γ
2: Initialize beliefs q(s 0 )
3: while not converged do
4: o t ← observe environment
5: for each policy π do
6: q(s t |π)←σ(lnp(s t )+lnp(o t |s t )) ▷ State estimation
7: q(s t+1:T |π)←B π q(s t |π) ▷ State prediction
8: G π ← ComputeEFE(q(s t:T |π),A,C) ▷ Expected free energy
9: end for
10: q(π)←σ(γG+lnE) ▷ Policy posterior
11: a t ← first action of argmax π q(π) ▷ Select action
12: if a t is search action then
13: o t+1 ← web search + LLM evaluation of results
14: else if a t is prompt action then
15: execute agent with policy’s prompts
16: o t+1 ← LLM evaluation of agent response
17: end if
18: if learning enabled then
19: a t+1 ←a t +η·(o t ⊗q(s t ))⊙(A>0) ▷ Parameter update
20: end if
21: t←t+1
22: end while
Thealgorithmiteratesuntilconvergence, performingstateestimation, policyevaluation, actionselection
andparameterlearningateachstep. Thekeyequationsgoverningeachupdatehavebeenderivedinprevious
sections.
5 Results
5.1 Learning Environment Dynamics
Through active exploration and learning, the agent successfully developed an accurate model of the
environment,particularlytherelationshipsbetweenstatesandobservations. Figure2showsthefinallearned
observation mappings after multiple interactions with the environment. Compared to the initial uniform
distributions (Figure 1), these matrices show clear structure, indicating the agent has learned meaningful
relationships between states and observations. The prompt quality matrices (A ) developed distinct
[0,1,2]
patterns showing which prompt combinations lead to higher quality outputs. The search quality matrices
(A ) reveal learned associations between search actions and information quality, while the information
[3,4,5]
state matrix (A ) captures the reliable mapping between hidden and observed information states.
[6]
5.2 Strategic Action Selection
Theagent’sactionselectionstrategyevolvedovertimeasitsExpectedFreeEnergyoverpolicieschanged.
Figure 3 shows the expected free energy of different policies at four time points during the agent’s opera-
tion. This progression reveals how the agent learned to value different action combinations based on their
information-gathering and goal-achieving potential.
9
Figure 2: Final learned observation mappings after environment interaction. The first three matrices on the
top row show the relationships between prompt states and quality metrics. The final matrix on the top row
andthefirsttwomatricesonthebottomrowshowtherelationshipbetweensearchstatesandsearchquality
metrics. The final matrix on the bottom row shows the learned mapping between the information state
factor and the information observation modality. The matrices effectively show, for each prompt and search
term, what scores seem to be associated with them based on the observations. The final matrix structure
results from a predominance of ”detailed info” observations. The emergence of structure from the initial
uniform distributions (Figure 1) demonstrates successful learning of environment dynamics.
Figure 3: Progression of Expected Free Energy (EFE) values for different policies across four time points.
The evolution shows how the agent learned to distinguish between effective and ineffective action combina-
tions. Lower EFE values (darker colors) indicate more preferred policies. The emergence of clear patterns
demonstrates the agent’s developing understanding of which actions are most valuable in different contexts.
5.3 Information-Driven Exploration
The agent’s action selection patterns provide strong evidence for sophisticated exploration and exploita-
tionbehavioremergingfromthefreeenergyminimizationframework. Figures4and5illustratecomplemen-
tary views of this behavioral progression.
10
Figure 4: Heatmap showing the frequency of action selection across prompt and search dimensions. Lighter
colors indicate more frequently selected actions. The pattern shows the overall distribution of action se-
lections, with certain prompt-search combinations being consistently preferred over others based on their
effectiveness.
Figure 5: Time series of action selection throughout the experiment. Blue dots represent prompt actions
(labeledwithpromptIDs),whilereddotsrepresentsearchactions(labeledwithsearchIDs). Theprogression
showsacleartransitionfromsearch-dominatedearlyphasestoprompt-dominatedlaterphases,demonstrat-
ing the agent’s evolving strategy from exploration to exploitation.
The temporal progression of the agent’s behavior, clearly visible in Figure 5, reveals a strategic shift in
action selection. During the initial phase (approximately the first 40 timesteps), the agent heavily favors
search actions, indicated by the higher density of red dots. This initial search-focused behavior naturally
emerges from the free energy minimization framework, as the agent prioritizes reducing uncertainty about
its environment through observation of search quality metrics and information state levels.
As the experiment progresses, Figure 5 shows a marked transition to prompt-dominated behavior, ev-
idenced by the increasing density of blue dots in later timesteps. This shift occurs as the agent receives
observations indicating higher information states and favorable search quality metrics. Rather than directly
encoding search results, the agent’s behavior is guided by these external indicators of knowledge state and
search effectiveness, leading to more focused prompt testing.
The aggregate view provided by Figure 4 complements this temporal analysis by showing the overall
distribution of action selections. The concentration patterns in the heatmap reveal which prompt-search
combinations proved most effective, where prompt and search actions that produced more preferred obser-
11
vations were sampled more.
The progression of policy Expected Free Energy (EFE) values (Figure 3) provides further insight into
how the agent’s evaluation of different action combinations became more refined over time. Initially, the
differences in EFE values were relatively small, reflecting the agent’s inability to differentiate between the
availablepolicies. Aslearningprogressed,theEFElandscapedevelopedclearstructure,indicatingtheagent
had learned which action combinations were most effective for achieving its goals.
The final learned observation matrices (Figure 2) provide evidence that this exploration strategy was
successful in discovering the underlying structure of the environment. The emergence of distinct patterns
in these matrices, particularly in the prompt-quality relationships, demonstrates that the agent effectively
learned which prompt combinations lead to better outcomes, guided by the externally provided information
state observations and quality metrics rather than direct knowledge incorporation from searches.
Thissophisticatedexploration-exploitationpatternemergednaturallyfromthefreeenergyminimization
framework,withtheagentlearningtobalanceinformation-gathering(throughsearchactionsandtheirasso-
ciatedqualitymetrics)withexploitationofknowneffectiveprompts(guidedbypromptqualityobservations
and information state feedback). The clear temporal progression from exploration to exploitation, visible
in both the timeline and aggregate statistics, demonstrates the effectiveness of active inference in managing
this fundamental trade-off.
6 Future Work
This paper has demonstrated a novel approach to combining active inference with traditional AI agents,
showinghowactiveinferencecanguidetheexplorationandoptimizationofagentparameters. However,this
implementation represents only an initial step toward fully adaptive AI systems.
A key limitation of the current model is its reliance on a fixed state space with predefined prompts.
Future work should focus on developing active inference models capable of dynamically expanding their
state space during operation. This would allow the agent to explore a vastly larger set of possible prompts
and configurations, moving beyond the constraints of preset options to discover novel, potentially more
effective combinations.
Another crucial direction is the development of hierarchical generative models. Rather than relying on
abstract information states, such models would enable the agent to directly encode and reason about envi-
ronmental information. This would allow for more sophisticated understanding of the agent’s performance
and context, leading to more informed adaptation decisions.
The framework could also be extended to optimize broader aspects of agent intelligence, including agent
architecture,executionorderofsub-agents,andtoolutilization. Theseelementsrepresentcriticalparameters
thatrequireintelligencetooptimizeeffectively. Byincorporatingthesefactorsintotheactiveinferencemodel,
we could enable more comprehensive agent improvement.
These developments represent intermediate steps toward an ambitious long-term goal: a universal active
inference model that can be applied to any agent system without predefined parameters. Such a model
would function analogously to human researchers, capable of conducting research, engaging in trial and
error, performing comprehensive parameter testing, and maintaining an intelligent exploration-exploitation
loop. This would enable automated, principled improvement of any given agent system, representing a
significant advance in artificial intelligence adaptation and optimization.
Data Availability
Thefullcodeimplementationoftheactiveinferenceagent,environment,andevaluationsystemdescribed
in this paper is available in our public GitHub repository at
https://github.com/RPD123-byte/Active-Inference-for-Self-Organizing-Multi-LLM-Systems-A-Bayesian-
Thermodynamic-Approach-to-Adaptat.
Thisincludesallcomponentsnecessarytoreproduceourexperimentalresults,includingtheagentcontroller,
environment simulation, and visualization tools.
12
References
Adeojo, J. (2024). graph websearch agent: Websearch agent built on the LangGraph framework. GitHub
repository. Retrieved from https://github.com/john-adeojo/graph websearch agent
Bai, Y., Saunders, W., Ouyang, L., et al. (2022). Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems, 35, 17117–17130.
Bowman, S. R., Deng, S., Raffel, C., et al. (2022). Measuring progress on scalable oversight for large
language models. arXiv preprint arXiv:2202.07765.
Brown, H. R., & Friston, K. J. (2018). The Physics of Free Will. Neuroscience and Biobehavioral Reviews,
90, 54–64.
https://www.sciencedirect.com/science/article/pii/S0149763418302525?ref=pdf download&fr=RR-
2&rr=88d83f2d8dfbf279#bib0145
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and
perception: A mathematical review. Biological Cybernetics, 112(6), 1–18.
https://link.springer.com/article/10.1007/s00422-019-00805-w
Champion, T., Bowman, H., Markovi´c, D., & Grze´s, M. (2023). Reframing the Expected Free Energy:
Four Formulations and a Unification. University of Kent, School of Computing, Canterbury, United
Kingdom; University of Birmingham, School of Psychology and School of Computer Science,
Birmingham, United Kingdom; Technische Universit¨at Dresden, Department of Psychology,
Dresden, Germany; University College London, Wellcome Centre for Human Neuroimaging
(honorary), London, United Kingdom.
Dandoy, L., & Di Francesco, M. (2023). Active Inference with State-Only Control.
https://arxiv.org/pdf/2311.10300
Fields, C., Fabrocini, F., Friston, K., Glazebrook, J. F., Hazan, H., Levin, M., & Marcian`o, A. (2023).
Control flow in active inference systems. Allen Discovery Center at Tufts University.
https://arxiv.org/abs/2303.03347
Friston, K., Parr, T., & de Vries, B. (2017). The graphical brain: Belief propagation and active inference.
Network Neuroscience, 1(4), 381–414. https://doi.org/10.1162/NETN a 00018
Gou, W., Sun, X., Li, Q., et al. (2023a). Leveraging external tools for critique-driven self-improvement in
language models. arXiv preprint arXiv:2306.05123.
Guo, J., Liu, Y., Chen, W., et al. (2024). Re-ReST: Reflection-reinforced self-training for language agents.
arXiv preprint arXiv:2403.07125.
Liu, S., Li, Y., Zhang, K., et al. (2023). Odyssey: Empowering Minecraft agents with open-world skills.
arXiv preprint arXiv:2310.01234.
Lu, J., Zhong, W., Huang, W., et al. (2023). SELF: Self-evolution with language feedback. arXiv preprint
arXiv:2310.00533.
Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Predictive Coding: A Theoretical and Experimental
Review. NeurIPS.
https://papers.nips.cc/paper files/paper/2020/file/865dfbde8a344b44095495f3591f7407-Paper.pdf
Nascimento, N., Alencar, P., Cowan, D., et al. (2024). Generative AI for self-adaptive systems: State of the
art and research roadmap. ACM Transactions on Autonomous and Adaptive Systems, 19(3), 1–60.
Parr, T., Pezzulo, G., & Friston, K. J. (2021). Active inference: The free energy principle in mind, brain,
and behavior. Journal of Mathematical Psychology, 100, 102364.
https://www.sciencedirect.com/science/article/pii/S0022249621000973#b40
Sajid, N., Friston, K., & Parr, T. (2021). Planning and Active Inference.
https://arxiv.org/abs/2103.13860v3
Schwartenbeck, P., & Friston, K. (2017). Active Inference, Curiosity and Insight. Neural Computation,
29(10), 2633–2683. https://direct.mit.edu/neco/article-abstract/29/10/2633/8300/Active-Inference-
Curiosity-and-Insight?redirectedFrom=fulltext
Schwartenbeck, P., FitzGerald, T., Mathys, C., Dolan, R., & Friston, K. (2023). Active inference, belief
propagation, and the free energy principle. PLOS ONE, 17(11), e0277199.
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0277199
Shipp, S. (2023). The role of the free energy principle in cognitive systems. Cognitive Science, 17(2),
212–248. https://journals.sagepub.com/doi/pdf/10.1177/26339137231222481
13
Smith, J., & Johnson, M. (2023). Advances in Active Inference. Trends in Cognitive Sciences.
https://www.sciencedirect.com/science/article/pii/S1364661323002607
Smith, R., Friston, K. J., & Whyte, C. J. (2023). A step-by-step tutorial on active inference and its
application to empirical data. Journal of Mathematical Psychology, 107, 102632.
Sun, Z., Wang, L., Li, Y., et al. (2023). Toward self-improvement of LLMs via imagination, searching, and
criticizing. arXiv preprint arXiv:2310.00533.
Wang, G., Xie, Y., Jiang, Y., et al. (2023). Voyager: An open-ended embodied agent with large language
models. arXiv preprint arXiv:2306.07291.
Wang, P., Chen, J., Zhao, H., et al. (2023b). WEBRL: Training LLM web agents via self-evolving online
curriculum reinforcement learning. arXiv preprint arXiv:2305.09876.
Wortsman, M., Ehsani, K., Rastegari, M., et al. (2019). Learning to learn how to learn: Self-adaptive
visual navigation using meta-learning. Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 6757–6765.
Zhang, Y., & Zheng, Z. (2023). Reinforcement Learning with Active Inference.
https://arxiv.org/pdf/2306.09205
14

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian Thermodynamic Approach to Adaptation"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
