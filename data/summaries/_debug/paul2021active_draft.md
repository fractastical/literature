### Active Inference for Stochastic ControlThe authors investigate active inference as a method for stochastic control, presenting a novel approach to address limitations in existing control algorithms. This summary extracts key claims, findings, and methodological details from the paper.1. "Active inference, a corollary of the free energy principle, is a formal way of describing the behaviour of self-organising systems that interface with the external world and maintain a consistent form over time [1,2,3]." The authors establish the foundational concept of active inference as a theory of self-organising systems.2. "Despite its roots in neuroscience, active inference has snowballed to many fields owing to its ambitious scope as a general theory of behaviour [4,5,6]." The paper highlights the broad applicability of active inference across various domains.3. "Optimal control is one such field, and several recent results place active inference as a promising optimal control algorithm [7,8,9]." The authors position active inference as a viable solution for optimal control problems.4. "However, research in the area has largely been restricted to low-dimensional and deterministic settings where defining, and evaluating, policies (i.e., action trajectories) is feasible [9]." The authors acknowledge the limitations of previous research.5. "This follows from the active inference process theory that necessitates equipping agents a priori with sequences of actions in time." The paper explains the core requirement of active inference – providing agents with pre-defined action sequences.6. "With8 available actions and a time-horizon of15, the total number of (definable) policies that would need to be considered →3.5×1013." The authors illustrate the computational challenge posed by the vast number of possible policies in complex control problems.7. "This becomes more of a challenge in stochastic environments with inherently uncertain transition dynamics, and no clear way to constrain the large policy space to a smaller subspace." The paper identifies the key difficulty in applying active inference to stochastic control.8. "Happily, recent advancements like sophisticated inference [10] propose a modified planning approach for finite-temporal horizons [11]." The authors introduce the key innovation – sophisticated inference, a recursive formulation for expected free energy.9. "Sophisticated inference [10], compared to the earlier formulation [9,12,9], provides a recursive form of the expected free energy that implements a deep tree search over actions (and outcomes) in the future." The authors detail the core mechanism of sophisticated inference.10. "We reserve further details for Section3.2." The authors indicate that the full technical details of sophisticated inference will be presented in a subsequent section.11. "In this paper, we evaluate the utility of active inference for stochastic control using the sophisticated planning objective. For this, we utilize the windy grid-world task [13], and assess our agent’s performance when varying levels of complexity are introduced.g., stochastic wind, partial observability, and learning of the transition dynamics." The authors describe the experimental setup – the windy grid-world task – and the complexity levels they investigate.12. "Through the numerical simulations, we demonstrate that active inference, compared to a Q-learning agent [13], provides a promising approach for stochastic control." The authors state their primary finding – the effectiveness of active inference compared to a standard Q-learning algorithm.13. "In this section, we describe the windy grid-world task, with additional complexity, used for evaluating our active inference agent (Section3). This is a classic grid-world task from reinforcement learning [13], with a predefined start (S) and goal (G) states (Fig.1)." The authors provide a detailed description of the experimental environment.14. "The aim is to navigate as optimally (i.e., with a minimum time horizon) as possible, taking into account the effect of the wind along the way. The wind runs up ward through the middle of the grid, and the goal state is located in one such column. The strength of the wind is noted under each column in Fig.1, and its amplitude is quantified by the number of columns shifted upward that were unintended by the agent. Here, the agent controls its movement through8 available actions (i.e., the King’s moves): North (N), South (S), East (E), West (W), NW, SW, SE, and NE. Every episode determines either at the allowed time horizon, or when the agent reaches the goal state." The authors elaborate on the task’s rules and objectives.15. "Wind properties In a deterministic setting, the amplitude of the wind remains constant. Conversely, in a stochastic setting, for wind columns, the effect varies by one from the mean values. We consider two settings: medium and highly stochastic. For medium stochasticity, the mean value is observed70% of the time and similarly40% of the time in the high stochastic case (Table2)." The authors detail the different stochasticity levels used in the experiment.16. "Table2: Stochastic nature of wind Medium70% of the time15% each for ±1 High40% of the time30% each for ±1" The authors present the specific parameters for the stochastic wind conditions.17. "Observability In the fully observable setting, the agent is aware of the current state i.e., there is no ambiguity about the states of affair. We formalise this as a Markov decision processes (MDP). Whereas in the partially observable environment, the agent measures an indirect function of the associated state i.e., current observation. This is used to infer the current state of the agent. We formalise this as a partially observable MDP (POMDP)." The authors explain the difference between fully observable and partially observable settings.16. "Transition dynamics known to agent In the known setup, the agent is equipped with the transition probabilities before hand. However, if these are not known, the agent begins the trials with a uninformative (uniform) priors and updates its beliefs (Eq.9) [14,12,9]." The authors describe the transition dynamics used in the experiment.17. "The generative model is formally defined as a tuple of finitedets (S,O,T,U,B,C,A): s∈S :stateswhereS ={1,2,3,...,70}ands isapredefined(fixed)startstate.o ∈ O : whereo = s, in the fullyobservable setting, and in partialobservabilityo=f(s)7.T ∈N+,andisafinitetimehorizonavailableperepisode.a∈U :actions,whereU ={N,S,E,W,NW,SW,SE,NE}.B :encodesthe transitiondynamics,P(s |s ,a ,B)i.e., the probabilitythatactiona takenatstates attimet−1resultsins attimet.C :priorpreferencesoveroutcomes,P(o|C).Here,C preferenceforthepredefinedgoalstate.A :encodes the likelihood distribution, P(o |s ,A) for the partially observable setting τ τ". The authors formally define the generative model.17. "To plan, expectedfreeenergy(G) [9] is used for planning in finite temporal horizons. We use the recursive formulation introduced in [10]. This is defined recursively as the immediate expected free energy plus the expected free energy for future actions. For a rigorous treatment of it, please refer to [10,11]. In this scheme, actions are considered as random variables at each time-step, assuming successive actions are conditionally independent. This comes with a cost of having to consider many action sequences in time. The search for policies in time is optimised both by restricting the search over future outcomes which has a non-trivial posterior probability (e.g., >1/16) as well as only evaluating policies with significant prior probabilities (e.g., >1/16) calculated from the expected free energy (i.e., Occam’s window). In the partially observable setting, the expected free energy accommodates ambiguity in future observations, prioritizing both preferences seeking as well as ambiguity reduction in observations [10]." The authors detail the planning mechanism.18. "We reserve further details for Section3.2." The authors indicate that the full technical details of sophisticated inference will be presented in a subsequent section.19. "The authors state: 'Despite its roots in neuroscience, active inference has snowballed to many fields owing to its ambitious scope as a general theory of behaviour [4,5,6].' The paper argues that optimal control is one such field, and several recent results place active inference as a promising optimal control algorithm [7,8,9]." The authors highlight the broad applicability of active inference across various domains.20. "They note: 'However, research in the area has largely been restricted to low-dimensional and deterministic settings where defining, and evaluating, policies (i.e., action trajectories) is feasible [9].' The study demonstrates that optimal control is one such field, and several recent results place active inference as a promising optimal control algorithm [7,8,9]." The authors highlight the broad applicability of active inference across various domains.21. "The authors state: 'This follows from the active inference process