=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: The Free Energy Principle drives neuromorphic development
Citation Key: fields2022free
Authors: Chris Fields, Karl Friston, James F. Glazebrook

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: We show how any system with morphological degrees of freedom and locally limited free
energywill, undertheconstraintsofthefreeenergyprinciple, evolvetowardaneuromorphic
morphology that supports hierarchical computations in which each “level” of the hierarchy
enacts a coarse-graining of its inputs, and dually a fine-graining of its outputs. Such hier-
archies occur throughout biology, from the architectures of intracellular signal transduction
pathways to the large-scale organization of perceptio...

Key Terms: principle, energy, neuromorphic, mathematics, university, quantum, development, department, eres, france

=== FULL PAPER TEXT ===

The Free Energy Principle drives
neuromorphic development
Chris Fieldsa,b∗ , Karl Fristonc, James F. Glazebrookd,e, Michael Levinb,f and
Antonino Marciano`g,h,i
a 23 Rue des Lavandi`eres, 11160 Caunes Minervois, FRANCE
b Allen Discovery Center at Tufts University, Medford, MA 02155 USA
c Wellcome Centre for Human Neuroimaging, University College London,
London, WC1N 3AR, UK
d Department of Mathematics and Computer Science,
Eastern Illinois University, Charleston, IL 61920 USA
e Adjunct Faculty, Department of Mathematics,
University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA
f Wyss Institute for Biologically Inspired Engineering at Harvard University,
Boston, MA 02115, USA
g Center for Field Theory and Particle Physics & Department of Physics
Fudan University, Shanghai, CHINA
h Laboratori Nazionali di Frascati INFN, Frascati (Rome), ITALY
i INFN sezione Roma “Tor Vergata”, I-00133 Rome, ITALY
July 21, 2022
Abstract
We show how any system with morphological degrees of freedom and locally limited free
energywill, undertheconstraintsofthefreeenergyprinciple, evolvetowardaneuromorphic
morphology that supports hierarchical computations in which each “level” of the hierarchy
enacts a coarse-graining of its inputs, and dually a fine-graining of its outputs. Such hier-
archies occur throughout biology, from the architectures of intracellular signal transduction
pathways to the large-scale organization of perception and action cycles in the mammalian
∗Correspondingauthorat: 23RuedesLavandi`eres,11160CaunesMinervois,FRANCE;E-mailaddress:
fieldsres@gmail.com
1
2202
luJ
02
]hp-tnauq[
1v43790.7022:viXra
brain. Formally, the close formal connections between cone-cocone diagrams (CCCD) as
models of quantum reference frames on the one hand, and between CCCDs and topological
quantum field theories on the other, allow the representation of such computations in the
fully-general quantum- computational framework of topological quantum neural networks.
Keywords
Bayesian active inference; Generative model; Quantum reference frame; Tomographic mea-
surement; Topological quantum neural network
Contents
1 Introduction 2
2 The FEP as a general physical principle 4
3 Defining the MB defines the environment 7
3.1 Informative versus uninformative sectors . . . . . . . . . . . . . . . . . . . . 7
3.2 Learning is learning a message-passing structure . . . . . . . . . . . . . . . . 8
3.3 Object identification by QRFs . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.4 The environment in practice: ANNs and neurons . . . . . . . . . . . . . . . 14
4 MBs with morphological degrees of freedom 16
5 Tomographic measurements minimize VFE 18
5.1 “Objects” as sectors in E . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5.2 Hierarchical measurements optimize the accuracy/complexity cost tradeoff . 18
5.3 Hierarchical QRFs as tomographic computers . . . . . . . . . . . . . . . . . 21
6 TQNNs as general neuromorphic systems 24
7 Conclusion 26
1 Introduction
The quest to understand how collections of cells form nervous systems that give rise to
cognitive capacities has driven research into computational systems using architectures ob-
served in neural tissues. The fundamentals of neuromorphic computing can be traced back
2
to the work of Mead [1], who pioneered the implementation of very large-scale integra-
tion (VLSI) methods. These kinds of functional (neuromimetic) architectures use analog
components that mimic neurobiological systems, and were conducive to solving real-world
problems with high efficiency and low cost. Hybrid analog-digital systems emulating spik-
ing neurons were also developed as an alternative to purely analog models [2]. Since then,
neuromorphic computers have evolved to further emulate the computational architectures
of neurons and of functional networks of neurons (for recent reviews, see [3, 4]). As living
systems, bothneuronsandnetworksofneuronsimplementcomputation, inpart, usingmor-
phology; differential delays between signals, for example, can be implemented by dendritic
or axonal processes of different lengths and widths. Changes in morphology also contribute
to the implementation of learning; for example, growing or regressing dendritic spines facil-
itates or inhibits synapse formation and hence location-specific interneural communication
[5, 6, 7, 8]. Spike-based and structural plasticity together implement memory-write cir-
cuits amenable to neuromorphic design [9] (and references therein). At the network scale,
activity-dependent pruning during neural development shapes both short- and long-range
cortical connectivity [10, 11, 12]. Hence from a biological perspective, a key feature of
neuromorphic computing is that it is dynamic: changes in morphology implement changes
in computation and vice-versa. This is exemplified in applications of hybrid analog/digital
VLSI devices implemented as neuromorphic vision sensors that model concept-learning in
relatively simple biological neural networks, such as described in [13]1.
Neuromorphic computing, foregrounds a separation of temporal scales implicit in natural
computation; namely, the distinction between fast inference and slow learning; sometimes
considered in the light of “dynamics on structure” [15]. However, on the neuromorphic
view, structure itself is dynamic, inheriting from fast inference (e.g., activity-dependent
plasticity) and scafolding inference (e.g., cortical hierarchies and other aspects of functional
brain architectures that rest upon synaptic connections) [16]. The use of morphology as
a computing resource is not, moreover, unique to neurons. It is an ancient biological
strategy, employed by plants, fungi, ameboid cells of diverse lineages, and even microbial
biofilms [17, 18, 19, 20, 21, 22, 23, 24]. All of these systems could, therefore, be considered
“neuromorphic” computers.
Here, we review and extend previous theoretical work suggesting that any system capable of
employing morphology as a computational resource will, if given a sufficiently informative
environment but locally-limited free energy, develop a “neuron-like” morphology. We show,
in particular, that this outcome can be expected on the basis of the Free Energy Principle
(FEP, [25, 26, 27]) applied to systems with morphological degrees of freedom but locally-
limitedfreeenergy. Locally-limitedfreeenergyrestrictslocalmeasurementstoafewdegrees
of freedom. Predictive power in this case is maximized if the environment is addressed
tomographically. Morphologicalplasticityisakeyenablerof–andwhensuitablyabstracted
a requirement for – tomographic measurements. As the FEP is completely scale-free, this
1As reported in [13], the common honeybee stands out as an exemplar having remarkable capabilities
for conceptualizing and categorizing (the bees having ≈ 106 neurons compared to ≈ 1011 neurons in the
human brain); in particular, their ability to distinguish between “odd” and “even” numeric quantities [14].
3
result applies at any spatiotemporal scale, and is indeed confirmed by systems from the µm
scaleofintracellularsignalingpathwaystotheplanetaryscaleoftheinternet. Wesuggeston
this basis that morphological plasticity, even if merely simulated, is an important resource
for neuromorphic computing.
In what follows, we first review, in §2, the basic ideas underlying the FEP, including the
definition of variational free energy (VFE) and its interpretation as Bayesian surprisal, and
thekeyconceptofaMarkovblanket(MB,[28,29])separatingatime-persistentsystemfrom
its environment. We show in §3 how defining the MB of a system defines its environment,
and consider the thermodynamics of the MB. We note a critical difference between current
artificial computing systems and organisms: the rigid segregation in the former between
free-energy exchange with the environment (via a power supply and heat exchangers) and
data exchange with the environment (via I/O interfaces or APIs). We then consider MBs
as measurement surfaces with locally-limited free energy resources in §4, and show how
morphological degrees of freedom enable varying the correlations between measurement
sites in nonuniform environments. This enables us to consider, in §5, how the FEP drives
morphologically-plastic systems – with locally-limited free energy resources – to measure
the environment’s state tomographically, using measurements made at different locations to
reconstruct a best predictive model of the state. We provide a fully-general physical model
of this process in §6, employing the formalism of topological quantum neural networks
(TQNNs, [30]). This shows that TQNNs provide general models of neuromorphic systems.
We conclude with implications, predictions, and next steps in §7.
2 The FEP as a general physical principle
Since its application to brain function [31, 32, 33, 25], the variational Free Energy Principle
(FEP) has been extended into an explanatory framework for living systems at all scales
[26, 34, 35, 36, 37]. When formulated as a general principle of classical physics, it charac-
terizes the behavior of all random dynamical systems that remain measurable, and hence
identifiable as distinct, persistent entities, over macroscopic times [27]. To summarize, it
is shown in [27] that any system that has a non-equilibrium steady state (NESS) solution
to its density dynamics i) possesses an internal dynamics that is conditionally independent
of the dynamics of its environment, and ii) will continuously “self-evidence” by returning
its state to (the vicinity of) its NESS. Condition i) can be thought of as a precondition for
any system to have a “state” that is clearly distinct from the state of its environment; it is
effectively the requirement that system and environment are weakly coupled. Given weak
couplingandlocalinteractions, thejointsystem–environmentstatespacecanbepartitioned
into internal (i.e., system), external (i.e., environment) and intermediary MB states. The
MB states can, in turn, be partitioned into sensory states that mediate the influence of
external states on internal states and active states that mediate the influence of internal
states on external states. In the language of perceptual psychology, the MB functions as
an “interface” [38] that encodes perceptions and actions. It is worth emphasizing that the
MB states are elements of the joint system-environment state space; while the MB states
4
are embedded in a physically-continuous spatial boundary in canonical examples such as
biological cells, this is not a requirement in general. With this partitioning, Condition ii)
then requires that the system behaves so as to preserve the functional integrity of its MB,
i.e. that its dynamics does not diverge following a perturbation. The FEP is the statement
that any measurable, i.e. bounded and macroscopically persistent, system will behave so
as to satisfy these requirements.
More formally, the FEP is a variational or least-action principle stating that a system
enclosed by an MB, and therefore having internal states µ(t) that are conditionally inde-
pendent of the states η(t) of its environment, will evolve in a way that tends to minimize a
variational free energy (VFE) that is an upper bound on (Bayesian) surprisal. This free en-
ergy is effectively the divergence between the variational density encoded by internal states
and the density over external states conditioned on the MB states. If π is a “particular”
state π = (b,µ), where b(t) is the state of the MB, the VFE F(π) can be written [27, Eq.
2.3],
F(π) = E [lnq (η)−lnp(η,b)]
q(η) µ
(cid:124) (cid:123)(cid:122) (cid:125)
Variationalfreeenergy
= E [−lnp(b|η)−lnp(η)] −E [−lnq ((cid:126)η)]
q q µ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Energyconstraint(likelihood&prior) Entropy
(1)
= D [q (η)|p(η)]−E [lnp(b|η)]
KL µ q
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Complexity Accuracy
= D [q (η)||p(η|b)] −lnp(b) ≥ −lnp(b)
KL µ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Divergence Logevidence
The VFE functional F(π) is an upper bound on surprisal (a.k.a. self-information) I(π) =
−logP(π) = −lnp(b) because the Kullback-Leibler divergence term (D ) is always non-
KL
negative. This KL divergence is between the density over external states η, given the MB
state b, and a variational density Q (η) over external states parameterized by the internal
µ
state µ. If we view the internal state µ as encoding a posterior over the external state
η, minimizing VFE is, effectively, minimizing a prediction error, under a generative model
supplied by the NESS density. In this treatment, the NESS density becomes a probabilistic
specificationoftherelationshipbetweenexternalorenvironmentalstatesandparticular(i.e.
“self”) states. We can interpret the internal and active states in terms of active inference,
i.e. a Bayesian mechanics [39], in which their expected flow can be read as perception
and action, respectively. In other words, active inference is a process of Bayesian belief
updating that incorporates active exploration of the environment. It is one way of reading
a generalized synchrony between two random dynamical systems that are coupled via a
Markov blanket.
We have recently reformulated the FEP within a scale-free, spacetime background-free
quantum information theory [40]. In this formulation, the MB is implemented by a de-
compositional boundary in the joint system-environment Hilbert space that functions as
5
a holographic screen, a topological generalization [41, 42] of the original geometric con-
struction [43, 44, 45]. The criterion of conditional independence is implemented by the
quantum-theoretic notion of joint-state separability, i.e. absence of entanglement across
the holographic screen. The action of the internal system dynamics implements a quantum
computation, which can be decomposed as a hierarchy of quantum reference frames (QRFs,
[46, 47]). Decomposition into QRFs has the advantage of assigning an explicit semantics,
interpretable as a system of units of measurement, to each “thread” of the computation.
Each QRF can, in turn, be given a functional specification as a category-theoretic struc-
ture, a “cone-cocone diagram” (CCCD) of Barwise-Seligman [48] classifiers. Such CCCDs
specify semantically-interpreted information flows within distributed systems (reviewed in
[49, 50]). In informational/logical terms a CCCD specifies “measurement” and “prepa-
ration” as dual memory read/write operations. We have employed this representation to
characterize neurons as hierarchical measurement devices [51] as discussed further in §4 be-
low. Didactically, this allows one to think of perception and action in terms of measurement
(read) and preparation (write) operators that stand in for answers (outputs) and questions
(inputs), about or from the environment as discussed below.
In either classical or quantum formulations, the FEP provides a generic theory of self-
organization for physical systems with sufficient dynamical stability to be identified over
time and subjected to multiple measurements, i.e. systems that can be considered “things”
that are distinct from their surrounding environments (see especially the discussion of this
point in [27]). The MB of any such “thing” underwrites its conditional independence
– between its internal states and the external states of its environment – by localizing
and thereby restricting information exchange between them. Hence the FEP provides
a generic characterization of physical interaction as information exchange, and a generic
characterization of internal system dynamics as (Bayesian) inference [27, 39, 40].
This reading of self organisation – sometimes referred to as self evidencing [52] – rests,
in a foundational way, on the notion of a generative model. Technically, this generative
model can be associated with the NESS density over the particular partition of systemic
states described above. This density can be factorized into a likelihood (the density over
particularstates, giventheircauses; i.e., externalstatesbeyondtheMB)andapriordensity
overparticularstatesthatarecharacteristicoftheparticleor‘thing’inquestion. Thestates
constitute the attracting set that underwrites the NESS solution to density dynamics. In
short, if there exists a MB – defined in terms of conditional dependencies under a NESS
density–thenthereisalawfuldescriptionofsystemicdynamicsthatcanbecastasgradient
flow on a free energy functional of a generative model. Teleologically, the generative model
specifies the states to which self-organization (i.e., evidencing) are attracted; namely, the
characteristic or preferred states of the “thing” in question. The role of a generative model
will be foregrounded in what follows; simply because the structure of a generative model
underwrites the dynamics and message-passing we associate with self organization.
6
3 Defining the MB defines the environment
3.1 Informative versus uninformative sectors
The partitioning of “everything” into “system” and “environment” (where in Eq. (1) the
MB is considered part of the system) built into the FEP formalism has the immediate
consequence that every system, by definition, interacts with exactly one other system, its
environment. The formalism is, moreover, completely symmetric: the system maintains a
well-defined, conditionally-independent state if and only if its environment does as well. We
can, indeed, think of system and environment as comprising a generative adversarial net-
work(GAN),witheachsideadapting, asitsresourcesallow, totheother’sactions[53]. This
symmetry is particularly manifest in the quantum formalism, which is a completely general
representation of two systems (i.e. components of a bipartite Hilbert-space decomposition)
open to interaction exclusively with each other.
This exclusive coupling of system to environment has two consequences, both of which have
been explored more explicitly within the quantum formalism [40]; see also [41, 54, 55] for
further discussion. First, all (thermodynamic) free energy acquired by the system from,
and all waste heat dissipated by the system to, its environment must traverse the MB.
The MB (or in the quantum formulation, the holographic screen B), is thus partitioned
into“informative”(or“observed”)and“uninformative”(or“unobserved”)sectorsasshown
in Fig. 1. The function of the uninformative sector is purely thermodynamic; formally, it
exchanges the free energy required to support irreversible classical computation [56, 57, 58].
7
Figure1: a)AsystemS isseparatedfromitsenvironmentE byaholographicscreenB that
implements an MB. Note that this depiction is purely topological; no geometry is assumed
for either the joint system SE or the boundary B. b) Both sensation (s) and action (a)
states on the screen B and divided into informative (i.e. data I/O) and uninformative
(i.e. thermodynamic I/O) sectors (clear versus hatched areas). Adapted from [53] Fig. 2,
CC-BY license.
The second consequence of the system–environment decomposition is that the system of
interest S has no access to the decompositional, or in quantum terms entanglement, struc-
ture of its environment E. Any ‘objects” detected by S in E are in fact sectors of mutually
correlated components of the state of the MB, or in the quantum formulation, sectors of
mutually correlated bits encoded on the screen B [40, 41, 54, 55]. The informative sector
of the MB can, therefore, be thought of as implementing an applications programming in-
terface (API) between S and E. Read and write operations to this API are implemented
by the internal dynamics of S (respectively, E). In the quantum formulation, these are
implemented by QRFs that effectively define the “data structures” encoded on each face of
B.
3.2 Learning is learning a message-passing structure
On the classical view of the Bayesian mechanics entailed by the FEP, the minimization
of VFE can be usefully considered at different timescales. For example, optimizing the
states or activities of a biological or artificial neural network (BNN or ANN) is distinct
from optimizing the connections or weights; which is distinct from optimising the structure
of the neural network per se. These three aspects of VFE minimization map neatly to
the distinction between inference, learning and model selection (a.k.a., structure learning),
respectively. We start with this observation because, to anticipate the discussion in §4
8
below, the structure just is the computational architecture in question and thereby spec-
ifies the nature of the message-passing entailed by inference and learning. Morphology
in three-dimensional (3d) physical space is an implementation resource for computational
architecture, as 3d layout in VLSI exemplifies.
On this view, the structure or morphology of any “thing” is subject to the same imperatives
as the message-passing; namely to maximize morphological or model evidence (or minimize
theassociatedVFEbound). Thiscanbecastasstructurelearninginradicalconstructivism
[59, 60, 61] or Bayesian model selection in statistics [62]. The implication here is that any
morphology must be a ‘good’ model of how its sensory states are caused by external states.
This is just an expression of the good regulator theorem from early formulations of self
organization in cybernetics [63, 64]. In other words, statistical correlations beyond the MB
mustbeinstalledinthegenerativemodel, intermsofsparsecoupling(i.e., message-passing)
among internal states (which themselves are “things” equipped with MBs). So what kind
of structures, architectures or morphology might one expect to find in things that are good
models of their external milieu?
If morphology maximizes model evidence, then the models implicit in any morphology
should comply with Occam’s principle – or Jaynes maximum entropy principle [65, 66] –
in virtue of having minimal complexity. This follows from the fact that log evidence (i.e.,
negative surprisal) is accuracy minus complexity. Equation (1) shows that complexity is the
degree of belief updating incurred by message-passing. Technically, complexity is the KL
divergence between posterior and prior, before and after belief updating. In short, a “good”
model is that which provides an accurate account but is as simple as possible. In turn, this
requires the right kind of “coarse graining” or compression [67], to provide an accurate
explanation for impressions on the sensory part of the holographic screen implemented
by the MB. So, what kind of coarse graining might emerge in a universe that features
probabilistic structure?
Thisexplanationcanonlybeintermsof“things”andtheirlawfulrelationshipsasdescribed
below. At this point, one can conjecture that things – and the (space-time) background
that describes their relationships in a parsimonious fashion – would feature in the structure
of generative models or morphology. This is evinced in a compelling way by neuroanatomy,
which speaks to a distinction between “what”, “where” and “when” in carving the senso-
rium at its joints. For example, one of the most celebrated aspects of brain connectivity
is the separation of dorsal and ventral streams that are thought to encode “where” and
“what” attributes of visual objects, respectively [68]. The argument here is that knowing
“what” something is does not tell you where it is and vice versa. This statistical indepen-
dence translates into a morphological separation between the dorsal and ventral streams.
This separation minimizes complexity and thereby maximizes the efficiency of (variational)
measure-message passing and belief updating in terms of statistical, algorithmic and ther-
modynamic complexity costs [69, 34]. Similar arguments can be made for a separation of
“what” and “when” [71]; in the sense that knowing “what” something is does not tell you
“when” it was “there”.
This kind of coarse graining (c.f., carving nature at its joints) is ubiquitous in statistics and
9
physics, where it emerges in the guise of mean field approximations; namely, factorizing
a probability density into conditionally independent factors [16, 72, 73, 74, 75]. Indeed,
variational free energy and message passing are defined under a mean field approximation
to a posterior density [69, 76]. Another important structural or morphological feature of
‘good’ generative models is their deep or hierarchical structure, with an implicit separation
of scales in the genesis of – or explanation for – sensory impressions.
The common theme here is a morphology underwritten by the sparsity or absence of
message-passing on some factor graph. This foregrounds the imperatives for shielding or se-
questering various internal states from other internal states, which brings us back to MBs;
however, these are internal MBs that define an internal morphology or message-passing
structure. One might conjecture that much of biological self-organization is concerned with
isolation and shielding, as a necessary part of internal autopoiesis (e.g., the role of enzymes
and catalysts, gap junctions, and many other highly controllable mechanisms for setting up
signaling paths and boundaries [77, 78, 79, 80]. This occurs at all scales, from subcellular
organelles that partition biophysical and chemical reactions to nascent organ compartment
boundaries, to the dynamics that guide which members of a swarm pass messages to which
others [81, 82, 83, 84, 85].
In this respect, morphogenetic self-organization, seen as a pattern formation, requires each
individual cell (and/or its progeny) to occupy its own place in the final morphology, and
autopoietic self-assembly results only when each cell successfully detects local patterning
signals as predicted by its own generative model [37, 86]. Morphological development thus
implies a pre-determined patterning to which a cell ensemble converges – the so-called Tar-
get Morphology [87, 88, 87]. Note that this is an essentially classical statement; it assumes
the existence of effectively-classical boundaries and hence distinctions between cells. If each
cell minimizes VFE then it infers its correct location and its function within the ensemble
[37, 86, 90]. Examples in the case of neurons include assortative neuronal migration to-
wards groups with very close or identical node degrees [91, 92] and amalgamation of groups
with a common stimulus, following which they ‘cast a vote’ to decide on how to proceed
collectively [93]. More generally, the Good Regulator Theorem again applies when each
cell, by evidencing its own existence, can vouch inferentially for the same model as the one
of the local group in which it is accommodated. In this way, the cell contributes to the
eventual release of effective signaling by the ensemble to other formations. This is a basis
for a theoretical framework of autopoiesis expressed in terms of VFE minimization, and
hence active inference [86].
A final consideration – afforded by the classical FEP – is that the same Bayesian mechanics
mustapplyinascale-freefashion[27,90,94,95]. Inotherwords,MarkovblanketsofMarkov
blankets (i.e., things composed of things) must evince the same kind of message-passing.
Forexample, theintracellularcomponentsofasinglecellmusthavetherightmorphologyto
maintain the cell’s MB (e.g., a cell surface). Similarly, the ensemble of cells that constitute
a multicellular structure must be so structured to maintain the MB of the tissue or organ
in question (e.g., a somatic cell on its endothelial surface) [96, 97]. In a similar vein, this
impliesthatthemessage-passingbetweenMarkovblankets(e.g., cellsandorgansthroughto
10
conspecifics and cultures) must (look from the outside as if they) comply with the same free
energy minimizing imperatives. This translates into efficient communication at the level of
intracellular communication, through to languages with minimal algorithmic complexity.
In short, message-passing between ‘things’ should incur the minimum amount of belief
updating, while communicating as accurately as possible. In what follows, we will see these
themes re-emerge, both in terms of biological intelligence and quantum information theory.
The semi-classical limit of a TQNN model, in particular, constructs generalizations with
the shortest possible trajectories and the maximum topological information as discussed in
§6.
3.3 Object identification by QRFs
From the perspective of an observer S, “things” are located in the environment E. As
is obvious from the definition of an MB, however, S cannot “see” E; S can only detect
encodingsonitsMBB. A“thing”forS is,therefore,aclusterofbitsonB withhighmutual
information and hence high joint predictability. Recognizing a “thing” – determining that
some bits have high mutual information – requires multiple measurements. In particular,
any “thing” X can be considered to have two components, a “reference” component R that
maintains a constant (up to measurement resolution and relevant coarse-graining) state (or
state density or expectation value), and a “pointer” component P with a time-varying state
that S considers “the state of interest of X [40, 41, 54, 55]. Ordinary items of laboratory
apparatusprovideacanonicalexample,asshowninFig. 2; onecanonlyidentifyavoltmeter
or an oscilloscope if most of their state variables – size, shape, brand name, etc. – remain
fixed while the “pointer” variables vary to indicate some measured value [98].
11
Figure 2: Identifying a laboratory apparatus S requires identifying some proper component
R that maintains a constant state |R(cid:105) (or density of time-averaged samples ρ or expecta-
R
tion value < ρ >) as the “pointer” state |P(cid:105) (or density of time-averaged samples ρ ) of
R P
interest varies. Adapted from [54] Fig. 2, CC-BY license.
Measurements of the states of R and P can, without loss of generality, be regarded as
implemented by QRFs [40, 41, 54, 55]. A QRF is simply a physical system with which a
measurementisenacted; suchasystemisaquantumreferenceframebecause,beingphysical,
it must atsome suitablescale beregarded asa quantumsystem, andat thatscale itencodes
unmeasurable, and hence unencodable or “nonfungible” [47] quantum phase information.
Such systems are intrinsically semantic: they report not just values, but also units of
measurement that render such values mutually comparable. Even a non-standardized QRF
such as the length of one’s arm defines a unit of measurement, although an idiosyncratic
one. Hence repeated observations, which must determine at minimum the state of R and
are therefore measurements, are intrinsically semantic: they are actions on the world that
yield mutually-comparable, and hence actionable observational outcomes.
12
In a quantum theoretic formulation, measurement and its dual, state preparation, have the
same formal representation; a “preparation” process is just a measurement reversed in time.
A QRF is, therefore, a preparation device as well as a measurement device: one can prepare
a0.75mboardwithameterstick,justasonecanmeasurea0.75mboard. Preparationisan
action on the environment; preparation and measurement together constitute interaction.
Indeed any physical interaction can be considered a sequence of alternating preparation
and measurement steps, as shown in detail in [40]. This duality is preserved in the classical
formulation, but remains implicit (i.e., perception as time-reversed action and vice versa).
As we have pointed out, the dual character of preparation and measurement as enacted
by QRFs allows their representation, in full generality, by category-theoretic structures;
namely, the CCCDs of classifiers in [48], as constructed in [49, 50]. This representation has
been extensively applied in computer science as reviewed in [49]; we prove its generality
in the present setting in [50], to which we refer for formal definitions and details. Such
structures have the form:
A (cid:104)(cid:104)
(cid:111)(cid:111) g21 (cid:47)(cid:47)
A
(cid:111)(cid:111) g32 (cid:47)(cid:47)
... A
1 g12 (cid:79)(cid:79)2 g23 (cid:53)(cid:53) k
h2
h1 h
k
(cid:54)(cid:54)C(cid:79)(cid:79) (cid:48) (cid:105)(cid:105) (2)
f1 f
k
f2
A
(cid:111)(cid:111) g21 (cid:47)(cid:47)
A
(cid:111)(cid:111) g32 (cid:47)(cid:47)
... A
1 g12 2 g23 k
where the A are Barwise-Seligman classifiers and C(cid:48), also a classifier, is the category-
i
theoretic limit of the outgoing maps h and the colimit of the incoming maps f . The
i i
diagram shown in Eq. (2) is required to commute, i.e. all directed sequences of maps from
anynodetoanyothernodeareequivalent. Theconstructiondevelopedin[50]furtherplaces
these diagrams within the context of general graph (e.g. ANN) networks; in particular, the
form of Diagram (2) clearly suggests a variational auto-encoder (VAE).
Structures of the form of Diagram (2), provided that they all mutually commute, can be
assembled into hierarchies of the form:
C
ψ ψ ... ψ
1 2 m
C(cid:48) C(cid:48) C(cid:48)
1 2 m
f f f f f f ... f f f
11 21 m1 12 22 m2 1m 2m mm
A A ... A A A ... A A A ... A
11 21 m1 12 22 m2 1m 2m mm
(3)
13
where here we have suppressed the outgoing arrows for ease of illustration. Such diagrams
represent simultaneous actions by multiple QRFs, or alternatively, the construction of a
functionally more complex QRF from simpler QRFs. Failure of commutativity prevents
such assembly, and can be interpreted as indicating quantum (or “true”) contextuality; we
do not pursue this here, but refer to [40, 99] for extensive discussion.
Diagram (3) clearly resembles a dendritic tree. It is this generic functional form that allows
the representation of neurons as hierarchies of QRFs [51]. We will show in what follows that
“neuromorphic” structures of this form follow as a consequence of the FEP whenever two
conditions are met: the existence of morphological degrees of freedom and the constraint
of locally-limited (thermodynamic) free energy. Before proceeding to show this, we briefly
consider two examples.
3.4 The environment in practice: ANNs and neurons
The idea that any system interacts with “its environment” as a whole follows immediately
from the concept of an MB or a holographic screen, which renders the environment a “black
box” of indeterminate internal structure [100, 101]. This is counter-intuitive, as we tend
to regard our own interactions as interactions with specific, identified objects. In fact, our
interactions are with, or more properly via, QRF-identified sectors of our MBs as described
above. To see this in a simpler case, it is useful to consider the “environments” of two
relevant systems, a node in a conventional ANN and a biological neuron.
It is commonplace to think of an ANN as interacting with training and test sets of, for
example, images that have a spatial (here 2d) structure. This, however, is an anthropo-
morphism; the ANN in fact interacts with sets of finitely-encoded and therefore rational
numbers. We can consider a node in a layered, feedforward ANN to have the following
structure:
{∆ }
i
...
x
1
x
2
· · (4)
· · o
· ·
x
n−1
x
n
where here {x } is the set of input values from upstream nodes, {∆ } is the set of training
i i
(backpropagated error) values, and the rational number o is the output. The “sensed
environment” of this node, i.e. the sensed state s of its MB, is the ordered pair ({x },{∆ });
i i
the “acted-upon environment” of the node, i.e. the action state a of its MB, is the rational
number o.
14
Note that drawing the node as Diagram (4) imposes on it a “morphological” degree of
freedom, namely its layout on the 2d Euclidean surface of the page. This, in turn, imposes
orders onto the sets {x } and {∆ }, making them vectors with the obvious metric. This
i i
morphologicaldegreeoffreedomisnot, however, intrinsictothenode; itappearsnowherein
amathematicalspecificationofthefunctionthatthenodecomputes,nordoesitcharacterize
the (completely abstract) sets {x } or {∆ } or the number o. This absence of morphology
i i
is, more than absence of hierarchical structure or spiking (which neurons can lack), what
renders a node in an ANN non-neuromorphic. Nodes in ANNs are non-neuromorphic
because they are amorphic; they have no morphology. The same clearly applies to the
CCCDs depicted in Diagrams (2) and (3), which as formal specifications of computations
are abstract descriptions of semantically- interpreted information flow.
Implementing an ANN as hardware gives it a morphology: the 3d morphology of the hard-
ware. It also confers a resource requirement for thermodynamic free energy; hence it adds
a thermodynamic sector to its MB. This exposure to energetic exchange with the envi-
ronment renders the implemented ANN a “thing” in the language of the FEP. How the
implemented ANN behaves, i.e. how it regulates its energetic exchange with its environ-
ment, determines whether it will persist over time. This regulation of energy exchange in
service of persistence, or survival, is the core meaning of embodiment. The ever-present
possibility of dysregulation is what renders embodiment “precarious” [102].
Let us now consider a neuron, which is by definition embodied and therefore has a mor-
phology. A neuron’s sensed environment (as represented by the MB state s) is, like the
sensed environment of any other system, defined by the sensory structures that it de-
ploys. In the case of a neuron, these are mostly post-synaptic specializations, including
clusters of post-synaptic receptors and channels as depicted in [51], Fig. 4a, b; we will
focus on these at the expense of more uniformly distributed biochemical and bioelectric
sensors. The neuron’s sensed environment is then the set {s } of activations detected by
i
these post-synaptic specializations. The neuron’s acted-upon environment is, similarly, the
set {a } of activations generated by its pre-synaptic specializations, again ignoring more
i
uniformly-distributed pumps, secretory systems, etc. These sets {s } and {a } comprise
i i
the neuron’s MB. Perception and action are linked together by the dynamics on the in-
ternal states, which are supported by all internal degrees of freedom of the cell, including
genome, mitochondria and other organelles, cytoskeletal network, etc.; these internal dy-
namics implement the cell’s generative model. Hence while it is commonplace to think of a
neuron as “detecting pressure” or “exciting a muscle” these descriptions are possible only
from a larger, tissue-scale perspective. From the neuron’s own perspective, it is acting to
regulate the bioelectrochemical gradients it detects as state variations of {s }. See [103] for
i
a worked example of dendritic self organization, in terms of structure learning, using the
minimization of VFE to implement model selection in terms of dendritic spines. In this
example, the morphology of the dendritic tree aligns itself with the temporal sequence of
presynaptic inputs that itself depends upon morphology of the neuropil [104]. However, at
no point does the (synthetic) neuron “know” its morphology.
Both inputs to and outputs from a neuron are organized spatially by its morphology. How-
15
ever, the neuron itself cannot detect or represent its morphology, though local changes in
morphology are locally detectable, e.g. by differential strain on the cytoskeleton. Hence
the neuron’s inputs and outputs remain, for the neuron itself, only sets without structure.
The overall function of the neuron, and hence the values of its outputs, depend however
on its computational (i.e. message-passing) architecture and hence on its morphology. It
is the role of morphology in determining function – and hence action on the environment –
that the FEP explains [51].
4 MBs with morphological degrees of freedom
ThestateboftheMB,orofthescreenB, andinparticularthestate(s,a)ofitsinformative
sector, has thus far been considered a state in some arbitrary (e.g. Hilbert) state space.
In particular, no positional (e.g. ordinary Euclidean 3d spatial) degrees of freedom have
been assumed. We now add to the MB states, as a parameter, an ancillary “morphological”
degree of freedom ξ that, as we will see, in naturally interpretable as a spatial degree of
freedom. This ancillary degree of freedom is ancillary in the sense of having no effect on
the total system–environment information exchange across the boundary B; in the purely-
topological notation of Fig. 1, the interaction H does not depend on ξ. As we show
SE
below, however, S’s QRFs partition B into sectors that are “localized” in the space defined
by ξ. Hence ξ usefully parameterizes H in a way that a neuron can take advantage of
SE
by varying its morphology to selectively deploy its QRFs to specific sectors of B. We can,
therefore, regard H as depending locally on ξ; this will be made explicit in §6 below when
SE
we assign spatial coordinates to the input states of a TQNN.
We further assume that the states |ξ(cid:105) of ξ (here adopting the Dirac notation for states)
are vectors and hence provide a distance measure (cid:104)ξ|ξ(cid:48)(cid:105). This effectively “geometrizes” the
states b by assigning to each a “location” ξ and allowing “distances’ between states to be
calculated. In this way ξ plays the role of the 2d geometry of the page in Diagram (4);
it allows the states b to be placed in an ordered array with the dimension of ξ. From a
physical perspective, the simplest geometrization of B represents the space of MB states b
as a 2d array of qubits (it hence considers a minimal binary encoding of the states b and
implements each bit with a quantum bit, e.g. a spin degree of freedom), and positions each
qubit in a voxel of volume 2∆x x 2∆x x 2c∆t as shown in Fig. 3, where ∆x is the minimal
“grain size” of space, ∆t is the minimal time to encode one bit, and c is the minimal
speed of “causal” classical information transfer. If ∆x and ∆t are the Planck length and
time, respectively, this reproduces the idea of a “stretched horizon” subject to the original,
geometric holographic principle, which encodes information at the maximum density given
by the Bekenstein bound [43, 44, 45]. For biological systems at temperature T ∼ 310K,
∆t ∼ 50 fs and ∆x ∼ 1 ˚A, and c is the speed of bond-vibration waves in macromolecules
[105], a scale roughly 25 orders of magnitude larger than the minimum set by quantum
theory.
16
Figure 3: One qubit degree of freedom (represented as a Bloch sphere), e.g. a spin, embed-
ded in a 3d voxel at some minimal scale ∆x, ∆t. Here c is the minimum speed of (classical)
information transfer.
In order to model neurons, we will assume that ξ has an “embedding” dimension in addition
to the “2+1” space + time structure shown in Fig. 3. As we will see in §5.3 below, the
interpretation of this extra dimension depends on the QRFs available to measure it.
Our two principal assumptions can now be stated:
1. The state (s,a) of the informative sector of the MB/screen B is non-uniform in ξ.
Parameterizing H with ξ therefore reveals local structure in H .
AB AB
2. The free energy available via the uninformative sector of the MB/screen B is suffi-
ciently limited that only a “few” cycles of classical computation can be performed on
each bit in the informative sector.
We will, for simplicity, also assume that the state of the uninformative sector has two
components, each with a uniform state. This allows us to treat thermodynamic exchange
with the environment as an interaction with “hot” and “cold” heat baths that supply free
energy and exhaust waste heat, respectively.
Qualitatively, Assumption 1 assures that the informative sector of E is potentially inter-
esting to S, while Assumption 2 limits S’s ability to “make sense” of E by performing
predictive computations. These assumptions thus keep S in a regime of finite VFE, avoid-
ing the “prefect prediction” limit of VFE → 0 that, as we show in [40], corresponds to loss
of separability (i.e. to an approach to quantum entanglement) between S and E.
On a classical view, these assumptions express the FEP in terms of maximizing accuracy
(Assumption 1), while minimizing the complexity cost of belief updating (Assumption 2).
17
In machine learning, a failure to minimize complexity leads to overfitting [106] that can be
read as the perfect prediction limit (c.f., quantum entanglement).
We can now develop our main result, showing in §5 below that given limited free energy,
the FEP imposes a hierarchy on the structure of computation over the informative state
(s,a) that, when parameterized by the morphological degree of freedom ξ, becomes a to-
mographical computation defined over effectively “spatial” measurements and producing
effectively “spatial” actions. This computation represents the informative sector of E as
˜
comprising “objects” that interact “causally” against a spatially-extended background E of
non-objects. We then show in §6 that this action of the FEP can be captured in full gener-
ality in the formalism of TQNNs, making explicit that “space” is emergent from connection
topology. In this formalism, the role of the spatial embedding (i.e. of ξ) is to enforce a
coarse-graining in which the “objects” detected by S are separable and hence statistically
conditionally independent. This is exactly the role of “space” in quantum field theories
[42]. It allows the mean-field assumption that allows us, as discussed above (§3.2) in the
classical setting, to talk about objects as persistent entities with their own MBs.
5 Tomographic measurements minimize VFE
5.1 “Objects” as sectors in E
Wehavepreviouslydemonstratedtheconverseofourdesiredresult: thatifthebitsencoded
on a sector X = RP of B have sufficient mutual information to satisfy the logical criteria
(e.g. as encoded by a CCCD) implemented by some QRF X = RP over some macroscopic
time interval τ – and in partcular, if the measured state density ρ of R remains fixed
R
throughout τ – then X will appear to the observer S to be an “object” or “persistent thing”
during τ. In particular, the state |RP(cid:105) (or density ρ ) will appear to be decoherent from
RP
(i.e. not entangled with and hence conditionally independent from) the remainder of B
[40,41,54,55]. Decoherencecorresponds,classically,toconditionalindependence[107,108].
It is what makes “thingness” and behavioral predictability possible.
Assumption 1 above states that E contains potentially-detectable objects; stated more
carefully, Assumption 1 states that B includes sectors that encode bits with significant
mutual information. These sectors present S with opportunities for predictability, i.e., for
local (on B) reduction of VFE. Our question is then: what computational (i.e. message-
passing) structure can take advantage of “islands” of predictability to minimize VFE while
remaining within the free-energy constraint imposed by Assumption 2?
5.2 Hierarchical measurements optimize the accuracy/complexity
cost tradeoff
While pure quantum (i.e. unitary) computation costs no free energy, Landauer’s Prin-
ciple imposes a cost of βk T, with β > ln2 and k and T Boltzmann’s constant and
B B
18
temperature, respectively, on classical bit erasure and hence on classical memory updating
[56, 57, 58]. Assumption 2, therefore, effectively limits the writing of classical memories.
Recording previous measurement outcomes for comparison with future ones is, therefore,
the energetically-limited step in computing predictions. Markov kernels with rational ma-
trix elements provide, for a given (finite) measurement resolution, the most efficient repre-
sentation of prior measurement outcomes and hence of prior probability distributions [40].
Hence the fundamental energetic tradeoff faced by any VFE-minimizing system – that is,
any system compliant with the FEP – is a tradeoff between the resolution with which both
prior and posterior probabilities are encoded and the predictive power that they provide2.
The optimal solution to the above tradeoff is, obviously, to only encode probabilities that
actually contribute to predictive power. Hence we can expect the FEP to drive systems
toward identifying and processing input data from only those sectors of their MBs that
encode bits with high mutual information, i.e. high redundancy or high error-correction
capacity; we will see this also for TQNNs below. Biologically, this corresponds to the
(phylogenetic) evolution or (ontogenetic) development of systems with sensory structures
and processing pathways specialized to the detectable affordances of their ecological niches
[109, 110]. Indeed, many authors have cast natural selection as, implicitly or explicitly, a
VFE minimizing process in terms of natural Bayesian model selection or structure learning
[111, 112, 113, 114, 115, 116].
SectorsofhighmutualinformationinduceaconnectiontopologyonB, withthesesectorsas
the open sets. This topological structure breaks the exchange symmetry of bit “positions”
(i.e., values of ξ) on B. This symmetry breaking corresponds to a choice of basis for the
Hamiltonian H ; again see [40, 41, 54, 55] for details. Under the FEP, the local action of
SE
the internal dynamics H on each such sector implements a QRF that alternately measures
S
and acts on the bits encoded by that sector. The action of this QRF can, without loss of
generality [50], be specified by a CCCD as shown in Fig. 4.
2Basically, for any sector X defined by a QRF X, a generic (k-)time-stamped quantum system A
confronts the task of minimizing prediction error Er (k) given by
X
Er (k)=d(MA(k),M (k))
X X X
where MA(k) and M (k) denote Markov kernels derived from observables, and d is the metric distance
X X
between kernels. The FEP in this case asserts that a generic quantum system will act so as to minimize
Er for each deployable QRF X (for details, see [40, §3, §4]).
X
19
Figure 4: a) A CCCD associated with a sector of the boundary B, depicted as an array of
qubits as in Fig. 3. The operators MS and ME are Hermitian components, in the bases
i j
chosen by S and E respectively, of the interaction Hamiltonian H ; see [40, 55] for details.
SE
Adapted from [55] Fig. 3, CC-BY license. b) Cartoon representation of k QRFs s ,...s
1 k
(red triangles, with the apex the limit/colimit of the corresponding CCCD) acting on high
mutual-information sectors of B.
A limit/colimit and infomorphisms from/to it exist over any mutually-commuting subset
of the CCCDs specifying actions of QRFs on B [105, Thm. 7.1]. Hence any mutually-
commuting subset of the CCCDs can be hierarchically composed as components of a larger
CCCD that processes their combined outputs, as shown in Diagram (3). This larger CCCD
specifies the action of a QRF that can be thought of as alternately measuring and prepar-
ing the states of the component QRFs in the hierarchy. Indeed, this larger QRF induces
a single topological quantum field theory (TQFT, [117]) on the collection of sectors mea-
sured/prepared by the component QRFs [50]; we will pursue the consequences of this in §6
below.
Whenever any of the component CCCDs specify (the component QRFs implement) non-
linear processes, e.g., logical AND or XOR, hierarchical decomposition implements coarse-
graining. The FEP will drive any system toward such coarse-graining provided the loss of
20
predictive power at the component level is compensated for by a gain of predictive power
at the higher level. This will be the case whenever the sectors spanned by the combined
CCCD/QRF encode significant mutual information about each other, i.e., in any situation
in which there are information relations between sectors and hence apparent “interactions”
between the “objects” that the sectors represent to S. The FEP, in other words, will
drive any system to discover “macro variables” that characterize and “emergent causality”
[118, 119] between its identified sectors.
Teleologically speaking, while scientists have only recently developed reliable tools with
which to quantitatively track the causal power of diferent levels of a system [120, 121,
122, 123, 124, 125, 126, 127], biological life forms emerging under realistic temporal and
energy (metabolic) constraints have always faced selection pressure to estimate causality of
meso-scale “objects” in their Umwelten. It is essential for survival that an agent spends its
precious resources attempting to affect or communicate with (or track) the features of its
environment that make a diference, which requires them to coarse-grain their experience
into models in which the massive stream of sensory information and potential activities (at
all scales) is cut up into convenient “objects that do things” for the purposes of efficient
action. Livingbeingscannotafforda“Laplace’sdaemon”(micro-reductionist)viewofcause
and efect, and the pressure to form models that acknowledge causal potency of higher levels
is baked in from the very beginning of the evolution of life.
Biological spiking (e.g., mammalian cortical) neurons are canonical examples of such hierar-
chical, coarse-graining measurement/preparation systems, with dendritic trees implement-
ing hierarchical measurement and axonal branches implementing hierarchical preparation,
namely, action on the surrounding environment [51]. Convolution of post-synaptic poten-
tials at dendritic branch points implement nonlinearities including logical AND and XOR
[128]. Gating of action potentials implements similar nonlinearities. These functions are,
indeed, precisely the features that most distinguish neurons from simplified models such as
Diagram (4), and precisely the features that most neuromorphic computing models seek to
replicate or at least emulate [3, 4].
5.3 Hierarchical QRFs as tomographic computers
As discussed in §3.2 above, the relationship between computational structure and morphol-
ogy examplified by neurons should characterize any system subject to the FEP. We are now
in a position to see why. When the morphological degree of freedom ξ is given the structure
of a vector space, it provides a distance measure (cid:104)ξ|ξ(cid:48)(cid:105) as discussed in §4 above; however,
we have given no interpretation of this distance. Hierarchical decomposition provides such
an interpretation: if sectors of high mutual information are regarded as “locations” on B –
and hence open sets in the connection topology are regarded as simply-connected “areas”
in the induced geometry – then mutual information between sectors provides a natural
distance measure. A hierarchy of QRFs, in this case, induces a hierarchy of distances on
B that are encoded by the values of the morphological degree of freedom ξ.
21
Note that the distance measure (cid:104)ξ|ξ(cid:48)(cid:105) is a formal description of B applicable to any inter-
action H in which at least one of the interacting systems, which we by convention label
SE
S, has an internal dynamics H of sufficient complexity to implement a QRF hierarchy.
S
This does not imply that S itself is capable of measuring the distance (cid:104)ξ|ξ(cid:48)(cid:105). Indeed we
have explicitly assumed that H does not depend on ξ. The system S will be capable of
SE
perceiving “space” and thus of measuring distance only if it implements a QRF for spatial
measurements. While vertebrates, cephalopods, and some arthropods appear capable of
perceiving space, this is not necessary for acting in space (as perceived by us), and the vast
majority of organisms, including perhaps all unicells, may lack spatial QRFs altogether
[96].
Following the reasoning deployed in §3.4, let us consider what a system S implementing
hierarchies of QRFs, but having no spatial QRFs perceives. The bits encoded by a high
mutual-information sector s of the informative sector (s,a) of B are written by the action
i
of E on that sector. We can think of them, therefore, as outcomes of measurements of basis
vectors of the effective Hilbert space H of the MB B; this is depicted in Fig. 4, with the
B
MS as single-qubit (e.g., z-spin s ) measurement operators. Hence, we can view each of
i z
S’s QRFs as measuring a projection or “slice” of H as shown in Fig. 5.
B
Figure 5: Cartoon showing S’s QRFs s ,s ,...s measuring “slices” of the effective Hilbert
1 2 k
space H ; each slice corresponds to a sector of B.
B
Measurements that (partially) reconstruct the state of some system by measuring indepen-
dent projections of that system are tomographic measurements; the (typically hierarchical)
process of reconstructing the total state is tomographic reconstruction. Hence QRFs acting
22
on sectors are implementing tomographic measurements of the state of B, and hierarchies
of QRFs are computing a tomographic reconstruction of the state of B.
By analogy with tomographic reconstruction from image planes as implemented by PET or
fMRI brain-imaging systems, we can think of these slices as having two spatial dimensions
as depicted in Fig. 5. The “depth” dimension (horizontal in Fig. 5) is, in this case,
notionally perpendicular to the 2d “surface” of B. This depth dimension can, therefore,
be identified with the embedding dimension of ξ assumed in §4 above.
The notion of “depth” and hence the embedding dimension of ξ has, in addition, a second
interpretation in terms of time; it can be identified with the total time required to process
the inputs from a particular sector through the multiple layers of a hierarchical QRF.
This means we can think of the embedded morphology of S in two complementary ways:
the morphology both “extends into” the state space being measured (i.e., into H ) and
B
“wraps around” the hierarchical computational structure, conferring a spatial (or space-
like) structure on computational (or message-passing) time. This dual aspect of embedding
isevidentinneurons, whichbothextendintotheir(3d)environmentsandrequiremoretime
to process distal inputs than proximal ones. In consequence, distal signals are degraded in
timeresolutionandlowerinamplitude, renderingtimeresolution(relativetosomeproximal
standard) and amplitude (relative to some proximal standard) alternative interpretations
of the embedding dimension. As discussed in [51], neurons also perform state tomography
in time, measuring multiple temporal replicates of input activity patterns to reconstruct
the relatively slowly-varying state of the (individual neuron’s) environment as a whole.
Perhaps the most celebrated examples of spatiotemporal encoding in the brain are the char-
acteristic responses of the hippocampus; variously read in terms of encoding space—with
place and (hierarchically superordinate) grid cells—or, tellingly, as having a key role in
memory and the encoding of time [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
140, 141, 142, 143, 144, 145]. From the current perspective, the very existence of place
cells—and perhaps receptive fields more generally—speak exactly to the course graining of
the brain’s implicit explanations for its sensorium. And, crucially, how neuronal computa-
tions leverage, or are scafolded by, the morphology of neurons and the connectomes which
they are embedded. This is manifest at many levels empirically; ranging from the emer-
gence of late waveforms in event-related potential research attributed to deep hierarchical
processing [146]; through to temporal gradients as we pass from the back (visual) brain
to the front ascending both hierarchal depth and temporal scales [147, 148, 149, 150] and
indeed the functional anatomy of the hippocampus [151]. The d´enouement of this analy-
sis suggests that space and time are perceptual constructs that supervene on the ordinal
structure and scheduling of message-passing on neuronal architectures with a certain mor-
phology – a morphology that is apt to explain sensory exchange with the environment, as
it is actively queried through (what we perceive as) navigation [152, 153].
What we have said so far here suggests a rather consequential hypothesis: that the transla-
tional, rotational, and boost symmetries of spacetime – hence the Poincar´e group and Spe-
cial Relativity – are consequences, for any observer S, of the independence of the Hilbert
space of B from the ancillary coordinate ξ. Observing these symmetries requires identi-
23
fying, via specific QRFs, objects as sectors on B. Symmetries of motion are, therefore,
consequences of the conservation by S of object identity through time, something that fol-
lows immediately from defining, for each object, a reference component R with fixed ρ .
R
Hence they are, effectively, consequences of the structure of H as a quantum computation.
S
We will pursue this remarkable hypothesis elsewhere; we turn now to the construction of
TQNNs as models of neuromorphic computation.
6 TQNNs as general neuromorphic systems
We show in [50] how any sequence of measurements by some fixed QRF on some fixed
sector(s) of a boundary B induces a TQFT on that sector (or those sectors). A TQFT on
a boundary B can be thought of as encoding all possible smooth transformations (e.g. all
possible Feynman paths) from some initial configuration B of B to some final configura-
in
tion B . With a suitable choice of basis, such TQFTs can be implemented by TQNNs
out
[30, 50]. These generalize conventional quantum ANNs [154, 155] by allowing the number
and organization of “layers” to be indeterminate. We start by clarifying this peculiar, novel
quantum feature of TQNNs, which stands at the forefront of the implementation of TQFTs
in machine learning.
TQNNs implement computations on quantum states of the Hilbert space associated to the
boundarysectorsB, andcanbeexpandedonthespin-networkbasis[30,50]. Spin-networks
are in turn supported on 1-complexes (graphs or loops) embedded on the boundary sectors
B, and are colored by certain irreducible representations (irreps) of whatever symmetry
describes the system. Note that this embedding requires B to have, or be extended to have,
spatial (i.e., ξ) degrees of freedom. The relevant symmetry is then either individuated by
someLiegrouporbysomequantumgroup,namelyanon-trivialHopf-algebra[156]. TQNNs
arethenrepresentedassuperpositionsofthebasiselementsontheboundarysectorB. Spin-
networks provide an orthonormal basis, but is worth reminding that loop states as well span
the Hilbert space of quantum states over B, and that a unitary transform exists [157] that
connects the two classes of states. The dynamical evolution of the TQNN is then described
byTQFTtransitionamplitudesbetweenB andB , whicharesupportedon2-complexes
in out
interpolating the TQNN’s 1-complexes [30, 50]. The role of symmetry, as customary in any
(effective) quantum field theory, is crucial in recovering the dynamics, as it dictates the
interaction among different TQNNs/quantum states embedded on the boundary sectors
B and B . The superposition principle induces a summation over an infinite set of
in out
interpolating 2-complexes, supporting virtually fluctuating TQNNs, namely the sum over
allthepossibleevolutionsofcoloredquantumstates. Thesuperpositionprinciplealsoforces
to sum over all the possible colours, i.e., all possible assignments of irreps of the symmetry
groupappropriatetodescribeacertainphysicaldataset. Topologicalchangesinthegraph’s
connectivity can be induced, starting from a maximal graph, by solely considering the sum
over the irreps. This is due to the fact that assigning a vanishing irrep to a link of a graph
corresponds to removing that link from the graph. Then all the interpolating topologies
24
can be obtained summing over all the compatible irreps and intertwiner quantum numbers
assigned respectively to the graph nodes and links.
The minimal number of spatial (i.e., ξ) dimensions required to embed 1-complex (graph
and/or loop) degrees of freedom in a boundary manifold B is 2, but a 2d spatial embedding
does not allow linking and knotting (i.e., entangling) of these states to take place, which
instead requires at least 3 spatial dimensions. A larger number of Hausdorff dimensions
is also achievable, and this would induce an encoding of higher-dimensional topological
features depending of the number of dimensions of the ambient space.
Inspecting the dynamics within the framework of the BF formalism, just taking into ac-
count BF Lagrangians, we can easily convince ourselves that topological BF theory can be
accomplished: in (1+1)-dimensions resorting to the Jackiw-Teitelboim gravity [158, 159],
either for a SO(2,1) or for a SO(3) gauge symmetry, in (2 + 1)-dimensions, considering
either the topological Einstein-Hilbert action, without accounting for the cosmological con-
stant, or a double Chern-Simons theory, the quantization of which has been proved in [160]
to be equivalent to the Turaev-Viro [161, 162] quantization of the Einstein-Hilbert action
with cosmological constant, and in (3 + 1)-dimensions, producing the topological Ooguri
[163] and Crane-Yetter [164, 165] models. That different realizations that can be recov-
ered shows that dynamics is affected by the number of dimensions, even when considering
simplified topological theories described by kinetic BF actions.
Having considered a mathematical framework in which functorial evolutions of graphs are
supported on 2-complexes, of which graphs are slices of co-dimension 1, and having em-
bedded both the structures respectively in boundary space sectors B and bulk spaces, of
which B are slices, it is natural to be convinced that “spatially” organized data, localized
on the B sectors, are analyzed hierarchically, in the space or parameter (respectively, in the
Euclidean and Lorentzian signature case) flow that induces the slicing of the bulk. This
simple consideration has profound consequences since it ensures that TQNNs are “neuro-
morphic” in a relevant sense. In general, boundary states can be thought as holographic
states embedded in lower dimensional projections of the bulk. Boundary states may then
encode information about the local curvature when quantum group irreps are considered.
For instance, if the irreps that are considered participate in the evaluation of the partition
function of the double Chern-Simons theory in 3-dimensions, i.e., the Turaev-Viro model
endowed with SU (2) irreps, the cosmological constant provides the curvature of the faces
q
that belong to the polyhedra dual to the lattice structure.
We can cast the previous framework in terms of CCCDs, as models of QRFs. Since data
are organized spatially, as quantum boundary states/TQNNs are embedded in auxiliary
spaces, the boundary sectors B, CCCDs automatically turn out to be hierarchical in their
representations. This implies an orientation for the convolution of CCCD, the inputs of
which, thus, are not all processed by one only combinatoric criterion extended to the whole
system. Theauxiliaryspatialdegreeoffreedomparticipatesinthecoarse-graining. Because
of the proven hierarchical structure, local correlations turn out to be more informative than
distant ones, as correlations are suppressed spatially, in inverse powers of the distances in-
volved in the auxiliary spaces. This is a relevant feature for this framework, and involves a
25
confrontation among possible alternative scenarios for coarse-graining: the renormalization
group flow a` la Wilson versus the Kadanov group — see e.g. [166]. Within the TQNN
framework accounted for here, an invariance under coarse-graining of the simplicial tessel-
lation of the space slices of the manifolds emerges, unless one involves a more refined and
theoretically challenging extra geometrical renormalization group flow construction — see
e.g. the Ricci flow renormalization group approach of [167]. A paradigmatic example is
once again provided by the Turaev-Viro model, which is invariant under refinement of the
triangulations. This is not true, for instance, for the Ponzano-Regge model [168], which
instantiates the spin-foam/BF like quantization of the Einstein Hilbert action.
It is crucial to notice, in order to make our considerations in this paper sharper, that the
dynamics of TQNNs is instantiated by quantum curvature constraints proper of TQFTs
in a way that is equivalent to imposing the FEP on the system. The imposition at the
quantum level of the curvature constraints amounts indeed to an extremization of the
classical action of either the TQFTs or the effective QFTs describing the specific systems at
hand. Indeed, within the semiclassical limit, a constraint that imposes the limitation of the
free energy in computational tasks is automatically recovered, imposing tight requirements
to the efficiency of TQNN algorithms. Efficiency, which can be modeled as a cost per link
in either the TQNN or the CCCD picture, then corresponds to a path minimization for the
2-complexes structures intertwining among the boundary states. A detailed analysis of the
link between the FEP and the semiclassical limit of TQNNs, which is relevant to unveil
generalization in deep-learning systems (DNNs), will be addressed elsewhere.
Finally, in concluding this section we emphasize that we have established TQNNs as a
general framework for neuromorphic computation. Notoriously, this is not the case for
standard classical DNNs, which rather constitute a less general framework, corresponding
to a specific (semi-classical) limit of TQNNs.
7 Conclusion
The results reviewed here show how any system with morphological degrees of freedom and
locally limited free energy will, under the constraints imposed by the FEP, evolve toward a
neuromorphic morphology that supports hierarchical computations in which each “level” of
the hierarchy enacts a coarse-graining of its inputs, and dually a fine-graining of its outputs.
Such hierarchies occur throughout biology, from the architectures of intracellular signal-
transduction pathways to the large-scale organization of perception and action processing
in the mammalian brain. The close formal connections between CCCDs as models of QRFs
on the one hand, and between CCCDs and TQFTs on the other, allow the representation
of such computations in the fully-general quantum-computational framework of TQNNs.
One practical implication of the above analysis – that inherits from the distinction be-
tween states and parameters of a generative model – is a fundamental distinction between
biomimetic computation on Turing machines and neuromorphic computing. From a classi-
cal perspective, optimizing the states of a neural network can be read as inference, while
26
optimizing themodel parameters (i.e., connection weightsin an ANN) correspondsto learn-
ing at a slow timescale. In the biomimetic schemes, the connection weights or model pa-
rameters are generally stored in working memory in the form of tensors to compute the
messages that are passed along nodes of a factor graph to instantiate inference at a fast
timescale. However, in practice, the vast majority of compute time (and, thermodynamic
expenditure) is taken by reading and writing the connectivity tensors from memory. This
means that the arguments based upon minimizing the complexity of generative models only
provide a lower bound on the thermodynamics of belief updating [56, 169, 170, 171, 172].
This lower bound that can only be realized if the connection weights are physically realized
as in neuromorphic architectures. This may be an important motivation that goes be-
yond biomimetic aspirations [106], especially in applications such as edge computing (e.g.,
surveillance drones).
A further pragmatic perspective on recent trends in the machine learning is afforded by the
notion of hierarchical computation. In virtue of the fact that these entail a local minimiza-
tion of variational free energy (with locally limited thermodynamic free energy), efficient
computing on deep networks should conform to these local constraints. Indeed, this is
apparent in the move away from backpropagation schemes to local energy-based schemes
[173]. This is nicely illustrated by the comparative analyses of backpropogation with pre-
dictive coding implementations of deep learning [174, 175, 176]. In the current setting,
hierarchical predictive coding can be regarded as an implementation of VFE minimization,
under hierarchical generative models [177].
More generally, the above results offer some directions for future research. The first is
understanding how the pressures that result in neuromorphic architectures impact evo-
lutionary developmental biology, which seeks to determine the origin of specific nervous
system patterns [178, 179, 180]. More than looking backwards, however, this kind of work
can drive advances in both bio-hybrid (biorobotics, chimeric) and software-based AI. A va-
riety of hybrots, organoids, and biobots are being created [181, 182, 183] as a way to escape
the fact that all of Earth’s biological forms are basically an N = 1 example of evolution
(barring advances in exobiology). The inclusion of neural (and non-neural bio-electrical)
components in these synthetic beings, often made in the absence of any genetic change
[184, 185, 186, 187], will help test predictions of generic laws driving the structure and
function of the body-wide communication system. Similarly, these principles could of use
in designing unconventional and traditional connectionist computational systems, as well
as help drive the discovery of interventions guiding cell behavior in regenerative medicine
settings [87].
Acknowledgements
K.J.F is supported by funding for the Wellcome Centre for Human Neuroimaging (Ref:
205103/Z/16/Z), a Canada-UK Artificial Intelligence Initiative (Ref: ES/T01279X/1) and
the European Union’s Horizon 2020 Framework Programme for Research and Innovation
27
undertheSpecificGrantAgreementNo. 945539(HumanBrainProjectSGA3). M.L.grate-
fully acknowledges funding from the Guy Foundation and the Finding Genius Foundation.
A.M. wishes to acknowledge support by the Shanghai Municipality, through the grant No.
KBH1512299, by Fudan University, through the grant No. JJH1512105, the Natural Sci-
ence Foundation of China, through the grant No. 11875113, and by the Department of
Physics at Fudan University, through the grant No. IDH1512092/001.
Conflict of interest
The authors declare no competing, financial, or commercial interests in this research.
References
[1] Mead, C. 1990 Neuromorphic electronic systems. Proc. IEEE 78(10), 169–1636.
[2] DeYong, M.; Findley; R.; Fields, C. 1992 The design, fabrication, and test of a new
VLSI hybrid analog-digital neural processing element. IEEE Trans. Neural Netw.
3(3), 363–374.
[3] Schuman, C.D.; Potok, T. E.; Patton, R. M.; Birdwell, D.; Dean, M. E.; Rose, G.
S.; Plank, J. S. 2017 A survey of neuromorphic computing and neural networks in
hardware. Preprint arXiv:1705.06963v1 [cs.NE].
[4] Tang, J.; Yuan, F.; Shen, X.; Wang, Z.; Rao, M.; He. Y.; Sun, Y.; Li, X.; Zhang, W.;
Li, Y.; Gao, B.; Qian, H.; Bi, G.; Song, S.; Yang, J.; Wu, H. 2019 Bridging biological
and artificial neural networks with emerging neuromorphic devices: Fundamentals,
progress, and challenges. Adv. Mater. 31, 1902761.
[5] Butz M, Wo¨rgo¨tter F, van Ooyen A. 2009 Activity-dependent structural plasticity.
Brain Res. Rev. 60, 287–305.
[6] Carulli, D.; Foscarin, S.; Rossi, F. 2011 Activity-dependent plasticity and gene ex-
pression modifications in the adult CNS. Front. Mol. Neurosci. 4, 50.
[7] Hogan, M.K.; Hamilton, G. F.; Horner, P .J. 2020 Neural stimulation and molecular
mechanisms of plasticity and regeneration: A review. Front. Cell. Neurosci. 14, 271.
[8] Runge K; Cardoso C; de Chevigne A. 2020 Dendritic spine plasticity: Function and
mechanisms. Front. Synaptic Neurosci. 12, 36.
[9] Indiveri, G.; Liu, S.-C. 2015 Memory and information processing in neuromorphic
systems. Proc.IEEE 10(10), 1–17.
28
[10] Shatz, C. J. 1990 Impulse activity and the patterning of connections during CNS
development. Neuron 5, 745–756.
[11] Rakic, P.; Bourgeois, J. P.; Goldman-Rakic, P. S. 1994 Synaptic development of the
cerebral cortex: Implications for learning, memory and mental illness. Prog. Brain
Res. 102, 227–243.
ˇ
[12] Petanjik, C.; Judaˇs, M.; Simi´c, G.; Kostovi´c, I. 2011 Extraordinary neoteny of synap-
tic spines in the human prefrontal cortex. Proc. Natl. Acad. Sci. USA 108, 13281–
13286.
[13] Sandin, F.; Khan, A. I; Dyer, A. G.: Amin, A. H. H. M.; Indiveri; G.; Chicca, E.;
Osipov, E. 2014 Concept learning in neuromorphic vision systems: what can we learn
from insects? J. of Software Engin. Appl. 7, 387–395.
[14] Howard, S. R.; Greentree, J.; Avargu`es-Weber,A.: Garcia, J. E.;Greentree, A. D;
Dyer, A. G. 2022 Numerosity categorization by parity in an insect and simple neural
network. Front. Ecol. Evol. 10, 805305.
[15] Aertsen, A.M., et al. 1989 Dynamics of neuronal firing correlation: Modulation of
“effective connectivity”. J. Neurophysiol. 61(5), 900–917.
[16] Parr, T., Sajid, N., Friston, K. J. 2020 Modules or mean-fields? Entropy 22(5), 552.
[17] Nakagaki, T.; Yamada, H.; Toth, A. 2000 Maze-solving by an amoeboid organism.
Nature 407, 470.
[18] Blackison, D. J., Casey, E. S., Weiss, M. R. 2008 Retention of memory through
metamorphosis: can a moth remember what it learned as a caterpillar? PLoS One
3(3):e1736.
[19] Baluˇska, F., Lev-Yadun, S., Mancuso, F. 2010 Swarm intelligence in plant roots.
trends Ecol. Evol. 25(12):682-683.
[20] Stal, L. J. 2012 Cyanobacterial mats and stromatolites. In: B.A. Whitton (ed.),
Ecology of Cyanobacteria II: Their Diversity in Space and Time, Springer, pp. 65–
125.
[21] Vandenberg, L. N., Adams, D. S., Levin, M. 2012 Normalized shape and location of
perturbed craniofacial structures in the Xenopus tadpole reveal an innate ability to
achieve correct morphology. Devel. Dyn. 241(5):863–878.
[22] Mu¨ller, V. C., Hoffmann, M. 2017 What Is morphological computation? On how the
body contributes to cognition and control. Artif. Life 23(1):1–24.
[23] Yokawa, K., Baluˇska, F. 2018 Sense of space: Tactile sense for exploratory behavior
of roots. Comm. Integr. Biol. 11, 1–5.
29
[24] Murugan NJ, Kaltman, DH, Jin, PH, Chien, M, Martinez R, Nguyen, CQ, Kane, A,
Novak R, Ingber DE, Levin M. 2021 Mechanosensation mediates long-range spatial
decision-making in an aneural organism. Adv. Mater. 2021, 2008161.
[25] Friston, K. J. 2010 The free-energy principle: A unified brain theory? Nature Reviews
Neuroscience 11, 127–138.
[26] Friston, K. J. 2013 Life as we know it. Journal of The Royal Society Interface 10,
20130475.
[27] Friston, K. J. 2019 A free energy principle for a particular physics. Preprint
arXiv:1906.10184 [q-bio.NC]. https://arxiv.org/abs/1906.10184
[28] Pearl, J. 1988 Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo CA: Morgan Kaufmann.
[29] Clark A. 2017 How to knit your own Markov blanket: Resisting the second law with
metamorphic minds. In (T. Wetzinger and W. Wiese, eds.) Philosophy and Predictive
Processing 3, 19pp. Frankfurt am Mainz Mind Group.
[30] Marcian`o, A.; Chen, D.; Fabrocini, F.; Fields, C.; Greco, E.; Gresnigt, N.; Jinklub,
K.; Lulli,M.,Terzidis,K.; Zappala,E.2022Quantumneuralnetworksandtopological
quantum field theories. Neural Networks 153, 164–178.
[31] Friston KJ. A theory of cortical responses. Philos Trans R Soc Lond B, Biol Sci
2005;360:815–36.
[32] Friston KJ, Kilner J, Harrison L. A free energy principle for the brain. J Physiol Paris
2006;100:70–87.
[33] Friston KJ, Stephan KE. Free-energy and the brain. Synthese 2007;159:417–58.
[34] Friston KJ, FitzGerald T, Rigoli F, Schwartenbeck P, Pezzulo G. Active inference: a
process theory. Neural Comput 2017;29:1–49.
[35] Ramstead MJ, Badcock PB, Friston KJ. Answering Schro¨dinger’s question: a free-
energy formulation. Phys Life Rev 2018;24:1–16.
[36] Ramstead MJ, Constant A, Badcock PB, Friston KJ. Variational ecology and the
physics of sentient systems. Phys Life Rev 2019;31:188–205.
[37] Kuchling F, Friston K, Georgiev G, Levin M.2020 Morphogenesis as Bayesian infer-
ence: A variational approach to pattern formation and control in complex biological
systems. Phys Life Rev 33, 88–108.
[38] Hoffman, D. D., Singh, M., Prakash, C. 2015 The interface theory of perception
Psychon. Bull. Rev. 22, 1480–1506.
30
[39] Ramstead MJ, Sakthivadivel DAR, Heins C, Koudahl M, Millidge B, Da Costa L,
KleinB,FristonKJ2022OnBayesianmechanics: Aphysicsofandbybeliefs.Preprint
arXiv:2205.11543 [cond-mat.stat-mech].
[40] Fields C, Friston K, Glazebrook JF, Levin M 2021 A free energy princi-
ple for generic quantum systems. Prog. Biophys. Mol. Biol. in press (doi:
10.1016/j.pbiomolbio.2022.05.006; preprintavailableasarXiv:2112.15242[quant-ph]).
[41] Fields, C.; Marcian`o, A. 2019 Holographic screens are classical information channels.
Quant. Rep. 2, 326–336.
[42] Addazi, A.; Chen, P.; Fabrocini, F.; Fields, C.; Greco, E.; Lulli, M.; Marciano`,
A.; Pasechnik, R. 2021 Generalized holographic principle, gauge invariance and the
emergence of gravity `a la Wilczek. Front. Astron. Space Sci. 8, 563450.
[43] ’t Hooft G. Dimensional reduction in quantum gravity. In Ali A, Ellis J, Randjbar-
Daemi S. (eds) Salamfestschrift. Singapore: World Scientific, 1993, pp. 284–296.
[44] Susskind L. The world as a hologram. J Math Phys 1995;36:6377–6396.
[45] Bousso R. The holographic principle. Rev Mod Phys 2002;74:825–874.
[46] Aharonov Y, Kaufherr T. Quantum frames of reference. Phys Rev D 1984;30:368–385.
[47] Bartlett SD, Rudolph T, Spekkens RW. Reference frames, super-selection rules, and
quantum information. Rev Mod Phys 2007;79:555–609.
[48] Barwise, J.; Seligman, J. 1997 Information Flow: The Logic of Distributed Systems
(CambridgeTractsinTheoreticalComputerScience44).CambridgeUniversityPress,
Cambridge, UK.
[49] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory I:
Category-theoretic concepts and tools. J. Expt. Theor. Artif. intell. 31, 177–213.
[50] Fields, C.; Glazebrook, J. F.; Marcian`o, A. (2022) Sequential measurements, topo-
logical quantum field theories, and topological quantum neural networks. Preprint
arXiv:2205.13184 [quant-ph].
[51] Fields, C.; Glazebrook, J. F.; Levin, M. 2022 Neurons as hierarchies of quantum
reference frames. BioSystems 219: 104714.
[52] Hohwy, J. 2016 The self-evidencing brain. Nouˆs 50(2), 259–285.
[53] Kuchling, F.; Fields, C.; Levin, M.2022Metacognitionasa consequence ofcompeting
evolutionary time scales. Entropy 24, 601.
[54] Fields, C.; Glazebrook, J. F. 2020 Representing measurement as a thermodynamic
symmetry breaking. Symmetry 12, 810.
31
[55] Fields, C.; Glazebrook, J. F.; Marciano`, A. 2021 Reference frame induced symmetry
breaking on holographic screens. Symmetry 13, 408.
[56] Landauer, R. 1961 Irreversibility and heat generation in the computing process. IBM
J. Res. Dev. 5, 183–195.
[57] Landauer, R. 1999 Information is a physical entity. Physica A 263, 63–67.
[58] Bennett, C.H. 1982 The thermodynamics of computation. Int. J. Theor. Phys. 21,
905–940.
[59] Tenenbaum, J.B., et al. 2011 How to grow a mind: Statistics, structure, and abstrac-
tion. Science 331(6022), 1279–1285.
[60] Salakhutdinov, R., Tenenbaum, J. B., Torralba, A. 2013 Learning with hierarchical-
deep models. IEEE Trans Pattern Anal Mach Intell 35(8), 1958–1971.
[61] Tervo, D.G., Tenenbaum, J. B., Gershman, S. J. 2016 Toward the neural implemen-
tation of structure learning. Curr. Opin. Neurobiol. 37, 99–105.
[62] Friston, K., Penny, W. 2011 Post hoc Bayesian model selection. NeuroImage 56(4),
2089–2099.
[63] Conant, R. C., Ashby, W. R. 1970 Every Good Regulator of a system must be a
model of that system. Int. J. Systems Sci. 1(2), 89–97.
[64] Seth, A. K. 2014 The cybernetic Bayesian brain: from interoceptive inference to
sensorimotor contingencies. In: T. Metzinger & J. M. Windt (Eds). Open MIND.
Frankfurt am Main: MIND Group, Ch. 35.
[65] Jaynes, E. T. 1957 Information theory and statistical mechanics. Phys. Rev. Ser. II,
106(4), 620–630.
[66] Sakthivadivel, D.A.R. 2022 A constraint geometry for inference and integration.
Preprint arXiv:2203.08119.
[67] Schmidhuber, J. 2010 Formal theory of creativity, fun, and intrinsic motivation (1990-
2010). IEEE Trans. Autonom. Mental Devel. 2(3), 230–247.
[68] Ungerleider, L.G., Haxby, J. V. 1994 “What” and “where” in the human brain. Curr.
Opin. Neurobiol. 4(2), 157–165.
[69] Winn, J. Bishop, C. M. 2005 Variational message passing. J. Mach. Learn. Res. 6,
661–694.
[70] Friston, K., Parr, T., de Vries, B. 2017 The graphical brain: Belief propagation and
active inference. Netw. Neurosci. 1(4), 381–414.
32
[71] Friston, K., Buzsaki, G. 2016 The functional anatomy of time: What and when in
the brain. Trends Cogn. Sci. 20(7), 500–511.
[72] Yedidia, J.S.,Freeman, W. T., Weiss, Y. 2005 Constructing free-energy approxima-
tions and generalized belief propagation algorithms. IEEE Trans. Inform. Th. 51(7),
2282–2312.
[73] Dauwels, J. 2007 On variational message passing on factor graphs. In IEEE Interna-
tional Symposium on Information Theory, 2007.
[74] Zhang, C. et al. 2018 Advances in variational inference. IEEE Trans. Pattern Anal.
Mach. Intell. 41(8), 2008–2026.
[75] Parr, T., et al., 2019 Neuronal message passing using mean-field, Bethe, and marginal
approximations. Sci. Rep. 9(1), 1889.
[76] Beal, M.J. 2003 Variational Algorithms for Approximate Bayesian Inference. PhD.
Thesis, University College London.
[77] Mathews, J.; Levin, M. 2017 Gap junctional signaling in pattern regulation: Physio-
logical network connectivity instructs growth and form. Dev. Neurobiol. 77(5), 643–
673.
[78] Yamashita, Y. M.; Inaba, M.; Buszczak, M. 2018 Specialized intercellular communi-
cations via cytonemes and nanotubes. Annu. Rev. Cell Devel. Biol. 34, 59–84.
[79] Naphade, S., et al. 2015 Brief reports: Lysosomal cross-correction by hematopoietic
stem cell-derived macrophages via tunneling nanotubes. Stem Cells 33(1), 301–309.
[80] Wang, X., et al. 2010 Animal cells connected by nanotubes can be electrically coupled
throughinterposedgap-junctionchannels.Proc. Natl. Acad. Sci. USA107(40),17194–
17199.
[81] Turner, J.S. 2011 Termites as models of swarm cognition. Swarm Intell. 5(1), 19–43.
[82] Deisboeck, T. S.; Couzin, I. D. 2009 Collective behavior in cancer cell populations.
Bioessays 31(2), 190–197.
[83] Couzin, I. D. 2009 Collective cognition in animal groups. Trends Cogn. Sci. 13(1),
36–43.
[84] Shapiro, J. A. 1995 The significances of bacterial colony patterns. Bioessays 17(7),
597–607.
[85] Shapiro, J.A. 1998 Thinking about bacterial populations as multicellular organisms.
Annu. Rev. Microbiol. 52, 81–104.
[86] Friston, K.; Levin, M.; Sengupta, B.; Pezzulo, G. 2015 Knowing one’s place: A free-
energy approach to pattern regulation. J. R. Soc. Interface 12, 20141383.
33
[87] Pezzulo, G.; Levin, M. 2015 Re-membering the body: Applications of computational
neuroscience to the top-down control of regeneration of limbs and other complex
organs. Integr. Biol. (Cambridge) 7, 1487–1517.
[88] Pezzulo, G.; Levin, M. 2016 Top-down models in biology: Explanation and control of
complex living systems above the molecular level. J. R. Soc. Interface 13, 2016.0555.
[89] Harris, A. K. 2018 The need for a concept of shape homeostasis. Biosystems 173,
65–72.
[90] Palacios, E. R.; Razi, A.; Parr, T.; Kirchoff, M.; Friston, K. 2020 On Markov blankets
and hierarchical self-organization. J. Theor. Biol. 486, 110089.
[91] Barrat, A.; Barth´elemy, M.; Vespignani, A. 2008 Dynamical processes on complex
networks. Cambridge University Press, Cambridge UK.
[92] Rubinov, M.; Sporns, O. 2010 Complex network measures of brain connectivity: Uses
and interpretations. NeuroImage 52, 1059–1069.
[93] Latham, P.; Dayan, P. 2005 Touch´e: the feeling of choice. Nature Neuroscience 8(4),
408–409.
[94] Parr, T., Da Costa, L., Friston, K. 2020 Markov blankets, information geometry
and stochastic thermodynamics. Philos. Trans. A Math. Phys. Eng. Sci. 378(2164),
20190159.
[95] Da Costa, L. et al. 2021 Bayesian mechanics for stationary processes. Proc. Math.
Phys. Eng. Sci. 477(2256), 20210518.
[96] Fields, C.; Glazebrook, J. F.; Levin, M. 2021 Minimal physicalism as a scale-free
substrate for cognition and consciousness. Neurosci. Cons. 7(2), niab013.
[97] Levin, M.2019Thecomputationalboundaryofa“self”: Developmentalbioelectricity
drives multicellularity and scale-free cognition. Front. Psychol. 10, 1688.
[98] Fields,C.2018Someconsequencesofthethermodynamiccostofsystemidentification.
Entropy 20, 797.
[99] Fields, C.; Glazebrook, J. F. 2022 Information flow in context-dependent hierarchical
Bayesian inference. J. Expt. Theor. Artif. intell. 34, 111–142.
[100] Ashby, W.R. Introduction to Cybernetics. London: Chapman and Hall, 1956.
[101] Moore, E. F. Gedanken experiments on sequential machines. In: Shannon, C. W.;
McCarthy, J. (Eds.) Autonoma Studies. Princeton, NJ: Princeton University Press,
pp. 129–155.
[102] Weber, A., Varela, F. 2000 Life after Kant: Natural Purposes and the Autopoietic
Foundations of Biological Individuality. Phenomenol. Cogn. Sci. 1, 97–125.
34
[103] Kiebel, S. J.; Friston, K. J. 2011 Free energy and dendritic self-organization. Front.
Syst. Neurosci. 5, 80.
[104] Branco, T.; Clark, B. A.; Hausser, M. 2010 Dendritic discrimination of temporal
input sequences in cortical neurons. Science 329(5999), 1671–1675.
[105] Fields, C.; Levin, M. 2021 Metabolic limits on classical information processing by
biological cells. BioSystems 209, 104513.
[106] Sengupta, B., K. Friston 2018 How robust are deep neural networks? Preprint
arXiv:1804.11313.
[107] Zurek, W.H.2003Decoherence, einselection, andthequantumoriginsoftheclassical.
Rev. Mod. Phys. 75, 715–775.
[108] Schlosshauer M. 2007 Decohenece and the Quantum to Classical Transition. Springer,
Berlin.
[109] Fields, C., Levin, M. 2020 Integrating evolutionary and developmental thinking into
a scale-free biology. BioEssays 42, 1900228.
[110] Fields, C., Levin, M. 2020 Does evolution have a target morphology? Organisms 4,
57–76.
[111] Campbell, J. O. 2016 Universal Darwinism as a process of Bayesian inference. Front.
Syst. Neurosci. 10, 49.
[112] Ramirez, J. C.; Marshall, J. A. R. 2017 Can natural selection encode Bayesian priors?
J. Theor. Biol. 426, 57–66.
[113] Da Costa, L., et al. 2020 Natural selection finds natural gradient. Preprint
arXiv:2001.08028.
[114] Vanchurin, V., et al. 2022 Toward a theory of evolution as multilevel learning. Proc.
Natl. Acad. Sci. USA 119(6), e2120037119.
[115] Frank, S. A. 2012 Natural selection V. How to read the fundamental equations of
evolutionary change in terms of information theory. J. Evol. Biol. 25(12), 2377–2396.
[116] Sella, G.; Hirsh, A. E. 2005 The application of statistical physics to evolutionary
biology. Proc. Natl. Acad. Sci. USA 102(27), 9541–9546.
`
[117] Atiyah, M. 1988 Topological quantum field theory. Pub. Math. IHES 68, 175–186.
[118] Hoel, E. P., Albantakis, L., Tononi, G. 2013 Quantifying causal emergence shows that
macro can beat micro. Proc. Natl. Acad. Sci. USA 110, 19790–19795.
[119] Hoel, E. P. 2017 When the map is better than the territory. Entropy 19, 188.
35
[120] Hoel, E.; Levin, M. 2020 Emergence of informative higher scales in biological systems:
A computational toolkit for optimal prediction and control. Commun. Integr. Biol.
13(1), 108–118.
[121] Hoel, E.P. 2018 Agent above, atom below: How agents causally emerge from their
underlying microphysics. In: Wandering Towards a Goal: How Can Mindless Mathe-
matical Laws Give Rise to Aims and Intention?. A. Aguirre, B. Foster, and Z. Merali
(Eds.) Springer International Publishing: Cham, pp. 63–76.
[122] Albantakis, L., et al. 2017 What caused what? An irreducible account of actual
causation. Preprint arXiv:1708.06716.
[123] Hoel, E.P., et al. 2016 Can the macro beat the micro? Integrated information across
spatiotemporal scales. Neurosci. Cons. 2016, niw012.
[124] Clif, O.M., et al. 2017 Quantifying long-range interactions and coherent structure in
multi-agent dynamics. Artif. Life 23(1), 34–57.
[125] Wibral, M., et al. 2014 Local active information storage as a tool to understand
distributed neural information processing. Front. Neuroinform. 8, 1.
[126] Wang, X.R., et al. 2012 Quantifying and tracing information cascades in swarms.
PLoS One 7(7), e40084.
[127] Lizier, J.T., et al. 2011 Multivariate information-theoretic measures reveal directed
information structure and task relevant changes in fMRI connectivity. J. Comput.
Neurosci. 30(1), 85–107.
[128] Gidon, A.; Zolnik, T.A.; Fidzinski, P.; Bolduan, F.; Papoutsi, A.; Poirazi, P.;
Holtkamp, M.; Vida, I.; Larkum, M. E. 2020 Dendritic action potentials and compu-
tation in human layer 2/3 cortical neurons. Science 367, 83–87.
[129] Milner, B.; Corkin, S;Teuber, H.L.1968Furtheranalysisofthehippocampalamnesic
syndrome: Fourteen year follow-up study of H.M. Neuropsychologia 1968(6), 215–234.
[130] O’Keefe, J.; Dostrovsky, J. 1971 The hippocampus as a spatial map. Preliminary
evidence from unit activity in the freely-moving rat. Brain Res. 34(1), 171–175.
[131] Buzs´aki, G.; Chen, L. S.; Gage, F. 1990 Spatial organization of physiological activity
in the hippocampal region: Relevance to memory formation. Prog. Brain Res. 83,
257–268.
[132] O’Keefe, J.; Recce, M. L. 1993 Phase relationship between hippocampal place units
and the EEG theta rhythm. Hippocampus 3(3), 317–330.
[133] Buzs´aki, G. 1998 Memory consolidation during sleep: a neurophysiological perspec-
tive. J. Sleep Res. 7 Suppl. 1, 17–23.
36
[134] Burgess, N., et al. 2000 Predictions derived from modelling the hippocampal role in
navigation. Biol. Cybern. 83(3), 301–312.
[135] Burgess, N.; Maguire, E. A.; O’Keefe, J. 2002 The human hippocampus and spatial
and episodic memory. Neuron 35(4), 625–641.
[136] Davis, M. H.; Johnsrude, I. S. 2003 Hierarchical processing in spoken language com-
prehension. J. Neurosci. 23(8), 3423–3431.
[137] Dragoi, G.; Buzsa´ki, G. 2006 Temporal encoding of place sequences by hippocampal
cell assemblies. Neuron 50(1), 145–157.
[138] Sejnowski, T. J.; Paulsen, O. 2006 Network oscillations: Emerging computational
principles. J. Neurosci. 26(6), 1673–1676.
[139] Burgess, N.; Barry, C.; O’Keefe, J. 2007 An oscillatory interference model of grid cell
firing. Hippocampus 17(9), 801–812.
[140] Wittner, L., et al. 2007 Three-dimensional reconstruction of the axon arbor of a CA3
pyramidal cell recorded and filled in vivo. Brain Struct. Funct. 212(1), 75–83.
[141] Moser, E.I.; Kropf, E.; Moser, M. B. 2008 Place cells, grid cells, and the brain’s
spatial representation system. Annu. Rev. Neurosci. 31, 69–89.
[142] Buzs´aki, G.; Moser, E. T. 2013 Memory, navigation and theta rhythm in the
hippocampal-entorhinal system. Nature Neurosci. 16(2), 130–138.
[143] Bush, D., et al. 2015 Using grid cells for navigation. Neuron 87(3), 507–520.
[144] Stachenfeld, K.L.; Botvinick, M. M.; Gershman, S. J. 2017 The hippocampus as a
predictive map. Nature Neurosci. 20(11), 1643–1653.
[145] Barron, H.C.; Auksztulewicz, R.; Friston, K. 2020 Prediction and memory: A predic-
tive coding account. Prog. Neurobiol. 192, 101821.
[146] Garrido, M.I., et al. 2007 Evoked brain responses are generated by feedback loops.
Proc. Natl. Acad. Sci. USA 104(52), 20961–20966.
[147] Hasson, U., et al. 2008 A hierarchy of temporal receptive windows in human cortex.
J. Neurosci. 28(10), 2539–2550.
[148] Kiebel, S.J.; Daunizeau, J.; Friston, K. 2008 A hierarchy of time-scales and the brain.
PLoS Comput. Biol. 4, e1000209.
[149] Cocchi, L., et al. 2016 A hierarchy of timescales explains distinct effects of local
inhibition of primary visual cortex and frontal eye fields. eLife 5, e15252.
[150] Wang, X.-J.; Kennedy, H. 2016 Brain structure and dynamics across scales: In search
of rules. Curr. Opin. Neurobiol. 37, 92–98.
37
[151] Pezzulo, G., et al. 2014 Internally generated sequences in learning and executing
goal-directed behavior. Trends Cogn. Sci. 18(12), 647–657.
[152] Kaplan, R.; K.J. Friston, K. J. 2018 Planning and navigation as active inference. Biol
Cybern. 112(4), 323–343.
[153] Friston, K.; Buzsa´ki, G. 2016 The functional anatomy of time: What and when in
the brain. Trends Cogn. Sci. 20, 500–511.
[154] Farhi, E., Neven, H. 2018 Classification with quantum neural networks on near-term
processors. Preprint arXiv:1802.06002.
[155] Beer, K., Bondarenko, D., Farrelly, T., Osborne, T. J., Salzmann, R., Scheiermann,
D. 2020 Training deep quantum neural networks. Nature Comm 11(1), 1–6.
[156] Baianu, I.; Glazebrook, J. F.; Brown, R. 2009 Algebraic topology foundations of
supersymmetryandsymmetrybreakinginquantumfieldtheoryandquantumgravity:
A review. Symmet. Integra. Geom. Meth. Applic. (SIGMA) 5, 051.
[157] Rovelli, C.; Smolin, L. 1995 Spin networks and quantum gravity. Phys. Rev. D 52,
5743–5759.
[158] Jackiw, R. 1985 Lower dimensional gravity. Nucl. Phys. B 252, 343–356.
[159] Teitelboim, C. 1983 Gravitation and hamiltonian structure in two spacetime dimen-
sions. Phys. Lett. B 126, 41–45.
[160] Gresnigt, N. Marciano, A. and Zappala, A. 2022 On the dynamical emergence of
SU (2) from the regularization of 2+1D gravity with cosmological constant. Preprint
q
arXiv:2201.01726 [gr-qc].
[161] Turaev, V.G. and Viro, O.Y. 1992 State sum invariants of 3-manifolds and quantum
6j-symbols. Topology 31(4), 865–902.
[162] Turaev, V.G. 1994 Quantum Invariants of Knots and 3-manifolds. de Gruyer, New
York.
[163] Ooguri, H. 1992 Topological lattice models in four dimensions. Mod. Phys. Lett. A 7,
2799–2810.
[164] Crane, L.; Yetter, D. 1993 A Categorial construction of 4−D topological quantum
field theories. In: Quantum Topology, L. Kauffman and R. Baadhio (Eds.) World
Scientific, Singapore, pp. 120–123.
[165] Crane, L.; Kauffman, L.; Yetter, D. 1997 State-sum Invariants of 4-Manifolds I. J.
Knot Theor. Ramifications 6, 177–234.
[166] Lulli, M.; Marciano, A.; Zappala, E. 2022 In preparation.
38
[167] Lulli, M.; Marciano, A.; Shan, X. 2021 Stochastic quantization of general relativity a`
la Ricci-flow. Preprint arXiv:2112.01490 [gr-qc].
[168] Ponzano, G.; Regge, T.1968SemiclassicallimitofRacahcoefficients.In: Spectroscopy
and Group Theoretical Methods in Physics, F. Bloch (Ed.) North-Holland, Amster-
dam.
[169] Bennett, C.H. 2003 Notes on Landauer’s principle, reversible computation, and
Maxwell’s Demon. Stud. Hist. Phil. Sci. B 34(3), 501–510.
[170] Jarzynski, C. 1997 Nonequilibrium equality for free energy differences. Phys. Rev.
Lett. 78(14), 2690–2693.
[171] Crooks, G.E.2007Measuringthermodynamiclength.Phys. Rev. Lett.99(10), 100602.
[172] Still, S., et al. 2012 Thermodynamics of prediction. Phys. Rev. Lett. 109(12), 120604.
[173] Scellier, B.; Bengio, Y. 2017 Equilibrium propagation: Bridging the gap between
energy-based models and backpropagation. Front. Comput. Neurosci. 11, 24.
[174] Millidge, B.; Tschantz, A.; Buckley, C. L. 2020 Predictive coding approximates back-
prop along arbitrary computation graphs. Preprint arXiv:2006.04182.
[175] Marino, J. 2021 Predictive coding, variational autoencoders, and biological connec-
tions. Neural Comput. 34(1), 1–44.
[176] Salvatori, T., et al. 2021 Reverse differentiation via predictive coding. Preprint
arXiv:2103.04689.
[177] Friston, K. 2008 Hierarchical models in the brain. PLoS Comput. Biol. 4(11),
e1000211.
[178] Randel, N., et al. 2015 Inter-individual stereotypy of the Platynereis larval visual
connectome. eLife 4, e08069.
[179] Jekely, G.; Keijzer, F.; Godfrey-Smith, P. 2015 An option space for early neural
evolution. Philos. Trans. R. Soc. Lond. B Biol. Sci. 370(1684), 2015.0181.
[180] Keijzer, F.; van Duijn, M.; Lyon, P. 2013 What nervous systems do: Early evolution,
input-output, and the skin brain thesis. Adapt. Behav. 21(2), 67–85.
[181] Clawson, W. P.; Levin, M. 2022 Endless forms most beautiful: Teleonomy and the
bioengineering of chimeric and synthetic organisms. Biol. J. Linnean Soc. in press.
[182] Sole, R.; Moses, M.; Forrest, S. 2019 Liquid brains, solid brains. Philos. Trans. R.
Soc. Lond. B Biol. Sci. 374(1774), 20190040.
[183] Macia, J.; Vidiella, B; Sole, R. V. 2017 Synthetic associative learning in engineered
multicellular consortia. J. R. Soc. Interface 14(129), 20170158.
39
[184] Kriegman, S., et al. 2021 Kinematic self-replication in reconfigurable organisms. Proc.
Natl. Acad. Sci. USA 118(49), e2112672118.
[185] Blackiston, D.; Lederer, E.; Kriegman, S.; Garnier, S.; Bongard, J.; Levin, M. 2021
A cellular platform for the development of synthetic living machines. Sci. Robot. 6,
eabf1571.
[186] Levin, M. 2021 Life, death, and self: Fundamental questions of primitive cognition
viewedthroughthelensofbodyplasticityandsyntheticorganisms.Biochem. Biophys.
Res. Commun. 564, 114–133.
[187] Kriegman, S.; Blackiston, D.; Levin, M.; Bongard, J. 2020 A scalable pipeline for
designing reconfigurable organisms. Proc. Natl. Acad. Sci. USA 117, 1853–1859.
40

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "The Free Energy Principle drives neuromorphic development"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
