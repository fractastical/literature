=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Influence of the Geometry of the world model on Curiosity Based Exploration
Citation Key: sergeantperthuis2023influence
Authors: Grégoire Sergeant-Perthuis, Nils Ruet, David Rudrauf

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: In human spatial awareness, 3-D projective geometry structures information integration
and action planning through perspective taking within an internal representation space.
The way different perspectives are related and transform a world model defines a specific
perception and imagination scheme. In mathematics, such collection of transformations
corresponds to a ‘group’, whose ‘actions’ characterize the geometry of a space. Imbuing
world models with a group structure may capture different age...

Key Terms: geometry, world, paris, eans, structure, curiosity, universit, france, group, projective

=== FULL PAPER TEXT ===

Influence of the Geometry of the world model on Curiosity
Based Exploration
Gr´egoire Sergeant-Perthuis gregoireserper@gmail.com
LCQB Sorbonne Universit´e & OURAGAN team, Inria Paris Paris, France.
Nils Ruet
CIAMS, Universit´e Paris-Saclay, Orsay & Universit´e d’Orl´eans, Orl´eans, France.
David Rudrauf
CIAMS, Universit´e Paris-Saclay, Orsay & Universit´e d’Orl´eans, Orl´eans, France.
Dimitri Ognibene
Dipartimento di Psicologia, Universit`a Milano-Bicocca, Milan, Italy.
Yvain Tisserand
CISA, University of Geneva, Geneva, Switzerland.
Editor: *
Abstract
In human spatial awareness, 3-D projective geometry structures information integration
and action planning through perspective taking within an internal representation space.
The way different perspectives are related and transform a world model defines a specific
perception and imagination scheme. In mathematics, such collection of transformations
corresponds to a ‘group’, whose ‘actions’ characterize the geometry of a space. Imbuing
world models with a group structure may capture different agents’ spatial awareness and
affordance schemes. We used group action as a special class of policies for perspective-
dependent control. We explored how such geometric structure impacts agents’ behavior,
comparing how the Euclidean versus projective groups act on epistemic value in active
inference, drive curiosity, and exploration behaviors. We formally demonstrate and sim-
ulate how the groups induce distinct behaviors in a simple search task. The projective
group’s nonlinear magnification of information transformed epistemic value according to
the choice of frame, generating behaviors of approach toward an object of interest. The
projective group structure within the agent’s world model contains the Projective Con-
sciousness Model, which is know to capture key features of consciousness. On the other
hand,theEuclideangrouphadnoeffectonepistemicvalue: noactionwasbetterthanthe
initial idle state. In structuring a priori an agent’s internal representation, we show how
geometry can play a key role in information integration and action planning.
Keywords: Geometricworldmodel;Exploration;EmbodiedCognitiveScience;Cognitive
Modeling; Perception-Action Coupling
1 Introduction
In artificial agent learning and control, intrinsic and extrinsic rewards can be combined to
optimize the balance between exploration and exploitation. Intrinsic rewards in Reinforce-
ment Learning (RL) (Hester and Stone, 2017; Merckling et al., 2022; Oudeyer et al., 2007)
or terms of epistemic value in active inference (Friston et al., 2015) have been brought forth
3202
tcO
81
]IA.sc[
2v88100.4032:viXra
as mechanisms mimicking curiosity and driving exploration, e.g. by integrating prediction
error or uncertainty to drive actions favoring their reduction. However, efficient exploration
is a computationally hard task. Recent neural planning models have increased planning
flexibility and generality (Sekar et al., 2020). Yet, it is well-known that models’ structures
heavily impact planning performance and tractability (Geffner and Bonet, 2013) as well as
learning complexity (Goyal and Bengio, 2022). A good representation of information may
improve learning and search efficiency.
These issues are particularly salient for computation-heavy, highly recursive machine
learning algorithms and applications, e.g. reinforcement learning (RL) in artificial agents
(Bonet and Geffner, 2019) or recursive modeling method (RMM) in multi agent systems
(MAS) and partially observable stochastic games (POSG) (Geffner and Bonet, 2013). Al-
thoughgenericneuralworldmodelscansupportexploration-relatedprocesses,incorporating
priorknowledgethatshapesinternalrepresentationstomoreeffectivelysupportexploration
across a broad range of environments, such as 3-D environments, may enable autonomous
agents to explore more complex and realistic settings on a larger scale (Goyal and Bengio,
2022). The exploration planning problem can thus be approached by considering how the
structure of representation impacts exploration behaviors. In this article, the structure of
representations is encoded into the geometry of the state space of an agent; we quantify the
impact of changing this geometry on the behavior of the agent.
Here, we do not consider mechanisms of representation learning, e.g. in which world
dynamicsandactioneffectsneedtobelearnedandrepresented, astypicallydoneinRL.We
specifically consider control and execution when object locations, world states, and maps
may not be known but dynamics, rewards, and action effects already are. We focus on
action selection for environment exploration and mapping.
We adopt the active inference framework, i.e. an implementation of the Bayesian Brain
Hypothesis aimed at generating adaptive behaviors in agents (Friston et al., 2006), that
has found applications in neuroscience (Da Costa et al., 2020) and proposed for modeling
molecularmachines(TimsitandSergeant-Perthuis,2021;Timsitetal.,2021). Itreliesonan
internal representation of the environment that an agent is driven to explore and exploit.
The agent continually updates its beliefs about plausible competing internal hypotheses
on the environment state. Under common sensory limitations, active inference relates to
Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998; Ognibene
et al., 2019). The epistemic value of states is a quantity that arises in active inference
(Friston et al., 2015). Its maximization drives the agent’s curiosity and actions.
For exploration or search in 3-D space, it is warranted to consider how geometrical
principles could be embedded into efficient control mechanisms, to regularize the internal
representationofinformationandmediateexplorationunderadriveofuncertaintyreduction
or information maximization. Geometrical considerations have previously been integrated
intoavarietyofoptimizationandmachinelearningapproaches,suchasRL,activeinference,
and Bayesian inference (See Related Works below), but not in the specific perspective we
introduce herein.
We build upon the hypothesis that 3-D internal representations of space in agents per-
forming active inference may correspond to specific geometries, with properties that can
be exactly analysed. More specifically, we consider how different first person perspec-
tives may relate to each other, through transformations of a world model, as a specific
perception and imagination scheme for agents. This entails considering the action of ge-
ometrical groups of transformations (in the mathematical sense of the concept in Group
theory; see Section 3.1) on the spatial distribution of information experienced and en-
coded by agents internally. The question is whether such group action could contribute
to information gain estimation and maximization, as an internal planning or perspective-
dependent control mechanism. Certain geometrical groups might imply internal repre-
sentations, policies, value functions and principles of action that are particularly relevant
for search and exploration. More specifically, we wish to compare how different groups
impact the quantification of epistemic value. We then wish to characterize how the opti-
mization of action from those different groups may yield different exploration behaviors.
We contrasted two separate toy models of an agent performing a simple search task us-
ing active inference based solely on epistemic drives. One model used Euclidean geome-
try and the other projective geometry for the agent’s internal space. We compared the
two models in terms of resulting exploration behaviors and effects on epistemic value.
We chose to compare Euclidean versus projective geometry based on previous work,
leveragingpsychologicalresearchonthephenomenologyofspatialconsciousnessanditsrole
in the control of behaviors (Rudrauf et al., 2017, 2020, 2022, 2023). This research suggested
that 3-D projective geometry plays a central role in human cognition and decision-making
by shaping information representation and subsequent drives. It also offers a mechanism
of changes of points of view and perspectives on a world model, including for perspective
taking in social cognition, which is critical for the development of strategies of action plan-
ning in humans (see Rudrauf et al. 2022, 2023). We used Euclidean geometry as a standard
baseline geometry for comparison (Ognibene and Demiris, 2013). Our geometrical rationale
implies a different understanding of how agents’ actions in their environment (here in the
behavioral sense of the term) are implemented and selected compared to usual active infer-
ence. Agents’ actions, such as navigation and approach-avoidance behaviors, can naturally
be seen as dual to internal changes of perspective, i.e. group actions, in their representa-
tion space. We thus used group actions as a predictive model of actual behavioral actions.
The approach allowed us to formally study and demonstrate how the geometry gov-
erning the internal representation space may directly impact the computation of epistemic
value and ensuing exploratory behaviors. Projective geometry versus Euclidean geometry
demonstrated remarkable properties of information integration for motion planning under
epistemic drive.
2 Related Works
2.1 Representation of Space and Exploration, in the Context of Machine
Learning and Control
The integration of geometrical mapping in machine learning has been proposed to reduce
the high-dimensionality of input spaces and provide efficient solutions for action selection
and navigation. Seminal neurally inspired models used projections on 2-D manifolds for
representation learning of complex spatial information and self-motion effects (Arleo et al.,
2004). The impact of changes of perspective in exploration has long been of interest (Og-
nibene and Demiris, 2013). Ferreira et col. (Ferreira et al., 2013) proposed an internal
3-D egocentric, spherical representation of space, to modulate information sampling and
uncertainty as a function of distance, and control a robot attention through Bayesian infer-
ence. This was a seminal example of how geometrical rationale could suggest solutions to
integrate perception and action planning.
Exploration methods must often maintain high-resolution representations of space to
maximize information gain following action. This may hinder exploration efficiency, in par-
ticular in large-scale environments. 3-D topological representations of ambient space have
been proposed as part of an abstract planning scheme, showing promising improvements of
exploration efficiency (Yang et al., 2021).
Active vision principles, combined with curiosity-based algorithms and RL, were ap-
plied to the learning of saliency maps in the context of autonomous robots’ navigation
(Craye et al., 2016). The approach yielded promising optimization solutions to both adap-
tive learning of task-independent, spatial representations, and efficient exploration policies,
whichcouldserveaspriortosupportlong-term,task-oriented,utility-drivenRLmechanisms
(Crayeetal.,2016)(seealsoOgnibeneandBaldassare,2014;SperatiandBaldassarre,2017).
Complex control tasks with continuous state and action spaces have been solved using
deep reinforcement learning (DRL) with joint learning of representations and predictions.
Such approach may entail non-stationarity, risks of instability and slow convergence, in par-
ticular in control tasks with active vision. Separating representation learning and policies’
computations may mitigate the issues, but may also lead to inefficient information repre-
sentations. Merckling et col. (Merckling et al., 2022) have sought to build compact and
meaningful representations based on task-agnostic and reward-free agent-environment in-
teractions. They used (recursive) state representation learning (SRL) while jointly learning
a state transition estimator with near-future prediction objective, to contextually remove
distracting information and reduce the exploration problem complexity. Positive outcomes
were maximized through inverse predictive modeling, and prediction error was used to fa-
voractionsreducinguncertainty, whichimprovedsubsequentperformanceinRLtasks. The
authors emphasized that dealing with partial observability through memory and active vi-
sion may require new solutions to both representation learning of hidden information and
exploration strategies.
Uncertainty-based methods using intrinsic reward and exploration bonuses to plan tra-
jectories have been criticized for inducing non-stationary decaying surprise, and for being
hard to structure and optimize (Guo et al., 2021). Maximum State-Visitation Entropy
(MSVE) was introduced to maximize state exploration uniformity, but optimization has
been often challenging for large state spaces. Guo et col. (Guo et al., 2021) have intro-
duced Geometric Entropy Maximization, which leverages geometry-aware entropy based on
Adjacency Regularization (AR) and a similarity function, in order to optimize the MSVE
problem at scale.
Geometrical constraints considered across these related works were not integrated into
a global model, and were somewhat ad hoc. They pertained to a lower level of processing
than the one we are concerned with here. However, they emphasize the current needs and
challenges for integrating geometry in learning, control and navigation.
Methods and algorithms combining computer vision, machine learning and optimiza-
tion, e.g. for robotic planning, have integrated group theoretic concepts to obtain, for
instance, invariance to rotation and translation in image processing (Lee and Moore, 2004;
Qin et al., 2019; Meng et al., 2017). Likewise, the leveraging of geometrical concepts, in
Deep learning, e.g. for learning manifolds and graphs, has been growing in recent years,
demonstrating very promising results for representation learning (Gerken et al., 2021; Cao
et al., 2022). The approach introduces combinatorial structures to leverage prior knowledge
of geometry on the data of interest, e.g. applying ‘convolutional Neural Networks’ to non-
Euclidean space. However, the Euclidean group E , or more specifically SE (see Lee and
3 3
Moore, 2004), which includes translations and rotations, but excludes reflections, or simply
SO , the 3-dimensional rotation group (Gerken et al., 2021), are the groups being typically
3
considered.
Here, in addition to the Euclidean group, we also consider PGL(R3), the projective
general linear group in 3-D, which acts on a projective space through projective trans-
formations. The projective group is central to computer vision, for instance to generate
2-D images from 3-D information, but is used in such context in a restricted manner. We
sought approaches based on cognitive science, considering spatial cognition and its relations
to action at a higher level of integration, which does not reduce to the visual modality, but
instead assume the mapping of multimodal information on a supramodal internal space of
representation.
2.2 Projective Consciousness Model (PCM) and Active Inference
It has been shown that geometrically constrained active inference can be used as a frame-
work to understand and model central aspects of human spatial consciousness, through
the Projective Consciousness Model (PCM) (Rudrauf et al., 2017, 2022). According to
this model, consciousness accesses and represents multimodal information through a Global
Workspace (Dehaene et al., 2017) within which subjective perspectives on an internal world
model can be taken. The process contributes to appraise possible actions based on their ex-
pected utility and epistemic value (Rudrauf et al., 2022). In publications on PCM (Rudrauf
et al., 2017; Williford et al., 2018; Rudrauf et al., 2022, 2023; Williford et al., 2022), it was
hypothesized that such internal representation space is geometrically structured as a 3−D
projective space, denoted P (R). Changes of perspective then correspond to the choice of
3
a projective transformation ψ, i.e. an action from PGL . A projective transformation can
3
also be modeled as a linear isomorphism M ∈ GL (R) up to a multiplicative constant.
ψ 4
The model yieled an explanation for the Moon illusion (Rudrauf et al., 2020) with falsifi-
able predictions on how strong the effect should be depending on context; as well as for
the generation of adaptive and maladaptive behaviors, consistent with developmental and
clinical psychology (see Rudrauf et al., 2022). Though essential in integrative spatial cogni-
tion, notably for understanding multi-agent social interactions, perspective taking is rarely
integral to existing models of consciousness or formally implemented (Koch et al., 2016;
Kleiner and Tull, 2021; Mashour et al., 2020; Dehaene et al., 2017; Merker et al., 2022).
The PCM assumes that projective mechanisms of perspective changes are integral to the
global workspace of consciousness, both in non-social and social contexts. The advantages
of mechanisms of perspective taking for cybernetics remains to be fully formulated (see
Rudrauf et al., 2022).
3 Methodology
The experiment we considered is that of an agent, denoted as a, which is looking for an
object O in the ‘real world’, the 3-D Euclidean space E := R3. The set of moves of
3
the agent is denoted M. The position of O is denoted o ∈ E . The agent ‘represents’
3
the position of the object O inside its ‘internal world model’. We consider ‘internal world
models’, spaces denoted W, that are such that there is a group acting on them; we call such
spaces, group structured world models. This group accounts for the change of coordinates
that each movement of the agent induces when the positions of the object are expressed in
the agents reference frame. We consider two spaces in particular:
1. Euclidean case: W is the 3-D vector space, W = R3
2. Projective case: W is the 3-D projective space, denoted as P (R)
3
We will denote B(W) the Borel σ-algebra of the respective topological spaces.
InSection3.2, weexplainhowthe‘realworld’andthe‘internalworldmodel’arerelated
to one another in both the Euclidean and Projective case. Figure 1 illustrates the setup of
the toy model and main transformations considered. The agent’s internal beliefs about the
position of the object are encoded by a probability measure on W that the agent updates
through observations. The agent explores its environment through the computation of an
epistemicvalue, themaximizationofwhichcapturescuriosity-basedexploration. InSection
3.3, we explain how epistemic value is defined for group structured internal representations.
In Section 3.4 we give the details of the exploration algorithm.
3.1 Group Structured World Model
Let us first recall what a group is.
Definition 1 (Group, §2 Chapter 1 Lang, 2012). A group is a set G with an operation . :
G×G → G that is associative, such that there is an element e ∈ G for which e.g = g for any
g ∈ G, and any g ∈ G has an inverse denoted g−1 defined as satisfying, g.g−1 = g−1.g = e.
We call a group structured world model, a world model provided with a group action;
we now make this statement formal.
Definition 2 (Group structured world model). W is a group structured world model for
the group G when there is a map h : G×W → W denoted as h(g,x) = g.x for g ∈ G and
x ∈ X, such that,
1. (g.g ).x = g.(g .x) for all g,g ∈ G, x ∈ W
1 1 1
2. e.x = x, for all x ∈ W
In the Euclidean case the group structured world model, W, is the 3-D vector space
R3; it is structured by the group of invertible matrices GL (R). In the Projective case, the
3
group structured world model, W, is the projective space P (R); it is structured by the
3
group of projective linear transformations PGL(R3).
Figure 1: Toy model setup and main transformations
Upper-tier. Agent a simulates move m in Euclidean space E. R0 and R are its frames in E before
and after the move, oriented toward object O. Vertical arrows indicate transformations ϕ from the
external to the internal space. Lower-tier. Rendering of the effect of the internal group action
ψ(m) corresponding to move m in the Euclidean versus projective case. (Made with Unity).
3.2 Relating the ‘Real World’ to the ‘Internal World Model’
We assume that the ‘real world’ is the 3-D Euclidean space, E . We assume that the ‘real
3
world’comeswithwithanEuclideanframeR ,i.e. apointC andthreeindependentvectors
E
e ,e ,e . This frame is used to set up the experiment: the configurations of the object and
0 1 2
agent across time are encoded in this frame; it is fixed once and for all before starting
the experiment. Therefore we now identify E with R3, C with (0,0,0) and e ,e ,e with
3 0 1 2
the respective basis vectors, (1,0,0),(0,1,0),(0,0,1). The agent, denoted as a, is modeled
as a solid in the ‘real world’; it has its own Euclidean frame (the solid reference frame),
R := (P,u ,u ,u ), with P the center of a and u ,u ,u three unitary vectors that form a
0 1 2 0 1 2
basis.
In the Euclidean case, the map that relates E and its group structured world model,
3
W, is the affine map, ϕ , that changes the coordinate in R to coordinates in R.
R E
In the Projective case, this map is a projective transformation. The choice of such a
projective transformation is dictated by Proposition A.1 (Rudrauf et al., 2022). Let us now
recall some facts about that transformation.
Let for any (x,y,z) ∈ R3,
(cid:18) (cid:19)
x y z
ρ(x,y,z) = , , (1)
γz+1 γz+1 γz+1
with γ ∈ R a strictly positive parameter.
+
The (projective) transformation ϕp , from E to W, which relates the ‘real world’ to the
R 3
‘internal world model’ in the projective case, is posed to be ϕp := ρ◦ϕ .
R R
Proposition 1. When the agent a makes the move m ∈ M, its solid reference frame
changes from R to Rm. In the Euclidean case this move induces an invertible affine map,
from the ‘internal world model’ to itself. In the Projective case it induces a projective
transformation, ψ ∈ PGL(R3).
m
We denote 1 or x → 1[x ∈ U] the characteristic function of subset U, i.e. the functions
U
that is equal to 1 for x ∈ U and 0 elsewhere.
Remark 1. In both cases there is a dense open subset, U, of W which is in continuous
bijection with R3. From the Lebesgue measure dx on R3, we define the following measure
onW, dλ := 1 dx. Inwhatfollowswedonotraisethistechnicalpointanymoreandsimply
U
refer to dλ as the Lebesgue measure on W.
3.3 Beliefs, Policies and Epistemic Value
3.3.1 Beliefs
Theagentakeepsinternalbeliefsaboutthepositionoftheobjectrepresentedinits‘internal
world model’; these beliefs are encoded by a probability measure Q ∈ P(W), where P(W)
X
denotes the set of probability measures on W. Probability measures will be denoted with
upper case letters and their densities with lower case letters. These beliefs are updated
according to noisy sensory observations of the position of O. ‘Markov Kernels’ can be used
to formalize noisy sensors. Let us recall their definition.
A ‘Markov Kernel’ Π from Ω to Ω is a map Π : Ω × Ω → [0,1] such that for any
1 1
(cid:80)
ω ∈ Ω , Π(ω|ω ) = 1, i.e. a map that sends any ω ∈ Ω to a probability measure
1 1 ω∈Ω 1 1 1
Π ∈ P(Ω).
|ω1
The uncertainty on the sensors of a is captured by a Markov kernel P from W to
Y|X
W. It is a parameter of the experiment: it is fixed before the agent starts looking for O.
The couple (P ,Q ) defines the following probability density, P ∈ P(W ×W): for
Y|X X X,Y
any x,y ∈ W,
P (dx,dy) := p (y|x)q (x)dxdy (2)
X,Y Y|X X
where dx is the Lebesgue measure on W. An observation of the position of the object
yo ∈ W triggers an update of the belief Q to the belief with density
X
p (x|yo)q (x)dx
X,Y X
Q = (3)
X|yo (cid:82) p (x|yo)q (x)dx
x∈W X,Y X
3.3.2 Policies
Recall that the agent has a set of moves it can make M; moves m ∈ M are associated
with the group action ψ : W → W (Proposition 1). The agent plans the consequence of
m
its moves on its internal world model one step ahead: each change of frame induces the
following Markov Kernel, for any m ∈ M, A ∈ B(W), and x ∈ W,
0
p (A|x ,m) = 1[ψ (x ) ∈ A] (4)
X1|X0,m 0 m 0
Each move m spreads a prior Q on X into the following prior on X : ∀A ∈ B(W),
X 0 1
(cid:90)
ψ Q (A) := 1[ψ (x ) ∈ A]q (x )dx (5)
m,∗ X m 0 X 0
= Q (ψ−1A) (6)
X m
We chose to denote this probability measure as ψ Q , because it is the standard
m,∗ X
mathematical notation for the ‘pushforward measure’ by ψ . The generative model the
m
agent uses to plan its future actions is summarized in Figure 2.
3.3.3 Epistemic Value
Thefollowingdefinitionisarestatementoftheepistemic value introducedin(Fristonetal.,
2015) in the case of the kernel P : W → W.
Y|X
Definition 3 (Epistemic Value). For any probability measure Q ∈ P(W), the epistemic
X
value of this measure is:
C(Q ) :=E (cid:2) H(P |Q ) (cid:3) (7)
X PY X|Y X
(cid:90) (cid:90) p (x|y)
X|Y
= p (y)dy p (x|y)ln dx (8)
Y X|Y q (x)
X
H is the relative entropy, also called Kullback-Leibler divergence.
1[ψ (x ) ∈ A]
m 0
X = W X
0 1
ϕ
Rm
P
Y|X
Y = W ∋ yo Y ∋ yo
o ∈ E m
3 ϕ
R
Figure 2: m ∈ M is a move of the agent a, 1[ψ (x ) ∈ A] defines the kernel induced by
m 0
movem,P isthenoisysensor. Thediagramconstitutedofsolidarrowsdefines
Y|X
the generative model the agent uses to plan its actions. o is the position of the
object in the ‘real world’, yo ∈ W is the representation of o in the ‘internal world
model’ of a with respect to the solid reference frame R, yo is the same for the
m
reference frame Rm after move m.
Reexpressing Equation 7, it becomes apparent that epistemic value is simply a mutual
information:
(cid:90) p (x,y)
X,Y
C(Q ) = p ln dxdy (9)
X X,Y
p (y)q (x)
Y X
Weproposetodefinetheepistemicvalueofmovemastheepistemicvalueoftheinduced
prior on X ,
1
C(m) := C(ψ Q ) (10)
m,∗ X
3.4 Exploration Algorithm
Let us now put the previous elements together to describe the exploration behavior pro-
grammed in our agent. The agent a is initialized in a configuration of the ‘real world’, with
solid reference frame R0; the object O is positioned at o ∈ E . a starts with an initial belief
3
Q0 ∈ P(W)onthepositionofO. Itplansonestepaheadtheconsequenceofmovem; move
X
m induces a group action ψ : W → W that pushes forward the belief Q0 to ψ Q0 . The
m X m,∗ X
agent then evaluates the epistemic value of (P ,ψ Q0 ) for each move m and chooses
Y|X m,∗ X
the move that maximizes this value, m∗. a executes the move m∗ which transforms its solid
reference frame R to R. It can then observe (with its ’noisy sensors’) the position of O
0
which is yo := ϕ (o) in its internal world model, which triggers the update of prior ψ Q0
R m,∗ X
to the distribution conditioned on the observation: (cid:0) ψ Q0 (cid:1) . The process is iterated
m,∗ X |yo
m
with this new prior. The exploration algorithm is summarized in Algorithm 1.
4 Theoretical Predictions
We wish to understand how the group by which the internal world model is structured
influences the exploration behavior of the agent. The Euclidean case serves as the reference
model; in this case the world model shares the same structure as the real world: it is the
‘classical’wayofmodelingthisexplorationproblem. TheProjective case correspondstothe
Algorithm 1: Curiosity based Exploration for agent a
Data: Initialization: Q0 initial belief, R0 initial solid reference frame of a
X
1 Q X ← Q0 X ;
2 while True do
3 m∗ ← argmax m∈M C(ψ m,∗ Q X );
4 R ← solid reference frame of a after move m∗;
5 Q X ← ψ m,∗ Q X ;
6 yo ← ϕ R (o);
7 Q X ← Q X|yo ;
8 end
hypothesis underlying the PCM. The following Theorem states that this experiment allows
us to discriminate when the behavior of the agent is dictated by ‘objective’ perspectives
(Euclideanchangeofframe)versus by‘subjective’perspectives(projectivechangeofframe)
on its environment.
We consider the following noisy sensor, for any x,y ∈ R3,
3
P (y|x) = 1[∥x−y∥ ≤ ϵ] (11)
Y|X 4πϵ3
where ∥.∥ designates the Euclidean norm on R3, i.e. ∥x∥2 = x2 +x2 +x2; ϵ > 0 is a
0 1 2
strictly positive real number.
Theorem (Discrimination of behavior with respect to internal representations). Let us
assume that staying still is always a possible move for the agent.
Euclideancase: when the agent has an objective representation of its environment, given
by an affine map, the agent stays still.
Projective case: Assume now that the set of moves M is finite; assume furthermore that
after any possible move, the agent faces O, in other words, we assume that the agent knows
in which direction to look in order to find the object but is still uncertain on where the
object is exactly. If it has a ‘subjective’ perspectives, i.e. its representation is given through
a projective transformation, it will choose the moves that allows it to approach O (for any
ϵ small enough).
Proof. The details of the proof are given in Appendix A.2. Let us here sketch the proof.
The agent circumscribes a region of space in which it believes it is likely to find the object.
This region corresponds to the error the agent tolerates on the measurement it makes of
the position of O; we can also see it as the precision up to which the agent measures the
position of O. In the Euclidean case, the region in which the agent circumscribes the object
appears to always be of the same size, irrespective of the agent’s configuration with respect
to the object. Therefore not moving ends up being an optimal option and the agent will not
approach the object without additional extrinsic reward. In the Projective case, the agent
can ‘zoom’ on this region in order to gain more precision in measuring o; the configurations
of the agent in which this region is magnified are more informative regarding the position
of O and therefore preferred by the agent. The only way for the agent to actually zoom
onto this area is to approach the location it believes O is likely to be, therefore the agent
will end up approaching O.
Remark 2. This particular choice of Markov kernel (Equation 11) allows for an explicit
expression of epistemic value which simplifies the proof of the result; however we expect the
result to hold for a larger class of kernels.
In the next section we present an implementation of this experiment and simulation
results.
5 Simulations
5.1 Methods
Algorithm 1 is implemented in the following manner (source code available at https://
github.com/NilsRuet/effect-of-geometry-on-exploration). Beliefs and the Markov
kernelcorrespondingtosensorswereconsideredtobemultivariatenormaldistributions,that
is P ∼ N(µ , Σ ) and Q ∼ N(µ , Σ ). Belief update through the action of a
Y|X Y|X Y|X X X X
groupwasapproximatedusingaGaussiandistribution; aprojectivetransformationchanges
a Gaussian distribution into a non Gaussian one which is difficult to describe. Therefore we
replace this non-Gaussian distribution with a Gaussian distribution with same mean and
variance.
We assumed µ = x (which implies µ = µ ) and Σ = ϵ2I where I is the identity
Y|X y x Y|X
matrix and ϵ > 0 a positive real number. As a result, for a given observation yo, Q and
X|yo
C(ψ Q ) can be computed efficiently. The joint distribution P on X,Y is a Gaussian
m,∗ X
distribution:
P(x,y) = P (y|x)Q (x)
Y|X X
P(x,y) ∼ N(µ , Σ )
X,Y X,Y
with µ = (µ ,µ ) and
X,Y X X
(cid:18) (cid:19)
Σ Σ
Σ = XX XX (12)
X,Y Σ ϵ2I+Σ
XX XX
The variance of Y is Σ = ϵ2I+Σ .
YY XX
The joint distribution being Gaussian entails that the distribution of X conditioned on
y = yo is also Gaussian, thus Q ∼ N(µ , Σ ). Applying Proposition 3.13.(Eaton,
X|yo X|yo X|yo
2007) to our setting, the mean and covariance of the conditioned distribution are given by:
µ = µ +Σ Σ−1 (yo−µ ) (13)
X|yo X XX YY X
Σ = Σ −Σ Σ−1 Σ (14)
X|yo XX XX YY XX
Epistemic value is computed using the Kullback-Leibler divergence. With full knowl-
edge of the joint distribution, in the Gaussian case, following the expression of entropy for
gaussian vectors (Chapter 12 Equation (12.39) Cover and Thomas, 2006) it is computed as:
1 (detΣ )(detΣ )
XX YY
C(Q ) = I(X;Y) = ln (15)
X
2 detΣ
X,Y
The set of moves that can be selected by the agent is restricted to translations as the
agent must always face the object. This constraint, as well as the choice of a simple model
of noisy sensor (with homogeneous precision and resolution), was motivated by the aim of
making formal demonstrations of theorems tractable, and implementations tightly related
to the theoretical predictions. This is a departure from how spatial sampling typically
operates in perception, e.g. decreasing resolution of the visual field with eccentricity (see
(Rudrauf et al., 2022, 2023) for more realistic but less formally analyzable applications of a
broaderversionofthePCM).However,thespecificaimhereinofdemonstratingfundamental
properties of the action of different geometrical groups on epistemic value and ensuing
behaviors of approach motivated such restrictions.
The set of possible translations is composed of eight translations with the same norm,
with evenly distributed angles (one of them being oriented toward the object irrespective
of the position of the agent), and also contained an idle state, i.e. no translation. Here
the angles correspond to the angles of the translation and not a rotation angle of the solid
frame of the agent as the agent must always face the object.
We approximated the belief after the action m of a given group using a Gaussian distri-
bution, ψ Q ∼ N(µ , Σ ). The mean and covariance matrix are approximated using
m,∗ X m m
numerical integration:
(cid:90)
1
µ = xp(ψ−1(x)) dx (16)
m m |detJ (ψ−1(x))|
ψm m
(cid:90)
1
Σ = (x−µ )(x−µ )Tp(ψ−1(x)) dx (17)
m m m m |detJ (ψ−1(x))|
ψm m
We ran two sets of simulations. In the first one (Figure 3 left tier), the agent started
fromaninitialpositionwiththeobjectalwayslocatedatafixedposition, andthealgorithm
was applied for 20 iterations, for both the Euclidean and Projective internal spaces. The
agent started at (0,0) and the object was located at (0,2) in the world frame E spanning
2
the agent’s displacement floor. If all translations were associated with epistemic values that
only varied within a small range (±1e−4) as compared to the epistemic value of the idle
state, reflecting numerical imprecision, the idle state was selected (the agent did not move).
The aim of this set of simulations was to compare trajectories of agents displacements
through time across the two geometries. In the second set of simulations (Figure 3 right
tier), the agent started at the center of a 20×20 grid of possible positions of the object.
Thepositionsthataretooclosetotheagentareexcludedsothatasufficienteffectcanbe
observed. For each object position, we considered only the first set of translations that the
agent could envision from its initial position. We computed epistemic value across the set
of possible translations of the agent for each object position and for both the Euclidean and
projectiveinternalspaces. Theaimofthissetofsimulationswastobeabletosystematically
compare epistemic values across the two geometries for all possible positions of objects.
5.2 Results
Figure 3 left tier shows a representative example of trajectories obtained in the Euclidean
versus projective cases. In the projective case, the agent always approached the object. In
theEuclideancase,theagentalwaysstayedidle. Figure3righttiershowsepistemicvalueas
Figure 3: Simulation results
Left-tier. Trajectory of the agent for the projective versus Euclidean internal spaces. Right-tier.
Epistemic value as a function of directions of translation with respect to the object direction, for
the projective versus Euclidean internal spaces. Points are average values across comparable
directions, and error-bars are standard errors.
afunctionoftranslationdirectionexpressedinradiansforboththeprojectiveandEuclidean
cases. The direction of 0 radian corresponds to the object direction. We averaged epistemic
values across object positions within comparable directions. In the Projective case, average
epistemic value was maximal for the direction of the object, and decreased with directions
farther away from it. In the Euclidean case, average epistemic value was identical across
directions. Maximum average epistemic value was much higher for the projective case than
for the Euclidean case.
6 Discussion
In this article, we introduced a generative model for environment exploration based on
a first-person perspective in which actions are encoded as changes in perspective. The
families of geometry for the world model encode possible ‘kinds’ of perspective-taking on
the environment and structures the representation of sensory evidence within the world
model of the agent. In other words, each family corresponds to a specific perception and
imagination scheme for the agent. We encoded two such families, namely the Euclidean
versus projective group as acting on the internal world model of the agent, i.e., within the
geometric properties of this internal world model. We showed that different geometries
induce different behaviors, focusing on the two cases: when the internal world model of
the agent followed Euclidean geometry versus projective geometry. This result contributes
to understanding how integrative geometrical processing and principles can play a central
role in cybernetics. In our approach, the geometry of the world model links perceptual and
imaginary representations with actions and behaviors.
Althoughbeyondthescopeofthisarticle, weareinterestedingeneralizingtheapproach
to compare the results obtained with those that could be obtained with other models in
which geometry plays a central integrative role, e.g. as in (Ferreira et al., 2013) who used
an internal 3-D egocentric, spherical representation of space. Such an approach would need
to be expressed in terms of group action to make it formally comparable to our approach.
Likewise, it would be interesting to compare other groups, and other more realistic models
of sensors, and what types of behaviors they would induce. This would be useful to refine
predictionsandenvisionexperimentaldesignsforempiricalvalidationinhumans. Wewould
also like to use more sophisticated settings (see for instance Rudrauf et al., 2022, 2023),
even though it may be incompatible with the derivation of analytical solutions, but could
lead to richer simulations and induction of behaviors.
We also wish to further examine how the geometry of a latent space intertwines with
information processing. One motivation is theoretical, as we would like to assess how ge-
ometry changes learning behavior (Goyal and Bengio, 2022). In this contribution, we have
discarded representation learning per se, as it was beyond the scope of this study focusing
on planning. In future work, we intend to use deep learning to learn group structured
representations. It is important to note that such approach differs from geometric deep
learning (Bronstein et al., 2021; Sergeant-Perthuis et al., 2022) as we do not seek to learn
equivariant representations: a group structure will only be considered for the internal world
model but none will be presupposed on the observation side. Likewise, we are interested in
examining how geometry may play a role in overt and covert attention.
Our contribution can simplify the design of novel agent architectures where exploratory
and sensory choices of actions naturally emerge as a consequence of the internal representa-
tionandthereflectionoftheperceptualmechanismoftheagent’sembodiment. Thestudyof
world models with projective geometries was motivated by ongoing work in computational
psychology aimed at reproducing features of consciousness. Projective geometry induces
effects of magnification and focalization on information that appear immediately relevant
for spatial attention, and more generally for contextual salience. Another motivation for
this research is more practical, as we would like to use such principles to design virtual
and robotic artificial agents mimicking human cognition and behaviors following (Rudrauf
et al., 2022, 2023).
References
A. Arleo, F. Smeraldi, and W. Gerstner. Cognitive navigation based on nonuniform ga-
bor space sampling, unsupervised growing networks, and reinforcement learning. IEEE
Transactions on Neural Networks, 15(3):639–652, 2004. doi: 10.1109/TNN.2004.826221.
Blai Bonet and Hector Geffner. Learning first-order symbolic representations for planning
from the structure of the state space. arXiv preprint arXiv:1909.05546, 2019.
Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep
learning: Grids, groups, graphs, geodesics, and gauges, 2021. URL https://arxiv.org/
abs/2104.13478.
Wenming Cao, Canta Zheng, Zhiyue Yan, and Weixin Xie. Geometric deep learning:
progress, applications and challenges. Science China Information Sciences, 65(2):126101,
2022.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series
in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN
0471241954.
C´eline Craye, David Filliat, and Jean-Fran¸cois Goudou. Rl-iac: An exploration policy for
onlinesaliencylearningonanautonomousmobilerobot. In2016IEEE/RSJInternational
Conference on Intelligent Robots and Systems (IROS), pages 4877–4884, 2016. doi: 10.
1109/IROS.2016.7759716.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and
Karl Friston. Active inference on discrete state-spaces: A synthesis. Journal of
Mathematical Psychology, 99:102447, 2020. ISSN 0022-2496. doi: https://doi.org/10.
1016/j.jmp.2020.102447. URL https://www.sciencedirect.com/science/article/
pii/S0022249620300857.
Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is consciousness, and could ma-
chines have it? Science, 358(6362):486–492, 2017.
MorrisL.Eaton. Multivariatestatistics: Avectorspaceapproach. LectureNotes-Monograph
Series, 53:i–512, 2007. ISSN07492170. URLhttp://www.jstor.org/stable/20461449.
Jo˜ao Ferreira, Jorge Lobo, Pierre Bessiere, Miguel Castelo-Branco, and Jorge Dias. A
bayesian framework for active artificial perception. IEEE transactions on cybernetics, 43
(2):699–711, 2013.
KarlFriston, JamesKilner, andLeeHarrison. Afreeenergyprincipleforthebrain. Journal
of Physiology-Paris, 100(1):70–87, 2006. ISSN 0928-4257. doi: https://doi.org/10.1016/
j.jphysparis.2006.10.001. URL https://www.sciencedirect.com/science/article/
pii/S092842570600060X. Theoretical and Computational Neuroscience: Understand-
ing Brain Functions.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald,
and Giovanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6
(4):187–214, 2015. ISSN 17588936. doi: 10.1080/17588928.2015.1020053.
Hector Geffner and Blai Bonet. A concise introduction to models and methods for auto-
mated planning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 8
(1):1–141, 2013.
Jan E Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson,
Christoffer Petersson, and Daniel Persson. Geometric deep learning and equivariant
neural networks. arXiv preprint arXiv:2105.13926, 2021.
Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cog-
nition. Proceedings of the Royal Society A, 478(2266):20210068, 2022.
Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Alaa Saade, Shantanu Thakoor, Bilal
Piot, Bernardo Avila Pires, Michal Valko, Thomas Mesnard, Tor Lattimore, and R´emi
Munos. Geometric entropic exploration. arXiv preprint arXiv:2101.02055, 2021.
Todd Hester and Peter Stone. Intrinsically motivated model learning for developing curious
robots. Artificial Intelligence, 247:170–186, 2017.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting
in partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998.
Johannes Kleiner and Sean Tull. The mathematical structure of integrated information
theory. Frontiers in Applied Mathematics and Statistics, 6, 2021. ISSN 2297-4687. doi:
10.3389/fams.2020.602973. URL https://www.frontiersin.org/article/10.3389/
fams.2020.602973.
Christof Koch, Marcello Massimini, Melanie Boly, and Giulio Tononi. Neural correlates of
consciousness: progress and problems. Nature Reviews Neuroscience, 2016.
Serge Lang. Algebra, volume 211. Springer Science & Business Media, 2012.
PeiYeanLeeandJohnBMoore. Geometricoptimizationfor3dposeestimationofquadratic
surfaces. In Conference Record of the Thirty-Eighth Asilomar Conference on Signals,
Systems and Computers, 2004., volume 1, pages 131–135. IEEE, 2004.
George A. Mashour, Pieter Roelfsema, Jean-Pierre Changeux, and Stanislas Dehaene. Con-
scious processing and the global neuronal workspace hypothesis. Neuron, 105(5):776–
798, 2020. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2020.01.026. URL
https://www.sciencedirect.com/science/article/pii/S0896627320300520.
Zehui Meng, Hao Sun, Hailong Qin, Ziyue Chen, Cihang Zhou, and Marcelo H Ang. Intel-
ligent robotic system for autonomous exploration and active slam in unknown environ-
ments. In 2017 IEEE/SICE International Symposium on System Integration (SII), pages
651–656. IEEE, 2017.
Astrid Merckling, Nicolas Perrin-Gilbert, Alex Coninx, and St´ephane Doncieux. Ex-
ploratory state representation learning. Frontiers in Robotics and AI, 9, 2022.
Bjorn Merker, Kenneth Williford, and David Rudrauf. The integrated information theory
of consciousness: a case of mistaken identity. Behavioral and Brain Sciences, 45, 2022.
Dimitri Ognibene and Gianluca Baldassare. Ecological active vision: four bioinspired prin-
ciples to integrate bottom–up and adaptive top–down attention tested with a simple
camera-arm robot. IEEE transactions on autonomous mental development, 7(1):3–25,
2014.
Dimitri Ognibene and Yiannis Demiris. Towards active event recognition. In Twenty-Third
International Joint Conference on Artificial Intelligence, pages 2495–2501, 2013.
Dimitri Ognibene, Lorenzo Mirante, and Letizia Marchegiani. Proactive intention recogni-
tion for joint human-robot search and rescue missions through monte-carlo planning in
pomdp environments. In Miguel A. Salichs, Shuzhi Sam Ge, Emilia Ivanova Barakova,
John-John Cabibihan, Alan R. Wagner, A´lvaro Castro-Gonz´alez, and Hongsheng He,
editors, Social Robotics, pages 332–343, Cham, 2019. Springer International Publishing.
ISBN 978-3-030-35888-4.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems
for autonomous mental development. IEEE transactions on evolutionary computation,
11(2):265–286, 2007.
Hailong Qin, Zehui Meng, Wei Meng, Xudong Chen, Hao Sun, Feng Lin, and Marcelo H
Ang. Autonomousexplorationandmappingsystemusingheterogeneousuavsandugvsin
gps-denied environments. IEEE Transactions on Vehicular Technology, 68(2):1339–1350,
2019.
D. Rudrauf, G. Sergeant-Perthuis, O. Belli, Y. Tisserand, and G. Di Marzo Serugendo.
Modeling the subjective perspective of consciousness and its role in the control of be-
haviours. Journal of Theoretical Biology, 534:110957, 2022. ISSN 0022-5193. doi:
https://doi.org/10.1016/j.jtbi.2021.110957. URL https://www.sciencedirect.com/
science/article/pii/S0022519321003763.
D Rudrauf, G Sergeant-Perhtuis, Y Tisserand, T Monnor, V De Gevigney, and O Belli.
Combining the Projective Consciousness Model and Virtual Humans for immersive psy-
chological research: a proof-of-concept simulating a ToM assessment. ACM Transactions
on Interactive Intelligent Systems, 2023.
DavidRudrauf,DanielBennequin,IsabelaGranic,GregoryLandini,KarlFriston,andKen-
neth Williford. A mathematical model of embodied consciousness. Journal of theoretical
biology, 428:106–131, 2017.
David Rudrauf, Daniel Bennequin, and Kenneth Williford. The moon illusion explained by
the projective consciousness model. Journal of Theoretical Biology, 507:110455, 2020.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and
Deepak Pathak. Planning to explore via self-supervised world models. In International
Conference on Machine Learning, pages 8583–8592. PMLR, 2020.
Gr´egoire Sergeant-Perthuis, Jakob Maier, Joan Bruna, and Edouard Oyallon. On
non-linear operators for geometric deep learning. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural In-
formation Processing Systems, volume 35, pages 10984–10995. Curran Associates,
Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
474815daf1d4096ff78b7e4fdd2086a5-Paper-Conference.pdf.
Valerio Sperati and Gianluca Baldassarre. Bio-inspired model learning visual goals and
attention skills through contingencies and intrinsic motivations. IEEE Transactions on
Cognitive and Developmental Systems, 10(2):326–344, 2017.
Youri Timsit and Gr´egoire Sergeant-Perthuis. Towards the idea of molecular brains.
International Journal of Molecular Sciences, 22(21), 2021. ISSN 1422-0067. doi:
10.3390/ijms222111868. URL https://www.mdpi.com/1422-0067/22/21/11868.
Youri Timsit, Gr´egoire Sergeant-Perthuis, and Daniel Bennequin. Evolution of ribo-
somal protein network architectures. Scientific Reports, 11(1):625, January 2021.
ISSN 2045-2322. doi: 10.1038/s41598-020-80194-4. URL https://doi.org/10.1038/
s41598-020-80194-4.
Kenneth Williford, Daniel Bennequin, Karl Friston, and David Rudrauf. The projective
consciousness model and phenomenal selfhood. Frontiers in Psychology, 9:2571, 2018.
Kenneth Williford, Daniel Bennequin, and David Rudrauf. Pre-reflective self-consciousness
& projective geometry. Review of Philosophy and Psychology, 13(2):365–396, 2022.
Fan Yang, Dung-Han Lee, John Keller, and Sebastian Scherer. Graph-based topo-
logical exploration planning in large-scale 3d environments. In 2021 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pages 12730–12736, 2021. doi:
10.1109/ICRA48506.2021.9561830.
Appendix A. Proof of Proposition and Theorem
A.1 Proof of Proposition 1
Any 3-D affine transformation is encoded by a matrix M = (m ;i,j = 1..3) and a vector
i,j
(m ;j = 1..3); let (mR;i,j = 1..3) be the matrix associated to ϕ and (mR ;j = 1..3) its
j,4 i,j R 4,j
vector.
Projective case: ϕp = ρ ◦ ϕ is the projective map with expression in homogeneous
R R
coordinates given by the matrix,
 
m m m m
1,1 1,2 1,3 1,4
m 2,1 m 2,2 m 2,3 m 2,4
 
m 3,1 m 3,2 m 3,3 m 3,4
0 0 γ 1
By construction, the transition map in the projective case, ψp , is ϕp ◦ϕp −1 ; it is the
m Rm R
composition of two projective transformations, therefore it is a projective transformation.
A.2 Proof of Theorem 1
We will denote Bϵ the Euclidean ball of radius 1 around y ∈ R3,
y
i.e. Bϵ = {x ∈ R3| ∥x−y∥ ≤ ϵ}.
y
Lemma 1. For any Q ∈ P(W), both in Euclidean and Projective cases, for any affine map
or projective transformation ψ : W → W,
(cid:90)
C(ψ Q) = − dyQ(ψ−1(Bϵ))lnQ(ψ−1(Bϵ)) (18)
∗ y y
Proof.
3
C(ψ Q) = ×
∗ 4πϵ3
(cid:90) (cid:90) 1[x ∈ Bϵ]
ψ Q(dx ) dy1[x ∈ Bϵ]ln 1 y
∗ 1 1 y (cid:82) ψ Q(dx )1[x ∈ Bϵ]
∗ 1 1 y
(cid:90) (cid:90)
3
= − dylnQ(ψ−1(Bϵ) ψ Q(dx )1[x ∈ Bϵ]
4πϵ3 y ∗ 1 1 y
(cid:90)
3
= − dyQ(ψ−1(Bϵ))lnQ(ψ−1(Bϵ)) (19)
4πϵ3 y y
Proof of Theorem:
Euclidean case: for any set of moves M, and for any m ∈ M, ψ is a rotation; therefore
m
for any y ∈ W, ψ−1(Bϵ) = Bϵ . Then, for any prior Q ∈ P(W),
m y ψm −1(y)
(cid:90)
3
C(ψ Q) = − dyQ(Bϵ )lnQ(Bϵ )
∗,m 4πϵ3 ψm −1(y) ψm −1(y)
(cid:90)
3
= − dyQ(Bϵ)lnQ(Bϵ) (20)
4πϵ3 y y
In this case, the epistemic value is independent from the change of Euclidean frame, and
not moving is a perfectly valid choice for the agent to maximize it, at each time step of the
exploration algorithm (Algorithm 1).
Remark 3. The fact that staying still is a valid strategy arises as the agent assumes (or
believes) that it has access to the whole configuration space of O. If it knew it had limited
access to it, through for example limited sight, we expect the agent would look around until
the object O would be in sight, and then stop moving.
Projective case:
Consider two projective transformations ψ,ψ : W → W, if for any y ∈ W,
1
ψ−1(Bϵ) ⊆ ψ−1(Bϵ) (21)
y 1 y
then,
−Q(ψ−1(Bϵ))lnQ(ψ−1(Bϵ)) (22)
y y
≥ −Q(ψ−1(Bϵ))lnQ(ψ−1(Bϵ)) (23)
1 y 1 y
Thissuggeststhatthemovesthatmaximizeepistemicvaluearethosewhereψ−1 shrinks
m
the zone around yo = ρ(ϕ (o)), which is the representation of O in the internal world of the
R
agent. In particular, it means magnifying the zone around ρ(ϕ (o)) in the agent’s new
Rm
frame, Rm, after move m. The only way to do so is to select moves that bring the agent
closer to O. Let us denote yo := ρ(ϕ (o)). Let us now make the previous argument more
m Rm
formal. We assume that the set of moves M is finite. Let Q = q dλ be any initial prior
0 0
on W = P (R), at stating time t = 0. After one step, move m is chosen and the agent
3 1
updates its prior as,
q (x) ∼ = 1[x ∈ Bϵ ]q (x) (24)
1 yo 0
m1
∼
where = means proportional to. The prior we now consider is Q denoted simply as Q.
1
One shows that there is α > 0, such that for all m ∈ M, and ϵ > 0 small enough,
3
C(ψ Q) = − ×
∗,m 4πϵ3
(cid:90)
dy1[y ∈ Bαϵ]Q(ψ−1(Bϵ))lnQ(ψ−1(Bϵ)) (25)
yo m y m y
m
(26)
Let ≈ stand for ‘approximately equal to’ (equal at first order in expansion in powers of
ϵ). Then from the previous statement the summand can be approximated by its value in
yo :
m
C(ψ Q) ≈ −α3Q(ψ−1(Bϵ ))lnQ(ψ−1(Bϵ )) (27)
∗,m m yo m yo
m m
Furthermore, Q(ψ−1(Bϵ )) ≈ 4πϵ3 q1(yo) , where |det∇ψ |(yo) is the absolute
m y m o 3 |det∇ψm|(yo) m
value of the Jacobian determinant of ψ at yo. The epistemic value is maximized when
m
|det∇ψ |(yo) is maximized. By definition, ψ = ρ◦ϕ ◦ϕ−1 ◦ρ−1, therefore, by the
m m Rm R
chain rule of differentiation
|det∇ψ |(yo)
m
= |det∇ρ|(ϕ (o)).|detϕ |(o).|det∇[ϕ−1◦ρ−1]|(yo) (28)
Rm Rm R
Let us make explicit each terms in the previous equation. ϕ is a rigid movement
Rm
therefore, |detϕ |(o) = 1. |det∇[ϕ−1◦ρ−1]|(yo) does not depend on m so we can label it
Rm R
as a constant C. ϕ (o) is the coordinate of O in the Euclidean frame Rm; let us denote
Rm
(xm,ym,zm) these coordinates, i.e. (xm,ym,zm) := ϕ (o). Then,
Rm
1
|det∇ρ|(xm,ym,zm) = (29)
(γzm+1)4
Therefore, |det∇ψ |(yo) = C 1 .
m (γzm+1)4
As we assumed that for any move m ∈ M, the object O is always in front of the agent,
then zm ≥ 0; in this case, zm is also the distance of the agent to the object. Epistemic value
is maximized when zm is minimized and therefore the agent selects moves that reduce its
distance to the object. Denote one of such move m∗; the argument then loops back with
the new reference frame Rm∗ and updated belief q ← ψ q .
m,∗ |yo
m

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Influence of the Geometry of the world model on Curiosity Based Exploration"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
