=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Adaptive Active Inference Agents for Heterogeneous and Lifelong Federated Learning
Citation Key: danilenka2024adaptive
Authors: Anastasiya Danilenka, Alireza Furutanpey, Victor Casamayor Pujol

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: system, agents, adaptive, slos, resources, applications, heterogeneous, federated, learning, inference

=== FULL PAPER TEXT ===

1
Adaptive Active Inference Agents for Heterogeneous and Lifelong
Federated Learning
Anastasiya Danilenka , Alireza Furutanpey , Victor Casamayor Pujol , Boris Sedlak , Anna Lackinger ,
Maria Ganzha , Marcin Paprzycki , Schahram Dustdar , Fellow, IEEE
Abstract—Handlingheterogeneityandunpredictabilityaretwo applications must process streams of high-dimensional data
coreproblemsinpervasivecomputing.Thechallengeistoseam- that a service could ideally process at the source to fulfill
lessly integrate devices with varying computational resources in
a sub-10 millisecond latency Service Level Objective (SLO).
adynamicenvironmenttoformacohesivesystemthatcanfulfill
The caveat is that resources in proximity are constrained. the needs of all participants. Existing work on adaptive systems
typically focuses on optimizing individual variables or low-level Typical solutions involve task partitioning and lightweight
Service Level Objectives (SLOs), such as constraining the usage data reduction methods that minimize the penalty for of-
of specificresources. While low-level controlmechanisms permit floading to remote resources [5]. The second characteristic is
fine-grained control over a system, they introduce considerable heterogeneity, i.e., resource-asymmetry, vendor specifications,
complexity,particularlyindynamicenvironments.Tothisend,we
and usage patterns. Although pervasive applications follow
propose drawing from Active Inference (AIF), a neuroscientific
framework for designing adaptive agents. Specifically, we intro- an overall common objective, a system must consider the
duceaconceptualagentforheterogeneouspervasivesystemsthat individual properties and objectives of participants. Third,
permits setting global systems constraints as high-level SLOs. is the continuously drifting problem domain intrinsic to the
Instead of manually setting low-level SLOs, the system finds dynamic environments of pervasive applications, such that
an equilibrium that can adapt to environmental changes. We
the source distribution drifts over time and data volume is
demonstrate the viability of our AIF agents with an extensive
experiment design, using heterogeneous and lifelong federated non-static. Conclusively, a necessary precondition is a system
learningasanapplicationscenario.Weconductourexperiments thatcanadapttonon-identicallyandindependentlydistributed
onaphysicaltestbedofdeviceswithdifferentresourcetypesand (non-IID) data. Moreover, the system must facilitate collab-
vendor specifications. The results provide convincing evidence
oration between heterogeneous devices to fulfill their SLOs
thatanAIFagentcanadaptasystemtoenvironmentalchanges. by distributing workload fairly and considering the individual
In particular, the AIF agent can balance competing SLOs
in resource heterogeneous environments to ensure up to 98% properties of participants.
fulfillment rate. The focus of this paper is on lifelong heterogeneous feder-
ated learning (FL) as we find it best encapsulates the primary
Index Terms—Adaptive Computing, Service Level Objectives,
Active Inference, Federated Learning, Edge Computing. challenges of pervasive applications that share the described
characteristics. In general, FL participants collaborate for a
common objective, i.e., to maximize the prediction perfor-
I. INTRODUCTION mance. Yet, each participant has a private local validation
The Distributed Computing Continuum is an emerging set to determine whether their criteria are locally met. Time
paradigm for systems that can seamlessly integrate multiple constraints that ensure smooth operations and resource asym-
layers of computing infrastructure [1]. Computing contin- metryfurtherinstigatefrictionwhenattemptingtosatisfylocal
uum systems promise to enable infrastructure-critical perva- objectives. Hence, despite a common objective, to fulfill the
sive applications with stringent requirements, such as Mobile SLOs of each participant individually, a delicate balance is
Augmented Reality (MAR) for cognitive applications [2] necessary. Lastly, the dynamic environment gradually drifts
and remote sensing for disaster management [3]. There are the distribution and varies the data volume.
three recurrent characteristics among pervasive applications This work aims to demonstrate the viability of Active In-
deployed on a continuum. First, is their reliance on AI-based ference (AIF) in designing adaptive agents that can gracefully
methodsfortasksthatclassicalcontrolstructurescannotsolve handlethechallengingrequirementsofpervasiveapplications.
efficiently or with sufficient precision [4]. For example, MAR While AIF is a neuroscientific framework, recent work has
shown promising results by conceiving methods from the
*Correspondingauthor:AnastasiyaDanilenka underlying ideas for workload scheduling in distributed sys-
AnastasiyaDanilenkaandMariaGanzhaarewithFacultyofMathematics tems[6],[7].Inparticular,wefindthattheobjectivesofActive
and Information Science, Warsaw University of Technology (email: anas-
Inference and pervasive computing intrinsically intertwine.
tasiya.danilenka.dokt@pw.edu.pl,maria.ganzha@pw.edu.pl)
Alireza Furutanpey, Boris Sedlak, Anna Lackinger, and Schahram Contextawarenessiscrucialforpervasiveapplicationsasthese
Dustdar are with Distributed Systems Group, TU Wien (email: systems operate in dynamic environments and must adapt to
a.furutanpey@dsg.tuwien.ac.at, boris.sedlak@dsg.tuwien.ac.at,
changesintheirsurroundings.Preciselycontextawarenessisa
a.lackinger@dsg.tuwien.ac.at,dustdar@dsg.tuwien.ac.at)
Victor Casamayor Pujol is with Distributed Intelligence and definingcharacteristicofAIFagents.However,thecurrentap-
Systems-Engineering Lab, Universitat Pompeu Fabra (email: plication of AIF for systems is more conceptual and only par-
victor.casamayor@upf.edu)
tially implements the core components of the AIF framework.
Marcin Paprzycki is with Systems Research Institute, Polish Academy of
Sciences(email:paprzyck@ibspan.waw.pl) Other work on systems that adapt to changing requirements
This work has been submitted to the IEEE for possible publication. Copyright may be transferred
withoutnotice,afterwhichthisversionmaynolongerbeaccessible.
5202
raM
8
]GL.sc[
2v99090.0142:viXra
2
typically focuses on optimizing individual variables, such as concept drift can lead to both prolonged time until model
learning rate or setting low-level SLOs as constraints on convergence and reduced model performance, stressing the
specificresources[8],[9].Despiteprovidingmorefine-grained importance of treating concept drifts in FL and the need for
control, it is unreasonable to expect application developers to targetedsolutionsfordifferenttypesofconceptdrifts[11].Yet,
understandtheimplicationsofeachlow-levelconstrainttothe there is limited research in lifelong learning for FL [12]. The
overall system, particularly in dynamic environments where current approaches to adapt to concept drifts rely on custom
resources are scarce and availability is less predictable. In drift detectors to understand when the drift occurs [13], [14],
contrast, our AIF agent permits setting high-level SLO targets whichresultsintheneedtotunesaiddetectorstotheusecase
tofindanequilibrium,definedasthesystemconfigurationthat and potential drift scenarios, leaving the challenge on how
fulfills all its SLOs, without attempting to enforce constraints todistinguishdriftsfromanomalousdata.Moreover,although
from possibly conflicting low-level SLOs. conceptdriftsareoneofthemostchallengingissuestofacefor
We design experiments that accurately reflect the relevant lifelong FL, they are not the only one, as discussed further in
real-worldconditionsbyimplementingaphysicaltestbedcon- this section. This highlights the need for more self-adapting
sisting of heterogeneous devices with varying resource types mechanisms, e.g., by introducing meta-learning concepts to
and computational capabilities. Additionally, we leverage a FL[15],thatcanmaintaintheperformanceoftheFLsystems.
controlled process for data generation to evaluate the adapt- 2) Heterogeneous Federated Learning: We refer to Fed-
ability to a dynamic environment precisely. We extensively erated Learning as heterogeneous when data is non-IID and
evaluate our agents with a strong emphasis on reproducibil- hardware specifications vary among participants. Moreover,
ity. The results underpin the claim that an AIF agent can hardware heterogeneity typically implies that resources are
successfully balance competing SLOs among clients despite constrained, as less powerful devices, often located closer to
considerable resource asymmetry and adapt to the dynamic the data source, must also be accommodated in the learning
environment.Still,wetransparentlydiscusscurrentlimitations process.
by accentuating the parts of our result that best show our OptimizingFLworkflows,especiallyinthepresenceofhet-
agent’sweaknesses.Theintentionistofosterresearchinterest erogeneities,isimportantforminimizingthetime-to-accuracy
in AIF from a systems perspective, as we sincerely believe of model training [16].
that it poses an exceptionally promising research direction for In that context, Kundroo et al. [9] proposed FedHPO, a
pervasive applications and the compute continuum. Naturally, federated optimization algorithm that accelerates each client’s
weopen-sourceourrepositoryasanadditiontothecommunity training by modifying its hyperparameters, such as learning
to reproduce, scrutinize, and extend our approach 1. rate or epochs. However, FedHPO introduces additional algo-
We summarize our contributions as: rithm parameters to set and tune, e.g., patience or thresholds
• An adaptive mechanism for heterogeneous lifelong FL to guide the optimization process, which limits its flexibility
based on AIF which allows handling non-IID data distri- in dynamic environment usage. To optimize for local training
butionsandheterogeneousdevicecharacteristicsinherent time in dynamic and heterogeneous devices conditions, an
in pervasive computing environments. asynchronous FL approach FedTS was proposed by Li et
• A conceptual AIF agent that balances multiple SLOs al. [17], empowering the FL server to detect and optimize
during model training. When SLOs have competing for slower clients. Still, the scheme focuses on ensuring time
targets, agents can autonomously infer optimal training constraints for heterogeneous devices and does not cover
configurations without manual intervention. lifelong scenarios with dynamic data distributions.
• EmpiricalevaluationofAIFagentsforpervasiveFLtasks Several studies also explored multi-objective optimization
under real-world-inspired conditions, incorporating data (MOO) in FL to balance competing objectives. One approach
andresourceheterogeneitythroughareproducibleexper- istooptimizeneuralnetworkmodelsinsteadofclient-specific
imental setup with a physical testbed of heterogeneous hyperparameteroptimization[18],[19].Additionally,asignif-
devices. icantnumberofexistingresearchfocusesonoptimizingclient
selection or clustering instead of adjusting the parameters
II. BACKGROUNDANDRELATEDWORK of individual clients to reach multi-dimensional goals [20]–
A. Lifelong Heterogeneous Federated Learning [22]. Although these approaches offer practical solutions for
balanced MOO, they do not take into account the individual
In Federated Learning, participants train a global model to
parameters of heterogeneous clients. Moreover, a vast area of
maximize prediction performance without disclosing private
research lies in adopting Bayesian Optimization (BO) [23]
data. Participants optimize and validate the model parameters
to the needs of FL. Along with grid search, BO is used
with their local dataset in a training round before aggregating
for hyperparameter tuning in FL [24], still, the downside of
their weights globally.
both methods includes their inherent incentive to find the
1) Lifelong Federated Learning: FL is lifelong when train-
best possible hyperparameters set (either in one-shot fashion
ing continuously adapts to concept drifts and other changes
or during multiple communication rounds) which makes it
occurring in continuous data streams [10]. Introducing con-
moredifficulttoadaptthemtochangingconditionsoflifelong
cept drifts is an intrinsic property of the dynamic deploy-
learning. Another drawback of the classical BO is its focus
ment environment of pervasive applications. The presence of
on one singular objective to optimize, e.g., model accuracy.
1https://github.com/adanilenka/adaptive aif agents for fl This problem is starting to be addressed by Multi-objective
3
Bayesian Optimization (MOBO) [20]. Yet, the presented changestosuittheirpreferences.Theobjectiveistominimize
MOBO approach also does not consider lifelong learning the difference between the agent’s internal representation
scenarios. and real-world models, i.e., to adapt to its environment. In
A number of automated optimization tools are available to principle, the underlying framework of AIF generally applies
use for hyperparameter optimization tasks and were adopted toadaptivesystems[37].Therefore,itisreasonabletoassume
by the industry, such as HyperOpt [25] and Optuna [26], that AIF is a promising direction for computing continuums
with such platforms as RayTune [27] allowing for distributed that must adapt to a dynamic environment [6].
parameter tuning. Lately, FL research started to adopt hyper- AIF agents continuously evaluate the expected free energy
parameter optimization tools [28]–[32]. However, currently, (EFE) for different policies and assess their impact on un-
the application of hyperparameter tuning still does not cover derlying models. EFE constitutes the planning ability of AIF
lifelong learning scenarios. agents, as it allows for evaluating policies of custom length
Therefore, existing work on optimization in FL does not intothefuture,utilizingthecurrentunderstandingoftheworld
consider changing environments and lifelong FL scenarios model(generativeworldmodel)asthesourceforsimulations.
or lacks individual clients’ hyperparameter tuning in general, In a system’s context, an agent who understands the world
which is crucial for pervasive applications. modelwillminimizeEFEbyselectingpolicieslikelytofulfill
3) Service Level Objectives for Federated Learning: SLOs SLOs. We describe EFE with two distinct components [38]:
are definable constraints on a system that operators may use
PragmaticValue
as contracts with application developers [33], [34]. Low-level (cid:122) (cid:125)(cid:124) (cid:123)
EFE=−E [lnP(o|C)]−E D [Q(s|o)∥Q(s)]
SLOs quantify directly observable measures, such as CPU or Q(o|π) Q(o,s|π) KL
(cid:124) (cid:123)(cid:122) (cid:125)
memory usage. High-level SLOs abstract low-level SLOs to
InformationGain
reduce the difficulty of diagnosing and configuring complex (1)
and wide-spanning systems, i.e., compute continuums with The information gain (IG) estimates how much the model
measures such as throughput or monetary costs. can improve by choosing a particular policy and aims to
For our purposes, high-level SLOs provide an intuitive resolve the uncertainty currently present in the generative
interface to set targets for an AIF agent and quantitative world model. Thus, IG takes into consideration the approx-
measures to determine its adaptability to a dynamic envi- imate posterior Q over hidden states (variables in the world
ronment. In particular, maintaining prediction performance that the agent cannot observe) given observations and Q
and minimizing round duration are two primary objectives of hidden states only – our prior beliefs about the model
for lifelong heterogeneous federated learning. An SLO on before any observation. Here, the agent aims at maximizing
prediction performance ensures consistent solution quality. the divergence, thus, looking for the most informative future
In contrast, an SLO on timeliness is crucial as resources steps. Conversely, the pragmatic value (PV) estimates how
are constrained, and a considerably slower client can delay close a possible outcome (observation O) will be to the
globalweightupdates.TimeandpredictionperformanceSLOs agent’s preferred observations (C), focusing on meeting the
abstract more detailed system parameters that have an impact expectationdefinedfortheagent.Together,IGandPVbalance
on them, focusing on end-user experience and overall system the exploration/exploitation trade-off.
performance. The cases in which AIF was used to dynamically support
There exist multiple approaches that aim to combine SLOs computing systems are mainly focused on robotics; Oliver et
with dynamic processing requirements: Zhang et al. [35] al. [39] give a comprehensive overview of how AIF allows
presented Octopus – the framework that finds optimal ser- (robotic) systems to act under uncertainty. Nevertheless, the
vicesconfigurationsinmulti-tenantedgecomputingscenarios. application of AIF extends to continuous stream processing
Octopus predicts SLO fulfillment of two variables based on a systems, such as provided by Sedlak et al. [6], [7], which
deepneuralnetwork.Shubhaetal.[8]presentedAdaInf,which uses a wide set of processing metrics as sensory observations.
detects SLO violations of a GPU scheduling task whenever Actions taken by the processing system were elastic adapta-
variable drifts occur. Through AdaInf, it is possible to find tions,e.g.,scalingresourcesorquality,allowingtoempirically
SLO-fulfilling resource allocations between model training find system configurations that fulfill SLOs.
and inference. Although these approaches are SLO-aware, Proved useful for ensuring the adaptability of robotic and
orientedatcontinuousprocesses,andmayutilizeagentsforthe stream processing systems, AIF could address the existing
decision-makingprocess,theyareprimarilyusedforinference research gap in optimizing dynamic pervasive FL systems.
services and do not consider the scenario of optimizing the To sum up, while AdaInf and Octopus are SLO-aware in-
training of ML models, which require more flexible and high- ference services, they are designed for dynamic and resource-
level SLOs definitions, appropriate for the considered FL/ML efficient model serving, particularly in scenarios like multi-
scenario. model and edge inference. In contrast, traditional FL hy-
perparameter tuning methods, such as BO and grid search,
aim to achieve one best FL model but do not inherently
B. Active Inference
address the adaptability and continuity required for lifelong
Active inference is a neuroscientific framework based on learning. Active Inference, with its inherent adaptability, in-
the free energy principle (FEP) [36]. AIF agents adjust their nate exploration-exploitation trade-off handling, and potential
modelaccordingtonewobservationsandenactenvironmental explainability through causality, offers a promising theoretical
4
solution for the presented gap. To the best of our knowledge,
this paper is the first attempt to apply AIF to FL (and, by ex- Device
tension,lifelongFL),addressingthechallengesofadaptability Orchestrator Client AIF Agent
and heterogeneity in this domain.
Sends global model
Requests configuration
Evaluates all
III. PROBLEMSTATEMENT Re
c
t
o
u
n
rn
fi
s
g u
s
r
e
a
le
ti
c
o
t
n
ed c
b
o
a
n
s
f
e
ig
d
u
o
ra
n
t i
E
o
F
ns
E
We consider a lifelong heterogeneous FL system consisting Awaits for
all clients
of an orchestrator with N participants. In Figure 1 we can responses Performs
model training
observe a central model and orchestrator in the Cloud that
communicateswithdifferentIoT/Edgedevices.Theseproduce Returns updated model Sends new observation
Updates
astreamofdatathatisusedbyanMLmodel;atthesametime, Aggregates model BN
updates
these devices embed an AIF agent that adjusts the training
of their ML models to achieve optimal (SLO fulfillment) Next round
performance.
Fig.2. SequencediagramforoneFLroundoftheproposedmethod
configurationfromtheAIFAgent;(3)theAIFagentevaluates
configurations based on EFE and returns the best one; (4)
the client trains the model using the selected configuration;
(5) after training, the client sends a new observation to the
AIF Agent and returns the updated model to the Orchestrator,
which aggregates all updates; (6) the AIF agent updates its
world model (Bayesian Network) for future configuration
optimization; (7) the next FL round begins. Algorithm 1
summarizes the overall procedure for client-side training with
the rest of the section elaborating on notable details about the
process for an agent to find and choose optimal FL training
Fig.1. HeterogeneousFLwithdatastreamsandAIFagents configurations in a dynamic environment.
The objective of the FL system described in Figure 1 is to
maximize the overall SLO fulfillment across all timestamps. A. Learning a Simple World Model
ThechallengeistoensurethehighestpossibleSLOfulfillment ThegenerativemodelisatthecoreofanAIFagent,i.e.,as
(reaching equilibrium), given heterogeneous participants and an agent interacts with its environment, it updates its internal
data within a dynamic environment. Client hardware is het- world representation in a perception-action cycle to improve
erogeneous in vendor specification, available resource types, itsunderstandingandalignitsbehaviortoreachsetgoals.We
andoverallcomputationalcapacity.Forexample,somedevices chooseBayesianNetworks(BNs)astheyprovideinterpretable
mayhaveonboardaccelerators,suchasGPUs,andothersmay graphicalrepresentationsoflearnedcausalstructuresandoffer
onlyworkwithenergy-efficientCPUs.Thedatasourceisnon- a principled framework for probabilistic reasoning. This work
IID with temporal correlations, i.e., the training must adapt to considers discrete BNs with uniform priors. Each agent’s
non-stationary data. SLOs are set on a global system level initial Bayesian network structure is unknown, as there are
and all clients aim to fulfill the same SLOs. SLOs aim to no assumptions on prior knowledge of the environmental
ensuresmoothoperations,i.e.,timelytrainingandconsistently dynamic. The agents require only starting knowledge of the
adequate model performance, and clients check after each BNvariablesandtheirrespectivecardinalities,thus,specifying
training round whether the SLOs are fulfilled locally. theconsideredfeaturesintheenvironmentandtheirrespective
WeintroduceamechanismtocontrolandmanageSLOful- precision. We define the BN B of an agent as:
fillment by defining FL training configurations, which specify
training parameters that directly affect the system’s ability to B =(G,P)
fulfill SLOs. Configurations function as levers that an agent
where G = (V,E) is a directed acyclic graph (DAG), and P
can change to fulfill the high-level SLOs.
is the set of conditional probability distributions:
IV. PROPOSEDMETHOD P ={P(X
i
|Pa(X
i
))}n
i=1
This section presents the design of the AIF agents that
Pa(X ) represents the parents of X in G, for which the joint
i i
optimize a heterogeneous and lifelong federated system to
distribution of the variables is factorized as:
fulfill SLOs. Figure 2 illustrates how system entities interact.
n
(1) The orchestrator sends the current global model to the (cid:89)
P(X ,X ,...,X )= P(X |Pa(X ))
1 2 n i i
client, initiating a new FL round; (2) the client requests a
i=1
5
Algorithm 1: On Client Training Procedure
Learning Perform. Learning Perform. Learning Perform.
Rate SLO Rate SLO Rate SLO
Procedure TRAIN(global model, is lifelong)
train set ← FETCH NEXT TRAIN SET() T S i L m O e T S i L m O e T S i L m O e
config, expected ig ← INFER BEST CONFIG()
CPU Batch CPU Batch CPU Batch
With config: Usage Size Usage Size Usage Size
0 t t + n
updated model, metrics ←
Federated Rounds
TRAIN MODEL(global model, train set,
config) Fig. 3. BN structure update throughout FL training rounds (blue vertices
slos fulfilled ← CHECK SLO FULFILLMENT() representSLOs,green–configurationvariables)
If is lifelong:
new obs ← slos fulfilled ∪ config ∪
previous iterations and initializes structure re-learning of the
metrics
BN. Structure re-learning prioritizes the edges that include
UPDATE BN(new obs, expected ig)
SLOs as the dependent node, to ensure that the relations that
return updated model, metrics
have an immediate impact on agent decisions are considered
first. Conversely, if the observed IG is within expectations
Procedure INFER BEST CONFIG()
(equal or below expected IG), the agent only initializes a
configs←{}
parameterupdateontheBN,merelyaccommodatingnewdata
foreach c in possible configs do
thatfollowedtheexpectationsofthecurrentgenerativemodel.
EFE c , ig c ← CALCULATE EFE(c)
configs←configs∪(EFE ,ig ) To ensure the BN does not learn from early-stage ML
c c
model performance data, that do not accurately describe the
return possible configs
argmin(configs.EFE)
relationshipbetweenperformanceSLOandconfiguration,and
instead focuses on the lifelong part of the training only,
Procedure UPDATE BN(new obs, expected ig)
observations from early FL rounds are omitted until the
obs surprise ← CALCULATE SURPRISE(BN,
model performance stabilizes. When the global FL model
new obs)
performance gets sufficiently close to the target performance
if obs surprise>expected ig then
SLO, a lifelong learning flag signals the AIF agent to start
BN ← DO STRUCTURE LEARNING()
learning, before that, the configuration is taken at random, as
BN ← DO PARAMETER ESTIMATION(BN)
else no pre-defined strategies are defined for this warm-up stage.
BN ← DO PARAMETER UPDATE(BN,
new obs)
B. EFE and SLO-aware Configuration Selection
Due to SLO and configuration vertices present in the BN,
thetaskistochoosetheconfigurationwiththehighestchance
The BN vertices are divided into three categories: of fulfilling SLOs by correctly discovering the connections
betweentheverticesandresolvinguncertaintyabouttheworld.
1) Configuration vertices: Represent the (hy-
The agent calculates the EFE for each configuration available
per)parameters of the system that are available for
to the system to determine configuration optimality using
the agent to set.
the formula in Equation (1). Specifically, it calculates the
2) SLO vertices: Binary vertices that encode SLO being
pragmatic value as:
fulfilled or not and allow for finding dependencies
between SLOs (and their fulfillment) and other vertices (cid:88)
Pragmatic Value= P(SLOs|configuration)
of the BN.
(2)
SLOs
3) System vertices: Additional vertices provide a more
×preference vector
comprehensive overview of the system dynamic, such
as resource usage, and their connection to SLOs. and the Information Gain as:
Figure 3 illustrates the process of learning the structure of
Information Gain=I(A,q) (3)
the BN as the FL rounds progress. We use Hill Climb
Search [40] and Bayesian estimation to perform structural The preference vector encodes the agent’s goal, i.e., the
learningandparameterestimation.Weusevariableestimation desired outcome, and is expressed as a logarithm of the
to perform exact inference, such that an AIF agent utilizes normalized preferences. The list of outcomes consists of all
precisecomputationtoleveragetheuncertaintyofBNs.Asthe combinations of possible SLO values. Since the agent is to
FLroundsprogress,theBNcausalstructuresareprogressively fulfill all set SLOs, the state where all SLOs are fulfilled
discovered. Moreover, the agent experiences further refine the will have a higher preference. For example, for one binary
conditional probability distributions. outcome SLO, one can set the preference vector to [0.001,
To allow the agent to adapt to significant discrepancies 0.999], specifying that the second outcome is preferred.
between expected and observed outcomes, we distinguish Information gain quantifies the expected Bayesian surprise
between two update types. If the observed IG is higher than that measures how much observing new data would update
expected, the agent discards structure information from the the agent’s belief about hidden states. First, for each possible
6
configuration, the agent assesses which observation is most We choose time and performance as SLOs as balancing them
likely.Anobservationisaconfigurationandanassociatedout- is non-trivial. For example, focusing exclusively on fulfilling
come.Theagentusesaparticularconfigurationasevidenceto prediction performance may require spending an excessive
predict the possible observation with a maximum a posteriori amount of time and vice versa. The configurable hyperparam-
(MAP) query to simulate possible future. Then, following the eters are Batch Size BS ∈{8,32,64,256,512} and Learning
implementation in [41], the calculation (Equation 3) uses the Rate LR ∈ {0.0005,0.001,0.005,0.01}, as there is a clear
likelihood of SLOs fulfillment (matrix A) and the predictive connection to them and the system’s training objective and
density over hidden states q derived from the BN to predict considered drift types, e.g., learning rate tuning was proposed
IG of a specific configuration. to battle concept drift [12]. The values chosen were selected
In summary, the information gain and pragmatic value to span across reasonable ranges that can make a difference
balance a trade-off between exploring and taking the actions in terms of SLOs while being distinct from each other. Each
thatmostlikelyresultinSLOsfulfillment.Oncetheagenthas client initializes an independent data stream locally.
selected a configuration, the local training round starts. On The data generator G is also parametrized by a drift
completion, the agent collects lower-level metrics and checks parameter, drift, which controls the presence and speed of
SLO fulfillment. Lastly, it associates the outcomes with the data drifts, where drift=0 indicates no data drift.
configuration and adds it to the history dataset as a new In each federated round, clients train and validate their
observation for further updates. predictionmodelusingthedatasamplesavailableinanonline
learning fashion. At round t, each client n possesses two
V. EVALUATION datasets: Validation t = {(x b ,y b )}B b= t 1 ∼ G n and Train t =
Validation , where x is a feature vector, y label assigned to
A. Experiment Design t−1
the data sample and B the size of the data set drawn from
t
The experiment design examines the AIF agent’s behavior
the data generator at round t.
and adaptability to heterogeneity and lifelong FL.
This way, clients acquire a new batch of data for validation
1) Test Bed: We implement a physical testbed with con-
whilethepreviousbatchisre-usedfortraining.Previousround
straineddevicestoreplicateaheterogeneousresourceenviron-
training samples are discarded.
ment.Additionally,weuseavirtualmachinewithserver-grade 4) Baselines: We choose two baselines to present AIF
hardware for experiments in more controlled environments. agents. The first set focuses on the presentation of the be-
Table I summarizes the hardware specifications. haviour of AIF agents, aiming at identifying both adaptable
behaviour pattern and assessing agents’ performance. The
TABLEI second baseline compares AIF agents to the state-of-the-art
TESTBEDHARDWARESPECIFICATIONS
Optuna framework to assess the optimality of hyperparameter
Device CPU Accelerator choices made by the agents.
VirtualMachine 8xXeon@3.7GHz Tes.2560CC We define two baselines to illustrate AIF agents behaviour
OrinNX 8xCortex@2GHz Amp.1024CC32TC under data distribution drifts:
XavierNX 4xCortex@2GHz Vol.384CC48TC
1) Random: Represents a complete lack of adaptability
RaspberryPi4 4xCortex@1.8GHz N/A
RaspberryPi5 6xCortex@2.0GHz N/A and intelligent choice of hyperparameters. This baseline
randomlychoosesanewconfigurationforeachfederated
2) Implementation Details: We implement the prediction training round.
model as a simple Artificial Neural Network (ANN) with 2) Fixed optimal: Represents the case where parameter
PyTorch consisting of two fully connected layers using ReLU tuning was performed and the optimal configuration
activation for non-linearity. is set once at the beginning of the training and is
We extend the Flower [42] framework to support FL. We never changed. To select this configuration, each of the
implement the agent BNs with pgmpy [43] and information possible configurations was tested and the one with the
gain with pymdp [41]. We implement a controllable data highestmeanSLOsfulfillmentattheendoftheobserved
generationprocessusingRiver[44].Amoredetailedtechnical period (around round 50) was taken as the optimal.
description is out of scope and we refer interested readers to To align Optuna with the AIF agent, its hyperparameter
the accompanying repository. search process was modified. First, the hyperparameter study
3) Application Scenario: We emulate a dynamic environ- was set to cover all FL rounds, with the first hyperparameters
ment by controlling the data generation process to introduce set trial starting after the global model performance reached
concept and volume drifts. Each client device represents a sufficient accuracy (same as for AIF agent as described in
different participant. The challenge is, for the system to adapt Section IV) and performing one trial per FL round until the
to the drifts or to the varying computational resources of end of the training. To select the configuration for the current
participants. The experiments consider the fulfillment of two FL round, first, an Optuna trial is performed and added to the
binary high-level SLOs: study (stored in the local Optuna database). Then, the best
1) Time: fulfilled if a local training round does not exceed configuration is inferred, and the FL training round starts. To
a set limit (e.g., 2 seconds). maintain the integrity of Optuna trials in the lifelong learning
2) Prediction Performance: fulfilled if the primary vali- process, each trial is conducted using a copy of the global
dation metric (accuracy) exceeds a set value. model from the previous round, evaluated on the training
7
data from that same round. By isolating trials from ongoing
Performance SLO Time SLO
learning dynamics, we prevent information leakage in both Approach: aif fixed random
nt 1 1
directions–fromthecurrentglobalmodeltothetrialsandvice e
m
versa – preserving the validity of the optimization process. ulfill
To make Optuna SLO-aware, the study was designed in
v e
F
0.5 0.5
MOO fashion, where SLOs variables served as objectives. ati
Here, training time and 1 – validation set accuracy served m
ul
u
as two objectives for optimization (minimization). By default, C
0 0
50 100 50 100
for MOO Optuna returns not a single best configuration, but
Federated Round Federated Round
a Pareto front – a set of feasible configurations, leaving the
finalchoiceuptotheuser.Intheexperiments,wefirstattempt
Fig.4. MeancumulativeSLOsfulfillmentwithtwoquantitydrifts(redlines
to filter out only those configurations in the Pareto front that mark the drift start). Semitransparent lines show mean SLO fulfillment at a
fulfill both SLOs, then both time and performance metrics are singlefederatedround.
normalized and weighted equally into one feature, where the
best configuration is returned for the FL round training.
largerthan50%.Still,afterthesecondquantitydrifthappened,
Each experiment describes the results regarding SLO ful-
the time SLO became even more challenging, which led
fillment, as the evaluation metric expected by the users, and
to a more prominent decline in time SLO fulfillment. To
EFEdynamicsthatexplainthelearningandadaptationofAIF
understand the choices made by the AIF agents, we examine
agents.
mean EFE over all configurations at each epoch (Figure 5).
Cumulative SLO fulfillment at round t is calculated as:
(cid:80)t
SLO fulfilled
Each ex
S
p
L
e
O
rim
F
e
u
n
lfi
t
ll
w
m
a
e
s
nt t
re
=
peate
i
d
=1
ten
t
times wit
i
h differe
(4
n
)
t R
ate)
5
5
5
1
5
1
1
2
1
2
2
,
2
,
,
0
,
0
0
. 0
0
.
.
0
0
0
.0
0
0
0
1
1
5
5
EFE
1.4
random seeds that controlled the random processes, such as ni n g 2 2 5 5 6 6 , , 0 0 .0 .0 0 1 5 1.3
t
e
h
v
e
alu
d
a
a
t
t
i
a
on
g
,
en
th
e
e
rat
fi
or
rst
an
c
d
on
A
s
N
id
N
ere
w
d
ei
t
g
im
ht
e
s
st
i
e
n
p
iti
i
a
n
li
c
z
l
a
u
t
d
io
ed
n.
i
F
n
or
SL
th
O
e
Size,
Le ar 2 2 5
6
5 6
6 4
6 ,
4 ,
, 0
, 0
0 . 0
0 .
.
0
0 0
.0 0
0 0
1 5
1 5
1
1
.
.
1
2
fulfillment tracking for a particular client run was the one atc h 6 6 4 4 , , 0 0 .0 .0 0 0 0 1 5
where the “lifelong” flag (model performance stabilizes and o n ( B 3 3 2 2 , , 0 0 .0 .0 0 1 5
the agent starts learning) becomes true. The reported results ur
ati
3
3
2
2
,
,
0
0
.0
.0
0
0
0
1
5
are averaged across all participating clients and experiments,
o
nfi g
8,
8 ,
0 .
0
0
.
0
0
5
1
ifnotstatedotherwise.Thenumberoflocalepochswassetto C 8, 0.001
8, 0.0005
3 for all experiments with no client subsampling, i.e., all FL 50 100
Federated Round
clients participated in every round. The SGD optimizer was
used by default. It is also worth reminding that agents learn
Fig. 5. Mean EFE with two quantity drifts with each line representing one
their BNs from scratch in every experiment. possibletrainingconfiguration.Lowervaluesrepresentconfigurationsthatthe
AIFagentsfavor.
B. Agent Demonstration
It is visible how the configurations preferred by the AIF
Consider a real-life situation where clients experience agents change after the observed environmental changes. In
changes in the amount of collected data, e.g., seasonal de- the beginning, it is shown how agents slowly come to prefer
mands in shops specializing in certain types of products or configurations (256, 0.005) and (256, 0.001) (compared to
bursts of the number of service requests. For this experiment, the fixed baseline being (256, 0.01)). However, after the first
such quantity drift is modeled by increasing the number of quantity drift, this preference shifted to a larger batch size
samples drawn from the data generator every 50 epochs, of 512. This is an expected behavior as the amount of data
starting with 5,000 samples, then increasing to 10,000 and doubled,butthetimeconstraintremainedthesame.Still,after
15,000. The SLOs were set to 2 seconds for time and 97% theamountofdataincreasedagain,therewasnomorespaceto
for model performance SLO. The number of federated clients increase the batch size. Therefore, the increase of EFE across
wassetto10.ThedynamicofbothSLOfulfillmentsisshown all configurations can be observed, signaling that.
in Figure 4. As shown in Equation (1), the information gain term ac-
For the fixed “optimal” baseline, the cumulative fulfillment counts for explorative behavior and is compared to the actual
was high for both SLOs until the first quantity drift occurred observed information gain during each federated round to
at the 50th round. After this round, time SLO stopped being estimate how “surprised” the agent is. Figure 6 a shows the
consistently fulfilled, which led to a steady decline in time mean information gain across clients.
SLO fulfillment. However, looking at the AIF approach, it is Here,theprocessofagentsadaptingtotheenvironmentcan
evidentthatdespitethetimeSLObecomingmorechallenging be seen, with the observed IG decreasing as the BN becomes
tofulfill,itstillmanagestorecoverafterthequantitydrift,with more confident. However, two prominent spikes occur right
the mean time SLO fulfillment per round being consistently after the quantity drifts, illustrating the “surprised” agents
8
a) b) observed changes in the distributions, Fisher’s exact test was
Observed IG Expected IG performed on the distributions with batch sizes 256 and 512
0.15
(too low values were filtered out to focus the test on the sen-
0.4
G 0.1 sible configurations as the rest represent wrong configurations
I
n G and are irrelevant for the test). The p-value of this test was
M e a 0.05 I 0.2 reported to be 0.0293, showing the statistical significance in
theobservedchangesbetweenthedistributionofthepreferred
0 0
configurations before and after the first quantity drift.
50 100 50 100
ObservedEFEafterthesecondquantitydriftshowsthatthe
Federated Round Federated Round
agents’ behavior is dictated by the combination of available
configurations and set SLOs. To better estimate the effect
Fig.6. a)MeanobservedIGoverallclientsandrepetitions,b)Observedand
expectedinformationgainforoneclientatonerun SLOs have on the preferred configurations, a set of experi-
ments was conducted that modified the SLOs considered in
the experiments. Table II shows the SLOs chosen for each
detecting the environmental change.
experiment compared to the SLOs considered in the previous
To better illustrate the dynamics of looking for the best
experiment.
configuration after the second quantity drift, the entire history
of expected and observed IG of one client can be observed
TABLEII
(Figure 6 b). This client initially settled down for a config-
COMPARISONOFTIMEANDPERFORMANCESLOSFORDIFFERENT
uration of (512, 0,01), which worked for the agent for 68 EXPERIMENTS
epochs due to the observed IG being less or equal to the
Experiment Time SLO (s) Performance SLO (%)
agent’s expectations. However, at round 68, the agent was
Fulfillable SLOs 2 97
surprisedbecausethetimeSLOwasnotfulfilleddespiteusing Unfulfillable SLOs 0.1 99.5
the “time-proven” configuration. Despite being surprised, the Easily Fullfillable SLOs 3600 50
agent only retrained the structure of the BN (indicated by the Time Relaxed 3600 97
Performance Relaxed 2 50
abrupt change in the expected IG). It happened several times
more, but the agent preferred exploiting its knowledge. After
Figure 8 shows the mean EFE over five clients used for
round 100, the agent again started to be surprised, leading
experiments and ten repetitions.
to a change in strategy. The agent went exploring, as visible
by the increased expected information gain. These spikes are
alsoassociatedwiththeagentchoosingpreviouslyunexplored (a) Both SLOs relaxed (b) Both SLOs unfulfillable
(
c
T
o
o
o
b
h
r
r
s
i
r
s
e
p
e
r
s
o
e
v
p
x
o
e
o
a
r
d
l
n
m
y
e
d
p
n
s
e
v
l
x
e
t
i
o
p
r
i
o
l
l
o
n
t
l
h
u
r
m
e
e
s
d
t
e
r
a
)
n
a
g
t
t
c
e
e
a
o
s
n
n
n
t
d
h
fi
c
o
g
c
h
w
u
a
o
n
r
o
a
A
s
i
t
n
i
I
i
o
n
F
d
n
g
e
s
a
p
.
g
c
e
o
e
F
n
n
n
d
o
t
fi
e
r
s
g
n
i
t
u
t
n
r
l
r
e
y
s
a
a
t
t
a
b
t
i
n
o
a
c
c
n
l
h
a
e
a
n
,
(
n
c
6
r
g
e
4
o
e
,
u
b
s
n
0
e
d
t
i
.
w
n
00
1
e
t
5
0
h
e
)
4
n
e
.
Configuration
(BS,
LR)
2
5
3
6
2
5
5
1
3
6
2
4
5
1
6
2
8
2
4
,
,
6
2
,
,
,
,
,
,
,
0
0
0
0
0
0
0
.
.
0
0
.
.
0
0
0
0
.
.
.
.
.
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
5
5
5
5
5
5
5
5
5
EFE:
1
1
1
1
.
.
.
2
4
6
exploration and exploitation. 8, 0.0005 (c) Performance SLO relaxed (d) Time SLO relaxed 0.8
t
t
fi
e
io
r
n
s
A
n
t
s
s
s
q
e
p
p
u
th
r
a
a
e
e
r
n
f
a
t
e
s
t
i
r
e
e
t
r
y
t
e
u
e
d
d
p
x
r
p
b
i
f
e
f
y
o
t
r
r
,
i
t
m
h
b
th
e
e
e
e
f
n
a
o
t
e
g
r
s
x
e
e
,
p
n
i
t
e
t
t
h
s
r
e
i
i
s
a
m
s
t
p
e
e
t
o
c
h
n
s
o
r
t
s
e
n
c
i
e
d
b
o
l
c
n
e
q
r
s
u
i
i
t
t
s
a
o
i
t
c
n
e
a
r
t
d
e
i
l
t
p
y
p
o
r
o
f
e
d
i
s
t
n
r
e
e
i
t
f
n
n
s
t
t
,
(
c
c
b
a
l
o
i
n
e
e
n
f
d
n
o
fi
t
r
a
s
g
e
t
u
a
t
t
r
n
h
h
a
d
e
e
-
Configuration
(BS,
LR)
2
5
3
6
2
5
5
1
3
6
2
4
5
1
6
2
8
2
4
,
,
6
2
,
,
,
,
,
,
,
0
0
0
0
0
0
0
.
.
0
0
.
.
0
0
0
0
.
.
.
.
.
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
5
5
5
5
5
5
5
5
5
end of the experiment) into distributions over configurations. 8, 0.0005
50 100 50 100
These distributions are given in Figure 7. Federated Round Federated Round
Fig.8. MeanEFEunderdifferentSLOssetupsandtwoquantitydrifts
20
Before quantity drifts The EFE shows that having not challenging or unrealistic
After the 1st quantity drift
nts 15 After the 2nd quantity drift SLOs (Figure 8 a and b, respectively) leads to EFE being
of
Clie
10 consistent across all available configurations, with EFE being
% high when the target SLOs are unfulfillable and EFE being
5
equally low when the targets are too easy to fulfill. The
0
8, 0.0 8 0 , 0 5 0.0 3 0 2 5 , 0. 3 0 2 0 , 0 5 0. 3 0 2 0 , 1 0. 6 0 4 1 , 0. 6 0 4 0 , 0 5 0. 6 0 4 0 , 1 0. 2 0 5 0 6 5 , 0 2 . 5 0 6 0 , 0 5 0 2 . 5 0 6 0 , 1 0 2 . 5 0 6 0 , 5 0 5 . 1 0 2 1 , 0 5 . 1 0 2 0 , 0 5 0 5 . 1 0 2 0 , 1 0 5 . 1 0 2 0 , 5 0.01 s w it h u i a le tio th n e i o s th d e i r ffe is re s n o t m w eh h o e w n o ch n a e ll o e f ng t i h n e g t t w o o fu S lfi L l O l. s Th is us r , e w la h x e e n d
Configuration
performanceisrelaxed,theagentsareincentivizedtooptimize
the behavior towards time SLO, leading to agents settling for
Fig.7. Chosenconfigurationsdistributionsbeforeandafterquantitydrifts
thelargestavailablebatchsize(512)regardlessofthelearning
Despite the expected behavior, a minority of agents still rate. The situation is different regarding time-relaxed SLO –
settle for clearly non-optimal configurations. Another obser- whentheperformanceistargeted,agentstendtoexploremore
vation is the shift in the preferred configurations after the configurationsasthetaskisstillnotthatchallengingforthem,
firstquantitydrifttowardsabiggerbatchsize.Toquantifythe leading to diverse behaviors. Still, the resulting EFE heatmap
9
expresses some preference bias compared to the experiment
with two SLOs relaxed. This “uncertain” behavior could be 512, 0.01
a
g
t
o
tr
a
i
l
b
s
u
.
ted to quantity drifts not impacting the performance
n g R
ate)
5
5
5
1 2
1
1
2 5
2
2
, 6
,
,
0 ,
0
0
. 0 0
.
.
0
0
0 .0
0
0
0 1
1
5
5
EFE
1.4
ni 256, 0.005
tec
T
ti
h
n
e
g
e
t
v
h
a
e
lu
c
a
h
ti
a
o
n
n
ge
s
s
ho
i
w
n
s
th
th
e
e
e
p
n
o
v
t
i
e
ro
n
n
ti
m
al
en
o
t
f
a
A
n
I
d
F
t
a
h
g
e
en
a
t
b
s
il
i
i
n
ty
d
t
e
o
-
Size,
Le ar
2
2
5
6
5
6
6
4
6
,
4
,
,
0
,
0
0
. 0
0
.
.
0
0
0
.0
0
0
0
1
5
1
5
1.2
initiate system re-configuration with no human supervision. atc h
6
6
4
4
,
,
0
0
.0
.0
0
0
0
1
5
H
pl
o
a
w
na
e
t
v
io
e
n
r,
s
th
w
e
er
S
e
L
i
O
den
fu
ti
l
fi
fi
e
ll
d
m
u
e
p
nt
u
i
n
s
ti
n
l
o
n
t
o
p
w
e
:
rf
(
e
1
c
)
t.
“
T
u
w
ns
o
up
m
e
a
rv
in
ise
e
d
x
”
-
ati o n
( B
3 3
3
2 2
2
, ,
,
0 0
0
. . 0 0
.0
0 0
1
1 5
B
to
N
di
s
s
t
c
r
o
u
v
c
e
tu
r
r
m
e
e
le
a
a
n
r
i
n
n
i
g
n
f
g
ul
u
c
s
a
in
u
g
sal
H
r
i
e
ll
la
C
ti
l
o
i
n
m
s
b
hip
S
s
ea
w
rc
h
h
en
m
l
a
im
y
i
s
te
tr
d
ug
d
g
a
l
t
e
a C o
nfi g
ur 32
8 8
,
, ,
8
0
,
0 0
. 0
. .
0
0 0
0
.
0 0
0
0
1 5
5
1
is available [45], (2) in the absence of observations with both 8, 0.0005
20 40 60 80
SLOs fulfilled, an agent may either stuck in forever exploring Federated Round
state or stick to the strategy that guarantees at least one SLO
fulfillment and focus on exploiting sub-optimal behavior. Fig.10. MeanEFEforthescenariowithapersistentconceptdrift
Afterdissectingthebehaviouroftheagentsinasimplersce-
nario, the next section focuses on more practical experimental
performance assessments and their combination (Figure 11).
setupsandpresentsevaluationofAIFagents’behaviourinthe
For the time SLO, the best configurations are those of larger
presence of concept drifts and resources heterogeneity.
batch sizes, as expected. For performance, Optuna focused
on a set of configurations from batch size 8 through 64 and
VI. RESULTS learningratesof0.001through0.01.Thecombinationofboth
A. Hyperparameter Tuning Comparison SLOs lands in the range of batch sizes (32, 64, 256) and
learningratesof(0.005,0.01),with(64,0.005)beingthemost
To demonstrate both the validity and advantages of the
preferred configuration.
proposed AIF agents, we compare them against Optuna, a
state-of-the-art framework for hyperparameter optimization in
(a) Time (b) Performance (c) Total
ML. Optuna serves as a strong baseline due to its efficiency
Total Score
in finding optimal configurations. However, unlike Optuna, 0.01
0.6
which focuses on static optimization, AIF agents offer con-
tinuous adaptability, making them better suited for dynamic at e 0.005 0.5
R
and heterogeneous FL environments. n g 0.4
1) Concept Drift: The first experiment included a concept e ar
ni
0.001 0.3
drift, which was present from the beginning of the training, L
0.2
and both approaches needed to find the best configuration to
0.0005
meet the time SLO of 3s and performance SLO of 85%. The 0.1
SLOS fulfillment is shown in Figure 9. 8 32 64 256512 8 32 64256512 8 32 64 256512
Batch Size Batch Size Batch Size
Performance SLO Time SLO Fig.11. Optunapreferredconfigurationsattheend(last10rounds)oftraining
Approach: aif optuna
e nt 1 1 Comparing the two approaches, it is seen that both AIF
m
ulfill agents and Optuna converge to similar configurations in this
F scenario.
ati v e 0.5 0.5 2) Concept and Quantity Drifts after Round 50: Next
ul evaluation scenario introduced two drifts at the same time,
m
C u simulating a change in both quantity and distribution of data
0 0
20 40 60 80 20 40 60 80 received by the FL clients.
Federated Round Federated Round
Thescenariowassetasfollows.First,10,000sampleswere
used per FL training round with no concept drift; after round
Fig.9. MeancumulativeSLOsfulfillmentforthescenariowithapersistent
conceptdrift 50 – a slight drift appeared and 20,000 samples started to be
drawnfromthedatagenerator.TheresultingSLOsfulfillment
It is seen that both Optuna and AIF have similar time SLO is presented in Figure 12 for time SLO – 4 seconds and
fulfillment, with performance SLO being slightly better for performance SLO – 90%.
AIF after training round 60. It is seen that adding concept drift to data leads to an im-
To assess what configurations were favored by both ap- mediatedeclineinperformanceSLOaspreviouslyestablished
proaches, EFE (Figure 10) and normalized objectives can be configurations and the trained model do not account for that.
examined for AIF and Optuna, respectively. However, AIF agents are able to return to a relatively high
For AIF, the best configuration based on EFE is (64, performanceSLOfulfillmentinafasterandmorereliableway
0.005). For Optuna, the result is based on separate time and than Optuna.
10
The first experiment featured a smaller neural network (64
Performance SLO Time SLO
Approach: aif optuna and 32 units in 2 layers). The comparison of SLO fulfillment
nt 1 1
e across various devices is shown in Figure 14, and mean EFE
m
ulfill is shown in Figure 15.
F
v e 0.5 0.5
ati
Performance SLO Time SLO
m
ul
Device: Orin NX RPI 5 Xavier NX
C
u
0 0 m e
nt 1 1
Fe
5
d
0
erated R
1
o
0
u
0
nd Fe
5
d
0
erated R
1
o
0
u
0
nd
ulfill
F
v e 0.5 0.5
Fig.12. MeancumulativeSLOsfulfillmentforthescenariowithconceptand ati
quantitydriftsafterround50 m
ul
u
C
0 0
20 40 60 20 40 60
Federated Round Federated Round
Based on EFE shown in Figure 13, it is seen that after the
drifts appeared after round 50, a set of the previously favored
Fig.14. MeanSLOsfulfillmentfordifferentdevices.TimeSLO:2seconds,
configurations(256,0.005)and(64,0.005)graduallylosttheir
performanceSLO:97%
importance and agents focused mostly on (256, 0.001).
e) (a) Orin NX (b) Xavier NX (c) RPI 5
at
o nfi g ur
ati o n ( B
atc h
Size, Le ar ni
n g
R
ate)
2
5
3
6
2 2
5
5
5
1
3 3
6
6
2
5
2
4
5 5
1
1
6
2
8
3
6
2 2
4
4
5
1
,
,
6 6
2
2
,
,
,
2
4
, ,
,
,
6
2
, ,
,
,
8 0
0
0
0
,
,
,
,
, 0
0 0
0
0
.
.
0 0
0
0
.
.
0
0
0
0
.
0
0
. .
.
.
0
0
. .
.
.
0 0
0 0
0
0
0
0
0 0
0
0
0
0
.
.
.
.
. 0
0
0
0 0
0
0
0
0
0
0
0 0
0
0
0
0
0
5
1
1
1 5
1
5
1
1
5
5
1 5
1
5
1
5
5
EFE
0 1 1
1
1
. .
.
.
9 1
2
3
C o nfi g
ur ati o n
( B
at c h Si z e,
L e
ar
ni
n g R
2
5
3
6
2
5
5
1
8
3
6
2
4
5
1
6
2
8
,
2
4
,
,
6
2
,
,
,
,
,
,
,
0
0
0 0
0
0
.
0
0
.
.
0
0
.
.
0
0
0 0
0
.
.
.
.
.
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
5
5
5
5
5
5
5
5
5
5
20 40 60 20 40 60 20 40 60
EFE
1 1
1 1
1
1
.
. .
.
.
1
2 3
4
5
C 8, 0.001 Federated Round Federated Round Federated Round
8, 0.0005
50 100 Fig.15. MeanEFEfordifferentdevices
Federated Round
Fig. 13. Mean EFE for the scenario with concept and quantity drifts after It is shown that all devices manage to fulfill the set SLOs.
round50 It is also worth noting that Raspberry Pi 5 managed to fulfill
performance SLO slightly faster than devices that used GPU
These experiments illustrate that AIF agents align with but occasionally struggled to maintain flawless time SLO
commonly used hyperparameter optimization methods while fulfillment.WhenlookingatmeanEFE,itisclearthatinorder
providingadaptabilitywherenecessary.Comparedtothestate- to better utilize its cores (as there is no GPU), Raspberry Pi
of-the-art Optuna framework adapted to the lifelong heteroge- can successfully employ a vast range of configurations, while
neous FL scenario, AIF agents are able to fully recover from devices with GPU choose bigger batch sizes to better utilize
the concept drift after 43 rounds, while Optuna fails to return their parallelization capabilities.
to its best performance even after 100 rounds after the drift The next experiment was conducted with the same set of
introduction. devices but with a wider neural network (5120 and 512 units
3) Device Heterogeneity: The next experiment focused on compared to 64 and 32 used in the previous experiment). The
inspecting the ability of AIF agents to adapt to the resources time SLO was adjusted to 15 seconds. The resulting SLOs
availabletotheagentslocatedattheedgedevicesandaligning fulfillment is shown in Figure 16 and mean EFE is shown
them with the SLOs set. in Figure 17. The performance SLO fulfillment at the final
For the experiments, three edge devices (Raspberry Pi 5, FL round was 96.9% for Orin NX and 98.7% for RPI 5 and
NvidiaOrinNX,andNvidiaXavierNX)wereusedasseparate Xavier NX, while the time SLO was 99% for Nvidia devices
federated clients and were tasked to participate in the FL for and 87.8% for RPI 5.
75 epochs, while a Raspberry Pi 4 device served as the FL Here the change in the model architecture impacted the
orchestrator.Nodatadriftswereintroducedinthisexperiment. optimalconfigurationchoiceforRaspberryPi5.Asseenfrom
Duetothedifferencesinresources,itwasexpectedthatagents the mean EFE, the batch size had to be increased to 256 to
would prefer different configurations under the same target fit into the time and performance SLOs, while Nvidia devices
SLOs. In addition to different available resources, the size utilized a more comprehensive range of configurations.
of the neural networks was also changed, so to change the Results presented in this section show how AIF agents
utilization of available resources on the device. display adaptive behaviour in the presence of changing data
11
Performance SLO Time SLO
ACKNOWLEDGMENTS
nt 1 Device: Orin NX 1 RPI 5 Xavier NX We thank Alexander Knoll for providing us with the hard-
m e ware infrastructure and Pantelis Frangoudis for his valuable
ulfill
suggestions and feedback.
F
v e 0.5 0.5 The work of Anastasiya Danilenka was conducted during
ati the research visit funded by the Warsaw University of Tech-
ul
m nology within the Excellence Initiative: Research University
u
C 0 0 (IDUB) programme. The work of Maria Ganzha was co-
20 40 60 20 40 60
Federated Round Federated Round funded by the Centre for Priority Research Area Artificial
IntelligenceandRoboticsofWarsawUniversityofTechnology
Fig.16. MeanSLOsfulfillmentfordifferentdeviceswithawidernetwork. within the Excellence Initiative: Research University (IDUB)
TimeSLO:15seconds,performanceSLO:97% programme. Further, this work was supported in part by
EU Horizon under Grants 101135576 (INTEND), 101079214
e) (a) Orin NX (b) Xavier NX (c) RPI 5 (AIoTwin), and 101070186 (TEADAL). Ayuda CNS2023-
at
g R 512, 0.005 EFE 144359financiadaporMICIU/AEI/10.13039/501100011033y
n
ni512, 0.0005 1.6 por la Unio´n Europea NextGenerationEU/PRTR.
e
ar
256, 0.005
L
Si z
e, 25
6
6
4
,
,
0
0
.0
.0
0
0
0
5
5
1.4 REFERENCES
h
at c 64, 0.0005 [1] S. Dustdar, V. Casamayor Pujol, and P. K. Donta, “On Distributed
ur
ati o n
( B
3
3
2
8
2
,
,
,
0
0
0
.0
.
.
0
0
0
0
0
0
5
5
5
1
1.2
[2]
C
D
T.
o
a
R
m
ta
a
p
u
E
u
s
t
n
c
i
g
n
h
i
g
,
n
W
e
C
e
.
o
r
H
n
in
t
u
i
g
n
m
,
u
v
m
u
o
m
e
l
r
.
,
S
3
C
y
5
.
s
,
t
S
e
n
t
m
o
ip
.
s
p
,
4
”
e
,
l
I
,
p
E
S
p
E
.
.
E
V
40
a
T
s
9
r
i
a
2
lj
n
–
e
s
4
v
a
1
i
c
c
0
t
,
5
io
C
,
n
.
A
s
E
p
o
l
r
v
n
.
e
2
K
z
0
i
n
o
2
o
,
3
w
S
.
l
.
e
D
dg
u
e
st
a
d
n
ar
d
,
g and K. Kro¨sl, “Towards a platform for smart city-scale cognitive
C o
nfi 8, 0.0005
20 40 60 20 40 60 20 40 60 assistance applications,” in 2021 IEEE Conference on Virtual Reality
and 3D User Interfaces Abstracts and Workshops (VRW), 2021, pp.
Federated Round Federated Round Federated Round
330–335.
[3] M.Aboualola,K.Abualsaud,T.Khattab,N.Zorba,andH.S.Hassanein,
Fig.17. MeanEFEfordifferentdevicesandwidernetwork
“Edgetechnologiesfordisastermanagement:Asurveyofsocialmedia
andartificialintelligenceintegration,”IEEEAccess,vol.11,pp.73782–
73802,2023.
as well as adaptation to the local resources to ensure the [4] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya,
fulfillment of set SLOs. “Edge intelligence: The confluence of edge computing and artificial
intelligence,”IEEEInternetofThingsJournal,vol.7,no.8,pp.7457–
7469,2020.
VII. CONCLUSION [5] A.Furtuanpey,P.Raith,andS.Dustdar,“Frankensplit:Efficientneural
feature compression with shallow variational bottleneck injection for
This work presented AIF agents that are able to adaptively mobileedgecomputing,”IEEETransactionsonMobileComputing,pp.
change their behavior in response to dynamic environments. 1–17,2024.
[6] B.Sedlak,V.C.Pujol,P.K.Donta,andS.Dustdar,“Equilibriuminthe
We evaluated the proposed AIF agents in lifelong heteroge-
Computing Continuum through Active Inference,” Future Generation
neous FL, utilizing a set of both dynamic data and diverse ComputerSystem,2024.
devices.WeshowedthatAIFagentsareabletofulfillcompet- [7] B. Sedlak, V. C. Pujol, A. Morichetta, P. K. Donta, and S. Dustdar,
“Adaptive Stream Processing on Edge Devices through Active Infer-
ingSLOsandunfoldedthebehaviorsofagents.Wecompared
ence,”2024,arXiv:2409.17937[cs].
AIF agents to the hyperparameter-tuning framework Optuna [8] S. S. Shubha and H. Shen, “AdaInf: Data Drift Adaptive Scheduling
adjusted to lifelong learning and showcased how the adaptive forAccurateandSLO-guaranteedMultiple-ModelInferenceServingat
EdgeServers,”inProceedingsoftheACMSIGCOMM2023Conference,
nature of AIF agents allows for faster performance recovery
ser. ACM SIGCOMM ’23. New York, NY, USA: Association for
in the presence of data drift. ComputingMachinery,Sep.2023,pp.473–485.
Future work can further expand the usage of the active [9] M.KundrooandT.Kim,“Federatedlearningwithhyper-parameteropti-
mization,”JournalofKingSaudUniversity-ComputerandInformation
inference framework to orchestrate distributed learning sys-
Sciences,vol.35,no.9,p.101740,2023.
tems, for instance, by fulfilling system-level SLOs, such as [10] A. L. Sua´rez-Cetrulo, D. Quintana, and A. Cervantes, “A survey on
fairnessofparticipationorglobalmodelperformance.Another machine learning for recurring concept drifting data streams,” Expert
SystemswithApplications,vol.213,p.118934,2023.
line of research can target the scalability of the proposed
[11] G.Yang,X.Chen,T.Zhang,S.Wang,andY.Yang,“Animpactstudy
framework, as the usage of BNs can potentially introduce of concept drift in federated learning,” in 2023 IEEE International
computationalbottleneckasthenumberofconsideredvertices ConferenceonDataMining(ICDM),2023,pp.1457–1462.
[12] E. Jothimurugesan, K. Hsieh, J. Wang, G. Joshi, and P. B. Gibbons,
and their cardinalities grow. Enhancements of the current
“Federated learning under distributed concept drift,” in Proceedings
method can improve the ability of the agents to find causal of The 26th International Conference on Artificial Intelligence and
dependencies in limited data, making them more robust, tar- Statistics,ser.ProceedingsofMachineLearningResearch,F.Ruiz,J.Dy,
andJ.-W.vandeMeent,Eds.,vol.206. PMLR,25–27Apr2023,pp.
getingthelimitationsofthediscreteBN,introducingtemporal
5834–5853.
dependencies to capture the environmental dynamics more [13] ——, “Federated learning under distributed concept drift,” in Interna-
precisely, and providing more nuanced SLOs specifications to tionalConferenceonArtificialIntelligenceandStatistics. PMLR,2023,
pp.5834–5853.
enable tracking SLOs in a range.
[14] L. Rahimli, F. M. Awaysheh, S. Al Zubi, and S. Alawadi, “Federated
learningdriftdetection:Anempiricalstudyontheimpactofconceptand
datadrift,”in20242ndInternationalConferenceonFederatedLearning
TechnologiesandApplications(FLTA),2024,pp.241–250.
12
[15] Z. Lu, H. Pan, Y. Dai, X. Si, and Y. Zhang, “Federated learning with in Privacy Regulation and Protection in Machine Learning, 2024.
non-iiddata:Asurvey,”IEEEInternetofThingsJournal,vol.11,no.11, [Online].Available:https://openreview.net/forum?id=J0IwrkICp8
pp.19188–19209,2024. [30] J.-M. Park, W.-J. Jang, T.-H. Oh, and S.-H. Lee, “Overcoming client
[16] Z.Jiang,W.Wang,B.Li,andQ.Yang,“Towardsefficientsynchronous data deficiency in federated learning by exploiting unlabeled data on
federated training: A survey on system optimization strategies,” IEEE theserver,”IEEEAccess,vol.12,pp.130007–130021,2024.
TransactionsonBigData,vol.9,no.2,pp.437–454,2022. [31] X. Li, Z. Wu, W. Zhang, Y. Zhu, R.-H. Li, and G. Wang, “Fedgta:
[17] Q. Li, Z. Gao, Y. Sun, Y. Wang, R. Wang, and H. Zhu, “An efficient Topology-aware averaging for federated graph learning,” Proc. VLDB
asynchronous federated learning protocol for edge devices,” IEEE In- Endow., vol. 17, no. 1, p. 41–50, Sep. 2023. [Online]. Available:
ternetofThingsJournal,vol.11,no.17,pp.28798–28808,2024. https://doi.org/10.14778/3617838.3617842
[18] H. Zhu and Y. Jin, “Multi-objective evolutionary federated learning,” [32] J. Parra-Ullauri, X. Zhang, A. Bravalheri, R. Nejabati, and
IEEE transactions on neural networks and learning systems, vol. 31, D. Simeonidou, “Federated hyperparameter optimisation with flower
no.4,pp.1310–1322,2019. and optuna,” in Proceedings of the 38th ACM/SIGAPP Symposium on
[19] G. Paragliola, “Evaluation of the trade-off between performance and AppliedComputing,ser.SAC’23. NewYork,NY,USA:Association
communicationcostsinfederatedlearningscenario,”FutureGeneration for Computing Machinery, 2023, p. 1209–1216. [Online]. Available:
ComputerSystems,vol.136,062022. https://doi.org/10.1145/3555776.3577847
[20] M.Badar,S.Sikdar,W.Nejdl,andM.Fisichella,“Fairtrade:Achieving [33] S. Nastic, A. Morichetta, T. Pusztai, S. Dustdar, X. Ding, D. Vij, and
pareto-optimal trade-offs between balanced accuracy and fairness in Y. Xiong, “Sloc: Service level objectives for next generation cloud
federated learning,” Proceedings of the AAAI Conference on Artificial computing,”IEEEInternetComputing,vol.24,no.3,pp.39–50,2020.
Intelligence,vol.38,no.10,pp.10962–10970,Mar.2024. [34] V. Casamayor Pujol, B. Sedlak, Y. Xu, P. K. Donta, and S. Dustdar,
[21] A. Lackinger, P. A. Frangoudis, I. Cˇilic´, A. Furutanpey, I. Murturi, “Invitedpaper:Deepslosforthecomputingcontinuum,”inProceedings
I. P. Zˇarko, and S. Dustdar, “Inference load-aware orchestration for ofthe2024WorkshoponAdvancedTools,ProgrammingLanguages,and
hierarchicalfederatedlearning,”in2024IEEE49thConferenceonLocal PLatformsforImplementingandEvaluatingAlgorithmsforDistributed
ComputerNetworks(LCN),2024,pp.1–9. Systems, ser. ApPLIED’24. New York, NY, USA: Association for
[22] S. Yan, P. Zhang, S. Huang, H. Sun, Y. Zhang, and A. Tolba, “Node ComputingMachinery,2024,p.1–10.
selectionalgorithmforfederatedlearningbasedondeepreinforcement [35] Z. Zhang, Y. Zhao, and J. Liu, “Octopus: SLO-Aware Progressive
learning for edge computing in iot,” Electronics, vol. 12, p. 2478, 05 Inference Serving via Deep Reinforcement Learning in Multi-tenant
2023. EdgeCluster,”inService-OrientedComputing,Cham,2023.
[23] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian [36] K.Friston,J.Kilner,andL.Harrison,“Afreeenergyprincipleforthe
optimization of machine learning algorithms,” in Advances in brain,”Journalofphysiology-Paris,vol.100,no.1-3,pp.70–87,2006.
Neural Information Processing Systems, F. Pereira, C. Burges, [37] K. J. Friston, M. J. Ramstead, A. B. Kiefer, A. Tschantz, C. L.
L. Bottou, and K. Weinberger, Eds., vol. 25. Curran Associates, Buckley,M.Albarracin,R.J.Pitliya,C.Heins,B.Klein,B.Millidge,
Inc., 2012. [Online]. Available: https://proceedings.neurips.cc/paper D. A. Sakthivadivel, T. S. C. Smithe, M. Koudahl, S. E. Tremblay,
files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf C.Petersen,K.Fung,J.G.Fox,S.Swanson,D.Mapes,andG.Rene´,
[24] Z.Dai,B.K.H.Low,andP.Jaillet,“Federatedbayesianoptimization “Designingecosystemsofintelligencefromfirstprinciples,”Collective
viathompsonsampling,”inAdvancesinNeuralInformationProcessing Intelligence, vol. 3, no. 1, p. 26339137231222481, 2024. [Online].
Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and Available:https://doi.org/10.1177/26339137231222481
H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 9687– [38] T.Parr,G.Pezzulo,andK.J.Friston,Activeinference:thefreeenergy
9699. [Online]. Available: https://proceedings.neurips.cc/paper files/ principleinmind,brain,andbehavior. MITPress,2022.
paper/2020/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf [39] G. Oliver, P. Lanillos, and G. Cheng, “An Empirical Study of Active
[25] J.Bergstra,D.Yamins,andD.Cox,“Makingascienceofmodelsearch: InferenceonaHumanoidRobot,”IEEETransactionsonCognitiveand
Hyperparameter optimization in hundreds of dimensions for vision DevelopmentalSystems,vol.14,no.2,pp.462–471,Jun.2022.
architectures,” in Proceedings of the 30th International Conference on [40] D.KollerandN.Friedman,ProbabilisticGraphicalModels:Principles
Machine Learning, ser. Proceedings of Machine Learning Research, andTechniques-AdaptiveComputationandMachineLearning. The
S.DasguptaandD.McAllester,Eds.,vol.28,no.1. Atlanta,Georgia, MITPress,2009.
USA: PMLR, 17–19 Jun 2013, pp. 115–123. [Online]. Available: [41] C.Heins,B.Millidge,D.Demekas,B.Klein,K.Friston,I.D.Couzin,
https://proceedings.mlr.press/v28/bergstra13.html and A. Tschantz, “pymdp: A python library for active inference in
[26] T.Akiba,S.Sano,T.Yanase,T.Ohta,andM.Koyama,“Optuna:Anext- discretestatespaces,”JournalofOpenSourceSoftware,vol.7,no.73,
generation hyperparameter optimization framework,” in Proceedings p.4098,2022.
of the 25th ACM SIGKDD International Conference on Knowledge [42] D. J. Beutel, T. Topal, A. Mathur, X. Qiu, J. Fernandez-Marques,
Discovery & Data Mining, ser. KDD ’19. New York, NY, USA: Y.Gao,L.Sani,H.L.Kwing,T.Parcollet,P.P.d.Gusma˜o,andN.D.
Association for Computing Machinery, 2019, p. 2623–2631. [Online]. Lane,“Flower:Afriendlyfederatedlearningresearchframework,”arXiv
Available:https://doi.org/10.1145/3292500.3330701 preprintarXiv:2007.14390,2020.
[27] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and [43] Ankur Ankan and Abinash Panda, “pgmpy: Probabilistic Graphical
I.Stoica,“Tune:Aresearchplatformfordistributedmodelselectionand Models using Python,” in Proceedings of the 14th Python in Science
training,”2018.[Online].Available:https://arxiv.org/abs/1807.05118 Conference,KathrynHuffandJamesBergstra,Eds.,2015,pp.6–11.
[28] M. Chadha, P. Khera, J. Gu, O. Abboud, and M. Gerndt, “Training [44] J. Montiel, M. Halford, S. M. Mastelini, G. Bolmier, R. Sourty,
heterogeneous client models using knowledge distillation in serverless R. Vaysse, A. Zouitine, H. M. Gomes, J. Read, T. Abdessalem, and
federated learning,” in Proceedings of the 39th ACM/SIGAPP A. Bifet, “River: machine learning for streaming data in python,” J.
Symposium on Applied Computing, ser. SAC ’24. New York, NY, Mach.Learn.Res.,vol.22,no.1,jan2021.
USA: Association for Computing Machinery, 2024, p. 997–1006. [45] N.K.Kitson,A.C.Constantinou,Z.Guo,Y.Liu,andK.Chobtham,“A
[Online].Available:https://doi.org/10.1145/3605098.3636015 survey of Bayesian Network structure learning,” Artificial Intelligence
[29] M. Krouka, A. Koskela, and T. Kulkarni, “Communication-efficient Review,vol.56,no.8,pp.8721–8814,Aug.2023.
differentiallyprivatefederatedlearningusingsecond-orderinformation,”

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Adaptive Active Inference Agents for Heterogeneous and Lifelong Federated Learning"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
