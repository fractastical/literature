=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference
Citation Key: prakki2024demonstrating
Authors: Rithvik Prakki

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Abstract: Activeinferenceprovidesapowerfulmathematicalframeworkforunderstandinghowagents—biological
or artificial—interact with their environments, enabling continual adaptation and decision-making. It
combines the principles of Bayesian inference and free energy minimization to model the process of per-
ception, action, and learning in uncertain and dynamic contexts. Unlike reinforcement learning, active
inference integrates both exploration and exploitation seamlessly, driven by a unified objective to m...

Key Terms: discrete, energy, environments, artificial, capabilities, continual, demonstrating, learning, application, inference

=== FULL PAPER TEXT ===

Demonstrating the Continual Learning Capabilities
and Practical Application of Discrete-Time Active
Inference
Rithvik Prakki
University of North Carolina at Chapel Hill
rprakki@unc.edu
Abstract
Activeinferenceprovidesapowerfulmathematicalframeworkforunderstandinghowagents—biological
or artificial—interact with their environments, enabling continual adaptation and decision-making. It
combines the principles of Bayesian inference and free energy minimization to model the process of per-
ception, action, and learning in uncertain and dynamic contexts. Unlike reinforcement learning, active
inference integrates both exploration and exploitation seamlessly, driven by a unified objective to mini-
mizeexpectedfreeenergy. Inthispaper,wepresentacontinuallearningframeworkforagentsoperating
in discrete time environments, using active inference as the foundation. We derive the core mathemat-
ical formulations of variational and expected free energy and apply these principles to the design of a
self-learning research agent. This agent continually updates its beliefs and adapts its actions based on
new data, without manual intervention. Through experiments in dynamically changing environments,
we demonstrate the agent’s ability to relearn and refine its internal models efficiently, making it highly
suitable for complex and volatile domains such as quantitative finance and healthcare. We conclude by
discussing how the proposed framework generalizes to other systems and domains, positioning active
inference as a robust and flexible approach for adaptive artificial intelligence.
4202
peS
03
]IA.sc[
1v04200.0142:viXra
1 Introduction
The Free Energy Principle (FEP), proposed by Friston, provides a unifying computational framework that
integrates learning, perception, action, and decision-making. This principle posits that biological systems
(or artificial agents) maintain themselves in their characteristic states by minimizing the difference between
the predictions of their internal model and their actual sensory data, a quantity known as free energy. In
essence,FEPsuggeststhatlivingsystemsaredrivenbytheneedtoreducesurpriseintheirinteractionswith
the environment, formalized through the minimization of variational free energy.
1.1 Active Inference and Free Energy Principle
AtthecoreofActive Inference (AIF),whichoperationalizestheFreeEnergyPrinciple,aretwokeyobjective
functions:
• Variational Free Energy (VFE): This function quantifies the fitness of an agent’s internal model
concerningthesensoryobservationsitreceivesfromitsenvironmentbymappinglatent(hidden)states
to sensory outcomes. The agent minimizes this free energy to maintain coherence between its beliefs
about the world and incoming sensory evidence.
• Expected Free Energy (EFE): This function governs the agent’s policy selection by combining
extrinsic value (goal-oriented behavior, aligned with the agent’s prior preferences) and epistemic value
(exploration-driven behavior to reduce uncertainty in the agent’s model of the world).
Discrete-TimeActiveInferenceisframedinthecontextofPartiallyObservableMarkovDecisionProcesses
(POMDPs), whereagentsusesensoryandproprioceptiveinformationtoformprobabilisticbeliefsaboutthe
hidden (latent) states of the world.
1.2 Mathematical Derivation of Variational Free Energy (VFE)
Variational Free Energy (VFE) is a quantity that measures the dissimilarity between an agent’s internal
model of the world and the real-world sensory data it receives. It is used to approximate Bayesian inference
inscenarioswherecalculatingexactposteriorsiscomputationallyintractable. ToderiveVFE,webeginwith
Bayes’ rule:
P(o|s)P(s)
P(s|o)= (1)
P(o)
Here,P(s|o)istheposteriordistributionoverhiddenstatessgivenobservationso,P(o|s)isthelikelihood
(i.e.,howlikelytheobservationsaregiventhehiddenstates),P(s)isthepriordistributionoverhiddenstates,
and P(o) is the marginal likelihood, also known as the evidence.
In active inference, the agent cannot compute this posterior P(s|o) directly, as the marginal likelihood
P(o) is intractable due to the high-dimensional nature of real-world data. Instead, the agent approximates
the posterior with a simpler distribution Q(s), known as the variational distribution, which the agent iter-
atively improves. The agent minimizes the difference between the true posterior P(s|o) and the variational
approximation Q(s) using the Kullback-Leibler (KL) divergence:
(cid:90) Q(s)
D [Q(s)∥P(s|o)]= Q(s)ln ds (2)
KL P(s|o)
Since the true posterior P(s|o) is unknown, we aim to minimize this divergence indirectly by minimizing
thefree energy F,whichisanupperboundonthenegativelog-evidence, −lnP(o). Toexpressthisinterms
of known quantities, we substitute Bayes’ rule into the expression for the KL divergence:
(cid:90) Q(s)
D [Q(s)∥P(s|o)]= Q(s)ln ds (3)
KL P(o|s)P(s)/P(o)
This can be split into three terms:
2
(cid:90) Q(s) (cid:90)
D [Q(s)∥P(s|o)]= Q(s)ln ds− Q(s)lnP(o|s)ds+lnP(o) (4)
KL P(s)
Thus, the KL divergence becomes:
D [Q(s)∥P(s|o)]=D [Q(s)∥P(s)]−E [lnP(o|s)]+lnP(o) (5)
KL KL Q(s)
In this expression, D [Q(s)∥P(s)] is the KL divergence between the approximate posterior Q(s) and
KL
the prior P(s), which we call the complexity term. The second term E [lnP(o|s)] is the expected
Q(s)
log-likelihood of the observations under the approximate posterior, known as the accuracy term.
The final term, lnP(o), is the log-evidence (also known as the marginal likelihood), which does not
depend on Q(s) and is therefore a constant with respect to the minimization of free energy.
Since lnP(o) is independent of Q(s), minimizing the KL divergence D [Q(s)∥P(s|o)] is equivalent to
KL
minimizing the variational free energy F, which we define as:
F[Q(s)]=D [Q(s)∥P(s)]−E [lnP(o|s)] (6)
KL Q(s)
This free energy consists of two competing terms:
• Complexity: D [Q(s)∥P(s)], which penalizes divergence between the approximate posterior and
KL
the prior. Lowering this term encourages simplicity in the model, favoring posteriors Q(s) that are
close to the prior P(s).
• Accuracy: E [lnP(o|s)], which measures how well the model’s predictions P(o|s) fit the observed
Q(s)
data o. Maximizing this term ensures that the model is accurately predicting sensory observations.
The free energy F can be minimized by iteratively updating the approximate posterior Q(s), driving the
agent to strike a balance between complexity (favoring simpler models) and accuracy (favoring models that
predict observations well).
1.2.1 Expected Free Energy (EFE)
While Variational Free Energy (VFE) focuses on inferring hidden states based on current observations,
Expected Free Energy (EFE) extends this principle into the future by evaluating potential actions (or
policies) that an agent can take. Just as VFE is used to update the agent’s beliefs about the hidden states
s given observations o, EFE evaluates the likely outcomes of different policies π, guiding the agent to select
actions that minimize future free energy.
Similar to VFE, the derivation of EFE follows from Bayes’ rule. However, instead of inferring hidden
states based solely on observations, we now consider how policies π influence both the hidden states and the
observations. The posterior over hidden states and policies is given by:
P(o|s,π)P(s,π)
P(s|o,π)= (7)
P(o,π)
Here, P(s|o,π) is the posterior over hidden states given observations and policies, P(o|s,π) is the likeli-
hoodoftheobservationsconditionedonboththehiddenstatesandthepolicy, P(s,π)isthejointpriorover
states and policies, and P(o,π) is the marginal likelihood of observations and policies.
Fromthisstartingpoint,wecanderivetheexpectedfreeenergy,G ,followingasimilarprocessasweused
π
forVFE.ThegoalofEFEistoselectpoliciesthatminimizethefreeenergyexpectedoverfutureobservations,
balancing the agent’s desire to gather information (epistemic value) and achieve its goals (extrinsic value).
1.3 Learning in Active Inference: The Dirichlet-Categorical Model
In Active Inference, learning occurs by updating the generative model’s parameters based on new observa-
tions. This process is typically modeled with a Dirichlet-Categorical framework, where Bayesian inference
is performed using a categorical distribution as the likelihood and a Dirichlet distribution as the prior.
3
A Dirichlet distribution, denoted by Dir(θ|α), is defined over probability vectors and serves as the
conjugate prior for the categorical distribution. This allows for sequential updating of beliefs as new data is
observed. The Dirichlet distribution is expressed as:
(cid:16) (cid:17)
Γ
(cid:80)K
α K
p(θ|α)= k=1 k (cid:89) θαk−1 (8)
(cid:81)K
Γ(α )
k
k=1 k k=1
Where α=(α ,α ,...,α ) are concentration parameters, and Γ(·) is the gamma function. The param-
1 2 K
etersα representpriorcountsthatencodetheagent’sconfidenceineachpossibleoutcomebeforeobserving
k
new data.
As the agent receives new observations, these concentration parameters are updated through a process
analogous to ”counting” the co-occurrences of certain states and observations over time. The posterior
distribution over parameters θ, given new observations x, becomes:
p(θ|x,α)∝Dir(θ|α+x) (9)
This form of learning enables the agent to accumulate evidence over time, updating its beliefs about
state-outcome mappings in a flexible and adaptive manner.
1.4 Learning the A Matrix
A crucial part of the generative model in Active Inference is the A matrix, which encodes the likelihood
mapping from hidden states s to observations o. Learning the A matrix is central to the agent’s ability to
adapt to its environment, as it allows the agent to refine its understanding of how hidden states generate
sensory data.
The update rule for the A matrix is expressed as:
(cid:88)
a =ω·a +η· o ⊗s (10)
t+1 t τ τ
τ
Where:
• a are the concentration parameters for the A matrix at time t,
t
• o is the observation at time τ,
τ
• s is the inferred hidden state at time τ,
τ
• η is the learning rate that controls the adaptation speed, and
• ω is the forgetting rate that controls the extent to which past observations are discounted.
This Hebbian-like learning rule updates the A matrix by strengthening associations between states and
observations that co-occur. For example, if the agent observes o = [1,0,0]T while in state s = [1,0]T, the
association between state 1 and observation 1 is reinforced. This allows the agent to build stronger beliefs
about the causal structure of the environment.
In addition, the process of policy selection relies on the minimization of Expected Free Energy (EFE),
which includes a novelty-seeking term that drives exploration. This term quantifies the expected change
in beliefs as a result of receiving a new observation. The EFE in this context is:
G =D [q(o |π)∥p(o )]+E [H[p(o |s )]]−E [D [q(A|o ,s )∥q(A)]] (11)
π KL τ τ q(sτ|π) τ τ p(oτ|sτ)q(sτ|π) KL τ τ
The third term drives the agent to seek novel observations that maximize the difference between prior
and posterior beliefs about the mappings in the A matrix. By doing so, the agent can actively explore and
refine its internal model, improving its understanding of how states generate observations.
4
1.5 Application to Self-Learning Agents
In this paper, we extend this framework to the design of self-learning agents capable of continually
updating their generative models to adapt to changing environments. Specifically, we focus on the learning
of the A matrix, as it is essential for the agent’s ability to infer the dynamics of its environment. By
using Active Inference and Dirichlet-Categorical models, we propose a mechanism for integrating continual
learning into agentic systems, allowing agents to optimize their performance over time while responding to
novel observations.
Thisabilitytolearncontinuallyovertimeensuresthattheagentcaninteractwithdynamicenvironments
without requiring constant updates or interventions by a human operator. This capability is especially
relevant in domains such as quantitative finance, where strategies can become obsolete over time due to
market fluctuations, or in research, where resources constantly evolve in their relevance. In these scenarios,
ActiveInferenceprovidesanabstractionlayerthatcontinuouslytunestheagent’sperformance,thusadapting
to new challenges in real-time.
2 Active Inference as an Agentic Learning Mechanism
2.1 Motivating the Problem
For every aspect of an agentic workflow, there are elements that need to be optimized over time. Consider
an agent that operates within a dynamic environment, where the relationships between states, observations,
and actions continuously evolve. If the system lacks the ability to adapt to these changes, its performance
will degrade. Traditional methods require manual intervention, continual updates, or predefined structures
that can become obsolete.
However, withActiveInference, bysimply defining the environmentandcreatingpathwaysfortheagent
to act on and receive observations from the environment, we can build agents that are self-sustaining and
highly adaptable over time. Active Inference agents can automatically learn new structures and processes
as needed without the need for external updates, making them resilient to change.
2.2 Developing the Framework
This section develops the logic behind the active inference framework used for implementing self-learning
within any agentic workflow. While the following discussion focuses on an example research agent, the
framework can be generalized to other systems and domains.
2.2.1 Definitions
Operating in discrete time, a Partially Observable Markov Decision Process (POMDP) models states, ob-
servations, policies, and all inputs and outputs of Free Energy. In this framework, the A matrix represents
the likelihood mapping from hidden states to observations. The state factors and observation modalities are
learned over time rather than pre-specified, allowing the agent to adapt as new data becomes available.
For example, in a research process, the state factors may represent different research methods, while
the observation modalities represent the usefulness of a research output. Over time, the system refines its
understanding of which research methods yield the most useful results in a given context.
2.2.2 Generative Model
Inthisframework,thegenerativemodelishierarchical,structuredacrosstwolevels. Thishierarchyiscrucial
forprovidingcontexttotheagent,enablingittounderstandboththeenvironmentitisinteractingwithand
the specific processes within that environment.
The top level of the generative model consists of one state factor: the industry, and one observation
modality: the industry cue. The purpose of this level is to provide the agent with an understanding of the
high-level context—essentially, which industry it is interacting with at a given time. The agent observes an
industry cue that informs it about the current industry in which the research processes are unfolding. This
5
ensures that the agent can interpret all subsequent observations with the knowledge of the specific industry
it is dealing with.
The bottom level of the generative model consists of two state factors: the industry and the research
process, with three observation modalities:
• Research process: Observes which research process is being used.
• Outcome: Observes the result of the research process in a given industry (i.e., a mapping between
the research process and industry).
• Industrycue: Observestheindustrycueagain,passeddownfromthetoplevel,tomaintainawareness
of the industry context.
Thus,theagentisprovidedwithbothstateandobservationinformationfromthetoplevel(industryand
its cue), which is passed down to the bottom level to act as a continuous context during its exploration of
research processes. When the agent observes data from the research process, it knows which industry these
observationsaretiedto,basedontheindustrycueprovidedatthetoplevel. Theobservationoftheresearch
process itself allows the agent to recognize which process is being applied, while the outcome provides
information about the result of applying that research process in the given industry.
Theoutcomeinthissimplifiedmodelisapresetsimulationthatmapseachresearchprocesstoaspecific
result within the industry. For example, in Industry 1, Research Process 1 may always yield an ”Excellent”
result, while Research Process 2 may yield a ”Good” result, and so on. In more practical applications, the
outcomewouldbedetermineddirectlybyinteractingwiththeenvironment,butinthissimplifiedmodel,the
outcome serves as a simulation to help the agent learn these mappings.
Bystructuringthegenerativemodelinthishierarchicalway,theagentiscapableofmaintainingawareness
of its industry context while refining its understanding of how different research processes perform within
that context. The hierarchical POMDP ensures that observations about industry cues are continuously
processed,providingthenecessarycontextforlearningthedynamicrelationshipsbetweenresearchprocesses
and their outcomes.
6
Figure1: HierarchicalPOMDPshowingresearchmethodsandtheiroutcomes. Theagentadaptstochanges
intheresearchenvironmentbyupdatingthestateandobservationfactors. Thisstructureallowsforcontinual
learning as new information becomes available.
2.2.3 Generative Process
The generative process in this model simulates the environment that the agent interacts with. In an ideal
scenario,thegenerativeprocesswouldinvolvereal-worlddatabeinggeneratedbytheenvironmentinresponse
to the agent’s actions. However, for the purpose of this research model, the generative process is simplified
and simulated based on the state-observation mappings present in the A matrix.
In this setup, the generative process takes the agent’s policy selections and uses them to sample obser-
vations from the predefined mappings in the A matrix. Specifically, after the agent selects a policy (which
represents its action in terms of selecting a research process), the generative process samples from the A
matrix to determine the corresponding outcome for the given research process in the current industry. The
A matrix contains the probabilities that govern how observations (i.e., outcomes) are generated based on
the agent’s belief about the current state (industry and research process).
Forexample,iftheagentselectsResearchProcess1inIndustry1,theAmatrixmayindicatethatthereis
ahighprobabilitythattheoutcomewillbe”Excellent.”Thegenerativeprocesssamplesfromthisprobability
distribution, providing the agent with an observation of ”Excellent.” The agent then uses this observation
to update its beliefs about the mappings between research processes and outcomes in the current industry.
The generative process is essentially a simulation of how the environment might behave, providing feed-
back to the agent about its actions. The agent is tasked with learning the structure of the environment by
7
minimizing Expected Free Energy (EFE). This involves selecting policies that are expected to reduce
the agent’s uncertainty about the state-observation mappings (i.e., learning the A matrix).
Because the agent is providing observations to itself via the sampling of the A matrix, the process of
learning becomes one of continuous refinement. Over time, the agent uses the observations it generates
(from its interaction with the environment) to improve its internal model, refining its understanding of how
different research processes produce outcomes in different industries. This allows the agent to optimize
its performance, selecting policies that are expected to maximize the accuracy of its predictions about the
environment.
Thus,thegenerativeprocessinthisframeworkisnotonlyresponsibleforgeneratingobservationsbutalso
fordrivingtheagent’slearningbysimulatingfeedbackloopsbetweentheagent’spoliciesandtheenvironment
it is modeling.
3 Results
To demonstrate the continual learning capabilities of the active inference agent, we tested it in two different
environments: the first with predefined state-outcome mappings and the second with modified mappings
for certain industries to simulate a shift in the environment. We used a scoring mechanism based on the
agent’sconfidenceinthecorrectoutcomes,whichiscalculatedfromthebeliefvaluesassignedtoeachpossible
outcome in the lowercase ’a’ matrix. This matrix represents the agent’s internal belief mappings between
states (industries and research processes) and observations (outcomes).
3.1 Scoring Mechanism
Toevaluatetheagent’slearningperformance, wedefineascoringmechanismthatreflectsboththeaccuracy
and confidence of the agent in its internal belief state. The agent’s belief about the relationship between the
hidden states (e.g., industries and research processes) and observations (e.g., outcomes) is represented by
the lowercase ’a’ matrix, denoted by a , where:
ijk
• i indexes the possible outcomes (e.g., Excellent, Good, etc.),
• j indexes the hidden states (e.g., industry),
• k indexes the research processes.
For each combination of hidden state j and research process k, the agent assigns a probability a to
ijk
each outcome i, which reflects the agent’s belief in the likelihood of that outcome.
3.1.1 Score Calculation
Thescoreforeachstate-processpair(j,k)iscalculatedbasedontheagent’sconfidenceinthecorrectoutcome
relative to its confidence in the incorrect outcomes. Let i∗ denote the index of the correct outcome for the
given pair (j,k), and let i denote the index of the incorrect outcome with the highest belief value (i.e.,
max
the highest a where i̸=i∗).
ijk
The score S for the pair (j,k) is defined as the difference between the agent’s belief in the correct
jk
outcome and the highest belief in any incorrect outcome:
S =a −maxa .
jk i∗jk ijk
i̸=i∗
Thus, iftheagenthasahighbeliefinthecorrectoutcomea andlowbeliefintheincorrectoutcomes,
i∗jk
S will be positive. Conversely, if the agent incorrectly assigns higher belief to an incorrect outcome, the
jk
score will be negative.
8
3.1.2 Total Score for an Iteration
The total score for an iteration t, denoted S(t), is calculated as the sum of normalized scores across all
industries j and research processes k:
16 4
(cid:88)(cid:88)
S(t) = Snorm.
jk
j=1k=1
This total score reflects the overall performance of the agent in the current iteration, taking into account
how well the agent has learned the correct mappings for each industry and research process.
3.1.3 Negative Scores
Negative scores occur when the agent’s belief in the correct outcome is lower than the belief in an incorrect
outcome. For example, if the belief values for Excellent and Good were reversed:
a =0.2, a =5, a =0.1, a =0.1, a =0.05,
11k 21k 31k 41k 51k
then the score would be:
S =0.2−5=−4.8,
1k
indicating the agent has mistakenly assigned much higher confidence to the incorrect outcome.
3.1.4 Overall Evaluation
By calculating and summing the scores across all industries and research processes, we obtain a measure of
the agent’s overall learning performance and how well it adapts to changes in the environment. This scoring
mechanismallowsustoassessboththeaccuracyandconfidenceoftheagent’sbeliefsabouttheenvironment.
3.2 First Environment: Predefined Outcomes
The first environment consists of predefined state-outcome mappings for each of the 16 industries and their
respectiveresearchprocesses. Theagentbeginsbyinteractingwiththeenvironment,learningthesemappings
over the first 10 trials. The first 10 trials shown in 2) and 3) are both the calculated score per iteration
for just state-observation mappings in the 1st industry and state-observation mappings for all industries,
respectively.
9
Figure2: Theresultsofthefirst10trialsintheoriginalenvironment. Theagentquicklylearnsthedynamics
of the environment, reaching a high score for Industry 1 within six iterations, demonstrating that the active
inference agent is able to learn environmental dynamics effectively.
3.3 Second Environment: First Industry Modified
Thesecond20trialsin2)werecarriedoutbythesameactiveinferenceagentplacedinanewenvironmentin
which only the state-observation mappings for the first industry were changed. The mappings for the other
15 industries were the same, so only the score for Industry 1 was calculated. The steep drop-off is caused by
an environment change where none of the agents’ former beliefs are relevant. The agent is able to learn this
new environment in a linearly increasing fashion, but doesn’t reach its former maximum.
Figure 3: Learning progress of the active inference agent across all industries in the original environment.
Afteraninitialdropduetouncertainty, theagentlearnsthecorrectmappings, achievingahigherscoreover
20 trials.
10
3.4 Third Environment: More Modifications
The second 20 trials in 3) were carried out by the active inference agent initially trained in environment 1
being placed in a new environment. Here several state-observation mappings were changed across a much
biggersetofindustries,representingabiggerregimeshiftinthisenvironmentthaninthesecondenvironment.
Here the agent is able to relearn the environment once again, but at a higher rate than in the sec-
ond environment. In fact, the agent overtakes its previous maximum score by the 18th trial in the new
environment.
4 Conclusion
The results from the different environments demonstrate key properties of Active Inference agents in terms
of adaptation and relearning.
In the second environment, where only one industry’s mappings were changed, the agent required more
iterations to relearn the altered relationships, which suggests that localized changes lead to more gradual
adaptation. The slower rate of learning can be attributed to the agent’s reliance on prior beliefs about
the unchanged aspects of the environment, which can introduce friction when adapting to small, isolated
changes. The lower learning rate can also potentially be attributed to the score calculation. Since the
score was only being calculated on the 1st industry it didn’t have the benefit of ballooning the score due to
increased confidence from other industries.
Incontrast,thethirdenvironment,wheremoreindustriesexperiencedchanges,showedfasteradaptation.
This can be interpreted as the agent being more flexible when faced with global environmental shifts. With
multiple industries changing simultaneously, the agent can globally adjust its model, resulting in faster
convergence and even surpassing the peak performance seen in the first environment. This quicker learning
rate may also be due to the opposite effect potentially seen in environment two. Since, there are several
unchanged mappings they’re increasing scores may have ballooned the overall score more than expected.
These results suggest that the structure of the environmental changes may impact adaptation rate. The
resultsconclusivelyshowthattheactiveinferenceagentisabletoreassessitsviewswhenplacedinchanging
environments and adapt its internal model in response.
5 Discussion
This paper demonstrates the potential of Active Inference as a framework for creating self-learning agents
that continuously adapt to their environment. By engaging in continual interaction with their surroundings
and refining internal models, Active Inference agents are able to autonomously adjust to changes over time
without requiring manual intervention. This feature makes Active Inference highly applicable in domains
where adaptability is crucial, such as research, quantitative finance, and healthcare.
5.1 Application to Quantitative Finance
Inquantitativefinance,tradingstrategiesoftenbecomeobsoleteduetoshiftingmarketconditions. Usingan
Active Inference agent, we can build a system where the agent continuously observes market indicators like
asset prices, volatility, and economic factors. The state factors could include market conditions and interest
rates, while observation modalities would encompass price movements and volume.
The agent would learn and update its internal model based on real-time data, adjusting its trading
strategies as market conditions evolve. The generative process simulates possible future market scenarios,
enabling the agent to select policies that minimize risk and maximize returns. This adaptability makes
the agent resilient to rapid market fluctuations, reducing the need for constant manual updates to trading
strategies.
5.1.1 Application to Healthcare
In healthcare, patient data changes over time, requiring constant adaptation in treatment plans. An Active
Inference agent could be applied to a clinical decision support system where state factors include patient
11
health and treatment options, and observation modalities could involve test results and symptom reports.
Theagentwouldcontinuouslyupdateitsmodelbasedonincomingpatientdata,refiningitsunderstanding
ofwhichtreatmentsaremosteffective. Bysimulatingdifferenttreatmentoutcomes,theagentcanrecommend
the best course of action, ensuring that treatment strategies evolve as the patient’s condition changes. This
continual learning leads to more personalized and effective healthcare without constant human intervention.
5.1.2 Future Work
Moreover,futureworkcouldexploremoresophisticatedgenerativemodelsandprocesses,allowingagentsnot
only to learn new mappings but also to discover new state factors and observation modalities. This would
enable agents to move beyond pre-defined structures, learning entirely new frameworks as they interact
with their environment. Additionally, integrating Active Inference with other AI paradigms, such as large
languagemodelsorreinforcementlearning,couldyieldevenmoreadaptableandcapableagents. Here,active
inference agents would serve as a mechanism to increase the temporal effectiveness of other agents they can
manipulate.
Active Inference offers significant potential for real-world applications that require resilience and long-
term sustainability. Whether in finance, healthcare, or other fields, this framework has the ability to keep
AI systems relevant and adaptive, reducing the need for human intervention and making it a key tool for
the future of adaptive artificial intelligence.
6 Data Availability
Findallrelevantcodeandfiguresathttps://github.com/RPD123-byte/Demonstrating-the-Continual-Learning-
Capabilities-and-Practical-Application-of-Discrete-Time-Active. Subroutines for spm-MDP-VB-X and spm-
MDP-check among others can be found in the spm12 package.
12
References
[1] Friston,K.,Parr,T.,&deVries,B.(2017).Thegraphicalbrain: Beliefpropagationandactiveinference.
Network Neuroscience, 1(4), 381-414. https://doi.org/10.1162/NETN_a_00018
[2] Parr, T., Pezzulo, G., & Friston, K. J. (2021). Active inference: The free energy principle in mind,
brain, and behavior. Journal of Mathematical Psychology, 100, 102364. https://www.sciencedirect.
com/science/article/pii/S0022249621000973#b40
[3] Buckley,C.L.,Kim,C.S.,McGregor,S.,&Seth,A.K.(2017).Thefreeenergyprincipleforactionand
perception: A mathematical review. Biological Cybernetics, 112(6), 1-18. https://link.springer.
com/article/10.1007/s00422-019-00805-w
[4] Smith, J., & Johnson, M. (2023). Advances in Active Inference. Trends in Cognitive Sciences. https:
//www.sciencedirect.com/science/article/pii/S1364661323002607
[5] Brown, H. R., & Friston, K. J. (2018). The Physics of Free Will. Neuroscience and Biobehavioral
Reviews, 90, 54-64. https://www.sciencedirect.com/science/article/pii/S0149763418302525?
ref=pdf_download&fr=RR-2&rr=88d83f2d8dfbf279#bib0145
[6] Schwartenbeck, P., & Friston, K. (2017). Active Inference, Curiosity and Insight. Neural Com-
putation, 29(10), 2633-2683. https://direct.mit.edu/neco/article-abstract/29/10/2633/8300/
Active-Inference-Curiosity-and-Insight?redirectedFrom=fulltext
[7] Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Predictive Coding: A Theoretical
and Experimental Review. NeurIPS. https://papers.nips.cc/paper_files/paper/2020/file/
865dfbde8a344b44095495f3591f7407-Paper.pdf
[8] Zhang, Y., & Zheng, Z. (2023). Reinforcement Learning with Active Inference. https://arxiv.org/
pdf/2306.09205
[9] Dandoy, L., & Di Francesco, M. (2023). Active Inference with State-Only Control. https://arxiv.
org/pdf/2311.10300
[10] Sajid, N., Friston, K., & Parr, T. (2021). Planning and Active Inference. https://arxiv.org/abs/
2103.13860v3
[11] Shipp, S. (2023). The role of the free energy principle in cognitive systems. Cognitive Science, 17(2),
212-248. https://journals.sagepub.com/doi/pdf/10.1177/26339137231222481
[12] Schwartenbeck, P., FitzGerald, T., Mathys, C., Dolan, R., & Friston, K. (2023). Active inference, belief
propagation, and the free energy principle. PLOS ONE, 17(11), e0277199. https://journals.plos.
org/plosone/article?id=10.1371/journal.pone.0277199
13

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
