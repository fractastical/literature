=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Quantifying system-environment synergistic information by effective information decomposition
Citation Key: yang2025quantifying
Authors: Mingzhe Yang, Linli Pan, Jiang Zhang

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: What is the most crucial characteristic of a system with life activity? Currently, many theories
have attempted to explain the most essential difference between living systems and general sys-
tems, such as the self-organization theory and the free energy principle, but there is a lack of a
reasonable indicator that can measure to what extent a system can be regarded as a system with
life characteristics, especially the lack of attention to the dynamic characteristics of life systems. In
this ar...

Key Terms: system, systems, synergistic, effective, environment, decomposition, life, information, school, indicator

=== FULL PAPER TEXT ===

APS/123-QED
Quantifying system-environment synergistic information by
effective information decomposition
Mingzhe Yang and Linli Pan
School of Systems Science, Beijing Normal University.
Jiang Zhang
School of Systems Science, Beijing Normal University and
Swarma Research
(Dated: January 29, 2025)
1
5202
naJ
82
]NM.oib-q[
1v67661.1052:viXra
Abstract
What is the most crucial characteristic of a system with life activity? Currently, many theories
have attempted to explain the most essential difference between living systems and general sys-
tems, such as the self-organization theory and the free energy principle, but there is a lack of a
reasonable indicator that can measure to what extent a system can be regarded as a system with
life characteristics, especially the lack of attention to the dynamic characteristics of life systems. In
this article, we propose a new indicator at the level of dynamic mechanisms to measure the ability
of a system to flexibly respond to the environment. We proved that this indicator satisfies the
axiom system of multivariate information decomposition in the partial information decomposition
(PID) framework. Through further disassembly and analysis of this indicator, we found that it
is determined by the degree of entanglement between system and environmental variables in the
dynamics and the magnitude of noise. We conducted measurements on cellular automata (CA),
random Boolean networks, and real gene regulatory networks (GRN), verified its relationship with
the type of CA and the Langton parameter, and identified that the feedback loops have high
abilities to flexibly respond to the environment on the GRN. We also combined machine learning
technology to prove that this framework can be applied in the case of unknown dynamics.
Keywords: synergy, flexibilty, effective information, partial information decomposition, gene regulatory
networks
I. INTRODUCTION
Manycomplexsystemsexhibitlife-likecharacteristics. Flexiblerobotsinteractseamlessly
with humans, and orderly online communities give rise to innovative crowdsourced products.
So what is the key difference between them and ordinary systems? The self-organization
theory is the first to answer this question [1]. People believe that a system must have
the ability of self-organization to be considered a system with life-like characteristics [2, 3].
However, life is not just an open system with self-organization [4]. More importantly, they
can still maintain the stability of their own structure and function when faced with a diverse
and changing environment. For example, snowflake is a self-organizing system [3], but it is
not considered to have the activity possessed by life. Once the environmental temperature
rises, the snowflake melts. If this snowflake could autonomously avoid high-temperature
2
environments and phase transitions that would damage its own structure, then it would be
considered as an adaptive system with life activity [4]. This kind of adaptive systems has
been discovered in a large number of papers in various fields and is a major characteristic
of complex systems [5–7].
In addition to qualitative discussions, we urgently need a formal framework to quantify
and identify this unique property of life. There are already metrics to measure the degree
of a system’s self-organization [8, 9]. However, to date, few indicators exist that assist us in
measuring to what extent a system can cope with environmental changes. The proposal of
the information theory of individuality [10] is to measure the individuality of living systems
fromtheperspectiveofinformationdynamics. Itdescribestheindividualsurvivalofasystem
as maximizing the transmission of its own information over time, so mutual information and
conditional mutual information are used to define the individuality of an organism.
However, in individual information theory, the calculation of the metric (mutual infor-
mation) depends on the state distribution of the observed data. The state distribution we
observe depends on the initial conditions of the system and the environment, as well as
the duration of the dynamical process (if the system is in a non-steady state). However,
the characteristic of life’s flexible response to the environment is not a property that varies
with time and state, but rather reflects the characteristics of the interaction mechanisms
between the system and the environment, representing a dynamical property. Therefore,
when discussing the features of complex systems, we should focus on the quantities defined
on their causal mechanisms, rather than the states [11]. These causal mechanisms should
be invariant in time. This constancy is crucial for describing the properties of the system.
Tononi and Hoel et al. [11, 12] have proposed information indicators measured at the
causal mechanism level, such as effective information (EI) [11, 12]. The causal mechanism
typically remains invariant with respect to state and time. It is commonly assumed that
dynamics are described by Markovian transition probability matrices (TPM) [11, 13], and
EI is designed as a function of TPM [14, 15]. EI is used to measure the strength of causal
effects in dynamics [11, 15], and the difference in EI between macro and micro dynamics is
employed to quantify the degree of emergence in complex systems [11, 13]. When dynamical
mechanisms are unknown, machine learning techniques can identify causal mechanisms from
the data [16]. Although EI itself does not account for the interactions between the system
and the environment [17], this does not prevent us from extending EI to develop a causal
3
metric that incorporates environmental influences. Consequently, we can leverage this kind
of indicator, along with other relevant metrics [18], to describe causal properties and craft
an indicator capable of measuring the system’s responsiveness to the environment at the
causal mechanism level.
Moreover, through partial information decomposition(PID) [19], we see that individu-
ality involves redundant information and synergistic information in which the system and
the environment are coupled together. Individual information theory uses the PID theory
to explain the physical meaning of individuality indicators, but failed to calculate informa-
tion atoms that describe the system’s flexible response to the environment [10]. Numerous
methods have been proposed to calculate information atoms, yet their computational out-
comes variously contravene common consensus on certain properties [20]. In this paper, we
introduce EI, which requires the input variables from the previous moment to be uniformly
distributed [11, 12] and starting from the existing PID axiomatic system [21], we derive a
computable definition of information atoms in a three-variable system. This allows us to
obtain information-theoretic metrics that precisely align with the meanings of uniqueness
and synergy.
In this letter, we will define an indicator at the causal mechanism level to characterize the
ability of a system to maintain its own structure and function when dealing with environ-
mental changes. Through mathematical proofs, we will show that this indicator is precisely
the synergistic information of the system itself when the system and the environment are
coupled together. In our numerical experiments, we demonstrate the correlation between
this indicator and the edge of chaos within cellular automata. Additionally, we apply this
indicator to gene regulatory networks (GRNs), uncovering the significance of feedback loops
(FBLs) and the responses of the steady states of a highly synergistic system to environmen-
tal alterations. Notably, FBLs are instrumental in carrying out the biological functions of
biological systems in reacting to environmental changes [22]. Additionally, we conducted a
machine learning experiment to demonstrate that causal mechanisms can be identified using
machine learning when data alone is available.
4
II. FORMULATION
Next, we provide the formal expressions for the system and the environment. There
are three variable combinations that have a causal influence on the system’s state at the
next moment: the system itself, the environment, and the joint variables of the system and
environment. Based on this, we define three causal mechanisms: the Individual Mechanism,
the External Driving Mechanism, and the Distinct Mechanism.
A. Distinct Mechanism
We define the System X as a subset of the World U, with respective state sets Ω
X
and Ω . Assuming that the dynamics of U satisfy Markovianity and are discrete, they
U
can be described by the conditional probability P(Ut+1 | Ut), where t denotes the time in
this stochastic process. To exclusively measure the dynamical properties, we eliminate the
influence of the World data distribution by introducing the do-operator [18], denoted as
do(Ut ∼ U(Ω )), where U represents the uniform distribution.
U
For the stochastic process at time t+1, our analysis focuses exclusively on the temporal
evolution of the target system Xt+1. This necessitates marginalizing over extraneous vari-
ables in the global system U, thereby restricting attention to the marginalized conditional
probability: P(Xt+1 | Ut). Formally, this probability measure is obtained through state
space projection: ∀xt+1 ∈ Ω ,P(Xt+1 = xt+1 | Ut) = (cid:80) P(Ut+1 = ut+1 | Ut).
X ut+1∈ΩU
πX(ut+1)=xt+1
Here π : Ω → Ω represents the canonical projection mapping that extracts the X-
X U X
component from the global state.
Referring to [23], we collectively term the system and environment as a Distinction. We
are solely concerned with the impact of the system and environment on the system’s state
at the next moment, thus we define the Distinct TPM as
P : P(Xt+1|Xt,Et). (1)
Xt,Et→Xt+1
The state spaces of the joint variable X,E and of X are denoted as Ω and Ω , respec-
(X,E) X
tively. Consequently, the shape of this TPM is |Ω | × |Ω |. In Figure 1(b), we focus
(X,E) X
on the causal arrows from Xt and Et pointing to Xt+1, which corresponds to the dynamics
described by the system’s distinct TPM.
5
FIG. 1. The causal diagram of the system’s interaction with the environment. (a) When a set
of variables is designated as the system, the spatial distinction between the system, environment,
and background conditions is made, with arrows representing the causal relationships between
variables. (b) The causal diagram of the interaction between X and E over time. Changes in the
filling of circles in the diagram represent state transitions. In the diagram, the green triangle and
the formula represent the EI from both the system and the environment to the system itself, the
blue lines and the formulas represent the two types of unique information, and the red lines and
theformularepresenttheeffectivesynergisticinformationbetweenthesystemanditsenvironment,
also known as flexibility. This diagram is consistent with the causal diagram described in [10].
B. Individual and External Driving Mechanisms
StartingwiththedistinctTPMP ,wecanderivetheTPMP ,whichcon-
Xt,Et→Xt+1 Xt→Xt+1
siders only the system’s internal dynamics and excludes environmental information, termed
the Individual Mechanism. Meanwhile, we extract the TPM P , which focuses ex-
Et→Xt+1
clusively on the environment’s impact on the system’s information, termed the External
Driving Mechanism. The definitions of these mechanisms are as follows, and their corre-
sponding relationships with the causal diagrams are also marked in Figure 1(a).
(cid:88)
P = |Ω |−1 p(Xt+1|Xt,Et). (2)
Xt→Xt+1 E
Et=et
(cid:88)
P = |Ω |−1 p(Xt+1|Xt,Et). (3)
Et→Xt+1 X
Xt=xt
6
Whenweobservetheprobabilitydistributionofenvironmentalstates, theconditionalproba-
bilityofthesystemcanbeobtainedthroughP(Xt+1 | Xt) = (cid:80) P(Et)P(Xt+1 | Xt,Et).
Et=et
Because we introduce the do-operator [18], which intervenes in both the environment and
the system to achieve a uniform distribution, denoted as do(Xt,Et ∼ U(Ω )) in Figure
X,E
1(b), we have P(Et) = |Ω |−1. Consequently, we derive the expressions for P and,
E Xt→Xt+1
similarly, for P as Eqs. (2) and (3) describe.
Et→Xt+1
C. Definition of Flexibility
After obtaining those TPMs, we introduce the EI function for an arbitrary causal mech-
anism (TPM). EI represents the effective information, which quantifies the strength of
causal effects for a TPM [11, 14]. EI is defined as a mutual information for an intervened
uniformly distributed input variable X and its corresponding output variable Y as shown
in the following formula.
EI(P ) ≡ I(X,Y | do(X ∼ U(Ω )))
X→Y X
N N (4)
1 (cid:88)(cid:88) N ·p
ij
= p log ,
N ij (cid:80)N p
i=1 j=1 k=1 kj
where p is an element of the TPM P , and N is the number of states of the input vari-
ij X→Y
able. For further details, see Appendix B. We can define EIs for the mechanisms P
Xt→Xt+1
and P which are also coined as Individual Driving Information and External Driving
Et→Xt+1
Information:
EI(P ) = I(Xt,Xt+1 | do(Xt,Et ∼ U(Ω ))), (5)
Xt→Xt+1 X,E
EI(P ) = I(Et,Xt+1 | do(Xt,Et ∼ U(Ω ))). (6)
Et→Xt+1 X,E
These equations represent, respectively, the portion of information for the next moment
of the system that is provided solely by the system itself and the portion provided solely
by the environment, respectively. Naturally, due to the properties of mutual information,
both types of effective information are non-negative. Within the effective joint mutual
information, after the subtraction of these two components, what remains is the information
thatisexclusivelyprovidedbythesystemandtheenvironmentinunion,termedasflexibility,
7
or the effective synergy between system and environment, denoted by Syn(P ).
Xt,Et→Xt+1
Syn(P ) = EI(P )−EI(P )−EI(P ). (7)
Xt,Et→Xt+1 Xt,Et→Xt+1 Xt→Xt+1 Et→Xt+1
The correspondence between these indicators and the causal diagram is presented in Figure
1(b).
D. Properties of Flexibility
Actually, Syn(P ) aligns with the definitions and axiomatic system of PID the-
Xt,Et→Xt+1
ory [24] regarding the requirements for synergistic information in trivariable systems.
˜
In the following paragraphs, we denote Z as the intervened version of any random vari-
able Z after the intervention do(Xt,Et ∼ U(Ω )). Consequently, we have the following
X,E
theorem:
Theorem 1 In a trivariable system, the flexibility defined in Eq.(7) is the synergistic infor-
mation of X
˜t,E ˜t
with respect to X
˜t+1.
This theorem pertains to a specific PID axiomatic system(please refer to Appendix A). For
the proof of this theorem, please refer to Appendix C1. The upper bound of synergy is
min{I(X
˜t;X ˜t+1|E ˜t),I(E ˜t;X ˜t+1|X ˜t)}.
The proof of this property is provided in Appendix
C2. We can further decompose the synergy term, i.e., Equation 7.
Corollary 1 The flexibility defined in Eq.(7) can be decomposed into two components: Ex-
pansiveness and Introversion.
We define Expansiveness, abbreviated as Exp, and Introversion, abbreviated as Int, as fol-
lows:
(cid:32) (cid:33) (cid:32) (cid:33)
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
Exp(P ) = H P + H P , (8)
Xt,Et→Xt+1
|Ω | |Ω |
x,e
|Ω | |Ω |
x,e
E X X E
e∈ΩE x∈ΩX x∈ΩX e∈ΩE
(cid:32) (cid:33)
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
Int(P ) = 2log |Ω |− H(P )−H P .
Xt,Et→Xt+1 2 X |Ω | x,e |Ω | x,e
X,E X,E
x∈ΩXe∈ΩE x∈ΩXe∈ΩE
(9)
The proof of Corollary 1 can be found in Appendix C3.
8
FIG. 2. Schematic diagrams of expansiveness and introversion. Different colors of the circles rep-
resent different system states, while different colors of the dashed lines indicate different environ-
mentalstatesatthattime. Arrowswithassociatednumericalvaluesdenotetransitionprobabilities
from system-environment states at time t to system states at time t+1. (a) shows the case of
low expansiveness and high introversion, while (b) shows the case of high expansiveness and low
introversion.
To elucidate the meanings of expansiveness and introversion, we first introduce the EI
function and its decomposition [13].
(cid:32) (cid:33) (cid:32) (cid:33)
N N
1 (cid:88) 1 (cid:88)
EI(P ) = − H(P ) +H P (10)
X→Y i i
N N
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
determinism non-degeneracy
Here, X denotes an arbitrary input variable with a state space of size N, Y represents the
corresponding output variable, P corresponds to the i-th row of the TPM P , and H(P)
i X→Y
(cid:16) (cid:17)
indicates the Shannon entropy of probability distribution P. The term − 1 (cid:80)N H(P )
N i=1 i
represents determinism; when it is high, it indicates that the system has low noise. The term
(cid:16) (cid:17)
H 1 (cid:80)N P representsnon-degeneracy; whenitislow(degeneracyishigh),itimpliesthat
N i=1 i
the system will deterministically converge to certain states, indicative of attractor dynamics.
When the system employs different TPMs corresponding to individual environmental
states, the environmental context becomes explicitly incorporated. In contrast, when en-
vironmental states remain unspecified, the system adopts an environment-averaged TPM.
9
Through the Effective Information (EI) decomposition framework, Equation 8 measures the
system’s state-specific differentiation under well-defined environments versus its stochastic
variability under environmental uncertainty. These dual aspects together constitute Expan-
sivenessasthesystem’soutwardadaptiveorientation. Correspondingly, Equation9captures
the system’s structured coordination in explicit environments versus its reduced differentia-
tion in ambiguous contexts, jointly characterizing Introversion as the internal consolidation
tendency. This demonstrates two distinct pathways through which the system enhances
flexibility: expansiveness and introversion. The relationship between the magnitudes of Exp
and Int under different conditions can be referred to in Figure 2.
Given that the Shannon entropy of the system’s probability distribution ranges from 0 to
log |Ω |, we can determine the numerical ranges for Exp and Int: 0 ≤ Exp(P ),
2 X Xt,Et→Xt+1
Int(P ) ≤ 2log |Ω |.
Xt,Et→Xt+1 2 X
III. RESULTS
In the subsequent experiments, we will validate the meaning and functionality of these
metrics.
A. Cellular Automaton
Cellular automata (CA), particularly one-dimensional elementary CA, are used to simu-
late artificial life, with Wolfram identifying 256 possible rule sets [25]. Wolfram classified CA
into four behavior types: stable, periodic, chaotic, and complex, with Class IV potentially
being computationally universal. However, this classification is subjective, leading Langton
to introduce the parameter λ = 1−n [2] to quantify CA behaviors, where n is the number of
8
outputs being 1 in the CA rule table. He demonstrated that CA behaviors can continuously
transition from Class I to Class III as λ varies from 0 to 1, with values between 0.3 and 0.6
corresponding to complex Class IV dynamics.
As depicted in Figure 3, we have validated the relationship between flexibility and its
decomposition with the behavior types of CA. In Figure 3(a), as CA become increasingly
complex, expansiveness rises while introversion declines, overall reflecting an increase in
flexibility. A similar phenomenon is observed in Figure 3(b). When λ is between 0.3 and
10
FIG. 3. (a) The trend of expansiveness, introversion and flexibility between the system and the
environment as the type of CA changes from Class I to Class IV. The line represents the mean, and
the band represents the standard deviation. (b) A comparative trend chart of them, along with
mutual information proposed by [2], as λ varies. (c) A comparative chart of the standard deviation
for the four indicators calculated. (d) A scatter plot of flexibility and mutual information for all
256 CA. The flexibility of a specific rule-based cellular automaton is calculated without a standard
deviation, whereas mutual information includes a standard deviation, depicted by error bars.
0.6, the increase in expansiveness exceeds the decrease in introversion, leading to an increase
in flexibility. The following mathematical relationship indicates that, in noise-free CA,
introversion is indeed a definite function of the λ.
(cid:32) (cid:33)
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
Int(P ) = 2log |Ω |− H(P )−H P
Xt,Et→Xt+1 2 X |Ω | x,e |Ω | x,e
X,E X,E
x∈ΩXe∈ΩE x∈ΩXe∈ΩE
(cid:32) (cid:33)
1 (cid:88) (cid:88)
= 2log |Ω |−H P
2 X |Ω | x,e
X,E
x∈ΩXe∈ΩE
= 2log |Ω |−H(λ,1−λ)
2 X
(11)
This explains why, in Figure 3(c), the standard deviation of introversion is zero, while
the fluctuations in effective synergy are attributed to expansiveness. Concurrently, it is
evident that mutual information, as a measure based on observational data, exhibits a
larger standard deviation, even though its trend aligns with that of effective synergy. In
11
Figure 3(d), we run iterations for 200 steps under each initial condition in a space of 10-cell
automata. Based on these observational data, we calculate the mutual information from
time t to t+1 for a single cell and compute the expectation and standard deviation over
all possible initial conditions. The varying standard deviations across different rules of CA
indicate that the selection of initial conditions is crucial for the computation of mutual
information for some rules. In contrast, the calculation of effective synergy is independent
of initial conditions.
B. Identify Flexible Motifs in Gene Regulatory Networks
We will measure the flexibility of gene regulatory networks (GRNs) using real data.
GRNs describe how a collection of genes governs key processes within a cell, which are
often modeled as Boolean networks. Kadelka et al. [26] established the most comprehensive
repository of expert-curated Boolean GRN models to date, encompassing both structural
configurations and Boolean functions. These models describe the regulatory logic under-
lying a variety of processes in numerous species across multiple kingdoms of life. Due to
computational constraints, our analysis was limited to a subset of these networks. We select
63 models, with node counts ranging from 5 to 67, encompassing animal, plant, fungal, and
bacterial domains.
To investigate which GRN structures exhibit enhanced environmental responsiveness in
real-world settings, we assessed the flexibility of various three-node subgraph configurations
in Figure 4(a). The analysis revealed that feedback loops (FBLs) demonstrated the highest
flexibility values. In fact, FBLs in GRNs carry important biological functions. For example,
FBL structures often exhibit dynamical compensation(DC), which is the ability of a model
to compensate for variation in a parameter [22]. Additionally, many oscillators in biological
systems originate from negative FBLs, known as repressilators [22]. They play a crucial
role in adapting to environmental changes and regulating their own cycles. In Figure 4(b),
the mean flexibility of FBLs is also high, confirming the above conclusions. Moreover, the
structure with the highest value among four-node structures is not a simple FBL but a
more connected structure that includes FBLs (see the highlighted part in the figure). This
structure has not yet been fully studied and named by biologists. Perhaps it carries some
interesting functions related to biological adaptation to the environment that have yet to be
12
discovered.
Toverifythatsystemswithhigherflexibilityhaveastrongerabilitytorespondtoenviron-
mental changes, we compared the evolution of gene activation states under environmental
shocks in Figure 4(c-d). The genes in (c) are from the GRN of macrophage activation,
with a flexibility of 0.566, while those in (d) are from the GRN of tumour cell invasion and
migration, with a flexibility of 0. Comparing the time series curves, the former exhibits a
greater diversity of steady states under different environmental conditions. We measured
more precisely the mean mutual information between consecutive moments when environ-
mental state changes lead to shifts in system steady states, finding a positive correlation
coefficient of 0.487 with flexibility. For further experimental details, see Appendix D.
C. Random Boolean Network and Machine Learning
To explore dynamical characteristics influencing flexibility beyond network topology, sub-
sequent experiments investigate the effects of dynamical parameters on random Boolean
networks (RBNs) with fixed structures. Furthermore, while previous experiments were con-
ducted under known dynamical mechanisms, practical scenarios frequently involve data-
driven problems with unknown underlying mechanisms. We therefore employ RBN simu-
lations integrated with machine learning methodologies. By first reconstructing governing
mechanisms from observational data and subsequently performing flexibility measurements,
we validate the applicability of our framework to systems with concealed dynamical rules.
In Figure 5, each variable can take on two values, 0 or 1. For any variable X in the
i
system, its update rule is defined as follows:
1
P(Xt+1 = 1 | ut) = (12)
i 1+exp(−k (cid:80)n w ut)
j=1 j,i j
EachvariableU ’sedgeactingonanothervariableX respectivelycorrespondstoaweight
j i
value w ∈ [0,1]. k ∈ [0,+∞] is a parameter controlling the noise magnitude. When k = 0,
j,i
thenoiseisthegreatest,thatis,regardlessofthevaluesoftheinputvariables,theconditional
probability is a uniform distribution. The larger k is, the smaller the noise intensity is. We
set the temperature T = 1, for k ̸= 0. In this experiment, a total of two variables were set.
k
One is the temperature T, and the other is the proportion of the system’s own variables
and the environmental variables’ effect on the system. We set the weight of the system’s
13
FIG. 4. (a) The mean and standard deviation of flexibility for all possible three-node subgraph
structures in various real environments, where the value for FBL is the highest. (b) The mean
flexibility for some four-node subgraph structures, with the two highlighted structures having the
highest values. (c) The time series of state changes for the system composed of the genes BAG4,
BAG4 TNFRSF1A, and TNF BAG4 TNFRSF1A in the GRN of macrophage activation, starting
from the initial state ”111” and switching environmental states every 10 steps, with a total of 8
randomly selected environmental states. (d) The time series generated by the system composed
of the genes CDH1, CDH2, and GF in the GRN of tumour cell invasion and migration, under
experimental conditions consistent with those in (c).
own effect as w ∈ [0,1] (the solid line in Figure 5, including self-loops), and the weights of
the effects of the environmental variables on the system (the dashed arrows in Figure 5) are
w = 0.5(1−w) and w = 1.5(1−w). It can be seen that the larger w is, the stronger
E1,i E2,i
the influence of the system on itself is, and the weaker the influence of the environment on
the system is. The trend graph of the flexibility varying with T and w is shown in Figure
6(a).
AsillustratedinFigure6(a), thesystemexhibitsnear-zeroflexibilitywheneitherintrinsic
self-influence or environmental influence dominates the dynamics. A maximum flexibility
14
FIG. 5. The schematic illustrates the experimental design framework, where nodes A, B, and C
constitute the core system interacting with environmental variables E and E . Edges indicate
1 2
interaction relationships, with weights w encoded by color-coded mathematical symbols (colored
j,i
circles denote self-loops). The continuously adjustable parameter w ∈ [0,1] governs interaction
intensities, whose functional role is defined through the mathematical formulation in Eq.12.
value emerges at optimal coupling strength w, demonstrating the critical role of balanced
interactions. Furthermore, increasing the temperature T generally induces monotonic re-
duction in flexibility across the parameter space. Notably, Figure 6(b) reveals a counterin-
tuitive phenomenon where moderate noise levels (T ≈ 0.2) paradoxically enhance flexibility
to peak values. The phenomenon of enhanced synergistic information under low noise levels
has been previously captured by other metrics [27]. Our experiments demonstrate that this
phenomenon originates from interactions at the dynamical mechanism level. More impor-
tantly, through the decomposition of flexibility, we reveal that systems leverage low noise to
achieve greater diversity and environmental sensitivity (Exp). The benefits of this trade-off
outweigh the loss of intrinsic order (Int) caused by noise, resulting in an optimal noise level
that maximizes flexibility. In biological systems, noise inherent in the interactions of genes
regulating circadian rhythms maintains oscillatory behavior without decay [22]. This sug-
gests that other complex systems may benefit similarly from controlled noise levels, enabling
optimal environmental responsiveness.
To validate the framework’s capability in reconstructing and quantifying system mecha-
nisms under unknown dynamics, we train a neural network (NN) with four fully-connected
layers (32→64→64→16→8) using data generated from conditional probabilities at w = 0.5
15
FIG. 6. The experimental result graph. In (a), it indicates The trend graph of the flexibility
varying with the temperature T and the weight ratio parameter w. (b) shows that when w = 0.5,
the changing trends of flexibility, expansiveness and introversion with respect to T. The solid line
represents the result of the ground-truth, which is the curve intercepted by the section of w = 0.5
in (a). The dashed line is the calculation result of the trained artificial neural network based on
machine learning from the generated data. The radius of the band is the standard deviation of the
results of 10 repeated experiments.
under uniform input distribution. The NN architecture employs LeakyReLU activations in
hidden layers and cross-entropy loss for one-hot encoded inputs. The alignment between
predicted (dashed) and theoretical (solid) curves in Figure 6(b) demonstrates successful ex-
tension of our measurement framework to data-driven scenarios through machine learning
integration.
IV. DISCUSSION
Overall, for systems and environments that satisfy the Markov dynamics assumption,
we defined on the TPM how to measure the synergistic influence of the system and the
environment on the system - flexibility, which captures the flexibility of the system. It is
neithercompletelydeterminedbythedynamicsofthesystemitself(differentfromIndividual
Driving Information) nor completely determined by the dynamics of the environment on the
system (different from External Driving Information), but corresponds to the part in the
dynamics of the system and the environment as a whole where the whole is greater than
the sum of the parts. In the experiments of CA, we verified that complex cellular automata
have higher flexibility. More importantly, on the Boolean network data of GRNs selected
16
by experts, we found that the structure of feedback loops in various real environments has
higher flexibility. This indicates that flexibility specifically points to the biological functions
carried by this structure, such as dynamic compensation, biological cycle regulation, and
so on [22]. Currently, we only compared different Boolean network structures and have not
distinguished different Boolean functions on the same structure. In the future, flexibility can
be used to predict whether new structures and new dynamic functions have the biological
functions we are interested in.
Through decomposition of flexibility into dual components, we established expansiveness
and introversion. These respectively measure the interaction variety between system and
environment, and the level of dynamic organization in behavioral patterns. As shown in the
machinelearningexperiments,thenoiseintensityinthedynamicsisinverselyproportionalto
themagnitudeofintroversion. Whileexpansivenessmeasures, apartfromthenoisefactor, to
what extent the influences on the system from the system itself and the environment cannot
bedecoupled. CombiningthemachinelearningexperimentsonRBNsandthecomputational
results on CAs, we found that when there is noise in the dynamics, the reduction of noise
increases flexibility by increasing the magnitude of introversion. And when the noise in
the dynamics remains unchanged, the coupling degree of the influences of the system and
the environment on the system is reflected by expansiveness, and at this time, the change
of flexibility is dominated by the change of expansiveness. The analysis of expansiveness
indicates that in the process of obtaining the variable space from the state space through
a certain partition, the known boundary between the system and the environment is only
one of several possible partitions, and synergy occurs when the original boundary is too
ambiguous, so that we need to find a new coarse-graining of the state space.
At present, there are still some areas for improvement in this framework. We assumed
that the dynamics satisfy Markovianity, while many real-world problems need to be solved
within a non-Markovian framework. Additionally, although we obtained indicators of mul-
tivariate information decomposition with good properties on the three-variable system with
two variables acting on one variable, in the future, the calculation problem of synergistic
information when the source variables reach three or more needs to be addressed. In this
paper, the calculation and experiments of the indicators are based on discrete systems, so
in the future, the definition and calculation method of flexibility on continuous systems also
need to be further proposed.
17
We currently assume that the dynamics are known. If the dynamics are unknown but the
data is accessible, machine learning techniques can also be used to obtain the underlying
dynamic mechanism first and then measure it. The machine learning in this paper is a
preliminary attempt to prove its feasibility. In the future, we can introduce more complex
neural network models to learn more complex dynamic mechanisms, and even take flexibility
as the optimization goal to train artificial models with higher flexibility.
ACKNOWLEDGMENTS
We wish to acknowledge the support of Swarma Research and the assistance of Lifei
Wang, a scientist from Swarma.
Appendix A: Partial Information Decomposition Theory
Partial Information Decomposition (PID) is a theoretical framework designed to address
the problem of multivariate information decomposition [19]. To calculate the information
transfer between multiple variables, people have proposed quantification indicators such as
total correlation [28] and interaction information [29]. However, they have very important
flaws in application, such as not satisfying non-negativity [30]. For this reason, Williams et
al. [19] proposed the PID theory, decomposing joint mutual information into three types of
informationatoms: redundantinformation, uniqueinformation, andsynergisticinformation.
As shown in Figure 7, in a three-variable system, redundant information represents the
part of information that either of the two source variables can provide to the target variable;
unique information represents the part of information that only one source variable can
provide and the other source variable cannot; synergistic information refers to the part
of information that can only be provided when the two source variables are together.
The contribution of PID theory lies in that it not only gives a qualitative division but
also provides a strict axiom system for redundant information [24], so that we can follow this
axiom system to find quantitative calculation methods. The axiom system initially proposed
18
FIG. 7. The PID framework in considering the information dynamics of the system and the
environment. The two source variables are the variable Xt of the system at time t and the variable
Et of the environment at time t, and the target variable is the variable Xt+1 of the system at time
t+1. The large outer ellipse represents the joint mutual information provided by the two source
variables to the target variable. The overlapping part of the two small inner ellipses represents
the redundant information provided by the two source variables. Each small ellipse represents the
mutual information of a single source variable to the target variable. The remaining part other
than the redundant information is the unique information. The area not covered by both small
ellipses in the large ellipse is the synergistic information.
by Williams et al. [24] is as follows:
(S)Red(A ,A ,...,A ;S) is symmetric in A ,A ,...,A . (A1)
1 2 k 1 2 k
(I)Red(A;S) = I(A;S). (A2)
(M)Red(A ,A ,...,A ;S) ≤ Red(A ,A ,...,A ;S),with equality if A ⊆ A . (A3)
1 2 k 1 2 k−1 k−1 k
Among them, S is the target variable, A ,A ,...,A ⊆ {X ,X ,...,X } is a combination
1 2 k 1 2 n
of source variables, and X is a source variable. Starting from these axioms, we can give the
i
definitions of unique information and synergistic information, as well as the decomposition
of mutual information and joint mutual information [21].
I(X ;S) = Red(X ,X ;S)+Un(X ;S|X ) (A4)
1 1 2 1 2
I(X ;S) = Red(X ,X ;S)+Un(X ;S|X ) (A5)
2 1 2 2 1
I(X ,X ;S) = Red(X ,X ;S)+Un(X ;S|X )+Un(X ;S|X )+Syn(X ,X ;S). (A6)
1 2 1 2 1 2 2 1 1 2
In the above equation, I(X ;S) = Red(X ;S), I(X ,X ;S) = Red({X ,X };S). Later,
i i 1 2 1 2
19
people found that these three axioms are not sufficient to describe concepts such as re-
dundancy in our cognition, and the specific calculation formula for redundant information
proposed by Williams et al. will yield abnormal results in some examples [31]. Therefore, on
the basis of the above three axioms, people have added other axiomatic requirements [21].
(LC)Red(A ,A ,...,A ;SS′) = Red(A ,A ,...,A ;S)+Red(A ,A ,...,A ;S′|S). (A7)
1 2 k 1 2 k 1 2 k
(Id)Red(A ,A ;A ∪A ) = I(A ;A ). (A8)
1 2 1 2 1 2
In these equations, Red(A ,A ,...,A ;S′|S) = (cid:80) p(s)Red(A ,A ,...,A ;S′|s). S′ is
1 2 k s∈S 1 2 k
also an arbitrary target variable. There are also some axioms mentioned in [21] that will
not be repeated here, because they can be derived from the above axioms. People have
been trying to propose a computable definition of redundant information that can satisfy all
the above axioms. For example, the calculation method proposed by Harder et al. [31] can
satisfyaxioms(S)(I)(M)(Id), butitdoesnotsatisfyaxiom(LC). Andthemethodproposed
by Finn and Lizier [32] satisfies axiom (LC), but does not satisfy axiom (Id). Therefore,
currently, there is no feasible calculation method for redundant, unique and synergistic
information that can satisfy all axiom constraints.
Appendix B: Effective Information
Effective information (EI) is a quantitative measure that is used to characterize the
strength of causal effects in markov dynamics. It was first proposed by Tononi et al.[33] as
a key indicator in integrated information theory. Subsequently, Hoel et al. [11] employed
this metric to quantify the strength of causal effects in dynamics and further defined causal
emergence. EI is calculated based on the TPM and is independent of other factors. Its
formal definition is as follows [14]:
EI(TPM) = I(Xt,Xt+1|do(Xt ∼ U(Ω )))
X
N N (B1)
1 (cid:88)(cid:88) N ·p
ij
= p log
N ij (cid:80)N p
i=1 j=1 k=1 kj
In the given context, Ω represents the state space of Xt. The expression do(Xt ∼
X
U(Ω )) indicates that the original definition of EI is the mutual information when the
X
20
input variables are intervened to be uniformly distributed (i.e., at maximum entropy). It is
essentially a function of the TPM [15]. In the second equivalent computational formula, p
ij
denotes the probability of transitioning from state i to state j, and N = |Ω |. In the main
X
text, Eqs.5, 6, and 7 all require the substitution of Eq. B1 for calculation.
Appendix C: Proofs
1. Proofs of Theorem 1
In Appendix A, we present the axiomatic system of PID theory. Based on this system,
we initially state the following lemma:
Lemma 1 Given axioms (S, I, M, LC, Id), for any arbitrary variables X , X , and Y, if
1 2
X ⊥ X , the redundant information Red(X ,X ;Y) = 0.
1 2 1 2
Proof 1 From axiom LC, it follows that:
Red(X ,X ;(X ,X ,Y)) = Red(X ,X ;(X ,X ))+Red(X ,X ;Y|(X ,X ))
1 2 1 2 1 2 1 2 1 2 1 2
(C1)
= Red(X ,X ;Y)+Red(X ,X ;(X ,X )|Y).
1 2 1 2 1 2
From the axioms (S, I, M), it first follows that redundant information is non-negative
and that redundant information is less than or equal to the joint mutual information provided
by the two source variables [24]. Consequently, conditional redundant information is also
less than or equal to conditional joint mutual information. Thus, we can derive the following
inequality:
Red(X ,X ;Y|(X ,X )) ≤ I(X ,X ;Y|(X ,X )) = 0. (C2)
1 2 1 2 1 2 1 2
From the Id axiom and the condition X ⊥ X , we can deduce that:
1 2
Red(X ,X ;(X ,X )) = I(X ,X ) = 0. (C3)
1 2 1 2 1 2
Thus,
Red(X ,X ;Y)+Red(X ,X ;(X ,X )|Y) = 0. (C4)
1 2 1 2 1 2
Given that redundant information is non-negative, we have:
Red(X ,X ;Y),Red(X ,X ;(X ,X )|Y) ≥ 0. (C5)
1 2 1 2 1 2
21
Combining this with Eq.(C1) and Eq.(C4), we conclude that:
Red(X ,X ;Y) = Red(X ,X ;(X ,X )|Y) = 0. (C6)
1 2 1 2 1 2
Next, we restate the content of Theorem 1 and provide a proof:
In a trivariable system, the flexibility defined in Eq. (7) is the synergistic information of
X
˜t
and E
˜t
with respect to X
˜t+1.
Proof 2 Given that we intervene Xt and Et to achieve a uniform distribution, resulting in
X
˜t
and E
˜t,
it follows that X
˜t
⊥ E
˜t.
Based on Lemma 1, we have:
Red(X
˜t,E ˜t;X ˜t+1)
= 0. (C7)
Drawing on the relationship between information atoms and mutual information, as given
in Eq. (A4):
Un(X
˜t;X ˜t+1|E ˜t)
= I(X
˜t;X ˜t+1)−Red(X ˜t,E ˜t;X ˜t+1)
= I(X
˜t;X ˜t+1).
(C8)
The same applies to Un(E
˜t;X ˜t+1|X ˜t).
Since E
˜t
is also uniformly distributed, according
to the definition and expression of effective information (please refer to Appendix B), we
obtain:
I(X
˜t;X ˜t+1)
= EI(P ). (C9)
Xt→Xt+1
Based on Eqs. (A6) and (C7), we derive the following:
Syn(X
˜t,E ˜t;X ˜t+1)
= I(X
˜t,E ˜t;X ˜t+1)−I(X ˜t;X ˜t+1)−I(E ˜t;X ˜t+1)
(C10)
= EI(P )−EI(P )−EI(P ).
Xt,Et→Xt+1 Xt→Xt+1 Et→Xt+1
This is precisely the flexibility defined in Eq. (7) in the main text.
2. Proofs of the Upper Bound of Synergy
Werestatethepropertyasfollows: TheupperboundofSyn(X
˜t,E ˜t;X ˜t+1)ismin{I(X ˜t;X ˜t+1|E ˜t),
I(E
˜t;X ˜t+1|X ˜t)}.
Proof 3 According to the definition of EI,
EI(P ) = I(X
˜t;X ˜t+1),
(C11)
Xt→Xt+1
22
while still satisfying do(Xt,Et ∼ U(Ω )). Therefore, by the chain rule of mutual informa-
X,E
tion, we have
Syn(X
˜t,E ˜t;X ˜t+1)
= I(X
˜t,E ˜t;X ˜t+1)−I(X ˜t;X ˜t+1)−I(E ˜t;X ˜t+1)
(C12)
= I(X
˜t;X ˜t+1|E ˜t)−I(X ˜t;X ˜t+1)
Due to the non-negativity of mutual information, it follows that Syn(X
˜t,E ˜t;X ˜t+1)
≤
I(X
˜t;X ˜t+1|E ˜t).
Similarly, Syn(X
˜t,E ˜t;X ˜t+1)
≤ I(E
˜t;X ˜t+1|X ˜t).
Thus, min{I(X
˜t;X ˜t+1|E ˜t),I(E ˜t;X ˜t+1|X ˜t)}
is the upper bound of Syn(X
˜t,E ˜t;X ˜t+1).
3. Proofs of Corollary 1
We first restate the definition of flexibility, namely Eq. 7,
Syn(P ) = EI(P )−EI(P )−EI(P ). (C13)
Xt,Et→Xt+1 Xt,Et→Xt+1 Xt→Xt+1 Et→Xt+1
as well as the expression for EI, namely Eq. 10,
(cid:32) (cid:33) (cid:32) (cid:33)
N N
1 (cid:88) 1 (cid:88)
EI(P ) = − H(P ) +H P (C14)
X→Y i i
N N
i=1 i=1
Substituting Eq. (10) into Eq. (7), we have
(cid:32) (cid:33)
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
Syn(P ) = − H(P )+H P
Xt,Et→Xt+1
|Ω |
x,e
|Ω |
x,e
X,E X,E
x∈ΩXe∈ΩE x∈ΩXe∈ΩE
(cid:32) (cid:32) (cid:33) (cid:32) (cid:33)(cid:33)
1 (cid:88) 1 (cid:88) 1 (cid:88) (cid:88)
− − H P +H P
x,e x,e
|Ω | |Ω | |Ω |
X E X,E
x∈ΩX e∈ΩE x∈ΩXe∈ΩE
(cid:32) (cid:32) (cid:33) (cid:32) (cid:33)(cid:33)
1 (cid:88) 1 (cid:88) 1 (cid:88) (cid:88)
− − H P +H P
x,e x,e
|Ω | |Ω | |Ω |
E X X,E
e∈ΩE x∈ΩX x∈ΩXe∈ΩE
(cid:32) (cid:33) (cid:32) (cid:33)
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
= H P + H P
x,e x,e
|Ω | |Ω | |Ω | |Ω |
E X X E
e∈ΩE x∈ΩX x∈ΩX e∈ΩE
(cid:124) (cid:123)(cid:122) (cid:125)
expansiveness
(cid:32) (cid:33)
1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
2log |Ω |− H(P )−H P −2log |Ω |
2 X |Ω | x,e |Ω | x,e 2 X
X,E X,E
x∈ΩXe∈ΩE x∈ΩXe∈ΩE
(cid:124) (cid:123)(cid:122) (cid:125)
introversion
= Exp(P )+Int(P )−2log |Ω |.
Xt,Et→Xt+1 Xt,Et→Xt+1 2 X
(C15)
23
Appendix D: Additional Experiments with GRN Data
To illustrate the intuitive connection between flexibility and a system’s flexible response
to environmental changes, we plotted the scatter diagram shown in Figure 8. Here, MIs
represent the average magnitude of mutual information between consecutive moments for
various experimental conditions. It takes into account the impact of environmental changes
on the system, accumulating mutual information only when environmental alterations lead
to shifts in system steady states; otherwise, the system’s mutual information under the new
environment is recorded as 0. Additionally, since it measures the system’s own mutual infor-
mation, it quantifies whether the system maintains maximal intrinsic information transfer
across any environment. Although the calculation of MIs is affected by sampling and does
not have an exact correspondence with flexibility, the trend of the fitted line indicates a
positive correlation between the two. Their Pearson correlation coefficient is 0.487. Con-
ducting a hypothesis test with the null hypothesis of no significant correlation between the
two yields a p-value less than 0.05, indicating a significant correlation. The network struc-
ture and function settings are derived from the GRN controlling apoptosis as described in
the data from [26].
[1] S. A. Kauffman, The origins of order: Self-organization and selection in evolution (Oxford
University Press, 1993).
[2] C. G. Langton, Computation at the edge of chaos: Phase transitions and emergent computa-
tion, Physica D: nonlinear phenomena 42, 12 (1990).
[3] J. S. Lansing, Complex adaptive systems, Annual review of anthropology 32, 183 (2003).
[4] K. J. Friston and K. E. Stephan, Free-energy and the brain, Synthese 159, 417 (2007).
[5] J. H. Holland and J. H. Miller, Artificial adaptive agents in economic theory, The American
economic review 81, 365 (1991).
[6] J. H. Holland, Adaptation in natural and artificial systems: an introductory analysis with
applications to biology, control, and artificial intelligence (MIT press, 1992).
[7] J. H. Holland, Complex adaptive systems, Daedalus 121, 17 (1992).
24
FIG. 8. We searched all three-node subgraph structures in the GRN of apoptosis and calculated
their flexibility (Syn) as well as the mean mutual information (MIs) between consecutive moments
when environmental state changes lead to shifts in system steady states. The calculation of MIs
considered all possible initial states of the system and all possible environmental states, with the
sequence of environmental state transitions being randomly determined each time. This allowed
us to generate a scatter plot of MIs versus Syn, with the red line representing the fitted linear
regression of the plot.
[8] F. Rosas, P. A. Mediano, M. Ugarte, and H. J. Jensen, An information-theoretic approach
to self-organisation: Emergence of complex interdependencies in coupled dynamical systems,
Entropy 20, 793 (2018).
[9] C. Gershenson and N. Fern´andez, Complexity and information: Measuring emergence, self-
organization, and homeostasis at multiple scales, Complexity 18, 29 (2012).
[10] D. Krakauer, N. Bertschinger, E. Olbrich, J. C. Flack, and N. Ay, The information theory of
individuality, Theory in Biosciences 139, 209 (2020).
[11] E. P. Hoel, L. Albantakis, and G. Tononi, Quantifying causal emergence shows that macro
can beat micro, Proceedings of the National Academy of Sciences 110, 19790 (2013).
[12] M. Oizumi, L. Albantakis, and G. Tononi, From the phenomenology to the mechanisms of
consciousness: integrated information theory 3.0, PLoS computational biology 10, e1003588
(2014).
[13] E. P. Hoel, When the map is better than the territory, Entropy 19, 188 (2017).
25
[14] B.Yuan, J.Zhang, A.Lyu, J.Wu, Z.Wang, M.Yang, K.Liu, M.Mou,andP.Cui,Emergence
and causality in complex systems: A survey of causal emergence and related quantitative
studies, Entropy 26, 108 (2024).
[15] J. Zhang, R. Tao, K. H. Leong, M. Yang, and B. Yuan, Dynamical reversibility and a new
theory of causal emergence, arXiv preprint arXiv:2402.15054 (2024).
[16] M. Yang, Z. Wang, K. Liu, Y. Rong, B. Yuan, and J. Zhang, Finding emergence in data by
maximizing effective information, National Science Review , nwae279 (2024).
[17] T. F. Varley, Flickering emergences: The question of locality in information-theoretic ap-
proaches to emergence, Entropy 25, 54 (2022).
[18] J. Pearl and D. Mackenzie, The book of why: the new science of cause and effect (Basic books,
2018).
[19] P. L. Williams and R. D. Beer, Nonnegative decomposition of multivariate information, arXiv
preprint arXiv:1004.2515 (2010).
[20] J. T. Lizier, N. Bertschinger, J. Jost, and M. Wibral, Information decomposition of target
effects from multi-source interactions: Perspectives on previous, current and future work
(2018).
[21] N. Bertschinger, J. Rauh, E. Olbrich, and J. Jost, Shared information—new insights and
problems in decomposing information in complex systems, in Proceedings of the European
conference on complex systems 2012 (Springer, 2013) pp. 251–269.
[22] U. Alon, An introduction to systems biology: design principles of biological circuits (Chapman
and Hall/CRC, 2019).
[23] L. Albantakis, L. Barbosa, G. Findlay, M. Grasso, A. M. Haun, W. Marshall, W. G. Mayner,
A.Zaeemzadeh,M.Boly,B.E.Juel,et al.,Integratedinformationtheory(iit)4.0: formulating
the properties of phenomenal existence in physical terms, PLoS computational biology 19,
e1011465 (2023).
[24] P. L. Williams, Information dynamics: Its theory and application to embodied cognitive sys-
tems, Ph.D. thesis, Indiana University (2011).
[25] S. Wolfram, Statistical mechanics of cellular automata, Reviews of modern physics 55, 601
(1983).
[26] C. Kadelka, T.-M. Butrie, E. Hilton, J. Kinseth, A. Schmidt, and H. Serdarevic, A meta-
analysis of boolean network models reveals design principles of gene regulatory networks,
26
Science Advances 10, eadj0822 (2024).
[27] P. Orio, P. A. Mediano, and F. E. Rosas, Dynamical noise can enhance high-order statistical
structure in complex systems, Chaos: An Interdisciplinary Journal of Nonlinear Science 33
(2023).
[28] S. Watanabe, Information theoretical analysis of multivariate correlation, IBM Journal of
research and development 4, 66 (1960).
[29] H. Te Sun, Multiple mutual informations and multiple interactions in frequency data, Inf.
Control 46, 26 (1980).
[30] R. Yeung, Information theory and network coding (Springer, 2008).
[31] M. Harder, C. Salge, and D. Polani, Bivariate measure of redundant information, Physical
Review E—Statistical, Nonlinear, and Soft Matter Physics 87, 012130 (2013).
[32] C. Finn and J. T. Lizier, Pointwise partial information decompositionusing the specificity and
ambiguity lattices, Entropy 20, 297 (2018).
[33] G. Tononi and O. Sporns, Measuring information integration, BMC neuroscience 4, 1 (2003).
27

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Quantifying system-environment synergistic information by effective information decomposition"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
