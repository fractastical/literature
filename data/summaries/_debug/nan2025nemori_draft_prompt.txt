=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science
Citation Key: nan2025nemori
Authors: Jiayan Nan, Wenquan Ma, Wenlong Wu

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: LargeLanguageModels(LLMs)demonstrateremarkablecapabilities,yettheirinabilitytomaintain
persistentmemoryinlongcontextslimitstheireffectivenessasautonomousagentsinlong-terminter-
actions. Whileexistingmemorysystemshavemadeprogress,theirrelianceonarbitrarygranularity
fordefiningthebasicmemoryunitandpassive,rule-basedmechanismsforknowledgeextraction
limitstheircapacityforgenuinelearningandevolution. Toaddressthesefoundationallimitations,we
presentNemori,anovelself-organizingmemoryarchitectureinspire...

Key Terms: china, shanghai, inspired, memory, organizing, cognitive, agent, nemori, self, llms

=== FULL PAPER TEXT ===

NEMORI: SELF-ORGANIZING AGENT MEMORY INSPIRED BY
COGNITIVE SCIENCE
JiayanNan*,† WenquanMa*,‡ WenlongWu** YizeChen***
†SchoolofComputerScienceandTechnology,TongjiUniversity,Shanghai,China
‡SchoolofStatisticsandDataScience,ShanghaiUniversityofFinanceandEconomics,Shanghai,China
**SchoolofInstrumentationandOptoelectronicEngineering,BeihangUniversity,Beijing,China
***TankaAI
{njy@tongji.edu.cn, wenquan.ma@stu.sufe.edu.cn, wlw@buaa.edu.cn, chenyize@tanka.ai}
ABSTRACT
LargeLanguageModels(LLMs)demonstrateremarkablecapabilities,yettheirinabilitytomaintain
persistentmemoryinlongcontextslimitstheireffectivenessasautonomousagentsinlong-terminter-
actions. Whileexistingmemorysystemshavemadeprogress,theirrelianceonarbitrarygranularity
fordefiningthebasicmemoryunitandpassive,rule-basedmechanismsforknowledgeextraction
limitstheircapacityforgenuinelearningandevolution. Toaddressthesefoundationallimitations,we
presentNemori,anovelself-organizingmemoryarchitectureinspiredbyhumancognitiveprinciples.
Nemori’s core innovation is twofold: First, its Two-Step Alignment Principle, inspired by Event
Segmentation Theory, provides a principled, top-down method for autonomously organizing the
rawconversationalstreamintosemanticallycoherentepisodes,solvingthecriticalissueofmemory
granularity. Second,itsPredict-CalibratePrinciple,inspiredbytheFree-energyPrinciple,enables
theagenttoproactivelylearnfrompredictiongaps,movingbeyondpre-definedheuristicstoachieve
adaptiveknowledgeevolution. Thisoffersaviablepathtowardhandlingthelong-term,dynamic
workflows of autonomous agents. Extensive experiments on the LoCoMo and LongMemEval
S
benchmarksdemonstratethatNemorisignificantlyoutperformspriorstate-of-the-artsystems,withits
advantagebeingparticularlypronouncedinlongercontexts.
Code: The MVP implementation of Nemori is available as open-source software at https://github.com/
nemori-ai/nemori.
1 Introduction
TheamnesiaofLargeLanguageModels(LLMs)standsasthecorebottlenecktothegrandervisionofautonomous
agentscapableofgenuinelearningandintelligentself-evolution. Forinstance,LLMsexhibitastriking,seemingly
personalizedcontextualabilitywithinasingleinteraction;however,thisillusionshatterswithnewsessions,astheagent,
devoidofpriorhistory,greetstheuserasastranger. Thisinabilitytomaintainlong-termmemoryprimarilystemsfrom
twocoretechnicalconstraints:theirlimitedcontextwindow,rootedinquadraticO(n2)attentioncomplexity[1],andthe
LostintheMiddlephenomenon,hinderingeffectiveinformationutilizationinlongcontexts[2]. Consequently,unless
long-termmemoryofuserinteractionscanbeeffectivelyaddressed,thegrandervisionofhuman-likeself-evolutionfor
agentswillremainunattainable[3,4,5].
Fortunately,thesolutiontothisamnesialiesinselectiveinformationprovisionviaIn-ContextLearning(ICL,6),a
principlemostprominentlyrealizedatscalebytheRetrieval-AugmentedGeneration(RAG,7)framework. Analogous
to human selective recall, the key lies in storing vast information in an organized manner for efficient, contextual
retrieval[8]. RAGeffectivelygroundsLLMsinexternalfactualdocuments,mitigatinghallucinationandproviding
1*Equalcontribution.
2Correspondingauthor:JiayanNan(njy@tongji.edu.cn).
5202
guA
72
]IA.sc[
3v14330.8052:viXra
Nemori–arXivPreprint
domain-specificknowledge[9,10]. Thisparadigm’ssuccessisbuiltuponapowerful,retrieval-centricphilosophy[11].
Thisnaturallyraisesacompellingquestion: couldthispowerfulphilosophyalsoberepurposedtoaugmentanagent’s
memoryofitsownpast,solvingitsamnesia?
However,RAG’scorecharacteristics,designedforstaticknowledgebases,fundamentallymisalignwiththedemands
ofdynamicconversation,givingrisetoMemory-AugmentedGeneration(MAG,12),anewparadigmfocusedonthe
self-organizationofanagent’sownlivedexperience. Thismisalignmentmanifestsonthreecriticallevels: itsstateless
informationpatchapproachpreventsstatefullearning[13,14];itsrelianceonofflineindexingisantitheticaltoonline
conversationalstreamprocessing[15];anditsfocusonfactretrievalprovesinsufficientforthecomplex,local-global
reasoninginherentindialogue[16]. ThisprofoundparadigmmismatchhighlightsMAG’snecessity, transforming
traditionalretrievalfromstaticlibrariesintoanautonomousprocessoforganizinganagent’slived,temporalexperiences
intoanoptimizedrepresentation[5]. MAGisessentialnotonlyforcoherentconversationsbutalsoasafoundational
componentforachievingthelong-termgoalofagentself-evolution.
WhiletheMAGparadigmrepresentsasignificantstepbeyondtraditionalRAG,existingmethodshaveyettounlock
thefullpotentialofhuman-likeself-organization. Wearguethatthisstemsfromafundamentalneglectoftheself
aspectofself-organization. Thequalityofafinalmemoryunit,y,iscriticallylimitedbyalackofself,whichmanifests
as two sequential challenges, y = f(x): the input chunks (x) and the organizing mechanism itself (f). The first
challengeconcernstheinputchunks(x). ExistingMAGsystemsoftenadoptarbitraryorunspecifiedsegmentation,
inherently leading to a loss of contextual information. The root cause of this is the failure to leverage the agent’s
ownself capabilitiestoautonomouslyconverttherawstreamintosemanticallycoherentchunks. Thesecond,more
advancedproblemistheorganizingfunctionitself(f). Existingmethodsstruggletobalanceretainingdetailswith
forming abstractions, resulting in redundant or incomplete memory representations. This stems from a failure to
recognizethatmemorynaturallyfollowsadualcognitivestructure,whichseparatesepisodicdetailsfromsemantic
knowledge. Crucially,(f)alsolacksaproactiveself-learningmechanismtobridgethegapbetweenmemoryandthe
rawconversation. Existingsystemswithinapre-definedpromptarelimitedtowhatfitsthepresetschema,highlighting
theneedforanend-to-end,proactiveprocessthatintrinsicallyfocusesonnon-redundantcontentgeneration.
Toaddressthesefundamentalchallenges,weintroduceNemori,anovelself-organizingmemoryarchitecturebuilt
upon a dual-pillar cognitive framework. This framework offers a principled solution to the sequential challenges
of defining the input chunks (x) and designing the organizing function (f). First, for the input chunking problem
(x), the framework’s Two-Step Alignment Principle offers a principled, top-down solution. It begins with the
Boundary Alignment step, which, inspired by Event Segmentation Theory [17], autonomously organizes the raw
conversationalstreamintosemanticallycoherentexperiencechunks. Computationally,weachievethisbyadaptingand
simplifyingtechniquesfromdialoguetopicsegmentation,allowingNemoritomovebeyondarbitrarysegmentation.
Second,thechallengeoftheorganizingfunction(f)isaddressedthroughatwo-prongedapproach. Theinitialstepis
RepresentationAlignment(asub-principleoftheTwo-StepAlignmentPrinciple),whichsimulatesthenaturalhuman
narrationofEpisodicMemory[18]totransformrawchunksintorich,narrativememory. Thisiscomplementedbyour
Predict-CalibratePrinciple,aproactivelearningmechanisminspiredbytheFree-energyPrinciple[19]. Thisprinciple
Problem Input Chunks Organizing Function
(x) (f)
Two-Step Alignment
Principle Boundary Alignment Representation Predict-Calibrate
Alignment Principle
Computation Topic Segmentation Episodic Memory Semantic Mem-
Generation ory Generation
Figure1: TheconceptualframeworkofNemori,illustratingthemappingfromproblemtoprincipletocomputation.
The framework addresses two core challenges: defining appropriate input chunks (x) and designing an effective
organizingfunction(f). TheTwo-StepAlignmentPrinciple(comprisingBoundaryAlignmentandRepresentation
Alignment)solvestheinputchunkingandinitialrepresentationproblem. Concurrently,thePredict-CalibratePrinciple
providesaproactivemechanismfortheorganizingfunction,whichoperationalizesthemviathreecoremodules: Topic
Segmentation,EpisodicMemoryGeneration,andSemanticMemoryGeneration,asillustratedhere.
2
Nemori–arXivPreprint
positsthatgenuinelearningstemsfromactivelydistillingpredictiongaps,analogoustotheeffectivehumanstrategy
ofattemptingataskbeforereflectingondiscrepanciesagainstastandard,whichfostersdeeperunderstandingthan
passivelyreviewingsolutions. Together,theseprinciplesformasynergistic,complementarylearningsystem[20]that
underpinsNemori’sarchitecture,whichoperationalizesthemviathreecoremodules: TopicSegmentation,Episodic
MemoryGeneration, andSemanticMemoryGeneration, illustratedinFigure1. Ourmaincontributionsareas
follows: (1)Weproposeanovel,dual-pillarframeworkfordynamicconversationalmemory,inspiredbycognitive
science: theTwo-StepAlignmentPrincipleforfaithfulexperiencerepresentationandthePredict-CalibratePrinciple
for proactive knowledge distillation. (2) We design and implement Nemori, a complete memory architecture that
operationalizesthisframework,incorporatingtechnicalinnovationslikeatop-downintelligentboundarydetectorandan
asynchronouspredict-calibratepipeline. (3)WedemonstrateNemori’seffectivenessandrobustnessthroughextensive
experiments,confirmingitsignificantlyoutperformspriorstate-of-the-artsystemsontheLoCoMoandLongMemEval
S
benchmarks,withitsadvantagebeingparticularlypronouncedinlongercontexts.
2 RelatedWorks
2.1 BeyondStaticRAG:TheFrontierofStreamingMemory
Ourworkissituatedwithinthebroadparadigmofnon-parametricmemory,whichenhancesLLMswithanexternal
memory store, distinct from parametric memory [21] or hybrid approaches [22]. Within this domain, Retrieval-
AugmentedGeneration(RAG,7)isdominant,designedforretrievingfromstaticknowledgebasestogroundLLMsin
externalfactsandprovidedomain-specificknowledge[9,10]. Incontrast,ourworkcontributestoMemory-Augmented
Generation(MAG),adistinctcurrentfocusingonconstructingandretrievingfromadynamicmemoryofanagent’s
ownlived,temporalexperiences[5].
2.2 TheInputChunkChallenge(x): SolvingtheGranularityGapinAgentMemory
Afoundational,yetoftenoverlooked,challengewithintheMAGparadigmisdefiningthebasicunitofexperience,
theinputchunk(x). Cognitivesciencesuggestsanidealmemoryunitshouldcorrespondtoacoherentevent[17],yet
priorworkrevealsaspectrumofheuristic-basedandincompleteapproachesthatneglecttheagent’sself capabilityto
autonomouslydefinetheseunits.
Prevailing methods often adopt arbitrary or heuristic segmentation. The most primitive approaches use a Single
Message(anindependentuserinputorsystemoutput,23,24)oranInteractionPair(abundled“userinput+system
response”,25,26),resultinginfragmentedmemoriesthatlackbroadersemanticcontext. Toimprovecoherence,other
systemsemployexternalstructureslikePre-definedSessions(externalstructures,27,28)oroutsourcethetaskvia
User-definedChunks(outsourcingthedefinitiontohumanusers,29). Whileproducinghigher-qualitychunks,these
methodscompromisescalabilityduetosignificantoperationaloverheadandlimitedautomation.
Concurrently, asignificantbodyofworkfocusesonmemorystorageandretrieval, treatingthememoryunitasan
UnspecifiedUnitorablackbox[30,31,32,12]. Thesesystemsadvancememorymanagementbutdonotaddressthe
foundationalissueofhowmeaningfulunitsareformedinthefirstplace. Figure2visuallyillustratesthelimitationsof
theseapproachescomparedtoourproposedepisodicmethod.
Thisbringsustothefrontier: theSelf-organizedEpisode. PioneeringworklikeEM-LLMoperationalizedthisvia
abottom-up,token-levelmechanismbasedonpredictivesurprise[33],whichcontrastswiththetop-downreasoning
requiredforholisticsocialinteractions. Whiletechniquesfromdialoguetopicsegmentationexist[34],theirpurpose
isgeneraltopicanalysis,notcreatingmemoryunitsforagents. Thiscallsforafullyautomated,top-down,cognitive-
groundedapproachtomodelunderlyingevents,achallengeourTwo-StepAlignmentPrincipleisdesignedtosolve,
beginningwithitsBoundaryAlignmentstep.
2.3 TheOrganizingFunctionChallenge(f): FromPassiveStoragetoProactiveLearning
Beyondthegranularityofinputchunks(x),thesecondkeychallengeistheorganizingfunction(f),whichgoverns
howmemoryisstructuredandevolved. Dual-memorysystems,whichtypicallymaintainrawepisodicmemoriesand
abstracted semantic knowledge, form a prominent approach, theoretically rooted in the Complementary Learning
Systemstheory(CLS,20)whichpositsthatthebrainusescomplementaryfast-learning(episodic)andslow-learning
(semantic)systems.
However, pioneering systems applying this concept, such as HEMA [25] and Mem0 [26], address the organizing
function(f)withsignificantlimitations. EarlyworklikeHEMAappliedCLStheorybyusingpassivesummarizationto
createglobalsummariesfromrawdialogue. Mem0advancedthisbyextractingconsolidatedfactualentries,which
3
Nemori–arXivPreprint
enhancessemanticqueryabilitybutoftencomesatthecostofcompromisingtheoriginalepisodiccontext.Consequently,
formemoryrepresentation,bothapproachesrelyonsimplifiedtransformationsratherthanaprincipled,cognitively-
inspired method for narrative generation, a challenge our Representation Alignment principle addresses. More
critically, their mechanism for knowledge evolution remains a passive, extraction-based process. This reveals an
unaddressedgap: theabsenceofaproactivelearningmechanismfortheagenttoautonomouslyevolveitsknowledge
base. OurworkfillsthisgapwiththePredict-CalibratePrinciple. InspiredbytheFree-energyPrinciple[19],itmoves
beyondpassiveextraction(e.g. pre-definedextractionrules)byactivelydistillingpredictiongaps,enablingthesystem
tolearnfromitsownerrorsforatrulysynergistic,complementarylearningprocess.
3 Methodology
TheNemorimethodologyprovidesaconcretecomputationalimplementationofourdual-pillarcognitiveframework:the
Two-StepAlignmentPrincipleandthePredict-CalibratePrinciple. AsillustratedinFigure3,thesystemiscomposed
ofthreecoremodules: TopicSegmentation, EpisodicMemoryGeneration, andSemanticMemoryGeneration, all
supportedbyaunifiedretrievalsystem. Thefirsttwomodules,TopicSegmentationandEpisodicMemoryGeneration,
work in concert to operationalize the Two-Step Alignment Principle for faithful experience representation. The
third module, Semantic Memory Generation, is designed to realize the Predict-Calibrate Principle for proactive
knowledgeevolution. Inthefollowingsections,wewilldetailthemechanismsofeachcomponentinaccordancewith
thisprinciple-drivenstructure.
3.1 TheTwo-StepAlignmentPrincipleinPractice
The first principle is operationalized through two sequential modules: a boundary detector that realizes semantic
BoundaryAlignment,andanepisodegeneratorthatrealizesnarrativeRepresentationAlignment.
3.1.1 BoundaryAlignmentviaIntelligentDetection.
Theprocessofidentifyingepisodicboundariesbeginswithamessagebuffer,denotedasB foreachuseru,which
u
accumulatesincomingconversationalmessages. Thesequenceofmessagesinthebufferatagiventimetisrepresented
asM = {m ,m ,...,m },whereeachmessagem isatuple(ρ ,c ,τ )containingtheroleρ ∈ {user,assistant},
1 2 t i i i i i
themessagecontentc ,andthetimestampτ .
i i
Foreachnewincomingmessagem ,anLLM-basedboundarydetector,f ,isinvokedtodetermineifameaningful
t+1 θ
semanticboundaryhasbeencrossed. Ratherthanasimpleprobability,thedetector’soutputisastructuredresponse
containingbothabooleandecisionandaconfidencescore:
(b ,c )=f (m ,M) (1)
boundary boundary θ t+1
whereb ∈{True,False}andc ∈[0,1]. Thefunctionf makesthisdeterminationbyevaluatingseveral
boundary boundary θ
factors,includingcontextualcoherence(i.e.,thesemanticsimilaritybetweenmessages),temporalmarkers(e.g.,bythe
way),shiftsinuserintent(e.g.,fromaskingforinformationtomakingadecision),andotherstructuralsignals.
Standard RAG Interaction Pair Episode
[2024-12-01] User: What color is apple? What color is apple? What color is apple?
[2024-12-01] Assistant: It is red.
[2024-12-01] User: Yes! I love it. It is red. It is red.
Yes!I love it.
[ [ 2 2 0 0 2 2 4 4 - - 1 1 2 2 - - 0 0 1 2 ] ] A U s s s e i r s : t a G n o t o : d It m is o v r e n r i y n g h ! ealthy! Yes!I love it. It is very healthy!
It is very healthy!
Good morning!
Figure2: Anillustrationofdifferentconversationsegmentationmethods. StandardRAG(left)oftenreliesonarbitrary,
fixed-sizechunking,whichcanbreakthesemanticintegrityofadialogue(asshownbythesplitintheapplediscussion).
TheInteractionPairmodel(middle)groupsuser-assistantturnsbutcanstillseparaterelatedusermessages. Incontrast,
our proposed Episodic segmentation (right), guided by semantic boundary detection, correctly groups the entire
conversationabouttheappleintoasingle,coherentepisode,preservingtheinteraction’slogicalflow.
4
Nemori–arXivPreprint
Topicsegmentationistriggeredwheneitheroftwoconditionsismet: ahigh-confidencesemanticshiftisdetected,or
thebufferreachesitscapacity. Thisisformallyexpressedas:
T=(b ∧c >σ )∨(|M|≥β ) (2)
boundary boundary boundary max
where σ is a configurable confidence threshold, |M| is the number of messages in buffer M, and β is a
boundary max
predefinedmaximumbuffersize. Upontriggering(i.e.,whenT=True),themessagesequenceM ispassedtothenext
moduleforepisodicmemorygeneration,leavingthenewmessagem toinitializethesubsequentbuffer.
t+1
3.1.2 RepresentationAlignmentviaNarrativeGeneration.
TheEpisodicMemoryGenerationmodulereceivestheSegmentedConversation,denotedasM,uponthedetectionofa
boundary. Itspurposeistotransformthisrawsegmentintoastructuredepisodicmemory,e. Thistransformationis
performedbyanLLM-basedEpisodeGenerator,g ,whichreframesthesegmenteddialogueintoacoherent,narrative
ϕ
representation. Theoutputofthisprocessisastructuredtuple:
e=(ξ,ζ)=g (M) (3)
ϕ
whereξrepresentsaconcisetitlethatencapsulatestheepisode’scoretheme,andζ isadetailedthird-personnarrative
thatpreservesthesalientinformationandcontextoftheinteraction. Thisstructuredformatofcombiningatitlewitha
richnarrativealignswithourRepresentationAlignmentprinciple. Subsequently,thecompleteepisodicmemoryeis
storedintheEpisodicMemoryDatabase,whileitstitleξ ispassedtotheSemanticMemoryGenerationmoduleto
initiatethelearningcycle.
3.2 ThePredict-CalibratePrincipleinPractice
Thesecond,proactivelearningprinciple,Predict-Calibrate,isoperationalizedbytheSemanticMemoryGeneration
module. Thiscomponentservesasthecoreofagentlearningandevolution, implementinganovelmechanismfor
incrementalknowledgeacquisitioninspiredbytheFreeEnergyPrinciplefromcognitivescience[19]. Asdepictedin
Figure3,thislearningprocessoperatesinathree-stagecycle.
3.2.1 Stage1: Prediction.
Thecyclebeginswhenthemodulereceivesthetitleξofanewlygeneratedepisodee =(ξ,ζ). Thefirststageofthe
new
cycleistoforecasttheepisode’scontentbasedonexistingknowledge. Thisprocessunfoldsintwomainparts: first
retrievingrelevantmemories,andthenmakingtheprediction.
MemoryRetrieval. ToidentifyrelevantknowledgefromtheSemanticMemoryDatabaseK,thesystemretrievesa
setofrelevantmemories,K ,forthenewepisode’scontent. Thisretrievalisperformedbyourunifiedretrieval
relevant
mechanism,whichtakestheembeddingofthenewepisode’sconcatenatedtitleandcontentasaquery,alongwiththe
Topic Segmentation Episodic Memory Generation Semantic Memory Generation
Semantic
Memory
New TITLE TITLE Semantic
Message Memory DB
Content Episodic
Memory Episode Possible
Predictor Episode?
Message Buffer
Episodic Memory
DB
Episode Predicted
Generator Episode
Semantic
Topic Memory
Change?
What's
Gap?
Boundary Detector Segmented Conversation Semantic Knowledge
Distiller
Figure3: TheNemorisystemfeaturesthreemodules: TopicSegmentation,EpisodicMemoryGeneration,andSemantic
MemoryGeneration. ItsegmentsconversationsintoEpisodicMemory,thenusesaPredict-Calibratecycletodistillnew
SemanticMemoryfrompredictiongapsagainstoriginalconversations.
5
Nemori–arXivPreprint
semanticmemorydatabaseK,amaximumnumberofresultsm,andaconfigurablesimilaritythresholdσ . Theresult
s
isthedefinitivesetofrelevantmemories:
K =Retrieve(embed(ξ⊕ζ),K,m,σ ) (4)
relevant s
Thisensureshigh-qualitycontextualinformationforthesubsequentpredictionstage.
EpisodePrediction. Withtherelevantknowledgeretrieved,anLLM-basedEpisodePredictor,h ,thenforecaststhe
ψ
episode’scontent,eˆ,basedontheepisode’stitleξandthefinalsetofrelevantknowledgeK :
relevant
eˆ=h (ξ,K ) (5)
ψ relevant
3.2.2 Stage2: Calibration.
Inthecalibrationstage,thepredictedcontenteˆiscomparedagainstthegroundtruthoftheinteraction. Crucially,this
groundtruthisnotthegeneratedepisodicnarrativeζ,buttheoriginal,unprocessedSegmentedConversationblock,
M. AnLLM-basedSemanticKnowledgeDistiller,r ,processesthiscomparisontoidentifythepredictiongap—the
ω
novelorsurprisinginformationthattheexistingknowledgebasefailedtopredict. Fromthisgap,anewsetofsemantic
knowledgestatements,K ,isdistilled:
new
K =r (eˆ,M) (6)
new ω
3.2.3 Stage3: Integration.
Finally,thenewlygeneratedandvalidatedknowledgestatements,K ,areintegratedintothemainSemanticMemory
new
DatabaseK. Thiscompletesthelearningcycle,enrichingtheagent’sknowledgebaseandrefiningitsinternalmodelof
theworld.
3.3 UnifiedMemoryRetrieval
The system employs a unified vector-based retrieval approach, denoted as Retrieve(q,D,m,σ ), optimized for
s
accessingbothepisodicandsemanticmemories. Thisfunctiontakesaqueryq,amemorydatabaseD,amaximum
number of results m, and an optional similarity threshold σ to return a set of relevant memories. The retrieval
s
mechanism uses dense vector search with cosine similarity to identify semantically relevant memories through a
three-stageprocess: similaritycomputation,candidateselection,andthreshold-basedfiltering.
4 Experiment
Inthissection,weconductaseriesofexperimentsontwobenchmarkdatasetstoinvestigatetheeffectivenessofNemori.
Ourresearchisdesignedtoaddressthefollowingkeyresearchquestions(RQs):
RQ1: HowdoesNemoriperforminlong-termconversationalmemorytaskscomparedtostate-of-the-artmethods?
RQ2: WhatarethecontributionsofNemori’skeycomponentstoitsoverallperformance?
RQ3: Howdoesthemodel’sperformancechangewithadjustmentstothenumberofretrievedepisodicmemories?
RQ4: HowwelldoesNemoriscaletosignificantlylongerandmorechallengingconversationalcontexts?
4.1 ExperimentalSetup
4.1.1 Datasets.
WeevaluateNemoriontwodistinctbenchmarkstoensureacomprehensivevalidationofourapproach.
• LoCoMo [35]: 10 dialogues with 24K average tokens, featuring 1,540 questions across four reasoning
categories.
• LongMemEval [36]: 500conversationswith105Kaveragetokens. WhilestructurallysimilartoLoCoMo,it
S
presentssignificantlygreaterchallengesthroughlonger,morerealisticconversationalcontexts,allowingusto
assessscalabilityunderdemandingconditions.
6
Nemori–arXivPreprint
4.1.2 Baselines.
WebenchmarkNemoriagainstfivepowerfulandrepresentativebaselines,categorizedasfollows:
• StandardMethod: FullContext,whichprovidestheentiredialoguehistorytotheLLM,representingthe
theoreticalupperboundofinformationavailability.
• Retrieval-AugmentedMethod: RAG-4096,astandardretrieval-augmentedgenerationapproachthatchunks
dialoguesinto4096-tokensegmentsfordenseretrieval.
• Memory-AugmentedSystems: Wecompareagainstthreestate-of-the-artmemorysystems: LangMem[37],
whichusesahierarchicalmemorystructure;Zep[38],acommercialsolutionbasedontemporalknowledge
graphs;andMem0[26],asystemthatextractsandmaintainspersonalizedmemories.
4.1.3 EvaluationMetrics.
OntheLoCoModataset,ourprimaryevaluationmetricistheLLM-judgescore,whereweemploygpt-4o-miniasthe
judge. WesupplementthiswithF1andBLEU-1scoresforamorecompletepicture. FortheLongMemEval dataset,
S
wealsousetheLLM-judgescore,butwithpromptsadaptedtoitsspecificquestion-answeringformat.
4.1.4 Reproducibility.
Toensurefaircomparison,Mem0andZeputilizetheircommercialAPIstoretrievememorycontexts,whicharethen
fedtogpt-4o-miniandgpt-4.1-miniforanswergeneration. Allothermethods,includingNemori,employgpt-4o-mini
andgpt-4.1-miniasbothinternalbackbonemodelsandanswergenerationmodels. ForNemorispecifically,embeddings
aregeneratedwithtext-embedding-3-small. Keyhyperparametersweresetasfollows: similaritythresholdσ =0.0,
s
boundary detection confidence σ = 0.7, and max buffer size β = 25. For retrieval settings across all
boundary max
experiments,wemaintainafixedratiobetweenepisodicandsemanticmemoryretrieval: weretrievetop-kepisodic
memoriesandtop-m = 2k semanticmemories. Inthemainexperiments, k = 10(thusm = 20), whileinRQ3’s
hyperparameteranalysis,k variesfrom2to20. Tobalanceinformativenessandefficiency,onlythetop-2episodic
memoriesincludetheiroriginalconversationtext,ashigher-similarityepisodestendtobemoreuseful.
4.2 MainResults(RQ1)
ToanswerRQ1,wereporttheperformancecomparisonontheLoCoModatasetinTable1. Ourobservationsareas
follows: SuperiorPerformanceAcrosstheBoard. Nemoriconsistentlyoutperformsallbaselinemethodsacrossboth
Method TemporalReasoning OpenDomain Multi-Hop Single-Hop Overall
LLMScore F1 BLEU-1 LLMScore F1 BLEU-1 LLMScore F1 BLEU-1 LLMScore F1 BLEU-1 LLMScore F1 BLEU-1
inim-o4-tpg
FullContext 0.562±0.004 0.441 0.361 0.486±0.005 0.245 0.172 0.668±0.003 0.354 0.261 0.830±0.001 0.531 0.447 0.723±0.000 0.462 0.378
LangMem 0.249±0.003 0.319 0.262 0.476±0.005 0.294 0.235 0.524±0.003 0.335 0.239 0.614±0.002 0.388 0.331 0.513±0.003 0.358 0.294
Mem0 0.504±0.001 0.444 0.376 0.406±0.000 0.271 0.194 0.603±0.000 0.343 0.252 0.681±0.000 0.444 0.377 0.613±0.000 0.415 0.342
RAG 0.237±0.000 0.195 0.157 0.326±0.005 0.190 0.135 0.313±0.003 0.186 0.117 0.320±0.001 0.222 0.186 0.302±0.000 0.208 0.164
Zep 0.589±0.003 0.448 0.381 0.396±0.000 0.229 0.157 0.505±0.007 0.275 0.193 0.632±0.001 0.397 0.337 0.585±0.001 0.375 0.309
Nemori(Ours) 0.710±0.000 0.567 0.466 0.448±0.005 0.208 0.151 0.653±0.002 0.365 0.256 0.821±0.002 0.544 0.432 0.744±0.001 0.495 0.385
inim-1.4-tpg FullContext 0.742±0.004 0.475 0.400 0.566±0.010 0.284 0.222 0.772±0.003 0.442 0.337 0.869±0.002 0.614 0.534 0.806±0.001 0.533 0.450
LangMem 0.508±0.003 0.485 0.409 0.590±0.005 0.328 0.264 0.710±0.002 0.415 0.325 0.845±0.001 0.510 0.436 0.734±0.001 0.476 0.400
Mem0 0.569±0.001 0.392 0.332 0.479±0.000 0.237 0.177 0.682±0.003 0.401 0.303 0.714±0.001 0.486 0.420 0.663±0.000 0.435 0.365
RAG 0.274±0.000 0.223 0.191 0.288±0.005 0.179 0.139 0.317±0.003 0.201 0.128 0.359±0.002 0.258 0.220 0.329±0.002 0.235 0.192
Zep 0.602±0.001 0.239 0.200 0.438±0.000 0.242 0.193 0.537±0.003 0.305 0.204 0.669±0.001 0.455 0.400 0.616±0.000 0.369 0.309
Nemori(Ours) 0.776±0.003 0.577 0.502 0.510±0.009 0.258 0.193 0.751±0.002 0.417 0.319 0.849±0.002 0.588 0.515 0.794±0.001 0.534 0.456
Table1: DetailedperformancecomparisononLoCoModatasetbyquestiontype. Boldindicatesthebestperformance
foreachmetric.
backbonemodels. Withgpt-4o-mini,NemoriachievesanoverallLLMscoreof0.744,whichalreadysurpassesthe
FullContextbaseline’sscoreof0.723. Withgpt-4.1-mini,Nemorifurtherimprovesto0.794. Thisdemonstratesthe
powerfulcapabilityofNemori’sself-organizingmemorysystem. Moreover,thesystemscalesupeffectivelyasthe
underlyingmodelcapabilitiesstrengthen.
ExceptionalTemporalReasoning. TheadvantageofourmethodisespeciallypronouncedintheTemporalReasoning
category, whereNemoriachievesscoresof0.710and0.776. Thisvalidatestheeffectivenessofourepisode-based
memorystructure, whichnaturallypreservesthechronologicalflow. AkeyreasonforthissuperiorityisNemori’s
abilitytoperform“reasoningduringmemoryformation.” Forinstance,whenfacedwiththequestion“WhendidJon
receivementorship?”,theFull-Contextbaseline,confusedbytheterm“yesterday”intheoriginaltext,incorrectly
answeredwiththeconversationdate(June16). Incontrast,Nemori’sdualmemorysystemretrievedboththerelevant
episodicmemoryandasemanticmemorythathadalreadyprocessedthetemporalinformationintoaclearfact: “Jon
7
Nemori–arXivPreprint
wasmentoredonJune15,2023.” Bycombiningepisodiccontextwithpre-reasonedsemanticfacts,Nemoritransforms
complexreasoningtasksintosimpleinformationretrieval,significantlyboostingaccuracy.
Method LLMScore Tokens Search Total
(ms) (ms)
FullContext 0.723 23,653 – 5,806
LangMem 0.513 125 19,829 22,082
Mem0 0.613 1,027 784 3,539
RAG-4096 0.302 3,430 544 2,884
Zep 0.585 2,247 522 3,255
Nemori 0.744 2,745 787 3,053
Table2: PerformanceandefficiencycomparisononLoCoModatasetwithgpt-4o-mini.
EfficiencyAdvantages. Table2highlightsNemori’sefficiency. Whiledeliveringsuperiorperformance,Nemoriuses
only2,745tokensonaverage,an88%reductioncomparedtothe23,653tokensrequiredbytheFullContextbaseline.
ThisdemonstratesthatNemorinotonlyimprovesaccuracybutdoessowithremarkablecomputationalefficiency.
4.3 AblationStudy(RQ2)
ToanswerRQ2,weconductedanablationstudytoquantifythecontributionofeachkeycomponentinNemori. The
results,summarizedinTable3,leadtoseveralkeyinsights:
OverallPerformance
Method
LLMScore F1 BLEU-1
inim-o4-tpg
w/oNemori 0.006 0.005 0.009
Nemori-s 0.518 0.346 0.272
w/oe 0.615 0.434 0.340
w/os 0.705 0.470 0.370
Nemori 0.744 0.495 0.385
inim-1.4-tpg w/oNemori 0.012 0.016 0.015
Nemori-s 0.623 0.391 0.322
w/oe 0.696 0.461 0.396
w/os 0.756 0.501 0.435
Nemori 0.794 0.534 0.456
Legend:w/oNemori=withoutNemoriframework;w/oe=withoutepisodicretrieval;
w/os=withoutsemanticretrieval;Nemori=fullframework
Table3: AblationstudyonNemoricomponents. Nemori-susesdirectsemanticextraction.
CoreFrameworkNecessity. RemovingtheentireNemoriframework(w/oNemori)causesperformancetocollapseto
near-zero. Thisconfirmsthefundamentalnecessityofastructuredmemoryarchitectureforperformingthesetasks.
ValidationofthePredict-CalibratePrinciple. AnimportantfindingcomesfromcomparingNemori(w/oe)with
Nemori-s. Bothconfigurationsrelysolelyonsemanticmemory,butdiffercriticallyinhowthatmemoryisgenerated.
Nemori(w/oe)usesourproposedPredict-CalibratePrincipletoproactivelydistillknowledge,whileNemori-srelies
onnaive,directextractionfromrawconversationlogs. Theperformancegapbetweenthemissubstantial(e.g.,ascore
of0.615forNemori(w/oe)vs. 0.518fornemori-songpt-4o-mini). Thisresultprovidesadirectempiricalvalidation
ofourprinciple,demonstratingthatproactivelylearningfrompredictiongapsproducesasignificantlymoreeffective
knowledgebasethansimple,reactiveextraction.
ComplementaryRolesofMemoryTypes. Removingeitherepisodicmemory(w/oe)orsemanticmemory(w/os)
fromthefullNemorimodelleadstoperformancedegradation. Thelargerdropfromremovingepisodicmemory(from
0.744to0.615)comparedtosemanticmemory(from0.744to0.705)highlightsthecomplementaryandessentialroles
ofbothmemorysystemsinourdual-memoryarchitecture.
8
Nemori–arXivPreprint
0.76
0.74
0.72
0.70
0.68
2 5 10 15 20 30
Top-k Episodes
erocS
MLL
GPT-4o-mini
0.82
0.80
0.78
0.76
Nemori Performance
default Full Context Baseline 0.74
2 5 10 15 20 30
Top-k Episodes
erocS
MLL
GPT-4.1-mini
Nemori Performance
default Full Context Baseline
Figure4: Impactoftop-kepisodesonLLMscoreacrossdifferentmodels. Bothmodelsshowperformancerisessharply
untilk=10andthenplateaus. ThereddashedlinesrepresentFullContextbaselineperformanceforcomparison.
4.4 HyperparameterAnalysis(RQ3)
ToanswerRQ3,weconductedasensitivityanalysisonthenumberofretrievedepisodicmemories,k,tounderstand
itsimpactonmodelperformance. Throughoutthisanalysis,wemaintainedthesemanticmemoryretrievalcountat
m = 2k topreservetherelativebalancebetweenmemorytypes. Theresults,showninFigure4,revealaclearand
consistentpattern: performancerisessharplyaskincreasesfrom2to10(withmcorrespondinglyincreasingfrom4
to20),andthenlargelyplateaus,withminimalmarginalgainsfork > 10. Thisobservationofdiminishingreturns
isinsightful. Itdemonstratesthatthemodel’sperformanceisnotcontingentonretrievinganever-largernumberof
memories,butrathercanachievenear-optimalperformancewithinarelativelysmall,targetedretrievalwindow.
Model-Dependent Performance Ceiling Analysis. An intriguing observation from Figure 4 is the differential
relationship between Nemori and the Full Context baseline across different model capabilities. With gpt-4o-mini,
NemoriachievesaclearperformanceadvantageoverFullContext(0.744vs0.723),whilewithgpt-4.1-mini,Nemori
approaches but does not substantially exceed the baseline (0.794 vs 0.806). This pattern suggests an interesting
interaction between model capacity and memory system effectiveness. For more capable models, the LoCoMo
datasetmayrepresentarelativelystraightforwardtaskwhererawprocessingpowercaneffectivelyutilizeextensive
contextwithoutsophisticatedmemoryorganization. However,aswedemonstrateinRQ4withthemorechallenging
LongMemEval benchmark,bothmodelsbenefitsignificantlyfromNemori’sstructuredmemoryapproachwhenfacing
S
trulycomplex,long-contextscenarios. Thisfindinghighlightsacrucialdesignprinciple:thevalueofintelligentmemory
systemsbecomesmorepronouncedastaskcomplexityincreases,particularlyinresource-constrainedenvironments
wherecomputationalefficiencyisparamount.
QuestionType Full-context Nemori
(101Ktokens) (3.7-4.8Ktokens)
inim-o4-tpg
single-session-preference 6.7% 46.7%
single-session-assistant 89.3% 83.9%
temporal-reasoning 42.1% 61.7%
multi-session 38.3% 51.1%
knowledge-update 78.2% 61.5%
single-session-user 78.6% 88.6%
Average 55.0% 64.2%
inim-1.4-tpg
single-session-preference 16.7% 86.7%
single-session-assistant 98.2% 92.9%
temporal-reasoning 60.2% 72.2%
multi-session 51.1% 55.6%
knowledge-update 76.9% 79.5%
single-session-user 85.7% 90.0%
Average 65.6% 74.6%
Note:Nemoriachieveshigheraccuracywhileusing95-96%lesscontextthanFull-contextbaseline.
Table4: PerformancecomparisononLongMemEval datasetacrossdifferentquestiontypes.
S
9
Nemori–arXivPreprint
4.5 GeneralizationStudy(RQ4)
ToanswerRQ4,weevaluatedNemoriontheLongMemEval dataset[36]. WhilestructurallysimilartoLoCoMoin
S
itsconversationalnature,LongMemEval presentsasignificantlygreaterchallengeintermsofscale,withanaverage
S
contextlengthof105Ktokens. Thisservesasacrucialstresstestforlong-termmemoryretentionandgeneralization.
TheresultsinTable4demonstrateNemori’sstrongperformanceunderthesedemandingconditions. Acloseranalysis
revealstwokeyfindings. First, Nemorishowssuperiorperformanceonuserpreferencetasks. Thisisbecauseits
concise,high-qualitystructuredmemoryenablesthemodeltofocusmoreeffectivelyonuserhabitsandinclinations,
whichareoftendilutedwithinthebaseline’sextensivecontext. Second,thebaseline’sbetterperformanceonsingle-
session-assistanttaskssuggeststhatNemoricanlosesomefine-graineddetails,apotentiallimitationtobeaddressedin
futurework.
5 Conclusion
Inthiswork,weintroducedNemori,acognitively-inspiredmemoryarchitecturethatoffersaprincipledsolutionto
agentamnesia. ByintegratingtheTwo-StepAlignmentPrincipleforcoherentexperiencesegmentationandthenovel
Predict-CalibratePrincipleforproactiveknowledgedistillation,Nemorireframesmemoryconstructionasanactive
learningprocess. Extensiveexperimentsdemonstrateitseffectiveness: Nemorinotonlysignificantlyoutperformsstate-
of-the-artsystemsontheLoCoMoandLongMemEvalsbenchmarksbutalsosurpassestheFullContextbaselinewith
88%fewertokens,whileshowingstronggeneralizationincontextsupto105Ktokens. Byshiftingtheparadigmfrom
passivestoragetoactiveknowledgeevolution,Nemoriprovidesafoundationalcomponentfordevelopingautonomous
agentscapableofgenuine,human-likelearning.
References
[1] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,and
IlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,30,2017.
[2] NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.
Lostinthemiddle: Howlanguagemodelsuselongcontexts. arXivpreprintarXiv:2307.03172,2023.
[3] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,JiakaiTang,XuChen,
YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents. FrontiersofComputerScience,
18(6):186345,2024.
[4] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKamar,PeterLee,
YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneralintelligence: Earlyexperimentswith
gpt-4. arXivpreprintarXiv:2303.12712,2023.
[5] JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,andMichaelSBernstein.
Generativeagents: Interactivesimulacraofhumanbehavior. InProceedingsofthe36thannualacmsymposium
onuserinterfacesoftwareandtechnology,pages1–22,2023.
[6] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNee-
lakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners. Advances
inneuralinformationprocessingsystems,33:1877–1901,2020.
[7] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-
intensivenlptasks. Advancesinneuralinformationprocessingsystems,33:9459–9474,2020.
[8] ZihongHe,WeizheLin,HaoZheng,FanZhang,MattWJones,LaurenceAitchison,XuhaiXu,MiaoLiu,PerOla
Kristensson,andJunxiaoShen. Human-inspiredperspectives: Asurveyonailong-termmemory. arXivpreprint
arXiv:2411.00489,2024.
[9] ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,DanSu,YanXu,EtsukoIshii,YeJinBang,AndreaMadotto,
andPascaleFung. Surveyofhallucinationinnaturallanguagegeneration. ACMcomputingsurveys,55(12):1–38,
2023.
[10] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YixinDai,JiaweiSun,HaofenWang,
and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint
arXiv:2312.10997,2(1),2023.
[11] SamuelJGershman,IlaFiete,andKazukiIrie. Key-valuememoryinthebrain. Neuron,113(11):1694–1707,
2025.
10
Nemori–arXivPreprint
[12] ZhiyuLi,ShichaoSong,ChenyangXi,HanyuWang,ChenTang,SiminNiu,DingChen,JiaweiYang,ChunyuLi,
QingchenYu,etal. Memos: Amemoryosforaisystem. arXivpreprintarXiv:2507.03724,2025.
[13] Yuanzhe Hu, Yu Wang, and Julian McAuley. Evaluating memory in llm agents via incremental multi-turn
interactions. arXivpreprintarXiv:2507.05257,2025.
[14] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,
generate,andcritiquethroughself-reflection. arXivpreprintarXiv:2310.11511,2023.
[15] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis. Efficientstreaminglanguagemodels
withattentionsinks. arXivpreprintarXiv:2309.17453,2023.
[16] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha
Metropolitansky,RobertOsazuwaNess,andJonathanLarson. Fromlocaltoglobal: Agraphragapproachto
query-focusedsummarization. arXivpreprintarXiv:2404.16130,2024.
[17] JeffreyMZacksandBarbaraTversky. Eventstructureinperceptionandconception. Psychologicalbulletin,
127(1):3,2001.
[18] EndelTulvingetal. Episodicandsemanticmemory. Organizationofmemory,1(381-403):1,1972.
[19] KarlFriston. Thefree-energyprinciple: aunifiedbraintheory? Naturereviewsneuroscience,11(2):127–138,
2010.
[20] JamesLMcClelland,BruceLMcNaughton,andRandallCO’Reilly. Whytherearecomplementarylearning
systemsinthehippocampusandneocortex: insightsfromthesuccessesandfailuresofconnectionistmodelsof
learningandmemory. Psychologicalreview,102(3):419,1995.
[21] ZeyuZhang,QuanyuDai,XiaoheBo,ChenMa,RuiLi,XuChen,JiemingZhu,ZhenhuaDong,andJi-RongWen.
Asurveyonthememorymechanismoflargelanguagemodelbasedagents. ACMTransactionsonInformation
Systems,2024.
[22] HongliYu,TinghongChen,JiangtaoFeng,JiangjieChen,WeinanDai,QiyingYu,Ya-QinZhang,Wei-YingMa,
JingjingLiu,MingxuanWang,etal. Memagent: Reshapinglong-contextllmwithmulti-convrl-basedmemory
agent. arXivpreprintarXiv:2507.02259,2025.
[23] KaiMei,XiZhu,WujiangXu,WenyueHua,MingyuJin,ZelongLi,ShuyuanXu,RuosongYe,YingqiangGe,
andYongfengZhang. Aios: Llmagentoperatingsystem. arXivpreprintarXiv:2403.16971,2024.
[24] CharlesPacker, VivianFang, Shishir_GPatil, KevinLin, SarahWooders, andJoseph_EGonzalez. Memgpt:
Towardsllmsasoperatingsystems. 2023.
[25] KwangseobAhn. Hema:Ahippocampus-inspiredextendedmemoryarchitectureforlong-contextaiconversations.
arXivpreprintarXiv:2504.16754,2025.
[26] PrateekChhikara,DevKhant,SaketAryan,TaranjeetSingh,andDeshrajYadav. Mem0: Buildingproduction-
readyaiagentswithscalablelong-termmemory. arXivpreprintarXiv:2504.19413,2025.
[27] DerongXu,YiWen,PengyueJia,YingyiZhang,YichaoWang,HuifengGuo,RuimingTang,XiangyuZhao,
Enhong Chen, Tong Xu, et al. Towards multi-granularity memory association and selection for long-term
conversationalagents. arXivpreprintarXiv:2505.19549,2025.
[28] WanjunZhong,LianghongGuo,QiqiGao,HeYe,andYanlinWang. Memorybank: Enhancinglargelanguage
modelswithlong-termmemory. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,
pages19724–19731,2024.
[29] RyanYenandJianZhao. Memolet: Reifyingthereuseofuser-aiconversationalmemories. InProceedingsofthe
37thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pages1–22,2024.
[30] YuWangandXiChen.Mirix:Multi-agentmemorysystemforllm-basedagents.arXivpreprintarXiv:2507.07957,
2025.
[31] Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory: Tracing
hierarchicalmemoryformulti-agentsystems. arXivpreprintarXiv:2506.07398,2025.
[32] AkashVishwakarma,HojinLee,MohithSuresh,PriyamShankarSharma,RahulVishwakarma,SparshGupta,and
YuvrajAnupamChauhan. Cognitiveweave: Synthesizingabstractedknowledgewithaspatio-temporalresonance
graph. arXivpreprintarXiv:2506.08098,2025.
[33] Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras,
HaithamBou-Ammar,andJunWang. Human-likeepisodicmemoryforinfinitecontextllms. arXivpreprint
arXiv:2407.09450,2024.
11
Nemori–arXivPreprint
[34] HaoyuGao,RuiWang,Ting-EnLin,YuchuanWu,MinYang,FeiHuang,andYongbinLi. Unsuperviseddialogue
topicsegmentationwithtopic-awarecontrastivelearning. InProceedingsofthe46thInternationalACMSIGIR
ConferenceonResearchandDevelopmentinInformationRetrieval,SIGIR’23,page2481–2485,NewYork,NY,
USA,2023.AssociationforComputingMachinery.
[35] AdyashaMaharana,Dong-HoLee,SergeyTulyakov,MohitBansal,FrancescoBarbieri,andYuweiFang. Evaluat-
ingverylong-termconversationalmemoryofLLMagents. InLun-WeiKu,AndreMartins,andVivekSrikumar,
editors,Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
LongPapers),pages13851–13870,Bangkok,Thailand,August2024.AssociationforComputationalLinguistics.
[36] DiWu,HongweiWang,WenhaoYu,YuweiZhang,Kai-WeiChang,andDongYu. Longmemeval: Benchmarking
chat assistants on long-term interactive memory. In The Thirteenth International Conference on Learning
Representations,2025.
[37] HarrisonChase. Langchain. https://github.com/langchain-ai/langchain,2022. Accessed:2025-07-20.
[38] PrestonRasmussen,PavloPaliychuk,TravisBeauvais,JackRyan,andDanielChalef. Zep: atemporalknowledge
grapharchitectureforagentmemory. arXivpreprintarXiv:2501.13956,2025.
12

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
