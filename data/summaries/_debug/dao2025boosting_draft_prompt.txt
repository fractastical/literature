=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Boosting MCTS with Free Energy Minimization
Citation Key: dao2025boosting
Authors: Mawaba Pascal Dao, Adrian M. Peter

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Active Inference, grounded in the Free Energy Principle, provides a powerful lens
for understanding how agents balance exploration and goal-directed behavior in un-
certain environments. Here, we propose a new planning framework, that integrates
Monte Carlo Tree Search (MCTS) with active inference objectives to systematically
reduce epistemic uncertainty while pursuing extrinsic rewards. Our key insight is that
MCTS—alreadyrenownedforitssearchefficiency—canbenaturallyextendedtoincor-
porate free...

Key Terms: boosting, energy, mcts, monte, search, tree, minimization, inference, carlo, active

=== FULL PAPER TEXT ===

1
Boosting MCTS with Free Energy Minimization
1 1
Mawaba Pascal Dao , Adrian M. Peter
1
FloridaInstituteofTechnology
Keywords: Free Energy, Monte Carlo Tree Search, Cross-Entropy Method, Infor-
mationGain,IntrinsicExploration,ContinuousActionSpaces
Abstract
Active Inference, grounded in the Free Energy Principle, provides a powerful lens
for understanding how agents balance exploration and goal-directed behavior in un-
certain environments. Here, we propose a new planning framework, that integrates
Monte Carlo Tree Search (MCTS) with active inference objectives to systematically
reduce epistemic uncertainty while pursuing extrinsic rewards. Our key insight is that
MCTS—alreadyrenownedforitssearchefficiency—canbenaturallyextendedtoincor-
porate free energy minimization by blending expected rewards with information gain.
Concretely, the Cross-Entropy Method (CEM) is used to optimize action proposals at
5202
naJ
22
]IA.sc[
1v38031.1052:viXra
the root node, while tree expansions leverage reward modeling alongside intrinsic ex-
ploration bonuses. This synergy allows our planner to maintain coherent estimates of
valueanduncertaintythroughoutplanning,withoutsacrificingcomputationaltractabil-
ity. Empirically,webenchmarkourplanneronadiversesetofcontinuouscontroltasks,
where it demonstrates performance gains over both stand-alone CEM and MCTS with
randomrollouts.
1 Introduction
The integration of search mechanisms into decision-making frameworks has consis-
tently led to significant performance improvements across various domains. Monte
CarloTreeSearch(MCTS),apowerfulsearch-basedplanningmethod,hasbeenpartic-
ularly successful in discrete domains such as game-playing, with notable applications
likeAlphaGocombiningMCTSwithdeepneuralnetworkstoachievesuperhumanper-
formance in the game of Go (Silver et al., 2016, 2017). However, extending MCTS
to more general settings, particularly within the Active Inference framework, presents
bothchallengesandopportunities.
Active Inference, rooted in the Free Energy Principle (Friston, 2010), provides a
unifying framework for understanding action and perception as processes of minimiz-
ing free energy. Recent advancements have explored the integration of Active Infer-
ence with MCTS to enable sophisticated planning under uncertainty in both discrete
and continuous state-action spaces (Fountas et al., 2020; Tschantz et al., 2020). These
methods demonstrate the potential to balance exploitation and exploration naturally by
2
incorporating free energy minimization as a criterion for action selection. However,
keychallengesremain,includingthecomputationaldemandsofplanningincontinuous
spaces,ensuringreliablevalueestimationduringtree-basedsearch,andextendingthese
methodstopracticalapplicationsandbenchmarksbeyondexampleproblems.
In this paper, we propose a novel framework that integrates MCTS with Active In-
ference to address these challenges. Our approach introduces mechanisms for efficient
planning in continuous state-action spaces while aligning the generative model of Ac-
tiveInferencewiththetreesearchprocess.
Ourcontributionscanbesummarizedasfollows:
• Root Action Distribution Planning: We propose a novel mechanism where
a single Gaussian action distribution is fitted at the root node using the Cross-
EntropyMethod(CEM).Thisrootactiondistributionisutilizedconsistentlythrough-
outthetreetraversalandsimulationphases,significantlyreducingcomputational
complexity while ensuring value estimation remains aligned with actual action
selection. By constraining the tree size, we maintain the validity of the root ac-
tiondistribution,enablingefficientandreliableplanning.
• Enhanced Exploration through Information Gain: Our method incorporates
intrinsic exploration by integrating epistemic value (Information Gain) into the
planning process. This dual exploration mechanism, achieved through both the
expected free energy criterion and MCTS exploration, improves the agent’s abil-
itytonavigatehigh-dimensionalcontinuousdomains.
Theremainderofthispaperisorganizedasfollows. InSection2,wediscussrelated
3
work, providing a comprehensive overview of existing methods that integrate MCTS
with Active Inference. Section 3 provides a background on MCTS, the Free Energy
Principle, and Active Inference. Section 4 describes our proposed planner in detail,
explaining each component and its mathematical grounding. Section 5 presents exper-
imental results demonstrating the effectiveness of our approach. Finally, in the con-
clusion, we discuss the broader implications of our findings and highlight promising
directionsforfutureresearch.
2 Related Work
Active Inference, rooted in the Free Energy Principle (Friston, 2010), offers a uni-
fied framework for understanding perception and action as inference processes. It has
demonstrated broad applicability across neuroscience and machine learning, modeling
phenomena such as curiosity (Schwartenbeck et al., 2018), dopaminergic discharges
(FitzGerald et al., 2015), and animal navigation. However, a significant challenge lies
in the computational complexity of evaluating all possible policies, which grows expo-
nentiallywiththeplanninghorizon.
Model-basedReinforcementLearning(RL)methodsaimtolearnamodeloftheen-
vironment’s dynamics and use it for planning (Moerland et al., 2023). These methods
can be more sample-efficient than model-free approaches, as they can simulate expe-
riences without interacting with the environment. Chua et al. (2018) proposed PETS
(ProbabilisticEnsembleswithTrajectorySampling),whichusesanensembleofproba-
bilisticmodelsforplanningincontinuousactionspaces. Similarly,Hafneretal.(2019)
4
introduced PlaNet, a model-based RL method that learns a latent dynamics model for
planning. Byleveragingprobabilisticmodels,thesemethodsproviderobustuncertainty
quantification,whichiscriticalforexplorationandplanningunderuncertainty.
Tschantz et al. (2020) proposed the Free Energy of Expected Future (FEEF) as a
tractable objective for decision-making in RL environments. Their method incorpo-
rates a model-based Cross-Entropy Method (CEM) for policy optimization, achieving
abalancebetweenexplorationandexploitationinsparseandcontinuouscontroltasks.
MonteCarloTreeSearch(MCTS),adecision-makingframework,hasprovenvalu-
ableinaddressingthecomputationalcomplexityofevaluatinganexponentiallygrowing
number of possible policies as the planning horizon increases. MCTS achieves this by
samplingasubsetofpossiblepolicies. EarlyapplicationsofMCTSfocusedondiscrete
domains,suchasgameplaying(Coulom,2006),withsignificantsuccessesinAlphaGo
(Silver et al., 2016, 2017). While extensions to continuous action spaces, such as pro-
gressivewidening (Coulom,2006)and hierarchical optimization(Bubecket al.,2011),
have broadened its scope, these approaches are typically employed in Reinforcement
Learning(RL)contexts.
RecentadvancementshavecombinedMCTSwithActiveInferencetoaddresschal-
lenges in planning under uncertainty. For instance, Fountas et al. (2020) proposed an
MCTS-based Active Inference framework that replaces traditional selection criteria,
such as the Upper Confidence Bounds applied to Trees (UCT) (Kocsis and Szepesva´ri,
2006), with an expected free energy (EFE)-based criterion. Their approach employs
a deep neural network to approximate posterior distributions and utilizes Monte Carlo
sampling to evaluate free energy terms efficiently. This integration demonstrated im-
5
proved performance in tasks requiring sophisticated planning, such as the dSprites
datasetandtheAnimal-AIenvironment.
Branching Time Active Inference (BTAI), introduced by Champion et al. (2022),
further unified MCTS and Active Inference by framing planning as Bayesian model
expansion. BTAI treats the tree structure as part of the generative model itself, dy-
namically expanding the model to incorporate future observations and latent variables.
This approach reduced the computational overhead associated with traditional Active
Inferencemodels,allowingapplicationsingraphnavigationandothercomplextasks.
Despite these advances, the integration of MCTS with the Free Energy Principle in
practical applications remains underexplored. Most implementations adopt MCTS as a
planningtoolwithinRLframeworks,leavingthepotentialofActiveInference—particularly
itscapacityforintrinsicmotivationanduncertaintyminimization—relativelyuntapped.
Inthiswork,weaddressthesegapsbyintegratingMCTSwithActiveInferenceina
novelway. Ourframeworkemploysamodel-basedapproach,usingMCTSforplanning
EFE-optimal paths in continuous state-action spaces. We build upon methods such as
progressive widening and ensemble modeling to extend MCTS to continuous domains
while maintaining compatibility with the generative model of Active Inference. This
approach enables efficient exploration and exploitation in environments characterized
byhighuncertaintyandsparserewards.
6
3 Background
3.1 Active Inference and the Free Energy Principle
The Free Energy Principle, originating in neuroscience, posits that systems act to min-
imize a quantity called free energy, which measures how well an internal generative
model predicts observations (Friston, 2010). This principle unifies perception and ac-
tion under the framework of probabilistic inference, where agents aim to align their
beliefswithobserveddataandpredictfuturestates.
Freeenergyisdefinedas:
F(Q,y) = D [Q(x)∥P(x|y)]−lnP(y), (1)
KL
where:
• Q(x): Theapproximateposteriordistributionoverhiddenstatesx.
• P(x|y): Thetrueposteriordistributionoverhiddenstates,givenobservationsy.
• lnP(y): Thelogevidence(marginallikelihood)oftheobservations.
• D [Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate
KL
andtrueposteriordistributions.
Minimizingfreeenergyinvolvestwocomponents:
1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate
posteriorQ(x)withthetrueposteriorP(x|y).
7
2. Maximizing the log evidence lnP(y), which ensures that the generative model
predictsobservationsaccurately.
Building on this principle, Active Inference provides a framework for decision-
making, where agents minimize variational free energy by simultaneously improving
their beliefs about the environment (perception) and selecting actions that shape future
observations (action). This process naturally balances exploration (uncertainty reduc-
tion)andexploitation(goalachievement).
KeyprocessesinActiveInferenceinclude:
• Perception: Updating beliefs about the hidden states of the environment using
variationalinferencetoaligntheapproximateposteriorQ(x)withthetrueposte-
riorP(x|y).
• Action: Selecting actions thatminimize free energy, shaping future observations
toconformtotheagent’sgenerativemodel.
Unlike traditional reinforcement learning, which separates reward maximization
andexplorationintodistinctobjectives,ActiveInferenceunifiestheseobjectivesbyin-
corporating uncertainty reduction as an intrinsic component of decision-making. This
intrinsic motivation encourages agents to explore uncertain states while also achieving
extrinsicgoals.
3.2 Monte Carlo Tree Search (MCTS)
MonteCarloTreeSearch(MCTS)isaheuristicsearchalgorithmdesignedfordecision-
makinginlargeandcomplexstatespaces(Browneetal.,2012). Itincrementallybuilds
8
a search tree by iteratively simulating playouts, balancing exploration and exploitation
to identify promising actions. Each node in the tree represents a state, and edges repre-
sentactions.
TheMCTSprocessistypicallydividedintofourkeyphases:
• Selection: Startingfromtherootnode,thealgorithmtraversesthetreebyselect-
ing child nodes based on a specific policy until a leaf node is reached. The most
commonly used selection policy is based on Upper Confidence Bounds (UCB),
describedbelow.
• Expansion: Iftheleafnodedoesnotrepresentaterminalstate,oneormorechild
nodesareaddedtothetree,representingunexploredactions.
• Simulation: A simulation, or playout, is performed from the expanded node,
where actions are sampled according to a default policy (e.g., random actions)
until a terminal state is reached. The return from this simulation provides an
estimateofthevalueoftheexpandednode.
• Backpropagation: Theresultsofthesimulationarepropagatedbackupthetree,
updatingthevaluesandvisitcountsofallnodesalongthepath.
By iteratively performing these steps, MCTS progressively refines its estimates of
action values, focusing computational resources on the most promising parts of the
search space. This property makes MCTS highly effective in problems with large state
spacesanduncertainoutcomes.
9
3.2.1 UpperConfidenceBound1(UCB1)
TheUpperConfidenceBound1(UCB1)algorithmisawidelyusedtechniqueinMCTS
toaddresstheexploration-exploitationtradeoffduringtreetraversal(Aueretal.,2002).
UCB1 assigns a score to each child node, balancing the average reward observed (ex-
ploitation)withtheuncertaintyofthenode(exploration).
TheUCB1valueforselectingachildnodeiiscomputedas:
(cid:114)
lnN
UCB1 = Q +C , (2)
i i ucb
N
i
where:
• Q istheaveragereward(meanvalue)ofchildi.
i
• N isthetotalnumberoftimestheparentnodehasbeenvisited.
• N isthenumberoftimeschildihasbeenvisited.
i
• C istheexplorationconstantthatcontrolsthedegreeofexploration.
ucb
The first term, Q , promotes exploitation by preferring actions that have yielded
i
(cid:113)
higher rewards on average. The second term, C lnN, encourages exploration by
ucb Ni
assigning a higher bonus to actions that have been selected fewer times, thus having
higher uncertainty. The logarithmic factor lnN ensures that as the number of visits N
totheparentnodeincreases,theexplorationbonusdecreases,allowingthealgorithmto
focusmoreonexploitationovertime.
ByintegratingUCB1intoMCTS,thealgorithmeffectivelybalancestheneedtoex-
plore newactions that mightlead to betterrewards with theneed to exploitactions that
10
have already shown promising results. This balance is crucial for converging towards
optimalpoliciesindecision-makingtasks.
4 Proposed Planner
We now present our MCTS-CEM planning framework, which integrates Monte Carlo
TreeSearchwiththeCross-EntropyMethod(CEM)attherootnodetohandlecontinu-
ousactionseffectively. Figure1providesahigh-leveloverview,highlightingthreemain
components:
1. (A)Therootnode,initializedwiththeagent’scurrentstates .
0
2. (B)TheprocessoffittingasingleGaussianactiondistributionusingCEMatthe
rootnode.
3. (C)Thesubsequenttree-basedplanning(MCTS)stage,whichusesthefittedroot
actiondistributionforexploration,rollouts,andleaf-nodesimulations.
4.1 Root Action Distribution Planning
The key idea is to learn one Gaussian distribution over actions at the root node, then
reuse it throughout tree-based planning and simulation, thereby ensuring consistent es-
(cid:0) (cid:1)
timatesofvalueandreward. Wedenotethisrootactiondistributionbya ∼ N µ, Σ .
where µ and Σ are optimized via CEM to maximize expected returns plus any
epistemic(informationgain)terms.
11
Figure1: MCTS-CEMDiagram.
A:InitializetheMCTStreewiththeagent’scurrentstates .
0
B: Fit the root node’s action distribution using CEM. Actions are evaluated by min-
imizing expected free energy (G ), with next states sampled using the current action
i
Gaussian. TheepistemicvalueEVi iscomputedastheKLdivergence,andrewards(ri)
t t
approximatelnP(yi). Thetop-performingactionsrefinethedistributioniteratively.
t
C:UsethefittedactiondistributionforactionsamplingduringMCTSexploration,bal-
ancingexploitationandexplorationwithUCB-likeselectionandconsistentsimulations
attheleaves.
12
4.1.1 FittingtheRootActionDistributionviaCEM
At the beginning of planning (from root state s ), we perform Cross-Entropy Method
0
optimizationtoobtainµandΣ. Figure2illustratesthisprocess(labeled“3. Evaluation
ofEachCandidate”):
1. InitializeaGaussianactiondistributionwithmeanµ = 0andcovarianceΣ =
0 0
I.
2. Sample candidates {a(i)}n candidates from the current Gaussian. Each a(i) is an H-
i=1
stepactionsequencefortheplanninghorizon.
3. Evaluate each candidate via short model-based rollouts, scoring it by the sum
ofextrinsicrewardandtheepistemicvalue(aninformation-gainterm)acrossthe
horizon.
4. Refitthedistributiontothetop-k performers:
(cid:88) (cid:88)
µ = 1 a(i), Σ = 1 (a(i) −µ )(a(i) −µ )⊤, (3)
new k new k new new
i∈S i∈S
whereS istheindexsetoftop-k candidates.
5. RepeatforafixednumberofCEMiterationsuntilconvergence.
Theoutputisasingle,optimizedactiondistributionN(µ,Σ)centeredonpromising
actionsfromthemodel’sperspective.
13
Figure 2: Root Action Distribution Fitting Using CEM: This diagram focuses
on subsection 3 of component B in the MCTS-CEM process. Candidate actions
{a(1),...,a(n candidates )} are sampled from a Gaussian distribution. Their evaluations,
basedontheexpectedfreeenergyobjectiveG ,approximateextrinsicvaluelnP(yi) ≈
i t
ri andepistemicvalueEVi fromKLdivergence. Thetopk candidatesrefinethedistri-
t t
bution,optimizingexplorationandexploitation.
14
4.1.2 MCTS-CEMPlanningwiththeFittedRootActionDistribution
OncetherootactiondistributionhasbeenfitviaCEM,wekeepitfixedfortheduration
ofthetree-basedplanning. Figure3depictsthisstage:
Figure 3: MCTS-CEM Planning (Component C). After fitting the root action distri-
bution,MCTSusesittodriveactionsamplingateachexpansionstepandforleaf-node
simulations.
MCTS Expansion and Action Selection. When selecting actions at each decision
node, we sample from N(µ, Σ) rather than optimizing new distributions at non-root
nodes. This design assumes that states near the root are sufficiently representative of
15
whatwewillencounterdeeperinthetree;hence,asingleGaussiancanremaineffective
astheagentexpandsitslookahead. IfN newactionsmustbeconsidered,wedraw
children
N distinctsamplesfromtherootdistributiontopopulatechildnodes.
children
Simulation/Rollout. When a leaf node is reached and we need to estimate its value
(i.e., to “roll out” until horizon), we again sample future actions from the same root
distribution. This ensures that the value estimates at leaf nodes reflect outcomes under
the same policy used to expand the tree, resulting in consistent planning. In practice,
short rollouts can be performed, or we can simply evaluate a truncated horizon to keep
computations manageable. A recognized limitation is that as states become farther
from theroot, the originallyfitted action distributionmay become suboptimalfor these
distant states. This can lead to rollouts that underestimate the true value if the root
distribution does not align well with deeper states. Nonetheless, because we use the
same distribution throughout, our value estimates remain consistent with the policy we
havecommittedtoattheroot.
By maintaining one fitted action distribution at the root and using it consistently
throughout expansions and rollouts, MCTS-CEM ties together the policy used to score
stateswiththepolicyusedtoexplorethem. Thisavoidsneedlessre-optimizationofac-
tions at every node. While this approach may yield suboptimal action choices in states
that significantly diverge from the root, it provides self-consistent planning in terms of
policy and value estimation. Addressing this limitation to enable broader applicability
to larger search horizons is an exciting area for future work. Empirical results in Sec-
tion 5 demonstrate that this strategy—even with a truncated horizon—leads to robust
16
performancegains,especiallyinsparseorhigh-dimensionalcontinuoustasks.
4.2 Epistemic Value as Information Gain Bonus
The expected free energy G(π) for a policy π can be decomposed into two key com-
ponents: the extrinsic value (preferences over observations) and the epistemic value
(informationgain). Formally,itisdefinedas(Fristonetal.,2015):
 
H
(cid:88)
G(π) = E q(s1:H|π) −lnP(o t |s t )+D KL [q(s t |π)∥p(s t |s t−1 ,a t−1 )], (4)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
t=1
ExtrinsicValue EpistemicValue
where q(s |π) is the variational posterior over states given policy π, P(o |s ) is
1:H t t
the likelihood of observations given states, D [·∥·] is the Kullback-Leibler (KL) di-
KL
vergence, and p(s |s ,a ) is the prior predictive distribution of states under the
t t−1 t−1
policy. The extrinsic value encodes the agent’s preferences over observations, while
the epistemic value quantifies the expected information gain about the environment’s
dynamics.
In reinforcement learning, the agent’s goal is to maximize cumulative rewards. To
incorporatetheFreeEnergyPrinciple,weapproximatetheextrinsicvalue−lnP(o |s )
t t
with the negative reward function −r(s ,a ). This is a standard practice when aligning
t t
reinforcement learning with the Free Energy framework, as rewards are viewed as rep-
resentingtheagent’spreferencesoverstatesoroutcomes(Fristonetal.,2009;Tschantz
et al., 2020; Millidge, 2020). This approximation allows us to interpret reward maxi-
mizationasaformoffreeenergyminimization,reframingtheagent’sextrinsicmotiva-
tionintermsoftheprinciple.
17
The epistemic value measures how much an agent’s belief about the next state
changes when new information is available, capturing the expected information gain
fromtakinganaction. Foracandidateactionsequenceai,theepistemicvalueattimet
is defined as the expected KL divergence between the approximate posterior and prior
predictivedistributions.
EVi = E (cid:2) lnq(s |si,ai)−lnp(s |si,ai) (cid:3) . (5)
t q(st+1|si
t
,ai
t
) t+1 t t t+1 t t
Tocomputetheepistemicvaluepractically,weapproximatetheexpectedKLdiver-
gence as the difference between the entropy of the aggregated predictive distribution
and the average entropy of the individual model predictions. This approximation is in-
spiredbytheBayesianActiveLearningbyDisagreement(BALD)framework(Houlsby
et al., 2011; Gal et al., 2017), where mutual information is used to quantify epistemic
uncertainty.
Starting from Equation (5) and with some minor abuse of notation, we observe that
the log-ratio lnq(s | si,ai) − lnp(s | si,ai) = lnq(s). Taking the expectation
t+1 t t t+1 t t p(s)
undertheposteriorq(·),wehave
(cid:90)
q(s)
D [q(s)∥p(s)] = q(s)ln ds (6)
KL
p(s)
(cid:90) (cid:90)
= q(s)lnq(s)ds− q(s)lnp(s)ds (7)
(cid:90)
= −H(q)− q(s)lnp(s)ds, (8)
(cid:82) (cid:82)
whereH(q) = − q(s)lnq(s)dsistheentropy ofq(s). Theterm q(s)lnp(s)ds
can be challenging to compute directly. However, if q(s) and p(s) are both Gaussian
18
distributions or mixtures of Gaussians, and they are close to each other (i.e., q(s) ≈
p(s)),wecanapproximatethisterm.
Since q(s) is the aggregated predictive distribution from the ensemble, and p(s)
representstheindividualmodelpredictions,wecanapproximate:
(cid:90) M (cid:90)
1 (cid:88)
q(s)lnp(s)ds ≈ q(s)lnp (s)ds. (9)
m
M
m=1
Thecross-entropybetweenq(s)andp(s)isdefinedas:
(cid:90)
H(q,p) = − q(s)lnp(s)ds. (10)
Therefore,wehave:
(cid:90)
q(s)lnp(s)ds = −H(q,p). (11)
Assuming that q(s) is close to p(s), the cross-entropy H(q,p) can be approximated
bytheentropyofp(s):
H(q,p) ≈ H(p). (12)
Similarly,foreachensemblemodelp (s):
m
(cid:90)
q(s)lnp (s)ds = −H(q,p ) ≈ −H(p ). (13)
m m m
SubstitutingbackintotheexpressionfortheKLdivergence,weobtain:
D [q∥p] ≈ −H(q)−(−H(p)) = −H(q)+H(p). (14)
KL
19
Similarly,usingEquation(9),wehave:
M
1 (cid:88)
D [q∥p] ≈ −H(q)+ H(p ). (15)
KL m
M
m=1
Therefore,theepistemicvaluebecomes:
EV = D [q(s |si,ai)∥p(s |si,ai)] (16)
t,i KL t+1 t t t+1 t t
M
(cid:0) (cid:1) 1 (cid:88) (cid:0) (cid:1)
≈ −H q(s |si,ai) + H p (s |si,ai) . (17)
t+1 t t M m t+1 t t
m=1
Since q(s |si,ai) is the aggregated predictive distribution p(s |si,ai), we can
t+1 t t t+1 t t
write:
M
(cid:0) (cid:1) 1 (cid:88) (cid:0) (cid:1)
EV ≈ H p(s |si,ai) − H p (s |si,ai) . (18)
t,i t+1 t t M m t+1 t t
m=1
This approximation relies on the assumptions that the individual model distribu-
tions from the ensemble p (s |si,ai) and the aggregated distribution p(s |si,ai)
m t+1 t t t+1 t t
are approximately Gaussian, and that the ensemble disagreement reflects the epistemic
uncertaintyabouttheenvironment’sdynamics.
In the BALD framework (Houlsby et al., 2011; Gal et al., 2017), the mutual infor-
mationbetweenpredictionsandmodelparametersisexpressedas:
I[y;θ|x] = H (cid:0)E [p(y|x,θ)] (cid:1) −E [H(p(y|x,θ))], (19)
p(θ) p(θ)
whereyistheprediction,xistheinput,andθrepresentsthemodelparameters. This
parallels our expression for the epistemic value, where the first term is the entropy of
theaggregatedpredictions,andthesecondtermistheaverageentropyoverthemodels.
20
Calculating the entropy of the aggregated predictive distribution H(p(s |si,ai))
t+1 t t
directlyischallengingsinceitisamixtureofGaussianswithoutaclosed-formentropy
expression. To estimate this entropy, we employ the Kozachenko–Leonenko entropy
estimator (Kozachenko and Leonenko, 1987), a non-parametric method based on k-
nearestneighbordistancesamongsamples. TheKozachenko–Leonenkoestimatorfora
setofN samples{x }N inRd isgivenby:
i i=1
N
d (cid:88)
H ≈ ψ(N)−ψ(k)+lnc + lnϵ , (20)
Kozachenko-Leonenko d i
N
i=1
where ψ(·) is the digamma function, c is the volume of the d-dimensional unit
d
ball, ϵ is twice the distance from sample x to its k-th nearest neighbor, and d is the
i i
dimensionality of the state space. In our implementation, we generate samples from
the aggregated predictive distribution by sampling from each ensemble model’s out-
put and aggregating the results. We set k = 3 for the k-nearest neighbors and com-
pute pairwise distances between samples to find ϵ . Using Equation (20), we estimate
i
H(p(s |si,ai)).
t+1 t t
For the individual model entropies H(p (s |si,ai)), we use the closed-form ex-
m t+1 t t
pressionfortheentropyofaGaussiandistribution:
1
(cid:2) (cid:3)
H(N(µ,Σ)) = ln (2πe)ddetΣ , (21)
2
whereΣisthecovariancematrix(assumeddiagonalinourcase).
Our planner aims to minimize the expected free energy by balancing the trade-off
between exploiting known rewarding actions and exploring uncertain regions. We de-
finethecombinedobjectivefunctionforcandidateias:
21
H−1
(cid:88)(cid:2) (cid:3)
G = −r(si,ai)+λ·EVi , (22)
i t t t
t=0
wherer(si,ai)istherewardfunction,EVi istheepistemicvalue(InformationGain)
t t t
attimet,andλisaweightingfactorbalancingexploitationandexploration.
During planning, we generate a set of candidate action sequences {a } and evalu-
i
ate them using the combined objective function G . We use the Cross-Entropy Method
i
(CEM) to iteratively refine the action distribution by selecting the top-performing can-
didates and fitting a Gaussian distribution over them. The process involves initializing
the action distribution’s mean and standard deviation, sampling candidate action se-
quences, evaluating them using Equation (22), selecting the top performers, updating
the action distribution based on these candidates, and repeating the process for a fixed
numberofiterations. ByminimizingG ,theplannerselectsactionsthatareexpectedto
i
yieldhighrewardswhilealsoprovidingvaluableinformationabouttheenvironment.
Our approach ensures a principled balance between exploration and exploitation.
The negative reward term −r(si,ai) encourages the agent to select actions that maxi-
t t
mizeexpectedrewards,aligningwiththegoalofrewardmaximizationinreinforcement
learning. Theepistemicvalueλ·EVi incentivizestheagenttochooseactionsthatreduce
t
uncertainty, leading to better knowledge of the environment’s dynamics. This balance
is crucial for efficient learning, as it prevents the agent from overly exploiting known
rewardingactionswithoutimprovingitsunderstandingoftheenvironment.
22
4.3 Algorithmic Details and Pseudocode
Below, we provide the pseudocode for MCTS-CEM (Algorithm 1), which illustrates
how the root action distribution is used during planning. This algorithm reflects the
approach described in Sections 4.1–4.1.2, including how reward and epistemic value
areintegratedduringtherollouts.
23
Algorithm1: MCTS-CEMwithRootActionDistribution
Input: States0,fittedGaussianN(µ,Σ),ensemble{pm},rewardr(·,·),horizonH,#simsNsim,c,γ
Output: Selectedactiona∗
1: Createrootnodev0withstates0.
2: forsim=1...Nsimdo
3: Selection: v←v0
4: whilevnotterminaldo
5: ifvnotfullyexpandedthen
6: break
7: else
8: Selectchildv′viaUCB:
(cid:113)
v′=arg max [Q(c)+c lnNv]
c∈Children(v) Nc
9: v←v′
10: endif
11: endwhile
12: Expansion:
13: ifvnotfullyexpandedandnotterminalthen
14: Samplea∼N(µ,Σ)
15: snew←f(sv,a),createchildnodevnew
16: AddvnewtoChildren(v);v←vnew
17: endif
18: Simulation(Rollout): G←0, s←sv
19: fort=0...rolloutH−1do
20: a∼N(µ,Σ)
21: rt←r(s,a); G←G+γtrt
22: s←mean{pm(s|s,a)}{(orsample)}
23: endfor
24: Backpropagation: PropagateGupthetree
25: endfor
26: ActionSelection:
27: a∗←argmaxac Nc(childrenofv0)
28: return a∗
24
5 Experiments
In this section, we present experiments conducted to evaluate our proposed MCTS-
CEM.WecompareMCTS-CEMtotwootherplanners:
1. CEM Planner: A regular Cross Entropy Method planner that uses the same
model-based rollout used by Tschantz et al. (2020) during the simulate phase
ofourMCTS-CEMalgorithm.
2. MCTS-Random: An MCTS planner that employs a random policy during roll-
outs. Critically, the simulate phase of MCTS-Random does not use any Free
Energy Minimization for action selection. Instead, at each step of the planning
horizon, it samples a random action from a uniform distribution bounded by the
environment’sactionspace.
We compare these three planners across five different environments. Specifically,
in the Pendulum and Sparse Mountain Car environments, we run each planner for 10
episodes per trial, while in HalfCheetah-Run and HalfCheetah-Flip we run each for
1000 episodes. We repeat every trial five times with different random seeds to account
for variability in performance. All planners are configured with the same planning
horizonandnumberofsimulationsperplanningstepwithineachenvironmenttoensure
a fair comparison. The experiments use the same model dynamics and reward function
acrossallplanners.
25
5.1 Pendulum
Figure 4: The Pendulum environment, where the agent applies torque to swing the
pendulumtoitsuprightposition.
Figure 5: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
Pendulum environment, showing the cumulative reward over episodes averaged over
five trials with different random seeds. Error bars represent the standard deviation
across trials. In this well-shaped, deterministic setting, the additional exploration pro-
vided by MCTS-CEM results in performance comparable to CEM, indicating limited
addedbenefitbeyondastraightforwardCross-EntropyMethodapproach.
In the Pendulum environment (Figure 4), we observe that while MCTS-CEM consis-
tently outperformsMCTS-Random, it does notprovide a significantimprovement over
26
CEM. This can be attributed to the nature of the environment, which features a one-
dimensional action space representing the torque applied to the pendulum’s free end.
Therewardfunctioniswell-shapedandcontinuous:
r = −(θ2 +0.1·θ ˙2 +0.001·torque2), (23)
where θ is the pendulum’s angle, normalized between [−π,π], with 0 being the
uprightposition. Theminimumrewardis−16.27whenthependulumisfullydisplaced
with maximum velocity, and the maximum reward is 0 when the pendulum is perfectly
uprightwithnotorqueapplied. Giventhesimplenatureofthisrewardstructureandthe
deterministic dynamics of the environment, even random action selection by MCTS-
Randomcanachievemaximumrewardintheearlyepisodes.
The comparable performance between CEM and MCTS-CEM suggests that the
additional exploration performed by MCTS-CEM might be unnecessary in this well-
defined and deterministic environment. Once the agent achieves the maximum reward
state, further exploration does not add value, as MCTS-CEM continues to search novel
state-actionpairsevenwhentheoptimalpolicyisalreadyknown. Theexplorationterm
in MCTS encourages novelty, which, while beneficial in more complex environments,
maybecounterproductiveherewhereaknowndeterministicpolicyissufficienttocon-
sistentlyachieveoptimalperformance. OurfindingsalignwiththoseofBellemareetal.
(2016), who demonstrated that in environments with sparse rewards, methods incor-
poratingintrinsicmotivation—suchascount-basedexplorationstrategies—areparticu-
larlyeffective. Incontrast,insimpler,well-shapedenvironments,relyingonestablished
optimalpolicies,asCEMdoes,mightbemoreefficientthanthecontinualplanningthat
27
MCTS-CEMperforms.
5.2 Sparse Mountain Car
Figure 6: The Sparse Mountain Car environment, where the agent must navigate a car
totheflagbybuildingmomentum.
Figure 7: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
Sparse Mountain Car environment, showing the cumulative reward over episodes av-
eraged over five trials with different random seeds. Error bars represent the standard
deviation across trials. Although CEM initially outperforms MCTS-CEM due to its re-
liance on an approximate reward model, MCTS-CEM surpasses both baselines in later
episodes by refining reward estimates and leveraging broader UCB-based exploration
touncoverhigh-rewardtrajectories.
28
IntheSparseMountainCarenvironment(Figure6),thegoalistogeneratecontrolsthat
driveacartothetopofahillmarkedbyaflag,representingthegoalstate. Thecarlacks
sufficientaccelerationtoclimbthehilldirectly. Instead,itmustfirstmovebackwardup
asmallerhilltogainenoughmomentumtoascendthelargerhillinfront. Inthissparse
reward setting, the agent receives a positive reward of +1 only upon reaching the goal
state,whileincurringanegativerewardateverytimestepitdoesnotreachthegoal.
Results on the Sparse Mountain Car environment (Figure 7) highlight the strengths
of MCTS-CEM, particularly in later episodes. Initially, CEM outperforms MCTS-
CEM,butasmoreepisodesareplayed,MCTS-CEMimprovessignificantly,ultimately
achieving higher maximum rewards than both CEM and MCTS-Random. This im-
provement can be attributed to the synergy between CEM’s root distribution optimiza-
tion and the UCB-based tree search within MCTS-CEM. The UCB formula for child
selectioninourplannerisgivenby:
(cid:114)
lnN
v
UCB = Q +c· , (24)
i i
N
i
whereQ istheestimatedreturnofchildi,N isthevisitcountoftheparentnodev,
i v
and N is the visit count of the child itself. Initially, MCTS-CEM’s double reliance on
i
anapproximaterewardmodelcancauseperformancetolagbehindCEM,sincereward
models may overestimate certain state-action pairs (Pathak et al., 2017). However, as
moredataisgatheredovermultipleepisodes,therewardmodelbecomesmoreaccurate.
CombinedwiththebroadersearchfromUCBexpansions,MCTS-CEMdiscovershigh-
rewardtrajectoriesmoreeffectivelythanCEMinthissparseenvironment.
The ability of MCTS-CEM to tap into a wider range of potential future states with
29
more accurate reward estimates allows it to excel in environments like Mountain Car,
where momentum and long-term planning are key to achieving success. This demon-
strates the potential of MCTS-CEM in sparse reward environments, where effective
planning from multiple future states is crucial for overcoming challenges like sparse
rewardsanddelayedreturns(Sutton&Barto,2018).
5.3 HalfCheetah-Run
Figure 8: The HalfCheetah-Run environment, where the agent controls a two-
dimensionalcheetahtomaximizeforwardvelocity.
30
Figure 9: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
HalfCheetah-Run environment. Each curve shows the reward over episodes averaged
across five trials with different random seeds. The shaded regions around each curve
represent the standard deviation across those trials. MCTS-CEM consistently achieves
higher returns by leveraging both UCB-based exploration and CEM’s optimization in
continuous action spaces; however, occasional performance dips, or “policy collapse”
(Millidge,2019),(Fristonetal.,2009),highlightthechallengeofmanagingexploration
andexploitationinpractice.
In the HalfCheetah-Run environment (Figure 8), the objective is for the agent, control-
ling a simulated two-dimensional cheetah, to run as quickly as possible in the forward
direction. The agent receives dense rewards based on its forward velocity, offering
frequent feedback that helps MCTS-CEM correct suboptimal trajectories during plan-
31
ning. As illustrated in Figure 9, MCTS-CEM consistently outperforms both the CEM
Planner and MCTS-Random by leveraging both upper confidence bound (UCB)-based
explorationandCEM’sstrengthincontinuousactionoptimization.
Despite this strong overall performance, both MCTS-CEM and CEM occasionally
exhibit abrupt dips in reward—an effect we refer to as “policy collapse,” following
Millidge (2019) and Friston et al. (2009). Here, an imbalance between exploration
and exploitation, coupled with potential inaccuracies in the planner’s value or reward
estimates,cancausetheagenttomomentarilyovercommittosuboptimalactions. Nev-
ertheless, MCTS-CEM significantly mitigates these collapses compared to plain CEM,
likelyduetothe“doubleexploration”stemmingfromboththeMonteCarlotreesearch
(viaUCB)andtheCEM-basedoptimizationincontinuousactionspace. Thisduallayer
ofexplorationmakestheplannermorerobusttoepisodicfailures,asitcanmorequickly
identify and correct suboptimal trajectories. In practice, policy collapse can be further
alleviated by tuning the weight of intrinsic exploration terms and improving model fi-
delity(e.g.,byadditionaltrainingdataorlargerensembles),therebyhelpingtomaintain
amorereliablebalancebetweenexplorationandexploitation.
32
5.4 HalfCheetah-Flip
Figure 10: The HalfCheetah-Flip environment, where the agent controls a two-
dimensionalcheetahtoperformfrontflips.
33
Figure11: PerformancecomparisonofMCTS-CEM,CEM,andMCTS-Randomonthe
HalfCheetah-Flip environment. Each curve shows the reward over episodes averaged
across five trials with different random seeds. The shaded regions around each curve
represent the standard deviation across those trials. MCTS-CEM and CEM exhibit
nearly identical performance, highlighting the relative simplicity of the task for more
advancedplanners.
IntheHalfCheetah-Flipenvironment(Figure10),theobjectiveisfortheagent,control-
lingasimulatedtwo-dimensionalcheetah,toperformfrontflipsratherthanmaximizing
forward velocity. The agent receives dense rewards based on the quality and frequency
of its flips. Unlike HalfCheetah-Run, the task requires simple motor coordination and
involvescoarsercontroloverthecheetah’sdynamics.
As illustrated in Figure 11, MCTS-CEM and CEM exhibit nearly identical perfor-
34
mance, with no significant advantage observed for the additional planning provided by
MCTS-CEM. This is in stark contrast to environments like HalfCheetah-Run, where
MCTS-CEM demonstrated a clear performance advantage. The similarity in perfor-
mance could be attributed to the nature of the task: flipping requires coarser motor
control than running, and the additional computational effort of MCTS planning does
not yield substantial improvements. Instead, simpler optimization through CEM alone
appearssufficientforachievingnear-optimalresultsinthisenvironment.
This result emphasizes the importance of task characteristics in determining the
utility of MCTS-CEM. While the algorithm excels in sparse reward environments or
well-shaped tasks involving complex, high-dimensional controls, its benefits are less
apparent in tasks requiring coarser motor coordination and less intricate action plan-
ning. This insight highlights the nuanced trade-offs between computational overhead
andplanningefficacyindifferenttypesofcontinuouscontroltasks.
6 Conclusion
In this work, we introduced MCTS-CEM, a model-based planner that unifies Monte
Carlo Tree Search (MCTS) with the Free Energy Principle’s uncertainty minimization
objective. By adopting ensemble-based rollouts and incorporating epistemic value es-
timates as an intrinsic exploration bonus, our planner naturally balances the drive to
maximizeextrinsicrewardswiththeneedtoreduceuncertaintyabouttheenvironment’s
dynamics.
A key element of our approach is fitting a single Gaussian action distribution at the
35
root node using the Cross-Entropy Method (CEM). We then use this root distribution
consistently throughout the expansion and simulation phases, ensuring that the policy
underlying tree search remains coherent with the policy used for value estimation in
leaf-node rollouts. This strategy avoids redundant re-optimization at deeper nodes and
lends stability to planning. Moreover, the integration of information gain as part of the
Free Energy minimization criterion enables a principled exploration mechanism that
boostsperformanceinenvironmentsfeaturingsparseanddelayedrewards.
Empirically,wevalidatedMCTS-CEMonavarietyofcontinuouscontroltasks,in-
cluding Pendulum, Sparse Mountain Car, and HalfCheetah. Across these benchmarks,
our method consistently outperformed or matched baseline planners that either rely
solely on CEM or combine MCTS with simplistic (random) policies. Notably, MCTS-
CEMdemonstratedsuperiorrobustnesstorandomseedvariabilityandscaledfavorably
with increasing amounts of environment interaction, suggesting that its exploration-
drivenplanningiswell-suitedtolong-horizontaskswithlimitedfeedbacksignals.
Overall,thisworkhighlightsthepromiseofcouplingMCTSwithFreeEnergymin-
imization for active inference in high-dimensional control problems. By merging a
powerful search paradigm with an epistemic drive to reduce model uncertainty, we
bring a unified view of planning, exploration, and policy refinement to reinforcement
learning. However, our results also reveal challenges related to balancing exploration
andexploitation,particularlywhenintrinsicexplorationbonusesdominateextrinsicre-
wardoptimization,leadingtoepisodicpolicycollapse. Futureresearchshouldfocuson
mitigating this issue by exploring methods such as adaptive regularization of intrinsic
exploration bonuses, incorporating uncertainty-aware thresholds, and improving the fi-
36
delityofrewardmodelsthroughensemblemethodsoradditionaltraining. Furthermore,
addressing the current limitation of truncated search horizons by extending the planner
todeeperormoreflexibleexpansionsrepresentsanexcitingdirectionforbroadeningthe
applicabilityofourapproach. Addressingthesechallengeswillsolidifytheapplication
ofactiveinferenceprinciplesinscalable,model-basedreinforcementlearning.
Acknowledgments
This research was conducted as part of the requirements for my PhD. My PhD is sup-
portedbyfundingfromAirForceResearchLaboratory(AFRL)grantNo. FA8650-21-
C-1147. Anyopinions,findings,conclusions,orrecommendationscontainedhereinare
those of the authors and do not necessarily represent the official policies or endorse-
ments,eitherexpressedorimplied,oftheAFRLortheU.S.Government.
References
Coulom, R. (2006). Efficient Selectivity and Backup Operators in Monte-Carlo Tree
Search. InternationalConferenceonComputersandGames,72–83.
Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen,
P., Tavener, S., Perez, D., Samothrakis, S., & Colton, S. (2012). A survey of Monte
Carlo Tree Search methods. IEEE Transactions on Computational Intelligence and
AIinGames,4(1),1–43.
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. (2016).
37
VIME: Variational Information Maximizing Exploration. Advances in Neural Infor-
mationProcessingSystems,1109–1117.
Lindley, D. V. (1956). On a Measure of the Information Provided by an Experiment.
TheAnnalsofMathematicalStatistics,27(4),986–1005.
Rosin,C.D.(2011). Multi-armedbanditswithepisodecontext. AnnalsofMathematics
andArtificialIntelligence,61(3),203–230.
Rubinstein,R.Y.(1999).TheCross-EntropyMethodforCombinatorialandContinuous
Optimization. MethodologyandComputinginAppliedProbability,1(2),127–190.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G.,
Schrittwieser,J.,Antonoglou,I.,Panneershelvam,V.,Lanctot,M.,etal.(2016).Mas-
teringthegameofGowithdeepneuralnetworksandtreesearch. Nature,529(7587),
484–489.
Silver,D.,Schrittwieser,J.,Simonyan,K.,Antonoglou,I.,Huang,A.,Guez,A.,Hubert,
T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of Go without
humanknowledge. Nature,550(7676),354–359.
Coue¨toux, A., Hoock, J.-B., Sokolovska, N., Teytaud, O., & Bonnard, N. (2011). Con-
tinuous Upper Confidence Trees. International Conference on Learning and Intelli-
gentOptimization,433–445.
Friston,K.J.(2010). Thefree-energyprinciple: aunifiedbraintheory? NatureReviews
Neuroscience,11(2),127–138.
38
Gelly, S., & Silver, D. (2007). Combining online and offline knowledge in UCT. Pro-
ceedingsofthe24thInternationalConferenceonMachineLearning,273–280.
Bubeck,S.,Munos,R.,Stoltz,G.,&Szepesva´ri,C.(2011). X-ArmedBandits. Journal
ofMachineLearningResearch,12,1655–1695.
Mansley, C. R., Weinstein, A., & Littman, M. L. (2011). Sample-Based Planning for
Continuous Action Markov Decision Processes. Proceedings of the 21st Interna-
tionalConferenceonAutomatedPlanningandScheduling,335–338.
Moerland, T. M., Broekens, J., Plaat, A., & Jonker, C. M. (2023). Model-based rein-
forcement learning: A survey. Foundations and Trends in Machine Learning, 16(1),
1–118.
Chua, K., Calandra, R., McAllister, R., & Levine, S. (2018). Deep Reinforcement
Learning in a Handful of Trials using Probabilistic Dynamics Models. Advances in
NeuralInformationProcessingSystems,4754–4765.
Hafner, D., Lillicrap, T., Norouzi, M., & Ba, J. (2019). Learning Latent Dynamics for
PlanningfromPixels. Proceedingsofthe36thInternationalConferenceonMachine
Learning,2555–2565.
Schmidhuber, J. (2010). Formal Theory of Creativity, Fun, and Intrinsic Motivation
(1990–2010). IEEE Transactions on Autonomous Mental Development, 2(3), 230–
247.
Nagabandi,A.,Kahn,G.,Fearing,R.S.,&Levine,S.(2018). NeuralNetworkDynam-
39
ics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),7559–7566.
Lakshminarayanan,B.,Pritzel,A.,&Blundell,C.(2017). SimpleandScalablePredic-
tiveUncertaintyEstimationusingDeepEnsembles. AdvancesinNeuralInformation
ProcessingSystems,6402–6413.
DeBoer,P.-T.,Kroese,D.P.,Mannor,S.,&Rubinstein,R.Y.(2005). ATutorialonthe
Cross-EntropyMethod. AnnalsofOperationsResearch,134(1),19–67.
Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R.
(2016). Unifying Count-Based Exploration and Intrinsic Motivation. Advances in
NeuralInformationProcessingSystems,1471–1479.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-Driven Explo-
ration by Self-Supervised Prediction. Proceedings of the IEEE Conference on Com-
puterVisionandPatternRecognitionWorkshops,16–17.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT
Press.
Guez, A., Silver, D., & Dayan, P. (2013). Scalable and Efficient Bayes-Adaptive Rein-
forcementLearningBasedonMonte-CarloTreeSearch. JournalofArtificialIntelli-
genceResearch,48,841–883.
Auer,P.,Cesa-Bianchi,N.,&Fischer,P.(2002).Finite-timeAnalysisoftheMultiarmed
BanditProblem. MachineLearning,47(2-3),235–256.
40
Champion,T.,DaCosta,L.,Bowman,H.,&Grzes´,M.(2022). BranchingTimeActive
Inference: Thetheoryanditsgenerality. NeuralNetworks,151,295–316.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active inference
agents using Monte-Carlo methods. Proceedings of the 34th Conference on Neural
InformationProcessingSystems(NeurIPS).
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T., Kronbichler, M., &
Friston,K.(2018). Computationalmechanismsofcuriosityandgoal-directedexplo-
ration. eLife,7,e41703.
FitzGerald, T. H. B., Dolan, R. J., & Friston, K. (2015). Dopamine, reward learning,
andactiveinference. FrontiersinComputationalNeuroscience,9,136.
Kocsis, L., & Szepesva´ri, C. (2006). Bandit Based Monte-Carlo Planning. In J.
Fu¨rnkranz, T. Scheffer, & M. Spiliopoulou (Eds.), Machine Learning: ECML 2006.
Lecture Notes in Computer Science (Vol. 4212, pp. 282–293). Springer, Berlin, Hei-
delberg.
Tschantz,A.,Millidge,B.,Seth,A.K.,&Buckley,C.L.(2020). ReinforcementLearn-
ing through Active Inference. Proceedings of the Workshop on Bridging AI and
CognitiveScience(ICLR2020).
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., & Pezzulo, G. (2015).
Activeinferenceandepistemicvalue. CognitiveNeuroscience,6(4),187–214.
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020). Scaling Active Infer-
41
ence. InProceedingsofthe2020InternationalJointConferenceonNeuralNetworks
(IJCNN)(pp.1–8).IEEE.
Millidge, B. (2020). Deep active inference as variational policy gradients. Journal of
MathematicalPsychology,96,102348.
Friston, K. J., Daunizeau, J., & Kiebel, S. (2009). Reinforcement Learning or Active
Inference? PLoSONE,4(7),e6421.
Houlsby, N., Husza´r, F., Ghahramani, Z., & Lengyel, M. (2011). Bayesian
Active Learning for Classification and Preference Learning. arXiv preprint
arXiv:1112.5745.
Gal, Y., Islam, R., & Ghahramani, Z. (2017). Deep Bayesian Active Learning with
ImageData. Proceedingsofthe34thInternationalConferenceonMachineLearning,
1183–1192.
Kozachenko, L. F., & Leonenko, N. N. (1987). Sample Estimate of the Entropy of a
RandomVector. ProblemsofInformationTransmission,23(2),95–101.
Millidge, B. (2019). Deep active inference as variational policy gradients. Journal of
MathematicalPsychology,96,102348.
42

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Boosting MCTS with Free Energy Minimization"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
