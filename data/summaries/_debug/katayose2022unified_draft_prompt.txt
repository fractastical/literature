=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A unified theory of learning
Citation Key: katayose2022unified
Authors: Taisuke Katayose

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Recently machine learning using neural networks (NN) has been developed, and many new
methodshavebeensuggested. Thesemethodsareoptimizedforthetypeofinputdataandwork
very effectively, but they cannot be used with any kind of input data universally. On the other
hand, the human brain is universal for any kind of problem, and we will be able to construct
artificial general intelligence if we can mimic the system of how the human brain works. We
consider how the human brain learns things uniformly, ...

Key Terms: taisuke, human, theory, information, learning, data, katayose, brain, unified, kind

=== FULL PAPER TEXT ===

OU-HET-1138
A unified theory of learning
∗
Taisuke Katayose
Department of Physics, Osaka University, Toyonaka 560-0043, Japan
Abstract
Recently machine learning using neural networks (NN) has been developed, and many new
methodshavebeensuggested. Thesemethodsareoptimizedforthetypeofinputdataandwork
very effectively, but they cannot be used with any kind of input data universally. On the other
hand, the human brain is universal for any kind of problem, and we will be able to construct
artificial general intelligence if we can mimic the system of how the human brain works. We
consider how the human brain learns things uniformly, and find that the essence of learning is
the compression of information. We suggest a toy NN model which mimics the system of the
human brain, and we show that the NN can compress the input information without ad hoc
treatment, only by setting the loss function properly. The loss function is expressed as the sum
oftheself-informationtorememberandthelossoftheinformationalongwiththecompression,
and its minimum corresponds to the self-information of the original data. To evaluate the self-
information to remember, we provided the concept of memory. The memory expresses the
compressed information, and the learning proceeds by referring to previous memories. There
are many similarities between this NN and the human brain, and this NN is a realization of
the free-energy principle which is considered to be a unified theory of the human brain. This
work can be applied to any kind of data analysis and cognitive science.
∗taisuke.katayose@het.phys.sci.osaka-u.ac.jp
2202
rpA
42
]GL.sc[
2v14961.3022:viXra
Contents
1 Introduction 2
2 Consideration about human learning 3
2.1 General definition of learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Correlation and probability bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 Probability bias and redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.4 Detection of redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.5 Compression of information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3 The model of neural network 5
4 The self-information and the loss function 7
4.1 The self-information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 The loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.3 The meaning of the loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5 The role of memory and the hidden variables 12
5.1 The redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 The hidden variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.3 The role of memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.4 The memory as input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6 The uncertainty of the solution 15
6.1 The case with α=0, β =0.01 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.2 The case with α=0.01, β =0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.3 The case with α=0.2, β =0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.4 The case with α=0.5, β =0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.5 Properties of the loss functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
7 Discussion 18
8 Conclusion 19
1
1 Introduction
Recently machine learning using neural networks (NN) has been developed and many new methods
have been suggested [1]. For example, convolutional NN[2] for image processing, attention model[3]
for natural language processing, and generative adversarial networks[4] for classification problem
have achieved prominent results. These models are constructed by considering the properties of the
input data and reflecting these properties in the model. On the other hand, thinking about the
human brain, it does not seem to need such ad hoc treatment and can adapt to any kind of problem.
Then, what determines how the human brain works, and why is the human brain so versatile?
To answer these questions, we need to reconsider the meaning of learning and find out the theory
behind it. In physics, there is the principle of least action, and the motion of objects is determined
by minimizing the action. There must be a corresponding concept to the theory of learning, and
this is nothing but the loss function. Learning proceeds by minimizing the loss function. From this
perspective, we consider that the essence of the problem is in the loss function and the NN model
is just a tool to find a minimum of the loss function. In other words, the upgrade of the NN model
may improve the convergence of the calculation but cannot solve the unsolvable problem because
of the bad setting of the loss function. This situation is commonly happening in the study of the
NN, so we need not only to make a new NN model for the individual cases but also to reconsider
the essential meaning of the loss function. We claim that the loss function is not to be designed
for good results but to be defined theoretically. The goal of this paper is to understand the way
how the human brain works and to derive the general loss function which enables the NN to work
similarly to the human brain by minimizing it. We emphasize that the actual implementation is not
discussed in this paper.
To achieve our goal, we need to define learning mathematically and derive the loss function in the
calculable form. By careful consideration, we found that learning is the compression of information.
We make a toy NN model and then define the loss function which enables the compression of
information. This NN is a kind of autoencoder, and we do not need teacher data or ad hoc treatment
for this NN. We referred to the system of the human brain to construct this NN model, and the
outstanding property is recording the output from the middle layer every time a new input enters.
This is called memory, and the loss function is defined using these memories. The memory denotes
the compressed information, and it can express the abstract information as same as actual human
memory. We can find many similarities between this NN and the human brain, and this work also
can be used in cognitive science.
This paper is arranged as follows. In Sec.2, we consider the essential meaning of learning and
mention that learning is the compression of information. In Sec.3 we define a toy NN model which
mimics the system of the human brain. This is an autoencoder type NN model, and we do not go
into the details of the model, because our goal is to derive the universal loss function. In Sec.4,
we define the self-information to remember and loss of information and show that the sum of these
2
quantities corresponds to the self-information of the input data. We also show that the loss function
is defined as the prediction of these quantities through the NN. In Sec.5, we discuss the concept of
memory and introduce the concept of the hidden variables. We show that the memory expresses
the compressed and abstract information as similar to the memory of the human brain. In Sec.6,
we mention the uncertainty of the solution to minimize the loss function, and we suggest a new
loss function to solve this problem. We demonstrate how this loss function works by using simple
examples. In Sec.7, we discuss the reason why our model does not need ad hoc treatment. We also
discuss unclear part of the concept of memory and the similarity between this NN and the human
brain. In Sec.8, we summarize our study.
2 Consideration about human learning
In this section, we consider how the human brain learns things. To make it easy to understand, we
discuss the concepts step by step.
2.1 General definition of learning
The word learning has a wide range of meaning, and this word is used when we acquire some skills
through training. For thinking about learning, we need to define what is learning mathematically.
Let us give two examples of learning and find out the common point. For the first example, we can
say that a baby learned the word cat when he understands what is a cat. For the second example,
we can say that a dog learned tricks, after repeating the training such as feeding him if he follows
what the owner says. Then, what is the thing these learning have in common? Our answer to
the question is that learning is the discovery of the correlation between information. In the first
example, the baby notices that he tends to hear the word cat pronounced when he is seeing a cat,
and he discovers the correlation between hearing the pronounced word cat and seeing a cat. In the
second example, the dog notice that he can get food after following what the owner says, and he
discovers the correlation between having food and following what the owner says. This statement
applies to any kind of learning and we can identify the learning as the discovery of the correlation
between the information.
2.2 Correlation and probability bias
Next, let us explain the correlation between the information as discussed in Sec.2.1. To reveal the
essence of the correlation between information, let us explain it using coding theory. Suppose the
two bits binary system, which can take the following states defined as
E ≡ (0,0), E ≡ (0,1), E ≡ (1,0), E ≡ (1,1). (1)
1 2 3 4
In this system, the first bit takes a 0/1 value and the second bit also takes a 0/1 value, and we
consider each bit expresses different information. Making the correspondence to the case with the
3
baby learns the word cat, the first bit expresses whether the baby hears the word cat and the second
bit expresses whether the baby sees a cat. In such a case, there emerges the correlation between the
first bit and the second bit, and we can think that the probabilities of getting E and E become
1 4
higher. This probability bias is caused by the correlation between the information, and we can say
that the learning is the detection of the probability bias.
2.3 Probability bias and redundancy
The next question is how the brain detects such probability bias automatically. To clarify what
happens when the probability distribution is biased, let us consider the sample case as in Sec.2.2.
We set the probability distribution of the states as follows:
P(E ) = 0.6, P(E ) = 0.1, P(E ) = 0.1, P(E ) = 0.2. (2)
1 2 3 4
where P(·) denotes the probability that the argument happens, and we set the probabilities of E
1
and E higher than others. If the probability is biased, it is known that redundancy emerge. The
4
redundancy R is defined as
R ≡ H −H, (3)
max
where H is the information entropy defined as
(cid:88)
H ≡ − P(E )logP(E ), (4)
i i
i
and H is the maximum value of the information entropy when we change the probability distri-
max
bution. For the case with Eq.(2), H becomes maximum when E happens with the same probability,
i
and H is calculated as
max
4
(cid:88)
H = − (1/4)log(1/4)
max
(5)
i=1
= 1.386··· .
The information entropy H is calculated as
4
(cid:88)
H = − P(E )logP(E )
i i
(6)
i=1
= 1.088··· .
Then the redundancy for this case is calculated as
R = H −H = 0.298··· . (7)
max
The redundancy R always has a positive value for a biased probability distribution, and we can say
that learning is the detection of redundancy.
4
2.4 Detection of redundancy
The next question is how our brain detects such redundancy. Let us consider the coded information
which has redundancy. For example, let us consider the following binary code:
C = 111000111111000111000111000000. (8)
Then we can compress this information with following replacement:
000 → 0
111 → 1 (9)
C → C(cid:48) = 1011010100.
Even for general cases, using some methods, we can compress the information. Then, where does the
redundancy go? The answer is that redundancy is embedded in how we compress the information.
If there is redundancy in the information, we can find some way to compress it, and the redundancy
corresponds to the rule of how we compress it. Summarizing the statement of Sec.2 up to now,
the learning is the discovery of the correlation, the correlation makes the probability bias, the
probability bias makes the redundancy, and the redundancy can be compressed by the proper rule
for the compression. We can conclude that learning is the discovery of the rule for the compression
of information.
2.5 Compression of information
The next question is how our brain compresses the information. This is nothing but the theme of
this paper, and our answer is explained as follows. The ultimate motivation of the human brain is
to survive, and for this purpose, the human brain tries to record incoming information as much as
possible. However, the amount of incoming information is very huge, and the human brain is limited,
so it can record only compressed information. For example, when we see the image of an apple,
millions of our optic neurons carry the information to our brain, but we only remember very abstract
information such as “the image was an apple.”, and we forget the details of the image. This is lossy
compression, and there is the loss of information by forgetting the detail of the information. In this
process, the human brain tries to reduce the amount of information to remember and also tries to
reduce the loss of information, as a survival strategy. We develop the quantitative evaluation of these
amountsofinformationandfindthattheidealdatacompressioncanbeachievedbyminimizingthese
values. In the following paper, we discuss the evaluation of these values and how the compression is
achieved by considering the NN model as a toy model of our brain.
3 The model of neural network
In this section, we define a new NN model as a toy model of the human brain to evaluate the amount
of information to remember and the loss of the information. Our goal is to derive these quantities,
5
Input layer Middle layer Output layer
Encoding layers Decoding layers
Figure 1: The conceptual diagram of autoencoder NN model. The autoencoder has input layer, encoding
layer, middle layer, decoding layers and output layer. The dimension of the middle layer is less than that
of the input layer. The output of the NN tries to reproduce the input layer from the middle layer with less
dimension.
and we do not care about the actual implementation of the model.
First, asanexampleoftheNNmodelwhichcompressestheinputdatawithoutadhoctreatment,
let us explain the overview of autoencoder NN[5]. The autoencoder has the input layer, encoding
layers, middle layer, decoding layers, and output layer as shown in Fig.1. Here, the dimension of
the middle layer is less than that of the input layer, and the dimension of the output layer is the
same as that of the input layer. The loss function is defined as the difference between the input
and output, so the NN tries to restore the input data from the reduced information in the middle
layer. We can consider this system as the compression and decompression of the input data, and
the middle layer can extract the features of the input data.
However, this model has two problems to achieve our goal. First, there is no guideline to choose
the dimension of the middle layer, so we choose it looking at the result. We have to step out of such
ad hoc treatment. The second problem is there is no evaluation of the amount of information to
remember, and the NN only tries to reduce the loss of the information. We need a new system to
treat the amount of information to remember and the loss of the information on the same ground.
We suggest a new NN model to solve these problems. For the first problem, we do not constrain
the dimension of the middle layer if it is large enough to contain the features of the input. For the
second problem, we record the value of variables in the middle layer to the storage every time a new
input enters, and use them to evaluate the amount of information to remember. We call the value
of the middle layer the memory in the following paper, and we need to calculate the self-information
of the memory. The conceptual diagram for this model is shown in Fig.2. We will discuss how the
self-information of the memory is evaluated in Sec.4.
6
Input layer Middle layer Output layer
Memories
Encoding layers Decoding layers Storage
To storage
Figure 2: The conceptual diagram of our NN model. Different from the usual autoencoder model, the
dimension of the middle layer does not need to be less than that of the input layer. Instead, we record the
value of the middle layer to the storage every time when new input enters and the calculation is done. We
call the output from the middle layer the memory.
To clarify the situation, we explain it more concretely. Suppose the dimension of the input layer
is 4, the dimension of the middle layer is 3, and the dimension of the input layer is 4. The input E
has the form of a vector with dimension 4, and it is written as
E = (e ,e ,e ,e ). (10)
1 2 3 4
The output from the middle layer, or the memory M, has the form of a vector with dimension 3,
and it is written as
M = (m ,m ,m ), (11)
1 2 3
where M is calculated from the input E and we record M to the storage. The output O has the
form of a vector with dimension 4, and it is written as
O = (o ,o ,o ,o ), (12)
1 2 3 4
whereO is calculated from thememoryM. If anew inputE(cid:48) enters, the NN calculatea newmemory
M(cid:48) and a new output O(cid:48), and we record M(cid:48) to the storage. Repeating this process, we have a lot of
memories in the storage. Here, we consider that e and m take discrete values, namely, E and M
i i
are discrete vectors. This is a very important assumption, and we will mention it in Sec.4.
4 The self-information and the loss function
Our goal is to formularize the amount of information to remember and the loss of information. The
amount of information is usually discussed in the framework of self-information. In this framework,
7
we mention how to define the amount of information of the memory and the loss of information.
Finally, we derive the loss function which can be used generally.
4.1 The self-information
Suppose that E is the event that occurs with the probability P(E). We can consider that the rare
event has large information, and we can formularize the self-information I(E) as
I(E) = −logP(E). (13)
Let us consider E as the input of the NN defined in Sec.3. Here, we do not restrict the type of E.
We consider that E corresponds to the raw data of our optic nerve or auditory nerve when we see or
hear something. For example, if E is the event of seeing a cat, we enter E as a form of pixel image
data of a cat. Strictly speaking, our nerves carry the chemical substance, and the number of them
must be discrete not continuous. Hence, we consider E as a discrete vector. For the usual event, we
cannot define P(E) because there are so many types of inputs and the same input does not occur
twice. However, if we could prepare the infinite number of input data sets, the same events occur
many times for the discrete vector, and we can define P(E) well. In this case, P(E) takes a very
small but non-zero value, and in the following, we take P(E) as this meaning.
Next, we will derive the amount of information to remember. We defined the memory as the
value of variables in the middle layer in Sec.3, and let us consider the the amount of information of
the memory M. We also take M as a discrete vector. By the definition of the self-information, we
can write the self-information of M as
I(M) = −logP(M). (14)
Here, the question is what P(M) is. This is the probability of getting the memory M, and we can
define P(M) by using the recorded memories as
n(M)
P(M) = , (15)
N
where n(M) is the number of the memory M recorded up to now and N is the total number of
memories in the storage. We emphasize that the NN may record the same memory from different
inputs because the memory is a discrete vector, and we count how many times the same memory M
was recorded, which corresponds to n(M). However, if the parameter space of the memory is much
larger than N, n(M) equals 0 usually, and it becomes difficult to define P(M) well. The prescription
for this problem will be mentioned in Sec.4.2.
Next, let us consider the loss of information along with the compression. When E enters as the
input, we have all the information of E, but after recording M, we only have the information that
M is recorded. In this situation, there is a possibility that M is recorded from the different input
E(cid:48) and we cannot tell which was the actual input only from the memory M. For example, suppose
8
drawing one card from playing cards, such as king of hearts. Then, suppose that we only remember
that the suit of the card was the heart and forget the number. For this case, we lose the information
of number and this loss of information L can be evaluated as
L = −logP(King of hearts|Hearts)
(16)
= −log(1/13),
where P(A|B) denote the conditional probability where A happens under the condition of B. Gener-
alizing this statement to our NN model, the loss of information L when E enters under the condition
that M is recorded is written as
L = −logP(E|M). (17)
Here, we can observe interesting equations from these quantities. From the Bayes’ theorem, we can
write the probability of input E as
P(E) = P(M)P(E|M). (18)
Then taking the log of both sides and converting the sign, we get
(cid:0) (cid:1)
I(E) = −logP(M)−logP(E|M)
(19)
= I(M)+L.
This corresponds to the conservation of self-information, which means that some portion of the input
information will be embedded in the memory and the other portion of the input information will be
lost.
4.2 The loss function
Next, we explain how to estimate I(M) and L using the NN defined in Sec.3. Then, we will see this
estimation is nothing but the loss function that we want to derive.
Let us start from the estimation of I(M). For this purpose, we need to estimate n(M). However,
we have the problem that n(M) tends to be 0 when the parameter space of the memory is large. For
example, when the middle layer has 100 dimensions and each node takes a binary value, the number
of cases is 2100. This value must be larger than N, and exactly the same memory will not appear.
To solve this problem, we add the condition that a similar memory should express similar data, and
take the average among the near region. From this condition, we can consider P(M) (cid:39) P(M(cid:48)) if
M (cid:39) M(cid:48). Then we define the estimation of P(M) as
(cid:0) (cid:1)
P(M) ≡ avg P(X)|
X∈S
avg (cid:0) n(X)| (cid:1) (20)
X∈S
= ,
N
9
M 4 Neighborhood S
M
1
d
1
M
d
d 3
2
M M M
5 2 3
Figure 3: The conceptual diagram of neighborhood S from the memory M. The memories can be recorded
as the lattice points, and black dots denote the memories in the storage. This diagram corresponds to the
case of taking n = 3 in Eq.(22), and we take the average among the lattice points in the blue circle to
calculate P(M).
where P(M) denotes the estimation of P(M), avg(·) is the function to take the average of the argu-
ment, and S is the neighborhood of the memory M. By taking the average among the neighborhood
S, we can get a non-zero value for P(M) if we set S as large enough to contain a non-zero number
of memories. Here, the problem is how to define S. One suggestion is as follows. Let us name the
memories in the storage as M ,M ,...,M , then we calculate the distance between a new memory
1 2 N
M and a recorded memory M as
i
(cid:112)
d = (M −M )2. (21)
i i
Next, we rename d , arranging in ascending order as d ≤ d ≤ ··· ≤ d is satisfied. Then we define
i 1 2 N
S using the distance between memories as
(cid:8) (cid:12) (cid:9)
S = X(cid:12)(X −M)2 ≤ (d )2 , (22)
n
where n is the number of the memories that are contained in the neighborhood S, and we can choose
an arbitrary number for n. By doing this, P(M) takes the average in the neighborhood within the
radius d . We show the conceptual image of the neighborhood S in Fig.3. This suggestion for the
n
definition of S is just an example, and it will be improved in future work.
Next, we mention the estimation L using this NN. First, let us discuss the case of Eq.(16) more
10
deeply. The conditional probabilities under the condition of getting hearts are written as
P(Ace of spades|Hearts) = 0,
.
.
.
P(King of spades|Hearts) = 0,
P(Ace of hearts|Hearts) = 1/13,
.
.
.
P(King of hearts|Hearts) = 1/13,
(23)
P(Ace of diamonds|Hearts) = 0,
.
.
.
P(King of diamonds|Hearts) = 0,
P(Ace of clubs|Hearts) = 0,
.
.
.
P(King of clubs|Hearts) = 0.
This is the list of P(X|Hearts) where X can take any kind of playing cards, and we can think that
P(King of hearts|Hearts) corresponds to the case with X = King of hearts. By the analogy of this
case, we first consider P(X|M) to get P(E|M) where X can be any type of input data. Hence, the
output of the NN tries to estimate the probability distribution P(X|M), and let us call this output
P(X|M). We give an example of such an output. Let us take the input E as binary data set with
dimension 3, such as (0,1,0). Through the NN, this input is compressed into the memory M and
the output predicts the probability of the input from the compressed information M. The output
has 3 nodes and each node tries to predict the probability of the corresponding node of input data
to have the value 1. For example, if the output is (0.1,0.9,0.2), this output predicts that the first
node of the input takes 1 with 10%, the second node of the input takes 1 with 90%, and the third
node of the input takes 1 with 20%. In this case, the P(E|M) is calculated as
P(E|M) = (1−0.1)×0.9×(1−0.2). (24)
Though we just explained the case with dimension 3, we can generalize this for any number of input
dimensions. Moreover, any type of input data can be recast into binary data, and it means that we
can always use this method in principle.
Finally, we set the loss function L(E) to be the sum of the estimation of I(M) and L as
L(E) ≡ −logP(M)−logP(E|M). (25)
11
4.3 The meaning of the loss function
To discuss the meaning of the loss function, we need to mention the important property of the
information entropy. Suppose P(X) and Q(X) is the probability distribution about X and
(cid:88) (cid:88)
P(X) = Q(X) = 1 (26)
X X
is satisfied. In this case, we can write the following inequality as
(cid:88) (cid:88)
− P(X)logP(X) ≤ − P(X)logQ(X), (27)
X X
where left-hand side corresponds to the information entropy, or the expectation value of self-
information for P(X). This inequality always holds for any probability distribution P(X) and
Q(X), and the equal sign holds only when P(X) = Q(X) is satisfied.
Using this inequality for the true probability distribution of input P(E) = P(M)P(E|M) and
the prediction of the probability distribution of input P(E) = P(M)P(E|M), we get
(cid:88) (cid:88)
P(E)(−logP(M)−logP(E|M)) ≤ P(E)(−logP(M)−logP(E|M)). (28)
E E
Here, the left-hand side corresponds to the expectation value of I(E), and the right-hand side
corresponds to the expectation value of L(E). Then we can write the following inequality as
I(E)| ≤ L(E)| , (29)
exp exp
where ·| means the expectation value of the function. This inequality has a very important
exp
meaning. As the learning proceeds, the loss function L(E) gets closer to the true self-information
of the input I(E), and the minimum point corresponds to I(E). Using this loss function, we can
estimate the actual self-information I(E) without knowing the true probability distribution P(E).
5 The role of memory and the hidden variables
In Sec.2, we mentioned that learning is the detection of redundancy. In this section, we discuss how
the redundancy is compressed, and we also consider the role of the memory.
5.1 The redundancy
Although we briefly explained about the redundancy in Sec.2.3, we will explain the more general
and complicated cases here. Suppose that we take the animal images as the input of the NN, and
each image is expressed as the pixel binary data. Let us discuss what is redundancy in this case.
Taking the number of the pixels of the input data as l, the number of the cases of the input data is
12
2l. If there is no probability bias, each data will appear completely randomly, and we can calculate
the maximum of the information entropy H as
max
H = −log2−l, (30)
max
Among the random inputs, only a very tiny parameter region can be considered to be an animal
image. Hence, the input of animal image have very peaky probability bias on the parameter space
of the input, and the expectation value of self-information is written as
(cid:88)
I(E)| = − P(E)logP(E)
exp
(31)
E
= −log2−l −R,
where E denotes the input of animal image, and R is the redundancy defined in Eq.(3). From the
eq.(29), the NN compresses R as the learning proceed.
5.2 The hidden variables
The input of the animal image has l variables, and these variables are strongly correlated and form
probability bias. We can consider this situation as follows. There are hidden variables such as
the kind of the animal, the size of the animal, and the posture of the animal, and the input data
is decided by these hidden variables. Here, the dimension of the hidden variables is less than l.
Considering the meaning of the redundancy R from this perspective, we find there are two types of
redundancy. The first is the redundancy from the mapping of the hidden variables to input data.
For example, if the hidden variables specify the kind of animal as a cat, there must be mapping
to input data that looks like a cat, and it makes the correlation between the input variables. The
second is the redundancy from the prior probability distribution of hidden variables. For example,
if the probability of the hidden variables which denote a cat is much higher than that of a kangaroo,
it also can be a source of redundancy.
5.3 The role of memory
The NN tries to reduce −logP(M), which means the NN tries to increase P(M), and from Eq.(20),
the NN tries to record the memories as near as possible. We show the conceptual diagram in Fig.4.
Namely, the memory tries to record the information with fewer variables, and this is nothing but
the hidden variables. From this consideration, we guess that the redundancy from mapping of the
hidden variables to input data is embedded in the encoding layer, and the redundancy from the prior
probability distribution of the hidden variables is embedded in the probability distribution of the
memory. The hidden variables are considered to have abstract information, such as “this is a cat”
or “this is sitting”, and recording the memory corresponds to recording such abstract information.
This is very similar to the human brain which only remembers abstract information and forgets
trivial information.
13
Probability distribution Probability distribution Probability distribution
Parameter space of the memory Parameter space of the memory Parameter space of the memory
Figure 4: The conceptual diagram which shows how the memories are recorded and how the probability
distribution of the memory is updated. Each vertical black line denotes each memory.
Next, let us discuss the probability distribution of the memory. Suppose the learning of the
animal image, there considered to be many hidden variables, such as “having hair”, “having a tail”,
or “having slippery skin”. Among the combination of these hidden variables, some combinations will
appearfrequently. Forexample,thecombinationof“havingslipperyskin”and“havingastreamlined
shape”meansthecharacteristicsoffish,andotheranimalsalsohavesuchacombinationofthehidden
variables. These combinations have a higher probability than random combinations, and it means
that there emerge some peaks of the probability distribution in the parameter space of memory. We
can consider that each peak corresponds to the species of animals. If the learning proceed more,
there emerge minor peaks which correspond to a detailed classification of the animals. We illustrate
this situation in Fig.5. In this figure, the darker region has a higher probability distribution, which
means the density of the recorded memories is higher in such region. The important point is that
such a structure of the probability distribution of memory automatically emerges without ad hoc
treatment. We guess that this is exactly the way how the human brain remembers and learns things.
5.4 The memory as input
Next, let us consider what happens if we set the memories as input. In Sec.5.3, we mentioned that
memories also have their probability distribution. This means that memories are also controlled by
other hidden variables, and we can compress the information again. Repeating this process many
times, we can find deeper hidden variables which express more abstract information. We can also set
different types of memories as input simultaneously, then the NN will learn the correlation between
different types of information. For example, if we enter the memories from voice data of the name of
animals and the memories from image data of animals simultaneously, the NN will learn the name
of animals. The essence of learning is finding the hidden variables by compressing the input data,
and the NN will learn more deeply by repeating this process.
14
Parameter space of memory
[animal]
[pigeon]
[cat]
[sparrow]
[bird]
[dog]
[salmon]
[mammal]
[fish]
[shark]
Figure 5: The conceptual diagram of the probability distribution in the parameter space of memory. We
considered the case of learning the image of animals, and a darker region denote higher probability, in other
words, memories are distributed with a higher density in a darker region. The peaks of the probability
distribution correspond to the species of animals.
6 The uncertainty of the solution
Here, we mention the uncertainty of the solution which minimizes this loss function. The self-
information of the memory and the loss of information are inextricably linked, and there are several
types of realization to obtain the minimum of the loss function whether the NN tends to record
information as much as possible or abandon information as much as possible. To control this
uncertainty, we suggest a new loss function as
L(E;α,β) = −(1+α)logP(M)−(1+β)logP(E|M) (32)
where α and β is a small non-negative constant. In the following, we demonstrate how this loss
function works by changing the value of α and β, using a very simple example. To help understand
how this loss function works, we consider two-dimensional binary input data and name them as
E ≡ (0,0), E ≡ (0,1), E ≡ (1,0), E ≡ (1,1). (33)
1 2 3 4
We also set the memory to have the shape of two-dimensional binary data and name them as
M ≡ (0,0), M ≡ (0,1), M ≡ (1,0), M ≡ (1,1). (34)
1 2 3 4
Then, for instance, let us consider the case mentioned in Sec.2.2 and set the probability as same as
Eq.(2)
P(E ) = 0.6, P(E ) = 0.1, P(E ) = 0.1, P(E ) = 0.2. (35)
1 2 3 4
The expectation value of the self-information of input data is calculated as
4
(cid:88)
I(E)| = − P(E )logP(E )
exp i i
(36)
i=1
= 1.088··· .
15
Next, we consider when the loss function takes its minimum, changing the value of α and β. Here,
there are only four types of memories, and we do not need to average over the neighborhood to
calculate P(M).
6.1 The case with α=0, β =0.01
Let us take L(E;0,0.01) as the loss function. The minimum of the loss function is achieved when
the encoding layers work as
E → M ,
1 1
E → M ,
2 2
(37)
E → M ,
3 3
E → M ,
4 4
and the decoding layers work as
M → (p ,p ) = (0,0),
1 1 2
M → (p ,p ) = (0,1),
2 1 2
(38)
M → (p ,p ) = (1,0),
3 1 2
M → (p ,p ) = (1,1).
4 1 2
Then the expectation value of the loss function is
L(E,0,0.01)| =−0.6×[(1+0)log0.6+(1+0.01)log1]
exp
−0.1×[(1+0)log0.1+(1+0.01)log1]
−0.1×[(1+0)log0.1+(1+0.01)log1] (39)
−0.2×[(1+0)log0.2+(1+0.01)log1]
=1.088··· .
6.2 The case with α=0.01, β =0
Next, let us take L(E;0.01,0) as the loss function. The minimum of the loss function is achieved
when the encoding layers work as
E → M ,
1 1
E → M ,
2 1
(40)
E → M ,
3 2
E → M ,
4 2
and the decoding layers work as
M → (p ,p ) = (0,1/7),
1 1 2
(41)
M → (p ,p ) = (1,2/3),
2 1 2
16
Then the expectation value of the loss function is
L(E,0.01,0)| =−0.6×[(1+0.01)log0.7+(1+0)log6/7]
exp
−0.1×[(1+0.01)log0.7+(1+0)log1/7]
−0.1×[(1+0.01)log0.3+(1+0)log1/3] (42)
−0.2×[(1+0.01)log0.3+(1+0)log2/3]
=1.095··· .
6.3 The case with α=0.2, β =0
Next, let us take L(E;0.2,0) as the loss function. The minimum of the loss function is achieved
when the encoding layers work as
E → M ,
1 1
E → M ,
2 1
(43)
E → M ,
3 1
E → M ,
4 2
and the decoding layers work as
M → (p ,p ) = (1/8,1/8),
1 1 2
(44)
M → (p ,p ) = (1,1),
2 1 2
Then the expectation value of the loss function is
L(E,0.2,0)| =−0.6×[(1+0.2)log0.8+(1+0)log49/64]
exp
−0.1×[(1+0.2)log0.8+(1+0)log7/64]
−0.1×[(1+0.2)log0.8+(1+0)log7/64] (45)
−0.2×[(1+0.2)log0.2+(1+0)log1]
=1.203··· .
6.4 The case with α=0.5, β =0
Next, let us take L(E;0.5,0) as the loss function. The minimum of the loss function is achieved
when the encoding layers work as
E → M ,
1 1
E → M ,
2 1
(46)
E → M ,
3 1
E → M ,
4 1
17
and the decoding layers work as
M → (p ,p ) = (0.3,0.3), (47)
1 1 2
In this case, there are only four type of memories, and we do not need to average over the neighbor-
hood to calculate P(M). Then the expectation value of the loss function is
L(E,0.5,0)| =−0.6×[(1+0.5)log1+(1+0)log0.49]
exp
−0.1×[(1+0.5)log1+(1+0)log0.21]
−0.1×[(1+0.5)log1+(1+0)log0.21] (48)
−0.2×[(1+0.5)log1+(1+0)log0.09]
=1.221··· .
6.5 Properties of the loss functions
From these observations, we can state as follows. If α is zero, the NN tries to record as much
information as possible, and if β is zero, the NN tries to abandon as much information as possible.
As α becomes bigger, the NN starts to reduce the information to remember, and abandons the
information of the correlation between two nodes. For the case with α = 0.01,β = 0 and α =
0.2,β = 0, the NN uses two memories M and M . Considering the case mentioned in Sec.2.2, the
1 2
memory M corresponds to the concept which connects the word cat and the visual image of a cat.
2
7 Discussion
First, we discuss how we solved the problem mentioned in Sec.1. We wondered about the trend of
the current study of the NN, in which people focus on how to get good results by ad hoc treatment.
People are trying to improve the NN model, the loss function, or the style of input data. However,
considering the human brain, it does not need such treatments and can learn everything automat-
ically in daily life. There must be a unified theory that controls the human brain, and we do not
need ad hoc treatment under such a theory. This theory is nothing but the loss function we defined
in Eq.(25). Let us mention that our theory includes previous ad hoc treatments. For example,
convolutional NN is doing coarse-graining, and this can be considered as some kind of compression
of the input data. We know the fact coarse-graining does not lose so much information, and this
fact is implemented in the NN ad hoc. The situation for other good-looking models are more or less
same. People consider the property of the input data and try to use this property and extract the
features of input data by improving the NN models. This corresponds to teaching the NN how to
compress the input data ad hoc. Our theory offers a unified treatment for any kind of input data.
Next, we discuss the memory. We mentioned that the memory expresses the hidden variable
in Sec.5.2, but the realization is not unique. The probability distribution of the memory would
18
be different if we clean up all the memories and restart the learning. Moreover, if we change the
definition of the neighborhood S considered in Sec.4.1 or change the value of α and β considered
in Sec.6, the probability distribution of the memory will change more drastically. There is a lot of
uncertainty, and we need to improve the theory about the hidden variables. Let us also mention
about the treatment of old memories. As the learning proceeds, many memories are recorded in the
storage, butthememoriesrecordedintheearlystageoflearningdonotfollowthedesiredprobability
distribution. To improve the convergence of the calculation, we should remove such memories from
the storage. This also remains as future work.
Finally, we discuss the similarity between this NN and the human brain. This NN is suggested
as a toy model of the human brain, and we do not need ad hoc treatment for the different inputs.
We can use the same type of NN for a different types of input data, and this is very similar to
our brain which can adapt to any kind of problem. This is achieved only by minimizing the loss
function, which is defined as the sum of the self-information to remember and the loss of information.
This situation amazingly agrees with the free-energy principle[6]. The free-energy principle is the
theory in cognitive science proposed by Karl Friston, in which the cognition, learning, and action
of the organism are determined by minimizing the function called free-energy. Our loss function
corresponds to this free-energy, and it is formularized in a calculable way. To calculate the self-
information to remember, we introduced the concept of memory. This is the core of our theory,
and it has many similarities with our actual memory. The memory denotes compressed information,
and it can express abstract information. The human brain also remembers only compressed and
abstract information. In the paragraph above, we discussed the benefit to forget old memories, and
this situation is also similar to our brain, as the human brain tends to forget old memories which
are not often referred to.
8 Conclusion
In this paper, we proposed a unified theory of learning. Learning is the discovery of the correlation
between the information, and this can be achieved through the compression of the information. To
explain how this compression is carried out, we treat a NN model as a toy model of the human
brain and derived a new loss function. In this process, we proposed the concept of memory, and
this is the core of our theory. The loss function is expressed as the sum of the self-information of
the memory and the loss of information. This reflects the self-information of the input data, and
we can estimate it without knowing the actual probability distribution of the input data only by
calculating the minimum of the loss function. To estimate the self-information of the memory, the
NN refers to the recorded memories. Usually, the density of the memories is very low, and we need
to refer to similar memories.
We can think that the correlation of the information is caused by the hidden variables, and
the memory expresses these hidden variables. The hidden variables also have their probability
19
distribution, and it emerges as the density distribution of the memories. The peaks of the density
distribution correspond to the features of the input data. The memories have their probability
distribution, and it means we can find deeper hidden variables by setting the memories as input.
We can also set the different tyeps of memories as input simultaneously, then the NN will find the
correlation between different data. This means the NN can learn the abstract concept, such as the
meaning of the words, by setting the memories of audio data and image data as input. Deeper
hidden variables express more abstract information, and the reputation of this process is nothing
but how the human brain learns.
We also found many common features in this NN model and the human brain. We do not need
ad hoc treatment for the different types of inputs, and this is the same as the human brain which
learns by itself. This is achieved only by minimizing the loss function which denotes the sum of the
information to remember and the loss of the information, and this corresponds to the free-energy
theorem which is considered to be the unified theory of the human brain. The memory of this NN
works very similarly to our actual memory, and it can express very abstract information. The similar
information is recorded as a similar memory. We also mentioned the benefit to forget old memories.
This work has a lot of implications for a very wide range of fields, and it can be applied to any
kind of data analysis. However, there will be many difficulties in the implementation of this work in
the actual NN system. We only offered the definition of the loss function, and we have not checked
the NN numerically. To improve the convergence of the calculation, we will need a lot of study.
We also offered the concepts of memory and hidden variables, but these concepts have so many
theoretical uncertainties. They are totally new concepts and we do not know the properties of them
at all. They will be understood through the calculation of concrete examples. We need not only
experimental approach but also theoretical approach to the NN.
Acknowledgments
The author would like to thank Prof. Haruhiro Katayose for the useful discussion and suggestions.
This work is supported in part by the JSPS KAKENHI Grant No.20H00160.
References
[1] J. Schmidhuber, Neural Networks 61, 85 (2015).
[2] K. O’Shea and R. Nash, CoRR abs/1511.08458 (2015), 1511.08458 .
[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin, CoRR abs/1706.03762 (2017), 1706.03762 .
[4] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, CoRR
abs/1606.03498 (2016), 1606.03498 .
20
[5] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504 (2006).
[6] K. Friston, Nature Reviews Neuroscience 11, 127 (2010).
21

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A unified theory of learning"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
