=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Modeling arousal potential of epistemic emotions using Bayesian information gain: Inquiry cycle driven by free energy fluctuations
Citation Key: yanagisawa2023modeling
Authors: Hideyoshi Yanagisawa, Shimon Honda

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Abstract: Epistemic emotions, such as curiosity and interest, drive the inquiry process. This study proposes a novel
formulation of epistemic emotions such as curiosity and interest using two types of information gain
generated by the principle of free energy minimization: Kullbackâ€“Leibler divergence (KLD) from
Bayesian posterior to prior, which represents free energy reduction in recognition, and Bayesian surprise
(BS), which represents the expected information gain by Bayesian prior update. By applying ...

Key Terms: arousal, inquiry, energy, driven, uncertainty, gain, potential, emotions, information, modeling

=== FULL PAPER TEXT ===

Yanagisawa, H. & Honda, S.
Modeling arousal potential of epistemic emotions using Bayesian
information gain: Inquiry cycle driven by free energy fluctuations
Hideyoshi Yanagisawa*a, Shimon Hondab
a The University of Tokyo, 7-3-1 Bunkyo, Hongo, Tokyo, 113-8656, JAPAN. hide@mech.t.u-tokyo.ac.jp
b The University of Tokyo, 7-3-1 Bunkyo, Hongo, Tokyo, 113-8656, JAPAN. hondar8@g.ecc.u-tokyo.ac.jp
*Corresponding author
Abstract
Epistemic emotions, such as curiosity and interest, drive the inquiry process. This study proposes a novel
formulation of epistemic emotions such as curiosity and interest using two types of information gain
generated by the principle of free energy minimization: Kullbackâ€“Leibler divergence (KLD) from
Bayesian posterior to prior, which represents free energy reduction in recognition, and Bayesian surprise
(BS), which represents the expected information gain by Bayesian prior update. By applying a Gaussian
generative model with an additional uniform likelihood, we found that KLD and BS form an upward-
convex function of surprise (minimized free energy and prediction error), similar to Berlyneâ€™s arousal
potential functions, or the Wundt curve. We consider that the alternate maximization of BS and KLD
generates an ideal inquiry cycle to approach the optimal arousal level with fluctuations in surprise, and
that curiosity and interest drive to facilitate the cyclic process. We exhaustively analyzed the effects of
prediction uncertainty (prior variance) and observation uncertainty (likelihood variance) on the peaks of
the information gain function as optimal surprises. The results show that greater prediction uncertainty,
meaning an open-minded attitude, and less observational uncertainty, meaning precise observation with
attention, are expected to provide greater information gains through a greater range of exploration. The
proposed mathematical framework unifies the free energy principle of the brain and the arousal potential
theory to explain the Wundt curve as an information gain function and suggests an ideal inquiry process
driven by epistemic emotions.
Keywords: Emotion, free energy, Bayes, arousal, curiosity, inquiry.
1. Introduction
Inquiry is an essential cognitive process in human activities such as scientific research, creation, and
education. American philosopher Charles Sanders Peirce defines inquiry as a cycle of three inferences:
abduction, deduction, and induction (Peirce, 1974). In the observation of surprising phenomena,
abduction infers a possible cause of the observation, deduction predicts unknown effects based on the
inferred cause, and induction tests the prediction and updates the causal knowledge. A voluntary inquiry
1
Yanagisawa, H. & Honda, S.
process is facilitated by epistemic emotions such as surprise, curiosity, interest, and confusion (Kashdan
& Silvia, 2009; Vogl, Pekrun, Murayama, & Loderer, 2020). Psychologist Berlyne defined two types of
epistemic curiosity: diversive and specific (Berlyne, 1966; Silvia, 2012). Diversive curiosity seeks
novelty, and thus, in this type of curiosity, surprise triggers abductive reasoning. On the other hand,
specific curiosity drives induction, which seeks evidence of deductive reasoning to resolve confusion.
Emotions are generally mapped to a dimensional space (Lang, 1995; Russell, 1980). The most
commonly used dimensions are arousal and valence, termed the core affect (Russell, 2003). Arousal is the
intensity of emotions, whereas valence is the dimension of the positive and negative poles. A recent
functional magnetic resonance imaging (fMRI) study showed that arousal and valence are correlated with
neural activity in the orbitofrontal cortex and amygdala, respectively (Wilson-Mendenhall, Barrett, &
Barsalou, 2013). The emotional dimensions are not independent, and arousal affects valence. Berlyneâ€™s
arousal potential theory suggests that an appropriate level of arousal potential induces a positive hedonic
response, whereas extreme arousal induces a negative response (Berlyne, 1960). Thus, valence forms an
inverse-U-shaped function of the arousal potential, termed the Wundt curve (Fig. 1). Berlyne suggests that
epistemic curiosity approaches the optimal arousal potential, where the hedonic response (or positive
valence) is maximized (Berlyne, 1960, 1966; Silvia, 2012).
Berlyne also illustrated a number of arousal potential factors such as novelty, complexity, and
uncertainty (Berlyne, 1960). Yanagisawa mathematically explains that the free energy, which is
information on the brainâ€™s prediction error or surprise (Friston, Kilner, & Harrison, 2006), represents the
arousal potential because free energy is decomposed into information quantity terms representing
perceived novelty, complexity, and uncertainty (Yanagisawa, 2021). This free-energy arousal model
suggests that an appropriate level of free energy or surprise induces a positive emotional valence based on
Berlyneâ€™s Wundt curve, which is supported by experimental evidence (Honda, Yanagisawa, & Kato,
2022; Sasaki, Kato, & Yanagisawa, 2023).
By contrast, the free energy principle (FEP) (Friston et al., 2006), known as the unified brain
theory (Friston, 2010), suggests that the brain must minimize its free energy during perception and action.
Previous studies have proposed that decreasing and increasing free energy (or expected free energy)
correspond to positive and negative valence, respectively (Clark, Watson, & Friston, 2018; Hesp et al.,
2021; Joffily & Coricelli, 2013; Seth & Friston, 2016; Wager et al., 2015; Yanagisawa, Wu, Ueda, &
Kato, 2023), and that high and low free energies indicate uncertain and certain states, respectively.
Reducing free energy resolves uncertainty and produces positive emotions.
The FEP argument that minimizing free energy corresponds to a positive valence seems to
contradict the argument of arousal potential theory that an appropriate level of arousal potential
(represented by free energy (Yanagisawa, 2021)) maximizes positive valence. To resolve this
contradiction and integrate the FEP-based valence and arousal potential theories, we propose a novel
valence framework based on the theory that a decrement in free energy and its expectation explain the
2
Yanagisawa, H. & Honda, S.
valence of epistemic emotions. A decrease in free energy represents information gain and an epistemic
value (Friston et al., 2017; Parr, Pezzulo, & Friston, 2022). The more information gain (epistemic value)
one obtains or expects, the more positive the valence one experiences.
Based on this framework, we formulated emotion valence functions of the arousal potential
using decrements in free energy (or information gains). By applying a Gaussian generative model with an
additional uniform likelihood, we demonstrated that the epistemic valence function forms an inverse-U
shape and analyzed the effects of prediction error and uncertainties on the peaks of the valence functions.
We associated epistemic emotions such as curiosity and interest with the free-energy-based valence
model. Furthermore, we proposed an inquiry cycle model based on free-energy-based epistemic emotions.
Positive
Arousal
(Surprise)
Negative
Fig. 1 Arousal potential function, or Wundt curve. Appropriate level of arousal maximizes positive
emotion valence (optimal arousal level).
2. Method
2.1 Free energy formulations
FEP suggests that the brain must minimize its free energy through recognition, action, and learning
(Friston et al., 2006). Assume an agent recognizes a hidden state ğ‘  as a cause of an observation ğ‘œ given
by an action based on a policy ğœ‹. We assume that the agent has a generative model ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667) as its
knowledge about the probabilistic relationship between hidden states and observation and a recognition
density ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667) of hidden states for a given policy. The free energy of a policy ğœ‹ is defined as a function
of an observation representing the difference between a recognition density and a generative model
averaged by the recognition density in terms of their energies (negative log probability).
ğ¹ (cid:3095) (cid:3404)âŸ¨lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)lnğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)âŸ© (cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) (1)
The free energy represents the prediction error of recognition from the knowledge, i.e., the generative
model. It refers to uncertainty and the prediction error of signals in a Bayesian brain theory (Knill &
Pouget, 2004). The first and second terms on the right-hand side denote the negative-state entropy and
3
ecnelaV
Optimal arousal level
Wundt curve
Familiar, simple Novel, complex
Yanagisawa, H. & Honda, S.
internal energy, respectively. Thus, the definition corresponds to the Helmholtz free energy when the
temperature is one.
With the definition of conditional probability, the generative model is factorized into true
posterior and evidence: ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)(cid:3404)ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)ğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667). With this factorization, the free energy is expanded
to the summation of a Kullbackâ€“Leibler (KL) divergence and Shannon surprise (hereafter referred to as
surprise).
ğ¹ (cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:3398)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667) (2)
(cid:3095) (cid:3012)(cid:3013)
The first-term KL divergence forms the true posterior to the recognition density, which represents a
statistical difference between the two distributions: ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:3404)âŒ©lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)
(cid:3012)(cid:3013)
lnğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) . When the recognition approximates the true posterior to minimize free energy, the KL
divergence becomes zero, and the free energy is approximated to the second term, i.e., surprise. Thus, the
lower bound of free energy is surprise. Surprise is a negative log of the model evidence, ğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667), and
refers to the information content used to process given observations, representing cognitive load
(Yanagisawa, 2021).
The generative model is decomposed to a state prior ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667) for a given policy and a likelihood
function ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667).
ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)(cid:3404)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667) (2)
With this decomposition, the free energy is expanded to another two terms.
ğ¹ (cid:3095) (cid:3404)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:4671)(cid:3398)âŒ©lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) (3)
The first term is a KL divergence of state prior from recognition. This term represents the complexity of
the generative model. The second term is the difference between likelihood and recognition. This term
indicates negative model accuracy. Thus, minimizing the free energy signifies minimizing the complexity
and maximizing the accuracy of the model.
2.2 Information gain in recognition
Assume that an initial recognition density before an action based on a policy ğœ‹ is approximated to the
state prior. The initial free energy ğ¹ is a summation of KL divergence and surprise.
(cid:3095)(cid:2868)
ğ¹ (cid:3095)(cid:2868) (cid:3404)âŒ©lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)âŒª (cid:3043)(cid:4666)ğ‘ |ğœ‹(cid:4667) (cid:3404)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:3398)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667) (4)
An agent receives an observation ğ‘œ by the action based on the policy ğœ‹. The recognition density
approximates the true posterior by minimizing the free energy. The KL divergence becomes zero, and the
free energy decreases to the lower bound ğ¹ , corresponding to surprise.
(cid:3095)(cid:3019)
ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667):ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)â†’ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667), ğ¹ (cid:3404)(cid:3398)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667) (5)
(cid:3095)(cid:3019)
The decrease in free energy in the recognition process is equivalent to the KL divergence from the true
posterior to the initial recognition, ğ¾ğ¿ğ· . Herein, ğ¾ğ¿ğ· denotes the information gain from recognizing
(cid:3095) (cid:3095)
the causal state of observations given by an action based on a policy ğœ‹.
ğ›¥ğ¹ (cid:3404)ğ¹ (cid:3398)ğ¹ (cid:3404)ğ¾ğ¿ğ· (cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671) (6)
(cid:3019) (cid:3095)(cid:2868) (cid:3095)(cid:3019) (cid:3095) (cid:3012)(cid:3013)
4
Yanagisawa, H. & Honda, S.
A greater KLD indicates that the recognition of an observation under a policy provides greater
information gain. Thus, KLD represents the epistemic value of recognizing an observation under a policy.
This suggests that an agent prefers to recognize observations with a greater KLD and is motivated to act
based on a policy that likely obtains such observations. Therefore, we infer that KLD increases positive
valence by increasing information gain (epistemic value) in recognition.
2.3 Information gain expected from Bayesian updating prior belief: Bayesian surprise
The free energy minimized by a recognition, ğ¹ , approximates surprise. The minimized free energy
(cid:3095)(cid:3019)
equals a summation of complexity and inverse accuracy with a recognition approximated to the true
posterior, ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3406)ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667).
ğ¹ (cid:3404)(cid:3398)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667)(cid:3404)ğµğ‘† (cid:3397)ğ‘ˆ (7)
(cid:3095)(cid:3019) (cid:3095) (cid:3095)
ğµğ‘† (cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:4671) (8)
(cid:3095) (cid:3012)(cid:3013)
ğ‘ˆ (cid:3095) (cid:3404)(cid:3398)âŒ©lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3043)(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667) (9)
The complexity and inverse accuracy terms represent the Bayesian surprise ğµğ‘† and perceived
(cid:3095)
uncertainty U, respectively, and their summation (surprise) denotes the arousal potential (Yanagisawa,
2021). The Bayesian surprise, ğµğ‘† , is a KL divergence from posterior to prior, i.e., the deviation of
(cid:3095)
recognition from prior expectation. It represents the novelty of the recognized observation and is
correlated with the surprise response to novel stimuli (Yanagisawa, Kawamata, & Ueda, 2019). The
surprise response decreases with repeated exposure to the same novel stimuli. Such habituation is
formulated as a decrease in BS in the Bayesian update of the prior (Ueda, Sekoguchi, & Yanagisawa,
2021).
By repeatedly observing the same observation ğ‘œ by an action under the same policy, the prior
is updated by Bayesian updating such that the prior comes close to the true posterior, i.e.,
ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667):ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)â†’ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667). When the prior is updated to the posterior, ğµğ‘† is zero, and the free energy
(cid:3095)
decreases to the inverse accuracy term. We refer to this term as uncertainty because it refers to the
perceived uncertainty (Yanagisawa, 2021). Thus, the lower bound of free energy after the prior updating
is the uncertainty, ğ¹ (cid:3409)ğ‘ˆ , whereas the upper bound of the free energy decrease is the Bayesian
(cid:3095)(cid:3013) (cid:3095)
surprise, ğµğ‘† .
(cid:3095)
ğ›¥ğ¹ (cid:3404)ğ¹ (cid:3398)ğ¹ (cid:3409)ğµğ‘† (10)
(cid:3013) (cid:3095)(cid:3019) (cid:3095)(cid:3013) (cid:3095)
Herein, ğµğ‘† is equivalent to the maximum information gain expected from the prior update based on
(cid:3095)
observing a sufficient number of the same observations given under the same policy. A greater ğµğ‘†
(cid:3095)
denotes a greater information gain expected from the update with the action under the policy ğœ‹. Thus, BS
represents the expected epistemic value given by the model (prior) update or learning. This suggests that
an agent prefers novel observations with a greater BS, which is expected to provide a chance to learn new
information (update its own generative model), and that the agent is motivated to approach such novel
observations. Therefore, we infer that BS increases emotional valence in anticipation of information gain
5
Yanagisawa, H. & Honda, S.
from updating prior beliefs.
2.4 Linking free energy reduction, information gain, and arousal potential
Fig. 2 summarizes the two-step free energy reduction and information gain. The free energy given an
observation ğ‘œ decreases by ğ¾ğ¿ğ· as the first information gain when one succeeds in recognizing the state
as a cause of the observation. The minimized free energy approximates surprise. The surprise is a
summation of ğµğ‘† and ğ‘ˆ. When oneâ€™s prior is updated to approximate the true posterior, the free energy
is decreased by ğµğ‘†, which is the expected second information gain.
Recognition
KL divergence
Belief update
Bayesian surprise
Surprise
Uncertainty
time
Fig. 2 Two-step free energy reduction and information gain. Decreases in free energy in recognition and
belief update correspond to KL divergence (KLD) and Bayesian surprise, respectively.
The upper bound of the total free energy reduction (or information gain) from recognizing and updating
state beliefs, given an observation, is a summation of the two KL divergences, i.e., information gain.
ğ›¥ğ¹ (cid:3397)ğ›¥ğ¹ (cid:3409) ğ¾ğ¿ğ· (cid:3397)ğµğ‘† (cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:3397)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:4671)â‰¡ğ¼ğº
(cid:3019) (cid:3013) (cid:3095) (cid:3095) (cid:3012)(cid:3013) (cid:3012)(cid:3013)
(11)
We consider that the total information gain represents the epistemic values that explain the emotional
valence of the arousal potential.
The two types of KL divergence denote the difference between the prior and posterior. When
the posterior given an observation is the same as the prior, the KL divergences are zero, and the
observation provides minimum free energy and minimum surprise (or maximum evidence). Hence, an
observation that provides minimal free energy does not provide any KL divergence or information gain.
To provide epistemic value with an emotional valence, given information gain, a certain level of surprise
representing arousal potential (Yanagisawa, 2021) is required by observing unexpected outcomes that
give certain KL divergences. However, if the likelihood of an observation is far from the prior
6
ygrene
eerF
Total information gain
Yanagisawa, H. & Honda, S.
distribution, where the likelihood does not provide any information, the posterior is not updated from the
prior. In this case, the KL divergences are zero, and the observation does not provide any information.
Therefore, we consider that an appropriate level of surprise maximizes the KL divergences (information
gains) and that an appropriate level represents the optimal arousal potential that maximizes the positive
valence for its epistemic value.
KL divergence is an asymmetric operation. Hence, although both KL divergences, ğ¾ğ¿ğ· and
(cid:3095)
ğµğ‘† , denote differences between the prior and posterior, they are different from each other. This suggests
(cid:3095)
that the two KL divergences as functions of surprise are different. KLD signifies information gain due to
recognition, whereas BS signifies information gain expected from updating prior beliefs. Namely, KLD is
the current, whereas BS is the future, information gain. This suggests that maximizing KLD and BS are
different strategies for approaching the optimal arousal level that maximizes the total epistemic value
with a positive valence.
2.5 Analytical methodology
We modeled the two information gains, KLD and BS, as functions of surprise using a Gaussian-like
generative model with a flat likelihood of uniform noise and demonstrated that the two functions, KLD
and BS, form an inverse-U shape and have different peaks. Using the function model, we analyzed the
effect of Gaussian parameters, the difference between the prior mean and likelihood peak as prediction
error (Yanagisawa, 2016), variance of prior as prediction uncertainty, and variance of likelihood as
observation uncertainty on the peaks of the information gain functions. From the analysis, we elucidated
the conditions for optimal prediction errors and uncertainties of prediction and observation to maximize
the information gains in an ideal inquiry process.
3. Results
3.1 Gaussian model of information gains
The Gaussian Bayesian model has been used in past research studies to analyze the characteristics of free
energy and Bayesian surprise (Buckley, Kim, McGregor, & Seth, 2017; Yanagisawa et al., 2023;
Yanagisawa, 2021). The Laplace approximation suggests that a Gaussian distribution is applied around
the mode of unknown distributions. The Gaussian form is useful for analyzing the effect of interpretable
and independently manipulatable parameters on free energy and KL divergence. The distance between the
prior mean and likelihood peak, ğ›¿, represents prediction error; the variance of prior, ğ‘  , represents prior
(cid:3043)
uncertainty; and the variance of likelihood, ğ‘  , represents observation uncertainty. The likelihood
(cid:3013)
function of n data randomly sampled from a source following a Gaussian distribution is
ğ‘(cid:4666)ğ‘œ(cid:3041)|ğ‘ (cid:4667)(cid:3404)(cid:3436) (cid:2869) (cid:3440) (cid:3041) exp(cid:4674) (cid:2879)(cid:4666)(cid:3046)(cid:2879)(cid:3042)(cid:3364)(cid:4667)(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4675), (12)
(cid:3493)(cid:2870)(cid:3095)(cid:3046)(cid:3261) (cid:2870)(cid:3046)(cid:3261)
where ğ‘œÌ… and ğ‘‰ denote the mean and variance of the observed data, respectively. With a Gaussian prior
distribution ğ‘(cid:4666)ğ‘ (cid:4667)(cid:3404)ğ‘(cid:3435)ğœ‚,ğ‘  (cid:3439)â‰¡ğ‘ , the posterior distribution is also of a Gaussian form.
(cid:3043) (cid:3043)(cid:3045)(cid:3036)
7
Yanagisawa, H. & Honda, S.
ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:3041)(cid:4667)(cid:3404)
(cid:3043)(cid:3435)ğ‘œ(cid:3041) (cid:3627)ğ‘ (cid:3439)(cid:3043)(cid:4666)(cid:3046)(cid:4667)
(cid:3404)ğ‘(cid:3435)ğœ‚ ,ğ‘  (cid:3439)â‰¡ğ‘ , (13)
(cid:3043)(cid:4666)(cid:3042)(cid:3289)(cid:4667) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3042)(cid:3046)(cid:3047)
where ğœ‚ (cid:3404)
(cid:3041)(cid:3046)(cid:3291)(cid:3042)(cid:3364)(cid:2878)(cid:3046)(cid:3261) (cid:3086)
and ğœ‚ (cid:3404)
(cid:3041)(cid:3046)(cid:3291)(cid:3042)(cid:3364)(cid:2878)(cid:3046)(cid:3261) (cid:3086)
. The evidence ğ‘(cid:4666)ğ‘œ(cid:4667) is a marginal likelihood:
(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)
(cid:3041)
ğ‘(cid:4666)ğ‘œ(cid:3041)(cid:4667)(cid:3404)(cid:1516) (cid:2998) ğ‘(cid:4666)ğ‘œ(cid:3041)|ğ‘ (cid:4667)ğ‘(cid:4666)ğ‘ (cid:4667)ğ‘‘ğ‘  (cid:3404)(cid:3495) (cid:3046)(cid:3261) (cid:3436) (cid:2869) (cid:3440) exp(cid:3428)(cid:3398) (cid:3041) ğ›¿(cid:2870)(cid:3398) (cid:3041)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) ğ‘‰(cid:3432) â‰¡ğ‘’(cid:4666)ğ›¿(cid:4667)
(cid:2879)(cid:2998) (cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261) (cid:3493)(cid:2870)(cid:3095)(cid:3046)(cid:3261) (cid:2870)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:2870)(cid:3046)(cid:3261)(cid:3435)(cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439)
(14)
where ğ›¿ (cid:3404) ğœ‚(cid:3398)ğ‘œÌ… is a prediction error. The evidence is an inverse exponential function of the square of
the prediction error. Hence, we describe the Gaussian evidence as ğ‘’(cid:4666)ğ›¿(cid:4667). The evidence exponentially
decreases as the prediction error increases. Surprise, the lower bound of free energy, is a negative log
function of evidence, i.e., (cid:3398)logğ‘(cid:4666)ğ‘œ(cid:3041)(cid:4667)(cid:3404)(cid:3398)logğ‘’(cid:4666)ğ›¿(cid:4667). Thus, the surprise is a quadratic function of a
prediction error.
The free energy of n observations randomly obtained from a stimulus source following a
Gaussian distribution of variance ğ‘  is formed as a quadratic function of the prediction error with
(cid:3013)
coefficients of variance (for the derivation, see (Yanagisawa, 2021)):
ğ¹ (cid:3404)ğ´ ğ›¿(cid:2870)(cid:3397)ğµ , (15)
(cid:3007) (cid:3007)
(cid:2869) (cid:3041) (cid:2869)
where the coefficients are functions of uncertainties, i.e., ğ´ (cid:3404) and ğµ (cid:3404) (cid:3419)ln(cid:3435)ğ‘›ğ‘  (cid:3397)ğ‘ (cid:3439) (cid:3397)
(cid:3007) (cid:2870)(cid:3041)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287) (cid:3007) (cid:2870) (cid:3043) (cid:3039)
(cid:4666)ğ‘›(cid:3398)1(cid:4667)lnğ‘  (cid:3397)ğ‘›ln2ğœ‹(cid:3397)ğ‘›ğ‘‰/ğ‘ (cid:3423). To simplify further analysis, we consider the case of a single data
(cid:3039) (cid:3039)
(cid:2869) (cid:2869)
observation, n=1. When n=1, the coefficients A and B are simplified as ğ´ (cid:3404) and ğµ (cid:3404)
F F (cid:3007) (cid:2870)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287) (cid:3007)
(cid:2869)
(cid:3419)ln(cid:3435)ğ‘  (cid:3397)ğ‘ (cid:3439) (cid:3397)ln2ğœ‹(cid:3423).
(cid:3043) (cid:3039)
(cid:2870)
The gradient ğ´ is the inverse of the sum of two variances. Thus, both uncertainties increase
(cid:3007)
the sensitivity of the free energy to prediction error.
Using the same Gaussian model with a single data observation, we derive the information
gains, KLD and BS, as quadratic functions of the prediction error with the coefficients of variance:
ğ¾ğ¿ğ· (cid:3404)ğ· (cid:4666)ğ‘(cid:4666)ğ‘ (cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:4667)(cid:4667)(cid:3404)ğ´ ğ›¿(cid:2870)(cid:3397)ğµ , (16)
(cid:3012)(cid:3013) (cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005)
where the coefficients are ğ´ (cid:3404)
(cid:3046)(cid:3291)
and ğµ (cid:3404)(cid:3398)ln
(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287)(cid:3397) (cid:3046)(cid:3291)
; and
(cid:3012)(cid:3013)(cid:3005) (cid:2870)(cid:3046)(cid:3261)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:3012)(cid:3013)(cid:3005) (cid:3046)(cid:3261) (cid:3046)(cid:3261)
ğµğ‘†(cid:3404)ğ· (cid:4666)ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:4667)||ğ‘(cid:4666)ğ‘ (cid:4667)(cid:4667)(cid:3404)ğ´ ğ›¿(cid:2870)(cid:3397)ğµ , (17)
(cid:3012)(cid:3013) (cid:3003)(cid:3020) (cid:3003)(cid:3020)
where the coefficients are ğ´ (cid:3404)
(cid:3046)(cid:3291)
and ğµ (cid:3404)ln
(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287)(cid:3398) (cid:3046)(cid:3291)
.
(cid:3003)(cid:3020) (cid:2870)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:3118) (cid:3003)(cid:3020) (cid:3046)(cid:3261) (cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3287)
The prediction error always increases both KLD and BS. We found that the observation variation ğ‘ 
(cid:3013)
always increases the gradient of both KLD and BS because the partial derivatives of the gradients are
always negative, i.e.,
(cid:3105)(cid:3002)(cid:3260)(cid:3261)(cid:3253)
(cid:3407)0 and
(cid:3105)(cid:3002)(cid:3251)(cid:3268)
(cid:3407)0. This signifies that the lower the observation uncertainty
(cid:3105)(cid:3046)(cid:3261) (cid:3105)(cid:3046)(cid:3261)
8
Yanagisawa, H. & Honda, S.
(i.e., the more precise the observation), the more susceptible the information gains (KLD and BS) are to
prediction errors.
However, the effects of prediction uncertainty on the sensitivity of prediction errors are
inversed between KLD and BS. We found that the prediction uncertainty increases sensitivity for KLD
but decreases it for BS because the partial derivatives are
(cid:3105)(cid:3002)(cid:3260)(cid:3261)(cid:3253)
(cid:3408)0 and
(cid:3105)(cid:3002)(cid:3251)(cid:3268)
(cid:3407)0. Thus, the lower the
(cid:3105)(cid:3046)(cid:3291) (cid:3105)(cid:3046)(cid:3291)
prediction uncertainty, the more susceptible the information gains in recognition, i.e., KLD, and the more
susceptible the information gains expected from Bayesian updating prior beliefs, i.e., BS.
To compare KLD and BS in the gradient of functions of prediction error, we derived the
difference and found that KLD is always greater than BS because the coefficients are always positive.
ğ¾ğ¿ğ·(cid:3398)ğµğ‘†(cid:3404)ğ´ ğ›¿(cid:2870)(cid:3397)ğµ (cid:3408)0, (18)
(cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020)
where ğ´ (cid:3404)
(cid:3046)(cid:3291) (cid:3118)
(cid:3408)0 and ğµ (cid:3404)
(cid:3046)(cid:3291) (cid:3118)
(cid:3408)0. Therefore, for any prediction error, the
(cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020) (cid:2870)(cid:3046)(cid:3261)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439) (cid:3012)(cid:3013)(cid:3005)(cid:2879)(cid:3003)(cid:3020) (cid:3046)(cid:3261)(cid:3435)(cid:3046)(cid:3291)(cid:2878)(cid:3046)(cid:3261)(cid:3439)
information gain in recognition is greater than that expected from updating the prior.
3.2 Convexity of information gain function by considering uniform noise
The Gaussian model suggests that prediction error always increases information gain. This is because the
likelihood function is distributed over an infinite band and deviates from the prior distribution as the
prediction error increases. However, the Laplace approximation is valid only around the mode, and there
is no guarantee that the tail of the likelihood is infinitely distributed following a Gaussian distribution.
The rate-coding hypothesis suggests that the likelihood function is coded by the distribution of the firing
rates of neurons. A single stimulation fires specific neural populations but not all neurons. Neurons that
do not receive external stimulation fire spontaneously (Raichle, 2006). The frequency of the spontaneous
firing activity of neurons is lower than the millisecond-order frequency of stimuli-driven neural activity
(Destexhe, Rudolph, & ParÃ©, 2003). We infer that such spontaneous firing activity is independent of the
neural activity evoked by sensory observations and provides no information about the cause of sensory
stimuli (observation). To represent the influence of such independent and spontaneous neural activity, we
added an independent uniformed likelihood with very small constant probability ğœ€ to the observation-
based likelihood (Jones, 2016).
ğ‘ (cid:4666)ğ‘œ|ğ‘ (cid:4667)(cid:3404)ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)(cid:3397)ğœ€ (19)
(cid:3084)
9
Yanagisawa, H. & Honda, S.
Prior Posterior likelihood
ğ‘(cid:4666)ğ‘ (cid:4667) ğ‘(cid:4666)ğ‘ (cid:4667) (cid:3406) ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:4667) ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)
ğ‘ 
(cid:3039)
ğ‘ 
ğœ€ (cid:3043)
ğ‘ 
Prediction error ğ›¿
Fig. 3 Gaussian Bayesian model with uniform likelihood. ğ‘  : prior variance, ğ‘ : Gaussian likelihood
(cid:3043) (cid:3039)
variance, ğ›¿: prediction error, and ğœ€: probability of uniform likelihood.
This uniform likelihood addition flattens the tail of the Gaussian likelihood function, as shown in Fig. 3.
The effect of the Gaussian tail becomes negligible as the prediction error increases. Therefore, we infer
that adding a uniform likelihood is the simplest modeling method to represent the likelihood of
spontaneous neural activity and to ignore the effect of the Gaussian likelihood tail.
The evidence with the likelihood function is the Gaussian evidence and the constant
probability.
(cid:2998)
ğ‘ (cid:4666)ğ‘œ(cid:4667)(cid:3404)(cid:1516) ğ‘ (cid:4666)ğ‘œ|ğ‘ (cid:4667)ğ‘(cid:4666)ğ‘ (cid:4667)ğ‘‘ğ‘ (cid:3404)ğ‘’(cid:4666)ğ›¿(cid:4667)(cid:3397)ğœ€ (20)
(cid:3084) (cid:2879)(cid:2998) (cid:3084)
Note that surprise increases monotonically with respect to the prediction error. We find that the posterior
distributions with the likelihood function form a weighted linear model of the Gaussian posterior and
prior.
ğ‘ (cid:4666)ğ‘ |ğ‘œ(cid:4667)(cid:3404)
(cid:3043)(cid:4666)(cid:3046)(cid:4667)(cid:3043)(cid:3332)(cid:4666)ğ‘œ|ğ‘ (cid:4667)
(cid:3404)
(cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:3015)(cid:3291)(cid:3290)(cid:3294)(cid:3295)(cid:2878)(cid:3084)(cid:3015)(cid:3291)(cid:3293)(cid:3284)
(cid:3404)ğ‘¤ ğ‘ (cid:3397)ğ‘¤ ğ‘ , (21)
(cid:3084) (cid:3043)(cid:3332)(cid:4666)(cid:3042)(cid:4667) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3045)(cid:3036) (cid:3043)(cid:3045)(cid:3036)
(cid:3032)(cid:4666)(cid:3083)(cid:4667) (cid:3084)
where ğ‘¤ (cid:3404) and ğ‘¤ (cid:3404) are the standardized linear weights. When the prediction error is
(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084) (cid:3043)(cid:3045)(cid:3036) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084)
small, the term ğœ€ğ‘ is negligible because ğœ€ is very small compared to ğ‘’(cid:4666)ğ›¿(cid:4667). In this case, the posterior
(cid:3043)(cid:3045)(cid:3036)
is approximated to the Gaussian posterior, ğ‘ (cid:4666)ğ‘ |ğ‘œ(cid:4667)(cid:3406)ğ‘ . Thus, the prediction error increases both
(cid:3084) (cid:3043)(cid:3042)(cid:3046)(cid:3047)
information gains, KLD and BS. By contrast, when the prediction error increases toward infinity, the
evidence converges to zero, lim ğ‘’(cid:4666)ğ›¿(cid:4667)(cid:3404)0, where ğ‘‰ (cid:3404)0, because the evidence is the inverse exponential
(cid:3083)â†’(cid:2998)
function of the prediction error. In this case, the Gaussian posterior is negligible, and thus, the posterior is
approximated to the Gaussian prior, lim ğ‘ (cid:4666)ğ‘ |ğ‘œ(cid:4667)(cid:3404)ğ‘ . When the posterior is equal to the prior, both
(cid:3084) (cid:3043)(cid:3045)(cid:3036)
(cid:3083)â†’(cid:2998)
information gains, KLD and BS, are zero. Thus, in the case of a large prediction error, where ğ‘’(cid:4666)ğ›¿(cid:4667) is
very small compared to ğœ€, and ğœ€ğ‘ is dominant in the posterior, the information gains decrease to zero
(cid:3043)(cid:3045)(cid:3036)
as prediction error increases toward infinity. We use ğœ€ (cid:3404)10(cid:2879)(cid:2871) for the following analysis.
The standardized linear weights ğ‘¤ and ğ‘¤ represent the dominances of the Gaussian
(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3045)(cid:3036)
10
Yanagisawa, H. & Honda, S.
posterior and prior, respectively, in the mixed posterior distribution. Fig. 4 shows the dominances as
functions of prediction error ğ›¿. When the prediction error is zero or small, the Gaussian posterior is
dominant. For a certain prediction error, the prior becomes dominant as the prediction error increases.
Fig. 5 shows an example of posterior distributions switching over from Gaussian posterior dominance to
prior dominance. In the switching over area of prediction errors, the Gaussian posterior and prior are
mixed with certain weights, ğ‘¤ and ğ‘¤ .
(cid:3043)(cid:3042)(cid:3046)(cid:3047) (cid:3043)(cid:3045)(cid:3036)
Using the posterior function, we derived KLD and BS:
(cid:3084)
ğ¾ğ¿ğ· (cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ (cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:4667)(cid:4671)(cid:3404)ğ¾ğ¿ğ· (cid:3397)ln(cid:4672)1(cid:3397) (cid:4673)(cid:3398)ğ¼, (22)
(cid:3012)(cid:3013) (cid:3015) (cid:3032)(cid:4666)(cid:3083)(cid:4667)
where ğ¾ğ¿ğ· is a KLD using only the Gaussian likelihood, and ğ¼ is an improper integral:
(cid:3015)
ğ¼ (cid:3404)(cid:1516)
(cid:2879)
(cid:2998)
(cid:2998)
ğ‘
(cid:3043)(cid:3045)(cid:3036)
ln(cid:3436)1(cid:3397)
(cid:3032)(cid:4666)(cid:3083)
(cid:3084)(cid:3015)
(cid:4667)(cid:3015)
(cid:3291)
(cid:3291)
(cid:3293)
(cid:3290)
(cid:3284)
(cid:3294)(cid:3295)
(cid:3440)ğ‘‘ğ‘ (cid:3404)(cid:1516)
(cid:2879)
(cid:2998)
(cid:2998)
ğ‘
(cid:3017)(cid:3045)(cid:3036)
ln(cid:4674)1(cid:3397)ğœ€(cid:4666)2ğœ‹ğ‘ 
(cid:3039)
(cid:4667) (cid:3289) (cid:3118)exp(cid:4676) (cid:3041)(cid:4666)(cid:3046)(cid:2879)
(cid:2870)
(cid:3042)(cid:3364)
(cid:3046)
(cid:4667)
(cid:3287)
(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4677)(cid:4675)ğ‘‘ğ‘ . (23)
Using the KLD, we derived BS as
(cid:2869) (cid:3084)
ğµğ‘†(cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:4667)||ğ‘(cid:4666)ğ‘ (cid:4667)(cid:4671)(cid:3404) (cid:4674)ğ‘’(cid:4666)ğ›¿(cid:4667)(cid:4676)ğµğ‘† (cid:3398)ln(cid:4672)1(cid:3397) (cid:4673)(cid:3397)ğ½(cid:4677)(cid:3398)ğœ€ğ¾ğ¿ğ·(cid:4675), (24)
(cid:3012)(cid:3013) (cid:3032)(cid:4666)(cid:3083)(cid:4667)(cid:2878)(cid:3084) (cid:3015) (cid:3032)(cid:4666)(cid:3083)(cid:4667)
where ğµğ‘† is the ğµğ‘† of using only the Gaussian likelihood, and ğ½ is an improper integral:
(cid:3015)
ğ½ (cid:3404)(cid:1516)
(cid:2879)
(cid:2998)
(cid:2998)
ğ‘
(cid:3043)(cid:3042)(cid:3046)(cid:3047)
ln(cid:3436)1(cid:3397)
(cid:3032)(cid:4666)(cid:3083)
(cid:3084)(cid:3015)
(cid:4667)(cid:3015)
(cid:3291)
(cid:3291)
(cid:3293)
(cid:3290)
(cid:3284)
(cid:3294)(cid:3295)
(cid:3440)ğ‘‘ğ‘ (cid:3404)(cid:1516)
(cid:2879)
(cid:2998)
(cid:2998)
ğ‘
(cid:3043)(cid:3042)(cid:3046)(cid:3047)
ln(cid:4674)1(cid:3397)ğœ€(cid:4666)2ğœ‹ğ‘ 
(cid:3039)
(cid:4667) (cid:3289) (cid:3118)exp(cid:4676) (cid:3041)(cid:4666)(cid:3046)(cid:2879)
(cid:2870)
(cid:3042)(cid:3364)
(cid:3046)
(cid:4667)
(cid:3287)
(cid:3118)(cid:2878)(cid:3041)(cid:3023) (cid:4677)(cid:4675)ğ‘‘ğ‘ . (25)
Because improper integrals ğ¼ and ğ½ could not be solved analytically, we used a computational approach
for further analysis.
Fig. 4 Dominances of Gaussian posterior and prior in posterior distribution as functions of prediction
error. The dominances swatch over at a certain prediction error level. (Variances: ğ‘  (cid:3404)10.0, ğ‘ =1.0.)
(cid:3043) (cid:3039)
11
roiretsop
ni
ecnanimoD
Yanagisawa, H. & Honda, S.
0.6
0.5
0.4
0.3
0.2
0.1
0
-10 -5 0 5 10 15
Fig. 5 Posterior distributions for different prediction errors (ğ›¿=2.0, 4.0, 5.0, and 6.0). The dominances in
the posterior distributions switch over from Gaussian posterior to prior. (Variances: ğ‘  (cid:3404)10.0, ğ‘ =1.0.)
(cid:3043) (cid:3039)
Fig. 6 (a) shows the information gains and their total value, ğ¼ğº (cid:3404)ğ¾ğ¿ğ·(cid:3397)ğµğ‘†, as functions of the
prediction errors. All information gains are upward-convex functions of the prediction errors. This
convexity is general because when the prediction error is small, the Gaussian posterior is dominant in the
posterior, and information gains increase as the prediction error increases; whereas when the prediction
error is larger than a certain level, the prior becomes dominant, and the information gains decrease to zero
as the prediction error increases.
Fig. 7 shows surprise as a function of the prediction error. Surprise increases monotonically
with respect to the prediction error. Thus, the information gains are also upward-convex functions of
surprise, and the total information gain ğ¼ğº that induces positive emotions by reducing free energy is an
upward-convex function of surprise (and prediction error). We infer that the upward-convex function of
the total information gain represents the arousal potential function (i.e., the Wundt curve). Fig. 6 (b)
shows an example of information gain as a function of surprise.
12
ytilibaborP
Î´=2.0
0.6
Prior
Posterior 0.5
Likelihood
0.4
0.3
0.2
0.1
0
-10 -5 0 5 10 15
ytilibaborP
Î´=4.0
Prior
Posterior
Likelihood
0.6
0.5
0.4
0.3
0.2
0.1
0
-10 -5 0 5 10 15
ytilibaborP
Î´=5.0
Prior 0.6
Posterior
Likelihood 0.5
0.4
0.3
0.2
0.1
0
-10 -5 0 5 10 15
ytilibaborP
Î´=6.0
Prior
Posterior
Likelihood
Yanagisawa, H. & Honda, S.
Fig. 6 Example of information gain functions of (a) prediction error and (b) surprise using Gaussian
model with uniform noise. KLD and BS represent free energy reduction in recognition and prior updating
(learning), respectively. Total information gain ğ¼ğº is a summation of KLD and BS. (Uncertainties: ğ‘  (cid:3404)
(cid:3043)
10.0, ğ‘ =1.0.)
(cid:3039)
Fig. 7 Surprise as a function of prediction error. (variances: ğ‘  (cid:3404)10.0, ğ‘ =1.0.)
(cid:3043) (cid:3039)
Information gain functions are upward-convex and have a peak. We define the prediction errors that
maximize information gains ğ¾ğ¿ğ·, ğµğ‘†, and ğ¼ğº as optimal prediction errors ğ›¿ , ğ›¿ and ğ›¿ ,
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008)
respectively. Similarly, we define the surprises that maximize information gains ğ¾ğ¿ğ·, ğµğ‘†, and ğ¼ğº as
optimal surprises ğ‘† , ğ‘† , and ğ‘† , respectively. We use the term â€œoptimalâ€ because it represents the
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008)
optimal arousal level that maximizes information gain (epistemic value) that evokes emotional valence.
When the prediction errors are greater than ğ›¿ and smaller than ğ›¿ , ğ¾ğ¿ğ· and ğµğ‘† have a negative
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
relationship, where ğ¾ğ¿ğ· decreases as ğµğ‘† increases, and vice versa. The prediction error that maximizes
the total information gain ğ›¿ always falls into this area. Alternate maximizations of ğ¾ğ¿ğ· and ğµğ‘† by
(cid:3010)(cid:3008)
decreasing and increasing the prediction error and surprise in this area iteratively reach the optimal
13
niaG
noitamrofnI
niaG
noitamrofnI
7
6
5
4
3
2
1
0 2 4 6 8
Prediction Error
esirpruS
Yanagisawa, H. & Honda, S.
surprise ğ‘† . This alternation generates fluctuations of surprise. The magnitude of fluctuation is
(cid:3010)(cid:3008)
determined by the difference between KLD and BS in the optimal prediction error ğ· (cid:3404)ğ›¿ (cid:3398)ğ›¿ and
(cid:3083) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)
surprise ğ· (cid:3404)ğ‘† (cid:3398)ğ‘† . In the next section, we analyze the effects of uncertainties on the optimal
(cid:3020) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)
prediction errors and surprise, together with their differences.
3.3 Effects of uncertainties on information gains
The optimal prediction error and surprise change depending on uncertainties. We found the optimal
prediction error and optimal surprise for all combinations of likelihood variances ğ‘  [1.0, 50] and prior
(cid:3039)
variance ğ‘  [1.0, 50] in steps of 0.1 using the MATLAB fminbnd.m function, which is based on golden
(cid:3043)
section search and parabolic interpolation.
Fig. 8 shows the maximum information gain as a function of the two uncertainties, ğ‘  and ğ‘  .
(cid:3039) (cid:3043)
All maximum information gains decrease as ğ‘  increases, and increase as ğ‘  decreases. While ğ‘ 
(cid:3039) (cid:3043) (cid:3043)
approaches zero, the sensitivity of ğ‘  to the maximum information gains are low. The sensitivity of ğ‘ 
(cid:3039) (cid:3039)
increases as ğ‘  increases. The peak of the maximum information gain is observed when ğ‘  is small, and
(cid:3043) (cid:3039)
ğ‘  is large. The maximum information gains of a large ğ‘  and large ğ‘  are greater than those of a small
(cid:3043) (cid:3039) (cid:3043)
ğ‘  and small ğ‘  . Fig. 9 shows examples of the maximum information gain as a function of ğ‘  and ğ‘  . The
(cid:3039) (cid:3043) (cid:3039) (cid:3043)
maximum information gains increase exponentially as ğ‘  decreases. Thus, the sensitivity of ğ‘  to the
(cid:3039) (cid:3039)
maximum information gain increases as ğ‘  decreases. By contrast, the sensitivity of ğ‘  to information
(cid:3039) (cid:3043)
gain is significant when ğ‘  is small (e.g., from 1.0 to 10.0 in this example).
(cid:3043)
Fig. 8 Maximum information gains as function of uncertainties ğ‘  and ğ‘  . (a) Max KLD, (b) Max BS,
(cid:3039) (cid:3043)
and (c) Max ğ¼ğº.
14
Yanagisawa, H. & Honda, S.
Fig. 9 Maximum information gains as functions of (a) likelihood variance when ğ‘  (cid:3404)10 and (b) prior
(cid:3043)
variance when ğ‘  (cid:3404)1.0.
(cid:3039)
Fig. 10 shows the optimal prediction errors, ğ›¿ , ğ›¿ and ğ›¿ , as functions of likelihood variance ğ‘ 
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008) (cid:3039)
and prediction variance ğ‘  . These two variances increase the optimal prediction errors. The sensitivity of
(cid:3043)
ğ‘  is greater than that of ğ‘  in KLD. Fig. 11 shows an example of the optimal prediction error as a
(cid:3039) (cid:3043)
function of each uncertainty. All functions are monotonically increasing convex. ğ›¿ is more sensitive
(cid:3012)(cid:3013)(cid:3005)
to ğ‘  than ğ›¿ . Thus, the difference ğ›¿ and ğ›¿ decreases as ğ‘  increases. By contrast, ğ›¿ is less
(cid:3039) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3039) (cid:3012)(cid:3013)(cid:3005)
sensitive to ğ‘  than ğ›¿ . Thus, the difference increases as ğ‘  increases.
(cid:3043) (cid:3003)(cid:3020) (cid:3043)
Fig. 10 Optimal prediction errors as functions of observation and prediction uncertainties for (a) ğ¾ğ¿ğ·, (b)
ğµğ‘†, and (c) ğ¼ğº.
15
niaG
noitamrofnI
mumixaM
niaG
noitamrofnI
mumixaM
Yanagisawa, H. & Honda, S.
Fig. 11 Optimal prediction errors as functions of uncertainties, (a) likelihood variance when ğ‘  =10.0 and
(cid:3043)
(b) prediction variance when ğ‘ =1.0.
(cid:3039)
Fig. 12 Optimal surprises as functions of observation and prediction uncertainties for (a) KLD, (b) BS,
and (c) ğ¼ğº.
Fig. 12 shows the optimal surprises ğ‘† , ğ‘† , and ğ‘† as functions of the two uncertainties. ğ‘ 
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3010)(cid:3008) (cid:3039)
monotonically increases all optimal surprises. However, the effects of ğ‘  are different. ğ‘  decreases
(cid:3043) (cid:3043)
ğ‘† and increases ğ‘† . Fig. 13 shows examples of optimal surprises as functions of each uncertainty.
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
ğ‘† is more sensitive to ğ‘  than ğ‘† , and thus, ğ‘† approaches ğ‘† as ğ‘  increases. Consequently, the
(cid:3012)(cid:3013)(cid:3005) (cid:3039) (cid:3003)(cid:3020) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3039)
difference between ğ‘† and ğ‘† decreases as ğ‘  increases. By contrast, ğ‘  decreases ğ‘† and
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3039) (cid:3043) (cid:3012)(cid:3013)(cid:3005)
increases ğ‘† . Thus, the difference between ğ‘† and ğ‘† increases as ğ‘  increases.
(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3043)
16
rorrE
noitciderP
lamitpO
rorrE
noitciderP
lamitpO
Yanagisawa, H. & Honda, S.
Fig. 13 Optimal surprises as functions of (a) likelihood variance when ğ‘  (cid:3404)10.0 and (b) prediction
(cid:3043)
variance when ğ‘  (cid:3404)1.0.
(cid:3039)
Fig. 14 shows the differences in the optimal prediction error and optimal surprise. Both differences are
always positive, and thus, ğ›¿ (cid:3408)ğ›¿ and ğ‘† (cid:3408)ğ‘† . Both differences increase as ğ‘  decreases and ğ‘ 
(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3039) (cid:3043)
increases. Thus, the larger the ğ‘  , and the smaller the ğ‘ , the larger the differences in both the optimal
(cid:3043) (cid:3039)
prediction errors and surprises. ğ‘  has the greatest sensitivity to increase the difference when ğ‘  is large.
(cid:3039) (cid:3043)
For the optimal prediction errors, ğ‘  has the greatest sensitivity to increase the difference when
(cid:3043)
ğ‘  is small. The difference in the optimal prediction error is larger when both ğ‘  and ğ‘  are large than
(cid:3039) (cid:3039) (cid:3043)
when both ğ‘  and ğ‘  are small. By contrast, the difference in the optimal surprise is larger when both ğ‘ 
(cid:3039) (cid:3043) (cid:3039)
and ğ‘  are small than when both ğ‘  and ğ‘  are large.
(cid:3043) (cid:3039) (cid:3043)
Fig. 14 Difference in optimal prediction error ğ· and difference in optimal surprise ğ· .
(cid:3083) (cid:3020)
4. Discussions
4.1 Arousal potential functions and curiosities
The results of the analysis using a Gaussian generative model with an additional uniform likelihood
suggest that the two information gains, KLD and BS, form upward-convex functions of surprise and
17
esirpruS
lamitpO
esirpruS
lamitpO
Yanagisawa, H. & Honda, S.
prediction errors (i.e., the distance between the prior mean and likelihood peak). The prediction error
monotonically increases surprise, as shown in Fig. 7. Fig. 15 shows a schematic of the information gain
functions that conceptualize the analytical results, as shown in Fig. 6 and the related emotions. Surprise,
(cid:3398)lnğ‘(cid:4666)ğ‘œ(cid:4667), corresponds to free energy minimized in recognition. A previous study argued that surprise
represents arousal potential because minimized free energy consists of the summation of information
content provided by novelty and perceived complexity, which are collative variables as dominant factors
of arousal potential (Yanagisawa, 2021).
Berlyne suggested that an appropriate level of arousal potential induces a positive hedonic
response, termed the optimal arousal level (Berlyne, 1960). Extreme arousal level caused by novel and
complex stimuli may cause confusion. By contrast, a low arousal level with familiar and simple stimuli
results in boredom. Thus, emotional valence shapes the upward-convex function of the arousal potential,
termed the Wundt curve.
Berlyne also suggested that two epistemic curiosities, diversive and specific, drive the
approach to the optimal arousal level (Berlyne, 1966). Diversive curiosity drives the pursuit of novelty,
whereas specific curiosity drives the search for evidence of oneâ€™s model predictions. Consequently,
diversive curiosity increases the arousal potential to climb the Wundt curve on the left, from a low level
of arousal (boredom). By contrast, specific curiosity motivates a decrease in the arousal potential to climb
the Wundt curve on the right side from a high arousal level (confusion). The alternation between the two
curiosity-driven activities approaches the optimal arousal level.
ğ¾ğ¿ğ· is a free energy reduction in recognition of a state ğ‘  given an observation ğ‘œ that
increases model evidence, ğ‘(cid:4666)ğ‘œ(cid:4667)(cid:3404)âŒ©ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª , where recognition ğ‘(cid:4666)ğ‘ (cid:4667) is updated from a prior ğ‘(cid:4666)ğ‘ (cid:4667) to
(cid:3044)(cid:4666)(cid:3046)(cid:4667)
true posterior ğ‘(cid:4666)ğ‘ |ğ‘œ(cid:4667). ğµğ‘† is the expected information gain given by novel stimuli that corresponds to
human surprise response to novelty (Itti & Baldi, 2009; Sekoguchi, Sakai, & Yanagisawa, 2019; Ueda et
al., 2021; Yanagisawa et al., 2019). Therefore, we consider that specific curiosity drives an increase in
KLD, whereas diversive curiosity drives an increase in BS.
18
Yanagisawa, H. & Honda, S.
IG
KLD
BS
Boredom Pleasure Interest Confusion
Arousalpotential: Surprise F
Fig. 15 Schematic of arousal potential functions and related emotions. Valence of epistemic emotions
represented by information gains forms upward-function of arousal potential represented by free energy
or surprise. Diversive and specific curiosity drive to maximize KLD and BS, respectively. These alternate
maximizations achieve optimal arousal level with fluctuation of surprise. Emotions such as boredom,
pleasure, interest, and confusion are induced by free energy and its fluctuations (see main text for detailed
discussion)
4.2 Inquiry process and epistemic emotions
The analytical result shown in Fig. 14 demonstrate that the optimal surprise and optimal prediction error
of BS is always greater than that of KLD, i.e., ğ‘† (cid:3408)ğ‘† and ğ›¿ (cid:3408)ğ›¿ , respectively. This result
(cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005)
suggests that maximizing information gain through novelty seeking (driven by diversive curiosity)
requires a greater prediction error, causing greater surprise than that from maximizing information gain
through evidence seeking (driven by specific curiosity).
When surprise is less than ğ‘† , both KLD and BS monotonically increase as surprise
(cid:3012)(cid:3013)(cid:3005)
increases. By contrast, when surprise is greater than ğ‘† , both KLD and BS monotonically decrease as
(cid:3003)(cid:3020)
surprise increases. Thus, the two curiosities increase and decrease prediction errors in the former and
latter areas of surprise, respectively. However, when surprise is greater than ğ‘† and less than ğ‘† , KLD
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
decreases, and BS increases as surprise increases. Thus, in this area of surprise, maximizing both the
KLD and BS at same time is impossible. We infer that the two types of curiosity alternately maximize
KLD and BS. This alternating maximization of information gains generates fluctuations of surprise. The
optimal arousal level, as a maximum summation of KLD and BS, falls into this area. Therefore, the
optimum arousal level, ğ‘† , involves fluctuations in surprise by alternately seeking novelty and evidence,
(cid:3010)(cid:3008)
driven by the two types of curiosity.
19
FÎ”
niag
noitamrofnI
:ecnelav
cimetsipE
Optimal
arousal
Yanagisawa, H. & Honda, S.
We consider that alternating the two kinds of curiosity by increasing and decreasing prediction
errors represents an ideal inquiry process that achieves optimal arousal. This process provides continuous
positive emotions through the continuous acquisition of maximum information gain (i.e., epistemic
value). For example, â€œinterestâ€ is defined as disfluency reduction in fluencyâ€“disfluency theory (Graf &
Landwehr, 2015). We previously formalized disfluency reduction as free energy reduction in recognition
(i.e., KLD) from increased free energy (Yanagisawa et al., 2023). This corresponds to an increase in KLD
from the high-surprise state shown in Fig. 15. Thus, â€œinterestâ€ is achieved by specific curiosity (i.e.,
climbing a hill of KLD from the right side in Fig. 15). By contrast, increasing KLD from the low-surprise
state (i.e., climbing a hill of KLD from the left side in Fig. 15) may explain â€œpleasureâ€ defined as increase
in fluency (Graf & Landwehr, 2015). We have previously formalized fluency as KLD in recognition
(Yanagisawa et al., 2023).
BS denotes the expected information gain, as discussed in the Methods section. Active
inference suggests that an agent infers an optimal policy of action that minimizes expected free energy.
The expected free energy includes the negative expected information gain as an epistemic value. This
epistemic value drives curious behavior (Friston et al., 2017). Thus, diversive curiosity, formalized as
maximizing the BS, corresponds to curiosity in active inference. We discuss the mathematical
interpretations of KLD and BS in terms of the expected free energy in a later section.
4.3 Effect of uncertainties on optimal arousal level and epistemic values
We analyzed the effects of prediction and observation uncertainties, manipulated using prior and
likelihood variances, on optimal information gains. Table 1 summarizes the effects of the two
uncertainties in four quadrants for combinations of small and large uncertainties. A small prediction
uncertainty ğ‘  indicates that the prior belief is certain because of, for example, prior experience and
(cid:3043)
knowledge. However, prior beliefs are not always correct. The prediction error represents the error of
prior belief from reality. Thus, a case with small ğ‘  and large prediction error indicates a preconceived
(cid:3043)
notion. By contrast, a large ğ‘  denotes that the prior belief is uncertain, owing to, for example, a lack of
(cid:3043)
prior knowledge and experience. Thus, observation uncertainty ğ‘  indicates precision of observations.
(cid:3039)
We evaluate the condition of uncertainties using four indices: maximum information gain
(maxğ¼ğº), optimal prediction errors (ğ›¿ , ğ›¿ ), optimal surprises (ğ‘† , ğ‘† ), difference in optimal
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
prediction errors (ğ· ), and difference in optimal surprises (ğ· ). As shown in Fig. 8, the condition
(cid:3083) (cid:3046)
combining a small ğ‘  and large ğ‘  provides the largest maxğ¼ğº with the largest ğ· between small ğ›¿
(cid:3039) (cid:3043) (cid:3083) (cid:3012)(cid:3013)(cid:3005)
and moderate ğ›¿ . A larger ğ· signifies a wider exploration range through alternations of diversive and
(cid:3003)(cid:3020) (cid:3083)
specific curiosities. Smaller ğ‘† and ğ‘† indicate less surprise as a cognitive load in the inquiry
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
process. Therefore, the condition combining a small ğ‘  and large ğ‘  is the best solution to achieve the
(cid:3039) (cid:3043)
ideal inquiry process with the largest epistemic value (information gain; maxğ¼ğº) and the largest range of
exploration (ğ· ) under less cognitive load (ğ‘† and ğ‘† ).
(cid:3083) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
20
Yanagisawa, H. & Honda, S.
The condition combining a small ğ‘  and small ğ‘  is expected to yield the second largest
(cid:3039) (cid:3043)
epistemic value (information gain) under less cognitive load (ğ‘† , ğ‘† ); however, the range of
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
exploration (ğ· ) is small. The condition combining a large ğ‘  and large ğ‘  is expected to result in a small
(cid:3083) (cid:3039) (cid:3043)
information gain with a moderate range of exploration at the largest prediction error level. The condition
combining a large ğ‘  and small ğ‘  is the worst case, corresponding to the smallest information gain and
(cid:3039) (cid:3043)
the smallest exploration range.
As overall trends, prediction uncertainty ğ‘  increases the range of exploration (ğ· ). This
(cid:3043) (cid:3083)
suggests that an extremely certain prior brief, such as a preconceived notion and strong assumption,
suppresses the range of exploration, whereas an open mind involving a flat prior belief widens the range
of exploration. The observation uncertainty ğ‘  decreases the expected maximum information gain (max
(cid:3039)
ğ¼ğº). This suggests that precise observation increases expected information gains (epistemic value) with
positive emotions. ğ‘  can be decreased in different ways; for example, by increasing the precision of
(cid:3039)
stimuli, paying attention to stimuli, and improving the accuracy of the observation models.
Table 1 Summary of the effects of likelihood variance (observation uncertainty) ğ‘  and prior variance
(cid:3039)
(prediction uncertainty) ğ‘  on maximum information gain, maxğ¼ğº, optimal prediction errors, ğ›¿ , ğ›¿ ,
(cid:3043) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
optimal surprises, ğ‘† , ğ‘† , difference in optimal prediction errors, ğ· , and difference in optimal
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3083)
surprises, ğ· . ğ‘‹:â‡’ğ‘Œ signifies that X dominantly affects Y. Solid and broken underlines denote positive
(cid:3046)
and negative effects on epistemic emotions, respectively.
ğ‘  :â‡’ ğ· Small ğ‘  : Large ğ‘  :
(cid:3043) (cid:3083) (cid:3043) (cid:3043)
ğ‘ :â‡’maxğ¼ğº, ğ· small ğ·
(cid:3039) (cid:3046) (cid:3083)
Small ğ‘ : Small ğ‘  and small ğ‘  : Small ğ‘  and large ğ‘  :
(cid:3039) (cid:3039) (cid:3043) (cid:3039) (cid:3043)
large maxğ¼ğº Large maxğ¼ğº, Largest maxğ¼ğº,
Small ğ›¿ smallest ğ›¿ , ğ›¿ , small ğ›¿ , moderate ğ›¿ ,
(cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
Small ğ‘† , ğ‘† smallest ğ‘† ,ğ‘† , small ğ‘† , ğ‘† ,
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
large ğ· . small ğ· , large ğ· . largest ğ· , ğ· .
(cid:3046) (cid:3083) (cid:3046) (cid:3083) (cid:3046)
Large ğ‘ : Large ğ‘  and small ğ‘  : Large ğ‘  and large ğ‘  :
(cid:3039) (cid:3039) (cid:3043) (cid:3039) (cid:3043)
small maxğ¼ğº smallest maxğ¼ğº, small maxğ¼ğº,
moderate ğ›¿ ,ğ›¿ , largest ğ›¿ ,ğ›¿ ,
(cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
large ğ‘† largest ğ‘† , moderate ğ‘† , large ğ‘† , largest ğ‘† ,
(cid:3012)(cid:3013)(cid:3005) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020) (cid:3012)(cid:3013)(cid:3005) (cid:3003)(cid:3020)
smallest ğ· , ğ· . moderate ğ· , ğ· .
(cid:3083) (cid:3046) (cid:3083) (cid:3046)
4.4 Expected free energy and information gains
An active inference framework suggests that an agentâ€™s action policy is selected to minimize expected
free energy (Friston et al., 2017; Parr et al., 2022; Smith, Friston, & Whyte, 2022). Here, we discuss the
relationship between the expected free energy and the two types of information gains, KLD and BS, as
21
Yanagisawa, H. & Honda, S.
drivers of specific and diversive curiosity, respectively.
Before giving observations by action, an agent calculates expected free energy under a policy ğœ‹.
ğº (cid:3095) (cid:3404)âŒ©lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667) , (26)
where ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)(cid:3404)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667). This definition implies that the expected free energy is the free energy
averaged by likelihood ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667) of observations expected by future action under a policy ğœ‹. The expected
free energy forms the prior distribution of the policies. A policy is randomly selected based on the policy
prior ğ‘(cid:4666)ğœ‹(cid:4667)(cid:3404)ğœ(cid:4666)(cid:3398)ğ›¾ğº (cid:4667) such that the expected free energy is minimized, where ğœ(cid:4666)ğ¸(cid:4667) is a softmax
(cid:3095)
function that transforms from energy ğ¸ to probability, and ğ›¾ is the precision of policy representing the
confidence of policy selection.
The expected free energy is expanded in two terms using the decomposition of a generative
model ğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)(cid:3404)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667).
ğº (cid:3095) (cid:3404)âŒ©lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667) (cid:3398)âŒ©lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)
(cid:3404)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:4671)(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)âŒ©lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667) (cid:4671)
(cid:3404)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ¶(cid:4667)(cid:4671)(cid:3397)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ‘ˆ (cid:3095) (cid:4671) (27)
The first term is a KL divergence from a state prior to a recognition density under a policy, ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667). We
assume that the state prior is given from the agentâ€™s preference ğ¶, ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3404)ğ‘(cid:4666)ğ‘ |ğ¶(cid:4667). A preference refers
to a desired state expected to be achieved through actions based on policy selection. The KL divergence,
termed risk in state, refers to the difference between the preferred state and the state expected by acting
with a policy. A lower KL divergence indicates that the desired state is more likely to be achieved. The
second term is expected uncertainty, which represents uncertainty averaged over expected observations.
This term is called ambiguity because it is equivalent to the entropy of likelihood,
(cid:3398)âŒ©lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667) (cid:3404)âŒ©(cid:3398)lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3043)(cid:4666)ğ‘œ|ğ‘ (cid:4667)(cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) (cid:3404)âŒ©ğ»(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) .
The expected uncertainty is decomposed into two terms using a conditional probability definition:
(cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667)(cid:3044)(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)
ğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)(cid:3404) .
(cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667)
ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ‘ˆ (cid:3095) (cid:4671)(cid:3404)(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:3427)âŒ©lnğ‘(cid:4666)ğ‘œ|ğ‘ (cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667) (cid:3431)
(cid:3404)(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:3427)âŒ©lnğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:3398)lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3397)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667) (cid:3431)
(cid:3404)(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:4671)(cid:4671)(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667)(cid:4671) (28)
The first term of the expected uncertainty is a negative KL divergence from approximate posterior to prior
averaged by expected observations with a policy ğœ‹. This KL divergence corresponds to the Bayesian
surprise, ğµğ‘† . Thus, this term signifies the expected information gain by prior updating using predicted
(cid:3095)
observations under policy ğœ‹. Note that the observation is not yet given, and ğµğ‘† is averaged based on
(cid:3095)
the predicted distribution under a policy, ğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667).
The second term is entropy under a policy. By definition, surprise is the sum of the negative
KLD and free energy.
(cid:3398)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667)(cid:3404)(cid:3398)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:3397)âŒ©lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)lnğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) (29)
22
Yanagisawa, H. & Honda, S.
Thus, the entropy is a summation of the negative predictive KLD and the predicted free energy.
(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)lnğ‘(cid:4666)ğ‘œ|ğœ‹(cid:4667)(cid:4671)(cid:3404)(cid:3398)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ· (cid:3012)(cid:3013) (cid:3427)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3627)|ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:3431)(cid:3397) ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ¹ (cid:3095) (cid:4671) (30)
In summary, the expected free energy under a policy is the sum of the risk, predicted free energy, and
negative predicted information gains.
ğº (cid:3404)ğ‘…ğ‘–ğ‘ ğ‘˜(cid:3397)ğ‘ğ¹ (cid:3398)(cid:4666)ğ‘ğ¾ğ¿ğ· (cid:3397)ğ‘ğµğ‘† (cid:4667) (31)
(cid:3095) (cid:3095) (cid:3095) (cid:3095)
where
risk in state: ğ‘…ğ‘–ğ‘ ğ‘˜ (cid:3404)ğ· (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ¶(cid:4667)(cid:4671), (32)
(cid:3012)(cid:3013)
predicted free energy: ğ‘ğ¹ (cid:3095) (cid:3404)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:3427)âŒ©lnğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3398)lnğ‘(cid:4666)ğ‘ ,ğ‘œ|ğœ‹(cid:4667)âŒª (cid:3044)(cid:4666)ğ‘ |ğœ‹(cid:4667) (cid:3431), (33)
predicted KLD: ğ‘ğµğ‘† (cid:3095) (cid:3404)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:4671)(cid:4671), (34)
and predicted Bayesian surprise: ğ‘ğ¾ğ¿ğ· (cid:3095) (cid:3404)ğ”¼ (cid:3044)(cid:4666)ğ‘œ|ğœ‹(cid:4667) (cid:4670)ğ· (cid:3012)(cid:3013) (cid:4670)ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)||ğ‘(cid:4666)ğ‘ |ğ‘œ,ğœ‹(cid:4667)(cid:4671)(cid:4671). (35)
When preference is expected to be fully satisfied by a policy, the predicted state equals the state
prior, ğ‘(cid:4666)ğ‘ |ğœ‹(cid:4667)(cid:3406)ğ‘(cid:4666)ğ‘ |ğ¶(cid:4667). In this case, the risk term becomes zero. However, the expected free energy still
remains. The remaining expected free energy is the predicted free energy minus the two predicted
information gains.
ğº (cid:3406)ğ‘ğ¹ (cid:3398)(cid:4666)ğ‘ğ¾ğ¿ğ· (cid:3397)ğ‘ğµğ‘† (cid:4667) (36)
(cid:3095) (cid:3095) (cid:3095) (cid:3095)
The remaining expected free energy is minimized by maximizing the predicted information gains,
ğ‘ğ¾ğ¿ğ· (cid:3397)ğ‘ğµğ‘† , and minimizing the predicted free energy. Therefore, the two types of expected
(cid:3095) (cid:3095)
information gains, ğ‘ğ¾ğ¿ğ· ,ğ‘ğµğ‘† , drive the agentâ€™s action based on the active inference framework. This
(cid:3095) (cid:3095)
corresponds to the expected drives of the two types of curiosity.
4.5 Limitations and further discussions
The analytical results are based on a Gaussian generative model. A Gaussian model was used to
independently manipulate the prediction errors and uncertainties and analyze their effects on information
gains. Although Laplace approximation and the principle of maximum entropy reasonably support the
Gaussian assumption, true distributions can be more complex than Gaussian distributions. For specific
applications with complex distributions, further analysis based on the method proposed in this study
paper for specific applications with complex distributions.
This study focusses on emotions induced by epistemic values (epistemic emotions) such as curiosity and
interest. However, emotions are affected by individual preference and appraisal of the situation against
objectives(Ellsworth & Scherer, 2003). We may expand the emotion model to include such preference-
based emotions by introducing the pragmatic value formalized as risk term in expected free energy(Parr et
al., 2022). The model does not consider individual capacity to process information. Surprise (free energy)
exceeding the capacity may affect negative emotions.
This study was limited to analyzing two types of information gain linked to epistemic emotions as
functions of surprise in a context-independent manner. Epistemic emotions based on epistemic values,
such as curiosity, can be observed based on the agentâ€™s behavior. Active inference, where an action policy
23
Yanagisawa, H. & Honda, S.
is inferred to minimize the expected free energy, can be used to simulate agent behavior based on
epistemic emotions in a specific context (Friston et al., 2017). As discussed, the expected free energy
includes two types of information gain. In future studies, it will be necessary to accumulate evidence of
the model predictions based on correspondence between agent simulations and actual human behavior in
a variety of specific contexts.
5. Conclusion
This study mathematically formulated arousal potential functions of epistemic emotions, such as curiosity
and interest, that drive inquiry processes, based on information gains. Decrements in free energy in
Bayesian recognition and prior belief update correspond to two types of information gain, i.e., KLD and
BS, respectively. Free energy reduction induces positive emotions by reducing surprise caused by
prediction errors and uncertainty, which provide information gains (i.e., epistemic value). We
demonstrated that the two types of information gain form upward-convex curve functions of surprise
using a Gaussian generative model with a uniform noise likelihood, and defined epistemic emotions as
information gains (or decrements of free energy). An analysis using the model exhaustively revealed the
effects of prediction and observation uncertainties on the peak of information gain functions as the
optimal arousal level. Specifically, the analytical results suggest that the greater the prediction uncertainty
and the lower the observation uncertainty, the greater the information gained through a larger exploration
range.
These results provide general and fundamental knowledge to increase the valence of epistemic
emotions that facilitate the inquiry process because the model is deduced from the synthesis of free
energy minimization as the first principle of the brain and the well-established arousal potential theory.
Therefore, this model framework is applicable to diverse areas that deal with epistemic emotions and
motivations, such as education, creativity, aesthetics, affective computing, and related cognitive sciences.
Further studies are needed to accumulate empirical evidence for the principle-based model and
understand the relationship between the inquiry process and emotions in diverse complex situations.
Acknowledgements
This research was supported by Japan Society for the Promotion of Science (KAKENHI Grant Number
21H03528, Mathematical model development of emotion dimensions based on variation of uncertainty and
its application to inverse problems).
24
Yanagisawa, H. & Honda, S.
References
Berlyne, D. E. (1960), Conflict, arousal, and curiosity, 350. New York: McGraw-Hill Book Company.
Berlyne, D. E. (1966). Curiosity and exploration. Science, 153(3731), 25â€“33.
doi:10.1126/science.153.3731.25.
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and
perception: A mathematical review. Journal of Mathematical Psychology, 81, 55â€“79.
doi:10.1016/j.jmp.2017.09.004.
Clark, J. E., Watson, S., & Friston, K. J. (2018). What is mood? A computational perspective.
Psychological Medicine, 48(14), 2277â€“2284. doi:10.1017/S0033291718000430.
Destexhe, A., Rudolph, M., & ParÃ©, D. (2003). The high-conductance state of neocortical neurons in vivo.
Nature Reviews. Neuroscience, 4(9), 739â€“751. doi:10.1038/nrn1198.
Ellsworth, P. C., & Scherer, K. R. (2003). Appraisal processes in emotion. Handbook of affective sciences,
1199, 572â€“595.
Friston, K. (2010). The free-energy principle: A unified brain theory? Nature Reviews. Neuroscience,
11(2), 127â€“138. doi:10.1038/nrn2787.
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active inference,
curiosity and insight. Neural Computation, 29(10), 2633â€“2683. doi:10.1162/neco_a_00999.
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology,
Paris, 100(1â€“3), 70â€“87. doi:10.1016/j.jphysparis.2006.10.001.
Graf, L. K. M., & Landwehr, J. R. (2015). A dual-process perspective on fluency-based aesthetics: The
pleasure-interest model of aesthetic liking. Personality and Social Psychology Review, 19(4),
395â€“410. doi:10.1177/1088868315574978.
Hesp, C., Smith, R., Parr, T., Allen, M., Friston, K. J., & Ramstead, M. J. D. (2021). Deeply felt affect:
The emergence of valence in deep active inference. Neural Computation, 33(2), 398â€“446.
doi:10.1162/neco_a_01341.
Honda, S., Yanagisawa, H., & Kato, T. (2022). Aesthetic shape generation system based on novelty and
complexity. Journal of Engineering Design, 33(12), 1016â€“1035.
doi:10.1080/09544828.2022.2155343.
Itti, L., & Baldi, P. (2009). Bayesian surprise attracts human attention. Vision Research, 49(10), 1295â€“
1306. doi:10.1016/j.visres.2008.09.007.
Joffily, M., & Coricelli, G. (2013). Emotional valence and the free-energy principle. PLOS Computational
Biology, 9(6), e1003094. doi:10.1371/journal.pcbi.1003094.
Jones, P. R. (2016). A tutorial on cue combination and Signal Detection Theory: Using changes in
sensitivity to evaluate how observers integrate sensory information. Journal of Mathematical
Psychology, 73, 117â€“139. doi:10.1016/j.jmp.2016.04.006.
Kashdan, T. B., & Silvia, P. J. (2009). Curiosity and interest: The benefits of thriving on novelty and
25
Yanagisawa, H. & Honda, S.
challenge. Oxford handbook of positive psychology, 2, 367â€“374.
Knill, D. C., & Pouget, A. (2004). The Bayesian brain: The role of uncertainty in neural coding and
computation. Trends in Neurosciences, 27(12), 712â€“719. doi:10.1016/j.tins.2004.10.007.
Lang, P. J. (1995). The emotion probe. Studies of motivation and attention. American Psychologist, 50(5),
372â€“385. doi:10.1037//0003-066x.50.5.372.
Parr, T., Pezzulo, G., & Friston, K. J. (2022), Active inference: The free energy principle in mind, brain,
and behavior. Cambridge, MA: MIT Press.
Peirce, C. S. (1974), Collected papers of Charles Sanders Peirce. Cambridge, MA: Harvard University
Press.
Raichle, M. E. (2006). Neuroscience. The brainâ€™s dark energy. Science, 314(5803), 1249â€“1250.
doi:10.1126/science. 1134405.
Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39(6),
1161â€“1178. doi:10.1037/h0077714.
Russell, J. A. (2003). Core affect and the psychological construction of emotion. Psychological Review,
110(1), 145â€“172. doi:10.1037/0033-295x.110.1.145.
Sasaki, H., Kato, T., & Yanagisawa, H. (2023). Quantification of â€œnoveltyâ€ based on free-energy principle
and its application for â€œaesthetic likingâ€ for industrial products. Research in Engineering Design.
doi:10.1007/s00163-023-00422-6.
Sekoguchi, T., Sakai, Y., & Yanagisawa, H. (2019). Mathematical model of emotional habituation to
novelty: Modeling with Bayesian update and information theory IEEE International Conference
on Systems, Man and Cybernetics (SMC), 2019 (pp. 1115â€“1120).
doi:10.1109/SMC.2019.8914626.
Seth, A. K., & Friston, K. J. (2016). Active interoceptive inference and the emotional brain. Philosophical
Transactions of the Royal Society of London. Series B, Biological Sciences, 371(1708),
20160007. doi:10.1098/rstb.2016.0007.
Silvia, P. J. (2012). Curiosity and motivation. The Oxford handbook of human motivation, 157â€“166.
Smith, R., Friston, K. J., & Whyte, C. J. (2022). A step-by-step tutorial on active inference and its
application to empirical data. Journal of Mathematical Psychology, 107.
doi:10.1016/j.jmp.2021.102632.
Ueda, K., Sekoguchi, T., & Yanagisawa, H. (2021). How predictability affects habituation to novelty.
PLOS ONE, 16(6), e0237278. doi:10.1371/journal.pone.0237278.
Vogl, E., Pekrun, R., Murayama, K., & Loderer, K. (2020). Surprisedâ€“curiousâ€“confused: Epistemic
emotions and knowledge exploration. Emotion, 20(4), 625â€“641. doi:10.1037/emo0000578.
Wager, T. D., Kang, J., Johnson, T. D., Nichols, T. E., Satpute, A. B., & Barrett, L. F. (2015). A Bayesian
model of category-specific emotional brain responses. PLOS Computational Biology, 11(4),
e1004066. doi:10.1371/journal.pcbi.1004066.
26
Yanagisawa, H. & Honda, S.
Wilson-Mendenhall, C. D., Barrett, L. F., & Barsalou, L. W. (2013). Neural evidence that human
emotions share core affective properties. Psychological Science, 24(6), 947â€“956.
doi:10.1177/0956797612464242.
Yanagisawa, H. (2016). A computational model of perceptual expectation effect based on neural coding
principles. Journal of Sensory Studies, 31(5), 430â€“439. doi:10.1111/joss.12233.
Yanagisawa, H. (2021). Free-energy model of emotion potential: Modeling arousal potential as
information content induced by complexity and novelty. Frontiers in Computational
Neuroscience, 15, 698252. doi:10.3389/fncom.2021.698252.
Yanagisawa, H., Kawamata, O., & Ueda, K. (2019). Modeling emotions associated with novelty at
variable uncertainty levels: A Bayesian approach. Frontiers in Computational Neuroscience,
13(2), 2. doi:10.3389/fncom.2019.00002.
Yanagisawa, H., Wu, X., Ueda, K., & Kato, T. (2023). Free energy model of emotional valence in dual-
process perceptions. Neural Networks: The Official Journal of the International Neural Network
Society, 157, 422â€“436. doi:10.1016/j.neunet.2022.10.027.
27

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Modeling arousal potential of epistemic emotions using Bayesian information gain: Inquiry cycle driven by free energy fluctuations"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
