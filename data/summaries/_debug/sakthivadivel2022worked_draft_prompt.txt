=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: A Worked Example of the Bayesian Mechanics of Classical Objects
Citation Key: sakthivadivel2022worked
Authors: Dalton A R Sakthivadivel

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: brook, systems, stony, objects, mechanics, worked, example, mathematics, tracking, physical

=== FULL PAPER TEXT ===

2202
peS
6
]hp-ssalc.scisyhp[
2v69921.6022:viXra
A WORKED EXAMPLE OF THE BAYESIAN MECHANICS OF
CLASSICAL OBJECTS
Dalton A R Sakthivadivel
darsakthi.github.io
VERSES Research Lab, Los Angeles, CA, 90016, USA
Department of Mathematics, Stony Brook University, Stony Brook, NY, USA
Department of Physics and Astronomy, Stony Brook University, Stony Brook, NY, USA
(Dated: 7th September 2022)
Bayesian mechanics is a new approach to studying the mathematics and physics of inter-
acting stochastic processes. Here, we provide a worked example of a physical mechanics for
classicalobjects,whichderivesfromasimpleapplicationthereof. Wesummarisethecurrent
state of the art of Bayesian mechanics in doing so. We also give a sketch of its connections
to classical chaos, owing to a particular =2 supersymmetry.
N
CONTENTS
I. Introduction 2
II. Mechanics 4
A. Classical physics in one dimension 4
B. The basics of Bayesian mechanics 4
C. A physics of beliefs 8
D. A physics by beliefs 10
III. A general equation for Bayesian classical mechanics 14
IV. A question of quantum ontology 18
V. The matching of modes 21
VI. The tracking of modes 22
A. Terminal mode-matching 22
B. Infinite mode-tracking 24
VII. Path-tracking, and a simple case of G-theory 26
2
A. Path-tracking 26
B. A first idea of G-theory 29
References 31
I. INTRODUCTION
Under the free energy principle [Fri19], Bayesian mechanics is a new approach to studying
the mathematics and physics of interacting stochastic processes. In essence, Bayesian mechanics
is a particular sort of mathematical physics for coupled random dynamical systems, providing
a mechanical theory for how the statistical properties of physical systems change in a space of
Bayesian beliefs, based on how their physical properties change in space and time [Fri12, PDCF20,
RSH+22]. Bayesian mechanics is particularly interested in systems with some notion of regularity,
termed ‘self-evidencing’ systems in previous literature.1 A self-evidencing system occupies an
attractor in the state space, and has some set of stereotypical behaviours definitional of the sort
of system it is. The key deliverable of Bayesian mechanics is that a random dynamical system is
an estimator for statistics or parameters of another system to which it is coupled, making it an
inference machine, and that we can understand attractors and phases in state space in virtue of
these belief dynamics.
Prosaically, the point of learning and representation in ‘genuine’ inference agents, such as bio-
logical and artificial neural networks, is to mirror the environment within the agent’s own struc-
ture—its tissue or weight configurations.2 In this sense, however, encoding data from the envir-
onment in an agential structure is simply a coupling between agent and environment. Indeed, the
foregoing statement is simply a statement that a coupled random dynamical system is the preim-
age of some function into its environment, σ, instantiating some representation of its environment
within its own structure. This is the ‘self’ of the agent referred to as a coupled random dynamical
system. Bayesian mechanics is thus a new set of techniques for understanding the relationship
between statistical quantities and the physical dynamics encoding those quantities, or reflected by
those quantities, in coupled random dynamicalsystems, with application to newareas of statistical
mechanics like stochastic thermodynamics [Fri19, PDCF20].
1 This term of art originates in neuroscience [Hoh16], where thefree energy principle has its origins, as an attempt
toexplainthephysicsandphilosophyoflearninginthehumancortex—viewedasaBayesianmechanicalproblem,
abrainlearningaboutanenvironmentisonerandomdynamicalsystemperforminginferenceaboutanother,with
an aim towards the attractor states characteristic of allostasis [Wie48]. Note also that this paper uses ‘system’
differentlyto[Fri19],whichsays‘particle’wherewesaysystemand‘system’wherewesayagent-environmentloop.
2 This remark has been paraphrased from Maxwell J D Ramstead.
3
Albeitconceptuallypowerful,theuseofthefreeenergyprinciple(FEP)andBayesian mechanics
to describe specific physical systems is rarely codified in the literature. There are examples where
it reproduces known algorithms like various types of control [MTSB20, DCFHP21, STvdM+22]),
as well as simple dissipative systems [AMTB22] and more complex systems exhibiting Lorentzian
chaos [FHU+21], but recent work has treated it as a purely formal position that systems constrain
themselves to fall within acceptable regimes of certain existential variables, thus inducing such
attractor structures in the state space.
More specifically, in [Sak22c, RSH+22], Bayesian mechanics is introduced as the mechanics of
beliefs—butit is challenging to determine the physical mechanics of systems carrying those beliefs.
This would require solving difficult PDEs for non-equilibrium steady state densities in general, or
else, equations of motion for internal states on the synchronous statistical manifold. Likewise, it is
often claimed that the FEP is as simple and general as the principleof stationary action [FCS+22],
and it can be sketched out how the Bayesian mechanics of internal states of classical objects might
look [Fri19]. Nevertheless, there has yet to be a systematic investigation of even the Bayesian
mechanics of classical physics, despite it being readily available due to recent formulations as a
least action principle.
Here, inspired by remarks in [RSH+22], we give a worked example of the classical mechanics of
objects with trivial (e.g., infinitely precise) belief dynamics—in a fairly direct sense, the simplest
case of Bayesian mechanics [Sak22c]. In doing so we provide a general formulation of classical
physics for the working Bayesian mechanic. This paper not only surveys recent literature (includ-
ing that of the author) and gives a constructive example of a Bayesian mechanical system, but
ideally, will ground futurediscussions of the free energy principle even more solidly in conventional
mathematics and physics. Of independentinterest, it also serves as a derivation of classical physics
from the principle of constrained maximum calibre, and recovers old results on supersymmetry
theory in classical mechanics.
Acknowledgements
The author thanks Lancelot Da Costa, Karl J Friston, Brennan Klein, and Maxwell J D Ram-
steadforvaluableconversations. ThispaperhasbenefittedfromcommentsbyConorHeinsandIgor
V Ovchinnikov, as well members and collaborators of the Mathematically Structured Programming
Group at the University of Strathclyde, especially Matteo Capucci.
4
II. MECHANICS
A. Classical physics in one dimension
The mechanics of classical objects are embodied by Newton’s law that systems accelerate along
force gradients, by precisely the direction and magnitude of the total force applied—no more, and
no less. The derivation of this fact is simple. Let K = 1mv2, where q is the position of some
2 t t
point mass at t and its velocity is the time derivative v = q˙ , and V be some scalar potential.
t t
Now define a path as a particular function3 q(t) (e.g., a parabolic path could be q(t) = q 1gt2,
0 − 2
whilst a straight path could be q(t) = q ), and introduce the temporary time variable τ. Taking
0
the action functional
t 1
S[q(t)] = mv2 V(q )dτ (1)
2 τ − τ
Z0
a path of least action (or more generally, for which the action is stationary) obeys the equation
∂ V = m∂ v
q t
−
by standard arguments in functional analysis. One can refer to [RSH+22, Section 2] and references
therein for an overview; alternatively, see [Cal22] for a more advanced pedagogical treatment, and
[GS00, Arn13] for mathematically sophisticated resources. This is Newton’s second law,
F = ma. (2)
Thoughappearingesoteric, this logical sequencesimplyexpressesthatapathof leastaction always
follows (2)—that is, asystem always accelerates along aforce gradient, never usingextraenergy by
resisting or compounding that force. For an appropriate specification of forces F, we get various
sorts of mechanics, like motion in gravitational fields or classical approximations of fluid flow (also
called continuum mechanics). Given mass data, and initial conditions q˙(0) and q(0) (along with
other boundary conditions), we can produce dynamical trajectories for some particular system by
actually using the law of motion given by Newtonian mechanics under the least action principle.
B. The basics of Bayesian mechanics
Bayesian mechanics can be seen as an account of the laws of motion deriving from the free
energyprinciple,concerninghowBayesian beliefs—andhence,systemswithbeliefs,suchascoupled
3 Imaginerealisationsofarandomprocessasindividualfunctionsconsistingofadriftaddedtoaparticularsequence
of noise values; indeed,this is our motivation.
5
random dynamical systems, which perform inference over the things to which they couple—behave
undercertaindeterminants ofprobabilistic motion. Muchlikeclassical mechanics serves anaccount
of systems that obey Newton’s second law by minimising the classical action, Bayesian mechanics
is an account of systems that engage in approximate Bayesian inference by minimising surprisal.
In this and the following section, we will iterate over the main constructs of Bayesian mechanics
at a progressively finer scale, unpacking the main insights from [Fri19, Sak22c, RSH+22]. The
rest of the paper will contextualise those 250 or so pages within a worked example of physics
in the classical world, furnishing new results along the way, such as the derivation of classical
physics from the principle of constrained maximum entropy, as well as the (re-)introduction of
supersymmetry as an explanation for classical chaos. The paper will proceed mostly linearly, first
discussing the abstract principle that all coupled stochastic dynamical systems satisfying certain
properties(e.g., theexistence ofaMarkov blanket)follow, thendiscussinghowthatprincipleyields
a law of motion for inferential systems, and then three examples of dynamics under that law of
motion. (See [RSH+22] for the original schematic of this distinction.)
The pure physics of the FEP arguably dates back to two landmark papers in the literature,
[Fri19] and [PDCF20]. In [Fri19, PDCF20], and later in [DCFHP21], the idea was introduced
that the FEP has gestured at a new sort of physics—one about the mechanics of Bayesian beliefs,
and how they reflect the behaviour of systems carrying those beliefs. In [Sak22c, RSH+22] it is
discussed that one can understand this in the same sense as classical mechanics arises from the
least action principle, or identically, that diffusion arises from the maximisation of entropy, with
that least action principle being laid out in [FCS+22] (to be formulated in detail in forthcoming
work). Dually, we can understand our own beliefs about a system modelling its environment—or
the system’s model of itself—as being ruled by Bayesian mechanics, under the observation and
updating rules which are a consequence thereof. A full deconstruction can be found in [RSH+22].
What is central to Bayesian mechanics? Beginning with the most recent formulation in
[FCS+22], the FEP is nothing but the least action principle applied to some surprisal4 S =
ln p( ) , where the application of this ‘least surprisal principle’ to specific objects determines
− { − }
the mechanical theory about those objects. Let a stochastic process X under p(x,t) with sample
paths γ be described by the It¯o stochastic differential equation
dX = f(X ,t)dt+√2DdW . (3)
t t t
Here, X is a random variable at t and f(X ,t) is a vector field yielding the drift at any state X ,
t t t
4 Theemptyargumentp(−)indicatesouragnosticismabouttheinputtop;itisimportanttonotethatthesurprisal
of different states constitutes a boundary condition yielding different sorts of dynamics[RSH+22, Section 3].
6
which may itself change over time. Since dW is a Wiener process, the expected state at some
t
time t is precisely f(X ,t). Immediately we arrive at an important qualification: by hypothesis,
t
the ensuing discussion applies to cases where fluctuations are distributed as zero-mean Gaussian
densities with constant variance D. This aligns with an ideal heat bath assumption.5
Let ω = v f(X ,t), where ω can also be regarded as a label for the heuristic W˙ , be a
t t t t t
−
fluctuation of any realisation of this flow at t. Note that a realisation x(t) = X = x is
s s s∈0:t
{ }
(abusing types slightly) nothing but a sample path γ, and so, we have
γ˙ E [γ] (4)
t p(γ) t
−
for ω . A quadratic6 form (ω ) can naturally be defined on the tangent space to the state space,
t t
L
such that the surprisal is its integral along a given path γ of :
L
t 1
S[γ]= ω ,ω dτ .
τ τ
4Dh i
Z0
The Lagrangian, the integrand of S[γ], is a function of noise. The surprisal of a path is then
proportional to half its accumulated squared deviation from the expected flow f(X ,t), normalised
t
bythescalingconstant√2D. Morally,thisisinthesamesenseastheclassicalactionisproportional
to half the square of the accumulated deviation of motion from a potential well [RSH+22, Section
2]. That this action equals7 the surprisal of a path γ for a given initial condition, p(x(t) x ), is a
0
|
simple consequence of the path probability measure being defined as
p(x(t) x )= exp λS[γ] (5)
0
| {− }
in [Sei12], which is indeed the canonical definition of such an object in any abstract Wiener space
[Øks03,U¨st06],andisthepointofattack in[FCS+22]. Sucha‘path-dependantsurprisal’isreferred
to as the stochastic entropy by [Sei12], and is deeply related to statistics on the path space of a
random walk (see [DB78], as well as [NS20] and related work on logarithmic heat kernels8). This
definitionoftheaction isconsistentwiththequadraticform 1 v ,v definedinclassical mechanics.
2h t t i
The action generates the Fokker-Planck equation for the probability of a state x at t,
∂ p(x,t) = ∂ [f(x,t)p(x,t)]+D∂ p(x,t),
t x xx
−
5 The same limitation appears in both [Sei12] and [FCS+22], which this work rests on. It is characteristic of most
all of statistical physics,even frameworks valid away from equilibrium.
6 Note that in the Stratonovich convention, more amenable to calculus on manifolds, there is an additional term
in the Lagrangian L indicated; see e.g. equation 15 here: [CLVW19]. Physically, by ignoring that term, we have
assumed perturbations to the flow have short characteristic timescales and are not ‘remembered’ for long, and
also, that thesurprisal ought to bequadratic.
7 Note that the surprisal we go on to describe concerns the probability of an entire trajectory given a particular
initial state—an entirepath upto t.
8 The authorthanksRobert W Neel for suggesting thispoint of discussion.
7
by giving rise to a probability density over coordinate and time pairs.
Definetworandomdynamicalsystemsηandµ. Invirtueof (4),theactionofeithersystem—and
thus, ultimately, the surprisal—is parameterised by some modal or expected path. Suppose that
η and µ are coupled by some function σ, that one has an additional random dynamical system b
capturing the interactions between the two, and that conditional expectations µˆ = E [µ ]
b,t p(µt|bt) t
and ηˆ = E [η ] exist; moreover, assume the statistics of the two processes can be distin-
b,t p(ηt|bt) t
guished, in the sense of being independent conditioned on b .9 By construction, σ(µˆ ) = ηˆ . It
t b,t b,t
is immediate that µ is an estimator for the statistics of η [Sak22c]. That is to say, in the case of
random systems whose physical dynamics are coupled, these statistical quantities are also coupled,
in a way that can be interpreted as the systems performing inference over each other. Recall that
the path which minimises (4) is the expected path. Our claim follows from the observation that
the least surprising path of µ is the one which tracks the dynamics of η across the synchronisation
function σ, and vice-versa; this means that systems that minimise their own surprisal must ‘know’
something about their environment. Systems that minimise surprisal given a control parameter
σ−1(ηˆ ) are particularly good models of an environment, whilst systems that fluctuate with high
b,t
probability are not.
Hence, inference over an environment is equivalent in this sense to occupying unsurprising
states. When we discuss models of ‘things’ or systems, we are interested in states which are
somehow ‘thing-like,’ or systemic, i.e., attractors which define that thing, which that thing should
not fluctuate too far away from [Sak22c]. More broadly, general things which perform estimation
must stay coherent in order to be an estimator (that is to say—we must have a µ to have a σ; see
[RSF22]); that this coherence is a necessary condition for synchronisation (read: estimation under
a coupling), and that surprisal-minimisation follows from synchronisation, is simply the statement
that things which exist reflect data about their environments. It is like saying that things which
exist in the universe mustobey the laws of physics as fundamentallaws—when thewind blows and
shakes a tree branch, on one reading, it is because the tree branch has inertia, but not enough to
resist theforce of thewind; on another, it is becausethetree branch reduces its surprisalaboutthe
state of the world by modelling it.10 It is surprising not to follow these laws—for a tree branch to
spontaneously resist the force of the wind entails a contradiction, which are usually surprising in
theiron-clad realmof physicallaw. Eitherthatbranchnolonger exists, inwhichcaseitcanneither
9 This framework degenerates in the case where σ is the identity, but this case is vacuous, since it assumes η is
identical to µ.
10 The author owes this example to a similar remark stated to him by David I Spivak. For non-agential objects like
the boughs of a tree, this latter view is artificially teleological—or perhaps even pure metaphor—and is referred
to as ‘as if’ inference. Though analogical in general, it has been argued that this does not constitute a failure of
themodel [Sak22c, Section 4.3 and Remark 5.1]; [RSH+22, Section 6.3].
8
move with the wind nor be still, or, it has suddenly become much heavier than a tree branch. As a
tautology, things that are surprising are things that ought not happen as we expect, like breaking
the laws of physics. The aim of this paper is, in some sense, to see how insightful this analogy to
forces and motion is.
We can understand this surprisal minimisation as the system holding an inferred model of
the world parameterised by a mode and a variance, the ‘recognition density’ r(η;ηˆ ,ϑˆ ), arising
b η|b
from minimising a free energy functional. Suppose the system estimates the moments of the
environment, such that a density r(η µ) exists where σ(µ ) = η such that σ(µˆ ) = ηˆ . Now
b b b b
|
consider the functional
ln r(η µ) r(η µ)dη ln p(η,b,µ) r(η µ)dη, (6)
{ | } | − { } |
Z Z
the divergence between the variational model and the true joint density. This expands to
ln r(η µ) r(η µ)dη ln p(η b,µ) r(η µ)dη ln p(b,µ) . (7)
{ | } | − { | } | − { }
Z Z
If r(η µ) = p(η,b,µ) then it also decomposes and the entire functional, including the surprisal
|
of blanket and internal states, evaluates to zero; this is exact Bayesian inference. If the system
estimates external states by modelling r(η µ) = p(η b,µ) = p(η b)—which we can show
| | |
[Sak22c, Lemmas 4.1 and 4.2; Theorem 4.1] occurs when µ = µˆ such that (again, abusing types
b
slightly) σ(µ) = ηˆ —then the divergence in (7) vanishes. In other words, if the system estimates
b
data about the environment by engaging in mode-matching, then an upper bound on ln p(b,µ)
− { }
is minimised.
The principle of maximum calibre is a generalisation of maximum entropy to trajectories
[PGLD13]. Using this technology, we can construct the same model over trajectories of a pro-
cess:
ln r(η(t) µ(t)) r(η(t) µ(t))dη(t) ln p(η(t),b(t),µ(t)) r(η(t) µ(t))dη(t), (8)
{ | } | − { } |
Z Z
which appears exactly as expected. More about maximum calibre, and what has been called
‘path-tracking,’ will be discussed later.
C. A physics of beliefs
Theparticular idea of inference undersynchronisation in Section IIB—which packages together
surprisal minimisation, estimation, and coupling into a modelling framework—is referred to as
9
approximate Bayesian inference [Sak22c, Theorem 4.1], and in particular, is referred to as ‘mode-
matching’ when these parameters are stationary: by minimising surprisal, the most likely state is
σ−1(ηˆ ). The a priori assumption that systems minimise surprising events can be justified using
b,t
large deviation principles [Tou09], and as such, most any set of coupled random dynamical systems
can be expected to perform approximate Bayesian inference of some sort. What is more striking
is that two distinct (in the above sense) systems which estimate each other’s statistics necessarily
come equipped with a pair (b ,σ), chosen such that they fluctuate around each other’s most likely
t
states, andthatany two systems with therightinput/outputflow (such that they fluctuatearound
eachother’sstates)comewith(b ,σ)suchthattheyestimateeachother’sstatistics[Sak22c,Lemma
t
4.3].
Bayesian mechanics formulates changes in physical states as changes in probabilities estimated
by η and µ [PDCF20, RSH+22]. In this sense, Bayesian mechanics is continuous with the rest
of statistical physics, and is merely a novel (and possibly more general) way of modelling how
inference techniques like the principle of maximum entropy play a role in physics. Since we can
understand the average state of µ as the preimage of σ, we can understand it as a parameter for
the probabilities of states of η—in a sense we do this automatically, since we can understand a
system µ existing a particular way in virtue of the likely states η of the things interacting with
it, causing or not causing particular µ—and in so doing, we can relate µ to a belief mechanics
by thinking of µ as encoding inferences about the assignment of probabilities to states of η. In
simpler terms, systems exist in a particular way based on their environment. When we model a
system, we automatically model it as modelling its environment by encoding this sort of statistical
estimation in our model of the system. We expect a thing to exist as a particular ‘thing’ based
on whether or not it can exist that way in a given environment. This places constraints on what
σ−1(ηˆ )mustbeforasystemtobe‘system-like’ (e.g., stone-like, human-like, andsoforth); dually,
b,t
it informs what σ(µˆ ) must be, given we have a particular system (e.g., humans require oxygen to
b,t
breathe and cannot live for very long beneath water; stones do not mind.). In both cases we have
a sort of allostatic attractor characteristic of the system. Besides the explicitly non-teleological11
notion of the constraints or intended preimage which are definitional of a system, an important
dual observation is that this parameterisation of likely internal and external states is one reading
11 By teleological we mean an explanation of the way something ‘is’ which is based on the ‘purpose’ of the system,
the means by which a goal is achieved—here, the minimisation of surprisal—rather than its intrinsic nature—an
enforced definitionforitself, called anontological potential[Sak22c,RSH+22]. Indeed,onecanexplicitly contrast
this with the normative role a set of constraints on system states plays. See [MB17] or [GN22] for recent reviews
ofthedifferencesbetweentheseapproachesin modellingself-organising systems,andhowtheyalso reinforceeach
other—namely,theminimisationofsurprisalcanbeseenasallowingthesystemtomeetsomesetofconstraintsas
a goal, evincing an attractor in the state space. This appears to be one sort of statement of our duality [Sak22c,
Section 3, Section 6.2].
10
of Bayesian mechanics which is consistent with the idea of perception or estimation in Bayesian
inference.
It is a deep result in [Fri19] that the existence of a system via the presence of a Markov blanket
is the minimisation of surprisal, and in [Sak22c] it is shown that we can understand that surprisal
minimisation as the control of key existential or essential variables which are definitional of the
system. Mathematically, as we stated above, it is equivalent to the fluctuations of a realisation of
the system about that modal value parameterising the surprisal, and this is even more obvious in
theform indicated in (5). Conceptually it is equivalent to asking that systems stay coherent if they
are (i) at some optimal set-point despite their surroundings,or dually, (ii) encoding optimal beliefs
abouttheir surroundings. As wehave already stated, there are two senses in which this is true: if a
systemestimates itsenvironment, itisorganised cohesively intothepreimageofσ; dually, asystem
is an estimator only if it exists in a cohesive fashion to begin with. More complicated formulations
of Bayesian mechanics update this statistical estimation to a sort of information gathering, where
systems that are able to stay cohesive for longer and enforce an intended state learn enough about
η to change either ηˆ or the coupling σ−1(ηˆ ), and hence maintain an intended µˆ .
b,t b,t b,t
Viewingthisasamaximumentropyproblem(see[RSH+22]or[RSF22]foroverviews,or[Sak22c]
for details), we have in (6) the constraint that the surprisal of the model of external states is, on
average, zero—it is a perfect model [Sak22c, Proposition 4.1]. In (7), we ask that the surprisal of
the approximate model is on average no greater than the intrinsic surprisal of the system [Sak22c,
Proposition 4.2].
D. A physics by beliefs
We may also view such a constraint as applying to the internal states of the system; as stated,
this is the control-theoretic view, which we are automatically sympathetic to in virtue of speaking
of internal states as parameters of the free energy functional.
We have leveraged the fact that, in virtue of being coupled, a value for µ parameterises a recog-
nition density, and that this minimises a bound on surprisal, which arises from the system fitting
a simpler parametric model r(η;σ(µ )) to an approximate posterior density p(η b). Assume both
b
|
densities are Gaussian or that p(η b) is well approximated by a Gaussian, a so-called Laplace
|
approximation. Let Σ synchronise variances, just as σ did means or maximum a posteriori estim-
ates (what we have called modes). Consider the following: the KL divergence for the univariate
11
Gaussians
D r η;σ(µ ),Σ(ϑ ) p(η;ηˆ ,ϑˆ )
KL b µ|b b η|b
k
h i
(cid:0) (cid:1)
evaluates easily to
ϑˆ η|b Σ(ϑ µ|b )2+(σ(µ b ) ηˆ b )2 1
ln + − ,
(
Σ(ϑ
µ|b
)
)
2ϑˆ2 − 2
η|b
and this expression vanishes when µ = µˆ and ϑ = ϑˆ , such that σ and Σ act on these
b µ|b µ|b
parameters appropriately. Generalisations to the multivariate case follow easily. (Looking at this
functional form, it is obvious that the variational free energy couples the two systems.)
We might also observe that the system minimises a surprisal function of its own beliefs when
the KL divergence vanishes, since ln r(η µ = µˆ ) = (σ(µ ) ηˆ )2 = 0 in that case, where
b b b
− { | } −
E [ ln r(η µ) ] = E (σ(µ ) ηˆ )2
r r b b
− { | } −
=(cid:2) Σ(ϑ ) (cid:3) (9)
µ|b
= E (σ(µ ) ηˆ )2 = E [ ln p(η b) ]
p(η|b) b b p(η|b)
− − { | }
(cid:2) (cid:3)
by construction (recovering Proposition 4.2 of [Sak22c]).
Moreover, the system minimises a surprisal function of its own states when the KL divergence
vanishes,since ln p(µ η = ηˆ ) = (µ σ−1(ηˆ ))2 = 0. Whilstsubtlerthanitfirstappears,12 our
b b b
− { | } −
na¨ıve suspicion that the systems synchronise across both sides of the blanket is valid at the point
of synchrony, since the roots of these two equations are the same. This allows us to view Bayesian
mechanics dually, as a control problem, where the system maintains an unsurprising set-point µˆ .
b
There is no guarantee of either being possible in general; the system may be constantly surprised
if, for instance, it cannot occupy the expected value mirroring the environment (at which point it
will cease to exist as it was, and transition to a new dynamical regime with whatever mean it can
occupy) or if the Gaussian approximation indicated here is a bad model of the environment (under
which samples of η are surprising). This ‘sleight of hand’ is what lends the tautological truth that
systems which exist do unsurprising things—and that coupled systems which exist synchronise
their statistics—a more interesting, model-based interpretation.
As stated, if we have the two constraints
E [µ η] = σ−1(ηˆ ) = µˆ (10)
p(µ|η) b b
|
12 See“AdjointStatisticalInferences,”forthcoming, foranextendedtreatmentofthefunctionalformofthisduality.
12
and
E µ E [µ η] 2 = Σ−1(ϑˆ ),
p(µ|η) p(µ|η) η|b
− |
(cid:2) (cid:3)
where η is now a choice of parameter, then we have a maximum entropy problem: the expected
surprisal of µ given b is equal to the intended value of the expected deviation of µ given b from
the synchronised value of the parameter, which is the intended average. The foregoing statement
that the average surprisal is non-zero, but the surprisal is zero for the average state, is that the
distributionofµgivenbisaGaussian,whoseentropy(expectedsurprisal)issomeintendedvariance
(expected quadratic fluctuation): we have
ln p(µ η) = (µ σ−1(ηˆ ))2
b
− { | } −
for the stationary solution to that maximum entropy problem. Note that thedualof theconstraint
equation (10)—i.e., that the mean of r(η µ) is the same as the mean of p(η b),
| |
E [η µ]= σ(µˆ )= ηˆ = E [η b], (11)
r(η|µ) b b p(η|b)
| |
is precisely the constraint that the mode of one density synchronises to the mode of the other.
This reasoning also applies to the variance constraint following (10).
In (9) and (11), we constrain the variance and mean of two different densities to be equal. Our
observation that this is necessary and sufficient for the minimisation of variational free energy is
nothing more than an instance of the information projection theorem. Note also a consequence of
(9), that the average surprisal is generically non-zero: the expectation of this surprisal, which is
equivalent to avariance, isΣ−1(ϑˆ ). Thatis, if thevariance is non-zero, then sotoois theaverage
η|b
surprisal. Instead, the surprisal of the average state is zero and hence the ideal system, on average,
minimises surprisal, since the average state of that system is a surprisal minimiser. In parallel to
this, the system explores the state space by fluctuating precisely how it must in order to sample
the full suite of environmental states, ϑˆ . Since the path measure is formulated such that the
η|b
average of fluctuations is the expected surprisal, and thus, the entropy, this is really a consequence
that the entropy of a constrained maximum entropy distribution is whatever the average value of
the constraint is, which is not generically zero (see [PGLD15] for an argument along these lines).
Prosaically, we could say that the average surprisal of the system is not minimised in general, but
the surprisal is minimised by the system on average.
Finally, a remark on notation before we proceed further: notice that we have imposed the
constraint that the internal state, sans explicit consideration of the blanket state, is on average
13
the synchronised average internal state given a blanket state. Implicitly, we have incorporated a
constraint that the input-output flows characterising the Markov blanket are aligned, such that
blanket state b is shared. Inother words,this is notonly a constraint thatthesystem synchronises;
we also fold in a constraint that the interface b is shared, such that synchronisation is possible.
The suggestion of this very point goes back to defining the synchronisation function as a function
of two arguments, µ , recapitulating the tensor-Hom adjunction in [Sak22c, Lemma 4.3]. Indeed,
b
there it is shown that the synchronisation function exists if and only if the interface is shared, in
which case, a partial function ξ(b, ) of µ exists.13
−
This maximum entropy model is our belief about the state of the system whose beliefs are
minimising free energy [RSF22]. Since this is equivalent (in fact, dual) to a problem of variational
free energy minimisation, the same laws of motion for belief updating that Bayesian mechanics
defines apply to our own beliefs, and symmetrically, such laws are implemented by our beliefs
about the systems being modelled. This makes Bayesian mechanics also a control problem: we
model a system as leveraging its model—and these laws of motion in belief space—to engage
in a kind of KL control, where the control parameter is the surprisal-minimising average state.
Generalising, we could think of path-tracking [RSH+22] as a path integral control problem given
a reference trajectory µˆ (t), conjectured in [Sak22b] and discussed later in this paper (see also
b
[BDCF+22, STvdM+22] for recent work in this direction).
Indeed, when we speak about control, we can dualise the above problem to ask that a system
controls itself to maintain a system-like set-point. That is, we ask that the self-surprisal (the
magnitude of fluctuations)
ln p(µ η ) = (µ σ−1(ηˆ ))2
t t t b,t
− { | } −
is minimised, and moreover, that the accumulated surprisal
t
ln p(µ(t) η(t)) = (µ σ−1(ηˆ ))2
τ b,τ
− { | } −
Z0
is minimised. This inherits directly from our above discussion about path probability measures.
Bayesian mechanics leads to various types of approximate Bayesian inference, just like classical
mechanics admits different applications of Newton’s laws of motion (e.g., the continuum mechanics
of fluid flow, or orbital mechanics for satellite motion). As stated, when that parameter is trivial,
we have mode-matching; when it is dynamic, this is referred to as mode-tracking [RSH+22]. When
13 The same idea appears elsewhere in statistical physics in various disguises; see [Lyn22] or [Mye22] for detailed
formulations of input-outputcomposition.
14
applied to beliefs about the trajectories of external and active states, we have a more general
version of Bayesian mechanics only recently explored, including active inference [BDCF+22]. This
has been referred to as ‘path-tracking’ in [RSH+22]. The taxonomy described here exists in the
same sense as minimising the action of the Lagrangian = K V yields Newton’s second law of
L −
motion, which can be applied to various sorts of systems when we know what sort of function V
is. We will work out in some detail what this taxonomy looks like in the world of classical physics.
In summary, Bayesian mechanics contains two key pieces of data: the surprisal action, and the
synchrony map σ (and thus, implicitly, a boundary). These data define beliefs and belief dynam-
ics, respectively, and pair them with acharacteristic geometry (information geometry [Ama16], the
study of statistical manifolds, or so-called ‘spaces of beliefs’). It is interesting that most physical
theories are paired with a characteristic geometry in which they play out [ADH10], such as sym-
plectic geometry in classical physics [AGN01]. Later in the paper we will point out the appearance
of symplectic forms in Bayesian mechanics, which is notable given the analogy we draw.
III. A GENERAL EQUATION FOR BAYESIAN CLASSICAL MECHANICS
We begin with a classical particle described by a position variable, q, at some time t. The
mass of the particle will be denoted by m. The position plays the role of an internal state for the
particle. The particle has an external environment interacting with it, whose states η determine
what forces are acting on it. This paper will build classical physics as the Bayesian mechanics of
classical objects in an environment.
Why have we chosen classical physics as the setting of our exercise? Classical physics is well-
established as theconsequenceof aparticularly simpleleast action principle, with as many flavours
of kinematics as there are forms of approximate Bayesian inference [RSH+22, Section 3] meaning
that there is an opportunity to formulate straightforward least surprisal rules for this case. More
interestingly, because the dynamics of classical systems are non-dissipative and exhibit periodic or
chaotic motion, classical physics leads directly to challenging mathematical and physical situations
which are of interest to describe. The characterisation of classical physics as a mechanics for
infinitely precise beliefs makes it easier to handle some of this difficulty, which we will observe
in Section VII. Finally, if Bayesian mechanics is a physics whose boundary conditions are related
to boundaries, we will see classical physics is particularly easy to formulate: the sparsity of the
coupling is almost obvious.
Let F be the vector of total force applied to the system at a time t, F . The reception
t i i,t
P
15
of an applied force is like a blanket state for the particle, which can couple to and affect internal
states. The attentive reader will likely have noted that a force is not a state of the particle, but is
an interaction of the environment with the states of the particle. Initially it may seem suspectthat
this Markov blanket is not part of the system, nor is it a physical state at all. It is perhaps a type
error to identify a force with the measurement of that force, but beyond that, our construction is
unproblematicforthefollowingreasons: (i)themeasurementofaforceF is exactlyasensorystate,
t
(ii) a Markov blanket can be construed as simply that set of states which enforces the separation
or distinguishability of two systems [Sak22d], which a force certainly is, (iii) the failureof a force to
map onto an internal state is precisely the failure for separation to be enforced, which is also when
surprisalis high (see [Sak22c], whereit is proven that the integrity of the boundaryis equivalent to
low system surprisal under a good variational model) meaning this functions as a Markov blanket
regardless of its physicality. Moreover, simple objects have, in general, correspondingly simple
Markov blankets.14 A single sensory state for a single classical point particle fits that bill.
That being stated, we consider F itself to be like a sensory state of the particle. As such, we
t
have an inverse synchrony map,
σ−1 : η F q .
t t t
→ →
The former map, η−1, merely sends external states of the world to the forces the world applies on
objects, which will be done implicitly throughout the paper, as we provide worked examples with
particular applied forces. Let s be a temporary time variable. The latter map, µ(F ) = q , is the
η,t t
solution to an integral equation determined by Newton’s second law,
F(η,s)
F(η,t) = q .
t
7→ m
ZZ
In other words, the particular functional form for the coupling σ we have used is one that sends
the average acceleration of the system to the average force applied to the system,
σ−1(η )= q .
F,t F,t
The ideal path of internal states, which encodes an optimal (i.e., unsurprising) belief about
what the system is being told to do by the environment, consists of precisely these q , for whom
t
σ(q ) = η . Note the consistency with more recent formulations of the FEP: for as long as
F,t F,t
there exists a particle, there exists some (possibly trivial) blanket distinguishing that particle from
14 See “Path Integrals, Particular Kinds and Strange Things,” forthcoming. See also [FFGL22] for an argument
that boundaries are information-theoretic objects; it follows that things with many states and a large amount of
information to betransmitted need suitably complex boundaries, and things that donot, do not.
16
its environment along its path of evolution [FCS+22, Sak22b]. We can argue that such a blanket
exists—that any classical particle under the partition indicated above is sparsely coupled on the
time-scale over which it exists—simply by pointing out that we can read off F and need not
t
consider η to get internal states q . Physically, this is the intuitive statement that it is only a force
t t
that matters to motion, not what generated that force. Hence, by construction, for as long as a
classical particle exists to be acted on, it has a blanket. See [HDC22, Sak22d] for more on sparse
coupling.
Just as we presume that the mechanics of beliefs should lead to physical mechanics (control),
minimisation of the surprisal should lead to physical mechanics (Newton’s laws of motion) and
trajectories that abide by those mechanics. In establishing that physical mechanics follows from
Bayesian mechanics, we first focus on beliefs about internal states, and relate them to the beliefs
carried by internal states afterwards. The surprisal Lagrangian, ln p( ) , is applied either to
− { − }
p(q ) given F , or is applied symmetrically to the probability of p(η ) given F . We would like to see
t t t t
whether the minimisation of surprisal under σ recovers classical physics in the context of Bayesian
mechanics, sowetrytominimise ln p(q η ) undertheparameterisation ln p(q ;σ−1(η )) .
t t t F,t
− { | } − { }
Here, wetake amoment toremindourselves thatdualisingthe objectin theLagrangian to internal
states gives us a physics of our beliefs about the system, or the system’s beliefs about itself, which
is the duality indicated in [Sak22c]. Contrast this with the surprisalLagrangian on external states,
which gives a physics of the system’s beliefs about its environment, in [FCS+22]. So, to set the
stage, we change to the dual problem to (8) (i.e., finding p(µ(t) η(t)) instead).
|
Under a noise injection, or else some uncertainty associated to the belief about a position at
t, this becomes a problem of mapping means to means, in the sense of the approximate Bayesian
inference lemma. We are primarily interested in the probability of deviating from the path of least
action, such that our (state-wise) surprisal is a measure of
p(q ;σ−1(ηˆ )) = p(q ;qˆ ).
t F,t t F,t
Suppose the acceleration at a given time is constrained such that, on average, it obeys the forces
being applied to it at that time. This is an instantaneous picture, licensing a non-dynamical
application of Bayesian mechanics (note that we used that in Section IIA as well, where the
instantaneous story behind derivatives licenses dropping the time variable from q(t), a position in
time). Denoting an expectation with E and neglecting subscripts, this gives us the equation
F(η)
E [q] = . (12)
p(q)
m
ZZ
17
Suppose we also constrain the system to be classical, in the sense of having infinite precision. This
is a variance constraint, namely, that
E q E [q] 2 = 0.
p(q) p(q)
−
h i
(cid:0) (cid:1)
When asking about paths, we ask that the accumulated variance of or along a path is also zero:
t
E q(τ) E [q(τ)] 2 dτ =0. (13)
p(q,t) p(q,τ)
−
(cid:20)Z0 (cid:21)
(cid:0) (cid:1)
A similar law for the total squared displacement of a path has been used to produceNewton’s laws
from the principle of maximum path entropy (maximum calibre) before [GDG14]. Both of the
above equations are simply a constraint that the optimal acceleration does not deviate from what
the environment tells it to, which implies the approximate Bayesian inference lemma when under
the additional constraint that the average state is the value of the synchronisation map [Sak22c].
Here we have arrived at an important point: our belief about a system is an FEP-theoretic
model of the system. We believe the system constrains itself to be the optimal parameter for
some belief over what it is supposed to do in its environment. That parameter happens to be
the least surprising internal state given some external state and a shared blanket state. This is
whatismeantinpreviousdiscussionsaboutbeliefsaboutinternalstates beingdualtobeliefsabout
externalstates. Thatis, inthesensethatbuildingamodelofasystem whichconstrains thesystem
to exist at an ideal internal state is maximum entropy, maximising the entropy of our model under
these constraints is the ‘doing’ of the free energy principle.15
Theoptimal(i.e., least biased[PGLD13])belief underthesetwoconstraints canbederivedfrom
the principle of constrained maximum entropy, yielding
2
F(η)
p(q) = exp λ q λ (14)
1 2
− − m
( (cid:12) ZZ (cid:12) )
(cid:12) (cid:12)
(cid:12) (cid:12)
with λ
1
,λ
2
> 0 being Lagrange multipliers for(cid:12) the constraints(cid:12)indicated above. Considered dy-
namically, this equation can be given as a path probability density
t F(η,s) 2
p(q,t) = exp λ (t) q(τ) λ (τ) dτ . (15)
1 2
− − m
( Z0 (cid:12) ZZ (cid:12) )
(cid:12) (cid:12)
(cid:12) (cid:12)
When λ−1(t) 1 and λ (t) = 1 for all t, w(cid:12)e can reproduce classica(cid:12)l dynamics. In particular,
1 ≪ 2
in the limit λ , there is no uncertainty at all, and the most likely path under those con-
1
→ ∞
straints—the classical path of least action, by construction—is the only path we lend any non-zero
15 The idea that the maximum entropy principle under existential variables is the free energy principle under a
synchronisation map was first suggested in [And21]. A proof of this can be found in [Sak22c].
18
probability to. This is something like a classical limit for our path probability, in the same sense
as taking ~ 0 recovers classical mechanics from Feynman’s path integral. Moreover, a reliable
→
heuristic is that fluctuations in the theory are scaled by D such that λ D−1. The degree to
1
∝
which something can explore states within some allostatic boundsis precisely the variance under a
Laplace approximation, yielding an important role for the Lagrange multipliers on the maximum
entropy side of the story. This role is not necessarily evident in the FEP proper, nor in conceptual
treatments of this duality which are divorced of process-level details.
Finally, note that (15) is (5) for a modal path given by (12). This will be our general equation
for Bayesian classical mechanics.16
IV. A QUESTION OF QUANTUM ONTOLOGY
We began with the aim of showing that classical physics can be derived from Bayesian mech-
anics, by showing that deviations from a classical law of motion are surprising given a particular
action functional,andthatBayesian mechanicsdescribestheminimisationofsurprisal. Dosystems
actually infer whattheirclassical laws of motion are, andfollow those inferences toavoid surprisal?
More to the point: is there a less ‘just so’ aspect of reproducing classical physics from the assump-
tion that the least surprising trajectory of a system minimises the classical action? Can we do this
without arbitrarily assuming the laws of classical physics and merely showing that unsurprising
systems obey those laws? There is indeed a more concrete interpretation of this inference, one
which makes the idea behind (15) more subtle.
In fact, what we have shown in the foregoing statements is that, under surprisal minimisation,
a system takes a classical path when that path is the average. We can demonstrate that this has
some further meaning by showing Bayesian mechanics is naturally derived from simpler arguments
about the role of probability in quantum mechanics, such that the modal path of any fluctuating
system is a classical path, and surprisal minimisation already exists in quantum mechanics. That
is, we can derive Bayesian mechanics from the idea that a system is classical on average, just as
we can derive classical mechanics from the idea that systems obey Bayesian mechanics for classical
averages. This suggests a view that (i) systems with randomness do inference over their classical
laws, and (ii) in the quantum setting we recover classical physics by doing inference. A more
complete quantum physics manual for the Bayesian mechanic is to be written elsewhere.
16 Notethat,foraheuristicintegraloveraninfinitesimaltime,i.e.,fromttodt,(15)isequalto(14). Thismakesthe
equationwiththetimeintegralthegeneralcase. Intuitively,whatwehavenotedisastatementthatinstantaneous
variations in a path accumulate along thepath as thepath goes forward in time.
19
Begin from the supposition that classical equations of motion are asymptotics of quantum
equations of motion, given by the empirical observation that we can measure classical effects more
readily than quantum ones, but also, that classical equations of motion depend on parameters
with underlying quantum effects, in such a way that quantum effects still bleed into the classical
world when we ‘zoom in’ to the extent that those parameters are no longer renormalised.17 This
leads directly to the correspondence principle, a law of large numbers for quantum probabilities.
The consequence of this classical ‘limit’ is that the evolution of the most likely state of a quantum
system gives us what we define as classical physics. Proven by Ehrenfest in 1927, we can rewrite
this result (assuming ∂ q(t) is bounded above almost surely—physically, a thoroughly reasonable
tt
| |
assumption) as
E ∂ V(q) = mE ∂ q(t) .
p(q(t)) q p(q(t)) tt
−
(cid:2) (cid:3) (cid:2) (cid:3)
So, assuming distance constraints like those above, such that we have a Gaussian measure or
otherwise a Laplace approximation—a constraint which is quadratic in fluctuations—the most
likely path ought to be a classical equation of motion. We will repeat this argument in Bayesian
mechanical language, aided by the technology of the path integral.18
In order to reproduce the idea that, probabilistically, quantum fluctuations are merely correc-
tions toclassical estimates, wetakeaWiener measurewherethevarianceof pathprobability—as it
is given by the probability of thevelocity on such a path—is scaled by some characteristic constant
m,
2~
m t
Z−1exp ∂ q(s)2ds dq(t).
−2~ s
(cid:26) Z0 (cid:27)
Note that, technically, we have Wick rotated our field theory. Note also that, appealing to the
Trotter product formula, we can separate out the potential and assume it is zero everywhere on
the support.
As we remarked before, setting λ = ~ and taking the scaling of quantum fluctuations to
1
zero, and making use of the fact that fluctuations are precisely what contribute to the surprisal,
we have a statement that in the quantum regime of Bayesian mechanical dynamics, the least
surprising trajectory of the system is one that follows an overlying classical equation of motion.
Indeed, under the WKB approximation, the most likely path in the path integral is the classical
equation of motion of the field. Without quantum fluctuations about this classical solution we
17 This is also referred to as an adiabatic approximation, and appears in semi-classical physics.
18 Wepoint thereader to [Hal13] for details.
20
have classical physics, whereas in perturbative approximations to quantum mechanics, such as the
quantum effective action, we add those quantum fluctuations in as higher order correction terms
to a classical ansatz.19
So, we are now armed with two facts: basic physical observations suggest that classical paths
are most likely paths, and, the canonical measure on paths is in terms of fluctuations about such
a classical mode. We wish to see if the mathematical fact that this is the canonical description
of path probability reflects the physical fact that classical equations of motion are the most likely
paths of a system. Our formulation suggests that taking the limit ~ 0 would be the right
→
approach. Formally, this limit scales fluctuations to zero, revealing the most likely state as the one
with constant velocity v(0): a classical equation of motion under our identically zero potential,
from where we derive no applied force and hence no acceleration.
Thoughwe omit theproof, one can indeedprove thattaking ~ 0 results in classical equations
→
of motion. As noted by Feynman, this most likely path is what is most likely to be determined by
observation (experiment), and thus determines the classical limit of the path integral. That this
story—the most likely path is the classical one due to a priori assumptions about the state and
measurability of the world—implies the minimisation of surprisal, as well as following from it (as
discussed in Section III) is non-trivial.
What does it actually mean when we pass from ‘most likely’ to ‘least surprising?’ Least surpris-
ing to whom? Certainly theexperimenter—there is a sense in which the entire problem is dualised,
and we are minimising the surprisal of our beliefs about what a system does. That is, the two
random dynamical systems coupled here are a quantum particle and an environment (including,
perhaps, an external observer).
Do quantum particles do inference over where their classical paths are, organising themselves
on average into the preimage of the average state of the observer, who expects to see a system
follow a force applied? In the sense of estimating what that path is by taking a path which
wastes the least energy (so to speak) in response to a force and in absence of quantum noise—and
encoding such a path in their own dynamics, thereby inferring what another object ‘tells’ them to
do classically—they do. This makes Bayesian mechanics a useful way of modelling how a quantum
system treats information and interactions withits environment,20 defaulting to classical equations
of motion on average. It is in this Bayesian mechanical sense that classical physics is a result of
19 This section assumes there is a unique such classical solution to the system. Degenerate classical minima are
handled by instanton theory, which we will not cover here. An excellent overview of the topic can be found in
[VZNS82].
20 We will refer the reader to [FFGL22] and related references for more details about quantum information theory
undertheFEP.
21
Bayesian inference in a quantum regime. It is not so trivial that, at the quantum level, such a
particle is inferring what its classical trajectory—in modern parlance, inferring what its coherent
state—is, given some noise statistics and a macroscopic or higher-level force acting on it. It is this
higher-level inference that organises the noisy, quantum system into a classical system.21
More relevant to our case would be to return to the example of the boughs of a tree. A particle
undergoing a force must minimise surprisal to stay cohesive, as we argued, and therefore can be
read as inferring where the environment is directing it: what the environment is doing and how it
is interacting with the particle. At the classical level, this is a concrete inference problem, wherein
we try to find the maximum a posteriori estimate of a density with thermal or quantum noise,
furnishing the classical path ‘intended’ by the environment.
V. THE MATCHING OF MODES
This section begins a worked example of the typology described in [RSH+22, Section 3]. We
beginwith‘mode-matching.’ Mode-matchingistheapplicationofBayesianmechanicstostationary
objects which engage in approximate Bayesian inference [Fri19, Sak22c, RSH+22]. In this case, by
definition of stationarity, the most likely internal state is fixed. Typically valid over only a brief
time-scale—since nothing is stationary forever and nothing which is stationary and non-adaptive
resists entropy for long—this is the simplest case of Bayesian mechanics.
Inspired by [Sak22c], we formulate mode-matching under approximate Bayesian inference as
internal states being constrained to be optimal parameters for a recognition density. Again, this is
fully equivalent to the proper FEP. Using Theorem 4.2 (ibid), we can formulate the minimisation
of surprisal applied to internal states ln p(q η ) as a demand that the log-probability equals
F
− { | }
some constraint on those internal states, with further precision-based minimisation possible over
an ensemble of states. Here, that constraint is
2
F(η)
S[q]= ln p(q η) = λ q .
1
− { | } − m
(cid:18) ZZ (cid:19)
Note the similarity to equation 3.5 in [DCFHP21]. Under approximate Bayesian inference (and a
further, but generically acceptable, Laplace assumption), the ideal state is the most likely state,
which is the minimiser of this squared displacement.
The system we describe could be a stone performing inference over the cancelling of its gravit-
21 The same phenomena has been demonstrated in self-organising soft matter systems [RHT+21], suggesting this
may be a promising line of research on collective excitations in condensed matter.
22
ational pull and the normal force emanating from the ground, obeying
F = F +F = 0. (16)
g N
−
X
In this case, the stone’s acceleration is zero, and under the initial conditions v(0) = 0, q(0) = 0,
it goes straight to—and remains at—the mode q = 0. Referring back to the Introduction, the
mode is a particular attractor which is a fixed point for the system. Indeed, for stationary initial
conditions, the surprisal above is zero precisely when (q 0)2 = 0.
−
Notably, this also means that for Bayesian mechanics to be consistent in the classical setting,
it must (for stationary modes) imply Newton’s first law—i.e., that for every applied force, there is
a force of equal magnitude applied in the opposite direction.
VI. THE TRACKING OF MODES
Mode-tracking can be summarised as the existence of a target mode, i.e., a desired mode that
systems are tracking towards, either for a finite time or constantly. This allows us to describe
the most likely flow of autonomous states as a flow of beliefs [Fri19, PDCF20, AMTB22, RS22],
and involves the iteration of approximate Bayesian mechanics [Sak22c]. Within mode-tracking we
introduce two further distinctions, which is a more granular approach than the three-fold structure
described in the Introduction: systems which track, but settle to, a mode, and systems which
constantly chase a mode. Theformer is an exampleof a system that terminates in mode-matching,
whilst the latter is an example of a system that is in constant motion. For both cases, we pass to
an idea of dynamics, gesturing at an application of the principle of maximum calibre [Sak22b].
A. Terminal mode-matching
Suppose the total force applied is dynamic, but eventually equilibrates. An example would be
a stone tossed into the air, which travels through a gravitational field and eventually returns to
the ground. The variant of (16) corresponding to that motion is F = F , and the solution of the
g
−
integral equation (12) under that F is
1
q(t) = q(0)+v(0)t gt2 (17)
− 2
where g is the acceleration due to gravity. This equation has a steady state value where the mode
q(t) = 0 and remains there, reached at some hitting time t . For instance: for v(0) = 0 metres
hit
23
per second and q(0) 4.91 metres above the ground, t 1 second. So, we can consider the full,
hit
≈ ≈
dynamic-in-time force as
F (t)+1 (t)F (t),
g hit N
−
where 1( ) is an indicator function—the constant function equal to one on t t , and zero
hit
− ≥
elsewhere. This terminates in the mode-matching explored in the previous section, since for all
t t , we have F = 0. Indeed, q(t t ) = 0 by construction; this is a stationary mode.
hit hit
≥ ≥
By solving (12P) as a constraint relation, we have actually asked that the average path obeys
(17) and that no other path q(t) deviates from that path. That is, we want q(t)= qˆ(t) in the ideal
case. As such, classical objects can be modelled as performing inference over the forces driving
their motion, and given that they find known laws of motion unsurprising, get driven to the mode
described in Section V by following (17). For example—it would be surprising to the very fabric
of space-time if a stone which had landed on the ground at some t were to spontaneously jump
hit
after. Thus, by obeying (17) and following classical laws of motion, the surprisal of motion is
minimised. We will detail this below.
We begin by asking that q(t) should equal qˆ(t). Under our other constraint on the expected
path—the solution to (12), given above—eventually, qˆ(t) = 0 (in particular, for all t t ). We
hit
≥
could view this as dynamical inference more generally, or, the construction of a realisation of some
path under a steady state density with mode zero, such that qˆ(t) = 0 after some convergence
time (here, t ). In that case, the system goes directly to qˆas its kinetic energy ‘dissipates’ into
hit
potential energy.
Ingreater detail: apath consists of alist of states. Each such state is increasingly morelikely as
we approach the mode, and indeed, a mean-reverting process will go to a mode on average under
a quadratic potential. As such, the average path taken by the system performs a gradient ascent
on the probability density, or equivalently, a gradient descent on the surprisal. Note that we are
not working in a path space here; rather, the path exists on a probability density, as a lift of a
list of states, which is indeed simply a path—but, we don’t speak about surprisal minimisation
on such a path directly, instead speaking about the tendency of any path to go to a fixed point.
Mathematically, this means that the motion of q(t) is a gradient descent on q(t) qˆ(t)2, with
| − |
some convergence time t , and q(t) = qˆ(t) = 0 for t t . Since the logarithm of (14) is this
hit hit
≥
distance when λ = 1, this is equivalently a minimisation of surprisal. That is, we have
2
∂ q(t) = λ ln p(q) ,
t 1
− ∇ { }
24
such that the least surprising state is the mode, and the system takes a path towards that mode.
Moreover, the least surprising path to the least surprising state ought to traverse the distance
from some initial q(0) to qˆthe quickest, which (for a fixed velocity) is the path given by a direct
gradient descent on the distance. It would be of interest in future work to give a unified view of
these approaches, i.e., to prove that undercertain asymptotic conditions, least surprisingpaths are
paths towards least surprising states.
Since λ scales fluctuations, it is proportional to the inverse diffusion coefficient, or the cov-
1
ariance, more generally. Additionally, since there is no random motion and no opportunity to
explore, the motion is described by a pure gradient descent, and so this yields one component of
the Helmholtz decomposition discussed in [Fri19]. Note that the instantaneous Lagrange multi-
plier λ plays a different role than the dynamical Lagrange multiplier λ(t). In particular, the former
selects states whilst the latter selects paths, a distinction that becomes important when we deal
with paths towards least surprising states, as we have here. We still desire λ (t)−1 0 as we
1
→
did in Section IV, to reproduce classical path selection. In the Gaussian case this is precisely our
uncertainty over paths, as we mentioned.
B. Infinite mode-tracking
This section will discuss itinerant objects whose gravitational field is such that the mode is
never stationary—which we could call mode-chasing, as a sub-type of mode-tracking. Consider a
planet in a gravitational potential equal in all directions: there is no stationary state, and hence,
no meaningful mode, to the dynamics of this system. Na¨ıvely, there is no parameter through
which to minimise surprisal. This does not mean the FEP cannot describe satellite motion. On
the contrary—the application of the FEP to complicated systems is where it truly shines [RS22].
Here, we can iterate self-evidencing to find out what constant motion dictated by a consistently
dynamical principle might look like.
Like any classical system, in the absence of a force acting on it, a satellite system will continue
to move on its trajectory. This is Newton’s first law—the usual aphorism being, ‘a body in motion
tends to stay in motion; a body at rest tends to stay at rest...’ with the phrase ‘... unless
acted upon by an outside force’ often appended. As such, the hitting time formulation of Section
VIA is no longer directly useful, but having no hitting time certainly is. Note that the lack of
a mode—but presence of circular, solenoidal flows—for true classical systems22 is consistent with
22 That is, energetically conservativesystems, orsystemsin theabsenceof dissipativeforces. Althoughnotadissip-
25
[DCFHP21, Sak22c].
Suchasystemhasnodissipativecomponent, sinceitispurelyclassical. Thismeansthegradient
descent describing the mode-matching dynamics we discussed in Section VIA degenerates, in the
opposite sense as any exploratory component of the flow degenerated in that section. Here, we
have a system which travels along a level set of a sphere of radius r, and in particular, travels
such that the surprisalof states (parameterised by a stationary mode) is a non-zero constant along
a path. By simple arguments in symplectic geometry—the geometric study of flows in classical
physics [AGN01, dS08]—flows which are level sets of some Lagrangian23 and which admit radial
coordinates are described by a skew-symmetric matrix. Indeed, level sets of the sphere centred on
qˆ, projected down to the state space, are integral curves of the following equation:
q 1 (t) 0 ν q 1 (t) 1 qˆ 1 (t)
∂ = + , (18)
t
     ν  
q (t) ν 0 q (t) qˆ (t)
2 2 2
−
      
where ν controls the system’s frequency, or speed of travel along one such level set. Note that this
system of equations corresponds to the second-order ordinary differential equation
∂ q(t) = ν2q(t)
tt
−
for either coordinate q or q (simply perform the matrix multiplication in (18), take the derivative
1 2
∂ q (t), and insert the expression for ∂ q (t) into the result) whose solution is
tt i t j
q (t) = rcos(νt)+rsin(νt)+qˆ ,
1 1
q (t)= rsin(νt) rcos(νt)+qˆ
2 2
−
for some constant r > 0. Note also that
ln p(q,t) = 2λ (q(t) λ qˆ). (19)
1 2
−∇ { } −
As such, for an appropriate choice of λ (namely, one half, or half the coefficients of the gradient
1
descent operator, should it exist) and λ (namely, 1/ν), inserting (19) component-wise into (18)
2
−
yields
1
∂ q (t) = i ln p(q,t) = Qiq (t)+ qˆ
t i −∇ j { } j j ν i
with Q1 = ν and Q2 = ν, which is exactly the other piece of the Helmholtz decomposition.
2 1 −
ative system, one can contrast this with the loss of energy of motion that occurs upon colliding with the ground
in Section VIA. It is consistent that such systems havea mode whilst unperturbedsatellite motion does not.
23 Notethat we typically consider a Hamiltonian, which is metric isomorphic toa Lagrangian.
26
As we stated, the matrix operator indicated is skew-symmetric, and, the gradient on the sphere
is locally orthogonal totheselevel sets—that is, movingon alevel setdoes notchangethegradient.
Since there is no mode for these dynamics, we cannot describe a gradient descent on surprisal in
the sense of Section VIA, but we can say that it is a gradient descent for which the value of
the gradient is preserved, with velocity scaled by the matrix indicated. Future work will make
the arguments given here—with respect to the Helmholtz decomposition being an artefact of the
geometric nature of certain flows—more formal.
VII. PATH-TRACKING, AND A SIMPLE CASE OF G-THEORY
This section will progress the construction to more complex forms of path-tracking that are not
amenable to the mode-based descriptions we discussed previously.
What has been called G-theory is the duality between maximum calibre and the genre of
Bayesian mechanics applying to surprisal on paths [RSH+22], which we have begun to explore
here. This pitches G-theory as the generalisation of the duality explored in [Sak22c], extending
that construction to paths. In its full generality it is thought to accommodate descriptions of
complexsystems(likenon-Markovian behaviours,movingattractors, andnon-stationary statistics)
more naturally and with greater fidelity than in the past. Some inspiration for this comes from the
previously mentioned principleof maximumcalibre, which does famously wellon difficultproblems
in non-equilibrium statistical physics [PGLD13, DWW+18, GDAD20]. These results suggest that,
whateverG-theorywillprovetobe,itwillbeacanonicalmodellingframeworkforcomplexsystems.
Here we provide a simple example of this framework by formulating path-tracking, the least
surprisal principle on paths—our third sort of approximate Bayesian inference—as an explicit
problem of dynamical constraints. We then formulate a connection to chaos by examining the
path-based nature of G-theory in greater detail.
A. Path-tracking
Recall our results on mode-tracking in Section VIA. The construction there is obviously ineleg-
ant—besidesformulatingthepathover thestatespaceinsteadofdoingproperdynamicalinference,
it exchanged the proper accumulated squared displacement in (15) with a less general instantan-
eous squared displacement at t . Foreshadowing a more general extension to paths, the most
hit
natural formulation of this problem is readily seen as a gradient ascent on the path probability
27
density. Under maximum calibre, our constraints lead to a probability density
exp λ (t) q(t) qˆ(t)2 ,
1
− | − |
n o
where the expected path (17) is denoted by qˆ(t), and q(t) qˆ(t) 2 is limitingly zero. For dynamic
| − |
F, this is a moving Gaussian, with mode centred along the path for a given state-wise marginal.
That is, it is centred on the list of q’s visited classically for a list of times, like a crest in the path
space that runs directly over the intended states (and hence, a sort of mountain of probability for
realisations flowing along the state space, concentrating them in that region). Path tracking is
obvious in this situation—it appears to follow a gradient descent on the action, finding the most
likely path by finding the summit at each t of p(q(t)).
We can indeed still discuss a gradient descent in this case, but it is a functional gradient on
the action S, such that the gradient descent is on deviations along a path, minimising fluctuations
fromthepathof leastaction—and thisis precisely theprincipleofleast action, orof leastsurprisal,
whenthepath of least action is theexpected path (guaranteed by (4), as discussed inSection IIB).
Let δq(t) be some first order variation of a path away from a path of stationary action at t (see
[RSH+22, Figure 1] for a depiction), which is in fact a realisation at t of some fluctuation away
from the expected path. Analytically, this means that we have
δq(t) = ˆS[q(t)]
−∇
where the kernel of the gradient in the path space, ˆ, is a path such that the system only changes
∇
to second order under variation—that is, it is a path q(t) such that the distance between q(t) and
F(η,t) is minimised. This equation simply expresses that the path of least variation is the
RstRationary, or expected, path. This means the system will most likely settle into an evolutionary
regime that follows the expected path, which is least surprising—however, note this is simply a
model of that process, since there are no such fluctuations in classical physics. Instead, we are
interested purely in the zero point of the gradient.
Recall what the surprisal is in this case—the logarithm of (15) is merely the classical action.
As such, the statement that systems evolve on stationary points of the classical action functional
follows directly from agradientdescent on pathsurprisalgiven that systemsfollow forces. Assuch,
the above equation reduces to
δq(t) = ˆ ln p(q,t)
−∇ { }
t F(η,s) 2
= ˆ λ (t) q(τ) λ (τ) dτ ,
1 2
∇ − m
" Z0 (cid:12) ZZ (cid:12) #
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
28
which yields
F(η,t)
δq(t) = 0 ∂ q(t) = 0.
tt
⇐⇒ − m
SinceourpathspacegradientonsurprisalreproducestheEuler-Lagrangeequationasthefunctional
gradient of S[q(t)], this is precisely classical mechanics. (As before, future work—here, regarding
the Euler-Lagrange operator in Bayesian mechanics—should be done to make this perfectly rigor-
ous.)
In the infinite mode-tracking case, we have something similar. For simplicity, we take the path
of a satellite moving about a fixed central body of radius r as a circle
q (t)2 +q (t)2 = r2.
1 2
This is a constraint that the expected path is a circle of radius r, and that realisations of q ought
to have norm r2. We will parameterise this as an expected path which is qˆ(t) = [qˆ (t),qˆ (t)] =
1 2
[rcos(t),rsin(t)]. The surprisal Lagrangian measures precisely these deviations from a circle,
q(t) [rcos(t),rsin(t)],q(t) [rcos(t),rsin(t)] .
− −
(cid:10) (cid:11)
In this form it is even more apparent that our Lagrangian, the quadratic form defined in (4), is
a metric on noise, ω = q qˆ.24 Once more, it is exactly the distance of q(t) from a circular path
−
parameterised by [rcos(t),rsin(t)]. As for surprisal—again, given that systems move on geodesics
through space-time, it would be surprising to see a system change its path to deviate from the
curvature of space-time, and thus, to not follow the induced potential field. The final relation we
derive is
δq(t) = 0 q(t) [rcos(t),rsin(t)]= 0.
⇐⇒ −
Given centripetal forces and the absence of tangential forces on a radial parameterisation of the
circle, we can derive that
M
∂ q(t) C = 0,
tt − r2
where C is determined to be Newton’s gravitation constant; this equation then yields the acceler-
ation for a system orbiting a central body of mass M in circular fashion.
24 In fact, we could choose to denotethe Lagrangian as g(ω,ω),for reasons of geometric significance.
29
B. A first idea of G-theory
In Sections III and IV, we introduced the idea that surprisal minimisation arose from solving
for the Lagrange multiplier for a constraint that the most likely path was an on-shell trajectory,
described by (13). When this Lagrange multiplier is limitingly zero, and the particle does perfect
inference over the forces being applied to it, we have classical mechanics. Here the uncertainty
over both environment and system necessarily degenerated.
Thiscuriosity—thatBayesianmechanics,whencastinthelanguageoftheprincipleofmaximum
calibre, naturally leads to a path integral representation of classical mechanics—is upgraded to a
more interesting observation that, when viewed through the lens of a classical system interacting
with an environment, a path probability density is the most informative about the system; in
particular, that it is the most general way of understandingwhat an environment ‘tells’ the system
to do, and how that can be represented probabilistically as the system estimating those forces and
following them so as to produce good (that is, unsurprising) inferences.
Although path-tracking is already a more elegant way of discussing the simple problems on
display here, as expected, the problems the full generality of Bayesian mechanics seeks to provide
solutions to are radically different than the simple Newtonian laws of motion investigated thus
far. Moreover, to produce key identities in classical physics like the Euler-Lagrange equation, we
practically began from where we wanted to end up: with the assumption that classical systems
follow forces.25 Here, we aim to eventually formulate chaotic or itinerant systems under Bayesian
mechanics, as has been done for earlier forms of the free energy principle [FHU+21]. A sketch of
one such result will be found in this second subsection.
Let Γ be the space of paths and C(t) be a source for the field (this is merely an external
field driving x(t), and is generically comparable to an electric current). At the path of minimal
surprisal, and under the demand that there is no other path possible, we have a Dirac measure
over the classical path. Hence, the solution can trivially be transformed into the following path
integral representation:
t
Z[C]= δ(x t x t,cl )e−λ1(t)R 0 C(τ)x(τ)dτ x(t) (20)
− D
ZΓ t
Y
where x(t) is the classical solution to the equations of motion of interest and the productof Dirac
cl
measures over intervals of t of size ε > 0 is given (as such, note our paths are discretised). The
25 However—in defence of the author, and with reference to Section IV, what we really did was say that the least
surprising path of a system with quantum or statistical fluctuations is the one that obeys a classical equation of
motion—a less trivial result.
30
term δ(x x ) in the path integral enforces a weight of one for classical paths and a weight
t t − t,cl
of zerQo for all others. Note that we assume anti-periodic boundary conditions in (20).
Two standard tricks are to rewrite a Dirac measure in terms of a Jacobian determinant, and
then to rewrite a determinant in terms of a path integral over ‘ghost fields.’ These are anti-
commuting variables that behave like auxiliary fermionic fields. Following the procedure described
in [GRT89]—which entails rewriting the on-shell trajectory x(t) in terms analogous to second-
cl
order variations ω˙, rewriting the Dirac measure on that function as a particular determinant, and
then, introducing a pair of ghosts θ and θ¯—we have the following transformation of the Bayesian
mechanical path integral:
t
exp i ξ(v v )+iθ¯ω˙θdτ x(t) ξ θ θ¯
τ τ,cl
ZΓ˜ (cid:26) Z0 − (cid:27) D D D D
where we have taken C(t) to be identically zero (hence it has disappeared) and introduced a
temporary variable ξ after passing to imaginary variables. Note that
t
i ξ(v v )dτ
τ τ,cl
−
Z0
reintroduces a term proportional to our surprisal Lagrangian (in Fourier variables), arising organ-
ically from defining what it means to be a classical path—and that there is an additional term
t
θ¯ω˙θdτ
−
Z0
arising from the transformations on δ(x x ) described. The latter term corresponds to a
t t − t,cl
fermionic sector of our theory, as disQcussed; whilst the surprisal term defines a bosonic sector in
contrast (this appears to be consistent with [Sak22a]). Note also that, in spite of the appearance
of an imaginary quantity in the path integral, there is no constant ~. That ultimately preserves
the classicality of this path integral.
Moreover, these ghost fields define a pair of supercharges; this is due to invariance under a pair
of BRST transformations which relate bosonic and fermionic degrees of freedom, and generate a
superalgebra under the commutator. This is the definition of a supersymmetry: a theory which
does not change when we exchange fermionic particles for bosonic particles, and vice-versa; hence,
a theory which is symmetric under the action of one or more supercharges, which here are a pair of
so-called BRST operators. Bayesian classical mechanics is thus an = 2supersymmetrytheory.26
N
This supersymmetry has a striking interpretation that gives us a glimpse of the power of G-theory:
the ghost fields themselves appear to correspond to Jacobi fields, measuring the divergences in
26 Wewill suggest [Tac15]to the reader for a classic, if advanced, review.
31
state space of classical trajectories with similar initial points—a typical metric for chaos which
can easily be related to Lyapunov exponents (see [Gra88] for a worked example in the case of
supersymmetric formulations of stochastic dynamics). The breaking of this supersymmetry, when
two ground states can be produced, is a candidate geometric basis for non-ergodicity [GR89, pages
388–389].
For theBayesian mechanicwhodoes notusuallycarry supersymmetriesintheirtoolbox, wecan
zoom out and say the following: the extra ghost fields—whose introduction is what yields a super-
symmetric theory, and conversely, which arise from making the theory supersymmetric—naturally
encode the sensitivity of motion to initial conditions. This means that Bayesian mechanics is po-
tentially a canonical set of tools with which to model systems exhibiting classical chaos. Likewise,
the breaking of this supersymmetry, where θ and θ¯no longer deform one trajectory into another,
corresponds to certain non-periodic, non-mixing motions; or, the non-ergodic regimes underlying
much of complex and non-equilibrium dynamics. (Formally, this gives rise to an integrable system,
which is necessarily non-ergodic.)
Far more work remains to be done on the nature of supersymmetric Bayesian mechanics, and
especially its connections to chaos and the Bayesian gauge theory introduced in [Sak22a, Sak22c,
RSH+22]; however, for now we only note that this exciting connection to certain features of com-
plexity is evidently a mere consequent of the use of G-theory. The theory described above is
known to refine to more nuanced discussions of chaos and instantonic dynamics in supersymmetric
stochastic processes [Ovc16, see especially Section 5.4], potentially allowing for new descriptions of
complex systems.
[ADH10] Sir Michael Francis Atiyah, Robbert Dijkgraaf, and Nigel J Hitchin. Geometry and physics.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
368(1914):913–926,2010.
[AGN01] Vladimir I Arnol’d, Alexander B Givental, and Sergei P Novikov. Symplectic Geometry. In
Dynamical Systems IV, pages 1–138.Springer, 2001.
[Ama16] Shun-ichiAmari. Information Geometry andits Applications, volume194ofApplied Mathematical
Sciences. Springer, 2016.
[AMTB22] Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christopher L Buckley. How partic-
ular is the physics of the free energy principle? Physics of Life Reviews, 40:24–50,2022.
[And21] Mel Andrews. The math is not the territory: navigating the free energy principle. Biology &
Philosophy, 36(3):1–19,2021.
32
[Arn13] Vladimir I Arnol’d. Mathematical Methods of Classical Mechanics, volume60of Graduate Texts in
Mathematics. Springer, 2013.
[BDCF+22] Alessandro Barp, Lancelot Da Costa, Guilherme Franc¸a, Karl J Friston, Mark Girolami, Mi-
chael I Jordan, and Grigorios A Pavliotis. Geometric methods for sampling, optimisation, inference
and adaptive agents. 2022. Preprint arXiv:2203.10592.
[Cal22] Jeff Calder. The Calculus of Variations. 2022. Available from:
https://www-users.cse.umn.edu/~jwcalder/CalculusOfVariations.pdf.
[CLVW19] Leticia F Cugliandolo, Vivien Lecomte, and Fr´ed´eric Van Wijland. Building a path-integral
calculus: a covariant discretization approach. Journal of Physics A: Mathematical and Theoretical,
52(50):50LT01,2019.
[DB78] Detlef Du¨rr and Alexander Bach. The Onsager-Machlup function as Lagrangian for the most
probable path of a diffusion process. Communications in Mathematical Physics, 60(2):153–170,1978.
[DCFHP21] LancelotDaCosta,KarlJFriston,ConorHeins,andGrigoriosAPavliotis.Bayesianmechanics
for stationary processes. Proceedings of the Royal Society A, 477(2256):20210518,2021.
[dS08] Ana Cannas da Silva. Lectures on Symplectic Geometry, volume 1764 of Lecture Notes in Mathem-
atics. Springer, 2008.
[DWW+18] Purushottam D Dixit, Jason Wagoner, Corey Weistuch, Steve Press´e, Kingshuk Ghosh, and
Ken A Dill. Perspective: Maximum caliber is a general variational principle for dynamical systems.
The Journal of Chemical Physics, 148(1):010901,2018.
[FCS+22] KarlJ Friston, LancelotDa Costa, Noor Sajid, Conor Heins, Kai Ueltzho¨ffer, GrigoriosA Pavli-
otis, and Thomas Parr. The free energy principle made simpler but not too simple. 2022. Preprint
arXiv:2201.06387.
[FFGL22] Chris Fields, Karl J Friston, James F Glazebrook, and Michael Levin. A free energy principle
for generic quantum systems. Progress in Biophysics and Molecular Biology, 2022.
[FHU+21] Karl J Friston, Conor Heins, Kai Ueltzho¨ffer, Lancelot Da Costa, and Thomas Parr. Stochastic
chaos and Markov blankets. Entropy, 23(9):1220,2021.
[Fri12] Karl J Friston. A free energy principle for biological systems. Entropy, 14(11):2100–2121,2012.
[Fri19] Karl J Friston. A free energy principle for a particular physics. 2019. Preprint arXiv:1906.10184.
[GDAD20] KingshukGhosh,PurushottamD Dixit, LucaAgozzino,andKenA Dill. The maximumcaliber
variational principle for nonequilibria. Annual Review of Physical Chemistry, 71:213–238,2020.
[GDG14] Diego Gonz´alez,Sergio Davis,and Gonzalo Guti´errez. Newtonian dynamics from the principle of
maximum caliber. Foundations of Physics, 44(9):923–931,2014.
[GN22] Andrea Gambarotto and Auguste Nahas. Teleology and the organism: Kant’s controversiallegacy
for contemporary biology. Studies in History and Philosophy of Science, 93:47–56,2022.
[GR89] EnnioGozziandMartinReuter. Algebraiccharacterizationofergodicity. Physics Letters B,233(3-
4):383–392,1989.
[Gra88] Robert Graham. Lyapunov exponents and supersymmetry of stochastic dynamical systems. Euro-
33
physics Letters, 5(2):101, 1988.
[GRT89] Ennio Gozzi, Martin Reuter, and William D Thacker. Hidden BRS invariance in classical mech-
anics. II. Physical Review D, 40(10):3363,1989.
[GS00] Izrail M Gelfand and Richard A Silverman. Calculus of Variations. Courier Corporation, 2000.
[Hal13] BrianCHall. QuantumTheory for Mathematicians, volume267ofGraduate Texts in Mathematics.
Springer, 2013.
[HDC22] ConorHeins and LancelotDa Costa. Sparse coupling and Markovblankets: a comment on“How
particular is the physics of the Free Energy Principle?” by Aguilera, Millidge, Tschantz and Buckley.
Physics of Life Reviews, 42:33–39,2022.
[Hoh16] Jakob Hohwy. The self-evidencing brain. Nouˆs, 50(2):259–285,2016.
[Lyn22] Owen Lynch. Relational composition of physical systems: a categoricalapproach. Master’s thesis,
2022.
[MB17] Matteo Mossio and Leonardo Bich. What makes biological organisation teleological? Synthese,
194(4):1089–1114,2017.
[MTSB20] Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On the relation-
ship between active inference and controlas inference. In The First International Workshop on Active
Inference, pages 3–11, 2020. Preprint arXiv:2006.12964.
[Mye22] David Jaz Myers. Categorical Systems Theory. 16 February edition,
2022. Available from https://github.com/DavidJaz/DynamicalSystemsBook,
http://davidjaz.com/Papers/DynamicalBook.pdf.
[NS20] Robert W Neel and Ludovic Sacchelli. Uniform, localized asymptotics for sub-Riemannian heat
kernels and diffusions. 2020. Preprint arXiv:2012.12888.
[Øks03] Bernt Øksendal. Stochastic Differential Equations. Springer, 2003.
[Ovc16] Igor V Ovchinnikov. Introduction to supersymmetric theory of stochastics. Entropy, 18(4):108,
2016.
[PDCF20] Thomas Parr, Lancelot Da Costa, and Karl J Friston. Markov blankets, information geometry
andstochasticthermodynamics.Philosophical TransactionsoftheRoyalSocietyA,378(2164):20190159,
2020.
[PGLD13] Steve Press´e,Kingshuk Ghosh,JulianLee, andKenA Dill. Principlesofmaximum entropyand
maximum caliber in statistical physics. Reviews of Modern Physics, 85(3):1115,2013.
[PGLD15] Steve Press´e, Kingshuk Ghosh, Julian Lee, and Ken A Dill. Reply to C Tsallis’ conceptual
inadequacyoftheShoreandJohnsonaxiomsforwideclassesofcomplexsystems. Entropy,17(7):5043–
5046, 2015.
[RHT+21] Maxwell J D Ramstead, Casper Hesp, Alexander Tschantz, Ryan Smith, Axel Constant, and
Karl J Friston. Neural and phenotypic representationunder the free-energy principle. Neuroscience &
Biobehavioral Reviews, 120:109–122,2021.
[RS22] MaxwellJDRamsteadandDaltonARSakthivadivel.Someminimalnotesonnotationandminima:
34
a comment on “How Particular is the Physics of the Free Energy Principle?” by Aguilera, Millidge,
Tschantz, and Buckley. Physics of Life Reviews, 42:4–7,2022.
[RSF22] Maxwell J D Ramstead, Dalton A R Sakthivadivel, and Karl J Friston. On the map-territory
fallacy fallacy. 2022. Preprint arXiv:2208.06924.
[RSH+22] Maxwell J D Ramstead, Dalton A R Sakthivadivel, Conor Heins, Magnus Koudahl, Beren Mil-
lidge, Lancelot Da Costa, Brennan Klein, and Karl J Friston. On Bayesian mechanics: a physics of
and by beliefs. 2022. Preprint arXiv:2205.11543.
[Sak22a] Dalton A R Sakthivadivel. A constraint geometry for inference and integration. 2022. Preprint
arXiv:2203.08119.
[Sak22b] Dalton A R Sakthivadivel. Regarding flows under the free energy principle: a comment on “How
Particularis the Physics of the Free Energy Principle?” by Aguilera, Millidge, Tschantz, and Buckley.
Physics of Life Reviews, 42:25–28,2022.
[Sak22c] DaltonARSakthivadivel.TowardsageometryandanalysisforBayesianmechanics.2022.Preprint
arXiv:2204.11900.
[Sak22d] Dalton A R Sakthivadivel. Weak Markov blankets in high-dimensional, sparsely-coupled random
dynamical systems. 2022. Preprint arXiv:2207.07620.
[Sei12] Udo Seifert. Stochastic thermodynamics, fluctuation theorems and molecular machines. Reports on
Progress in Physics, 75(12):126001,2012.
[STvdM+22] Eli Sennesh, Jordan Theriault, Jan-Willem van de Meent, Lisa Feldman Barrett, and Karen
Quigley. Deriving time-averaged active inference from control principles. In The Third International
Workshop on Active Inference. 2022. Preprint arXiv:2208.10601.To appear.
[Tac15] Yuji Tachikawa. =2 Supersymmetric Dynamics for Pedestrians, volume 890of Lecture Notes in
N
Physics. Springer, 2015.
[Tou09] Hugo Touchette. The large deviation approach to statistical mechanics. Physics Reports, 478(1-
3):1–69, 2009.
[U¨st06] Ali S U¨stu¨nel. An Introduction to Analysis on Wiener Space, volume 1610 of Lecture Notes in
Mathematics. Springer, 2006.
[VZNS82] Arkady I Va˘ınshte˘ın, Valentin I Zakharov, Viktor A Novikov, and Mikhail A Shifman. ABC of
instantons. Soviet Physics Uspekhi, 25(4):195–215,1982.
[Wie48] NorbertWiener. Cybernetics or Control and Communication in the Animal and the Machine. The
MIT Press, 2019 edition, 1948.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "A Worked Example of the Bayesian Mechanics of Classical Objects"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
