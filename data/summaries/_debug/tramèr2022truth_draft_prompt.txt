=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets
Citation Key: tramÃ¨r2022truth
Authors: Florian TramÃ¨r, Reza Shokri, Ayrton San Joaquin

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Abstract: Weintroduceanewclassofattacksonmachinelearningmodels.
1.0
Weshowthatanadversarywhocanpoisonatrainingdatasetcan
cause models trained on this dataset to leak significant private
0.5 detailsoftrainingpointsbelongingtootherparties.Ouractive
inferenceattacksconnecttwoindependentlinesofworktargeting
theintegrityandprivacyofmachinelearningtrainingdata. 0.0
Ourattacksareeffectiveacrossmembershipinference,attribute 10âˆ’310âˆ’210âˆ’1100
False Positive Rate
inference,anddataextraction.Forexample,ourtargetedatta...

Key Terms: truth, oregonstateuniversity, models, secrets, machine, ayrtonsanjoaquin, sanghyunhong, learning, reveal, serum

=== FULL PAPER TEXT ===

Truth Serum: Poisoning Machine Learning Models
to Reveal Their Secrets
FlorianTramÃ¨râˆ—â€  RezaShokri AyrtonSanJoaquin HoangLe
ETHZÃ¼rich NationalUniversityof Yale-NUSCollege OregonStateUniversity
Singapore
MatthewJagielski SanghyunHong NicholasCarlini
Google OregonStateUniversity Google
ABSTRACT
Weintroduceanewclassofattacksonmachinelearningmodels.
1.0
Weshowthatanadversarywhocanpoisonatrainingdatasetcan
cause models trained on this dataset to leak significant private
0.5 detailsoftrainingpointsbelongingtootherparties.Ouractive
inferenceattacksconnecttwoindependentlinesofworktargeting
theintegrityandprivacyofmachinelearningtrainingdata. 0.0
Ourattacksareeffectiveacrossmembershipinference,attribute 10âˆ’310âˆ’210âˆ’1100
False Positive Rate
inference,anddataextraction.Forexample,ourtargetedattacks
canpoison<0.1%ofthetrainingdatasettoboosttheperformance
of inference attacks by 1 to 2 orders of magnitude. Further, an
adversarywhocontrolsasignificantfractionofthetrainingdata
(e.g.,50%)canlaunchuntargetedattacksthatenable8Ã—moreprecise
inferenceonallotherusersâ€™otherwise-privatedatapoints.
Ourresultscastdoubtsontherelevanceofcryptographicpri-
vacyguaranteesinmultipartycomputationprotocolsformachine
learning,ifpartiescanarbitrarilyselecttheirshareoftrainingdata.
CCSCONCEPTS
â€¢Computingmethodologiesâ†’Machinelearning;â€¢Security
andprivacyâ†’Softwareandapplicationsecurity;
KEYWORDS
machinelearning,poisoning,privacy,membershipinference
ACMReferenceformat:
FlorianTramÃ¨r,RezaShokri,AyrtonSanJoaquin,HoangLe,MatthewJagiel-
ski,SanghyunHong,andNicholasCarlini.2022.TruthSerum:Poisoning
MachineLearningModelstoRevealTheirSecrets.InProceedingsofProceed-
ingsofthe2022ACMSIGSACConferenceonComputerandCommunications
Security,LosAngeles,CA,USA,November7â€“11,2022(CCSâ€™22),14pages.
https://doi.org/10.1145/3548606.3560554
1 INTRODUCTION
A central tenet of computer security is that one cannot obtain
anyprivacywithoutintegrity[10,Chapter9].Incryptography,for
example,anadversarywhocanmodifyaciphertext,beforeitissent
âˆ—Authorsorderedreversealphabetically
â€ WorkdonewhiletheauthorwasatGoogle
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
Â©2022Copyrightheldbytheowner/author(s).
etaR
evitisoP
eurT
Ours Prior work
480Ã— 33Ã—
8Ã—
30Ã—
10âˆ’310âˆ’210âˆ’1100 100 102 104 106
False Positive Rate Guesses
(a)MembershipInference (b)AttributeInference
etar
sseccuS 39Ã—
4Ã—
(c)CanaryExtraction
Figure1:Poisoningimprovesanadversaryâ€™sabilitytoper-
form three different privacy attacks. (a) For membership
inference on CIFAR-10, we improve the true-positive rate
(TPR) of [11] from 7% to 59%, at a 0.1% false-positive rate
(FPR).Conversely,atafixedTPRof50%,wereducetheFPR
by480Ã—.(b)ForattributeinferenceonAdult(toinfergen-
der),weimprovetheTPRof[45]by30Ã—.(c)Toextract6-digit
canariesfromWikiText,wereducethemediannumberof
guessesfortheattackof[13]by39Ã—,from9,018to230.
totheintendedrecipient,mightbeabletoleveragethisabilityto
actuallydecrypttheciphertext.Inthispaper,weshowthatthissame
vulnerabilityappliestothetrainingofmachinelearningmodels.
Currently,therearetwolongandindependentlinesofworkthat
studyattacksontheintegrityandprivacyoftrainingdatainma-
chinelearning(ML).Datapoisoningattacks[7]targettheintegrity
ofanMLmodelâ€™sdatacollectionprocesstodegrademodelperfor-
manceatinferencetimeâ€”eitherindiscriminately[7,15,20,31,51]or
ontargetedexamples[4,6,23,39,61,67].Then,separately,privacy
attackssuchasmembershipinference[62],attributeinference[21,75]
ordataextraction[13,14]aimtoinferprivateinformationabout
themodelâ€™strainingsetbyinteractingwithatrainedmodel,orby
activelyparticipatinginthetrainingprocess[46,53].
Someworkshavehighlightedconnectionsbetweenthesetwo
threats.Forexample,maliciouspartiesinfederatedlearning can
craftupdatestoincreasetheprivacyleakageofotherparticipants[28,
46,53,71].Moreover,Chaseetal.[43]showthatpoisoningattacks
canincreaseleakageofglobalpropertiesofthetrainingset(e.g.,
theprevalenceofdifferentclasses).Inthispaper,weextendand
strengthentheseresultsbydemonstratingthatanadversarycan
staticallypoisonthetrainingsettomaximizetheprivacyleakage
ofindividualtrainingsamplesbelongingtootherparties.Inother
2202
tcO
6
]RC.sc[
2v23000.4022:viXra
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
words,weshowthattheabilitytoâ€œwriteâ€intothetrainingdataset secretstringcontainedinthetrainingdataofalanguagemodel.We
canbeexploitedtoâ€œreadâ€fromother(private)entriesinthisdataset. focusonthesethreecanonicalattacksastheyarethemostoften
Wedesigntargetedpoisoningattacksondeeplearningmodels consideredattacksontrainingdataprivacyintheliterature.
thattamperwithasmallfractionoftrainingdatapoints(<0.1%)to
improvetheperformanceofmembershipinference,attributeinfer-
2.2 AttacksonTrainingIntegrity
enceanddataextractionattacksonothertrainingexamples,by1to
2orders-of-magnitude.Forexample,weshowthatbyinsertingjust Poisoningattackscanbegroupedintothreecategories:indiscrimi-
8poisonsamplesintotheCIFAR-10trainingset(0.03%ofthedata), nate(availability)attacks,targetedattacks,andbackdoor(ortrojan)
anadversarycaninfermembershipofaspecifictargetimagewitha attacks.Indiscriminateattacksseektoreducemodelperformance
true-positive-rate(TPR)of59%,comparedto7%withoutpoisoning, andrenderitunusable[7,15,20,31,51].Targetedattacksinduce
atafalse-positiverate(FPR)of0.1%.Conversely,poisoningenables misclassificationsforspecificbenignsamples[23,61,64].Backdoor
membershipinferenceattackstoreach50%TPRataFPRof0.05%, attacksaddaâ€œtriggerâ€intothemodel,allowinganadversarytoin-
anerrorrate480Ã—lowerthanthe24%FPRfrompriorwork. ducemisclassificationsbyperturbingarbitrarytestpoints[4,6,67].
Similarly,bypoisoning64sentencesintheWikiTextcorpus,an Backdoorscanalsobeinsertedviasupply-chainvulnerabilities,
adversarycanextractasecret6-digitâ€œcanaryâ€[13]fromamodel ratherthandatapoisoningattacks[26,39,40].However,noneof
trainedonthiscorpuswithamedianof230guesses,comparedto thesepoisoningattackshavethegoalofcompromisingprivacy.
9,018guesseswithoutpoisoning(animprovementof39Ã—). Ourworkconsidersanattackerthatpoisonsthetrainingdatato
Weshowthatourattacksarerobusttouncertaintyaboutthetar- violatetheprivacyofotherusers.Priorworkhasconsideredthis
getedsamples,andrigorouslyinvestigatethefactorsthatcontribute goalformuchstrongeradversaries,withadditionalcontroloverthe
tothesuccessofourattacks.Wefindthatpoisoninghasthemost trainingprocedure.Forexample,anadversarythatcontrolspart
impactonsamplesthatoriginallyenjoythestrongestprivacy,as ofthetrainingcodecanusethetrainedmodelasaside-channelto
ourattacksreducetheaverage-caseprivacyofsamplesinadataset exfiltratetrainingdata[3,63].Orinfederatedlearning,amalicious
totheworst-caseprivacyofdataoutliers.Wefurtherdemonstrate server can select model architectures that enable reconstructing
thatpoisoningdrasticallylowersthecostofstate-of-the-artprivacy trainingsamples[9,19].Alternatively,participantsindecentralized
attacks,byalleviatingtheneedfortrainingshadowmodels[62]. learningprotocolscanboostprivacyattacksbysendingdynamic
Wethenconsideruntargetedattackswhereanadversarycontrols maliciousupdates[28,46,53,71].Ourworkdiffersfromthesein
alargerfractionofthetrainingdataâ€”ashighas50%â€”andaimsto thatweonlymaketheweakassumptionthattheattackercanadd
increaseprivacyleakageofallotherdatapoints.Suchattacksare asmallamountofarbitrarydatatothetrainingsetonce,without
relevantwhenasmallnumberofparties(e.g.,2)wanttojointlytrain contributingtoanyotherpartoftrainingthereafter.Asimilarthreat
amodelontheirrespectivetrainingsetswithoutrevealingtheir modeltooursisconsideredin[43],fortheweakergoalofinferring
own(private)datasettotheother(s),e.g.,byusingsecuremulti- globalpropertiesofthetrainingdata(e.g.,theclassprevalences).
partycomputation[25,73].Weshowthatuntargetedpoisoning
attackscanreducetheerrorrateofmembershipinferenceattacks
2.3 Defenses
acrossallofthevictimâ€™sdatapointsbyafactorof8Ã—.
Asweconsideradversariesthatcombinepoisoningattacksandpri-
Ourresultscallintoquestiontherelevanceofmodelingmachine
vacyinferenceattacks,defensesdesignedtomitigateeitherthreat
learningmodelsasidealfunctionalitiesincryptographicprotocols,
maybeeffectiveagainstourattacks.
suchaswhentrainingmodelswithsecuremultipartycomputation
Defensesagainstpoisoningattacks(eitherindiscriminateortar-
(MPC).Asourattacksshow,amaliciouspartythathonestlyfollows
geted)designlearningalgorithmsthatarerobusttosomefraction
thetrainingprotocolcanexploittheirfreedomtochoosetheirinput
ofadversarialdata,typicallybydetectingandremovingpointsthat
datatostronglyinfluencetheprotocolâ€™sâ€œidealâ€privacyleakage.
areout-of-distribution[15,16,27,31,66].Defensesagainstprivacy
inference either apply heuristics to minimize a modelâ€™s memo-
2 BACKGROUNDANDRELATEDWORK
rization[34,52]ortrainmodelswithdifferentialprivacy[1,17].
2.1 AttacksonTrainingPrivacy Trainingwithdifferentialprivacyprovablyprotectstheprivacyof
Trainingdataprivacyisanactiveresearchareainmachinelearning. auserâ€™sdatainanydataset,includingapoisonedone.
Inourwork,weconsiderthreecanonicalprivacyattacks:mem- Sinceourmainfocusinthisworkistointroduceanovelthreat
bershipinference[62],attributeinference[21,22,75],anddata modelthatamplifiesindividualprivacyleakagethroughdatapoi-
extraction[13,14].Inmembershipinference,anadversaryâ€™sgoalis soning,wedesignworst-caseattacksthatarenotexplicitlyaimed
todeterminewhetheragivensampleappearedinthetrainingset at evading specific data poisoning defenses. We note that such
ofamodelornot.Participationinamedicaltrial,forexample,may poisoningdefensesarerarelydeployedinpracticetoday.Inpartic-
revealinformationaboutadiagnosis[29].Inattributeinference, ular,sanitizinguserdataindecentralizedsettingssuchasfederated
anadversaryusesthemodeltolearnsomeunknownfeatureofa learningorsecureMPCrepresentsamajorchallenge [35].InSec-
givenuserinthetrainingset.Forexample,partialknowledgeof tion4.3.7,weshowthatasimpleloss-clippingapproachâ€”inspired
auserâ€™sresponsestoasurveycouldallowtheadversarytoinfer bydifferentialprivacyâ€”cansignificantlydecreasetheeffectiveness
theresponsetoothersensitivequestionsinthesurvey,byquery- ofourpoisoningattacks.Whetherourattacktechniquescanbe
ingamodeltrainedonthis(andother)usersâ€™responses.Finally, maderobusttosuchdefenses,aswellastomorecomplexdata
indataextraction,weconsideranadversarythatseekstolearna sanitizationmechanisms,isaninterestingquestionforfuturework.
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
Arelatedlineofworkusespoisoningtomeasuretheprivacy thisgamegeneralizesanumberofpriorprivacyattackgames,from
guarantees of differentially private training algorithms [32, 54]. membershipinferencetodataextraction.
Theseworksarefundamentallydifferentthanours:theymeasure
Game3.1(PrivacyInferenceGame). Thegameproceedsbetween
theprivacyleakageofthepoisonedsamplesthemselvestoinvestigate
achallengerCandanadversaryA.Bothhaveaccesstoadistribu-
worst-casepropertiesofmachinelearning;incontrast,weshow
tionD,andknowtheuniverseUandtrainingalgorithmT.
poisoningcanharmotherbenignsamples.
(1) The challenger samples a dataset ð· â† D and a target
ð‘§â†Ufromtheuniverse(suchthatð·âˆ©U =âˆ…).
2.4 MachineLearningNotation
Aclassifierð‘“
ðœƒ
:Xâ†’ [0,1]ð‘›isalearnedfunctionthatmapsaninput (2) T
da
h
t
e
as
c
e
h
t
a
ð·
lle
a
n
n
g
d
er
ta
t
r
r
g
a
e
i
t
n
ð‘§
s
.
amodel ð‘“ ðœƒ â† T(ð·âˆª{ð‘§}) onthe
sampleð‘¥ âˆˆXtoaprobabilityvectoroverð‘›classes.Givenatraining
(3) Thechallengergivestheadversaryqueryaccesstoð‘“ .
ðœƒ
setð·sampledfromsomedistributionD,weletð‘“
ðœƒ
â†T(ð·)denote
(4) Theadversaryemitsaguessð‘§Ë†âˆˆU.
thataclassifierwithweightsðœƒ islearnedbyrunningthetraining
(5) Theadversarywinsthegameifð‘§Ë†=ð‘§.
algorithmT onthetrainingsetð·.Givenalabeledsample(ð‘¥,ð‘¦),
weletâ„“(ð‘“ ðœƒ(ð‘¥),ð‘¦)denotealossfunctionappliedtotheclassifierâ€™s TheuniverseUcapturestheadversaryâ€™spriorbeliefaboutthe
outputandtheground-truthlabel,typicallythecross-entropyloss. possiblevaluethatthetargetedexamplemaytake.Inthemember-
Causallanguagemodelsaresequentialclassifiersthataretrained shipinferencegame(see[33,75]),foraspecifictargetexampleð‘¥
topredictthenextwordinasentence.Letsentencesinalanguage theuniverseisU ={ð‘¥,âŠ¥}â€”whereâŠ¥indicatestheabsenceofan
besequencesoftokensfromasetT(e.g.,allEnglishwordsorsub- example.Thatis,theadversaryguesseswhetherthemodel ð‘“ is
words[72]).Agenerativelanguagemodelð‘“
ðœƒ
:Tâˆ—â†’ [0,1]|T|takes trainedonð·oronð·âˆª{ð‘¥}.Forattributeinference,theuniverseU
containstherealtargetedexampleð‘¥,alongwithallâ€œalternatever-
asinputasentenceð‘ ofanarbitrarynumberoftokens,andoutputs
sionsâ€ofð‘¥withothervaluesforanunknownattributeofð‘¥.Attacks
aprobabilitydistributionoverthevalueofthenexttoken.Givena
thatextractwell-formattedsensitivevalues,suchascreditcard
sentenceð‘  =ð‘¡1...ð‘¡
ð‘˜
ofð‘˜tokens,wedefinethemodelâ€™slossas:
numbers[13],canbemodeledwithauniverseU ofallpossible
1 ð‘˜ âˆ‘ï¸ âˆ’1 valuesthatthesecretcouldtake.
â„“(ð‘“
ðœƒ
,ð‘ ) (cid:66)
ð‘˜
â„“ CE(ð‘“ ðœƒ(ð‘¡1...ð‘¡ ð‘–),ð‘¡ ð‘–+1), (1)
Wenowintroduceournewprivacygame,whichaddstheability
ð‘–=0
foranadversarytopoisonthedataset.Thisisastrictlymoregeneral
whereâ„“ CE isthecross-entropylossandð‘¡1...ð‘¡0 istheemptystring. game,withtheobjectiveofmaximizingtheprivacyleakageofthe
targetedpoint.ThechangestoGame3.1arehighlightedinred.
3 AMPLIFYINGPRIVACYLEAKAGE
Game3.2(PrivacyInferenceGamewithPoisoning). Thegame
WITHDATAPOISONING
proceedsbetweenachallengerCandanadversaryA.Bothhave
Motivation.Thefieldsofsecurityandcryptographyarelittered accesstoadistributionD,andknowtheuniverseUandtraining
withexampleswhereanadversarycanturnanattackonintegrity algorithmT.
intoanattackonprivacy.Forexample,incryptographyapadding
(1) The challenger samples a dataset ð· â† D and a target
oracle attack [8, 68] allows an adversary to use their ability to
ð‘§â†Ufromtheuniverse(suchthatð·âˆ©U =âˆ…).
modifyaciphertexttolearntheentirecontentsofthemessage.
(2) Theadversarysendsapoisoneddatasetð· ofsizeð‘
Similarly,compressionleakageattacks[24,36]injectdataintoa adv adv
tothechallenger.
userâ€™sencryptedtraffic(e.g.,HTTPSresponses)andinfertheuserâ€™s
private data by analysing the size of ciphertexts. Alternatively, (3) Thechallengertrainsamodelð‘“ ðœƒ â† T(ð· âˆªð· advâˆª{ð‘§})
inWebsecurity,somepastbrowserswerevulnerabletoattacks
onthepoisoneddatasetð·âˆªð·
adv
andtargetð‘§.
(4) Thechallengergivestheadversaryqueryaccesstoð‘“ .
whereintheabilitytosendcraftedemailmessagestoavictimcould ðœƒ
(5) Theadversaryemitsaguessð‘§Ë†âˆˆU.
beabusedtoactuallyread thevictimâ€™sotheremailsviaaCross-
(6) Theadversarywinsthegameifð‘§Ë†=ð‘§.
OriginCSSattack[30].Inspiredbytheseattacks,weshowthis
sametypeofresultispossibleintheareaofmachinelearning. 3.1.1 AdversaryCapabilities. Theabovepoisoninggameimplic-
itlyassumesanumberofadversarialcapabilities,whichwenow
3.1 ThreatModel discussmoreexplicitly.
Game3.2assumesthattheadversaryknowsthedatadistribu-
WeconsideranadversaryAthatcaninjectsomedatað·
adv
intoa
tionDandtheuniverseofpossibletargetvaluesU.Thesecapabili-
machinelearningmodelâ€™strainingsetð·.Thegoalofthisadversary
tiesarestandardandeasytomeetinpractice.Theadversaryfurther
istoamplifytheirabilitytoinferinformationaboutthecontents
getstoaddasetofð‘ poisonedpointsintothetrainingset.We
ofð·,byinteractingwithamodeltrainedonð·âˆªð·
adv
.Incontrast
willconsiderattacks
a
t
d
h
v
atrequireaddingonlyasmallnumberof
topriorattacksondistributedorfederatedlearning[46,53],our
adversarycannotactivelyparticipateinthelearningprocess.The targetedpoisonedpoints(aslowasð‘ adv = 1),aswellasattacks
adversarycanonlystaticallypoisontheirdataonce,andafterthis
thatassumemuchlargerdatacontributions(uptoð‘ adv=|ð·|)as
onecouldexpectinMPCsettingswithasmallnumberofparties.
canonlyinteractwiththefinaltrainedmodel.
We impose no restrictions on the adversaryâ€™s poisons being
Theprivacygame. Weconsideragenericprivacygame,wherein â€œstealthyâ€.Thatis,weallowforthepoisoneddatasetð· tobe
adv
theadversaryhastoguesswhichelementfromsomeuniverseU arbitrary.Aswewillsee,designingpoisoningattacksthatmaximize
wasusedtotrainamodel.ByappropriatelydefiningtheuniverseU privacy leakage is non-trivialâ€”even when the adversary is not
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
constrainedintheirchoiceofpoisons.Aspoisoningattacksthat helptheadversaryinsolvingthedistinguishinggameâ€”andinfact
targetdataprivacyhavenotbeenstudiedsofar,weaimhereto actuallymakesitmoredifficulttodistinguishmembership.
understandhoweffectivesuchattackscouldbeintheworstcase, Instead,theadversaryshouldalterthetrainingsetð· soasto
andleavethestudyofattackswithfurtherconstraints(suchas maximizetheinfluenceofthetarget (ð‘¥,ð‘¦).Thatis,wewantthe
â€œcleanlabelâ€poisoning[61,67])tofuturework. poisonedtrainingsetð· âˆªð·
adv
tobesuchthattheinclusionof
Finally,thegameassumesthattheadversarytargetsaspecific thetarget(ð‘¥,ð‘¦)providesamaximalchangeinthetrainedmodelâ€™s
exampleð‘§.Wecallthisatargetedattack.Wealsoconsideruntargeted behavioronsomeinputsoftheadversaryâ€™schoice.
attacksinSection4.4,wheretheattackercraftsapoisoneddataset Toillustratethisprinciple,webeginbydemonstratingaprovably
ð· toharmtheprivacyofallsamplesinthetrainingsetð·. perfect privacy-poisoning attack for the special case of nearest-
adv
neighborclassifiers.WealsoproposeanalternativeattackforSVMs
3.1.2 SuccessMetrics. Whentheuniverseofsecretvaluesis inAppendixD.Wethendescribeourdesignprinciplesforempirical
small(asformembershipinference,where|U|=2,orforattribute
attacksondeepneuralnetworks.
inferencewhereitisthecardinalityoftheattribute),wemeasurean
attackâ€™ssuccessratebyitstrue-positiverate(TPR)andfalse-positive
Warm-up:provablyamplifyingmembershipleakageinkNNs.
rate(FPR)overmultipleiterationsofthegame.Following[11],we
Considerað‘˜-NearestNeighbor(kNN)classifier(assume,wlog.,that
focusinparticularontheattackperformanceatlowfalse-positive
ð‘˜isodd).Givenalabeledtrainingsetð·,andatestsampleð‘¥,this
rates(e.g.,FPR=0.1%),whichmeasurestheattackâ€™spropensityto
classifierfindstheð‘˜nearestneighborsofð‘¥ inð·,andoutputsthe
preciselytargettheprivacyofsomeworst-caseusers.
majoritylabelamongtheseð‘˜neighbors.Weassumetheattacker
Formembershipinference,wenaturallydefineatrue-positive
hasblack-boxqueryaccesstothetrainedclassifier.
asacorrectguessofmembership,i.e.,ð‘§Ë† = ð‘§ whenð‘§ = ð‘¥,anda We demonstrate how to poison a kNN classifier so that the
false-positiveasanincorrectmembershipguess,ð‘§Ë†â‰ ð‘§whenð‘§=ð‘¥.
classifierlabelsatargetexample(ð‘¥,ð‘¦)correctlyifandonlyif the
Forattributeinference,wedefineaâ€œpositiveâ€asanexample
targetisintheoriginaltrainingsetð·.Thisattackthusletsthe
withaspecificvaluefortheunknownattribute(e.g.,iftheunknown
adversarywinthemembershipinferencegamewith100%accuracy.
attributeisgender,wedefineâ€œfemaleâ€asthepositiveclass).
Ourpoisoningattack(seeAlgorithm1inAppendixD)creates
Forcanaryextraction,wheretheuniverseofpossibletarget
adatasetð·
adv
ofsizeð‘˜ thatcontainsð‘˜âˆ’1copiesofthetargetð‘¥,
valuesislarge(e.g.,allpossiblecreditcardnumbers),weamend
halfcorrectlylabeledasð‘¦andhalfmislabeledasð‘¦â€²â‰ ð‘¦.Wefurther
Game3.2toallowtheadversarytoobtainâ€œpartialcreditâ€byemit-
addonepoisonedexampleð‘¥â€²atasmalldistanceð›¿fromð‘¥ andalso
tingmultipleguesses.Specifically,following[13],welettheadver-
mislabeledasð‘¦â€²(weassumethatnootherpointinthetrainingset
saryoutputanordering(apermutation)ð‘Ë† =ðœ‹(U)ofthesecretâ€™s
ð·iswithindistanceð›¿fromð‘¥).Thisattackmaximizestheinfluence
possiblevalues,frommostlikelytoleastlikely.Wethenmeasure
ofthetargetedpoint,byturningitintoatie-breakerforclassifying
theattackâ€™ssuccessbytheexposure [13](inbits)ofthecorrect
ð‘¥ whenitisamember.
secretð‘§:
Theattackerinfersthatthetargetexample(ð‘¥,ð‘¦)isamember,
exposure(ð‘§;ð‘Ë† ) (cid:66)log 2(|U|)âˆ’log 2 (cid:16) rank(ð‘§;ð‘Ë† ) (cid:17) . (2) ifandonlyifthetrainedmodelcorrectlyclassifiesð‘¥ asclassð‘¦.To
seethattheattackworks,considerthetwopossibleworlds:
Theexposurerangesfrom0bits(whenthecorrectsecretð‘§isranked
astheleastlikelyvalue),tolog 2(|U|)bits(whentheadversaryâ€™s â€¢ Thetargetisinð·:Thereareð‘˜copiesofð‘¥ inthepoisoned
mostlikelyguessisthecorrectvalueð‘§).
trainingsetð·âˆªð·
adv
:theð‘˜âˆ’1poisonedcopies(halfare
correctlylabeled)andthetarget(ð‘¥,ð‘¦).Thus,themajority
voteamongtheð‘˜neighborsyieldsthecorrectclassð‘¦.
3.2 AttackOverview
â€¢ Thetargetisnotinð·:Asallpointsinð·areatdistanceat
Webeginwithahigh-leveloverviewofourpoisoningattackstrate-
leastð›¿fromthetargetð‘¥,theð‘˜neighborsselectedbythe
gies. For simplicity of exposition, we focus on the special case
modelaretheadversaryâ€™sð‘˜poisonedpoints,amajorityof
ofmembershipinference.Ourattacksforattributeinferenceand
whicharemislabeledasð‘¦â€².Thus,themodeloutputsð‘¦â€².
canaryextractionfollowsimilarprinciples.
Givenatargetsample(ð‘¥,ð‘¦),thestandardprivacygame(formem- InAppendixD,weshowthatourattackisnon-trivial,inthat
bershipinference)inGame3.1askstheadversarytodistinguish thereexistpointsforwhichpoisoningisnecessarytoachieveperfect
twoworlds,wherethemodelisrespectivelytrainedonð·âˆª{(ð‘¥,ð‘¦)} membershipinference.Infact,weshowthatforsomepoints,anon-
or onð·. When we give the adversary the ability to poison the poisoningadversarycannotinfermembershipbetterthanchance.
datasetinGame3.2,thegoalisnowtoalterthedatasetð·sothat
theabovetwoworldsbecomeeasiertodistinguish. Amplifyingprivacyleakageindeepneuralnetworks. Theabove
Notethatthisgoalisverydifferentfromsimplymaximizingthe attackonkNNsexploitstheclassifierâ€™sspecificstructurewhich
modelâ€™smemorizationofthetarget(ð‘¥,ð‘¦).Thiscouldbeachieved letsusturnanyexampleâ€™smembershipintoaperfecttie-breaker
withthefollowing(bad)strategy:poisonthedatasetð·byadding forthemodelâ€™sdecisiononthatexample.Indeepneuralnetworks,
multipleidenticalcopiesof(ð‘¥,ð‘¦)intoit.Thiswillensurethatthe itisunlikelythatexamplescanexhibitsuchaclearcutinfluence
trainedmodelð‘“ stronglymemorizesthetarget(i.e.,themodelwill (i.e.,duetothestochasticityoftraining,itisunlikelythataspecific
ðœƒ
correctlyclassifyð‘¥ withveryhighconfidence).However,thiswill modelbehaviorwouldoccurifandonlyif anexampleisamember).
betrueinbothworlds,regardlessofwhetherthetarget(ð‘¥,ð‘¦)was Instead,wecouldtrytocasttheadversaryâ€™sgoalasanoptimiza-
intheoriginaltrainingsetð· ornot.Thisstrategythusdoesnot tionproblem,ofselectingapoisoneddatasetð· adv thatmaximizes
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
thedistinguishabilityofmodelstrainedwithorwithoutthetar- 100
get(ð‘¥,ð‘¦).Yet,solvingsuchanoptimizationproblemisdaunting.
Whilepriorworkdoesoptimizepoisonstomaximallyalterasingle
modelâ€™sconfidenceonaspecifictargetpoint[61,67,78],herewe
wouldinsteadneedtooptimizeforadifferenceindistributionsof
thedecisionsoftwomodelstrainedontwoneighboringdatasets. 10âˆ’1
Ratherthantacklethisoptimizationproblemdirectly,weâ€œhand-
craftâ€strategiesthatempiricallyincreaseasampleâ€™sinfluenceon
themodel.Westartfromtheobservationinpriorworkthatthemost
vulnerableexamplestoprivacyattacksaredataoutliers[11,75].
Suchexamplesareeasytoattackpreciselybecausetheyhavealarge
10âˆ’2
influenceonthemodel:amodeltrainedonanoutlierhasamuch 10âˆ’3 10âˆ’2 10âˆ’1 100
lowerlossonthissamplethanamodelthatwasnottrainedonit. False Positive Rate
Yet,inourthreatmodel,theattackercannotcontrolormodifythe
targetedexampleð‘¥(andð‘¥isunlikely,apriori,tobeanoutlier).Our
insightthenistopoisonthetrainingdatasetsoastotransformthe
targetedexampleð‘¥ intoanoutlier.Forexample,wecouldfoolthe
modelintobelievingthatthetargetedpointð‘¥ ismislabeled.Then,
thepresenceofthecorrectlylabeledtarget(ð‘¥,ð‘¦)inthetrainingset
islikelytohavealargeinfluenceonthemodelâ€™sdecision.
InSection4,weshowhowtoinstantiatethisattackstrategyto
boostmembershipinferenceattacksonstandardimagedatasets.We
thenextendthisattackstrategyinSection5tothecaseofattribute
inferenceattacksfortabulardatasets.Finally,inSection6wepro-
poseattackstrategiestailoredtolanguagemodels,thatmaximize
theleakageofspeciallyformattedcanarysequences.
4 MEMBERSHIPINFERENCEATTACKS
Membershipinference(MI)capturesoneofthemostgenericnotions
ofprivacyleakageinmachinelearning.Indeed,anyformofdata
leakagefromamodelâ€™strainingset(e.g.,attributeinferenceordata
extraction)impliestheabilitytoinfermembershipofsometraining
examples.Asaresult,membershipinferenceisanaturaltargetfor
evaluatingtheimpactofpoisoningattacksondataprivacy.
Inthissection,weintroduceandanalyzedatapoisoningattacks
thatimprovemembershipinferencebyonetotwoordersofmagni-
tude.Section4.2describesatargetedattackthatincreasesleakage
ofaspecificsample(ð‘¥,ð‘¦),andSection4.3containsananalysisof
thisattackâ€™ssuccess.Section4.4exploresuntargetedattacksthat
increaseprivacyleakageonalltrainingpointssimultaneously.
4.1 ExperimentalSetup
We extend the recent attack of [11] that performs membership
inferenceviaaper-examplelog-likelihoodtest.Theattackfirst
trainsð‘ shadowmodelssuchthateachsample(ð‘¥,ð‘¦)appearsinthe
trainingsetofhalfoftheshadowmodels,andnotintheotherhalf.
Wethencomputethelossesofbothsetsofmodelsonð‘¥:
ð¿ in={â„“(ð‘“(ð‘¥),ð‘¦) : ð‘“ trainedon(ð‘¥,ð‘¦)},
ð¿out={â„“(ð‘“(ð‘¥),ð‘¦) : ð‘“ nottrainedon(ð‘¥,ð‘¦)}
andfitGaussiandistributionsN(ðœ‡ in ,ðœŽ i 2 n )toð¿ in ,andN(ðœ‡out,ðœŽ o 2 ut )
toð¿out (withalogitscalingofthelosses,asin[11]).Then,toinfer
membershipofð‘¥inatrainedmodelð‘“ ,wecomputethelossofð‘“ on
ðœƒ ðœƒ
ð‘¥,andperformastandardlikelihood-ratiotestforthehypotheses
thatð‘¥ wasdrawnfromN(ðœ‡ in ,ðœŽ i 2 n )orfromN(ðœ‡out,ðœŽ o 2 ut ).
etaR
evitisoP
eurT
poison x16
poison x8
poison x4
poison x2
poison x1
No poison
Figure2:Targetedpoisoningattacksboostmembershipin-
ferenceonCIFAR-10.For250randomdatapoints,weinsert
1to16mislabelledcopiesofthepointintothetrainingset,
andruntheMIattackof[11]with128shadowmodels.
Toamplifytheattackwithpoisoning,theadversarybuildsa
poisoneddatasetð· thatisaddedtothetrainingsetofð‘“ .The
adv ðœƒ
adversaryalsoaddsð· toeachshadowmodelâ€™strainingset(so
adv
thatthesemodelsareassimilaraspossibletothetargetmodelð‘“ ).
ðœƒ
WeperformourexperimentsonCIFAR-10andCIFAR-100[38]â€”
standardimagedatasetsof50,000samplesfromrespectively10and
100classes.Thetargetmodels(andshadowmodels)useaWide-
ResNetarchitecture[76]trainedfor100epochswithweightdecay
andcommondataaugmentations(randomimageflipsandcrops).
Foreachdataset,wetrainð‘ =128modelsonrandom50%splitsof
theoriginaltrainingset.1Themodelsachieve91%testaccuracyon
CIFAR-10and67%test-accuracyonCIFAR-100onaverage.
4.2 TargetedPoisoningAttacks
Wenowdesignourpoisoningattacktoincreasethemembership
inferencesuccessrateforaspecifictargetexampleð‘¥.Thatis,the
attackerknowsthedataofð‘¥(butnotwhetheritisusedtotrainthe
model)anddesignsapoisoneddatasetð· adaptivelybasedonð‘¥.
adv
Labelflippingattacks. Wefindthatlabelflippingattacksarea
verypowerfulformofpoisoningattackstoincreasedataleakage.
Given a targeted exampleð‘¥ with labelð‘¦, the adversary inserts
themislabelledpoisonsð· adv={(ð‘¥,ð‘¦â€²),...,(ð‘¥,ð‘¦â€²)}forsomelabel
ð‘¦â€²â‰ ð‘¦.Therationaleforthisattackisthatamodeltrainedonð·
adv
willlearntoassociateð‘¥ withlabelð‘¦â€²,andthenowâ€œmislabelledâ€
target (ð‘¥,ð‘¦) willbetreatedasanoutlierandhaveaheightened
influenceonthemodelwhenpresentinthetrainingset.
ToinstantiatethisattackonCIFAR-10andCIFAR-100,wepick
250targetedpointsatrandomfromtheoriginaltrainingset.For
eachtargetedexample(ð‘¥,ð‘¦),thepoisoneddatasetð·
adv
containsa
mislabelledexample(ð‘¥,ð‘¦â€²)replicatedð‘Ÿ times,forð‘Ÿ âˆˆ{1,2,4,8,16}.
Wereporttheaverageattackperformanceforafullleave-one-out
cross-validation(i.e.,weevaluatetheattack128times,usingone
modelasthetargetandtherestasshadowmodels).
1Thetrainingsetsofthetargetmodelandshadowmodelsthuspartiallyoverlap
(althoughtheadversarydoesnotknowwhichpointsareinthetargetâ€™strainingset).
Carlinietal.[11]showthattheirattackisminimallyaffectediftheattackerâ€™sshadow
modelsaretrainedondatasetsfullydisjointfromthetargetâ€™strainingset.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
25 0 2525 0 2525 0 2525 0 2525 0 2525 0 25
âˆ’ nopoison âˆ’ poisonx1 âˆ’ poisonx2 âˆ’ poisonx4 âˆ’ poisonx8 âˆ’ poisonx16 False Positive Rate
Figure 3: Our poisoning attack separates the loss distribu-
tionsofmembersandnon-members,makingthemmoredis-
tinguishable.ForfiverandomCIFAR-10examples,weplot
the(logit-scaled)lossdistributiononthatexamplewhenit
isamember(red)ornot(blue).Thehorizontalaxisvaries
thenumberoftimestheadversarypoisonstheexample.
Results. Figure2andFigure15(appendix)showtheperformance
ofourmembershipinferenceattackonCIFAR-10andCIFAR-100
respectively,aswevarythenumberofpoisonsð‘Ÿ persample.
Wefindthatthis attackisremarkably effective. Evenwitha
single poisoned example (ð‘Ÿ = 1), theattackâ€™s true-positive rate
(TPR)ata0.1%false-positiverate(FPR)increasesby1.75Ã—.With8
poisons(0.03%ofthemodelâ€™strainingsetsize),theTPRincreasesby
afactor8Ã—onCIFAR-10,from7%to59%.OnCIFAR-100,poisoning
increasesthebaselineâ€™sstrongTPRof22%to69%ataFPRof0.1%.
Alternatively,wecouldaimforafixedrecallandusepoisoning
toreducetheMIattackâ€™serrorrate.Withoutpoisoning,anattack
thatcorrectlyidentifieshalfofthetargetedCIFAR-10members(i.e.,
aTPRof50%)wouldalsoincorrectlylabel24%ofnon-membersas
members.Withpoisoning,thesamerecallisachievedwhileonly
mislabeling0.05%ofnon-membersâ€”afactor480Ã—improvement.
OnCIFAR-100,alsofora50%TPR,poisoningreducestheattackâ€™s
false-positiveratebyafactor100Ã—,from2.5%to0.025%.
As we run multiple targeted attacks simultaneously (for effi-
ciencysake),thetotalnumberofpoisonsislarge(upto4,000misla-
belledpoints).Yet,thepoisonedmodelâ€™stestaccuracyisminimally
reduced(from92%to88%)andtheMIsuccessrateonnon-targeted
pointsremainsunchanged.Thus,wearenotcompoundingtheef-
fectsofthe250targetedattacks.Asasanitycheck,werepeatthe
experimentwithonly50targetedpoints,andobtainsimilarresults.
4.3 AnalysisandAblations
Wehaveshownthattargetedpoisoningattackssignificantlyin-
crease membership leakage. We now set out to understand the
principlesunderlyingourattackâ€™ssuccess.
4.3.1 Whydoesourattackwork? InFigure3weplotthedis-
tributionofmodelconfidencesforfiveCIFAR-10examples,when
theexampleisamember(inred)andwhenitisnot(inblue).On
thehorizontalaxis,wevarythenumberofpoisons(i.e.,howmany
timesthisexampleismislabeledinthetrainingset).Withoutpoi-
soning(leftcolumn),thedistributionsoverlapsignificantlyformost
etaR
evitisoP
eurT most vulnerable (targeted x4)
most vulnerable (no poison)
least vulnerable (targeted x4)
least vulnerable (no poison)
Figure4:Poisoningcausespreviously-safedatapointstobe-
comevulnerable.Werunourattackforthe5%ofpointsthat
areoriginallymost-andleast-vulnerabletomembershipin-
ferencewithoutpoisoning.Whilepoisoninghaslittleeffect
forthemostvulnerablepoints,poisoningtheleastvulnera-
blepointsimprovestheTPRata0.1%FPRbyafactor430Ã—.
examples.Asweincreasethenumberofpoisons,theconfidences
shiftsignificantlytotheleft,asthemodelbecomeslessandless
confidentintheexampleâ€™struelabel.Butcrucially,thedistributions
alsobecomeeasiertoseparate,becausethe(relative)influenceof
thetargetedexampleonthetrainedmodelisnowmuchlarger.
Toillustrate,considerthetopexampleinFigure3(labeledâ€œshipâ€).
Withoutpoisoning,thisexampleâ€™sconfidenceisintherange[99.99%,
100%]whenitisamember,and[99.98%,100%]whenitisnot.Con-
fidentlyinferringmembershipisthusimpossible.With16poisons,
however,theconfidenceonthisexampleisintherange[0.4%,28.5%]
whenitisamember,and[0%,2.4%]whenitisnotâ€”thusenabling
precisemembershipinferencewhentheconfidenceexceeds2.4%.
4.3.2 Whichpointsarevulnerabletoourattack? Ourpoisoning
attackcouldincreasetheMIsuccessrateindifferentways.Poison-
ingcouldincreasetheattackaccuracyuniformlyacrossalldata
points,oritmightdisparatelyimpactsomedatapoints.Weshow
that the latter is true: our attack disparately impacts inliers
thatwereoriginallysafe frommembershipinference.This
resulthasstrikingconsequences:evenifauserisaninlierand
thereforemightnotbeworriedaboutprivacyleakage,anactive
poisoningattackerthattargetsthisusercanstillinfermembership.
InFigure4,weshowtheperformanceofourpoisoningattack
onthosedatapointsthatareinitiallyeasiestandhardesttoinfer
membershipfor.Werunthemembershipinferenceattackof[11]on
allCIFAR-10points,andselectthe5%ofsampleswheretheattack
succeedsleastoftenandmostoften(averagedoverall128models).
Wethenre-runthebaselineattackontheseextremalpointswitha
newsetofmodels(toensureourselectionofpointsdidnotoverfit)
andcomparewithourlabelflippingattackwithð‘Ÿ =4.
Poisoninghasaminoreffectondatapointsthatarealreadyout-
liers:hereeventhebaselineMIattackhasahighsuccessrate(73%
TPRata0.1%FPR)andthusthereislittleroomforimprovement.2
2InFigure3,weseethatexamplesforwhichMIsucceedswithoutpoisoningtendto
alreadybeoutliers.Forexample,thethirdandfourthexamplefromthetopareaâ€œbirdâ€
mislabelledasâ€œcatâ€intheCIFAR-10trainingset,andaâ€œhorseâ€confusedasaâ€œdeerâ€.
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
100
10âˆ’1
with shadow models
poison x16
poison x4
poison x1
No poison
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
Figure5:Membershipinferenceattackswithpoisoningdo
not require shadow models. With poisoning, the global
thresholdattackof[75]performsnearlyaswellonCIFAR-
10astheattackof[11]thatuses128shadowmodelstocom-
puteindividualdecisionthresholdsforeachexample.
Forpointsthatareoriginallyhardesttoattack,however,poisoning
improvestheattackâ€™sTPRbyafactor430Ã—,from0.1%to43%.
4.3.3 Areshadowmodelsnecessary? Following[11,41,58,70,
74],ourMIattackreliesonshadowmodelstocalibratetheconfi-
dencesofindividualexamples.Indeed,asweseeinthefirstcolumn
ofFigure3,theconfidencesofdifferentexamplesareondifferent
scales,andthustheoptimalthresholdtodistinguishamemberfrom
anon-membervariesgreatlybetweenexamples.Yet,asweincrease
thenumberofpoisonedsamples,weobservethatthescaleofthe
confidencesbecomesunifiedacrossexamples.Andwith16poisons,
thethresholdthatbestdistinguishesmembersfromnon-members
isapproximatelythesameforallexamplesinFigure3.
Asaresult,weshowinFigure5thatwithpoisoning,theuse
ofshadowmodelsforcalibrationisnolongernecessaryto
obtainastrongMIattack.Bysimplysettingaglobalthresholdon
theconfidenceofatargetedexample(asin[75])theMIattackworks
nearlyaswellasourfullattackthattrains128shadowmodels.
Thisresultrendersourattackmuchmorepracticalthanprior
attacks.Indeed,inmanysettings,trainingevenasingleshadow
modelcouldbeprohibitivelyexpensivefortheattacker(intermsof
accesstotrainingdataorcompute).Incontrast,theabilitytopoison
asmallfractionofthetrainingsetmaybemuchmorerealistic,espe-
ciallyforverylargemodels.Recentworks[11,48,70,74]showthat
non-calibratedMIattacks(withoutpoisoning)performnobetter
thanchanceatlowfalse-positives(seeFigure5).Withpoisoning
however,thesenon-calibratedattacksperformextremelywell.At
aFPRof0.1%,anon-calibratedattackwithoutpoisoninghasaTPR
of0.1%(randomguessing),whereasanon-calibratedattackwith
16targetedpoisonshasaTPRof43%â€”animprovementof430Ã—.
4.3.4 Does the choice of label matter? Our poisoning attack
injectsatargetedexamplewithanincorrectlabel.Fortheresultsin
Figure2andFigure15,weselectanincorrectlabelatrandom(ifwe
replicateapoisonð‘Ÿ times,weusethesamelabelforeachreplica).
InFigure6weexplorealternativestrategiesforchoosingthe
incorrectlabelonCIFAR-10.Weconsiderthreeotherstrategies:
etaR
evitisoP
eurT
poison x4 (random)
poison x4 (best)
poison x4 (worst)
poison x4 (random-multi)
No poison
Figure6:ComparisonofmislabellingstrategiesonCIFAR-
10.Assigningthesamerandomincorrectlabelto4poison
copiesperformsbetterthanmislabelingasthemostlikely
incorrectclass(best)ortheleastlikelyclass(worst).Assign-
ingdifferentincorrectlabelstothe4copies(random-multi)
severelyreducestheattacksuccessrate.
â€¢ best:mislabelthepoisonsasthemostlikelyincorrectclass
forthatexample(aspredictedbyapre-trainedmodel).
â€¢ worst:mislabelthepoisonsastheleastlikelyclass.
â€¢ random-multi:sampleanincorrectlabelatrandom(with-
outreplacement)foreachoftheð‘Ÿ poisons.
Thesethreestrategiesperformworsethantherandomapproach.
OnbothCIFAR-10(Figure6)andCIFAR-100(Figure16)theâ€œbestâ€
andâ€œworstâ€strategiesdoslightlyworsethanrandommislabeling.
Theâ€œrandom-multiâ€strategydoesmuchworse,andunder-performs
thebaselineattackwithoutpoisoningatlowFPRs.Thisstrategy
hastheoppositeeffectofouroriginalattack,asitforcesthemodel
topredictanear-uniformdistributionacrossclasses,whichisonly
minimallyinfluencedbythepresenceorabsenceofthetargeted
example.Overall,thisexperimentshowsthattheexactchoiceof
incorrectlabelmatterslittle,aslongasitisconsistent.
4.3.5 Cantheattackbeimprovedbymodifyingthetarget? Our
poisoningattackonlytamperswithatargetâ€™slabelð‘¦,whileleaving
theexampleð‘¥ unchanged.Itisconceivablethatanattackthatalso
altersthesamplebeforepoisoningcouldresultinevenstronger
leakage.InAppendixA.2weexperimentwithanumberofsuch
strategies,inspiredbytheliteratureonclean-labelpoisoningat-
tacks[61,67,78].Butweultimatelyfailedtofindanapproachthat
improvesuponourattackandleaveitasanopenproblemtodesign
betterprivacy-poisoningstrategiesthatalterthetargetsample.
4.3.6 Doestheattackrequireexactknowledgeofthetarget? Ex-
istingmembershipinferenceattacks,whichcanbeusedforauditing
MLprivacyvulnerabilities,typicallyassumeexactknowledgeof
thetargetedexample(sothattheadversarycanquerythemodelon
thatexample).Ourattackisnodifferentinthisregard:itrequires
knowledgeofthetargetattrainingtime(inordertopoisonthe
model)andatevaluationtimetoruntheMIattack.
Wenowevaluatehowwellourattackperformswhentheadver-
saryhasonlypartialknowledgeofthetargetedexample.Asweare
dealingwithimageshere,definingsuchpartialknowledgerequires
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
100
10âˆ’1
poison x16
poison x8 10âˆ’2
poison x4
poison x2
poison x1
No poison 10âˆ’3
None 1.0 0.1 0.01 0.001
Loss clip
Figure7:ForCIFAR-10modelstrainedwithlossesclipped
toð¶ =1,poisoningonlymoderatelyincreasesthesuccessof
MIattacks.Withmorethan1mislabeledcopyofthetarget,
poisoningharmstheattackatlowfalse-positives.
somecare.Wewillassumethatinsteadofknowingtheexacttarget
exampleð‘¥,theadversaryknowsanexampleð‘¥Ë†thatâ€œlookssimilarâ€
toð‘¥.Theattackerneedstoguesswhetherð‘¥ wasusedtotraina
model.Tothisend,theattackerpoisonsthetargetmodel(andthe
shadowmodels)byinjectingmislabeledversionsofð‘¥Ë†andqueries
thetargetmodelonð‘¥Ë†toformulateaguess.
DetailsofthisexperimentareinAppendixA.3.Figure19shows
that our attack (as well as the baseline without poisoning)
arerobusttoanadversarywithonlypartialknowledgeof
thetarget.AtaFPRof0.1%,theTPRisreducedby<1.6Ã—forboth
thebaselineattackandourattackwith4poisonspertarget.
4.3.7 Canwemitigatetheattackbyboundingoutlierinfluence?
Aswehaveshown,ourattacksucceedsbyturningdatapointsinto
outliers,whichthenhaveahighinfluenceonthemodelâ€™sdeci-
sions.Ourprivacy-poisoningattackcanthuslikelybemitigatedby
boundingtheinfluencethatanoutliercanhaveonthemodel.For
example,trainingwithdifferentialprivacy[1,17]wouldprevent
ourattack,asitboundstheinfluencethatanyoutliercanhavein
anydataset(includingapoisonedone).Algorithmsfordifferentially
privatedeeplearningboundthesizeofthegradientsofindividual
examples[1].Here,weoptforaslightlysimplerapproachthat
boundsthelossesofindividualexamples(thetwoapproachesare
equivalentifweassumesomeboundonthemodelâ€™sactivationsin
aforwardpass).Boundinglossesratherthangradientshasthead-
vantageofbeingmuchmorecomputationallyefficient,asitsimply
requiresscalinglossesbeforebackpropagation.
InFigure7,weplottheMIsuccessratewithandwithoutpoison-
ing,wheneachexampleâ€™scross-entropylossisboundedtoð¶ =1.
Clippinginthiswayonlyslightlyreducesthesuccessrateofthe
attackwithoutpoisoning,butsignificantlyharmsthesuccessof
thepoisoningattackatlowfalse-positives.Whileourattackwith
ð‘Ÿ =1poisonspertargetstillimprovesoverthebaseline,including
additionalpoisonsweakenstheattack,astheoriginalsampleâ€™sloss
cannolongergrowunboundedtocounteractthepoisoning.
InFigure20inAppendixA.4,weshowtheeffectoftraining
withlossclippingonthedistributionsofmemberandnon-member
confidences for five random CIFAR-10 samples, analogously to
RPF
%1.0@
RPT
90
88
86
84
With poison 82
Without poison
80
)%(
ycaruccA
tseT
Figure 8: Aggressive loss clipping reduces the success rate
ofMIattacksonCIFAR-10nearlytochance.However,poi-
soningcanstillboosttheattacksuccessbyupto3Ã—,andthe
modelâ€™stesterrorisincreasedby45%.
Figure3.Poisoningthemodelwithmislabeledsamplesstillshifts
theconfidencestoverylowvalues,buttheinclusionofthecorrectly
labeledtargetnolongerclearlyseparatesthetwodistributions.
Whilelossclippingthusappearstobeasimpleandeffectivede-
fenseagainstourpoisoningattack,itisnoprivacypanacea.Indeed,
theoriginalbaselineMIattackretainshighsuccessrate.Asweshow
inFigure8,furtherreducingtheclippingbound(toð¶ =10âˆ’3)does
reducethebaselineMIattacktonear-chance.Butinthisregime,
poisoningdoesagainincreasetheattackâ€™ssuccessrateatlowfalse-
positivesbyafactor3Ã—.Moreover,aggressiveclippingreducesthe
modelâ€™stestaccuracyfrom91%to87%â€”anincreaseinerrorrate
of45%(equivalenttoundoingthreeyearsofprogressinmachine
learningresearch).Finally,wealsoshowinSection4.4thatanal-
ternativeuntargetedattackstrategy,thatincreasesleakageofall
datapoints,remainsresilienttomoderatelossclipping.
4.4 UntargetedPoisoningAttacks
Sofarwehaveconsideredpoisoningattacksthattargetaspecific
example(ð‘¥,ð‘¦)thatisknown(exactlyorpartially)totheadversary.
Wenowturntomoregeneraluntargeted attacks,wherethead-
versaryaimstoincreasetheprivacyleakageofallhonesttraining
points.Asthisisamuchmorechallenginggoal,wewillconsider
adversaries who can compromise a much larger fraction of the
trainingdata.Thisthreatisrealisticinsettingswhereasmallnum-
berofpartiesdecidetocollaborativelytrainamodelbypooling
theirrespectivedatasets(e.g.,twoormorehospitalsthattraina
jointmodelusingsecuremulti-partycomputation).
Setup. We consider a setting where the training data is split
betweentwoparties.Onepartyactsmaliciouslyandchoosestheir
datasoastomaximizetheleakageoftheotherpartyâ€™sdata.
Weadapttheexperimentalsetupoftargetedattacksdescribedin
Section4.1.Forouruntargetedattack,weassumetheadversaryâ€™s
poisoneddatasetð· isofthesamesizeasthevictimâ€™straining adv
datað·. We propose an untargeted variant of our label flipping
attack,inwhichtheattackerpickstheirdatafromthesamedis-
tributionasthevictim,i.e.,ð· adv â† D,butflipsalllabelstothe
samerandomlychosenclass:ð· ð‘Žð‘‘ð‘£ ={(ð‘¥1,ð‘¦),...,(ð‘¥ ð‘adv ,ð‘¦)}.This
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
Inanattributeinferenceattack,theadversaryhaspartialknowl-
edgeofsometrainingexampleð‘¥,andabusesaccesstoatrained
modeltoinferunknownfeaturesofthisexample.Forsimplicity
ofexposition,weconsiderthecaseofinferringabinaryattribute
(e.g.,whetherauserismarriedornot),givenknowledgeofthe
otherfeaturesofð‘¥,andoftheclasslabelð‘¦.Inthecontextofour
privacygame,Game3.2,theuniverseUconsistsofthetwopossible
â€œversionsâ€ofatargetexample,ð‘§0 = (ð‘¥0,ð‘¦),ð‘§1 = (ð‘¥1,ð‘¦),whereð‘¥ð‘–
untargeted poisoning denotesthetargetexamplewithvalueð‘–fortheunknownattribute.
No poison
5.1 AttackSetup
Westartfromthestate-of-the-artattributeinferenceattackof[45].
Givenatrainedmodelð‘“ ,thisattackcomputesthelossesforboth
ðœƒ
Figure9:Anuntargetedpoisoningattackthatconsistently versionsofthetarget,â„“(ð‘“ ðœƒ(ð‘¥0),ð‘¦)andâ„“(ð‘“ ðœƒ(ð‘¥1),ð‘¦)andpicksthe
mislabels50%oftheCIFAR-10trainingdataincreasesmem- attributevaluewithlowestloss.
bershipinferenceacrossallotherdatapoints. Similarlytomanypriormembershipinferenceattacks,thisat-
tackdoesnotaccountfordifferentexampleshavingdifferentloss
distributions. Indeed, for some examples the losses â„“(ð‘“ ðœƒ(ð‘¥0),ð‘¦)
attackaimstoturnallpointsthatareofadifferentclassthanð‘¦into
andâ„“(ð‘“ ðœƒ(ð‘¥1),ð‘¦)areverysimilar,whileforotherexamplesthedis-
tributionsareeasiertodistinguish.Followingrecentadvancesin
outliers,sothattheirmembershipbecomeseasiertoinfer.
membership inference attacks [11, 58, 70, 74] we thus design a
ToevaluatetheattackonCIFAR-10,weselect12,500pointsat
strongerattackthatusesshadowmodelstocalibratethelossesof
randomfromthetrainingsettobuildthepoisoneddatasetð· .
adv differentexamples.Wetrainð‘ shadowmodels,suchthatthetwo
Thehonestpartyâ€™sdatasetð·consistsof12,500pointssampledfrom
versions(ð‘¥ð‘–,ð‘¦)ofthetargetedexampleeachappearinthetraining
theremainingpartofthetrainingset.Wetrainatargetmodelonthe
setofhalftheshadowmodels.Foreachsetofmodels,wethen
jointdatasetð·âˆªð·
adv
.Theattackerfurthertrainsshadowmodels
computethedifferencebetweenthelossoneitherversionofð‘¥:
byrepeatingtheaboveprocessofsamplinganhonestdatasetð·
andcombiningitwiththeadversaryâ€™sfixeddatasetð· adv .Intotal, ð¿0= (cid:8)â„“(ð‘“(ð‘¥0 ),ð‘¦)âˆ’â„“(ð‘“(ð‘¥1 ),ð‘¦) : ð‘“ trainedon(ð‘¥0,ð‘¦) (cid:9) ,
w
on
e
a
tr
s
a
e
i
t
n
o
ð‘
f2
=
5,0
1
0
2
0
8
p
m
o
o
in
d
t
e
s
ls
d
.
i
W
sjo
e
in
ru
t
n
fro
th
m
e
ð·
mem
,
b
h
e
a
r
l
s
f
h
o
ip
fw
in
h
f
i
e
c
r
h
en
a
c
r
e
e
a
a
t
c
t
t
a
u
c
a
k
l
ð¿1= (cid:8)â„“(ð‘“(ð‘¥0 ),ð‘¦)âˆ’â„“(ð‘“(ð‘¥1 ),ð‘¦) : ð‘“ trainedon(ð‘¥1,ð‘¦) (cid:9) .
adv
membersofthetargetmodel.Weaverageresultsovera128-fold Asintheattackof[11],wefitGaussiandistributionstoð¿0 and
leave-one-outcross-validationwherewechooseoneofthe128 ð¿1 .Givenatargetmodelð‘“
ðœƒ
,wecomputethedifferenceinlosses
modelsasthetargetandtheothersastheshadowmodels. â„“(ð‘“ ðœƒ(ð‘¥0),ð‘¦)âˆ’â„“(ð‘“ ðœƒ(ð‘¥1),ð‘¦)andperformalikelihood-ratiotestbe-
tweenthetwoGaussians.Thisattackactsasourbaseline.
Results. Figure9showstheperformanceofouruntargetedattack.
Tothenimproveonthiswithpoisoning,weinjectð‘Ÿ/2mislabelled
Poisoningreliablyincreasestheprivacyleakageofallthehonest samplesoftheform(ð‘¥0,ð‘¦â€²),andð‘Ÿ/2oftheform(ð‘¥1,ð‘¦â€²)intothe
partyâ€™sdatapoints.AtaFPRof0.1%,theattackâ€™sTPRacrossall
trainingset.Mislabelingbothversionsofthetargetforcesthemodel
thevictimâ€™sdatagrowsfrom9%withoutpoisoningto16%withour
tohavesimilarlylargelossoneitherversion.Thetruevariantof
untargetedattack.Conversely,atafixedrecall,untargetedpoison-
thetargetsamplewillthenhavealargeinfluenceononeofthese
ingreducestheattackâ€™serrorratedrastically.Withourpoisoning
losses,whichwillbedetectablebyourattack.
strategy,theattackercancorrectlyinfermembershipforhalfofthe
Forcompleteness,wealsoconsideranâ€œimputationâ€baseline[45]
honestpartyâ€™sdata,atafalse-positiverateofonly3%,comparedto
thatinferstheunknownattributefromthedatadistributionalone.
anerrorrateof24%withoutpoisoningâ€”animprovementofafactor
Thatis,givensamples (ð‘¥,ð‘¦) fromthedistributionD,wetraina
8Ã—.Weincluderesultsforotheruntargetedpoisoningstrategies,as
model to infer the value of one attribute of ð‘¥, given the other
wellasreplicationsonadditionaldatasetsinAppendixA.5.
attributesandclasslabelð‘¦.Thisbaselineletsusquantifyhowmuch
Wefurtherevaluatethisuntargetedattackagainstthesimple
extrainformationaboutasampleâ€™sunknownattributeisleakedbya
â€œlossclippingâ€defensefromSection4.3.7.Incontrasttothetargeted
modeltrainedonthatsample,comparedtowhatanadversarycould
case,wefindthatmoderateclipping(ð¶ =1)hasnoeffectonthe
infersimplyfrominherentcorrelationsinthedatadistribution.
untargetedattackandthatwithmorestringentclipping,themodelâ€™s
testaccuracyisseverelyreduced.
5.2 ExperimentalSetup
WerunourattackontheAdultdataset[37],atabulardatasetwith
5 ATTRIBUTEINFERENCEATTACKS
demographicinformationof48,842users.Thetargetmodelisa
OurresultsinSection4showthatdatapoisoningcansignificantly three-layerfeedforwardneuralnetworktopredictwhetherauserâ€™s
increase an adversaryâ€™s ability to infer membership of training incomeisabove$50K.Thetargetmodel(andtheattackâ€™sshadow
data.Wenowturntoattacksthatinferactualdata.Webeginby models)aretrainedonarandom50%ofthefullAdultdataset.Our
consideringattributeinferenceattacksinthissection,andconsider modelsachieve84%testaccuracy.Weconsiderattributeinference
canaryextractionattacksonlanguagemodelsinthenextsection. attacksthatinfereitherauserâ€™sstatedgender,orrelationshipstatus
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
100
10âˆ’1
10âˆ’2
10âˆ’3
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
numbers,etc.)fromalanguagemodeltrainedonanunlabeledtext
corpus.Languagemodelsareaprimetargetforpoisoningattacks,
astheirtrainingdatasetsareoftenminimallycurated[5,60].
Asin[13],weinjectcanariesintothetrainingdatasetofalan-
guagemodel,andthenevaluatewhetheranattacker(whomay
poison x16 poisonpartofthedataset)canrecoverthesecretcanary.Ourca-
poison x8 nariestaketheformð‘  =â€œPrefix â€,wherePrefixisan
poison x4 arbitrarystringthatisknown(fullyorpartially)totheattacker,fol-
poison x2 lowedbyarandom6-digitnumber.3Thissetupmirrorsascenario
No poison
whereasecretnumberappearsinastandardizedcontextknownto
baseline imputation
theadversary(e.g.,aPINinsertedinanHTMLinputform).
WetrainsmallvariantsoftheGPT-2model[56]ontheWikiText-
2dataset[47],astandardlanguagemodelingcorpusofapproxi-
mately3milliontokensoftextfromWikipedia.Weinjectacanary
Figure10:Targetedpoisoningattacksboostattributeinfer-
into this dataset with a 125-token prefix followed by a random
enceonAdult.Withoutpoisoning,theattack[45]performs
6-digitsecret(125tokensrepresentabout500characters;wealso
worsethanabaselineimputationthatinfersgenderbased
consideradversarieswithpartialknowledgeoftheprefixinSec-
oncorrelationstatistics.Withð‘Ÿ â‰¥ 4poisons,ourimproved
tion6.4).Givenatrainedmodelð‘“ ,theattackerpromptsthemodel
attacksurpassesthebaselineforthefirsttime. ðœƒ
withtheprefixfollowedbyall106 possiblevaluesofthecanary,
andranksthemaccordingtothemodelâ€™sloss.Following[13],we
computetheexposureofthesecretastheaveragenumberofbits
(afterbinarizingthisfeatureintoâ€œmarriedâ€andâ€œnotmarriedâ€as
leakedtotheadversary(seeEquation(2)).Tocontroltherandom-
in[45]).Wedefinetheattributesâ€œfemaleâ€andâ€œnotmarriedâ€asthe
nessfromthechoiceofrandomprefixandofrandomsecret,we
positiveclassineachcase(i.e.,atrue-positivecorrespondstothe
inject45differentcanariesintoamodel,andtrain45targetmodels
attackercorrectlyguessingthatauserisfemale,ornotmarried).
Wepick500targetpointsatrandom,andtrain10targetmodels
(foratotalof452 =2025differentprefix-secretcombinations).We
thenmeasuretheaverageexposureacrossallcombinations.
thatcontainthese500pointsintheirtrainingsets.Wefurthertrain
Weconsidertwopoisoningattackstrategiestoincreaseexposure
128shadowmodelsontrainingsetsthatcontainthese500targets
ofasecretcanary,thatrelyondifferentadversarialcapabilities:
withtheunknownattributechosenatrandom.Thetrainingsets
ofthetargetmodelsandshadowmodelsareaugmentedwiththe (1) Prefixpoisoningassumesthattheadversarycanselectthe
adversaryâ€™spoisoneddatasetð·
adv
thatcontainsð‘Ÿ âˆˆ{1,2,4,8,16} Prefixstringthatprecedesthesecretcanary.Thisthreat
mislabelledcopiesofeachtarget. modelcapturessettingswheretheattackercanselectatem-
Toevaluatetheimputationbaseline,wetrainthesamethree- plateinwhichusersecretsareinput.Alternatively,since
layerfeedforwardneuralnetworktopredictgender(orrelationship trainingsetsforlanguagemodelsareoftenconstructed
status)givenauserâ€™sotherfeaturesandclasslabel.Wetrainthis byconcatenating allofauserâ€™stextsources,thisattack
modelontheentireAdultdatasetexceptforthe500targetpoints. couldbeinstantiatedbyhavingtheattackersendames-
sagetothevictimbeforethevictimwritessomesecret
5.3 Results information.
(2) Suffix poisoning assumes that the adversary knows the
Ourattackforattributingauserâ€™sstatedgenderisplottedinFig-
Prefixstringprecedingthecanary,butcannotnecessarily
ure10.ResultsforinferringrelationshipstatusareinFigure26.
modifyit.Here,theadversaryinsertspoisonedcopiesof
Asformembershipinference,poisoningsignificantlyimprovesat-
thePrefixwithachosensuffixintothetrainingdata.
tributeinference.AtaFPRof0.1%,theattackof[45]hasaTPRof
1%,whileourattackwith16poisonsgetsaTPRof30%.Conversely, As we will see, both types of poisoning attacks significantly
toachieveaTPRof50%,theattackwithoutpoisoningincursaFPR increasetheexposureofcanaries.Anattackerthatcombinesboth
of39%,whileourattackwith16poisonshasaFPRof1.2%â€”an formsofattackcanreducetheirguessworktorecovercanariesby
errorreductionof33Ã—.Inparticular,theattackwithoutpoison- afactorof39Ã—,comparedtoabaselineattackwithoutpoisoning.
ingperformsworsethanthetrivialimputationbaseline.Accessto
anon-poisonedmodelthusdoesnotappeartoleakmoreprivate 6.1 CanaryExtractionwithCalibration
informationthanwhatcanbeinferredfromthedatadistribution. Weagainbeginbyshowingthatexistingcanaryextractionattacks
canbesignificantlyimprovedbyappropriatelycalibratingtheattack
6 EXTRACTIONINLANGUAGEMODELS usingshadowmodels(again,similartostate-of-the-artmembership
Intheprevioussections,wefocusedonattacksthatinferasingle inferenceattacks[11,41,58,70,74]).
bit ofinformationâ€”whetheranexampleisamemberornot(in Asabaseline,weruntheattackof[13],whichsimplyranksall
Section4),orthevalueofsomebinaryattributeoftheexample(in possiblecanaryvaluesaccordingtothetargetmodelâ€™sloss.Wefind
Section5).Wenowconsiderthemoreambitiousgoalofinferring
3Welimitourselvesto6-digitsecretswhichallowsustoefficientlyenumerateoverall
secretswithmuchhigherentropy.Following[13],weaimtoextract
possiblesecretvalueswhencomputingthesecretâ€™sexposure.Asin[13],wecouldalso
well-formattedsecrets(e.g.,creditcardnumbers,socialsecurity considerlongersecretsandapproximateexposurebysampling.
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
8
7
6
5
4
3
1 2 4 8 16 32 64 128
Number of shadow models
)stib(
erusopxE
10.5
10.0
9.5
With calibration 9.0
Without calibration
8.5
8.0
7.5
7.0
6.5
Best Foreign Code Random Worst
Figure11:Calibrationwithshadowmodelssignificantlyim-
provestheattackof[13]forextractingcanaries.
thatthisattackachievesonlyalowcanaryexposureof3.1bitson
averageinoursetting(i.e,theadversarylearnslessthan1digit
ofthesecret).4Wefindthateventhoughthemodelâ€™slossonthe
random6-digitsecretdoesdecreasethroughouttraining,thereare
manyother6-digitnumbersthatareapriorimuchmorelikelyand
thatthereforeyieldlowerlosses(suchas000000,or123456).
Theissuehereisagainoneofcalibration.Anylanguagemodel
trainedonalargedatasetwilltendtoassignhigherlikelihoodto
thenumber123456thanto,say,thenumber418463.However,a
modeltrainedwiththecanary418463willhaveacomparatively
muchhigherconfidenceinthiscanarythanalanguagemodelthat
wasnottrainedonthisspecificcanary.
Aswedidwithourmembershipandattributeinferenceattacks,
wethusfirsttrainanumberofshadowmodels.Wetrainð‘ shadow
modelsð‘” onrandomsubsetsofWikiText(withoutanyinserted
ð‘–
canaries).Then,foratargetmodelð‘“ ,prefixð‘andcanaryguessð‘,
ðœƒ
weassigntoð‘thecalibratedconfidence:
ð‘
1 âˆ‘ï¸
logð‘“ ðœƒ(ð‘+ð‘)âˆ’
ð‘
logð‘” ð‘–(ð‘+ð‘) . (3)
ð‘–=1
Apotentialcanaryvaluesuchas123456willhavealowcalibrated
score,asallmodelsassignithighconfidence.Incontrast,thetrue
canary(e.g.,418643)willhavehighcalibratedconfidenceasonly
thetargetmodelð‘“ assignsamoderatelyhighconfidencetoit.We
ðœƒ
thencomputeexposureexactlyasinEquation(2),withpossible
canaryvaluesrankedaccordingtotheircalibratedconfidence.
Figure11showsthattheuseofshadowmodelsvastlyincreases
exposureofcanaries.Withjust2shadowmodels,weobtainan
averageexposureof7.1bits,areductioninguessworkof16Ã—
comparedtoanon-calibratedattack.Withadditionalshadow
models,theexposureincreasesmoderatelyto7.4bits.Conversely,
thefractionofcanariesrecoveredinfewerthan100guessesin-
creasesfrom0.1%to10%withcalibration(animprovementof100Ã—).
6.2 PrefixPoisoning
Thefirstpoisoningattackweconsiderisonewheretheadversary
canchoosetheprefixthatprecedesthesecretcanary.Weevaluate
4Carlinietal.[13]reporthigherexposuresfornumericsecretsbecausetheyuseworse
models(LSTMs)trainedonsimplerdatasetsthatcontainveryfewnumbers.
)stib(
erusopxE
Prefix poisoning
No poison
Figure 12: Secret canaries are easier to extract if the ad-
versarycanforcethemtoappearinanout-of-distribution
context. Canaries that appear after a piece of source code
(Code),uniformlyrandomtokens(Random),orasequence
oftokenswithworst-caseloss(Worst)aresignificantlymore
exposedthancanariesthatappearafterrandomWikiText
sentences(Nopoisonbaseline).Insertingcanariesaftersen-
tencesinanon-Englishlanguage(Foreign)orasequenceof
tokenswithbest-caseloss(Best)doesnotincreaseexposure.
theimpactofvariousout-of-distributionprefixchoicesontheex-
posureofthesecretsthatsucceedthem.Wepickfiveprefixeseach
fromthefollowingdistributions:
â€¢ Foreign:theprefixisinalanguageotherthanEnglish,with
anon-Latinalphabet:Chinese,Japanese,Russian,Hebrew
orArabic.
â€¢ Code:theprefixisapieceofsourcecode(inJavaScript,
Java,C,HaskellorRust).
â€¢ Random:tokenssampledfromGPT-2â€™svocabulary.
â€¢ Best:aninitialrandomtokenprefixfollowedbygreedily
samplingthemostlikelytokenfromapretrainedmodel.
â€¢ Worst:aninitialrandomtokenprefixfollowedbygreedily
samplingtheleast-likelytokenfromapretrainedmodel.
Figure12showsthatcanariesthatappearinâ€œdifficultâ€contexts
(wherethemodelhasdifficultypredictingthenexttoken)have
muchhigherexposurethancanariesthatappearinâ€œeasyâ€contexts.5
6.3 SuffixPoisoning
Whiletheabilitytochooseorinfluenceasecretvalueâ€™sprefixmay
existinsomesettings,itisastrongassumptionontheadversary.We
thusnowturntoamoregeneralsettingwheretheprefixpreceding
asecretcanaryisfixedandout-of-controloftheattacker.
We consider attacks inspired by the mislabeling attacks that
weresuccessfulformembershipinferenceandattributeinference.
Yet,aslanguagemodelsareunsupervised,wecannotâ€œmislabelâ€a
sentence.Instead,weproposeasuffixpoisoningattackthatinserts
theknownprefixfollowedbyanarbitrarysuffixmanytimesinto
thedataset(therebyâ€œmislabelingâ€thetokensthatsucceedtheprefix,
i.e.,thecanary).Theattackâ€™srationaleisthatthepoisonedmodel
willhaveanextremelylowconfidenceinanyvalueforthecanary,
thusmaximizingtherelativeinfluenceofthetruecanary(similarly
5Thenon-EnglishlanguageswechosedoappearinsomeWikipediaarticlesincluded
inWikiText.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
13
12
11
10
9
8
7
6
1 2 4 8 16 32 64 128
Number of Poisons
)stib(
erusopxE
12 Prefix+suffix poisoning
Suffix poisoning 11
No poison 10
9
8
7
6
5
4 8 16 32 64 96 128
Tokens of prefix known
Figure 13: Canaries are easier to extract if the model has
highconfidenceinanincorrectcontinuationoftheprefix.
We insert the prefix padded with zeros 1 â‰¤ ð‘Ÿ â‰¤ 128 times
intothetrainingsettoincreaseexposure.Whencombined
withprefixpoisoning(wheretheattackerchoosesthepre-
fix),ourattackincreasesexposureby4bitsonaverage.
tohowourMIattackpoisonsthemodeltohaveverylowconfidence
inthetruelabel,tomaximizetheinfluenceofthetargetedpoint).
Werepeattheprefix1 â‰¤ ð‘Ÿ â‰¤ 128times,paddedbyastream
ofzeros(weconsiderother,lesseffectivesuffixchoicesinAppen-
dixC.1).Figure13showsthesuccessrateoftheattack.Paddingthe
prefixwithincorrectsuffixesreliablyincreasesexposurefrom7.4
bitsto9.2bitsafter64poisoninsertions(0.3%ofthedatasetsize).
Finally,weconsiderapowerfulattackerthatcombinesbothour
prefix-poisoningandsuffix-poisoningstrategies,byfirstchoosing
anout-of-distributionprefixthatwillprecedethesecretcanary,and
furtherinsertingthisprefixpaddedbyzerosð‘Ÿtimesintothetraining
data.Thisattackincreasesexposureto11.4bitsonaveragewith
64poisoninsertions.Forhalfofthecanaries,theattackerfindsthe
secretinfewerthan230guesses,comparedto9,018guesseswithout
poisoningâ€”animprovementof39Ã—.Conversely,theproportion
ofcanariesthattheattackercanrecoverwithatmost100guesses
increasesfrom10%withoutpoisoningto42%withpoisoning.
6.4 AttackswithRelaxedCapabilities
Thelanguagemodelpoisoningattacksweevaluatedsofarassumed
that(1)theadversaryknowstheentireprefixthatprecedesacanary;
(2)theadversaryhastheabilitytotrainshadowmodels.Below,we
relaxbothoftheseassumptionsinturn.
Partialknowledgeoftheprefix. InFigure14,wemeasureexposure
asafunctionofthenumberoftokensofthePrefixstringknown
totheattacker.Weassumetheattackerknowstheð‘›lasttokensof
theprefix(about4ð‘›characters)immediatelyprecedingthecanary.
Theattackerthusqueriesthemodelwithonlytheseð‘›tokensof
knowncontexttoextractacanary.Moreover,whenpoisoningthe
model,theattackerhastheabilitytochoosetheð‘›lasttokensofthe
prefix,andtoinsertthemtogetherwithanarbitrarysuffix64times
intothedataset.Wefindthattheattackâ€™sperformanceincreases
steadilywiththenumberoftokensknowntotheadversary.This
mirrorsthefindingsofCarlinietal.[12],whoshowthatprompting
)stib(
erusopxE
Prefix+suffix poisoning
No poison
Figure14:Ourprivacy-poisoningattackperformsbetter,the
morecontextisknowntotheadversary.Werunourattack
inasettingwheretheadversaryknowsthelastð‘˜tokensim-
mediatelyprecedingthesecretcanary.Poisoningimproves
exposureiftheadversaryknowsatleast8tokensofcontext.
alanguagemodelwithlongerprefixesincreasesthelikelihoodof
extractingmemorizedcontent.Aslongastheattackerknowsmore
thanð‘› = 8tokensofcontext(6Englishwordsonaverage),they
canincreaseexposureofsecretsbypoisoningthemodel.
Attackswithoutshadowmodels. InSection6.1weshowedthat
canaryextractionattacksaresignificantlyimprovediftheadver-
saryhastheabilitytotrainshadowmodelsthatcloselymimicthe
behaviorofthetargetmodel.
This assumption is standard in the literature on privacy at-
tacks[11,41,58,62,70,74],andweshowthatasfewas2shadow
modelsprovidenearlythesamebenefitas>100models.Yet,even
trainingasingleshadowmodelmightbeexcessivelyexpensivefor
verylargelanguagemodels(priorworkhassuggestedthatexisting
publiclanguagemodelscouldbeusedasproxiesforshadowmod-
els[14]).Incontrast,theabilitytopoisonalargelanguagemodelâ€™s
trainingsetmaybemoreaccessible,especiallysincethesemodels
aretypicallytrainedonlargeminimallycurateddatasources[5,60].
Wefindthatpoisoningsignificantlyboostsexposureevenifthe
attackercannottrainanyshadowmodelsandusesthebaselineat-
tackof[13].Interestingly,theabilitytopoisonthedatasetpro-
videsroughlythesamebenefitastheabilitytotrainshadow
models:witheitherability,exposureincreasesfrom3.1bitsto7.3
and7.4bitsrespectivelyâ€”areductioninaverageguessworkof18-
20Ã—.Combiningbothabilities(i.e.,poisoningthetargetmodeland
trainingshadowmodels)compoundstoanadditional16Ã—decrease
inaverageguesswork(anaverageexposureof11.4bits).
7 DISCUSSIONANDCONCLUSION
Weintroduceanewattackonmachinelearningwhereanadversary
poisonsatrainingsettoharmtheprivacyofotherusersâ€™data.For
membershipinference,attributeinference,anddataextraction,we
showhowattackscantamperwithtrainingdata(aslittleas<0.1%)
toincreaseprivacyleakagebyoneortwoorders-of-magnitude.
Byblurringthelinesbetweenâ€œworst-caseâ€andâ€œaverage-caseâ€
privacyleakageindeepneuralnetworks,ourattackshavevarious
implications,discussedbelow,fortheprivacyexpectationsofusers
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
andprotocoldesignersincollaborativelearningsettings. REFERENCES
[1] MartinAbadi,AndyChu,IanGoodfellow,H.BrendanMcMahan,IlyaMironov,
Untrusteddataisnotonlyathreattointegrity.Largeneuralnetworks
KunalTalwar,andLiZhang.Deeplearningwithdifferentialprivacy.InACM
aretrainedonmassivedatasetswhicharehardtocurate.Thisissue SIGSACConferenceonComputerandCommunicationsSecurity,page308â€“318.
isexacerbatedformodelstrainedindecentralizedsettings(e.g.,fed- ACM,2016.
[2] YoshinoriAono,TakuyaHayashi,LihuaWang,andShihoMoriai. Privacy-
eratedlearning,orsecureMPC)wherethedataofindividualusers
preservingdeeplearningviaadditivelyhomomorphicencryption.IEEETransac-
cannotbeinspected.Priorworkobservesthatprotectingmodel tionsonInformationForensicsandSecurity,13(5):1333â€“1345,2017.
integrityischallenginginsuchsettings[4,6,7,23,31,51,61,64]. [3] EugeneBagdasaryanandVitalyShmatikov.Blindbackdoorsindeeplearning
models.InUSENIXSecuritySymposium,pages1505â€“1521,2021.
Ourworkhighlightsanew,orthogonalthreattotheprivacyofthe [4] EugeneBagdasaryan,AndreasVeit,YiqingHua,DeborahEstrin,andVitaly
modelâ€™strainingdata,whenpartofthetrainingdataisadversarial. Shmatikov.Howtobackdoorfederatedlearning.InInternationalConferenceon
ArtificialIntelligenceandStatistics,pages2938â€“2948.PMLR,2020.
Thus,eveninsettingswherethreatstomodelintegrityarenota
[5] EmilyMBender,TimnitGebru,AngelinaMcMillan-Major,andShmargaret
primaryconcern,modeldeveloperswhocareaboutprivacymay Shmitchell.Onthedangersofstochasticparrots:Canlanguagemodelsbetoo
stillneedtoaccountforpoisoningattacksanddefendagainstthem. big? InACMConferenceonFairness,Accountability,andTransparency,pages
610â€“623,2021.
Neuralnetworksarepoorâ€œidealfunctionalitiesâ€.Thereisalineof [6] ArjunNitinBhagoji,SupriyoChakraborty,PrateekMittal,andSeraphinCalo.
Analyzingfederatedlearningthroughanadversariallens.InInternationalCon-
workthatcollaborativelytrainsMLmodelsusingsecuremultiparty ferenceonMachineLearning,pages634â€“643.PMLR,2019.
computation(MPC)protocols[2,49,50,69].Theseprotocolsare [7] BattistaBiggio,BlaineNelson,andPavelLaskov. Poisoningattacksagainst
supportvectormachines.InInternationalConferenceonMachineLearning,page
guaranteedtoleaknothingmorethananidealfunctionalitythat
1467â€“1474,2012.
computesthedesiredfunction[25,73].Suchprotocolswereini- [8] DanielBleichenbacher.Chosenciphertextattacksagainstprotocolsbasedonthe
tiallydesignedforcomputationswherethisidealleakageiswell RSAencryptionstandardPKCS#1.InAnnualInternationalCryptologyConference,
pages1â€“12.Springer,1998.
understoodandbounded(e.g.,inYaoâ€™smillionairesproblem[73],the
[9] FranziskaBoenisch,AdamDziedzic,RoeiSchuster,AliShahinShamsabadi,Ilia
functionalwaysleaksexactlyonebitofinformation).Yet,forflexi- Shumailov,andNicolasPapernot.Whenthecuriousabandonhonesty:Federated
blefunctionssuchasneuralnetworks,theidealleakageismuch learningisnotprivate.arXivpreprintarXiv:2112.02918,2021.
[10] DanBonehandVictorShoup. AGraduateCourseinAppliedCryptography.
harder to characterize and bound [13, 14, 62]. Worse, our work http://toc.cryptobook.us/,2020.
demonstratesthatanadversarythathonestlyfollowsthepro- [11] NicholasCarlini,SteveChien,MiladNasr,ShuangSong,AndreasTerzis,and
FlorianTramer. Membershipinferenceattacksfromfirstprinciples. InIEEE
tocolcanincreasetheamountofinformationleakedbythe SymposiumonSecurityandPrivacy,pages1897â€“1914.IEEE,2022.
ideal functionality, solely by modifying their inputs. Thus, [12] NicholasCarlini,DaphneIppolito,MatthewJagielski,KatherineLee,Florian
thesecuritymodelofMPCfailstocharacterizeallmaliciousstrate- Tramer,andChiyuanZhang.Quantifyingmemorizationacrossneurallanguage
models.arXivpreprintarXiv:2202.07646,2022.
giesthatbreachusersâ€™privacyincollaborativelearningscenarios.
[13] NicholasCarlini,ChangLiu,ÃšlfarErlingsson,JernejKos,andDawnSong.These-
cretsharer:Evaluatingandtestingunintendedmemorizationinneuralnetworks.
Worst-caseprivacyguaranteesmattertoeveryone.Priorworkhas
InUSENIXSecuritySymposium,pages267â€“284,2019.
foundthatitismainlyoutliersthatareatriskofprivacyattacks[11, [14] NicholasCarlini,FlorianTramer,EricWallace,MatthewJagielski,ArielHerbert-
18,74,75].Yet,beinganoutlierisafunctionofnotonlythedata Voss,KatherineLee,AdamRoberts,TomBrown,DawnSong,UlfarErlingsson,
etal.Extractingtrainingdatafromlargelanguagemodels.InUSENIXSecurity
pointitself,butalsoofitsrelationtootherpointsinthetrainingset. Symposium,2021.
Indeed,ourworkshowsthatasmallnumberofpoisonedsamples [15] MosesCharikar,JacobSteinhardt,andGregoryValiant.Learningfromuntrusted
data.InACMSIGACTSymposiumonTheoryofComputing,pages47â€“60,2017.
suffice to transform inlier points into outliers. As such, our at- [16] IliasDiakonikolas,GautamKamath,DanielKane,JerryLi,JacobSteinhardt,and
tacksreducetheâ€œaverage-caseâ€privacyleakagetowardsthe AlistairStewart.Sever:Arobustmeta-algorithmforstochasticoptimization.In
â€œworst-caseâ€leakage.Ourresultsimplythatmethodsthataudit InternationalConferenceonMachineLearning,pages1596â€“1606.PMLR,2019.
[17] CynthiaDwork,FrankMcSherry,KobbiNissim,andAdamSmith.Calibrating
privacywithaverage-casecanaries[13,44,57,65,77]mightun-
noisetosensitivityinprivatedataanalysis.InTheoryofCryptographyConference,
derestimatetheactualworst-caseleakageunderasmallpoisoning pages265â€“284.Springer,2006.
attack,andworst-caseauditingapproaches[32,54]mightmore [18] VitalyFeldman.Doeslearningrequirememorization?Ashorttaleaboutalong
tail.InACMSIGACTSymposiumonTheoryofComputing,pages954â€“959,2020.
accuratelymeasureamodelâ€™sprivacyformostusers. [19] LiamFowl,JonasGeiping,StevenReich,YuxinWen,WojtekCzaja,MicahGold-
blum,andTomGoldstein.Decepticons:Corruptedtransformersbreachprivacy
Ourworkshows,yetagain,thatdataprivacyandintegrityare infederatedlearningforlanguagemodels.arXivpreprintarXiv:2201.12675,2022.
intimatelyconnected.Whilethisconnectionhasbeenextensively [20] LiamFowl,MicahGoldblum,Ping-yehChiang,JonasGeiping,WojciechCzaja,
andTomGoldstein. Adversarialexamplesmakestrongpoisons. Advancesin
studiedinotherareasofcomputersecurityandcryptography,we
NeuralInformationProcessingSystems,34,2021.
hopethatfutureworkcanshedfurtherlightontheinterplaybe- [21] MattFredrikson,SomeshJha,andThomasRistenpart.Modelinversionattacks
tweendatapoisoningandprivacyleakageinmachinelearning. thatexploitconfidenceinformationandbasiccountermeasures.InACMSIGSAC
ConferenceonComputerandCommunicationsSecurity,pages1322â€“1333,2015.
[22] MatthewFredrikson,EricLantz,SomeshJha,SimonLin,DavidPage,andThomas
ACKNOWLEDGMENTS Ristenpart.Privacyinpharmacogenetics:Anend-to-endcasestudyofpersonal-
izedwarfarindosing.InUSENIXSecuritySymposium,2014.
WethankAlinaOprea,HarshChaudhari,MartinStrobel,Abhradeep
[23] JonasGeiping,LiamHFowl,W.RonnyHuang,WojciechCzaja,GavinTay-
Thakurta,ThomasSteinke,andAndreasTerzisforhelpfuldiscus- lor,MichaelMoeller,andTomGoldstein.Witchesâ€™brew:Industrialscaledata
sionsandfeedback. poisoningviagradientmatching.InInternationalConferenceonLearningRepre-
sentations,2021.
Partoftheworkpublishedhereisderivedfromacapstoneproject [24] YoelGluck,NealHarris,andAngeloPrado.Breach:revivingthecrimeattack.
submittedtowardsaBSc.from,andfinanciallysupportedby,Yale- http://breachattack.com,2013.
[25] OdedGoldreich,SilvioMicali,andAviWigderson. Howtoplayanymental
NUSCollege,anditispublishedherewithpriorapprovalfromthe
game,oracompletenesstheoremforprotocolswithhonestmajority.InACM
College. SIGACTSymposiumonTheoryofComputing,1987.
[26] TianyuGu,KangLiu,BrendanDolan-Gavitt,andSiddharthGarg. BadNets:
Evaluatingbackdooringattacksondeepneuralnetworks.IEEEAccess,7,2019.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
[27] NealGupta,WRonnyHuang,LiamFowl,ChenZhu,SoheilFeizi,TomGoldstein, IntelligenceandSecurity,pages27â€“38,2017.
andJohnDickerson. Strongbaselinedefensesagainstclean-labelpoisoning [52] MiladNasr,RezaShokri,andAmirHoumansadr.Machinelearningwithmem-
attacks.https://openreview.net/forum?id=B1xgv0NtwH,2019. bershipprivacyusingadversarialregularization.InACMSIGSACConferenceon
[28] BrilandHitaj,GiuseppeAteniese,andFernandoPerez-Cruz.Deepmodelsunder ComputerandCommunicationsSecurity,pages634â€“646,2018.
theGAN:Informationleakagefromcollaborativedeeplearning.InACMSIGSAC [53] MiladNasr,RezaShokri,andAmirHoumansadr.Comprehensiveprivacyanal-
ConferenceonComputerandCommunicationsSecurity,pages603â€“618,2017. ysisofdeeplearning:Passiveandactivewhite-boxinferenceattacksagainst
[29] NilsHomer,SzabolcsSzelinger,MargotRedman,DavidDuggan,WaibhavTembe, centralizedandfederatedlearning.InIEEESymposiumonSecurityandPrivacy,
JillMuehling,JohnVPearson,DietrichAStephan,StanleyFNelson,andDavidW pages739â€“753.IEEE,2019.
Craig.Resolvingindividualscontributingtraceamountsofdnatohighlycomplex [54] MiladNasr,ShuangSongi,AbhradeepThakurta,NicolasPapemoti,andNicholas
mixturesusinghigh-densitysnpgenotypingmicroarrays. PLoSgenetics,4(8), Carlin.Adversaryinstantiation:Lowerboundsfordifferentiallyprivatemachine
2008. learning.InIEEESymposiumonSecurityandPrivacy,pages866â€“882.IEEE,2021.
[30] Lin-ShungHuang,ZackWeinberg,ChrisEvans,andCollinJackson.Protecting [55] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
browsersfromcross-originCSSattacks.InACMSIGSACConferenceonComputer SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
andCommunicationsSecurity,pages619â€“629,2010. etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.In
[31] MatthewJagielski,AlinaOprea,BattistaBiggio,ChangLiu,CristinaNita-Rotaru, InternationalConferenceonMachineLearning,pages8748â€“8763.PMLR,2021.
andBoLi.Manipulatingmachinelearning:Poisoningattacksandcountermea- [56] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
suresforregressionlearning.InIEEESymposiumonSecurityandPrivacy,pages Sutskever,etal.Languagemodelsareunsupervisedmultitasklearners.OpenAI
19â€“35.IEEE,2018. blog,2019.
[32] MatthewJagielski,JonathanUllman,andAlinaOprea.Auditingdifferentially [57] SwaroopRamaswamy,OmThakkar,RajivMathews,GalenAndrew,HBren-
privatemachinelearning:HowprivateisprivateSGD? AdvancesinNeural danMcMahan,andFranÃ§oiseBeaufays.Trainingproductionlanguagemodels
InformationProcessingSystems,33:22205â€“22216,2020. withoutmemorizinguserdata.arXivpreprintarXiv:2009.10031,2020.
[33] BargavJayaraman,LingxiaoWang,DavidEvans,andQuanquanGu.Revisiting [58] AlexandreSablayrolles,MatthijsDouze,CordeliaSchmid,YannOllivier,and
membershipinferenceunderrealisticassumptions.InProceedingsonPrivacy HervÃ©JÃ©gou.White-boxvsblack-box:Bayesoptimalstrategiesformembership
EnhancingTechnologies,2021. inference. InInternationalConferenceonMachineLearning,pages5558â€“5567.
[34] JinyuanJia,AhmedSalem,MichaelBackes,YangZhang,andNeilZhenqiang PMLR,2019.
Gong.MemGuard:Defendingagainstblack-boxmembershipinferenceattacks [59] HadiSalman,AndrewIlyas,LoganEngstrom,SaiVemprala,AleksanderMadry,
viaadversarialexamples.InACMSIGSACConferenceonComputerandCommu- andAshishKapoor.Unadversarialexamples:Designingobjectsforrobustvision.
nicationsSecurity,pages259â€“274,2019. AdvancesinNeuralInformationProcessingSystems,34,2021.
[35] PeterKairouz,HBrendanMcMahan,BrendanAvent,AurÃ©lienBellet,MehdiBen- [60] RoeiSchuster,CongzhengSong,EranTromer,andVitalyShmatikov.Youauto-
nis,ArjunNitinBhagoji,KallistaBonawitz,ZacharyCharles,GrahamCormode, completeme:Poisoningvulnerabilitiesinneuralcodecompletion.InUSENIX
RachelCummings,etal. Advancesandopenproblemsinfederatedlearning. SecuritySymposium,pages1559â€“1575,2021.
FoundationsandTrendsÂ®inMachineLearning,14(1â€“2):1â€“210,2021. [61] AliShafahi,WRonnyHuang,MahyarNajibi,OctavianSuciu,ChristophStuder,
[36] JohnKelsey.Compressionandinformationleakageofplaintext.InInternational TudorDumitras,andTomGoldstein.Poisonfrogs!Targetedclean-labelpoisoning
WorkshoponFastSoftwareEncryption,pages263â€“276.Springer,2002. attacksonneuralnetworks.AdvancesinNeuralInformationProcessingSystems,
[37] RonnyKohaviandBarryBecker.UCImachinelearningrepository:Adultdata 31,2018.
set.https://archive.ics.uci.edu/ml/machine-learning-databases/adult,1996. [62] RezaShokri,MarcoStronati,CongzhengSong,andVitalyShmatikov.Member-
[38] AlexKrizhevskyandGeoffreyHinton.Learningmultiplelayersoffeaturesfrom shipinferenceattacksagainstmachinelearningmodels.InIEEESymposiumon
tinyimages,2009. SecurityandPrivacy,pages3â€“18.IEEE,2017.
[39] YingqiLiu,ShiqingMa,YousraAafer,Wen-ChuanLee,JuanZhai,Weihang [63] CongzhengSong,ThomasRistenpart,andVitalyShmatikov.Machinelearning
Wang,andXiangyuZhang.Trojaningattackonneuralnetworks.InNetwork modelsthatremembertoomuch.InACMSIGSACConferenceonComputerand
andDistributedSystemSecuritySymposium,2018. CommunicationsSecurity,pages587â€“601,2017.
[40] YuntaoLiu,YangXie,andAnkurSrivastava. Neuraltrojans. In2017IEEE [64] OctavianSuciu,RaduMarginean,YigitcanKaya,HalDaumeIII,andTudor
InternationalConferenceonComputerDesign(ICCD),pages45â€“48.IEEE,2017. Dumitras.WhendoesmachinelearningFAIL?Generalizedtransferabilityfor
[41] YunhuiLong,LeiWang,DiyueBu,VincentBindschaedler,XiaofengWang, evasionandpoisoningattacks.InUSENIXSecuritySymposium,2018.
HaixuTang,CarlAGunter,andKaiChen.Apragmaticapproachtomembership [65] OmDipakbhaiThakkar,SwaroopRamaswamy,RajivMathews,andFrancoise
inferencesonmachinelearningmodels.InIEEEEuropeanSymposiumonSecurity Beaufays.Understandingunintendedmemorizationinlanguagemodelsunder
andPrivacy,pages521â€“534.IEEE,2020. federatedlearning.InWorkshoponPrivacyinNaturalLanguageProcessing,2021.
[42] AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,and [66] BrandonTran,JerryLi,andAleksanderMadry.Spectralsignaturesinbackdoor
AdrianVladu.Towardsdeeplearningmodelsresistanttoadversarialattacks.In attacks.AdvancesinNeuralInformationProcessingSystems,31,2018.
InternationalConferenceonLearningRepresentations,2018. [67] AlexanderTurner,DimitrisTsipras,andAleksanderMadry. Label-consistent
[43] S.Mahloujifar,E.Ghosh,andM.Chase.Propertyinferencefrompoisoning.In backdoorattacks.arXivpreprintarXiv:1912.02771,2019.
IEEESymposiumonSecurityandPrivacy,pages1569â€“1569,LosAlamitos,CA, [68] SergeVaudenay.SecurityflawsinducedbyCBCpaddingâ€”applicationstoSSL,
USA,may2022.IEEEComputerSociety. IPSEC,WTLS... InInternationalConferenceontheTheoryandApplicationsof
[44] ManiMalekEsmaeili,IlyaMironov,KarthikPrasad,IgorShilov,andFlorian CryptographicTechniques,pages534â€“545.Springer,2002.
Tramer.Antipodesoflabeldifferentialprivacy:PATEandALIBI.Advancesin [69] SameerWagh,DivyaGupta,andNishanthChandran.SecureNN:3-partysecure
NeuralInformationProcessingSystems,34,2021. computationforneuralnetworktraining. ProceedingsonPrivacyEnhancing
[45] ShaguftaMehnaz,SayantonVDibbo,EhsanulKabir,NinghuiLi,andElisaBertino. Technologies,2019(3):26â€“49,2019.
Areyoursensitiveattributesprivate?Novelmodelinversionattributeinference [70] LaurenWatson,ChuanGuo,GrahamCormode,andAlexandreSablayrolles.On
attacksonclassificationmodels.InUSENIXSecuritySymposium,2022. theimportanceofdifficultycalibrationinmembershipinferenceattacks. In
[46] LucaMelis,CongzhengSong,EmilianoDeCristofaro,andVitalyShmatikov.Ex- InternationalConferenceonLearningRepresentations,2022.
ploitingunintendedfeatureleakageincollaborativelearning.InIEEESymposium [71] YuxinWen,JonasA.Geiping,LiamFowl,MicahGoldblum,andTomGoldstein.
onSecurityandPrivacy,pages691â€“706.IEEE,2019. Fishingforuserdatainlarge-batchfederatedlearningviagradientmagnification.
[47] StephenMerity,CaimingXiong,JamesBradbury,andRichardSocher.Pointer InInternationalConferenceonMachineLearning,pages23668â€“23684.PMLR,2022.
sentinelmixturemodels.InInternationalConferenceonLearningRepresentations, [72] YonghuiWu,MikeSchuster,ZhifengChen,QuocV.Le,MohammadNorouzi,
2017. WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,Jeff
[48] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg- Klingner,ApurvaShah,MelvinJohnson,XiaobingLiu,LukaszKaiser,Stephan
Kirkpatrick,andRezaShokri. Quantifyingprivacyrisksofmaskedlanguage Gouws,YoshikiyoKato,TakuKudo,HidetoKazawa,KeithStevens,George
modelsusingmembershipinferenceattacks. arXivpreprintarXiv:2203.03929, Kurian,NishantPatil,WeiWang,CliffYoung,JasonSmith,JasonRiesa,AlexRud-
2022. nick,OriolVinyals,GregCorrado,MacduffHughes,andJeffreyDean.Googleâ€™s
[49] PaymanMohasselandPeterRindal. ABY3:Amixedprotocolframeworkfor neuralmachinetranslationsystem:Bridgingthegapbetweenhumanandma-
machinelearning.InACMSIGSACConferenceonComputerandCommunications chinetranslation.arXivpreprintarXiv:1609.08144,2016.
Security,pages35â€“52,2018. [73] AndrewCYao.Protocolsforsecurecomputations.In23rdannualSymposium
[50] PaymanMohasselandYupengZhang.SecureML:Asystemforscalableprivacy- onFoundationsofComputerScience,pages160â€“164.IEEE,1982.
preservingmachinelearning.InIEEESymposiumonSecurityandPrivacy,pages [74] JiayuanYe,AadyaaMaddi,SasiKumarMurakonda,andRezaShokri.Enhanced
19â€“38.IEEE,2017. membershipinferenceattacksagainstmachinelearningmodels.InACMSIGSAC
[51] LuisMuÃ±oz-GonzÃ¡lez,BattistaBiggio,AmbraDemontis,AndreaPaudice,Vasin ConferenceonComputerandCommunicationsSecurity,2022.
Wongrassamee,EmilCLupu,andFabioRoli.Towardspoisoningofdeeplearning [75] SamuelYeom,IreneGiacomelli,MattFredrikson,andSomeshJha.Privacyrisk
algorithmswithback-gradientoptimization. InACMWorkshoponArtificial inmachinelearning:Analyzingtheconnectiontooverfitting.InIEEEComputer
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
SecurityFoundationsSymposium,pages268â€“282.IEEE,2018. informationleakageofupdatestonaturallanguagemodels. InACMSIGSAC
[76] SergeyZagoruykoandNikosKomodakis.Wideresidualnetworks.InBritish ConferenceonComputerandCommunicationsSecurity,pages363â€“375,2020.
MachineVisionConference,2016. [78] ChenZhu,WRonnyHuang,HengduoLi,GavinTaylor,ChristophStuder,and
[77] SantiagoZanella-BÃ©guelin,LukasWutschitz,ShrutiTople,VictorRÃ¼hle,An- TomGoldstein.Transferableclean-labelpoisoningattacksondeepneuralnets.
drewPaverd,OlgaOhrimenko,BorisKÃ¶pf,andMarcBrockschmidt.Analyzing InInternationalConferenceonMachineLearning,pages7614â€“7623.PMLR,2019.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
A ADDITIONALEXPERIMENTSFOR
MEMBERSHIPINFERENCEATTACKS
A.1 ResultsonCIFAR-100
InFigure15,wereplicatetheexperimentfromSection4.2onCIFAR-
100.TheexperimentalsetupisexactlythesameasonCIFAR-10.
100
6Ã—10âˆ’1
4Ã—10âˆ’1
3Ã—10âˆ’1
2Ã—10âˆ’1
10âˆ’1
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
poison x16
poison x8
poison x4
poison x2
poison x1
No poison
Figure15:Targetedpoisoningattacksboostmembershipin-
ferenceonCIFAR-100.For250randomdatapoints,weinsert
1to16mislabelledcopiesofthepointintothetrainingset,
andruntheMIattackof[11]with128shadowmodels.
In Figure 16, we replicate the experiment in Figure 6, where
wevarythechoiceoftargetclassformislabelledpoisons.Asfor
CIFAR-10,wemislabeltheð‘Ÿ poisonspertargetas:(1)thesame
randomincorrectclassforeachoftheð‘Ÿ samples(random);(2)the
mostlikelyincorrectclass(best);theleast-likelyclass(worst);ora
differentrandomincorrectclassforeachoftheð‘Ÿ poisonedcopies
(random-multi).
SimilarlytoCIFAR-10,wefindthatthechoiceofrandomlabel
matterslittleaslongasitisusedconsistentlyforallð‘Ÿ poisons,with
therandomstrategyperformingbest.
100
6Ã—10âˆ’1
4Ã—10âˆ’1
3Ã—10âˆ’1
2Ã—10âˆ’1
10âˆ’1
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
A.2 AttacksThatModifytheTarget
Inthissection,weconsideralternativepoisoningstrategiesthat
also modify the target sample ð‘¥, and not just the class labelð‘¦.
Allstrategiesweconsideredperformedworsethanourbaseline
strategythanmislabelstheexactsampleð‘¥ (â€œexactâ€inFigure17).
Wefirstconsiderstrategiesthatmimicthepolytopepoisoning
strategyof[78],whichâ€œsurroundsâ€thetargetexamplewithmisla-
beledsamplesinfeaturespace.Whiletheoriginalattackdoesthisto
enhancethetransferabilityofclean-labelpoisoningattacks,ouraim
isinsteadtomaximizetheinfluenceofthetargetedexamplewhen
itisamember.Tothisend,insteadofaddingð‘Ÿ identicalmislabeled
copiesofð‘¥ intothetrainingset,weinsteadaddð‘Ÿ mislabelednoisy
versionsofð‘¥,orð‘Ÿ mislabeledaugmentationsofð‘¥ (e.g.,rotations
andshifts).Figure17showsthatbothstrategiesperformworse
thanourbaselineattack(forð‘Ÿ =8poisonspertarget).
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
poison x4 (random)
poison x4 (best)
poison x4 (worst)
poison x4 (random-multi)
No poison
Figure16:ComparisonofmislabellingstrategiesonCIFAR-
100.Assigningthesamerandomincorrectlabeltothe4poi-
sonedcopiesofthetargetperformsbetterthanmislabeling
as the 2nd most likely class (best) or the least likely class
(worst).Assigningeachofthe4copiesadifferentincorrect
label(random-multi)reducestheMIattacksuccessrate.
etaR
evitisoP
eurT
poison exact
poison augments
poison noised
poison unadversarial
no poison
Figure 17: Mislabeling the exact target ð‘¥ performs better
thanpoisoningstrategiesthatmodifythetargetð‘¥,withdata
augmentations,Gaussiannoise,orunadversarialexamples.
Eachattackaddsð‘Ÿ =8poisonspertarget.
Weconsideranadditionalstrategy,thatreplacesthesampleð‘¥
byanunadversarialexample[59]forð‘¥.Thatis,givenanexample
(ð‘¥,ð‘¦) weconstructasampleð‘¥Ë† thatisveryclosetoð‘¥,sothata
trained model labelsð‘¥Ë† as classð‘¦ with maximal confidence. We
thenuseð‘Ÿ mislabeledcopiesofthisunadversarialexample,(ð‘¥Ë†,ð‘¦â€²)
as our poisons. Our aim with this attack is to force the model
tomislabelavariantofthetargetð‘¥ thatthemodelismaximally
confidentinâ€”inthehopethatthiswouldmaximizetheinfluenceof
thecorrectlylabeledtarget.Unfortunately,wefindthatthisstrategy
alsoperformsmuchworsethanourbaselinestrategythatsimply
mislabelstheexacttargetð‘¥.
Togenerateanunadversarialexample[59]for(ð‘¥,ð‘¦),wepicka
modelpre-trainedonCIFAR-10,andusethePGDattackof[42]to
findanexampleð‘¥Ë†thatminimizethemodelâ€™slossâ„“(ð‘“(ð‘¥Ë†),ð‘¦)under
the constraint âˆ¥ð‘¥Ë† âˆ’ð‘¥âˆ¥âˆž â‰¤
25
8
5
. We run PGD for 200 steps. To
improvethetransferabilityoftheunadversarialexample,weuse
atargetmodelthatconsistsofanensembleof20differentWide
ResNetspre-trainedonrandomsubsetsofCIFAR-10.
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
A.3 AttackswithPartialKnowledgeofthe
Target
Inthissection,weevaluateourattackwhentheadversaryhasonly
partialknowledgeofthetargetedexample.Specifically,theadver-
sarydoesnotknowtheexactCIFAR-10imageð‘¥thatis(potentially)
usedtotrainamodel,butonlyaâ€œsimilarâ€imageð‘¥Ë†.
Tochoosepairsofsimilarimagesð‘¥ â‰ˆð‘¥Ë†,weextractfeaturesfrom
theentireCIFAR-10trainingsetusingCLIP[55]andmatcheach
exampleð‘¥ withitsnearestneighborð‘¥Ë†infeaturespace.Random
examplesofsuchpairsareshowninFigure18.Thesepairsoften
correspondtothesameobjectpicturedunderdifferentanglesor
scales,andthusreasonablyemulateascenariowheretheattacker
knowsthetargetedobject,butnottheexactpictureofitthatwas
usedtotrainthemodel.
Figure18:ExamplesofnearneighborsinCIFAR-10usedfor
theattackinFigure19.
Toevaluatetheattack,wetrainð‘€targetmodels,halfofwhich
aretrainedonaparticulartargetimageð‘¥.Weensurethatnone
ofthesetargetmodelsaretrainedontheneighborimageð‘¥Ë† that
isknowntotheadversary.Theadversarythentrainsð‘ shadow
models,halfofwhicharetrainedontheimageð‘¥Ë† thatisknown
totheadversary.Wesimilarlyensurethannoneoftheshadow
modelsaretrainedontherealtargetð‘¥.Usingtheshadowmodels,
theadversarythenmodelsthedistributionoflossesofð‘¥Ë†whenitis
amemberandwhenitisnot,asdescribedinSection4.1.Finally,
theadversaryqueriesthetargetmodelsontheknownimageð‘¥Ë†and
guesseswhetheritwasamemberornot(ofcourse,ð‘¥Ë† isnever a
memberofthetargetmodel,butweusetheadversaryâ€™sguessasa
proxyforguessingthemembershipofthereal,unknowntargetð‘¥).
TheattackresultsareinFigure19.Wefindthatthemembership
inferenceattackof[11],withorwithoutpoisoning,isrobusttoan
adversarywithonlypartialknowledgeofthetarget.
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
A.4 BoundingOutlierInfluencewithLoss
Clipping
InFigure20,weshowthedistributionoflossesforindividualCIFAR-
10examples,formodelstrainedwithlossclipping(seeSection4.3.7).
Similarly to Figure 3, we find that poisoning shifts the modelâ€™s
lossesbecausethepoisonedmodelbecomeslessconfidenceinthe
targetexample.However,poisoningdoesnothelpinmakingthe
distributionsmoreseparable.Onthecontrary,asweincreasethe
numberofpoisons,evenexamplesthatwereoriginallyeasytoinfer
membershiponbecomehardtodistinguish.
25 0 2525 0 2525 0 2525 0 2525 0 2525 0 25
âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ nopoison poisonx1 poisonx2 poisonx4 poisonx8 poisonx16
Figure 20: For models trained with clipped losses, poison-
ingshiftsthelossdistributionsofmembers(red)andnon-
members(blue),butdoesnotmakethemmoreseparable.
A.5 UntargetedMembershipInferenceAttacks
Alternativestrategiesanddatasets. InFigure21,Figure22,and
Figure23,weshowtheresultsofdifferentuntargetedpoisoning
strategiesonCIFAR-10andCIFAR-100,aswellasforanSVMclas-
sifiertrainedontheTexas100dataset(see[62]fordetailsonthis
dataset).
100
10âˆ’1
poison x4 (exact target)
poison x4 (similar target)
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
No poison (exact target)
False Positive Rate
No poison (similar target)
Figure19:OurMIattack(with4poisons)worksonCIFAR-
10evenwhentheadversarydoesnotknowtheexacttarget,
butonlyanearneighbor.
etaR
evitisoP
eurT
same class label flipping
random label flipping
next class label flipping
No poison
Figure21:Comparisonofuntargetedpoisoningattackson
CIFAR-10.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
100
6Ã—10âˆ’1
4Ã—10âˆ’1
3Ã—10âˆ’1
2Ã—10âˆ’1
10âˆ’1
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
same class label flipping
random label flipping
next class label flipping
No poison
Figure22:Comparisonofuntargetedpoisoningattackson
CIFAR-100.
100
10âˆ’1
10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
column),thetwodistributionsoverlapformostexamples.With
anuntargetedpoisoningattack,theconfidencesdecreaseandthe
distributionsbecomemoreseparable,whichmakesmembership
inferenceeasier.
âˆ’20 0 20 âˆ’20 0 20 âˆ’20 0 20 âˆ’20 0 20
no poison single class random next class
Figure 24: Untargeted poisoning makes the loss distribu-
tions of members and non-members easier to distinguish.
Forfiverandomlychosendatapoints,weshowthedistribu-
random label flipping
tionofmodelsâ€™losses(inlogitscale)onthatexamplewhenit
next class label flipping
isamember(red)andwhenitisnot(blue).Thex-axisshows
same class label flipping
differenttypesofuntargetedpoisoningstrategies.
No poison
Disparate impact of untargeted poisoning. To examine which
pointsaremostvulnerabletotheuntargetedpoisoningattack,we
performthesameanalysisasinSection4.3.2.Wefirstpickoutthe
Figure23:Comparisonofuntargetedpoisoningattackson
5%ofleast-andmost-vulnerablepointsforasetofmodelstrained
Texas100.
withoutpoisoning.WethenrunanMIattackonbothtypesofpoints
(foranewsetofmodels)withandwithoutpoisoninginFigure25.
Thebest-performingstrategyonCIFAR-10andCIFAR-100,same
Untargetedpoisoningdoesnotsignificantlyaffectthepointsthat
classlabelflipping,mislabelsalloftheadversaryâ€™spointsintoa
wereinitiallymostvulnerable.Forthepointsthatarehardestto
single class. We consider two alternative untargeted poisoning
attackwithoutpoisoning,ouruntargetedattackincreasestheTPR
strategies: random label flipping where each of the adversaryâ€™s
ata0.1%FPRfrom0.1%to3.7%â€”animprovementof37Ã—.
points is randomly mislabeled into an incorrect class, and next
classlabelflipping wheretheadversarymislabelseachexample
(ð‘¥,ð‘¦) intothenextclass (ð‘¥,ð‘¦+1 mod |Y|).OnbothCIFAR-10 100
andCIFAR-100,consistentlymislabellingallpoisonedexamples
intothesameclassresultsinthestrongestattack.OntheTexas100
dataset,simplymislabelingtheadversaryâ€™sdataatrandomperforms
slightlybetter.
10âˆ’1
OnCIFAR-10,wealsoexperimentedwithstrategieswherethe
adversaryâ€™sshareofthedataisout-of-distribution,e.g.,byusing
randomlymislabeledimagesfromCIFAR-100orMNIST,orsim-
plyimagesthatconsistofrandomnoise.However,wecouldnot
findapoisoningstrategythatperformedaswellasconsistently 10âˆ’2
10âˆ’3 10âˆ’2 10âˆ’1 100
mislabellingin-distributiondata.
False Positive Rate
Distributionofconfidences. Similarlytothetargetedattack,the
untargetedpoisoningattackalsomakestheMIattackeasierby
makingindividualexamplesâ€™confidencedistributionsmoresepara-
ble.InFigure24,wepickfiverandomCIFAR-10examplesandplot
thelogit-scaledconfidenceofthedatapointwhenitisamember
(red)andnotamember(blue).Intheunpoisonedmodel(leftmost
etaR
evitisoP
eurT most vulnerable (with poison)
most vulnerable (no poison)
least vulnerable (with poison)
least vulnerable (no poison)
Figure25:Untargetedpoisoningcausespreviously-safedata
pointstobecomevulnerable.Whilepoisoninghaslittleef-
fectonthemostvulnerablepoints,poisoningtheleastvul-
nerablepointsimprovestheTPRata0.1%FPRby37Ã—.
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
B ADDITIONALEXPERIMENTSFOR
ATTRIBUTEINFERENCEATTACKS
InFigure26,wereplicatetheexperimentfromSection5.3butinfer
a userâ€™s relationship status (â€œmarriedâ€ or â€œnon-marriedâ€) rather
thantheirgender.Theattackandexperimentalsetuparethesame
asdescribedinSection5.2.
Theresults,showninFigure26arequalitativelysimilarasthose
forinferringgenderinFigure10.AtaFPRof0.1%,theattackof[45]
(withoutpoisoning)achievesaTPRof4%,whileourattackwith16
poisonsobtainsaTPRof18%.Atfalse-positiveratesof>5%allthe
attributeinferenceattacks(evenwithpoisoning)performworse
thanatrivialimputationbaseline.
100
10âˆ’1
10âˆ’2
10âˆ’3
10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive Rate
etaR
evitisoP
eurT
12
11
10
9
8
7
6
5
1 2 4 8 16 32 64 128
Number of Poisons
poison x16
poison x8
poison x4
poison x2
No poison
baseline imputation
Figure 26: Targeted poisoning attacks boost attribute in-
ference (for inferring relationship status) on Adult. With-
out poisoning, the attack of [45] performs no better than
abaselineimputationthatinfersrelationshipstatusbased
on correlations with other attributes. With poisoning, the
attack significantly outperforms the baseline at low false-
positives.
C ADDITIONALEXPERIMENTSFOR
CANARYEXTRACTION
C.1 StrategiesforSuffixPoisoning
InSection6.3weshowedthatwecouldincreaseexposureafter
poisoningacanaryâ€™sprefixbyre-insertingitmultipletimesintothe
trainingsetpaddedwithzeros.InFigure27,weconsideralternative
suffixpoisoningstrategies,thatareultimatelylesseffective.Padding
theprefixwithalistofrandomtokensorarandom6-digitnumber
alsoprovidesamoderateincreaseinexposure(to8.5bitsand8.3
bitsrespectively),aslongasthesamerandomsuffixisre-usedforall
poisons.Ifweinsertthepoisonmanytimeswithdifferentrandom
suffixes,thepoisoningactuallyhurtstheattack.Thismirrorsour
findinginFigure6andFigure16thatmislabelingatargetpoint
withdifferentincorrectlabelshurtsMIattacks.
)stib(
erusopxE
Zero token Rand tokens
Rand token (rep) Rand numbers
Rand number (rep) No poison
Figure27:Poisoningacanaryâ€™sprefixbypaddingitwithze-
rosismoreeffectivethanalternativestrategiesthatpadwith
randomtokensorrandom6-digitnumbers.Replicatingthe
samepaddingforeachpoisonedcopy(rep)ismuchmoreef-
fectivethanusingadifferentrandompaddingforeachcopy.
C.2 CanaryExtractiononaFixedBudget
InSection6,weevaluatedcanaryextractionattacksintermsof
theaverageexposureofdifferentcanariesinsertedintoatraining
set.Anincreaseinaverageexposuredoesnotnecessarilytellus
whethertheattackismakingextractionofcanariesmorepractical
(e.g.,anattackmightallowtheadversarytorecovercanariesthat
usedtorequire200,000guessesinâ€œonlyâ€100,000guesses,without
makinganydifferenceforthosecanariesthatcanbeextractedin
lessthan100,000guesses).Thisisnotthecaseforourattack.As
weshowinFigure28,poisoningincreasestheattackerâ€™ssuccess
rateinextractingcanariesforanybudgetofguesses.Forexample,
iftheadversaryislimitedto100guessesforacanary,theirsuccess
rategrowsfrom10%withoutpoisoningto41%withpoisoning.
1.0
0.8
0.6
0.4
0.2
0.0
100 102 104 106
Guesses
etar
sseccuS
Prefix+suffix poisoning
Suffix poisoning
No poison
Figure 28: For any fixed budget of guesses, poisoning in-
creasestheattackerâ€™ssuccessrateinrecoveringasecretca-
nary.
CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA FlorianTramÃ¨retal.
Here,weshowthatpoisoningisindeednecessarytoobtainsuch
astrongattack.Tomakethisargument,weprovethatwithout
poisoningthereexistpointswheremembershipinferencecannot
clean
succeedbetterthanchance.
poisoned
Wesaythatapoint (ð‘¥,ð‘¦) âˆˆ ð· isunused bythemodelifthe
target
modelâ€™soutputonanypointisunaffectedbytheremovalof(ð‘¥,ð‘¦)
poisoning
fromthetrainingsetð·.Suchpointsareeasytoconstruct:e.g.,con-
sideraclusterofâ‰«ð‘˜close-bypointsthatallthesharethesamela-
bel.Removingonepointfromthecenterofthisclusterwillnotaffect
themodelâ€™soutputonanyinput(asimpleone-dimensionalexample
visualizationisgiveninFigure29).Foranysuchunusedpoint,infer-
ringmembershipisimpossible:themodelâ€™sinput-outputbehavior
isidenticalwhetherthemodelistrainedonð· oronð·\{(ð‘¥,ð‘¦)}.
However,thepoisoningstrategyinAlgorithm1stillsucceedson
thesepoints,andthusprovablyincreasesprivacyleakage.
Figure 30: Consider a squares versus circles classification
task.Byinsertingtheredsquareasapoison,wecannowper-
formmembershipinferenceonthetargetbluecircle.Butbe-
causethisbluecirclewasnotasupportvectorbeforepoison-
ing, membership inference was impossible. By adding the
redsquarepoisonedpoint,themaximum-marginclassifier Figure 29: Unused points in a one-dimensional ð‘˜-nearest
shiftstotheorangeline,andthetargetbecomesasupport
neighborsclassifierwithð‘˜ =3.Bluetrianglesarefromclass
vector.(Thesupportvectorsforeachmodelarecircledinthe
0,andorangesquaresfromclass1.Removingoneofthecir-
modelâ€™scolor.) cle â€œunusedâ€ training samples does not affect the modelâ€™s
decisiononanytestpoint.
D PROVABLYAMPLIFYINGPRIVACY
Poisoningsupportvectormachines. Here,weconsideranadver-
LEAKAGE
sarywhoreceivesblack-boxaccesstoalinearSVM.Bydefinition,
In this section, we provide additional theoretical analysis that
onlysupportvectorsareusedatinferencetimeandthusdistinguish-
provesatargetedpoisoningattackcanachieveperfectmembership
ingbetweenanSVMtrainedonð·andonetrainedonð·/{(ð‘¥,ð‘¦)}
inferenceinthecaseofk-NearestNeighbors(kNNs)andlinear
isimpossibleunlessthesample(ð‘¥,ð‘¦)isasupportvectorinatleast
SupportVectorMachines(SVMs).
oneofthesetwomodels.Weshowthatthereexistpointsthatcan
be forcedâ€”bya poisoning attackâ€”to become support vectors if
Algorithm1:ð‘˜-nearestneighborspoisoning theyaremembersofthetrainingset.Bycomputingthedistance
betweenthepoisonedpointsandtheclassifierâ€™sdecisionboundary
Data:Targetpoint(ð‘¥,ð‘¦),nearestneighborcountð‘˜,
(whichcanbedonewithblack-boxmodelaccess),theadversary
minimumdistanceð›¿
cantheninferwith100%accuracywhethersometargetedpoint
FunctionkNNPoison(ð‘¥,ð‘¦,ð‘˜,ð›¿):
wasamemberornot.
Pickanincorrectlabelð‘¦â€²â‰ ð‘¦
Unlikeforð‘˜-nearestneighborsmodels,wewillnotbeabletore-
ð‘‹ adv={ð‘¥, (cid:32)(cid:32) ... (cid:32) , (cid:32) ð‘¥} // Make ð‘˜âˆ’1 copies of x vealmembershipforanypoint.Instead,ourattackcanonlysucceed
(cid:124)(cid:123)(cid:122)(cid:125)
ð‘˜âˆ’1 onexamplesthatlieontheconvexhullofexamplesfromoneclass.
ð‘Œ adv={ð‘¦,...,ð‘¦,ð‘¦â€²,...,ð‘¦â€²} // Evenly balance classes (Howevernotethatinhighdimensionsalmostallpointsareonthe
(cid:32)(cid:32) (cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) boundaryoftheconvexhull,andalmostnopointsarecontainedin
(ð‘˜âˆ’1)/2 (ð‘˜âˆ’1)/2
theinterior.)Weproposeasufficientconditionforsuchapointto
ð‘‹ adv=ð‘‹ advâˆª{ð‘¥â€²}, suchthatâˆ¥ð‘¥âˆ’ð‘¥â€²âˆ¥=ð›¿
beforcedtobeasupportvector,whichwecallprotruding:
ð‘Œ adv=ð‘Œ advâˆª{ð‘¦â€²}
// Mislabel next-closest sample
returnð· adv=ð‘‹ adv ,ð‘Œ adv Definition D.1. For a binary classification dataset ð·, a point
(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡) âˆˆ ð· is protruding if there exists someð‘¤,ð‘0,ð‘1 so that
FunctionMI(ð‘¥,ð‘¦,ð‘“ kNN): theplaneð‘¤ Â·ð‘¥ +ð‘0 = 0linearlyseparatesð· andð‘¤ Â·ð‘¥ +ð‘1 = 0
ð‘¦Ë†â†ð‘“ kNN(ð‘¥) // Query the model (as a black-box) linearlyseparatesð·/{(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡)}âˆª{(ð‘¥ ð‘¡ ,1âˆ’ð‘¦ ð‘¡)}.
Ifð‘¦Ë†=ð‘¦
Intuitively,thisdefinitionsaysthatapointisprotrudingifthere
returnâ€œmemberâ€
exists some way to linearly separate the two classes, such that
Else
thisprotrudingpointistheclosesttrainingexampletothedecision
returnâ€œnon-memberâ€
boundary.Then,ifthatpointâ€™slabelwereflipped,itsufficestoâ€œshiftâ€
thedecisionboundary(i.e.,bymodifyingtheoffsetð‘)tolinearly
Poisoningð‘˜-nearestneighborclassifiers. InSection3.2weintro-
separatethedataagain.Wegiveanexampleofaprotrudingpoint
ducedastrategythatusedpoisoningtoobtain100%membership
(the target point) in Figure 30. If a point is protruding, we can
inferenceaccuracyonatargetedpointforkNNs(seeAlgorithm1).
TruthSerum:PoisoningMachineLearningModelstoRevealTheirSecrets CCSâ€™22,November7â€“11,2022,LosAngeles,CA,USA
insertapoisonedpointoftheoppositeclassclosetoittoforcethe When(ð‘¥
ð‘¡
,ð‘¦ ð‘¡)âˆ‰ð·,themarginoftheresultinghyperplanemust
protrudingpointtobecomeasupportvector. belargerthanð›¿/4,asð‘“ isahyperplanewhichlinearlyseparates
TheoremD.2. Letð·beabinaryclassificationdatasetcontaining
ð·âˆª{(ð‘¥
ð‘
,ð‘¦ ð‘)}withamarginofð›¿/2. â–¡
aprotrudingpoint(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡).Thenthereexistssome(ð‘¥ ð‘ ,ð‘¦ ð‘)sothat Ouranalysishereassumesthattheadversaryknowseverything
ð·âˆª{(ð‘¥ ð‘ ,ð‘¦ ð‘)}has(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡)asasupportvector,andalargermargin aboutthetrainingsetexceptforwhether(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡)isamember,and
when(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡)âˆ‰ð·. thatthedatasetislinearlyseparable.
Wealsorunabriefexperimenttoshowthatuntargetedwhite-
Proof. Without loss of generality, assumeð‘¦ ð‘¡ = 0. Because boxattacksonSVMsarealsopossible.Givenwhite-boxaccessto
(ð‘¥
ð‘¡
,ð‘¦ ð‘¡)isprotruding,weknowthereexistssomeð‘¤,ð‘0,ð‘1 satisfying
anSVM,theadversarycandirectlyrecoverthedataofthesupport
theconditionsofthedefinition.Writeð‘suchthatð‘“(ð‘¥)=ð‘¤Â·ð‘¥+ð‘
vectors,asthesearenecessarytoperforminference.Ouruntargeted
hasð‘“(ð‘¥ ð‘¡)=0.Letð›¿bethedistancefromtheplaneð‘“(ð‘¥)=0tothe
attacksincreaseprivacyleakagebyforcingthetrainedmodelto
nearestpointinð·/{(ð‘¥
ð‘¡
,ð‘¦ ð‘¡)}.Wehaveð›¿ >0becausetheplaneð‘“
usemoredatapointsassupportvectors.WetrainSVMsonFashion
liesstrictlyinbetweentheplanesð‘¤Â·ð‘¥+ð‘0=0andð‘¤Â·ð‘¥+ð‘1=0,
MNIST restricted to the first two classes, using 2000 points for
whichbothlinearlyseparateð·/{(ð‘¥
ð‘¡
,ð‘¦ ð‘¡)}.
trainingandinjecting200poisoningpointsaccordingtoasimple
Thenconsiderthepoisoning(ð‘¥
ð‘
,ð‘¦ ð‘)=(ð‘¥
ð‘¡
+ð‘¤
2||
ð›¿
ð‘¤||
,1).When
labelflippingstrategy.Over5trials,anunpoisonedlinearSVMhas
(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡) âˆˆð·,themaximummarginseparatorofð·âˆª{(ð‘¥ ð‘ ,ð‘¦ ð‘)}is anaverageof121supportvectors,andanunpoisonedpolynomial
ð‘¤Â·ð‘¥+ð‘+ 4|| ð›¿ ð‘¤|| =0.Thedistancefromeachpointinð·/{(ð‘¥ ð‘¡ ,ð‘¦ ð‘¡)} kernelSVMhasanaverageof176supportvectors.Whenadding
tothisplanemustbeatleast 3ð›¿,asthishasshiftedð‘“ byadistance thelabelflippingattack,thepoisonedlinearSVMgrowsto512
4
ofð›¿/4.Then(ð‘¥
ð‘¡
,ð‘¦ ð‘¡)willbeasupportvectorofthisplane,witha supportvectorsfromthecleantrainingset,andthepolynomial
marginofð›¿/4. SVM grows to 642 support vectors from the clean training set,
increasingthenumberofleakeddatapointsbyafactorof4.2Ã—and
3.7Ã—,respectively.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
