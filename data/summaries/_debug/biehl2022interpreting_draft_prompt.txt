=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Interpreting systems as solving POMDPs: a step towards a formal understanding of agency
Citation Key: biehl2022interpreting
Authors: Martin Biehl, Nathaniel Virgo

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: systems, system, understanding, martin, state, interpreting, beliefs, formal, step, towards

=== FULL PAPER TEXT ===

Interpreting systems as solving POMDPs: a step
towards a formal understanding of agency⋆
Martin Biehl1[0000−0002−1670−6855] and Nathaniel Virgo2[0000−0001−8598−590X]
1 Cross Labs, Cross Compass, Tokyo 104-0045, Japan
martin.biehl@cross-compass.com
2 Earth-Life Science Institute, Tokyo Institute of Technology, Tokyo 152-8550, Japan
Abstract. Underwhatcircumstancescanasystembesaidtohavebe-
liefs and goals, and how do such agency-related features relate to its
physicalstate?Recentworkhasproposedanotionofinterpretationmap,
a function that maps the state of a system to a probability distribution
representingitsbeliefsaboutanexternalworld.Suchamapisnotcom-
pletely arbitrary, as the beliefs it attributes to the system must evolve
overtimeinamannerthatisconsistentwithBayes’theorem,andconse-
quently the dynamics of a system constrain its possible interpretations.
Herewebuildonthisapproach,proposinganotionofinterpretationnot
just in terms of beliefs but in terms of goals and actions. To do this we
make use of the existing theory of partially observable Markov decision
processes(POMDPs):wesaythatasystemcanbeinterpretedasasolu-
tiontoaPOMDPifitnotonlyadmitsaninterpretationmapdescribing
its beliefs about the hidden state of a POMDP but also takes actions
that are optimal according to its belief state. An agent is then a system
togetherwithaninterpretationofthissystemasaPOMDPsolution.Al-
thoughPOMDPsarenottheonlypossibleformulationofwhatitmeans
tohaveagoal,thisneverthelessrepresentsasteptowardsamoregeneral
formal definition of what it means for a system to be an agent.
Keywords: Agency · POMDP · Bayesian filtering · Bayesian inference
1 Introduction
This work is a contribution to the general question of when a physical system
can justifiably be seen as an agent. We are still far from answering this question
in full generality but employ here a set of limiting assumptions / conceptual
commitments that allow us to provide an example of the kind of answer we are
looking for.
The basic idea is inspired by but different from Dennett’s proposal to use
so-called stances [4], which says we should interpret a system as an agent if
⋆ This project was made possible through the support of Grant 62229 from the John
Templeton Foundation. The opinions expressed in this publication are those of the
authorsanddonotnecessarilyreflecttheviewsoftheJohnTempletonFoundation.
Work on this project was also supported by a grant from GoodAI.
5202
luJ
11
]IA.sc[
2v91610.9022:viXra
2 M. Biehl, N. Virgo
taking the intentional stance improves our predictions of its behavior beyond
those obtained by the physical stance (or the design stance, but we ignore this
stance here). Taking the physical stance means using the dynamical laws of the
(microscopic) physical constituents of the system. Taking the intentional stance
means ignoring the dynamics of the physical constituents of the system and
instead interpreting it as a rational agent with beliefs and desires. (We content
ourselveswithonlyascribinggoals insteadofdesires.)Aquantitativemethodto
perform this comparison of stances can be found in [12].
In contrast to using a comparison of prediction performance of different
stancesweproposetodecidewhetherasystemcanbeinterpretedasanagentby
checking whether its physical dynamics are consistent with an interpretation as
a rational agent with beliefs and goals. In other words, assuming that we know
what happens in the system on the physical level (admittedly a strong assump-
tion), we propose to check whether we can consistently ascribe meaning to its
physicalstates,suchthattheyappeartoimplementaprocessofbeliefupdating
and decision making.
A formal example definition of what it means for an interpretation to be
consistent was recently published in [16]. This establishes a notion of consistent
interpretation as a Bayesian reasoner, meaning something that receives inputs
andusesthemtomakeinferencesaboutsomehiddenvariable,butdoesnottake
actions or pursue a goal.
Briefly, such an interpretation consists of a map from the physical / internal
states ofthe systemto Bayesian beliefs abouthiddenstates(thatis, probability
distributions over them), as well as a model describing how the hidden states
determine the next hidden state and the input to the system. To be consistent,
if the internal state at time t is mapped to some belief, then the internal state
at time t+1 must map to the Bayesian posterior of that belief, given the input
that was received in between the two time steps.
In other words, the internal state parameterizes beliefs and the system up-
dates the parameters in a way that makes the parameterized belief change ac-
cording to Bayes law. A Bayesian reasoner is not an agent however. It lacks
both goals and rationality since it neither has a goal nor actions that it could
rationally take to bring the goal about.
Here we build on the notion of consistent interpretations of [16] and show
how it can be extended to also include the attribution of goals and rationality.
For this we employ the class of problems called partially observable Markov
decision processes (POMDPs), which are well suited to our purpose. These pro-
vide hidden states to parameterize beliefs over, a notion of a goal, and a notion
of what it means to act optimally, and thus rationally, with respect to this goal.
Note that both the hidden states and the goal (which will be represented by
rewards) are not assumed to have a physical realization. They are part of the
interpretationandthereforeonlyneedtoexistinthemathematicalsense.Infor-
mally, the hidden state is assumed by the agent to exist, but need not match a
state of the true external world.
Interpreting systems as solving POMDPs 3
Wewillseethatgivenapairofaphysicalsystem(asmodelledbyastochastic
Mooremachine)andaPOMDPitcaninprinciplebecheckedwhetherthesystem
does indeed parameterize beliefs over the hidden states and act optimally with
respect to the goal and its beliefs (definition 5). We then say the system can be
interpreted as solving the POMDP, and we propose to call the pair of system
and POMDP an agent. This constitutes an example of a formal definition of a
rational agent with beliefs and goals.
To get there however we need to make some conceptual commitments /
assumptions that restrict the scope of our definition. Note that we do not make
these commitments because we believe they are particularly realistic or useful
for the description of real world agents like living organisms, but only because
they make it possible to be relatively precise. We suspect that each of these
choiceshasalternativesthatleadtoothernotionsofagents.Furthermore,wedo
not argue that all agents are rational, nor that they all have beliefs and goals.
These are properties of the particular notion of agent we define here, but there
are certainly other notions of agent that one might want to consider.
The first commitment is with respect to the notion of system. Generally, the
question of which physical systems are agents may require us to clarify how we
obtainacandidatephysicalsystemfromacausallycloseduniverseandwhatthe
type of the resulting candidate physical system is. This can be done by defining
what it means to be an individual and / or identifying some kind of boundary.
Steps in this direction have been made in the context of cellular automata e.g.
by [1,2] and in the context of stochastic differential equations by [5,7].
We here restrict our scope by assuming that the candidate physical system
is a stochastic Moore machine (definition 2). A stochastic Moore machine has
inputs,adynamicandpossiblystochasticinternalstate,andoutputsthatdeter-
ministically depend on the internal state only. This is far from the most general
types of system that could be considered, but it is general enough to represent
the digital computers controlling most artificial agents at present. It it also sim-
ilar to a time and space discretized version of the dynamics of the internal state
of the literature on the free energy principle (FEP) [7].
Already at this point the reader may expect that the inputs of the Moore
machine will play the role of sensor values and the outputs that of actions and
thiswillindeedbethecase.Furthermore,theroleofthe“physicalconstituents”
or physical state (of Dennett’s physical stance) will be played by the internal
state of the machine and this state will be equipped with a kind of consistent
Bayesian interpretation. In other words, it will be parameterizing/determining
probabilistic beliefs. This is similar to the role of internal states in the FEP.
For our formal notion of beliefs we commit to probability distributions that
are updated in accordance with Bayes law.
The third commitment is with respect to a formal notion of goals and ra-
tionality. As already mentioned, for those we employ POMDPs. These provide
both a formal notion of goals via expected reward maximization and a formal
notion of rational behavior via their optimal policy.
4 M. Biehl, N. Virgo
Combining these commitments we want to express when exactly a system
can be interpreted as a rational agent with beliefs and goals.
Rational agents take the optimal actions with respect to their goals and be-
liefs. The convenient feature of POMDPs for our purposes is that the optimal
policies are usually expressed as functions of probabilistic beliefs about the hid-
den state of the POMDP. For this to work, the probabilistic beliefs must be
updated correctly according to Bayesian principles. It then turns out that these
standard solutions for POMDPs can be turned into stochastic Moore machines
whose states are the (correctly updated) probabilistic beliefs themselves and
whose outputs are the optimal actions.
This has two consequences. One is that it seems justified to interpret such
stochastic Moore machines as rational agents that have beliefs and goals. An-
other is that there are stochastic Moore machines that solve POMDPs. Accord-
ingly, our definition of stochastic Moore machines that solve POMDPs (defini-
tion 5) applies to these machines.
In addition to such machines, however, we want to include machines whose
statesonlyparameterize (andarenotequalto)theprobabilisticbeliefsoverhid-
den states and who output optimal actions.3 We achieve this by employing an
adapted notion of a consistent interpretation (definition 3). A stochastic Moore
machine can then be interpreted as solving a POMDP if it has this kind of con-
sistent interpretation with respect to the hidden state dynamics of the POMDP
and outputs the optimal policy.
We also show that the machines obeying our definition are optimal in the
samesenseasthemachineswhosestatesarethecorrectlyupdatedbeliefs,sowe
find it justified to interpret those machines as rational agents with beliefs and
goals as well.
Beforewegoontothetechnicalpartwewanttohighlightafewmoreaspects.
The first is that the existence of a consistent interpretation (either in terms of
filtering or in terms of agents) only depends on the stochastic Moore machine
that’s being interpreted, and not on any properties of its environment. This
is because a consistent interpretation requires an agent’s beliefs and goals to
be consistent, and this is different from asking whether they are correct. An
agent may have the wrong model, in that it doesn’t correspond correctly to the
true environment. Its conclusions in this case will be wrong, but its reasoning
can still be consistent; see [16] for further discussion of this point. In the case
of POMDP interpretations this means that the agent’s actions only need to be
optimalaccordingtoitsmodeloftheenvironment,buttheymightbesuboptimal
according to the true environment.
This differs from the perspective taken in the original FEP literature con-
cerned with the question of when a system of stochastic differential equations
contain an agent performing approximate Bayesian inference [5,6,14,3,7].4 This
3 Thesemachinesareprobablyequivalenttothesufficientinformationstateprocesses
in [9, definition 2] but establishing this is beyond the scope of this work.
4 The FEP literature includes both publications on how to construct agents that
solveproblems(e.g.[8])andpublicationsonwhenasystemofstochasticdifferential
Interpreting systems as solving POMDPs 5
literaturealsointerpretsasystemasmodellinghiddenstatedynamics,butthere
themodelisderivedfromthedynamicsoftheactualenvironment(thesocalled
“external states”), and hence cannot differ from it. We consider it helpful to be
able to make a clear distinction between the agent’s model of its environment
and its true environment. The case where the model is derived from the true
environment is an interesting special case of this, but our framework covers the
general case as well. To our knowledge, the possibility of choosing the model
independently from the actual environment in a FEP-like theory was first pro-
posedin[16],andhassincealsoappearedinasettingclosertotheoriginalFEP
one [13].
We will see here (definition 3) that the independence of model from actual
environmentextendstoactionsinsomesense.Evenamachinewithoutanyout-
puts can have a consistent interpretation modelling an influence of the internal
state on the hidden state dynamics even though it can’t have an influence on
the actual environment. Such “actions” remain confined to the interpretation.
Another aspect of using consistent interpretations of the internal state and
thus the analogue of the physical state / the physical constituents of the system
is that it automatically comes with a notion of coarse-graining of the internal
state. Since interpretations map the internal state to beliefs but don’t need to
do so injectively they can include coarse-graining of the state.
Alsonote,allourcurrentnotionsofinterpretationintermsofBayesianbeliefs
require exact Bayesian updating. This means approximate versions of Bayesian
inferenceorfilteringareoutsideofthescope.Thislimitsthescopeofourexample
definition in comparison with the FEP which, as mentioned, also uses beliefs
parameterized by internal states but considers approximate inference. On the
other hand this keeps the involved concepts simpler.
Finally, we want to mention that [11] recently proposed an agent discovery
algorithm. This algorithm is based on a definition of agents that takes into
account the creation process of the system. An agent discovery algorithm based
ontheapproachpresentedherewouldtakeasinputamachine(definition1)ora
stochasticMooremachine(definition2)andtrytofindaPOMDPinterpretation
(definition 5). The creation process of the machine (system) would not be taken
into account. This is one distinction between our notion of an agent and that of
[11]. A more detailed comparison would be interesting but is beyond the scope
of this work.
The rest of this manuscript presents the necessary formal definitions that
allow us to precisely state our example of an agent definition.
2 Interpreting stochastic Moore machines
Throughout the manuscript we write PX for the set of all finitely supported
probability distributions over a set X. This ensures that all probability distri-
equations contain an agent performing approximate Bayesian inference. Only the
latterliteratureaddressesaquestioncomparabletotheoneaddressedinthepresent
manuscript.
6 M. Biehl, N. Virgo
butions we consider only have a finite set of outcomes that occur with non-zero
probability. We can then avoid measure theoretic language and technicalities.
For two sets X,Y a Markov kernel is a function ζ : X → PY. We write ζ(y|x)
fortheprobabilityofy ∈Y accordingtotheprobabilitydistributionζ(x)∈PY.
If we have a function f : X → Y we sometimes write δ : X → PY for the
f
Markovkernelwithδ (y)(whichis1ify =f(x)and0else)thendefiningthe
f(x)
probability of y given x.
Wegivethefollowingdefinition,whichisthesameastheoneusedin[16],but
specialisedtothecasewhereupdatefunctionsmaptothesetoffinitelysupported
probability distributions and not to the space of all probability distributions.
Definition 1. Amachineisatuple(M,I,µ)consistingofasetMcalled inter-
nalstatespace;asetI called inputspace;andaMarkovkernelµ:I×M→PM
called machinekernel,takinganinputi∈I andacurrentmachinestatem∈M
to a probability distribution µ(i,m)∈PM over machine states.
The idea is that at any given time the machine has a state m ∈ M. At each
time step it recieves an input i ∈ I, and updates stochastically to a new state,
accordingtoaprobabilitydistirbutionspecifiedbythemachinekernel.Ifweadd
afunction thatspecifiesan output giventhe machinestate weget thedefinition
of a stochastic Moore machine.
Definition 2. A stochasticMooremachine is a tuple (M,I,O,µ,ω) consisting
of a machine with internal state space M, input space I, and machine kernel
µ:I×M→PM; a set O called the output space; and a function ω :M→O
called expose function taking any machine state m∈M to an output ω(m)∈O.
Note that the expose function is an ordinary function and not stochastic.
We need to adapt the definition of a consistent Bayesian filtering interpre-
tation [16, Definition 2]. For our purposes here we need to include models of
dynamichiddenstatesthatcanbeinfluenced.Inparticularweneedtointerpret
a machine as modelling the dynamics of a hidden state that the machine itself
can influence. This suggests that the interpretation includes a model of how the
state of the machine influences the hidden state. We here call such influences
“actions” and the function that takes states to actions action kernel.
Definition 3. GivenamachinewithstatespaceM,inputspaceI andmachine
kernel µ : I ×M → PM, a consistent Bayesian influenced filtering interpre-
tation (H,A,ψ,α,κ) consists of a set H called the hidden state space; a set A
calledthe actionspace;aMarkovkernelψ :M→PH called interpretationmap
mappingmachinestatestoprobabilitydistributionsoverthehiddenstatespace;a
function α:M→A called actionfunction mapping machine states to actions5;
and a Markov kernel κ : H×A → P(H×I) called the model kernel mapping
pairs (h,a) of hidden states and actions to probability distributions κ(h,a) over
pairs (h′,i) of next hidden states and an input.
5 We choose actions to be deterministic functions of the machine state because the
stochastic Moore machines considered here also have deterministic outputs. Other
choices may be more suitable in other cases.
Interpreting systems as solving POMDPs 7
These components have to obey the following equation. First, in string dia-
gram notation (see appendix A of [16] for an introduction to string diagrams for
probability in a similar context to the current paper):
H H H H
ψ ψ ψ
κ κ
α A I I = α A I I (1)
M µ M M µ M
,
Second, in more standard notation, we must have for each m ∈ M, h′ ∈ H,
i∈I, and m′ ∈M:
(cid:32) (cid:33)
(cid:88) (cid:88)
κ(h′,i|h,a)ψ(h|m)δ (a) µ(m′|i,m)=
α(m)
h∈H a∈A
(2)
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
ψ(h′|m′) κ(h′′,i|h,a)ψ(h|m)δ (a) µ(m′|i,m).
α(m)
h∈Ha∈Ah′′∈H
In appendix A we show how to turn eq. (2) into a more familiar form.
Note that we defined consistent Bayesian influenced filtering interpretations
for machines that have no actual output but that it also applies to those with
outputs. If we want an interpretation of a machine with outputs we may choose
the action space as the output space and the action kernel as the output kernel,
but we don’t have to. Interpretations can still be consistent.
Also note that when A is a space with only one element we recover the
original definition of a consistent Bayesian filtering interpretation from [16].
3 Interpreting stochastic Moore machines as solving
POMDPs
Definition 4. A partially observable Markov decision process (POMDP) can
be defined as a tuple (H,A,S,κ,r) consisting of a set H called the hidden state
space; a set A called the actionspace; a set S called the sensorspace; a Markov
kernel κ:H×A→P(H×S) called the transition kernel taking a hidden state
h and action a to a probability distribution over next hidden states and sensor
values; and a function r : H×A → R called the reward function returning a
real valued reward depending on the hidden state and an action.
To solve a POMDP we have to choose a policy (as defined below) that max-
imizes the expected cumulative reward either for a finite horizon or discounted
with an infinite horizon. We only deal with the latter case here.
POMDPs are commonly solved in two steps. First since the hidden state is
unknown,probabilitydistributionsb∈PH (calledbeliefstates)overthehidden
state are introduced and an updating function f :PH×A×S →PH for these
belief states is defined. This updating is directly derived from Bayes rule [10]:
(cid:80) κ(h′,s|h,a)b(h)
b′(h′)=f(b,a,s)(h′):=Pr(h′|b,a,s):= h∈H . (3)
(cid:80) κ(h¯′,s|h¯,a)b(h¯)
h¯,h¯′∈H
8 M. Biehl, N. Virgo
(Note that an assumption is that the denominator is greater than zero.) Then
anoptimalpolicyπ∗ :PH→Amappingthosebeliefstatestoactionsisderived
from a so-called belief state MDP (see appendix D for details). The optimal
policy can be expressed using an optimal value function V∗ : PH → R that
solves the following Bellman equation [9]:
 
V∗(b)=max   (cid:88) b(h)r(h,a)+γ (cid:88) κ(h′,s|h,a)b(h)V∗(f(b,a,s))  . (4)
a∈A 
h∈H s∈S
h,h′∈H
The optimal policy is then [9]:
 
π∗(b)=argmax   (cid:88) b(h)r(h,a)+γ (cid:88) κ(h′,s|h,a)b(h)V∗(f(b,a,s))  . (5)
a∈A  
h∈H s∈S
h,h′∈H
Note that the belief state update function f determines optimal value function
and policy.
Define now f (b,s) := f(b,π∗(b),s). Then note that if we consider PH a
π∗
state space, S an input space, A an output space, δ : PH×S → PPH a
fπ∗
machine kernel, and π∗ :PH →A an expose kernel, we get a stochastic Moore
machine.6
ThismachinesolvesthePOMDPandcanbedirectlyinterpretedasarational
agent with beliefs and a goal. The beliefs are just the belief states themselves,
the goal is expected cumulative reward maximization, and the optimal policy
ensures it acts rationally with respect to the goal.
OurdefinitionofinterpretationsofstochasticMooremachinesassolutionsto
POMDPs includes this example and extends it to machines whose states aren’t
probability distributions / belief states directly but instead are parameters of
such belief states that get (possibly stochastically) updated consistently.
We now state this main definition and then a proposition that ensures that
our definition only applies to stochastic Moore machines that parameterize be-
liefs correctly as required by eq. (3). This ensures that the optimal policy ob-
tained via eq. (5) is also the optimal policy for the states of the machine.
Definition 5. Given a stochastic Moore machine (M,I,O,µ,ω), a consistent
interpretation as a solution to a POMDP is given by a POMDP (H,O,I,κ,r)
andan interpretationmapψ :M→PHsuchthat(i)(H,O,ψ,ω,κ)isaconsis-
tentBayesianinfluencedfilteringinterpretationofthemachinepart(M,I,µ)of
the stochastic Moore machine; and (ii) the machine expose function ω :M→O
(which coincides with the action function in the interpretation) maps any ma-
chine state m to the action π∗(ψ(m)) specified by the optimal POMDP policy for
the belief ψ(m) associated to machine state m by the interpretation. Formally:
ω(m)=π∗(ψ(m)). (6)
6 Ifthedenominatorineq.(3)iszeroforsomevalues∈Sthendefinee.g.f π∗(b,s)=b.
Interpreting systems as solving POMDPs 9
Note that the machine never gets to observe the rewards of the POMDP we
use to interpret it. An example of a stochastic Moore machine together with an
interpretation of it as a solution to a POMDP is given in appendix C.
Proposition 1. Consider a stochastic Moore machine (M,I,O,µ,ω), together
withaconsistentinterpretationasasolutiontoaPOMDP,givenbythePOMDP
(H,O,I,κ,r) and Markov kernel ψ :M→PH. Suppose it is given an input i∈
I, and that this input has a positive probability according to the interpretation.
(That is, eq. (14) is obeyed.) Then the parameterized distributions ψ(m) update
as required by the belief state update equation (eq. (3)) whenever a = π∗(b)
i.e. whenever the action is equal to the optimal action. More formally, for any
m,m′ ∈ M with µ(m′|i,m) > 0 and i ∈ I that can occur according to the
POMDP transition and sensor kernels, we have for all h′ ∈H
ψ(h′|m′)=f(ψ(m),π∗(ψ(m)),i)(h′). (7)
Proof. See appendix B.
With this we can see that if V∗ is the optimal value function for belief states
b∈PH of eq. (4), then V¯∗(m):=V∗(ψ(m)) is an optimal value function on the
machine’s state space with optimal policy ω(m)=π∗(ψ(m)).
4 Conclusion
We proposed a definition of when an stochastic Moore machine can be inter-
preted as solving a partially observable Markov decision process (POMDP). We
showedthatstandardsolutionsofPOMDPshavecounterpartmachinesthatthis
definitionappliesto.Ourdefinitionemploysanewlyadaptedversionofaconsis-
tent interpretation. We showed that with this our definition includes additional
machineswhosestatespacesareparametersofprobabilisticbeliefsandnotsuch
beliefs directly. We suspect these machines are closely related to information
state processes [9] but the precise relation is not yet known to us.
References
1. Beer, R.D.: The cognitive domain of a glider in the game of life. Artificial Life
20(2), 183–206 (2014). https://doi.org/10.1162/ARTL a 00125
2. Biehl, M., Ikegami, T., Polani, D.: Towards information based spatiotemporal
patterns as a foundation for agent representation in dynamical systems. In: Pro-
ceedings of the Artificial Life Conference 2016. pp. 722–729. The MIT Press (Jul
2016).https://doi.org/10.7551/978-0-262-33936-0-ch115,https://mitpress.mit.
edu/sites/default/files/titles/content/conf/alife16/ch115.html
3. Da Costa, L., Friston, K., Heins, C., Pavliotis, G.A.: Bayesian Mechanics for
Stationary Processes. arXiv:2106.13830 [math-ph, physics:nlin, q-bio] (Jun 2021),
http://arxiv.org/abs/2106.13830, arXiv: 2106.13830
10 M. Biehl, N. Virgo
4. Dennett, D.C.: True Believers : The Intentional Strategy and Why It Works. In:
Heath, A.F. (ed.) Scientific Explanation: Papers Based on Herbert Spencer Lec-
tures Given in the University of Oxford, pp. 53–75. Clarendon Press (1981)
5. Friston, K.: Life as we know it. Journal of The Royal Society Inter-
face 10(86) (Sep 2013). https://doi.org/10.1098/rsif.2013.0475, http://rsif.
royalsocietypublishing.org/content/10/86/20130475
6. Friston, K.: A free energy principle for a particular physics. arXiv:1906.10184 [q-
bio] (Jun 2019), http://arxiv.org/abs/1906.10184, arXiv: 1906.10184
7. Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzho¨ffer, K., Pavliotis,
G.A., Parr, T.: The free energy principle made simpler but not too simple
(Jan 2022). https://doi.org/10.48550/arXiv.2201.06387, http://arxiv.org/abs/
2201.06387, number: arXiv:2201.06387 arXiv:2201.06387 [cond-mat, physics:nlin,
physics:physics, q-bio]
8. Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., Pezzulo, G.:
Activeinferenceandepistemicvalue.CognitiveNeuroscience6(4),187–214(2015).
https://doi.org/10.1080/17588928.2015.1020053
9. Hauskrecht, M.: Value-Function Approximations for Partially Observable Markov
Decision Processes. Journal of Artificial Intelligence Research 13, 33–94
(Aug2000).https://doi.org/10.1613/jair.678,http://arxiv.org/abs/1106.0234,
arXiv:1106.0234 [cs]
10. Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: Planning and acting in
partially observable stochastic domains. Artificial Intelligence 101(1–2), 99–
134 (May 1998). https://doi.org/10.1016/S0004-3702(98)00023-X, http://www.
sciencedirect.com/science/article/pii/S000437029800023X
11. Kenton, Z., Kumar, R., Farquhar, S., Richens, J., MacDermott, M., Everitt, T.:
DiscoveringAgents(Aug2022).https://doi.org/10.48550/arXiv.2208.08345,http:
//arxiv.org/abs/2208.08345, arXiv:2208.08345 [cs]
12. Orseau, L., McGill, S.M., Legg, S.: Agents and Devices: A Relative Definition
of Agency. arXiv:1805.12387 [cs, stat] (May 2018), http://arxiv.org/abs/1805.
12387, arXiv: 1805.12387
13. Parr, T.: Inferential dynamics: Comment on: How particular is the physics of the
free energy principle? by Aguilera et al. Physics of Life Reviews 42, 1–3 (Sep
2022).https://doi.org/10.1016/j.plrev.2022.05.006,https://www.sciencedirect.
com/science/article/pii/S1571064522000276
14. Parr, T., Da Costa, L., Friston, K.: Markov blankets, information geometry and
stochastic thermodynamics. Philosophical Transactions of the Royal Society A:
Mathematical,PhysicalandEngineeringSciences378(2164),20190159(Feb2020).
https://doi.org/10.1098/rsta.2019.0159, https://royalsocietypublishing.org/
doi/full/10.1098/rsta.2019.0159
15. Sondik,E.J.:TheOptimalControlofPartiallyObservableMarkovProcessesOver
theInfiniteHorizon:DiscountedCosts.OperationsResearch26(2),282–304(Mar
1978), http://www.jstor.org/stable/169635
16. Virgo, N., Biehl, M., McGregor, S.: Interpreting Dynamical Systems as Bayesian
Reasoners.arXiv:2112.13523[cs,q-bio](Dec2021),http://arxiv.org/abs/2112.
13523, arXiv: 2112.13523
A Consistency in more familiar form
Onewaytoturneq.(2)intoaprobablymorefamiliarformistointroducesome
abbreviationsandlookatsomespecialcases.Wefollowasimilarstrategyto[16].
Interpreting systems as solving POMDPs 11
Let
(cid:88) (cid:88)
ψ (h′,i|m):= κ(h′,i|h,a)ψ(h|m)δ (a) (8)
H,I α(m)
h∈Ha∈A
and
(cid:88)
ψ (i|m):= ψ (h′,i|m). (9)
I H,I
h′∈H
Then consider the case of a deterministic machine and choose the m′ ∈M that
actually occurs for a given input i ∈ I such that µ(m′|i,m) = 1 or abusing
notation m′ =m′(i,m). Then we get from eq. (2):
ψ (h′,i|m)=ψ(h′|µ(i,m))ψ (i|m). (10)
H,I I
If we then also consider an input i∈I that is subjectively possible as defined in
[16] which here means that ψ (i|m)>0 we get
I
ψ (h′,i|m)
ψ(h′|m′(i,m))= H,I . (11)
ψ (i|m)
I
This makes it more apparent that in the interpretation the updated machine
state m′ = m′(i,m) parameterizes a belief ψ(h′|m′(i,m)) which is equal to the
posteriordistributionoverthehiddenstategiveninputi.Inthenon-deterministic
case, note that when µ(m′|i,m)=0 the consistency equation imposes no condi-
tion,whichmakessensesincethatmeansthemachinestatem′ canneveroccur.
When µ(m′|i,m) > 0 we can divide eq. (2) by this to also get eq. (10). The
subsequent argument for m′ = m′(i,m) then must hold not only for this one
possible next state but instead for every m′ with µ(m′|i,m). So in this case (if s
issubjectivelypossible)anyofthepossiblenextstateswillparameterizeabelief
ψ(h′|m′) equal to the posterior.
B Proof of proposition 1
For the readers’s convenience we recall the proposition:
Proposition 2. Consider a stochastic Moore machine (M,I,O,µ,ω), together
withaconsistentinterpretationasasolutiontoaPOMDP,givenbythePOMDP
(H,O,I,κ,r) and Markov kernel ψ :M→PH. Suppose it is given an input i∈
I, and that this input has a positive probability according to the interpretation.
(That is, eq. (14) is obeyed.) Then the parameterized distributions ψ(m) update
as required by the belief state update equation (eq. (3)) whenever a = π∗(b)
i.e. whenever the action is equal to the optimal action. More formally, for any
m,m′ ∈ M with µ(m′|i,m) > 0 and i ∈ I that can occur according to the
POMDP transition and sensor kernels, we have for all h′ ∈H
ψ(h′|m′)=f(ψ(m),π∗(ψ(m)),i)(h′). (12)
12 M. Biehl, N. Virgo
Proof. By assumption the machine part (M,I,µ) of the stochastic Moore ma-
chinehasaconsistentBayesianinfluencedfilteringinterpretation(H,O,ψ,ω,κ).
This means that the belief ψ(m) parameterized by the stochastic Moore
machine obeys eq. (2). This means that for every possible next state m′ (i.e.
µ(m′|s,m)>0) we have
(cid:88) (cid:88)
κ(h′,i|h,a)ψ(h|m)δ (a)=
ω(m)
h∈Ha∈A
(cid:32) (cid:33) (13)
(cid:88) (cid:88) (cid:88)
ψ(h′|m′) κ(h′′,i|h,a)ψ(h|m)δ (a)
ω(m)
h∈Ha∈Ah′′∈H
and for every subjectively possible input, that is, for every input i∈I with
(cid:88) (cid:88) (cid:88)
κ(h′′,i|h,a)ψ(h|m)δ (a)>0 (14)
ω(m)
h∈Ha∈Ah′′∈H
(see below for a note on why this assumption is reasonable) we will have:
(cid:80) (cid:80) κ(h′,i|h,a)ψ(h|m)δ (a)
ψ(h′|m′)= h∈H a∈A ω(m) (15)
(cid:80) (cid:80) (cid:80) κ(h′′,i|h,a)ψ(h|m)δ (a)
h∈H a∈A h′′∈H ω(m)
(cid:80) κ(h′,i|h,ω(m))ψ(h|m)
= h∈H . (16)
(cid:80) (cid:80) κ(h′′,i|h,ω(m))ψ(h|m)
h∈H h′′∈H
Now consider the update function for which the optimal policy is found eq. (3):
(cid:80) κ(h′,s|h,a)b(h)
f(b,a,s)(h′):= h∈H (17)
(cid:80) κ(h¯′,s|h¯,a)b(h¯)
h¯,h¯′∈H
andpluginthebeliefb=ψ(m)parameterizedbythemachinestate,theoptimal
actionπ∗(ψ(m))specifiedforthatbeliefbytheoptimalpolicyπ∗,andthes=i:
(cid:80) κ(h′,i|h,π∗(ψ(m)))ψ(m)(h)
f(ψ(m),π∗(m),i)(h′):= h∈H . (18)
(cid:80) κ(h¯′,i|h¯,π∗(ψ(m)))ψ(m)(h¯)
h¯,h¯′∈H
Also introduce κ and write ψ(h|m) for ψ(m)(h) as usual
(cid:80) κ(h′,i|h,π∗(ψ(m)))ψ(h|m)
f(ψ(m),π∗(m),i)(h′):= h∈H (19)
(cid:80) κ(h¯′,i|h¯,π∗(ψ(m)))ψ(h¯|m)
h¯,h¯′∈H
=ψ(h′|m′). (20)
Which is what we wanted to prove.
Note that if eq. (14) is not true and the probability of an input i is impossible
according to the POMDP transition function, the kernel ψ, and the optimal
policy ω then eq. (13) puts no constraint on the machine kernel µ since both
sides are zero. So the behavior of the stochastic Moore machine in this case
is arbitrary. This makes sense since according to the POMDP that we use to
interpret the machine this input is impossible, so our interpretation should tell
us nothing about this situation.
Interpreting systems as solving POMDPs 13
C Sondik’s example
We now consider the example from [15]. This has a known optimal solution. We
constructed a stochastic Moore machine from this solution which has an inter-
pretation as a solution to Sondik’s POMDP. This proves existence of stochastic
Moore machines with such interpretations.
Consider the following stochastic Moore machine:
– State space M := [0,1]. (This state will be interpreted as the belief proba-
bility of the hidden state being equal to 1.)
– input space I ={1,2}
– machine kernel µ : I ×M → PM defined by deterministic function g :
I×M→M:
µ(m′|s,m):=δ (m′) (21)
g(s,m)
where
(cid:40)
15 − 1 if 0≤m≤0.1188
g(S =1,m):= 6m+20 2 (22)
9 − 72 if 0.1188≤m≤1.
5 5m+60
and
(cid:40)
2+ 20 if 0≤m≤0.1188
g(S =2,m):= 3m−15 (23)
−1 − 12 if 0.1188≤m≤1.
5 5m−40
– output space O :={1,2}
– expose kernel ω :M→O defined by
(cid:40)
1 if 0≤m<0.1188
ω(m):= (24)
2 if 0.1188≤m≤1.
A consistent interpretation as a solution to a POMDP for this stochastic Moore
machine is given by
– The POMDP with
• state space H:={1,2}
• action space equal to the output space O of the machine above
• sensor space equal to the input space I of the machine above
• model kernel κ:H×O →H×I defined by
κ(h′,s|h,a):=ν(h′|h,a)ϕ(s|h′,a) (25)
where ν :H×O →PH and ϕ:H×O →PI are shown in table 1
• reward function r :H×O →R also shown in table 1.
– Markov kernel ψ :M→PH given by:
ψ(h|m):=mδ1(h)(1−m)δ2(h). (26)
14 M. Biehl, N. Virgo
Action a∈O ν(h′|h,A=a) ϕ(s|h′,A=a) r(h,A=a)
    (cid:32) (cid:33)
1/51/2 1/53/5 4
1    
4/51/2 4/52/5 −4
    (cid:32) (cid:33)
1/22/5 9/102/5 0
2    
1/23/5 1/103/5 −3
Table 1. Sondik’s POMDP data.
To verify this we have to check that (H,O,ψ,ω,κ) is a consistent Bayesian
influenced filtering interpretation of the machine (M,I,µ). For this we need to
check eq. (2) with δ (a):=δ (a). So for each each m∈[0,1], h′ ∈{1,2},
α(m) ω(m)
i∈{1,2}, and m′ ∈[0,1] we need to check:
(cid:32) (cid:33)
(cid:88) (cid:88)
κ(h′,i|h,a)ψ(h|m)δ (a) µ(m′|i,m)=
ω(m)
h∈H a∈A
(27)
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
ψ(h′|m′) κ(h′′,i|h,a)ψ(h|m)δ (a) µ(m′|i,m).
ω(m)
h∈Ha∈Ah′′∈H
This is tedious to check but true. We would usually also have to show that ω is
indeed the optimal policy for Sondik’s POMDP but this is shown in [15].
D POMDPs and belief state MDPs
Here we give some more details about belief state MDPs and the optimal value
functionandpolicyofeqs.(4)and(5).Thereisnooriginalcontentinthissection
and it follows closely the expositions in [9,10].
We first define an MDP and its solution and then discuss then add some
details about the belief state MDP associated to a POMDP.
Definition 6. A Markov decision process (MDP) can be defined as a tuple
(X,A,ν,r) consisting of a set X called the statespace, a set A called the action
space, a Markov kernel ν : X ×A → P(X) called the transition kernel, and a
reward function r :X ×A→R. Here, the transition kernel takes a state x∈X
and an action a∈A to a probability distribution ν(x,a) over next states and the
reward function returns a real-valued instantaneous reward r(x,a) depending on
the hidden state and an action.
A solution to a given MDP is a control policy. As the goal of the MDP we
here choose the maximization of expected cumulative discounted reward for an
infinite time horizon (an alternative would be to consider finite time horizons).
This means an optimal policy maximizes
(cid:34) ∞ (cid:35)
(cid:88)
E γt−1r(x ,a ) . (28)
t t
t=1
Interpreting systems as solving POMDPs 15
where 0 < γ < 1 is a parameter called the discount factor. This specifies the
goal.
Toexpresstheoptimalpolicyexplicitlywecanusetheoptimalvaluefunction
V∗ :X →R. This is the solution to the Bellman equation [10]:
(cid:32) (cid:33)
(cid:88)
V∗(x)=max r(x,a)+γ ν(x′|a,x)V∗(x′) . (29)
a∈A
x′∈X
Theoptimalpolicyisthenthefunctionπ∗ :X →Athatgreedilymaximizesthe
optimal value function [10]:
(cid:32) (cid:33)
(cid:88)
π∗(x)=argmax r(x,a)+γ ν(x′|a,x)V∗(x′) . (30)
a∈A
x′∈X
D.1 Belief state MDP
ThebeliefstateMDPforaPOMDP(seedefinition4)isdefinedusingthebelief
state update function of eq. (3). We first define this function again here with an
additional intermediate step:
f(b,a,s)(h′):=Pr(h′|b,a,s) (31)
Pr(h′,s|b,a)
= (32)
Pr(s|b,a)
(cid:80) κ(h′,s|h,a)b(h)
= h∈H . (33)
(cid:80) κ(h¯′,s|h¯,a)b(h¯)
h¯,h¯′∈H
The function f(b,a,s) returns the posterior belief over hidden states h given
prior belief b∈PH, an action a∈A and observation s∈S. The Markov kernel
δ :PH×S×A→PPHassociatedtothisfunctioncanbeseenasaprobability
f
of the next belief state b′ given current belief state b, action a and sensor value
s:
Pr(b′|b,a,s)=δ (b′). (34)
f(b,a,s)
Intuitively, the belief state MDP has as its transition kernel the probability
Pr(b′|b,a) expected over all next sensor values of the next belief state b′ given
thatthecurrentbeliefstateisbtheactionisaandbeliefsgetupdatedaccording
to the rules of probability, so
(cid:88)
Pr(b′|b,a)= Pr(b′|b,a,s)Pr(s|b,a) (35)
s
(cid:88) (cid:88)
= δ (b′) κ(h′,s|h,a)b(h). (36)
f(b,a,s)
s∈S h,h′∈H
This gives some intuition behind the definition of belief state MDPs.
16 M. Biehl, N. Virgo
Definition 7. GivenaPOMDP(H,A,S,κ,r)the associatedbeliefstateMarkov
decision process (belief state MDP) is the MDP (PH,A,β,ρ) where
– the state space PH is the space of probability distributions beliefs over the
hidden state of the POMDP. We write b(h) for the probability of a hidden
state h∈H according to belief b∈PH.
– the action space A is the same as for the underlying POMDP
– the transition kernel κ:PH×A→PH is defined as [10, Section 3.4]
(cid:88) (cid:88)
β(b′|b,a):= δ (b′) κ(h′,s|h,a)b(h). (37)
f(b,a,s)
s∈S h,h′∈H
– the reward function ρ:PH×A→R is defined as
(cid:88)
ρ(b,a):= b(h)r(h,a). (38)
h∈H
So the reward for action a under belief b is equal to the expectation under
belief b of the original POMDP reward of that action a.
D.2 Optimal belief-MDP policy
Using the belief MDP we can express the optimal policy for the POMDP.
The optimal policy can be expressed in terms of the optimal value function
of the belief MDP. This is the solution to the equation [9]
(cid:32) (cid:33)
(cid:88)
V∗(b)=max ρ(b,a)+γ β(b′|a,b)V∗(b′) (39)
a∈A
b′∈PH
 
(cid:88) (cid:88) (cid:88)
V∗(b)=maxρ(b,a)+γ δ
f(b,a,s)
(b′) κ(h′,s|h,a)b(h)V∗(b′)
a∈A
b′∈PHs∈S h,h′∈H
(40)
 
(cid:88) (cid:88)
V∗(b)=maxρ(b,a)+γ κ(h′,s|h,a)b(h)V∗(f(b,a,s)). (41)
a∈A
s∈Sh,h′∈H
This is the expression we used in eq. (4). The optimal policy for the belief MDP
is then [9]:
 
(cid:88) (cid:88)
π∗(b)=argmaxρ(b,a)+γ κ(h′,s|h,a)b(h)V∗(f(b,a,s)). (42)
a∈A
s∈Sh,h′∈H
This is the expression we used in eq. (5).

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Interpreting systems as solving POMDPs: a step towards a formal understanding of agency"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
