=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Integrating cognitive map learning and active inference for planning in ambiguous environments
Citation Key: maele2023integrating
Authors: Toon Van de Maele, Bart Dhoedt, Tim Verbelen

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2023

Key Terms: graph, ambiguous, environments, planning, maps, research, cognitive, clone, learning, inference

=== FULL PAPER TEXT ===

Integrating cognitive map learning and active
inference for planning in ambiguous
environments
Toon Van de Maele1, Bart Dhoedt1, Tim Verbelen2,⋆ and Giovanni Pezzulo3,∗
1 IDLab, Department of Information Technology, Ghent University - imec, Belgium
2 VERSES Research Lab, Los Angeles, USA
3 Institute of Cognitive Sciences and Technologies, National Research Council, Italy
toon.vandemaele@ugent.be
Abstract. Living organisms need to acquire both cognitive maps for
learningthestructureoftheworldandplanningmechanismsabletodeal
withthechallengesofnavigatingambiguousenvironments.Althoughsig-
nificant progress has been made in each of these areas independently,
the best way to integrate them is an open research question. In this pa-
per, we propose the integration of a statistical model of cognitive map
formation within an active inference agent that supports planning un-
der uncertainty. Specifically, we examine the clone-structured cognitive
graph (CSCG) model of cognitive map formation and compare a naive
clone graph agent with an active inference-driven clone graph agent, in
three spatial navigation scenarios. Our findings demonstrate that while
bothagentsareeffectiveinsimplescenarios,theactiveinferenceagentis
more effective when planning in challenging scenarios, in which sensory
observations provide ambiguous information about location.
Keywords: Cognitive map · Active inference · Navigation · Planning
1 Introduction
Cognitive maps [1] are mental representations of spatial and conceptual rela-
tionships.Theyareconsideredessentialcomponentsforintelligentreasoningand
planning,astheyareoftenassociatedwithnavigationinhumansandrodents[2].
Forthisreason,alotofrecentdevelopmentsinbothneuroscienceandcomputer
science have been building computational models of cognitive maps [3].
These advances in the field [4,5] are very impressive in learning abstract
representations and even show that biological patterns such as grid cells [4], or
splittercells[5]canemergefromlearning.However,theseworkstypicallydonot
focus on complex planning tasks and only consider naive or greedy strategies.
In this paper, we investigate the potential of active inference as a planning
mechanism for these cognitive maps. Active inference is a corollary of the free
energy principle which states that intelligent agents infer actions that minimize
⋆ Equal Contribution
3202
guA
61
]IA.sc[
1v70380.8032:viXra
2 T. Van de Maele et al.
theirexpectedfreeenergy.Thisisaproxyorboundonexpectedsurprise,yielding
a natural trade-off between exploration and goal-driven exploitation [6,7]. We
aim to investigate the impact of active inference as a planning mechanism on
the performance of cognitive maps in spatial navigation strategies, especially in
terms of disambiguating the “mental position” and decision-making efficiency.
In particular, we look at the clone-structured cognitive graph (CSCG) [5]:
a unifying model for two essential properties of cognitive maps. First, flexible
planning behavior, i.e. if observations are not consistent with the expected ob-
servation in the plan, the plan can be adapted. Second, the model is able to
disambiguate aliased observations depending on the context in which it is en-
countered,e.g.inspatialalternationtasksatthesamelocationdifferentdecisions
are made depending on context [8]. Given the CSCG’s inherent mechanism for
disambiguating aliased observations, we hypothesize that coupling it with ac-
tive inference as a planning system will enable the identification of the optimal
sequence that accurately represents the agent’s location.
Toinvestigatethishypothesizedbenefitofactiveinference,wecompareboth
a naive clone graph and an active inference-driven clone graph for navigating
toward goals on two separate metrics: the number of steps it takes for an agent
to reach the goal and the overall success rate. We design three distinct spa-
tial navigation scenarios, each with a different complexity. First, we consider a
slightlyambiguous(openroom)environmentdescribedby[5]whereweevaluate
the structure learning mechanism and planning algorithms for both models. We
then increase the level of ambiguity in a maze described in [9] where we believe
thatinformation-seekingbehaviorwillbecrucialforself-localization.Finally,we
evaluate the performance in the T-maze, where an agent is punished for mak-
ing the wrong choice by ending the episode. To summarize, the contributions of
this paper are: (i) we show how to use the learned structure of a CSCG as the
generativemodelwithintheactiveinferenceframework,(ii)weshowthatactive
inference agents are significantly faster in disambiguating the state in highly
ambiguous environments than greedy planning agents, and (iii) we show that
active inference agents make more careful decisions by first gathering evidence,
yielding higher success rates for finding the reward in the T-maze environment.
2 Methods
Inthissection,wefirstdescribethemechanismsdrivingstandardclone-structured
cognitive graphs for structure learning. Then we provide a brief summary of the
activeinferenceframeworkandhowtheactionisdriventhroughBayesianinfer-
ence. Finally, we conclude this section by showing how the CSCG can be used
as a generative model within the active inference framework.
2.1 Clone-Structured Cognitive Graphs
Clone-structured cognitive graphs (CSCG) [5] are a computational implementa-
tion of a cognitive map that models the joint probability of a sequence of action
Active Inference with Cognitive Maps 3
(a) (b)
Fig.1:(a)Amappingofasequenceofobservationstodistinctclonestatesinthe
clone-structured cognitive graph. The color indicates clones belonging to a spe-
cificobservation,i.e.foreachcoloredobservationtherearetwoclonesstatesfrom
whichitcantransitionintoeitherclonestatebelongingtothenextobservation.
(b) The factor graph describing an active inference driven partially observable
Markov decision process (POMDP). π denotes the policy, which is sampled ac-
cording to the expected free energy G, dependent on the preference matrix C.
Thehiddenstatesoftheagents areinitializedusingthepriormatrixD.These
t
states are then transitioned according to the B matrix, conditioned on the se-
lected policy. Finally, the observed outcome variables are generated through the
likelihoodfactor(Amatrix).Observedvariablesaredenotedinlightbluecircles,
while unobserved variables are denoted in white circles. The factors describing
the generative model are denoted in a dark blue square.
and observation pairs. They are a variation of the action-augmented hidden
Markov model, where the next state and action are conditioned on the current
state and action. The crucial difference is that these clone-structured cognitive
graphs are able to disambiguate aliased observations based on the context (e.g.
the previously visited trajectory), which is a property that is also observed in
hippocampal splitter cells.
InorderforaCSCGtobeabletodisambiguateobservations,itneedsdistinct
states for each observation based on its context - in this case, the previous
observations and actions. All states corresponding to a single observation are
called the clones of this observation, and by design, each state deterministically
maps to a single observation. In essence, a CSCG is a hidden Markov model in
whichmultipledifferentvaluesofthehiddenstatepredictidenticalobservations
(i.e. their corresponding columns in the transition matrix are non-identical). A
pair of the clone states in a CSCG is therefore a set of two values that a hidden
state might take which share identical likelihood contingencies, but differ in
4 T. Van de Maele et al.
their transition probabilities. A depiction of the clone graph, as described in [5]
is shown in Figure 1a.
The CSCGs are optimized by minimizing the variational free energy over
a sequence of observation-action pairs using the Baum-Welch algorithm [10],
an expectation-maximization scheme for hidden Markov models. Through this
optimization and random initialization, the model will converge to use distinct
clonestatesfordifferentsequencesinthedata.Thisdistinctionbetweenclonesis
furtherimprovedbyoptimizingthelearnedmodelparametersthroughaViterbi
decoding step, only keeping the states necessary for the maximum likelihood
paths in the learned model.
2.2 Clone graph agent
Wedefineaclonegraphagentthatusesagreedyplanningapproachtoselectthe
actions. Planning using the clone-structured cognitive graphs is done by setting
a fixed target state (or states), and forward propagating the messages starting
from the current state. When one of the target states is assigned a non-zero
probability, a path is found and the maximum likelihood states are backward
propagated to retrieve the corresponding action sequence, or policy. The prob-
ability of each policy is computed as the belief over the current state Q(s|o˜,a˜).
Once the agent’s belief over state collapses to a single state, the planning mech-
anism falls back to the one described in [5], where the current state is known.
2.3 Active inference agent
Actionableagents,whetherbiologicalorartificial,areseparatedbytheirenviron-
ment through sensory inputs (perception) and action. The agent’s observations
are indirectly observed through its different sensory modalities, while the world
state is also only indirectly affected by the agent’s actions. This separation be-
tween the hidden variables (action, observation, agent state, and world state) is
commonly referred to as the Markov blanket.
Thefreeenergyprincipleproposesthatanagentpossessesagenerativemodel
that describes how outcomes are generated from the world state and how the
worldstateisaffectedbytheagent’sactions.Theprinciplestatesthattheagents
willminimizetheirsurprise,boundedbythevariationalfreeenergybyupdating
the parameters of the generative model (learning) or inferring the hidden state
(perception). Active inference agents can infer the action that minimizes the
“expected free energy (G)” (or in other words, the free energy of the future
courses of actions) [6].
Active inference assumes that actions are inferred through the minimization
of the expected free energy G. This means that the posterior over a policy is
proportional to the expected free energy G, which can be computed for each
policy. More specifically, approximate posterior over policy Q(π) is computed
as the softmax (σ) over the categorical over all the policies with a value of the
respective expected free energy G, γ is a temperature variable:
Q(π)=σ(−γG(π)),
Active Inference with Cognitive Maps 5
Where the expected free energy G of this model, for a fixed time horizon T, is
defined as in [11]:
T
(cid:88)
G(π)= G(π,τ)
τ=t+1
G(π,τ)≥−E (cid:2) D [Q(s |o ,π)||Q(s |π)] (cid:3) −E (cid:2) logP(o) (cid:3)
Q(oτ|π) KL τ τ τ Q(oτ|π)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Epistemicvalue PragmaticValue
This equation decomposes in two distinct terms: an epistemic value computing
the information gain term over the belief over the state, and a pragmatic value
(or utility) term with respect to a preferred distribution over the observation
P(o). In active inference, the goal of an agent is encoded in this prior belief as
a preference. In a CSCG, planning is done by setting a preferred state, whereas
in active inference this is typically done by setting the preferred observation.
In order to make both approaches comparable, here we always plan by setting
preferred states (and assume an identity mapping between the state and obser-
vation).
Evaluating the expected free energy G for all the considered policies is ex-
ponential w.r.t. the time horizon T. This limits the tree depth to low values
for which this is practically computable. To mitigate this limitation, we set the
preference for each state proportional to the distance toward the goal state (in
thecognitivemap).Whilethissystemsimplifiescomputingtheutilitytobesuf-
ficient for a depth of one, the planning mechanism still requires larger depths
for achieving (non-greedy) long-term information-seeking behavior.
CSCG as the generative model for active inference We consider active
inference in the discrete state space formulation [12], as shown in the factor
graph in Figure 1b. The generative model is therefore described by a set of four
specificmatrices:theAmatrixdefinesthelikelihoodmodel,orhowobservations
are generated from states: P(o|s), the B matrix defines the transition model, or
howthebeliefoverstatechangesconditionedonanactiona :P(s |s ,a ).The
t t+1 t t
C matrix describes the preference of the agent P(s), and finally, the D matrix
describes the prior belief over the initial state P(s).
First, we learn the world structure using a CSCG through the minimization
of the evidence lower bound with respect to the model parameters as described
in [5]. We then map the parameters of the learned hidden Markov model to the
four matrices describing the active inference model.
First, we reduce the model by only considering the states for which the
(cid:80) (cid:80)
transition probability marginalized over action and next state p(s |s,a),
s a t
assuming a uniform distribution over s and a, is larger than the threshold of
0.0001. The A matrix can be directly constructed by setting P(o |s )=1 for all
i j
remaining clones s of observation o .
j i
To construct the B matrix, the transition matrix from the trained CSCG
can be taken directly. A crucial difference between the POMPD in discrete time
active inference and the CSCG is that the actions are state-conditioned in the
6 T. Van de Maele et al.
latter. This means that starting in some states, an action can not be taken.
In the learned transition matrix, the following condition does not always hold:
(cid:80)
P(s |s ,a ) = 1. We convert this transition matrix to proper proba-
st+1 t+1 t t
bilities by adding a novel dispreferred state s , for which we set the transition
d
probability to 1 in these illegal cases, and for which this state transitions to it-
self for each possible action. We then normalize the transition matrix such that
probabilities sum to 1. We also add a P(o |s )=1 mapping in the A matrix.
d d
The preference of the agent, or C matrix, is not present in the standard
formulation of the CSCG. However, the agent is able to plan toward a goal
that is set in state space. We model this by setting a preference over this state,
or set of states in case of an observation-space preference or multiple target
goals. Additionally, for the newly added state s to which the illegal actions are
d
mapped,wesetaverylowvalue(asifitwoulddriveyoutoastatethatisfarther
away from the goal than the maximum distance) in order to drive the agent to
avoid these actions when planning according to its expected free energy.
The prior distribution over the initial state, matrix D, is initialized as a
uniformprioroverallthestates.Theagentthusstartswithnoknowledgeabout
the state it is in and has to gather evidence to change this belief.
3 Results
In this work, we compare the behavior of two agents that select their actions
usingaCSCG:theformer(“clonegraph”,Section2.2)agentplansusingagreedy
approach, whereas the latter (“active inference”, Section 2.3) agent uses active
inference and expected free energy to plan ahead. We also compare these two
agentswitharandom(“random”)agentbaseline.Inparticular,welookatgoal-
driven behavior in three distinct environments each requiring a different level
of information-seeking behavior. First, we consider an open room as proposed
in [5] in which the agent has to reach a uniquely defined corner, for which the
goal is provided as a goal observation. Second, we consider a more ambiguous
environment in which the agent has to reach the uniquely defined center of a
room, but it first needs to localize itself within the room. Finally, we evaluate
the approach on the T-maze, where the agent should first observe a cue, as a
wrong decision is “fatal”.
In each experiment, we first train the generative models as CSCGs and then
convert them to discrete state space matrices for active inference within the
PyMDP framework [11].
3.1 Navigating in an open room environment
In this first experiment, we investigate the performance of all agents in a simple
environment where we hypothesize that there is no immediate gain in using
the active inference framework for information-seeking behavior. As the clone
graph agent is still able to integrate observations to improve its belief over its
Active Inference with Cognitive Maps 7
current state, we expect both agents to gather enough evidence to accurately
plan toward the goal.
For this maze, we consider an open room environment based on the one de-
scribed in [5]. We recreate the environment within the Minigrid [13] framework.
The room is defined by a four-by-four grid in which the agent can freely navi-
gate by selecting actions like “turn left”, “turn right” or “move forward”. The
agent observes a three-by-three patch around its current position, as shown in
Figure 2b. Each corner of the environment is uniquely defined by an observable
colored patch, as shown in Figure 2a and Figure 2b. Each observed patch is
mapped to a unique index as observation. In this environment, this corresponds
to 21 observations.
WelearnthestructureoftheroombyfirsttrainingaCSCG,initializedwith
20 clones for each observation, as described in Section 2. The model parameters
werelearnedusingarandom-walksequenceconsistingof100kobservation-action
pairs. We then set the preference of the agent to the two observations reaching
the corner, e.g. for the bottom right corner this is the observation of reaching
it from the left and from the top. As described in Section 2, we select the clone
states for which the likelihood of this observation is 1 and set the preference for
all these states for both the clone graph and active inference planning schemes.
Werunanexperimentforallthreeagentswheretheagentstartsinarandom
(ambiguous,i.e.lookingatthecenter)poseandhastoreacharandomlyselected
corner as the goal. We run this for 400 separate trials, where each trial was
seeded with the same random seed, ensuring that the different agents start with
the same starting position and goal. We provide the agents with 25 timesteps
to reach the goal and report the success rate and episode length for each of the
agents. Qualitatively, in Figure 2a, we observe that the behavior between the
clone graph agent and the active inference agent is very similar; it first picks a
cornerwhichiseitherthegoalandtheepisodeendsoraninformativelandmark,
and then the agent moves towards the goal.
Quantitatively, we observe the duration of the episode and see that the av-
erage episode length shown in Figure 2c is significantly larger for the random
agent with respect to both the clone graph agent (2-sample independent t-test,
p-value=7.6·10−6) and the active inference agent (2-sample independent t-test,
p-value=3.6·10−5), illustrating that the model has learned the structure of the
worldandisnotmovingrandomly.Secondly,weobservethattheaverageepisode
length of the clone graph agent does not significantly differ from the active in-
ference agent (2-sample independent t-test, p-value=0.237), illustrating that for
thisenvironmenttheinformation-seekingbehaviordoesnotbenefitperformance.
ThisisfurtherevidencedbythesuccessrateshowninFigure2d,wheretheper-
formance of both agents does not significantly differ as they are identical at a
100% success rate.
From this experiment, we conclude that in an environment where the agent
canquicklyfindanunambiguouslandmarksuchasthecornersintheopenroom,
both agents have similar performance.
8 T. Van de Maele et al.
(a)
(b) (c) (d)
Fig.2:(a)Qualitativeresultsofnavigatingtheopenroommazeforthedifferent
agents with different random seeds. The agent is tasked with reaching a partic-
ular corner in the maze. The trajectory of the agent is marked, and the arrow
points the direction in which the agent is looking. (b) The two three-by-three
observations defining a goal in a corner of the open room maze. (c) A box plot
representing the statistics of the amount of time until the goal is reached (only
the success scenarios are considered) over 400 trials. (d) The success rate of the
agent in reaching the goal observation (computed over 400 trials).
3.2 Self-localization in an ambiguous maze
Inthepreviousenvironment,theagentwasabletoquicklyself-localizeasrandom
actions would easily disambiguate where in the environment they are. In this
experiment, we increase the level of ambiguity and evaluate whether the active
inference agent is able to self-localize faster than the clone graph agent.
For this experiment, we consider the highly ambiguous maze from Friston et
al. [9] shown in Figure 3a. In this environment, the agent is only able to observe
Active Inference with Cognitive Maps 9
(a)
(b) (c)
Fig.3: (a) Qualitative results of navigating the ambiguous maze with the three
different agents. The green square marks the goal observation, the trajectory of
theagentismarkedinblack.Inthismaze,theagentcanonlyobservethecurrent
tile, and the color of the tile represents the observation the agent receives. (b)
Showstheamountofstepsneededforreachingthetarget,onlymeasuredforthe
success cases. (c) Shows the success rate, computed over 400 trials for the three
agents.
the one-by-one tile the agent is currently standing on, i.e. if it is a red, white, or
green tile. While the red and white tiles are highly ambiguous, there is only a
singlegreentileatthecenterofthemaze.Theagentisabletonavigatethemaze
through actions like “up”, “down”, “left” or “right”, and is only limited by a
wall around the maze. Unique observation tiles are again mapped to categorical
indices.
We construct a CSCG with 40 clones per observation and optimize it over
a sequence of 10k steps in the environment until convergence. We then set the
preference for this environment as the green tile, in a similar fashion as we did
10 T. Van de Maele et al.
in the experiment in Section 3.1 for both the clone graph agent and the active
inference agent.
In this environment, the agent’s goal is always to go to the green tile in the
center of the room. However, the agent starts at a random position on a white
tile.Weagainrunthisexperimentfor400trialsforeachagent,seededovertrials
such that the starting position is the same for each agent. Each episode has a
max duration of 25 steps, and we record the episode length and the success rate
of the agents. Qualitatively, we can see the trajectories taken by the clone and
active inference agents in Figure 3a. We observe that both agents are able to
solve the task, seemingly moving randomly in the maze. However, we also ob-
serve the random agent navigating in the maze, which typically does not reach
thegoal.Quantitatively,weagainmeasurethattheclonegraphagent(2-sample
independent t-test, p-value=1·10−99) and active inference agent (2-sample in-
dependent t-test, p-value=1·10−168) significantly differ from the random agent,
showing goal-directed behavior. However, we now observe that the clone graph
agent with a mean episode duration of 10.92 steps is significantly slower than
the active inference agent with a mean episode duration of 7.92 steps (2-sample
independentt-test,p-value=3.46·10−22)eventhoughtheirsuccessrateissimilar
with 98.5% for the clone graph agent and 100% for the active inference agent.
From this experiment, we conclude that in highly ambiguous environments,
agentsusingactiveinferenceforgoal-drivenbehaviordisambiguatetheirlocation
and reach the goal faster than agents who do not.
3.3 Solving the T-Maze
In this final experiment, we consider an environment where making informative
decisionsiscrucial.Wecomparetheperformanceoftheagentsinthequintessen-
tialactiveinferenceenvironment:theT-maze[14].Inthisenvironment,theagent
mustmakeachoicetogoeitherintheleftortherightcorridorwithoutbeingable
toobservethelocationofthereward(wehideitbehindadoor),andtheepisode
ends when it makes a decision. The agent is, however, able to disambiguate the
location of the reward by observing a colored cue behind itself.
We create the environment again in the Minigrid environment [13], and the
agent has three-by-three patches as observations and can act by either “turn-
ing left”, “turning right” or “moving forward”. The agent always starts in an
upwards-looking position, looking away from the cue. Additionally, when the
agent wants to walk through a door, it immediately goes to the tile behind the
door, ending the episode either in reward or not.
WetrainaCSCGwith5clonesperobservationon500distinctepisodeswith
a maximum length of 50 steps, however, these episodes are typically shorter as
the agent goes through a door. Similar to the open room environment, we map
eachthree-by-threeobservationpatchtoauniqueindexandadditionally,wealso
map the reward to a separate observation. This yields 17 unique observations
the agent can observe. We then set the preference to the rewarding observation
for both the clone and active inference agents, and depending on context, the
agent should be able to infer a different path towards the goal.
Active Inference with Cognitive Maps 11
(a)
(b) (c)
Fig.4: (a) Qualitative results of navigating the T-maze with the three different
agents. The green square marks the goal observation, and the black arrows the
trajectory followed by the agent. At the bottom of the T, there is a colored
cue, blue marks that the goal is on the right, while red marks that the goal is
on the left. (b) Shows the number of steps needed for reaching the target, only
measured for the success cases. (c) Shows the success rate, computed over 400
trials for the three agents.
We again conduct 400 random trials, where the seed is again fixed for each
trial within an agent, ensuring that for each trial the goal location is the same.
When we evaluate the behavior of the agents qualitatively (Figure 4a), we ob-
serve that the active inference agent always moves forward, turns around and
checks for the cue, and then moves towards the correct goal location. In con-
trast, the clone graph agent randomly picks a direction as it has not accurately
inferred in which state it currently is. Interestingly, when the stochasticity of
the action sampling forces the agent to turn around and it observes the cue,
it chooses the correct action. This explains the 56.75% success rate, which is
slightly higher than the expected 50% of selecting actions randomly. In this en-
vironment, where thoughtless decisions are punished, the active inference agent
issignificantlymoreaccuratewithasuccessrateof100%(2-sampleindependent
z-testforproportions,p-value=6.25·10−50).Interestingly,theclonegraphagent
12 T. Van de Maele et al.
issignificantlyfasterwithanaverageof4.5stepsthantheactiveinferenceagent
with an average of 5 steps (2-sample independent t-test, p-value=2.86·10−5).
This is attributed to the fact that the agent does not take the time to observe
the cue and moves towards wherever it believes the goal is.
Fromthisexperiment,weconcludethatininformation-criticaldecision-making
environments using active inference provides a significant benefit over greedy
planning strategies.
4 Discussion
We relate our work to representation learning in complex environments. In the
contextoflearningcognitivemaps,workhasbeendonethatexplicitlyseparates
the underlying spatial structure of the environments with the specific items
observed [4]. While this model does not entail a generative model, other ap-
proachesdoconsiderthehippocampusasagenerativemodel[15]andshowthat
through generative processes novel plans can be created. Model-based reinforce-
ment learning systems learn similar world models directly from pixels [16] and
are able to achieve high performance on RL benchmarks. All these approaches
typically treat planning as a trivial problem that can be solved through forward
rollouts, or by value optimization using the Bellman equation, however, they do
not consider the belief over the state as a parameter.
Within the active inference community, a lot of work has been applied to
planning in different types of environments. Casting navigation as inferring the
sequence of actions under the generative model using deep neural networks has
been done before in [17,18], where the approximate posterior is implemented
through a variational deep neural network. The active inference framework has
alsobeensuccessfulinsolvingvariousRLbenchmarks[19,20].Theseapproaches
show that inferring action through surprise minimization is powerful in solving
a wide range of tasks, although they do not explicitly deal with aliasing in
observations.
We believe that the combination of both approaches can yield a promising
avenue for building cognitive maps in silico that can be used to solve important
real-world tasks such as navigation.
The CSCG has been shown to be a powerful model for flexible planning and
disambiguating aliased observation, making it the perfect candidate for integra-
tion within the active inference framework. Through this interaction with the
inherentuncertainty-resolvingbehaviorofactiveinference,wehaveobservedsig-
nificant improvements in terms of success rate or episode lengths depending on
the specific environment.
Another open issue that we plan to resolve in the future is the fact that the
CSCG is currently learned in an offline fashion. Therefore our current approach
is not benefitting from the curiosity- or novelty-based scheme of active infer-
ence[21,7],whichwehypothesizetoimprovethetrainingefficiencywithrespect
to the number of required samples.
Active Inference with Cognitive Maps 13
5 Conclusion
We first propose a mechanism for using the clone-structured cognitive graph
withintheactiveinferenceframework.Thisallowsustousethenaturallycontext-
dependentdisambiguatingofaliasedobservationsinthegenerativemodelwithin
theactiveinferenceframeworkthatnaturallywillseekthesequencebestaligned
with this purpose. Through evaluation in three distinct environments, we have
highlighted the advantages of active inference compared to more simplistic and
greedyplanningmethods.Weshowthatinnaturallyunambiguousenvironments,
the active inference and clone agents perform similarly in both success rate and
time to reach the goal. Additionally, we have observed that the active inference
agent exhibits a significantly higher success rate in environments requiring in-
formed decision-making. Finally, we show that in environments where an agent
has to make an informed decision, the active inference agent has a significantly
higher success rate. These results corroborate the benefits of using an active
inference approach.
Acknowledgments This research received funding from the Flemish Govern-
ment (AI Research Program). This research was supported by a grant for a
research stay abroad by the Flanders Research Foundation (FWO).
References
1. J. O’Keefe and L. Nadel, “Pr´ecis of O’Keefe & Nadel’s The hippocampus as a
cognitive map,” Behavioral and Brain Sciences, vol. 2, pp. 487–494, Dec. 1979.
2. M.Peer,I.K.Brunec,N.S.Newcombe,andR.A.Epstein,“StructuringKnowledge
withCognitiveMapsandCognitiveGraphs,”TrendsinCognitiveSciences,vol.25,
pp. 37–54, Jan. 2021.
3. J. C. R. Whittington, D. McCaffary, J. J. W. Bakermans, and T. E. J. Behrens,
“Howtobuildacognitivemap,”NatureNeuroscience,vol.25,pp.1257–1272,Oct.
2022.
4. J. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and
T.E.Behrens,“TheTolman-EichenbaumMachine:UnifyingSpaceandRelational
Memory through Generalization in the Hippocampal Formation,” Cell, vol. 183,
pp. 1249–1263.e23, Nov. 2020.
5. D.George,R.V.Rikhye,N.Gothoskar,J.S.Guntupalli,A.Dedieu,andM.La´zaro-
Gredilla, “Clone-structured graph representations enable flexible learning and vi-
carious evaluation of cognitive maps,” Nature Communications, vol. 12, p. 2392,
Apr. 2021.
6. T.Parr,G.Pezzulo,andK.J.Friston,ActiveInference:TheFreeEnergyPrinciple
in Mind, Brain, and Behavior. The MIT Press, 2022.
7. P. Schwartenbeck, J. Passecker, T. U. Hauser, T. H. FitzGerald, M. Kronbich-
ler, and K. J. Friston, “Computational mechanisms of curiosity and goal-directed
exploration,” eLife, vol. 8, p. e41703, May 2019.
8. S. P. Jadhav, C. Kemere, P. W. German, and L. M. Frank, “Awake Hippocampal
Sharp-Wave Ripples Support Spatial Memory,” Science, vol. 336, pp. 1454–1458,
June 2012.
14 T. Van de Maele et al.
9. K. Friston, L. Da Costa, D. Hafner, C. Hesp, and T. Parr, “Sophisticated Infer-
ence,” 2020. Publisher: arXiv Version Number: 1.
10. C.F.J.Wu,“Ontheconvergencepropertiesoftheemalgorithm,”The Annals of
Statistics, vol. 11, no. 1, pp. 95–103, 1983.
11. C.Heins,B.Millidge,D.Demekas,B.Klein,K.Friston,I.Couzin,andA.Tschantz,
“pymdp: A Python library for active inference in discrete state spaces,” 2022.
Publisher: arXiv Version Number: 2.
12. L.DaCosta,T.Parr,N.Sajid,S.Veselic,V.Neacsu,andK.Friston,“Activeinfer-
ence on discrete state-spaces: A synthesis,” Journal of Mathematical Psychology,
vol. 99, p. 102447, Dec. 2020.
13. M. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic gridworld environ-
ment for gymnasium,” 2018.
14. K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo, “Active
Inference: A Process Theory,” Neural Computation, vol. 29, pp. 1–49, Jan. 2017.
15. I. Stoianov, D. Maisto, and G. Pezzulo, “The hippocampal formation as a hier-
archical generative model supporting generative replay and continual learning,”
Progress in Neurobiology, vol. 217, p. 102329, Oct. 2022.
16. D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to Control: Learning
Behaviors by Latent Imagination,” Mar. 2020. arXiv:1912.01603 [cs].
17. O.C¸atal,S.Wauthier,C.DeBoom,T.Verbelen,andB.Dhoedt,“LearningGen-
erativeStateSpaceModelsforActiveInference,”FrontiersinComputationalNeu-
roscience, vol. 14, p. 574372, Nov. 2020.
18. O. C¸atal, T. Verbelen, T. Van De Maele, B. Dhoedt, and A. Safron, “Robot nav-
igation as hierarchical active inference,” Neural Networks, vol. 142, pp. 192–204,
Oct. 2021.
19. A.Tschantz,M.Baltieri,A.K.Seth,andC.L.Buckley,“Scalingactiveinference,”
2019. Publisher: arXiv Version Number: 1.
20. Z. Fountas, N. Sajid, P. A. M. Mediano, and K. Friston, “Deep active inference
agentsusingMonte-Carlomethods,”Oct.2020. arXiv:2006.04176[cs,q-bio,stat].
21. R. Kaplan and K. J. Friston, “Planning and navigation as active inference,” Bio-
logical Cybernetics, vol. 112, pp. 323–343, Aug. 2018.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Integrating cognitive map learning and active inference for planning in ambiguous environments"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
