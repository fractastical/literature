=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: An Internal Model Principle For Robots
Citation Key: weinstein2024internal
Authors: Vadim K. Weinstein, Tamara Alshammari, Kalle G. Timperi

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2024

Key Terms: principle, system, surprise, environment, robots, robot, minimization, internally, make, model

=== FULL PAPER TEXT ===

An Internal Model Principle For Robots
Vadim K. Weinstein, Tamara Alshammari, Kalle G. Timperi,
Mehdi Bennis, Steven M. LaValle
University of Oulu
Abstract. When designing a robot’s internal system, one often makes assumptions about the
structureoftheintendedenvironmentoftherobot.Onemayevenassignmeaningtovariousinternal
components of the robot in terms of expected environmental correlates. In this paper we want to
make the distinction between robot’s internal and external worlds clear-cut. Can the robot learn
about its environment, relying only on internally available information, including the sensor data?
Are there mathematical conditions on the internal robot system which can be internally verified
and make the robot’s internal system mirror the structure of the environment? We prove that
sufficiency is such a mathematical principle, and mathematically describe the emergence of the
robot’s internal structure isomorphic or bisimulation equivalent to that of the environment. A
connectiontothefree-energyprincipleisestablished,whensufficiencyisinterpretedasalimitcase
of surprise minimization. As such, we show that surprise minimization leads to having an internal
modelisomorphictotheenvironment.ThisalsoparallelstheGoodRegulatorPrinciplewhichstates
thatcontrollingasystemsufficientlywellmeanshavingamodelofit.Unlikethementionedtheories,
ours is discrete, and non-probabilistic.
Keywords: Internal Model Principle · Free Energy Principle · Semantics · Good Regulator Prin-
ciple · Surprise Minimization · Sufficiency · Information Transition Systems
1 Introduction
From an algorithmic perspective, we are familiar with internal robot models used for mo-
tion planning, navigation, and so on, represented with coordinates in Rn and geometric
primitives. We have also witnessed the rapid rise of deep learning approaches to robotics,
which encode internal models as parameters in a neural net, tuned from copious amounts
of input-output data. Such systems still require extensive training with vast amounts
of supervisory data from human experts and numerous reinforcement learning trials in
virtual environments [2]. Despite this, the derived robots remain unable to function effec-
tively in unfamiliar environments [15], unlike humans who can adapt to new situations
with ease. According to [18], this discrepancy may be due to the ability of humans to de-
velop internal models of how the world works. Currently, these internal models are often
pre-defined for training environments, making it difficult for robots to operate in different
settings. Therefore, it is crucial to understand how robots can independently construct
internal models that correlate with their external environments in a useful way without
relying on prior assumptions.
To address this challenge, we explore fundamental questions: What does it mean for a
robottoexploreitsenvironment?Howshouldweconceptualizeit?Whatdoesitmeanfora
robottohaveausefulinternalstructureinrelationshiptoitsexternalenvironment?These
questions are fundamental to advancing robotics as they address the core challenges of
enabling robots to develop a deeper understanding and adaptability to their surroundings.
4202
nuJ
71
]OR.sc[
1v73211.6042:viXra
2 V. K. Weinstein et al.
1.1 Connection to other disciplines and related work
The relationship between the structure of an internal model and that of the external en-
vironment has been studied in the context of control theory. In [5] it was shown that an
effective regulator of a given system needs to contain an isomorphic copy of the system
itself. In [6] this statement is formulated in the context of linear, time-invariant, finite-
dimensional, multi-variable control systems. The incorporation of an internal model was
formalized in terms of the divisibility of the invariant factors of the matrix describing
the (linear) dynamics of the compensator by the minimal polynomial of a matrix repre-
senting the dynamics of the exogenous signals affecting the plant. It was concluded that
the synthesis of a plant and a compensator (controller) is structurally stable only if the
compensator incorporates an internal model of the plant dynamics.
Meanwhile, for biophysics and cognitive systems, the field of active inference [22] seeks
to interpret the notions of agent, mind, and brain through the general framework of the
free-energy principle [9,23]. Its stated aim is to explain behavior of agents through the
over-arching objective of surprise minimization via action. A recent work [11] illustrates
how collective behavior can emerge from the simultaneous updating of each agent’s inter-
nal variables, guided by the minimization of surprise (indirectly via the minimization of
free energy). However, the internal variables have pre-assigned relationships with the en-
vironment (local distance between the agent and its neighbors) governed by pre-assigned
dynamics.
In the context of discrete-time stochastic dynamical systems, the problem of discov-
ering economical, predictive representations based solely on action-observation dynamics
has been formalized in the theory of predictive state representations (PSR) [19,20]. Based
on the earlier, deterministic framework of diversity-based inference [24], PSR suggests a
way to model such systems through maintaining a vector of probabilities for the likelihood
ofobservingcertainfinite-lengthaction-observationsequences,calledcoretests.Maintain-
ing a relatively low-dimensional vector of such probabilities turns out to be sufficient for
effectively modelling the statistical behaviour of the unknown dynamics.
From the perspective of classical logic, one typically deals with a language L (such as
first-order logic) and L-model M with domain dom(M). A crucial aspect of the model
involves functions that map elements of dom(M) to constant symbols in L, subsets of
dom(M)n to n-ary relation symbols in L, and functions from dom(M)n to dom(M) to n-
aryfunctionsinL.Thismappingisthesemanticfunctionandassignsmeaningtoelements
of the language L, relating them to elements of the world. This models, in particular, our
understanding of natural language. For example, the word “truck” represents an actual ob-
ject in the world, and as humans, we inherently understand this connection. Regardless of
whether we can explain the origin or the “reality” of this connection, we can still formalize
it using semantic functions. By formalizing these connections, we can better understand
and analyze the structure of languages and theories. Consequently, these semantic func-
tions allow us to introduce the following informal definition: A logical theory or a theory
of agency is representationalist if and only if it relies on semantic functions. In this sense,
all logical theories – from classical first-order and second-order logic to intuitionistic logic
and dependence logic – are representationalist.
An Internal Model Principle For Robots 3
Whentheinternalstructure(ofahumanorrobot“brain”)ispostulatedtobearcontent
or represent external states, the theory doing so is called representational. For example,
in robotics, the internal state is often understood as serving as a representation1 of its
external environment. However, the mapping, the semantic function, which assigns mean-
ings to this representation to relate it to elements of the external environment is often
overlooked. Theoreticians or designers frequently assume that the environment is struc-
tured in a certain way and that sensor data reflects that structure in some pre-defined
manner [14,27]. This assumption helps them to infuse the symbols in the robot’s internal
structurewithmeanings.Nonetheless,suchanassumptionlimitstheapplicabilityofmany
robotic solutions to more realistic scenarios, such as search and rescue tasks, where the
environment is previously unknown. Navigating an unknown environment presents signif-
icant challenges. A high degree of complexity and autonomy is required for the robot to
make decisions based on the data it observes [1]. To design a robot capable of autonomous
learning, we should separate the meaning assigned by the designers to the labels in the
robot’s brain from the meaning the robot itself assigns to them. For the robot to be truly
flexible and able to generalize to unseen environments, it should be capable of learning
meanings based on its own histories of interaction.
Various semantic mappings were explicitly analysed in recent work in the context
of perception engineering and robotics by a subset of the authors [17]. Understanding
the nature of semantic functions is essential for developing robots capable of autonomous
learningandflexibleadaptation.Inthispaper,wemakethefollowingphilosophicalpropo-
sition: a semantic function is inherently arbitrary. By this, we mean that the semantic
function does not arise from structure in some non-arbitrary or systematic way. As an
example of structurally non-arbitrary assignment is the assignment of the fundamental
group to a topological space. Quoting the linguist Ferdinand de Saussure, “the bond be-
tween the signifier and the signified is arbitrary” [4,26]. In Saussure’s theory, the “signifier”
is the sound-image (or the form of a word), and the “signified” is the concept (or the mean-
ing associated with that word). The quote describes the relationship between the signifier
and the signified as an arbitrary relation, meaning that there is no inherent connection
between the form of a word and its meaning; rather, this relationship is established by
convention within a language community.
Enactivism, a branch of (philosophy of) cognitive science, argues that postulating
representations and semantic content is often philosophically questionable and practically
useless[10,12,13,28].Thisargumentcouldbeextendedtoroboticsinthelightoftheabove
discussion. Enactivism argues that the complex structure of the brain is there not to
represent the world around us, but rather to make our engagement with it more efficient.
A counterargument is that building elaborate representations could actually help with
efficient engagement. There are two counter-counterarguments. First is that elaborate
represenations might be useful, but they might also be unnecessary. Is it possible to
circumvent elaborate representations and go directly to efficient engagement? The second
is that when some internal configuration has structural similarities to the environment, it
does not mean it is a representation. In our context specifically we see (Corollary 34) that
1 That is, a correspondence between the properties or elements of the internal model with those of the external
environment is assumed either implicitly or explicitly.
4 V. K. Weinstein et al.
such internal structures emerge even when it is not the intention or the goal of the system
to generate them. For more on the second counter-counterargument from a philosophical
perspective, see [12,13].
1.2 Contribution
We develop a mathematical framework which enables robots to learn an internal model of
theexternalenvironmentbyrelyingsolelyonquantitieswhichcanbeinternallyevaluated.
This can be seen as emergent structural semantics. The robot only engages in resolving
internal “conflicts” such as minimization of surprise.
We mathematically prove that resolving these internal conflicts invariably results in
the emergence of an internal structure that is structurally similar to the environment.
The main statement capturing this intuition is Theorem 28.
One application even shows that sometimes only proprioceptive data is enough for the
emergence of an internal structure which is isomorphic to the external state space, even
though no other sensor data is available, see Remark 2 and Example 2. An example of
“embodied cognition”.
In this regard, our work can be viewed as a first step towards a discrete, combinatorial,
non-probabilistic formulation of the free-energy principle (FEP) [7,8,9,22]. The FEP has
gained significant attention as a model of the brain and “living systems” in general. It has
two principal components. One is the analysis of the independence of a “living system”
from the rest of the world, formalized through Markov blankets. Another component is
the idea of minimization of surprise by the system and the emergent internal structure
which results from it. This aspect of the FEP is the one we “replicate” in this paper.
Unlike most available accounts on the FEP, our framework is not probabilistic, but rather
combinatorial and in line with the previous work on history information spaces [25,30].
Our results are also limit-cases in which the surprise in not just minimized, but actually
brought to zero. The result can also be seen as a combinatorial and deterministic version
of the Good Regulator Theorem presented in [5].
2 Surpriseless couplings and bisimulation
The main tenet of the FEP that we focus on is surprise minimization. The notion of
sufficiency is the quintessential notion of surprise minimization in the framework of infor-
mation transition systems [16,25,30]. By definition, sufficiency is a condition posed on an
equivalencerelationoveradynamicalsystemwhichstatesthateachequivalence“predicts”
thenextequivalenceclass.Thisallowstotransferthenotionofsurpriseminimizationfrom
the probabilistic context (where it is measured by entropy) to the context of countable
discrete dynamical systems and transition systems. In this section we introduce basic
definitions and the first result stating a connection between surprise minimization and
equivalence (in this case bisimulation equivalence) of the internal state space with the
external state space.
We will fix U and Y to be the sets of motor commands and sensor data, respectively.
An Internal Model Principle For Robots 5
Definition 1 A deterministic transition system (DTS) is a triple S = (S,A,τ) where
τ: S ×A → S is the transition function. A labeled DTS is (S,A,τ,h) where (S,A,τ) is
DTS and h is some labeling function. A (labeled) sensorimotor transition system is a DTS
with A = U ×Y and a possible labeling function h with range in Y. A (labeled) internal
system is a (resp. labeled) sensorimotor transtion system with S = I the internal state
space and τ = φ the information transition function. The external system, or environment
is a labeled DTS with S = X, A = U, τ = f and the range of h being Y. Both external
and internal systems are special cases of sensorimotor transition systems. ⊣
One may ask why is the input to the internal system U×Y, and not just Y. Typically
we expect the behaviour of a robot to depend on the history of sensor data, but not on its
own motor commands which are determined by the sensor data anyway. We do this for
our framework to be general enough. For example, in a situation where the robot’s policy
has not been determined yet, all possible sequences of motor commands are possible. Or
perhaps the DTS is a model of only a part of the robot, a part which does not (yet) decide
the motor commands and has to be prepared for all possibilities. An internal system is
exploratory, if it is prepared to “try” all possible motor commands indpendently of the
sensor data. Mathematically, (I,U ×Y,φ) is exploratory, if the value of φ(ι,u,y) is not
dependent on y, i.e. for all y,y′ ∈ Y we have φ(ι,u,y) = φ(ι,u,y′). For exploratory
internal systems we may drop the component Y from the definition. This leads to:
Definition 2 An exploratory internal system is a triple (I,U,φ) in which I is the internal
state space, and φ: I ×U → I. ⊣
Since we are interested in constructing internal systems and not in finding policies, we
will deal mostly with exploratory internal systems. We justified the title “exploratory” by
the intuition that the agent is “curious” and tries all possible paths notwithstanding the
sensor data. Our theorems about emergence of bisimulation equivalence and isomorphism
willjustifythistitleformallybecausetheyworkpreciselyforsuchsystems.Anotherreason
to introduce the class of exploratory systems is to simplify some of the mathematical
exposition.
Definition 3 Given a set A, the free DTS generated by A, denoted F(A), is the DTS
(A<N,A,⌢) where the state space A<N is the set of finite sequences of elements of A, and
the state-transition function is the concatenation of sequences. When A = U, this DTS
is the collection of all possible actuator sequences. If A = U ×Y, then it is the collection
of all actuation-observation, or sensorimotor, sequences and is equivalent to the history
information space I of [25], see also [16]. ⊣
hist
Definition 4 [Universal systems] The system F(U ×Y), the free DTS generated by the
set U × Y is the universal information transition system (with respect to U and Y).
Every connected internal transition system is a quotient of F(U ×Y), see Corollary 23.
The universal exploratory system is F(U). Every connected exploratory internal system
is its quotient. ⊣
6 V. K. Weinstein et al.
Definition 5 For a DTS (S,A,τ), s ∈ S, and a¯ ∈ A<N, define s ∗ a¯ by induction on
the length of a¯. If a¯ = ∅, then s ∗ a¯ = s, and if s ∗ a¯ is defined and a ∈ A, then
s∗(a¯⌢a) = τ(s∗a¯,a). ⊣
Definition 6 Given an internal system I = (I,U × Y,φ) and an external system X =
(X,U,f,h), define the coupled system X ⋆I to be the DTS (X ×I,U,g) where g: X ×
I ×U → X ×I is defined by
g(x,ι,u) = (f(x,u),φ(ι,u,h(x))). (1)
For exploratory systems, if the component Y is not written, the equation (1) becomes
g(x,ι,u) = (f(x,u),φ(ι,u)). ⊣
In view of Definition 6, U and Y constitute the “interface” between the internal and
external.
Definition 7 Given a finite sequence u¯ = (u ,...,u ) ∈ Un and states x ∈ X, ι ∈ I,
1 n
there are unique sequences x¯ = (x ,...,x ) and ¯ι = (ι ,...,ι ) such that x = x, ι = ι
1 n 1 n 1 1
and for each k < n, g(x ,ι ,u ) = (x ,ι ). Denote this x¯ by x∗u¯ and ¯ι by ι⋄u¯.
k k k k+1 k+1
Note that the definition of x∗u¯ here coincides with Definition 5. This is why we use
the same notation for it. But the operation ⋄ on I requires the coupling to be defined.
If the external system and/or the initial states need to be specified, we denote ι ⋄ u¯ =
ι⋄ u¯ = ι⋄ u¯ ⊣
X X,x,ι
Remark 1. If we view U<N as the free monoid under concatenation, then x (cid:55)→ x∗u¯ and
ι (cid:55)→ ι⋄u¯ are actions of this monoid on X and I, respectively. ⊣
The coupling restricts the internal system by removing the impossible sensorimotor
sequences. More precisely:
Definition 8 Suppose I = (I,U × Y,φ) is an internal system and X = (X,U,f,h)
external, and let (x ,ι ) ∈ X × I. Let I ↾ be the DTS (I,U,ψ) where ψ(ι,u) =
0 0 X,x0,ι0
ι⋄ u. If the initial states are clear from the context, we drop them from the subscript.
X,x0,ι0
⊣
Definition 9 [Surpriseless] Suppose I = (I,U ×Y,φ) and X = (X,U,f,h) are internal
and external systems, respectively. We say that (x ,ι ) ∈ X×I is surpriseless, if there are
0 0
no sequences u¯ = (u ,...,u ) ∈ Un and u¯′ = (u′,...,u′ ) ∈ Um such that ι ⋄u¯ = ι ⋄u¯′,
1 n 1 m 0 0
but h(x ∗u¯) ̸= h(x ∗u¯′). ⊣
0 0
Asurpriseoccursifthesensorydatacannotbe“predicted” basedontherobot’sinternal
state. Note that the condition of being surpriseless can be evaluated within the internal
model because the definition speaks only about internal states and sensory data.
Theorem 10 The coupling of F(U × Y) to any environment X = (X,U,f,h) is sur-
priseless.
An Internal Model Principle For Robots 7
Proof. In the universal automaton, any two sequences u¯,baru′ yield a different internal
state ι ⋄u¯ ̸= ι ⋄ if and only if u¯ ̸= u¯′. But if u¯ = u¯′, then also x ∗u¯ = x ∗u¯′, so we have
0 0 0 0
checked the definition. ⊔⊓
Definition 11 [Bisimulation] Suppose I = (I,U ×Y,φ,h′) is a labeled internal system.
Suppose X = (X,U,f,h) is an external system. We say that x ∈ X is bisimulation
0
equivalent to ι ∈ I in X ⋆I, if there is a relation R ⊆ X×I such that (B1.) (x ,ι ) ∈ R,
0 0 0
(B2.) for all (x,ι) ∈ R and all u ∈ U, (x∗u,ι⋄u) ∈ R, and (B3.) for all (x,ι) ∈ R we
have h(x) = h′(ι). ⊣
At first glance, bisimulation equivalence cannot be evaluated within the internal sys-
tem because by definition it requires finding a binary relation between the internal and
the external. In Theorem 14 below we prove, however, that it is equivalent to being sur-
priseless.
Definition 12 A DTS (S,A,τ) is strongly connected, if for all s,s′ ∈ S there is a¯ ∈ A
with s∗a¯ = s′. ⊣
Lemma 13 Suppose the coupled system X ⋆I is strongly connected. If (x ,ι ) ∈ X ×I
0 0
ˆ ˆ
is surpriseless, then there is a well-defined h: I → Y such that h(ι) = h(x ∗u¯) for some
0
(all) u¯ ∈ U<N with ι ∗u¯ = ι.
0
Proof.Itisenoughtoshowthatforallu¯,u¯′ ∈ U<N,ifι ⋄u¯ = ι ⋄u¯′,thenh(x∗u¯) = h(x∗u¯′),
0 0
but this is exactly the definition of surpriseless. ⊔⊓
Theorem 14 Suppose the coupled system X ⋆I is strongly connected. The following are
equivalent
1. (x ,ι ) is surpriseless in X ⋆I
0 0
ˆ
2. h is well-defined
ˆ ˆ
3. h is well-defined and x is bisimulation equivalent to ι in X ⋆I over h.
0 0
Proof. We already proved 1 → 2 (Lemma 13). Assume 2. Let
R = {(x,ι) ∈ X ×I | ∃u¯ ∈
U<N
((x,ι) = (x ∗u¯,ι⋄u¯))}.
0
ˆ
Conditions (B1) and (B2) are clearly satisfied. If (x,ι) ∈ R, then h(x) = h(x ∗u¯) = h(ι)
0
ˆ
by definition of h which proves also condition (B3). This proves 2 → 3. Clearly 3 → 2.
Suppose that 2 and we will prove 1. Suppose u¯,u¯′ are such that ι ⋄u¯ = ι ⋄u¯′. Since h ˆ is
0 0
well-defined, we have h(x ∗u¯) = h ˆ (ι ⋄u¯) = h ˆ (ι ⋄u¯′) = h(x ∗u¯′). ⊔⊓
0 0 0 0
Theorem 14 shows that if the agent had a method to reduce surprise, then once suc-
cessful, its internal system would be bisimulation equivalent to the environment. This
result is reminiscent of the Good Regulator [5]. Surpriselessness implies the theoretical
possibility to control the system completely. In a surpriseless system, the internal state
determines the results of all possible future actions (see also Lemma 27). On the other
hand we just proved that such potential to control the system implies bisimulation equiv-
alence. Bisimulation equivalence, however, is significantly weaker than isomorphism. Also,
Theorem 14 is non-constructive, it does not say whether or when surpriseless couplings
exist, or how to obtain them. These questions will be addressed in the next section.
8 V. K. Weinstein et al.
3 Theory for General Deterministic Transition Systems
In the previous section we showed that if an internal state space is adapted to the envi-
ronment with the goal of minimizing surprise, then it will become bisimulation equivalent
to it. What are the conditions under which surprise minimization leads to the internal
system being isomorphic, and not just bisimilar, to the external system? A bisimulation
equivalence between transition systems is an isomorphism, if both systems are minimal in
some sense. The robot cannot know whether the environment is in this sense minimal, but
it can “know” when its internal system is. It is cumbersome to speak about the function
ˆ
h of Lemma 13 because it is not always well-defined. Also, Theorem 14 does not shed
light on how to systematically obtain surpriseless internal systems when nothing is known
about the environment or when do they even exist. We will now present a framework
which enables a more systematic characterization of when the internal system is both sur-
priseless and isomorphic to the external system, and how to obtain them, theoretically,
but not algorithmically.2 This approach will pave the path to algorithm design in future
work, see Section 6.
Definition 15 Given a DTS S = (S,A,τ). We say that an equivalence relation E on S
is sufficient, or S-sufficient, if for all s,s′ ∈ S and a ∈ A we have that (s,s′) ∈ E implies
(τ(s,a),τ(x′,a)) ∈ E.AnequivalencerelationE′ isarefinement ofanequivalencerelation
E, if E′ ⊆ E. We say that an equivalence relation E′ is a minimal sufficient refinement of
E, if E′ is sufficient, E′ ⊆ E and for all sufficient refinements E′′ ⊆ E we have E′′ ⊆ E′.
⊣
The following is Theorem 4.19 of [30].
Theorem 16 Suppose S = (S,A,τ) is a DTS. Let E be an equivalence relation on
S. Then the minimal S-sufficient refinement of E exists and is unique. We denote it
by MSR(E) = MSR (E). ⊔⊓
S
Definition 17 Let S = (S ,A,τ ) be a DTS for i ∈ {0,1}. A function h: S → S is
i i i 0 1
called a homomorphism from S to S , if for all (s,a) ∈ S ×A we have
0 1 0
τ (h(s),a) = h(τ (s,a)).
1 0
It is called an epimorphism, if it is onto, and isomorphism if it is a bijection. ⊣
Definition 18 Let E be an equivalence relation on S. If s ∈ S, denote by [s], or by [s] ,
E
the equivalence class of s which is the set {s′ ∈ S | (s,s′) ∈ E}. The set of all equivalence
classes is denoted by S/E. Suppose that h: S → Y is a function and E is an equivalence
relation on Y. Define
h−1(E) = {(s,s′) ∈ S ×S | (h(s),h(s′)) ∈ E}.
2 Our result shows how to find such systems by computing minimal sufficient refinements on infinite trees. As
such this is not computationally feasible, but gives a theoretical doorway towards designing algorithms.
An Internal Model Principle For Robots 9
It is easy to verify that h−1(E) is an equivalence relation on S. Denote by E the equiv-
h
alence relation h−1(id ). Equivalently, (s,s′) ∈ E ⇐⇒ h(s) = h(s′). A function
Y h
h: S → Y is E-invariant, if E is a refinement of E . We say that an equivalence re-
h
lation E is h-closed, if E is a refinement of E. ⊣
h
We leave the following two observations for the reader to prove:
Lemma 19 Let S = (S,A,τ) be DTS and h: S → Y. Then E is h-closed, and h is
h
E -invariant. ⊔⊓
h
Lemma 20 Suppose S = (S,A,τ) is a DTS, E is an S-sufficient equivalence relation,
and h: S → Y. If E is h-closed, then (h/E): S/E → Y defined by (h/E)([s]) = h(s) is
well-defined. ⊔⊓
Lemma 21 (Lemma 4.5 of [30]) Suppose S = (S,A,τ) is a DTS, and E is an S-sufficient
equivalence relation. Let S′ = S/E and define τ′: S′×A → S′ by τ′([s],a) = [τ(s,a)] (also
denoted τ′ = τ/E). Then τ′ is well-defined and S′ = (S′,A,τ′) is a DTS (also denoted by
S′ = S/E). ⊔⊓
Proposition 22 (Pullback) Suppose S = (S ,A,τ ) is a DTS for i ∈ {0,1}. Sup-
i i i
pose that E is an S -sufficient equivalence relation on S . Suppose that h: S → S is
1 1 1 0 1
an epimorphism. Then E = h−1(E ) is an S -sufficient equivalence relation on S and
0 1 0 0
∼
S /E = S /E .
0 0 1 1
Proof.Supposes,s′ ∈ S areE -equivalentandsupposea ∈ A.Thenwehaveh(s)E h(s′)
0 0 1
and so by the sufficiency of E we have τ (h(s),a)E τ (h(s′),a). On the other hand be-
1 1 1 1
cause h is a homomorphism, h(τ (s,a)) = τ (h(s),a) and h(τ (s′,a)) = τ (h(s′),a). Com-
0 1 0 1
bining these, we have that h(τ (s,a)) and h(τ(s′,a)) are E -equivalent. By the definition
0 1
of E , this means that τ (s,a) and τ (s′,a) are E -equivalent which proves that E is
0 0 0 0 0
S -sufficient.
0
∼
Now we prove that S /E = S /E . To simplify notation, for all i ∈ {0,1}, denote [s]
0 0 1 1 i
instead of [s] , and also S′ and τ′ instead of S /E and τ /E . Define h ˆ : S′ → S′ by
Ei i i i i i i 0 1
ˆ
h([s] ) = [h(s)] (2)
0 1
ˆ
By the definition of E it is easy to see that h is well-defined. Suppose that s ∈ S and
0 0
a ∈ A are arbitrary. Then
h ˆ (τ′([s] ,a)) ( = a) h ˆ ([τ (s,a)] ) ( = b) [h(τ (s,a))] ( = c) [τ (h(s),a)] ( = d) τ′([h(s)] ,a)
0 0 0 0 0 1 1 1 1 1
( = e) τ′(h ˆ ([s] ),a)
1 0
Here (a) and (d) follow from the definitions of τ′ and τ′, (b) and (e) from (2), and (c)
0 1
from that h is a homomorphism.
ˆ
This shows that h is a homomorphism from S /E to S /E . It remains to show that
0 0 1 1
ˆ
h is a bijection. Since h is onto (by the definition of an epimorphism), it is standard to
check that h ˆ is also onto. Suppose s,s′ ∈ S are not E -equivalent. Then by the definition
0 0
10 V. K. Weinstein et al.
of E we have that h(s) and h(s′) are not E -equivalent, so by (2) we have h ˆ ([s] ) ̸=
0 1 0
h ˆ ([s′] ) which proves that h ˆ is one-to-one. Thus we have proved that h ˆ is a bijective
0
homomorphism, i.e. an isomorphism, from S /E to S /E , so the proof is complete. ⊔⊓
0 0 1 1
Corollary 23 (Universality) Let F(A) be as in Definition 3. The free DTS of is uni-
versal in the sense that if S = (S,A,τ) is any strongly connected DTS, then there is an
equivalence relation E on A<N such that F(A)/E ∼ = S.
Proof. Fix s ∈ S. Define the map h: A<N → S by induction on the length of a¯ as follows.
0
Let h(∅) = s , and if h(a¯) is defined, let h(a¯⌢a) = τ(h(a¯),a). Let E = id . Then it
0 1 S
is clearly S-sufficient. Let E = h−1(E ). By strong connectedness, h is an epimorphism.
0 1
Now the result follows from Proposition 22. ⊔⊓
Corollary 23 means that finding a DTS is equivalent to finding an equivalence relation
on F(A). It follows that finding an appropriate internal system for a robot’s brain can
be reformulated as finding such an equivalence relation. The condition of sufficiency is
essentially the same as surprise minimization: by definition of sufficiency the equivalence
class of a state predicts the equivalence class of the following state. This condition can be
evaluated within the internal system, but we will show analogously to Theorem 14 that if
the robot can achieve sufficiency, then it achieves equivalence. In fact, minimal sufficiency
will imply isomorphism between the internal and external systems under some conditions.
Definition 24 Suppose H ⊆ S × S is any set. Then ⟨H⟩ is the equivalence relation
generated by H. It is defined to be the set of all pairs (s ,s ) ∈ S × S such that there
0 1
exists a finite sequence (x ,...,x ) of elements of S such that s = x , s = x , and for
0 k 0 0 1 k
all i ∈ {0,...,k −1} we have (x ,x ) ∈ H or (x ,x ) ∈ H. ⊣
i i+1 i+1 i
Lemma 25 Suppose S = (S ,A,τ ) is DTS for i ∈ {0,1}, E,E0,E1,...,Em−1 are equiv-
i i i
alence relations on S , E an equivalence relation on S , and h: S → S a function. Then
0 1 1 0 1
the following hold:
1. Each Ei is a refinement of ⟨E0 ∪···∪Em−1⟩,
2. If Ei are S -sufficient for all i < m, then so is ⟨E0 ∪···∪Em−1⟩,
0
3. If Ei is a refinement of E for all i < m, then ⟨E0 ∪···∪Em−1⟩ is also a refinement
of E.
4. If E is a refinement of Ei for all i < m, then E is a refinement of ⟨E0∪···∪Em−1⟩.
5. If Ei is h-closed for all i < m, then so is ⟨E0 ∪···∪Em−1⟩.
6. E is a refinement of h−1(E ).
h 1
7. If h is onto, and E is h-closed, then
h(E) = {(h(s),h(s′)) ∈ S ×S | (s,s′) ∈ E}
1 1
is an equivalence relation on S .
1
8. If h is onto, E is h-closed, and E is a refinement of h−1(E ), then h(E) is a refinement
1
of E .
1
9. If h is an epimorphism, E is S -sufficient and h-closed, then h(E) is S -sufficient.
0 1
10. If h is a homomorphism and E is S -sufficient, then h−1(E ) is S -sufficient,
1 1 1 0
An Internal Model Principle For Robots 11
11. If h is a homomorphism, then E is S -sufficient.
h 0
Proof. From definitions it follows that
Ei ⊆ E0 ∪···∪Em−1 ⊆ ⟨E0 ∪···∪Em−1⟩
and by definition of a refinement we have 1. 2 is Theorem 4.15 of [30]. Suppose (s ,s ) ∈
0 1
⟨E0∪···∪Em−1⟩andx ,...,x witnessthis.Thenbytheassumption,eachpair(x ,x )
0 k−1 i i+1
isinE.BytransitivityofE thenalso(s ,s ) ∈ E.Thisproves3.4followseasilyfrom1.By
0 1
definition of being h-closed, 5 follows directly from 4. Suppose (s,s′) ∈ E . Then h(s) =
h
h(s′), so by reflexivity of E , (h(s),h(s′)) ∈ E which proves 6. For 7, reflexivity follows
1 1
from the surjectivity of h. Symmetry follows from the symmetry of E. For transitivity,
assume (s ,s ) ∈ h(E) and (s ,s ) ∈ h(E). Let s′,s′,s′′,s′ be some elements of S such
1 2 2 3 1 2 2 3 0
that h(s′) = s , h(s′) = h(s′′) = s , and h(s′) = s′′, and so that (s′,s′) ∈ E and
1 1 2 2 2 3 3 1 2
(s′′,s′) ∈ E. But (s′,s′′) ∈ E which is a refinement of E, so also (s′,s′′) ∈ E. Now by
2 3 2 2 h 2 2
transitivity of E, we have (s′,s′) ∈ E and so (s ,s ) = (h(s′),h(s′)) ∈ h(E) and we are
1 3 1 3 1 3
done. By 7 h(E) is an equivalence relation and since E ⊆ h−1(E ) implies h(E) ⊆ E ,
1 1
this proves 8.
For 9, let (s ,s ) ∈ h(E) and a ∈ A. Let s′,s′ ∈ S witness that (s ,s ) ∈ h(E).
1 2 1 2 0 1 2
Then τ(s ,a) = τ(h(s′),a) and τ(s ,a) = τ(h(s′),a) and (s′,s′) ∈ E. By the assumed
1 1 2 2 1 2
sufficiency of E, and the properties of a homomorphism, we have τ(s ,a) = τ(h(s′),a) =
1 1
h(τ(s′,a)) = h(τ(s′,a)) = τ(h(s′),a) = τ(s ,a) which proves the required sufficiency
1 2 2 2
of h(E). The proof of 10 is analogous. 11 follows from 10 and the sufficiency of id . ⊔⊓
S1
The following theorem is the key to understanding why internal processing is “enough”.
It shows that the operator MSR commutes with any epimorphism. One can compute the
minimalsufficientrefinementofarelationandthentaketheinverseimageoftheresult.Or,
one can first take the inverse image, and then compute the minimal sufficient refinement
of that. According to Theorem 26 both give the same result. This is valuable for us,
because the inverse image of the relation (before applying MSR) corresponds to what the
robot can sense. Then, the robot can internally apply MSR, and can be sure to get the
same as if MSR was applied in the external system first. We explore the applications of
Theorem 26 to the robot system in Section 4.
Theorem 26 Let S = (S ,A,τ ) be DTS for i ∈ {0,1}, and suppose that h: S → S is
i i i 0 1
an epimorphism. Suppose E is an equivalence relation on S . Then
1 1
MSR (h−1(E )) = h−1(MSR (E )).
S0 1 S1 1
Proof. Denote E = h−1(MSR (E )). By Proposition 22 the relation E is sufficient.
0 S1 1 0
It is also a refinement of h−1(E ) because MSR (E ) ⊆ E implies h−1(MSR (E )) ⊆
1 S1 1 1 S1 1
h−1(E ). By Theorem 16 it is now enough to show that E is a minimal sufficient refine-
1 0
ment of h−1(E ). Suppose for a contradiction that there exists an S -sufficient refinement
1 0
E′ of h−1(E ) such that E′ ̸⊆ E . Let E′ = h(⟨E ∪ E′ ∪ E ⟩). By Lemma 25(2) and
0 1 0 0 1 0 0 h
25(11) the relation ⟨E ∪ E′ ∪ E ⟩ is S -sufficient, and by Lemma 25(1) and 25(4) it is
0 0 h 0
h-invariant. So by 25(9) E′ is S -sufficient. Since E′ ̸⊆ E , we have by 25(1) that
1 1 0 0
⟨E ∪E′ ∪E ⟩ ̸⊆ E
0 0 h 0
12 V. K. Weinstein et al.
and so
E′ = h(⟨E ∪E′ ∪E ⟩) ̸⊆ h(E ) = MSR (E ). (3)
1 0 0 h 0 S1 1
On the other hand E and E′ are refinements of h−1(E ), so by 25(3) and 25(6) ⟨E ∪
0 0 1 0
E′ ∪E ⟩ is a refinement of h−1(E ). By 25(8) we now have that E′ is a refinement of E .
0 h 1 1 1
Combining this with (3) above, and the sufficiency of E′, we have a contradiction with
1
the minimality of MSR (E ). ⊔⊓
S1 1
Lemma 27 If E is a sufficient equivalence relation on S = (S,A,τ), then for all a¯ ∈ A<N
and all s,s′ ∈ S we have
(s,s′) ∈ E =⇒ (s ∗a¯,s′ ∗a¯) ∈ E.
0
Proof. By induction on the length of a¯. ⊔⊓
4 Application to Internal and External Systems
Recall that we conventionally fix U and Y to be the sets of motor commands and sensor
data, respectively.
In this section we will work with the universal exploratory system (Definition 2) I =
F(U) in which the initial state is always the empty sequence. Given an external system
X = (X,U,f,h), and initial state x ∈ X, let f ˆ : U<N → X denote the function f ˆ (u¯) =
0 x0 x0
ˆ
x ∗u¯. Note that f is a homomorphism from F(U) to X.
0 x0
Theorem 28 Let X = (X,U,f,h) be a strongly connected external system with the initial
statex ∈ X.SupposethatE isanyequivalencerelationonX.LetE = MSR (f
ˆ−1(E)),
0 I F(U) x0
and E = MSR(E). Then
X
∼
I/E = X/E .
I X
ˆ
Proof. Since X is connected, f is an epimorphism. By Theorem 26 we have
x0
E = MSR (f
ˆ−1(E))
= f
ˆ−1(MSR
(E)) = f
ˆ−1(E
).
I) F(U) x0 x0 X x0 X
The result now follows from Proposition 22. ⊔⊓
It is useful to note that MSR(E ) can be a measure of symmetry of the environment
h
(X,U×Y,f,h). A bisimulation is trivial, if it is the identity relation. An autobisimulation
is a bisimulation of a DTS with itself, a relation on X ×X. The following is Theorem 4.4
of [30].
Theorem 29 An environment X = (X,U,f,h), has a non-trivial autobisimulation if and
only if MSR(E ) ̸= id . ⊔⊓
h X
The notion of autobisimulation is a weak version of automorphism. There is also a
one-sided version of this theorem for automorphisms:
Theorem 30 If there is a non-trivial automorphism of X, then MSR(E ) ̸= id .
h X
An Internal Model Principle For Robots 13
Proof. An automorphism is a special case of an autobisimulation, so the result follows
from Theorem 29. ⊔⊓
So we can call the environment chiral, if MSR(E ) = id . The equivalence relation
h X
f
ˆ−1(E
) on F(U) equates those paths which lead to identical sensor data. So we may
x0 h
call it the relation of sensory indistinguishability. The following corollary shows that if
the environment is chiral, then taking the quotient of the history information space by
the minimal sufficient refinement of the sensory indistinguishability yields a structure
isomorphic to the environment.
Corollary 31 Suppose X = (X,U,f,h) is a strongly connected environment, and x ∈ X.
0
Suppose that MSR (E ) = id . Then
X h X
F(U)/MSR(f
ˆ−1(E
))
∼
= X.
x0 h
∼
Proof. Since X/id = X, the result follows from Theorem 28. ⊔⊓
X
Taking the minimal sufficient refinement can be interpreted as minimizing surprise
while also minimizing computational resources. The minimization of surprise is due to
the definition of sufficiency which say that the current equivalence class predicts the next
one. Minimization of resources is captured by the minimality of the refinement, as it
yields the smallest possible quotient space. Isomorphism is a special case of bisimulation,
so applying Theorem 14 we get:
Corollary 32 Suppose X is as in Corollary 31, and I = F(U)/MSR(f
ˆ−1(E
)). Then
x0 h
I ⋆X is surpriseless. ⊔⊓
An interesting class of chiral environments consists of those environments in which E
h
has one equivalence class which is a singleton. Call such an equivalence relation pointed.
We will show that environments with pointed E are chiral under some very minimal
h
assumptionsonX.Anautomaton(S,A,τ)isminimally distinguishing,ifforalls ,s ,s ∈
0 1 2
S and all a ∈ A we have that if τ(s ,a) = τ(s ,a) = s , then one of the following holds:
1 2 0
s = s , s = s or s = s .
0 1 1 2 0 2
Theorem 33 Let S = (S,A,τ) be a strongly connected and minimally distinguishing. If
E is a pointed equivalence relation on X, then MSR(S) = id .
X
Proof. Let s ∈ S be an element such that {s } is an E-equivalence class. Let s ,s ∈ S.
0 0 1 2
We will show that s ̸= s implies (s ,s ) ∈/ MSR(E). Let a¯ be of minimal length such
1 2 1 2 i
thats ∗a¯ = s fori ∈ {1,2}.Theseexistbyconnectedness.Denotek = |a¯ |fori ∈ {1,2}.
i i 0 i i
Without loss of generality assume that k ⩽ k . There are two cases: k < k and k = k .
1 2 1 2 1 2
Case 1: k < k . By the minimality of |a¯ |, we have s ∗a¯ ̸= s , but s ∗a¯ = s . By
1 2 2 2 1 0 1 1 0
the choice of s we have now (s ∗a¯ ,s ∗a¯ ) ∈/ E, and so (s ∗a¯ ,s ∗a ) ∈/ MSR(E). By
0 2 1 1 1 2 1 1 1
Lemma 27 we now conclude that (s ,s ) ∈/ MSR(E).
1 2
Case 2: k = k . By induction on k = k = k . We will show again that
1 2 1 2
s ̸= s =⇒ s ∗a¯ ̸= s ∗a (4)
1 2 1 1 2 1
14 V. K. Weinstein et al.
which by the same argument as above implies that (s ,s ) ∈/ MSR(E). We will proceed
1 2
by induction on k.
If k = 0, then a¯ = a¯ are both empty, so s = s = s , and the premise s ̸= s is
1 2 1 2 0 1 2
false. Suppose we have proved (4) for all k ⩽ n and that k = n+1. Assume s ̸= s . Let
1 2
a¯ = a0···an. Let j be the smallest one such that
1 1 1
s ∗a0···aj = s ∗a0···aj. (5)
1 1 1 2 2 2
If such j does not exist, then we are done. Otherwise we will deduce a contradiction.
Define
s′ = s ∗a0···aj−1,
1 1 1 1
s′ = s ∗a0···aj−1,
2 2 1 1
s′ = s ∗a0···aj−1aj = s ∗a0···aj−1aj = τ(s′,aj) = τ(s′,aj).
0 1 1 1 1 2 1 1 1 1 1 2 1
Now by the assumption that X is minimally distinguishing we must now have one of the
following: (a) s′ = s′, (b) s′ = s′, or (c) s′ = s′. By the minimality of j we cannot have
0 1 1 2 0 2
(b). Let
a¯<j = a0···aj−1 and a¯>j = aj+1···an.
1 1 1 1 1 1
If (a) holds, then we have
s = s ∗(a¯<j⌢aj⌢a¯>j) = ((s ∗(a¯<j⌢aj))∗a¯>j
0 1 1 1 1 1 1
= s′ ∗a¯>j s ∗(a¯<j⌢aj) = s
0 1 1 1 1 0
= s′ ∗a¯>j by (a)
1 1
= (s ∗a¯<j)∗a¯>j = s ∗(a¯<j⌢a¯>j)
1 1 1 1 1 1
contradicting the minimality of |a¯ |. If (c) holds we get similarly a contradiction with the
1
minimality of |a¯ |:
2
s = s′ ∗a¯>j = s′ ∗a¯>j by (c)
0 0 1 2 1
= (s ∗a¯<j)∗a¯>j = s ∗(a¯<j⌢a¯>j).
2 1 1 2 1 1
This completes the proof of the theorem. ⊔⊓
We say that a sensor mapping h: X → Y is pointed, if E is a pointed equivalence
h
relation which is equivalent to saying that there is some y ∈ Y such that h−1(y) is a
singleton.
Corollary 34 Let (X,U,f,h) be a strongly connected minimally distinguishing external
system. Let x ∈ X, and suppose that h: X → Y is a pointed sensor mapping. Let
0
E = MSR (f
ˆ−1(E
)). Then
F(U) x0 h
∼
F(U)/E = X.
Proof. By Theorem 33, MSR (S) = id . Then apply Corollary 31. ⊔⊓
X X
An Internal Model Principle For Robots 15
Remark 2. Supposethestatespaceconsistsofpositionsoftherobot’sbodyintheenviron-
ment. Then it follows that the robot does not need any “external” sensors to successfully
mirror the environment internally. It is enough to have proprioceptive feedback to single
one specific position of its own body. Striving for sufficiency (surprise minimization) takes
care of the rest by Corollary 34. ⊣
Finally, we give an application for a mathematically ideal situation where the motor
commands generate a group acting on the external state space:
Corollary 35 Suppose that (G,+) is a group generated by U ⊆ G, and suppose that
τ: X × G → X is a transitive action of G on X. Consider the environment X =
(X,U,f,h) where f = τ↾(X ×U) and h is pointed. Let E = MSR (f ˆ−1(E )). Then
F(A) x0 h
∼
F(A)/E = X.
Proof. By Corollary 34 it is enough to show that X is connected and minimally distin-
guishing. Since the action is transitive, X is connected. Suppose x ∗u = x ∗u = x for
1 2 0
some x ,x ,x ∈ X and u ∈ U. Multiplying by the inverse of u on the right this implies
0 1 2
that x = x . ⊔⊓
1 2
5 Examples
Example 1. Simple examples of finite external systems which can be “learned” by taking
minimal sufficient refinements over the exploratory internal state space are depicted in
Figures 1 and 2. It is easy to see that in both cases the external state space is minimally
distinguishing and the sensor mapping is pointed, so we can apply Corollary 34. ⊣
Inthefinalexamplebelowweshowthatverylimitedproprioceptivefeedbackisenough
for the emergent of internal structure isomorphic to the environment.
Example 2. (Robot arm) In Figure 3 there is a robot arm with three joints. For the sake
of applicability of our theory, assume that the state space is discrete. Each joint can turn
by, say, ±1◦. If the arm is pushing against an obstacle while applying a rotation of any of
the joints, then the external state stays the same as before. The configuration space is a
subset of the (discrete) 3-torus. Assume that there is one (and possibly only one!) position
of the arm where it receives a proprioceptive feedback. It could be the original position of
the arm where it receives a “click” and no sensor feedback in any other position. Then this
sensor mapping is pointed. It is also not hard to see that the robot arm satisfies minimal
distinguishability: Suppose the arm is in some positions x or x and u ±1◦ rotation of
1 2
one of the joints. If this rotation does not result in hitting an obstacle for either x or x ,
1 2
then x ̸= x implies x ∗u ̸= x ∗u because the rotation u acts “homeomorphically” on
1 2 1 2
the torus. If, however, say, x faces an obstacle when rotating by u, then x = x ∗u, so
1 1 1
we are done. Applying Corollary 34 we see that if such robot arm succeeds in minimizing
the surprise of “when is the proprioceptive ‘click’ feedback received”, then it will end up
building an isomorphic copy of the external state space. ⊣
16 V. K. Weinstein et al.
(a) (b)
(c) (d)
Fig.1: The environment (a) has four states. Agent can move clockwise and counter-
clockwise, but in states 1 and 4 nothing happens, if it tries to go left or right respectively.
In(b)thebinarytreeofactionobservationsequencesisdepicted.Inmostnodesthesensor
data is “white” but when the agent is in the left-most state, the data is “green”. In (c) we
show the minimal sufficient refinement of (b), and in (d) we have taken the quotient of
(c) with respect to the refinement. It turns out to be isomorphic to (a), as predicted by
Corollary 34.
An Internal Model Principle For Robots 17
(a) (b)
(c) (d)
Fig.2:Sameidea,asinFigure1,butwithacircularenvironment.InthiscaseCorollary35
could be applied, with the group Z/4Z acting on X.
Fig.3: Robot arm with three joints and obstacles.
18 V. K. Weinstein et al.
6 Discussion
Our aim was to introduce a mathematical principle that the robot can internally evaluate,
and if satisfied, ensures that the internal system becomes isomorphic or bisimulation
equivalenttotheenvironment.Indoingso,weproposedaframeworkwithoutpre-assigned
meanings or assumed correlations between the internal and the external systems. Yet, a
correlation emerges between them due to structural coupling. The proposed mathematical
principle is sufficiency, which can be interpreted as the lack of surprise. In this way, our
framework can be seen as an extension of the FEP framework to the combinatorial realm
of finite or countable automata. We propose to explore the ideas of FEP further within
this framework. One future direction is to explore the notion of Markov blankets in the
context of discrete non-probabilistic systems. After all, the notion of independence is more
ubiquitous than that dictated by probability theory [3, Ch.2], [21,29].
One drawback of our results is that both our assumptions and conclusions are very
strong. The assumption of sufficiency is unrealistic, and the resulting similarity between
the internal and external models is too strong for most applications. Thus, another di-
rection for future research is to explore significantly weaker notions. For example, in-
stead of minimizing surprise globally, the agent may focus on minimizing surprise rel-
ative to its goals. We envision replacing minimal sufficient refinements by equivalences
that retain only task-relevant information. This approach will guide the theory towards
game-theoreticaspects,incorporatingvonNeumann’sconceptsofturn-takinganddiscrete
strategies, as well as Nash’s payoff functions and Aumann’s epistemology works.
We are also working on extending the framework to incorporate multiple sensing
modalities within one agent’s brain, as well as multiple agents, thereby uncovering new
pathways in communication theory.
Acknowledgments. Authors1,3,and5weresupportedbyaEuropeanResearchCouncilAdvancedGrant(ERC
AdG, ILLUSIVE: Foundations of Perception Engineering, 101020977).
References
1. Antsaklis, P.: Autonomy and metrics of autonomy. Annual Reviews in Control 49, 15–26 (2020)
2. Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A.: Deep reinforcement learning: A brief
survey. IEEE Signal Processing Magazine 34(6), 26–38 (2017)
3. Baldwin, J.T.: Fundamentals of Stability Theory. Cambridge University Press (Mar 2017)
4. Berger, A.A.: Semiotics and society. Society 51(1), 22–26 (Feb 2014 2014/02//)
5. Conant,R.C.,Ashby,W.R.:Everygoodregulatorofasystemmustbeamodelofthatsystem.International
Journal of Systems Science 1(2), 89–97 (1970)
6. Francis,B.,Wonham,W.:Theinternalmodelprincipleofcontroltheory.Automatica12(5),457–465(1976)
7. Friston, K.: A theory of cortical responses. Philosophical Transactions - Royal Society. Biological Sciences
360(1456), 815–836 (2005)
8. Friston, K.: The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)
9. Friston, K.: A free energy principle for biological systems. Entropy 14(11), 2100–2121 (2012)
10. Gallagher, S.: Enactivist Interventions: Rethinking the Mind. Oxford University Press (2017)
11. Heins,C.,Millidge,B.,Costa,L.D.,Mann,R.P.,Friston,K.J.,Couzin,I.D.:Collectivebehaviorfromsurprise
minimization. Proceedings of the National Academy of Sciences of the United States of America 121(17)
(2024)
12. Hutto, D.D., Myin, E.: Radicalizing enactivism: Basic minds without content. MIT Press (2012)
13. Hutto, D.D., Myin, E.: Evolving Enactivism. MIT Press (2017)
An Internal Model Principle For Robots 19
14. Kantaros,Y.,Zavlanos,M.M.:Sampling-basedoptimalcontrolsynthesisformultirobotsystemsunderglobal
temporal tasks. IEEE Transactions on Automatic Control 64(5), 1916–1931 (2019)
15. Lanillos, P., Meo, C., Pezzato, C., Meera, A.A., Baioumy, M., Ohata, W., Tschantz, A., Millidge, B., Wisse,
M., Buckley, C.L., et al.: Active inference in robotics and artificial agents: Survey and challenges. arXiv
preprint arXiv:2112.01871 (2021)
16. LaValle, S.M.: Planning Algorithms. Cambridge University Press, Cambridge, U.K. (2006), also available at
http://lavalle.pl/planning/
17. LaValle,S.M.,Center,E.G.,Ojala,T.,Pouke,M.,Prencipe,N.,Sakcak,B.,Suomalainen,M.,Timperi,K.G.,
Weinstein, V.: From virtual reality to the emerging discipline of perception engineering. Annual Review of
Control, Robotics, and Autonomous Systems 7(1) (Nov 2023)
18. LeCun, Y., Courant: A path towards autonomous machine intelligence version 0.9.2, 2022-06-27 (2022)
19. Littman, M., Sutton, R.S.: Predictive representations of state. Advances in neural information processing
systems 14 (2001)
20. Ma,B.,Tang,J.,Chen,B.,Pan,Y.,Zeng,Y.:Tensoroptimizationwithgrouplassoformulti-agentpredictive
state representation. Knowledge-based Systems 221, 106893 (2021)
21. Paolini, G.: Independence logic and abstract independence relations. Mathematical Logic Quarterly 61(3),
202–216 (May 2015)
22. Parr,T.,Pezzulo,G.,Friston,K.J.:Activeinference:TheFreeEnergyPrincipleinMind,BrainandBehaviour
(2022)
23. Ramstead,M.J.D.,Friston,K.J.,Hipólito,I.:Isthefree-energyprincipleaformaltheoryofsemantics?from
variational density dynamics to neural and phenotypic representations. Entropy 22(8), 889 (2020)
24. Rivest, R.L., Schapire, R.E.: Diversity-based inference of finite automata. Journal of the Association for
Computing Machinery 41(3), 555–589 (1994)
25. Sakcak,B.,Weinstein,V.,LaValle,S.M.:Thelimitsoflearningandplanning:Minimalsufficientinformation
transition systems. In: 2022 International Workshop on the Algorithmic Foundations of Robotics (WAFR)
(2022)
26. deSaussure,F.,Baskin,W.,Meisel,P.,Saussy,H.:CourseinGeneralLinguistics.ColumbiaUniversityPress
(2011)
27. Tumova,J.,Dimarogonas,D.V.:Multi-agentplanningunderlocalltlspecificationsandevent-basedsynchro-
nization. Automatica 70, 239–248 (2016)
28. Varela,F.,Rosch,E.,Thompson,E.:TheEmbodiedMind:CognitiveScienceandHumanExperience.Cam-
bridge, Massachusetts: MIT Press (1992)
29. Väänänen, J.: Dependence Logic: A New Approach to Independence Friendly Logic. London Mathematical
Society Student Texts, Cambridge University Press (2007)
30. Weinstein, V., Sakcak, B., LaValle, S.M.: An enactivist-inspired mathematical model of cognition. Frontiers
in Neurorobotics 16 (2022)

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "An Internal Model Principle For Robots"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
