=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Learning Generative Models for Active Inference using Tensor Networks
Citation Key: wauthier2022learning
Authors: Samuel T. Wauthier, Bram Vanhecke, Tim Verbelen

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: state, quantum, states, tensor, models, learning, generative, inference, networks, active

=== FULL PAPER TEXT ===

Learning Generative Models for Active Inference
using Tensor Networks
Samuel T. Wauthier1, Bram Vanhecke2, Tim Verbelen1, and Bart Dhoedt1
1 IDLab, Department of Information Technology at Ghent University – imec,
Technologiepark-Zwijnaarde 126, B-9052 Ghent, Belgium
{firstname.lastname}@ugent.be
2 University of Vienna, Faculty of Physics and Faculty of Mathematics,
Quantum Optics, Quantum Nanophysics and Quantum Information,
Boltzmanngasse 5, 1090 Vienna, Austria
bram.andre.roland.vanhecke@univie.ac.at
Abstract. Active inference provides a general framework for behavior
and learning in autonomous agents. It states that an agent will attempt
to minimize its variational free energy, defined in terms of beliefs over
observations, internal states and policies. Traditionally, every aspect of a
discreteactiveinferencemodelmustbespecifiedbyhand,i.e.bymanually
defining the hidden state space structure, as well as the required distri-
butions such as likelihood and transition probabilities. Recently, efforts
have been made to learn state space representations automatically from
observationsusingdeepneuralnetworks.Inthispaper,wepresentanovel
approach of learning state spaces using quantum physics-inspired tensor
networks. The ability of tensor networks to represent the probabilistic
nature of quantum states as well as to reduce large state spaces makes
tensor networks a natural candidate for active inference. We show how
tensor networks can be used as a generative model for sequential data.
Furthermore, we show how one can obtain beliefs from such a generative
model and how an active inference agent can use these to compute the
expected free energy. Finally, we demonstrate our method on the classic
T-maze environment.
Keywords: Active inference · Tensor networks · Generative modeling.
1 Introduction
Activeinferenceisatheoryofbehaviorandlearninginautonomousagents[5].An
active inference agent selects actions based on beliefs about the environment in
an attempt to minimize its variational free energy. As a result, the agent will try
to reach its preferred state and minimize its uncertainty about the environment
at the same time.
This scheme assumes that the agent possesses an internal model of the world
andthatitupdatesthismodelwhennewinformation,intheformofobservations,
becomes available. In current implementations, certain components of the model
must be specified by hand. For example, the hidden space structure, as well
2202
peS
8
]GL.sc[
2v31780.8022:viXra
2 S. T. Wauthier et al.
as transition dynamics and likelihood, must be manually defined. Deep active
inference models deal with this problem by learning these parts of the model
through neural networks [13,2].
Tensornetworks,asopposedtoneuralnetworks,arenetworksconstructedout
ofcontractionsbetweentensors.Inrecentyears,tensornetworkshavefoundtheir
place within the field of artificial intelligence. More specifically, Stoudenmire and
Schwab [12] showed that it is possible to train these networks in an unsupervised
manner toclassify imagesfrom theMNIST handwritten digitsdataset [8]. Impor-
tantly, tensor networks have shown to be valuable tools for generative modeling.
Han et al. [6] and Cheng et al. [3] used tensor networks for generative modeling
of the MNIST dataset, while Mencia Uranga and Lamacraft [9] used a tensor
network to model raw audio.
Tensornetworks,whichwereoriginallydevelopedtorepresentquantumstates
in many-body quantum physics, are a natural candidate for generative models
for two reasons. Firstly, they were developed in order to deal with the curse
of dimensionality in quantum systems, where the dimensionality of the Hilbert
space grows exponentially with the number of particles. Secondly, they are used
torepresentquantumstatesandare,therefore,inherentlycapableofrepresenting
uncertainty, or, in the case of active inference, beliefs. For example, contrary
to neural networks, tensor networks do not require specifying a probability
distribution for the hidden state variables or output variables. Furthermore,
tensor networks can be exactly mapped to quantum circuits, which is important
for the future of quantum machine learning [1].
In this paper, we present a novel approach to learning state spaces, likelihood
and transition dynamics using the aforementioned tensor networks. We show
that tensor networks are able to represent generative models of sequential data
and how beliefs (i.e. probabilities) about observations naturally roll out of the
model. Furthermore, we show how to compute the expected free energy for such
a model using the sophisticated active inference scheme. We demonstrate this
using the classic T-maze environment.
Section 2 elaborates on the inner workings of a tensor network and explains
how to train such a model. Section 3 explains the environment and how we
applied a tensor network for planning with active inference. In section 4, we
present and discuss the results of our experiments on the T-maze environment.
Finally, in section 5, we summarize our findings and examine future prospects.
2 Generative modeling with tensor networks
A generative model is a statistical model of the joint probability P(X) of a set
of variables X = (X ,X ,...,X ). As previously mentioned, quantum states
1 2 n
inherently contain uncertainty, i.e. they embody the probability distribution of a
measurement of a system. It is natural, then, to represent a generative model
as a quantum state. Quantum states can be mathematically described through
a wave function Ψ(x) with x=(x ,x ,...,x ) a set of real variables, such that
1 2 n
Learning Generative Models for Active Inference using Tensor Networks 3
the probability of x is given by the Born rule:
|Ψ(x)|2
P(X =x)= , (1)
Z
with Z = (cid:80) |Ψ(x)|2, where the summation runs over all possible realizations
{x}
of the values of x.
Recent work [10] has pointed out that quantum states can be efficiently
parameterized using tensor networks. The simplest form of tensor network is the
matrixproductstate(MPS),alsoknownasatensortrain[11].Whenrepresenting
a quantum state as an MPS, the wave function can be parameterized as follows:
(cid:88)(cid:88)(cid:88) (cid:88)
Ψ(x)= ... A(1)(x )A(2) (x )A(3) (x )···A(n) (x ). (2)
α1 1 α1α2 2 α2α3 3 αn−1 n
α1 α2 α3 αn−1
Here, each A(i) (x ) denotes a tensor of rank 2 (aside from the tensors on the
αi−1αi i
extremities which are rank 1) which depends on the input variable x . This way,
i
the wave function Ψ(x) is decomposed into a series of tensors A(i).
Importantly, each possible value of an input variable x must be associated
i
with a vector of unit norm [12]. That is, each value that x can assume must be
i
represented by a vector in a higher-dimensional feature space. Furthermore, to
allow for a generative interpretation of the model, the feature vectors should be
orthogonal[12].Thismeansthatthevectorsassociatedtothepossiblevaluesofx
i
willformanorthonormalbasisoftheaforementionedfeaturespace.Foravariable
which can assume n discrete values, this feature space will be n-dimensional. The
dimensionality of the space is referred to as the local dimension.
The unit norm and orthogonality conditions can be satisfied by defining
a feature map φ(i)(x ), which maps each value onto a vector. For example, if
i
x ∈{0,1,2},apossiblefeaturemapistheone-hotencodingofeachvalue:(1,0,0)
i
for 0, (0,1,0) for 1, and (0,0,1) for 2. The feature map φ(i)(x ) allows us to
i
rewrite the A(i)(x ) in Eq. 2 as
i
A(i) (x )= (cid:88) T(i) φ(i)(x ), (3)
αi−1αi i αi−1βiαi βi i
βi
where T(i) is a tensor of rank 3. Here, we have further decomposed A(i) into
αi−1βiαi
a contraction of tensor T(i) and the feature vector φ(i)(x ). In graphical notation,
i
the MPS (cf. Eq. 2) becomes:
T(1) T(2) T(3) ··· T(n)
Ψ(x)=
φ(1) φ(2) φ(3) φ(n)
Givenadataset,anMPScanbetrainedusingamethodbasedonthedensity
matrix renormalization group (DMRG) algorithm [12]. This algorithm updates
model parameters with respect to a given loss function by “sweeping” back and
4 S. T. Wauthier et al.
forth across the MPS. In our case, we maximize the negative log-likelihood
(NLL),i.e.wemaximizethemodelevidencedirectly[6].Aftertraining,thetensor
network can be used to infer probability densities over unobserved variables by
contracting the MPS with observed values. For a more in-depth discussion on
tensor networks, we refer to the Appendix.
3 Environment
The environment used to test the generative model is the classic T-maze as
presented by Friston et al. [5]. As the name suggests, the environment consists of
a T-shaped maze and contains an artificial agent, e.g. a rat, and a reward, e.g.
somecheese.Themazeisdividedintofourlocations:thecenter,therightbranch,
the left branch, and the cue location. The agent starts off in the center, while
the reward is placed in either the left branch or the right branch, as depicted in
Figure 1. Crucially, the agent does not know the location of the reward initially.
Furthermore, once the agent chooses to go to either the left or the right branch,
it is trapped and cannot leave. For the agent, the initial state of the world is
uncertain. It does not know which type of world it is in: a world with the reward
on the left, or a world with the reward on the right. In other words, it does not
know its context. However, going to the cue reveals the location of the reward
and enables the agent to observe the context. This resolves all ambiguity and
allows the agent to make the correct decision.
The implementation of the environment was provided by pymdp [7]. In this
package, the agent receives an observation with three modalities at every step:
the location, the reward, and the context. The location can take on four possible
values: center, right, left, and cue, and indicates which location the agent is
currently in. The reward can take on three possible values: no reward, win, and
loss.The“noreward” observationindicatesthattheagentreceivednoinformation
about the reward, while the “win” and “loss” observations indicate that the agent
either received the reward or failed to obtain the reward, respectively. Logically,
“noreward” canonlybeobservedinthecenterandcuelocations,while“win” and
“loss” can only be observed in the left and right locations. Finally, the context
Fig.1. Possible contexts of the T-maze environment as presented by Friston et al. [5].
The agent starts off in the center. The reward (yellow) is located in either the left
branch (left image) or right branch (right image). The cue reveals where the reward is:
red indicates the reward is on the left, green indicates the reward is on the right.
Learning Generative Models for Active Inference using Tensor Networks 5
cantakeontwopossiblevalues:leftandright.Whenevertheagentisinlocations
“center”, “left” or “right”, the context observation will be randomly selected from
“left” or “right” uniformly. Only when the agent is in the cue location, will the
context observation yield the correct context. Further, the possible actions that
the agent can take include: center, right, left, and cue, corresponding to which
location the agent wants to go to.
We modified the above implementation slightly to better reflect the environ-
ment brought forth by Friston et al. [5]. In the original description, the agent is
only allowed to perform two subsequent actions. Therefore, the number of time
steps was limited to two. Furthermore, in the above implementation, the agent is
able to leave the left and right branches of the T-maze. Thus, we prevented the
agent from leaving whenever it chose to go to the left or right branches.
3.1 Modeling with tensor networks
The tensor network was adapted in order to accommodate the environment
and be able to receive sequences of actions and observations as input. Firstly,
the number of tensors was limited to the number of time steps. Secondly, each
tensor received an extra index so that the network may receive both actions and
observations. This led to the following network structure:
a a
1 2
Ψ(x)= T(1) T(2) T(3)
o o o
1 2 3
where we used a and o to denote the feature vectors corresponding to action
i i
a and observation o . Note that the first tensor did not receive an action input,
i i
since we defined that action a leads to observation o .
i i+1
As mentioned in section 2, the feature maps φ(i) can generally be freely
chosen as long as the resulting vectors are correctly normalized. However, it is
useful to select feature maps which can easily be inverted, such that feature
space vectors can readily be interpreted. In this case, since both observations
and actions were discrete, one-hot encodings form a good option. The feature
vectors for actions were one-hot encodings of the possible actions. The feature
vectors for observations were one-hot encodings of the different combinations
of observation modalities, i.e. 4×3×2=24 one-hot vectors corresponding to
different combinations of the three different modalities.
In principle, there is nothing stopping us from learning feature maps, as
long as the maps are correctly normalized. For practical purposes, the learning
algorithm should make sure the feature maps are invertible. Whether feature
maps should be learned before training the model or can be learned on-the-fly is
an open question.
6 S. T. Wauthier et al.
At this point, it is important to mention that the feature map is not chosen
withtheintentofimposingstructureonthefeaturespacebasedonpriorknowledge
of the observations (or actions). On the contrary, any feature map should assign
a separate feature dimension to each possible observation, keeping the distance
between that observation and all other observations within the feature space
equal and maximal. To this end, the feature map can be thought of as being a
part of the likelihood.
3.2 Planning with active inference
Once trained, the tensor network constructed in section 3.1 provides a generative
model of the world. In theory, this model can be used for planning with active
inference. At this point, it is important to remark that the network does not
provide an accessible hidden state. While the bonds between tensors can be
regarded as internal states, they are not normalized and, therefore, not usable.
This poses a problem in the computation of the expected free energy, given by [5]
(cid:88)
G(π)= G(π,τ) (4)
τ
G(π,τ)=E [logQ(s |π)−logP(s ,o |o˜,π)] (5)
Q(oτ|sτ,π) τ τ τ
≈D (Q(o |π)||P(o ))+E [H(Q(o |s ,π))], (6)
KL τ τ Q(sτ|π) τ τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
expectedcost expectedambiguity
withhiddenstatess ,observationso andpolicyπ(τ)=a ,wherethe∼-notation
τ τ τ
denotes a sequence of variables x˜=(x ,x ,...,x ) over time and P(o ) is the
1 2 τ−1 τ
expected observation. This computation requires access to the hidden state s
τ
explicitly.
To remedy this, we suppose that the state s contains all the information
τ
gathered across actions and observations that occur at times t < τ. Mathe-
matically, we assume Q(s |π)≈Q(o |π) and Q(o |s ,π)≈Q(o |o ,π) with
τ <τ τ τ τ <τ
o =(o ,...,o ). This way, we are able to approximate the expected am-
<τ 1 (τ−1)
biguity in Eq. 6. While these assumptions may give the impression that the
calculation is computationally expensive, if the contraction with previous ac-
tions and observations has been performed once, it never has to be computed
again, since the resulting tensor can be reused in subsequent calculations. At this
point, the resulting tensor contains all the information from previous actions and
observations.
When planning, we must re-evaluate the likelihood (and thus the expected
free energy) for every time step, while imposing that the previous time steps are
fixed. Indeed, we will perform sophisticated inference [4]. Under this scheme, the
Learning Generative Models for Active Inference using Tensor Networks 7
expected free energy is given by
G(o ,a )= D (Q(o |a )||P(o ))+E [H(P(o |s ))]
τ τ KL τ+1 <τ+1 τ+1 Q(sτ+1|a<τ+1) τ+1 τ+1
(cid:124) (cid:123)(cid:122) (cid:125)
expectedfreeenergyofnextaction
+E [G(o ,a )], (7)
Q(aτ+1|oτ+1)Q(oτ+1|a<τ+1) τ+1 τ+1
(cid:124) (cid:123)(cid:122) (cid:125)
expectedfreeenergyofsubsequentactions
Q(a |o )=σ[−G(o ,a )] (8)
τ τ τ τ
This defines a tree search over actions and observations in the future.
4 Results and discussion
In this section, we demonstrate how the model’s beliefs shift over time. Later, we
show how a tensor network agent behaves when planning under sophisticated
inference.
The data set was constructed by including one of every possible path through
the maze, i.e. 202 sequences of actions and observations. The model was trained
over500epochswithabatchsizeof10,whereoneepochconsistedofoneright-to-
left-to-right sweep per batch. The learning rate was set to 10−4 and was further
reduced by 10 % whenever the loss increased too much (i.e. by more than 0.5).
Additionally, bonds started with 8 dimensions. The singular value cutoff point
was set to 10 % of the largest singular value.
4.1 Belief shift
Since the initial observation o is always center position, no reward and context
1
right or left with 50 % chance, we used the observation “center, no reward and
context right” to obtain the beliefs in each case. The results are analogous in the
case of “center, no reward and context left”.
Figure 2 (top) displays beliefs over o given a . From the results, it is clear
2 1
that the agent does not know which reward it will receive, if it were to go to the
left or right branch immediately. In addition, it does not know which context
it will observe, even if the agent were to go towards the cue. Once the agent
observes o , the beliefs shift. Figure 2 (bottom) shows beliefs over o given a
2 3 2
when the agent has seen the cue with context “right“. Since the agent has seen
the cue, it is very certain about the reward it will receive if it goes to the left or
the right branch. If it stays in the cue location, it is also very certain that it will
observe the same context again.
4.2 Action selection
With the outcome in section 4.1, we were able to perform action selection based
on the sophisticated inference scheme described in section 3.2. For this, we used
8 S. T. Wauthier et al.
Fig.2. (top) Model beliefs over observation o given action a per modality. (bottom)
2 1
Model beliefs for observation o given action a per modality, when the agent has
3 2
observed the cue with context “right”. Actions 0, 1, 2 and 3 correspond to center, right,
left and cue, respectively. Positions 0, 1, 2 and 3 correspond to center, right, left and
cue location, respectively. Rewards 0, 1 and 2 correspond to no reward, win and loss,
respectively. Context 0 and 1 correspond to right and left, respectively.
the following preferred observation per modality:
(cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
P(position)=σ( 0000 ), P(reward)=σ( 03−3 ), P(context)=σ( 00 ).
(9)
Figure 3 (top) shows the expected free energy for action a . Given that the
1
expected free energy is lowest for the action that brings the agent to the cue,
the agent will choose to go to the cue in the first action. This is because, after
observing the cue, the cue location provides a lower entropy on the context
modality, as well as virtually 100% certainty on where the reward is located.
Figure 3 (bottom) shows the expected free energy for action a when the
2
agent has chosen to go to the cue location and has observed either “cue, no
reward, context right” or “cue, no reward, context left”. In this case, the agent
will choose to go to either the left or the right branch, depending on the context
it observed, i.e. context right will lead to action right and vice-versa.
The net result is that the agent will first go to the cue in order to resolve
ambiguity and, subsequently, go to the branch with the reward.
Learning Generative Models for Active Inference using Tensor Networks 9
Fig.3. (top) Expected free energy for action a . (bottom) Expected free energy for
1
action a when observation o was “cue, no reward, context right” and “cue, no reward,
2 2
context left”, respectively.
5 Conclusion
We introduced a generative model based on tensor networks that is able to learn
from sequential data. In addition, we showed how one can obtain beliefs from
such a generative model and how a (sophisticated) active inference agent can
use these to compute the expected free energy. Demonstration on the T-maze
environment pointed out that such an agent is able to correctly select actions.
In the future, we plan to apply tensor networks to other environments, as
well as make an in-depth comparison with neural networks, in order to better
establish the benefits and drawbacks of the method. Moreover, we will adapt
the network to allow sequences of random lengths and look into incorporating
observations with continuous variables, which may also allow us to undo the
assumptions made in section 3.2. Both these changes will broaden the range of
applicability of generative models based on tensor networks.
Acknowledgments
This research received funding from the Flemish Government under the “Onder-
zoeksprogrammaArtificiëleIntelligentie(AI)Vlaanderen” programme.Thiswork
has received support from the European Union’s Horizon 2020 program through
Grant No. 863476 (ERC-CoG SEQUAM).
10 S. T. Wauthier et al.
References
1. The tensor network, https://tensornetwork.org/, last accessed 21 June 2022
2. Çatal,O.,Wauthier,S.,DeBoom,C.,Verbelen,T.,Dhoedt,B.:Learninggenerative
state space models for active inference. Frontiers in Computational Neuroscience
14, 103 (2020). https://doi.org/10.3389/fncom.2020.574372
3. Cheng,S.,Wang,L.,Xiang,T.,Zhang,P.:Treetensornetworksforgenerativemod-
eling. Phys. Rev. B 99, 155131 (Apr 2019). https://doi.org/10.1103/PhysRevB.
99.155131
4. Friston, K., Da Costa, L., Hafner, D., Hesp, C., Parr, T.: Sophisticated Inference.
NeuralComputation33(3),713–763(Mar2021).https://doi.org/10.1162/neco_
a_01351
5. Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,O’Doherty,J.,Pezzulo,G.:
Active inference and learning. Neuroscience & Biobehavioral Reviews 68, 862–879
(2016). https://doi.org/10.1016/j.neubiorev.2016.06.022
6. Han, Z.Y., Wang, J., Fan, H., Wang, L., Zhang, P.: Unsupervised generative
modeling using matrix product states. Phys. Rev. X 8, 031012 (Jul 2018). https:
//doi.org/10.1103/PhysRevX.8.031012
7. Heins, C., Millidge, B., Demekas, D., Klein, B., Friston, K., Couzin, I.D., Tschantz,
A.: pymdp: A python library for active inference in discrete state spaces. Journal
of Open Source Software 7(73), 4098 (2022). https://doi.org/10.21105/joss.
04098
8. LeCun, Y., Cortes, C., Burges, C.J.C.: The mnist database of handwritten digits
(1998), http://yann.lecun.com/exdb/mnist/, last accessed 10 June 2022
9. Mencia Uranga, B.n., Lamacraft, A.: Schrödingerrnn: Generative modeling of
raw audio as a continuously observed quantum state. In: Lu, J., Ward, R. (eds.)
ProceedingsoftheFirstMathematicalandScientificMachineLearningConference.
ProceedingsofMachineLearningResearch,vol.107,pp.74–106.PMLR(20–24Jul
2020), https://proceedings.mlr.press/v107/mencia-uranga20a.html
10. Orús, R.: Tensor networks for complex quantum systems. Nature Reviews Physics
1(9), 538–550 (Sep 2019). https://doi.org/10.1038/s42254-019-0086-7
11. Perez-Garcia, D., Verstraete, F., Wolf, M.M., Cirac, J.I.: Matrix product state
representations. Quantum Info. Comput. 7(5), 401–430 (Jul 2007)
12. Stoudenmire, E., Schwab, D.J.: Supervised learning with tensor networks.
In: Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R. (eds.)
Advances in Neural Information Processing Systems. vol. 29. Curran
Associates, Inc. (2016), https://proceedings.neurips.cc/paper/2016/file/
5314b9674c86e3f9d1ba25ef9bb32895-Paper.pdf
13. Ueltzhöffer,K.:Deepactiveinference.BiologicalCybernetics112(6),547–573(Dec
2018). https://doi.org/10.1007/s00422-018-0785-7
Learning Generative Models for Active Inference using Tensor Networks 11
Appendix 1 Notes on tensor networks
The summation over a common index as in Eq. 2 is also called a contraction.
Performing the contraction between two tensors yields a new tensor with a rank
equal to the sum of the ranks of the two contracted tensors minus two times the
numberofindicescontractedover.Thatis,contractingtwotensorsofrank2over
a single index gives a new tensor of rank 2, which is simply matrix multiplication:
(cid:80)
A B . Similarly, contracting over the indices of a single tensor of rank
j ij jk
(cid:80)
2 is simply the trace: A . In this sense, contraction is a generalization of
i ii
these operations. Contracting indices in different ways gives rise to different
structures. Examples of other possible networks are: tree tensor networks (TTN)
and projected entangled pair states (PEPS).
Tensornetworkscanmoreeasilybeunderstoodusingtheirgraphicalnotation.
Eachtensorisrepresentedbyanode,whilecontractionsarerepresentedbyedges.
Freeedges,i.e.edgeswhichdonotconnecttwotensors,correspondtofreeindices
whichhavenotbeensummed.Thesecanbeusedtorepresentsitesinthenetwork
which are able to receive input or which can be used as input.
Some examples of tensors in graphical notation are:
– vector ,
– matrix ,
– rank-3 tensor .
Some examples of operations that can be represented by contractions are:
– dot product ,
– matrix multiplication ,
– trace .
Foradetailedaccountontensornetworksandtheirgraphicalnotation,please
refer to [1].
Appendix 2 Training
The loss function must be chosen in such a way that the model captures the
probability distribution of the data [6]. A straightforward method for estimating
the parameters of a probability distribution is maximum likelihood estimation.
In machine learning terms, this means we will optimize the parameters of the
model with respect to the negative log-likelihood (NLL):
1 (cid:88)
L= logP(x), (10)
|D|
x∈D
where D denotes the data set. Through NLL minimization, the generative model
becomes more similar to the probability distribution of the data.
12 S. T. Wauthier et al.
a) ··· ··· = ··· ···
b) ∆B = Z(cid:48) − 2 (cid:80) Ψ(cid:48)(x)
Z |D| x∈D Ψ(x)
c) B(cid:48) = B +α ∆B
SVD
d) B∗ U S V =
Fig.4. Training scheme for an MPS. a) Contraction of two adjacent tensors. b) Com-
puting the update to the contracted tensor. c) Updating the contracted tensor. d)
Decomposition of the contracted tensor using SVD.
Training proceeds as depicted in Figure 4. Firstly, tensor T(i) and T(i+1) are
contracted to form the tensor B(i,i+1). The update to B(i,i+1) is then computed
using the loss function:
∂L Z(cid:48) 2 (cid:88) Ψ(cid:48)(x)
∆B(i,i+1) = = − , (11)
∂B(i,i+1) Z |D| Ψ(x)
αi−1βiβi+1αi+1 x∈D
where Z(cid:48) = 2 (cid:80) Ψ(cid:48)(x)Ψ(x) and Ψ(cid:48) is the derivative of Ψ with respect to
x∈D
B(i,i+1). Subsequently, the elements of B(i,i+1) are adjusted by adding ∆B(i,i+1)
multiplied by the learning rate. Finally, the newly computed B(cid:48)(i,i+1) is de-
composed into two tensors again. This decomposition is typically done through
singular value decomposition (SVD), where the singular value matrix is then
contracted with either the left or the right tensor, such that we are left with two
tensors.
By starting this scheme at the leftmost tensor and iteratively moving one
tensortotheright,thealgorithmcanupdatetheentireMPS.Indeed,itispossible
to update one tensor T(i) at a time, however, the current method allows
αi−1βiαi
the dimensions of the indices α (graphically, the edges connecting T(i) nodes),
i
the so-called bond dimensions, to vary during training. This is made possible
by truncated SVD, which truncates dimensions with singular values that fall
beneath some manually specified threshold. Truncating dimensions with small
singular values can be interpreted as truncating less informative dimensions. As
a result, truncated SVD ensures that the model remains as small as possible,
while containing the most information. Moreover, the size of the model will vary
depending on how much information it must learn.
Appendix 3 Computing probabilities with tensor
networks
One of the benefits of tensor networks is that we can easily obtain exact joint
(and conditional) probability distributions without requiring parameterization.
Learning Generative Models for Active Inference using Tensor Networks 13
After training the model, given that the network is correctly normalized (Z =1),
the joint probability distribution is given by
···
P(x ,x ,x ,...,x )=
1 2 3 n
···
where we have omitted tensor labels for simplicity. Marginal distributions can be
found by discarding the offending variables:
···
(cid:80)
P(x )= P(x ,x ,x ,...,x )=
1 {x}\{x1} 1 2 3 n
···
where the sum over the variables x ,x ,...,x is equivalent to contracting the
2 3 n
matching tensors. Finally, conditional probability distributions can be found by
combining the previous results:
P(x ,x ,x ,...,x )
P(x ,x ,...,x |x )= 1 2 3 n . (12)
2 3 n 1 P(x )
1
Appendix 4 Physical intuition
In order to garner a feeling for the physics and mathematics used throughout
this work, this section describes in (mostly) words what the physical meaning of
the constituents of the tensor network is.
Let x be an observable, i.e. a quantity that can be measured (or observed).
Examples of such physical quantities are position and momentum. Furthermore,
let {0,1,2} be the set of values that x can assume.
In quantum mechanics, the set of values that an observable can assume are
eigenvalues. This entails that there is a set of eigenvectors corresponding to these
eigenvalues. In turn, the eigenvectors form an eigenbasis of the state space. In
other words, every value that x can assume has a corresponding vector and each
of these vectors is a basis vector of the state space, meaning that each different
value is represented in the state space by a different dimension. For example, we
may have the vectors (1,0,0) for 0, (0,1,0) for 1, and (0,0,1) for 2.
Further, an MPS is typically used to represent a state within the state space.
ThismeansthattheMPSrepresentsa(multi)vector.Thisvectorisnotnecessarily
oneoftheeigenvectorsmentionedearlier,butitcanbevirtuallyanyvectorwithin
the state space. When learning the parameters of an MPS, it is rotated and
stretched in such a way that it represents the data.
14 S. T. Wauthier et al.
If an MPS does not necessarily coincide with any of the eigenvectors, what
valuewillitproducewhenameasurementisperformed?Itwillproduceanyofthe
eigenvalueswithacertainprobability.Theseprobabilitiesaregivenbythesquare
of the inner product between the MPS and each eigenvector. In our example, say
the MPS was represented by the vector (0.55,0.55,0.63), we would measure 0
with 30% probability, 1 with 30% probability and 2 with 40% probability.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Learning Generative Models for Active Inference using Tensor Networks"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
