=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Compositional Active Inference II: Polynomial Dynamics. Approximate Inference Doctrines
Citation Key: smithe2022compositional
Authors: Toby St. Clere Smithe

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2022

Key Terms: systems, statistical, compositional, doctrines, dynamics, polynomial, inference, approximate, active, games

=== FULL PAPER TEXT ===

Compositional Active Inference II:
Polynomial Dynamics. Approximate Inference Doctrines.
Toby St. Clere Smithe
University of Oxford
&
Topos Institute
toby@topos.institute
August 26, 2022
We develop the compositional theory of active inference by introducing activity, functorially
relating statistical games to the dynamical systems which play them, using the new notion of
approximateinferencedoctrine. Inordertoexhibitsuchfunctors,wefirstdevelopthenecessary
theory of dynamical systems, using a generalization of the language of polynomial functors to
supply compositional interfaces of the required types: with the resulting polynomially indexed
categoriesofcoalgebras,weconstructmonoidalbicategoriesofdifferentialanddynamical“hier-
archicalinferencesystems”,inwhichapproximateinferencedoctrineshavesemantics. Wethen
describe “externally parameterized” statistical games, and use them to construct two approxi-
mate inference doctrines found in the computational neuroscience literature, which we call the
‘Laplace’ and the ‘Hebb-Laplace’ doctrines: the former produces dynamical systems which op-
timize the posteriors of Gaussian models; and the latter produces systems which additionally
optimizetheparameters(or‘weights’)whichdeterminetheirpredictions.
1 Introduction
Inthefirstpaperinthisseries[1],weintroducedacompositionalframeworkinwhichtomakesenseofthe
‘statisticalgames’playedbyadaptiveandcyberneticsystems,withaviewtogeneralizingandcontextualizing
thefreeenergyprinciplethatliesattheheartoftheoriesofactiveinference[2].Yet,thesestatisticalgamesare
butoneaspectofanactiveadaptivesystem,andifatheoryofactiveinferenceistobeatheoryofanything,
thenitmustalsoacknowledgeactivity! Asastartingpoint, theframeworkofstatisticalgamesaccountsfor
systems that are open to their environment, and whose predictive performance is accordingly contextual,
but the next step — and the step taken in this paper — is to animate these statistical games, constructing
dynamical systems that play these games, and that can be correspondingly embodied in a changing world.
Thebehavioursofthesemodelsystemscanthenbecomparedwithobservationsofnaturaladaptivesystems,
andthemodelscanthenberefinedaccordingly.
It is a remarkable fact that our most infamous natural adaptive system, the mammalian brain, seems in
part to exemplify the hierarchical bidirectional structure of statistical games: certain neural circuits in sen-
sorycortexexhibitforward-lookingpredictionsalongsidebackward-lookingcorrectionsthattogethercanbe
1
2202
guA
52
]OA.niln[
1v37121.8022:viXra
modelledasakindofdynamicalBayesianinferenceprocess,andwhichappeartocoupletogethertoapproxi-
matehierarchicallystructuredBayesiannetworks[3]. Understandingthisresemblanceisoneoftheprincipal
motivationsforthiswork.
Since the brain is best understood as an ‘open’ (i.e., embodied and interacting) dynamical system, this
resemblance seems to imply a functorial relationship between a category of statistical models on the one
handandacategoryofopendynamicalsystemsontheother:thefunctorwouldtakeanappropriatelydefined
statisticalmodelorstatisticalgame, andreturnadynamicalsystemthatcouldbeunderstoodasplayingthe
game (or inverting the model); the functoriality of this relationship would ensure that the compositional
(including hierarchical) structure of the model would be recapitulated in the compositional structure of the
resultingdynamicalsystem.
Exhibiting functors of this type, which collectively we call approximate inference doctrines, is the task of
Section4,andindeedwefindthattheaforementionedneuralcircuitmodelsarisepreciselyinthisway. Not
only does this explain the mathematical origin of the structure of these circuits, but it simplifies the job
of modelling, as one no longer needs to perform a complicated computation for each model: instead, it is
sufficient to obtain the dynamics for each factor of the model, and compose them according to the rules of
thecategory. (Inthispaper,wefocusonfunctorsfromstatisticalmodelstodynamicalsystems. Oneclaimof
thefreeenergyframeworkisthatitfurnishesauniversalwaytounderstandadaptivedynamicalsystemsin
terms of Bayesian inference [4], suggesting functors in the opposite direction which we might hypothesize
tobeappropriatelyadjoint. Understandingthisrelationshipisthesubjectoffuturework.)
Overviewofthispaper Beforewecanexhibitanysuchfunctors,weneedtolaytheappropriatemathe-
maticalgroundwork. Forourpurposes,therearetwooverlappingaspects: amathematicallanguageinwhich
totalkaboutstochasticinteractingsystems;andadefinitionofopendynamicalsystemthatcanbeexpressed
inthislanguageandthatcanbecastintothetherelevantcompositionalform.
In§2therefore,weintroducethecategoryofpolynomialfunctorsasourchoiceoflanguageforinteraction.
WethinkofapolynomialasplayingaformalroleakintothatofthenotionofMarkovblanketintheinformal
activeinferenceliterature,asitdefinestheshapeorboundaryorinterfaceofatypeofsystem;morphismsof
polynomialsdescribehowinformationflowsbetweentheboundariesofcoupledsystems. In§2.1,wegeneral-
izetheusualcategoryofpolynomialsinordertocapturestochasticinteractionsandtheflowofprobabilistic
information.
Then, in §3, we turn our attention to dynamics. We begin the section by defining a general notion of
dynamical system on an interface using the language of polynomials. We then package these systems up
intocategoriesindexedbypolynomials: eachcategoryrepresentsacollectionofwaysthataninterfacemay
be animated. Subsequently, in §3.1, we bring these categories together with the category of polynomials
itselftoconstructanewcollectionofcategoriesofhierarchicalbidirectionaldynamicalsystemswhichhave
the necessary compositional structure to define approximate inference doctrines; then, in §3.2, we present
corresponding categories of differential systems, which often form a useful intermediate step on the way to
dynamicalsystems,andshowhowtoobtaindynamicalsystemsfromthem.
Finally,in§4,weintroduceapproximateinferencedoctrines,concentratingontwothatareneuroscientifi-
callyrelevant. Webeginthesectionbyintroducingtwopiecesofauxiliarytechnology:categoriesofGaussian
channels(§4.2,tocapturethetwoneuroscientificdoctrines);andparameterizedstatisticalgames(§4.1,tocap-
tureparameterlearning likesynapticplasticity). Thisputsusin thepositionatlast todefinetwodoctrines:
the Laplace doctrine (§4.3) for Gaussian channels; and the Hebb-Laplace doctrine (§4.4) for parameterized
Gaussianchannels,wherenotonlyisthemodelinvertedbuttheparametersarelearnt,too.
2
2 Polynomial functors: a language for interacting systems
Inordertobeconsideredadaptive,asystemmusthavesomethingtoadaptto. This‘something’isoftenwhat
we call the system’s environment, and we say that the system is open to its environment. The interface or
boundary separating the system from its environment can be thought of as ‘inhabited’ by the system: the
system is embodied by its interface of interaction; the interface is animated by the system. In this way, the
system can affect the environment, by changing the shape or configuration of its interface1; through the
coupling, these changes are propagated to the environment. In turn, the environment may impinge on the
interface: its own changes, mediated by the coupling, arrive at the interface as immanent signals; and the
type of signals to which the system is alive may depend on the system’s configuration (as when an eye can
onlyperceiveifitslidisopen). Thus,informationflowsacrosstheinterface.
Themathematicallanguagecapturingthiskindofinhabitedinteractionisthatofpolynomialfunctors,which
we adopt following Spivak and Niu [5]. Informally, a polynomial functor is determined by a type or set of
possible configurations, along with, for each possible configuration, a corresponding type or set of possible
immanent signals (‘inputs’). We will often write p to denote a polynomial, pp1q its possible configurations,
andforeachi : pp1q,prisforthecorrespondinginputs.
Inthissection,weintroducethebasictheoryofpolynomialfunctors;inthefollowingsubsection,weextend
the theory to allow for more general kinds of interaction, to allow for explicitly probabilistic information
flows. Taking a broader view, in this paper we only make use of a fragment of the richness of polynomial
interaction: justenoughtobuildopenandhierarchicaldynamicalsystemsthatcanperforminferencewithin
asinglesystem. Laterinthisseries,wewillexpandouruseofthelanguagetotreatmultipleinteractingactive
inference systems, toprovide something like atheory of “polynomial life”, buildingon our earlier work[6].
Now,however,webeginbyintroducingtheformaldefinitionoftheclassicalcategoryofpolynomialfunctors.
Definition2.1. LetE bealocallyCartesianclosedcategory(suchasSet),anddenotebyyAtherepresentable
copresheafyA :“ EpA,´q : E Ñ E. Apolynomialfunctor pisacoproductofrepresentablefunctors,written
ř
p :“ ypi , where pp1q : E is the indexing object. The category of polynomial functors in E is the
i:pp1q
fullsubcategoryPoly ãÑ rE,EsoftheE-copresheafcategoryspannedbycoproductsofrepresentables. A
E
morphismofpolynomialsisthereforeanaturaltransformation.
Remark 2.2. Every polynomial functor P : E Ñ E corresponds to a bundle p : E Ñ B in E, for which
B “ Pp1q and for each i : Pp1q, the fibre p is Ppiq. We will henceforth elide the distinction between a
i ř
copresheaf P and its corresponding bundle p, writing pp1q :“ B and pris :“ p , where E “ pris. A
i i
natural transformation f : p Ñ q between copresheaves therefore corresponds to a map of bundles. In the
case of polynomials, by the Yoneda lemma, this map is given by a ‘forwards’ map f : pp1q Ñ qp1q and
1
a family of ‘backwards’ maps f# : qrf p-qs Ñ pr-s indexed by pp1q, as in the left diagram below. Given
1
f : p Ñ q andg : q Ñ r,theircompositeg˝f : p Ñ r isasintherightdiagrambelow.
f# pgfq#
E f˚F F E f˚g˚G G
{ {
p q p r
B B
f1
C B B
g1˝f1
D
wherepgfq# isgivenbythepp1q-indexedfamilyofcompositemapsrrg pf p-qqs Ý f Ý ˚ Ý g Ñ # qrf p-qs Ý f ÝÑ # pr-s.
1 1 1
Wenowrecallahandfulofusefulfactsaboutpolynomialsandtheirmorphisms,eachofwhichisexplained
inSpivakandNiu[5]andsummarizedinSpivak[7].
1Suchchangescanbeverygeneral: considerforinstancethechangesinvolvedinproducingsound(e.g.,rapidvibrationoftissue)
orlight(e.g.,connectingaluminescentcircuit,orthemolecularinteractionsinvolvedtherein).
3
ř
Proposition2.3. Polynomialmorphismsp Ñ ycorrespondtosectionspp1q Ñ prisofthecorresponding
i
bundlep.
Proposition2.4. ThereisanembeddingofE intoPoly givenbytakingobjectsX : E tothelinearpoly-
E
nomialsXy : Poly andmorphismsf : X Ñ Y tomorphismspf,id q : Xy Ñ Yy.
E X
Proposition2.5. Thereisasymmetricmonoidalstructurepb,yqonPoly thatwecalltensor, andwhich
ř ř E
is given on objects by p b q :“ yprisˆqrjs and on morphisms f :“ pf ,f#q : p Ñ p1 and
i:pp1q j:qp1q 1
g :“ pg ,g#q : q Ñ q1 byf bg :“ pf ˆg ,f#ˆg#q.
1 1 1
Proposition2.6. pPoly ,b,yqissymmetricmonoidalclosed,withinternalhomdenotedr´,“s. Explicitly,
ř E ř
wehaverp,qs “ y i:pp1q qrf1piqs. GivenanobjectA : E,wehaverAy,ys – yA.
f:pÑq
Proposition2.7. Thecompositionofpolynomialfunctorsq˝p : E Ñ E Ñ E inducesamonoidalstructure
on Poly , which we denote Ÿ, and call ‘composition’ or ‘substitution’. Its unit is again y. Famously, Ÿ-
E
comonoidscorrespondtocategoriesandtheircomonoidhomomorphismsarecofunctors[8]. IfTisamonoid,
thenthecomonoidstructureony TcorrespondswitnessesitasthecategoryBT. MonomialsoftheformSyS
canbeequippedwithacanonicalcomonoidstructurewitnessingthecodiscretegroupoidonS.
2.1 Generalizedpolynomialsforstochasticfeedback
The category of polynomial functors Poly introduced above for a locally Cartesian closed category E can
E
be considered as a category of ‘deterministic’ polynomial interaction; notably, morphisms of polynomials,
which encode the coupling of systems’ interfaces, do not explicitly incorporate any kind of randomness or
uncertainty.Eveniftheuniverseisdeterministic,however,thefinitenessofsystemsandtheirgeneralinability
toperceivethetotalityoftheirenvironmentsmakeitaconvenientmodellingchoicetosupposethatsystems’
interactions may be uncertain; this will be useful not only in allowing for stochastic interactions between
systems,butalsotodefinestochasticdynamicalsystems‘internally’toacategoryofpolynomials.
To reach the desired generalization, we begin by recalling that Poly is equivalent to the category of
ş E
Grothendiecklensesfortheself-indexingofE[5,9]:Poly – E{´op,wheretheoppositeistakenpointwise
E
on each E{B; this is the formal basis for Remark 2.2. We define our categories of generalized polynomials
from this perspective, by considering categories indexed by their “deterministic subcategories”: this allows
ustodefinecategoriesofGrothendiecklenseswhichbehavelikePoly (whenrestrictedtothedeterministic
E
case),butalsoadmituncertaininputs.
Notation2.8. SupposeCisasymmetricmonoidalcategory. WewriteComonpCqtodenotethesubcategory
ofcommutativecomonoidsandcomonoidhomomophismsinC.
Example 2.9. Suppose P : E Ñ E is a probability monad2 on E. Then every object in ` K(cid:96)pPq˘ is equipped
withacanonicalcomonoidstructure(thecopy-discardstructure[11, §2]), andComon K(cid:96)pPq isthewide
subcategoryof‘deterministic’channels. Intuitively,thisfollowsalmostbydefinition: adeterministicprocess
is one that has no informational side-effects; that is to say, whether we copy a state before performing the
process on each copy, or perform the process and then copy the resulting state, or whether we perform the
processandthenmarginalize,orjustmarginalize,makesnodifferencetotheresultingstate. Thisisjustwhat
itmeansfortheprocesstobeacomonoidhomomorphism;inotherwords,deterministicprocessesintroduce
` ˘
nonewcorrelations. Infact,Comon K(cid:96)pPq – E.
2By‘probabilitymonad’,wemeanamonadPonE takingeachobjectXtoanobjectPXthatbehaveslikea‘spaceofprobability
distributionsonX’. Themonadmultiplicationperformsa‘weightedaverage’ofdistributions,andthemonadunitreturnsthe
pointor‘Diracdelta’distributiononeachelement. Formoreinformationonandanumberofexamplesofprobabilitymonads,
wereferthereadertoJacobs[10].WewilloftenwritePtodenoteagenericprobabilitymonad.
4
Withtheseideasinmind,wemakethefollowingdefinitions.
Definition2.10. SupposepC,b,Iqisacopy-deletecategorysuchthatComonpCqisfinitelycompleteand
I is terminal in ComonpCq. Define an indexed category P : ComonpCqop Ñ Cat as follows. For each
objectB : ComonpCq, thecategoryPpBqhasasobjectsthehomomorphismsE Ñ B ofComonpCqsuch
thatforanyotherhomomorphismA Ñ B,thepullbackAˆ E satisfiestheuniversalpropertyinC. Given
B
a morphism f : C Ñ B, the functor Ppfq : PpBq Ñ PpCq is given by pullback: Ppfq :“ f˚; this is
well-definedbytheuniversalproperty.
Definition2.11. SupposeeachfunctorPpfq : PpBq Ñ PpCqhasaleftadjoint, denotedΣ . Wedefinethe
ş f
category Poly of polynomials in C to be the category of P-lenses: Poly :“ Pop, where the opposite is
C C
takenpointwise.
Example 2.12. When C is any locally Cartesian closed category such as Set, equipped with its Cartesian
monoidalstructure,Definition2.10recoversitsself-indexingandhencePoly istheusualcategoryofpoly-
C
nomialsinC.
Example 2.13. Suppose E is a finitely complete category and M is a monoidal monad on E. Denote by ι
theidentity-on-objectsinclusionE ãÑ K(cid:96)pMqgivenonmorphismsbypost-composingwiththeunitηofthe
monad structure. Setting C “ K(cid:96)pMq, we find that for B : E, PpBq is the full subcategory of K(cid:96)pMq{B on
those objectsιp : EÑ‚ B which correspondto mapsE ÑÝ p B Ý η ÝÑB MB in theimage ofι. Given a morphism
f : C Ñ BinE,thefunctorPpfqtakesobjectsιp : EÑ‚ Btoιpf˚pq : f˚EÑ‚ Cwheref˚pisthepullbackofp
alongf inE,includedintoK(cid:96)pMqbyι.NowsupposethatαisamorphismpE,ιp : EÑ‚ Bq Ñ pF,ιq : FÑ‚ Bq
inPpBq,andnotethatsincewemusthaveιq‚α “ ιp,αmustcorrespondtoafamilyofmapsα : prxs Ñ
x
Mqrxsforx : B. Therefore,PpfqpαqcanbedefinedpointwiseasPpfqpαq :“ α : prfpyqs Ñ Mqrfpyqs
y fpyq
fory : C.
Notation 2.14. For any such monoidal monad M where E has dependent sums, we will write Poly as
M
shorthanddenotingthecorrespondinggeneralizedcategoryofpolynomialsPoly . Sinceeverycategory
K(cid:96)pMq
C corresponds to a trivial monad which we can also denote by C, this notation subsumes that of Definition
2.11.
Remark2.15. WecanthinkofPoly asadependentversionofthecategoryofM-monadiclenses,inthe
M
senseofClarkeetal.[12,§3.1.3].
UnwindingExample2.13further, wefindthattheobjectsofPoly arethesamepolynomialfunctorsas
M
constitutetheobjectsofPoly . Themorphismsf : p Ñ q arepairspf ,f#q,wheref : B Ñ C isamapin
E 1 1
E andf# isafamilyofmorphismsqrf
1
pxqsÑ‚ prxsinK(cid:96)pMq,makingthefollowingdiagramcommute:
ř ř ř
f#
Mprxs qrf pxqs qrys
x:B b:B 1 y:C
{
ηB ˚p q
B B
f1
C
Our principal example of interest is of this form, being Poly for a probability monad P on E 3. We we
P
considereachsuchcategoryPoly tobeacategoryofpolynomialswithstochasticfeedback.
P
3Ideally,EwouldalsobelocallyCartesianclosed,sothatPoly recapitulatesmuchofthebasicstructureofPoly (seeRemark
P Set
2.17):suchexamplesincludethecategoryQBSofquasi-Borelspacesequippedwiththequasi-Boreldistributionmonad[13],or
thecategorySetequippedwiththefinitely-supporteddistributionmonad.
5
Remark2.16. ByassumingthatthecategoryChasamonoidalstructurepb,Iq,itscorrespondinggeneralized
category of polynomials Poly inherits a tensor akin to that defined in Proposition 2.5, and which we also
C
denotebypb,Iq:thedefinitiononlydiffersbysubstitutingthestructurepb,IqonCfortheproductpˆ,1qon
E.ThisfollowsfromthemonoidalGrothendieckconstruction:Pislaxmonoidal,withlaxatortakingp : PpBq
andq : PpCqtopbq : PpBbCq.
Ontheotherhand,forPoly alsotohaveaninternalhomrq,rsrequireseachfibreofPtobeclosedwith
C
respecttothemonoidalstructure. Incasesofparticularinterest,ComonpCqwillbelocallyCartesianclosed,
andrestrictingPtoitsself-indexingproducesfibreswhicharethusCartesianmonoidalclosed. Inthesecases,
wecanthinkofthebroaderfibresofP,andthusPoly itself,asbeing‘deterministically’closed. Thismeans,
C
for the stochastic example Poly , we get an internal hom satisfying the adjunction Poly pp b q,rq –
P P
Poly pp,rq,rsqonlywhenthebackwardscomponentsofmorphismspbq Ñ rare‘uncorrelated’between
P
pandq.
Remark 2.17. For Poly to behave faithfully like the usual category of polynomial functors, we should
C
wantthesubstitutionfunctorsPpfq : PpCq Ñ PpBqtohaverightadjointsaswellasleft. Asinthepreceding
remark, these only obtain in restricted circumstances; we will consider the case of Poly for a monad M,
M
writingf˚ todenotethefunctorPpfq.
Denote the putative right adjoint by Π
f
: PpBq Ñ PpCq, and for ιp : EÑ‚ B suppose that pΠ
f
Eqrys is
givenbythesetof‘partialsections’σ : f´1tyu Ñ ME ofpoverf´1tyuasinthecommutativediagram:
f´1tyu tyu
σ
{
ME
ηB ˚p
B
f
C
ThenwewouldneedtoexhibitanaturalisomorphismPpBqpf˚D,Eq – PpCqpD,Π Eq. Butthiswillonly
f
obtain when the ‘backwards’ components h# : Drys Ñ MpΠ Eqrys are in the image of ι—otherwise, it is
y f
notgenerallypossibletopullf´1tyuoutofM.
3 Open dynamical systems on polynomial interfaces
Having constructed Poly , we are now in a position to construct, for each p : Poly , a category of open
C C
dynamicalsystemsCoalg T ppqwithinterfacep,andwecanevenstatethedefinitionentirelyinthelanguage
C
of Poly . Here, T is a monoid object pT,`,0q in ComonpCq that represents time, which is necessary in
C
generaltoensurethatthedynamicscan‘flow’appropriately;slightlymoreformally,wewillneedtoensure
thatevolvingthedynamicsfortimet : Tandthens : Tproducesthesametrajectoryasevolvingitfortime
t`s,andthatevolvingitfornotime0 : Tinducesnochange. IfwechooseC “ K(cid:96)pPqforP aprobability
monad, we obtain categories of stochastic systems that we call open Markov processes, although we develop
thetheoryinamoregeneralcontext(allowingforothertypesoftransition,asasnondeterministic).
Wefirstgiveaconcisedefinition,internaltoPoly ,beforeunpackingitintoamoreelementaryform.
C
Definition3.1. Anopendynamicalsystemwithinterfacep : Poly ,statespaceS : C andtimepT,`,0qis
C
apolynomialmorphismβ : SyS Ñ rTy,pssuchthat,foranysectionσ : p Ñ y,theinducedmorphism
SyS ÝÑ β rTy,ps Ý r Ý T Ý y, Ý σ Ñ s rTy,ys ÝÑ „ y T
isaŸ-comonoidhomomorphism.
Unpackingthisdefinitiongivesusthefollowingcharacterization:
6
Proposition3.2. Anopendynamicalsystemβ : SyS Ñ rTy,psinPoly consistsinatriplepS,βo,βuqof
C ř ř
astatespaceS : C andtwomorphismsβo : TˆS Ñ pp1qinComonpCqandβu : prϑopt,sqs Ñ S
ř t:T s:S
inC,suchthat,foranysectionσ : pp1q Ñ prisofp,themorphismsβσ : TˆS Ñ S givenby
i:pp1q
ÿ ÿÿ
S Ý β Ý o Ý pt Ý q˚ ÝÑ σ prβopt,sqs Ý β Ñ u S
t:T t:Ts:S
` ˘
form an object in the functor category Cat BT,C , where BT is the delooping of T. We call the closed
systemβσ,inducedbyasectionσ ofp,theclosureofβ byσ. Equivalently,wecansaythatβσ : TˆS Ñ S
formsanactionofthemonoidTonS inC.
Open dynamical systems on p form a category, which we denote by Coalg T ppq. We can exhibit this
C
category abstractly, by noting that a morphism SyS Ñ r of polynomials is equivalent to a morphism S Ñ
rpSq in C: that is, to an r-coalgebra; morphisms of open dynamical systems then correspond to coalgebra
homomorphisms, and this gives us a category. For our purposes here, however, it is more illuminating to
exhibitCoalg T ppqexplicitly.
C
Proposition3.3. OpendynamicalsystemsonpwithtimeTformacategory,denotedCoalg T.Itsmorphisms
C
aredefinedasfollows. Letϑ :“ pX,ϑo,ϑuqandψ :“ pY,ψo,ψuqbetwosuchsystems. Amorphismf : ϑ Ñ
ř
ψconsistsinamorphismf : X Ñ Y inCsuchthat,foranytimet : Tandglobalsectionσ : pp1q Ñ pris
i:pp1q
ofp,thefollowingsquarecommutes:
ř
X
ϑoptq˚σ
prϑopt,xqs
ϑuptq
X
x:X
f f
ř
Y prψopt,yqs Y
ψoptq˚σ ψuptq
y:Y
Theidentitymorphismid onϑisgivenbytheidentitymorphismid onitsstatespaceX. Compositionof
ϑ X
morphismsisgivenbycompositionofthemorphismsofthestatespaces.
SinceopendynamicalsystemsonparemorphismsSyS Ñ rTy,psofpolynomials,thereisanaturalcovari-
antreindexingofsystemsalongmorphismsp Ñ q,givenbypostcomposingwiththemaprTy,ps Ñ rTy,qs
inducedbythefunctorrTy,´s.ThisgivesCoalg T p´qthestructureofanopindexedcategoryPoly Ñ Cat,
C C
whichwespelloutinthefollowingproposition.
Proposition 3.4. Coalg T ppq extends to an opindexed category, Coalg T p´q : Poly Ñ Cat. Suppose
C C C
ϕ : p Ñ q is a morphism of polynomials. We define a corresponding functor Coalg T pϕq : Coalg T ppq Ñ
C C
Coalg T pqqasfollows.SupposepX,ϑo,ϑuq : Coalg T ppqisanobject(system)inCoalg T ppq.ThenCoalg T pϕqpX,ϑo,ϑuq
C C C C
isdefinedasthetriplepX,ϕ ˝ϑo,ϑu˝ϑo˚ϕ#q : Coalg T pqq,wherethetwomapsareexplicitlythefollowing
1 C
composites:
ÿ ÿ ÿ ÿ
TˆX Ý ϑ Ñ o pp1q Ý ϕ Ñ1 qp1q, qrϕ ˝ϑopt,xqs Ý ϑ Ý o Ý ˚ϕ ÝÑ # prϑopt,xqs Ý ϑ Ñ u X.
1
t:Tx:X t:Tx:X
On morphisms, Coalg T pϕqpfq : Coalg T pϕqpX,ϑo,ϑuq Ñ Coalg T pϕqpY,ψo,ψuq is given by the same
C C C
underlyingmapf : X Ñ Y ofstatespaces.
Itissometimesusefultorelatedynamicalsystemswithdifferenttimemonoids—forinstance,todiscretizea
continuous-timesystem,ortoadjustthetimescaleofevolutionofasystem—andforthesepurposeswehave
thefollowingproposition.
7
Proposition3.5. Anymapf : T1 Ñ TofmonoidsinducesanindexedfunctorCoalg T Ñ Coalg T1.
C C
Proof. WefirstconsidertheinducedfunctorCoalg T ppq Ñ Coalg T1 ppq,whichwedenoteby∆p. Notethat
C C f
wehaveamorphismrfy,ps : rTy,ps Ñ rT1y,psofpolynomialsbysubstitution(precomposition). Asystem
β in Coalg T is a morphism SyS Ñ rTy,ps for some S, and so we define ∆p pβq to be rf,ps˝β : SyS Ñ
C f
rTy,ps Ñ rT1y,ps. To see that this satisfies the monoid action axiom, consider that the closure ∆p pβqσ for
f
anysectionσ : p Ñ y isgivenby
ÿ ÿÿ
S Ý β Ý o Ý pf Ý p Ý tq Ý q˚ ÝÑ σ prβopfptq,sqs Ý β Ñ u S
t:T1 t:T1s:S
whichisanobjectinthefunctorcategoryCatpBT1,Cqsincef isamonoidhomomorphism. Onmorphisms
ofsystems,thefunctor∆p actstrivially.
f
To see that ∆ collects into an indexed functor, consider that it is defined on each polynomial p by the
f
contravariantactionrf,psoftheinternalhomr´,“s,andthatthereindexingCoalg T pϕqforanymorphism
ϕ of polynomials is similarly defined by the covariant action rTy,ϕs. By the bifunctoriality of r´,“s, we
haverT1y,ϕs˝rfy,ps “ rfy,ϕs “ rfy,qs˝rTy,ϕs,andsoCoalg T1 pϕq˝∆p “ ∆q ˝Coalg T.
C f f C
Corollary 3.6. For each k : R, the canonical inclusion ι : N ãÑ R : i ÞÑ ki induces a corresponding
k
‘discretization’indexedfunctorDisc :“ ∆ : Coalg R Ñ Coalg N.
k ι C C
Usingthetensorproductbofpolynomials,wecanputsystems’interfaces“inparallel”,anditwillbeuseful
todothesameforthesystemsthemselves. Wecandothisusingthecorrespondinglaxmonoidalstructureof
Coalg T p´q.
C
Proposition3.7. Coalg T p´qislaxmonoidalpPoly ,b,yq Ñ pCat,ˆ,1q.Thecomponentsλ : Coalg T ppqˆ
C C p,q C
Coalg T pqq Ñ Coalg T ppbqq of the laxator natural transformation λ are the functors defined as follows.
C C
Onobjects,givenβ : XyX Ñ rTy,psoverpandγ : YyY Ñ rT,qsoverq,thesystemλ pβ,γqisthesystem
p,q
pX bYqypXbYq ÝÑ “ XyX bYyY Ý β Ý b ÝÑ γ rTy,psbrT,qs Ý υ ÝpÑ,q rTy,pbqs
withstatespaceX ˆY. Theforwardscomponent
` ˘ ` ˘ ` ˘
υ : ComonpCq T,pp1q ˆComonpCq T,qp1q Ñ ComonpCq T,pp1qˆqp1q
1
ofυ formstheproductoftwotrajectories,takingf : T Ñ pp1qandg : T Ñ qp1qto
p,q
υ pf,gq :“ T ÝÑ TbT Ý f Ý b Ñ g pp1qbqp1q.
1
Thebackwardscomponentswitnesssimultaneousinputs;inelementwiseform,wehave
ÿ ÿ
υ# : prfptqsbqrgptqs Ñ prfptqsbqrgpt1qs
f,g
t:T t,t1:T
pt,a,bq ÞÑ pt,t,a,bq.
On morphisms ϕ : β Ñ β1 and ψ : γ Ñ γ1, λ pϕ,ψq : λ pβ,γq Ñ λ pβ1,γ1q is defined by taking the
p,q p,q p,q
productoftheunderlyingmapsofstatespacesϕ : X Ñ X1 andψ : Y Ñ Y1. Wewilloverloadthenotation,
writingβbγ inplaceofλ pβ,γq,andsimilarlyϕbψ onmorphisms.
p,q
Finally, the unitor (cid:15) : 1 Ñ Coalg T pyq is the functor taking the unique object ‹ in the terminal category 1
C
tothe(‘closed’)systemp1,!o,!uqovery withtrivialstatespace,trivialoutputmap,andtrivialupdatemap. It
sendstheuniquemorphismid in1totheidentitymapon1.
‹
8
Proofsketch. Firstly, it is straightforward to check that the functors λ and (cid:15) return well-defined systems
p,q
andmorphisms,andthattheyarethemselveswell-definedasfunctors. Next,wecheckthatthefunctorsλ
p,q
collectintoanaturaltransformation. ThisfollowsalmostimmediatelyfromthefunctorialityofrTy,´b“s :
Poly ˆPoly Ñ Poly . Finally,wecheckthattheaxiomsofassociativityandunitalityaresatisfied. This
C C C
followsfromtheassociativityandunitalityofthemonoidalstructurepb,yqonPoly .
C
NotethatCoalg Treallyislax monoidal—thelaxatorsarenotequivalences—sincenotallsystemsoverthe
C
parallelinterfacepbq factorintoasystemoverpalongsideasystemoverq.
3.1 Monoidalbicategoriesofhierarchicalinferencesystems
Whereasitisthemorphisms(1-cells)ofcategoriesoflensesandstatisticalgamesthatrepresentopensystems,
itistheobjects(0-cells)oftheopindexedcategoriesCoalg T4thatplaythisrole;infact,theobjectsofCoalg T
C C
each represent both an open system and its (polynomial) interface. In order to supply dynamical semantics
for statistical games—functors from categories of statistical games to categories of dynamical systems—we
need to cleave the dynamical systems from their interfaces, making the interfaces into 0-cells and systems
into1-cellsbetweenthem,therebylettingthesystems’typesandcompositionmatchthoseofthegames.
To do this, we will associate to each pair of objects pA,Sq and pB,Tq of a category of Bayesian lenses5
a polynomial vAyS,ByTw whose config
`
urations corr
˘
espond to lenses and whose inputs correspond to the
lenses’ inputs. The categories Coalg T vAyS,ByTw will then form the hom-categories of bicategories of
P
hierarchicalinferencesystems,anditisinthesebicategoriesthatwewillfindourdynamicalsemantics.
Definition3.8. LetBayesLens bethecategoryof(non-dependent)BayesianlensesinC,withC enriched
C
in ComonpCq. Then for any pair of objects pA,Sq and pB,Tq in BayesLens , we define a polynomial
C
vAyS,ByTwinPoly by
C
ÿ
vAyS,ByTw :“ yCpI,AqˆT .
` ˘
l:BayesLens pA,Sq,pB,Tq
C
Remark3.9. WecanthinkofvAyS,ByTwasan‘externalhom’polynomialforBayesLens ,playingarole
C
analogoustotheinternalhomrp,qsinPoly . Its‘bipartite’structure—withdomainandcodomainparts—is
C
whatenablescleavingsystemsfromtheirinterfaces,whicharegivenbytheseparts. Thedefinition,andthe
followingconstructionofthemonoidalbicategory,areinspiredbytheoperadOrgintroducedbySpivak[14]
andgeneralizedbyStClereSmithe[15].
Remark3.10. Noteth
`
atvAyS,ByTw˘ isstrictlyspeakingamonomial,sinceitcanbewrittenintheformIyJ
forI “ BayesLens pA,Sq,pB,Tq andJ “ CpI,AqˆT.However,wehavewrittenitinpolynomialform
C
withtheviewtoextendingitinfutureworktodependentlensesanddependentoptics[16,17]—wherewe
willcallsystemsoversuchexternalhompolynomialscilia, asthey“controloptics”—andthesegeneralized
externalhomswillinfactbetruepolynomials.
Proposition 3.11. Definition 3.8 defines a functor BayesLensop ˆ BayesLens Ñ Poly . Suppose
C C C
c :“ pc ,c#q : pZ,Rq ÑÞ pA,Sq and d :“ pd ,d#q : pB,Tq ÑÞ pC,Uq are Bayesian lenses. We obtain
1 1
a morphism of polynomials vc,dw : vAyS,ByTw Ñ vZyR,CyUw as follows. Since the configurations of
vAyS,ByTwarelensespA,Sq ÑÞ pB,Tq,theforwardsmapactsbypre-andpost-composition:
` ˘ ` ˘
vc,dw :“ d p´q c : BayesLens pA,Sq,pB,Tq Ñ BayesLens pZ,Rq,pC,Uq
1 C C
(cid:11) (cid:11)
l ÞÑ d l c
(cid:11) (cid:11)
ş
4or,moreprecisely,theircorrespondingopfibrations CoalgT
C
5Wewillassumethattheselensesarenon-dependentlenses,asinSt.ClereSmithe[1].
9
Foreachsuchl,thebackwardsmapvc,dw # hastypeCpI,ZqbU Ñ CpI,AqbT inC,andisobtainedby
l
analogywiththebackwardscompositionruleforBayesianlenses. Wedefine
vc,dw # :“ CpI,ZqbU Ý c Ý1˚Ý b ÝÑ U CpI,AqbU ÝÝ b ÝÑ U CpI,AqbCpI,AqbU ¨¨¨
l
¨¨¨ Ý C Ý pI Ý , Ý A Ý qb Ý l Ý1˚Ý b ÝÑ U CpI,AqbCpI,BqbU Ý C Ý p Ý I, Ý A Ý qb Ý d Ý # Ý b ÝÑ U CpI,AqbCpU,TqbU ¨¨¨
¨¨¨ Ý
C
Ý
pI
Ý
,A
ÝÝ
qb
Ý
e
Ý
vUÝ,ÑT
CpI,AqbT
wherel istheforwardspartofthelensl : pA,Sq ÑÞ pB,Tq,andc :“ CpI,c qandl :“ CpI,l qarethe
1 1˚ 1 1˚ 1
push-forwardsalongc andl ,andev istheevaluationmapinducedbytheenrichmentofCinComonpCq.
1 1 U,T
InthespecialcasewhereC “ K(cid:96)pPqandComonpCq “ E,wecanwritevc,dw # asthefollowingmapinE,
l
depictedasastringdiagram:
PA
PZ
c
1˚
str
vc,dw # “
l
l
1˚
PT
d5
U
Here, we have assumed that K(cid:96)pPqpI,Aq “ PA, and define d5 : PB ˆ U Ñ PT ` to be the image of
d# : PB Ñ K(cid:96)pPqpU,Tq under the Cartesian closure of E, and str : PAˆPT Ñ P PAˆTq the (right)
strengthofthestrongmonadP.
Proof. Weneedtocheckthatthemappingsdefinedaboverespectidentitiesandcomposition. Itiseasytosee
thatthedefinitionpreservesidentities:intheforwardsdirection,thisfollowsfromtheunitalityofcomposition
inBayesLens ;inthebackwardsdirection,becausepushingforwardsalongtheidentityisagaintheidentity,
C
andbecausethebackwardscomponentoftheidentityBayesianlensistheconstantstate-dependentmorphism
ontheidentityinC.
Tocheckthatthemappingpreservescomposition,weconsiderthecontravariantandcovariantpartssep-
arately. Supposeb :“ pb ,b#q : pY,Qq ÑÞ pZ,Rqande :“ pe ,e#q : pC,Uq ÑÞ pD,VqareBayesianlenses.
1 1
We consider the contravariant case first: we check that vc b,ByTw “ vb,ByTw˝vc,ByTw. The forwards
direction holds by pre-composition of lenses. In the backw(cid:11)ards direction, we note from the definition that
onlytheforwardschannelc playsaroleinvc,ByTw #,andthatroleisagainpre-composition. Wetherefore
1 l
onlyneedtocheckthatpc ‚b q “ c ˝b ,andthisfollowsimmediatelyfromthefunctorialityofCpI,´q.
1 1 ˚ 1˚ 1˚
We now consider the covariant case, that vAyS,e dw “ vAyS,ew˝vAyS,dw. Once again, the forwards
directionholdsbycompositionoflenses. Forsimplicit(cid:11)yofexposition,weconsiderthebackwardsdirectionin
thecaseC “ K(cid:96)pPqandreasongraphically. Inthiscase,thebackwardsmapontheright-handsideisgiven,
10
foralensl : pA,Sq ÑÞ pB,Tqbythefollowingstringdiagram:
PA
str
PA
PU
d5
l d ˚
1˚ 1˚
e5
V
Itiseasytoverifythatthecompositionofbackwardschannelshereispreciselythebackwardschannelgiven
bye d—compareSt. ClereSmithe[1,Theorem3.14]or[18,Theorem5.2]—whichestablishestheresult. The
case(cid:11)forgeneralC isdirectlyanalogous,ontheothersideofthetensor-homadjunction.
Nowthatwehavean‘externalhom’,wemightexpectalsotohaveacorresponding‘externalcomposition’,
representedbyafamilyofmorphismsofpolynomials;weestablishsuchafamilynow,anditwillbeimportant
inourbicategoricalconstruction.
Definition3.12. Wedefinean‘externalcomposition’naturaltransformationc,withcomponents
vAyS,ByTwbvByT,CyUw Ñ vAyS,CyUw
givenintheforwardsdirectionbycompositionofBayesianlenses. Inthebackwardsdirection,foreachpair
oflensesc : pA,Sq ÑÞ pB,Tqandd : pB,Tq ÑÞ pC,Uq,weneedamap
˘
c# : CpI,AqbU Ñ CpI,AqbT bCpI,BqbU
c,d
whichwedefineasfollows:
c# :“ CpI,AqbU ÝÝÝ b ÝÑ CpI,AqbCpI,AqbU bU ¨¨¨
c,d
¨¨¨ Ý C Ý pI Ý ,A Ý q Ý b Ý c Ý1˚Ý b Ý U Ý b ÝÑ U CpI,AqbCpI,BqbU bU ¨¨¨
CpI,Aqb bCpI,BqbUbU
¨¨¨ ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ CpI,AqbCpI,BqbCpI,BqbU bU
CpI,AqbCpI,Bqbd#bUbU
¨¨¨ ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ CpI,AqbCpI,BqbCpU,TqbY bU
¨¨¨ Ý C Ý p Ý I, Ý A Ý qb Ý C Ý p Ý I Ý ,B Ý q Ý ev ÝUÝ,TÝ b ÝÑ U CpI,AqbCpI,BqbT bU
CpI,AqbswapbU
¨¨¨ ÝÝÝÝÝÝÝÝÝÝÑ CpI,AqbT bCpI,BqbU
wherec andev areasin3.11.
1˚ U,T
In the case where C “ K(cid:96)pPq, we can equivalently (and more legibly) define c# by the following string
c,d
11
diagram:
PA
PA
PT
c
1˚
c# :“
c,d PB str
d5
U
U
whered5 andstrarealsoasinProposition3.11.
We leave to the reader the detailed proof that this definition produces a well-defined natural transforma-
tion,notingonlythattheargumentisanalogoustothatofProposition3.11:oneobservesthat,intheforwards
direction,thedefinitionissimplycompositionofBayesianlenses(whichisimmediatelynatural);intheback-
wardsdirection,oneobservesthatthedefinitionagainmirrorsthatofthebackwardscompositionofBayesian
lenses.
Next,weestablishthestructureneededtomakeourbicategorymonoidal.
Definition3.13. Wedefineadistributivelawdofv´,“woverb,anaturaltransformationwithcomponents
vAyS,ByTwbvA1yS1 ,B1yT1 w Ñ vAyS bA1yS1 ,ByT bB1yT1 w,
notingthatAySbA1yS1 “ pAbA1qypSbS1qandByT bB1yT1 “ pBbB1qypTbT1q. Theforwardscomponent
isgivensimplybytakingthetensorofthecorrespondingBayesianlenses,usingthemonoidalproduct(also
denoted b) in BayesLens . Backwards, for each pair of lenses c : pA,Sq ÑÞ pB,Tq and c1 : pA1,S1q ÑÞ
C
pB1,T1q,weneedamap
d# : CpI,AbA1qbT bT1 Ñ CpI,AqˆT ˆCpI,A1qˆT1
c,c1
forwhichwechoose
CpI,AbA1qbT bT1 ÝÝÝ b Ý T Ý b Ý T Ñ 1 CpI,AbA1qbCpI,AbA1qbT bT1¨¨¨
¨¨¨ Ý C Ý pI Ý ,p Ý r Ý oj ÝAÝ qb Ý C Ý p Ý I, Ý pr Ý o Ý j AÝ 1q Ý b Ý T Ý b Ý T Ñ 1 CpI,AqbCpI,A1qbT bT1¨¨¨
¨¨¨ Ý C Ý pI Ý ,A ÝÝ qb Ý s Ý w Ý ap Ý b Ý T Ñ 1 CpI,AqbT bCpI,A1qbT1
whereswapisthesymmetryofthetensorbinC. Notethatd# sodefineddoesnotinfactdependoneither
c,c1
corc1.
We now have everything we need to construct a monoidal bicategory Hier T of dynamical hierarchical
C
inferencesystemsinC,followingtheintuitionoutlinedatthebeginningofthissection.
Remark 3.14. The notion of bicategory that we adopt is the standard one of ‘category weakly enriched in
Cat’, so that between any two 0-cells we have a category of 1-cells (and 2-cells between them), such that
compositionof1-cellsisassociativeandunitaluptonaturalisomorphism.
12
Definition3.15. LetHier T
C
denote
`
themonoidalb
˘
icategorywhose0-cells
`
areobjectspA
˘
,SqinBayesLens
C
,
and whose hom-categories Hier T pA,Sq,pB,Tq are given by Coalg T vAyS,ByTw . The identity 1-cell
C C
id : pA,Sq Ñ pA,SqonpA,Sqisgivenbythesystemwithtrivialstatespace1,trivialupdatemap,and
pA,Sq
outputmapthatconstantlyemitstheidentityBayesianlenspA,Sq ÑÞ pA,Sq. Thecompositionofasystem
pA,Sq Ñ pB,TqthenasystempB,Tq Ñ pC,Uqisdefinedbythefunctor
` ˘ ` ˘
T T
Hier pA,Sq,pB,Tq ˆHier pB,Tq,pC,Uq
C ` ˘ C ` ˘
“ Coalg T vAyS,ByTw ˆCoalg T vByT,CyUw
C C
` ˘
ÝÑ λ Coalg T vAyS,ByTwbvByT,CyUw
C
` ˘ ` ˘
Ý C Ý o Ý a Ý lg Ý T CÝ pc Ñ q Coalg T vAyS,CyUw “ Hier T pA,Sq,pC,Uq
C C
whereλisthelaxatorandcistheexternalcompositionmorphismofDefinition3.12.
Themonoidalstructurepb,yqonHier T derivesfromthestructuresonPoly andBayesLens ,justifying
C C C
ouroverloadednotation. On0-cells,pA,SqbpA1,S1q :“ pAbA1,S bS1q. On1-cellspA,Sq Ñ pB,Tqand
pA1,S1q Ñ pB1,T1q,thetensorisgivenby
` ˘ ` ˘
Hier T pA,Sq,pB,Tq ˆHier T pA1,S1q,pB1,T1q
C ` ˘ C ` ˘
“ Coalg T vAyS,ByTw ˆCoalg T vA1yS1 ,B1yT1 w
C C
` ˘
ÝÑ
λ
Coalg
T vAyS,ByTwbvA1yS1 ,B1yT1
w
C
` ˘
Ý C Ý o Ý a Ý lg Ý T CÝ pd Ñ q Coalg T vAyS bA1yS1 ,ByT bB1yT1 w
` C ˘
“ Hier T pA,SqbpA1,S1q,pB,TqbpB1,T1q
C
wheredisthedistributivelawofDefinition3.13. Thesamefunctors
` ˘ ` ˘ ` ˘
Hier T pA,Sq,pB,Tq ˆHier T pA1,S1q,pB1,T1q Ñ Hier T pA,SqbpA1,S1q,pB,TqbpB1,T1q
C C C
inducethetensorof2-cells;concretely,thisisgivenonmorphismsofdynamicalsystemsbytakingtheproduct
ofthecorrespondingmorphismsbetweenstatespaces.
We do not give here a proof that this makes Hier T into a well-defined monoidal bicategory; briefly, the
C
resultfollowsfromthefactsthattheexternalcompositioncandthetensorbareappropriatelyassociativeand
unital, that Coalg T is lax monoidal, that v´,“w is functorial in both positions, and that v´,“w distributes
P
naturallyoverb.
Beforewemoveontoconsideringdoctrinesofapproximateinference,itwillbeusefultospelloutconcretely
theelementsofamorphismpA,Sq Ñ pB,TqinHier T .
K(cid:96)pPq
Proposition3.16. SupposePisamonadonaCartesianclosedcategoryE.Thena1-cellϑ : pA,Sq Ñ pB,Tq
inHier T isgivenbyatupleϑ :“ pX,ϑo,ϑo,ϑuqof
K(cid:96)pPq 1 2
• achoiceofstatespaceX,
• aforwardsoutputmapϑo : TˆX ˆA Ñ PB inE,
1
• abackwardsoutputmapϑo : TˆX ˆPAˆT Ñ PS inE,and
2
• anupdatemapϑu : TˆX ˆPAˆT Ñ PX inE,
satisfyingthe‘flow’conditionofProposition3.2.
Proof. Theresultfollowsimmediatelyuponunpackingthedefinitions,usingtheCartesianclosureofE.
13
3.2 Differentialand‘cybernetic’systems
Approximateinferencedoctrinesdescribehowsystemsplaystatisticalgames,andareparticularlyofinterest
whenoneaskshowsystems’performancemayimproveduringsuchgame-playing. Oneprominentmethod
of performance improvement involves descending the gradient of the statistical game’s loss function, and
we will see below that this method is adopted by both the Laplace and the Hebb-Laplace doctrines. The
appearance of gradient descent prompts questions about the connections between such statistical systems
andother‘cybernetic’systemssuchasdeeplearnersorplayersofeconomicgames,bothofwhichmayalso
involvegradientdescent[19,20];indeed,ithasbeenproposed[21]thatparameterizedgradientdescentshould
formthebasisofacompositionalaccountofcyberneticsystemsingeneral6.
In order to incorporate gradient descent explicitly into our own compositional framework, we follow the
recipes above to define here first a category of differential systems opindexed by polynomial interfaces and
thenamonoidalbicategoryofdifferentialhierarchicalinferencesystems. Wethenshowhowwecanobtain
dynamical from differential systems by integration, and sketch how this induces a “change of base” from
dynamicaltodifferentialhierarchicalinferencesystems.
Notation 3.17. Write Diff for the subcategory of compact smooth manifold objects in ComonpCq and
C
differentiable morphisms between them. Write T : Diff Ñ VectpDiff q for the corresponding tangent
C C
bundlefunctor,whereVectpDiff qis(thetotalcategoryof)thefibrationofvectorbundlesoverDiff and
C C
their homomorphisms. Write U : VectpDiff q Ñ Diff for the functor that forgets the bundle structure.
C C
WriteT :“ UT : Diff Ñ Diff fortheinducedendofunctor.
C C
RecallthatmorphismsAyB Ñ pinPoly correspondtomorphismsA Ñ pB inC.
C
Definition 3.18. For each p : Poly , define the category DiffSys ppq as follows. Its objects are objects
C C
M : Diff , each equipped with a morphism m : MyTM Ñ p of polynomials in Poly , such that for any
C C
sectionσ : p Ñ yofp,thecompositemorphismσ˝m : MyTM Ñ ycorrespondstoasectionmσ : M Ñ TM
ofthetangentbundleTM Ñ M.Amorphismα : pM,mq Ñ pM1,m1qinDiffSys ppqisamapα : M Ñ M1
C
inDiff suchthatthefollowingdiagramcommutes:
C
M m pTM
α pTα
M1 pTM1
m1
Proposition3.19. DiffSys definesanopindexedcategoryPoly Ñ Cat. Givenamorphismϕ : p Ñ qof
C C
polynomials, DiffSys pϕq : DiffSys ppq Ñ DiffSys pqqactsonobjectsbypostcompositionandtrivially
C C C
onmorphisms.
Proposition3.20. ThefunctorDiffSys islaxmonoidalpPoly ,b,yq Ñ pCat,ˆ,1q.
C C
Proofsketch. Note that T is strong monoidal, withTp1q – 1 and TpMqbTpNq – TpM bNq. The unitor
1 Ñ DiffSys pyqisgivenbytheisomorphism1yT1 – 1y1 – yinducedbythestrongmonoidalstructureof
C
T. Thelaxatorλ : DiffSys ppqˆDiffSys pqq Ñ DiffSys ppbqqissimilarlydetermined: givenobjects
p,q C C C
6Ourownviewoncyberneticsissomewhatmoregeneral,sincenotallsystemsthatmaybeseenascyberneticareexplicitlystruc-
turedasgradient-descenders,andnorevenisexplicitdifferentialstructurealwaysapparent. Inearlierwork,wesuggestedthat
statisticalinferencewasperhapsmoreinherenttocybernetics[22],althoughtodaywebelievethatabetter,thoughmoreinformal,
definitionofcyberneticsystemisperhaps“anintentionally-controlledopendynamicalsystem”. Nonetheless,weacknowledge
thatthisnotionof“intentionalcontrol”maygenerallybereducibletoastationaryactionprinciple,againindicatingtheimpor-
tanceofdifferentialstructure.Weleavethestatementandproofofthisgeneralprincipletofuturework.
14
m : MyTM Ñ pandn : NyTN Ñ q,taketheirtensormbn : pM bNqyTMbTN andprecomposewiththe
inducedmorphismpM bNqyTpMbNq Ñ pM bNqyTMbTN;proceedsimilarlyonmorphismsofdifferential
systems. ThesatisfactionoftheunitalityandassociativitylawsfollowsfromthemonoidalityofT.
WenowdefineamonoidalbicategoryDiffHier ofdifferentialhierarchicalinferencesystems,following
C
thedefinitionofHier T above.
C
Definition3.21. LetDiffHier
C
denotethemonoidal
`
bicategorywho
˘
se0-cellsareagaintheob
`
jectspA,Sqo
˘
f
BayesLens andwhosehom-categoriesDiffHier pA,Sq,pB,Tq aregivenbyDiffSys vAyS,ByTw .
C C C
Theidentity1-cellid : pA,Sq Ñ pA,SqonpA,Sqisgivenbythedifferentialsystemy Ñ vAyS,ByTw
pA,Sq
with state space 1, trivial backwards component, and forwards component that picks the identity Bayesian
lens on pA,Sq. The composition of differential systems pA,Sq Ñ pB,Tq then pB,Tq Ñ pC,Uq is defined
bythefunctor
` ˘ ` ˘
DiffHier pA,Sq,pB,Tq ˆDiffHier pB,Tq,pC,Uq
C ` ˘ `C ˘
“ DiffSys vAyS,ByTw ˆDiffSys vByT,CyUw
C C
` ˘
ÝÑ λ DiffSys vAyS,ByTwbvByT,CyUw
C
` ˘ ` ˘
Ý D Ý i Ý ff Ý S Ý ys ÝCÝ pc Ñ q DiffSys vAyS,CyUw “ DiffHier pA,Sq,pC,Uq
C C
whereλisthelaxatorofProposition3.20andcistheexternalcompositionmorphismofDefinition3.12.
Themonoidalstructurepb,yqonDiffHier issimilarlydefinedfollowingthatofHier T.On0-cells,pA,Sqb
C C
pA1,S1q :“ pAbA1,S bS1q. On 1-cells pA,Sq Ñ pB,Tq and pA1,S1q Ñ pB1,T1q (and their 2-cells), the
tensorisgivenbythefunctors
` ˘ ` ˘
DiffHier pA,Sq,pB,Tq ˆDiffHier pA1,S1q,pB1,T1q
C C
` ˘ ` ˘
“ DiffSys vAyS,ByTw ˆDiffSys vA1yS1 ,B1yT1 w
C C
` ˘
ÝÑ
λ
DiffSys
vAyS,ByTwbvA1yS1 ,B1yT1
w
C
` ˘
Ý C Ý o Ý a Ý lg Ý T CÝ pd Ñ q DiffSys vAyS bA1yS1 ,ByT bB1yT1 w
` C ˘
“ DiffHier pA,SqbpA1,S1q,pB,TqbpB1,T1q
C
wheredisthedistributivelawofDefinition3.13.
FollowingProp. 3.16,wehavethefollowingcharacterizationofadifferentialhierarchicalinferencesystem
pA,Sq Ñ pB,TqinK(cid:96)pPq,forP : E Ñ E.
Proposition3.22. A1-cellδ : pA,Sq Ñ pB,TqinDiffHier isgivenbyatupleδ :“ pX,δo,δo,δ#qof
K(cid:96)pPq 1 2
• achoiceof‘statespace’X : Diff ;
E
• aforwardsoutputmapδo : X ˆA Ñ PB inE,
1
• abackwardsoutputmapδo : X ˆPAˆT Ñ PS inE,
2
• astochasticvectorfieldδ# : X ˆPAˆT Ñ PTX inE.
Wecanobtaincontinuous-timedynamicalsystemsfromdifferentialsystemsbyintegration, andconsider
howtodiscretizetheseflowstogivediscrete-timedynamicalsystems.
Proposition3.23. IntegrationinducesanindexedfunctorFlow : DiffSys Ñ Coalg R.
C C
15
Proof. Suppose pM,mq is an object in DiffSys ppq. The morphism m : MyTM Ñ p consists of a map
C ř
m : M Ñ pp1q in ComonpCq along with a morphism m# : prm pxqs Ñ TM in C. Since, for any
1 x:M 1
section σ : p Ñ y, the induced map mσ : M Ñ TM is a vector field on a compact manifold, it generates a
uniqueglobalflowFlowppqpmqσ : RˆM Ñ M [23,Thm.s12.9,12.12],whichfactorsas
ÿ ÿ ÿ
m˚σ Flowppqpmqu
M ÝÝÝ1Ñ prm pxqs ÝÝÝÝÝÝÝÑ M .
1
t:R t:Rx:M
WethereforedefinethesystemFlowppqpmqtohavestatespaceM,outputmapm (forallt : R),andupdate
1
mapFlowppqpmqu. SinceFlowppqpmqσ isaflowforanysectionσ,itimmediatelysatisfiesthemonoidaction
condition. Onmorphismsα : m Ñ m1, wedefineFlowppqpαqbythesameunderlyingmaponstatespaces;
thisisagainwell-definedbytheconditionthatαiscompatiblewiththetangentstructure. Givenamorphism
ϕ : p Ñ qofpolynomials,boththereindexingDiffSys pϕqandCoalg R pϕqactbypostcomposition,andso
C C
itiseasytoseethatCoalg R pϕq˝Flowppq – Flowpqq˝DiffSys pϕqnaturally.
C C
Remark3.24. FromProposition3.23andtheearlierCorollary3.6,weobtainafamilyofcompositeindexed
functorsDiffSys Ý F Ý l Ý o Ñ w Coalg R Ý D Ý is Ý c Ñk Coalg Ntakingeachdifferentialsystemtoadiscrete-timedynamical
C C C
systeminC.Below,wewilldefineapproximateinferencedoctrinesindiscretetimethatarisefromprocessesof
(stochastic)gradientdescent,andwhichthereforefactorthroughdifferentialsystems,buttheforminwhich
these are given—and in which they are found in the informal literature (e.g., Bogacz [24])—is not obtained
via the composite Disc ˝Flow for any k, even though there is a free parameter k that plays the same role
k
(intuitively,a‘learningrate’). Instead,onetypicallyadoptsthefollowing‘naïve’discretizationscheme.
Let CartDiffSys denote the sub-indexed category of DiffSys spanned by those systems with Carte-
C C
sian state spaces Rn. Naive discretization induces a family of indexed functors Naive : CartDiffSys Ñ
k C
Coalg N, for k : R, which we illustrate for a single system pRn,mq over a fixed polynomial p, with m :
C
Rny RnˆRn Ñ p(sinceTRn – RnˆRn). Thissystemisdeterminedbyapairofmorphismsm : Rn Ñ pp1q
ř 1
andm# : prm pxqs Ñ RnˆRn,andwecanwritetheactionofm# aspx,yq ÞÑ px,v pyqq.
x:Rn 1 x
Usingthese,wedefineadiscrete-timedynamicalsystemβoverpwithstatespaceRn. Thisβisgivenbyan
ř
outputmapβo,whichwedefinetobeequaltom ,βo :“ m ,andanupdatemapβu : prβopxqs Ñ Rn,
1 1 x:Rn
whichwedefinebypx,yq ÞÑ x`kv pyq. Together,thesedefineasysteminCoalg N ppq,andthecollection
x C
ofthesesystemsβ producesanindexedfunctorbythedefinitionNaive ppqpmq :“ β.
k
By contrast, the discrete-time system obtained via Disc ˝ Flow involves integrating a continuous-time
k
onefork unitsofrealtimeforeachunitofdiscretetime: althoughthisingeneralproducesamoreaccurate
simulation of the trajectories implied by the vector field, it is computationally more arduous; to trade off
simulation accuracy against computational feasibility, one may choose a more sophisticated discretization
schemethanthatsketchedabove,oratleastchoosea“sufficientlysmall”timescalek.
Finally,wecanusetheforegoingideastotranslatedifferentialhierarchicalinferencesystemstodynamical
hierarchicalinferencesystems.
Corollary3.25. LetCartDiffHier denotetherestrictionofDiffHier tohom-categoriesinCartDiffSys .
C C C
TheindexedfunctorsDisc : Coalg R Ñ Coalg N,Flow : DiffSys Ñ Coalg R,andNaive : CartDiffSys Ñ
k C C C C k C
Coalg N induce functors (respectively) HDisc : Hier R Ñ Hier N, HFlow : DiffHier Ñ Hier R and
C k C C C C
HNaive : CartDiffHier Ñ Hier N bychangeofbaseofenrichment.
k C C
4 Approximate inference doctrines
Wearenowinapositiontobuildthebridgebetweenabstractstatisticalmodelsandthedynamicalsystemsthat
playthem,withthecategoriesofhierarchicaldynamicalsystemsdevelopedintheprevioussectionsupplying
16
the semantics. These bridges will be functors, which we call approximate inference doctrines. In general,
theywillbefunctorsfromcategoriesofparameterizedstatisticalmodels,whoseparametersformpartofthe
dynamicalstatespaces,andoftenweareparticularlyinterestedinonlyaparticularclassofstatisticalmodels,
which typically form a subcategory of a broader category of stochastic channels. We therefore make the
followingdefinition.
Definition4.1. LetDbeasubcategoryofPC. AnapproximateinferencedoctrineforDintimeTisafunctor
D Ñ Hier
T.
C
Here,PC denotestheexternalparameterizationofC,tothedefinitionofwhichwenowturn.
4.1 Externalparameterization
Inthepreviousinstalmentofthisseries,weconsideredparameterizedBayesianlenses[1,§3.4]andstatistical
games[1,Cor. 4.14,Ex. 5.5],inordertotreatsystemswiththeabilitytoimprovetheirstatisticalperformance.
Approximateinferencedoctrinesoperationalizethisimprovement,butinthiscontextitispreferabletocon-
sider statistical systems that are ‘externally’ rather than ‘internally’ parameterized: the improvement of the
performance istypically aprocess thatis ‘external’ tothe solutionof thestatistical problem (e.g., inference)
itself;forinstance,learningisoftenassumed[25]totakeplaceonaslowertimescalethaninference.
Technically, we can see this distinction by considering the type of an internally parameterized Bayesian
lens, following St. Clere Smithe [1, §3.4]. If pγ,ρq : pA,SqÝ p Ý Θ Ý ,Ω Ñ q pB,Tq is such a lens, then its forward
|
channelγ hasthetypeΘbAÑ‚ B,andthebackwardschannelρhasthetypeCpI,ΘbAq Ñ CpT,ΩbSq.
Notice that this means that in general the inversion ρ depends on a joint prior over Θ b A, and produces
an updated state over ΩbS, even though one is often interested only in a family of inversions of the type
CpI,Aq Ñ CpT,Sq parameterized by Ω, with the updating of the parameters taking place in an external
process that ‘observes’ the performance of the statistical game. We make this distinction formal using the
notionofexternalparameterization.
Definition 4.2. Given a category C enriched in pE,ˆ,1q, we define the external parameterization PC of C
in E as the following bicategory. 0-cells are the objects of C, and each hom-category PCpA,Bq is given by
the slice category E{CpA,Bq. The composition of 1-cells is by composing in C after taking the product of
parameters: givenf : Θ Ñ CpA,Bqandg : Ω Ñ CpB,Cq,theircompositeg˝f is
gˆf ‚
g˝f :“ ΩˆΘ ÝÝÑ CpB,CqˆCpA,Bq ÑÝ CpA,Cq
where‚isthecompositionmapforC inE. Theidentity1-cellsarethepointsontheidentitymorphismsin
C. Forinstance,theidentity1-cellonAisthecorrespondingpointid : 1 Ñ CpA,Aq. Wewilldenote1-cells
A
using our earlier notation for parameterized morphisms: for instance, f : A ÝÑ Θ B and id : A ÑÝ 1 A. The
A
horizontalcompositionof2-cellsisgivenbytakingtheirproduct.
Asanexample,letusconsiderexternallyparameterizedstatisticalgames.
Example4.3. ThecategoryPSGame ofexternallyparameterizedstatisticalgamesinChasas0-cellspairs
C
of objects in C (as in the case of Bayesian lenses or plain statistical games). Its 1-cells pA,Sq ÝÑ Θ pB,Tq are
parameterizedgames,consistinginachoiceofparameterspaceΘ,anexternallyparameterizedlensf : Θ Ñ
ř
BayesLens ppA,Sq,pB,Tqq, and an externally parameterized loss function φ : Ctxpf q Ñ R. The
C ϑ:Θ ϑ
identityonpA,Sqisgivenbythetriviallyparameterizedelementid : 1 Ñ BayesLens ppA,Sq,pA,Sqq,
pA,Sq C
equippedwiththezerolossfunction,asinthecaseofunparameterizedstatisticalgames.Givenparameterized
17
games pf,φq : pA,Sq ÝÑ Θ pB,Tq and pg,ψq : pB,Tq Ý Θ Ñ 1 pC,Uq, we form their composite as follows. The
compositeparameterizedlensisgivenbytakingtheproductoftheparameterspaces:
` ˘ ` ˘ ` ˘
ΘˆΘ1 Ý f Ý ˆ Ñ g BayesLens pA,Sq,pB,Tq ˆBayesLens pB,Tq,pC,Uq ÝÑ BayesLens pA,Sq,pC,Uq
C C (cid:11) C
Thecompositefitnessfunctionisgivenaccordingly:
ÿ ÿ ÿ
Ctxpg f q ÝÑ Ctxpg f q2 Ý p Ý g ϑÝ 1 ˚ Ý , Ý f ϑÝ˚Ñ q Ctxpf qˆCtxpg q Ý p Ý φ Ýϑ , Ý ψ ÝϑÑ 1q RˆR ÝÑ ` R
ϑ1 ϑ ϑ1 ϑ ϑ ϑ1
(cid:11) (cid:11)
ϑ:Θ,ϑ1:Θ1 ϑ,ϑ1 ϑ,ϑ1
Forconcision, whenwesayparameterizedstatisticalgame orparameterizedlens intheabsenceoffurther
qualification,wewillhenceforthmeantheexternally(asopposedtointernally)parameterizedversions.
Remark4.4. Inpriorwork,thisexternalparameterizationconstructionhasbeencalled‘proxying’[26]. We
preferthemoreexplicitname‘externalparameterization’,reserving‘proxying’foraslightlydifferentdouble-
categoricalconstructiontoappearinfuturework.
Remark 4.5. Before moving on to examples of approximate inference doctrines, let us note the similarity
ofthenotionsofexternalparameterization,differentialsystem,anddynamicalsystem: bothofthelattercan
be considered as externally parameterized systems with extra structure, where the extra structure is a mor-
phismorfamilyofmorphismsbackinto(analgebraof)theparameterizingobject: inthecaseofdifferential
systems,this‘algebra’isthetangentbundle;fordynamicalsystems,itistrivial;andforgettingthisextrastruc-
turereturnsamereexternalparameterization. Approximateinferencedoctrinesarethusfunctorialwaysof
equippingmorphismswiththisextrastructure,andinthisrespecttheyareclosetothecurrentunderstanding
ofgeneralcompositionalgametheory[20,21].
4.2 ChannelswithGaussiannoise
Our motivating examples from the computational neuroscience literature are defined over a subcategory
of channels between Cartesian spaces with additive Gaussian noise [24, 25, 27]; typically one writes x ÞÑ
fpxq ` ω for a deterministic map f : X Ñ Y and ω sampled from a Gaussian distribution over Y. This
choice is made, as we will see, because it permits some simplifying assumptions which mean the resulting
dynamical systems resemble known neural circuits. In this section, we develop the categorical language in
which we can express such Gaussian channels. We begin by introducing the category of probability spaces
and measure-preserving maps, which we then use to define channels of the general form x ÞÑ fpxq ` ω,
beforerestrictingtothefinite-dimensionalGaussiancase.
` ˘
Definition4.6. LetP-SpcbethecategoryComon 1{K(cid:96)pPq ofprobabilityspacespM,µqwithµ : 1Ñ‚ M
in K(cid:96)pPq (i.e., 1 Ñ PM in E), and whose morphisms f : pM,µq Ñ pN,νq are measure-preserving maps
f : M Ñ N (i.e.,suchthatf ‚µ “ ν inK(cid:96)pPq).
Wecanthinkofx ÞÑ fpxq`ωasamapparameterizedbyanoisesource,andsotoconstructacategoryof
suchchannels,wecanusetheParaconstructioninitsactegoricalform.Wewillusethemonoidal-actegorical
definition of Para given in St. Clere Smithe [1, §2.3], following Capucci et al. [21]; for a comprehensive
reference on actegory theory, see Capucci and Gavranović [28]. The first step is to spell out the actegory
structure.
Proposition4.7. LetP : E Ñ E beaprobabilitymonadonthesymmetricmonoidalcategorypE,ˆ,1q.Then
there is a P-Spc-actegory structure ˚ : P-Spc Ñ CatpE,Eq on E as follows. For each pM,µq : P-Spc,
definepM,µq˚p´q : E Ñ E bypM,µq˚X :“ MˆX. Foreachmorphismf : pM,µq Ñ pM1,µ1qinP-Spc,
definef ˚X :“ f ˆid .
X
18
` ˘
Proofsketch. Theactiononmorphismsiswell-definedbecauseeachmorphismf : MÑ‚ N inComon 1{K(cid:96)pPq
correspondstoamapf : M Ñ N inE;itisclearlyfunctorial. Theunitorandassociatorareinheritedfrom
theCartesianmonoidalstructurepˆ,1qonE.
The resulting Para bicategory, Parap˚q, can be thought of as a bicategory of maps each of which is
equippedwithanindependentnoisesource;thecompositionofmapstakestheproductofthenoisesources,
and 2-cells are noise-source reparameterizations. The actegory structure ˚ is symmetric monoidal, and the
1-categoricaltruncationParap˚q [1,Prop. 2.47]isacopy-deletecategory[11,Def. 2.2](also[1,Def. 2.20])
1
aswenowsketch.
Proposition 4.8. Consider the actegory structure ˚ of Proposition 4.7. Then Parap˚q is a copy-delete
1
category.
Proofsketch. The monoidal structure is defined following Proposition 2.44 of St. Clere Smithe [1]. We need
to define a right costrength ρ with components pN,νq˚pX ˆYq ÝÑ „ X ˆppN,νq˚Yq. Since ˚ is defined
byforgettingtheprobabilitystructureandtakingtheproduct, thecostrengthisgivenbytheassociatorand
symmetryinE:
„ „ „
pN,νq˚pXˆYq “ NˆpXˆYq ÝÑ NˆpY ˆXq ÝÑ pNˆYqˆX ÝÑ XˆpNˆYq “ XˆppN,νq˚Yq
It is clear that this definition gives a natural isomorphism; the rest of the monoidal structure follows from
thatoftheproductonE.
We now need to define a symmetry natural isomorphism β : X ˆY ÝÑ „ Y ˆX in Parap˚q. This is
X,Y
givenbythesymmetryoftheproductinE,undertheembeddingofE inParap˚qthattakeseverymaptoits
parameterizationbytheterminalprobabilityspace.
Therestofthecopy-deletestructureisinheritedsimilarlyfromE.
If we think of K(cid:96)pPq as a canonical category of stochastic channels, for Parap˚q to be considered as a
1
subcategoryofGaussianchannels,weneedthefollowingresult.
Proposition4.9. Thereisanidentity-on-objectsstrictmonoidalembeddingofParap˚q intoK(cid:96)pPq. Given
1
amorphismf : X Ý p Ý Ω Ý ,µ Ñ q Y inParap˚q
1
,formthecompositef ‚pµ,id
X
q : XÑ‚ Y inK(cid:96)pPq.
Proofsketch. First,thegivenmappingpreservesidentities: theidentityinParap˚qistriviallyparameterized,
andisthereforetakentotheidentityinK(cid:96)pPq. Themappingalsopreservescomposites,bythenaturalityof
theunitorsofthesymmetricmonoidalstructureonK(cid:96)pPq. Thatis,givenf : X Ý p Ý Ω Ý ,µ Ñ q Y andg : Y Ý p Ý Θ Ý ,ν Ñ q Z,
theircompositeg˝f : X Ý p Ý Θ Ý b Ý Ω Ý ,ν Ý b Ý µ Ñ q Z istakento
XÝÑ „ ‚ 1b1bXÝ ν Ý b Ý ν Ý‚ b Ý id ÝXÑΘbΩbXÝ g Ý ˝ ‚Ñ f Z
where here g ˝ f is treated as a morphism in K(cid:96)pPq. Composing the images of g and f under the given
mappinggives
XÝÑ „ ‚ 1bXÝ µ Ý b Ý‚ id ÝXÑΩbXÝÑ f ‚ YÝÑ „ ‚ 1bYÝ ν Ý b ‚ÝÑ Y ΘbYÑÝ g ‚ Z
whichisequalto
XÝÑ „ ‚ 1b1bXÝ ν Ý b Ý µ Ý‚ b Ý id ÝXÑΘbΩbXÝ i Ý dΘÝ‚ b ÝÑ f ΘbYÑÝ g ‚ Z
whichinturnisequaltotheimageofthecompositeabove.
19
Thegivenmappingisthereforefunctorial. Toshowthatitisanembeddingistoshowthatitisfaithfuland
injectiveonobjects. SinceParap˚qandK(cid:96)pPqhavethesameobjects,theembeddingistriviallyidentity-on-
objects (and hence injective); it is similarly easy to see that it is faithful, as distinct morphisms in Parap˚q
aremappedtodistinctmorphismsinK(cid:96)pPq.
Finally, since the embedding is identity-on-objects and the monoidal structure on Parap˚q is inherited
fromthatonK(cid:96)pPq(producingidenticalobjects),theembeddingisstrictmonoidal.
WenowrestrictourattentiontoGaussianmaps.
Definition 4.10. We say that f : XÑ‚ Y in K(cid:96)pPq is Gaussian if, for any x : X, the state fpxq : PY is
Gaussian7. Similarly, we say that f : X Ý p Ý Ω Ý ,µ Ñ q Y in Parap˚q is Gaussian if its image under the embedding
Parap˚q ãÑ K(cid:96)pPqisGaussian. GivenacategoryofstochasticchannelsC,writeGausspCqforthesubcat-
1
egorygeneratedbyGaussianmorphismsandtheircompositesinC. GivenaseparableBanachspaceX,write
GausspXqforthespaceofGaussianstatesonX.
Example 4.11. A class of examples of Gaussian morphisms in Parap˚q that will be of interest to us in
section4.4isoftheformx ÞÑ fpxq`ω forsomemapf : X Ñ Y andω distributedaccordingtoaGaussian
distributionoverY. WritingErωsforthemeanofthisdistribution,theresultingchannelinK(cid:96)pPqemitsfor
eachx : X aGaussiandistributionwithmeanfpxq`Erωsandvariancethesameasthatofω.
Remark4.12. Ingeneral,Gaussianmorphismsarenotclosedundercomposition: pushingaGaussiandistri-
butionforwardalonganonlineartransformationwillnotgenerallyresultinanotherGaussian. Forinstance,
consider the Gau ` ssian morp ˘ hisms x ÞÑ fpx `q `ω and ˘ y ÞÑ gpyq `ω1. Their composite in Parap˚q is the
morphismx ÞÑ g fpxq`ωq `ω ` 1;evenifg ˘ fpxq`ωq isGaussian-distributed,thesumoftwoGaussiansis
ingeneralnotGaussian,andsog fpxq`ωq `ω1willnotbeGaussian. Thisnon-closureunderliesthepower
ofstatisticalmodelssuchasthevariationalautoencoder,whichareoftenconstructedbypushingaGaussian
forwardalongalearntnonlineartransformation[29],inordertoapproximateanunknowndistribution;since
samplingfromGaussiansisrelativelystraightforward,thismethodofapproximationcanbecomputationally
tractable.TheGaussconstructionhereisanabstractionoftheGaussian-preservingtransformationsinvoked
by Shiebler [30], and is to be distinguished from the category Gauss introduced by Fritz [31], whose mor-
phisms are affine transformations (which do preserve Gaussianness) and which are therefore closed under
` ˘
composition;thereisnonethelessanembeddingofFritz’sGaussintoourGauss K(cid:96)pPq .
Proposition 4.13. Let FdCartSpcpEq denote the full subcategory of E spanned by finite-dimensional
Cartesian spaces Rn, where n : N. Let P-FdCartSpc denote the corresponding subcategory of P-Spc.
` ˘
Let‹ : P-FdCartSpc Ñ Cat FdCartSpcpEq,FdCartSpcpEq bethecorrespondingrestrictionofthe
monoidalaction˚ : P-Spc Ñ CatpE,EqfromProposition4.7. ThenParap‹qisamonoidalsubbicategory
ofParap˚q.
We will write P : FdCartSpcpEq Ñ FdCartSpcpEq to denote the restriction of the probability
Fd
monadP : E Ñ E toFdCartSpcpEq.
Finally,wegivethedensityfunctionrepresentationofGaussianchannelsinK(cid:96)pP q.
Fd
Proposition4.14. EveryGaussianchannelc : XÑ‚ Y inK(cid:96)pP
Fd
qadmitsadensityfunctionp
c
: Y ˆX Ñ
r0,1s with respect to the Lebesgue measure on Y. Moreover, since Y “ Rn for some n : N, this density
functionisdeterminedbytwomaps: themeanµ : X Ñ Rn,andthecovariance Σ : X Ñ Rnˆn inE. We
c c
callthepairpµ ,Σ q : X Ñ RnˆRnˆn thestatisticalparametersforc.
u c
7We admit Dirac delta distributions, and therefore deterministic channels, as Gaussian, since delta distributions can be seen as
Gaussianswithinfiniteprecision.
20
Proof. Thedensityfunctionp : Y ˆX Ñ r0,1ssatisfies
c
A E a
1
logp py|xq “ (cid:15) ,Σ pxq´1(cid:15) ´log p2πqndetΣ pxq
c c c c c
2
where(cid:15) : Y ˆX Ñ Y : py,xq ÞÑ y´µ pxq.
c c
4.3 TheLaplacedoctrine
Ourfirstexampleofadoctrinearisesinthecomputationalneuroscienceliterature,whichhassoughttoexplain
the apparently ‘predictive’ nature of sensory cortical circuits using ideas from the theory of approximate
inference[3];thegeneralnameforthisneuroscientifictheoryispredictivecoding,andthetaskofapredictive
codingmodelistodefineadynamicalsystemwhosestructuresandbehavioursmimicthoseobservedinneural
circuits in vivo. One way to satisfy this constraint is to describe a procedure that turns a statistical problem
intoadynamicalsystemofaformknowntobesimulablebyaneuralcircuit: thatistosay,therearecertain
classesofdynamicalsystemswhichareknowntoreproducethephenomenologyofneuralcircuitsandwhich
arebuiltoutofpartsthatcorrespondtoknownbiologicalstructures,andsoa“biologicallyplausible”model
ofpredictivecodingshouldproduceaninstanceofsuchaclass,givenastatisticalproblem.
This procedure pushes the ‘plausibility’ constraint back to the level of the statistical problem (since there
are presently no known neural circuit models that can solve any inference problem in general), and one
restrictionthatisusefullymadeisthatallnoisesourcesinthemodelareGaussian. Thisrestrictionallowsus
tomakeanapproximation,knownastheLaplaceapproximation,tothelossfunctionofanautoencodergame
which in turn entails that performing stochastic gradient descent on this loss function (with respect to the
mean of the posterior distribution) generates a dynamical system that is biologically plausible (up to some
levelofbiologicalplausibility)[3,24].
In this section, we begin by defining the Laplace approximation and the resulting dynamical system, and
goontoshowbothhowitarisesandhowtheprocedureisfunctorial: thatis,weshowthatitconstitutesan
approximateinferencedoctrine,anddescribehowthispresentationclarifiestheroleofwhathasbeencalled
the “mean field” assumption in earlier literature [27]. (We leave the study of the biological plausibility of
compositionaldynamicalsystemsforfuturework.)
Lemma4.15(Laplaceapproximation). Suppose:
1. pγ,ρ,φq : pX,Xq Ñ pY,Yq is a simple D -autoencoder game with Gaussian channels between
KL
finite-dimensionalCartesianspaces;
2. forallpriorsπ : GausspXq,thestatisticalparametersofρ : Y Ñ PX aredenotedpµ ,Σ q : Y Ñ
π ρπ ρπ
R|X|ˆR|X|ˆ|X|,where|X|isthedimensionofX;and
3. forally : Y,theeigenvaluesofΣ pyqaresmall.
ρπ
Thenthelossfunctionφ : Ctxpγ,ρq Ñ Rcanbeapproximatedby
“ ‰ “ ‰
φpπ,kq “ E Fpyq « E FLpyq
y„ π|γ|k y„ π|γ|k
(cid:76) (cid:77) (cid:76) (cid:77)
where
FLpyq “ E pµ pyq,yq´S rρ pyqs (1)
pπ,γq ρπ X π
“ ´logp py|µ pyqq´logp pµ pyqq´S rρ pyqs
γ ρπ π ρπ X π
where S rρ pyqs “ E r´logp px|yqs is the Shannon entropy of ρ pyq, and p : Y ˆX Ñ r0,1s,
x π x„ρπpyq ρπ π γ
p : X Ñ r0,1s, and p : X ˆ Y Ñ r0,1s are density functions for γ, π, and ρ respectively. The
π ρπ π
approximationisvalidwhenΣ satisfies
ρπ
` ˘
Σ pyq “ B2E pµ pyq,yq´1 . (2)
ρπ x pπ,γq ρπ
21
WecallFL theLaplacianfreeenergy andE thecorrespondingLaplacianenergy.
pπ,γq
Proof. FollowingProposition4.14,wecanwritethedensityfunctionsas:
b
@ D
1
logp py|xq “ (cid:15) ,Σ ´1(cid:15) ´log p2πq|Y|detΣ
γ γ γ γ γ
2 b
@ D
1
logp px|yq “ (cid:15) ,Σ ´1(cid:15) ´log p2πq|X|detΣ (3)
ρπ
2
ρπ ρπ ρπ
b
ρπ
@ D
1
logp pxq “ (cid:15) ,Σ ´1(cid:15) ´log p2πq|X|detΣ
π π π π π
2
whereforclaritywehaveomittedthedependenceofΣ onxandΣ ony,andwhere
γ ρπ
(cid:15) : Y ˆX Ñ Y : py,xq ÞÑ y´µ pxq,
γ γ
(cid:15) : X ˆY Ñ X : px,yq ÞÑ x´µ pyq, (4)
ρπ ρπ
(cid:15) : X ˆ1 Ñ X : px,˚q ÞÑ x´µ .
π π
Then,recallfrom[1,Remark5.12]thatwecanwritethefreeenergyFpyqasthedifferencebetweenexpected
energyandentropy:
„ 
p px|yq
Fpyq “ E log ρπ
x„ρπpyq p γ py|xq¨p π pxq
“ E r´logp py|xq´logp pxqs´S rρ pyqs
γ π X π
x„ρπpyq
“ ‰
“ E E px,yq ´S rρ pyqs
pπ,γq X π
x„ρπpyq
Next,sincetheeigenvaluesofΣ pyqaresmallforally : Y,wecanapproximatetheexpectedenergybyits
ρπ
second-orderTaylorexpansionaroundthemeanµ pyq:
ρπ
@ ` ˘ D
1
Fpyq « E pµ pyq,yq` (cid:15) pµ pyq,yq, B2E pµ pyq,yq¨(cid:15) pµ pyq,yq
pπ,γq ρπ 2 ρπ ρπ x pπ,γq ρπ ρπ ρπ
“ ‰
´S ρ pyq .
X π
` ˘
where B2E pµ pyq,yqistheHessianofE withrespecttoxevaluatedatpµ pyq,yq.
x pπ,γq ρπ pπ,γq ρπ
Notethat
@ ` ˘ D “` ˘ ‰
(cid:15) pµ pyq,yq, B2E pµ pyq,yq¨(cid:15) pµ pyq,yq “ tr B2E pµ pyq,yq Σ pyq , (5)
ρπ ρπ x pπ,γq ρπ ρπ ρπ x pπ,γq ρπ ρπ
thattheentropyofaGaussianmeasuredependsonlyonitscovariance,
“ ‰
1
S ρ pyq “ logdetp2πeΣ pyqq ,
X π
2
ρπ
andthattheenergyE pµ pyq,yqdoesnotdependonΣ pyq. Wecanthereforewritedowndirectlythe
pπ,γq ρπ ρπ
covarianceΣ˚ pyqminimizingFpyqasafunctionofy. Wehave
ρπ
` ˘
1 1
B Fpyq « B2E pµ pyq,yq` Σ ´1.
Σρπ 2 x pπ,γq ρπ 2 ρπ
SettingB Fpyq “ 0,wefindtheoptimumasexpressedbyequation(2)
Σρπ
` ˘
Σ˚ pyq “ B2E pµ pyq,yq´1 .
ρπ x pπ,γq ρπ
Finally,onsubstitutingΣ˚ pyqinequation(5),weobtainthedesiredexpressionofequation(1)
ρπ
Fpyq « E pµ pyq,yq´S rρ pyqs “: FLpyq.
pπ,γq ρπ X π
22
Remark4.16. Theterms(cid:15) : Y ˆX Ñ Y (&c.) ofeq. (4)areknownaserrorfunctions,sincetheyencodethe
γ
differencebetweeny : Y andtheexpectedelementµ pxq : Y givenx : X. Inapplications,oneoftenthinks
γ
oftheseerrorsaspredictionerrors,interpretingµ asthesystem’spredictionoftheexpectedstateofY.
γ
Inthiscontextonethenalsodefinestheprecision-weighted errors
η py,xq :“ Σ pxq´1(cid:15) py,xq : Y ˆX Ñ Y , (6)
γ γ γ
notingthattheinversecovariancematrixΣ pxq´1 canbeinterpretedasencodingthe‘precision’ofabelief:
γ
roughly speaking, low variance (or ‘diffusivity’) means high precision8. The log-densities of eq. (4.15) are
thenunderstoodasmeasuringtheprecision-weightedlengthoftheerrorvectors.
Definition4.17. Supposeγ : XÑ‚ Y isaGaussianchannelinK(cid:96)pPq.Thenthediscrete-timeLaplacedoctrine
defines a system Lpγq : pX,Xq Ñ pY,Yq in Hier N as follows (using the representation of
GausspK(cid:96)pP Fdqq
Proposition3.16).
• ThestatespaceisX;
• theforwardsoutputmapLpγqo : X ˆX Ñ GausspYqisgivenbyγ:
1
Lpγqo :“ X ˆX Ý p Ý ro ÝÑ j 2 X ÝÑ γ GausspYq
1
• thebackwardsoutputmapLpγqo : X ˆGausspXqˆY Ñ GausspXqisgivenby:
2
Lpγqo : X ˆGausspXqˆY Ñ R|X|ˆR|X|ˆ|X| ãÑ GausspXq
2 ` ˘ (7)
px,π,yq ÞÑ x,Σ px,π,yq
ρ
where the inclusion picks the Gaussian state with the given statistical parameters, whose covariance
` ˘
Σ px,π,yq :“ B2E px,yq´1 isdefinedfollowingequation(2)(Lemma4.15);
ρ x pπ,γq
• theupdatemapLpγqu : XˆGausspXqˆY Ñ GausspXqreturnsapointdistributionontheupdated
mean
Lpγqu : X ˆGausspXqˆY Ñ GausspXq
` ˘
px,π,yq ÞÑ ηP µ px,π,yq
X ρ
whereηP : X Ñ GausspXqdenotestheunitofthemonadP andµ isdefinedby
X ρ
µ px,π,yq :“ x`λB µ pxqTη py,xq´λη pxq.
ρ x γ γ π
Here, the precision-weighted error terms η are as in equation (6) (Remark 4.16), and λ : R is some
`
choiceof‘learningrate’.
Remark4.18. NotethattheupdatemapLpgquasdefinedhereisactuallydeterministic,inthesensethatitis
definedasadeterministicmapfollowedbytheunitoftheprobabilitymonad. However,thegeneralstochastic
settingisnecessary,becausethecompositionofsystemdependsonthecompositionofBayesianlenses,which
isnecessarilystochastic.
Definition4.19. ALaplacianstatisticalgameisaparameterizedstatisticalgamepγ,ρ,φq : pX,Xq Ý X Ñ pY,Yq
satisfyingthefollowingconditions:
1. X andY arefinite-dimensionalCartesianspaces;
8Considertheone-dimensionalcase:asthevarianceσofanormaldistributiontendsto0,thedistributionapproachesaDiracdelta
distribution,whichis“infintelyprecise”.
23
2. theforwardchannelγ isanunparameterizedGaussianchannel;
3. thebackwardchannelρisparameterizedbyX anddefinedasthebackwardsoutputmapoftheLaplace
doctrine(equation(7)ofDefinition4.17);thatis,
ρ : X ˆGausspXqˆY Ñ R|X|ˆR|X|ˆ|X| ãÑ GausspXq
` ˘
px,π,yq ÞÑ x,Σ px,π,yq
ρ
` ˘
wheretheinclusionpickstheGaussianwithmeanxandΣ px,π,yq “ B2E px,yq´1;
ř ` ˘ ρ x pπ,γq “ ‰
4. thelossfunctionφ : Ctx γ,ρ Ñ Risgivenforeachx : Xbyφ pπ,kq “ E FLpyq ,
x:X x x y„ π|γ|k
whereFL istheLaplacianfreeenergy (cid:76) (cid:77)
“ ‰
FLpyq “ E px,yq´S ρpx,π,yq
pπ,γq X “ ‰
“´logp py|xq´logp pxq´S ρpx,π,yq
γ π X
asdefinedinequation(1)ofLemma4.15.
(By“unparameterizedchannel”,wemeanachannelparameterizedbythetrivialspace1;thepairpγ,ρqcon-
stitutes a parameterized Bayesian lens with parameter space X, where the choice of γ simply forgets the
parameter,discardingitalongtheuniversalmapX Ñ 1.)
Proposition 4.20. Given a Laplacian statistical game pγ,ρ,φq : pX,Xq Ñ pY,Yq, Lpγq is obtained by
stochasticgradientdescentofthelossfunctionφwithrespecttothemeanxoftheposteriorρpx,π,yq.
“ ‰
Proof. Wehaveφ pπ,kq “ E FLpyq ,where
x y„ π|γ|k
(cid:76) (cid:77) “ ‰
FLpyq “ ´logp py|xq´logp pxq´S ρpx,π,yq .
γ π X
SincetheentropyS rρ pyqsdependsonlyonthevarianceΣ px,π,yq,tooptimizethemeanxitsuffices
X π ρ
toconsideronlytheenergyE px,yq. Wehave
pπ,γq
E px,yq “ ´logp py|xq´logp pxq
pπ,γq γ π
A E
@ D
1 1
“ ´ (cid:15) py,xq,Σ pxq´1(cid:15) py,xq ´ (cid:15) pxq,Σ ´1(cid:15) pxq
γ γ γ π π π
2 b b 2
`log p2πq|Y|detΣ pxq`log p2πq|X|detΣ
γ π
andastraightforwardcomputationshowsthat
B E px,yq “ ´B µ pxqTΣ pxq´1(cid:15) py,xq`Σ ´1(cid:15) pxq.
x pπ,γq x γ γ γ π π
Wecanthereforerewritethemeanparameterµ px,π,yqemittedbytheupdatemapLpγqu as
ρ
µ px,π,yq “ x`λB µ pxqTη py,xq´λη pxq
ρ x γ γ π
“ x´λB E px,yq
x pπ,γq
“ x´λB FLpyq
x
wherethelastequalityholdsbecausetheentropydoesnotdependonx. ThisshowsthatLpγqu descendsthe
gradientoftheLaplacianenergywithrespecttox.
ToseethenthatLpγquperformsstochasticgradientdescentofφ,notethatinthedynamicalsemantics,the
input y : Y is supplied by the context. In Hier T , the dynamics in the context are stochastic,
GausspK(cid:96)pP Fdqq
24
meaningthat eachy : Y isingeneral sampledfroma randomvariablevaluedinY. Ifwefixthe contextto
sampley from π|γ|k then,foragivenx : X,theexpectedtrajectoryofµ isgivenby
ρ
(cid:76) (cid:77) “ ‰
E µ px,π,yq
ρ
y„ π|γ|k
“ ‰
(cid:76) (cid:77)
“ E x´λB FLpyq
x
y„ π|γ|k
“ ‰
(cid:76) (cid:77)
“ x´λB E FLpyq bylinearityofexpectation
x
y„ π|γ|k
(cid:76) (cid:77)
“ x´λB φ pπ,kq.
x x
Since π|γ|k is just a placeholder for the random variable from which y is sampled, this establishes the
result.(cid:76) (cid:77)
Using the preceding proposition, we obtain the following theorem, expressing the Laplacian statistical
gamesintheimageofanapproximateinferencedoctrine.
Theorem4.21. LetG denotethesubcategoryofPSGame generatedbyLaplacianstatisticalgames
K(cid:96)pP Fdq
pγ,ρ,φq : pX,Xq Ý X Ñ pY,Yqandbythestructuremorphismsofamonoidalcategory.
Then L extends to a strict monoidal functor GausspK(cid:96)pP qq ãÑ G Ñ Hier N , where the
Fd GausspK(cid:96)pP Fdqq
first factor is the embedding taking any such γ to the corresponding Laplacian game, and the second factor
performsstochasticgradientdescentoflossfunctionswithrespecttotheirexternalparameterization.
Ithelpstoseparatetheproofofthetheoremfromtheproofofthefollowinglemma.
Lemma4.22. Thereisanidentity-on-objectsstrictmonoidalembeddingofGausspK(cid:96)pP qqintoG.
Fd
Proof. The structure morphisms of GausspK(cid:96)pP qq are mapped to the (trivially parameterized) structure
Fd
morphisms of G, and any Gaussian channel γ : XÑ‚ Y is mapped to the unique Laplacian statistical game
withγasthe(unparameterized)forwardchannel,andthe(parameterized)backwardchannelandlossfunction
determinedbythedefinitionofLaplacianstatisticalgame.Itisclearthatthisdefinitiongivesafaithfulfunctor,
andthusanembedding. Sinceitpreservesexplicitlythemonoidalstructure,itisalsostrictmonoidal.
ProofofTheorem4.21. Thanks to Lemma 4.22, we now turn to the functor G Ñ Hier N , which
GausspK(cid:96)pP Fdqq
wewillalsodenotebyL;thecompositefunctorisobtainedbypullingthisfunctorG Ñ Hier N
GausspK(cid:96)pP Fdqq
backalongtheembeddingGausspK(cid:96)pP qq ãÑ G.
Fd
Suppose then that g :“ pγ,ρ,φq : pX,Xq Ý X Ñ pY,Yq is a Laplacian statistical game. Proposition 4.20
tells us that Lpgq is obtained by stochastic gradient descent of the loss function φ with respect to the mean
parameter of the backwards channel ρ. By definition of ρ, this mean parameter is given precisely by the
external parameterization, and so we have that Lpgq is obtained by stochastic gradient descent of φ with
respecttothisparameterization.
To extend L to a functor accordingly, we need to check that performing stochastic gradient descent with
respect to the external parameterization preserves identities and composition. First we note that, following
Definition 4.17, the dynamical systems in the image of L emit lenses by filling in the parameterization with
the dynamical state, and by the preceding remarks, update the state by stochastic gradient descent. Next,
notethatidentityparameterizedlensesaretriviallyparameterized,sothereisnoparameterto‘fillin’,andno
state to update; similarly, the loss function of an identity game is the constant function on 0, and therefore
has zero gradient. On identity games pX,Xq ÑÝ 1 pX,Xq, therefore, L returns the system with trivial state
space1thatconstantlyoutputstheidentitylenspX,Xq ÑÞ pX,Xq: butthisisjusttheidentityonpX,Xqin
Hier N ,soLpreservesidentities.
GausspK(cid:96)pP Fdqq
25
We now consider composites. Suppose h :“ pδ,σ,ψq : pY,Yq ÝÑ Y pZ,Zq is another Laplacian game
satisfying the hypotheses of the theorem. Since Hier N is a bicategory, we need to show that
GausspK(cid:96)pP Fdqq
Lphq˝Lpgq – Lph˝gq. Infact, wewillshowthestrongerresultthatLphq˝Lpgq “ Lph˝gq, whichmeans
demonstratingequalitiesbetweenthestatespaces,outputmaps,andupdatemapsofthesystemsontheleft-
andright-handsides.
On state spaces, the equality obtains since the composition of externally parameterized games (Example
4.3) returns a game whose parameter space is the product of the parameter spaces of the factors. Similarly,
composition of systems in Hier N (after Definition 3.15) returns a system whose state space is
GausspK(cid:96)pP Fdqq
theproductofthestatespacesofthefactors. Finally, Lactsbytakingparameterspacestostatespaces, and
wehaveX ˆY “ X ˆY.
Next, we note that the output of a composite system in Hier N is given by composing the
GausspK(cid:96)pP Fdqq
outputsofthefactors. ThisisthesameastheoutputreturnedbyLonacompositegame,sinceoutputsinthe
` ˘
imageofLjustfillintheexternalparameterusingthedynamicalstate. Therefore Lphq˝Lpgq o “ Lph˝gqo.
Wenowconsidertheupdatemaps,beginningbycomputingLph˝gqu. ThestatespaceisXˆY andh˝g
hastypepX,Xq Ý X Ý ˆ ÝÑ Y pZ,Zq,soLph˝gquhastypeXˆY ˆGausspXqˆZ Ñ GausspXˆYq. Following
ř
Example4.3,thecompositelossfunctionpψφq : Ctxph g q Ñ Risgivenby:
µρ:X,µσ:Y µσ
(cid:11)
µρ
“ ` ˘‰
pψφqpµ ,µ ,π,kq “ E FL ρpµ q ,γ;π ,y
ρ σ
y„σpµσqγ‚πX ‚
(cid:76)
π|γ|δ˚
“
k
(cid:77) `
ρ πX X
˘‰
` E FL σpµ q ,δ;γ ‚π ,z
σ γ‚πX X
z„ pMbγq‚π|δ|k
(cid:76) (cid:77)
Here,µ andµ aretheparametersinX andY,respectively,andwewriteg andh toindicatethecorre-
ρ σ µρ µσ
spondinglenseswiththoseparameters. Thecontextispπ,kq,withπ : 1Ñ‚ MbX inGausspK(cid:96)pPqqandπ
X
denotingitsXmarginal,andwithcontinuationk : GausspK(cid:96)pPqqp1,MbZq Ñ GausspK(cid:96)pPqqp1,NbZq,
forsomechoicesofresidualobjectsM andN. Thebackwardschannelsρandσareexternallyparameterized
an ` d state-dependent, so that ρpµ ρ q πX ˘ : YÑ‚ X is returned by ρpµ ρ`q at π X . Explicitly, ρ has the type X ˘ Ñ
E GausspXq,GausspK(cid:96)pPqqpY,Xq , andσ hasthetypeY Ñ E GausspYq,GausspK(cid:96)pPqqpZ,Yq . Fi-
nally,δ˚k isthefunction
GausspK(cid:96)pPqqp1,Mbδq k
GausspK(cid:96)pPqqp1,M bYq ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ GausspK(cid:96)pPqqp1,M bZq ÝÑ GausspK(cid:96)pPqqp1,N bZq
obtainedbypullingbackk alongδ.
Wethereforehave π|γ|δ˚k “ pM bγq‚π|δ|k ,meaningthatwecanrewritethelossfunctionas
(cid:76)« (cid:77) (cid:76) (cid:77) ff
` ˘ “ ` ˘‰
E FL σpµ q ,δ;γ ‚π ,z ` E FL ρpµ q ,γ;π ,y .
z„ π|γ|δ˚k
σ γ‚πX X
y„σpµσqγ‚πX pzq
ρ πX X
(cid:76) (cid:77)
In the dynamical semantics for stochastic gradient descent, z and π are supplied by the inputs to the dy-
X
namicalsystem: theinputsreplacethecontextforthegame. Rewritingthelossaccordinglygivesafunction
` ˘ “ ` ˘‰
f : pz,π ,µ ,µ q ÞÑ FL σpµ q ,δ;γ ‚π ,z ` E FL ρpµ q ,γ;π ,y .
X ρ σ σ γ‚πX X ρ πX X
y„σpµσqγ‚πX pzq
Next,wecomputeB fpz,π q. Weobtain
pµρ,µσq X
˜ ¸
“ ` ˘‰ ` ˘
B fpz,π q “ B E FL ρpµ q ,γ;π ,y , B FL σpµ q ,δ;γ ‚π ,z
pµρ,µσq X
˜
µρ
y„σpµσqγ‚πX pzq
ρ πX X µσ σ γ‚πX X
¸
“ ` ˘‰ ` ˘
“ E B FL ρpµ q ,γ;π ,y , B FL σpµ q ,δ;γ ‚π ,z .
µρ ρ πX X µσ σ γ‚πX X
y„σpµσqγ‚πX pzq
26
Now,Lph˝gqu isdefinedasreturningthepointdistributiononpµ ,µ q´λB fpz,π q:
ρ σ pµρ,µσq X
pµ ,µ q´λB fpz,π q
ρ˜ σ pµρ,µσq X ¸
“ ` ˘‰ ` ˘
“ E µ ´λB FL ρpµ q ,γ;π ,y , µ ´λB FL σpµ q ,δ;γ ‚π ,z .
ρ µρ ρ πX X σ µσ σ γ‚πX X
y„σpµσqγ‚πX pzq
Wecansimplifythisexpressionbymakingsomeauxiliarydefinitions
` ˘
ρupa,π,yq :“ a´λB FL ρpaq ,γ;π,y
a ` π ˘
σupb,π1,zq :“ b´λB FL σpbq ,δ;π1,z
b π1
sothat
˜ ¸
pµ ,µ q´λB fpz,π q “ E rρupµ ,π ,yqs, σupµ ,γ ‚π ,zq . (8)
ρ σ pµρ,µσq X
y„σpµσqγ‚πX pzq
ρ X σ X
Bycurryingρupµ ,π ,yqintoafunctionρupµ ,π q : Y Ñ PX,wecansimplifythisstillfurther,since
ρ X ρ X
E rρupµ ,π ,yqs “ ρupµ ,π q‚σpµ q pzq.
ρ X ρ X σ γ‚πX
y„σpµσqγ‚πX pzq
Sinceequation(8)definesLph˝gqu,wehave
` ˘
Lph˝gqupµ ,µ ,π,zq “ ηP ρupµ ,πq‚σpµ q pzq, σupµ ,γ ‚π,zq (9)
ρ σ XˆY ρ σ γ‚π σ
where ηP : X ˆY Ñ GausspX ˆYq is the component of the unit of the monad P at X ˆY, which
XˆY
takesvaluesinDiracdeltadistributionsandisthereforeGaussian.
Next,wecomputetheupdatemapofthesystemLphq˝Lpgq,usingDefinitions3.12and3.15(whichdefine
composition in Hier N ). This update map is given by composing the ‘double strength’9 dst :
GausspK(cid:96)pP Fdqq
GausspXqˆGausspYq Ñ GausspX ˆYqafterthefollowingstringdiagram:
X
Y GausspXq
Lpgqu
GausspXq
(10)
γ
˚
σ5
Z GausspYq
Lphqu
9Thedoublestrengthisalsoknownasthe‘commutativity’ofthemonadP withtheproductˆ.Itsaysthatapairofdistributions
πonX andχonY canalsobethoughtofasajointdistributionpπ,χqonXˆY. ItisGaussianonGaussians,astheproduct
oftwoGaussiansisagainGaussian.
27
Here,σ5 denotestheuncurryingoftheparamet ` erizedstate-dependentchannelσ : ˘ Y Ñ StatpYqpZ,Yq: we
can equivalently write the type of σ as Y Ñ E GausspYq,GausspK(cid:96)pPqqpZ,Yq , which we can uncurry
twicetogivethetypeY ˆGausspYqˆZ Ñ GausspYq.
ObservenowthatwecanwriteLpgqu andLphqu as
` ˘
Lpgqupa,π,yq “ ηP ρupa,π,yq
X` ˘
Lphqupb,π1,zq “ ηP σupb,π1,zq
Y
andth ` atη X P ˆY “˘ dstpη X P,η Y Pq. Readingthestringdiagramandapplyingthisequality,wefindthatitrepre-
sents Lphq˝Lpgq u pµ ,µ ,π,zqas
ρ σ
` ˘
ηP ρupµ ,πq‚σpµ q pzq, σupµ ,γ ‚π,zq
XˆY ρ σ γ‚π σ
whichispreciselythesameasthedefinitionofLph˝gqu inequation(9).
` ˘
Therefore,asrequired, Lphq˝Lpgq u “ Lph˝gqu.
Finally,becausethefunctorLisidentity-on-objects,theunitandmultiplicationofitsmonoidalstructureare
easilyseentobegivenbyidentitymorphisms,andsoLisstrictmonoidal:Lmapsthestructuremorphismsto
constantdynamicalsystemsemittingthestructuremorphismsofHier N ,andsotheassociativity
GausspK(cid:96)pP Fdqq
andunitalityconditionsaresatisfied.
Remark4.23. Fromthediagram(10),wecanrefineourunderstandingofwhatisknownintheliteratureas
themeanfieldapproximation[27,aroundeq.39],inwhichtheposterioroverXbY isassumedateachinstant
oftimetohaveindependentmarginals. Wenotethat,eventhoughthebackwardsoutputmapsemitposterior
distributionswithmeansdeterminedentirelybytheirlocalparameterization,andeventhoughtheseparame-
tersareupdatedbythetensorLpgqubLphqu,theresultingdynamicalstatesarecorrelatedacrosstimebythe
composition rule: this is made very clear by the wiring of diagram (10), since both factors Lpgqu and Lphqu
havecommoninputs. Wealsonotethat,evenifthemeansoftheemittedposteriorsareentirelyparameter-
determined, this is not true of their covariances, which are functions of both the prior and the observation.
Theoperationalresultoftheseobservationsisthatthefunctorial(andpictorial)approachadvocatedhere(as
opposedtowritingdownacomplete,andcomplex,jointdistributionforeachmodelofinterestandproceed-
ingfromthere)helpsusunderstandthestructuralpropertiesofcomplexsystems—whereitisotherwiseeasy
togetlostintheweeds.
Remark4.24. AboveweexhibitedtheLaplacedoctrinedirectlyasafunctor
N
GausspK(cid:96)pP qq ãÑ G Ñ Hier .
Fd GausspK(cid:96)pP Fdqq
Infact,Proposition4.20impliesthatitfactorsfurther,as
GausspK(cid:96)pP qq ãÑ G Ý ∇ Ñ DiffHier Ý H Ý N Ý a Ý iv Ý e Ñk Hier N
Fd GausspK(cid:96)pP Fdqq GausspK(cid:96)pP Fdqq
where ∇ : G Ñ DiffHier takes an externally parameterized statistical game and returns a
GausspK(cid:96)pP Fdqq
differential system that performs gradient descent on its loss function with respect to its parameterization.
Weleavethepreciseexhibitionofthisfactorisationforfuturework.
4.4 TheHebb-Laplacedoctrine
The Laplace doctrine constructs dynamical systems that produce progressively better posterior approxima-
tions given a fixed forwards channel, but natural adaptive systems do more than this: they also refine the
28
forwards channels themselves, in order to produce better predictions. In doing so, these systems better re-
alize the abstract nature of autoencoder games, for which improving performance means improving both
predictionaswellasinversion. Tobeabletoimprovetheforwardschannelrequiresallowingsomefreedom
initschoice,whichmeansgivingitanontrivialparameterization.
The Hebb-Laplace doctrine that we introduce in this section therefore modifies the Laplace doctrine by
fixing a class of parameterized forwards channels and performing stochastic gradient descent with respect
toboththeseparametersaswellastheposteriormeans;wecallittheHebb-Laplacedoctrineastheparticu-
lar choice of forwards channels results in their parameter-updates resembling the ‘local’ Hebbian plasticity
knownfromneuroscience,inwhichthestrengthoftheconnectionbetweentwoneuronsisadjustedaccord-
ingtotheircorrelation. (Here,wecouldthinkofthe‘neurons’asencodingthelevelofactivityalongabasis
vector.)
We begin by defining the category of these parameterized forwards channels, after which we introduce
Hebbian-LaplaciangamesandtheresultingHebb-Laplacedoctrine,whichisderivedsimilarlytotheLaplace
doctrineabove. RecallfromDefinition4.2thatwewritePC todenotetheexternalparameterizationofC in
itsbaseofenrichmentE.
Definition4.25. LetHdenotethesubcategoryofPGausspParap‹qqgeneratedbythestructuremorphisms
ofthesymmetricmonoidalcategoryGausspParap‹qq(triviallyparameterized),andbymorphismsX Ñ Y
oftheform(writteninE)
Θ Ñ GausspParap‹qqpX,Yq
X ´ ¯
θ ÞÑ x ÞÑ θhpxq`ω
where h is a differentiable map X Ñ Y, Θ is the vector space of square matrices on X, and ω is sampled
X
fromaGaussiandistributiononY.
NotethatthereisacanonicalembeddingofPGausspParap‹qqintoPK(cid:96)pP q,obtainedintheimageof
Fd
Proposition4.9undertheexternalparameterizationP.
Definition4.26. AHebbian-Laplacianstatisticalgameisaparameterizedstatisticalgamepγ,ρ,φq : pX,Xq Ý Θ ÝXÝ ˆ Ý X Ñ
pY,Yqsatisfyingthefollowingconditions:
1. X andY arefinite-dimensionalCartesianspaces;
2. theforwardchannelγ isamorphisminH(i.e.,oftheformx ÞÑ θhpxq`ω);
3. thebackwardchannelisasforaLaplacianstatisticalgame(Definition4.19);
4. the loss function is as for a Laplacian statistical game, with the substitution γ ÞÑ γpθq for parameter
θ : Θ .
X
WewillwriteG todenotethesubcategoryofPSGame generatedbyHebbian-Laplacianstatisticalgames
H
andbythestructuremorphismsofamonoidalcategory.
Definition 4.27. Suppose γ : X Ñ Y is a morphism in H. Then the discrete-time Hebb-Laplace doctrine
defines a system Hpγq : pX,Xq Ñ pY,Yq in Hier N as follows (using the representation of
GausspK(cid:96)pP Fdqq
Proposition3.16).
• ThestatespaceisΘ ˆX (whereΘ isagainthevectorspaceofsquarematricesonX);
X X
• theforwardsoutputmapHpγqo : Θ ˆX ˆX Ñ GausspYqisgivenbyγ:
1 X
Hpγqo :“ Θ ˆX ˆX Ý p Ý ro Ý j Ý1Ñ,3 Θ ˆX Ý γ Ñ 5 GausspYq
1 X X
where γ5 is the uncurried form of the morphism γ : Θ Ñ GausspParap‹qqpX,Yq in the image of
X
theembeddingofHinPK(cid:96)pPq;
29
• thebackwardsoutputmapHpγqo : Θ ˆX ˆGausspXqˆY Ñ GausspXqisgivenby:
2 X
Hpγqo : Θ ˆX ˆGausspXqˆY Ñ R|X|ˆR|X|ˆ|X| ãÑ GausspXq
2 X ` ˘
pθ,x,π,yq ÞÑ x,Σ pθ,x,π,yq
ρ
where the inclusion picks the Gaussian state with the given statistical parameters, whose covariance
` ˘
Σ pθ,x,π,yq :“ B2E px,yq´1 isdefinedfollowingequation(2)(Lemma4.15);
ρ x pπ,γpθqq
• theupdatemapHpγqu : Θ ˆXˆGausspXqˆY Ñ GausspΘ ˆXqoptimizestheparameterfor
X X
γ aswellasthemeanoftheposterior(asintheLaplacedoctrine):
Hpγqu : Θ ˆX ˆPX ˆY Ñ PpΘ ˆXq
X X ` ˘
pθ,x,π,yq ÞÑ ηP θupθ,x,yq,µ pθ,x,π,yq
ΘXˆX ρ
whereηP denotestheunitofthemonadP,andθu andµ aredefinedby
ρ
θupθ,x,yq :“ θ´λ η py,xqhpxqT
θ γpθq
µ pθ,x,π,yq :“ x`λ B hpxqTθTη py,xq´λ η pxq.
ρ ρ x γpθq ρ π
Here, λ ,λ : R are chosen learning rates, and the precision-weighted error terms η are again as in
θ ρ `
equation(6)(Remark4.16).
Remark 4.28. The ‘Hebbian’ part of the Hebb-Laplace doctrine enters in the forwards-parameter update
map,θupθ,x,yq “ θ´λ η py,xqhpxqT,sincethechangeinparametersisproportionaltosomethingre-
θ γpθq
semblingthecorrelationbetween‘pre-synaptic’and‘post-synaptic’activity. Here,thepost-synapticactivity
isrepresentedbythetermhpxq:wemaythinkofthecomponentsofthevectorxaseachrepresentingthe“in-
ternalactivity”ofasingleneuron,andthe“activationfunction”hasreturningthecorrespondingfiringrates;
theseare‘post-synaptic’asthefiringisemitteddownaneuron’saxon,whichoccurscomputationally‘after’
theneuron’ssynapticinputs. Thesynapticinputs(generatingthepre-synapticactivity)arethenthoughtto
berepresentedbytheerrortermη py,xq,sothatexpectedtrajectoryoftheouterproductη py,xqhpxqT
γpθq γpθq
computesthecorrelationbetweenpre-andpost-synapticacivity.
Note that this means that typically one assumes that λ ă λ , because the neural activity x itself must
θ ρ
changeonafastertimescalethanthesynapticweightsθ,inorderforθ tolearnthesecorrelations.
Giventheforegoingdefinition,weobtainthefollowingtheorem.
Theorem 4.29. The Hebb-Laplace doctrine H defines an identity-on-objects strict monoidal functor H ãÑ
G Ñ Hier N .
H GausspK(cid:96)pP Fdqq
ThistheoremfollowsinthesamewayasthecorrespondingresultfortheLaplacedoctrine;andsowebegin
with a small lemma, and subsequently show that the doctrine arises by stochastic gradient descent, before
puttingthepiecestogethertoprovethetheoremitself.
Lemma4.30. Thereisanidentity-on-objectsstrictmonoidalembeddingH ãÑ G .
H
Proofsketch. The proof proceeds much as the proof of Lemma 4.22, except that the forwards channels of
gamesintheimageoftheembeddingaregivenbytheparameterizedmorphismsofH.
Proposition 4.31. Given a Hebbian-Laplacian statistical game pγ,ρ,φq : pX,Xq Ý Θ ÝXÝ ˆ Ý X Ñ pY,Yq, Hpγq is
obtainedbystochasticgradientdescentofthelossfunctionφwithrespecttotheweightmatrixθ : Θ ofthe
X
channelγ andthemeanx : X oftheposteriorρ.
30
Proof. The proof proceeds much as the proof of Proposition 4.20, except now the forwards channel γ is pa-
rameterized: thisgivesusanotherfactoragainstwhichtoperformgradientdescent,andfurthermoremeans
thatγpθqmustbesubstitutedforγ inexpressionsinthederivationofµ .
ř ρ ` ˘
Thefirstsuchexpressionisthedefinitionofthelossfunctionφ : pθ,xq:ΘXˆX` Ctx γpθq, ˘ ρpxq Ñ R; we
willwriteφ pθ,xq forthecompo “ nentof ‰ φatpθ,xqwiththecorrespondingtypeCtx γpθq,ρpxq Ñ R. Wehave
φ pπ,kq “ E FLpyq ,wherenow
pθ,xq y„ π|γpθq|k
(cid:76) (cid:77) “ ‰
FLpyq “ ´logp py|xq´logp pxq´S ρpx,π,yq .
γpθq π X
Wefind
B FLpyq “ B E
x x pπ,γpθqq
“ ´B µ pxqTΣ pxq´1(cid:15) py,xq`Σ ´1(cid:15) pxq
x γpθq γpθq γpθq π π
“ ´B hpxqTθTη py,xq`η pxq
x γpθq π
and
B FLpyq “ B E
θ θ pπ,γpθqq
A E
B
“ ´ θ (cid:15) py,xq,Σ pxq´1(cid:15) py,xq
2 γpθq γpθq γpθq
A E
` ˘
B
“ ´ θ y´θhpxq,Σ pxq´1 y´θhpxq
2 γpθq
` ˘
“ Σ pxq´1 y´θhpxq hpxqT
γpθq
“ Σ pxq´1(cid:15) py,xqhpxqT
γpθq γpθq
“ η py,xqhpxqT .
γpθq
Consequently,wehave
µ pθ,x,π,yq “ x`λ B hpxqTθTη py,xq´λ η pxq
ρ ρ x γpθq ρ π
“ x´λ B FLpyq
ρ x
and
θupθ,x,yq “ θ´λ η py,xqhpxqT
θ γpθq
“ θ´λ B FLpyq,
θ θ
andthismeansthatwecanwrite
´ ¯
Hpγqupθ,x,π,yq “ ηP ˝ pθ,xq´pλ ,λ qB FLpyq
ΘXˆX
´
θ ρ
¯
pθ,xq
“ ηP ˝ p´λB FLpyq
ΘXˆX p
wherep :“ pθ,xqandλ :“ pλ ,λ q, whichestablishesthatHpγqu descendsthegradientofthe freeenergy
θ ρ
withrespecttotheparameterizationp.
Finally,withy sampledfromafixedcontext,wecanseethattheexpectedtrajectoryofHpγqfollows
´ ¯
E p´λB FLpyq
p
y„´π|γpθq|k ¯
(cid:76) (cid:77) “ ‰
“ p´λB E FLpyq
p
´ y„ π|γpθ¯q|k
(cid:76) (cid:77)
“ p´λB φ pπ,kq
p p
whichdemonstratesthatHpγqperformsstochasticgradientdescentofthelossfunction.
31
ProofofTheorem4.29. Lemma 4.30 gives us the first factor H ãÑ G , so we only need to establish that the
H
Hebb-Laplace doctrine obtains by pulling a functor G Ñ Hier N back along this inclusion.
H GausspK(cid:96)pP Fdqq
Wenowturntoestablishingthatstochasticgradientdescentreturnsthedesiredidentity-on-objectsfunctor
G Ñ Hier N .Proposition4.31showsthatHisobtainedbyapplyingstochasticgradientdescent
H GausspK(cid:96)pP Fdqq
tomorphismsinG ,soweneedtoshowthattheresultingmappingisfunctorial.
H
AsinthecaseofTheorem4.21,thestructuremorphismsarepreservedtrivially:theyhavetrivialparameter-
ization,andsostochasticgradientdescentreturnsthetrivialsystemsconstantlyemittingthecorresponding
lenses;inparticular,thismeansthatstochasticgradientdescentpreservesidentities.
We now show that, for composable games h and g, Hphq˝Hpgq “ Hph˝gq. This means demonstrating
equalities between state spaces, output maps, and update maps. As for Theorem 4.21, the state spaces are
givenbytheexternalparameterization, andtheparameterizationofthecompositegameh˝g andthestate
space of the composite system Hphq˝Hpgq are both given by taking the product of the factors, and so the
statespacesontheleft-andright-handsidesofthedesiredequationareequal.
The proof that the equality holds for output maps is also as in the proof of Theorem 4.21: the output of
a composite system is given by composing the output lenses of the factors, which is the same as the output
returned by H on a composite game, since outputs in the image of H are obtained by filling in the external
parameter.
` ˘
Wenowturntotheupdatemaps,forwhichweneedtoshowthat Hphq˝Hpgq u “ Hph˝gqu. Suppose
g :“ pγ,ρ,φq : pX,Xq Ñ pY,Yq and h :“ pσ,δ,ψq : pY,Yq Ñ pZ,Zq are Hebbian-Laplacian statistical
games; we will denote the corresponding parameters by pθ ,µ q and pθ ,µ q respectively. Following the
γ ρ δ σ
proofofTheorem4.21,wecanwritethelossfunctionofthecompositegamepσ,δ,ψq˝pγ,ρ,φqas
”
` ˘
E FL σpµ q ,δpθ q;γpθ q‚π ,z
z„ π|γpθγq|δpθ δq˚k
σ γpθγq‚πX δ γ X
ı
(cid:76) (cid:77) “ ` ˘‰
` E FL ρpµ q ,γpθ q;π ,y .
ρ πX γ X
y„σpµσqγpθγq‚πX pzq
(This expression is obtained by making the substitutions γ ÞÑ γpθ q and δ ÞÑ δpθ q in the corresponding
γ δ
expressionintheproofofTheorem4.21.)
Asbefore,z andπ aresuppliedbytheinputstothedynamicalsystem,andsoweobtainafunction
X
` ˘
f : pz,π ,θ ,µ ,θ ,µ q ÞÑ FL σpµ q ,δpθ q;γpθ q‚π ,z
X γ ρ δ σ σ γpθγq‚πX δ“ `γ X ˘‰
` E FL ρpµ q ,γpθ q;π ,y .
ρ πX γ X
y„σpµσqγpθγq‚πX pzq
If we write p :“ pθ ,µ q and q :“ pθ ,µ q, then pp,qq denotes the parameter for h˝g. Since H performs
γ ρ δ σ
stochasticgradientdescentwithrespecttotheparameterization,Hph˝gqu isthereforedefinedasreturning
thepointdistributiononpp,qq´λB fpz,π q,whereλ :“ pλ ,λ q,andλ “ pλ ,λ qandλ “ pλ ,λ q.
` pp,qq X p q p γ ρ q δ σ
WehaveB f “ B f,B fqandso
pp,qq p q
` ˘
pp,qq´λB fpz,π q “ p´λ B fpz,π q,q´λ B fpz,π q .
pp,qq X p p X q q X
Wemakesomeauxiliarydefinitions
` ˘
gupθ ,µ ,π,yq :“ pθ ,µ q´λ B FL ρpµ q ,γpθ q;π,y
γ ρ γ ρ p pθγ,µρq ` ρ π γ ˘
hupθ ,µ ,π1,zq :“ pθ ,µ q´λ B FL σpµ q ,δpθ q;π1,z
δ σ δ σ q pθ
δ
,µσq σ π1 δ
32
andfindthat
pp,qq´λB fpz,π q
pp,qq X
“ pθ ,µ ,θ ,µ q´λB fpz,π q
` γ ρ δ σ pθγ,µρ,θ δ ,µσq X ˘
“ pθ ,µ q´λ B fpz,π q,pθ ,µ q´λ B fpz,π q
˜ γ ρ p pθγ,µρq X δ σ q pθ δ ,µσq X ¸
“ ‰ ` ˘
“ E gupθ ,µ ,π ,yq , hu θ ,µ ,γpθ q‚π ,z
γ ρ X δ σ γ X
´
y„σpµσqγpθγq‚πX pzq
¯
` ˘
“ gupθ ,µ ,π q‚σpµ q pzq, hu θ ,µ ,γpθ q‚π ,z .
γ ρ X σ γpθγq‚πX δ σ γ X
WritingPQtodenotethecompositeparameterspaceΘ ˆXˆΘ ˆY,theforegoingcomputationdefines
X Y
Hph˝gqu : PQˆGausspXqˆZ Ñ GausspPQqas
´ ¯
` ˘
Hph˝gqupθ ,µ ,θ ,µ ,π,zq “ ηP gupθ ,µ ,π q‚σpµ q pzq, hu θ ,µ ,γpθ q‚π ,z . (11)
γ ρ δ σ PQ γ ρ X σ γpθγq‚πX δ σ γ X
` ˘
Theupdatemapofthecompositesystem Hphq˝Hpgq u isgivenbycomposingthedoublestrengthdst :
GausspPqˆGausspQq Ñ GausspP ˆQqafterthestringdiagram
X
Θ
X
GausspPq
Hpgqu
Y
Θ
Y
GausspXq
γ5
˚
σ5
GausspQq
Hphqu
Z
whereγ5 indicatestheuncurryingofthepushforwardsoftheparameterizedforwardschannelγ:
˚
` ˘
γ : Θ Ñ Gauss Parap‹q pX,Yq
X
embeds
ÞÝÝÝÝÑ Θ Ñ GausspK(cid:96)pPqqpX,Yq
X
` ˘
ÞÝ p Ý ´ Ý q Ñ˚ Θ Ñ E GausspK(cid:96)pPqqp1,Xq,GausspK(cid:96)pPqqp1,Yq
X
„
ÞÝÑ Θ Ñ EpGausspXq,GausspYqq
X
ÞÝ p Ý ´ ÝÑ q5 γ5 : Θ ˆGausspXq Ñ GausspYq.
˚ X
33
Next,notethatwecanwriteHpgqu andHphqu as
` ˘
Hpgqupθ ,µ ,π,yq “ ηP gupθ ,µ ,π,yq
γ ρ P` ` γ ρ ˘˘
Hphqupθ ,µ ,π1,zq “ ηP hu θ ,µ ,π1,z
δ σ Q δ σ
where P :“ Θ X ˆ X and Q :“ Θ Y ˆ Y, and that ` η P P Q “ dstpη ˘P P,η Q Pq. Reading the string diagram and
comparingwithequation(11),wethereforefindthat Hphq˝Hpgq u “ Hph˝gqu.
Finally, theproof thatHisstrictmonoidal ispreciselyanalogoustothe proofthatLisstrictmonoidal: H
is identity-on-objects and maps structure morphisms to structure morphisms, so that the associativity and
unitalityconditionsareimmediatelysatisfied.
5 References
[1] Toby St. Clere Smithe. “Compositional Active Inference I: Bayesian Lenses. Statistical Games”. In:
(09/09/2021).arXiv:2109.04461[math.ST].
[2] ThomasParr,GiovanniPezzulo,andKarlJ.Friston.ActiveInference.TheFreeEnergyPrincipleinMind,
Brain,andBehavior.MITPress,2022,p.288.isbn:9780262045353.
[3] A.M.Bastosetal.“Canonicalmicrocircuitsforpredictivecoding”.In:Neuron76.4(11/2012),pp.695–
711.doi:10.1016/j.neuron.2012.10.038.
[4] KarlFriston.“Afreeenergyprincipleforaparticularphysics”.In:(06/24/2019).arXiv:http://arxiv.org/abs/1906.10184v1[q-bio.NC].
[5] DavidISpivakandNelsonNiu.PolynomialFunctors:AGeneralTheoryofInteraction.2021.url:https://raw.githubusercontent.com/ToposInstitute/poly/main/Book-Poly.pdf.
[6] Toby St. Clere Smithe. “Polynomial Life: the Structure of Adaptive Systems”. In: Fourth International
ConferenceonAppliedCategoryTheory(ACT2021).Ed.byK.Kishida.Vol.EPTCS370.2021,pp.133–147.
doi:10.4204/EPTCS.370.28.
[7] DavidI.Spivak.“AreferenceforcategoricalstructuresonPoly”.In:(02/01/2022).arXiv:2202.00534[math.CT].
[8] Danel Ahman and Tarmo Uustalu. “Directed Containers as Categories”. In: EPTCS 207, 2016, pp. 89-98
(04/05/2016).doi:10.4204/EPTCS.207.5.arXiv:1604.01187[cs.LO].
[9] DavidI.Spivak.“Poly:Anabundantcategoricalsettingformode-dependentdynamics”.In:(05/05/2020).
arXiv:2005.01894[math.CT].
[10] BartJacobs.“Fromprobabilitymonadstocommutativeeffectuses”.In:JournalofLogicalandAlgebraic
MethodsinProgramming 94(01/2018),pp.200–237.doi:10.1016/j.jlamp.2016.11.006.
[11] Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Diagrams”. In: Math.
Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). doi: 10.1017/S0960129518000488. arXiv:
http://arxiv.org/abs/1709.00322v3[cs.AI].
[12] BryceClarkeetal.“Profunctoroptics,acategoricalupdate”.In:(01/21/2020).arXiv:2001.07488v1[cs.PL].
[13] Chris Heunen et al. “A Convenient Category for Higher-Order Probability Theory”. In: (01/10/2017).
doi:10.1109/lics.2017.8005137.arXiv:http://arxiv.org/abs/1701.02547[cs.PL].
34
[14] DavidI.Spivak.“Learners’languages”.In:(03/01/2021).arXiv:2103.01189[math.CT].
[15] TobyStClereSmithe.“Opendynamicalsystemsascoalgebrasforpolynomialfunctors,withapplication
topredictiveprocessing”.In:(06/08/2022).arXiv:2206.03868[math.CT].
[16] PietroVertechi.“DependentOptics”.In:(04/20/2022).arXiv:2204.09547[math.CT].
[17] DylanBraithwaiteetal.“Fibreoptics”.In:(12/21/2021).arXiv:2112.11145[math.CT].
[18] TobySt.ClereSmithe.“BayesianUpdatesComposeOptically”.In:(05/31/2020).arXiv:2006.01631v1[math.CT].
[19] GeoffreyS.H.Cruttwelletal.“CategoricalFoundationsofGradient-BasedLearning”.In:Programming
LanguagesandSystems.SpringerInternationalPublishing,2022,pp.1–28.doi:10.1007/978-3-030-99336-8_1.
[20] MatteoCapucci.“Diegeticrepresentationoffeedbackinopengames”.In:(06/24/2022).arXiv:2206.12338[cs.GT].
[21] MatteoCapuccietal.“Towardsfoundationsofcategoricalcybernetics”.In:(05/13/2021).arXiv:2105.06332[math.CT].
[22] TobySt.ClereSmithe.“CyberKittens,orSomeFirstStepsTowardsCategoricalCybernetics”.In:Pro-
ceedings3rdAnnualInternationalAppliedCategoryTheoryConference2020(ACT2020).2020.
[23] John M. Lee. Smooth Manifolds. New York, NY: Springer New York, 2012, pp. 1–31. isbn: 978-1-4419-
9982-5.doi:10.1007/978-1-4419-9982-5_1.url:https://doi.org/10.1007/978-1-4419-9982-5_1.
[24] RafalBogacz.“Atutorialonthefree-energyframeworkformodellingperceptionandlearning”.In:Jour-
nalofMathematicalPsychology76(02/2017),pp.198–211.doi:10.1016/j.jmp.2015.11.003.
[25] ChristopherLBuckleyetal.“Thefreeenergyprincipleforactionandperception:Amathematicalre-
view”.In:JournalofMathematicalPsychology81(05/24/2017),pp.55–79.arXiv:http://arxiv.org/abs/1705.09156v1[q-bio.NC].
[26] Matteo Capucci, Bruno Gavranović, and Toby St. Clere Smithe. “Parameterized Categories and Cate-
goriesbyProxy”.In:CategoryTheory2021.2021.
[27] K.Fristonetal.“VariationalfreeenergyandtheLaplaceapproximation”.In:Neuroimage34.1(01/2007),
pp.220–234.doi:10.1016/j.neuroimage.2006.08.035.
[28] MatteoCapucciandBrunoGavranović.“ActegoriesfortheWorkingAmthematician”.In:(03/30/2022).
arXiv:2203.16351[math.CT].
[29] DiederikP.Kingma.“VariationalInference&DeepLearning.ANewSynthesis”.PhDthesis.University
ofAmsterdam,2017.url:https://hdl.handle.net/11245.1/8e55e07f-e4be-458f-a929-2f9bc2d169e8.
[30] DanShiebler.“CategoricalStochasticProcessesandLikelihood”.In:Compositionality3,1(2021)(05/10/2020).
doi:10.32408/compositionality-3-1.arXiv:2005.04735[cs.AI].
[31] TobiasFritz.“AsyntheticapproachtoMarkovkernels,conditionalindependenceandtheoremsonsuffi-
cientstatistics”.In:(08/19/2019).arXiv:http://arxiv.org/abs/1908.07021v3[math.ST].
35

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Compositional Active Inference II: Polynomial Dynamics. Approximate Inference Doctrines"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
