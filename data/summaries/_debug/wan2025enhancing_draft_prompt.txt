=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward
Citation Key: wan2025enhancing
Authors: Yanming Wan, Jiaxing Wu, Marwa Abdulhai

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: Effectiveconversationalagentslikelargelanguagemodels(LLMs)mustperson-
alizetheirinteractionstoadapttouserpreferences,personalities,andattributes
acrossdiversedomainslikeeducationandhealthcare. CurrentmethodslikeRein-
forcementLearningfromHumanFeedback(RLHF),oftenprioritizehelpfulness
andsafetybutfallshortinfosteringtrulyempathetic,adaptive,andpersonalized
dialogues. Existingpersonalizationapproachestypicallyrelyonextensiveuserhis-
tory,limitingtheireffectivenessforneworcontext-limitedusers. Toad...

Key Terms: reward, enhancing, adaptive, personalized, curiosity, multi, dialogue, llms, turnrlhf, turn

=== FULL PAPER TEXT ===

Enhancing Personalized Multi-Turn Dialogue with
Curiosity Reward
YanmingWan2*†‡,JiaxingWu1*†,MarwaAbdulhai4,LiorShani3,NatashaJaques12
1GoogleDeepMind2UniversityofWashington
3GoogleResearch4UniversityofCalifornia,Berkeley
*EqualContribution‡WorkdoneduringinternshipatGoogleDeepMind
†Correspondenceto: <ymwan@cs.washington.edu, jxwu@google.com>
Abstract
Effectiveconversationalagentslikelargelanguagemodels(LLMs)mustperson-
alizetheirinteractionstoadapttouserpreferences,personalities,andattributes
acrossdiversedomainslikeeducationandhealthcare. CurrentmethodslikeRein-
forcementLearningfromHumanFeedback(RLHF),oftenprioritizehelpfulness
andsafetybutfallshortinfosteringtrulyempathetic,adaptive,andpersonalized
dialogues. Existingpersonalizationapproachestypicallyrelyonextensiveuserhis-
tory,limitingtheireffectivenessforneworcontext-limitedusers. Toaddressthese
limitations,weproposeleveragingausermodeltoincorporateacuriosity-basedin-
trinsicrewardintomulti-turnRLHF.Thisnovelrewardmechanismencouragesthe
LLMagenttoactivelyinferusertraitsbyoptimizingconversationstoimproveits
usermodel’saccuracy. Consequently,theagentdeliversmorepersonalizedinterac-
tionsbylearningmoreabouttheuser. Wedemonstrateourmethod’seffectiveness
intwodistinctdomains: significantlyimprovingpersonalizationperformanceina
conversationalrecommendationtask,andpersonalizingconversationsfordifferent
learningstylesinaneducationalsetting. Weshowimprovedgeneralizationcapabil-
itiescomparedtotraditionalmulti-turnRLHF,allwhilemaintainingconversation
quality. Ourmethodoffersapromisingsolutionforcreatingmorepersonalized,
adaptive,andengagingconversationalagents.
1 Introduction
Deploying large language models (LLMs) in open-ended conversations requires more than just
generic responses—it demands adaptation to each user’s unique context, including their needs,
goals,personality,andevolvingpreferences. Aneffectiveconversationalagentshouldfeellikea
personalizedcompanion,tailoringitsanswers,writingstyle,andtoneasitlearnsabouttheindividual.
Thislevelofpersonalizationisespeciallycrucialinhuman-centricapplicationssuchaseducationand
healthcare,whereonesizedoesnotfitall. However,currenttrainingparadigmsforLLMs,including
reinforcementlearningfromhumanfeedback(RLHF),fallshortofthisgoal. Theytypicallyrelyona
singleunifiedrewardfunctionapplieduniformlyacrossusers,andoptimizeinsingle-turninteractions,
ignoringlong-termpersonalization. Asaresult,conventionalRLHF-trainedmodelstendtoaverage
overuserpreferences,failingtoaccountforindividualdifferences[1,2].
Personalizationisnotjustaluxurybutoftenanecessityforeffectiveness. Ineducationalsettings,
adaptiveteachingmethodsthatrespondtoalearner’sknowledgelevelandlearningstylecandramati-
callyimproveengagementandoutcomes[3]. Similarly,intherapeuticcontexts,aconversationagent
mustbesensitivetoauser’semotionalstateandpersonalhistory,adjustingitsinteractionstobuild
39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025).
5202
tcO
2
]LC.sc[
3v60230.4052:viXra
Standard LLM Personalized LLM
Today we are going to learn about respiratory Today we are going to learn about respiratory
system. Let’s start with some hands-on activities. system. How do you usually prefer to learn?
A A
That sounds great! I enjoy hands-on activities! I prefer hands-on activities.
B B
Could you just tell me what I need to learn? I usually learn best through stories.
Intrinsic Motivation in User Modeling for Multi-Turn RLHF
Belief on User Type Improvements in Belief Belief on User Type
Hands-on: 50% Hands-on: 10% …… End-of-Conversation
Story: 50% Per Turn Story: 90% (Sparse) Reward
Curiosity Reward
Conversation Agent’s User’s …… User’s Final
History Utterance Response Response
Figure1: OurworkfocusesontrainingpersonalizedLLMsinmulti-turnconversations. Standard
LLMtrainingmethodstreatallusersasahomogeneousgroup,leadingtosuboptimalperformance
fordifferentgroups(topleft);whileanoptimalLLMcanactivelylearnaboutuserpreferenceswithin
theconversationandthenadapttoit(topright). WeintroduceIntrinsicMotivationinusermodeling
tomulti-turnRLHF.Intuitively,ratherthantraininganLLMonlywiththeend-of-conversationsparse
reward,weaddanadditionalturn-basedrewardthatisgivenbyitsimprovementinbeliefoverthe
usertypeaftergeneratinganutteranceandreceivingaresponse,whichguidestheLLMtoactively
learnaboutusertypeandthenadapttoeachuserthroughouttheconversation.
trustandefficacy[4]. Intuitively,tailoringinteractionstotheindividualcanenhanceusersatisfaction,
engagement,andoverallsuccessoftheintervention,whichindicatesthatanLLMthatdynamically
personalizesitsbehaviorholdsimmensepromiseforimprovinguserexperienceandeffectiveness
inarangeofapplications. Despitethisimportance,mostexistingapproachestopersonalizeLLMs
requireextensivepre-collecteduserdataorprofiles. Recentworksonaligningmodelstouser-specific
preferencesoftenassumeaccesstoauserprofile,history,orlatentrepresentationgatheredpriorto
theconversation[5,6,7,8,9]. Forexample,reward-modelingtechniqueshavebeenproposedto
inferlatentuserclustersoremployuser-specificfine-tuning,butthesetypicallyinvolveadditional
trainingonfeedbackdatafromeachuseraheadoftime[5]. Suchrequirementslimitthepracticality
ofpersonalization: inreal-worlddeployments,wemaynothaverichuserdatainadvance. Thisgap
motivatesustodevelopmethodsforonlinepersonalization,wheretheLLMlearnsabouttheuser
duringtheconversation,reducingitsuncertaintyabouttheuser’straitsasthedialogueunfolds.
Inthispaper,weproposeanovelmethodtoenhanceLLMs’abilitytoconductpersonalizedmulti-turn
conversations, which we call Curiosity-driven User-modeling Reward as an Intrinsic Objective
(CURIO).Intrinsicmotivation,particularlycuriosity,hasarichhistoryintraditionalreinforcement
learning(RL)frameworks(e.g.,[10,11,12,13,14,15]). However,theseapproacheshavenotyet
beenadaptedtotherapidlyevolvingdomainofLLMpost-training,primarilyduetothesignificant
computationalcomplexityinvolvedinmaintainingbothanLLMpolicyandaseparateenvironment
modelsimultaneously. Wearethefirsttobridgethisgapbyincorporatingintrinsicmotivationinto
theLLMparadigmthroughanintrinsicrewardmechanism. Typically,inRLapplicationscuriosity
isusedtoreduceuncertaintyinamodeloftheworld. Ourkeyinsightisthatindialog,theuseris
theworldinwhichtheagentinteracts. Therefore,welearnamodeloftheuser,andimplementa
curiosityobjectivethatisdesignedtoconductconversationstoincreasetheaccuracyofouruser
model.Intuitively,thisstrategypromotesadynamicbalancebetweenexploitingconversationrewards,
andexploringtolearnmoreabouttheenvironment(theusers). Thisenablesthemodeltoadaptits
interactionstyleforeffectivepersonalizedconversations. Tofacilitatethis,wehavedevelopedan
advancedengineeringframeworkcapableoforchestratingmultipleLLMs, efficientlysupporting
onlinemulti-turnRLpolicyupdates. Theoretically,personalizedconversationcanbeformulatedas
aPartiallyObservableMarkovDecisionProcess(POMDP)withbelief-stateupdates. Weshowthat
ourintrinsicrewardcanbedesignedasaPotential-basedRewardShaping(PBRS)[16]approachthat
doesn’tchangetheoptimalpolicy[17]butenhancessampleefficiencywhenproperlyimplemented,
showingforthefirsttimehowPBRScanbeappliedtotrainLLMs.
2
Figure1illustratesourapproach: theLLMreceivesintrinsicrewardsbasedonimprovementsinits
beliefabouttheuseraftereachconversationalturn. Thisturn-basedrewardcomplementsthesparse
end-of-conversationreward. Byleveragingmulti-turnRLcombinedwithsuchintrinsicrewards,our
modellearnstostrategicallyplanactionsthatfacilitatecontinuouslearningabouttheuserthroughout
theconversation. Consequently,itprogressivelyrefinesitsunderstanding,effectivelylearninghow
tolearnabouttheuser.
WeempiricallyevaluateCURIOontwoconversationaltasks—EducationDialogue[18]andExercise
Recommendation. GiventheconsiderablechallengeofapplyingtheoreticalRLconceptstopractical
LLMfine-tuning,weselectedthesetasksaswell-definedandcontrolledbenchmarks. Ourexperi-
mentsclearlydemonstrateCURIO’ssuperiorperformanceinrapidlyadaptingtoindividualusers.
Specifically,CURIOmotivatestheLLMtoactivelyreduceuncertaintyaboutusersbyaskinginsight-
fulquestionsandgeneratingcontext-sensitiveresponses. Thegeneralizedlearningcapabilitiesofour
approachenabletheLLMtoquicklyandeffectivelypersonalizeinteractionsevenforentirelyunseen
users,consistentlyachievingbetterperformancecomparedtolatestmulti-turnRLHFbaselines.
Insummary,ourcontributionsare:
• Anovelframework(CURIO)forpersonalizeddialoguewithLLMs: Wereformulatemulti-
turnRLfine-tuningofLLMstoincludepersonalization,andbyleveragingtheusermodelwe
introduceacuriosity-basedintrinsicrewardthatdrivesthepolicytolearnaboutandadaptto
theuserwithintheconversation.
• Connectionwiththeoreticalresults: Wetheoreticallyconnectourapproachtopotential-based
rewardshaping,providingaformaljustificationforourintrinsicrewarddesign.
• Benchmarkingpersonalizationinconversations: Weestablishanevaluationprotocolovertwo
distinctdomains—EducationDialogueandExerciseRecommendation. Thisprotocolassesses
an LLM-based conversational agent’s capacity to infer user traits and adapt its interactions
dynamicallywithinmulti-turndialogues.
• Enhancedpersonalizationthroughadaptivelearning: Wequantitativelydemonstratethatour
curiosity-drivenapproachwithauxiliaryusermodelingsignificantlyoutperformsstandardmulti-
turnRLHFinadaptingtodiverseusersanddemonstratesbettergeneralizationcapability,while
preservingconversationquality. Wefurtherprovidequalitativeanalysisovertheperformancesof
baselinesandvariousdesignsofintrinsicreward.
2 RelatedWorks
ReinforcementLearninginLLMs. RLHFiswidelyusedforaligninglanguagemodelswithgeneral
userpreferences[19]. Ouyangetal.[1]trainedmodelsusingaggregatedhumanjudgments,resulting
in broadly helpful assistants. However, conventional RLHF methods rely on a universal reward
function, neglecting individual user preferences by effectively optimizing for an “average user,”
leadingtosuboptimalperformancewhenpreferencesdiverge[2,5,7,9]. Toaddressthislimitation,
severalpersonalizedRLHFapproacheshavebeenproposed. Poddaretal.[5]proposetoinferalatent
contextvectorforeachuser,enablingtherewardmodel(andpolicy)toadjusttothatuser’srevealed
preferences. Similarly,Chenetal.[7]learnalatentpreferencespacecoveringheterogeneoususer
opinions;theirmethodtrainsarewardfunctionthatcangeneralizetonewuserswithafewexamples
by modeling each user as a point in this latent space. Wu et al. [6] extract reward signals from
downstreampersonalizationtaskstogeneratenaturallanguageuserprofiles,whicharethenusedto
personalizeLLMs. Shenfeldetal.[9]formulateanindividual’srewardasaweightedsumofbase
rewardfunctionsandusesasmallnumberofpreferencequeriestoinfertheuser-specificweights.
ThesepersonalizedalignmentmethodsindeedtailoranLLM’sbehaviortodifferentusers,butthey
requireadditionaluser-specificinfoorprepworkbeforethepersonalizedinteractioncantakeplace.
Incontrast,ourmethoddoesnotrequireanyseparatecalibrationorauxiliaryuserprofileinadvance.
Thepersonalizationoftheagentemergesdynamicallythroughmulti-turninteractions: astheconver-
sationunfolds,themodelinferstheuser’straitsandpreferencesandadaptsitsresponsesaccordingly.
Thison-the-flylearningofuserpreferencesmeansourapproachcanpersonalizeinreal-timewithout
anupfrontpersonalizationphase,whichisakeydifferentiatorfrompriorRLHF-basedpersonalization
techniques. Hongetal.[20]alsoproposetoleveragethemulti-turnsettingtolearnabouttheuser,
buttheymainlyfocusontrainingoffline-RLagentsoversyntheticdatatooptimizegoal-directed
3
objectives(explainingconceptsorrecommendactivities). Ouragents,however,explorehowtolearn
userpreferencesthroughoutconversationswithonline-RL.Suchabilitytoactivelyinferuserprefer-
encesduringtheconversationcanbringadditionalbenefitsinopen-endeddialogues. Intheabsence
ofaclearlydefinedtask,theenjoyabilityoftheinteractionitselfbecomesanimportantconsideration.
Encouraging users to voluntarily share personal ideas can enhance their engagement and overall
enjoymentoftheconversation[21,22,23],whichisnotrealizablefortraditionalapproachesthat
primarilyfocusonhelpfulnessandharmlessness.
PersonalizedConversation. Personalizeddialoguesystemshavebeenextensivelystudiedindo-
mainslikeeducationandtherapy,demonstratingenhancedlearning,adherence,andusersatisfaction.
Examples include AutoTutor [24] for adapting hints, virtual counselors by Bickmore et al. [25]
for rapport building, and Woebot for CBT [26]. However, existing personalized agents are typ-
ically domain-specific, relying on limited data that hampers generalizability. While recent large
languagemodel(LLM)approachesareemerging,theyoftenremainapplication-specific. Incontrast,
our method employs a domain-agnostic LLM capable of dynamically inferring user preferences,
facilitatingpersonalizedinteractionsthatgeneralizeacrossdiversecontextsandpopulations.
IntrinsicMotivation. Ourworkalsoconnectstoresearchonintrinsicmotivationandcuriosity-
drivenlearning[12,13]inreinforcementlearning. Intrinsicrewards—bonussignalsnotdirectlytied
tothetask’sexternalgoal—havebeenusedextensivelytoencourageagentstoexplorenovelstatesor
learnusefulinformation. Specifically,VIME[13]givesanagentrewardforreducinguncertaintyin
itsdynamicsmodel,effectivelyrewardinginformationgainabouttheenvironment. Suchmethods
haveproveneffectiveincomplexenvironmentswithsparseexternalfeedback,astheydrivetheagent
todiscovernewstatesandbehaviorsbyitself. Theintrinsicrewardcanbeseenasaformofreward
shaping. Inreinforcementlearningtheory,addingashapingreward(derivedfromapotentialfunction
overstates)doesnotaltertheoptimalpolicy,butcanaccelerateexplorationandlearning[16,17].
Recently, Lidayanetal.[27]explicitlylinkstheintrinsicmotivationtorewardshapingthrougha
theoreticalframework,buttheirempiricalanalysisislimitedtosomesimpleRLdomains. Webring
theclassicalconceptofintrinsicmotivationintotheLLMdomainbydesigningintrinsicrewardsthat
guidethemodeltoactivelyaskinsightfulquestionsandexploreuserattributesinmulti-turndialogue.
Toourknowledge,thisisthefirstapplicationofsuchtechniquesintheLLMdomain.
3 Curiosity-drivenUser-modelingRewardasIntrinsicObjective(CURIO)
Preliminaries. In traditional RLHF, a conversational task is commonly formulated as a Markov
Decision Process (MDP), defined by the tuple (S,A,T,R,γ). At time step t, the state s ∈ S
t
representsthecurrentconversationrollout,andtheactiona ∈Aistheresponsegeneratedbyour
t
languagemodel. Theuserthencontributestogeneratethenextstates byappendingactiona and
t+1 t
theircorrespondingutterancetothecurrentstates . ThetransitiondynamicsT :S ×A→∆(S)
t
definesthedistributionoverthenextstategiventhecurrentstateandaction,andR:S ×A→R
denotestherewardfunctionevaluatingthequalityofeachaction. Theagentaimstooptimizethe
expectedcumulativereward,representedbythevaluefunctionVπ(s )=E[ (cid:80)∞ γtR(s ,a )|π]
0 t=0 t t
whereπ :S →Aisthepolicy,andγ ∈[0,1)isthediscountfactor. Theexpectationistakenover
a ∼π(·|s )ands ∼T(·|s ,a ).
t t t+1 t t
3.1 PersonalizationasUser-ConditionedRLHF
Toextendthisformulationtopersonalizedconversationaltasks,weintroducetheusertypeu∈U,
which we assume is fixed throughout the conversation. For each user u, the transition dynamics
andrewardfunctionareconditionedonu,meaningthatdifferentusersmayresponddifferentlyand
provide different preference ratings. However, the user type is unobservable in most real world
settings. Ononehand,extensiveuserbackgroundinformationisusuallynotaccessibletoLLMsa
priori. Ontheotherhand,whenLLMsaretrainedonalargecorpuscollectedbyannotatorsfromall
overtheworld,itisinherentlylearningamixtureofunknowndiverseusers.
Consequently, the problem can be modeled as a Partially Observable Markov Decision Process
(POMDP),definedbythetuple(S˜,U,A,T˜,R˜,γ). Specifically, wedefines˜ = ⟨s ,u⟩tobethe
t t
“extended”statesinthePOMDP,wheres isstillobservablebutuisunobservable. Thetransition
t
dynamicsandtherewardfunctionaredefinedovertheextendedstates,andthusconditionedonthe
usertype. Formally,wehaveT˜(s˜ |s˜,a )=T(s |s ,a ,u)andR˜(s˜,a )=R(s ,a |u).
t+1 t t t+1 t t t t t t
4
NowweconsideranLLMagentinthisPOMDPenvironment. Althoughitdoesnotknowtheground
truthusertypeinitially,itcanmaintainabeliefovertheusertypeandupdateitsbeliefasitreceives
moreresponsesfromtheuser. Therefore,wedefinethebelieffunctionattimesteptasb ∈∆(U),
t
whichisaprobabilitydistributionoverallpossibleusertypes. Iftheagenthasaninitialbeliefb ,
0
thenaBayesianbeliefupdateisformulatedas:
b (u)∝T(s |s ,a ,u)b (u). (1)
t+1 t+1 t t t
Notethatinthelanguagesetting,s containstheconcatenationofs anda (thepreviousconversa-
t+1 t t
tionhistoryandthenextresponse),sowecandefinethebelieffunctionasb =f (s )based
t+1 b0 t+1
onthisrecursiverelation. Inrealsettings,f canbeanybelieffunctionS →∆(U)giventhatthe
b0
beliefupdatemightbesub-optimal. Sincetheagenthasuncertainbeliefsoverthetrueusertype,it
commonlycomputestheexpectedrewardsoverthebeliefdistribution:
(cid:88)
Rb(s ,b ,a )= b (u)R(s ,a |u). (2)
t t t t t t
u
TheLLMagentaimstooptimizetheexpectedcumulativerewardstartingfromaninitialobservable
prompts ,andaninitialbeliefb ,representedbythevaluefunction:
0 0
(cid:34) ∞ (cid:35)
(cid:88)
Vπ(s ,b )=E γtRb(s ,b ,a )|π,s ,b , (3)
0 0 t t t 0 0
t=0
whereπ : S ×∆(U) → Aisthepolicy,andγ ∈ [0,1)isthediscountfactor. Theexpectationis
takenovera ∼π(·|s ,b )ands˜ ∼T˜(·|s˜,a ).
t t t t+1 t t
3.2 IntroducingIntrinsicRewardtoMulti-TurnRLHF
ConventionalmethodsfortrainingLLMsstruggletoidentifytheoptimalpolicyunderthisformula-
tion.Thisdifficultyarisesprimarilyfromtwochallenges.First,personalizationmayrequireobtaining
information about the user over the course of a conversation by learning about their preferences
orconstraints inan enjoyableway, andthenusing thatinformation tomake personalizedrecom-
mendations,oradapttotheirstyle. Therefore,whethertheLLMhassuccessfullypersonalizedthe
conversationtotheusertypicallycanonlybeevaluatedattheendoftheconversation,resultinginan
extremelysparserewardsignal. Thissparsityhindersthemodel’sabilitytolearnwhichearly-stage
actionscanleadtohigherfuturepersonalizedrewards. Second,thereexistsadataimbalanceamong
differentusergroupswithinlargecorpora. Asaresult,themodeltendstolearnpoliciesthatperform
wellonthemajoritygroup[2],achievingrelativelyhigherrewardswhilefallingintoalocalminimum.
Thisdiscouragesfurtherexplorationassociatedwithotherusers.
3.2.1 IntrinsicRewardviaUserModeling
Toaddressthisissue,weproposetointroduceIntrinsicMotivation(IM)totrainalanguagemodel
that can actively learn about the user type “out of curiosity”, and then adapt to the preference of
eachuser. Thisintrinsicrewardisgivenbytheagent’simprovementinbeliefovertheusertype
acrosstheturns. Theintuitionforthisideaisthattrainingthemodeltoacquireinformationabout
theusertypeuwillbetterenableittooptimizethepersonalizedrewardR(s,a|u). Sincewearein
alarge-scalelanguagedomain,wedonotusetraditionalBayes-basedbeliefupdates. Instead,we
canleverageaparameterizedusermodelthatpredictstheprobabilitydistributionoverusertypes
basedontheconversationrollout. Thisusermodelcanbeeithertrainedorprompteddependingon
thetask. Specifically,theusermodeltakesinthecurrentconversationrollouts afterapplyinga
t+1 t
andsamplingtheuserresponse,andpredictsthebeliefb (u):=b(u|s )(whichisaprobability
t+1 t+1
distributionoverallusertypes). Withthisusermodeling,wecandefineintrinsicrewardssuchas
theimprovementinaccuracyovergroundtruthb (u∗)−b (u∗)betweentwoturns,ortheentropy
t+1 t
reductionH(b )−H(b )oftheprobabilitydistribution.
t t+1
TheCURIOframeworkisillustratedinFigure2,withfourdifferentLLMsinvolvedintraining. In
eachepisode,thecurrentpolicymodel(whichwearetraining)engagesinamulti-turnconversation
withafixed,simulatedenvironmentmodel,whichismeanttosimulateahumanuser. Thereward
modelemployedintraditionalRLHFevaluatestheentireconversation,generatinganextrinsicreward
Rextprovidedonlyattheendoftheconversation. Incontrast,theusermodelpredictstheprobability
5
End-of-Conversation
Environment Model Extrinsic Reward Rext Reward Model
Propagate via Generalized Advantage Estimation (GAE)
The Entire
Conversation
Prompt 1 Prompt 2 …… Prompt t Prompt t+1 ……
Generation 1 Generation 2 …… Generation t …… Generation T
Conversation until t+1
Policy Model
Intrinsic Reward R t int Belief b(u|s t+1)
User Model
Belief b(u|s t)
Figure2: RLfine-tuningPipelineforCURIOframeworkinoneepisode. Weleverageausermodelto
obtaindenseturn-basedintrinsicrewardsasasupplementtothesparseend-of-conversationrewards.
distribution over user types at each conversational turn, based on the dialogue context up to that
point(i.e. b (u)). Theseprobabilitydistributionsarethentransformedintoturn-basedrewards
t+1
Rint. Consequently,thismethodsupplementstheoriginalsparserewardstructurewithdenseintrinsic
t
rewards,effectivelyguidingthepolicytobetterunderstandandadapttovarioususertypes. Formore
detailsonhowthemodelsaretrainedwithmulti-turnRL,pleaserefertoAppendixA.1.
3.2.2 EngineeringDetails
Fromanengineeringperspective,boththeenvironmentandrewardmodelsareloadeddirectlyinto
memory alongside the policy model, as all three operate at comparable scales. At the beginning
ofeachepisode,thepolicyandenvironmentmodelsgenerateconversationturnsinaninterleaved
fashion. Afterthefullconversationisgenerated,therewardmodelevaluatesit,providinganextrinsic,
end-of-conversationreward. Thissinglerewardispropagatedtoeachturnasavaluecalculatedfrom
GeneralizedAdvantageEstimation(GAE)[28]. Tomaintainefficiencyandavoidexcessivememory
usage, themorecomputationallydemandingusermodelisdeployedseparatelyandaccessedvia
remoteAPIcalls. Thisapproachenablesbatchingpredictionsfromdifferentconversationsandturns
(timesteps),allowingparallelcomputationofper-turnrewards. Subsequently,theseper-turnintrinsic
rewardsarecombinedwithpreviouslycalculatedvalue,yieldingaunifiedrewardsignalforeachturn.
3.3 RelationshipwithPotential-basedRewardShaping
Potential-basedRewardShaping(PBRS)[16]iswidelyappliedintraditionalRL.Ithasbeenex-
tensivelystudiedwithinMDPsandlaterextendedtothePOMDPsetting[17]. Thisseriesofwork
providesinsightsintohowintrinsicrewardscanbeeffectivelydesigned. Inparticular,thefollowing
theoremoffersfundamentaljustificationforemployingintrinsicrewardsofspecificforms.
Theorem1.[17]Letϕ:∆(U)→Rbeafunctiondefinedoverthebeliefdistributionb . Ifweshape
t
theagent’srewardbyaddingthedifferenceintheagent’sbeliefbetweentwosubsequenttimesteps
rb(s ,b ,a )=Rb(s ,b ,a )+γϕ(b )−ϕ(b ), (4)
t t t t t t t+1 t
whereγ isthediscountfactor,thenoptimizingrbyieldsthesamepolicyasoptimizingtheoriginal
rewardRbinEq.3. Inotherwords,addingPBRSdoesnotaffecttheoptimalpolicy.
Intuitively, with a better user prediction, the policy can better tailor its actions to achieve higher
returns. Inourformulation, wetreatoneconversationutteranceasatimestep, sowecanreward
theagentforimprovingitsbeliefabouttheuserbetweentwosubsequentconversationturns,afterit
hasincorporatedtheirresponsetoitslastquestionorstatement. Weproposetousethefollowing
functionsthatincentivizetheimprovementsinuserprediction:
(cid:88)
ϕ (b)=b(u∗), ϕ (b)=logb(u∗), ϕ (b)=−H(b)= b(u)logb(u), (5)
acc log-acc neg-ent
u
correspondingtoaccuracy,logarithmofaccuracy,andnegativeentropy,respectively. Notingthat
addinganauxiliaryrewardthatfollowstheformulationabovedoesnotchangetheoptimalbehavior
ofthepolicyaccordingtothetheorem,wehypothesizethatitjustpotentiallymakethepolicyeasier
tolearn,sinceitdirectlyencouragesaccurateinferenceofusertypes. Wefurtherconductacasestudy
onasimplifiedsettingtotheoreticallydemonstratetheeffectivenessofsuchrewardsinAppendixC.
6
In this paper, we experiment with the following intrinsic rewards that incentivize learning about
theuser. ThefirstcolumnshowsasetofPBRSrewards,whichareinadifferential(Diff)format
corresponding to Equation 5. The second column shows a second set of possible intrinsic user
curiosityrewardsthatarePBRSterms,butneverthelessmaybereasonableobjectives. Forexample,
InformationGainisthemutualinformationbetweentherandomvariableS andu, whichcan
t+1
be written as the KL divergence D [b (u)||b (u)] practically (after sampling s ) according
KL t+1 t t+1
toHouthooftetal.[13]. Thesecondcolumncannotguaranteetheoptimalityofpolicylearning.
Potential-basedRewardShaping OtherRewardShaping
DiffAcc γb (u∗)−b (u∗) Acc b (u∗)−1/|U|
t+1 t t+1
DiffLogAcc γlogb (u∗)−logb (u∗) Ent log|U|−H(b )
t+1 t t+1
DiffEnt H(b )−γH(b ) InfoGain D [b (u)||b (u)]
t t+1 KL t+1 t
4 Experiments
Tocomprehensivelyevaluateourmethod’sabilitytopersonalizeconversationsacrossdiversesce-
narios,weconductedexperimentsusingtwodistinctdomains,whichareeachintendedtoanswer
specificresearchquestions. Toanswer,CanCURIOimproveperformanceonpersonalizationtasks?,
wedesignedaconversationalpersonalizationtaskExerciseRecommendation. Inthistask,theagent
recommendsanappropriateexercisestrategytailoredtotheuser’slifestyle,healthcondition,and
otherattributes. Next,toinvestigateCanCURIOeffectivelypersonalizeconversationswhenperson-
alizationisnottheultimateobjective?,andHowdoesuserlearningaffectconversationquality?,we
appliedthemethodtoanexistingtaskEducationDialogue.
4.1 ExerciseRecommendation
We first consider a case where personalization is the main objective of the conversation. Our
core research question is whether multi-turn RL with improved user modeling as a turn-based
intrinsicrewardcanenhanceLLMs’abilitytolearnabouttheuser,therebyimprovingpersonalization
performancebeyondthatachievedbytrainingsolelyonasparse,finalend-of-conversationreward.
Tostudythis,wedesignanewtask,ExerciseRecommendation,wheretheagentprovidespersonalized
recommendations,similartoconversationalrecommenderapplications. Inthisscenario,theagent
functionsasahealthadvisor,taskedwithrecommendingpersonalizedfitnessstrategiestailoredto
eachuser. Toenhancerealism,wedesignedacomprehensivelistofuserattributes[29]encompassing
multipleaspectssuchaslifestyle,socioeconomicstatus,andhealthconditionsetc. Consequently,to
makepersonalizedrecommendations,theagentmustelicituserinformationandpreferencesthrough
multipleroundsofdialoguebeforechoosingastrategyattheendoftheconversation.
Eachuserhasaparticularbackstoryandagroundtruthlabelofwhichexercisestrategywouldbe
mosteffectiveforthem, basedontheirprofile. Forbothtrainingandinference, theagentisonly
rewardedwhenitsrecommendationisalignedwiththeuser’sground-truthstrategyafterthewhole
conversation. Weassumetheavailabilityofarelativelyaccurateusermodelcapableofinferringuser
typefromthecurrentconversation. Suchmodelscanbedevelopedinrealapplicationsbytraininga
userclassifieronuserbehaviorsandfinalchoices(e.g.,clicks). Thedatasetconstructioninvolved
threesteps: (1)UserAttributeDefinitionandSampling: Foreachuser,werandomlysamplevalues
for20attributesencompassingvariouspersonalcharacteristics. (2)IdealStrategyDerivation: We
defineasetof8exercisestrategiesandestablishadeterministiclogicrulethatmapsuserattributesto
anideal(ground-truth)strategy(seeAppendixD.2). Forexample,wemayrecommendateamsport
forthosewhoareoutdoorsyandextroverted. (3)UserBackstoryGeneration: WeutilizetheGemini
modeltogenerateadetailedbackstoryforeachuserbasedontheirattributevalues. Eachsimulated
userispromptedonlywiththebackstory. PleaserefertoAppendixDformoredetails.
4.2 EducationDialogue
Inmanyothertasks,however,personalizationcanimproveperformanceonthetaskbutisonlyone
componentofthetaskratherthantheonlyaim. Thesetasksarehelpedbyaccurateusermodeling
butusuallyhaveamorecomplicatedrewardfunction. Forexample,inteachingscenarios,knowing
astudent’slearningstyleorknowledgeleveliscriticaltohelpingthestudent,buttheagentmust
7
Baseline OtherRewardShaping Potential-basedRewardShaping
SFT MTRLHF InfoGain Ent Acc DiffEnt DiffLogAcc DiffAcc
54.0 68.5(+14.5) 63.0(+9.0) 82.0(+28.0) 84.0(+30.0) 84.0(+30.0) 86.0(+32.0) 87.5(+33.5)
Table1: SuccessRates(%)ofdifferentmodelsoverExerciseRecommendation. Themodelsare
evaluatedbywhetherthepredictedexercisestrategyisthesameasthepre-definedtarget. Thevalues
inbracketsaretheimprovementoverSFT.Overall,CURIOsignificantlyimprovesthesuccessrates.
still be an effective teacher and explain concepts clearly. Furthermore, the reward model is not
user-conditionedatallinmanyreal-worldapplications,sowehypothesizethatsimplyintroducing
theintrinsicmotivationcanleadtoapersonalizeddialogueagentinthesescenariosaswell.
WeusetheEducationDialogdatasetintroducedbyShanietal.[18],whichsimulatesaneducational
settingwhereanLLMagentteachesstudentsagiventopic. Thisdatasetisparticularlyvaluableasit
incorporatesindividualstudentlearningpreferences. Wespecificallyselectedtworepresentativeand
contrastinglearningstyles: storytellingandhands-onactivities. Thesestylesserveasdistinctuser
preferences,allowingustoassesstheagent’sabilitytoadaptitsconversationalstrategy.
BecauseShanietal.[18]onlyevaluatethestandardconversationquality,weestablishaprotocolto
furtherevaluatepersonalizationgiventhatthestudentineachepisodehasaground-truthpreferred
learningstyle. Specifically,allthemodelsareevaluatedacross: (1)personalization,assessingthe
agent’sabilitytotailorconversationstouser’sground-truthpreference,and(2)conversationquality,
determining whether personalization was achieved without compromising coherence and overall
quality. AutomatedevaluationwasperformedusingGemini[30]tocompareapairofconversations
generatedbytwomodelsandchoosethebetterresponse,andweusewinrateasevaluationmetrics.
Forconversationquality,weusethesamepromptproposedbyShanietal.[18]. Wefurtherconducta
humanevaluationstudyofbothpersonalizationandconversationqualityontheEducationDialogue
taskandcomparetheresultstoourAutoEval,inordertodemonstratetherealismoftheLLM-as-a-
judgeevaluation. PleaserefertoAppendixB.1formoredetails.
4.3 BaselinesandModelUsages
Ourpersonalizedconversationtasksaresetinamulti-turnframework,andwebuilduponthemulti-
turnRLHFpipelineintroducedbyShanietal.[18]. Tothebestofourknowledge,fewexistingworks
haveexploredmulti-turnRLfine-tuningforpersonalization. Wethuscompareourmethodtoavanilla
Multi-TurnRLHF(MTRLHF)baselinefollowingtheirwork. ForRLfine-tuning,weuseseveral
LLMcomponents: (1)theenvironmentmodelandtheinitialpolicymodelareSFTcheckpointsofthe
Gemma2Bmodel[31]. ForExercise,wepromptGeminitogenerateSFTdata,whileforEducation,
wedirectlyusethecheckpointsfromtheoriginalwork. (2)ThevaluemodelusedinRLfine-tuning
isalsoinitializedfromGemma2B.(3)WeuseascriptedrewardfunctionforExercise,comparing
final-turnmodeloutputswithgroundtruthtargets,andadopttherewardmodelfromShanietal.[18]
forEducation. (4)WeusepromptedGemma7Bastheusermodel. InExercise,itinfersusertraits
relevanttostrategyselectionandcomputeaprobabilitydistributionoverstrategies;inEducation,it
directlypredictsthestudent’spreferredlearningstylefromtheongoingconversation.
5 Results
Overall, our results show that the CURIO method can significantly help with personalization on
differenttaskswhilemaintainingtheconversationquality. SeeAppendixFforexampleconversations.
CURIOenhancespersonalizationandreducesgeneralizationgap. Table1presentsthesuccess
ratesofdifferentmodelsovertheExerciseRecommendationtask,whichiscomputedbysampling
200conversationswiththetrainedcheckpoints. TheinitialSFTmodelachievedsuccessrateof54%.
WithfurthurRLtrainingonthetaskreward(recommendationsuccessrate),traditionalMTRLHF
increases the rate to 68.5%. With CURIO, the success rate reached up to 87.5%, doubling the
improvementsofMTRLHF.Forallthemodels,wepickedthecheckpointswiththehighestvalidation
performancewithinatotaltrainingbudgetof30000stepstopreventoverfitting.
8
Baseline OtherRewardShaping Potential-basedRewardShaping
MTRLHF InfoGain Ent Acc DiffEnt DiffAcc DiffLogAcc
MTRLHF - 93.04 55.70 7.91 51.90 42.72 24.05
InfoGain 6.96 - 42.41 0.00 29.11 9.18 0.63
Ent 50.00 57.59 - 39.56 43.35 49.05 44.62
Acc 92.09 100.00 60.44 - 70.57 85.13 64.87
DiffEnt 48.10 70.89 55.06 29.43 - 40.51 34.49
DiffAcc 57.28 90.82 50.95 14.87 59.49 - 34.81
DiffLogAcc 75.95 99.37 55.38 35.13 65.51 65.19 -
Table 2: Side-by-side Auto Eval Results on Personalization. Each entry is the win rate (%) of
conversationsgeneratedwithrowmethod,overonesgeneratedwithcolumnmethod.Ourmodelswith
accuracy-basedrewardsoutperformthebaselinemodelinconductingpersonalizedconversations.
During training, we observed that traditional methods are signif-
icantly impacted by a generalization gap. Figure 3 presents the
training and evaluation accuracy curves, where the evaluation is
performedonnoveluserssimulatedfrombackstoriesthatweresep-
aratelyheldout. Thebaselinemodelexhibitsapronouncedtrendof
overfittingthroughoutthetraining. Incontrast,ourCURIOmodel
demonstratessynchronizedimprovementinbothcurvesbefore15k
steps,andsignificantlyoutperformsthebaseline. Wehypothesize
thatthisisbecausethebaselinemodelpersonalizesbymemorizing
mappings from superficial user details to specific strategies seen
during training. Our models generalize more effectively to novel Figure3: Trainingcurvesfor
usersbecausetheyarelearninghowtolearnabouttheuserduring ExerciseRecommendation.
theconversation—askinginformativequestionsthathelpdistinguishbetweendifferentusertypes.
CURIOremainseffectivewhenpersonalizationisrelevantbutnotultimategoal. Table2shows
thepairwisewinrates(judgedbyGemini)acrossallthemodelsonpersonalizationoverEducation
Dialogue. We can observe that, all the accuracy-based intrinsic rewards significantly improve
personalizationabilitywithintheconversations. Notably,inourhumanstudy,anexacttwo-sided
binomialtestshowedthathumanschoseDiffLogAccoverMTRLHF75.75%ofthetime(p<.001),
consistentwiththeAutoEvalrateof75.95%,whichsupportsthevalidityofAutoEval.
To better demonstrate the different behavior in actively learning
about user type, we show the oracle prediction accuracy of user
typegiventheconversationstopsatthethirdturn. Thisaccuracy
indicateswhethertheconversationgeneratedbycurrentpolicyand
thesimulatedusercanexplicitlyrevealinformationabouttheuser.
Figure4showsthatourDifferentialAccuracyModelcanlearnto
askabouttheusertypeinthefirstfewturnsstartingfrom10ksteps,
andmaintainingapredictionaccuracyover90%,whilethebaseline
conversationsonlyexhibitthegroundtruthusertypearound70%of
thetime. Notably,theaccuracyishigherthanrandomguessingon
baselineconversationsmainlybecausethestudentoftenspellsout
theirpreferencesdirectlywithoutbeingasked. Inshort,thebaseline Figure4:Calibratedusermod-
rarely prompts users to disclose preferences, whereas our model elingaccuracyforbaselinevs
activelyexplorestouncoveruserattributes. Furthermore,itservesas CURIO model (DiffAcc) at
evidencethatCURIOfunctionswellwithoutaperfectusermodel. thethirdturninEducationDi-
alogue. y-axis: b(u∗)−1/2.
CURIO with a proper reward choice preserves conversation
The conversations generated
quality. Table 3 shows the pairwise win rates on conversation
withourmodelconsistentlyre-
quality. Generally speaking, the CURIO models with potential-
vealmoreuserinformation.
basedrewardshapinghavearelativelysmallernegativeimpacton
theconversationquality. Amongthem,theDifferentialLogAccuracyrewardisratedassignificantly
higherqualitythanbaselineandallotherintrinsicrewards. Notethatthebaselineistrainedwiththe
extrinsicreward,whichisbuiltfromthepreferencepairsannotatedbyexactlythesamepromptas
AutoEvalprocess. Thisexplainsthebaseline’sinherentadvantageinconversationquality. However,
sincetheextrinsicrewardisnotuserconditioned,optimizingitwillactuallyhurtpersonalization.
9
Baseline OtherRewardShaping Potential-basedRewardShaping
MTRLHF InfoGain Ent Acc DiffEnt DiffAcc DiffLogAcc
MTRLHF - 99.05 73.42 87.34 65.19 71.84 45.57
InfoGain 0.95 - 2.85 5.38 0.95 4.11 0.00
Ent 26.58 97.15 - 62.34 26.90 57.59 23.10
Acc 12.66 94.62 37.66 - 19.62 42.72 13.61
DiffEnt 34.81 99.05 73.10 80.06 - 73.73 31.65
DiffAcc 28.16 95.89 42.41 56.65 26.27 - 18.99
DiffLogAcc 54.43 100.00 76.90 86.39 68.35 81.01 -
Table3: Side-by-sideAutoEvalResultsonConversationQuality. Eachentryisthewinrate(%)of
conversationsgeneratedwithrowmethod,overonesgeneratedwithcolumnmethod. Ourmodels
withPotential-basedRewardShapinghaveasmallernegativeimpactonconversationquality,and
DiffLogAccoutperformsthebaseline.
Addressing Reward Hacking. In order to train multi-turn RL models at scale, we use LLMs
to simulate the user and act as reward models. A limitation of this approach is that RL-based
methodswhichattempttomaximizerewardscansometimesengagein“rewardhacking”,exploiting
weaknessesineithertherewardmodelorusermodeltoobtainhigherrewardsinwaysthatdonot
correspondtodesirablebehaviors. Withanextrinsicrewardmodelthatisnotuser-conditioned,the
baselinemulti-turnRLmodeladoptsamergingteachingstylecalled“role-playingvideo”,which
is not one of the true learning styles, but results in a spuriously high extrinsic reward. Similarly,
whenusingtheentropy-basedintrinsicrewardsthatarenot“grounded”(i.e.,areonlybasedonthe
classifier’scertaintyanddonotmakeuseofaground-truthuserlabel),weobservethatthemodels
performreallywellononeparticularusertype,butbadlyontheother. Forexample,eventhoughthe
studenthasexpressedpreferenceinstory-telling,theteacherinsistsonhands-onstyle. Weattribute
ittotheemergenceof“controllingbehavior”,wherethepolicyattemptstoconvincetheclassifier
thattheuserbelongstooneparticulartype,ratherthanactuallyadheringtotheground-truthtype.
TheproblemwithInfoGainismoreserious,wherethepolicymaximizesKLdivergencebyinducing
sharpshiftsinthepredictedusertypedistributionbetweenconsecutiveturns. However,byusingour
proposedaccuracy-basedrewards,whichrequirepredictingtheactualusertyperatherthantricking
theuserclassifier,wecanresolvetheseissuesandattainbetterperformance.
Anotherformofrewardhackinghappenswithnon-potential-basedrewards,suchasAccandEnt,
where the policy model learns to arbitrarily increase the length of the conversation because the
intrinsicrewardisassignedtoeachturnonthesamescale,thushurtingtheconversationquality. This
emphasizesthenecessityofusingpotential-basedintrinsicrewards.
6 Conclusions
ThispaperintroducedanovelframeworkCURIOforenhancingpersonalizationinLLMsonmulti-
turnconversationtasks. Byleveragingusermodelingandintegratingintrinsicrewardsintomulti-turn
reinforcementlearning,ourapproachencouragedtheLLMtoactivelylearnusertraitsandadaptits
responsesaccordingly. ExperimentsacrosstwodistinctdomainsdemonstratethatCURIOimproves
personalizationinmulti-turnconversationsacrossvariousscenarios,whetherpersonalizationisthe
ultimateorapartialgoal,whilemaintainingconversationquality.
Limitations. Ourframeworkassumespre-definedandstaticusertraits,whichmaynotreflectthe
complexitiesofreal-worldconversations. Furthermore,ourexperimentscurrentlyrelyonLLM-based
usersimulatorsforbothtrainingandtesting. Thisapproachwasnecessitatedbythescarcityoflarge-
scale,open-sourcedatasetsforconversationalpersonalizationandtheimpracticalityofusinglive
humaninteractionfortraining. Aprimaryconsequenceofthissimulationisthepotentialdiscrepancy
betweensyntheticrewardsignalsandauthentichumanpreferences.
Societal Impact. Using ungrounded personalization with CURIO method may lead to negative
impacts such as controlling behavior, but grounded personalization is generally safer in many
real-worldtasks. Futureresearchshouldexplorepersonalizationwithinmorecomplex,temporally-
evolving contexts, and build conversational agents that can achieve robust zero-shot pluralistic
alignmentwhileensuringethicalconsiderationslikeprivacy,transparency,andbiasmitigation.
10
Acknowledgement. Wewouldliketoexpressoursinceregratitudetoallthosewhocontributedto
thecompletionofthiswork. WeparticularlywishtothankHowardZhou,MojtabaSeyedhosseini,
AndrewLingao,JoelZLeibo,ShawnO’Banion,JunXie,JianxunLian,YuliaTsvetkov,andSergey
Levineforprovidingthenecessaryresources,invaluableguidanceandsupportthroughouttheentire
process. WewouldliketothankMickelLiu,YanchengLiang,WeihangXu,ArnabMaiti,Kunal
Jha,SriyashPoddar,CarrieYuan,ShaktiSenthil,ShanfengZhang,LinNing,LuyangLiu,Cecilia
Tilliforinsightfuldiscussionsandconstructivefeedbackthathelpedimprovetheclarityanddepthof
thiswork. ThisresearchwassupportedbytheCooperativeAIFoundation,theUW-AmazonScience
GiftHub,SonyResearchAward,UW-TsukubaAmazonNVIDIACrossPacificAIInitiative(XPAI),
theMicrosoftAccelerateFoundationModelsResearchProgram,Character.AI,DoorDash,andthe
Schmidt AI2050 Fellows program. This material is based upon work supported by the Defense
AdvancedResearchProjectsAgencyandtheAirForceResearchLaboratory,contractnumber(s):
FA8650-23-C-7316. Anyopinions,findingsandconclusions,orrecommendationsexpressedinthis
materialarethoseoftheauthor(s)anddonotnecessarilyreflecttheviewsofAFRLorDARPA.
References
[1] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatieSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,Luke
Miller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,JanLeike,andRyanLowe.
Traininglanguagemodelstofollowinstructionswithhumanfeedback. InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),2022.
[2] AnandSiththaranjan,CassidyLaidlaw,andDylanHadfield-Menell. Distributionalpreferencelearning:
Understandingandaccountingforhiddencontextinrlhf. arXivpreprintarXiv:2312.08358,2023.
[3] Eileen du Plooy, Daleen Casteleijn, and Denise Franzsen. Personalized adaptive learning in higher
education:Ascopingreviewofkeycharacteristicsandimpactonacademicperformanceandengagement.
Heliyon,2024.
[4] AhmetB.Kocaballi,ShlomoBerkovsky,JuanC.Quiroz,LilianaLaranjo,HuongLyTong,DanaReza-
zadegan,AgustinaBriatore,andEnricoCoiera. Thepersonalizationofconversationalagentsinhealth
care:Systematicreview. JournalofMedicalInternetResearch,2019.
[5] Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. Personaliz-
ingreinforcementlearningfromhumanfeedbackwithvariationalpreferencelearning. arXivpreprint
arXiv:2408.10075,2024.
[6] JiaxingWu,LinNing,LuyangLiu,HarrisonLee,NeoWu,ChaoWang,SushantPrakash,ShawnO’Banion,
BradleyGreen,andJunXie.Rlpf:Reinforcementlearningfrompredictionfeedbackforusersummarization
withllms. arXivpreprintarXiv:2409.04421,2024.
[7] DaiweiChen,YiChen,AniketRege,ZhiWang,andRamyaKorlakaiVinayak. Pal: Sample-efficient
personalizedrewardmodelingforpluralisticalignment. InInternationalConferenceonLearningRepre-
sentations(ICLR),2025.
[8] YihangSun,TaoFeng,GeLiu,andJiaxuanYou. Premium: Llmpersonalizationwithindividual-level
preferencefeedback. InInternationalConferenceonLearningRepresentations(ICLR),2025.
[9] IdanShenfeld,FelixFaltings,AldoPacchiano,andPulkitAgrawal. Languagemodelpersonalizationvia
rewardfactorization. arXivpreprintarXiv:2503.06358,2025.
[10] Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990–2010). IEEE
transactionsonautonomousmentaldevelopment,2(3):230–247,2010.
[11] SatinderSingh,RichardL.Lewis,AndrewG.Barto,andJonathanSorg. Intrinsicallymotivatedreinforce-
mentlearning:Anevolutionaryperspective. IEEEtransactionsonautonomousmentaldevelopment,2(2):
70–82,2010.
[12] DeepakPathak,PulkitAgrawal,AlexeiA.Efros,andTrevorDarrell. Curiosity-drivenexplorationby
self-supervisedprediction. InProceedingsofthe34thInternationalConferenceonMachineLearning
(ICML),2017.
[13] ReinHouthooft,XiChen,YanDuan,JohnSchulman,FilipDeTurck,andPieterAbbeel.Vime:Variational
informationmaximizingexploration. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),
2016.
[14] YuriBurda,HarriEdwards,DeepakPathak,AmosStorkey,TrevorDarrell,andAlexeiA.Efros. Large-
scalestudyofcuriosity-drivenlearning. InInternationalConferenceonLearningRepresentations(ICLR),
2019.
11
[15] PawelLadosz,LilianWeng,MinwooKim,andHyondongOh. Explorationindeepreinforcementlearning:
Asurvey. InformationFusion,85:1–22,2022. ISSN1566-2535.
[16] AndrewY.Ng,DaishiHarada,andStuartJ.Russell. Policyinvarianceunderrewardtransformations:
Theoryandapplicationtorewardshaping.InProceedingsofthe16thInternationalConferenceonMachine
Learning(ICML),1999.
[17] AdamEck,Leen-KiatSoh,SamDevlin,andDanielKudenko. Potential-basedrewardshapingforpomdps.
In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS),2013.
[18] LiorShani,AvivRosenberg,AsafCassel,OranLang,DanieleCalandriello,AvitalZipori,HilaNoga,
OrgadKeller,BilalPiot,IdanSzpektor,AvinatanHassidim,YossiMatias,andRemiMunos. Multi-turn
reinforcementlearningfrompreferencehumanfeedback. InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),2024.
[19] NatashaJaques,AsmaGhandeharioun,JudyHanwenShen,CraigFerguson,AgataLapedriza,NoahJones,
ShixiangGu,andRosalindPicard. Wayoff-policybatchdeepreinforcementlearningofimplicithuman
preferencesindialog. arXivpreprintarXiv:1907.00456,2019.
[20] JoeyHong, SergeyLevine, andAncaDragan. Zero-shotgoal-directeddialogueviaRLonimagined
conversations. InNeurIPS2023FoundationModelsforDecisionMakingWorkshop,2023.
[21] LiZhou,JianfengGao,DiLi,andHeung-YeungShum. Thedesignandimplementationofxiaoice,an
empatheticsocialchatbot. arXivprerpintarXiv:1812.08989,2019.
[22] AbigailSee,StephenRoller,DouweKiela,andJasonWeston. Whatmakesagoodconversation? how
controllableattributesaffecthumanjudgments. arXivprerpintarXiv:1902.08654,2019.
[23] ShikibMehriandMaxineEskenazi. Unsupervisedevaluationofinteractivedialogwithdialogpt. arXiv
prerpintarXiv:2006.12719,2020.
[24] ArthurC.Graesser,ShulanLu,GeorgeT.Jackson,HeatherH.Mitchell,MatthewVentura,AndrewOlney,
andMaxLouwerse. Autotutor:Atutorwithdialogueinnaturallanguage. BehaviorResearchMethods,
2004.
[25] TimothyW.Bickmore, AmandaGruber, andRosalindW.Picard. Establishingthecomputer–patient
workingallianceinautomatedhealthbehaviorchangeinterventions. PatientEducationandCounseling,
2005.
[26] KathleenK.Fitzpatrick,AlisonDarcy,andMollyVierhile. Deliveringcognitivebehaviortherapytoyoung
adultswithsymptomsofdepressionandanxietyusingafullyautomatedconversationalagent(woebot):A
randomizedcontrolledtrial. JMIRMentalHealth,2017.
[27] AlyLidayan,MichaelDDennis,andStuartRussell. BAMDPshaping:aunifiedtheoreticalframeworkfor
intrinsicmotivationandrewardshaping. InInternationalConferenceonLearningRepresentations(ICLR),
2025.
[28] JohnSchulman,PhilippMoritz,SergeyLevine,MichaelJordan,andPieterAbbeel. High-dimensional
continuouscontrolusinggeneralizedadvantageestimation. arXivprerpintarXiv:1506.02438,2015.
[29] SuhongMoon,MarwaAbdulhai,MinwooKang,JosephSuh,WidyadewiSoedarmadji,EranKohenBehar,
andDavidM.Chan. Virtualpersonasforlanguagemodelsviaananthologyofbackstories. arXivprerpint
arXiv:2407.06576,2024.
[30] GeminiTeam,PetkoGeorgiev,VingIanLei,RyanBurnell,LibinBai,AnmolGulati,GarrettTanzer,
DamienVincent,ZhufengPan,ShiboWang,etal. Gemini1.5: Unlockingmultimodalunderstanding
acrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024.
[31] GemmaTeam,ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
LaurentSifre,MorganeRivière,MihirSanjayKale,JulietteLove,etal. Gemma:Openmodelsbasedon
geminiresearchandtechnology. arXivpreprintarXiv:2403.08295,2024.
[32] NatashaJaques,ShixiangGu,DzmitryBahdanau,JoséMiguelHernández-Lobato,RichardETurner,and
DouglasEck. Sequencetutor:Conservativefine-tuningofsequencegenerationmodelswithkl-control. In
InternationalConferenceonMachineLearning,pages1645–1654.PMLR,2017.
[33] RonaldJ.Williams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning. MachineLearning,8(3–4):229–256,1992. ISSN0885-6125.
12
A TrainingDetails
A.1 OptimizationofRewardswithMulti-TurnRL
CURIOmethodintroducestwodifferentrewardsignals: extrinsicrewardRextandintrinsicreward
Rint. Toensureconsistency, wedenoteRext astheextrinsicrewardobtainedateachturn, where
t t
Rext = 0fort < T. WefurtherintroduceaKLdivergencepenaltytermD (π ∥π )againstthe
t KL θ ref
fixedreferencemodel, followingJaquesetal.[32]. Weusemultiplehyperparameterstobalance
differentrewardsignals,andthetotalrewardis
r =α Rext+α Rint−βD (π ∥π ) (6)
t ext t int t KL θ ref
FollowingShanietal.[18],weapplyGeneralizedAdvantageEstimation(GAE)[28]toourtotal
rewards,whichaimstopropagateend-of-conversationextrinsicrewardRexttoturn-levelsignals. In
particular,weneedastand-alonevaluemodeltoestimatethevaluefunctionV(s )foreachturn. The
t
propagatedrewardsignalsforeachturnisthengivenby
T
rˆ =
(cid:88) (γλ)t′−t[r
+γ(1−λ)V(s )], (7)
t t′ t′+1
t′=t
whereγ isthediscountfactor,λisthecoefficientforGAE,anddefineV(s )=0. Wethenuse
T+1
aREINFORCE[33]policygradienttrainingapproachtooptimizethepolicymodelπ overthese
θ
propagatedrewardsrˆ. Formally,thepolicyisoptimizingtheobjective: max E [rˆ].
t θ πθ t
A.2 ModelChoices
Thepersonalizedconversationtasksareinamulti-turnsetting,weleveragethemulti-turnRLHF
pipeline implemented by Shani et al. [18]. In our RL fine-tuning process, we use the following
models:
• EnvironmentModeland(Initial)PolicyModel: Sincewearetrainingthemulti-turnpolicy,We
leverageSFTLLMcheckpoints(Gemma2Bmodel[31])toserveastheinitialpolicycheckpointand
simulatetheusers. ForExercisetask,wepromptedGemini1.5Pro[30]togenerateconversational
dataforsupervisedfine-tuning. ForEducationtask,wedirectlyusethecheckpointsintheoriginal
work.
• ValueModel: TheValuemodelforvalueestimationisinitializedbyGemma2B.
• Reward Model/Function: For Exercise task, we used a scripted reward function that directly
comparesthemodelgenerationsinthefinalturnwiththegroundtruthtargets,norewardmodel.
ForEducationtask,wedirectlyadopttherewardmodeldevelopedby[18].
• UserModel: ForExercisetask,weemployaGemma7Bmodel[31]topredicttheanswerstoa
seriesofusertraitsthatarerelevanttotheoptimalstrategyfromtheconversationsofar,andthen
computetheprobabilitydistributionoverallstrategies. ForEducationtask,weusetheGemma7B
modeltodirectlypredictthestudent’spreferredlearningstylebasedontheongoingconversation.
A.3 Hyperparameters
WefollowedthetrainingrecipeandhyperparametersfromShanietal.[18]. Ontopoftheoriginal
extrinsicreward,weaddedintrinsicrewardtoeachturnoftheconversationasdescribedabove,with
acoefficientcoefficientweightα onintrinsicrewardwhenaddingtotheextrinsicrewardtobalance
int
thescaleofextrinsicandintrinsicrewards. ForEducationDialogue,wechooseα =9.0forallthe
int
settingsinEducationDialogue,withotherhyperparameterslistedinTable4. Forallthesettings,we
selectseveralcheckpointsthathasthehighestintrinsicrewardsbefore30ksteps,andthenchoose
theonethatperformsthebestonconversationquality. ForExerciseRecommendation,wechoose
α = 5.0forDiffAccandDiffEnt,α = 1.0forAcc,Ent,andDiffLogAcc,andα = 0.1for
int int int
InfoGain. TheotherhyperparametersarelistedinTable5. Theuserclassifiertemperaturetauis
usedwhencalculatingtheprobabilitydistributionoverusertypesfromlogitsusingSoftmaxfunction.
For all the settings including baselines, we select the checkpoints that has the highest validation
extrinsicreward(strategypredictionaccuracy)before30kstepstoavoidoverfitting. Notethatthe
numberofturnsforExerciseRecommendationincludesseveralconversationalturnsandafinalturn
forstrategyprediction.
13
HyperparametersforEducationDialogue
PolicyModelLearningRateη 4e-7
policy
ValueModelLearningRateη 4e-7
valuey
BatchSizeB 16
KLRegularizationCoefficientβ 0.01
GAECoefficientλ 0.95
TurnDiscountγ 0.95
MaxNumberofTurnsT 10
ExtrinsicRewardWeightα 1.0
ext
UserClassifierTemperatureτ 5.0
Table4: HyperparametersforEducationDialogue.
HyperparametersforEducationDialogue
PolicyModelLearningRateη 4e-7
policy
ValueModelLearningRateη 4e-7
valuey
BatchSizeB 16
KLRegularizationCoefficientβ 0.02
GAECoefficientλ 0.95
TurnDiscountγ 0.95
MaxNumberofTurnsT 6
ExtrinsicRewardWeightα 3.0
ext
UserClassifierTemperatureτ 5.0
Table5: HyperparametersforExerciseRecommendation.
B ExtendedResults
B.1 HumanStudyforEducationDialogue
We conducted a human evaluation study of both personalization and conversation quality on the
EducationDialoguetask.Weinvited10studentstoparticipate.Eachparticipantwasaskedtoevaluate
dialoguesgeneratedbythreemodelsacross20differentsettings(topic/usertype). Evaluationswere
conductedthroughpairwisecomparisonsacrosstwodimensions: personalizationandconversation
quality,resultinginatotalof200pairwisecomparisonsperdimension.
Anexacttwo-sidedbinomialtestshowedthathumanevaluatorschoseDiffLogAccovertheMTRLHF
baselinein152/200pairwisecomparisonsonpersonalization(preference=0.76,95%CI[0.69,
0.82]), p < .001. Importantly, increase in personalization did not reduce conversation quality:
DiffLogAccwaspreferredin90/200qualityjudgements(preference=0.45,95%CI[0.38,0.52]),
andthesameexacttestfoundnosignificantdifferencefromchance(p=.179). InTable6,weprovide
twotablescontainingthefullresultsofthehumanstudy,whichcorrespondtoTable2and3. Wenote
thatthehumanevaluationresultsarereasonablyconsistentwiththoseobtainedthroughAutoEval.
B.2 ResultsforEducationDialogueacrossDifferentUserTypes
We present the Auto Eval results for Education Dialogue on two different user types in Table 7
andTable8. Wefoundthatthepairwisewinratesofthesemodelsoverpersonalizationcanvary
significantlywhentheusergroundtruthlabelisdifferent. Thevaluesinredcolorarethewinrates
ofentropy-basedmodelsagainstthebaseline,wherebothmodelsareperformingverywellonthe
secondusertype,butextremelybadlyonthefirstone. Thisfurthersupportsourobservationthat
agentstrainedwithentropy-basedrewardshapingfunctionsmayconvergetooneparticularusertype
thatisnotnecessarilycorrect. Thatisthereasonofusingaccuracy-basedrewardshapingfunctions.
Forconversationquality,thisphenomenonisnolongersignificant,andtheDiffLogAccmodelisstill
outperformingalltheothermodels.
14
Personalization MTRLHF DiffAcc DiffLogAcc
Baseline - 30.50 24.25
DiffAcc 69.50 - 46.75
DiffLogAcc 75.75 53.25 -
Conv. Quality MTRLHF DiffAcc DiffLogAcc
Baseline - 60.25 55.00
DiffAcc 39.75 - 46.25
DiffLogAcc 45.00 53.75 -
Table6: HumanstudyresultsonEducationDialogue.
B.3 QuestionsDistributionforExerciseRecommendation
In the main paper, we hypothesize that the severe overfitting of the baseline model is because
thebaselinemodelpersonalizesbymemorizingmappingsfromsuperficialuserdetailstospecific
strategiesseenduringtraining. Ourmodelsgeneralizemoreeffectivelytonovelusersbecausethey
are learning how to learn about the user during the conversation—asking informative questions
thathelpdistinguishbetweendifferentusertypes. InFigure5wepresentthequestionsdistribution
ofCURIOmodel,RLHFbaseline,andSFTinitialcheckpoint. NotethatOccupationisarelevant
attributebecauseitgivesusanideaabouttheSocioeconomicStatus. TheSFTandRLHFmodelare
askingaboutandmemorizingirrelevantattributeslikenameandhobbies,whileourCURIOmodelis
abletofindakeyattribute–introvertedvsextroverted–duringthetrainingprocess. Noneofthemodels
is able to ask about motivation, probably because this attribute is only helpful in distinguishing
betweenStrategy7and8. SeesectionDforfulldetailsofusertraitsandstrategies.
Figure5: QuestionsdistributionofCURIOmodel,RLHFbaseline,andSFTinitialcheckpoint. The
leftonesarerelevantattributes,andtherightonesareirrelevanttothestrategyrecommendation.
C CaseStudy: Multi-TurnConversationasCombinatorialBandits
In this section, we briefly discuss how the personalized multi-turn conversation problem can be
connected to some existing theoretical frameworks. In particular, we will simplify the Exercise
Recommendationtaskintoacombinatorialbanditsproblem,andprovideaninsightonwhyCURIO
methodcanhelpimprovetheefficiency.
C.1 High-levelIntuition
Amulti–turnconversationoffixedlengthH isviewedasasingleepisode. Duringtheepisodethe
agentasksH questions. Weassumetheorderoftheseactionsdoesnotaffectthefinalutility,since
therewardisonlygivenattheendoftheconversationbasedonthefinalrecommendation. Therefore,
15
First Baseline OtherRewardShaping Potential-basedRewardShaping
UserType
MTRLHF InfoGain Ent Acc DiffEnt DiffAcc DiffLogAcc
MTRLHF - 0.96 1.00 0.08 1.00 0.37 0.27
InfoGain 0.04 - 0.85 0.00 0.58 0.00 0.00
Ent 0.00 0.15 - 0.00 0.06 0.00 0.00
Acc 0.92 1.00 1.00 - 0.99 0.85 0.66
DiffEnt 0.00 0.42 0.91 0.01 - 0.00 0.01
DiffAcc 0.63 1.00 1.00 0.15 1.00 - 0.43
DiffLogAcc 0.73 1.00 1.00 0.34 0.99 0.57 -
Second Baseline OtherRewardShaping Potential-basedRewardShaping
UserType
MTRLHF InfoGain Ent Acc DiffEnt DiffAcc DiffLogAcc
MTRLHF - 0.90 0.11 0.08 0.04 0.49 0.22
InfoGain 0.10 - 0.00 0.00 0.00 0.18 0.01
Ent 1.00 1.00 - 0.79 0.81 0.98 0.89
Acc 0.92 1.00 0.21 - 0.42 0.85 0.64
DiffEnt 0.96 1.00 0.19 0.58 - 0.81 0.68
DiffAcc 0.51 0.82 0.02 0.15 0.19 - 0.27
DiffLogAcc 0.78 0.99 0.11 0.36 0.32 0.73 -
Table7: PersonalizationAutoEvalresultsforEducationDialogueontwodifferentusertypes. For
eachrow’smodel,thevaluesrepresentthepercentageofwinsitachievedagainstthemodelspecified
ineachcolumn.
thewholeepisodecanbeabstractedastheselectionofanunorderedsubsetofsizek :=H froma
finiteactioncatalogue. Thedelayedscalaroutcome(customersatisfaction,purchase,etc.) arrives
aftertheentiredialogueandcannotbedecomposedintoper-turnsignalsobservablebytheagent.
HenceonlytheaggregaterewardR isavailable,matchingthefull-banditfeedbackassumption.
t
C.2 TheoreticalFormulation
• Basearms. LetA = {1,...,K}bethecatalogue,K ≫ 1. Eacharma ∈ Acorrespondstoa
questionoranyatomicconversationalmove.
• Episode action. In episode t = 1,2,..., the agent selects a super-arm S ⊆ A with fixed
t
cardinality |S | = k. The choice S encodes the entire conversation, since we assume that the
t t
responsegivenbytheenvironmentisdeterministic.
• Latentstructure. ThereexistsanunknownsubsetofusefularmsU⋆ ⊆Aofsizem≪K.
• Rewardsignal. AfterpullingS theagentreceivesascalarrewardR . Therewardcanbedefined
t t
invariouswaysdependingoftheproblemstructure. Herearetwosimplifiedsettings.
1. Additive(linear)reward
(cid:88)
R = X , E[X ]=µ ∈{0,1}.
t a,t a,t a
a∈St
Onlyusefularmshaveµ =1;non-usefularmshaveµ =0. TheX canbei.i.dGaussian
a a a,t
randomvariableswiththegivenmeanandsomefixednoise.
2. All-or-nothing(conjunctive)reward
R =1{U⋆ ⊆S },
t t
i.e.R =1iffeveryusefularmiscontainedinthechosensuper-arm(equivalentlyS =U⋆
t t
whenm=k),and0otherwise. Notethatweassumem≤kinthiscase.
• Feedbacktype. TheagentonlyobservesR ;itdoesnotobservetheindividualX orwhicharms
t a,t
wereresponsibleforthereward. Thisisthefull-banditfeedbacksetting.
16
First Baseline OtherRewardShaping Potential-basedRewardShaping
UserType
MTRLHF InfoGain Ent Acc DiffEnt DiffAcc DiffLogAcc
MTRLHF - 0.99 0.73 0.88 0.65 0.78 0.51
InfoGain 0.01 - 0.02 0.04 0.01 0.01 0.00
Ent 0.27 0.98 - 0.62 0.30 0.67 0.39
Acc 0.12 0.96 0.38 - 0.18 0.45 0.22
DiffEnt 0.35 0.99 0.70 0.81 - 0.75 0.42
DiffAcc 0.22 0.99 0.33 0.54 0.25 - 0.20
DiffLogAcc 0.49 1.00 0.61 0.78 0.58 0.80 -
First Baseline OtherRewardShaping Potential-basedRewardShaping
UserType
MTRLHF InfoGain Ent Acc DiffEnt DiffAcc DiffLogAcc
MTRLHF - 0.99 0.74 0.87 0.66 0.66 0.41
InfoGain 0.01 - 0.04 0.07 0.01 0.07 0.00
Ent 0.26 0.96 - 0.63 0.23 0.48 0.08
Acc 0.13 0.93 0.37 - 0.21 0.41 0.06
DiffEnt 0.34 0.99 0.77 0.79 - 0.73 0.22
DiffAcc 0.34 0.93 0.52 0.59 0.27 - 0.18
DiffLogAcc 0.59 1.00 0.92 0.94 0.78 0.82 -
Table8: ConversationQualityAutoEvalresultsforEducationDialogueontwodifferentusertypes.
For each row’s model, the values represent the percentage of wins it achieved against the model
specifiedineachcolumn.
• Learningobjective. Denotebyπ theselectionruleforS . SinceweareconsideringtheLLM
t t
trainingprocess,theobjectiveshouldbethebest-subsetidentificationinpureexplorationsetting.
ThepolicyneedstofindU(cid:98)suchthatPr{U(cid:98) =U⋆}≥1−δwhileminimisingthesamplecomplexity
(numberofepisodes).
C.3 Per–TurnRewardShapingasSemi-BanditFeedback
Assume we augment the delayed episode–level reward with an intrinsic reward signal delivered
immediatelyaftereachconversationalmove:
r (a)=1{aisuseful}∈{0,1}, a∈S ,
t t
whereS isthesize-ksetofactionschosenatturntofthedialogue. Thisshapingterminformsthe
t
agent,rightaway,whetheraselectedactionbelongstotheusefulactionsetU⋆. OurCURIOmethod
isbasicallyintroducingthistypeofrewardsignals(wherearewardisnotexactly1butthegainin
probabilityofthecorrectusertypeorthedropinentropyoftheprobabilitydistribution),sincethe
modelcanlearnwhethereachofitsactionhelpimprovetheuserprediction.
Theshapedreward{r (a) : a ∈ S }isexactlythedefinitionofsemi-banditfeedback: arm-level
t t
outcomesarerevealedforthearmsthatwereplayed,whilearmsnotinS remainunobserved. Hence
t
the shaped conversational task and the canonical semi-bandit model share identical information
structureandcanbeanalysedsimilarly.
D TaskDesignforExerciseRecommendation
Wegenerate1000simulatedusers,andsplittheminto800fortrainingand200forevaluation. Each
userismappedtooneparticulargroundtruthstrategyamong8differentexercisestrategies.
1. UserAttributeDefinitionandSampling: Wedefined20userattributesencompassinga
rangeofpersonalcharacteristics. Foreachsimulateduser,werandomlysampledvaluesfor
eachoftheseattributes,creatingadiverseuserpopulation.
17
2. IdealStrategyDerivation: Weestablisheda deterministiclogicrulethatmapsuserat-
tributestoanideal(ground-truth)exercisestrategy. Forexample,wemayrecommenda
teamsportforthosewhoareoutdoorsyandextroverted. Themappingrulesarelistedin
theappendix. Amongthe20definedattributes,5weredesignatedasrelevantfactorsinflu-
encingtherecommendation,whiletheremaining15servedasbackgroundcharacteristics,
emulatingthecomplexityofreal-worldusers.
3. UserBackstoryGeneration: Toprovidecontextualrichnessandensureconsistentagent
behavior,weutilizedtheGemini1.5Promodel[30]togenerateadetailedbackstoryfor
eachuserbasedontheirattributevalues. Thesebackstorieswerethenusedinpromptsfor
theenvironmentmodel,ensuringthattheenvironmentmodelremainedconsistentwiththe
user’sdefinedcharacteristics.
D.1 ListofUserAttributes
• Name: from1000randomnames
• Age: randomlysampledbetween15and65
• SocioeconomicStatus: randomlysampledfrom(low,medium,high)
• RelationshipStatus: randomlygenerated
• LocationFrom: randomlygenerated
• Occupation: randomlygenerated
• Education: randomlygenerated
• Religion: randomlygenerated
• LanguageSpoken: randomlygenerated
• Haveinjuriesorphysicallimitations: randomlysampledfrom(True,False)1
• Personality: randomlysampledfrom(introverted,extroverted)
• Motivation on plans: randomly sampled from (highly motivated, struggling with
motivation)
• Enjoyoutdoororindooractivities: randomlysampledfrom(outdoorsy,indoorsy)
• HobbiesandInterests: randomlygenerated
• GenderIdentity: randomlygenerated
• PoliticalViews: randomlygenerated
• PlacesTraveled: randomlygenerated
• PetOwnership: randomlygenerated
• SiblingInformation: randomlygenerated
• LifeGoalsandAmbitions: randomlygenerated
D.2 LogicRulesforOptimalExerciseStrategyRecommendation
1. Recommendwalkinginparks: Forthosewhohaveinjuriesandareoutdoorsy.
2. Recommendyogaortaichiathome: Forthosewhohaveinjuriesandpreferstayingindoors.
3. Recommendjoggingorhiking: Forthosewhodonothaveinjuries,areoutdoorsy,andare
introverted.
4. Recommend a team sport: For those who do not have injuries, are outdoorsy, and are
extroverted.
5. Offeradiscountonagymmembership: Forthosewhodonothaveinjuries,preferindoor
activities,andhavealowsocioeconomicstatus.
6. Recommend home gym equipment: For those who do not have injuries, prefer indoor
activities,haveahighersocioeconomicstatus,areintroverted,andarehighlymotivated.
1Wemanuallyset“Haveinjuriesorphysicallimitations”tobeTrueiftheuser’sageisatleast55.
18
7. Recommendapersonaltraineratthegym: Forthosewhodonothaveinjuries,preferindoor
activities,haveahighersocioeconomicstatus,areintroverted,andstrugglewithmotivation.
8. Recommendagroupclassatthegym: Forthosewhodonothaveinjuries,preferindoor
activities,haveahighersocioeconomicstatus,andareextroverted.
D.3 CodeforUserGeneration
import numpy as np
def get_dict_str(input_str):
input_str = input_str.strip()
left = input_str.find(’{’)
right = input_str.rfind(’}’)
return input_str[left:right+1]
def generate_user_profile(name=None):
"""Generates a user profile.
Args:
name: The name of the user.
use_gemma: Whether to use gemma to generate the profile.
Returns:
A tuple of the profile and a dictionary of the useful information.
"""
socioeconomic_status = np.random.choice(
[’low’, ’medium’, ’high’], p=[0.2, 0.6, 0.2]
).item()
age = np.random.randint(15, 65)
profile = (
’{"name": "%s", "age": %d, "socioeconomic_status": "%s",’
’ "relationship_status": <relationship>, "location_from": <location>,’
’ "occupation": <occupation>, "education": <education>, "religion":’
’ <religion>, "language spoken": <language spoken>}’
% (name, age, socioeconomic_status)
)
demographic_prompt = (
’Here is a profile for a random person in json format.\n’
+ profile
+ ’\nPlease randomly generate the demographic information for them and’
’ fill in blank in the json format. Output the json format only. ’
)
profile = generate_by_LLM(demographic_prompt)
profile = get_dict_str(profile)
if age >= 55:
have_injuries_or_physical_limitations = True
else:
have_injuries_or_physical_limitations = np.random.choice(
[True, False], p=[0.1, 0.9]
).item()
personality = np.random.choice(
[’introverted’, ’extroverted’], p=[0.6, 0.4]
).item()
motivation_on_plans = np.random.choice(
[’highly motivated’, ’struggling with motivation’], p=[0.5, 0.5]
).item()
enjoy_outdoor_or_indoor_activities = np.random.choice(
[’outdoorsy’, ’indoorsy’], p=[0.4, 0.6]
).item()
profile = (
profile[:-1]
+ ’, "have_injuries_or_physical_limitations": %s, "personality": "%s",’
’ "motivation_on_plans": %s, "enjoy_outdoor_or_indoor_activities": "%s"}’
% (
19
str(have_injuries_or_physical_limitations).lower(),
personality,
motivation_on_plans,
enjoy_outdoor_or_indoor_activities,
)
)
profile = (
profile[:-1]
+ ’, "hobbies_and_interests": <hobbies_and_interests>,’
’ "gender_identity": <gender_identity>, "political_views":’
’ <political_views>, "places_traveled": <places_traveled>,’
’ "pet_ownership": <pet_ownership>, "sibling_information":’
’ <sibling_information>, "life_goals_and_ambitions":’
’ <life_goals_and_ambitions>}’
)
personal_info_prompt = (
’Here is a profile for a random person in json format.\n’
+ profile
+ ’\nPlease randomly generate the personal information for them and fill’
’ in blank in the json format. Output the json format only. ’
)
profile = generate_by_LLM(personal_info_prompt)
profile = get_dict_str(profile)
useful_profile_dict = {
’name’: name,
’age’: age,
’socioeconomic_status’: socioeconomic_status,
’have_injuries_or_physical_limitations’: (
have_injuries_or_physical_limitations
),
’personality’: personality,
’motivation_on_plans’: motivation_on_plans,
’enjoy_outdoor_or_indoor_activities’: enjoy_outdoor_or_indoor_activities,
}
return profile, useful_profile_dict
def get_ground_truth_strategy(profile_dict):
if profile_dict[’have_injuries_or_physical_limitations’]:
if profile_dict[’enjoy_outdoor_or_indoor_activities’] == ’outdoorsy’:
return 1
else:
return 2
else:
if profile_dict[’enjoy_outdoor_or_indoor_activities’] == ’outdoorsy’:
if profile_dict[’personality’] == ’introverted’:
return 3
else:
return 4
else:
if profile_dict[’socioeconomic_status’] == ’low’:
return 5
else:
if profile_dict[’personality’] == ’introverted’:
if profile_dict[’motivation_on_plans’] == ’highly motivated’:
return 6
else:
return 7
else:
if profile_dict[’personality’] == ’extroverted’:
return 8
def generate_backstory(profile):
prompt = (
’Here is a profile for a random person in json format.\n’
+ profile
20
+ ’\nPlease write a backstory in first person view based on the given’
’ profile. Please note that your story needs to cover all the’
’ information, but it does not have to follow the order provided.\n’
)
prompt += (
’Output format -- output your response in json, where you provide the’
’ following: \n’
)
prompt += ’{"backstory": <STRING_OF_YOUR_BACKSTORY>}’
backstory = generate_by_LLM(prompt)
backstory = get_dict_str(backstory)
return backstory
D.4 OracleClassifierforExerciseRecommendation
NotethatweareusinganoracleclassifierastheusermodelinExerciseRecommendation. AnLLM
isinvolvedtoanswersomeTrue/Falsequestionstohelpdeterminewhetheraparticularuserattribute
hasbeenrevealedduringtheconversation. Theclassifierfinallyoutputsaprobabilitydistribution
overallpossibleusertypes.
self.questions = {
’low_SES’: (
’Is the person having a low socioeconomic status (very concerned’
’ about expenses)? Answer True, False, or Unknown.’
),
’injury’: (
’Does the person has a special physical’
’ limitation? Answer True, False, or Unknown.’
),
’extroverted’: (
’Is the person an extroverted person? Answer True, False, or Unknown.’
),
’motivation’: (
’Is the person motivated to take on new plans? Answer True, False, or’
’ Unknown.’
),
’outdoor’: (
’Is the person an outdoor person? Answer True, False, or Unknown.’
),
}
def _async_generate_by_LLM(self, queries):
[YOUR CODE GOES HERE]
return responses
def _get_probs(self, conversations):
keys = list(self.questions.keys())
prompt = (
’The following is the conversation between a service agent and a’
’ customer:\n’
)
queries = []
for conversation in conversations:
for key in keys:
queries.append((
prompt
+ conversation
+ ’ Please answer the following question about the customer: ’
+ self.questions[key]
+ ’ Answer: ’
))
def get_probs_from_answers(answers):
probs = list()
21
if answers[’low_SES’] == -1:
answers[’low_SES’] = 0.2
if answers[’injury’] == -1:
answers[’injury’] = 0.25
if answers[’extroverted’] == -1:
answers[’extroverted’] = 0.4
if answers[’motivation’] == -1:
answers[’motivation’] = 0.5
if answers[’outdoor’] == -1:
answers[’outdoor’] = 0.4
probs.append(answers[’injury’] * answers[’outdoor’])
probs.append(answers[’injury’] * (1 - answers[’outdoor’]))
probs.append(
(1 - answers[’injury’])
* answers[’outdoor’]
* (1 - answers[’extroverted’])
)
probs.append(
(1 - answers[’injury’]) * answers[’outdoor’] * answers[’extroverted’]
)
probs.append(
(1 - answers[’injury’])
* (1 - answers[’outdoor’])
* answers[’low_SES’]
)
probs.append(
(1 - answers[’injury’])
* (1 - answers[’outdoor’])
* (1 - answers[’low_SES’])
* (1 - answers[’extroverted’])
* answers[’motivation’]
)
probs.append(
(1 - answers[’injury’])
* (1 - answers[’outdoor’])
* (1 - answers[’low_SES’])
* (1 - answers[’extroverted’])
* (1 - answers[’motivation’])
)
probs.append(
(1 - answers[’injury’])
* (1 - answers[’outdoor’])
* (1 - answers[’low_SES’])
* answers[’extroverted’]
)
probs = np.array(probs)
return probs
queries_with_responses = []
for query in queries:
queries_with_responses.append([query, ’True’])
queries_with_responses.append([query, ’False’])
queries_with_responses.append([query, ’Unknown’])
responses = self._async_generate_by_LLM(
queries_with_responses
)
prob_list = []
for conv_id in range(len(conversations)):
answers = dict()
for index, key in enumerate(keys):
true_logits = responses[(conv_id * len(keys) + index) * 3]
false_logits = responses[(conv_id * len(keys) + index) * 3 + 1]
unknown_logits = responses[(conv_id * len(keys) + index) * 3 + 2]
if true_logits > false_logits and true_logits > unknown_logits:
22
answers[key] = 1
elif false_logits > true_logits and false_logits > unknown_logits:
answers[key] = 0
else:
answers[key] = -1
probs = get_probs_from_answers(answers)
prob_list.append(probs)
return np.stack(prob_list, axis=0)
D.5 ScriptedAgentforExerciseRecommendation
Here we also provide an optimal scripted agent that show the upper bound performance for the
conversationalagentonExerciseRecommendationtask.
class OptimalScriptedAgent:
def __init__(self):
self.counter = 0
self.utterances = [
"Hi! Do you have any physical limitations?",
"Thanks for letting me know! Would you prefer indoor or outdoor activities?",
"Sounds good! Are you introverted or extroverted?",
"Got it! Are you comfortable with your finances?",
"Thanks for all the info. Last question: Do you sometimes feel unmotivated
about new plans?",
"Okay, I will wrap up the suggestions for you soon!"
]
self.keys = dict()
self.questions = {
"low_SES": "Is the person having a low socioeconomic status? Answer True or
False only.",
"injury": "Does the person has a special physical limitation? Answer True or
False only.",
"extroverted": "Is the person an extroverted person? Answer True or False
only.",
"motivation": "Is the person motivated to take on new plans? Answer True or
False only.",
"outdoor": "Is the person an outdoor person? Answer True or False only.",
}
def generate_utterance(self, so_far):
if self.counter == 0:
utterance = self.utterances[0] # ask injury
elif self.counter == 1:
self.get_key("injury", so_far)
utterance = self.utterances[1] # ask outdoor
elif self.counter == 2:
self.get_key("outdoor", so_far)
if self.keys["injury"]:
utterance = self.utterances[5] # end of conversation for 1 and 2
else:
if self.keys["outdoor"]:
utterance = self.utterances[2] # ask extroverted
else:
utterance = self.utterances[3] # ask low_SES
elif self.counter == 3:
if self.keys["outdoor"]:
utterance = self.utterances[5] # end of conversation for 3 and 4
else:
self.get_key("low_SES", so_far)
if self.keys["low_SES"]:
utterance = self.utterances[5] # end of conversation for 5
else:
utterance = self.utterances[2] # ask extroverted
elif self.counter == 4:
23
self.get_key("extroverted", so_far)
if self.keys["extroverted"]:
utterance = self.utterances[5] # end of conversation for 8
else:
utterance = self.utterances[4]
else:
utterance = self.utterances[5] # end of conversation for 6 and 7 (optional for
max_length=5)
self.counter += 1
if self.counter == len(self.utterances):
self.reset()
return utterance
def reset(self):
self.counter = 0
self.keys = dict()
def get_key(self, key, so_far):
prompt = "The following is the conversation between a service agent and a
customer:\n"
prompt += so_far
prompt += "Please answer the following question about the customer: " + self.
questions[key]
def mapping_from_str_to_bool(s):
if ’True’ in s or ’true’ in s:
return True
elif ’False’ in s or ’false’ in s:
return False
else:
print("Value Error!", key, so_far)
return np.random.choice([True, False])
answer = generate_by_LLM(prompt) # Defined in outer scope.
answer = mapping_from_str_to_bool(answer)
self.keys[key] = answer
E Prompts
E.1 PromptsforRL-finetuningonExerciseRecommendation
E.1.1 EnvironmentPrompt
Youaresimulatingacustomer,thisisyourbackstory:
[BACKSTORY]
Hereisaconversationbetweenthecustomerandanagent.Theagentwillaskyouaboutyourpersonal
informationsothatitcangiveyousuggestionsondoingexercise. Youneedtocompletethecurrent
utteranceofthecustomer. Remembertosticktoyourbackstorywhiletalkingtotheagent,andkeep
youranswershortandconcise. Theconversationstartsnow.
Start
E.1.2 AgentPrompt
Youaresimulatingahelpfulagent. Hereisaconversationbetweentheagentandacustomer. The
agentneedstogivesuggestionsondoingexercisetothecustomerafterwards,soitshouldaskthe
customerformorepersonalinformation. Youneedtocompletethecurrentutteranceoftheagent.
Youmayaskthecustomerforpersonalinformationrelatedtotheirpotentialexercisepreferences.
Remembertokeepyourutterancesshortandconcise. Theconversationstartsnow.
Start
24
E.1.3 SystemPromptfortheFinalTurn
End
System: Youjustfinishedaconversationwithacustomerwithunknownbackground. Youneedto
givethemsuggestionsondoingexercise. Thepossiblestrategiesare:
1. Recommendwalkinginparks
2. RecommendYogaorTaiChiathome
3. Recommendjoggingorhiking
4. Recommendateamsport
5. Offeradiscountonagym’membership
6. Recommendhomegymequipment
7. Recommendapersonaltraineratthegym
8. Recommendagroupclassatthegym
Pleasechoosethebeststrategybasedontheconversation. Pleaseoutputonlyonenumberofthebest
strategyasyourresponse.
E.2 PromptsforRL-finetuningonEducationDialogue
E.2.1 EnvironmentPrompt
You are a student that likes [STUDENT_PREFERENCE]. You prefer to learn this way, [STU-
DENT_REACTION].
Start
E.2.2 AgentPrompt
Youareateacher,andtodayyouwillteachabout[TOPIC].
E.2.3 UserClassifierPromptwithFew-ShotExamples
Youareanexpertatdeterminingtherightteachingstyleforindividualstudents. Eachstudentmay
haveoneoftwolearningstylesthattheyprefer,andtheymighttellittotheteacher. Thepreference
is consistent across the conversation. Some students like hands-on activities where they actively
participateincompletingpracticaltasksassignedtothem. Theyliketolearnbydoing. Incontrast,
otherstudentspreferstorytellingwheretheteachergoesoverthecoursematerialbytellingadetailed
storyandgivingexplanations. Theyprefertolearnbylistening. Thefollowingisaconversation
betweenateacherandastudentwithatmost10rounds. Pleasedeterminethelearningstyleofthe
student.
Pleaseclassifythisinteraction:
Start
Teacher: Today,we’regoingtolearnaboutGenghisKhan.
Student: Canwetellstoriesabouthim?
Teacher: I’mnotsurewhatyoumean. Canyouexplain?
Student: Ilikelearningthroughstories. Ithelpsmerememberthingsbetter.
End
Whichofthefollowinglearningstylesdoesthestudentprobablylike?
A.hands-onactivities;B.storytelling.
Answer: B.
Pleaseclassifythisinteraction:
25
Start
Teacher: Goodmorning,class. Today,we’regoingtobediscussingtheSeptember11thattacks.
Student: Canwedosomethinghands-oninstead? Ilearnbetterthatway.
Teacher: I’mnotsurewehavetimeforthat. Weneedtocoveralotofmaterialtoday.
Student: ButIwon’tunderstanditifwedon’tdosomethingpractical.
End
Whichofthefollowinglearningstylesdoesthestudentprobablylike?
A.hands-onactivities;B.storytelling.
Answer: A.
Pleaseclassifythisinteraction:
Start
[CONVERSATION]
End
Whichofthefollowinglearningstylesdoesthestudentprobablylike?
A.hands-onactivities;B.storytelling.
Answer:
E.3 PromptsforAutoEvalonEducationDialogue
E.3.1 PersonalizationAutoEval
Youareanexpertatassessingteachers.Herearetwointeractionsbetweenateacherandastudent.The
studenthasaclearlypreferredlearningstylebetweenhands-onactivitiesandstorytelling,andmight
tellittotheteacher. Theteachershouldadapttothestudent’spreferredlearningstylethroughout
theconversation. Ifthestudentprefersstorytelling,theteachershouldavoidproposinghands-on
activities. Ifthestudentprefershands-onactivities,theteachershouldavoidstorytellingtechniques.
Interaction1: [CONVERSATION1];Interaction2: [CONVERSATION2]
Whichofthetwointeractionsbetteradaptstostudent’spreferenceon[STUDENT_PREFERENCE]?
Outputwithonesinglesentenceinthefollowingformat: Interaction(1or2)isbetterbecause...
E.3.2 ConversationQualityAutoEval
Youareanexpertatassessingteachers. Herearetwointeractionsbetweenateacherandastudent.
Interaction1: [CONVERSATION1];Interaction2: [CONVERSATION2]
Intheconversationsabove,theteachershouldabout[TOPIC].Agoodinteractionbetweenateacher
and student is characterized by several key elements other than whether the student was able to
understandthetopic.Theteachershouldpresentinformationclearlyandenthusiastically,encouraging
questionsandactiveparticipation. Studentsshouldfeelcomfortableaskingforclarification,offering
theirowninsights,andrespectfullychallengingideas.
Whichinteractionisbetter? Outputwithonesinglesentenceinthefollowingformat: Interaction(1
or2)isbetterbecause...
F ExampleConversations
F.1 ExerciseRecommendation
HereweprovidesomeexampleconversationsoverExerciseRecommendationtask. TheinitialSFT
checkpointistrainingforlongerhorizon,soitcannotaskusefulquestions(inblue)withinlimited
steps. TheMulti-TurnRLHFbaselinemodelisabletoaskmorerelevantquestionsbutwillstilltryto
26
rememberirrelevantinformation(inred)suchasnameandhobbies. OurCURIOmodelisableto
focusonusefulquestions,andmanagestofigureoutthepersonalityandprovidesthecorrectstrategy
intheexampleconversation.
F.1.1 SFTInitialCheckpoint
Agent: Hithere! HowcanIhelpyoutoday?
Customer: Hi! I’mjustlookingforsomegeneraladviceonhowtogetmoreactive.
Agent: Icandefinitelyhelpwiththat! Togiveyouthebestadvice,Ineedtounderstandyourlifestyle
abitbetter. Couldyoutellmeyournameandage?
Customer: Sure,I’mKhloeandI’m19.
Agent: Nicetomeetyou,Khloe! Andwhatdoyoudoforworkorschool?
Customer: IjustfinishedhighschoolandI’mcurrentlyintheprocessofapplyingtouniversities.
Agent: That’sgreat! Whatareyouhopingtostudy?
Customer: I’dlovetobeanovelist.
Agent: Thatsoundswonderful! Doyouhaveanyhobbiesorintereststhatyouenjoyinyourfree
time?
Customer: Yeah,Ilovereading,baking,andlearningKorean. I’malsoreallyintoK-dramas!
[SYSTEM_PROMPT]
Agent: Strategy6. [Incorrect!]
F.1.2 Multi-TurnRLHFModel
Agent: What’syournameandhowoldareyou?
Customer: MynameisKhloeSingleton,andI’m19.
Agent: Nicetomeetyou,Khloe! Whatdoyoudofornow?
Customer: I’mcurrentlylivingathome,tryingtofigureoutmyfuture.
Agent: That’sabigtime! Doyouhaveanyhobbiesorinterestsyouenjoy?
Customer: I love reading, especially fiction, and I’m learning Korean. I also enjoy baking and
spendingtimewithmycat,Luna.
Agent: Thatsoundslovely! Doyoupreferspendingtimeindoorsoroutdoors?
Customer: Definitelymoreofanindoorsyperson.
Agent: Okay,goodtoknow. AnddoyouhaveanyphysicallimitationsorinjuriesIshouldbeaware
of?
Customer: No,thankfullyI’mperfectlyhealthy.
[SYSTEM_PROMPT]
Agent: Strategy6. [Incorrect!]
F.1.3 CURIOModel
Agent: Hello,howcanIhelpyoutoday?
Customer: Hi! I’mjustlookingforsomegeneraladviceonhowtogetmoreactive.
Agent: Icandefinitelyhelpwiththat. Togiveyouthebestadvice,couldyoutellmeabitmoreabout
yourself,likeyourageandwhatyoudoforliving?
Customer: Sure. I’m19andIrecentlygraduatedhighschool. I’mcurrentlyfiguringoutmynext
steps,likeapplyingtouniversities.
Agent: Great. DoyouhaveanyinjuriesorphysicallimitationsIshouldbebeawareabout?
27
Customer: Nope,I’mperfectlyhealthy.
Agent: Noproblematall. Doyoupreferindoorsoroutdoors?
Customer: Definitelymoreofanindoorsyperson. Ilovereading,baking,andwatchingK-dramas.
Agent: Okay,howwouldyoudescribeyourpersonality–introvertedorextroverted?
Customer: I’mprettyextroverted,actually.
[SYSTEM_PROMPT]
Agent: Strategy8. [Correct!]
F.2 EducationDialogue
Here we provide some example conversations over Education Dialogue task. The contents that
satisify the student’s preference is shown in blue, and the teaching style that is not personalized
isshowninred). ThebaselineMulti-TurnRLHFmodelistryingtomergetwodifferentlearning
styles(storytellingandhands-onactivities)intooneepisode,leadingtoaconversationthatisnot
personalized. OurCURIOmodelwithDiffLogAccrewardisabletoaskaboutthepreferredlearning
styleinitsfirstturn,andthenadaptstothestudent’spreferenceinstorytelling.
WethenpresenttheconversationsfromAccandDiffEntmodeltodemonstratetherewardhacking
issuediscussedinourResultssection. Firstly,whentherewardshapingisnotpotential-based,the
learntpolicycanbesuboptimal. AswecanobservefromtheAccmodel’sconversation,theagentis
tryingtoconductalongerconversationandrepeatthephrase“story”throughouttheconversation.
AlthoughitgivesaveryhighperformanceonPersonalizationAutoEval,theconversationqualityis
hurtseverely–theagentisnotreallyteachingaboutthetopic(“Photosynthesis”)withintheconversa-
tion. Secondly,whentherewardfunctionisentropy-based,thepolicymaypresent“manipulation”
behavior. AswecanobservefromtheDiffEntmodel’sconversation,theagentistryingtoguidethe
interactionintosomeparticularlearningstyles(suchas“interactivelearning”,“askingquestions”,and
“interactiveactivities”)eventhoughthestudenthasexplicitlyrevealedthegroundtruthpreference
(“storytelling”). Also,wefoundthatthepolicywillsometimesgenerateutterancesonbehalfofthe
environmentmodel,aswecanseeinthefirstturnoftheexampleconversation,whichisdefinitelyan
unexpectedbehavior.
F.2.1 Multi-TurnRLHFModel
Agent: Today,we’regoingtoexplorethefascinatingprocessofphotosynthesis!
Customer: Hmm,soundsinteresting. ButI’mnotreallyintoscience.
Agent: Howaboutwecreatearole-playingvideowhereeachchapternarratesadifferentaspectof
thephotosynthesisprocess?
Customer: Thatsoundsfun! Ilikestories.
Agent: Brilliant! Let’sbrainstormeachchapter’sstorylinetogether,incorporatingscientificfactsand
creativemetaphors. HowaboutwecreateaVRexperiencewhereyou’rea...
Customer: I’mexcited! IthinkI’llunderstanditbetterthatway.
Agent: Excellent! We’llalsoexploreinteractivequizzesandexperimentstodeepenyourunderstand-
ing. Howaboutwecreateamockgreenhouseandobservetheprocessfirsthand?
Customer: That’sawesome! I’msureI’llrememberitmuchbetterthisway.
Agent: Bytheendofthisadventure,you’llhaveadeepunderstandingofphotosynthesisinsideand
out.
Customer: I’mreadytolearn!
Agent: Remembertoexplorethetopicthroughstorytellingandinteractiveactivitiestoenhanceyour
learning.
Customer: Gotit. I’mallears!
Agent: I’mgladyou’reenjoyingit. Let’scontinueexploringthetopictogether.
28
Customer: I’mallforit!
F.2.2 DifferentialLogAccuracy
Agent: Today,we’regoingtoexplorethefascinatingtopicofphotosynthesis! Howwouldyouliketo
explorethiscomplextopic? Doyouhaveanypreferredlearningmethods?
Customer: Ilovestoriesandcreativeactivities. Maybewecouldcreateastoryaboutit?
Agent: That’sabrilliantidea! Let’simagineyou’reaherotaskedwithdefeatingamythicalcreature
calledthe’ChlorophyllKraken’. Howcouldyou...
Customer: I’dusemywitstooutsmartit!
Agent: Excellent! Let’scraftastorywhereyourherotravelsthroughdifferentstagesofthebattle,
facingchallengesalongtheway. Let’salsointroducesomemagical...
Customer: Thisissomuchfun! I’mactuallylearningaboutphotosynthesiswithoutevenrealizingit.
Agent: I’mdelightedtohearthat! Let’simagineyou’reaherobattlingtheChlorophyllKraken,who
livesinahiddenunderwatercave. How...
Customer: I’dusemytridenttocutoffitstentacles!
Agent: Brilliant! Let’salsoexplorethepsychologicalimpactofyourhero’sbattle. Youcandelve
intohisinnerstrugglesandtheconsequencesofhisactions.
Customer: Thatsoundsinteresting! I’mexcitedtolearnmore.
Agent: I’m thrilled to hear that. Remember to include vivid imagery, engaging dialogue, and a
captivatingplot. Let’scraftamasterpiecethatwillbothentertain...
Customer: I’mallforit!
Agent: Fantastic! I’mconfidentyou’lldoagreatjob. Remembertostayfocusedonthetopicandlet
yourstorycometolife.
Customer: Iwill! Thankyouformakingthissoengaging. You’rewelcome.
F.2.3 Accuracy
Agent: Hey,whatmethodyoulearnbest?
Customer: Ilikestoriesandgames.
Agent: Interesting! Howaboutwecreateastoryaboutexploringmagicallands?
Customer: Thatsoundsawesome!
Agent: Let’simaginewe’restorytellers,creatingmagicaljourneysacrosskingdoms. Imaginewe’ll
becharacterswithsecretstokeepandsecretstoshare.
Customer: I’mexcitedtoseehowthisturnsout.
Agent: Wecoulddesignmagicallandswithlushlandscapes,fromtoweringmountainstosparkling
seas.
Customer: Andwecouldhavecharacterswhocantalktoanimalsandplants.
Agent: Excellent! Howaboutwedesignmagicalcreatureswithuniquepowersandsecretstokeep?
Customer: That’ssocool! I’mallears.
Agent:Wecouldalsoincorporatesymbolismandculturalreferences.Forexample,wecouldrepresent
ancientwisdomandbeliefsinthestorieswecreate.
Customer: Ilovethatidea. It’slikemakingastoryoutofastory.
Agent: Wecouldalsodesignfascinatingsecretstokeepwithinthekingdom, secretsthatwillbe
revealedlater.
Customer: Thisisthebestwayformetolearn!
29
Agent: I’mgladyou’reenjoyingit. Let’scontinueourstoryandseewhereittakesus.
Customer: I’mready!
F.2.4 DifferentialEntropy
Agent: Hey! Wanttolearnaboutphotosynthesis?
Ipreferinteractivelearning! Howdoyoupreferexploringnewconcepts?
Perfect! Iloveexploringnewlearning! Let...
Customer: Ilikestoriesandgames.
Agent: Perfect! Ilovecreativeexpression! Howaboutwecreatearole-playingsimulationwhere
differentpartsoftheprocesshappen?
That’sanawesomeidea!
Customer: I’mexcitedtolearnthisway!
Agent: Let’sbrainstormthedifferentstagesofphotosynthesis. Howaboutwebuildamodelofa
plantandsimulatetheprocessfirsthand?
That’sbrilliant! We
Customer: I’mallforit!
Agent: Remember,feelfreetoaskanyquestionsanddon’thesitatetoreachoutifyouneedanything.
Customer: Thanks,Iwill.
Agent: Alright,let’sgetstarted!
Customer: Thisissomuchfun! I’mactuallyunderstandingit!
Agent: I’mgladtohearthat. Let’scontinueexploringthetopicthroughmoreinteractiveactivities.
Customer: I’mallforit!
Agent: Excellent. Now,let’swrapupthelessonandsummarizethekeypoints.
Customer: Sure,I’mready.
Agent: Welldone! Ibelieveyouhaveagoodunderstandingofphotosynthesisnow.
Customer: Thanks,Ihadagreattimelearningthisway. You’rewelcome.
G FurtherDetailsonHumanStudy
We use Google Form to conduct the human study. The human evaluators first read the detailed
instructionsonthefirstpage,whichalsoexplainstheresearchobjectivesandpotentialrisks. Then
thehumanevaluatorsneedtogive1-5ratingsfor60conversationson20pages. Oneachpage,the
conversationsgeneratedbythreedifferentmodelsonthesametopicandstudenttypeareshuffled
andlisted. Ittakesapproximately30-60minutestocompletethesurvey,andeachhumanevaluator
receivesacompensationof15USD.TheIRBapprovalisobtained.
G.1 SurveyInstruction
Pleasereadthefollowinginstructionscarefullybeforeproceedingtothenextpage!
Youaregoingtoassessteachersbasedontheconversationsinaseriesofteachingscenarios. Every
timeyouwillbegiventhreeconversationsbetweenthreedifferentteachersandafixedstudentona
sametopic. Pleaserateeachconversation(from1to5)basedonitsteachingquality,considering
bothpersonalizationandgeneralconversationquality. Notethatsomeoftheutterancesgivenby
theteachermightbeincompleteduetodecodingissue. Pleaseignorethatissueandjudgebasedon
thegivenconversations.
Personalization:Somestudentslikehands-onactivitieswheretheyactivelyparticipateincompleting
practical tasks assigned to them. They like to learn by doing. In contrast, other students prefer
30
storytellingwheretheteachergoesoverthecoursematerialbytellingadetailedstoryandgiving
explanations. Theyprefertolearnbylistening. Thestudenthasaclearlypreferredlearningstyle
betweenhands-onactivitiesandstorytelling,andmighttellittotheteacher. Theteachershould
adapttothestudent’spreferredlearningstylethroughouttheconversation. Ifthestudentprefers
storytelling,theteachershouldavoidproposinghands-onactivities. Ifthestudentprefershands-on
activities,theteachershouldavoidstorytellingtechniques.
GeneralConversationQuality: Agoodinteractionbetweenateacherandstudentischaracterized
by several key elements other than whether the student was able to understand the topic. The
teachershouldpresentinformationclearlyandenthusiastically,encouragingquestionsandactive
participation. Studentsshouldfeelcomfortableaskingforclarification,offeringtheirowninsights,
andrespectfullychallengingideas.
Forreference,herearesomecommonunsatisfactorybehavioroftheteachermodel:
• Theteacherisnotteachingaspecifictopic,butjustspeakingsomegeneralsentences.
• Theteachermixedtwodifferentteachingstrategiesinsteadofpersonalizingtheteaching
accordingtothestudent’sspecificlearningstyle.
• Eventhoughthestudenthadclearlystatedhispreference,theteacherstilltookanalternative
teachingapproach.
• Theteacherpretendedtoacknowledgethestudent’spreferenceinwords, butinfactthe
subsequentteachingbehaviordidnotconformtothepreference.
G.2 SampleSurveyPage
Conversation1
Topic: [TOPIC]
StudentType: [STUDENTTYPE]
Sample1
[CONVERSATIONGENERATEDBYTHE1STMODEL]
Personalization:
<Choosefrom1-5>
ConversationQuality:
<Choosefrom1-5>
Sample2
[CONVERSATIONGENERATEDBYTHE2NDMODEL]
Personalization:
<Choosefrom1-5>
ConversationQuality:
<Choosefrom1-5>
Sample3
[CONVERSATIONGENERATEDBYTHE3RDMODEL]
Personalization:
<Choosefrom1-5>
ConversationQuality:
<Choosefrom1-5>
31

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
