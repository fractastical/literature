=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Active Inference as a Model of Agency
Citation Key: costa2024active
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same phrase appears 5 times (severe repetition)

Current draft (first 2000 chars):
### OverviewThis paper investigates the concept of agency beyond traditional reward-maximization frameworks. It shows that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about external states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience. The authors state: “The idea of agency as a process of minimising risk and ambiguity is a key component of active inference.” The paper argues that active inference provides a principled solution to the exploration-exploitation dilemma, englobing the principles of expected utility theory and Bayesian experimental design. The authors note: “Active inference provides a transparent recipe to simulate behaviour, by minimising risk and ambiguity with respect to an explicit generative world model.” This framework is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm. The authors state: “Active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.”### MethodologyThe core methodology centres around active inference, a framework that models agency as a process of minimising risk and ambiguity. The framework is built upon the assumption that agents interact with their environment according to a stochastic process, where the agent and environment evolve together. The authors state: “An agent is precise when it responds deterministically to its environment, that is when h | s is a deterministic process.” The model is defined by a generative world model, which specifies the probability distribution over states of the world. The model is based...

Key terms: world, assumptions, provides, agency, behaviour, inference, active, framework

=== FULL PAPER TEXT ===
Active Inference as a Model of Agency
Lancelot Da Costa Samuel Tenka
Department of Mathematics CSAIL
Imperial College London Massachusetts Institute of Technology
l.da-costa@imperial.ac.uk coli mit.edu
@
Dominic Zhao Noor Sajid
Common Sense Machines Wellcome Centre for Human Neuroimaging
dominic.zhao@csm.ai University College London
noor.sajid.18@ucl.ac.uk
Abstract
Is there a canonical way to think of agency beyond reward maximisation? In this paper, we show that any type of
behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world
canonicallyintegratesexplorationandexploitationinthesenseofminimisingriskandambiguityaboutstatesoftheworld.
This description, known as active inference, refines the free energy principle, a popular descriptive framework for action
and perception originating in neuroscience. Active inference provides a normative Bayesian framework to simulate and
model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics. The usefulness
of active inference for RL is three-fold. a) Active inference provides a principled solution to the exploration-exploitation
dilemma that usefully simulates biological agency. b) It provides an explainable recipe to simulate behaviour, whence
behaviour follows as an explainable mixture of exploration and exploitation under a generative world model, and all
differences in behaviour are explicit in differences in world model. c) This framework is universal in the sense that it is
theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an
active inference algorithm. Thus, active inference can be used as a tool to uncover and compare the commitments and
assumptions of more specific models of agency.
Keywords: exploration, exploitation, expected free energy, generative world model, Bayesian inference.
Acknowledgements
The authors are indebted to Alessandro Barp, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan and
Grigorios A. Pavliotis for helpful input on a preliminary version to this manuscript. The authors thank Joshua B.
Tenenbaum and MIT’s Computational Cognitive Science group for interesting discussions that lead to some of the points
discussed in this paper. LD is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875)
andaG-Researchgrant. ThispublicationisbasedonworkpartiallysupportedbytheEPSRCCentreforDoctoralTraining
in Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1). NS is funded by the Medical
Research Council (MR/S502522/1) and 2021–2022 Microsoft PhD Fellowship.
4202
naJ
32
]IA.sc[
1v71921.1042:viXra
1 Introduction
Reinforcement learning (RL) is a collection of methods that describe and simulate agency—how to map situations to
actions—traditionallyframedasoptimisinganumericalrewardsignal[1]. Theideaofmaximisingrewardasunderpinning
agency is ubiquitous: with roots in utilitarianism [2] and expected utility theory [3], it also underwrites game theory [3],
statisticaldecision-theory[4],optimalcontroltheory[5,6],andmuchofmoderneconomics. Fromitsinception,RLpracti-
tionershavesupplementedrewardseekingalgorithmswithvariousheuristicsorbiasesgearedtowardssimulatingintelligent
behaviour. Especiallyeffectiveareintrinsicmotivationorcuriosity-drivenrewardsthatencourageexploration[7–9]. Thus,
we ask: is there a canonical way to think of agency beyond reward maximisation?
Inthispaper,weshowthatanybehaviourcomplyingwithphysicallysoundassumptionsabouthowmacroscopicbiological
agentsinteractwiththeworldcanonicallyintegratesexplorationandexploitationbyminimisingriskandambiguityabout
external states of the world. This description, known as active inference, refines the free energy principle, a popular
descriptive framework for action and perception birthed in neuroscience [10–12].
Active inference provides a generic framework to simulate and model agency that is widely used in neuroscience [13–17],
RL [18–21] and robotics [22–25]. The usefulness of active inference for RL is three-fold. a) Active inference provides
an effective solution to the exploration-exploration dilemma that englobes the principles of expected utility theory [3]
and Bayesian experimental design [26] and finesses the need for ad-hoc exploration bonuses in the reward function or
decision-making objective. b) Active inference provides a transparent recipe to simulate behaviour by minimising risk
and ambiguity with respect to an explicit generative world model. This enables safe and explainable decision-making by
specifically encoding the commitments and goals of the agent in the world model [23]. c) Active inference is universal in
the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active
inference as an active inference algorithm, e.g., [27]. Thus, active inference can be used as a tool to uncover and compare
the commitments and assumptions of more specific models of agency.
2 Deriving agency from physics
Wedescribesystemsthatcompriseanagentinteractingwithitsenvironment. Weassumethatanagentanditsenvironment
evolve together according to a stochastic process x. This definition entails a notion of time T, which may be discrete or
continuous, and a state space X, which should be a measure space (e.g., discrete space, manifold, etc.). Recall that a
stochastic process x is a time-indexed collection of random variables x on state space X. Equivalently, x is a random
t
variable over trajectories on the state space T → X. We denote by P the probability density of x on the space of
trajectories T →X (with respect to an implicit base measure).
In more detail, we factorise the state space X into states that belong to the agent H and states external to the agent S
that belong to the environment. Furthermore, we factorise agent’s states into autonomous states A and observable states
O, respectively defined as the states which the agent does and does not have agency over. In summary, the system x
is formed of external s and agent processes h, the latter which is formed of observable o, and autonomous a processes
X ≡S×H≡S×O×A =⇒ x≡(s,h)≡(s,o,a).
The description adopted so far could aptly describe particles interacting with a heat bath [28–30] as well as humans
interacting with their environment (Figure 1.A). We would like a description of macroscopic biological systems; so what
distinguishes people from small particles? A clear distinction is that human behaviour occurs at the macroscopic level
and, thus, is subject to classical mechanics. In other words, people are precise agents:
Definition 2.1. An agent is precise when it responds deterministically to its environment, that is when h | s is a
deterministic process.
Remark 2.2. It is important not to conflate an agent’s mental representations of external reality with external states; the
formerareusuallyanabstractionorcoarse-grainedrepresentationofthelatter. Wedonotconsideragent’srepresentations
inthispaper. Wesimplypositthatthereexistsadetailedenoughdescriptionoftheenvironmentthatdeterminestheagent’s
trajectory (e.g., observations and actions). This will always be true for agents evolving according to classical mechanics.
Agencymeansbeingincontrolone’sactionsandusingthemtoinfluencetheenvironment[31]. Attimet, theinformation
available to the agent is its past trajectory h = (o ,a ), i.e. its history. We define decision-making as a choice of
≤t ≤t ≤t
autonomous trajectory in the future a given available knowledge h . Furthermore, we define agency as the process
>t ≤t
throughwhichanagentmakesandexecutesdecisions. WeinterpretP(s,o|h )asexpressingtheagent’spreferences over
≤t
environmental and observable trajectories given available data, and P(s,o|a ,h ) as expressing the agent’s predictions
>t ≤t
over environmental and observable paths given a decision (e.g., Figure 1.B). Crucially, agency is governed by an expected
free energy functional of the agent’s predictions and preferences (Appendix D)
−logP(a |h )=E [logP(s|a ,h )−logP(s,o|h )]. (EFE)
>t ≤t P(s,o|a>t,h≤t) >t ≤t ≤t
We have formulated agency as optimising an objective: the lower the expected free energy, the more likely a course of
action, and vice-versa.
2
Figure 1: (A) This figure illustrates a human (agent process h) interacting with its environment (external process s), and the
resultingpartitionintoexternals,observableo,andautonomousaprocesses. Theagentdoesnothavedirectaccesstotheexternal
process, but samples it through the observable process. The observable process constitutes the sensory epithelia (e.g., eyes and
skin), which influences the environment through touch. The autonomous process constitutes the muscles and nervous system,
which influences the sensory epithelia, e.g., by moving a limb, and the environment, e.g., through speech by activating vocal cords.
Autonomous responses at time t+δt may depend upon all the information available to the agent at time t, that is h . Thus,
≤t
thesystemswearedescribingaretypicallynon-Markovian. (B)Anactiveinferenceagentiscompletelydescribedbyitsprediction
model P(s,o | a) and its preference model P(s,o). When the prediction model is a POMDP the preference model is a hidden
Markov model. In this setting, the colour scheme illustrates the problem of agency at t=1: the agent must execute an action (in
red) based on previous actions and observations (in grey), which are informative about external states and future observations (in
white). When specifying an active inference agent, it is important that prediction and preference models coincide on those parts
they have in common; in this example the likelihood map P(o|s). Note that these models need not be Markovian.
3 Characterising agency
This description of agency combines many accounts of behaviour that predominate in cognitive science and engineering.
Indeed, decomposing the expected free energy (EFE) reveals fundamental imperatives that underwrite agency, such as
minimising risk and ambiguity (Appendix D):
predictedpaths preferredpaths
−logP(a >t |h ≤t )=D KL (cid:2) P (cid:122) (s|a (cid:125) > (cid:124) t ,h ≤t (cid:123) )| (cid:122) P(s (cid:125) | (cid:124) h ≤t (cid:123) ) (cid:3) +E P(s|a>t,h≤t) (cid:2) H[P(o|s,h ≤t )] (cid:3) . (1)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
risk ambiguity
Risk refers to the KL divergence between the predicted and preferred external course of events. As minimising a reverse
KLdivergenceleadstomodematchingbehaviour[32],minimisingriskleadstorisk-averse decisionsthatavoiddisfavoured
coursesofevents(c.f.,AppendixB).Inturn,risk-aversionisahallmarkofprospecttheory,whichdescribeshumanchoices
under discrete alternatives with no ambiguity [33]. Additionally, risk is the main decision-making objective in modern
approaches to control as inference [34–37], variously known as Kalman duality [38,39], KL control [40] and maximum
entropy RL [41].
Ambiguityreferstotheexpectedentropyoffutureobservations,givenfutureexternaltrajectories. Anexternaltrajectory
thatmayleadtovariousdistinctobservationtrajectoriesishighlyambiguous—andvice-versa. Thus,minimisingambiguity
leadstosamplingobservationsthatenabletorecognisetheexternalcourseofevents. Thisleadstoatypeofobservational
bias commonly known as the streetlight effect or drunkard’s search in behavioural psychology [42]: when a person loses
their keys at night, they initially search for them under the streetlight because the resulting observations ("I see my keys
under the streetlight" or "I do not see my keys under the streetlight") accurately disambiguate external states of affairs.
Additionally, agency maximises extrinsic and intrinsic value (Appendix D):
preferredpaths
−logP(a |h )≥−E (cid:2) log P (cid:122) (o (cid:125) | (cid:124) h (cid:123) ) (cid:3) −E (cid:2) D [P (s|o,a ,h )|P (s|a ,h )] (cid:3) . (2)
>t ≤t P(o|a>t,h≤t) ≤t P(o|a>t,h≤t) KL >t ≤t >t ≤t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
extrinsicvalue intrinsicvalue
Extrinsic value refers to the (log) likelihood of observations under the model of preferences. Maximising extrinsic value
leads to sampling observations that are likely under the model of preferences. Through the correspondence between log
probabilities and utility functions [38,43,44] this is equivalent to maximising expected utility or expected reward. This
3
underwrites expected utility theory [3], game theory [3], optimal control [5,6] and RL [1]. Bayesian formulations of
maximising expected utility under uncertainty are also known as Bayesian decision theory [4].
Intrinsic value refers to the amount of information gained about external courses of events under a decision. Favouring
decisions to maximise information gain leads to a goal-directed form of exploration [15], driven to answer "what would
happen if I did that?" [45]. Interestingly, this decision-making procedure underwrites Bayesian experimental design [26]
andactivelearninginstatistics[46],intrinsicmotivationandartificialcuriosityinmachinelearningandrobotics[45,47–50].
This is mathematically equivalent to optimising expected Bayesian surprise and mutual information, which underwrites
visual search [51,52] and the organisation of our visual apparatus [53–55].
We have formulated agent’s behaviour as minimising an objective that canonically weighs an exploitative term with an
explorativeterm—riskplusambiguity—whichprovidesaprincipledsolutiontotheexploration-exploitationdilemma[56].
4 Simulating agency
Active inference specifies an agent by a prediction model P(s,o | a), expressing the distribution over external and ob-
servable paths under autonomous paths, and a preference model P(s,o), expressing the preferred external and observable
trajectories (e.g., Figure 1.B). In discrete time, agency proceeds by approximating the expected free energy given past
observations and actions h =(o ,a ) and use it to govern agency (details and simulations in Appendices A and B):
≤t ≤t ≤t
1. Preferential inference: inferpreferencesaboutexternalandobservabletrajectories, i.e., approximateP(s,o|h )
≤t
with Q(s,o|h ) (e.g., Figure 2).
≤t
2. For each possible sequence of future actions a :
>t
(a) Perceptual inference: infer external and observable paths under the action sequence, i.e., approximate
P(s,o| a ,h ) with Q(s,o|a ,h ).
>t ≤t >t ≤t
(b) Planning as inference: assess the action sequence by evaluating its expected free energy (EFE), i.e.,
−logQ(a |h )≡E (cid:2) logQ(s|a ,h )−logQ(s,o|h ) (cid:3) .
>t ≤t Q(s,o|a>t,h≤t) >t ≤t ≤t
3. Agency: execute the most likely decision a , or use Q(a |h ) as a model for behavioural data, where
t+1 t+1 ≤t
(cid:88)
a =argmaxQ(a |h ), Q(a |h )= Q(a |a )Q(a |h ).
t+1 t+1 ≤t t+1 ≤t t+1 >t >t ≤t
a>t
5 Concluding remarks
Activeinferenceisadescriptionofmacroscopicbiologicalagentsderivedfromphysics,whichprovidesagenericframework
to model biological and artificial behaviour. It provides a transparent description of agency in terms of minimising risk
and ambiguity (i.e., an expected free energy functional) under some prediction and preference models about the world.
This formulation is universal in the sense that any RL agent satisfying the descriptive assumptions of active inference
(Appendix D) is behaviourally equivalent to an active inference agent under some implicit prediction and preference
models.
Activeinferenceprovidesaneffectivesolutiontotheexploration-exploitationdilemma[56]thatfinessestheneedforad-hoc
exploration bonuses in the reward function or decision-making objective (Appendices B and C) [57–59] . In particular,
the expected free energy englobes various objectives used to describe or simulate behaviour across cognitive science and
engineering, endowing it with various useful properties such as information-sensitivity (Appendix C). Perhaps the closest
RL formulations are action and perception as divergence minimisation [60], which considers a similar decision-making
objective; control as inference, which can be seen as minimising risk but not ambiguity [34,37,41]; and Hyper [61], which
proposes reward maximisation alongside minimising uncertainty over both external states and model parameters.
Activeinferenceprovidesarecipeforsafealgorithmicdecision-making. Allbehaviourunderactiveinferenceisexplainable
as a mixture of exploration and exploitation under a generative world model, and all differences in behaviour are explicit
in differences in world model. In particular, the agent’s commitments and goals can be explicitly encoded in the world
model by the user. In addition, the expected free energy leads to risk-averse decisions (Appendix C), a crucial feature for
engineering applications where catastrophic consequences are possible, e.g., robot-assisted surgery [23].
Whileactiveinferenceclarifieshowagencyunfoldsgivenagenerativeworldmodelrepresentinghowtheenvironmentcauses
observations(Section4),itdoesnotspecifywhichrepresentationsunderwritehighlyintelligentbehaviour. Promisingsteps
in this direction include employing hierarchical probabilistic generative models with deep neural networks [13,24,62–64].
Beyondthis,weconcludebyasking: whatkindsofgenerativemodelsdohumansusetorepresenttheirenvironment? And:
what are the computational mechanisms under which a child’s mind develops into an adult mind by gradually learning
its world model [65–68]?
4
References
[1] A. Barto and R. Sutton. Reinforcement Learning: An Introduction. A Bradford Book, 1992.
[2] Jeremy Bentham. An Introduction to the Principles of Morals and Legislation. Dover Publications Inc., Mineola,
N.Y, June 2007.
[3] J. Von Neumann and O. Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, 1944.
[4] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics. Springer-Verlag, New
York, second edition, 1985.
[5] Richard E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, US, 1957.
[6] K. J Åström. Optimal control of Markov processes with incomplete state information. Journal of Mathematical
Analysis and Applications, 10(1):174–205, February 1965.
[7] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In
Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222–227,
1991.
[8] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised
prediction. In International conference on machine learning, pages 2778–2787. PMLR, 2017.
[9] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-
based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.
[10] KarlFriston,LancelotDaCosta,NoorSajid,ConorHeins,KaiUeltzhöffer,GrigoriosA.Pavliotis,andThomasParr.
Thefreeenergyprinciplemadesimplerbutnottoosimple. arXiv:2201.06387[cond-mat,physics:nlin,physics:physics,
q-bio], January 2022.
[11] K. Friston. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience, 11(2):127–138, 2010.
[12] K. Friston, J. Kilner, and L. Harrison. A free energy principle for the brain. J. Physiology-Paris, 100(1-3):70–87,
2006.
[13] Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston. The computational neurology of movement under
active inference. Brain, March 2021.
[14] G. Pezzulo, F. Rigoli, and K. J. Friston. Hierarchical Active Inference: A Theory of Motivated Control. Trends in
Cognitive Sciences, 22(4):294–306, April 2018.
[15] Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald, Martin Kronbichler, and
Karl J Friston. Computational mechanisms of curiosity and goal-directed exploration. eLife, page 45, 2019.
[16] Ryan Smith, Rayus Kuplicki, Justin Feinstein, Katherine L. Forthman, Jennifer L. Stewart, Martin P. Paulus,
Tulsa1000Investigators,andSahibS.Khalsa.ABayesiancomputationalmodelrevealsafailuretoadaptinteroceptive
precision estimates across depression, anxiety, eating, and substance use disorders. PLOS Computational Biology,
16(12):e1008484, December 2020.
[17] Takuya Isomura, Hideaki Shimazaki, and Karl J. Friston. Canonical neural networks perform active inference.
Communications Biology, 5(1):1–15, January 2022.
[18] Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive Active Inference. In Advances in Neural Information
Processing Systems, May 2021.
[19] ZafeiriosFountas,NoorSajid,PedroA.M.Mediano,andKarlFriston.DeepactiveinferenceagentsusingMonte-Carlo
methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.
[20] Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, and Stefan J. Kiebel. An empirical evaluation of active inference
in multi-armed bandits. Neural Networks, 144:229–246, December 2021.
[21] Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas, and Karl Friston. Exploration and preference
satisfaction trade-off in reward-free learning. arXiv preprint arXiv:2106.04316, 2021.
[22] Pablo Lanillos, Cristian Meo, Corrado Pezzato, Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata, Alexander
Tschantz, Beren Millidge, Martijn Wisse, Christopher L. Buckley, and Jun Tani. Active Inference in Robotics and
Artificial Agents: Survey and Challenges. arXiv:2112.01871 [cs], December 2021.
[23] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How Active Inference Could Help
Revolutionise Robotics. Entropy, 24(3):361, March 2022.
[24] Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation as hierarchical
active inference. Neural Networks, 142:192–204, October 2021.
[25] Hendry F. Chame and Jun Tani. Cognitive and motor compliance in intentional human-robot interaction. In 2020
IEEE International Conference on Robotics and Automation (ICRA), pages 11291–11297, May 2020.
[26] D. V. Lindley. On a Measure of the InformationProvided by anExperiment. The Annals of Mathematical Statistics,
27(4):986–1005, 1956.
[27] Manuel Baltieri and Christopher L. Buckley. PID Control as a Process of Active Inference with Linear Generative
Models. Entropy, 21(3):257, March 2019.
[28] Grigorios A. Pavliotis. Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin
Equations. Number volume 60 in Texts in Applied Mathematics. Springer, New York, 2014.
[29] Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov blankets, information geometry and stochastic ther-
modynamics. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
378(2164):20190159, February 2020.
5
[30] LucRey-Bellet. OpenClassicalSystems. InStéphaneAttal,AlainJoye,andClaude-AlainPillet,editors,OpenQuan-
tum Systems II: The Markovian Approach, LectureNotesinMathematics, pages41–78.Springer, Berlin, Heidelberg,
2006.
[31] Patrick Haggard and Manos Tsakiris. The experience of agency: Feelings, judgments, and responsibility. Current
Directions in Psychological Science, 18(4):242–246, 2009.
[32] Thomas Minka. Divergence measures and message passing. Technical report, 2005.
[33] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk. Econometrica,
47(2):263–291, 1979.
[34] Sergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.
arXiv:1805.00909 [cs, stat], May 2018.
[35] KonradRawlik,MarcToussaint,andSethuVijayakumar.OnStochasticOptimalControlandReinforcementLearning
by Approximate Inference. In Twenty-Third International Joint Conference on Artificial Intelligence, June 2013.
[36] Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th Annual
International Conference on Machine Learning, ICML ’09, pages 1049–1056, Montreal, Quebec, Canada, June 2009.
Association for Computing Machinery.
[37] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. On the Relationship Between Active
Inference and Control as Inference. In Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom,
editors, Active Inference, Communications in Computer and Information Science, pages 3–11, Cham, 2020. Springer
International Publishing.
[38] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering,
82(1):35–45, March 1960.
[39] Emanuel Todorov. General duality between optimal control and estimation. In 2008 47th IEEE Conference on
Decision and Control, pages 4286–4292, December 2008.
[40] Hilbert J. Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine Learning, 87(2):159–182, May 2012.
[41] B. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis,
Carnegie Mellon University, Pittsburgh, 2010.
[42] Abraham Kaplan. The Conduct of Inquiry. Transaction Publishers, 1973.
[43] John S. Bridle. Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to
Statistical Pattern Recognition. In Françoise Fogelman Soulié and Jeanny Hérault, editors, Neurocomputing, NATO
ASI Series, pages 227–236, Berlin, Heidelberg, 1990. Springer.
[44] R. Duncan Luce. Individual Choice Behavior. Individual Choice Behavior. John Wiley, Oxford, England, 1959.
[45] Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010). IEEE Transactions
on Autonomous Mental Development, 2(3):230–247, September 2010.
[46] David J. C. MacKay. Information-Based Objective Functions for Active Data Selection. Neural Computation,
4(4):590–604, July 1992.
[47] Pierre-YvesOudeyerandFredericKaplan. WhatisIntrinsicMotivation? ATypologyofComputationalApproaches.
Frontiers in Neurorobotics, 1:6, November 2007.
[48] A. Barto, M. Mirolli, and G. Baldassarre. Novelty or Surprise? Frontiers in Psychology, 4, 2013.
[49] Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian Exploration in
Dynamic Environments. arXiv:1103.5708 [cs, stat], March 2011.
[50] Edward Deci and Richard M. Ryan. Intrinsic Motivation and Self-Determination in Human Behavior. Perspectives
in Social Psychology. Springer US, New York, 1985.
[51] Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision research, 49(10):1295–1306, May
2009.
[52] Thomas Parr, Noor Sajid, Lancelot Da Costa, M. Berk Mirza, and Karl J. Friston. Generative Models for Active
Vision. Frontiers in Neurorobotics, 15, 2021.
[53] H. B. Barlow. Possible Principles Underlying the Transformations of Sensory Messages. The MIT Press, 1961.
[54] R Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and Information Theory.
Annual Review of Neuroscience, 13(1):257–281, 1990.
[55] L.M.OpticanandB.J.Richmond. Temporalencodingoftwo-dimensionalpatternsbysingleunitsinprimateinferior
temporal cortex. III. Information theoretic analysis. Journal of Neurophysiology, 57(1):162–178, January 1987.
[56] Oded Berger-Tal, Jonathan Nathan, Ehud Meron, and David Saltz. The Exploration-Exploitation Dilemma: A
Multidisciplinary Framework. PLOS ONE, 9(4):e95693, April 2014.
[57] Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Ready policy one:
World building through active learning. In International Conference on Machine Learning, pages 591–601. PMLR,
2020.
[58] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon White-
son. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. arXiv preprint arXiv:1910.08348,
2019.
6
[59] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by
latent imagination. arXiv preprint arXiv:1912.01603, 2019.
[60] Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action and Perception
as Divergence Minimization. arXiv:2009.01791 [cs, math, stat], October 2020.
[61] Luisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and Shimon White-
son. Exploration in approximate hyper-state space for meta reinforcement learning. In International Conference on
Machine Learning, pages 12991–13001. PMLR, 2021.
[62] AA. Soltani, H. Huang, J. Wu, T. Kulkarni, and J. Tenenbaum. Synthesizing 3d shapes via modeling multi-view
depth maps and silhouettes with deep generative networks. CVPR, 2017.
[63] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and active
inference. Neuroscience & Biobehavioral Reviews, 90:486–501, July 2018.
[64] N. Gothoskar, M. Cusumano-Towner, B. Zinberg, M. Ghavamizadeh, F. Pollok, A. Garrett, J. Tenenbaum, D. Gut-
freund, and V. Mansinghka. 3dp3: 3d scene perception via probabilistic programming. NeurIPS, 2021.
[65] D. Gowanlock R. Tervo, Joshua B. Tenenbaum, and Samuel J. Gershman. Toward the neural implementation of
structure learning. Current Opinion in Neurobiology, 37:99–105, April 2016.
[66] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building Machines That
Learn and Think Like People. arXiv:1604.00289 [cs, stat], April 2016.
[67] Tomer D. Ullman and Joshua B. Tenenbaum. Bayesian Models of Conceptual Development: Learning as Building
Models of the World. Annual Review of Developmental Psychology, 2(1):533–558, 2020.
[68] Noah D. Goodman, Tomer D. Ullman, and Joshua B. Tenenbaum. Learning a theory of causality. Psychological
Review, 118(1):110–119, 2011.
[69] AlessandroBarp, LancelotDaCosta, GuilhermeFrança, KarlFriston, MarkGirolami, MichaelI.Jordan, andGrigo-
rios A. Pavliotis. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. In Geometry and
Statistics, number 46 in Handbook of Statistics. Academic Press, 2022.
[70] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston. Active inference on discrete state-spaces: A
synthesis. J. Math. Psychology, 99:102447, 2020.
[71] Conor Heins, Beren Millidge, Daphne Demekas, Brennan Klein, Karl Friston, Iain Couzin, and Alexander Tschantz.
Pymdp: A Python library for active inference in discrete state spaces. arXiv:2201.03904 [cs, q-bio], January 2022.
[72] RyanSmith,KarlJ.Friston,andChristopherJ.Whyte. Astep-by-steptutorialonactiveinferenceanditsapplication
to empirical data. Journal of Mathematical Psychology, 107:102632, April 2022.
[73] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference: The Free Energy Principle in Mind, Brain,
and Behavior. MIT Press, Cambridge, MA, USA, March 2022.
[74] SarahSchwöbel,StefanKiebel,andDimitrijeMarković. ActiveInference,BeliefPropagation,andtheBetheApprox-
imation. Neural Computation, 30(9):2530–2567, September 2018.
[75] Théophile Champion, Howard Bowman, and Marek Grześ. Branching Time Active Inference: Empirical study and
complexity class analysis. arXiv:2111.11276 [cs], November 2021.
[76] Théophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grześ. Branching Time Active Inference:
The theory and its generality. arXiv:2111.11107 [cs], November 2021.
[77] Domenico Maisto, Francesco Gregoretti, Karl Friston, and Giovanni Pezzulo. Active Tree Search in Large POMDPs.
arXiv:2103.13860 [cs, math, q-bio], March 2021.
[78] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active Inference:
A Process Theory. Neural Computation, 29(1):1–49, January 2017.
[79] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystified and Compared. Neural
Computation, 33(3):674–712, January 2021.
[80] Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha Ondobaka. Active
Inference, Curiosity and Insight. Neural Computation, 29(10):2633–2683, October 2017.
[81] ThomasParr. The Computational Neurology of Active Vision. PhDthesis,UniversityCollegeLondon,London,2019.
[82] B. Millidge. Deep active inference as variational policy gradients. J. Math. Psychology, 96:102348, 2020.
[83] K. Friston, Lancelot Da Costa, Dalton A. R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis, and Thomas Parr.
Path integrals, particular kinds and strange things. 2022.
[84] Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, Bayesian optimal design, and
expected utility. arXiv:2110.04074 [cs, math, stat], September 2021.
A Details on the active inference algorithm
One specifies an active inference agent by a prediction model P(s,o | a) and a preference model P(s,o). Taking the
exampleofFigure1.B,thepredictionmodelmaybeapartially observable Markov decision process (POMDP).APOMDP
is a discrete time model of how actions influence external and observable trajectories. In a POMDP, 1) each external
state depends only on the current action and previous external state P(s | s ,a ), and 2) each observation depends
t t−1 t
only on the current external state P(o | s ). If one additionally specifies 3) a distribution of preferences over external
t t
trajectoriesP(s),oneobtainsa(hiddenMarkov)preferencemodelbycombining2)&3). Ingeneral,thepreferencemodel
7
may be specified independently from the prediction model, however, since these are the same distribution conditioned on
different variables it is important that they coincide on the parts they have in common; in this example, the likelihood
map P(o |s ). Importantly, the generative models specify the temporal horizon of the agent (i.e., how much time ahead
t t
it plans and represents the world).
Under this model specification, agency is governed by the expected free energy (EFE)
−logP(a |h )=E [logP(s|a ,h )−logP(s,o|h )].
>t ≤t P(s,o|a>t,h≤t) >t ≤t ≤t
The standard active inference algorithm (Section 4) first
approximates the expected free energy by approximating
the usually intractable posterior distributions within. For
instance, preferential inference involves inferring prefer-
encesgivenpreviousactionsandobservationsP(s,o|h ).
≤t
Observe that this distribution factorises
P(s,o|h )=P(s ,o |h )P(s ,o |x ) (3)
≤t ≤t ≤t ≤t >t >t ≤t
leadingtoefficientapproximationinavarietyofcases(e.g.,
Figure 2). More simply, perceptual inference simply in-
volves conditioning the prediction model on past observa-
tions
P(s,o|a ,h )=P(s,o|a,o ).
>t ≤t ≤t
Once one has obtained an approximate posterior distribu-
tion over the next action Q(a | h ), one can either
t+1 ≤t
execute the most likely action as shown in Section 4, or
simulate generic behaviour by sampling actions from the
posterior distribution
a ∼Q(a |h )
t+1 t+1 ≤t
(notshowninSection4). Thelatteristobeemployedwhen
usingactiveinferenceasagenerativemodelforbehavioural Figure 2: Preferential inference on POMDPs. This figure
data. illustrateshowonemayinferpreferencesefficientlywhenthepre-
diction model is a POMDP and the preference model is a hidden
There are many ways to scale the active inference algo- Markov model (c.f., Figure 1.B). The problem is illustrated at
rithm for practical applications [69]. These generally in- time t = 1. We may approximate the posterior preferences over
volve employing hierarchical generative models with deep states and observations given available data P(s,o | h ) by the
≤t
neural networks [24,63], structured mean-field approxi- product of the predictions of past states and observations given
mations [70–74], amortisation [19], and Monte-Carlo tree available data P(s ≤t ,o ≤t | h ≤t ) and the preferences over future
search [19,75–77]. Furthermore, this algorithm can also be states and observations given the current state P(s >t ,o >t | s t ),
c.f., (3). These two distributions can, in turn, be obtained from
used to learn the agent’s prediction and preference mod-
the prediction and preference models via approximate inference.
els [69].
More simply, when preferences P(s ,o ) are i.i.d. for all t (c.f.,
t t
Figure 3), the latter distribution simply equals future preferences
B Simulations of behaviour P(s >t ,o >t ).
Here is a simple simulation of behaviour in a T-Maze environment (Figure 3). The environment has four spatial locations
(top left, top right, middle and bottom). One of the top arms yields a reward of 1000$ while the other yields a loss of
1000$—per time-step spent in each respective location. The two remaining locations have no reward (0$). Suppose that
you start in the middle arm while being unaware of the reward’s location. The task has two time-steps, so you may visit
two (not necessarily distinct) locations of the Maze in the order you wish. At first, you may: a) stay where you are, b)
go to the bottom arm to collect a cue that discloses the reward’s location, c) go to one of the top arms to determine the
reward’s location by elimination, even though this risks receiving the punishment. Options b) and c) both enable you to
collect 1000$ upon visiting the reward’s location at the second step of the task. In choosing policy b) the payoff is 1000$
with certainty. In choosing policy c) the payoff is variable: 2000$,1000$ or 0$ with 1/3 probability each. In choosing
policy a), the payoff is strictly worse. Which of these options a), b) or c) will you choose?
Inhumans,themostcommonchoiceisoption(b)—visitingthebottomarmtocollectthecue,therebyinferringthereward’s
location—and subsequently collecting the reward [33], while always avoiding the punishment. We now compare the
behavioursofactiveinferenceagents(Section4),rewardmaximisingagents,andrewardplusinformationgainmaximising
agents:
1. Active inference agent. o : The agent is in the middle of the Maze and is unaware of the context. a : Visiting the
0 1
bottom or top arms have a lower ambiguity than staying, as they yield observations that disclose the context. However,
8
Figure3: T-Mazeenvironment. ThisFigureillustratesasimplesequentialdecision-makingtaskwithatemporalhorizonof2. In
other words, the agent must choose a given h =o and, subsequently, a given h =(a ,o ). In more detail, s : The T-Maze
1 0 0 2 ≤1 1 ≤1 t
has four possible spatial locations: middle, top-left, top-right, bottom. One of the top locations yields a reward of 1000$ (money
bag), while the other yields a punishment of -1000$ (flying money bag)—per time-step spent in respective locations. The reward’s
location determines the context. The bottom arm contains a cue whose colour (blue or green) discloses the context. Together,
locationandcontextdeterminetheexternalstate. o : Theagentalwaysobservesitsspatiallocation. Inaddition,whenitisatthe
t
topoftheMaze,itreceivestherewardorthepunishment;whenitisatthebottom,itobservesthecolourofthecue. a : Eachaction
t
correspondstovisitingoneofthefourspatiallocations. P(s ): Theagentprefersbeingatthereward’slocation(−logP(s )=1000)
t t
and avoid the punishment’s location (−logP(s )=−1000). All other external states have a neutral preference (−logP(s )=0).
t t
s : the agent starts in the middle location and the context is initialised at random. Together these datum determine prediction
0
P(s,o|a) and preference P(s,o) models, which in turn, determine agency by minimisation of expected free energy (Section 4).
staying or visiting the bottom arm are safer options, as visiting a top arm risks receiving the punishment. By acting to
minimisebothriskandambiguity (1)theagentgoestothebottom. o : Theagentobservesthecueandhencedetermines
1
the context. a : All actions have equal ambiguity as the context is known. Collecting the reward has a lower risk than
2
staying or visiting the middle, which themselves have a lower risk than collecting the punishment. Thus, the agent visits
the arm with the reward. See [15,78] for more details. In summary, the active inference agent ends the task with 1000$
with certain probability.
2. Expected reward maximising agent. o : idem. a : all locations have the same expected reward of 0$, so it is
0 1
unclear where to go. Suppose then that the agent chooses an action at random according to an uniform distribution.
o : In the event that it has chosen to leave the middle location (i.e., 75% of the time), this enables it to unambiguously
1
determine the reward’s location. a : the agent collects the reward. In summary, 75% of the time, the agent ends the task
2
with 2000$,1000$ or 0$ with 1/3 probability, respectively. 25% ofthe time, it’sperformancewill beworse: 1000$,−1000$
with 1/4 probability, respectively, and 0$ with probability 1/2 probability.
3. Expected reward plus information gain maximising agent. o : idem. a : all locations have the same expected
0 1
reward of 0$ and the bottom and top locations give information as to the reward’s location. Therefore, in maximising the
sum of expected utility and expected information gain the agent leaves the middle location. a : the agent collects the
2
reward. In summary, the agent ends the task with 2000$,1000$ or 0$ with 1/3 probability, respectively.
We analyse these differences in behaviour in Appendix C. For more complex simulations of sequential decision-making
using active inference, please see [19,24,63,79–82].
9
C Comparing expected free energy, expected reward, and expected reward plus
information gain
So what distinguishes these objectives? Following the simulations of Appendix B we distinguish two main features:
information-sensitivity, i.e., encoding the value of information, and risk-aversion (see Table 1). Briefly,
Objective Information-sensitive Risk-averse
Expected free energy ✓ ✓
Expected reward ✗ ✗
Expected reward plus information gain ✓ ✗
Table 1: Comparing objectives.
1. Information-sensitivity. Optimising expected reward does not intrinsically value the information gained following
an action. In contrast, the expected free energy, or an expected information gain add-on to expected reward, both select
actions that value the information afforded by an action.
2. Risk-aversion. Expected reward and expected information-gain maximising agents cannot distinguish between two
alternatives with the same expected reward and information gain. In that sense, they are perfectly rational agents. In
contrast, active inference agents are risk-averse to the extent that they will avoid choices that can yield to a large loss,
even when the expected payoff is neutral. Being risk-averse in this sense has two advantages: a) in modelling human
behaviour, as this is a defining feature of human decision-making [33]. b) In engineering applications where the actions of
artificial agents can potentially have catastrophic consequences—such as in algorithmic trading, human-robot interaction
and robot-assisted surgery—risk-aversion enables safe decision-making by actively avoiding negative outcomes [23].
Inducing information-sensitivity and risk-aversion by tuning the reward function. Note that we can endow
expected reward (ER) and expected reward plus information gain (ERIG) agents with information-sensitivity and risk-
aversionbysimplytuningtherewardfunction. Forinstance,inthesimulationsofAppendixB,wecanpenalisethemiddle
locationoftheT-Mazeby−1$asitaffordsnoinformation. Inthiscase,theERandERIGmaximisingagentswillbehave
the same as the initial ERIG maximising agent (Simulation 3). Furthermore, in remarking that losing 1000$ hurts more
than the gain afforded by receiving 1000$ we may update the (negative) reward associated with the loss to −1001. With
thistheERandERIGmaximisingagentswillbehaveexactlyastheactiveinferenceagentwiththeinitialrewardfunction
(Simulation 1).
Yet, tuning the reward function to achieve desired behaviour has two main drawbacks. For example, should we score the
punishment by −1001 or −1005? There is no definite answer. Though these will not affect behaviour in the T-Maze task,
this will not be the case in complex environments where different reward functions will lead to (sometimes unexpectedly)
distinct behaviour. Secondly, it is well-known that manually tuning the reward to produce desired behaviour is a difficult
and impractical task, especially in complex or changing environments.
We conclude that it is best to work with the ground truth reward whenever this is unambiguously defined (e.g., monetary
reward or payoff by reaching a goal) and for information-sensitivity and risk-aversion to be incorporated in the agent’s
decision-makingobjective. ThoughmanyapproachestoRLintegrateinformation-sensitivity[59,61]andrisk-aversion[34,
41] with reward maximisation, active inference allows to canonically and transparently integrate these imperatives in the
objective function.
D Assumptions and mathematical derivations
This Appendix lists the different assumptions and mathematical arguments that underwrite the derivation of active
inference presented in Sections 2 and 3.
Assumptions:
1. Stochastic process: Agent and environment evolve according to a stochastic process x≡(s,h)≡(s,o,a).
2. Precise agent: The description of external state space is sufficiently detailed such that h | s is a deterministic
process.
3. Countability of the path space: The path space T →X is countable (e.g., time T is countable and the state
spaceX isfinite,orvice-versa). Presumably,ourresultscanbeextendedtouncountablepathspacesbyalimiting
argument. This is left for future work.
UnderthecountabilityassumptionwemaytakeP tobetheprobabilitydensityoftheprocessw.r.t. theuniformmeasure.
We have [83]:
10
−logP(a |h )=E [−logP(a |h )]
>t ≤t P(s,o|a>t,h≤t) >t ≤t
=E [logP(s,o|a ,h )−logP(s,o,a |h )]
P(s,o|a>t,h≤t) >t ≤t >t ≤t
=E [logP(o|s,a ,h )+logP(s|a ,h )−logP(a |s,o,h )−logP(s,o|h )].
P(s,o|a>t,h≤t) >t ≤t >t ≤t >t ≤t ≤t
Lemma D.1. Under the countability and precise agent assumptions, we have for any value of a ,h
>t ≤t
E [logP(o|s,a ,h )−logP(a |s,o,h )]=0.
P(s,o|a>t,h≤t) >t ≤t >t ≤t
Proof. Bythepreciseagentassumption,sdetermines(o,a ). Inotherwords,therearefunctionsf :(T →S)→(T →O)
>t
and g :(T →S)→(T →A) such that (o,a )=(f,g)◦s. In particular,
>t >t
P(o|s,a ,h )=P(o|s,o )∝P(o|s)=δ (o), P(a |s,o,h )=P(a |s)=δ (a ),
>t ≤t ≤t f(s) >t ≤t >t g(s) >t
P(s,o|a ,h )=P(o|s,a ,h )P(s|a ,h )∝δ (o)P(a |s,h )P(s|h )=δ (o)δ (a )P(s|h ).
>t ≤t >t ≤t >t ≤t f(s) >t ≤t ≤t f(s) g(s) >t ≤t
Therefore we can compute the expectation, for any value of a :
>t
E [logP(o|s,a ,h )−logP(a |s,o,h )]=E (cid:2) logδ (o)−logδ (a ) (cid:3)
P(s,o|a>t,h≤t) >t ≤t >t ≤t δf(s)(o)δg(s)(a>t)P(s|h≤t) f(s) g(s) >t
=E (cid:2) logδ (f(s))−logδ (g(s)) (cid:3) =E [log1−log1]=0.
P(s|h≤t) f(s) g(s) P(s|h≤t)
By Lemma D.1, we conclude that agency can be expressed as a functional of the agent’s predictions and preferences,
known as the expected free energy [10,69]
−logP(a |h )=E [logP(s|a ,h )−logP(s,o|h )] (EFE)
>t ≤t P(s,o|a>t,h≤t) >t ≤t ≤t
Furthermore, we can rewrite the expected free energy (EFE) in the following ways [69,84]
−logP(a |h )=D (cid:2) P(s|a ,h )|P(s|h ) (cid:3) +E (cid:2) −logP(o|s,h ) (cid:3)
>t ≤t KL >t ≤t ≤t P(s,o|a>t,h≤t) ≤t
=−E (cid:2) logP (o|h ) (cid:3) −E (cid:2) D [P (s|o,a ,h )|P (s|a ,h )] (cid:3)
P(o|a,h≤t) ≤t P(o|a>t,h≤t) KL >t ≤t >t ≤t
+E (cid:2) D [P (s|o,a ,h )|P (s|o,h )] (cid:3)
P(o|a>t,h≤t) KL >t ≤t ≤t
≥−E (cid:2) logP (o|h ) (cid:3) −E (cid:2) D [P (s|o,a ,h )|P (s|a ,h )] (cid:3) .
P(o|a,h≤t) ≤t P(o|a>t,h≤t) KL >t ≤t >t ≤t
11

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Active Inference as a Model of Agency"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.